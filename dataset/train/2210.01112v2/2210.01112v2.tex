\documentclass{article}
\usepackage{xcolor}         \usepackage{graphicx}
\usepackage{amsmath,bm}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs, multirow,makecell} \usepackage{soul}\usepackage{adjustbox}
\usepackage{caption}


\definecolor{revise}{RGB}{0, 0, 0}
\newcommand{\revise}[1]{\textcolor{revise}{#1}}

\usepackage[final]{corl_2022} 

\title{Generative Category-Level Shape and Pose Estimation with Semantic Primitives}





\author{
\makecell[c]{Guanglin Li~~~Yifeng Li~~~Zhichao Ye~~~Qihang Zhang \\ Tao Kong ~~~ Zhaopeng Cui~~~Guofeng Zhang}
	\\
\makecell[c]{State Key Lab of CAD\&CG, Zhejiang University ~~~ ByteDance AI Lab\\ Multimedia Laboratory, The Chinese University of Hong Kong}\\
}


\begin{document}
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
{\let\thefootnote\relax\footnotetext{
Work done during Guanglin Li's internship at ByteDance.}}

\begin{abstract}
Empowering autonomous agents with 3D understanding for daily objects is a grand challenge in robotics applications.
When exploring in an unknown environment, existing methods for object pose estimation are still not satisfactory due to the diversity of object shapes.
In this paper, we propose a novel framework for category-level object shape and pose estimation from a single RGB-D image.
To handle the intra-category variation, we adopt a semantic primitive representation that encodes diverse shapes into a unified latent space, which is the key to establish reliable correspondences between observed point clouds and estimated shapes.
Then, by using a SIM(3)-invariant shape descriptor, we gracefully decouple the shape and pose of an object, thus supporting latent shape optimization of target objects in arbitrary poses.
Extensive experiments show that the proposed method achieves SOTA pose estimation performance and better generalization in the real-world dataset. Code and video are available at \url{https://zju3dv.github.io/gCasp}.



\end{abstract} \keywords{Category-level Pose Estimation, Shape Estimation} 
\section{Introduction}
Estimating the shape and pose for daily objects is a fundamental function which has various applications, including 3D scene understanding, robot manipulation and autonomous warehousing~\cite{corl_detect,corl_sornet,corl_manipulation,corl_place,corl_reori,sucar2020nodeslam}. 
Early works for this task are focused on \textit{instance-level} pose estimation~\cite{xiang2017posecnn,wang2019densefusion,peng2019pvnet,he2020pvn3d,li2019cdpn}, which aligns the observed object with the given CAD model. 
However, such a setting is limited in real-world scenarios since it is hard to obtain the exact model of a casual object in advance.
To generalize to these unseen but semantically familiar objects, \textit{category-level} pose estimation is raising more and more research attention~\cite{NOCS,FS-Net,SPD,SGPA,deng2022icaps,lin2022sar} since it could potentially handle various instances of the same category in real scenes.

Existing works on category-level pose estimation usually 
try to predict pixel-wise normalized coordinates for instances within one class~\cite{NOCS} or adopt a canonical prior model with shape deformations to estimate object poses~\cite{SPD, SGPA}.
Although great advances have been made, these one-pass prediction methods are still faced with difficulties when large shape differences exist within the same category.
To handle the variety of intra-class objects, some works~\cite{shan2021ellipsdf,deng2022icaps} leverage neural implicit representation~\cite{park2019deepsdf} to fit the shape of the target object by iteratively optimizing the pose and shape in a latent space, and achieve better performance.
However, in such methods, the pose and shape estimation are coupled together and rely on each other to get reliable results. Thus, their performance  is unsatisfactory in real scene~(see Sec.~\ref{ssec:compare_sota}).
So we can see that the huge intra-class shape differences and coupled estimation of shapes and poses are two main challenges for the existing category-level pose estimation methods. 

To tackle these challenges, we propose to estimate the object poses and shapes with \textit{semantic primitives} from a generative perspective.
The insight behind this is that the objects of a category are often composed of components with the same semantics although their shapes are various, \emph{e.g.}, a cup is usually composed of a semicircular handle and a cylindrical body (see Fig.~\ref{fig:teaser}(a)). 
This property inspired us to handle huge differences in intra-class shapes by fusing this category-level semantic information. Specifically, 
we propose to encode object shapes into semantic primitives, which construct a unified semantic representation among objects in the same category (see Fig.~\ref{fig:teaser}(b)).
Following Hao et al.~\cite{DualSDF}, we map diverse shapes and the corresponding semantic primitives into the same shared latent space of a generative model.
In this way, we can obtain the correspondence between the observed point cloud and the latent space by dividing the observed point cloud into several semantic primitives with a part segmentation network.

In order to disentangle the shape and pose estimation, we further
extract a novel SIM(3)-invariant descriptor from the geometric distribution of the point cloud's semantic primitives. 
By leveraging such SIM(3)-invariant descriptors, we can perform latent optimization of object shapes in arbitrary poses and obtain the resulting shape consistent with the observation.
After obtaining the estimated shape, we use a correspondence-based algorithm to recover the size and pose by aligning semantic primitives of the target object with that of the estimated shape.


In summary, our contributions are as follows:
\textbf{1)}
We propose a novel category-level pose estimation method, which uses semantic primitives to model diverse shapes and bridges the observed point clouds with implicitly generated shapes.
\textbf{2)}
We propose an online shape estimation method, which enables object shape optimization in arbitrary poses with a novel SIM(3)-invariant descriptor.
\textbf{3)}
Even when training on the synthetic dataset only, our method shows good generalization and achieves state-of-the-art category-level pose estimation performance on the real dataset.

\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{0.cm}
    \includegraphics[width=\textwidth]{pictures/teaser.pdf}
    \caption{(a)~An example of different instances in the same categories, we highlight the same semantic parts~(\emph{e.g.},~lens of cameras) between different instances.
    (b)~The detailed shapes~(top row) are decomposed into semantic primitives~(bottom row). Different colors on primitives indicate different semantic labels, which are consistent among different shapes.
    (c)~An overview of the generative model consisting of two auto-decoders (\emph{i.e.},  and ) sharing the same latent space.  captures fine details, and  represents the shapes by simple and semantically consistent shape primitives. }
    \label{fig:teaser}
\vspace{-0.4cm}
\end{figure}
 \section{Related Works}
\textbf{Instance-Level Object Pose Estimation.}
Instance-level pose estimation assumes the CAD model of test objects is known in the training phase, and previous methods are mainly divided into three categories. The first category of methods~\cite{schwarz2015rgb,wang2019densefusion,xiang2017posecnn,kehl2017ssd} directly regress the 6D object pose from RGB or RGB-D images. PoseCNN~\cite{xiang2017posecnn} extends 2D detection architecture to regress 6D object pose from RGB images, and DenseFusion~\cite{wang2019densefusion} combines color and depth information from the RGB-D input and makes the regressed pose more accurate. The second are correspondence-based methods ~\cite{peng2019pvnet,he2020pvn3d,li2019cdpn,oberweger2018making}. They predict the correspondences between 2D images and 3D models, and then recover the pose by Perspective-n-Point algorithm~\cite{lepetit2009epnp}. For example, PVNet~\cite{peng2019pvnet} predicts the 3D keypoints on the RGB image by a voting scheme. CDPN~\cite{li2019cdpn} predicts the dense correspondences between image pixels and 3D object surface. The third category is rendering-based methods~\cite{park2020latentfusion,yen2021inerf}, which recover the pose by minimizing the re-projection error between the posed 3D model and 2D image through a differentiable render. Compared with these instance-level methods, our category-level method does not need the known CAD model as prior.

\textbf{Category-Level Object Pose Estimation.}
Category-level methods \cite{NOCS, CASS, FS-Net} aim at estimating the pose for the arbitrary shape of the same category. To overcome the intra-category variations, NOCS~\citep{NOCS} introduces normalized object canonical space, which establishes a unified coordinate space among instances of the same category. CASS~\cite{CASS} learns a variational auto-encoder to recover object shape and predicts the pose in an end-to-end neural network. These methods directly regress the pose or coordinates, and thus struggle to form an intra-category representation.
Other methods utilize a prior model of each category for the category-level representation.
For example, SPD~\cite{SPD} and SGPA~\cite{SGPA} predict deformation on canonical prior models and recover the 6D pose through correspondence between the deformed model and the observed point cloud. However, when facing a significant difference between the prior model and the target object, the deformation prediction tend to fail.
DualPose~\cite{DualPose} takes the consistency between an implicit pose decoder and an explicit one into account and refines the predicted pose during testing. Besides, FS-Net~\cite{FS-Net} proposes a novel data augmentation mechanism to overcome intra-category variations. SAR-Net~\cite{lin2022sar} recovers the shape by modeling symmetric correspondence of objects, which makes pose and size prediction easier. Rencently, CenterSnap~\cite{irshad2022centersnap} jointly optimizes
detection, reconstruction and pose when training and achieves real-time inference performance.
To utilize the diverse and continuous shapes in generative models, iCaps\cite{deng2022icaps} and ELLIPSDF\cite{shan2021ellipsdf} introduce the generative models~\cite{park2019deepsdf, DualSDF} to their shape and pose estimation. Shapes and poses are jointly optimized in these works. Unlike these previous works, we establish a semantically consistent representation and decouple the shape and pose estimation, which makes the optimization more robust.
 \section{Method}
\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{0.cm}
    \includegraphics[width=\textwidth]{pictures/overview.pdf}
    \caption{Overview of the proposed method. 
    The input of our method is the point cloud observed from a single-view depth image.
(a)~Our method first extracts semantic primitives  from object point cloud  by a part segmentation network . (b)~We calculate a SIM(3)-invariant shape descriptor  from  and optimize a shape embedding  in the latent space by the difference between  and , where  and  are the coarse and fine branches of the generative model , detailed in Sec.~\ref{sec:objectrep}. (c)~The similarity transformation  is recovered through the semantic correspondences between  and optimized shape .
    (d)~Further, we can simply apply the transformation  on the fine geometry generated through , and reconstruct the object-level scene as a by-product besides our main results, which is detailed in the Sec.~\ref{supp:sec:mesh} in the supp. material. 
    }
    \label{fig:overview}
\vspace{-0.4cm}
\end{figure}

\textbf{Problem Formulation.}~
Given observed point cloud  of an object with  known category, our goal is to estimate the complete shape, scale and 6D pose for this object. The recovered shape is represented by a shape embedding  in a latent space. The estimated scale and 6D pose are represented as a similarity transformation , where scale , 3D rotation  and 3D translation . SIM(3) and SO(3) indicate the Lie group of 3D similarity transformations and 3D rotation, respectively.

\textbf{Overview.}~
As illustrated in Fig~\ref{fig:overview}, our method takes the RGB-D image as input. 
A Mask-RCNN\cite{he2017mask} network is used to obtain the mask and category of each object instance. In the following stages, our method only processes the observed instance point cloud back-projected from the masked depth image. 
To overcome the huge intra-category variations, 
we utilize a unified intra-category representation of semantic primitives ~(Sec~\ref{sec:objectrep}). 
Then, we process the observed point cloud with a part segmentation network to extract semantic primitives which establish a connection between the observed object and the latent space~(Sec~\ref{sec:segmentation}). 
To estimate the shape of the target with its pose unknown, we design a SIM(3)-invariant descriptor to optimize the generative shape in the latent space~(Sec~\ref{sec:srt_feature}). 
Finally, we estimate the 6D pose and scale of the object through the semantic correspondence between the observed point cloud and our recovered generative shape~(Sec~\ref{sec:pose}).

\subsection{Semantic Primitive Representation}
\label{sec:objectrep}



The huge intra-category differences make it challenging to represent objects with a standard model.
To overcome this problem, we utilize a semantic primitive representation.
The insight behind the semantic primitive representation is that although different instances in the same category have various shapes, they tend to have similar semantic parts, \emph{e.g.}, each camera instance has a lens, and each laptop has four corners (see Fig~\ref{fig:teaser}(a)). 
In semantic primitive representation, each instance is decomposed into several parts, and each part always corresponds to the same semantic part in different instances~(see Fig~\ref{fig:teaser}(b)). 
To generate this representation, we learn a generative model  following the network structure in \cite{DualSDF} and map instances of the same category into a latent space. 


Specifically, Fig~\ref{fig:teaser}(c) gives an overview of the generative model. 
The generative model  expresses shapes at two granular levels, one capturing detailed shapes and the other representing an abstracted shape using semantically consistent shape primitives.
We note the fine-scale model as  and the coarse-scale one as .  and  are two auto-decoders sharing the same latent code~. , where  is a 3D position in canonical space and  denotes the signed distance function. , where  is the number of primitives and  is the parameters of a primitive. We use sphere primitives here and , where  is the 3D center of a sphere and  is the radius. Please refer to \cite{DualSDF} and Sec.~\ref{supp:sec:dualsdf} in the supp. material for more details.


\subsection{Extract Semantic Primitives from Point Cloud}
\label{sec:segmentation}
We use a part segmentation network to obtain the semantic primitives of the observation point cloud in two stages. 
First, we predict a semantic label  for each point  in the observed point cloud , and then a centralization operation is performed on the points to extract the center of each primitive.

\textbf{Semantic Label Prediction.}~
For each point , its semantic label  is defined in terms of closest primitive center :

We treat point label prediction as a classification problem and follow the part segmentation version of 3D-GCN~\cite{3D-GCN} to perform point-wise classification. 
Standard cross entropy is used as the loss function. 
In addition, because of imperfect segmentation masks produced by Mask R-CNN, the point cloud back-projected from the masked depth image may contain extra background points. 
For these outlier points, we add another ``dust'' label in the point-wise classification to filter out them.

\textbf{Symmetric Loss.}~
Many common objects in robot manipulation have symmetrical structures, and most objects~(\emph{e.g.,~bottles}) are symmetrical about an axis. The unique pre-labeled ground truth cause ambiguity when the object rotates around the axis. In order to eliminate the influence of symmetric structure on our classification task, we introduce the symmetric loss as the same as \cite{NOCS}. For each symmetric object in the dataset, we define a symmetric axis and make the loss identical when the object rotates around the axis. Specifically, we generate a separate ground truth for each possible angle of rotational symmetry. Given the rotation angle , the rotated semantic label  is defined as:

where  is the position of  after  degrees of rotation about the axis. Then we define the symmetric loss  as:

where  is the standard cross entropy,  is the predicted label by the network, and we set  for non-symmetric objects and  for symmetric objects in our experiments. 

\textbf{Primitive centralization.}
After predicting the semantic label of each 3D point, we count the semantic labels  which appear in the partially observed point cloud. Then, we calculate the primitive centers  from the labeled points by averaging the points with the same label. Specifically, 

where  is the total number of points with predicted label ,  is an indication function indicating  whether the predicted label  equals .

\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{0.cm}
    \includegraphics[width=0.4\textwidth]{pictures/srt.pdf}
     \caption{The proposed atomic SIM(3)-invariant descriptor. The angle  and , which we called atomic descriptors are invariant with different object poses. And our shape descriptor concatenated by multiple atomic descriptors is also invariant.}
    \label{fig:srt_feature}
\vspace{-0.4cm}
\end{figure}
\subsection{Shape Optimization with SIM(3)-Invariant Descriptor}
\label{sec:srt_feature}

We decouple the shape estimation from the pose estimation to perform shape estimation for the observed object when the pose is unknown.
In order to achieve this pose-invariant shape optimization, we propose a novel SIM(3)-invariant descriptor extracted from semantic primitive representation.

\textbf{SIM(3)-invariant Descriptor.}~Given a set of 3D points  and a SIM(3)-transformation , a SIM(3)-invariant descriptor  can be expressed as:

To make our descriptor SIM(3)-invariant, first, we obtain a translation-invariant descriptor by calculating the vectors  composed of every two points:

After that, we normalize the vector  to eliminate the scale factor :

Finally, we eliminate the effect of rotation transformation by calculating the dot product between two vectors~(see Fig.~\ref{fig:srt_feature}):

\revise{
where  denotes the dot product of two vectors, and  denotes a function which takes 4 points as input and a SIM(3)-invariant scalar as output~(as the angles shown in Fig.~\ref{fig:srt_feature})}

\revise{
We call  the atomic descriptor. However, only a single atomic descriptor calculated from 4 points~(primitives centers) is not enough to describe an object's shape. Thus, we design the SIM(3)-invariant shape descriptor through a function , which concatenates multiple atomic descriptors to form a SIM(3)-invariant shape descriptor. With our estimated primitives  and their semantic labels , the set  is denoted as:}


\revise{
and the SIM(3)-invariant shape descriptor  is denoted as:}


\revise{
where  denotes the concatenate operation which concatenates multiple scalars into a vector in order, and  is a subset randomly chosen from  for each optimization. We choose the subset of  because  would be large, which is computational expensive. In Sec.~\ref{sec:ablation}, an ablation is conducted to analyze the effect of , which indicates the number of atomic descriptors in a shape descriptor.
}

\textbf{Online Shape Optimization.}~
Given the shape embedding  in the latent space and estimated primitives  with semantic labels , we can measure the shape difference between the generative shape  and the observation  through the SIM(3)-invariant descriptor. We denote this difference as :

where  is the primitive centers of .
Obviously,  is a differentiable function and  is a differentiable neural network. We optimize the shape embedding  via gradient descent in an online iterative way and set  to weight the regularization term. 

\subsection{6D Pose and Size Estimation}
\label{sec:pose}
With the primitive centers  of the optimized shape and the observed point cloud, we use Umeyama algorithm~\cite{Umeyama} to compute the SIM(3) transformation by minimizing following error function:

 \section{Experiments}

\begin{table}[t]\centering
\caption{Comparison of our method with other five SOTA methods~\cite{NOCS,SPD,SGPA,lin2022sar,deng2022icaps} on REAL275 and CAMERA25. `S' is synthetic data and `R' is real data.}

\setlength{\abovecaptionskip}{0.cm}
\setlength{\belowcaptionskip}{0.cm}
\label{tab:nocs}
\resizebox{\textwidth}{!}{
\setlength\tabcolsep{1pt}
\begin{tabular}{cccccccccccc}\toprule
\multirowcell{2}{Dataset} &\multirowcell{2}{Methods} &\multirowcell{2}{Training\\Data} &\multirowcell{2}{Canonical\\Model} &\multirowcell{2}{} &\multirowcell{2}{} &\multirowcell{2}{} &\multirowcell{2}{} &\multirowcell{2}{} &\multirowcell{2}{} &\multirowcell{2}{Learnable\\Parameters(M)} \\
& & & & & & & & & & \\
\midrule
\multirowcell{6}{REAL275\synthetic)} &NOCS~\cite{NOCS} &RGB(S+R) &Regress &83.9 &69.5 &32.3 &40.9 &48.2 &64.6 &- \\
&SGPA~\cite{SGPA} &RGBD(S+R) &Deform &93.2 &88.1 &70.7 &74.5 &\textbf{82.7} &\textbf{88.4} &23.3 \\
&SAR-Net~\cite{lin2022sar} &D(S) &Deform &86.8 &79.0 &66.7 &70.9 &75.3 &80.3 &6.3 \\
&Ours &D(S) &Generate &\textbf{95.7} &\textbf{89.3} &\textbf{71.7} &\textbf{77.0} &80.1 &86.9 &\textbf{2.3} \\
\bottomrule
\end{tabular}}
\end{table}

 
\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{0.2cm}
    \includegraphics[width=\textwidth]{pictures/qualitative_results.pdf}
    \caption{Qualitative comparisons between our method and SGPA~\cite{SGPA} on REAL275 dataset~\cite{NOCS}. The red arrows and dotted box point out the inaccurate results of SGPA compared with our method.}
    \label{fig:Qualitative}
    \vspace{-0.4cm}
\end{figure}

\subsection{Datasets and Evaluation Metrics}
We evaluate our method on NOCS~\cite{NOCS} dataset which consists of six objects categories including \textit{bottle, bowl, camera, can, laptop} and \textit{mug}. The NOCS dataset contains a virtual dataset called CAMERA25 and a real-world dataset called REAL275. The CAMERA25 dataset contains 300K synthetic images and there are 25K images for evaluation among them. The REAL275 contains 4.3K real-scene images for training and 2.75K real-scene images for evaluation.

Two aspects of metrics are adopted for evaluation: 3D IoU and pose error. 3D IoU measures the overlap of two posed 3D bounding boxes, and we compute the average precision at threshold values of  and . The pose error measures the rotation and translation errors between the predicted pose and ground truth pose, and we compute the average precision at threshold values (), (), () and (). 


\subsection{Comparison with State-of-the-Art Methods}
\label{ssec:compare_sota}
We compared our proposed method with several SOTA methods~\cite{NOCS,SPD,SGPA,lin2022sar,deng2022icaps}. NOCS~\cite{NOCS} directly regress pixel-level canonical coordinate. SPD~\cite{SPD} and SGPA~\cite{SGPA} are the methods which recover canonical shape by deforming a prior shape. iCaps~\cite{deng2022icaps} and our methods utilize a generative model to refine and recover the canonical model respectively. Table~\ref{tab:nocs} shows the quantitative results. Overall, our method needs the minimum parameters learned for the shape and pose estimation task and shows the results of the smallest domain gap between the synthetic dataset and the real-world dataset. 
In detail, 
our method outperforms other existing methods on all metrics but  on the real-world REAL275 dataset. Fig.~\ref{fig:Qualitative} shows qualitative comparisons with SGPA~\cite{SGPA}.
On the synthetic CAMERA25 dataset, our method also shows improvement on most metrics compared with other methods, except for a small gap with SGPA~\cite{SGPA} on two metrics, () and ().
Besides, Fig.~\ref{fig:curve} shows the average precision curves of each category under different thresholds on IoU, rotation and translation on REAL275 and CAMERA25 dataset.
Compared to the other generative method iCaps~\cite{deng2022icaps}, our method shows far better results, indicating the advantage of our decoupled shape and pose estimation.
It should be noted here that our trained generative model~(Sec.~\ref{sec:objectrep}) consists of about 4.3M parameters for each category. The generative models are only pre-trained on ShapeNetCore~\cite{chang2015shapenet} and need no more fine-tuning for the specific shape and pose estimation task. Thus, we compare the \textit{learnable parameters} with other methods. Specifically, our method needs only additionally train a part segmentation network~(2.3M) for all categories on the synthetic training data provided by CAMERA25~\cite{NOCS}, which leads to our better generalization in real scenes. 

\subsection{Ablation Study}
\label{sec:ablation}
\revise{
To justify the design choices of our method, we explore the following issues through experiments:
\begin{itemize}
\vspace{-0.1cm}
  \setlength\itemsep{0em}
    \item Effectiveness of the SIM(3)-invariant descriptor. Does the descriptor-based shape optimization make the optimized shape more accurate and benefit the pose estimation?
    \item Effect of different numbers of primitives. Semantic primitives are the basis of our method. How many primitives are better for pose estimation?
    \item Effect of different numbers of atomic descriptors . Our shape descriptor is concatenated by multiple atomic descriptors. How many atomic descriptors are more effective for shape optimization?
\end{itemize}
}

\begin{table}
\begin{minipage}{0.505\textwidth}


\caption{Pose estimation results with recovered shape by mean, random and optimized shape embedding on REAL275.}
\label{tab:ablation_deform}
\resizebox{\textwidth}{!}{\setlength\tabcolsep{1pt}
\begin{tabular}{ccccccccccccc}\toprule
Embedding & & & &  &  & \\
\midrule
Mean &77.7 &54.5 &31.7 &36.0 &51.4 &61.0\\
Random &76.8 &51.6 &28.7 &34.4 &47.6 &60.3\\
Optimized & \textbf{79.0} &\textbf{65.3} &\textbf{46.9} &\textbf{54.7} &\textbf{64.2} &\textbf{76.3} \\
\bottomrule
\end{tabular}} \end{minipage}
\begin{minipage}
{0.005\linewidth}\hfill\vspace{0.5cm}
\end{minipage}
\begin{minipage}{0.475\textwidth}


\caption{Results of our method with different numbers of primitives on REAL275}
\label{tab:ablation_number_keypoints}
\resizebox{\textwidth}{!}{\setlength\tabcolsep{1pt}
\begin{tabular}{ccccccc}\toprule
Number& & & &  &  & \\
\midrule
64 &\textbf{79.2} &63.7 &41.2 &51.4 &58.9 &74.9 \\
128 &78.9 &64.5 &43.5 &51.2 &61.2 &74.6 \\
256 &79.0 &\textbf{65.3} &\textbf{46.9} &\textbf{54.7} &\textbf{64.2} &\textbf{76.3} \\
512 &77.2 &63.3 &45.7 &53.6 &61.7 &74.4 \\
\bottomrule
\end{tabular}}
 \end{minipage}
\vspace{-0.2cm}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=0.495\textwidth]{supp_pictures/REAL_mAP.pdf}
    \includegraphics[width=0.495\textwidth]{supp_pictures/CAMERA_mAP.pdf}
    \caption{Curves of each category under different thresholds on IoU, rotation and translation on REAL275~(left) and CAMERA~(right) dataset.}
    \label{fig:curve}
    \vspace{-0.4cm}
\end{figure}




\textbf{Effectiveness of SIM(3)-invariant descriptors.}~
\revise{We verify the effectiveness of our SIM(3)-invariant descriptors in two aspects. First, we verify that descriptor-based shape optimization improves the accuracy of shape reconstruction. We compare the reconstruction error of our optimized shape with the shape when the shape embedding ~(denoted as ``Mean" shape) and a random shape when the shape embedding ~(denoted as "Random" shape).
Table~\ref{ablation:recon} shows that our optimized shapes are more accurate.  In Table~\ref{ablation:recon}, we also compare our method with SGPA~\cite{SGPA}, even without shape loss to supervise our method, our coarse-level shape reconstruction shows comparable results with SGPA. Second, we verify that descriptor-based shape optimization improves the pose estimation results. We compare the results of the pose estimation under the three canonical shapes~(mean, random and optimized) in the Table~\ref{tab:ablation_deform}. The results show that we get far better pose estimation results based on the optimized shape. Besides, Fig.~\ref{fig:deform_process} 
visualizes cases where shapes become more consistent with observations as optimization progresses.}

\textbf{Effect of different numbers of primitives.}~
\revise{
We conduct an ablation study on the number of semantic primitives. We experiment the number with 64, 128, 256, and 512. As shown in Table~\ref{tab:ablation_number_keypoints}, the results show that the accuracy of pose estimation is improved with the increase of the number of primitives, which indicates that more primitives can represent more complex geometry. The results show a decline when the number is set to 512 because 512 primitives are relatively redundant for our 1024 input points, and more primitive classes also decline the accuracy of part segmentation network~(Sec.~\ref{sec:segmentation}). Based on these observations, we use 256 primitives as the default setting in other experiments.}

\begin{table}
\begin{minipage}{0.55\textwidth}


\caption{\revise{Comparison of shape reconstruction accuracy of our methods and SGPA in CD metric () on REAL275. Mean and Random refer to the shape when shape embedding  and .}}
\resizebox{\textwidth}{!}{\setlength\tabcolsep{3pt}
\begin{tabular}{cccccccc}
\toprule
Methods&bottle &bowl &camera &can &laptop &mug &all \\
\midrule
SGPA~\cite{SGPA} &2.93 &\textbf{0.89} &\textbf{5.51} &1.75 &\textbf{1.62} &\textbf{1.12} &\textbf{2.44} \\
Mean &9.55 &4.39 &12.4 &3.03 &6.59 &2.26 &6.05 \\
Random &11.6 &5.49 &16.1 &3.74 &7.12 &3.63 &7.56 \\
Optimized &\textbf{2.05} &1.55 &10.1 &\textbf{1.63} &2.12 &2.93 &3.46 \\
\bottomrule
\end{tabular}}
\label{ablation:recon}
 \end{minipage}
\begin{minipage}
{0.01\linewidth}\hfill\vspace{0.5cm}
\end{minipage}
\begin{minipage}{0.435\textwidth}
\centering
\caption{\revise{Comparison of shape reconstruction accuracy when different number of atomic primitives are randomly chosen to optimize the shape in CD () metric.}}
\resizebox{0.93\textwidth}{!}{\setlength\tabcolsep{6.5pt}
\begin{tabular}{ccc}
\toprule
 &w/o. RANSAC &w. RANSAC \\
\midrule
 &6.79~~6.50 &4.38~~5.41 \\
 &4.86~~5.38 &3.97~~5.15 \\
 &3.68~~4.34 &3.59~~4.54 \\
 &3.46~~4.05 &3.44~~4.13 \\
\bottomrule
\end{tabular}}
\label{tab:atomic}
 \end{minipage}
\vspace{-0.2cm}
\end{table}

\begin{figure}[!t]
    \centering
    \setlength{\abovecaptionskip}{0.cm}
    \includegraphics[width=0.95\textwidth]{pictures/deform_main.pdf}
    \caption{Visualization of the shape optimization process. As optimization progresses, our optimized shapes are more consist with observations.}
    \label{fig:deform_process}
    \vspace{-0.4cm}
\end{figure}

\textbf{Effect of different numbers of atomic descriptors.}~
\revise{
As illustrated in Sec.~\ref{sec:srt_feature}, a single atomic descriptor is insufficient to describe an object's shape, and concatenating all atomic descriptors to form a shape descriptor would be computationally expensive and unnecessary. Thus, we conduct experiments and analyze the influence of the number of atomic descriptors on the shape optimization results. Specifically, we randomly choose  atomic descriptors to form the shape descriptor and use the chamfer distance to evaluate the quality of the optimized shape.
As shown in Table.~\ref{tab:atomic}, the reconstruction error of the optimized shape decreases as the number of atomic descriptors increases, and applying RANSAC in the random selection can further reduce this error. In the main experiments, we choose  atomic descriptors depending on the computational resources and do not use RANSAC for efficiency.}
 \section{Conclusion and Limitations}
We propose a novel category-level shape and pose estimation framework by utilizing the semantic primitive representation. Leveraging a novel SIM(3)-invariant descriptor, we decouple shape and pose estimation and optimize the implicitly generated shape in an online iterative way. Experiments results demonstrate the advantages
of our method on the real-scene data compared with other SOTA methods. 

\textbf{Limitations.}~As other methods~\cite{SPD,SGPA,lin2022sar}, our method also relies on the pre-processed masks of object instances by Mask R-CNN and \revise{extra annotation of symmetry axes when training. Furthermore, although we can reconstruct the object mesh (Sec.~\ref{supp:sec:mesh} in the supp. material), this by-product has no influence or contribution to our work. Future work could consider the mesh reconstruction or implicit texture reconstruction~\cite{irshad2022shapo} to get better shape and pose estimation results} Besides, our method does not consider the occlusion and collision between objects, and may cause ambiguity from single-view observation. Feasible future work could fuse color information into our input, or expand the proposed method to multi-view observation to address these problems. 





\clearpage
\acknowledgments{We would like to thank Bangbang Yang for the useful discussion on this paper.}



\bibliography{example.bib}  \newpage
\renewcommand\thesection{\Alph{section}}
\setcounter{section}{0}
\section{Implementation Details}
\label{supp:sec:details}
\subsection{Training Details}
The object point cloud is back-projected from the depth image with the Mask-RCNN segmentation results provided by~\cite{SPD}. We randomly sample  points from the object point cloud as our input. 
The network architecture and training protocol of our generative model  keep the same as~\cite{DualSDF}. We train a single generative model for each category with the CAD models provided by ShapeNetCore~\cite{chang2015shapenet}. 
Our part segmentation network architecture follows the part segmentation version of 3D-GCN~\cite{3D-GCN}, and we set  primitives in our main experiments. The segmentation network is trained on CAMERA dataset~(about 600K instances) with a batch size of 32 on a single NVIDIA Tesla V100 graphics card. Moreover, we set the initial learning rate as  and multiply it by a factor 0.2 every 10 epochs. 
We use the ground truth pose provided by~\cite{6-pack} and apply the pose on generated primitives to annotate the ground truth of part segmentation.
\subsection{Training Data Augmentation}
To obtain the object point cloud which is the input of our method from given images, we utilize Mask~R-CNN~\cite{he2017mask} to segment the object masks from images. 
Because the results provided by Mask~R-CNN are often imperfect, the object point cloud back-projected from the masked depth image would contain background points.
To filter out these outlier points, we train our part segmentation network~(Sec.~3.2) by making augmentation on the ground truth segmentation masks. 
As shown in Fig.~\ref{fig:data_aug}, we adopt the following two strategies~\cite{lin2022sar} on the ground truth mask :~(a)~random 0 to 5 dilations on the 2D mask. (b)~random 0 to 15-pixel crop on the mask's bounding box. 

\begin{figure}[!hp]
    \centering
    \includegraphics[width=0.8\textwidth]{supp_pictures/data_aug.pdf}
    \caption{Augmentation on synthetic training data. (a)~Dilation on 2D mask. (b)~Random crop on mask's bounding box. The black points in the object point cloud are noise points.}
    \label{fig:data_aug}
\end{figure}

\subsection{Training a DualSDF Model}
\label{supp:sec:dualsdf}
In order to generate the semantic primitive representation for each shape, we pre-trained a generative model for each category following the network structure of DualSDF~\cite{DualSDF}.
The generative model represents an object shape in two granularities. One coarse-level branch represents the coarse structure by a certain number of simple shape primitives, and the other fine-level branch represents the fine structure of the object by SDF. 

As we mentioned in Sec.~3.1, given a shape embedding , the coarse-level branch could be expressed as , and the fine-level branch could be expressed as . A signed distance field~(SDF) refers to the closest distance from a point  to the surface of the object model.  indicates  is inside the model and  indicates outside. 
The two auto-decoders  and  are supervised by a set of pairs of 3D points  and their corresponding ground truth signed distance values . The primitive set  can be learned by minimizing the difference between predicted and ground truth signed distance values:

where  is a truncated L1 loss and  indicates the distance from  to the nearest surface of . We use sphere as primitive here, thus , where  is the sphere center and  is the sphere radius.  is expressed as:

In our implementation, we add a truncated value to the ground truth  to make these primitives evenly distributed on the object's surface. Specifically, the Eq.~\ref{loss} can be rewritten as:

where


and  is a truncated value in our experiments. 

The generated primitive examples of six categories are shown in Fig.~\ref{fig:primitives}, and we can see that fewer primitives lack geometric details but will make it easier for our part segmentation network~(Sec.~3.2) to learn. 
Fig.~\ref{fig:primitives} also shows that 512 primitives are redundant to represent a shape, which drops all metrics on pose estimation. Sec.~4.3 shows the ablation study on the different number of primitives.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{supp_pictures/primitives_vis.pdf}
    \caption{Visualization of the generated primitive representations of six categories~(\emph{bottle, bowl, camera, can, laptop} and \emph{mug}) with different numbers~(64, 128, 256 and 512) of primitives. We can see that 64 primitives~(top row) lack of geometric details and 512 primitives~(bottom row) are redundant to represent a shape.}
    \label{fig:primitives}
\end{figure} \setcounter{section}{1}
\section{By-Product: Object Mesh Reconstruction}
\label{supp:sec:mesh}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{supp_pictures/recon_vis.pdf}
    \caption{Visualization of our mesh reconstruction experiment. The mesh of each object is recovered by the optimized shape embedding  and the fine-level decoder . We transform the object model to the world coordinate by our estimated pose. The top row is the original scene and the bottom row is our object-level scene reconstruction.}
    \label{fig:recon_vis}
\end{figure}
Although the fine-level decoder  provides a fine-level structural representation of the objects, we do not exploit it in our main experiment~(shape and pose estimation). To explore the potential of the shared latent space shared by  and , we use our optimized shape embedding  to generate a mesh model for each object through the fine-level decoder .

\begin{table}[!tp]
\centering
\caption{\revise{Comparison of shape reconstruction accuracy of our reconstructed primitives and mesh in CD metric () on REAL275.}}
\resizebox{0.6\textwidth}{!}{\setlength\tabcolsep{3pt}
\begin{tabular}{cccccccc}
\toprule
Shape&bottle &bowl &camera &can &laptop &mug &all \\
\midrule
Semantic Primitives &2.05 &1.55 &10.1 &1.63 &2.12 &2.93 &3.45 \\
Mesh &3.51 &3.27 &8.55 &3.55 &2.68 &2.89 &4.08 \\
\bottomrule
\end{tabular}}
\label{supp:tab:mesh}
\end{table} Specifically, for each object, we grid the canonical frame to  points and get the SDF value of each point by the optimized shape embedding  and the trained fine-level generative model . After that, we utilize the Marching Cubes algorithm~\cite{lorensen1987marching} to recover the object mesh. Table~\ref{supp:tab:mesh} shows a comparison of shape reconstruction of the primitives and mesh.
With the mesh of each object, we can reconstruct the object-level scene by transforming these meshes to the world coordinate using our estimated pose. Fig.~\ref{fig:recon_vis} shows qualitative results of our object mesh reconstruction. Without further optimization, the different reconstructed meshes keep consistent with their observations. For example, the reconstructed camera lens in the first scene~(first column) is longer, while the one in the second scene~(second column) is shorter. \setcounter{section}{2}
\section{Shape Ambiguity of Partially Observed Point Cloud}
\label{supp:sec:ambiguity}
Fig.~\ref{fig:partial} shows two cases of shape ambiguity when the objects are highly occluded, and these cases lead to some failure cases of our method on CAMERA~\cite{NOCS} dataset.
The top row shows a bottle with the top half occluded. Although our estimated shape is consistent with the partially observed point cloud, it still leads to translation errors when evaluating~(about  translation error in this case). The bottom row shows a laptop with its keyboard occluded. In this case, the screen of our estimated laptop is consistent with the observation, but there is still a rotation error when evaluating~(about  rotation error in this case). 
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{supp_pictures/partial.pdf}
    \caption{Shape ambiguity cases. When an object is occluded causing shape ambiguity, although our estimated shape and GT shape are both consistent with the observation, this still causes relatively large translation errors~(top row) and rotation errors~(bottom row) in the evaluation.}
    \label{fig:partial}
\end{figure} \setcounter{section}{3}
\section{More Results on NOCS-REAL275}
\label{supp:nocs_qualitative}
Fig.~\ref{fig:more_qualitative} shows more qualitative results of our method compared with SGPA~\cite{SGPA}. Compared with our method, the results of SGPA are relatively poor in some cases of laptops and cameras. \setcounter{section}{4}
\section{More Visualization of Shape Optimization}
\label{supp:deform_vis}
Fig.~\ref{fig:more_deform} gives more visualization of the process of our shape optimization. As the optimization proceeds, the structure of the object is optimized to be consistent with the observation. For example, the lens of the camera is gradually stretched, the angle between the screen and keyboard of the laptop gradually increases, and the handle of the mug becomes rectangular. Throughout our framework, shape optimization accounts for the bulk of the time, taking about 0.005s per iteration.
 \setcounter{section}{5}
\section{Real World Experiments}
\label{supp:real}
Applicability to unseen scenarios and settings is critical for robotic tasks. To verify the effectiveness of our model in real scenarios other than the dataset provided by NOCS~\cite{NOCS}, we test our method on several different real-world scenarios. These scenarios contain different kinds of household objects of different shapes, and the photos of these scenarios are taken from the view of a robotic arm. We use Detic~\cite{detic} to pre-process the masks of each object and treat the cups with handles as mugs and those without handles as cans.
Fig.~\ref{fig:real_world} shows visualization results, and tight orientated bounding boxes mark out objects. The red, green and blue axes represent the x, y, z axes of the canonical coordinate. 

Besides, we conduct robotic grasping experiments to demonstrate the application of our category-level pose estimation. Specifically, we first estimate pose of each object from the view of a robotic arm. And then, we use the MoveIt!\cite{chitta2012moveit} to plan a feasible path for the robot arm to grasp the top part of objects. Refer to our video for details.

 \setcounter{section}{6}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{supp_pictures/more_qua.pdf}
    \caption{More qualitative comparisons between our method and SGPA~\cite{SGPA} on REAL275 dataset. The red dotted boxes indicate the results of SGPA with worse accuracy than ours.}
    \label{fig:more_qualitative}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{supp_pictures/more_deform.pdf}
    \caption{More visualization of our shape optimization process. The objects in the figure are optimized by 20 iterations~(from left to right). As the optimization proceeds, the shape of the objects changes significantly. For example, the lens of the camera is gradually stretched, the angle between the screen and keyboard of the laptop gradually increases, and the handle of the mug becomes rectangular.}
    \label{fig:more_deform}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{supp_pictures/real_results.pdf}
    \caption{Results of real scenes. These scenarios contain different kinds of household objects of different shapes. The objects are marked out by tight orientated bounding boxes. The red, green and blue axes represent the x, y, and z axes of the canonical coordinate, respectively.}
    \label{fig:real_world}
\end{figure}
 
\end{document}
