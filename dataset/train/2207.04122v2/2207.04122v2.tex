\section{Experiments}\label{sec:experiment}

We evaluate the performance of \system on real-world datasets of entity matching (EM) and data cleaning.
Specifically, for EM, we evaluate the performance of both the blocking and matching stages. 
For data cleaning, we combine the error detection and correction stages and evaluate the quality of the final corrections.
We also conduct a case study of column semantic type discovery on the VizNet dataset. 
\revicde{Due to space limitation, we provide more results on running time, parameter sensitivity, data profiling, and error analysis in the appendix of the technical report ~\cite{DBLP:journals/corr/abs-2207-04122}.}

\subsection{Experiment settings}



For EM, we use 5 datasets provided by \dm~\cite{DBLP:conf/sigmod/MudgalLRDPKDAR18} which are widely used in previous studies.
These datasets are for training and evaluating EM models for various domains including products, publications, and businesses.
Each dataset consists of two entity tables A and B to be matched. Blocking methods take these 2 tables as input and
generate candidates of matched pairs.
For matching, each dataset provides sets of labeled entity pairs where each pair has a binary label
indicating whether it is a match / non-match.
The goal is to decide whether each pair represents the same real-world object, e.g. publication or product.
The original datasets are split into training, validation, and test sets 
at a 3:1:1 ratio.
Since \system targets application scenarios with insufficient label examples,
we consider the settings of \emph{semi-supervised} and \emph{unsupervised} EM
where \system only uses 500 or 0 labels from the training and validation set.
Table \ref{tab:em_dataset} shows the statistics of the datasets.



\setlength{\tabcolsep}{2.5pt}
\begin{table}[!t]
\caption{Statistics of EM datasets. }\label{tab:em_dataset}
\small
\begin{tabular}{cccccc} \toprule
Datasets            & TableA & TableB & Train+Valid & Test & \%pos   \\ \midrule
Abt-Buy (AB)        & 1,081   & 1,092   & 7,659        & 1,916 & 10.7\% \\
Amazon-Google (AG)  & 1,363   & 3,226   & 9,167        & 2,293 & 10.2\% \\
DBLP-ACM (DA)       & 2,616   & 2,294   & 9,890        & 2,473 & 18.0\% \\
DBLP-Scholar (DS)   & 2,616   & 64,263  & 22,965       & 5,742 & 18.6\% \\
Walmart-Amazon (WA) & 2,554   & 22,074  & 8,193        & 2,049 & 9.4\% \\ \bottomrule
\end{tabular}
\vspace{-3mm}
\end{table}

For data cleaning, we use 4 benchmark datasets provided by previous studies~\cite{DBLP:journals/pvldb/MahdaviA20,DBLP:conf/sigmod/MahdaviAFMOS019}.
Each dataset consists of a dirty spreadsheet and the goal is to generate a correction for each erroneous cells
(Error Correction, EC). A related task is to identify cells that contain errors, called Error Detection (ED).
Following the settings of \baran,
\system uses the same external EC modules for generating candidate corrections for each clean/dirty cell.
\system then utilizes the methods introduced in Section~\ref{sec:cleaning} to obtain the pairwise model for matching cells with their candidate corrections.
The details of datasets are shown in Table~\ref{tab:cleaning_dataset}.


\setlength{\tabcolsep}{2pt}
\begin{table}[!ht]
\caption{Statistics of data cleaning datasets. The error types are missing value (MV), typo (T), formatting issue (FI),
and violated attribute dependency (VAD)~\cite{DBLP:journals/pvldb/MahdaviA20}. 
\rev{*Note that the tax dataset is sampled from the original dataset of 200k rows.}}\label{tab:cleaning_dataset}
\small
\begin{tabular}{cccccc} \toprule
Datasets & size & \%error & Error Types    &  Coverage & \#candidates \\ \midrule
beers    & 2,410  11        & 16\%    & MV, FI, VAD    & 94.9\%       & 63.4         \\
hospital & 1,000  20        & 3\%     & T, VAD         & 89.5\%       & 68.3         \\
rayyan   & 1,000  11        & 9\%     & MV, T, FI, VAD & 51.4\%       & 215.6         \\
tax*      & 5,000  15        & 4\%     & T, FI, VAD     & 92.7\%       & 1,442.3        \\ \bottomrule
\end{tabular}\vspace{-3mm}
\end{table}


\subsubsection{Baseline methods}





We compare \system with 
SOTA EM methods \dm and \ditto and also 3 specialized methods for semi-supervised
or unsupervised EM:

\smallskip
\noindent
\textbf{\dm}~\cite{DBLP:conf/sigmod/MudgalLRDPKDAR18} is a hybrid deep learning method based on
Recurrent Neural Network (RNN) and the attention mechanism. We report the scores from the original paper.

\smallskip
\noindent
\textbf{\ditto}~\cite{ditto2021} is the SOTA matching solution based on pre-trained LMs.
\ditto has a series of optimizations including data augmentation (DA).
We choose RoBERTa~\cite{DBLP:journals/corr/abs-1907-11692} as the base LM for \ditto which was shown to achieve the best performance.











\smallskip
\noindent
\textbf{\rot}~\cite{DBLP:conf/sigmod/Miao0021} is a semi-supervised approach that leverages
meta-learning to combine augmentations from multiple DA operators. \rot was shown to achieve the
SOTA results on EM in the low-resourced setting. 

\smallskip
\noindent
\textbf{\zeroer}~\cite{DBLP:conf/sigmod/WuCSCT20} is an unsupervised EM solution based on
Gaussian Mixture Models (GMM) for learning match/unmatch distributions.

\smallskip
\noindent
\textbf{\autofj}~\cite{DBLP:conf/sigmod/LiCCHC21} is an unsupervised approach that
automatically builds fuzzy join programs. This method relies on the assumption that
either Table A or B is a reference table with no or few duplicates.

For the blocking stage of EM, we compare \system with \textbf{\dbl}~\cite{DBLP:journals/pvldb/ThirumuruganathanLTOGPFD21}, 
a recently proposed deep learning framework achieving state-of-the-art (SOTA) results on blocking. \dbl follows \dm and leverages
a variety of deep learning techniques including self-supervised learning.


For data cleaning, we compare \system with the EC method
\textbf{\baran}~\cite{DBLP:journals/pvldb/MahdaviA20}. 
\baran leverages active learning to learn an ensemble model of multiple external EC methods.
The original setting of Baran assumes a perfect ED output or uses Raha~\cite{DBLP:conf/sigmod/MahdaviAFMOS019}
as the separate ED stage. 


\rev{
For column matching, we consider two
SOTA baseline methods 
\textbf{\sherlock}~\cite{DBLP:conf/kdd/HulsebosHBZSKDH19} and 
\textbf{\sato}~\cite{DBLP:journals/pvldb/ZhangSLHDT20}.
Both methods use ML/DL such as word2vec and LDA to represent
columns as dense vectors.}


For all the above baseline methods, we use the source code from the original code repositories to produce the results.










\subsubsection{Environment and hyper-parameters}

We implemented \system using PyTorch~\cite{DBLP:conf/nips/PaszkeGMLBCKLGA19} and Huggingface~\cite{DBLP:conf/emnlp/WolfDSCDMCRLFDS20}.
Unless otherwise specified, 
we use the RoBERTa-base model~\cite{DBLP:journals/corr/abs-1907-11692} as the pre-trained LM and AdamW as the optimizer for all the experiments.
We fix the projector dimension to 768 and the pre-training related 
hyper-parameters  to (3.9e-3, 0.07).
We pre-trained each model using 3 epochs and fine-tuned it for 50 epochs. We fix the size of the pre-training corpus to 10,000
by up or down-sampling the set of all data items.
We set the batch size to 64 and the learning rate to 5e-5.
The matching and data cleaning tasks use the F1 score as the main evaluation metric. 
\rev{
We list the hyper-parameters related to \system's optimizations in Table \ref{tab:hyperparameters}.
}
For each run of the experiment, we select the epoch with the highest F1 on the validation set and report results on the test set.
All experiments are run on a server machine with a configuration similar to 
a p4d.24xlarge AWS EC2 machine with 8 A100 GPUs.

\begin{table}[!ht]
\centering
\scriptsize
\caption{Hyper-parameters of \system and their choices.
We underlined the best combination found via grid search.}
\label{tab:hyperparameters}
\begin{tabular}{ccc}
\toprule
Hyper-Parameter & Meaning                                         & Range  \\ \midrule
cutoff\_ratio   & \%tokens to apply the cutoff DA                 & {[}0.01, 0.03, \underline{0.05}, 0.08{]}                                                                            \\
num\_clusters   & Number of clusters for neg. sampling        & {[}30, 60, \underline{90}, 120{]}                                                                                \\
alpha\_bt       & Weight of the redundancy regularizer            & \{0.1, 0.01, \underline{1e-3}, 1e-4\}
\\
multiplier      & Size of the training set w. pseudo labels & {[}2, 4, 6, \underline{8}, 10{]} \\ \bottomrule                                                
\end{tabular}\vspace{-2mm}
\end{table}


\subsection{Main results for Entity Matching}






\iffalse

\subsubsection{Fully-supervised matching}
\begin{table}[ht]
\vspace{-3mm}
	\small
	\caption{F1 scores for fully-supervised EM.}\label{tab:super}
	\resizebox{0.48\textwidth}{!}{
	\begin{tabular}{ccccc}
		\toprule
		 & \dm & \ditto & \system (w/o \textsf{RR})& \system  \\
		\midrule
Abt-Buy            & 62.8 & 89.8 & 98.3 \green{(+8.5)}  & 98.3 \green{(+8.5)}  \\
Amazon-Google      & 70.7 & 75.6 & 92.9 \green{(+17.3)} & 94.5 \green{(+18.9)} \\
Beer               & 78.8 & 94.4 & 100.0 \green{(+5.6)} & 96.6 \green{(+2.2)} \\
DBLP-ACM           & 98.5 & 99.0   & 100.0 \green{(+1.0)} & 100.0 \green{(+1.0)} \\
DBLP-Scholar & 94.7 & 95.6 & 98.9 \green{(+3.3)}  & 98.9 \green{(+3.3)}  \\
Fodors-Zagats        & 100.0  & 100.0  & 100.0 \green{(+0.0)}  & 100.0 \green{(+0.0)}  \\
iTunes-Amazon      & 91.2 & 97.1 & 98.2 \green{(+1.1)}  & 98.1 \green{(+1.0)}  \\
Walmart-Amazon     & 73.6 & 86.8 & 95.7 \green{(+8.9)}  & 93.3 \green{(+6.5)} \\
		\bottomrule
	\end{tabular}}
\vspace{-2mm}
\end{table}

For this experiment, we set the learning rate of \system to 3e-5 and the number of fine-tuning epochs to 40.
Since all training examples are used, there is no need to apply the pseudo-labeling step.
We apply each DA operator listed in Table \ref{tab:defaultda} to find the optimal choice for each dataset. 
In addition to the full-fledged \system, we also report the results of \system (w/o \textsf{RR})
which removes the Redundancy Regularization optimization in Section~\ref{sec:barlow_twins} from \system.
Note that one potential optimization for \system is to
apply its blocking stage to further reduce the size of the train/test
sets which will naturally increase the F1 score. 
We exclude this optimization to make the comparison more fair
(same in semi-supervised and unsupervised training).

We summarize the results in Table \ref{tab:super}.
\system outperforms the previous SOTA matching solution \ditto by up to 18.9\% F1 score
and 5.2\% on average. In 3 out of the 8 tasks, \system (w/o \textsf{RR}) achieves the perfect 100\% F1 score.
The performance gain of \system is mainly due to its capability of learning high-quality entity representations
and the similarity-aware fine-tuning steps. 
We also find that the Redundancy Regularization optimization is useful in situations where the task is 
relatively more difficult, such as on the Amazon-Google dataset.

\fi






\setlength{\tabcolsep}{3.5pt}
\begin{table}[t]
\centering
	\footnotesize
	\caption{\small F1 scores for semi-supervised matching (EM). 
\system uses 500 uniformly sampled pairs from train+valid.}\label{tab:semis}
\scriptsize
\begin{tabular}{ccccccc}
		\toprule
		 & AB & AG & DA & DS & WA & average  \\
		\midrule
\dm(full) & 62.8 & 69.3 & 98.4 & 94.7 & 67.6 & 78.6 \\
\ditto(500)      & 70.1 & 44.7 & 95.9 & 89.4 & 49.4 & 69.9 \\
\ditto(750)      & 79.4 & 56.3 & 96.3 & 90.4 & 65.7 & 77.6 \\
\rot(500)      & 69.7 & 54.0 & 95.9 & 91.9 & 50.1 & 72.3 \\
\rot(750)      & 80.1 & 57.8 & 95.7 & 90.3 & 68.5 & 78.5 \\ 

SimCLR & 65.7 & 43.7 & 96.0 & 89.0 & 41.1 & 67.1 \\
\system(-cut,-RR,-cls)      & 75.8 & 56.8 & 95.0 & 90.6 & 65.6 & 76.7 \\
\system(-cut,-RR)            & 81.0 & 58.6 & 95.1 & 91.1 & 62.8 & 77.7 \\
\system(-cut)                 & 81.7 & 58.8 & 95.3 & 91.3 & 62.8 & 78.0 \\
\system(-PL)                  & 73.9 & 41.8 & 96.0 & 90.2 & 40.6 & 68.5 \\
\system(-RR)                  & 79.6 & 59.1 & 95.3 & 91.1 & 64.3 & 77.9 \\
\system(-cls)                 & 74.8 & 55.0 & 95.0 & 90.6 & 65.5 & 76.2 \\
\midrule
\system                        & 81.1 & 59.3 & 95.2 & 89.9 & 66.1 & 78.3 \\

 (vs. \rot, 500) & \green{(+11.4)} & \green{(+5.3)} & \red{(-0.7)} & \red{(-2.0)} & \green{(+16.0)} & \green{(+6.0)} \\
 (vs. SimCLR) & \green{(+15.4)} & \green{(+15.6)} & \red{(-0.8)} & \green{(+0.9)} & \green{(+25.0)} & \green{(+11.2)} \\
\bottomrule
	\end{tabular}\end{table}




For semi-supervised learning, we set the label budget to 500 for \system.
We use the same 500 labels for validation for further label saving.
For the baselines \ditto and \rot, we allow for 250 more training instances (in total 750) 
and compare the results with \system with 500 labels to show the label efficiency of \system.
We fix the DA operator to be token\_del with span cutoff.
For pseudo-labeling, we tuned the positive/negative thresholds 
 and found that adding 7x extra 
labels (i.e., multiplier=8)
works the best with the clustering optimization.
We fix the number of fine-tuning steps unchanged when adding the extra labels.




We also conduct an ablation analysis to show the effectiveness of each optimization.
\rev{We remove each of the following optimizations at a time
to test their overall effectiveness:
\begin{itemize}
\item \textsf{PL}: the pseudo labeling technique (Section~\ref{sec:pseudo});
\item \textsf{Cls}: the clustering-based negative sampling (Section~\ref{sec:clustering});
\item \textsf{Cut}: the cutoff operator for DA (Section~\ref{sec:cutoff});
\item \textsf{RR}: the redundancy regularization technique (Section~\ref{sec:barlow_twins}).
\end{itemize}
We denote each variant by ``\system (-X, -Y, \dots)''
indicating that optimizations X, Y, etc. are turned off.
Note that SimCLR~\cite{DBLP:conf/icml/ChenK0H20} 
is equivalent to \system with all 4 optimizations turned off.}


Table \ref{tab:semis} summarizes the results under the setting of semi-supervised learning.
\rev{When using 500 labels only,
\system achieves the overall best results and has a performance gain over \rot (500) by up to 16\%.
While \rot is already a label-efficient solution, 
\system can achieve comparable F1 scores (78.3 vs. 78.5)
by using 1/3 fewer labels (vs. \rot (750)). 
This F1 score is also close to \dm (full) 
using 23x more labels.
}

Pseudo-labeling (PL) is the most effective optimization
among the 4 tested options as removing it causes near 10\%
performance degradation. The 3 other optimizations,
Cls, Cut and, RR, are especially effective on the 
more challenging WA dataset. All optimizations combined
achieved up to 25\% F1 improvement on the WA dataset
or 11.2\% on average.




\smallskip
\noindent
\textbf{Unsupervised EM. }
Table~\ref{tab:uns} shows results of unsupervised EM.
Note that for pseudo labeling, \system requires prior knowledge of the positive label ratio
which is available as a dataset statistics. \system requires no more supervision beyond it.
\rev{
The results for \zeroer and \autofj are 
numbers either taken from the original papers or obtained by running the open-sourced code, 
whichever is higher.}


\rev{Compared to the two SOTA methods, \system outperforms \zeroer by up to 22.8\% (or 7.7\% on average)
and \autofj 
by up to 34.8\% (or 8.9\% on average) 
F1 score respectively.}
The average performance of the unsupervised \system is only 4\% lower than that with 500 labels.
\system's optimizations are again beneficial as they improve the base performance by 0.9\% on average. 
The main source of the performance gain is again from the high-quality pseudo labels
which are only slightly worse (6\%) than the semi-supervised setting. These results further illustrate the advantage of \system 
in the low-resource scenarios.






\setlength{\tabcolsep}{5pt}
\begin{table}[t]
    
\small
	\centering
	\caption{\small F1 scores for unsupervised matching (EM). 
}\label{tab:uns}
	\footnotesize
	\begin{tabular}{ccccccc}
		\toprule
		& AB & AG & DA & DS & WA & average  \\ \midrule
		\zeroer &  52.0 & 48.0 & 96.0  & 86.0  & 51.0 & 66.6 \\
		\autofj & 61.3 & 24.3 & 91.6 & 88.5 & 61.3 & 65.4 \\
		\midrule
		\revicde{Sudowoodo (-cut,-RR,-cls)} &  75.2 & 53.5  & 90.2 & 85.0 & 63.3 & 73.4 \\
		\system & 74.8 & 59.1 & 91.6 & 87.1 & 61.2 & 
		\textbf{74.3} \\
\bottomrule
	\end{tabular}
	\vspace{-4mm}
\end{table}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\textwidth]{blocking.pdf}
    \caption{\small Recall-CSSR (candidate set size ratio) curves of \system vs. \dbl. 
    Compared to \dbl, \system produces candidates of better quality as it has both higher recall and lower CSSR (bottom right).
    The numbers for \dbl are from Figure 6 of \cite{DBLP:journals/pvldb/ThirumuruganathanLTOGPFD21}.}
    \label{fig:blocking}
	\vspace{-2mm}
\end{figure*}
\smallskip
\noindent
\textbf{EM Blocking. } 
For blocking, \system generates candidate pairs by
applying kNN search over 
the learned vector representations of
the right entity table (Table B)
for  to 20. We find \system achieves
the best results using the
DistilBERT~\cite{DBLP:journals/corr/abs-1910-01108} LM
and a projector size of 4,096. We pre-train \system for 5 epochs
and use the last model checkpoint to generate candidates for evaluation.

We compare the quality of the candidate sets with the SOTA learning-based blocking method \dbl.
Following \cite{DBLP:journals/pvldb/ThirumuruganathanLTOGPFD21},
we report the recall scores and the candidate set size ratio (CSSR) which equals the candidate set size
divided by the total number of candidates (i.e., TableA  TableB).
For each dataset, we compute the recall score as the fraction
of positive pairs from the training, validation, and test sets 
that are retained by the candidate set.

Figure \ref{fig:blocking} shows the Recall-CSSR curves and
Table \ref{tab:block} shows the recalls and candidate set sizes of \dbl and \system. Here \dbl's numbers 
are taken from \cite{DBLP:journals/pvldb/ThirumuruganathanLTOGPFD21}.
In Table \ref{tab:block}, 
we report \system's (recall, \#cand) for the first  where 
\system's recall is higher than \dbl.
The results show that \system can consistently achieve the same recall level of \dbl while
having a significantly smaller candidate set (up to 84.8\% smaller, AB).
By having a smaller candidate set, \system reduces the difficulty of the matching stage 
which can lead to further 
matching performance improvement or 
saving of human efforts.
These results confirm \system's versatility in
benefiting multiple sub-tasks of the EM application.

\revicde{We provide ablation study, running time, data profiling and error analysis in Appedix A, B and E respectively in our technical report version \cite{DBLP:journals/corr/abs-2207-04122} }

\setlength{\tabcolsep}{2.3pt}
\begin{table}[ht]
\small
	\caption{\system for blocking. We report the recall score (R) and the number of candidates pairs (\#cand).}\label{tab:block}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{c|cccccccccc}\toprule
          & \multicolumn{2}{c}{AB} & \multicolumn{2}{c}{AG} & \multicolumn{2}{c}{DA} & \multicolumn{2}{c}{DS} & \multicolumn{2}{c}{WA} \\
          & R         & \#cand     & R         & \#cand     & R         & \#cand     & R         & \#cand     & R         & \#cand     \\ \midrule
DL-Block  & 87.2      & 21,600      & 97.1      & 68,200      & 99.6      & 13,100      & 98.1      & 392,400     & 92.2      & 51,100     \\ 
\system & \textbf{88.6}      & \textbf{3,276}       & \textbf{97.3}      & \textbf{48,390}      & 99.6      & \textbf{11,470}      & \textbf{98.4}      & \textbf{257,052}     & \textbf{95.0}      & \textbf{44,148}      \\ \bottomrule
\end{tabular}}
\vspace{-6mm}
\end{table}












\subsection{Results for data cleaning}\label{subsec-dcexp}

We evaluate \system on data cleaning
comparing it with the SOTA EC solution \baran~\cite{DBLP:journals/pvldb/MahdaviA20}.
We run \system with all optimizations turned on except pseudo labeling.
We use the span\_shuffle DA operator with span cutoff for pre-training.
We did not use blocking since the candidate correction sets are small (see Table \ref{tab:cleaning_dataset}).
After pre-training, we fine-tuned the representation model on \rev{20 uniformly sampled rows (same amount of
supervision for \baran using active learning).
We evaluate the F1 score of \system on the remaining rows
following the same setting of \baran.}



Table \ref{tab:dc} summarizes the results.
\baran has two settings: Raha+\baran uses Raha~\cite{DBLP:conf/sigmod/MahdaviAFMOS019} as the ED model 
run before \baran. The second setting (Perfect ED + \baran) assumes an ED model that perfectly identifies all the dirty cells.
We also report a method RoBERTa-base which fine-tunes the LM without \system's pre-training step.

\system achieves the best overall results even outperforming
\baran with perfect ED by 2.1\% on average.
\system outperforms Raha+\baran by a large margin and
in 3 out of 4 datasets by up to 44\% F1 score (hospital).
The results indicate that \system's strategy of formulating EC as a matching problem is beneficial.
The contrastive pre-training is also important because without this step,
RoBERTa LM has a performance degradation 
of up to 15.6\% (hospital) F1 score.
These results show the potential of integrating \system into a data cleaning pipeline in real-world scenarios.


\revicde{We provide detailed ablation results
 of \system on EC in Appendix \ref{app:dc}  of our technical report version\cite{DBLP:journals/corr/abs-2207-04122}.}

\begin{table}[ht]
\small
	\centering
	\caption{Error correction (EC) F1 scores for data cleaning. \system does not rely on a separate Error detection (ED) stage. }\label{tab:dc}
	\begin{tabular}{cccccc}
		\toprule
		& beers & hospital & rayyan & tax  & average  \\
		\midrule
Raha + \baran & 95.74 & 45.32 & 46.10 & 70.07 & 64.31 \\
		Perfect ED + \baran & 90.43 & {90.48} & 63.51 & 80.73 & 81.29 \\
		RoBERTa-base & {90.75} & 73.33 & 61.83 & 87.62 & 78.38 \\
		\system & 90.71 & 89.01 & {62.91} & {91.22} & \textbf{83.46} \\
		\bottomrule
	\end{tabular}
	\vspace{-4mm}
\end{table}


\subsection{Results for \revicde{semantic type detection}}\label{sec:coltype}

We conduct a case study where \system is applied to the column matching task on the VizNet dataset.
Following the setting in \cite{DBLP:journals/pvldb/ZhangSLHDT20}, we extracted more than 80k tables with 119k columns 
from the dataset annotated with 78 semantic types.
We apply the column matching approach described in Section \ref{sec:column}.
We use kNN blocking with  to extract pairs of nearest columns as candidate matches.
After blocking, we use the ground truth labels 
from the original dataset to label 2k uniformly sampled pairs 
for training where we labeled a pair to be ``match'' 
if and only if they have the same original semantic type.
We split this dataset into the training/validation/test sets according to the 2:1:1 ratio.


\begin{table*}[!ht]
	\caption{Column clusters discovered by \system. 
		The shown values are the first elements of 5 randomly selected columns having the same type.
		The headers (name, club, \dots) are the majority ground truth types
		from~\cite{DBLP:conf/kdd/HulsebosHBZSKDH19,DBLP:journals/pvldb/ZhangSLHDT20}.
		\system discovers fine-grained types 
		(e.g., central EU city)
		beyond the original set of 78 labels.
		We also provide our interpretations of such clusters/types (the term after the ``''). }
	\label{tab:column}
	\small
	\resizebox{0.98\textwidth}{!}{
		\begin{tabular}{c|c|c|c|c|c|c|c|c} \toprule
			name           & club & language           & \begin{tabular}{c} state  \\  US State \end{tabular} & 
			\begin{tabular}{c} result  \\  ball game result \end{tabular}   & 
			\begin{tabular}{c} name  \\  company name \end{tabular}                           & weight         & 
			\begin{tabular}{c} city  \\  central EU city \end{tabular}      & 
			\begin{tabular}{c} result  \\  baseball in-game event \end{tabular}                       \\ \midrule
			Geerhart, Kyle   & AMS  & Afrikaans          & TX    & Win      & Lone Pine Capital LLC          & 50 lbs or less & Marburg   & single, left field                       \\
			Hossfeld, Nick  & SDSM & Polski             & NV    & Win, 3-1 & T. Rowe Price Associates, Inc. & 38kg           & Stollberg & pop fly out, center field                  \\
			Dege, Henry & GAKW & Spanish            & LA    & 3-1 L    & Trigran Investments, Inc.      & 40 lbs         & Pratteln  & strikeout                    \\
			Carlisle, Brenden   & WSM  & English (built-in) & AZ    & W 9-0    & Icahn Associates Corp.         & up to 25 lbs   & Berlin    & pitcher to first base \\
			Tatlow, Jedidiah & DCM  & Turkish            & NJ    & Win      & Apple Inc.                     & 5 lbs          & Osnabruck & walk             \\ \bottomrule
	\end{tabular}}
	\vspace{-3mm}
\end{table*}

\rev{
We compare the matching quality of \system with those of
the SOTA methods, \sherlock and \sato. Table \ref{tab:coltype}
shows the detailed results. Here, we use \sherlock and \sato
to extract features of the input pairs of columns
for training pairwise matching models
of Logistic Regression (LR), SVM, Random Forest (RF), and Gradient
Boosting Tree (GBT). Among the 4 models, GBT achieves the best
F1 on the validation set. 
The results show that \system outperforms
both \sherlock and \sato with GBT classifiers 
by  4.5\% and 3.7\% respectively.
}

\rev{
Using the learned column embedding and matching models,
\system discovers over 5k column clusters (semantic types) from the table corpus.
By inspecting the results, we find that \system can discover more fine-grained types 
that are not present in the 78 ground truth types (Table \ref{tab:column}).
Note that \system achieves this result by using only 1k column pair labels instead of 
more than 80k multi-class single-column labels required by \sato or \sherlock.}

\revicde{We provide more detailed experiments in the technical report version (see Appendix \ref{app:ctd}) to compare different methods in our technical  report version\cite{DBLP:journals/corr/abs-2207-04122}.}

\setlength{\tabcolsep}{3.5pt}
\begin{table}[!t]
\small
\centering
\caption{Results for column matching.} \label{tab:coltype}
\begin{tabular}{ccccccc} \toprule
             & \multicolumn{3}{c}{Valid}  & \multicolumn{3}{c}{Test}   \\
             & Precision & Recall & F1    & Precision & Recall & F1    \\ \midrule
\sherlock & 81.49     & 82.23  & 81.86 & \textbf{85.50}     & 82.34  & 83.89 \\
\sato     & \textbf{83.69}     & 83.43  & 83.56 & 84.57     & 84.33  & 84.45 \\
\system    & {83.62}     & \textbf{89.16}  & \textbf{86.30} & 85.37     & \textbf{91.45}  & \textbf{88.31} \\ \bottomrule
\end{tabular}
\vspace{-4mm}
\end{table}

\iffalse

\subsection{Parameter sensitivity}\label{sec:detail}

\rev{
We also conduct a full parameter sensitivity
analysis exploring the search space listed in
Table \ref{tab:hyperparameters}.
The full results are in Appendix \ref{app:para}.
Recall that the 4 hyper-parameters are
the cutoff ratio, the number of clusters for
negative sampling,  for redundancy regularization,
and the multiplier for the pseudo label (PL) ratio.
In summary, the performance of \system stays
stable as we vary the hyper-parameters.
For example, by varying the number of clusters
from 30 to 120, the average F1 score changes 
in a small range between 77.7\% to 78.3\%.
\system is relatively more sensitive to
 and the PL multiplier, but the changes
are still within a small manageable range.
}

\subsection{Running time}
\rev{
We report the full running time results of
\system in Appendix \ref{app:time}.
The results show that for semi-supervised EM, 
although \system requires extra running time overhead for 
pre-training, its overall running time is still
comparable to LM-based methods like \ditto.
\system also outperforms \rot on 3 out of 5 datasets.
\system is also efficient for blocking
as it finishes the largest dataset (DS) within 305 seconds.
For data cleaning, \system is comparable with the baseline method that directly fine-tunes the RoBERTa model.}



\subsection{Data profiling and error analysis}\label{sec:casestudy}
\rev{
To better understand the performance advantage
of \system over existing methods,
we made an in-depth dataset profiling analysis for EM.
Full details of the analysis can be found
in Appendix \ref{app:profiling}.
The goal is to understand in which cases \system 
outperforms previous best methods such as \ditto.
We conduct the analysis by splitting each EM dataset
into 5 ``difficulty'' levels. We measure the difficulty
of a split by its \emph{Jaccard similarity scores}.
The basic idea is that if the positive (negative)
pairs have low (high) average Jaccard similarity, 
the task will be harder
because it is unlikely an EM solution based on
textual similarity can easily separate the positive
and negative classes.
Besides, across the 5 difficulty levels,
\system consistently outperforms \ditto.
The performance improvement is more significant
for harder sub-tasks as \system achieves
1.13 to 1.52 times higher F1 scores.
The results highlight \system's advantage of
learning semantically meaningful entity embedding.}


\rev{
Next, we inspect example predictions made by
\system in EM and data cleaning datasets.
The examples for EM (Figure \ref{fig:em-examples}) 
confirm that \system
correctly predicts textually dissimilar records
being match and textually similar records being 
non-match.
The examples for data cleaning (Figure 
\ref{fig:dc-examples}) show that 
among the noisy candidates generated from the
\baran pipeline, \system correctly avoids
all unnecessary corrections and identifies
the right correction for most of 
the erroneous cells (5/7).}

\fi