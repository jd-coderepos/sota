
\section{Experiments}
\label{sec:experiments}

\subsection{Benchmark and Implementation Details}
\label{sec:exp:bench-implem}

\para{Benchmarks.} We consider three representative benchmarks for video question answering: NExT-QA \cite{xiao2021next}, VALUE-How2QA \cite{li2021value,li2020hero}, and MSR-VTT-MC \cite{xu2016msr}. We also examine the generality of our ATP model for text-to-video retrieval on DiDeMo \cite{hendricks2018localizing}, MSR-VTT \cite{xu2016msr}, and ActivityNet \cite{krishna2017dense}. For each benchmark, we follow standard protocols outlined by prior work \cite{lei2021less,xu2021videoclip,xiao2021next,li2021value} for dataset processing, metrics, and settings; see supplement for details and analysis. We choose these benchmarks specifically to provide a broad coverage of durations, source video domains (general activities, instructional, etc.), and designs. 

\para{Implementation.} We implement our ATP model with a few-layer, low-capacity transformer \cite{vaswani2017attention} in PyTorch \cite{paszke2019pytorch}, and train all models using the Adam \cite{kingma2014adam} optimizer. Main paper results here reported on ViT-B-32 (CLIP) inputs for consistency \cite{radford2018improving,dosovitskiy2020image,li2021value}.
See supplement\footnote{Please see project website for supplementary material and code release.}
for more.

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.95\linewidth]{figures/fig5.pdf}}
\vskip -0.1in
\caption{\textbf{Oracle upper bound analysis.} As a preliminary step, we analyze the performance upper bound of ATP under oracle conditions (with respect to the downstream task). Recall and accuracy (y-axis) averaged over multiple random samples of $n$ frames (x-axis). We observe that the upper bounds are competitive with state-of-the-art video models even when choosing one embedding from relatively few-frame samples. Dashed reference lines are state-of-the-art models (\textbf{(a,b)} \cite{xu2021videoclip}, \textbf{(c)} \cite{li2021value}, \textbf{(d)} \cite{bain2021frozen}, \textbf{(e)} \cite{xiao2021next}, \textbf{(f)} \cite{lei2021less}).
}
\label{fig:exp-oracle-analysis}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=0.95\linewidth]{figures/fig6.pdf}}
\vskip -0.1in
\caption{\textbf{ATP analysis (qualitative results).} We visualize example videos from the NExT-QA dataset \cite{xiao2021next}, along with the selections ATP made from a random sparse sample of frames. Both questions shown here are examples of ``causal-how'' questions in the dataset (shown with the top-4 answer options, for clarity). \textbf{(a)} We find that our ATP model can select informative frames for the downstream Video QA task, when possible, and that many questions initially intended to assess causal or temporal understanding can be answered from single-frame semantics. \textbf{(b)} Conversely, for (video, question) inputs that necessitate a deeper \textit{multi-frame} understanding of event relationships or dynamics, ATP's selected embedding is insufficient to answer the query. See Sec.~\ref{sec:exp:atp} (additional visuals and datasets in supplement).
}
\label{fig:exp-nextqa-qual}
\end{center}
\vskip -0.3in
\end{figure*}


\subsection{Analyzing Video-Language with ATP}
\label{sec:exp:atp}

\para{Preliminary (upper bound) analysis.} As a preliminary step, we first examine the performance of ATP under oracle conditions (with respect to the downstream video-language task) to establish a kind of upper bound for ATP on the set of benchmarks. In this analysis, we sample $n$ input frames from the video, varying $n$, and encode them with a pretrained model. In this oracle setting \textit{only}, ATP then selects a frozen embedding from this set that maximizes the downstream groundtruth accuracy on the video-language task. Note that in this analysis, the oracle empowered ATP is still bottlenecked by what the image-level representation is able to capture. We repeat this analysis for multiple samples (dependent on the video lengths), and report the average. As shown in Figure~\ref{fig:exp-oracle-analysis}, we observe that upper bound accuracies are competitive with state-of-the-art video models, even with relatively small $n$ sample sizes, suggesting the promise for analyzing these datasets with a learnable ATP.

\para{ATP analysis (video QA).} We apply a learnable ATP model to analyze a suite of standard video-language benchmarks. We first center our analysis discussion on video question-answering (video QA) benchmarks, since we find these benchmarks provide strong potential for deep multi-event understanding. Per Section~\ref{sec:exp:bench-implem}, we focus on three representative benchmarks for analysis: NExT-QA \cite{xiao2021next}, VALUE-How2QA \cite{li2021value,li2020hero}, and MSR-VTT-MC \cite{xu2016msr}. 
We re-iterate that our \textit{primary} goal with ATP is one of analysis: to better characterize these \textit{instantiations} of the video-language task.
In Tables \ref{tbl:vid-qa:msrvttmc}, \ref{tbl:vid-qa:how2qa-value}, and \ref{tbl:vid-qa:next-qa}, we report results for each benchmark.

\begin{table}[t]
\centering
\scalebox{0.83}{
\begin{tabular}{l c}
\textit{MSR-VTT-MC} & Accuracy  \\
\hline
ActBERT\cite{zhu2020actbert} & 85.7\\
ClipBERT\cite{lei2021less} & 88.2\\
MERLOT \cite{zellersluhessel2021merlot} & 90.9\\
VideoCLIP \cite{xu2021videoclip} & 92.1\\
\hline
    CLIP (single-frame) & 84.8 \\ 
    Ours (ATP; $1 \leftarrow 4 $)& 91.4 \\ 
    Ours (ATP; $1 \leftarrow 8 $)& \textbf{92.5} \\ 
    Ours (ATP; $1 \leftarrow 16 $)& \textbf{93.2} \\ 
\end{tabular}
}
\caption{\textbf{VideoQA on MSR-VTT-MC.} We find that our learned ATP model significantly outperforms prior work, indicating that this dataset can be largely addressed with image-level understanding. (1 $\leftarrow n$ means 1 embedding chosen from $n$ sampled.)}
\label{tbl:vid-qa:msrvttmc}
\end{table}


\begin{table}[bt]
\centering
\scalebox{0.83}{
\begin{tabular}{l c}
\textit{VALUE-How2QA} & Accuracy  \\
\hline
Random & 25.0 \\
HERO \cite{li2020hero} & 60.4\\
HERO+ \cite{li2020hero} & 60.9\\ \hline
	CLIP (single-frame) & 50.1 \\
	CLIP (mean pooling) & 55.7 \\ 
	Ours (ATP)& \textbf{65.1} \\ 
\end{tabular}
}
\caption{\textbf{VideoQA on VALUE-How2QA.} We observe strong performance over previous state-of-the-art baselines on instructional video data. HERO+ baseline here has the same preprocessing as our model, and all models leverage the same CLIP features (HERO baselines additionally leverage heavy motion features \cite{feichtenhofer2019slowfast,li2021value}).}
\label{tbl:vid-qa:how2qa-value}
\end{table}


On MSR-VTT-MC (Table~\ref{tbl:vid-qa:msrvttmc}), our learned ATP model outperforms recent state-of-the-art video-language models \cite{xu2021videoclip,zellersluhessel2021merlot,lei2021less}, when considering relatively few frames at inference and despite its substantial (single-frame) bottleneck constraints on model capacity, capability, and inputs. Critically, ATP substantially improves over standard atemporal baselines, including random single-frame and mean-pooling with CLIP \cite{radford2021learning}, offering a stronger bound.

On VALUE-How2QA (Table~\ref{tbl:vid-qa:how2qa-value}), we find that our learned ATP model offers significantly stronger accuracies than prior state-of-the-art models. Note that the HERO baselines here also use the same input CLIP embeddings, and no auxiliary text inputs, for fair comparison. One takeaway finding from our analysis of this benchmark was that counting questions, often designed to track state over the course of a video, were in fact often addressable by a single well-chosen frame that showed sufficient number of the items.

\begin{table}[bt]
\centering
\scalebox{0.83}{
    \begin{tabular}{l c c c c}
\textit{NExT-QA} & Acc  & Acc-D  & Acc-T  & Acc-C  \\
\hline
Random & 20.0 & 20.0 & 20.0 & 20.0 \\
\hline
\textsc{Main Dataset} & \multicolumn{4}{r}{(\textit{Section~\ref{sec:exp:atp}})}\\
	CLIP (single-frame) & 43.7 & 53.1 & 39.0 & 43.8 \\ 
	HGA \cite{jiang2020reasoning} & 49.7 & 59.3 & 50.7 & 46.3\\
	HGA \cite{jiang2020reasoning} + CLIP \cite{radford2021learning} & 50.4 & 59.3 & 52.1 & 46.8\\
    Ours (ATP) & 49.2  & 58.9 & 46.7 & 48.3 \\
\hline
    Ours (Temp[ATP])& \textbf{51.5} & 65.0 & 49.3 & 48.6 \\ 
    Ours (Temp[ATP] + ATP)& \textbf{54.3} & 66.8 & 50.2 & 53.1 \\ 
\hline
\hline
\textsc{ATP$_{hard}$-Subset} & \multicolumn{4}{r}{(\textit{Section~\ref{sec:exp:next-qa}})}\\
    Ours (ATP) & 20.2 & 23.9 & 22.6 & 19.6 \\ 
    Ours (Temporal[ATP])& 38.8 & 46.8 & 36.5 & 38.4 \\ 
	HGA \cite{jiang2020reasoning} & 44.1 & 51.2 & 45.3 & 43.3 \\
\end{tabular}
}
\caption{\textbf{VideoQA on NExT-QA.} We report accuracies on the overall main dataset and descriptive (D), temporal (T), and causal (C) splits. See Section~\ref{sec:exp:next-qa} for details on the ``Temp[ATP]'' and ``Temp[ATP] + ATP'' models, and details on our ATP$_{hard}$ subset.}
\vspace{-.5em}
\label{tbl:vid-qa:next-qa}
\end{table}


Finally, on NExT-QA (Table~\ref{tbl:vid-qa:next-qa}), we find that even this recent benchmark, which is explicitly designed for temporal and causal understanding, can have a non-trivial subset of questions answerable by simple single-frame event recognition. In Figure~\ref{fig:exp-nextqa-qual}, we show two different ``causal-how'' questions, which aim to assess both causality and dynamics. In the case of  Figure~\ref{fig:exp-nextqa-qual}(a) specifically, we observe that as long as the ATP model is able to select the informative frame with a clear depiction of the child on the cycle, the answer is readily apparent without deep video-level understanding. Quantitatively, our ATP model provides a stronger bound than standard image-level baselines; we also augment the the HGA baseline with CLIP features for fair comparison.






\para{ATP analysis (retrieval).} We also apply our learnable ATP model on standard retrieval benchmarks: DiDeMo \cite{hendricks2018localizing}, MSR-VTT \cite{xu2016msr}, and ActivityNet \cite{krishna2017dense}. In Table~\ref{tbl:vid-lang:all}, we observe that our technique generalizes well to other discriminative video-language settings, establishing stronger bounds on image-centric performance and showing competitive accuracies with recent state-of-the-art methods. Our ATP model's performance on \textit{paragraph} retrieval settings, like ActivityNet, highlights an area of improvement for image-bottleneck understanding: because paragraphs describe multiple dense events in long videos, it can be difficult to use a single frame embedding to capture this description well. We provide an extended discussion of prior work comparisons, limitations, and potential future directions for text-to-video retrieval as part of our supplement.



\begin{table}[t]

\centering
\setlength\tabcolsep{0.1pt}
\scalebox{.83}{
    \begin{tabular}{l |c c | c c | c c}
  & \multicolumn{2}{|c|}{\textit{MSR-VTT}} & \multicolumn{2}{c|}{\textit{DiDeMo}} & \multicolumn{2}{c}{\textit{ActivityNet}} \\
  & ~R@1~  & ~R@5~  & ~R@1~  & ~R@5~  & ~R@1~  & ~R@5~  \\
\hline
     Support Set\cite{patrick2021supportset} & 30.1 & 58.5 & - & - & 29.2 & 61.6\\
     VideoCLIP \cite{xu2021videoclip} & 30.9 & 55.4 & 16.6$^*$ & 46.9$^*$ & - & - \\
     ClipBERT \cite{lei2021less} & 22.0 & 46.8 & 20.4 & 48.0 & 21.3 & 49.0\\
	\hline   
    CLIP (single-frame)~ & 21.6 & 44.6 & 20.2 & 42.5 & 12.5 & 30.3 \\ 
    Ours (ATP)& 27.8 & 49.8 & 26.1 & 50.5 & 17.7 & 41.8\\ 
    \end{tabular}
}
\caption{\textbf{Video-language (text-to-video) retrieval.} We show that our ATP analysis technique generalizes beyond video question answering settings. ($^*$ indicates zero-shot reported by prior work; see supplement for a more complete prior work comparisons table.)}
\label{tbl:vid-lang:all}
\end{table}

\subsection{Improving Dataset and Model Design with ATP}
\label{sec:exp:next-qa}

Finally, we consider how to leverage our ATP model and its insights to improve both dataset and model design. For this section, we choose to focus on the NExT-QA benchmark \cite{xiao2021next} as a case study, since it is a key recent effort towards improving the field's focus on causal and temporal understanding in video-language tasks.

\para{Improving dataset design with ATP.} From our initial analysis of the NExT-QA benchmark in Section~\ref{sec:exp:atp}, we found that ATP provides a surprising degree of accuracy on causal and temporal questions, despite its strong image-centric bottleneck. Because ATP provides a stronger bound on the capability of image-level understanding for these questions, it can help better disentangle questions that necessitate full video-level understanding (such questions will be largely unanswerable for the ATP model) from ones that do not. 

We accomplish this by considering an ensemble of ATP models on the dataset, and leveraging their confidences and agreement to determine a subset of ATP$_{hard}$ questions. We determine any heuristics through k-fold cross validation on the \textit{training} set. In parallel, we manually annotate a subset of the validation set for (video, question) pairs that predicate video-level understanding (see supplement for procedure details and limitations of our ATP technique). The results of our final analysis are shown in Figure~\ref{fig:exp-nextqa-disentanglement}. We find that our ATP based technique maintains the recall of the video-level understanding questions on both the causal and temporal dataset splits, while simultaneously improving upon their precision (by filtering out ``easy'' questions). 



\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figures/fig7.pdf}}
\vskip -0.1in
\caption{\textbf{Improving dataset design with ATP.} We analyze the original NExT-QA \cite{xiao2021next} benchmark, and find that our ATP models (denoted ATP$_{hard}$) are able to better disentangle input (video, question) pairs that \textit{truly} necessitate video-level understanding compared with the original dataset, on both \textbf{(a)} casual and \textbf{(b)} temporal splits. This indicates promise for leveraging ATP in-the-loop for future dataset designs. See Section~\ref{sec:exp:next-qa} for analysis details.
}
\label{fig:exp-nextqa-disentanglement}
\end{center}
\vskip -0.3in
\end{figure}

Furthermore, we can also show how this ATP$_{hard}$ subset better benchmarks progress on video-level causal and temporal understanding (in Table~\ref{tbl:vid-qa:next-qa}) that may have been otherwise obscured. While ATP nearly matches the other models on the main dataset due to the inclusion  of ``easier'' questions, this harder subset reveals a substantial gap relative to the state-of-the-art temporal reasoning model. 

Together, these results suggest ATP in-the-loop can be an effective tool during future dataset design and creation.

\para{Improving model design with ATP.} As described in Section~\ref{sec:ta:temporal-model}, we can leverage ATP to provide candidate frame embeddings for a downstream temporal model. As a first step towards improving temporal modeling (and efficiency), we introduce this model (denoted Temp[ATP] in Table~\ref{tbl:vid-qa:next-qa}) and benchmark it on the NExT-QA dataset. This model achieves a new state-of-the-art accuracy, outperforming the HGA (and HGA + CLIP) baselines on the main NExT-QA dataset, while operating at significantly reduced processing cost due to ATP (see supplement for efficiency discussion).

We make two additional observations: first, on the ATP$_{hard}$ subset, we find that this temporal model recovers much of the performance gap between ATP and the HGA model (we attribute the remaining gap to HGA's incorporation of additional motion features, which can aid in addressing some challenging dynamics questions), further verifying the potential dataset design contribution of ATP. Second, we observe that the aggregated confidence scores of the ATP ensemble provides a clear disentanglement signal on hard vs. easy problems, without access to groundtruth. 
Setting a heuristic threshold with k-fold cross validation on training, we use this signal to smartly ensemble ATP and Temp[ATP] further. For questions ATP can address, we do not need to consider additional (potentially noisy) frames, and we can skip the full temporal model. For ones ATP is less confident, the temporal model is run. This ensemble (denoted Temp[ATP] + ATP in Table~\ref{tbl:vid-qa:next-qa}) achieves a significant further accuracy and efficiency increase on NExT-QA.

%