This section presents an extensive evaluation of our approach to generating shellcodes from natural language descriptions. 
We conducted the experimental analysis to target the following experimental objectives.


\vspace{0.1cm}
\noindent
 \textit{Feasibility in applying NMT for shellcode generation.}\\
We first perform an initial assessment on the feasibility of using NMT for shellcode generation with reasonably good accuracy, by applying techniques commonly used for code generation (e.g., generating Python code from natural language). 
We evaluate a broad set of state-of-the-art models for code generation, in combination with different techniques for data processing. In this initial stage, we adopt automatic evaluation metrics.



\vspace{0.1cm}
\noindent
 \textit{Accuracy of NMT at generating assembly code snippets.}\\
In this experimental objective, we deepen the analysis of the accuracy of NMT models. 
This is a cumbersome task since automatic metrics do not catch the deeper linguistic features of generated code, such as its semantic correctness \cite{DBLP:journals/corr/abs-2105-03311}. Therefore, it is also advisable for NMT studies to perform an evaluation through manual analysis, by using additional metrics in order to have a more precise and complete evaluation. 
The second experimental objective still focuses on the analysis of individual intents and their corresponding translations into code snippets. 





\vspace{0.1cm}
\noindent
 \textit{Accuracy of the NMT at generating whole shellcodes.}\\
We investigate if it is possible to apply NMT to generate full shellcodes, i.e., entire assembly programs from a set of intents. 
Ideally, the generated code is entirely or mostly correct, in order to reduce the human effort towards developing assembly programs. 
Therefore, in this experimental objective, we evaluate how many entire shellcodes are correctly generated by NMT (unlike the previous experimental objective, where we analyze individual code snippets regardless of which program they belong to).


\vspace{0.1cm}
\noindent
 \textit{Types of errors incurred by NMT in the generation of shellcodes.}\\
In this experimental objective, we are concerned with diagnosing the error predictions in the code generation task. We qualitatively analyze a representative sample of the most frequent mistakes, including both syntactic and semantic ones, to get more insight into the severity of the errors, and to understand potential areas of improvement for future work.




\subsection{Model Implementation}
We implement the Seq2Seq model using {\fontfamily{qcr}\selectfont xnmt} \cite{neubig2018xnmt}. We use an Adam optimizer \cite{kingma2014adam} with  and , while the learning rate  is set to . We set all the remaining hyper-parameters in a basic configuration: layer dimension = , layers = , epochs (with early stopping enforced) = , beam size = . 

Our CodeBERT implementation uses an encoder-decoder framework where the encoder is initialized to the pre-trained CodeBERT weights, and the decoder is a transformer decoder. The decoder is composed of  stacked layers. The encoder follows the RoBERTa architecture~\cite{liu2019roberta}, with  attention heads, hidden layer dimension of ,  encoder layers,  for the size of position embeddings. We use the Adam optimizer~\cite{kingma2014adam}. The total number of parameters is 125M. The max length of the input is  and the max length of inference is . The learning rate , batch size = , beam size = , and train\_steps = .

We performed our experiments on a Linux machine. Seq2seq utilized  CPU cores and  GB RAM. CodeBERT utilized 8 CPU cores,  GB RAM, and  GTX1080Ti GPUs.
The computational time needed to generate the output depends on the settings of the hyper-parameters and the size of the dataset. 
On average, the training time for the Seq2Seq model was  minutes, while CodeBERT required for the training on average  minutes.
Once the models are trained, the time to translate intent into a code snippet is below 1 second and can be considered negligible. 

\subsection{Test Set}
To perform the experimental evaluation, we split our entire dataset into train/dev/test sets by using an // ratio.
To divide the data between training, dev, and test set, we did not individually sample intent-snippet pairs from the dataset, but we took groups of intent-snippet pairs that belonged to the same shellcode, in order to be able to evaluate generate shellcodes in their entirety (see \S~\ref{subsec:RQ3}). The test set contains  complete shellcodes (e.g. the entire Listing~\ref{list:shellcode2}). 


We selected the 30 shellcodes of the test set in order to maximize the heterogeneity among the programs and mitigate bias. We anticipated that these biases could affect the evaluation: the type of attack (as they may entail different instructions and constructs); the authors of the shellcode (as it may also affect the programming style); and the complexity of the shellcode (as more complex shellcodes may also be more difficult to describe and to translate). We divided the shellcodes according to the type of the attack (shell spawning, break chroot, fork bomb, etc.), and sampled the shellcodes uniformly across these classes. When sampling within each class, we double-checked that no programmer was over-represented. We used the shellcode length as a proxy for complexity, and we increased the sample size until the distribution of the shellcode length was comparable to the distribution of the whole population (min=, max=, mean=, median=).
The histograms in \figurename{}~\ref{fig:test_statistic} summarize the statistic of the programs in the test set in terms of lines of code.
Additional information on the test set is presented in the \appendixname~\ref{appendix:test_set}. 

 \begin{figure}[ht]
    \centering
    \subfloat[Number of assembly lines of code.\label{fig:program_length}]{\includegraphics[width=0.5\columnwidth]{program_histogram.pdf}}
    \subfloat[Number of multi-lines snippets.\label{fig:multiline}]{\includegraphics[width=0.5\columnwidth]{multiline_histogram.pdf}}\hfill
    \caption{Histograms visualizing the statistics of the  30 shellcodes in the test set.}
    \label{fig:test_statistic}
     
\end{figure}

 











\subsection{Feasibility in applying NMT for shellcode generation}
\label{subsec:RQ1}



We first analyze the feasibility of Seq2Seq with attention mechanism and CodeBERT for the generation of shellcodes and investigate the impact of the data processing described in \S{}~\ref{sec:approach}. In this stage, we use automatic evaluation metrics. 
Automatic metrics are commonly used in the field of machine translation. They are reproducible, easy to be tuned, and time-saving. The \emph{BiLingual Evaluation Understudy} (BLEU) \cite{papineni2002bleu} score is one of the most popular automatic metric \cite{oda2015learning,DBLP:journals/corr/LingGHKSWB16,gemmell2020relevance,tran2019does}. This metric is based on the concept of \textit{n-gram}, i.e., the adjacent sequence of  \textit{items} (e.g., syllables, letters, words, etc.) from a given example of text or speech. In particular, this metric measures the degree of n-gram overlapping between the strings of words produced by the model and the human translation references at the corpus level. BLEU measures translation quality by the accuracy of translating n-grams to n-grams, for n-gram of size  to  \cite{han2016machine}.
The \emph{Exact match accuracy} (ACC) is another automatic metric often used for evaluating neural machine translation  \cite{DBLP:journals/corr/LingGHKSWB16,yin2017syntactic,yin2018tranx,yin2019reranking}. It measures the fraction of the exact match between the output predicted by the model and the reference.

To assess the influence of our tailoring to NMT for the assembly language (e.g., the intent parser), we compare three ``variants'' of NMT by varying the steps of the data processing pipeline (see \S~\ref{sec:approach}):
\begin{itemize}
    \item \textit{w/o data processing}: the model performs the translation task without applying any step of the data processing pipeline. 
    \item \textit{w/o intent parser}: in this case, the model is trained on processed data, but without adopting the intent parser.
    \item \textit{with intent parser}: the data processing pipeline also includes the intent parser. 
\end{itemize}

Table~\ref{tab:automatic_evaluation} shows the results of this analysis. 
The table shows that the data processing aids the Seq2Seq model also without the use of the intent parser, while CodeBERT does not take benefit from the basic data processing steps. The performance of both models significantly increases when the data processing is used in combination with the intent parser. Indeed, the full data processing pipeline improves all the metrics by \% on average for Seq2Seq and by \% on average for CodeBERT when the results of the models are compared without using the data processing process. 
The table also highlights that CodeBERT outperforms the Seq2Seq model across all metrics. We conducted a \textit{paired t-test} and found that the differences between the results obtained by CodeBERT with the intent parser and all the other model configurations are statistically significant for all metrics (at ). 


\begin{table}[ht]
\footnotesize
\centering
\caption{Automated evaluation of the translation task. Bolded values are the best performance. IP: Intent Parser. ( p0.05)}
\label{tab:automatic_evaluation}
\begin{tabular}{
>{\centering\arraybackslash}m{1.5cm}| 
>{\centering\arraybackslash}m{1.25cm}
>{\centering\arraybackslash}m{1.25cm}
>{\centering\arraybackslash}m{1.25cm}|
>{\centering\arraybackslash}m{1.25cm}
>{\centering\arraybackslash}m{1.25cm}
>{\centering\arraybackslash}m{1.25cm}}
\toprule
\multirow{2}{*}{\textbf{Automated}} & 
\multicolumn{3}{c}{\textbf{Seq2Seq}} & \multicolumn{3}{c}{\textbf{CodeBERT}}\\
\textbf{Metrics (\%)} & \textit{w/o data processing} & \textit{w/o IP} & \textit{with IP} & \textit{w/o data processing} & \textit{w/o IP} & \textit{with IP} \\
\midrule
\textit{BLEU-1} & 69.99 & 74.57  & 93.46 & 78.42 & 80.11 & \textbf{94.95*}\\
\textit{BLEU-2} & 64.18 & 69.82 & 91.98 & 75.11 & 75.89  & \textbf{93.61*}\\
\textit{BLEU-3} & 60.09 & 66.35 & 90.87 & 72.75 & 73.15  & \textbf{92.68*}\\
\textit{BLEU-4} & 56.43 & 62.97 & 90.03 & 70.54 & 70.11  & \textbf{91.70*}\\
\textit{ACC} & 39.44 & 51.55 & 82.92 & 69.57 & 67.39 & \textbf{89.75*}\\ 
\bottomrule
\end{tabular}
\end{table}






\begin{comment}
\begin{table*}[ht]
\centering
\caption{State-of-the-art performances on different corpus used for automatic code generation task.}
\label{tab:dataset_comp}
\begin{tabular}{
>{\centering\arraybackslash}m{1.5cm}| 
>{\centering\arraybackslash}m{2cm}
>{\centering\arraybackslash}m{1.5cm}
>{\centering\arraybackslash}m{1.5cm}
>{\centering\arraybackslash}m{1.5cm}
>{\centering\arraybackslash}m{1.5cm}}
\toprule
\textbf{Metrics (\%)} 
&\textbf{\datasetname{}} 
& \textbf{CoNaLa} & \textbf{Django} & \textbf{Hearthstone} & \textbf{Magic the Gathering}\\
\midrule
\textit{BLEU-4} & 91.70 & 32.26 \cite{xu2020incorporating} & 82.30 \cite{gemmell2020relevance} & 74.50 \cite{gemmell2020relevance} & 61.40 \cite{ling2016latent}\\
\textit{ACC} & 89.75 & 3.00 \cite{yin2019reranking} & 62.30 \cite{ling2016latent} & 4.50 \cite{ling2016latent} & 4.80 \cite{ling2016latent}\\
\bottomrule
\end{tabular}
\end{table*}
\end{comment}




To estimate the actual goodness of the results, we compared the best performance achieved on the \datasetname{} dataset with the state-of-the-art best performances on the Django dataset \cite{oda2015learning}, a corpus widely used for code generation tasks~\cite{ling2016latent,yin2017syntactic,yin2018tranx,yin2019reranking,hayati2018retrieval,dong2018coarse,gemmell2020relevance,xu2020incorporating} and consisting of  pairs of Python statements for the Django Web application framework alongside the corresponding English pseudo-code. 
The state-of-the-art best performances on this dataset provide BLEU-4 score and accuracy equal to ~\cite{hayati2018retrieval} and ~\cite{yin2019reranking}, respectively, and are therefore lower than the best results in Table~\ref{tab:automatic_evaluation}.
We attribute these differences to the nature of the assembly language, which is a low-level language. Indeed, even if this work targets the IA-32 processor, which is a CISC architecture, the instruction set of the assembly language is still limited if compared to high-level languages, such as Python, which include a wide number of libraries and functions and, therefore, are more complex to automatically generate.


We also investigate the performance of the code generation task on single-line snippets vs. multi-line snippets by performing a fine-grained evaluation.
Table~\ref{tab:automatic_evaluation_single_vs_multi} shows the performance of CodeBERT (with data processing) for single vs. multi-line snippets. Unsurprisingly, we find that accuracy is negatively affected by the length of snippets, while BLEU scores are higher for multi-line snippets. This is because multi-line snippets are longer, there is more opportunity for BLEU scores to be higher (there can be more n-grams that are matched in longer snippets), in contrast to single line snippets. And likewise, since the accuracy metric is an exact match on the entire snippet, performance on multi-line snippets is lower than for single line snippets.


\begin{table*}[ht]
\centering
\caption{Automatic evaluation of the translation task comparing single-line and multi-line snippets from the test set. Bolded values are the best performance.}
\label{tab:automatic_evaluation_single_vs_multi}
\begin{tabular}{
>{\centering\arraybackslash}m{2.5cm}| 
>{\centering\arraybackslash}m{1.75cm}
>{\centering\arraybackslash}m{1.75cm}}
\toprule
\textbf{Automated  Metrics (\%)} & \textbf{Single-line snippets} & \textbf{Multi-line snippets}\\
\midrule
\textit{BLEU-1} & 93.64 & \textbf{98.14} \\
\textit{BLEU-2} & 92.24 & \textbf{96.86} \\
\textit{BLEU-3} & 91.29 & \textbf{95.84}\\
\textit{BLEU-4} & 90.21 & \textbf{94.91}\\
\textit{ACC} & \textbf{90.51} & 85.42 \\
\bottomrule
\end{tabular}
\end{table*}

This first analysis allows us to conclude that \textit{the state-of-the-art NMT models can be applied for the generation of code used to exploit the software, and provide high performance when used in combination with data processing}.



\subsection{Accuracy of NMT at generating assembly code snippets}
\label{subsec:RQ2}

In \S{}~\ref{subsec:RQ1}, we used the code written by the programmers (i.e., the authors of the shellcodes) as ground truth for the evaluation. Therefore, when the model predicts the assembly code snippets starting from their natural language description, the predicted output is compared to code composing the original shellcode attacks. 
However, since the same English intent can be translated into different but equivalent assembly snippets, automated metrics (such as BLEU scores) are not perfect in that they do not credit semantically correct code that fails to match the reference.
For example, the snippets \texttt{jz label} and \texttt{je label} are semantically identical, even if they use different instructions (\texttt{jz} vs. \texttt{je}). Furthermore, these metrics do not indicate whether the generated code would compile or not. 
Accordingly, we define two new metrics: a generated output snippet (single or multi-line) is considered \textbf{\textit{syntactically correct}} if it is correctly structured in assembly language and compiles correctly.
The output is considered \textbf{\textit{semantically correct}} if the snippet is an appropriate translation in assembly language given the intent description. 
Consider the intent \textit{transfer the contents of the \texttt{ebx} register into the \texttt{eax} register}. If the approach generates the snippet \texttt{mov ebx, eax}, then the snippet is considered syntactically correct (it would compile), but not semantically correct because the order of the operands is inverted. 
These two metrics allow us to assess the deeper linguistic features of the code~\cite{han-etal-2021-translation}. The semantic correctness implies syntax correctness, while a snippet can be syntactically correct but semantically incorrect. When a snippet is syntactically incorrect it is also semantically incorrect. The evaluation of the semantic equivalence between the output predicted by the models and the code written by the authors of the shellcodes provides the best insights into the quality of the output since it allows us to assess the correctness of the predicted code even if its syntax differs from the ground truth. This is the reason why we did not limit the analysis to automatic metrics, and manually evaluated the semantic meaning of generated code.

To evaluate the syntactic correctness of the outputs, we used the NASM compiler in order to check whether the code is compilable, while we evaluated the semantic correctness by checking if the code generated by the models is a correct translation of the English intent. 
We performed this analysis manually, by checking every single line of generated code. This analysis could not be performed automatically, since an English intent can be translated into several forms that are different, but semantically equivalent. For the same reason, manual (“human”) evaluation is a common practice in NMT studies. The manual evaluation also gives better insights into the quality of machine translation and allows us to analyze errors in the output. To reduce the possibility of errors in manual analysis, multiple authors performed this evaluation independently, obtaining a consensus for the semantic correctness of the output predicted by the models.



Table~\ref{tab:manual_evaluation} shows the percentage of syntactically and semantically correct snippets across all the examples of the test set. We evaluated the performance of Seq2Seq and CodeBERT, both using data processing. Both syntactic and semantic evaluations were performed by compiling the generated snippets under the NASM compiler. Table~\ref{tab:manual_evaluation} shows that both approaches are able to generate   of syntactically correct snippets. 
\textit{Paired t-tests} indicated that the differences between the models are not statistically significant for the syntactic correctness, but they are statistically significant for semantic correctness (at ). 


\begin{table}[ht]
\centering
\caption{Code correctness evaluation of the translation task given the whole test set. Bolded values are the best performance. ( p0.01)}
\label{tab:manual_evaluation}
\begin{tabular}{
>{\centering\arraybackslash}m{3cm}| >{\centering\arraybackslash}m{1.5cm}
>{\centering\arraybackslash}m{1.5cm}}
\toprule
\textbf{Code Correctness Metrics  (\%)}  & \textbf{Seq2Seq with data processing} & \textbf{CodeBERT with data processing}\\
\midrule
\textit{Syntactically Correct} & 96.58 & \textbf{97.20} \\
\textit{Semantically Correct} & 85.40 & \textbf{93.16*} \\
\bottomrule
\end{tabular}
\end{table}


Again, we further investigated the results provided by CodeBERT, by evaluating the performance of the model on single vs. multi-line snippets. 
Table~\ref{tab:manual_evaluation_single_vs_multi} highlights that the multi-line snippets affect model performance on syntactic correctness, although we find no statistically significant difference in model performance on the semantic correctness metric.


\begin{table}[ht]
\centering
\caption{Code correctness evaluation of the translation task comparing single-line and multi-line snippets from the test set. Bolded values are the best performance.}
\label{tab:manual_evaluation_single_vs_multi}
\begin{tabular}{
>{\centering\arraybackslash}m{3cm}| >{\centering\arraybackslash}m{1.6cm}
>{\centering\arraybackslash}m{1.6cm}}
\toprule
\textbf{Code Correctness Metrics (\%)} & \textbf{Single-line snippets} & \textbf{Multi-line snippets}\\
\midrule
\textit{Syntactically Correct} & \textbf{97.81} & 93.75 \\
\textit{Semantically Correct} & 93.06 & \textbf{93.75} \\
\bottomrule
\end{tabular}
\end{table}



Table~\ref{tab:successful_cases} show illustrative examples of code snippets that the model can successfully translate (i.e., the snippets generated by the approach are syntactically and semantically correct). 
Rows 3, 6, and 8 are examples of correct snippets that are penalized by automated metrics, even if they do not exactly match the ground truth. 
Despite some slight differences with the ground truth, the generated code is semantically correct, due to the ambiguity of the assembly language. Thus, these differences are still considered correct by our manual analysis. 
We note correctly generated examples of multi-line snippets in rows 2, 3, 4, and 6. Also, we observe in row 3, the ability to generate multi-line snippets from a relatively abstract intent.



We conclude that \textit{both Seq2Seq and CodeBERT provide syntactically and semantically correct code snippets with high accuracy. Moreover, CodeBERT provides the best performance in the task of generating shellcodes from natural language intents.}
Due to these findings, we consider CodeBERT (with data processing) as our reference NMT model for the following experimental objectives.


\begin{table*}[t]
\centering
\caption{Illustrative examples of successfully generated snippets using our approach. Differences between the output and ground truth are bolded. Such differences are penalized by automatic metrics even though they are correct.}
\label{tab:successful_cases}
\begin{tabular}{
>{\centering\arraybackslash}m{0.75cm}|
 >{\centering\arraybackslash}m{3.5cm}|
 >{\centering\arraybackslash}m{3cm}|
 >{\centering\arraybackslash}m{3cm}}
\toprule
\textbf{Row} & \textbf{Natural Language Intent} & \textbf{Ground Truth} & \textbf{Model Output}\\
\midrule
1 & \textit{Move the byte at the address [edi] into bl} & \texttt{mov bl, byte [edi]} & \texttt{mov bl, byte [edi]} \\
\midrule
2 & \textit{Jump to the \_start label if the value in the eax register is not equal to the doubleword addressed by edi} & \texttt{scasd \textbackslash{n} jnz \_start} &	\texttt{scasd \textbackslash{n} jnz \_start} \\
\midrule
3 & \textit{Put /bin/sh into ebx} & \texttt{push 0x68732f2f \textbackslash{n} push 0x6e69622f \textbackslash{n} mov ebx, esp} &\texttt{push \textbf{long} 0x68732f2f \textbackslash{n} push \textbf{long} 0x6e69622f \textbackslash{n} mov ebx, esp} \\
\midrule
4 & \textit{Push the value 0x61702f2f and the value 0x6374652f onto the stack and point the ebx register to the stack register} & \texttt{push 0x61702f2f \textbackslash{n} push 0x6374652f \textbackslash{n} mov ebx, esp} & \texttt{push 0x61702f2f \textbackslash{n} push 0x6374652f \textbackslash{n} mov ebx, esp} \\
\midrule
5 & \textit{Perform a logical xor between the address specified by [ecx + 116] and the dh register and save the result in [ecx + 116]} & \texttt{xor [ecx + 116], dh} & \texttt{xor [ecx + 116], dh} \\
\midrule
6 & \textit{If the contents of the al register is equal to the value 0xf2 then jump to the \_start label} & \texttt{cmp al, 0xf2 \textbackslash{n} jz \_start} & \texttt{cmp al, 0xf2 \textbackslash{n} \textbf{je} \_start} \\
\midrule
7 & \textit{Move esi into the doubleword starting at the address esp-4} & \texttt{mov dword [esp-4], esi} & \texttt{mov dword [esp-4], esi} \\
\midrule
8 & \textit{Call kernel}	& \texttt{int 0x80} & \texttt{int 0x80\textbf{h}} \\
\bottomrule
\end{tabular}
\end{table*}


\subsection{Accuracy of the NMT at generating whole shellcodes}
\label{subsec:RQ3}
The ultimate goal of developers is to craft entire shellcodes. The previous evaluation showed that NMT can generate individual code snippets that are likely the correct ones. Thus, NMT can be queried by developers to translate specific parts of a program. Here, we raise the bar for the evaluation, by analyzing to which extent NMT can generate an \emph{entire} shellcode. To this purpose, we consider groups of intents from the same exploit and compare the resulting code snippets with the original shellcode. We use two new metrics to evaluate the ability of the approach to generate semantically and syntactically correct code for entire shellcodes.

Let  be the the number of total lines of the -th program in the test set (). Let also consider  as the number of automatically-generated snippets for the -th program that are syntactically correct, and  as the number of automatically-generated snippets that are semantically correct. For every program of the test set, we define the \textit{\textbf{syntactic correctness}} of the program  as the ratio , and the \textit{\textbf{semantic correctness}} of the program as the ratio . 
To perform a conservative evaluation on multi-line snippets, even if only one line of code of the generated snippets is syntactically (semantically) incorrect, we consider all the lines belonging to the multi-line block as syntactically (semantically) incorrect. 
Both metrics range between  and .


For each , we computed the values  and  for the assembly programs in the test set. 
We found that the average syntactic correctness over all the programs of the test set is  (standard deviation is ). Similarly, we estimated the average semantic correctness, which is equal to  (standard deviation is ). Out of  programs, we found that  are \textbf{\textit{compilable}} with NASM and \textbf{\textit{executable}} on the target system.








Since even one incorrect line of code suffices to thwart the effectiveness of a shellcode, we analyzed how many shellcodes could be generated with no errors. We consider a shellcode as \textit{\textbf{fully correct}} if all the assembly instructions composing the shellcode are individually semantically correct (i.e., ).
This evaluation metric is a demanding one. Even if one single line of the shellcode is not semantically correct, then the whole program is considered as not correctly generated. Despite this conservative evaluation, our approach is able to correctly generate 16 out of 30 whole shellcodes. 
Figure~\ref{fig:shellcode_length} shows the summary statistics with a density and a box plot, differentiating the \textit{fully correct} shellcodes  from the \textit{incorrect} ones. As expected, the complexity of the shellcode - in terms of lines of assembly code - impacts the ability of the approach to correctly generate the whole program. 
However, the average (and the median) length of the shellcodes incorrectly generated by the model is affected by the three assembly programs of lengths 55, 59, and 61. If we consider these shellcodes as outliers, then the group of fully correct shellcodes and the group of the incorrectly generated shellcodes are very similar in terms of size.
\textit{We interpret these results as a promising indication towards our ultimate goal of generating entire shellcode programs automatically from short natural language intents.}



 \begin{figure}[ht]

    \centering
    
    \subfloat[Density plot.\label{fig:density_plot}]{\includegraphics[width=0.49\textwidth]{density.pdf}}
    \hfill
    \subfloat[Box plot.\label{fig:time_events}]{\includegraphics[width=0.49\textwidth]{boxplot.pdf}}
    \caption{Plots visualizing the statistics, in terms of lines of assembly code, of the  30 shellcodes in the test set. The labels \textit{Fully Correct} and \textit{Incorrect} refer to the shellcodes that are generated by the approach as fully correct () and incorrect (), respectively..}
    \label{fig:shellcode_length}
     
\end{figure}

\begin{comment}
\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{img/kde.pdf}
    \caption{Density-plots visualizing the statistics, in terms of lines of assembly code, of the  30 shellcodes in the test set. The labels \textit{Fully Correct} and \textit{Incorrect} refer to the shellcodes that are generated by the approach as fully correct () and incorrect (), respectively.}
    \label{fig:shellcode_length}
\end{figure}



\begin{table*}[ht]
\centering
\caption{Descriptive statistics, in terms of lines of assembly code, of the  30 shellcodes in the test set. \textit{Fully Correct} and \textit{Incorrect} refer to the shellcodes that are generated by the approach as fully correct () and incorrect (), respectively. Distribution shows a density plot.}
\label{tab:shellcode_quartiles}
\begin{tabular}{
>{\centering\arraybackslash}m{1.5cm}| 
>{\centering\arraybackslash}m{1cm}
>{\centering\arraybackslash}m{1cm}
>{\centering\arraybackslash}m{1cm}
>{\centering\arraybackslash}m{1cm} |
>{\centering\arraybackslash}m{3cm}}
\toprule
 & \textbf{ Qu.} & \textbf{ Qu.} & \textbf{ Qu.} & \textbf{ Qu.} & \textbf{Distribution}\\
\midrule
& & & & & \multirow{3}{*}[0cm]{\includegraphics[width=2.9cm]{img/kde2.pdf}}\0.4cm]
\textit{Incorrect} & 20.75  & 28.50 & 39.00 & 61.00 & 
\0.4cm]
\bottomrule
\end{tabular}
\end{table*}
\end{comment}




\subsection{Types of errors incurred by NMT in the generation of shellcodes}
\label{subsec:RQ4}

In the last experiment objective, we performed a manual inspection of the model's mispredictions. We noticed that the failure outputs fall down in the following three failure types;
\begin{itemize}
    \item \textbf{\textit{Failure Type A}}: translation failure in generating the correct label, instruction, operand(s), or delimiter(s).
    \item \textbf{\textit{Failure Type B}}:  translation failure in identifying the correct order and/or the addressing mode of operands.
    \item \textbf{\textit{Failure Type C}}: intent parser's failure in identifying one or more of the explicitly stated identifiers.
\end{itemize}


The failure types A and B are due to the lack of ability of the model to perform the correct translation of the English intent in the assembly code. The failure type C, instead, is attributed to the intent parser failure. Indeed, even if the performance of the translation task benefits from the work of the intent parser (see \S{}~\ref{subsec:RQ1}), it is not perfect and can lead to a failure prediction by wrongly identifying the variable or register names, labels, etc. 

Moreover, the error predictions can be further classified as syntactically incorrect and semantically incorrect. We remark that the syntactic incorrectness implies the semantic one.
To better illustrate the problem, we present in Table~\ref{tab:failure_cases} a qualitative evaluation using cherry- and lemon-picked examples of failure prediction from our test set.

\begin{table*}[ht]
\scriptsize
\centering
\caption{Illustrative examples of incorrect outputs. The prediction errors are \textcolor{red}{\textbf{red/bold}}. \textcolor{red}{\textbf{\cancel{Slashed}}} text refers to omitted predictions. \textit{\textbf{Syn}} indicates a syntactically and semantically incorrect snippet, while \textit{\textbf{Sem}} indicates a semantically incorrectness output.} 
\label{tab:failure_cases}
\begin{tabular}{
>{\centering\arraybackslash}m{0.5cm}|
 >{\centering\arraybackslash}m{3cm}|
 >{\centering\arraybackslash}m{2.75cm}|
 >{\centering\arraybackslash}m{2.75cm}|
 >{\centering\arraybackslash}m{1cm}}
\toprule
\textbf{Row} &
\textbf{Natural Language Intent} & \textbf{Ground Truth} & \textbf{Model Output} & {\textbf{Failure Type}}\\
\midrule
1 & \textit{Perform the xor operation between the location pointed by ecx and dh} & \texttt{xor [ecx], dh} & \texttt{xor \textcolor{red}{\textbf{ecx, [dh]}}} & {\textbf{\textit{B, Syn}}}\\
\midrule
2 & \textit{Jump to the \_start label if the value in the eax register is not equal to the doubleword addressed by edi else jump to the edi register} & \texttt{scasd \textbackslash{n} jnz \_start \textbackslash{n} jmp edi} & \texttt{scasd \textcolor{red}{\textbf{\textbackslash{\textbackslash}}} jnz \_start \textbackslash{n} jmp edi} & {\textbf{\textit{A, Syn}}} \\
\midrule
3 & \textit{Define the array of bytes encodedshellcode and initialize it to 0x32, 0x51, 0x30, 0x74, 0x69, 0x63, 0x6f, 0xe4, 0x8a, 0x54, 0xe2, 0x0c, 0x81, 0xc1, 0x69, 0x30, 0x69, 0x30, 0x6a, 0x8a, 0x51, 0xe3, 0x8a, 0xb1, 0xce} & \texttt{encodedshellcode: db 0x32, 0x51,0x30, 0x74, 0x69, 0x63, 0x6f, 0xe4, 0x8a, 0x54, 0xe2, 0x0c, 0x81, 0xc1, 0x69, 0x30, 0x69, 0x30, 0x6a, 0x8a, 0x51, 0xe3, 0x8a, 0xb1, 0xce} & \texttt{\textcolor{red}{\cancel{\textbf{encodedshellcode:}}} db 0x32, 0x51, 0x30, 0x74, 0x69, 0x63, 0x6f, 0xe4, 0x8a, 0x54, 0xe2, 0x0c, 0x81, 0xc1, 0x69, 0x30, 0x69, 0x30, 0x6a, 0x8a, 0x51, 0xe3, 0x8a, 0xb1, 0xce} &{\textbf{\textit{A, Sem}}} \\
\midrule
4 & \textit{Call decoder} &    \texttt{call decoder} & \texttt{\textcolor{red}{\cancel{\textbf{call}}} decoder\textcolor{red}{\textbf{:}}}& {\textbf{\textit{A, Sem}}} \\
\midrule
5 & \textit{Decrement the ecx register and jump to the main\_push label if the contents of the ecx register is not zero else move the value 30 into the cl register} & \texttt{loop main\_push \textbackslash{n} mov cl, 30} & \texttt{loop main\_push \textbackslash{n} mov \textcolor{red}{\textbf{var3}}, \textcolor{red}{\textbf{esp}}} & {\textbf{\textit{C, Syn}}} \\
\midrule
6 & \textit{Perform logical or between the cx register and 0xfff} &    \texttt{or cx, 0xfff} & \texttt{\textcolor{red}{\textbf{x}}or \textcolor{red}{\textbf{0xfff}}, \textcolor{red}{\textbf{cx}}}& {\textbf{\textit{A, B, Syn}}} \\
\midrule
7 & \textit{Perform a logical xor between the location pointed by [ecx + 116] and the bh register and save the result in [ecx + 116]} &    \texttt{xor [ecx + 116], bh} & \texttt{xor byte [ecx + 116], \textcolor{red}{\textbf{0x3}}}& {\textbf{\textit{A, C, Sem}}} \\
\midrule
8 & \textit{Move 2001Q into cx} & \texttt{mov cx, 2001Q} & \texttt{mov cx, 2001\textcolor{red}{{\cancel{\textbf{Q}}}}}& {\textbf{\textit{C, Sem}}} \\
\midrule
9 & \textit{Push the word 0666q onto the stack} & \texttt{push word 0666q} & \texttt{push word 06\textcolor{red}{\textbf{44o}}}& {\textbf{\textit{C, Sem}}} \\
\midrule
10 & \textit{Declare the read label} & \texttt{read:} & \texttt{\textcolor{red}{\textbf{section}}:}& {\textbf{\textit{A, C, Syn}}}\\
\bottomrule
\end{tabular}
\end{table*}




The first row showcases an example of failing to model because of implicit knowledge. The intent does not mention the indirect addressing mode (specified by the bracket [] in NASM syntax). 
In the second row, we note that the model failed to generate the newline token properly to separate the snippets with lines. This causes a syntax issue, and since it does not compile we count it as syntactically incorrect.
The third row shows an example in which a byte string is declared without defining the label, while the fourth row illustrates the model's failure to predict the right instruction (the definition of the function \texttt{decoder} instead of the execution of the function). Both outputs do not raise an exception when compiled, therefore they are syntactically but not semantically correct.
In the fifth row, we note that the intent parser correctly identifies \texttt{main\_push} in the standardization process, but fails to recognize the \texttt{cl} register and misidentifies \texttt{ecx} instead. We also note that the model predicted a \texttt{mov} operation between two registers (\texttt{register, esp}) rather than a register and a value.  The predicted register does not exist in the intent hence, the output is a \texttt{var3}. 
The sixth row shows an example with incorrect instruction and inverse operands order.
The remaining examples include the intent parser failing to identify explicitly stated identifiers or letters in values sometimes in long intents such as in the case of the \texttt{bh} register (row 7) and occasionally in simple contexts such as in the case of \texttt{read} (row 10). The last row is considered also syntactically incorrect since it is not possible to declare a label with the \texttt{section} assembly directive. 
This goes to show when there is a mistake in the standardization step, the translation may fail to work around it even if the intent seems simple.



The failure outputs also provide indications on what it can be done to increase the performance of the code generation task. Most of the errors can be easily identified by the programmers: incorrect addressing modes (first row),  wrong newline character (second row), missing labels (e.g., \texttt{encodedshellcode} in row number 3), wrong instructions (row 4, 6), undefined variables (e.g., \texttt{var3} in row 5), wrong operand orders (row number 6), etc.
The syntactically incorrect predictions, i.e., the predictions that do not follow the syntax, can be identified with a compiler and can be fixed through an ``intelligent" post-processing phase, which should be trained to identify and fix the failure outputs. This is part of the future work.




\subsection{Discussion and Lessons Learned}
\label{subsec:discussion}
The experimental analysis pointed out that NMT models can efficiently generate assembly code for real shellcodes, starting from their natural description. When used in combination with data processing, the accuracy of the code generation task is high enough to support developers in developing software exploits. 
Even if the size and the complexity of an English intent increase, the performance of the translation task is not negatively affected. 
CodeBERT achieves the best performance and further justifies its wide usage to address software engineering tasks. 
The model is able to generate whole software exploits with syntactic and semantic correctness greater than . It is also able to generate programs that are fully correct, i.e., compilable and executable on the target system. However, the complexity of the software attacks (in terms of lines of code) reduces the accuracy of generating entire programs.
The analysis also pointed out that the most common error predictions are easily identifiable and can be fixed during the post-processing process. 
