\documentclass[11pt]{article}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{algpseudocode}
\usepackage{algorithm,algorithmicx}
\usepackage{color}
\usepackage[pdftex]{graphicx}
\usepackage{comment}
\usepackage{hyperref}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-0.5in}

\newlength{\defbaselineskip}
\setlength{\defbaselineskip}{\baselineskip}
\setlength{\marginparwidth}{0.8in}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\bigO}{\mathcal{O}}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\A}{\mathcal{A}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\T}{\mathcal{T}}
\DeclareMathOperator{\D}{\mathcal{D}}
\DeclareMathOperator{\E}{\mathcal{E}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}



\allowdisplaybreaks

\begin{document}

\title{Low-distortion Subspace Embeddings in Input-sparsity Time \\ and
  Applications to Robust Linear Regression }

\author{
  Xiangrui Meng
  \thanks{
    Most of this work was done while the author was at 
    ICME, Stanford University supported by NSF DMS-1009005.
    Current affiliation: LinkedIn Corporation,
    Mountain View, 94403.
    Email: ximeng@linkedin.com.
  }
  \and
  Michael W. Mahoney
  \thanks{
    Dept.\ of Mathematics,
    Stanford University,
    Stanford, CA 94305.
    Email: mmahoney@cs.stanford.edu
  }
}

\date{}
\maketitle




\begin{abstract}\noindent
Low-distortion subspace embeddings are critical building blocks for developing
improved random sampling and random projection algorithms for common linear
algebra problems.
Here, we show that, given a matrix , with , and
a , with a constant probability, we can construct a low-distortion
embedding matrix  that embeds , the
 subspace spanned by 's columns, into ; the distortion of our embeddings is only , and
we can compute  in  time, i.e., input-sparsity time.
Our result generalizes the input-sparsity time  subspace embedding
proposed recently by Clarkson and Woodruff; and for completeness, we present a
simpler and improved analysis of their construction for .
These input-sparsity time  embeddings are optimal, up to constants, in
terms of their running time; and the improved running time propagates to
applications such as -distortion  subspace embedding
and relative-error  regression.
For , we show that a -approximate solution to the 
regression problem specified by the matrix  and a vector  can be
computed in  time; and for
, via a subspace-preserving sampling procedure, we show that a -distortion embedding of  into  can be
computed in  time, and we also show that a
-approximate solution to the  regression problem  can be computed in  time.
Moreover, we can also improve the embedding dimension or equivalently the sample
size to  without increasing the
complexity.
\end{abstract}


\section{Introduction}
\label{sxn:intro}

Regression problems are ubiquitous, and the fast computation of their 
solutions is of interest in many large-scale data applications.
A parameterized family of regression problems that is of particular interest 
is the \emph{overconstrained  regression problem}: given a matrix 
, with , a vector , a norm 
 parameterized by , and an error parameter 
, find a -approximate solution 
 to:

i.e., find a vector  such that 
, where the  norm of a
vector  is , defined to be
 for .
Special cases include the  regression problem, also known as Least
Squares Approximation problem, and the  regression problem, also known
as the Least Absolute Deviations or Least Absolute Errors problem.
The latter is of particular interest as a \emph{robust estimation} or
\emph{robust regression} technique, in that it is less sensitive to the presence
of outliers than the former.
We are most interested in this paper in the  regression problem due 
to its robustness properties, but our methods hold for general , 
and thus we formulate our results in .

It is well-known that for , the overconstrained  regression
problem is a convex optimization problem; for  and , it is an
instance of linear programming; and for , it can be solved with
eigenvector-based methods such as with the QR decomposition or the Singular
Value Decomposition of .
In spite of their low-degree polynomial-time solvability,  regression
problems have been the focus in recent years of a wide range of random sampling
and random projection algorithms, largely due to a desire to develop improved
algorithms for large-scale data applications~\cite{AMT10,MSM11_TR,CDMMMW13_SODA}.
For example, Clarkson~\cite{Cla05} uses subgradient and sampling methods to
compute an approximate solution to the overconstrained  regression
problem in roughly  time; and Dasgupta et
al.~\cite{DDHKM09_lp_SICOMP} use well-conditioned bases and subspace-preserving
sampling algorithms to solve general  regression problems, for
, in roughly  time.
A similar subspace-preserving sampling algorithm was developed by Drineas,
Mahoney, and Muthukrishnan~\cite{DMM06} to compute an approximate solution to
the  regression problem.
The algorithm of~\cite{DMM06} relies on the estimation of the  leverage
scores\footnote{Recall that for an  matrix , with , the
  \emph{ leverage scores} of the rows of  are equal to the diagonal
  elements of the projection matrix onto the span of .
  That is, if  is a QR decomposition of , or if  is the
  thin SVD of , then the leverage scores equal the Euclidean norms squared of
  the rows of the  matrix , and thus they can be computed exactly
  in  time.
  See~\cite{Mah-mat-rev_BOOK,DMMW12_ICML} for details; and note that they can be
  generalized to  and other  norms~\cite{CDMMMW13_SODA} as well as
  to arbitrary  matrices, with both  and  large, if one
  specifies a low-rank parameter~\cite{CUR_PNAS,DMMW12_ICML}.}
of  to be used as an importance sampling distribution, but when combined with
the results of Sarl\'{o}s~\cite{Sarlos06} and Drineas et
al.~\cite{DMMS07_FastL2_NM10} (that quickly preprocess  to uniformize those
scores) or Drineas et al.~\cite{DMMW12_ICML} (that quickly computes
approximations to those scores), this leads to a random projection or random
sampling (respectively) algorithm for the  regression problem that runs
in roughly  time~\cite{DMMS07_FastL2_NM10,Mah-mat-rev_BOOK}.
More recently, Sohler and Woodruff~\cite{SW11} introduced the Cauchy Transform
to obtain improved  embeddings, thereby leading to an algorithm for the
 regression problem that runs in  time; and Clarkson
et al.~\cite{CDMMMW13_SODA} use the Fast Cauchy Transform and ellipsoidal rounding
methods to compute an approximation to the solution of general 
regression problems in roughly  time.

These algorithms, and in particular the algorithms for , form the basis for
much of the large body of recent work in randomized algorithms for low-rank
matrix approximation, and thus optimizing their properties can have immediate
practical benefits.
See, e.g., the recent monograph of Mahoney~\cite{Mah-mat-rev_BOOK} and
references therein for details.
Although some of these algorithms are near-optimal for dense inputs, they all
require  time, which can be large if the input matrix is 
very sparse.
Thus, it was a significant result when Clarkson and
Woodruff~\cite{CW12sparse_TR} developed an algorithm for the  regression
problem (as well as the related problems of low-rank matrix approximation and
 leverage score approximation) that runs in \emph{input-sparsity time},
i.e., in  time, where  is the
number of non-zero elements in  and  is an error parameter.
This result depends on the construction of a \emph{sparse embedding matrix}
 for .
By this,
we mean the following: 
for an  matrix , an  matrix  such that, 

for all .
That is,  embeds the column space of  into , while approximately
preserving the  norms of all vectors in that subspace.
Clarkson and Woodruff achieve their improved results for -based problems
by showing how to construct such a  with  and showing
that it can be applied to an arbitrary  in 
time~\cite{CW12sparse_TR}.
(In particular, this embedding result improves the result of Meng, Saunders, and
Mahoney~\cite{MSM11_TR}, who in their development of the parallel least-squares
solver \textsc{LSRN} use a result from Davidson and
Szarek~\cite{davidson2001local} to construct a constant-distortion embedding for
 that runs in  time.)
Interestingly, the analysis of Clarkson and Woodruff coupled ideas from the data
streaming literature with the structural fact that there cannot be too many
high-leverage constraints/rows in .
In particular, they showed that the high-leverage parts of the subspace may be
viewed as heavy-hitters that are ``perfectly hashed,'' and thus contribute no
distortion, and that the distortion of the rest of the subspace as well as the
``cross terms'' may be bounded with a result of Dasgupta, Kumar, and
Sarl\'{o}s~\cite{DKT10}.

In this paper, we provide improved low-distortion subspace embeddings for
, for all , in input-sparsity time; and we show that, by
coupling with recent work on fast subspace-preserving sampling
from~\cite{CDMMMW13_SODA}, these embeddings can be used to provide
-approximate solutions to  regression problems, for
, in nearly input-sparsity time.
In more detail, our main results are the following.
\begin{itemize}
\item For , we obtain an improved result for the input-sparsity time
  -distortion embedding of~\cite{CW12sparse_TR}.
In particular, for the same embedding procedure, we obtain improved bounds 
  for the embedding dimension with a much simpler analysis 
  than~\cite{CW12sparse_TR}.
  See Theorem~\ref{thm:sparse_l2} of Section~\ref{sxn:l2} for a precise 
  statement of this result.
  Our analysis is direct and does \emph{not} rely on splitting the 
  high-dimensional space into a set of heavy-hitters consisting of the 
  high-leverage components and the complement of that heavy-hitting set.  
  In addition, since our result directly improves the  embedding 
  result of Clarkson and Woodruff~\cite{CW12sparse_TR}, it immediately leads 
  to improvements for the  regression, low-rank matrix approximation, 
  and  leverage score estimation problems that they consider.
\item For , we obtain a low-distortion sparse embedding matrix 
  such that  can be computed in input-sparsity time.
That is, we construct an embedding matrix  such that, for all ,
  
  with a constant probability, and  can be computed in 
  time.  
  See Theorem~\ref{thm:sparse_l1} of Section~\ref{sxn:l1} for a precise
  statement of this result.  
  Here, our proof involves splitting the set , where  is an  well-conditioned
  basis for the span of , into two parts, informally a subset where
  coordinates of high  leverage dominate  and the complement of
  that subset. 
  This  result leads to immediate improvements in -based
  problems.
  For example, by taking advantage of the fast version of subspace-preserving
  sampling from~\cite{CDMMMW13_SODA}, we can construct and apply a
  -distortion sparse embedding matrix for  in
  ~time. 
  In addition, we can use it to compute a -approximation to the
   regression problem in 
  time, which in turn leads to immediate improvements in -based matrix
  approximation objectives, e.g., for the  subspace approximation
  problem~\cite{bd09,SW11,CDMMMW13_SODA}.
\item For , for all , we obtain a low-distortion sparse
  embedding matrix  such that  can be computed in input-sparsity
  time.
  That is, we construct an embedding matrix 
  such that, for all ,
  
  with a constant probability, and  can be computed in 
  time.
  See Theorem~\ref{thm:sparse_lp} of Section~\ref{sxn:lp} for a precise
  statement of this result.
  Here, our proof generalizes the  result, but we need to prove upper
  and lower tail bound inequalities for sampling from general -stable
  distributions that are of independent interest.
  Although these distributions don't have closed forms for  in
  general, we prove that there exists an order among the Cauchy distribution, a
  -stable distribution with , and the Gaussian distribution such
  that for all  we can use the upper bound from the Cauchy
  distribution and the lower bound from the Gaussian distribution.
  As with our  result, this  result has several extensions: in
   time, we can construct and
  apply a -distortion sparse embedding matrix for ; in
   time, we can compute a
  -approximation to the  regression problem; and in
   time, we can construct and apply a
  near-optimal (in terms of embedding dimension and distortion factor) 
  embedding~matrix.
\end{itemize}

\noindent
The -distortion subspace embedding (for , ,
that we construct from the input-sparsity time embedding and the fast
subspace-preserving sampling) has embedding dimension , where the somewhat large 
term directly multiplies the  term. 
We can also improve this, showing that it is possible, without increasing the
overall complexity, to decouple the large  and
 via another round of sampling and conditioning,
thereby obtaining an embedding dimension that is a small  times
. 
See Theorem~\ref{thm:improved-dim} of Section~\ref{sxn:improve} for a precise
statement of this result.  

\bigskip

\textbf{Remark.}
Subsequent to our posting a preliminary version of this paper on the
arXiv~\cite{MM12_TR}, Clarkson and Woodruff let us know that, independently of
us, they used a result from~\cite{CDMMMW13_SODA} to extend their 
subspace embedding from~\cite{CW12sparse_TR} to provide a nearly input-sparsity
time algorithm for  regression, for all .
This is now posted as Version 2 of~\cite{CW12sparse_TR}.
Their approach requires solving a rounding problem of size , which depends on  (possibly very large).
Our approach does not contain this intermediate step and it only needs
 storage.
Moreover, to the best of our knowledge, their method does not provide
low-distortion  subspace embeddings in input-sparsity time, as we are
able to provide (in a simple and oblivious way).

\textbf{Remark.}
In the first version of this paper, the embedding dimension for  in
Theorem~\ref{thm:sparse_l2} was .
Subsequent to the dissemination of this version, Drineas pointed out to us that,
with a slight modification to our original proof, our result could very easily
be improved to .
Nelson and Nguyen also let us know that, at about the same time and using the
same technique, but independent of us, they too obtained and first published the
 embedding result~\cite{nelson2012osnap}.

\section{Background}
\label{sxn:lbackground} 

We use  to denote the  norm of a vector,  the
spectral norm of a matrix,  the Frobenius norm of a matrix, and
 the element-wise  norm of a matrix.
Given  with full column rank and , we 
use  to denote the  subspace spanned by 's columns.
In this paper, we are interested in fast embedding of  into a
-dimensional subspace of , with distortion
either  or , for some , as well as
applications of this embedding to problems such as  regression.
We assume that .
To state our results, we assume that we are capable of computing a
-approximate solution to an  regression problem of size
 for some , as long as  is independent of .
Let us denote the running time needed to solve this smaller problem by
.
In theory, we have 
(see Rokhlin and Tygert~\cite{RT08} and Drineas et 
al.~\cite{DMMS07_FastL2_NM10}), and , for general  (see, e.g.,
Mitchell~\cite{mitchell2003polynomial}).



\paragraph{Conditioning.}
The  subspace embedding and  regression problems are closely
related to the concept of conditioning.  
We state here two related notions of -norm conditioning and then a lemma
that characterizes the relationship between them.

\begin{definition}[-norm Conditioning (from \cite{CDMMMW13_SODA})]
  \label{def:lpnormcond}
  Given an  matrix  and , let
  
  Then, we denote by  the \emph{-norm condition number of
    }, defined to be:
  
  For simplicity, we will use , , and 
   when the underlying matrix is clear.
\end{definition}

\begin{definition}[-conditioning (from \cite{DDHKM09_lp_SICOMP})]
\label{def:lpbasis}
  Given an  matrix  and , let  be the dual
  norm of .
  Then  is \emph{-conditioned} if (1) ,
  and (2) for all , .
  Define  as the minimum value of  such that
   is -conditioned. 
\end{definition}

\begin{lemma}[Equivalence of  and  (from
  \cite{CDMMMW13_SODA})]
  \label{lemma:kappa_equiv}
  Given an  matrix  and , we always have
  
\end{lemma}

\noindent
\textbf{Remark.}
Given the equivalence established by Lemma~\ref{lemma:kappa_equiv}, we will say
that  is \emph{well-conditioned in the  norm} if  or
, independent of . 

Although for an arbitrary matrix , the condition numbers
 and  can be arbitrarily large, we can often
find a matrix  such that  is well-conditioned. 
This procedure is called \emph{conditioning}, and there exist two approaches for
conditioning: via low-distortion  subspace embedding and via ellipsoidal
rounding.

\begin{definition}[Low-distortion  Subspace Embedding]
  Given an  matrix  and ,  is a low-distortion embedding of  if 
  and
  
\end{definition}

\noindent
\textbf{Remark.}
Given a low-distortion embedding matrix  of , let  be the ``R''
matrix from the QR decomposition of . 
Then, the matrix  is well-conditioned in the  norm. 
To see this, note that we~have

where the first inequality is due to low distortion and the second inequality is
due to . 
By similar arguments, we can show that . 
Hence, by combining these results, the matrix  is well-conditioned in
the  norm. 

For a discussion of ellipsoidal rounding, we refer readers to Clarkson et
al.~\cite{CDMMMW13_SODA}. 
In this paper, we simply cite the following lemma, which is based on ellipsoidal
rounding.

\begin{lemma}[Fast -conditioning (from \cite{CDMMMW13_SODA})]
  \label{lemma:lp_cond_2d}
  Given an  matrix  and , it takes at most
   time to find a matrix  such that
  .
\end{lemma}

\paragraph{Subspace-preserving sampling and  regression.}
Given  such that  is well-conditioned in the
 norm, we can construct a -distortion embedding,
specifically a subspace-preserving sampling, of  in  additional time and with a constant probability.
This result from Clarkson et al.~\cite[Theorem 5.4]{CDMMMW13_SODA} improves 
the subspace-preserving sampling algorithm proposed by Dasgupta et 
al.~\cite{DDHKM09_lp_SICOMP} by estimating the row norms of 
(instead of computing them exactly) to define importance sampling 
probabilities.

\begin{lemma}[Fast Subspace-preserving Sampling (from \cite{CDMMMW13_SODA})]
  \label{lemma:fast_sampling}
  Given a matrix , , ,
  and a matrix  such that  is well-conditioned,
  it takes  time to compute a sampling matrix  (with only one nonzero element per row) with 
  such that with a constant probability,
  
\end{lemma}

\noindent
Given such a subspace-preserving sampling algorithm, Clarkson et
al.~\cite[Theorem 5.4]{CDMMMW13_SODA} show that it is straightforward to
compute a -approximate solution to an 
regression problem.

\begin{lemma}[ Regression via Sampling (from \cite{CDMMMW13_SODA}]
  \label{lemma:fast_reg}
  Given an  regression problem specified by , , and , let  be a -distortion
  embedding matrix of the subspace spanned by 's columns and  from
  Lemma~\ref{lemma:fast_sampling}, and let  be an optimal solution to
  the subsampled problem .
  Then  is a -approximate solution to
  the original problem.
\end{lemma}

\noindent
\textbf{Remark.}
Collecting these results, we see that a low-distortion  subspace
embedding is a fundamental building block (and very likely a bottleneck) for
-distortion  subspace embeddings, as well as for a
-approximation to an  regression problem. 
This motivates our work and its emphasis on finding low-distortion subspace 
embeddings more efficiently.

\paragraph{Stable distributions.}
\label{sec:stable-distributions}

The properties of -stable distributions are essential for constructing
input-sparsity time low-distortion  subspace embeddings.

\begin{definition}[-stable Distribution]
  A distribution  over  is called -stable, if for any  real
  numbers , we have
  
  where  and .
  By ``'', we mean  and  have the same distribution.
\end{definition}

\noindent
By a result due to L{\'e}vy~\cite{levy1925calcul}, it is known that -stable
distributions exist for ; and from Chambers et
al.~\cite{chambers1976method}, it is known that -stable random variables can
be generated efficiently, thus allowing their practical use.
Let us use  to denote the ``standard'' -stable distribution, for
, specified by its characteristic function .
It is known that  is the standard Cauchy distribution, and that  is
the Gaussian distribution with mean  and variance .

\paragraph{Tail inequalities.}
We note two inequalities from Clarkson et al.~\cite{CDMMMW13_SODA} regarding the
tails of the Cauchy distribution.

\begin{lemma}[Cauchy Upper Tail Inequality]
  \label{lemma:cauchy_upper}
  For , let  be  (not necessarily independent) standard
  Cauchy variables, and  with .
  Let .
  For any ,
  
  For simplicity, we assume that  and , and then we have
  .
\end{lemma}

\begin{lemma}[Cauchy Lower Tail Inequality]
  \label{lemma:cauchy_lower}
  For , let  be independent standard Cauchy random variables, and
   with . 
  Let . 
  Then, for any ,
  
\end{lemma}

\noindent
We also note the following result about Gaussian variables.
This is a direct consequence of Maurer's inequality~(\cite{maurer2003bound}),
and we will use it to derive lower tail inequalities for -stable
distributions.

\begin{lemma}[Gaussian Lower Tail Inequality]
  \label{lemma:gaussian_lower}
  For , let  be independent standard Gaussian random
  variables, and  with . 
  Let . 
  Then, for any ,
  
\end{lemma}




\section{Main Results for  Embedding}
\label{sxn:l2} 

Here is our main result for input-sparsity time low-distortion subspace
embeddings for .
See also Nelson and Nguyen~\cite{nelson2012osnap} for a similar result with a
slightly better constant.

\begin{theorem}[()-distortion Embedding for ]
  \label{thm:sparse_l2}
  Given a matrix  and , let  where  has each column chosen independently and
  uniformly from the  standard basis vectors of  and  is a diagonal matrix with diagonal entries chosen independently
  and uniformly from .
  Given any , let .
  Then with probability at least ,
  
  In addition,  can be computed in  time.
\end{theorem}

\noindent
The construction of  in this theorem is the same as the construction in
Clarkson and Woodruff~\cite{CW12sparse_TR}.
For them,  in order to achieve  distortion with a constant probability. 
Theorem~\ref{thm:sparse_l2} shows that it actually suffices to set .
Surprisingly, the proof is rather simple.
Let , where  is an orthonormal basis for .
Compute  and apply Markov's inequality to , which implies  and hence
the embedding result.
See Appendix~\ref{sec:proof_l2} for a complete proof. 

\noindent
\textbf{Remark.} 
The  running time is indeed optimal, up to constant factors, 
for general inputs.
Consider the case when  has an important row  such that  becomes
rank-deficient without it.
Thus, we have to observe  in order to compute a low-distortion embedding.
However, without any prior knowledge, we have to scan at least a constant
portion of the input to guarantee that  is observed with a constant
probability, which takes  time.
Note that this optimality result applies to general .

The results of Theorem~\ref{thm:sparse_l2} propagate to related applications,
e.g., to the  regression problem, the low-rank matrix approximation
problem and the problem of computing approximations to the  leverage
scores.
Since it underlies the other applications, only the  regression
improvement is stated here explicitly; its proof is basically combining our
Theorem~\ref{thm:sparse_l2} with Theorem~19 of~\cite{CW12sparse_TR}.

\begin{corollary}[Fast  Regression]
  With a constant probability, a -approximate solution to an
   regression problem can be computed in  time.
\end{corollary}


\noindent
\textbf{Remark.}  
Although our simpler direct proof leads to a better result for  subspace
embedding, the technique used in the proof of Clarkson and
Woodruff~\cite{CW12sparse_TR}, which splits coordinates into ``heavy'' and
``light'' sets based on the leverage scores, highlights an important structural
property of  subspace: that only a small subset of coordinates can have
large  leverage scores.  
(We note that the technique of splitting coordinates is also used by Ailon and
Liberty~\cite{AL11} to get an unrestricted fast Johnson-Lindenstrauss transform;
and that the difficulty in finding and approximating the large-leverage
directions was---until
recently~\cite{Mah-mat-rev_BOOK,DMMW12_ICML}---responsible for difficulties in
obtaining fast relative-error random sampling algorithms for  regression
and low-rank matrix approximation.)  
An analogous structural fact holds for  and other  spaces.  
Using this property, we can construct novel input-sparsity time  
subspace embeddings for general , as we discuss in the next 
two~sections.


\section{Main Results for  Embedding}
\label{sxn:l1} 

Here is our main result for input-sparsity time low-distortion subspace
embeddings for .

\begin{theorem}[Low-distortion Embedding for ]
  \label{thm:sparse_l1}
  Given  with full column rank, let , where  has each column chosen
  independently and uniformly from the  standard basis vectors of , and
  where  is a diagonal matrix with diagonals chosen
  independently from the standard Cauchy distribution.
  Set  with  sufficiently large.
  Then with a constant probability, we have
  
  In addition,  can be computed in  time.
\end{theorem}

\noindent
The construction of the  subspace embedding matrix is different than its
 norm counterpart only by the diagonal elements of  (or ): whereas
we use  for the  norm, we use Cauchy variables for the 
norm. 
The proof of Theorem~\ref{thm:sparse_l1} uses the technique of splitting
coordinates, the fact that the Cauchy distribution is -stable, and the upper
and lower tail tail inequalities regarding the Cauchy distribution from
Lemmas~\ref{lemma:cauchy_upper} and~\ref{lemma:cauchy_lower}.
See Appendix~\ref{sec:proof_l1} for a complete proof.

\noindent
\textbf{Remark.} 
As mentioned above, the  running time is optimal.
Whether the distortion  is optimal is still an open
question. 
However, for the same construction of , we can provide a ``bad'' case that
provides a lower bound.
Choose .
Suppose that  is sufficiently large such that with an overwhelming
probability, the top  rows of  are perfectly hashed, i.e., , , where  is the -th
diagonal of .
Then, the distortion of  is . 
Therefore, at most an  factor of the distortion is due to
artifacts in our analysis.

Our input-sparsity time  subspace embedding of
Theorem~\ref{thm:sparse_l1} improves the -time
embedding by Sohler and Woodruff~\cite{SW11} and the -time 
embedding of Clarkson et al.~\cite{CDMMMW13_SODA}.
In addition, by combining Theorem~\ref{thm:sparse_l1} and
Lemma~\ref{lemma:fast_sampling}, we can compute a -distortion
embedding in  time, i.e., in \emph{nearly} 
input-sparsity time.

\begin{theorem}[()-distortion Embedding for ]
  \label{thm:sparse_l1-eps}
  Given , it takes  time to
  compute a sampling matrix  with  such that with a constant probability, 
  embeds  into  with distortion .
\end{theorem}

Our improvements in Theorems~\ref{thm:sparse_l1} and~\ref{thm:sparse_l1-eps}
also propagate to related -based applications, including the 
regression and the  subspace approximation problem considered
in~\cite{SW11,CDMMMW13_SODA}.
As before, only the regression improvement is stated here explicitly.
For completeness, we present in Algorithm \ref{alg:fast_l1_reg} our algorithm
for solving  regression problems in nearly input-sparsity time.
The brief proof of Corollary~\ref{cor:l1reg}, our main quality-of-approximation
result for Algorithm~\ref{alg:fast_l1_reg}, may be found in
Appendix~\ref{sxn:pf-cor-l1reg}.

\begin{algorithm}
  \caption{Fast  Regression Approximation in  Time}
  \label{alg:fast_l1_reg}
  \begin{algorithmic}[1]
    \Require  with full column rank, , and
    .  

    \Ensure A -approximation solution  to , with a constant probability.

    \State Let  and denote  the  subspace spanned by
    's columns and .
    
    \State Compute a low-distortion embedding  of  (Theorem~\ref{thm:sparse_l1}).
    
    \State Compute  from  such
    that  is well-conditioned (QR or
    Lemma~\ref{lemma:lp_cond_2d}).

    \State Compute a -distortion embedding  of 
    (Lemma~\ref{lemma:fast_sampling}).

    \State Compute a -approximate solution  to .
  \end{algorithmic}
\end{algorithm}

\begin{corollary}[Fast  Regression]
  \label{cor:l1reg}
  With a constant probability, Algorithm~\ref{alg:fast_l1_reg} computes a
  -approximate solution to an  regression problem in
   time.
\end{corollary}

\noindent
\textbf{Remark.}
For readers familiar with the impossibility results for dimension reduction in
~\cite{CS02,LN04,BC05}, note that those results apply to arbitrary point
sets of size  and are interested in embeddings that are ``oblivious,'' in
that they do not depend on the input data.
In this paper, we only consider points in a subspace, and the
subspace-preserving sampling procedure of~\cite{DDHKM09_lp_SICOMP} that we use
is data-dependent.  



\section{Main Results for  Embedding}
\label{sxn:lp} 

In this section, we use the properties of -stable distributions to generalize
the input-sparsity time  subspace embedding to  norms, for . 
Generally,  does not have explicit PDF/CDF, which increases the difficulty
for theoretical analysis. 
Indeed, the main technical difficulty here is that we are not aware of 
analogues of Lemmas~\ref{lemma:cauchy_upper} and~\ref{lemma:cauchy_lower} that
would provide upper and lower tail inequality for -stable distributions.
(Indeed, even Lemmas~\ref{lemma:cauchy_upper} and~\ref{lemma:cauchy_lower} were
established only recently~\cite{CDMMMW13_SODA}.)

Instead of analyzing  directly, for any , we
establish an order among the Cauchy distribution, the -stable distribution,
and the Gaussian distribution, and then we derive upper and lower tail
inequalities for the -stable distribution similar to the ones we used to
prove Theorem~\ref{thm:sparse_l1}.
We state these technical results here since they are of independent interest.
We start with the following lemma, which is proved in
Appendix~\ref{sxn:pf-equiv} and which establishes this order.

\begin{lemma}
  \label{lemma:equiv}
  For any , there exist constants  and 
  such that
  
  where  is a standard Cauchy variable, ,  is a standard
  Gaussian variable.
  By ``'' we mean , i.e., , where  is the
  corresponding~CDF.
\end{lemma}

\noindent
Our numerical results suggest that the constants  and  
are not too far away from .
See Figure~\ref{fig:stable_cdf}, which plots of the CDFs of  for 
, based on which we conjecture 
, for all 
.
This implies that  and 
, 
which therefore provides a value for the constants  and .

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{stable_cdf.pdf}
\caption{The CDFs () of  for  (bottom, i.e., red or
    dark gray),  (top, i.e., yellow or light gray), where  and the scales of the axes are chosen to magnify the upper (as ) and lower (as ) tails.
    These empirical results suggest  for all .}
  \label{fig:stable_cdf}
\end{figure}

Lemma~\ref{lemma:equiv} suggests that we can use Lemma~\ref{lemma:cauchy_upper}
(regarding Cauchy random variables) to derive upper tail inequalities for
general -stable distributions and that we can use
Lemma~\ref{lemma:gaussian_lower} (regarding Gaussian variables) to derive lower
tail inequalities for general -stable distributions.
The following two lemmas establish these results; the proofs of these lemmas are
provided in Appendix~\ref{sxn:pf-lp-stable-upper} and
Appendix~\ref{sxn:pf-lp-stable-lower}, respectively.

\begin{lemma}[Upper Tail Inequality for -stable Distributions]
  \label{lemma:stable_upper}
  Given , for , let  be  (not necessarily
  independent) random variables sampled from , and  with
  .
  Let .
  Assume that .
  Then for any ,
  
\end{lemma}

\begin{lemma}[Lower Tail Inequality for -stable Distributions]
  \label{lemma:stable_lower}
  For , let  be independent random variables sampled from
  , and  with . Let . Then,
  
\end{lemma}

Given these results, here is our main result for input-sparsity time 
low-distortion subspace embeddings for .
The proof of this theorem is similar to the proof of 
Theorem~\ref{thm:sparse_l1}, except that we replace the  norm 
 by  and use the tail inequalities from 
Lemmas~\ref{lemma:stable_upper} and~\ref{lemma:stable_lower} (rather than 
Lemmas~\ref{lemma:cauchy_upper} and~\ref{lemma:cauchy_lower}).  


\begin{theorem}[Low-distortion Embedding for ]
  \label{thm:sparse_lp}
  Given  with full column rank and , let  where  has each column
  chosen independently and uniformly from the  standard basis vectors of
  , and where  is a diagonal matrix with diagonals
  chosen independently from .
  Set  with  sufficiently large.
  Then with a constant probability, we have
  
  In addition,  can be computed in  time.
\end{theorem}


\noindent
Similar to the  case, our input-sparsity time  subspace
embedding of Theorem~\ref{thm:sparse_lp} improves the -time
embedding of Clarkson et al.~\cite{CDMMMW13_SODA}.
As we mentioned in Section~\ref{sxn:intro}, their construction (and hence the
construction of \cite{CW12sparse_TR}) works for all , but it
requires solving a rounding problem of size  as an
intermediate step, which may become intractable when  is very large in a
streaming environment, while our construction only needs 
storage.
By combining Theorem~\ref{thm:sparse_lp} and
Lemma~\ref{lemma:fast_sampling}, we can compute a -distortion
embedding in  time.

\begin{theorem}[()-distortion Embedding for ]
  \label{thm:sparse_lp-eps}
  Given  and , it takes  time to compute a sampling matrix  with
   such that with a constant
  probability,  embeds  into  with distortion .
\end{theorem}

\noindent
These improvements for  subspace embedding also propagate to related
-based applications.
In particular, we can establish an improved algorithm for solving the 
regression problem in nearly input-sparsity time.



\begin{corollary}[Fast  Regression]
  Given , with a constant probability, a
  -approximate solution to an  regression problem can be
  computed in
  
  time.
\end{corollary}





For completeness, we also present a result for low-distortion dense 
embeddings for  that the tail inequalities from 
Lemmas~\ref{lemma:stable_upper} and~\ref{lemma:stable_lower} enable us to 
construct.
See Appendix~\ref{sxn:pf-dense_lp} for a proof of the following theorem.

\begin{theorem}[Low-distortion Dense Embedding for ]
  \label{thm:dense_lp}
  Given  with full column rank and , let
   whose entries are i.i.d.\ samples from
  .
  If  for  sufficiently large, with a constant
  probability, we have
  
  In addition,  can be computed in 
   time.
\end{theorem}

\noindent
\textbf{Remark.}
The result in Theorem~\ref{thm:dense_lp} is based on a dense  subspace
embeddings that is analogous to the dense Gaussian embedding for  and
the dense Cauchy embedding of~\cite{SW11} for .
Although the running time (if one is simply interested in FLOP counts in RAM) of
Theorem~\ref{thm:dense_lp} is somewhat worse than that of
Theorem~\ref{thm:sparse_lp}, the embedding dimension and condition number
quality (the ratio of the upper bound on the distortion and the lower bound on
the distortion) are much better.
Our numerical implementations, both with the  norm~\cite{CDMMMW13_SODA}
and with the  norm~\cite{MSM11_TR}, strongly suggest that the latter
quantities are more important to control when implementing randomized regression
algorithms in large-scale parallel and distributed settings.


\section{Improving the Embedding Dimension}
\label{sxn:improve} 

In Theorem~\ref{thm:sparse_l1} and Theorem~\ref{thm:sparse_lp}, the embedding
dimension is , where the
 term is a somewhat large polynomial of  that directly multiplies
the  term.  
(See the remark below for comments on the precise value of the  term.)
This is not ideal for the subspace embedding and the  regression,
because we want to have a small embedding dimension and a small subsampled
problem, respectively.
Here, we show that it is possible to decouple the large polynomial of  and
the  term via another round of sampling and
conditioning without increasing the complexity.
See Algorithm~\ref{alg:dim} for details on this procedure.
Theorem~\ref{thm:improved-dim} provides our main quality-of-approximation result
for Algorithm~\ref{alg:dim}; its proof can be found in
Appendix~\ref{sxn:pf-imp-dim}.

\begin{algorithm}
  \caption{Improving the Embedding Dimension}
  \label{alg:dim}
  \begin{algorithmic}[1]
    \Require  with full column rank, , and
    .

    \Ensure A -distortion embedding  of .

    \State Compute a low-distortion embedding  of  (Theorems~\ref{thm:sparse_l1} and
    \ref{thm:sparse_lp}).
    
    \State Compute  from  such
    that  is well-conditioned (QR or
    Lemma~\ref{lemma:lp_cond_2d}).

    \State Compute a -distortion embedding  of  (Lemma~\ref{lemma:fast_sampling}).
    
    \State Compute  such that  (Theorem~\ref{lemma:lp_cond_2d}).

    \State Compute a -distortion embedding  of 
    (Lemma~\ref{lemma:fast_sampling}).
  \end{algorithmic}
\end{algorithm}

\begin{theorem}[Improving the Embedding Dimension]
  \label{thm:improved-dim}
  Given , with a constant probability, Algorithm~\ref{alg:dim}
  computes a -distortion embedding of  into
  ) in
   time.
\end{theorem}

\noindent
Then, by applying Theorem~\ref{thm:improved-dim} to the  regression
problem, we can improve the size of the subsampled problem and hence the overall
running time.

\begin{corollary}[Improved Fast  Regression]
  Given , with a constant probability, a
  -approximate solution to an  regression problem can be
  computed in
  
  time.
  The second term comes from solving a subsampled problem of size
  .
\end{corollary}

\noindent
\textbf{Remark.}
We have stated our results in the previous sections as  without
stating the value of the polynomial because there are numerous trade-offs
between the conditioning quality and the running time. 
For example, let .
We can use a rounding algorithm instead of QR to compute the  matrix. 
If we use the input-sparsity time embedding with the -rounding
algorithm of~\cite{CDMMMW13_SODA}, then the running time to compute the
-distortion embedding is  and the embedding dimension is 
(ignoring  factors). 
If, on the other hand, we use QR to compute , then the running time is
 and the embedding dimension is
. 
However, with the result from this section, the running time is simply
 and the  term can be absorbed by the  term. 


\section{Acknowledgments}

The authors want to thank Petros~Drineas for reading a preliminary version of
this paper and pointing out that the embedding dimension in
Theorem~\ref{thm:sparse_l2} can be easily improved from 
to  using the same technique.
The authors also want to thank Jelani~Nelson and Huy~Nguyen for letting us know
about their independent work on  embedding.

\bibliographystyle{plain}
\bibliography{sparse_embed}




\appendix
\section{Appendix}


\subsection{Proof of Theorem~\ref{thm:sparse_l2} (()-distortion Embedding for )}
\label{sec:proof_l2}

Let the  matrix  be an orthonormal basis for the range of the  matrix . 
Rather than proving the theorem by establishing that
 
holds for all , as is essentially done in, e.g., \cite{DMM06} and
\cite{CW12sparse_TR}, we note that , and we directly bound the extent
to which the embedding process perturbs this product.
To do so, define

That is,

where  is the -th element of ,  is the -th diagonal
element of , and  is the -th element of .
We will use the following facts in the proof:

We have,

and we also have

Given these results, it is easy to obtain that 

For any , set .
Then, by Markov's inequality,

Therefore, with probability at least , we have , which implies



\subsection{Proof of Theorem~\ref{thm:sparse_l1} (Low-distortion Embedding for )}
\label{sec:proof_l1}

We start with the following result, which establishes the existence of the
so-called Auerbach's basis of a -dimensional normed vector space.
For our proof, we will only need its existence and not an algorithm to construct
it.

\begin{lemma} 
  \label{lemma:auerbach}
  (Auerbach~\cite{auerbach1930area}) Let  be a -dimensional
  normed vector space.
  There exists a basis  of , called Auerbach basis, such
  that  and  for , where
   is a basis of  dual to .
\end{lemma}

\noindent
This Auerbach's lemma implies that a -conditioned basis matrix of
 exists, which will be denoted by  throughout the proof.
By definition, 's columns are unit vectors in the  norm 
(thus , where recall that  denotes the element-wise 
 norm of a matrix) 
and .
Denote by  the -th row of , .
Define  the  leverage scores of .
We have .
Let  to be determined later, and define two index sets  and .
It is easy to see that  where  is used to
denote the size of a finite set, and  where

Similarly, when an index set appears as a superscript, we mean zeroing out
elements or rows that do not belong to this index set, e.g.,  and .
Define

For any , we have ,

and thus .
Define  and .
Given , define a mapping  such
that , , and split  into two subsets:
 and .
Consider these events:
\begin{itemize}
\item :  for some .

\item :  for some .

\item : .    

\item : 
  for some .

\item : 
  for some .
\end{itemize}
Recall that we set  in Theorem~\ref{thm:sparse_l1}. 
We will show that, with  sufficiently large and proper choices of
, , , and , the event 
leads to an upper bound of  for all ,
 and  lead to a lower bound of  for
all  with probability at least , and ,
, and  together imply an lower bound of
 for all .

\begin{lemma}
  \label{lemma:upper}
  Provided , we have
  
\end{lemma}
\begin{proof}
  For any , we can find an  such that .
  Then,
  
\end{proof}
\begin{lemma}
  \label{lemma:L_1}
  Provided , for any fixed , we have
  
\end{lemma}

\begin{proof}
  Let . We have,
  
  where  are independent Cauchy variables.
  Let .
  Since , we have .
  By Lemma~\ref{lemma:cauchy_lower},
  
  By assumption  and , we obtain the result.
\end{proof}

\begin{lemma}
  \label{lemma:L_all}
  Assume both  and .
  If  and  satisfy
  
  for some  regardless of , then, with probability at
  least , we have
  
\end{lemma}
\begin{proof}
  Set  and create an -net
   such that for any , we can find a
   such that .
  Since  for all , there exist such an -net
  with at most  elements (Bourgain et al.~\cite{BLM89}).
  By Lemma~\ref{lemma:L_1}, we can apply a union bound for all the elements in
  :
  
  For any , we have, noting that ,
  
  So we establish a lower bound for all .
\end{proof}

\begin{lemma}
  \label{lemma:lower_H}
  Provided  and , if , we have
  
\end{lemma}
\begin{proof}
  For any , we have,
  
  which creates a lower bound for all .
\end{proof}

We continue to show that, with  sufficiently large, by setting  and choosing , , ,
and  properly, we have each event with probability at least  and thus

Moreover, the condition in Lemma~\ref{lemma:L_all} holds with ,
and the condition in Lemma~\ref{lemma:lower_H} holds.
Therefore,  has the desired property with probability at least ,
which would conclude the proof of Theorem~\ref{thm:sparse_l1}.

\begin{lemma}
  \label{lemma:E_U}
  With probability at least ,  holds with .
\end{lemma}
\begin{proof}
  With  fixed, we have,
  
  where  are \emph{dependent} Cauchy random variables. We have
  
  Apply Lemma~\ref{lemma:cauchy_upper},
  
  Setting  and , we have
  
  We assume that  and .
\end{proof}

\begin{lemma}
  \label{lemma:E_L}
  For any , if , we have,
  
\end{lemma}
\begin{proof}
  Let .
  We have , ,
  and .
  Fixed ,  are independent, .
  By Bernstein's inequality,
  
  where we use Holder's inequality: .
  To obtain a union bound for all  with probability , we need
  
  Given , it suffices to choose  and .
  Note that .
  We have
  
  Increasing  will decrease the failure rate, so it holds for all .
\end{proof}

\begin{lemma}
  With probability at least ,  holds with .
\end{lemma}
\begin{proof}
  By Lemma~\ref{lemma:E_L}, with probability at least , 
  holds with
  
\end{proof}

\begin{lemma}
  With the above choices of  and , the condition in
  Lemma~\ref{lemma:L_1} holds with  for sufficiently large
  .
\end{lemma}
\begin{proof}
  With , and , the first term in
  
  increases much slower than the second term as  increases, while both
  are at the order of .
  Therefore, if  is sufficiently large, the condition hold with .
\end{proof}

\begin{lemma}
  If , event  holds with probability at least
  .
\end{lemma}
\begin{proof}
  Given  and , let  if  and  otherwise.
  It is easy to see that .
  Therefore,
  
  It suffices if .
\end{proof}

\begin{lemma}
  With probability at least , event  holds with .
\end{lemma}
\begin{proof}
  Let  be a Cauchy variable. We have
  
   is at most . Then
  
  Therefore,  would suffice.
\end{proof}

\begin{lemma}
  With probability at least , event  holds with
  .
  Thus with  sufficiently large and the above choice of , the
  condition in Lemma~\ref{lemma:lower_H}  holds.
\end{lemma}
\begin{proof}
  We have,
  
  By Markov's inequality,
  
  Assume that .
  Similar to the proof of Lemma~\ref{lemma:E_U}, we have
  
  where  are \emph{dependent} Cauchy variables.
  Apply Lemma~\ref{lemma:cauchy_upper},
  
  It suffices to choose  to make the RHS less
  than .
  So with probability at least , we have  holds with
  .
\end{proof}


\subsection{Proof of Corollary~\ref{cor:l1reg} (Fast  Regression)}
\label{sxn:pf-cor-l1reg}

By Theorem~\ref{thm:sparse_l1} and Lemma~\ref{lemma:fast_sampling}, we know that
Steps 2 and 4 of Algorithm \ref{alg:fast_l1_reg} succeed with a constant
probability. 
Conditioning on this event, we have

where the last inequality is due to .
By Theorem~\ref{thm:sparse_l1}, Step 2 takes  time, and Step 3
takes  time because  has  rows.
Then, by Lemma~\ref{lemma:fast_sampling}, Step 4 takes  time, and Step 5 takes  time.
Therefore, the total running time of Algorithm~\ref{alg:fast_l1_reg} is as
stated.



\subsection{Proof of Lemma~\ref{lemma:equiv}}
\label{sxn:pf-equiv}

First, we know that

Next, we state the following lemma, which is due to
Nolan~\cite{nolan2012stable}.
\begin{lemma}
  \label{lemma:tail}
  (Nolan~\cite[Thm.~1.12]{nolan2012stable}) Let  with .
  Then as ,
  
  where .
\end{lemma}
\noindent
By Lemma~\ref{lemma:tail}, it follows that, as ,

For the Cauchy distribution, we have

Hence, there exist  and  such that for all ,

Note that all the -stable distributions with  have finite and
positive density at .
Therefore, there exists  such that for all ,

Let .
We get .
For the Gaussian distribution, we have, as ,

which converges to zero much faster than , so we can apply similar
arguments to obtain .



\subsection{Proof of Lemma~\ref{lemma:stable_upper} (Upper Tail Inequality for -stable Distributions)}
\label{sxn:pf-lp-stable-upper}

Let , , where  is the CDF of the
standard Cauchy distribution and  is the CDF of .
 follows the standard Cauchy distribution, and, by Lemma~\ref{lemma:equiv},
we have .
Therefore, for any ,

The last inequality is from Lemma~\ref{lemma:cauchy_upper}.



\subsection{Proof of Lemma~\ref{lemma:stable_lower} (Lower Tail Inequality for -stable Distributions)}
\label{sxn:pf-lp-stable-lower}

Let  be independent random variables sampled from the standard Gaussian
distribution, .
By Lemma~\ref{lemma:equiv}, we have

The lower tail inequality from Lemma~\ref{lemma:gaussian_lower} concludes the
proof.



\subsection{Proof of Theorem~\ref{thm:dense_lp} (Low-distortion Dense Embedding for
  )}
\label{sxn:pf-dense_lp}

The proof is similar to the proof of Sohler and Woodruff~\cite[Theorem~5]{SW11},
except that the Cauchy tail inequalities are replaced by tail inequalities for
the stable distributions.
For simplicity, we omit the complete proof but show where to apply those tail
inequalities.
By Lemma~\ref{lemma:auerbach}, there exists a -conditioned
basis matrix of , denoted by .
Thus, , where recall that  denotes the element-wise 
 norm of a matrix.
We have,

where .
Applying Lemma~\ref{lemma:stable_upper}, we get  with a constant probability.
Define .
For any fixed , we have

where .
Applying Lemma~\ref{lemma:stable_upper}, we get  with an exponentially small probability with respect to .
By choosing  with  sufficiently large and an
-net argument on , we can obtain a union lower bound of  on all the elements of  with a constant probability.
Then,

which gives us the desired result.


\subsection{Proof of Theorem~\ref{thm:improved-dim} (Improving the Embedding Dimension)}
\label{sxn:pf-imp-dim}

Each of Steps 1, 3, and 5 of Algorithm~\ref{alg:dim} succeeds with a constant
probability.
We can control the success rate of each by adjusting the constant factor in the
embedding dimension, such that all steps succeed with a constant probability.
Conditioning on this event, we have  because

By Lemma~\ref{lemma:kappa_equiv}, , and
then by Lemma~\ref{lemma:fast_sampling}, the embedding dimension of  is
.



\end{document}
