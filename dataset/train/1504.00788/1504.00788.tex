\documentclass[10pt,conference,letterpaper]{IEEEtran}
\usepackage{times,amsmath,epsfig,amssymb}

\usepackage{graphicx}
\usepackage{balance}  \usepackage{type1cm}     \usepackage{algorithm}     \usepackage{algorithmic}     \usepackage{xspace}     \usepackage{booktabs}     \usepackage[bf,tableposition=top]{caption}     \usepackage{siunitx}          \usepackage[hyphens]{url}     \DeclareGraphicsExtensions{.pdf,.png,.jpg,.eps}
\graphicspath{{./img/}}

\usepackage[square,numbers]{natbib}     \setlength{\bibsep}{1pt}
\renewcommand{\bibfont}{\small\raggedright}
\def\newblock{}

\newcommand{\spara}[1]{\smallskip\noindent\textbf{#1}}
\newcommand{\mpara}[1]{\medskip\noindent\textbf{#1}}
\newcommand{\para}[1]{\noindent\textbf{#1}}

\newenvironment {squishlist}
{\begin{list}{}
  { \setlength{\itemsep}{1pt}
     \setlength{\parsep}{1pt}
     \setlength{\topsep}{1pt}
     \setlength{\partopsep}{1pt}
     \setlength{\leftmargin}{1.5em}
     \setlength{\labelwidth}{1em}
     \setlength{\labelsep}{0.5em} } }
{\end{list}}

\newcommand{\load}[1]{\ensuremath{L_{w}^{t}(#1)}\xspace}
\newcommand{\workload}[1]{\ensuremath{W_{w}^{t}(#1)}\xspace}
\newcommand{\totalworkload}{\ensuremath{W_{w}^{t}}\xspace}

\newcommand{\tuple}[1]{\ensuremath{\langle #1 \rangle}\xspace}
\newcommand{\hash}[1]{\ensuremath{\mathcal{H}_{#1}}\xspace}
\newcommand{\pei}{\textsc{pei}\xspace}
\newcommand{\peis}{{\pei}s\xspace}
\newcommand{\pe}{\textsc{pe}\xspace}
\newcommand{\pes}{{\pe}s\xspace}
\newcommand{\potc}{\textsc{p\textup{o}tc}\xspace}
\newcommand{\dspe}{\textsc{dspe}\xspace}
\newcommand{\dspes}{{\dspe}s\xspace}
\newcommand{\dagr}{\textsc{dag}\xspace}
\newcommand{\dagrs}{\textsc{dag}s\xspace}
\newcommand{\pkg}{\textsc{Partial Key Grouping}\xspace}
\newcommand{\pkgs}{\textsc{pkg}\xspace}
\newcommand{\kg}{\textsc{kg}\xspace}
\newcommand{\sg}{\textsc{sg}\xspace}
\newcommand{\sources}{\ensuremath{\mathcal{S}}\xspace}
\newcommand{\numsources}{S\xspace}
\newcommand{\workers}{\ensuremath{\mathcal{W}}\xspace}
\newcommand{\numworkers}{W\xspace}
\newcommand{\keyspace}{\ensuremath{\mathcal{K}}\xspace}
\newcommand{\keysize}{K\xspace}
\newcommand{\mycomment}[1]{}

\DeclareMathOperator*{\expect}{\mathbb{E}}
\DeclareMathOperator*{\avg}{avg}
\DeclareMathOperator*{\argmin}{argmin}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{question}{Question}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{obs}{Observation}[section]
\newtheorem{claim}{Claim}[section]


\title{The Power of Both Choices: Practical Load Balancing~for~Distributed~Stream~Processing~Engines}
\author{{Muhammad Anis Uddin Nasir{\small },
Gianmarco De Francisci Morales{\small },
David Garc\'ia-Soriano{\small }}\\
{Nicolas Kourtellis{\small },
Marco Serafini{\small 5}^{\#}^{*}^{\Qatar Computing Research Institute, Doha, Qatar\\
\fontsize{9}{9}\selectfont\ttfamily\upshape
anisu@kth.se,
gdfm@apache.org,
davidgs@yahoo-inc.com\\
kourtell@yahoo-inc.com,
mserafini@qf.org.qa
}
\begin{document}
\maketitle
\begin{abstract}


We study the problem of load balancing in distributed stream processing engines, which is exacerbated in the presence of skew.
We introduce \pkg (\pkgs), a new stream partitioning scheme that adapts the classical ``power of two choices'' to a distributed streaming setting by leveraging two novel techniques: \emph{key splitting} and \emph{local load estimation}.
In so doing, it achieves better load balancing than key grouping while being more scalable than shuffle grouping.



We test \pkgs on several large datasets, both real-world and synthetic.
Compared to standard hashing, \pkgs reduces the load imbalance by up to several orders of magnitude, and often achieves nearly-perfect load balance.
This result translates into an improvement of up to 60\% in throughput and up to 45\% in latency when deployed on a real Storm cluster.
\end{abstract}

\section{Introduction}
\label{sec:intro}









Distributed stream processing engines (\dspes) such as S4,\footnote{\url{https://incubator.apache.org/s4}} Storm,\footnote{\url{https://storm.incubator.apache.org}} and Samza\footnote{\url{https://samza.incubator.apache.org}} have recently gained much attention owing to their ability to process huge volumes of data with very low latency on clusters of commodity hardware.
Streaming applications are represented by directed acyclic graphs (\dagr) where vertices, called \emph{processing elements} (\pes), represent operators, and edges, called \emph{streams}, represent the data flow from one \pe to the next.
For scalability, streams are partitioned into sub-streams and processed in parallel on a replica of the \pe called \emph{processing element instance} (\pei).




Applications of \dspes, especially in data mining and machine learning, typically require accumulating state across the stream by grouping the data on common fields \cite{ben-haim2010spdt,berinde2010heavyhitters}.
Akin to MapReduce, this grouping in \dspes is usually implemented by partitioning the stream on a \emph{key} and ensuring that messages with the same key are processed by the same \pei.
This partitioning scheme is called \emph{key grouping}.
Typically, it maps keys to sub-streams by using a hash function.
Hash-based routing allows each source \pei to route each message solely via its key,  without needing to keep any state or to coordinate among \peis.
Alas, it also results in load imbalance as it represents a ``single-choice'' paradigm~\citep{one_choice_load}, and because it disregards the popularity of a key, i.e., the number of messages with the same key in the stream, as depicted in Figure~\ref{fig:imbalance}.

Large web companies run massive deployments of \dspes in production.
Given their scale, good utilization of the resources is critical.
However, the skewed distribution of many workloads causes a few \peis to sustain a significantly higher load than others. This suboptimal load balancing leads to poor resource utilization and inefficiency.



Another partitioning scheme called \emph{shuffle grouping} achieves excellent load balancing by using a round-robin routing, i.e., by sending a message to a new \pei in cyclic order, irrespective of its key.
However, this scheme is mostly suited for stateless computations.
Shuffle grouping may require an additional aggregation phase and more memory to express stateful computations (Section~\ref{sec:preliminaries}).
Additionally, it may cause a decrease in accuracy for data mining algorithms (Section~\ref{sec:applications}).


\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.3]{imbalance}
\caption{Load imbalance generated by skew in the key distribution when using key grouping. The color of each message represents its key.}
\label{fig:imbalance}
\end{center}
\end{figure}



In this work, we focus on the problem of load balancing of stateful applications in \dspes when the input stream follows a skewed key distribution.
In this setting, load balancing is attained by having upstream \peis create a balanced partition of messages for  downstream \peis, for each edge of the \dagr.
Any practical solution for this task needs to be both \emph{streaming} and \emph{distributed}: the former constraint enforces the use of an online algorithm, as the distribution of keys is not known in advance, while the latter calls for a decentralized solution with minimal coordination overhead in order to ensure scalability.

\clearpage

To address this problem, we leverage the ``power of two choices''~\citep{mitzenmacher2001power} (\potc), whereby the system picks the least loaded out of two candidate \peis for each key.
However, to maintain the semantics of key grouping while using \potc (i.e., so that one key is handled by a single \pei), sources would need to track which of the two possible choices has been made for each key.
This requirement imposes a coordination overhead every time a new key appears, so that all sources agree on the choice.
In addition, sources should then store this choice in a routing table. 
Each edge in the \dagr would thus require a routing table for every source, each with one entry per key.
Given that a typical stream may contain billions of keys, this solution is not practical.

Instead, we propose to relax the key grouping constraint and allow each key to be handled by \emph{both} candidate \peis.
We call this technique \emph{key splitting}; it allows us to apply \potc without the need to agree on, or keep track of, the choices made.
As shown in Section~\ref{sec:evaluation}, key splitting guarantees good load balance even in the presence of skew.

A second issue is how to estimate the load of a downstream \pei.
Traditional work on \potc assumes global knowledge of the current load of each server, which is challenging in a distributed system.
Additionally, it assumes that all messages originate from a single source, whereas messages in a \dspe are generated in parallel by multiple sources.


In this paper we prove that, interestingly, a simple \emph{local load estimation} technique, whereby each source independently tracks the load of downstream \peis, performs very well in practice.
This technique gives results that are almost indistinguishable from those given by a global load oracle.


The combination of these two techniques (key splitting and local load estimation) enables a new stream partitioning scheme named \emph{\pkg}.




In summary, we make the following contributions.
\begin{squishlist}
\item We study the problem of load balancing in modern distributed stream processing engines.
\item We show how to apply \potc to \dspes in a principled and practical way, and propose two novel techniques to do so: key splitting and local load estimation.
\item We propose \pkg, a novel and simple stream partitioning scheme that applies to any \dspe.
When implemented on top of Apache Storm, it requires a single function and less than 20 lines of code.\footnote{Available at \url{https://github.com/gdfm/partial-key-grouping}}
\item We measure the impact of \pkgs on a real deployment on Apache Storm.
Compared to key grouping, it improves the throughput of an example application on real-world datasets by up to , and the latency by up to .
\end{squishlist}
 \section{Preliminaries and Motivation}
\label{sec:preliminaries}





We consider a \dspe running on a cluster of machines that communicate by exchanging messages following the flow of a \dagr, as discussed.
In this work, we focus on balancing the data transmission along a single edge in a \dagr.
Load balancing across the whole \dagr is achieved by balancing along each edge independently.
Each edge represents a single stream of data, along with its partitioning scheme.
Given a stream under consideration, let the set of upstream \peis (sources) be \sources, and the set of downstream \peis (workers) be \workers, and their sizes be  and  (see Figure~\ref{fig:imbalance}).


The input to the engine is a sequence of messages  where  is the timestamp at which the message is received,  is the message key, and  is the value.
The messages are presented to the engine in ascending order by timestamp.


A {\em stream partitioning} function  maps each key in the key space to a natural number, at a given time .
This number identifies the worker responsible for processing the message.
Each worker is associated to one or more keys.


We use a definition of \emph{load} similar to others in the literature (e.g., Flux~\citep{shah2003flux}).
At time , the load of a worker  is the number of messages handled by the worker up to :



In principle, depending on the application, two different messages might impose a different load on workers.
However, in most cases these differences even out and modeling such application-specific differences is not necessary.

We define {\em imbalance} at time  as the difference between the maximum and the average load of the workers:


We tackle the problem of identifying a stream partitioning function that minimizes the imbalance,
while at the same time avoiding the downsides of shuffle grouping.






\subsection{Existing Stream Partitioning Functions}
\label{sec:existing-partitioning}
Data is sent between \pes by exchanging messages over the network.
Several primitives are offered by \dspes for sources to partition the stream, i.e., to route messages to different workers.
There are two main primitives of interest: \emph{key grouping} (\kg) and \emph{shuffle grouping} (\sg).


\kg ensures that messages with the same key are handled by the same \pei (analogous to MapReduce).
It is usually implemented through hashing. 

\sg routes messages independently, typically in a round-robin fashion.
\sg provides excellent load balance by assigning an almost equal number of messages to each \pei.
However, no guarantee is made on the partitioning of the key space, as each occurrence of a key can be assigned to any \peis.
\sg is the perfect choice for \emph{stateless} operators.
However, with \emph{stateful} operators one has to handle, store and aggregate multiple partial results for the same key, thus incurring additional costs. 




In general, when the distribution of input keys is skewed, the number of messages that each \pei needs to handle can vary greatly.
While this problem is not present for stateless operators, which can use \sg to evenly distribute messages, stateful operators implemented via \kg suffer from load imbalance.
This issue generates a degradation of the service level, or reduces the utilization of the cluster which must be provisioned to handle the peak load of the single most loaded server.




\spara{Example.}
To make the discussion more concrete, we introduce a simple application that will be our running example: \emph{streaming top-k word count}.
This application is an adaptation of the classical MapReduce word count to the streaming paradigm where we want to generate a list of top-k words by frequency at periodic intervals (e.g., each  seconds).
It is also a common application in many domains, for example to identify trending topics in a stream of tweets.

\spara{Implementation via key grouping.} Following the MapReduce paradigm, the implementation of word count described by~\citet{neumeyer2010s4} or~\citet{noll2013rolling} uses \kg on the source stream.
The counter \pe keeps a running counter for each word.
\kg ensures that each word is handled by a single \pei, which thus has the total count for the word in the stream.
At periodic intervals, the counter \peis send their top-k counters to a single downstream aggregator to compute the top-k words.
While this application is clearly simplistic, it models quite well a general class of applications common in data mining and machine learning whose goal is to create a model by tracking aggregated statistics of the data.


Clearly \kg generates load imbalance as, for instance, the \pei associated to the key ``the" will receive many more messages than the one associated with ``Barcelona".
This example captures the core of the problem we tackle: the distribution of word frequencies follows a Zipf law where few words are extremely common while a large majority are rare.
Therefore, an even distribution of keys such as the one generated by \kg results in an uneven distribution of messages.

\spara{Implementation via shuffle grouping.} An alternative implementation uses shuffle grouping on the source stream to get partial word counts.
These counts are sent downstream to an aggregator every  seconds via key grouping.
The aggregator simply combines the counts for each key to get the total count and selects the top-k for the final result.


Using \sg requires a slightly more complex logic but it generates an even distribution of messages among the counter \peis.
However, it suffers from other problems.
Given that there is no guarantee which \pei will handle a key, each \pei potentially needs to keep a counter for \emph{every} key in the stream.
Therefore, the memory usage of the application grows linearly with the parallelism level.
Hence, it is not possible to scale to a larger workload by adding more machines: the application is not scalable in terms of memory.
Even if we resort to approximation algorithms, in general, the error depends on the number of aggregations performed, thus it grows linearly with the parallelism level.
We analyze this case in further detail along with other application scenarios in Section~\ref{sec:applications}.



\subsection{Key grouping with rebalancing}
One common solution for load balancing in \dspes\ is operator migration~\citep{shah2003flux,cherniack2003scalable,xing2005dynamic,gedik2013partitioning,balkesen2013adaptive,castro2013integrating}.
Once a situation of load imbalance is detected, the system activates a rebalancing routine that moves part of the keys, and the state associated with them, away from an overloaded server.
While this solution is easy to understand, its application in our context is not straightforward for several reasons.



Rebalancing requires setting a number of parameters such as how often to check for imbalance and how often to rebalance.
These parameters are often application-specific as they involve a trade-off between imbalance and rebalancing cost that depends on the size of the state to migrate. 

Further, implementing a rebalancing mechanism usually requires major modifications of the \dspe at hand.
This task may be hard, and is usually seen with suspicion by the community driving open source projects, as witnessed by the many variants of Hadoop that were never merged back into the main line of development~\citep{abouzeid2009hadoopdb,Dittrich2010hadooppp,yang2007mapreducemerge}.

In our context, rebalancing implies migrating keys from one sub-stream to another.
However, this migration is not directly supported by the programming abstractions of some \dspes.
Storm and Samza use a \emph{coarse-grained} stream partitioning paradigm.
Each stream is partitioned into as many sub-streams as the number of downstream \peis.
Key migration is not compatible with this partitioning paradigm, as a key cannot be uncoupled from its sub-stream. 
In contrast, S4 employs a \emph{fine-grained} paradigm where the stream is partitioned into one sub-stream per key value, and there is a one-to-one mapping of a key to a \pei.
The latter paradigm easily supports migration, as each key is processed independently.




A major problem with mapping keys to \peis explicitly is that the \dspe must maintain several routing tables: one for each stream. Each routing table has one entry for each key in the stream.
Keeping these tables is impractical because the memory requirements are staggering.
In a typical web mining application, each routing table can easily have billions of keys.
For a moderately large \dagr with tens of edges, each with tens of sources, the memory overhead easily becomes prohibitive.

Finally, as already mentioned, for each stream there are several sources sending messages in parallel.
Modifications to the routing table must be consistent across all sources, so they require coordination, which creates further overhead.
For these reasons we consider an alternative approach to load balancing. 






 \section{Partial Key Grouping}
\label{sec:algos}




The problem described so far currently lacks a satisfying solution.
To solve this issue, we resort to a widely-used technique in the literature of load balancing: the so-called \emph{``power of two choices''} (\potc).
While this technique is well-known and has been analyzed thoroughly both from a theoretical and practical perspective~\citep{adler1995parallelrandomized,azar1999balanced-allocations,byers2003geometricgeneralizations,lenzen2011parallelrandomized,mitzenmacher2001power,mitzenmacher2001potc-survey}, its application in the context of \dspes is not straightforward and has not been previously studied.





Introduced by \citet{azar1999balanced-allocations}, \potc is a simple and elegant technique that allows to achieve load balance when assigning units of load to workers.
It is best described in terms of ``balls and bins''.
Imagine a process where a stream of balls (units of work) is distributed to a set of bins (the workers) as evenly as possible.
The \emph{single-choice paradigm} corresponds to putting each ball into one bin selected uniformly at random.
By contrast, the power of two choices selects two bins uniformly at random, and puts the ball into the least loaded one.
This simple modification of the algorithm has powerful implications that are well known in the literature (see Sections~\ref{sec:theory},~\ref{sec:rel-work}). 









\spara{Single choice.}
The current solution used by all \dspes to partition a stream with key grouping corresponds to the single-choice paradigm.
The system has access to a single hash function .
The partitioning of keys into sub-streams is determined by the function ,
where  is the modulo operator.

The single-choice paradigm is attractive because of its simplicity: the routing does not require to maintain any state and can be done independently in parallel.
However, it suffers from a problem of load imbalance~\citep{mitzenmacher2001power}.
This problem is exacerbated when the distribution of input keys is skewed.


\spara{PoTC.}
When using the power of two choices, we have two hash functions  and .
The algorithm maps each key to the sub-stream assigned to the least loaded worker between the two possible choices, that is:
~=~.


The theoretical gain in load balance with two choices is exponential compared to a single choice.
However, using more than two choices only brings constant factor improvements~\citep{azar1999balanced-allocations}.
Therefore, we restrict our study to two choices.

\potc introduces two additional complications.
First, to maintain the semantics of key grouping, the system needs to \emph{keep state} and track the choices made.
Second, the system has to \emph{know the load} of the workers in order to make the right choice.
We discuss these two issues next.











\subsection{Key Splitting}

A na\"{i}ve application of \potc to key grouping requires the system to store a bit of information for each key seen, to keep track of which of the two choices needs to be used thereafter.
This variant is referred to as \emph{static} \potc.

Static \potc incurs some of the problems discussed for key grouping with rebalancing.
Since the actual worker to which a key is routed is determined dynamically, sources need to keep a routing table with an entry per key.
As already discussed, maintaining this routing table is often impractical.

In order to leverage \potc and make it viable for \dspes, we relax the requirement of key grouping.
Rather than mapping each key to one of the two possible choices, we allow it to be mapped to \emph{both choices}.
Every time a source sends a message, it selects the worker with the lowest current load among the two candidates associated to that key.
This technique, called \emph{key splitting}, introduces several new trade-offs.

First, key splitting allows the system to operate in a decentralized manner, by allowing multiple sources to take decisions independently in parallel.
As in key grouping and shuffle grouping, no state needs to be kept by the system and each message can be routed independently.

Key splitting enables far better load balancing compared to key grouping.
It allows using \potc to balance the load on the workers:
by splitting each key on multiple workers, it handles the skew in the key popularity.
Moreover, given that \emph{all} its decisions are dynamic and based on the current load of the system (as opposed to static \potc), key splitting adapts to changes in the popularity of keys over time.

Third, key splitting reduces the memory usage and aggregation overhead compared to shuffle grouping.
Given that each key is assigned to \emph{exactly two} \peis, the memory to store its state is just a constant factor higher than when using key grouping.
Instead, with shuffle grouping the memory grows linearly with the number of workers .
Additionally, state aggregation needs to happen only once for the two partial states, as opposed to  times in shuffle grouping.
This improvement also allows to reduce the error incurred during aggregation for some algorithms, as discussed in Section~\ref{sec:applications}.



From the point of view of the application developer, key splitting gives rise to  a novel stream partitioning scheme called \pkg, which lies in-between key grouping and shuffle grouping.


Naturally, not all algorithms can be expressed via \pkgs.
The functions that can leverage \pkgs are the same ones that can leverage a combiner in MapReduce, i.e., associative functions and monoids.
Examples of applications include na\"{i}ve Bayes, heavy hitters, and streaming parallel decision trees, as detailed in Section~\ref{sec:applications}.
On the contrary, other functions such as computing the median cannot be easily expressed via \pkgs.




\spara{Example.}
Let us examine the streaming top-k word count example using \pkgs.
In this case, each word is tracked by two counters on two different \peis. Each counter holds a partial count for the word, while the total count is the sum of the two partial counts.
Therefore, the total memory usage is , i.e., .
Compare this result to \sg where the memory is .
Partial counts are sent downstream to an aggregator that computes the final result.
For each word, the application sends two counters, and the aggregator performs a constant time aggregation.
The total work for the aggregation is .
Conversely, with \sg the total work is again .
Compared to \kg, the implementation with \pkgs requires additional logic, some more memory and has some aggregation overhead.
However, it also provides a much better load balance which maximizes the resource utilization of the cluster.
The experiments in Section~\ref{sec:evaluation} prove that the benefits outweigh its cost.







	



\subsection{Local Load Estimation}
\potc requires knowledge of the load of each worker to take its routing decision.
A \dspe is a distributed system, and, in general, sources and workers are deployed on different machines.
Therefore, the load of each worker is not readily available to each source.



Interestingly, we prove that no communication between sources and workers is needed to effectively apply \potc.
We propose a \emph{local load estimation} technique, whereby each source independently maintains a local load-estimate vector with one element per worker.
The load estimates are updated by using only local information of the portion of stream sent by each source.
We argue that in order to achieve global load balance it is sufficient that each source independently balances the load it generates across all workers.

The correctness of local load estimation directly follows from our standard definition of load in Section~\ref{sec:preliminaries}.
The load on a worker  is simply the sum of the loads that each source  imposes on the given worker:

Each source  can keep an estimate of the load on each worker  based on the load it has generated . As long as each source keeps its own portion of load balanced, then the overall load on the workers will also be balanced.
Indeed, the maximum overall load is at most the sum of the maximum load that each source sees locally. 
It follows that the maximum imbalance is also at most the sum of the local imbalances. 



 \section{Analysis}
\label{sec:theory}

\newcommand{\naturals}{{\mathbb N}}
\DeclareRobustCommand{\calA}[0]{{\mathcal A}}
\DeclareRobustCommand{\calP}[0]{{\mathcal P}}
\DeclareRobustCommand{\calQ}[0]{{\mathcal Q}}
\DeclareRobustCommand{\calD}[0]{{\mathcal D}}
\DeclareRobustCommand{\calS}[0]{{\workers}}
\DeclareRobustCommand{\calK}[0]{{\keyspace}}
\newcommand{\code}[1]{{\textsc #1}}
\newcommand{\indic}{\mathbb{I}\,}




We proceed to analyze the conditions under which \pkgs achieves good load balance.
Recall from Section~\ref{sec:preliminaries} that we have a set  of  workers at our disposal and receive a sequence of  messages   with values from a key universe . Upon receiving the -th message with value , we need to decide its placement among the workers; decisions are irrevocable. We assume one message arrives per unit of time.
Our goal is to minimize the eventual maximum load , which is the same as minimizing the imbalance .
A simple placement scheme such as shuffle grouping provides an imbalance of at most one, but
we would like to limit the number of workers processing each key to .


\spara{Chromatic balls and bins.}
We model our problem in the framework of balls and bins processes, where keys correspond to colors, messages to colored balls, and workers to bins.
Choose
  independent hash
functions  uniformly at random. Define the \code{greedy-} scheme as follows:
at time , the -th ball (whose color is ) is placed on the bin with minimum current load among
, i.e., .
Recall that with key splitting there is no need to remember the choice for the next time a ball of the same color appears.


Observe that when ,
each ball color is assigned to a unique bin so no choice has to be made; this models hash-based key grouping.
At the other extreme, when , all   bins are valid choices, and we obtain shuffle
grouping.

\spara{Key distribution.}
Finally, we assume the existence of an underlying discrete distribution  supported on  from which ball colors
are drawn, i.e.,  is a sequence of  independent samples from . 
Without loss of generality, we identify the set  of keys with
 or, if  is finite of cardinality , with .
We assume them ordered by decreasing probability: if  is the probability of drawing key  from , then
 and . We also identify the set  of bins with~.


\subsection{Imbalance with \pkg}

\spara{Comparison with standard problems.}
As long as we keep getting balls of different colors, our process is identical to the standard \code{greedy-} process of \citet{azar1999balanced-allocations}.
This occurs with high probability provided that  is small enough.
But for sufficiently large  (e.g., when ), repeated keys will start to arrive.
Recall that for any number of choices , the maximum imbalance after throwing  balls \emph{of different colors} into  bins with the standard \code{greedy-} process is . Unfortunately, such strong bounds (independent of ) cannot apply to our setting.
To gain some intuition on what may go wrong, consider the following examples where .

Note that for the maximum load not to be much larger than the average load, the number of bins used must not exceed , where  is the maximum key probability.
Indeed, at any time we expect
the two bins  to contain together at
least a  fraction of all balls, just counting the occurrences of a single key. Hence the expected maximum load among the two grows at a rate
of at least  per unit of
time, while the overall average load increases by exactly  per unit of time. Thus, if , the expected imbalance at time  will be lower bounded by
, which grows \emph{linearly} with . This holds irrespective of the placement scheme used.




However, requiring  is not enough to prevent imbalance . Consider the uniform distribution
over  keys. Let  be the set of all bins that belong to one of the potential choices for some key. 
As is well-known, the expected size of  is . So all  keys use only an  fraction of all
bins, and roughly  bins will remain unused. In fact the imbalance after  balls
will be at least .
The problem is that most concrete instantiations of our two random hash functions cause the existence of
an ``overpopulated'' set  of bins inside which
the average bin load must grow faster than the average load across all bins.
(In fact, this case subsumes our first example above, where  was .)


Finally, even in the absence of overpopulated bin subsets, some inherent imbalance
is     due to deviations between the empirical and true key distributions.
For instance, suppose there are two
keys  with equal probability  and  bins. With constant probability, key 1 is assigned to bins 
and key 2 to bins . This situation looks perfect because the \code{greedy-2} choice will send each
occurrence of key 1 to bins  alternately so the loads of bins  will always equal up to .
However, the number of balls with key 1 seen is likely to deviate from  by roughly , so either the top two or the bottom two bins will receive
 balls, and the imbalance will be  with constant probability.

In the remainder of this section we carry out our analysis, which broadly construed asserts that the above are the only impediments to achieve good balance. 

\spara{Statement of results.}
We  noted  that once the number of bins exceeds  (where  is the maximum key
frequency), the maximum load will be dominated by the loads of the bins to which the most frequent key is mapped. 
Hence the main case of interest is where .



We focus on the case where the number of balls is large compared to the number of bins. 
The following results show that partial key grouping can significantly reduce the maximum load (and the imbalance), compared to key
grouping.
\begin{theorem}\label{thm:main}
Suppose we use  bins and let .
Assume a key distribution  with maximum probability .
Then
the imbalance after  steps of the \code{Greedy-} process satisfies, with probability at least ,

\mycomment{
the following hold:
\begin{itemize}
\item For any , the imbalance after  steps of the \code{Greedy-} process satisfies, with probability at least ,

Moreover, with probability at least  over the choice of the  hash functions, any load balancing policy using the same set of
choices for each key must have imbalance
.
\item For some distributions ,
the imbalance after  steps of the \code{Greedy-1} process satisfies, with probability at least ,

\end{itemize}
}
\end{theorem}

As the next result shows, the bounds above are best-possible.\footnote{\scriptsize However, the imbalance can be much smaller than the worst-case bounds from Theorem~\ref{thm:main} if the probability of most keys is much smaller than , which is the case in many setups. }
\begin{theorem}\label{thm:lb}
There is a distribution  satisfying the hypothesis of Theorem~\ref{thm:main} such that
the imbalance after  steps of the \code{Greedy-} process satisfies, with probability at least ,

\end{theorem}

We omit the proof of Theorem~\ref{thm:lb} (it follows by considering a uniform distribution over  keys).
The next section is devoted to the proof of the upper bound, Theorem~\ref{thm:main}. 



\subsection{Proof}
\mycomment{
First define .



\begin{theorem}
Conditioned on , with high probability after  throws the normalized imbalance after running the \code{Greedy-} process satisfies

Also, the optimal normalized imbalance after the hash functions are chosen satisfies

\end{theorem}

\begin{theorem}
With high probability,
     
and
for ,
        
\end{theorem}

In particular, when , the maximum load will be  whether we use one choice or  choices.
On the other hand,
From these results we see that when , the imbalance with  or
In the typical case we consider where ,

}



\spara{Concentration inequalities.}
We recall the following results, which we need to prove our main theorem.
\begin{theorem}[Chernoff bounds]\label{chernoff}
Suppose  is a finite sequence of independent random variables with  and let , .
Then for all ,

where
\end{theorem}


\begin{theorem}[McDiarmid's inequality]\label{bounded_dif}
Let  be a vector of independent random variables and let  be a function satisfying
    
whenever the vectors  and  differ in just one coordinate. Then

\end{theorem}

\spara{The  measure of bin subsets.}
For every nonempty set of bins  and , define

We will be  interested in  (which measures the probability that a random key from  will have its choice inside ) and 
(which measures the probability that a random key from  will have all its choices inside ).
Note that  and .

\begin{lemma}\label{lem:mu1}
For every ,
     
and, if ,
       
\end{lemma}
\begin{proof}
The first claim follows from linearity of expectation and the fact that .
For the second, let . Using Theorem~\ref{chernoff},
 is at most
    

since .
\end{proof}

\mycomment{
    \begin{corollary}
    With high probability, for all  it holds that
    
    \end{corollary}
    \begin{proof}
    Since , it suffices to show the claim when .
    This follows from Lemma~\ref{lem:mu1} by setting , , and applying the union bound.
    \end{proof}
}

\begin{lemma}\label{lem:mu2}
For every ,
     
and, provided that ,
       
\end{lemma}
\begin{proof}
Again the first claim is easy. For the second, let . Using Theorem~\ref{chernoff},
 is at most
    
    
since .
\end{proof}

\begin{corollary}\label{coro:mu2}
Assume , .  Then, with high probability, 
       
\end{corollary}
\begin{proof}
We use Lemma~\ref{lem:mu1} and the union bound. The probability that the claim fails to hold is bounded by
                                                                  
where we used  , valid for all .
\end{proof}


For a scheduling algorithm  and a set  of bins, write  for the maximum load among the bins in  after  balls have been
processed by .
\begin{lemma}\label{lem:perfect}
Suppose there is a set  of bins such that for all ,
        
Then d satisfies  with high probability.
\end{lemma}
\begin{proof}
We use a coupling argument. Consider the following two independent processes  and :  proceeds as
\code{Greedy-}, while  picks the bin for each ball independently at
random from  and increases its load. Consider any time~ at which the load vector is 
and  is the set of bins with
maximum load. After handling the -th ball, let  denote the event that  increases the maximum load in  because the new ball has all choices in
, and  denote the event that  increases the maximum load in . Finally, let  denote the event
that  increases the maximum load in  because the new ball has some choice in  and some choice in , but the load of one of  its choices in  is no larger. We identify these events with their indicator
random variables.

Note that the maximum load in  at the end of Process  is , while  at the
end of Process  is . Conditioned on any load vector , the probability of  is 
So , which implies that for any ,  But with high probability, the maximum load of Process  is , so 
holds with at least the same probability. On the other hand,  because each occurrence of
 increases the maximum load on , and once a time  is reached such that , event  must cease to happen. Therefore
,
yielding the result.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:main}]
Let 
Observe that every bin  has  and this implies that, conditioned on any choice of hash functions, the maximum load of all bins outside  is
at most  with high probability.\footnote{This is by majorization with the process that just throws every ball to the \emph{first}
    choice; see, e.g,~\citet{azar1999balanced-allocations}.} Therefore our task reduces to showing that the maximum load of the bins in  is .

Consider the sequence  of random variables given by , and let  denote the number of bins  with .
By Lemma~\ref{lem:mu1}, 
.
Moreover, the function 
satisfies the hypothesis of Theorem~\ref{bounded_dif}.We conclude that, with high probability, 

Now assume that the thesis of Corollary~\ref{coro:mu2} holds, which happens except with probability .
Then we have that for all , 
Thus Lemma~\ref{lem:perfect} applies to . This means that after throwing  balls, the maximum load among the bins in  is , as
we wished to show.
\end{proof}



































\mycomment{




\begin{theorem}
Suppose  is a sequence of independent random variables with  and let , .
Then for all ,


and, for all ,

\end{theorem}

\begin{corollary}
Let  be the sum of  of the keys assigned to bin .
Then for all ,


and, for all ,

\end{corollary}

\begin{theorem}
Suppose  is a sequence of independent random variables with  and let , .
Then



\end{theorem}

\begin{theorem}\label{chernoff}
Let  be independent random variables in  and .
For all ,

and for all 


\end{theorem}

\subsection{notes}
The collision probability  satisfies .
By the Hoeffding bound, the weighted load of each bin is .
\todo{Actually the  I'm defining is simply the expansion parameter of a random 2-regular bipartite graph! We can probably reference to results and
    make everything shorter}.

When , the maximum fractional load is 
        (the second bound is better when ).
        Whereas for , .

Build a graph : from sink to each of the  keys, from each key  to its two bin choices , and then from each bin to the sink with
capacity . We want to minimize  subject to being able to pass flow  from source to sink. This is a parametric max-flow problem, we
can e.g. perform binary search on .
Let  be the bipartite graph representing that key  has bin  as a choice.
The LP formulation of the balancing problem when we know the probabilities is as follows.

minimize 
subject to  for all bins .
           , i.e.,
            for all keys .

           .

By multiplying the first inequalities by  and the last by , we obtain the dual formulation

maximize 
subject to
         for all keys  (can be replaced with ).
         (can be replaced with ).
        .

This is equivalent to
maximize 
subject to
         for all keys 
         (redundant).

A solution to the dual is a lower bound for the primal. This admits a probabilistic interpretation: you pick your distribution , I pick a
distribution of bins, and select one of them at random without telling you. The objective function is a lower bound on the load of the bin I select
(for each key, the expected contribution is that term). By LP duality, optimality implies that the expected load of the bin selected is equal to the
maximum load among all bins, hence my distribution is supported on the bins with maximum load. So without loss of generality there is  such that  is either equal to  or .
Define

and



Having defined , we show that it controls the slope of the linear-rate increase in imbalance (when it exists),
       and that  the normalized imbalance converges asymptotically almost surely to .




\subsection{notes2}
Then the solution to the LP problem is , and the imbalance per unit of time will be .

If  and , then this is less than .
For example, if  and , then




Clearly .
\begin{theorem}
With high probability, .
\end{theorem}
\begin{proof}
The lower bound follows from the trivial inequalities  and .

We apply Chernoff with , .
Fix , . Let  be  if  has both bins in , and 0 otherwise. Note 
and , where .
The sum has expectation .
Note that , which has expectation .
We applyChernoff

\end{proof}
Observe that if  are all very small, then
 will be around .
\subsection{obs1}
\mycomment{
It is well known that with one choice, the imbalance grows as .

After seeing  keys, I have occupied  bins. Let .
There is now a set of fractional size  with mass ,
      hence . If , this is larger than one.

For example, consider the scenario that we have  keys with probabilities . Assign the first  keys; the right neighbourhood is
then roughly  because the probability that a bin is hit is . So about an  fraction is never hit. So we have
 balls and a right set of size . Now every other ball falls within this set with probability . So we expect another
 balls to fall there. In total we have a set of  bins with a left neighbourhood of . Therefore the 
is at least , where .

For each unordered pair of bins, the number of keys assigned to that
pair is a binomial , with expectation . The binomial can be approximated by a Poisson with parameter .
The probability that a pair of bins has load

For example, if we have  keys with , then there is a pair of bins with 1-load  and the imbalance will be this after just  throws.
If we have  keys with , then  and the whole thing still applies (up to ).
In the geometric power of two choices, there are  keys and the maximum  is .




Let us first consider the easy case where . Our process then gives an  approximation algorithm:
\begin{theorem}
Assume . With high probability, the maximum load at the end of the colored balls and bins process is at most .
\end{theorem}

This is a straightforward consequence of the following Chernoff-like bound (take ):
\begin{lemma}
Let  be independent random variables in , where .
Then

\end{lemma}
\begin{proof}
There are two cases. Both are based on the standard version


If , then the rhs is at most  and we are done. This is the case where the average load is higher than , or 
is small enogh.
If , then

\end{proof}
}

\subsection{obs}
Let the weight of a key denote the number of times it occurs in the stream.
We may want to assume that the key sequence is a series of  random draws from a key distribution , in which case we can normalize the
 weights by making them equal to the corresponding probabilities. Then the maximum weight is related to the min-entropy of ,
 and the average key weight is the collision probability of .


Possibly related to key splitting: take the case . Then the maximum load is
 , even if each time a ball is placed we are allowed to move balls among the  bins to equalize loads as much as possible.
 On the other hand, if we have access to all  choices for the  balls, we can place each ball into one of its choices such that we end up with
 a \emph{constant} maximum load. The algorithm picks a constant bound  and starts with  active balls; at each step it finds a bin that has at
 least one but no more than  active balls that have chosen it, assign these active balls to that bin, and remove those balls from the set of active
 balls. If the algorithm ends with no active balls remaining, it succeeded.

The power of  choices guarantees perfect balancing. This is closely related to Erdos and Renyi's result that a random bipartite graph of degree 
has a perfect matching.
}

 \section{Evaluation}
\label{sec:evaluation}

We assess the performance of our proposal by using both simulations and a real deployment.
In so doing, we answer the following questions:
\begin{squishlist}
\item[\textbf{Q1:}]
What is the effect of key splitting on \potc?
\item[\textbf{Q2:}]
How does local estimation compare to a global oracle?
\item[\textbf{Q3:}]
How robust is \pkg? \item[\textbf{Q4:}]
What is the overall effect of \pkg on applications deployed on a real \dspe?
\end{squishlist}



\subsection{Experimental Setup}

\begin{table}[t]
\caption{Summary of the datasets used in the experiments: number of messages, number of keys and percentage of messages having the most frequent key ().} \centering
\small
\begin{tabular}{l c r r r}
\toprule
Dataset		&	Symbol	&	Messages				&	Keys			& 	(\%)		\\
\midrule
Wikipedia		&	WP		&	\num{22}M		&	\num{2,9}M		&	\num{9,32}	\\
Twitter		&	TW		&	\num{1,2}G		&	\num{31}M		&	\num{2,67}	\\
Cashtags		&	CT		&	\num{690}k		&	\num{2,9}k		&	\num{3,29}	\\
\midrule
Synthetic 1 	&	LN	&	\num{10}M		&	\num{16}k			&	\num{14,71}	\\
Synthetic 2 	&	LN	&	\num{10}M		&	\num{1,1}k		&	\num{7,01}	\\
\midrule
LiveJournal	&	LJ		&	\num{69}M		&	\num{4,9}M		&	\num{0,29}	\\
Slashdot0811	&	SL	&	\num{905}k		&	\num{77}k			&	\num{3,28}	\\
Slashdot0902 	&	SL	&	\num{948}k		&	\num{82}k			&	\num{3,11}	\\
\bottomrule
\end{tabular}
\label{tab:summary-datasets}
\end{table}






\spara{Datasets.}
Table~\ref{tab:summary-datasets} summarizes the datasets used.
We use two main real datasets, one from \emph{Wikipedia} and one from \emph{Twitter}.
These datasets were chosen for their large size, their different degree of skewness, and because they are representative of Web and online social network domains.
The Wikipedia dataset (WP)\footnote{\url{http://www.wikibench.eu/?page_id=60}} is a log of the pages visited during a day in January 2008. Each visit is a message and the page's URL represents its key.
The Twitter dataset (TW) is a sample of tweets crawled during July 2012.
Each tweet is parsed and split into its words, which are used as the key for the message.


An additional Twitter dataset comprises a sample of tweets crawled in November 2013.
The keys for the messages are the \emph{cashtags} in these tweets.
A \emph{cashtag} is a ticker symbol used in the stock market to identify a publicly traded company preceded by the dollar sign (e.g., \_1_2\mu_11.789\sigma_12.366\mu_22.245\sigma_21.133_1_264WO(1/p_1)50100_5\numsources=5T_p_5_1\numsources=5T_p=1I(t)W=1050W=5W=100W=10W=50_5W=50_5_1S=5_1_20.11\frac{1}{10}500\approx 60\%\approx 37\%45\%3.6M30\%2.9M7.2M0.4T30CXWW \times D \times C \times LDCLWW \times D \times C2DCLWif_ii\hat{f}_i\Delta_j\Delta_{f}WWmn\mathcal{X}\mathcal{X}\mathcal{X}d \times md \geq 1$). 
Sparrow considers only independent tasks that can be executed by any worker.
In \dspes, a message can only be sent to the workers that are accumulating the state corresponding to the key of that message.
Furthermore, \dspes deal with messages that arrive at a much higher rate than Sparrow's fine-grained tasks, so we prefer to use local load estimation.

In the domain of graph processing, several systems have been proposed to solve the load balancing problem, e.g., Mizan~\cite{khayyat2013mizan}, GPS~\cite{salihoglu2013gps}, and xDGP~\cite{vaqueroxdgp}.
Most of these systems perform dynamic load rebalancing at runtime via vertex migration.
We have already discussed why rebalancing is impractical in our context in Section~\ref{sec:preliminaries}.


Finally, SkewTune~\cite{kwon2012skewtune} solves the problem of load balancing in MapReduce-like systems by identifying and redistributing the unprocessed data from the stragglers to other workers.
Techniques such as SkewTune are a good choice for batch processing systems, but cannot be directly applied to \dspes. 






% 
\section{Conclusion}
Despite being a well-known problem in the literature, load balancing has not been exhaustively studied in the context of distributed stream processing engines.
Current solutions fail to provide satisfactory load balance when faced with skewed datasets. To solve this issue, we introduced \pkg, a new stream partitioning strategy that allows better load balance than key grouping while incurring less memory overhead than shuffle grouping.
Compared to key grouping, \pkgs is able to reduce the imbalance by up to several orders of magnitude, thus improving throughput and latency of an example application by up to 45\%. 

This work gives rise to further interesting research questions.
Is it possible to achieve good load balance without foregoing atomicity of processing of keys?
What are the necessary conditions, and how can it be achieved?
In particular, can a solution based on rebalancing be practical?
And in a larger perspective, which other primitives can a \dspe offer to express algorithms effectively while making them run efficiently?
While most \dspes have settled on just a small set, the design space still remains largely unexplored.






\bibliographystyle{IEEEtranN}

\bibliography{references}
\end{document}
