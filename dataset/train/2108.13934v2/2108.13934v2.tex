Table \ref{tbl.datasets} gives statistics on the two zero-shot slot filling datasets in KILT. While the T-REx dataset is larger by far in the number of instances, the training sets have a similar number of distinct relations. We use only 500k training instances of T-REx in our experiments to increase the speed of experimentation.

\begin{table}
\begin{center}
\begingroup
\renewcommand{\arraystretch}{1.2} 
\resizebox{\linewidth}{!}{\begin{tabular}{r|ccc|ccc}
 & \multicolumn{3}{c|}{\textbf{Instances}} & \multicolumn{3}{c}{\textbf{Relations}}\\ \hline
\textbf{Dataset} & \textbf{Train}  & \textbf{Dev} & \textbf{Test} & \textbf{Train}  & \textbf{Dev} & \textbf{Test}\\
\hline
zsRE  &	148K & 3724 & 4966 &	84 & 12 & 24 \\
T-REx &	2284K & 5000 & 5000 &	106 & 104 & 104
\end{tabular}
}
\endgroup
\end{center}
\caption{\label{tbl.datasets}Slot filling datasets in KILT}
\end{table}

Since the transformers for passage encoding and generation can accept a limited sequence length, we segment the documents of the KILT knowledge source (2019/08/01 Wikipedia snapshot) into passages. The ground truth provenance for the slot filling tasks is at the granularity of paragraphs, so we align our passage segmentation on paragraph boundaries when possible.  If two or more paragraphs are short enough to be combined, we combine them into a single passage and if a single paragraph is too long, we truncate it.

\subsection{\kgi{} Hyperparameters}
\begin{table}[t]
\small
\begin{center}
\begin{tabular}{rrr}
\textbf{Hyperparameter} & \textbf{DPR}  & \textbf{RAG} \\
\hline
learn rate  & 5e-5 & 3e-5 \\
batch size & 128 & 128 \\
epochs & 2 & 1\\
warmup instances & 0 & 10000 \\
learning schedule & linear & triangular \\
max grad norm & 1 & 1 \\
weight decay & 0 & 0 \\
Adam epsilon & 1e-8 & 1e-8
\end{tabular}
\end{center}
\caption{\kgi{} hyperparameters}
\label{tbl.hypers}
\end{table}

We have not done hyperparameter tuning, instead using hyperparameters similar to the original works on training DPR and RAG. Table \ref{tbl.hypers} shows the hyperparameters used in our experiments. 
We train our models on T-REx using only the first 500k instances.
For \kgi{1} we use the same hyperparameters except that zsRE is trained for two epochs. In both \kgi{} systems we use the default of five passages retrieved for each query for use in RAG.

\subsection{Model Details}

\paragraph{Number of parameters}
\kgi{} is based on RAG and has the same number of parameters:  for the BERT query and passage encoders and  for the BART sequence-to-sequence generation component:  in total.

\paragraph{Computing infrastructure}
Using a single NVIDIA V100 GPU DPR training of two epochs takes approximately 24 hours for T-REx and 2 hours for zsRE.
Using a single NVIDIA P100 GPU RAG training for 500k T-REx instances takes two days and 147k instances of zsRE takes 15 hours.
The FAISS index on the KILT knowledge source requires a machine with large memory, we use 256GB memory - 128GB is insufficient for the indexes without scalar quantization.

\subsection{Slot Filling Evaluation}

As an initial experiment we tried RAG with its default index of Wikipedia, distributed through Hugging Face. We refer to this as RAG-KKS, or RAG without the KILT Knowledge Source, as reported in Table \ref{tbl.dev}. Since the passages returned are not aligned to the KILT provenance ground truth, we do not report retrieval metrics for this experiment.  

Motivated by the low retrieval performance reported for the RAG baseline by \citet{kilt}, we experimented with replacing the DPR retrieval with simple BM25 (RAG+BM25) over the KILT knowledge source. We provide the raw BM25 scores for the passages to the RAG model, to weight their impact in generation. We also experimented with the Natural Questions trained DPR, with only RAG training on KILT (RAG+DPR). 

We use the approach explained in Section \ref{sec.approach} to train both the DPR and RAG models. \kgi{0} is a version of our system using DPR with hard negative samples from BM25.  The successor system, \kgi{1} incorporates DPR training using DNS.

The metrics we report include accuracy and F1 on the slot filler, where F1 is based on the recall and precision of the tokens in the answer, allowing for partial credit on slot fillers.  Our systems, except for RAG-KKS, also provide provenance information for the top answer.  R-Precision and Recall@5 measure the quality of this provenance against the KILT ground truth provenance.  Finally, KILT-Accuracy and KILT-F1 are combined metrics that measure the accuracy and F1 of the slot filler \textit{only when the correct provenance is provided}. 


\begin{table}[t]
\begin{center}
\small
\begingroup
\renewcommand{\arraystretch}{1.2} 
\begin{tabular}{r|cccc}
& \textbf{R-Prec}  & \textbf{R@5} & \textbf{Acc.} & \textbf{F1} \\
\hline \multicolumn{5}{c}{\textbf{zsRE}} \\ \hline
RAG-KKS  &	 &  & 38.72 & 46.94\\
RAG+BM25 &	58.86 & 80.24 & 45.73 & 55.18  \\
RAG+DPR & 68.13 & 79.19 & 46.03 & 55.75 \\
\kgi{0} & 96.24 & 97.53 & 69.58 & 77.24  \\
\kgi{1} & \best{98.60} & \best{99.70} & \best{71.32} & \best{78.85} \\	
\hline \multicolumn{5}{c}{\textbf{T-REx}} \\ \hline
RAG-KKS  &	 &  & 63.28 & 67.67   \\
RAG+BM25 &	46.40 & 67.31 & 69.10 & 73.11  \\
RAG+DPR & 53.04 & 65.54 & 73.02 & 76.97 \\
\kgi{0} & 61.30 & 71.18 & 76.58 & 80.27 \\  
\kgi{1} & \best{74.34} & \best{82.89} & \best{84.04} & \best{86.89}
\end{tabular}
\endgroup
\end{center}
\caption{Dev sets performance for different retrieval methods}
\label{tbl.dev}
\end{table}

\begin{table*}[t]
\begin{center}
\small
\begingroup
\renewcommand{\arraystretch}{1.2} 
\begin{tabular}{r|ccccccc}
 & \textbf{R-Prec}  & \textbf{Recall@5} & \textbf{Accuracy} & \textbf{F1}  & \textbf{KILT-AC} & \textbf{KILT-F1}\\
\hline \multicolumn{7}{c}{\textbf{zsRE}} \\ \hline
\kgi{1} & \best{98.49} & \best{99.23} & \best{72.55} & \best{77.05} & \best{72.31} & \best{76.69} \\
\kgi{0}  & 94.18 & 95.19 & 68.97 & 74.47 & 68.32 & 73.45 \\
DensePhrases & 57.43 & 60.47 & 47.42 & 54.75 & 41.34 & 46.79 \\
GENRE & 95.81 & 97.83 & 0.02 & 2.10 & 0.00 & 1.85 \\
Multi-DPR & 80.91 & 93.05 & 57.95 & 63.75 & 50.64 & 55.44 \\
RAG {\small (KILT organizers)} & 53.73 & 59.52 & 44.74 & 49.95 & 36.83 & 39.91 \\
BART & N/A & N/A & 9.14 & 12.21 & N/A & N/A \\
\hline \multicolumn{7}{c}{\textbf{T-REx}} \\ \hline
\kgi{1}  & 74.36  & 83.14 & \best{84.36} & \best{87.24} & \best{69.14} & \best{70.58} \\
\kgi{0}  & 59.70 & 70.38 & 77.90 & 81.31 & 55.54 & 56.79 \\
DensePhrases & 37.62 & 40.07 & 53.90 & 61.74 & 27.84 & 32.34 \\
GENRE &	\best{79.42} & \best{85.33} & 0.10 & 7.67 & 0.04 & 6.66 \\
Multi-DPR & 69.46 & 83.88 & 0.00 & 0.00 & 0.00 & 0.00 \\
RAG {\small (KILT organizers)} &	28.68 & 33.04 & 59.20 & 62.96 & 23.12 & 23.94 \\
BART & N/A & N/A & 45.06 & 49.24 & N/A & N/A
\end{tabular}
\endgroup
\end{center}
\caption{KILT leaderboard top systems performance on slot filling tasks}
\label{tbl.test}
\end{table*}

Table \ref{tbl.dev} reports an evaluation on the development set, while Table \ref{tbl.test} reports the test set performance of the top systems on the KILT leaderboard.  \kgi{0} and \kgi{1} are our systems, while DensePhrases, GENRE, Multi-DPR, RAG for KILT and BART are explained briefly in Section \ref{sec.related}.  \kgi{1} gains dramatically in slot filling accuracy over the previous best systems, with gains of over 14 percentage points in zsRE and even more in T-REx.  The combined metrics of KILT-AC and KILT-F1 show even larger gains, suggesting that the \kgi{1} approach is effective at providing justifying evidence when generating the correct answer.  We achieve gains of 21 to 41 percentage points in KILT-AC.


Relative to Multi-DPR, we see the benefit of weighting passage importance by retrieval score and marginalizing over multiple generations, compared to the strategy of concatenating the top three passages and running a single sequence-to-sequence generation. 
GENRE is still best in retrieval for T-REx, suggesting that at least for a corpus such as Wikipedia, generating the title of the page can be very effective. 
A possible explanation for this behaviour is that most relations for a Wikipedia entity are mentioned in its corresponding page.