

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr}      

\usepackage{pifont}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}



\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}



\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[table]{xcolor}
\usepackage[pagebackref=true,breaklinks=true,colorlinks=true,bookmarks=false,linkcolor={red!50!black},urlcolor={darkgray!80!black},citecolor={green!50!black}]{hyperref}
\usepackage[group-separator={,}]{siunitx}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs} \usepackage{makecell}
\usepackage{graphbox}
\usepackage{footmisc}
\usepackage{bm}
\usepackage{caption}
\usepackage{arydshln}
\usepackage{dashrule}
\usepackage{subcaption}
\usepackage{enumitem}
\setlist[itemize]{noitemsep,nolistsep}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\Crefname{figure}{Figure}{Figures}
\crefname{figure}{Fig.}{Figs.}
\Crefname{equation}{Equation}{Equations}
\crefname{equation}{Eq.}{Eqs.}




\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


\usepackage{graphicx}
\usepackage{makecell}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{booktabs}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{hyperref}


\usepackage{caption}
\captionsetup{font=footnotesize}
\captionsetup[sub]{font=footnotesize}


\renewcommand{\paragraph}[1]{\noindent \textbf{#1}}

\newcommand{\blue}[1]{\textbf{\textcolor{mblue}{#1}}}
\newcommand{\rblue}[1]{{\textcolor{mblue}{#1}}}
\newcommand{\overall}[1]{\textbf{\textcolor{mgreenblue}{#1}}}
\newcommand{\nonecolor}[1]{}
\newcommand{\orange}[1]{\textbf{\textcolor{orange}{#1}}}
\newcommand{\oorange}[1]{{\textcolor{orange}{#1}}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\bred}[1]{\textbf{\textcolor{red}{#1}}}
\newcommand{\green}[1]{\textcolor{mgreen}{#1}}
\newcommand{\yellow}[1]{\textcolor{yellow}{#1}}
\newcommand{\gray}[1]{\textcolor{mgray}{#1}}
\newcommand{\lightgray}[1]{\textcolor{mlgray}{#1}}
\newcommand{\supplement}{supplementary materials}
\newcommand{\textbfg}[1]{\textbf{\textcolor{mgreen}{#1}}}

\newcommand{\frag}[0]{{{\textit{fragments}}}}

\newcommand{\cfchen}[1]{\textcolor{blue}{CF: #1}}
\newcommand{\cfcomment}[1]{\textcolor{blue}{\emph{CF Comment: #1}}}
\newcommand{\LL}[1]{\textcolor{red}{\emph{LL: #1}}}
\newcommand{\HN}[1]{\textcolor{red}{\emph{HN: #1}}}
\colorlet{lightcyan}{cyan!8}
\colorlet{lightpink}{pink!20}
\colorlet{lightgray}{gray!10}

\definecolor{mgray}{gray}{0.35}
\definecolor{mlgray}{gray}{0.85}

\definecolor{mred}{RGB}{238, 34, 12}
\definecolor{mgreen}{RGB}{1, 127, 0}
\definecolor{mblue}{RGB}{0, 77, 128}
\definecolor{mgreenblue}{RGB}{1,102,98}
\definecolor{orange}{RGB}{240, 120,0}

\usepackage[table]{xcolor}
\usepackage[pagebackref=true,breaklinks=true,colorlinks=true,bookmarks=false]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\title{Exploring Video Quality Assessment on User Generated Contents 
 \\ from Aesthetic and Technical Perspectives }
\author{Haoning Wu\footnotemark[1]~ \qquad Erli Zhang\footnotemark[1]~ \qquad Liang Liao\footnotemark[1]~ \qquad Chaofeng Chen  \qquad \\ Jingwen Hou  \qquad  Annan Wang \qquad Wenxiu Sun \qquad Qiong Yan \qquad Weisi Lin \\
  S-Lab, Nanyang Technological University \qquad   Sensetime Research and Tetras AI
}
\begin{document}


\maketitle



\begin{abstract}
The rapid increase in user-generated content (UGC) videos calls for the development of effective video quality assessment (VQA) algorithms. However, the objective of the UGC-VQA problem is still ambiguous and can be viewed from two perspectives: the \textbf{\textit{\green{technical perspective}}}, measuring the perception of distortions; and the \textbf{\textit{\blue{aesthetic perspective}}}, which relates to preference and recommendation on contents. To understand how these two perspectives affect overall subjective opinions in UGC-VQA, we conduct a large-scale subjective study to collect human quality opinions on the overall quality of videos as well as perceptions from aesthetic and technical perspectives. The collected Disentangled Video Quality Database (\textbf{DIVIDE-3k}) confirms that human quality opinions on UGC videos are universally and inevitably affected by both aesthetic and technical perspectives. In light of this, we propose the \underline{D}isentangled \underline{O}bjective \underline{V}ideo Quality \underline{E}valuato\underline{r} (\textbf{DOVER}) to learn the quality of UGC videos based on the two perspectives. The DOVER proves state-of-the-art performance in UGC-VQA under very high efficiency. With perspective opinions in DIVIDE-3k, we further propose \textbf{DOVER++}, the first approach to provide reliable clear-cut quality evaluations from a single aesthetic or technical perspective. Code at \small{\url{ https://github.com/VQAssessment/DOVER}}.


\end{abstract}

\begin{figure}
    \center  \includegraphics[width=0.976\linewidth]{ICCV_WhichBetter_compressed.pdf}
    \vspace{-7pt}
    \captionof{figure}{\textbf{Which video has better quality}: a clear video with meaningless contents \textbf{{(a)}} or a blurry video with meaningful contents {\textbf{(b)}}? Viewing from different perspectives (\textbf{\textit{\blue{aesthetic}}}/\textbf{\textit{\green{technical}}}) may produce different judgments, motivating us to collect \textbf{DIVIDE-3k}, which is the first UGC-VQA dataset with opinions from multiple perspectives. More multi-perspective quality comparisons in our dataset are shown in \textcolor{brown}{\textit{supplementary \textbf{Sec. A}.}}}
    \vspace{-18pt}
    \label{fig:ugc_process}
\end{figure}



\section{Introduction}
\label{sec:intro}





Understanding and predicting human quality of experience (QoE) on diverse in-the-wild videos has been a long-existing and unsolved problem. Recent Video Quality Assessment (VQA) studies have gathered enormous human quality opinions~\cite{pvq,mlsp,kv1k,vqc,ytugccc} on in-the-wild user-generated contents (UGC) and attempted to use machine algorithms~\cite{vsfa,rfugc,internetvqa} to learn and predict these opinions, known as the \textbf{UGC-VQA problem}~\cite{videval}. However, due to the diversity of contents in UGC videos and the lack of reference videos during subjective studies, these human-quality opinions are still ambiguous and may relate to different perspectives.

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{The authors contribute equally to this paper.} 


Conventionally, VQA studies~\cite{videval,tlvqm,niqe,tpqi,fastvqa} are concerned with the \textit{\textbf{\green{technical perspective}}}, aiming at measuring distortions in videos (\textit{e.g.},~\textit{blurs, artifacts}) and their impact on quality, so as to compare and guide technical systems such as cameras~\cite{dxomark,spaq}, restoration algorithms~\cite{basicvsr,swinir,mmp} and compression standards~\cite{h264}. Under this perspective, the video with clear textures in Fig.~\ref{fig:ugc_process}(a) should have notably better quality than the blurry video in Fig.~\ref{fig:ugc_process}(b). On the other hand, several recent studies~\cite{sfa,vsfa,rfugc,mlsp,discovqa} notice that preferences on non-technical semantic factors (\textit{e.g.},~\textit{contents, composition}) also affect human quality assessment on UGC videos. Human experience on these factors is usually regarded as the \textit{\textbf{\blue{aesthetic perspective}}}~\cite{avaiaa,mlspiaa,distilliaa,racniaa,piaadataset,objiaa} of quality evaluation, which considers the video in Fig.~\ref{fig:ugc_process}(b) as better quality due to its more meaningful contents and is preferred for content recommendation systems on platforms such as YouTube or TikTok. However, how aesthetic preference plays the impact on final human quality opinions of UGC videos is still debatable~\cite{pvq,mlsp} and requires further validation. 







To investigate the impact of aesthetic and technical perspectives on human quality perception of UGC videos, we conduct the first comprehensive subjective study to collect opinions from both perspectives, as well as overall opinions on a large number of videos. 
We also conduct subjective reasoning studies to explicitly gather information on how much each individual's overall quality opinion is influenced by aesthetic and technical perspectives. With overall 450K opinions on 3,590 diverse UGC videos, we construct the first \underline{Di}sentangled \underline{Vi}deo Quality \underline{D}atabas\underline{e} (\textbf{DIVIDE-3k}). After calibrating our study on the DIVIDE-3k with existing UGC-VQA subjective studies, we observe that human quality perception on UGC videos is broadly and inevitably \textit{\textbf{affected by both aesthetic and technical perspectives}}. As a consequence, the overall subjective quality scores between the two videos in Fig.~\ref{fig:ugc_process} with different qualities from either one of the two perspectives could be similar.

















Motivated by the observation from our subjective study, we aim to develop an objective UGC-VQA method that accounts for both aesthetic and technical perspectives. To achieve this, we design the View Decomposition strategy, which divides and conquers aesthetic-related and technical-related information in videos, and propose the \underline{D}isentangled \underline{O}bjective \underline{V}ideo Quality \underline{E}valuato\underline{r} (\textbf{DOVER}).  DOVER consists of two branches, each dedicated to focusing on the effects of one perspective. Specifically, based on the different characteristics of quality issues related to each perspective, we carefully design inductive biases for each branch, including \textit{specific inputs, regularization strategies, and pre-training}. The two branches are supervised by the overall scores (affected by both perspectives) to adapt for existing UGC-VQA datasets~\cite{vqc,kv1k,pvq,cvd,qualcomm,ytugc}, and additionally supervised by aesthetic and technical opinions exclusively in the \textbf{DIVIDE-3k} (denoted as \textbf{DOVER++}). Finally, we obtain the overall quality prediction via a subjectively-inspired fusion of the predictions from the two perspectives. With the subjectively-inspired design, the proposed DOVER and DOVER++ not only reach better accuracy on the overall quality prediction but also provide more reliable quality prediction from aesthetic and technical perspectives, catering for practical scenarios.





\begin{comment}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{videos_in_divide3k.jpg}
    \vspace{-16pt}
    \caption{Several example videos in the proposed \textbf{DIVIDE-3k} database.}
    \label{fig:dividesample}
    \vspace{-16pt}
\end{figure}
\end{comment}




























\begin{comment}
\begin{table}[]
    \centering
    \setlength\tabcolsep{3pt}
    \renewcommand\arraystretch{1.2}
    \caption{Comparing the \textbf{DIVIDE-3k} with recent VQA databases.}
    \vspace{-9pt}
    \resizebox{\linewidth}{!}{\begin{tabular}{l|cccccc} \hline
         Database & UGC? & Subjects & \#Content & \#Video & Perspective? & Labels   \\ \hline
         KoNViD-1k (2018)\cite{kv1k} & \cmark & Crowdsource & 1200 & 1200 & \xmark &  \\
         LIVE-VQC (2019)\cite{vqc} & \cmark & Crowdsource & 585 &  585 & \xmark &  \\
         Youtube-UGC (2020)\cite{ytugccc} & \cmark & Crowdsource & 1380 & 1380 & \xmark &  \\
         LSVQ (2020)\cite{pvq} & \cmark & Crowdsource & {39,076} & {39,076} & \xmark &  \\ 
         MSU-VQB (2022)\cite{msuvqbench} & \xmark & Crowdsource & 36 & 2,486 & \xmark &  \\ \hline
         
         \textbf{DIVIDE-3k} (Ours) & \cmark & \textbf{In-lab} & {3,590} & {3,590} & \cmark &  \\ \hline
    \end{tabular}}
    \label{tab:dataset}
    \vspace{-17pt}
\end{table}
\end{comment}












Our contributions can be summarized as four-fold:

\begin{enumerate} [topsep=0pt,itemsep=2pt,parsep=0pt]
\renewcommand{\labelenumi}{\theenumi)}
\item We collect the \textbf{DIVIDE-3k} (3,590 videos), the first UGC-VQA database that contains 450,000 subjective quality opinions from aesthetic and technical perspectives as well as their effects on overall quality scores.

\item By analyzing opinions, we observe that human quality perception is broadly affected by both aesthetic and technical perspectives in the UGC-VQA problem, better explaining the human perceptual mechanism on it.


\item We propose the \textbf{DOVER}, a subjectively-inspired video quality evaluator with two branches focusing on aesthetic and technical perspectives. The DOVER demonstrates state-of-the-arts on the \textbf{all} UGC-VQA datasets.

\item Our methods can provide quality predictions from a single perspective, which can be applied as metrics for camera systems (\textbf{\green{\textit{technical}}}) or content recommendation (\blue{\textit{aesthetic}}), or for personalized VQA~(Sec.~\ref{sec:personalized}).










\end{enumerate}








\section{Related Works}






\paragraph{Databases and Subjective Studies on UGC-VQA.} 
 Unlike traditional VQA databases \cite{livevqa,csiqvqa,cvd,qualcomm}, UGC-VQA databases~\cite{pvq,vqc,kv1k,ytugccc} directly collect from real-world videos from direct photography, YFCC-100M~\cite{yfcc} database or YouTube~\cite{ytugc} videos. With each video having unique content and being produced by either professional or non-professional users \cite{rfugc,internetvqa}, quality assessment of UGC videos can be more challenging and less clear-cut compared to traditional VQA tasks. Additionally, the subjective studies in UGC-VQA datasets are usually carried out by crowdsourced users \cite{crowdsource} with no reference videos. These factors may lead to the ambiguity of subjective quality opinions in UGC-VQA which can be affected by different perspectives.


\paragraph{Objective Methods for UGC-VQA.}
Classical VQA methods \cite{niqe,bofqa,rrstedqa,diivine,stgreed,vmaf,tlvqm,videval,brisque,viideo,vbliinds,tpqi} employ handcrafted features to evaluate video quality. However, they do not take the effects of semantics into consideration, resulting in reduced accuracy on UGC videos.
Noticing that UGC-VQA is deeply affected by semantics, deep VQA methods \cite{fastvqa, dctqa, cnn+lstm,deepvqa, gstvqa, vsfa, mlsp, rirnet, dstsvqa, svqa, mdtvsfa} are becoming predominant in this problem. For instance, VSFA~\cite{vsfa} conducts subjective studies to demonstrate videos with attractive content receive higher subjective scores. Therefore, it uses the semantic-pretrained  ResNet-50~\cite{he2016residual} features instead of handcrafted features, followed by plenty of recent works~\cite{lsctphiq,pvq,cnntlvqm,mlsp,fastvqa,discovqa} that improve the performance for UGC-VQA. However, these methods, which are directly driven by ambiguous subjective opinions, can hardly explain what factors are considered in their quality predictions, hindering them from providing reliable and explainable quality evaluations on real-world scenarios (\textit{e.g.}, distortion metrics and recommendations).







\begin{comment}
\section{Aesthetic and Technical Influences in VQA}

\subsection{Aesthetic Influence in UGC-VQA}


Researches on Image Quality Assessment (IQA) have long been explicitly separating \cite{atqa} image aesthetic assessment (IAA)~\cite{ava,distilliaa} and technical-related traditional image quality assessment \cite{paq2piq,dbcnn}. Unlike the technical quality assessment that usually focuses on levels of distortions, the metrics for aesthetic-related quality are related to a higher-level perception on visual contents, including the meaningfulness of visual objects and the organization among visual objects~\cite{racniaa}. In the UGC-VQA problem, an amount of subjective~\cite{ytugccc,rfugc,mlsp} and objective studies \cite{vsfa,rapique,cnntlvqm} also suggests the non-negligible \textit{aesthetic influence in UGC-VQA}. 

To validate the existence of aesthetic influence, the best way is to remove the effects of technical quality issues to see how only aesthetics affect overall quality perception on UGC videos. Henceforth, we conduct two levels of studies: \textbf{(1)} Zero-shot CLIP~\cite{clip} matching on processed videos that remove technical quality issues by downsampling and sparse frame sampling. \textbf{(2)} \textit{Subjective} quality ratings on UGC videos with clear guidance on aesthetics. Due to the extremely high cost of reliable large scale subjective studies, the CLIP matching will be conducted on whole UGC-VQA datasets (YouTube-UGC~\cite{ytugccc} and KoNViD-1k~\cite{kv1k}), while the subjective studies will only be conducted on a small subset of KoNViD-1k. 


\paragraph{Zero-shot CLIP matching experiments.} We compute zero-shot CLIP quality scores  as the softmax result between paired cosine similarities between a downsampled video  (to better remove technical effects \textit{\textit{e.g.} blurs/noises}) and corresponding text prompts :

where  denotes the encoding processes in CLIP.

\begin{table}[htbp]
\footnotesize
\caption{PLCCbetween matched score to prompt pairs and MOS.} \label{table:clipprompt}
\vspace{-10pt}
\setlength\tabcolsep{5pt}
\renewcommand\arraystretch{1.25}
\footnotesize
\center
\resizebox{\linewidth}{!}{\begin{tabular}{l:c|cc}
\hline
{Methods/Datasets} & YouTube-UGC & KoNViD-1k      \\ \hline
CLIP (on downsampled + sparse frame videos)  & \textbf{0.5443} & \textbf{0.5799} \\\hline
\nonecolor{lightpink} NIQE~\cite{niqe} (spatial technical quality index) & 0.2180 & 0.5490 \\\nonecolor{lightpink} TPQI~\cite{tpqi} (temporal technical quality index)  & 0.2776 & 0.5531 \\\hline
\end{tabular}}
\end{table}

The zero-shot correlations as illustrated in Tab.~\ref{table:clipprompt} suggest that both the direct perception on the \textbf{\textit{aesthetics}} and \textbf{\textit{meaningfulness}} of contents are notably correlated to quality ratings. The correlation is not very high, which might be due to relatively weak activation on the text prompts. If we use more directed text prompt pairs in Eq.~\ref{eq:2} (good/bad quality), the correlation will be much higher and significantly outperform NIQE. Considering that the input videos are already downsampled (especially for YouTube-UGC where a proportion of videos are 4K), we could assume that the CLIP scores only consider the rest aesthetic influence, which can reach competitive zero-shot performance on UGC-VQA. 

\paragraph{Subjective Studies.}



\subsection{Technical Influence in UGC-VQA}
\label{sec:3b}

Unlike professional videos like movies watched in movie theaters, the quality of UGC video is often limited by a number of technical issues, such as not-so-advanced cameras, built-in processing systems, strong compression due to limited bandwidth. In this section, we discuss potential technical quality issues in a UGC video related to different processes during the technical production of a UGC video.

\paragraph{Issues related to Video Capturing.} With the popularity of mobile photography, the technical quality issues in the video capturing process have attracted more and more attention. As examined by existing studies~\cite{cvd,qualcomm}, technical quality issues related to \green{\textit{exposure, stability, focus, noises}} are more likely to happen in videos than images. For instance, if the scene changes rapidly in a video, the exposure value (EV) and focus distance adapted for the previous scene may be inappropriate to the next scene, which lead to more frequent failures on exposure and focus in videos than in still images. The stability-related issues (shaking, flicker) are also specific to videos. In general, issues related to video capturing shall be well considered in the modeling of a technical quality evaluator on UGC videos.



\paragraph{Issues related to Video Post-processing.}  The raw-captured videos need to be post-processed into the final watchable video file, which leads to additional quality issues, such as inappropriate color mapping, mis-alignment between frames (after framewise post-processing), and several degradations originated by machine-learning-based processing algorithms (\textit{\textit{e.g.}} video super-resolution~\cite{basicvsr,vrt}). The issues related to post-processing are well discussed for images~\cite{giqa,risa,pipal} and commonly noticed in mobile-photographed UGC videos~\cite{vqc}.

\paragraph{Issues related to Compression/Transmission.} Due to the limitation of storage and transmission bandwidths, most videos need to be compressed by algorithms such as~\cite{jpeg,h264} before finally fed to users. Therefore, the artifacts resulted by compression and transmission are the primary concerns of earliest VQA databases~\cite{csiqvqa,livevqa}. Existing works~\cite{pvq,mlsp} also demonstrate the common existence of compression artifacts in UGC videos.
\end{comment}















\section{The DIVIDE-3k Database}


In this section, we introduce the proposed Disentangled Video Quality Database (\textbf{DIVIDE-3k}, Fig.~\ref{fig:divideprocess}), along with the multi-perspective subjective study. The database includes 3,590 UGC videos, on which we collected 450,000 human opinions. Different from other UGC-VQA databases~\cite{pvq,mlsp,kv1k}, the subjective study is conducted \bred{in-lab} to reduce the ambiguity of perspective opinions.

\subsection{Collection of Videos}

\paragraph{Sources of Videos.} The 3,590-video database is mainly collected from two sources: 1) the YFCC-100M~\cite{yfcc} social media database; 2) the Kinetics-400~\cite{k400data} video recognition database, collected from YouTube, which has in total 400,000 videos. Voices are removed from all videos. 

\paragraph{Getting the subset for annotation.} 
Similar to existing studies~\cite{kv1k,pvq}, we would like the sampled video database able to represent the overall quality of the original larger database. Therefore, we first histogram all 400,000 videos with spatial~\cite{niqe}, temporal~\cite{tpqi}, and semantic indices~\cite{clipiqa}. Then, we randomly select a subset of 3,270 videos from the 400,000 videos that match the histogram from the three dimensions~\cite{matchhistogram} as in~\cite{pvq,kv1k}. Several examples from DIVIDE-3k are provided in the supplementary. We also select 320 videos from the LSVQ~\cite{pvq}, the most recent UGC-VQA database, to examine the calibration between DIVIDE-3k and existing UGC-VQA subjective studies (see in Tab.~\ref{tab:ugccalibration}).


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{ICCV_SubjectiveProcess_compressed.pdf}\vspace{-10pt}    \caption{The in-lab subjective study on videos in \textbf{DIVIDE-3k}, including Training, Instruction, Annotation and Testing, discussed in Sec.~\ref{sec:subprocess}.}
    \label{fig:divideprocess}
    \vspace{-17pt}
\end{figure}


\subsection{In-lab Subjective Study on Videos}
\label{sec:subprocess}
To ensure a clear understanding of the two perspectives, we conduct in-lab subjective experiments instead of crowdsourced, with 35 trained annotators (including 19 male and 16 female) participating in the full annotation process of Training, Testing and Annotation. All videos are downloaded to local computers before annotation to avoid transmission errors. The main process of the subjective study is illustrated in Fig.~\ref{fig:divideprocess}, discussed step-by-step as follows. \textcolor{brown}{\textit{Extended details about the study are in supplementary \textbf{Sec. A}.}}




\paragraph{Training.} Before annotation, we provide clear criteria with abundant examples of the three quality ratings to train the annotators. For \textbf{\textit{\blue{aesthetic rating}}}, we select example images with {\green{\textit{good}}}, {\oorange{\textit{fair}}} and {\red{\textit{bad}}} aesthetic quality from the aesthetic assessment database AVA~\cite{avaiaa}, each for 20 images, as calibration for aesthetic evaluation. For \textit{\textbf{\green{technical rating}}}, we instruct subjects to rate purely based on technical distortions and provide 5 examples for each of the following eight common distortions: \textit{1) noises; 2) artifacts; 3) low sharpness; 4) out-of-focus; 5) motion blur; 6) stall; 7) jitter; 8) over/under-exposure.} For overall quality rating, we select 20 videos each with {\green{\textit{good}}}, {\oorange{\textit{fair}}} and {\red{\textit{bad}}} quality as examples, from the UGC-VQA dataset LSVQ~\cite{pvq}. 



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{ICCV_Histograms_compressed.pdf}
    \vspace{-16pt}
    \caption{\textbf{Statistics in DIVIDE-3k}: \textbf{(a)} The correlations between aesthetic and technical perspectives, and distributions \textbf{(b)} of overall quality () \& \textbf{(c)} subject-rated proportion of technical impact on overall quality.}
    \label{fig:histogram}
    \vspace{-10pt}
\end{figure}

\paragraph{During Experiment: Instruction and Annotation.} We divide the subjective experiments into 40 videos per group, and 9 groups per stage. Before each stage, we instruct the subjects on how to label each specific perspective:

\begin{itemize}
\item \textbf{\blue{Aesthetic Score}}: Please rate the video's quality based on aesthetic perspective (\textit{e.g.}, semantic preference).
\item \textbf{\green{Technical Score}}: Please rate the video's quality with only consideration of technical distortions.
\item \textbf{\overall{Overall Score}}: Please rate the quality of the video.
\item \textbf{\orange{Subjective Reasoning}}: Please rate how \underline{\textbf{your}} overall score is
 impacted by aesthetic or technical perspective.
\end{itemize}
Specifically, for the subjective reasoning, subjects need to rate the proportion of \textbf{\green{\textit{technical}}} impact in the overall score for each video among , while rest proportion is considered as \textbf{\blue{\textit{aesthetic}}} impact.

\paragraph{Testing with Golden Videos.} For testing, we randomly insert 10 \textbf{golden videos} in each stage as a spot check to ensure the quality of annotation, and the subject will be rejected and not join the next stage if the annotations on the golden videos severely deviate from the standards.

\begin{figure*}
    \centering
    \includegraphics[width=1.00\linewidth]{ICCV_BestWorst_compressed.pdf}
    \vspace{-16pt}
    \caption{Videos with best and worst scores in aesthetic perspective, technical perspective and overall quality perception in the \textbf{DIVIDE-3k}. The aesthetic perspective is more concerned with semantics or composition of videos, while the technical perspective is more related to low-level textures and distortions.}
    \label{fig:dividebestworst}
    \vspace{-10pt}
\end{figure*}














\subsection{Observations}
\label{sec:3c}


\begin{table}
    \centering
    \footnotesize
    \renewcommand\arraystretch{1.3}
    \setlength\tabcolsep{5pt}
    \caption{\textbf{Effects of Perspectives}: {The correlations between different perspectives and overall quality~()} for all 3,590 videos in \textbf{DIVIDE-3k}.}
    \vspace{-8pt}
    \resizebox{\linewidth}{!}{\begin{tabular}{l|c|c|c|c}
        \hline
        Correlation to  & \blue{} & \green{} &  & \\ \hline
         Spearman (SROCC) & 0.9350 & 0.9642 & 0.9827 & \bred{0.9834}  \\
         Kendall (KROCC) & 0.7894 & 0.8455 & 0.8909 & \bred{0.8933} \\
         \hline
    \end{tabular}}
    \label{tab:subconclusion}
    \vspace{-10pt}
\end{table}

\begin{table}
    \centering
    \footnotesize
    \renewcommand\arraystretch{1.2}
    \setlength\tabcolsep{8pt}
    \caption{\textbf{Calibration with Existing}: The correlations of between different ratings in \textbf{DIVIDE-3k} and existing scores in LSVQ~\cite{pvq} ().}
    \vspace{-8pt}
    \resizebox{\linewidth}{!}{\begin{tabular}{l|c|c|c|c}
        \hline
        Correlation to  & \blue{} & \green{} &  & \\ \hline
         Spearman (SROCC) & 0.6956 & 0.7374 & 0.7632 & \bred{0.7680}  \\
         Kendall (KROCC) & 0.5073 & 0.5469 & 0.5797 & \bred{0.5822} \\
         \hline
    \end{tabular}}
    \label{tab:ugccalibration}
    \vspace{-14pt}
\end{table}

\paragraph{Effects of Two Perspectives.} To validate the effects of two perspectives, we first quantitatively assess the correlation between the two perspectives and overall quality. Denote the mean aesthetic opinion as \blue{}, mean technical opinion as \green{}, mean overall opinion as , the Spearman and Kendall correlation between different perspectives are listed in Tab.~\ref{tab:subconclusion}. From Tab.~\ref{tab:subconclusion}, we notice that the weighted sum of both perspectives is a better approximation of overall quality than either single perspective. Consequently, methods~\cite{vsfa,bvqa2021,pvq} that naively regress from overall  might not provide pure technical quality predictions due to the inevitable effect of aesthetics. The best/worst videos (Fig.~\ref{fig:dividebestworst}) in each dimension also support this observation.

\paragraph{Calibration with Existing Study.} To validate whether the observation can be extended for existing UGC-VQA subjective studies, we select 320 videos from LSVQ~\cite{pvq} to compare quality opinions from multi-perspectives with existing scores of these videos. As shown in Tab.~\ref{tab:ugccalibration}, the overall quality score is more correlated with the existing score than scores from either perspective, further suggesting considering human quality opinion as a fusion of both perspectives might be a better approximation in the UGC-VQA problem.





\paragraph{Subjective Reasoning.} In the DIVIDE-3k, we conducted the first subjective reasoning study during the human quality assessment. Fig.~\ref{fig:histogram}(c) illustrates the mean technical impact for each video, ranging among . The results of reasoning further explicitly validate our aforementioned observation, that human quality assessment is affected by opinions from both aesthetic and technical perspectives.


\section{The Approaches: DOVER and DOVER++}
\label{sec:4}

Observing that overall quality opinions are affected by both aesthetic and technical perspectives from subjective studies in \textbf{DIVIDE-3k}, we propose to distinguish and investigate the aesthetic and technical effects in a UGC-VQA model based on the View Decomposition strategy (Sec.~\ref{sec:viewdecomposition}). The proposed \underline{D}isentangled \underline{O}bjective \underline{V}ideo Quality \underline{E}valuato\underline{r} (\textbf{DOVER}) is built up with an aesthetic branch (Sec.~\ref{sec:aesbranch}) and a technical branch (Sec.~\ref{sec:tecbranch}). The two branches are separately supervised, either both by overall scores (denoted as \textbf{DOVER}) or by respective aesthetic and technical opinions (denoted as \textbf{DOVER++}), discussed in Sec.~\ref{sec:objectives}. Finally, we discuss the subjectively-inspired fusion (Sec.~\ref{sec:fusion}) to predict the overall quality from DOVER.



\begin{comment}
\subsection{Observation: Entangled Nature of UGC-VQA}
\label{sec:3a}

\begin{figure}
    \centering
    \includegraphics[width=0.97\linewidth]{Fig_DOVER_Fig_compressed_compressed.pdf}
    \vspace{-6pt}
    \caption{Entangled nature of UGC-VQA: Extremely low-quality videos (lowest 5\% scores) are with bad aesthetics or/and bad technical quality.}
    \vspace{-15pt}
   \label{fig:entangle}
\end{figure}

Traditionally, aesthetic and technical quality assessments have long been studied separately~\cite{atqa}. In aesthetic evaluation~\cite{ava,aadb,cadb}, photographs are usually taken by technically professional equipment (\textit{e.g.}, DSLR), and their quality highly depends on the semantics of objects in photos and whether they are professionally organized (\textit{e.g.}, rule of thirds, symmetry). In contrast, images or videos collected for technical quality assessment~\cite{csiqvqa,cvd,qualcomm} are usually with various shooting equipment or processing algorithms. Different from traditional settings with distinct discrimination between them, aesthetic quality issues (Fig.~\ref{fig:ugc_process}(a)) and technical quality issues (Fig.~\ref{fig:ugc_process}(b)) are \underline{\textit{both prevalent}}~\cite{vsfa,mlsp,rfugc} in the acquisition of UGC videos, and affect their quality perception. In Fig.~\ref{fig:entangle}, we also notice that extremely low-quality videos (lowest 5\% scores) in YouTube-UGC~\cite{ytugccc} have either aesthetic or technical quality problems, further showing that quality perception of UGC videos is entangled by both effects. Although objective existence, most VQA algorithms neglect the entangled nature of UGC-VQA, making them hard to understand whether the perceived quality degradation is caused by aesthetic or technical issues. Henceforth, we aim to disentangle their effects to investigate the perception mechanism in UGC-VQA. 

\end{comment}












\subsection{Methodology: Separate the Perceptual Factors}
\label{sec:viewdecomposition}

From DIVIDE-3k, we notice that aesthetic and technical perspectives in UGC-VQA are usually associated with different perceptual factors. Specifically, as illustrated in (Fig.~\ref{fig:dividebestworst}\blue{\textbf{(a)\&(b)}}), aesthetic opinions are mostly related to \textit{semantics, composition} of objects~\cite{objiaa,cadb,distilliaa}, which are typically high-level visual perceptions. In contrast, the technical quality is largely affected by low-level visual distortions, \textit{e.g.}, \textit{blurs, noises, artifacts}~\cite{dbcnn,fastvqa,discovqa,pvq,paq2piq} (Fig.~\ref{fig:dividebestworst}\green{\textbf{(e)\&(f)}}). 

The observation inspires the View Decomposition strategy that separates the video into two views: the \textbf{Aesthetic View} () that focus on aesthetic perception, and \textbf{Technical View} () for vice versa. With the decomposed views as inputs, two separate aesthetic () and technical branches () evaluate different perspectives separately:



Despite that most perception related to the two perspectives can be separated, a small proportion of perceptual factors are related to both perspectives, such as \textbf{brightness} related to both \green{\textit{exposure (technical)}}~\cite{qualcomm} and \rblue{\textit{lighting (aesthetic)}}~\cite{piaadataset}, or \textbf{motion blurs} (which is occasionally considered as \rblue{\textit{good aesthetics}} but typically regarded as \red{\textit{bad technical quality}}~\cite{atqa}). Thus, we don't separate these factors and keep them in both branches. Instead, we employ inductive biases (\textit{pre-training, regularization}) and specific supervision in the DIVIDE-3k to further drive the two branches' focus on corresponding perspectives, introduced as follows.




\subsection{The Aesthetic Branch}
\label{sec:aesbranch}


To help the aesthetic branch focus on the aesthetic perspective, we first pre-train the branch with Image Aethetic Assessment database AVA~\cite{avaiaa}. We then elaborate the Aesthetic View () and additional regularization objectives.






\paragraph{The Aesthetic View.} \textit{Semantics} and \textit{Composition} are two key factors deciding the aesthetics of a video~\cite{aadb,cadb,distilliaa}. Thus, we obtain Aesthetic View (see Fig.~\ref{fig:dover_model}(b)) through \textit{spatial
downsampling}~\cite{bicubic} and \textit{temporal sparse frame sampling}~\cite{tsn} which preserves the semantics and composition of original videos. The downsampling strategies are widely applied in many existing state-of-the-art aesthetic assessment methods ~\cite{distilliaa,objiaa,racniaa,nima,gpfcnn}, further proving that they are able to preserve aesthetic information in visual contents. 
Moreover, the two strategies significantly 
{reduce the sensitivity}~\cite{videval, tlvqm, niqe, tpqi} on technical distortions such as \textit{blurs, noises, artifacts} (via spatial downsampling), \textit{shaking, flicker} (via temporal sparse sampling), so as to focus on aesthetics.



\begin{comment}
Given a video , the Aesthetic View  are formulated as:

where  is the blur kernel for downsampling,  is the remained frames after sparse sampling,  denotes uniform index sampling between . 
\end{comment}



\begin{comment}
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{ICCV_ViewDecomposition_compressed.pdf}
    \vspace{-9pt}
    \caption{Examples for (a) the Aesthetic View () and (b) the Technical View () under \textit{View Decomposition}, focusing on different perspectives.}
   \label{fig:asvtsv}
    \vspace{-16pt}
\end{figure}
\end{comment}



\paragraph{Cross-scale Regularization.} To better reduce technical impact in this branch, we obtain the over-downsampled view () during training by over-downsampling the videos with up to  downscaling ratio. We then observe that the  can barely keep any technical distortions but still remains similar aesthetics with  and even the original videos (see Fig.~\ref{fig:dover_model}(b) \textit{upper-right}). Furthermore, conclusions from existing study \cite{dissimilarity} suggest that feature dissimilarity among different scales (\textit{e.g.},  and ) is related to technical distortions. Henceforth, we impose the {Cross-scale Restraint} (, Fig.~\ref{fig:dover_model}(e)) as a regularization to further reduce the technical influences in the aesthetic prediction by encouraging the feature similarity between  and :

where  and  are output features for  and .

\begin{figure}    
\centering
   \includegraphics[width=\linewidth]{ICCV_DoverModel_compressed.pdf}
   \vspace{-6pt}
    \caption{The proposed \underline{D}isentangled \underline{O}bjective \underline{V}ideo Quality \underline{E}valuato\underline{r} (\textbf{DOVER)} and \textbf{DOVER++} via \textbf{(a)} View Decomposition (Sec.~\ref{sec:viewdecomposition}), with the \textbf{(b)} Aesthetic Branch (Sec.~\ref{sec:aesbranch}) and the \textbf{(c)} Technical Branch (Sec.~\ref{sec:tecbranch}). The equations to obtain the two views are in Supplementary \textbf{Sec. E}.} \label{fig:dover_model}
   \vspace{-16pt}
\end{figure}

\subsection{The Technical Branch}
\label{sec:tecbranch}
In the technical branch, we would like to keep the technical distortions but obfuscate the aesthetics of the videos. Thus, we design the Technical View () as follows.

\paragraph{The Technical View.} We introduce \frag~\cite{fastvqa} (as in Fig.~\ref{fig:dover_model}(c)) as Technical View () for the technical branch. The \frag~are composed of randomly cropped patches stitched together to retain the technical distortions. Moreover, it discarded most content and disrupted the compositional relations of the remaining, therefore severely corrupting aesthetics in videos.
Temporally, we apply \textit{continuous frame sampling} for  to retain temporal distortions.

\begin{comment}

where  denote positions of cropped patches,  is the number of grids where patches are cropped, and  denotes cropping a patch sized . 
\end{comment}

\paragraph{Weak Global Semantics as Background.} Many studies~\cite{dbcnn,fastvqa,fastervqa} suggest that technical quality perception should consider global semantics to better assess distortion levels. Though most content is discarded in , the technical branch can still reach 68.6\% accuracy for Kinetics-400~\cite{k400data} video classification, indicating it can preserve weak global semantics as background information to distinguish textures (\textit{e.g.}, sands) from distortions (\textit{e.g.}, noises).





\subsection{Learning Objectives}
\label{sec:objectives}

\paragraph{Weak Supervision with Overall Opinions.} With the observation in Sec.~\ref{sec:3c}, the overall  can be approximated as a weighted sum of \blue{} and \green{}. Moreover, the subjectively-inspired inductive biases in each branch can reduce the perception of another perspective. The two observations suggest that if we use overall opinions to separately supervise the two branches, the prediction of each branch could be majorly decided by its corresponding perspective. Henceforth, we propose the Limited View Biased Supervisions (), which minimize the relative loss\footnote{A criterion~\cite{qaloss} based on the linear and rank correlation between predictions and labels. Details provided in supplementary \textbf{Sec.~E}.} between predictions in each branch with the overall opinion , as the objective of DOVER, applicable on all databases:


\paragraph{Supervision with Opinions from Perspectives.} With the {DIVIDE-3k} database, we further improve the accuracy for disentanglement with the Direct Supervisions () on corresponding perspective opinions for both branches:

and the proposed \textbf{DOVER++} is driven by a fusion of the two objectives to jointly learn more accurate overall quality as well as perspective quality predictions for each branch:

\subsection{Subjectively-inspired Fusion Strategy} 
\label{sec:fusion}


From the subjective studies, we observe that the  can be well-approximated as . Henceforth, we propose to similarly obtain the final overall quality prediction () from two perspectives:  via a simple weighted fusion. With better accuracy on all datasets (Tab.~\ref{tab:abl2branch}), the strategy by side validates the subjective observations in Sec.~\ref{sec:3c}. 
















\label{sec:4d}









\begin{comment}
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{New_Fig_5_compressed.pdf}
    \vspace{-8pt}
    \caption{Two representation-level fusion (transfer learning) strategies. The \textit{head-only} transfer examines the representation abilities of  and , while the \textit{end-to-end} transfer evaluates the completeness of the views.}
    \label{fig:twofinetune}
    \vspace{-15pt}
\end{figure}
\end{comment}

\section{Experimental Evaluation}

In this section, we answer two important questions:

\begin{itemize}
    \item  Can the aesthetic and technical branches better learn the effects of corresponding perspectives (Sec.~\ref{sec:evadisentangle})? 
    \item  Can the fused model more accurately predict overall quality in UGC-VQA problem (Sec.~\ref{sec:evaoverall})?
\end{itemize}   
Moreover, we include ablation studies~(Sec.~\ref{sec:abl}) and an outlook for personalized quality evaluation~(Sec.~\ref{sec:personalized}). 









\subsection{Experimental Setups}
\label{sec:impdetail}

\paragraph{Implementation Details.} 
In the aesthetic branch, we use  with size  during inference and over-downsampled  size  to better exclude technical quality issues.  frames are sampled uniformly from each video and the backbone is inflated-ConvNext~\cite{convnext} \green{Tiny} pre-trained with AVA~\cite{avaiaa}. In the technical branch, we crop single patches at size  from  spatial grids and sample a clip of 32 continuous frames during training, and three clips during inference. The backbone of the technical branch is Video Swin Transformer~\cite{swin3d} \green{Tiny} with GRPB~\cite{fastvqa}.  is set as , and  is set as .









\paragraph{Datasets.} Despite evaluating DOVER and DOVER++ on the proposed \textbf{DIVIDE-3k} (3,590 videos) database, we also evaluate DOVER with the large-scale UGC-VQA dataset, LSVQ~\cite{pvq} (39,072 videos), and on three smaller UGC-VQA datasets, KoNViD-1k~\cite{kv1k} (1,200 videos), LIVE-VQC~\cite{vqc} (585 videos), and YouTube-UGC~\cite{ytugccc} (1,380 videos). 


\begin{table}
    \centering
    \footnotesize
    \renewcommand\arraystretch{1.2}
    \setlength\tabcolsep{8pt}
    \caption{\textbf{Quantitative Evaluation on Perspectives} of DOVER (weakly-supervised) and DOVER++ (fully-supervised) in the DIVIDE-3k, by evaluating the correlation across different predictions and subjective opinions. \textcolor{purple}{\textit{w/o} Decomposition} denotes both branches with original videos as inputs.}
    \vspace{-8pt}
    \resizebox{\linewidth}{!}{\begin{tabular}{l:c|cc}
        \hline
        Method & SROCC/PLCC & 
        \blue{} & \green{} \\ \hline
        \multirow{2}{79pt}{\textcolor{purple}{\textit{w/o} Decomposition} \\textit{w/} \&)}  &  \blue{}   &{0.7482/0.7576}& 0.7941/0.8039  \\
        & \green{}  &0.7234/0.7430& {0.8190/0.8233} \\ \hline
        \multirow{2}{75pt}{\textbf{DOVER}    \\\textcolor{purple}{\textit{w/o}} \&} &  \blue{}   &\underline{0.7489/0.7607}& 0.7877/0.8044  \\
        & \green{}  &0.7153/0.7382&\underline{0.8213/0.8295} \\ \hline
        \multirow{2}{75pt}{\textbf{DOVER++}  
  \\\textit{w/} \&} & \blue{} &\bred{0.7683/0.7779}&0.7584/0.7708 \\
        & \green{} &0.7015/0.7230&\bred{0.8376/0.8443} \\
         \hline
         
    \end{tabular}}
    \label{tab:disentanglement}
    \vspace{-9pt}
\end{table}

\begin{figure}
    \centering
    \vspace{-3pt}
    \includegraphics[width=0.98\linewidth]{ICCV_Qualitative_compressed.pdf}
    \vspace{-9pt}
    \caption{\textbf{Qualitative Studies on Perspectives} of DOVER/DOVER++: Visualizations of videos in the \textbf{DIVIDE-3k} where aesthetic and technical predictions are diverged. \textcolor{brown}{\textit{More visualizations in supplement. \textbf{Sec.~D}.}}}
    \vspace{-16pt}
    \label{fig:qualitativedivide}
\end{figure}


\subsection{Evaluation on Two Perspectives}
\label{sec:evadisentangle}

In this section, we quantitatively and qualitatively evaluate the perspective prediction ability of proposed methods in the \textbf{DIVIDE-3k} (Sec.~\ref{sec:60}). The divergence map and pairwise user studies further prove that the two branches in DOVER better align with human opinions on corresponding perspectives on existing UGC-VQA databases (Sec.~\ref{sec:6a}).

\subsubsection{Evaluation on the DIVIDE-3k}
\label{sec:60}

\paragraph{Quantitative Studies.} In Tab.~\ref{tab:disentanglement}, we evaluate the cross-correlation between the aesthetic and technical predictions in DOVER or DOVER++ and human opinions from the two perspectives in the DIVIDE-3k, compared with baseline (\textit{with respective labels as supervision, but without View Decomposition}). DOVER shows a stronger perspective preference than the baseline even without using the respective labels, proving the effectiveness of the decomposition strategy. DOVER++ more effectively disentangle the two perspectives with each branch around \textbf{7\%} more correlated with respective opinions than opinions from another perspective.



\paragraph{Qualitative Studies.} In Fig.~\ref{fig:qualitativedivide}, we visualize several videos with diverged predicted aesthetic and technical scores. The two videos with better aesthetic scores (Fig.~\ref{fig:qualitativedivide}\textbf{(a)\&(b)}) have clear semantics yet suffer from blurs and artifacts; on the contrary, the two with better technical scores (Fig.~\ref{fig:qualitativedivide}\textbf{(c)\&(d)}) are sharp but with chaotic composition and unclear semantics. These examples align with human perception of the two perspectives, proving that both variants can effectively provide disentangled quality predictions.



\vspace{-8pt}
\subsubsection{Evaluation on Existing UGC-VQA Datasets}
\label{sec:6a}
\paragraph{The Divergence Map.} In Fig.~\ref{fig:div}, we visualize the divergence map between predictions in two branches (trained and tested on LSVQ~\cite{pvq}) and examine the videos where two branches score most differently, noted in \orange{\textit{orange circles}}. Among these videos, the aesthetic branch can distinguish between bad (chaotic scene, Fig.~\ref{fig:div} \textit{downright}) and {good (symmetric view, Fig.~\ref{fig:div} \textit{upleft}) aesthetics}, while the technical branch can detect technical quality issues (\textit{blurs, over-exposure, compression errors} at Fig.~\ref{fig:div} \textit{upleft}).


\begin{figure}
    \centering
    \vspace{-3pt}
    \includegraphics[width=0.93\linewidth]{DOVER_Fig_1_compressed.pdf}
    \vspace{-12pt}
    \caption{The divergence map of technical and aesthetic predictions of DOVER in LSVQ~\cite{pvq} dataset. Similar as Fig.~\ref{fig:qualitativedivide}, the videos with diverged scores also align with human opinions of aesthetic and technical quality.}
    \vspace{-10pt}
    \label{fig:div}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.91\linewidth]{ICCV_Votes_compressed.pdf}
    \vspace{-9pt}
    \caption{\textbf{User Studies on Diverged Pairs} when technical and aesthetic branches in DOVER predict differently, demonstrating that predictions of each branch are more aligned with corresponding subjective opinions.}
    \vspace{-17pt}
    \label{fig:votes}
\end{figure}


\begin{comment}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{New_Fig_7_compressed_compressed.pdf}
    \vspace{-18pt}
    \caption{\textbf{gMAD}~\cite{gmad} between aesthetic branch and technical branch, where one evaluator predicts similar scores but another predicts remarkbly different. All scores are rescaled to between  (higher is better). Zoom-in for clearer view.}
    \label{fig:gmad}
    \vspace{-8pt}
\end{figure}



 \paragraph{gMAD Examples.} We conduct the group Maximum Differentiation~\cite{gmad} (gMAD) comparison between aesthetic branch and technical branch to find video groups with \textbf{similar} aesthetic branch prediction but \textbf{\textit{remarkbly different}} technical branch prediction (or \textit{vice versa}). As shown in Fig.~\ref{fig:gmad}, videos with ascending technical branch scores are with improving technical quality from \textit{blurs and artifacts} to \textit{clear textures}, and increasing aesthetic branch scores are with improving aesthetics from \textit{incomplete and occluded targets} to \textit{professional composition considering rule of thirds}), suggesting their sensitivity on respective aspects of quality.
\end{comment}

\begin{table*}[htbp]
\footnotesize
\setlength\tabcolsep{7pt}
\renewcommand\arraystretch{1.1}
\footnotesize
\center
\caption{Benchmark on official splits on the large-scale UGC-VQA dataset LSVQ~\cite{pvq}. First, second and third bests are labelled in \bred{red}, \blue{blue} and \textbf{boldface}.}\label{tab:peer}
\vspace{-8pt}
\resizebox{\linewidth}{!}{\begin{tabular}{l:ccc|cc|cc|cc|cc}
\hline
\textbf{Training Set: LSVQ~\cite{pvq}}& \multicolumn{3}{c|}{Inference Computational Cost} & \multicolumn{4}{c|}{Intra-dataset Evaluations}     &  \multicolumn{4}{c}{Generalization Evaluations}            \\ \hdashline
{\textbf{Testing Set}/} & \multicolumn{3}{c|}{\textit{on a 1080P, 10-second video}} & \multicolumn{2}{c|}{\textbf{LSVQ}}   & \multicolumn{2}{c|}{\textbf{LSVQ}}        &  \multicolumn{2}{c|}{\textbf{KoNViD-1k}}  & \multicolumn{2}{c}{\textbf{LIVE-VQC}}             \\ \hline
Methods    & GFLOPs & CPU Time & GPU Time & SROCC& PLCC  & SROCC& PLCC         & SROCC& PLCC                    & SROCC& PLCC                             \\ \hline 
\multicolumn{11}{l}{\textit{Classical Approaches (based on handcraft features):}}             \\ \hdashline


TLVQM (TIP, 2019) \cite{tlvqm} & NA & 248s & NA   & 0.772 &  0.774  & 0.589 & 0.616     & 0.732 &     0.724                   & 0.670 &  0.691 \\
VIDEVAL (TIP, 2021) \cite{videval} & NA & 895s & NA  & 0.795 &  0.783  & 0.545 & 0.554     & 0.751 &     0.741                   & 0.630 &  0.640 \\\hdashline   \multicolumn{10}{l}{\textit{Deep Approaches (based on deep neural network features):}}             \\ \hdashline 

VSFA (ACM MM, 2019) \cite{vsfa}  & 40919 & 466s & 11.1s     & 0.801 &  0.796  & 0.675 & 0.704     & 0.784 &     0.795                   & 0.734 &  0.772 \\

 Patch-VQ (CVPR, 2021) \cite{pvq} & 58501 & 539s & 13.8s & 0.814 &  0.816  & 0.686 &   0.708         & 0.781 &     0.781                   & 0.747 &  0.776                               \\ 
 Patch-VQ (CVPR, 2021) \cite{pvq}  & \multicolumn{3}{c|}{\textit{-- -- same as above -- --}}  & 0.827 &  0.828 & 0.711 &  0.739     & 0.791 &     0.795  & 0.770 &  0.807 \\ 
 {Li \textit{et al.} (TCSVT, 2022) }\cite{bvqa2021} & 112537 & 1567s & 27.6s &  {0.852} & {0.855} & {0.771} & {0.782} &{0.834} & {0.837}         &  {0.816} & {0.824} \\




{FAST-VQA (ECCV, 2022) } \cite{fastvqa} & \green{\textbf{279.1}} & \green{\textbf{8.8s}} & \green{\textbf{45ms}} & {\blue{0.876}} & {\blue{0.877}}  & {\blue{0.779}} & {\blue{0.814}} & \blue{0.859} & \blue{0.855} & {\blue{0.823}} & {\blue{0.844}}  \\ \hline



{\textbf{DOVER} (Ours)} & \green{\textbf{282.3}} & \green{\textbf{9.7s}} & \green{\textbf{47ms}}  &  {\bred{{0.888}}} & {\bred{{0.889}}}  & {\bred{{0.795}}} & {\bred{{0.830}}} & {\bred{{0.884}}} & {\bred{{0.883}}}& {\bred{{0.832}}} & {\bred{{0.855}}}  \\ \hdashline
\textit{Improvement to existing best} & -- & -- & -- & +1.3\% & +1.3\% & +2.0\% & +2.0\% & +2.9\% & +3.3\% & +1.0\% & +1.3\% \\
\hline
\end{tabular}}
\vspace{-7pt}
\end{table*}

\begin{comment}
\begin{table}
    \centering
    \footnotesize
    \renewcommand\arraystretch{1.2}
    \caption{Results of \textbf{Pairwise User Studies}: the concordance between subjective votes on \textit{\textbf{aesthetic quality}} or \textit{\textbf{technical quality}}, and relative predictions of the pair from either aesthetic or technical branch in DOVER.}
    \vspace{-8pt}
    \resizebox{\linewidth}{!}{\begin{tabular}{l|c|c|c}
        \hline
         Concordance between & \textbf{Aesthetic Predictions} & \textbf{Technical Predictions} & \textbf{MOS Labels} \\ \hline
         - \textbf{\textit{aesthetic quality}} & \textbf{69\%} (\textbf{138}/200) & 31\% (62/200) & 58\% (116/200) \\
         - \textbf{\textit{technical quality}} & 26\% (52/200) & \textbf{74\%} (\textbf{148}/200) & 62.5\% (125/200) \\
         \hline
    \end{tabular}}
    \label{tab:votes}
    \vspace{-14pt}
\end{table}
\end{comment}

\begin{table*}
\footnotesize
\caption{Performance benchmark on existing smaller UGC-VQA datasets. All experiments are conducted under 10 train-test splits.}\label{table:transfer}
\setlength\tabcolsep{7pt}
\renewcommand\arraystretch{1.1}
\footnotesize
\centering
\vspace{-8pt}
\resizebox{\textwidth}{!}{\begin{tabular}{l:l|cc|cc|cc|cc}
\hline
\multicolumn{2}{c|}{\textbf{Target (Fine-tuning) Quality Dataset}}       & \multicolumn{2}{c|}{\textbf{LIVE-VQC} (585)}   & \multicolumn{2}{c|}{\textbf{KoNViD-1k} (1200)}    & \multicolumn{2}{c|}{\textbf{YouTube-UGC} (1380)}   &  \multicolumn{2}{c}{\textit{Weighted Average}}        \\ \hline
\multirow{2}{120pt}{Methods} & \multirow{2}{75pt}{\textbf{Source (Pre-training) Quality Dataset}}& \multicolumn{2}{c|}{(240P - \textbf{1080P})}   & \multicolumn{2}{c|}{(540P)}      
 & \multicolumn{2}{c|}{(360P - \textbf{2160P(4K)})}         &  \\ 
     &           & SROCC& PLCC  & SROCC& PLCC         & SROCC& PLCC               &SROCC   & PLCC                             \\ \hline 

\nonecolor{lightcyan} TLVQM (TIP, 2019) \cite{tlvqm} & {NA (\textit{pure handcraft})}     & 0.799 &  0.803  & 0.773 & 0.768   & 0.669 &  0.659   & 0.732  & 0.726    \\

\nonecolor{lightcyan} VIDEVAL (TIP, 2021) \cite{videval} &   {NA (\textit{pure handcraft})}   & 0.752 &  0.751  & 0.783 & 0.780           & 0.779 &  0.773    &     0.772 & 0.772       \\ \hdashline
\nonecolor{lightcyan} RAPIQUE (OJSP, 2021) \cite{rapique} & {\textit{handcraft} + KoNiQ\cite{koniq}}       & 0.755 &  0.786  & 0.803 & 0.817                & 0.759 &  0.768    & 0.774 &     0.790 \\ 
\nonecolor{lightcyan} CNN+TLVQM (ACMMM, 2020) \cite{cnntlvqm}  &   {\textit{handcraft} + KoNiQ\cite{koniq}}   & 0.825 & 0.834 & 0.816 & 0.818  & 0.809 & 0.802 & 0.815 & 0.814  \\
\nonecolor{lightcyan} CNN+VIDEVAL (TIP, 2021) \cite{videval}   &  {\textit{handcraft} + KoNiQ\cite{koniq}}   & 0.785 & 0.810 & 0.815 & 0.817  & 0.808 & 0.803 & 0.806 & 0.810 \\
\hdashline
\nonecolor{lightcyan} VSFA (ACMMM, 2019) \cite{vsfa}  & \textit{None}        & 0.773 &  0.795  & 0.773 & 0.775   & 0.724 &  0.743   & 0.752 & 0.765 \\
\nonecolor{lightcyan} Patch-VQ (CVPR, 2021) \cite{pvq} & PaQ-2-PiQ\cite{paq2piq}  & {0.827} &  {0.837}  & 0.791 &   0.786        & NA &  NA   & NA & NA              \\
\nonecolor{lightcyan} CoINVQ (CVPR, 2021) \cite{rfugc} & \textit{self-collected} & NA &  NA & 0.767 &  0.764    & {0.816} &     {0.802} & NA & NA    \\ 
\nonecolor{lightcyan} Li \textit{et al.} (TCSVT, 2022) \cite{bvqa2021} & \textit{fused} (\cite{bid,spaq,livechallenge,koniq}) & 0.834 & 0.842 & 0.834 & 0.836 & 0.818 & 0.826 & 0.823 & 0.833  \\ 
\nonecolor{lightgray}
{{FAST-VQA} (ECCV, 2022)\cite{fastvqa}} &  LSVQ~\cite{pvq} &  {\blue{0.849}} & {\blue{0.862}} & {\blue{0.891}} & {\blue{0.892}} & {\blue{0.855}} & {\blue{0.852}}  & \blue{0.868} & \blue{0.869} \\ \hdashline
\nonecolor{lightgray} {\textbf{DOVER} (ours)} &   LSVQ~\cite{pvq} &  \textbf{\red{0.860}} & \textbf{\red{0.875}} & \textbf{\red{0.909}} & \textbf{\red{0.906}} & \textbf{\red{0.890}} & \textbf{\red{0.891}}  & \textbf{\red{0.891}} & \textbf{\red{0.891}}  \\ \nonecolor{lightgray} -- \textit{improvement to existing best} & & +1.6\% & +1.4\% & +2.0\% & +1.6\% & +3.9\% & +3.8\% & +2.6\% & +2.5\%  \\ 
\hline
\end{tabular}}
\vspace{-10pt}
\end{table*}







\paragraph{Pairwise User Studies.}
\label{sec:6b}
We further conduct \textbf{\textit{user studies}} to measure whether the two evaluators can distinguish the two perspectives on these diverged cases. Specifically, we evaluate on diverged pairs  where aesthetic branch predicts  is obviously better (\textit{at least one score higher when scores are in the range }) yet technical branch predicts  is obviously better. After random sampling 200 pairs in this way, we ask 15 subjects to choose {\textbf{which one has better {{aesthetic}} (or {{technical}}) quality in the pair}}. After post-processing the subject choices with popular votes, we calculate the agreement rates between subjective votes and predictions (in Fig.~\ref{fig:votes}). Each subjective perspective is notably more agreed with corresponding branch predictions, demonstrating that even without the respective labels, the DOVER can still learn to primarily disentangle the two perspectives. \textcolor{brown}{\textit{More details are in supplementary (\textbf{Sec.~B}).}}









\subsection{Evaluation on Overall Quality Prediction}
\label{sec:evaoverall}



\subsubsection{Results on Existing UGC-VQA Datasets}
\label{sec:benchmark}


\begin{table}
\footnotesize
\vspace{-3pt}
\caption{Performance benchmark on the \textbf{DIVIDE-3k}. All experiments are conducted under 10 train-test splits with random seed . }\label{table:divide3k}
\setlength\tabcolsep{5.8pt}
\renewcommand\arraystretch{1.2}
\footnotesize
\centering
\vspace{-8pt}
\resizebox{\linewidth}{!}{\begin{tabular}{l:l|ccc}
\hline
\multicolumn{2}{c}{Training/Testing on}    & \multicolumn{3}{c}{\textbf{DIVIDE-3k} (3590)}  \\ \hline
 Methods & \textbf{Pre-training Dataset} & SROCC& PLCC& KROCC\\ \hline
 TLVQM (2019)~\cite{tlvqm} & {NA (\textit{pure handcraft})}  & 0.6461 & 0.6807 & 0.4699 \\
 VIDEVAL (2021)~\cite{videval} & {NA (\textit{pure handcraft})} &  0.7056 & 0.7162 & 0.5233 \\ \hdashline
 RAPIQUE (2021)~\cite{rapique} & \textit{handcraft} + KoNiQ~\cite{koniq} &  0.7341 & 0.7547 & 0.5498 \\ \hdashline
 VSFA (2019)~\cite{vsfa} & NA & 0.7254 & 0.7386 & 0.5395 \\
 MDTVSFA (2021)~\cite{mdtvsfa} & NA & 0.7522 & 0.7409 & 0.5647 \\
  UNIQUE (2021)~\cite{unique} & \textit{fused} (\cite{bid,spaq,livechallenge,koniq}) & 0.7529 & 0.7637 & 0.5634 \\
 Li \textit{et al.} (2022)~\cite{bvqa2021} & \textit{fused} (\cite{bid,spaq,livechallenge,koniq}) & 0.7967 & 0.8125 & 0.6138 \\
 FAST-VQA (2022)~\cite{fastvqa} &  LSVQ~\cite{pvq} & \textbf{0.8184} & \textbf{0.8288} & \textbf{0.6285} \\ \hline
 \textbf{DOVER} (Ours) &  LSVQ~\cite{pvq} & \blue{0.8331} & \blue{0.8438} & \blue{0.6480} \\ \hdashline 
 
  \textbf{DOVER++} (Ours) &  LSVQ~\cite{pvq} & \bred{0.8442} & \bred{0.8537} & \bred{0.6603} \\ \hline 

\end{tabular}}
\vspace{-15pt}
\end{table}

\paragraph{Results on LSVQ.} In Tab.~\ref{tab:peer}, we train the DOVER on the large-scale UGC-VQA dataset, LSVQ~\cite{pvq}, and test it on five different existing UGC-VQA datasets. The proposed DOVER outperforms state-of-the-arts for intra-dataset evaluations by improving up to \bred{2.0\%} PLCC.
When testing on datasets other than LSVQ as generalization evaluation, the DOVER has shown more competitive performance. It improves PLCC on FAST-VQA by \bred{3.3\%} on KoNViD-1k, the UGC-VQA dataset with more diverse contents, further suggesting the importance of modelling from the aesthetic perspective in quality assessment on videos of diverse contents.



\begin{table}
\footnotesize
\caption{Zero-shot or cross-dataset evaluations on the \textbf{DIVIDE-3k}. None of the listed methods has been trained on the DIVIDE-3k. } 
\vspace{-8pt}
\label{table:zeroshotdivid3k}
\setlength\tabcolsep{10pt}
\renewcommand\arraystretch{1.2}
\footnotesize
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{l:l|ccc}
\hline
\multicolumn{2}{c}{Evaluating on}    & \multicolumn{3}{c}{\textbf{DIVIDE-3k} (3590)}  \\ \hline
 \multicolumn{5}{l}{\textit{Zero-shot (Opinion-Unaware) VQA Approaches:}} \\ \hdashline
 Methods & \textbf{Training on} & SROCC& PLCC& KROCC\\ \hline
 NIQE (2013)~\cite{niqe} & None & 0.3524 & 0.3839 & 0.2634 \\
TPQI (2022)~\cite{tpqi} &None & 0.4407 & 0.4432 & 0.3045 \\ 
 CLIP-IQA (2022)~\cite{clipiqa} &  CLIP~\cite{clip} & 0.5882 & 0.5910 & 0.4067 \\ 
 BVQI (2023)~\cite{buonavista} & CLIP~\cite{clip} & 0.6678 & 0.6802 & 0.4842 \\ \hdashline
  \multicolumn{5}{l}{\textit{Cross-dataset Evaluation (training on LSVQ):}} \\ \hdashline

 {Patch-VQ} (2021)~\cite{pvq} &  \multirow{3}{0pt}{LSVQ~\cite{pvq}} & 0.6454 & 0.6713 & 0.4489 \\ 
  {Li et al.} (2022)~\cite{bvqa2021} & & 0.7318 & 0.7524 & 0.5395 \\ \cdashline{1-1} \cdashline{3-5}
\textbf{DOVER} (Ours) &   & \bred{0.7727} & \bred{0.7806} & \bred{0.5799} \\  \hline 
\end{tabular}}
\vspace{-15pt}
\end{table}


\paragraph{Results on Smaller UGC-VQA Datasets.} Following \cite{fastvqa}, we pre-train the proposed DOVER on LSVQ instead of IQA datasets~\cite{koniq,bid,paq2piq} and then fine-tune the proposed method on three smaller UGC-VQA datasets and list the results in Tab.~\ref{table:transfer}. DOVER has reached unprecedented performance on all three datasets (mean PLCC ), and outperformed FAST-VQA with an average of \bred{2.6\%} improvement under exactly the same training process. The results further prove the effectiveness of considering aesthetic and technical perspectives separately and explicitly in UGC-VQA.

\subsubsection{Results on the DIVIDE-3k}
\label{sec:evadivide}

\paragraph{Training and Testing on DIVIDE-3k.} We first benchmark recent state-of-the-arts by conducting training and testing in the DIVIDE-3k. As shown in Tab.~\ref{table:divide3k}, the two semantic-unaware classical methods~\cite{tlvqm,videval} are performing notably worse and DOVER again achieves state-of-the-art. It is also noteworthy that with aesthetic and technical scores as auxiliary labels, DOVER++ further improves the performance for overall quality prediction. This further suggests that better modeling of the two perspectives can finally benefit overall quality assessment in the UGC-VQA problem.


\paragraph{Zero-shot and Cross-dataset Evaluations.} We also benchmark the opinion-unaware (\textit{i.e.} zero-shot) VQA approaches on the \textbf{DIVIDE-3k}. Among them, the recent BVQI~\cite{buonavista} reaches the best performance by considering both technical and semantic (aesthetic-related) criteria. Moreover, we benchmark the best approaches in Tab.~\ref{tab:peer} on the cross-dataset generalization from LSVQ to the DIVIDE-3k, where the proposed DOVER again outperforms other methods, suggesting the alignment between the proposed objective approach and subjective database.

\begin{comment}


\begin{table*}
\setlength\tabcolsep{2pt}
\renewcommand\arraystretch{1.15}
\footnotesize
\caption{Ablation study on \textbf{\textit{Aesthetic View and regularization strategies}} in aesthetic branch: effects on overall accuracy and accuracy of aesthetic branch only.} 
\vspace{-10pt}
\centering
\label{tab:ablaesthetic branch}
\resizebox{\linewidth}{!}{\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
\hline
& \multicolumn{5}{c|}{\textbf{Overall Accuracy of DOVER}} & \multicolumn{5}{|c}{\textbf{Accuracy of aesthetic branch Only} (only for reference)} \\
\hline
\textbf{Testing Set}/         & \multicolumn{1}{c|}{\textbf{LSVQ}}   & \multicolumn{1}{c|}{\textbf{LSVQ}}        &  \multicolumn{1}{c|}{\textbf{KoNViD-1k}}  & \multicolumn{1}{c|}{\textbf{LIVE-VQC}}  & \multicolumn{1}{c|}{\textbf{YouTubeUGC}}     & \multicolumn{1}{c|}{\textbf{LSVQ}}   & \multicolumn{1}{c|}{\textbf{LSVQ}}        &  \multicolumn{1}{c|}{\textbf{KoNViD-1k}}  & \multicolumn{1}{c|}{\textbf{LIVE-VQC}}  & \multicolumn{1}{c}{\textbf{YouTubeUGC}}                    \\ \cline{2-11}
Variants/Metric                   & SROCC/PLCC  & SROCC/PLCC         & SROCC/PLCC                    & SROCC/PLCC & SROCC/PLCC    & SROCC/PLCC  & SROCC/PLCC         & SROCC/PLCC                    & SROCC/PLCC & SROCC/PLCC \\ \hline                
\multicolumn{10}{l}{\textit{Group 1: Variants for Aesthetic View:}}\\\hdashline
\textit{cropping instead of downsampling} & 0.878/0.878 & 0.770/0.809 & 0.858/0.854 & 0.823/0.842 & 0.735/0.750 & 0.808/0.814 & 0.638/0.675 & 0.733/0.778         &  0.740/0.775 & 0.665/0.681 \\ 
\textit{keeping spatial aspect ratio} & 0.887/0.887 & 0.793/0.828 & 0.883/0.883 & 0.831/0.854 & 0.769/0.787 &  0.857/0.858 & 0.740/0.786 & 0.846/0.855 & 0.792/0.825 & 0.743/0.760 \\ \hdashline
\textit{temporal continuous frames} & 0.880/0.881 & 0.780/0.819 & 0.863/0.859 & 0.828/0.847 & 0.758/0.776 & 0.832/0.834 & 0.716/0.765 & 0.827/0.829 & 0.758/0.798 & 0.710/0.732 \\ 
\textit{temporal global random frames} & 0.883/0.884 & 0.788/0.824 & 0.868/0.867 & 0.830/0.849 & 0.764/0.780 & 0.843/0.845 & 0.726/0.777 & 0.833/0.842 & 0.778/0.813 & 0.737/0.758 \\ \hline
\multicolumn{10}{l}{\textit{Group 2: Variants for Regularization Strategies:}}\\\hdashline
\textit{w/o} Multi-scale Learning & 0.884/0.885 & 0.787/0.823 & 0.876/0.875 & 0.830/0.851 & 0.766/0.781 &  0.855/0.853 & 0.743/0.787 & 0.842/0.851 & 0.781/0.814 & 0.736/0.753 \\ \hline
\nonecolor{lightgray} \textbf{\textit{Accu. for technical branch only}} (for reference) &  0.877/0.878 & 0.778/0.812 & 0.861/0.855 & 0.825/0.844 & 0.730/0.746 & NA & NA & NA & NA & NA \\ 
\textbf{DOVER (Ours)} & \bred{0.888}/\bred{0.889} & \bred{0.795}/\bred{0.830} & \bred{0.884}/\bred{0.883} & \bred{0.832}/\bred{0.855} & \bred{0.772}/\bred{0.788} &  0.855/0.856 & 0.738/0.782 & 0.843/0.852 & 0.792/0.826 & 0.744/0.763 \\ 
\hline
\end{tabular}}
\vspace{-10pt}
\end{table*}

\end{comment}










\subsection{Ablation Studies}
\label{sec:abl}

\paragraph{Effects of View Decomposition.} In Tab.~\ref{tab:abllvbs}, we compare the proposed View Decomposition strategy with common strategies in UGC-VQA by keeping other parts the same. First of all, it is much better than the variant \textit{w/o Decomposition} that directly takes the original videos as inputs of both branches, showing the effectiveness of decomposition. Moreover, with backbone and input kept the same, DOVER with separate supervisions is also notably better than \textit{Feature Aggregation}, which first concatenates features from two branches together and then regress them to the quality scores, as applied by several existing approaches~\cite{bvqa2021,svqa,pvq}.




\begin{table}
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.2}
\footnotesize
\caption{\textbf{Ablation Study of DOVER (I):} the View Decomposition scheme.} 
\vspace{-9pt}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{l|c|c|c|c}
\hline
\textbf{Testing Set}/         & \multicolumn{1}{c|}{\textbf{LSVQ}}   & \multicolumn{1}{c|}{\textbf{LSVQ}}        &  \multicolumn{1}{c|}{\textbf{KoNViD-1k}}  & \multicolumn{1}{c}{\textbf{LIVE-VQC}}            \\ \cline{2-5}
Variants/Metric                   & SROCC/PLCC  & SROCC/PLCC         & SROCC/PLCC                    & SROCC/PLCC \\ \hline                

\textcolor{purple}{\textit{w/o} Decomposition} & 0.859/0.858 & 0.752/0.798 & 0.851/0.850 & 0.816/0.834 \\
\textit{Feature Aggregation} & 0.873/0.874 & 0.776/0.811 & 0.863/0.864 & 0.813/0.839 \\ \hline
\textbf{DOVER (Ours)} & \bred{0.888}/\bred{0.889} & \bred{0.795}/\bred{0.830} & \bred{0.884}/\bred{0.883} & \bred{0.832}/\bred{0.855}\\ 
\hline
\end{tabular}}
\label{tab:abllvbs}
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.17}
\footnotesize
\caption{\textbf{Ablation Study of DOVER (II):} Accuracy of single branch predictions and the effect of subjectively-inspired fusion (denoted as \textit{SIF}).} 
\vspace{-9pt}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{ccc|c|c|c|c}
\hline
\multicolumn{3}{c}{\textbf{Testing Set}/}         & \multicolumn{1}{c|}{\textbf{LSVQ}}   & \multicolumn{1}{c|}{\textbf{LSVQ}}        &  \multicolumn{1}{c|}{\textbf{KoNViD-1k}}  & \multicolumn{1}{c}{\textbf{LIVE-VQC}}            \\ \hline
 &  &  \textit{SIF}       & SROCC/PLCC  & SROCC/PLCC         & SROCC/PLCC                    & SROCC/PLCC\\ \hline                

\cmark & & &  0.855/0.856 & 0.738/0.782 & 0.844/0.853 & 0.792/0.826 \\
& \cmark & &  0.877/0.878 & 0.778/0.812 & 0.861/0.855 & 0.825/0.844 \\ 
\hline
\cmark & \cmark  & & 0.885/0.886 & 0.792/0.826 & 0.880/0.880 & 0.829/0.849 \\  \hdashline
\rowcolor{lightpink} \cmark & \cmark & \cmark & \bred{0.888}/\bred{0.889} & \bred{0.795}/\bred{0.830} & \bred{0.884}/\bred{0.883} & \bred{0.832}/\bred{0.855}\\ \hline
\end{tabular}}
\label{tab:abl2branch}
\vspace{-10pt}
\end{table}

\paragraph{Effects of Subjectively-Inspired Fusion.} We discuss the fusion strategy in Tab.~\ref{tab:abl2branch}. As shown in the table, only considering one branch will bring a notable performance decrease, and directly obtaining the fused quality as  without weights is also less accurate than subjectively-inspired fusion. These results further validate the subjective observations found in the DIVIDE-3k.

\paragraph{Ablation Studies of DOVER++.} In Tab.~\ref{tab:abldoverpp}, we further discuss whether the extra objective () can improve accuracy of overall quality prediction. By combining  with , it contributes to around 1\% performance gain. It is also noteworthy that even without direct  labels for supervision, the  only can still outperform . All these results suggest that explicitly considering ``quality" in UGC-VQA into a sum of two perspectives is a good approximation to the human perceptual mechanism.

\subsection{Outlook: Personalized Quality Evaluation}
\label{sec:personalized}

 During the subjective reasoning study, we further find out that the effect of each perspective varies among different individuals. For instance, the video in Fig.~\ref{fig:variantreasoning}{(a)} has {\rblue{better aesthetics}} and \red{worse technical quality} (\textit{blurry, under-exposed}), and different individuals consider the technical impact differently while rating the overall opinion (Fig.~\ref{fig:variantreasoning}{(b)}).
 Moreover, with more consideration of the technical perspective, subjects tend to rate lower scores on the video. With DOVER++, if we adaptively fuse between  and , we find that the differently-fused results can better predict the quality perception of individual subject groups, suggesting its primary capability to provide quality evaluation catering for personalized requirements.


















\begin{comment}

\subsection{Effects on Concrete Designs} 
\label{sec:abl2}
In the next part, we discuss the effects of different concrete designs. As the Technical View in the technical branch are following designs validated in FAST-VQA~\cite{fastvqa}, we only evaluate effects of designs on Aesthetic View, pre-training and regularization strategies (Eq.~\ref{eq:mslearning}) in the aesthetic branch.

\paragraph{Effects on Aesthetic View.} We discuss the design of Aesthetic View in Group 1 of Tab.~\ref{tab:ablaesthetic branch}. Specifically, we prove that the spatial \textit{downsampling} is far better than \textit{cropping}: the \textit{cropping} does not only have worse performance for the aesthetic branch itself but also brings almost no contributions to the overall accuracy. Keeping spatial aspect ratio also leads to a bit worse overall performance. Similarly, the temporal \textit{sparse sampling} is also far better than \textit{continuous frames}. The spatial and temporal comparisons demonstrate that retaining all objects and their organizations is important in preserving the aesthetics of videos.



\paragraph{Effects on Regularization Strategies.} The effects of the multi-scale learning for the aesthetic branch (as in Eq.~\ref{eq:mslearning}) are shown in Tab.~\ref{tab:ablaesthetic branch} Group 2. On the one hand, it does not improve the independent accuracy of the aesthetic branch. Nevertheless, it proves better overall accuracy on all evaluation sets. The results have suggested the multi-scale learning further avoids the technical effects for the aesthetic branch and effectively diversifies the two evaluators, contributing to the final disentanglement.

\end{comment}







\begin{table}
\setlength\tabcolsep{7pt}
\renewcommand\arraystretch{1.1}
\footnotesize
\caption{\textbf{Ablation Study of DOVER++}: Effects of different objectives.} 
\vspace{-9pt}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l:cc|ccc}
\hline
 \multicolumn{3}{c|}{Loss Objectives}                 & \multicolumn{3}{c}{\textbf{DIVIDE-3k} (3590)} \\ \hline
  &  &  & SROCC       & PLCC & KROCC \\
\hline
\textit{w/o} \&   & \cmark  &   & 0.8331 & 0.8438 & 0.6480 \\ \hdashline
\multirow{2}{75pt}{\textit{w/} \&}   &   & \cmark  & \blue{0.8357} & \blue{0.8455} & \blue{0.6521} \\ 
& \cellcolor{lightpink} \cmark & \cellcolor{lightpink} \cmark & \cellcolor{lightpink}\bred{0.8442} & \cellcolor{lightpink}\bred{0.8537} & \cellcolor{lightpink}\bred{0.6603} \\
\hline
\end{tabular}}
\label{tab:abldoverpp}
\vspace{-10pt}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{ICCV_SubjectVary_compressed.pdf}
    \vspace{-8pt}
    \caption{For the video \textbf{(a)}, the impact of aesthetic and technical perspectives on the final quality rating \textbf{(b) varies among individuals}. By adjusting fusion weights, DOVER++ can align with opinions from different groups.}\label{fig:variantreasoning}
    \vspace{-15pt}
\end{figure}




\section{Conclusion}

In this paper, we present the DIVIDE-3k database and the first subjective study aimed at exploring the impact of aesthetic and technical perspectives on UGC-VQA, which reveals that both perspectives impact human quality opinions. In light of this observation, we propose the objective quality evaluators, DOVER and DOVER++, that achieve two objectives: \textbf{1)} significantly improving overall UGC-VQA performance; \textbf{2)} decoupling effects of two perspectives, so as to be applicable to specific real-world scenarios where pure technical or aesthetic quality metrics are needed.









\clearpage
{\small
\bibliographystyle{IEEEtran}
\bibliography{egbib}
}

\end{document}