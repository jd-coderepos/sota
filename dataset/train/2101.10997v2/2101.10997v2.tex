\begin{figure*}[t]
    \centering \includegraphics[trim = 0 210 55 0, width=0.95\textwidth]{figures/network_overview_v3}\caption{An overview of our burst super-resolution architecture. Each image  in the input burst is first passed independently through the encoder. The resulting feature maps are then warped to the base frame () coordinates using the flow vectors  predicted by the alignment module. The aligned feature maps are then merged using an attention-based fusion module, using fusion weights computed by the weight predictor. The merged feature map  is then passed through to decoder module to obtain the super-resolved RGB image as output.}\vspace{-4mm}
    \label{fig:network_overview}
\end{figure*}

\section{Burst Super-Resolution Network}
In this section, we describe our burst super-resolution network. Our network inputs multiple noisy, RAW, low-resolution (LR) images captured in a single burst.
The architecture processes and combines the information in individual images to generate a high-resolution (HR) RGB image as output. Thus, our network performs joint denoising, demosaicking, and SR. Since the images in a burst are captured in a rapid sequence from a hand-held device, they include small inter-frame offsets. This ensures multiple aliased versions of the same scene, providing additional signal information for SR. Consequently, by effectively merging the information from the whole burst, our network can better reconstruct the underlying scene to generate a higher quality output, compared to single frame approaches. 

An overview of our architecture is shown in Figure~\ref{fig:network_overview}. Our network takes a RAW burst sequence  of any arbitrary size  as input. Here, each image  is the RAW sensor data obtained from the camera. The images in the burst are first encoded independently in order to obtain deep feature representations . Next, we align and warp each of the feature maps to a common reference frame  using the offsets estimated by an alignment network. The aligned feature maps are then combined by our fusion module to obtain a merged feature map . We propose an attention-based fusion approach that predicts element-wise fusion weights. This allows the network to adaptively select the most useful information from each image in the burst. The merged feature map is then passed to the decoder module which outputs the final RGB image , where  is the super-resolution factor. We detail each network module of our architecture in the subsequent sections.

\subsection{Encoder} 
The encoder module  independently maps each input burst image  to a deep feature representation . To ensure translational invariance, we first pack each  block in the raw Bayer pattern along the channel dimension, obtaining a 4 channel image  at half the initial resolution. This LR image is passed through the encoder, consisting of an initial convolutional layer followed by a series of residual blocks. In order to achieve a high-dimensional encoding that allows more effective fusion of several frames, we radically expand the feature dimensionality with a final convolutional layer. The resulting -dimensional encoding  thus achieves a rich embedding of the input image. We use  in our experiments.

\subsection{Alignment Module} 
One of the important challenges in burst SR is that the pixel-wise displacement between the images is unknown. The displacements stem from both global camera motion and scene variations. In order to achieve an effective fusion of multiple frames, the information first needs to be aligned. We address this problem by explicitly aligning the individual image embeddings  to a common reference LR image, called the \emph{base frame}. For convenience, we let the first image  denote the base frame.
Camera motion is often modelled using a homography when imaging static and distant scenes. However, we found these assumptions to seldom hold in the real-world scenario. Thus, we allow greater flexibility in our alignment module by computing dense pixel-wise optical flow  between every burst image  and the reference image . Pixel-wise flow can capture global camera motion while also accounting for any object motion in the scene. The estimated flow vectors  are then used to warp the feature maps  to the base frame using a bilinear kernel

Here,  denotes the warping operation,  is the flow estimator, while  is the warped feature map. The warped feature maps , as well as the computed flow vectors  are then passed to the fusion module. Here, the flow vectors  for the base frame is set to 0.
While any state-of-the-art optical flow network~\cite{IMKDB17,Sun2018PWCNetCF,Teed2020RAFTRA,GOCor_Truong_2020} can be employed as our flow estimator , we use the PWC-Net~\cite{Sun2018PWCNetCF} approach due to it's high accuracy and speed. 
Since PWC-Net is trained to operate on RGB images, we discard one of the two green channels in  to generate input RGB images. 

\subsection{Fusion Module} 
The fusion module combines information across the individual burst images to generate a merged feature embedding . In order to be able to operate on bursts of arbitrary sizes, the fusion module must be able to merge any number of input frames. Consequently, it is infeasible to \eg directly concatenate the input feature maps along the channel dimension. We further found simple pooling operations such as element-wise max or average pool across the burst to provide unsatisfactory results. This is because the fusion module needs be able to merge adaptively based on \eg image content, noise levels, etc. For instance, it can be beneficial to have uniform fusion weights for textureless regions in order to perform denoising. On the other hand, it is preferable to have low fusion weights for any mis-aligned frame in order to avoid ghosting artifacts.  
We therefore propose an attention-based fusion approach, where element-wise fusion weights are predicted by a weight predictor network . This provides flexibility to the network to effectively extract the useful information from each image, while also being able to process arbitrary number of input images. 


The weight predictor network  utilizes both the aligned feature maps  and the flow vectors  to estimate the unnormalized attention weights  for each embedding . We first project  to a lower dimension feature map  for computational efficiency. To compute the attention weights for , we use the projected base frame feature map , as well as the residual  between  and . The base frame map  contains information about the local image content. This is informative to determine \eg whether to use uniform fusion weights to achieve denoising, or perform edge-aware fusion in order to avoid over smoothing edges. On the other hand, the residual  can provide an estimate of alignment errors and thus help assign low fusion weights to misaligned regions. Additionally, we use the flow vectors  for weight estimation as they provide the sub-pixel sampling location of the image data. 
We obtain the sub-pixel offset by computing modulo  of the flow vectors  and pass it through a small CNN to obtain the flow features . The reference frame features , the feature residual , and the flow features  are concatenated along the channel dimension and passed through a residual network to obtain the raw fusion weights . The raw fusion weights are then normalized across the burst using a softmax function to obtain the final attention weights . The merged feature map  is then be obtained as the following weighted sum,

Here,  denotes element-wise multiplication. The merged feature map  is then passed to the decoder module to generate the final output.

\subsection{Decoder}
The decoder module generates the output high-resolution RGB image from the fused feature map . We first project the input feature map to  channels and pass it through a residual network. Next, we upsample this to the desired resolution  using sub-pixel convolution~\cite{Shi2016RealTimeSI}. We use a convolution layer to increase the feature dimension to , obtaining a tensor of shape . The feature vectors at each spatial location are then re-arranged into a  map to obtain a higher resolution feature map of shape . Here,  is the output feature dimension of the sub-pixel convolution layer.
Compared to performing na\"ive upsampling using \eg bilinear interpolation, sub-pixel convolution allows us to effectively decode the sub-pixel information encoded in the different feature channels. 
In order to avoid checkerboard artifacts, we use the ICNR initialization~\cite{Aitken2017CheckerboardAF} for the sub-pixel convolution layer and additionally apply Gaussian smoothing to its output. 
The upsampled feature map is then passed through another set of residual blocks, followed by a conv layer to obtain the high resolution RGB image .

\section{BurstSR Dataset}
The aim of this work is to propose a burst SR method for real-world photography applications. 
In order to validate the performance of our approach, it is essential to train and evaluate our models on real data. 
Hence, we collect a new dataset, called BurstSR. To the best of our knowledge, it is the first real world burst super-resolution dataset.
The BurstSR dataset consists of  RAW burst sequences, and corresponding high-resolution ground truths. Each burst sequence contains  RAW images captured using identical camera settings (\eg exposure, ISO). All bursts are captured using a handheld smartphone camera. Our dataset therefore contains natural hand tremors, resulting in small random offsets between the images within a burst that are essential for MFSR~\cite{Tsai1984MultiframeIR}.
For each burst sequence, we also capture a high-resolution image using a DSLR camera mounted on a tripod to serve as ground truth. 
Our BurstSR dataset will be released upon publication. We believe that it can serve as an important training set and benchmark for the community, in order to raise the interest in the important MFSR problem.

We capture the burst images in our datset using a handheld Samsung Galaxy S8 smartphone camera. 
In order to capture and store RAW bursts, we developed a custom app using Camera2 API. On pressing the shutter, the app runs the camera's auto-focus, auto-exposure, and auto-white-balance algorithms to determine the camera settings. These settings are then used to capture a fixed number of RAW images. 
The corresponding ground truth images for each burst are collected using a Canon 5D Mark IV DSLR camera mounted on a tripod. We use a zoom lens with a focal length of 70mm to obtain images with  times higher spatial resolution compared to burst images captured from the phone camera. The images are taken using a smaller aperture size (F18) to have a wider depth of field. Other capture settings are automatically determined by the camera. We hold the phone camera just above the DSLR when taking bursts in order to minimize misalignments between the two images. Additionally, we use a timer on the DSLR to synchronize the capture time between the two cameras. In order to minimize the effect of any error in temporal synchronization, we try to capture static scenes with little (e.g.\ leaves moving due to wind) or no motion. 
We collect  bursts in total, which are split into train, validation, and test sets consisting of , , and  sequences, respectively.



\section{Training}
In this section, we describe our training pipeline in detail. Due to the high cost and effort associated with collecting real-world paired data for MFSR, it is impractical to obtain large scale real world datasets for training our model from scratch. 
We therefore exploit methods for synthetic data generation to first pre-train our networks. The resulting model serves as a strong initialization, which is then finetuned on our BurstSR dataset to perform real-world SR. 

\subsection{Synthetic data training}
\label{sec:synthetic_training}
We generate synthetic RAW bursts for pre-training our model using the sRGB images from the training split of Zurich RAW to RGB dataset~\cite{ignatov2020replacing}. Given a sRGB image, we apply the inverse camera pipeline described in~\cite{Brooks2019UnprocessingIF} to obtain raw sensor values. 
Next, we generate a synthetic burst of size  by applying random translations and rotations to the converted RGB image. The translation and rotation values are sampled independently from the range [-24, 24] pixels and [-1, 1] degrees, respectively. The transformed images are then downsampled by the desired super-resolution factor  to obtain the low resolution RGB burst. We use bilinear kernel for both image translation/rotation and downsampling. Next, we add shot and read noise to the burst images, as described in~\cite{Brooks2019UnprocessingIF}.  We then discard two color channels per pixel according to the Bayer CFA to obtain the mosaicked RAW burst. We extract  crops from the resulting RAW burst for our training. Our network is trained in a fully supervised manner by minimizing the  loss between the network prediction and the ground truth image. The loss is computed in the linear sensor space, before any post processing \eg gamma compression or tone-mapping.



\subsection{Real data training}
\label{sec:real_data_training}
In order to reconstruct the HR image using multiple aliased LR observations, a MFSR model needs to learn the image formation process in a camera. 
However, due to differences in the image formation process in a real camera and the one modelled by our synthetic pipeline, a network trained on only synthetic data is thus expected to have sub-optimal performance when applied to real data. Hence, we fine-tune the pre-trained synthetic data model on our real world BurstSR dataset in order to adapt the model to the particular camera sensor. 

\parsection{Data Processing}
Here, we describe the pipeline used to pre-process the collected BurstSR data for training. Since the images captured using phone and DSLR cameras have different field of views (FOV), we first crop out matching field of view from each image in the burst. This is done by estimating a homography between the first image in the burst and DSLR image using SIFT~\cite{Lowe1999ObjectRF} and RANSAC~\cite{Fischler1981RandomSC}. Next, we extract  crops from the burst images in a sliding window manner, with a stride of  pixels. For each crop, we again estimate homography between the crop and the corresponding region in DSLR image to perform local alignment. The aligned DSLR image region is then downsampled to  to obtain the ground truth crop. In order to filter out crops with incorrect alignment, we discard phone-DSLR pairs which have a normalized cross correlation of less than  between them. 

\parsection{Training loss} There are several challenges when training our model on real bursts due to the unavoidable mis-alignments between the input burst and the ground truth. Firstly, even though we align the burst images to DSLR using homography, there can still be misalignments between the pair due to perspective shifts, error in homography estimation, etc. Secondly, since the burst and ground truth images are captured using two different sensors, there is a color mis-match between the two. Thus, it is not feasible to train the model by directly computing a pixel-wise error between the network prediction  and the ground truth . 

In order to handle the spatial mis-alignment issue, we first estimate the optical flow  between the prediction and ground truth using PWC-Net. The estimated flow is then used to warp the network prediction to the ground truth co-ordinates. Next, we estimate a global color mapping between the burst and the ground truth in order to handle the color mis-match. We first downsample the ground truth image to the same resolution as the input burst images. The estimated flow  is then used to align the first image in the burst to the downsampled ground truth. In order to minimize the effect of small mis-alignments, we apply Gaussian smoothing on both the images to obtain the processed burst image  and ground truth image . Given this aligned input-ground truth pair, we estimate a pixel-wise color mapping  between the two images. We assume that the color mapping is linear and model it as a  color correction matrix, which is computed by minimizing a least squares loss. Using the estimated color correction matrix, we can map the network prediction to the same color space as the ground truth and compute pixel-wise error.
Our training loss  is thus computed as

Here,  is the aligned and color mapped network prediction. The summation is over all pixel coordinates  in the image. The factor  is a binary masking variable used to filter out image regions which are not aligned correctly. It is set to  in regions where the error  after color mapping the processed burst image  is greater than a threshold. Note that the images  and  have lower-resolution compared to the model prediction . Thus the error map  is upsampled to the same resolution as , before computing the mask .

\subsection{Training details}
We use pre-trained PWC-Net weights for our flow estimator . All other modules are initialized using~\cite{He2015DelvingDI}. Our model is first trained using the synthetic data for 100k iterations, and then fine-tuned on the BurstSR dataset for an additional 40k iterations. We use the ADAM~\cite{Kingma2015AdamAM} optimizer for out training. Data augmentation is performed using random cropping and flipping. Our entire training takes 30 hours on a single Nvidia V100 GPU. All our networks are trained using a burst size of 8.
