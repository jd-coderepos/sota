


In light of the attacks presented in
Section~\ref{sec:manipulation_attack}, we first extend the rudimentary
contextual attacker model from \cite{TruongPerCom14} as follows:


\begin{itemize} \itemsep0em 


\item We allow \multimodal attackers who can simultaneously control
multiple sensor modalities, in addition to the \singlemodal attacker of
\cite{TruongPerCom14}. 


\item 
We assume that a contextual attacker can
manipulate radio contexts in \emph{both directions}. The same 
assumption applies to Gas sensors in light of our aerosol spray attack.
\end{itemize}



\subsection{Analysis Methodology} \label{sec:analysis_method}



\verOne{ 

\subsubsection{Datasets}

To fairly evaluate the resilience of co-presence detection systems in the
presence of our contextual attacker, we used the same datasets and the same set
of features originally used to evaluate the systems in question. The previous
audio-radio system \cite{TruongPerCom14} used a dataset 
to evaluate resistance against \singlemodal attackers. The previous physical system
\cite{ShresthaFC2014} used a dataset 
to model a \zeromodal attacker. We use these datasets to evaluate the
resistance of the respective systems against \multimodal attackers.  In
addition, 
we conducted new audio relaying experiments to collect data and evaluate audio-based co-presence detection performance.
Furthermore, we collected a new dataset corresponding to the
audio-radio-physical system (which was not considered in prior works). 

\textbf{Data-PerCom}. The dataset from Truong et al. \cite{TruongPerCom14} contains 2303 samples, of which 1140 samples (49.5\%) are from co-present devices and 1163 (50.5\%) from non co-present devices. Each sample contains data from sensor modalities available at the time on the respective devices (2117 with audio, 1600 with Bluetooth, 782 with GPS and 2269 with Wi-Fi).  Recording time varies from sensor to sensor: 2 minutes for GPS scanning, 10 scans for Wi-Fi (about 30 seconds), 10 seconds for ambient audio, and 10 scans for Bluetooth (up to 12 seconds for each scan).

\textbf{Data-FC}. The data from Shrestha et al. \cite{ShresthaFC2014} contains sensor data 
for ambient temperature, precision gas, humidity, and 
altitude. Data was recorded and labeled according to the location and time
of the place. The data was also marked how the device was held, i.e.,
either in hand or in pocket. The experiment was conducted in a variety of
places, not just confined to labs and typical university offices. The 
locations included: parking lots, office premises, restaurants,
chemistry labs, libraries as well as halls with live performance and 
driving on interstate highways. We collected a total of 207 samples at 21
different locations. The different samples collected from the same place are
``paired'' to generate co-presence data instances whereas those from different
places are paired to generate non co-presence data instances. We ended up with
21320 instances of which 20134 instances belonging to non co-presence class and
1186 instances belonging to co-presence class.

\textbf{Data-TMC-audio}. To assess how an attacker can manipulate ambient audio via the
streaming attack (Section \ref{sec:audiomanipulate}), we conducted a
set of experiments to collect about 100 audio samples for the non
co-presence case. The audio streaming was done over two different channels:
Wi-Fi and cellular data. \prover was a Galaxy Nexus device while
\verifier was a Galaxy S3 device. Unidirectional streaming of the audio from
\prover's side to \verifier's side was done between a pair of devices (from
a Galaxy S4 to an iPhone 5 in the case of the cellular data channel,
and from a MacBook Air to a ThinkPad Carbon X1 in the Wi-Fi
channel). The attacker devices used a Skype connection as the audio
relay channel.

\textbf{Data-TMC}. We extended the data collector used in \cite{TruongPerCom14} to record physical
sensor data using an attached Sensordrone device (as used in
\cite{ShresthaFC2014}). Different device models were used to record sensor
data. Each device, in a pair of devices, was connected to its own Sensordrone
device. Two users were involved in the data collection. Data was collected at
different locations in two countries for ten days. The resulting dataset has
203 non co-presence samples and 335 co-presence samples.






\subsubsection{Features for co-presence detection}
We summarize features used for co-presence detection in prior works.

\textbf{Audio features}. Halevi et al. \cite{DBLP:conf/esorics/HaleviMSX12} proposed the use of (only) audio
for co-presence detection. Audio features include max cross correlation and time frequency distance.

\begin{itemize}
\item Max cross correlation: \\ 
\item Time frequency distance: \\ 
where, ,  =
 is the Euclidean norm of the distance.
\end{itemize}
Here  and  denote the raw (16-bit PCM) audio signals recorded by  and  and FFT(),
FFT() denotes the Fast Fourier Transforms of the corresponding signals.

\textbf{Radio (Bluetooth, Wi-Fi, GPS) features}. Truong et al. \cite{TruongPerCom14} used a set of features with small variance for radio sensor modalities. 
They let a sample from an RF sensor modality be of
the form (, ) where  is an identifier of a sensed device and
 is the associated signal strength.  Also, they let  and  denote the
set of records sensed by a pair of bound devices  and , and let
 and  denote the number of different beacons (i.e., Wi-Fi
access points, satellites or Bluetooth devices) observed by devices
 and . We define the following sets:

\noindent . \\
\noindent . \\
\noindent , . \\
\noindent  =  . \\
\noindent  =    
, \\\hspace*{1.3cm}  is modality-specific (see below). \\
\noindent . \\
\noindent . \\
\noindent . \\
\noindent
.

 consists of devices seen by both  and ;
 represents all devices seen by  or  with
 filled in as the ``signal strength'' for devices that are
\textit{not} seen by either device.  
\begin{enumerate}

\item \label{eq:jaccard}Jaccard distance: 


\item \label{eq:hamming}Mean of Hamming distance: 


\item \label{eq:euclid}Euclidean distance: 


\item \label{eq:expdif}Mean exponential of difference: 


\item \label{eq:sumsqranks}Sum of squared of ranks: 



where,
 (respectively ) is the rank of 
() in the set  () sorted in
ascending order.



\item \label{eq:subsetcnt} Subset count: .
Here  is the scanning time (seconds)

  if , , \\\hspace*{1.4cm}()

  otherwise. ,  are the set of records by 
 and  respectively at the  second

\end{enumerate}





Features~\ref{eq:jaccard}-\ref{eq:sumsqranks} are used for Wi-Fi ( is -100.). Features~\ref{eq:jaccard}-\ref{eq:euclid} are used with BDADDR as 
identifier () and average RSSI as signal strength () for Bluetooth( is -100). 


\textbf{Physical sensor features}. Shrestha et al. \cite{ShresthaFC2014} used Hamming distance as a feature on physical sensors for co-presence. 
Let  and  be a sensor reading captured by two devices at
locations  and . The Hamming distance is calculated as follows:

\begin{center}

\end{center}

Given n different sensor modalities and the input data for the k modality
 and  from
two samples, we have . With the 
data corresponding to  modalities, we obtain a feature vector of
 elements of .


\subsubsection{Classification techniques}

In evaluating prior systems, we used the same classification techniques as
in the original evaluations (Decision Tree and Random Forest), implemented in
Scikit-learn~\cite{scikit-learn}. The results are reported after running
ten-fold cross validation. 
We use \textit{False Positive Rate (FPR) as a metric to 
represent the attacker's success probability}.  FPR corresponds to ``non
co-presence'' samples which are mislabeled as ``co-presence'', reflecting the
security of the system (higher the FPR, lower the security).  We use False
Negative Rate (FNR) as a metric to represent the usability of the system. FNR
represents ``co-presence'' samples that are mislabeled as ``non co-presence''
(lower the FNR, better the usability).  F1 score is reported only for the
overall performance of the classification model under \zeromodal attack.



Whenever multiple sensor modalities are used, we fuse the data from these
modalities before feeding it to the classifier. We considered the following
fusion approaches.



\textbf{Features-fusion}. The features of all sensor modalities are together fed to the classifier (see Fig. \ref{fig:feature-fusion}). The
decision of co-presence or non co-presence is made one-time only based on the output of the prediction model. Prior work \cite{TruongPerCom14,ShresthaFC2014} implemented this fusion technique.

\begin{figure}[htpb] 
	\centering
	\includegraphics[scale=0.2]{figures/feature-fusion.pdf}
	\caption{Features-fusion technique.}
	\label{fig:feature-fusion}
\end{figure} 


\textbf{Decisions-fusion}. Each of the  sensors (with all its features) is used separately by the
classifier. As result there are  decisions made.  All decisions are then
combined to produce a final decision. This is an approach that has not been
used for co-presence detection in previous works. Decisions-fusion can
aggregate decisions from single sensor modalities or from subsets of sensor
modalities, for example, three subsets can be built on top of seven sensors:
acoustic = \{\audio\}, radio = \{\bluetooth, \wifi\}, physical = \{\altitude,
\gas, \humidity, \temperature\}. In the latter fusion approach, classifiers of
subsets are built using features-fusion. Fig. \ref{fig:decision-fusion} illustrates decision-fusion technique.

\begin{figure}[htpb] 
	\centering
	\includegraphics[scale=0.2]{figures/decision-fusion.pdf}
	\caption{Decisions-fusion technique.}
	\label{fig:decision-fusion}
\end{figure} 

} 

\begin{comment}
We consider two methods to aggregate decisions. The first method is based on a simple
majority voting
(hereafter referred to as \emph{equal voting})
 which takes binary decisions from all sensors with equal
weights. The second method 
is a novel variant, we call \emph{weighted voting}, which
fuses decisions with different weights assigned to
each sensor. We start with a list of sensors, , sorted by the order
of attack resilience according to the single modality attack, the weight assigned
to  is computed as: , where ,  is the
position of  in the sorted list. As an example, the decisions-fusion weighted voting for Audio-Radio is done as follows. Sensors \audio, \bluetooth, \wifi have performance: 3\%,
2.7\%, 99.8\% (Table \ref{tbl:all-mani}), respectively. The sorted
list is therefore [\wifi, \audio, \bluetooth]. When these sensors are fused, their weights will
be assigned as: 1/6 for \wifi, 2/6 for \audio and 3/6 for \bluetooth. Note that
in equal voting, the weight of each sensor is 1/3. 
When more sensors are fused, weights will be adapted with similar scheme.
}

\end{comment}








\subsection{Analysis Results}

\subsubsection{Audio-Only System}
\label{sec:audio-only}
Halevi et al. \cite{DBLP:conf/esorics/HaleviMSX12} proposed the use of (only) audio
for co-presence detection. Their work showed that audio is a good ambient
context resulting in 100\% accuracy and 0\% False Positive Rate (FPR).

The audio features used in \cite{DBLP:conf/esorics/HaleviMSX12} are based on
audio frequency. Therefore, to evaluate the impact of frequency on the attack
feasibility, we tested three different ranges of ambient audio frequencies
collected by controlled experiments where we set up the ambient noise
surrounding recording devices falling into different categories. \textit{Low
ambient audio} (frequency less than 100 Hz); \textit{Medium ambient audio}
(frequency in the human audible range, at around 500 Hz); \textit{High ambient
audio} (frequency 5000 Hz or more).


\begin{table}[htbp]
\centering
	\scriptsize
    \caption{Relay attack success rate (FPR) for audio streaming via Wi-Fi and
    Cellular networks} 
    \label{tbl:audio-streaming}

    \begin{tabular}{@{}lcrcr@{}}
        \toprule
        {\bf Acoustic relaying environments} &\phantom{} & {\bf Wi-Fi} &\phantom{} & {\bf Cellular} \\
	(\prover  freq  \verifier  freq) &\phantom{} & {} &\phantom{} & {} \\
        \midrule
        {\em High  Medium} && 100\% && 40\% \\
        {\em High  Low} && 100\% && 20\% \\
        {\em Medium  Medium} && 100\% && 0\% \\
        {\em Medium  Low} && 100\% && 60\% \\
        {\em Low  Low} && 20\% && 0\% \\
        {\em Others}                && 0\%  && 0\% \\ 
        \bottomrule
    \end{tabular}
\end{table}



\verOne { We used the dataset \textbf{Data-PerCom} for ambient audio to build the classification model
(F1 of 0.86 and FPR of 9.3\%).  The 100 samples we collected via audio streaming
channels in dataset \textbf{Data-TMC-audio} are fed to the classifier for prediction.
} Table~\ref{tbl:audio-streaming} presents the FPR of non co-presence detection
under the streaming attacks over Wi-Fi and cellular data channels. The results
indicate that the attacker (1) has a higher chance of success using the Wi-Fi
channel and (2) could be thwarted when either the ambient audio at \prover is low
frequency or if the ambient audio at \verifier is high frequency.







This simple streaming attack with commodity devices shows that the
audio-only system is highly vulnerable to relay attacks, especially via the
Wi-Fi channel. The attack has very high success rate regardless of hardware
variations and network delays inherent to streaming. However, an attacker can
succeed only when relaying ambient audio from a higher frequency acoustic
environment to a similar or lower frequency acoustic environment, such that,
the higher frequency dominates the lower frequency, and makes \verifier falsely
record \prover's ambient noise instead of the real ``localized'' ambient noise. 


The audio features we used, i.e., the ones proposed in
\cite{DBLP:conf/esorics/HaleviMSX12}, are not sensitive to time
synchronization. This is effective in terms of co-presence detection (i.e.,
results in very low FNR). However, as we can see from our experiments, these
features also enable the attacker to succeed in the relay attack with a very
high chance. Other audio features, such as the ones proposed in
\cite{schurmann2013audio}, require tight synchronization and could be more
resistant to relaying. Unfortunately, because of their high sensitivity to
synchronization, these features did not perform well in the benign
(co-presence) case based on our experiments (i.e., resulted in high FNR).










\subsubsection{Audio-Radio System}
Truong et al. \cite{TruongPerCom14}, evaluated the performance of an
audio-radio system against a unidirectional, \singlemodal attacker. They showed
that while the system achieves good performance (F1 of 0.98) and high security
(FPR of 2.0\%), a contextual attacker could increase the FPR: from 0.18\% to 65.8\%
(manipulating \wifi), from 1.1\% to 1.2\% (\bluetooth); from 1.62\% to 3.01\%
(audio). 
\verOne { Now, we will analyze the same system and same dataset \textbf{Data-PerCom} against a bi-directional (for
radio), \multimodal attacker. 
} To model the attack, in each run, the non
co-presence samples in the test data were transformed as below.






\noindent\textit{Audio}: Because raw audio data is additive, and one-side
context manipulation for audio is tested, an adversary can be modelled by
replacing \verifier side audio () to be the sum of its own ambient
audio and \prover side audio ().
    
\noindent \textit{Radio} (\bluetooth and \wifi): In \cite{TruongPerCom14}, the
set of radio records from two devices  and  are defined as: , and , where () with
 is an identifier and  is associated signal strength of a beacon; 
and  denote the number of different beacons (i.e., Wi-Fi access points or
Bluetooth devices). The both-sides contextual adversary can be modeled by
replacing  with , and  with .



We considered two approaches of fusing sensor data against bi-directional relay
attacks and showed which of them is more suitable for resisting against the
presence of contextual attackers.


\begin{table*}[t] 
\centering 
\caption{
\footnotesize{
\verOne { This table shows how well different types of context-manipulating attackers perform in scenarios employing different types of fusion. The horizontal blocks refer to increasingly powerful attackers with the ability to manipulate zero, one or two context sensor modalities.
\textbf{Values in the table are FPRs with/without different contextual attacks in various audio/radio/physical systems}. \textbf{Notations:} Sets of manipulated sensors are put inside curly braces \{\}.  
} \{\} denotes an arbitrary
            set of sensor modalities.  Fuse-F: features-fusion, Fuse-D-S:
            decisions-fusion from single modalities, Fuse-D-M: decisions-fusion
            from subsets of modalities.  \textbf{Result highlights}:
            Manipulation of sensor modalities, especially multiple of them, can
            significantly reduce security (increase FPR) in most cases.
    Decisions-fusion can help improve security when dominant sensors are
    manipulated, but it may reduce usability (increase FNR).
}
\label{tbl:all-mani}}
\resizebox{\textwidth}{!}{\begin{tabular}{@{}l c rr c rr c rrr@{}}
\toprule 
&
& 
\multicolumn{2}{c}{{\bf Audio-Radio}}   & \phantom{aaa} 
&
\multicolumn{2}{c}{{\bf Physical}}   &  \phantom{aaa} 
&
\multicolumn{3}{c}{{\bf Audio-Radio-Physical}}  \\
\cmidrule{3-4}  \cmidrule{6-7}  \cmidrule{9-11} 
&&  
{\em Fuse-F} &
{{\em Fuse-D-S}} && 
{\em Fuse-F} & 
{\em Fuse-D-S} && 
{\em Fuse-F} & 
{\em Fuse-D-S} & 
{\em Fuse-D-M} \\
&& (1) & (2) && (3) & (4) && (5) & (6) & (7) \\
\cmidrule{3-4}  \cmidrule{6-7}  \cmidrule{9-11} 
&& 
2.0\%   & 
2.0\%  && 
7.5\%   & 
13.0\%  && 
3.0\%   & 
27.1\%  & 
6.9\%\\                                                          
\multirow{-1}{*}{\begin{rotate}{90} {\bf Zero-modality} \end{rotate}}&& 
\colorbox{gray!90}{(FNR: 1.4\%)}   &
\colorbox{gray!90}{(FNR: 12.0\%)}  && 
\colorbox{gray!90}{(FNR: 3.9\%)}   & 
\colorbox{gray!90}{(FNR: 14.5\%)}  &&
(FNR: 0.0\%)    & 
(FNR: 0.3\%)   & 
(FNR: 0.0\%) \\
&& 
(F1: 0.977)    & 
(F1: 0.925)    && 
(F1: 0.928)    & 
(F1: 0.861)    &&
(F1:0.990)     & 
(F1: 0.923)    & 
(F1: 0.980) \\
\cmidrule{3-4}  \cmidrule{6-7}  \cmidrule{9-11} 
&& 
\{\audio{}\}: 3.0\%       &
\{\audio{}\}: 3.0\%       && 
\{\temperature{}\}: 8.3\%     & 
\{\temperature{}\}: 17.0\%    &&
\{\audio{}\}: 87.7\%     &
\{\audio{}\}: 45.3\%       &
\{\audio{}\}: 36.9\% \\
&& 
\{\bluetooth{}\}: 2.7\%       &
\{\bluetooth{}\}: 9.0\%       && 
\{\gas{}\}: 11.9\%        & 
\{\gas{}\}: 20.0\%        && 
\colorbox{gray!50}{\{\bluetooth{}\}: 100\%}         & 
\colorbox{gray!50}{\{\bluetooth{}\}: 45.8\%} &
\colorbox{gray!50}{\{\bluetooth{}\}: 36.9\%} \\
&& 
\colorbox{gray!20}{\{\wifi{}\}: 99.8\%}       & 
\colorbox{gray!20}{\{\wifi{}\}: 8.0\%}        && 
\{\humidity{}\}: 15.3\%   &
\{\humidity{}\}: 24.4\%   && 
\{\wifi{}\}: 12.3\%    & 
\{\wifi{}\}: 44.8\%   &
\{\wifi{}\}: 35.0\% \\
&&                      
&
&& 
\colorbox{gray!30}{\{\altitude{}\}: 55.1\%}   &
\colorbox{gray!30}{\{\altitude{}\}: 33.1\%}   && 
\{\altitude{}\}: 5.4\%    & 
\{\altitude{}\}: 37.9\%        &
\{\altitude{}\}: 6.9\%\\    
&&                      
& 
&&                      
& 
&& 
\{\gas{}\}: 5.9\%       & 
\{\gas{}\}: 29.6\%      &
\{\gas{}\}: 6.9\%\\    
&&                      
&
&&                      
& 
&& 
\{\humidity{}\}: 3.4\%      &
\{\humidity{}\}: 29.1\%       &
\{\humidity{}\}: 6.9\%\\
\multirow{-3}{*}{\begin{rotate}{90} {\bf Single-modality} \end{rotate}}&&                      
&
&&                      
& 
&& 
\{\temperature{}\}: 3.4\%       &
\{\temperature{}\}: 31.5\%      &
\{\temperature{}\}: 6.9\%\\
\cmidrule{3-4}  \cmidrule{6-7}  \cmidrule{9-11}
&& 
\{\audio{}, \bluetooth{}\}: 3.6\%     &
\{\audio{}, \bluetooth{}\}: 96.0\%    && 
\{\gas{}, \temperature{}\}: 13.9\%    & 
\{\gas{},\temperature{}\}: 40.1\%     && 
\colorbox{gray!70}{\{\bluetooth{}\}\{\}: 100\%}             & 
\{2 sensors\}:                      & 
\{\audio{}, \bluetooth{}\}\{\}: \\
&& 
\{\bluetooth{}, \wifi{}\}: 99.8\%     & 
\{\audio{}, \wifi{}\}: 96.0\%         && 
\{\gas{}, \humidity{}\}: 15.7\%       &  
\{\humidity{}, \temperature{}\}: 41.9\% && 
                                      & 
32.0-75.4\%                          & 
\% \\
&& 
\{\audio{}, \wifi{}\}: 100\%          & 
\{\bluetooth{}, \wifi{}\}: 100\%      && 
\{\humidity{}, \temperature{}\}: 29.6\% &  
\{\altitude{}, \temperature{}\}: 50.6\% && 
\{\audio{}\}\{\}74.9\%  & 
\{3 sensors\}:                      &
\{\audio{}, \wifi{}\}\{\}: \\
&& 
\{\audio{}, \bluetooth{}, \wifi{}\}: 100\%      &
\{\audio{}, \bluetooth{}, \wifi{}\}: 100\%      && 
\{\gas{}, \humidity{}, \temperature{}\}: 31.1\%   & 
\{\gas{}, \humidity{}\}: 57.5\%               && 
                                               & 
37.4-97.5\%                                    & 
\% \\
&&                              
&                               
&&
\{\altitude{}\}\{\}: & 
\{\altitude{}, \humidity{}\}: 61.2\%          && 
\{\}\{\audio{}, \bluetooth{}\}:      &
\{4 sensors\}:                              &
\{\altitude{}, \gas{}, \humidity{}, \temperature{}\}:\\


&&                              
&
&& 
64.7-100\%&  
\{\altitude{}, \gas{}\}: 65.5\%               && 
                                       & 
97.5-100\%                                   & 
9.9\% \\


&&                              
&
&& 
&
rest: 100\%     &&                    
                &             
rest: 10\%      & 
\colorbox{gray!70}{\{\bluetooth{}, \wifi{}\}: 36.9\%} \\


\multirow{-4}{*}{\begin{rotate}{90} {\bf Multi-modality}\end{rotate}}&&
&                
&&                             
&                          
&&                    
&                  
&
rest: 6.9-87.7\% \\
\bottomrule
\end{tabular}
}
\end{table*}
 


Table~\ref{tbl:all-mani} (columns 1 and 2) presents the analysis results of
training model combining all three audio-radio modalities (\audio, \bluetooth
and \wifi) and testing with different attacks. Zero-modality attack shows the
very low FPR with both fusion methods. The FNR for decisions-fusion is higher
compared to that for features-fusion.
For features-fusion, the results are aligned with the ones reported in~\cite{TruongPerCom14}.

In \singlemodal attack, manipulating Wi-Fi, the dominant feature, results in a
very high success rate with features-fusion. The results change when
decisions-fusion was applied.
In such case, manipulating any single sensor, even the most powerful one, does not
significantly degrade the overall security. The FPR in case \wifi was
manipulated decreases from 99.8\% (features-fusion) down to 8\% (decisions-fusion).
We recall that the performance difference of audio and radio sensors is not
large (as reported in \cite{TruongPerCom14}, F1 ranges from 0.857 for \audio to
0.989 for \wifi).
This explains why decisions-fusion reduces the overall performance 
slightly (F1 reduces from 0.977 to 0.925) in case of \zeromodal attack but
significantly improves the security under a single-modality attack. The
security is very low in \multimodal attack, and neither of the fusion
approaches could restore the security level when majority of the sensors are
under attacker's control.  
We earlier argued that audio and radio modalities can be manipulated simultaneously.









\subsubsection{Physical System}
\verOne { Shrestha et al. \cite{ShresthaFC2014} introduced four physical modalities (\altitude, \humidity, \gas,
and \temperature) for co-presence detection. 
} The performance of
the features-fusion based classifier trained with their dataset is good (F1 of
0.957, FPR of 5.81\%) against a \zeromodal adversary.

Based on our attacks against physical modalities (Section \ref{sec:physical}),
we consider an adversarial model where an attacker can manipulate the physical
context on one side (unattended verifier) to match the sensor readings at the
other side (prover). 
\verOne { To model this attack, using the dataset \textbf{Data-FC}, we transformed all non co-presence samples in the
test set to the ``attack'' value (distance 0).  
} The distance
is set to 0 as data collection in \cite{ShresthaFC2014} was done by a single
device at a given point of time, hence, no hardware effect or calibration error
was taken into account.  The non co-presence class in the dataset is about 18
times larger than co-presence class. To correct this imbalance, we applied the
same under-sampling as in \cite{ShresthaFC2014}: we divided the non co-presence
samples into 19 subsets, ran several rounds of cross validation taking 10
subsets in each round and aggregated the results in the end. In addition to
the features-fusion employed in \cite{ShresthaFC2014}, we tested the 
decisions-fusion similar to our audio-radio system analysis in the previous section.

Table~\ref{tbl:all-mani} (columns 3 and 4) shows our analysis results.  The
system performance in zero-modality attack is well-aligned with the one
reported in \cite{ShresthaFC2014}.  As in \cite{ShresthaFC2014}, among four
physical modalities, \altitude performs the best. Consequently, manipulating
only \altitude degrades the security vastly with features-fusion (FPR increases
to over 50\%). Decisions-fusion in general brings lower security and lower
performance/usability in \zeromodal attack and \singlemodal attack.  However,
it avoids the dominance of sole sensor in case the attacker can control such
sensor (\altitude in this case). Decisions-fusion can also help improve
security against a \multimodal attacker who manipulates \altitude along with
other sensors. Compared to audio-radio system, in physical system, attacking
each single modality results in higher success rate. 















\subsubsection{Audio-Radio-Physical System}


\verOne { Unlike the dataset for physical sensors \cite{ShresthaFC2014} which was
collected from one device at a time only, the dataset \textbf{Data-TMC} that we collected for this work contains data from pairs of
devices, and therefore hardware variance and calibration errors between
co-presence device sensors need to be taken into account. 
} When we try to model
the contextual attack on given sensor(s), distance 0 does not ensure that the attack will succeed.  As
the classifier is trained with data which may contains noise, we compute the
mode of the histogram for distance values for the co-presence samples. As the data
aggregated is from two participants, histograms of distance values are not
uninomial but multinomial. Multinomial distribution implies several modes. For
each physical sensor, we choose a mode value and assign it as the distance value.
The mode values for \altitude, \gas, \humidity and \temperature are 13.54, 0.3,
6.61 and 0.153, respectively. 
As the manipulation by replacing the radio data at
both sides has to be identical, the distance features for radio sensors are set to 0.
















Table~\ref{tbl:all-mani} (columns 5, 6 and 7) reports our analysis results with
different fusion methods. Under \zeromodal attack, features-fusion performs the
best while decisions-fusion from single modalities performs the worst.
Features-fusion uses all possible features for training so that the classifier
can be built based on the best features or best combination of features
(\bluetooth and \audio with our current dataset). Thus, it returns the
best results (in the absence of context manipulation) compared to any other
ways of fusing sensor data. Decisions-fusion based on single modalities lets
the worst sensors being able to contribute to the voting scheme, thus bringing
down the overall performance. This is the case in our dataset where radio
sensors and audio sensor perform better than physical sensors. Note that if all
sensors perform equally well, features-fusion and decisions-fusion would not
differ much. Decisions-fusion from subsets of sensors has a moderate
performance, worse than features-fusion but better than decisions-fusion from
single modalities.  This hybrid approach avoids mis-learning as in the case of
using a single modality only.

Let us now assess the security of this co-presence detection system when any
single modality is controlled by the attacker. Depending on how sensors are
fused, the impact of manipulated sensor varies. In features-fusion, as the
classifier decision relies on the best features of dominant sensors, the FPR
increases drastically when such sensors are manipulated (i.e. \audio or
{\bluetooth} in our dataset). In contrast, when weaker sensors (physical or
{\wifi}) are manipulated, it has a relatively small impact on the security as
the resulting FPR increases a bit compared to a zero-modal attack (especially
for \wifi). Decisions-fusion reduces attacker success rate when single sensor
is manipulated, for example, FPR of manipulating \bluetooth decreases from
100\% (features-fusion) to 36.9\% (decisions-fusion). 
Recall that manipulating single sensor is not difficult as we demonstrated in Section
\ref{sec:manipulation_attack}.

An attacker has the highest chance to succeed if he can control the dominant
sensors or a subset of sensors that contain the dominant sensors. In such case,
the success rate could reach 100\% with only one single dominant sensor (i.e.
\bluetooth in our dataset) if the system uses features-fusion or with
majority dominant sensors (i.e. \audio and \bluetooth). In most cases,
attacking the set of weak sensors (e.g. \{\altitude, \gas, \humidity,
\temperature{}\}) does not impact the security much, except when system uses
decisions-fusion from single modalities.






\begin{comment}

We have seen that decisions-fusion reduces attack success rates in cases where
the minority of the sensors are manipulated.  However, this may come at the
cost of higher FNR which represents the usability of co-presence systems.


Decisions-fusion from single sensors improves security when individual sensors
perform well. However, it increases the attack success rate for weak sensors.
For example, to the audio-radio-physical system, attacking weak sensors such as
\humidity or \gas brings relatively high success rate compared to
features-fusion.  Decisions-fusion from subsets of sensors reduces the FPR in
general especially when dominant sensors are controlled by the attacker. 


The dataset we used for analyzing the attacks is relatively small. It was
collected from limited number of devices and might not represent all possible
scenarios and environments.  However, it was sufficient to demonstrate the
impact of attacks and defensive solutions.

Typically, during the authentication/deauthentication process, the prover moves
nearer to/farther away from the verifier. In this case, all the radio signals
will be changing gradually, i.e., if prover and verifier are moving towards
APs, new APs will be shown, or their signal strengths will continuously grow,
while if they are moving further away from APs, their strengths will decrease
or the APs may be not visible at all. If the verifier or prover device detects
a lot more APs (or Bluetooth devices) nearby all of a sudden, it may be
indicative of a radio manipulation attack. The system can be made aware of
such situations.

We noticed that when the verifier is in an environment which has high frequency
noise, an attacker tends to fail with audio streaming. This can be used to
design an active defense mechanism such that whenever audio contextual
information is requested, the verifier can emit a high frequency audio to
reduce the chances of attacker success.

\end{comment}
