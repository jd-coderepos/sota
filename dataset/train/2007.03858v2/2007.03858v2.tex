


\begin{figure*}
    \centering
    \includegraphics[width=0.48\linewidth]{challenging_multi_images1}
    \includegraphics[width=0.48\linewidth]{challenging_multi_images2}
    \caption{Comparison between the single-image reconstruction results and the multi-image results from input images with moderate pose deviations. In these  cases, our method is still able to integrate information from different frames.}
    \label{fig:challenging_multi_images}
\end{figure*}

\section{Experiments}
In this section we evaluate our approach. Details about the implementation and the evaluation dataset are given in Sec.\ref{sec:results:impl_details} and Sec.\ref{sec:results:dataset} respectively. In Sec.\ref{sec:results:results} we demonstrate that our method is able to reconstruct human models with challenging poses. We then compare our method against state-of-the-art methods in Sec.\ref{sec:results:comparison}. We also evaluate our contributions both qualitatively and quantitatively in Sec.\ref{sec:results:ablation}. The quantitative evaluation results are given in Tab.\ref{tab:quant_evalation}. We mainly use geometry reconstruction performance for evaluation, which is our main focus. 
\label{sec:results}


\subsection{Implementation Details}
\label{sec:results:impl_details}

\textbf{Network Architecture. } 
For image feature extraction, we adapt the same 2D image encoders in PIFu\cite{pifuSHNMKL19} (i.e., Hourglass Stack\cite{Hourglass16} for geometry and CycleGAN\cite{CycleGAN2017} for texture), which take as input an image of 512512 resolution and outputs a 256-channel feature map with a resolution of 128128. For volumetric feature extraction, we use a 3D convolution network which consists of two convolution layers and three residual blocks. Its input resolution is 128128128 and its output is a 32-channel feature volume with a resolution of 323232. We replace batch normalization with group normalization to improve the training stability. The  feature decoder is implemented as a multi-layer perceptron, where the number of neurons is , where  for the geometry network while   for the texture network. 

\textbf{Training Data. } 
To achieve state-of-the-art reconstruction quality, we collect 1000 high-quality textured human scans with various clothing, shapes, and poses from Twindom\footnote{\url{https://web.twindom.com/}}. We randomly split the Twindom dataset into a training set of 900 scans and a testing of 100 scans. To augement pose variety, we also randomly sample 600 models from DeepHuman\cite{Zheng2019DeepHuman} dataset. 
We render the training models from multiple view points using Lambertian diffuse shading and spherical harmonics lighting\cite{SURREAL}. We render the images with a weak perspective camera and image resolution of 512  512. To obtain the ground-truth SMPL annotations for the training data, we apply MuVS\cite{huang2017muvs} to the multi-view images for pose computation and then solve for the shape parameters to further register the SMPL to the scans. For point sampling during training, we use the same scheme proposed in PIFu\cite{pifuSHNMKL19}, in which the authors combine uniform sampling and adaptive sampling based on the surface geometry and use embree algorithm\cite{wald2014embree} for occupancy querying. 

\textbf{Network training. } 
We use Adam optimizer for network training with the learning rate of , the batch size of 3, the number of epochs of 10, and the number of sampled points of 5000 per subject. The learning rate is decayed by the factor of 0.1 at every 10000-th iteration.
We also combine predicted and ground-truth SMPL models during network training. Specifically, in each batch, we randomly select one image to replace the predicted SMPL model with the ground-truth when constructing the depth-ambiguity-aware reconstruction loss. In this way we can guarantee the best performance is obtained once the underlining SMPL becomes more accurate. 
The multi-image network is fine-tuned from the models trained for single-view network with three random views of the same subject using a learning rate of  and a batch size of 1. Before training the PaMIR network, we first fine-tune the pre-trained GCMR network on our training set. 

\textbf{Network testing. } 
When testing, our network only requires an RGB image as input and outputs both the parametric model and the reconstructed surface with texture. To maximize the performance, We run the body reference optimization step for all results  unless otherwise stated. Fifty iterations are needed for the optimization and take about 40 seconds. 

\textbf{Network complexity. } 
We report the module complexity in Tab.\ref{tab:complexity}. To reconstruct the 3D human model given an RGB image, we use the hierarchical SDF querying method proposed in OccNet\cite{Mescheder2019OccupancyNetwork} to reduce network querying times. Overall, taking the body reference optimization step into account, it takes about 50s to reconstruct the geometry of the 3D human model and 1s to recover its surface color. 

\begin{table}
    \centering
    \caption{The number of parameters and execution time of each network module}
    \label{tab:complexity}
	\begin{threeparttable}
    \begin{tabular}{lcc}
        \toprule
         Module & \#Parameters & Execution Time  \\
         \midrule
         GCMR  & 46,874,690 & 0.15s\tnote{} \\
         Geometry Network  & 27,225,105 & 0.25s\tnote{}\\ 
         Texture Network & 13,088,268 & 0.29s\tnote{}\\
         \bottomrule
    \end{tabular}
	\begin{tablenotes}
        \scriptsize
        \item[] Measured using one RGB image. 
        \item[] Measured using one RGB image and 10k query points. 
        \end{tablenotes}
        \end{threeparttable}
 \end{table}





\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{comparison_large2}
  \caption{Qualitative comparison against state-of-the-art methods for single-image human model reconstruction: (a) input images,  (b) results by HMD\cite{HMD2019}, (c) Tex2Shape\cite{tex2shape2019}, (d) Moulding Humans\cite{MouldingHumans2019}, (e) DeepHuman\cite{Zheng2019DeepHuman}, (f) PIFu\cite{pifuSHNMKL19}, (g) ours. }
  \label{fig:comparison} \end{figure*}

\subsection{Evaluation Dataset}
\label{sec:results:dataset}
For qualitative evaluation on single-image reconstruction, we utilize real-world full-body images collected from the DeepFashion dataset\cite{liuLQWTcvpr16DeepFashion} and from the Internet. We remove the background of these real-world image using neural semantic segmentation\cite{liang2018LIP} followed by Grabcut refinement\cite{rother2004grabcut}. For qualitative evaluation on multi-image reconstruction, we use the open-source dataset from VideoAvatar\cite{VideoAvater} which contains 24 RGB sequences of different subjects turning 360 degree with a rough A-pose in front of a camera. The testing split of the Twindom dataset is used for quantitative comparison. In addition, for quantitative comparison and evaluation we also use the BUFF dataset\cite{shapeundercloth:CVPR17}, which provides 26 4D human sequences with various clothes and over 9000 scans in total. To reduce computation burden, We select 300 scans of different poses from the buff dataset. We render both the Twindom testing data and the BUFF data from 12 views spanning every 30 degrees in yaw axis using the same method in Sec.\ref{sec:results:impl_details}. 




\subsection{Results}
\label{sec:results:results}

We demonstrate our approach for single-image 3D human reconstruction in Fig.\ref{fig:teaser} and Fig.\ref{fig:results}. The input images in Fig.\ref{fig:teaser} and Fig.\ref{fig:results} covers various body poses (dancing, Kungfu, sitting and running), and also covers different clothes (loose pants, skirts, sports suits and casual clothes). The results demonstrate the ability of our method to reconstruct high-quality 3D human models and its robust performance to tackle various human poses and clothing styles. 

We also test our performance for multi-image human model reconstruction using the VideoAvatar dataset in Fig.\ref{fig:video_avatar_results}. We uniformly sample 5 frames for each sequence for surface reconstruction. Note that for each subject, the poses in different frames are different due to the body movements and no camera extrinsic parameter is provided, so PIFu is not suitable to reconstruct human models in this case. In contrast, our method can reconstruct full-body models with high-resolution details, proving the generalization capability of our PaMIR representation. 
We also present two example results of applying multi-image feature fusion on images with moderate pose deviation in Fig.\ref{fig:challenging_multi_images}. As the figure shows, our method is still able to reconstruct the overall shapes of the subjects in these cases.



\subsection{Comparison}
\label{sec:results:comparison}

We qualitatively compare our method with several state-of-the-art methods including HMD\cite{HMD2019}, Tex2Shape\cite{tex2shape2019}, Moulding Humans\cite{MouldingHumans2019}, DeepHuman\cite{Zheng2019DeepHuman} and PIFu\cite{pifuSHNMKL19}. Among them, HMD\cite{HMD2019} and Tex2Shape\cite{tex2shape2019} are parametric methods based on SMPL\cite{SMPL:2015} model deformation, PIFu\cite{pifuSHNMKL19} uses a deep implicit function as geometry representation, Moulding Humans\cite{MouldingHumans2019} uses the combination of a front depth map and a back depth map for representation, and DeepHuman\cite{Zheng2019DeepHuman} combines volumetric representation with the SMPL model. Note that Tex2Shape only computes shapes, so we use the pose parameters obtained by our method to skin the its results. 
For simplicity, we omit comparisons with the works that have already been compared like BodyNet\cite{BodyNet} and SiCloPe\cite{SiCloPe2019}.
In our experiments, DeepHuman, PIFu and Moulding Humans are all retrained on the our dataset, while parametric methods like HMD and Tex2shape are not because our dataset contains loose garments like dresses which will deteriorate the performance of those methods. 





We conduct qualitative comparison in Fig.\ref{fig:comparison}. As shown in the figure, HMD and Tex2Shape have difficulties dealing with loose clothes and cannot reconstruct the surface geometry accurately; Moulding Human\cite{MouldingHumans2019} fails to handle challenging poses (due to the lack of semantic constraints), and produces broken body parts when self-occlusions occur, which is the essential limitation of its double depth representation;  DeepHuman\cite{Zheng2019DeepHuman} cannot recover high-frequency geometrical details although it succeeds to reconstruct the rough shapes from the images;  PIFu\cite{pifuSHNMKL19} struggles to reconstruct the models in challenging poses and also suffer from self-occlusions. In contrast, our method is able to reconstruct plausible 3D human models under challenging body poses and various clothing styles. In terms of surface quality and pose generalization capacity, our method is superior to other state-of-the-art methods. 




\begin{figure*}
	\begin{center}
		\includegraphics[width=0.9\linewidth]{vis_pair_large}
	\end{center}
\caption{Two examples of multi-modal output. Given an input image (a), our method can output different possible reconstruction results (top row of (b)(c)) corresponding to different body pose hypothesis (bottom row of (b)(c) ). From the overlapped figure (the last columns of (b)(c)), we can see that the right arm and the left leg of reconstructed mesh adjusts accordingly with the right arm and the left leg of the body hypothesis, while other parts of the mesh keep consistent. Better view in color. }
	\label{fig:pair1}
\end{figure*}


\begin{table*}
	\caption{Numerical comparison results.  }
	\centering
	\begin{tabular}{l|c|c|c|c}
        \hline
        Dataset & \multicolumn{2}{c}{BUFF} & \multicolumn{2}{|c}{TWINDOM} \\
        \hline
		Method & P2S (cm) &  Chamfer (cm) & P2S (cm) &  Chamfer (cm)  \\
		\hline
		HMD\cite{Zheng2019DeepHuman} & 2.48 & 3.92 & 2.50 & 4.01\\
		Moulding Humans\cite{MouldingHumans2019} & 2.25 & 2.68 & 2.84 & 3.35 \\
		DeepHuman\cite{Zheng2019DeepHuman} & 2.15 & 2.80 & 2.35 & 2.97\\
		PIFu\cite{pifuSHNMKL19} & 1.93 & 2.22 & 2.34 & 2.65\\
		Ours & 1.52 & 1.92 & 1.80 & 2.12 \\
		Ours using ground-truth SMPL & 0.709 & 0.936 & 0.744 & 1.00 \\
        \hline
	\end{tabular}
	\label{tab:quant_comparison}
\end{table*}




We quantitatively compare our method with the state-of-the-art methods using both Twindom testing dataset and BUFF rendering dataset to evaluate the geometry reconsturction accuracy. 
Similar to the experiments in PIFu\cite{pifuSHNMKL19}, we use point-to-surface error as well as the Chamfer distance as error metric. 
The numerical results are presented in Tab.\ref{tab:quant_comparison}. The quantitative comparison shows that our method outperforms the state-of-the-art methods in terms of surface reconstruction accuracy. 
We also provide the errors when ground-truth SMPL annotations are available to present the upper limit of our reconstruction accuracy if the SMPL estimation is perfect. Overall, our method is more general, more robust and more accurate than HMD\cite{HMD2019},  Moulding Humans\cite{MouldingHumans2019}, DeepHuman\cite{Zheng2019DeepHuman} and PIFu\cite{pifuSHNMKL19}. 






For multi-image setups, we also conduct qualitative comparison against state-of-the-art methods \cite{VideoAvater} and \cite{alldieck19cvpr}. 
The method proposed in \cite{VideoAvater} is an optimization-based method which deforms the SMPL template according to the silhouettes.
The optimization is performed on the whole video sequence. 
In contrast, the method in \cite{alldieck19cvpr} is a learning-based method that deforms the SMPL template based on only 8 views of images. 
The comparison results are shown in Fig.\ref{fig:video_avatar_results}. 
From the results we can see that although all methods are able to reconstruct the overall shapes correctly, our method is able to recover more surface details than the other two methods. 
This is because our non-parametric representation allows more flexible surface reconstruction. 
We do not perform quantitative comparison because there is no such benchmark available. 


\subsection{Evaluation}
\label{sec:results:ablation}


\subsubsection{PaMIR Representation}


The superiority of our PaMIR representation is already proved through the comparison experiments in Sec.\ref{sec:results:results} and \ref{sec:results:comparison}. In this evaluation, we demonstrate the advantage of our PaMIR representation from another aspect: it can support multi-modal outputs. To be more specific, our method can output multiple possible human models corresponding to different body pose hypotheses. Two examples are shown in Fig.\ref{fig:pair1}. In both experiments we manually adjust one part of the body in order to generate multiple pose hypotheses. As a result, the reconstruction results change in accordance with the input SMPL models. In contrast, non-parametric method like PIFu\cite{pifuSHNMKL19} or Moulding Humans\cite{MouldingHumans2019} can only generate a specific mesh for each image, showing their limited and overfit capability. 


\begin{table}
	\caption{Numerical Ablation Study.  }
	\centering
	\begin{threeparttable}
	\begin{tabular}{lc}
		\toprule
		Method\tnote{} & P2S (cm)   \\
		\midrule
		GT SMPL & 1.71 \\
		Pred SMPL wo/ DAAL& 1.81 \\
		Pred SMPL w/ DAAL& 1.60\\
		Full (Pred SMPL w/ DAAL + Optimization) &  1.52 \\
		\bottomrule
	\end{tabular}
	\begin{tablenotes}
	   \scriptsize
       \item[] GT = ground-truth, Pred = predicted, DAAL = depth-ambiguity-aware loss
     \end{tablenotes}
    \end{threeparttable}
	\label{tab:quant_evalation}
\end{table}

\subsubsection{Depth-ambiguity-aware Loss}


\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\linewidth]{eval_adaptive_loss}
	\end{center}
	\caption{Qualitative evaluation on depth-ambiguity-aware training loss: (a) the input images, (b) the corresponding SMPL models estimated by GCMR, (c) our reconstruction results, (d) the reconstruction results of the baseline network. }
	\label{fig:eval_adaptive_loss}
\end{figure}



To conduct ablation study on our depth-ambiguity-aware loss, we implement a baseline network that is trained using the traditional reconstruction loss and compare it against our network.  After the same number of training epochs, we test both network on real-world input images. Two example results are shown in Fig.\ref{fig:eval_adaptive_loss}. As we can see in the figure, with our depth-ambiguity-aware reconstruction loss, the network is able to learn to reconstruct 3D human models that have consistent poses with the estimated SMPL models. On the contrary, the models reconstructed by the baseline network without our depth-ambiguity-aware loss are less consistent with the SMPL models and have more artifacts. The quantitative study is presented in Tab.\ref{tab:quant_evalation}. From the numeric comparison between the 3rd and 4th rows of Tab.\ref{tab:quant_evalation}, we can conclude that our  depth-ambiguity-aware loss avoids the negative impact of the depth inconsistency between predicted SMPL models and ground-truth scans, and improves the accuracy of the reconstruction results. 

\subsubsection{Training Scheme}
\label{sec:eval:end2end}


We also implement another baseline network that is trained using the ground-truth SMPL annotations to evaluate our training scheme. We compare this baseline network with our network on different input images. In Fig.\ref{fig:eval_endtoend}, we present some cases in which the predicted SMPL models are not well aligned with image keypoints and silhouettes. As we can see in Fig.\ref{fig:eval_endtoend}, under the scenarios of inaccurate SMPL estimation, the baseline network fails to reconstruct complete full-body models, while our network is able to provide plausible results in accordance with image observations. Note that this feature is of vital importance. For instance, in the last example of Fig.\ref{fig:eval_endtoend}, the right forearm of the baseline model is completely missing. In this case, the reference body optimization step could not work because no matter how the right forearm moves, the occupancy probability value of its vertices are always closed to zero. The quantitative study is presented in Tab.\ref{tab:quant_evalation}. From the numerical comparison between the 2nd and 4th rows of Tab.\ref{tab:quant_evalation}, we can conclude that our training scheme improves the accuracy of our reconstruction results at inference time when no accurate SMPL annotation is available. 

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\linewidth]{eval_endtoend}
	\end{center}
	\caption{Qualitative evaluation on our training scheme: (a) the input images, (b) the estimated SMPL models rendered on top of the input images, (c) our reconstruction results, (d) the reconstruction results of the baseline network. }
	\label{fig:eval_endtoend}
\end{figure}

\subsubsection{Reference Body Optimization}


\begin{table}
    \centering
    \caption{Mean Per Joint Position Error (MPJPE, unit: cm) Before/After Body Reference Optimization. }
    \label{tab:smpl_accu}
    \begin{tabular}{lcc}
        \toprule
         Dataset & BUFF & TWINDOM   \\
         \midrule
         Before Optimization & 2.65 & 2.85 \\
         After Optimization  & 2.49 & 2.74 \\ 
         \bottomrule
    \end{tabular}
 \end{table}

To evaluate the effectiveness of our reference body optimization step, we compare the body fitting results before and after optimization using the evaluation images in Sec.\ref{sec:eval:end2end}. The results are presented in Fig.\ref{fig:eval_body_optimization}. As shown in the figure, the optimization step can further register the SMPL model to the image observation, resulting into more accurate body pose estimation. This is also proven in the quantitiative evaluation in Tab.\ref{tab:smpl_accu}.  From the numerical results in the last two rows of Tab.\ref{tab:quant_evalation}, we can also see that the mesh reconstruction is also improved after reference body optimization. 


\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\linewidth]{eval_body_optimization}
	\end{center}
	\caption{Evaluation of reference body optimization. Note that after body reference optimization, the SMPL models are more accurately aligned with the image observations. }
	\label{fig:eval_body_optimization}
\end{figure}


\subsubsection{Multi-image Setup}
To evaluate the detail changes using more or less images, we conduct a quantitative evaluation in Tab.\ref{tab:normal_improvement}. Specifically, we perform feature fusion with 1, 2, 3 and 4 views of the same model and extract the meshes. Then we measure the normal reprojection error\cite{pifuSHNMKL19} from 8 uniform viewpoints to evaluate the detail improvements. \revise{The numerical results presented in Tab.\ref{tab:normal_improvement}  suggest that more geometric details are recovered after fusing information from multiple input images. }

\begin{table}
    \centering
    \caption{Normal Reprojection Error with Inputs Images From Various Numbers of View Points. }
    \label{tab:normal_improvement}
    \begin{tabular}{lcccc}
        \toprule
         View Num & 1 & 2 & 3 & 4   \\
         \midrule
         Normal Reprojection Error & 0.161 & 0.147 & 0.139 & 0.135 \\
         \bottomrule
    \end{tabular}
 \end{table}






