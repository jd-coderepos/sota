

\documentclass{article}

\usepackage{arxiv}          \usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}	      \usepackage{enumitem}	      \usepackage{tabularx}       \usepackage{caption}        \usepackage[lofdepth,lotdepth]{subfig}
\usepackage[style=numeric,sorting=none]{biblatex}

\addbibresource{BMCNNwHFCs.bib}

\date{} \renewcommand{\undertitle}{} \renewcommand{\headeright}{} 

\newcommand{\rightparenthesis}{)} 

\newenvironment{figure_in_table}{\captionsetup{type=figure}}{}

\hypersetup{
    colorlinks=true,
    urlcolor=blue,
    pdftitle={A Branching and Merging Convolutional Network with Homogeneous Filter Capsules},
    pdfpagemode=FullScreen,
}

\title{A Branching and Merging Convolutional Network with Homogeneous Filter Capsules}

\author{
  Adam~Byerly\\
  Department of Electronic and Electrical Engineering\\
  Brunel University London\\
  Uxbridge, UB8 3PH UK \\
  Department of Computer Science and Information Systems\\
  Bradley University\\
  Peoria, Il, 61615 USA\\
  \texttt{abyerly@fsmail.bradley.edu} \\
  \And{}
  Tatiana~Kalganova \\
  Department of Electronic and Electrical Engineering\\
  Brunel University London\\
  Uxbridge, UB8 3PH UK \\
  \texttt{tatiana.kalganova@brunel.ac.uk} \\
  \And{}
  Ian~Dear \\
  Department of Electronic and Electrical Engineering\\
  Brunel University London\\
  Uxbridge, UB8 3PH UK \\
  \texttt{ian.dear@brunel.ac.uk}
}

\raggedbottom{}

\begin{document}

\maketitle

\begin{abstract}
We present a convolutional neural network design that, rather than using a single stem of convolutional layers, uses multiple stems, each having branched off of a prior stem.  This leads to a different effective receptive field and level of abstraction for each branch moving on to a unique classification stage.  The classification stages are then merged together, each effectively voting their level of confidence in the classification.  A further novelty to our design is that we do not use any fully connected layers at all, but rather each of the final filters in each branch is transformed into a pair of homogeneous vector capsules.  As the capsules are formed from entire filters, we refer to them as filter capsules.  This design, in combination with a domain-specific set of randomly applied augmentation techniques, establishes a new state of the art for the MNIST dataset with an accuracy of \textbf{99.84\%} for an ensemble of these models, as well as establishing a new state of the art for a single model (\textbf{99.79\%} accurate).  These accuracies were achieved with a 75\% reduction in both the number of parameters and the number of epochs of training relative to the previously best performing capsule network on MNIST\@.  All training was performed using the Adam optimizer and experienced no overfitting.
\end{abstract}

\keywords{Capsules, Convolutional Neural Network (CNN), Homogeneous Vector Capsules (HVCs), Homogeneous Filter Capsules (HFCs), MNIST}
  
\section{Introduction and Related Work}\label{sec:introduction}

Many of the best performing convolutional neural networks (CNNs) of the past several years have explored multiple paths from input to classification~\cite{Szegedy2015a}\cite{Szegedy2015b}\cite{He2015}\cite{Zhou2020}\cite{Wang2020}\cite{Ciresan2012}.  The idea behind multiple path designs is to enable one or more of the following to contribute to the final classification: (a) different levels of abstraction, (b) different effective receptive fields, and (c) valuable information learned early to flow more easily to the classification stage.

In~\cite{He2015} (and subsequent extensions~\cite{Srivastava2015}\cite{Xie2017}\cite{Jgou2017}\cite{Zhang2020}) the authors added extra paths through the network with \textit{residual blocks} which are meta-layers that contained one or more convolutional operations as well as a ``skip connection'' that allowed information learned earlier in the network to skip over the convolutional operations.  Similarly, in~\cite{Szegedy2015a} and~\cite{Szegedy2015b}, the authors presented a network architecture that made heavy use of \textit{inception blocks}, which are meta-layers that branch from a previous layer into anywhere from 3 to 6 branches of varying layers of convolutions.  Then the branches were merged back together by concatenating the filters of those branches.  Let  be the average number of branches of different length (in terms of successive convolutions) and  be the number of successive inception blocks.  Then  effective receptive fields and levels of abstraction are present at the output of the final inception block.  Additionally, the designs presented in both of these papers included two output stems (one branching out before going through additional inception blocks and the other after all inception blocks) each producing classification predictions.  These classifications were combined via static weighting to produce the final prediction.  In contrast to the aforementioned work, in this paper, we present a network design that produces 3 output stems, each coming after a different number of convolutions, and \textit{thus representing different effective receptive fields and levels of abstraction}.  We conduct experiments that include statically weighted combinations as in~\cite{Szegedy2015a} and~\cite{Szegedy2015b}.  We then go further and investigate learning the branch weights \textit{simultaneously with all of the other network parameters via backpropagation}.  Again, in contrast to the aforementioned work, in these experiments, each of the separate classifications were performed with capsules rather than simple fully connected layers.

Capsules (vector-valued neurons) have become a more active area of research since~\cite{Sabour2017}, which demonstrated near state of the art performance on MNIST~\cite{Lecun2010} classification (at 99.75\%) by using capsules and a routing algorithm to determine which capsules in a previous layer feed capsules in the subsequent layer.  In~\cite{Hinton2018}, the authors extended this work by conducting experiments with an alternate routing algorithm.  Research in capsules since has focused mostly on various computationally expensive routing algorithms (\cite{Venkataraman2020}\cite{Amer2020}).  In~\cite{Byerly2019}, we proposed a capsule design that used element-wise multiplication between capsules in subsequent layers and relied on backpropagation to do the work that prior capsule designs were relying on routing mechanisms for.  We referred to this capsule design as homogeneous vector capsules (HVCs).  In this paper, we use HVCs that are formed one-to-one from entire convolutional filters.  We see them as a sub-type of homogeneous vector capsules which we refer to as a Homogeneous Filter Capsules (HFCs).  By using this capsule design, we avoid the the computationally expensive routing mechanisms of prior capsule work and we surpass the performance of~\cite{Sabour2017} on MNIST, all while requiring 75\% fewer parameters and 75\% fewer epochs of training.

Our analysis of the existing literature shows that of the many branching methods explored, those that produced multiple final classifications merged those classifications via static weighting, which presupposes the relative importance of each output.  In this paper we include and compare the results of both statically weighting the classification branches and learning the weights of the classification branches via backpropagation.

\subsection{Our Contribution}
Our contribution is as follows:
\begin{enumerate}
  \item We present a novel method for branching a CNN that allows for multiple effective receptive fields and levels of abstraction where each branch makes it's own classification prediction.  These classifications are then merged together, each contributing a ``vote''.  We present the results of experiments that include and compare both statically weighting the votes and learning the weights of the votes via backpropagation simultaneously with the rest of the network parameters.
  \item We do classification without any fully connected layers, but rather with a form of HVCs we call homogeneous filter capsules (HFCs).  HVCs (and by extension HFCs) are simpler, less computationally expensive, and require 75\% fewer parameters and 75\% fewer epochs of training than the previously best performing capsule network on MNIST.\@
  \item  This design, in combination with a domain-specific set of randomly applied augmentation techniques, establishes a new state of the art for the MNIST dataset with an accuracy of \textbf{99.84\%} for an ensemble of these models, as well as establishing a new state of the art for a single model (\textbf{99.79\%} accurate).
\end{enumerate}

\section{Proposed Network Design}\label{sec:proposed_network_design}

The starting point for our network design was a conventional convolutional neural network following many widely used practices.  These include stacked  convolutions, each of which used ReLU~\cite{Glorot2011} activation preceded by batch normalization~\cite{Ioffe2015}.  We also followed the common practice of increasing the number of filters in each subsequent convolutional operation relative to the previous one.  Specifically, our first convolution uses 32 filters and each subsequent convolution uses 16 more filters than the previous one, resulting in 160 filters present in the final convolution.  Additionally, the final operation before classification was to softmax the logits and to use categorical cross entropy for calculating loss.

One common design element found in many convolutional neural networks which we intentionally avoided was the use of any pooling operations.  We agree with Geoffrey Hinton's assessment~\cite{Hinton2018b} of pooling as an operation to be avoided due to the information it ``throws away''.  Effectively, pooling is a form of down-sampling and, in the presence of sufficient computational power, should be avoided.  With the MNIST data being , we have no need to down-sample as there exists sufficient computational power for images of this size.  In choosing not to down-sample, we face the potential dilemma of how to reduce the dimensionality as we descend deeper into the network.  This dilemma is solved by  choosing not to zero-pad the convolution operations and thus each convolution operation by its nature reduces the dimensionality by 2 in both the horizontal and vertical dimensions.  We deem choosing not to zero-pad as preferable in its own right in that zero padding effectively adds information not present in the original sample.

Beyond these design choices, we chose to employ two novel design elements:

\begin{enumerate}[label=\arabic*\rightparenthesis]
\item Rather than having a single monolithic design such that each operation in our network feeds into the next operation and only the next operation, we chose to create multiple branches.  After the first two sets of three convolutions, in addition to feeding to the subsequent convolution, we also branched off the output to be forwarded on to an additional operation (detailed next).  Thus, after all convolutions have been performed, we have three branches in our network.
\begin{enumerate}[label=\alph*\rightparenthesis]
\item The first of which has been through three  convolutions and consists of 64 filters each having an effective receptive field of 7 of the original image pixels.
\item The second of which has been through six  convolutions and consists of 112 filters each having an effective receptive field of 11 of the original image pixels.
\item The third of which has been through nine  convolutions and consists of 160 filters each having an effective receptive field of 15 of the original image pixels.
\end{enumerate}
\item For each branch, rather than flattening the outputs of the convolutions into scalar neurons, we instead transformed each filter into a vector to form the first capsule in a pair of homogeneous vector capsules.  We then performed element-wise multiplication of each of these vectors with a set of weight vectors (one for each class) of the same length.  This results in \textit{n}x\textit{m} weight vectors where \textit{n} is the number of filters that were transformed into the first set of capsules and \textit{m} is the number of classes.  We summed across the filters to form the second capsule in the pair of homogeneous vector capsules.  It is after this that we applied batch normalization and then ReLU activation.  Because these capsules are formed one-to-one from entire filters, we see them as a sub-type of homogeneous vector capsules which we refer to as a Homogeneous Filter Capsules (HFCs).  We reduce each vector to a single value per class by summing the components of the vector.  These values can be thought of as the branch-level logits.
As the filter size coming out of the first branch is , the length of the HFC vectors for this branch is 484. For the second branch, consisting of  sized filters, the vectors are of length 256, and for the third branch, consisting of  sized filters, the vectors are of length 100.  Figure\autoref{fig:network_design_a} diagrams the transformation from filter maps through the homogeneous filter capsules to the output logits.
\end{enumerate}

Before classifying, we needed to reconcile the three branch-level sets of logits with each image only belonging to one class.  We accomplished this by stacking the branch-level logits into vectors of length 3, one for each class.  We then reduced by summation each vector to a single value to form the final set of logits to be classified from.  Figure\autoref{fig:network_design_b} shows the high-level view of the entire network.

\begin{figure}[!htbp]
  \centering
  \hfill
  \subfloat[The transformation from filter maps through the homogeneous filter capsules to the output logits.  In this illustration, a single  filter map and 9-dimensional capsule are shown and represent the \emph{n} filter maps and capsules, respectively.]{
    \includegraphics[width=0.35\textwidth]{images/conv_to_caps_to_logits.png}\label{fig:network_design_a}}
  \hfill
  \subfloat[The entire network from input to classification.]{
    \includegraphics[width=0.4\textwidth]{images/bmhfc_arch.png}\label{fig:network_design_b}}
  \hfill{} \caption{Proposed Network Design}\label{fig:network_design}
\end{figure}

We used no weight decay regularization~\cite{Hinton1987}, a staple regularization method that improves generalization by penalizing the emergence of large weight values.  Nor did we use any form of dropout regularization~\cite{Hinton2012}\cite{Wan2013} which are regularization methods designed to stop the co-adaptation of weights.  This decision was made so that we could investigate the generalization properties of our novel network design elements in the absence of other techniques associated with good generalization.  In addition, we intentionally left out any form of ``routing'' algorithm as in~\cite{Sabour2017} and~\cite{Hinton2018}, preferring to rely on traditional trainable weights (see \autoref{tab:trainable_parameters}) and backpropagation.

\begin{table}[!htbp]
  \caption{Trainable Parameters}
  \centering
  \begin{tabularx}{.6\textwidth}{@{}Xr@{}}
    \toprule
      Parameter Type & Count \\
    \midrule
      Convolutional Filters & 756,000 \\
      Capsules & 756,480 \\
      Batch Normalization & 1,707 \\
    \midrule
      Total & 1,514,187 \\
    \bottomrule
  \end{tabularx}\\
  \captionsetup{width=.57\textwidth}
  \caption*{The experiments for which the branch weights were learned required an additional 3 trainable parameters (one for each branch weight).}\label{tab:trainable_parameters}
\end{table}

\section{Experimental Setup}\label{sec:experimental_setup}

\subsection{Merge Strategies}

In~\cite{Szegedy2015a} and~\cite{Szegedy2015b}, the authors chose to give static, predetermined weights to both output branches and then added them together.  In our case, we conducted three separate experiments of 32 trials each in order to investigate the effects of predetermined equal weighting of the branch outputs compared to learning the branch weights via backpropagation:

\begin{enumerate}[label=\arabic*\rightparenthesis]
\item \textbf{Not learnable}.  For this experiment, we merged the three branches together with equal weighting in order to investigate the effect of disallowing any one branch to have more impact than any other.
\item \textbf{Learnable with randomly initialized branch weights}. (Abbreviated as \textbf{Random Init.} subsequently.)  For this experiment, we allowed the weights to be learned via backpropagation.  We initialized the 3 trainable parameters using a Glorot uniform distribution~\cite{Glorot2010}, which for the case of a vector of 3 trainable parameters happens to be a random uniform distribution within the range [-1,1].
\item \textbf{Learnable with branch weights initialized to one}.  (Abbreviated as \textbf{Ones Init.} subsequently.)  For this experiment, we also allowed the weights to be learned via backpropagation.  The difference with the \textbf{Random Init.} experiment being that we initialized the weights to 1.  We conducted this experiment in addition to the \textbf{Random Init.} experiment in order to understand the difference between starting with random weights and starting with equal weights that are subsequently allowed to diverge during training.
\end{enumerate}

\subsection{Data Augmentation}

Most (but not all~\cite{Hasanpour2016}\cite{Chang2015}) of the state of the art MNIST results achieved over the past decade have used data augmentation~\cite{Sato2015}\cite{Wan2013}\cite{Ciresan2012}.  In addition to the network design, a major part of our work involved applying an effective data augmentation strategy that included transformations informed specifically by the domain of the data.  For example, we wanted to be sure we did not rotate our images into being more like a different class (e.g.\ rotating an image of the digit 2 by 180 degrees to create something that would more closely resemble a malformed 5).  Nor did we want to translate the image content off of the canvas and perhaps cut off the left side of an 8 and thus create a 3.  Choosing data augmentation techniques specific to the domain of interest is not without precedent (see for example~\cite{Ciresan2012} and~\cite{Sabour2017}, both of which used data augmentation techniques specific to MNIST).

By modern standards, in terms of dataset size, MNIST has a relatively low number of training images.  As such, judicious use of appropriate data augmentation techniques is important for achieving a high level of generalizability in a given model.  In terms of structure, hand-written digits show a wide variety in their rotation relative to some shared true ``north'', position within the canvas, width relative to their height, and the connectedness of the strokes used to create them.  Throughout training for all trials, every training image in every epoch was subjected to a series of four operations in order to simulate a greater variety of the values for these properties.

\begin{enumerate}[label=\arabic*\rightparenthesis] \item \textbf{Rotation}.  First, we randomly rotated each training image by up to 30 degrees in either direction.  The amount of rotation applied to each training image was chosen by multiplying the value 30 by a sample drawn from a random normal distribution with mean 0 and standard deviation 0.33, clamped to a minimum of -1 (which would result in a left rotation of 30 degrees) and a maximum of 1 (which would result in a right rotation of 30 degrees).  Whether to actually apply this rotation was chosen by drawing from a Bernoulli distribution with probability p of 0.5 (a fair coin toss).
\item \textbf{Translation}.  Second, we randomly translated each training image within the available margin present in that image.  In~\cite{Sabour2017}, the authors limited their augmentation to shifting the training images randomly by up to 2 pixels in either or both directions.  The limit of only 2 pixels for the translation ensured that the translation is label-preserving.  As the MNIST training data has varying margins of non-digit space in the available  pixel canvas, using more than 2 pixels randomly, would be to risk cutting off part of the digit and effectively changing the class of the image.  For example, a 7 that was shifted too far left could become more appropriately classed as a 1, or an 8 or 9 shifted far enough down could be more appropriately classed as a zero.  The highly structured nature of the MNIST training data allows for an algorithmic analysis of each image that will provide the translation range available for that specific image that will be guaranteed to be label-preserving.  \autoref{fig:example_mnist_digit} shows an example of an MNIST training image that has an asymmetric translation range that, as long as any translations are performed such that the digit part of the image is not moved by more pixels than are present in the margin, will be label preserving.  In other words, the specific training example shown in \autoref{fig:example_mnist_digit} could be shifted by up to 8 pixels to the left or 4 to the right and up to 5 up or 3 down, and after doing so, all of the pixels belonging to the actual digit will still be in the resulting translated image.  The amount within this margin to actually translate a training image was chosen as follows:
\begin{enumerate}[label=\alph*\rightparenthesis]
\item Whether to translate up or down and whether to translate left or right were drawn independently from a Bernoulli distribution with probability p of 0.5 (a fair coin toss).
\item The amount of translation across the margin in each chosen direction was determined from the absolute values of two independent samples drawn from a normal distribution with mean 0 and standard deviation 0.33 and clamped to a maximum translation of the entire margin as to avoid translating out of the image's bounds.
\end{enumerate}

\begin{figure}[ht]
  \centering
  \includegraphics[width=2.5in]{images/MNIST_8_w_margins.png}
  \caption{Example MNIST digit w/annotated margins.}\label{fig:example_mnist_digit}
\end{figure}

\item \textbf{Width}.  Third, we randomly adjusted each training image's width.  MNIST images are normalized to be within a  central patch of the  canvas.  This normalization is ratio-preserving, so all images are 20 pixels in the height dimension but vary in the number of pixels in the width dimension.  This variance not only occurs across digits, but intra-class as well, as different peoples' handwriting can be thinner or wider than average.  In order to train on a wider variety of these widths, we randomly compressed each image's width and then added equal zero padding on either side, leaving the digit's center where it was prior.  This was inspired by a similar approach adopted in~\cite{Ciresan2012}.  In their work, they created 6 additional versions of the MNIST training data set, where they normalized the width of the digits to 10, 12, 14, 16, 18, and 20 pixels.  They then fed those data sets as well as the original MNIST data into 7 columns in their architecture.  In our work, we compressed the width of each sample randomly within a range of 0--25\%.  The portion of that range of compression was the absolute value of a sample drawn from a random normal distribution with mean 0 and standard deviation 0.33, clamped to a maximum of 100\% (that is 100\% of the 25\%).
\item \textbf{Random Erasure}.  Fourth, we randomly erased (setting to 0) a  grid of pixels chosen from the central  grid of pixels in each training image.  The X and Y coordinates of the patch were drawn independently from a random uniform distribution.  This was inspired by the random erasing data augmentation method in~\cite{Zhong2017}.  The intention behind this method was to expose the model to a greater variety of (simulated) connectedness within the strokes that make up the digits.  An alternative interpretation would be to see this as a kind of feature-space dropout.
\end{enumerate}

\subsection{Training}

In~\cite{Byerly2019}, the authors show that Homogeneous Vector Capsules (HVCs) enable the use of adaptive gradient descent methods in convolutional neural networks, a practice previously deemed sub-optimal and prone to extreme overfitting.  We followed the training methodology they used and trained with the Adam optimizer~\cite{Kingma2014} using all of the default/recommended parameter values, including the base learning rate of 0.001.  Also, as in both~\cite{Byerly2019} and~\cite{Sabour2017}, we exponentially decayed the base learning rate.  For our experiments, which trained for 300 epochs, we applied an exponential decay rate of 0.98 per epoch, clamped to a minimum of .  And like in~\cite{Byerly2019}, we were able to continue to train for many epochs without experiencing overfitting. (See \autoref{fig:average_test_accuracies} and \autoref{fig:average_test_losses}.)

Test accuracy was measured using the exponential moving average of prior weights with a decay rate of 0.999.~\cite{Izmailov2018}

\section{Experimental Results}\label{sec:experimental_results}

\subsection{Individual Models}

For each of our three experiments, we ran 32 trials, each of which with weights randomly initialized prior to training and, due to the stochastic nature of the data augmentation, a different set of training images.  As a result, training progressed to different points in the loss surface resulting in a range of values for the top accuracies that were achieved on the test set.  See \autoref{tab:individual_models}.

\begin{table}[!htbp]
  \centering
  \begin{minipage}{.66\textwidth}
    \caption{Individual Models}
    \begin{tabularx}{\textwidth}{@{}Xrrrr@{}}
      \toprule
	      Experiment &Min &Max &Mean &SD \\
      \midrule
	      Not Learnable &99.71\% &\textbf{99.79\%} &0.997500 &0.0002190 \\
	      Random Init. &99.72\% &\textbf{99.78\%} &0.997512 &0.0001499 \\
	      Ones Init. &99.70\% &99.77\% &0.997397 &0.0001885 \\
      \bottomrule
    \end{tabularx}\\\label{tab:individual_models}
    \captionsetup{justification=justified,singlelinecheck=false}
    \caption*{32 trials of each individual model showed no statistically significant difference between the approach where the branch weights were not learnable and when the branch weights were learnable and randomly initialized.  The approach with learnable branch weights initialized to one was inferior to the approach that did not use learnable branch weights (p-value 0.024045) as well as being inferior to the approach with learnable weights initialized randomly (p-value 0.004449).  (SD is standard deviation)}
  \end{minipage}
\end{table}

The trial that achieved 99.79\% test accuracy establishes a new state of the art for a single model where the previous state of the art was 99.77\%~\cite{Sato2015}.  4 additional trials achieved 99.78\% test accuracy, also surpassing the previous state of the art.

Although all three experiments produced similar results, the experiment that initialized the learnable branch weights randomly had a higher average accuracy across a greater number of epochs than the experiment that used non-learnable and equal branch weights and higher than all epochs of the experiment that initialized the learnable branch weights to one (see \autoref{fig:average_test_accuracies}).  Additionally, the experiment that initialized the learnable branch weights randomly had a lower loss than either of the other two experiments across all epochs (see \autoref{fig:average_test_losses}).

\subsection{Ensembles}

Ensembling multiple models together and predicting based on the majority vote among the ensembled models routinely outperforms the individual models' performances.  Ensembling can refer to either completely different model architectures with different weights or the same model architecture after being trained multiple times and finding different sets of weights that correspond to different locations in the loss surface.  The previous state of the art of 99.82\% was achieved using an ensemble of 30 different randomly generated model architectures~\cite{Kowsari2018}.  Our ensembling method used the same architecture but with different weights.  We calculated the majority vote of the predictions for all possible combinations of the weights produced by the 32 trials.  We matched the previous state of the art with 4,544 ensembles. We surpassed this with 44 ensembles that achieved an accuracy of 99.83\% and established a new state of the art of 99.84\% with one ensemble.  See \autoref{tab:ensembles}.

\begin{table}[!htbp]
  \centering
  \begin{minipage}{.66\textwidth}
    \caption{Ensembles}
    \begin{tabularx}{\textwidth}{@{}Xccc@{}}
      \toprule
	      Accuracy: &\textbf{99.84\%} &\textbf{99.83\%} &99.82\% \\
      \midrule
	      Not Learnable &0 &4 &1,183 \\
	      Random Init. &0 &21 &2,069 \\
	      Ones Init. &1 &19 &1,292 \\
      \bottomrule
    \end{tabularx}\\\label{tab:ensembles}
    \captionsetup{justification=justified,singlelinecheck=false}
    \caption*{Shown here are the number of ensembles that were generated that either matched the previous state of the art of 99.82\% or exceeded it.}
  \end{minipage}
\end{table}

\begin{tabular}{@{}p{0.21in}p{2.70in}lp{0.05in}p{2.70in}l@{}}
  \multicolumn{3}{c}{\includegraphics[width=3in]{images/experiment_avg_top1s.png}} &
  \multicolumn{3}{c}{\includegraphics[width=3in]{images/experiment_avg_losses.png}} \-0.1in]
  &
  \begin{figure_in_table}\caption{Final branch weights (after 300 epochs) for all 32 trials (across the x-axis) of the experiment for which the branch weights were initialized to one.}\label{fig:branch_weights_ones_init}\end{figure_in_table} &
  &
  &
  \begin{figure_in_table}\caption{Final branch weights (after 300 epochs) for all 32 trials (across the x-axis) of the experiment for which the branch weights were initialized randomly.}\label{fig:branch_weights_rand_init}\end{figure_in_table}
  &
  \\
\end{tabular}

\subsection{Troublesome Digits}

Across all 96 trials, there was total agreement on 9,907 out of the 10,000 test samples.  There were 48 digits that were misclassified more often than not across all 96 trials, but only 21 digits were misclassified more often than not within the 32 trials of any one experiment.  This shows that although the accuracies of the models in the three experiments were quite similar, the different merge strategies of the three experiments did have a significant effect on classification.

Across all 96 trials, only 4 samples were misclassified in all models.  Those samples, as numbered by the order they appear in the MNIST test dataset (starting from 0) are 1901, 2130, 2293, and 6576.  The \textbf{Not Learnable} experiment had no trial that correctly predicted sample 3422.  The \textbf{Random Init.} experiment had no trial that correctly predicted sample 2597 correctly.  The \textbf{Ones Init.} experiment had no trial in which either 2597 or 3422 were predicted correctly.  It is interesting that, in addition to the 4 that no trial predicted correctly, the 2 that the \textbf{Ones Init.} experiment predicted incorrectly were the fifth digits not predicted by the \textbf{Not Learnable} and \textbf{Random Init.} experiments.

\begin{figure}[!htbp]
  \centering
  \setlength\tabcolsep{1pt}
  \begin{tabular}{@{}cccccc@{}}
    \includegraphics[width=0.4in]{images/misclassified_1901.png} &
    \includegraphics[width=0.4in]{images/misclassified_2130.png} &
    \includegraphics[width=0.4in]{images/misclassified_2293.png} &
    \includegraphics[width=0.4in]{images/misclassified_2597.png} &
    \includegraphics[width=0.4in]{images/misclassified_3422.png} &
    \includegraphics[width=0.4in]{images/misclassified_6576.png} \\
    9 & 4 & 9 & 5 & 6 & 7 \\
    1901 & 2130 & 2293 & 2597 & 3422 & 6576 \\
  \end{tabular}
  \caption{The Most Troublesome Digits}\label{fig:most_troublesome_digits}
\end{figure}

\subsection{MNIST State of the Art}

In \autoref{tab:previous_sota} we present a comparison of previous state of the art MNIST results for both single model evaluations and ensembles along with the results achieved in our experiments.
 
\begin{table}[!htbp]
  \caption{Current and Previous MNIST State of the Art Results}
  \centering
  \begin{tabularx}{\textwidth}{@{}p{.1in}Xrr@{}}
    \toprule
      Paper & & Year & Accuracy \\
    \midrule
      \multicolumn{4}{l}{Single Models} \\
    \midrule
      & Dynamic Routing Between Capsules\cite{Sabour2017} & 2017 & 99.75\% \\
      & Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures\cite{Hasanpour2016} & 2016 & 99.75\% \\
      & Batch-Normalized Maxout Network in Network\cite{Chang2015} & 2015 & 99.76\% \\
      & APAC:\@Augmented PAttern Classification with Neural Networks\cite{Sato2015} & 2015 & 99.77\% \\
      & Multi-Column Deep Neural Networks for Image Classification\cite{Ciresan2012} & 2012 & 99.77\% \\
      & \textbf{Using the method proposed in this paper (Branching \& Merging CNN w/HFCs)} & \textbf{2020} & \textbf{99.79\%} \\
    \midrule
      \multicolumn{4}{l}{Ensembles} \\
    \midrule
      & Regularization of Neural Networks using DropConnect\cite{Wan2013} & 2013 & 99.79\% \\
      & RMDL:\@Random Multimodel Deep Learning for Classification\cite{Kowsari2018} & 2018 & 99.82\% \\
      & \textbf{Using the method proposed in this paper (An ensemble of Branching \& Merging CNN w/HFCs)} & \textbf{2020} & \textbf{99.84\%} \\
    \bottomrule
  \end{tabularx}\label{tab:previous_sota}
\end{table}

How long a model takes to train is an important factor to consider when evaluating a neural network.  Indeed, it is an enabling factor during initial experimentation as faster training leads to a greater exploration of the design space.  In \autoref{tab:epochs_of_training} we present a comparison of the number of epochs of training used in experiments for the results achieved in the networks shown in \autoref{tab:previous_sota}.  Across all trials, our design achieved peak accuracy in an average of 88.35 epochs, with a minimum peak achieved in 25 epochs and a maximum peak achieved at epoch 266.  One trial achieved an accuracy of 99.78\%, surpassing the previous state of the art, in 56 epochs.  Since, we allowed all trials to run for up to 300 epochs, that is the number we report in \autoref{tab:epochs_of_training}.

\begin{table}[!htbp]
  \caption{Epochs of Training}
  \centering
  \begin{tabularx}{\textwidth}{@{}Xr@{}}
    \toprule
    	Paper & Epochs \\
    \midrule
      Dynamic Routing Between Capsules\cite{Sabour2017} & 1,200 \\
      APAC:\@Augmented PAttern Classification with Neural Networks\cite{Sato2015} & 15,000 \\
      Multi-Column Deep Neural Networks for Image Classification\cite{Ciresan2012} & 800 \\
      Regularization of Neural Networks using DropConnect\cite{Wan2013} & 1,200 \\
      RMDL:\@Random Multimodel Deep Learning for Classification\cite{Kowsari2018} & 120 \\
      \textbf{Branching \& Merging CNN w/HFCs} & 300 \\
    \bottomrule
  \end{tabularx}\\
  \captionsetup{justification=justified,singlelinecheck=false}
  \caption*{Neither~\cite{Hasanpour2016} nor~\cite{Chang2015} report on how many epochs their designs were trained for.}\label{tab:epochs_of_training}
\end{table}

\subsection{Experiments With and Without Branching and Merging and Homogeneous Filter Capsules}

In order to understand the effect of the novel design elements we introduced, we ran additional sets of paired experiments wherein the first set of experiments in a pair used the network design as described in this paper and the second set of experiments excludes the branching and merging and capsules.  This second network design is the same network design described in this paper and as shown in \autoref{fig:network_design_b} excluding the the first two branches, the stacking operation that combines the branches, and the homogeneous filter capsules in the main branch which were replaced with the typical flattening of the final convolution and a fully connected layer of neurons, one for each output class.  This results in a very typical convolutional neural network with 9  convolutions.  In our experiments, we labeled these two models M1 and M2, respectively (see \autoref{tab:additional_models}).

Then, in addition to performing these sets of paired experiments on MNIST, we performed them on Fashion-MNIST~\cite{Xiao2017}, CIFAR-10, and CIFAR-100~\cite{Krizhevsky2009}.

For the MNIST and Fashion-MNIST experiments, we used the same augmentation strategy as described in this paper.  For the CIFAR-10 and CIFAR-100 experiments, the augmentation strategy was as described in this paper with the exception of the translation operation (described in \autoref{sec:experimental_setup}), which works by detected the empty space around the MNIST and Fashion-MNIST images.  There is no such empty space in the CIFAR-10 and CIFAR-100 images.  Additionally, since CIFAR-10 and CIFAR-100 images are  pixels, are full color, and are comprised of more complex features, we ran an additional pair of experiments for each of these datasets that used additional convolutions.  We labeled these two additional models M3 and M4 (see \autoref{tab:additional_models}).

\textbf{For all four datasets, a model that included the branching and merging and HFCs achieved the highest mean accuracy with statistical significance.}

For MNIST, the higher accuracy was statistically significant with a p-value of 0.000757.  For Fashion-MNIST, the higher accuracy was statistically significant with a p-value of 0.000003.  Given that both of these datasets are monochromatic images with a size of  and our network was designed with those properties in mind, this is not especially surprising.  The fact that the accuracy for Fashion-MNIST was not competitive with current state of the art for that dataset is also not surprising as our network design was optimized for accuracy on classification of the Arabic numerals in the MNIST dataset.  This design encompasses the number of parameters used as well as the domain specific augmentation strategy, both of which would quite likely be different for achieving state of the art Fashion-MNIST accuracy.

For CIFAR-10, model M2 (without branching and merging and HFCs) achieved a statistically significant higher accuracy than M1 with a p-value of 0.027646, while model M3 achieved a statistically significant higher accuracy than M4 with a p-value of 0.018672.  The higher accuracy of model M3 relative to M2 was statistically significant with a p-value of 0.049882.

For CIFAR-100, model M1 (with branching and merging and HFCs) achieved a statistically significant higher accuracy than M2 with a p-value of 0.000196, and model M3 achieved a statistically significant higher accuracy than M4 with a p-value of 0.  The higher accuracy of model M3 relative to M1 was not statistically significant after 5 trials with a p-value of 0.094366.

\begin{table}[!htbp]
  \caption{Models Used for Experiments With and Without Branching and Merging and Homogeneous Filter Capsules}
  \centering
  \begin{tabularx}{\textwidth}{@{}lX@{}}
    \toprule
      Model & Description \\
    \midrule
      M1 & The model described in this paper \\
      M2 & M1 without branching and merging and HFCs \\
      M3 & M1 with 5 additional  convolutions and branches after convolutions 5, 9, and 13 \\
      M4 & M2 with 5 additional  convolutions \\
    \bottomrule
  \end{tabularx}\label{tab:additional_models}
\end{table}

\begin{table}[!htbp]
  \caption{Experiments With and Without Branching and Merging and Homogeneous Filter Capsules}
  \centering
  \begin{tabularx}{\textwidth}{@{}p{.05in}Xrrrr@{}}
    \toprule
      & & \multicolumn{4}{c}{Accuracy} \\
      & & Min & Max & Mean & SD \\
    \midrule
      \multicolumn{6}{@{}l}{MNIST} \\
    \midrule
      & M1 & 99.74\% & \textbf{99.76\%} & 0.997540 & 0.0000894 \\
      & M2 & 99.66\% & 99.73\% & 0.996940 & 0.0002702 \\
    \midrule
      \multicolumn{6}{@{}l}{Fashion-MNIST} \\
    \midrule
      & M1 & 93.42\% & \textbf{93.66\%} & 0.935440 & 0.0008989 \\
      & M2 & 92.85\% & 93.04\% & 0.929480 & 0.0008871 \\
    \midrule
      \multicolumn{6}{@{}l}{CIFAR-10} \\
    \midrule
      & M1 & 84.57\% & 85.44\% & 0.849020 & 0.0033656 \\
      & M2 & 85.01\% & 85.70\% & 0.853200 & 0.0024617 \\
      & M3 & 85.36\% & \textbf{85.71\%} & 0.855580 & 0.0014550 \\
      & M4 & 85.09\% & 85.50\% & 0.853040	& 0.0017530 \\
    \midrule
      \multicolumn{6}{@{}l}{CIFAR-100} \\
    \midrule
      & M1 & 50.42\% & 51.73\% & 0.512740 & 0.0051013 \\
      & M2 & 49.07\% & 50.19\% & 0.495760 & 0.0040470 \\
      & M3 & 51.41\% & \textbf{51.96\%} & 0.516320 & 0.0022410 \\
      & M4 & 46.27\% & 47.59\% & 0.469400	& 0.0053245 \\
    \bottomrule
  \end{tabularx}\\
  \captionsetup{justification=justified,singlelinecheck=false}
  \caption*{We conducted 5 trials of each unique type of experiment in order to establish statistical significance.  (SD is standard deviation)}\label{tab:additional_experiments}
\end{table}

\section{Conclusion}\label{sec:conclusion}

In this paper, we proposed using a simple convolutional neural network and established design principles as a basis for our architecture.  We then presented a design that branched out of the series of stacked convolutions at different points to capture different levels of abstraction and effective receptive fields, and from these branches, rather than flattening to individual scalar neurons, used capsules instead.

We also investigated three different methods of merging the output of the branches back into a single set of logits.  Each of the three merge strategies generated models that could be ensembled to create new state of the art results.  Although the experiment that initialized the branch weights to ones produced an ensemble with a higher accuracy than the other two experiments, the experiment that initialized the branch weights randomly produced the most ensembles at or exceeding the previous state of the art, as well as having a slightly higher average and lower standard deviation across the trials.  This suggests that the random initialization method is preferred.

Beyond the network architecture, we proposed a robust and domain specific data augmentation strategy aimed at simulating a wider variety of renderings of the digits.

In doing this work, we established new MNIST state of the art accuracies for both a single model and an ensemble.  In addition to the network design and augmentation strategy, the ability to use an adaptive gradient descent method~\cite{Byerly2019} allowed us to achieve this on consumer hardware (2x NVIDIA GeForce GTX 1080 Tis in an otherwise unremarkable workstation) and was an enabling factor in both initial explorations and the training of the 96 trials.

\printbibliography{}

\bigskip

The code used for all experiments and summary level data is publicly available on GitHub at: \href{https://github.com/AdamByerly/BMCNNwHFCs}{https://github.com/AdamByerly/BMCNNwHFCs}

\bigskip

\appendix

\section{Appendix}\label{sec:appendix}

\subsection{Digits Disagreed Upon}

What follows is the complete set of 93 digits that were predicted correctly by at least one model and incorrectly by at least one model.  These in combination with the digits from \autoref{fig:most_troublesome_digits} represent the complete set of digits that were not predicted correctly by all 96 trials.  Each image is captioned first by the class label in the test data set associated with the image, then the number of trials that predicted it correctly, and last the index of the digit in the test data. For example, the first image presented below has a class label of 9, 24 trials predicted that correctly, and it exists at index 193 in the MNIST test data.

\vspace{0.15in}

\centering
\setlength\tabcolsep{1pt}
\begin{tabular}{@{}ccccccccccccccc@{}}
  \includegraphics[width=0.4in]{images/hard_digits/193_9.png} &
  \includegraphics[width=0.4in]{images/hard_digits/247_4.png} &
  \includegraphics[width=0.4in]{images/hard_digits/321_2.png} &
  \includegraphics[width=0.4in]{images/hard_digits/359_9.png} &
  \includegraphics[width=0.4in]{images/hard_digits/409_1.png} &
  \includegraphics[width=0.4in]{images/hard_digits/412_5.png} &
  \includegraphics[width=0.4in]{images/hard_digits/445_6.png} &
  \includegraphics[width=0.4in]{images/hard_digits/447_4.png} &
  \includegraphics[width=0.4in]{images/hard_digits/582_8.png} &
  \includegraphics[width=0.4in]{images/hard_digits/625_6.png} &
  \includegraphics[width=0.4in]{images/hard_digits/659_2.png} &
  \includegraphics[width=0.4in]{images/hard_digits/708_4.png} &
  \includegraphics[width=0.4in]{images/hard_digits/716_1.png} &
  \includegraphics[width=0.4in]{images/hard_digits/846_7.png} &
  \includegraphics[width=0.4in]{images/hard_digits/938_3.png} \\
  9 & 4 & 2 & 9 & 1 & 5 & 6 & 4 & 8 & 6 & 2 & 4 & 1 & 7 & 3 \\
  24 & 31 & 6 & 77 & 31 & 33 & 85 & 76 & 59 & 74 & 70 & 30 & 31 & 31 & 88 \\
  193 & 247 & 321 & 359 & 409 & 412 & 445 & 447 & 582 & 625 & 659 & 708 & 716 & 846 & 938 \.1 in]
\end{tabular}
\begin{tabular}{@{}ccccccccccccccc@{}}
  \includegraphics[width=0.4in]{images/hard_digits/1903_7.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2018_1.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2035_5.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2040_5.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2053_4.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2098_2.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2326_0.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2355_1.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2414_9.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2454_6.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2462_2.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2597_5.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2654_6.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2720_9.png} &
  \includegraphics[width=0.4in]{images/hard_digits/2771_4.png} \\
  7 & 1 & 5 & 5 & 4 & 2 & 0 & 1 & 9 & 6 & 2 & 5 & 6 & 9 & 4 \\
  93 & 31 & 31 & 88 & 31 & 31 & 61 & 31 & 61 & 34 & 44 & 1 & 76 & 58 & 9 \\
  1903 & 2018 & 2035 & 2040 & 2053 & 2098 & 2326 & 2355 & 2414 & 2454 & 2462 & 2597 & 2654 & 2720 & 2771 \.1 in]
\end{tabular}
\begin{tabular}{@{}ccccccccccccccc@{}}
  \includegraphics[width=0.4in]{images/hard_digits/4504_2.png} &
  \includegraphics[width=0.4in]{images/hard_digits/4507_1.png} &
  \includegraphics[width=0.4in]{images/hard_digits/4571_6.png} &
  \includegraphics[width=0.4in]{images/hard_digits/4699_6.png} &
  \includegraphics[width=0.4in]{images/hard_digits/4740_3.png} &
  \includegraphics[width=0.4in]{images/hard_digits/4761_9.png} &
  \includegraphics[width=0.4in]{images/hard_digits/4823_9.png} &
  \includegraphics[width=0.4in]{images/hard_digits/4860_4.png} &
  \includegraphics[width=0.4in]{images/hard_digits/5654_7.png} &
  \includegraphics[width=0.4in]{images/hard_digits/5955_3.png} &
  \includegraphics[width=0.4in]{images/hard_digits/6371_3.png} &
  \includegraphics[width=0.4in]{images/hard_digits/6597_0.png} &
  \includegraphics[width=0.4in]{images/hard_digits/6625_8.png} &
  \includegraphics[width=0.4in]{images/hard_digits/6783_1.png} &
  \includegraphics[width=0.4in]{images/hard_digits/6883_1.png} \\
  2 & 1 & 6 & 6 & 3 & 9 & 9 & 4 & 7 & 3 & 3 & 0 & 8 & 1 & 1 \\
  79 & 77 & 31 & 88 & 29 & 72 & 23 & 86 & 43 & 61 & 91 & 31 & 40 & 31 & 30 \\
  4504 & 4507 & 4571 & 4699 & 4740 & 4761 & 4823 & 4860 & 5654 & 5955 & 6371 & 6597 & 6625 & 6783 & 6883 \.1 in]
\end{tabular}
\begin{tabular}{@{}ccc@{}}
  \includegraphics[width=0.4in]{images/hard_digits/9750_3.png} &
  \includegraphics[width=0.4in]{images/hard_digits/9839_2.png} &
  \includegraphics[width=0.4in]{images/hard_digits/9850_0.png} \\
  3 & 2 & 0 \\
  31 & 61 & 93 \\
  9750 & 9839 & 9850 \.1 in]
\end{tabular}

\end{document}
