\PassOptionsToPackage{usenames}{color}
\pdfoutput=1 \documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{style/acl2018}



\usepackage{graphicx} \usepackage{url} 

\aclfinalcopy \def\aclpaperid{1158} 

\usepackage{times}
\usepackage{natbib}



\usepackage{relsize} 





\usepackage{microtype}
\usepackage{ dsfont }
\usepackage[boxed]{algorithm2e}
\renewcommand\AlCapFnt{\small}
\usepackage[small,bf,skip=5pt]{caption}
\usepackage{sidecap} \usepackage{rotating}	

\usepackage{paralist}

\usepackage{titlesec}
\titleformat*{\subparagraph}{\itshape}
\titlespacing{\subparagraph}{1em}{0pt}{1em}

\usepackage{lingmacros}
\newcommand{\exref}[1]{(\ref{#1})} 

\usepackage[shortlabels]{enumitem} \setitemize{noitemsep,topsep=0em} \setenumerate{noitemsep,leftmargin=0em,itemindent=13pt,topsep=0em}

\usepackage{xspace}
\usepackage{xparse} 

\newcommand{\indicator}[1]{I_{\{#1\}}} 


\usepackage{textcomp}


\usepackage{framed}

\usepackage{listings}



\usepackage{amssymb}	\usepackage{amsmath}

\usepackage{mathptmx}	\usepackage[scaled=.8]{beramono}
\usepackage[scaled=.85]{helvet}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{MnSymbol}	

\usepackage{latexsym}


\addtolength{\textfloatsep}{-0.2cm}
\addtolength{\abovedisplayskip}{-0.2cm}
\addtolength{\belowdisplayskip}{-0.2cm}





\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs} \usepackage{multicol}
\usepackage{footnote}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}} 


\usepackage[usenames]{color}
\usepackage{xcolor}

\newcommand{\cfbox}[2]{\colorlet{currentcolor}{.}{\color{#1}\fbox{\color{currentcolor}#2}}}

\usepackage[normalem]{ulem} \usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage[edges]{forest}
\usetikzlibrary{arrows,positioning,calc} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}






\usepackage{nameref}
\usepackage{cleveref}


\crefformat{part}{\S#2#1#3}
\crefformat{chapter}{\S#2#1#3}
\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\crefformat{paragraph}{\P#2#1#3}
\crefformat{subparagraph}{\P#2#1#3}
\crefmultiformat{section}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsection}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsubsection}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{paragraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subparagraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{section}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsubsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{paragraph}{\mbox{\P\P#3#1#4--#5#2#6}}
\crefrangeformat{subparagraph}{\mbox{\P\P#3#1#4--#5#2#6}}
\crefname{part}{Part}{Parts}
\Crefname{part}{Part}{Parts}
\crefname{chapter}{ch.}{ch.}
\Crefname{chapter}{Ch.}{Ch.}
\crefname{figure}{figure}{figures}
\crefname{subfigure}{figure}{figures}
\Crefname{subfigure}{Figure}{Figures}
\crefname{appsec}{appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}
\crefname{algocf}{algorithm}{algorithms}
\Crefname{algocf}{Algorithm}{Algorithms}
\crefname{enums,enumsi}{example}{examples}
\Crefname{enums,enumsi}{Example}{Examples}
\crefname{}{example}{examples} \Crefname{}{Example}{Examples}
\crefformat{enums}{(#2#1#3)}
\crefformat{enumsi}{(#2#1#3)}
\crefrangeformat{enums}{\mbox{(#3#1#4--#5#2#6)}}
\crefrangeformat{enumsi}{\mbox{(#3#1#4--#5#2#6)}}
\crefformat{}{(#2#1#3)}
\crefname{xnumi}{example}{examples} \crefname{xnumi}{example}{examples} \Crefname{xnumii}{Example}{Examples} \Crefname{xnumii}{Example}{Examples} \crefformat{xnumi}{(#2#1#3)} \crefformat{xnumii}{(#2#1#3)} \crefrangeformat{enums}{\mbox{(#3#1#4--#5#2#6)}}
\crefrangeformat{enumsi}{\mbox{(#3#1#4--#5#2#6)}}
\crefrangeformat{xnumi}{\mbox{(#3#1#4--#5#2#6)}} \crefrangeformat{xnumii}{\mbox{(#3#1#4--#5#2#6)}} \crefmultiformat{enumsi}{(#2#1#3}{, #2#1#3)}{, #2#1#3}{, #2#1#3)}
\crefmultiformat{xnumi}{(#2#1#3}{, #2#1#3)}{, #2#1#3}{, #2#1#3)} \crefmultiformat{xnumii}{(#2#1#3}{, #2#1#3)}{, #2#1#3}{, #2#1#3)} \crefrangemultiformat{enumsi}{(#3#1#4--#5#2#6}{, #3#1#4--#5#2#6)}{, #3#1#4--#5#2#6}{, #3#1#4--#5#2#6)}
\crefrangemultiformat{xnumi}{(#3#1#4--#5#2#6}{, #3#1#4--#5#2#6)}{, #3#1#4--#5#2#6}{, #3#1#4--#5#2#6)} \crefrangemultiformat{xnumii}{(#3#1#4--#5#2#6}{, #3#1#4--#5#2#6)}{, #3#1#4--#5#2#6}{, #3#1#4--#5#2#6)} 

\ifx\creflastconjunction\undefined \newcommand{\creflastconjunction}{, and\nobreakspace} \else \renewcommand{\creflastconjunction}{, and\nobreakspace} \fi 

\newcommand*{\Fullref}[1]{\hyperref[{#1}]{\Cref*{#1}: \nameref*{#1}}}
\newcommand*{\fullref}[1]{\hyperref[{#1}]{\cref*{#1}: \nameref{#1}}}
\newcommand{\fnref}[1]{fn.~\ref{#1}} \newcommand{\Fnref}[1]{Fn.~\ref{#1}}

\renewcommand\thesubfigure{(\alph{subfigure})}


\newcommand{\backtick}[0]{\textasciigrave}


\NewDocumentEnvironment{itmize}{}{\begin{itemize}[noitemsep]}{\end{itemize}}
\NewDocumentEnvironment{enumrate}{}{\begin{enumerate}[noitemsep]}{\end{enumerate}}
\let\Item\item
\renewcommand\enddescription{\endlist\global\let\item\Item}
\NewDocumentEnvironment{describe}{}{\renewcommand\item[1][]{\Item \textbf{##1:} }\begin{itemize}[\null,leftmargin=0em]}{\end{itemize}} \NewDocumentEnvironment{edescribe}{}{\renewcommand\item[1][]{\Item \textbf{##1:} }\begin{enumerate}}{\end{enumerate}}

\newcommand{\exemplars}{\mathrm{ex}}
\newcommand{\fulltext}{\mathrm{ft}}

\usepackage{color}
\newcommand\bmmax{0} \usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0.05,0.6,0.05}
\definecolor{mdblue}{rgb}{0,0,0.7}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{ltgray}{rgb}{0.7,0.7,0.7}
\definecolor{purple}{rgb}{0.7,0,1.0}
\definecolor{lavender}{rgb}{0.65,0.55,1.0}

\lstset{
  upquote=true,
  showstringspaces=false,
  formfeed=\newpage,
  tabsize=1,
  commentstyle=\itshape\color{lavender},
  basicstyle=\normalsize\smaller[0.5]\ttfamily,
  keywordstyle=\bfseries\color{magenta},
  emph={upward,downward,tc},
  emphstyle=\underbar,
  aboveskip=1ex plus .25ex minus .25ex,
  belowskip=1ex plus .25ex minus .25ex,
  xleftmargin=1em,
  xrightmargin=1em
}
\lstnewenvironment{Python}[1][]
  {\lstset{language=Python,
           morekeywords=as,
           #1}}
  {}
  
\lstnewenvironment{Output}[1][]
  {\lstset{#1}}
  {}
\renewcommand{\lstlistingname}{Algorithm}


\newcommand{\ensuretext}[1]{#1}
\newcommand{\jhmarker}{\ensuretext{\textcolor{mdgreen}{\ensuremath{^{\textsc{J}}_{\textsc{H}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\mkmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{M}}_{\textsc{K}}}}}}
\newcommand{\smmarker}{\ensuretext{\textcolor{orange}{\ensuremath{^{\textsc{S}}_{\textsc{M}}}}}}
\newcommand{\vsmarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{V}}_{\textsc{S}}}}}}
\newcommand{\jpmarker}{\ensuretext{\textcolor{purple}{\ensuremath{^{\textsc{J}}_{\textsc{P}}}}}}
\newcommand{\ajbmarker}{\ensuretext{\textcolor{mdblue}{\ensuremath{^{\textsc{AJ}}_{\textsc{B}}}}}}
\newcommand{\oamarker}{\ensuretext{\textcolor{brown}{\ensuremath{^{\textsc{O}}_{\textsc{A}}}}}}
\newcommand{\asmarker}{\ensuretext{\textcolor{brown}{\ensuremath{^{\textsc{A}}_{\textsc{S}}}}}}

\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
\newcommand{\jh}[1]{\arkcomment{\jhmarker}{#1}{mdgreen}}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\mk}[1]{\arkcomment{\mkmarker}{#1}{red}}
\newcommand{\sm}[1]{\arkcomment{\smmarker}{#1}{orange}}
\newcommand{\jp}[1]{\arkcomment{\jpmarker}{#1}{purple}}
\newcommand{\ajb}[1]{\arkcomment{\ajbmarker}{#1}{mdblue}}
\newcommand{\vs}[1]{\arkcomment{\vsmarker}{#1}{blue}}
\newcommand{\oa}[1]{\arkcomment{\oamarker}{#1}{brown}}
\newcommand{\as}[1]{\arkcomment{\asmarker}{#1}{mdgreen}}

\newcommand{\com}[1]{}
\newcommand{\wts}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu} \newcommand{\cost}{c}


\newcommand{\term}[1]{\textbf{#1}} 

\newcommand{\citeposs}[2][]{\citeauthor{#2}'s (\citeyear[#1]{#2})}
\newcommand{\Citeposs}[2][]{\Citeauthor{#2}'s (\citeyear[#1]{#2})}

\addtolength{\textfloatsep}{-.3cm} \addtolength{\abovedisplayskip}{-1cm} \addtolength{\belowdisplayskip}{-1cm} \setlength{\belowcaptionskip}{-.15cm}
\setlength{\intextsep}{0pt plus 2pt}   


\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\z@}{.2ex \@plus 1ex \@minus .2ex}{-1em}{\normalfont\normalsize\bfseries}}
\makeatother



\newcommand{\w}[1]{\textit{#1}}	\newcommand{\p}[1]{\textbf{\textsf{#1}}} \newcommand{\lbl}[1]{\textsc{#1}} \newcommand{\sst}[1]{\lbl{#1}} \newcommand{\nsst}[1]{\sst{n:#1}} \newcommand{\vsst}[1]{\sst{v:#1}} \newcommand{\psst}[1]{\textcolor{mdgreen}{\sst{#1}}} \newcommand{\olbl}[1]{\textcolor{purple}{\textrm{#1}}} 

\newcommand{\rf}[2]{\psst{#1}\psst{#2}}
\newcommand{\rff}[3]{\psst{#1}\psst{#2}\psst{#3}}


\newcommand{\tg}[1]{\texttt{#1}}	\newcommand{\gfl}[1]{\mbox{\textsmaller{\texttt{#1}}}}	\newcommand{\tagdef}[1]{#1\hfill} \newcommand{\tagt}[2]{\ensuremath{\underset{\textrm{\textlarger{\tg{#2}}}\strut}{\w{#1}\rule[-.3\baselineskip]{0pt}{0pt}}}} \newcommand{\glosst}[2]{\ensuremath{\underset{\textrm{#2}}{\textrm{#1}}}} \newcommand{\AnnA}[0]{\mbox{\textbf{Ann-A}}} \newcommand{\AnnB}[0]{\mbox{\textbf{Ann-B}}} \newcommand{\sys}[1]{\mbox{\textbf{#1}}}   \newcommand{\dataset}[1]{\mbox{\textsc{#1}}}	\newcommand{\datasplit}[1]{\mbox{\textbf{#1}}}	

\newcommand{\fnf}[1]{\textsc{\textsf{#1}}} \newcommand{\fnr}[1]{\textbf{\textsf{#1}}} \newcommand{\fnrel}[1]{\textsl{#1}} \newcommand{\fnst}[1]{\textsl{#1}} \newcommand{\fnlu}[1]{\textsf{#1}} \newcommand{\pbf}[1]{\mbox{\textsf{#1}}} \newcommand{\pbr}[1]{\textbf{\textsf{#1}}} \newcommand{\vpred}[1]{\textbf{#1}} 


\newcommand{\lex}[1]{\textit{#1}} \newcommand{\pex}[1]{\textit{#1}} 

\newcommand{\gap}[0]{\ \ } \newcommand{\tat}[0]{\textasciitilde}

\newcommand{\shortlong}[2]{#1} \newcommand{\confversion}[1]{#1}
\newcommand{\srsversion}[1]{}
\newcommand{\finalversion}[1]{#1}
\newcommand{\shortversion}[1]{}
\newcommand{\considercutting}[1]{#1}
\newcommand{\longversion}[1]{#1} \newcommand{\subversion}[1]{#1} \newcommand{\draftnotice}[1]{} \newcommand{\anonversion}[1]{#1} \newcommand{\nonanonversion}[1]{} 

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}




\hyphenation{WordNet}
\hyphenation{WordNets}
\hyphenation{FrameNet}
\hyphenation{SemCor}
\hyphenation{SemEval}
\hyphenation{ParsedSemCor}
\hyphenation{VerbNet}
\hyphenation{PennConverter}
\hyphenation{an-aly-sis}
\hyphenation{an-aly-ses}
\hyphenation{base-line}
\hyphenation{comb-over}
\hyphenation{de-ve-lop-ed}
\hyphenation{news-text}
\hyphenation{nomi-nal}
\hyphenation{per-cept}
\hyphenation{per-cepts}
\hyphenation{post-edit-ing}
\hyphenation{shriv-eled}
\hyphenation{Huddle-ston}
\hyphenation{par-ti-ci-pant}
\hyphenation{par-ti-ci-pants}
\hyphenation{par-ti-ci-pa-tion}


\usepackage{gb4e} 


\title{Comprehensive Supersense Disambiguation of\\ English Prepositions and Possessives}

\newcommand{\emldisplay}[2]{\texttt{\href{mailto:#1}{#2}}}
\newcommand{\eml}[1]{\textsmaller[.5]{\emldisplay{#1}{#1}}}

\author{
Nathan Schneider\thanks{~~\emldisplay{nathan.schneider@georgetown.edu}{nathan.schneider@georgetown.edu}} \\
	\textsmaller[.5]{Georgetown University} 
     \And
Jena D. Hwang \\
	\textsmaller[.5]{IHMC} 
\And
Vivek Srikumar \\
	\textsmaller[.5]{University of Utah} 
\AND
Jakob Prange\\
\bf Austin Blodgett \\
	\textsmaller[.5]{Georgetown University} 
\And 
Sarah R. Moeller \\
	\textsmaller[.5]{University of Colorado Boulder} 
\And
Aviram Stern\\
\bf Adi Bitan\\
\bf Omri Abend \\
	\textsmaller[.5]{Hebrew University of Jerusalem} 
}


\date{}

\begin{document}
\maketitle
\begin{abstract}
  Semantic relations are often signaled with prepositional or possessive marking---but extreme polysemy bedevils their analysis and automatic interpretation.
  We introduce a new annotation scheme, corpus, and task for the disambiguation 
  of prepositions and possessives in English.
  Unlike previous approaches, our annotations are comprehensive with respect to types and tokens of these markers; 
  use broadly applicable supersense classes rather than fine-grained dictionary definitions; 
  unite prepositions and possessives under the same class inventory; 
  and distinguish between a marker's \emph{lexical} contribution and the \emph{role} it marks in the context of a predicate or scene.
Strong interannotator agreement rates, as well as encouraging disambiguation results with established supervised methods, speak to the viability of the scheme and task.
\end{abstract}

\section{Introduction}

Grammar, as per a common metaphor, gives speakers of a language a shared toolbox to construct and deconstruct meaningful and fluent utterances. Being highly analytic, English relies heavily on word order and closed-class function words like prepositions, determiners, and conjunctions. Though function words bear little semantic content, they are nevertheless crucial to the meaning. Consider prepositions: they serve, for example, to convey place and time (\pex{We met \p{at}/\p{in}/\p{outside} the restaurant \p{for}/\p{after} an hour}), to express configurational relationships like quantity, possession, part/whole, and membership (\pex{the coats \p{of} dozens \p{of} children \p{in} the class}), and to indicate semantic roles in argument structure (\pex{Grandma cooked dinner \p{for} the children} vs.~\pex{Grandma cooked the children \p{for} dinner}).
Frequent prepositions like \p{for} are maddeningly polysemous, their interpretation depending especially on the object of the preposition---\pex{I rode the bus \p{for} 5 dollars/minutes}---and the governor of the prepositional phrase (PP): \pex{I Ubered\slash asked \p{for} \\vdots\vdots\vdots\kappa\kappauuuuFF_1FF_1FF_1$ score of 90.2\% (P=85.3\%, R=95.6\%) on the test set.}

\paragraph{Classification.}
Along with the statistical classifier results in \cref{tab:overall}, we also
report performance for the most frequent baseline, which selects
the most frequent role--function label pair given the (gold) lemma
according to the training data.  Note that all learned classifiers,
across all settings, outperform the most frequent baseline for both
role and function prediction. The feature-rich and the neural models
perform roughly equivalently despite the significantly different
modeling strategies. 

\paragraph{Function and scene role performance.} 
Function prediction is consistently more accurate than role prediction,
with roughly a 10-point gap across all systems. This mirrors a
similar effect in the interannotator agreement scores (see
\cref{sec:iaa}), and may be due to the reduced ambiguity of functions compared to roles (as attested by the baseline's higher accuracy for functions than roles), and by the more literal nature
of function labels, as opposed to role labels that often require more
context to determine.

\paragraph{Impact of automatic syntax.}
Automatic syntactic analysis decreases scores by 4 to 7
points, most likely due to parsing errors which affect the
identification of the preposition's object and governor.  
In the auto ID\slash auto syntax condition, 
the worse target ID performance with automatic parses (noted above)
contributes to lower classification scores.




\subsection{Errors \& Confusions}
\label{sec:errors-confusions}

We can use the structure of the SNACS hierarchy
to probe classifier performance. As with the interannotator study, we evaluate the
accuracy of predicted labels when they are coarsened post~hoc by moving up the
hierarchy to a specific depth. \Cref{tab:coarsening-disambig}
shows this for the feature-rich classifier for different
depths, with depth-1 representing the coarsening of the labels into
the 3~root labels. Depth-4 (Exact) represents the full results in \cref{tab:overall}. 
These results show that the classifiers often mistake a label for another that is nearby in the hierarchy. 
\begin{table}[t]\centering\small
\begin{tabular}{lccc}
        & Labels    & Role   & Function \\
\toprule
Exact   & 47        & 67.9\% & 79.4\%   \\
Depth-3 & 43        & 67.9\% & 79.6\%   \\
Depth-2 & 26        & 76.2\% & 86.2\%   \\
Depth-1 & 3         & 86.0\% & 93.8\%   \\
\end{tabular}
\caption{Accuracy of the feature-rich model (gold identification and syntax) on the test set (480~tokens) with different levels of hierarchy coarsening of its output.
``Labels'' refers to the number of labels in the training set after coarsening. 
}
\label{tab:coarsening-disambig}
\end{table}
Examining the most frequent confusions of both models, 
we observe that \psst{Locus} is overpredicted (which makes sense as it is most frequent overall), and \psst{SocialRole}--\psst{OrgRole} and
\psst{Gestalt}--\psst{Possessor}  are often confused (they are close in the hierarchy: one inherits from the other).















\com{
\begin{table*}[] 
    \centering
    \small
\begin{tabular}{@{}llll|lll|lll@{}} 
        \toprule
        & \multicolumn{3}{c}{All} & \multicolumn{3}{c}{MWEs Only} & \multicolumn{3}{c}{MWPs Only} \\ 
        \midrule
        & Role      & Func.          & Exact        & Role           & Func.            & Exact &  Role           & Func.            & Exact      \\
        \midrule
        Neural: gold syntax & & & & & & & & & \\
        Neural: auto syntax & & & & & & & & & \\
        MF & & & & & & & & & \\
        \bottomrule
    \end{tabular}
\caption{Accuracy of disambiguation baselines with gold-standard unit identification (in percents).   
             Rows correspond to systems, while columns correspond to the evaluation metrics.
             }
\end{table*}

\begin{table*}[] 
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{@{}llll|lll|lll|lll|lll|lll|lll|lll|lll@{}} 
        \toprule
        & \multicolumn{9}{c}{All} & \multicolumn{9}{c}{MWEs Only} & \multicolumn{9}{c}{MWPs Only} \\  & \multicolumn{3}{c}{Role} & \multicolumn{3}{c}{Func.} & \multicolumn{3}{c|}{Exact} & \multicolumn{3}{c}{Role} & \multicolumn{3}{c}{Func.} & \multicolumn{3}{c|}{Exact} & \multicolumn{3}{c}{Role} & \multicolumn{3}{c}{Func.} & \multicolumn{3}{c}{Exact} \\
        \midrule
        & P & R & F & P & R & F & P & R & F & P & R & F & P & R & F & P & R & F & P & R & F & P & R & F & P & R & F \\
        \midrule
        NN: gold & & & & & & & & & & & & & & & & & & & & & & & & & & &\\
        NN: auto & & & & & & & & & & & & & & & & & & & & & & & & & & &\\
        MF & & & & & & & & & & & & & & & & & & & & & & & & & & & \\
        \bottomrule
    \end{tabular}
    }
    \caption{Precision, Recall, and F-score of disambiguation baselines with automatic unit identification (in percents).   
             Rows correspond to systems, while columns correspond to the evaluation metrics.    
    }
    
\end{table*}
}

\com{
\begin{table*}[]
	\centering
	\begin{tabular}{@{}lccc|ccc|ccc|ccc@{}}
		\toprule
		& \multicolumn{3}{c|}{Gold ID} & \multicolumn{9}{c}{Auto ID} \\
		& Role & Func. & Exact & \multicolumn{3}{c}{Role} & \multicolumn{3}{c}{Func.} & \multicolumn{3}{c}{Exact} \\
		& Acc. & Acc. &  Acc. & P & R & F & P & R & F & P & R & F  \\
		\midrule
		NN-Gold & 0.83 & 0.83 & 0.79 & 0.7 & 0.3 & 0.4 & 0.7 & 0.3 & 0.4 & 0.7 & 0.3 & 0.4 \\
		NN-Auto & 0.79 & 0.79 & 0.67 & 0.58 & 0.29 & 0.39 & 0.58 & 0.29 & 0.39 & 0.58 & 0.29 & 0.39 \\
		Tratz-Gold & & & & & & & & & & & &\\
		Tratz-Auto & & & & & & & & & & & &\\
		MF & 0.33 & 0.67 & 0.33 & 0.33 & 0.17 & 0.22 & 0.33 & 0.17 & 0.22 & 0.33 & 0.17 & 0.22 \\
		\bottomrule
	\end{tabular}
	\caption{Performance of disambiguation baselines on Multi-word expressions. Table structure is the same as in \cref{tab:overall}.
		\label{tab:mwes}
	}
	
\end{table*}

\begin{table*}[]
	\centering
	\begin{tabular}{@{}lccc|ccc|ccc|ccc@{}}
		\toprule
		& \multicolumn{3}{c|}{Gold ID} & \multicolumn{9}{c}{Auto ID} \\
		& Role & Func. & Exact & \multicolumn{3}{c}{Role} & \multicolumn{3}{c}{Func.} & \multicolumn{3}{c}{Exact} \\
		& Acc. & Acc. &  Acc. & P & R & F & P & R & F & P & R & F  \\
		\midrule
		NN-Gold & 0.78 & 0.78 & 0.78 & 0.6 & 0.4 & 0.5 & 0.6 & 0.4 & 0.5 & 0.6 & 0.4 & 0.5 \\
		NN-Auto & 0.89 & 0.89 & 0.89 & 0.67 & 0.44 & 0.53 & 0.67 & 0.44 & 0.53 & 0.67 & 0.44 & 0.53 \\
		Tratz-Gold & & & & & & & & & & & &\\
		Tratz-Auto & & & & & & & & & & & &\\
		MF & 0.78 & 0.89 & 0.78 & 0.57 & 0.44 & 0.50 & 0.57 & 0.44 & 0.50 & 0.57 & 0.44 & 0.50 \\
		\bottomrule
	\end{tabular}
	\caption{Performance of disambiguation baselines on Multi-word prepositions. Table structure is the same as in \cref{tab:overall}.
		\label{tab:mwps}
	}
	
\end{table*}
}



\section{Conclusion}

This paper introduced a new approach to comprehensive analysis of the semantics of prepositions and possessives in English, backed by a thoroughly documented hierarchy and annotated corpus.
We found good interannotator agreement and provided initial supervised disambiguation results.
We expect that future work will develop methods to scale the annotation process beyond requiring highly trained experts;
bring this scheme to bear on other languages; 
and investigate the relationship of our scheme to more structured semantic representations, which could lead to more robust models.
Our guidelines, corpus, and software are available at \url{https://github.com/nert-gu/streusle/blob/master/ACL2018.md}.


\section*{Acknowledgments}

We thank Oliver Richardson, whose codebase we adapted for this project; Na-Rae Han, Archna Bhatia, Tim O'Gorman, Ken Litkowski, Bill Croft, and Martha Palmer for helpful discussions and support; and anonymous reviewers for useful feedback.
This research was supported in part by DTRA HDTRA1-16-1-0002/Project \#1553695, by DARPA 15-18-CwC-FP-032, and by grant 2016375 from the United States--Israel Binational Science Foundation (BSF), Jerusalem, Israel.


\bibliographystyle{aclnatbib}
\bibliography{style/pssdisambig}

\newpage

\phantom{xxxxxxx}

\newpage

\appendix



\section{Detailed IAA Analysis}\label{sec:detailed-iaa}

\paragraph{Individual annotators.}
Five annotators took part in this study. All are computational linguistics researchers with advanced training in linguistics. 
Their involvement in the development of the scheme falls on a spectrum:
Annotator~A was the leader of the project and lead author of the guidelines. 
Annotator~B was the second most active figure in guidelines development for an extended period, but took a break of several months in the period when the guidelines were finalized (prior to the pilot study). 
Annotator~C was involved in the later stages of guidelines development.
Annotator~D was involved only at the very end of guidelines development, and primarily learned the scheme from reading the annotation manual. 
Annotator~E was not involved in developing the guidelines and learned the scheme solely from reading the manual (and consulting with the guidelines developers for clarification on a few points).
Annotators A, B, and C are native speakers of English, while Annotators D and E are nonnative but highly fluent speakers.

\Cref{tab:pairwise-iaa} shows that
agreement rates of individual pairs of annotators 
range between 71.8\% and 78.7\% for roles and between 74.1\% and 88\% for functions. 
This is high for a scheme with so many labels to choose from.
Interestingly, there is not an obvious relationship in general between annotators' backgrounds (native language, amount of exposure to the scheme) and their agreement rates.
It is encouraging that Annotators D and E, despite recently learning the scheme from the guidelines, had similar agreement rates to others.

\paragraph{Common confusions.}
In \cref{fig:confusion} we visualize labels confused by annotators in chapters 4 and 5 of \emph{The Little Prince} (\cref{sec:iaa}), summed over all pairs of annotators. The red and blue lines correspond to the local semantic groupings of categories in the hierarchy. Confusions happening within the triangles closest to the diagonal are therefore more expected than confusions farther out in the matrix. 
As discussed in \cref{sec:iaa}, most disagreements actually do fall within these clusters (of varying granularity), indicating the scheme's robustness.

The three most frequently confused scene roles are \psst{Agent}/\psst{Originator} (\textit{\textbf{his} report}, under \psst{Participant}), \psst{Gestalt}/\psst{Whole} (\textit{the soil \textbf{of} that planet}, \psst{Gestalt} is the parent of \psst{Whole}), and \psst{Theme}/\psst{Topic} (\textit{I am not at all sure \textbf{of} success}, \psst{Theme} is the parent of \psst{Topic}).
The three most frequently confused functions are \psst{Gestalt}/\psst{Possessor} (\textit{\textbf{your} planet}, \psst{Gestalt} is the parent of \psst{Possessor}), \psst{Theme}/\psst{Topic}, and \psst{Locus}/\psst{Manner} (\textit{the astronomer had presented it ... \textbf{in} a great demonstration}, both are children of \psst{Circumstance}).

\begin{figure*}[]\small\centering
\includegraphics[width=1.3\textwidth]{fig/confusions.pdf}
\caption{Confusion matrices for role (bottom/left) and function (top/right) labels, summed across all annotator pairs.}
\label{fig:confusion}
\end{figure*}

\begin{table}\centering\small
\begin{tabular}{llcccccc}
  &      & B    & C    & D    & E    & avg  & plr \\
\toprule
\multirow{2}{*}{A} & role & 78.2 & 74.1 & 78.7 & 74.5 & 76.4 & 86.1 \\
                   & fxn  & 81.5 & 84.3 & 88.0 & 81.5 & 83.8 & 90.3 \\\midrule
\multirow{2}{*}{B} & role & 	 & 73.1 & 74.5 & 71.8 & 74.4 & 82.9 \\
                   & fxn  &      & 77.3 & 81.0 & 74.1 & 78.5 & 83.8 \\\midrule
\multirow{2}{*}{C} & role &      &      & 73.6 & 72.7 & 73.4 & 80.1 \\
                   & fxn  &      &      & 83.3 & 80.6 & 81.4 & 88.0 \\\midrule
\multirow{2}{*}{D} & role &      &      &      & 73.1 & 75.0 & 84.7 \\
                   & fxn  &      &      &      & 81.0 & 83.3 & 91.7 \\\midrule
\multirow{2}{*}{E} & role &      &      &      &      & 73.0 & 83.3 \\
                   & fxn  &      &      &      &      & 79.3 & 86.1 \\
\end{tabular}
\caption{Pairwise interannotator agreement rates, each annotator's average agreement rate with others (``avg''), and each annotator's rate of agreeing with the label chosen by the plurality of annotators (``plr''). Tokens for which there is no plurality (6 for both role and function) are included and counted as disagreement for all annotators.
Figures are exact label match percentages.}
\label{tab:pairwise-iaa}
\end{table}

\section{Features of the Feature-rich Model}

For each of the neighboring words of the word or phrase to be classified (as described in \cref{sec:classifier}),  we extracted indicator features for:
\begin{enumerate}[nosep]
\item the lowercased word, capitalization, and universal and extended
  POS tags,
\item the word being present in WordNet,
\item WordNet synsets for the first and all senses,
\item the WordNet lemma and lexicographer file name,
\item part, member, and substance holonyms of the word,
\item Roget thesaurus divisions of the word, if it exists,
\item any named entity label associated with the word,
\item its two and three letter character prefixes and suffixes, and
\item common affixes that produce nouns, verbs, adjectives, spatial or
  temporal words, and gerunds.
\end{enumerate}

\section{Hyperparameters for the Neural Model}

\Cref{tab:hyperparams} presents the hyperparameters used by the neural system, for each of the four settings. 

\begin{table*}[]
	\small
	\centering
	\begin{tabular}{@{}l|cccc@{}}
		\toprule
		Hyperparameter & Auto ID/Auto Prep. & Auto ID/Gold Prep. & Gold ID/Auto Prep. & Gold ID/Gold Prep.\\
		\midrule
		External Word2vec embd. dimension & 300 & 300 & 300 & 300 \\
		Token internal embd. dimension & 50 & 100 & 10 & 10 \\
		Update token Word2vec embd.?  & No & No & No & No \\
		Update lemma Word2vec embd.?  & Yes & Yes & Yes & No \\
		MLP layer dimension  & 80 & 80 & 100 & 100 \\
		MLP activation  & tanh & tanh & relu & relu \\
		BiLSTM hidden layer dimension  & 80 & 100 & 100 & 100 \\
		MLP Dropout Prob.  & 0.32 & 0.31 & 0.37 & 0.42 \\
		LSTM Dropout Prob.  & 0.45 & 0.24 & 0.38 & 0.49 \\
		Learning rate  & 0.15 & 0.15 & 0.15 & 0.15 \\
		Learning rate decay  & 0 & 0 & 10^{-4} & 0 \\
		POS embd. dimension  & 5 & 25 & 25 & 5 \\
		UD dependencies embd. dimension  & 5 & 25 & 10 & 25 \\
		NER  embd. dimension  & 5 & 5 & 10 & 5 \\
		GOVOBJ-CONFIG embd. dimension  & 3 & 3 & 3 & 3 \\
		LEXCAT embd. dimension  & 3 & 3 & 3 & 3 \\
		
		\bottomrule
	\end{tabular}
	\caption{\label{tab:hyperparams}
		Selected hyperparameters of the neural system for each of the four settings. With the exception of the external Word2vec embeddings dimension (which is fixed), the parameters were tuned using random grid search on the development set.
	}
	
\end{table*}

\end{document}
