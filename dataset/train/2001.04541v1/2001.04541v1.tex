\begin{figure}[t]
    \centering
    \resizebox{\linewidth}{!}{
        \includegraphics[width=0.3\textwidth]{files/Concept_fig.pdf}
    }
    \caption{Conceptual diagram of our approach for visual storytelling. The key difference from a typical \textsc{seq2seq} model is the component of predicting anchor word embeddings from the images. The predictions are then fused with the image features as the inputs for generating desired narrative sentences.}
    \label{fig:overall}
\end{figure}

\begin{table}[t]
    \small
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{l c c c c}
            \toprule
            & B@4 & M & R & C  \\
            \midrule
            Image Only &  13.9 \tiny $\pm 0.4$ & 35.2 \tiny $\pm 0.1$ & 29.5 \tiny $\pm 0.3$ & 8.4 \tiny $\pm 0.9$\\
            \midrule
            \textbf{Anchoring}\\
            \  with {\bf Noun} & \textbf{17.2\tiny $\pm 0.2$} & \textbf{39.0\tiny $\pm 0.2$} & 33.8\tiny $\pm 0.1$ & \textbf{15.7\tiny $\pm 0.4$} \\
            \ with {\bf   Verb} & 16.5\tiny $\pm 0.1$ & 37.9\tiny $\pm 0.2$ & \textbf{34.7\tiny $\pm 0.2$} & 13.0\tiny $\pm 0.2$ \\
            \ with {\bf  Adj.} & 15.2\tiny $\pm 0.3$ & 36.6\tiny $\pm 0.1$ & 31.9\tiny $\pm 0.2$ & 11.3\tiny $\pm 0.2$ \\
            \ with {\bf   Adv.} & 14.9\tiny $\pm 0.3$ & 35.9\tiny $\pm 0.1$ & 31.0\tiny $\pm 0.2$ & 10.4\tiny $\pm 0.3$ \\
            \bottomrule
        \end{tabular}
    }
    \caption{Adding ground-truth words as anchor words to a \textsc{seq2seq} model significantly improves its performance where only image features are used. The higher numerical value indicates better performance.}
    \vspace{-0.2in}
    \label{tab:gt_anchor}
\end{table}




\section{Approach}
\label{sApproach}

The task of visual storytelling is to generate a sequence of narrative sentences $\{ \, \mathbf{S}_i, \, i=1, 2, \ldots, {N}\}$, one for each of the $N$ images $\{ \, \mathbf{I}_i, \, i=1, 2, \ldots, {N}\}$. The order of the images is important and is fixed. Each of the generated sentences $\mathbf{S}_i$ could contain a variable length of words.

The main idea behind our approach is straightforward. For each image, we learn and apply a model to predict its anchor word embedding. The predicted embedding is then concatenated with the image feature. The combined feature is fed into a \textsc{seq2seq}~\cite{sutskever2014sequence} where the narrative sentence is generated as output. Fig.~\ref{fig:overall} illustrates the model design.

The key challenge is to learn the anchor word prediction model when the dataset does not provide anchor words explicitly. We begin by describing how we overcome this challenge. Then we introduce our model in detail.

\subsection{What is an anchor word?}

We are inspired by the comparison between narrative stories and captions on the same sequence of images, shown in Table~\ref{tab:Diff_VIST_stats}.  In particular, a large number of words used in narration do not appear in captions. Intuitively speaking, they are less likely visually grounded.

Thus, we conjecture that possible candidates for anchor words are the words in the narrative sentences. The analysis in Table~\ref{tab:gt_anchor} confirms the usefulness of this hypothesis.

Specifically,  we train a model as in Fig.~\ref{fig:overall} with two variants. In the first variant, we supply only the image features. The results are reported in the row labeled as ``Image Only''.   In the second variant (``Anchoring''), we select all the noun (alternatively, verb, adjective, or adverb) words as anchor words -- one word per sentence in the story. We then train the \textsc{seq2seq} model by combining the image feature and the word embedding end to end.  The results are reported in rows labeled with the part-of-speech  (POS) tags of the selected words. For simplicity, all anchor words have the same POS tags. If there are multiple words with the same POS tags in each sentence, we randomly select one.

There are two points worth making. First, adding anchor words, irrespective of their types, significantly improves the performance of the \textsc{seq2seq} model with image features only. Note that the results in ``Image Only'' is on par with state-of-the-art results~\cite{wang2018no}. Secondly, among all POS tags, nouns as anchor words seem to be the most beneficial ones on all metrics except R(OUGE) where verbs improve more.

In the rest of this paper, we use nouns in the stories as the anchor words.



\begin{table*}[th]
    \small
    \centering
        \begin{tabular}{l c c c c c c c}
            \toprule
            Method & B@1 & B@2 & B@3 & B@4 & M & R & C  \\
            \midrule
            AREL \cite{wang2018no} & 63.8 & 39.1 & 23.2 & 14.1 & 35.0 & 29.5 & 9.4\\
            Show, Reward and Tell \cite{wang2018show} & 43.4 & 21.4 & 10.4 & 5.2 & - & - & 11.4 \\
            HSRL w/ Joint Training \cite{huang2018hierarchically} & - & - & - & 12.3 & 35.2& 30.8 & 10.7 \\
            \textsc{seq2seq}+Heuristics~\cite{huang2016visual} & - & - & - & - & 31.4 & - & -\\
            H-Attn-Rank\cite{yu2017hierarchically} & - & - & 20.8 & - & 33.9 & 29.8 & 7.4 \\
            \textbf{StoryAnchor}: Image Only & 62.2 \tiny $\pm2.5$ & 38.3 \tiny $\pm1.7$ & 22.7 \tiny $\pm0.8$ & 13.9 \tiny $\pm 0.4$ & 35.2 \tiny $\pm 0.1$ & 29.5 \tiny $\pm 0.3$ & 8.4 \tiny $\pm 0.9$ \\
            \textbf{StoryAnchor}: w/ Predicted Nouns & 65.1 \tiny $\pm 0.3$ & 40.0 \tiny $\pm 0.2$ & 23.4 \tiny $\pm 0$ & 14.0 \tiny $\pm 0.1$ & 35.5 \tiny $\pm 0.0$ & 30.0 \tiny $\pm 0.1$ & 9.9 \tiny $\pm 0.1$ \\
            \bottomrule
        \end{tabular}
    \caption{Comparison of  state-of-the-art method for the visual storytelling task on the VIST dataset. Our ``Image Only'' model is a reimplementation of XE+SS \cite{wang2018no} with the authors' public available codes.}
    \label{tab:SOTA}
    \vspace {-15pt}
\end{table*}


\subsection{Model and Learning}
\label{hwpred}

The data for our learning task is augmented with a list of anchor words $\{w_i \mid i=1, 2,\ldots, N\}$ corresponding to the images. Next, we explain how to learn each component.

\vspace{-10pt}

\paragraph{Anchor word embedding predictor} We learn a model $F(\mathbf{I}_i)$ to predict $w_i$. $F(\cdot)$ is parameterized by a  one-hidden-layer multi-layer perception (MLP) with ReLU non-linearity. The input could be the features for the $i$th image or all the images in the same sequence. In practice, there is no significant difference.

To be able to generalize to new anchor words, we predict its embedding and cast learning $F(\cdot)$ as a regression problem. To obtain the target (ie, the ``ground-truth'' embedding) for the word $w_i$, we take the embeddings from the ``Anchoring'' model in Table~\ref{tab:gt_anchor}. $F(\cdot)$ is then optimized to reduce the mean squared error between the predictions and the target anchor words embeddings.

\vspace{-10pt}

\paragraph{Story generation model} Similar to state-of-the-art visual storytelling methods~\cite{wang2018no,huang2016visual}, we use a \textsc{seq2seq} model~\cite{sutskever2014sequence} as story generator. Concretely, a bidirectional gated recurrent neural network~\cite{chung2014empirical}(GRU) is used to encode the concatenated feature of the image and the predicted anchor word embedding and to produce a sequence of hidden states
    \vspace{-5pt}
\begin{equation}
\vv_i = \text{BiGRU}(\mathbf{I}_i, F(\mathbf{I}_i))
    \vspace{-5pt}
\end{equation}
The sequence of the hidden states is then decoded by a one-layer GRU.  Both the encoder and the decoder are trained to maximize the likelihood of ground-truth stories.

\subsection{Other Implementation Details}

\paragraph{Visual and textual representation} We extracted the 2048 dimension feature from the penultimate layer of ResNet-152~\cite{he2016deep} as visual representations. The 512-dimensional word embedding is randomly initialized, which are fine-tuned in the training. Note that the anchor words are sharing the word embeddings with the words in the vocabulary.

\vspace{-10pt}

\paragraph{Model details} The concatenated features of the image and the anchor word embedding are projected into a 2048 dimensional feature with a one-hidden-layer MLP. Then, a one-layer BiGRU with 256-dimensional hidden states generates contextual embedding $\vv_i$ of 512 dimensions, to serve as hidden states representation. A standard \textsc{seq2seq} decoder with one-layer GRU with 512 hidden dimensions is used on top of these hidden states to generate a story.

\vspace{-10pt}

\paragraph{Optimization} As mentioned, the model is trained in two stages. In the first stage, ground-truth anchor words (nouns in the stories) are used to train the encoder-decoder as well as the embeddings end to end. The model is trained with mini-batches and ADAM for 100 epochs. Each mini-batch contains 64 sampled stories. The learning rate is initialized as 4e-4 and schedule sampling~\cite{bengio2015scheduled} has been used. The probability of schedule sampling is first set to be 0.05, increased by 0.05 every 5 epochs till 25 epochs. In the second stage, the predictor $F(\cdot)$ is trained. Specifically, we use the model that achieves the highest Meteor score on the validation set in the first stage training as a pre-trained model. We use the same optimization hyper-parameter to train the predictor with encoder-decoder model in an end-to-end way. The encoder-decoder and the word embeddings are kept fixed.


\paragraph{Inference} At the inference time (ie, narrating a sequence of images), we perform beam search for sentence decoding with a beam size of 3.
