

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{color}
\usepackage{xcolor}
\usepackage{hhline}
\usepackage{lineno}
\usepackage{pdfpages}
\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2021}

\icmltitlerunning{Recurrent Rational Networks}

\definecolor{orange}{rgb}{1, 0.5, 0}
\newcommand{\orange}[1]{\textcolor{orange}{#1}} 

\definecolor{blue}{rgb}{0.2, 0.2, 0.9}
\newcommand{\blue}[1]{\textcolor{blue}{#1}} 

\definecolor{green}{rgb}{0.2, 0.7, 0.2}
\newcommand{\green}[1]{\textcolor{green}{#1}} 

\newcommand{\purple}[1]{\textcolor{purple}{#1}} 

\newcommand{\eg}{\emph{e.g.}~} 
\newcommand{\Eg}{\emph{E.g.}~}
\newcommand{\ie}{\emph{i.e.}~} 
\newcommand{\Ie}{\emph{I.e.}~}
\newcommand{\cf}{\emph{cf.}~} 
\newcommand{\Cf}{\emph{Cf.}~}
\newcommand{\etc}{\emph{etc.}~} 
\newcommand{\etal}{\emph{et al.}~}

\begin{document}

\twocolumn[
\icmltitle{Recurrent Rational Networks}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Quentin Delfosse}{to}
\icmlauthor{Patrick Schramowski}{to}
\icmlauthor{Alejandro Molina}{to}
\icmlauthor{Kristian Kersting}{to,goo}
\end{icmlauthorlist}

\icmlaffiliation{to}{AI and Machine Learning Group, CS Department, TU Darmstadt, Germany}
\icmlaffiliation{goo}{Centre for Cognitive Science, TU Darmstadt, Germany}


\icmlcorrespondingauthor{Quentin Delfosse}{quentin.delfosse@cs.tu-darmstadt.de}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
Latest insights from biology show that intelligence does not only emerge from the connections between the neurons, but that individual neurons shoulder more computational responsibility. Current Neural Network architecture design and search are biased on fixed activation functions. Using more advanced learnable activation functions provide Neural Networks with higher learning capacity. However, general guidance for building such networks is still missing.
In this work, we first explain why rationals offer an optimal choice for activation functions. We then show that they are closed under residual connections, and inspired by recurrence for residual networks we derive a self-regularized version of Rationals: Recurrent Rationals.
We demonstrate that (Recurrent) Rational Networks lead to high performance improvements on Image Classification and Deep Reinforcement Learning.
\end{abstract}

\section{Introduction}
Neural Networks' efficiency in approximating any function has made them the most used approximation function for many machine learning tasks. They consist of successive layers, that topologically reshape the input space, transform and extract new features, that are then separately activated. Neuro-Scientists first explained that the brainpower resides in the combinations happening through trillions of connections. However, research has progressively shown that individual neurons are actually more capable than first thought, and latest results have shown that dendritic compartments can compute complex functions, (\eg XOR), previously categorized as unsolvable by single-neuron systems \cite{gidon2020dendritic}.

In current neural network design, most of the research is focusing on well-performing architectural patterns \cite{liu2018progressive, XieZLL19}, while keeping a fixed, predetermined activation function. Nevertheless, many different functions have been adopted across different domains (\eg Leaky ReLU in YOLO \cite{RedmonDGF16}, GELU in GTP-3 \cite{BrownMRSKDNSSAA20}, Tanh in PPO \cite{SchulmanWDRK17}). This indicates that the performance of neural networks jointly depends on the task, the architecture, hyper-parameters, the dataset (or environment), as well as the activation functions.
\begin{figure}[t]
    \centering
    \includegraphics[width=1.\columnwidth]{images/main_paper/rrn.png}
    \caption{Recurrent Rational Network in its folded and unfolded forms. The input passed along the layers and recurrently in the rational function of the network.}
    \label{fig:recratarch}
\end{figure}

To reduce the bias introduced by a fixed activation function and achieve higher expressive power, one can use learnable activation functions. While Neural Architecture Search uses Reinforcement Learning or evolutionary algorithms on a preset family of activation functions to find performing patterns \cite{zoph2016neural, liu2018progressive}, one can directly make use of gradient descent to optimize parametric activation functions. In this way, one can learn a linear combination of an arbitrary family of activation functions \cite{manessi2018learning}, or use polynomial activation functions, where the polynomials' coefficients are considered as weights to be optimized \cite{goyal2019learning}.

A finer approach consists in learning a rational function (i.e. a ratio of polynomials, see Eq~\ref{eq:rf}). Rational functions and polynomials can converge to any continuous function, but rational functions are better approximants than polynomials in terms of convergence \cite{telgarsky2017neural}. Rational activation functions were first introduced as Pad√© Activation Units \cite{molina2019pad}.

Molina \etal\yrcite{molina2019pad} have shown that Rational Units outperform other activation functions on several supervised learning tasks. However, neural networks embedding rationals tend to overfit and ways to regularize the rationals are required.

In this paper, we uncover important properties of rationals that explain their performance. We show that they can learn various features, carried by different handcrafted activation functions \cite{clevert2016fast,elfwing2018sigmoid,RamachandranSearching,Misra20,georgescu2020non}. 
In addition, we demonstrate that, if properly constructed, they can express similar behavior to residual blocks, as they are closed under residual connection. Therefore, we use rational functions to replace residual blocks in a residual neural network and show that the performance is maintained or even increased, while the number of parameters decreases. Furthermore, inspired by weight sharing for Residual Networks, we introduce Recurrent Rational Networks, whose recurrence acts as a natural regularizer. We evaluate Rational and Recurrent Rational both on supervised learning tasks and Reinforcement Learning. Our empirical evidence\footnote{ GPU hours, carried on DGX-2 Machine with Nvidia Tesla V100with 32GB.} demonstrates that (recurrent) rational functions dominate domain. We used five different seeds for networks and (potentially environment) initialization. For all the details and hyper parameters of our experiments, refer to the appendix.

Our source code\footnote{\url{github.com/ml-research/rational_sl}\\ and \url{github.com/ml-research/rational_rl}} to reproduce our experimental evaluation and our Rational Network library\footnote{\url{github.com/ml-research/rational_activations}}  
are publicly available. 

\textbf{Contributions.}
We make the following contributions: 
\begin{itemize}
\item We investigate the performance gains and analyze important properties of Rational Units, \eg the natural embedding of residual connections. 
\item Moreover, we introduce Recurrent Rational Networks and showcase their advantages over Rational Networks on supervised image classification tasks.
\item We demonstrate that Rational Networks bring massive improvements in Reinforcement Learning and even further increase the performances with our introduced Recurrent Rational Networks.
\item We provide a Rational Activation Functions library to create (Recurrent) Rational Networks compatible with most popular machine learning frameworks PyTorch, Keras, Tensorflow and MXNet.
\end{itemize}

We proceed as follows. We start by describing the role of activation in Neural Networks. Then, we analyze the properties of rational activation functions, notably showing that they can incorporate residual blocks. Inspired by recurrence for residual networks, we introduce Recurrent Rational Networks. We demonstrate that these self-regularized networks improve generalization performances on supervised learning classification. We then evaluate on Reinforcement Learning tasks, and show Rational Networks outperforms every non-learnable baselines and our Recurrent version dominate. 
Before concluding, we discuss best practices and outlooks for these emerging Rational Network types.


\section{On the Role of Activation Functions}
Let us start by investigating the role of activation functions in (deep) neural networks.

\textbf{Activation Functions Perform Internal Space Transformations.}
Contrary to the rest of the layer, activation functions separately act on every dimension of the input, \ie they perform elementwise operations. They can be compared to the brains' dendrites that perform scalar transformations on the signal they propagate, further merged into the neuron body \cite{li2020power}. The transformation operated by the activation function cannot extract new feature spaces by recombining the inputs, as convolutions do, but only stretch, compress and cut the space. We refer to these transformations as internal transformations, and to the ones performed by layers (without their activation functions) as external ones. 
Different types of internal transformations exist: Naitzat \etal\yrcite{naitzat2020topology} showed that ReLU and its variants not only help to tackle the vanishing gradient problem but allow non-homeomorphic topological transformations. In contrast, Sigmoid or Tanh can only perform homeomorphic ones. In other terms, ReLU can fold its input space, sending the whole negative to one point, whereas Sigmoid and Tanh continuously squish it.

\textbf{Diversity Inside the Network.}
Dendrites do not share identical behavior inside the biological brain \cite{tavosanis2012dendritic}. However, most common neural network architectures use a fixed function across the network during learning. Some architectures use a cell that contains different activation functions, as LSTMs \cite{GreffSKSS15}, or in Networks discovered by Neural Architecture Search \cite{liu2018progressive}. Providing each layer with the ability to refine the activation function for its specific need leads to higher modeling capacity. In Rational Networks, we thus place different rational functions at each layer, as done in \cite{molina2019pad}.

\textbf{Rationals Embed Desired Properties.}
Researchers have often searched for a golden activation function. A combination of exhaustive and reinforcement learning-based search led to SiLU/Swish \cite{RamachandranSearching}, later refined with Mish by Misra \etal\yrcite{Misra20}, who showed common properties of well-performing functions such as bounded negative domains and non-monotonic shapes. This properties are learned by some of our trained Rationals (\textit{cf.} Fig.~\ref{fig:rat_nice_props} \textit{right}).
Inspired by the biological results described above, ADA, particularly suited for performing XOR operation, was recently introduced \cite{georgescu2020non}. This complex operation is feasible because of the spike (high non-monocity) of ADA. We, here again, observe this property learned by a trained Rational Unit. (\textit{cf.} Fig.~\ref{fig:rat_nice_props} \textit{left}).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{images/main_paper/rat_nice_properties.pdf}
    \caption{Rational Units (Rat.) discover biologically relevant operations automatically. Rational Unit from trained DQN Agents on Time Pilot compared to ADA \cite{georgescu2020non}, which like biological neurons can perform XOR, and Recurrent one on Tuthankham to dSiLU, with bounded negative domains and non-monotonic shape.
    \label{fig:rat_nice_props}}
\end{figure}

\section{Rationals to Replace Residual Blocks}
Veit \etal\yrcite{VeitWB16} have exhibited Residual 
Networks (ResNet) ability to tackle the Vanishing Gradient Problem. 
However, they were first introduced following the idea that, for very deep networks, it is easier to start learning from an identity layer and deviate from it, rather than trying to converge to it if necessary \cite{HeZRS16}.
ResNets propagate an input  through two paths: a transforming block of layers that preserve dimensionality () and a residual connection (identity). The output  of such a block satisfies Eq~\ref{eq:resnet_eq}.

Rational functions can also follow this precept. They can be initialized as the identity function, by having  and  for all  in Eq~\ref{eq:rf}. 

\textbf{Rationals are Closed Under Residual Connections.}
Moreover, adding a residual connection to a Rational Unit  gives:

where: 

This shows that rationals (with ) embed residual connections, and as rationals are also closed under compositions, successive (residual) rationals can be embedded into a single rational function. Hence, in order to provide Rational Units with residual connections,
one should select a numerator with a higher order than the denominator.

\textbf{ResNets Need Internal Space Transformations.}
However, ResNet blocks differ from rationals as they act on multi-dimensional input space. Whereas the rational acts on every dimension separately, they perform internal transformations. 

For very deep ResNet, Greff \etal\yrcite{greff2016highway} showed that Residual blocks are unrolled iterative estimation that refines upon their input representation. They show that feature recombination does not occur inside the blocks, which thus perform only internal transformations. These transitions to new levels of representations (external transformations) occur during dimensionality change. This view explains why lesioning and shuffling do not affect ResNet performances significantly, and is also compatible with the findings obtained by Molina \etal\yrcite{molina2019pad}, in pruned ResNets. We hypothesize that the Residual Networks mentioned in these papers could benefit more from internal transformations than from the external ones that these blocks can provide.

To test this hypothesis, we conduct lesioning experiments on a pretrained ResNet101, on which we either remove the residual block (as in \cite{VeitWB16}), or we replace it with a Rational units. 
We finetune the surrounding blocks and in our case the identity-initialized rational. Using Rational Units lead to performance improvements, even above the original model (\cf Tab.~\ref{tab:surgery_comp}). We are here mainly interested in the train loss, as it shows how effectively the model can learn. Nevertheless, the test accuracy is also provided.

\begin{table}[t]
\centering
{\def\arraystretch{1.}\tabcolsep=3.3pt
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Eval                     & Lesion   & L2.B3              & L3.B13 & L3.B19 & L4.B2 \\ \hline \hline
\multirow{2}{*}{train}  & Std. (Veit \textit{et al.})   & 100.9    & 120.2     & 90.5      & 58.9     \\ \cline{2-6} 
                        & Rat. (our)        & \textbf{101.1 }   & \textbf{120.3}     & \textbf{104.0}     & \textbf{91.1}     \\ \hline \hline
\multirow{2}{*}{test}   & Std. (Veit \textit{et al.})   & \textbf{93.1}     & 102.0     & 97.1      & 81.7     \\ \cline{2-6} 
                        & Rat. (our)        & 90.5     & \textbf{102.6}     & \textbf{97.6}      & \textbf{85.3}    \\ \hline\hline
\multicolumn{2}{|c|}{\% dropped params}     & 0.63     & 2.51     & 2.51      & 10.0     \\ \hline
                         
\end{tabular}
}
\caption{Rational functions improve lesioning. Recovery percentage for retrained networks after lesioning \cite{VeitWB16} of a ResNet layer's (L) block (B). Residual blocks were lesioned, \ie replaced, with a Identity (Std.) or a Rational (Rat.) from a pretrained ResNet101 (44M parameters). Then, the surrounding blocks (and implanted Rational Activation) are retrained for 15 epochs. The best results are highlighted \textbf{bold}.}
   \label{tab:surgery_comp}
\end{table}

These findings empirically show that Residual Networks (ResNets) with rational activation functions are less affected when compressing the neural network, i.e. that internal transformations performed by rationals compensate for the dropped neural connections.

Nonetheless, retraining with rationals on pretrained network, whose architecture and weights have been optimized along with identity blocks might have led the architecture to a local optimum. Using rationals from the beginning might lead to a different optimum, and would require more investigations. 

Additionally, Lu \etal\yrcite{luZLD18} have bridged the gap between Numerical Differential Equations and Residual Networks. Binding together Rational Units with the latter thus implicitly connect rational functions to Linear Sytems.



\begin{table}[t]
\centering
{\def\arraystretch{1.}\tabcolsep=5.2pt
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{2}{|c|}{Network}  & LReLU          & RN                    & RRN                   \\ \hline
\multirow{2}{*}{VGG8} & \multicolumn{1}{|c|}{Train}     &    &    &       \\ \cline{2-5}
& \multicolumn{1}{|c|}{Test}      &    &            &    \\ \hline
\multirow{2}{*}{VGG11} & \multicolumn{1}{|c|}{Train}     &    &    &    \\ \cline{2-5}
& \multicolumn{1}{|c|}{Test}      &    &       &    \\ \hline
\end{tabular}
}
\caption{Recurrence helps the network to regularize and thus improves the performances on the test set. Train and test accuracies (with std.~dev.) for different architectures trained with Leaky ReLU (baseline), Rational and Recurrent Rational Networks. The best results are highlighted \textbf{bold} and results outperforming the LReLU baseline with  markers.\label{tab:cifars_all_archs}}
\end{table}



\section{Recurrent Rational Networks}
In the previous section, we bridged the gap between rationals and Residual Networks. Continuing along these lines, Greff \etal\yrcite{greff2016highway} further indicate that sharing the weights in blocks can improve learning performances, as shown for Highway \cite{lu2016small} and Residual \cite{liao2016bridging} Networks. 

\textbf{Recurrent Rational Networks.}
Liao \etal\yrcite{liao2016bridging} link Residual Learning, Recurrent Neural Networks and Visual Cortex. They first show that very deep ResNets approximate shallow Recurrent Neural Networks with weight sharing among the layer. Inspired by this findings, we introduce Recurrent Rational Networks (RRN), a novel type a Recurrent Network where weights are shared between rationals. In this type of Networks, the input is propagated through different layers, but always passed in the same rational activation function.

Moreover, Molina \etal\yrcite{molina2019pad} showed that rational tend to overfit and suggested to further investigate regularization for Rationals. We advocate that recurrence can act as a natural regularizer for rationals. We test this hypothesis by comparing Rational Networks and Recurrent Rational Networks to a baseline in Tab.~\ref{tab:cifars_all_archs} and later on Reinforcement Learning Tasks (\cf Section~\ref{sec:reinforcementlearning}). Our empirical results also indicate that recurrence helps regularize on CIFAR100.


\textbf{Localized Recurrence.}
Liao \etal\yrcite{liao2016bridging} also investigate where weight sharing could be the most beneficial for the network. They show that for visual tasks, recurrent computations are more beneficial in early visual areas than at middle and later stages, and as explained in their conclusion, these findings later found psychophysics support \cite{EberhardtCS16}. We thus evaluate networks different recurrence localizations, and report the results in Tab.~\ref{tab:cifars_lenet_mixed} (and Tab.~\ref{tab:cifar100_vgg_mixed} in the appendix). Our findings partially confirm these hypotheses, as the best performing networks are the one sharing weights in the two early and the two last layers. Localized recurrence is thus a good balance between modeling power of the network and regularization.


\begin{table}[t]
\centering
{\def\arraystretch{1.}\tabcolsep=5.5pt
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\multicolumn{2}{|c|}{Location}    & \multicolumn{1}{c|}{All}     & \multicolumn{1}{c|}{Early}    & \multicolumn{1}{c|}{Middle}     & \multicolumn{1}{c|}{Late}     \\ \hline
\multirow{2}{*}{\textit{10}}  & Train  & 74.3.4 & \textbf{76.6.6} & 75.1.5  & 75.91. \\ \cline{2-6}
                & Test   & 74.9.6 & \textbf{76.7.5} & 74.9.4  & 75.7.0  \\ \hline
\multirow{2}{*}{\textit{100}} & Train  & 40.71. & 43.5.8 & 43.71. & \textbf{44.4.0}  \\ \cline{2-6}
                & Test   & 40.4.3 & \textbf{42.5.4} & 42.3.0  & 42.1.9 \\ \hline
\end{tabular}
}
\caption{Recurrence in early stages leads to the best performances. Mean accuracies with std. dev. are provided. The evaluation is conducted on a simple Convolutional Neural Network on CIFAR10 (\textit{10}) and CIFAR100 (\textit{100}) for Recurrent Rational Networks and networks where recurrence is shared for two successive layers. For local recurrent networks, best train accuracy (for \textit{10}) and test accuracies are obtained when weights are shared at the first two layers, best train accuracy (for \textit{100}) for recurrence in the last layers. The best results are highlighted \textbf{bold}.}
   \label{tab:cifars_lenet_mixed}
\end{table}

For bigger networks, testing every possible localized recurrence combination is resource intensive, as the number of network architectures to test grows exponentially ( different networks for  activation functions). Therefore, having a way to automatically detect which layer need weight sharing would considerably reduce computations. In Fig.~\ref{fig:lenet_cifar10}, we plot the shapes of the learned activation functions and note that the first two functions have similar profiles as well as the last two functions. However, we here want to quantify objectively the similarity, or the distance, between profiles of functions.

\begin{figure}[t]
    \centering
    \includegraphics[width=.98\columnwidth]{images/main_paper/lenet_rn_cifar10_with_dist.pdf}
    \caption{Learned rational functions confirm biological intuition that weight sharing in early layers is beneficial. Shown are the profiles of rational functions learned at each layer (increasing layers from left to right) of a simple convolutional network on CIFAR10. As one can see, similar profiles are learned in the early resp. late layers. The input distributions, as well as the \textit{neural distance} between each function are reported on the graph, and confirms the intuition.}
    \label{fig:lenet_cifar10}
\end{figure}

\textbf{Neurally equivalent activation functions.}
We first concentrate on equivalence between neural networks, to derive properties of a suited metric.
Research on neural network equivalence is usually interested in changing the shape of the neural network, especially reducing its amount of parameters \cite{al2010equivalence, kumar2019equivalent}. Two networks are defined to be equivalent if they produce the same output for any given input. 
In this work, we focus on equivalence between neural networks that differ only in their activation functions. The neural networks we consider are equipped with linear layers such as convolutional or fully connected ones. To provide some intuition, we first point out that such neural networks, at any given layer, dividing the activation function by a constant and multiplying the weights of the next layer by this constant produces an equivalent neural network. Any learned activation function is thus scale independent (for vertical scaling). Such transformations can be applied to the bias with addition on the activation function. Formally, we consider  and  two functions such that:

we derive:

Thus, replacing  by  at a given layer of a neural network and changing the weights and bias of this layer or the next one produces an equivalent neural network. 

\textbf{Equivalence between activation is scale and shift independent.} Considering this, we introduce a new metric between two functions  and  in Equation  \ref{eq:neural_dist}, that is shift and scale ---both vertical and horizontal--- independent.


With this definition,  is actually a quasi-pseudo-metric, as it does not respect the symmetry and the positive definiteness. We can easily turn it into a pseudo-metric by taking the minimum between ) and  as the new metric, thus solving the symmetry. However, the positive definite property (i.e. ) is not verified on purpose, as this is used as neural equivalence property. We provide the distance measured between the rational functions learned at each layer in the Fig.~\ref{fig:lenet_cifar10}. The two smallest distances correspond to the pairs of functions with similar profiles, and confirm that merging those functions is an optimal choice, as it is a good balance between regularization and modeling capacity.

\section{Boosting DQN using Recurrent RNs}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{images/main_paper/score_bar_plot_h.pdf}
    \caption{Rational and Recurrent Rational Networks lead to superhuman performance. DQN agents equipped with the Leaky ReLU baseline (LReLU), Rational Networks (RN) and Recurrent Rational Networks (RRN). The asterisk (*) is highlighting games for which RN/RRN performs better than human baseline, whereas baseline does not (). \label{fig:scores_bar_plot}}
\end{figure}
\label{sec:reinforcementlearning}
As mentioned in the introduction, different activation functions are often used for different tasks. Earlier work has shown that Rationals are suited for supervised learning classification \cite{molina2019pad} and image generation \cite{BoulleNT20}. In this section, we would like to extend the use of Rational Networks to Deep Reinforcement Learning.

Deep Reinforcement Learning requires additional constraints on the network, as the policy influences the input and output distributions, and it has often be shown that unstable policies could lead to a drop in performance \cite{SchulmanLAJM15, SchulmanWDRK17}. Therefore, plugging rational networks in a Reinforcement Learning agent might lead to instabilities. In this section, we demonstrate that rational networks indeed enhance the performance of the original DQN agent. We also evaluate the self-regularized Recurrent Rational version, that helps to prevent potential instabilities introduced by the higher modeling capacity of Rational Networks. Here the scores are normalized according to Eq~\ref{eq:score_formulae}.


\begin{table}[t]
\centering
{\def\arraystretch{1.15}\tabcolsep=4.5pt
\begin{tabular}{|l|l|l|l|l|}
\hline
\multirow{2}{*}{\textbf{Game}} & \multicolumn{2}{c|}{\textbf{Learnable}} & \multicolumn{2}{c|}{\textbf{Fixed}}   \\ \cline{2-5} 
& \multicolumn{1}{c|}{\textbf{RRN}} & \multicolumn{1}{c|}{\textbf{RN}}    & \multicolumn{1}{c|}{\textbf{SiLU}}       & \multicolumn{1}{c|}{\textbf{d+SiLU}} \\ 
\hline
Asterix      &             &     & 27.9      & 116    \\ \hline
Battlezone   &      &            & 200           & 99     \\ \hline
Breakout     &      &            & 16.8          & 2.1    \\ \hline
Enduro       &    &       & 22            & 2.3    \\ \hline
Jamesbond    &     &    & 70.5          & 61.2   \\ \hline
Kangaroo     &     & 911           &    & 118    \\ \hline
Pong         & &  & 95.1          & 103    \\ \hline
Qbert        &    &       & 25.7          & 5      \\ \hline
Seaquest     &     &    & 646           & 32.4   \\ \hline
Skiing       &    &       & 82            & 106    \\ \hline
SpaceInv.    &      &     & 97.6          & 95.3   \\ \hline
Tennis       &     &    & 300           & 900    \\ \hline
Timepilot    &   &      & 129           & 122.4  \\ \hline
Tutankham    &   &  & 195950        & 9750   \\ \hline
VideoPinball &    &       & 12.7          & -0.9   \\ \hline\hline
\textbf{ LReLU}&        &       &              &  \\ \hline
\end{tabular}
}
\caption{Rational Networks leading to dramatic performance improvement. Normalized mean scores (\cf Eq~\ref{eq:score_formulae} in percentage with LReLU as baseline), averaged over  reruns, of DQN agents equipped with Recurrent Rational and Rational Networks, and with SiLU and SiLU + dSiLU. The best results are highlighted \textbf{bold} and runner-up with  markers. The last row summarizes the number of improvements over the Leaky ReLU baseline.}
\label{tab:lrelu_normalized_scores}
\end{table}
\begin{table}[t]
\centering
{\def\arraystretch{1.15}\tabcolsep=4.pt
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{\textbf{Game}}} &  \multicolumn{3}{c|}{ \textbf{DQN} }                        & \textbf{DDQN}  \\ \cline{2-5} 
\multicolumn{1}{|c|}{} & \textbf{LReLU}   & \textbf{RRN}   & \textbf{RN}   & \textbf{LReLU} \\ \hline 
Asterix       &          &     &         & 3723     \\ \hline
Battlezone    &         &       &       & 22775    \\ \hline
Breakout      &          &         &         & 79       \\ \hline
Enduro        &          &       &          & 353      \\ \hline
Jamesbond     &           &        &        & 45       \\ \hline
Kangaroo      &          &        &        & 484      \\ \hline
Pong          &           &       &       & 12       \\ \hline
Qbert         &         &     &         & 8954     \\ \hline
Seaquest      &          &        &        & 898      \\ \hline
Skiing        &       &    &        & -26892   \\ \hline
SpaceInv.     &          &        &         & 490      \\ \hline
Tennis        &        &        &        & -18.4    \\ \hline
Timepilot     &         &     &         & 1016     \\ \hline
Tutankham     &          &         &         & 36.4     \\ \hline
VideoPinball  &        &     &        & 62151    \\ \hline \hline
\textbf{ Wins}& 0/15  &           &           & 0/15         \\ \hline
\end{tabular}
}
\caption{DQN equipped with Recurrent Rational and Rational Networks always outperform the more complex DDQN algorithm, equipped with the baseline. Mean raw scores (averaged over  reruns) are provided. DDQN agents are equipped with Leaky ReLU (LReLU). The DQN baseline is also provided. The best results are highlighted \textbf{bold} and runner-up with  markers. The last row summarizes the total wins. \label{tab:ddqn_comp_scores}}
\end{table}

\textbf{Rational Networks in DQN.}
We evaluate Rational Networks and Recurrent Rational Networks on Deep Reinforcement Learning regression tasks, using the original DQN architecture \cite{mnih2015human} on 15 different games of the Atari 2600 domain. 
Our first experiments show that DQN agents equipped with Rational Networks always outperform their baseline. Importantly, Recurrent Rational Networks further outperform Rational Networks on most (9 out of 15) games.
Moreover, on 67\% of the games where the DQN baseline does not, DQN agents equipped with RN and RRN surpasses human experts (\textit{cf.} Fig.~ \ref{fig:scores_bar_plot}).

\textbf{Comparison to Swish/SiLU.}
The SiLU activation function () and its derivative dSiLU were introduced by Elfwing \etal\yrcite{elfwing2018sigmoid}. The authors showed that using SiLU or a combination of SiLU (on convolutional layers) and its derivative dSiLU (on fully connected layers) in DQN agents perform better on several games than ReLU networks. Swish, a refined version of SiLU was later discovered via Reinforcement Learning based automatic search in \cite{ramachandran2017searching} (as , with  constant or as trainable parameter). 

SiLU is ---to our knowledge--- the only activation function specifically designed for Reinforcement Learning task. We train DQN agents equipped with SiLU and with the combination (d+SiLU) proposed by Elfwing \etal on the same set of games. Results are summarized in Tab.~ \ref{tab:lrelu_normalized_scores}. Agents with SiLU were only able to outperform Rational Networks on one game out of 15 (Kangaroo), whereas Recurrent Rational Networks achieve better performance than every non-learnable activation function for every game, doubling the score of the runner up, which is the Rational Network, on Space Invaders. 

\textbf{Rational Flexibility to Solve Overestimation.}
A known problem of the DQN algorithm is overestimation. Overestimation is due to the combination Bootstrapping, Off-policy learning and a function approximator (Neural Network) operated by DQN. To solve this problem, Van \etal\yrcite{van2016deep} introduced DDQN, that uses an addition network to separate action selection and action evaluation. 

Q-learning overestimation was initially investigated in \cite{thrun1993issues} and notably attributed to insufficiently flexibility in the function approximation, that lacks accuracy. Learnable rational functions provide additional flexibility and accuracy while adding a minimal number of parameters to the network, that might help tackle the overestimation problem. To verify this hypothesis, we compare the performance of our DQN agent equipped with (Recurrent) Rational Networks to a standard DDQN agent in Tab.~\ref{tab:ddqn_comp_scores}.
Our result show that for every tested game, indeed both Rational and Recurrent Rational DQN agents outperform the DDQNs equipped with the baseline activation.

\textbf{DDQN only delays performance drops.}
As our (Recurrent) Rational DQN agents continue learning for longer than  epochs on several games (\cf Asterix on Fig.~\ref{fig:Agents_sample_final}, we let our agent train for  epochs. This shows that for several games (Breakout, JamesBond, Kangaroo, Seaquest, TimePilot), DDQNs do not solve performance drops, but only delay them, sometimes lowering its performance below DQNs.
A complete graph, with the performance evolution of every tested agent on every game is provided in the appendix.

\begin{figure}[t]
\centering
\includegraphics[width=1.\columnwidth]{images/main_paper/main_games_scores.pdf} \caption{DDQN delays performance drop instead of solving it. Evolution of the scores of DQN agents with Rational Networks (RN) and Recurrent RN (RRN) and DQN and DDQN with LReLU. The line corresponds to the mean score of every agent and the shaded area to the std. dev. of five executed runs. A figure including the evolution on every game is provided in the appendix.}
\label{fig:Agents_sample_final}
\end{figure}


\section{Rationals: Beyond Existing Activations}
Accross our rational journey, we found several best practices that we discuss in this section.

\textbf{Importance of Tracking the Input.}
Properties of different new activation functions \cite{georgescu2020non, Misra20} were reported as crucial for neural networks performances. To verify this hypothesis, the input distribution of the function should be taken into consideration, as the network can make use of the different parts of the activation function's domain. This is particularly relevant for highly learnable functions such as rational functions. Therefore, in every activation function graph provided in this work, we also provided the input distribution. An efficient and direct way of tracking the distribution is provided in our rational framework. 

\textbf{Refining the Neural Equivalence Metric.}
Continuing along these lines, we present in Equation \ref{eq:dense_neural_dist} a refined version of our neural distance that takes into account the input distribution .

This new neural distance gives more importance to parts that are more used. One could also use explainability techniques \cite{Haofan2019Score, shao2021aaai} to track the importance across the input of the rational function.

\textbf{Embedding Different Function.}
In Fig.~\ref{fig:act_func_shapes} we provide the learned Rational and Recurrent Rational functions for trained networks on several games. We observe that several learned profiles are similar to the dSiLU one (notably Recurrent Rationals), and that every activation function is non-monotonous (as the one of supervised learning experiments, \cf Fig.~\ref{fig:lenet_cifar10}), experimentally confirming that non-monotonicity helps the learning process \cite{Misra20}.

\begin{figure*}[t]
\centering
\includegraphics[width=0.90\textwidth]{images/main_paper/main_games_af.pdf}
\caption{Learned rational activation functions of Reinforcement Learning Agents automatically reduced their co-domains. Several input distribution has converged to multimodal Gaussians (e.g. Asterix, Space Invaders and Tutankham). (Left) activation functions at the successive layers in Rational Networks. As for supervised learning, several desired properties (as nonmoncity and bounded negative domains are also learned by these parametric functions). (Right) Common activation function of every layer in a Recurrent Rational Network. A Figure with parametric functions for every games is provided in the appendix.}
\label{fig:act_func_shapes}
\end{figure*}

Furthermore, if we look at the inputs of those functions, we see that they follow Gaussian distributions. For some of them (Asterix, Kangaroo, Space Invaders, Tutankham), multimodality appears in the distribution along learning. This lead us to think that the input distribution is split during training, and that the rational learns several functions, one next to the other for these different input groups. If these modes are produced by different neurons of the layer, this means that several neuron groups need different activation functions, and that the network could benefit from having several rational activation functions across one layer. Detecting this need by tracking the input distribution during learning could lead to performance improvements. 

\textbf{The need for a new batch normalization.}
Consequently, using batch normalization prevents this behavior, as at every layer, the input distribution is normalized. Future work might benefit from a batch normalization that would not prevent input distribution split.

\textbf{Rationals' Automatic Scaling.}
Another interesting property of both recurrent rationals and rationals on Reinforcement Learning is that for all of them, the size of the co-domain is lower than one, besides their long domain range. While the supervised learning experiments consist in classifying, DQN requires a precise regression for the Q-value estimation. This enforces the network to accurately estimate the expected return of the current policy. We hypothesize that this automatic vertical scaling mainly observed for reinforcement learning (\cf Fig.~\ref{fig:act_func_shapes} vs Fig.~\ref{fig:lenet_cifar10}) helps the network to accurately predict the Q-Value.

\section{Conclusion}
In this paper, we provided several explanations on the role of activation functions and shed some light on the reasons why rational functions are so efficient notably with their implicit residual connection embedding. Importantly, we introduced Recurrent Rational Networks, whose nature self-regularizing behavior allow to improve generalization performances, solving the biggest flaw of Rational Networks. For the first time, the evaluation of learnable rational activations was brought to Deep Reinforcement Learning, exhibiting drastic performance improvements. Importantly, we show that Recurrent Rational Networks further increase the performance of DQN agents. Alongside releasing an efficient library for building Rational and Recurrent Rational Networks, we discussed several best practices for their use, such as tracking the input distribution and similarity of the functions across a Rational Network for automatic merging.


Our work provides several interesting avenues for future work.
To implement Rational Networks, rational functions have so far been inserted in existing neural networks architectures. To investigate this further, one should incorporate Rational Networks into other architectures, \eg Transformers \cite{VaswaniSPUJGKP17},  and evaluate them on different tasks.
Most interesting is the incorporation of Rational Networks into Neural Architecture Search \cite{liu2018progressive} to discover new efficient Rational Network Architectures.
Finally, rich insight could be gained from the Linear System field \cite{luZLD18}, linked with rationals through residual connections in this work.



\bibliography{references}
\bibliographystyle{icml2021}

\end{document}
