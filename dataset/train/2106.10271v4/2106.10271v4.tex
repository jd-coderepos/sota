\documentclass[lettersize,journal]{IEEEtran}

\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{multirow}
\usepackage{bbm} \usepackage{tabularx}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{subfig}

\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage[T1]{fontenc}

\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\def\R{\mathbb{R}}

\usepackage[breaklinks=true,bookmarks=false,colorlinks=true]{hyperref}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}


\begin{document}

\title{End-to-end Temporal Action Detection with Transformer}

\author{Xiaolong Liu,~Qimeng Wang,~Yao Hu,~Xu Tang,~Shiwei Zhang,~Song Bai,~and~Xiang~Bai,~\IEEEmembership{Senior Member,~IEEE}
\thanks{This work was supported by National Key R\&D Program of China (No. 2018YFB1004600). \textit{(Corresponding author: Xiang Bai.)}}
\thanks{X. Liu (email: brucelio@outlook.com) and Q. Wang are with the School of Electronic Information and Communications, Huazhong University of Science and Technology. X. Bai (email: xbai@hust.edu.cn) is with the School of Artificial Intelligence and Automation, Huazhong University of Science and Technology. 
Y. Hu, X. Tang, and S. Zhang are with Alibaba Group. S. Bai is with ByteDance Inc.
Part of this work was done when X. Liu was an intern at Alibaba Group.}\thanks{This paper has supplementary downloadable material available at http://ieeexplore.ieee.org., provided by the author.}
}

\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\10 \sim 10^210^3 \sim 10^4\times\boldsymbol{X}_V \in \R ^{T_S\times C}T_SCC=256\boldsymbol{X}_E \in \R^{T_S \times C}L_E\boldsymbol{z}_q \in \R ^{C}qt_q \in [0, 1]\boldsymbol{X}\in \R^{T_S\times C}\boldsymbol{h}_m \in \R ^ {T_S \times (C/M)}mm \in \{1, 2, ..., M\}\boldsymbol{X}Ka_{mqk} \in [0, 1]\Delta t_{mqk} \in [0, 1]t_qX((t_q+\Delta t_{mqk})T_S)(t_q+\Delta t_{mqk})T_Sa_{mqk}\Delta t_{mqk}\boldsymbol{z}_q\sum_{k=1}^K a_{mqk}=1\boldsymbol{W}^{V}_m \in \R ^{C\times (C/M)}\boldsymbol{W}^{O} \in \R ^{C\times C}\tau\tauC_F=2048C=256\boldsymbol{X}_E N_q\hat{\boldsymbol{ z}}^{(0)}=\{\hat{\boldsymbol{ z}}_{i}^{(0)}\}_{i=1}^{N_q}N_q\hat{Y}=\{\hat{y}_i\}L_Dlz^{(l)}\boldsymbol{X}_E\hat{\boldsymbol{z}}_{i} \in \R^{C}TDA(\hat{\boldsymbol{z}}_{i}, \hat{t}_{i}, \boldsymbol{X}_E)\hat{t}_{i}\boldsymbol{X}_Ef_{IR}\hat{\boldsymbol{ z}}_{i}^{(0)}f_{IR}\hat{\boldsymbol{z}}_{i}^{(0)}\hat{\boldsymbol{p}_i}\hat{s}_i=(\hat{t}_i, \hat{d}_i)\hat{y}_i\hat{t}_i\hat{d}_i\hat{s}_i^{(l-1)}=(\hat{t}_i^{(l-1)}, \hat{d}_i^{(l-1)})(l-1)l(\Delta \hat{t}_i^{(l)}, \Delta \hat{d}_i^{(l)})\hat{s}_i^{(l-1)}\hat{s}_i^{(l)}=(\hat{t}_i^{(l)}, \hat{d}_i^{(l)})\sigma(\cdot)\sigma^{-1}(\cdot)\hat{t}_i^{(0)}\hat{t}_if_{IR}\hat{d}_i^{(l)}\hat{d}_i^{(1)}\hat{t}_i^{(0)}\hat{t}_i^{(l-1)}(l-1)l\boldsymbol{X}_E\hat{s}_i\boldsymbol{X}_E\boldsymbol{X}_{s_i}\in\R^{T_R \times C}s_i\boldsymbol{X}_ET_R\epsilon(\hat{t}_i, \epsilon \hat{d}_i)\hat{g_i}\hat{g_i}g_is_iY=\{y_j\}_{j=1}^{N_q}\varnothing\piy_j\hat{y}_{\pi(j)}\mathcal{C}(y_j, \hat{y}_{\pi(j)})c_js_jy_j\mathcal{L}_{cls}(\boldsymbol{p}_{\pi(j)}, c_j)\mathcal{L}_{seg}(s_j, \hat{s}_{\pi(j)})\mathcal{L}_{L1}L_1\mathcal{L}_{iou}\lambda_{iou}\lambda_{coord}L_{cls}\hat{\pi}\lambda_{act}\hat{y}_i\sqrt{\hat{\boldsymbol{p}}_i(\hat{c}_i) \cdot \hat{g}_i}\hat{c}_i[0.3:0.7:0.1]\{0.5, 0.75, 0.95\}[0.5:0.95:0.05]\alpha_\alphaL_EL_D\lambda_{iou}\lambda_{coord}\lambda_{act}MK\{\Delta p_{mqk}\}_{m=1}^{8} =(k, 0, -k, 0, k, 0, -k, 0)\epsilonT_R2\times10^{-4}^\sharp\dagger\ddagger^\S_{0.3}_{0.4 }_{0.5}_{0.6}_{0.7}^\dagger^\ddagger^\ddagger^\Ss_gs_w|s_g \cap s_w|/|s_g||\cdot|\S*^\S_{0.5}\times\times\times\times\times_{0.5}_{0.75}_{0.95}_{0.5}_{0.75}_{0.95}_{0.5}_{0.75}_{0.95}0\sim0.10.1\sim0.20.2\sim1\timesL_EL_D_{0.5}_{0.75}_{0.95}N_qN_q=40N_q=30N_q=10N_qL_EL_DL_DL_EL_EL_EL_EL_DL_DL_DKK_{0.5}_{0.75}_{0.95}KKK=4MM_{0.5}_{0.75}_{0.95}MMM=8\epsilonT_R\epsilonT_R\epsilonT_R\epsilon=1.5\epsilon=1T_RT_R=16N_qN_q$ is worth studying in future works.


\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{fig/fig10_failure_case.pdf}
\caption{Failure cases. TadTR misses the two short actions that are hard to detect while the 1D CNN-based method MUSES partially detects them.
}
\label{fig:failure_cases}
\end{figure*}


\section{Conclusion}
We propose TadTR, a simple end-to-end method for temporal action detection (TAD) based on Transformer. It views the TAD task as a direct set prediction problem and maps a series of learnable embeddings to action instances in parallel by adaptively extracting temporal context in the video.
It simplifies the pipeline of TAD and removes hand-crafted components such as anchor setting and post-processing. We make three improvements to enhance the Transformer with locality awareness to better adapt to the TAD task.
Extensive experiments validate the remarkable performance and efficiency of TadTR and the effectiveness of different components. TadTR achieves state-of-the-art or competitive performance on HACS Segments, THUMOS14, and ActivityNet-1.3 with lower computation costs. 
We hope that this work could trigger the development of Transformers and efficient models for temporal action detection.
The current implementation of TadTR is based on offline extracted CNN features for a fair comparison with previous methods. In the future, we plan to explore joint learning of the video encoder and TadTR~\cite{Liu_2022_CVPR}, and temporal action detectors purely based on Transformers.   

\appendix
In this supplement, we present several visualization results. Fig.~\ref{fig:smoothing_effect} illustrates the smoothing effect of dense attention. Fig.~\ref{fig:attention_vis} supplements Fig. 7 in the main document and gives more examples to demonstrate temporal deformable attention.


\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{fig/video_test_0000026_window_640_768_sim_and_attn.pdf}
    \caption{Different frames in a video is usually highly similar. The dense attention tends to cast uniform attention to different locations in the input sequence at initialization. Left: The similarity matrix of each pair of snippets in CNN features of a randomly selected video. Middle: The attention weight. Right: The similarity matrix of the output feature of the dense-attention. Best viewed in color.}
    \label{fig:smoothing_effect}
\end{figure*}

\begin{figure*}[!b]
\centering
\subfloat[Futsal]{
\includegraphics[width=\textwidth]{fig/_1eywdZVkpA_enc_dec_weight_18_Futsal.pdf}}

\subfloat[Diving]{
\includegraphics[width=\textwidth]{fig/video_test_0000602_window_512_640_enc_dec_weight_6_Diving.pdf}}
\caption{Visualization of temporal deformable attention. The first row is uniformly sampled video frames. The second row visualizes the attention at two randomly picked reference points in the last encoder layer. The third row visualizes the attention for the predicted action in the last decoder layer. We use different markers to represent sampling points in different heads and separate points from different heads vertically. The color of a point indicates the attention weight. Best viewed in color.}
\label{fig:attention_vis}
\end{figure*}


\bibliographystyle{IEEEtran}
\bibliography{ref}



\vfill
\end{document}
