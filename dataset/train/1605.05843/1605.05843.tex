

\section{Results}
\label{sec:result}
We investigate the performance of SoftNIC and ClickOS in different scenarios in this section. 
The thesis of the evaluation is simple: can these NFV dataplanes achieve line rate, and if not, what are the bottlenecks?
We first look at the baseline scenario with a single software middlebox, running different NFs with varying CPU speed (\cref{sec:frequency}). Based on the results we analyze and identify performance bottlenecks of both dataplanes (\cref{sec:cores}). We then deploy multiple NFs in two scenarios that are commonplace in practice: NF chaining where packets go through the middleboxes sequentially for processing (\cref{sec:result_chain}), and NF colocation where multiple NFs colocate on the same server and work independently (\cref{sec:colocation}).


\subsection{Baseline Performance}
\label{sec:frequency}

\begin{figure*}[!th]
    \vspace{-1mm}
    \captionsetup{justification=centering}
    \centering
    \includegraphics[width=1.0\textwidth]{CPU_freq.pdf}
    \caption{Throughput with different CPU speeds and packet sizes. We show throughput in both Mpps and Gb/s.}
    \label{fig:CPU_freq}
    \vspace{-5mm}
\end{figure*}


We start with just a single VNF. Since software packet processing is CPU-intensive, we want to see if CPU speed is the bottleneck here. In this set of experiments, we vary the CPU speed from the configurable range of 1.2GHz to 2.6GHz for our CPU without Turbo Boost, and investigate the throughput with different packet sizes. We modify the CPU frequency using {\tt cpufreq-set} and {\tt xenpm set-scaling-speed} for SoftNIC and ClickOS, respectively.



Figure~\ref{fig:CPU_freq} demonstrates the performance of L3FWD. We observe the following. 
First, performance increases with CPU speed for small packets (64B--256B), which is expected---a faster CPU can process more instructions and thus more packets. For 64B and 128B packets, performance improvement is commensurate with CPU speed-up between 1.2GHz to 2.4GHz. With 64B packet for instance, at 1.2GHz throughput of SoftNIC and ClickOS is 4.31Mpps and 2.10Mpps, respectively, while at 2.4GHz it roughly doubles at 8.60Mpps and 4.26Mpps, respectively. The improvement is smaller at 2.6GHz.
Second, both NFV dataplanes achieve line rate for packets bigger than 128B, but have problems dealing with smaller packets even at 2.6GHz.
SoftNIC can achieve 10Gbps with 128B packets at 2.4GHz, and ClickOS can process 256B packets at 10Gbps at 2.6GHz. Yet for 64B packets, even at 2.6GHz, neither achieves line rate: SoftNIC tops at 9.34Mpps and ClickOS 5.34Mpps. 
Third, SoftNIC outperforms ClickOS in all cases, especially for small packets. We compare our results with those reported in the ClickOS original paper \cite{MARO14} and confirm they are similar. For example, in Figure~13 of \cite{MARO14}, throughput of L3FWD and Firewall with 64B packets are 4.25Mpps and 5.40Mpps, respectively, and ours are 5.34Mpps and 5.03Mpps, respectively.
Finally, we find that the performance difference between L3FWD and firewall is minimal, as shown in Table~\ref{table:rst_3NF}. We thus only show L3FWD results hereafter for brevity. 

\begin{table}[ht]
\vspace{-1mm}
\centering
\caption{Throughput of different NFs with varying CPU speed. Packet size is 64 bytes. }
\label{table:rst_3NF}
\small
\begin{tabular}{|l|l|l|l|l|}
\hline
          & \multicolumn{2}{l|}{SoftNIC (Mpps)} & \multicolumn{2}{l|}{ClickOS (Mpps)} \\ \hline
CPU     & L3FWD     & Firewall      & L3FWD     & Firewall    \\ \hline
1.2GHz       & 4.31     & 4.45            & 2.10    & 1.98       \\ \hline
1.6GHz      & 5.73    & 5.92           & 2.82     & 2.75       \\ \hline
2.0GHz        & 7.19     & 7.38          & 3.44     & 3.27       \\ \hline
2.4GHz       & 8.60     & 8.87            & 4.26     & 4.24       \\ \hline
2.6GHz      & 9.34     & 9.59           & 5.34     & 5.03       \\ \hline
\end{tabular}
\vspace{-2mm}
\end{table}


\zhixiong{We also use an empirical packet size distribution from Facebook's web server cluster \cite{RZBP15} to see how the software dataplanes perform in a practical environment. The median packet size is 120B, and most packets are less than 256B. We configure {\tt pkt-gen} to sample the trace and generate packets first at 10Gbps, and observe that the average throughput in SoftNIC is 8.662Mpps. However a production network is rarely fully utilized. Facebook reports their median link utilization is 10\%--20\% \cite{RZBP15}. 
This implies that the median packet processing requirement is 0.87Mpps--1.73Mpps. Both SoftNIC and ClickOS are able to provide such capability in lowest CPU frequency of 1.2GHz. }



\revise{These observations have interesting implications to NFV resource allocation. They suggest that there are ample opportunities for the operator to downclock the CPU in order to save energy and electricity cost in the average case. Care has to be taken though, of course, to ensure performance does not suffer when there are sudden bursts of small packets.
This may be a useful resource allocation strategy for operators as well as meaningful research directions to look into for the networking community. Our observations also motivate us to identify performance bottlenecks in both NFV dataplanes for small packets, which we explain next. }







\subsection{Performance Bottlenecks}
\label{sec:cores}



One may argue that the performance deficiency of software dataplanes in small packet regime is acceptable in practice, since small packets may be less common. However, these systems may not be able to achieve line rates in the emerging 40G or 100G networks \cite{JYAV15}, even for large packets. Therefore we believe it is important for us to understand the performance bottleneck and improve performance. 


To identify bottlenecks, we conduct the following analysis. 
For SoftNIC, we observe from using {\tt monitor port} command that about 5Mpps 64B packets are lost in the pipeline between pNIC0 and vNIC0. 
To see if VMs are the bottleneck, we allocate more vCPUs and memory and observe the L3FWD throughput with 64B, 96B, and 128B packets. As shown in Figure~\ref{fig:bottleneck}, however, this results in little improvement. After discussing with SoftNIC authors, we suspect that the bottleneck is the {\tt vhost-net} queue in vNIC0. SoftNIC currently does not support multiple {\tt vhost-net} queues for vNIC, which explains why adding resources to the VM does not help. 
To verify the analysis, we conduct another experiment by adding a round-robin module (RR) and another two vNICs to the L3FWD VM as shown in Figure~\ref{fig:rrtop}. Traffic is evenly split between the two input vNICs. This time throughput reaches line rates for all packet sizes as shown in Figure~\ref{fig:bottleneck}. 

\begin{figure}[th!]
\centering
    \includegraphics[width=0.8\linewidth]{bottleneck}
     \caption{Small packet throughput of SoftNIC with different resource allocations. For example ``SoftNIC-1C-1G-2vNICs'' means we use 1 vCPU, 1GB RAM, and 2vNICs (one for input and one for output) for the VM. For the last setting, we use 2 vCPUs, 1GB RAM, and 4vNICs (two for input and two for output). }
    \label{fig:bottleneck}
\vspace{-3mm}
\end{figure}

\begin{figure}[ht]
\vspace{-2mm}
\centering
    \includegraphics[width=0.8\linewidth]{rr}
    \caption{The pipeline of 2 NFs with a round-robin module in SoftNIC. }
    \label{fig:rrtop}
    \vspace{-2mm}
\end{figure}



For {ClickOS}, we analyze the CPU utilization of the L3FWD instance with the CPU at the highest 2.6GHz. As shown in Table~\ref{cpu}, ClickOS uses 100\% CPU when processing 64B and 128B packets, and larger packets lead to much lower CPU utilization. This implies that more CPU resource may be needed here. However, ClickOS currently does not have SMP support \cite{MARO14}, preventing us from adding more cores to the VM. This also means adding more vNICs does not help without more CPU. Another possible solution is to use multiple VNFs working in parallel. This naturally requires a load balancer (LB) to split the traffic. VALE is a simple L2 switch without any load balancing capability \cite{rizzo2012vale}. Adding a LB VM does not work either since the LB itself becomes the bottleneck.
Therefore we are unable to resolve the bottleneck without modification to ClickOS itself. 



\begin{table}[h]
\centering
\small
\caption{The CPU utilization of L3FWD in ClickOS. CPU is at 2.6GHz. }
\label{cpu}
\begin{tabular}{llllll}
\hline
\multicolumn{1}{|l|}{64B}   & \multicolumn{1}{l|}{128B}  & \multicolumn{1}{l|}{256B}   & \multicolumn{1}{l|}{512B} & \multicolumn{1}{l|}{1024B}  & \multicolumn{1}{l|}{1500B}  \\ \hline
\multicolumn{1}{|l|}{100\%} & \multicolumn{1}{l|}{100\%} & \multicolumn{1}{l|}{70.1\%} & \multicolumn{1}{l|}{63\%} & \multicolumn{1}{l|}{53.6\%} & \multicolumn{1}{l|}{53.8\%} \\ \hline
                            &                            &                             &                           &                             &                          
\end{tabular}
\vspace{-6mm}
\end{table}

To summarize, the results here verify that SoftNIC's bottleneck is the {\tt vhost-net} queue of the vNIC. This can be resolved by sending traffic to two vNICs of one VNF in parallel to fully utilize multiple vCPUs. A complete fix requires SoftNIC to add support for multiple {\tt vhost-net} queues. We have confirmed our analysis with the SoftNIC team already. We also present evidence to suggest that ClickOS should add SMP support that allows it to utilize multiple CPU cores. In any case, we note that it is imperative for the NFV software dataplane architecture to provide horizontal scaling of its performance, in order to better utilize multiple cores and physical NIC queues. We believe this is an interesting open research area as the NICs evolves to 40Gbps and beyond.



\subsection{NF Chaining}
\label{sec:result_chain}

It is common to deploy multiple software middleboxes on the same machine. In this section we look into the NF chaining scenario, where the processing pipeline consists of a chain of different middleboxes. We are interested to see if the performance of an NF chain can match that of just a single NF. 
We compose chains of different lengths: the 1-NF chain uses only a L3FWD; the 2-NF chain uses a firewall followed by a L3FWD; and the 3-NF chain adds a L2FWD to the end of the 2-NF chain. 

\begin{figure}[ht]
\centering
    \vspace{-0.5mm}
    \includegraphics[width=0.9\linewidth]{rst_chain}
    \caption{Throughput with varying length of NF chain.}
    \label{fig:rst_chain}
    \vspace{-2mm}
\end{figure}

The results are shown in Figure~\ref{fig:rst_chain} with all vCPUs running on the same physical CPU. The performance of SoftNIC suffers mild degradation for 64B--128B packets, as can be seen from the overlapping lines of 2-NF chain and 3-NF chain. We suspect there is a bottleneck in chaining the vNICs of different VMs because both firewall and L3FWD can achieve higher performance individually as shown in \cref{sec:frequency}.  
Performance of ClickOS also degrades as the chain grows, especially for small packets. 
A ClickOS L3FWD achieves line rate with 256B packets, but a 2-NF chain or 3-NF chain cannot. A 3-NF chain cannot even reach line rate with 512B packets. 
Note that here we use multiple VALE switches with independent vCPUs pinning to different cores to chain the VMs as suggested by \cite{MARO14}. We believe the overhead of copying packets in VALE attributes to the performance penalty.

When deploying a NF chain, an important factor we must consider is the affinity of vCPUs and the effect of NUMA. 
As an example Figure~\ref{fig:top_chain} depicts two possibilities of vCPU settings with a 3-NF chain. We can pin each vCPU to the same physical CPU, or pin them to CPUs in different sockets. The latter is unavoidable sometimes as the commodity CPUs have limited cores per CPU, and DPDK-based NFV dataplanes like SoftNIC require many dedicated cores as mentioned in \cref{sec:softnic} and \cref{sec:software}.
\begin{figure}[ht]
\vspace{-2mm}
    \centering
    \includegraphics[width=0.9\linewidth]{chain}
    \caption{Pipeline of NF-Chains}
\vspace{-4mm}
    \label{fig:top_chain}
\end{figure}


We perform another measurement to evaluate the effect of NUMA on NF chaining. 
Figure~\ref{fig:rst_chain2} shows the result for SoftNIC as a case study. 
NUMA has a significant impact on performance. For the 2-NF chain, assigning two vCPUs and TCs to different sockets cuts the throughput of 64B packet by nearly half. For the 3-NF chain (the third VNF runs in a different socket than the first two), line rate is only reached for 512B and larger packets. The performance discrepancy is mainly because operations between different NUMA sockets can cause cache misses and ping pong effect \cite{levinthal2009performance}. \zhixiong{To mitigate NUMA effect, we attempt to bridge NFs in a chain via NICs across servers. For example, in a 3-NF chain, NF1 and NF2 are located on server A on the same NUMA socket, and NF3 is located in server B. The two servers are connected by 10GbE NIC. We observe that this eliminates the NUMA effect: throughput of the chain is identical to the case when all 3 NFs share the same CPU socket as shown in Figure~\ref{fig:rst_chain2}. }


\begin{figure}[ht]
\centering
    \includegraphics[width=0.9\linewidth]{rst_chain2}
    \caption{Throughput of NF chaining with the NUMA effect. ``*'' here denotes the case when the vCPUs belong to different sockets. ``\string^'' here denotes the case when NFs are chained up via NICs of different servers to avoid penalty from NUMA.}
    \label{fig:rst_chain2}
    \vspace{-1.5mm}
\end{figure}

\revise{To summarize, the results here show that SoftNIC works adequately with small performance drop in a NF chain, while ClickOS's throughput becomes lower with longer chains. They also demonstrate the importance of carefully assigning cores to VMs of the chain due to NUMA, which implies that it is not always best to colocate VNFs of a chain on the same server. A practical strategy is to place them on different servers to avoid NUMA effect. These observations are useful for real NFV deployment. }



\subsection{NF Colocation}
\label{sec:colocation}

We also measure the performance when multiple NFs colocate on the same server. This is another common deployment scenario of NFV. In the experiments here, we instantiate multiple VMs, bundling each of them to an independent packet generator in the same server, and measure the aggregated throughput.  
For SoftNIC, we build {\tt pkt-gen} in separate VMs and connect them to L3FWD VNFs by independent TCs. On the other hand for ClickOS, we directly use {\tt pkt-gen} to generate packets on the VALE switch connected to the VM. We pin {\tt pkt-gen} and the corresponding VM to the same CPU socket for better performance. Note this is the only scenario where we generate packets at the hypervisor. We scale to at most 3 bundles beyond which the number of cores on our CPU is not enough (our CPU has 16 cores: SoftNIC needs 4 cores for each bundle, and the hypervisor needs cores too).

\begin{figure}[ht]
\centering
    \vspace{-2mm}
    \includegraphics[width=0.9\linewidth]{scaling_hor}
    \caption{Aggregated throughput of multiple colocating L3FWD. }
    \label{fig:scaling}
    \vspace{-4mm}
\end{figure}

As we observe in Figure~\ref{fig:scaling}, both NFV dataplanes perform very well. In almost all results, throughput scales linearly as we colocate more bundles of VMs and packet generators. 
This demonstrates that current technologies provide satisfactory performance isolation and guarantee with multi-core CPUs for realizing NFV.




