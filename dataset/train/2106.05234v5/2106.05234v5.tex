\documentclass{article}









\usepackage[nonatbib,final]{neurips_2021}




\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{amsmath}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage{comment}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage{threeparttable}


\newcommand{\yes}{\ding{51}}
\newcommand{\no}{\ding{55}}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{xspace}
\usepackage{amsthm}
\newcommand{\C}{\mathbb{C}} \newcommand{\F}{\mathbb{F}} \newcommand{\N}{\mathbb{N}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\R}{\mathbb{R}} \newcommand{\Z}{\mathbb{Z}} 

\newcommand{\tr}{\mathop{\mathrm{tr}}}
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\rank}{\mathop{\mathsf{rank}}}
\newcommand{\vect}{\mathop{\mathrm{vec}}}
\newcommand{\nnz}{\mathop{\mathrm{nnz}}}

\newcommand*{\mini}{\mathop{\mathrm{minimize}}}
\newcommand*{\maxi}{\mathop{\mathrm{maximize}}}
\newcommand*{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand*{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand*{\arginf}{\mathop{\mathrm{arginf}}}
\newcommand*{\argsup}{\mathop{\mathrm{argsup}}}
\newcommand{\st}{\text{subject to: }}
\newcommand{\dom}{\mathop{\mathrm{dom}}}
\newcommand{\grad}{{\nabla}}
\newcommand{\hess}{\grad^{2}}

\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\tv}{\mathsf{TV}}
\newcommand{\EE}{\mathbb{E}} \newcommand{\HH}{\mathsf{H}} \newcommand{\id}{\mathsf{Id}} \newcommand{\KK}{\mathds{K}} \newcommand{\MM}{\mathsf{M}} 

\newcommand{\RRbar}{\overline{\RR}} \newcommand{\RRi}{\RR_{\infty}} \newcommand{\pp}{\mathsf{p}} \newcommand{\II}{\mathsf{I}} 




\newcommand{\sgn}{\mathop{\mathrm{sign}}}
\newcommand{\traj}{\mathop{\mathrm{Traj}}}
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\cone}{\mathrm{cone}}
\newcommand{\const}{\mathrm{constant}}
\newcommand{\ri}{\mathop{\mathrm{ri}}}
\newcommand{\sri}{\mathop{\mathrm{sri}}}
\newcommand{\cl}{\mathop{\mathrm{cl}}}
\newcommand{\intr}{\mathop{\mathrm{int}}}
\newcommand{\bd}{\mathop{\mathrm{bd}}}
\newcommand{\emp}{\mathop{\mathrm{emp}}}
\newcommand{\core}{\mathop{\mathrm{core}}}
\newcommand{\epi}{\mathop{\mathrm{epi}}}
\newcommand{\co}{\mathop{\mathrm{co}}}
\newcommand{\med}{\mathop{\mathrm{med}}}
\newcommand{\eproof}{}






\newcommand{\atil}{\tilde{a}}
\newcommand{\btil}{\tilde{b}}
\newcommand{\ctil}{\tilde{c}}
\newcommand{\dtil}{\tilde{d}}
\newcommand{\etil}{\tilde{e}}
\newcommand{\ftil}{\tilde{f}}
\newcommand{\gtil}{\tilde{g}}
\newcommand{\htil}{\tilde{h}}
\newcommand{\itil}{\tilde{i}}
\newcommand{\jtil}{\tilde{j}}
\newcommand{\ktil}{\tilde{k}}
\newcommand{\ltil}{\tilde{l}}
\newcommand{\mtil}{\tilde{m}}
\newcommand{\ntil}{\tilde{n}}
\newcommand{\otil}{\tilde{o}}
\newcommand{\ptil}{\tilde{p}}
\newcommand{\qtil}{\tilde{q}}
\newcommand{\rtil}{\tilde{r}}
\newcommand{\stil}{\tilde{s}}
\newcommand{\ttil}{\tilde{t}}
\newcommand{\util}{\tilde{u}}
\newcommand{\vtil}{\tilde{v}}
\newcommand{\wtil}{\tilde{w}}
\newcommand{\xtil}{\tilde{x}}
\newcommand{\ytil}{\tilde{y}}
\newcommand{\ztil}{\tilde{z}}

\newcommand{\ahat}{\hat{a}}
\newcommand{\bhat}{\hat{b}}
\newcommand{\chat}{\hat{c}}
\newcommand{\dhat}{\hat{d}}
\newcommand{\ehat}{\hat{e}}
\newcommand{\fhat}{\hat{f}}
\newcommand{\ghat}{\hat{g}}
\newcommand{\hhat}{\hat{h}}
\newcommand{\ihat}{\hat{i}}
\newcommand{\jhat}{\hat{j}}
\newcommand{\khat}{\hat{k}}
\newcommand{\lhat}{\hat{l}}
\newcommand{\mhat}{\hat{m}}
\newcommand{\nhat}{\hat{n}}
\newcommand{\ohat}{\hat{o}}
\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\rhat}{\hat{r}}
\newcommand{\shat}{\hat{s}}
\newcommand{\that}{\hat{t}}
\newcommand{\uhat}{\hat{u}}
\newcommand{\vhat}{\hat{v}}
\newcommand{\what}{\hat{w}}
\newcommand{\xhat}{\hat{x}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\zhat}{\hat{z}}

\newcommand{\abar}{\bar{a}}
\newcommand{\bbar}{\bar{b}}
\newcommand{\cbar}{\bar{c}}
\newcommand{\dbar}{\bar{d}}
\newcommand{\ebar}{\bar{e}}
\newcommand{\fbar}{\bar{f}}
\newcommand{\gbar}{\bar{g}}
\newcommand{\hbr}{\bar{h}}
\newcommand{\ibar}{\bar{i}}
\newcommand{\jbar}{\bar{j}}
\newcommand{\kbar}{\bar{k}}
\newcommand{\lbar}{\bar{l}}
\newcommand{\mbar}{\bar{m}}
\newcommand{\nbar}{\bar{n}}
\newcommand{\obar}{\bar{o}}
\newcommand{\pbar}{\bar{p}}
\newcommand{\qbar}{\bar{q}}
\newcommand{\rbar}{\bar{r}}
\newcommand{\sbar}{\bar{s}}
\newcommand{\tbar}{\bar{t}}
\newcommand{\ubar}{\bar{u}}
\newcommand{\vbar}{\bar{v}}
\newcommand{\wbar}{\bar{w}}
\newcommand{\xbar}{\bar{x}}
\newcommand{\ybar}{\bar{y}}
\newcommand{\zbar}{\bar{z}}

\newcommand{\abf}{\mathbf{a}}
\newcommand{\bbf}{\mathbf{b}}
\newcommand{\cbf}{\mathbf{c}}
\newcommand{\dbf}{\mathbf{d}}
\newcommand{\ebf}{\mathbf{e}}
\newcommand{\fbf}{\mathbf{f}}
\newcommand{\gbf}{\mathbf{g}}
\newcommand{\hbf}{\mathbf{h}}
\newcommand{\ibf}{\mathbf{i}}
\newcommand{\jbf}{\mathbf{j}}
\newcommand{\kbf}{\mathbf{k}}
\newcommand{\lbf}{\mathbf{l}}
\newcommand{\mbf}{\mathbf{m}}
\newcommand{\nbf}{\mathbf{n}}
\newcommand{\obf}{\mathbf{o}}
\newcommand{\pbf}{\mathbf{p}}
\newcommand{\qbf}{\mathbf{q}}
\newcommand{\rbf}{\mathbf{r}}
\newcommand{\sbf}{\mathbf{s}}
\newcommand{\tbf}{\mathbf{t}}
\newcommand{\ubf}{\mathbf{u}}
\newcommand{\vbf}{\mathbf{v}}
\newcommand{\wbf}{\mathbf{w}}
\newcommand{\xbf}{\mathbf{x}}
\newcommand{\ybf}{\mathbf{y}}
\newcommand{\zbf}{\mathbf{z}}

\newcommand{\abftil}{\tilde{\abf}}
\newcommand{\bbftil}{\tilde{\bbf}}
\newcommand{\cbftil}{\tilde{\cbf}}
\newcommand{\dbftil}{\tilde{\dbf}}
\newcommand{\ebftil}{\tilde{\ebf}}
\newcommand{\fbftil}{\tilde{\fbf}}
\newcommand{\gbftil}{\tilde{\gbf}}
\newcommand{\hbftil}{\tilde{\hbf}}
\newcommand{\ibftil}{\tilde{\ibf}}
\newcommand{\jbftil}{\tilde{\jbf}}
\newcommand{\kbftil}{\tilde{\kbf}}
\newcommand{\lbftil}{\tilde{\lbf}}
\newcommand{\mbftil}{\tilde{\mbf}}
\newcommand{\nbftil}{\tilde{\nbf}}
\newcommand{\obftil}{\tilde{\obf}}
\newcommand{\pbftil}{\tilde{\pbf}}
\newcommand{\qbftil}{\tilde{\qbf}}
\newcommand{\rbftil}{\tilde{\rbf}}
\newcommand{\sbftil}{\tilde{\sbf}}
\newcommand{\tbftil}{\tilde{\tbf}}
\newcommand{\ubftil}{\tilde{\ubf}}
\newcommand{\vbftil}{\tilde{\vbf}}
\newcommand{\wbftil}{\tilde{\wbf}}
\newcommand{\xbftil}{\tilde{\xbf}}
\newcommand{\ybftil}{\tilde{\ybf}}
\newcommand{\zbftil}{\tilde{\zbf}}

\newcommand{\abfhat}{\hat{\abf}}
\newcommand{\bbfhat}{\hat{\bbf}}
\newcommand{\cbfhat}{\hat{\cbf}}
\newcommand{\dbfhat}{\hat{\dbf}}
\newcommand{\ebfhat}{\hat{\ebf}}
\newcommand{\fbfhat}{\hat{\fbf}}
\newcommand{\gbfhat}{\hat{\gbf}}
\newcommand{\hbfhat}{\hat{\hbf}}
\newcommand{\ibfhat}{\hat{\ibf}}
\newcommand{\jbfhat}{\hat{\jbf}}
\newcommand{\kbfhat}{\hat{\kbf}}
\newcommand{\lbfhat}{\hat{\lbf}}
\newcommand{\mbfhat}{\hat{\mbf}}
\newcommand{\nbfhat}{\hat{\nbf}}
\newcommand{\obfhat}{\hat{\obf}}
\newcommand{\pbfhat}{\hat{\pbf}}
\newcommand{\qbfhat}{\hat{\qbf}}
\newcommand{\rbfhat}{\hat{\rbf}}
\newcommand{\sbfhat}{\hat{\sbf}}
\newcommand{\tbfhat}{\hat{\tbf}}
\newcommand{\ubfhat}{\hat{\ubf}}
\newcommand{\vbfhat}{\hat{\vbf}}
\newcommand{\wbfhat}{\hat{\wbf}}
\newcommand{\xbfhat}{\hat{\xbf}}
\newcommand{\ybfhat}{\hat{\ybf}}
\newcommand{\zbfhat}{\hat{\zbf}}

\newcommand{\abfbar}{\bar{\abf}}
\newcommand{\bbfbar}{\bar{\bbf}}
\newcommand{\cbfbar}{\bar{\cbf}}
\newcommand{\dbfbar}{\bar{\dbf}}
\newcommand{\ebfbar}{\bar{\ebf}}
\newcommand{\fbfbar}{\bar{\fbf}}
\newcommand{\gbfbar}{\bar{\gbf}}
\newcommand{\hbfbar}{\bar{\hbf}}
\newcommand{\ibfbar}{\bar{\ibf}}
\newcommand{\jbfbar}{\bar{\jbf}}
\newcommand{\kbfbar}{\bar{\kbf}}
\newcommand{\lbfbar}{\bar{\lbf}}
\newcommand{\mbfbar}{\bar{\mbf}}
\newcommand{\nbfbar}{\bar{\nbf}}
\newcommand{\obfbar}{\bar{\obf}}
\newcommand{\pbfbar}{\bar{\pbf}}
\newcommand{\qbfbar}{\bar{\qbf}}
\newcommand{\rbfbar}{\bar{\rbf}}
\newcommand{\sbfbar}{\bar{\sbf}}
\newcommand{\tbfbar}{\bar{\tbf}}
\newcommand{\ubfbar}{\bar{\ubf}}
\newcommand{\vbfbar}{\bar{\vbf}}
\newcommand{\wbfbar}{\bar{\wbf}}
\newcommand{\xbfbar}{\bar{\xbf}}
\newcommand{\ybfbar}{\bar{\ybf}}
\newcommand{\zbfbar}{\bar{\zbf}}

\newcommand{\asf}{\mathsf{a}}
\newcommand{\bsf}{\mathsf{b}}
\newcommand{\csf}{\mathsf{c}}
\newcommand{\dsf}{\mathsf{d}}
\newcommand{\esf}{\mathsf{e}}
\newcommand{\fsf}{\mathsf{f}}
\newcommand{\gsf}{\mathsf{g}}
\newcommand{\hsf}{\mathsf{h}}
\newcommand{\isf}{\mathsf{i}}
\newcommand{\jsf}{\mathsf{j}}
\newcommand{\ksf}{\mathsf{k}}
\newcommand{\lsf}{\mathsf{l}}
\newcommand{\msf}{\mathsf{m}}
\newcommand{\nsf}{\mathsf{n}}
\newcommand{\osf}{\mathsf{o}}
\newcommand{\psf}{\mathsf{p}}
\newcommand{\qsf}{\mathsf{q}}
\newcommand{\rsf}{\mathsf{r}}
\newcommand{\ssf}{\mathsf{s}}
\newcommand{\tsf}{\mathsf{t}}
\newcommand{\usf}{\mathsf{u}}
\newcommand{\vsf}{\mathsf{v}}
\newcommand{\wsf}{\mathsf{w}}
\newcommand{\xsf}{\mathsf{x}}
\newcommand{\ysf}{\mathsf{y}}
\newcommand{\zsf}{\mathsf{z}}

\newcommand{\att}{\mathtt{a}}
\newcommand{\btt}{\mathtt{b}}
\newcommand{\ctt}{\mathtt{c}}
\newcommand{\dtt}{\mathtt{d}}
\newcommand{\ett}{\mathtt{e}}
\newcommand{\ftt}{\mathtt{f}}
\newcommand{\gtt}{\mathtt{g}}
\newcommand{\htt}{\mathtt{h}}
\newcommand{\itt}{\mathtt{i}}
\newcommand{\jtt}{\mathtt{j}}
\newcommand{\ktt}{\mathtt{k}}
\newcommand{\ltt}{\mathtt{l}}
\newcommand{\mtt}{\mathtt{m}}
\newcommand{\ntt}{\mathtt{n}}
\newcommand{\ott}{\mathtt{o}}
\newcommand{\ptt}{\mathtt{p}}
\newcommand{\qtt}{\mathtt{q}}
\newcommand{\rtt}{\mathtt{r}}
\newcommand{\stt}{\mathtt{s}}
\newcommand{\ttt}{\mathtt{t}}
\newcommand{\utt}{\mathtt{u}}
\newcommand{\vtt}{\mathtt{v}}
\newcommand{\wtt}{\mathtt{w}}
\newcommand{\xtt}{\mathtt{x}}
\newcommand{\ytt}{\mathtt{y}}
\newcommand{\ztt}{\mathtt{z}}

\newcommand{\abld}{\boldsymbol{a}}
\newcommand{\bbld}{\boldsymbol{b}}
\newcommand{\cbld}{\boldsymbol{c}}
\newcommand{\dbld}{\boldsymbol{d}}
\newcommand{\ebld}{\boldsymbol{e}}
\newcommand{\fbld}{\boldsymbol{f}}
\newcommand{\gbld}{\boldsymbol{g}}
\newcommand{\hbld}{\boldsymbol{h}}
\newcommand{\ibld}{\boldsymbol{i}}
\newcommand{\jbld}{\boldsymbol{j}}
\newcommand{\kbld}{\boldsymbol{k}}
\newcommand{\lbld}{\boldsymbol{l}}
\newcommand{\mbld}{\boldsymbol{m}}
\newcommand{\nbld}{\boldsymbol{n}}
\newcommand{\obld}{\boldsymbol{o}}
\newcommand{\pbld}{\boldsymbol{p}}
\newcommand{\qbld}{\boldsymbol{q}}
\newcommand{\rbld}{\boldsymbol{r}}
\newcommand{\sbld}{\boldsymbol{s}}
\newcommand{\tbld}{\boldsymbol{t}}
\newcommand{\ubld}{\boldsymbol{u}}
\newcommand{\vbld}{\boldsymbol{v}}
\newcommand{\wbld}{\boldsymbol{w}}
\newcommand{\xbld}{\boldsymbol{x}}
\newcommand{\ybld}{\boldsymbol{y}}
\newcommand{\zbld}{\boldsymbol{z}}

\newcommand{\Atil}{\tilde{A}}
\newcommand{\Btil}{\tilde{B}}
\newcommand{\Ctil}{\tilde{C}}
\newcommand{\Dtil}{\tilde{D}}
\newcommand{\Etil}{\tilde{E}}
\newcommand{\Ftil}{\tilde{F}}
\newcommand{\Gtil}{\tilde{G}}
\newcommand{\Htil}{\tilde{H}}
\newcommand{\Itil}{\tilde{I}}
\newcommand{\Jtil}{\tilde{J}}
\newcommand{\Ktil}{\tilde{K}}
\newcommand{\Ltil}{\tilde{L}}
\newcommand{\Mtil}{\tilde{M}}
\newcommand{\Ntil}{\tilde{N}}
\newcommand{\Otil}{\tilde{O}}
\newcommand{\Ptil}{\tilde{P}}
\newcommand{\Qtil}{\tilde{Q}}
\newcommand{\Rtil}{\tilde{R}}
\newcommand{\Stil}{\tilde{S}}
\newcommand{\Ttil}{\tilde{T}}
\newcommand{\Util}{\tilde{U}}
\newcommand{\Vtil}{\tilde{V}}
\newcommand{\Wtil}{\tilde{W}}
\newcommand{\Xtil}{\tilde{X}}
\newcommand{\Ytil}{\tilde{Y}}
\newcommand{\Ztil}{\tilde{Z}}

\newcommand{\Ahat}{\hat{A}}
\newcommand{\Bhat}{\hat{B}}
\newcommand{\Chat}{\hat{C}}
\newcommand{\Dhat}{\hat{D}}
\newcommand{\Ehat}{\hat{E}}
\newcommand{\Fhat}{\hat{F}}
\newcommand{\Ghat}{\hat{G}}
\newcommand{\Hhat}{\hat{H}}
\newcommand{\Ihat}{\hat{I}}
\newcommand{\Jhat}{\hat{J}}
\newcommand{\Khat}{\hat{K}}
\newcommand{\Lhat}{\hat{L}}
\newcommand{\Mhat}{\hat{M}}
\newcommand{\Nhat}{\hat{N}}
\newcommand{\Ohat}{\hat{O}}
\newcommand{\Phat}{\hat{P}}
\newcommand{\Qhat}{\hat{Q}}
\newcommand{\Rhat}{\hat{R}}
\newcommand{\Shat}{\hat{S}}
\newcommand{\That}{\hat{T}}
\newcommand{\Uhat}{\hat{U}}
\newcommand{\Vhat}{\hat{V}}
\newcommand{\What}{\hat{W}}
\newcommand{\Xhat}{\hat{X}}
\newcommand{\Yhat}{\hat{Y}}
\newcommand{\Zhat}{\hat{Z}}

\newcommand{\Abar}{\bar{A}}
\newcommand{\Bbar}{\bar{B}}
\newcommand{\Cbar}{\bar{C}}
\newcommand{\Dbar}{\bar{D}}
\newcommand{\Ebar}{\bar{E}}
\newcommand{\Fbar}{\bar{F}}
\newcommand{\Gbar}{\bar{G}}
\newcommand{\Hbar}{\bar{H}}
\newcommand{\Ibar}{\bar{I}}
\newcommand{\Jbar}{\bar{J}}
\newcommand{\Kbar}{\bar{K}}
\newcommand{\Lbar}{\bar{L}}
\newcommand{\Mbar}{\bar{M}}
\newcommand{\Nbar}{\bar{N}}
\newcommand{\Obar}{\bar{O}}
\newcommand{\Pbar}{\bar{P}}
\newcommand{\Qbar}{\bar{Q}}
\newcommand{\Rbar}{\bar{R}}
\newcommand{\Sbar}{\bar{S}}
\newcommand{\Tbar}{\bar{T}}
\newcommand{\Ubar}{\bar{U}}
\newcommand{\Vbar}{\bar{V}}
\newcommand{\Wbar}{\bar{W}}
\newcommand{\Xbar}{\bar{X}}
\newcommand{\Ybar}{\bar{Y}}
\newcommand{\Zbar}{\bar{Z}}

\newcommand{\Abf}{\mathbf{A}}
\newcommand{\Bbf}{\mathbf{B}}
\newcommand{\Cbf}{\mathbf{C}}
\newcommand{\Dbf}{\mathbf{D}}
\newcommand{\Ebf}{\mathbf{E}}
\newcommand{\Fbf}{\mathbf{F}}
\newcommand{\Gbf}{\mathbf{G}}
\newcommand{\Hbf}{\mathbf{H}}
\newcommand{\Ibf}{\mathbf{I}}
\newcommand{\Jbf}{\mathbf{J}}
\newcommand{\Kbf}{\mathbf{K}}
\newcommand{\Lbf}{\mathbf{L}}
\newcommand{\Mbf}{\mathbf{M}}
\newcommand{\Nbf}{\mathbf{N}}
\newcommand{\Obf}{\mathbf{O}}
\newcommand{\Pbf}{\mathbf{P}}
\newcommand{\Qbf}{\mathbf{Q}}
\newcommand{\Rbf}{\mathbf{R}}
\newcommand{\Sbf}{\mathbf{S}}
\newcommand{\Tbf}{\mathbf{T}}
\newcommand{\Ubf}{\mathbf{U}}
\newcommand{\Vbf}{\mathbf{V}}
\newcommand{\Wbf}{\mathbf{W}}
\newcommand{\Xbf}{\mathbf{X}}
\newcommand{\Ybf}{\mathbf{Y}}
\newcommand{\Zbf}{\mathbf{Z}}

\newcommand{\Abftil}{\tilde{\Abf}}
\newcommand{\Bbftil}{\tilde{\Bbf}}
\newcommand{\Cbftil}{\tilde{\Cbf}}
\newcommand{\Dbftil}{\tilde{\Dbf}}
\newcommand{\Ebftil}{\tilde{\Ebf}}
\newcommand{\Fbftil}{\tilde{\Fbf}}
\newcommand{\Gbftil}{\tilde{\Gbf}}
\newcommand{\Hbftil}{\tilde{\Hbf}}
\newcommand{\Ibftil}{\tilde{\Ibf}}
\newcommand{\Jbftil}{\tilde{\Jbf}}
\newcommand{\Kbftil}{\tilde{\Kbf}}
\newcommand{\Lbftil}{\tilde{\Lbf}}
\newcommand{\Mbftil}{\tilde{\Mbf}}
\newcommand{\Nbftil}{\tilde{\Nbf}}
\newcommand{\Obftil}{\tilde{\Obf}}
\newcommand{\Pbftil}{\tilde{\Pbf}}
\newcommand{\Qbftil}{\tilde{\Qbf}}
\newcommand{\Rbftil}{\tilde{\Rbf}}
\newcommand{\Sbftil}{\tilde{\Sbf}}
\newcommand{\Tbftil}{\tilde{\Tbf}}
\newcommand{\Ubftil}{\tilde{\Ubf}}
\newcommand{\Vbftil}{\tilde{\Vbf}}
\newcommand{\Wbftil}{\tilde{\Wbf}}
\newcommand{\Xbftil}{\tilde{\Xbf}}
\newcommand{\Ybftil}{\tilde{\Ybf}}
\newcommand{\Zbftil}{\tilde{\Zbf}}

\newcommand{\Abfhat}{\hat{\Abf}}
\newcommand{\Bbfhat}{\hat{\Bbf}}
\newcommand{\Cbfhat}{\hat{\Cbf}}
\newcommand{\Dbfhat}{\hat{\Dbf}}
\newcommand{\Ebfhat}{\hat{\Ebf}}
\newcommand{\Fbfhat}{\hat{\Fbf}}
\newcommand{\Gbfhat}{\hat{\Gbf}}
\newcommand{\Hbfhat}{\hat{\Hbf}}
\newcommand{\Ibfhat}{\hat{\Ibf}}
\newcommand{\Jbfhat}{\hat{\Jbf}}
\newcommand{\Kbfhat}{\hat{\Kbf}}
\newcommand{\Lbfhat}{\hat{\Lbf}}
\newcommand{\Mbfhat}{\hat{\Mbf}}
\newcommand{\Nbfhat}{\hat{\Nbf}}
\newcommand{\Obfhat}{\hat{\Obf}}
\newcommand{\Pbfhat}{\hat{\Pbf}}
\newcommand{\Qbfhat}{\hat{\Qbf}}
\newcommand{\Rbfhat}{\hat{\Rbf}}
\newcommand{\Sbfhat}{\hat{\Sbf}}
\newcommand{\Tbfhat}{\hat{\Tbf}}
\newcommand{\Ubfhat}{\hat{\Ubf}}
\newcommand{\Vbfhat}{\hat{\Vbf}}
\newcommand{\Wbfhat}{\hat{\Wbf}}
\newcommand{\Xbfhat}{\hat{\Xbf}}
\newcommand{\Ybfhat}{\hat{\Ybf}}
\newcommand{\Zbfhat}{\hat{\Zbf}}

\newcommand{\Abfbar}{\bar{\Abf}}
\newcommand{\Bbfbar}{\bar{\Bbf}}
\newcommand{\Cbfbar}{\bar{\Cbf}}
\newcommand{\Dbfbar}{\bar{\Dbf}}
\newcommand{\Ebfbar}{\bar{\Ebf}}
\newcommand{\Fbfbar}{\bar{\Fbf}}
\newcommand{\Gbfbar}{\bar{\Gbf}}
\newcommand{\Hbfbar}{\bar{\Hbf}}
\newcommand{\Ibfbar}{\bar{\Ibf}}
\newcommand{\Jbfbar}{\bar{\Jbf}}
\newcommand{\Kbfbar}{\bar{\Kbf}}
\newcommand{\Lbfbar}{\bar{\Lbf}}
\newcommand{\Mbfbar}{\bar{\Mbf}}
\newcommand{\Nbfbar}{\bar{\Nbf}}
\newcommand{\Obfbar}{\bar{\Obf}}
\newcommand{\Pbfbar}{\bar{\Pbf}}
\newcommand{\Qbfbar}{\bar{\Qbf}}
\newcommand{\Rbfbar}{\bar{\Rbf}}
\newcommand{\Sbfbar}{\bar{\Sbf}}
\newcommand{\Tbfbar}{\bar{\Tbf}}
\newcommand{\Ubfbar}{\bar{\Ubf}}
\newcommand{\Vbfbar}{\bar{\Vbf}}
\newcommand{\Wbfbar}{\bar{\Wbf}}
\newcommand{\Xbfbar}{\bar{\Xbf}}
\newcommand{\Ybfbar}{\bar{\Ybf}}
\newcommand{\Zbfbar}{\bar{\Zbf}}

\newcommand{\Asf}{\mathsf{A}}
\newcommand{\Bsf}{\mathsf{B}}
\newcommand{\Csf}{\mathsf{C}}
\newcommand{\Dsf}{\mathsf{D}}
\newcommand{\Esf}{\mathsf{E}}
\newcommand{\Fsf}{\mathsf{F}}
\newcommand{\Gsf}{\mathsf{G}}
\newcommand{\Hsf}{\mathsf{H}}
\newcommand{\Isf}{\mathsf{I}}
\newcommand{\Jsf}{\mathsf{J}}
\newcommand{\Ksf}{\mathsf{K}}
\newcommand{\Lsf}{\mathsf{L}}
\newcommand{\Msf}{\mathsf{M}}
\newcommand{\Nsf}{\mathsf{N}}
\newcommand{\Osf}{\mathsf{O}}
\newcommand{\Psf}{\mathsf{P}}
\newcommand{\Qsf}{\mathsf{Q}}
\newcommand{\Rsf}{\mathsf{R}}
\newcommand{\Ssf}{\mathsf{S}}
\newcommand{\Tsf}{\mathsf{T}}
\newcommand{\Usf}{\mathsf{U}}
\newcommand{\Vsf}{\mathsf{V}}
\newcommand{\Wsf}{\mathsf{W}}
\newcommand{\Xsf}{\mathsf{X}}
\newcommand{\Ysf}{\mathsf{Y}}
\newcommand{\Zsf}{\mathsf{Z}}

\newcommand{\Att}{\mathtt{A}}
\newcommand{\Btt}{\mathtt{B}}
\newcommand{\Ctt}{\mathtt{C}}
\newcommand{\Dtt}{\mathtt{D}}
\newcommand{\Ett}{\mathtt{E}}
\newcommand{\Ftt}{\mathtt{F}}
\newcommand{\Gtt}{\mathtt{G}}
\newcommand{\Htt}{\mathtt{H}}
\newcommand{\Itt}{\mathtt{I}}
\newcommand{\Jtt}{\mathtt{J}}
\newcommand{\Ktt}{\mathtt{K}}
\newcommand{\Ltt}{\mathtt{L}}
\newcommand{\Mtt}{\mathtt{M}}
\newcommand{\Ntt}{\mathtt{N}}
\newcommand{\Ott}{\mathtt{O}}
\newcommand{\Ptt}{\mathtt{P}}
\newcommand{\Qtt}{\mathtt{Q}}
\newcommand{\Rtt}{\mathtt{R}}
\newcommand{\Stt}{\mathtt{S}}
\newcommand{\Ttt}{\mathtt{T}}
\newcommand{\Utt}{\mathtt{U}}
\newcommand{\Vtt}{\mathtt{V}}
\newcommand{\Wtt}{\mathtt{W}}
\newcommand{\Xtt}{\mathtt{X}}
\newcommand{\Ytt}{\mathtt{Y}}
\newcommand{\Ztt}{\mathtt{Z}}

\newcommand{\Abld}{\boldsymbol{A}}
\newcommand{\Bbld}{\boldsymbol{B}}
\newcommand{\Cbld}{\boldsymbol{C}}
\newcommand{\Dbld}{\boldsymbol{D}}
\newcommand{\Ebld}{\boldsymbol{E}}
\newcommand{\Fbld}{\boldsymbol{F}}
\newcommand{\Gbld}{\boldsymbol{G}}
\newcommand{\Hbld}{\boldsymbol{H}}
\newcommand{\Ibld}{\boldsymbol{I}}
\newcommand{\Jbld}{\boldsymbol{J}}
\newcommand{\Kbld}{\boldsymbol{K}}
\newcommand{\Lbld}{\boldsymbol{L}}
\newcommand{\Mbld}{\boldsymbol{M}}
\newcommand{\Nbld}{\boldsymbol{N}}
\newcommand{\Obld}{\boldsymbol{O}}
\newcommand{\Pbld}{\boldsymbol{P}}
\newcommand{\Qbld}{\boldsymbol{Q}}
\newcommand{\Rbld}{\boldsymbol{R}}
\newcommand{\Sbld}{\boldsymbol{S}}
\newcommand{\Tbld}{\boldsymbol{T}}
\newcommand{\Ubld}{\boldsymbol{U}}
\newcommand{\Vbld}{\boldsymbol{V}}
\newcommand{\Wbld}{\boldsymbol{W}}
\newcommand{\Xbld}{\boldsymbol{X}}
\newcommand{\Ybld}{\boldsymbol{Y}}
\newcommand{\Zbld}{\boldsymbol{Z}}

\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}

\newcommand{\Abb}{\mathbb{A}}
\renewcommand{\Bbb}{\mathbb{B}}
\newcommand{\Cbb}{\mathbb{C}}
\newcommand{\Dbb}{\mathbb{D}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Fbb}{\mathbb{F}}
\newcommand{\Gbb}{\mathbb{G}}
\newcommand{\Hbb}{\mathbb{H}}
\newcommand{\Ibb}{\mathbb{I}}
\newcommand{\Jbb}{\mathbb{J}}
\newcommand{\Kbb}{\mathbb{K}}
\newcommand{\Lbb}{\mathbb{L}}
\newcommand{\Mbb}{\mathbb{M}}
\newcommand{\Nbb}{\mathbb{N}}
\newcommand{\Obb}{\mathbb{O}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Sbb}{\mathbb{S}}
\newcommand{\Tbb}{\mathbb{T}}
\newcommand{\Ubb}{\mathbb{U}}
\newcommand{\Vbb}{\mathbb{V}}
\newcommand{\Wbb}{\mathbb{W}}
\newcommand{\Xbb}{\mathbb{X}}
\newcommand{\Ybb}{\mathbb{Y}}
\newcommand{\Zbb}{\mathbb{Z}}

\renewcommand{\vec}[1]{\mathbf{\boldsymbol{#1}}}

\newcommand{\avec}{\vec{a}}
\newcommand{\bvec}{\vec{b}}
\newcommand{\cvec}{\vec{c}}
\newcommand{\dvec}{\vec{d}}
\newcommand{\evec}{\vec{e}}
\newcommand{\fvec}{\vec{f}}
\newcommand{\gvec}{\vec{g}}
\newcommand{\hvec}{\vec{h}}
\newcommand{\ivec}{\vec{i}}
\newcommand{\jvec}{\vec{j}}
\newcommand{\kvec}{\vec{k}}
\newcommand{\lvec}{\vec{l}}
\newcommand{\mvec}{\vec{m}}
\newcommand{\nvec}{\vec{n}}
\newcommand{\ovec}{\vec{o}}
\newcommand{\pvec}{\vec{p}}
\newcommand{\qvec}{\vec{q}}
\newcommand{\rvec}{\vec{r}}
\newcommand{\svec}{\vec{s}}
\newcommand{\tvec}{\vec{t}}
\newcommand{\uvec}{\vec{u}}
\newcommand{\vvec}{\vec{v}}
\newcommand{\wvec}{\vec{w}}
\newcommand{\xvec}{\vec{x}}
\newcommand{\yvec}{\vec{y}}
\newcommand{\zvec}{\vec{z}}

\newcommand{\Avec}{\vec{A}}
\newcommand{\Bvec}{\vec{B}}
\newcommand{\Cvec}{\vec{C}}
\newcommand{\Dvec}{\vec{D}}
\newcommand{\Evec}{\vec{E}}
\newcommand{\Fvec}{\vec{F}}
\newcommand{\Gvec}{\vec{G}}
\newcommand{\Hvec}{\vec{H}}
\newcommand{\Ivec}{\vec{I}}
\newcommand{\Jvec}{\vec{J}}
\newcommand{\Kvec}{\vec{K}}
\newcommand{\Lvec}{\vec{L}}
\newcommand{\Mvec}{\vec{M}}
\newcommand{\Nvec}{\vec{N}}
\newcommand{\Ovec}{\vec{O}}
\newcommand{\Pvec}{\vec{P}}
\newcommand{\Qvec}{\vec{Q}}
\newcommand{\Rvec}{\vec{R}}
\newcommand{\Svec}{\vec{S}}
\newcommand{\Tvec}{\vec{T}}
\newcommand{\Uvec}{\vec{U}}
\newcommand{\Vvec}{\vec{V}}
\newcommand{\Wvec}{\vec{W}}
\newcommand{\Xvec}{\vec{X}}
\newcommand{\Yvec}{\vec{Y}}
\newcommand{\Zvec}{\vec{Z}}

\newcommand{\yvecbar}{\bar{\vec{y}}}
\newcommand{\wvecbar}{\bar{\vec{w}}}
\newcommand{\xvecbar}{\bar{\vec{x}}}
\newcommand{\yvectil}{\tilde{\vec{y}}}
\newcommand{\yvechat}{\hat{\vec{y}}}






\ifx\BlackBox\undefined
\newcommand{\BlackBox}{\rule{1.5ex}{1.5ex}}  \fi

\ifx\QED\undefined
\def\QED{~\rule[-1pt]{5pt}{5pt}\par\medskip}
\fi

\ifx\proof\undefined
\newenvironment{proof}{\par\noindent{\em Proof:\ }}{\hfill\BlackBox\
a_{i}^{(l)}=\text { AGGREGATE }^{(l)}\left(\left\{h_{j}^{(l-1)}: j \in \mathcal{N}(v_i)\right\}\right), \quad
h_{i}^{(l)}=\text { COMBINE }^{(l)}\left(h_{i}^{(l-1)}, a_{i}^{(l)}\right) ,
\label{eqn:agg-comb}

h_{G}=\operatorname{READOUT}\left(\left\{h_{i}^{(L)} \mid v_i \in G \right\}\right).

    Q = HW_Q,\quad K = HW_K,\quad V = HW_V,\label{eqn:attention-alpha}\\
    A = \frac{QK^\top}{\sqrt{d_K}}, \quad \attn{H} = \softmax{A}V,\label{eqn:attention-alpha2}

\label{eqn:degree_encoding}
    h_i^{(0)} = x_i + z^-_{\text{deg}^{-}(v_i)} + z^+_{\text{deg}^{+}(v_i)},

\label{eqn:rnpe}
    A_{ij}=\frac{(h_iW_{Q})(h_jW_{K})^T}{\sqrt{d}}  + b_{\phi(v_i,v_j)},

    A_{ij}=\frac{(h_iW_{Q})(h_jW_{K})^T}{\sqrt{d}} + b_{\phi(v_i,v_j)} + c_{ij},\ \text{where}\ c_{ij}=\frac{1}{N}\sum_{n=1}^{N} x_{e_n}(w^{E}_{n})^T,
    \label{eqn:attn-edge}

    h^{'(l)} &= \text{MHA}(\text{LN}(h^{(l-1)})) + h^{(l-1)}\\
    h^{(l)} &= \text{FFN}(\text{LN}(h^{'(l)})) + h^{'(l)}



\paragraph{Special Node.} As stated in the previous section, various graph pooling functions are proposed to represent the graph embedding. Inspired by ~\cite{gilmer2017neural}, in Graphormer, we add a special node called {\tt[VNode]} to the graph, and make connection between {\tt[VNode]} and each node individually. In the AGGREGATE-COMBINE step, the representation of {\tt[VNode]} has been updated as normal nodes in graph, and the representation of the entire graph  would be the node feature of {\tt[VNode]} in the final layer. In the BERT model~\cite{devlin2019bert,liu2019roberta}, there is a similar token, i.e., {\tt[CLS]}, which is a special token attached at the beginning of each sequence, to represent the sequence-level feature on downstream tasks. While the {\tt[VNode]} is connected to all other nodes in graph, which means the distance of the shortest path is  for any  and , the connection is not physical. To distinguish the connection of physical and virtual, inspired by ~\cite{ke2020rethinking}, we reset all spatial encodings for  and  to a distinct learnable scalar.




\subsection{How Powerful is Graphormer?}
\label{sec:expressive}
In the previous subsections, we introduce three structural encodings and the architecture of Graphormer. Then a natural question is: \emph{Do these modifications make Graphormer more powerful than other GNN variants? } In this subsection, we first give an affirmative answer by showing that Graphormer can represent the AGGREGATE and COMBINE steps in popular GNN models:

\begin{fact}
\label{fact:recover_gnn}
By choosing proper weights and distance function , the Graphormer layer can represent AGGREGATE and COMBINE steps of popular GNN models such as GIN, GCN, GraphSAGE.
\end{fact}
The proof sketch to derive this result is: 1) Spatial encoding enables self-attention module to distinguish neighbor set  of node  so that the softmax function can calculate mean statistics over ; 2) Knowing the degree of a node, mean over neighbors can be translated to sum over neighbors; 3) With multiple heads and FFN, representations of  and  can be processed separately and combined together later. We defer the proof of this fact to Appendix A.

Moreover, we show further that by using our spatial encoding, Graphormer can go beyond classic message passing GNNs whose expressive power is no more than the 1-Weisfeiler-Lehman (WL) test. We give a concrete example in Appendix A to show how Graphormer helps distinguish graphs that the 1-WL test fails to.

\paragraph{Connection between Self-attention and Virtual Node.} 
Besides the superior expressiveness than popular GNNs, we also find an interesting connection between using self-attention and the virtual node heuristic~\cite{gilmer2017neural,li2017learning,ishiguro2019graph,hu2020open}. As shown in the leaderboard of OGB~\cite{hu2020open}, the virtual node trick, which augments graphs with additional supernodes that are connected to all nodes in the original graphs, can significantly improve the performance of existing GNNs. Conceptually, the benefit of the virtual node is that it can aggregate the information of the \emph{whole graph} (like the READOUT function) and then propagate it to \emph{each node}. However, a naive addition of a supernode to a graph can potentially lead to inadvertent over-smoothing of information propagation~\cite{ishiguro2019graph}. We instead find that such a graph-level aggregation and propagation operation can be naturally fulfilled by vanilla self-attention without additional encodings. Concretely, we can prove the following fact:
\begin{fact}
\label{fact:readout}
By choosing proper weights, every node representation of the output of a Graphormer layer without additional encodings can represent MEAN READOUT functions.
\end{fact}
This fact takes the advantage of self-attention that each node can attend to all other nodes. Thus it can simulate graph-level READOUT operation to aggregate information from the whole graph. Besides the theoretical justification, we empirically find that Graphormer does not encounter the problem of over-smoothing, which makes the improvement scalable. The fact also inspires us to introduce a special node for graph readout (see the previous subsection).







\section{Experiments}

We first conduct experiments on the recent OGB-LSC~\cite{hu2021ogb} quantum chemistry regression (i.e., PCQM4M-LSC) challenge, which is currently the biggest graph-level prediction dataset and contains more than 3.8M graphs in total. Then, we report the results on the other three popular tasks: ogbg-molhiv, ogbg-molpcba and ZINC, which come from the OGB~\cite{hu2020open} and benchmarking-GNN~\cite{dwivedi2020benchmarking} leaderboards. Finally, we ablate the important design elements of Graphormer. A detailed description of datasets and training strategies could be found in Appendix B.


\subsection{OGB Large-Scale Challenge}
\paragraph{Baselines.} We benchmark the proposed Graphormer with GCN~\cite{kipf2016semi} and GIN~\cite{xu2018how}, and their variants with virtual node (-VN)~\cite{gilmer2017neural}. They achieve the state-of-the-art valid and test mean absolute error (MAE) on the official leaderboard\footnote{\url{https://github.com/snap-stanford/ogb/tree/master/examples/lsc/pcqm4m\#performance}}~\cite{hu2021ogb}. In addition, we compare to GIN's multi-hop variant~\cite{brossard2020graph}, and 12-layer deep graph network DeeperGCN~\cite{li2020deepergcn}, which also show promising performance on other leaderboards. We further compare our Graphormer with the recent Transformer-based graph model GT~\cite{dwivedi2021generalization}.

\paragraph{Settings.} We primarily report results on two model sizes: \textbf{Graphormer} (), and a smaller one \textbf{Graphormer\xspace} (). Both the number of attention heads in the attention module and the dimensionality of edge features  are set to 32. We use AdamW as the optimizer, and set the hyper-parameter  to 1e-8 and  to (0.99,0.999). The peak learning rate is set to 2e-4 (3e-4 for \textbf{Graphormer\xspace}) with a 60k-step warm-up stage followed by a linear decay learning rate scheduler. The total training steps are 1M. The batch size is set to 1024. All models are trained on 8 NVIDIA V100 GPUS for about 2 days.

\begin{table}[ht]
\small
\centering
\caption{Results on PCQM4M-LSC. * \ indicates the results are cited from the official leaderboard~\cite{hu2021ogb}. }
\label{tab:pcq-table}
\begin{tabular}{c|c|cc}
\toprule
method            & \#param. & train MAE     & validate MAE       \\ \hline
GCN~\cite{kipf2016semi}  & 2.0M & 0.1318   & 0.1691 (0.1684*)  \\

GIN~\cite{xu2018how} & 3.8M & 0.1203 & 0.1537 (0.1536*)   \\ 

GCN-{\scriptsize VN} ~\cite{kipf2016semi,gilmer2017neural} & 4.9M & 0.1225  & 0.1485 (0.1510*)  \\

GIN-{\scriptsize VN}~\cite{xu2018how,gilmer2017neural} & 6.7M & 0.1150   & 0.1395 (0.1396*)   \\ 

GINE-{\scriptsize VN} ~\cite{brossard2020graph,gilmer2017neural} & 13.2M & 0.1248 & 0.1430  \\ 

DeeperGCN-{\scriptsize VN}~\cite{li2020deepergcn,gilmer2017neural} & 25.5M & 0.1059        & 0.1398   \\

\hline
GT~\cite{dwivedi2021generalization}  & 0.6M & 0.0944 & 0.1400  \\ 

GT-{\scriptsize Wide}~\cite{dwivedi2021generalization} & 83.2M  & 0.0955 & 0.1408 \\ 

\hline

Graphormer & 12.5M &  0.0778  & 0.1264   \\ 
Graphormer & 47.1M  & \textbf{0.0582} & \textbf{0.1234}    \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Results.} Table \ref{tab:pcq-table} summarizes performance comparisons on PCQM4M-LSC dataset. From the table, GIN-{\scriptsize VN} achieves the previous state-of-the-art validate MAE of 0.1395. The original implementation of GT~\cite{dwivedi2021generalization} employs a hidden dimension of 64 to reduce the total number of parameters. For a fair comparison, we also report the result by enlarging the hidden dimension to 768, denoted by GT-{\scriptsize Wide}, which leads to a total number of parameters of 83.2M. While, both GT and GT-{\scriptsize Wide} do not outperform GIN-{\scriptsize VN} and DeeperGCN-{\scriptsize VN}. Especially, we do not observe a performance gain along with the growth of parameters of GT.

Compared to the previous state-of-the-art GNN architecture, Graphormer noticeably surpasses GIN-{\scriptsize VN} by a large margin, e.g., 11.5\% relative validate MAE decline. By using the ensemble with ExpC~\cite{yang2020breaking}, we got a 0.1200 MAE on complete test set and won the first place of the graph-level track in OGB Large-Scale Challenge\cite{hu2021ogb, ying2021first}. As stated in Section \ref{sec:expressive}, we further find that the proposed Graphormer does not encounter the problem of over-smoothing, i.e., the train and validate error keep going down along with the growth of depth and width of models.

\subsection{Graph Representation}

In this section, we further investigate the performance of Graphormer on commonly used graph-level prediction tasks of popular leaderboards, i.e., OGB~\cite{hu2020open} (OGBG-MolPCBA, OGBG-MolHIV), and benchmarking-GNN~\cite{dwivedi2020benchmarking} (ZINC). Since pre-training is encouraged by OGB, we mainly explore the transferable capability of  a Graphormer model pre-trained on OGB-LSC (i.e., PCQM4M-LSC). Please note that the model configurations, hyper-parameters, and the pre-training performance of pre-trained Graphormers used for MolPCBA and MolHIV are different from the models used in the previous subsection. Please refer to Appendix B for detailed descriptions. For benchmarking-GNN, which does not encourage large pre-trained model, we train an additional Graphormer\xspace (, total param.) from scratch on ZINC.

\paragraph{Baselines.} We report performance of GNNs which achieve top-performance on the official leaderboards\footnote{\url{https://ogb.stanford.edu/docs/leader_graphprop/}\\ \url{https://github.com/graphdeeplearning/benchmarking-gnns/blob/master/docs/07_leaderboards.md}} \emph{without additional domain-specific features}. Considering that the pre-trained Graphormer leverages external data, for a fair comparison on OGB datasets, we additionally report performance for fine-tuning GIN-{\scriptsize VN} pre-trained on PCQM4M-LSC dataset, which achieves the previous state-of-the-art valid and test MAE on that dataset. 


\paragraph{Settings.} We report detailed training strategies in Appendix B. 
In addition, Graphormer is more easily trapped in the over-fitting problem due to the large size of the model and the small size of the dataset. Therefore, we employ a widely used data augmentation for graph - FLAG~\cite{kong2020flag}, to mitigate the over-fitting problem on OGB datasets.

\begin{table}[t]
\begin{minipage}{0.47\linewidth}
\centering
\small
\caption{Results on MolPCBA.}
\label{tab:pcba}
\begin{tabular}{c|c|c}
\toprule
  method   & \#param. & AP (\%) \\
  \hline
    DeeperGCN-{\scriptsize VN+FLAG}~\cite{li2020deepergcn} & 5.6M & 28.420.43\\
    DGN~\cite{beaini2020directional} & 6.7M & 28.850.30 \\
    GINE-{\scriptsize VN}~\cite{brossard2020graph} & 6.1M & 29.170.15 \\
    PHC-GNN~\cite{le2021parameterized} & 1.7M & 29.470.26 \\
    GINE-{\scriptsize APPNP}~\cite{brossard2020graph} & 6.1M & 29.790.30 \\
\hline
GIN-{\scriptsize VN}\cite{xu2018how} (fine-tune) &3.4M& 29.020.17 \\
\hline
Graphormer-{\scriptsize FLAG} &119.5M& \textbf{31.39}0.32  \\
\bottomrule
\end{tabular}
\end{minipage}
\begin{minipage}{0.65\linewidth}
\centering
\small
\caption{Results on MolHIV.}
\label{tab:hiv}
\begin{tabular}{c|c|c}
\toprule
  method   & \#param. & AUC (\%) \\
  \hline
GCN-{\scriptsize GraphNorm}~\cite{brossard2020graph,cai2020graphnorm} & 526K & 78.831.00 \\
PNA~\cite{corso2020principal} & 326K & 79.051.32 \\
PHC-GNN~\cite{le2021parameterized} & 111K & 79.341.16 \\
DeeperGCN-{\scriptsize FLAG}~\cite{li2020deepergcn} & 532K & 79.421.20\\
DGN~\cite{beaini2020directional} & 114K & 79.700.97 \\
\hline
GIN-{\scriptsize VN}\cite{xu2018how} (fine-tune) &3.3M& 77.801.82 \\
\hline
Graphormer-{\scriptsize FLAG} &47.0M& \textbf{80.51}0.53  \\
\bottomrule
\end{tabular}
\end{minipage}
\end{table}

\begin{table}[ht]
\small
\centering
\caption{Results on ZINC. }
\label{tab:zinc}
\begin{tabular}{c|c|c}
\toprule
method                & \#param.  & test MAE \\ \hline
GIN~\cite{xu2018how}  & 509,549   & 0.5260.051 \\
GraphSage~\cite{hamilton2017inductive}  & 505,341   & 0.3980.002 \\
GAT~\cite{velivckovic2018graph}  & 531,345 & 0.3840.007 \\
GCN~\cite{kipf2016semi}  & 505,079   & 0.3670.011	 \\
GatedGCN-PE~\cite{bresson2017residual} & 505,011   & 0.2140.006 \\
MPNN (sum)~\cite{gilmer2017neural}  & 480,805   & 0.1450.007 \\
PNA~\cite{corso2020principal}         & 387,155   & 0.1420.010 \\
\hline
GT~\cite{dwivedi2021generalization} & 588,929 & 0.2260.014 \\
SAN~\cite{Kreuzer2021rethinking} & 508, 577 & 0.1390.006 \\
\hline
Graphormer & 489,321 & \textbf{0.122}0.006  \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\small
\centering
\caption{
Ablation study results on PCQM4M-LSC dataset with different designs. }
\label{tab:ablation-table}
\begin{tabular}{cccccccc}
\toprule
\multicolumn{2}{c}{Node Relation Encoding} & \multirow{2}{*}{Centrality} & \multicolumn{3}{c}{Edge Encoding}  & \multirow{2}{*}{valid MAE} \\ \cline{1-2} \cline{4-6}
 Laplacian PE\cite{dwivedi2021generalization}&Spatial&&via node&via Aggr&via attn bias(Eq.\ref{eqn:attn-edge})&&\\ \hline
  -&-&-&-&-&-&0.2276\\ \hline
  \yes&-&-&-&-&-&0.1483\\ \hline
  -&\yes&-&-&-&-&0.1427\\ \hline
  -&\yes&\yes&-&-&-&0.1396\\ \hline
  -&\yes&\yes&\yes&-&-&0.1328\\ \hline
  -&\yes&\yes&-&\yes&-&0.1327\\ \hline
  -&\yes&\yes&-&-&\yes&0.1304\\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Results.} Table \ref{tab:pcba}, \ref{tab:hiv} and \ref{tab:zinc} summarize performance of Graphormer comparing with other GNNs on MolHIV, MolPCBA and ZINC datasets. Especially, GT~\cite{dwivedi2021generalization} and SAN~\cite{Kreuzer2021rethinking} in Table \ref{tab:zinc} are recently proposed Transformer-based GNN models. Graphormer consistently and significantly outperforms previous state-of-the-art GNNs on all three datasets by a large margin. Specially, except Graphormer, the other pre-trained GNNs do not achieve competitive performance, which is in line with previous literature~\cite{hu2020strategies}. In addition, we conduct more comparisons to fine-tuning the pre-trained GNNs, please refer to Appendix C.

\subsection{Ablation Studies}
We perform a series of ablation studies on the importance of designs in our proposed Graphormer, on PCQM4M-LSC dataset. The ablation results are included in Table \ref{tab:ablation-table}. To save the computation resources, the Transformer models in table \ref{tab:ablation-table} have 12 layers, and are trained for 100K iterations.

\paragraph{Node Relation Encoding.} We compare previously used positional encoding (PE) to our proposed spatial encoding, which both aim to encode the information of distinct node relation to Transformers. There are various PEs employed by previous Transformer-based GNNs, e.g., Weisfeiler-Lehman-PE (WL-PE)~\cite{zhang2020graph} and Laplacian PE~\cite{belkin2003laplacian,dwivedi2020benchmarking}. We report the performance for Laplacian PE since it performs well comparing to a series of PEs for Graph Transformer in previous literature~\cite{dwivedi2021generalization}. Transformer architecture with the spatial encoding outperforms the counterpart built on the positional encoding, which demonstrates the effectiveness of using spatial encoding to capture the node spatial information.


\paragraph{Centrality Encoding.} Transformer architecture with degree-based centrality encoding yields a large margin performance boost in comparison to those without centrality information. This indicates that the centrality encoding is indispensable to Transformer architecture for modeling graph data.

\paragraph{Edge Encoding.} We compare our proposed edge encoding (denoted as via attn bias) to two commonly used edge encodings described in Section \ref{sec:edge} to incorporate edge features into GNN, denoted as via node and via Aggr in Table \ref{tab:ablation-table}. From the table, the gap of performance is minor between the two conventional methods, but our proposed edge encoding performs significantly better, which indicates that edge encoding as attention bias is more effective for Transformer to capture spatial information on edges.





\section{Related Work}
In this section, we highlight the most recent works which attempt to develop standard Transformer architecture-based GNN or graph structural encoding, but spend less effort on elaborating the works by adapting attention mechanism to GNNs~\cite{li2019graph,yun2019graph,cai2020graph,hu2020heterogeneous,baek2021accurate,velivckovic2018graph,wang2020direct,zhang2020graph,shi2020masked}.



\subsection{Graph Transformer}

There are several works that study the performance of pure Transformer architectures (stacked by transformer layers) with modifications on graph representation tasks, which are more related to our Graphormer. For example,  several parts of the transformer layer are modified in ~\cite{rong2020self}, including an additional GNN employed in attention sub-layer to produce vectors of , , and , long-range residual connection, and two branches of FFN to produce node and edge representations separately. They pre-train their model on 10 million unlabelled molecules and achieve excellent results by fine-tuning on downstream tasks. Attention module is modified to a soft adjacency matrix in ~\cite{maziarka2020molecule} by directly adding the adjacency matrix and RDKit\footnote{\url{https://www.rdkit.org/}}-computed inter-atomic distance matrix to the attention probabilites. Very recently, Dwivedi \textit{et al.} \cite{dwivedi2021generalization} revisit a series of works for Transformer-based GNNs, and suggest that the attention mechanism in Transformers on graph data should only aggregate the information from neighborhood (i.e., using adjacent matrix as attention mask) to ensure graph sparsity, and propose to use Laplacian eigenvector as positional encoding. Their model GT surpasses baseline GNNs on graph representation task. A concurrent work~\cite{Kreuzer2021rethinking} propose a novel full Laplacian spectrum to learn the position of each node in a graph, and empirically shows better results than GT.

\subsection{Structural Encodings in GNNs}
\paragraph{Path and Distance in GNNs.} Information of path and distance is commonly used in GNNs. For example,  an attention-based aggregation is proposed in ~\cite{chen2019path} where the node features, edge features, one-hot feature of the distance and ring flag feature are concatenated to calculate the attention probabilites; similar to~\cite{chen2019path}, path-based attention is leveraged in~\cite{yangspagan} to model the influence between the center node and its higher-order neighbors; a distance-weighted aggregation scheme on graph is proposed in~\cite{you2019position}; it has been proved in~\cite{li2020distance} that  adopting distance encoding (i.e., one-hot feature of the distance as extra node attribute) could lead to a strictly more expressive power than the 1-WL test.

\paragraph{Positional Encoding in Transformer on Graph.} Several works introduce positional encoding (PE) to Transformer-based GNNs to help the model capture the node position information. For example, Graph-BERT~\cite{zhang2020graph} introduces three types of PE to embed the node position information to model, i.e., an absolute WL-PE which represents different nodes labeled by Weisfeiler-Lehman algorithm, an intimacy based PE and a hop based PE which are both variant to the sampled subgraphs. Absolute Laplacian PE is employed in ~\cite{dwivedi2021generalization} and empircal study shows that its performance surpasses the absolute WL-PE used in ~\cite{zhang2020graph}.

\paragraph{Edge Feature.} Except the conventionally used methods to encode edge feature, which are described in previous section, there are several attempts that exploit how to better encode edge features: an attention-based GNN layer is developed in~\cite{gong2019exploiting} to encode edge features, where the edge feature is weighted by the similarity of the features of its two nodes; edge feature has been encoded into the popular GIN~\cite{xu2018how} in~\cite{brossard2020graph}; in~\cite{dwivedi2021generalization}, the authors propose to project edge features to an embedding vector, then multiply it by attention coefficients, and send the result to an additional FFN sub-layer to produce edge representations; 


\section{Conclusion}\label{conclusion}
We have explored the direct application of Transformers to graph representation. With three novel graph structural encodings, the proposed Graphormer works surprisingly well on a wide range of popular benchmark datasets. While these initial results are encouraging, many challenges remain. For example, the quadratic complexity of the self-attention module restricts Graphormer's application on large graphs. Therefore, future development of efficient Graphormer is necessary. Performance improvement could be expected by leveraging domain knowledge-powered encodings on particular graph datasets. Finally, an applicable graph sampling strategy is desired for node representation extraction with Graphormer. We leave them for future works.

\section{Acknowledgement}
We would like to thank Mingqi Yang and Shanda Li for insightful discussions. 



\small

\bibliography{ref}
\bibliographystyle{plain}




\newpage
\appendix

\section{Proofs}\label{App:proofs}
\subsection{SPD can Be Used to Improve WL-Test}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{attachment/wl_failure_case.pdf}
    \caption{These two graphs cannot be distinguished by 1-WL-test. But the SPD sets, i.e., the SPD from each node to others, are different: The two types of nodes in the left graph have SPD sets  while the nodes in the right graph have SPD sets .}\label{fig:wl_fail}

\end{figure}

1-WL-test fails in many cases~\cite{NEURIPS2019_bb04af0f,li2020distance}, thus classic message passing GNNs also fail to distinguish many pairs of graphs. We show that SPD might help when 1-WL-test fails, for example, in Figure~\ref{fig:wl_fail} where 1-WL-test fails, the sets of SPD from all nodes to others successfully distinguish the two graphs.

\subsection{Proof of Fact~\ref{fact:recover_gnn}}
\paragraph{MEAN AGGREGATE.} We begin by showing that self-attention module with Spatial Encoding can represent MEAN aggregation. This is achieved by in Eq.~\eqref{eqn:rnpe}: 1) setting  if  and  otherwise where  is the SPD; 2) setting  and  to be the identity matrix. Then  gives the average of representations of the neighbors. 

\paragraph{SUM AGGREGATE.} The SUM aggregation can be realized by first perform MEAN aggregation and then multiply the node degrees. Specifically, the node degrees can be extracted from Centrality Encoding by an additional head and be concatenated to the representations after MEAN aggregation. Then the FFN module in Graphormer can represent the function of multiplying the degree to the dimensions of averaged representations by the universal approximation theorem of FFN.

\paragraph{MAX AGGREGATE.} Representing the MAX aggregation is harder than MEAN and SUM. For each dimension  of the representation vector, we need one head to select the maximal value over -th dimension in the neighbor by in Eq.~\eqref{eqn:rnpe}: 1) setting  if  and  otherwise where  is the SPD; 2) setting  which is the -th standard basis;  and the bias term (which is ignored in the previous description for simplicity) of  to be ; and , where  is the temperature that can be chosen to be large enough so that the softmax function can approximate hard max and  is the vector whose elements are all 1.

\paragraph{COMBINE.} The COMBINE step takes the result of AGGREGATE and the previous representation of current node as input. This can be achieved by the AGGREGATE operations described above together with an additional head which outputs the features of present nodes, i.e., in Eq.~\eqref{eqn:rnpe}: 1) setting  if  and  otherwise where  is the SPD; 2) setting  and  to be the identity matrix. Then the FFN module can approximate any COMBINE function by the universal approximation theorem of FFN.
\subsection{Proof of Fact~\ref{fact:readout}}
\paragraph{MEAN READOUT.} This can be proved by setting , the bias terms of  to be , and  to be the identity matrix where  should be much larger than the scale of  so that  dominates the Spatial Encoding term.


\section{Experiment Details}



\begin{table}[ht]
\caption{Statistics of the datasets.}
\centering\label{tab:datasets}
\begin{tabular}{cccccccc}
\toprule
Dataset     & Scale & \# Graphs & \# Nodes & \# Edges & Task Type  \\ \hline
PCQM4M-LSC  & Large & 3,803,453 & 53,814,542	& 55,399,880 & Regression  \\ \hline
OGBG-MolPCBA & Medium & 437,929   & 11,386,154& 12,305,805& Binary classification \\ \hline
OGBG-MolHIV & Small & 41,127	& 1,048,738 & 1,130,993 & Binary classification \\ \hline
ZINC (sub-set) & Small & 12,000  & 277,920 & 597,960 & Regression \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Details of Datasets}
We summarize the datasets used in this work in Table \ref{tab:datasets}. PCQM4m-LSC is a quantum chemistry graph-level prediction task in recent OGB Large-Scale Challenge, originally curated under the PubChemQC project ~\cite{nakata2017pubchemqc}. The task of PCQM4M-LSC is to predict DFT(density functional theory)-calculated HOMO-LUMO energy gap of molecules given their 2D molecular graphs, which is one of the most practically-relevant quantum chemical properties of molecule science. PCQM4M-LSC is unprecedentedly large in scale comparing to other labeled graph-level prediction datasets, which contains more than 3.8M graphs. Besides, we conduct experiments on two molecular graph datasets in popular OGB leaderboards, i.e., OGBG-MolPCBA and OGBG-MolHIV. They are two molecular property prediction datasets with different sizes. The pre-trained knowledge of molecular graph on PCQM4M-LSC could be easily leveraged on these two datasets. We adopt official scaffold split on three datasets following ~\cite{hu2021ogb,hu2020open}. In addition, we employ another popular leaderboard, i.e., benchmarking-gnn~\cite{dwivedi2020benchmarking}. We use the ZINC datasets, which is the most popular real-world molecular dataset to predict graph property regression for contrained solubility, an important chemical property for designing generative GNNs for molecules. Different from the scaffold spliting in OGB, uniform sampling is adopted in ZINC for data splitting.



\subsection{Details of Training Strategies}


\subsubsection{PCQM4M-LSC}

\begin{table*}[ht]
\centering 
\caption{Model Configurations and Hyper-parameters of Graphormer on PCQM4M-LSC. } \label{tab:pcq_details}
\begin{threeparttable}
\begin{tabular}{lcc}
\toprule
& Graphormer & Graphormer \\ \hline
\textbf{\#Layers} & 6 & 12  \\ 
\textbf{Hidden Dimension } & 512 & 768  \\ 
\textbf{FFN Inner-layer Dimension} & 512 & 768  \\ 
\textbf{\#Attention Heads} & 32 & 32  \\ 
\textbf{Hidden Dimension of Each Head} & 16 & 24  \\ 
\textbf{FFN Dropout} & 0.1 & 0.1  \\ 
\textbf{Attention Dropout} & 0.1 & 0.1 \\ 
\textbf{Embedding Dropout} & 0.0 & 0.0 \\ 
\textbf{Max Steps} & 1 & 1 \\
\textbf{Max Epochs} & 300 & 300 \\ 
\textbf{Peak Learning Rate} & 3e-4 & 2e-4 \\ 
\textbf{Batch Size} & 1024 & 1024 \\ 
\textbf{Warm-up Steps} & 60 & 60 \\ 
\textbf{Learning Rate Decay} & Linear & Linear  \\ 
\textbf{Adam } & 1e-8 & 1e-8 \\ 
\textbf{Adam (, )} &  (0.9, 0.999) & (0.9, 0.999) \\ 
\textbf{Gradient Clip Norm} &  5.0 & 5.0  \\ 
\textbf{Weight Decay} & 0.0 & 0.0  \\ 
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table*}

We report the detailed hyper-parameter settings used for training Graphormer in Table \ref{tab:pcq_details}. We reduce the FFN inner-layer dimension of  in ~\cite{vaswani2017attention} to , which does not appreciably hurt the performance but significantly save the parameters. The embedding dropout ratio is set to 0.1 by default in many previous Transformer works~\cite{devlin2019bert,liu2019roberta}. However, we empirically find that a small embedding dropout ratio (e.g., 0.1) would lead to an observable performance drop on validation set of PCQM4M-LSC. One possible reason is that the molecular graph is relative small (i.e., the median of \#atoms in each molecule is about 15), making graph property more sensitive to the embeddings of each node. Therefore, we set embedding dropout ratio to 0 on this dataset.



\subsubsection{OGBG-MolPCBA}


\begin{table*}[ht]
\centering 
\caption{Hyper-parameters for Graphormer on OGBG-MolPCBA, where the \textbf{text in bold} denotes the hyper-parameters we eventually use. } \label{tab:pcba_details}
\begin{threeparttable}
\begin{tabular}{lc}
\toprule
& Graphormer \\ \hline
\textbf{Max Epochs} & \{2, 5, \textbf{10}\} \\
\textbf{Peak Learning Rate} & \{2e-4, \textbf{3e-4}\}  \\ 
\textbf{Batch Size} & 256 \\ 
\textbf{Warm-up Ratio} & 0.06 \\ 
\textbf{Attention Dropout} & 0.3 \\ 
\textbf{} & \{1, 2,3,\textbf{4}\} \\ 
\textbf{} & 0.001 \\ 
\textbf{} & 0.001 \\ 
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table*}



\paragraph{Pre-training.} We first report the model configurations and hyper-parameters of the pre-trained Graphormer on PCQM4M-LSC. Empirically, we find that the performance on MolPCBA benefits from the large pre-training model size. Therefore, we train a deep Graphormer with 18 Transformer layers on PCQM4M-LSC. The hidden dimension and FFN inner-layer dimension are set to 1024. We set peak learning rate to 1e-4 for the deep Graphormer. Besides, we enlarge the attention dropout ratio from 0.1 to 0.3 in both pre-training and fine-tuning to prevent the model from over-fitting. The rest of hyper-parameters remain unchanged. The pre-trained Graphormer used for MolPCBA achieves a valid MAE of 0.1253 on PCQM4M-LSC, which is slightly worse than the reports in Table \ref{tab:pcq-table}. 

\paragraph{Fine-tuning.} Table \ref{tab:pcba_details} summarizes the hyper-parameters used for fine-tuning Graphormer on OGBG-MolPCBA. We conduct a grid search for several hyper-parameters to find the optimal configuration. The experimental results are reported by the mean of 10 independent runs with random seeds. We use FLAG~\cite{kong2020flag} with minor modifications for graph data augmentation. In particular, except the step size  and the number of steps , we also employ a projection step in ~\cite{zhu2020freelb} with maximum perturbation . The performance of Graphormer on MolPCBA is quite robust to the hyper-parameters of FLAG. The rest of hyper-parameters are the same with the pre-training model.


\subsubsection{OGBG-MolHIV}

\begin{table*}[h]
\centering 
\caption{Hyper-parameters for Graphormer on OGBG-MolHIV, where the \textbf{text in bold} denotes the hyper-parameters we eventually use. } \label{tab:hiv_details}
\begin{threeparttable}
\begin{tabular}{lc}
\toprule
& Graphormer \\ \hline
\textbf{Max Epochs} & 8 \\
\textbf{Peak Learning Rate} & 2e-4  \\ 
\textbf{Batch Size} & 128 \\ 
\textbf{Warm-up Ratio} & 0.06 \\ 
\textbf{Dropout} & 0.1 \\ 
\textbf{Attention Dropout} & 0.1 \\ 
\textbf{} & \{1,\textbf{2},3,4\} \\ 
\textbf{} & \{0.001, 0.01, 0.1, \textbf{0.2}\} \\ 
\textbf{} & \{\textbf{0}, 0.001, 0.01, 0.1\} \\ 
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table*}


\paragraph{Pre-training.} We use the Graphormer reported in Table \ref{tab:pcq-table} as the pre-trained model for OGBG-MolHIV, where the pre-training hyper-parameters are summarized in Table \ref{tab:pcq_details}.

\paragraph{Fine-tuning.} The hyper-parameters for fine-tuning Graphormer on OGBG-MolHIV are presented in Table \ref{tab:hiv_details}. Empirically, we find that the different choices of hyper-parameters of FLAG (i.e., step size , number of steps , and maximum perturbation ) would greatly affect the performance of Graphormer on OGBG-MolHiv. Therefore, we spend more effort to conduct grid search for hyper-parameters of FLAG. We report the best hyper-parameters by the mean of 10 independent runs with random seeds.


\subsubsection{ZINC}

To keep the total parameters of Graphormer less than 500K per the request from benchmarking-GNN leaderboard~\cite{dwivedi2020benchmarking}, we train a slim 12-layer Graphormer with hidden dimension of 80, which is called Graphormer\xspace in Table \ref{tab:zinc}, and has about 489K learnable parameters. The number of attention heads is set to 8. Table \ref{tab:zinc_details} summarizes the detailed hyper-parameters on ZINC. We train 400K steps on this dataset, and employ a weight decay of 0.01.


\begin{table*}[ht]
\centering 
\caption{Model Configurations and Hyper-parameters on ZINC(sub-set). } \label{tab:zinc_details}
\begin{threeparttable}
\begin{tabular}{lc}
\toprule
& Graphormer \\ \hline
\textbf{\#Layers} & 12  \\ 
\textbf{Hidden Dimension} & 80  \\ 
\textbf{FFN Inner-Layer Hidden Dimension} & 80  \\ 
\textbf{\#Attention Heads} & 8  \\ 
\textbf{Hidden Dimension of Each Head} & 10  \\ 
\textbf{FFN Dropout} & 0.1  \\ 
\textbf{Attention Dropout} & 0.1 \\ 
\textbf{Embedding Dropout} & 0.0 \\ 
\textbf{Max Steps} & 400 \\
\textbf{Max Epochs} & 10  \\ 
\textbf{Peak Learning Rate} & 2e-4  \\ 
\textbf{Batch Size} & 256 \\ 
\textbf{Warm-up Steps} & 40 \\ 
\textbf{Learning Rate Decay} & Linear \\ 
\textbf{Adam } & 1e-8 \\ 
\textbf{Adam (, )} &  (0.9, 0.999) \\ 
\textbf{Gradient Clip Norm} &  5.0  \\ 
\textbf{Weight Decay} & 0.01 \\ 
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table*}

\subsection{Details of Hyper-parameters for Baseline Methods}
In this section, we present the details of our re-implementation of the baseline methods.

\subsubsection{PCQM4M-LSC}

The official Github repository of OGB-LSC\footnote{\url{https://github.com/snap-stanford/ogb/tree/master/examples/lsc/pcqm4m}} provides hyper-parameters and codes to reproduce the results on leaderboard. These hyper-parameters work well on almost all popular GNN variants, except the DeeperGCN-{\scriptsize VN}, which results in a training divergence. Therefore, for DeeperGCN-{\scriptsize VN}, we follow the official hyper-parameter setting\footnote{\url{https://github.com/lightaime/deep_gcns_torch/tree/master/examples/ogb/ogbg_mol\#train}} provided by the authors~\cite{li2020deepergcn}. For a fair comparison to Graphormer, we train a 12-layer DeeperGCN. The hidden dimension is set to 600. The batch size is set to 256. The learning rate is set to 1e-3, and a step learning rate scheduler is employed with the decaying step size and the decaying factor  as 30 epochs and 0.25. The model is trained for 100 epochs.

The default dimension of laplacian PE of GT~\cite{dwivedi2021generalization} is set to 8. However, it will cause 2.91\% small molecules (less than 8 atoms) to be filtered out. Therefore, for GT and GT-{\scriptsize Wide}, we set the dimension of laplacian PE to 4, which results in only 0.08\% filtering out. We adopt the default hyper-parameter settings described in ~\cite{dwivedi2021generalization}, except that we decrease the learning rate to 1e-4, which leads to a better convergence on PCQM4M-LSC.

\begin{table*}[t]
\centering 
\caption{Hyper-parameters for fine-tuning GROVER on MolHIV and MolPCBA. } \label{tab:grover_details}
\begin{threeparttable}
\begin{tabular}{lcc}
\toprule
& GROVER & GROVER \\ \hline
\textbf{Dropout} & \{0.1, 0.5\} & \{0.1, 0.5\} \\ 
\textbf{Max Epochs} & \{10, 30, 50\} & \{10, 30\} \\ 
\textbf{Learning Rate} & \{5e-5, 1e-4, 5e-4, 1e-3\} & \{5e-5, 1e-4, 5e-4, 1e-3\} \\ 
\textbf{Batch Size} & \{64, 128\} & \{64, 128\} \\ 
\textbf{Initial Learning Rate} & 1e-7 & 1e-7 \\
\textbf{End Learning Rate} & 1e-9 & 1e-9 \\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table*}


\begin{table}[t]
\centering
\small
\caption{Comparison to pre-trained Transformer-based GNN on MolHIV. * \ indicates that additional features for molecule are used.}\label{tab:hiv-appendix}
\begin{tabular}{c|c|c}
\toprule
  method   & \#param. & AUC (\%) \\
\hline
Morgan Finger Prints + Random Forest*& 230K & \textbf{80.60}0.10 \\
\hline
GROVER*\cite{rong2020self} &48.8M & 79.330.09 \\
GROVER*\cite{rong2020self} &107.7M & 80.320.14 \\
\hline
Graphormer-{\scriptsize FLAG} &47.0M& 80.510.53  \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[t]
\centering
\small
\caption{Comparison to pre-trained Transformer-based GNN on MolPCBA. * \ indicates that additional features for molecule are used.}\label{tab:pcba-appendix}
\begin{tabular}{c|c|c}
\toprule
  method   & \#param. & AP (\%) \\

\hline
GROVER*\cite{rong2020self} &48.8M & 16.770.36 \\
GROVER*\cite{rong2020self} &107.7M & 13.050.18 \\
\hline
Graphormer-{\scriptsize FLAG} &47.0M& \textbf{31.39}0.32  \\
\bottomrule
\end{tabular}
\end{table}




\subsubsection{OGBG-MolPCBA}
To fine-tune the pre-trained GIN-{\scriptsize VN} on MolPCBA, we follow the hyper-parameter settings provided in the original OGB paper~\cite{hu2020open}. To be more concrete, we load the pre-trained checkpoint reported in Table \ref{tab:pcq-table} and fine-tune it on OGBG-MolPCBA dataset. We use the grid search on the hyper-parameters for better fine-tuning performance. In particular, the learning rate is selected from ; the dropout ratio is selected from ; the batch size is selected from . 

\subsubsection{OGBG-MolHIV}
Similarly, we fine-tune the pre-trained GIN-{\scriptsize VN} on MolHIV by following the hyper-parameter settings provided in the original OGB paper~\cite{hu2020open}. We also conduct the grid search to look for optimal hyper-parameters. The ranges for each hyper-parameter of grid search are the same as the previous subsection.



\section{More Experiments}


As described in the related work, GROVER is a Transformer-based GNN, which has 100 million parameters and pre-trained on 10 million unlabelled molecules using 250 Nvidia V100 GPUs. In this section, we report the fine-tuning scores of GROVER on MolHIV and MolPCBA, and compare with proposed Graphormer.

We download the pre-trained GROVER models from its official Github webpage\footnote{\url{https://github.com/tencent-ailab/grover}}, follow the official instructions\footnote{\url{https://github.com/tencent-ailab/grover/blob/main/README.md\#finetuning-with-existing-data}} and fine-tune the provided pre-trained checkpoints with careful search of hyper-parameters (in Table \ref{tab:grover_details}).  We find that GROVER could achieve competitive performance on MolHIV only if employing additional molecular features, i.e., morgan molecular finger prints and 2D features\footnote{\url{https://github.com/tencent-ailab/grover\# optional-molecular-feature-extraction-1}}. Therefore, we report the scores of GROVER by taking these two additional molecular features. Please note that, from the leaderboard\footnote{\url{https://ogb.stanford.edu/docs/leader_graphprop/}}, we can know such additional molecular features are very effective on MolHIV dataset.


Table \ref{tab:hiv-appendix} and \ref{tab:pcba-appendix} summarize the performance of GROVER and GROVER comparing with Graphormer on MolHIV and MolPCBA. From the tables, we observe that Graphormer could consistently outperform GROVER even without any additional molecular features.



\section{Discussion \& Future Work}\label{sec:discuss}

\paragraph{Complexity.} Similar to regular Transformer, the attention mechanism in Graphormer scales quadratically with the number of nodes  in the input graph, which may be prohibitively expensive for large  and precludes its usage in settings with limited computational resources. Recently, many solutions have been proposed to address this problem in Transformer~\cite{ke2020rethinking,wang2020linformer,ying2021lazyformer,luo2021stable}. This issue would be greatly benefit from the future development of efficient Graphormer.

\paragraph{Choice of centrality and .} In Graphormer, there are multiple choices for the network centrality and the spatial encoding function . For example, one can leverage the  distance in 3D structure between two atoms in a molecule. In this paper, we mainly evaluate general centrality and distance metric in graph theory, i.e., the degree centrality and the shortest path. Performance improvement could be expected by leveraging domain knowledge powered encodings on particular graph dataset.

\paragraph{Node Representation.} There is a wide range of node representation tasks on graph structured data, such as finance, social network, and temporal prediction. Graphormer could be naturally used for node representation extraction with an applicable graph sampling strategy. We leave it for future work. 

\end{document}
