\documentclass[runningheads,letter]{llncs}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{url}
\urldef{\mailsa}\path|yodaiken@fsmlabs.com|
\urldef{\mailsc}\path|@fsmlabs.com|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\seq}[1]{\langle#1\rangle }
\newcommand{\ess}{\Lambda}
\newcommand{\union}{\bigcup}
\newcommand{\Ma}{\mathcal{M}}
\newcommand{\Aprod}{\mathcal{A}}
\newcommand{\Fprod}{\mathcal{F}}
\newcommand{\st}{\mathit{start}}
\newcommand{\conc}{\circ}
\newcommand{\Bfunc}{\left\{\begin{array}{ll}}
\newcommand{\Efunc}{\end{array}\right.}
\newcommand{\Beq}{\begin{eqnarray}}
\newcommand{\Eeq}{\end{eqnarray}}
\newcommand*{\longhookrightarrow}{\ensuremath{\lhook\joinrel\relbar\joinrel\rightarrow}}
\newcommand*{\xLongleftarrow}{\ensuremath{\Leftarrow\joinrel\Relbar\joinrel\Relbar\joinrel\Relbar\joinrel\Relbar\joinrel\Relbar\joinrel\Relbar\joinrel\Relbar\joinrel\Relbar}}

\newtheorem{thm}{Theorem}
\newtheorem{dfn}{Definition}[section]
\newtheorem{lem}{Lemma}[section]
\newcommand{\Sample}{\mathcal{S}}
\newcommand{\Partial}{\mathcal{P}}
\newcommand{\Impli}{\mathcal{I}}
\begin{document}

\mainmatter  

\title{State machine models of timing and circuit design}

\titlerunning{State machine solutions}

\author{Victor Yodaiken\thanks{Reformatted and edited Jan-18-10 }
}
\authorrunning{Yodaiken Jan-19-10}


\institute{FSMLabs Inc. 
\mailsa\\
\url{http://www.yodaiken.com}}



\toctitle{State machine solutions}
\tocauthor{VY}
\maketitle


\begin{abstract}
This paper illustrates a technique for specifying
the detailed timing, logical operation, and compositional circuit design
of digital circuits in terms of ordinary state machines with output
(transducers). The method is illustrated here with specifications
of gates, latches, and other simple circuits and via the construction of
devices starting with a SR latch built
from gates and then moving on to more complex devices.
Circuit timing and transients are
treated in some detail.
The method is based on ``classical" automata and recursive functions on strings.
No formal methods, extended state machines,
or process algebras are involved but a reference is made
to potential applications of the Krohn-Rhodes theorem and other group/monoid
based algebraic techniques. 
\keywords{ transducer, Moore machine, primitive recursion, composition, parallel}
\end{abstract}


\section{Introduction}

Both the time sensitive behavior and circuit architecture of 
digital circuits can be effectively modelled as ordinary state machines using
recursive functions for specifications. This note starts with the example of
logic gates and latches constructed from cross-coupled gates, where timing
and transient signals must be taken into account. Section \ref{sec:back} makes
the case that the methods used in section \ref{sec:example1} actually are
equivalent to ordinary automata. Section \ref{sec:example2} carries us
into proofs and more
examples from simple digital circuit design. The focus in this paper is on the by-hand 
methods of specification and verification, but the methods described here seem
to be well suited to automation. One of the goals of this work is to show that 
automata do not need to be extended or enhanced to capture 
properties of complex circuits. A final section discusses some possible implications involving semigroups.


\section{Basics \label{sec:example1}}
\subsection{Behavior}
Behavior should encompass
the possibility of transient outputs and inputs, conditions that leave
output undetermined, and ``don't care" inputs and outputs.
The basic idea here is to model circuits as state
machines with output, possibly state machines constructed as products of
simpler state machines. The recursive function technique permits convenient specification of
enormous state machines, state machines with parameters, and quite arbitrary
interconnect. 

A state machine with output (a transducer) $M$ can be treated as a map from sequences
of inputs to output values with $M(w)$ corresponding to the output of $M$
in the state reached by following the sequence $w$ from $M$'s initial state. If we put $M(\ess)=x_0$ where $\ess$ is the empty sequence, we have defined
the initial state output of $M$ --- something that is usually unspecified
in state machines modelling circuits. If we then put $M(wa)= f(M(w),a)$ where
$wa$ is the sequence obtained by appending input $a$ to sequence $w$, we have
totally determined the input/output behavior of $M$.

For state machines
modelling circuits, inputs can be ``samples" of the input signals applied to
the input pins during the shortest interval of interest (time units may be
picoseconds or microseconds or femtoseconds, depending on the devices being
studied). A sample assigns signal levels to pins. To
start, we can suppose signal levels are limited to $\set{0,1}$. Samples are then
maps $InputPins\to \set{0,1}$ where the set of input pins depends on the device
in question.  Then $Time(w)$ defined by $Time(\ess)=0$ and
$Time(wa)=1+Time(w)$ is a (not finite) state machine that simply counts the
number of time intervals that passed since the initial state. But $High(w,p)$
defined by $High(\ess,p)=0$ and $High(wa,p) = (High(w,p)+1)* a(p)$ tells us how
long pin $p$ has been kept high in consecutive preceding time intervals.

 Say that
$G$ is an OR gate with input pins $P$ and propagation delay $t$ only if 
$High(w,p)\geq t$ for some $p\in P$ implies that $G(w)=1$.  
There are an infinite number of state machines that satisfy this constraint --- corresponding
to the process variability of actual gates.

Instead of defining $Low$ to correspond to $High$, define a single function to see
how long a pin has been \emph{held} at a level.
\begin{eqnarray}
\mbox{For }b\in \set{0,1}\mbox{ and pin }p\nonumber \\
H(\ess,p,b)=0\\
H(wa,p,b)=\Bfunc  H(w,p,b)+1&\mbox{if }a(p)=b\\
                    0&\mbox{otherwise.}\Efunc \end{eqnarray}

Or more compactly $H(wa,p,b) = (H(w,p,b)+1)*(1 - |a(p)-b|)$.

A NAND gate can be specified as follows.
\begin{dfn} $N$ is a NAND-gate with propagation delay $t>0$ on pins $P$ only if
\begin{eqnarray}
\mbox{Whenever }H(w,p,0)\geq t \mbox{ for any }p\in P\mbox{ then }N(w)=1\label{cons:nand1}\\
\mbox{and }\nonumber \\
\mbox{Whenever }H(w,p,1)\geq t \mbox{ for all }p\in P\mbox{ then }N(w)=0\label{cons:nand2}
\end{eqnarray}
\end{dfn}

The idea here is to minimally specify what we can expect from a gate and not constrain 
for example transient behavior unless we have some reason to be able to specify in more detail.

Given a function $f$, let's have $\#f$ count how long the output of $f$ has been stable.
\begin{eqnarray}
\#f(\ess)=0\\
\#f(wa)=\Bfunc  \#f(w)+1&\mbox{if }f(wa)=f(w)\\
                    0&\mbox{otherwise.}\Efunc \label{pound}\end{eqnarray}

So it's useful to note that if $N$ meets the requirements for a NAND-gate with delay $t$
then holding an input pin low for $t+k$ time units causes output to be kept stable for at least
$k$ time units. This is a pretty obvious fact, but let's prove it just to show a style of recursion based
proof that seems eminently automatable.
\begin{eqnarray}
\mbox{For any }k\geq 0[ \#N(w) \geq H(w,p,0) - (t+k)]\label{lemma1}
\end{eqnarray}
Note that $\#N(w)\geq 0$ by definition. If $H(w,p,0)\leq t$ then $H(w,p,0)-(t+k) \leq 0$ so there is
nothing to prove. But if $H(w,p,0)>t$ then 
the NAND gate constraints require that $N(w)=1$ and the rest follows by induction on $w$.
For $w=\ess$ there is nothing to prove since $H(\ess,p,0)=0$ by definition.
Suppose that $H(wa,p,0)\geq t$. If $H(wa,p,0)=t$ then $H(wa,p,0)-(t+k)=0$ and there is still
nothing to prove since $\#N(w)\geq 0$ by definition.
If $H(wa,p,0)>t$ then it must be that $H(w,p,0)\geq t$ since by construction $H(wa,p,0)-H(w,p,0) \leq 1$. Thus
$N(w)=1$ and $N(wa)=1$ so $\#N(wa) = \#N(w)+1$ and if $\#N(w)\geq H(w,p,0)$ as assumed by recursion, the inequality must still
hold in the $wa$ state.

I'll postpone multi-output circuits to section \ref{sec:example2}.
\subsection{Composition}



Now consider how two NAND-gates could be cross-coupled to create a latch. In 
this case, the inputs to the composite circuit \emph{induce} inputs for the
two component gates. Suppose that $N_1$ and $N_2$ are solutions to constraints
\ref{cons:nand1} and \ref{cons:nand2} with input pins $\set{1,2}$.
We want to construct some $L$ that
contains copies of $N_1$ and $N_2$ and that accepts input samples that are maps
$\set{set,reset}\to \set{0,1}$. When an input $a$ is applied to $L$ an input $c_1$ will
be generated for the copy of $N_1$ and an input $c_2$ for the copy of $N_2$. Input $c_1(1)=a(reset)$
and input $c_2(1)=a(set)$ as the two inputs are connected directly to the input pins of the components.
But $c_1(2)$ will be set to the output of $N_2$ in the current state and $c_2(2)$ to the output of $N_1$
in the current state.
\begin{center}
\fbox{\begin{minipage}{5in}
\[\begin{array}{rcl}\mbox{Input:}w\rightarrow &\left(\begin{array}{l}
\mbox{Input:}u_1\rightarrow \mbox{Machine:}N_1\rightarrow \mbox{Output:}x_1\\
\mbox{Input:}u_2\rightarrow \mbox{Machine:}N_2\rightarrow \mbox{Output:}x_2
\end{array}\right)\rightarrow &\rightarrow \mbox{Output:}(x_1,x_2)\\
\Uparrow\qquad& &\qquad \Downarrow\\
\Leftarrow&\xLongleftarrow\joinrel\Relbar\mbox{ feedback }\xLongleftarrow&\Longleftarrow\joinrel\Relbar\end{array}\]
\end{minipage}
}
\end{center}

Define $L(w) = (N_1(u_1), N_2(u_2))$ 
where $u_1$ and $u_2$ are the induced input sequences that are themselves
functions of $L$ and $w$. We need to define a mapping $w\mapsto (u_1,u_2)$ and
we do so recursively. Note that when $w=\ess$ we must have $u_1=u_2=\ess$ so that
the components do not see any input until the composite system sees input.

\begin{dfn} $L$ is a constructed RS-Latch with base propagation delay $t$
on pins $\set{r,s}$ only if
\begin{eqnarray}
L(w) = (N_1(u_1), N_2(u_2))\\
\mbox{where each }N_i\mbox{ is a NAND-gate with delay }t\mbox{ on pins}\set{1,2}\\
w\mapsto (u_1,u_2)\\
\mbox{ and when }w=\ess\mbox{ then }w\mapsto (\ess,\ess)\\
\mbox{ and when }w\mapsto (u_1,u_2)\mbox{ then }wa\mapsto (u_1c_1,u_2c_2)\\
\mbox{ where }c_1(1)=a(reset), c_1(2)= N_2(u_2), c_2(1)=a(set), c_2(2)= N_1(u_1)
\end{eqnarray}
\end{dfn}

Define $C(w) = N_1(u_1)$ to extract the $q$ output from the latch.

\subsection{Abstraction}
Stepping back from the construction, consider an abstract behavioral specification of an RS-latch.

\begin{eqnarray}
latched(\ess,b,t_{latch}) = 0\\
latched(wa,b,t_{latch}) \\
=\Bfunc
	1&\mbox{if }b=1\mbox{ and }H(wa,reset,0)\geq t_{latch}\mbox{ and } H(wa,set,1)\geq t_{latch}\\
	&\mbox{or if }b=0\mbox{ and }H(wa,reset,1)\geq t_{latch}\mbox{ and } H(wa,set,0)\geq t_{latch}\\
&\mbox{or if }a(set)=a(reset)=1\mbox{ and }latched(w,b,t_{latch})>0\\	
	0&\mbox{otherwise}\Efunc\nonumber
\end{eqnarray}

We want to say $C$ is a SR-latch with delay $t_{latch}$ if and only if
whenever $latched(w,b,t_{latch})=1$ we must have $C(w)=b$.
Without proof, if $L$ is constructed from NAND-gates of delay $t$ as specified above, and $C(w)=N_1(u_1)$ then
$C$ is a RS-latch with delay $3t+2$. The $+2$ is an artifact of the model, which imposes a single
time unit delay for a signal to propagate between the output and input of any connected components\footnote{ 
If that requirement is a problem the model can be adjusted. For example, we could require that for any circuit element $E$, $E(wa)=E(wc)$ for any
samples $a$ and $c$ --- that is, there is at least one unit of delay between
the application of a signal and a response. A sensible limitation in any case.
Because the output does not depend on input, we can calculate the output using a dummy input.
So for example make $c$ be any appropriate sample and let, $wa\mapsto (u_1c_1,u_2c_2)$ where $c_2(2)=N_1(u_1c)$
--- eliminating the interconnect delay.}.
Of course, this is not a surprising fact,
since engineers have been using latches since the 1950s and, by all indications, they work as intended.

Just for fun, consider a NAND-gate constructed from 3 gates. Suppose $N_1$ and 
$N_2$ and $N_3$ are all NAND-gates with delay $t$ on pins $\set{1,2,3}$ and 
we want to build a NAND gate with input pins $\set{1,2,3,4,5,6,7}$ from them.
Put $N'(w) =  (N_1(u_1), N_2(u_2), N_3(u_3))$ and let $N(w)= N_3(u_3)$. 
Define $w\mapsto (\ess,\ess,\ess)$ when $w=\ess$. Then if
$w\mapsto (u_1,u_2,u_3)$ define the mapping $wa\mapsto (u_1c_1,u_2c_2,u_3c_3)$
where 
\begin{eqnarray}
c_1(1)=a(1), c_1(2)=a(2), c_1(3)=a(3)\\
c_2(1)=a(4), c_2(2)=a(5), c_2(3)=a(6)\\
c_3(1)=a(7), c_3(2)= N_1(u_1), c_3(3)= N_2(u_2)\end{eqnarray}

The glaring difference between $N'$ and the constructed $L$ is that the
mapping from $w$ to the $u_i$ involves recursive feedback for $L$ but not
for $N'$.

\section{Background\label{sec:back}}

\subsection{Representations}
We're representing state machines (transducers) with functions on strings.
\begin{center}
\fbox{
\begin{minipage}{4in}
Correspondence between a transducer $M$ and a string function $f$.
\[\begin{array}{c}
\mbox{Input:}w\Rightarrow \mbox{Machine:}M\Rightarrow \mbox{Output:}x\\
f(w)=x
\end{array} \]
\end{minipage}
}
\end{center}

A Moore machine or transducer is usually given by a 6-tuple
\[M=(A,X, S,\st,\delta,\gamma)\]
where $A$ is the alphabet, $X$ is a set of outputs, $S$ is a set of states, $\st\in S$ is the initial
state, $\delta: S\times A\to S$ is the transition function
and $\gamma:S\to X$is the output function.
While it is usual to limit Moore machines to finite state sets, and such
finite state machines are sufficient to represent any constructable digital
system, it's convenient to have infinite state machines available for specifications. For example, a hypothetical infinite counter may be useful in describing the periodic
behavior of a finite state device. 

Given $M$, use primitive recursion on sequences to 
extend  the transition function $\delta$ to $A^*$ by:
\begin{eqnarray}
\delta^*(s,\ess)=s \mbox{ and } \delta^*(s,wa)=\delta(\delta^*(s,w),a).
\end{eqnarray}

So $\gamma(\delta^*(\st,w))$ is
the output of $M$ in the state reached by following $w$ from $M$'s initial
state.
Call $f_M(w) =\gamma(\delta^*(\st,w))$ the 
\emph{representing function} of $M$. A representing function is ``finite" if
and only if it represents a finite state machine. 

The transformation from string function to transducer is also simple.
Given $f:A^*\to X$ define  $f_w(u) = f(w\circ u)$ where $\circ$ indicates string
concatenation. Note that it is possible for $w\neq u$ but $f_w=f_u$ meaning
that  $f(w\conc z)=f(u\conc z)$ for all
$z\in A^*$.  Intuitively $f_w = f_u$ when both lead to the same state. Let $S_f=\set{f_w:w\in A^*}.$.
Say $f$ is finite if and only if $S_f$ is finite. Define
$\delta_f(f_w,a)=f_{wa}$ and define $\gamma(f_w)= f_w(\ess) = f(w)$. Then
with $\st_f = f_\ess$ we have a Moore machine
\[\Ma(f)=\set{S_f,\st_f,\delta_f,\gamma_f}\]
and, by construction $f$ is the representing function for $\Ma(f)$. See section \ref{sec:conjecture} for a short
discussion of the next step of abstraction: the monoid induced by the state machine.

Any $M_2$ that has $f$ as a representing function can differ
from $M_1=\Ma(f)$ only in names of states and by including unreachable and/or
duplicative states. That is, there may be some  $w$ so that
$\delta_1^*(\st_1,w) \neq \delta_2^*(\st_2,w)$ but since $f_w= f_w$ it must
be the case that the states are identical in output and in the output of any
states reachable from them. If we are using Moore machines to
represent the behavior of digital systems, these differences are not particularly interesting and we can treat $\Ma(f)$ as \emph{the} Moore machine represented
by $f$.

\subsection{Products}

Gecseg\cite{Gecseg}  describes a general automata product suitable to composition of digital circuits that is used in a simplified way here and in a more
general way in \cite{yodaikenprpresent}.

Suppose we have a collection of (not necessarily distinct) Moore machines
$M_i=(A_i,X_i,S_i,\st_i,\delta_i,\lambda_i)$ for $(0 < i \leq n)$
that are to be connected to construct a new machine with alphabet $A$ 
using a connection map $g$.
The intuition is that when an input $a$ is applied to the system,
the connection map computes an input for
$M_i$ from the input $a$ and the outputs of the factors (\emph{feedback}).

\begin{dfn}{\rm
\textbf{General product of automata for digital circuits}\\
Given $M_i=(A_i,X_i,S_i,\st_i,\delta_i,\gamma_i)$ and $h$ and 
$g$ where $g(i,a,\vec{x})\in A_i$, define the Moore machine:
$M = \Aprod_{i=1}^n [ M_i, g,h] = (A,X,S,\st, \delta,\gamma)$\\
\begin{itemize}
\item $S = \set{(s_1 \ldots,s_n): s_i\in X_i}$ and $\st = (\st_1 \ldots ,\st_n)$
\item $X=\set{h(x_1 \ldots  ,x_n): x_i\in X_i}$ and $\gamma((s_1 \ldots , s_n))= h(\gamma_1(s_1)\ldots ,\gamma_n(s_n) )$.
\item $\delta((s_1 \ldots , s_n),a) = (\delta_1(s_1,g(1,a,\gamma(s))) \ldots , \delta_n(s_n,g(n,a,\gamma(s))))$.
\end{itemize} }
\end{dfn}

I'll state without proof a theorem proved elsewhere that should be reasonably
obvious\cite{yodaikenprpresent}.
\begin{thm}\label{thm:main}{\rm
\textbf{If} each $f_i$ represents $M_i$
and $f(w) = h(f_1(u_1)\dots,f_n(u_n))$\\
and when $w=\ess$, $u_i=\ess$\\
and if when $w\mapsto (\dots u_i \dots)$, then $wa\mapsto (\dots u_ic_i \dots)$ where $c_i = g(i,a,f(w))$.\\
and $M= \Aprod_{i=1}^n [M_i, h,g]$
\textbf{then} $f$ represents $M$
}\end{thm}

\section{More examples\label{sec:example2}}

Let's step up the complexity of the examples to  multi-output-pin circuits.
In this case, we just add another parameter to select which output pin is
being referenced.
A single bit adder might output have output pins
$\set{sum,carry_{out}}\to \set{0,1}$ and accept
input samples $\set{v1,v2,carry_{in}}$. 
\begin{eqnarray}
D \mbox{ is a delay }t\mbox{ adder iff and only if }\nonumber\\
H(w,carry_{in},b_0)\geq t\mbox{ and }H(w,v1,b_1)\geq t\mbox{ and }H(w,v2,b_2)\geq t\rightarrow\nonumber\\
D(w,carry_{out})= \lfloor(b_0+b_1+b_2)/2\rfloor\\
\mbox{and }
D(w,sum)= (b_0+b_1+b_2)\bmod 2 \nonumber
\end{eqnarray}

An $n$ bit  adder then has inputs that are maps from 
$\set{ carry_{in}, v1_1\dots v1_n,v2_1\dots v2_n}$ to $\set{0,1}$.  The output pins are $\set{r_1,\dots r_n,c_{out}}$. In contrast to a gate, there are never
``do not care" inputs. Define $Last(wa)=a$. Define $Stable(\ess)=0$
and 
\[Stable(wa) = \Bfunc
		1+Stable(w)&\mbox{if }a=Last(w)\\
		0&\mbox{otherwise}.\Efunc \]

Let
$LastIn1(w)=(Last(w)(carry_{in}),Last(w)(v1_1),\dots Last(w)(v1_n))$
and $LastIn2(w)=(Last(w)(v2_1),\dots Last(w)(v2_n))$
Given a binary $n$-vector $\vec{b}=(b_1,\dots b_n)$, define projection $\vec{b}[i] = b_i$ and
then $unsigned(\vec{b}) = \Sigma_{i=1}^n \vec{b}[i]*2^{i-1}$ assuming both that the lower indexed bits
are the lower order bits and that the length of the vector is known.
What we want of an $n$-bit adder $V$ is that
when inputs have been stable for long enough $Stable(w) >t_{adder}$.
\[(*)\quad unsigned(LastIn1(w)+unsigned(LastIn2(w)) = \Sigma_{i=1}^n V(w,r_i)2^{i-1} + 2^nV(w,carry_{out})\]

We can construct a ripple carry adder as follows.
\begin{eqnarray}
V \mbox{ is a constructed ripple carry adder with }n \mbox{ bits if and only if}\nonumber\\
V(w)=(D_1(u_1,sum),\dots D_n(u_n,sum),D_n(u_n,carry_{out}))\\
\mbox{ where each }D_i\mbox{ is a delay }t\mbox{ single bit adder}\\
\mbox{and if }w=\ess\mbox{ then }w\mapsto (\ess, \dots \ess)\\
\mbox{and if }w\mapsto (u_1\dots,u_n)\mbox{ then }\nonumber\\
(wa)\mapsto (u_1c_1\dots ,u_nc_n)\mbox{ where }\nonumber\\
c_1(carry_{in})=a(carry_{in}), c_{i+1}(carry_{in})=D_i(u_i,carry_{out})\\
c_i(v1) = a(v1_i), c_i(v2)=a(v2_i)\end{eqnarray}


If $t$ is the delay for the component adders, then 
we want to require that $Stable(n*t+n)$ should imply $(*)$.
I'm just going to present proof sketches since the idea should be clear and
the main claim here is that these proofs should be reasonably easy to 
automate. I want to particularly show how component properties can be pushed
up the composition ladder because if $P(f(w))$ tells us that $f(w)$ has a 
property $P$ and we can show it for all $w$, then $P(f(u_i))$ must be true.
However, there are additional constraints on $u_i$ because of the definition
of the map from $w$ to $u_i$.

Proofs are usually by induction: one strings and on the component index.
First, there is a lemma
for how long the output of an adder is stable when the input is held stable.
Recall the definition of $\#f$ above in equation $\ref{pound}$ and 
consider $\#D(w,sum)$. 

Ignoring the composite system, say that $t$ delay adder $D$ must have the 
property that:
\begin{equation}
Stable(w)\geq t+k \nonumber\\
\rightarrow \#D(w,sum) \geq k\mbox{ and }\#D(w,carry_{out})\geq k\label{lemma2}
\end{equation}

Note this $w$ ranges over input sequences for single-bit adders - we'll turn to
$w$ for the n-bit adder below.
Proof: Let $w=\ess$ and there is nothing to prove since $\#D(\ess)=0$
and $Stable(\ess)=0$. Suppose the lemma true for $w$ and consider $wa$.
Suppose $\#D(w)=j$ and
consider $\#D(wa)$ where we extend the $\#f$ notation to allow for pin arguments.
If $k=0$ then
all we have to prove is that $\#D(w)\geq 0$ which is trivially true. So 
consider $k>0$ and $Stable(wa)\geq t+ k'+1$ for some $k'\geq 0$.
Note that $Stable(w)=j$ implies that $H(w,p,b_p)\geq j$ for
each input  pin $p$ for some value of $b_p$.
So by the definition of
an adder the outputs of $D$ are determined when $Stable(w)\geq t+k$.
From the definition of $H$ we have $H(wa,p,b_p)\leq H(w,p,b_p)$ only when $H(wa,p,b_p)=0$
which we know is false (or else $Stable(wa)=0$.
So $H(w,p,b_p)< H(wa,p,b_p)$ which means $H(wa,p,b_p)= 1+H(w,p,b_p)$
so $H(w,p,b_p)=t+k'$ and $k'\geq 0$ so $H(w,p)\geq t$ which means that the
outputs at state $w$ were determined by the inputs which have not changed
by state $wa$. QED.

Now switch back to the composite structure.
By construction, $H(w,v1_i,b1_i)$ $ = H(u_i,v1,b1_i)$ and $H(w,v2_i,b2_i)$ $=H(u_i,v2,b2_i)$ and 
$H(w,carry_{in},b_0)$ $ = H(u_1,carry_{in},b_0)$. So by the lemma at \ref{lemma2} and the initial assumption
$\#D_1(u_1) \geq (n-1)t+n$ since the inputs have been stable for
$n*t+n$. Now we note that for $i>1$, we must have
$\#D_{i-1}(u_{i-1}) +1 \leq H(u_i,carry_{in},b_i)$. The proof is by induction
using the definition of the mapping from $w$ to the $u_i$.  This tells us that the inputs for $\#D_1(u_1) \geq t+k+1 \rightarrow \#D_i(u_i) \geq k$. The $1$ gets lost because of the 1 unit delay between output pins and input pins - see the
note above. At this point it's a matter of putting the parts together to complete the proof.

\section{Possible future directions\label{sec:conjecture}}


One of the advantages of working with ``vanilla" state machines is that they are fundamentally related to algebra --- algebra in the sense of groups and monoids, not ``algebra" in the sense of ``process algebra" or even universal algebra. It's
well known that every state machine induces a monoid, and there is a decomposition theory for state machines that parallels
and extends group decomposition via the Jordon-Holder theorem.  The researchers who first looked at algebraic automata
theory were focused on so-called ``cascade" products in which there is no feedback\cite{Arbibabs,Holcombe,ginzburg}.
For example, the composed nand-gate
above has no feedback between the $u_i$ --- information moves in one direction only. The constructed SR latch, on the
other hand, requires that the output of $N_1(u_i)$ be fed back into $u_2$. The standard digital architecture distinction
between combinational and sequential circuits appears related to this distinction. 

Recall $S_f$ the state set induced by $f:A^*\to X$. Let $Mon(f)$ be the monoid consisting
of the set of maps
$\sigma_w:S_f\to S_f$ for $w\in A^*$ so that $\sigma_w f_u = f_{u\conc w}$ and the operation
$\sigma_u\sigma_z = \sigma_{u\conc z}$. The set of maps will be a superset of $S_f$ because
$f_w = f_z$ does not imply that $\sigma_w = \sigma_z$ since $\sigma_w f_u = f_{u\conc w}$ which
may be distinct from $\sigma_z f_u - f_{u\conc z}$. But if $S_f$ is finite then $Mon(f)$ must be
finite and certainly $\sigma_\ess$ is the identity and
$$\sigma_w (\sigma_u \sigma_v)=\sigma_{w\conc u \conc v} = (\sigma_w \sigma_u)\sigma_v.$$

The product used here was ignored by the pioneers of algebraic automata theory for a variety of reasons, but
perhaps one of them was that the practitioner interest was in state minimization which leads naturally to
questions of decomposition. The full generality of feedback products also loses the analogy to the
wreath products of the Jordon-Holder theorem that are key to the Krohn-Rhodes decomposition. But
the product used here is quite constrained. Each $c_i(p)= a(p')$ or $c_i(p) = G_j(u_j,p')$ where $G_i$ is
a component factor because the 
communication mimics the connection of wires so that no computation can be done in the connection map. There
are additional constraints imposed by fan-out and fan-in limits and we may want more constraints such as
the $E(wa)=E(wa')$ constraint mentioned above that prevents instant response to inputs.
We can conjecture that this constrained
product also has structural implications for the construction of monoids. With the caution that
what follows is extremely early conjecture let's look at some possibilities relating combinations versus
sequence circuits to group structure.  What we call
combinatorial circuits in digital circuit engineering seems to produce state systems with only trivial loops - where$f_w = f_{wa}$.  This limits the complexity of subgroups within the underlying monoid. So products
with non-trivial feedback may be the only ways to introduce longer loops into the state structure.

\bibliography{all}
\bibliographystyle{plain}
\end{document}
