\section{Experiment} \label{experiment}

In this section, we conduct a comprehensive evaluation across a wide range of problems and datasets to verify the effectiveness of SRM.
We re-implemented all competitors to compare under consistent settings for fair comparison. 

\subsection{Object Classification} \label{image_classification}

We first evaluate SRM on general object classification with ImageNet-1K~\cite{russakovsky2015imagenet} and CIFAR-10/100~\cite{krizhevsky2009learning}, in comparison with state-of-the-art methods such as Squeeze-and-Excitation (SE)~\cite{hu2018squeeze} and Gather-Excite (GE)\footnote{Among the several variants of GE, we compared with GE-$\theta$ which is mainly explored in their paper.}~\cite{hu2018gather}.
On the extension of \cite{brendel2019approximating,geirhos2019imagenet}, which suggest the crucial role of styles in the decision making by standard CNNs, we further demonstrate the potential of styles for improving the general performance of CNNs.

\paragraph{ImageNet-1K.}
The ImageNet-1K dataset~\cite{russakovsky2015imagenet} consists of 1,000 classes with 1.3 million training and 50,000 validation images.
We follow the standard practice for data augmentation and optimization \cite{he2016deep}.
The input images are randomly cropped to  224$\times$224 patches and random horizontal flipping is applied.
The networks are trained by SGD with a batch size of 256 on 8 GPUs, a momentum of 0.9, and a weight decay of 0.0001.
We train the networks for 90 epochs from the scratch with an initial learning rate of 0.1 which is divided by 10 every 30 epochs.
Single center crop evaluation is performed on 224$\times$224 patches where each image is first resized so that the shorter side is 256. 

Figure \ref{fig:imagenet} illustrates the training and validation curves of ResNet-50 with SRM and other feature recalibration methods.
Throughout the whole training process, SRM exhibits considerably higher accuracy than SE and GE on both training and validation curves.
This implies that utilizing styles with SRM is more effective than modeling channel interdependencies with SE or gathering global context with GE, in both terms of facilitating training and improving generalization.
Table~\ref{table:imagenet} also demonstrates that 
SRM significantly boosts the performance of the baseline architecture (ResNet-50/101) with almost the same number of parameters and computations.
On the other hand, due to its tendency of slow convergence as mentioned in~\cite{hu2018gather}, GE does not exhibit improved performance in a deeper network under a fixed-length training schedule. 
It is worth noting that SRM outperforms SE and GE with orders of magnitude less additional parameters.
For example, SE-ResNet-50 and GE-ResNet-50 require 2.53M and 5.56M additional parameters to ResNet-50, respectively, but SRM-ResNet-50 only requires 0.06M (2.37\% of SE and 1.08\% of GE) which shows the exceptional parameter efficiency of SRM.

\begin{table}
\caption{Top-1 and top-5 accuracy (\%) on the ImageNet-1K validation set and complexity comparison.}
\vspace{-1em}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
Model & Params & GFLOPs & top-1 & top-5 \\
\hline
ResNet-50 & 25.56M & 3.86 & 75.89 & 92.85 \\
SE-ResNet-50 & 28.09M & 3.87 & 76.80 & 93.39 \\
GE-ResNet-50 & 31.12M & 3.87 & 76.75 & 93.41 \\
SRM-ResNet-50 & 25.62M & 3.88 & \textbf{77.13}& \textbf{93.51} \\
\hline
ResNet-101 & 44.55M & 7.58 & 77.40 & 93.59 \\
SE-ResNet-101 & 49.33M & 7.60 & 78.08 & 93.95 \\
GE-ResNet-101 & 53.58M & 7.60 & 77.36 & 93.64 \\
SRM-ResNet-101 & 44.68M & 7.62 & \textbf{78.47} & \textbf{94.20}\\
\hline
\end{tabular}
\end{center}
\label{table:imagenet}
\end{table}

\begin{table}
\caption{Accuracy (\%) on the CIFAR-10/100 test sets with a ResNet-56 baseline and complexity comparison.}
\vspace{-1em}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
 & \multicolumn{2}{|c|}{CIFAR-10} & \multicolumn{2}{|c}{CIFAR-100} \\ \cline{2-5}
Model & Params & top-1 & Params & top-1 \\
\hline
Baseline & 0.87M &  93.77 & 0.89M & 74.76 \\
SE & 0.97M & 94.60 & 0.99M & 76.10  \\
GE & 1.91M & 94.32 & 1.94M & 76.02  \\
SRM & 0.89M  & \textbf{95.05} & 0.91M  & \textbf{76.93} \\
\hline
\end{tabular}
\end{center}
\label{table:cifar}
\end{table}

\paragraph{CIFAR-10/100.}
We also evaluate the performance of SRM on the CIFAR-10/100 dataset~\cite{krizhevsky2009learning} which consists of 50,000 training and 10,000 test images of 32$\times$32 pixels.
On the training phase, each image is zero-padded with 4 pixels then randomly cropped to the original size, and evaluation is performed on the original images.
The networks are trained with SGD for 64,000 iterations with a mini-batch size of 128 on a single GPU, a momentum of 0.9, and a weight decay of 0.0001. The initial learning rate is set to 0.2 which is divided by 10 at 32,000 and 48,000 iterations.
As presented in Table~\ref{table:cifar}, SRM considerably improves the accuracy on both CIFAR-10 and 100 with minimal parameter increases, which suggests that the effectiveness of SRM is not constrained to ImageNet.


\subsection{Style-Related Classification}

The proposed idea views channel-wise recalibration as an adjustment of intermediate styles, which is achieved by exploiting the global statistics of respective feature maps.
This interpretation motivates us to explore the effect of SRM on style-related tasks where explicitly manipulating style information could bring prominent benefits.

\begin{table}
\caption{Top-1 and top-5 accuracy (\%) on the validation sets of Stylized-ImageNet and ImageNet with a ResNet-50 baseline, when trained on Stylized-ImageNet.}
\vspace{-1em}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
 & \multicolumn{2}{|c|}{Stylized-ImageNet} & \multicolumn{2}{|c}{ImageNet} \\ \cline{2-5}
 & top-1 & top-5 & top-1 & top-5 \\
\hline
Bseline &\ \  53.93 \  & 76.75 & 56.11 & 79.17\\
SE & 58.31 & 80.80 & 60.15 & 82.54 \\
SRM & \textbf{60.69} & \textbf{82.56} & \textbf{62.12} & \textbf{84.06} \\
\hline
\end{tabular}
\end{center}
\label{table:stylized-imagenet}
\end{table}

\begin{table}
\caption{Accuracy (\%) on the Office-Home dataset with a ResNet-18 baseline, averaged over 5-fold cross validation.}
\vspace{-1em}
\begin{center}
\begin{tabular}{c|c|c|c|c|c}
\hline
  & Ar & Cl & Pr & Rw & Avg. \\
\hline
Baseline & 37.49 & 60.73 & 72.81 & 52.12 & 55.47 \\ 
SE & 39.55 & 62.75 & 75.60 & 55.52 & 58.36  \\ 
SRM & \textbf{40.50} & \textbf{64.97} & \textbf{76.12} & \textbf{56.30} & \textbf{59.47}  \\ 
\hline
\end{tabular}
\end{center}
\label{table:officehome}
\end{table}


\begin{figure*}
\newcommand{\addstyle}[1]{\includegraphics[width=6em, height=6em]{figure/style_transfer/#1.jpg}}
\begin{center}
\setlength{\tabcolsep}{0.05em}
\begin{tabular}{cccccc}
Style & Content & BN  & BN+SE & BN+SRM & IN \\
\addstyle{rain-princess/style} & \addstyle{rain-princess/content} & \addstyle{rain-princess/bn}  & \addstyle{rain-princess/se} & \addstyle{rain-princess/srm} & \addstyle{rain-princess/in} \\
\addstyle{candy/style} & \addstyle{candy/content} & \addstyle{candy/bn}  & \addstyle{candy/se} & \addstyle{candy/srm} & \addstyle{candy/in} \\
\addstyle{la-muse/style} & \addstyle{la-muse/content} & \addstyle{la-muse/bn}  & \addstyle{la-muse/se} & \addstyle{la-muse/srm} & \addstyle{la-muse/in} \\
\end{tabular}
\end{center}
\vspace{-1em}
\caption{Example style transfer results. While both BN+SRM and BN+SE improve the stylization quality compared to BN, BN+SRM yields much higher quality which is comparable to IN.
More examples are provided in Figure~\ref{fig:style_transfer_supp}.}
\label{fig:style_transfer}
\end{figure*} 

\paragraph{Stylized-ImageNet.}
We first investigate how SRM handles synthetically increased diversity of styles.
We employ Stylized-ImageNet introduced by \cite{geirhos2019imagenet}, which is constructed by transferring each image in ImageNet to the style of a random painting in the \textit{Painter by Numbers} dataset\footnote{\url{https://www.kaggle.com/c/painter-by-numbers/}} (total 79,434 paintings).
Since the randomly transferred style is irrelevant to the object category, it is a much harder dataset than ImageNet to train on.
We train ResNet-50 based networks on Stylized-ImageNet from scratch\footnote{Although \cite{geirhos2019imagenet} uses ImageNet pretrained networks, we train networks from scratch to focus on the characteristics on Stylized-ImageNet.} following the same training policy as the ImageNet experiment, and report the validation accuracy on Stylized-ImageNet and the original ImageNet in Table~\ref{table:stylized-imagenet}.
SRM not only brings impressive improvements over the baseline and SE on Stylized-ImageNet, but also generalizes better to the original ImageNet.
This supports our claim that SRM learns to suppress the contribution of nuisance styles, which helps the network to concentrate more on meaningful features. 



\paragraph{Multi-Domain Classification.}
We also verify the effectiveness of SRM in tackling natural style variations inherent in different input domains.
We adopt the Office-Home dataset~\cite{venkateswara2017deep} which consists of 15,588 images from 65 categories across 4 heterogeneous domains: Art (Ar), Clip-art (Cl), Product (Pr) and Real-world (Rw).
We combine all training sets of the 4 domains and train domain-agnostic networks based on ResNet-18, following the same setting as the ImageNet experiment except that the networks are trained with a batch size of 64 on 1 GPU.
Table \ref{table:officehome} shows the top-1 accuracy averaged over 5-fold cross validation.
SRM consistently improves the accuracy with significant margins across all domains, which indicates the capability of SRM for alleviating the style discrepancy over different domains.
It also implies the potential of SRM to be utilized in domain adaptation problems~\cite{tzeng2017adversarial,hoffman2018cycada} which entail style disparity between the source and target domains.

\begin{table}
\caption{Top-1 and top-5 accuracy (\%) on the Describable Texture Dataset averaged over 5-fold cross validation. }
\vspace{-1em}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
 & \multicolumn{2}{|c|}{ResNet-32} & \multicolumn{2}{|c}{ResNet-56} \\ \cline{2-5}
 & top-1 & top-5 & top-1 & top-5 \\
\hline
Baseline & 44.96 & 73.85 & 45.46 & 75.54  \\
SE & 45.20 & 75.60 & 48.63 & 77.40  \\
SRM & \textbf{46.50} & \textbf{76.63} & \textbf{50.44} & \textbf{79.37}  \\
\hline
\end{tabular}
\end{center}
\label{table:dtd}
\end{table}

\paragraph{Texture Classification.}
We further evaluate SRM on texture classification using Describable Texture Dataset (DTD)~\cite{cimpoi14describing} which comprises 5,640 images across 47 texture categories such as cracked, bubbly, marbled, etc.
This task offers to assess a different perspective of the network: the ability to extract most textural patterns that elicit visual impressions prior to recognizing objects in images~\cite{cimpoi2016deep}.
We follow the data processing setting of \cite{rebuffi2017learning}, and the same training policy as our CIFAR experiment. 
The results from 5-fold cross validation with ResNet-32 and ResNet-56 baselines are reported in table \ref{table:dtd}, in which SRM achieves outstanding performance improvements. 
It demonstrates that SRM successfully models the importance of individual styles and emphasizes the target textures, enhancing the representational power regarding style attributes.


\begin{figure}
\begin{center}
    \includegraphics[width=\linewidth]{etc_loss.pdf}
    \vspace{-1em}
    \caption{Quantitative comparison of style loss (left) and content loss (right) with a style image of \textit{Rain Princess} (the first row in Figure~\ref{fig:style_transfer}).}
\label{fig:loss}
\end{center}
\end{figure}

\begin{figure*}
\begin{center}
    \includegraphics[width=0.99\textwidth]{etc_prune_acc.pdf}
\caption{Top-1 validation accuracy of ResNet-50 on ImageNet after pruning channels of each stage according to estimated channel weights.
Stage 1 is omitted because it consists of a single convolutional layer where a recalibration module is not applied.
    } 
    \label{fig:prune_acc}
\end{center}
\end{figure*}


\subsection{Style Transfer}
We finally examine the benefit of SRM in a generative problem of style transfer.
We utilize a single style feed-forward algorithm~\cite{johnson2016perceptual} implemented in the official PyTorch repository\footnote{\url{https://github.com/pytorch/examples/tree/master/fast\_neural\_style}}.
The networks are trained with content images from the MS-COCO dataset~\cite{lin2014microsoft}, following the default configurations in the original code.


Figure~\ref{fig:loss} depicts the training curves of style and content loss with different recalibration methods.
As reported in the literature~\cite{ulyanov2017improved, nam2018batch}, removing the style from the content image with instance normalization (IN)~\cite{ulyanov2016instance} brings a huge improvement over using the standard batch normalization (BN)~\cite{ioffe2015batch}.
Surprisingly, the BN-based network equipped with SRM (BN+SRM) reaches almost the same level of style/content loss with IN, while the network with SE (BN+SE) exhibits much inferior style/content loss.
This demonstrates the distinct effect of SRM, which mimics the behavior of IN by dynamically suppressing unnecessary styles from input images.
We also show qualitative examples in Figure~\ref{fig:style_transfer}.
Although BN+SE somewhat improves the stylization quality compared to BN, it is still far behind the performance of IN.
In contrast, BN+SRM not only successfully transfers to target style but also better represents the important styles of the content images (e.g. green glass and blue sky), generating competitive results to IN.
Overall, the advantage of SRM is not restricted to discriminative tasks but can be extended to generative frameworks, which remains as future work.
%
