\def\year{2021}\relax
\documentclass[letterpaper]{article} \usepackage{aaai21}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \usepackage{url}
\usepackage{amsfonts}
\usepackage{microtype}      \usepackage[noend]{algpseudocode}
\usepackage{algorithmicx,algorithm}
\usepackage{amsmath}
\usepackage{epstopdf}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{bm,bbm}
\usepackage{color}
\usepackage{wrapfig}
\usepackage{array}
\usepackage{hyperref}

\usepackage{lipsum} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \newcommand{\etal}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}.,}
\newcommand{\eg}{\textit{e}.\textit{g}.,}
\nocopyright


\pdfinfo{
/Title (Diversity Helps: Unsupervised Few-shot Learning via Distribution Shift-based Augmentation)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen, Francisco Cruz, Marc Pujol-Gonzalez)
/TemplateVersion (2021.1)
} 



\setcounter{secnumdepth}{2} 







\title{Diversity Helps: Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation}
\author{Tiexin Qin\thanks{contributed equally},\quad Wenbin Li\footnotemark[1],\quad Yinghuan Shi,\quad Yang Gao \\
State Key Laboratory for Novel Software Technology, Nanjing University, China\\
\tt\small qtx@smail.nju.edu.cn \quad \tt\small \{liwenbin,syh,gaoy\}@nju.edu.cn}
\iffalse
\author{

Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,  \\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz,
    Marc Pujol-Gonzalez
    \\
}

\affiliations{


    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\




    2275 East Bayshore Road, Suite 160\\
    Palo Alto, California 94303\\
publications21@aaai.org

}
\fi
\iffalse
\title{My Publication Title --- Single Author}
\author {
Author Name \\
}

\affiliations{
    Affiliation \\
    Affiliation Line 2 \\
    name@example.com
}
\fi

\iffalse
\title{My Publication Title --- Multiple Authors}
\author {


        First Author Name,\textsuperscript{\rm 1}
        Second Author Name, \textsuperscript{\rm 2}
        Third Author Name \textsuperscript{\rm 1} \\
}
\affiliations {
\textsuperscript{\rm 1} Affiliation 1 \\
    \textsuperscript{\rm 2} Affiliation 2 \\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi

\begin{document}
\maketitle
\begin{abstract}
Few-shot learning aims to learn a new concept when only a few training examples are available, which has been extensively explored in recent years. However, most of the current works heavily rely on a large-scale labeled auxiliary set to train their models in an episodic-training paradigm. Such a kind of supervised setting basically limits the widespread use of few-shot learning algorithms.
Instead, in this paper, we develop a novel framework called \emph{Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation} (ULDA), which pays attention to the distribution diversity inside each constructed pretext few-shot task when using data augmentation. Importantly, we highlight the value and importance of the distribution diversity in the augmentation-based pretext few-shot tasks, which can effectively alleviate the overfitting problem and make the few-shot model learn more robust feature representations.
In ULDA, we systemically investigate the effects of different augmentation techniques and propose to strengthen the distribution diversity (or difference) between the query set and support set in each few-shot task, by augmenting these two sets diversely (\ie~distribution shifting). In this way, even incorporated with simple augmentation techniques (\eg~random crop, color jittering, or rotation), our ULDA can produce a significant improvement. In the experiments, few-shot models learned by ULDA can achieve superior generalization performance and obtain state-of-the-art results in a variety of established few-shot learning tasks on Omniglot and \emph{mini}ImageNet. The source code is available in \textcolor{blue}{\emph{https://github.com/WonderSeven/ULDA}}.
\end{abstract}

\section{Introduction}

The ability of learning from limited labeled examples is a hallmark of human intelligence, yet it remains a challenge for modern machine learning systems. This problem recently has attracted significant attention from the machine learning community, which is formalized as few-shot learning (FSL). To solve this problem, a large-scale auxiliary set is generally required to learn transferable knowledge to boost the learning of the target few-shot tasks. Specifically, one kind of FSL methods usually resort to using metric losses to enhance the discriminability of the representation learning, such that a simple nearest neighbor or linear classifier is able to achieve satisfactory classification results~\cite{Snell2016NIPS,Vinyals2016NIPS}. Another kind of FSL methods incorporates the concept of meta-learning and aims to enhance the ability of quickly updating with a few labeled examples~\cite{FinnICML2017,RaviICLR2017,MunkhdalaiICML2017}. Alternatively, some FSL methods address this problem by generating more examples from the provided ones~\cite{Gao2018NIPS,Chen2019AAAI,ChenCVPR2019}.

\begin{table}[!tbp]\small
\normalsize
\vspace*{-1cm}
\begin{center}
\caption{The results of -way -shot tasks on \emph{mini}ImageNet by using different augmentation methods on \emph{ProtoNet} to construct the query and support sets. TA and AA indicate traditional augmentation and AutoAugment, respectively.}
\label{tab:compare_augmentation}
\begin{tabular}{cccc}
\toprule[1pt]
\textbf{Support} &  \textbf{Query} & \textbf{5-way 1-shot} & \textbf{5-way, 5-shot} \\  
\midrule
TA              & TA              & 32.58 & 44.40 \\ AA              & AA           & 31.53 & 41.83 \\ \textbf{TA}     & \textbf{AA}  & \underline{34.07} & \underline{47.31} \\ \textbf{AA}     & \textbf{TA}     & \textbf{35.37} & \textbf{49.16} \\ \bottomrule[1pt]
\end{tabular}
\end{center}
\vspace{-0.2cm}
\end{table}


\begin{figure}[tbp]
\begin{center}
\includegraphics[width=0.47\textwidth]{figs/train_test_split2.pdf}
\end{center}
\vspace{-0.3cm}
\caption{The train and test accuracy curves on the 5-way 1-shot tasks, corresponding to the four combinations of different augmentation methods (\ie~TA and AA) in Table~\ref{tab:compare_augmentation}. As seen, the diverse combinations (\ie~the red and yellow lines) enjoy a smaller risk of overfitting (\ie~lower train accuracy and higher test accuracy) than the identical combinations (\ie~the green and blue lines).}
\label{fig:train_test_curves}
\vspace{-0.3cm}
\end{figure}

\iffalse
\begin{figure*}[tbp]\begin{minipage}[b]{.3\linewidth}
        \centering
        \scriptsize{
        \begin{tabular}{cccc}
        \toprule[1pt]
        \textbf{Support} &  \textbf{Query} &  \textbf{(5, 1)} & \textbf{(5, 5)} \\
        \hline
        TA              & TA              & 32.58 & 44.40 \\ AA              & AA           & 31.53 & 41.83 \\ \textbf{TA}     & \textbf{AA}  & \underline{34.07} & \underline{47.31} \\ \textbf{AA}     & \textbf{TA}     & \textbf{35.37} & \textbf{49.16} \\ \bottomrule[1pt]
        \end{tabular}
        }
        \captionof{table}{The results of -way -shot ((, ) for short) tasks on \emph{mini}ImageNet by using different augmentation methods on ProtoNet~\cite{Snell2016NIPS} to construct the query and support sets. TA and AA indicate traditional augmentation and AutoAugment~\cite{Cubuk2019CVPR}, respectively.}
\label{tab:compare_augmentation}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{.67\linewidth}
    \centering
        \includegraphics[width=\linewidth, height=3cm]{figs/train_test_split2.pdf}
        \captionof{figure}{The train and test accuracy curves on the 5-way 1-shot tasks, corresponding to the four combinations of different augmentation methods (\ie~TA and AA) in Table~\ref{tab:compare_augmentation}.  
        As seen, the diverse combinations (\ie~the red and yellow lines) enjoy a smaller risk of overfitting (\ie~lower train accuracy and higher test accuracy) than the identical combinations (\ie~the green and blue lines).}
\label{fig:train_test_curves}
    \end{minipage}
    \vspace{-0.6cm}
\end{figure*}
\fi

Although the aforementioned FSL methods can achieve promising results, most of these methods are fully supervised, which means that they are heavily relying on a large-scale fully labeled auxiliary set (\eg~a subset from ImageNet in previous works~\cite{Snell2016NIPS,FinnICML2017,RaviICLR2017}).
Through this fully labeled auxiliary set, plenty of supervised few-shot tasks (episodes) can be constructed for model training (\ie~episodic-training mechanism~\cite{Vinyals2016NIPS}). However, in many real-world applications, such a fully supervised condition is relatively severe. It greatly hinder the widespread use of these FSL methods for real applications. Because data labeling for a large-scale dataset is normally time-consuming, laborious, and even very expensive for some domain-professional areas like biomedical data analysis. In contrast, large unlabeled data is easily accessible to many real problems. This gives rise to a more challenging problem, called \textit{unsupervised few-shot learning}, which tries to learn few-shot models by using an unlabeled auxiliary set.

As for unsupervised few-shot learning, only a few works have been proposed. For example, CACTUs~\cite{Hsu2019ICLR}, a two-stage method, firstly uses a clustering algorithm to obtain pseudo labels, and then trains a model under the common supervised few-shot setting with these pseudo labels. Different from CACTUs, both AAL~\cite{AAL2019ICML} and UMTRA~\cite{UMTRA2019NIPS} take each instance as one class and randomly sample multiple examples to construct a support set. Next, they generate a pseudo query set according to the support set by leveraging data augmentation techniques. In this paper, we are more interested in this data augmentation based direction, because it can not only achieve promising results but also can be easily learned in an end-to-end manner. However, we find that the existing data augmentation based methods (\ie~AAL and UMTRA) are sensitive to the selection of augmentation techniques and usually do not contain sufficient regularity for model learning. What's more, they are easily suffering from the overfitting problem during training, because they choose the same data augmentation technique for both the query set and support set. This will make the distributions between the augmented query set and support set too similar. In other words, they construct too many ``easy pretext few-shot tasks'' for the downstream few-shot training by only using one single data augmentation technique. 
In some cases, this technique equals to directly copy original samples several times.
We argue that such \textit{excessive distribution similarity} between the query and support set (\ie~easy pretext few-shot tasks) is the main point of leading to overfitting issue in unsupervised few-shot model training.



To tackle the above overfitting problem, we claim that strengthening the \textit{distribution diversity (or difference)} between the augmented query set and support set (\ie~hard pretext few-shot tasks) can significantly alleviate the overfitting problem during the model training and make the learned model have a much better generalization ability. To simply verify this point, we perform a preliminary experiment (see 
Table~\ref{tab:compare_augmentation} and Figure~\ref{fig:train_test_curves}). 
We observe that there is a high risk of overfitting when using same augmentation technique.
Also, when using different (or diverse) augmentation techniques, the classification performance can be significantly improved over using same augmentation technique.







Therefore, in this paper, we introduce a novel framework named \textit{Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation} (ULDA) following the above statement. To be specific, our ULDA augments the query set and support set in diverse ways, aiming to make a significant distribution shift between these two sets. The main contributions of our work could be summarized into the following three folds: 
\begin{enumerate}
    \item We argue that the \textit{distribution diversity} between the augmented query set and support set is a key point in data augmentation based unsupervised few-shot learning, for the first time in the literature.
    \item  We propose a \textit{Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation} (ULDA) framework and a new simple augmentation method named \emph{Distribution Shift-based Task Internal Mixing} (DSTIM) to strengthen the distribution diversity when constructing the pretext few-shot training tasks.
    \item Extensive experiments on both Omniglot and \emph{mini}ImageNet datasets demonstrate the superiority of our proposed ULDA and DSTIM.
\end{enumerate}









\section{Related Work}

\label{sec:related_work}
We briefly review the related work about supervised and unsupervised few-shot learning, respectively. 


\textbf{Few-shot learning (FSL).} FSL aims to learn a new concept on very limited training examples, which has promising practical application value. A vast number of methods has been proposed in recent years. These methods can be roughly categorized into three classes, \ie~\emph{metric-based}, \emph{optimization-based}, and \emph{hallucination-based methods}.


The metric-based methods aim to learn discriminative feature representations by using deep metric learning, with the help of intra-class and inter-class constraints~\cite{Vinyals2016NIPS,Snell2016NIPS,Sung2018CVPR,Li2019DN4}. They employ various metric losses (\eg~pairwise loss, triplet loss) to enhance the discriminability of the learned features. The optimization-based methods strive for enhancing the flexibility of the learned model such that it can be readily updated with a few labeled examples \cite{RaviICLR2017,FinnICML2017,LeeCVPR2019meta,ChenICLR2019close}. Alternatively, the hallucination-based methods attempt to address the data scarcity problem by directly generating more new examples \cite{ZhangCVPR2019,AlfassyCVPR2019,ChenCVPR2019,Chen2019AAAI,ChenTIP2019}.


Most methods train their models under the episodic-training paradigm \cite{Vinyals2016NIPS}. They organize a large labeled auxiliary dataset into plenty of mimetic few-shot tasks where each task contains a \emph{support} set and a \emph{query} set. The \emph{support} set is used to acquire task-specific information and the \emph{query} set is used to evaluate the generalization performance of the model. Based on episodic-training, the model expects to learn transferable representations or knowledge, with which, it can generalize to new unseen tasks.


\begin{figure*}[!t]
\vspace{-0.2cm}
\centering
\includegraphics[width=0.75\textwidth]{figs/framework.pdf}
\caption{The framework of the proposed ULDA which starts from an unlabeled auxiliary dataset. First, randomly select  examples and assign  random labels to them. After that, the proposed \textit{distribution shift-based augmentation} module is used to construct a pretext few-shot task (consists of an augmented query set and an augmented support set). Specifically, the query set and support set are augmented by the augmentation operators  and , respectively. Finally, the constructed pretext few-shot task is adopted to train the few-shot learning model in a supervised way.}
\label{fig:framework}
\vspace{-0.2cm}
\end{figure*}


\textbf{Unsupervised few-shot learning.}
Currently, a few works propose \emph{unsupervised few-shot learning} to tackle the huge requirement of a large labeled auxiliary set in supervised few-shot learning. Hsu \etal~\cite{Hsu2019ICLR} propose CACTUs which uses a clustering algorithm to obtain pseudo labels and then constructs few-shot tasks with these pseudo labels. Differently, Khodadadeh \etal~\cite{UMTRA2019NIPS} and Antoniou \etal~\cite{AAL2019ICML} both propose to randomly sample multiple examples to construct the support set and generate a pseudo query set via data augmentation based on the support set.

Our work belongs to the data augmentation based methods. The main difference is that the existing methods~\cite{UMTRA2019NIPS,AAL2019ICML} easily suffers from the overfitting problem, while our proposed ULDA can significantly alleviate this problem. This is because there is usually a large distribution similarity between the query set and support set in the existing methods, while our ULDA strengthens a distribution shift between the augmented query set and support set. Note that similar operations seemingly have appeared in other research fields, such as FixMatch~\cite{Sohn2020FixMatch} in semi-supervised learning and SimCLR~\cite{Chen2020SimCLR} in unsupervised representation learning. However, we highlight that we do not need to use weak augmentations to assign a higher degree of confidence for unlabeled samples like FixMatch did, and we draw a different observation (diversity helps) from SimCLR (combination is better). In fact, we believe that our observation/perspective is unique in the specific field of unsupervised few-shot learning, which could not be directly borrowed to other fields.








\iffalse
\subsection{Motivation from Data Augmentation based Task Construction}

Inspired by the literature, we know that the key issue in \textit{unsupervised few-shot learning} is how to construct effect pretext (pseudo) few-shot tasks from an unlabeled auxiliary set . If we are able to construct enough pretext few-shot tasks (which have pseudo labels), and then we can directly learn a few-shot model in a supervised way, by using the episodic-training mechanism~\cite{Vinyals2016NIPS}.

To that end, the latest methods, such as AAL~\cite{AAL2019ICML} and UMTRA~\cite{UMTRA2019NIPS}, employ the data augmentation techniques to address the above issue. As defined in Section~\ref{problem_formulation}, a pretext few-shot task usually consists of a pseudo support set  and a pseudo query set . For the construction of the support set , they randomly sample  unlabeled data-points as the support examples and randomly assign  labels (classes) for these examples, \ie~. Next, they augment each image (which has been labeled with pseudo classes) in  to generate multiple examples within the same class. These augmented examples are taken as the pseudo query set , which has the same label space as . The pretext few-shot tasks constructed in the above way have been verified to be effective and have shown promising results on various datasets~\cite{AAL2019ICML,UMTRA2019NIPS}. This is because data augmentation can naturally maintain the label of the augmented examples, which can produce reliable pseudo labels for unlabeled examples.

However, the limitation of the above methods is clear. Although the pseudo  enjoys the same label space with the pseudo  by leveraging the nature of data augmentation on label maintenance,  and  have too similar distributions. The reason is that they only adopt one single data augmentation technique for both query and support sets. This limitation will easily leads to a serious overfitting problem in the training stage. As a result, these methods are sensitive to the choice of the data augmentation techniques. This phenomenon can be well explained by our preliminary experiment in Table~\ref{tab:compare_augmentation}. As seen, when we simply adopt two different data augmentation methods for the query and support sets, the performance can be significantly improved over the single augmentation manner. It means that the distribution diversity between the query set and support set is beneficial to alleviate the overfitting.

This motivates us to study how to increase the distribution difference (diversity) between the pseudo query set and support set, under the principle of maintaining the same label space of these two in this paper. In doing so, the overfitting problem during training can be effectively alleviated and more robust representations can be learned to tackle the challenging problem of unsupervised few-shot learning.
\fi


\section{Our Method}

\subsection{Problem Formulation}
\label{problem_formulation}
The goal of \textit{unsupervised few-shot learning} is to first train a model on a large-scale \textit{unlabeled} auxiliary set , and then apply this trained model on a novel labeled test set , which is composed of a set of few-shot tasks. Note that, according to the setting of FSL, there are only a few labeled examples (\eg~ or  examples) in each class for each few-shot task in . To effectively leverage the unlabeled auxiliary set  for model training, following the episodic-training mechanism~\cite{Vinyals2016NIPS}, we still try to generate a series of pretext -way -shot tasks (episodes) from  by using an data augmentation framework. In particular, each pretext few-shot task is composed of a pseudo support set (for training) and a pseudo query set (for validation). The pseudo support set consists of  classes and  examples per class (\eg~=1 in our paper), termed as , while the query set  contains  generated examples augmented based on the pseudo support set. At each iteration, the model is trained by one episode (task) to minimize the classification loss on query set according to support set. After tens of thousands of episodes training, the model is expected to reach convergence and perform well on novel few-shot tasks.


\label{method:ULDA}
\subsection{The Proposed ULDA Framework}
To detail the proposed \textit{Distribution Shift-based Data Augmentation (ULDA)} framework (see Figure~\ref{fig:framework}), we first layout the pretext few-shot task construction procedure in unsupervised few-shot learning. Next, we detail the two key modules in ULDA: (1) \emph{distribution shift-based data augmentation module}, (2) \emph{metric-based few-shot learning module}. 

\begin{figure*}[!tbp]
\vspace{-0.3cm}
\centering
\includegraphics[width=0.9\textwidth]{figs/transforms.pdf}
\vspace{-0.2cm}
\caption{Illustrators of the employed augmentation techniques in this work. Top: Original images, Bottom: augmented images, transformed by an augmentation operator.}
\label{fig:transforms}
\vspace{-0.2cm}
\end{figure*}
 
\noindent\textbf{Task Construction for Unsupervised FSL.}
We randomly sample a mini-batch of  data-points  from the unlabeled auxiliary set  as the initial support samples and construct one pretext few-shot task on augmented examples derived from this initial support set. Specifically, we take each data-point as one class and assign random labels for these data-points , which is a common strategy in unsupervised learning in the literature~\cite{Wu2018CVPR,He2019MoCo}.

During the augmentation on the support set, for the -th initial support image  (\ie~the -th support class), we perform the augmentation operator  from  () on this sample to obtain an augmented support image . Also, for the augmentation on the query set, we randomly select  augmentation operators , , ...,  to augment each initial support image (\ie~each support class) to obtain  augmented query images. So each constructed pretext few-shot task  consists of an augmented support set  and an augmented query set  (\ie ):

where  means to perform the sampled operator  on the -th initial support image  in the initial support set.  means to perform the sampled operator  on the -th initial support image  from the initial support set.

In this work, we emphasize that maintaining a diversity between  and  (\ie~ and ) benefits the performance. This will be thoroughly discussed in the following section. We summarize the main sampling strategy of ULDA in Algorithm \ref{algorithm}.









\iffalse
According to the above analysis, we propose a complete framework \emph{Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation} (ULDA), which intends to learn the representations by maximizing the agreement between support and query sets in the latent space even when there exists a large distribution shift during constructing these two sets. As shown in Figure~\ref{fig:framework}, our framework is composed of the following two major components.

\textbf{(1)} A \emph{distribution shift-based data augmentation module} that specifically considers the \emph{distribution diversity} in the constructed few-shot tasks during data augmentation. Formally, we form two different sets of augmentation operators for the support and query sets in each constructed few-shot task, which are denoted as  and , respectively. In general, both the commonly-used augmentation operators (\eg~random crop, color jittering and rotate) and the recently proposed augmentation operators (\eg~AutoAugment~\cite{Cubuk2019CVPR}) could be the elements of  and . When we have obtained  and , the augmentation process is straightforward: 1) randomly sampling multiple data-points as the initial support set, 2) performing augmentation operators in  on these initial support samples to obtain one augmented support set, and 3) similarly performing augmentation operators in  on these initial support samples to obtain one augmented query set.
\textbf{(2)} A \emph{metric-based few-shot learning module} that consists of a feature extractor  and a non-parametric classifier. The feature extractor  first learns to map the augmented query and support examples into an appropriate feature space, and then the non-parametric classifier (\eg~NN) performs classification based on the distances between the query and support examples. Note that our framework allows various alternatives of the metric-based few-shot learning methods. In this paper, we employ ProtoNets~\cite{Snell2016NIPS} to be the backbone as a demonstration of our framework. Moreover, we will discuss an extension to one optimization-based few-shot learning method in Section \ref{sec:extension}.


We randomly sample a mini-batch of  data-points  from the unlabeled auxiliary set  as the initial support samples and construct one pretext few-shot task on augmented examples derived from this initial support set. Specifically, we take each data-point as one class and randomly assign labels for these data-points , which is a commonly used strategy in unsupervised learning in the literature~\cite{Wu2018CVPR,He2019MoCo}.



As aforementioned, during the augmentation on the support set, for the -th initial support image  (\ie~the -th support class), we perform the augmentation operator  from  () on this sample to obtain an augmented support image . Also, for the augmentation on the query set, we randomly select  augmentation operators , , ...,  to augment each initial support image (\ie~each support class) to obtain  augmented query images. So for each few-shot task  consists of an augmented support set  and an augmented query set :

where  means to perform the sampled operator  on the -th initial support image  in the initial support set.  means to perform the sampled operator  on the -th initial support image  from the initial support set.




In this work, we emphasize that maintaining a diversity between  and  (\ie~) benefits the performance. This will be thoroughly discussed in the following section. Besides, it has been verified in~\cite{UMTRA2019NIPS} that the labels in our constructed few-shot tasks maintain class distinctions in most cases which is however important.
\fi





\noindent\textbf{Distribution Shift-based Augmentation Module.}
Data augmentation technique plays a key role in aforementioned task construction procedure. However, in traditional methods~\cite{UMTRA2019NIPS, AAL2019ICML}, the generated tasks do not contain sufficient regularity for model learning as the generated examples are particularly suspect to visual similarity with the original images.
To alleviate this problem, we propose to increase the distribution diversity between the augmented support set and query set with a novel distribution shift-based data augmentation module, which employs diverse data augmentation operators to generate the support set and query set.



To systematically study the impact of diverse data augmentation, we consider to use both the commonly-used data augmentations and recently proposed augmentations.
Random crop and color jittering are widely used together in few-shot learning, we bind them as traditional augmentation (\textbf{TA} for short). Typically, for rotation, each image is converted among four directions in . The learned AutoAugment (\textbf{AA} for short) method proposed in~\cite{Cubuk2019CVPR} is also investigated for its promising performance in UMTRA~\cite{UMTRA2019NIPS}. 

In addition to the above existing augmentation techniques, we also propose a new \emph{Distribution Shift-based Task Internal Mixing (DSTIM)} augmentation strategy, which is composed of two  new augmentation operators, \ie~ and . Specifically, we visualize all the augmentation operators used in this work in Figure~\ref{fig:transforms}. To understand the efficacy of each individual augmentation operator and the difference of different combinations of augmentation operators, we conduct  a serial of experiments detailed in our supplementary material.


\begin{table*}[!tp]\small
\centering
\extrarowheight=-1pt
\caption{Unsupervised few-shot classification results (\%) under -way -shot (\ie~(N, K)) setting on Omniglot. }\label{tab:Omniglot}
\begin{tabular}{p{185pt}<{\raggedright}p{50pt}<{\raggedright}p{45pt}<{\centering}p{45pt}<{\centering}p{45pt}<{\centering}p{45pt}<{\centering}}
\toprule[1pt]
\textbf{Algorithms} &  \textbf{Clustering} & \textbf{(5, 1)} & \textbf{(5, 5)} &  \textbf{(20, 1)} & \textbf{(20, 5)} \\
\hline
\textbf{Training from scratch}                    & N/A           & 52.50\scalebox{0.75}{}  & 74.78\scalebox{0.75}{}  & 24.91\scalebox{0.75}{}  & 47.62\scalebox{0.75}{} \\
\hline
\small{\textbf{-nearest neighbors}}  & DeepCluster   & 49.55\scalebox{0.75}{}  & 68.06\scalebox{0.75}{}  & 27.37\scalebox{0.75}{}  & 46.70\scalebox{0.75}{} \\
\textbf{linear classifier}                        & DeepCluster   & 48.28\scalebox{0.75}{}  & 68.72\scalebox{0.75}{}  & 27.80\scalebox{0.75}{}  & 45.82\scalebox{0.75}{} \\
\textbf{MLP with dropout}                         & DeepCluster   & 40.54\scalebox{0.75}{}  & 62.56\scalebox{0.75}{}  & 19.92\scalebox{0.75}{}  & 40.71\scalebox{0.75}{} \\
\textbf{cluster matching}                         & DeepCluster   & 43.96\scalebox{0.75}{}  & 58.62\scalebox{0.75}{}  & 21.54\scalebox{0.75}{}  & 31.06\scalebox{0.75}{} \\
\textbf{AAL-ProtoNes}~\cite{AAL2019ICML}          & N/A           & 84.66\scalebox{0.75}{}  & 88.41\scalebox{0.75}{}  & 68.79\scalebox{0.75}{}  & 74.05\scalebox{0.75}{} \\
\textbf{AAL-MAML++}~\cite{AAL2019ICML}            & N/A           & 88.40\scalebox{0.75}{}  & \textbf{97.96}\scalebox{0.75}{}  & 70.21\scalebox{0.75}{}  & 88.32\scalebox{0.75}{} \\
\textbf{CACTUs-ProtoNets}~\cite{Hsu2019ICLR}      & ACAI          & 68.12\scalebox{0.75}{}  & 83.58\scalebox{0.75}{}  & 47.75\scalebox{0.75}{}  & 66.27\scalebox{0.75}{} \\
\textbf{CACTUs-MAML}~\cite{Hsu2019ICLR}           & ACAI          & 68.84\scalebox{0.75}{}  & 87.78\scalebox{0.75}{}  & 48.09\scalebox{0.75}{}  & 73.36\scalebox{0.75}{} \\
\textbf{UMTRA}~\cite{UMTRA2019NIPS}               & N/A           & 83.80\scalebox{0.75}{}  & 95.43\scalebox{0.75}{}  & 74.25\scalebox{0.75}{}  & 92.12\scalebox{0.75}{} \\
\hline
\textbf{ULDA-ProtoNets}(\textbf{ours})               & N/A           & \textbf{91.00}\scalebox{0.75}{}  & \textbf{98.14}\scalebox{0.75}{}  & \textbf{78.05}\scalebox{0.75}{}  & \textbf{94.08}\scalebox{0.75}{} \\
\textbf{ULDA-MetaOptNet}(\textbf{ours})            & N/A           & \textbf{90.51}\scalebox{0.75}{}  & 97.60\scalebox{0.75}{}  & \textbf{76.32}\scalebox{0.75}{}  & \textbf{92.48}\scalebox{0.75}{} \\
\hline
\multicolumn{6}{c}  {\textit{Supervised (Upper Bound)}} \\
\hline
ProtoNets               & N/A           & 98.35\scalebox{0.75}{} & 99.58\scalebox{0.75}{}   & 95.31\scalebox{0.75}{}  & 98.81\scalebox{0.75}{} \\
MAML                    & N/A           & 94.46\scalebox{0.75}{} & 98.83\scalebox{0.75}{}   & 84.60\scalebox{0.75}{}  & 96.29\scalebox{0.75}{} \\
\bottomrule[1pt]
\end{tabular}
\end{table*}


\begin{algorithm}[t]
\caption{The main sampling strategy in ULDA}
\label{algorithm}
\hspace*{0.02in}{\bf require:}
: class-count, : meta-test size, : episodic number\\
\hspace*{0.02in}{\bf require:}
: unlabeled auxiliary set\\
\hspace*{0.02in}{\bf require:}
: two sets of different augmentation operators
\begin{algorithmic}[1]
\For{ = }
\State Sample  data-points  from .
\State Randomly assign labels to sampled data-points: =.
\State Generate support set  by using operator sampled from  to augment each sample in .
\State Generate query set  by using  operator sampled from  to augment each sample in .
\State 
\EndFor
\State \Return 
\end{algorithmic}
\end{algorithm}

\noindent\textbf{DSTIM.}
Inspired by the recent works of generating new examples near the boundary of a classifier in \cite{Zhang2018ICLRmixup,Qiao2019ICCV}, we originally propose a task-level augmentation technique which is termed as \emph{Distribution Shift-based Task Internal Mixing (DSTIM)}. DSTIM is a simple yet effective method consisting of two augmentation operators  and , both of which perform convex combination differently between all images in the operated data set. To be specific, for each instance  in support (or query) set, we randomly select another instance  from the same set.  synthesizes a new example  as follows:

where , so . This means  can extends the distribution of the synthesized example to the margin of the selected two examples. In contrast, for , the synthesized example  can be obtained as below:

where , so . 
 can generate a new instance by performing subtraction between two images. And this operation can extend to get away from other examples.


Combining with these two operators, we can extend the distribution of raw examples to two opposite directions which thus strengthen the distribution shift between the two operated sets. Moreover, as we keep the value of  between 0.5 and 1.5, this will leads to the synthetic label  rather than , so it is an identity-preserved augmentation. In this work, we use  to augment images in the support set and  for the query set. 

\iffalse
Inspired by the recent works of generating new examples near the boundary of a classifier in \cite{Zhang2018ICLRmixup,Qiao2019ICCV}, we originally propose a task-level augmentation technique which is termed as \emph{Distribution Shift-based Task Internal Mixing (DSTIM)}. DSTIM is a simple yet effective method consisting of two augmentation operators  and . These two operators perform convex combination differently between all images in the operated data set. To be specific, for each instance  in support (or query) set, we randomly select another instance  from the same set and synthesize a new example  as follows:

where for , , so , while for , , so . When ,  can generate a new instance by performing subtraction on two images. 
Note that, both  and  are input images rather than features. We process each instance a few times with Eq. (\ref{eqn:dstim}) to form a new task with more distribution shifts between the augmented support set and query set. In this work, we use  to augment images in the support set and  for the query set. Basically, DSTIM extends the distribution of raw task by incorporating the prior that if two examples are similar to each other in the original pixel space, then it is possible that they are closer in the feature space. The  operator extends the distribution to the margin of two examples whilst the  operator extends to get away from other examples. Besides, as we keep the value of  larger than 0.5, this will leads to the synthetic label  rather than , so it is an identity-preserved augmentation.
\fi


\noindent\textbf{Metric-based FSL Module.}
Metric-based few-shot learning algorithms are a kind of simple and effective methods to address the few-shot problems, which aim to enhance the discriminability of learned feature representations via deep metric learning. The main component of these algorithms is a feature extractor , which is a convolutional neural network (CNN) with parameters . Given an episode (few-shot task) , the feature extractor will map each image  in  into a -dimensional feature, \textit{i.e.,} . In the learned feature space, the images in query set are forced to a labeled image in support set when they share similar semantic information~\cite{Sung2018CVPR,Li2019DN4}. Normally, Euclidean distance or cosine distance is employed to measure the distance or similarity between two examples. As the feature extractor plays a key role in the final classification results, the diversity of the augmented examples is crucial to exhibit the feature extractor to extract discriminative features. Crucially, our proposed ULDA framework can just satisfy this purpose, by increasing the distribution diversity between the augmented support set and query set. Therefore, to construct an effective unsupervised few-shot learning model, we tailor our ULDA into a representative existing metric-based few-shot learning algorithm, ProtoNets~\cite{Snell2016NIPS}, and name this new model as ULDA-ProtoNets. Obviously, our ULDA framework is universal and extensible, which can be simply tailored to other existing few-shot models. This part will be further discussed in Section~\ref{sec:extension}.






Given a \emph{N}-way \emph{K}-shot episode , ProtoNets computes the ``prototype'' via averaging features for each class in the support set with the feature extractor :

where  and . These ``prototypes'' are used to build a simple nearest neighbor classifier. Then, given a new image  from query set, the classifier outputs a normalized classification score computed with Euclidean distance for each class :

where . So, the image  will be classified to its closest prototype. The few-shot loss function  for updating the parameter  is formalized as:



Note that, the distance between  and its corresponding prototype will not change if we keep . And this makes no sense to secure the discriminability of the feature extractor. Besides, as we use rotation as an augmentation technique, we can also incorporate with a self-supervised loss  to predict the rotation angle. 

where  is the parameters of an additional classifier for predicting the rotation angle for each query image  from . Specifically, this classifier is implemented by one fully connected layer.


Therefore, the overall loss function can be formulated as:

where  is a balancing hyper-parameter.


\begin{table*}[!tp]\small
\centering
\extrarowheight=-1pt
\caption{Unsupervised few-shot classification results (\%) under -way -shot (\ie~(N,~K)) setting on \emph{mini}ImageNet. ``-'' means the results are not reported in their source papers.}
\label{tab:miniImageNet}
\vspace{-0.2cm}
\begin{tabular}{p{185pt}<{\raggedright}p{50pt}<{\raggedright}p{45pt}<{\centering}p{45pt}<{\centering}p{45pt}<{\centering}p{45pt}<{\centering}}
\toprule[1pt]
\textbf{Algorithms} &  \textbf{Clustering} & \textbf{(5, 1)} & \textbf{(5, 5)} &  \textbf{(5, 20)} & \textbf{(5, 50)} \\
\hline
\textbf{Training from scratch}                    & N/A           & 27.59\scalebox{0.75}{}  & 38.48\scalebox{0.75}{}  & 51.53\scalebox{0.75}{}  & 59.63\scalebox{0.75}{} \\
\hline
\small{\textbf{-nearest neighbors}}  & DeepCluster   & 28.90\scalebox{0.75}{}  & 42.25\scalebox{0.75}{}  & 56.44\scalebox{0.75}{}  & 63.90\scalebox{0.75}{} \\
\textbf{linear classifier}                        & DeepCluster   & 29.44\scalebox{0.75}{}  & 39.79\scalebox{0.75}{}  & 56.19\scalebox{0.75}{}  & 65.28\scalebox{0.75}{} \\
\textbf{MLP with dropout}                         & DeepCluster   & 29.03\scalebox{0.75}{}  & 39.67\scalebox{0.75}{}  & 52.71\scalebox{0.75}{}  & 60.95\scalebox{0.75}{} \\
\textbf{cluster matching}                         & DeepCluster   & 22.20\scalebox{0.75}{}  & 23.50\scalebox{0.75}{}  & 24.97\scalebox{0.75}{}  & 26.87\scalebox{0.75}{} \\
\textbf{AAL-ProtoNes}~\cite{AAL2019ICML}          & N/A           & 37.67\scalebox{0.75}{}  & 40.29\scalebox{0.75}{}  & -      & - \\
\textbf{AAL-MAML++}~\cite{AAL2019ICML}            & N/A           & 34.57\scalebox{0.75}{}  & 49.18\scalebox{0.75}{}  & -      & - \\
\textbf{CACTUs-ProtoNets}~\cite{Hsu2019ICLR}      & DeepCluster   & 39.18\scalebox{0.75}{}  & 53.36\scalebox{0.75}{}  & 61.54\scalebox{0.75}{}  & 63.55\scalebox{0.75}{} \\
\textbf{CACTUs-MAML}~\cite{Hsu2019ICLR}           & DeepCluster   & 39.90\scalebox{0.75}{}  & 53.97\scalebox{0.75}{}  & \textbf{63.84}\scalebox{0.75}{}  & \textbf{69.64}\scalebox{0.75}{} \\
\textbf{UMTRA}~\cite{UMTRA2019NIPS}               & N/A           & 39.93\scalebox{0.75}{}  & 50.73\scalebox{0.75}{}  & 61.11\scalebox{0.75}{}  & 67.15\scalebox{0.75}{} \\
\hline
\textbf{ULDA-ProtoNets}(\textbf{ours})              & N/A           & \textbf{40.63}\scalebox{0.75}{}  & \textbf{56.18}\scalebox{0.75}{}  & \textbf{64.31}\scalebox{0.75}{}  & 66.43\scalebox{0.75}{} \\
\textbf{ULDA-MetaOptNet}(\textbf{ours})           & N/A           & \textbf{40.71}\scalebox{0.75}{}  & \textbf{54.49}\scalebox{0.75}{}  & 63.58\scalebox{0.67}{}  & \textbf{67.65}\scalebox{0.67}{} \\
\hline
\multicolumn{6}{c}  {\textit{Supervised (Upper Bound)}} \\
\hline
ProtoNets               & N/A           & 46.56\scalebox{0.75}{} & 62.29\scalebox{0.75}{}   & 70.05\scalebox{0.75}{}  & 72.04\scalebox{0.75}{} \\
MAML                    & N/A           & 46.81\scalebox{0.75}{} & 62.13\scalebox{0.75}{}   & 71.03\scalebox{0.75}{}  & 75.54\scalebox{0.75}{} \\
\bottomrule[1pt]
\end{tabular}
\end{table*}

\begin{table*}[!tbp]\small
\begin{center}
\caption{Unsupervised few-shot classification results in \% of -way -shot (N,~K) learning methods on \emph{tiered}ImageNet.}
\vspace{-0.2cm}
\label{tab:tieredImageNet}
\small{
\begin{tabular}{p{185pt}<{\raggedright}p{50pt}<{\raggedright}p{45pt}<{\centering}p{45pt}<{\centering}p{45pt}<{\centering}p{45pt}<{\centering}}
\toprule[1pt]
\textbf{Algorithms}       & \textbf{Clustering}  & \textbf{(5, 1)} & \textbf{(5, 5)} &  \textbf{(5, 20)} & \textbf{(5, 50)} \\
\hline
{\textbf{Training from scratch}}  & N/A  & 26.27\scalebox{0.75}{} & 34.91\scalebox{0.75}{} & 38.14\scalebox{0.75}{} & 38.67\scalebox{0.75}{} \\
\hline
{\textbf{ULDA-ProtoNets(ours)}}          & N/A  & \textbf{41.60}\scalebox{0.67}{} & \textbf{56.28}\scalebox{0.67}{} & \textbf{64.07}\scalebox{0.67}{} & \textbf{66.00}\scalebox{0.67}{} \\
{\textbf{ULDA-MetaOptNet(ours)}}         & N/A  & \textbf{41.77}\scalebox{0.67}{} & \textbf{56.78}\scalebox{0.67}{} & \textbf{67.21}\scalebox{0.67}{} & \textbf{71.39}\scalebox{0.67}{} \\
\hline
\multicolumn{6}{c}  {\emph{Supervised (Upper Bound)}} \\
\hline
ProtoNets         & N/A  & 46.66\scalebox{0.75}{} & 66.01\scalebox{0.75}{} & 77.62\scalebox{0.75}{} & 81.70\scalebox{0.75}{} \\
MetaOptNet        & N/A  & 47.32\scalebox{0.75}{} & 66.16\scalebox{0.75}{} & 77.68\scalebox{0.75}{} & 80.61\scalebox{0.75}{} \\
\bottomrule[1pt]
\end{tabular}
}
\end{center}
\end{table*}


\subsection{Extension to Optimization-based FSL}
\label{sec:extension}
Different from the metric-based FSL algorithms, the optimization-based FSL algorithms strive for enhancing the flexibility of a few-shot model such that it can be readily updated using a few labeled examples. Most of these algorithms are generally based on meta learning. See Section \ref{sec:related_work} for more details. To further verify the effectiveness and scalability of our proposed ULDA framework, we extend ULDA to a recently proposed optimization-based FSL algorithm, \ie~MetaOptNet~\cite{LeeCVPR2019meta}, and name this new method as ULDA-MetaOptNet in the following parts.





\section{Experiments}
\label{sec:experiments}

In this section, we detail the experimental settings and compare our ULDA with the state-of-the-art approaches on two challenging datasets, \ie~Omniglot~\cite{lake2011one} and \emph{mini}ImageNet~\cite{Vinyals2016NIPS}, which are widely used in the literature.


\subsection{Experimental Setting}
\noindent\textbf{Datasets.} 
The \textbf{Omniglot} dataset comprises 1623 characters from 50 different alphabets. Each character contains 20 instances written by different persons. We follow the experiment protocol described by~\cite{AAL2019ICML}: classes 1-1150, 1150-1200 and 1200-1623 are used for training, validation and test, respectively.

The \textbf{\emph{mini}ImageNet} is the most popular benchmark in the field of few-shot learning, which was introduced in~\cite{Vinyals2016NIPS}. It is composed of 100 classes selected from ImageNet~\cite{Alex2012NIPS}, and each class contains 600 images with the size of . We follow the data splits proposed by~\cite{RaviICLR2017}, which splits the total 100 classes into 64 classes for training, 16 classes for validation and 20 classes for test, respectively. 


\noindent\textbf{Backbone network.}
We employ a four-layer convolutional neural network as the feature extractor backbone, which is widely adopted in the few-shot learning literature~\cite{Snell2016NIPS,FinnICML2017}. Each layer comprises a 64 filters ( kernel) convolutional layer, a batch normalization layer, a ReLU layer and a  max-pooling layer. Moreover, different ResNet \cite{He2016ResNet} architectures are also employed to validate the expansibility of our framework.

\begin{figure*}[!tbp]
\begin{center}
  \includegraphics[width=0.90\textwidth]{figs/train_test_total1.pdf}
\end{center}
\vspace{-0.3cm}
  \caption{The train and test accuracy curves on the 5-way 1-shot tasks. As seen, the diverse combinations especially our proposed ULDA (the red lines) enjoys a smaller risk of overfitting and a higher test accuracy.}
  \label{fig:total_train_test_curves}
\end{figure*}

\noindent\textbf{Training strategy.}
We conduct -way -shot classification tasks on the aforementioned datasets. We randomly sample and construct 10,000 pretext few-shot tasks in each epoch and train our networks for a total of 60 epochs. For \emph{mini}ImageNet, we employ AutoAugment~\cite{Cubuk2019CVPR} to augment the support set and traditional augmentation together with rotation to augment the query set. For Omniglot, we use AutoAugment for support set and random crop for query set. Note that, self-supervised loss is not employed in Omniglot. All backbone networks are optimized by Adam~\cite{Kingma2014Adam}. The initial learning rate is set as 0.001 and multiplied by 0.06, 0.012, 0.0024 after 20, 40, and 50 epochs, respectively. We conduct all the experiments on GTX 2080Ti. For a fair comparison, the hyper parameters in all of these methods are kept to be the same.

\noindent\textbf{Parameter setup.} We set  in Eq.~(\ref{eqn:total_loss_function}). In Eq.~(\ref{eqn:tim_add}), we empirically set =0.8 for  and in Eq.~(\ref{eqn:tim_sub}), =0.6 for . Our model is robust to different values of  according to our experiments (see more details in our supplementary material). Thus, we set it in a slightly different manner following our distribution-diversity argument.


\subsection{Unsupervised Few-shot Learning Results}
To verify the effectiveness of our approach for unsupervised few-shot learning, we compare our framework with the state-of-the-art (SOTA) methods in various settings. Moreover, to make our results more convincing, we randomly sample 1,000 episodes from the test set for evaluation. Also, we take the top-1 mean accuracy as evaluation criterion and repeat this process five times. Besides, the  confidence intervals are also reported.

\iffalse
\begin{figure*}[tbp]
	\centering
	\begin{minipage}{.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth]{figs/com_sep.pdf}
        \vspace{-0.2cm}
		\caption{Comparison between composite augmentation and diverse augmentation on \emph{mini}ImageNet.}
		\label{fig:com_sep}
	\end{minipage}
    \quad
	\begin{minipage}{.47\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth]{figs/KL_FID.pdf}
        \vspace{-0.4cm}
		\caption{The performance changing with the value of distribution divergence on \emph{mini}ImageNet.}
		\label{fig:distribution_difference}
	\end{minipage}
\end{figure*}
\fi


\iffalse
\begin{table*}[tbp]\small
\begin{center}
\caption{5-way 1-shot accuracy (\%) on \emph{mini}ImageNet with different network architectures.}
\label{tab:different_architecture}
\small{
\begin{tabular}{cccccc}
\toprule[1pt]
              & Conv64F  & ResNet12  & ResNet18 & ResNet34 & ResNet50 \\
\hline
ULDA-ProtoNets  & 40.63\scalebox{0.75}{} & \textbf{42.73}\scalebox{0.75}{} & 42.05\scalebox{0.75}{} & 40.48\scalebox{0.75}{} & 39.48\scalebox{0.75}{} \\
\bottomrule[1pt]
\end{tabular}
}
\end{center}
\vspace{-0.4cm}
\end{table*}
\fi



\noindent\textbf{Results on Omniglot.}
The comparative results between a variety of baseline and recently proposed methods on Omniglot are presented in Table \ref{tab:Omniglot}. ULDA shows currently the best results across different tasks. Compared with previous best results, our ULDA-ProtoNets gains 2.6\%, 0.18\%, 3.8\% and 1.96\% under 5-way 1-shot, 5-way 5-shot, 20-way 1-shot and 20-way 5-shot settings, respectively. Similarly, our ULDA-MetaOptNet can also achieve very competitive results especially in the 5-way 1-shot and 20-way 1-shot settings.




\noindent\textbf{Results on \emph{mini}ImageNet.}
The experimental results on \emph{mini}ImageNet are summarized in Table \ref{tab:miniImageNet}. Our ULDA achieves the state-of-the-art results on both 5-way 1-shot, 5-way 5-shot and 5-way 20-shot settings and achieves competitive results on 5-way 50-shot settings. 
Besides, the results of ULDA are very close to the results of supervised few-shot learning approaches with a labeled auxiliary set, \ie~ProtoNets and MAML. Note that, when using the same few-shot learning algorithm (\ie~ProtoNets), our ULDA framework outperforms all other methods across different classification tasks. Compared with CACTUs-ProtoNets, our ULDA-ProtoNets gains 1.45\%, 2.82\%, 2.77\%, 2.88\% performance boost under 5-way 1-shot, 5-shot, 20-shot and 50-shot settings, respectively. The reason is that CACTUS uses clustering algorithms to obtain the pseudo labels before constructing few-shot tasks, but the quality of these pseudo labels will limit the final results. In contrast, our ULDA does not have this limitation. When compared with AAL, which is the closest work to ours, our ULDA can still achieve 2.96\% and 15.89\% performance boost for 5-way 1-shot and 5-way 5-shot, respectively.


\textbf{Results on \emph{tiered}ImageNet.}
We turn to \emph{tiered}ImageNet, a more challenging dataset, which contains more complex classes and examples than \emph{mini}ImageNet. Since the recent unsupervised few-shot leaning methods (\ie~CACTUs, UMTRA) did not report their results on this dataset, we only compare our methods with the baseline method \textit{training from scratch}. The results are illustrated in Table \ref{tab:tieredImageNet}. Our ULDA performs much better than learning from scratch and slightly weaker than the supervised methods.




\begin{figure}[!tbp]
\centering
\includegraphics[width=0.40\textwidth]{figs/com_sep.pdf}
\vspace{-0.3cm}
\caption{Comparison between composite augmentation and diverse augmentation on \emph{mini}ImageNet.}
\label{fig:com_sep}
\end{figure}

\begin{figure}[!tbp]
\centering
\includegraphics[width=0.40\textwidth]{figs/KL_FID.pdf}
\vspace{-0.3cm}
\caption{The performance changing with the value of distribution divergence on \emph{mini}ImageNet.}
\label{fig:distribution_difference}
\vspace{-0.2cm}
\end{figure}


\subsection{Ablation Study on \emph{mini}ImageNet}
\label{sec:ablation_study}
\iffalse
\begin{table}[htbp]\small
\label{tab:ablation_study}
\begin{center}
\caption{\textbf{Ablation Study.} The various blocks we employ to improve the test accuracy (\%) on 5-way \emph{mini}ImageNet benchmark.}
\small{
\begin{tabular}{p{55pt}<{\centering}p{40pt}<{\centering}p{40pt}<{\centering}p{40pt}<{\centering}|p{55pt}<{\centering}p{55pt}<{\centering}}
\toprule[1pt]
\textbf{Separate Aug.} & \textbf{Rotate} & \textbf{SSL loss} &  \textbf{DSTIM} & \textbf{(5, 1)} & \textbf{(5, 5)}\\
\hline
           &            &            &            & 32.58\scalebox{0.75}{} & 44.40\scalebox{0.75}{} \\
\checkmark &            &            &            & 35.37\scalebox{0.75}{} & 49.16\scalebox{0.75}{} \\
           & \checkmark &            &            & 34.85\scalebox{0.75}{} & 46.88\scalebox{0.75}{}\\
\checkmark & \checkmark &            &            & 39.28\scalebox{0.75}{} & 53.55\scalebox{0.75}{} \\
\checkmark & \checkmark & \checkmark &            & 40.32\scalebox{0.75}{} & 54.91\scalebox{0.75}{} \\
\checkmark & \checkmark & \checkmark & \checkmark & \textbf{40.63}\scalebox{0.67}{} & \textbf{55.41}\scalebox{0.67}{} \\
\bottomrule[1pt]
\end{tabular}
}
\vspace{0.2cm}
\end{center}
\vspace{-0.5cm}
\end{table}
\fi



\iffalse
\begin{figure}[!tbp]
\label{fig:distribution_difference}
\centering
\includegraphics[width=0.45\textwidth]{figs/KL_divergence_edge.pdf}
\includegraphics[width=0.45\textwidth]{figs/FID_edge.pdf}
\caption{The result with different distribution margin between support and query set. The geometries with yellow outline are augmented with the same method.}
\vspace{-0.5cm}
\end{figure}
\fi



\noindent\textbf{The Overfitting Problem}
In these series of experiments, we study the overfitting problem of different diverse augmentation combinations during the model learning procedure. The results on \emph{mini}ImageNet under 5-way 1-shot are shown in Figure \ref{fig:total_train_test_curves}. Here, ULDA is the proposed framework in this paper, which employs AutoAugment to generate support set and combines traditional augmentation with rotation to generate query set. Moreover, DSTIM is also employed here. All results are averaged among 1,000 tasks. As expected, diverse augmentation can efficiently alleviate the over-fitting problem. Moreover, when incorporated with our proposed augmentation method DSTIM, the distribution difference between query set and support set can be further enlarged, \ie the generated pretext few-shot tasks enjoy more challenges, which can effectively alleviate the overfitting problem in unsupervised learning manner. As seen in Figure \ref{fig:total_train_test_curves}, our proposed ULDA obtains a lower train accuracy curves but meanwhile a relative higher test accuracy curves.

\noindent\textbf{Composite Augmentation vs. Diverse Augmentation.}
Another way to alleviate the overfitting problem in unsupervised FSL is that we can compose different augmentation operators together (\ie~a larger augmentation operator set ) to increase the whole diversity of the generated samples as introduced in \cite{Chen2020SimCLR}, but we still adopt the same  to augment both the query and support set. We call this \textit{composite augmentation}. Differently, our ULDA employs a \textit{diverse augmentation}, \ie~augmenting the query set and support set separately. To figure out the difference between these two augmentation ways, we conduct a serial of experiments on \emph{mini}ImageNet (see Figure \ref{fig:com_sep}). When we employ more complex operators, both the diverse augmentation and composite augmentation boost the performance. Notably, the diverse augmentation always performs better than the composite augmentation. It shows that the former can gain more distribution shift, which is more beneficial for alleviating the overfitting problem. 




\begin{table}[tbp]\footnotesize
\begin{center}
\tabcolsep=2.3pt
\caption{5-way 1-shot accuracy (\%) on \emph{mini}ImageNet with different network architectures.}
\label{tab:different_architecture}
\vspace{-0.3cm}
\begin{tabular}{cccccc}
\toprule[1pt]
                & ResNet12  & ResNet18 & ResNet34 & ResNet50 \\
\hline
ULDA-ProtoNets  & \textbf{42.73}\scalebox{0.75}{} & 42.05\scalebox{0.75}{} & 40.48\scalebox{0.75}{} & 39.48\scalebox{0.75}{} \\
\bottomrule[1pt]
\end{tabular}
\end{center}
\vspace{-0.3cm}
\end{table}

\noindent\textbf{Comparisons with different backbones.}
We further perform a series of experiments on ResNets~\cite{He2016ResNet} with different depths. Note that the settings are kept almost the same as the above experiments expect the learning rate. We set the learning rate to 0.1 following \cite{LeeCVPR2019meta}. The results are reported in Table~\ref{tab:different_architecture}. As seen, our ULDA can achieve much higher results with much deeper networks, \textit{e.g.,} ResNet12 and ResNet18. For example, when using ResNet12 as the backbone, our ULDA-ProtoNets can even further gain  improvements over a Conv64F-based version, which is also significantly better than other SOTA methods.
However, the performance of our ULDA-ProtoNets begins to drop with ResNet18/ResNet34/ResNet50, which indicates that these models suffer from a new overfitting risk. We may need to further increase the diversity between the constructed tasks. We leave this as our future work. 



\noindent\textbf{Effectiveness of distribution shift-based augmentation module.} Despite the promising results achieved by our entire framework, we also expect to know how it works, especially the relationship between the distribution shift in generated two sets and the final results.
With this purpose, we employ the aforementioned augmentation techniques (\ie~random crop, color jittering, rotation, AutoAugment and our proposed DSTIM) and combine them in various ways to produce these two sets with different distribution shift. Besides, we use Kullback-Leibler divergence (KL divergence) and Fr\'echet Inception Distance (FID)~\cite{Heusel2017NIPS} to evaluate the distribution difference. The results are illustrated in Figure \ref{fig:distribution_difference}.
We can draw the conclusion from these results that the models tend to perform much better when trained on pretext few-shot tasks that have large distribution difference. 





\begin{figure}[!tbp]
\centering
\includegraphics[width=0.42\textwidth]{figs/tsne_fea.pdf}
\vspace{-0.3cm}
\caption{t-SNE plots in feature space. (a) common augmentation, (b) our ULDA. Zoom in for best visual effect.}
\label{fig:tsne_fea}
\vspace{-0.2cm}
\end{figure}




In order to intuitively show the effect of our framework, we also visualize the augmentation effect in feature space in Figure \ref{fig:tsne_fea}. As seen, when augmenting support set and query set with the same augmentation techniques, the generated query set gathers tightly around support set, and these tend to exist heavy overlap in these augmented data-points. However, with our ULDA, the generated examples share more diversity and more distribution difference between the support and query set. We will analyze this issue in supplementary material.



\section{Conclusion}
In this paper, we present an unsupervised few-shot learning framework that aims to increase the diversity of generated few-shot tasks based on data augmentation. We argue that when strengthening the distribution shift between the support set and query set in each few-shot task with different augmentation techniques can increase the generalization ability for model training. A serial of experiments have been conducted to demonstrate the correctness of our finding. We also incorporate our framework with two representative few-shot learning algorithms, \ie~ProtoNets and MetaOptNet, and achieve the state-of-the-art results across a variety of few-shot learning tasks established on Omniglot and \emph{mini}ImageNet.




\appendix   \setcounter{table}{0}   \setcounter{figure}{0}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}
\numberwithin{figure}{section}
\numberwithin{table}{section}


\section{The Comparison of Different Diverse Augmentation Combinations}
To verify the diverse augmentation which we claim on, we do a serial of experiments. Here, we employ the aforementioned augmentation techniques (\ie~TA, AA, R and our proposed DSTIM) and combine them in various ways to produce different distribution shift when constructing query and support set. Besides, we use Kullback-Leibler divergence (KL divergence) and Fr\'echet Inception Distance (FID) to measure the distribution difference. Also, the results in Table \ref{tab:difference_compare_augmentation} are the detailed values of Figure 5 in our original paper.
\begin{table}[htbp]\footnotesize
\caption{The comparison with different augmentation methods on \emph{mini}ImageNet. The results in \% of -way -shot (, ) are reported.}
\label{tab:difference_compare_augmentation}
\begin{center}
\begin{tabular}{p{40pt}<{\raggedright}p{45pt}<{\raggedright}p{13pt}<{\centering}p{16pt}<{\centering}p{30pt}<{\centering}p{30pt}<{\centering}}
\toprule[1pt]
\textbf{} &  \textbf{} & \textbf{KL} & \textbf{FID} &  \textbf{(5, 1)} & \textbf{(5, 5)} \\
\hline
TA & TA   & 0.00 & 16.07  & 32.58\scalebox{0.75}{}  & 44.40\scalebox{0.75}{} \\ AA & AA   & 0.15 & 19.52  & 31.53\scalebox{0.75}{}  & 41.83\scalebox{0.75}{} \\ TA & AA   & 0.41 & -      & 34.07\scalebox{0.75}{}  & 47.31\scalebox{0.75}{} \\ AA & TA   & 0.46 & 133.97 & 35.37\scalebox{0.75}{}  & 49.16\scalebox{0.75}{} \\ AA & R    & 0.73 & 183.06 & 39.18\scalebox{0.75}{}  & 53.30\scalebox{0.75}{} \\ AA & R+TA & 2.66 & 172.22 & 39.28\scalebox{0.75}{}  & 53.55\scalebox{0.75}{} \\ AA & R+    & 2.93 & 181.14 &  39.42\scalebox{0.75}{} & 53.87\scalebox{0.75}{} \\ AA+ & R+     & 4.45   & 185.27  &39.52\scalebox{0.75}{} & 54.26\scalebox{0.75}{} \\ AA+ & R+TA+  & \textbf{5.33} & \textbf{202.42} & \textbf{39.64}\scalebox{0.75}{}  & \textbf{54.37}\scalebox{0.75}{} \\ \bottomrule[1pt]
\end{tabular}
\end{center}
\vspace{-0.2cm}
\end{table}

\section{Feature Representation}

In order to intuitively show the effect of our framework, we also visualize the augmentation efficacy in feature space in Figure \ref{fig:tsne_feature}. We find that, when augmenting support set and query set using the same augmentation technique, the generated query set gathers tightly around support set, and this case tends to exist heavy overlap in these augmented data-points. However, by using our approach, the generated examples share more diversity and more distribution difference between the support set and query set.

\begin{figure}[!tbp]
\begin{center}
  \includegraphics[width=0.48\textwidth]{figs/transforms_feature2.pdf}
\end{center}
  \caption{Visualization of feature transformations in generated support images and query images. Same color means generated from the same data-point. The generated images own more diversity and there exist little overlap between generated support images and query images via our approach.}
    \label{fig:tsne_feature}
\end{figure}

\section{The Value of \texorpdfstring{}~~in DSTIM}
Our default setting is  for  and  for . The performance remains stable with using different values of . The results are shown in Table \ref{tab:different_lambda}. 


\begin{table}[h]\small
\caption{The comparison with different augmentation methods on \emph{mini}ImageNet. The results in \% of -way -shot (, ) are reported.}
\label{tab:different_lambda}
\vspace{-0.3cm}
\begin{center}
\begin{tabular}{cccc}
\toprule[1pt]
 for  &   for  & \textbf{(5, 1)} & \textbf{(5, 5)} \\
\hline
0.6 & 0.6 & 40.08  & 54.33  \\
0.6 & 0.8 & 40.06  & 54.47  \\
0.8 & 0.6 & \textbf{40.63}  & \textbf{55.41}  \\
0.8 & 0.8 & 39.92  & 54.76  \\
\bottomrule[1pt]
\end{tabular}
\end{center}
\vspace{-0.25cm}
\end{table}





\iffalse
\section*{Broader Impact}

This work has the following potential positive impact in the society: (1) motivate the investigation of unsupervised few-shot learning which is still an open problem, (2) motivate the systematical study of data augmentation and the specialization of employed augmentation techniques according to different tasks. At the same time, this work may have some negative consequences because the application of unsupervised learning is still in its early stage, where more studies are required for its deployment.
\fi


\newpage
\bibliography{egbib}











\end{document}
