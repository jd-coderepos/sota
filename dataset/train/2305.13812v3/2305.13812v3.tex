\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{emnlp2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{booktabs} \usepackage{fancyhdr}  

\newcommand{\methodcomp}{MosaiCLIP}
\newcommand{\methodcompbold}{\textbf{MosaiCLIP}}
\newcommand{\methodcompshort}{MC}
\newcommand{\methodcompNoCurric}{MosaiCLIP\textsubscript{NoCurric}}
\newcommand{\methodcompNoCurricbold}{\textbf{MosaiCLIP\textsubscript{NoCurric}}}
\newcommand{\methodcompwiseft}{MosaiCLIP\textsubscript{WiSE-FT}}
\newcommand{\methodcompwiseftbold}{\textbf{MosaiCLIP\textsubscript{WiSE-FT}}}

\newcommand{\clip}{CLIP}
\newcommand{\negclip}{NegCLIP}
\newcommand{\todo}[1]{{\color{red} TODO: #1}}
\newcommand{\chk}[1]{{\color{blue}#1}}


\usepackage{graphicx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{float}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage[normalem]{ulem}
\renewcommand\theadalign{l}

    \makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or *\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
    \makeatother

\title{Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality}

\author{
    Harman Singh\thanks{\hspace{0.3em} Work done while at Meta.}\hspace{0.3em},  
    \textbf{Pengchuan Zhang},
    \textbf{Qifan Wang}\textbf{,} \\
    \textbf{Mengjiao Wang}\textbf{,}
    \textbf{Wenhan Xiong}\textbf{,}
    \textbf{Jingfei Du}\textbf{,}
    \textbf{Yu Chen}
    \\
    Meta AI \quad
    Anytime.AI\\
    \texttt{harmansingh.iitd@gmail.com} \\
    \texttt{\{pengchuanzhang, wqfcr, mengjiaow, xwhan, jingfeidu\}@meta.com} \\
    \texttt{ychen@anytime-ai.com}
}

\begin{document}
\maketitle

\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\uv}{\boldsymbol{u}}
\newcommand{\vv}{\boldsymbol{v}}
\newcommand{\xv}{\boldsymbol{x}}
\newcommand{\tv}{\boldsymbol{t}}
\newcommand{\gv}{\boldsymbol{g}}

\begin{abstract}
Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graph-structured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images and text that aligns sentences of various complexities to the same image. We also introduce novel negative mining techniques in the scene graph space for improving attribute binding and relation understanding. Through extensive experiments, we demonstrate the effectiveness of our approach that significantly improves attribute binding, relation understanding, systematic generalization, and productivity on multiple recently proposed benchmarks (For example, improvements 
up to  for systematic generalization,  for relation understanding over a strong baseline), while achieving similar or better performance than CLIP on various general multimodal tasks.

\end{abstract} \section{Introduction}
\begin{figure}[h!]
    \centering
    \begin{minipage}{0.53\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/v2Main_figure_1_compclip.png}
        \subcaption{}
    \end{minipage}
    \begin{minipage}{0.44\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/systematicity_avg_all_pretraining.png}
        \subcaption{}
        \includegraphics[width=\columnwidth]{figures/aro_avg_3datasetsavg_finetuning.png}
        \subcaption{}
    \end{minipage}
    \caption{\textbf{(Left)} a) A typical example from the ARO benchmark for testing attribute understanding of VLMs. VLMs struggle with matching the image to the correct caption (in green). \textbf{(Right)} Average scores of \methodcomp{} (our method) compared with \negclip{} and \clip{} on prominent compositionality benchmarks for measuring b) Systematic Generalization c) Attribute, Relation, and Word Order understanding.}
    \label{fig:overview_results}
    \vspace{-0.5cm}
\end{figure}

Recent progress in contrastive learning using large-scale image-text data for joint image-text representation learning has led to Vision-Language models (VLMs) like CLIP \citep{radford2021learning} and ALIGN \citep{jia2021scaling} that show remarkable zero-shot classification and retrieval capabilities. However, recent works have shown that these models struggle at compositional reasoning \cite{yuksekgonul2022and, thrush2022winoground, ma2022crepe}. In particular, they struggle with binding correct attributes to the correct objects, understanding relations between objects, generalizing systematically to unseen combinations of concepts and to larger and more complex sentences.

Some works have made progress on this problem. \citet{yuksekgonul2022and} show that hard negative mining of images and text during fine-tuning is a promising first step to improving compositionality. However, performance gains are highly dependent on how clean the training data is, and generalizing to unseen combinations of concepts remains a challenge. \citet{doveh2023teaching} use LLMs for hard negative mining and \citet{cascantebonilla2023going} explore using synthetic datasets to improve compositional understanding in VLMs. Synthetic datasets lead to a domain gap compared to natural datasets. We aim to develop a general-purpose approach for improving compositionality of all such contrastively trained VLMs.

In this paper, we consider a scene graph representation of the image and text. We observe that multiple sub-graphs of the text scene graph with different semantic complexities can be matched with the same image. Performing this matching improves fine-grained and hierarchical understanding of text and thereby, of images. 
We achieve this by developing a scene graph-based text decomposition strategy that creates a scene graph for any given text, decomposing it into sub-graphs, and matching an image to multiple sentences derived from these sub-graphs (See Fig. \ref{fig:method_overview} for an overview). Each sub-graph represents a distinct part of the image, aligning well with CLIP's original image-text matching objective. Focused on improving {attribute binding} and {relation understanding}, we develop novel hard negative graph creation strategies which helps VL contrastive learning.
We provide a novel Image-to-Multi-Text contrastive loss for matching individual images to multiple sentences. Our approach of matching texts of different complexity (from coarse-grained to fine-grained) to the image leads to fine-grained and hierarchical text understanding. Our resulting model is \methodcomp{}.

Our approach leads to significant improvements across compositionality benchmarks. For example, Figure \ref{fig:overview_results} b) and c) shows that MosaiCLIP improves performance by  and  on CREPE and ARO dataset over a strong baseline and by  over CLIP. \textbf{Our contributions encompass:}
\begin{itemize}\item A novel graph-based text decomposition and augmentation framework and a coarse-to-fine contrastive learning objective for matching images to text sub-graphs of varying complexity.
    \item Hard-negative mining techniques using graph transformations of the text scene graphs, that are seamlessly coupled with our text decomposition strategy, and applied over any text. 
    \item A thorough \textit{analysis} for understanding why \methodcomp{} improves vision-language compositionality, disentangling the effect of image and text encoders and providing a novel tree-score based analysis showing that \methodcomp{} exhibits improved hierarchical text understanding.
    \item Extensive experiments over three model architectures, two pre-training datasets, three fine-tuning datasets and test over four compositionality benchmarks (11 datasets) to prove the efficacy of \methodcomp{} for improving compositionality.
\end{itemize} \section{Related Work}
\label{sec_related}
\paragraph{Contrastive Vision-Language Pre-training:}
Large-scale contrastive learning for Vision and Language is utilized to create models like CLIP \citep{radford2021learning} and ALIGN \citep{jia2021scaling}. These models showcase impressive performance on a variety of tasks, including image classification, text and image retrieval, image captioning \citep{mokady2021clipcap}, object detection \citep{zhong2022regionclip, li2022grounded} etc.
\paragraph{Visio-Linguistic Compositionality:}
Various studies have introduced benchmarks for assessing the compositional reasoning abilities of vision-language foundation models (VLMs). For instance, Winoground \citep{thrush2022winoground} is a handpicked collection of 400 test cases, each comprising two images and two sentences. Sentences have the same word content and differ in word-order. \citet{diwan-etal-2022-winoground} show that the Winoground dataset tests additional challenges along with compositionality, including handling ambiguous image-text pairs and unusual examples. \citet{yuksekgonul2022and} proposed the ARO benchmark for probing VLMs ability to understand Attribute, Relations, and Word-Order. \citet{ma2022crepe} proposed CREPE for measuring two aspects of compositionality: systematic generalization and productivity. All benchmarks suggest that contrastively trained VLMs have severe difficulty in compositional reasoning. As a remedy, NegCLIP \citep{yuksekgonul2022and} and Teaching SVLC \citep{doveh2023teaching} create targeted rule-based and LLM-guided hard negative sentences, SyViC \citep{cascantebonilla2023going} fine-tunes CLIP with million scale synthetic images-text pairs, for improving relational and attribute understanding. We observe that previous methods are either highly dependent on how clean the training data is, use expensive LLM's for data augmentation or use synthetic datasets that require special solutions to resolve the synthetic-to-real domain gap. We hence develop a coarse-to-fine contrastive learning framework that matches images with texts of multiple complexities, which serves as a general-purpose solution to improve fine-grained and hierarchical text understanding, thereby improving compositionality.
\paragraph{Scene Graphs} are structured representations of visual scenes, consisting of objects, their attributes, and relationships between objects. Scene graphs are beneficial for a range of tasks including image retrieval \citep{wu2019unified, johnson2015image}, image captioning \citep{yang2019auto}, and image generation \citep{johnson2018image} among others. \begin{figure*}[h]
    \centering
\subcaptionbox{Text scene graph creation and decomposition (Sec. \ref{subsec_sg_decomposition}).}{\includegraphics[width=\textwidth]{figures/Method_SG_bird_construction_nosketch_v2.png}}
    \hfill
    \subcaptionbox{Hard negative sub-graph creation from positive sub-graphs (Sec. \ref{subsec_neg_graph_creation}).}{\includegraphics[width=0.33\textwidth]{figures/negative_graphs.png}}
    \subcaptionbox{Coarse-to-fine contrastive learning (Sec. \ref{subsec_coarsetofine_loss}).}{\includegraphics[width=0.62\textwidth]{figures/MosaiCLIP_method_v2.png}}
    \caption{Overview of our approach. \textbf{a)} Depiction of the scene graph of an image (hypothetical) and a scene graph parsed from text. The text scene graph is a sub-graph of the image scene graph. The text scene graph is decomposed into sub-graphs from which \textbf{b)} minimally perturbed hard-negative sub-graphs are created. \textbf{c)} The Ground truth similarity matrix used for a batch of data during contrastive learning. Solid boxes represent a match between the image and the corresponding text. Different from CLIP, each image can be matched to multiple texts in our method.}
    
    \label{fig:method_overview}
    \vspace{-0.3cm}
\end{figure*}
\section{Methodology}
\label{sec_method}
\subsection{Overview}
\label{subsec_method_overview}
Here we present the key high-level ideas of our approach. We first present a graph-centric view of the standard image-text matching objective in CLIP, which serves as a motivation to develop our approach (Sec. \ref{subsec_itg_alignment}). We create scene graphs derived from the text, decompose them into multiple sub-graphs (Sec. \ref{subsec_sg_decomposition}) and apply augmentations on these sub-graphs to create negative sub-graphs (Sec. \ref{subsec_neg_graph_creation}) which are used as hard negatives in a batch. Sec. \ref{subsec_coarsetofine_loss} formally defines the Image-to-Multi-Text and Text-to-Image losses used for a batch of V-L inputs which is key for learning from multiple positive and negative texts derived from sub-graphs. Matching images with coarse-to-fine sub-graphs results in improved fine-grained and hierarchical understanding of text. Sec. \ref{subsec_curriculum_training} provides a two-stage curriculum learning strategy for improved fine-tuning performance.

\subsection{Image-Text-Graph Alignment}
\label{subsec_itg_alignment}
Our approach builds on the idea that the standard image-text contrastive learning in CLIP can be viewed as a matching between an image scene graph and its sub-graph. Formally, given an image-text pair , the image can be viewed by its scene graph, . The text scene graph is given by . Then . According to this assumption, during contrastive learning in CLIP, we implicitly bring the representation of the image scene graph close to \textit{one} of its sub-graph (the text scene graph).
Now, let  represent the set of sub-graphs of a graph . According to the assumption above, . Hence ,  becomes a correct matching pair during contrastive learning.
We match multiple sub-graphs of the text scene graph to the same image, while also including hard negative sub-graphs in the batch. Matching between graphs is an implicit concept, and all graphs are first converted to text via templates, converted to embeddings using transformer-based (text) encoders, and matched to image embeddings.


\subsection{Scene Graph Guided Text Decomposition}
\label{subsec_sg_decomposition}
Scene graphs are succinct representations of images. However, an image scene graph generator used for generating a scene graph for any given input image is expensive to train since it requires supervised scene graph annotations for training \citep{li2017scene, xu2017scene, zhang2019graphical}, and also leads to issues like low coverage or biased generations against the long tail nature of objects and relationship annotations. We instead use the text scene graph created using an off-the-shelf text scene graph parser\footnote{https://github.com/vacancy/SceneGraphParser} \citep{wu2019unified}. This serves as a proxy for the scene graph of (part of) the image and is assumed to be a sub-graph of the image scene graph, as also depicted by Figure \ref{fig:method_overview}.

Let the text scene graph obtained be , where  represent the nodes of the graph, which are either objects or their attributes.  are the edges of the graph that represent relations between objects. See Fig. \ref{fig:method_overview} for an example of a text scene graph. As shown in the figure, we decompose this scene graph into multiple \textit{positive} sub-graphs , , where  is the max number of decomposed sub-graphs and is a hyperparameter. Each sub-graph is a representation of a part of the image. We then convert sub-graphs to sentences so that they can be easily processed by transformer-based (text) encoders commonly used to train CLIP. For this, we use a simple template-based approach. For e.g., we create templates of the form "  " if we need to convert a graph having two nodes (, ) and a relation , into a sentence format. Corresponding to each sub-graph, we obtain one positive text for the image, creating a positive text set .

\subsection{Negative Sub-Graph Creation}
\label{subsec_neg_graph_creation}
Corresponding to sub-graphs in , we create negative sub-graphs . Sub-graphs in  are a minimally perturbed versions of the positive sub-graphs in . Similar to positive sub-graphs, we convert sub-graphs in  to text using the same template-based approach, and obtain . Texts in  serve as hard negative texts in a given batch, see Fig. \ref{fig:method_overview}. We focus on creating negative sub-graphs that improve the attribute binding and relation understanding capabilities of the model, for which we use the following strategies for negative graph creation:
We first consider an external set of objects (), attributes (), and relations (). \\
\textbf{1) Node Swapping and Replacement}: We \textit{swap} nodes in sub-graphs, these can be swaps of nodes which are attributes or objects. We also \textit{replace} nodes with external nodes from ,  based on their type. \textbf{2) Edge Replacement}: We \textit{replace} edges with randomly sampled edges from the external relations set, . \textbf{3) Connecting Sub-graphs}: Here we \textit{join} two sub-graphs. For this, we use one sub-graph from , and another random graph created using nodes and edges sampled from external sets . This creates an overall hard negative graph. Sub-graphs are joined by simply joining nodes from both graphs through a randomly sampled edge from . 
These strategies result in minimally perturbed hard negative sub-graphs for improving attribute and relation understanding. We define multiple graph transformations  --  using the above techniques and create hard negative sub-graphs. See Appendix Sec. \ref{sg_decomp} for more details regarding negative sub-graph creation.


\subsection{Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space}
\label{subsec_coarsetofine_loss}
Given an image-text batch during training , consider separately the batch of images  and a batch of texts .
The sentences in the text batch are first converted to scene graphs to obtain a batch of scene graphs , followed by decomposition to sub-graphs to obtain the positive sub-graph batch , .  negative sub-graphs are sampled and added to the batch to obtain .
We convert these sub-graphs to text to obtain the final text batch .

Consider an image encoder model  parameterized by , a text encoder  parameterized by . For any image , text ,  is the unnormalized image feature, and  is the unnormalized text feature. As common practice, the features are normalized to obtain  and . 
\noindent The \textbf{Image-to-Multi-Text} contrastive loss is given by:
{

}where . \\
The \textbf{Text-to-Image} contrastive loss is only calculated for the positive texts. It is given by:
{

}where .
, in which  represent the texts in , obtained from positive and negative sub-graphs respectively. The overall loss is . 

\subsection{Curriculum and Robust Fine-tuning}
\label{subsec_curriculum_training}
\noindent For fine-tuning experiments, we develop a two-stage curriculum learning strategy motivated by recent work \citep{goyal2022finetune, wortsman2022robust, kumar2022fine} that show how fine-tuning can distort pre-trained features and closely mimicking the contrastive pre-training objective while fine-tuning CLIP can help mitigate this problem \citep{goyal2022finetune}. However, our coarse-to-fine contrastive learning objective naturally deviates from pre-training in two ways.  Existence of hard negative texts in the batch, and  Having multiple positive and negative texts for an image. This can lead to a \textit{gap} in pre-training vs fine-tuning objective, and a lower than optimal performance after fine-tuning. To solve this, our two-stage curriculum learning strategy first fine-tunes the model while sampling (at max) a single positive and negative sub-graph per image, followed by fine-tuning it with multiple positive and negative sub-graphs. The hardness of data in this curriculum learning setup is defined by the amount of difference the fine-tuning setup has as compared to the pre-training setup. According to this intuition, it is easier for the model to first learn to handle hard negatives in a batch and then learn to handle multiple positive and hard negative sentences at once. We see consistent improvements using this strategy compared to a direct one-step fine-tuning, which we term as \methodcompNoCurric{} in our ablations. For better performance on non-compositonal tasks, we use the robust fine-tuning approach \citep{wortsman2022robust} of weight space ensembling of the vision encoder, before and after fine-tuning. This model is called \methodcompwiseft{} \section{Experiments}
\label{sec_experiments}
\noindent \textbf{Evaluation Datasets:}
We test \methodcomp{} and baselines on large scale benchmarks that require compositional reasoning: \underline{CREPE-Systematicity} \cite{ma2022crepe} measures systematic generalization, \underline{ARO} \cite{yuksekgonul2022and} measures attribute, relation and word-order understanding, \underline{SVO} \cite{hendricks-nematzadeh-2021-probing} measures verb (relation) understanding, \underline{VL-Checklist} \cite{zhao2022vlchecklist} measures relation, attribute and object understanding. We use \underline{CREPE-Productivity} \cite{ma2022crepe} for measuring model's ability to productively generalize to more complex and long sentences.
Methods for improving compositionality should be tested on general downstream tasks used to evaluate the quality of learned representations of language and vision. For this, we utilize the popular ELEVATER benchmark \cite{li2022elevater} consisting of 20 datasets and ImageNet \cite{deng2009ImageNet} following prior work \citep{doveh2023teaching}.\\
\noindent \textbf{Baselines:} We compare with all recent techniques used for improving compositionality of CLIP style models including NegCLIP \citep{yuksekgonul2022and}, Teaching SVLC \citep{doveh2023teaching} and Syn-CLIP \citep{cascantebonilla2023going} along with CLIP \citep{radford2021learning} as well as CLIP-FT (fine-tuned) on datasets we use. See Appendix Sec. \ref{baselines} for more details. 

\begin{table*}[h!]
\small
  \fontsize{7.7}{10pt}\selectfont
      \centering
      \setlength{\tabcolsep}{2.5pt}
      {
      \begin{tabular}{lccccc|ccccccc|ccccccc|c}
        \toprule
        \multicolumn{1}{l}{FineTun. data } & \multicolumn{5}{c|}{COCO} & \multicolumn{7}{c|}{CC-FT} & \multicolumn{7}{c|}{YFCC-FT} &\\
        \cmidrule(lr){2-6} \cmidrule(lr){7-13} \cmidrule(lr){14-20}
        \multicolumn{1}{l}{Benchmark}  & \multicolumn{3}{c}{\textbf{ARO}} & \multicolumn{1}{c}{\textbf{VLC}} & \multicolumn{1}{c|}{\textbf{SVO}} & \multicolumn{3}{c}{\textbf{ARO}} & \multicolumn{2}{c}{\textbf{CREPE}} & \multicolumn{1}{c}{\textbf{VLC}} & \multicolumn{1}{c|}{\textbf{SVO}} & \multicolumn{3}{c}{\textbf{ARO}} & \multicolumn{2}{c}{\textbf{CREPE}} & \multicolumn{1}{c}{\textbf{VLC}} & \multicolumn{1}{c|}{\textbf{SVO}} & Meta\\
        \cmidrule(lr){2-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-9} \cmidrule(lr){10-11} \cmidrule(lr){12-12} \cmidrule(lr){13-13} \cmidrule(lr){14-16} \cmidrule(lr){17-18} \cmidrule(lr){19-19} \cmidrule(lr){20-20}
        Method  & Rel. & Attr. & Ord. & Avg. & Avg. & Rel. & Attr. & Ord. & CU & AU & Avg. & Avg. & Rel. & Attr. & Ord. & CU & AU & Avg. & Avg. & Avg.\\
        \midrule
          Random                         & 50.0 & 50.0 & 20.0 & 50.0 & 50.00 & 50.0 & 50.0 & 20.0 & 14.3 & 20.0 & 50.0 & 50.00 & 50.0 & 50.0 & 20.0 & 14.3 & 20.0 & 50.0 & 50.00 & 38.35\1pt]
          \clip{}-FT                      & 58.9 & 65.3 & 38.4 & 71.3 & 90.15 & 58.1 & 63.3 & 42.7 & 45.8 & 35.6 & 70.1 & 88.56 & 51.4 & 63.1 & 25.3 & 36.4 & 38.3 & 68.9 & 85.27 & 57.73\1pt]
          \midrule
          \rowcolor{cyan!12}
          \methodcompbold{}             & \textbf{82.6} & \textbf{78.0} & \textbf{87.1} & \textbf{81.4} & \textbf{90.67} & \textbf{80.4} & \textbf{69.8} & \textbf{85.5} & \textbf{72.4} & \textbf{40.9} & \textbf{77.6} & \textbf{88.73} & \textbf{74.3} & \textbf{66.9} & \textbf{84.4} & \textbf{48.8} & \textbf{41.5} & \textbf{75.1} & \textbf{85.36} & \textbf{74.29}\1pt]
    \clip{}-FT & 79.0 & 64.7 & 54.3 & 41.7 & 59.3 & 25.2 \1pt]
    Teaching SVLC\textsuperscript{}\textsuperscript{2} & 85.0 & 72.0 & 69.0 & - - & - - & - -\1pt]
          \midrule
          & \clip{} & 51.0 & 56.6 & 25.5 & 44.1 & 37.3 & 65.6 & 82.21 & 53.8 & 56.2 & 18.4 & 39.6 & 41.7 & 66.2 & 76.27 & 51.03\1pt]
          \rowcolor{cyan!12}
          \cellcolor{white} \multirow{-3}{*}{\rotatebox[origin=c]{90}{Swin-T}} & \methodcompbold{} & \textbf{84.3} & \textbf{76.8} & 55.5 & \textbf{92.1} & \textbf{44.5} & \textbf{72.4} & \textbf{85.62} & \textbf{74.7} & \textbf{66.1} & \textbf{35.8} & \textbf{89.6} & \textbf{45.3} & \textbf{71.8} & \textbf{77.87}  & \textbf{69.46}\1pt]
          & \negclip{} & 80.5 & 66.5 & \textbf{60.5} & 82.0 & 41.4 & 69.5 & 82.03 & 68.0 &	58.5 & 37.1 & 67.2 & 41.5 & 66.1 & 75.18  & 64.00\1pt]
          \bottomrule
      \end{tabular}
      
      \caption{Pre-training results on all compositionality benchmarks (4 benchmarks, 10 datasets) over four expt. settings (two pre-training datasets, two backbones). See Table \ref{clip_fine-tune_all} for abbreviations and Sec. \ref{results} for more details.}
      \label{pre-training_results_all}
\end{table*}
  
\noindent \textbf{Training and Evaluation Details:} \\
\noindent \underline{Fine-tuning}: NegCLIP \cite{yuksekgonul2022and} was developed by fine-tuning CLIP on the COCO dataset \citep{lin2014microsoft}, however, COCO images might overlap with benchmarks like CREPE and ARO which may lead to confounding of results. Hence we consider 2 additional similar sized fine-tuning datasets randomly sampled from CC-12M \citep{sharma-etal-2018-conceptual, changpinyo2021cc12m} and YFCC-15M \citep{thomee2016yfcc100m} and call them CC-FT, YFCC-FT. We also use CC3M \citep{sharma-etal-2018-conceptual} for comparing with recent baselines. We fine-tune the commonly used OpenAI CLIP-ViT-B32 model and report results on all datasets, except for CREPE dataset which tests the \textit{systematic generalization} for which we used OpenCLIP \cite{Ilharco_OpenCLIP_2021} models pre-trained on \{CC-12M, YFCC-15M\}, fine-tune them on \{CC-FT, YFCC-FT\}, and report results on \{CC-12M,YFCC-15M\} splits of CREPE. See Appendix \ref{eval_data_details} for more information on evaluation datasets.

\noindent \underline{Pre-training}: We pre-train \methodcomp{}, \negclip{} and \clip{} on two prominent large-scale pre-training datasets, CC-12M and YFCC-15M, and use two different backbones (ResNet-50 and Swin-Tiny) following prior work \citep{yang2022unified} and report zero-shot performance on all test datasets. See Appendix \ref{hyperparams} for hyperparameters details.

  
\subsection{Results}
\label{results}
In this section we provide experimental results in both pre-training and fine-tuning settings to show the efficacy of our approach. These are as follows:\\
\newline
\noindent \textbf{Fine-tuning:} Main fine-tuning results are shown in Table \ref{clip_fine-tune_all} and \ref{tab:comparison_other_baselines_main}, where we fine-tune CLIP models using our method and compare it to baselines. Notably, we see that the generalization performance on unseen compounds and atoms as measured by the CREPE dataset is up to {} higher than NegCLIP. Additionally \methodcomp{} shows upto {, , } of improvement over \negclip{} in understanding relations, attributes and word order respectively. \methodcomp{} also shows consistent improvements in the verb understanding task as measured by the SVO dataset. \textbf{Additional Comparisons}: We also compare with latest contemporary works in Table \ref{tab:comparison_other_baselines_main} and Appendix Sec. \ref{comparison_other_baselines}. We find significant improvements (upto  on ARO) over models that use LLMs or synthetic data for making CLIP more compositonal.\\
    
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/20_datasets_avg_4_pretrain_settings_score_diff.png}
    \caption{\methodcomp{}'s average score difference with \negclip{} on 20 datasets from {\color{blue} ELEVATER} benchmark.}
    \label{fig:20_datasets_avg_all_pretraining}
    \vspace{-0.5cm}
\end{figure}
    
\begin{figure*}[h!]
    \centering
    \begin{minipage}[b]{0.19\textwidth}
        \centering
        \subcaptionbox{CREPE-Productivity}{\includegraphics[width=\textwidth]{figures/productivity_vitb32_yfcc100k_swap_atom_avg.png}
        }
    \end{minipage}
    \hfill
    \vline
    \hfill
    \begin{minipage}[b]{0.24\textwidth}
        \centering
        \subcaptionbox{Tree Scores}{\includegraphics[width=\textwidth]{figures/tree-score-coco.png}}
    \end{minipage}
    \hfill
    \vline
    \hfill
    \begin{minipage}[b]{0.23\textwidth}
        \centering
        \subcaptionbox{ARO-Relation}{\includegraphics[width=\textwidth]{figures/vl_disentangle_aro_relation.png}}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.23\textwidth}
        \centering
        \subcaptionbox{ARO-Attribute}{\includegraphics[width=\textwidth]{figures/vl_disentangle_aro_attribution.png}}
    \end{minipage}
    \hfill
    \caption{\textbf{a)} Results on {\color{blue} CREPE-Productivity} dataset \textbf{b)} Tree-score comparison of \methodcomp{} with \negclip{}: \methodcomp{} shows improved hierarchical understanding of language. \textbf{c) and d)} Selectively fine-tuning of image, text encoders and measure performance on different datasets. Also see similar results for SVO in Figure \ref{fig:freezing_expts__SVO}.}
    \label{fig:freezing_expts_ourmethod_and_tree_score_clip_pretr}
    \vspace{-0.5cm}
\end{figure*}

\noindent \textbf{Pre-training:} Table \ref{pre-training_results_all} shows pre-training results over all benchmarks. CREPE results show a significant gain in ability to systematically generalize to unseen combinations of concepts. Across pre-training settings, \methodcomp{} improves over \negclip{} by up to {} when evaluated against HN-Comp (CU), HN-Atom (AU) hard negatives respectively. Significant improvements are observed in attribute and relation understanding, giving gains of up to {} respectively across pretraining settings. We also note that order understanding of \methodcomp{} is worse than that of \negclip{} for the CC-12M pre-training dataset, while better than \negclip{} for the YFCC-15M dataset. Notably, there is a large variance in \negclip{}'s performance across pre-training datasets as seen in Table \ref{pre-training_results_all}, and it also performs poorly when the pre-training dataset has higher noise (e.g. YFCC-15M). \methodcomp{} is fairly consistent and more robust to the change in the pre-training dataset. In Appendix \ref{data_eff} we find that \methodcomp{} can provide improvements over \negclip{} while using as low as \textbf{0.3x} of the total pre-training or fine-tuning data.\\
\newline
\noindent \textbf{Results on classification and retrieval}:
\begin{table}[h]
\fontsize{10.5}{13pt}\selectfont
    \centering
    \begin{tabular}{lcccc|c}
    \toprule
    {Model} & \multicolumn{2}{c}{\textbf{COCO}} &  \multicolumn{2}{c}{\textbf{Flickr30K}} & {AVG.}\\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    & {I2T} & {T2I} & {I2T} & {T2I} & \\
    \midrule
    CLIP & 20.7 & 13.1 & 36.2 & 24.1 & 23.5 \\
    NegCLIP & 20.1 & 12.9 & 38.6 & 23.3 & 23.7 \\
    \rowcolor{cyan!12}
    MosaiCLIP & \textbf{25.9} & \textbf{16.5} & \textbf{44.5} & \textbf{29.5} & \textbf{29.1} \\
    \bottomrule
    \end{tabular}
    \caption{Comparison of Recall@1 scores of \methodcomp{} with \negclip{} and \clip{}. All models are pre-traind on YFCC-15M with swin-Tiny backbone}
    \label{classification-results}
    \vspace{-1.5em}
\end{table}
On average, \methodcomp{} achieves  better performance on the ELEVATER classification benchmark compared to \negclip{} and \clip{} while pre-training and maintains similar accuracy as \clip{} while fine-tuning. We also try using our method along with the robust fine-tuning technique (WiSE-FT) so that performance degradation during fine-tuning is minimal, as shown in Appendix Table \ref{ft_zs_21_table}. See Fig. \ref{fig:20_datasets_avg_all_pretraining} for average results on ELEVATER over four training settings and Table \ref{classification-results} for results on retrieval benchmarks where we see a  point improvement over \negclip{}. We use the popular Karpathy splits having a 5K and 1K sized test set for obtaining the COCO and Flickr30k retrieval scores respectively.
Hence \methodcomp{}'s training strategy improves or maintains the quality of learned representations while improving compositonality. Figures \ref{fig:20_datasets_swin_CC}-\ref{fig:20_datasets_rn50_YFCC} show detailed results on ELEVATER.\\
\newline
\noindent \textbf{Productivity}:
As defined by \citet{ma2022crepe}, a productive VL model can handle arbitrarily long and complex sentences and is an important aspect of compositionality. Although we do not explicitly train our models for generalization to longer sentences, the improved hierarchical language understanding using our methods lead to an emergent behavior such that \methodcomp{} generalizes better than \negclip{} and \clip{} to more complex sentences. We can see this effect in Fig. \ref{fig:freezing_expts_ourmethod_and_tree_score_clip_pretr} a) and Appendix Fig. \ref{fig:Productivity_clip_ft_cc_coo} and \ref{fig:Productivity_clip_pretr}. We report the average of retrieval over swap and atom splits and find \methodcomp{} significantly improves over \negclip{} by upto  across different text complexities (4-12). \\
\newline
\noindent \textbf{Application to more advanced VLMs:}
While our focus in this work has been on CLIP style, dual encoder models due to their various benefits, we believe our methods are model agnostic and aimed at improving contrastive learning through our coarse-to-fine learning framework and negative mining techniques. In this section we test our model on an advanced VLM, BLIP.
We modified BLIP’s original image-text contrastive learning objective and create two variants, one called BLIP+NegCLIP where we use NegCLIP style hard negatives and the other BLIP+MosaiCLIP which uses our methods of scene graph guided text decomposition and negative sub-graph creation. We fine-tune BLIP model taken from the official BLIP repository and use the “BLIP w/ ViT-B and CapFilt-L model (pre-trained on 129M examples)” as our base model. Results for fine-tuning experiment using COCO dataset is shown in Table \ref{blip-expts}. We use the hyperparameters used by the official codebase (for the task of fine-tuning on COCO dataset for image-text retrieval). For each setting, we report performance of four models, namely BLIP (before fine-tuned version), BLIP-FT (vanilla fine-tuned version), BLIP+NegCLIP, BLIP+MosaiCLIP. The model are evaluated on the ARO dataset to measure attribute, relation and word-order understanding, using the evaluation scripts provided by the authors of the dataset \citep{yuksekgonul2022and}.
\begin{table}[h]
    \centering
    \begin{tabular}{lcccccc}
    \toprule
    \textbf{Model} & \textbf{Rel} & \textbf{Attr} & \textbf{Ord} & \textbf{Avg} \\
    \midrule
    BLIP & 53.5 & 91.0 & 53.5 & 66.0 \\
    BLIP-FT & 58.9 & 88.4 & 58.9 & 68.7 \\
    BLIP+NegCLIP & 63.6 & 90.7 & 63.6 & 72.6 \\
    \rowcolor{cyan!12}
    BLIP+MosaiCLIP & \textbf{69.9} & \textbf{91.1} & \textbf{69.9} & \textbf{77.0} \\
    \bottomrule
    \end{tabular}
    \caption{Comparison of BLIP \citep{li2022blip} and fine-tuned version of BLIP with BLIP models that have integrated \negclip{} and \methodcomp{} methodology while training. Fine-tuning has been performed on COCO.}
    \label{blip-expts}
\end{table}
We find that compared to vanilla fine-tuning, both NegCLIP and MosaiCLIP methodologies bring improvements to relation and word order understanding, while maintaining or improving performance on attribute understanding. The MosaiCLIP methodology significantly improves relational reasoning performance and word-order understanding compared to the NegCLIP methodology, up to 6.3\%. Attribute understanding performance remains nearly the same as the baseline BLIP performance, with the MosaiCLIP methodology bringing in slight gains over NegCLIP’s methodology. On average MosaiCLIP’s methodology brings more improvements to BLIP than NegCLIP or vanilla fine-tuning.



\subsection{Analysis}
\label{sec_analysis}
We provide a detailed analysis of our models and baselines, across different dimensions as follows:\\
\newline
\noindent\textbf{Disentangling \methodcomp{} improvements:}
We quantify the relative importance of the vision and language side by freezing the language and vision encoder individually while fine-tuning all models. See Fig. \ref{fig:freezing_expts_ourmethod_and_tree_score_clip_pretr} c,d for the results. 
Notably, we find that \textbf{1)} \uline{Language encoder has significant scope for improvement over \negclip{}'s language encoder}, and \methodcomp{} is able to successfully exploit this potential and deliver an enhanced compositional understanding of language, which is evident by performance increase of  over \negclip{} when only the language encoder is fine-tuned, as shown in Fig. \ref{fig:freezing_expts_ourmethod_and_tree_score_clip_pretr} c,d.
\textbf{2)} \uline{Improvements brought by \methodcomp{} over \negclip{} in the text encoder are always higher than improvements in the image encoder}. This is evident from Fig. \ref{fig:freezing_expts_ourmethod_and_tree_score_clip_pretr} c,d where the performance increase over \negclip{} when only the language encoder is fine-tuned is always higher as compared to when only the image encoder is fine-tuned; for example, ,  for ARO-Relation, ARO-Attribution.
\textbf{3)} \uline{\methodcomp{} brings significant improvements on the image encoder side} (higher than \negclip{}) \textit{without} using any image negative mining, unlike \negclip{}.
\begin{figure*}[h!]
    \centering
    {\includegraphics[width=\textwidth]{figures/2_lines_All_Qualitative_mosaiclip_attribute.png}}
    \caption{Qualitative analysis on ARO dataset (Top:ARO-Attribution, Bottom: ARO-Relation). Models highlighted in blue match the image to the \colorbox{lime}{correct sentence} (in green) while the models in white match the image to the \colorbox{pink}{incorrect sentence} (in red). Here, models are taken from our fine-tuning experiments on COCO from Table \ref{clip_fine-tune_all}.}
    \label{fig:quali_aro}
    \vspace{-1em}
    
\end{figure*}

\noindent\textbf{\methodcomp{} improves hierarchical text understanding:}
For further understanding \methodcomp{}'s improved compositional understanding, we provide a novel analysis by considering the recently proposed Tree-Score \citep{murty2022characterizing} that measures the degree to which a transformer (text) encoder processes text in a hierarchical manner. We hypothesize that having tree-like hierarchical computation over language can be one leading factor for explaining the compositionality (or lack thereof) of CLIP-like models. Along with this, we have previously shown that the language encoder has the most prominent effect in improving compositionality in the case of \methodcomp{} . These two reasons motivate the use of tree-score to compare the language encoder's hierarchical understanding capability. Fig. \ref{fig:freezing_expts_ourmethod_and_tree_score_clip_pretr} a) shows that \uline{\methodcomp{}'s language encoder has higher tree-scores than \negclip{}'s language encoder}, suggesting that \methodcomp{} performs more tree-like computations. This explains the improved language compositionality of \methodcomp{} since a hierarchical tree-structured computation allows the language encoder to better understand input text compositionally, thereby improving vision-language compositionality. This is in line with the hypothesis that human's semantic understanding of sentences involves a hierarchical (tree-structured) computation which has significant evidence \citep{crain,hale-etal-2018-finding,pallier2011cortical} and this leads to their compositional generalization capability.\\
\newline
\noindent \textbf{\methodcomp{} is Robust:}
Noisy texts often have meaningful sub-texts which can be exploted by \methodcomp{}, hence \methodcomp{} often achieves consistent performance increase regardless of noise in the pre-training or fine-tuning dataset. For example, \negclip{} achieves significantly low performance on ARO when fine-tuned with YFCC-FT (having more noise in text) as compared CC-FT or COCO as shown in Table \ref{clip_fine-tune_all}. \negclip{} takes a  hit in performance across various ARO datasets when the fine-tuning dataset is changed from COCO to YFCC, whereas, \methodcomp{} achieves similar performance using both datasets. Appendix Sec. \ref{robustness_detailed_results} shows that pre-trained MosaiCLIP is robust to natural distributon shifts.\\
\newline
\noindent \textbf{Qualitative Analysis:}
We take \methodcomp{}, \negclip{} and \clip{} fine-tuned on COCO and filter out examples from the ARO dataset where \methodcomp{} and \negclip{}'s disagree. Some notable examples in Fig. \ref{fig:quali_aro} include cases where \negclip{} and \clip{} often struggle to understand simple concepts like understanding the color of the cat and table ({top-left} Fig. \ref{fig:quali_aro} or understanding the "is holding" relation b/w sandwich and the box in {bottom-right} Fig. \ref{fig:quali_aro}.
\vspace{-0.3em}
\subsection{Ablations}
\label{abl_main_paper}
Table \ref{ablations_cc_ft} and Appendix Tables \ref{clip_fine-tune_all_curric_rob_abl},\ref{ft_zs_21_table} show the effect of curriculum learning and robust fine-tunining where we find that curriculum learning can bring consistent improvements of up to  on average and robust-finetuning (WiSE-FT) technique performs the best on zero-shot tasks (i.e. minimal forgetting while fine-tuning), while still improving over \negclip{} by about  on compositional reasoning tasks. Table \ref{ablations:different_kind_subgraphs} shows the effects of different kinds of sub-graphs sampled during training. 
More details including the effect of sampling larger number of sub-graphs are presented in Appendix Sec. \ref{sec_ablations}.

\label{main_paper_ablations}
\begin{table}[h!]
\small
  \fontsize{7.5}{10pt}\selectfont
      \centering
      \setlength{\tabcolsep}{2.5pt}
      {
      \begin{tabular}{lccccccc|c}
        \toprule
        \multicolumn{1}{l}{Benchmark } & \multicolumn{3}{c}{\textbf{ARO}} & \multicolumn{2}{c}{\textbf{CREPE}} & \multicolumn{1}{c}{\textbf{VLC}} & \multicolumn{1}{c|}{\textbf{SVO}} & Meta \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8}
        Method  & Rel. & Attr. & Ord. & CU & AU & Avg. & Avg. & Avg. \\
        \midrule
          \methodcompbold{}             & \textbf{80.4} & \textbf{69.8} & \textbf{85.5} & \textbf{72.4} & 40.9 & 77.6 & 88.73 & \textbf{73.6}\1pt]
          \methodcompwiseftbold{}       & 78.8 & 69.4 & 82.6 & 67.5 & \textbf{41.2} & 76.4 & 88.08 & 72.0\1pt]
      \midrule 
      without  & 81.7 & 76.6 & 78.8 & 68.7 & 73.5 & 66.2\1pt]
      without ,  & 79.0 & 70.4 & 68.8 & 64.9 & 57.4 & 63.6\\label{eq:t2i_clip}
\Lcal_{t2i}	= & - \frac{1}{|\Bcal|} \sum_{j=1}^{|\Bcal|}
\log \frac{ \exp(\tau \uv_i^T \vv_j )  }{\sum_{i=1}^{|\Bcal|}  \exp(\tau \uv_{i}^T \vv_{j} )  }
\label{eq:i2t_clip}
\Lcal_{i2t}	= & - \frac{1}{|\Bcal|} \sum_{i=1}^{|\Bcal|}
\log \frac{ \exp(\tau \uv_j^T \vv_j )  }{\sum_{j=1}^{|\Bcal|}  \exp(\tau \uv_{i}^T \vv_{j} )}
1pt]
          \negclip{}                      & 81.7 & 72.7 & 85.7 & 75.6 & 90.20 & 71.5 & 65.4 & 84.5 & 53.1 & 37.5 & 72.4 & 88.36 & 57.8 & 63.1 & 52.1 & 38.8 & 39.0 & 70.4 & 83.90 &  67.57\1pt]
          \methodcompNoCurricbold{}    & 81.6 & 76.8 & \textbf{87.4} & \textbf{81.4} & 90.20 & 79.0 & 69.6 & 80.6 & 71.1 & 40.2 & \textbf{77.7} & \textbf{88.91} & 74.1 & \textbf{67.2} & 77.8 & 46.6 & 40.5 & 75.7 & 84.97 & 73.23\1pt]
        \bottomrule
      \end{tabular}
      }
    
      \caption{Ablating the effect of Curriculum learning and Robust fine-tuning. \methodcompNoCurricbold{} refers to the version of our model without any curriculum learning. \methodcompwiseftbold{} refers to the version where the image encoder of the final model (after fine-tuning) and before fine-tuning are weight-space ensembled. \clip{} and \negclip{} scores are also shown for reference. See Appendix Sec. \ref{curric_ft_effect}. }
      \label{clip_fine-tune_all_curric_rob_abl}
  \end{table*}

\begin{table}[h!]
  \fontsize{10}{10pt}\selectfont
      \centering
      \setlength{\tabcolsep}{2.5pt}
{
        \begin{tabular}{lcc}
        \toprule
        Method & ZS(21) & Compositional Score\\
        & & (Meta Avg.)\\
        \midrule
        \clip{}   & \textbf{56.4} & 60.60\\
        \negclip{}    & \underline{56.8} & 67.57\\
        \methodcompNoCurric{}     & 55.8 & \underline{73.23}\\
        \methodcompwiseft{}   & \underline{56.8} & 72.88\\
        \methodcomp{}     & 55.7 & \textbf{74.29}\\
        \bottomrule
        \end{tabular}
    }
    \caption{Zero Shot accuracy on 21 multimodal datasets from ELEVATER and ImageNet. Results are average of the three fine-tuning datasets. \methodcomp{} has negligible drop in performance in general (compared to the gains on compositionality benchmarks), and one can boost performance by using \methodcompwiseft{} which has equal performance as compared to \negclip{} on 21 muldimodal datasets. Meta Avg. Compositional Score is taken from Table \ref{clip_fine-tune_all_curric_rob_abl}. Second best results are \underline{underlined}. \textbf{Conclusion:} One can use \methodcomp{} for getting the best compositional reasoning capabilities with minimal performance degradation on multimodal tasks, and use \methodcompwiseft{} for no degradation in performance on multimodal tasks, while still performing well on compositional reasoning.}
    \label{ft_zs_21_table}
\end{table}

\subsection{Effect of robust fine-tuning}
\label{robust_ft_effect}
Among many other techniques developed for mitigating forgetting in large models when they are fine-tuned, one prominent one is robust fin-tuning-WiSE-FT, \citep{wortsman2022robust}. Following \citet{wortsman2022robust} we perform weight-space ensembling on the image encoder before and after fine-tuning using our method and call this model \methodcompwiseft{}. The results on compositionality benchmarks can be seen in Table \ref{clip_fine-tune_all_curric_rob_abl} while results on 21 multimodal tasks from ELEVATER and ImageNet can be seen in Table \ref{ft_zs_21_table}. We find that \methodcompwiseft{} has a slight performance decrease on some compositonal benchmarks as compared to \methodcomp{}, however, it is significantly better than \negclip{} on most benchmarks. The real benefit of using \methodcompwiseft{} is that it leads to least forgetting, and there is little to no performance degradation on 21 tasks as showin in Table \ref{ft_zs_21_table}.

\subsection{Data efficiency}
\label{data_eff}
We find that our technique leads to significant data efficiency requiring about 0.3x-0.6x fo the total fine-tuning or pre-training data to match or exceed \negclip{} performance. Results are shown in Tables \ref{data_eff_pretr} and \ref{data_eff_ft}.
\begin{table}[h!]
\small
\centering
    \begin{tabular}{llcc|c}
    \toprule
    {Method} & Fraction of data& \multicolumn{2}{c|}{ARO} & \multicolumn{1}{c}{SVO} \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-5}
    & & Rel. & Attr. & Avg. \\
    \midrule
    \negclip{} & Full & 73.6 & 58.9 & 76.10 \\
    \midrule
    \multirow{5}{*}{\methodcomp{}} & 0.3x & 71.6 & {\color{blue}60.6} & 70.82 \\
    & 0.5x & {\color{blue}74.3} & 60.8 & 74.04 \\
    & 0.6x & 74.7 & 63.8 & {\color{blue}75.76} \\
    & 0.8x & 77.0 & 66.3 & 77.22 \\
    & Full & 74.7 & 66.1 & 77.87 \\
    \bottomrule
    \end{tabular}

    \caption{Data efficiency of \methodcomp{} during pre-training. Numbers in blue are lowest numbers that are within  or greater than \negclip{} performance. Pre-Training dataset: YFCC-15M.}
    \label{data_eff_pretr}
\end{table}

\begin{table}[h!]
\small
\centering
    \begin{tabular}{llcc|c}
    \toprule
    {Method} & Fraction of data& \multicolumn{2}{c|}{ARO} & \multicolumn{1}{c}{SVO} \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-5}
    & & Rel. & Attr. & Avg. \\
    \midrule
    \negclip{} & Full & 71.5 & 65.4 & 88.36 \\
    \midrule
    \multirow{5}{*}{\methodcomp{}} & 0.3x & {\color{blue}70.8} & {\color{blue}67.7} & {\color{blue}88.70} \\
    & 0.5x & 74.5 & 68.6 & 88.80 \\
    & 0.6x & 75.3 & 69.3 & 88.76 \\
    & 0.8x & 78.2 & 69.8 & 88.98 \\
    & Full & 79.0 & 69.6 & 88.91 \\
    \bottomrule
    \end{tabular}

    \caption{Data efficiency of \methodcomp{} during fine-tuning. Numbers in blue are lowest numbers that are within  or greater than \negclip{} performance. Fine-tuning dataset: CC-FT. Curriculum learning has not been used for these experiments.}
    \label{data_eff_ft}
\end{table}

\subsection{Computational cost}
\label{computational_cost}
Even though \methodcomp{} uses the same global batch size of image-text pairs, it requires more compute as compared to \negclip{} or \clip{} owing to the fact that decomposing sub-graph leads to a larger effective text-batch size and hence a larger contrastive learning matrix. It is a common practice in literature to trade-off larger compute for improving \clip{}'s compositionality, as also done by previous methods Syn-CLIP \citep{cascantebonilla2023going} that generate data using external graphics engines, and Teaching-SVLC \citep{doveh2023teaching} which use LLMs requiring massive compute even during inference. \\
\textbf{Providing \negclip{} with more compute:} One can argue that providing more compute to \negclip{} can lead to better performance, however, on the contrary we found that \negclip{}'s performance decreases as batch size is scaled (from 256 to 4096, much beyond \methodcomp{}'s text or image batch size), as shown in Table \ref{scale_negclip_batch_size}. \\
\textbf{Performance-Compute Tradeoff:} It is to be noted that \methodcomp{} performance continues to increase up to a threshold, as sub-graphs are increased as shown in Table \ref{fig:abalation_num_posneg_crepe} and \ref{fig:abalation_num_posneg_aro} hence this provides a clean tradeoff between number of sub-graphs and compute, and a practitioner can choose the number of sub-graphs their compute availablility. Along with this, in Appendix Sec. \ref{data_eff} we showed that we can achieve improved performance compared to \negclip{} with as low as 0.3x data closing the gap between \negclip{} and \methodcomp{} compute even more. \textit{It is to be noted that \methodcomp{} is a drop in replacement for \clip{} after training and requires the same inference cost as \clip{}.}

\begin{table}[h!]
\small
\centering
    \begin{tabular}{lccc}
    \toprule
    Batch Size (B) & \multicolumn{2}{c|}{ARO} & \multicolumn{1}{c}{SVO} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-4}
    & Rel. & Attr. & Avg. \\
    \midrule
    512 & 68.9 & 65.6 & 88.68 \\
    1024 & 67.6 & 65.1 & 88.93 \\
    2048 & 65.7 & 64.2 & 88.72 \\
    4096 & 62.5 & 63.7 & 88.11 \\
    \bottomrule
    \end{tabular}

    \caption{Performance of \negclip{} with increasing batch size. A batch size of B corresponds to an effective batch size of 8*B in \negclip{} after image and text negative mining. Fine-tuning dataset: CC-FT.}
    \label{scale_negclip_batch_size}
\end{table}

\section{Additional Results and Experiments}
\label{additional_expt_results}

\subsection{Comparison with recent baselines}
\label{comparison_other_baselines}
We compare with recently published and contemporary works \citep{cascantebonilla2023going, doveh2023teaching}. \citet{doveh2023teaching} show that one can create rule-based hard negative sentences and Large Language Models (LLMs) based hard negative sentences and use them when training CLIP style models to obtain an improved model that is better at handling tasks that require compositional reasoning.
We fine-tune on CC3M \citep{sharma-etal-2018-conceptual} for a fair comparison with \citet{doveh2023teaching}. Results are reported in Table \ref{tab:comparison_other_baselines}. A fair comparison with Syn-CLIP \citet{cascantebonilla2023going} is not possible since their synthetic dataset is not released. However in Table \ref{tab:comparison_other_baselines} we find that performance difference is large between \methodcomp{} and Syn-CLIP showing that our general coarse-to-fine grained approach is better than using targeted synthetic datasets for inducing compositional understanding in VLMs. Comparisons with \citet{doveh2023teaching} in Table show that our approach is competitve or better at attribute, relation and object understanding as measured by the VL-Checklist benchmark \citep{zhao2022vlchecklist}. Zero Shot performance on 21 datasets suffers minimally using our approach, and is even better than \citep{zhao2022vlchecklist}. It is to be noted that both approaches Syn-CLIP \citep{cascantebonilla2023going} and \citet{doveh2023teaching} are orthogonal to our approach and combining them with our coarse-to-fine understanding approach will likely result in much better performance overall, as compared to individual techniques. In particular, Syn-CLIP \citep{cascantebonilla2023going} faces the issue of having long captions for images, and they average out embeddings of parts of the caption before matching it to the image. This issue can be eaily resolved using our framework which can easily handle multiple positive captions for an image. Performing this ablation would be future work for us, once synthetic datasets like that used by \citet{cascantebonilla2023going} are open-sourced and gain more popularity. Our approach can similarly also include captions generated from LLMs, as explored by \citet{doveh2023teaching}.

\begin{table*}[h]
  \centering
  \begin{tabular}{l@{\hspace{0.75em}}ccc|ccc|c}
    \toprule
    Benchmark  & \multicolumn{3}{c|}{\textbf{VL-Checklist}} & \multicolumn{3}{c|}{\textbf{ARO}} & \multicolumn{1}{c}{\textbf{ZS(21)}} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-8}
    \multirow{1}{*}{Method} & Obj. & Attr. & Rel. & Rel. & Attr. & Ord. & Avg. \\
    \midrule
    \clip{} & 81.6	& 67.6	& 63.1	& 	59.9 & 63.6 & 53.3 & \textbf{56.4} \1pt]
    Syn-CLIP\textsuperscript{} {\tiny \citep{cascantebonilla2023going}} & - - & 70.4 & 69.4 & 71.4 & 66.9 & 65.1 & 55.3 \1pt]
    \midrule
    \rowcolor{cyan!12}
    \methodcompNoCurricbold{} & 86.4 &  \textbf{75.0} & 69.6  & 83.2 & \textbf{78.6} & 77.3 & 54.9 \1pt]
    \rowcolor{cyan!12}
    \methodcompbold{} & 86.4  &   73.7 & 71.9   & \textbf{83.7} & 78.0 & 79.4 &  53.5 \\
    \bottomrule
  \end{tabular}

  \caption{Comparison of \methodcomp{} with recently published and contemporary works Syn-CLIP \citep{cascantebonilla2023going} and Teaching SVLC \citet{doveh2023teaching}. Results are reported on {\color{blue} VL-Checklist}, {\color{blue} ARO} and Average Zero Shot results on 21 datasets from {\color{blue} ELEVATER} and {\color{blue} Imagenet}. Performance numbers of these models are reported from their respective papers (blank fields (---) are not reported in respective papers). \textsuperscript{}Uses million-scale synthetic data for fine-tuning. \textsuperscript{}Uses external Large Language Models (LLMs) like BLOOM \citep{bloom} for text augmentation and hard negative text creation. See Sec. \ref{comparison_other_baselines} for more details.}
  \label{tab:comparison_other_baselines}
\end{table*}

\subsection{Standard deviations for fine-tuning results}
\label{std_dev_results}
Here we provide fine-tuning results on the CC-FT dataset \textit{with standard deviations} over 3 random seeds where OpenAI CLIP-ViT-B-32 is fine-tuned on CC-FT using \methodcomp{} and baseline techniques. See Table \ref{tab:std_dev_cc100k} for the results. The main paper Table \ref{clip_fine-tune_all} have average results for CC-FT while for COCO and YFCC-FT fine-tuning datasets, the results are for one seed.
We do-not run multiple pre-training experiments since they significantly more costly.
\begin{table*}[h!]
\small
\centering
    \begin{tabular}{lccc|ccc}
        \toprule
        Benchmark  & \multicolumn{3}{c|}{\textbf{ARO}} & \multicolumn{3}{c}{\textbf{SVO-Probes}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7}
        Method  & Rel. & Attr. &  Ord. & Obj. & Subj. & Verb.\\
        \midrule
        \clip{}-FT & 58.1\scriptsize0.63 & 63.3\scriptsize0.28 & 42.7\scriptsize0.18 & 93.17\scriptsize0.11 & 88.64\scriptsize0.17 & 83.87\scriptsize0.03 \\
        \negclip{} & 71.5\scriptsize0.40 & 65.4\scriptsize0.58 & 84.5\scriptsize0.11 & 92.90\scriptsize0.09 & 88.16\scriptsize0.11 & 84.02\scriptsize0.02 \\
        \midrule
        \rowcolor{cyan!12}
        \methodcompNoCurricbold{} & 79.0\scriptsize0.66 & 69.6\scriptsize0.19 & 80.6\scriptsize0.17 & 93.37\scriptsize0.04 & 89.74\scriptsize0.13 & 83.62\scriptsize0.04 \\
        \rowcolor{cyan!12}
        \methodcompbold{} & 80.4\scriptsize0.63 & 69.8\scriptsize0.21 & 85.5\scriptsize0.16 & 93.45\scriptsize0.04 & 89.39\scriptsize0.07 & 83.35\scriptsize0.05 \\
        \bottomrule
    \end{tabular}

  \caption{Fine-Tuning Results on CC-FT dataset \textit{with standard deviations} across 3 random seeds. These results correspond to the CC-FT fine-tuning results in main paper Table \ref{clip_fine-tune_all}. Here the base model which is fine-tuned using different techniques is OpenAI-CLIP-ViT-B-32.}
  \label{tab:std_dev_cc100k}
\end{table*}

\subsection{Robustness to natural distribution shifts}
\label{robustness_detailed_results}
We find that pre-trained \methodcomp{} shows robustness to natural distribution shifts as measured by ImageNet natural distribution shifts benchmark. Results are presented in Table \ref{tab:rob_nat_shifts}. We believe that \methodcomp{} sees a larger variety of texts in the form of sub-graphs which can provide it with extra supervision for tackling natural distribution shifts. Intutively, sub-graphs can lead to diversity of texts being seen by the model during training and this might lead to broader coverage of concepts and concept combinations, resulting in improved robustness. Along with this a coarse to fine 
 hierarchical understanding of texts and thereby, of images should intuitively help in improving performance on robustness benchmarks given that the model will now be able to recognise details in images and texts more accuractely.

\begin{table*}[h!]
\small
  \centering
  \begin{tabular}{lllcccccccc}
      \toprule
      \multicolumn{3}{l}{} & \multicolumn{2}{c}{\textbf{ImageNet-A}} & \multicolumn{2}{c}{\textbf{ImageNet-R}} & \multicolumn{2}{c}{\textbf{ImageNet-S}} & \multicolumn{2}{c}{\textbf{ImageNet-V2}} \\
      \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
      {Arch.} & {Data} & {Method} & Top1 & Top5 & Top1 & Top5 & Top1 & Top5 & Top1 & Top5 \\
      \midrule
      & & \clip{} & 6.4 & 24.5 & 42.6 & 68.8 & 22.2 & 45.5 & 28.2 & 54.1 \\
      & & \negclip{} & 6.6 & 25.0 & 43.1 & 68.7 & 22.2 & 45.4 & 29.4 & 55.2 \\
      \rowcolor{cyan!12}
      \cellcolor{white} & \cellcolor{white} \multirow{-3}{*}{{CC-12M}} & \methodcompbold{} & \textbf{9.1} & \textbf{29.4} & \textbf{48.6} & \textbf{74.3} & \textbf{27.2} & \textbf{52.6} & \textbf{33.6} & \textbf{61.6} \\
      \cmidrule{2-11}
      & & \clip{} & 10.9 & 34.2 & 20.6 & 42.0 & 6.4 & 16.7 & 26.1 & 49.9 \\
      & & \negclip{} & 11.4 & 35.6 & 20.0 & 41.7 & 6.0 & 16.0 & 27.2 & 50.7 \\
      \rowcolor{cyan!12}
      \cellcolor{white} \multirow{-6}{*}{\rotatebox[origin=c]{90}{Swin-T}} & \cellcolor{white} \multirow{-3}{*}{{YFCC-15M}} & \methodcompbold{} & \textbf{14.6} & \textbf{40.2} & \textbf{22.3} & \textbf{44.9} & \textbf{6.8} & \textbf{17.7} & \textbf{32.0} & \textbf{57.2} \\
      \midrule
      & & \clip{} & 7.3 & 27.4 & 41.4 & 67.8 & 21.7 & 44.3 & 29.8 & 56.4 \\
      & & \negclip{} & 7.7 & 27.7 & 41.0 & 66.9 & 21.7 & 43.9 & 30.2 & 56.0 \\
      \rowcolor{cyan!12}
      \cellcolor{white} & \cellcolor{white} \multirow{-3}{*}{{CC-12M}} & \methodcompbold{} & \textbf{11.1} & \textbf{35.6} & \textbf{52.1} & \textbf{76.9} & \textbf{29.5} & \textbf{55.4} & \textbf{37.0} & \textbf{66.5} \\
      \cmidrule{2-11}
      & & \clip{} & 13.4 & 37.3 & 17.2 & 37.2 & 4.9 & 13.6 & 25.8 & 49.4 \\
      & & \negclip{} & 12.9 & 38.0 & 18.0 & 37.3 & 5.1 & 14.7 & 26.0 & 49.0 \\
      \rowcolor{cyan!12}
      \cellcolor{white} \multirow{-6}{*}{\rotatebox[origin=c]{90}{RN-50}} & \cellcolor{white} \multirow{-3}{*}{{YFCC-15M}} & \methodcompbold{} & \textbf{17.4} & \textbf{46.6} & \textbf{21.0} & \textbf{42.7} & \textbf{6.5} & \textbf{16.9} & \textbf{32.2} & \textbf{57.9} \\
      \bottomrule
  \end{tabular}

  \caption{Results on {\color{blue} ImageNet - Natural Distribution Shifts datasets}. \methodcompbold{} leads to improved robustness to natural distribution shifts. \negclip{} performs similarly as \clip{}. Models are zero-shot tested on ImageNet-A \citep{Hendrycks_2021_CVPR}, ImageNet-R \citep{hendrycks2021many}, ImageNet-S(ketch) \citep{wang2019learning} and ImageNet-V2 \citep{recht2019imagenet}.}
  \label{tab:rob_nat_shifts}
\end{table*}


\section{Dataset Details}
\label{data_details}
\begin{table*}[h!]
  \centering
  \begin{tabular}{lcc@{\hspace{0.75em}}ll}
    \toprule
    \textbf{Benchmark/Dataset} & \textbf{\#Examples} & \textbf{\#Subtasks} & \textbf{Subtask Examples} & \textbf{Datasets Used}\\
    & & & & \textbf{for Creation}\\
    \midrule
    \multicolumn{5}{c}{Compositional Reasoning (Evaluation)}\\
    \midrule
    \rowcolor{gray!12}
    ARO & 77K & 3 & Attribute, Relation, & Visual Genome,\\
    \rowcolor{gray!12}
    & & & Order understanding & COCO, Flickr \\
    CREPE-Systematicity & 642K & 2 & Systematic generalization & \\
    & & & generalization & Visual Genome\\
    \rowcolor{gray!12}
    VL-Checklist & 410K & 3 & Attribute, Relation & Visual Genome\\
    \rowcolor{gray!12}
    & & & Object understanding & HAKE, VAW, SWiG\\
    SVO-Probes & 48K & 3 & Verbs (Relations) & --\\
    & & & understanding & \\
    \rowcolor{gray!12}
    CREPE-Productivity & 183K & 9 & Productivity & Visual Genome\\
    \midrule
    \multicolumn{5}{c}{Fine-Tuning datasets} \\
    \midrule
    \rowcolor{gray!12}
    COCO & 109K & -- & -- & --\\
    CC-FT & 100K & -- & -- & --\\
    \rowcolor{gray!12}
    YFCC-FT & 100K & -- & -- & --\\
    CC-3M & 3.11M & -- & -- & --\\
    \midrule
    \multicolumn{5}{c}{Pre-Training datasets} \\
    \midrule
    \rowcolor{gray!12}
    CC-12M & 11.26M & -- & -- & --\\
    YFCC-15M & 14.20M & -- & -- & --\\
    \bottomrule
  \end{tabular}

  Citations: ARO\citep{yuksekgonul2022and}, CREPE\citep{ma2022crepe}, VL-Checklist\citep{zhao2022vlchecklist}, SVO\citep{hendricks-nematzadeh-2021-probing}, Visual Genome\citep{krishnavisualgenome}, COCO\citep{lin2014microsoft}, Flickr\citep{flickr}, HAKE\citep{hake}, VAW\citep{vaw}, SWiG\citep{swig} 
  \caption{Details of datasets used in this study for testing compositional reasoning, for fine-tuning and pre-training models. See Appendix Sec. \ref{data_details} for more details.}
  \label{data_details_table}
\end{table*}
Here we provide detailes about datasets used for fine-tuning, pre-training and evaluating models in this study. A summary is shown in Table \ref{data_details_table}
\subsection{Fine-tuning datasets}
Following \negclip{} \citep{yuksekgonul2022and} we use the COCO dataset released by \citep{yuksekgonul2022and} having 109k samples that had hard negative sentences that \citep{yuksekgonul2022and} create for training NegCLIP. As mentioned in the main paper, COCO dataset images are used for creating Visual Genome \citep{krishnavisualgenome}, and this is further used to create datasets such as CREPE \citep{ma2022crepe}, ARO \citep{yuksekgonul2022and} and a part of VL-Checklist \citep{zhao2022vlchecklist}. This can lead to confounding and potentially misleading results, since it is unclear if the performance increase using any method comes from the fine-tuning dataset (COCO) being close to the domain of test datasets, or if it's the fine-tuning methodology that leads to an increase in performance. Hence, for rigourous experimentation of the developed methods, one must use other datasets to fine-tune contrastively trained VLMs. We randomly sample similar sized (100k datapoints) from popular pre-training datasets CC-12M and YFCC-15M, and call these smaller datasets CC-FT and YFCC-FT. To train \negclip{}, hard negative sentences and images are required, for which we first use the code released by \citep{yuksekgonul2022and}\footnote{\url{https://github.com/mertyg/vision-language-models-are-bows}} to create hard negatives sentences as well as sample three hard negative images for each image based on OpenAI CLIP ViT-B/32 features, strictly following \citep{yuksekgonul2022and}.
For comparing with contemporary works \citep{doveh2023teaching}, \citep{cascantebonilla2023going} (as shown in Table \ref{tab:comparison_other_baselines_main}), we use CC3M \citep{sharma-etal-2018-conceptual} since it's used by these baselines, and makes a direct comparison possible with them.
\subsection{Pre-training datasets}
We use popular and standard large scale pre-training datasets CC-12M \citep{changpinyo2021cc12m} and YFCC-15M \citep{thomee2016yfcc100m} for pre-training all models in this study, including \clip{}, \negclip{} and \methodcomp{}.
\subsection{Evaluation datasets}
\label{eval_data_details}
Here we list the evaluation detailes used in this study and also provide a short description for each
\noindent \textbf{CREPE-Systematicity} \cite{ma2022crepe}: CREPE provides systematic generalization datasets to test models trained on popular pre-training datasets including CC-12M and YFCC-15M. While creating CREPE, \citet{ma2022crepe} make sure to split the dataset into seen and unseen parts, which correspond to weather the model has seen or not seen the combination of concepts, when pre-trained with popular pre-training datasets. We measure and report performance on both seen and unseen splits in our work.\\
\noindent \textbf{ARO} \cite{yuksekgonul2022and}: This benchmark consists of four datasets, including VG-Relation, VG-Attribution, COCO-Order, and Flickr-Order. The first two measure attribute and relation understanding of VL models, respectively, and the last two measure the word order understanding of VL models. VG-Relation and VG-Attribution consist of tuples having an image and two texts (one positive and one negative), and the model's task is to match the image with the correct text. order datasets have four negative texts and one positive text for each image, and the task is again to match the image with the correct text.\\
\noindent \textbf{SVO-Probes} \cite{hendricks-nematzadeh-2021-probing}: This dataset consists of tuples having two images and one text. All texts and images have a subject, verb, and object, and the images differ in only one of subject, verb, or object. This dataset helps in understanding if VL models can compositionally understand combinations of objects having a relation between them. The original dataset contains 48K examples.\footnote{Some image links provided by the the original repository(\url{https://github.com/deepmind/svo_probes}) were broken. In total, 36k data points were retrievd and used in this study.}\\ 
\noindent \textbf{CREPE-Productivity} \cite{ma2022crepe}: Productivity dataset tests the model's ability to generalize to longer and more complex sentences, with complexity ranging from 4 atoms to 12 atoms, where an atom can be an attribute, relation, or object. The CREPE-Productivity dataset has a number of test sets for each sentence complexity ranging from 4 atoms to 12 atoms.\\
\noindent \textbf{VL-Checklist} \cite{zhao2022vlchecklist}: This benchmark is created by combining annotations from datasets like Visual Genome \citep{krishnavisualgenome}, SWiG \citep{swig}, HAKE \citep{hake}, VAW \citep{vaw}. Each image in the resulting dataset has two captions, a positive and a negative. The positive caption is taken from the source dataset of the image, while the negative caption differs from the positive in only one word which makes it a hard negative and helps in testing compositional and fine-grained understanding of VLMs across various dimensions like attributes, relations, and size and locations of objects.\\

\section{Baselines:}
\label{baselines}
Here we list the baselines used in this study and also provide a short description for each.
\noindent \textbf{CLIP}\citep{radford2021learning}: Our first baseline is CLIP model released by OpenAI CLIP\citep{radford2021learning} and OpenCLIP \citep{Ilharco_OpenCLIP_2021}. In particular we use the ViT-B/32 model for fine-tuning results Table \ref{clip_fine-tune_all} of the main paper, except for CREPE dataset, which requires using models pre-traoined on specific datasets, for which we use ResNet-50 (RN-50) models pre-trained on CC-12M and YFCC-15M released by OpenCLIP repository\footnote{\url{https://github.com/mlfoundations/open_clip}} \citep{Ilharco_OpenCLIP_2021}.\\
\noindent \textbf{CLIP-FT}: For disentangling the effects of fine-tuning data, and fine-tuning methodology, we create a CLIP-FT baseline where we simply fine-tune the pre-trained CLIP model on the dataset at hand, by using the standard contrastive learning technique used by CLIP. \\
\noindent \textbf{NegCLIP}\citep{yuksekgonul2022and} [ICLR 2023]: NegCLIP is trained using negative mining of texts and images. \citet{yuksekgonul2022and} create sentence level hard negatives by swapping different linguistic elements. They also additionally include hard-negative images and their corresponding texts in the batch by fetching K nearest neighbours (K=3) for each image in the feature space constructed using a pretrained CLIP model.\\
\noindent \textbf{Teaching SVLC}\citep{doveh2023teaching} [CVPR 2023]: This method uses LLM's like BLOOM \citep{bloom} along with rules to create additional positive and negative sentences for each image while fine-tuning CLIP.\\
\noindent \textbf{Syn-CLIP}\citep{cascantebonilla2023going} [Arxiv 2023]: Syn-CLIP uses a million scale synthetic dataset to fine-tune CLIP and improve it's performance on compositional reasoning tasks. The synthetic data is created using a 3D physics-based simulation platform built on Unity3D, called ThreeDWorld \citep{gan2021threedworld}. This contemporary work is complementary to our data-centric approach and we believe our methods can help fine-tuning with synthetic datasets as well. \citet{cascantebonilla2023going} in their paper showed how dense and long captions can be obtained for synthetic images and which require splitting into sub-captons followed by averaging of features from all captions while fine-tuning CLIP. This is one avenue where we believe our method can be useful since our method inherently allows matching of images to multiple texts. This is part of future work, once such synthetic datasets are released and are easily available. \\

\section{Detailed Experimental Results}
\label{detailed_expt_results}
In the main paper Table \ref{clip_fine-tune_all} and Table \ref{pre-training_results_all} we had provided concise results for some datasets, based on lack of space due to extensive experimental results. Here we provide detailed results on these datasets:
\subsection{VL-Checklist: detailed results}
\label{vl_checklist_results}
Detailed Fine-tuning results on VL-Checklist dataset are provided in Table \ref{vl_checklist_results_finetune}. These are an extension to the VL-Checklist results provided in the main paper Table \ref{clip_fine-tune_all}. Detailed Pre-training results for VL-Checklist dataset are provided in Table \ref{vl_checklist_results_pretrain} which are an extension to the VL-Checklist results provided in the main paper Table \ref{pre-training_results_all}.

  \begin{table*}[h!]
\centering
      \begin{tabular}{lr@{\hspace{0.75em}}r@{\hspace{0.75em}}r@{\hspace{0.75em}}|r@{\hspace{0.75em}}r@{\hspace{0.75em}}r@{\hspace{0.75em}}|r@{\hspace{0.75em}}r@{\hspace{0.75em}}r@{\hspace{0.75em}}}
        \toprule
        Benchmark  & \multicolumn{9}{c}{\textbf{VL-Checklist}}\\
        \cmidrule(lr){2-10}
        Fine-tuning data  & \multicolumn{3}{c|}{CC-100K} & \multicolumn{3}{c|}{YFCC-100K} & \multicolumn{3}{c}{COCO}\\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        Method & Obj. & Attr. & Rel.  & Obj. & Attr. & Rel.  & Obj. & Attr. & Rel. \\
        \midrule
          \clip{}                         &  81.6	& 67.6	& 63.1  &  81.6	& 67.6	& 63.1  &  81.6	& 67.6	& 63.1  \1pt]
          \negclip{}                      & 82.1 &  71.4 & 70.3   & 81.0 &  68.1 & 67.1  & 85.2 &  67.2 & 63.0  \1pt]
          \rowcolor{cyan!12}
          \methodcompwiseftbold{}     & 85.3 &  71.4 & 72.4  & 83.6  &  69.5 & 69.6  & 88.5 &   \textbf{75.5} & \textbf{77.0}  \1pt]
\bottomrule
      \end{tabular}
    
      \caption{Fine-tuning results on the {\color{blue} VL-Checklist} benchmark, for testing compositionality in terms of attribute, relation and object understanding. OpenAI CLIP VIT-B-32 pre-trained model is used as the base model for fine-tuning. See Sec. \ref{vl_checklist_results} for more details.}
      \label{vl_checklist_results_finetune}
  \end{table*}

  \begin{table}[h!]
  \fontsize{8.}{10pt}\selectfont
      \centering
      \begin{tabular}{p{0.4cm}lp{0.4cm}p{0.4cm}p{0.4cm}|p{0.4cm}p{0.4cm}p{0.4cm}}
          \toprule
          \multicolumn{2}{l}{Benchmark } & \multicolumn{6}{c}{\textbf{VL-Checklist}}\\
          \cmidrule(lr){3-8}
          \multicolumn{2}{l}{Pre-training data } & \multicolumn{3}{c|}{CC-12M} & \multicolumn{3}{c}{YFCC-15M}\\
          \cmidrule(lr){3-5} \cmidrule(lr){6-8}
          {Arch.} & Method & Obj. & Attr. & Rel.  & Obj. & Attr. & Rel.  \\
          \midrule
          & \clip{} & 75.2  &  61.1 & 60.6  & 73.6 &   63.0 & 62.0  \1pt]
          \rowcolor{cyan!12}
          \cellcolor{white} \multirow{-3}{*}{\rotatebox[origin=c]{90}{Swin-T}} & \methodcompbold{} & \textbf{80.0}  &  \textbf{72.9} & 64.4  & \textbf{79.3} &   \textbf{71.3} & \textbf{64.8}  \1pt]
          & \negclip{} & 75.4 & 67.6 & \textbf{65.5}  & 72.9 &   65.8 & 59.7  \1pt]
\bottomrule
      \end{tabular}
    
      \caption{Pre-training results on {\color{blue} VL-Checklist} benchmark, for testing compositionality in terms of attribute, relation and object understanding. Results for both backbones Swin-Tiny and RN-50 are shown. See Sec. \ref{vl_checklist_results} for more details.}
      \label{vl_checklist_results_pretrain}
  \end{table}

\subsection{SVO-Probes: detailed results}
\label{svo_detailed_results}
Detailed Fine-tuning results on SVO-Probes dataset are provided in Table \ref{detailed_svo_fine_tune}. These are an extension to the SVO-Probes results provided in the main paper Table \ref{clip_fine-tune_all}. Detailed Pre-training results for SVO-Probes dataset are provided in Table \ref{detailed_svo_pre_train} which are an extension to the SVO-Probes results provided in the main paper Table \ref{pre-training_results_all}.

\begin{table}[h!]
\footnotesize
  \centering
  \begin{tabular}{p{0.15cm}p{2.2cm}p{0.7cm}p{0.7cm}p{0.7cm}p{0.7cm}}
      \toprule
      & \multicolumn{1}{l}{Benchmark } & \multicolumn{4}{c}{\textbf{SVO-Probes}} \\
      \cmidrule(lr){3-6}
      & Method & Obj & Subj & Verb & Avg \\ 
      \cmidrule{2-6}
       & \clip{} & 88.13 & 83.85 & 78.76 & 83.58 \\ 
       \midrule
       & \clip{}-FT & 93.17 & 88.64 & 83.87 & 88.56 \\ 
       & \negclip{} & 92.90 & 88.16 & \textbf{84.02} & 88.36 \\ 
       \rowcolor{cyan!12}
       \cellcolor{white}
       & \methodcompNoCurricbold{} & 93.37 & \textbf{89.74} & 83.62 & \textbf{88.91} \\ 
       \rowcolor{cyan!12}
       \cellcolor{white}
       & \methodcompwiseftbold{} & 92.65 & 88.69 & 82.90 & 88.08 \\ 
       \rowcolor{cyan!12}
       \cellcolor{white} \multirow{-5}{*}{\rotatebox[origin=c]{90}{CC-100K}}
       & \methodcompbold{} & \textbf{93.45} & 89.39 & 83.35 & 88.73 \\ 
      \midrule
       & \clip{}-FT & 89.63 & 85.83 & \textbf{80.36} & 85.27 \\ 
       & \negclip{} & 88.43 & 84.05 & 79.21 & 83.90 \\ 
       \rowcolor{cyan!12}
       \cellcolor{white}
       & \methodcompNoCurricbold{} & 89.49 & 85.59 & 79.83 & 84.97 \\ 
       \rowcolor{cyan!12}
       \cellcolor{white}
       & \methodcompwiseftbold{} & 87.86 & 84.97 & 78.30 & 83.71 \\
       \rowcolor{cyan!12}
       \cellcolor{white} \multirow{-5}{*}{\rotatebox[origin=c]{90}{YFCC-100K}}
       & \methodcompbold{} & \textbf{89.93} & \textbf{86.45} & 79.71 & \textbf{85.36} \\
      \midrule
       & \clip{}-FT & 93.60 & 91.37 & 85.48 & 90.15 \\ 
       & \negclip{} & 93.59 & 91.43 & \textbf{85.58} & 90.20 \\ 
       \rowcolor{cyan!12}
       \cellcolor{white}
       & \methodcompNoCurricbold{} & 94.14 & 92.22 & 84.23 & 90.20 \\ 
       \rowcolor{cyan!12}
       \cellcolor{white}
       & \methodcompwiseftbold{} & 93.13 & 92.07 & 83.75 & 89.65 \\
       \rowcolor{cyan!12}
       \cellcolor{white} \multirow{-5}{*}{\rotatebox[origin=c]{90}{COCO}}
       & \methodcompbold{} & \textbf{94.16} & \textbf{93.04} & 84.82 & \textbf{90.67} \\
      \bottomrule
  \end{tabular}

  \caption{Detailed Fine-tuning results on the {\color{blue} SVO-Probes} dataset. See Sec. \ref{svo_detailed_results} for more details.}
  \label{detailed_svo_fine_tune}
\end{table}

\begin{table}[h!]
\small
  \centering
  \begin{tabular}{l@{\hspace{0.8em}}l@{\hspace{0.8em}}l@{\hspace{0.8em}}c@{\hspace{0.8em}}c@{\hspace{0.8em}}c@{\hspace{0.8em}}c@{\hspace{0.8em}}}
      \toprule
      \multicolumn{3}{l}{Benchmark } & \multicolumn{4}{c}{\textbf{SVO-Probes}} \\
      \cmidrule{4-7}
      {Arch.} & {Data} & {Method} & Obj & Subj & Verb & Avg \\ 
      \midrule  
      & & \clip{} & 88.43 & 82.58 & 79.33 & 82.21 \\ 
      & & \negclip{} & 88.38 & 81.83 & 79.40 & 82.04 \\ 
      \rowcolor{cyan!12}
      \cellcolor{white} & \cellcolor{white} \multirow{-3}{*}{\rotatebox[origin=c]{90}{\tiny CC-12M}} & \methodcompbold{} & \textbf{91.89} & \textbf{87.11} & \textbf{82.20} & \textbf{85.62} \\
      \cmidrule{2-7}
      & & \clip{} & 83.38 & 77.09 & 72.80 & 76.27 \\ 
      & & \negclip{} & 84.07 & 76.87 & 72.28 & 76.10 \\ 
      \rowcolor{cyan!12}
      \cellcolor{white} \multirow{-6}{*}{\rotatebox[origin=c]{90}{Swin-T}} & \cellcolor{white} \multirow{-3}{*}{\rotatebox[origin=c]{90}{\tiny YFCC-15M}} & \methodcompbold{} & \textbf{86.20} & \textbf{79.24} & \textbf{73.61} & \textbf{77.87} \\ 
      \midrule
      & & \clip{} & 87.86 & 82.54 & 79.45 & 82.13 \\ 
      & & \negclip{} & 87.58 & 82.47 & 79.42 & 82.03 \\ 
      \rowcolor{cyan!12}
      \cellcolor{white} & \cellcolor{white} \multirow{-3}{*}{\rotatebox[origin=c]{90}{\tiny CC-12M}} & \methodcompbold{} & \textbf{90.18} & \textbf{85.22} & \textbf{80.48} & \textbf{83.86} \\ 
      \cmidrule{2-7}
      & & \clip{} & 82.61 & 76.21 & 72.27 & 75.60 \\ 
      & & \negclip{} & 81.40 & 76.05 & 72.06 & 75.18 \\ 
      \rowcolor{cyan!12}
      \cellcolor{white} \multirow{-6}{*}{\rotatebox[origin=c]{90}{RN-50}} & \cellcolor{white} \multirow{-3}{*}{\rotatebox[origin=c]{90}{\tiny YFCC-15M}} & \methodcompbold{} & \textbf{84.25} & \textbf{79.83} & \textbf{73.29} & \textbf{77.42} \\
      \bottomrule
  \end{tabular}

  \caption{Detailed Pre-training results on the {\color{blue} SVO-Probes} dataset. See Sec. \ref{svo_detailed_results} for more details.}
  \label{detailed_svo_pre_train}
\end{table}

\subsection{CREPE-Systematicity: detailed results}
\label{crepe_detailed_results}
Here we provide detailed results on CREPE-Systematicity dataset used for measuring systematic generalization. In the main paper we had only provided the results related to systematic generalization (i.e., the unseen split), but here we provide results on both the seen and unseen split, for both hard negative retrieval sets (Comp and Atom) that are used when evaluating performance on CREPE by \citet{ma2022crepe}.
Detailed Fine-tuning results on CREPE-Systematicity dataset on both the seen and unseen splits are provided in Table \ref{clip_fine-tune_crepe}. These are an extension to the CREPE-Systematicity results provided in the main paper Table \ref{clip_fine-tune_all}. Detailed Pre-training results for CREPE-Systematicity dataset are provided in Table \ref{pre-training_results_crepe} which are an extension to the CREPE-Systematicity results provided in the main paper Table \ref{pre-training_results_all}. 


\begin{table*}[h!]
  \small
      \centering
      \begin{tabular}{lrrrr|rrrr}
        \toprule
        \multicolumn{1}{r}{(Pre-training, Fine-tuning) data } & \multicolumn{4}{c|}{CC-12M, CC-100K} & \multicolumn{4}{c}{YFCC-15M, YFCC-100K}\\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9}
        \multicolumn{1}{r}{Retrieval Set } & \multicolumn{2}{c}{Comp} & \multicolumn{2}{c|}{Atom} & \multicolumn{2}{c}{Comp} & \multicolumn{2}{c}{Atom} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
        \multicolumn{1}{r}{\begin{tabular}{@{}l@{\hspace{7em}}r@{}}Method & Split \end{tabular}} & Seen & Unseen & Seen & Unseen & Seen & Unseen & Seen & Unseen \\
  
        \midrule 
          \clip{}                         & 48.3 & 45.1 & 39.2 & 35.0 & 42.0 & 39.8 & 43.4 & 39.5\1pt]
          \negclip{}                      & 55.1 & 53.1 & 41.5 & 37.5 & 41.9 & 38.8 & 42.8 & 39.0\1pt]
          \rowcolor{cyan!12}
          \methodcompwiseftbold{}     &  68.4 & 67.5 & 46.1 & \textbf{41.2}  &  48.9 & 48.1 & \textbf{46.2} & \textbf{43.6} \1pt]
        \bottomrule
      \end{tabular}
    
      \caption{Fine-tuning results on the {\color{blue} CREPE - Systematicity} datasets. We take OpenCLIP models pre-trained on CC-12M, and YFCC-15M, fine-tune them on CC-100K, and YFCC-100K, respectively, and test them on CC-12M, YFCC-15M split of CREPE dataset, respectively. See Sec. \ref{results} for more details. We recalculate CLIP results since \citet{ma2022crepe} do not normalize \clip{} embeddings before taking the dot product for text and image embeddings, resulting in an incorrect score.}
      \label{clip_fine-tune_crepe}
  \end{table*}
  
\begin{table*}[h!]
  \small
  \centering
  \begin{tabular}{llrrrr|rrrr}
  \toprule
  & \multicolumn{1}{r}{Pre-training data } & \multicolumn{4}{c|}{CC-12M} & \multicolumn{4}{c}{YFCC-15M}\\
  \cmidrule(lr){3-6} \cmidrule(lr){7-10}
  & \multicolumn{1}{r}{Retrieval Set } & \multicolumn{2}{c}{Comp} & \multicolumn{2}{c|}{Atom} & \multicolumn{2}{c}{Comp} & \multicolumn{2}{c}{Atom}\\
  \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
  {Arch.} & \multicolumn{1}{r}{\begin{tabular}{@{}l@{\hspace{5em}}r@{}}Method  & Split \end{tabular}} & Seen & Unseen & Seen & Unseen & Seen & Unseen & Seen & Unseen \\
    \midrule 
    & \clip{} & 45.9 &	44.1 &	41.7 &	37.3 &  40.2 &	39.6 &	42.9 &	41.7\1pt]
    \rowcolor{cyan!12}
    \cellcolor{white} \multirow{-3}{*}{\rotatebox[origin=c]{90}{Swin-T}} & \methodcompbold{}  & \textbf{85.3} & \textbf{92.1} & \textbf{49.3} &	\textbf{44.5} &  \textbf{80.7} & \textbf{89.6} & \textbf{48.2} & \textbf{45.3} \1pt]
    & \negclip{} & 78.6 & 82.0 & 46.8 & 41.4 & 61.5	& 67.2	& 43.5	& 41.5 \1pt]
    \bottomrule
  \end{tabular}

  \caption{Pre-training results on {\color{blue} CREPE - Systematicity} datasets. Models are pre-trained using CC-12M and YFCC-15M datasets and tested on the corresponding CC-12M and YFCC-15M split of the CREPE dataset. Results for both backbones Swin-Tiny and RN-50 are shown. See Sec. \ref{results} for more details.}
  \label{pre-training_results_crepe}
  \end{table*}

\section{Reproducibility}
\label{reproducibility}
Here we provide necessary details to reproduce our work, that might not have been included in the main paper.
\subsection{Training and hyperparameter details}
\label{hyperparams}
\underline{Fine-tuning:} For all fine-tuning experiments, we follow \citet{yuksekgonul2022and} for hyperparameters. In particular, all models are fine-tuned for 5 epochs, with a batch size of 256, using a cosine learning rate schedule with 50 steps of warmup and random-crop augmentation during training. AdamW is used for optimization.  is used as the initial learning rate. Training is performed using 4 NVIDIA A100 GPUs for all models. From the ARO dataset,  examples from attribute and relation splits are used as validation examples, and the rest are used as the test set for all models. On all other datasets, we evaluate zero-shot performance. For \methodcomp{}, we find that sampling a maximum of 3 positive and 6 negative sub-graphs per image during fine-tuning gives the best result on the ARO validation set and hence is used in all our experiments (including pre-training experiments). 
For \methodcomp{}, we keep sub-graph sampling probabilities as . We vary  in  while fine-tuning on the randomly chosen YFCC dataset. We choose the best model according to the ARO val-set and keep the hyperparameters the same for all other fine-tuning datasets.

\noindent\underline{Pre-training:} For pre-training experiments, we follow the training protocol used in \citet{yang2022unified, radford2021learning}. In particular, all models are trained for 32 epochs, with a batch size of 4096, using a cosine learning rate schedule with 5000 steps of warmup and random-crop augmentation during training. AdamW is used for optimization. The initial learning rate is , and weight decay is set to . Training is performed using 64 NVIDIA A100 GPUs.
\negclip{}'s hard negative text creation method often results in no negative text for some texts in the pre-training dataset. Removing all such image-text pairs with no possible hard negative text results in poor performance for \negclip{} (due to fewer data to pre-train on). If we include these image-text pairs, the text batch size might differ for different GPUs since some image-text pairs are without hard negative texts and this causes instabilities. We hence keep a cache of sentences from previous batches and add it to the batch as negative examples so that all GPUs have the same text batch size during training. The same is done for \methodcomp{} since not all images might have the same number of unique positive and negative sub-graphs available. For \negclip{} we create hard negative sentences using code released by \citep{yuksekgonul2022and}. For \methodcomp{} training, for each image, we always use one hard negative text createdusing \negclip{}'s swapping technique, followed by positive and negative subgraphs created using our method. Sub-graph sampling probabilities are kept as , .
\subsection{Tree-Score details:} \citet{murty2022characterizing} devised a method to calculate the tree-score of a transformer over a given dataset of sentences . This tree-score measures the functional tree-structuredness of a given transformer encoder. See \citet{murty2022characterizing} for exact details for the algorithm to calculate the tree-scores. We use the code released by the authors\footnote{https://github.com/MurtyShikhar/TreeProjections} for the purpose of calculating tree-scores for CLIP's language encoder. In practice we use 5K sentences from the COCO-validation set as the held ouot test set  over which we calculate the tree-scores.

\subsection{Computing Infrastructure and Run-Time:} We use NVIDIA A100 GPUs for all our experiments. Pre-training experiments took about 1.5 days per model while using 64 GPUs. Fine-tuning experiments on CC-FT, YFCC-FT and COCO took about 45 mins each and experiments on CC3M took 5 hours per model, while using 4 GPUs.

\subsection{Model Parameters:} We use standrad CLIP models and as part of all models, is a transformer language encoder having 12 layers, 8 attention heads and 512 as it's width. For vision encoders we use 1. ResNet-50 hvaing 23M trainable parameters and 2. Transformer vision encoders a) Swin-Tiny with patch-size 4 and window size 7 following \citep{yang2022unified} and b) ViT-B-32 which has patch size 32, 12 layers and 12 attention heads.

\subsection{Evaluation Metrics:} Strictly following the respective papers and released code\footnote{ARO: \url{https://github.com/mertyg/vision-language-models-are-bows}, SVO-Probes \url{https://github.com/deepmind/svo_probes}, VL-Checklist: \url{https://github.com/om-ai-lab/VL-CheckList}}, for ARO, VL-Checklist, SVO we use accuracy as the metric as defined by the respecitve papers. And for CREPE-Productivty, and CREPE-Systematicity \footnote{CREPE Code: \url{https://github.com/RAIVNLab/CREPE}} we use Recall@1 as our metric of evaluation.

\subsection{Summary Statistics of results:} We provide standard deviation results using 3 random seeds in Appendix Section \ref{std_dev_results} for Fine-tuning experiments on the CC-FT dataset. For all other datasets, including the expensive pre-training runs we use a single seed for our experiments.

\newpage
\begin{figure}[h!]
    \centering
    \subcaptionbox{Finetuning data: CC-100k}{\includegraphics[width=0.45\columnwidth]{figures/productivity_vitb32_cc100k_swap_atom_avg.png}}
    \hfill
    \subcaptionbox{Finetuning data: COCO}{\includegraphics[width=0.45\columnwidth]{figures/productivity_vitb32_coco_swap_atom_avg.png}}
    \caption{Fine-tuninig Results on {\color{blue} CREPE - Productivity} (generalization to longer and more complex sentences). Fine-tuning datasets are mentioned below each figure.}
    \label{fig:Productivity_clip_ft_cc_coo}
\end{figure}

\begin{figure}[h!]
    \centering
    \subcaptionbox{Swin-Tiny, CC-12M}{\includegraphics[width=0.45\columnwidth]{figures/productivity_pretr_swin_cc_swap_atom_avg.png}}
    \hfill
    \subcaptionbox{Swin-Tiny, YFCC-15M}{\includegraphics[width=0.45\columnwidth]{figures/productivity_pretr_swin_yfcc_swap_atom_avg.png}}
    \hfill
    \subcaptionbox{RN-50, CC-12M}{\includegraphics[width=0.45\columnwidth]{figures/productivity_pretr_RN_cc_swap_atom_avg.png}}
    \hfill
    \subcaptionbox{RN-50, YFCC-15M}{\includegraphics[width=0.45\columnwidth]{figures/productivity_pretr_RN_yfcc_swap_atom_avg.png}}
    \caption{Pre-Training Results on {\color{blue} CREPE - Productivity} (generalization to longer and more complex sentences). Pre-Training model and datasets are mentioned below each figure.}
    \label{fig:Productivity_clip_pretr}
\end{figure}

\begin{figure}[h!]
    \centering
    \centering
    \subcaptionbox{SVO}{\includegraphics[width=0.7\columnwidth]{figures/vl_disentangle_svo.png}}
    \caption{Extension of Figure \ref{fig:freezing_expts_ourmethod_and_tree_score_clip_pretr} c), d). Selectively fine-tuning of image, text encoders and measure performance on {\color{blue} SVO-Probes} dataset.}
    \label{fig:freezing_expts__SVO}
\end{figure}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/20_datasets_swin-tiny_CC.png}
    \caption{Comparing of \clip{}, \negclip{} and \methodcomp{} on 20 datasets of from the {\color{blue} ELEVATER} \citep{li2022elevater} benchmark. Models in this graph are pretrained with CC-12M data and have Swin-Tiny as the vision backbone. See Sec. \ref{results} for more details.}
    \label{fig:20_datasets_swin_CC}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figures/20_datasets_swin-tiny_YFCC.png}
  \caption{Comparing of \clip{}, \negclip{} and \methodcomp{} on 20 datasets of from the {\color{blue} ELEVATER} \citep{li2022elevater} benchmark. Models in this graph are pretrained with YFCC-15M data and have Swin-Tiny as the vision backbone. See Sec. \ref{results} for more details.}
  \label{fig:20_datasets_swin_YFCC}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figures/20_datasets_RN-50_CC.png}
  \caption{Comparing of \clip{}, \negclip{} and \methodcomp{} on 20 datasets of from the {\color{blue} ELEVATER} \citep{li2022elevater} benchmark. Models in this graph are pretrained with CC-12M data and have ResNet-50 as the vision backbone. See Sec. \ref{results} for more details.}
  \label{fig:20_datasets_rn50_CC}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figures/20_datasets_RN-50_YFCC.png}
  \caption{Comparing of \clip{}, \negclip{} and \methodcomp{} on 20 datasets of from the {\color{blue} ELEVATER} \citep{li2022elevater} benchmark. Models in this graph are pretrained with YFCC-15M data and have ResNet-50 as the vision backbone. See Sec. \ref{results} for more details.}
  \label{fig:20_datasets_rn50_YFCC}
\end{figure*} 

\end{document}