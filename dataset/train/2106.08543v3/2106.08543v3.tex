\section{Experiments}
    \label{section:experiments}
    \begin{figure}[t!]
        \centering
        \includegraphics[width=\linewidth]{figures/figure_attract_repel_loss.png}
        \captionsetup{font=footnotesize}
        \caption{
            \textbf{Mechanism of Attract \& Repel loss.} The reference embeddings attract the positives and repel the negatives, making intra-class distribution dense and inter-class distribution sparse. For simplicity, we visualize only one positive and negative instance.
        }
        \label{fig:attract_repel_loss}
    \end{figure}
        
    \begin{table*}[t!]
        \captionsetup{font=footnotesize}
        \caption{
            \textbf{Comparison with the state-of-the arts on Visual Genome benchmark.} \textit{R@k} denotes Recall in the top- predictions.
        }
        \centering
        \resizebox{0.85\linewidth}{!}{
        \begin{tabular}{@{}cccccccccccc@{}}
            \toprule[0.15em]
            \multicolumn{3}{c}{} & \multicolumn{3}{c}{\textit{SGGen}} & \multicolumn{3}{c}{\textit{SGCls}} & \multicolumn{3}{c}{\textit{PredCls}} \\ \cmidrule(l){4-12} 
            \multicolumn{3}{c}{Models} & R@20 & R@50 & R@100 & R@20 & R@50 & R@100 & R@20 & R@50 & R@100 \\ \midrule
            \multicolumn{3}{c}{\textsc{Imp}~\cite{xu2017scene}} & - & 3.4 & 4.2 & - & 21.7 & 24.4 & - & 44.8 & 53.0 \\
            \multicolumn{3}{c}{\textsc{MotifNet}~\cite{zellers2018neural}} & 21.4 & 27.2 & 30.3 & 32.9 & 35.8 & 36.5 & 58.5 & 65.2 & 67.1 \\
            \multicolumn{3}{c}{\textsc{Graph R-CNN}~\cite{yang2018graph}} & - & 11.4 & 13.7 & - & 29.6 & 31.6 & - & 54.2 & 59.1 \\
            \multicolumn{3}{c}{\textsc{Kern}~\cite{chen2019knowledge}} & - & 27.1 & 29.8 & - & 36.7 & 37.4 & - & 65.8 & 67.6 \\
            \multicolumn{3}{c}{\textsc{Cmat}~\cite{chen2019counterfactual}} & \textit{22.1} & 27.9 & 31.2 & 35.9 & \textbf{39.0} & \textit{39.8} & 60.2 & 66.4 & 68.1 \\
            \multicolumn{3}{c}{\textsc{VCTree}~\cite{tang2019learning}} & 22.0 & 27.9 & 31.3 & 35.2 & 38.1 & 38.8 & 60.1 & 66.4 & 68.1 \\
            \multicolumn{3}{c}{\textsc{RelDN}~\cite{zhang2019graphical}} & 21.1 & \textbf{28.3} & \textbf{32.7} & \textbf{36.1} & 36.8 & 36.8 & \textbf{66.9} & \textbf{68.4} & \textit{68.4} \\
            \midrule
            \rowcolor{Gray}
            \multicolumn{3}{c}{\textsc{\textbf{LOGIN (Ours)}}} & \textbf{22.2} & \textit{28.2} & \textit{31.4} & \textit{35.5} & \textit{38.8} & \textbf{40.5} & \textit{61.1} & \textit{66.6} & \textbf{68.7} \\
            \bottomrule[0.15em]
        \end{tabular}
        }
        \label{tab:recall}
    \end{table*}
    
    \begin{table*}[t!]
        \captionsetup{font=footnotesize}
        \caption{
            \textbf{The SGG results on mean Recall (mR@K).} \textit{mR@k} denotes average \textit{R@K} over all predicate categories.
        }
        \centering
        \resizebox{\linewidth}{!}{
        \begin{tabular}{@{}cccccccccccc@{}}
            \toprule[0.15em]
            \multicolumn{3}{c}{} & \multicolumn{3}{c}{\textit{SGGen}} & \multicolumn{3}{c}{\textit{SGCls}} & \multicolumn{3}{c}{\textit{PredCls}} \\ \cmidrule(l){4-12} 
            \multicolumn{3}{c}{Models} & mR@20 & mR@50 & mR@100 & mR@20 & mR@50 & mR@100 & mR@20 & mR@50 & mR@100 \\ \midrule
            \multicolumn{3}{c}{\textsc{Imp}~\cite{xu2017scene}} & - & 3.8 & 4.8 & - & 5.8 & 6.0 & - & 9.8 & 10.5 \\
            \multicolumn{3}{c}{\textsc{MotifNet}~\cite{zellers2018neural}} & 4.2 & 5.7 & 6.6 & 6.3 & 7.7 & 8.2 & 10.8 & 14.0 & 15.3 \\
            \multicolumn{3}{c}{\textsc{Kern}~\cite{chen2019knowledge}} & - & 6.4 & 7.3 & - & 9.4 & 10.0 & - & 17.7 & 19.2 \\
            \multicolumn{3}{c}{\textsc{VCTree}~\cite{tang2019learning}} & 5.2 & 6.9 & 8.0 & 8.2 & 10.1 & 10.8 & 14.0 & 17.9 & 19.4 \\
            \midrule
            \rowcolor{Gray}
            \multicolumn{3}{c}{\textsc{\textbf{LOGIN (Ours)}}} & \textbf{5.9} & \textbf{7.7} & \textbf{9.1} & \textbf{8.6} & \textbf{11.2} & \textbf{12.4} & \textbf{16.0} & \textbf{19.2} & \textbf{22.3} \\
            \bottomrule[0.15em]
        \end{tabular}
        }
        \label{tab:mean_recall}
    \end{table*}
    
    In this section, we conduct comprehensive studies to validate the efficacy of LOGIN.
    We perform extensive ablation experiments to thoroughly demonstrate the effectiveness of each building block of LOGIN.
    LOGIN is evaluated on Visual Genome~\cite{krishna2017visual} benchmark and achieves state-of-the-art results.
    Notably, in our proposed Bidirectional Relationship Classification (BRC) task, LOGIN successfully distinguishes asymmetric relationships and is more accurate than existing methods.
    
    The model referred to as the \textsc{Baseline} in this section is a model without any proposed design principles. It directly predicts the entity and predicate categories from the RoI-Aligned visual features of entity instances and that of union of two entity instances, respectively.
    
    \subsection{Settings}
    \label{section:setting}
        \paragraph{Model Parameter and Training Details}
            For a fair comparison, most of the settings and details follow pioneer work~\cite{xu2017scene, zellers2018neural}.
            We adopt the Faster R-CNN~\cite{ren2015faster} detector with VGG backbone~\cite{simonyan2014very}.
            Following~\cite{zellers2018neural}, we use per-class NMS to reduce the number of entity proposals. The number of entity proposals is 64 (\ie, =64).
            We optimize the model using SGD with the following details: initial learning rate (1e-3), momentum (0.9), and weight decay (5e-4).
            We first pre-train the detector on Visual Genome Dataset and then train the proposed scene graph generation head while fixing the detector weight.
            To model geometric relationships, we first concatenate two extra channels with coordinates hard-coded (277) to the initial visual representation and then pass them through a convolutional layer~\cite{liu2018intriguing}. As for the Attract \& Repel loss, we sample negatives by the number of positives to avoid being heavily affected by negatives.
        
        \begin{table}[t!]
            \captionsetup{font=footnotesize}
            \caption{
                \textbf{Comparison with recent approaches in the BRC task.}
            }
            \centering
            \resizebox{\linewidth}{!}{
            \begin{tabular}{@{}cccccc@{}}
                \toprule[0.15em]
                \multicolumn{2}{c}{} & \multicolumn{4}{c}{\textit{PredCls}} \\ \cmidrule(l){3-6} 
                \multicolumn{2}{c}{Models} & pR@2 & pR@4 & pR@8 & pR@16 \\ \midrule
                \multicolumn{2}{c}{\textsc{Imp}~\cite{xu2017scene}} & 6.3 & 9.1 & 12.2 & 15.0 \\
                \multicolumn{2}{c}{\textsc{MotifNet}~\cite{zellers2018neural}} & 7.7 & 11.5 & 15.9 & 19.5 \\
                \multicolumn{2}{c}{\textsc{Graph R-CNN}~\cite{yang2018graph}} & 7.9 & 11.7 & 16.3 & 21.0 \\
                \multicolumn{2}{c}{\textsc{Kern}~\cite{chen2019knowledge}} & 7.7 & 12.1 & 16.7 & 20.7 \\
                \multicolumn{2}{c}{\textsc{VCTree}~\cite{tang2019learning}} & 8.0 & 11.9 & 16.1 & 21.0 \\
                \multicolumn{2}{c}{\textsc{RelDN}~\cite{zhang2019graphical}} & 8.0 & 12.5 & 16.4 & 20.8 \\
                \midrule
                \rowcolor{Gray}
                \multicolumn{2}{c}{\textsc{\textbf{LOGIN (Ours)}}} & \textbf{8.6} & \textbf{13.1} & \textbf{17.6} & \textbf{21.1} \\
                \bottomrule[0.15em]
            \end{tabular}
            }
            \label{tab:brc}
        \end{table}
        
        \paragraph{VG Dataset} 
            We train and evaluate LOGIN on Visual Genome (VG) Dataset~\cite{krishna2017visual}. We use the publicly released pre-processed data (train and test split is 75K and 32K)~\cite{xu2017scene}. The number of entity and predicate categories are 150 and 50, respectively.
            
        \paragraph{BR Dataset}
            We build a Bidirectional Relationship (BR) dataset to evaluate the direction awareness of the model.
            The BR dataset is a subset of VG dataset and is created by filtering out relationships with only one edge between the two nodes.
            As a result, the BR dataset always includes the relationships that have two bidirectional edges between the nodes (\eg, , ).
            As shown in~\figref{fig:Motivation}, about 93\% of bidirectional edges form different relationships depending on the direction (\ie, direction-sensitive), and only about 7\% of bidirectional edges have the same relationship regardless of direction (\ie, direction-agnostic).
            The distribution of BR Dataset is shown in \figref{fig:bidirectional_dist}. Here, the five most frequent entity categories and predicate categories are ``\texttt{man} (5466), \texttt{window} (1976), \texttt{woman} (1912), \texttt{building} (1640), \texttt{shirt} (1632)'', and ``\texttt{on} (8766), \texttt{has} (6669), \texttt{of} (3137), \texttt{with} (2292), \texttt{wearing} (2238)'', respectively. Note that the top-5 predicate categories account for about 73\% of the total predicates.
            This shows that the dominant predicate categories are often used in various contexts repeatedly, implying that variance may be high even within the same predicate category.
            That is to say, this biasness supports our argument that dealing with \textit{ambiguity} issue is essential.
            
        \begin{table*}[t!]
            \captionsetup{font=footnotesize}
            \caption{\textbf{(a) Ablation studies on network design. (b) Optimal variable search.}}
            \centering
            \begin{minipage}[t]{0.66\linewidth}
                \centering
                \begin{tabular}{@{}c@{}}
                    \textrm{(a) Ablation Studies}
                \end{tabular}
                \resizebox{\linewidth}{!}{
                \begin{tabular}{@{}cccccccccccc@{}}
                    \toprule[0.15em]
                    & \multicolumn{4}{c}{Ablations} & \multicolumn{3}{c}{\textit{SGCls}} & \multicolumn{4}{c}{\textit{PredCls}} \\ \cmidrule(l){6-12}
                    Exp &  LIH     &  DSE     &  GIH     &  & R@20  & R@50  & R@100 & pR@2  & pR@4  & pR@8  & pR@16 \\ \midrule
                    1   &          &          &          &          & 30.8  & 34.7  & 36.2  &  0.2  &  0.4  &  0.7  &  1.3 \\
                    2   &\checkmark&          &          &          & 33.5  & 37.5  & 39.6  &  8.0  &  11.5 & 17.0  & 20.3 \\
                    3   &          &\checkmark&          &          & 31.8  & 36.2  & 37.7  &  8.1  &  11.5 & 16.4  & 20.1 \\
                    4   &          &          &\checkmark&          & 33.2  & 38.4  & 39.9  &  8.2  &  12.0 & 16.9  & 20.6 \\
                    5   &          &          &          &\checkmark& 31.2  & 35.6  & 36.9  &  7.4  &  11.1 & 16.0  & 19.7 \\
                    6   &\checkmark&\checkmark&\checkmark&          & 34.4  & 38.5  & 40.3  &  8.4  &  12.9 & 17.5  & 21.0 \\
                    \rowcolor{Gray}
                    7   &\checkmark&\checkmark&\checkmark&\checkmark& \textbf{34.5} & \textbf{38.8} & \textbf{40.5} & \textbf{8.6} & \textbf{13.1} & \textbf{17.6} & \textbf{21.1} \\
                    \bottomrule[0.15em]
                \end{tabular}
                }
            \end{minipage}\hfill \begin{minipage}[t]{0.32\linewidth}
                \centering
                \begin{tabular}{@{}c@{}}
                    \textrm{(b) Optimal Variable Search}
                \end{tabular}
                \resizebox{\linewidth}{!}{
                \begin{tabular}{@{}ccccc@{}}
                    \toprule[0.15em]
                    \multicolumn{2}{c}{} & \multicolumn{3}{c}{\textit{SGCls}} \\ \cmidrule(l){3-5}
                    \multicolumn{2}{c}{Variables} & R@20 & R@50 & R@100 \\ \midrule
                    \multicolumn{1}{c|}{\multirow{3}{*}{\rot{Feature}}} & \textsc{avgpool} & 34.3  & 38.4  & 40.2 \\
                    \multicolumn{1}{c|}{} & \textsc{maxpool} & 34.1  & 38.4  & 40.1 \\
                    \multicolumn{1}{c|}{} & \cellcolor{Gray} \textsc{\textbf{flatten}} & \cellcolor{Gray} \textbf{34.5}  & \cellcolor{Gray} \textbf{38.8}  & \cellcolor{Gray} \textbf{40.5}  \\ \midrule
                    \multicolumn{1}{c|}{\multirow{3}{*}{\rot{GIH}}} & \textsc{2-layers} & 34.1 & 38.4 & 39.9 \\
                    \multicolumn{1}{c|}{} & \cellcolor{Gray} \textsc{\textbf{4-layers}} & \cellcolor{Gray} \textbf{34.5} & \cellcolor{Gray} \textbf{38.8} & \cellcolor{Gray} 40.5 \\
                    \multicolumn{1}{c|}{} & \textsc{6-layers} & 34.1 & 38.5 & \textbf{40.6} \\
                    \bottomrule[0.15em]
                \end{tabular}
                }
            \end{minipage}
            \label{tab:ablation}
        \end{table*}
        
        \paragraph{Evaluation Setup}
            Model is evaluated with the following three standard evaluation criteria~\cite{xu2017scene}:
            \begin{enumerate}
            \item Predicate Classification (\textit{PredCls}): Given ground truth boxes and labels, predict edge labels.
            \item Scene Graph Classification (\textit{SGCls}): Given ground truth boxes, predict box and edge labels.
            \item Scene Graph Generation (\textit{SGGen}): Predict boxes, box labels, and edge labels.
            \end{enumerate}
            
            As for SGG, following the prior works~\cite{xu2017scene, zellers2018neural, yang2018graph}, we use Recall@K (R@K) as an evaluation metric since mAP-like metrics are not appropriate due to the sparse annotation in Visual Genome. Specifically, we use image-wise Recall@\{20,50,100\}, which computes the fraction of ground-truth triplets found in the top- predicted triplets.
            We also adopt the mean Recall@K (mR@K) metric~\cite{tang2019learning, chen2019knowledge, tang2020unbiased} for evaluation, which retrieves each individual predicate and then averages R@K over all predicate categories.
            
            As for BRC, conventional triplet recall-based metrics only consider uni-direction, making it difficult to make a rigorous evaluation of direction awareness. To this end, we have come to introduce a new metric called \textit{pair-wise Recall (pR@K)} that fits the BRC task. The proposed metric is considered to be ``matched" only when bidirectional relationships are both correct. Formally, the pR@K calculates the fraction of the total amount of matched bidirectional relationships (BRs):
            
            This constraint severely penalizes if the relationship predictions in the opposite direction are the same. Therefore, models without direction awareness cannot receive a high score on this metric.
            For example, if only union features are used, there is no chance that asymmetric relationships are correctly predicted since only the same results are output for BRs.
            To get a high score from this metric, the model needs direction awareness that is essential to correctly predict asymmetric relationships that account for most BRs in the BR datasets.
            Specifically, we use  in the BRC task since only a few bidirectional relationships are annotated per image (3 BRs / image).
            
        \begin{table}[t!]
            \captionsetup{font=footnotesize}
            \caption{
                \textbf{Comparison of feature fusion methods.} (a) Scene Graph Classification results on Visual Genome dataset. (b) Bidirectional Relationship Classification results on BR dataset.
            }
            \centering
            \begin{tabular}{@{}c@{}}
                \textrm{(a) Scene Graph Classification}
            \end{tabular}
            \begin{tabular}{@{}ccccc@{}}
                \toprule[0.15em]
                \multicolumn{2}{r}{} & \multicolumn{3}{c}{\textit{SGCls}} \\ \cmidrule(l){3-5}
                \multicolumn{2}{c}{Fusion} & R@20 & R@50 & R@100 \\ \midrule
                \multicolumn{2}{c}{\textsc{Baseline}} & 30.8 & 34.7 & 36.2 \\ 
                \multicolumn{2}{c}{w/o Permutation} & 33.6 & 38.0 & 39.9 \\
                \multicolumn{2}{c}{Sequential} & 34.1 & 38.4 & 40.1 \\
                \rowcolor{Gray}
                \multicolumn{2}{c}{\textbf{Parallel (Ours)}} & \textbf{34.5} & \textbf{38.8} & \textbf{40.5} \\
                \bottomrule[0.15em]
            \end{tabular}
            \begin{tabular}{@{}c@{}}
                \\
                \textrm{(b) Bidirectional Relationship Classification}
            \end{tabular}
            \begin{tabular}{@{}cccccc@{}}
                \toprule[0.15em]
                \multicolumn{2}{r}{} & \multicolumn{4}{c}{\textit{PredCls}} \\ \cmidrule(l){3-6}
                \multicolumn{2}{c}{Fusion} & pR@2 & pR@4 & pR@8 & pR@16 \\ \midrule
                \multicolumn{2}{c}{\textsc{Baseline}} & 0.2 & 0.4 & 0.7 & 1.3 \\ 
                \multicolumn{2}{c}{w/o Permutation} & 8.2 & 12.2 & 17.0 & 20.6 \\
                \multicolumn{2}{c}{Sequential} & 8.3 & 12.7 & 17.3 & 20.9 \\
                \rowcolor{Gray}
                \multicolumn{2}{c}{\textbf{Parallel (Ours)}} & \textbf{8.6} & \textbf{13.1} & \textbf{17.6} & \textbf{21.1} \\
                \bottomrule[0.15em]
            \end{tabular}
            \label{tab:fusion}
        \end{table}
    
        \begin{figure}[t!]
            \centering
            \includegraphics[width=\linewidth]{figures/figure_fusion.pdf}
            \captionsetup{font=footnotesize}
            \caption{
              \textbf{Illustration of feature fusion methods to obtain initial predicate representation.} (a) \textsc{Baseline}: use union feature only. (b) w/o Permutation: concatenate all and fuse them without permutation. (c) Sequential: fuse subject and object first, and then with union. (d) Parallel: fuse all the permutations at once where the subject precedes object. Here, , ,  respectively denotes subject, object, and union.
            }
            \label{fig:fusion}
        \end{figure}
    
    \subsection{Comparison with State-of-the-Art}
    \label{section:results}
        \paragraph{Scene Graph Generation (SGG)}
            The Recall performance of the proposed method and existing methods are compared in~\tabref{tab:recall} for each evaluation criterion.
            We compare LOGIN with the recent approaches~\cite{xu2017scene, zellers2018neural, yang2018graph, chen2019knowledge, chen2019counterfactual, tang2019learning, zhang2019graphical}.
            While LOGIN appears to show competitive results against the state-of-the-arts in all criteria, note that there is no specific method that achieves the best performances in every evaluation criteria, making it difficult to judge the superiority among the SGG methods.
            
            We also benchmark LOGIN under the mean Recall (mR@K) criteria. The results are shown in~\tabref{tab:mean_recall}. The mean Recall is measured by averaging the Recall per each class for the entire classes. Therefore, unlike conventional Recall (R@K), it is irrelevant to the number of samples in each class, and even if high performance is obtained in a class with a large number of samples, it is difficult to achieve good values if low performance is obtained in a class with a small number of samples. That is, every class should obtain a good overall recall to achieve high performance. In short, it is important to accurately predict the class with few data, especially in tail, among long-tailed VG dataset. The long-tailed distribution of VG dataset also implies that the dominant predicates frequently appear in multiple contexts. Thus, it is also related to the ambiguity issue.
            We see that LOGIN consistently outperforms recent methods in mean Recall criteria (see~\tabref{tab:top20} for Recall of individual predicate), implying that our system effectively deals with ambiguity issue.
        
        \paragraph{Bidirectional Relationship Classification (BRC)}
            To independently evaluate the direction-awareness of the model, we specifically use \textit{PredCls} criteria, which is orthogonal to the entity detection.
            We compare LOGIN with recent approaches~\cite{xu2017scene, zellers2018neural, yang2018graph, chen2019knowledge, tang2019learning, zhang2019graphical}. The results are summarized in~\tabref{tab:brc}.
            Here, although~\cite{xu2017scene, yang2018graph, chen2019knowledge} use the initial predicate representation as a union feature, they enable understanding of relational direction by incorporating contexts with iterative bipartite message passing, attentional graph convolution, and knowledge embedded routing, respectively.
            By using direction sensitive embedding and contextual information at the same time, LOGIN can outperform the recent methods by a large margin (6\% of mean performance gain compared to the state-of-the-art), implying that directional bias as well as contexts are crucial in recognizing direction.
            LOGIN is in a competitive position for VG dataset, which mainly contains uni-directional relationships, but significantly improves performance, especially for bidirectional relationships, which are common in the real world.
            
    \subsection{Quantitative Analysis}
    \label{section:quantitative}
        \paragraph{Model Ablations}
            We consider several ablations to investigate the importance of the major design choices in~\tabref{tab:ablation} (a). For clarity, we show the performance in the SGG task and the BRC task in a single table. 
            Exp 1 is the result of a vanilla version of LOGIN, \ie, \textsc{Baseline}, which shows the abysmal result, especially in the BRC setting. This means that the \textsc{Baseline} has no understanding of relational direction at all; thus, it can only predict symmetric relationships correctly.
            Exp 2 - Exp 5 examine the individual contributions of each model component. Especially, LIH (Exp2) and GIH (Exp4) have a significant impact on both SGG and BRC settings. It is noteworthy that contextual information (driven from GIH) also plays a key role in recognizing directions.
            We can see in Exp3 that DSE is relatively unremarkable in SGG settings, while it improves performance in BRC settings by a large margin.
            Although the unary effect of Attract \& Repel Loss  (Exp 4) is not significant, using the loss with other components (Exp7) can further push the performance than without it (Exp 6).
            When all model components are combined (Exp 7), the model achieves the best performance in both SGG and BRC tasks, which implies that each component contains an orthogonal factor that complementarily boosts the performance.
            
        \paragraph{Optimal Variables}
            We conduct experiments to decide optimal variables of LOGIN in~\tabref{tab:ablation}
            The optimal feature extraction method is first investigated. Here, flattening the feature maintains richer information than pooling, thus shows the best results among the three choices: \textsc{avgpool}, \textsc{maxpool}, \textsc{flatten}.
            Then we examine the optimal number of layers in GIH: 4-layers produce the best results.
            Stacking multiple layers enables multi-hop communication, though it also increases the chance of introducing noisy information.
            On the other hand, stacking few layers cannot fully capture the higher-order contexts.
        
        \paragraph{Design Choices of DSE}
            In this experiment, we further explore the four design choices of Direction-Sensitive Encoding (DSE). Specifically, we investigate two approaches that have been adopted in most existing SGG literature -- (a) using only a union feature~\cite{xu2017scene, lu2016visual, dai2017detecting, yang2018graph, qi2019attentive, chen2019knowledge, chen2019counterfactual, li2017scene, li2018factorizable} (\textsc{Baseline}) and (b) fusing subject, object, and union without-permutations~\cite{zhang2017visual, yang2018shuffle, zellers2018neural, zhang2019graphical, tang2019learning, tang2020unbiased} -- and two variants of subject, object, and union fusion under the \textit{subject-precedes-object} constraint -- (c) sequential fusion and (d) parallel fusions (see~\figref{fig:fusion}).
            Except for the (a) among the four cases, the ordering of the subject and the object is fixed and therefore meets the directionality condition. Additionally, (c) and (d) consider the sum of all possible permutations. The difference between (c) and (d) is the order of fusion.
            We conduct experiments in two settings for performance comparison on fusion methods. The results are summarized in \tabref{tab:fusion}.
            Here, the performance difference between the four fusion methods in the SGC setting~(\tabref{tab:fusion} (a)) is not prominent, while the significance of combining three features is particularly evident in the BRC setting~(\tabref{tab:fusion} (b)), suggesting that union feature alone cannot give relational direction.
            In both settings, the use of permutations at the fusion phase showed better results than otherwise, and especially when fused in parallel, it showed the best results.

        \begin{table}[t!]
            \captionsetup{font=footnotesize}
            \caption{
                \textbf{Effectiveness of Graph Interaction Head (GIH)} compared to other graph neural networks (\eg, GCN~\cite{kipf2017semi}, GAT~\cite{velivckovic2018graph}).
            }
            \setlength{\tabcolsep}{10pt}
            \centering
            \begin{tabular}{@{}c@{}}
                \textrm{(a) Scene Graph Classification}
            \end{tabular}
            \resizebox{0.8\linewidth}{!}{
            \begin{tabular}{@{}ccccc@{}}
                \toprule[0.15em]
                \multicolumn{2}{r}{} & \multicolumn{3}{c}{\textit{SGCls}} \\ \cmidrule(l){3-5}
                \multicolumn{2}{c}{Methods} & R@20 & R@50 & R@100 \\ \midrule
                \multicolumn{2}{c}{\textsc{LOGIN /w GCN}~\cite{kipf2017semi}} & 33.8 & 37.5 & 39.7 \\ 
                \multicolumn{2}{c}{\textsc{LOGIN /w GAT}~\cite{velivckovic2018graph}} & 33.2 & 37.1 & 39.5 \\
                \rowcolor{Gray}
                \multicolumn{2}{c}{\textbf{\textsc{LOGIN /w GIH} (Ours)}} & \textbf{34.5} & \textbf{38.8} & \textbf{40.5} \\
                \bottomrule[0.15em]
            \end{tabular}
            }
            \begin{tabular}{@{}c@{}}
                \\
                \textrm{(b) Bidirectional Relationship Classification}
            \end{tabular}
            \resizebox{\linewidth}{!}{
            \begin{tabular}{@{}cccccc@{}}
                \toprule[0.15em]
                \multicolumn{2}{r}{} & \multicolumn{4}{c}{\textit{PredCls}} \\ \cmidrule(l){3-6}
                \multicolumn{2}{c}{Fusion} & pR@2 & pR@4 & pR@8 & pR@16 \\ \midrule
                \multicolumn{2}{c}{\textsc{LOGIN /w GCN}~\cite{kipf2017semi}} & 7.8 & 12.4 & 16.5 & 20.5 \\
                \multicolumn{2}{c}{\textsc{LOGIN /w GAT}~\cite{velivckovic2018graph}} & 7.9 & 12.3 & 16.3 & 20.3 \\
                \rowcolor{Gray}
                \multicolumn{2}{c}{\textbf{\textsc{LOGIN /w GIH} (Ours)}} & \textbf{8.6} & \textbf{13.1} & \textbf{17.6} & \textbf{21.1} \\
                \bottomrule[0.15em]
            \end{tabular}
            }
            \label{tab:gih}
        \end{table}

        \paragraph{Effectiveness of GIH}
            We examine the effectiveness of GIH by comparing GIH with two representative message passing graph neural networks in~\tabref{tab:gih}: Graph Convolutional Network (GCN)~\cite{kipf2017semi} and Graph Attention Network (GAT)~\cite{velivckovic2018graph}.
            GCN aggregates feature information via a non-euclidean convolution operation from a nodeâ€™s neighborhood.
            As opposed to GCNs, GAT allows for implicitly assigning different importances to nodes of a same neighborhood, enabling a leap in model capacity.
            Unlike them, layer-wise propagation rule of GIH considers not only nodes but also edges as a neighborhood, allowing the model to leverage higher-order contexts for node update.
            From the results, we see that GAT does not improve the performance upon the GCN.
            The results demonstrate the effectiveness of GIH in predicting both object and relationships categories (Scene Graph Classification). Since LOGIN equipped with GIH exploits richer information (\eg, edge), it is also strong in understanding relational direction (Bidirectional Relationship Classification).

        \begin{table}[t!]
            \captionsetup{font=footnotesize}
            \caption{
                \textbf{Per-type predicate classification results.} Only top-20 frequent predicates are shown. The evaluation metric is \textit{R@50}.
            }
            \centering
            \resizebox{\linewidth}{!}{
            \begin{tabular}{@{}ccc|ccc@{}}
                \toprule[0.15em]
                predicate & Baseline & LOGIN & prediate & Baseline & LOGIN \\
                \midrule
                on & 66.3 & 88.1 & sitting on & 32.2 & 61.0 \\
                has & 47.7 & 87.5 & under & 35.9 & 52.5 \\
                wearing & 68.9 & 93.7 & riding & 26.3 & 83.0 \\
                of & 42.8 & 82.4 & in front of & 8.9 & 29.4 \\ 
                in & 47.7 & 64.1 & standing on & 16.7 & 37.7 \\
                near & 19.4 & 52.3 & at & 39.5 & 57.8 \\
                with & 18.1 & 45.9 & attached to & 12.1 & 21.0 \\
                behind & 20.7 & 57.0 & carrying & 23.9 & 62.1 \\
                holding & 31.4 & 78.7 & walking on & 10.5 & 59.3 \\
                above & 18.2 & 51.2 & over & 9.5 & 28.4 \\
                \bottomrule[0.15em]
            \end{tabular}
            }
            \label{tab:top20}
        \end{table}

        \paragraph{Per-type Predicate Recall}
            We expect the model to better understand each predicate by allowing attention mechanism of LIH to capture the predicate label-specific representation well and Attract \& Repel Loss to help separate inter-class and aggregate intra-class predicates in the embedding space. In order to ensure that the proposed model solves the ambiguity issue well, we compare our LOGIN with Baseline under the Recall@50 metric for the top-20 frequent predicates in~\tabref{tab:top20}.
            Compared to Baseline, we observe a significant performance improvement in all predicate classes.
            Specifically, our system better understands the geometric predicate (\eg., on, in front of, behind, above, under), possessive predicates (\eg, has, of, wearing), and semantic predicates (\eg, holding, walking).
            This suggests that explicit separation on predicate embedding space properly solves the ambiguity problem.
            
    \subsection{Qualitative Analysis}
    \label{section:qualitative}
        \begin{figure*}[p!]
            \centering5
            \includegraphics[width=\linewidth]{figures/figure_qualitative.pdf}
            \captionsetup{font=footnotesize}
            \caption{
                \textbf{Qualitative examples.} The first column shows input images with entity proposals. From the second to fourth columns, we show the scene graphs of ground-truth, \textsc{Baseline}, and \textsc{LOGIN} respectively. The bounding boxes or nodes are colored in either blue (correct) or red (wrong). The predicates are colored in either green (correct) or yellow (wrong). Examples of the first two rows contain bidirectional relationships, but not the rest. We see that LOGIN produces more diverse predicates and can successfully distinguish asymmetric relationships while \textsc{Baseline} model fails.
            }
            \label{fig:QualitativeResults}
        \end{figure*}
        
        To better see how LOGIN understands the relational direction, we provide qualitative examples in~\figref{fig:QualitativeResults}.
        Here, We compare the result of \textsc{Baseline} model and \textsc{LOGIN} with the corresponding ground-truth scene graph.
        As we can see in the results of first two rows, \textsc{Baseline} model produces the same result for a pair of entities regardless of direction. What is worse is that the whole scene graphs use almost the same predicates for defining relationships. In other words, the \textsc{Baseline} model neither considers relational-direction nor lexical diversity.
        On the other hand, LOGIN can successfully identify relational direction, thanks to the embedded direction-awareness, and it is also more diverse in terms of vocabulary.
        More interestingly, even though predictions of LOGIN are not matched, the results are seemingly plausible. For example, in the third row, detected \texttt{tail}, \texttt{legs}, and \texttt{face} of an \texttt{elephant} are false positives in terms of ground-truth, but they seem to be correct in reality. Also, relationships associated with false positives are somewhat reasonable (\eg, , ).
        