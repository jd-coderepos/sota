

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage[accsupp]{axessibility}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{color}
\usepackage{boldline}
\usepackage{ulem}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{bm}
\usepackage{soul}
\definecolor{r}{RGB}{202,12,22} 
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{2811} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{Recurrent Dynamic Embedding for Video Object Segmentation}

\author{
Mingxing Li,
Li Hu,
Zhiwei Xiong,
Bang Zhang,
Pan Pan,
Dong Liu
\\
University of Science and Technology of China \\
Alibaba DAMO Academy, Alibaba Group \\ 
{\tt\small mxli@mail.ustc.edu.cn \{zwxiong, dongeliu\}@ustc.edu.cn}\\
{\tt\small\{hooks.hl, zhangbang.zb, panpan.pp\}@alibaba-inc.com}
}
\maketitle

\begin{abstract}
\renewcommand{\thefootnote}{}
\footnotetext{ Equal contribution.   Corresponding author. This work was done during Mingxing Li's internship at Alibaba.}
\vspace{-.5em}
Space-time memory (STM) based video object segmentation (VOS) networks usually keep increasing memory bank every several frames, which shows excellent performance. However, 1) the hardware cannot withstand the ever-increasing memory requirements as the video length increases. 2) Storing lots of information inevitably introduces lots of noise, which is not conducive to reading the most important information from the memory bank. In this paper, we propose a Recurrent Dynamic Embedding (RDE) to build a memory bank of constant size. Specifically, we explicitly generate and update RDE by the proposed Spatio-temporal Aggregation Module (SAM), which exploits the cue of historical information. To avoid error accumulation owing to the recurrent usage of SAM, we propose an unbiased guidance loss during the training stage, which makes SAM more robust in long videos. Moreover, the predicted masks in the memory bank are inaccurate due to the inaccurate network inference, which affects the segmentation of the query frame. To address this problem, we design a novel self-correction strategy so that the network can repair the embeddings of masks with different qualities in the memory bank. Extensive experiments show our method achieves the best tradeoff between performance and speed. Code is available 
at \url{https://github.com/Limingxing00/RDE-VOS-CVPR2022}.
\end{abstract}



\section{Introduction\label{sec:intro}}
Video object segmentation (VOS) is a fundamental task for video understanding, including lots of applications, such as autonomous driving and video editing. This work focuses on semi-supervised VOS setting. In this setting, given the instances annotation of the first frame, the VOS algorithms segment the instances in other frames.

\begin{figure}[t]
\centering \includegraphics[width=.95\linewidth]{figure/fig1.pdf}
\vspace{-.3em}
\caption{The inference pipelines of the segmentation of frame . \raisebox{.4pt}{\textcircled{\raisebox{-.8pt} {c}}} denotes concatenation.  denotes the sampling interval for the update of the memory bank. (a) shows the network read the space-time memory (STM) pattern memory bank to segment frame . As the length of videos increases, the STM pattern memory bank has an ever-increasing size. In (b), we update a recurrent dynamic embedding (RDE) to build a memory bank of the constant size, which is maintained by a spatio-temporal aggregation module (SAM).}
\label{fig:fig1}
\vspace{-1em}
\end{figure}


Matching based networks \cite{oh2018fast,yang2018efficient,lu2020video, oh2019video, seong2020kernelized, zhang2020transductive, cheng2021modular, hu2021learning,liang2021video,mao2021joint,seong2021hierarchical} are popular for semi-supervised VOS. These networks have a memory bank mechanism, which encodes some frames into embeddings and stores those embeddings in the memory bank to assist the segmentation of the query frame.
Some methods only use the embeddings of a limited number of frames, such as 
the ground-truth (GT) frame \cite{hu2018videomatch}, the latest frame
(for brevity, the latest frame of the query frame is abbreviated as the latest frame) \cite{perazzi2017learning} and both of them \cite{oh2018fast,yang2018efficient,lu2020video}. These methods do not make full use of historical frames in the video. STM based methods \cite{oh2019video, seong2020kernelized,zhang2020transductive,cheng2021modular,hu2021learning,liang2021video,mao2021joint,seong2021hierarchical} store the embeddings every several (e.g., 5) frames in the STM pattern memory bank as shown in Figure \ref{fig:fig1}(a). Although STM based methods utilize equal interval sampling to mine the historical information in the video, as the length of videos increases, the STM pattern memory bank has an ever-increasing size and inevitably introduces lots of noise. Exponential moving average (EMA) based methods \cite{li2020fast, liang2020video, wang2021swiftnet} try to address the problems. The EMA based methods index some pixel embeddings from the embeddings of the query frame and the memory bank according to certain criteria and fuse these pixel embeddings in the EMA way.  However, the EMA based methods have a strong limitation because of the direct summation operation (see details in Sec. \ref{subsec:revisit}). 







In this paper, we address two problems. 1) How to build and update a memory bank of the constant size to effectively and efficiently store historical information? 2) Except for the GT frame, other masks are inaccurate owing to the inaccurate network inference, how to correct the poor embedding encoded from the inaccurate masks?




For problem 1, 
we propose a recurrent dynamic embedding (RDE) to provide a richer representation for VOS. As shown in Figure \ref{fig:fig1}(b), to generate and update RDE, we propose a  spatio-temporal aggregation module (SAM) to organize the cue of the historical information (previous RDE) and the embedding of the latest frame adaptively. 
SAM includes three parts: \textit{extracting}, \textit{enhancing} and \textit{squeezing}. The \textit{extracting} part is responsible for organizing the spatio-temporal relationship between previous RDE and the embedding of the latest frame. Then, the \textit{enhancing} part reinforces the spatio-temporal relationship and the \textit{squeezing} part aggregates and compresses the spatio-temporal information. We refer to the memory bank maintained by SAM as the SAM pattern memory bank. 

One potential risk of the SAM pattern memory bank is the recurrent update of RDE may cause error accumulation. However, we have no GT for training the generated RDE directly. To tackle this problem, we propose to employ auxiliary supervision for the distribution of RDE. In the training process, we additionally build a STM pattern memory bank (see Figure \ref{fig:fig1}(a)) to obtain the uncompressed information and its read results, which are used to estimate the distribution for RDE. Thus we design an unbiased guidance loss to control the approach degree of the two distributions. Relying on the unbiased guidance loss, the training of the network is more stable and has higher performance with no extra computation overhead for deployment.



For problem 2, we design a novel self-correction strategy, which enforces the network to repair the embeddings of masks with different qualities in the memory bank. Specifically, we first simulate different perturbated masks and then constrain the embeddings encoded by perturbated masks to be close to the embedding encoded by the GT mask with a mask consistency loss. The mask consistency loss enforce the network to learn the self-correction ability for inaccurate masks in the embedding space during the training stage.

To investigate the effectiveness of the proposed methods, we conduct experiments on DAVIS 2017, DAVIS 2016 and YouTube-VOS 2019. The proposed method achieves state-of-the-art performance on DAVIS 2017 validation set (86.1\%  \&, 27 FPS), DAVIS 2017 test set (78.9\%  \&), DAVIS 2016  (91.6\% \&, 35 FPS) and superior performance on YouTube-VOS 2019 (83.3\% \&) without the multi-scale inference. Furthermore, we  demonstrate the effectiveness of our method in the synthetic long video. For the synthetic long video,  and FPS of our method are almost unchanged as the length of the synthetic long video increases.

Our contributions can be summarized as follows:
\vspace{-0.7em}
\begin{itemize}
    \item We propose an easy-to-extend recurrent dynamic embedding (RDE) to provide a richer representation for VOS compared with the embedding of the GT frame and the latest frame, which is maintained by the proposed spatio-temporal aggregation module (SAM).
    \vspace{-.5em}
    \item To avoid error accumulation owing to the recurrent usage of SAM, we propose an unbiased guidance loss during the training stage, which makes SAM  more robust in long videos. 
    \vspace{-.5em}
    \item Considering inaccurately predicted masks in the memory bank affect the segmentation performance due to the inaccurate network inference, we design a novel self-correction strategy, which enforces the network to learn the self-correction ability for inaccurate masks in the embedding space.
    \vspace{-.5em}
    \item Extensive experiments on several benchmarks and  the synthetic long video show the effectiveness and superiority of our method.
\end{itemize}
\vspace{-.5em}



\begin{figure*}[!t]
\centering \includegraphics[width=1\linewidth]{figure/network.pdf}
\vspace{-1.2em}
\caption{
Illustrating the architectures: (a) The main pipeline of our framework. During the training stage, we maintain two individual memory banks which are updated in the STM pattern and our SAM pattern separately. During the inference, we only utilize our SAM pattern memory bank.  denotes the sampling interval for the update of the memory bank. (b) Self-correction strategy. The proposed mask consistency loss  enforces the mask encoder to learn the self-correction ability for the inaccurate masks. (c) The structure of SAM, which organizes the historical information and the embedding of the latest frame adaptively. }
\vspace{-1em}
\label{fig:network}
\end{figure*}


\section{Related Work \label{sec:related}}
\subsection{Semi-supervised VOS}
Semi-supervised VOS mainly focuses on propagating the certain object mask of one frame. It can be roughly divided semi-supervised VOS into three categories: 1) Online fine-tuning based methods \cite{xiao2018monet, maninis2018video}, which usually learn general segmentation features and fine-tune the network to the target video during the test time. 2) Propagation based methods \cite{luiten2018premvos, chen2020state}, which refine the target segmentation mask in a temporal label propagation way. 3) Matching based methods \cite{oh2019video, seong2020kernelized, zhang2020transductive, cheng2021rethinking, mao2021joint} which encode some frames into embeddings and store those embeddings in the memory bank to segment the query frame. 
\subsection{Matching based VOS network}
STM~\cite{oh2019video} is a popular network in matching based methods, which constructs a continuously updated memory bank of historical frames. Compared with using limited frames (the GT frame \cite{hu2018videomatch} or the latest frame \cite{perazzi2017learning}), memory bank excavates the information of historical frames even more. Recently, matching based networks have received widespread attention. \cite{seong2020kernelized, lu2020video, cheng2021modular, hu2021learning} improve the readout operation of the memory bank, \cite{xie2021efficient} applies a local attention with the help of the optical flow, \cite{ge2021video} utilizes the global and instance embedding learning to address multi-objects VOS. Although these methods have achieved satisfactory performance, they ignore two key problems: 1) As the number of video frames increases, the hardware cannot afford the ever-increasing memory requirements. 2) Storing lots of information inevitably introduces lots of noise, which is not conducive to reading the most important information from the memory bank.



\subsection{Efficient VOS network}
\vspace{-0.3em}
The methods of efficient VOS usually belong to propagation based methods or matching based methods. SAT \cite{chen2020state} is one of propagation based methods, which deals with each object as a tracklet and segments the object via two feedback loops. OSMN \cite{yang2018efficient} is one of the matching based methods, which adopt the GT frame and the latest frame to guide the segmentation of the query frame with two modulators. Recently, the most popular inference setting for VOS is to save the feature embedding of historical frames every 5 frames (STM pattern). Some methods \cite{li2020fast, liang2020video, wang2021swiftnet} try to use exponential moving average (EMA) to build a more efficient characterization to record the historical information. However, these methods only perform between the most similar embeddings due to the direct summation operation (see details in Sec. \ref{subsec:revisit}), which is a strong limitation. 






\section{Method}
\subsection{Revisit Memory Bank Update with EMA}
\label{subsec:revisit}
In STM \cite{oh2019video}, the image and mask are encoded into two embedding spaces, named \textit{key} and \textit{value}. In addition to the \textit{key} and \textit{value} of the GT frame and the latest frame, previous EMA based methods build an independent embedding, . Take \textit{key} update as an example, let  denotes the \textit{key} at time  and  denotes the \textit{key} of the  query frame  , where  and  are the coordinate of the spatial position. \cite{li2020fast, liang2020video} utilize EMA to update the historical embedding  with the query embedding   by certain rules (see details in the supplementary material). The new embedding  in the memory bank can be formulated as follows:

where  is a hyper-parameter to control the update strength and  denotes the update interval. We argue that EMA based methods have a strong limitation, in which the two additional items in Eq. \ref{eq:EMA} must be similar in the parameter space because of the summation operation. Thus these methods \cite{li2020fast, liang2020video} index the most similar embeddings to update.  Our method associates the embeddings to update the extra embedding adaptively.
\subsection{Framework Overview}
\paragraph{Encoders.}
The main pipeline of our framework is illustrated in Fig \ref{fig:network}(a). For a query frame of size , the image encoder  is responsible for extracting image features. We also adopt a mask encoder  to encode a certain frame and its mask to store into the memory bank. Both the encoders adopt ResNet-50 \cite{he2016deep} as the backbone and use two simple projection heads following STM \cite{oh2019video} to obtain two embeddings, \textit{key}  and \textit{value} . Here  and  are the numbers of the channel dimension (,  in our experiments). 

\vspace{-0.5em}
\paragraph{Memory Reading and Decoder.}
Following STCN \cite{cheng2021rethinking}, for the SAM pattern memory bank  at time ,  we keep target-agnostic key  and target-specific value , where  denotes the -th object. For the similarity  between the key from the SAM pattern memory bank  and the key of the query frame , we perform negative squared Euclidean distance, which can be formulated as 

where  and  are the coordinate of the spatial
position of  and  separately. And the softmax operation is applied on the spatial dimension for similarity  to obtain the  softmax-normalized affinity matrix , . Relying on , the readout feature  of the -th object from the SAM memory bank can be obtained by the matrix multiplication : 

The readout feature  concatenates with the value of the query frame to pass through the light-weight decoder described in \cite{cheng2021rethinking} to get the segmentation results  of the -th object at frame . Similar to the SAM pattern memory bank, we concatenate the readout feature  from the STM pattern memory bank  with the value of the query frame to obtain the segmentation results  of the -th object at frame .






\subsection{SAM Pattern Memory Bank}
The main challenge of keeping the size of the memory bank constant is how to select the most useful information. The STM pattern memory bank can store the historical information losslessly, but has ever-increasing size and inevitably introduces lots of noise. In our design, we build a SAM pattern memory bank to address the challenge. During the training stage, the STM and SAM pattern memory banks are maintained at the same time. During the inference, we only use the SAM pattern memory bank, which can keep the size of the memory bank constant. Specifically, the STM pattern memory bank  includes , while the SAM pattern memory bank  includes .  
\paragraph{Recurrent Dynamic Embedding.}
We find the embedding of the latest frame changes over time, providing more useful information for the segmentation of the query frame but lacking the use of historical information. We propose a recurrent dynamic embedding (RDE) in the memory bank to fuse the the cue of the historical information with the the embedding of the latest frame to provide a richer representation for VOS. We denote the RDE embedding at time  as .  
\paragraph{Spatio-temporal Aggregation Module.} 
To generate and update RDE, we propose a spatio-temporal aggregation module (SAM), which exploits the cue of historical information.  SAM includes three parts: \textit{extracting}, \textit{enhancing} and \textit{squeezing} as shown in Figure \ref{fig:network}(c). The \textit{extracting} part is responsible for organizing the spatio-temporal relationship between the embedding of previous RDE  ( denotes the sampling interval) and the embeddings of the latest frame . First, we concatenate previous RDE  and the embedding of the latest frame  to obtain the feature . Take  update as an example,  

where  denotes concatenation operation in the time dimension. Inspired by self-attention mechanism \cite{wang2018non}, in the \textit{extracting} part, we organize the spatio-temporal relationship between  previous RDE  and the embedding of the latest frame  to obtain the aggregation feature ,
 
 is a normalization factor, which presents the total number of the spatial position of . The function ,  and  are  convolution in our implementation.  denotes  processed by the max-pooling operation (no down sampling on the time axis), which can decrease the computational complexity. 

Relying on the aggregation feature , in the \textit{enhancing} part, we enhance   in the form of residuals by atrous spatial pyramid pooling (ASPP) \cite{chen2017deeplab}. Finally, in the \textit{squeezing} part, we compress the enhanced feature by a simple  convolution, which is denoted as   function. The formula can be expressed as

previous RDE and the embedding of the latest frame adaptively fuse and maintain the constant size for the memory bank, where the key mapping is   . For the multiple objects, we concatenate the object dimension to the batch dimension like the implementation of STM \cite{oh2019video}. For the key and value of RDE, we maintain two different SAMs respectively.





\paragraph{Unbiased Guidance Loss.}
One potential risk of the SAM pattern memory bank is the update of RDE may cause error accumulation, especially when it is used repeatedly. Another problem is that the key and value of RDE are generated separately by two different SAMs, the distribution of them is difficult to directly define. Suppose the update process of the STM pattern memory bank is a good teacher, the estimated distribution read from the SAM pattern memory bank ought to approach the estimated distribution read from the STM pattern memory bank. 
Thus, during the training stage, we maintain two individual memory banks for the segmentation of the query frame, which is updated in the STM and SAM patterns separately.
We propose an unbiased guidance loss , which controls the distribution of the readout feature from the SAM pattern memory bank  to approach the distribution of the readout feature from the STM pattern memory bank . The unbiased guidance loss  is computed as follows:

 function denotes Kullback–Leibler (KL) divergence, which is a non-symmetric measure of the difference between two distributions.




\paragraph{Self-correction Strategy.}
Considering the quality of the mask in the memory bank affects the segmentation of the query frame, we propose a mask consistency loss   to constrain the consistency of the embedding of masks of different qualities and the embedding of the GT mask.  
We first obtain the key  and value  of the first frame. And we perform perturbation transform such as the random dilation and eroding on the first frame to obtain the perturbated key  and perturbated value . The mask consistency loss  can be calculated by 

where  function denotes KL divergence. 


\paragraph{Overall Loss Functions.}
During the training stage, we sample 5 frames. Inspired by the slowfast network \cite{feichtenhofer2019slowfast}, we utilize the SAM pattern memory bank to segment the third and fifth frames to handle different rate of videos. Besides, we utilize STM pattern memory bank to segment the second and fourth frames for the training stability.  We use bootstrapped cross entropy (BCE) following \cite{cheng2021modular} to supervise the final segmentation results, which is computed as follows:

where  and  denote the segmentation results read from the STM pattern memory bank and the SAM pattern memory bank separately.  denotes the GT mask of the -th object at frame . The overall loss function is computed as follows:

where  and  are hyper-parameters to control the strength. We set  and  in our experiments.  is the indicator function.

\paragraph{Inference Strategy.}
As shown in Figure \ref{fig:network}(a), during the inference, we employ SAM recurrently to update RDE. Specifically, in a video of any length, SAM inputs previous RDE at time  and the embeddings of the latest frame at time   to generate RDE at time , where  is the sampling interval. The new RDE is stored in the SAM pattern memory bank to assist the segmentation of the query frame and the old RDE is discarded.
































\section{Experiments}
\subsection{Datasets and Metrics}
\paragraph{DAVIS.} DAVIS 2016 \cite{davis16} is a popular benchmark for  video single object segmentation, whose validation set includes  20 videos. DAVIS 2017 \cite{davis17} is a popular benchmark for video multiple objects segmentation, whose validation set and test set are 30 densely annotated videos. 
\vspace{-0.3em}
\paragraph{YouTube-VOS.} YouTube-VOS 2019 \cite{youtube} is a large-scale benchmark for multi-object video segmentation, providing 3,471 videos for the training (65 categories) and 507 videos for the validation. There are additional 26 unseen categories in the validation set for evaluating the generalization.
\vspace{-0.3em}
\paragraph{Metrics.}
For the DAVIS datasets, we use the region similarity , the contour accuracy  and their average  to evaluate the segmentation results. For YouTube-VOS 2019, we follow the official evaluation server to report   and  of the seen and unseen categories, and the average of them. 



\subsection{Implementation Details}
\paragraph{Training Stages.}
Following STCN \cite{cheng2021rethinking}, we first train the network equipped with the STM pattern memory bank on the static datasets \cite{wang2017learning,shi2015hierarchical,zeng2019towards,cheng2020cascadepsp,li2020fss} with 75k iterations and batch size of 64.  The static images are processed by synthetic deformations like STM \cite{oh2019video}. Secondly, we train the network quipped with the SAM and STM pattern memory banks on  BL30K \cite{chang2015shapenet,denninger2019blenderproc} proposed in \cite{cheng2021modular} with 500k iterations and batch size of 8. Finally, we fine-tune the network quipped with the SAM and STM pattern memory banks  on YouTube-VOS and DAVIS 2017 with 75k iterations and batch size of 16 (main stage). BatchNorm layers are frozen during the training stage following \cite{oh2019video}.
\vspace{-0.5em}
\paragraph{Training Details.} 
\label{sec:training details}
We adopt four 16 GB Tesla V100 GPUs to implement Pytorch. All networks are optimized by Adam optimizer \cite{kingma2014adam}. We pretrain the network on the static datasets and  BL30K  with the initial learning rate of 2e-5 and 1e-5. And we fine-tune the network on the main stage with the initial learning rate of 2e-5. The data augmentation is the same as STCN \cite{cheng2021rethinking}. Besides, we sample 3 frames in the first pre-training stage and 5 frames in other stages.
\vspace{-0.5em}
\paragraph{Inference Details.} 
\label{sec:Inference details}
During inference, we only use the SAM pattern memory bank. Specifically, in addition to maintaining our RDE by SAM, we sample the embedding of the latest frame and two repeated embedding of the GT frame. This setting is to keep a sampling balance between the accurate template information (GT frame) and dynamic information (latest frame or our RDE). We use top-k filters \cite{cheng2021modular}  on all datasets. The sampling interval  is set to 3 on all DAVIS datasets and 4 on YouTube-VOS 2019.  





\begin{table}[t]
\centering
\begin{tabular}{l c c c c c}
\hlineB{3}
Method & CC & {\&} &  &  & FPS\\ \midrule
STM~\cite{oh2019video}&&81.8&79.2&84.3&10.2\\
KMN \cite{seong2020kernelized}&&82.8 &80.0 &85.6 & \textless 8.4\\
JOINT \cite{mao2021joint} &&83.5 & 80.8& 86.2 &4.0 \\
LCM~\cite{hu2021learning}&&83.5 &80.5 &86.5 & \textless 8.5 \\
RMNet \cite{xie2021efficient} &&83.5&81.0&86.0&\textless 11.9\\
MiVOS  \cite{cheng2021modular} 
&& 84.5  & 81.7 & 87.4 & 11.2\\
HMMN \cite{seong2021hierarchical}&&84.7 &81.9 &87.5 & \textless 10.0 \\
STCN  \cite{cheng2021rethinking} && \textbf{85.3}  & \textbf{82.0} & \textbf{88.6} & \textbf{20.2} \\ \hline
GCNet~\cite{li2020fast}&&71.4&69.3&73.5&\textless 25.0\\
Liang \textsl{et al.}~\cite{liang2020video}   && 74.6  & 73.0 & 76.1 & 4.0\\ 
G-FRTM \cite{park2021learning} &&76.4 & -&- & 18.2 \\
PReMVOS~\cite{luiten2018premvos} &  & 77.8& 73.9& 81.7 & 0.01\\
SwiftNet~\cite{wang2021swiftnet}  && 81.1 & 78.3 & 83.9 &\textless 25.0\\ 
SST~\cite{duke2021sstvos} &&82.5 & 79.9& 85.1 & -\\ 
Ge \textsl{et al.}~\cite{ge2021video}  &&82.7 & 80.2& 85.3 & 6.7\\
\textbf{RDE-VOS}  &&  84.2  & 80.8  & 87.5  & \textbf{27.0} \\
\textbf{RDE-VOS}   && \textbf{86.1} & \textbf{82.1} & \textbf{90.0} & \textbf{27.0} \\
\hlineB{3}
\end{tabular}
\caption{Results on the DAVIS 2017 validation set. CC denotes constant cost during the inference.  indicates YouTube-VOS \cite{youtube} is added during the training stage.  denotes BL30K \cite{cheng2021modular} is added during the training stage.}
\label{tab:davis17}
\end{table}
\begin{table}[t]
	\centering
	\setlength{\tabcolsep}{1.8mm}{\begin{tabular}{lcccccc}
		\hlineB{3}
		Method
		 &CC& 600p &  &  &   \\
		\midrule
		STM~\cite{oh2019video} & &  & 72.2 & 69.3 & 75.2 \\
		KMN ~\cite{seong2020kernelized} & &  & 77.2 & 74.1 & 80.3\\
		RMNet ~\cite{xie2021efficient}&  &   & 75.0 & 71.9 & 78.1  \\
		Ge \textsl{et al.}~\cite{ge2021video}&  &&75.2 & 72.0& 78.3 \\
STCN  \cite{cheng2021rethinking} & &  & 77.8 & 74.3 & 81.3\\ 
		MiVOS  \cite{cheng2021modular}  & &  & \textbf{78.6} & \textbf{74.9} & \textbf{82.2} \\\hline
		CFBI ~\cite{yang2020collaborative} & &  & 74.8 & 71.1 & 78.5 \\
		Ge \textsl{et al.}~\cite{ge2021video} &  &&75.2 & 72.0& 78.3\\
		CFBI+ ~\cite{yang2021collaborative} & &  & 75.6 & 71.6 & 79.6 \\
		\textbf{RDE-VOS} &  &  & 77.4 & 73.6 & 81.2 \\
		\textbf{RDE-VOS} &  &  & \textbf{78.9} & \textbf{74.9} & \textbf{82.9}\\
		\hlineB{3}
	\end{tabular}}
	\caption{Results on the DAVIS 2017 test set. 600p denotes evaluating on 600p resolution.
}
	\vspace{-1em}
	\label{tab:davis17-test}
\end{table}


\subsection{Compare with the State-of-the-art Methods}
We denotes the memory bank of the constant size during the inference as \textbf{Constant Cost} (\textbf{CC}). As the video length increases during the inference, the CC methods can maintain a relatively stable speed and constant requirements of the memory.
For brevity, our \uline{R}ecurrent \uline{D}ynamic \uline{E}mbedding for  \uline{VOS} method is denoted as \textbf{RDE-VOS}. 
\vspace{-0.7em}
\paragraph{DAVIS.}
We compare the proposed method with previous state-of-the-art methods for VOS on the DAVIS 2017 validation set, DAVIS 2017 test set and DAVIS 2016 validation set.  On the DAVIS 2017 validation set, as shown in Table \ref{tab:davis17}, our method even outperforms STCN \cite{cheng2021rethinking} by 0.7\% for {\&} and runs about 35\% faster (27 \textit{vs} 20.2 FPS). Compared with SwiftNet \cite{wang2021swiftnet}, our method suppresses it by 5\%  for {\&} and has a slight advantage for the speed (+2 FPS). On the DAVIS 2017 test set, as shown in Table \ref{tab:davis17-test}, our method still has great advantages. On DAVIS 2016 validation set, as shown in Table \ref{tab:davis16}, our method outperforms CC method SwiftNet \cite{wang2021swiftnet} by 1.2\% for {\&} and runs about 40\% faster (35 \textit{vs} 25 FPS). Compared with STCN \cite{cheng2021rethinking}, our method is 30\% faster while {\&} is almost unchanged (-0.1\%). We also show the qualitative result of the validation set of DAVIS 2017 in Figure \ref{fig:results}. More qualitative results can be found in the supplementary material. 


\vspace{-0.5em}
\paragraph{YouTube-VOS.}
On a large-scale YouTube-VOS 2019 validation set, we compare our method with recent state-of-the-art methods in Table \ref{tab:youtube19}. Although our method does not surpass STCN on YouTube-VOS 2019 validation set, it still surpasses other state-of-the-art methods, regardless of whether BL30K is added.
\vspace{-1em}
\paragraph{Synthetic Long Video.}
Recently, the popular benchmarks include short video clips. For example, DAVIS 2017 only has 67 frames per video clip on average. However, many practical applications need to handle more frames. Compared with STCN \cite{cheng2021rethinking}, we demonstrate the effectiveness of our method in the scene where includes more frames. Take a DAVIS 2017 case ``cows" (the basic length is 104) as an example, exerting video forward and backward as a basic unit, we repeatedly sample multiple basic units to synthesize a long video. This synthesis method ensures each frame contains GT and the adjacent frames have smooth transitions. As shown in Figure~\ref{fig:long-video}, as the length of the synthetic long video increases, the performance and speed of our method is almost unaffected, while the performance and speed of STCN obviously decrease. Here we do not change any hyper-parameter compared with the setting on the DAVIS datasets. Besides, we utilize the official code of STCN and minimize the sampling interval to 60 frames under maximizing the usage of the GPU memory. All input data is stored in the CPU and inferred on one GPU.
 \begin{table}[t]
\centering
\begin{tabular}{l c c c c c}
\hlineB{3}
Method & CC & {\&} &  &  & FPS\\ \midrule
RMNet \cite{xie2021efficient}&&88.8 &88.9 &88.7 & 11.9 \\
STM~\cite{oh2019video}&&89.3&88.7&89.9&6.3\\
KMN~\cite{seong2020kernelized}&&90.5 &89.5 &91.5 &8.4 \\
LCM~\cite{hu2021learning}&&90.7 &89.9 &91.4 &8.5 \\
HMMN \cite{seong2021hierarchical} &&90.8 &89.6 &92.0 & 10.0 \\
MiVOS \cite{cheng2021modular} &&91.0 &89.7 &92.4 & 16.9 \\
STCN  \cite{cheng2021rethinking} && \textbf{91.7} &\textbf{90.4} &\textbf{93.0} & \textbf{26.9} \\
\midrule
GCNet~\cite{li2020fast}&&86.6 &87.6 &85.7 & 25.0 \\
CFBI+~\cite{yang2021collaborative}&&89.9 &88.7 &91.1 &5.9 \\
SwiftNet~\cite{wang2021swiftnet}&&90.4 &\textbf{90.5} &90.3 & 25.0\\
\textbf{RDE-VOS}  && 91.1 & 89.7 & 92.5 & \textbf{35.0} \\
\textbf{RDE-VOS}  && \textbf{91.6} & 90.0 & \textbf{93.2} & \textbf{35.0} \\
\hlineB{3}
\end{tabular}
\caption{Results on the DAVIS 2016 validation set. CC denotes constant cost during the inference.
}
\label{tab:davis16}
\end{table}
\begin{table}[!t]
\centering
\resizebox{1\columnwidth}{!}{
\begin{tabular}{l c c c c c c}
\hlineB{3}
Method & CC & Overall&  &  &  &  \\ \midrule
STM \cite{oh2019video} & & 79.2 &79.6 & 83.6 & 73.0 & 80.6 \\
MiVOS \cite{cheng2021modular} & &82.4 &80.6 & 84.7 & 78.2 & 85.9 \\
STCN \cite{cheng2021rethinking} &&\textbf{84.2} &\textbf{82.6} & \textbf{87.0} &\textbf{79.4} & \textbf{87.7} \\ \hline
CFBI \cite{yang2020collaborative} & &81.0 &80.6 & 85.1 & 75.2 & 83.0 \\
SST \cite{duke2021sstvos}  && 81.8 &  80.9 & - & 76.6 & - \\  
\textbf{RDE-VOS } && 81.9  & 81.1 & 85.5   & 76.2  & 84.8 \\
\textbf{RDE-VOS } && \textbf{83.3} & \textbf{81.9} & \textbf{86.3} & \textbf{78.0} & \textbf{86.9} \\
\hlineB{3}
\end{tabular}}
\caption{Results on the YouTube-VOS 2019 validation set. 
}
\label{tab:youtube19}
\vspace{-1.5em}
\end{table}
\vspace{-1em}
\paragraph{Inference Time.}
We evaluate the inference time on one Tesla V100 GPU with full floating point precision. On the validation set of DAVIS 2017 and DAVIS 2016, as shown in Table \ref{tab:davis17} and \ref{tab:davis16}, our method has a great advantage in speed compared with STCN (27 \textit{vs} 20.2 FPS on DAVIS 2017 and 35 \textit{vs} 26.9 FPS on DAVIS 2016). 


\begin{figure}[t]
\centering \includegraphics[width=1\linewidth]{figure/long-video.pdf}
\vspace{-1.5em}
\caption{
\textcolor[RGB]{70, 130, 180}{} and  \textcolor[RGB]{255, 0, 0}{\textbf{\textit{FPS}}} of our method and STCN \cite{cheng2021rethinking} on the synthetic long video. Note different colored lines refer to different metrics. When the length of the synthetic long video is 1, 10, 15 and 20 times of the original,  and FPS of our method are almost unchanged. However, both  and FPS of STCN have an obvious reduction.}
\label{fig:long-video}
\vspace{-1em}
\end{figure}
\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{figure/results2.pdf}
\caption{Qualitative results on the DAVIS 2017 validation set. We compare MiVOS \cite{cheng2021modular} and STCN \cite{cheng2021rethinking} under the challenging scale and deformation case, and our method has a notable improvement.}
\label{fig:results}
\vspace{-1.3em}
\end{figure*}
\subsection{Ablation Study}
\paragraph{Dataset Setting.}
We compare the results whether to adopt BL30K \cite{cheng2021modular} in Table \ref{tab:davis17}, \ref{tab:davis17-test}, \ref{tab:davis16} and \ref{tab:youtube19}. Without the BL30K pre-training, our method has superior performance on all datasets with a higher speed compared with other state-of-the-art methods. After adding the BL30K pre-training, our method has a stable improvement on all datasets.

\paragraph{Inference Setting.}
\label{sec:ablation-inference}
Table \ref{tab:ablation-inference} shows different inference strategies adopting the memory bank. Compared with only using the embedding from the first frame or the latest frame, only using our RDE has the best performance of 81.8\% for {\&}. Besides, based on using the embeddings from the first frame, the latest frame, and both of them, adding our RDE can further improve  {\&} by 13.7\%, 1.8\% and 0.8\% separately.  Based on using RDE and the embedding of both the first frame and the latest frame, we explore the sampling balance of the accurate template information (GT frame) and dynamic information (latest frame or our RDE).  We find additionally sample the embedding of the GT frame to keep the sampling balance between the two types of information can further improve  {\&} by 0.7\%. We use this strategy in all experiments unless otherwise specified.
We also show the ablation of different sampling intervals , where the sampling interval of 3 provides the best result.
\vspace{-0.5em}
\paragraph{Loss Function Setting.}
In Table \ref{tab:ablation-loss}, we perform ablation of different loss functions without the BL30K \cite{cheng2021modular} pre-training. Both our proposed  and   can improve the performance to different degrees, and their combination can maximize the performance (+1.7\% {\&}). Besides, although we do not use the STM pattern memory bank during the inference, we find supervising the segmentation results of the STM pattern item  in Eq. \ref{eq:segloss} can assist the training of the SAM pattern (+1.2\%  {\&}). 

\begin{table}[!t]
	\centering
	\begin{tabular}{lccc}
	\hlineB{3}
    Variants &&& \\ \hline
    \multicolumn{4}{c}{Strategy permutation} \\\hline
    RDE & 81.8 & 78.0 & 85.7 \\\hline
    First frame & 71.6 & 67.8 & 75.4\\ 
    First frame \& RDE & 85.3 & 81.6 & 89.0 \\\hline
    Latest frame & 80.4 & 76.9 & 83.8\\
    Latest frame \& RDE & 82.2 & 78.4& 86.0\\\hline
    First frame \& latest frame & 84.6  & 81.0 & 88.2 \\
    F \& L \& RDE  & 85.4 & 81.6& 89.2 \\ \hline
    First frame 2  \& latest frame & 85.1  & 81.5 & 88.7 \\
    First frame \& latest frame 2 & 84.0  & 80.4 & 87.6\\
    2F \& L \& RDE & \textbf{86.1} & \textbf{82.1} & \textbf{90.0} \\ \hline
    \multicolumn{4}{c}{Sampling interval } \\\hline
    2F \& L \& RDE ( 2) & 85.1 & 81.4 & 88.9 \\ 
    2F \& L \& RDE ( 3) & \textbf{86.1} & \textbf{82.1} & \textbf{90.0} \\
    2F \& L \& RDE ( 4) & 85.1&81.5 &88.8\\
    2F \& L \& RDE ( 5) & 84.2&80.5 &87.9 \\
    \hlineB{3}
	\end{tabular}
	\caption{Ablation of inference strategies on DAVIS 2017 validation set. F \& L \& RDE represents first frame, latest frame and RDE. 2F represents we sample the embeddings of the GT frame twice in order to keep balance of the accurate template information and dynamic information, which is used in all experiments unless otherwise specified. }
	\label{tab:ablation-inference}
	\vspace{-1.3em}
\end{table}
\begin{table}[t]
	\centering
	\resizebox{1\columnwidth}{!}{
	\begin{tabular}{ccccc}
	\hlineB{3}
    \multicolumn{1}{c|}{} &Ablation Settings &     &&\\ \hline
\multicolumn{1}{c|}{\multirow{3}{*}{Loss}} & w/o  &83.7 & 80.5 & 86.9    \\
	 \multicolumn{1}{c|}{}  &w/o  & 82.9 & 79.5 & 86.4 \\ 
	 \multicolumn{1}{c|}{}  &w/o  \&  & 82.5 & 79.1 & 86.0 \\ 
	 \multicolumn{1}{c|}{}  &  w/o STM pattern item  & 83.0 & 79.4 & 86.6 \\\hline
\multicolumn{1}{c|}{}  &Full &  \textbf{84.2} & \textbf{80.8}  & \textbf{87.5}\\
\hlineB{3}
	\end{tabular}}
	\vspace{-0.5em}
	\caption{Ablation of different loss functions without the BL30K \cite{cheng2021modular} pre-training.}
	\label{tab:ablation-loss}
	\vspace{-1em}
\end{table}


\subsection{Limitations.}
During the inference, we set the sampling interval of the RDE update to . This simple setting is easy to plug in other matching based VOS methods. We fix the sampling interval  on the DAVIS datasets and achieve the new state-of-the-art performance. We increase the sampling interval on YouTube-VOS by 1 to fit the motion pattern on YouTube-VOS. 
A better solution for future work is to use a learnable discriminator \cite{goodfellow2020generative} or gate mechanism \cite{yu2019free} to adaptively control the update interval of SAM, which can handle different scenes better.












\section{Conclusion}
In this paper, we explore how to build and update a memory bank of the constant size to maximize the segmentation performance of the query frame. The key insight is we propose a recurrent dynamic embedding (RDE) to provide a richer representation for VOS compared with the embeddings of the GT frame and the latest frame. To generate and update RDE, we propose a novel spatio-temporal aggregation module (SAM), which organizes the cue of the historical information and the embedding of the latest frame adaptively. To avoid error accumulation owing to the recurrent usage of SAM, we propose an unbiased guidance loss during the training stage, which makes SAM more robust in long videos. Besides, we design a novel self-correction strategy so that the network can encode and self-repair the embeddings of masks with different qualities. 
\paragraph{Acknowledgement.}
We acknowledge funding from National Key R\&D Program of China under Grant 2017YFA0700800, National Natural Science Foundation of China under Grants 61931014, 62131003 and 62021001, and Fundamental Research Funds for the Central Universities under No. WK3490000006.





\small
\normalem
\bibliographystyle{ieee_fullname}
\bibliography{egbib}


\end{document}
