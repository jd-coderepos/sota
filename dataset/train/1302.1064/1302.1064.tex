\documentclass[runningheads]{llncs}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{epsfig}
\usepackage{color}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{url}

\renewcommand{\topfraction}{.9}
\renewcommand{\floatpagefraction}{.8}

\newtheorem{obs}{Observation}
\newtheorem{lemm}[obs]{Lemma}
\newtheorem{thrm}[obs]{Theorem}
\newtheorem{corr}[obs]{Corollary}
\def\algoname#1{\mbox {\textsc{#1}}}
\def\Oh{\mathcal{O}}

\def\codetab{\ \ \ \ }

\def\itbf#1{\textit{\textbf{#1}}}
\def\PRECOPY{\mbox{\textsc{Precopy}}}
\def\BWT{\mbox{\rm L}}
\def\LCS{\mbox{\rm LCS}}
\def\ILF{\mbox{\rm LF}}
\def\lcs{\mbox{\rm lcs}}
\def\str#1{\texttt{#1}}
\def\MATRIX{\mbox{}}
\def\ORIGINAL{\mbox{\rm T}}
\def\FIRST{\mbox{\rm F}}
\def\COUNT{\mbox{\rm C}}
\def\LL{\mbox{\rm LL}}
\def\definition#1{\textit{#1}}
\def\reverse#1{\hat{#1}}

\def\ST{\mbox{\rm {\sf ST}}}
\def\SA{\mbox{\rm {\sf SA}}}
\def\ISA{\mbox{\rm {\sf ISA}}}
\def\LZ{\mbox{\rm {\sf LZ}}}
\def\LF{\mbox{\rm {\sf LF}}}
\def\FMI{\mbox{\rm {\sf FMI}}}
\def\FM{\mbox{\rm {\sf FM}}}
\def\rank{\textsf{rank}}
\def\pathlabel{\mbox{\rm {\sf pathlabel}}}
\def\lcp{\mbox{\rm {\sf lcp}}}
\def\rmq{\mbox{\rm {\sf rmq}}}
\def\x{\mbox{\rm {\sf x}}}
\def\X{\textsf{X}}
\def\MS{\mbox{\rm {\sf MS}}}
\def\B{\textsf{B}}
\def\ssB{\mbox{\rm {\sf {\scriptsize B}}}}
\def\A{\textsf{A}}
\def\Pr{\mbox{\rm {\sf P}}}
\def\R{\mbox{\rm {\sf R}}}
\def\Y{\mbox{\rm {\sf Y}}}
\def\Y{\textsf{Y}}
\def\ssY{\mbox{\rm {\sf {\scriptsize Y}}}}
\def\Z{\textsf{Z}}
\def\C{\mbox{\rm {\sf C}}}
\def\D{\mbox{\rm {\sf D}}}
\def\BWT{\mbox{\rm {\sf BWT}}}
\def\LCP{\mbox{\rm {\sf LCP}}}
\def\LPF{\mbox{\rm {\sf LPF}}}
\def\MBA{\mbox{\rm {\sf M_{{\scriptsize B|A}}}}}
\def\AB{\mbox{\rm {\sf {\scriptsize AB}}}}
\def\BA{\mbox{\rm {\sf {\scriptsize BA}}}}
\def\YZ{\mbox{\rm {\sf {\scriptsize YZ}}}}
\def\ZY{\mbox{\rm {\sf {\scriptsize ZY}}}}
\def\POS{\mbox{\rm POS}}
\def\LEN{\mbox{\rm LEN}}
\def\PLCP{\mbox{\rm PLCP}}
\def\slink{\mbox{\rm {\sf slink}}}
\def\wlink{\mbox{\rm {\sf wlink}}}
\def\NSV{\mbox{\rm {\sf NSV}}}
\def\PSV{\mbox{\rm {\sf PSV}}}
\def\RMQ{\mbox{\rm {\sf RMQ}}}
\def\LZSCAN{\mbox{\sf LZscan}}
\def\LZFMICN{\mbox{\sf LZ-FMI-CN}}
\def\LZFMIBPR{\mbox{\sf LZ-FMI-BPR}}
\def\LZFMIISA{\mbox{\sf LZ-FMI-ISA}}

\def\O{\mbox{\rm O}}


\def\balgorithm#1{{\bf Algorithm #1}}
\def\bproc{{\bf procedure\ }}
\def\bfunc{{\bf function\ }}
\def\bvar{{\bf var\ }}
\def\barray{{\bf array\ }}
\def\bof{{\bf of\ }}
\def\bfor{{\bf for\ }}
\def\bforeach{{\bf foreach\ }}
\def\bto{{\bf to\ }}
\def\bdownto{{\bf downto\ }}
\def\bwhile{{\bf while\ }}
\def\brepeat{{\bf repeat\ }}
\def\buntil{{\bf until\ }}
\def\band{{\bf and\ }}
\def\bor{{\bf or\ }}
\def\bdo{{\bf do\ }}
\def\bif{{\bf if\ }}
\def\bthen{{\bf then\ }}
\def\belse{{\bf else\ }}
\def\belsif{{\bf elsif\ }}
\def\bnot{{\bf not\ }}
\def\bgoto{{\bf goto\ }}
\def\breturn{{\bf return\ }}
\def\boutput{{\bf output\ }}
\def\bbreak{{\bf break\ }}
\def\bmax{{\bf max\ }}
\def\la{\leftarrow}
\def\ra{\rightarrow}
\def\llra{\longleftrightarrow}
\def\q{\quad}
\def\qq{\qquad}
\def\+{\!+\!}
\def\-{\!-\!}
\def\com#1{\hspace{6pt}{\bf ---}\hspace{6pt}{\sl #1}}
\def\rem#1{\hspace{24pt}{\sl #1}}
\def\var#1{\textrm{#1}}

\begin{document}
\title{Lightweight Lempel-Ziv Parsing\thanks{Supported by Academy of Finland grant 118653 (ALGODAN)}}

\author{
Juha K{\"a}rkk{\"a}inen
\and
Dominik Kempa
\and
Simon J. Puglisi
}

\institute{
    Department of Computer Science,\\
    University of Helsinki\\
    Helsinki, Finland\\
    \email{\{firstname.lastname\}@cs.helsinki.fi}\ otherwise. We also define  iff , except when , in which case .
Let , for symbol  be the number of symbols
in  lexicographically smaller than .  The function
, for string , symbol , and integer , returns
the number of occurrences of  in .  It is well known that
.  Furthermore, we can
compute the left extension using  and .  If  is
the -interval,
then 
 is 
the -interval.
This is called \emph{backward search}.

\paragraph{NSV/PSV and RMQ.} For an array , the \emph{next and previous
  smaller value} (NSV/PSV) operations are defined as
  
and
  .  
A related operation on  is \emph{range minimum
  query}:  is  such that  is the
minimum value in . Both NSV/PSV operations and RMQ operations
over the LCP array can be used for implementing right contraction (see
Section~\ref{sec-ms}).

\paragraph{LZ77.}
Before defining the LZ77 factorization, we introduce the concept of a
{\em longest previous factor} (LPF).  The LPF at position  in
string  is a pair  such that, ,
, and  is maximized.
In other words,  is the longest
prefix of  which also occurs at some position  in
. Note that if  is the leftmost occurrence of a symbol in
 then  does not exist. In this case we adopt the convention
that  and . When  does exist we call
 the {\em source} for position . Note also
that there may be more than one potential source (that is, 
value), and we do not care which one is used.

The LZ77 factorization (or LZ77 parsing) of a string  is then just
a greedy, left-to-right parsing of  into longest previous
factors. More precisely, if the th LZ factor (or {\em phrase}) in
the parsing is to start at position , then we output 
(to represent the th phrase), and then the th phrase starts
at position , unless , in which case the next
phrase starts at position .  When , the substring
 is called the {\em source} of phrase
. We denote the number of phrases in the
LZ77 parsing of  by .

\paragraph{Matching Statistics.}
Given two strings  and , the matching statistics of 
w.r.t.~, denoted  is an array of  pairs,
, , ..., ,
such that for all ,  is the longest substring starting at position 
in  that is also a substring of . The observant reader will
note the resemblance to the LPF array. Indeed, if we replace
 with  in the computation of the LZ factorization
of , the result is the relative LZ factorization of 
w.r.t.~~\cite{RLZspire2010}.

\section{Lightweight, Scan-based LZ77 Parsing}
\label{sec-algorithm}

In this section we present a new algorithm for LZ77 factorization
called \LZSCAN.

\paragraph{Basic Algorithm.}

Conceptually \LZSCAN\ divides  up into  fixed size
blocks of length : , , ... . The last
block could be smaller than , but this does not change the
operation of the algorithm.  In the description that follows we will
refer to the block currently under consideration as , and to the
prefix of  that ends just before  as . Thus, if , then .  

To begin with we will assume no LZ factor or its source
crosses a boundary of the block . Later we will show how to remove
these assumptions.

The outline of the algorithm for processing a block  is shown below.
\begin{enumerate}
\item Compute 
\item Compute  from ,  and 
\item Compute  from  and
  
\item Factorize  using 
\end{enumerate}
Step~1 is the computational bottleneck of the algorithm in theory and
practice. Theoretically, the time complexity of Step~1 is
, where  is the time complexity of
the rank operation on  (see, e.g.,~\cite{bgnn2010}). 
Thus the total time complexity of
\LZSCAN\ is  using  words of space in
addition to input and output. 
The practical implementation of Step~1 is described
in
Section~\ref{sec-ms}. In the rest of this section,
we describe the details of the other steps.

\paragraph{Step 2: Inverting Matching Statistics.}

We want to compute  but we cannot afford the space of
the large data structures on  required by standard methods.
Instead, we compute first  involving large data
structures on , which we can afford, and only a scan of  (see
Section~\ref{sec-ms} for details). We then \emph{invert}  to
obtain .
The inversion algorithm is given in Fig.~\ref{fig-msinvert}.

\begin{figure}
  \begin{tabbing}
    00: \=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\kill
    \balgorithm{MS-Invert}\\
    1:\>\bfor  \bto  \bdo \\
    2:\>\bfor  \bto  \bdo\\
    3:\>\>\\
    4:\>\>\\
    5:\>\>\bif  \bthen \\
    6:\>\\
    7:\>\bfor  \bto  \bdo\\
    8:\>\>\\
    9:\>\>\\
    10:\>\>\bif  \bthen
\\
    11:\>\>\belse
\\
    12:\>\\
    13:\>\bfor  \bdownto  \bdo\\
    14:\>\>\\
    15:\>\>\\
    16:\>\>\bif  \bthen
\\
    17:\>\>\belse

  \end{tabbing}
  \caption{Inverting matching statistics}
  \label{fig-msinvert}
\end{figure}


Note that the algorithm accesses each entry of  only once
and the order of these accesses does not matter. Thus we can execute
the code on lines~3--5 immediately after computing 
in Step~1 and then discard that value. This way we can avoid storing
.

\paragraph{Step3: Computing LPF.}

Consider the pair  for 
that we want to compute and assume  (otherwise  is the position
of the leftmost occurrence of  in , which we can easily detect).
Clearly, either  and , or 
and , where .
Thus computing  from
 and  is easy. 

The above is true if the sources do not cross the block
boundary, but the case where  but  is not
handled correctly. An easy correction is to replace  with
 in all of the steps. 

\paragraph{Step 4: Parsing.}

We use the standard LZ77 parsing to factorize  except
 is replaced with .

So far we have assumed that every block starts with a new phrase, or,
put another way, that a phrase ends at the end of every block. Let
 the last factor in , after we have factorized 
as described above. This may not be a true LZ factor when considering
the whole  but may continue beyond the end of .
To find
the true end point, we treat
 as a pattern, and apply the constant
extra space pattern matching algorithm of Crochemore~\cite{c1992}, looking for
the longest prefix of  starting in . We must
modify the algorithm from~\cite{c1992} so that it matches prefixes
rather than whole occurrences of the pattern, but this is possible
without increasing its time or space
complexity.

\section{Computation of matching statistics}
\label{sec-ms}

In this section, we describe how to compute the matching statistics 
. As mentioned in Section~\ref{sec-algorithm}, what 
we really want is . However, the only difference
is that the starting point of the computation is the -interval in
 instead of the -interval.

Similarly to most algorithms for computing the matching statistics, 
we first construct some data structures on  and then scan
. During the whole LZ factorization, most of the time is spend on
the scanning and the time for constructing the data structures is
insignificant in practice. Thus we omit the construction details here.
The space requirement of the data structures is more important but not
critical as we can compensate for increased space by
reducing the block size . Using more space (per character of )
is worth doing if it increases scanning speed more than it increases
space. Consequently, we mostly use plain, uncompressed
arrays.

\paragraph{Standard  approach.}

The standard approach of computing the matching statistics using the
suffix array is to compute for each position  the longest prefix
 of the suffix  such that the
-interval in  is non-empty. Then
, where  is any suffix in the
-interval.  This can be done either with a forward scan of
, computing each -interval from -interval using
the extend right and contract left operations~\cite{ako2004}, or with
a backward scan computing each -interval from
-interval using the extend left and contract right
operations~\cite{og2011}.  We use the latter alternative but with
bigger and faster data structures.

The extend left operation is implemented by backward search. We need
the array  of size  and an implementation of the rank
function on . For the latter, we use the fast
rank data structure of~\cite{fgm2012}, which uses  bytes.

The contract right operation is implemented using the NSV and PSV
operations on  as in~\cite{og2011}, but instead of a
compressed representation, we store the NSV and PSV values as plain
arrays. As a nod towards reducing space, we store the NSV/PSV values
as offsets using 2 bytes each. If the offset is too large (which is
very rare), we obtain the value using the NSV/PSV data structure of
C{\'a}novas and Navarro~\cite{cn2010}, which needs less than  bytes.
Here the space saving was worth it as it had essentially no effect on
speed.

The peak memory use of the resulting algorithm is
 bytes.

\paragraph{New approach.}

Our second approach is similar to the first, but instead of
maintaining both end points of the -interval, we keep just one,
arbitrary position  within the interval. In principle, we perform
left extension by backward search, i.e.,
. However, checking whether
the resulting interval is empty and performing right contractions if
it is, is more involved. To compute  and  from
 and , we execute the following steps:
\begin{enumerate}
\item Let . If , set
   and . 
\item Otherwise, let  be the nearest occurrence of  in
   before the position . Compute the rank of that
  occurrence  and . If
  , set  and
  . 
\item Otherwise, let  be the nearest occurrence of  in
   after the position  and compute
  . If , set
   and .
\item Otherwise, set  and .
\end{enumerate}

The implementation of the above algorithm is based on the arrays
,  and , where .
All the above operations can be performed by scanning  and
 starting from the position  and accessing one value in
. To avoid long scans, we divide  and  into blocks of
size , and store for each block and each symbol , the
values ,  and  that would get computed if scans
starting inside the block continued beyond the block boundaries.

The peak memory use is 
bytes. This is more than in the first approach, but this is more than
compensated by increased scanning speed.

\paragraph{Skipping repetitions.}

During the preceding stages of the LZ factorization, we have 
built up knowledge of repetition present in , which can be
exploited to skip (sometimes large) parts of  during the
matching-statistics scan.
Consider an LZ factor . 
Because, by definition,  occurs earlier in  too, any
source of an LZ factor of  that is completely inside 
could be replaced with an equivalent source in that earlier
occurrence.  Thus such factors can be skipped during the computation
of  without an effect on the factorization.

More precisely, if during the scan we compute 
and find that  
for an LZ factor , we will compute 
and continue the scanning from . However, we will do this only
for long phrases with .  To compute 
from scratch, we use right extension operations implemented by a
binary search on .

To implement this ``skipping trick'' we use a bitvector of  bits to
mark LZ77 phrase boundaries adding  bytes to the peak memory.

\section{Algorithms Based on Compressed Indexes}
\label{sec-oldalgs}

We went to some effort to ensure the baseline system used to evaluate
 in our experiments was not a ``straw man''. This required
careful study and improvement of some existing approaches, which we
now describe.

\paragraph{FM-Index.} The main data structure in all the algorithms
below is an
implementation of the FM-index (FMI)~\cite{fm2005}. It consists of two
main components:
\begin{itemize}
\item \textit{ with support for the rank operation.} This enables
  backward search and the LF operation as described in
  Section~\ref{sec-preliminaries}. We have tried several rank data
  structures and found the one by
  Navarro~\cite[Sect.~7.1]{nav2004} to be the best in practice.
\item \textit{A sampling of .} This together with the LF
  operation enables arbitrary  access since
   for any .
  The sampling rate is a major space--time tradeoff parameter.
\end{itemize}
In many implementations of FMI, the construction starts with computing
the uncompressed suffix array but we cannot afford the space.
Instead, we construct  directly using the algorithm of
Okanohara and Sadakane~\cite{os2009}.
The method uses
roughly -- bytes of space
but destroys the text, which is required later during LZ
parsing. Thus, once we have , we build a rank structure over it
and use it to invert the . During the inversion process we
recover and store the text and gather the  sample values.

\paragraph{CPS2 simulation.} The CPS2 algorithm~\cite{cps2008} is an
LZ parsing algorithm based on . To compute the LZ factor
starting at , it computes the -interval for
 as long as the -interval contains a
value , indicating an occurrence of  starting at .

The key operations in CPS2 are right extension and checking
whether an  interval contains a value smaller than .  Kreft
and Navarro~\cite{kn2010} as well as Ohlebusch and Gog~\cite{og2011}
are using  for , the reverse of , 
which allows simulating right extension
on  by left extension on . The
two algorithms differ in the way they implement the interval checks:
\begin{itemize}
\item Kreft and Navarro use the RMQ operation. They use the RMQ data
  structure by Fischer and Heun~\cite{fh2007} but we use the one by
  C{\'a}novas and Navarro~\cite{cn2010}. The latter is easy and fast to
  construct during BWT inversion but queries are slow without an
  explicit . We speed up queries by replacing a general RMQ with
  the check whether the interval contains a value smaller than .
  This implementation is called .
\item Ohlebusch and Gog use NSV/PSV queries. The position  of 
  in  must be in the -interval. Thus we just need
  to check whether either  or  is in the interval
  too. They as well as we implement NSV/PSV using a balanced
  parentheses representation (BPR). This representation is initialized
  by accessing the values of  left-to-right, which makes the
  construction slow using . However, NSV/PSV queries with this data
  structure are fast, as they do not require accessing .
  This implementation is called .
\end{itemize}

\paragraph{ISA variant.} Among the most space efficient prior LZ
factorization algorithms are those of the ISA
family~\cite{kp2013} that use a sampled , a full  and a
rank/LF implementation that relies on the presence of the full
. We reduce the space further by replacing  and the rank/LF
data structure with the FM-index described above to obtain an
algorithm called .

\clearpage

\section{Experiments}
\label{sec-experiments}

We performed experiments with 
the files listed in Table~\ref{tab:files}.  All tests were conducted
on a 2.53GHz Intel Xeon Duo CPU with 32GB main memory and 8192K L2
Cache. The machine had no other significant CPU tasks running. The
operating system was Linux (Ubuntu 10.04) running kernel 3.0.0-26. The
compiler was g++ (gcc version 4.4.3) executed with the {\tt -O3
  -static -DNDEBUG} options. Times were recorded with the C {\tt
  clock} function. All algorithms operate strictly in-memory.

\paragraph{LZscan vs. other algorithms.} We compared the 
implementation using our new approach for matching statistics boosted
with the ``skipping trick'' (Section~\ref{sec-ms}) to algorithms based
on compressed indexes (Section~\ref{sec-oldalgs}). The experiments measured
the time to compute LZ factorization with varying amount of available
working space. The results are shown in Figure~\ref{fig-all}. In almost
all cases  outperforms other algorithm across the whole space
spectrum. Moreover, it can operate with very small available memory (close
to  bytes) unlike other algorithms, which all require at least  space
to compute . It achieves a superior performance for highly
repetitive data even at very low memory levels.

\paragraph{Variants of LZscan.}
The second experiment measured the improvement of our new matching
statistics computation over standard approach (see Section~\ref{sec-ms}).
Additionally, each variant was tested with and without the ``skipping trick'',
giving 4 combinations in total. The results are plotted in
Figure~\ref{fig-variants}. In nearly all cases applying any of our new
techniques improves the runtime over the standard approach, but the best
effect in all cases is achieved when the techniques are combined together.
The total speedup then varies from a factor of 2 (dna) up to 12 (einstein),
clearly depending on the repetitiveness of input.

\begin{table}[bt]
  \centering {\small
  \begin{tabular}[tab:space-basic]{l@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1em}}c@{\hspace{1em}}l@{\hspace{1em}}l}
\hline
Name &  &  &  & Source & Description \\
\hline
dna & 16 & 14.2 & 100 & S & Human genome\\
english & 215 & 14.1 & 100 & S & Gutenberg Project \\
sources & 227 & 16.8 & 100 & S & Linux and GCC sources \\
\hline
cere & 5 & 84 & 100 & R & yeast genome\\
einstein & 121 & 2947 & 100 & R & Wikipedia articles\\
kernel & 160 & 156 & 100 & R & Linux Kernel sources\\
\hline
  \end{tabular} }\vspace{1ex}
\caption[Caption for LOF]
{Data set used in the experiments. The files are 100MB prefixes of files from the
  Pizza \& Chili  standard corpus\footnotemark[2] (S)  and the Pizza \& Chili
  repetitive corpus\footnotemark[3] (R).
  The value of  (average length of an LZ77 phrase) is included as a
  measure of repetitiveness. }
  \label{tab:files}
\end{table}

\footnotetext[2]{\url{http://pizzachili.dcc.uchile.cl/texts.html}}
\footnotetext[3]{\url{http://pizzachili.dcc.uchile.cl/repcorpus.html}}

\begin{figure}
\minipage{0.5\textwidth}
  \includegraphics[trim = 0mm 25mm 0mm 0mm, width=\linewidth]{./graphs/tradeoffs/dna.pdf}
\endminipage\hfill
\minipage{0.5\textwidth}
  \includegraphics[trim = 20mm 25mm -20mm 0mm, width=\linewidth]{./graphs/tradeoffs/cere.pdf}
\endminipage
\vspace{1ex}
\newline
\minipage{0.5\textwidth}
  \includegraphics[trim = 0mm 25mm 0mm 0mm, width=\linewidth]{./graphs/tradeoffs/english.pdf}
\endminipage\hfill
\minipage{0.5\textwidth}
  \includegraphics[trim = 20mm 25mm -20mm 0mm, width=\linewidth]{./graphs/tradeoffs/einstein.pdf}
\endminipage
\vspace{1ex}
\newline
\minipage{0.5\textwidth}
  \includegraphics[trim = 0mm 25mm 0mm 0mm, width=\linewidth]{./graphs/tradeoffs/source.pdf}
\endminipage\hfill
\minipage{0.5\textwidth}
  \includegraphics[trim = 20mm 25mm -20mm 0mm, width=\linewidth]{./graphs/tradeoffs/kernel.pdf}
\endminipage
\vspace{4ex}
\caption{\label{fig-all} Time-space tradeoffs for various LZ77
  factorization algorithms. The times do not include reading from or
  writing to disk. For algorithms with multiple parameters controlling
  time/space we show only the optimal points, that is, points forming
  the lower convex hull of the points ``cloud'' corresponding to various
  settings. The vertical line is the peak memory usage of 
  construction algorithm \cite{os2009}. For comparison, we show the
  runtimes of ~\cite{kp2013}, currently
  the fastest LZ77 factorization algorithm using   bytes.}
\end{figure}


\begin{figure}
\minipage{0.5\textwidth}
  \includegraphics[trim = 0mm 25mm 0mm 0mm, width=\linewidth]{./graphs/ms/dna.pdf}
\endminipage\hfill
\minipage{0.5\textwidth}
  \includegraphics[trim = 20mm 25mm -20mm 0mm, width=\linewidth]{./graphs/ms/cere.pdf}
\endminipage
\vspace{1ex}
\newline
\minipage{0.5\textwidth}
  \includegraphics[trim = 0mm 25mm 0mm 0mm, width=\linewidth]{./graphs/ms/english.pdf}
\endminipage\hfill
\minipage{0.5\textwidth}
  \includegraphics[trim = 20mm 25mm -20mm 0mm, width=\linewidth]{./graphs/ms/einstein.pdf}
\endminipage
\vspace{4ex}
\caption{\label{fig-variants} Time-space tradeoffs for different variants of
   algorithm. The variants differ in the subprocedure
  computing matching statistics. Each of two approaches can be additionally boosted
  by enabling the ``skipping trick'' yielding 4 different combinations.
  See Section~\ref{sec-ms} for details. The times do not include reading
  from or writing to disk.}
\end{figure}

\bibliographystyle{abbrv}
\bibliography{lz}

\end{document}
