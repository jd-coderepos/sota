\section{Experiments}
\label{sec:experiment}

In this section, we conduct experiments on five real and five synthetic datasets to evaluate the performance of the proposed DnC-SC methods.
The comparison experiments against several state-of-the-art spectral clustering methods show better performance on clustering quality and efficiency for DnC-SC methods.
Besides that, the analysis of the parameters is performed.
For each experiment, the test method is repeated 20 times, and the average performance is reported.
All experiments are conducted in Matlab R2020a on a Mac Pro with 3 GHz 8-Core Intel Xeon E5 and 16 GB of RAM.

\subsection{Datasets and Evaluation Measures}

\begin{table}[!t]
  \centering
  \caption{Properties of the real and synthetic datasets.}
  \label{table:datasets}
  \begin{center}
    \begin{tabular}{p{1.2cm}<{\centering}|p{1.3cm}<{\centering}|p{1.5cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}}
      \toprule
      \multicolumn{2}{c|}{Dataset}         &\#Object     &\#Dimension      &\#Class\\
      \midrule
                                        & \emph{USPS}      & 9298       & 256 & 10 \\
      \multirow{5}{*}{\emph{Real}}      & \emph{PenDigits} & 10,992     & 16  & 10 \\
                                        & \emph{Letters}   & 20,000     & 16  & 26 \\
                                        & \emph{MNIST}     & 70,000     & 784 & 10 \\
                                        & \emph{Covertype} & 581,012    & 54  & 7  \\
      \midrule
      \multirow{5}{*}{\emph{Synthetic}} & \emph{TS-60K}    & 600,000    & 2   & 3  \\
                                        & \emph{TM-1M}     & 1,000,000  & 2   & 2  \\
                                        & \emph{TC-6M}     & 6,000,000  & 2   & 3  \\
                                        & \emph{CG-10M}    & 10,000,000 & 2   & 11 \\
                                        & \emph{FL-20M}    & 20,000,000 & 2   & 13 \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}


\begin{figure}\begin{center}
    {\subfigure[\emph{TS-60K} ($1\%$)]
      {\includegraphics[width=0.31\columnwidth]{figs/data_TM_60K.pdf}}}
    {\subfigure[\emph{TM-1M} ($1\%$)]
      {\includegraphics[width=0.31\columnwidth]{figs/data_TM_1M.pdf}}}
    {\subfigure[\emph{TC-6M} ($1\%$)]
      {\includegraphics[width=0.31\columnwidth]{figs/data_threecircle.pdf}}}
    {\subfigure[\emph{CG-10M} ($0.1\%$)]
      {\includegraphics[width=0.31\columnwidth]{figs/data_CG10M.pdf}}}
    {\subfigure[\emph{FL-20M} ($0.1\%$)]
      {\includegraphics[width=0.31\columnwidth]{figs/Flower20M.pdf}}}
    \caption{Illustration of the five synthetic datasets. Note that only a $1\%$ or $0.1\%$ samples of each dataset is plotted.}
    \label{fig:fiveSynDS}
  \end{center}
\end{figure}

Our experiments are conducted on ten large-scale datasets, varying from nine thousand to as large as twenty million data points. Specifically, the five real datasets are \emph{USPS} \cite{cai2010graph} \footnote{\label{cai_deng_data} http://www.cad.zju.edu.cn/home/dengcai/Data/MLData.html}, \emph{PenDigits} \cite{asuncion2007uci} \footnote{https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits}, \emph{Letters} \cite{frey1991letter} \footnote{https://archive.ics.uci.edu/ml/datasets/Letter+Recognition}, \emph{MNIST} \cite{cai2011speed} \footref{cai_deng_data}, and \emph{Covertype} \cite{blackard1999comparative} \footnote{https://archive.ics.uci.edu/ml/datasets/covertype}. The five synthetic datasets are \emph{Two Spiral-60K} (\emph{TS-60K}), \emph{Two
  Moons-1M} (\emph{TM-1M}), \emph{Three Circles-6M} (\emph{TC-6M}), \emph{Circles and Gaussians-10M} (\emph{CG-10M}) \cite{huang2019ultra} \footnote{\label{huang}https://www.researchgate.net/publication/330760669}, \emph{Flower-20M} (\emph{FL-20M}) \cite{huang2019ultra} \footref{huang}. Figure ~\ref{fig:fiveSynDS} shows the synthetic datasets.
The properties of the datasets are summarized in Table~\ref{table:datasets}.

We adopt two widely used evaluation metrics, i.e., Normalized Mutual Information (NMI) \cite{slonim2000agglomerative} and Accuracy (ACC) \cite{yan2009fast}, to evaluate the clustering results.
Let $X=[x_1,x_2,...,x_n]$ be the data matrix. For each data point $x_i$, denote $t_i$ and $c_i$ as the cluster label of ground truth and obtained cluster label from clustering methods, respectively. The ACC is defined as:
\begin{equation}
  \text{ACC}=\frac{\sum_{i=1}^n \delta(t_i,\text{map}(c_i))}{n},
\end{equation}
where $n$ is the number of data  and $\delta(t_i,c_i)$ is a function to check $t_i$ and $c_i$ are equal or not,  returning 1 if equals otherwise returning 0. The map$(c_i)$ is a best mapping function that maps each predicted label to the most possibly true cluster label by permuting operations \cite{xu2003document}.

Let $T$ denote a set of clusters of ground truth and $C$ obtained from clustering methods. Mutual information (MI) is defined as
\begin{equation}
  MI(T,C)=\sum_{t_i\in T,c_i\in C}p(t_i,c_i)\text{ln}\frac{p(t_i,c_i)}{p(t_i)p(c_i)},
\end{equation}
where $p(t_i)$ and $p(c_i)$ are marginal probabilities that a sample happens to belong to cluster $t_i$ or $c_i$ while $p(t_i,c_i)$ is the joint probabilities that a sample happens to belong to cluster both $t_i$ and $c_i$.
The NMI is the normalization of MI by the joint entropy as follow:
\begin{equation}
  \label{NMI}
  NMI(T,C)= \frac{\sum_{t_i\in T,c_i\in C}p(t_i,c_i)\text{ln}\frac{p(t_i,c_i)}{p(t_i)p(c_i)}}
  {-\sum_{t_i\in T,c_i\in C}p(t_i,c_i)\text{ln}(p(t_i,c_i))}
  ,\end{equation}

A better clustering result will provide a larger value of NMI/ACC. Both NMI and ACC are in the range of $[0,1]$.


\subsection{Baseline Methods and Experimental Settings}

In this experiment, we compare the proposed method with two baseline clustering methods, which are $k$-means clustering and spectral clustering (SC) \cite{chen2010parallel}, as well as six state-of-the-art large-scale spectral clustering methods. The compared spectral clustering methods are listed as follows:

\begin{enumerate}
  \item \textbf{SC} \cite{chen2010parallel}: original spectral clustering \footnote{\label{psc}http://alumni.cs.ucsb.edu/~wychen/sc.html}.
  \item \textbf{Nystr\"{o}m} \cite{fowlkes2004spectral}: Nystr\"{o}m spectral clustering \footref{psc}.
  \item \textbf{LSC-K} \cite{cai2014large}: landmark based spectral clustering using $k$-means based landmark selection \footnote{\label{LSC}http://www.cad.zju.edu.cn/home/dengcai/Data/Clustering.html}.
  \item \textbf{LSC-R} \cite{cai2014large}: landmark based spectral clustering using random landmark selection \footref{LSC}.
  \item \textbf{LSC-KH} \cite{ye2018large}: Landmark-based spectral clustering using $k$-means partition to find the hubs as the landmarks \footnote{\label{mycode}https://github.com/Li-Hongmin/MyPaperWithCode}.
  \item \textbf{LSC-RH} \cite{ye2018large}: Landmark-based spectral clustering using random partition to find the hubs as the landmarks \footref{mycode}.
  \item \textbf{U-SPEC} \cite{huang2019ultra}: Ultra-Scalable Spectral Clustering \footref{huang}.
\end{enumerate}

There are several common parameters among the methods mentioned above. We set these parameters as follow:
\begin{itemize}
  \item We set the number of landmarks or representatives as $p=1000$ for DnC-SC, U-SPEC, Nystr\"{o}m, LSC-K, and LSC-R methods. The parameter analysis on $p$ will be further conducted in Section~\ref{sec:para_p}.
  \item We set the $K=5$ for the number of nearest neighbors for DnC-SC, U-SPEC, LSC-K, and LSC-R.
        The parameter analysis on $K$ will be further conducted in Section~\ref{sec:para_K}.
  \item The DnC-SC method has a unique parameter $\alpha$. In the experiments, $\alpha = 200$ is used for the datasets whose size is less than 100,000, otherwise $\alpha=50$.
\end{itemize}


\begin{table*}[]
  \centering
  \caption{Clustering performance (ACC\% $\pm$ std) for large-scale spectral clustering methods}
  \label{table:compare_spectrals_acc}
  \resizebox{0.95\textwidth}{!}{\begin{threeparttable}
      \begin{tabular}{@{}c||c||ccccccc|c@{}}
        \toprule
        Dataset          & KM                    & SC                 & Nystr{\"{o}}m       & LSC-K                       & LSC-R               & LSC-KH              & LSC-RH              & U-SPEC                      & DnC-SC                      \\
        \midrule
        \emph{USPS}      & 67.01$_{\pm 0.70}$    & 73.21$_{\pm 3.10}$ & 69.47$_{\pm 1.38}$  & 74.02$_{\pm 7.34}$          & 73.90$_{\pm 4.42}$  & 73.66 $_{\pm 5.18}$ & 73.89 $_{\pm 4.27}$ & 80.79$_{\pm 3.13}$          & \textbf{82.55$_{\pm 1.96}$} \\
        \emph{PenDigits} & 64.40$_{\pm 4.73}$    & 67.23$_{\pm 4.35}$ & 72.46$_{\pm 0.18}$  & 82.30$_{\pm 2.95}$          & 81.55$_{\pm 3.79}$  & 82.17 $_{\pm 4.09}$ & 81.55 $_{\pm 5.12}$ & 81.74$_{\pm 4.95}$          & \textbf{82.27$_{\pm 1.33}$} \\
        \emph{Letters}   & 25.56$_{\pm 1.00}$    & 31.21$_{\pm 0.76}$ & 31.30$_{\pm 0.40}$  & 33.20$_{\pm 2.52}$          & 32.34$_{\pm 0.15}$  & 31.13 $_{\pm 0.88}$ & 31.60 $_{\pm 1.67}$ & 33.20$_{\pm 1.16}$          & \textbf{33.54$_{\pm 1.21}$} \\
        \emph{MINST}     & 56.60$_{\pm 2.71}$    & N/A                & 57.02$_{\pm 3.66}$  & \textbf{80.96$_{\pm 0.10}$} & 62.00$_{\pm 3.99}$  & 66.59 $_{\pm 5.33}$ & 67.60 $_{\pm 6.02}$ & 72.00$_{\pm 3.33}$          & 74.24$_{\pm 2.14}$          \\
        \emph{Covertype} & 24.04$_{\pm 0.22}$    & N/A                & 21.65$_{\pm 1.30}$  & \textbf{24.71$_{\pm 1.45}$} & 23.62$_{\pm 1.10}$  & N/A                 & N/A                 & 24.40$_{\pm 2.20}$          & 23.48$_{\pm 1.86}$          \\
        \emph{TS-60K}    & 56.96$_{\pm 0.00}$    & N/A                & 55.94$_{\pm 10.17}$ & 70.37$_{\pm 4.57}$          & 62.91$_{\pm 13.74}$ & N/A                 & N/A                 & 65.78$_{\pm 13.63}$         & \textbf{81.00$_{\pm 9.29}$} \\
        \emph{TM-1M}     & 75.21$_{\pm 0.00}$    & N/A                & 64.63$_{\pm 8.40}$  & 51.76$_{\pm 0.54}$          & 66.41$_{\pm 26.68}$ & N/A                 & N/A                 & \textbf{99.96$_{\pm 0.01}$} & \textbf{99.96$_{\pm 0.01}$} \\
        \emph{TC-6M}     & 33.34$_{\pm 0.00}$    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 99.86$_{\pm 0.03}$          & \textbf{99.87$_{\pm 0.02}$} \\
        \emph{CG-10M}    & 60.47$_{\pm 2.91}$    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 66.77 $_{\pm 3.97}$         & \textbf{66.83$_{\pm 4.61}$} \\
        \emph{FL-20M}    & 50.07$_{\pm 2.91}$    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 80.17 $_{\pm 3.97}$         & \textbf{81.90$_{\pm 5.61}$} \\
        \midrule
        \midrule
        Avg. score       & \multicolumn{1}{c}{-} & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 70.45                       & \textbf{72.5}9              \\
        \midrule
        \midrule
        Avg. rank        & \multicolumn{1}{c}{-} & 5.80               & 5.10                & 2.80                        & 3.90                & 4.70                & 4.5                 & 2.30                        & \textbf{1.50}               \\
        \bottomrule
      \end{tabular}\begin{tablenotes}
        \item[*] N/A denotes the case when MATLAB reports the error of out of memory.
      \end{tablenotes}
    \end{threeparttable}
  }

\end{table*}



\begin{table*}[]
  \centering
  \caption{Clustering performance (NMI\% $\pm$ std) for large-scale spectral clustering methods}
  \label{table:compare_spectrals_nmi}
  \resizebox{0.95\textwidth}{!}{\begin{tabular}{@{}c||c||ccccccc|c@{}}
      \toprule
      Dataset           & KM                    & SC                 & Nystr{\"{o}}m       & LSC-K                       & LSC-R               & LSC-KH              & LSC-RH              & U-SPEC              & DnC-SC                      \\
      \midrule
      \emph{USPS     }  & 61.28$_{\pm 0.42}$    & 77.90$_{\pm 0.55}$ & 65.07$_{\pm 1.23}$  & 81.37$_{\pm 1.92}$          & 76.22$_{\pm 0.76}$  & 76.41 $_{\pm 1.74}$ & 76.24 $_{\pm 1.00}$ & 81.86$_{\pm 1.95}$  & \textbf{82.86$_{\pm 0.21}$} \\
      \emph{PenDigits}  & 67.65$_{\pm 1.18}$    & 71.70$_{\pm 1.21}$ & 65.48$_{\pm 0.21}$  & 80.78$_{\pm 0.55}$          & 79.15$_{\pm 1.74}$  & 80.78 $_{\pm 0.55}$ & 79.15 $_{\pm 1.74}$ & 81.68$_{\pm 2.33}$  & \textbf{82.01$_{\pm 1.08}$} \\
      \emph{Letters  }  & 34.95$_{\pm 0.54}$    & 34.96$_{\pm 0.63}$ & 40.07$_{\pm 0.41}$  & 44.68$_{\pm 1.56}$          & 42.36$_{\pm 0.86}$  & 42.31 $_{\pm 0.75}$ & 42.20 $_{\pm 1.30}$ & 45.11$_{\pm 0.54}$  & \textbf{45.37$_{\pm 0.85}$} \\
      \emph{MINST    }  & 50.90$_{\pm 1.10}$    & N/A                & 49.05$_{\pm 1.55}$  & \textbf{76.81$_{\pm 0.18}$} & 62.53$_{\pm 1.87}$  & 65.08 $_{\pm 2.16}$ & 65.14 $_{\pm 2.47}$ & 69.15$_{\pm 0.76}$  & 72.00$_{\pm 0.51}$          \\
      \emph{Covertype}  & 7.55$_{\pm 0.00}$     & N/A                & 7.98$_{\pm 0.98}$   & \textbf{9.21$_{\pm 0.14}$}  & 8.06$_{\pm 0.07}$   & N/A                 & N/A                 & 8.19$_{\pm 0.04}$   & 8.30$_{\pm 0.30}$           \\
      \emph{TS-60K   }  & 22.22$_{\pm 0.00}$    & N/A                & 21.64$_{\pm 14.69}$ & 39.16$_{\pm 9.25}$          & 39.80$_{\pm 17.52}$ & N/A                 & N/A                 & 62.52$_{\pm 17.01}$ & \textbf{73.84$_{\pm 5.08}$} \\
      \emph{TM-1M    }  & 19.21$_{\pm 0.00}$    & N/A                & 8.03$_{\pm 8.58}$   & 0.10$_{\pm 0.05}$           & 28.11$_{\pm 48.63}$ & N/A                 & N/A                 & 99.52$_{\pm 0.08}$  & \textbf{99.52$_{\pm 0.05}$} \\
      \emph{TC-6M    }  & 34.95$_{\pm 0.54}$    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 99.14$_{\pm 0.19}$  & \textbf{99.15$_{\pm 0.08}$} \\
      \emph{CG-10M    } & 64.94$_{\pm 1.61}$    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 79.98$_{\pm 2.10}$  & \textbf{80.91$_{\pm 3.59}$} \\
      \emph{FL-20M}     & 65.02$_{\pm 2.91}$    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 86.77 $_{\pm 3.97}$ & \textbf{87.67$_{\pm 3.18}$} \\
      \midrule
      \midrule
      Avg. score        & \multicolumn{1}{c}{-} & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 71.39               & \textbf{72.39}              \\
      \midrule
      \midrule
      Avg. rank         & \multicolumn{1}{c}{-} & 5.40               & 5.30                & 3.10                        & 4.20                & 4.50                & 4.70                & 2.00                & \textbf{1.40}               \\
      \bottomrule
    \end{tabular}}
\end{table*}



\begin{table*}[]
  \centering
  \caption{Time costs(s) of large-scale spectral clustering methods.}
  \label{table:compare_spectrals_time}
  \resizebox{0.95\textwidth}{!}{\begin{tabular}{@{}c||c||ccccccc|c@{}}
      \toprule
      Dataset           & KM                    & SC                 & Nystr{\"{o}}m          & LSC-K                   & LSC-R                      & LSC-KH              & LSC-RH              & U-SPEC                  & DnC-SC                         \\
      \midrule
      \emph{USPS     }  & 0.37$_{\pm 0.18}$     & 3.15$_{\pm 0.18}$  & 1.44$_{\pm 0.04}$      & 1.35$_{\pm 0.09}$       & \textbf{0.64$_{\pm 0.14}$} & 0.71 $_{\pm 0.06}$  & 0.88 $_{\pm 0.07}$  & 3.36$_{\pm 0.25}$       & 1.25$_{\pm 0.07}$              \\
      \emph{PenDigits}  & 0.05$_{\pm 0.05}$     & 3.15$_{\pm 0.11}$  & 1.61$_{\pm 0.10}$      & 1.20$_{\pm 0.37}$       & 0.77$_{\pm 0.34}$          & 0.71 $_{\pm 0.05}$  & 0.68 $_{\pm 0.07}$  & 2.07$_{\pm 0.95}$       & \textbf{0.64$_{\pm 0.08}$}     \\
      \emph{Letters  }  & 0.26$_{\pm 0.05}$     & 13.67$_{\pm 2.35}$ & 4.70$_{\pm 0.17}$      & 3.89$_{\pm 0.28}$       & 2.03$_{\pm 0.34}$          & 2.26 $_{\pm 0.17}$  & 2.63 $_{\pm 0.28}$  & 1.58$_{\pm 0.06}$       & \textbf{0.90$_{\pm 0.10}$}     \\
      \emph{MINST    }  & 21.40$_{\pm 1.02}$    & N/A                & 6.54$_{\pm 0.11}$      & 17.29$_{\pm 0.82}$      & 5.80$_{\pm 0.31}$          & 18.04 $_{\pm 2.35}$ & 15.38 $_{\pm 2.43}$ & 11.96$_{\pm 0.32}$      & \textbf{5.11$_{\pm 0.51}$}     \\
      \emph{Covertype}  & 14.02$_{\pm 4.39}$    & N/A                & 571.69$_{\pm 144.60}$  & 354.74$_{\pm 90.80}$    & 41.00$_{\pm 12.38}$        & N/A                 & N/A                 & 15.96$_{\pm 1.44}$      & \textbf{13.15$_{\pm 3.00}$}    \\
      \emph{TS-60K   }  & 1.39$_{\pm 0.18}$     & N/A                & 1283.33$_{\pm 248.12}$ & 167.29$_{\pm 39.99}$    & 16.35$_{\pm 1.62}$         & N/A                 & N/A                 & 17.36$_{\pm 20.89}$     & \textbf{4.01$_{\pm 1.16}$}     \\
      \emph{TM-1M    }  & 1.12$_{\pm 0.17}$     & N/A                & 3401.61$_{\pm 410.03}$ & 3997.21$_{\pm 1436.73}$ & 591.02$_{\pm 127.86}$      & N/A                 & N/A                 & 7.85$_{\pm 0.21}$       & \textbf{6.46$_{\pm 1.13}$}     \\
      \emph{TC-6M    }  & 35.23$_{\pm 1.72}$    & N/A                & N/A                    & N/A                     & N/A                        & N/A                 & N/A                 & 30.46$_{\pm 1.52}$      & \textbf{25.05$_{\pm 3.04}$}    \\
      \emph{CG-10M    } & 134.42$_{\pm 9.28}$   & N/A                & N/A                    & N/A                     & N/A                        & N/A                 & N/A                 & 381.72$_{\pm 72.24}$    & \textbf{281.05$_{\pm 77.04}$}  \\
      \emph{FL-20M}     & 311.94$_{\pm 2.91}$   & N/A                & N/A                    & N/A                     & N/A                        & N/A                 & N/A                 & 1530.30 $_{\pm 578.44}$ & \textbf{837.38$_{\pm 213.70}$} \\
      \midrule
      \midrule
      Avg. score        & \multicolumn{1}{c}{-} & N/A                & N/A                    & N/A                     & N/A                        & N/A                 & N/A                 & 165.96                  & \textbf{117.50}                \\
      \midrule
      \midrule
      Avg. rank         & \multicolumn{1}{c}{-} & 5.80               & 4.50                   & 4.40                    & 2.60                       & 4.30                & 4.20                & 3.30                    & \textbf{1.50 }                 \\
      \bottomrule
    \end{tabular}}
\end{table*}

\subsection{Comparison with Large-scale Spectral Clustering Methods}
\label{sec:cmp_spectral}


In this section, we compare the proposed DnC-SC method with five state-of-the-art spectral clustering methods, as well as the $k$-means clustering and original spectral clustering methods as the baseline methods.

We report the experimental results in Tables ~\ref{table:compare_spectrals_acc}, ~\ref{table:compare_spectrals_nmi} and ~\ref{table:compare_spectrals_time}, where we use N/A to denote the case when MATLAB reports the error of out of memory.
Only two methods (proposed DnC-SC and U-SPEC) pass all datasets because they can approximately compute the similarity matrix within a limited memory.
The proposed DnC-SC method achieves the best clustering performance of both ACC and NMI ten times on ten benchmark datasets according to Table ~\ref{table:compare_spectrals_acc} and ~\ref{table:compare_spectrals_nmi}.
The proposed DnC-SC method achieves the best efficiency nine times on ten benchmark datasets according to Table ~\ref{table:compare_spectrals_time}.

In addition, we report the average performance score and rank for each method in Tables ~\ref{table:compare_spectrals_acc}, ~\ref{table:compare_spectrals_nmi} and ~\ref{table:compare_spectrals_time}.
The proposed DnC-SC method achieves the best average scores of both ACC and NMI.
The DnC-SC method shows average ranks of 1.50 of ACC and 1.40 of NMI, which implies the best clustering quality in all spectral clustering methods.
Moreover, the DnC-SC method costs much less average time than the other competitors and achieves a rank of 1.50, which implies the most efficient method in this experiment.
Overall, the proposed DnC-SC method shows significant effectiveness and efficiency comparing with six state-of-the-art large-scale spectral clustering methods.

\begin{table*}\centering
  \caption{Clustering performance (ACC(\%), NMI(\%), and time costs(s)) for different methods by varying number of landmark $p$.}
  \label{table:compare_para_p}
  \begin{threeparttable}
    \begin{tabular}{m{0.08\textwidth}<{\centering}|m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}}
      \toprule
      \emph{Dataset} & \emph{Letters} & \emph{MNIST} & \emph{TS-60K} & \emph{TM-1M} \\
      \midrule
      \multirow{1}{*}{ACC}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_Lettersacc.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_MINSTacc.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_TM_60Kacc.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_TM_1Macc.pdf}\\
      NMI
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_Lettersnmi.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_MINSTnmi.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_TM_60Knmi.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_TM_1Mnmi.pdf}\\
      Time cost
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_Letterstime.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_MINSTtime.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_TM_60Ktime.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_TM_1Mtime.pdf}\\
      &\multicolumn{4}{c}{\includegraphics[width=0.8\textwidth]{figs/legend_p.pdf}}\\
      \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \item[*] LSC-KH and LSC-RH cannot be conduct on the \emph{TM-60K} and \emph{TM-1M} dataset due to the memory bottleneck.
    \end{tablenotes}
  \end{threeparttable}
\end{table*}

\begin{table*}\centering
  \caption{Clustering performance (ACC(\%), NMI(\%), and time costs(s)) for different methods by varying number of nearest landmarks $K$.}
  \label{table:compare_para_Knn}
  \begin{threeparttable}
    \begin{tabular}{m{0.08\textwidth}<{\centering}|m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}}
      \toprule
      \emph{Dataset} & \emph{Letters} & \emph{MNIST} & \emph{TS-60K} & \emph{TM-1M} \\
      \midrule
      \multirow{1}{*}{ACC}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_Lettersacc.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_MINSTacc.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_TM_60Kacc.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_TM_1Macc.pdf}\\
      NMI
      &\includegraphics[width=0.18\textwidth]{figs/para_k_Lettersnmi.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_MINSTnmi.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_TM_60Knmi.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_TM_1Mnmi.pdf}\\
      Time cost
      &\includegraphics[width=0.18\textwidth]{figs/para_k_Letterstime.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_MINSTtime.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_TM_60Ktime.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_TM_1Mtime.pdf}\\
      &\multicolumn{4}{c}{\includegraphics[width=0.6\textwidth]{figs/legend_k.pdf}}\\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table*}


\begin{table*}\centering
  \caption{Clustering performance (ACC(\%), NMI(\%), and time costs(s)) for different methods by varying number of nearest landmark $K$ and selection rate $\alpha$.}
  \label{table:compare_para_K_u}
  \begin{threeparttable}
    \begin{tabular}{m{0.08\textwidth}<{\centering}|m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}}
      \toprule
      \emph{Dataset} & \emph{Letters} & \emph{MNIST} & \emph{TS-60K} & \emph{TM-1M} \\
      \midrule
      \multirow{1}{*}{ACC}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kLetters_acc.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kMINST_acc.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kTM_60K_acc.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kTM_1M_acc.pdf}\\
      NMI
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kLetters_nmi.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kMINST_nmi.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kTM_60K_nmi.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kTM_1M_nmi.pdf}\\
      Time cost
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kLetters_time.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kMINST_time.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kTM_60K_time.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kTM_1M_time.pdf}\\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table*}


\begin{table}\centering
  \caption{Clustering performance (ACC(\%), NMI(\%), and time costs(s)) for DnC-SC using divide-and-conquer based landmark selection and $k$-means based landmark selection.}
  \label{table:compare_sel_strategies}
  \begin{threeparttable}
    \begin{tabular}{m{0.75cm}<{\centering}|m{1.45cm}<{\centering}m{1.45cm}<{\centering}m{1.45cm}<{\centering}m{1.45cm}<{\centering}}
      \toprule
      \emph{Data} & \emph{Letters} & \emph{MNIST} & \emph{TS-60K} & \emph{TM-1M} \\
      \midrule
      \multirow{1}{*}{ACC}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_acc_Letters.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_acc_MINST.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_acc_TM_60K.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_acc_TM_1M.pdf}\\
      NMI
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_nmi_Letters.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_nmi_MINST.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_nmi_TM_60K.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_nmi_TM_1M.pdf}\\
      Time cost
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_time_Letters.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_time_MINST.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_time_TM_60K.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_time_TM_1M.pdf}\\
      &\multicolumn{4}{c}{\includegraphics[width=7cm]{figs/legend_com_selection.pdf}}\\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table}

\begin{table}\centering
  \caption{Clustering performance (ACC(\%), NMI(\%), and time costs(s)) for DnC-SC using approximate $K$-nearest landmarks and exact $K$-nearest landmarks.}
  \label{table:compare_approxKNN}
  \begin{threeparttable}
    \begin{tabular}{m{0.75cm}<{\centering}|m{1.45cm}<{\centering}m{1.45cm}<{\centering}m{1.45cm}<{\centering}m{1.45cm}<{\centering}}
      \toprule
      \emph{Data} & \emph{Letters} & \emph{MNIST} & \emph{TS-60K} & \emph{TM-1M} \\
      \midrule
      \multirow{1}{*}{ACC}
      &\includegraphics[width=1.7cm]{figs/com_aknn_acc_Letters.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_acc_MINST.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_acc_TM_60K.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_acc_TM_1M.pdf}\\
      NMI
      &\includegraphics[width=1.7cm]{figs/com_aknn_nmi_Letters.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_nmi_MINST.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_nmi_TM_60K.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_nmi_TM_1M.pdf}\\
      Time cost
      &\includegraphics[width=1.7cm]{figs/com_aknn_time_Letters.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_time_MINST.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_time_TM_60K.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_time_TM_1M.pdf}\\
      &\multicolumn{4}{c}{\includegraphics[width=5cm]{figs/legend_com_k.pdf}}\\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table}

We conduct a series of parameters analysis experiments to demonstrate the performance of the proposed method varying different parameter settings.
We select four dataset (\emph{Letters}, \emph{MNIST}, \emph{TS-60K}, and \emph{TM-1M}) as benchmark datasets to conduct the following experiments.

\subsubsection{Number of Landmarks $p$}
\label{sec:para_p}

We first conduct parameter analysis to compare the large-scale spectral clustering methods by varying the number of landmarks $p$ (also called landmarks) and report the experimental results in Table~\ref{table:compare_para_p}.
In general, we can see that a larger value of $p$ brings a better performance of ACC and NMI but cost more time.
The proposed DnC-SC achieves the best ACC and NMI scores on all datasets except the \emph{MNIST}.
On \emph{MNIST} dataset, the proposed DnC-SC method shows the second-best ACC and NMI scores after the LSC-K method.
In terms of time cost, the proposed DnC-SC method shows the best efficiency on all datasets.
Overall, the proposed DnC-SC method shows significant effectiveness and efficiency in this comparison.


\subsubsection{Number of Nearest Landmarks $K$}
\label{sec:para_K}

We then conduct parameter analysis to compare the large-scale spectral clustering methods by varying the number of the nearest landmark $K$ and report the experimental results in Table~\ref{table:compare_para_Knn}.
Note that the Nystr{\"{o}}m method does not have the parameter $K$.
Therefore, we do not show the results of the Nystr{\"{o}}m method in this experiment.
According to Table ~\ref{table:compare_para_Knn}, the performance of most methods varies for different $K$ values.
The proposed method shows the best ACC and NMI of performance for the three of four datasets, and the second-best ACC and NMI on the \emph{MNIST} dataset.
Overall, the proposed DnC-SC shows superior effectiveness and the best efficiency on this comparison.

\subsubsection{Number of Nearest Landmarks $K$ and selection rate $\alpha$}
\label{sec:para_K_u}

To further demonstrate the proposed method, we evaluate the performances by varying parameters $K$ and $\alpha$ and report the experimental results in Table~\ref{table:compare_para_K_u}.
For proposed DnC-SC methods, the selection rate parameter $\alpha$ directly affects the computational complexity of landmark selection, while the number of nearest landmarks $K$ affects similarity construction, respectively.
As we can see, a larger $K$ or $\alpha$ generally leads more time cost while not necessarily achieves better performance.
Overall, the proposed method shows considerable robustness with various parameters on ACC and NMI.

\subsubsection{Efficiency analysis}

To explore the efficiency of the proposed method in each computational phase, we report the time costs of three different phrases: landmark selection, similarity construction, and graph partitioning.
We choose LSC-K, LSC-R, and U-SPEC algorithms that have similar mechanisms for comparison.
We list the strategies and methods used in each method in Table \ref{table:compare_three_phases}.
The experimental results are reported in Table \ref{table:compare_intime}.

For landmark selection, the LSC-K and LSC-R methods apply $k$-means and a random selection, respectively; the U-SPEC method uses a hybrid selection that conducts $k$-means on a small set of random candidates;
DnC-SC utilizes the divide-and-conquer selection.
Looking at the runtime of the landmark selection, we see that the random selection of LSC-R takes a little time, while the $k$-means selection takes much more time.
The divide-and-conquer selection of DnC-SC is the second-fastest method just behind the random selection.

For similarity construction, the LSC-K and LSC-R compute the exact similarity matrix without approximation, while U-SPEC and DnC-SC calculate the similarity by approximate schemes.
Compared with U-SPEC, DnC-SC uses the results of landmark selection to improve the approximate scheme.
For the runtime of similarity construction, we find that DnC-SC takes significantly less time than other methods, especially for the larger-scale dataset (\emph{TM-1M}).
Note that the approximate similarity matrix of U-SPEC takes more time than LSC-K or LSC-R in \emph{MNIST} dataset.
However, the similarity of U-SPEC takes less computational complexity than LSC-K or LSC-R.
This is because U-SPEC uses serial calculations in the approximation process.
In MATLAB, it will be much faster to perform the approximation in a batch processing manner (with optimized matrix computation) than in a serial processing manner.

For graph partitioning, LSC-K and LSC-R utilize SVD based method, while U-SPEC and DnC-SC apply transfer cuts.
Theoretically, both two graph partitioning methods can be considered as efficient solutions for bipartite graph partitioning \cite{li2012segmentation,cai2014large}.
But the transfer cuts take less computational complexity.
In Table \ref{table:compare_intime}, we can see that U-SPEC and DnC-SC take less time than LSC-K and LSC-R, which is consistent with the theoretical complexity.

Overall, DnC-SC shows the best efficiency in four methods, which is mainly due to the proposed landmark selection and approximate similarity construction.

\begin{table*}[]
  \centering
  \caption{Comparison for three phases for different methods.}
  \label{table:compare_three_phases}
  \begin{tabular}{@{}l|llll@{}}
    \toprule
    Phase                   & LSC-K     & LSC-R     & U-SPEC                          & DnC-SC                       \\ \midrule
    Landmark Selection      & $k$-means & Random    & Hybrid representative selection & Divide-and-conquer selection \\
    Similarity Construction & Exact     & Exact     & Approximate                     & Approximate                  \\
    Graph Partitioning      & SVD based & SVD based & Transfer cuts                   & Transfer cuts                \\ \bottomrule
  \end{tabular}
\end{table*}

\begin{table}\centering
  \caption{Comparison of time costs in each phase for different methods.}
  \label{table:compare_intime}
  \begin{threeparttable}
    \begin{tabular}{m{0.8cm}<{\centering}|m{6cm}<{\centering}}
      \toprule
      \emph{Data} & Time costs                                                   \\

      \midrule
      \multirow{1}{*}{\emph{Letters}}
                  & \includegraphics[width=6cm]{figs/com_intime_acc_Letters.pdf} \\
      \emph{MNIST}
                  & \includegraphics[width=6cm]{figs/com_intime_acc_MINST.pdf}   \\
      \emph{TS-60K}
                  & \includegraphics[width=6cm]{figs/com_intime_acc_TM_60K.pdf}  \\
      \emph{TM-1M}
                  & \includegraphics[width=6cm]{figs/com_intime_acc_TM_1M.pdf}   \\
                  & {\includegraphics[width=6cm]{figs/legned_intime.pdf}}        \\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table}


\subsection{Influence of Landmark Selection Strategies}
\label{sec:cmpSelStrat}

Some existing works have shown that the performance of large-scale spectral clustering heavily relies on the proper strategy of landmark selection \cite{li2020hubness}.
In our proposed landmark selection, we propose a divide-and-conquer selection strategy and light-$k$-means to find a good balance between effectiveness and efficiency.
We test the purposed method with different landmark selection methods, i.e., $k$-means based landmark selection, divide-and-conquer selection without light-$k$-means, and divide-and-conquer selection with light-$k$-means.

In this section, we compare the performances between the divide-and-conquer based landmark selection and the $k$-means base landmark selection.
The experimental results are reported in Table~\ref{table:compare_sel_strategies}.
As we mentioned, the divide-and-conquer based landmark selection algorithm recursively solves the optimization problems \ref{eq: opt}, which $k$-means methods can also solve.
We have pointed out the lack of efficiency of directly applying $k$-means on large-scale datasets in Section \ref{sec:dnc-sc_complexity}.
Note that the number of maximum iterations of $k$-means in landmark selection is turned as 5, which is the same setting as LSC-K and U-SPEC implementation.
In Table~\ref{table:compare_sel_strategies}, $k$-means based landmark selection algorithm generally shows better ACC and NMI on most datasets except \emph{TM-1M} dataset, while the difference in performance is not significant.
Compared to $k$-means based selection, our divide-and-conquer based landmark selection algorithm strikes a balance between efficiency and effectiveness.
It achieves significantly better efficiency than the $k$-means based selection and yields competitive clustering quality compared to the $k$-means based selection.

\subsubsection{Performance comparison on simulation scenarios}
To further investigate the performance of divide-and-conquer selection, we conduct a simulation experiment to simulate different scenarios for landmark selection.
For landmark selection, the number of landmarks is considered much larger than the desired number of clusters. 
If we view the landmark selection as a clustering task, then the landmark selection will be considered as a special clustering case with a large number of clusters.
Therefore, we generate four synthetic datasets with 500, 1000, 1500, 2000 clusters, respectively. 
The synthetic datasets are 2-dimensional isotropic Gaussian blobs, which are shown in Figure~\ref{fig:simulation_landmarks}.
We treat divide-and-conquer selection as a clustering algorithm to compare the clustering performance with $k$-means.
We report the clustering performance of NMI and time costs for all simulation scenarios in Table~\ref{tab:simulation_landmark_NMI} and Table~\ref{tab:simulation_landmark_time}. 

Though divide-and-conquer selection shows slightly lower NMI than $k$-means, its time cost is much less.
As landmark increases, the performance degradation associated with divide-and-conquer selection becomes progressively insignificant, while the improvement of efficiency becomes more significant.
The experimental results imply that the divide-and-conquer selection is suitable for a larger number of landmarks while $k$-means selection is suitable for a smaller number of landmarks.
Usually, more landmarks will lead to a better clustering result for large-scale spectral clustering \cite{cai2014large,huang2019ultra}.
Thus, the divide-and-conquer selection is more suitable than $k$-means selection for large-scale spectral clustering.

\begin{figure}\begin{center}
    {\subfigure[\emph{500 Gaussian blobs} ]
      {\includegraphics[width=0.45\columnwidth]{figs/100K-500.pdf}}}
    {\subfigure[\emph{1000 Gaussian blobs} ]
      {\includegraphics[width=0.45\columnwidth]{figs/100K-1000.pdf}}}
    {\subfigure[\emph{1500 Gaussian blobs} ]
      {\includegraphics[width=0.45\columnwidth]{figs/100K-1500.pdf}}}
    {\subfigure[\emph{2000 Gaussian blobs}]
      {\includegraphics[width=0.45\columnwidth]{figs/100K-2K.pdf}}}
    \caption{Illustration of four datasets with 500, 1000, 1500 and 2000 isotropic Gaussian blobs. The number of samples is 100,000 for each dataset.}
    \label{fig:simulation_landmarks}
  \end{center}
\end{figure}

\begin{table}[]
    \centering
    \caption{The simulation performance of NMI(\%) varying different landmark selection scenarios.}
    \label{tab:simulation_landmark_NMI}
    \begin{tabular}{@{}lll@{}}
    \toprule
    Datasets & divide-and-conquer                 & $k$-means            \\ \midrule
    500 Gaussian blobs             & 90.76 & 92.01 \\
    1000 Gaussian blobs            & 87.67 & 88.51 \\
    1500 Gaussian blobs            & 85.90 & 86.40 \\
    2000 Gaussian blobs            & 84.59  & 84.82  \\ \bottomrule
    \end{tabular}
    \end{table}
    
    
    \begin{table}[]
    \centering
    \caption{The simulation time costs(s) varying different landmark selection scenarios.}
    \label{tab:simulation_landmark_time}
    \begin{tabular}{@{}lll@{}}
    \toprule
    Datasets & divide-and-conquer               & $k$-means            \\ \midrule
    500 Gaussian blobs             & 0.33 & 5.95  \\
    1000 Gaussian blobs            & 0.44 & 11.43 \\
    1500 Gaussian blobs            & 0.49 & 17.02 \\
    2000 Gaussian blobs            & 0.64 & 24.43 \\ \bottomrule
    \end{tabular}
    \end{table}







\subsection{Influence of Approximated $K$-nearest Landmarks}
\label{sec:cmpApproxKNN}

In this section, we compare the approximated $K$-nearest landmarks and exact $K$-nearest landmarks.
The experimental results are reported in Table \ref{table:compare_approxKNN}.
The approximated $K$-nearest landmarks approach first finds the possible candidates according to the center's nature of landmarks and then searches the $K$-nearest landmarks among them.
The exact $K$-nearest landmarks approach costs $O(Npd)$ computational time, while the proposed approximation can reduce the time cost to $O(NKd)$.
As the Tables~\ref{table:compare_approxKNN} shows, the exact $K$-nearest landmarks approach achieves slightly better ACC and NMI scores than the proposed approximation.
However, the performances of the two methods are not significantly different.
In terms of time cost, the proposed approximation approach shows highly efficient performance compared with the exact $K$-nearest landmarks.
Note that the exact $K$-nearest landmarks approach can not be conducted on datasets whose sizes are more than one million due to the high computational cost.
Overall, the proposed approximate $K$-nearest landmark approach shows the robustness and efficiency of this experiment.

\begin{table*}[]
  \centering
  \caption{Ablation Study on the proposed divide-and-conquer selection strategy, light-$k$-means, and approximate of K-nearest landmarks.}
  \label{tab:ablation}
  \begin{tabular}{@{}llllllll@{}}
    \toprule

    \multirow{3}{*}{Datasets} & \multirow{3}{*}{Landmark selection} & \multicolumn{6}{c}{$K$-nearest landmarks}                                  \\ \cmidrule(l){3-8}
    &                                     & \multicolumn{3}{l|}{Approximate}                 & \multicolumn{3}{l}{Exact}   \\ \cmidrule(l){3-8}
                                &                     & ACC(\%)        & NMI(\%)          & \multicolumn{1}{l|}{Time(s)}         & ACC(\%)          & NMI(\%)          & Time(s)   \\ \midrule
    \multirow{3}{*}{Letters}    & $k$-means           & {34.06}        & {\textbf{46.58}} & \multicolumn{1}{l|}{{3.89}}          & 34.41            & 45.56            & 4.05      \\
                                & DnC-$k$-means       & \textbf{34.71} & 45.19            & \multicolumn{1}{l|}{1.22}            & 33.76            & 45.17            & 1.34      \\
                                & DnC-light-$k$-means & {33.54}        & {45.37}          & \multicolumn{1}{l|}{{\textbf{0.90}}} & {33.93}          & {45.91}          & {1.05}    \\
    \midrule
    \multirow{3}{*}{MNIST}      & $k$-means           & {75.34}        & {73.07}          & \multicolumn{1}{l|}{{15.29}}         & \textbf{79.28}   & \textbf{74.74}   & 29.02     \\
                                & DnC-$k$-means       & 74.46          & 73.11            & \multicolumn{1}{l|}{9.50}            & 74.12            & 74.12            & 24.81     \\
                                & DnC-light-$k$-means & {74.24}        & {72.00}          & \multicolumn{1}{l|}{{\textbf{5.11}}} & {74.04}          & {74.04}          & {21.00}   \\
    \midrule
    \multirow{3}{*}{TS-60K}     & $k$-means           & {83.27}        & {\textbf{77.18}} & \multicolumn{1}{l|}{{165.21}}        & \textbf{86.41}   & 76.51            & 172.72    \\
                                & DnC-$k$-means       & 81.06          & 73.92            & \multicolumn{1}{l|}{8.14}            & 84.30            & 70.15            & 12.75     \\
                                & DnC-light-$k$-means & {81.00}        & {73.84}          & \multicolumn{1}{l|}{{\textbf{4.01}}} & {80.82}          & {73.12}          & {9.12}    \\
    \midrule
    \multirow{3}{*}{TM-1M}      & $k$-means           & {99.23}        & {99.50}          & \multicolumn{1}{l|}{{3997.12}}       & 99.95            & 99.59            & 4023.12   \\
                                & DnC-$k$-means       & 99.95          & 99.48            & \multicolumn{1}{l|}{12.78}           & \textbf{99.97}   & \textbf{99.57}   & 25.65     \\
                                & DnC-light-$k$-means & {99.96}        & {99.52}          & \multicolumn{1}{l|}{{\textbf{6.46}}} & {99.95}          & {99.45}          & {19.30}   \\
    \midrule
    \midrule
    \multirow{3}{*}{Avg. score} & $k$-means           & {72.98}        & {{74.08}}        & \multicolumn{1}{l|}{{1045.38}}       & {\textbf{75.01}} & {\textbf{74.10}} & {1057.23} \\
                                & DnC-$k$-means       & {72.55}        & {72.93}          & \multicolumn{1}{l|}{{7.91}}          & {73.04}          & {72.25}          & {16.14}   \\
                                & DnC-light-$k$-means & {72.19}        & {72.68}          & \multicolumn{1}{l|}{{\textbf{4.12}}} & {72.19}          & {73.13}          & {{12.62}} \\
    \bottomrule
  \end{tabular}\end{table*}

\subsection{Ablation Study}

To strike a good balance between efficiency and effectiveness, the proposed method applies three strategies: (a) divide-and-conquer selection, (b) light-$k$-means, and (c) approximate of $K$-nearest landmarks.
An ablation study about the influence of the combination of each part is conducted to show the contribution of each strategy.
The experimental results are reported in Table \ref{tab:ablation}.
Modules (a) and (b) are used in landmark selection.
In Table \ref{tab:ablation}, DnC-$k$-means indicates a modified divide-and-conquer selection that utilizes $k$-means algorithm for the dividing process, and DnC-light-$k$-mean indicates the original divide-and-conquer selection that utilizes the light-$k$-means algorithm for the dividing process.
To show the effects of (a) and (b), we choose $k$-means selection as the baseline.
For $K$-nearest landmarks, we provide the exact $K$-nearest landmark option for each landmark selection method.
There are three landmark selections and two $K$-nearest landmark methods provided in this ablation study.
Thus we have six combinations for comparison. 

Table \ref{tab:ablation} shows the performance on six combinations according to different landmark selection and $K$-nearest landmarks methods.
The bold texts represent the best ACC, NMI, and Time for each dataset.
We first compare the different landmark selection methods:
the $k$-means selection archives the best ACC on two datasets and the best NMI on three datasets, but takes much more runtime on all datasets;
DnC-$k$-means selection archives the best ACC on two datasets and best NMI on one dataset with much less runtime than $k$-means selection;
our DnC-light-$k$-means takes the least time on all datasets and shows a competitive performance of ACC and NMI.
For $K$-nearest landmark, the extra approach archives the best ACC on three datasets and the best NMI on two datasets, which slightly outperforms the approximate approach.
We also report the average score for each combination.
The combination of $k$-means and extra $K$-nearest landmarks show the best average scores of ACC and NMI, but the most time-consuming.
Our proposed method that is the combination of DnC-light-$k$-means and approximate $K$-nearest landmarks shows the fastest speed and competitive performance of ACC and NMI.

Overall, the proposed method significantly improves the efficiency of large-scale spectral clustering while keeping the clustering quality acceptable.
In detail, we can see that the use of strategy (a) provides the most important contribution to the computational efficiency, while modules (b) and (c) further reduce the computational cost.