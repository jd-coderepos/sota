\section{Experiments}
\label{sec:experiments}



\begin{table}[!t]
  \centering
  \renewcommand{\arraystretch}{0.75}
  \caption{Statistics of the proposed ZSFooD and existing datasets that are widely used for ZSD.}
  	\setlength{\tabcolsep}{1.1mm}{
    \begin{tabular}{lccccccc}
    \toprule
    \multirow{2}[4]{*}{Dataset} & \multicolumn{3}{c}{Classes} & \multicolumn{3}{c}{Images} \\
\cmidrule{2-4}   \cmidrule{5-7}          & S     & U     & Total & Training & Test  & Total \\
    \midrule
    PASCAL VOC \cite{everingham2010pascal}   & 16    & 4     & 20    & 10,728 & 10,834 & 21,562 \\
    MS COCO \cite{coco} & 65    & 15    & 80    & 82,783 & 40,504 & 123,287 \\
UECFOOD-256 \cite{kawano14c}  & 205    & 51     & 256    & 20,452 & 5,732 & 26,184 \\
    ZSFooD  & 184 & 44 & 228 & 10,463 & 10,140 & 20,603 \\
    \bottomrule
    \end{tabular}}
  \label{tab:dataset_comparison}\end{table}


\begin{table}[!t]
  \centering
  \renewcommand{\arraystretch}{0.75}
  \caption{Comparison with baseline methods on ZSFooD (\%).}
    \setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{clcccc}
    \toprule
    \multirow{2}[4]{*}{Metric} & \multirow{2}[4]{*}{Method} & \multirow{2}[4]{*}{ZSD} & \multicolumn{3}{c}{GZSD} \\
\cmidrule{4-6}          &       &       & Seen  & Unseen & HM \\
    \midrule
   \multirow{6}[2]{*}{Recall@100} & ConSE~\cite{norouzi2014zero} & 39.7 & 58.0 & 38.1 & 46.4 \\
& BLC~\cite{zheng2020background}  & 41.2  & 55.3   & 40.5  & 46.8 \\
          & CZSD~\cite{yan2022semantics}  &   48.0    & 86.1  &  44.8 &  58.9 \\
   & SU~\cite{hayat2020synthesizing}    & 45.3  & 82.3  & 44.1  & 57.4 \\
          & RRFS~\cite{huang2022robust}  & 48.8  & 86.6  & 47.6  & 61.4 \\
          & \textbf{SeeDS}  & \textbf{52.9}  & \textbf{87.0}  & \textbf{49.8}  & \textbf{63.3} \\
    \midrule
   \multirow{6}[2]{*}{mAP} & ConSE~\cite{norouzi2014zero} & 0.8 & 54.3 & 0.7 & 1.4 \\
& BLC~\cite{zheng2020background}  & 1.1   & 51.1   & 0.9   & 1.8  \\
      & CZSD~\cite{yan2022semantics}  &  4.0   &    81.2   &   2.1  & 4.1 \\
       & SU~\cite{hayat2020synthesizing}    & 3.9   & 79.1  & 2.3   & 4.5  \\
          & RRFS~\cite{huang2022robust}  & 4.3   & 82.7  & 2.7   & 5.2  \\
          & \textbf{SeeDS}  & \textbf{5.9}   & \textbf{82.8}  & \textbf{3.5} & \textbf{6.7}  \\
    \bottomrule
    \end{tabular}}
  \label{tab:ZSFooD}\end{table}

\subsection{Experimental Settings}

\noindent\textbf{Dataset Splittings.}
We adopt two datasets to evaluate the ZSFD performance: our constructed ZSFooD and widely-used UECFOOD-256 \cite{kawano14c}. UECFOOD-256 is a food detection dataset reformable into a ZSFD benchmark but mainly contains images with a single food object, and thus only provides 28,429 bounding box annotations. Compared with it, ZSFooD has 20,603 food images collected in 10 restaurant scenarios, each with multiple food objects annotated with bounding boxes. ZSFooD is more challenging with 95,322 bounding boxes and 291 classes. 
Following the setting in \cite{bansal2018zero, rahman2018polarity}, categories in ZSFooD and UECFOOD-256 are split into 184 seen classes and 44 unseen classes, and 205 seen classes and 51 unseen classes, respectively. We also compare our method with ZSD baselines on PASCAL VOC 2007+2012~\cite{everingham2010pascal} and MS COCO 2014 \cite{coco} using the given splitting \cite{hayat2020synthesizing, huang2022robust}. Note that two different splits are adopted for MS COCO: 48/17 seen/unseen split and 65/15 seen/unseen split. We replace the ingredient corpus with the corpus of texture and color words and replace the cuisine corpus with the corpus of shape and edge words when implementing SeeDS for general ZSD.

\begin{table}[!t]
  \centering
  \renewcommand{\arraystretch}{0.75}
  \caption{Comparison with baselines on UECFOOD-256 (\%).}
    \setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{clcccc}
    \toprule
    \multirow{2}[4]{*}{Metric} & \multirow{2}[4]{*}{Method} & \multirow{2}[4]{*}{ZSD} & \multicolumn{3}{c}{GZSD} \\
\cmidrule{4-6}          &       &       & Seen  & Unseen & HM \\
    \midrule
   \multirow{4}[2]{*}{Recall@100} 
          & CZSD~\cite{yan2022semantics}  &  60.7   & \textbf{57.6}  &  45.5 & 50.8 \\
   & SU~\cite{hayat2020synthesizing}    & 61.9 & 52.5  & 52.8  & 52.6 \\
          & RRFS~\cite{huang2022robust}  &  64.8 & 54.9  & 55.1  &  55.0 \\
          & \textbf{SeeDS}  & \textbf{74.0}  & 55.2  & \textbf{61.4}  & \textbf{58.1} \\
    \midrule
   \multirow{4}[2]{*}{mAP} 
      & CZSD~\cite{yan2022semantics}  &  22.0  &   \textbf{20.8}   &   16.2  & 18.2 \\
       & SU~\cite{hayat2020synthesizing}    &  22.4   &  19.3 & 20.1  & 19.7 \\
          & RRFS~\cite{huang2022robust}  &  23.6  & 20.1  &  22.9  &  21.4 \\
          & \textbf{SeeDS}  & \textbf{27.1}   & 20.2 & \textbf{26.0} & \textbf{22.7}  \\
    \bottomrule
    \end{tabular}}
  \label{tab:UEC256}\end{table}

\noindent\textbf{Evaluation Metrics.}
Similar to previous works \cite{bansal2018zero, huang2022robust}, we use mean Average Precision (mAP) and Recall@100 with IoU threshold 0.5 for the evaluation on ZSFooD, UECFOOD-256 and PASCAL VOC. For MS COCO, we report mAP and Recall@100 with IoU thresholds of 0.4, 0.5, and 0.6. We also report the performance of methods under the setting of GZSD. The Harmonic Mean (HM) of seen and unseen is the key metric used for GZSD performance. 

\noindent\textbf{Implementation Details.}
We adopt the Faster-RCNN \cite{fasterrcnn} with the ResNet-101 \cite{resnet} as the backbone for fair comparisons. For training the synthesizer, Adam \cite{adam2014} is used with a learning rate of 1e-4 with a weight decay of 1e-5 for all experiments. To align the experimental settings with baselines \cite{hayat2020synthesizing,huang2022robust}, we synthesize 500/500/500/250 features for each unseen class of ZSFooD/UECFOOD-256/PASCAL VOC/MS COCO to train the classifier. We set $T = 100$ for the noise sampling process of RFDDM. The linear start and the linear end for noise scalars are set to $\beta_1$ = 8.5e-4 and $\beta_T$ = 1.2e-2, respectively. We empirically set $\lambda_1 = 1$, $\lambda_2 = 1$ and $\lambda_3 = 0.1$ without meticulous tuning for ensuring stable training. Word embedding vectors of class names and corpora are extracted by CLIP \cite{radford2021learning} for ZSFooD and UECFOOD-256, and extracted by FastText \cite{mikolov2018advances} for PASCAL VOC and MS COCO.


\begin{table}[!t]
  \centering
  \renewcommand{\arraystretch}{0.75}
  \caption{Comparison of mAP on PASCAL VOC (\%).}
    \setlength{\tabcolsep}{3.8mm}{
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}[4]{*}{Model} & \multirow{2}[4]{*}{ZSD} & \multicolumn{3}{c}{GZSD} \\
\cmidrule{3-5}          &       & Seen  & Unseen & HM \\
    \midrule
    SAN~\cite{rahman2019zero}   & 59.1  & 48.0    & 37.0    & 41.8 \\
    HRE~\cite{berkan2018zero}   & 54.2  & \textbf{62.4}  & 25.5  & 36.2 \\
    PL~\cite{rahman2018polarity}    & 62.1  & -     & -     & - \\
    BLC~\cite{zheng2020background}   & 55.2  & 58.2  & 22.9  & 32.9 \\
    SU~\cite{hayat2020synthesizing}    & 64.9  & -     & -     & - \\
    RRFS~\cite{huang2022robust}  & 65.5  & 47.1  & 49.1  & 48.1 \\
    \textbf{SeeDS}  & \textbf{68.9} & 48.5  & \textbf{50.6} & \textbf{49.5} \\
    \bottomrule
    \end{tabular}}
  \label{tab:voc}\end{table}

\begin{table}[t]
  \centering
  \renewcommand{\arraystretch}{0.75}
  \caption{Comparison of Class-wise AP and mAP for different methods on unseen classes of PASCAL VOC (\%).}
    \setlength{\tabcolsep}{2.5mm}{
    \begin{tabular}{lccccc}
    \toprule
    Method & car   & dog   & sofa  & train & mAP \\
    \midrule
    SAN~\cite{rahman2019zero} & 56.2  & 85.3  & 62.6  & 26.4  & 57.6  \\
    HRE~\cite{berkan2018zero} & 55.0  & 82.0  & 55.0  & 26.0  & 54.5  \\
    PL~\cite{rahman2018polarity} & \textbf{63.7}  & 87.2  & 53.2  & 44.1  & 62.1  \\
    BLC~\cite{zheng2020background} & 43.7  & 86.0  & 60.8  & 30.1  & 55.2  \\
    SU~\cite{hayat2020synthesizing} & 59.6  & 92.7  & 62.3  & 45.2  & 64.9  \\
    RRFS~\cite{huang2022robust} & 60.1  & 93.0  & 59.7 & 49.1  & 65.5  \\
    \textbf{SeeDS}  & 60.4 & \textbf{95.3} & \textbf{65.9}  & \textbf{53.8} & \textbf{68.9} \\
    \bottomrule
    \end{tabular}}
  \label{tab:vocclass}\end{table}

\begin{table}[t]
  \centering
  \renewcommand{\arraystretch}{0.75}
  \caption{ZSD performance comparison on MS COCO (\%).}
    \setlength{\tabcolsep}{0.9mm}{
    \begin{tabular}{lcrrrr}
    \toprule
    \multirow{2}[4]{*}{Model} & \multirow{2}[4]{*}{Split} & \multicolumn{3}{c}{Recall@100} & \multicolumn{1}{c}{mAP} \\
\cmidrule{3-6}          &       & \multicolumn{1}{c}{IoU=0.4} & \multicolumn{1}{c}{IoU=0.5} & \multicolumn{1}{c}{IoU=0.6} & \multicolumn{1}{c}{IoU=0.5} \\
    \midrule
    SB~\cite{bansal2018zero}    & 48/17 & \multicolumn{1}{c}{34.5} & \multicolumn{1}{c}{22.1} & \multicolumn{1}{c}{11.3} & \multicolumn{1}{c}{0.3} \\
    DSES~\cite{bansal2018zero}  & 48/17 & \multicolumn{1}{c}{40.2} & \multicolumn{1}{c}{27.2} & \multicolumn{1}{c}{13.6} & \multicolumn{1}{c}{0.5} \\
    PL~\cite{rahman2018polarity}    & 48/17 & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{43.5} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{10.1} \\
    BLC~\cite{zheng2020background}   & 48/17 & \multicolumn{1}{c}{51.3} & \multicolumn{1}{c}{48.8} & \multicolumn{1}{c}{45.0} & \multicolumn{1}{c}{10.6} \\
    RRFS~\cite{huang2022robust}  & 48/17 & \multicolumn{1}{c}{58.1} & \multicolumn{1}{c}{53.5} & \multicolumn{1}{c}{47.9} & \multicolumn{1}{c}{13.4} \\
    \textbf{SeeDS}  & 48/17 &    \multicolumn{1}{c}{\textbf{59.2}}   & \multicolumn{1}{c}{\textbf{55.3}} &   \multicolumn{1}{c}{\textbf{48.5}}    & \multicolumn{1}{c}{\textbf{14.0}} \\
    \midrule
    PL~\cite{rahman2018polarity}    & 65/15 & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{37.7} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{12.4} \\
    BLC~\cite{zheng2020background}   & 65/15 & \multicolumn{1}{c}{57.2} & \multicolumn{1}{c}{54.7} & \multicolumn{1}{c}{51.2} & \multicolumn{1}{c}{14.7} \\
    SU~\cite{hayat2020synthesizing}    & 65/15 & \multicolumn{1}{c}{54.4} & \multicolumn{1}{c}{54.0} & \multicolumn{1}{c}{47.0} & \multicolumn{1}{c}{19.0} \\
    RRFS~\cite{huang2022robust}  & 65/15 & \multicolumn{1}{c}{65.3} & \multicolumn{1}{c}{62.3} & \multicolumn{1}{c}{55.9} & \multicolumn{1}{c}{19.8} \\
    \textbf{SeeDS}  & 65/15 &    \multicolumn{1}{c}{\textbf{66.5}}   &   \multicolumn{1}{c}{\textbf{64.0}}    &    \multicolumn{1}{c}{\textbf{56.8}}   & \multicolumn{1}{c}{\textbf{20.6}} \\
    \bottomrule
    \end{tabular}}
  \label{tab:cocozsd}\end{table}

\begin{table}[!t]
  \centering
  \renewcommand{\arraystretch}{0.75}
  \caption{GZSD performance comparison on MS COCO (\%).}
    \setlength{\tabcolsep}{1.2mm}{
    \begin{tabular}{lccccccc}
    \toprule
    \multirow{2}[4]{*}{Model} & \multirow{2}[4]{*}{Split} & \multicolumn{3}{c}{Recall@100} & \multicolumn{3}{c}{mAP} \\
\cmidrule{3-8}          &       & S     & U     & HM    & S     & U     & HM \\
    \midrule
    PL~\cite{rahman2018polarity}    & 48/17 & 38.2  & 26.3  & 31.2  & 35.9  & 4.1   & 7.4  \\
    BLC~\cite{zheng2020background}   & 48/17 & 57.6  & 46.4  & 51.4  & 42.1  & 4.5   & 8.1 \\
    RRFS~\cite{huang2022robust}  & 48/17 & 59.7  & 58.8  & 59.2  & 42.3  & 13.4  & 20.4  \\
    \textbf{SeeDS}  & 48/17 &  \textbf{60.1}     &   \textbf{60.8}   &    \textbf{60.5}   &    \textbf{42.5}  &     \textbf{14.5}  & \textbf{21.6} \\
    \midrule
    PL~\cite{rahman2018polarity}    & 65/15 & 36.4  & 37.2  & 36.8  & 34.1  & 12.4  & 18.2  \\
    BLC~\cite{zheng2020background}   & 65/15 & 56.4  & 51.7  & 53.9  & 36.0  & 13.1  & 19.2  \\
    SU~\cite{hayat2020synthesizing}    & 65/15 & 57.7  & 53.9  & 55.7  & 36.9  & 19.0  & 25.1  \\
    RRFS~\cite{huang2022robust}  & 65/15 & 58.6  & 61.8  & 60.2  & 37.4  & 19.8  & 26.0  \\
    \textbf{SeeDS}  & 65/15 &   \textbf{59.3}   &  \textbf{62.8}  &  \textbf{61.0}     &  \textbf{37.5}   &  \textbf{20.9}  &  \textbf{26.8} \\
    \bottomrule
    \end{tabular}}
  \label{tab:cocogzsd}\end{table}

\subsection{Experiments on ZSFD datasets}
\label{sec:main_results}

\noindent\textbf{Evaluation on ZSFooD.}
We reimplement baseline methods and show ZSFD results on ZSFooD in Table~\ref{tab:ZSFooD}. Compared with the second-best method RRFS, ``ZSD'', ``Unseen'' and ``HM'' are improved by 1.6\%, 0.8\%, and 1.5\% mAP, respectively. For Recall@100, ``ZSD'', ``Unseen'' and ``HM'' are improved by 4.1\%, 2.2\% and 1.9\% mAP, respectively. The improvements demonstrate the effectiveness of the proposed SeeDS in detecting unseen fine-grained food objects. Our proposed SeeDS enhances feature synthesizer by utilizing the multi-source semantic knowledge from S$^3$M, and help detectors achieve better ZSFD performance when trained with diverse features generated by RFDDM. We also observe that the mAP performance of ``Unseen'' for all ZSD methods is much lower than mAP of ``Seen'', which denotes that the larger number of classes in ZSFooD makes ZSFD on unseen objects extremely challenging. Note that the ``Seen'' performance in the setting of GZSD has not been improved since classifier parameters for seen classes are mainly influenced by the backbone detector trained on seen objects, while ZSFD specifically focuses on improving detection on unseen classes.

\noindent\textbf{Evaluation on UECFOOD-256.} 
Experimental results on UECFOOD-256 are shown in Table~\ref{tab:UEC256}. Compared with RRFS, our SeeDS improves mAP by 3.5\%, 3.1\%, and 1.3\%, and Recall@100 by 9.2\%, 6.3\%, and 3.1\% for ``ZSD'', ``Unseen'' and ``HM'', respectively. For Recall@100, ``ZSD'', ``Unseen'' and ``HM'' are improved by 9.2\%, 6.3\%, and 3.1\%, respectively. These results underline the effectiveness of our SeeDS framework, especially in complex GZSD scenarios with simultaneous seen and unseen food objects. We also observe that the mAP for all baseline methods can not reach a similarly high number as in ZSD, indicating ZSFD as a challenging task with potential for further method development.

\subsection{Experiments on general ZSD datasets}

\begin{table}[t]
  \centering
  \renewcommand{\arraystretch}{0.75}
  \caption{Ablation studies measured by mAP (\%).}
    \setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{cccccccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[4]{*}{Dataset}} & \multicolumn{2}{c}{Methods} & \multirow{2}[4]{*}{ZSD} & \multicolumn{3}{c}{GZSD} \\
\cmidrule{2-3}\cmidrule{5-7}          &  S$^3$M & RFDDM   &       & S     & U     & HM \\
    \midrule
    \multirow{3.5}[4]{*}{ZSFooD}  &   &    &  4.3   & 82.7  & 2.7   & 5.2   \\
      &    \checkmark  &  &   5.0   & 82.8   &  3.3 &  6.3 \\
       &   &  \checkmark  &  5.7  &  82.8 &  3.1 &  6.0 \\
       & \checkmark     & \checkmark     &  \textbf{5.9}   & \textbf{82.8}  & \textbf{3.5} & \textbf{6.7}  \\
    \midrule
    \multirow{3.5}[4]{*}{PASCAL VOC}      &       &        & 65.5  & 47.1  & 49.1  & 48.1 \\
       &   \checkmark     &  &    68.0   & 48.5 & 49.8 & 49.1 \\
        &     &    \checkmark    & 68.2 & 48.4 &  49.6 & 49.0 \\
      & \checkmark     & \checkmark     & \textbf{68.9} & \textbf{48.5}  & \textbf{50.6} & \textbf{49.5}  \\
    \bottomrule
    \end{tabular}}
  \label{tab:ablation}\end{table}

\begin{figure}[t]
	\centering
	\includegraphics[width=7.8cm]{tnse.pdf}
	\caption{The t-SNE visualization of synthesized features.}
	\label{fig:label_tsne}
	\vspace{-0.35cm}
\end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\textwidth]{results.pdf}
	\caption{Detection results by baseline RRFS and our approach on ZSFooD, UECFOOD-256, PASCAL VOC and MS COCO. Seen classes are shown in green boxes and unseen in red boxes. Zoom in for a better experience.}
	\vspace{-0.3cm}
	\label{fig:qualitative}
\end{figure*}

\noindent\textbf{Evaluations on PASCAL VOC and MS COCO.}
To further evaluate the performance of SeeDS in general ZSD, we conduct extension experiments on PASCAL VOC and MS COCO. Our SeeDS outperforms all baselines under ``ZSD'' setting, increasing the mAP by 3.4\% compared with the latest ZSD baseline RRFS \cite{huang2022robust}. Furthermore, our method obtains better performance under a more challenging setting of GZSD. The ``Seen'', ``Unseen'' and ``HM'' are improved by 1.4\%, 1.5\% and 1.4\% compared with the RRFS. Results show that our method achieves a more balanced performance on the seen and unseen classes for GZSD. The class-wise AP performance on PASCAL VOC is reported in Table~\ref{tab:vocclass}. We can observe that our approach achieves the best performance in most classes.

We evaluate the ZSD performance on MS COCO with different IoU thresholds of 0.4, 0.5 and 0.6. As seen in Table \ref{tab:cocozsd}, our method outperforms all baseline methods, achieving significant gain on both mAP and Recall@100. For the 47/17 split, our method improves the mAP and Recall@100 by 0.6\% and by 1.8\% at IoU=0.5 compared with RRFS, respectively. For the 65/15 split, our SeeDS improves the mAP and Recall@100 by 0.8\% and by 1.7\% at IoU=0.5, respectively. As shown in Table \ref{tab:cocogzsd}, our SeeDS also outperforms the RRFS under the GZSD setting, where ``S'' denotes performance on seen classes and ``U'' denotes performance on unseen classes. The absolute ``HM'' performance gain of our method is 1.2\% mAP and 1.3\% Recall@100 for the 48/17 split, and 0.8\% mAP and 0.8\% Recall@100 for the 65/15 split. Results demonstrate that our model exceeds existing ZSD methods in terms of both mAP and Recall@100.

\subsection{Ablation Study}
\label{sec:ablation}

We conduct quantitative ablation analysis for two key modules, including the S$^3$M and RFDDM modules. Table \ref{tab:ablation} reports the ``ZSD'' and ``GZSD'' performance of mAP at IoU=0.5 for ZSFooD and PASCAL VOC. We replace the ingredient corpus with the texture corpus and replace the cuisine corpus with the shape corpus when implementing S$^3$M on PASCAL VOC. As shown in Table \ref{tab:ablation}, the SeeDS incorporating the strategy of S$^3$M improves the ``HM'' by 1.1\% on ZSFooD and 1.0\% on PASCAL VOC. This indicates that the synthesizer with richer semantic information can synthesize more robust visual features for ZSFD and ZSD. We can observe that the ``ZSD'' performance has been improved by 1.4\% on ZSFooD and 1.3\% on PASCAL VOC compared with the baseline when the core GAN-based synthesizer is replaced with the RFDDM. These performance gains demonstrate the effectiveness of implementing the diffusion model in the ZSD framework. With respect to the ``ZSD'', ``U'' and ``HM'', the 1.6\%, 0.8\% and 1.5\% mAP improvement on ZSFooD and the 3.5\%, 1.5\% and 1.4\% mAP improvement on PASCAL VOC are obtained by the SeeDS that adopts both S$^3$M and RFDDM modules compared with the baseline. It shows that all proposed modules are vital for providing more robust synthesized features for training an effective Zero-shot Detector, which is able to improve the ZSFD and ZSD performance by a large margin.

\subsection{Qualitative Results}
\label{sec:qualitative}

\noindent\textbf{Feature distribution visualization.}
To further demonstrate the effectiveness of our model in optimizing the distribution structure in generating, we utilize t-SNE \cite{van2008visualizing} to visualize generated unseen features on ZSFooD and PASCAL VOC. Region features generated by the baseline RRFS and our approach are illustrated in Figure \ref{fig:label_tsne}, where we select a quarter of the categories in ZSFooD to make visualization clear. Generated features for similar classes (e.g., \emph{Stewed Noodles} and \emph{Noodles with Scallion Oil} for ZSFooD, and \emph{Car} and \emph{Train} for PASCAL VOC) are confused with each other by the baseline method because of high similarity in their semantic representation. In this case, we observe that our synthesized features are more discriminative, which form well-separated clusters. Furthermore, discriminative synthesized features can help learn a more robust unseen classifier for ZSFD and ZSD.

\noindent\textbf{Detection Results.}
We visualize the results of ZSFD on ZSFooD and UECFOOD-256 in Figure~\ref{fig:qualitative}, where the first row is the output by RRFS and the second row is by our SeeDS. The baseline RRFS fails to predict true class labels for several unseen food objects, while our model provides more accurate ZSFD results. The proposed separable semantic learning in SeeDS effectively leverages domain knowledge of ingredients and cuisines to train an unseen classifier based on inter-class separable synthesized features. Furthermore, the incorporation of RFDDM further improves the performance of the synthesizer in generating fine-grained features that are robust for ZSFD. It is worth noting that ZSD baselines are often affected by similar visual features among fine-grained categories, even when their ingredients differ. For instance, the \emph{Hot and Sour, Fish and Vegetable Ragout} is mistakenly recognized as the \emph{Miso Soup} by RRFS. In contrast, SeeDS is able to discriminate between the \emph{Classic Pizza} and the \emph{Mushroom Pizza} via the difference in ingredients.