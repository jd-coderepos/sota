\documentclass[11pt]{article}

\usepackage{times}
\usepackage{latexsym}
\usepackage{amsfonts,amsthm,amssymb}
\usepackage{euscript}
\usepackage{amstext}
\usepackage{graphicx}
\usepackage{color}
\usepackage{url,hyperref}
\usepackage{algorithmic}
\setlength{\textwidth}{6.5in} \setlength{\topmargin}{0.0in}
\setlength{\headheight}{0in} \setlength{\headsep}{0.0in}
\setlength{\textheight}{9in} \setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}


\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{assumption}[lemma]{Assumption}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{remark}[lemma]{Remark}
\newtheorem{prob}{Problem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{observation}{Observation}

\renewcommand{\proofname}{\textbf{Proof}}
\newenvironment{proofof}[1]{\smallskip\noindent{\bf Proof of #1}}{\hspace*{\fill}\par}

\newcommand{\Eqref}[1]{Eq.~(\ref{equation:#1})}
\newcommand{\obsref}[1]{Observation~\ref{observation:#1}}
\newcommand{\lemref}[1]{Lemma~\ref{lemma:#1}}
\newcommand{\clmref}[1]{Claim~\ref{claim:#1}}
\newcommand{\exmref}[1]{Example~\ref{example:#1}}
\providecommand{\conref}[1]{Conjecture~\ref{conj:#1}}
\newcommand{\algref}[1]{Algorithm~\ref{alg:#1}}
\newcommand{\factref}[1]{Fact~\ref{fact:#1}}
\newcommand{\proref}[1]{Proposition~\ref{prop:#1}}
\newcommand{\propref}[1]{Proposition~\ref{prop:#1}}
\newcommand{\exeref}[1]{Exercise~\ref{exer:#1}}
\newcommand{\probref}[1]{Problem~\ref{prob:#1}}
\newcommand{\theoref}[1]{Theorem~\ref{theo:#1}}
\newcommand{\theref}[1]{Theorem~\ref{theo:#1}}
\newcommand{\corref}[1]{Corollary~\ref{cor:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\Figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\chapref}[1]{Chapter~\ref{chap:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\apndref}[1]{Appendix~\ref{apnd:#1}}

\newcommand{\subsecref}[1]{Section~\ref{subsec:#1}}
\newcommand{\remref}[1]{Remark~\ref{rem:#1}}
\newcommand{\defref}[1]{Definition~\ref{def:#1}}
\newcommand{\mchapref}[1]{\ref{chap:#1}}
\newcommand{\msecref}[1]{\ref{sec:#1}}
\newcommand{\msubsecref}[1]{\ref{subsec:#1}}
\newcommand{\mtheoref}[1]{\ref{theo:#1}}
\newcommand{\mfigref}[1]{\ref{fig:#1}}
\newcommand{\eqnref}[1]{(\ref{eqn:#1})}

\providecommand{\conlab}[1]{\label{conj:#1}}
\newcommand{\remlab}[1]{\label{rem:#1}}
\newcommand{\obslab}[1]{\label{observation:#1}}
\newcommand{\alglab}[1]{\label{alg:#1}}
\newcommand{\eqlab}[1]{\label{equation:#1}}
\newcommand{\exmlab}[1]{\label{example:#1}}
\newcommand{\lemlab}[1]{\label{lemma:#1}}
\newcommand{\clmlab}[1]{\label{claim:#1}}
\providecommand{\deflab}[1]{\label{def:#1}}
\newcommand{\factlab}[1]{\label{fact:#1}}
\newcommand{\prolab}[1]{\label{prop:#1}}
\newcommand{\proplab}[1]{\label{prop:#1}}
\newcommand{\exelab}[1]{\label{exer:#1}}
\newcommand{\theolab}[1]{\label{theo:#1}}
\newcommand{\problab}[1]{\label{prob:#1}}
\newcommand{\thelab}[1]{\label{theo:#1}}
\newcommand{\chaplab}[1]{\label{chap:#1}}
\newcommand{\eqnlab}[1]{\label{eqn:#1}}
\newcommand{\corlab}[1]{\label{cor:#1}}
\newcommand{\figlab}[1]{\label{fig:#1}}
\newcommand{\tablab}[1]{\label{tab:#1}}
\newcommand{\seclab}[1]{\label{sec:#1}}
\newcommand{\apndlab}[1]{\label{apnd:#1}}
\newcommand{\subseclab}[1]{\label{subsec:#1}}

\newcommand{\opt}{\textrm{\sc OPT}}
\newcommand{\etal}{et al.\ }
\newcommand{\eps}{\epsilon}
\newcommand{\Algorithm}[1]{{\texttt{\bf{#1}}}} 

\newcommand{\lp}{\texttt{LP}}
\newcommand{\len}{\mathfrak{\rho}}

\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}

\newcommand{\wlaps}{\textrm{\sc LAPS}}
\newcommand{\blaps}{\textrm{\sc BLAPS}}
\newcommand{\laps}{\textrm{\sc LAPS}}

\newcommand{\On}{\texttt{On}}
\newcommand{\alg}{\texttt{NCAlg}}
\newcommand{\Opt}{\texttt{Opt}}

\newcommand{\dopt}{\Delta \opt}
\newcommand{\da}{\Delta A}
\newcommand{\dphi}{\Delta \Phi}
\newcommand{\ddphi}{\frac{\mathrm{d}}{\mathrm{dt}} \Phi}
\newcommand{\ddopt}{\frac{\mathrm{d}}{\mathrm{dt}} G^*}
\newcommand{\ddhopt}{\frac{\mathrm{d}}{\mathrm{dt}} G^*}
\newcommand{\ddg}{\frac{\mathrm{d}}{\mathrm{dt}} G}
\newcommand{\ddt}{\frac{\mathrm{d}}{\mathrm{dt}} }
\newcommand{\dt}{\mathrm{dt}}

\newcommand{\cJ}{{\cal J}}
\newcommand{\cP}{{\cal P}}
\newcommand{\cR}{{\cal R}}
\newcommand{\cS}{{\cal S}}
\newcommand{\cQ}{{\cal Q}}


\newcommand{\ahat}{\hat{A}(t)}
\newcommand{\db}[2]{
\scriptsize{
\begin{tabular}{cc}
   \\
  
\end{tabular}}
}
\newcommand{\cone}{\frac{1}{\beta}}
\newcommand{\ctwo}{\frac{8k}{\beta^3}}
\newcommand{\ctwok}{\frac{8k^2}{\beta^3}}
\newcommand{\seq}{\mathcal{S}}
\newcommand{\parr}{\mathcal{P}}
\newcommand{\old}{\mathcal{O}}
\newcommand{\young}{\mathcal{Y}}


\newcommand{\rank}{\mathbf{rank}}

\newcommand{\pca}{\frac{1}{\eps}}
\newcommand{\pcb}{(\frac{2}{\eps})^{2k+1}}
\newcommand{\cyo}{k(\frac{2}{\eps})^{2k+1}}


\newcommand{\sio}{\sum_{i \in N_o(t)}}
\newcommand{\sia}{\sum_{i \in N_a(t)}}


\begin{document}

\title{Scheduling to Minimize Energy and Flow Time in Broadcast Scheduling }


\author{
Benjamin Moseley\thanks{Department of Computer Science, University of
Illinois, 201 N.\ Goodwin Ave., Urbana, IL 61801. {\tt
bmosele2@illinois.edu}. Partially supported by NSF grant
CNS-0721899.}\\\\\vspace{4mm} 
}


\date{}
\maketitle \vspace{-7mm}

\begin{abstract}








In this paper we initiate the study of minimizing power consumption in the broadcast scheduling model.  In this setting there is a wireless transmitter.  Over time requests arrive at the transmitter for pages of information.  Multiple requests may be for the same page.  When a page is transmitted, all requests for that page receive the transmission simulteneously.  The speed the transmitter sends data at can be dynamically scaled to conserve energy.  We consider the problem of minimizing flow time plus energy, the most popular scheduling metric considered in the standard scheduling model when the scheduler is energy aware.   We will assume that the power consumed is modeled by an \emph{arbitrary} convex function. For this problem there is a  lower bound.  Due to the lower bound, we consider the resource augmentation model of Gupta \etal \cite{GuptaKP10}.  Using resource augmentation, we give a \emph{scalable} algorithm.  Our result also gives a scalable \emph{non-clairvoyant} algorithm for minimizing weighted flow time plus energy in the standard scheduling model.









\end{abstract}

\setcounter{page}{0} \thispagestyle{empty} \clearpage



\section{Introduction}


Wireless transmitters can typically be utilized in a variety of ways.  This  presents the designer with tradeoffs between power, signal range, bandwidth, cost, ect.  In this paper we consider the trade off between power and performance.  Reducing the energy consumption is of importance in many systems.  For example, in ad hoc wireless networks a typical transmitter is battery operated and, therefore, energy conservation is critical.  There are two modes of a wireless transmitter where power can be saved. (1) During idle times (2) During transmit times. Work on the first mode focuses on  putting the system to sleep when not in use \cite{Chen01,Ad00}.  By appropriately putting the system to sleep, energy consumption can be drastically reduced.   Another way the transmitter can reduce power is scaling the speed of the wireless transmission  \cite{IraniSG07}. By using more power a signal can be sent at a faster rate or, to save power, the signal can be sent at a slower rate.    We focus on reducing energy in the second mode. Reducing energy by changing the transmission speed is naturally related to the well studied model of speed scaling in scheduling theory.  However, now that we are in interested in wireless networks, this motivates a generalization of the standard speed scaling model.
 
Companies such as IBM and AMD are producing processors whose speed can be dynamically scaled by the operating system.  Now an operating systems can control the power consumed in the system by scaling the processor speed.  To model this is scheduling theory, it is assumed that there is a power function  where  is the power use when running the processor at speed .  A scheduler now not only chooses the job to schedule, but also the speed used to process the job.  In other words, a scheduler determines a job selection policy and an energy policy.  This is known as the speed scaling scheduling model.  

The standard speed scaling model is similar to our setting, except that we are interested in speed scaling in wireless networks.  Like the speed scaling setting, we assume there  is a power function  where  is the power used when transmitting at speed . To model the wireless network, we will assume a well-studied model known as the broadcast model.  In a broadcast network, there is a single server and  pages of information are stored at the server.  Over time clients send requests for pages to the server.  Multiple clients could be interested in the same page of information.  When the sever broadcasts a page , all outstanding requests for that page are simultaneously satisfied.  This is how the broadcast model differs from the standard scheduling model.  In the standard model, a each request (job) is for a unique page and broadcasting (processing) a page satisfies exactly one request. In the \emph{online} setting, the server is not aware of a request until it arrives to the system.  The broadcast model is motivated by applications in multicast systems and in wireless and LAN networks \cite{Wong88,AcharyaFZ95,AksoyF98,Hall03}.  Along with practical interest, the broadcast model has been studied extensively in scheduling literature \cite{BarnoyBNS98,AksoyF98,AcharyaFZ95,BartalM00,Hall03,GandhiKPS06}.  Similar models have been considered in queuing theory and the stochastic scheduling model \cite{DebS73,Deb84,Weiss79,WeissP81}. 


The goal of the scheduler is to satisfy the requests in an order which optimizes a quality of service metric. Naturally, this metric depends on the needs of the system.  When speed scaling is allowed, the server has a dual objective.   One is to minimize the power usage and the other is to optimize the quality of service the clients receive.  The most popular metric in speed scaling literature is minimizing a  linear combination of flow time and total energy \cite{AlbersF07,BansalPS09,ChanELLMP09,GuptaKP10,BansalCP09}.  The flow time\footnote{Flow time is also known as waiting time or response time} of a request is the amount of time the server takes to satisfy the  request.  Let  denote the total flow time of a schedule over all requests and let  denote the energy consumption of the schedule.  The scheduler focuses on minimizing . This has a natural interpretation.   Say the system is willing to spend one unit of energy to reduce    units of flow time.  For example, a system designer may be able to justify spending 1 erg of energy to decrease the flow time by  micro seconds.  The system designer would then desire a schedule that minimizes .  By scaling the units of energy and flow time, we can assume that .  The main contribution of this work is to initiate the study of energy in the broadcast model. We focus on the the problem of minimizing flow time plus energy in the broadcast setting. Besides practical interest in the model, we believe this problem is of theoretical interest as it is a natural extension of previous work. \\




\noindent {\bf Results:}  In this paper we give an algorithm for minimizing total flow time plus energy in the online broadcast setting where the power function is an \emph{arbitrary} convex function.  There is a  lower bound on the problem of minimizing  flow time in broadcast scheduling when all broadcasts are sent at a fixed rate and energy is not included in the objective \cite{KalyanasundaramPV00}. Since the power function is arbitrary, this lower bound also extends to the problem of minimizing flow time plus energy.  Thus we will resort to resource augmentation \cite{KalyanasundaramP95}.  The resource augmentation model we consider is the same as that introduced recently in \cite{GuptaKP10}.  Here, if our algorithm is given  speed advantage over the adversary, then our algorithm uses power  when broadcasting at a rate of .  The rest of the paper will be devoted to proving the following theorem.

\begin{theorem}
\label{thm:main}
There exists an algorithm that with  resource augmentation is -competitive for minimizing total flow time plus energy in broadcast scheduling with an arbitrary power function where . 
\end{theorem}

Our algorithm is called \emph{scalable} since the minimum amount of resource augmentation is used to be -competitive. Given the strong lower bound, this is the best result that can be shown in the worst case setting up to a constant in the competitive ratio. Broadcast scheduling is a generalization of the standard scheduling model.  The standard model can be interpreted as each request being for a unique page.  In fact, energy plus flow time in the broadcast scheduling strictly generalizes weighted flow time plus energy in the standard speed scaling model.  Thus, our result also gives a scalable algorithm for minimizing energy plus weighted flow time  in standard scheduling setting.  In the standard model, an algorithm is said to be \emph{non-clairvoyant} if it is online and does not know the processing time of a job.    The algorithm we introduce is non-clairvoyant when interpreted in standard scheduling model.

\begin{theorem}\label{thm:non}
There exists a non-clairvoyant algorithm that with  resource augmentation is -competitive  for weighted flow time plus energy in the standard speed scaling model.
\end{theorem}

Before this work, no non-trivial non-clairvoyant algorithm was known for (un)weighted flow time plus energy when the power function can be arbitrary in the standard speed scaling setting. However, it was known that no non-clairvoyant algorithm can be -competitive for this problem \cite{ChanELLMP09}.  Thus, resource augmentation is necessary for an algorithm to be -competitive. \\
 


\noindent {\bf Relation to Previous work:} In the standard scheduling setting, speed scaling has traditionally used power functions of the form  for some .  These power functions were motivated by the fact that  the power used by CMOS based processors is approximately . Recently, modeling the power function as an arbitrary convex was suggested to capture the power consumption of more complex systems \cite{BansalKN09}. As mentioned, in this work, we adopt this new general model.

 There is a large amount of literature on scheduling in the broadcast model.  The most popular scheduling metric considered is minimizing average flow time. Recently, it was shown that there exists an online scheduler that is a scalable algorithm for minimizing the total flow time of a schedule when broadcasts are sent at a fixed rate and energy is not considered in the objective \cite{ImM10, BansalKN09}. This work builds on the ideas and techniques given in \cite{BansalKN09}.  In particular, this paper uses the algorithm of \cite{BansalKN09} with the addition feature of being power aware.  Since the power function considered in this paper is an arbitrary convex function, our work generalizes these results.  \\



\noindent {\bf Previous work on Flow-time in Broadcast Scheduling:}  Minimizing flow time (without energy) in broadcast scheduling where broadcasts are sent at a fixed speed has a rich history beginning with the seminal work of \cite{KalyanasundaramPV00,BartalM00}.  In \cite{KalyanasundaramPV00},  Kalyanasundaram \etal showed that no online deterministic algorithm can be -competitive.  This has since been extended to show that no randomized online algorithm can be -competitive \cite{BansalCKN05}.  Due to these strong lower bounds, most previous work has focused on analyzing algorithms in a resource augmentation model \cite{KalyanasundaramP95}.  In this resource augmentation analysis, the online algorithm is given -speed and is compared to a -speed adversary.  An algorithm is said to be -speed -competitive for some objective function if the algorithm's objective when given  speed is within a  factor of the optimal solution's objective schedule given -speed for every request sequence.    An algorithm that is -speed -competitive is called scalable where  since it is -competitive when given the minimum advantage over the adversary.  



In the offline setting,  the problem was shown to be NP-Hard \cite{ErlebachH02,ChangEGK08}.  The first -speed -approximation was found in \cite{KalyanasundaramPV00}. The best positive result in the offline setting using resource augmentation is a -speed -approximation \cite{BansalCKN05}.    Without resource augmentation, \cite{BansalCKN05} gave a -approximation.  This has since been improved to a  approximation \cite{BansalCS06}.  It is long standing open question whether or not this problem admits a -approximation.

It has been difficult to find competitive online algorithms for broadcast scheduling.  The first online algorithm was given by Edmonds and Pruhs in \cite{EdmondsP03}.  This algorithm was shown to be -speed -competitive algorithm via a reduction to a different scheduling problem known as arbitrary speed up curves.   This reduction takes a -speed -competitive algorithm for the speed up curves setting and converts it into a -speed -competitive algorithm for the broadcast setting.  In \cite{EdmondsP04} Edmonds and Pruhs showed that a natural algorithm Longest-Wait-Fist (LWF) is -speed -competitive via a global charging argument.  The analysis of LWF has been improved to show that the algorithm is -speed -competitive \cite{Chekuri09lwf}. 

Later, Edmonds and Pruhs \cite{EdmondsP09} gave an algorithm  that is scalable in the arbitrary speed up curves setting.  Using the reduction \cite{EdmondsP03} this gives a -speed -competitive algorithm for the broadcast setting. It was a long standing problem whether or not there existed a scalable online algorithm in the broadcast model.  This question was  resolved by Im and Moseley and Bansal \etal by finding scalable algorithms \cite{ImM10,BansalKN09}. The reduction of \cite{EdmondsP03} was improved by Bansal \etal \cite{BansalKN09} to show that a -speed -competitive algorithm for the speed up curves setting can be converted into a -speed -competitive algorithm for the broadcast setting.  This shows that , the broadcast version of , is a scalable algorithm in the broadcast model.  Besides improving the reduction, in \cite{BansalKN09} the algorithm  was analyzed directly via a potential function analysis.  This analysis has since been extended to show an algorithm that is -speed -competitive for the -norm of flow time \cite{GuptaIKMP10}.  Recently, a scalable algorithm was found for the -norms of flow time for all  \cite{EdmondsIM10}.  \\



\noindent {\bf Previous work on Speed-Scalaing:}  All of the previous work on broadcast scheduling has assumed that the scheduler broadcasts at some fixed speed. Although speed scaling has not been considered in broadcast scheduling, it has been considered extensively in the standard scheduling model.  As mentioned, the standard scheduling model can be interpreted as each request being for a unique page.  Recall that is the speed scaling setting, a power function  is given where  is the power used when running the processor at speed .    We first consider the traditional model where  and .     In \cite{PruhsUW08} an efficient algorithm was given for  the problem of minimizing flow time offline subject to a bound on the amount of power consumed.  This algorithm can be extended to find a schedule that minimizes the flow time plus energy in the offline setting.  The problem of minimizing flow time plus energy online was first studied by \cite{AlbersF07}.  \cite{BansalPS09} showed that the algorithm that runs jobs with power proportional to the number of outstanding requests is -competitive for unit work jobs.  They also showed the Highest-Density-First algorithm coupled with this power strategy is  competitive for weighted flow plus energy.     This is since been improved in \cite{LamLTW08}  for unweighted flow plus energy to give a -competitive algorithm. In \cite{ChanELLMP09} a -competitive \emph{non-clairvoyant} algorithm was given.  Chan \etal in \cite{ChanEP09} gave a -competitive algorithm for the speed-up curves setting when there is  processors. 

 Recently, \cite{BansalCP09} introduced the problem of minimizing flow time plus energy with an arbitrary convex power function.  Surprisingly, they were able to give an algorithm that is -competitive in this general setting for flow plus energy.  This resolved a central question on whether an algorithm could be -competitive where the competitive ratio does not depend on .   Gupta \etal gave a scalable algorithm for minimizing weighted flow time plus energy in the case where there are  machines, each machine may have a different power function, and each power function is an arbitrary convex function \cite{GuptaKP10}. As mentioned, in this paper we adopt the assumption that the power function is an arbitrary convex function.\ \\





\section{Preliminaries}



We begin by introducing some notation.  There are  pages stored at the server.  Each page has a size .   A request for page  is satisfied if it receives  pieces of page  in sequential order.  That is, a request receives the transmission of page  from start to finish in that order. This will be further elaborated on later.   Notice that by this definition, multiple requests can be satisfied by a single broadcast. Let  denote the th request for page . Let  be the time this request arrives to the server.  In the online setting, this is the first time the server is aware of the request. Let  be the time the server satisfies request . The total flow time of a schedule is .  The following definitions will be useful.

\begin{definition}[convex function]
A real valued function  is convex if and only if for any  and any real valued  and  it is the case that  
\end{definition}

\begin{definition}[concave function]
A real valued function  is concave if and only if for any  and any real valued  and  it is the case that  
\end{definition}

Using the definition of a concave function, we can easily show the following proposition.

\begin{proposition}
\label{prop:one}
For any real valued concave function  where , the following holds
\begin{itemize}
\item For any positive real number , 

\item  For any positive real number  and any , 

\end{itemize}
\end{proposition}

We will be given a power function .  The value of  is the power used when the server broadcasts with speed .  In \cite{BansalCP09,GuptaKP10} with a small loss in the competitive ratio, it was assumed that power function  satisfies the following conditions,

\begin{enumerate}
\item At all speeds,  is defined, continuous and differentiable

\item  P(0) = 0

\item P is strictly increasing

\item P is strictly convex

\item P is unbounded

\end{enumerate}

In this paper we adopt these assumptions on .  Notice that the speed could be bounded. If the algorithm is given  resource augmentation and the maximum speed is  then our algorithm's maximum speed is .  Let the function .  That is,  is the maximum speed of the processor with a limit of  on the power used.  Notice that  and that  is concave.  Let  denote the speed used at time . Let  be the total energy used.  The goal of the scheduler is to minimize .

\subsection{Fractional Broadcast Scheduling}
In \emph{integral} broadcast scheduling (the standard model), a request  for page   is satisfied if it receives each of  pieces of page  in sequential order.  That is, a page is divided into pieces.  At any time the scheduler decides which piece to broadcast.  A request is satisfied if it receives each of the pieces from start to finish, in that order.   We now define a different version of the broadcast scheduling problem called \emph{fractional} broadcast scheduling.  In this setting, the  unit sized pieces of page  are indistinguishable.  Now a request  is satisfied once the server devotes a total of  time units to page  after time .  In \cite{BansalKN09} it was shown that an online algorithm running at a fixed speed that satisfies a request  by time  in a fractional schedule can be converted into a different online algorithm that completes  by time  in an integral broadcast schedule.  Here, the new algorithm is given an additional  resource augmentation.  

It is not obvious that this generalizes when the server can use varying speeds over time. In Appendix \ref{sec:red} we extend their result to the speed scaling setting. We prove the following theorem.  This theorem may be of independent interest, since it can be used to reduce integral broadcast scheduling to the fractional setting for a variety of objective functions where speeds can vary over time. 

\begin{theorem}
\label{thm:red}
Let  be a fractional broadcast schedule where the server can vary its speed over time and let .  The schedule  can be converted into a schedule  using  resource augmentation such that the schedule  satisfies the following properties
\begin{itemize}
\item The power used by  is at most the power used by .
\item If a request  is fractionally satisfied at time  under  then this request is integrally satisfied at time  under the schedule .
\item If the algorithm that generates  is online then so is the algorithm that generates .
\end{itemize}

\end{theorem}

This theorem implies the following corollary.
\begin{corollary} \label{cor}
An algorithm with  resource augmentation that is -competitive for flow time plus energy in fractional broadcast scheduling can be converted into an algorithm that is -competitive for integral broadcast scheduling and uses  resource augmentation where .  
\end{corollary}

Due to the previous corollary, we will focus on fractional broadcast scheduling for the rest of this paper.  It is important to note that if each request is for a unique page, the fractional broadcast setting is equivalent to the standing scheduling setting. This reduction is not needed for the proof of Theorem \ref{thm:non}

\subsection{The Algorithm}

Let  contain the unsatisfied requests under our algorithm's schedule at time .  Our algorithm broadcasts at speed  at time .   Intuitively, since the flow time of the schedule at time  increases by , the scheduler can afford to use this much power at time .   Now we define how our algorithm distributes its processing power.  Here the algorithm  is used.  This algorithm was introduced in \cite{EdmondsP09,BansalKN09}.    We will be assuming that our algorithm is given  resource augmentation and this implies the total speed our algorithm uses at time  is  where .  The algorithm   takes a parameter .  We will fix  later.  Let  be the  requests in  with the latest arrival times. At any time , the algorithm equally distributes its processing power amongst the requests in .  That is, for a request  page  is broadcasted at a rate of . Notice that there could be multiple requests for page  in . Let  be the set of requests for page  in . A page  is broadcasted at a rate of  at time .  We call  the amount  contributes to how much page  is broadcasted.  Notice that at any time, the algorithm uses total speed .

For brevity, throughout this paper will will be using the following simplifying assumption.   We assume that at each time ,  is integral.  A simple extension of the analysis can be made if this is not the case with the following elaboration.   Let  now be the  requests in  with latest arrival times. For the  requests with latest arrival times in ,  keeps the value of  the same.  Let  be the only other request in . For this request, we set .  That is,  is given processing power proportional to , the amount  `overlaps' the  latest arriving requests.




\section{Analysis}

We will be using a potential function argument \cite{Edmonds00}.   Let  be the increase in our algorithm's objective at time .  Likewise, let  be the increase in 's objective at time . Notice that  because there are  outstanding requests at time , which increases the flow time of the schedule by  and the scheduler uses power  at time .  Let  denote our algorithm's total cost and let  denote the optimal solution's total cost.  We will define a potential function  that will satisfy the following conditions:

\begin{enumerate}
\item {\bf Boundary Condition}:  is 0 before any request arrives and  after all requests complete
\item {\bf Arrival/Completion Condition}:  does not increase when a request is completed by  or  or when a request arrives.
\item {\bf Running Condition}:  At all times  it is the case that  where  is some positive constant.
\end{enumerate}



By integrating the running condition over time, this is sufficient to show that our algorithm is -competitive. This can be seen as follows.  The second equality holds due to .





Now we define our potential function.   We assume without loss of generality that all requests arrive at distinct times, which will simplify the definition of the potential function.  For a request  let  at time  be the number of jobs that have arrived during   that are unsatisfied by the algorithm.  Recall that  is the amount page  is broadcasted by our algorithm at time  due to  and  is the amount of page  that must be broadcasted after  to satisfy .  Let .   Let  be the amount of page  that is broadcasted at time  by  and let .  We define the variable  for a request  to be .  Our potential function is,  





 Our potential function combines the potential functions of \cite{BansalKN09} and \cite{GuptaKP10}. The potential function of \cite{BansalKN09} was used for broadcast scheduling without energy, which needs to somehow have the power function reflected in the potential if it is to work in our setting. To incorporate the power function we use some ideas given in \cite{GuptaKP10}.   

As is usually the case, our potential is designed to approximate the algorithm future cost after subtracting the optimal solution's future cost.    Our algorithm gives higher priority to jobs that have arrived recently.  The potential function is designed to capture the the remaining cost of the algorithm if it satisfies requests in the opposite order of arrival.    Fix a request .  If the optimal solution satisfied request  then the value of  should be thought of as the remaining volume of page  that must be broadcasted to satisfy request .  Assume  has the highest rank in . Then  is the speed that will be used at time .  The value of  is the number of requests waiting for request  to be satisfied. Thus,  if requests are satisfied in opposite order of arrival,  is an estimate on the flow time that will be accumulated while the algorithm satisfies  because it will take at least  time units to satisfy request .

\subsection{Change of the Potential Function}


It is easy to see that the potential function is not affected when the optimal solutions completes a request. Also the potential function does not increase when a request  arrives since . Further, the potential is  before all requests arrive and after all requests are completed.  We now show that  does not increase when a request is satisfied by the algorithm.  This lemmas, combined with the previous arguments shows that  satisfies the boundary and arrival/completion conditions.

\begin{lemma}
\label{lem:comp}
 does not increase when the algorithm completes a request.
\end{lemma}
\begin{proof}

Consider a time  where that algorithm completes a request .   We can assume that ; otherwise, trivially there is no increase in .   The change in  is,




To show that , we need to show that 



This is true by definition of  and Proposition \ref{prop:one}
\end{proof}



Now we concentrate on the running condition, the final property of  that needs to be shown.   Fix a time .  Let  be the set of requests unsatisfied by  at time .  First lets consider the change in  due to the algorithm's processing.   Recall that the algorithm broadcasts page  at a rate of  due to a request .  Further, notice that  for any request , since  must broadcast  units of page  after  to satisfy page .  Therefore, for any  we have . Notice that the  of each request the algorithm  is currently working on is at least .  This is because  consists of the  requests with latest arrival times.  Using this, we can upperbound the change in  due to the algorithm's processing,



Next, we consider the change in  due to the adversary's processing.  Let  be the page which the adversary broadcasts.  Let  be the speed the optimal solution processes page  at.  Let  be the power  uses at time .    The adversary can affect  for any request .  We first observe the following,



The last inequality holds since .  This is because the algorithm needs to only broadcast page  for a total of  amount of time to satisfy all outstanding requests for page . Let  be the request in  such that  is maximized.  We can upper bound the increase in  due to ' processing as,



Our goal is to show that .  First we consider the case when the adversary uses power at least .  In this case, the increase in the algorithm's objective can be charged directly to the optimal solution, along with any increase in the potential function.  


\begin{lemma}
\label{lem:optpower}
If  (equivalently, ) then .
\end{lemma}
\begin{proof}
First we bound the increase in  due to 's processing. Intuitively, the increase in 's objective, due to using a lot of power, is large enough to absorb the increase in  and  the increase in algorithm's  objective.  By increasing  now and charging it to , we can later use the decrease in  to pay for increases in the algorithm's objective. Recall that  is request in  for page  that maximizes .  Let  and let .    Notice that  and .    The function  is concave.  Therefore,  by Proposition \ref{prop:one}.  The increase in  due to 's processing can be bounded as follows.
   


The total power used by  is at least  since  is convex and the speed  uses is . Hence, . Recall that .  Knowing that  we have that,



\end{proof}


Due to the previous lemma, for the rest of this paper we can concentrate on the case where .  We begin by bounding the increase in  due to 's processing using this assumption,






The first inequality holds since we assumed that .  The second inequality follows from definition of  and Proposition \ref{prop:one}. We can now prove the final case of the running condition.

\begin{lemma}\label{lem:main}
If  and  then .
\end{lemma}
\begin{proof}
We know that .  The increase in  due to 's processing is at most  and the change in  due to the algorithm's processing is at most .  Combining these, we have



The second to last inequality follows from .  The last inequality follows since the optimal solution's flow time increases by  at time .
\end{proof}


Combing Lemmas (\ref{lem:optpower}) and (\ref{lem:main}) along with setting  to be  gives the running condition, namely . Thus, we have shown all the properties of , which gives the following theorem.

\begin{theorem}
The algorithm with  is -speed -competitive for flow plus energy in fractional broadcast scheduling.
\end{theorem}

By using  the reduction from integral to fractional broadcast scheduling in Corollary (\ref{cor}) and scaling , we have proven Theorem \ref{thm:main}.



\section{Conclusion}

In this paper we initiated the study of energy in the broadcast scheduling model.  We showed a scalable algorithm for average flow time plus energy.  It is important to note that the algorithm  explicitly depends on the speed . That is,  depends on the speed . Recently,  many algorithms developed for scheduling have this dependency \cite{ChekuriIM09,EdmondsP09,GuptaIKMP10}.  It would be interesting to find an algorithm which does not depend on  or show no such algorithm exists.  It would also be of interest to consider objective functions that include energy minimization for the broadcast setting.


\bibliographystyle{plain}
\bibliography{Energybroad}


\appendix

\section{Reduction of Integral to Fractional Broadcast Scheduling}\label{sec:red}

In this section we prove Theorem \ref{thm:red}. Consider any sequence of requests and let  denote a valid fractional broadcast schedule.   We now define an algorithm to construct an integral schedule .  In the integral schedule we will use  resource augmentation over the schedule .  Recall that in an integral schedule, a request  must receive  unit sized pieces of page  in \emph{sequential} order.  We will assume that the fractional schedule works on at most one page during a unit time slot.  We can assume this without loss of generality because we can set a unit time slot to be arbitrarily small, since we are assuming preemption. Let  denote the time request  is fractionally satisfied in .  Let  denote the time that  is integrally satisfied in . Our algorithm and analysis build on the reduction from fractional to integral broadcast scheduling given in  \cite{BansalKN09} where broadcasts are always sent at a fixed speed. Since the processor speeds can vary in our setting, in our analysis we will have to be careful about how speed is distributed and how processing power is accounted for.




Our algorithm will keep track of a queue  of tuples.  A tuple will be of the form .   Here  corresponds to a page. The value of  will correspond to the part of page  that will be broadcasted.  The variable  will be called the \emph{width} and  will be the \emph{start time}.     Each tuple  will correspond to some request  and the width will be .  This request will determine the part of page  that will be broadcasted and the speed of the broadcast.  The request corresponding to a tuple could be updated.  However, after the first piece of the page is broadcasted, the request corresponding to the tuple will not be changed. 

At any time when not broadcasting a page, our algorithm will determine a tuple  to broadcast.  Let  correspond to this tuple.  Let  denote the speed used by   in the th time slot devoted to page  after time .  Notice that a  volume of page  is broadcasted in this time slot under .   When broadcasting the tuple , we mean that our algorithm is broadcasting the page  with speed .  Our algorithm stops broadcasting  once a  volume of work is completed.  Notice that this takes  time because our algorithm runs at speed  and  took one time unit to broadcast the  volume of page .  If  then the algorithm broadcasts  from the beginning of the page.  Otherwise, our algorithm broadcasts  from where it left off.    Besides scaling the speeds, the algorithm is the same as that used in  \cite{BansalKN09}.


\begin{center}
\begin{tabular}[r]{|c|}
\hline
\textbf{Algorithm}: GenRounding(t) \\

\begin{minipage}{14cm}
\begin{algorithmic}
\STATE All requests arrive unmarked
\STATE Simulate the schedule 
\FOR{Any unmarked request   completed by  at time }
\IF{There is a tuple  for page  where }
\STATE Update the width of  to be 
\ELSE
\STATE Insert the tuple  into 
\ENDIF
\ENDFOR
\STATE Dequeue the tuple  with minimum width, breaking ties arbitrarily
\STATE Let  be the request which gave  the width 
\STATE Broadcast the tuple   
\IF{This broadcast was the first unit piece of page . That is, }
\STATE Set 
\ENDIF
\IF{This broadcast was the last unit piece of page . That is, }
\STATE Mark all requests for page  that arrived before time 
\ELSE
\STATE Update  to be  
\ENDIF
\end{algorithmic}
\end{minipage}\\\\

\hline
\end{tabular}
\end{center}

It can be observed that our algorithm is online if  is online. First we show that the power used by our algorithm is most the power used by .

\begin{lemma}\label{lem:overlap}
Consider any two tuples  and  for the same page   broadcast by . Let  and  be the requests which gave these tuples their width and speed, respectively.  Let  then     
\end{lemma}
\begin{proof}
The algorithm  put the tuple  into  at time .  Hence, the start time  of  must be after .   Since the algorithm inserted a new tuple for , it must be that . Hence,   .
\end{proof}

Consider any unit time slot in .  The previous lemma implies that only one tuple broadcasted in  will correspond to this time slot.  This implies the following claim.

\begin{claim}\label{clm:power}
The power used by  is at most the power used by .
\end{claim}

It remains to show that  for any request . Fix a request .  If at time  the request  is marked then  and we are done. Thus, we assume that  is unmarked at time .  Since,  is unmarked, there exists a tuple  in  at time  where .  Let  be the earliest time before time  where every tuple  dequeued during  has width less than .  Let  be the latest time after time  where every tuple dequeued during  has width less than . By definition of the algorithm, at each time during  only tuples with width at most   are dequeued.  Let  denote the set of tuples broadcasted during .  Notice by definition of the algorithm,  once a tuple is broadcasted, the width of the tuple never changes.


\begin{claim}\label{clm1}
Consider any  and let  be the request that gave  its width.  Then .
\end{claim}
\begin{proof}
Since the tuple  was broadcasted during  the width  must be less than .   By definition of  there are no tuples with width less than  in  at time .  Thus,  and by definition of width, .
\end{proof}


Let .  All   tuples in  correspond to some unit time slot where  preformed a broadcast.  By the previous claim, all of these broadcasts occurred after time . Further,  by definition of the algorithm all of these broadcasts occurred before time .  This is because, a tuple corresponding to a request  will not be inserted into  until after time  and all of the requests corresponding to tuples in  are satisfied in  by time .  This implies that the schedule  uses at least  time slots during . Thus we have,  


Since our algorithm has  resource augmentation, it takes  time to broadcast one tuple.  Since our algorithm is always busy during  we have that 

 

 
This implies that .  For the request  we know that  by Claim \ref{clm1}. Thus, . Knowing that  and , we have .  Combining this with Claim \ref{clm:power} and the fact that our algorithm is online, we have Theorem \ref{thm:red}.




















\iffalse

 First we consider the case where .  That is, a sufficiently large number of requests  are in 's queue.

\begin{lemma}
\label{lem:optflow}
If  and  then .
\end{lemma} 
\begin{proof}
For this case, we will charge the increase in  and the increase in  directly to .  To do this, we effectively ignore the decrease in  due to the algorithms processing.  First we bound , .  Here the second inequality holds because we assumed .  Now we can prove the lemma using (\ref{eqn:optproc}) and the fact that ,



\end{proof}


For the final case, we assume .  That is, we assume there are only a few requests in  which are in 's queue.  For this case, we use the credit in  to pay for the increase in the algorithm's objective.

\begin{lemma}
\label{lem:phi}
If ,  and  then .
\end{lemma}
\begin{proof}
In this case, the total increase in  objective  will be offset by the decrease in .  The decrease in  due to the algorithm's processing is given in (\ref{eqn:algproc}). Combining this with the increase in  due to 's processing given in (\ref{eqn:optproc}) and  we have,


\end{proof} 



We also know that each request in  is satisfied in  by broadcasts during .   Thus, a request  causes  to broadcast   unit pieces during .  Knowing that the algorithm is given  resource augmentation, we have that 

 Let  be the set of requests which gave the tuples in  their width.  For a request  let  be the total number of unit time slots devoted to page  during  in .    For a request  it must be the case that .  This is because a tuple corresponding to a request  has width at most  and by definition of  we have .  Thus for each request  we have that  and by Claim (\ref{clm1}) .  This implies that the schedule  uses at least  time slots during  to satisfy the requests in . Thus we have,  


\begin{proof}
Let  be the request which gave  the width . Knowing that  by definition of  it can be observed that .  By definition of time  there are no tuples in  just before time  of width less than or equal to .  Hence, we have that , which implies .
\end{proof}

Using the previous claim, we now sort the tuples in  so that .  Let  denote the total time the schedule  devotes to page  during . 










\begin{claim}\label{clm2}
 for any 
\end{claim}
\begin{proof}
Let  be the time that  was first inserted into .  It must be the case that .  If this is not the case then the tuple  would not have been inserted into .  Let  be the request that forced forced  to be put into .  By definition of the algorithm  and .  Hence, .
\end{proof}


\begin{claim}\label{clm3}
Let .  It is the case that .
\end{claim}
\begin{proof}
Let  be the first request which gave  width at most .  It is easy to see that this requests exists by definition of .  Further, by definition of , we have that  and  using the same reasoning that was used in Claim\ref{cml1}.  

We consider two cases.  If  then we have that  and, therefore, .  Hence, .  Otherwise, .  Using this and that  we have .  By definition of , the claim follows.
\end{proof}

Now we combine Claims (\ref{clm2}) and (\ref{clm3}) to get



Now we want to combine this over all pages and bound it by the volume of work done by  during .  This is where our analysis differs from \cite{BansalKN09}, since we will have to consider  using varying speeds during .

\fi




\end{document}