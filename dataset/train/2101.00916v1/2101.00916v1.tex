\def\year{2021}\relax
\documentclass[letterpaper]{article} 
\usepackage{aaai21}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \usepackage[dvipsnames, svgnames, x11names]{xcolor}
\urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \setcounter{secnumdepth}{2}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\setlength\titlebox{2.5in}
\title{How to Train Your Agent to Read and Write}

\newcommand{\norm}[1]{\lVert#1\rVert}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       
\usepackage{amsfonts}       
\usepackage{nicefrac}      
\usepackage{microtype}      
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{floatrow}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{lscape}
\usepackage[switch]{lineno}
\floatsetup[table]{capposition=bottom}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\newcommand{\e}[1]{{\small }}
\newcommand{\ph}[1]{\textcolor{blue}{#1}} 

\def\rev{\textcolor{black}}
\def\xgh{\textcolor{black}}
\def\hmg{\textcolor{black}}
\def\warning{\textcolor{red}}
\def\wzq{\textcolor{black}}
\def\ll{\textcolor{black}}
\def\dcr{\textcolor{black}}
\def\kui{\textcolor{black}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[theorem]

\def\eg{\emph{e.g., }} 
\def\Eg{\emph{E.g., }}
\def\ie{\emph{i.e., }} 
\def\Ie{\emph{I.e., }}
\def\etc{\emph{etc., }} 
\def\wrt{\emph{w.r.t. }} 
\def\etal{\emph{et al. }}


\begin{document}
\author{
\\
Li Liu,\thanks{Authors contributed equally.} Mengge He, Guanghui Xu, Mingkui Tan,\thanks{Corresponding author.} Qi Wu
\\
}
\affiliations{
 School of Software Engineering, South China University of Technology, \\
 Pazhou Laboratory, 
 University of Adelaide, \\
 Key Laboratory of Big Data and Intelligent Robot, Ministry of Education  \\
\{seliushiya, semenggehe, sexuguanghui\}@mail.scut.edu.cn, mingkuitan@scut.edu.cn, qi.wu01@adelaide.edu.au
}
\maketitle
\begin{abstract}
Reading and writing research papers is one of the most privileged abilities that a qualified researcher should master. However, it is difficult for new researchers (\eg{students}) to fully {grasp} this ability.
It would be fascinating if we could train an intelligent agent to help people read and summarize papers, and perhaps even discover and exploit the potential knowledge clues to write novel papers. 
Although there have been existing works focusing on summarizing (\emph{i.e.}, reading) the knowledge in a given text or generating (\emph{i.e.}, writing) a text based on the given knowledge, the ability of simultaneously reading and writing is still under development. Typically, this requires an agent to fully understand the knowledge from the given text materials and generate correct and fluent novel paragraphs, which is very challenging in practice.
In this paper, we propose a Deep ReAder-Writer (DRAW) network, which consists of a \textit{Reader} that can extract knowledge graphs (KGs) from input paragraphs and discover potential knowledge,
a graph-to-text \textit{Writer} that generates a novel paragraph, and a \textit{Reviewer} that reviews the generated paragraph from three different aspects. Extensive experiments show that our DRAW network outperforms considered baselines and several state-of-the-art methods on AGENDA and M-AGENDA datasets. Our code and supplementary are released at https://github.com/menggehe/DRAW.
\end{abstract}


\section{Introduction}


\hmg{Currently,}
hundreds of papers are published online every day even 
\hmg{on small topics.}
However, a study~\cite{Wang2019PaperRobotID} shows that US scientists can only read 264 papers per year on average.
\hmg{Thus, researchers are exhausted by following the sharply increased numbers of papers, much less to understanding the research and coming up with new ideas to write novel papers~\cite{Gopen1990TheSO,Buenz2019EssentialEF}.}
In practice, writing novel papers requires not only the abilities of reading and reasoning but also the ability of creative thinking, which is nontrivial for most fresh researchers~\cite{DBLP:conf/aaai/XiaoWHJ20}. It would be fantastic if an agent could help people, especially \hmg{new} researchers, to read and write. However, building such an agent encounters several challenges.


First, to understand multiple related works, the agent needs to capture complex logic in the related works, which is nontrivial.
Several knowledge extraction methods~\cite{zhang-etal-2006-composite,gerber-chai-2010-beyond,Yoshikawa2010CoreferenceBE} achieve it by identifying entities in the texts, extracting the relationships between these entities, and representing them as a knowledge graph (KG).
However, 
\hmg{they have trouble in discovering potential connections among these entities, which hampers a comprehensive understanding of related works.}



\begin{figure}[t]
    \includegraphics[width=1 \linewidth]{figures/cartoon4.pdf}
    \caption{An intuitive understanding of our DRAW network. First, the DRAW network reads multiple related works and discovers potential knowledge among them. And then, it writes a new paragraph based on knowledge graph. Last, it reviews the output and uses feedback rewards to improve the quality of writing.}
    \label{fig:simple_framework}
\end{figure}



Second, after generating a KG, the agent is then required to decode a fluent novel paragraph from the KG. In practice, however, how to evaluate the quality of the generated texts accurately is still an open problem. Existing methods~\cite{KoncelKedziorski2019TextGF,Wang2019PaperRobotID}
adopt the teacher-forcing scheme that aims to match the tokens in the generated texts to the tokens in the target texts.
However, these methods only focus on token-level matching while ignoring sentence-level and graph-level evaluation of the generated texts.


In this paper, we propose a method named 
Deep ReAder-Writer (DRAW). Our DRAW network is able to read multiple texts, discover potential knowledge, and then write a novel paragraph. 
From Figure~\ref{fig:simple_framework}, the DRAW network consists of three modules: ~\ie \textit{Reader}, \textit{Writer} and \textit{Reviewer}.
Specifically, the \textit{Reader} first extracts KGs from the research texts and discovers potential knowledge to enrich the KGs.
The \textit{Reader} considers the multi-hop neighborhood to predict new links among conceptual nodes. 
Then, the \textit{Writer} writes a novel paragraph to describe the main idea of the enriched KGs using a graph attention network, which aggregates the global and local graph information.
Inspired by the review process of research papers, we further propose a \textit{Reviewer} \dcr{module} to evaluate the quality of the generated paragraphs and return rewards as feedback signals to refine the \textit{Writer}. To be specific, given a generated paragraph, the \textit{Reviewer} will output three feedback signals, including (1) \hmg{a} quality reward, which reflects the metric scores of the generated paragraph; (2) \hmg{an} adversarial reward, which denotes the probability of the generated paragraph passing the Turing test; and (3) \hmg{an} alignment reward, which represents the matching score between the generated paragraphs and the enriched KGs.
In this way, the \textit{Writer} {is able} to write better paragraphs {that} clearly represent the key idea of the enriched KGs.


In summary, our main contributions are threefold:

\begin{itemize}
    \item We propose a Deep ReAder-Writer (DRAW) network  that reads multiple
    research texts
    and then discovers potential knowledge to write a novel paragraph covering the key idea of the source inputs.
    
    
    \item 
We propose a feedback mechanism to review whether the generated paragraph is consistent with the enriched KG, and whether the generated 
    paragraph is human written, thereby greatly improving the quality of paragraph generation.
    
    \item Extensive experiments show that our \textit{Writer}-\textit{Reviewer} leads to significant improvements in \hmg{the} KGs-to-text generation task and outperforms the state-of-the-art methods.

\end{itemize}


\section{Related Work} 
\subsubsection{Automatic writing.}

PaperRobot~\cite{Wang2019PaperRobotID} performs as an automatic research assistant to incrementally write 
\hmg{to}
chemical-related research datasets. 
It enriches KGs by predicting links of input papers' KGs. According to a given title, it then selects several entities that are related to the title in enriched KGs to generate texts. 
However, PaperRobot neglects to consider the multi-hop neighborhood to predict links, which is very important for capturing potential relationships.
In addition, the generated texts do not \hmg{closely} align with the KGs.
To address this, we use a graph attention network to consider the multi-hop neighborhood, capturing the complex and hidden information that is inherently implicit in the neighborhood. 
Moreover, we design a \textit{Reviewer} to measure the quality of the generated text from different dimensions to \hmg{effectively} align with the {KGs}.
In particular, our DRAW network is different from the multi-document summary~\cite{Ling2013Multi}, which compresses the lengthy document content into several relatively short paragraphs. We not only extract important knowledge but also discover potential knowledge from multiple paragraphs by predicting links and \hmg{writing} a novel paragraph.


\subsubsection{Link prediction.}

Some translation-based approaches~\cite{NIPS2013_5071,wang_zhen,Lin2015LearningEA} are widely used in link prediction but result in poor {representation ability}.
Recently, CNN based models \cite{Dettmers2018Convolutional2K, Nguyen2018ANE} have been proposed for relation prediction.
{These methods only focus on the entity and its neighborhood while not considering the relationships among these nodes.
Other methods~\cite{Kipf2017SemiSupervisedCW,inbook} take the relationships among the entities and their 1-hop neighbors into consideration. However, they still omit the information from multi-hop neighborhood.}
Instead, we propose a \textit{Reader} module to capture semantic information of the multi-hop neighborhood in the KG.

\subsubsection{Graph-to-Text task.} 
Graph-to-Text is an active research area. Some works generate texts based on structured knowledge~\cite{GTR-LSTM, SQLtoTextGW, nie-etal-2018-operation}, while several neural graph-to-text models use different encoders based on GNN~\cite{inproceedings,Guo,Huang2020Location} and Transformer~\cite{Vaswani2017AttentionIA} architectures to learn graph representations. \citeauthor{KoncelKedziorski2019TextGF} proposes a novel graph transformer encoder, {which} leverages the topological structure of KGs to generate texts. 
However, it ignores the global graph information, which is important for text generation. 
To solve this, \citeauthor{ribeiro2020modeling} introduce a novel architecture that aggregates both global and local graph information to generate texts. 
However, such an encoder-decoder framework presents some problems such as word repetition and lack of diversity. 
To solve these issues, we propose a \textit{Reviewer} module to review the generated paragraphs and refine the quality of paragraphs using feedback rewards.
Our \textit{Reviewer} consists of three modules to review and evaluate whether the generated paragraphs are real and to align with the given KGs, in order to improve the text generation ability.

\section{Proposed Method}

\begin{figure*}[t]
\centering
{
\includegraphics[width=0.95\linewidth, ]{figures/DRAGON7.pdf}
\caption{An overview of our Deep ReAder-Writer (DRAW) Network. The DRAW network consists of three modules, namely \textit{Reader}, \textit{Writer} and \textit{Reviewer}. Given multiple related works, the \textit{Reader} first extracts knowledge to construct initial knowledge graphs (KGs) and performs link prediction to enrich KGs. Based on the enriched KGs, the \textit{Writer} captures global and local topology information using a graph encoder and generates a novel paragraph with a text decoder. In particular, the \textit{Reviewer} employs three feedback modules to measure the quality of the generated paragraph.}
\label{fig:overview_framework}
}
\end{figure*}

In this paper, we focus on generating novel paragraphs via reading multiple AI-related paragraphs. To this end, we propose a Deep ReAder-Writer (DRAW) network that
consists of three modules, namely \textit{Reader}, \textit{Writer}, and \textit{Reviewer} as shown in Figure~\ref{fig:overview_framework}. 
To understand and sort out the textual logic of given paragraphs, the \textit{Reader} first `reads' and extracts {knowledge graphs} (KGs) from them. And then, considering the multi-hop neighborhood, the \textit{Reader} 
predicts new links between conceptual nodes, namely potential knowledge, to enrich the KGs.
The \textit{Writer} adopts a graph encoder to encode the rich semantic information in KGs, and delivers it to a text encoder to generate a novel paragraph.
{Inspired by the adversarial learning \cite{cao2019multi, wang2018graphgan, cao2020pami, Chen2020Generating}, }
we also devise a \textit{Reviewer} to evaluate the quality of the generated paragraph, which serves as a feedback signal to refine the \textit{Writer}.
We \hmg{relate} the details of these modules in the following sections.


\subsection{Reader: Text-to-Graph}
To extract the textual logic from the given related paragraphs, we use the standard SciIE~\cite{Luan2018MultiTaskIO}, a science domain information extraction system 
{to constrcu knowledge graphs}
Specifically, the output of the SciIE system is a list of triplets, where each triplet consists of two entities and the corresponding relation. 
The knowledge graph denoted as
, 
where  is the node set,  is the edge set and  represents the number of nodes.
 and  represent the extracted entities and the relations, respectively.
However, the initial knowledge graph  does not exploit potential knowledge.
To address this, we perform a link prediction to predict new links between entities based on the initial KGs.

\subsubsection{Link prediction.}
Given KGs , we obtain the entity embedding  and relation embedding  with two separated embedding layers, where  is the feature dimension.
Formally, given entity embedding ,  and relation embedding  between them, the triplet is represented by .
To aggregate more information, we introduce auxiliary edges between one entity and its n-hop neighborhood. For the entity and its n-hop neighborhood, we sum the embeddings of all the relations in the path between them as the auxiliary relation embedding.
We apply a linear transformation to update the entity representation  by:

where  is a trainable parameter and  denotes the concatenation operation. 
A particular entity  may be involved in multiple triplets and its neighborhood can be denoted as , where  denotes the -th neighborhood of \hmg{the} -th entity. 
To learn the importance of each triplet for the entity, we apply a self-attention to calculate attention weights 
as follows:

With the help of the attention weights, we update feature  by fusing the information from its neighborhood, \ie

where  is a trainable parameter and  is the Sigmoid function.

Based on the original relation feature 
, 
we apply a linear transformation to obtain the updated relation embedding .
After updating the node and relation embeddings, we need to 
determine whether there is a relationship between two given entities. An intuitive way is to calculate the probability for each triple. Following ConvKB~\cite{Nguyen2018ANE}, we train a scoring function to perform the relation prediction as follows:

where  denotes a convolution operation, {} is a set of convolution filters, and  is a linear transformation layer. Following~\cite{Nathani2019LearningAE}, we assign a score  to the triplet  in Eqn.(\ref{eq:relation_score}), which indicates the probability that the triplet holds. 
For each entity, we first traverse all entities and relationships to construct triples, and then we select the triplet with the highest score as the new link.
In this way, the \textit{Reader} can capture potential relations between different nodes and derive a new graph .
Finally, we denote the enriched knowledge graph as .



\subsection{Writer: Graph-to-Text}
Based on the enriched graph  with  entities, we propose a \textit{Writer} to generate novel paragraphs, 
which consists of a graph encoder and a text decoder.
Specifically, the \textit{Writer} first uses the graph encoder to extract the knowledge representations and then writes a new paragraph with the text decoder~\cite{Vaswani2017AttentionIA}.
\subsubsection{Graph encoder.}
A comprehensive understanding of a KG  is the first step to generate the desired paragraph. However, it is difficult to directly capture rich semantic information in the knowledge graph . 
To address this,
we extract the knowledge representations within two sub-encoders,~\ie~global-graph encoder and local-graph encoder.
Following CGE-LW \cite{ribeiro2020modeling}, we integrate global context information and local topology information to generate new paragraphs.

To aggregate global context information, we first concatenate all of the node features  and feed them into the global-graph encoder  as follows:

where  is a standard Transformer encoder~\cite{Vaswani2017AttentionIA}, which contains multi-head self-attention layers and feed-forward networks. In the global-graph encoder, we treat the knowledge graphs  as a fully connected graph without labeled edges. Based on the self-attention mechanism, the global-graph encoder is suitable for discovering the global correlation between nodes. Each node  has the ability to capture all nodes' information.

To better represent the interaction between nodes, we need to build local relations between each node and its neighborhood. However, the global-graph encoder does not explicitly consider such graph topology information. To address this, we use the local-graph encoder to model the local relations. 
For each node, we first calculate attention weights for its adjacent nodes since the different 
\hmg{types of relationships have considerable discrepancies }
in impact when fusing information. Based on the attention weights, we obtain the hidden node features  by

Here,  denotes the model parameters and   denotes the hidden features which encode the local interaction between the -th node and its neighborhood.
\hmg{ denotes the neighbourhood of the -th node.
We also perform the multi-head attention operation to learn structural information from different perspectives.}
\hmg{we employ a GRU~\cite{Cho2014LearningPR} to merge local information between different layers as follows:}




where the final node representation .


\subsubsection{Text decoder.}

Based on the node representations , we use the standard Transformer decoder~\cite{Vaswani2017AttentionIA} to generate a novel paragraph  with  words in an auto-regression manner. At each step , the text decoder consumes the previously generated tokens as additional input and outputs a probability  over candidate vocabularies. 
We train the \textit{Writer} with supervised learning as follows:

where  is the ground-truth one-hot vector at step  and \hmg{generates words} by selecting the element with the highest score at this step. In practice, the text decoder also can use other sequence generation models, such as LSTM~\cite{LSTM} and so on. 


\subsection{Reviewer: Feedback Rewards}

The encoder-decoder framework
has made great progress in many sequence generation tasks, including text summarization and image captioning. Nevertheless, it suffers from some problems. 
For each training sample, such a framework tends to use only one word as ground-truth at each generation step, even if other candidate words are also reasonable. \hmg{This} leads to a lack of diversity in the generated text.
Moreover, the language is so complex that it requires us to evaluate the quality of the generated paragraph from different dimensions, such as grammatical correctness, topic relevance, language and logic coherence, etc. Inspired by the review process of a research paper, we propose a \textit{Reviewer} module to review the generated paragraph from different dimensions. The output of \textit{Reviewer} can be used as an auxiliary training signal to optimize the \textit{Writer}, which is similar to researchers further polishing the paper based on reviews.

Specifically, we design three feedback rewards in the \textit{Reviewer}. 
{First, we use the metric scores of the generated paragraph as a reward to meet the rules of these metrics. }
Second, we train a Turing-Test discriminator to determine whether the paragraph is generated by an agent or written by a human, which draws on the idea of adversarial training and requires the paragraph to conform to the natural language specification. 
Third, we design an alignment module to align the generated paragraphs and the corresponding enriched knowledge graphs, which ensures the correctness and completeness of the generated texts.
Different from teacher-forcing methods, the \textit{Reviewer} focuses on sentence-level and graph-level alignment.
Given a generated paragraph, however, the above evaluation processes are non-differentiable. As discussed in SeqGAN~\cite{Yu2017SeqGANSG}, the discrete signals 
are limited in passing the gradient update from the \textit{Reviewer} to the \textit{Writer}. To address this, we denote the outputs of \textit{Reviewer} as rewards  and maximize expectation rewards  via reinforcement learning. Formally, the goal of \textit{Reviewer} can be represented by
    ,
where  denotes the trainable parameters of our model, and  is the paragraph generated by the \textit{Writer} based on the generation probability  \wrt~. Specifically, the reward function is denoted as

where , , \hmg{and}   correspond to the three modules of \hmg{the} \textit{Reviewer}.  and  control the contribution of the corresponding reward. Following policy gradient methods~\cite{Williams92simplestatistical,Schulman2017ProximalPO}, we can solve the above problem in batch training as follows:

\xgh{where  is the training batch size. Now, we introduce these reward modules in detail.}

\subsubsection{Quality reward.}
Given a generated paragraph , we can calculate some quantitative metrics for it, such as BLEU\cite{10.3115/1073083.1073135}, METEOR~\cite{Denkowski2014MeteorUL}, CIDEr~\cite{Vedantam2015CIDErCI}, \textit{etc}. Directly using these metrics as the training reward can boost the sentence generation quality.
In this paper, we simply adopt the BLEU score  as the reward since the BLEU score is one of the most popular automated and inexpensive metrics. In practice, the BLEU can be replaced with any metric that needs to be optimized.

\subsubsection{Adversarial reward.}
Based on a paragraph , this module acts as a discriminator to determine whether  is manual annotation (Real) or generated by the machine (Fake).
Following \cite{Yu2017SeqGANSG}, we use Convolutional Neural Network (CNN) to extract text features since it can capture sequence information and has shown \hmg{exhibited} high performance in the complicated sequence classification task \cite{Zhang2015TextUF}. 
Specifically, given a generated paragraph , we first concatenate the token embedding as the text representation. 
\hmg{We then }use different numbers of kernels with different window sizes to extract different features over the text representation and produce a new feature map.
After applying a max-pooling operation, we perform a fully connected layer with Sigmoid activation to output a probability, which denotes the probability \hmg{that} the input text is real. The calculation can be formulated as .
Inspired by adversarial training \cite{cao18a}, this module aims to minimize the performance gap between humans and the \textit{Writer}.

\subsubsection{Alignment reward.}

A paragraph  is supposed to align its enriched KG  since  is generated by \textit{Writer} according to the . In this sense, we propose to compute the similarity between  and  based on the attention mechanism. Given an abstract  with  words, we first use Long Short-Term Memory (LSTM) to extract text representation , where ,  . Following AttnGAN~\cite{AttnGANFT}, we obtain the hidden representation as follows:

where , , \hmg{and}  are trainable parameters,  is a scaling factor and  are node features obtained from the \textit{Writer}. With the help of \hmg{the} self-attention mechanism~\cite{Vaswani2017AttentionIA}, the hidden feature  not only fuses the text representations but also merges graph information.
Then, we calculate the cosine similarity as matching score  as follows:

Thus far, we can obtain the rewards ,  , \hmg{and}
 from above the \textit{Reviewer} modules.
Finally, to train our DRAW network, we define the overall training loss as follows:

where  is a trade-off parameter.   trains the DRAW network within supervised learning while  allows the DRAW network to explore diverse generation via reinforcement learning and evaluate the generation from multiple orientations.


\section{Experiments}
\subsection{Datasets}
\subsubsection{AGENDA dataset.} 
AGENDA is one of the most popular KGs-to-text datasets, which concludes 40,000 pair samples collected from the proceedings of 12 top AI conferences.
Each sample consists of a title, an abstract, and the corresponding KG, which is extracted by the SciIE system.
The KG is composed of recognized scientific terms and their relationships.
In particular, the types of scientific terms include Task, Metric, Method, Material, and Other. \hmg{The} types of relationships include Used-for, Conjunction, Feature-of, Part-of, Compare, Evaluate-for, and Hyponym-of.

\subsubsection{M-AGENDA dataset.} 
To further demonstrate the effectiveness of our DRAW network, we create a new dataset, called M-AGENDA. 
Specifically, we first calculate the cosine similarity between each abstract and the others in the AGENDA dataset.
We select two most-related instances for each one and combine these three as a new data example in the M-AGENDA dataset.


\subsection{Experimental Settings}
\subsubsection{Implementation details.} 
Our DRAW network consists of three well-design modules, \ie{\textit{Reader}, \textit{Writer} and \textit{Reviewer}.}
\hmg{We first train our \textit{Reader}, \textit{Writer} and \textit{Reviewer} on AGENDA dataset. Then, we use the trained \textit{Reader} and \textit{Writer} model on the M-AGENDA to generate novel paragraphs.}
To speed up convergence early in training, we adopt different pretraining strategies for each module. For the \textit{Reader},
we first use TransE~\cite{NIPS2013_5071} to train entity and relation embeddings. 
\hmg{We then} aggregate information passed from a 2-hop neighborhood to update the embedding of each node.
Following~\cite{Nathani2019LearningAE}, we use Adam optimization with an initial learning rate of 0.1. 
For the \textit{Writer}, we pre-train for 30 epochs with early stopping. Following~\cite{ribeiro2020modeling}, we use Adam optimization with an initial learning rate of 0.5. To ensure the generation effect, we set the maximum generation length to 430.
For the \textit{Reviewer}, we pre-train the adversarial module with SGD optimization and initialize a learning rate of 0.001. When pre-training the graph encoder of the alignment module, we use the same model and parameters of \textit{writer}. \hmg{In addition}, we systematically adjust the values of  and  to conduct several ablation studies.
We find that the experimental results of different coefficient combinations 
fluctuate only around 0.1, \hmg{causing} little effect on the results.
\textit{Writer}-\textit{Reviewer} obtains the best results with { . We set the trade-off parameter }.
We implement our method with PyTorch.


\begin{table}
	\centering
	
	\begin{tabular}{l|c c c }
		\hline
	Model & BLEU & METEOR   &CIDEr
		\\ \hline
		GraphWriter 
		
		& 14.44    & 18.80  &28.30 \\ 
		GraphWriter+RBS  &15.17  & 19.59  &- \\
		Graformer  & 17.33  &21.43  &- \\
		CGE-LW     & 18.01    & 22.34  &33.06  \\ \hline
        \textit{Writer}-\textit{Reviewer} (\textbf{Ours}) & \textbf{19.60}    & \textbf{24.03}  &\textbf{45.21} \\ \hline
	\end{tabular}
	\caption{Quantitative evaluations of generation systems on the AGENDA dataset (higher is better).}
	\label{tab:Automatic Evaluations}
\end{table}

\begin{table}[t]
	\centering

	\begin{tabular}{l|c c}
		\hline
		\multirow{2}[0]{*}{Paragraph} & \multicolumn{2}{c}{Turing Test Results}\\
		\cline{2-3}
		 & Human & Machine             \\ \hline
		Written by Human & 68\%    & 32\% \\ 
		Written by \textbf{DRAW} & 48\%   & 52\% \\ \hline
	\end{tabular}
	\caption{Quantitative results of Turing test.}
	\label{tab:Turing}
\end{table}


\begin{table}[t]
	\centering
	
	\begin{tabular}{l|ccc}
		\hline
		Model & BLEU & METEOR   &CIDEr         \\ \hline
		\textit{Writer}  & 18.01    & 22.34   &33.06 \\
		\textit{Writer}+Adversarial  & 19.37    & 23.87   &39.30 \\ 
		\textit{Writer}+Alignment & 19.33    & 24.00   &43.49 \\ 
		\textit{Writer}+Quality & 19.50    & 24.03    &44.40 \\ \hline
		\textit{Writer}-\textit{Reviewer} (\textbf{Ours)}     &\textbf{19.60}    & \textbf{24.03}   &\textbf{45.21} \\ \hline
	\end{tabular}
	\caption{Ablation study for modules used in the \textit{Reviewer} on the AGENDA dataset.}
	\label{tab:reviewer}
\end{table}

\begin{table}[t]
	\centering
  	
	\resizebox{\textwidth}{!}
	{
	\begin{tabular}{l|ccc}
		\hline
		Model       & Grammar   & Coherence   & Informativeness    \\\hline
		PaperRobot       & 5.11      &4.95    &5.01     \\ 
		CGE-LW       & 6.77     & 6.29   & 6.57   \\ \hline
		\textbf{DRAW (Ours) }             &\textbf{7.63}     &\textbf{6.83}  &\textbf{7.10}   \\ \hline
	\end{tabular}
\caption{Automatic evaluations results (higher is better).}
	
	\label{tab:group_human}
	}
\end{table}





\begin{table*}[t]
    \begin{center}
	\centering
	\begin{tabular}{l p{15cm}}
	\toprule
	
        Initial KGs & \textbf{41} entities, \textbf{18} relations: (global scene-level contextual information, PART-OF, spatial context recurrent convnet model) ; (wikipedia, USED-FOR, multilingual ner systems) ; (local image de-scriptors, CONJUNCTION, spatial configurations)
        \dots
        \\ \midrule
        
        PaperRobot & In this paper we propose a novel approach for multilingual \textcolor{orange}{named entity} recognition tasks  . The proposed method is based on semantic similarity measure that can be used to improve \textcolor{orange}{word retrieval} performance by using \textcolor{orange}{wikipedia} type of words from \textcolor{orange}{text documents} and then build an efficient query \textcolor{orange}{language model} which allows users with similar information between entities as clusters across different domains : part-of-speech tags are generated through each user 's document representation ; our knowledge base system was evaluated over state-of-the-art approaches trained object \dots   [\textit{covering 6 entities.}]
        
         \\ 
    		CGE-LW  &  in this paper ,\dots we propose a \textcolor{orange}{spatial context recurrent convnet model} to incorporate \textcolor{orange}{global scene-level contextual information} into a spatial context recurrent convnet model for \textcolor{orange}{object retrieval} .\dots, and \textcolor{red}{the contextual information from candidate boxes is used for object retrieval}. a positional language model that captures \textcolor{red}{contextual information from candidate boxes for object retrieval}. the proposed system is evaluated on the \textcolor{orange}{tac-kbp 2010 data},and the experimental results show that the proposed system can significantly improve the entity linking performance\dots [\textit{covering 21 entities.}]
		 

		
  \\ 
		DRAW &  in this paper , we propose a novel approach to  \textcolor{blue}{entity linking} based on \textcolor{blue}{statistical language model-based information retrieval} , which exploits both \textcolor{blue}{local contexts and global world knowledge} to improve the \textcolor{blue}{entity linking} performance.\dots, we propose a \textcolor{orange}{spatial context recurrent convnet model} to integrate \textcolor{orange}{global context features} with local \textcolor{blue}{image de-scriptors} ,\textcolor{orange}{ spatial configurations} , and \textcolor{blue}{global scene-level contextual information} into a spatial context recurrent convnet model\dots, and a recurrent network with \textcolor{orange}{local and global information} to guide the search for \textcolor{orange}{candidate boxes} for \textcolor{orange}{object retrieval}\dots [\textit{covering 26 entities.}]
  \\ \bottomrule
	\end{tabular}
	\caption{ Example outputs of various models. \xgh{To better visualize the generated text, we omit information irrelevant to the comparisons.} Repetitive words are represented in \textcolor{red}{red} and entities included in KGs are represented in \textcolor{orange}{orange}. The potential knowledge is represented in \textcolor{blue}{blue} with the corresponding superscript.
	}
	\label{tab:example}
	\end{center}
\end{table*}

\subsubsection{Evaluation metrics.} 
To demonstrate the quality of the generated paragraphs, we report both quantitative results and human study results.
We divide our evaluation into two parts: KGs-to-text evaluation and overall performance evaluation.

For KGs-to-text evaluations, we adopt three general quantitative evaluation metrics, \ie BLEU~\cite{10.3115/1073083.1073135}, METEOR~\cite{Denkowski2014MeteorUL} and CIDEr~\cite{Vedantam2015CIDErCI} to 
evaluate our \textit{Writer}-\textit{Reviewer}. In addition, to demonstrate the realness of the paragraphs generated by our model,
we also set up a Turing test. Specifically, we randomly select 100 abstracts and shuffle them to \hmg{find} an evaluation set, where half of the abstracts are written by authors and the rest are generated by our \textit{Writer}-\textit{Reviewer}. 
After that, we test the turkers on Amazon Mechanical Turk (AMT) to determine whether the paragraphs in the evaluation set are written by humans.

For overall performance evaluation, 
we set up a human study to rate the abstracts generated by DRAW network, CGE-LW and PaperRobot. 
For each model, we randomly select 50 generated paragraphs and score them in terms of {`grammar', `informativeness', and `coherence' on Amazon Mechanical Turk (AMT)}. 
Specifically, the metric `grammar' measures the paragraphs written in well-formed English. The metric `informativeness' denotes whether the paragraphs make use of appropriate scientific terms. The metric `coherence' denotes that the generated text conforms to general specifications. For example, a complete abstract should include a brief introduction \hmg{to} a task, describe the solution, analyze and discuss the results, and so on.
Each metric \hmg{described above}, contains 10 levels, \hmg{with} rankings from 1 to 10 (from bad to good). 

\hmg{Following the relation prediction task~\cite{Nathani2019LearningAE}, we evaluate our link prediction  method of \textit{Reader} on the proportion of correct entities in the top N ranks (Hits@N) for N=1,3, and 10.}

\subsection{KGs-to-text Evaluation on AGENDA Dataset}
To verify our model on KGs-to-text task, we compare our \textit{Writer}-\textit{Reviewer} against several state-of-the-art models including GraphWriter \cite{KoncelKedziorski2019TextGF}, GraphWriter+RBS \cite{An2019RepulsiveBS}, Graformer \cite{Schmitt2020ModelingGS} and CGE-LW \cite{ribeiro2020modeling} on the AGENDA dataset. 

\subsubsection{Results.} 
We report the results of our method and other compared models \hmg{with respect to} three quantitative evaluation metrics in Table~\ref{tab:Automatic Evaluations}. As shown in Table~\ref{tab:Automatic Evaluations},
our \textit{Writer}-\textit{Reviewer} \wzq{achieves better performance than all the compared models in three quantitative evaluation metrics.
Specifically, our \textit{Writer}-\textit{Reviewer}} outperforms the 
state-of-the-art method
CGE-LW by 1.6 points in BLEU, 1.7 points in METEOR and 12.2 points in CIDEr. 
These results demonstrate the superiority of our \textit{Writer}-\textit{Reviewer} in the KGs-to-text task.

In addition, we carry out a human evaluation to demonstrate the effectiveness of our \textit{Writer}-\textit{Reviewer}. 
To be specific, for each paragraph in the evaluation set, we ask the human to choose whether these paragraphs are written by human-authors.
From these results in Table~\ref{tab:Turing}, nearly half of the paragraphs generated by our \textit{Writer}-\textit{Reviewer} are reviewed as written by humans. More critically, 32\% of the paragraphs written by humans are chosen as written by the AI system. These results demonstrate that our \textit{Writer}-\textit{Reviewer} can generate \hmg{realistic paragraphs similar to those written by humans}.

\begin{table}[t]
	\centering

	\begin{tabular}{l|cccc}
	\hline
		\multirow{2}[0]{*}{Method} &
		 \multicolumn{3}{c}{Hits@N} \\
		 \cline{2-5}
	     & @1 & @3 & @10 &   \\
	 \hline
	    PaperRobot   &11.9  &19.5  &42.4    \\
		\textbf{Our}   &\textbf{36.8}  & \textbf{46.0}   & \textbf{56.1} 
		 \\
     \hline
	\end{tabular}
	
	\caption{Accuracy of the link prediction on the M-AGENDA dataset. Hits@N values are in percentage.}
	
	\label{tab:automatic_metric}
	
\end{table}
\subsubsection{Ablation studies in \textit{Reviewer}.}
To investigate the effect of different modules in 
\textit{Reviewer}, we conduct an ablation study.
As shown in Table~\ref{tab:reviewer},
\textit{Writer} combined with one of the modules in \textit{Reviewer} arbitrarily obtains better performance than \textit{Writer}, which demonstrates the effectiveness of the modules in \textit{Reviewer}.  \textit{Writer} combined with all the modules in \textit{Reviewer}, namely \textit{Writer}-\textit{Reviewer}, achieves best performance.




\subsection{Evaluation \wzq{on M-AGENDA Dataset}}

To show the effectiveness of our DRAW network, we conduct experiments on the M-AGENDA dataset. Since the M-AGENDA dataset does not provide ground-truth, we conduct human study instead of quantitative evaluations. Specifically, for each metric in the human study, we average the scores of the paragraphs rated by the humans as the final score.



\subsubsection{Results of DRAW.}
We report the experimental results of our DRAW network and other compared methods in Table~\ref{tab:group_human}. 
From these results, our DRAW network achieves the best performance in terms of `grammar', `coherence', and `informativeness'. 
Specifically, 
PaperRobot~\cite{Wang2019PaperRobotID} obtains poor performance due to the neglect of the topological structure between entities.
CGE-LW~\cite{ribeiro2020modeling} takes advantage of the graph information effectively and achieves 6.77, 6.29, and 6.57 points in terms of three metrics, but it also ignores the fact that the generated paragraphs are supposed to match the KGs.
Different from the methods above, our DRAW network not only performs link prediction with multi-hop information in the \textit{Reader} but also matches the graphs and the generated paragraphs, and thus achieves the best performance. 
More ablation experiments about \textit{Reader} can be found in the supplementary material.

\subsubsection{Results of \textit{Reader}.}
\hmg{
As shown in Table \ref{tab:automatic_metric}, we report the experimental results of the link prediction method of our \textit{Reader} and PaperRobot. Our method achieves the Hits@1, Hits@3, Hits@10 scores of 36.8, 46.0, and 56.1, outperforming the PaperRobot by 24.5, 26.5, and 13.9 points, respectively. 
It demonstrates the effectiveness of our link prediction method.
}

\subsubsection{Visualization analysis.}

As shown in Table~\ref{tab:example}, we visualize a generated paragraph of our DRAW network. 
\hmg{More visualization results can be found in the supplementary material.}
We see that our DRAW network has the ability to cover more entities (represented in \textcolor{orange}{orange}), while PaperRobot  mentions less entities in the given KG. In addition, CGE-LW tends to repeat unrelated entities/sentences (represented in \textcolor{red}{red}).
With the help of \textit{Reviewer}, the generated text of DRAW network is fluent and grammatically correct. Moreover, our DRAW network is able to discover the potential relationships between entities (represented in \textcolor{blue}{blue} superscript.)


\section{Conclusions and Future Work}
In this paper, we propose a Deep ReAder-Writer (DRAW) network that reads multiple AI-related abstracts and then writes a new paragraph to represent enriched knowledge combining the potential knowledge
covering the topics mentioned in the source abstracts. 
Inspired by the review process, we propose a
\textit{Reviewer} to rate the quality of the generated texts from different dimensions, which \hmg{serve} as feedback signals to refine our DRAW network.
Ablation experiments demonstrate the effectiveness of our method. Moreover, \textit{Writer}-\textit{Reviewer} achieves state-of-the-art results on KGs-to-text generation task.
In terms of human study, some generations of our DRAW network successfully pass the Turing test and confuse the turkers. In future study, we will extend the DRAW network to write a complete paper in an iterative manner and develop more techniques to discover novel ideas, such as creating new entities.

\section*{Acknowledgments}
This work was partially supported by 
Key-Area Research and Development Program of Guangdong Province 2018B010107001,
National Natural Science Foundation of China (NSFC) 61836003 (key project),
Program for Guangdong Introducing Innovative and Entrepreneurial Teams 2017ZT07X183,
International Cooperation open Project of State Key Laboratory of Subtropical Building Science, South China University of Technology (2019ZA01),
Fundamental Research Funds for the Central Universities D2191240.


\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi:\discretionary{}{}{}#1}\else
  \providecommand{\doi}{doi:\discretionary{}{}{}\begingroup
  \urlstyle{rm}\Url}\fi

\bibitem[{An(2019)}]{An2019RepulsiveBS}
An, B. 2019.
\newblock Repulsive Bayesian Sampling for Diversified Attention Modeling.
\newblock In \emph{workshop of NeurIPS}.

\bibitem[{Bordes et~al.(2013)Bordes, Usunier, Garcia-Duran, Weston, and
  Yakhnenko}]{NIPS2013_5071}
Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and Yakhnenko, O. 2013.
\newblock Translating Embeddings for Modeling Multi-relational Data.
\newblock In \emph{NeurIPS}.

\bibitem[{Buenz(2019)}]{Buenz2019EssentialEF}
Buenz, E.~J. 2019.
\newblock Essential elements for high-impact scientific writing.
\newblock \emph{Nature} doi: 10.1038/d41586-019-00546-7.

\bibitem[{Cao et~al.(2018)Cao, Guo, Wu, Shen, Huang, and Tan}]{cao18a}
Cao, J.; Guo, Y.; Wu, Q.; Shen, C.; Huang, J.; and Tan, M. 2018.
\newblock Adversarial Learning with Local Coordinate Coding.
\newblock In \emph{ICML}.

\bibitem[{Cao et~al.(2020)Cao, Guo, Wu, Shen, Huang, and Tan}]{cao2020pami}
Cao, J.; Guo, Y.; Wu, Q.; Shen, C.; Huang, J.; and Tan, M. 2020.
\newblock Improving Generative Adversarial Networks with Local Coordinate
  Coding.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}
  .

\bibitem[{Cao et~al.(2019)Cao, Mo, Zhang, Jia, Shen, and Tan}]{cao2019multi}
Cao, J.; Mo, L.; Zhang, Y.; Jia, K.; Shen, C.; and Tan, M. 2019.
\newblock Multi-marginal wasserstein gan.
\newblock In \emph{NeurIPS}.

\bibitem[{Chen et~al.(2020)Chen, Zhang, Tan, Xiao, Huang, and
  Gan}]{Chen2020Generating}
Chen, P.; Zhang, Y.; Tan, M.; Xiao, H.; Huang, D.; and Gan, C. 2020.
\newblock Generating Visually Aligned Sound From Videos.
\newblock \emph{{IEEE} Trans. Image Process.} 29: 8292--8302.

\bibitem[{Cho et~al.(2014)Cho, Merrienboer, Çaglar G{\"u}lçehre, Bahdanau,
  Bougares, Schwenk, and Bengio}]{Cho2014LearningPR}
Cho, K.; Merrienboer, B.~V.; Çaglar G{\"u}lçehre; Bahdanau, D.; Bougares, F.;
  Schwenk, H.; and Bengio, Y. 2014.
\newblock Learning Phrase Representations using RNN Encoder-Decoder for
  Statistical Machine Translation.
\newblock In \emph{EMNLP}.

\bibitem[{Denkowski and Lavie(2014)}]{Denkowski2014MeteorUL}
Denkowski, M.~J.; and Lavie, A. 2014.
\newblock Meteor Universal: Language Specific Translation Evaluation for Any
  Target Language.
\newblock In \emph{ACL}.

\bibitem[{Dettmers et~al.(2018)Dettmers, Minervini, Stenetorp, and
  Riedel}]{Dettmers2018Convolutional2K}
Dettmers, T.; Minervini, P.; Stenetorp, P.; and Riedel, S. 2018.
\newblock Convolutional 2D Knowledge Graph Embeddings.
\newblock In \emph{AAAI}.

\bibitem[{Feng et~al.(2018)Feng, Jinpeng, Jin-Ge, Rong, and
  Chin-Yew}]{nie-etal-2018-operation}
Feng, N.; Jinpeng, W.; Jin-Ge, Y.; Rong, P.; and Chin-Yew, L. 2018.
\newblock Operation-guided Neural Networks for High Fidelity Data-To-Text
  Generation.
\newblock In \emph{EMNLP}.

\bibitem[{Gerber and Chai(2010)}]{gerber-chai-2010-beyond}
Gerber, M.; and Chai, J. 2010.
\newblock Beyond {N}om{B}ank: A Study of Implicit Arguments for Nominal
  Predicates.
\newblock In \emph{ACL}.

\bibitem[{Gopen and Ja(1990)}]{Gopen1990TheSO}
Gopen, G.~D.; and Ja, S. 1990.
\newblock The Science of Scientific Writing.
\newblock \emph{American Scientist} .

\bibitem[{Hochreiter and Schmidhuber(1997)}]{LSTM}
Hochreiter, S.; and Schmidhuber, J. 1997.
\newblock Long Short-term Memory.
\newblock \emph{Neural Computation} .

\bibitem[{Huang et~al.(2020)Huang, Chen, Zeng, Du, Tan, and
  Gan}]{Huang2020Location}
Huang, D.; Chen, P.; Zeng, R.; Du, Q.; Tan, M.; and Gan, C. 2020.
\newblock Location-Aware Graph Convolutional Networks for Video Question
  Answering.
\newblock In \emph{AAAI}, 11021--11028.

\bibitem[{Kipf and Welling(2017)}]{Kipf2017SemiSupervisedCW}
Kipf, T.; and Welling, M. 2017.
\newblock Semi-Supervised Classification with Graph Convolutional Networks.
\newblock In \emph{ICLR}.

\bibitem[{Koncel-Kedziorski et~al.(2019)Koncel-Kedziorski, Bekal, Luan, Lapata,
  and Hajishirzi}]{KoncelKedziorski2019TextGF}
Koncel-Kedziorski, R.; Bekal, D.; Luan, Y.; Lapata, M.; and Hajishirzi, H.
  2019.
\newblock Text Generation from Knowledge Graphs with Graph Transformers.
\newblock In \emph{NAACL-HLT}.

\bibitem[{Lin et~al.(2015)Lin, Liu, Sun, Liu, and Zhu}]{Lin2015LearningEA}
Lin, Y.; Liu, Z.; Sun, M.; Liu, Y.; and Zhu, X. 2015.
\newblock Learning Entity and Relation Embeddings for Knowledge Graph
  Completion.
\newblock In \emph{AAAI}.

\bibitem[{Ling and Hui(2013)}]{Ling2013Multi}
Ling, F.~U.; and Hui, Z. 2013.
\newblock Multi-document summary using LDA and spectral clustering.
\newblock \emph{Computer Engineering \& Applications} .

\bibitem[{Luan et~al.(2018)Luan, He, Ostendorf, and
  Hajishirzi}]{Luan2018MultiTaskIO}
Luan, Y.; He, L.; Ostendorf, M.; and Hajishirzi, H. 2018.
\newblock Multi-Task Identification of Entities, Relations, and Coreference for
  Scientific Knowledge Graph Construction.
\newblock In \emph{EMNLP}.

\bibitem[{Min et~al.(2006)Min, Jie, Jian, and
  GuoDong}]{zhang-etal-2006-composite}
Min, Z.; Jie, Z.; Jian, S.; and GuoDong, Z. 2006.
\newblock A Composite Kernel to Extract Relations between Entities with Both
  Flat and Structured Features.
\newblock In \emph{ACL}.

\bibitem[{Nathani et~al.(2019)Nathani, Chauhan, Sharma, and
  Kaul}]{Nathani2019LearningAE}
Nathani, D.; Chauhan, J.; Sharma, C.; and Kaul, M. 2019.
\newblock Learning Attention-based Embeddings for Relation Prediction in
  Knowledge Graphs.
\newblock In \emph{ACL}.

\bibitem[{Nguyen et~al.(2018)Nguyen, Nguyen, Nguyen, and Phung}]{Nguyen2018ANE}
Nguyen, D.~Q.; Nguyen, T.; Nguyen, D.~Q.; and Phung, D.~Q. 2018.
\newblock A Novel Embedding Model for Knowledge Base Completion Based on
  Convolutional Neural Network.
\newblock In \emph{NAACL-HLT}.

\bibitem[{Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu}]{10.3115/1073083.1073135}
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
\newblock BLEU: A Method for Automatic Evaluation of Machine Translation.
\newblock In \emph{ACL}.

\bibitem[{Ribeiro, Gardent, and Gurevych(2019)}]{inproceedings}
Ribeiro, L.; Gardent, C.; and Gurevych, I. 2019.
\newblock Enhancing AMR-to-Text Generation with Dual Graph Representations.
\newblock In \emph{EMNLP-IJCNLP}.

\bibitem[{Ribeiro et~al.(2020)Ribeiro, Zhang, Gardent, and
  Gurevych}]{ribeiro2020modeling}
Ribeiro, L. F.~R.; Zhang, Y.; Gardent, C.; and Gurevych, I. 2020.
\newblock Modeling Global and Local Node Contexts for Text Generation from
  Knowledge Graphs.
\newblock \emph{Transactions of the Association for Computational Linguistics}
  .

\bibitem[{Schlichtkrull et~al.(2018)Schlichtkrull, Kipf, Bloem, Berg, Titov,
  and Welling}]{inbook}
Schlichtkrull, M.; Kipf, T.; Bloem, P.; Berg, R.; Titov, I.; and Welling, M.
  2018.
\newblock Modeling Relational Data with Graph Convolutional Networks.
\newblock In \emph{ESWC}.

\bibitem[{Schmitt et~al.(2020)Schmitt, Ribeiro, Dufter, Gurevych, and
  Schutze}]{Schmitt2020ModelingGS}
Schmitt, M.; Ribeiro, L. F.~R.; Dufter, P.; Gurevych, I.; and Schutze, H. 2020.
\newblock Modeling Graph Structure via Relative Position for Better Text
  Generation from Knowledge Graphs.
\newblock \emph{ArXiv} abs/2006.09242.

\bibitem[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov}]{Schulman2017ProximalPO}
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017.
\newblock Proximal Policy Optimization Algorithms.
\newblock \emph{ArXiv} abs/1707.06347.

\bibitem[{Trisedya et~al.(2018)Trisedya, Jianzhong, Rui, and Wei}]{GTR-LSTM}
Trisedya, B.; Jianzhong, Q.; Rui, Z.; and Wei, W. 2018.
\newblock GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data.
\newblock In \emph{ACL}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{Vaswani2017AttentionIA}
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.~N.;
  Kaiser, L.~u.; and Polosukhin, I. 2017.
\newblock Attention is All you Need.
\newblock In \emph{NeurIPS}.

\bibitem[{Vedantam, Zitnick, and Parikh(2015)}]{Vedantam2015CIDErCI}
Vedantam, R.; Zitnick, C.~L.; and Parikh, D. 2015.
\newblock CIDEr: Consensus-based image description evaluation.
\newblock In \emph{CVPR}.

\bibitem[{Wang et~al.(2018)Wang, Wang, Wang, Zhao, Zhang, Zhang, Xie, and
  Guo}]{wang2018graphgan}
Wang, H.; Wang, J.; Wang, J.; Zhao, M.; Zhang, W.; Zhang, F.; Xie, X.; and Guo,
  M. 2018.
\newblock Graphgan: Graph representation learning with generative adversarial
  nets.
\newblock In \emph{AAAI}.

\bibitem[{Wang et~al.(2019)Wang, Huang, Jiang, Knight, Ji, Bansal, and
  Luan}]{Wang2019PaperRobotID}
Wang, Q.; Huang, L.; Jiang, Z.; Knight, K.; Ji, H.; Bansal, M.; and Luan, Y.
  2019.
\newblock PaperRobot: Incremental Draft Generation of Scientific Ideas.
\newblock In \emph{ACL}.

\bibitem[{Williams(1992)}]{Williams92simplestatistical}
Williams, R.~J. 1992.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine Learning} .

\bibitem[{Xiao et~al.(2020)Xiao, Wang, He, and Jin}]{DBLP:conf/aaai/XiaoWHJ20}
Xiao, L.; Wang, L.; He, H.; and Jin, Y. 2020.
\newblock Copy or Rewrite: Hybrid Summarization with Hierarchical Reinforcement
  Learning.
\newblock In \emph{AAAI}.

\bibitem[{Xu et~al.(2018{\natexlab{a}})Xu, Wu, guo Wang, Yu, Chen, and
  Sheinin}]{SQLtoTextGW}
Xu, K.; Wu, L.; guo Wang, Z.; Yu, M.; Chen, L.; and Sheinin, V.
  2018{\natexlab{a}}.
\newblock SQL-to-Text Generation with Graph-to-Sequence Model.
\newblock In \emph{EMNLP}.

\bibitem[{Xu et~al.(2018{\natexlab{b}})Xu, Zhang, Huang, Zhang, Gan, Huang, and
  He}]{AttnGANFT}
Xu, T.; Zhang, P.; Huang, Q.; Zhang, H.; Gan, Z.; Huang, X.; and He, X.
  2018{\natexlab{b}}.
\newblock AttnGAN: Fine-Grained Text to Image Generation with Attentional
  Generative Adversarial Networks.
\newblock In \emph{CVPR}.

\bibitem[{Yoshikawa et~al.(2010)Yoshikawa, Riedel, Hirao, Asahara, and
  Matsumoto}]{Yoshikawa2010CoreferenceBE}
Yoshikawa, K.; Riedel, S.; Hirao, T.; Asahara, M.; and Matsumoto, Y. 2010.
\newblock Coreference based event-argument relation extraction on biomedical
  text.
\newblock \emph{Journal of Biomedical Semantics} .

\bibitem[{Yu et~al.(2017)Yu, Zhang, Wang, and Yu}]{Yu2017SeqGANSG}
Yu, L.; Zhang, W.; Wang, J.; and Yu, Y. 2017.
\newblock SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient.
\newblock In \emph{AAAI}.

\bibitem[{Zhang and LeCun(2015)}]{Zhang2015TextUF}
Zhang, X.; and LeCun, Y. 2015.
\newblock Text Understanding from Scratch.
\newblock \emph{ArXiv} abs/1502.01710.

\bibitem[{Zhen et~al.(2014)Zhen, Jianwen, Jianlin, and Zheng}]{wang_zhen}
Zhen, W.; Jianwen, Z.; Jianlin, F.; and Zheng, C. 2014.
\newblock Knowledge Graph Embedding by Translating on Hyperplanes.
\newblock In \emph{AAAI}.

\bibitem[{Zhijiang et~al.(2019)Zhijiang, Yan, Zhiyang, and Wei}]{Guo}
Zhijiang, G.; Yan, Z.; Zhiyang, T.; and Wei, L. 2019.
\newblock Densely Connected Graph Convolutional Networks for Graph-to-Sequence
  Learning.
\newblock \emph{Transactions of the Association for Computational Linguistics}
  .

\end{thebibliography}
 \bibliography{DRAW}


\onecolumn
\appendix



\end{document}
