

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{*****} \def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{Synthetic Sample Selection for Generalized Zero-Shot Learning}

\author{Shreyank N Gowda\\
University of Edinburgh\\
{\tt\small s.narayana-gowda@sms.ed.ac.uk}
}
\maketitle

\begin{abstract}
Generalized Zero-Shot Learning (GZSL) has emerged as a pivotal research domain in computer vision, owing to its capability to recognize objects that have not been seen during training. Despite the significant progress achieved by generative techniques in converting traditional GZSL to fully supervised learning, they tend to generate a large number of synthetic features that are often redundant, thereby increasing training time and decreasing accuracy. To address this issue, this paper proposes a novel approach for synthetic feature selection using reinforcement learning. In particular, we propose a transformer-based selector that is trained through proximal policy optimization (PPO) to select synthetic features based on the validation classification accuracy of the seen classes, which serves as a reward. The proposed method is model-agnostic and data-agnostic, making it applicable to both images and videos and versatile for diverse applications. Our experimental results demonstrate the superiority of our approach over existing feature-generating methods, yielding improved overall performance on multiple benchmarks.
\end{abstract}

\section{Introduction}
\label{sec:intro}

In recent years, deep learning \cite{resnet,densenet,vgg,colornet} has made remarkable strides in recognition accuracy, approaching human levels. However, the practical implementation of deep learning models is limited by the need for a significant number of labeled samples and the high cost of large-scale datasets \cite{imagenet}. It is often infeasible to collect sufficient labeled data for all classes, presenting a significant challenge for traditional supervised learning methods. To address this challenge, several approaches have been proposed, including semi-supervised learning, transfer learning, and few-shot learning. Zero-shot learning (ZSL) \cite{palatucci2009zero} is a subset of these methods, which refers to tasks where there is no training data available for unseen classes and disjoint label sets between training and test data.

ZSL is a unique form of cross-modal retrieval learning that relies on knowledge transfer from known classes to unknown classes through attribute sharing. The most common ZSL techniques use an intermediate semantic representation, such as visual features or semantic word vectors \cite{socher2013zero,frome2013devise,ye2017zero,kodirov2017semantic}, shared between the labeled auxiliary dataset and the unlabeled target dataset. The projection from the low-level feature space to the semantic space is learned from the auxiliary dataset, without adapting to the target dataset. The Generalized ZSL scenario, where both seen and unseen class samples are available at test time, is considered to be more realistic than the standard ZSL setup \cite{xian2018zero}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{teaser_sss.png}
    \caption{Comparison of feature-generating frameworks pipelines (a) standard pipeline where features that look ``real" are used to train the generator (b) proposed pipeline where the generator is trained based on the performance of the seen class classifier. }
    \label{fig:teaser}
\end{figure}

In Generalized ZSL, classifiers tend to be biased towards seen classes, leading to the misclassification of test samples from unseen classes as seen classes. To address the problem of the lack of visual data for unseen classes, researchers have proposed the use of Generative Adversarial Networks (GAN) \cite{goodfellow2020generative} to generate synthetic visual features by leveraging attribute databases. However, while GANs have helped in zero-shot learning \cite{verma2018generalized,clswgan,yu2020episode,cyclewgan}, they do not explicitly learn to represent data in a structured way that is easily interpretable by humans or other models. They also suffer from the problems of mode collapse \cite{jahaniansteerability}, class imbalance \cite{arora2017gans}, and computational expense \cite{guo2019deep} when generating high-dimensional data. But what we care about most is that a lot of synthetic samples generated are used directly for training a classifier without studying if these samples actually help the classifier learn. Instead, these samples are chosen based on ``realness". Figure~\ref{fig:teaser} shows a comparison of the standard pipeline and our proposed pipeline for feature-generating approaches.

To address the limitations of GANs in synthetic feature selection, we propose a novel reinforcement learning-based approach that automatically selects generated features that improve model performance. Specifically, we use a transformer model \cite{vaswani2017attention} for synthetic sample selection and use validation classification accuracy as the reward for RL training. We employ the proximal policy optimization (PPO) \cite{ppo} algorithm to update the model weights during training. Our proposed approach aims to pick samples that help classification and not just generate real-looking samples. We dub our synthetic sample selection method as ``\textbf{SPOT}" for \textbf{S}election using \textbf{P}roximal policy \textbf{O}p\textbf{T}imization.

Furthermore, our proposed approach is model-agnostic and data-agnostic, as we evaluate our method on multiple benchmark datasets in images and videos and various feature-generating models. Our comprehensive experiments demonstrate that our approach consistently improves model performance across different datasets and models, highlighting the effectiveness and versatility of our proposed method. By leveraging RL-based synthetic feature selection, we can more effectively generate synthetic data that captures the underlying structure of the data, improving the generalization performance of downstream models.



\section{Related Work}
\label{sec:background}

\paragraph{Zero-Shot Learning in Images}

Zero-shot learning (ZSL) is a challenging problem in computer vision, where the task is to recognize object categories without any training examples for them. Various approaches have been proposed to solve this problem in images. One of the early works \cite{lampert2009learning} in this field used attributes, such as color and shape, to describe the object categories and mapped them to a visual space. They then used a nearest-neighbor classifier to recognize unseen object categories. However, this approach suffers from the semantic gap problem, where the attributes do not always correlate well with the visual features.

To address this problem, more recent works have explored the use of deep learning techniques to learn a joint embedding space for the visual and semantic features. One such approach is proposed by Frome et al. (2013) \cite{frome2013devise}, where they used a deep neural network to learn a joint embedding space for the visual and textual features of the objects. They then used a nearest-neighbor classifier to recognize unseen object categories. Another approach is proposed by Socher et al. (2013) \cite{socher2013zero}, where they used a recursive neural network to learn a compositional representation of the textual descriptions of the object categories. 

More recently, there has been a trend towards using generative models to solve the ZSL problem. One such approach is proposed by Xian et al. \cite{clswgan}, where they used a generative adversarial network (GAN) to generate visual features for unseen object categories. They then used a joint embedding space to match the generated features with the semantic features and recognize unseen object categories. Another approach is proposed by Schonfeld et al. (2019) \cite{schonfeld2019generalized}, where they used a GAN to generate visual features conditioned on the textual descriptions of the object categories. ZeroGen~\cite{zerogen} uses pre-trained language models to synthesize a dataset for a zero-shot task and then trains a small task model on it. NereNet~\cite{nerenet} generates unseen samples by combining noise from similar seen classes with unseen class attributes using GAN. CMC-GAN~\cite{cmcnet}, performs data hallucination of unseen classes by performing semantics-guided intra-category knowledge transfer across image categories.

We add our proposed module of synthetic sample selection to multiple feature-generating frameworks and show that this leads to improved performance for all these models across all datasets.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{overview_sss.png}
    \caption{Overall pipeline of our proposed SPOT. The feature generator generates features that the selector module ranks based on the seen class classifier's performance. The selector is updated based on the performance of the classifier on the selected features. The proposed pipeline is model and data-agnostic.}
    \label{fig:overview}
\end{figure*}


\paragraph{Zero-Shot Learning in Videos}

The initial study by Rohrbach et al.\cite{rohrbach12eccv} utilized script data from cooking activities to facilitate the transfer of knowledge to unseen categories. Gan et al.\cite{gan2016learning} considered each action class as a domain and tackled the problem of identifying semantic representations as a multisource domain generalization task. To extract semantic embeddings of class labels, popular approaches employ label embeddings such as word2vec~\cite{word2vec}, which solely requires class names. Several methods have used a shared embedding space between video features and class labels~\cite{xu2016multi,xu2017transductive}, error-correcting codes \cite{qin2017zero}, pairwise relationships between classes \cite{gan2016concepts}, interclass relationships \cite{gan2015exploring}, out-of-distribution detectors~\cite{OD}, synthetic features~\cite{syn, GGM2018} and graph neural networks~\cite{gao2019know}.

Recently, it has been observed that clustering joint visual-semantic features results in better representations for zero-shot action recognition~\cite{claster}. Similar to CLASTER, ReST \cite{rest} jointly encodes video data and textual labels for zero-shot action recognition. In ReST, transformers are utilized to conduct modality-specific attention. On the other hand, JigSawNet \cite{jigsaw} models visual and textual features jointly but disassembles videos into atomic actions in an unsupervised manner and establishes group-to-group relationships between visual and semantic representations instead of the one-to-one relationships that CLASTER and ReST establish.

However, since we only evaluate on feature generating approaches, we compare directly to OD~\cite{OD}, CLSWGAN~\cite{clswgan}, GGM~\cite{GGM2018} and Bi-dir GAN~\cite{syn} and show that using our synthetic feature selection approach all methods can be improved significantly.



\paragraph{Reinforcement Learning for Data Valuation}

The quantification of data value for a specific machine learning task, known as data valuation \cite{smart,jia2019towards,wu2022davinz}, has numerous applications such as domain adaptation, corrupted sample detection, and robust learning. Various techniques have been proposed to estimate data values based on different criteria, including influence functions, Shapley values, leave-one-out errors, and data deletion. However, these methods are computationally expensive, necessitate model perturbations or retraining, and do not consider the interactions among data points. Recently, an adaptive approach to data valuation using reinforcement learning \cite{yoon2020data}, in which data values are jointly learned with the predictor model using a data value estimator that is trained using a reinforcement signal reflecting task performance. Similarly, Learn2Augment~\cite{L2A} performs data valuation of augmented samples created by combining foreground and background videos using reinforcement learning to quantify the value of an augmented sample.

Synthetic sample selection~\cite{ye2020synthetic} for medical image segmentation is an under-investigated research area that focuses on the quality control of synthetic images for data augmentation purposes. Synthetic images are not always realistic and may contain misleading features that distort data distribution when mixed with real images. As a result, the effectiveness of synthetic images in medical image recognition systems cannot be ensured when they are randomly added without quality assurance. A reinforcement learning-based synthetic sample selection approach is proposed in which synthetic images containing reliable and informative features are chosen.

However, none of the above approaches consider the extreme case of zero-shot learning. In the case of zero-shot learning, synthetic features are often biased towards seen classes, and the generated synthetic features do not represent the true distribution. Training a model to generate realistic-looking features will produce features similar to the training distribution without any guarantee on the effect on zero-shot evaluation. Therefore, we propose a data valuation method for synthetic features based on classification performance rather than their realism.



\section{Methodology}
\label{sec:method}

The overall framework of the proposed method is visually depicted in Figure~\ref{fig:overview}, which provides an illustrative overview of the various components employed. In this section, we delve deeper into the individual constituents of the model and explore in detail the novel SPOT selector that has been put forth. It is imperative to note that the proposed pipeline is model and data-agnostic, which implies that the choice of classifier model and network backbone is dependent solely on the feature-generating framework itself.

The development of the SPOT selector draws significant inspiration from the synthetic sample selector introduced in ~\cite{ye2020synthetic}. However, our approach seeks to tackle the more challenging task of zero-shot learning, where data from unseen classes is limited. Furthermore, we demonstrate that training the selector using data from seen classes can enhance the selection of better features for data from unseen classes. This approach represents a significant contribution and serves to bridge the gap between the seen and unseen class data.

\subsection{Feature Generating Network (FGN)}
As previously highlighted, it is worth noting that the proposed pipeline is entirely independent of the feature-generating approach itself. This aspect renders the framework highly versatile and adaptable to a diverse range of feature-generating models, which may be employed in place of the FGN utilized in this study.

Examples of alternate feature-generating models that could be employed include the WGAN~\cite{clswgan} and Cycle-WGAN~\cite{cyclewgan}. The utilization of such models would permit the proposed pipeline to be seamlessly integrated into a broader range of applications and extend the reach and scope of the framework. The flexibility afforded by this design decision represents a critical feature of the proposed pipeline and enables the framework to be readily adapted to a range of diverse use cases as shown with consistent improvements in multiple image and video datasets.

\subsection{Selector}

The reasoning behind selecting the particular selector is rooted in the interdependence of features among the potential images. We hypothesise that the sequence in which the features are generated is not completely autonomous, as the later additions must differentiate themselves from the earlier ones in order to ensure diversity across the entire set of augmented training data. We use a transformer-based architecture~\cite{vit} to be our selector. The input is a feature vector of a dimension dependent on the FGN used. The goal of the selector is to tell us if the generated feature vector is good for classification performance. The selector takes in a feature vector and outputs a score that tells us how good that feature vector is for classification. To do this, the selector outputs a binary action: select or not select. However, we do not have ground truth to tell us how good the generated feature is and hence optimizing the selector is not trivial.

To address the possibility of a relationship between augmented images without relying heavily on sequential assumptions, we utilize the self-attention mechanism through the implementation of the transformer~\cite{vit} model as our selector. The transformer architecture eliminates all recurrent structures, requiring feature vectors to be combined with their positional embeddings using sinusoidal functions prior to being input into the encoder layer of the transformer. The primary component of the transformer encoder is the multi-head attention block, comprised of  self-attention layers, where  denotes the number of heads. In each self-attention layer, input features are projected to three separate feature spaces - query , key , and value - by multiplying learnable weight matrices. The resulting attention map is obtained through the following process:



Each head of the multi-head attention block represents a distinct projected feature space for the input, achieved by multiplying the same input embedding with different weight matrices. These separate outputs are then concatenated to form the final attention map, which is expressed as:





Once we obtain the attention map, the context vector is then fed to the feed-forward layer as follows:


Given that the objective of the selector is to produce a binary action for every input feature vector, the decoder for the transformer model is a linear layer that functions as the policy network. Overall, the use of the transformer as the selector within our image selection framework, based on reinforcement learning, is beneficial due to its self-attention mechanism that effectively captures the interdependencies among the input feature vectors. We conducted a thorough ablation study regarding the selection of the selector and this can be seen in Section 4.2.

To optimize the selector, we turn to reinforcement learning as this is a common solution ~\cite{yoon2020data,L2A}. In particular, we use proximal policy optimization~\cite{ppo}. Details are explained in the next section.

\subsection{Proximal Policy Optimization}

As previously stated, we turn to reinforcement learning approach to update the selector model. A proficient policy gradient method is fundamental to effectively utilize reward feedback as input to the selector in the reinforcement learning process. Among various policy gradient algorithms, Proximal Policy Optimization (PPO)~\cite{ppo} has gained popularity due to its computational efficiency and satisfactory performance, surpassing previous approaches like TRPO~\cite{trpo}. Additionally, PPO alleviates the instability encountered during RL training. PPO achieves comparable performance with reduced complexity by replacing the KL convergence constraint enforced in TRPO with a clipped probability ratio between the current and previous policy within a small interval around 1. At each time step t, with  representing the advantage function, the objective function is defined as follows:





Here, . As a component of the transformer output, the learned state-value   serves as a baseline for the q-value to mitigate the variance of rewards during the training process. The probability of actions is denoted by . The q-value at time t, , is defined as a smoothed version of the maximum validation accuracy observed among the last five epochs in the classification task. As our target tasks are trained on the seen class data and needs to generalize to the unseen class data, it is crucial to obtain a robust estimation of the reward's changing pattern. To achieve this, we employ the Exponential Moving Average (EMA) algorithm to smooth the original reward curve. Thus, the final reward at time  is obtained as follows:



Drawing inspiration from the concept of importance sampling, the weight assigned to the current policy is influenced by earlier policies. The probability ratio between the previous and current policies, denoted by , is mathematically defined as:


Here,  refers to the number of synthetic samples in the candidate pool. If at any given timestep ,  then  is discarded. Else, it is added to the original training set.

\section{Experimental Analysis}
\label{sec:exp}



\subsection{Implementation Details}

Since we propose a plug-and-play component to feature-generating networks, the backbone and technical details follow the exact same implementation that the feature-generating model uses. Here, we talk about the technical details of running SPOT.

The candidate features generated by the feature-generating framework are passed into the selector network which is a 8-layer encoder having a 8-head multi-head attention block. The output is a vector  which is the action vector and a value vector  that is used together to calculate the reward for the policy gradient algorithm.

The classifier network depends on the feature-generating network being used since our proposed method is model agnostic. Similar to ~\cite{L2A,ye2020synthetic} we use the EMA-smoothed validation accuracy obtained from the last 5 epochs as the reward with =0.5 when using the classifier on the validation set. As long as this average is increasing, we continue updating our policy. The policy function  is obtained from the softmax layer of the seen-class classifier model.

 in Eq. 4 is set to 0.15 (see Ablations for empirical comparison), this helps to set an upper and lower bound at the current time step  and previous one  for the ratio of the policy function. The number of selected synthetic features are dependent on the selector and varies according to model and dataset. However, we set the learning rate to be fixed for the PPO at 2e - 04. 

\subsection{Ablation Study}

We have made a few choices with regards to the hyperparameter selection and choice of RL optimization algorithms and in this section, we show empirical reasons why the choices were made. Figure~\ref{fig:ablation} shows the performance differences when using different RL optimization algorithms to modify the selector. Similar to ~\cite{ye2020synthetic}, we also consider alternative choices such as GRU and LSTMs for the selector. In terms of RL algorithms, we compare to REINFORCE~\cite{reinforce} and TRPO~\cite{trpo}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{bar-graph.png}
    \caption{Ablation comparison of different combinations of RL algorithms with selector choices (GRU or Transformer). `G' = GRU, `Tr' = Transformer, `R' = REINFORCE, `T' = TRPO and `P' = PPO.}
    \label{fig:ablation}
\end{figure}

\subsection{Images}

\setlength{\tabcolsep}{2pt}
\begin{table}[t]
\small
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Model                        & CUB  & AWA & SUN  & FLO  \\
\hline \hline
WGAN                         & 57.3 & 68.2 & 60.8 & 67.2 \\
WGAN + \textbf{SPOT}         & \textbf{60.7} & \textbf{71.1} & \textbf{63.3} & \textbf{69.9} \\
\hline
Cycle-WGAN                   & 57.8 & 65.6 & 59.7 & 68.6 \\
Cycle-WGAN + \textbf{SPOT} & \textbf{61.1} & \textbf{69.7} & \textbf{62.5} & \textbf{70.9} \\
\hline
f-VAEGAN & 61.0 & 71.1 & 64.7 & 67.7 \\
f-VAEGAN + \textbf{SPOT} & \textbf{62.8} & \textbf{72.7} & \textbf{66.0} & \textbf{69.2} \\
\hline
CMC-GAN                      & 61.4 & 71.4 & 63.7 & 69.8 \\
CMC-GAN + \textbf{SPOT} & \textbf{62.9} & \textbf{73.1} & \textbf{65.1} & \textbf{71.9} \\
\hline
\end{tabular}
\end{center}
\caption{Results on zero-shot image classification using recent feature-generating frameworks.
}
\label{tbl:results:zsl_image}
\end{table}

\begin{table*}[htb]
\small
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|l|}{Model} & \multicolumn{3}{c|}{CUB}                                                                                  & \multicolumn{3}{c|}{AWA1}                                                                                 & \multicolumn{3}{c|}{SUN}                                                                                  & \multicolumn{3}{c|}{FLO}                                                                                  \\ \cline{2-13} 
\multicolumn{1}{|l|}{}                       & \multicolumn{1}{c|}{S}            & \multicolumn{1}{c|}{U}            & \multicolumn{1}{c|}{H}            & \multicolumn{1}{c|}{S}            & \multicolumn{1}{c|}{U}            & \multicolumn{1}{c|}{H}            & \multicolumn{1}{c|}{S}            & \multicolumn{1}{c|}{U}            & \multicolumn{1}{c|}{H}            & \multicolumn{1}{c|}{S}            & \multicolumn{1}{c|}{U}            & \multicolumn{1}{c|}{H}            \\ \hline
WGAN                                         & 43.7                              & 57.7                              & 49.7                              & 57.9                              & 61.4                              & 59.6                              & 42.6                              & 36.6                              & 39.4                              & 59.0                              & 73.8                              & 65.6                              \\
WGAN+\textbf{SPOT}                                    & \textbf{44.1}                     & \textbf{60.9}                     & \textbf{51.1}                     & \textbf{58.6}                     & \textbf{64.9}                     & \textbf{61.6}                     & \textbf{42.8}                     & \textbf{39.1}                     & \textbf{40.9}                     & \textbf{59.3}                     & \textbf{75.9}                     & \textbf{66.6}                     \\
\hline
Cycle-WGAN                                   & 46.0                              & 60.3                              & 52.2                              & 56.4                              & 63.5                              & 59.7                              & \textbf{48.3}                     & 33.1                              & 39.2                              & 59.1                              & 71.1                              & 64.5                              \\
Cycle-WGAN+\textbf{SPOT}                              & \textbf{46.5}                     & \textbf{62.9}                     & \textbf{53.5}                     & \textbf{56.9}                     & \textbf{66.1}                     & \textbf{61.1}                     & 48.1                              & \textbf{36.2}                     & \textbf{41.3}                     & \textbf{59.4}                     & \textbf{74.4}                     & \textbf{66.1}                     \\
\hline
f-VAEGAN                                     & 48.4                              & 60.1                              & 53.6                              & 57.6                              & 70.6                              & 63.5                              & 45.1                              & 38.0                              & 41.3                              & 56.8                              & 74.9                              & 64.6                              \\
f-VAEGAN+\textbf{SPOT}                               & \textbf{48.8}                     & \textbf{62.8}                     & \textbf{54.9}                     & \textbf{57.9}                     & \textbf{73.3}                     & \textbf{64.7}                     & \textbf{45.5}                     & \textbf{41.1}                     & \textbf{43.2}                     & \textbf{57.0}                     & \textbf{77.2}                     & \textbf{65.6}                     \\
\hline
CMC-GAN                                      & 52.6                              & 65.1                              & 58.2                              & 63.2                              & 70.6                              & 66.7                              & 48.2                              & 40.8                              & 44.2                              & 64.5                              & 80.2                              & 71.5                              \\
CMC-GAN+\textbf{SPOT}                                 & \textbf{53.1}                     & \textbf{66.7}                     & \textbf{59.1}                     & \textbf{63.3}                     & \textbf{73.8}                     & \textbf{68.1}                     & \textbf{48.9}                     & \textbf{44.1}                     & \textbf{46.4}                     & \textbf{64.6}                     & \textbf{82.8}                     & \textbf{72.6}                     \\
\hline
NereNET                                      & 51.0                              & 56.5                              & 53.6                              & -                                 & -                                 & -                                 & 45.7                              & 38.1                              & 41.6                              & -                                 & -                                 & -                                 \\
NereNET+\textbf{SPOT}                                 & \textbf{51.3}                     & \textbf{58.4}                     & \textbf{54.6}                     & -                                 & -                                 & -                                 & \textbf{45.9}                     & \textbf{40.4}                     & \textbf{43.0}                     & -                                 & -                                 & -                                 \\
\hline
FREE                                         & \textbf{55.7} & 59.9    & 57.7         & 62.9         & 69.4         & 66.0      & 47.4        & 37.2          & 41.7          & 67.4       & 84.5         & 75.0     \\
FREE+\textbf{SPOT}                                    & 55.5         & \textbf{62.2} & \textbf{58.6} & \textbf{63.1} & \textbf{72.1} & \textbf{67.3} & \textbf{47.8} & \textbf{39.9} & \textbf{43.5} & \textbf{67.8} & \textbf{86.3} & \textbf{75.9} \\
\hline
DAA                                          & 66.1          & 65.5 & 65.8        & 64.3         & 76.6        & 69.9         & 47.8          & 38.7        & 42.8         & -                                 & -                                 & -                                 \\
DAA+\textbf{SPOT}                                     & \textbf{66.3} & \textbf{67.7} & \textbf{67.0} & \textbf{64.6} & \textbf{77.9} & \textbf{70.6} & \textbf{48.1} & \textbf{40.3} & \textbf{43.8} & -                                 & -                                 & -                               \\ 
\hline
\end{tabular}
\end{center}
\caption{Results on generalized zero-shot image classification on 4 challenging benchmarks.
}
\end{table*}

\subsubsection{Datasets and Evaluation Protocol}


Our method is evaluated on four challenging benchmark datasets, namely AWA \cite{lampert2013attribute}, CUB (Caltech UCSD Birds 200) \cite{cub}, SUN (SUN Attribute) \cite{sun}, and FLO \cite{nilsback2008automated}. CUB and SUN are fine-grained datasets, while AWA and FLO are coarse-grained datasets.  We adopt the same seen/unseen splits and class embeddings to ensure consistency with previous work as in \cite{xian2017zero}. AWA1 contains 30,475 instances across 50 categories. CUB comprises 11,788 images of 200 bird classes (150/50 for seen/unseen classes) with 312 attributes. SUN contains 14,340 images from 717 scene classes (645/72 for seen/unseen classes) with 102 attributes. FLO consists of 8,189 images from 102 flower classes with an 82/20 class split for seen and unseen classes, respectively. These datasets are widely used in the literature, enabling a direct comparison of our results with those of previous studies.



\subsubsection{Zero-Shot Learning}

We compare strictly with recent state-of-the-art feature-generating approaches and as such compare to WGAN~\cite{clswgan}, Cycle-WGAN~\cite{cyclewgan}, f-VAEGAN~\cite{fvaegan}, NereNet~\cite{nerenet} and CMC-GAN~\cite{cmcnet}. Table~\ref{tbl:results:zsl_image} shows the results. We see consistent gains with increase of up to 4.1\% when we add the proposed SPOT to any of the models.



\subsubsection{Generalized Zero-Shot Learning}

We perform a much more extensive comparison in the generalized setting as this is where most feature generating frameworks perform experiments. We compare against WGAN~\cite{clswgan}, Cycle-WGAN~\cite{cyclewgan}, f-VAEGAN~\cite{fvaegan}, CMC-GAN~\cite{cmcnet}, NereNet~\cite{nerenet}, FREE~\cite{free} and DAA~\cite{daa}. We see consistent improvements on the unseen class accuracies as this is where selected features make a difference. We see gains of up to 3.3\% on the unseen class accuracies. As a result, there is consistent improvement on the harmonic mean of the seen and unseen class accuracies as well.



\subsection{Videos}

\subsubsection{Datasets and Evaluation Protocol}

For videos, we use the widely adopted Olympic Sports \cite{olympics}, HMDB-51 \cite{hmdb}, and UCF-101 \cite{ucf101} datasets to evaluate our method for zero-shot action recognition and compare it against recent state-of-the-art feature generating models \cite{OD, clswgan, finegrain}. The aforementioned datasets comprise 783, 6766, and 13320 videos and are associated with 16, 51, and 101 classes, respectively. To enable comparison with existing works \cite{OD, clswgan, finegrain, GGM2018, syn}, we adopt the widely used 50/50 splits proposed by Xu et al. \cite{xu2017transductive}, where half of the classes are considered as seen and the other half as unseen. We report the average accuracy and standard deviation over 10 independent runs, following previous approaches.

Moreover, we extend our evaluation to include TruZe~\cite{truze}, which was recently introduced to address the issue of overlapping classes between the pre-training dataset (Kinetics~\cite{i3d}) and the unseen classes in zero-shot settings. The TruZe split acknowledges the presence of such overlapping classes, which contradicts the fundamental assumption that the unseen classes have not been previously seen.


\subsubsection{Zero-Shot Learning}

Table~\ref{tbl:results:zsl} shows the effect of using our proposed SPOT selector to enhance the performance of state-of-the-art feature-generating frameworks on the zero-shot setting. We compare with the most recent best-performing methods, which include the Bi-Dir GAN~\cite{syn}, GGM~\cite{GGM2018}, OD~\cite{OD}, WGAN~\cite{clswgan} and FFG~\cite{finegrain} (fine-grained feature generation framework).

\setlength{\tabcolsep}{2pt}
\begin{table}[t]
\small
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Method & Olympics & HMDB51 & UCF101\\
\hline\hline
Bi-Dir GAN \cite{syn} & 53.2  10.5 & 21.3  3.2 &	24.7  3.7\\
Bi-Dir GAN \cite{syn} + \textbf{SPOT} & \textbf{56.6  10.1} & \textbf{25.1  3.4} &	\textbf{27.7  3.5}\\
\hline

GGM \cite{GGM2018} & 57.9  14.1 & 20.7  3.1 & 24.5  2.9 \\
GGM \cite{GGM2018} + \textbf{SPOT} & \textbf{62.4  12.4} & \textbf{25.1  2.8} & \textbf{27.4  2.5} \\
\hline
OD \cite{OD} & 65.9  8.1 & 30.2  2.7 & 38.3  3.0\\
OD \cite{OD} + \textbf{SPOT} & \textbf{68.7  7.5} & \textbf{34.4  2.2} & \textbf{40.9  2.6}\\
\hline
WGAN \cite{clswgan} & 64.7  7.5 & 29.1  3.8 & 37.5  3.1 \\
WGAN \cite{clswgan} + \textbf{SPOT} & \textbf{68.1  7.1} & \textbf{33.8  2.4} & \textbf{40.6  2.4} \\
\hline

FFG \cite{finegrain} & - & 32.4  2.3 & 27.6  2.4 \\
FFG \cite{finegrain} + \textbf{SPOT} & - & \textbf{35.9  2.5} & \textbf{30.9  2.2} \\
\hline
\end{tabular}
\end{center}
\caption{Results on zero-shot action recognition on the Olympics, HMDB51 and UCF101 datasets. }
\label{tbl:results:zsl}
\end{table}

We observe that the proposed method consistently outperforms all approaches across all datasets by gains of up to 4.5\%.



\subsubsection{Generalized Zero-Shot Learning}

We evaluate SPOT on the generalized setting where at test time both seen and unseen class samples are used. Table~\ref{tbl:results:gzsl} shows the results, with the harmonic mean of the seen and unseen class accuracies. The proposed SPOT selector consistently improves all approaches across all datasets by gains of up to 4.2\%. 

\setlength{\tabcolsep}{2pt}
\begin{table}
\small
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Method & Olympics & HMDB51 & UCF101\\
\hline\hline
Bi-Dir GAN \cite{syn} & 44.2  11.2 & 7.5  2.4 &	22.7  2.5\\
Bi-Dir GAN \cite{syn} + \textbf{SPOT} & \textbf{48.4  10.3} & \textbf{10.9  3.1} &	\textbf{25.1  3.8}\\
\hline

GGM \cite{GGM2018} & 52.4  12.2  & 20.1  2.1 & 23.7  1.2 \\
GGM \cite{GGM2018} + \textbf{SPOT} & \textbf{55.3  11.9} & \textbf{23.4  2.3} & \textbf{27.1  3.1} \\
\hline

WGAN \cite{clswgan} & 59.9  5.3 & 32.7  3.4 & 44.4  3.0 \\
WGAN \cite{clswgan} + \textbf{SPOT} & \textbf{62.4  5.5} & \textbf{34.4  2.9} & \textbf{46.2  2.6} \\
\hline

OD\cite{OD}  & 66.2  6.3 & 36.1  2.2 & 49.4  2.4\\
OD \cite{OD} + \textbf{SPOT} & \textbf{69.1  6.5} & \textbf{38.2  2.5} & \textbf{51.8  2.5}\\
\hline

FFG \cite{finegrain} & - & 37.4  1.9 & 40.4  2.2 \\
FFG \cite{finegrain} + \textbf{SPOT} & - & \textbf{39.8  1.4} & \textbf{42.8  1.7} \\

\hline
\end{tabular}
\end{center}
\caption{Results on generalized zero-shot setting. Reported results are the harmonic mean of the seen and unseen class accuracies. 
}
\label{tbl:results:gzsl}
\end{table}

\subsection{Results on TruZe}

We also evaluate on the stricter TruZe~\cite{truze} split that ensures no overlap between the pre-trained model and test classes. Results are shown in Table~\ref{tbl:truze}. We only evaluate on OD and WGAN as these are the two feature-generating approaches that have results reported on the TruZe split. Again, we see that using SPOT for selection consistently improves the performance of the feature-generating framework.

\begin{table}[t]
\small
\begin{center}
\begin{tabular}{| *{5}{c|} }
\hline
Method & \multicolumn{2}{c|}{UCF101 } & \multicolumn{2}{c|}{HMDB51}\\
  & ZSL & GZSL  & ZSL & GZSL \\
 \hline\hline
WGAN  & 22.5 & 36.3  & 21.1 & 31.8 \\
WGAN + \textbf{SPOT} & \textbf{25.3} & \textbf{39.1} & \textbf{23.8} & \textbf{33.3} \\
\hline
OD & 22.9 & 42.4  & 21.7 & 35.5 \\
OD + \textbf{SPOT}  & \textbf{25.5} & \textbf{44.1} & \textbf{24.0} & \textbf{37.1} \\
\hline
\end{tabular}
\end{center}
\caption{Results on TruZe. We report the mean class accuracy for zero-shot; for generalized zero-shot, we report the harmonic mean of seen and unseen class accuracies. }
\label{tbl:truze}
\end{table}

\section{Conclusion}
\label{sec:conclusion}

In conclusion, although generative techniques have made significant progress in transforming traditional GZSL to fully supervised learning, they often generate redundant synthetic features, which can lead to reduced accuracy. To overcome this limitation, we have proposed an approach for synthetic feature selection using reinforcement learning, which involves training a transformer-based selector using proximal policy optimization (PPO) to select synthetic features based on the validation classification accuracy of seen classes as the reward. Our proposed method is model-agnostic and data-agnostic and hence is suitable for images and videos. The experimental results of our approach demonstrate its superiority over existing feature-generating methods, with improved overall performance observed across multiple benchmarks. Overall, our approach represents a significant contribution towards addressing the issue of synthetic feature redundancy in GZSL, and we believe that it has the potential to be widely applied in real-world scenarios.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{PaperForReview}
}

\end{document}
