


























In this section, we first give an overview of FC for a brief understanding of our method.
Then we present our motivation and key modules for ultra-large-scale datasets training.
After that, we show the theoretical/empirical analysis over these modules.
Finally we demonstrate the training details for better reproduction.

\begin{figure*}[htp]
\centering
\includegraphics[width=0.8\textwidth]{pipeline_2.png}
\caption{The pipeline of FC. We use instance and id data loader to generate mixed batches (I  III, II  IV), which are later fed into G-Net and P-Net respectively. The features from G-Net will update DCP in the manner of LRU, and features from P-Net will be used to compute loss together with DCP.}


\label{pipeline}
\end{figure*}


\subsection{Overview of FC}
The problem we tackle is to accelerate the training speed and reduce the hardware costs of ultra-large-scale face datasets (face identities > 10M) without obvious degradation of performance. 
To this end, we propose FC framework for ultra-large-scale face datasets training.
As shown in Figure \ref{pipeline}, given ultra-large-scale face datasets, we utilize instance-based loader to generate an instance batch as data loader usually does. Meanwhile, identity-based loader selects two images randomly from the same identity to form the paired identities batch.
Subsequently, we mix up the images from instance and pair identity batches as shown in Figure \ref{pipeline} and feed them into G-Net and P-Net.
Inspired by MoCo \cite{he2020momentum}, G-Net has the same structure as P-Net and inherits parameters from P-Net in a moving average manner.
G-Net and P-Net are used to generate identities' centers and extract face features for face recognition, respectively.
Then, we introduce DCP as a substitute for the FC layer.
DCP is randomly initialized and updated by the features from G-Net at each iteration.
The update strategy of DCP follows the rule: using the current features to replace the most outdated part of features in DCP. For positive samples, we use the common cross entropy loss. For negative samples, we minimize the cosine similarities between negative samples and DCP. The whole FC is optimized by cross entropy loss and cosine similarities simultaneously.









\subsection{Motivation}
Before digging into FC, we provide some motivations by rethinking the loss function cooperated with FC layer.
For convenience, we consider the Softmax as follows:

where  is batchsize and  stands for the number of whole face identities.
For each iteration of the training process, the update of the classifier  is performed as the following equations:

Obviously, all the classifiers  will be updated in each iteration, which means each classifier has the same chance to be optimized.
The goal of face recognition is to distinct persons from different identities with the mechanism where the features from the same identity are pulled together and features belonging to different identities are pushed away.
As the main problem of training with ultra-large-scale dataset is the explosive size of FC layer,
We can consider the whole FC as a set of classifiers.
In order to reduce the computation cost, it is intuitive for us to optimize fixed ratio of the classifiers in each iteration during the training process.
Specifically, we utilize a vector as follows to represent whether a given classifier is in optimization queue.

where  is a constant stands for the length of the optimization queue,  denotes the classifier  is (not in)/(in) optimization queue.
We draw the corresponding objective for this setting.

The classifiers update on basis of the following equations: 

Formally, equation \ref{update_part_W} is similar to equation \ref{update_W}, the selection mechanism for vector  will influence the update process of the classifier directly.
We should design feasible selection mechanism for better optimization of classifiers under the constraint that only partial classifiers will be updated at each iteration.
However, this straightforward method still suffer the heavy pressure from storing the whole set of classifiers.
As a matter of fact, in our novel framework, we only offer limited space to store a fixed ratio of classifier/features dynamically.
\subsection{Identity-Based and Instance-Based Loaders}\label{dual_loader}
In this subsection, we introduce the details of our dual data loader.
For convenience, we denote the batchsize as \textbf{M}.
Practically, we utilize the instance-based loader to sample \textbf{M} images from a given face dataset randomly to get the instance batch.
In the meanwhile, the identity-based loader is applied to provide identity batch by selecting \text{M} identities randomly without replacement from the whole identities and sample two images for each identity.
We divide the instance batch into two parts, with \textbf{M/2} images for each part.
For paired identity batch, we split it by face identity to form two parts with same set of face identities.
We mix up the four parts to get  (as illustrated in Figure \ref{pipeline}), where
 represents the union operation for sets. 

\textbf{Why Dual data loaders?}
As aforementioned, we design dual data loader to improve the update efficiency of DCP parameters.
To better understand our design, we analyze the different influences between identity-based and instance-based loaders as follows.
Let  denote batch size,  be the total number of identities of the given the dataset,  () as the minimum (maximum) number of images for one person in the dataset,  be the average number of images per identity.
Here the shape of DCP mentioned in the main paper is .
 is the magnitude of DCP,  is the capacity for each placeholder in DCP,  represents feature dimension.
The total images of given dataset can be denoted as .
We evaluate the update speed by estimating the minimum of epochs for  given face identity to update .
\begin{itemize}
\item If we only use instance-based loader, the update speed of identities' centers . So only using instance-based loader may lead to following problems.
1. If the number of identities is severely imbalanced, the update speed of the identities' centers that have rare number of images is too slow.
2. If we sample  images that belong to  different identities, the DCP may have no positive samples for this iteration.
In this case, cross entropy, which is crucial for classification, cannot be calculated.

\item If we only use identity-based loader, we can obtain the average fastest update speed () of each identity.
However, identity-based loader re-sample identities that have rare number of images too many times, so it needs to use about  times more iterations than instance-based loader to sample all images from the dataset.
Further, the sample probabilities for each instance of identities with rich intra-class images are too low, the identity-based loader can not sample plenty of intra-class images during the training phase.

\item Using the dual data loaer can inherit the benefits from instance-based and identity-based loaders.
First, dual data loader provides appropriate ratios between positive and negative images, which is very important for DCP.
Second, dual data loader keeps high update efficiency (speed) of identities' centers and various intra-class images.
\end{itemize}



\textbf{Feature Extraction} We take  and  as input to Probe and Gallery Nets respectively to extract the face features and generate the identities' centers.
The process can be formulated as follows:

where the probe and gallery net are abbreviated as  and  with parameters denoted as  respectively. 
The symbol  is set to split features whose identities belong to DCP (subsection \ref{DCP_sec}) from those not belong to DCP.
 represent the features extracted by the Gallery Net.
For each batch, we denote number of identities in DCP as  and number of identities not in DCP as .

\subsection{Dynamic Class Pool}\label{DCP_sec}
In this subsection, we introduce the details of the Dynamic Class Pool (DCP). 
Inspired by sliding window \cite{lampert2008beyond} in object detection task, we can utilize a dynamic identity group that slides the whole face identities by iterations.

We called this sliding identity group as DCP, which can be regarded as a substitute for the FC layer.
Firstly, we define a tensor  with size of  which is initialized with Gaussian distribution,
where  is the capacity or the number of face identities the DCP can hold,
 represents the number of features that belong to the same identity (we set the default as ).
We store  in DCP and update the most outdated features of DCP using the  in each iteration.
The updating rule is similar to least recently used (LRU)\footnote{https://www.interviewcake.com/concept/java/lru-cache} policy which can be formulated as,

For the current batch, with the update of the DCP, we obtain pseudo feature center for each identity in DCP, including the identities contained in .
As claimed in equation \ref{feature_P_G}, features from P-Net can be divided into two types compared to DCP.
One is , the other is .
For , we can calculate its logits by the following equation,

where  denotes the inner product operation,  represents the logits of .
Therefore, we can formulate the Cross-Entropy loss as follows:

where  is the j-th classifier,  is the identity of .
For features  whose IDs are not in DCP, we add a constraint to minimize the cosine similarity between  and , which can be formulated as,

where  is the operation of calculating the cosine similarity,  represents the average operation along the axis of  in DCP. 
The total loss is  =  + .


\subsection{Empirical Analysis}
\textbf{DCP}
As shown in equation \ref{part_loss} and \ref{ce_loss}, the cross entropy loss we utilize for DCP is similar to the loss for FC formally.
With special setting of vector  in equation \ref{V_setting}, we can represent  in the form of equation \ref{part_loss}. 
For further verification of the effect of this mechanism on the training with DCP, we provide some empirical analysis.






\begin{algorithm}[htp]
\caption{Update Mechanism of DCP}
\label{FIFO_kai}
\KwIn{

\noindent DCP:  initialized with Gaussian distribution.

\noindent Index for the identity batch: .

\noindent Batch Size: .
}
\For{}{
utilize the G-Net to extract features from -th batch as the pseudo feature centers denoted as ;\\
\textbf{if} :\\
\quad store  sequentially in those unoccupied position in DCP.\\
\textbf{else}:\\
update DCP as shown in Equation \ref{DCP}}

\end{algorithm}


As mentioned in subsection \ref{dual_loader} and equation \ref{DCP}, the identities in DCP are updated in an LRU mechanism as shown in Algorithm \ref{FIFO_kai}.
As identity-based loader goes through the dataset in terms of identities,
partial components() of vector  can be determined by shuffling the whole face identities and taking the corresponding -th part of it, where .
When we use identity-based loader,
then by the setting of  and property of LRU rules, each classifier/pseudo feature center can be updated at least  times.
This means that every classifier can have the similar chance to be optimized in our settings.
DCP may have the following benefits: 1) The size of DCP is independent from magnitude of face identities, which can be far smaller than FC. Therefore the computational cost is greatly reduced; 2) The hardware especially storage occupancy of DCP is also smaller than FC and the communication cost can be reduced dramatically.
These benefits are the reasons why we call our method as Faster Face Classification.


\subsection{Experimental Details}
We train our FC on a single server with 8 Tesla V100 32G GPUs.
We utilize ResNet100, ResNet50 and MobileFaceNet as our backbones to evaluate the efficiency of FC.
The learning rate is initialized as 0.1 with SGD optimizer and divided by 10 at 10, 14, 17 epochs.
The training is terminated at 20 epochs.
The length (number of ID) of DCP is defaulted as 10\% of total face identities.
The batch size is 512 \textit{i.e.}, 256 images from identity-based loader and 256 images from instance-based loader. 




%
