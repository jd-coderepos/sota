



\documentclass[fleqn,usenatbib]{mnras}



\usepackage[T1]{fontenc}

\DeclareRobustCommand{\VAN}[3]{#2}
\let\VANthebibliography\thebibliography
\def\thebibliography{\DeclareRobustCommand{\VAN}[3]{##3}\VANthebibliography}



\usepackage{graphicx}	\usepackage{amsmath}	\usepackage{amssymb}	\usepackage{hyperref}
\def\UrlBreaks{\do\/\do-}
\usepackage{siunitx}
\usepackage{newtxtext,newtxmath}
\usepackage{dsfont}
\usepackage{subcaption}
\usepackage[labelfont=bf, labelsep=period]{caption}
\usepackage{booktabs}
\captionsetup{compatibility=false}









\title[Score-based modelling]{Realistic galaxy image simulation via score-based generative models}

\author[M. J. Smith et al.]
{\parbox{\textwidth}{
Michael J. Smith\thanks{mike@mjjsmith.com},
James E. Geach,
Ryan A. Jackson,
Nikhil Arora,
Connor Stone \\ and
St\'ephane Courteau}\\\\
Centre of Data Innovation Research, Department of Physics, Astronomy, and Mathematics, University of Hertfordshire, Hatfield, AL10 9AB\\
Centre of Astrophysics Research, Department of Physics, Astronomy, and Mathematics, University of Hertfordshire, Hatfield, AL10 9AB\\
Department of Astronomy and Yonsei University Observatory, Yonsei University, Seoul 03722, Republic of Korea\\
Department of Physics, Engineering Physics, and Astronomy, Queen’s University, Kingston, ON K7L 3N6, Canada
}



\pubyear{2022}

\begin{document}

\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

\begin{abstract}
    We show that a Denoising Diffusion Probabilistic Model (DDPM), a class of
    score-based generative model, can be used to produce realistic mock
    images that mimic observations of galaxies. Our method is tested with Dark
    Energy Spectroscopic Instrument (DESI) {\it grz} imaging of galaxies from the
    Photometry and Rotation curve OBservations from Extragalactic Surveys
    (PROBES) sample and galaxies selected from the Sloan Digital Sky Survey.
    Subjectively, the generated galaxies are highly realistic when compared
    with samples from the real dataset. We quantify the similarity by
    borrowing from the deep generative learning literature, using the
    `Fr\'{e}chet Inception Distance' to test for subjective and morphological
    similarity. We also introduce the `Synthetic Galaxy Distance' metric
    to compare the emergent physical properties (such as total magnitude,
    colour and half light radius) of a ground truth parent and synthesised
    child dataset. We argue that the DDPM approach produces sharper and more
    realistic images than other generative methods such as adversarial networks
    (with the downside of more costly inference), and could be used to produce
    large samples of synthetic observations tailored to a specific imaging
    survey. We demonstrate two potential uses of the DDPM: (1) accurate
    in-painting of occluded data, such as satellite trails, and (2) domain
    transfer, where new input images can be processed to mimic the properties
    of the DDPM training set.  Here we `DESI-fy' cartoon images as a proof of
    concept for domain transfer.  Finally, we suggest potential applications
    for score-based approaches that could motivate further research on this
    topic within the astronomical community.
\end{abstract}

\begin{keywords}
    methods: data analysis -- methods: statistical
\end{keywords}





\section{Introduction}

Synthetic data will play a pivotal role as we journey further into astronomy's
epoch of big data, especially for large extragalactic surveys
\citep{cite_sdss,cite_ska,cite_euclid,cite_lsst}.  It will be required to train
machine learning methods, to interpret observations, and to test theoretical
frameworks. Indeed, one form of synthetic data comes from theoretical models.
For example, in the field of galaxy formation and evolution, simulations using
semi-analytical approaches have been successful in reproducing many of the bulk
observable and emergent properties of galaxies over a significant fraction of
cosmic time
\citep[e.g.][]{cite_somerville1999,cite_cole2000,cite_bower2006,cite_croton2006}.
Semi-analytical models (SAMs) employ approximations derived from more detailed
numerical simulations and empirical calibrations from data to model galaxy
formation and evolution. So it is possible to generate, for example, a `mock'
catalogue of galaxies with predicted optical photometry \citep{cite_lagos2019}.
Hydrodynamical models of galaxy formation track the evolution of baryons and
dark matter within representative volumes
\citep[e.g.][]{cite_dubois2014,cite_vogelsberger2014,cite_eagle,cite_khandai2015,cite_kaviraj2017},
and when pushed to high spatial resolution, can predict galaxy morphologies on
physical scales commensurate with the angular scales achievable with current
observational facilities. When radiative transfer schemes are applied for the
propagation of (for example) starlight through the volume, realistic synthetic
observations can be produced, to be compared with nature
\citep[e.g.][]{cite_camps2016,cite_trayford2017,cite_lovell2021}.

\begin{figure*}
    \includegraphics{./figures/ddpm.pdf}
    \caption{It is easy (and achievable without learnt parameters) to add noise
    to an image, but more difficult to remove it. DDPMs attempt to learn an
    iterative removal process through an appropriate neural network.
    .}
    \label{fig_ddpm}
\end{figure*}
   
To properly mimic real astronomical data, with all its wonderful
idiosyncrasies, requires a full and detailed understanding of the telescope
response, instrumental properties, and observing conditions, not to mention the
nuances of any data reduction procedure. These non-trivial steps are
typically unique to a given set of observations. There is a short-cut however:
armed with enough examples of observations from a given survey, it should be
possible to derive a data-driven approach to mimic not only the content of
interest -- astronomical signal -- but also the properties of the data themselves.
Deep generative models enable precisely that.

Great attention has been given to applications of deep
generative learning to problems in astronomy lately. 
Generative Adversarial Networks
\citep[GAN;][]{cite_goodfellow2014} have been used for deconvolution
\citep{cite_schawinski2017}, synthetic galaxy generation
\citep{cite_ravanbakhsh2016,cite_fussell2019}, dark matter simulation
\citep{cite_mustafa2017,cite_tamosiunas2020}, and deep field imagery generation
\citep{cite_smith2019}. 
Variational Auto-Encoders
\citep[VAE;][]{cite_kingma2014} have been used to simulate galaxy observations
\citep{cite_ravanbakhsh2016,cite_spindler2021,cite_lanusse2021}, as have
flow-based models \citep{cite_rezende2015,cite_bretonniere2021}. In this paper,
we show that it is possible to simulate realistic galaxy imagery with a Denoising
Diffusion Probablisitic Model (DDPM).  

DDPMs were introduced by \citet{cite_sohldickstein2015} and were first shown to
produce high quality synthetic samples by \citet{cite_ho2020}.
They belong to a family of generative deep learning models that employ
denoising `score matching' via annealed Langevin dynamic sampling
\citep{cite_song2020,cite_ho2020,cite_ajm2020,cite_song2021}.  This family of
score-based generative models (SBGMs) can generate imagery of a quality and
diversity surpassing state of the art GAN models, a startling result
considering the historic disparity in interest and development between the two
techniques \citep{cite_song2021,cite_nichol2021,cite_dhariwal2021}.  

SBGMs have already been used to super-resolve images
\citep{cite_kadkhodaie2020,cite_saharia2021}, translate between image domains
\citep{cite_sasaki2021}, separate superimposed images \citep{cite_jayaram2020},
and in-paint information \citep{cite_kadkhodaie2020,cite_song2021}. At the time
of writing, there is only one example of score-based modelling in the
astronomy literature \citep{cite_remy2020}. This is despite some obvious uses
in astronomical data pipelines. For example: an implementation like
\citet{cite_sasaki2021} could be used for survey-to-survey photometry
translation similarly to \citet{cite_buncher2021}; the source image separation
model described in \citet{cite_jayaram2020} could be applied as an
astronomical object deblender \citep[for
example:][]{cite_stark2018,cite_reiman2019,cite_arcelin2021}; and information
inpainting could be used to remove nuisance objects from observations
\citep{cite_kadkhodaie2020,cite_song2021}.

This paper is organised as follows. Section~\ref{sec_ddpm} introduces the
DDPM formulation used in this paper. In Section~\ref{sec_application} and
Section~\ref{sec_results}, we show that DDPMs are capable of generating diverse
synthetic galaxy observations that are both statistically and qualitatively
indistinguishable from observations found in the training set. We also
demonstrate that DDPMs can in-paint occluded information in an observation,
such as satellite trails\footnote{A
growing problem due to the rapidly increasing population of satellites,
exacerbated by mega-constellations \citep{cite_kocifaj2021}.}, and show that we
can inject realism into entirely unrealistic cartoon imagery. A discussion of
our results and suggestions for future research are presented in
Section~\ref{sec_discussion}.

\section{Denoising diffusion probabilistic models} \label{sec_ddpm}

Denoising Diffusion Probabilistic Models (DDPMs) define a diffusion process
that projects a complex image domain space onto a simple domain space. In the
original formulation, this diffusion process is fixed to a predefined Markov
chain that adds a small amount of Gaussian noise with each step.
Figure~\ref{fig_ddpm} illustrates that this `simple domain space' can be noise
sampled from a Gaussian distribution: .

\subsection{Forward process}

We define a Markov chain to slowly add Gaussian noise to our data: 

The amount of noise added per step is controlled with a variance schedule
, such that

This process is applied iteratively to the input image, . 
If we define the above equation to only depend on , we
can immediately calculate an image representation  for any 
\citep{cite_ho2020}. Defining  and
:

where  and
 is a combination of Gaussians. Substituting this
expression into Equation~\ref{eqn_forwardbeta} removes the 
dependency and yields



\begin{figure*}
    \includegraphics[width=\linewidth]{./figures/shuffled_letters__.pdf}
    \caption{A montage of generated galaxies designed to mimic the PROBES data
    set, interspersed with real examples from the dataset itself. The images
    have been shuffled and the synthetic/real data split is 50/50. All images
    are {\it grz} RGB composites with identical scaling (we have performed a
    99.5\% percentile clip to better show low surface brightness features). A key
    stating which galaxies are real and which are generated is provided at the
    end of this manuscript. More generated galaxies can be found at
    \url{http://mjjsmith.com/thisisnotagalaxy}.}
    \label{FIG_GALAXIES}
\end{figure*}

\subsection{Reverse process}

DDPMs attempt to reverse the forward process by applying a Markov chain with
learnt Gaussian transitions. In our case these transitions are learnt via an
appropriate neural network, :

While  can be learnt\footnote{See for
example \citet{cite_nichol2021}.}, we follow \citet{cite_ho2020} and fix it to
an iteration-dependent constant , where
\mbox{}.

By recognising that DDPMs are a restricted class of Hierarchical VAE, 
we see that we can train  by optimising the evidence lower bound
\citep[ELBO, introduced in][]{cite_kingma2014} that can be written as a
summation over the Kullback-Leibler divergences at each iteration
step\footnote{
    See Appendix~B in \citet{cite_sohldickstein2015} and
    Appendix~A in \citet{cite_ho2020} for the full derivation.  
}:

In the \citet{cite_ho2020} formulation, the first term in
Equation~\ref{eqn_elbo} is a constant during training and the final term is
modelled as an independent discrete decoder. This leaves the middle summation.
Each term in that summation can be written as

where  is the neural network's estimation of the forward process
posterior mean . In practice it would be preferable to predict the noise
addition in each iteration step (), as  has a distribution that by
definition is centred about 0, with a well defined variance.  To this end we
can define  as

and by combining Equations~\ref{eqn_lossmu} and \ref{eqn_mu} we get


\citet{cite_ho2020} empirically found that a simplified version of the loss
described in Equation~\ref{eqn_complexloss} results in better sample quality.
We therefore use a simplified version of Equation~\ref{eqn_complexloss} as our
loss, and optimise to predict the noise required to reverse a forward process
iteration step:

where .

By recognising that , we see that Equation~\ref{eqn_loss} is equivalent to
denoising score matching over  noise levels \citep{cite_vincent2011}.  This
connection establishes a link between DDPMs and other SBGMs \citep[such
as][]{cite_song2019,cite_song2020,cite_ajm2020}.  

Here we use a modified U-Net as 
\citep{cite_ronneberger2015,cite_salimans2017}, and train via the Adam
optimiser \citep{cite_kingma2015}.  The U-Net comprises of three downsample
blocks, a bottleneck block, and three upsample blocks. Each downsample block
comprises of two residual blocks \citep{cite_srivastava2015,cite_he2015}, a
self-attention layer \citep{cite_bahdanau2014,cite_cheng2016}, and a strided
convolution layer. The bottleneck comprises of a self-attention layer
sandwiched by two residual blocks. Each upsample block comprises of two
residual blocks, a self-attention layer, and a transposed convolution layer. As
in a standard U-Net, residual connections link the downsample and upsample
blocks. To provide information about the current iteration step, an embedding
representing the reverse process iteration step is periodically injected into
the U-Net via a summation.  Mish activation is used throughout
\citep{cite_mish}. The full implementation is released under the AGPLv3 licence
and is available at \url{https://github.com/Smith42/astroddpm}.

To run inference for the reverse process, we progressively remove the predicted
noise  from our image. The predicted noise is weighted
according to our variance schedule:


If we take , we can use  to generate entirely novel data that are
similar, but not identical to, those found in the training set. We can also use
 to perform image domain translation, and inpainting.
Section~\ref{sec_results} describes these applications in further detail.

\section{Simulating DESI galaxy images} \label{sec_application}

We train our models on minimally processed native resolution (~pixels at 0.262\,pixel) Dark Energy Spectroscopic Instrument
\citep[DESI;][]{cite_desi} Legacy Survey Data Release 9 galaxy imagery. The
, , and  band images have an average atmospheric seeing of approximately 1.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./figures/nearest.pdf}
    \caption{Pixelspace nearest neighbour to generated PROBES galaxies. The leftmost
    column shows a galaxy generated with the model , the other
    columns show that galaxy's closest training set neighbours in pixelspace.
    Moving along a row takes us further away from
    the simulated galaxy in pixelspace.}
    \label{fig_closest}
\end{figure}

\subsection{Data sample, preparation, and training}

We train two models on two different datasets for 750,000 global steps each
across three NVIDIA Tesla V100 GPUs, corresponding to  hours wall
time per model. We fill all available VRAM and set the batch size to 56.
The two datasets are described below.

\begin{enumerate}
    \item We train on the Photometry and Rotation curve OBservations  from
        Extragalactic Surveys (PROBES) galaxy dataset as imaged by the Dark
        Energy Spectroscopic Instrument \citep[DESI;][]{cite_desi} Legacy
        Survey Data Release 9. The PROBES dataset is described in
        \citet{cite_stone2019} and \citet{cite_stone2021a}. It contains 1962
        late-type galaxies with no large neighbours or other obscuring features
        (such as bright stars). Most of the objects are well resolved,
        exhibiting spiral arms, bars, and other features characteristic of
        late-type systems. The model trained on this dataset produces galaxies
        that obviously exhibit internal structure. We refer to this as the
        `PROBES' dataset.

    \item We also train on a dataset of 306\,006 galaxies whose coordinates are
        taken from Sloan Digital Sky Survey \citep[SDSS;][]{cite_sdss} Data
        Release 7 \citep{cite_sdssdr7} and a modified catalogue from
        \cite{cite_wilman2010}. This volume complete sample has an
        -band absolute magnitude limit of  and a redshift limit
        of . See \citet{cite_arora2019} for details.
        This catalogue covers a wide range of environments from clusters to
        groups and field systems. As in the PROBES dataset, the galaxy images
        are taken from DESI \citep{cite_desi}. We use this dataset and the
        corresponding trained model to compare population level galaxy
        statistics (Sec.~\ref{sec_sgd}). For brevity we refer to this as the
        `SDSS' dataset.
\end{enumerate}

All images are cropped about the target galaxy to a shape of \mbox{} pixels. The only destructive pre-processing performed is a upper
and lower percentile clipping, with the percentiles calculated across the
entire dataset. This clipping removes any `hot' or `cold' pixels. To calculate
the upper flux truncation point we evaluate the 99.9th percentile fluxes for
each galaxy across the full dataset. To separate the long tail from
the bulk of the data, we fit a two-cluster {\it k}-means \citep{cite_lloyd1982}.
The two-cluster {\it k}-means returns a boundary at approximately
~analogue-to-digital~unit~(ADU) for the SDSS dataset, and ~ADU for the
PROBES dataset, and so we set these values as our upper truncation points and
normalisation constants. The lower flux truncation point is set as the minimum
off-source pixel-wise root mean square across the entire dataset. We found this
value to be very close to zero across all bands in both the SDSS and PROBES
datasets, and therefore set the lower flux truncation as ~ADU. We apply a
min-max normalisation to the images with the following equation:

with ~ADU being the upper flux truncation for the SDSS dataset, and ~ADU for the PROBES dataset. We reverse this normalisation when post-processing
inferrals from the model.

\section{Results} \label{sec_results}

Figure~\ref{FIG_GALAXIES} shows a random selection of generated galaxies,
alongside a random selection of real galaxies. The images are shuffled and we
can see that the simulated and real galaxies are subjectively
indistinguishable, at least to the authors (we of course invite the reader to
make their own assessment of fidelity, referring to the answer key given at the
end of this paper). Figure~\ref{fig_closest} presents a random selection of
generated galaxies' nearest neighbours in pixelspace. Since the pixelspace
search does not return identical galaxies, we conclude that the DDPM is not
simply regurgitating imagery, and is indeed generating novel data.  We found a
systematic offset in the simulated pixel fluxes and corrected for it in
post-processing.  To estimate the offset, we calculated the median pixel value
in each of the 10,000 mock and 10,000 real galaxy observations. Each set is
sorted and the medians paired according to their place in the sorted sets.
Finally we fit a linear function to the resulting 10,000 median flux pairs. The
gradient of the fit was used as a scaling factor for the simulated galaxy
images.  We found the multiplier to be  in ,  in , and 
in  for our DESI observations.  Unfortunately, the exact cause of these
offsets could not be determined.  We propose that this discrepancy is due to a
fundamental property of the neural network and its interaction with sparse
imagery such as our galaxy images.

\subsection{Quantifying similarity} \label{sec_sgd}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./figures/histogram_.pdf}
    \caption{Histogram comparison between galaxies generated by the SDSS DDPM,
    and galaxies contained in the SDSS training set. The half
    light radius histogram follows a lognormal distribution, as do the
    magnitude and colour histograms in flux space. Therefore, we can calculate
    Cohen's  effect size for each histogram pair. As a rule of thumb, if  the effect size is considered `small' and a sign of negligible
    difference \citep{cite_cohen1988}.}
    \label{fig_hists_qqs}
\end{figure}

To quantify the similarity of the visual and morphological characteristics of
our galaxies, we borrow from the deep generative learning literature and
calculate the Fr\'echet Inception Distance
\citep[FID;][]{cite_heusel2017,cite_seitzer2020}. The FID is the distance
\citep{cite_dowson1982} between Gaussians fitted to two Inception-v3
\citep{cite_szegedy2016} penultimate layer feature representations. The
penultimate layer nodes are deep in the network and mimic a human's perception
when viewing images. Therefore, if the Gaussians are similar (and the
corresponding FID is small), the images will be visually similar too.

We run FID on 10,000 random samples and present the results in
Table~\ref{tab_results}.  While we cannot yet contextualise our
FID within the literature, we provide the value for future comparison.  
Figure~\ref{FIG_GALAXIES} is presented for a basic visual and morphological
comparison; we cannot discern between the synthesised and real galaxies, which
suggests that the visual and morphological characteristics of our datasets are
well replicated.

To demonstrate that we capture emergent, measurable properties of the galaxies,
we directly compare size and flux distributions. Fluxes are measured via a
summation within a fixed aperture with a diameter of 12~pixels (3),
and we use the half light radius as a simple measure of size. To summarise the
distance between the `ground truth' photometry training set properties and the
properties of the simulated set we use the Wasserstein-1 distance\footnote{
    Since we are dealing with large datasets a Kolmogorov-Smirnov
    (KS) test is not appropriate as it becomes overpowered with a very large sample size.  We
    instead use the related Wasserstein-1 distance to provide an absolute value
    that represents the difference between our distribution pairs, and also
    calculate Cohen's  effect size as a direct intuitive substitution for
    the -values that would otherwise result from KS tests
    (Figure~\ref{fig_hists_qqs}).
}:

where  and  are the respective cumulative distribution functions of  and .
 
Following Equation~\ref{eqn_wasserstein}, we propose a `synthetic galaxy
distance' metric that captures the difference between emergent properties of a
synthetic and reference galaxy photometry dataset:

where  is the half light radius, and , , and  are aperture
magnitudes in specific bands and  and  denote different datasets. The SGD
returns a single number, where a lower value denotes a closer match between 
and .  When combined with the FID for visual and morphological similarity,
a good overview of the similarity between two large galaxy photometry datasets
is obtained. Figure~\ref{fig_hists_qqs} shows the results for the individual
tests and the SGD summary is in Table~\ref{tab_results}.  We run SGD on 10,000
random samples.

\begin{table}
    \centering
    \caption{Wasserstein-1 distance between emergent property distributions.
     is the DDPM described in this paper. `SDSS' is
    a comparison between two different randomly selected sets of 10,000
    galaxies from the training set. We provide the `SDSS' Wasserstein-1 distances
    as a baseline `perfect' inference.}
    \setlength\tabcolsep{3pt}
    \begin{tabular}{l c c c c c c c c}
        \toprule
        &  &  &  &  &  &  & SGD & FID \\
        \cmidrule(r){1-7} \cmidrule(l){8-9}
         & 0.013 & 0.012 & 0.023 & 0.055 & 0.015 & 0.010 & 0.127 & 19 \\
        SDSS & 0.008 & 0.010 & 0.014 & 0.018 & 0.006 & 0.004 & 0.060 & 0.95 \\
        \bottomrule
    \end{tabular}
    \label{tab_results}
\end{table}

\begin{figure}
    \includegraphics[width=\linewidth,angle=0]{./figures/sat_.pdf}
    \caption{Inpainting of galaxy observations defaced by satellite trails. The
    first column shows the original ({\it r}-band)  PROBES galaxy,
    . The second column shows the defaced galaxy,
    . The third column shows a random guided draw from the
    model .}
    \label{fig_sats}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./figures/sat_res.pdf}
    \caption{A pixelwise comparison between the ground truth images' occluded
    fluxes, and the recovered images' in Figure~\ref{fig_sats}, expressed as a
    fractional error. The left panel shows the residual for all pixels, and the
    right panel restricts the analysis to pixels exceeding 3 times the
    background r.m.s. (~ADU).  We find good agreement
    between the predicted pixel fluxes and the ground truth fluxes, with
    virtually all significant pixels within 10\% of their true values.}
    \label{fig_sats2}
\end{figure}

While we cannot yet contextualise our SGD within the literature, we provide the
value for future comparison. We also present Figure~\ref{fig_hists_qqs} to
otherwise show that our model captures physical properties of the galaxies. We
calculate Cohen's  effect size for each histogram pair, and in all cases
find  indicating a `small' or negligible effect
\citep{cite_cohen1988}. 
Cohen's  effect size is defined here as

where  is the mean of the fitted distribution, and  is the standard
deviation. The subscripts  and  denote different datasets.

\subsection{Satellite trail removal via guided diffusion}

The DDPM can be used to remove simulated galaxy satellite trails from images.
We simulate satellite trails by superimposing a bright linear strip onto a real
image. The strips have a random direction, brightness, width, and periodicity.
In this demonstration we present monochrome {\it r}-band images from the PROBES
dataset.

To perform guided diffusion, we run the reverse process on the occluded part of
the galaxy, in this case the satellite trails. The other image pixels are drawn
directly from the forward process, and so are not updated. The occluded pixels 
are updated with guidance information from the surrounding pixels. 
As Figures~\ref{fig_sats} and \ref{fig_sats2} show, this process
retrieves excellent representations of the original galaxies, essentially
in-painting the missing data with high accuracy. Figure~\ref{fig_sats2} shows
that significant (3) pixels that were occluded by a trail have
recovered fluxes within 10\% of their true values. Since satellite trails are
not present in the training set, a guided draw from the learnt model is
effective at `interpolating' the occluded pixels. A similar approach would work
for other unwanted artefacts, such as glints and ghosts, provided they do not
appear frequently in the training set.

\subsection{Domain transfer} \label{sec_domaintransfer}

The DDPM can also be used to make another input image resemble a DESI Legacy
Survey observation. To perform this domain transfer, we first run the forward
process for  iterations. We then take the noisy image, and run the reverse
process.  This results in a DESI Legacy Survey-like observation that shares
high level features with the input image. Figure~\ref{fig_transfer}
demonstrates this technique on cartoons, setting .  If  is set at a
high value, the DDPM produces an image that more closely resembles one that
might be found in the training set. However, fine detail in the conditioning
image is lost as it is erased by the forward noise addition process.  The
cartoon input is transformed into an image resembling it, but with the
properties of a DESI survey image. Once the cartoon images have been
`DESI-fied', we can search for the nearest neighbour in pixelspace in the real
dataset, and this is shown as a final column in Figure~\ref{fig_transfer}.
Thus, this approach paves the way for pixel-based searching of large survey
imaging databases. For example, one could potentially sketch a particular
morphology or configuration (e.g. an Einstein ring), apply the model tailored
to that survey and then recover the best match in the real data. One could also
apply this technique to inject realism into simulated galaxies, such as those
predicted by hydrodynamical simulations. 

Since the first release of this paper \citet{cite_preechakul2021} and
\citet{cite_saharia2021} have both explored image-to-image translation 
with a score-based model. \citet{cite_preechakul2021} showed that DDPMs 
can produce semantically meaningful embeddings, given an appropriate 
architecture. 
In their paper, they demonstrate that their autoencoding DDPM can 
interpolate along the embedding space and age and de-age images of faces. 
In astronomy, one can imagine using a `survey' embedding to interpolate 
between surveys.

\citet{cite_saharia2021} took a different approach and explicitly trained 
their model to reverse the forward process of an ill-posed inverse problem.   
For an ill-posed inverse problem such as noise addition, one can define 
the forward process in a classical way, and use a DDPM in the inverse 
process to retrieve the uncorrupted image.  
\citet{cite_saharia2021} did this and showed that a DDPM
can colourise greyscale images, and remove JPEG compression artefacts.  

For more difficult problems that do not have a well defined forward 
process, we can use a model similar to that introduced in \citet{cite_sasaki2021} 
to translate between two different image domains. We intend to explore astronomy
related image-to-image translation applications more deeply in follow up
work.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./figures/more_realistic_cartoon2.pdf}
    \caption{Cartoon images are made to look like DESI observations via
    the model . 
    The first column shows the input image, the middle columns show
    random draws from the PROBES model, and the final column shows
    the pixelspace nearest neighbour to the generated images.}
    \label{fig_transfer}
\end{figure}

\begin{figure*}     
  \includegraphics[keepaspectratio=true,width=\textwidth]{./figures/apod.jpg}
\caption{A sample of DDPM-generated APOD imagery: AI-APODs. More can be found
    at \url{http://mjjsmith.com/thisisnotanapod} and you can follow a Twitter
    bot \url{https://twitter.com/ThisIsNotAnApod}. A version of this figure has
    been featured on NASA's APOD
    \url{https://apod.nasa.gov/apod/ap211109.html}.}
    \label{fig_apod}
\end{figure*}

\subsection{A fun aside: mock Astronomy Picture of the Day}

As a fun aside, we have trained a DDPM on images from NASA's Astronomy
Picture of the Day (APOD) archive. The dataset comprises 11,428 RGB JPEG
images resized to a  shape. We trained this model for 900,000
global steps on a single V100 GPU. This allows us to generate new APODs that do
not actually exist. Figure~\ref{fig_apod} shows a curated sample of `AI-APODs'
generated from this model. We leave it to the reader to critically assess their
merits, but some common themes are apparent: images resembling nebulae,
galaxies, landscapes, moons, and aurorae are present. Random AI-APODs generated
from the model can be found at \url{http://mjjsmith.com/thisisnotanapod}, and a
Twitter bot will post images at \url{https://twitter.com/ThisIsNotAnApod}.

\section{Conclusions} \label{sec_discussion}

We show that score-based generative modelling is a viable method for synthetic
galaxy image generation, and that this approach preserves emergent properties
such as galaxy size and total flux over different photometric bands, in addition
to producing realistic morphologies. We achieve this fidelity without
explicit encoding of physics or instrumental effects, a great advantage
when simulating physically ill-defined objects. This is a completely
data-driven approach to synthetic data generation.

There are downsides. Naturally, SBGMs require significant computational
resources to train. However, unlike most other generative deep learning methods
SBGMs also require significant resources to infer data. Since they need to
diffuse the data for  cycles\footnote{In this work, }, it takes
 times longer to produce a batch of synthetic data compared to an equivalent
GAN, VAE, or other single shot generative model. However, there may be routes
to reduce the inference time for SBGMs, with promising results already
\citep{cite_ajm2021,cite_song2021}.

SBGMs have clear astronomical applications, from object deblending
\citep{cite_jayaram2020} to survey-to-survey translation
\citep{cite_sasaki2021} to occluded object in-painting
\citep{cite_kadkhodaie2020,cite_song2021} to super-resolving imagery
\citep{cite_saharia2021}. Unlike GANs, SBGMs do not suffer from mode collapse
and are trivial to train.  SBGMs produce imagery that has a
diversity and fidelity that rivals state of the art GAN models
\citep{cite_song2021,cite_nichol2021,cite_dhariwal2021}. Unlike VAEs,
SBGMs do not constrain information to a fixed bottleneck vector and
thus do not suffer from blurring, and instead produce sharp, realistic imagery
\citep{cite_spindler2021}. For all these reasons, we believe that SBGMs 
are ripe for exploitation by the astronomical community, and we hope 
this paper motivates further work in this topic. 

\section*{Data and code availability}
  
The full PyTorch \citep{cite_pytorch} implementation of the model presented
here is available at \url{https://github.com/Smith42/astroddpm}, and the code
to calculate the Synthetic Galaxy Distance (SGD) can be accessed at
\url{https://github.com/Smith42/synthetic-galaxy-distance}. To calculate the
Fr\'echet Inception Distance (FID), we used
\url{https://github.com/mseitzer/pytorch-fid}.  

\section*{Carbon emissions}

The training of deep learning models requires considerable energy, contributing
to carbon emissions. The energy used to train AstroDDPM to completion is
estimated to be \SI{450}{\kilo\watt\hour}, corresponding to
\SI{105}{\kilo\gram.CO2e} according to the Machine Learning Emissions
Calculator described in \citet{cite_lacoste2019}.  To counteract further
emission from redundant retraining, we follow the recommendations of
\citet{cite_strubell2019} and make available the fully trained models, as well
as the code to run them. Also, we will make available trained models for any
improvements that we make to AstroDDPM in the future.

\section*{Answer key for Figure~\ref{FIG_GALAXIES}}

00 Real, 01 Real, 02 Real, 03 Mock, 04 Real, 05 Real, 06 Real, 07 Mock, 08 Mock, 09 Mock, 10 Real, 11 Real, 12 Mock, 13 Mock, 14 Mock, 15 Mock, 16 Real, 17 Real, 18 Mock, 19 Real, 20 Real, 21 Real, 22 Real, 23 Mock, 24 Mock, 25 Mock, 26 Mock, 27 Mock, 28 Mock, 29 Real, 30 Mock, 31 Real, 32 Real, 33 Real, 34 Real, 35 Mock, 36 Real, 37 Mock, 38 Mock, 39 Mock, 40 Mock, 41 Mock, 42 Mock, 43 Mock, 44 Real, 45 Real, 46 Mock, 47 Real, 48 Real, 49 Real, 50 Mock, 51 Real, 52 Mock, 53 Real, 54 Real, 55 Real, 56 Real, 57 Real, 58 Mock, 59 Real, 60 Mock, 61 Mock, 62 Mock, 63 Mock

\section*{Acknowledgements}

JEG is supported by the Royal Society. This research made use of the University
of Hertfordshire's High Performance Computing facility
(\url{http://uhhpc.herts.ac.uk/}).  The galaxy icon in Figure~\ref{fig_ddpm},
and the cartoons in Figure~\ref{fig_transfer} are by Agata Kuczmi\'nska and are
available under the \mbox{CC-BY-4.0} licence at
\url{https://goodstuffnononsense.com/hand-drawn-icons/space-icons/}. We are
grateful to the Natural Sciences and Engineering Research Council of Canada,
the Ontario Government, and Queen’s University for critical support through
various scholarships and grants. This work was supported in part by the Yonsei
University Research Fund (Yonsei Frontier Lab. Young Researcher Supporting
Program) of 2021. RAJ acknowledges support from the Korean National Research
Foundation (NRF-2020R1A2C3003769). We thank the MNRAS reviewer for helpful
comments and suggestions.

The Legacy Surveys consist of three individual and complementary projects: the
Dark Energy Camera Legacy Survey (DECaLS; Proposal ID \#2014B-0404; PIs: David
Schlegel and Arjun Dey), the Beijing-Arizona Sky Survey (BASS; NOAO Prop. ID
\#2015A-0801; PIs: Zhou Xu and Xiaohui Fan), and the Mayall z-band Legacy Survey
(MzLS; Prop. ID \#2016A-0453; PI: Arjun Dey). DECaLS, BASS and MzLS together
include data obtained, respectively, at the Blanco telescope, Cerro Tololo
Inter-American Observatory, NSF’s NOIRLab; the Bok telescope, Steward
Observatory, University of Arizona; and the Mayall telescope, Kitt Peak
National Observatory, NOIRLab. The Legacy Surveys project is honoured to be
permitted to conduct astronomical research on Iolkam Du’ag (Kitt Peak), a
mountain with particular significance to the Tohono O’odham Nation.

NOIRLab is operated by the Association of Universities for Research in
Astronomy (AURA) under a cooperative agreement with the National Science
Foundation.

This project used data obtained with the Dark Energy Camera (DECam), which was
constructed by the Dark Energy Survey (DES) collaboration. Funding for the DES
Projects has been provided by the U.S. Department of Energy, the U.S. National
Science Foundation, the Ministry of Science and Education of Spain, the Science
and Technology Facilities Council of the United Kingdom, the Higher Education
Funding Council for England, the National Center for Supercomputing
Applications at the University of Illinois at Urbana-Champaign, the Kavli
Institute of Cosmological Physics at the University of Chicago, Center for
Cosmology and Astro-Particle Physics at the Ohio State University, the Mitchell
Institute for Fundamental Physics and Astronomy at Texas A\&M University,
Financiadora de Estudos e Projetos, Fundacao Carlos Chagas Filho de Amparo,
Financiadora de Estudos e Projetos, Fundacao Carlos Chagas Filho de Amparo a
Pesquisa do Estado do Rio de Janeiro, Conselho Nacional de Desenvolvimento
Cientifico e Tecnologico and the Ministerio da Ciencia, Tecnologia e Inovacao,
the Deutsche Forschungsgemeinschaft and the Collaborating Institutions in the
Dark Energy Survey. The Collaborating Institutions are Argonne National
Laboratory, the University of California at Santa Cruz, the University of
Cambridge, Centro de Investigaciones Energeticas, Medioambientales y
Tecnologicas-Madrid, the University of Chicago, University College London, the
DES-Brazil Consortium, the University of Edinburgh, the Eidgenossische
Technische Hochschule (ETH) Zurich, Fermi National Accelerator Laboratory, the
University of Illinois at Urbana-Champaign, the Institut de Ciencies de l’Espai
(IEEC/CSIC), the Institut de Fisica d’Altes Energies, Lawrence Berkeley
National Laboratory, the Ludwig Maximilians Universitat Munchen and the
associated Excellence Cluster Universe, the University of Michigan, NSF’s
NOIRLab, the University of Nottingham, the Ohio State University, the
University of Pennsylvania, the University of Portsmouth, SLAC National
Accelerator Laboratory, Stanford University, the University of Sussex, and
Texas A\&M University.

BASS is a key project of the Telescope Access Program (TAP), which has been
funded by the National Astronomical Observatories of China, the Chinese Academy
of Sciences (the Strategic Priority Research Program “The Emergence of
Cosmological Structures” Grant \# XDB09000000), and the Special Fund for
Astronomy from the Ministry of Finance. The BASS is also supported by the
External Cooperation Program of Chinese Academy of Sciences (Grant \#
114A11KYSB20160057), and Chinese National Natural Science Foundation (Grant \#
11433005).

The Legacy Survey team makes use of data products from the Near-Earth Object
Wide-field Infrared Survey Explorer (NEOWISE), which is a project of the Jet
Propulsion Laboratory/California Institute of Technology. NEOWISE is funded by
the National Aeronautics and Space Administration.

The Legacy Surveys imaging of the DESI footprint is supported by the Director,
Office of Science, Office of High Energy Physics of the U.S. Department of
Energy under Contract No. DE-AC02-05CH1123, by the National Energy Research
Scientific Computing Center, a DOE Office of Science User Facility under the
same contract; and by the U.S. National Science Foundation, Division of
Astronomical Sciences under Contract No. AST-0950945 to NOAO.





\bibliographystyle{mnras}
\bibliography{mnras_template} \bsp	

\label{lastpage}
\end{document}
