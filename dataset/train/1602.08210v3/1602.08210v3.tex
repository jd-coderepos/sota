\onecolumn
\title{Supplementary  Materials: Architectural Complexity Measures of Recurrent Neural Networks}


\section{Proofs}
\label{sec:proofs}

To show theorem \ref{thm:main-main}, we first consider the most general case in which  is defined (Theorem \ref{thm:main}). Then we discuss the mild assumptions under which we can reduce to the original limit (Proposition \ref{prop:assump}). Additionally, we introduce some notations that will be used throughout the proof. If
 is a node in the unfolded graph, it has a corresponding node in the folded graph, which is denoted by .

\begin{thm}
\label{thm:main}
  Given an RNN cyclic graph and its unfolded representation , we denote  the set of directed
  cycles in . For , denote  the length of  and  the sum of
   along . Write .\footnote{ is not defined when there does not exist a path from time  to time . We simply omit undefined cases when we consider the limsup. In a more rigorous sense, it is the limsup of a subsequence of , where  is defined.} we have :
  \begin{itemize}
    \item  The quantity  is periodic, in the sense that .
    
    \item Let , then
    
  \end{itemize}
\end{thm}

\begin{proof}
  The first statement is easy to prove. Because of the periodicity of the graph,
  any path from time step \  to  corresponds to an isomorphic
  path from time step  to . Passing to limit, and we can
  deduce the first statement.
  
   Now we prove the second statement. Write .
  First we prove that .
  Let  be a node such that if we
  denote  the image of  on the
  cyclic graph, we have . Consider the
  subsequence  of
  . From the definition of  and the fact that
   is a directed circle, we have , by considering the path
  on  corresponding to following  
  -times. So we have
  
  Next we prove . It
  suffices to prove that, for any , there exists ,
  such that for any path  with , we have . We denote 
  as the image of  on the cyclic graph.  is a walk with
  repeated nodes and edges. Also, we assume there are in total  nodes
  in cyclic graph .
  
  We first decompose  into a path and a set of directed
  cycles. More precisely, there is a path  and a sequence of
  directed cycles  on
   such that:
  \begin{itemize}
    \item The starting and end nodes of  is the same as .
    (If  starts and ends at the same node, take  as empty.)
    
    \item The catenation of the sequences of directed edges  is a
    permutation of the sequence of edges of .
  \end{itemize}
  The existence of such a decomposition can be proved iteratively by removing
  directed cycles from . Namely, if  is not a paths, there
  must be some directed cycles  on . Removing  from
  , we can get a new walk . Inductively apply this removal,
  we will finally get a (possibly empty) path and a sequence of directed
  cycles. For a directed path or loop , we write  the
  distance between the ending node and starting node when travel through
   once. We have
  
  where  is all the edges of
  .  denotes the module of : .
  
  So we have:
  
  \ For convenience, we denote  to be the length of
  path  and directed cycles . Obviously we have:
  
  And also, we have
  
  So we have:
  
  In which we have for all  :
  
  So we have:
  
  in which  and  are constants depending only on the RNN
  .
  
  Finally we have:
  
  take , we can prove the fact that .
  
  \ 
\end{proof}

\begin{proposition}
\label{prop:assump}
  Given an RNN and its two graph representations  and
  , if  such that
    achieves the maximum in Eq.(\ref{eqn:recdepth2}) and 
  the corresponding path of  in  visits nodes at every time step,
  then we have
  
\end{proposition}

\begin{proof}
  We only need to prove, in such a graph, for all  we have
  
  Because it is obvious that
  
  Namely, it suffice to prove, for all , for all , there is an , such that when , we
  have . On the other hand,
  for , if we assume , then according to condition  we
  have
  
  We can see that if we set , the inequality we wanted to prove.
  
  \ 
\end{proof}

We now prove Proposition \ref{prop:sup} and Theorem \ref{thm:feed} as follows.


\begin{proposition}
  Given an RNN with recurrent depth , we denote
  
  The supremum  exists and we have the following least upper bound:
  
\end{proposition}

\begin{proof}
  We first prove that . Write . It is easy to verify
   is periodic, so it suffices to prove for each , . Hence it suffices to prove
  
  From the definition, we have
  
  So we have
  
  From the proof of Theorem \ref{thm:main}, \ there exists two constants  and
   depending only on the RNN , such that
  
  So we have
  
  Also, we have , so for any ,
  
  
\end{proof}

\begin{thm}
  Given an RNN and its two graph representations  and
  , we denote  the set of directed path
  that starts at an input node and ends at an output node in .
  For , denote  the length and
   the sum of  along . Then we have:
  
\end{thm}

\begin{proof}
Let  be a path in  from an input node  to an output node , where  and . We denote 
  as the image of  on the cyclic graph. 
From the proof of Theorem \ref{thm:main}, for each  in , we can decompose it into a path  and a sequence of directed cycles  on
   satisfying those properties listed in Theorem \ref{thm:main}.
We denote  to be the length of
  path  and directed cycles . We know  for all  by definition. Thus,

Note that . Therefore,

for all time step  and all integer . The above inequality suggests that in order to take the supremum over all paths in , it suffices to take the maximum over a directed path in . On the other hand, the equality can be achieved simply by choosing the corresponding path of  in . The desired conclusion then follows immediately.

\end{proof}

Lastly, we show Theorem \ref{thm:rsc}.

\begin{thm}
\label{thm:main2}
  Given an RNN cyclic graph and its unfolded representation , we denote  the set of directed
  cycles in . For , denote  the length of  and  the sum of
   along . Write . We have :
  \begin{itemize}
    \item  The quantity  is periodic, in the sense that .
    
    \item Let , then
    
  \end{itemize}
\end{thm}
\begin{proof}
  The proof is essentially the same as the proof of the first theorem. So we
  omit it here. 
\end{proof}

\begin{proposition}
\label{prop:assump2}
  Given an RNN and its two graph representations  and
  , if  such that
    achieves the minimum in Eq.(\ref{eqn:main2}) and  the
  corresponding path of  in  visits nodes at every time step,
  then we have
  
\end{proposition}
\begin{proof}
  The proof is essentially the same as the proof of the Proposition \ref{prop:assump}. So we
  omit it here. 
\end{proof}

\newpage



\newpage
\section{Experiment Details}
\subsection{RNNs with }
In this section we explain the functional dependency among nodes in RNNs with  in detail.

The transition function for each node is the  function. The output of a node  is a vector . To compute the output for a node, we simply take all incoming nodes as input, and sum over their affine transformations and then apply the  function (we omit the bias term for simplicity).

where  represents a real matrix.

\begin{figure}[htp]
\center
\includegraphics[width=5cm]{./figures/lstm_ex.pdf}
\vspace{-10pt}
\caption{``Bottom-up'' architecture ().}
\label{fig:bu_ex}
\end{figure}
As a more concrete example, consider the ``bottom-up'' architecture in Figure \ref{fig:bu_ex}, with which we did the experiment described in Section \ref{sec:depth_nontrivial}. To compute the output of node , 


\subsection{LSTMs}
\label{sec:LSTM}
In this section we explain the Multidimensional LSTM (introduced by \cite{Graves2007}) which we use for experiments with LSTMs.

The output of a node  of the LSTM is a 2-tuple (,), consisting of a cell memory state  and a hidden state . The transition function  is applied to each node indistinguishably. We describe the computation of  below in a sequential manner (we omit the bias term for simplicity). 


Note that the Multidimensional LSTM includes the usual definition of LSTM as a special case, where the extra forget gates are 0 (i.e., bias term set to -) and extra weight matrices are 0. We again consider the architecture  in Fig. \ref{fig:bu_ex}. We first compute the block input, the input gate and the output gate by summing over all affine transformed outputs of , and then apply the activation function. For example, to compute the input gate, we have

Next, we compute one forget gate for each pair of . The way of computing a forget gate is the same as computing the other gates. For example, the forget gate in charge of the connection of  is computed as,

Then, the cell state is simply the sum of all element-wise products of the input gate with the block output and forget gates with the incoming nodes' cell memory states,

Lastly, the hidden state is computed as usual,





\subsection{Recurrent Depth is Non-trivial}
\label{sec:exp_dr_nontrivial}
The validation curves of the 4 different connecting
architectures , ,  and  on text8
dataset for both RNN-small and LSTM-small are shown below:
\begin{figure}[htp]
\center
\includegraphics[width=350pt]{./figures/butd_appendix.pdf}
\vspace{-10pt}
\caption{Validation curves for , ,  and  on test8 dataset.
Left: results for RNN-small. Right: results for LSTM-small.}
\label{fig:butd_ex}
\end{figure}

\subsection{Full Comparisons on Depths}
\label{sec:full_com}
Figure \ref{fig:3x3lm_ex}
shows all the validation curves for the 9 architectures on text8 dataset, with
their  and  respectively.
We initialize hidden-to-hidden
matrices from uniform distribution.


\begin{figure}[htp]
\center
\includegraphics[width=\textwidth]{./figures/3x3_ml_appendix.pdf}
\vspace{-10pt}
\caption{Validation curves of 9 architectures with feedforward depth 
and recurrent depth  on test8 dataset.
For each figure in the first row, we fix  and draw  curves with different .
For each figure in the second row, we fix  and draw  curves with different .
}
\label{fig:3x3lm_ex}
\end{figure}



Also, to see if increasing feedforward depth/ recurrent depth helps
for long term dependency problems, 
we evaluate these 9 architectures on sequential MNIST task, with roughly the
same number of parameters(~8K, where the first architecture with  and  has hidden size of 90.). 
Hidden-to-hidden matrices
are initialized from uniform distribution.


\begin{figure}[h]
\center
\includegraphics[width=\textwidth]{./figures/3x3_mnist_appendix.pdf}
\vspace{-10pt}
\caption{Test accuracies of 9 architectures with feedforward depth 
and recurrent depth  on sequential MNIST. For each figure,
we fix  and draw  curves with different .
}
\label{fig:3xxmnist_ex}
\end{figure}



Figure \ref{fig:3xxmnist_ex}
clearly show that, 
as the feedforward depth
increases, the model performance stays roughly the same.
In addition, note that increasing recurrent depth might even result in performance decrease. 
This is possibly because that larger recurrent depth amplifies the gradient
vanishing/exploding problems, which is detrimental on long term
dependency tasks.


\newpage
\subsection{Recurrent Skip Coefficients}

The test curves for all the experiments are shown in Figure \ref{fig:mnist_ex}. 
In Figure \ref{fig:mnist_ex}, we observed that
obtaining good performance on MNIST requires larger  than for pMNIST.
We hypothesize that this is because, for the sequential MNIST dataset, 
each training example contains many consecutive zero-valued subsequences, each of
length  to . Thus within those subsequences, the input-output gradient flow
could tend to vanish. However, when the recurrent skip coefficient is large enough to cover those
zero-valued subsequences, the model starts to perform better.
With MNIST, even though the random permuted order seems harder to learn, the permutation on the other hand
blends zeros and ones to form more uniform sequences, and this may explain why training is easier, less hampered
by by the long sequences of zeros.

\subsection{Recurrent Skip Coefficients vs. Skip Connections}
Test curves for all the experiments are shown in Figure \ref{fig:verify_ex}.
Observe that in most cases,
the test accuracy of (3) is worse than (2) in the beginning
while beating (2) in the middle of the training.
This is possibly because in the first several time steps, it is easier
for (2) to pass information to the output thanks to the skip connections,
while only after multiples of  time steps, (3) starts to show its
advantage with recurrent skip connections\footnote{It will be more clear if one checks the length
of the shortest path from an node at time  to to a node at time  in both architectures.}.
The shorter paths in (2) make its gradient flow more easily in the beginning,
but in the long run, (3) seems to be more superior, because of its more prominent skipping effect over time. 

\begin{figure}[htp]
\center
\includegraphics[width=350pt]{./figures/mnist_appendix.pdf}
\vspace{-10pt}
\caption{Test curves on MNIST/MNIST, with  and . The numbers in the legend
denote the recurrent skip coefficient  of each architecture.}
\label{fig:mnist_ex}
\end{figure}




\begin{figure}[htp]
\center
\includegraphics[width=350pt]{./figures/mnist_verify_appendix.pdf}
\vspace{-10pt}
\caption{Test curves on MNIST/MNIST for architecture (1), (2), (3) and (4), with .
The recurrent skip coefficient  of each architecture is shown in the legend.}
\label{fig:verify_ex}
\end{figure}




{\small 
\bibliography{1recdepth}
\bibliographystyle{unsrt}
}
\appendix



