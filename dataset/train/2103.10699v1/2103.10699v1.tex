\documentclass[twocolumn]{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{doi}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage[abs]{overpic}
\usepackage[dvipsnames]{xcolor}
\usepackage{float}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{pdfpages}
\usepackage{listings}
\usepackage{comment}
\usepackage{ftnright}
\usepackage{enumitem}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{pgfplots}

\usepackage[
backend=bibtex,
style=numeric,
citestyle=numeric
]{biblatex}
\addbibresource{references.bib}

\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*}
\setlist[enumerate]{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*}








\newcommand{\B}[1]{\textbf{#1}}
\newcommand{\R}[1]{\color{red}#1}
\DeclareMathOperator*{\argmax}{argmax}
\def\ours{\makebox[0pt]{\hspace{-4em}Ours}}


\lstset{
basicstyle=\ttfamily,
	commentstyle=\color[HTML]{2596be}
}


\title{MDMMT: Multidomain Multimodal Transformer for Video Retrieval}





\author{
	Maksim Dzabraev, Maksim Kalashnikov, Stepan Komkov, Aleksandr Petiushko \\
	Lomonosov Moscow State University \\
	Huawei Moscow Research Center \\
	dzabraev.maksim@intsys.msu.ru, kalashnikov.maxim@intsys.msu.ru, \\
	stepan.komkov@intsys.msu.ru, petyushko.alexander1@huawei.com
}





\hypersetup{
pdftitle={Multidomain multimodal video retrieval},
pdfauthor={M.Dzabraev, M.Kalashnikov, S.Komkov, A.Petiushko},
pdfkeywords={video, language, retrieval, multi-modal, multimodality, cross-modal, temporality, transformer, attention, text to video retrieval},
}

\begin{document}
\twocolumn

\twocolumn[
  \begin{@twocolumnfalse}
    \maketitle
  \end{@twocolumnfalse}
]

\centerline{\bf ABSTRACT}
We present a new state-of-the-art on the text to video retrieval task on MSRVTT and LSMDC benchmarks where our model outperforms all previous solutions by a large margin.
Moreover, state-of-the-art results are achieved with a single model on two datasets without finetuning. This multidomain generalisation is achieved
by a proper combination of different video caption datasets. We show that training on different datasets can improve test results of each other. Additionally we check
intersection between many popular datasets and found that MSRVTT has a significant overlap between the test and the train parts, and the same situation is observed for ActivityNet.



\keywords{video, language, retrieval, multi-modal, cross-modal, temporality, transformer, attention}



\section{Introduction} \label{sec:introduction}
Video is a quite popular data format, 500+ hours of video are uploaded on YouTube every minute. Many personal mobile phones
have gigabytes of video. Since video format gets more popular every year the importance of modern search methods is increasing as well.

In this work we present our research about text to video retrieval task. In this task system should return for a given textual query the most relevant video
segments from a gallery. The query is a textual description of what we want to find in the video. The query may describe objects, actions, sounds, ..., and
relations between them.

Such search methods are a promising direction for mobile devices because every year manufacturers increase the available memory on devices.
The large part of the memory is filled by media data. For end users it is getting difficult to search for a video made one or two years ago.
But users can easily describe the content of the video using natural language, which can be effectively used as a search query.


There are two major directions which allow calculate the relevance between a textual search query and a video segment. The first direction is single stream
approaches \cite{sun2019videobert}, where a query and a video together are given to a network and then become fused from the beginning of the processing. The schematic illustration of this approach
is presented in Fig.~\ref{fig:sstream_network}.

\begin{figure}[H]
  \begin{tabular}{c@{\hspace{0.5em}}c}
    \begin{minipage}{0.48\columnwidth}\includegraphics[width=0.95\textwidth,bb=0 0 306 207]{imgs/sstream_network.jpg}
\end{minipage}
    &
    \begin{minipage}{0.48\columnwidth}\includegraphics[width=0.95\textwidth, bb=0 0 347 252]{imgs/tstream_network.jpg}
\end{minipage} \\\\
    \begin{minipage}{0.48\columnwidth}
      \begin{subfigure}{\columnwidth}
	\caption{Scheme for a single-stream neural network.}\label{fig:sstream_network}
      \end{subfigure}
    \end{minipage} & 
    \begin{minipage}{0.48\columnwidth}
      \begin{subfigure}{\columnwidth}
	\caption{Scheme for a two-stream neural network.}\label{fig:tstream_network}
      \end{subfigure}
    \end{minipage} \\
  \end{tabular}

\caption{Two types of fusion} \label{fig:1}
\end{figure}

This type of approaches have access to all input data from the beginning of its processing
and can make a strong verdict about data. But these approaches have a significant drawback because it is not scalable:
every new query the search system should calculate the full forward pass for this query and for each video segment from the gallery.

Another direction is two stream neural networks \cite{mithun2018learning}, \cite{gabeur2020multimodal}, where a textual query and a video are processed by two different neural networks. As
a result the networks produce embeddings inside the same embedding space, where semantically close textual queries and video segments will
be placed next to each other. The schematic illustration is presented in Fig.~\ref{fig:tstream_network}.

The two stream approach is scalable, it allows to precompute video embeddings for all videos from the gallery, and to do only one forward pass with the text network for each new
query and then to compute the cosine similarity between the new query embedding and all precomputed embeddings.

To make a strong video retrieval solution it is important to show to the model a lot of situations, actions and objects from real life.
There exist a lot of video datasets, but none of them cover a significant portion of real life situations. One of the first steps to
tackle this problem is to formulate the rules for combining different existing datasets to a single large train database.

Text to video retrieval is a modern direction, where one of the first works was published at 2016 \cite{torabi2016learning}. One of the most universal solution for video retrieval task
is Multi Modal Transformer~\cite{gabeur2020multimodal} architecture which uses BERT~\cite{devlin2019bert} backbone for a video network. It allows in a natural way to process the temporal dependencies inside the multi modal
data source.

To train a text to video retrieval neural network the training database should consists of pairs: (a video segment, a textual description of this video segment). Traditionally such sort of datasets was created for a video captioning task.
But it turns out that these datasets perfectly can be used for a video retrieval task.
One of the first video captioning dataset was MSVD, which was created in 2010. Today there exist more than a dozen of different video captioning datasets.

The most popular datasets for text to video retrieval is MSRVTT~\cite{xu2016msr-vtt}, ActivityNet~\cite{krishna2017dense} and LSMDC~\cite{rohrbach2016movie}.
Many researchers test their solutions mostly on these three datasets.


Our main contributions in this work are the following:
\begin{itemize}
  \item We present a new state-of-the-art (SotA) result on MSRVTT and LSMDC benchmarks;
  \item We present a model which shows good results on three different benchmarks without finetuning: MSRVTT (SotA), LSMDC (SotA) and ActivityNet at the same time;
  \item We present a practical approach which helps us to find the overlap between the train and the test parts of used datasets.
\end{itemize}



 
\section{Related work}
\subsection{Datasets}
\label{sect:datasets}
MSRVTT~\cite{xu2016msr-vtt} was created in 2016. This dataset is traditionally used by researchers as the main dataset for testing text to video retrieval models. This dataset consists of 10k video segments, each segment has 20 captions.
The authors collected 257 popular search queries and gathered from YouTube 118 most relevant videos for each of them.
The dataset has 42 hours of video. The captions were made by 1327 amazon workers.

Today there are three different test/train splits. The official split is called \textbf{full} split, where the train part has 7k videos
and the test part has 3k videos. There are two important properties of this split: 1. there are no two video segments cropped from the same video so as
the first segment is placed in the train part and the second segment is placed in the test part; 2. there are no two video segments,
retrieved from the same query so as the first one is placed in the train part and the second one is placed in the test part.

Another two splits are called \textbf{1k-A}~\cite{yu2018joint} (sometimes called jsfusion) and \textbf{1k-B}~\cite{miech2020learning} (sometimes called miech).
Both of them have different 1k videos for testing.
They were created by randomly sampling 1k videos from the original test part (full split). 1k-A train part consists of the original train split and the
rest of the videos from the test part, so it has 1k videos for the test part and 9k videos for the train part. 1k-B has 1k videos for the test part and 6.5k videos for the train.
Additionally both splits use only one caption per segment (instead of 20 captions).

Unfortunately 1k-A and 1k-B mixed up the train and test parts. This led to violation in properties 1. and 2. which the full split satisfies.

Another problem is that all these splits have the overlap between the test and train parts, see~\ref{sect:cleaning_res} for details. 
To be strict we remove the overlap between the test part and the train part of MSRVTT full split.
We called this split MSRVTT \textbf{full clean}, and refer to it as M.
It is worth to mention that we do not modify the test part, we only remove some videos from the train part.






The Large Scale Movie Description Challenge (LSMDC)~\cite{rohrbach2016movie} is the extension of two independent datasets:
MPII Movie Description Dataset (MPII-MD)~\cite{rohrbach2015dataset}, and Montreal Video Annotation Dataset (M-VAD)~\cite{torabi2015using}.

Video segments for this dataset were cropped from movies, where movie textualized transcriptions were used as captions. A movie transcription
is an audio description of a video segment that helps blind people to watch movies by describing what happens, who appears in this time,
what is on background right now and so on.

In this work for testing we use LSMDC public test, which consists of 1k video segments.



ActivityNet captions dataset \cite{krishna2017dense} consists of 20k videos and 100k captions, where captions cover the full video length for the most of videos, and neighbour captions may intersect. 
The annotations were made with Amazon Mechanical Turk.


The situation when some video segments may overlap makes a problem for text to video retrieval testing.
Suppose we have two video-caption pairs  and  where the video segment  has a non empty overlap with the video segment .
Now suppose that for query  the system returns the video segment . Is it mistake or not? What to do in this case?

Many previous works used ActivityNet test dataset in a paragraph retrieval mode. In this mode all captions for all video segments are concatenated,
then the concatenated text is used as a textual query and the whole video should be retrieved for this query.
Such mode has two drawbacks. The first one is that paragraph retrieval is not a classical video retrieval mode, it is another task. One can ask:
if a model is good in paragraph retrieval will it be good for video retrieval? The second drawback is that queries will be long,
video segments will be long (compared to a classical video retrieval mode). This issue requires to enlarge the input for the model.

Another way to use the test part of ActivityNet is just to sample once a single random segment from each video. As a result we will have non intersected video segments
and captions with usual length. We use ActivityNet test part in this way. We take all videos from val1 and val2 parts, and sample a single random segment from each video.
All results on ActivityNet are reported on this split.







Additionally in this work the following datasets are used:
NIST TRECVID Twitter vines~\cite{2020trecvidawad},
TGIF~\cite{tgif-cvpr2016},
MSVD~\cite{chen-dolan-2011-collecting},
YouCook2~\cite{zhou2018weaklysupervised},
Something-something V2~\cite{goyal2017something},
Kinetics 700~\cite{smaira2020short},
HowTo100M~\cite{miech19howto100m}.



\subsection{Prior Art} \label{ssec:mmt}
A dominant approach to train video retrieval models is contrastive learning. The idea of this approach is that we have a set of pairs  and
elements of each pair should be placed next to each other in some metric space: , at the same time the element 
should be far from all other : . The bi-directional max-margin ranking loss~\cite{Karpathy2014DeepFE} represents this idea.

When training data have a lot of noise the MIL NCE loss~\cite{miech2020endtoend} can be applied in the training procedure. Suppose that we know that a video should be close to one of (or several) texts text, ..., text.
This approach tries to reduce the distance between the video and all text, ..., text at the same time.

All video captions datasets have the following problem. Suppose the distance between  is to be minimized while the distance between  is to be maximized, but
 and  are quite similar (from the semantical point of view). Maybe the optimal scenario in this situation is to minimize the distance between .
In~\cite{patrick2021supportset} the authors show the approach which deals with this problem.

As far as an input video is the temporal sequence of tokens (frames or video segments) it is important to efficiently aggregate the information from all tokens. Many ideas for such aggregation in the previous works are borrowed 
from the natural language processing. Convolution filters for aggregation are used in~\cite{patrick2021supportset}, a transformer encoder as a video aggregator is used in~\cite{gabeur2020multimodal},
many different aggregation functions are tested in~\cite{portilloquintero2021straightforward}.

We think that the most promising aggregation method is Multi Modal Transformer (MMT)~\cite{gabeur2020multimodal}.
MMT is a two stream solution designed for a text to video retrieval task.
The extraction of features from the input video stream is done in the following way.
An input video is preprocessed by several pretrained frozen
neural networks (these networks are called experts). Original solution uses seven modalities: motion, RGB, scene, face, OCR, speech, audio, and one pretrained network for each modality is used.
The motion modality is processed with video recognition networks like S3D, SlowFast, irCSN, where several input frames are used as a single input.
The RGB modality uses a single frame as an input. The audio modality uses the raw input sound from a video.
After embeddings are extracted from input data by these experts, it will be augmented by adding positional encoding tokens (representing time) and
expert tokens.
Then the augmented embeddings are passed through MMT backbone. MMT backbone is a standard transformer encoder architecture.
Each input modality produces one embedding, so in total there are seven output embedding from MMT.

For encoding the textual query the authors use pretrained BERT model where the output [CLS] token is used. The output is postprocessed with shallow networks (one network per modality) to extract
the modality related information, in total seven feature vectors will be produced.
In addition to embeddings from the text query seven weights representing how much the query describes one of seven modalities are produced. For example, if a query does not represent the sound,
the small weight for the audio modality should be produced.

The final similarity score is done by a sum of seven weighted dot products of embeddings.

The MMT is trained with the bi-directional max-margin ranking loss~\cite{Karpathy2014DeepFE}:

where  represent the batch size, the similarity between the -th query and the -th video inside this batch, and some predefined margin correspondingly.



 
\section{Methodology}
Our work is mostly based on MMT. We use the same loss and a similar architecture, but with different hyperparameters.
In this work we study the following questions:

\begin{itemize}
	\item Which publicly available pretrained motion expert is the best for text to video retrieval nowadays, Sec.~\ref{sec:motion_experts}.
	\item How to combine several video caption datasets in order to train a strong model without specialisation for a particular dataset, Sec.~\ref{sec:dset_compose}.
	\item How to find and prevent the overlap between the test and train parts when combining datasets, Sec.~\ref{sec:tt_isect}.
\end{itemize}


\subsection{Motion experts} \label{sec:motion_experts}
The MMT video backbone does not process the raw input video stream, and instead the input video stream
is processed by one or more pretrained experts, where each expert produces time series of features.
The most important modality is motion: a motion expert processes several video frames as a single input unit and extracts the information
about actions and objects within a segment.

We may say that the motion modality is the basis of the MMT. If a motion expert doesn't extract some information, there is a high probability that
MMT won't know about some events in the video stream. That's why improving the motion expert is very important.

We consider several best solutions from Kinetics~\cite{kay2017kinetics} benchmark as well as several promising video recognition models
and check which one works in the best way as a motion expert.
We present all details in Sec.~\ref{ssec:video_experts}.

\subsection{Dataset creation} \label{sec:dset_compose}
	It is possible to train a video retrieval model by two means. The first way is the way of specialization for a single domain.
For example: create model that will work good only for MSRVTT benchmark (or domain) but at the same time this model will show poor results on other datasets (domains).
In this way MMT \cite{gabeur2020multimodal} was trained. The authors trained three different models for MSRVTT, ActivityNet and LSMDC datasets. Each of these
three networks works good on domain  if and only if it was trained on , but at the same time works poor on another domain .
A proof of this statement we provide in Tab.~\ref{tab:cross-dataset-test}.

The second way is to create a model that will work good for all domains at the same time. We use this way.

Obviously the model trained in the first way can't work good with real users, because the event when a user writes a
search query similar to some caption from a small train database is very rare.

The second drawback here is that each video retrieval
train dataset is not that big, and it causes the situation that model doesn't see many words and real life situations during training. For example, MSRVTT has
only 9k videos and 200k captions in total for training, obviously this is not enough to train a neural network that will know most of real life
situations, different items and persons. To tackle with this problem we can take several datasets with videos and captions and concatenate it.

Different datasets have the different number of videos, the different number of captions per video, some datasets may have long captions, some may have short captions,
different rules for creating captions were used by human writers, and so on. Due to these factors some datasets may contain more information and require 
longer training time, some datasets may contain less information and require shorter training time. On the other hand, if we use long training time for
a small dataset it could lead to overfitting on this dataset (the data will be memorized). The "information sizes" of some used datasets are illustrated in Fig.~\ref{fig:dataset_information_size}.

\begin{figure}[h]
      \centering
\scalebox{0.90}{
\begin{tikzpicture}

\draw[->,ultra thick] (-0.2,0)--(6,0);
\draw[->,ultra thick] (0,-0.2)--(0,6);
\node[] at (3,-0.6) {number of unique captions};
\node[] at (-0.5, 3) {\rotatebox{90}{total video duration}};

\filldraw[color=red] (5.0,0.4468689629600283) circle (0.38461538461538464);
\node[above, color=red] at (5.0,0.831484347575413) {MSRVTT};
\filldraw[color=blue] (2.0596715219085295,5.0) circle (0.38461538461538464);
\node[above, color=blue] at (2.0596715219085295,5.384615384615385) {ActivityNet};
\filldraw[color=cyan] (3.0762452796259665,1.2725879748861566) circle (0.11538461538461539);
\node[above, color=cyan] at (3.0762452796259665,1.387972590270772) {LSMDC};
\filldraw[color=magenta] (0.6935203500569442,0.11533771921202855) circle (0.07692307692307693);
\node[above, color=magenta] at (0.6935203500569442,0.1922607961351055) {Vines};
\filldraw[color=pink] (0.35059641551279747,0.7212969347454151) circle (0.11538461538461539);
\node[above, color=pink] at (0.35059641551279747,0.8366815501300305) {YC2};
\filldraw[color=violet] (3.766888449319667,1.1040190706769908) circle (0.5);
\node[above, color=violet] at (3.766888449319667,1.6040190706769908) {TGIF};
\filldraw[color=green] (1.9095186717017323,0.10215175014212363) circle (0.023076923076923078);
\node[above, color=green] at (1.9095186717017323,0.1252286732190467) {MSVD};
\node[] at (0.29970628783791886,-0.7em) {10K};
\node[] at (1.4985314391895943,-0.7em) {50K};
\node[] at (2.9970628783791886,-0.7em) {100K};
\node[] at (4.975124378109453,-0.7em) {166K};
\node[] at (-1.3em,5.0) {466h};
\node[] at (-1.3em,1.1040190706769908) {102h};
\node[] at (-1.3em,0.4468689629600283) {41h};
\end{tikzpicture}
}
  \caption{Radius of the ball represent the ``information size'' of dataset. The biggest balls have more diversity in data.}
  \label{fig:dataset_information_size}
\end{figure}

Fig.~\ref{fig:dataset_information_size} is made with a simple algorithm. We take the original training procedure of MMT and for a given dataset we change the number of examples that
will be shown to a network during training. We define the radius of the ball as the number of training examples after which the performance gets saturated (i.e. increasing the training time
does not give the better model).

The key question is: what is the proper way for sampling examples from several datasets taking into account the different information size?

We use these obvious rules:

\begin{enumerate}
\item If a dataset  is larger than , we should sample from  more often than from ;
	\item Training on  and  combined requires longer train than training solely on  or ;
	\item Training on  and  combined may require a deeper model than for  or .
\end{enumerate}

If we achieve the same results on  after combining  and  it is still good because model gets better on . 
Our experiments show that the proper usage of rules 1--3 often improves the results for a specific test dataset (e.g. MSRVTT) after extending the train dataset.

We managed to combine the following datasets: MSRVTT, ActivityNet, LSMDC, TwitterVines, YouCook2, MSVD, TGIF and Something to something V2 (SomethingV2).
In total we increase the number of video segments by 40 times and the number of unique captions by 4 times compared with MSRVTT dataset. In Tab.~\ref{tab:dataset_size} we summarize the sizes of used datasets.
We separate SomethingV2 dataset from all other datasets because: 1. all video segments are created artificially, 2. the structure of text captions is quite limited. At the same time videos for all other datasets
are collected from the Internet and captions being created by humans have quite a rich structure.

\begin{table}[h]
	\centering
	\begin{tabular}{ |l|cccc| }
		\toprule
		\multirow{3}{*}{Dataset}  & Num   & Num   & Num        & Has\\
								  & video & pairs & unique     & YouTube\\
								  &       &       & captions   & Id	    \\
		\midrule
		MSRVTT			& 10k		    & 200k			& 167k & Yes \\
		ActivityNet		& 14k		    & 70k			& 69k  & Yes \\
		LSMDC			& 101k		    & 101k			& 101k & No  \\
		TwitterVines    & 6.5k		    & 23k			& 23k  & No  \\
		YouCook2		& 1.5k		    & 12k			& 12k  & Yes \\
		MSVD			& 1.5k		    & 80k			& 64k  & Yes \\
		TGIF			& 102k          & 125k          & 125k & No  \\
		\textit{\bf Sum above}  & \textit{\bf 236k} & \textit{\bf 611k}         & \textit{\bf 561k}  & --- \\
		SomethingV2		& 193k		    & 193k			& 124k & No \\
		\textit{\bf Sum above}  & {\it\bf 429k}      & {\it\bf 804k}            & {\it\bf 685k} & --- \\
\bottomrule
	\end{tabular}
	\caption{The "Num video" column represents the number of video clips in the dataset,
			 the "Num pairs" column represents the total number of video-caption pairs,
			 the "Num unique captions" column represents the number of unique captions in the dataset.}
	\label{tab:dataset_size}
\end{table}

\subsection{Intersection}
It is important to extend the training database carefully, not allowing the addition to the train part of video segments that already exist in the test part. 

To find the intersection between the test part and the train part we use the two stage filtration. The first stage is to use the YouTube ID, if it is available. We should
not allow to use in the test and train parts simultaneously any two video segments sampled from the same video. In the second stage we compute the similarity score between
each video from the test part and each video from the train part, then we manually assess the pairs with the highest scores.
In total we assessed more than 100K pairs of the most relevant segments, see Sec.~\ref{ssec:NDVS} for details.

We found the significant overlap between the MSRVTT 1k-A test and train parts, and the similar situation is with the 1k-B test and train parts and the less significant overlap is found between the MSRVTT full split test and train parts.
The similar situation is with the ActivityNet train and validation 1,2 parts.

Additionally we estimate (but did not find) the overlap between HowTo100M and MSRVTT, and found that it may be
significant. Our approach allows to approximately estimate the total number of videos in the intersection without finding the exact intersection, please see the details in Sec.~\ref{ssec:how_many_pairs}.
The similar estimation is for ActivityNet and Kinetics700, an our approximation shows that there may be a significant overlap, see all details in Sec.~\ref{ssec:NDVS}.

 
\section{Experiments}



\begin{table}[h]
	\centering
	\begin{tabular} {|l|l|}
		\toprule
		Abbreviate & Composition \\
		\midrule
		M				& MSRVTT full split\\
M                           & MSRVTT full clean split, see Sec.~\ref{sect:datasets}\\
M		& MSRVTT 1k-A split\\
M		& MSRVTT 1k-B split\\
A				& ActivityNet \\
A	        & ActivityNet val1 validation set\\
A	        & ActivityNet val2 validation set\\
A			& ActivityNet paragraph retrieval, see Sec.~\ref{sect:datasets}\\
L				& LSMDC \\
K				& Kinetics700 \\
V				& Twitter Vines \\
Y				& YouCook2 \\
HT100M				& HowTo100M \\
		\midrule
		\multirow{2}{*}{MALV}		& MSRVTT + ActivityNet + \\
						& LSMDC + TwitterVines \\
		\midrule
		\multirow{3}{*}{MALVYMT}	& MSRVTT + ActivityNet + \\
									& LSMDC + TwitterVines + \\
									& YouCook2 + MSVD + TGIF \\
		\midrule
		\multirow{4}{*}{MALVYMTS}	& MSRVTT + ActivityNet + \\
									& LSMDC + TwitterVines + \\
									& YouCook2 + MSVD + TGIF + \\
									& Something to Something V2 \\
		\bottomrule

	\end{tabular}
	\caption{The left column represents the abbreviate name for the set of datasets from the right column.}
	\label{tab:abbrev_names}
\end{table}

\subsection{Architecture}
We use exactly the same neural network architecture as original MMT \cite{gabeur2020multimodal}, our method is significantly based on their codebase. The difference is in the following:
1. we use the more aggressive dropout equals to 0.2 for the text BERT and the video BERT (against the original value of 0.1); 
2. we found that the deeper and wider transformer encoder for a video network gives better results~---
we use 6 layers and 8 heads for the motion only modality and 9 layers and 8 heads for the motion + audio setting (against 4 layer and 4 head in the original implementation).

\subsection{Stronger motion experts} \label{ssec:video_experts}
As the input data for MMT is embeddings from experts, the obvious question can arise: if a better expert is used, will we have a stronger model?
To answer this question we train MMT on MSRVTT dataset with the only motion modality. For motion experts we try several architectures
pretrained on different datasets, these models are presented in Tab.~\ref{tab:video_experts_fps32}. We take the architectures which show the best
results on Kinetics 400 benchmark having publicly available pretrained weights:
\cite{xie2018rethinking}
\cite{feichtenhofer2019slowfast}
\cite{tran2019video}
\cite{tran2018closer}
\cite{ghadiyaram2019largescale}
.

The results in Tab.~\ref{tab:video_experts_fps32} are made with the same hyperparameters as in \cite{gabeur2020multimodal}. For the train dataset we use only MSRVTT full clean split.
The first line in Tab.~\ref{tab:video_experts_fps32} represents the motion feature extractor from the original MMT paper.

\begin{table*}[h]
	\centering
	\begin{tabular}{|ll|l @{\hspace{1\tabcolsep}} l @{\hspace{1\tabcolsep}} l @{\hspace{1\tabcolsep}} l @{\hspace{1\tabcolsep}} l|}
		\toprule
			\multirow{2}{*}{Video expert}  & \multirow{2}{*}{Dataset} & \multicolumn{5}{c|}{Text  Video} \\
				&  & R@1	& R@5	& R@10	& MnR	& MdR \\
		\midrule
		s3d                 & Kinetics 600		    & 7.7     & 24.0    & 34.9    & 129.6   & 23.7\\
		SlowFast 32x2 R101  & Kinetics 600		    & 9.3     & 27.5    & 39.1    & 110.8   & 18.7 \\ 
		ipCSN152            & IG65M			    & 9.5     & 27.9    & 39.6    & 106.1   & 18.0 \\
		ipCSN152            & IG65M  K400	    & 8.3     & 25.2    & 36.5    & 124.3   & 21.0 \\
		ipCSN152            & Sports1M			    & 7.4     & 22.4    & 32.7    & 140.6   & 27.0 \\
		ipCSN152            & Sports1M  K400   & 7.8     & 24.2    & 35.2    & 129.9   & 23.0 \\ 
		irCSN152            & IG65M			    & 9.5     & 27.9    & 39.5    & 105.5   & 18.0 \\
		irCSN152            & IG65M  K400      & 8.4     & 25.3    & 36.5    & 120.4   & 21.0 \\
		irCSN152            & Sports1M			    & 6.9     & 21.6    & 31.6    & 141.9   & 28.7 \\
		irCSN152            & Sports1M  K400   & 7.7     & 24.1    & 35.1    & 127.6   & 23.0 \\
		r(2+1)d 152         & IG65M			    & 5.7     & 18.5    & 27.8    & 178.5   & 37.7 \\
		r(2+1)d 152         & IG65M  K400      & 5.5     & 18.1    & 27.3    & 184.1   & 39.3 \\
		r(2+1)d 152         & Sports1M  K400   & 5.3     & 17.3    & 26.0    & 193.4   & 42.3 \\
		r(2+1)d 34          & IG65M			    & 9.1     & 27.2    & 38.7    & 108.1   & 19.0 \\ 
		r(2+1)d 34          & IG65M  K400	    & 8.2     & 25.3    & 36.7    & 120.8   & 21.0 \\
		CLIP	            & CLIP			    & \B{14.4} &  \B{37.4} & \B{50.2} & \B{70.3} & \B{10.3} \\
		s3dg MIL-NCE        & HowTo100M			    & 8.6      & 26.3      & 37.9     & 104.4    & 19.3 \\
		\bottomrule
	\end{tabular}	
	\caption{Comparison of the best available pretrain models as the motion experts for MMT.
			 IG65M  K400 means that model was trained on IG65M and then fine tuned on Kinetics400.
			 Results for each experiment are computed over three runs with random seeds. The results are reported on MSRVTT full clean split.}
	\label{tab:video_experts_fps32}
\end{table*}

As we can see, usually stronger models provide better results, but not always. Refer to r(2+1)d 152 rows, this network demonstrates one of the best
performance on Kinetics 400 benchmark, but works poorly as motion expert. Maybe this network is over specialized for Kinetics 400. More shallow analogue of r(2+1)d 152 is r(2+1)d 34 which shows much better results.

An interesting observation is that the best results are achieved with the networks trained in the unsupervised manner. CLIP and models trained on IG65M outperform all other models
trained on Kinetics in the supervised manner.
Another weakly supervised dataset is Sports1M~\cite{KarpathyCVPR14}. Models trained on this dataset provide
weak embeddings similar to the weak s3d model trained on Kinetics dataset.
The CLIP~\cite{radford2learning} (ViT-B/32) image feature extractor with a large margin outperforms all other models. The model s3dg MIL-NCE is a video encoder from the work~\cite{miech2020endtoend}. This network was trained from scratch on HowTo100M dataset.

As we show in Sec.~\ref{sec:tt_isect} Kinetics dataset has an overlap with MSRVTT dataset, and we don't know whether it affects to overfitting or not. Also it is worth to mention that IG65M and CLIP datasets are not publicly available, so we do not know if there is an overlap with MSRVTT and other video retrieval datasets.

For more details about our usage of pretrained video experts please refer to Sec.~\ref{sec:pretrain_experts}.

\begin{table*}[h]
  \centering
\begin{tabular}{|l @{\hspace{1\tabcolsep}} |@{\hspace{1\tabcolsep}} l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l|}
    \toprule
    \multirow{2}{*}{model} & \multicolumn{5}{c|}{ActivityNet text  video} \\
                          & R@1 & R@5 & R@10 & MnR & MdR \\
    \midrule
      CLIP~\cite{radford2learning}                 & 0.02 & 0.06 & 0.2 & 2210 & 2251 \\
      MMT (A) motion+audio~\cite{gabeur2020multimodal} & 7.3 & 22.5 & 31 & 283.9 & 30 \\
      \ours MDMMT(MALVYMTS) L9H8 irCSN152+audio         & 15.1 & 38.3 & 51.5 & 92.4 & 10.0 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+audio             & 17.7 & 41.6 & 54.3 & 76.0 & 8.3 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+irCSN152+audio    & \B{20.1} & \B{45.1} & \B{58.0} & \B{70.8} & \B{7.0} \\
    \bottomrule
  \end{tabular}
\caption{Test results on our split (see Sec.~\ref{sect:datasets}) on ActivityNet.}
  \label{tab:models-anet}
\end{table*}
\subsection{Datasets combination} \label{ssec:main}

In this section we show our experiments about the combination of different datasets. Nowadays video-caption datasets are not big enough to capture all real life
situations, also some datasets may be biased. The combination of different datasets may help to tackle this problem. 

Our experiments show that the proper combination of datasets allows to train a single model that can capture the knowledge from all used datasets. Important thing here is that in most cases the 
model trained on the combination of datasets is better than the model trained on a single dataset. 

In our experiments we combine all datasets presented in Tab.~\ref{tab:dataset-wgh}.
The important thing is how to sample minibatches during training. In our experiments we first sample a dataset, then we uniformly sample a video segment, if this sampled
video segment has more than one caption we sample a single caption uniformly. Column weight in Tab.~\ref{tab:dataset-wgh} describes the probability of sampling the corresponding dataset.
To obtain the probability of sampling the dataset with the weight  we should divide  by the sum of all weights.

The weights for all datasets are manually adjusted. It is important to find a good weight combination, because if some weight will be larger than needed, this dataset will be
overseen and as a result the performance will be lower comparing to the optimal case.
The opposite case is when a small weight was selected, this causes the situation when during training a network does not see the required number of 
examples from this dataset. 

For experiments in this section we use MMT with the only motion modality. 
Embeddings for the motion modality are computed with irCSN152 pretrained on IG65M.
All configurations are trained with 50 epochs and different number examples per epoch.
The initial learning rate is 5e-5. After each epoch we multiply learning rate by 0.95.
The MALVYMTS (see Tab.~\ref{tab:abbrev_names} for abbreviations.) configuration is trained with 150K examples per epoch. Configurations with the less number of datasets are trained with the less number of
examples per epoch.
The number of examples per epoch can be represented as a product of 150K by a sum of normalized weights (weights from Tab.~\ref{tab:dataset-wgh} divided by a sum of all weights) for each dataset (the initial sum equals to 1):
.
If some dataset is removed from the training, we remove the corresponding coefficient from this sum, so the resulting length will be 150K multiplied by a value less than 1.

As far as we use the configurations M, A, L as the baselines, we need to be sure that the results for these configurations are the optimal values. 
In addition to the rule described above we try several values for a number of examples per epoch parameter, and report the results for the best found value.


\begin{table}[h]
      \centering
      \begin{tabular}{|c|c|}
	    \toprule
	    Dataset	    & Weight \\
	    \midrule
	    MSRVTT	    & 140 \\
	    ActivityNet	    & 100 \\
	    LSMDC	    & 70  \\
	    Twitter Vines   & 60  \\
	    YouCook2	    & 9   \\
	    MSVD	    & 9   \\
	    TGIF	    & 102 \\
	    Something V2    & 169 \\
	    \bottomrule
      \end{tabular}
      \caption{These datasets were used in our train procedure. The "Weight" column describes how often we sample examples from the dataset. The probability of obtaining an example from the
      dataset with the weight  equals to  divides by a sum of all weights.}
      \label{tab:dataset-wgh}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{|c|l @{\hspace{1\tabcolsep}} l @{\hspace{1\tabcolsep}} l|}
		  \toprule
	      \multirow{2}{*}{Dataset}  & \multicolumn{3}{c|}{Test Text  Video  R@5 } \\
         &  MSRVTT & ActivityNet &       LSMDC \\
		  \midrule
    M        & 29.0              & {\textit{13.4}} &  {\textit{12.9}} \\
    A        & {\textit{14.7}} & 30.9              &  {\textit{10.4}} \\
    L        & {\textit{8.8}}  & {\textit{7.2}}  &  24.7 \\
    MALV     &  32.1             & 32.0              &  26.5 \\
    MALVYMT  &  33.8             & 32.3              &  27.3 \\
    MALVYMTS &  \textbf{34.5}    & \textbf{32.4}     &  \textbf{27.4} \\ 
		  \bottomrule
	    \end{tabular}
  \caption{See abbreviations for the first column in Tab.~\ref{tab:abbrev_names}. The first three rows M,A,L report the quality of models trained on a single domain, and tested on other domains.
  \textit{Italic} means that the model did not see data from this domain during training. In this table the only motion modality (irCSN152) is used.}
  \label{tab:cross-dataset-test}
\end{table}

Tab.~\ref{tab:cross-dataset-test} summarizes our experiments on the datasets combination (for more details please refer to Sec.~\ref{sec:datasets_combination}). The main point here is that the proper combination of datasets leads to the best solution.

\subsection{Final result}
In this section we compare our solution with the prior art. Our two best solution uses three modalities: the audio, the motion and the RGB. To fuse modalities
we use MMT architecture with 9 layers and 8 heads. As a feature extractor for the audio stream the vggish~\cite{hershey2017cnn} network is used. For the video encoding we use
CLIP ViT-B/32 (RGB modality) and irCSN152 (motion modality) pretrained on IG65M dataset. The details about preprocessing videos for both networks are presented in Sec.~\ref{sec:pretrain_experts}.

Additionally we report separate results for motion + audio encoders and RGB + audio encoders because 
we do not know whether the IG65M or CLIP train database has a significant overlap with any of the test datasets or not.

All our models presented in Tab.~\ref{tab:models-anet},\ref{tab:models-msrvtt-full} and  \ref{tab:models-lsmdc}  are trained based on the pretrain HowTo100M model.
We present the details about pretraining in Sec.~\ref{sect:pretrain_ht100m}.

The results for MSRVTT are presented in Tab.~\ref{tab:models-msrvtt-full}. As we can see our solution MDMMT(MALVYMTS) L9H8 CLIP+irCSN152+audio
significantly outperforms all previous solutions on all splits: full, 1k-A and 1k-B. Our solution is better than the previous SotA (on R@5) on 8.7\%, 10.5\% and 14.4\% on
full, 1k-A and 1k-B correspondingly.
It is also worth to mention that our MDMMT (using only the motion, the RGB and the audio modalities) outperforms the original MMT (the motion, the RGB and the audio and 4 other modalities) by
8.7\%, 10.5\% and 14.4 (R@5) on full, 1k-A and 1k-B correspondingly.

We also report the results for the original CLIP~\cite{radford2learning}. The CLIP model has an image encoder and a text encoder, both pretrained in an unsupervised way. To test the CLIP model we take a single
frame from the middle of the video (this is the original testing protocol for CLIP). The row CLIP agg~\cite{portilloquintero2021straightforward} represents 
the usage of CLIP model with several frames using some specific aggregation procedure from this work.

In Tab.~\ref{tab:models-lsmdc} we report the results on LSMDC. On this benchmark we outperform the previous SotA solution by 8.6\%.


As we mention in Sec.~\ref{sect:datasets} we do not use the standard ActivityNet paragraph retrieval test protocol. Instead we use the text to video retrieval protocol.
To compare our solution with the previous work we take the previous SotA approach (MMT) in text to video retrieval and test it on our split.
The results are reported in Tab.~\ref{tab:models-anet}.
Our solution outperforms MMT by 22.6\%. The row MMT (A) motion+audio means that this network was trained only on ActivityNet dataset with the paragraph
retrieval mode. It is also worth to mention that CLIP shows very bad results on this benchmark. We try to aggregate with the mean pooling of 2, 4 and 16 uniformly taken embeddings, take the first 10, 20 and 70 words
from a caption, and no method improves the results.


The important property of our model is that we train a single model and test it on different test sets.
The authors of previous SotA approach (MMT) trained three different models for MSRVTT, ActivityNet and LSMDC, while in Tab.~\ref{tab:cross-dataset-test} we show that the model trained in such a manner
has poor generalization and can show good performance on the test part of the dataset  if and only if it was trained on the train part of the dataset . 






\begin{table*}[h]
  \centering
\vspace{-2em}\begin{tabular}{|l @{\hspace{1\tabcolsep}}  |@{\hspace{1\tabcolsep}} c @{\hspace{1\tabcolsep}} |@{\hspace{1\tabcolsep}} l @{\hspace{1\tabcolsep}} l @{\hspace{1\tabcolsep}} l @{\hspace{1\tabcolsep}} l @{\hspace{1\tabcolsep}} l|}
    \toprule
    \multirow{2}{*}{model} & \multirow{2}{*}{\rotatebox{90}{split}} & \multicolumn{5}{c|}{MSRVTT text  video} \\
			   &                        & R@1 & R@5 & R@10 & MnR & MdR \\
    \midrule
      Random baseline &\multirow{14}{*}{\rotatebox{90}{full}} & 0.0 & 0.2 & 0.3 & 1500 & 1500 \\
      VSE~\cite{mithun2018learning}                         && 5.0 & 16.4 & 24.6 & --- & 47 \\
      VSE++~\cite{mithun2018learning}                       && 5.7 & 17.1 & 24.8 & --- & 65 \\
      Multi Cues~\cite{mithun2018learning}                  && 7.0 & 20.9 & 29.7 & --- & 38 \\
      W2VV~\cite{Dong_2018}				    && 6.1 & 18.7 & 27.5 & --- & 45 \\
      Dual Enc.~\cite{dong2019dual}                         && 7.7 & 22.0 & 31.8 & --- & 32 \\
      CE~\cite{liu2020use}				    && 10.0 & 29.0 & 41.2 & 86.8 & 16.0 \\
      MMT (M) 7mod~\cite{gabeur2020multimodal}              && 10.7 &  31.1 & 43.4 &  88.2 & 15.0  \\
      CLIP~\cite{radford2learning}                 && 15.1 & 31.8 & 40.4 &  184.2  & 21 \\
      CLIP agg~\cite{portilloquintero2021straightforward}   && 21.5 & 41.1 & 50.4 &  --- & \B{4} \\
      \ours MDMMT(MALVYMTS) L9H8 irCSN152+audio             && 15.7 & 38.8 & 51.1 & 76.0 & 10.0 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+audio                 && 21.7 & 47.6 & 59.8 & 55.9 & 6.0 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+irCSN152+audio        && \B{23.1} & \B{49.8} & \B{61.8} & \B{52.8} & 6.0 \\
    \midrule
MMT (M) 7mod~\cite{gabeur2020multimodal} &\multirow{4}{*}{\rotatebox{90}{full}\hspace{0.1em}\rotatebox{90}{clean}}& 10.4 & 30.2 & 42.3 & 89.4 & 15.7 \\
      \ours MDMMT(MALVYMTS) L9H8 irCSN152+audio      && 15.8 & 38.9 & 51.0 & 76.4 & 10.0 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+audio && 21.5 & 47.4 & 59.6 & 57.7 & 6.0 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+irCSN152+audio && \B{22.8} & \B{49.5} & \B{61.5} & \B{53.8} & \B{6.0} \\
    \midrule
      Random baseline &\multirow{12}{*}{\rotatebox{90}{1k-A}} &  0.1 & 0.5 & 1.0 & 500.0 & 500.0\\
      JSFusion~\cite{yu2018joint}		                  && 10.2 & 31.2 & 43.2 & --- & 13 \\
      E2E~\cite{miech2020endtoend}                                && 9.9 & 24.0 & 32.4 & --- & 29.5 \\
      HT~\cite{miech19howto100m}		                  && 14.9& 40.2 & 52.8 & --- & 9 \\
      CE~\cite{liu2020use}                                        && 20.9 & 48.8 & 62.4 & 28.2 & 6.0 \\
      CLIP~\cite{radford2learning}	                  && 22.5 & 44.3 & 53.7 & 61.7 & 8 \\
      MMT (M) 7mod~\cite{gabeur2020multimodal} && 26.6   & 57.1   & 69.6   & 24.0   & 4.0 \\
      AVLnet\cite{rouditchenko2020avlnet}                         && 27.1 & 55.6 & 66.6 & --- & 4\\
      SSB~\cite{patrick2021supportset}		                  && 30.1 & 58.5 & 69.3 & --- & 3.0 \\
      CLIP agg~\cite{portilloquintero2021straightforward}         && 31.2 & 53.7 & 64.2 & --- & 4 \\
      \ours MDMMT(MALVYMTS) L9H8 irCSN152+audio   && 31.3 & 60.4 & 71.8 & 24.0 & 3.0 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+audio        && 38.9	& 68.3 & 78.8 & 17.3 & \B{2.0} \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+irCSN152+audio && \B{38.9} & \B{69.0} & \B{79.7} & \B{16.5} & \B{2.0} \\
    \midrule
      Random baseline &\multirow{10}{*}{\rotatebox{90}{1k-B}} &  0.1 & 0.5 & 1.0 & 500.0 & 500.0\\
      MEE~\cite{miech2020learning} && 13.6 & 37.9 & 51.0 & --- & 10.0 \\
      JPose~\cite{wray2019finegrained} && 14.3 & 38.1 & 53.0 & --- & 9 \\
      MEE-COCO~\cite{miech2020learning} && 14.2 & 39.2 & 53.8 & --- & 9.0 \\
      CE~\cite{liu2020use} && 18.2 & 46.0 & 60.7  & 35.3 & 7.0  \\
      MMT (M) 7mod~\cite{gabeur2020multimodal} &&  24.5  &  54.4	& 68.0  & 26.6 & 4.7 \\
      CLIP~\cite{radford2learning}					&& 24.5 & 46.2 & 56.8 & 60.9 & 7 \\
      \ours MDMMT(MALVYMTS) L9H8 irCSN152+audio&& 28.8 & 58.8 & 71.2 & 28.5 & 3.7\\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+audio    &&  35.1 &  66.5 & 77.6 & 21.5 & 2.7 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+irCSN152+audio       && \B{37.4} & \B{68.8} & \B{79.4} & \B{21.3} & \B{2.0} \\

    \bottomrule
  \end{tabular}
\caption{Results on MSRVTT dataset.}
  \label{tab:models-msrvtt-full}

  \vspace{\floatsep}

  \begin{tabular}{|l@{\hspace{1\tabcolsep}} |l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}|l|}
    \toprule
    \multirow{2}{*}{model} & \multicolumn{5}{c|}{LSMDC text  video} \\
			   & R@1 & R@5 & R@10 & MnR & MdR \\
    \midrule
      CT-SAN~\cite{yu2017endtoend}	                  & 5.1 & 16.3  & 25.2 & --- & 46 \\
      JSFusion~\cite{yu2018joint}		          & 9.1 & 21.2  & 34.1 & --- & 36 \\
      MEE~\cite{miech2020learning}                        & 9.3 & 25.1  & 33.4 & --- & 27 \\
      MEE-COCO~\cite{miech2020learning}                   & 10.1 & 25.6 & 34.6 & --- & 27 \\
      CE~\cite{liu2020use}                                & 11.2 & 26.9 & 34.8 & 96.8 & 25.3 \\
      CLIP agg~\cite{portilloquintero2021straightforward} & 11.3 & 22.7 & 29.2 & --- & 56.5 \\
      CLIP~\cite{radford2learning}               & 12.4 & 23.7 & 31.0  & 142.5 & 45  \\
      MMT (L) 7mod~\cite{gabeur2020multimodal}            & 12.9 & 29.9 & 40.1 & 75.0 & 19.3 \\
      \ours MDMMT(MALVYMTS) L9H8 irCSN152+audio       & 13.1 & 31.3 & 40.1 & 74.5 & 19.3 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+audio           & 17.2 & 34.9 & 45.3 & 65.6 & 14.0 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+irCSN152+audio  & \B{18.8} & \B{38.5} & \B{47.9} & \B{58.0} & \B{12.3} \\
    \bottomrule
  \end{tabular}
\caption{Test results on LSMDC public test (1k video)}
  \label{tab:models-lsmdc}
\end{table*}





 
\section{Conclusions and Discussion}
In this work we present a new text to video retrieval state-of-the-art model on MSRVTT and LSMDC benchmarks. We do not use ActivityNet dataset in the paragraph retrieval mode
as many previous works do, so we can't compare with them. But we show that on ActivityNet in the video retrieval mode we outperform the previous state-of-the-art model (MMT) by a large margin.
Our model has captured knowledge from many video caption datasets, thus it is able to show the best results on several datasets at the same time without finetuning.

We also present a practical approach to find the overlap between two different video datasets. Using this approach we find the overlap between several datasets.
Especially we find a large overlap between the MSRVTT test and train parts, and between the ActivityNet test and train parts. Removing this overlap from the MSRVTT train part significantly
decreases the performance of previous best models on MSRVTT benchmark.


\textit{Acknowledgments.} We would like to thank Andrey Ivanyuta and other colleagues from Intelligent Systems and Data Science Lab for helping to find the overlap between datasets.
 
\clearpage

\printbibliography






\appendix


\section{Pretrain experts usage} \label{sec:pretrain_experts}
The important data preparing stage is how to sample frames from a video to achieve the best performance.
For s3d experiments the input video is converted to 30 frames per second, for all other experiments we convert
the input video to 32 frames per second. As a result we compute a single embedding for each second, having 1 second window with 1 second shift (no overlapping).

The input frame size is important. We use the different sizes for the different models. For each model we use the recommended input size.
For s3d we resize a video to 256 on the short side and then take a 224x224 center crop.
For SlowFast 32x2 R101 we resize a video to 256 on the short side and then take a 256x256 center crop.
For ipCSN 152 and irCSN 152 we resize a video to 224 on the short side and take a 224x224 center crop.
For r(2+1)d 152 and r(2+1)d 34 we resize a video to 112 on the short side and then take a 112x112 center crop.

Pretrained models for ipCSN, irCSN and r(2+1)d are available here\footnote{https://github.com/facebookresearch/VMZ},
for SlowFast 32x2 R101 here\footnote{https://github.com/facebookresearch/SlowFast/blob/master/MODEL\_ZOO.md}, and for s3d here\footnote{https://github.com/princeton-vl/d3dhelper/blob/master/d3d\_helper.ipynb}.

For the CLIP model~\cite{radford2learning} we resize a video to 224 on the short side and take a center crop, then we extract 1 frame per second.
We use a publicly available image encoder. We do not use the text encoder from CLIP.

Model s3dg MIL-NCE is a video encoder from the work~\cite{miech2020endtoend}. This network was trained from scratch on HowTo100M dataset. For this network we resize
the input video stream to the size of 228x228 pixels, then take a center crop.
 
\section{Datasets combination} \label{sec:datasets_combination}
In Fig.~\ref{fig:msrvtt-dataset-combine},\ref{fig:activitynet-dataset-combine},\ref{fig:lsmdc-dataset-combine} we present 6 models.
Abbreviations MALV, MALVYMTS and MALVYMTS represent the same three models on these figures.
The first model, called M, is trained on the MSRVTT full clean split only,
the second one, called A, is trained on ActivityNet only. And the third model, called L, is trained on LSMDC only.
These three models are taken as baselines. Adding more datasets 
should be not worse than these baseline. The forth model is called MALV. This model is trained on
the combination of MSRVTT, ActivityNet, LSMDC and TwitterVines. As we can see MMALV gives +3.07\% on MSRVTT (full clean split), AMALV gives +1.06\% on ActivityNet,
and LMALV gives +1.77\% on LSMDC. The next model is called MALVYMT and it is trained on combination of MSRVTT, ActivityNet, LSMDC, TwitterVines, YouCook2, MSVD, TGIF.
The transitions MMALVYMT, AMALVYMT, LMALVYMT give +4.85\%, +1.45\% and +2.63\% correspondingly. The last transitions MMALVYMTS,
AMALVYMTS, LMALVYMTS slightly improve the performance on ActivityNet and LSMDC and significantly improve the performance on MSRVTT.
Finally, the combination of all datasets gives +5.5\% for MSRVTT, +1.47\% for ActivityNet and +2.74\% for LSMDC.

\begin{figure}[h]
      \centering
\scalebox{0.90}{
\begin{tikzpicture}
  \draw[->,ultra thick] (0,0)--(0.9\columnwidth,0);
  \draw[->,ultra thick] (0,0)--(0,0.8166666666666667\columnwidth);
  \node[align=left] at (0.3\columnwidth,0.7166666666666667\columnwidth) {MSRVTT full clean R@5 };
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.1\columnwidth,0){};
  \node[align=center] at (0.1\columnwidth,-0.05\columnwidth) {M};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.1\columnwidth,0.1\columnwidth){};
  \node[align=right] at (-0.07\columnwidth,0.1\columnwidth) {29.0};
  \draw[loosely dotted] (0.1\columnwidth,0.1\columnwidth) -- (0.1\columnwidth,0\columnwidth);
  \draw[loosely dotted] (0.1\columnwidth,0.1\columnwidth) -- (0\columnwidth,0.1\columnwidth);
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.25\columnwidth,0){};
  \node[align=center] at (0.25\columnwidth,-0.05\columnwidth) {MALV};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.25\columnwidth,0.41658278490608236\columnwidth){};
  \node[align=right] at (-0.07\columnwidth,0.41658278490608236\columnwidth) {32.1};
  \draw[loosely dotted] (0.25\columnwidth,0.41658278490608236\columnwidth) -- (0.25\columnwidth,0\columnwidth);
  \draw[loosely dotted] (0.25\columnwidth,0.41658278490608236\columnwidth) -- (0\columnwidth,0.41658278490608236\columnwidth);
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.5\columnwidth,0){};
  \node[align=center] at (0.5\columnwidth,-0.05\columnwidth) {MALVYMT};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.5\columnwidth,0.599237741699757\columnwidth){};
  \node[align=right] at (-0.07\columnwidth,0.599237741699757\columnwidth) {33.8};
  \draw[loosely dotted] (0.5\columnwidth,0.599237741699757\columnwidth) -- (0.5\columnwidth,0\columnwidth);
  \draw[loosely dotted] (0.5\columnwidth,0.599237741699757\columnwidth) -- (0\columnwidth,0.599237741699757\columnwidth);
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.8\columnwidth,0){};
  \node[align=center] at (0.8\columnwidth,-0.05\columnwidth) {MALVYMTS};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.8\columnwidth,0.6666666666666666\columnwidth){};
  \node[align=right] at (-0.07\columnwidth,0.6666666666666666\columnwidth) {34.5};
  \draw[loosely dotted] (0.8\columnwidth,0.6666666666666666\columnwidth) -- (0.8\columnwidth,0\columnwidth);
  \draw[loosely dotted] (0.8\columnwidth,0.6666666666666666\columnwidth) -- (0\columnwidth,0.6666666666666666\columnwidth);
  \draw[thick,color=black!60!green] (0.1\columnwidth,0.1\columnwidth) -- (0.25\columnwidth,0.41658278490608236\columnwidth);
  \draw[thick,color=black!60!green] (0.25\columnwidth,0.41658278490608236\columnwidth) -- (0.5\columnwidth,0.599237741699757\columnwidth);
  \draw[thick,color=black!60!green] (0.5\columnwidth,0.599237741699757\columnwidth) -- (0.8\columnwidth,0.6666666666666666\columnwidth);
\end{tikzpicture}
}
  \caption{Increasing R@5 metric on the MSRVTT full clean split while enriching the train part.}
  \label{fig:msrvtt-dataset-combine}
\end{figure}

\begin{figure}[h]
      \centering
\scalebox{0.90}{
\begin{tikzpicture}
  \draw[->,ultra thick] (0,0)--(0.9\columnwidth,0);
  \draw[->,ultra thick] (0,0)--(0,0.8166666666666667\columnwidth);
  \node[align=left] at (0.19\columnwidth,0.7166666666666667\columnwidth) {ActivityNet R@5 };
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.1\columnwidth,0){};
  \node[align=center] at (0.1\columnwidth,-0.05\columnwidth) {A};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.1\columnwidth,0.1\columnwidth){};
  \node[align=right] at (-0.07\columnwidth,0.1\columnwidth) {30.9};
  \draw[loosely dotted] (0.1\columnwidth,0.1\columnwidth) -- (0.1\columnwidth,0\columnwidth);
  \draw[loosely dotted] (0.1\columnwidth,0.1\columnwidth) -- (0\columnwidth,0.1\columnwidth);
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.25\columnwidth,0){};
  \node[align=center] at (0.25\columnwidth,-0.05\columnwidth) {MALV};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.25\columnwidth,0.5076325287125595\columnwidth){};
  \node[align=right] at (-0.07\columnwidth,0.5076325287125595\columnwidth) {32.0};
  \draw[loosely dotted] (0.25\columnwidth,0.5076325287125595\columnwidth) -- (0.25\columnwidth,0\columnwidth);
  \draw[loosely dotted] (0.25\columnwidth,0.5076325287125595\columnwidth) -- (0\columnwidth,0.5076325287125595\columnwidth);
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.5\columnwidth,0){};
  \node[align=center] at (0.5\columnwidth,-0.05\columnwidth) {MALVYMT};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.5\columnwidth,0.6612756789394085\columnwidth){};
\draw[loosely dotted] (0.5\columnwidth,0.6612756789394085\columnwidth) -- (0.5\columnwidth,0\columnwidth);
\node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.8\columnwidth,0){};
  \node[align=center] at (0.8\columnwidth,-0.05\columnwidth) {MALVYMTS};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.8\columnwidth,0.6666666666666666\columnwidth){};
  \node[align=right] at (-0.07\columnwidth,0.6666666666666666\columnwidth) {32.4};
  \draw[loosely dotted] (0.8\columnwidth,0.6666666666666666\columnwidth) -- (0.8\columnwidth,0\columnwidth);
  \draw[loosely dotted] (0.8\columnwidth,0.6666666666666666\columnwidth) -- (0\columnwidth,0.6666666666666666\columnwidth);
  \draw[thick,color=black!60!green] (0.1\columnwidth,0.1\columnwidth) -- (0.25\columnwidth,0.5076325287125595\columnwidth);
  \draw[thick,color=black!60!green] (0.25\columnwidth,0.5076325287125595\columnwidth) -- (0.5\columnwidth,0.6612756789394085\columnwidth);
  \draw[thick,color=black!60!green] (0.5\columnwidth,0.6612756789394085\columnwidth) -- (0.8\columnwidth,0.6666666666666666\columnwidth);
\end{tikzpicture}
}
      \caption{Increasing R@5 metric on the ActivityNet test set while enriching the train part.}
      \label{fig:activitynet-dataset-combine}
\end{figure}


\begin{figure}[h]
      \centering
\scalebox{0.90}{
\begin{tikzpicture}
  \draw[->,ultra thick] (0,0)--(0.9\columnwidth,0);
  \draw[->,ultra thick] (0,0)--(0,0.8166666666666667\columnwidth);
  \node[align=left] at (0.19\columnwidth,0.7166666666666667\columnwidth) {LSMDC R@5};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.1\columnwidth,0){};
  \node[align=center] at (0.1\columnwidth,-0.05\columnwidth) {L};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.1\columnwidth,0.1\columnwidth){};
  \node[align=right] at (-0.07\columnwidth,0.1\columnwidth) {24.7};
  \draw[loosely dotted] (0.1\columnwidth,0.1\columnwidth) -- (0.1\columnwidth,0\columnwidth);
  \draw[loosely dotted] (0.1\columnwidth,0.1\columnwidth) -- (0\columnwidth,0.1\columnwidth);
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.25\columnwidth,0){};
  \node[align=center] at (0.25\columnwidth,-0.05\columnwidth) {MALV};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.25\columnwidth,0.4663595833125108\columnwidth){};
  \node[align=right] at (-0.07\columnwidth,0.4663595833125108\columnwidth) {26.5};
  \draw[loosely dotted] (0.25\columnwidth,0.4663595833125108\columnwidth) -- (0.25\columnwidth,0\columnwidth);
  \draw[loosely dotted] (0.25\columnwidth,0.4663595833125108\columnwidth) -- (0\columnwidth,0.4663595833125108\columnwidth);
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.5\columnwidth,0){};
  \node[align=center] at (0.5\columnwidth,-0.05\columnwidth) {MALVYMT};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.5\columnwidth,0.6459452442507194\columnwidth){};
\draw[loosely dotted] (0.5\columnwidth,0.6459452442507194\columnwidth) -- (0.5\columnwidth,0\columnwidth);
\node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.8\columnwidth,0){};
  \node[align=center] at (0.8\columnwidth,-0.05\columnwidth) {MALVYMTS};
  \node [draw,shape=circle,fill=black,minimum size=0.2cm,inner sep=0pt] at (0.8\columnwidth,0.6666666666666666\columnwidth){};
  \node[align=right] at (-0.07\columnwidth,0.6666666666666666\columnwidth) {27.4};
  \draw[loosely dotted] (0.8\columnwidth,0.6666666666666666\columnwidth) -- (0.8\columnwidth,0\columnwidth);
  \draw[loosely dotted] (0.8\columnwidth,0.6666666666666666\columnwidth) -- (0\columnwidth,0.6666666666666666\columnwidth);
  \draw[thick,color=black!60!green] (0.1\columnwidth,0.1\columnwidth) -- (0.25\columnwidth,0.4663595833125108\columnwidth);
  \draw[thick,color=black!60!green] (0.25\columnwidth,0.4663595833125108\columnwidth) -- (0.5\columnwidth,0.6459452442507194\columnwidth);
  \draw[thick,color=black!60!green] (0.5\columnwidth,0.6459452442507194\columnwidth) -- (0.8\columnwidth,0.6666666666666666\columnwidth);
\end{tikzpicture}
}
      \caption{Increasing R@5 metric on the LSMDC test set while enriching the train part.}
      \label{fig:lsmdc-dataset-combine}
\end{figure}
 
\section{Test and train intersection} \label{sec:tt_isect}

In this section we present our analysis of overlapping of popular text to video datasets. 
Since we compose the train dataset from several different datasets it is important to be sure that there is no the same video segment
in the train part and in the test part. Our aim is to find the overlap between the train part of used datasets~---
MSRVTT, ActivityNet, LSMDC, YouCook2, MSVD, TGIF, TwitterVines, HowTo100M, Kinetics700
and the test parts of MSRVTT, ActivityNet and LSMDC, and then to remove found duplicates from the train parts.

Note that for training  we use Something to Something V2 dataset, but we do not try to find overlap between it and test datasets
because this dataset is artificially created, thus the probability to find duplicates is very low.

We decided to find the overlap only for MSRVTT, ActivityNet and LSMDC because these are the most popular datasets and we do not have enough human resources
to find the overlap for the test part of all other datasets.

Our cleaning method consists of two stages. The first stage is to match video segments by the YouTube ID (if the ID is available) 
and remove from train parts all video segments that have the corresponding pair in test parts. In Tab.~\ref{tab:dataset_size}
the information about the availability of YouTube IDs in datasets is presented.
We collect the YouTube ID for all videos from MSRVTT full test and ActivityNet validation 1,2 and remove corresponding video segments from the train part. 

The second stage is based on matching frames by embeddings. For each video we compute several embeddings then we compute the similarity between each video from the train part and the test part. After we manually assess several thousands of video segments with highest scores
for each pair of datasets. Then we extend found duplicates by either the YouTube ID or the internal dataset ID.
This means that if a video  is marked as a duplicate and a video  is not marked as a duplicate, but they have the same YouTube ID or same internal dataset ID,
we will remove  and  from the train part.
In case of LSMDC we do not have the YouTube ID, but have the name of the movie from which the video segment was taken, so if a video segment  is marked
as a duplicate, we remove all segments taken from the movie of .
The detailed description of the second stage is described in Sec.~\ref{ssec:NDVS}.

Surprisingly we found that the MSRVTT test has a significant overlap with the MSRVTT train part. This problem is relevant for the full, 1k-A and 1k-B splits.
The ActivityNet dataset suffers from the same problem.

For large datasets like HowTo100M and Kinetics700 we can not find the whole intersection, but we estimate the approximate number of videos in the intersection.
We found that HowTo100M may have about 300 (10\% of the MSRVTT full test part) video segments that can be in the MSRVTT full test part.

The similar situation is about Kinetics700 and ActivityNet datasets. Kinetics700 may have approximately 500-600 video segments (10\% of the ActivityNet test)
that may have duplicates in ActivityNet validation 1,2. Another problem with the Kinetics dataset is that many motion models are pretrained on it.

This circumstance means that researchers should carefully use HowTo100M and Kinetics700 along with MSRVTT and ActivityNet correspondingly,
because for today we don't know whether a neural network overfits for some portion of this intersection or not.

All duplicates can be considered as two groups of pairs. 
Pairs from the first group have the same videos, but different brightness, aspect ratio, size, presence/absence of a logo and so on.
The second group has pairs with quite similar videos, for example it can be the same person on the same background, doing the same things, but wearing different clothes.
We think that it is better to remove such videos from the train part to prevent overfitting. Several found examples are presented in Fig.~\ref{fig:msrvtt_dubl}.

\begin{figure*}[h!]
	\centering
	\includegraphics[width=13cm]{imgs/dup2/1_blur.jpg} 
	\includegraphics[width=13cm]{imgs/dup2/2_blur.jpg} 
	\includegraphics[width=13cm]{imgs/dup2/3_blur.jpg} 
	\includegraphics[width=13cm]{imgs/dup2/4_blur.jpg} 
	
	\caption{The left image is taken from the MSRVTT test split and the right one from MSRVTT Train. The numbers in the upper left corner represent the MSRVTT video ID.
	The faces are blurred in order to avoid legal claims.}
	\label{fig:msrvtt_dubl}
\end{figure*}








\subsection{Near duplicate video search} \label{ssec:NDVS}

\subsubsection{Approach}

In this section we explain our approach that is used to find the same or quite similar video segments in test and train parts.


Suppose we have two sets of videos  and  called the query set and the gallery set.
We want to find all pairs  where  and   have a common video segment. 

From each  and  we extract 1 frame per second. Each video is then represented by a sequence of pictures:
 and . Then a 2D pretrained neural network is used
to extract features from each image:  and .

Then we compute the matrix of cosines between the features from Q and G: .

Now each pair  is represented by the matrix:



Suppose that videos  and  are intersected at time moments  and , it is naturally to assume that
the next several seconds  and  () represent the same video segment.
Motivated by this fact we compute the mean cosine for each interval of K seconds (we use K=4):
.
The sum in the numerator is the sum of diagonal elements started with .

We define the intersection score between  as 



and the corresponding video segments as



where



Finally we sorted all  in the descending order and manually assess candidate pairs.

\subsubsection{Number of pairs to assess} \label{ssec:how_many_pairs}
Suppose we search duplicates in datasets Q and G and we have seen N pairs with the highest scores
and find M pairs with duplicates. The important question is: what is the total number of duplicates and how many
percents of them have we found.

For each pair of Q and G we construct the following test procedure. The first step is to augment Q, and let us call the result of augmentation as .
To augment a dataset we apply two transformations: 1. we randomly crop a side of each video, where each side can be 70\%--100\% of original side length
(aspect ratio can be changed); 2. we randomly shift the start of the video by a random value between 0 and 1 seconds.

Having Q,  and G we compute sets of positive and negative scores: Pos and Neg. The Pos is the set of scores between
i-th video from Q and the corresponding augmented video from . Neg is the set of scores between
each video from Q and G. Having Pos and Neg sets we can plot a curve, where  axis represents the fraction of found pairs with duplicates and Y axis
represents the number of negative pairs that we need to assess to find fraction  of positive pairs, call this curve . 
We present the algorithm that computes  using Pos and Neg sets in Lst.~\ref{lst:search_curve}.
Suppose we have seen  pairs and have found  pairs with duplicates. The total number of pairs with duplicates can be estimated as .
By the definition  connects the fraction of found positive pairs with the number of seen negative pairs. The value  represents approximation of the fraction of found
positive pairs. So if we know, that  is approximately  of positive pairs, then we can approximately compute 100\% of positive pairs as .

\begin{minipage}{\columnwidth}
\begin{lstlisting}[label=lst:search_curve, language=Python, caption={Numpy pseudocode for building the search curve },captionpos=b]
# first element is highest
P = np.sort(P)[::-1] # Pos
N = np.sort(N)[::-1] # Neg
xs = []
ys = []
for x, p in enumerate(P):
	# how many negative scores
	# greater than p ?
	j = np.searchsorted(N, p)
	xs.append(x)
	ys.append(j)
\end{lstlisting}
\end{minipage}





\subsubsection{Best 2D feature extractor}
The key component of a duplicate search system is a feature extractor. A good feature extractor significantly reduces the number of pairs for manual assessment.
To compare different 2D feature extractors we use the following test procedure. The test consists of two datasets.
The first dataset is the train part from the MSRVTT full split. The second dataset is random 596k videos
from the HowTo100M dataset. From each video of the taken part of HowTo100M we take a random 30 seconds segment.
We apply random augmentation to MSRVTT, as described in Sec.~\ref{ssec:how_many_pairs}.
Define MSRVTT as , the augmented MSRVTT dataset as  and the taken part of HowTo100M as G.
For each feature extractor we compute curve , as described in Sec.~\ref{ssec:how_many_pairs}.

The best expert has the lowest curve. For example, if we want to find 95\% of duplicates, we should see many of candidates, some of them are duplicates,
but majority of them are not. So, the value  is the approximation of how many not duplicates we need to see to find 95\% of duplicates.
Ideally , where all seen candidates are duplicates. So, a lower value  requires to see less number of false candidates, that's why
the lower curve is better.

We consider several feature extractors: resnet18 and resnet101 \cite{he2015deep} pretrained on ImageNet \cite{imagenet_cvpr09},
resnet50 pretrained on Places365 \cite{zhou2017places} and resnext101-32x8d, resnext101-32x32d,
resnext101-32x48d pretrained on one billion images from Instagram \cite{wslimageseccv2018} and finetuned on ImageNet.
We report search curves  for these pretrained networks in Fig.~\ref{fig:search_curves_all}.

There exist networks \cite{radford2learning} \cite{kordopatis2017near} trained especially for match the
duplicate frames or video segments, but they are not publicly available.

\begin{figure}[h]
	\centering
\begin{tabular}{@{}c@{}}
\begin{tikzpicture}

\definecolor{color0}{rgb}{0.886274509803922,0.290196078431373,0.2}
\definecolor{color1}{rgb}{0.203921568627451,0.541176470588235,0.741176470588235}
\definecolor{color2}{rgb}{0.596078431372549,0.556862745098039,0.835294117647059}
\definecolor{color3}{rgb}{0.984313725490196,0.756862745098039,0.368627450980392}
\definecolor{color4}{rgb}{0.556862745098039,0.729411764705882,0.258823529411765}

\begin{axis}[
axis background/.style={fill=white!89.8039215686275!black},
axis line style={white},
legend cell align={left},
legend style={fill opacity=0.8, draw opacity=1, text opacity=1, at={(0.03,0.97)}, anchor=north west, draw=white!80!black, fill=white!89.8039215686275!black},
log basis y={10},
tick align=outside,
tick pos=left,
width=1.1\columnwidth,
x grid style={white},
xmajorgrids,
xmin=-0.0487874465049929, xmax=1.04650499286733,
xtick style={color=white!33.3333333333333!black},
y grid style={white},
ymajorgrids,
ymin=1897.01146982173, ymax=4930647.80935615,
ymode=log,
ytick style={color=white!33.3333333333333!black}
]
\addplot [semithick, color0]
table {0.000998573466476462 3762
0.00171184022824536 4073
0.00242510699001427 4108
0.00313837375178317 4169
0.00385164051355207 4219
0.00456490727532097 4255
0.00527817403708987 4274
0.00599144079885877 4311
0.00670470756062767 4342
0.00741797432239658 4375
0.00813124108416548 4426
0.00884450784593438 4433
0.00955777460770328 4450
0.0102710413694722 4458
0.0109843081312411 4469
0.01169757489301 4510
0.0124108416547789 4521
0.0131241084165478 4544
0.0138373751783167 4561
0.0145506419400856 4572
0.0152639087018545 4579
0.0159771754636234 4594
0.0166904422253923 4610
0.0174037089871612 4635
0.0181169757489301 4654
0.018830242510699 4684
0.0195435092724679 4705
0.0202567760342368 4724
0.0209700427960057 4756
0.0216833095577746 4778
0.0223965763195435 4829
0.0231098430813124 4837
0.0238231098430813 4855
0.0245363766048502 4888
0.0252496433666191 4905
0.025962910128388 4922
0.0266761768901569 4931
0.0273894436519258 4944
0.0281027104136947 4953
0.0288159771754636 4998
0.0295292439372325 5015
0.0302425106990014 5024
0.0309557774607703 5031
0.0316690442225392 5041
0.0323823109843081 5058
0.033095577746077 5068
0.0338088445078459 5084
0.0345221112696148 5099
0.0352353780313837 5112
0.0359486447931526 5131
0.0366619115549215 5153
0.0373751783166904 5176
0.0380884450784593 5201
0.0388017118402282 5228
0.0395149786019971 5259
0.040228245363766 5269
0.0409415121255349 5282
0.0416547788873039 5296
0.0423680456490728 5324
0.0430813124108417 5343
0.0437945791726106 5360
0.0445078459343795 5392
0.0452211126961484 5401
0.0459343794579173 5426
0.0466476462196862 5435
0.0473609129814551 5459
0.048074179743224 5477
0.0487874465049929 5500
0.0495007132667618 5525
0.0502139800285307 5530
0.0509272467902996 5543
0.0516405135520685 5580
0.0523537803138374 5598
0.0530670470756063 5629
0.0537803138373752 5648
0.0544935805991441 5663
0.055206847360913 5689
0.0559201141226819 5716
0.0566333808844508 5732
0.0573466476462197 5749
0.0580599144079886 5757
0.0587731811697575 5766
0.0594864479315264 5773
0.0601997146932953 5788
0.0609129814550642 5812
0.0616262482168331 5825
0.062339514978602 5884
0.0630527817403709 5918
0.0637660485021398 5929
0.0644793152639087 5966
0.0651925820256776 5995
0.0659058487874465 6004
0.0666191155492154 6040
0.0673323823109843 6049
0.0680456490727532 6073
0.0687589158345221 6085
0.069472182596291 6122
0.0701854493580599 6150
0.0708987161198288 6160
0.0716119828815977 6174
0.0723252496433666 6198
0.0730385164051355 6219
0.0737517831669044 6250
0.0744650499286733 6264
0.0751783166904422 6276
0.0758915834522111 6307
0.07660485021398 6315
0.0773181169757489 6327
0.0780313837375178 6332
0.0787446504992867 6335
0.0794579172610556 6347
0.0801711840228245 6365
0.0808844507845934 6372
0.0815977175463623 6408
0.0823109843081312 6441
0.0830242510699001 6492
0.0837375178316691 6523
0.084450784593438 6553
0.0851640513552068 6568
0.0858773181169757 6587
0.0865905848787446 6610
0.0873038516405135 6619
0.0880171184022824 6651
0.0887303851640514 6670
0.0894436519258203 6688
0.0901569186875892 6711
0.0908701854493581 6761
0.091583452211127 6817
0.0922967189728959 6840
0.0930099857346648 6845
0.0937232524964337 6866
0.0944365192582026 6897
0.0951497860199715 6923
0.0958630527817404 6949
0.0965763195435093 6964
0.0972895863052782 6981
0.0980028530670471 6985
0.098716119828816 6997
0.0994293865905849 7007
0.100142653352354 7020
0.100855920114123 7036
0.101569186875892 7041
0.10228245363766 7061
0.102995720399429 7096
0.103708987161198 7119
0.104422253922967 7124
0.105135520684736 7129
0.105848787446505 7149
0.106562054208274 7158
0.107275320970043 7208
0.107988587731812 7235
0.108701854493581 7255
0.109415121255349 7266
0.110128388017118 7283
0.110841654778887 7319
0.111554921540656 7335
0.112268188302425 7363
0.112981455064194 7374
0.113694721825963 7388
0.114407988587732 7423
0.115121255349501 7432
0.11583452211127 7455
0.116547788873039 7466
0.117261055634807 7482
0.117974322396576 7492
0.118687589158345 7521
0.119400855920114 7527
0.120114122681883 7554
0.120827389443652 7597
0.121540656205421 7626
0.12225392296719 7639
0.122967189728959 7670
0.123680456490728 7695
0.124393723252496 7722
0.125106990014265 7732
0.125820256776034 7745
0.126533523537803 7775
0.127246790299572 7799
0.127960057061341 7824
0.12867332382311 7838
0.129386590584879 7850
0.130099857346648 7863
0.130813124108417 7870
0.131526390870185 7884
0.132239657631954 7892
0.132952924393723 7898
0.133666191155492 7928
0.134379457917261 7945
0.13509272467903 7955
0.135805991440799 7975
0.136519258202568 7999
0.137232524964337 8026
0.137945791726106 8033
0.138659058487874 8048
0.139372325249643 8074
0.140085592011412 8094
0.140798858773181 8132
0.14151212553495 8148
0.142225392296719 8165
0.142938659058488 8218
0.143651925820257 8242
0.144365192582026 8261
0.145078459343795 8284
0.145791726105563 8311
0.146504992867332 8334
0.147218259629101 8361
0.14793152639087 8409
0.148644793152639 8427
0.149358059914408 8448
0.150071326676177 8482
0.150784593437946 8490
0.151497860199715 8519
0.152211126961484 8549
0.152924393723253 8589
0.153637660485021 8618
0.15435092724679 8621
0.155064194008559 8637
0.155777460770328 8653
0.156490727532097 8667
0.157203994293866 8688
0.157917261055635 8699
0.158630527817404 8726
0.159343794579173 8753
0.160057061340942 8762
0.16077032810271 8775
0.161483594864479 8782
0.162196861626248 8799
0.162910128388017 8824
0.163623395149786 8856
0.164336661911555 8875
0.165049928673324 8905
0.165763195435093 8926
0.166476462196862 8955
0.167189728958631 8969
0.167902995720399 9031
0.168616262482168 9040
0.169329529243937 9098
0.170042796005706 9145
0.170756062767475 9151
0.171469329529244 9166
0.172182596291013 9187
0.172895863052782 9211
0.173609129814551 9235
0.17432239657632 9247
0.175035663338088 9289
0.175748930099857 9298
0.176462196861626 9329
0.177175463623395 9353
0.177888730385164 9375
0.178601997146933 9402
0.179315263908702 9422
0.180028530670471 9446
0.18074179743224 9480
0.181455064194009 9502
0.182168330955777 9517
0.182881597717546 9531
0.183594864479315 9542
0.184308131241084 9578
0.185021398002853 9632
0.185734664764622 9637
0.186447931526391 9660
0.18716119828816 9687
0.187874465049929 9701
0.188587731811698 9715
0.189300998573466 9731
0.190014265335235 9753
0.190727532097004 9791
0.191440798858773 9824
0.192154065620542 9830
0.192867332382311 9854
0.19358059914408 9866
0.194293865905849 9893
0.195007132667618 9922
0.195720399429387 9956
0.196433666191155 9992
0.197146932952924 10005
0.197860199714693 10019
0.198573466476462 10054
0.199286733238231 10076
0.2 10103
0.200713266761769 10118
0.201426533523538 10160
0.202139800285307 10179
0.202853067047076 10207
0.203566333808845 10243
0.204279600570613 10264
0.204992867332382 10292
0.205706134094151 10326
0.20641940085592 10356
0.207132667617689 10372
0.207845934379458 10390
0.208559201141227 10414
0.209272467902996 10440
0.209985734664765 10475
0.210699001426534 10488
0.211412268188302 10512
0.212125534950071 10539
0.21283880171184 10578
0.213552068473609 10598
0.214265335235378 10642
0.214978601997147 10653
0.215691868758916 10664
0.216405135520685 10685
0.217118402282454 10701
0.217831669044223 10750
0.218544935805991 10792
0.21925820256776 10830
0.219971469329529 10844
0.220684736091298 10867
0.221398002853067 10886
0.222111269614836 10908
0.222824536376605 10934
0.223537803138374 10954
0.224251069900143 10986
0.224964336661912 11015
0.22567760342368 11062
0.226390870185449 11082
0.227104136947218 11090
0.227817403708987 11125
0.228530670470756 11146
0.229243937232525 11172
0.229957203994294 11185
0.230670470756063 11225
0.231383737517832 11237
0.232097004279601 11291
0.232810271041369 11305
0.233523537803138 11327
0.234236804564907 11359
0.234950071326676 11378
0.235663338088445 11409
0.236376604850214 11418
0.237089871611983 11442
0.237803138373752 11452
0.238516405135521 11473
0.23922967189729 11498
0.239942938659058 11519
0.240656205420827 11563
0.241369472182596 11579
0.242082738944365 11599
0.242796005706134 11613
0.243509272467903 11632
0.244222539229672 11646
0.244935805991441 11660
0.24564907275321 11705
0.246362339514979 11739
0.247075606276748 11750
0.247788873038516 11764
0.248502139800285 11789
0.249215406562054 11813
0.249928673323823 11832
0.250641940085592 11862
0.251355206847361 11876
0.25206847360913 11890
0.252781740370899 11935
0.253495007132668 11988
0.254208273894437 11999
0.254921540656205 12028
0.255634807417974 12060
0.256348074179743 12065
0.257061340941512 12094
0.257774607703281 12108
0.25848787446505 12153
0.259201141226819 12191
0.259914407988588 12213
0.260627674750357 12253
0.261340941512126 12286
0.262054208273894 12295
0.262767475035663 12302
0.263480741797432 12311
0.264194008559201 12342
0.26490727532097 12359
0.265620542082739 12379
0.266333808844508 12401
0.267047075606277 12455
0.267760342368046 12483
0.268473609129815 12496
0.269186875891583 12508
0.269900142653352 12545
0.270613409415121 12568
0.27132667617689 12599
0.272039942938659 12614
0.272753209700428 12632
0.273466476462197 12646
0.274179743223966 12711
0.274893009985735 12718
0.275606276747504 12741
0.276319543509272 12763
0.277032810271041 12786
0.27774607703281 12810
0.278459343794579 12859
0.279172610556348 12905
0.279885877318117 12935
0.280599144079886 12946
0.281312410841655 12968
0.282025677603424 13008
0.282738944365193 13032
0.283452211126962 13058
0.28416547788873 13084
0.284878744650499 13103
0.285592011412268 13118
0.286305278174037 13129
0.287018544935806 13150
0.287731811697575 13176
0.288445078459344 13197
0.289158345221113 13227
0.289871611982882 13259
0.29058487874465 13289
0.291298145506419 13307
0.292011412268188 13326
0.292724679029957 13339
0.293437945791726 13352
0.294151212553495 13364
0.294864479315264 13387
0.295577746077033 13426
0.296291012838802 13466
0.297004279600571 13486
0.297717546362339 13508
0.298430813124108 13552
0.299144079885877 13563
0.299857346647646 13630
0.300570613409415 13646
0.301283880171184 13661
0.301997146932953 13684
0.302710413694722 13705
0.303423680456491 13717
0.30413694721826 13719
0.304850213980029 13735
0.305563480741797 13757
0.306276747503566 13803
0.306990014265335 13851
0.307703281027104 13867
0.308416547788873 13870
0.309129814550642 13909
0.309843081312411 13964
0.31055634807418 13992
0.311269614835949 14012
0.311982881597718 14032
0.312696148359486 14066
0.313409415121255 14087
0.314122681883024 14110
0.314835948644793 14135
0.315549215406562 14182
0.316262482168331 14216
0.3169757489301 14226
0.317689015691869 14292
0.318402282453638 14311
0.319115549215407 14357
0.319828815977175 14397
0.320542082738944 14413
0.321255349500713 14444
0.321968616262482 14464
0.322681883024251 14500
0.32339514978602 14518
0.324108416547789 14544
0.324821683309558 14579
0.325534950071327 14614
0.326248216833096 14641
0.326961483594864 14673
0.327674750356633 14699
0.328388017118402 14730
0.329101283880171 14756
0.32981455064194 14784
0.330527817403709 14802
0.331241084165478 14812
0.331954350927247 14824
0.332667617689016 14847
0.333380884450785 14870
0.334094151212553 14915
0.334807417974322 14956
0.335520684736091 14978
0.33623395149786 15032
0.336947218259629 15048
0.337660485021398 15076
0.338373751783167 15104
0.339087018544936 15129
0.339800285306705 15132
0.340513552068474 15181
0.341226818830243 15213
0.341940085592011 15230
0.34265335235378 15255
0.343366619115549 15275
0.344079885877318 15296
0.344793152639087 15332
0.345506419400856 15363
0.346219686162625 15464
0.346932952924394 15482
0.347646219686163 15505
0.348359486447932 15520
0.3490727532097 15565
0.349786019971469 15599
0.350499286733238 15622
0.351212553495007 15674
0.351925820256776 15687
0.352639087018545 15709
0.353352353780314 15723
0.354065620542083 15742
0.354778887303852 15757
0.355492154065621 15792
0.356205420827389 15826
0.356918687589158 15878
0.357631954350927 15895
0.358345221112696 15965
0.359058487874465 15999
0.359771754636234 16023
0.360485021398003 16037
0.361198288159772 16066
0.361911554921541 16100
0.36262482168331 16119
0.363338088445078 16134
0.364051355206847 16193
0.364764621968616 16218
0.365477888730385 16247
0.366191155492154 16274
0.366904422253923 16285
0.367617689015692 16329
0.368330955777461 16364
0.36904422253923 16401
0.369757489300999 16416
0.370470756062767 16450
0.371184022824536 16477
0.371897289586305 16486
0.372610556348074 16546
0.373323823109843 16571
0.374037089871612 16589
0.374750356633381 16645
0.37546362339515 16665
0.376176890156919 16686
0.376890156918688 16709
0.377603423680457 16758
0.378316690442225 16773
0.379029957203994 16828
0.379743223965763 16861
0.380456490727532 16890
0.381169757489301 16954
0.38188302425107 16965
0.382596291012839 17001
0.383309557774608 17052
0.384022824536377 17070
0.384736091298146 17123
0.385449358059914 17164
0.386162624821683 17191
0.386875891583452 17204
0.387589158345221 17230
0.38830242510699 17247
0.389015691868759 17305
0.389728958630528 17323
0.390442225392297 17343
0.391155492154066 17389
0.391868758915835 17410
0.392582025677603 17428
0.393295292439372 17466
0.394008559201141 17483
0.39472182596291 17500
0.395435092724679 17578
0.396148359486448 17599
0.396861626248217 17612
0.397574893009986 17656
0.398288159771755 17703
0.399001426533524 17731
0.399714693295292 17825
0.400427960057061 17862
0.40114122681883 17902
0.401854493580599 17922
0.402567760342368 18329
0.403281027104137 18377
0.403994293865906 18391
0.404707560627675 18408
0.405420827389444 18455
0.406134094151213 18468
0.406847360912981 18515
0.40756062767475 18541
0.408273894436519 18578
0.408987161198288 18618
0.409700427960057 18656
0.410413694721826 18691
0.411126961483595 19096
0.411840228245364 19202
0.412553495007133 19234
0.413266761768902 19246
0.41398002853067 19308
0.414693295292439 19339
0.415406562054208 19407
0.416119828815977 19438
0.416833095577746 19464
0.417546362339515 19534
0.418259629101284 19555
0.418972895863053 19593
0.419686162624822 19629
0.420399429386591 19670
0.421112696148359 19711
0.421825962910128 19745
0.422539229671897 19759
0.423252496433666 19787
0.423965763195435 19807
0.424679029957204 19821
0.425392296718973 19881
0.426105563480742 19904
0.426818830242511 19940
0.42753209700428 19952
0.428245363766048 19988
0.428958630527817 20020
0.429671897289586 20053
0.430385164051355 20066
0.431098430813124 20117
0.431811697574893 20139
0.432524964336662 20185
0.433238231098431 20212
0.4339514978602 20287
0.434664764621969 20350
0.435378031383738 20373
0.436091298145506 20409
0.436804564907275 20441
0.437517831669044 20458
0.438231098430813 20526
0.438944365192582 20533
0.439657631954351 20562
0.44037089871612 20629
0.441084165477889 20672
0.441797432239658 20719
0.442510699001427 20736
0.443223965763195 20755
0.443937232524964 20874
0.444650499286733 20895
0.445363766048502 20929
0.446077032810271 20957
0.44679029957204 21003
0.447503566333809 21068
0.448216833095578 21094
0.448930099857347 21123
0.449643366619116 21140
0.450356633380884 21172
0.451069900142653 21199
0.451783166904422 21574
0.452496433666191 21602
0.45320970042796 21637
0.453922967189729 21694
0.454636233951498 21726
0.455349500713267 21766
0.456062767475036 21825
0.456776034236805 21854
0.457489300998573 21902
0.458202567760342 21910
0.458915834522111 21941
0.45962910128388 22009
0.460342368045649 22028
0.461055634807418 22081
0.461768901569187 22142
0.462482168330956 22196
0.463195435092725 22212
0.463908701854494 22247
0.464621968616262 22288
0.465335235378031 22320
0.4660485021398 22347
0.466761768901569 22372
0.467475035663338 22403
0.468188302425107 22430
0.468901569186876 22440
0.469614835948645 22458
0.470328102710414 22462
0.471041369472183 22478
0.471754636233951 22510
0.47246790299572 22533
0.473181169757489 22567
0.473894436519258 22602
0.474607703281027 22615
0.475320970042796 22653
0.476034236804565 22664
0.476747503566334 22697
0.477460770328103 22712
0.478174037089872 22734
0.478887303851641 22762
0.479600570613409 22772
0.480313837375178 22813
0.481027104136947 22828
0.481740370898716 22850
0.482453637660485 22892
0.483166904422254 22923
0.483880171184023 22950
0.484593437945792 22993
0.485306704707561 23065
0.48601997146933 23107
0.486733238231098 23117
0.487446504992867 23153
0.488159771754636 23208
0.488873038516405 23242
0.489586305278174 23330
0.490299572039943 23344
0.491012838801712 23365
0.491726105563481 23416
0.49243937232525 23442
0.493152639087019 23494
0.493865905848787 23531
0.494579172610556 23574
0.495292439372325 23618
0.496005706134094 23653
0.496718972895863 23691
0.497432239657632 23702
0.498145506419401 23742
0.49885877318117 23799
0.499572039942939 23816
0.500285306704708 23859
0.500998573466476 23909
0.501711840228245 23936
0.502425106990014 23967
0.503138373751783 23990
0.503851640513552 24005
0.504564907275321 24036
0.50527817403709 24124
0.505991440798859 24129
0.506704707560628 24144
0.507417974322397 24163
0.508131241084166 24197
0.508844507845934 24223
0.509557774607703 24249
0.510271041369472 24283
0.510984308131241 24325
0.51169757489301 24343
0.512410841654779 24377
0.513124108416548 24496
0.513837375178317 24511
0.514550641940086 24556
0.515263908701854 24574
0.515977175463623 24589
0.516690442225392 24626
0.517403708987161 24677
0.51811697574893 24793
0.518830242510699 24845
0.519543509272468 24886
0.520256776034237 24898
0.520970042796006 24957
0.521683309557775 25366
0.522396576319544 25385
0.523109843081312 25413
0.523823109843081 25494
0.52453637660485 25517
0.525249643366619 25560
0.525962910128388 25613
0.526676176890157 25639
0.527389443651926 25698
0.528102710413695 25750
0.528815977175464 25777
0.529529243937233 25821
0.530242510699001 25860
0.53095577746077 26235
0.531669044222539 26280
0.532382310984308 26308
0.533095577746077 26332
0.533808844507846 26382
0.534522111269615 26443
0.535235378031384 26465
0.535948644793153 26508
0.536661911554921 26546
0.53737517831669 26602
0.538088445078459 26647
0.538801711840228 26672
0.539514978601997 26750
0.540228245363766 26771
0.540941512125535 26812
0.541654778887304 26837
0.542368045649073 26952
0.543081312410842 26992
0.543794579172611 27039
0.54450784593438 27062
0.545221112696148 27063
0.545934379457917 27113
0.546647646219686 27143
0.547360912981455 27179
0.548074179743224 27198
0.548787446504993 27256
0.549500713266762 27306
0.550213980028531 27340
0.5509272467903 27400
0.551640513552068 27454
0.552353780313837 27480
0.553067047075606 27509
0.553780313837375 27559
0.554493580599144 27570
0.555206847360913 27601
0.555920114122682 27628
0.556633380884451 27669
0.55734664764622 27769
0.558059914407989 27809
0.558773181169757 27844
0.559486447931526 27888
0.560199714693295 27931
0.560912981455064 27951
0.561626248216833 27993
0.562339514978602 28072
0.563052781740371 28120
0.56376604850214 28189
0.564479315263909 28268
0.565192582025678 28307
0.565905848787447 28379
0.566619115549215 28466
0.567332382310984 28475
0.568045649072753 28496
0.568758915834522 28591
0.569472182596291 28644
0.57018544935806 28699
0.570898716119829 28715
0.571611982881598 28738
0.572325249643367 28840
0.573038516405135 28909
0.573751783166904 28957
0.574465049928673 28981
0.575178316690442 29009
0.575891583452211 29074
0.57660485021398 29100
0.577318116975749 29118
0.578031383737518 29154
0.578744650499287 29190
0.579457917261056 29244
0.580171184022825 29278
0.580884450784593 29311
0.581597717546362 29356
0.582310984308131 29406
0.5830242510699 29452
0.583737517831669 29478
0.584450784593438 29528
0.585164051355207 29556
0.585877318116976 29593
0.586590584878745 29650
0.587303851640514 29685
0.588017118402282 29720
0.588730385164051 29787
0.58944365192582 29801
0.590156918687589 29846
0.590870185449358 29902
0.591583452211127 29951
0.592296718972896 29989
0.593009985734665 30032
0.593723252496434 30063
0.594436519258203 30111
0.595149786019971 30183
0.59586305278174 30218
0.596576319543509 30260
0.597289586305278 30293
0.598002853067047 30338
0.598716119828816 30398
0.599429386590585 30432
0.600142653352354 30474
0.600855920114123 30509
0.601569186875892 30562
0.602282453637661 30592
0.602995720399429 30632
0.603708987161198 30658
0.604422253922967 30724
0.605135520684736 30812
0.605848787446505 30844
0.606562054208274 30868
0.607275320970043 30892
0.607988587731812 31005
0.608701854493581 31028
0.609415121255349 31088
0.610128388017118 31169
0.610841654778887 31219
0.611554921540656 31286
0.612268188302425 31329
0.612981455064194 31383
0.613694721825963 31426
0.614407988587732 31455
0.615121255349501 31486
0.61583452211127 31541
0.616547788873039 31589
0.617261055634807 31627
0.617974322396576 31781
0.618687589158345 31871
0.619400855920114 31884
0.620114122681883 31943
0.620827389443652 31966
0.621540656205421 32021
0.62225392296719 32418
0.622967189728959 32528
0.623680456490728 32625
0.624393723252496 32657
0.625106990014265 32756
0.625820256776034 32798
0.626533523537803 32904
0.627246790299572 32987
0.627960057061341 33042
0.62867332382311 33143
0.629386590584879 33172
0.630099857346648 33214
0.630813124108417 33288
0.631526390870185 33336
0.632239657631954 33378
0.632952924393723 33418
0.633666191155492 33476
0.634379457917261 33519
0.63509272467903 33582
0.635805991440799 33630
0.636519258202568 33690
0.637232524964337 33761
0.637945791726106 33788
0.638659058487875 33831
0.639372325249643 33940
0.640085592011412 33957
0.640798858773181 33989
0.64151212553495 34014
0.642225392296719 34047
0.642938659058488 34066
0.643651925820257 34072
0.644365192582026 34144
0.645078459343795 34215
0.645791726105563 34260
0.646504992867332 34288
0.647218259629101 34345
0.64793152639087 34439
0.648644793152639 34474
0.649358059914408 34531
0.650071326676177 34563
0.650784593437946 34607
0.651497860199715 34709
0.652211126961484 34820
0.652924393723253 34888
0.653637660485021 34931
0.65435092724679 34976
0.655064194008559 35065
0.655777460770328 35169
0.656490727532097 35308
0.657203994293866 35369
0.657917261055635 35464
0.658630527817404 35505
0.659343794579173 35574
0.660057061340942 35672
0.66077032810271 35702
0.661483594864479 35843
0.662196861626248 35873
0.662910128388017 35903
0.663623395149786 35972
0.664336661911555 36078
0.665049928673324 36152
0.665763195435093 36207
0.666476462196862 36246
0.66718972895863 36348
0.667902995720399 36372
0.668616262482168 36455
0.669329529243937 36502
0.670042796005706 36556
0.670756062767475 36600
0.671469329529244 36641
0.672182596291013 36703
0.672895863052782 36765
0.673609129814551 36798
0.67432239657632 36833
0.675035663338088 36870
0.675748930099857 36910
0.676462196861626 37010
0.677175463623395 37056
0.677888730385164 37071
0.678601997146933 37122
0.679315263908702 37201
0.680028530670471 37222
0.68074179743224 37298
0.681455064194009 37433
0.682168330955777 37485
0.682881597717546 37562
0.683594864479315 37600
0.684308131241084 37656
0.685021398002853 37705
0.685734664764622 37746
0.686447931526391 37807
0.68716119828816 37903
0.687874465049929 37932
0.688587731811698 38005
0.689300998573466 38041
0.690014265335235 38101
0.690727532097004 38198
0.691440798858773 38505
0.692154065620542 38627
0.692867332382311 38751
0.69358059914408 38821
0.694293865905849 38875
0.695007132667618 38912
0.695720399429387 38959
0.696433666191156 39058
0.697146932952924 39102
0.697860199714693 39166
0.698573466476462 39233
0.699286733238231 39309
0.7 39384
0.700713266761769 39429
0.701426533523538 39447
0.702139800285307 39495
0.702853067047076 39619
0.703566333808844 39664
0.704279600570613 39692
0.704992867332382 39743
0.705706134094151 39774
0.70641940085592 39823
0.707132667617689 39916
0.707845934379458 39942
0.708559201141227 40056
0.709272467902996 40095
0.709985734664765 40178
0.710699001426534 40270
0.711412268188302 40357
0.712125534950071 40497
0.71283880171184 40543
0.713552068473609 40683
0.714265335235378 40790
0.714978601997147 40808
0.715691868758916 40831
0.716405135520685 40973
0.717118402282454 41050
0.717831669044223 41126
0.718544935805991 41287
0.71925820256776 41338
0.719971469329529 41372
0.720684736091298 41448
0.721398002853067 41498
0.722111269614836 41541
0.722824536376605 41615
0.723537803138374 41677
0.724251069900143 41793
0.724964336661912 41905
0.72567760342368 41976
0.726390870185449 42066
0.727104136947218 42114
0.727817403708987 42148
0.728530670470756 42269
0.729243937232525 42361
0.729957203994294 42487
0.730670470756063 42606
0.731383737517832 42697
0.732097004279601 42783
0.732810271041369 42881
0.733523537803138 42973
0.734236804564907 43047
0.734950071326676 43202
0.735663338088445 43297
0.736376604850214 43391
0.737089871611983 43452
0.737803138373752 43498
0.738516405135521 43533
0.73922967189729 43577
0.739942938659058 43624
0.740656205420827 43669
0.741369472182596 43732
0.742082738944365 43806
0.742796005706134 43831
0.743509272467903 43935
0.744222539229672 44005
0.744935805991441 44044
0.74564907275321 44108
0.746362339514979 44206
0.747075606276748 44285
0.747788873038516 44360
0.748502139800285 44403
0.749215406562054 44427
0.749928673323823 44562
0.750641940085592 44677
0.751355206847361 44756
0.75206847360913 44806
0.752781740370899 44823
0.753495007132668 45034
0.754208273894437 45124
0.754921540656205 45167
0.755634807417974 45217
0.756348074179743 45262
0.757061340941512 45309
0.757774607703281 45390
0.75848787446505 45461
0.759201141226819 45494
0.759914407988588 45686
0.760627674750357 45747
0.761340941512126 45871
0.762054208273894 45948
0.762767475035663 46027
0.763480741797432 46082
0.764194008559201 46152
0.76490727532097 46195
0.765620542082739 46302
0.766333808844508 46409
0.767047075606277 46820
0.767760342368046 46949
0.768473609129815 47129
0.769186875891583 47235
0.769900142653352 47349
0.770613409415121 47468
0.77132667617689 47549
0.772039942938659 47704
0.772753209700428 47811
0.773466476462197 47886
0.774179743223966 47949
0.774893009985735 48008
0.775606276747504 48119
0.776319543509272 48154
0.777032810271041 48363
0.77774607703281 48421
0.778459343794579 48542
0.779172610556348 48618
0.779885877318117 48793
0.780599144079886 48826
0.781312410841655 48954
0.782025677603424 49363
0.782738944365193 49473
0.783452211126962 49563
0.78416547788873 49732
0.784878744650499 49777
0.785592011412268 49850
0.786305278174037 49986
0.787018544935806 50061
0.787731811697575 50180
0.788445078459344 50464
0.789158345221113 50517
0.789871611982882 50575
0.790584878744651 50712
0.791298145506419 50935
0.792011412268188 51055
0.792724679029957 51105
0.793437945791726 51256
0.794151212553495 51318
0.794864479315264 51439
0.795577746077033 51501
0.796291012838802 51819
0.797004279600571 51899
0.797717546362339 52376
0.798430813124108 52458
0.799144079885877 52554
0.799857346647646 52742
0.800570613409415 52891
0.801283880171184 52973
0.801997146932953 53284
0.802710413694722 53307
0.803423680456491 53442
0.80413694721826 53588
0.804850213980029 53710
0.805563480741797 53758
0.806276747503566 53866
0.806990014265335 53949
0.807703281027104 54026
0.808416547788873 54129
0.809129814550642 54286
0.809843081312411 54346
0.81055634807418 54512
0.811269614835949 54705
0.811982881597718 54811
0.812696148359486 54986
0.813409415121255 55062
0.814122681883024 55214
0.814835948644793 55375
0.815549215406562 55469
0.816262482168331 55561
0.8169757489301 55713
0.817689015691869 55799
0.818402282453638 55933
0.819115549215407 56016
0.819828815977175 56113
0.820542082738944 56249
0.821255349500713 56341
0.821968616262482 56480
0.822681883024251 56595
0.82339514978602 56671
0.824108416547789 56715
0.824821683309558 56780
0.825534950071327 56932
0.826248216833096 57093
0.826961483594864 57306
0.827674750356633 57450
0.828388017118402 57627
0.829101283880171 57762
0.82981455064194 57834
0.830527817403709 58040
0.831241084165478 58155
0.831954350927247 58253
0.832667617689016 58340
0.833380884450785 58461
0.834094151212553 58567
0.834807417974322 58662
0.835520684736091 58868
0.83623395149786 58961
0.836947218259629 59053
0.837660485021398 59218
0.838373751783167 59322
0.839087018544936 59618
0.839800285306705 59720
0.840513552068474 59830
0.841226818830243 59954
0.841940085592011 60182
0.84265335235378 60430
0.843366619115549 60651
0.844079885877318 60831
0.844793152639087 60929
0.845506419400856 61193
0.846219686162625 61351
0.846932952924394 61395
0.847646219686163 61496
0.848359486447932 61695
0.8490727532097 61936
0.849786019971469 62061
0.850499286733238 62211
0.851212553495007 62299
0.851925820256776 62398
0.852639087018545 62592
0.853352353780314 62740
0.854065620542083 62905
0.854778887303852 63044
0.855492154065621 63242
0.856205420827389 63346
0.856918687589158 63592
0.857631954350927 63778
0.858345221112696 63953
0.859058487874465 64037
0.859771754636234 64240
0.860485021398003 64360
0.861198288159772 64442
0.861911554921541 64601
0.86262482168331 64811
0.863338088445078 64925
0.864051355206847 65292
0.864764621968616 65825
0.865477888730385 66188
0.866191155492154 66294
0.866904422253923 66627
0.867617689015692 66727
0.868330955777461 67147
0.86904422253923 67303
0.869757489300999 67535
0.870470756062767 67677
0.871184022824536 67760
0.871897289586305 67869
0.872610556348074 68049
0.873323823109843 68218
0.874037089871612 68321
0.874750356633381 68376
0.87546362339515 68442
0.876176890156919 68827
0.876890156918688 68913
0.877603423680457 69022
0.878316690442225 69148
0.879029957203994 69416
0.879743223965763 69668
0.880456490727532 69964
0.881169757489301 70180
0.88188302425107 70307
0.882596291012839 70418
0.883309557774608 70533
0.884022824536377 70713
0.884736091298145 70963
0.885449358059914 71259
0.886162624821683 71445
0.886875891583452 71579
0.887589158345221 71764
0.88830242510699 72038
0.889015691868759 72297
0.889728958630528 72481
0.890442225392297 72789
0.891155492154066 73182
0.891868758915835 73369
0.892582025677603 73518
0.893295292439372 73590
0.894008559201141 73876
0.89472182596291 74073
0.895435092724679 74407
0.896148359486448 74655
0.896861626248217 75043
0.897574893009986 75661
0.898288159771755 76094
0.899001426533524 76347
0.899714693295292 76568
0.900427960057061 76797
0.90114122681883 77122
0.901854493580599 77254
0.902567760342368 77409
0.903281027104137 77778
0.903994293865906 78349
0.904707560627675 78568
0.905420827389444 78791
0.906134094151213 78965
0.906847360912981 79457
0.90756062767475 79720
0.908273894436519 79943
0.908987161198288 80168
0.909700427960057 80398
0.910413694721826 80610
0.911126961483595 81215
0.911840228245364 81640
0.912553495007133 81821
0.913266761768902 82323
0.913980028530671 82668
0.914693295292439 82876
0.915406562054208 83469
0.916119828815977 84053
0.916833095577746 84620
0.917546362339515 85023
0.918259629101284 85251
0.918972895863053 85478
0.919686162624822 85928
0.920399429386591 86183
0.921112696148359 86461
0.921825962910128 86835
0.922539229671897 87621
0.923252496433666 87677
0.923965763195435 87789
0.924679029957204 88275
0.925392296718973 88485
0.926105563480742 89226
0.926818830242511 89831
0.92753209700428 89925
0.928245363766048 90101
0.928958630527817 90890
0.929671897289586 91313
0.930385164051355 91583
0.931098430813124 92038
0.931811697574893 92498
0.932524964336662 93242
0.933238231098431 93642
0.9339514978602 93992
0.934664764621969 94621
0.935378031383738 95231
0.936091298145506 96081
0.936804564907275 96300
0.937517831669044 96790
0.938231098430813 97450
0.938944365192582 97904
0.939657631954351 98450
0.94037089871612 98816
0.941084165477889 99767
0.941797432239658 100018
0.942510699001427 100715
0.943223965763195 101065
0.943937232524964 101464
0.944650499286733 102073
0.945363766048502 102852
0.946077032810271 103194
0.94679029957204 104152
0.947503566333809 104723
0.948216833095578 105403
0.948930099857347 106167
0.949643366619116 106874
0.950356633380884 108369
0.951069900142653 108602
0.951783166904422 108861
0.952496433666191 109188
0.95320970042796 109499
0.953922967189729 110190
0.954636233951498 110616
0.955349500713267 111793
0.956062767475036 112464
0.956776034236805 113466
0.957489300998573 114633
0.958202567760342 115351
0.958915834522111 116613
0.95962910128388 117601
0.960342368045649 118967
0.961055634807418 120385
0.961768901569187 121305
0.962482168330956 122684
0.963195435092725 123271
0.963908701854494 124523
0.964621968616262 124786
0.965335235378031 125777
0.9660485021398 127112
0.966761768901569 128715
0.967475035663338 129818
0.968188302425107 130958
0.968901569186876 132570
0.969614835948645 133474
0.970328102710414 134870
0.971041369472183 135691
0.971754636233952 137113
0.97246790299572 138268
0.973181169757489 142267
0.973894436519258 145147
0.974607703281027 146272
0.975320970042796 149229
0.976034236804565 151809
0.976747503566334 152611
0.977460770328103 153417
0.978174037089872 155760
0.97888730385164 158294
0.979600570613409 159709
0.980313837375178 161716
0.981027104136947 165030
0.981740370898716 169331
0.982453637660485 172625
0.983166904422254 173930
0.983880171184023 176495
0.984593437945792 180135
0.985306704707561 182632
0.98601997146933 183525
0.986733238231098 188001
0.987446504992867 195437
0.988159771754636 203124
0.988873038516405 211976
0.989586305278174 218950
0.990299572039943 229739
0.991012838801712 236868
0.991726105563481 246023
0.99243937232525 247567
0.993152639087019 263976
0.993865905848787 267742
0.994579172610556 280279
0.995292439372325 309989
0.996005706134094 337001
0.996718972895863 366932
};
\addlegendentry{resnet18-imagenet}
\addplot [semithick, color1]
table {0.000998573466476462 3537
0.00171184022824536 3760
0.00242510699001427 3775
0.00313837375178317 3807
0.00385164051355207 3815
0.00456490727532097 3853
0.00527817403708987 3863
0.00599144079885877 3875
0.00670470756062767 3877
0.00741797432239658 3880
0.00813124108416548 3892
0.00884450784593438 3901
0.00955777460770328 3901
0.0102710413694722 3919
0.0109843081312411 3932
0.01169757489301 3941
0.0124108416547789 3946
0.0131241084165478 3954
0.0138373751783167 3956
0.0145506419400856 3975
0.0152639087018545 3990
0.0159771754636234 3991
0.0166904422253923 3999
0.0174037089871612 4007
0.0181169757489301 4012
0.018830242510699 4022
0.0195435092724679 4029
0.0202567760342368 4032
0.0209700427960057 4034
0.0216833095577746 4037
0.0223965763195435 4047
0.0231098430813124 4053
0.0238231098430813 4058
0.0245363766048502 4060
0.0252496433666191 4065
0.025962910128388 4071
0.0266761768901569 4089
0.0273894436519258 4092
0.0281027104136947 4092
0.0288159771754636 4095
0.0295292439372325 4104
0.0302425106990014 4117
0.0309557774607703 4121
0.0316690442225392 4127
0.0323823109843081 4139
0.033095577746077 4143
0.0338088445078459 4147
0.0345221112696148 4161
0.0352353780313837 4164
0.0359486447931526 4164
0.0366619115549215 4172
0.0373751783166904 4179
0.0380884450784593 4189
0.0388017118402282 4198
0.0395149786019971 4199
0.040228245363766 4207
0.0409415121255349 4213
0.0416547788873039 4223
0.0423680456490728 4234
0.0430813124108417 4236
0.0437945791726106 4247
0.0445078459343795 4256
0.0452211126961484 4258
0.0459343794579173 4259
0.0466476462196862 4265
0.0473609129814551 4272
0.048074179743224 4277
0.0487874465049929 4290
0.0495007132667618 4293
0.0502139800285307 4300
0.0509272467902996 4302
0.0516405135520685 4304
0.0523537803138374 4321
0.0530670470756063 4322
0.0537803138373752 4325
0.0544935805991441 4332
0.055206847360913 4332
0.0559201141226819 4341
0.0566333808844508 4351
0.0573466476462197 4360
0.0580599144079886 4369
0.0587731811697575 4376
0.0594864479315264 4386
0.0601997146932953 4389
0.0609129814550642 4396
0.0616262482168331 4400
0.062339514978602 4404
0.0630527817403709 4408
0.0637660485021398 4410
0.0644793152639087 4413
0.0651925820256776 4417
0.0659058487874465 4417
0.0666191155492154 4420
0.0673323823109843 4426
0.0680456490727532 4430
0.0687589158345221 4437
0.069472182596291 4445
0.0701854493580599 4447
0.0708987161198288 4456
0.0716119828815977 4462
0.0723252496433666 4474
0.0730385164051355 4480
0.0737517831669044 4484
0.0744650499286733 4498
0.0751783166904422 4501
0.0758915834522111 4518
0.07660485021398 4538
0.0773181169757489 4540
0.0780313837375178 4541
0.0787446504992867 4543
0.0794579172610556 4545
0.0801711840228245 4549
0.0808844507845934 4559
0.0815977175463623 4569
0.0823109843081312 4592
0.0830242510699001 4606
0.0837375178316691 4613
0.084450784593438 4620
0.0851640513552068 4628
0.0858773181169757 4633
0.0865905848787446 4647
0.0873038516405135 4654
0.0880171184022824 4655
0.0887303851640514 4659
0.0894436519258203 4665
0.0901569186875892 4669
0.0908701854493581 4688
0.091583452211127 4692
0.0922967189728959 4697
0.0930099857346648 4698
0.0937232524964337 4704
0.0944365192582026 4714
0.0951497860199715 4719
0.0958630527817404 4725
0.0965763195435093 4734
0.0972895863052782 4743
0.0980028530670471 4750
0.098716119828816 4751
0.0994293865905849 4758
0.100142653352354 4767
0.100855920114123 4771
0.101569186875892 4777
0.10228245363766 4788
0.102995720399429 4795
0.103708987161198 4804
0.104422253922967 4806
0.105135520684736 4816
0.105848787446505 4836
0.106562054208274 4853
0.107275320970043 4859
0.107988587731812 4864
0.108701854493581 4879
0.109415121255349 4882
0.110128388017118 4886
0.110841654778887 4898
0.111554921540656 4904
0.112268188302425 4911
0.112981455064194 4913
0.113694721825963 4919
0.114407988587732 4925
0.115121255349501 4929
0.11583452211127 4934
0.116547788873039 4937
0.117261055634807 4942
0.117974322396576 4949
0.118687589158345 4957
0.119400855920114 4970
0.120114122681883 4991
0.120827389443652 4996
0.121540656205421 5001
0.12225392296719 5011
0.122967189728959 5020
0.123680456490728 5034
0.124393723252496 5050
0.125106990014265 5056
0.125820256776034 5062
0.126533523537803 5071
0.127246790299572 5076
0.127960057061341 5098
0.12867332382311 5113
0.129386590584879 5133
0.130099857346648 5139
0.130813124108417 5147
0.131526390870185 5153
0.132239657631954 5159
0.132952924393723 5162
0.133666191155492 5168
0.134379457917261 5182
0.13509272467903 5193
0.135805991440799 5204
0.136519258202568 5219
0.137232524964337 5230
0.137945791726106 5244
0.138659058487874 5255
0.139372325249643 5269
0.140085592011412 5284
0.140798858773181 5287
0.14151212553495 5292
0.142225392296719 5302
0.142938659058488 5313
0.143651925820257 5320
0.144365192582026 5339
0.145078459343795 5346
0.145791726105563 5355
0.146504992867332 5370
0.147218259629101 5385
0.14793152639087 5396
0.148644793152639 5398
0.149358059914408 5400
0.150071326676177 5405
0.150784593437946 5410
0.151497860199715 5421
0.152211126961484 5429
0.152924393723253 5439
0.153637660485021 5443
0.15435092724679 5451
0.155064194008559 5456
0.155777460770328 5468
0.156490727532097 5472
0.157203994293866 5485
0.157917261055635 5490
0.158630527817404 5496
0.159343794579173 5503
0.160057061340942 5510
0.16077032810271 5518
0.161483594864479 5530
0.162196861626248 5535
0.162910128388017 5563
0.163623395149786 5579
0.164336661911555 5593
0.165049928673324 5600
0.165763195435093 5606
0.166476462196862 5623
0.167189728958631 5633
0.167902995720399 5639
0.168616262482168 5651
0.169329529243937 5656
0.170042796005706 5656
0.170756062767475 5660
0.171469329529244 5669
0.172182596291013 5672
0.172895863052782 5683
0.173609129814551 5686
0.17432239657632 5692
0.175035663338088 5695
0.175748930099857 5703
0.176462196861626 5727
0.177175463623395 5733
0.177888730385164 5739
0.178601997146933 5759
0.179315263908702 5760
0.180028530670471 5762
0.18074179743224 5772
0.181455064194009 5782
0.182168330955777 5812
0.182881597717546 5819
0.183594864479315 5830
0.184308131241084 5845
0.185021398002853 5849
0.185734664764622 5859
0.186447931526391 5859
0.18716119828816 5871
0.187874465049929 5879
0.188587731811698 5889
0.189300998573466 5900
0.190014265335235 5917
0.190727532097004 5932
0.191440798858773 5942
0.192154065620542 5951
0.192867332382311 5956
0.19358059914408 5962
0.194293865905849 5969
0.195007132667618 5981
0.195720399429387 5989
0.196433666191155 5995
0.197146932952924 6005
0.197860199714693 6013
0.198573466476462 6034
0.199286733238231 6037
0.2 6066
0.200713266761769 6077
0.201426533523538 6093
0.202139800285307 6104
0.202853067047076 6107
0.203566333808845 6126
0.204279600570613 6141
0.204992867332382 6154
0.205706134094151 6179
0.20641940085592 6188
0.207132667617689 6197
0.207845934379458 6203
0.208559201141227 6206
0.209272467902996 6221
0.209985734664765 6236
0.210699001426534 6238
0.211412268188302 6250
0.212125534950071 6262
0.21283880171184 6269
0.213552068473609 6280
0.214265335235378 6296
0.214978601997147 6302
0.215691868758916 6310
0.216405135520685 6327
0.217118402282454 6338
0.217831669044223 6340
0.218544935805991 6348
0.21925820256776 6354
0.219971469329529 6376
0.220684736091298 6398
0.221398002853067 6425
0.222111269614836 6458
0.222824536376605 6473
0.223537803138374 6487
0.224251069900143 6496
0.224964336661912 6504
0.22567760342368 6516
0.226390870185449 6523
0.227104136947218 6537
0.227817403708987 6555
0.228530670470756 6558
0.229243937232525 6575
0.229957203994294 6585
0.230670470756063 6596
0.231383737517832 6609
0.232097004279601 6615
0.232810271041369 6624
0.233523537803138 6638
0.234236804564907 6642
0.234950071326676 6665
0.235663338088445 6679
0.236376604850214 6691
0.237089871611983 6701
0.237803138373752 6713
0.238516405135521 6727
0.23922967189729 6740
0.239942938659058 6767
0.240656205420827 6780
0.241369472182596 6794
0.242082738944365 6803
0.242796005706134 6818
0.243509272467903 6829
0.244222539229672 6836
0.244935805991441 6843
0.24564907275321 6861
0.246362339514979 6887
0.247075606276748 6895
0.247788873038516 6902
0.248502139800285 6904
0.249215406562054 6927
0.249928673323823 6933
0.250641940085592 6940
0.251355206847361 6948
0.25206847360913 6964
0.252781740370899 6970
0.253495007132668 6977
0.254208273894437 6996
0.254921540656205 7010
0.255634807417974 7031
0.256348074179743 7039
0.257061340941512 7049
0.257774607703281 7060
0.25848787446505 7067
0.259201141226819 7073
0.259914407988588 7081
0.260627674750357 7098
0.261340941512126 7118
0.262054208273894 7121
0.262767475035663 7145
0.263480741797432 7168
0.264194008559201 7179
0.26490727532097 7191
0.265620542082739 7212
0.266333808844508 7226
0.267047075606277 7232
0.267760342368046 7243
0.268473609129815 7249
0.269186875891583 7270
0.269900142653352 7285
0.270613409415121 7295
0.27132667617689 7307
0.272039942938659 7321
0.272753209700428 7325
0.273466476462197 7332
0.274179743223966 7342
0.274893009985735 7345
0.275606276747504 7357
0.276319543509272 7365
0.277032810271041 7369
0.27774607703281 7382
0.278459343794579 7401
0.279172610556348 7405
0.279885877318117 7416
0.280599144079886 7438
0.281312410841655 7456
0.282025677603424 7469
0.282738944365193 7492
0.283452211126962 7497
0.28416547788873 7502
0.284878744650499 7516
0.285592011412268 7520
0.286305278174037 7526
0.287018544935806 7544
0.287731811697575 7556
0.288445078459344 7574
0.289158345221113 7615
0.289871611982882 7623
0.29058487874465 7734
0.291298145506419 7750
0.292011412268188 7754
0.292724679029957 7770
0.293437945791726 7791
0.294151212553495 7801
0.294864479315264 7810
0.295577746077033 7825
0.296291012838802 7849
0.297004279600571 7863
0.297717546362339 7884
0.298430813124108 7918
0.299144079885877 7928
0.299857346647646 7934
0.300570613409415 7947
0.301283880171184 7958
0.301997146932953 7967
0.302710413694722 7985
0.303423680456491 7997
0.30413694721826 8000
0.304850213980029 8012
0.305563480741797 8024
0.306276747503566 8034
0.306990014265335 8048
0.307703281027104 8064
0.308416547788873 8074
0.309129814550642 8103
0.309843081312411 8125
0.31055634807418 8142
0.311269614835949 8174
0.311982881597718 8186
0.312696148359486 8209
0.313409415121255 8224
0.314122681883024 8232
0.314835948644793 8259
0.315549215406562 8273
0.316262482168331 8286
0.3169757489301 8308
0.317689015691869 8327
0.318402282453638 8381
0.319115549215407 8392
0.319828815977175 8407
0.320542082738944 8450
0.321255349500713 8466
0.321968616262482 8483
0.322681883024251 8493
0.32339514978602 8505
0.324108416547789 8528
0.324821683309558 8541
0.325534950071327 8556
0.326248216833096 8570
0.326961483594864 8591
0.327674750356633 8594
0.328388017118402 8609
0.329101283880171 8622
0.32981455064194 8636
0.330527817403709 8654
0.331241084165478 8664
0.331954350927247 8684
0.332667617689016 8711
0.333380884450785 8731
0.334094151212553 8743
0.334807417974322 8777
0.335520684736091 8787
0.33623395149786 8829
0.336947218259629 8845
0.337660485021398 8857
0.338373751783167 8895
0.339087018544936 8931
0.339800285306705 8941
0.340513552068474 8956
0.341226818830243 8970
0.341940085592011 9011
0.34265335235378 9023
0.343366619115549 9064
0.344079885877318 9082
0.344793152639087 9109
0.345506419400856 9120
0.346219686162625 9129
0.346932952924394 9140
0.347646219686163 9164
0.348359486447932 9177
0.3490727532097 9195
0.349786019971469 9209
0.350499286733238 9222
0.351212553495007 9586
0.351925820256776 9589
0.352639087018545 9622
0.353352353780314 9647
0.354065620542083 9668
0.354778887303852 9682
0.355492154065621 9705
0.356205420827389 9719
0.356918687589158 9737
0.357631954350927 9752
0.358345221112696 9764
0.359058487874465 9774
0.359771754636234 9792
0.360485021398003 9801
0.361198288159772 9804
0.361911554921541 9840
0.36262482168331 9880
0.363338088445078 9903
0.364051355206847 9923
0.364764621968616 9942
0.365477888730385 9955
0.366191155492154 9964
0.366904422253923 9991
0.367617689015692 10011
0.368330955777461 10030
0.36904422253923 10038
0.369757489300999 10050
0.370470756062767 10060
0.371184022824536 10067
0.371897289586305 10088
0.372610556348074 10091
0.373323823109843 10104
0.374037089871612 10110
0.374750356633381 10128
0.37546362339515 10174
0.376176890156919 10203
0.376890156918688 10241
0.377603423680457 10256
0.378316690442225 10261
0.379029957203994 10271
0.379743223965763 10297
0.380456490727532 10315
0.381169757489301 10333
0.38188302425107 10353
0.382596291012839 10368
0.383309557774608 10375
0.384022824536377 10397
0.384736091298146 10414
0.385449358059914 10447
0.386162624821683 10451
0.386875891583452 10478
0.387589158345221 10497
0.38830242510699 10504
0.389015691868759 10542
0.389728958630528 10574
0.390442225392297 10586
0.391155492154066 10607
0.391868758915835 10620
0.392582025677603 10635
0.393295292439372 10650
0.394008559201141 10678
0.39472182596291 10688
0.395435092724679 10707
0.396148359486448 10729
0.396861626248217 10758
0.397574893009986 10772
0.398288159771755 10791
0.399001426533524 10811
0.399714693295292 10824
0.400427960057061 10854
0.40114122681883 10890
0.401854493580599 10913
0.402567760342368 10923
0.403281027104137 10940
0.403994293865906 10967
0.404707560627675 10987
0.405420827389444 11014
0.406134094151213 11019
0.406847360912981 11048
0.40756062767475 11076
0.408273894436519 11092
0.408987161198288 11105
0.409700427960057 11127
0.410413694721826 11144
0.411126961483595 11174
0.411840228245364 11179
0.412553495007133 11201
0.413266761768902 11207
0.41398002853067 11218
0.414693295292439 11242
0.415406562054208 11252
0.416119828815977 11284
0.416833095577746 11305
0.417546362339515 11324
0.418259629101284 11342
0.418972895863053 11355
0.419686162624822 11377
0.420399429386591 11386
0.421112696148359 11420
0.421825962910128 11444
0.422539229671897 11471
0.423252496433666 11484
0.423965763195435 11494
0.424679029957204 11530
0.425392296718973 11538
0.426105563480742 11558
0.426818830242511 11566
0.42753209700428 11609
0.428245363766048 11643
0.428958630527817 11658
0.429671897289586 11670
0.430385164051355 11676
0.431098430813124 11695
0.431811697574893 11717
0.432524964336662 11745
0.433238231098431 11750
0.4339514978602 11774
0.434664764621969 11793
0.435378031383738 11834
0.436091298145506 11850
0.436804564907275 11870
0.437517831669044 11892
0.438231098430813 11910
0.438944365192582 11945
0.439657631954351 11962
0.44037089871612 11993
0.441084165477889 12023
0.441797432239658 12059
0.442510699001427 12073
0.443223965763195 12096
0.443937232524964 12134
0.444650499286733 12149
0.445363766048502 12164
0.446077032810271 12175
0.44679029957204 12190
0.447503566333809 12197
0.448216833095578 12223
0.448930099857347 12248
0.449643366619116 12264
0.450356633380884 12277
0.451069900142653 12321
0.451783166904422 12352
0.452496433666191 12385
0.45320970042796 12410
0.453922967189729 12427
0.454636233951498 12442
0.455349500713267 12492
0.456062767475036 12505
0.456776034236805 12530
0.457489300998573 12540
0.458202567760342 12551
0.458915834522111 12566
0.45962910128388 12594
0.460342368045649 12617
0.461055634807418 12632
0.461768901569187 12664
0.462482168330956 12735
0.463195435092725 12762
0.463908701854494 12782
0.464621968616262 12803
0.465335235378031 12832
0.4660485021398 12850
0.466761768901569 12870
0.467475035663338 12891
0.468188302425107 12950
0.468901569186876 12968
0.469614835948645 12998
0.470328102710414 13021
0.471041369472183 13039
0.471754636233951 13050
0.47246790299572 13085
0.473181169757489 13093
0.473894436519258 13109
0.474607703281027 13134
0.475320970042796 13163
0.476034236804565 13186
0.476747503566334 13220
0.477460770328103 13246
0.478174037089872 13277
0.478887303851641 13298
0.479600570613409 13313
0.480313837375178 13364
0.481027104136947 13424
0.481740370898716 13452
0.482453637660485 13477
0.483166904422254 13547
0.483880171184023 13565
0.484593437945792 13618
0.485306704707561 13679
0.48601997146933 13698
0.486733238231098 13718
0.487446504992867 13751
0.488159771754636 13767
0.488873038516405 13790
0.489586305278174 13819
0.490299572039943 13843
0.491012838801712 13864
0.491726105563481 13876
0.49243937232525 13924
0.493152639087019 13950
0.493865905848787 13976
0.494579172610556 13987
0.495292439372325 13994
0.496005706134094 14010
0.496718972895863 14021
0.497432239657632 14041
0.498145506419401 14059
0.49885877318117 14101
0.499572039942939 14129
0.500285306704708 14167
0.500998573466476 14177
0.501711840228245 14212
0.502425106990014 14240
0.503138373751783 14255
0.503851640513552 14295
0.504564907275321 14304
0.50527817403709 14364
0.505991440798859 14391
0.506704707560628 14432
0.507417974322397 14460
0.508131241084166 14470
0.508844507845934 14484
0.509557774607703 14506
0.510271041369472 14530
0.510984308131241 14551
0.51169757489301 14573
0.512410841654779 14590
0.513124108416548 14603
0.513837375178317 14621
0.514550641940086 14657
0.515263908701854 14719
0.515977175463623 14758
0.516690442225392 14784
0.517403708987161 14813
0.51811697574893 14850
0.518830242510699 14870
0.519543509272468 14887
0.520256776034237 14935
0.520970042796006 14956
0.521683309557775 14971
0.522396576319544 15029
0.523109843081312 15086
0.523823109843081 15105
0.52453637660485 15150
0.525249643366619 15179
0.525962910128388 15232
0.526676176890157 15246
0.527389443651926 15306
0.528102710413695 15313
0.528815977175464 15405
0.529529243937233 15433
0.530242510699001 15457
0.53095577746077 15472
0.531669044222539 15483
0.532382310984308 15519
0.533095577746077 15550
0.533808844507846 15577
0.534522111269615 15634
0.535235378031384 15683
0.535948644793153 15712
0.536661911554921 15760
0.53737517831669 15792
0.538088445078459 15817
0.538801711840228 15844
0.539514978601997 15870
0.540228245363766 15896
0.540941512125535 15912
0.541654778887304 15950
0.542368045649073 15996
0.543081312410842 16069
0.543794579172611 16087
0.54450784593438 16151
0.545221112696148 16183
0.545934379457917 16200
0.546647646219686 16229
0.547360912981455 16268
0.548074179743224 16310
0.548787446504993 16384
0.549500713266762 16413
0.550213980028531 16436
0.5509272467903 16477
0.551640513552068 16528
0.552353780313837 16540
0.553067047075606 16564
0.553780313837375 16590
0.554493580599144 16648
0.555206847360913 16674
0.555920114122682 16723
0.556633380884451 16757
0.55734664764622 16816
0.558059914407989 16826
0.558773181169757 16866
0.559486447931526 16895
0.560199714693295 16942
0.560912981455064 16989
0.561626248216833 17026
0.562339514978602 17050
0.563052781740371 17084
0.56376604850214 17141
0.564479315263909 17166
0.565192582025678 17188
0.565905848787447 17235
0.566619115549215 17302
0.567332382310984 17332
0.568045649072753 17378
0.568758915834522 17409
0.569472182596291 17459
0.57018544935806 17512
0.570898716119829 17540
0.571611982881598 17588
0.572325249643367 17602
0.573038516405135 17635
0.573751783166904 17683
0.574465049928673 17744
0.575178316690442 17771
0.575891583452211 17798
0.57660485021398 17842
0.577318116975749 17857
0.578031383737518 17862
0.578744650499287 17931
0.579457917261056 17967
0.580171184022825 17993
0.580884450784593 18022
0.581597717546362 18049
0.582310984308131 18083
0.5830242510699 18123
0.583737517831669 18162
0.584450784593438 18204
0.585164051355207 18264
0.585877318116976 18299
0.586590584878745 18337
0.587303851640514 18364
0.588017118402282 18381
0.588730385164051 18400
0.58944365192582 18427
0.590156918687589 18478
0.590870185449358 18486
0.591583452211127 18516
0.592296718972896 18555
0.593009985734665 18582
0.593723252496434 18647
0.594436519258203 18679
0.595149786019971 18687
0.59586305278174 18722
0.596576319543509 18754
0.597289586305278 18788
0.598002853067047 18815
0.598716119828816 18860
0.599429386590585 18932
0.600142653352354 18964
0.600855920114123 19007
0.601569186875892 19038
0.602282453637661 19078
0.602995720399429 19119
0.603708987161198 19142
0.604422253922967 19156
0.605135520684736 19215
0.605848787446505 19265
0.606562054208274 19335
0.607275320970043 19372
0.607988587731812 19456
0.608701854493581 19488
0.609415121255349 19520
0.610128388017118 19536
0.610841654778887 19555
0.611554921540656 19627
0.612268188302425 19669
0.612981455064194 19756
0.613694721825963 19805
0.614407988587732 19862
0.615121255349501 19956
0.61583452211127 19982
0.616547788873039 19993
0.617261055634807 20007
0.617974322396576 20031
0.618687589158345 20038
0.619400855920114 20067
0.620114122681883 20111
0.620827389443652 20126
0.621540656205421 20173
0.62225392296719 20211
0.622967189728959 20238
0.623680456490728 20263
0.624393723252496 20344
0.625106990014265 20389
0.625820256776034 20403
0.626533523537803 20445
0.627246790299572 20472
0.627960057061341 20525
0.62867332382311 20563
0.629386590584879 20606
0.630099857346648 20669
0.630813124108417 20725
0.631526390870185 20764
0.632239657631954 20827
0.632952924393723 20866
0.633666191155492 20928
0.634379457917261 20955
0.63509272467903 20982
0.635805991440799 20993
0.636519258202568 21037
0.637232524964337 21052
0.637945791726106 21089
0.638659058487875 21151
0.639372325249643 21207
0.640085592011412 21219
0.640798858773181 21277
0.64151212553495 21315
0.642225392296719 21340
0.642938659058488 21360
0.643651925820257 21376
0.644365192582026 21408
0.645078459343795 21469
0.645791726105563 21516
0.646504992867332 21575
0.647218259629101 21646
0.64793152639087 21704
0.648644793152639 21731
0.649358059914408 21794
0.650071326676177 21825
0.650784593437946 21889
0.651497860199715 21922
0.652211126961484 22004
0.652924393723253 22061
0.653637660485021 22124
0.65435092724679 22180
0.655064194008559 22259
0.655777460770328 22277
0.656490727532097 22314
0.657203994293866 22391
0.657917261055635 22430
0.658630527817404 22472
0.659343794579173 22509
0.660057061340942 22623
0.66077032810271 22743
0.661483594864479 22781
0.662196861626248 22843
0.662910128388017 22866
0.663623395149786 22905
0.664336661911555 22939
0.665049928673324 22983
0.665763195435093 23050
0.666476462196862 23091
0.66718972895863 23175
0.667902995720399 23193
0.668616262482168 23231
0.669329529243937 23267
0.670042796005706 23290
0.670756062767475 23338
0.671469329529244 23388
0.672182596291013 23434
0.672895863052782 23494
0.673609129814551 23525
0.67432239657632 23582
0.675035663338088 23648
0.675748930099857 23690
0.676462196861626 23770
0.677175463623395 23815
0.677888730385164 23861
0.678601997146933 23929
0.679315263908702 23984
0.680028530670471 24048
0.68074179743224 24076
0.681455064194009 24137
0.682168330955777 24152
0.682881597717546 24195
0.683594864479315 24263
0.684308131241084 24295
0.685021398002853 24333
0.685734664764622 24398
0.686447931526391 24469
0.68716119828816 24496
0.687874465049929 24604
0.688587731811698 24657
0.689300998573466 24671
0.690014265335235 24686
0.690727532097004 24772
0.691440798858773 24847
0.692154065620542 24904
0.692867332382311 24922
0.69358059914408 24984
0.694293865905849 25023
0.695007132667618 25062
0.695720399429387 25142
0.696433666191156 25213
0.697146932952924 25279
0.697860199714693 25344
0.698573466476462 25397
0.699286733238231 25478
0.7 25514
0.700713266761769 25537
0.701426533523538 25609
0.702139800285307 25690
0.702853067047076 25754
0.703566333808844 25854
0.704279600570613 25949
0.704992867332382 25989
0.705706134094151 26027
0.70641940085592 26112
0.707132667617689 26186
0.707845934379458 26213
0.708559201141227 26319
0.709272467902996 26353
0.709985734664765 26400
0.710699001426534 26441
0.711412268188302 26497
0.712125534950071 26576
0.71283880171184 26647
0.713552068473609 26750
0.714265335235378 26814
0.714978601997147 26841
0.715691868758916 26898
0.716405135520685 26989
0.717118402282454 27061
0.717831669044223 27122
0.718544935805991 27210
0.71925820256776 27247
0.719971469329529 27322
0.720684736091298 27380
0.721398002853067 27475
0.722111269614836 27622
0.722824536376605 27693
0.723537803138374 27769
0.724251069900143 27851
0.724964336661912 27980
0.72567760342368 28033
0.726390870185449 28184
0.727104136947218 28275
0.727817403708987 28398
0.728530670470756 28462
0.729243937232525 28502
0.729957203994294 28583
0.730670470756063 28624
0.731383737517832 28684
0.732097004279601 28743
0.732810271041369 28821
0.733523537803138 28873
0.734236804564907 28963
0.734950071326676 28988
0.735663338088445 29062
0.736376604850214 29126
0.737089871611983 29209
0.737803138373752 29276
0.738516405135521 29342
0.73922967189729 29410
0.739942938659058 29487
0.740656205420827 29608
0.741369472182596 29690
0.742082738944365 29826
0.742796005706134 29856
0.743509272467903 29953
0.744222539229672 30085
0.744935805991441 30156
0.74564907275321 30302
0.746362339514979 30424
0.747075606276748 30433
0.747788873038516 30565
0.748502139800285 30610
0.749215406562054 30670
0.749928673323823 30723
0.750641940085592 30814
0.751355206847361 30866
0.75206847360913 31064
0.752781740370899 31146
0.753495007132668 31272
0.754208273894437 31324
0.754921540656205 31407
0.755634807417974 31473
0.756348074179743 31619
0.757061340941512 31660
0.757774607703281 31782
0.75848787446505 31887
0.759201141226819 31998
0.759914407988588 32086
0.760627674750357 32184
0.761340941512126 32294
0.762054208273894 32337
0.762767475035663 32418
0.763480741797432 32527
0.764194008559201 32630
0.76490727532097 32696
0.765620542082739 32741
0.766333808844508 32794
0.767047075606277 32840
0.767760342368046 32857
0.768473609129815 32958
0.769186875891583 33004
0.769900142653352 33025
0.770613409415121 33060
0.77132667617689 33220
0.772039942938659 33329
0.772753209700428 33362
0.773466476462197 33453
0.774179743223966 33627
0.774893009985735 33648
0.775606276747504 33723
0.776319543509272 33897
0.777032810271041 33939
0.77774607703281 34015
0.778459343794579 34114
0.779172610556348 34216
0.779885877318117 34237
0.780599144079886 34442
0.781312410841655 34487
0.782025677603424 34603
0.782738944365193 34739
0.783452211126962 34804
0.78416547788873 34864
0.784878744650499 35061
0.785592011412268 35169
0.786305278174037 35349
0.787018544935806 35398
0.787731811697575 35494
0.788445078459344 35663
0.789158345221113 35866
0.789871611982882 35970
0.790584878744651 36010
0.791298145506419 36143
0.792011412268188 36321
0.792724679029957 36490
0.793437945791726 36503
0.794151212553495 36556
0.794864479315264 36640
0.795577746077033 36743
0.796291012838802 36802
0.797004279600571 36881
0.797717546362339 37029
0.798430813124108 37148
0.799144079885877 37283
0.799857346647646 37394
0.800570613409415 37492
0.801283880171184 37622
0.801997146932953 37757
0.802710413694722 37896
0.803423680456491 38035
0.80413694721826 38209
0.804850213980029 38314
0.805563480741797 38399
0.806276747503566 38474
0.806990014265335 38696
0.807703281027104 38849
0.808416547788873 38960
0.809129814550642 39043
0.809843081312411 39090
0.81055634807418 39260
0.811269614835949 39433
0.811982881597718 39484
0.812696148359486 39592
0.813409415121255 39808
0.814122681883024 39847
0.814835948644793 39887
0.815549215406562 39991
0.816262482168331 40059
0.8169757489301 40127
0.817689015691869 40251
0.818402282453638 40310
0.819115549215407 40388
0.819828815977175 40559
0.820542082738944 40672
0.821255349500713 40758
0.821968616262482 40812
0.822681883024251 40921
0.82339514978602 41033
0.824108416547789 41289
0.824821683309558 41451
0.825534950071327 41822
0.826248216833096 41884
0.826961483594864 42032
0.827674750356633 42191
0.828388017118402 42242
0.829101283880171 42313
0.82981455064194 42424
0.830527817403709 42674
0.831241084165478 42998
0.831954350927247 43243
0.832667617689016 43503
0.833380884450785 43627
0.834094151212553 43868
0.834807417974322 43932
0.835520684736091 44124
0.83623395149786 44323
0.836947218259629 44579
0.837660485021398 44652
0.838373751783167 44765
0.839087018544936 44963
0.839800285306705 45308
0.840513552068474 45379
0.841226818830243 45615
0.841940085592011 45735
0.84265335235378 45905
0.843366619115549 45953
0.844079885877318 46131
0.844793152639087 46286
0.845506419400856 46413
0.846219686162625 46668
0.846932952924394 46767
0.847646219686163 46970
0.848359486447932 47287
0.8490727532097 47526
0.849786019971469 47633
0.850499286733238 47845
0.851212553495007 47933
0.851925820256776 48103
0.852639087018545 48370
0.853352353780314 48446
0.854065620542083 48703
0.854778887303852 48867
0.855492154065621 49173
0.856205420827389 49428
0.856918687589158 49638
0.857631954350927 49693
0.858345221112696 49885
0.859058487874465 49957
0.859771754636234 50232
0.860485021398003 50436
0.861198288159772 50660
0.861911554921541 50787
0.86262482168331 50834
0.863338088445078 50931
0.864051355206847 51054
0.864764621968616 51191
0.865477888730385 51394
0.866191155492154 51621
0.866904422253923 51735
0.867617689015692 51838
0.868330955777461 52043
0.86904422253923 52190
0.869757489300999 52591
0.870470756062767 52782
0.871184022824536 53014
0.871897289586305 53241
0.872610556348074 53601
0.873323823109843 53776
0.874037089871612 53911
0.874750356633381 53986
0.87546362339515 54148
0.876176890156919 54253
0.876890156918688 54337
0.877603423680457 54640
0.878316690442225 54808
0.879029957203994 55066
0.879743223965763 55290
0.880456490727532 55413
0.881169757489301 55571
0.88188302425107 55753
0.882596291012839 55837
0.883309557774608 56168
0.884022824536377 56303
0.884736091298145 56641
0.885449358059914 56976
0.886162624821683 57603
0.886875891583452 57789
0.887589158345221 58027
0.88830242510699 58224
0.889015691868759 58314
0.889728958630528 58732
0.890442225392297 58980
0.891155492154066 59589
0.891868758915835 59660
0.892582025677603 59951
0.893295292439372 60316
0.894008559201141 60499
0.89472182596291 60738
0.895435092724679 61120
0.896148359486448 61328
0.896861626248217 61607
0.897574893009986 61875
0.898288159771755 62104
0.899001426533524 62212
0.899714693295292 62744
0.900427960057061 63065
0.90114122681883 63245
0.901854493580599 63557
0.902567760342368 63684
0.903281027104137 64155
0.903994293865906 64387
0.904707560627675 64819
0.905420827389444 65012
0.906134094151213 65149
0.906847360912981 65317
0.90756062767475 65583
0.908273894436519 65803
0.908987161198288 65949
0.909700427960057 66168
0.910413694721826 66550
0.911126961483595 67214
0.911840228245364 67557
0.912553495007133 67794
0.913266761768902 68223
0.913980028530671 68890
0.914693295292439 69211
0.915406562054208 69469
0.916119828815977 69797
0.916833095577746 70256
0.917546362339515 70547
0.918259629101284 70803
0.918972895863053 71161
0.919686162624822 71702
0.920399429386591 72092
0.921112696148359 72345
0.921825962910128 72593
0.922539229671897 72781
0.923252496433666 72896
0.923965763195435 73141
0.924679029957204 73438
0.925392296718973 73836
0.926105563480742 74197
0.926818830242511 74806
0.92753209700428 75055
0.928245363766048 75620
0.928958630527817 76335
0.929671897289586 76805
0.930385164051355 77256
0.931098430813124 77526
0.931811697574893 77885
0.932524964336662 78396
0.933238231098431 79235
0.9339514978602 79637
0.934664764621969 80256
0.935378031383738 80602
0.936091298145506 80841
0.936804564907275 81485
0.937517831669044 82108
0.938231098430813 82988
0.938944365192582 83277
0.939657631954351 83619
0.94037089871612 84908
0.941084165477889 85303
0.941797432239658 85793
0.942510699001427 86102
0.943223965763195 86897
0.943937232524964 87466
0.944650499286733 88181
0.945363766048502 88694
0.946077032810271 89308
0.94679029957204 90196
0.947503566333809 90716
0.948216833095578 91174
0.948930099857347 92097
0.949643366619116 92402
0.950356633380884 92682
0.951069900142653 93326
0.951783166904422 93702
0.952496433666191 94179
0.95320970042796 95002
0.953922967189729 95565
0.954636233951498 96618
0.955349500713267 98073
0.956062767475036 98564
0.956776034236805 98904
0.957489300998573 99918
0.958202567760342 101039
0.958915834522111 101262
0.95962910128388 102541
0.960342368045649 102980
0.961055634807418 104326
0.961768901569187 105705
0.962482168330956 106285
0.963195435092725 107271
0.963908701854494 108127
0.964621968616262 109420
0.965335235378031 110315
0.9660485021398 111736
0.966761768901569 112621
0.967475035663338 114508
0.968188302425107 117294
0.968901569186876 117644
0.969614835948645 118627
0.970328102710414 119462
0.971041369472183 120244
0.971754636233952 122398
0.97246790299572 126939
0.973181169757489 128101
0.973894436519258 129511
0.974607703281027 131651
0.975320970042796 133243
0.976034236804565 134337
0.976747503566334 135372
0.977460770328103 136563
0.978174037089872 139107
0.97888730385164 142069
0.979600570613409 144732
0.980313837375178 147121
0.981027104136947 150707
0.981740370898716 153217
0.982453637660485 154331
0.983166904422254 156288
0.983880171184023 161704
0.984593437945792 165068
0.985306704707561 174429
0.98601997146933 180037
0.986733238231098 185782
0.987446504992867 188612
0.988159771754636 193051
0.988873038516405 197597
0.989586305278174 201477
0.990299572039943 205159
0.991012838801712 218653
0.991726105563481 225131
0.99243937232525 239238
0.993152639087019 245597
0.993865905848787 265324
0.994579172610556 294338
0.995292439372325 321879
0.996005706134094 349672
0.996718972895863 424398
};
\addlegendentry{resnet50-places365}
\addplot [semithick, color2]
table {0.000998573466476462 3363
0.00171184022824536 3504
0.00242510699001427 3541
0.00313837375178317 3560
0.00385164051355207 3567
0.00456490727532097 3585
0.00527817403708987 3606
0.00599144079885877 3609
0.00670470756062767 3638
0.00741797432239658 3648
0.00813124108416548 3656
0.00884450784593438 3668
0.00955777460770328 3668
0.0102710413694722 3670
0.0109843081312411 3670
0.01169757489301 3680
0.0124108416547789 3690
0.0131241084165478 3692
0.0138373751783167 3720
0.0145506419400856 3730
0.0152639087018545 3736
0.0159771754636234 3739
0.0166904422253923 3741
0.0174037089871612 3749
0.0181169757489301 3754
0.018830242510699 3769
0.0195435092724679 3780
0.0202567760342368 3786
0.0209700427960057 3786
0.0216833095577746 3791
0.0223965763195435 3797
0.0231098430813124 3798
0.0238231098430813 3803
0.0245363766048502 3814
0.0252496433666191 3815
0.025962910128388 3826
0.0266761768901569 3841
0.0273894436519258 3848
0.0281027104136947 3850
0.0288159771754636 3852
0.0295292439372325 3862
0.0302425106990014 3867
0.0309557774607703 3880
0.0316690442225392 3881
0.0323823109843081 3881
0.033095577746077 3893
0.0338088445078459 3900
0.0345221112696148 3902
0.0352353780313837 3914
0.0359486447931526 3915
0.0366619115549215 3925
0.0373751783166904 3927
0.0380884450784593 3928
0.0388017118402282 3945
0.0395149786019971 3945
0.040228245363766 3954
0.0409415121255349 3959
0.0416547788873039 3960
0.0423680456490728 3962
0.0430813124108417 3968
0.0437945791726106 3975
0.0445078459343795 3977
0.0452211126961484 4002
0.0459343794579173 4011
0.0466476462196862 4036
0.0473609129814551 4039
0.048074179743224 4042
0.0487874465049929 4051
0.0495007132667618 4052
0.0502139800285307 4071
0.0509272467902996 4089
0.0516405135520685 4100
0.0523537803138374 4104
0.0530670470756063 4115
0.0537803138373752 4122
0.0544935805991441 4125
0.055206847360913 4129
0.0559201141226819 4140
0.0566333808844508 4153
0.0573466476462197 4158
0.0580599144079886 4169
0.0587731811697575 4178
0.0594864479315264 4185
0.0601997146932953 4186
0.0609129814550642 4191
0.0616262482168331 4197
0.062339514978602 4203
0.0630527817403709 4206
0.0637660485021398 4210
0.0644793152639087 4219
0.0651925820256776 4224
0.0659058487874465 4225
0.0666191155492154 4232
0.0673323823109843 4236
0.0680456490727532 4242
0.0687589158345221 4268
0.069472182596291 4270
0.0701854493580599 4274
0.0708987161198288 4277
0.0716119828815977 4282
0.0723252496433666 4282
0.0730385164051355 4286
0.0737517831669044 4296
0.0744650499286733 4302
0.0751783166904422 4310
0.0758915834522111 4334
0.07660485021398 4339
0.0773181169757489 4349
0.0780313837375178 4355
0.0787446504992867 4363
0.0794579172610556 4366
0.0801711840228245 4372
0.0808844507845934 4379
0.0815977175463623 4387
0.0823109843081312 4390
0.0830242510699001 4390
0.0837375178316691 4393
0.084450784593438 4399
0.0851640513552068 4400
0.0858773181169757 4411
0.0865905848787446 4431
0.0873038516405135 4435
0.0880171184022824 4442
0.0887303851640514 4445
0.0894436519258203 4452
0.0901569186875892 4453
0.0908701854493581 4469
0.091583452211127 4469
0.0922967189728959 4478
0.0930099857346648 4479
0.0937232524964337 4486
0.0944365192582026 4487
0.0951497860199715 4491
0.0958630527817404 4499
0.0965763195435093 4512
0.0972895863052782 4517
0.0980028530670471 4522
0.098716119828816 4525
0.0994293865905849 4536
0.100142653352354 4539
0.100855920114123 4540
0.101569186875892 4542
0.10228245363766 4549
0.102995720399429 4560
0.103708987161198 4565
0.104422253922967 4570
0.105135520684736 4577
0.105848787446505 4582
0.106562054208274 4584
0.107275320970043 4590
0.107988587731812 4601
0.108701854493581 4606
0.109415121255349 4613
0.110128388017118 4616
0.110841654778887 4623
0.111554921540656 4628
0.112268188302425 4628
0.112981455064194 4642
0.113694721825963 4645
0.114407988587732 4649
0.115121255349501 4658
0.11583452211127 4664
0.116547788873039 4678
0.117261055634807 4679
0.117974322396576 4693
0.118687589158345 4699
0.119400855920114 4707
0.120114122681883 4712
0.120827389443652 4724
0.121540656205421 4729
0.12225392296719 4732
0.122967189728959 4739
0.123680456490728 4741
0.124393723252496 4742
0.125106990014265 4748
0.125820256776034 4762
0.126533523537803 4765
0.127246790299572 4781
0.127960057061341 4792
0.12867332382311 4798
0.129386590584879 4800
0.130099857346648 4808
0.130813124108417 4814
0.131526390870185 4818
0.132239657631954 4837
0.132952924393723 4840
0.133666191155492 4847
0.134379457917261 4856
0.13509272467903 4857
0.135805991440799 4860
0.136519258202568 4868
0.137232524964337 4887
0.137945791726106 4889
0.138659058487874 4893
0.139372325249643 4895
0.140085592011412 4902
0.140798858773181 4904
0.14151212553495 4905
0.142225392296719 4918
0.142938659058488 4926
0.143651925820257 4928
0.144365192582026 4931
0.145078459343795 4938
0.145791726105563 4940
0.146504992867332 4944
0.147218259629101 4948
0.14793152639087 4951
0.148644793152639 4958
0.149358059914408 4962
0.150071326676177 4963
0.150784593437946 4970
0.151497860199715 4980
0.152211126961484 4988
0.152924393723253 4990
0.153637660485021 4994
0.15435092724679 4998
0.155064194008559 5019
0.155777460770328 5025
0.156490727532097 5031
0.157203994293866 5038
0.157917261055635 5041
0.158630527817404 5052
0.159343794579173 5066
0.160057061340942 5075
0.16077032810271 5090
0.161483594864479 5108
0.162196861626248 5123
0.162910128388017 5131
0.163623395149786 5134
0.164336661911555 5136
0.165049928673324 5164
0.165763195435093 5170
0.166476462196862 5175
0.167189728958631 5192
0.167902995720399 5192
0.168616262482168 5202
0.169329529243937 5212
0.170042796005706 5219
0.170756062767475 5223
0.171469329529244 5242
0.172182596291013 5254
0.172895863052782 5268
0.173609129814551 5273
0.17432239657632 5277
0.175035663338088 5281
0.175748930099857 5281
0.176462196861626 5282
0.177175463623395 5287
0.177888730385164 5294
0.178601997146933 5307
0.179315263908702 5311
0.180028530670471 5315
0.18074179743224 5318
0.181455064194009 5319
0.182168330955777 5321
0.182881597717546 5326
0.183594864479315 5333
0.184308131241084 5338
0.185021398002853 5342
0.185734664764622 5350
0.186447931526391 5357
0.18716119828816 5360
0.187874465049929 5374
0.188587731811698 5378
0.189300998573466 5389
0.190014265335235 5395
0.190727532097004 5407
0.191440798858773 5416
0.192154065620542 5423
0.192867332382311 5438
0.19358059914408 5451
0.194293865905849 5470
0.195007132667618 5487
0.195720399429387 5494
0.196433666191155 5506
0.197146932952924 5511
0.197860199714693 5529
0.198573466476462 5532
0.199286733238231 5538
0.2 5552
0.200713266761769 5559
0.201426533523538 5564
0.202139800285307 5567
0.202853067047076 5575
0.203566333808845 5582
0.204279600570613 5583
0.204992867332382 5586
0.205706134094151 5592
0.20641940085592 5595
0.207132667617689 5604
0.207845934379458 5617
0.208559201141227 5630
0.209272467902996 5633
0.209985734664765 5637
0.210699001426534 5644
0.211412268188302 5651
0.212125534950071 5656
0.21283880171184 5658
0.213552068473609 5665
0.214265335235378 5668
0.214978601997147 5674
0.215691868758916 5688
0.216405135520685 5694
0.217118402282454 5705
0.217831669044223 5708
0.218544935805991 5717
0.21925820256776 5728
0.219971469329529 5732
0.220684736091298 5741
0.221398002853067 5742
0.222111269614836 5750
0.222824536376605 5773
0.223537803138374 5780
0.224251069900143 5786
0.224964336661912 5787
0.22567760342368 5790
0.226390870185449 5796
0.227104136947218 5800
0.227817403708987 5808
0.228530670470756 5811
0.229243937232525 5823
0.229957203994294 5832
0.230670470756063 5838
0.231383737517832 5843
0.232097004279601 5853
0.232810271041369 5855
0.233523537803138 5861
0.234236804564907 5863
0.234950071326676 5877
0.235663338088445 5891
0.236376604850214 5897
0.237089871611983 5900
0.237803138373752 5917
0.238516405135521 5922
0.23922967189729 5932
0.239942938659058 5938
0.240656205420827 5948
0.241369472182596 5959
0.242082738944365 5969
0.242796005706134 5974
0.243509272467903 5980
0.244222539229672 5993
0.244935805991441 6012
0.24564907275321 6015
0.246362339514979 6015
0.247075606276748 6017
0.247788873038516 6026
0.248502139800285 6030
0.249215406562054 6041
0.249928673323823 6050
0.250641940085592 6056
0.251355206847361 6059
0.25206847360913 6064
0.252781740370899 6072
0.253495007132668 6089
0.254208273894437 6109
0.254921540656205 6117
0.255634807417974 6121
0.256348074179743 6133
0.257061340941512 6143
0.257774607703281 6145
0.25848787446505 6153
0.259201141226819 6169
0.259914407988588 6179
0.260627674750357 6184
0.261340941512126 6188
0.262054208273894 6204
0.262767475035663 6216
0.263480741797432 6226
0.264194008559201 6229
0.26490727532097 6246
0.265620542082739 6260
0.266333808844508 6271
0.267047075606277 6274
0.267760342368046 6281
0.268473609129815 6281
0.269186875891583 6287
0.269900142653352 6298
0.270613409415121 6301
0.27132667617689 6316
0.272039942938659 6325
0.272753209700428 6343
0.273466476462197 6350
0.274179743223966 6365
0.274893009985735 6385
0.275606276747504 6389
0.276319543509272 6392
0.277032810271041 6402
0.27774607703281 6402
0.278459343794579 6411
0.279172610556348 6414
0.279885877318117 6418
0.280599144079886 6423
0.281312410841655 6430
0.282025677603424 6445
0.282738944365193 6452
0.283452211126962 6465
0.28416547788873 6478
0.284878744650499 6504
0.285592011412268 6508
0.286305278174037 6517
0.287018544935806 6521
0.287731811697575 6536
0.288445078459344 6541
0.289158345221113 6555
0.289871611982882 6563
0.29058487874465 6575
0.291298145506419 6600
0.292011412268188 6620
0.292724679029957 6629
0.293437945791726 6640
0.294151212553495 6650
0.294864479315264 6658
0.295577746077033 6682
0.296291012838802 6697
0.297004279600571 6706
0.297717546362339 6709
0.298430813124108 6718
0.299144079885877 6728
0.299857346647646 6731
0.300570613409415 6732
0.301283880171184 6740
0.301997146932953 6750
0.302710413694722 6761
0.303423680456491 6774
0.30413694721826 6781
0.304850213980029 6784
0.305563480741797 6798
0.306276747503566 6802
0.306990014265335 6814
0.307703281027104 6826
0.308416547788873 6841
0.309129814550642 6848
0.309843081312411 6851
0.31055634807418 6857
0.311269614835949 6864
0.311982881597718 6876
0.312696148359486 6887
0.313409415121255 6891
0.314122681883024 6896
0.314835948644793 6907
0.315549215406562 6910
0.316262482168331 6921
0.3169757489301 6929
0.317689015691869 6934
0.318402282453638 6943
0.319115549215407 6945
0.319828815977175 6965
0.320542082738944 6972
0.321255349500713 6983
0.321968616262482 6991
0.322681883024251 7013
0.32339514978602 7026
0.324108416547789 7031
0.324821683309558 7037
0.325534950071327 7044
0.326248216833096 7057
0.326961483594864 7067
0.327674750356633 7085
0.328388017118402 7094
0.329101283880171 7104
0.32981455064194 7106
0.330527817403709 7114
0.331241084165478 7139
0.331954350927247 7147
0.332667617689016 7170
0.333380884450785 7177
0.334094151212553 7183
0.334807417974322 7198
0.335520684736091 7206
0.33623395149786 7220
0.336947218259629 7230
0.337660485021398 7250
0.338373751783167 7262
0.339087018544936 7264
0.339800285306705 7271
0.340513552068474 7285
0.341226818830243 7292
0.341940085592011 7299
0.34265335235378 7304
0.343366619115549 7314
0.344079885877318 7319
0.344793152639087 7323
0.345506419400856 7329
0.346219686162625 7343
0.346932952924394 7354
0.347646219686163 7370
0.348359486447932 7385
0.3490727532097 7391
0.349786019971469 7400
0.350499286733238 7419
0.351212553495007 7435
0.351925820256776 7445
0.352639087018545 7451
0.353352353780314 7465
0.354065620542083 7476
0.354778887303852 7485
0.355492154065621 7488
0.356205420827389 7490
0.356918687589158 7503
0.357631954350927 7516
0.358345221112696 7542
0.359058487874465 7567
0.359771754636234 7577
0.360485021398003 7601
0.361198288159772 7610
0.361911554921541 7615
0.36262482168331 7618
0.363338088445078 7639
0.364051355206847 7645
0.364764621968616 7651
0.365477888730385 7656
0.366191155492154 7667
0.366904422253923 7675
0.367617689015692 7681
0.368330955777461 7692
0.36904422253923 7699
0.369757489300999 7712
0.370470756062767 7721
0.371184022824536 7731
0.371897289586305 7739
0.372610556348074 7749
0.373323823109843 7778
0.374037089871612 7782
0.374750356633381 7783
0.37546362339515 7797
0.376176890156919 7797
0.376890156918688 7813
0.377603423680457 7857
0.378316690442225 7861
0.379029957203994 7869
0.379743223965763 7871
0.380456490727532 7877
0.381169757489301 7890
0.38188302425107 7905
0.382596291012839 7914
0.383309557774608 7926
0.384022824536377 7932
0.384736091298146 7950
0.385449358059914 7968
0.386162624821683 7976
0.386875891583452 7998
0.387589158345221 8005
0.38830242510699 8022
0.389015691868759 8031
0.389728958630528 8039
0.390442225392297 8056
0.391155492154066 8078
0.391868758915835 8081
0.392582025677603 8091
0.393295292439372 8113
0.394008559201141 8138
0.39472182596291 8157
0.395435092724679 8163
0.396148359486448 8179
0.396861626248217 8211
0.397574893009986 8215
0.398288159771755 8226
0.399001426533524 8240
0.399714693295292 8255
0.400427960057061 8301
0.40114122681883 8313
0.401854493580599 8341
0.402567760342368 8359
0.403281027104137 8381
0.403994293865906 8386
0.404707560627675 8411
0.405420827389444 8427
0.406134094151213 8430
0.406847360912981 8472
0.40756062767475 8478
0.408273894436519 8492
0.408987161198288 8517
0.409700427960057 8543
0.410413694721826 8559
0.411126961483595 8569
0.411840228245364 8597
0.412553495007133 8616
0.413266761768902 8631
0.41398002853067 8647
0.414693295292439 8671
0.415406562054208 8678
0.416119828815977 8682
0.416833095577746 8691
0.417546362339515 8700
0.418259629101284 8708
0.418972895863053 8718
0.419686162624822 8726
0.420399429386591 8737
0.421112696148359 8744
0.421825962910128 8768
0.422539229671897 8786
0.423252496433666 8795
0.423965763195435 8800
0.424679029957204 8805
0.425392296718973 8812
0.426105563480742 8822
0.426818830242511 8830
0.42753209700428 8844
0.428245363766048 8871
0.428958630527817 8884
0.429671897289586 8895
0.430385164051355 8910
0.431098430813124 8922
0.431811697574893 8931
0.432524964336662 8939
0.433238231098431 8954
0.4339514978602 8967
0.434664764621969 8970
0.435378031383738 8985
0.436091298145506 9017
0.436804564907275 9030
0.437517831669044 9041
0.438231098430813 9056
0.438944365192582 9062
0.439657631954351 9067
0.44037089871612 9077
0.441084165477889 9082
0.441797432239658 9087
0.442510699001427 9102
0.443223965763195 9116
0.443937232524964 9158
0.444650499286733 9165
0.445363766048502 9180
0.446077032810271 9190
0.44679029957204 9211
0.447503566333809 9221
0.448216833095578 9248
0.448930099857347 9258
0.449643366619116 9263
0.450356633380884 9271
0.451069900142653 9281
0.451783166904422 9285
0.452496433666191 9299
0.45320970042796 9318
0.453922967189729 9338
0.454636233951498 9348
0.455349500713267 9352
0.456062767475036 9370
0.456776034236805 9407
0.457489300998573 9427
0.458202567760342 9442
0.458915834522111 9453
0.45962910128388 9460
0.460342368045649 9466
0.461055634807418 9486
0.461768901569187 9507
0.462482168330956 9529
0.463195435092725 9538
0.463908701854494 9559
0.464621968616262 9570
0.465335235378031 9600
0.4660485021398 9605
0.466761768901569 9634
0.467475035663338 9651
0.468188302425107 9661
0.468901569186876 9672
0.469614835948645 9677
0.470328102710414 9689
0.471041369472183 9714
0.471754636233951 9727
0.47246790299572 9765
0.473181169757489 9779
0.473894436519258 9788
0.474607703281027 9822
0.475320970042796 9829
0.476034236804565 9848
0.476747503566334 9862
0.477460770328103 9875
0.478174037089872 9890
0.478887303851641 9905
0.479600570613409 9928
0.480313837375178 9946
0.481027104136947 9954
0.481740370898716 9961
0.482453637660485 9981
0.483166904422254 9989
0.483880171184023 9997
0.484593437945792 10017
0.485306704707561 10032
0.48601997146933 10040
0.486733238231098 10045
0.487446504992867 10079
0.488159771754636 10081
0.488873038516405 10093
0.489586305278174 10111
0.490299572039943 10131
0.491012838801712 10141
0.491726105563481 10147
0.49243937232525 10170
0.493152639087019 10179
0.493865905848787 10192
0.494579172610556 10201
0.495292439372325 10211
0.496005706134094 10243
0.496718972895863 10259
0.497432239657632 10269
0.498145506419401 10285
0.49885877318117 10295
0.499572039942939 10313
0.500285306704708 10327
0.500998573466476 10358
0.501711840228245 10367
0.502425106990014 10383
0.503138373751783 10392
0.503851640513552 10400
0.504564907275321 10428
0.50527817403709 10436
0.505991440798859 10447
0.506704707560628 10478
0.507417974322397 10484
0.508131241084166 10506
0.508844507845934 10523
0.509557774607703 10537
0.510271041369472 10550
0.510984308131241 10564
0.51169757489301 10579
0.512410841654779 10588
0.513124108416548 10619
0.513837375178317 10635
0.514550641940086 10655
0.515263908701854 10677
0.515977175463623 10695
0.516690442225392 10720
0.517403708987161 10754
0.51811697574893 10792
0.518830242510699 10802
0.519543509272468 10826
0.520256776034237 10836
0.520970042796006 10870
0.521683309557775 10884
0.522396576319544 10891
0.523109843081312 10912
0.523823109843081 10931
0.52453637660485 10975
0.525249643366619 10985
0.525962910128388 11010
0.526676176890157 11039
0.527389443651926 11067
0.528102710413695 11097
0.528815977175464 11111
0.529529243937233 11115
0.530242510699001 11135
0.53095577746077 11157
0.531669044222539 11177
0.532382310984308 11190
0.533095577746077 11215
0.533808844507846 11223
0.534522111269615 11258
0.535235378031384 11264
0.535948644793153 11289
0.536661911554921 11311
0.53737517831669 11328
0.538088445078459 11353
0.538801711840228 11367
0.539514978601997 11386
0.540228245363766 11392
0.540941512125535 11408
0.541654778887304 11425
0.542368045649073 11433
0.543081312410842 11449
0.543794579172611 11466
0.54450784593438 11491
0.545221112696148 11520
0.545934379457917 11545
0.546647646219686 11576
0.547360912981455 11632
0.548074179743224 11645
0.548787446504993 11675
0.549500713266762 11702
0.550213980028531 11734
0.5509272467903 11783
0.551640513552068 11822
0.552353780313837 11846
0.553067047075606 11868
0.553780313837375 11884
0.554493580599144 11888
0.555206847360913 11907
0.555920114122682 11912
0.556633380884451 11929
0.55734664764622 11981
0.558059914407989 12017
0.558773181169757 12028
0.559486447931526 12050
0.560199714693295 12067
0.560912981455064 12074
0.561626248216833 12084
0.562339514978602 12107
0.563052781740371 12125
0.56376604850214 12257
0.564479315263909 12278
0.565192582025678 12301
0.565905848787447 12343
0.566619115549215 12354
0.567332382310984 12368
0.568045649072753 12379
0.568758915834522 12395
0.569472182596291 12408
0.57018544935806 12428
0.570898716119829 12441
0.571611982881598 12460
0.572325249643367 12479
0.573038516405135 12498
0.573751783166904 12528
0.574465049928673 12560
0.575178316690442 12579
0.575891583452211 12597
0.57660485021398 12624
0.577318116975749 12631
0.578031383737518 12657
0.578744650499287 12667
0.579457917261056 12704
0.580171184022825 12725
0.580884450784593 12743
0.581597717546362 12769
0.582310984308131 12798
0.5830242510699 12814
0.583737517831669 12834
0.584450784593438 12866
0.585164051355207 12890
0.585877318116976 12907
0.586590584878745 12916
0.587303851640514 12946
0.588017118402282 12982
0.588730385164051 13009
0.58944365192582 13043
0.590156918687589 13088
0.590870185449358 13131
0.591583452211127 13167
0.592296718972896 13191
0.593009985734665 13228
0.593723252496434 13257
0.594436519258203 13270
0.595149786019971 13287
0.59586305278174 13310
0.596576319543509 13371
0.597289586305278 13395
0.598002853067047 13427
0.598716119828816 13445
0.599429386590585 13485
0.600142653352354 13531
0.600855920114123 13563
0.601569186875892 13579
0.602282453637661 13590
0.602995720399429 13614
0.603708987161198 13626
0.604422253922967 13635
0.605135520684736 13675
0.605848787446505 13688
0.606562054208274 13699
0.607275320970043 13704
0.607988587731812 13725
0.608701854493581 13755
0.609415121255349 13783
0.610128388017118 13822
0.610841654778887 13851
0.611554921540656 13888
0.612268188302425 13915
0.612981455064194 13961
0.613694721825963 13986
0.614407988587732 14003
0.615121255349501 14046
0.61583452211127 14078
0.616547788873039 14112
0.617261055634807 14132
0.617974322396576 14195
0.618687589158345 14220
0.619400855920114 14245
0.620114122681883 14288
0.620827389443652 14324
0.621540656205421 14350
0.62225392296719 14380
0.622967189728959 14413
0.623680456490728 14438
0.624393723252496 14448
0.625106990014265 14488
0.625820256776034 14493
0.626533523537803 14514
0.627246790299572 14542
0.627960057061341 14572
0.62867332382311 14625
0.629386590584879 14668
0.630099857346648 14789
0.630813124108417 14821
0.631526390870185 14824
0.632239657631954 14860
0.632952924393723 14900
0.633666191155492 14942
0.634379457917261 14999
0.63509272467903 15018
0.635805991440799 15039
0.636519258202568 15054
0.637232524964337 15076
0.637945791726106 15101
0.638659058487875 15138
0.639372325249643 15151
0.640085592011412 15210
0.640798858773181 15223
0.64151212553495 15253
0.642225392296719 15287
0.642938659058488 15321
0.643651925820257 15352
0.644365192582026 15392
0.645078459343795 15419
0.645791726105563 15439
0.646504992867332 15448
0.647218259629101 15502
0.64793152639087 15578
0.648644793152639 15604
0.649358059914408 15628
0.650071326676177 15661
0.650784593437946 15704
0.651497860199715 15729
0.652211126961484 15769
0.652924393723253 15807
0.653637660485021 15844
0.65435092724679 15896
0.655064194008559 15936
0.655777460770328 15983
0.656490727532097 16003
0.657203994293866 16031
0.657917261055635 16079
0.658630527817404 16104
0.659343794579173 16140
0.660057061340942 16221
0.66077032810271 16256
0.661483594864479 16285
0.662196861626248 16313
0.662910128388017 16337
0.663623395149786 16361
0.664336661911555 16395
0.665049928673324 16457
0.665763195435093 16467
0.666476462196862 16506
0.66718972895863 16544
0.667902995720399 16608
0.668616262482168 16640
0.669329529243937 16671
0.670042796005706 16711
0.670756062767475 16733
0.671469329529244 16800
0.672182596291013 16832
0.672895863052782 16840
0.673609129814551 17250
0.67432239657632 17288
0.675035663338088 17325
0.675748930099857 17338
0.676462196861626 17387
0.677175463623395 17420
0.677888730385164 17450
0.678601997146933 17474
0.679315263908702 17501
0.680028530670471 17539
0.68074179743224 17590
0.681455064194009 17631
0.682168330955777 17696
0.682881597717546 17762
0.683594864479315 17809
0.684308131241084 17853
0.685021398002853 17885
0.685734664764622 17920
0.686447931526391 17949
0.68716119828816 17998
0.687874465049929 18043
0.688587731811698 18090
0.689300998573466 18106
0.690014265335235 18153
0.690727532097004 18173
0.691440798858773 18252
0.692154065620542 18267
0.692867332382311 18343
0.69358059914408 18381
0.694293865905849 18440
0.695007132667618 18463
0.695720399429387 18483
0.696433666191156 18519
0.697146932952924 18588
0.697860199714693 18626
0.698573466476462 18664
0.699286733238231 18712
0.7 18765
0.700713266761769 18782
0.701426533523538 18856
0.702139800285307 18907
0.702853067047076 18962
0.703566333808844 19030
0.704279600570613 19080
0.704992867332382 19460
0.705706134094151 19483
0.70641940085592 19553
0.707132667617689 19591
0.707845934379458 19673
0.708559201141227 19710
0.709272467902996 19774
0.709985734664765 19813
0.710699001426534 19834
0.711412268188302 19873
0.712125534950071 19918
0.71283880171184 19973
0.713552068473609 20018
0.714265335235378 20063
0.714978601997147 20084
0.715691868758916 20113
0.716405135520685 20149
0.717118402282454 20198
0.717831669044223 20267
0.718544935805991 20268
0.71925820256776 20333
0.719971469329529 20376
0.720684736091298 20399
0.721398002853067 20437
0.722111269614836 20492
0.722824536376605 20539
0.723537803138374 20582
0.724251069900143 20648
0.724964336661912 20681
0.72567760342368 20718
0.726390870185449 20777
0.727104136947218 20843
0.727817403708987 20862
0.728530670470756 20900
0.729243937232525 20978
0.729957203994294 21033
0.730670470756063 21134
0.731383737517832 21163
0.732097004279601 21184
0.732810271041369 21217
0.733523537803138 21257
0.734236804564907 21273
0.734950071326676 21327
0.735663338088445 21426
0.736376604850214 21471
0.737089871611983 21553
0.737803138373752 21664
0.738516405135521 21693
0.73922967189729 21762
0.739942938659058 21821
0.740656205420827 21842
0.741369472182596 21850
0.742082738944365 21898
0.742796005706134 21940
0.743509272467903 21977
0.744222539229672 22044
0.744935805991441 22107
0.74564907275321 22167
0.746362339514979 22198
0.747075606276748 22249
0.747788873038516 22287
0.748502139800285 22333
0.749215406562054 22356
0.749928673323823 22420
0.750641940085592 22477
0.751355206847361 22522
0.75206847360913 22575
0.752781740370899 22633
0.753495007132668 22670
0.754208273894437 22713
0.754921540656205 22819
0.755634807417974 22868
0.756348074179743 22939
0.757061340941512 23022
0.757774607703281 23427
0.75848787446505 23532
0.759201141226819 23622
0.759914407988588 23679
0.760627674750357 23709
0.761340941512126 23738
0.762054208273894 23767
0.762767475035663 23843
0.763480741797432 23912
0.764194008559201 23953
0.76490727532097 23987
0.765620542082739 23995
0.766333808844508 24037
0.767047075606277 24118
0.767760342368046 24173
0.768473609129815 24225
0.769186875891583 24249
0.769900142653352 24355
0.770613409415121 24418
0.77132667617689 24469
0.772039942938659 24512
0.772753209700428 24564
0.773466476462197 24589
0.774179743223966 24650
0.774893009985735 24720
0.775606276747504 24828
0.776319543509272 24929
0.777032810271041 24990
0.77774607703281 25101
0.778459343794579 25140
0.779172610556348 25275
0.779885877318117 25333
0.780599144079886 25365
0.781312410841655 25429
0.782025677603424 25475
0.782738944365193 25521
0.783452211126962 25541
0.78416547788873 25613
0.784878744650499 25655
0.785592011412268 25718
0.786305278174037 25773
0.787018544935806 25809
0.787731811697575 25912
0.788445078459344 25974
0.789158345221113 26011
0.789871611982882 26073
0.790584878744651 26086
0.791298145506419 26129
0.792011412268188 26248
0.792724679029957 26340
0.793437945791726 26796
0.794151212553495 26844
0.794864479315264 27009
0.795577746077033 27089
0.796291012838802 27174
0.797004279600571 27239
0.797717546362339 27333
0.798430813124108 27460
0.799144079885877 27511
0.799857346647646 27568
0.800570613409415 27670
0.801283880171184 27761
0.801997146932953 27825
0.802710413694722 27870
0.803423680456491 27886
0.80413694721826 28008
0.804850213980029 28044
0.805563480741797 28118
0.806276747503566 28214
0.806990014265335 28238
0.807703281027104 28281
0.808416547788873 28316
0.809129814550642 28404
0.809843081312411 28448
0.81055634807418 28524
0.811269614835949 28595
0.811982881597718 28675
0.812696148359486 28753
0.813409415121255 28849
0.814122681883024 28882
0.814835948644793 28953
0.815549215406562 29012
0.816262482168331 29167
0.8169757489301 29199
0.817689015691869 29312
0.818402282453638 29360
0.819115549215407 29433
0.819828815977175 29568
0.820542082738944 29621
0.821255349500713 29798
0.821968616262482 29965
0.822681883024251 30071
0.82339514978602 30313
0.824108416547789 30382
0.824821683309558 30410
0.825534950071327 30454
0.826248216833096 30549
0.826961483594864 30637
0.827674750356633 30721
0.828388017118402 30765
0.829101283880171 30803
0.82981455064194 30850
0.830527817403709 30874
0.831241084165478 30965
0.831954350927247 31060
0.832667617689016 31086
0.833380884450785 31198
0.834094151212553 31267
0.834807417974322 31411
0.835520684736091 31477
0.83623395149786 31507
0.836947218259629 31634
0.837660485021398 31698
0.838373751783167 31851
0.839087018544936 31890
0.839800285306705 32095
0.840513552068474 32185
0.841226818830243 32260
0.841940085592011 32354
0.84265335235378 32445
0.843366619115549 32558
0.844079885877318 32683
0.844793152639087 32770
0.845506419400856 32870
0.846219686162625 32907
0.846932952924394 32969
0.847646219686163 33197
0.848359486447932 33335
0.8490727532097 33443
0.849786019971469 33585
0.850499286733238 33613
0.851212553495007 33694
0.851925820256776 33805
0.852639087018545 33896
0.853352353780314 34005
0.854065620542083 34119
0.854778887303852 34331
0.855492154065621 34507
0.856205420827389 34586
0.856918687589158 34661
0.857631954350927 34708
0.858345221112696 34917
0.859058487874465 34981
0.859771754636234 35059
0.860485021398003 35180
0.861198288159772 35269
0.861911554921541 35358
0.86262482168331 35560
0.863338088445078 35736
0.864051355206847 35833
0.864764621968616 36015
0.865477888730385 36082
0.866191155492154 36145
0.866904422253923 36187
0.867617689015692 36298
0.868330955777461 36462
0.86904422253923 36590
0.869757489300999 36671
0.870470756062767 36802
0.871184022824536 36966
0.871897289586305 37021
0.872610556348074 37153
0.873323823109843 37344
0.874037089871612 37537
0.874750356633381 37660
0.87546362339515 37767
0.876176890156919 37854
0.876890156918688 38030
0.877603423680457 38206
0.878316690442225 38288
0.879029957203994 38428
0.879743223965763 38554
0.880456490727532 38639
0.881169757489301 38854
0.88188302425107 38891
0.882596291012839 39151
0.883309557774608 39283
0.884022824536377 39392
0.884736091298145 39565
0.885449358059914 39694
0.886162624821683 39830
0.886875891583452 39926
0.887589158345221 40045
0.88830242510699 40164
0.889015691868759 40271
0.889728958630528 40414
0.890442225392297 40504
0.891155492154066 40708
0.891868758915835 40817
0.892582025677603 40979
0.893295292439372 41160
0.894008559201141 41635
0.89472182596291 41807
0.895435092724679 42032
0.896148359486448 42169
0.896861626248217 42364
0.897574893009986 42482
0.898288159771755 42635
0.899001426533524 42832
0.899714693295292 43069
0.900427960057061 43456
0.90114122681883 43577
0.901854493580599 43956
0.902567760342368 44109
0.903281027104137 44242
0.903994293865906 44313
0.904707560627675 44557
0.905420827389444 44658
0.906134094151213 44929
0.906847360912981 45123
0.90756062767475 45237
0.908273894436519 45424
0.908987161198288 45783
0.909700427960057 46073
0.910413694721826 46535
0.911126961483595 46654
0.911840228245364 46829
0.912553495007133 47443
0.913266761768902 47639
0.913980028530671 47755
0.914693295292439 48024
0.915406562054208 48170
0.916119828815977 48479
0.916833095577746 48593
0.917546362339515 48863
0.918259629101284 48915
0.918972895863053 49082
0.919686162624822 49410
0.920399429386591 49710
0.921112696148359 49836
0.921825962910128 50177
0.922539229671897 50391
0.923252496433666 50588
0.923965763195435 50824
0.924679029957204 51168
0.925392296718973 51583
0.926105563480742 52147
0.926818830242511 52773
0.92753209700428 53540
0.928245363766048 53768
0.928958630527817 53942
0.929671897289586 54304
0.930385164051355 54458
0.931098430813124 54878
0.931811697574893 54973
0.932524964336662 55460
0.933238231098431 55922
0.9339514978602 56173
0.934664764621969 56741
0.935378031383738 57717
0.936091298145506 58048
0.936804564907275 58548
0.937517831669044 58743
0.938231098430813 59204
0.938944365192582 59467
0.939657631954351 59794
0.94037089871612 60030
0.941084165477889 60548
0.941797432239658 60887
0.942510699001427 61438
0.943223965763195 61821
0.943937232524964 62200
0.944650499286733 62670
0.945363766048502 63162
0.946077032810271 63716
0.94679029957204 64040
0.947503566333809 64347
0.948216833095578 64853
0.948930099857347 65398
0.949643366619116 65886
0.950356633380884 66538
0.951069900142653 66794
0.951783166904422 67601
0.952496433666191 68232
0.95320970042796 68676
0.953922967189729 69465
0.954636233951498 70083
0.955349500713267 70792
0.956062767475036 71264
0.956776034236805 71507
0.957489300998573 72206
0.958202567760342 73345
0.958915834522111 74395
0.95962910128388 75392
0.960342368045649 76114
0.961055634807418 77812
0.961768901569187 78272
0.962482168330956 79474
0.963195435092725 79797
0.963908701854494 81198
0.964621968616262 82562
0.965335235378031 83052
0.9660485021398 85111
0.966761768901569 86786
0.967475035663338 87200
0.968188302425107 89206
0.968901569186876 90488
0.969614835948645 91902
0.970328102710414 93470
0.971041369472183 94624
0.971754636233952 96602
0.97246790299572 98315
0.973181169757489 101574
0.973894436519258 104715
0.974607703281027 106010
0.975320970042796 107818
0.976034236804565 109858
0.976747503566334 112633
0.977460770328103 116434
0.978174037089872 118857
0.97888730385164 121067
0.979600570613409 123991
0.980313837375178 127973
0.981027104136947 132100
0.981740370898716 136662
0.982453637660485 139321
0.983166904422254 141086
0.983880171184023 143979
0.984593437945792 146249
0.985306704707561 152731
0.98601997146933 159647
0.986733238231098 163312
0.987446504992867 166475
0.988159771754636 172166
0.988873038516405 182401
0.989586305278174 190132
0.990299572039943 195244
0.991012838801712 200674
0.991726105563481 222401
0.99243937232525 238042
0.993152639087019 254849
0.993865905848787 263830
0.994579172610556 293626
0.995292439372325 323473
0.996005706134094 390597
0.996718972895863 543547
};
\addlegendentry{resnet101-imagenet}
\addplot [semithick, white!46.6666666666667!black]
table {0.000998573466476462 2712
0.00171184022824536 2939
0.00242510699001427 3408
0.00313837375178317 3431
0.00385164051355207 3465
0.00456490727532097 3466
0.00527817403708987 3478
0.00599144079885877 3493
0.00670470756062767 3504
0.00741797432239658 3510
0.00813124108416548 3523
0.00884450784593438 3533
0.00955777460770328 3537
0.0102710413694722 3545
0.0109843081312411 3550
0.01169757489301 3553
0.0124108416547789 3564
0.0131241084165478 3568
0.0138373751783167 3575
0.0145506419400856 3587
0.0152639087018545 3588
0.0159771754636234 3589
0.0166904422253923 3589
0.0174037089871612 3591
0.0181169757489301 3593
0.018830242510699 3599
0.0195435092724679 3600
0.0202567760342368 3602
0.0209700427960057 3602
0.0216833095577746 3616
0.0223965763195435 3617
0.0231098430813124 3618
0.0238231098430813 3620
0.0245363766048502 3631
0.0252496433666191 3643
0.025962910128388 3644
0.0266761768901569 3665
0.0273894436519258 3667
0.0281027104136947 3667
0.0288159771754636 3674
0.0295292439372325 3678
0.0302425106990014 3681
0.0309557774607703 3687
0.0316690442225392 3687
0.0323823109843081 3698
0.033095577746077 3701
0.0338088445078459 3714
0.0345221112696148 3716
0.0352353780313837 3717
0.0359486447931526 3720
0.0366619115549215 3721
0.0373751783166904 3727
0.0380884450784593 3741
0.0388017118402282 3749
0.0395149786019971 3751
0.040228245363766 3756
0.0409415121255349 3757
0.0416547788873039 3758
0.0423680456490728 3766
0.0430813124108417 3773
0.0437945791726106 3780
0.0445078459343795 3780
0.0452211126961484 3781
0.0459343794579173 3781
0.0466476462196862 3782
0.0473609129814551 3784
0.048074179743224 3797
0.0487874465049929 3797
0.0495007132667618 3798
0.0502139800285307 3805
0.0509272467902996 3806
0.0516405135520685 3809
0.0523537803138374 3815
0.0530670470756063 3821
0.0537803138373752 3823
0.0544935805991441 3832
0.055206847360913 3840
0.0559201141226819 3845
0.0566333808844508 3849
0.0573466476462197 3864
0.0580599144079886 3865
0.0587731811697575 3876
0.0594864479315264 3879
0.0601997146932953 3883
0.0609129814550642 3892
0.0616262482168331 3894
0.062339514978602 3902
0.0630527817403709 3904
0.0637660485021398 3907
0.0644793152639087 3917
0.0651925820256776 3924
0.0659058487874465 3927
0.0666191155492154 3930
0.0673323823109843 3935
0.0680456490727532 3945
0.0687589158345221 3952
0.069472182596291 3955
0.0701854493580599 3959
0.0708987161198288 3967
0.0716119828815977 3977
0.0723252496433666 3983
0.0730385164051355 4004
0.0737517831669044 4011
0.0744650499286733 4012
0.0751783166904422 4013
0.0758915834522111 4024
0.07660485021398 4027
0.0773181169757489 4030
0.0780313837375178 4030
0.0787446504992867 4034
0.0794579172610556 4037
0.0801711840228245 4037
0.0808844507845934 4041
0.0815977175463623 4048
0.0823109843081312 4056
0.0830242510699001 4062
0.0837375178316691 4065
0.084450784593438 4065
0.0851640513552068 4074
0.0858773181169757 4081
0.0865905848787446 4088
0.0873038516405135 4094
0.0880171184022824 4096
0.0887303851640514 4099
0.0894436519258203 4114
0.0901569186875892 4116
0.0908701854493581 4123
0.091583452211127 4127
0.0922967189728959 4132
0.0930099857346648 4134
0.0937232524964337 4135
0.0944365192582026 4137
0.0951497860199715 4140
0.0958630527817404 4148
0.0965763195435093 4160
0.0972895863052782 4162
0.0980028530670471 4168
0.098716119828816 4172
0.0994293865905849 4177
0.100142653352354 4178
0.100855920114123 4191
0.101569186875892 4202
0.10228245363766 4204
0.102995720399429 4204
0.103708987161198 4214
0.104422253922967 4219
0.105135520684736 4221
0.105848787446505 4222
0.106562054208274 4223
0.107275320970043 4224
0.107988587731812 4225
0.108701854493581 4225
0.109415121255349 4227
0.110128388017118 4238
0.110841654778887 4243
0.111554921540656 4254
0.112268188302425 4264
0.112981455064194 4271
0.113694721825963 4272
0.114407988587732 4273
0.115121255349501 4281
0.11583452211127 4286
0.116547788873039 4296
0.117261055634807 4299
0.117974322396576 4305
0.118687589158345 4306
0.119400855920114 4308
0.120114122681883 4319
0.120827389443652 4330
0.121540656205421 4337
0.12225392296719 4345
0.122967189728959 4349
0.123680456490728 4355
0.124393723252496 4357
0.125106990014265 4358
0.125820256776034 4363
0.126533523537803 4368
0.127246790299572 4375
0.127960057061341 4378
0.12867332382311 4388
0.129386590584879 4392
0.130099857346648 4397
0.130813124108417 4405
0.131526390870185 4409
0.132239657631954 4412
0.132952924393723 4420
0.133666191155492 4424
0.134379457917261 4427
0.13509272467903 4429
0.135805991440799 4435
0.136519258202568 4439
0.137232524964337 4448
0.137945791726106 4452
0.138659058487874 4464
0.139372325249643 4467
0.140085592011412 4511
0.140798858773181 4514
0.14151212553495 4521
0.142225392296719 4526
0.142938659058488 4533
0.143651925820257 4536
0.144365192582026 4545
0.145078459343795 4561
0.145791726105563 4562
0.146504992867332 4567
0.147218259629101 4577
0.14793152639087 4592
0.148644793152639 4596
0.149358059914408 4599
0.150071326676177 4603
0.150784593437946 4606
0.151497860199715 4609
0.152211126961484 4610
0.152924393723253 4611
0.153637660485021 4614
0.15435092724679 4617
0.155064194008559 4620
0.155777460770328 4629
0.156490727532097 4635
0.157203994293866 4636
0.157917261055635 4645
0.158630527817404 4648
0.159343794579173 4652
0.160057061340942 4654
0.16077032810271 4656
0.161483594864479 4657
0.162196861626248 4662
0.162910128388017 4675
0.163623395149786 4678
0.164336661911555 4685
0.165049928673324 4704
0.165763195435093 4710
0.166476462196862 4710
0.167189728958631 4714
0.167902995720399 4716
0.168616262482168 4721
0.169329529243937 4723
0.170042796005706 4735
0.170756062767475 4739
0.171469329529244 4743
0.172182596291013 4750
0.172895863052782 4754
0.173609129814551 4762
0.17432239657632 4766
0.175035663338088 4767
0.175748930099857 4771
0.176462196861626 4774
0.177175463623395 4784
0.177888730385164 4784
0.178601997146933 4790
0.179315263908702 4791
0.180028530670471 4797
0.18074179743224 4803
0.181455064194009 4803
0.182168330955777 4803
0.182881597717546 4814
0.183594864479315 4827
0.184308131241084 4828
0.185021398002853 4833
0.185734664764622 4837
0.186447931526391 4847
0.18716119828816 4851
0.187874465049929 4854
0.188587731811698 4861
0.189300998573466 4865
0.190014265335235 4867
0.190727532097004 4869
0.191440798858773 4875
0.192154065620542 4894
0.192867332382311 4903
0.19358059914408 4905
0.194293865905849 4906
0.195007132667618 4915
0.195720399429387 4918
0.196433666191155 4918
0.197146932952924 4924
0.197860199714693 4930
0.198573466476462 4940
0.199286733238231 4941
0.2 4951
0.200713266761769 4959
0.201426533523538 4963
0.202139800285307 4964
0.202853067047076 4967
0.203566333808845 4969
0.204279600570613 4980
0.204992867332382 4986
0.205706134094151 4994
0.20641940085592 5002
0.207132667617689 5009
0.207845934379458 5011
0.208559201141227 5014
0.209272467902996 5020
0.209985734664765 5023
0.210699001426534 5028
0.211412268188302 5036
0.212125534950071 5041
0.21283880171184 5046
0.213552068473609 5048
0.214265335235378 5059
0.214978601997147 5064
0.215691868758916 5067
0.216405135520685 5071
0.217118402282454 5083
0.217831669044223 5093
0.218544935805991 5095
0.21925820256776 5097
0.219971469329529 5099
0.220684736091298 5107
0.221398002853067 5111
0.222111269614836 5120
0.222824536376605 5126
0.223537803138374 5137
0.224251069900143 5145
0.224964336661912 5159
0.22567760342368 5161
0.226390870185449 5167
0.227104136947218 5175
0.227817403708987 5179
0.228530670470756 5183
0.229243937232525 5185
0.229957203994294 5189
0.230670470756063 5192
0.231383737517832 5196
0.232097004279601 5202
0.232810271041369 5208
0.233523537803138 5222
0.234236804564907 5228
0.234950071326676 5233
0.235663338088445 5237
0.236376604850214 5244
0.237089871611983 5245
0.237803138373752 5251
0.238516405135521 5252
0.23922967189729 5259
0.239942938659058 5646
0.240656205420827 5652
0.241369472182596 5668
0.242082738944365 5757
0.242796005706134 5766
0.243509272467903 5774
0.244222539229672 5788
0.244935805991441 5793
0.24564907275321 5795
0.246362339514979 5795
0.247075606276748 5800
0.247788873038516 5812
0.248502139800285 5817
0.249215406562054 5820
0.249928673323823 5824
0.250641940085592 5833
0.251355206847361 5846
0.25206847360913 5854
0.252781740370899 5856
0.253495007132668 5860
0.254208273894437 5866
0.254921540656205 5866
0.255634807417974 5872
0.256348074179743 5875
0.257061340941512 5879
0.257774607703281 5883
0.25848787446505 5891
0.259201141226819 5897
0.259914407988588 5914
0.260627674750357 5923
0.261340941512126 5925
0.262054208273894 5938
0.262767475035663 5949
0.263480741797432 5960
0.264194008559201 5971
0.26490727532097 5980
0.265620542082739 5982
0.266333808844508 5992
0.267047075606277 5994
0.267760342368046 6000
0.268473609129815 6004
0.269186875891583 6022
0.269900142653352 6032
0.270613409415121 6048
0.27132667617689 6057
0.272039942938659 6063
0.272753209700428 6072
0.273466476462197 6076
0.274179743223966 6085
0.274893009985735 6090
0.275606276747504 6091
0.276319543509272 6112
0.277032810271041 6115
0.27774607703281 6121
0.278459343794579 6124
0.279172610556348 6136
0.279885877318117 6140
0.280599144079886 6149
0.281312410841655 6157
0.282025677603424 6162
0.282738944365193 6165
0.283452211126962 6168
0.28416547788873 6174
0.284878744650499 6174
0.285592011412268 6178
0.286305278174037 6185
0.287018544935806 6186
0.287731811697575 6205
0.288445078459344 6214
0.289158345221113 6224
0.289871611982882 6235
0.29058487874465 6246
0.291298145506419 6252
0.292011412268188 6268
0.292724679029957 6279
0.293437945791726 6287
0.294151212553495 6288
0.294864479315264 6290
0.295577746077033 6310
0.296291012838802 6325
0.297004279600571 6339
0.297717546362339 6342
0.298430813124108 6346
0.299144079885877 6359
0.299857346647646 6368
0.300570613409415 6374
0.301283880171184 6374
0.301997146932953 6375
0.302710413694722 6383
0.303423680456491 6388
0.30413694721826 6392
0.304850213980029 6403
0.305563480741797 6412
0.306276747503566 6414
0.306990014265335 6427
0.307703281027104 6434
0.308416547788873 6440
0.309129814550642 6444
0.309843081312411 6455
0.31055634807418 6458
0.311269614835949 6466
0.311982881597718 6470
0.312696148359486 6474
0.313409415121255 6480
0.314122681883024 6485
0.314835948644793 6505
0.315549215406562 6513
0.316262482168331 6518
0.3169757489301 6522
0.317689015691869 6528
0.318402282453638 6532
0.319115549215407 6547
0.319828815977175 6557
0.320542082738944 6559
0.321255349500713 6578
0.321968616262482 6580
0.322681883024251 6592
0.32339514978602 6599
0.324108416547789 6603
0.324821683309558 6609
0.325534950071327 6621
0.326248216833096 6622
0.326961483594864 6639
0.327674750356633 6641
0.328388017118402 6647
0.329101283880171 6663
0.32981455064194 6670
0.330527817403709 6673
0.331241084165478 6677
0.331954350927247 6686
0.332667617689016 6690
0.333380884450785 6710
0.334094151212553 6717
0.334807417974322 6729
0.335520684736091 6736
0.33623395149786 6741
0.336947218259629 6750
0.337660485021398 6767
0.338373751783167 6776
0.339087018544936 6785
0.339800285306705 6793
0.340513552068474 6802
0.341226818830243 6817
0.341940085592011 6828
0.34265335235378 6839
0.343366619115549 6852
0.344079885877318 6865
0.344793152639087 6878
0.345506419400856 6896
0.346219686162625 6905
0.346932952924394 6910
0.347646219686163 6917
0.348359486447932 6926
0.3490727532097 6942
0.349786019971469 6963
0.350499286733238 6978
0.351212553495007 6987
0.351925820256776 6999
0.352639087018545 7007
0.353352353780314 7019
0.354065620542083 7027
0.354778887303852 7040
0.355492154065621 7043
0.356205420827389 7049
0.356918687589158 7059
0.357631954350927 7068
0.358345221112696 7081
0.359058487874465 7095
0.359771754636234 7106
0.360485021398003 7122
0.361198288159772 7135
0.361911554921541 7139
0.36262482168331 7152
0.363338088445078 7170
0.364051355206847 7179
0.364764621968616 7183
0.365477888730385 7188
0.366191155492154 7193
0.366904422253923 7207
0.367617689015692 7211
0.368330955777461 7233
0.36904422253923 7246
0.369757489300999 7267
0.370470756062767 7277
0.371184022824536 7289
0.371897289586305 7294
0.372610556348074 7309
0.373323823109843 7335
0.374037089871612 7349
0.374750356633381 7354
0.37546362339515 7365
0.376176890156919 7377
0.376890156918688 7399
0.377603423680457 7424
0.378316690442225 7434
0.379029957203994 7453
0.379743223965763 7472
0.380456490727532 7482
0.381169757489301 7493
0.38188302425107 7510
0.382596291012839 7516
0.383309557774608 7528
0.384022824536377 7539
0.384736091298146 7544
0.385449358059914 7546
0.386162624821683 7557
0.386875891583452 7567
0.387589158345221 7574
0.38830242510699 7583
0.389015691868759 7593
0.389728958630528 7596
0.390442225392297 7609
0.391155492154066 7620
0.391868758915835 7640
0.392582025677603 7646
0.393295292439372 7657
0.394008559201141 7658
0.39472182596291 7662
0.395435092724679 7676
0.396148359486448 7678
0.396861626248217 7698
0.397574893009986 7709
0.398288159771755 7725
0.399001426533524 7749
0.399714693295292 7762
0.400427960057061 7765
0.40114122681883 7771
0.401854493580599 7776
0.402567760342368 7791
0.403281027104137 7797
0.403994293865906 7845
0.404707560627675 7864
0.405420827389444 7869
0.406134094151213 7895
0.406847360912981 7904
0.40756062767475 7918
0.408273894436519 7936
0.408987161198288 7968
0.409700427960057 7985
0.410413694721826 7986
0.411126961483595 7999
0.411840228245364 8001
0.412553495007133 8014
0.413266761768902 8025
0.41398002853067 8043
0.414693295292439 8056
0.415406562054208 8071
0.416119828815977 8077
0.416833095577746 8082
0.417546362339515 8093
0.418259629101284 8098
0.418972895863053 8106
0.419686162624822 8111
0.420399429386591 8116
0.421112696148359 8128
0.421825962910128 8142
0.422539229671897 8151
0.423252496433666 8174
0.423965763195435 8194
0.424679029957204 8203
0.425392296718973 8217
0.426105563480742 8234
0.426818830242511 8273
0.42753209700428 8274
0.428245363766048 8282
0.428958630527817 8310
0.429671897289586 8326
0.430385164051355 8331
0.431098430813124 8340
0.431811697574893 8358
0.432524964336662 8362
0.433238231098431 8366
0.4339514978602 8392
0.434664764621969 8408
0.435378031383738 8417
0.436091298145506 8435
0.436804564907275 8452
0.437517831669044 8456
0.438231098430813 8464
0.438944365192582 8486
0.439657631954351 8486
0.44037089871612 8495
0.441084165477889 8518
0.441797432239658 8551
0.442510699001427 8569
0.443223965763195 8603
0.443937232524964 8620
0.444650499286733 8632
0.445363766048502 8638
0.446077032810271 8656
0.44679029957204 8669
0.447503566333809 8682
0.448216833095578 8686
0.448930099857347 8691
0.449643366619116 8696
0.450356633380884 8718
0.451069900142653 8732
0.451783166904422 8747
0.452496433666191 8774
0.45320970042796 8785
0.453922967189729 8791
0.454636233951498 8806
0.455349500713267 8816
0.456062767475036 8837
0.456776034236805 8860
0.457489300998573 8877
0.458202567760342 8894
0.458915834522111 8916
0.45962910128388 8929
0.460342368045649 8944
0.461055634807418 8953
0.461768901569187 8971
0.462482168330956 8996
0.463195435092725 9015
0.463908701854494 9022
0.464621968616262 9024
0.465335235378031 9059
0.4660485021398 9079
0.466761768901569 9105
0.467475035663338 9110
0.468188302425107 9117
0.468901569186876 9136
0.469614835948645 9155
0.470328102710414 9178
0.471041369472183 9190
0.471754636233951 9196
0.47246790299572 9207
0.473181169757489 9222
0.473894436519258 9247
0.474607703281027 9261
0.475320970042796 9271
0.476034236804565 9276
0.476747503566334 9286
0.477460770328103 9305
0.478174037089872 9321
0.478887303851641 9323
0.479600570613409 9328
0.480313837375178 9343
0.481027104136947 9362
0.481740370898716 9380
0.482453637660485 9396
0.483166904422254 9416
0.483880171184023 9433
0.484593437945792 9445
0.485306704707561 9459
0.48601997146933 9465
0.486733238231098 9475
0.487446504992867 9480
0.488159771754636 9499
0.488873038516405 9526
0.489586305278174 9541
0.490299572039943 9573
0.491012838801712 9578
0.491726105563481 9587
0.49243937232525 9602
0.493152639087019 9613
0.493865905848787 9630
0.494579172610556 9651
0.495292439372325 9674
0.496005706134094 9684
0.496718972895863 9712
0.497432239657632 9717
0.498145506419401 9730
0.49885877318117 9743
0.499572039942939 9765
0.500285306704708 9776
0.500998573466476 9808
0.501711840228245 9838
0.502425106990014 9844
0.503138373751783 9857
0.503851640513552 9865
0.504564907275321 9876
0.50527817403709 9888
0.505991440798859 9930
0.506704707560628 9938
0.507417974322397 9956
0.508131241084166 9975
0.508844507845934 9987
0.509557774607703 9994
0.510271041369472 10007
0.510984308131241 10021
0.51169757489301 10051
0.512410841654779 10058
0.513124108416548 10070
0.513837375178317 10074
0.514550641940086 10089
0.515263908701854 10108
0.515977175463623 10120
0.516690442225392 10144
0.517403708987161 10153
0.51811697574893 10179
0.518830242510699 10188
0.519543509272468 10206
0.520256776034237 10219
0.520970042796006 10257
0.521683309557775 10273
0.522396576319544 10306
0.523109843081312 10331
0.523823109843081 10338
0.52453637660485 10352
0.525249643366619 10366
0.525962910128388 10401
0.526676176890157 10406
0.527389443651926 10439
0.528102710413695 10474
0.528815977175464 10488
0.529529243937233 10513
0.530242510699001 10548
0.53095577746077 10552
0.531669044222539 10569
0.532382310984308 10574
0.533095577746077 10579
0.533808844507846 10592
0.534522111269615 10623
0.535235378031384 10635
0.535948644793153 10650
0.536661911554921 10655
0.53737517831669 10684
0.538088445078459 10703
0.538801711840228 10749
0.539514978601997 10757
0.540228245363766 10757
0.540941512125535 10777
0.541654778887304 10798
0.542368045649073 10817
0.543081312410842 10827
0.543794579172611 10851
0.54450784593438 10888
0.545221112696148 10898
0.545934379457917 10909
0.546647646219686 10931
0.547360912981455 10960
0.548074179743224 10984
0.548787446504993 11001
0.549500713266762 11037
0.550213980028531 11053
0.5509272467903 11058
0.551640513552068 11077
0.552353780313837 11095
0.553067047075606 11104
0.553780313837375 11136
0.554493580599144 11174
0.555206847360913 11189
0.555920114122682 11224
0.556633380884451 11231
0.55734664764622 11243
0.558059914407989 11253
0.558773181169757 11272
0.559486447931526 11302
0.560199714693295 11318
0.560912981455064 11346
0.561626248216833 11364
0.562339514978602 11392
0.563052781740371 11441
0.56376604850214 11461
0.564479315263909 11491
0.565192582025678 11500
0.565905848787447 11523
0.566619115549215 11544
0.567332382310984 11565
0.568045649072753 11584
0.568758915834522 11605
0.569472182596291 11620
0.57018544935806 11647
0.570898716119829 11670
0.571611982881598 11681
0.572325249643367 11688
0.573038516405135 11695
0.573751783166904 11721
0.574465049928673 11740
0.575178316690442 11768
0.575891583452211 11785
0.57660485021398 11814
0.577318116975749 11820
0.578031383737518 11839
0.578744650499287 11865
0.579457917261056 11915
0.580171184022825 11945
0.580884450784593 11981
0.581597717546362 12011
0.582310984308131 12054
0.5830242510699 12070
0.583737517831669 12093
0.584450784593438 12100
0.585164051355207 12126
0.585877318116976 12178
0.586590584878745 12204
0.587303851640514 12267
0.588017118402282 12298
0.588730385164051 12326
0.58944365192582 12376
0.590156918687589 12407
0.590870185449358 12432
0.591583452211127 12462
0.592296718972896 12493
0.593009985734665 12515
0.593723252496434 12524
0.594436519258203 12568
0.595149786019971 12599
0.59586305278174 12636
0.596576319543509 12654
0.597289586305278 12700
0.598002853067047 12769
0.598716119828816 12787
0.599429386590585 12820
0.600142653352354 12833
0.600855920114123 12848
0.601569186875892 12860
0.602282453637661 12907
0.602995720399429 12924
0.603708987161198 12954
0.604422253922967 12997
0.605135520684736 13046
0.605848787446505 13089
0.606562054208274 13138
0.607275320970043 13165
0.607988587731812 13179
0.608701854493581 13229
0.609415121255349 13258
0.610128388017118 13310
0.610841654778887 13327
0.611554921540656 13355
0.612268188302425 13385
0.612981455064194 13441
0.613694721825963 13452
0.614407988587732 13480
0.615121255349501 13527
0.61583452211127 13553
0.616547788873039 13569
0.617261055634807 13670
0.617974322396576 13699
0.618687589158345 13766
0.619400855920114 13785
0.620114122681883 13808
0.620827389443652 13841
0.621540656205421 13873
0.62225392296719 13901
0.622967189728959 13918
0.623680456490728 13964
0.624393723252496 13986
0.625106990014265 14018
0.625820256776034 14034
0.626533523537803 14069
0.627246790299572 14176
0.627960057061341 14250
0.62867332382311 14298
0.629386590584879 14356
0.630099857346648 14408
0.630813124108417 14456
0.631526390870185 14477
0.632239657631954 14516
0.632952924393723 14537
0.633666191155492 14551
0.634379457917261 14600
0.63509272467903 14641
0.635805991440799 14769
0.636519258202568 14807
0.637232524964337 14819
0.637945791726106 14848
0.638659058487875 14877
0.639372325249643 14949
0.640085592011412 14982
0.640798858773181 15007
0.64151212553495 15075
0.642225392296719 15110
0.642938659058488 15135
0.643651925820257 15167
0.644365192582026 15184
0.645078459343795 15219
0.645791726105563 15268
0.646504992867332 15324
0.647218259629101 15365
0.64793152639087 15489
0.648644793152639 15531
0.649358059914408 15559
0.650071326676177 15974
0.650784593437946 16027
0.651497860199715 16057
0.652211126961484 16096
0.652924393723253 16135
0.653637660485021 16195
0.65435092724679 16270
0.655064194008559 16326
0.655777460770328 16368
0.656490727532097 16425
0.657203994293866 16491
0.657917261055635 16531
0.658630527817404 16558
0.659343794579173 16608
0.660057061340942 16622
0.66077032810271 16682
0.661483594864479 16725
0.662196861626248 16734
0.662910128388017 16772
0.663623395149786 16823
0.664336661911555 16869
0.665049928673324 16917
0.665763195435093 16971
0.666476462196862 16990
0.66718972895863 17030
0.667902995720399 17113
0.668616262482168 17172
0.669329529243937 17243
0.670042796005706 17282
0.670756062767475 17304
0.671469329529244 17345
0.672182596291013 17377
0.672895863052782 17517
0.673609129814551 17992
0.67432239657632 18068
0.675035663338088 18121
0.675748930099857 18168
0.676462196861626 18202
0.677175463623395 18276
0.677888730385164 18374
0.678601997146933 18418
0.679315263908702 18471
0.680028530670471 18494
0.68074179743224 18546
0.681455064194009 18642
0.682168330955777 18702
0.682881597717546 18752
0.683594864479315 18806
0.684308131241084 18839
0.685021398002853 18957
0.685734664764622 19027
0.686447931526391 19047
0.68716119828816 19093
0.687874465049929 19136
0.688587731811698 19221
0.689300998573466 19292
0.690014265335235 19352
0.690727532097004 19365
0.691440798858773 19452
0.692154065620542 19508
0.692867332382311 19533
0.69358059914408 19597
0.694293865905849 19647
0.695007132667618 19695
0.695720399429387 19736
0.696433666191156 19830
0.697146932952924 19861
0.697860199714693 19892
0.698573466476462 19918
0.699286733238231 19983
0.7 19997
0.700713266761769 20049
0.701426533523538 20076
0.702139800285307 20173
0.702853067047076 20238
0.703566333808844 20280
0.704279600570613 20333
0.704992867332382 20397
0.705706134094151 20423
0.70641940085592 20482
0.707132667617689 20569
0.707845934379458 20689
0.708559201141227 20793
0.709272467902996 20848
0.709985734664765 20903
0.710699001426534 20988
0.711412268188302 21082
0.712125534950071 21106
0.71283880171184 21236
0.713552068473609 21301
0.714265335235378 21424
0.714978601997147 21471
0.715691868758916 21609
0.716405135520685 21753
0.717118402282454 21866
0.717831669044223 21888
0.718544935805991 21960
0.71925820256776 22033
0.719971469329529 22044
0.720684736091298 22088
0.721398002853067 22190
0.722111269614836 22333
0.722824536376605 22384
0.723537803138374 22449
0.724251069900143 22546
0.724964336661912 22642
0.72567760342368 22695
0.726390870185449 22723
0.727104136947218 22793
0.727817403708987 22916
0.728530670470756 22985
0.729243937232525 23089
0.729957203994294 23166
0.730670470756063 23346
0.731383737517832 23385
0.732097004279601 23486
0.732810271041369 23535
0.733523537803138 24016
0.734236804564907 24073
0.734950071326676 24157
0.735663338088445 24325
0.736376604850214 24486
0.737089871611983 24551
0.737803138373752 24619
0.738516405135521 24693
0.73922967189729 24720
0.739942938659058 24737
0.740656205420827 24814
0.741369472182596 24857
0.742082738944365 24924
0.742796005706134 24977
0.743509272467903 25089
0.744222539229672 25244
0.744935805991441 25339
0.74564907275321 25374
0.746362339514979 25450
0.747075606276748 25520
0.747788873038516 25571
0.748502139800285 25672
0.749215406562054 25803
0.749928673323823 25896
0.750641940085592 25965
0.751355206847361 26058
0.75206847360913 26133
0.752781740370899 26186
0.753495007132668 26290
0.754208273894437 26342
0.754921540656205 26531
0.755634807417974 26677
0.756348074179743 26767
0.757061340941512 26836
0.757774607703281 26881
0.75848787446505 27003
0.759201141226819 27042
0.759914407988588 27236
0.760627674750357 27275
0.761340941512126 27414
0.762054208273894 27523
0.762767475035663 27721
0.763480741797432 27867
0.764194008559201 27966
0.76490727532097 27999
0.765620542082739 28098
0.766333808844508 28311
0.767047075606277 28415
0.767760342368046 28587
0.768473609129815 28643
0.769186875891583 28711
0.769900142653352 28760
0.770613409415121 28826
0.77132667617689 28933
0.772039942938659 29051
0.772753209700428 29167
0.773466476462197 29254
0.774179743223966 29433
0.774893009985735 29510
0.775606276747504 29611
0.776319543509272 29767
0.777032810271041 29862
0.77774607703281 29942
0.778459343794579 30043
0.779172610556348 30071
0.779885877318117 30159
0.780599144079886 30264
0.781312410841655 30397
0.782025677603424 30553
0.782738944365193 30610
0.783452211126962 30822
0.78416547788873 30971
0.784878744650499 31133
0.785592011412268 31234
0.786305278174037 31331
0.787018544935806 31450
0.787731811697575 31624
0.788445078459344 31729
0.789158345221113 31798
0.789871611982882 31865
0.790584878744651 32016
0.791298145506419 32162
0.792011412268188 32290
0.792724679029957 32492
0.793437945791726 32670
0.794151212553495 32771
0.794864479315264 32896
0.795577746077033 33013
0.796291012838802 33165
0.797004279600571 33276
0.797717546362339 33534
0.798430813124108 33695
0.799144079885877 33805
0.799857346647646 33875
0.800570613409415 34001
0.801283880171184 34097
0.801997146932953 34235
0.802710413694722 34492
0.803423680456491 34559
0.80413694721826 34656
0.804850213980029 34958
0.805563480741797 35061
0.806276747503566 35192
0.806990014265335 35359
0.807703281027104 35499
0.808416547788873 35588
0.809129814550642 35822
0.809843081312411 36097
0.81055634807418 36299
0.811269614835949 36723
0.811982881597718 36959
0.812696148359486 37167
0.813409415121255 37467
0.814122681883024 37571
0.814835948644793 37706
0.815549215406562 37938
0.816262482168331 38184
0.8169757489301 38426
0.817689015691869 38482
0.818402282453638 38667
0.819115549215407 38967
0.819828815977175 39050
0.820542082738944 39234
0.821255349500713 39440
0.821968616262482 39799
0.822681883024251 39951
0.82339514978602 40124
0.824108416547789 40482
0.824821683309558 40798
0.825534950071327 41186
0.826248216833096 41516
0.826961483594864 41933
0.827674750356633 42262
0.828388017118402 42343
0.829101283880171 42543
0.82981455064194 42725
0.830527817403709 42889
0.831241084165478 43171
0.831954350927247 43360
0.832667617689016 43723
0.833380884450785 44048
0.834094151212553 44416
0.834807417974322 44590
0.835520684736091 44780
0.83623395149786 45005
0.836947218259629 45156
0.837660485021398 45398
0.838373751783167 45910
0.839087018544936 46109
0.839800285306705 46243
0.840513552068474 46677
0.841226818830243 46892
0.841940085592011 47048
0.84265335235378 47216
0.843366619115549 47435
0.844079885877318 47662
0.844793152639087 48225
0.845506419400856 48473
0.846219686162625 48605
0.846932952924394 48781
0.847646219686163 48961
0.848359486447932 49238
0.8490727532097 49498
0.849786019971469 49813
0.850499286733238 50038
0.851212553495007 50633
0.851925820256776 50858
0.852639087018545 50997
0.853352353780314 51234
0.854065620542083 51514
0.854778887303852 51963
0.855492154065621 52335
0.856205420827389 52577
0.856918687589158 52884
0.857631954350927 53203
0.858345221112696 53464
0.859058487874465 53890
0.859771754636234 53960
0.860485021398003 54568
0.861198288159772 54991
0.861911554921541 55202
0.86262482168331 55533
0.863338088445078 55648
0.864051355206847 55966
0.864764621968616 56272
0.865477888730385 56763
0.866191155492154 57267
0.866904422253923 57409
0.867617689015692 57710
0.868330955777461 58049
0.86904422253923 58295
0.869757489300999 58576
0.870470756062767 59116
0.871184022824536 59525
0.871897289586305 59855
0.872610556348074 60224
0.873323823109843 60448
0.874037089871612 60734
0.874750356633381 61303
0.87546362339515 61827
0.876176890156919 62333
0.876890156918688 62599
0.877603423680457 62891
0.878316690442225 63476
0.879029957203994 63718
0.879743223965763 63928
0.880456490727532 64745
0.881169757489301 65253
0.88188302425107 65759
0.882596291012839 66504
0.883309557774608 67086
0.884022824536377 67915
0.884736091298145 68250
0.885449358059914 68793
0.886162624821683 69341
0.886875891583452 69718
0.887589158345221 70393
0.88830242510699 70848
0.889015691868759 71231
0.889728958630528 72176
0.890442225392297 72635
0.891155492154066 73316
0.891868758915835 74130
0.892582025677603 74765
0.893295292439372 75377
0.894008559201141 76261
0.89472182596291 76919
0.895435092724679 77300
0.896148359486448 77792
0.896861626248217 78210
0.897574893009986 78713
0.898288159771755 79738
0.899001426533524 80880
0.899714693295292 81425
0.900427960057061 81803
0.90114122681883 83056
0.901854493580599 83838
0.902567760342368 84971
0.903281027104137 85423
0.903994293865906 85785
0.904707560627675 86184
0.905420827389444 87051
0.906134094151213 87649
0.906847360912981 88099
0.90756062767475 89072
0.908273894436519 90054
0.908987161198288 90332
0.909700427960057 91402
0.910413694721826 92628
0.911126961483595 92885
0.911840228245364 93768
0.912553495007133 94944
0.913266761768902 95569
0.913980028530671 96236
0.914693295292439 97122
0.915406562054208 97827
0.916119828815977 98656
0.916833095577746 100198
0.917546362339515 101779
0.918259629101284 102655
0.918972895863053 103870
0.919686162624822 104289
0.920399429386591 105605
0.921112696148359 106626
0.921825962910128 109375
0.922539229671897 109944
0.923252496433666 110426
0.923965763195435 113817
0.924679029957204 114620
0.925392296718973 115830
0.926105563480742 117148
0.926818830242511 117783
0.92753209700428 119768
0.928245363766048 120566
0.928958630527817 121352
0.929671897289586 123023
0.930385164051355 126029
0.931098430813124 127791
0.931811697574893 128223
0.932524964336662 129482
0.933238231098431 130345
0.9339514978602 133061
0.934664764621969 134736
0.935378031383738 136486
0.936091298145506 139471
0.936804564907275 141114
0.937517831669044 142755
0.938231098430813 144736
0.938944365192582 145645
0.939657631954351 148325
0.94037089871612 151735
0.941084165477889 154034
0.941797432239658 157633
0.942510699001427 160264
0.943223965763195 164375
0.943937232524964 166344
0.944650499286733 167521
0.945363766048502 169307
0.946077032810271 172974
0.94679029957204 176575
0.947503566333809 177956
0.948216833095578 181050
0.948930099857347 184657
0.949643366619116 190123
0.950356633380884 192629
0.951069900142653 196679
0.951783166904422 200052
0.952496433666191 202280
0.95320970042796 203922
0.953922967189729 206006
0.954636233951498 207239
0.955349500713267 212817
0.956062767475036 216904
0.956776034236805 220354
0.957489300998573 222696
0.958202567760342 227385
0.958915834522111 234116
0.95962910128388 235420
0.960342368045649 240644
0.961055634807418 246013
0.961768901569187 251346
0.962482168330956 255063
0.963195435092725 260433
0.963908701854494 273553
0.964621968616262 280211
0.965335235378031 284069
0.9660485021398 287591
0.966761768901569 297510
0.967475035663338 306016
0.968188302425107 309901
0.968901569186876 314395
0.969614835948645 319324
0.970328102710414 328617
0.971041369472183 337731
0.971754636233952 346596
0.97246790299572 354730
0.973181169757489 362780
0.973894436519258 375296
0.974607703281027 387083
0.975320970042796 403325
0.976034236804565 412524
0.976747503566334 428089
0.977460770328103 443456
0.978174037089872 464816
0.97888730385164 473288
0.979600570613409 489396
0.980313837375178 508402
0.981027104136947 536705
0.981740370898716 559199
0.982453637660485 584695
0.983166904422254 600258
0.983880171184023 631181
0.984593437945792 656396
0.985306704707561 679530
0.98601997146933 733349
0.986733238231098 768335
0.987446504992867 814352
0.988159771754636 845680
0.988873038516405 910094
0.989586305278174 1027658
0.990299572039943 1124275
0.991012838801712 1220472
0.991726105563481 1454422
0.99243937232525 1662300
0.993152639087019 1820729
0.993865905848787 1960085
0.994579172610556 2192707
0.995292439372325 2613525
0.996005706134094 3029492
0.996718972895863 3448929
};
\addlegendentry{resnext101-32x8d-wsl}
\addplot [semithick, color3]
table {0.000998573466476462 3477
0.00171184022824536 3531
0.00242510699001427 3552
0.00313837375178317 3567
0.00385164051355207 3573
0.00456490727532097 3575
0.00527817403708987 3581
0.00599144079885877 3587
0.00670470756062767 3588
0.00741797432239658 3588
0.00813124108416548 3588
0.00884450784593438 3596
0.00955777460770328 3617
0.0102710413694722 3618
0.0109843081312411 3627
0.01169757489301 3629
0.0124108416547789 3629
0.0131241084165478 3630
0.0138373751783167 3631
0.0145506419400856 3633
0.0152639087018545 3644
0.0159771754636234 3662
0.0166904422253923 3663
0.0174037089871612 3674
0.0181169757489301 3679
0.018830242510699 3685
0.0195435092724679 3686
0.0202567760342368 3693
0.0209700427960057 3695
0.0216833095577746 3700
0.0223965763195435 3702
0.0231098430813124 3710
0.0238231098430813 3721
0.0245363766048502 3737
0.0252496433666191 3742
0.025962910128388 3744
0.0266761768901569 3751
0.0273894436519258 3762
0.0281027104136947 3780
0.0288159771754636 3792
0.0295292439372325 3799
0.0302425106990014 3811
0.0309557774607703 3814
0.0316690442225392 3821
0.0323823109843081 3829
0.033095577746077 3832
0.0338088445078459 3837
0.0345221112696148 3841
0.0352353780313837 3849
0.0359486447931526 3857
0.0366619115549215 3862
0.0373751783166904 3868
0.0380884450784593 3879
0.0388017118402282 3882
0.0395149786019971 3883
0.040228245363766 3887
0.0409415121255349 3892
0.0416547788873039 3898
0.0423680456490728 3905
0.0430813124108417 3910
0.0437945791726106 3915
0.0445078459343795 3920
0.0452211126961484 3927
0.0459343794579173 3935
0.0466476462196862 3946
0.0473609129814551 3956
0.048074179743224 3971
0.0487874465049929 3980
0.0495007132667618 3986
0.0502139800285307 3990
0.0509272467902996 3994
0.0516405135520685 3998
0.0523537803138374 4004
0.0530670470756063 4011
0.0537803138373752 4022
0.0544935805991441 4023
0.055206847360913 4026
0.0559201141226819 4034
0.0566333808844508 4040
0.0573466476462197 4043
0.0580599144079886 4045
0.0587731811697575 4050
0.0594864479315264 4055
0.0601997146932953 4056
0.0609129814550642 4057
0.0616262482168331 4071
0.062339514978602 4076
0.0630527817403709 4079
0.0637660485021398 4080
0.0644793152639087 4089
0.0651925820256776 4095
0.0659058487874465 4100
0.0666191155492154 4101
0.0673323823109843 4103
0.0680456490727532 4106
0.0687589158345221 4107
0.069472182596291 4119
0.0701854493580599 4120
0.0708987161198288 4122
0.0716119828815977 4124
0.0723252496433666 4125
0.0730385164051355 4126
0.0737517831669044 4134
0.0744650499286733 4139
0.0751783166904422 4141
0.0758915834522111 4143
0.07660485021398 4155
0.0773181169757489 4155
0.0780313837375178 4165
0.0787446504992867 4169
0.0794579172610556 4173
0.0801711840228245 4176
0.0808844507845934 4183
0.0815977175463623 4194
0.0823109843081312 4207
0.0830242510699001 4220
0.0837375178316691 4222
0.084450784593438 4226
0.0851640513552068 4228
0.0858773181169757 4230
0.0865905848787446 4236
0.0873038516405135 4245
0.0880171184022824 4247
0.0887303851640514 4251
0.0894436519258203 4258
0.0901569186875892 4259
0.0908701854493581 4272
0.091583452211127 4275
0.0922967189728959 4277
0.0930099857346648 4279
0.0937232524964337 4288
0.0944365192582026 4289
0.0951497860199715 4298
0.0958630527817404 4308
0.0965763195435093 4311
0.0972895863052782 4311
0.0980028530670471 4312
0.098716119828816 4314
0.0994293865905849 4318
0.100142653352354 4322
0.100855920114123 4325
0.101569186875892 4326
0.10228245363766 4327
0.102995720399429 4331
0.103708987161198 4331
0.104422253922967 4341
0.105135520684736 4344
0.105848787446505 4346
0.106562054208274 4349
0.107275320970043 4352
0.107988587731812 4354
0.108701854493581 4361
0.109415121255349 4367
0.110128388017118 4370
0.110841654778887 4374
0.111554921540656 4378
0.112268188302425 4391
0.112981455064194 4394
0.113694721825963 4400
0.114407988587732 4423
0.115121255349501 4425
0.11583452211127 4428
0.116547788873039 4434
0.117261055634807 4447
0.117974322396576 4451
0.118687589158345 4456
0.119400855920114 4461
0.120114122681883 4463
0.120827389443652 4466
0.121540656205421 4466
0.12225392296719 4471
0.122967189728959 4473
0.123680456490728 4478
0.124393723252496 4482
0.125106990014265 4486
0.125820256776034 4496
0.126533523537803 4506
0.127246790299572 4519
0.127960057061341 4526
0.12867332382311 4536
0.129386590584879 4540
0.130099857346648 4546
0.130813124108417 4548
0.131526390870185 4550
0.132239657631954 4555
0.132952924393723 4561
0.133666191155492 4563
0.134379457917261 4565
0.13509272467903 4568
0.135805991440799 4574
0.136519258202568 4576
0.137232524964337 4580
0.137945791726106 4586
0.138659058487874 4591
0.139372325249643 4598
0.140085592011412 4604
0.140798858773181 4620
0.14151212553495 4622
0.142225392296719 4628
0.142938659058488 4640
0.143651925820257 4648
0.144365192582026 4651
0.145078459343795 4660
0.145791726105563 4662
0.146504992867332 4666
0.147218259629101 4676
0.14793152639087 4683
0.148644793152639 4702
0.149358059914408 4706
0.150071326676177 4710
0.150784593437946 4711
0.151497860199715 4714
0.152211126961484 4720
0.152924393723253 4724
0.153637660485021 4732
0.15435092724679 4734
0.155064194008559 4738
0.155777460770328 4740
0.156490727532097 4746
0.157203994293866 4747
0.157917261055635 4751
0.158630527817404 4765
0.159343794579173 4774
0.160057061340942 4779
0.16077032810271 4781
0.161483594864479 4785
0.162196861626248 4799
0.162910128388017 4806
0.163623395149786 4807
0.164336661911555 4808
0.165049928673324 4818
0.165763195435093 4821
0.166476462196862 4828
0.167189728958631 4830
0.167902995720399 4835
0.168616262482168 4843
0.169329529243937 4846
0.170042796005706 4861
0.170756062767475 4865
0.171469329529244 4880
0.172182596291013 4886
0.172895863052782 4888
0.173609129814551 4893
0.17432239657632 4898
0.175035663338088 4906
0.175748930099857 4915
0.176462196861626 4926
0.177175463623395 4931
0.177888730385164 4938
0.178601997146933 4946
0.179315263908702 4948
0.180028530670471 4956
0.18074179743224 4961
0.181455064194009 4968
0.182168330955777 4976
0.182881597717546 4979
0.183594864479315 4983
0.184308131241084 4995
0.185021398002853 4999
0.185734664764622 5000
0.186447931526391 5006
0.18716119828816 5008
0.187874465049929 5010
0.188587731811698 5013
0.189300998573466 5015
0.190014265335235 5018
0.190727532097004 5026
0.191440798858773 5031
0.192154065620542 5031
0.192867332382311 5039
0.19358059914408 5045
0.194293865905849 5054
0.195007132667618 5062
0.195720399429387 5067
0.196433666191155 5074
0.197146932952924 5083
0.197860199714693 5086
0.198573466476462 5088
0.199286733238231 5097
0.2 5104
0.200713266761769 5108
0.201426533523538 5109
0.202139800285307 5110
0.202853067047076 5117
0.203566333808845 5124
0.204279600570613 5125
0.204992867332382 5130
0.205706134094151 5132
0.20641940085592 5139
0.207132667617689 5145
0.207845934379458 5145
0.208559201141227 5147
0.209272467902996 5156
0.209985734664765 5166
0.210699001426534 5173
0.211412268188302 5182
0.212125534950071 5190
0.21283880171184 5193
0.213552068473609 5205
0.214265335235378 5214
0.214978601997147 5215
0.215691868758916 5221
0.216405135520685 5230
0.217118402282454 5238
0.217831669044223 5246
0.218544935805991 5252
0.21925820256776 5256
0.219971469329529 5256
0.220684736091298 5269
0.221398002853067 5278
0.222111269614836 5285
0.222824536376605 5289
0.223537803138374 5299
0.224251069900143 5307
0.224964336661912 5311
0.22567760342368 5317
0.226390870185449 5323
0.227104136947218 5328
0.227817403708987 5332
0.228530670470756 5338
0.229243937232525 5341
0.229957203994294 5343
0.230670470756063 5356
0.231383737517832 5361
0.232097004279601 5370
0.232810271041369 5375
0.233523537803138 5389
0.234236804564907 5389
0.234950071326676 5391
0.235663338088445 5401
0.236376604850214 5403
0.237089871611983 5407
0.237803138373752 5409
0.238516405135521 5413
0.23922967189729 5414
0.239942938659058 5418
0.240656205420827 5418
0.241369472182596 5424
0.242082738944365 5427
0.242796005706134 5435
0.243509272467903 5448
0.244222539229672 5459
0.244935805991441 5463
0.24564907275321 5467
0.246362339514979 5474
0.247075606276748 5478
0.247788873038516 5480
0.248502139800285 5481
0.249215406562054 5485
0.249928673323823 5495
0.250641940085592 5496
0.251355206847361 5501
0.25206847360913 5501
0.252781740370899 5502
0.253495007132668 5509
0.254208273894437 5511
0.254921540656205 5513
0.255634807417974 5520
0.256348074179743 5537
0.257061340941512 5539
0.257774607703281 5542
0.25848787446505 5557
0.259201141226819 5568
0.259914407988588 5570
0.260627674750357 5576
0.261340941512126 5581
0.262054208273894 5598
0.262767475035663 5609
0.263480741797432 5618
0.264194008559201 5625
0.26490727532097 5630
0.265620542082739 5633
0.266333808844508 5636
0.267047075606277 5641
0.267760342368046 5643
0.268473609129815 5647
0.269186875891583 5648
0.269900142653352 5650
0.270613409415121 5657
0.27132667617689 5660
0.272039942938659 5662
0.272753209700428 5666
0.273466476462197 5675
0.274179743223966 5677
0.274893009985735 5677
0.275606276747504 5682
0.276319543509272 5686
0.277032810271041 5688
0.27774607703281 5693
0.278459343794579 5695
0.279172610556348 5701
0.279885877318117 5704
0.280599144079886 5705
0.281312410841655 5708
0.282025677603424 5714
0.282738944365193 5717
0.283452211126962 5725
0.28416547788873 5729
0.284878744650499 5732
0.285592011412268 5735
0.286305278174037 5737
0.287018544935806 5743
0.287731811697575 5750
0.288445078459344 5757
0.289158345221113 5764
0.289871611982882 5772
0.29058487874465 5789
0.291298145506419 5793
0.292011412268188 5797
0.292724679029957 5805
0.293437945791726 5811
0.294151212553495 5814
0.294864479315264 5818
0.295577746077033 5826
0.296291012838802 5832
0.297004279600571 5836
0.297717546362339 5845
0.298430813124108 5850
0.299144079885877 5856
0.299857346647646 5858
0.300570613409415 5863
0.301283880171184 5864
0.301997146932953 5868
0.302710413694722 5872
0.303423680456491 5874
0.30413694721826 5880
0.304850213980029 5881
0.305563480741797 5882
0.306276747503566 5883
0.306990014265335 5890
0.307703281027104 5891
0.308416547788873 5894
0.309129814550642 5896
0.309843081312411 5905
0.31055634807418 5911
0.311269614835949 5917
0.311982881597718 5923
0.312696148359486 5928
0.313409415121255 5932
0.314122681883024 5935
0.314835948644793 5940
0.315549215406562 5945
0.316262482168331 5948
0.3169757489301 5957
0.317689015691869 5965
0.318402282453638 5969
0.319115549215407 5979
0.319828815977175 5981
0.320542082738944 5985
0.321255349500713 5986
0.321968616262482 6002
0.322681883024251 6004
0.32339514978602 6004
0.324108416547789 6009
0.324821683309558 6011
0.325534950071327 6014
0.326248216833096 6020
0.326961483594864 6021
0.327674750356633 6026
0.328388017118402 6032
0.329101283880171 6037
0.32981455064194 6044
0.330527817403709 6051
0.331241084165478 6056
0.331954350927247 6059
0.332667617689016 6064
0.333380884450785 6065
0.334094151212553 6070
0.334807417974322 6075
0.335520684736091 6081
0.33623395149786 6083
0.336947218259629 6085
0.337660485021398 6094
0.338373751783167 6096
0.339087018544936 6100
0.339800285306705 6106
0.340513552068474 6111
0.341226818830243 6114
0.341940085592011 6123
0.34265335235378 6125
0.343366619115549 6136
0.344079885877318 6146
0.344793152639087 6156
0.345506419400856 6162
0.346219686162625 6164
0.346932952924394 6170
0.347646219686163 6180
0.348359486447932 6188
0.3490727532097 6189
0.349786019971469 6198
0.350499286733238 6207
0.351212553495007 6212
0.351925820256776 6214
0.352639087018545 6219
0.353352353780314 6221
0.354065620542083 6226
0.354778887303852 6230
0.355492154065621 6237
0.356205420827389 6244
0.356918687589158 6248
0.357631954350927 6251
0.358345221112696 6254
0.359058487874465 6259
0.359771754636234 6271
0.360485021398003 6271
0.361198288159772 6276
0.361911554921541 6277
0.36262482168331 6283
0.363338088445078 6287
0.364051355206847 6289
0.364764621968616 6300
0.365477888730385 6304
0.366191155492154 6308
0.366904422253923 6310
0.367617689015692 6323
0.368330955777461 6330
0.36904422253923 6335
0.369757489300999 6348
0.370470756062767 6359
0.371184022824536 6363
0.371897289586305 6372
0.372610556348074 6377
0.373323823109843 6387
0.374037089871612 6391
0.374750356633381 6398
0.37546362339515 6401
0.376176890156919 6402
0.376890156918688 6406
0.377603423680457 6424
0.378316690442225 6431
0.379029957203994 6438
0.379743223965763 6444
0.380456490727532 6458
0.381169757489301 6462
0.38188302425107 6473
0.382596291012839 6477
0.383309557774608 6483
0.384022824536377 6489
0.384736091298146 6493
0.385449358059914 6500
0.386162624821683 6504
0.386875891583452 6512
0.387589158345221 6519
0.38830242510699 6520
0.389015691868759 6534
0.389728958630528 6544
0.390442225392297 6553
0.391155492154066 6560
0.391868758915835 6563
0.392582025677603 6566
0.393295292439372 6568
0.394008559201141 6574
0.39472182596291 6579
0.395435092724679 6589
0.396148359486448 6595
0.396861626248217 6597
0.397574893009986 6608
0.398288159771755 6610
0.399001426533524 6611
0.399714693295292 6624
0.400427960057061 6630
0.40114122681883 6647
0.401854493580599 6655
0.402567760342368 6670
0.403281027104137 6678
0.403994293865906 6684
0.404707560627675 6694
0.405420827389444 6700
0.406134094151213 6709
0.406847360912981 6724
0.40756062767475 6733
0.408273894436519 6739
0.408987161198288 6759
0.409700427960057 6763
0.410413694721826 6766
0.411126961483595 6777
0.411840228245364 6782
0.412553495007133 6785
0.413266761768902 6789
0.41398002853067 6795
0.414693295292439 6799
0.415406562054208 6802
0.416119828815977 6804
0.416833095577746 6818
0.417546362339515 6826
0.418259629101284 6832
0.418972895863053 6844
0.419686162624822 6861
0.420399429386591 6866
0.421112696148359 6868
0.421825962910128 6887
0.422539229671897 6901
0.423252496433666 6907
0.423965763195435 6919
0.424679029957204 6923
0.425392296718973 6933
0.426105563480742 6936
0.426818830242511 6944
0.42753209700428 6971
0.428245363766048 6977
0.428958630527817 6990
0.429671897289586 6993
0.430385164051355 6996
0.431098430813124 7010
0.431811697574893 7016
0.432524964336662 7021
0.433238231098431 7028
0.4339514978602 7043
0.434664764621969 7049
0.435378031383738 7066
0.436091298145506 7071
0.436804564907275 7080
0.437517831669044 7094
0.438231098430813 7103
0.438944365192582 7126
0.439657631954351 7127
0.44037089871612 7132
0.441084165477889 7140
0.441797432239658 7150
0.442510699001427 7157
0.443223965763195 7163
0.443937232524964 7168
0.444650499286733 7177
0.445363766048502 7186
0.446077032810271 7192
0.44679029957204 7197
0.447503566333809 7199
0.448216833095578 7209
0.448930099857347 7217
0.449643366619116 7226
0.450356633380884 7237
0.451069900142653 7246
0.451783166904422 7250
0.452496433666191 7266
0.45320970042796 7284
0.453922967189729 7292
0.454636233951498 7298
0.455349500713267 7319
0.456062767475036 7326
0.456776034236805 7339
0.457489300998573 7345
0.458202567760342 7361
0.458915834522111 7373
0.45962910128388 7378
0.460342368045649 7384
0.461055634807418 7385
0.461768901569187 7394
0.462482168330956 7398
0.463195435092725 7421
0.463908701854494 7428
0.464621968616262 7430
0.465335235378031 7445
0.4660485021398 7449
0.466761768901569 7458
0.467475035663338 7467
0.468188302425107 7477
0.468901569186876 7488
0.469614835948645 7497
0.470328102710414 7502
0.471041369472183 7507
0.471754636233951 7510
0.47246790299572 7524
0.473181169757489 7524
0.473894436519258 7533
0.474607703281027 7537
0.475320970042796 7542
0.476034236804565 7548
0.476747503566334 7549
0.477460770328103 7552
0.478174037089872 7558
0.478887303851641 7566
0.479600570613409 7575
0.480313837375178 7581
0.481027104136947 7585
0.481740370898716 7604
0.482453637660485 7612
0.483166904422254 7622
0.483880171184023 7626
0.484593437945792 7631
0.485306704707561 7637
0.48601997146933 7646
0.486733238231098 7657
0.487446504992867 7662
0.488159771754636 7667
0.488873038516405 7675
0.489586305278174 7681
0.490299572039943 7696
0.491012838801712 7707
0.491726105563481 7717
0.49243937232525 7723
0.493152639087019 7738
0.493865905848787 7744
0.494579172610556 7760
0.495292439372325 7783
0.496005706134094 7789
0.496718972895863 7805
0.497432239657632 7818
0.498145506419401 7828
0.49885877318117 7835
0.499572039942939 7847
0.500285306704708 7857
0.500998573466476 7864
0.501711840228245 7872
0.502425106990014 7894
0.503138373751783 7910
0.503851640513552 7929
0.504564907275321 7941
0.50527817403709 7955
0.505991440798859 7970
0.506704707560628 7979
0.507417974322397 7984
0.508131241084166 8005
0.508844507845934 8016
0.509557774607703 8026
0.510271041369472 8035
0.510984308131241 8047
0.51169757489301 8056
0.512410841654779 8085
0.513124108416548 8088
0.513837375178317 8107
0.514550641940086 8126
0.515263908701854 8186
0.515977175463623 8194
0.516690442225392 8204
0.517403708987161 8211
0.51811697574893 8222
0.518830242510699 8224
0.519543509272468 8236
0.520256776034237 8252
0.520970042796006 8275
0.521683309557775 8282
0.522396576319544 8296
0.523109843081312 8307
0.523823109843081 8317
0.52453637660485 8331
0.525249643366619 8357
0.525962910128388 8369
0.526676176890157 8378
0.527389443651926 8393
0.528102710413695 8403
0.528815977175464 8424
0.529529243937233 8432
0.530242510699001 8436
0.53095577746077 8456
0.531669044222539 8466
0.532382310984308 8474
0.533095577746077 8482
0.533808844507846 8488
0.534522111269615 8504
0.535235378031384 8521
0.535948644793153 8525
0.536661911554921 8541
0.53737517831669 8552
0.538088445078459 8570
0.538801711840228 8577
0.539514978601997 8580
0.540228245363766 8584
0.540941512125535 8601
0.541654778887304 8626
0.542368045649073 8643
0.543081312410842 8657
0.543794579172611 8690
0.54450784593438 8709
0.545221112696148 8713
0.545934379457917 8719
0.546647646219686 8729
0.547360912981455 8738
0.548074179743224 8747
0.548787446504993 8756
0.549500713266762 8765
0.550213980028531 8774
0.5509272467903 8795
0.551640513552068 8807
0.552353780313837 8820
0.553067047075606 8824
0.553780313837375 8861
0.554493580599144 8872
0.555206847360913 8886
0.555920114122682 8894
0.556633380884451 8899
0.55734664764622 8929
0.558059914407989 8947
0.558773181169757 8970
0.559486447931526 8987
0.560199714693295 8991
0.560912981455064 9001
0.561626248216833 9015
0.562339514978602 9026
0.563052781740371 9044
0.56376604850214 9054
0.564479315263909 9067
0.565192582025678 9079
0.565905848787447 9090
0.566619115549215 9106
0.567332382310984 9143
0.568045649072753 9162
0.568758915834522 9178
0.569472182596291 9210
0.57018544935806 9241
0.570898716119829 9253
0.571611982881598 9283
0.572325249643367 9287
0.573038516405135 9302
0.573751783166904 9322
0.574465049928673 9350
0.575178316690442 9373
0.575891583452211 9381
0.57660485021398 9397
0.577318116975749 9403
0.578031383737518 9431
0.578744650499287 9457
0.579457917261056 9470
0.580171184022825 9478
0.580884450784593 9530
0.581597717546362 9540
0.582310984308131 9937
0.5830242510699 10055
0.583737517831669 10095
0.584450784593438 10105
0.585164051355207 10131
0.585877318116976 10149
0.586590584878745 10178
0.587303851640514 10196
0.588017118402282 10202
0.588730385164051 10210
0.58944365192582 10230
0.590156918687589 10244
0.590870185449358 10271
0.591583452211127 10274
0.592296718972896 10282
0.593009985734665 10323
0.593723252496434 10337
0.594436519258203 10350
0.595149786019971 10379
0.59586305278174 10422
0.596576319543509 10428
0.597289586305278 10460
0.598002853067047 10468
0.598716119828816 10481
0.599429386590585 10496
0.600142653352354 10512
0.600855920114123 10546
0.601569186875892 10569
0.602282453637661 10588
0.602995720399429 10615
0.603708987161198 10642
0.604422253922967 10660
0.605135520684736 10679
0.605848787446505 10719
0.606562054208274 10742
0.607275320970043 10762
0.607988587731812 10809
0.608701854493581 10835
0.609415121255349 10865
0.610128388017118 10880
0.610841654778887 10895
0.611554921540656 10926
0.612268188302425 10932
0.612981455064194 10968
0.613694721825963 10977
0.614407988587732 11014
0.615121255349501 11035
0.61583452211127 11052
0.616547788873039 11072
0.617261055634807 11144
0.617974322396576 11160
0.618687589158345 11202
0.619400855920114 11223
0.620114122681883 11282
0.620827389443652 11283
0.621540656205421 11334
0.62225392296719 11352
0.622967189728959 11371
0.623680456490728 11394
0.624393723252496 11459
0.625106990014265 11479
0.625820256776034 11509
0.626533523537803 11531
0.627246790299572 11546
0.627960057061341 11554
0.62867332382311 11567
0.629386590584879 11614
0.630099857346648 11625
0.630813124108417 11640
0.631526390870185 11657
0.632239657631954 11666
0.632952924393723 11698
0.633666191155492 11707
0.634379457917261 11733
0.63509272467903 11752
0.635805991440799 11783
0.636519258202568 11834
0.637232524964337 11857
0.637945791726106 11885
0.638659058487875 11905
0.639372325249643 11932
0.640085592011412 11975
0.640798858773181 12004
0.64151212553495 12017
0.642225392296719 12044
0.642938659058488 12062
0.643651925820257 12087
0.644365192582026 12098
0.645078459343795 12138
0.645791726105563 12153
0.646504992867332 12235
0.647218259629101 12269
0.64793152639087 12356
0.648644793152639 12366
0.649358059914408 12386
0.650071326676177 12396
0.650784593437946 12425
0.651497860199715 12459
0.652211126961484 12473
0.652924393723253 12511
0.653637660485021 12526
0.65435092724679 12553
0.655064194008559 12597
0.655777460770328 12606
0.656490727532097 12626
0.657203994293866 12653
0.657917261055635 12699
0.658630527817404 12744
0.659343794579173 12786
0.660057061340942 12806
0.66077032810271 12824
0.661483594864479 12865
0.662196861626248 12877
0.662910128388017 12895
0.663623395149786 12943
0.664336661911555 12994
0.665049928673324 13015
0.665763195435093 13064
0.666476462196862 13091
0.66718972895863 13120
0.667902995720399 13132
0.668616262482168 13175
0.669329529243937 13298
0.670042796005706 13315
0.670756062767475 13336
0.671469329529244 13344
0.672182596291013 13371
0.672895863052782 13399
0.673609129814551 13442
0.67432239657632 13482
0.675035663338088 13514
0.675748930099857 13545
0.676462196861626 13593
0.677175463623395 13628
0.677888730385164 13657
0.678601997146933 13682
0.679315263908702 13712
0.680028530670471 13759
0.68074179743224 13796
0.681455064194009 13818
0.682168330955777 13875
0.682881597717546 13913
0.683594864479315 13961
0.684308131241084 13983
0.685021398002853 14015
0.685734664764622 14045
0.686447931526391 14075
0.68716119828816 14107
0.687874465049929 14152
0.688587731811698 14167
0.689300998573466 14232
0.690014265335235 14243
0.690727532097004 14276
0.691440798858773 14302
0.692154065620542 14350
0.692867332382311 14365
0.69358059914408 14399
0.694293865905849 14443
0.695007132667618 14457
0.695720399429387 14560
0.696433666191156 14619
0.697146932952924 14651
0.697860199714693 14675
0.698573466476462 14716
0.699286733238231 14760
0.7 14804
0.700713266761769 14841
0.701426533523538 14937
0.702139800285307 14961
0.702853067047076 15024
0.703566333808844 15038
0.704279600570613 15055
0.704992867332382 15088
0.705706134094151 15136
0.70641940085592 15153
0.707132667617689 15160
0.707845934379458 15218
0.708559201141227 15252
0.709272467902996 15291
0.709985734664765 15333
0.710699001426534 15408
0.711412268188302 15490
0.712125534950071 15528
0.71283880171184 15587
0.713552068473609 15623
0.714265335235378 15724
0.714978601997147 15773
0.715691868758916 15870
0.716405135520685 15907
0.717118402282454 15951
0.717831669044223 15977
0.718544935805991 16054
0.71925820256776 16105
0.719971469329529 16142
0.720684736091298 16209
0.721398002853067 16274
0.722111269614836 16408
0.722824536376605 16425
0.723537803138374 16460
0.724251069900143 16483
0.724964336661912 16506
0.72567760342368 16523
0.726390870185449 16585
0.727104136947218 16617
0.727817403708987 16669
0.728530670470756 16709
0.729243937232525 16748
0.729957203994294 16785
0.730670470756063 16877
0.731383737517832 16918
0.732097004279601 16964
0.732810271041369 17037
0.733523537803138 17108
0.734236804564907 17166
0.734950071326676 17230
0.735663338088445 17283
0.736376604850214 17321
0.737089871611983 17407
0.737803138373752 17469
0.738516405135521 17499
0.73922967189729 17548
0.739942938659058 17589
0.740656205420827 17631
0.741369472182596 17678
0.742082738944365 17734
0.742796005706134 17825
0.743509272467903 17866
0.744222539229672 17932
0.744935805991441 17973
0.74564907275321 18029
0.746362339514979 18107
0.747075606276748 18186
0.747788873038516 18254
0.748502139800285 18336
0.749215406562054 18387
0.749928673323823 18482
0.750641940085592 18649
0.751355206847361 18715
0.75206847360913 18757
0.752781740370899 18814
0.753495007132668 18910
0.754208273894437 19002
0.754921540656205 19058
0.755634807417974 19133
0.756348074179743 19214
0.757061340941512 19268
0.757774607703281 19327
0.75848787446505 19383
0.759201141226819 19467
0.759914407988588 19527
0.760627674750357 19593
0.761340941512126 19638
0.762054208273894 19741
0.762767475035663 19792
0.763480741797432 19819
0.764194008559201 19888
0.76490727532097 19935
0.765620542082739 20007
0.766333808844508 20090
0.767047075606277 20143
0.767760342368046 20244
0.768473609129815 20316
0.769186875891583 20417
0.769900142653352 20510
0.770613409415121 20569
0.77132667617689 20617
0.772039942938659 20650
0.772753209700428 20708
0.773466476462197 20790
0.774179743223966 20986
0.774893009985735 21067
0.775606276747504 21243
0.776319543509272 21565
0.777032810271041 21635
0.77774607703281 21741
0.778459343794579 21795
0.779172610556348 21896
0.779885877318117 21997
0.780599144079886 22087
0.781312410841655 22209
0.782025677603424 22359
0.782738944365193 22474
0.783452211126962 22595
0.78416547788873 22682
0.784878744650499 22796
0.785592011412268 22859
0.786305278174037 23084
0.787018544935806 23203
0.787731811697575 23304
0.788445078459344 23395
0.789158345221113 23497
0.789871611982882 23571
0.790584878744651 23694
0.791298145506419 23798
0.792011412268188 23860
0.792724679029957 23947
0.793437945791726 24032
0.794151212553495 24229
0.794864479315264 24319
0.795577746077033 24345
0.796291012838802 24410
0.797004279600571 24480
0.797717546362339 25143
0.798430813124108 25277
0.799144079885877 25407
0.799857346647646 25523
0.800570613409415 25606
0.801283880171184 25732
0.801997146932953 25987
0.802710413694722 26044
0.803423680456491 26175
0.80413694721826 26262
0.804850213980029 26313
0.805563480741797 26546
0.806276747503566 26665
0.806990014265335 26719
0.807703281027104 26816
0.808416547788873 26876
0.809129814550642 27034
0.809843081312411 27110
0.81055634807418 27212
0.811269614835949 27370
0.811982881597718 27415
0.812696148359486 27547
0.813409415121255 27704
0.814122681883024 28210
0.814835948644793 28332
0.815549215406562 28660
0.816262482168331 28754
0.8169757489301 28907
0.817689015691869 29134
0.818402282453638 29254
0.819115549215407 29331
0.819828815977175 29442
0.820542082738944 29578
0.821255349500713 29658
0.821968616262482 29829
0.822681883024251 29964
0.82339514978602 30223
0.824108416547789 30324
0.824821683309558 30470
0.825534950071327 30718
0.826248216833096 30936
0.826961483594864 31137
0.827674750356633 31347
0.828388017118402 31514
0.829101283880171 31663
0.82981455064194 31819
0.830527817403709 31907
0.831241084165478 32071
0.831954350927247 32213
0.832667617689016 32365
0.833380884450785 32551
0.834094151212553 32885
0.834807417974322 32972
0.835520684736091 33122
0.83623395149786 33500
0.836947218259629 33776
0.837660485021398 34392
0.838373751783167 34561
0.839087018544936 34666
0.839800285306705 34948
0.840513552068474 35013
0.841226818830243 35174
0.841940085592011 35302
0.84265335235378 35455
0.843366619115549 35650
0.844079885877318 35812
0.844793152639087 35910
0.845506419400856 36093
0.846219686162625 36276
0.846932952924394 36373
0.847646219686163 36510
0.848359486447932 36755
0.8490727532097 37031
0.849786019971469 37481
0.850499286733238 37693
0.851212553495007 37925
0.851925820256776 38179
0.852639087018545 38604
0.853352353780314 38670
0.854065620542083 38857
0.854778887303852 39300
0.855492154065621 39469
0.856205420827389 39850
0.856918687589158 40156
0.857631954350927 40273
0.858345221112696 40668
0.859058487874465 40848
0.859771754636234 41064
0.860485021398003 41384
0.861198288159772 41600
0.861911554921541 41743
0.86262482168331 42108
0.863338088445078 42910
0.864051355206847 43185
0.864764621968616 43288
0.865477888730385 43571
0.866191155492154 43769
0.866904422253923 43935
0.867617689015692 44153
0.868330955777461 44550
0.86904422253923 44653
0.869757489300999 45054
0.870470756062767 45524
0.871184022824536 45928
0.871897289586305 46171
0.872610556348074 46362
0.873323823109843 46622
0.874037089871612 46837
0.874750356633381 47224
0.87546362339515 47391
0.876176890156919 47707
0.876890156918688 47896
0.877603423680457 48090
0.878316690442225 48501
0.879029957203994 48671
0.879743223965763 48847
0.880456490727532 49227
0.881169757489301 49422
0.88188302425107 49742
0.882596291012839 49901
0.883309557774608 50064
0.884022824536377 50623
0.884736091298145 50894
0.885449358059914 51153
0.886162624821683 51354
0.886875891583452 51767
0.887589158345221 52187
0.88830242510699 52346
0.889015691868759 52667
0.889728958630528 52876
0.890442225392297 53211
0.891155492154066 53644
0.891868758915835 53872
0.892582025677603 54132
0.893295292439372 54533
0.894008559201141 54886
0.89472182596291 55073
0.895435092724679 55269
0.896148359486448 55788
0.896861626248217 56276
0.897574893009986 56732
0.898288159771755 57120
0.899001426533524 57391
0.899714693295292 58149
0.900427960057061 58822
0.90114122681883 59170
0.901854493580599 59447
0.902567760342368 59903
0.903281027104137 60157
0.903994293865906 60736
0.904707560627675 61291
0.905420827389444 61788
0.906134094151213 61968
0.906847360912981 62155
0.90756062767475 62684
0.908273894436519 63027
0.908987161198288 64227
0.909700427960057 65394
0.910413694721826 66472
0.911126961483595 67181
0.911840228245364 67735
0.912553495007133 68133
0.913266761768902 68518
0.913980028530671 69395
0.914693295292439 69987
0.915406562054208 70274
0.916119828815977 71844
0.916833095577746 72052
0.917546362339515 73316
0.918259629101284 74939
0.918972895863053 75804
0.919686162624822 76716
0.920399429386591 77354
0.921112696148359 78746
0.921825962910128 79455
0.922539229671897 81472
0.923252496433666 82456
0.923965763195435 83697
0.924679029957204 84278
0.925392296718973 84922
0.926105563480742 85936
0.926818830242511 86867
0.92753209700428 87761
0.928245363766048 89526
0.928958630527817 90275
0.929671897289586 91125
0.930385164051355 92610
0.931098430813124 95020
0.931811697574893 96532
0.932524964336662 97564
0.933238231098431 98282
0.9339514978602 100484
0.934664764621969 101816
0.935378031383738 102775
0.936091298145506 104136
0.936804564907275 105583
0.937517831669044 106911
0.938231098430813 108136
0.938944365192582 109957
0.939657631954351 110909
0.94037089871612 113480
0.941084165477889 115535
0.941797432239658 117037
0.942510699001427 119420
0.943223965763195 120248
0.943937232524964 121445
0.944650499286733 122438
0.945363766048502 124594
0.946077032810271 125576
0.94679029957204 126412
0.947503566333809 128468
0.948216833095578 130063
0.948930099857347 131903
0.949643366619116 133664
0.950356633380884 136968
0.951069900142653 138665
0.951783166904422 139283
0.952496433666191 141453
0.95320970042796 144568
0.953922967189729 146640
0.954636233951498 152128
0.955349500713267 154256
0.956062767475036 157351
0.956776034236805 160794
0.957489300998573 167399
0.958202567760342 169456
0.958915834522111 173319
0.95962910128388 174967
0.960342368045649 179540
0.961055634807418 180632
0.961768901569187 186017
0.962482168330956 188845
0.963195435092725 192598
0.963908701854494 198024
0.964621968616262 202659
0.965335235378031 209789
0.9660485021398 217945
0.966761768901569 221411
0.967475035663338 225258
0.968188302425107 236348
0.968901569186876 241984
0.969614835948645 246546
0.970328102710414 251929
0.971041369472183 255843
0.971754636233952 264369
0.97246790299572 270184
0.973181169757489 278497
0.973894436519258 289223
0.974607703281027 296072
0.975320970042796 307144
0.976034236804565 314842
0.976747503566334 331545
0.977460770328103 338601
0.978174037089872 345208
0.97888730385164 367583
0.979600570613409 373188
0.980313837375178 385296
0.981027104136947 410282
0.981740370898716 428382
0.982453637660485 440523
0.983166904422254 458527
0.983880171184023 462925
0.984593437945792 494103
0.985306704707561 528301
0.98601997146933 554177
0.986733238231098 602204
0.987446504992867 683198
0.988159771754636 751029
0.988873038516405 782327
0.989586305278174 859604
0.990299572039943 980235
0.991012838801712 1047788
0.991726105563481 1162396
0.99243937232525 1269101
0.993152639087019 1411335
0.993865905848787 1551937
0.994579172610556 1799980
0.995292439372325 2014835
0.996005706134094 2290150
0.996718972895863 2629875
};
\addlegendentry{resnext101-32x16d-wsl}
\addplot [semithick, color4]
table {0.000998573466476462 3281
0.00171184022824536 3479
0.00242510699001427 3511
0.00313837375178317 3524
0.00385164051355207 3529
0.00456490727532097 3535
0.00527817403708987 3552
0.00599144079885877 3571
0.00670470756062767 3576
0.00741797432239658 3596
0.00813124108416548 3612
0.00884450784593438 3613
0.00955777460770328 3627
0.0102710413694722 3639
0.0109843081312411 3639
0.01169757489301 3650
0.0124108416547789 3656
0.0131241084165478 3672
0.0138373751783167 3683
0.0145506419400856 3685
0.0152639087018545 3696
0.0159771754636234 3699
0.0166904422253923 3700
0.0174037089871612 3706
0.0181169757489301 3706
0.018830242510699 3711
0.0195435092724679 3717
0.0202567760342368 3723
0.0209700427960057 3726
0.0216833095577746 3736
0.0223965763195435 3738
0.0231098430813124 3739
0.0238231098430813 3739
0.0245363766048502 3739
0.0252496433666191 3741
0.025962910128388 3741
0.0266761768901569 3742
0.0273894436519258 3742
0.0281027104136947 3742
0.0288159771754636 3749
0.0295292439372325 3750
0.0302425106990014 3755
0.0309557774607703 3755
0.0316690442225392 3755
0.0323823109843081 3756
0.033095577746077 3758
0.0338088445078459 3758
0.0345221112696148 3763
0.0352353780313837 3763
0.0359486447931526 3763
0.0366619115549215 3763
0.0373751783166904 3764
0.0380884450784593 3764
0.0388017118402282 3765
0.0395149786019971 3770
0.040228245363766 3772
0.0409415121255349 3772
0.0416547788873039 3772
0.0423680456490728 3778
0.0430813124108417 3778
0.0437945791726106 3778
0.0445078459343795 3779
0.0452211126961484 3780
0.0459343794579173 3780
0.0466476462196862 3785
0.0473609129814551 3801
0.048074179743224 3807
0.0487874465049929 3813
0.0495007132667618 3819
0.0502139800285307 3824
0.0509272467902996 3826
0.0516405135520685 3827
0.0523537803138374 3827
0.0530670470756063 3827
0.0537803138373752 3827
0.0544935805991441 3827
0.055206847360913 3828
0.0559201141226819 3828
0.0566333808844508 3840
0.0573466476462197 3840
0.0580599144079886 3840
0.0587731811697575 3845
0.0594864479315264 3845
0.0601997146932953 3855
0.0609129814550642 3860
0.0616262482168331 3865
0.062339514978602 3865
0.0630527817403709 3871
0.0637660485021398 3876
0.0644793152639087 3876
0.0651925820256776 3876
0.0659058487874465 3877
0.0666191155492154 3877
0.0673323823109843 3877
0.0680456490727532 3878
0.0687589158345221 3879
0.069472182596291 3879
0.0701854493580599 3879
0.0708987161198288 3879
0.0716119828815977 3880
0.0723252496433666 3885
0.0730385164051355 3890
0.0737517831669044 3890
0.0744650499286733 3890
0.0751783166904422 3895
0.0758915834522111 3895
0.07660485021398 3896
0.0773181169757489 3901
0.0780313837375178 3901
0.0787446504992867 3902
0.0794579172610556 3907
0.0801711840228245 3907
0.0808844507845934 3908
0.0815977175463623 3909
0.0823109843081312 3909
0.0830242510699001 3910
0.0837375178316691 3922
0.084450784593438 3923
0.0851640513552068 3929
0.0858773181169757 3930
0.0865905848787446 3932
0.0873038516405135 3937
0.0880171184022824 3937
0.0887303851640514 3938
0.0894436519258203 3939
0.0901569186875892 3951
0.0908701854493581 3952
0.091583452211127 3952
0.0922967189728959 3953
0.0930099857346648 3953
0.0937232524964337 3953
0.0944365192582026 3956
0.0951497860199715 3959
0.0958630527817404 3966
0.0965763195435093 3972
0.0972895863052782 3973
0.0980028530670471 3974
0.098716119828816 3977
0.0994293865905849 3982
0.100142653352354 3982
0.100855920114123 3982
0.101569186875892 3982
0.10228245363766 3982
0.102995720399429 3982
0.103708987161198 3988
0.104422253922967 3988
0.105135520684736 3992
0.105848787446505 3992
0.106562054208274 3992
0.107275320970043 3993
0.107988587731812 3998
0.108701854493581 3999
0.109415121255349 3999
0.110128388017118 4000
0.110841654778887 4000
0.111554921540656 4005
0.112268188302425 4006
0.112981455064194 4006
0.113694721825963 4008
0.114407988587732 4008
0.115121255349501 4010
0.11583452211127 4015
0.116547788873039 4015
0.117261055634807 4018
0.117974322396576 4018
0.118687589158345 4018
0.119400855920114 4018
0.120114122681883 4018
0.120827389443652 4018
0.121540656205421 4019
0.12225392296719 4019
0.122967189728959 4019
0.123680456490728 4019
0.124393723252496 4019
0.125106990014265 4020
0.125820256776034 4021
0.126533523537803 4022
0.127246790299572 4022
0.127960057061341 4022
0.12867332382311 4037
0.129386590584879 4042
0.130099857346648 4042
0.130813124108417 4042
0.131526390870185 4042
0.132239657631954 4042
0.132952924393723 4044
0.133666191155492 4044
0.134379457917261 4045
0.13509272467903 4046
0.135805991440799 4051
0.136519258202568 4052
0.137232524964337 4053
0.137945791726106 4054
0.138659058487874 4056
0.139372325249643 4057
0.140085592011412 4057
0.140798858773181 4058
0.14151212553495 4059
0.142225392296719 4060
0.142938659058488 4060
0.143651925820257 4060
0.144365192582026 4060
0.145078459343795 4060
0.145791726105563 4061
0.146504992867332 4061
0.147218259629101 4062
0.14793152639087 4062
0.148644793152639 4067
0.149358059914408 4067
0.150071326676177 4067
0.150784593437946 4069
0.151497860199715 4069
0.152211126961484 4073
0.152924393723253 4073
0.153637660485021 4077
0.15435092724679 4080
0.155064194008559 4089
0.155777460770328 4090
0.156490727532097 4092
0.157203994293866 4094
0.157917261055635 4094
0.158630527817404 4094
0.159343794579173 4098
0.160057061340942 4099
0.16077032810271 4100
0.161483594864479 4105
0.162196861626248 4106
0.162910128388017 4108
0.163623395149786 4108
0.164336661911555 4109
0.165049928673324 4110
0.165763195435093 4110
0.166476462196862 4120
0.167189728958631 4120
0.167902995720399 4122
0.168616262482168 4122
0.169329529243937 4128
0.170042796005706 4130
0.170756062767475 4137
0.171469329529244 4138
0.172182596291013 4143
0.172895863052782 4144
0.173609129814551 4145
0.17432239657632 4146
0.175035663338088 4147
0.175748930099857 4148
0.176462196861626 4150
0.177175463623395 4156
0.177888730385164 4157
0.178601997146933 4158
0.179315263908702 4183
0.180028530670471 4189
0.18074179743224 4191
0.181455064194009 4201
0.182168330955777 4202
0.182881597717546 4209
0.183594864479315 4209
0.184308131241084 4209
0.185021398002853 4209
0.185734664764622 4209
0.186447931526391 4211
0.18716119828816 4212
0.187874465049929 4219
0.188587731811698 4221
0.189300998573466 4224
0.190014265335235 4226
0.190727532097004 4227
0.191440798858773 4228
0.192154065620542 4568
0.192867332382311 4643
0.19358059914408 4650
0.194293865905849 4653
0.195007132667618 4654
0.195720399429387 4655
0.196433666191155 4655
0.197146932952924 4655
0.197860199714693 4655
0.198573466476462 4661
0.199286733238231 4667
0.2 4667
0.200713266761769 4669
0.201426533523538 4670
0.202139800285307 4673
0.202853067047076 4674
0.203566333808845 4674
0.204279600570613 4677
0.204992867332382 4677
0.205706134094151 4679
0.20641940085592 4679
0.207132667617689 4680
0.207845934379458 4682
0.208559201141227 4682
0.209272467902996 4684
0.209985734664765 4684
0.210699001426534 4684
0.211412268188302 4686
0.212125534950071 4688
0.21283880171184 4690
0.213552068473609 4697
0.214265335235378 4702
0.214978601997147 4703
0.215691868758916 4705
0.216405135520685 4707
0.217118402282454 4708
0.217831669044223 4710
0.218544935805991 4710
0.21925820256776 4711
0.219971469329529 4711
0.220684736091298 4718
0.221398002853067 4718
0.222111269614836 4720
0.222824536376605 4721
0.223537803138374 4721
0.224251069900143 4722
0.224964336661912 4722
0.22567760342368 4723
0.226390870185449 4723
0.227104136947218 4723
0.227817403708987 4723
0.228530670470756 4723
0.229243937232525 4723
0.229957203994294 4723
0.230670470756063 4723
0.231383737517832 4724
0.232097004279601 4724
0.232810271041369 4725
0.233523537803138 4726
0.234236804564907 4728
0.234950071326676 4735
0.235663338088445 4735
0.236376604850214 4735
0.237089871611983 4737
0.237803138373752 4738
0.238516405135521 4745
0.23922967189729 4752
0.239942938659058 4752
0.240656205420827 4753
0.241369472182596 4753
0.242082738944365 4753
0.242796005706134 4756
0.243509272467903 4757
0.244222539229672 4762
0.244935805991441 4762
0.24564907275321 4763
0.246362339514979 4763
0.247075606276748 4765
0.247788873038516 4765
0.248502139800285 4771
0.249215406562054 4773
0.249928673323823 4773
0.250641940085592 4773
0.251355206847361 4774
0.25206847360913 4775
0.252781740370899 4776
0.253495007132668 4778
0.254208273894437 4779
0.254921540656205 4779
0.255634807417974 4781
0.256348074179743 4781
0.257061340941512 4782
0.257774607703281 4782
0.25848787446505 4782
0.259201141226819 4787
0.259914407988588 4787
0.260627674750357 4798
0.261340941512126 4802
0.262054208273894 4803
0.262767475035663 4803
0.263480741797432 4803
0.264194008559201 4815
0.26490727532097 4815
0.265620542082739 4815
0.266333808844508 4816
0.267047075606277 4818
0.267760342368046 4823
0.268473609129815 4847
0.269186875891583 4874
0.269900142653352 4875
0.270613409415121 4875
0.27132667617689 4876
0.272039942938659 4878
0.272753209700428 4878
0.273466476462197 4884
0.274179743223966 4884
0.274893009985735 4884
0.275606276747504 4886
0.276319543509272 4886
0.277032810271041 4887
0.27774607703281 4888
0.278459343794579 4888
0.279172610556348 4889
0.279885877318117 4892
0.280599144079886 4893
0.281312410841655 4893
0.282025677603424 4899
0.282738944365193 4906
0.283452211126962 4911
0.28416547788873 4911
0.284878744650499 4912
0.285592011412268 4912
0.286305278174037 4912
0.287018544935806 4914
0.287731811697575 4914
0.288445078459344 4914
0.289158345221113 4914
0.289871611982882 4915
0.29058487874465 4916
0.291298145506419 4917
0.292011412268188 4918
0.292724679029957 4918
0.293437945791726 4920
0.294151212553495 4920
0.294864479315264 4920
0.295577746077033 4920
0.296291012838802 4921
0.297004279600571 4921
0.297717546362339 4921
0.298430813124108 4926
0.299144079885877 4932
0.299857346647646 4932
0.300570613409415 4943
0.301283880171184 4944
0.301997146932953 4945
0.302710413694722 4945
0.303423680456491 4946
0.30413694721826 4946
0.304850213980029 4947
0.305563480741797 4948
0.306276747503566 4948
0.306990014265335 4948
0.307703281027104 4948
0.308416547788873 4954
0.309129814550642 4954
0.309843081312411 4954
0.31055634807418 4956
0.311269614835949 4957
0.311982881597718 4959
0.312696148359486 4962
0.313409415121255 4963
0.314122681883024 4964
0.314835948644793 4966
0.315549215406562 4967
0.316262482168331 4968
0.3169757489301 4969
0.317689015691869 4970
0.318402282453638 4975
0.319115549215407 4976
0.319828815977175 4977
0.320542082738944 4977
0.321255349500713 4977
0.321968616262482 4977
0.322681883024251 4978
0.32339514978602 4980
0.324108416547789 4982
0.324821683309558 4983
0.325534950071327 4984
0.326248216833096 4984
0.326961483594864 4986
0.327674750356633 4989
0.328388017118402 4999
0.329101283880171 4999
0.32981455064194 5002
0.330527817403709 5003
0.331241084165478 5004
0.331954350927247 5006
0.332667617689016 5006
0.333380884450785 5008
0.334094151212553 5010
0.334807417974322 5016
0.335520684736091 5022
0.33623395149786 5028
0.336947218259629 5028
0.337660485021398 5030
0.338373751783167 5036
0.339087018544936 5036
0.339800285306705 5037
0.340513552068474 5040
0.341226818830243 5043
0.341940085592011 5043
0.34265335235378 5048
0.343366619115549 5049
0.344079885877318 5050
0.344793152639087 5061
0.345506419400856 5062
0.346219686162625 5063
0.346932952924394 5063
0.347646219686163 5064
0.348359486447932 5066
0.3490727532097 5067
0.349786019971469 5067
0.350499286733238 5068
0.351212553495007 5068
0.351925820256776 5068
0.352639087018545 5068
0.353352353780314 5069
0.354065620542083 5071
0.354778887303852 5072
0.355492154065621 5072
0.356205420827389 5073
0.356918687589158 5076
0.357631954350927 5076
0.358345221112696 5076
0.359058487874465 5077
0.359771754636234 5077
0.360485021398003 5079
0.361198288159772 5080
0.361911554921541 5086
0.36262482168331 5092
0.363338088445078 5093
0.364051355206847 5094
0.364764621968616 5095
0.365477888730385 5097
0.366191155492154 5104
0.366904422253923 5106
0.367617689015692 5107
0.368330955777461 5109
0.36904422253923 5109
0.369757489300999 5110
0.370470756062767 5113
0.371184022824536 5124
0.371897289586305 5125
0.372610556348074 5125
0.373323823109843 5126
0.374037089871612 5126
0.374750356633381 5126
0.37546362339515 5131
0.376176890156919 5132
0.376890156918688 5133
0.377603423680457 5135
0.378316690442225 5137
0.379029957203994 5137
0.379743223965763 5143
0.380456490727532 5144
0.381169757489301 5146
0.38188302425107 5147
0.382596291012839 5147
0.383309557774608 5147
0.384022824536377 5148
0.384736091298146 5148
0.385449358059914 5148
0.386162624821683 5149
0.386875891583452 5149
0.387589158345221 5150
0.38830242510699 5152
0.389015691868759 5152
0.389728958630528 5152
0.390442225392297 5158
0.391155492154066 5161
0.391868758915835 5161
0.392582025677603 5162
0.393295292439372 5170
0.394008559201141 5174
0.39472182596291 5174
0.395435092724679 5175
0.396148359486448 5178
0.396861626248217 5178
0.397574893009986 5184
0.398288159771755 5184
0.399001426533524 5185
0.399714693295292 5185
0.400427960057061 5185
0.40114122681883 5186
0.401854493580599 5192
0.402567760342368 5192
0.403281027104137 5194
0.403994293865906 5194
0.404707560627675 5194
0.405420827389444 5194
0.406134094151213 5195
0.406847360912981 5205
0.40756062767475 5206
0.408273894436519 5206
0.408987161198288 5206
0.409700427960057 5206
0.410413694721826 5208
0.411126961483595 5209
0.411840228245364 5213
0.412553495007133 5217
0.413266761768902 5218
0.41398002853067 5220
0.414693295292439 5227
0.415406562054208 5233
0.416119828815977 5234
0.416833095577746 5235
0.417546362339515 5235
0.418259629101284 5236
0.418972895863053 5238
0.419686162624822 5238
0.420399429386591 5245
0.421112696148359 5246
0.421825962910128 5251
0.422539229671897 5257
0.423252496433666 5258
0.423965763195435 5259
0.424679029957204 5265
0.425392296718973 5267
0.426105563480742 5269
0.426818830242511 5270
0.42753209700428 5271
0.428245363766048 5280
0.428958630527817 5281
0.429671897289586 5281
0.430385164051355 5284
0.431098430813124 5289
0.431811697574893 5290
0.432524964336662 5290
0.433238231098431 5292
0.4339514978602 5292
0.434664764621969 5294
0.435378031383738 5295
0.436091298145506 5299
0.436804564907275 5302
0.437517831669044 5302
0.438231098430813 5302
0.438944365192582 5308
0.439657631954351 5309
0.44037089871612 5310
0.441084165477889 5313
0.441797432239658 5315
0.442510699001427 5318
0.443223965763195 5319
0.443937232524964 5324
0.444650499286733 5335
0.445363766048502 5336
0.446077032810271 5338
0.44679029957204 5342
0.447503566333809 5342
0.448216833095578 5343
0.448930099857347 5346
0.449643366619116 5348
0.450356633380884 5350
0.451069900142653 5361
0.451783166904422 5368
0.452496433666191 5373
0.45320970042796 5374
0.453922967189729 5374
0.454636233951498 5375
0.455349500713267 5380
0.456062767475036 5385
0.456776034236805 5388
0.457489300998573 5390
0.458202567760342 5393
0.458915834522111 5393
0.45962910128388 5395
0.460342368045649 5398
0.461055634807418 5399
0.461768901569187 5400
0.462482168330956 5407
0.463195435092725 5415
0.463908701854494 5417
0.464621968616262 5417
0.465335235378031 5418
0.4660485021398 5424
0.466761768901569 5430
0.467475035663338 5435
0.468188302425107 5437
0.468901569186876 5437
0.469614835948645 5440
0.470328102710414 5446
0.471041369472183 5446
0.471754636233951 5446
0.47246790299572 5446
0.473181169757489 5447
0.473894436519258 5449
0.474607703281027 5452
0.475320970042796 5454
0.476034236804565 5454
0.476747503566334 5457
0.477460770328103 5458
0.478174037089872 5462
0.478887303851641 5465
0.479600570613409 5465
0.480313837375178 5466
0.481027104136947 5476
0.481740370898716 5482
0.482453637660485 5485
0.483166904422254 5489
0.483880171184023 5490
0.484593437945792 5505
0.485306704707561 5509
0.48601997146933 5519
0.486733238231098 5529
0.487446504992867 5534
0.488159771754636 5549
0.488873038516405 5555
0.489586305278174 5558
0.490299572039943 5560
0.491012838801712 5561
0.491726105563481 5563
0.49243937232525 5566
0.493152639087019 5576
0.493865905848787 5578
0.494579172610556 5581
0.495292439372325 5582
0.496005706134094 5585
0.496718972895863 5589
0.497432239657632 5592
0.498145506419401 5598
0.49885877318117 5600
0.499572039942939 5602
0.500285306704708 5604
0.500998573466476 5605
0.501711840228245 5614
0.502425106990014 5615
0.503138373751783 5616
0.503851640513552 5623
0.504564907275321 5623
0.50527817403709 5627
0.505991440798859 5629
0.506704707560628 5634
0.507417974322397 5634
0.508131241084166 5637
0.508844507845934 5638
0.509557774607703 5650
0.510271041369472 5654
0.510984308131241 5654
0.51169757489301 5660
0.512410841654779 5664
0.513124108416548 5667
0.513837375178317 5674
0.514550641940086 5676
0.515263908701854 5683
0.515977175463623 5687
0.516690442225392 5694
0.517403708987161 5695
0.51811697574893 5698
0.518830242510699 5701
0.519543509272468 5704
0.520256776034237 5706
0.520970042796006 5708
0.521683309557775 5712
0.522396576319544 5716
0.523109843081312 5720
0.523823109843081 5726
0.52453637660485 5730
0.525249643366619 5741
0.525962910128388 5748
0.526676176890157 5754
0.527389443651926 5754
0.528102710413695 5755
0.528815977175464 5755
0.529529243937233 5764
0.530242510699001 5767
0.53095577746077 5777
0.531669044222539 5779
0.532382310984308 5779
0.533095577746077 5785
0.533808844507846 5788
0.534522111269615 5791
0.535235378031384 5795
0.535948644793153 5799
0.536661911554921 5806
0.53737517831669 5809
0.538088445078459 5813
0.538801711840228 5814
0.539514978601997 5816
0.540228245363766 5817
0.540941512125535 5818
0.541654778887304 5818
0.542368045649073 5819
0.543081312410842 5819
0.543794579172611 5821
0.54450784593438 5828
0.545221112696148 5831
0.545934379457917 5834
0.546647646219686 5841
0.547360912981455 5844
0.548074179743224 5846
0.548787446504993 5853
0.549500713266762 5856
0.550213980028531 5861
0.5509272467903 5863
0.551640513552068 5868
0.552353780313837 5871
0.553067047075606 5871
0.553780313837375 5874
0.554493580599144 5879
0.555206847360913 5881
0.555920114122682 5883
0.556633380884451 5884
0.55734664764622 5893
0.558059914407989 5902
0.558773181169757 5903
0.559486447931526 5915
0.560199714693295 5917
0.560912981455064 5929
0.561626248216833 5939
0.562339514978602 5941
0.563052781740371 5948
0.56376604850214 5954
0.564479315263909 5963
0.565192582025678 5964
0.565905848787447 5967
0.566619115549215 5967
0.567332382310984 5968
0.568045649072753 5968
0.568758915834522 5971
0.569472182596291 5978
0.57018544935806 5991
0.570898716119829 5999
0.571611982881598 6000
0.572325249643367 6004
0.573038516405135 6005
0.573751783166904 6013
0.574465049928673 6017
0.575178316690442 6025
0.575891583452211 6031
0.57660485021398 6036
0.577318116975749 6037
0.578031383737518 6040
0.578744650499287 6040
0.579457917261056 6047
0.580171184022825 6057
0.580884450784593 6061
0.581597717546362 6065
0.582310984308131 6067
0.5830242510699 6076
0.583737517831669 6079
0.584450784593438 6080
0.585164051355207 6085
0.585877318116976 6087
0.586590584878745 6091
0.587303851640514 6094
0.588017118402282 6097
0.588730385164051 6100
0.58944365192582 6102
0.590156918687589 6108
0.590870185449358 6114
0.591583452211127 6116
0.592296718972896 6118
0.593009985734665 6119
0.593723252496434 6133
0.594436519258203 6138
0.595149786019971 6145
0.59586305278174 6153
0.596576319543509 6156
0.597289586305278 6157
0.598002853067047 6159
0.598716119828816 6165
0.599429386590585 6168
0.600142653352354 6169
0.600855920114123 6175
0.601569186875892 6185
0.602282453637661 6190
0.602995720399429 6191
0.603708987161198 6198
0.604422253922967 6199
0.605135520684736 6202
0.605848787446505 6203
0.606562054208274 6205
0.607275320970043 6215
0.607988587731812 6218
0.608701854493581 6224
0.609415121255349 6229
0.610128388017118 6232
0.610841654778887 6240
0.611554921540656 6246
0.612268188302425 6248
0.612981455064194 6255
0.613694721825963 6259
0.614407988587732 6273
0.615121255349501 6278
0.61583452211127 6280
0.616547788873039 6290
0.617261055634807 6292
0.617974322396576 6308
0.618687589158345 6319
0.619400855920114 6320
0.620114122681883 6324
0.620827389443652 6329
0.621540656205421 6340
0.62225392296719 6343
0.622967189728959 6350
0.623680456490728 6357
0.624393723252496 6358
0.625106990014265 6362
0.625820256776034 6368
0.626533523537803 6374
0.627246790299572 6377
0.627960057061341 6388
0.62867332382311 6390
0.629386590584879 6391
0.630099857346648 6406
0.630813124108417 6413
0.631526390870185 6420
0.632239657631954 6421
0.632952924393723 6426
0.633666191155492 6430
0.634379457917261 6432
0.63509272467903 6437
0.635805991440799 6439
0.636519258202568 6445
0.637232524964337 6450
0.637945791726106 6450
0.638659058487875 6463
0.639372325249643 6468
0.640085592011412 6473
0.640798858773181 6484
0.64151212553495 6490
0.642225392296719 6494
0.642938659058488 6496
0.643651925820257 6498
0.644365192582026 6499
0.645078459343795 6509
0.645791726105563 6515
0.646504992867332 6523
0.647218259629101 6528
0.64793152639087 6528
0.648644793152639 6534
0.649358059914408 6540
0.650071326676177 6547
0.650784593437946 6561
0.651497860199715 6562
0.652211126961484 6567
0.652924393723253 6573
0.653637660485021 6584
0.65435092724679 6599
0.655064194008559 6607
0.655777460770328 6610
0.656490727532097 6628
0.657203994293866 6631
0.657917261055635 6637
0.658630527817404 6639
0.659343794579173 6641
0.660057061340942 6649
0.66077032810271 6656
0.661483594864479 6668
0.662196861626248 6674
0.662910128388017 6675
0.663623395149786 6677
0.664336661911555 6685
0.665049928673324 6686
0.665763195435093 6695
0.666476462196862 6704
0.66718972895863 6709
0.667902995720399 6722
0.668616262482168 6724
0.669329529243937 6729
0.670042796005706 6740
0.670756062767475 6749
0.671469329529244 6757
0.672182596291013 6769
0.672895863052782 6771
0.673609129814551 6784
0.67432239657632 6797
0.675035663338088 6803
0.675748930099857 6811
0.676462196861626 6827
0.677175463623395 6829
0.677888730385164 6834
0.678601997146933 6836
0.679315263908702 6840
0.680028530670471 6850
0.68074179743224 6856
0.681455064194009 6857
0.682168330955777 6868
0.682881597717546 6871
0.683594864479315 6877
0.684308131241084 6879
0.685021398002853 6888
0.685734664764622 6893
0.686447931526391 6901
0.68716119828816 6903
0.687874465049929 6907
0.688587731811698 6915
0.689300998573466 6927
0.690014265335235 6944
0.690727532097004 6947
0.691440798858773 6957
0.692154065620542 6965
0.692867332382311 6980
0.69358059914408 6989
0.694293865905849 6995
0.695007132667618 7001
0.695720399429387 7016
0.696433666191156 7039
0.697146932952924 7048
0.697860199714693 7054
0.698573466476462 7060
0.699286733238231 7068
0.7 7075
0.700713266761769 7090
0.701426533523538 7098
0.702139800285307 7105
0.702853067047076 7116
0.703566333808844 7120
0.704279600570613 7130
0.704992867332382 7136
0.705706134094151 7142
0.70641940085592 7146
0.707132667617689 7149
0.707845934379458 7154
0.708559201141227 7162
0.709272467902996 7166
0.709985734664765 7183
0.710699001426534 7208
0.711412268188302 7212
0.712125534950071 7222
0.71283880171184 7227
0.713552068473609 7240
0.714265335235378 7261
0.714978601997147 7269
0.715691868758916 7281
0.716405135520685 7288
0.717118402282454 7297
0.717831669044223 7307
0.718544935805991 7311
0.71925820256776 7317
0.719971469329529 7327
0.720684736091298 7330
0.721398002853067 7337
0.722111269614836 7373
0.722824536376605 7382
0.723537803138374 7401
0.724251069900143 7413
0.724964336661912 7431
0.72567760342368 7447
0.726390870185449 7452
0.727104136947218 7460
0.727817403708987 7477
0.728530670470756 7488
0.729243937232525 7504
0.729957203994294 7513
0.730670470756063 7522
0.731383737517832 7536
0.732097004279601 7544
0.732810271041369 7551
0.733523537803138 7554
0.734236804564907 7556
0.734950071326676 7558
0.735663338088445 7586
0.736376604850214 7599
0.737089871611983 7612
0.737803138373752 7623
0.738516405135521 7633
0.73922967189729 7637
0.739942938659058 7664
0.740656205420827 7669
0.741369472182596 7679
0.742082738944365 7691
0.742796005706134 7725
0.743509272467903 7737
0.744222539229672 7752
0.744935805991441 7774
0.74564907275321 7792
0.746362339514979 7801
0.747075606276748 7810
0.747788873038516 7824
0.748502139800285 7831
0.749215406562054 7845
0.749928673323823 7857
0.750641940085592 7861
0.751355206847361 7864
0.75206847360913 7880
0.752781740370899 7891
0.753495007132668 7898
0.754208273894437 7917
0.754921540656205 7939
0.755634807417974 7946
0.756348074179743 7969
0.757061340941512 7980
0.757774607703281 7994
0.75848787446505 8000
0.759201141226819 8017
0.759914407988588 8044
0.760627674750357 8057
0.761340941512126 8065
0.762054208273894 8072
0.762767475035663 8084
0.763480741797432 8100
0.764194008559201 8109
0.76490727532097 8121
0.765620542082739 8157
0.766333808844508 8179
0.767047075606277 8203
0.767760342368046 8214
0.768473609129815 8229
0.769186875891583 8262
0.769900142653352 8287
0.770613409415121 8298
0.77132667617689 8314
0.772039942938659 8322
0.772753209700428 8346
0.773466476462197 8370
0.774179743223966 8379
0.774893009985735 8388
0.775606276747504 8401
0.776319543509272 8420
0.777032810271041 8438
0.77774607703281 8457
0.778459343794579 8474
0.779172610556348 8492
0.779885877318117 8507
0.780599144079886 8524
0.781312410841655 8547
0.782025677603424 8575
0.782738944365193 8585
0.783452211126962 8598
0.78416547788873 8611
0.784878744650499 8657
0.785592011412268 8690
0.786305278174037 8695
0.787018544935806 8702
0.787731811697575 8764
0.788445078459344 8784
0.789158345221113 8797
0.789871611982882 8805
0.790584878744651 8812
0.791298145506419 8828
0.792011412268188 8851
0.792724679029957 8870
0.793437945791726 8881
0.794151212553495 8886
0.794864479315264 8906
0.795577746077033 8925
0.796291012838802 8949
0.797004279600571 8967
0.797717546362339 8975
0.798430813124108 9036
0.799144079885877 9042
0.799857346647646 9057
0.800570613409415 9085
0.801283880171184 9100
0.801997146932953 9128
0.802710413694722 9162
0.803423680456491 9183
0.80413694721826 9198
0.804850213980029 9237
0.805563480741797 9252
0.806276747503566 9261
0.806990014265335 9274
0.807703281027104 9296
0.808416547788873 9349
0.809129814550642 9373
0.809843081312411 9422
0.81055634807418 9442
0.811269614835949 9460
0.811982881597718 9492
0.812696148359486 9502
0.813409415121255 9528
0.814122681883024 9563
0.814835948644793 9592
0.815549215406562 9612
0.816262482168331 9625
0.8169757489301 9638
0.817689015691869 9647
0.818402282453638 9653
0.819115549215407 9676
0.819828815977175 9712
0.820542082738944 9722
0.821255349500713 9752
0.821968616262482 9759
0.822681883024251 9778
0.82339514978602 9835
0.824108416547789 9868
0.824821683309558 9878
0.825534950071327 9904
0.826248216833096 9936
0.826961483594864 9961
0.827674750356633 9977
0.828388017118402 10011
0.829101283880171 10037
0.82981455064194 10061
0.830527817403709 10081
0.831241084165478 10098
0.831954350927247 10106
0.832667617689016 10153
0.833380884450785 10175
0.834094151212553 10194
0.834807417974322 10236
0.835520684736091 10268
0.83623395149786 10315
0.836947218259629 10348
0.837660485021398 10376
0.838373751783167 10409
0.839087018544936 10448
0.839800285306705 10471
0.840513552068474 10497
0.841226818830243 10522
0.841940085592011 10600
0.84265335235378 10622
0.843366619115549 10672
0.844079885877318 10722
0.844793152639087 10734
0.845506419400856 10798
0.846219686162625 11309
0.846932952924394 11391
0.847646219686163 11442
0.848359486447932 11499
0.8490727532097 11524
0.849786019971469 11583
0.850499286733238 11676
0.851212553495007 11751
0.851925820256776 11791
0.852639087018545 11822
0.853352353780314 11857
0.854065620542083 11895
0.854778887303852 11989
0.855492154065621 12026
0.856205420827389 12043
0.856918687589158 12102
0.857631954350927 12140
0.858345221112696 12190
0.859058487874465 12258
0.859771754636234 12281
0.860485021398003 12352
0.861198288159772 12382
0.861911554921541 12466
0.86262482168331 12530
0.863338088445078 12579
0.864051355206847 12627
0.864764621968616 12668
0.865477888730385 12688
0.866191155492154 12765
0.866904422253923 12826
0.867617689015692 12874
0.868330955777461 12933
0.86904422253923 12973
0.869757489300999 13524
0.870470756062767 13582
0.871184022824536 13683
0.871897289586305 13756
0.872610556348074 13790
0.873323823109843 14212
0.874037089871612 14359
0.874750356633381 14403
0.87546362339515 14452
0.876176890156919 14514
0.876890156918688 14577
0.877603423680457 14675
0.878316690442225 14722
0.879029957203994 14797
0.879743223965763 14852
0.880456490727532 14909
0.881169757489301 14969
0.88188302425107 15049
0.882596291012839 15138
0.883309557774608 15222
0.884022824536377 15439
0.884736091298145 15502
0.885449358059914 15528
0.886162624821683 15564
0.886875891583452 15690
0.887589158345221 15741
0.88830242510699 15774
0.889015691868759 15877
0.889728958630528 15988
0.890442225392297 16158
0.891155492154066 16345
0.891868758915835 16438
0.892582025677603 16477
0.893295292439372 16612
0.894008559201141 16688
0.89472182596291 16767
0.895435092724679 16921
0.896148359486448 17015
0.896861626248217 17117
0.897574893009986 17166
0.898288159771755 17225
0.899001426533524 17367
0.899714693295292 17429
0.900427960057061 17467
0.90114122681883 17608
0.901854493580599 17650
0.902567760342368 17773
0.903281027104137 17865
0.903994293865906 17976
0.904707560627675 18126
0.905420827389444 18194
0.906134094151213 18310
0.906847360912981 18387
0.90756062767475 18450
0.908273894436519 18511
0.908987161198288 18653
0.909700427960057 18775
0.910413694721826 18927
0.911126961483595 19034
0.911840228245364 19161
0.912553495007133 19218
0.913266761768902 19336
0.913980028530671 19470
0.914693295292439 19541
0.915406562054208 19826
0.916119828815977 19973
0.916833095577746 20057
0.917546362339515 20366
0.918259629101284 20673
0.918972895863053 20826
0.919686162624822 21039
0.920399429386591 21580
0.921112696148359 21729
0.921825962910128 21971
0.922539229671897 22153
0.923252496433666 22402
0.923965763195435 22588
0.924679029957204 22803
0.925392296718973 22960
0.926105563480742 23021
0.926818830242511 23450
0.92753209700428 23568
0.928245363766048 23720
0.928958630527817 23870
0.929671897289586 23979
0.930385164051355 24274
0.931098430813124 24374
0.931811697574893 24580
0.932524964336662 24829
0.933238231098431 25196
0.9339514978602 25502
0.934664764621969 25618
0.935378031383738 25843
0.936091298145506 26266
0.936804564907275 26495
0.937517831669044 26730
0.938231098430813 26912
0.938944365192582 27182
0.939657631954351 27582
0.94037089871612 27886
0.941084165477889 28097
0.941797432239658 28439
0.942510699001427 28779
0.943223965763195 29443
0.943937232524964 29827
0.944650499286733 30197
0.945363766048502 30419
0.946077032810271 30690
0.94679029957204 31333
0.947503566333809 31650
0.948216833095578 32007
0.948930099857347 32801
0.949643366619116 32895
0.950356633380884 33514
0.951069900142653 33766
0.951783166904422 34813
0.952496433666191 35280
0.95320970042796 35620
0.953922967189729 36179
0.954636233951498 37321
0.955349500713267 37570
0.956062767475036 38542
0.956776034236805 39089
0.957489300998573 40118
0.958202567760342 41755
0.958915834522111 42158
0.95962910128388 42585
0.960342368045649 42861
0.961055634807418 43862
0.961768901569187 44864
0.962482168330956 46139
0.963195435092725 48064
0.963908701854494 49217
0.964621968616262 50126
0.965335235378031 52278
0.9660485021398 53306
0.966761768901569 53877
0.967475035663338 55672
0.968188302425107 56687
0.968901569186876 57964
0.969614835948645 59144
0.970328102710414 60706
0.971041369472183 62326
0.971754636233952 63358
0.97246790299572 65451
0.973181169757489 67627
0.973894436519258 68504
0.974607703281027 69939
0.975320970042796 70997
0.976034236804565 72682
0.976747503566334 75265
0.977460770328103 77959
0.978174037089872 82650
0.97888730385164 84333
0.979600570613409 88887
0.980313837375178 91302
0.981027104136947 93973
0.981740370898716 95501
0.982453637660485 102868
0.983166904422254 105840
0.983880171184023 113061
0.984593437945792 119603
0.985306704707561 127230
0.98601997146933 131063
0.986733238231098 140523
0.987446504992867 145571
0.988159771754636 156738
0.988873038516405 166568
0.989586305278174 177630
0.990299572039943 183278
0.991012838801712 195495
0.991726105563481 218281
0.99243937232525 237435
0.993152639087019 276752
0.993865905848787 302390
0.994579172610556 317884
0.995292439372325 332492
0.996005706134094 417286
0.996718972895863 575310
};
\addlegendentry{resnext101-32x48d-wsl}
\end{axis}

\end{tikzpicture}
  \lapbox[0pt]{-0.92\columnwidth}{\rotatebox{90}{\hspace{6em}\# of negative pairs}} \\
								   \raisebox{3.5em}[0pt][0pt]{ratio of positive pairs}
		\end{tabular}

\caption{
		Search curves  for different pretrained models.
		Curve F is used to estimate the minimal number of negative pairs (y = ) that human assessors need to inspect before they find the fraction x of positive pairs. 
		The lower the curve  the better (need to inspect manually less pairs).
		The curves are built with the query set Q = MSRVTT full train, the gallery set G = random 596k videos from HowTo100M.}
	\label{fig:search_curves_all}
\end{figure}

As we see resnext101-32x48d-wsl shows the best result. We use this network for searching for duplicates.

It is worth to mention that here we just compare different networks on a fixed benchmark, and pick the best one. But the search curve  significantly depends on data.
This curve should be estimated for each used pair of datasets  and .

\subsubsection{Black frames}
Often two consecutive video segments are glued with several black frames. The cosine similarity of embeddings of two black or near black frames are close to 1.
In this case the most probable candidates for duplicates are black video segments. To prevent this we apply the following rule. Suppose we have a frame 
and the unit length embedding  computed from . 
We find the prevalent color in  and compute the area  filled by this color. Then we compute
the value , where  and  are the height and width of . If this fraction is greater than 0.7 we define ,
otherwise . To calculate similarity between embeddings  and  we use weighted cosine similarity: , instead of
classical cosine similarity. This rule removes majority of all near black frames from the most relevant candidates for duplicates.

\subsubsection{Screensavers detection}
Many videos from ActivityNet, HowTo100m, YouCook2 contain screensavers at the beginning or at the end. It causes a problem like mentioned above with near black frames,
because most of relevant proposals are the same screensavers, but the video content of the remainder video part are different.

Using the system described in Sec.~\ref{ssec:ndvs_gui} we search for duplicates in the ActivityNet dataset, where a lot of the most relevant segments are screensavers.
We collect several hundreds of screensavers and then compute embeddings for each of them. Let us call the resulting set of embeddings as E. Then we apply the following rule: if some embedding  has the similarity
greater that 0.9 to one of embeddings from E, we set . So if the video segment has a part of a screensaver, it will never be in the most relevant proposals.

\subsubsection{GUI}\label{ssec:ndvs_gui}
The important part of the video duplicate search system is the user interface. Without ergonomic and fast interface it is impossible to
assess tens thousands of video pairs. Our system is presented in Fig.~\ref{fig:ndvs_gui}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{imgs/ndvs_web.jpg}
	\caption{Web system used to find duplicates. Images on the first and third row are not duplicates and the second row contains duplicate.}
	\label{fig:ndvs_gui}
\end{figure}

The system shows video pairs with the highest scores on top. A user needs to scroll down a web page (new videos are loaded dynamically with ajax), and if a video duplicate is detected,
a user should press the \textit{Duplicate} button, if there are no duplicates in the current viewport, no action is required. When a user scrolls a web page,
all non-duplicate pairs automatically are saved to a log file. Additionally several users at the same time can assess video pairs.


\begin{table}[H]
	\centering
	\begin{tabular}{
			|
			c@{\hspace{1\tabcolsep}}
			|
			c@{\hspace{1\tabcolsep}}
			c@{\hspace{1\tabcolsep}}
			|
			c@{\hspace{1\tabcolsep}}
			c@{\hspace{1\tabcolsep}}
			|
			c@{\hspace{1\tabcolsep}}
			c@{\hspace{1\tabcolsep}}
			|
		}
		\toprule
			\multirow{2}{*}{dataset} &
			\multicolumn{2}{c|}{M} & 
			\multicolumn{2}{c|}{A} & 
			\multicolumn{2}{c|}{L}
			\\
		& test & train & test & train & test & train \\
		\midrule
			M             & 114&223 &  6&10     & 0&0 \\
			A             & 10&6    &  127&163 & 0&0 \\
			L             & 6&2744   & 0&0      &	0&0 \\
			YouCook2      & 13&27    & 7&10     & 0&0 \\
			MSVD          & 1&1      & 1&1      & 1&1 \\
			TGIF          & 6&8      & 0&0      & 0&0 \\
			Twitter Vines & 3&3      & 0&0      & 0&0 \\
			Kinetics700   & 4&5      & 456&464 & 0&0 \\
			HowTo100M     & 177&154    & 209&209 & 0&0 \\
		\bottomrule
	\end{tabular}

	\caption{The leftmost column represents train parts of datasets, and the upper row represents test parts of datasets.
			 Column "test" means how many video segments are in the test part that have the corresponding pair in the train part either with the same YouTube ID or
			 manually marked as a duplicate. Column "train" represents the number of video segments in the train part that have
			 corresponding pair in the test dataset either with the same YouTube ID or manually marked as a duplicate. All segments counted in the "train" column
			 are removed from the train part. For example consider the column "A" and the row "M". train=10 means that the MSRVTT train part contains 10 video segments that
			 have a pair in the ActivityNet test part. These 10 videos must be removed from train part when dataset are combined.			 
			 test=6 means that ActivityNet test has 6 video segments that have a pair in the MSRVTT test part.
			 }
	\label{tab:found_isect}
\end{table}
\subsection{Cleaning results}\label{sect:cleaning_res}

Recall that our cleaning method consists of two stages. In the first stage we throw out from the train part all video segments that have a pair with the same YouTube ID
in test parts of MSRVTT or ActivityNet. The second stage is matching video segments by embeddings and manually assess several thousands
pairs with the highest score.

In Tab.~\ref{tab:found_isect} we report how many duplicates are found for each pairs of datasets. This table represents the final
result after applying these two stages.





Separate results for the first and the second stages are reported in Sec.~\ref{sect:isect_ytvid}.

Note that columns "test" and "train" in Tab.~\ref{tab:found_isect} may have different values. Consider the situation when the test part have a video segment A, and
the train part have two video segments A1 and A2. And both are marked as duplicates with A. In this case the video segment A brings +1 to the "test" column
and A1, A2 bring +2 to the "train" column.


The most problematic datasets in terms of the number of duplicates are
MSRVTT and ActivityNet. These datasets overlap with itself (e.g. MSRVTT test overlap with MSRVTT train).
We found more than 100 duplicate pairs for both of them.
Other problematic datasets are HowTo100M and Kinetics700, these datasets are large, so we can't assess the required number of video pairs to find 95\% or 99\%
of duplicates. But we can assess a smaller number of pairs and using search curves  (see Sec.~\ref{ssec:how_many_pairs}) can extrapolate this value to 100\%.
We found that HowTo100M may have the intersection with MSRVTT test full by about 300 videos (10\% of the MSRVTT test full).
The similar situation is about the ActivityNet test set and Kinetics700, the intersection could be near 500-600 videos (10\% of the ActivityNet test set).



In Tab.~\ref{tab:mmt_cleaning} we report results on MSRVTT for MMT retraining with no cleaning, after cleaning by the YouTube ID and cleaning combination by the YouTube ID and the manual assessment.
The manual cleaning for 1k-A and 1k-B is incomplete because we only do cleaning for the full split. The following situation takes place for 1k-A, 1k-B splits: when 1k videos from the full test are taken for test
and the remaining 2k videos are moved to the train part, the additional overlapping is introduced, because these 1k and 2k videos are overlapping. We do not remove this overlap in this research.



\begin{table}[H]
	\centering
	\begin{tabular}{|ll@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l|}
		\toprule
		\multirow{2}{*}{split} & no    & \multirow{2}{*}{by ID} & by ID + \\
							   & clean &                        & manual \\

\midrule
		full &  31.1 &  31.1 &  30.2\\
		1k-A &  54.8 &  50.7 &  49.4\\
		1k-B &  51.1 &  46.1 &  46.4\\
		\bottomrule
	\end{tabular}

	\caption{Comparison for original MMT trained (7 modalities) on MSRVTT without cleaning, with cleaning by the YouTube ID only, and
			 with cleaning by the YouTube ID plus the manual assessment.}
	\label{tab:mmt_cleaning}
\end{table}

As you can see after cleaning the performance is significantly decreased on 1k-A and 1k-B splits for original MMT.






  









\subsubsection{Intersection by YouTube ID and embeddings}\label{sect:isect_ytvid}

In Tab.~\ref{tab:ytvid_isect} we report the intersection by the YouTube ID between test parts of MSRVTT (full, 1k-A, 1k-B) and ActivityNet with
train parts of MSRVTT (full, 1k-A, 1k-B), ActivityNet, Kinetics700, YouCook2, HowTo100m, MSVD.



\begin{table}[ht]
	\centering
	\begin{tabular}{
			|@{\hspace{0.5\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			|@{\hspace{0.5\tabcolsep}}
			c@{\hspace{1\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			|@{\hspace{0.5\tabcolsep}}
			c@{\hspace{1\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			|@{\hspace{0.5\tabcolsep}}
			c@{\hspace{1\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			|@{\hspace{0.5\tabcolsep}}
			c@{\hspace{1\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			|
		}
		\toprule
data &
		\multicolumn{2}{c|@{\hspace{0.5\tabcolsep}}}{M} &
		\multicolumn{2}{c|@{\hspace{0.5\tabcolsep}}}{M} &
		\multicolumn{2}{c|@{\hspace{0.5\tabcolsep}}}{M} &
		\multicolumn{2}{c|}{A}
		\\
		set &
		{\footnotesize test} & {\footnotesize train} & 
		{\footnotesize test} & {\footnotesize train} & 
		{\footnotesize test} & {\footnotesize train} &
		{\footnotesize test} & {\footnotesize train}
		\\
		\midrule
		M          	       & 0&0 & 0&0  & 104&179 & 2&4     \\
		M  & 2362&1990  & 372&415 & 827&1007 & 2&4     \\ 
		M  & 1689&1367  & 563&634 & 380&407  & 2&4     \\
		A	    	       & 0&0        & 0&0     & 0&0      & 0&0     \\
		K        		   & 5&4        & 1&1     & 0&0      & 408&408 \\
		Y       		   & 8&4        & 2&2     & 2&2      & 3&3     \\
		HT100M   		   & 147&117    & 39&38   & 57&53    & 175&175 \\
		MSVD		       & 3&1        & 2&1     & 0&0      & 1&1     \\
		\bottomrule
	\end{tabular}

	\caption{First stage. The leftmost column represents train parts of datasets, and the upper row represents test parts of datasets. Column "test" represents
	number of video segments in test part that have corresponding video in train part with the same YouTube ID. Column "train" represents number of video
	in train part that have corresponding pair in test part with the same ID. For example: if we combine M and YouCook2, we should remove 4 video from YouCook2 train.}
	\label{tab:ytvid_isect}
\end{table}

It is worth to mention that MSRVTT 1k-A test and 1k-B test have a large overlap ratio by the YouTube ID with the 1k-A train and the 1k-B train parts correspondingly. 
Both splits have the overlap ratio of about 38\% between the train part and the test part.
We also emphasize that the original MSRVTT full split does not overlap by the YouTube ID between the test and train parts.


\begin{table}[ht]
	\centering
	\begin{tabular}{
			|
			c
			|@{\hspace{0.5\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			|@{\hspace{0.5\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			|@{\hspace{0.5\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			c@{\hspace{0.5\tabcolsep}}
			|
		}
		\toprule
data &
			\multicolumn{3}{c| @{\hspace{0.5\tabcolsep}}}{M} & 
			\multicolumn{3}{c| @{\hspace{0.5\tabcolsep}}}{A} & 
			\multicolumn{3}{c|}{L}
			\\
		set &  
		{\footnotesize seen} &
		{\footnotesize found} &
		{\footnotesize total} & 
		{\footnotesize seen} &
		{\footnotesize found} &
		{\footnotesize total} & 
		{\footnotesize seen} &
		{\footnotesize found} &
		{\footnotesize total}
		\\
		\midrule
			M             & 10k&114&114 &    1k&      6&6 &   1k&0&0 \\ 
			A             & 10k&10&10   &    15k& 127&142 &   1k&0&0 \\
			L             & 3k& 6&6     &    2k&      0&0 &   ---&---&--- \\
			Y             & 2k& 13&13   &    1k&      7&7 &   1k&0&0 \\
			MSVD          & 1k& 1&1     &    1k&      1&1 &   1k&1&1 \\
			T             & 2k& 6&6     &    2k&      0&0 &   3k&0&0 \\
			V             & 2k& 3&3     &    0k&      0&0 &   1k&0&0 \\
			K             & 2k& 1&2     &    30k& 227&539 &   2k&0&0 \\
			HT100M        & 5k& 15&320  &    ---& ---&--- &   ---&---&--- \\
		\bottomrule
	\end{tabular}
	
	\caption{Second stage. The leftmost column represents train parts of datasets, and the upper row represents test parts of datasets.
			 Column "seen" represents the number of video segments that we manually assess for a given pair of datasets.
	         Column "found" represents the number of videos in the test part for which there exists the corresponding duplicate video segment in the train part.
			 Column "total" represents the approximately estimated total number of videos from the test part that have a duplicate pair in the train part.
			 Symbol "---" means that the intersection is not computed because it requires too much human resources.}
	\label{tab:manual_isect_Ntest}
\end{table}



In Tab.~\ref{tab:manual_isect_Ntest} we report the statistics for the second deduplication stage (searching by embeddings).
We do not compute an intersection for MSRVTT 1k-A and 1k-B splits. 

In this table we present the number of manually found duplicates and the estimated maximum number of duplicates for a given pair of datasets.
We managed to find the intersection for almost all pairs of datasets.

The maximum number of duplicates is computed based on the search curve . As we told in Sec.~\ref{ssec:how_many_pairs} the search curve significantly depends on data. We compute
the search curve for all pairs of datasets in Tab.~\ref{tab:manual_isect_Ntest}. The search curve for each particular pair of datasets is build exactly in the same way
as described in Sec.~\ref{ssec:how_many_pairs}. For example, to compute the search curve for MSRVTT test and ActivityNet train we define MSRVTT test as , ActivityNet train as , then augment
 to produce , and use the algorithm described in Sec.~\ref{ssec:how_many_pairs}.

Using the column "seen" from Tab.~\ref{tab:manual_isect_Ntest} we can compute how many pairs need to be assessed to find the full overlap between datasets. For example, inspect
5k pairs for HowTo100M dataset and MSRVTT (the row "HT100M" and the column "M"), we found 15 duplicates, so the approximate maximum number of duplicates is 320: 5k * (320 / 15) = 106k.
So, to find the full overlap using the current version of algorithm it is needed to manually assess 106k video pairs and it is too much, that's why we do not find full intersection
for this specific pair of datasets.
 
\section{Hyperparameters}
To train our best networks (MMT(MALVYMTS) L9H8 CLIP+audio,  MDMMT(MALVYMTS) L9H8 irCSN152+audio and MMT(MALVYMTS) L9H8 CLIP+irCSN152+audio)
we use 50 epochs and define a single epoch as 150K examples per
GPU (in total 1.2M examples per epoch on 8 GPUs). We use Adam optimizer without weight decay, the initial value for a learning rate is 5e-5, after each epoch we multiply the learning rate by 0.95.
Batch size of 32 examples per GPU is used. We do not exchange embeddings between GPUs.
We use bi-directional max-margin ranking loss with margin 0.05.
In Bert and the video transformer encoder we use dropout 0.2 in attention and in FFN block. We use 8 Nvidia V100 32GB GPUs. The training time is about 14 hours.
 
\section{Pretrained model}\label{sect:pretrain_ht100m}
The well known method to boost the performance in video retrieval tasks is to use a pretrained model. First the neural network is trained on some large dataset, then
at second stage it is finetuned for target target dataset. In video retrieval task HowTo100M dataset is often used for pretraining. In this work we use HowTo100M for pretraining in the same way.




\begin{table*}[htb!]
  \centering
  \begin{tabular}{|l @{\hspace{1\tabcolsep}} |l@{\hspace{1\tabcolsep}}| l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l|}
    \toprule
    \multirow{2}{*}{model} &\multirow{2}{*}{\rotatebox{90}{pretr}}   & \multicolumn{5}{c|}{MSRVTT full clean  text  video} \\
			   && R@1 & R@5 & R@10 & MnR & MdR \\
    \midrule
      \ours MDMMT(MALVYMTS) L9H8 irCSN152+audio	    &yes& 15.8 & 38.9 & 51.0 & 76.4 & 10.0 \\
      \ours MDMMT(MALVYMTS) L9H8 irCSN152+audio	    &no & 14.5 & 36.8 & 48.8 & 82.2 & 11.0\\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+audio             &yes& 21.5 & 47.4 & 59.6 & 57.7 & 6.0 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+audio             &no & 20.0 & 45.1 & 57.3 & 63.1 & 7.0 \\
    \bottomrule
  \end{tabular}
  \caption{Performance on the MSRVTT full clean split with and without pretrained model (HowTo100m).}
  \label{tab:pretr-msrvtt}


\vspace{\textfloatsep}

\begin{tabular}{|l @{\hspace{1\tabcolsep}} |l@{\hspace{1\tabcolsep}}| l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l|}
    \toprule
    \multirow{2}{*}{model} &\multirow{2}{*}{\rotatebox{90}{pretr}}   & \multicolumn{5}{c|}{ActivityNet  text  video} \\
			   && R@1 & R@5 & R@10 & MnR & MdR \\
    \midrule
      \ours MDMMT(MALVYMTS) L9H8 irCSN152+audio &yes& 15.1 & 38.3 & 51.5 & 92.4 & 10.0 \\
      \ours MDMMT(MALVYMTS) L9H8 irCSN152+audio &no & 12.0 & 33.7 & 46.3 & 119.9 & 13.0 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+audio     &yes& 17.7 & 41.6 & 54.3 & 76.0 & 8.3 \\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+audio     &no & 15.2 & 37.9 & 50.1 & 93.4 & 10.3 \\
    \bottomrule
  \end{tabular}
  \caption{Performance on ActivityNet with and without pretrained model (HowTo100m). The performance reported for the text to video retrieval task on our own subset of the original ActivityNet test part.
	   See Sec.~\ref{sect:datasets} for details.}
  \label{tab:pretr-anet}



\vspace{\textfloatsep}

\begin{tabular}{|l @{\hspace{1\tabcolsep}} |l@{\hspace{1\tabcolsep}}| l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l@{\hspace{1\tabcolsep}}l|}
    \toprule
    \multirow{2}{*}{model} &\multirow{2}{*}{\rotatebox{90}{pretr}}   & \multicolumn{5}{c|}{LSMDC  text  video} \\
			   && R@1 & R@5 & R@10 & MnR & MdR \\
    \midrule
      \ours MDMMT(MALVYMTS) L9H8 irCSN152+audio &yes& 13.1 & 31.3 & 40.1 & 74.5 & 19.3\\
      \ours MDMMT(MALVYMTS) L9H8 irCSN152+audio &no&  12.6 & 30.2 & 39.6 & 76.1 & 19.7\\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+audio     &yes& 17.2 & 34.9 & 45.3 & 65.6 & 14.0\\
      \ours MDMMT(MALVYMTS) L9H8 CLIP+audio     &no&  16.2 & 35.4 & 45.1 & 64.9 & 14.7\\
    \bottomrule
  \end{tabular}
  \caption{Performance on LSMDC with and without pretrained model (HowTo100m).}
  \label{tab:pretr-lsmdc}
\end{table*}

In our training procedure we use 8 Nvidia V100 32Gb GPUs, we train for 200 epochs where one epoch is defined as 80k examples on each GPU (in total network sees 640k examples on 8 GPUs per epoch).
We use batch size 64 for each GPU and do not exchange embeddings between GPU.
Initial learning rate is 5e-5. After each epoch we multiply learning rate by 0.98.
We use the full HowTo00M dataset. The model is trained either with two modalities: motion/RGB and audio
or with three modalities: motion, RGB and audio, depending on how many modalities are used in final model.
The total training time is about 24 hours. We use bi-directional max-margin ranking loss with margin 0.05.

In Tab.~\ref{tab:pretr-msrvtt},~\ref{tab:pretr-anet}~and~\ref{tab:pretr-lsmdc} we compare two our models: MDMMT(MALVYMTS) L9H8 irCSN152+audio and MDMMT(MALVYMTS) L9H8 CLIP+audio
when they are trained from the pretrained model or not. In these three tables we present the same four models (no special finetuning for the target dataset) tested on different datasets.

As we can see in Tab.~\ref{tab:pretr-msrvtt} the pretrained model increases R1 metric by 1\% and R5 by 2\%. The pretrained model also increase performance on ActivityNet dataset, see Tab.~\ref{tab:pretr-anet}.
For R1 metric the improvement is about 2\% and for R5 metric is about 4\%. For LSMDC dataset, see Tab~\ref{tab:pretr-lsmdc}, we have approximately the same results with and without pretraining.


 


\end{document}
