\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Fig/visual.png}
    \caption{Visual comparison on the Parkour dataset, Vimeo90K~\cite{vimeo90k} dataset and Vid4~\cite{frvsr} dataset. Example (a), (b), (c), (d) and (e) are selected from the large motion Parkour dataset. Example (f) and (g) are selected in the Vimeo90K~\cite{vimeo90k} dataset. Example (h) is from Vid4~\cite{frvsr} dataset. We mark the inset locations on the video stills on the left. To make our discussion clearer, we add arrows pointing to the region that we will be discussing in Sec.~\ref{subsec:visual}. \textcolor{red}{Best viewed in PDF.}}
    \label{fig:visual}
    \vspace{-10pt}
\end{figure*}

\section{Experiments}\label{sec:result}
In this section, we compare our work with recent state-of-the-art video super-resolution (VSR) and single image super-resolution (SISR) methods.
We select comparison methods based on their approaches to the super-resolution problem: VSR via explicit frame alignment (TOFlow~\cite{vimeo90k} and TGA~\cite{tga}), VSR via implicit frame alignment (EDVR~\cite{edvr}), VSR via regular non-local attention (PFNL~\cite{pfnl}) and SISR via regular non-local attention (CSNLN~\cite{mei}) applied to each video frame individually.
Similar to other VSR works, in this paper, we focus on the 4x scaling case for all the comparisons shown in this section.
To obtain the low-resolution input, we use bicubic down-sampling on the ground truth high-resolution frames.
According to our experiment, PFNL~\cite{pfnl} and TGA~\cite{tga} introduce serious aliasing artifacts to the results using bicubic down-sampled video.
To make the comparison fair, for PFNL~\cite{pfnl} and TGA~\cite{tga}, we apply Gaussian blur to the ground truth frames before down-sampling following the procedure in their papers.
Unless otherwise stated, our results shown in this section are generated with the memory size of $N=256$ in the memory-augmented attention module.
We conduct the experiment on a desktop computer with an NVIDIA 2080Ti GPU.
The average processing speed of our network is 251ms per 960x540 HR frame. 

\begin{table*}[t]
\footnotesize
\begin{tabular}{p{0.08\textwidth}||p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}||p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}||p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}||p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}}
            & \multicolumn{3}{c||}{(a) Parkour Dataset} & \multicolumn{3}{c||}{(b) Vimeo90K Dataset~\cite{vimeo90k}}    & \multicolumn{3}{c||}{(c)Vid4 Dataset~\cite{frvsr}} 
            & \multicolumn{3}{c}{(d)SPMC Dataset~\cite{spmc}} \\ \hline
            & PSNR in dB          & SSIM   &LPIPS         & PSNR in dB & SSIM &LPIPS  & PSNR in dB & SSIM &LPIPS  & PSNR in dB & SSIM   &LPIPS                  \\ \hline
Bicubic     & 29.51 (+3.97)              & 0.8712    &0.3101      & 29.75 (+4.96)    & 0.8476 & 0.2948 & 22.34 (+2.81)     & 0.6131   &0.5186  &25.67 (+3.55) &0.7241 &0.4270                 \\ \hline
\textbf{MANA (Ours)} & \textbf{\textcolor{red}{33.48}}              & \textbf{\textcolor{red}{0.9356}}   &\textbf{\textcolor{red}{0.1241}}       & \textbf{34.71}     & \textbf{0.9261} &\textbf{0.1101} & \textbf{25.15}     & \textbf{0.7796}    &\textbf{\textcolor{blue}{0.2744}}       &\textbf{\textcolor{red}{29.22}} &\textbf{\textcolor{red}{0.8458}} &\textbf{\textcolor{red}{0.2119}}           \\ \hline
EDVR~\cite{edvr}        & 31.61 (+1.87)              & 0.9113    &0.1900      & \textcolor{red}{35.68 (-0.97)}      & \textcolor{red}{0.9372} &\textcolor{blue}{0.1019} & \textcolor{red}{25.79 (-0.64)}     & \textcolor{red}{0.8063}    &\textcolor{red}{0.2489}      &27.98 (+1.24) &0.8109 &0.2715            \\ \hline
TOFlow~\cite{vimeo90k}      & 32.35 (+1.13)             & 0.9197     &0.1804     & 32.96 (+1.75)     & 0.9041 &0.1451 & 24.41 (+0.74)    & 0.7435   &0.3340      &28.55 (+0.67) &0.8327 &0.2661             \\ \hline
TGA~\cite{tga}         & 31.14 (+2.34)             & 0.9033    &0.2224      & \textcolor{blue}{35.03 (-0.32)}      & \textcolor{blue}{0.9310} &\textcolor{red}{0.1013}  & \textcolor{blue}{25.36 (-0.21)}     & \textcolor{blue}{0.7949}   &0.2834      &\textcolor{blue}{29.06 (+0.16)} &\textcolor{blue}{0.8449} &0.2390             \\ \hline
PFNL~\cite{pfnl}        & 32.04 (+2.44)            & 0.9189  &0.2244        & 31.86 (+2.85)     & 0.8959 &0.2012  & 25.01 (+0.14)    & 0.7788   &0.3204    &28.27 (+0.95) &0.8270 &0.3100               \\ \hline
CSNLN~\cite{mei}       & \textcolor{blue}{32.93 (+0.55)}              & \textcolor{blue}{0.9275}  &\textcolor{blue}{0.1357}        & 33.55 (+1.16)    & 0.9091 &0.1338 & 24.09 (+1.06)    & 0.7202 &0.3425 &28.79 (+0.43) &0.8275 &\textcolor{blue}{0.2343} \\ \hline
\end{tabular}
\caption{Quantitative comparison on (a) Parkour dataset, (b) Vimeo90K~\cite{vimeo90k} dataset and (c) Vid4 dataset. The metrics used are PSNR, SSIM and LPIPS. Larger numbers indicate better results for PSNR and SSIM, smaller numbers indicate better results for LPIPS. We also note the PSNR gain of our method comparing to other methods; a positive gain means that our method performs better than the corresponding method.}
\label{tab:quant}
\vspace{-15pt}
\end{table*}

\begin{table}[htb]
\scriptsize
\begin{tabular}{p{0.035\textwidth}<{\centering}|p{0.035\textwidth}<{\centering}|p{0.035\textwidth}<{\centering}|p{0.035\textwidth}<{\centering}|p{0.035\textwidth}<{\centering}|p{0.035\textwidth}<{\centering}|p{0.035\textwidth}<{\centering}|p{0.035\textwidth}<{\centering}}\label{tab:lpips}
            & PSNR & SSIM & LPIPS   &       & PSNR & SSIM & LPIPS   \\ \hline
\textbf{MANA}& \textbf{\textcolor{red}{38.62}} & \textbf{\textcolor{red}{0.9606}} & \textbf{\textcolor{blue}{0.1586}}& TGA &38.26                   & \textcolor{blue}{0.9588} & \textcolor{red}{0.1570}\\ \hline
EDVR & \textcolor{blue}{38.33} & 0.9544& 0.1641 & PFNL  &35.90                   & 0.9449  &0.1985 \\ \hline
TOFlow &36.55 & 0.9471 &0.1902 &CSNLN  & 37.79 & 0.9523  &0.1780 \\ \hline
\end{tabular}
\caption{Quantitative comparison on top 6\% large motion videos in Vimeo90K~\cite{vimeo90k} dataset.}
\label{tab:top5}
\vspace{-15pt}
\end{table}

\subsection{Datasets and Metrics}\label{subsec:dataset}
As discussed in Sec.~\ref{sec:intro}, the cross-frame non-local attention in our method enables VSR without frame alignment.
To validate the robustness of our method to large motion videos, we randomly collect 14 parkour video clips from the Internet.
Parkour is a form of extreme sport focusing on passing obstacles in a complex environment by running, climbing, and jumping.
Usually taken using egocentric wearable cameras, parkour videos are typical examples in the real-world where large camera motions are everywhere.
Example video stills from the \textit{Parkour dataset} are shown in Fig.~\ref{fig:dataset}.
We further evaluate our method on regular small motion videos using Vimeo90K~\cite{vimeo90k} test set and Vid4~\cite{frvsr}.
For all the test sets, we use the average PSNR and SSIM~\cite{ssim} on the RGB channels to quantitatively evaluate the performance of the methods.
In addition, we apply LPIPS~\cite{lpips} to evaluate the perceptual similarity between the super-resolved frames and the ground truth high-resolution frame.
Since the performance can be different across computation platforms and the quantitative metric calculation might be different in these works, we re-run their code and calculate the metrics in the same way on the same computer.

\subsection{Visual Comparisons}\label{subsec:visual}
The visual comparison of the examples from the Parkour dataset, Vimeo90K~\cite{vimeo90k} dataset and Vid4~\cite{frvsr} is shown in Fig.~\ref{fig:visual}.
To make the discussion concise, we label the ID at the bottom left of each video.
We also added arrows pointing at the regions we will be discussing.

Example (a), (b), (c), (d) and (e) are selected from the Parkour dataset.
These examples contain large motion and are challenging to existing VSR methods.
Our method can reconstruct repetitive patterns like Example (a) and (b), while explicit frame alignment methods TOFlow~\cite{vimeo90k} and TGA~\cite{tga} fail due to the inaccurate frame alignment.
EDVR~\cite{edvr} result is more blurry than our result in example (a) and (b), and \textit{the blurry issue is more visible when viewed in dynamics as we will show in the supplementary video}.
This indicates that the deformable convolution alignment cannot handle the alignment with large frame displacement.
The VSR and SISR methods PFNL~\cite{pfnl} and CSNLN~\cite{mei} using non-local attention also suffer from the blurry issue, potentially due to the non-local attention performance degradation problem discussed in Sec.~\ref{subsec:nonlocal}.

Example (c) focuses on general details of objects.
The frame-aligning VSR methods introduce ghosting artifacts (EDVR~\cite{edvr}) and deformation (TOFlow~\cite{vimeo90k} and TGA~\cite{tga}) due to the inaccurate alignment.
PFNL~\cite{pfnl} and CSNLN~\cite{mei} results have less detail than ours, indicating that our one-hot non-local attention improves the quality of regular non-local attention.
Example (d) focuses on human face shape and details.
As shown in the bicubic result, the original facial details are completely lost due to the down-sampling.
Our method reconstructs visually pleasing details of human faces thanks to the memory-augmented module, while the comparison methods introduce blur (EDVR~\cite{edvr}, TOFlow~\cite{vimeo90k} and PFNL~\cite{pfnl}) or reconstruct shapes that do not look like a human (TGA~\cite{tga} and CSNLN~\cite{mei}).

Example (e) contains thin structures.
Similar to examples (a) and (b), failure in frame alignment has negatively affected the VSR methods.
In this case, the performance of VSR methods EDVR~\cite{edvr}, TOFlow~\cite{vimeo90k} and TGA~\cite{tga} are even worse than the SISR method CSNLN~\cite{mei}.
Our method with one-hot non-local attention can achieve a comparable result to CSNLN~\cite{mei} in this example since our network does not require frame alignment.

As we will discuss in Sec.~\ref{subsec:quant}, the overall average quantitative metric score of our method is slightly inferior to that of EDVR~\cite{edvr} and TGA~\cite{tga} in the Vimeo90K~\cite{vimeo90k} and Vid4 dataset~\cite{frvsr} which are relatively easy for frame aligning VSR methods.
However, a larger deviation to the ground truth does not always indicate worse performance.
Example (f) and (g) are selected from the Vimeo90K~\cite{vimeo90k} test set.
Our method tends to produce visually sharper results than EDVR~\cite{edvr} and TGA~\cite{tga}, which is often more preferred in the video super-resolution task.
Example (h) is a widely used example in Vid4~\cite{frvsr}.
In this example, EDVR~\cite{edvr} and TGA~\cite{tga} generate the best results, but our result is comparable to their results.

To further evaluate the robustness of our method in real-world scenarios, we provide additional results in the supplementary material.
These videos are arbitrarily selected from different types of videos, covering animation, movies, and vlogs.
We encourage the readers to read the supplementary PDF for a more complete visual comparison.

\begin{table*}[t]
\footnotesize
\centering
\begin{tabular}{p{0.08\textwidth}||p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}||p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}||p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}||p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}|p{0.048\textwidth}<{\centering}}
            & \multicolumn{3}{c||}{Parkour Dataset} & \multicolumn{3}{c||}{Vimeo90K Dataset~\cite{vimeo90k}}    & \multicolumn{3}{c||}{Vid4 Dataset~\cite{frvsr}}  &  \multicolumn{3}{c}{SPMC Dataset~\cite{spmc}}     \\ \hline
            & PSNR (dB)          & SSIM    &LPIPS        & PSNR (dB) & SSIM &LPIPS  & PSNR (dB) & SSIM &LPIPS & PSNR (dB) & SSIM &LPIPS                        \\ \hline
No\_Mem     & 33.31              & 0.9343  &\textcolor{red}{0.1196}        & 34.47    & 0.9232 & 0.1696 & 25.09     & 0.7742  &0.3018 &29.03 &0.8389 &0.2256                    \\ \hline
$N=128$  & 33.44              & 0.9350     &0.1254     & 34.65       & 0.9254 &0.1886  & 25.13     & 0.7787  &0.2859 &29.24 &0.8449 & 0.2148                   \\ \hline
$\bm{N=256}$        & \textbf{\textcolor{red}{33.48}}              & \textbf{\textcolor{red}{0.9356}}    &\textbf{0.1241}      & \textbf{\textcolor{red}{34.71}}               & \textbf{\textcolor{red}{0.9261}} &\textbf{\textcolor{red}{0.1644}}  & \textbf{\textcolor{blue}{25.15}}     & \textbf{0.7796} &\textbf{\textcolor{red}{0.2744}} &\textbf{29.22} &\textbf{\textcolor{red}{0.8458}} &\textbf{\textcolor{red}{0.2119}}                     \\ \hline
$N=512$      & \textcolor{blue}{33.47}              & \textcolor{blue}{0.9354}   &0.1245       & \textcolor{blue}{34.71}               & \textcolor{blue}{0.9261} &\textcolor{blue}{0.1647}  & 25.14     & \textcolor{red}{0.7797}  &\textcolor{blue}{0.2840} &\textcolor{blue}{29.25} &0.8449 &\textcolor{blue}{0.2129}                    \\ \hline
$N=1024$         & 33.47 & 0.9354  &\textcolor{blue}{0.1238}  & 34.68   & 0.9257 &0.1649   & \textcolor{red}{25.15}     & \textcolor{blue}{0.7797}   &0.2852 &\textcolor{red}{29.26} &\textcolor{blue}{0.8452} &0.2142                   \\ \hline
\end{tabular}
\caption{Ablation study on the memory size in the memory-augmented attention module. The $N=256$ is selected for the experiments shown in Sec.~\ref{subsec:visual} and Sec.~\ref{subsec:quant}}
\label{tab:ablation}
\end{table*}

\subsection{Quantitative Comparisons}\label{subsec:quant}
In Table~\ref{tab:quant}(a), we compare the average PSNR and SSIM~\cite{ssim} of our method with the comparison methods on the Parkour dataset.
In this table, larger PSNR and SSIM and smaller LPIPS loss indicate better results.
We mark the best result in red and the second best result in blue.

The videos in the Parkour dataset have extremely large motions, making the accurate alignment of the frames difficult.
Among the comparison methods, TOFlow~\cite{vimeo90k} explicitly estimates the optical flow for warping neighbor frames; TGA~\cite{tga} uses homography to align neighbor frames; EDVR~\cite{edvr} implicitly align frames using learned kernel offset for deformable convolution.
The performances of these methods are even inferior to that of the SISR method CSNLN~\cite{mei} in the Parkour dataset, since fusing misaligned frames often cause blurry or ghosting artifacts in the result.
This indicates that the large motions have a negative effect on the performance of traditional VSR methods.

The comparison method PFNL~\cite{pfnl} uses pair-wise non-local attention on all the pixels in the entire segment.
As discussed in Sec.~\ref{subsec:nonlocal}, the traditional non-local attention is difficult to train due to the large GPU memory consumption and the performance degradation with a large number of pixels.
Although the performance of their method is better than frame alignment methods EDVR~\cite{edvr} and TGA~\cite{tga}, using traditional non-local attention on all video segments is worse than applying it only on a single frame like CSNLN~\cite{mei}.
Our cross-frame non-local attention mechanism significantly improves the performance of non-local attention by introducing the one-hot attention in the video super-resolution task.

In addition, we provide the quantitative comparison on the Vimeo90K dataset~\cite{vimeo90k}, the Vid4 dataset~\cite{frvsr} and the SPMC dataset~\cite{spmc} in Table~\ref{tab:quant}(b), (c) and (d) respectively.
The metrics and color labels are the same as Table~\ref{tab:quant}(a).
For these general small motion videos datasets, we also achieve better results than the explicit optical flow alignment VSR method TOFlow~\cite{vimeo90k} and the other non-local attention super-resolution methods PFNL~\cite{pfnl} and CSNLN~\cite{mei}.
Note that the single image non-local attention method CSNLN~\cite{mei} also outperforms video non-local attention method PFNL~\cite{pfnl} in the Vimeo90K dataset.
The PSNR and SSIM value of our method is slightly inferior to that of EDVR~\cite{edvr} and TGA~\cite{tga} in the Vimeo90K and Vid4 datasets.
EDVR~\cite{edvr} has a 0.97dB and 0.64dB PSNR gain to our method in Vimeo90K and Vid4 respectively.
TGA~\cite{tga} has a 0.32dB and 0.21dB PSNR gain to our method in Vimeo90K and Vid4 respectively.
The perceptual quality measured by the LPIPS~\cite{lpips} are similar among EDVR~\cite{edvr}, TGA~\cite{tga} and our method, with around 0.02 differences.
However, for the large motion examples in the Parkour dataset, our method has a larger PSNR gain (1.87dB and 2.34dB) and LPIPS gain (0.0659 and 0.0983) in the performance comparing to EDVR~\cite{edvr} and TGA~\cite{tga}.
Moreover, in Table.~\ref{tab:top5}, we computed the optical flow for videos in the Vimeo90K~\cite{vimeo90k} test set and ranked them based on the average flow magnitude.
It can be observed that even though EDVR~\cite{edvr} and TGA~\cite{tga} is slightly superior on average, our method is actually better on the large motion videos in Vimeo90K~\cite{vimeo90k}.

We also note that EDVR~\cite{edvr} is biased towards Vimeo90K~\cite{vimeo90k}, given the significant performance drop in the other small motion video dataset SPMC~\cite{spmc}. 
This implies that our method is more robust than the comparison methods.
We also provide the quantitative evaluation on the real-world videos in the supplementary material, which also supports the superior of our method in robustness.

\subsection{Ablation Study}\label{subsec:ablation}
In Table~\ref{tab:ablation}, we quantitatively compare the performance of different configurations in our network.
Specifically, we set the memory size $N$ of the memory-augmented attention module to 128, 256, 512 and 1024.
To verify the effectiveness of the memory-augmented attention module, we also experimented with the network with cross-frame non-local attention module only (labeled as \textit{No\_Mem} in Table~\ref{tab:ablation}).
Among these configurations, $N=256$ achieves the best result and is selected in the comparisons in Sec.~\ref{subsec:visual} and Sec.~\ref{subsec:quant}.
Using smaller memory ($N=128$) results in slight performance degradation.
The benefits saturate when using a larger memory ($N=512$ and $N=1024$), implying that the local details of low-resolution frames can be well represented in low-dimensional space.
The performance of our network degrades without the memory-augmented attention module.
However, solely using the cross-frame non-local attention module, our network outperforms comparison methods in the Parkour dataset and achieves comparable performance in the Vimeo90K dataset.
