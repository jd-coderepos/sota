\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{stfloats}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{bbding} 
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}





\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 \newcommand{\hw}[1]{\textcolor{red}{[hw: #1]}}
\newcommand{\zyx}[1]{\textcolor{red}{[zyx: #1]}}
\newcommand{\twoD}{2D\xspace}
\newcommand{\threeD}{{3D}\xspace} \iccvfinalcopy
\def\iccvPaperID{10080} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\title{Real-time Monocular Full-body Capture in World Space via Sequential Proxy-to-Motion Learning}


\author{Yuxiang Zhang, Hongwen Zhang\footnote[1]{} , Liangxiao Hu, Hongwei Yi, Shengping Zhang, Yebin Liu\footnote[1]{}\\
 Tsinghua University \quad
 Harbin Institute of Technology \\
 Max Planck Institute for Intelligent Systems
} 
\ificcvfinal\thispagestyle{empty}\fi

\begin{document}

\maketitle
\footnotetext[1]{Corresponding authors.}
\begin{abstract}
Learning-based approaches to monocular motion capture have recently shown promising results by learning to regress in a data-driven manner. However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time full-body capture while being accurate in world space. In this work, we contribute a sequential proxy-to-motion learning scheme together with a proxy dataset of 2D skeleton sequences and 3D rotational motions in world space. Such proxy data enables us to build a learning-based network with accurate full-body supervision while also mitigating the generalization issues. For more accurate and physically plausible predictions, a contact-aware neural motion descent module is proposed in our network so that it can be aware of foot-ground contact and motion misalignment with the proxy observations. Additionally, we share the body-hand context information in our network for more compatible wrist poses recovery with the full-body model. With the proposed learning-based solution, we demonstrate the first real-time monocular full-body capture system with plausible foot-ground contact in world space. More video results can be found at our project page: \href{https://liuyebin.com/proxycap}{https://liuyebin.com/proxycap}.

\end{abstract}
 \section{Introduction}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{imgs/teaser3.png}
    \caption{We demonstrate that our real-time monocular full-body capture system can produce accurate human motions with plausible foot-ground contact in world space. }
    \vspace{-2mm}
    \label{fig:descent_compare}
\end{figure}
Capturing full-body motions from monocular videos is an essential technology for various applications such as gaming, VR/AR, sports analysis, \etc.
One ultimate goal is to achieve real-time full-body capture while being accurate and physically plausible in world space.
Despite the recent advancements, this task is still far from being solved.


Compared with the optimization-based methods~\cite{sigal2008combined,guan2009estimating,bogo2016keep,lassner2017unite,huang2017towards,SMPL-X:2019,yi2022mover}, learning-based approaches~\cite{kanazawa2018end,kolotouros2019learning,zhang2021pymaf,pymafx2022} can predict 3D body motions directly, which are pretty efficient and have become increasingly popular.
As data-driven solutions, the accuracy of learning-based approaches is bounded by the quality of supervision, \ie, the motion annotations.
Meanwhile, their robustness heavily relies on the diversity of training data.
Since the motion capture datasets~\cite{ionescu2014human3, mehta2017monocular, sigal2010humaneva,huang2022capturing} are typically created with marker-based or multi-view systems and are limited in the diversity of body appearances and backgrounds, numerous efforts have been dedicated to enriching such data by generating pseudo labels~\cite{kolotouros2019learning,joo2021exemplar,moon2020neuralannot} for in-the-wild datasets.
Though these pseudo labels become increasingly aligned with the images, they lack accurate global motions due to the simplified camera assumptions and depth ambiguity.
To tackle this limitation, researchers have also created synthetic data~\cite{patel2021agora,bazavan2021hspace} by rendering human models with controllable cameras.
Despite the accurate motions and camera parameters of such synthetic data, there are distinct domain gaps between the real-world images and the rendered ones.



For physically plausible motion capture, there have been several attempts~\cite{rempe2021humor,yuan2021glamr} to correct the motion results in the world space via physics-based trajectory optimization~\cite{yuan2021glamr}.
However, these solutions typically require intensive computational resources, making them unsuitable for real-time applications.
Recent state-of-the-art solutions~\cite{yuan2021simpoe,luo2022embodied} alleviate this issue by incorporating the physics simulation in the learning process via reinforcement learning.
Despite their effectiveness, these methods typically require the modeling of a known 3D scene~\cite{yuan2021simpoe,luo2022embodied}.




To handle the above challenges, we propose a learning-based full-body motion capture method with real-time performance and plausible physical contact with the ground plane.
To this end, we start from the training data and propose a simple yet effective data generation strategy.
Our key insight is to leverage the existing large-scale motion sequence data, \ie, AMASS~\cite{mahmood2019amass}, and synthesize sequential proxy-to-motion pairs with virtual camera settings.
More specifically, we adopt the 2D skeleton sequences as proxy representations and associate them with the 3D rotational motion sequences in world space.
Such a data generation strategy addresses the limitation of previous datasets in the aspects of scale, accuracy, and generalization:
i) the data can be synthesized at a large scale and high quality as both body motions and virtual camera settings are accurate in world space;
ii) as one of the well-studied representations, 2D skeletons can be estimated in a pretty accurate and robust manner for existing state-of-the-art toolkits, which mitigates the generalization issue of previous synthetic datasets;
iii) The proxy data can be synthesized individually and then integrated to form a full-body proxy dataset by combining hand gestures and facial expressions with body motions.
Moreover, the proxy sequences also help handle the depth ambiguity compared to previous work~\cite{song2020human} using the proxy representations of a single frame.



Though the proposed proxy dataset can be more accurate and large-scale, it remains challenging for regression-based networks to learn physically plausible motions from proxy data. 
To address this, we propose a network architecture that takes a proxy sequence as input and produces 3D motions in world space.
Our network first predicts coarse 3D motions and then refines them to be more accurate and physically plausible. 
Specifically, we introduce a contact-aware neural motion descent module for iterative motion refinement.
In this module, we explore a cross-attention layer by using the contact status, proxy misalignment, and the current motion state as the query, key, and value.
By doing so, the neural descent module can produce more accurate motion updates based on foot-ground contact and motion misalignment with the proxy observations.


Our solution is well-suited for full-body motion capture by leveraging the full-body proxy dataset.
For better body-hand compatibility, our network further incorporates the context information from the hand part to predict more compatible wrist poses in the full-body model.
As a learning-based solution, our method for full-body motion prediction can be trained end-to-end and run in real-time during inference. Our contributions can be summarized as follows:
\begin{itemize}[leftmargin=*]
\itemsep0em 
    \item To overcome the accuracy and generalization issues, we adopt 2D skeleton sequences as proxy representations and synthesize proxy data from accurate 3D rotational motions in world space. The proxy data of different body parts can also be integrated as one with full-body motions.
\item For accurate and physically plausible predictions, we propose a contact-aware neural motion descent module so that our network can be aware of foot-ground contact and motion misalignment with the proxy observations.
\item For full-body motion capture, we leverage the body-hand context information in our network so that the wrist poses can be more compatible with the full-body model.
\item We demonstrate a real-time monocular full-body capture with plausible foot-ground contact in world space.
\end{itemize}


















 

\section{Related Work}

Monocular motion capture has been an active research field in recent years.
We give a brief review of the works related to ours and refer readers to~\cite{tian2022recovering} for a more comprehensive survey.


\textbf{Motion Capture Datasets.} Existing motion capture datasets are either captured with marker-based~\cite{sigal2010humaneva, ionescu2014human3} or marker-less~\cite{peng2021neural, yu2020humbi,zhang20204d,zhang2021lightweight} systems.
Due to the requirement of markers or multi-view settings, the diversity of these datasets is limited in comparison with in-the-wild datasets.
To enrich the motion datasets, numerous efforts~\cite{kolotouros2019learning,joo2021exemplar,moon2020neuralannot,muller2021self, sengupta2020synthetic} have been dedicated to generating pseudo-ground truth labels with better alignment in the image space but do not consider the motion in world space.
On the other hand, researchers have also resorted to using synthetic data~\cite{varol2017learning,su2022virtualpose, patel2021agora} by rendering human models with controllable viewpoints and backgrounds.
However, such synthetic datasets are either too expensive to create or have large domain gaps with real-world images. 


\textbf{Proxy Representations for Human Mesh Recovery.} Due to the lack of annotated data and the diversity of human appearances and backgrounds, learning accurate 3D motions from raw RGB images is challenging even for deep neural networks.
To alleviate this issue, previous approaches have exploited the different proxy representations, including silhouettes~\cite{pavlakos2018learning, varol2018bodynet}, 2D/3D landmarks~\cite{pavlakos2018learning,sun2019human, tung2017self, moon2020i2l, moon2020pose2pose,song2020human,ma20233d,li2021hybrik}, segmentation~\cite{kocabas2021pare, omran2018neural, rueegg2020chained, kocabas2021pare}, and IUV~\cite{zhang2020learning, xu2019denserac}.
These proxy representations can provide guidance for the neural network and hence make the learning process easier.
However, the proxy representations simplify the observation and introduce additional ambiguity in depth and scale, especially when using proxy representations in a single frame~\cite{song2020human,zhang2020learning, xu2019denserac}.
In this work, we alleviate this issue by adopting 2D skeleton sequences as proxy representations and propose to generate proxy data with accurate motions in world space.



\textbf{Full-body Motion Capture.} Recent state-of-the-art approaches~\cite{kocabas2021pare,zhang2021pymaf} have achieved promising results for the estimation of body-only~\cite{kocabas2021pare,zhang2021pymaf}, hand-only~\cite{li2022interacting}, and face-only~\cite{DECA_2020} models.
By combining the efforts together, these regression-based approaches have been exploited for monocular full-body motion capture.
These approaches~\cite{choutas2020monocular,rong2020frankmocap,zhou2021monocular,feng2021collaborative,moon2022Hand4Whole,pymafx2022} typically regress the body, hands, and face models by three expert networks and integrate them together with different strategies.
For instance, PIXIE~\cite{feng2021collaborative} learns the integration by collaborative regression, while PyMAF-X~\cite{pymafx2022} adopts an adaptive integration strategy with elbow-twist compensation to avoid unnatural wrist poses.
Despite the progress, it remains difficult for existing solutions to run at real-time while being accurate in world space.
In this work, we achieve real-time full-body capture with plausible foot-ground contact by introducing new data generation strategies and novel network architectures. 









\textbf{Neural Decent for Motion Capture.} Traditional optimization-based approaches~\cite{bogo2016keep} typically fit 3D parametric models to the 2D evidence but suffer from initialization sensitivity and the failure to handle challenging poses.
To achieve more efficient and robust motion prediction, there are several attempts to leverage the learning power of neural networks for iterative refinement.
HUND~\cite{zanfir2021neural} proposes a learning-to-learn approach based on recurrent networks to regress the updates of the model parameters. 
Song \etal~\cite{song2020human} propose the learned gradient descent to refine the poses of the predicted body model.
Similar refinement strategies are also exploited in PyMAF~\cite{zhang2021pymaf} and LVD~\cite{corona2022learned} by leveraging image features as inputs.
In our work, we propose a contact-aware neural decent module and exploit the usage of cross-attention for more effective motion updates.


\textbf{Physical Plausibility of Motion Capture.} Though existing monocular motion capture methods can produce well-aligned results, they may still suffer from the artifacts such as ground penetration and foot skating.
For more physically plausible reconstruction, previous works~\cite{kocabas2021spec,yuan2021glamr} have made attempts to leverage more accurate camera models during the learning process.
To encourage proper contact of human meshes, Rempe \etal~\cite{rempe2020contact} propose a physics-based trajectory optimization to learn the body contact information explicitly.
HuMoR~\cite{rempe2021humor} introduces a conditional VAE to learn a distribution of pose changes in the motion sequence, providing a motion prior for more plausible human pose prediction.
LEMO~\cite{zhang2021learning} learns the proposed motion smoothness prior and optimizes with the physics-inspired contact friction term.
Despite their plausible results, these methods typically require high computation costs and are unsuitable for real-time applications.
For more effective learning of the physical constraints, there are several attempts~\cite{yuan2021simpoe,luo2022embodied} to incorporate the physics simulation in the learning process via reinforcement learning.
However, these methods~\cite{yuan2021simpoe,luo2022embodied} typically depend on 3D scene modeling due to the physics-based formulation.
In our work, we achieve real-time capture with plausible foot-ground contact in world space by designing novel networks and leveraging accurate motion supervision from the proxy data.






 \section{Proxy Data Generation}
To address the precision and generalization challenges associated with extant datasets, we leverage 2D skeleton sequences as proxy representations and synthesize sequential proxy-to-motion data utilizing precise 3D rotational motions in world space. In this section, we begin by presenting the camera and motion decoupling approach in Section \ref{subsec:cam_decouple}. Subsequently, we introduce the synthetic data generation process in Section \ref{subsec:synthetic_data_generation}.







\subsection{Camera and Motion Decoupling}
\label{subsec:cam_decouple}





The traditional world-to-image projection typically involves transferring the world coordinates of the object to the camera space and then projecting them to the image plane.
To simplify the process, previous methods~\cite{videopose3d2020, poseformer2021, PoseTriplet2022,kanazawa2018end, kolotouros2019learning, zhang2021pymaf} directly regress human poses in the camera coordinate system and feed them to a projection function.
However, such a simplified process introduces an ambiguous problem applying the same projection function to different scenes.
Motivated by previous work~\cite{kocabas2021spec, 2022Ray3D}, we address this issue by decomposing the camera pose and the human motion, which allows us to predict the camera pose and the human motion in world space and calculate the foot-ground contact more directly.



In the following, we adopt the classical pinhole camera model to describe our camera settings.
As shown in Fig.~\ref{fig:cam_model}, the camera is fixed along the z-axis and positioned at the height of  above the ground.
The camera orientation is set to face squarely towards the front to reduce the degree of camera pose freedom with only two parameters of pitch  and roll .
In this setting, the camera rotation  and transition  can be calculated as following:


For body motion in world space, it can be represented with the rotational parameters  and global transition . 
With accurate , the body model can step on the ground  with plausible foot-ground contact.
Let the parameters of body motions in camera space be , their relationship with the world-space motion can be written as , where  denotes the body model.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{imgs/pipeline9.png}
    \caption{Illustration of the proposed method. 
    Our method takes the estimated 2D skeleton poses of the body and hand as inputs and recovers the 3D motions with plausible foot-ground contact in world space. 
    }
\label{fig:overview_pipeline}
    \vspace{-1mm}
\end{figure*}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{imgs/cam_model.png}
    \caption{Illustration of the camera pose and human motion decoupling in world space to obtain physical priors.}
\label{fig:cam_model}
    \vspace{-1mm}
\end{figure}















\subsection{Data Synthesis and Integration}
\label{subsec:synthetic_data_generation}
In our method, we adopt the 2D skeleton sequences as the proxy representation.
To generate proxy-to-motion pairs with accurate human kinematics, we synthesize proxy sequences based on the existing motion datasets with virtual cameras.
In the following, we describe the synthesis and integration of different types of labels in our proxy data, including body motions, hand gestures, and contact labels.


\noindent \textbf{Body proxy data.} We adopt the motion sequences from the AMASS dataset~\cite{mahmood2019amass} to generate proxy-to-motion pairs for the body part.
The AMASS dataset is a large-scale body motion sequence dataset that comprises 3772 minutes of motion sequence data, featuring diverse and complex body poses.
We downsample the motion data to 60 frames per second, resulting in 9407K frames.

\noindent \textbf{Integration with hand gestures.} Since the hand poses in the AMASS dataset are predominantly static, we augment the proxy data with hand poses from the InterHand~\cite{moon2020interhand2} dataset, which contains 1361K frames of gesture data captured at 30 frames per second.
We employ Spherical Linear Interpolation (Slerp) to upsample the hand pose data to 40, 50, and 60 fps and randomly integrate them with the body poses in the AMASS motion sequences.


\noindent \textbf{Integration with contact label.} To generate the contact label, we follow LEMO~\cite{lemo} and select 6 vertices on the foot of the SMPL and calculate the continuous contact indicators as follows:

where  and  denote the velocity in XY-plane and the height to the ground of the selected vertex.  and  is setting to  and , the hyper-parameter  and  is set to  and .
The resulting contact label can be directly integrated into the proxy data.

\noindent \textbf{Camera setting.} We adopt the same camera setup used in existing public datasets~\cite{ionescu2014human3,vonMarcard2018} for virtual camera settings.
Specifically, we set the fov from  to  and sampled the camera pose from: , , to ensure variability in camera viewpoints.
Meanwhile, we will apply a random rotation  around the z-axis to human and apply a random y-axis displacement  to simulate all cases that the person is near or far to the camera to augment the data.
We generate pseudo 2D skeleton annotations by adding Gaussian noise  to the 3D joints of the body model and then project them to the image plane.

















 \section{Method}
\label{sec:human_motion_recovery}


In our approach, the skeleton poses are first detected from a monocular video and then fed into our network to produce 3D motions in world space, as illustrated in Fig.~\ref{fig:overview_pipeline}.
In the following, we describe the proposed sequential proxy-to-motion learning scheme to achieve accurate and physically plausible motion prediction in world space.


\subsection{Sequential Full-body Motion Recovery}
\label{sec:temporal_motion_convolution}
At the first stage of the proposed scheme, the skeleton sequences are processed by temporal encoders and then fed into regressors for the predictions of full-body motion in world space.
Specifically, the temporal encoders are used to extract features from sequential 2D skeletons of the body and hand parts.
Following previous baseline~\cite{videopose3d2020}, we build these encoders upon temporal dilated convolutional networks.
Moreover, inspired by previous skeleton-based 3D human pose estimation~\cite{Moon_2019_ICCV_3DMPPE, videopose3d2020, poseformer2021, PoseTriplet2022, 2022Ray3D}, we adopt two separate branches in the body-specific regressors to learn the global trajectory and root-relative body motions.
Specifically, the trajectory branch learns the camera parameters  and the global human transition , while the motion branch learns the shape parameter  and pose parameter  of the body model.
Separating these two prediction tasks helps to decouple the global and local motions and hence produce more accurate motions in world space.



For better body-hand compatibility, 
we exploit the cross-attention mechanism to facilitate the motion context sharing during the body and hand recovery.
Specifically, we first obtain the initial body and hand features from the temporal encoders and split them as the Query , Key , and Value .
Then, these features will be updated by the cross-attention layer as follows: 

The updated body and hand features  will then be fed into the corresponding regressors to predict pose parameters.
We demonstrate that the fusion strategy can make our regressors benefit each other to produce more comparable wrist poses in the full-body model.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/feedback_net4.png}
    \caption{Illustration of the proposed cross-attention for neural motion descent. 
}
    \vspace{-5mm}
    \label{fig:feedback_net}
\end{figure}


\subsection{Contact-aware Neural Motion Descent}
\label{sec:contact_neural_motion_descent}

At the second stage of the proposed scheme, the coarse motion predictions will be refined to be more accurate and physically plausible with our proposed contact-aware neural motion descent module.
This module takes the foot-ground contact and misalignment status as input and produces updating human parameters during iterations.


To refine the predictions, our network needs to perceive the misalignment and the foot-ground contact status of the currently predicted motions , where the  and  denote the 3D joints and vertices of the body model at the -th iteration. 
Since we assume the camera is steady, the estimated motion parameters  in Sec.~\ref{sec:temporal_motion_convolution} should be transformed to the world space with a reference camera pose as the initial predictions . In practice, we take the predicted camera pose of the first frame as the reference camera pose and then transform subsequent estimated human motions accordingly.

For the misalignment status, we can project the 3D joints on the image plane and calculate the difference between the reprojected 2D joints and the proxy observations: .
For the contact status, we calculate the foot-ground contact based on the vertex velocity  in XY-plane and the distance  to the Z-ground plane.
Moreover, we also leverage the temporal features from inputs 2D skeletons to predict the contact labels , which will be used as an indicator to mask the foot-ground contact.
Then, the contact status of the current predictions can be denoted as , where  denotes the mask operation.





After obtaining the contact and misalignment status, we feed them into the neural motion descent module for motion updates.
As shown in Fig.~\ref{fig:feedback_net}, the descent module takes the two groups of tensors as input: 
i) the state group includes the current SMPL parameters , camera pose  and the sequential motion context ;
ii) the deviation group includes the current misalignment status  and contact status .
A straightforward solution would be using an MLP to process these two groups of tensors.
However, the values of these two groups exhibit significant differences.
For instance, the values of the state tensors change smoothly while the values of the deviation tensors may change rapidly along with the misalignment and contact status.
Simply concatenating them as inputs introduce difficulty in the learning process.
Note that the magnitude of the deviation tensors is highly correlated with the parameter updates.
When the body model is well-aligned without foot skating or ground penetration, the values of the deviation tensors are almost zero, indicating that the refiner should output zeros to prevent further changes in the pose parameters.
Otherwise, the refiner should output larger update values for motion adjustments.
To leverage such a correlation property, we exploit a cross-attention module to build a more effective architecture.
As shown in Fig.~\ref{fig:feedback_net}, two fully-connect layers are leveraged to process the tensors of the state and deviation groups and produce the Query, Key, and Value for the cross-attention module.
In this way, our contact-aware neural motion descent module can effectively learn the relationship between the state and deviation groups and hence produce more accurate motion updates.
Moreover, the sequential motion context  is also leveraged in our neural descent module to mitigate the depth uncertainty and improve the motion predictions.



Compared with previous work~\cite{rempe2021humor, zhang2021learning,song2020human, zanfir2021neural}, the proposed contact-aware neural motion descent module offers the advantage of freeing us from the heavy cost of explicit gradient calculations or the manual tuning of hyper-parameters during testing.
Furthermore, the module is capable of learning human motion priors with contact in world space from our synthetic dataset, which provides a more suitable descent direction and step to escape the local minima and achieve faster convergence.















\subsection{Loss Function}
\label{sec:loss_function}
In our solution, the full-body motion recovery module and the contact-aware neural motion descent module are trained separately.
Benefiting from the proxy-to-motion learning, the ground-truth full-body pose  and human body shape  can be obtained for supervision from our synthetic dataset.
Overall, the objective function of motion recovery can be written as follows:


Specifically,  involves 3D MPJPE loss and 3D trajectory L1 loss while  is the projected 2D MPJPE loss.
 represents L1 loss between the estimated human pose, shape and camera pose to our synthetic ground truth.
 is adopted from ~\cite{zeng2021smoothnet} by penalizing the velocity and acceleration between the estimation and the ground truth.
For the neural descent module, the objective loss can be written as:

where  is the iteration time and  is the decay ratio to emphasize the last iteration.
We set  in practice.
 involves the error of trajectory drifting, foot floating, or ground penetration.  refers to the loss between the predicted contact label to the ground truth.




 \section{Experiments}
In this section, we evaluate the effectiveness of our method on both the synthetic dataset and the public datasets.
We also conduct ablation studies to demonstrate the effectiveness of each part and report the computational complexity of each module in our real-time system.




\addtolength{\tabcolsep}{-5pt}
\begin{table}[t]
  \centering
  \footnotesize	
  \caption{Quantitative comparison with state-of-the-art methods on the Human3.6M dataset.  indicates that the input 2D pose is detected by the cascaded pyramid network (CPN), and  denotes the ground truth 2D pose input.}
    \begin{tabular}{cl|cc|cc}
    \toprule
    \multicolumn{2}{c|}{Method} & Ground-contact & Full-body & MPJPE & P-MPJPE\\
    \midrule
    \multirow{6}[2]{*}{\begin{sideways}Skeleton\end{sideways}} & PoseFormer~\cite{poseformer2021}  & \XSolidBrush & \XSolidBrush  & \textbf{31.3}  & - \\
       & PoseFormer~\cite{poseformer2021}   & \XSolidBrush & \XSolidBrush    & 44.3  & 34.6 \\
       & VideoPose3D~\cite{videopose3d2020}  & \XSolidBrush & \XSolidBrush  & 37.2 & \textbf{27.2}  \\
       & VideoPose3D~\cite{videopose3d2020}  & \XSolidBrush & \XSolidBrush   & 46.8  & 36.5 \\
& PoseTriplet~\cite{PoseTriplet2022}   & \XSolidBrush & \XSolidBrush  & 78.0  & 51.8 \\
       & Ray3D~\cite{2022Ray3D}  & \XSolidBrush & \XSolidBrush   & 34.4  & - \\
    \midrule
        \multirow{7}[2]{*}{\begin{sideways}Single Frame\end{sideways}} 
& I2L-MeshNet~\cite{moon2020i2l}  & \XSolidBrush & \XSolidBrush & \textbf{55.7}  & 41.1 \\
        & LearnedGD~\cite{song2020human}& \XSolidBrush & \XSolidBrush  & -     & 56.4 \\
          & HUND~\cite{zanfir2021neural}  & \XSolidBrush & \Checkmark  & 69.5  & 52.6 \\
          & Pose2Mesh~\cite{choi2020pose2mesh}  & \XSolidBrush & \XSolidBrush & 64.9  & 46.3 \\
          & HMR~\cite{kanazawa2018end}  & \XSolidBrush & \XSolidBrush  & 88.0  & 56.8 \\
          & SPIN~\cite{kolotouros2019learning}  & \XSolidBrush & \XSolidBrush   & 62.5  & 41.1 \\
& PyMAF-X~\cite{pymafx2022}  & \XSolidBrush & \Checkmark   & 58.1  & \textbf{40.2} \\
    \midrule
    \multirow{9}[2]{*}{\begin{sideways}Temporal\end{sideways}} & Kanazawa \etal~\cite{kanazawa2019learning}& \XSolidBrush & \XSolidBrush  &  -  & 56.9 \\
          & Arnab \etal~\cite{arnab2019exploiting} & \XSolidBrush & \XSolidBrush  & 77.8  & 54.3 \\
          & DSD~\cite{sun2019human}  & \XSolidBrush & \XSolidBrush  & 59.1  & 42.4 \\
          & VIBE~\cite{kocabas2020vibe} & \XSolidBrush & \XSolidBrush & 65.9  & 41.5 \\
          & TCMR~\cite{choi2021beyond}  & \XSolidBrush & \XSolidBrush & 62.3  & 41.1 \\
          & SimPoE~\cite{yuan2021simpoe}  & \Checkmark & \XSolidBrush & 56.7  & 41.6 \\
          & DnD~\cite{li2022d}  & \Checkmark & \XSolidBrush & 52.5   & 35.5 \\
          & Zhou \etal~\cite{zhou2021monocular} & \XSolidBrush & \Checkmark   & 50.3  & - \\
          & FrankMocap~\cite{rong2020frankmocap} & \XSolidBrush & \Checkmark   & -  & - \\
& Ours   & \Checkmark & \Checkmark  & 53.9  & 42.4 \\
       & Ours   & \Checkmark & \Checkmark & \textbf{40.5}  & \textbf{32.3} \\
    \bottomrule
    \end{tabular}\vspace{-3mm}
  \label{tab:3dpose}\end{table}\addtolength{\tabcolsep}{5pt}


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{imgs/results.png}
    \caption{Visualization of reconstruction results across different cases in the (a) 3DPW~\cite{vonMarcard2018}, (b) EHF~\cite{SMPL-X:2019}, and (c) Human3.6M~\cite{ionescu2014human3}  datasets and (d,e,f) our captured sequences .}
    \label{fig:results}
    \vspace{-2mm}
\end{figure*}

\subsection{Comparison with the State of the Art}
We evaluate our method on the public datasets, Human3.6M~\cite{ionescu2014human3}, 3DPW~\cite{vonMarcard2018}, EHF~\cite{SMPL-X:2019} and our synthetic dataset.
Tab.~\ref{tab:3dpose} shows the comparison results of our method with state-of-the-art ones.
Note that the methods reported in Tab.~\ref{tab:3dpose} include skeleton-based methods and mesh-based methods.
\begin{table}[ht!]
\caption{The action-wised comparison of different setups on the Human3.6M dataset.}
\centering
\begin{tabular}{c|cc|cc}
\toprule
\multicolumn{1}{c}{\multirow{2}[4]{*}{Method}} & \multicolumn{2}{c}{GT keypoints} & \multicolumn{2}{c}{CPN Detection} \\
\cmidrule{2-5} \multicolumn{1}{c}{}   & MPJPE  & \multicolumn{1}{c}{P-MPJPE}  & MPJPE & P-MPJPE \\ \midrule
In-door  & 48.2 & 37.9  &59.1 & 45.7\\
Synth  & 55.0 & 40.6  & 65.8 &  49.6\\
Fusion & 44.2 & 34.5  & 54.6 & 43.3 \\
Fusion+ND  &  \textbf{40.5} &  \textbf{32.3} & \textbf{53.9} & \textbf{42.4}\\
\bottomrule
\end{tabular}
\vspace{-3mm}
\label{tab:h36m_action}
\end{table}
Although the skeleton-based methods achieve higher MPJPE/P-MPJPE scores, they are limited in the ability to capture vertex-to-ground contact information and human kinematics.
Our algorithm yields comparable results to recent state-of-the-art mesh-based methods and presents several advantages such as real-time efficiency, full-body motion capture capability and plausible ground-contact constraints.
As shown in Fig.~\ref{fig:results}, we demonstrate that our method generalizes well in three public datasets and our captured sequences.








\subsection{Ablation Studies}









In this section, we conduct ablation studies using various settings to validate each component of our proposed method.
To validate the effectiveness of the proposed synthetic dataset, we adopt our network without the neural motion descent module as the Baseline method.
As reported in Tab.~\ref{tab:h36m_action}, we conduct ablation experiments with four distinct settings: ``In-door'': training the Baseline method on Human3.6M only; ``Synth'': training the Baseline method on the proposed synthetic datasets only; ``Fusion'':training the Baseline method on both the Human3.6M and synthetic datasets; ``Fusion+ND'': training our completed solution with the neural motion descent module on both datasets. The fusion training schedule demonstrates superior accuracy compared to training solely on the Human3.6M or synthetic dataset, as observed in both CPN-detected 2D inputs and ground truth (GT) 2D inputs. The reason is that training solely on Human3.6M leads to overfitting due to the limited motion diversity, while training solely on the synthetic dataset may result in a domain gap between the pseudo motion pair and dataset annotations. Furthermore, we present the results of our complete pipeline with the neural descent module, which shows improved performance.



\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{imgs/gp_ff4.png}
    \caption{The ablation study of foot floating (FF) and ground penetration (GP). We vary the threshold from 1cm to 3cm to calculate the corresponding FF and GP metrics. }
\label{fig:contact}
\end{figure}


\begin{table}[th!]
  \centering
  \caption{The ablation study on our synthetic dataset.}
    \begin{tabular}{ccccccc}
    \toprule
    Temp & Desc & Body & Hand & Global & FF & GP\\
    \midrule
    \XSolidBrush & \XSolidBrush &  42.3  & 11.6 & 60.0  & 6.69 & 6.71 \\
    \Checkmark & \XSolidBrush & 36.2  & 10.3 & 54.8 & 8.3 &  4.1 \\
    \XSolidBrush & \Checkmark  & 39.2  & 9.6 & 52.3 & 0.081 &  0.23 \\
    \midrule
    \Checkmark  & \Checkmark   & \textbf{36.1 } & \textbf{9.1} & \textbf{47.4} & \textbf{0.058} &  \textbf{0.086}\\
    \bottomrule
    \end{tabular}\label{tab:synth_ablation}\end{table}

\begin{table}[ht!]
\caption{Time costs of each module in our pipeline.}
\centering
\begin{tabular}{ccc}
\hline
Network & Input &Speed \\ \hline
Body Crop Net & &  1ms \\
Body Landmark Net &  & 2.5ms \\ 
Hand Crop Net &  & 1ms \\
Hand Landmark Net &  & 0.8ms \\ 
Holistic Recovery Net &  & 3ms \\ 
Neural Descent Net &  & 10ms \\ \hline
\end{tabular}
\vspace{-3mm}
\label{tab:performance}
\end{table}

We also conduct ablation studies of the motion context sharing scheme (``Temp'') and the contact-aware neural motion descent module (``Desc'') in Tab.~\ref{tab:synth_ablation}. 
We report different settings with measurements on body MPJPE, hand MPJPE, global MPJPE (including trajectories), ground penetration~\cite{yuan2021simpoe} (GP), and foot floating (FF) for comparison.
Results show that both the motion context sharing scheme and the contact-aware neural motion descent module improve the performance. Evaluations on the FF metric and the GP metric are shown in Fig.~\ref{fig:contact}. Here, the FF metric is defined as the percentage of the frames with foot-ground distances far from a specific threshold.


Moreover, we present a qualitative comparison with the previous neural descent method~\cite{song2020human} and the representative full-body motion recovery method PyMAF-X~\cite{pymafx2022} in Fig.~\ref{fig:descent_compare}.
We demonstrate that our method can avoid unnatural body leaning and knee bending in Fig.~\ref{fig:descent_compare} (a) and prevent foot skating and ground penetration in Fig.~\ref{fig:descent_compare}(b).








\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{imgs/compare.png}
    \caption{Qualitative Comparison. (a) left: LearnedGD~\cite{song2020human}, right: Ours, (b) top: PyMAF-X~\cite{pymafx2022}, down: Ours.
}
    \vspace{-2mm}
    \label{fig:descent_compare}
\end{figure}

\subsection{Computational Complexity}
In this section, we compare the inference speed of our method.
Our real-time monocular full-body capture system can be implemented on a single Laptop (Intel i7-12800HX CPU, NVIDIA RTX 3080Ti GPU).
Specifically, for the 2D pose estimator, we leverage the off-the-shelf framework Mediapipe~\cite{lugaresi2019mediapipe} and re-implement it on half-precision arithmetic on the NVIDIA TensorRT platform.
We report the inference time of each module in Tab.~\ref{tab:performance}. 

















 \section{Conclusion}
In this paper, we present a real-time full-body motion capture method with physically plausible foot-ground contact in world space.
We leverage a sequential proxy-to-motion learning scheme and synthesize a proxy dataset of 2D skeleton sequences with accurate 3D rotational motions in world space.
For more accurate and physically plausible results, we propose a contact-aware neural motion descent module in our network.
Besides, we leverage the body-hand context information in our network so that the wrist poses can be more compatible with the full-body model.
We demonstrate a real-time monocular full-body capture system and show superior results to existing solutions with more plausible foot-ground contact in world space.


\paragraph{Limitations.}

Similar to previous work~\cite{song2020human}, it is difficult for our method to capture the body shape from sparse skeletons.
Meanwhile, though our method shows more plausible results in world space, the mesh-to-image alignment is slightly inferior to state-of-the-art image-based solutions~\cite{zhang2021pymaf,pymafx2022} especially when the camera setting differs from the training data.

 
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}
\end{document}