\documentclass{article}
\pdfoutput=1


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak



\newcommand{\ind}[1]{\mathbb{I}\left[{#1}\right]}
\newcommand{\GNN}{\mathrm{GNN}}
\newcommand{\RNN}{\mathrm{RNN}}

\newcommand{\real}{\mathbb{R}}
\newcommand{\second}[1]{\textit{\textbf{#1}}} 
\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{symbols}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{ctable}
\usepackage{pifont}\usepackage{capt-of}\usepackage{bbm}

\usepackage{svg}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} \usepackage{wrapfig}

\usepackage[font=small]{caption}




\definecolor{lg}{HTML}{8DC44F}
\definecolor{dg}{HTML}{38761d} \definecolor{ly}{HTML}{FFC718}
\definecolor{dy}{HTML}{bf9000} \definecolor{lr}{HTML}{cc0000} \definecolor{lb}{HTML}{1155cc} 





\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


\begin{document}

\title{Chirality Nets for Human Pose Regression}

\author{
Raymond A. Yeh\thanks{Indicates equal contribution.}, \;\;
Yuan-Ting Hu\textsuperscript{*}, \;\;
Alexander G. Schwing\\
Department of Electrical Engineering, University of Illinois at Urbana-Champaign\\
\texttt{\{yeh17, ythu2, aschwing\}@illinois.edu}
}

\maketitle

\begin{abstract}
We propose Chirality Nets, a family of deep nets that is equivariant to the ``chirality transform,'' \ie, the transformation to create a chiral pair. Through parameter sharing, odd and even symmetry, we propose and prove variants of standard building blocks of deep nets that satisfy the equivariance property, including fully connected layers, convolutional layers, batch-normalization, and LSTM/GRU cells. The proposed layers lead to a more data efficient representation and a reduction in computation by exploiting symmetry. We evaluate  chirality nets on the task of human pose regression, which naturally exploits the left/right mirroring of the human body. We study  three pose regression tasks: 3D pose estimation from video, 2D pose forecasting, and skeleton based activity recognition. Our approach achieves/matches state-of-the-art results, with more significant gains on small datasets and limited-data settings. 
\end{abstract} 


\section{Introduction}
\label{sec:intro}
Human pose regression tasks such as human pose estimation, human pose forecasting and skeleton based action recognition, have numerous applications in video understanding, security and human-computer interaction. For instance, collaborative virtual reality applications rely on accurate pose estimation for which significant advances have been reported in recent years.

Specifically, recent state-of-the-art approaches use supervised learning to address pose regression and employ deep nets. Input and output  of those nets depend on the task: inputs are typically 2D or 3D human pose key-points stacked into a vector; the output may represent human pose key-points for pose estimation or a classification probability for activity recognition. To improve accuracy of those tasks, a variety of deep net architectures have been proposed~\cite{martinez2017human, chao2017forecasting, hossain2018exploiting, lee2018propagating, pavllo20193d, si2018skeleton}, generally relying on common deep net building blocks, such as, fully connected, convolutional or recurrent layers. Unlike for image datasets, to enlarge the size of human pose datasets, a  reflection (left-right flipping) of the pose coordinates as illustrated in step (1) of \figref{fig:sym_prop} is not sufficient. The chirality of the human pose requires to additionally switch the labeling of left and right as illustrated in step (2) of \figref{fig:sym_prop}.

However, while this two-step data augmentation is conceptually easy to employ during training, we argue that even better accuracy is possible for human pose regression tasks if this pose symmetry is directly built into the deep net. In particular, if confronted with either of the poses illustrated on the left or right hand side of \figref{fig:sym_prop} the  output of a  deep net should be equivariant to the transformation, \ie, the output is also transformed in a ``predefined way.''  For example, if the network's output is also a human pose, the output pose should follow the same transformation. On the other hand, for an activity recognition task, the output probability should remain unchanged. The equivariant map, for pose estimation, is illustrated in \figref{fig:equi_prop} and we make the equivariance property more precise  later. 

To encode this form of equivariance for human pose regression tasks, we propose ``chirality nets.'' Specifically, the output of a chirality net is guaranteed to be equivariant \wrt a transformation composed of reflections and label switching. To build chirality nets, we develop 
chirality equivariant versions of  commonly used layers. Specifically, we design and prove equivariance for versions of fully connected, convolutional, batch-normalization, dropout, and LSTM/GRU layers and element-wise non-linearities such as tanh or soft-sign.
 The main common design principle for chirality equivariant layers is  odd and even symmetric sharing of model parameters. Hence, in addition to being equivariant, transforming a typical deep net into its chiral counterpart results in a reduction of the number of trainable parameters, and lower computation complexity due to the symmetry in the model weights.  We find a smaller number of trainable parameters reduces the sample complexity, \ie, the models need less training data.


We demonstrate the generalization and effectiveness of our approach on three pose regression tasks over four datasets: 3D pose estimation on the Human3.6m~\cite{h36m_pami} and HumanEva dataset~\cite{sigal2010humaneva}, 2D pose estimation on the Penn Action dataset~\cite{zhang2013actemes} and skeleton-based action recognition on Kinetics-400 dataset~\cite{kay2017kinetics}. Our approach achieves state-of-the-art results with guarantees on equivariance, lower number of parameters, and robustness in low-resource settings. 


 


\begin{figure}[t]
	\centering
	\includegraphics[height=3.0cm]{fig/pose_ops_v4.pdf}
	\vspace*{-0.2cm}
	\caption{
	Illustration of the chirality transformation. The transformation includes two operations, (1) a reflection of the pose, \ie, a negation of the x-coordinates; and (2) a switch of the left / right joint labeling. The ordering of the two operations are interchangeable. 
	}
	\label{fig:sym_prop}
	\vspace{-0.5cm}
\end{figure} 
\section{Related Work}
\label{sec:rel}
First we briefly review invariance and equivariance in machine learning and computer vision as well as human pose regression tasks.

\textbf{Invariant and equivariant representation.} 
Hand-crafted invariant and equivariant representations have been utilized widely in computer vision systems for decades, \eg, scale invariance of SIFT~\cite{lowe1999object}, orientation invariance of HOG~\cite{dalal2005histograms}, affine invariance of the Harris detector~\cite{mikolajczyk2004scale}, shift-invariant systems in image processing~\cite{vetterli2014foundations}, \etc.

These properties have also been adapted  to learned representations. A widely known property is the translation equivariance of convolutional neural nets (CNN)~\cite{lecun1999object}: through spatial or temporal parameter sharing, a shifted input leads to a shifted output. Group-equivariant CNNs extend the equivariance to rotation, mirror reflection and translation~\cite{cohen2016group} by replacing the shift operation with a more general set of transformations. Other representations for building equivariance into deep nets have also been proposed, \eg, the Symmetric Network~\cite{gens2014deep}, the Harmonic Network~\cite{worrall2017harmonic} and the Spherical CNN~\cite{cohen2018spherical}.


The aforementioned works focus on deep nets where the input are images. While related, they are not directly applicable to human pose. For example, a reflection with respect to the y-axis in the image domain corresponds to a permutation of the pixel locations, \ie, swapping the pixel intensity between each pixel's reflected counterpart. In contrast, for human pose, where the input is a vector representing the   human joints' spatial coordinates, a reflection corresponds to the \textit{negation of the value} for each of the joints  reflected dimension. 

The input representation of deep nets for human pose is more similar to pointsets. Prior work has explored building  permutation equivariant deep nets, \ie,  any permutation of input elements  results in the same permutation of output elements.

In~\cite{zaheer2017deep, qi2017pointnet}. Both works utilize parameter sharing to achieve permutation equivariance. Following these works, graph nets generalize the family of permutation equivariant networks and demonstrate success on numerous applications~\cite{scarselli2009graph, kipf2017semi, hamilton2017representation, gilmer2017neural, battaglia2018relational, kipf2018neural, Yeh_2019_CVPR, Liu_2019_CORL}. 

For human pose,  equivariance to \emph{all permutations} is too strong of a property. Recall, our aim is to build models equivariant to the chiral symmetry, which only involves \textit{a specific permutation}, \eg, the switch between left and right joints, shown in step (2) of \figref{fig:sym_prop}. 

Most relevant to our approach is work by~\citet{ravanbakhsh2017equivariance}. \citet{ravanbakhsh2017equivariance}  explore which type of equivariance can be achieved through parameter sharing. Their approach captures one specific permutation in the pose symmetric transform, but does not capture the negation from the reflection, shown in \figref{fig:sym_prop} step (1). In contrast, our approach considers both operations (1) and (2) jointly, which leads to a different formulation. Lastly, to the best of our knowledge,~\cite{ravanbakhsh2017equivariance} only discusses theoretically  the construction of equivariant networks. In this work, we design and implement a variety of building blocks for deep nets and demonstrate the benefits on a wide range of practical applications in human pose regression tasks. 


\textbf{Human pose applications.} For 3D pose estimation from images, recent approaches utilize a two-step approach: (1) 2D pose keypoints are predicted given a video; (2) 3D keypoints are estimated given 2D joint locations. The 2D to 3D estimation is formulated as a regression task via deep nets~\cite{pavlakos17volumetric, tekin2017learning, martinez2017simple, sun2017compositional, fang2018learning, pavlakos2018ordinal, yang20183d, luvizon20182d, hossain2018exploiting, lee2018propagating, pavllo20193d}. Capturing the temporal information is crucial and has been explored in 3D pose estimation~\cite{hossain2018exploiting,lee2018propagating} as well as in action recognition~\cite{tran2018closer,Hussein_2019_CVPR}, video segmentation\cite{hu_nips_2017,hu_eccv_2018a} and learning object dynamics~\cite{martinez2017human,minderer2019unsupervised}. Most recently, \citet{pavllo20193d} propose to use temporal convolutions to better capture the temporal information for 3D pose estimation over previous RNN based methods. They also performed train and test time augmentation based on the chiral-symmetric transformation. For test time augmentation, they compute the output for both the original input and the transformed input, using the average outputs as the final prediction.  In contrast to our work, we note that \citet{pavllo20193d} need to transform the output of the transformed input  back to the original pose. To carefully assess the benefits of chirality nets, in this work, we closely follow the experiment setup of \citet{pavllo20193d}. 

For 2D keypoint forecasting, we follow the setup of standard temporal modeling: conditioning on past observations to predict the future. To improve temporal modeling, recent works, have utilized different sequence to sequence models  for this task~\cite{martinez2017human, chao2017forecasting, chiu2019action}. In this work, we closely follow the experiment setup of \citet{chiu2019action}. 


For action recognition, skeleton based methods have been explored extensively recently~\cite{yan2018stgcn,zhang2018adding,li2018co,si2018skeleton} due to robustness to illumination changes and cluttered background. Here we closely follow the experimental setup of~\citet{yan2018stgcn}. 

 \begin{figure}[t]
	\centering
	\hspace{-0.5cm}
		\includegraphics[height=5.8cm]{fig/pose_equi_v4.pdf}\\
	\vspace*{-0.9cm}
	\caption{
	Illustration of chirality equivariance for the task of 2D to 3D pose estimation.}
	\label{fig:equi_prop}
	\vspace{-0.5cm}
\end{figure}
 
\section{Chirality Nets}
\label{sec:app}

In the following we first provide the  problem formulation for human pose regression, before defining   chirality nets,  equivariance and the chirality transform. Subsequently we discuss how to develop typical layers such as the fully connected layer, the convolution, \etc, which make up chirality nets. 

The Pytorch implementation and unit-tests of the proposed layers are part of the supplementary material. We have also included a short Jupyter notebook demo to illustrate the key concepts.

\subsection{Problem Formulation}
Chirality nets can be applied to regression tasks on  coordinates of joints for human pose related task, \ie, the input corresponds to 2D or 3D coordinates of human joints. For readability, we introduce the input and output representations for a single frame. Note that for our experiments we generalize chirality nets  to multiple frames by introducing a time dimension. 

We let $\rvx \in \mathbb{R}^{|J^{\tt in}|\cdot|D^{\tt in}|}$ denote the chirality net input, where $J^{\tt in}$ is the set of all joints and $D^{\tt in}$ is the dimension index set for an input coordinate. For example, $J^{\tt in} = \{\text{`right wrist'},  \text{`right shoulder'}, \ldots\}$ and $D^{\tt in} = \{0, 1\}$, for 2D  input joint coordinates.  Similarly, we let  $\rvy \in \mathbb{R}^{|J^{\tt out}|\cdot|D^{\tt out}|}$ refer to the chirality net output. Note that the dimension of the spatial coordinates at the input and output may be different, \eg, prediction from 2D to 3D. Also, the  number of joints may differ, \eg, when mapping between different key-point sets. 

For human pose regression, the task is to learn the parameters $\theta$ of a model $F_\theta$ by minimizing a loss function, $\cL(\theta) = \sum_{(\rvx, \rvy) \in \cD} \ell(F_\theta(\rvx), \rvy)$ over the training dataset $\cD$. Hereby, sample loss $\ell(F_\theta(\rvx), \rvy)$ compares prediction $F_\theta$ to ground-truth $\rvy$.

\subsection{Chirality Nets, Chirality Equivariance, and Chirality Transforms}\label{seq:eq_def}

Chirality nets exhibit chirality equivariance, \ie, their output is transformed in a ``predefined manner'' given that the chirality transform is applied at the input. Note that the input and output dimensions $D^{\tt in}$ and $D^{\tt out}$ may differ. To define this chirality equivariance, we hence  need to consider a pair of transformations, one for the input data, $\mathcal{T}^{\tt in}$, and one for the output data, $\mathcal{T}^{\tt out}$. The corresponding equivariance map is illustrated in \figref{fig:equi_prop} for the task of 2D to 3D pose estimation. Formally,  we say a function $F_\theta$ is chirality equivariant \wrt $(\mathcal{T^{\tt in}, T^{\tt out}})$ if 
$$\mathcal{T}^{\tt out}(F_\theta(\rvx)) = F_\theta(\mathcal{T}^{\tt in}(\rvx)) \;\; \forall \rvx \in \mathbb{R}^{|J^{\tt in}||D^{\tt in}|}.$$
To define the chirality transform on the input data, \ie, $\mathcal{T}^{\tt in}$, we split the set of joints $J^{\tt in}$ into \emph{ordered tuples} of ${J}^{\tt in}_{\tt l}$, ${J}^{\tt in}_{\tt r}$, and ${J}^{\tt in}_{\tt c}$, each denoting left, right and center joints of the input. 
Importantly, these tuples are sorted such that the corresponding left/right joints are at corresponding positions in the tuple.  We also split the dimension index set $D^{\tt in}$  into $D^{\tt in}_{\tt n}$ and $D^{\tt in}_{\tt p}:=  D^{\tt in}  \backslash D^{\tt in}_{\tt n}$,  indicating the coordinates to, or not to, negate.

For readability and without loss of generality, assume the dimensions of the input $\rvx$
follow the order of ${J}^{\tt in}_{\tt l}$, ${J}^{\tt in}_{\tt r}$, ${J}^{\tt in}_{\tt c}$, \ie,  $\rvx = [\rvx_{\tt l}, \rvx_{\tt r}, \rvx_{\tt c}]$. Within each vector $\rvx_{(\cdot)}$, we place the coordinates in the set $D^{\tt in}_{\tt n}$  before the remaining ones, \ie, $\rvx_{\tt l} = [\rvx_{\tt ln}, \rvx_{\tt lp}]$. 

Given this construction of the input $\rvx$, the reflection illustrated in step (1) of \figref{fig:sym_prop} is a matrix multiplication with a $(|{J}^{\tt in}||D^{\tt in}|) \times (|{J}^{\tt in}||D^{\tt in}|)$ diagonal matrix $T^{\tt in}_{\tt neg}$, defined as follows:
$$T_{\tt neg}^{\tt in} = {\tt diag (
[
{\bf -1}_{|{J}^{\tt in}_{\tt l}|\cdot |D^{\tt in}_{\tt n}|},  {\bf 1}_{|{J}^{\tt in}_{\tt l}|\cdot |D^{\tt in}_{\tt p}|}, 
{\bf -1}_{|{J}^{\tt in}_{\tt r}|\cdot |D^{\tt in}_{\tt n}|},  {\bf 1}_{|{J}^{\tt in}_{\tt r}|\cdot |D^{\tt in}_{\tt p}|}, 
{\bf -1}_{|{J}^{\tt in}_{\tt c}|\cdot |D^{\tt in}_{\tt n}|},  {\bf 1}_{|{J}^{\tt in}_{\tt c}|\cdot |D^{\tt in}_{\tt p}|}
])},$$
where ${\bf 1}_{K}$ indicates a vector of ones of length $K$. 

The switch operation illustrated in step (2) of \figref{fig:sym_prop} is a matrix multiplication with a permutation matrix of dimension $(|{J}^{\tt in}||D^{\tt in}|) \times (|{J}^{\tt in}||D^{\tt in}|)$, defined as follows:
$$T^{\tt in}_{\tt swi}  = \begin{bmatrix} 
\bf 0 & \mathbf{I}_{|{J}^{\tt in}_{\tt l}|\cdot |D^{\tt in}|} & \bf 0 \\
\mathbf{I}_{|{J}^{\tt in}_{\tt l}|\cdot |D^{\tt in}|}  & \bf 0 & \bf 0\\
\bf 0 & \bf 0 & \mathbf{I}_{|{J}^{\tt in}_{\tt c}|\cdot |D^{\tt in}|} 
\end{bmatrix},$$
where ${\bf I}_{K}$ denotes an identity matrix of size $K \times K$. 

Given those matrices, the chirality transform of the input $\mathcal{T^{\tt in}}(\rvx)$ is obtained via $\mathcal{T^{\tt in}}(\rvx) = T^{\tt in}_{\tt neg} T^{\tt in}_{\tt swi} \rvx$. The chirality transform of the output, $\mathcal{T^{\tt out}}$, is defined similarly, replacing ``${\tt in}$'' with ``${\tt out}$''.

In the following, we  introduce layers that satisfy the $(\mathcal{T^{\tt in}, T^{\tt out}})$ chirality equivariance property. This enables to construct a chirality net $F_\theta$, as the composition of equivariant layers remains equivariant.  Note that $(\mathcal{T^{\tt in}, T^{\tt out}})$ chirality equivariance can be specified separately for every deep net layer which provides additional flexibility. 
In the following we discuss how to construct  layers which satisfy chirality equivariance. 

\subsection{Chirality Layers}

\textbf{Fully connected layer.} 
A fully connected layer performs the mapping $\rvy = f_{\text{FC}}(\rvx ; W, b) := W\rvx + b$. 
We achieve equivariance through parameter sharing and odd symmetry:

$
W \!=\! \begin{bmatrix}
{\color{lb}
\begin{bmatrix}
W_{\tt ln, ln} & W_{\tt ln, lp} \\
W_{\tt lp, ln} & W_{\tt lp, lp}\\
\end{bmatrix}
} 
& 
{\color{lr}
\begin{bmatrix}
W_{\tt ln, rn} & W_{\tt ln, rp} \\
W_{\tt lp,rn} & W_{\tt lp, rp}\\
\end{bmatrix}
}
&
{\color{dy}
\begin{bmatrix}
W_{\tt ln, cn} & W_{\tt ln, cp} \\
W_{\tt lp, cn} & W_{\tt lp, cp}\\
\end{bmatrix}
}\\
{\color{lr}
\begin{bmatrix}
W_{\tt ln, rn} & -W_{\tt ln, rp} \\
-W_{\tt lp,rn} & W_{\tt lp, rp}\\
\end{bmatrix}
}
& 
{\color{lb}
\begin{bmatrix}
W_{\tt ln, ln} & -W_{\tt ln, lp} \\
-W_{\tt lp, ln} & W_{\tt lp, lp}\\
\end{bmatrix}
} 
&
{\color{dy}
\begin{bmatrix}
W_{\tt ln, cn} & -W_{\tt ln, cp} \\
-W_{\tt lp, cn} & W_{\tt lp, cp}\\
\end{bmatrix}
}\\
{\color{black}
\begin{bmatrix}
W_{\tt cn, ln} & W_{\tt cn, lp} \\
\bf 0 & W_{\tt cp, lp}\\
\end{bmatrix}
} 
& 
{\color{black}
\begin{bmatrix}
W_{\tt cn, ln} & -W_{\tt cn, lp} \\
\bf 0 & W_{\tt cp, lp}\\
\end{bmatrix}
}
&
{\color{dg}
\begin{bmatrix}
W_{\tt cn, cn} & \bf 0\\
\bf 0 & W_{\tt cp, cp}\\
\end{bmatrix}
}\\
\end{bmatrix}
$,
$b = \begin{bmatrix}
\begin{bmatrix}
\color{lr}
b_{\tt ln} \\ 
\color{lr}
b_{\tt lp}
\end{bmatrix} \\ 
\begin{bmatrix}
\color{lr}
-b_{\tt ln} \\ 
\color{lr}
b_{\tt lp} 
\end{bmatrix} \\ 
\begin{bmatrix} \bf 0 \\ 
b_{\tt cp}\end{bmatrix}\\
\end{bmatrix}\!.
$

We  color code the shared parameters using identical colors. Each $W_{(\cdot), (\cdot)}$ denotes a matrix, where the first and the second subscript characterize the dimensions of the output and the input. For example, $W_{{\tt ln},{\tt rp}}$ computes the output's left ($\tt l$) joint's negated ($\tt n$) dimensions, from the input's right ($\tt r$) joint's non-negated, \ie, positive ($\tt p$), dimensions. Note that $W_{{\tt ln},{\tt rp}}$ is a matrix of dimension $|J^{\tt out}_{\tt l}| \cdot |D_{\tt n}^{\tt out}| \times |J^{\tt in}_{\tt r}| \cdot |D_{\tt p}^{\tt in}|$. We refer to this layer as the chiral fully connected layer. 

\textbf{1D convolution layers~\cite{waibel1995phoneme, lecun1999object}.}
Pose symmetric 1D convolution layers can be based on fully connected layers. A 1D convolution is a fully connected layer with shared parameters across the time dimension, \ie, at each time step the computation is the sum of fully connected layers over a window:
$$\rvy_t = \sum\limits_{\tau} W_{\tau}\rvx_{t-\tau} + b = \sum\limits_{\tau}  f_{\text{FC}}(\rvx_{t-\tau}; W_{\tau}, b).$$ 
Consequently, we  enforce equivariance at each time step by employing the  symmetry pattern of fully connected layers at each time slice. 

\textbf{Element-wise nonlinearities.} Nonlinearities are applied element-wise and do not contain parameters.  These operations maintain the input dimension, therefore, $\mathcal{T^{\tt out}}$ and $\mathcal{T^{\tt in}}$ are identical. A nonlinearity $f$ that is an odd function, \ie, $ f(-x) = -f(x)$, 
such as tanh,  hardtanh, or soft-sign satisfies the equivariance property. See the following proof:
\bea\nonumber
\mathcal{T^{\tt out}}( f(\rvx)) =& T^{\tt out}_{\tt neg}T^{\tt out}_{\tt swi} (f(\rvx)) \overset{\text{elementwise } f}{=}  T^{\tt out}_{\tt neg}f(T^{\tt out}_{\tt swi}\rvx)) \\ 
\overset{\text{odd func. } f}{=}& f(T^{\tt out}_{\tt neg}T^{\tt out}_{\tt swi}  \rvx)
= f(\mathcal{T}^{\tt in}(\rvx)) \;\; \forall \rvx \in \mathbb{R}^{|J^{\tt in}||D^{\tt in}|}. \nonumber
\eea
\textbf{LSTM and GRU layers~\cite{hochreiter1997long, cho2014learning}.}

LSTM and GRU modules which satisfy chirality can be obtained from fully connected layers. 

However, na\"ively setting all matrix multiplies within an LSTM to satisfy the equivariance property will not lead to an equivariant LSTM because gates are elementwise \textit{multiplied} with the cell state. If both  gate and cell  preserve the negation then the product will not. Therefore, we  change the weight sharing scheme for the gates. We set $D_{n}^{\tt out}$ for the gates to be the empty set, \ie,  the gates will be invariant to negation at the input, $T^{\tt in}_{\tt neg}$, but still equivariant to the switch operation, $T^{\tt in}_{\tt swi}$. With this setup, the product of the gates and the cell's output will preserve the sign, as the gates are invariant to negation and passed through a Sigmoid to be within the range of $(0,1)$.  GRU modules are modified  in the same manner. 



\textbf{Batch-normalization~\cite{ioffe2015batch}.}
A batch normalization layer performs an element-wise standardization, followed by an element-wise affine layer (with learnable parameters $\gamma$ and $\beta$).
For $\gamma$ and $\beta$, we follow the 
the principle applied to fully connected layers. 

Equivariance for $\mu$, and $\sigma$ is obtained by
computing the mean and standard deviation on the ``augmented batch'' and by keeping track of its running average. 

\textbf{Dropout~\cite{srivastava2014dropout}.}
At test time, dropout scales the input by $p$, where $p$ is the dropout probability. The equivariance property is satisfied because of  the associativity property of a scalar multiplication. 

\subsection{Reduction in model parameters, FLOPS, and training/test details}
\textbf{Model parameters.} Our model shares parameters between dimensions representing the left and right joints. For each layer, the number of parameters are reduced by a factor of 
$\frac{|(|J_{l}^{\tt in}| + |J_{\tt c}^{\tt in}|) \cdot (|J_{\tt l}^{\tt out}| + |J_{\tt c}^{\tt out}|)}{|J^{\tt in}|\cdot |J^{\tt out}|}$. Recall $|J^{\tt in}| = |J_{\tt l}^{\tt in}| + |J_{\tt r}^{\tt in}| + |J_{\tt c}^{\tt in}|$. The output dimension size is computed similarly.

\textbf{FLOPS.} Chirality nets  also have lower FLOPS.  Due to the symmetry, instead of multiplying and adding each of the elements independently, we  add the symmetric values first before applying a single multiplication per symmetric pair. 
{
Concretely, consider $\mathbf{w} = [w_1, w_1]$, $\mathbf{x} = [x_1, x_2]$, and their inner product $\mathbf{w}^{T}\mathbf{x}$. Instead of  computing $w_1\cdot x_1 + w_1\cdot x_2$, we exploit symmetry and use instead  $w_1 \cdot (x_1 + x_2)$, which removes one multiplication operation. 
}
This is a common speed up trick used in symmetric FIR filters~\cite{note1998implementing, yeh2016stable}. The number of multiplications reduces by a factor of $\frac{|J_{l}^{\tt in}| + |J_{c}^{\tt in}|}{|J^{\tt in}|}$. Additionally, baseline models utilize test-time augmentation, which requires two forward passes through the network for each input, whereas the proposed nets only use a single forward pass. 

\textbf{Training and test details.}  
During training it is important to apply the chirality transform for data-augmentation, \ie, with 50\% probability we apply $\mathcal{T}^{\tt in}$ and $\mathcal{T}^{\tt out}$ to  input and label. This  ensures that the mini-batch statistics match our assumption on the chirality, \ie, poses that form a chiral pair are both valid, which is important for the batch-normalization layer. 
Moreover, during training we use a standard dropout layer. While we could impose  dropped units to be chiral equivariant, we found this lead to over-fitting in practice. This is expected as imposing chirality on the added noise reduces the randomness. 
Importantly, during test no data-augmentation is performed and a single forward pass is sufficient to obtain an `averaged' result. 

 \begin{figure}
\centering
\begin{tabular}{c}
\includegraphics[width=\textwidth]{fig/illustration.pdf}\vspace{-0.3cm}\\
(a) \hspace{1.5in} (b) \hspace{1.5in} (c)\\
\end{tabular}
\vspace{-0.3cm}
\caption{
Illustration of  pose regression tasks: (a) 2D to 3D pose estimation;
(b) 2D pose forecasting; and (c)  skeleton-based action recognition.} 
\label{fig:task_ill}
\vspace{-0.4cm}
\end{figure} \section{Experiments}
\label{sec:exp}
{
We evaluate our approach on a variety of tasks, including 2D to 3D pose estimation, 2D pose forecasting, and skeleton based action recognition. For each task, we describe the dataset, metric, and implementation before discussing the results.
}



\begin{table}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{2pt}
\begin{tabular*}{\textwidth}{l|ccccccccccccccc|c}
\specialrule{.15em}{.05em}{.05em}
Approach & Dir. & Disc. & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WalkD. & Walk & WalkT. & Avg\\
\hline
\hline
Pavlakos~\cite{pavlakos2018ordinal} (CVPR`18) & 48.5 & 54.4 & 54.4 & 52.0 & 59.4 & 65.3 & 49.9 & 52.9 & 65.8 & 71.1 & 56.6 & 52.9 & 60.9 & 44.7 & 47.8 & 56.2 \\
Yang~\cite{yang20183d} (CVPR`18) & 51.5 & 58.9 & 50.4 & 57.0 & 62.1 & 65.4 & 49.8 & 52.7 & 69.2 & 85.2 & 57.4 & 58.4 & \bf 43.6 & 60.1 & 47.7 & 58.6 \\
Luvizon~\cite{luvizon20182d} (CVPR`18) ($\diamond$)& 49.2 & 51.6 & 47.6 & 50.5 & 51.8 & 60.3 & 48.5 & 51.7 & 61.5 & 70.9 & 53.7 & 48.9 & 57.9 & 44.4 & 48.9 & 53.2 \\
Hossain~\cite{hossain2018exploiting} (ECCV`18)($\dagger$, $\diamond$) & 48.4 & 50.7 & 57.2 & 55.2 & 63.1 & 72.6 & 53.0 & 51.7 & 66.1 & 80.9 & 59.0 & 57.3 & 62.4 & 46.6 & 49.6 & 58.3 \\
Lee~\cite{lee2018propagating} (ECCV`18)($\dagger$, $\diamond$) & \bf 40.2 & 49.2 & 47.8 & 52.6 & 50.1 & 75.0 & 50.2 & \bf 43.0 & \bf 55.8 & 73.9 & 54.1  & 55.6 & 58.2 & 43.3 & 43.3 & 52.8 \\
Pavllo~\cite{pavllo20193d} (CVPR`19) & 47.1 & 50.6 & 49.0 & 51.8 & 53.6 & 61.4 & 49.4 & 47.4 & 59.3 & 67.4 & 52.4 & 49.5 & 55.3 & 39.5 & 42.7 & 51.8 \\
Pavllo~\cite{pavllo20193d} (CVPR`19)($\dagger$) & 45.9 & 47.5 & 44.3 & \underline{46.4} & 50.0 & 56.9 & 45.6 & 44.6 & 58.8 & 66.8 & 47.9 & 44.7 & 49.7 & 33.1 & 34.0 & 47.7\\
Pavllo~\cite{pavllo20193d} (CVPR`19)($\dagger$, $\ddagger$) & 45.2 & 46.7 & \bf 43.3 & \bf 45.6 & \bf 48.1 & \bf 55.1 & \bf 44.6 & 44.3 & \underline{57.3} & 65.8 & \bf 47.1 & 44.0 & 49.0 & 32.8 & 33.9 & \underline{46.8} \\
\hline
Ours, single-frame &  47.4 & 49.9 &  47.4 &  51.1 & 53.8  & 61.2 & 48.3 & 45.9 & 60.4 & 67.1 & 52.0 & 48.6  & 54.6  & 40.1 &  43.0 & 51.4\\
Ours ($\dagger$) & \underline{44.8} & \bf 46.1 & \bf 43.3 & \underline{46.4} & \underline{49.0} & \underline{55.2} & \bf 44.6 & \underline{44.0} &  58.3 & \bf 62.7 & \bf 47.1  & \bf 43.9  & \underline{48.6} & \bf 32.7 & \bf 33.3  & \bf 46.7\\
\specialrule{.15em}{.05em}{.05em}
\end{tabular*}
\caption{Results on the Human3.6M dataset: reconstruction error using Protocol 1 (MPJPE) in mm. The best result is boldface and the second best is underlined. $\dagger$ indicates temporal models, $\diamond$ uses ground-truth bounding box, and $\ddagger$ indicates test-time augmentation.} 
\label{tab:human36_results}
\vspace{-0.8cm}
\end{table} \begin{table}[t]
\centering
\begin{minipage}{0.54\textwidth}
\scriptsize
\setlength{\tabcolsep}{2pt}
\begin{tabular*}{\textwidth}{l|ccc|ccc|ccc|c}
\specialrule{.15em}{.05em}{.05em}
 &  \multicolumn{3}{c|}{Walk} & \multicolumn{3}{c|}{Jog} & \multicolumn{3}{c|}{Box} & Avg.\\
App. & S1 & S2 & S3 & S1 & S2 & S3 & S1 & S2 & S3 & -\\
\hline
\hline
Pavlakos~\cite{pavlakos17volumetric} & 22.3 & 19.5 & \underline{29.7} & 28.9 & 21.9 & 23.8 & -- & -- & -- & --\\
Pavlakos~\cite{pavlakos2018ordinal}& 18.8 & 12.7 & \bf 29.2 & 23.5 & 15.4 & 14.5  & -- & -- & -- & --\\
Lee~\cite{lee2018propagating} & 18.6 & 19.9 & 30.5 & 25.7 & 16.8 & 17.7 & 42.8 & 48.1 & 53.4 & --\\
Pavllo~\cite{pavllo20193d} & \underline{14.1} & 10.4 & 46.8 & \underline{21.1} & 13.3 & 14.0 & 23.8 & 34.5 & 32.3 & 31.1\\
Pavllo~\cite{pavllo20193d} ($\ddagger$) & \bf 13.9 & \bf 10.2 & 46.6  &  \bf 20.9 & \bf 13.1 & \underline{13.8}  & \underline{23.8} & \underline{33.7} & \underline{32.0} & \underline{30.8}\\
\hline
Ours & 15.2 & \underline{10.3} &  47.0 & 21.8 & \bf 13.1 & \bf 13.7 & \bf 22.8 & \bf 31.8 & \bf 31.0 & \bf 30.6\\

\specialrule{.15em}{.05em}{.05em}
\end{tabular*}
\caption{Results on HumanEva-I for multi-action (MA) models reported in Protocol 2 (P-MPJPE), lower the better. $\ddagger$ indicates test time augmentation.}
\label{tab:human_eva_results}
\end{minipage}\hspace{0.1cm}
\begin{minipage}{0.43\textwidth}
\centering
\includegraphics[width=\textwidth]{results/limited_results_v1}
\vspace{-0.4cm}
\captionof{figure}{Comparisons between our approach and~\cite{pavllo20193d} in limited data settings evaluated using Protocol 1 on Human3.6M. }
\label{fig:limited_data_results}
\end{minipage}\vspace{-0.7cm}
\end{table} \subsection{2D to 3D pose estimation}
\textbf{Task.} 3D human pose estimation can be decoupled into the tasks of 2D keypoint detection and 2D to 3D pose estimation. We focus on the latter task, \ie, given a sequence of 2D keypoints, the task is to estimate the corresponding 3D human pose. See \figref{fig:task_ill} (a) for an illustration. 

\textbf{Dataset and metric.}
We evaluate on two standard datasets, the Human3.6M~\cite{h36m_pami} and the HumanEva-I~\cite{sigal2010humaneva}. 
Human3.6M is a large scale dataset of human motion with 3.6 million video frames. The dataset consists of 11 subjects performing 15 different actions. Following prior work~\cite{pavlakos17volumetric, tekin2017learning, martinez2017simple, sun2017compositional, luvizon20182d, pavllo20193d}, each human pose is represented by a 17-joint skeleton. We use the same train and test subject splits. HumanEva-I is a smaller  dataset consisting of four subjects and six actions. To be consistent with prior work~\cite{pavlakos2018ordinal, lee2018propagating, pavllo20193d}, we use the same train and test splits evaluated over the actions of (walk, jog, and box). For both of these datasets, we consider the setting where we train one model for all actions. 

We report the two standard metrics used in prior work: Protocol 1 (MPJPE) which is the mean per-joint position error between the prediction and ground-truth~\cite{martinez2017simple, pavlakos17volumetric, pavllo20193d}  and Protocol 2 (P-MPJPE) which is the error, after alignment, between the prediction and ground-truth~\cite{martinez2017simple, sun2017compositional, hossain2018exploiting, pavllo20193d}. 



\textbf{Implementation details.} Our model follows the supervised training procedure and network design of~\citet{pavllo20193d}. Our network is the identical temporal convolutional network architecture, where each layer is replaced with its
chiral version, \ie, 1D dilated convolution, batch-normalization, and dropout layers. We also replace ReLU non-linearities with Tanh to achieve equivariance. No additional architecture changes were made. For Human3.6M, we use 2D keypoints extracted from CPN~\cite{chen2018cascaded} with Mask R-CNN~\cite{he2017mask} bounding boxes released by~\citet{pavllo20193d}. For HumanEva-I, we use the 2D keypoint detections from Mask R-CNN released by~\citet{pavllo20193d}. 

\textbf{Results.}
In \tabref{tab:human36_results}, we report the performance on the Human3.6M data using Protocol 1 (MPJPE). Our approach outperforms the state-of-the-art~\cite{pavllo20193d} which uses test-time augmentation by 0.1 mm in overall average and achieves the best results in eight out of fifteen sub-categories. For the single-frame models, we observe a more significant reduction in error of 0.4 mm over~\cite{pavllo20193d} with test time augmentation. 
Additionally, when comparing without  test-time augmentation, our approach outperforms by 1 mm. 
We note that,  test-time augmentation employed by \citet{pavllo20193d} involves running the network twice for each input. In contrast, our approach only requires a single forward pass.

Next, on HumanEva-I dataset, we also observed an increase in performance using Protocol 1.  On average, our approach achieves a 32.2mm error. This is a 0.8mm decrease over the current state-of-the-art of 33.0mm~\cite{pavllo20193d} and a 1.1mm decrease over~\cite{pavllo20193d} without test-time augmentation of 33.3mm.

We also performed evaluation using Protocol 2 (P-MPJPE). On Human3.6M we observe that our approach performs worse than~\citet{pavllo20193d} by 0.3mm. We note that the loss function is chosen to optimize Protocol 1, therefore our models are performing better at what they are optimized for. 
In \tabref{tab:human_eva_results}, we
report the performance on HumanEva-I using Protocol 2 (P-MPJPE). Our model achieves a 0.2 mm reduction in error over~\citet{pavllo20193d} on average. Most of the gain is obtained for the boxing action, possibly due to the symmetric nature of the movement.

\textbf{Limited data settings.} A benefit of fewer model parameters is the potential to obtain better models with less  data. To confirm this, we perform experiments by varying the amount of training data, starting from  0.1\% of subject 1 (S1) to using three subjects S1, S5, S6. The results with comparison to~\cite{pavllo20193d} are shown in \figref{fig:limited_data_results}. We observe that our approach consistently out-performs~\cite{pavllo20193d} in this low resource settings, except at S1 0.1\%. 
For the reported numbers, we use a batch-size of 64, and all other hyper-parameters are identical between the models. 
If we further decrease the batch-size to 32 for S1 0.1\%, our approach improves to 100.4mm where \cite{pavllo20193d} improves to 102.3mm.



\begin{table}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\begin{tabular*}{\textwidth}{l|cccccccccccccccc|c}
\specialrule{.15em}{.05em}{.05em}
& \multicolumn{16}{c}{Prediction Steps} & Avg.\\
Approach & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & -\\
\hline
\hline
Residual~\cite{martinez2017human} (CVPR`17) & 82.4 & 68.3 & 58.5 & 50.9 & 44.7 & 40.0 & 36.4 & 33.4 & 31.3 & 29.5 & 28.3 & 27.3 & 26.4 & 25.7 & 25.0 & 24.5  & 39.5\\
3D-PFNet~\cite{chao2017forecasting}(CVPR`17) & 79.2 & 60.0 & 49.0 & 43.9 & 41.5 & 40.3 & 39.8 & 39.7 & 40.1 & 40.5 & 41.1 & 41.6 & 42.3 & 42.9 & 43.2 & 43.3  & 45.5\\
TP-RNN~\cite{chiu2019action} (WACV`19) & 84.5 & 72.0 & 64.8 &  60.3 & 57.2 & 55.0 & 53.4 & 52.1 & 50.9 & 50.0 & 49.3 & 48.7 & 48.3 & 47.9 & 47.6 & 47.3 & 55.6\\
\hline
Baseline w/o aug. & 87.3 & 75.7 & 68.5 & 64.0 & 61.0 & 59.1 & \bf 57.6 & 56.3 & 55.4 & 54.9 & 54.5 & 54.5 & 54.4 & 54.5 & 54.6 & \bf 54.7 & 60.4\\
Baseline w/ aug. & 86.9 & 75.2 & 67.9 & 63.5 & 60.4 & 58.4 & 57.0 & 55.8 & 55.1 & 54.5 & 54.1 & 54.0 & 53.9 & 53.9 & 54.0 & 54.0 &  59.9\\
Baseline w/ aug.($\ddagger$) & 87.0 & 75.5 & 68.4 & 64.1 & 61.0 & 59.1 & 57.5 & 56.3 & 55.5 & 55.0 & \bf 54.7 & \bf 54.7 & \bf 54.6 & \bf 54.7 & \bf 54.7 & \bf 54.7 & 60.5\\
Ours & \bf 87.5 & \bf 77.0 & \bf 68.7 & \bf 64.2 & \bf 61.2 & \bf 59.2 & \bf 57.6 & \bf 56.5 & \bf 55.7 & \bf 55.1 & \bf 54.7 & 54.6 & 54.4 & 54.5 & 54.5 & 54.5 &  \bf 60.6\\
\specialrule{.15em}{.05em}{.05em}
\end{tabular*}
\caption{Results on Penn action dataset, performance reported in terms of PCK@0.05 (higher the better). ($\ddagger$) indicates using test time augmentation.}
\label{tab:penn_action_result}
\vspace{-1.cm}
\end{table} \subsection{2D pose forecasting}
\textbf{Task.} 2D pose forecasting is the pose regression task of predicting the future human pose, represented in 2D keypoints, given present and past human pose. See \figref{fig:task_ill} (b) for an illustration. 

\textbf{Dataset and metric.}
We evaluate on the Penn Action dataset~\cite{zhang2013actemes}. The dataset consists of  2236 videos with 15 actions. Each frame is annotated with 2D keypoints of 13 human joints. We use the same train and test split as in~\cite{chao2017forecasting, chiu2019action}. Following~\citet{chiu2019action} we consider initial velocity as being part of the input and a single model is used for all actions. 
For a fair comparison with prior work, we report  the `Percentage of Correct Keypoint' metric with a 0.05 threshold (PCK@0.05), which assesses the accuracy of the predicted keypoints. A predicted keypoint is considered correct if it is within a 0.05 radius of the ground-truth when considering normalized distance.

\textbf{Implementation details.} Our non-chiral equivariant baseline model is a sequence-to-sequence model based on~\cite{martinez2017human}. We made several modifications to match the hyperparameters in~\cite{chiu2019action}, \ie, we used StackedRNN~\cite{pascanu2014howto} with 2 layers and added dropout layers. Additionally, we utilize teacher forcing~\cite{williams1989learning} during training, while prior work did not. We find this to stabilize training and enable the use of the Adam~\cite{kingma2015adam, ams_grad} optimizer without diverging. We performed data augmentation via the chirality transform, \ie, with 0.5 probability we apply $\mathcal{T}^{\tt in}$ and $\mathcal{T}^{\tt out}$ to the input and the ground-truth correspondingly.
For our pose symmetric model, we replaced all the non-symmetric layers, \eg, fully connected layers and LSTM cells with their corresponding
chiral version.


\textbf{Results.}
In \tabref{tab:penn_action_result}, we report the performance of our models and the state-of-the-art. The baseline model without augmentation outperforms the state-of-the-art~\cite{chiu2019action}. The gain comes from the use of Stacked-LSTM and teacher forcing during training. With  additional  train and test time data-augmentation, our baseline model further improves. In addition our pose symmetric model outperforms the baseline, in terms of average PCK@0.05. We observe more significant improvements for the first ten prediction steps. 



\begin{wraptable}{r}{0.35\textwidth}
\vspace{-1.7cm}
\scriptsize
\setlength{\tabcolsep}{2pt}
\centering
\setlength{\tabcolsep}{5pt}
\begin{tabular*}{0.35\textwidth}{lcc}
	\specialrule{.15em}{.05em}{.05em}
	Approach & Top-1 & Top-5\\
	\hline
	\hline
	Feature Encoding~\cite{fernando2015modeling} & 14.9\% & 25.8\% \\
	Deep LSTM~\cite{shahroudy2016ntu} & 16.4\%& 35.3\% \\
	Temporal-Conv~\cite{kim2017interpretable} & 20.3\% & 40.0\% \\
	ST-GCN~\cite{yan2018stgcn} & 30.7\% & \underline{52.8}\% \\
	Ours-Conv & \underline{30.8\%} & 52.6\%\\
	Ours-Conv-Chiral & \bf 30.9\% & \bf 53.0\% \\
	\specialrule{.15em}{.05em}{.05em}
\end{tabular*}
\vspace{-0.2cm}
\caption{Results of the skeleton based action recognition baselines on the Kinetics-400 dataset~\cite{kay2017kinetics} reported in Top-1 and Top-5 accuracy.}
\label{tab:action_results}
\vspace{-0.8cm}
\end{wraptable} \subsection{Skeleton based action recognition}
\textbf{Task.} Skeleton based action recognition aims at predicting human action based on skeleton sequences. See \figref{fig:task_ill} (c) for an illustration. 

\textbf{Dataset and metric.} We use the Kinetics-400 dataset~\cite{kay2017kinetics} in our experiments. The dataset contains 400 action classes and 306,245 clips in total. Following the experimental setup by~\cite{yan2018stgcn}, we use OpenPose~\cite{cao2018openpose} to locate the 18 human body joints. Each joint is represented as $(x,y,c)$, where $x$ and $y$ are the 2D coordinates of the joint and $c$ is the confidence score of the joint given by OpenPose. Following~\cite{kay2017kinetics}, we report the classification accuracy at top-1 and top-5.

\textbf{Implementation details.} Our baseline model, `Ours-Conv,' follows `Temporal-Conv'~\cite{kim2017interpretable}, modified  to have not only temporal convolution but also spatial convolution. The temporal convolution considers the intra-frame information while the spatial convolution considers the inter-frame information. For the recognition task, we need chiral invariance,~\ie, a chiral pair should be classified as the same action class. To this end, we use a chiral invariance layer where we let both $J^{\tt out}_{\tt r}$, $J^{\tt out}_{\tt l}$ as well as $D^{\tt out}_{\tt n}$  be empty sets, which means there are no left and right joints but only center joints and there is no dimension that will be negated in the output of the layer after applying chirality transform. Note that the chirality transform exchanges the left and right joints and negates the dimensions in the dimension index set $D^{\tt out}_{\tt n}$. Given $J^{\tt out}_{\tt r}$, $J^{\tt out}_{\tt l}$ and $D^{\tt out}_{\tt n}$ are all empty, it's trivial that the output will be chiral invariant. For the chiral invariance model, `Ours-Conv-Chiral,' we replace  all the non-symmetric layers before the chiral invariance layer with their corresponding chiral equivariance version. All the layers after the chiral invariance layer remain identical to the `Ours-Conv' model. There are in total 10 layers of spatial and temporal convolution and we put the chiral invariance layer at the fourth layer. We use the SGD optimizer with a momentum of $0.9$ as in~\cite{yan2018stgcn}. 

\textbf{Results.} In \tabref{tab:action_results}, we report the action recognition performance of our model and the skeleton-based approaches. We observe that the baseline model `Ours-Conv'  performs on par with ST-GCN~\cite{yan2018stgcn} and the chiral invariant model, `Ours-Conv-Chiral' outperforms both ST-GCN and Ours-Conv on Top-1 and Top-5 accuracy, achieving the state-of-the-art performance on the Kinetics-400 dataset among skeleton based action recognition methods. 
 \section{Conclusion}
\label{sec:conc}
We introduce  chirality equivariance for pose regression tasks and develop deep net layers that satisfy this property. Through parameter sharing and odd/even symmetry, we design equivariant versions of  commonly used layers in deep nets, including fully connected, 1D convolution, LSTM/GRU cells, and batch normalization layers. With these equivariant layers at hand, we build Chirality Nets, which guarantee  equivariance from the input to the output. Our models naturally lead to a reduction in trainable parameters and computation due to symmetry. Our experimental results on three human pose regression tasks over four datasets demonstrate state-of-the-art performance and the wide practical impact of the proposed layers.  
\vspace{12pt}
\noindent\textbf{Acknowledgments:}
This work is supported in part by NSF under Grant No.\ 1718221 and MRI \#1725729, UIUC, Samsung, 3M, Cisco Systems Inc.\ (Gift Award CG 1377144) and Adobe. We thank NVIDIA for providing GPUs used for this work and Cisco for access to the Arcetri cluster. RY is supported by a Google PhD Fellowship.

{\small
\bibliography{paper}
\bibliographystyle{abbrvnat}
}

\clearpage
\appendix
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand\thefigure{A\arabic{figure}}
\renewcommand{\theHtable}{Appendix.\thetable}
\renewcommand{\theHfigure}{Appendix.\thefigure}

\newcommand*{\dictchar}[1]{
    \clearpage
    \twocolumn[
    \centerline{\parbox[c][3cm][c]{\textwidth}{
            \centering
            \fontsize{14}{14}
            \selectfont
            {#1}}}]
}

\onecolumn
{\centering \Large \textbf{Supplementary material: Pose Symmetric Network for Human Pose Regression}}

\section{Code and Test Cases}
In the supplemental materials, we have included Pytorch implementation of the proposed layers. Each layer also comes with unit-tests validating the chirality-equivaraince. Please read the \texttt{README.md} for directory structures, usage and required dependencies. There is also a Jupyter notebook and it's HTML output visualizing the concepts introduced in the paper. 

\section{Additional Description for Equivariant Layers}\label{extra_layers}

\subsection{Equivariant fully connected layers}
Recall, we achieve equivariance through parameter sharing and odd symmetry.

A fully connected layer performs the mapping $\rvy = f_{\text{FC}}(\rvx ; W, b) := W\rvx + b$. 
Recall, we achieve equivariance through parameter sharing and odd symmetry:

$
W = \begin{bmatrix}
{\color{lb}
\begin{bmatrix}
W_{\tt ln, ln} & W_{\tt ln, lp} \\
W_{\tt lp, ln} & W_{\tt lp, lp}\\
\end{bmatrix}
} 
& 
{\color{lr}
\begin{bmatrix}
W_{\tt ln, rn} & W_{\tt ln, rp} \\
W_{\tt lp,rn} & W_{\tt lp, rp}\\
\end{bmatrix}
}
&
{\color{dy}
\begin{bmatrix}
W_{\tt ln, cn} & W_{\tt ln, cp} \\
W_{\tt lp, cn} & W_{\tt lp, cp}\\
\end{bmatrix}
}\\
{\color{lr}
\begin{bmatrix}
W_{\tt ln, rn} & -W_{\tt ln, rp} \\
-W_{\tt lp,rn} & W_{\tt lp, rp}\\
\end{bmatrix}
}
& 
{\color{lb}
\begin{bmatrix}
W_{\tt ln, ln} & -W_{\tt ln, lp} \\
-W_{\tt lp, ln} & W_{\tt lp, lp}\\
\end{bmatrix}
} 
&
{\color{dy}
\begin{bmatrix}
W_{\tt ln, cn} & -W_{\tt ln, cp} \\
-W_{\tt lp, cn} & W_{\tt lp, cp}\\
\end{bmatrix}
}\\
{\color{black}
\begin{bmatrix}
W_{\tt cn, ln} & W_{\tt cn, lp} \\
\bf 0 & W_{\tt cp, lp}\\
\end{bmatrix}
} 
& 
{\color{black}
\begin{bmatrix}
W_{\tt cn, ln} & -W_{\tt cn, lp} \\
\bf 0 & W_{\tt cp, lp}\\
\end{bmatrix}
}
&
{\color{dg}
\begin{bmatrix}
W_{\tt cn, cn} & \bf 0\\
\bf 0 & W_{\tt cp, cp}\\
\end{bmatrix}
}\\
\end{bmatrix}
$,
$b = \begin{bmatrix}
\begin{bmatrix}
\color{lr}
b_{\tt ln} \\ 
\color{lr}
b_{\tt lp}
\end{bmatrix} \\ 
\begin{bmatrix}
\color{lr}
-b_{\tt ln} \\ 
\color{lr}
b_{\tt lp} 
\end{bmatrix} \\ 
\begin{bmatrix} \bf 0 \\ 
b_{\tt cp}\end{bmatrix}\\
\end{bmatrix}
$

Here, we prove that the design is chiral-equivariant. Through multiplying out the matrices, we can show $W\mathcal{T}(\mathbf{x})+b$ = $\mathcal{T}(W\mathbf{x}+b)$, as follows:

\textit{Proof:}

$\mathbf{x}  = 
\begin{bmatrix}
x_{\tt ln} &
x_{\tt lp} &
x_{\tt rn} &
x_{\tt rp} &
x_{\tt cn} &
x_{\tt cp}
\end{bmatrix}^{T}
$
then  $\mathcal{T}(\mathbf{x}) = 
\begin{bmatrix}
-x_{\tt rn} &
x_{\tt rp} &
-x_{\tt ln} &
x_{\tt lp} &
-x_{\tt cn} &
x_{\tt cp}
\end{bmatrix}^{T}$

With linear algebra,
$$
W\mathbf{x} +b =
\begin{bmatrix}
W_{\tt ln,ln} (x_{\tt ln}) + W_{\tt ln,lp} (x_{\tt lp}) + W_{\tt ln,rn} (x_{\tt rn}) + W_{\tt ln,rp} (x_{\tt rp}) + W_{\tt ln,cn} (x_{\tt cn}) + W_{\tt ln,cp} (x_{\tt cp}) + b_{\tt ln}\\ 
W_{\tt lp,ln} (x_{\tt ln}) + W_{\tt lp,lp} (x_{\tt lp}) + W_{\tt lp,rn} (x_{\tt rn}) + W_{\tt lp,rp} (x_{\tt rp}) + W_{\tt lp,cn} (x_{\tt cn}) + W_{\tt lp,cp} (x_{\tt cp}) + b_{\tt lp}\\ 
W_{\tt ln, rn} (x_{\tt ln}) - W_{\tt ln, rp} (x_{\tt lp}) + W_{\tt ln,ln} (x_{\tt rn}) - W_{\tt ln,lp} (x_{\tt rp}) + W_{\tt ln,cn} (x_{\tt cn}) - W_{\tt ln, cp} (x_{\tt cp}) - b_{\tt ln}\\
-W_{\tt lp,rn} (x_{\tt ln}) + W_{\tt lp,rp} (x_{\tt lp}) - W_{\tt, lp,ln} (x_{\tt rn}) + W_{\tt lp,lp} (x_{\tt rp}) - W_{\tt lp, cn} (x_{\tt cn}) + W_{\tt lp,cp} (x_{\tt cp}) + b_{\tt lp}\\ 
W_{\tt cn,ln} (x_{\tt ln}) + W_{\tt cn, lp} (x_{\tt lp}) + W_{\tt cn,ln} (x_{\tt rn}) - W_{\tt cn, lp} (x_{\tt rp}) + W_{\tt cn,cn} (x_{\tt cn}) + \mathbf{0} \cdot (x_{\tt cp}) + \mathbf{0}\\
 \mathbf{0} \cdot (x_{\tt ln}) + W_{\tt cp, lp} (x_{\tt lp}) + \mathbf{0} \cdot (x_{\tt rn}) + W_{\tt cp,lp} (x_{\tt rp}) + \mathbf{0} \cdot (x_{\tt cn}) + W_{\tt cp,cp} (x_{\tt cp}) + b_{\tt cp}
\end{bmatrix}
$$
$$
\mathcal{T}(W\mathbf{x} +b) = 
\begin{bmatrix}
-W_{\tt ln, rn} (x_{\tt ln}) + W_{\tt ln, rp} (x_{\tt lp}) - W_{\tt ln,ln} (x_{\tt rn}) + W_{\tt ln,lp} (x_{\tt rp}) - W_{\tt ln,cn} (x_{\tt cn}) + W_{\tt ln, cp} (x_{\tt cp}) + b_{\tt ln}\\
-W_{\tt lp,rn} (x_{\tt ln}) + W_{\tt lp,rp} (x_{\tt lp}) - W_{\tt, lp,ln} (x_{\tt rn}) + W_{\tt lp,lp} (x_{\tt rp}) - W_{\tt lp, cn} (x_{\tt cn}) + W_{\tt lp,cp} (x_{\tt cp}) + b_{\tt lp}\\ 
-W_{\tt ln,ln} (x_{\tt ln}) - W_{\tt ln,lp} (x_{\tt lp}) - W_{\tt ln,rn} (x_{\tt rn}) - W_{\tt ln,rp} (x_{\tt rp}) - W_{\tt ln,cn} (x_{\tt cn}) - W_{\tt ln,cp} (x_{\tt cp}) - b_{\tt ln}\\ 
W_{\tt lp,ln} (x_{\tt ln}) + W_{\tt lp,lp} (x_{\tt lp}) + W_{\tt lp,rn} (x_{\tt rn}) + W_{\tt lp,rp} (x_{\tt rp}) + W_{\tt lp,cn} (x_{\tt cn}) + W_{\tt lp,cp} (x_{\tt cp}) + b_{\tt lp}\\ 
-W_{\tt cn,ln} (x_{\tt ln}) - W_{\tt cn, lp} (x_{\tt lp}) - W_{\tt cn,ln} (x_{\tt rn}) + W_{\tt cn, lp} (x_{\tt rp}) - W_{\tt cn,cn} (x_{\tt cn}) - \mathbf{0} \cdot (x_{\tt cp}) - \mathbf{0}\\
 \mathbf{0} \cdot (x_{\tt ln}) + W_{\tt cp, lp} (x_{\tt lp}) + \mathbf{0} \cdot (x_{\tt rn}) + W_{\tt cp,lp} (x_{\tt rp}) + \mathbf{0} \cdot (x_{\tt cn}) + W_{\tt cp,cp} (x_{\tt cp}) + b_{\tt cp}
\end{bmatrix}
$$
$$
W\mathcal{T}(\mathbf{x}) +b = 
\begin{bmatrix}
W_{\tt ln,ln} (-x_{\tt rn}) + W_{\tt ln,lp} (x_{\tt rp}) + W_{\tt ln,rn} (-x_{\tt ln}) + W_{\tt ln,rp} (x_{\tt lp}) + W_{\tt ln,cn} (-x_{\tt cn}) + W_{\tt ln,cp} (x_{\tt cp}) + b_{\tt ln}\\ 
W_{\tt lp,ln} (-x_{\tt rn}) + W_{\tt lp,lp} (x_{\tt rp}) + W_{\tt lp,rn} (-x_{\tt ln}) + W_{\tt lp,rp} (x_{\tt lp}) + W_{\tt lp,cn} (-x_{\tt cn}) + W_{\tt lp,cp} (x_{\tt cp}) + b_{\tt lp}\\ 
W_{\tt ln, rn} (-x_{\tt rn}) - W_{\tt ln, rp} (x_{\tt rp}) + W_{\tt ln,ln} (-x_{\tt ln}) - W_{\tt ln,lp} (x_{\tt lp}) + W_{\tt ln,cn} (-x_{\tt cn}) - W_{\tt ln, cp} (x_{\tt cp}) - b_{\tt ln}\\
-W_{\tt lp,rn} (-x_{\tt rn}) + W_{\tt lp,rp} (x_{\tt rp}) - W_{\tt, lp,ln} (-x_{\tt ln}) + W_{\tt lp,lp} (x_{\tt lp}) - W_{\tt lp, cn} (-x_{\tt cn}) + W_{\tt lp,cp} (x_{\tt cp}) + b_{\tt lp}\\ 
W_{\tt cn,ln} (-x_{\tt rn}) + W_{\tt cn, lp} (x_{\tt rp}) + W_{\tt cn,ln} (-x_{\tt ln}) - W_{\tt cn, lp} (x_{\tt lp}) + W_{\tt cn,cn} (-x_{\tt cn}) + \mathbf{0} \cdot (x_{\tt cp}) + \mathbf{0}\\
 \mathbf{0} \cdot (-x_{\tt rn}) + W_{\tt cp, lp} (x_{\tt rp}) + \mathbf{0} \cdot (-x_{\tt ln}) + W_{\tt cp,lp} (x_{\tt lp}) + \mathbf{0} \cdot (-x_{\tt cn}) + W_{\tt cp,cp} (x_{\tt cp}) + b_{\tt cp}
\end{bmatrix}
$$

observe that $W\mathcal{T}(\mathbf{x}) +b = \mathcal{T}(W\mathbf{x}+b)$, which proves the claim. \hfill $\square$

 
\subsection{Equivariant 1D convolution layers}
{
\textbf{1D convolution layers~\cite{waibel1995phoneme, lecun1999object}.}
}
Pose symmetric 1D convolution layers can be based on fully connected layers. A 1D convolution is a fully connected layer with shared parameters across the time dimension, \ie, at each time step the computation is the sum of fully connected layers over a window:
$$\rvy_t = \sum\limits_{\tau} W_{\tau}\rvx_{t-\tau} + b = \sum\limits_{\tau}  f_{\text{FC}}(\rvx_{t-\tau}; W_{\tau}, b).$$ 
Consequently, we  enforce equivariance at each time step by employing the  symmetry pattern of fully connected layers at each time slice. 
$$
W_{\tau} = \begin{bmatrix}
{\color{lb}
\begin{bmatrix}
W_{\tt ln, ln, \tau} & W_{\tt ln, lp, \tau} \\
W_{\tt lp, ln, \tau} & W_{\tt lp, lp, \tau}\\
\end{bmatrix}
} 
& 
{\color{lr}
\begin{bmatrix}
W_{\tt ln, rn, \tau} & W_{\tt ln, rp, \tau} \\
W_{\tt lp,rn, \tau} & W_{\tt lp, rp, \tau}\\
\end{bmatrix}
}
&
{\color{dy}
\begin{bmatrix}
W_{\tt ln, cn, \tau} & W_{\tt ln, cp, \tau} \\
W_{\tt lp, cn, \tau} & W_{\tt lp, cp, \tau}\\
\end{bmatrix}
}\\
{\color{lr}
\begin{bmatrix}
W_{\tt ln, rn, \tau} & -W_{\tt ln, rp, \tau} \\
-W_{\tt lp,rn, \tau} & W_{\tt lp, rp, \tau}\\
\end{bmatrix}
}
& 
{\color{lb}
\begin{bmatrix}
W_{\tt ln, ln, \tau} & -W_{\tt ln, lp, \tau} \\
-W_{\tt lp, ln, \tau} & W_{\tt lp, lp, \tau}\\
\end{bmatrix}
} 
&
{\color{dy}
\begin{bmatrix}
W_{\tt ln, cn, \tau} & -W_{\tt ln, cp, \tau} \\
-W_{\tt lp, cn, \tau} & W_{\tt lp, cp, \tau}\\
\end{bmatrix}
}\\
{\color{black}
\begin{bmatrix}
W_{\tt cn, ln, \tau} & W_{\tt cn, lp, \tau} \\
\bf 0 & W_{\tt cp, lp, \tau}\\
\end{bmatrix}
} 
& 
{\color{black}
\begin{bmatrix}
W_{\tt cn, ln, \tau} & -W_{\tt cn, lp, \tau} \\
\bf 0 & W_{\tt cp, lp, \tau}\\
\end{bmatrix}
}
&
{\color{dg}
\begin{bmatrix}
W_{\tt cn, cn, \tau} & \bf 0\\
\bf 0 & W_{\tt cp, cp, \tau}\\
\end{bmatrix}
}\\
\end{bmatrix},
$$
for all $\tau$. 
The bias of a 1D convolution is identical to that of a fully connected layer, \ie, the  same bias is added for each time step. Hence the same parameter sharing is used. 



\subsection{Equivariant LSTM and GRU layers}
LSTM and GRU modules which satisfy chirality can be obtained from fully connected layers.  
However, na\"ively setting all matrix multiplies within an LSTM to satisfy the equivariance property will not lead to an equivariant LSTM because gates are elementwise \textit{multiplied} with the cell state. If both  gate and cell  preserve the negation then the product will not. Therefore, we  change the weight sharing scheme for the gates. We set $D_{n}^{\tt out}$ for the gates to be the empty set, \ie,  the gates will be invariant to negation at the input, $T^{\tt in}_{\tt neg}$, but still equivariant to the switch operation, $T^{\tt in}_{\tt swi}$. With this setup, the product of the gates and the cell's output will preserve the sign, as the gates are invariant to negation and passed through a Sigmoid to be within the range of $(0,1)$.  GRU modules are modified  in the same manner. 

More formally, the computation in an LSTM module are as follows:
\begin{center}
\begin{tabular}{lc}
 $i_t = \sigma(W^\text{ii} x_t + b^\text{ii} + W^\text{hi} h_{(t-1)} + b^\text{hi})$ & (Input Gate)\\
 $o_t = \sigma(W^\text{io} x_t + b^\text{io} + W^\text{ho} h_{(t-1)} + b^\text{ho})$ & (Output  Gate)\\
 $f_t = \sigma(W^\text{if} x_t + b^\text{if} + W^\text{hf} h_{(t-1)} + b^\text{hf})$ & (Forget Gate) \\
 $g_t = \tanh(W^\text{ig} x_t + b^\text{ig} + W^\text{hg} h_{(t-1)} + b^\text{hg})$ &  (Cell State) \\
 $ c_t = f_t \cdot c_{(t-1)} + i_t \cdot g_t$ & \\
 $h_t = o_t \cdot \tanh(c_t)$ & (Recurrent State)\\
\end{tabular},
\end{center}
where $\sigma$ denotes an element-wise sigmoid non-linearity.

Observe that the LSTM operations consist of fully connected layers. For the cell state's parameters, \eg, $W^\text{ig}, W^\text{hg}, b^\text{ig}, b^\text{hg}$, we  follow the weight sharing scheme discussed for fully connected layers. 

Due the to multiplication in the cell state, we redesigned the parameter sharing for the input, output and forget gate, to be invariant to $T^{\tt in}_{\tt neg}$, by setting $D_{n}^{\tt out}$  to be the empty set: no negation is needed for all dimension. This results in the following parameter sharing scheme for the parameters $W^\text{ii}, b^\text{ii}, W^\text{hi}, b^\text{hi}, W^\text{io}, b^\text{io}, W^\text{ho}, b^\text{ho}, W^\text{if}, b^\text{if}, W^\text{hf}, b^\text{hf}$:
\begin{center}
$
W = \begin{bmatrix}
{\color{lb}
\begin{bmatrix}
W_{\tt lp, ln} & W_{\tt lp, lp}\\
\end{bmatrix}
} 
& 
{\color{lr}
\begin{bmatrix}
W_{\tt lp,rn} & W_{\tt lp, rp}\\
\end{bmatrix}
}
&
{\color{dy}
\begin{bmatrix}
W_{\tt lp, cn} & W_{\tt lp, cp}\\
\end{bmatrix}
}\\
{\color{lr}
\begin{bmatrix}
-W_{\tt lp,rn} & W_{\tt lp, rp}\\
\end{bmatrix}
}
& 
{\color{lb}
\begin{bmatrix}
-W_{\tt lp, ln} & W_{\tt lp, lp}\\
\end{bmatrix}
} 
&
{\color{dy}
\begin{bmatrix}
-W_{\tt lp, cn} & W_{\tt lp, cp}\\
\end{bmatrix}
}\\
{\color{black}
\begin{bmatrix}
\bf 0 & W_{\tt cp, lp}\\
\end{bmatrix}
} 
& 
{\color{black}
\begin{bmatrix}
\bf 0 & W_{\tt cp, lp}\\
\end{bmatrix}
}
&
{\color{dg}
\begin{bmatrix}
\bf 0 & W_{\tt cp, cp}\\
\end{bmatrix}
}\\
\end{bmatrix}
$,
$b = \begin{bmatrix}
\begin{bmatrix}
\color{lr}
\color{lr}
b_{\tt lp}
\end{bmatrix} \\ 
\begin{bmatrix}
\color{lr}
\color{lr}
b_{\tt lp} 
\end{bmatrix} \\ 
\begin{bmatrix}
b_{\tt cp}\end{bmatrix}\\
\end{bmatrix}
$.
\end{center}
This LSTM is chirality equivariant, as  the computation of the cell state is equivariant. Other computations are linear combinations of chirality equivariant operations, which remains equivariant. We note that the chirality equivariant GRU module  is modified by following the same sharing scheme for the gates.

\subsection{Equivariant batch-norm layers}
A batch normalization layer performs an element-wise standardization, followed by an element-wise affine layer (with learnable parameters $\gamma$ and $\beta$): 
$$
\rvy = f_{\text{BN}}(\rvx):= \gamma \cdot \frac{\rvx - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta.
$$
\textit{Equivariance for $\gamma$, and $\beta$} is obtained by 
following the  principle applied to fully connected layers: we achieve equivariance via parameter sharing and odd symmetry:
\begin{center}
$\gamma = 
\begin{bmatrix}
\color{lr}
\begin{bmatrix}
\gamma_{\tt ln} & \gamma_{\tt lp}
\end{bmatrix} &
\color{lr}
\begin{bmatrix}
\gamma_{\tt ln} & \gamma_{\tt 1p}
\end{bmatrix} &
\begin{bmatrix}
\gamma_{\tt cn} & \gamma_{\tt cp}
\end{bmatrix}
\end{bmatrix}^T$
and 
$\beta = \begin{bmatrix}
\color{lr}
\begin{bmatrix}
\beta_{\tt ln} & \beta_{\tt lp} 
\end{bmatrix}&
\color{lr}
\begin{bmatrix}-\beta_{\tt ln} & \beta_{\tt lp} 
\end{bmatrix}&
\begin{bmatrix}
\bf 0 & \beta_{\tt cp}
\end{bmatrix}\\
\end{bmatrix}^{T}$.
\end{center}

\textit{Equivariance for $\mu$, and $\sigma$} is obtained by
computing the mean and standard deviation on the ``augmented batch'' and by keeping track of its running average. Formally, given a batch $\mathcal{B}$ of data,
\begin{center}
$\mu = \frac{1}{2|\mathcal{B}|} \sum_{\rvx \in \mathcal{B}} \rvx + \mathcal{T}^{\tt in}(\rvx)$, \hspace{0.1cm} $\sigma =  \sqrt{\frac{\sum_{\rvx \in \mathcal{B}} (\rvx - \mu)^2 + (\mathcal{T}^{\tt in}(\rvx)- \mu)^2}{2 |\mathcal{B}|}}$.
\end{center}
\subsection{Dropout.}
At test time, dropout scales the input by $p$, where $p$ is the dropout probability. The equivariance property is satisfied because of  the associativity property of a scalar multiplication. 
The input and output dimension and symmetry of a dropout layer are identical. Therefore, $\mathcal{T^{\tt out}}$ and $\mathcal{T^{\tt in}}$ are identical. From the definition:
$$
\mathcal{T}^{\tt out}( p \cdot \rvx)  = \mathcal{T^{\tt in}}(p \cdot \rvx) = T^{\tt in}_{\tt neg}T^{\tt in}_{\tt swi} (p \cdot \rvx) = p \cdot (T^{\tt in}_{\tt neg}T^{\tt in}_{\tt swi}\rvx)
=p \cdot (\mathcal{T}^{\tt in}(\rvx)) \;\; \forall \rvx \in \mathbb{R}^{|J^{\tt in}||D^{\tt in}|}.$$
Hence, a dropout layer naturally satisfies the equivariance property. At training-time, we do not enforce equivariance for the dropped units, \ie, we do not jointly drop symmetric units  as we found this to prevent overfitting. This is likely application dependent.

\section{Additional Results}\label{extra_results}
\subsection{3D pose estimation}
In \tabref{tab:supp_eva_quan}, we report the HumanEva-I for multi-action models evaluated on Protocol 1 (MPJPE). Our approach have benefits the most from the Boxing action while maintaing the performance on other actions. We also provide qualitative evaluation in \figref{fig:supp_eva_walk} and \figref{fig:supp_eva_box}. We observe that our model successfully estimates 3D poses from 2D key-points. We have also attached animations in the supplemental.

\begin{table}[t]
\centering
\begin{tabular}{l|ccc|ccc|ccc|c}
\specialrule{.15em}{.05em}{.05em}
 &  \multicolumn{3}{c|}{Walk} & \multicolumn{3}{c|}{Jog} & \multicolumn{3}{c|}{Box} & Avg.\\
App. & S1 & S2 & S3 & S1 & S2 & S3 & S1 & S2 & S3 & -\\
\hline
\hline
Pavllo~\cite{pavllo20193d} & 17.6 & 12.5 & 37.6  & 28.1 & 19.1 & 19.2 & 29.5 & 44.0 & 43.1 & 33.3\\
Pavllo~\cite{pavllo20193d} ($\ddagger$) & \bf 17.5 & 12.3 & 37.4 & \bf 27.7 & 19.0 & 19.0 & 27.7 & 43.4& 42.5 & 33.0\\
\hline
Ours & 18.9 & \bf 12.3 & 38.1 & 28.5 & \bf 18.1 & \bf 18.2 & \bf 27.1 & \bf 40.9 & \bf 40.2 & \bf 32.2\\
\specialrule{.15em}{.05em}{.05em}
\end{tabular}
\caption{Results on HumanEva-I for multi-action (MA) models reported in Protocol 1 (MPJPE), lower the better. $\ddagger$ indicates test time augmentation.}
\label{tab:supp_eva_quan}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[scale=0.3]{supp_fig/eva/w1}
\includegraphics[scale=0.3]{supp_fig/eva/w2}
\includegraphics[scale=0.3]{supp_fig/eva/w3}
\caption{Qualitative visualization of 2D to 3D pose estimation for the action ``Walking" on HumanEva-I dataset.}
\label{fig:supp_eva_walk}
\end{figure}
\begin{figure}[t]
\centering
\includegraphics[scale=0.3]{supp_fig/eva/b1}
\includegraphics[scale=0.3]{supp_fig/eva/b2}
\includegraphics[scale=0.3]{supp_fig/eva/b3}
\caption{Qualitative visualization of 2D to 3D pose estimation for the action ``Boxing" on HumanEva-I dataset.}
\label{fig:supp_eva_box}
\end{figure}


\subsection{Skeleton based action recognition}
In~\figref{fig:supp_action}, we show the visualization of the input skeleton sequences computed by OpenPose~\cite{cao2018openpose} and the predicted action class by our chiral invariant skeleton based action recognition model.
\begin{figure}[t]
\centering
\hspace{-0.6cm}\includegraphics[width=0.2\textwidth]{supp_fig/action/pushup/00001}
\includegraphics[width=0.2\textwidth]{supp_fig/action/pushup/00003}
\includegraphics[width=0.2\textwidth]{supp_fig/action/pushup/00005}
\includegraphics[width=0.2\textwidth]{supp_fig/action/pushup/00007}
\includegraphics[width=0.2\textwidth]{supp_fig/action/pushup/00009}
Push up\\
\hspace{-0.6cm}\includegraphics[width=0.2\textwidth]{supp_fig/action/cleanandjerk/00000}
\includegraphics[width=0.2\textwidth]{supp_fig/action/cleanandjerk/00003}
\includegraphics[width=0.2\textwidth]{supp_fig/action/cleanandjerk/00005}
\includegraphics[width=0.2\textwidth]{supp_fig/action/cleanandjerk/00006}
\includegraphics[width=0.2\textwidth]{supp_fig/action/cleanandjerk/00007}
Clean and jerk\\
\hspace{-0.6cm}\includegraphics[width=0.2\textwidth]{supp_fig/action/juggling/00000}
\includegraphics[width=0.2\textwidth]{supp_fig/action/juggling/00003}
\includegraphics[width=0.2\textwidth]{supp_fig/action/juggling/00005}
\includegraphics[width=0.2\textwidth]{supp_fig/action/juggling/00008}
\includegraphics[width=0.2\textwidth]{supp_fig/action/juggling/00007}
Juggling balls\\
\hspace{-0.6cm}\includegraphics[width=0.2\textwidth]{supp_fig/action/piano/00005}
\includegraphics[width=0.2\textwidth]{supp_fig/action/piano/00006}
\includegraphics[width=0.2\textwidth]{supp_fig/action/piano/00007}
\includegraphics[width=0.2\textwidth]{supp_fig/action/piano/00008}
\includegraphics[width=0.2\textwidth]{supp_fig/action/piano/00009}
Playing piano\\
\hspace{-0.6cm}\includegraphics[width=0.2\textwidth]{supp_fig/action/jogging/00005}
\includegraphics[width=0.2\textwidth]{supp_fig/action/jogging/00007}
\includegraphics[width=0.2\textwidth]{supp_fig/action/jogging/00008}
\includegraphics[width=0.2\textwidth]{supp_fig/action/jogging/00009}
\includegraphics[width=0.2\textwidth]{supp_fig/action/jogging/00010}
Jogging\\

\caption{Visualization of the input skeleton sequences and the corresponding predicted action classes of our method on the Kinetics-400 dataset~\cite{kay2017kinetics}.}
\label{fig:supp_action}
\end{figure} 
\section{Implementation Details}\label{extra_implement}
\subsection{3D pose estimation}
\textbf{Implementation details.}
Our model follows the temporal convolutional architecture proposed by~\citet{pavllo20193d}, and replaced all layers with their chiral versions; code for the layers are attached in the supplemental as well.
We also changed ReLU to tanh to achieve chiral equivariance. For the temporal models, we follow their 4 blocks design which has the receptive field of 243. For the single frame model, we follow their 3 blocks design. These models all contains 1020 hidden dimensions so it is a factor the number of joints, 17, this is slightly smaller than the 1024 used in~\cite{pavllo20193d}. We also use their data processing and batching stragety as described in Section 5 and Appendix A.5 of~\cite{pavllo20193d}. For training the model, we utilized the Adam optimizer with beta1=0.9 and beta2=0.9999. We decay the batch-normalizations' momentum as suggested in~\cite{pavllo20193d}. Other details follows the publicly available implementation by~\citet{pavllo20193d}.  We enforced chiral equivariance by choosing the $|D_n^{\tt out}|$ to be $\frac{1}{3}$ of the hidden dimension. The $|D_n^{\tt in}|$ for the input layer is 17 and the $|D_n|^{\tt out}$ for the output layer is 17, as one for each joint.

\subsection{2D pose forecasting}
\textbf{Implementation details.}
The non-chiral equivariant baseline is a seq2seq model consisting of an encoder and decoder, which are stacked-LSTMs with hidden size of 1040 and 2 stacked layers. We trained using teacher forcing with the Adam optimizer. The batch-size is 256, and we trained for 30 epochs. Dropout is applied to the LSTMs' hidden layer with drop probability of 0.5. Following prior works, we use max norm gradient clipping of 5, a learning rate of 0.005 with a decay of 0.95 every 2 epochs. The data processing and evaluation setting follows~\cite{chiu2019action}. Other details follows the publicly available implementation by~\citet{chiu2019action}. We enforced chiral equivariance by choosing the $|D_n^{\tt out}|$ to be $\frac{1}{2}$ of the hidden dimension, as the output is two dimensional per joint.

\subsection{Skeleton-based action recognition}
\textbf{Implementation details.}
The non-chiral version of the model, Ours-Conv, follows Temporal-Conv~\cite{kim2017interpretable} while we modified the model to have not only temporal convolution but also spatial convolution. There are ten spatial-temporal convolution blocks and each block we first perform spatial convolution and then temporal convolution. The temporal convolution considers the intra-frame information while the spatial convolution considers the inter-frame information. For the recognition task, we need chiral invariance,~\ie, a chiral pair should be classified as the same action class. To this end, we use a chiral invariance layer where we let both $J^{\tt out}_{\tt r}$, $J^{\tt out}_{\tt l}$ as well as $D^{\tt out}_{\tt n}$ to be empty sets, which means there are no left and right joints but only center joints and there is no dimension that will be negated in the output of the layer after applying the chirality transform. Note that the chiral transformation exchange the left and right joints and negate the dimension in the index set $D^{\tt out}_{\tt n}$. Given $J^{\tt out}_{\tt r}$, $J^{\tt out}_{\tt l}$ and $D^{\tt out}_{\tt n}$ are all empty, it's obvious that the output will be chiral invariance. For the chiral invariance model, Ours-Conv-Chiral, we replace the all the non-symmetric layers before the chiral invariance layer with their corresponding chiral equivariance version. All the layers after the chiral invariance layer remains the same as in the Ours-Conv model. Similar to~\cite{kim2017interpretable}, there are in total 10 convolution blocks in Ours-Conv and we put the chiral invariance layer at the fourth layer. Also, we gradually reduce the ratio of the dimension to be negated ($|{D^{\tt out}_{\tt n}}|/|{D^{\tt out}}|$) from $\frac{1}{3}$ to $\frac{1}{6}$ at the first layer, from $\frac{1}{6}$ to $\frac{1}{12}$ at the second layer and from $\frac{1}{12}$ to $0$ at the third layer. We use the SGD optimizer with a momentum of $0.9$ as in~\cite{yan2018stgcn} with a batch size of 256. We train the model for 90 epochs.

 
\end{document}
