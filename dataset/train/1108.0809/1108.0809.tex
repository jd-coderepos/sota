\documentclass[leqno,11pt]{article}  

\usepackage{lineno}
\pagestyle{plain}
  \pagenumbering{arabic}

\usepackage{lmodern}

\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{algorithmicext}
\usepackage{ifthen}
\newconstruct{\PROC}{\textbf{procedure}}{}{\ENDPROC}{\textbf{end procedure}}
\newconstruct{\FUNC}{\textbf{function}}{}{\ENDFUNC}{\textbf{end function}}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{mathrsfs }
\usepackage[normalem]{ulem}
\usepackage{paralist}
\newcommand{\msg}[1]{\langle#1\rangle}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\red}[1]{{\textcolor{red}{#1}}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\add}[1]{{\red{#1}}}
\newcommand{\del}[1]{\sout{#1}}
\newcommand{\comment}[1]{{\red{{\em #1}}}}
\newcommand{\todo}[1]{\noindent\textbf{TODO: }\marginpar{****}\textit{\red{{#1}}}\textbf{ :ODOT}}




\usepackage{fullpage}

\DeclareMathAlphabet{\mathsc}{OT1}{cmr}{m}{sc}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{property}{Property}

\newtheorem{observation}{Observation}



\renewcommand{\geq}{\geqslant}
\renewcommand{\ge}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\le}{\leqslant}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\la}{\leftarrow}
\newcommand{\bigO}{\mathcal{O}}
\renewcommand{\algorithmiccomment}[1]{/\!/ #1}
\newcommand{\pair}[1]{\langle#1\rangle}
\newcommand{\eps}{\varepsilon}
\newcommand{\lel}{{\mathsc{LE}}}
\newcommand{\dd}{{\mathsc{DD}}}
\newcommand{\infl}{{\mathsc{Infl}}}
\newcommand{\Rev}{{\mathsc{Reverse}}}
\newcommand{\For}{{\mathsc{Forward}}}
\newcommand{\val}{{\mathsc{val}}}
\newcommand{\lo}{{\mathsc{Low}}}
\newcommand{\hi}{{\mathsc{High}}}
\renewcommand{\mid}{{\mathsc{Mid}}}
\newcommand{\hid}{{\mathsc{Hid}}}
\newcommand{\A}{{\mathcal{A}}}
\newcommand{\D}{{\mathcal{D}}}
\renewcommand{\L}{{\mathcal{L}}}
\newcommand{\C}{{\mathcal{C}}}
\newcommand{\V}{{\mathcal{V}}}
\newcommand{\R}{{\mathcal{R}}}
\newcommand{\sa}{{\sc Stable Agreement}}
\newcommand{\ssa}{{\sc Strong Stable Agreement}}
\newcommand{\ssd}{{\sc Sufficient Support Detection}}
\newcommand{\bc}{{\sc Binary Consensus}}
\renewcommand{\t}{{\sc true}}
\newcommand{\f}{{\sc false}}



\newboolean{short}
\setboolean{short}{false}


\newcommand{\shortOnly}[1]{\ifthenelse{\boolean{short}}{#1}{}}
\newcommand{\onlyShort}[1]{\ifthenelse{\boolean{short}}{#1}{}}
\newcommand{\longOnly}[1]{\ifthenelse{\boolean{short}}{}{#1}}
\newcommand{\onlyLong}[1]{\ifthenelse{\boolean{short}}{}{#1}}

\newcommand*\patchAmsMathEnvironmentForLineno[1]{\expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
  \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
  \renewenvironment{#1}{\linenomath\csname old#1\endcsname}{\csname oldend#1\endcsname\endlinenomath}}\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{\patchAmsMathEnvironmentForLineno{#1}\patchAmsMathEnvironmentForLineno{#1*}}\AtBeginDocument{\patchBothAmsMathEnvironmentsForLineno{equation}\patchBothAmsMathEnvironmentsForLineno{align}\patchBothAmsMathEnvironmentsForLineno{flalign}\patchBothAmsMathEnvironmentsForLineno{alignat}\patchBothAmsMathEnvironmentsForLineno{gather}\patchBothAmsMathEnvironmentsForLineno{multline}}




\begin{document}
\title{Distributed Agreement in Dynamic Peer-to-Peer Networks\footnote{A preliminary version of this paper appeared in the Proceedings of the  ACM/SIAM Symposium on Discrete Algorithms (SODA), 2012, 551-569.}}
\author{John Augustine\thanks{Department of Computer Science and Engineering, Indian Institute of Technology Madras, Chennai, India.  \hbox{E-mail}:~{\tt augustine@cse.iitm.ac.in}. Work done while at the 
Division of Mathematical Sciences, Nanyang Technological University, Singapore 637371.}  \and Gopal Pandurangan \thanks{Division of Mathematical
Sciences, Nanyang Technological University, Singapore 637371 and Department of Computer Science, Brown University, Box 1910, Providence, RI 02912, USA.  \hbox{E-mail}:~{\tt gopalpandurangan@gmail.com}. Work supported in part by the following grants: Nanyang Technological University grant M58110000, Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 2 grant MOE2010-T2-2-082,
US NSF grant CCF-1023166, and a grant from the US-Israel Binational Science
Foundation (BSF).} \and Peter Robinson \thanks{Department of Computer Science, National University of Singapore. \hbox{E-mail}:~{\tt robinson@comp.nus.edu.sg}}
\and Eli Upfal\thanks{Department of Computer Science, Brown University, Box 1910,
Providence, RI 02912, USA. \hbox{E-mail}:~{\tt eli@cs.brown.edu}}}

\date{}

\maketitle



\begin{abstract}
Motivated by the need for robust and fast distributed computation in highly dynamic Peer-to-Peer (P2P)
networks, we study algorithms for  the fundamental  distributed agreement problem.   P2P
networks  are  highly dynamic networks  that experience heavy
node {\em  churn} (i.e., nodes  join and leave the network continuously over time). Our
goal  is to design fast  algorithms (running in a small number of rounds) that
guarantee, despite high node churn rate,  that almost all nodes   reach a stable
agreement.  Our main contributions are randomized  distributed algorithms that
guarantee   {\em stable almost-everywhere agreement} with high probability even under
high adversarial churn in a polylogarithmic number of rounds. In particular, we
present the following results:
\begin{enumerate}
\item An -round  ( is the stable network size) randomized
  algorithm that achieves almost-everywhere agreement with high probability
  under up to {\em linear} churn  {\em per round} (i.e., , for some small
  constant ), assuming that the churn is controlled by an
  oblivious adversary (that has  complete knowledge and control of what nodes join
  and leave and  at what time  and has unlimited computational power, but is
  oblivious to the random choices made by the algorithm). Our algorithm requires 
only polylogarithmic  in   bits to be processed and sent (per round) by each node.
\item An  -round randomized algorithm that achieves
  almost-everywhere agreement with high probability under up to  churn  per round (for some small ), where  is the size of the input value domain, that works even under an adaptive adversary
  (that also knows the past random choices made by the algorithm). This algorithm requires up to polynomial in  bits (and up to  bits)
to be processed and sent (per round) by each node. 
\end{enumerate}


Our algorithms are the first-known, fully-distributed,  agreement algorithms that work under highly dynamic settings (i.e., high churn rates per step). Furthermore, they are  localized (i.e., do not require any global topological knowledge), simple, and easy to implement. 
 These algorithms can serve as building blocks for implementing other non-trivial
distributed computing tasks in dynamic P2P networks. 
\end{abstract}








\section{Introduction}

Peer-to-peer (P2P) computing is emerging as one of the key networking technologies in recent years with many application systems, e.g., Skype, BitTorrent, Cloudmark etc. However, many of these  systems are not truly P2P, as they are not
fully decentralized --- they typically use hybrid P2P along with centralized intervention.
For example, Cloudmark  \cite{Cloudmark} is a large spam detection system used by millions of people that operates by maintaining
a hybrid P2P network; it uses a central authority to regulate and charge users for participation in the network.
A key reason for the lack of fully-distributed P2P systems is  the difficulty in designing highly robust algorithms for large-scale dynamic P2P networks. Indeed, P2P networks  are  highly dynamic  networks  characterized by  high degree of node {\em  churn}  --- i.e., nodes continuously join and leave the network. Connections (edges) may be added or deleted at any time and thus the topology changes very dynamically. In fact,  
measurement studies of real-world P2P networks~\cite{FPJKA07,SGG02,SW02, SR06} show that the churn rate is quite high:
nearly 50\% of peers in real-world networks can be replaced within an hour. (However,  despite a large churn rate, these studies  also show that the
total number of peers in the network is relatively {\em stable}.)  We note that peer-to-peer algorithms have been proposed
 for a wide variety
of  computationally challenging tasks such as collaborative
filtering~\cite{Canny02}, spam detection~\cite{Cloudmark},
data mining~\cite{DBGWK06}, worm detection and
suppression~\cite{MS05,VAS04}, and privacy protection of archived
data~\cite{GKLL09}. However, all algorithms proposed for these
problems have no theoretical guarantees of being able to
work in a network with a dynamically changing topology and a linear churn rate per round. This is  a major bottleneck in implementation and
wide-spread use of these algorithms.

In this paper, we take a step towards  designing robust  algorithms for large-scale dynamic
peer-to-peer networks. In particular, we  study the fundamental  distributed agreement problem in  P2P networks (the formal problem statement and model is given in Section \ref{sec:model}). An efficient solution to the agreement problem can be used as a building block for robust and efficient solutions to other problems as mentioned above. 
However, the distributed agreement problem in P2P networks is challenging since the goal
is to guarantee {\em almost-everywhere} agreement, i.e., almost all nodes
\footnote{In sparse, bounded-degree networks,
an adversary can always isolate some number of non-faulty nodes, hence almost-everywhere is
the best one can hope for in such networks \cite{DPPU88}.} 
should reach consensus,  even under high churn rate. The churn rate
can be as much as linear  {\em per time step (round)},  i.e., up to a constant fraction of the stable network size can be replaced per time step.  
Indeed, until recently, almost all the work 
 known in the literature (see e.g., \cite{DPPU88,KKKSS10, KS10,KSS06, Upfal94})  have addressed the almost-everywhere agreement problem only in
 static (bounded-degree) networks and these approaches do not work for dynamic networks with changing topology. 
Such approaches fail in dynamic networks where both nodes {\em and} edges can change by a large amount
in {\em every} round.
   For example, the work of Upfal \cite{Upfal94} showed how
 one can achieve almost-everywhere agreement under up to a {\em linear} number --- up to , for a sufficiently small  ---  of Byzantine faults in a bounded-degree expander network ( is the network size).  The algorithm required  rounds and polynomial (in ) number of messages; however,  the local computation required by
each processor is exponential.  Furthermore, the algorithm  requires  knowledge
of the global topology, since at the start, nodes need to have this information
``hardcoded''.
 The work of King et al. \cite{KSSV06} is important in the context of P2P networks, as it was the first to study scalable (polylogarithmic communication and number of rounds) algorithms for distributed agreement (and leader election)
that are tolerant to Byzantine faults. However, as pointed out by the authors, their algorithm works only for static networks; similar to Upfal's algorithm, the nodes require 
hardcoded information on the network topology to begin with  and thus the
algorithm does not work when the topology changes.  In fact, this work (\cite{KSSV06})
raises the open question of whether one can design agreement  protocols that
can work in highly dynamic networks with a large churn rate.


\iffalse
\subsection{Distributed Agreement Problem} 
Formally, in the distributed agreement
problem, we are given a distributed network of processors and each
processor begins with an initial value drawn from some domain of possible
values. A processors  can repeatedly exchange messages with its neighbors
and perform local computations and  during some point in the computation,
each non-faulty processor must irreversibly decide on a value, subject to
two core conditions: (1) No two non-faulty processors may decide on
different values; (2) if a non-faulty processors decides on some value
, then some processor had  as its input value.  The ability to
achieve this type of agreement (or consensus) is important in a wide range
of applications, such as database management, fault-tolerant analysis of
aggregate data, and coordinated control of multiple agents or peers. 
There is a long line of research on various versions of the agreement
problem with many important results (see e.g., \cite{Lyn96, AW04}
and the references therein). 
The work of Dwork et al. \cite{DPPU88} studied the Byzantine agreement problem (where faulty processes can be malicious) in bounded-degree  networks
under the condition of {\em almost-everywhere} agreement. In almost-everywhere agreement, only {\em almost} all (non-faulty) processors need to reach agreement as opposed to all in everywhere agreement. 
Almost-everywhere agreement  is  necessary if one needs to tolerate a substantial number of faults in
distributed networks of bounded degree, while everywhere agreement 
is too restrictive as it needs high network connectivity to tolerate a
large number of faults \cite{Dolev82}. In particular, in the context of emerging large-scale networks (e.g., peer-to-peer networks, data center networks, and sensor networks), the faulty processes will grow with the size of the network, while the degree of
interconnection will remain essentially fixed.  Moreover, in the 
\fi



\subsection{Our Main Results}
 Our first contribution is a
rigorous theoretical framework for the design and analysis of algorithms for
highly dynamic distributed systems with churn. We briefly describe the key ingredients of our model here. (Our model is described in detail in  Section
\ref{sec:model}.) Essentially, we model a P2P network as a bounded-degree expander graph   whose topology --- both nodes and edges ---  can change arbitrarily from round to round and is controlled by an adversary. However, we assume
that the total number of nodes in the network is stable.  The number of node
changes {\em per round} is called the {\em churn rate} or {\em churn limit}. We
consider a churn rate of up to some ,
where  is the stable network size. Note that our model is quite general in the
sense that we only assume that the topology is an expander at every step; no other special
properties are assumed. Indeed,  expanders have been used extensively to
model dynamic P2P networks\footnote{Expander graphs
have been used extensively as candidates to solve the agreement and related problems
in bounded degree graphs even in static settings (e.g., see \cite{DPPU88,KKKSS10, KS10,KSS06, Upfal94}).  Here we show that similar expansion properties are beneficial in the more challenging setting of dynamic networks.}   in which the expander property is preserved under insertions and
deletions of nodes (e.g., \cite{LS03,PRU01}). 
 Since we do not make assumptions on how the topology is preserved,
 our model is applicable to all such expander-based networks.  (We note that  various prior work on  dynamic network models make similar assumptions on preservation of  topological  properties  --- such as connectivity, expansion etc. ---
 at every step  under dynamic {\em edge} insertions/deletions --- cf. Section \ref{sec:related}.  The issue
 of how such properties are preserved are abstracted away from the model, which allows one to focus on
 the dynamism. Indeed, this abstraction has been a feature of most dynamic
 models e.g., see the survey  of \cite{santoro}.) 
 
We study stable, almost-everywhere,  agreement in our model. By ``almost-everywhere'',
we mean that almost all nodes, except possibly  nodes (where  is the order of the churn and  is a suitably small constant --- cf. Section~\ref{sec:model}) should reach agreement on a common value.
(This agreed value must be the input value of some node.) By ``stable'' we mean that the agreed value is preserved subsequently after the agreement is reached.
 
Our main contribution is the design and analysis of randomized  distributed
algorithms that guarantee   stable almost-everywhere agreement with high
probability (i.e., with probability , for an arbitrary fixed constant ) even under high
adversarial churn in a polylogarithmic number of
rounds.  Our algorithms also  guarantee stability once agreement has been
reached.  In particular, we present the following
results (the precise theorem statements are given in the respective sections
below):
\begin{enumerate}
\item (cf.\ Section \ref{sec:oblivious}) An -round  ( is the stable
network size) randomized algorithm that achieves almost-everywhere
agreement with high probability under up to {\em linear} churn {\em per round}
(i.e., , for some small constant ), assuming
that the churn is controlled by an oblivious adversary (that has complete
knowledge of what nodes join and leave and  at what time, but is oblivious
to the random choices made by the algorithm). Our algorithm requires 
only polylogarithmic  in   bits to be processed and sent (per round) by each node.

\item (cf.\ Section \ref{sec:adaptive}) An  -round
randomized algorithm that achieves almost-everywhere agreement with high
probability under up to  churn {\em per round}, for some
small , that works even under an adaptive adversary (that also
knows the past random choices made by the algorithm). Here  refers
to the size of the domain of input values. This algorithm requires up to polynomial in  bits (and up to  bits)
to be processed and sent (per round) by each node. 

\item (cf.\ Section \ref{sec:impossibility}) We also show that no
deterministic algorithm can guarantee almost-everywhere agreement
(regardless of the number of rounds), even under constant churn rate. 
\end{enumerate}

To the best of our knowledge, our algorithms are the first-known,
fully-distributed,  agreement algorithms that work under highly dynamic
settings. Our algorithms are  localized (do not require any global topological
knowledge), simple, and easy to implement. 
 These algorithms can serve as building blocks for implementing other non-trivial
distributed computing tasks in P2P networks. 

\onlyShort{
Due to lack of space, the full proofs are in the full paper \cite{APRU11}.
}

\subsection{Technical Contributions}
The main technical challenge that we have to overcome is designing and analyzing
distributed algorithms in networks where both nodes and edges can change by a large amount.
Indeed, when the churn rate is linear, i.e., say  per round, in constant () number of rounds the entire network can be renewed!
 
We derive techniques for information spreading  (cf.\ Section
\ref{sec:techniques}) for doing non-trivial distributed computation in such networks. The first technique that we use
is flooding.   We show that in an expander-based
P2P network even under linear churn rate, it is possible to spread
information by flooding if  sufficiently many (a -fraction of the
order of the churn)
nodes initiate the information spreading (cf.~Lemma~\ref{lem:beta}). In other words,  even an adaptive adversary 
cannot ``suppress'' more than  a small fraction
of the values. The precise statements
and proofs are in Section \ref{sec:techniques}. 

To analyze these flooding techniques we introduce the dynamic
distance, which describes the effective distance between two nodes with respect
to the causal influence. We define the notions of influence sets and dynamic
distance (or flooding time) in dynamic networks with node churn.  (Similar
notions have been defined for dynamic graphs with
a fixed set of nodes, e.g., \cite{kuhn-survey,BCF09}).  In (connected) networks
where the nodes are fixed, the effective diameter (e.g., \cite{kuhn-survey}) is
always finite. In the highly dynamic setting considered here, however, the
effective distance between two nodes might be infinite, thus we need a more
refined definition for influence set and dynamic distance.

The second technique that we use is ``support estimation'' (cf.\ Section
\ref{sec:support}). Support estimation is a randomized technique that allows us to estimate the aggregate count (or sum)
of values of all or a subset of nodes in the network. Support estimation is done in conjunction with flooding
and uses properties of the exponential distribution (similar to \cite{Cohen97, AoyamaS08}).
Support estimation allows us to estimate the aggregate value quite precisely
with high probability even under linear churn. But this works only for an
oblivious adversary; to get similar results for the adaptive case, we need to
increase the amount of bits that can be processed and sent by a node in every
round. 

Apart from support estimation, we also use our flooding techniques in the
agreement algorithm for the oblivious case (cf.\ Algorithm~\ref{alg:bc}) to sway
the decision one way or the other. \onlyLong{For the adaptive case (cf.\
Algorithm~\ref{alg:adaptive}), we use the variance
property of a certain probability distribution to achieve the same effect with
constant probability.}

\subsection{Other Related Work}
\label{sec:related}
\subsubsection{Distributed Agreement}
The distributed agreement (or consensus) problem is important in a wide range
of applications, such as database management, fault-tolerant analysis of
aggregate data, and coordinated control of multiple agents or peers. 
There is a long line of research on various versions of the 
problem with many important results (see e.g., \cite{AW04,Lyn96}
and the references therein). 
The relaxation of achieving agreement ``almost everywhere'' was introduced
by \cite{DPPU88} in the context of fault-tolerance in networks of bounded degree where all
but  nodes achieve agreement despite  faults.
This result was improved by \cite{Upfal94}, which showed how to guarantee
almost everywhere agreement in the presence of a linear fraction of faulty
nodes. Both the work of \cite{DPPU88, Upfal94} crucially use expander graphs to show their results. We also refer to the related results of Berman and Garay on the
butterfly network~\cite{BG93}. 

\subsubsection{Byzantine Agreement}
We note that Byzantine adversaries are quite different from the adversaries considered in this paper.  A Byzantine
adversary can have nodes behaving arbitrarily, but no new nodes  are added (i.e., no churn),
whereas in our case (an external) adversary  controls the churn and topology of
the network but {\em not} the behavior of the nodes. Despite this difference it is worthwhile to mention that
there has been significant work in designing peer-to-peer networks that
are provably robust to a large number of Byzantine
faults~\cite{FS02,HK03,NW03,Scheideler05}. These focus only on robustly enabling storage and retrieval of data items. 
The problem of achieving almost-everywhere agreement among nodes in P2P networks (modeled as an expander graph) is
considered by King et al.\ in \cite{KSSV06} in the context of the leader
election problem; essentially, \cite{KSSV06} is a sparse (expander) network
implementation of the full information protocol of \cite{KSS06}.  
More specifically, \cite{KSSV06} assumes that the
adversary corrupts a constant fraction  of the
processes that are under its control throughout the run of the algorithm.
The protocol of \cite{KSSV06} guarantees that with constant probability an
uncorrupted leader will be elected and that a 
fraction of the uncorrupted processes know this leader. Again, we note that the
failure assumption of \cite{KSSV06} is quite different from the one we use: Even though we do not assume corrupted nodes, the adversary is free
to subject different nodes to churn in every round. Also note that
the algorithm of \cite{KSSV06} does not work for dynamic networks. 

Other works on handling Byzantine nodes in the context of P2P networks include \cite{Scheideler05,awerbuch:group,fiat:dynamically,fiat:making,awerbuch:random,castro:secure,young:practical}.

In \cite{podc13}, we have developed an almost-everywhere agreement algorithm that tolerates up to  churn and  churn per round, in a dynamic network model.


\subsubsection{Dynamic Networks}
Dynamic networks have been studied extensively over the past three
decades.  Some of the early studies focused on dynamics that arise out
of faults, i.e., when edges or nodes fail.  A number of fault models,
varying according to extent and nature (e.g., probabilistic
vs.\ worst-case) and the resulting dynamic networks have been analyzed
(e.g., see~\cite{AW04,Lyn96}).  There have
been several studies on models that constrain the rate at which
changes occur, or assume that the network eventually stabilizes (e.g.,
see~\cite{afek+ag:dynamic,dolev:stabilize,gafni+b:link-reversal}). 
Some of the early work on general dynamic networks
include~\cite{afek+gr:slide,awerbuch+pps:dynamic} which introduce
general building blocks for communication protocols on dynamic
networks.  Another notable work is the local
balancing approach of~\cite{awerbuch+l:flow} for solving routing and
multicommodity flow problems on dynamic networks.
Most of these papers develop algorithms that will work under the assumption that 
the network will eventually stabilize and stop changing.

Modeling general dynamic networks has gained renewed attention with
the recent advent of heterogeneous networks composed out of ad hoc,
and mobile devices. To address highly unpredictable network dynamics, stronger adversarial
models have been studied
by~\cite{avin+kl:dynamic,disc12,OW05,KOM11} and others;
see the recent survey of \cite{santoro} and the references therein. The works of  \cite{KOM11, avin+kl:dynamic, disc12} study a model
in which the communication graph can change completely from one round
to another, with the only constraint being that the network is
{\em connected at each round} (\cite{KOM11} and \cite{disc12} also consider a stronger model where the constraint is that
the network should be an expander or should have some specific expansion  in each round).
The model has also been applied to agreement problems in dynamic networks;
various versions of coordinated consensus (where all nodes must agree) have been
considered  in \cite{KOM11}. 
  The recent work of \cite{clementi-podc12},
studies the flooding time of {\em Markovian} evolving dynamic graphs,
a special class of evolving graphs.
 
We note that the model of~\cite{kuhn+lo:dynamic}
allows only edge changes from round to round while the nodes remain
fixed. In this work, we introduce a dynamic
network model  where both nodes and edges
can change by a large amount (up to a linear fraction of the network
size).  
Therefore, the framework we introduce in Section~\ref{sec:model} is more general
than the model of \cite{kuhn+lo:dynamic}, as it is additionally applicable to dynamic
settings with node churn.
The same is true for the notions of dynamic distance and influence
set that we introduce in Section~\ref{sec:dynamic}, since in our model the
dynamic distance is not necessarily finite. In fact, according to
\cite{kuhn-survey}, coping with churn is one of the important open problems in the
context of dynamic networks.  Our paper takes a step in this direction.

An important aspect of our algorithms  is that they will work and terminate correctly even when the
  network keeps continually changing.  We note that there has been
considerable prior work in dynamic P2P networks (see \cite{PRU01} and the references therein) but these do not assume that
the network keeps continually changing over time.







Due to the mobility of nodes, mobile ad-hoc networks can also be considered as
dynamic networks. The focus of \cite{OW05} are the minimal requirements
that are necessary to correctly perform flooding and routing in highly dynamic
networks where edges can change but the set of nodes remains the same. In the
context of agreement problems, electing a leader among mobile nodes that may
join or leave the network at any time is the focus of \cite{CRW11}. To make
leader election solvable in this model, Chung et al.\ introduce the notion of
-connectedness, which ensures information propagation among all nodes that
remain long enough in the network. Note that, in contrast to our model, this
assumption prohibits the adversary from permanently isolating parts of the
network. The recent work of~\cite{haeupler+k:dynamic} presents
information spreading algorithms on dynamic networks based on network
coding~\cite{ahlswede+cly:coding}. 

\subsubsection{Fault-Tolerance}

In most work on fault-tolerant agreement problems 
the adversary a priori commits to a fixed set of faulty nodes.
In contrast, \cite{DGMSS11} considers an adversary that can corrupt the
state of some (possibly changing) set of  nodes in every round.
The median rule of \cite{DGMSS11} provides an elegant way to ensure that
most nodes stabilize on a common output value within  rounds,
assuming a complete communication graph. The median rule, however, only
guarantees that this agreement lasts for some polynomial number of rounds,
whereas we are able to retain agreement ad infinitum.

Expander graphs and spectral properties have already been applied
extensively to improve the network design and fault-tolerance in
distributed computing (cf.\ \cite{Upfal94,DPPU88,BBCES2006}).
Law and Siu\ \cite{LS03} provide a distributed algorithm for maintaining an expander in
the presence of churn with high probability by using Hamiltonian cycles.
In \cite{PT11} it is shown how to maintain the expansion property of a network
in the self-healing model where the adversary can delete/insert a new node in
every step.
In the same model, \cite{IPDPS14} present a protocol that maintains constant node degrees and constant expansion (both with probability ) against an adaptive adversary, while requiring only logarithmic (in the network size) messages, time, and topology changes per deletion/insertion.
In \cite{skipexp}, it is shown that a SKIP graph (cf.\ \cite{skip}) contains a constant degree expander as a subgraph with high probability.
Moreover, it requires only constant overhead for a node to identify its incident edges that are part of this expander.
Later on, \cite{skipplus} presented a self-stabilizing algorithm that converges from any weakly connected graph to a SKIP graph in time polylogarithmic in the network size, which yields a protocol that constructs an expander with high probability.
In  \cite{hyperring} the authors introduce the hyperring, which is a search data structure supporting insertions and deletions, while being able to handle concurrent requests with low congestion and dilation, while guaranteeing  expansion and  node degree.
The -Flipper algorithm of \cite{mahlmann} transforms any undirected graph into an expander (with high probability) by iteratively performing flips on the end-vertices of paths of length .
Based on this protocol, the authors describe how to design a protocol that supports deletions and insertions of nodes.
Note that, however, the expansion in \cite{mahlmann} is only guaranteed with high probability however, assuming that the node degree is .

Information spreading in distributed networks is the focus of
\cite{CH:PODC10} where it is shown that this problem requires 
rounds in graphs with a certain conductance in the push/pull model 
where a node can communicate with a
randomly chosen neighbor in every round. 

Aspnes et al.\ \cite{ARS07} consider information spreading via expander
graphs against an adversary, which is related to the flooding
techniques we derive in Section~\ref{sec:techniques}. More specifically,
in \cite{ARS07} there are two opposing parties ``the alert'' and ``the
worm'' (controlled by the adversary) that both try to gain control of the
network. In every round each alerted node can alert a constant number of
its neighbors, whereas each of the worm nodes can infect a constant
number of non-alerted nodes in the network. In \cite{ARS07}, Aspnes et al.\ show that there is a simple strategy to prevent all
but a small fraction of nodes from becoming infected and, in case that the
network has poor expansion, the worm will infect almost all nodes.



The work of \cite{BBCES2006} shows that, given a network that is initially
an expander and assuming some linear fraction of faults, the remaining
network will still contain a large component with good expansion.  These
results are not directly applicable to dynamic networks with large amount
of churn like the ones we are considering, as the topology might be
changing and linear churn per round essentially corresponds to
 total churn after  rounds---the minimum amount
of time necessary to solve any non-trivial task in our model.

In the context of maintaining properties in P2P networks, Kuhn et al.\
consider in \cite{KSW10} that up to  nodes can crash or join per constant number of time
steps. Despite this amount of churn, it is shown in \cite{KSW10} how to maintain a low peer
degree and bounded network diameter in P2P systems by using the hypercube and
pancake topologies.  Scheideler and Schmid show in \cite{SS09} how to maintain a
distributed heap that allows join and leave operations and, in addition, is
resistent to Sybil attacks. A robust distributed implementation of a distributed
hash table (DHT) in a P2P network is given by \cite{AS09}, which can withstand
two important kind of attacks: adaptive join-leave attacks and adaptive
insert/lookup attacks by up to  adverserial peers. 
Note that, however, that collisions are likely to occur once the number of attacks becomes .




\section{Model and Problem Statement} \label{sec:model}

We are interested in establishing  stable agreement in a
dynamic peer-to-peer network in which the nodes and the edges change over time.
The computation is structured into synchronous rounds, i.e., we assume that
nodes run at the same processing speed and any message that is sent by some node
 to its (current) neighbors in some round  will be received by the end of .
To ensure scalability, we restrict the number of bits sent per round by each
node to be polylogarithmic in the size of the input value domain (cf.\
Section~\ref{sec:sa}). For dealing with the
much more powerful adaptive adversary, we relax this requirement in
Sections~\ref{sec:supportAdaptive} and \ref{sec:adaptive}.
We model dynamism in the network as a family of undirected graphs . At the beginning of each round  we start with the network topology
. Then, the adversary gets to change the network from  to
 (in accordance to rules outlined below). 
As is typical, an
edge  indicates that  and  can communicate in round  by
 passing messages. 
For the sake of
readability, we use  as a shorthand for
 
Each node  has a unique identifier and is {\em churned in} at some round
 and {\em churned out} at some . More precisely, for each node
, there is a maximal range  such that  and
for every , . Any information about the
network at large is only learned through
the messages that  receives. It has no a priori knowledge about who its neighbors will be in
the future. Neither does  know when (or whether) it will be churned out. 
Note that we do not assume that nodes have access to perfect clocks\onlyLong{,
but we show (cf.  Section~\ref{subsec:maintain}) how the nodes can synchronize
their clocks}. 

We make the following assumptions about the kind of changes that our dynamic
network can encounter:

\begin{description}
\item[Stable Network Size:] For all , , where  is a suitably
  large positive integer.  This assumption simplifies our analysis. Our algorithms will work correctly as long as the number of nodes is reasonably stable (say, between  and  for some suitably small constant  ). Also, we assume that  (or a constant factor estimate of ) is common knowledge among the nodes in
  the network\footnote{This assumption is important; estimating  accurately in our model is an interesting problem in itself.}.
\item[Churn:]  For each , 

where  is the {\em churn limit}, which is some fixed  fraction of the
\emph{order of the churn} ; the equality in the above equation
ensures that the network size remains stable.  Our work is aimed at high levels
of churn up to a churn limit  that is linear in , i.e., . 
\item[Bounded Degree Expanders:] The sequence of graphs  is an
  expander family with a vertex expansion of at least , which is a fixed positive constant.\footnote{Note that the value of  determines , i.e.\ the fraction of churn that we can tolerate. In particular, to tolerate
 linear amount of churn, we require constant expansion.  In principle, our results can potentially be extended to graphs with weaker expansion guarantees as well; however the amount of churn that can be tolerated will
 be reduced.}
  In
  other words, the adversary must ensure that for every  and  every  such that ,
  the number of nodes in  with a neighbor in  is at least
  .   
  Note that we do not explicitly consider the costs (communication and computation) of maintaining an expander under churn.
  Instead, we assume that the duration of each time step in our model are normalized to be large enough to encompass an expander maintenance protocol such as \cite{LS03,IPDPS14}.
  
\end{description}

A run of a distributed algorithm consists of an infinite number of
rounds. 
We assume that the following events occur (in order) in every round :
\begin{enumerate}
    \item  A set of at most  nodes are churned in and another set of 
      nodes are churned out. The edges of  may be changed as well, but
       has to have a vertex expansion of at least . These changes
      are under the control of the adversary.
\item  The nodes broadcast messages to their (current) neighbors. 
    \item  Nodes receive messages broadcast by their neighbors.
    \item  Nodes perform computation that can change their state and determine
      which messages to send in round .
		\end{enumerate}


\subsection*{Bounds on Parameters}
Recall that the churn limit , where  is a constant and  is the churn order. When ,   is the fraction of the nodes churned out/in and therefore we require   to be less than 1 and must adhere to Equation~\eqref{eq:beta}.
Moreover, we require the bound  regarding the right hand side of \eqref{eq:beta}.
However, when ,   can exceed 1. In the remainder of this paper, we consider  to be a  small
constant independent of , such that


It will become apparent in Section~\ref{sec:techniques} that \eqref{eq:beta} presents a sufficient condition for preventing the adversary from containing the information propagated by a set of  nodes.

and that the \emph{churn expansion ratio}
 presents a sufficient condition for information
propagation in our model (cf.\ Lemma~\ref{lem:beta}).
Finally, we assume that  is suitably large (cf. Equations~\ref{eq:betaUpperOblivous}~and~\ref{eq:numberOfNodesAdaptive}). 

\subsection{Stable Agreement} \label{sec:sa}
We now define the {\sc Almost Everywhere} \sa\ problem (or just the \sa\ problem for brevity). Each node  has an associated input
value from some value domain of size ; subsequent new nodes come with value . 
Let  be the set of all input values associated with nodes in  at the start of round
1. Every node  is equipped with a special decision variable  (initialized to ) that can be written at
most once. We say that a node  \emph{decides on } when  assigns  to its . Note that this decision is
irrevocable, i.e., every node can decide at most once in a run of an algorithm. As long as , we say that  is {\em undecided}.
\sa\ requires that a large fraction of the nodes come to a stable
agreement on one of the values in . More precisely, \emph{an algorithm solves
\sa\ in  rounds}, if it exhibits the following characteristics in every run,
for any fixed  adhering to \eqref{eq:beta}.
\begin{description}
\item[Validity:] If, in some round , node  decides on a value
  , then . 
\item[Almost Everywhere Agreement:] We say that \emph{the network has
  reached strong almost everywhere agreement by round }, if at least  nodes in  have decided on the same value  and every
  other node remains undecided, i.e., its decision value is . In particular, no
  node ever decides on a value  in the same run, for .
\item[Stability:] Let  be the earliest round where nodes have reached almost
  everywhere agreement on value . We say that an algorithm \emph{reaches stability by round } if, at every
  round , at least  nodes in  have decided on
  .
\end{description}
We also consider a weaker variant of the above problem that we call 
{\sc Almost Everywhere} \bc\ (or simply, \bc) where
the input values in  are restricted to .


We consider two types of adversaries for our randomized algorithms. An
\emph{oblivious} adversary  must commit in advance to the entire sequence of
graphs . In other
words, an oblivious adversary must commit independently of the random choices
made by the algorithm. 
We also consider the more powerful
  \emph{adaptive} adversary that can observe the entire state of the network in
  every round  (including all the random choices made until  round ), and then chooses the nodes to be churned out/in and how to
  change the topology of .

For the sake of readability, we treat  as an integer and omit the
necessary ceiling or floor operations if their application is clear from the
context.










\section{Techniques for Information Spreading} \label{sec:techniques}
In this section, we first derive and analyze techniques to spread information in the network despite churn.
First, we show that the adversary is unable to prevent a sufficiently large set of nodes (of size at least ) to propagate their information to almost all other nodes (cf.\ Lemma~\ref{lem:beta}). 
Building on this result, we analyze the capability of individual nodes to spread their information.
We show in Lemma~\ref{lem:supp} and Corollary~\ref{cor:suppress} that at most  nodes can be hindered by the adversary.
Finally, we show in Lemmas~\ref{lem:UniversalInfluence} and \ref{lem:UniversalInfluence2} that there is a large set of nodes  such that all nodes in  are able to propagate their information to a large \emph{common} set of nodes.

In Sections~\ref{sec:support} and \ref{sec:supportAdaptive}, we describe how to use the previously derived techniques on information spreading to estimate the ``support'' (i.e.\ number) of nodes that belong to a specific category (either red or blue).
These protocols will form a fundamental building block for our \sa\ algorithms.

Due to the high amount of churn and the dynamically changing network, we use message flooding to disseminate and gather information. We now
precisely define flooding. Any node can initiate a  message for flooding.
Messages that need to be flooded have an indicator bit {\sc bFlood} set to 1.
Each of these messages also contains a terminating condition.  The initiating
node sends copies of the message to itself and its neighbors. When a node receives a
message with {\sc bFlood} set to 1, it continues  to send copies of that message to itself
and its neighbors in subsequent rounds until the terminating condition is
satisfied. 


\subsection{Dynamic Distance and Influence Set} \label{sec:dynamic}
Informally, the dynamic distance from node  to node  
is the number of rounds required for a message at  to reach .
We now formally define the notion of {\em dynamic distance} of a node  from  starting at
round , denoted by . When the subscript  is omitted, we
 assume that . 

Suppose node  joins the network at round , and,
from round  onward,  initiates a message  for flooding whose
terminating condition is: {\sc v}. If  is churned out before
, then  is undefined. Suppose  the first of those flooded
messages reaches  in round . Then, .
Note that this definition allows  to be infinite under two
scenarios. Firstly, node  may be churned out before any copy of  reaches
. Secondly, at each round,  can be shielded by churn nodes that absorb the
flooded messages and are then removed from the network before they can propagate
these messages any further. The influence set of a node  after  rounds
starting at round  is given by: 

Note that we require  . Intuitively, we want
the influence set of  (in this dynamic setting) to capture the nodes
\emph{currently} in the network that were influenced by .
Note however  that the influence set of a node  is meaningful even after  is churned out.
Analoguously, we define 

for any set of nodes .


If we consider only a single node , an (adaptive) adversary can easily prevent the
influence set of this node from ever reaching any significant size by simply
shielding  with churn nodes that are replaced in every round.\footnote{An
oblivious adversary can achieve the same effect with constant probability for
linear churn.}  


\subsection{Properties of Influence Sets}
We now focus our efforts on characterizing influence sets. This will help us in understanding how we can use flooding to spread information in the network.
For the most part of this section we assume that the network is controlled by an
adaptive adversary (cf.\ Section~\ref{sec:sa}).
The following lemma shows that the number of nodes that are sufficient to influence almost all the nodes in the network is given by the churn-expansion ratio (cf.\ Equation\ \eqref{eq:beta}):

\begin{lemma}\label{lem:beta}
Suppose that the adversary is adaptive. Consider any set  (for
any ) such that .  
Then, after
 
number of rounds, it holds that

When considering linear churn, i.e., , the bound  becomes a constant
independent of . On the other hand, when considering a churn order of
, we get .
\end{lemma}
\longOnly{
\begin{proof}
Our proof assumes that  for simplicity as the arguments extend quite easily to arbitrary values of .
We proceed in two parts: First we show that the  nodes in   influence at least 
nodes in some  rounds. More precisely, we show that . We  use vertex expansion in a
straightforward manner to establish this part. Then, in the second part we show
that nodes in  go on to influence more than  nodes.   We cannot use the vertex expansion in a straightforward manner in the second part because the cardinality of the set that is expanding in influence is larger than . Rather, we use a slightly more subtle argument in which we use vertex expansion going backward in
time. The second part requires another  rounds. Therefore, the two parts together complete the proof when we set . 

To begin the first part, consider  at the start of round 1 with . 
In round , up to  nodes in  can be churned out. Subsequently, the remaining  nodes in  influence some nodes outside  as  is
an expander with vertex expansion at least . More precisely, we can say that
 At the start of round , the graph
changes dynamically to . In particular, up to  nodes  might be
churned out and they may all be in  in the worst case. However, the influenced set will again expand. Therefore,  cannot be less than
. Of course, there will be more churn at the start of round 3 followed by expansion leading to:

This cycle of churn  followed by expansion continues and we get the following
bound at the end of some round :


Therefore, after 
 
rounds, we get 


Now we move on to the second part of the proof. Let .
If , we are done.
Therefore, for the sake of a contradiction,  assume that  . Let , i.e.,  is the set of nodes in  that were not influenced by  at (or before) round . 
Moreover,  because we have assumed that . We will start at round  and work our way
backward. For , let , be the set of
all vertices in  that, starting from round , influenced  some vertex in  at or
before round . More precisely, 
Suppose that . Then 

since  by
\eqref{eq:part1}. Consider a node
. Note that  was influenced by  and went
on to influence some node in  before (or at) round . However, by definition, no node  in  can be influenced by any node in  at or before round . We have thus reached a contradiction. 

We are left with showing that .
We start with  and work our way backwards. We know that . We want to compute the cardinality of . We
first focus on an intermediate set , which we define as 
 
Since  is an expander, . 
Furthermore, it is also clear that each node in  could influence some node in . 
Notice that  is the set of nodes in  that were churned in only at the start of round . 
Therefore, 

Continuing to work our way backwards in time, we get

Or more generally,

We now want the value of  for which  In other words, we want a value of  such that

which is obtained when . Therefore, it is
easy to see that if we set , we get , thereby completing the proof. 
\qed
\end{proof}
}
At  first glance, it might appear to be counterintuitive that the order of the
bound  decreases with increasing churn. When the adversary has the benefit of churn that is linear in , our bound on  is a constant, but when the adversary is limited to a churn order of , we get 
. This, however, turns out to be fairly
natural when we note that the size of the set  of nodes that we start out with is in proportion to the churn limit.

We say that a node  is \emph{suppressed for  rounds} or \emph{shielded by churn} if ; otherwise we say it is \emph{unsuppressed}. The following lemma shows
that given a set with cardinality at least  some node in that set will be
unsuppressed. 



\begin{lemma}\label{lem:supp}
Consider the adaptive adversary.
Let  be any subset of , , such that .
Let  be the bound derived in
Lemma~\ref{lem:beta}. There is at least one  such that for some ,  is unsuppressed, i.e.,  

In particular, when the
order of the churn is ,  becomes a constant, and we have . 
\end{lemma}
\onlyLong{
Before we proceed with our key arguments of the proof, we state a property of bipartite graphs that we will use subsequently.
\begin{property} \label{prop:bipartite}
Let  be a bipartite graph in which  and every vertex  has at least one neighbor in . There is a subset  of cardinality at most 
 such that  

\end{property}
\begin{proof}(of Property~\ref{prop:bipartite})
Consider each node in  to be a unique color. Color each node in  using the color of a neighbor in  chosen arbitrarily. Now partition  into maximal subsets of nodes with like colors. 
Consider the parts of the partition sorted in decreasing order of their cardinalities. 
We  now greedily choose the first   colors in the sorted order of parts of . We call the chosen colors . Observe that colors in  cover at least as many nodes in  as those not in .
Suppose the colors in   cover fewer than  nodes in .
Then the remaining colors will cover , but that is a
contradiction. Therefore, colors in  cover at least 
nodes in . The nodes in  that have the colors in  are the nodes that
comprise , thereby completing our proof. \qed
\end{proof}
\begin{proof}(of Lemma~\ref{lem:supp})
Again, our proof assumes  because it generalizes to arbitrary values of
 quite easily.  From Lemma~\ref{lem:beta}, we know that the influence of all
nodes in  taken together will reach  nodes in  rounds.
This does not suffice because we are interested in showing that there is at least one
node in  that (individually) influences  nodes in  for some .  


From Lemma~\ref{lem:beta}, we know that  (collectively) will influence at least  nodes in T rounds, i.e.,

From Property~\ref{prop:bipartite}, we know that there is a set 
of cardinality at most  such that 
Recalling
that , we know that .  We can again use Lemma~\ref{lem:beta} to say that  influences
more than  nodes in additional  rounds and, by transitivity,
 influences more than  nodes after  rounds. We therefore
have . Again, we can choose a set  (using Property~\ref{prop:bipartite}) that consists of  nodes in  such  that . Subsequently applying
Lemma~\ref{lem:beta} extends the influence set of  to more than  after
 rounds. 

In every iteration  of the above argument, the size of the set  decreases
by a constant fraction until we are left with a single node
 such that .
\qed
\end{proof}
}

Can  (or more nodes)  be suppressed for any significant number of (say, ) rounds? This is in immediate contradiction to Lemma~\ref{lem:supp} because any such suppressed set of nodes must contain an unsuppressed node. This leads us to the following corollary.

\begin{corollary}\label{cor:suppress}
The number of nodes that can be suppressed for  rounds is less
than , even if the network is controlled by an adaptive
adversary.
\end{corollary}
\onlyLong{
\begin{corollary}\label{cor:random}
Consider an oblivious adversary that must commit to the entire
sequence of graphs in advance. If we choose a node  uniformly at random
from , with probability at least ,
then  will be unsuppressed, i.e.,

\end{corollary}
\begin{proof}
Let  be the set of nodes suppressed for  rounds. Under an oblivious adversary, the node  chosen unformly at random from  will not be in  with probability , and hence, will not be suppressed with that same probability.
\qed
\end{proof}

The following two lemmas show that there exists a set  of unsuppressed nodes, all of which can influence a large common set of nodes, given enough time.

\begin{lemma}\label{lem:UniversalInfluence}
Consider a dynamic network under linear churn that is controlled by an adaptive
adversary. In some  rounds, there
is a set of unsuppressed nodes  of cardinality more than  such that  
\end{lemma}

\begin{proof}
Let  be any set of unsuppressed nodes, i.e., in some  rounds for some constant , the influence set of each 
has cardinality more than . Note that, however, we {\em cannot}
guarantee that, for any two vertices  and  in , 
 
Assume for simplicity
that  is a power of 2. Consider any pair of vertices , both
members of . Recalling that , we can say that
 Therefore,
considering that the intersected set  of nodes has cardinality at least , we can apply
Lemma~\ref{lem:beta} leading to .  We can partition  into  a set  of
 pairs such that for each pair, the intersection of influence
sets has cardinality more than  after  rounds.
Similarly, we can construct a set  of quadruples by disjointly pairing the
pairs in . Using a similar argument, we can say that for any  ,

Progressing
analogously, the set  will equal  and we can conclude that 

Since , it holds that , thus completing the proof. 
\qed
\end{proof}
}

\onlyLong{
\begin{lemma}\label{lem:UniversalInfluence2}
Suppose that up to  nodes can be subjected to churn in any round
by an adaptive adversary. In
some  rounds, there is a set of unsuppressed nodes  of cardinality at least  such that  
\end{lemma}
\begin{proof}
Since we assume that , the bound  of Lemma~\ref{lem:beta} is
in .
Therefore, by instantiating Corollary~\ref{cor:suppress}, we know that each of the unsuppressed nodes
in  (which is of cardinality at least ) will influence
more than  nodes in  time. We can use  the same
argument as in Lemma~\ref{lem:UniversalInfluence} to show that in 
rounds, all the unsuppressed nodes have a common influence set of size at least
. That common influence set will grow to at least 
nodes within another  rounds. Thus a total of  rounds
is sufficient to fulfill the requirements.
\qed
\end{proof}
}


\shortOnly{
In a dynamic network with churn limit , the entire set of nodes in the
network can be churned out and new nodes churned in within  rounds. This
calls for maintaining important global information (such as a global clock) in
the network. This can be achieved quite easily by appropriate message flooding.
Details can be found in the full paper \cite{APRU11}.
}
\longOnly{
\subsection{Maintaining Information in the Network} \label{subsec:maintain}
In a dynamic network with churn limit , the entire set of nodes in the
network can be churned out and new nodes churned in within  rounds. How
do the new nodes even know what algorithm is running? How do they know how far
the algorithm has progressed? To address these basic questions, the network
needs to maintain some global information that is not lost as the nodes in the
network are churned out. There are two basic pieces of information that need to
be maintained so that a new node can join in and participate in the execution of
the distributed algorithm: 
\begin{enumerate}
	\item the algorithm that is currently executing, and
	\item the number of rounds that have elapsed in the execution of the algorithm. In other words, a global clock has to be maintained.
\end{enumerate}
We assume that the nodes in  are all synchronized in their understanding of what algorithm to execute and the global clock. The nodes in the network continuously flood information on what algorithm is running so that when a new node arrives, unless it is shielded by churn, it receives this information and can start participating in the algorithm. To maintain  the clock value, nodes send their current clock value to their immediate neighbors. When a new node receives the clock information from a neighbor, it sets its own clock accordingly. Since  nodes are not malicious or faulty,  Lemma~\ref{lem:beta} ensures that information is correctly maintained in more than  nodes.}


\subsection{Support Estimation Under an Oblivious Adversary } \label{sec:support} \label{sec:supportOblivious}
Suppose we have a dynamic network  with  nodes colored red in .  is also called the {\em support} of red nodes. 
We want the nodes in the network to estimate  under an oblivious adversary.
We assume that the adversary chooses  and which  nodes in  to color red, but it
does not know the random choices made by the algorithm. Furthermore, we assume that churn can be linear in , i.e., . 

Our algorithm uses random numbers drawn from the exponential distribution, whose probability density function, we recall, is parameterized by  and given by  for all . Furthermore, we notice that the expected value of a random number drawn from the exponential distribution of parameter  is .
\onlyLong{
We now present two properties of exponential random variables that are crucial to our context. Consider   independent random variables , each following the exponential distribution of rate . 
\begin{property}[see~\cite{grinstead1997introduction} for example]
The minimum among all 's, for , is  an exponentially distributed random variable with parameter . \label{prop:min}
\end{property}
The idea behind our algorithm exploits Property~\ref{prop:min} in the following manner. If each of the  red nodes  generate an exponentially distributed random number with parameter 1, then the minimum  among those  random numbers will also be  exponentially distributed, but with parameter . Thus  serves as an estimate of . To get a more accurate estimation of , we exploit the following property that provides us with sharp concentration when the process is repeated a sufficient number of times.
\begin{property}[see \cite{AoyamaS08} and pp.\ 30, 35 of \cite{DemboZ98}]  \label{prop:shah}
Let . Then, for any , 

\end{property}
}


\begin{algorithm}[h]
  \begin{algorithmic}[1]
   \footnotesize
  \EMPTY The following pseudocode is executed at every node .
  
  \EMPTY  controls the precision of our estimate. Its exact value is worked
out in the proof of Theorem~\ref{thm:ObliviousSupport}.
 
   \EMPTY 
  \item[\bf At round 1:]
  
  \STATE  Draw  random numbers , each from the exponential random distribution with rate
. 

\COMMENT{Each  is chosen with a precision that ensures that the smallest possible positive value is at most ;} 

\COMMENT{Note that  bits suffice.}

\STATE For each , create a message  containing  and a terminating condition: {\sc has encountered
a message  with a smaller random number}.

\COMMENT{Notice that a node  will flood exactly one message at each index  --- in particular the  smallest random number encountered by node  with message index }


\STATE For each , initiate flooding of message .



   \EMPTY 
  \item[\bf For the next  rounds:]
  
\STATE Continue flooding messages respecting their termination conditions.

\COMMENT It is easy to see that the number of bits transmitted per round through a link is at most .

 \EMPTY 
  \item[\bf At the end of the  rounds:]
  
   \STATE For each , the node  holds a message . Let  be the random number contained in . 
   \STATE . \label{lno:bars}
   \STATE Node  outputs  as its estimate of .
   \COMMENT{Now that the estimation is completed, all messages can be terminated.}
    \end{algorithmic}
  \caption{Algorithm to estimate the support  of red nodes when .}\label{alg:support}
\end{algorithm}
\normalsize




We now present our algorithm for estimating  in pseudocode format (assuming ); see Algorithm~\ref{alg:support}. 


\begin{theorem}\label{thm:ObliviousSupport}
Consider an oblivious adversary and let  be a an arbitrary fixed constant .
Let . By executing Algorithm~\ref{alg:support} to estimate both  and , we can estimate  to within  for any   with probability at least .
\end{theorem}
\onlyLong{
\begin{proof} 

Without loss of generality, let . 
Out of the  red nodes up to  nodes (chosen obliviously) can be
suppressed, leaving us with 

unsuppressed red nodes (since ). In a slight abuse of notation, we
use  and  to denote both the cardinality and the set of  red nodes and
unsuppressed red nodes, respectively.  We define  
note that  and
 (cf.\ Lemma~\ref{lem:UniversalInfluence}). Let 
be some node in . Let 
For all , . 
Notice that  computed by  in line number~\ref{lno:bars} of Algorithm~\ref{alg:support} is based on random numbers generated by all nodes in .
Therefore, at round ,
node  is estimating  using the exponential random numbers that  were
drawn by nodes in . Since our adversary is oblivious, the choice of 
is independent of the choice of the random numbers generated by each .
Therefore,  is an exponentially distributed random number with
rate   (cf.\ Property~\ref{prop:min}). For any ,
let . When  parallel
iterations are performed, where , the required accuracy is obtained with
probability  (cf.\ Property~\ref{prop:shah}). 
\qed
\end{proof}




\subsection{Support Estimation Under an Adaptive Adversary} \label{sec:supportAdaptive}
The algorithm for support estimation under an oblivious adversary (cf.\ 
Section~\ref{sec:supportOblivious}) does not work under an adaptive 
adversary. To
estimate the support of red nodes in the network, each red node draws
a random number from the exponential distribution and floods it in an attempt to
spread the smallest random number. When the adversary is adaptive, the smallest
random numbers can easily be targeted and suppressed. To mitigate this difficulty, we consider a different algorithm in which the number of bits communicated is larger. In particular, the number of bits communicated per round by each node executing  this algorithm is at most polynomial in . 


Let  be the
support of the red nodes. Every node floods its unique identifier along with a
bit that indicates whether it is a red node or not.  At most 
nodes' identifiers can be suppressed by the adversary for  rounds leaving  at least  unsuppressed identifiers (cf.\ Corollary~\ref{cor:suppress}). Each node
counts the number of unique red identifiers  and non-red identifiers  that
flood over it and estimates  to be .  

This support
estimation technique generalizes quite easily to arbitrary churn order.
Therefore, we state the following theorem more generally.
\begin{theorem}\label{thm:AdaptiveSupport}
Consider the algorithm mentioned above in which nodes flood their unique
identifiers indicating whether they are red nodes or not and assume that the
network is controlled by an adaptive adversary.  Let  be the
order of the churn; we assume for simplicity that  is either  or
.  Then the following holds:
\begin{compactenum}
	\item At least   nodes estimate   between  and . Furthermore, these nodes are aware that their estimate is within  and .
	\item The remaining nodes are aware that their estimate of  might fall
    outside .
	\end{compactenum}
	When , it requires only  rounds, but when , it requires  rounds.
\end{theorem}

\begin{proof}
Let  be any one of the  nodes that receive at least  unsuppressed identifiers (cf.\ Lemma~\ref{lem:UniversalInfluence} and
Lemma~\ref{lem:UniversalInfluence2}). Let  and  be the number of unique
identifiers from red nodes and non-red nodes, respectively, that flood over .
Let . This means that  estimates  to be
. Note that  and since , 
is estimated between  and .
Furthermore, since  received  identifiers, it can be sure
that its estimate is between  and .

If a node does not receive at least  identifiers, then it is
aware that its estimate of  might not be within .

From Lemma~\ref{lem:UniversalInfluence}, when ,  the algorithm takes  rounds to complete because we want to ensure that unsuppressed nodes have flooded the network. When , as a consequence of Lemma~\ref{lem:UniversalInfluence2}, the algorithm requires  rounds. 
\qed
\end{proof}

}










\section{\sa\ Under an Oblivious Adversary} \label{sec:oblivious}

In this section we will first present Algorithm~\ref{alg:bc} for the simpler
problem of reaching \bc, where the input values are restricted to 
(cf.\ Section~\ref{sec:sa}). We will then use this algorithm as a subroutine
for solving \sa\ in Section~\ref{sec:obliviousSA}.

Throughout this section we assume suitable choices of  and  such
that the upper bound 

can be satisfied for ; note that \eqref{eq:betaUpperOblivous} must hold
in addition to bound \eqref{eq:beta}.
Moreover, we assume that a node can send and process up to
 bits in every round, where  is the size of the input value
domain.  

\subsection{\bc} \label{sec:obliviousBC}

A node  that executes Algorithm~\ref{alg:bc} proceeds in a sequence of
 checkpoints that are interleaved by  rounds. Each node  has a bit variable  that stores its current output value. At each
checkpoint , node  initiates support estimation of the number of nodes
currently having 1 as their output bit by using the algorithm described in
Section~\ref{sec:support}. (At checkpoint , nodes estimate both:
the support of 1 and 0.) 
The outcome of this support estimation will be
available in checkpoint  where  has derived the estimation .
If  believes that the support of 1 is small (), it
sets its own output  to 0; if, on the other hand,  is large (),  sets its output  to 1. This guarantees stability once
agreement has been reached by a large number of nodes. When the support of 1 is
roughly the same as the support of 0, we need a way to sway the decision to one
side or the other. This is done by flooding the network whereby the flooding
message of node  is weighted by some randomly chosen value, say . The adversary can only
guess which node has the highest weight and therefore, with constant
probability, the flooding message with this highest weight (i.e., smallest
random number) will be used to set the output bit by almost all nodes in the
network. 




\begin{algorithm}[h]
  \begin{algorithmic}[1]
  \footnotesize
\EMPTY Let  be initialized to .
  \EMPTY
  Let  be the current output bit of . If , then  is
  initialized to the input value of ; otherwise it is set to . 
  
  Let  be the first checkpoint round. Subsequent checkpoint rounds are
  given by  , for . 
  For the terminating checkpoint , we choose an , i.e.,
  .
\EMPTY 
  \item[\bf At every checkpoint round  \underline{excluding} :] 
    
  \STATE Initiate support estimation (to be completed in checkpoint round ).
  \STATE Generate a random number  uniformly from  for suitably large but constant . (With high probability, we want exactly one node to have generated .)
  \STATE Initiate flooding of  with terminating condition: {\sc
  has encountered another message initiated by  with )  (current round ).}
  
  \EMPTY
  \item[\bf At every checkpoint round  \underline{except} :] 
  
  \STATE Use the support estimation initiated at checkpoint round . Let
   be 's estimated support value for the number of nodes that had an
  output of 1. 
  \IF{} \label{line:onequarter}
    \STATE .
  \ELSIF{}
    \STATE .
  \ELSIF{ has received flooded messages initiated in }
    \STATE Let  be the message with the smallest random number that flooded over .
    \STATE .
  \ENDIF
  
  \EMPTY
  \item[\bf At terminating checkpoint round :]
  \IF{} \label{line:oneHighOblivious}
   		\STATE .
      \STATE Flood a 1-decision message ad infinitum.
  \ELSIF{}
   		\STATE .
      \STATE Flood 0-decision message ad infinitum.
\ENDIF
  \EMPTY
  \item[\bf If  receives a -decision message:]
    \STATE 
  \end{algorithmic}
  \caption{\bc\ under an oblivious adversary; code executed by node .} \label{alg:bc}
\end{algorithm}
\normalsize


\begin{theorem} \label{thm:ssd}
Assume that the adversary is oblivious and that the churn limit per round is
.
Algorithm \ref{alg:bc} reaches stability in  rounds and achieves \bc\ with high probability.
\end{theorem}
\onlyLong{
\begin{proof} 
  Throughout this proof we repeatedly invoke the properties of the support estimation as stated in Theorem~\ref{thm:ObliviousSupport}, which succeeds with probability .
  Assuming that , suffices to guarantee that all of the  invocations of the support estimation are accurate with high probability.

We first argue that Validity holds: Suppose that all nodes start with
input value~1. The only way a node can set its output to 0 is by passing
Line~\ref{line:onequarter}. This can happen for at most  nodes.
The only way that more nodes can set their output to 0 is if they estimate the
support of 1 to be in .  If  is suitably
small, Theorem~\ref{thm:ObliviousSupport} guarantees that with high probability this
will not happen at any node. 
The argument is analogous for the case where all nodes start with 0.

Next we show Almost Everywhere Agreement:
Let  be the number of nodes at checkpoint round  that output 1.
Let , , and , respectively, be the sets of nodes in
 for which , , and 
; note that nodes are placed in ,
, and  based on their  values, which are estimates of
, not . Clearly, we have that .

For some , let  be the node that generated the
smallest random number in checkpoint round  among all nodes in
. With high probability,  will be unique. By
Corollary~\ref{cor:random}, with probability  (a constant),  is
unsuppressed, implying that  will be used by all nodes in .
Consider the following cases:
\begin{description}
\item[Case A ():] From
  Theorem~\ref{thm:ObliviousSupport}, we know that with high probability
   implying .
  Therefore,  will continue to be very small leading to small estimates
   in subsequent checkpoints. After  checkpoints, this causes
  at least  nodes to decide on , with high probability. Moreover,
  it is easy to see that the remaining  nodes will not be able to pass
  Line~\ref{line:oneHighOblivious}, since the adversary cannot artificially
  increase the estimated support of nodes with 1. (Recall from
  Section~\ref{sec:support} that by suppressing
  the minimum random variables, the adversary can only make the estimate
  smaller.)
  
  (We are presenting separate Cases B, C, and D for clarity. Equivalently, we could have treated them together as one case with the condition that   leading to the implication that with high probability either  or .)

	
\item[Case B ():] With high
   probability,  implying  . Note first that nodes in  will set their output bits to 0.
   Since , there are at least  nodes
   in  that output 0. Of these, at most  could have been
   suppressed. So, with probability at least ,  is an
   unsuppressed node that outputs 0. When  outputs 0, nodes in 
   will set their output bits to 0. Thus, considering  and , we
   have at least  nodes that set their output bits to  with
   constant probability. 
For a suitably small  and , this will lead to
Case A in the next iteration, which means that subsequently nodes agree on 0.
	 
   \item[Case C ():]
     With high probability, . With constant probability
     ,  will be an unsuppressed node and nodes in  will
     set their output bits to the same value . This will  lead to 
     Case A in the next iteration. 

   \item[Case D ():] 
     This is similar to Case B, i.e., with constant probability, at least   nodes will reach agreement on 1.


   \item[Case E ():] This is similar to Case A.
     With high probability, at least  nodes will decide on 1. 

\end{description}
Note that, when a checkpoint falls either under Case A or Case E, with high
probability, it will remain in that case. When a checkpoint falls under Case B,
Case C, or Case D, with constant probability, we get either Case A or Case E in
the following checkpoint. Therefore, in  rounds, at least
 nodes will reach agreement with high probability and all other
nodes will remain undecided.

For property Stability, note that if a node has decided on some value 
in checkpoint , it continues to flood its decision message. We showed that, with high probability, at least
 nodes will decide on the same bit value. Therefore, it follows by
Lemma~\ref{lem:beta} that agreement will be maintained  ad infinitum among at
least  nodes. \qed
\end{proof}
}

\onlyLong{\subsection{A 3-phase Algorithm for \sa}} \label{sec:obliviousSA}
We will now describe how we use Algorithm~\ref{alg:bc} as a building block for
solving \sa:
In order to use Algorithm~\ref{alg:bc} to solve \sa, we will need to make a
couple of crucial  adaptations. \onlyShort{The first one is needed to guarantee
the Validity property of \sa, while the second one deals with the fact that only
nodes starting with 1 will initiate \bc. These adaptations are fully described in
the attached full paper.}
\onlyLong{
\begin{itemize}
\item Suppose every vertex in  has some auxiliary information. We can
  easily adapt Algorithm~\ref{alg:bc} so that when a node  decides on a bit
  value , then, it also inherits the auxiliary information of some  whose initial bit value was . This is guaranteed because our
  algorithm ensures Validity.
  The auxiliary information can be piggybacked on the messages that  generates throughout the course of the algorithm.

\item For a typical agreement algorithm, we assume that all nodes simultaneously
  start running the algorithm. We want to adapt our algorithm
  so that only nodes in  that have an initial output bit of 1 initiate
  the algorithm, while nodes that start with 0 are considered passive, i.e.,
  these nodes do not generate messages themselves, but still forward flooding
  messages and start generating messages from the next checkpoint onward as soon
  as they notice that an instance of the algorithm is running. 
  
  We now sketch how the algorithm can be adapted: In the first checkpoint , each node  with a
  1 initiates support estimation and flooding of message . If
  the number of nodes with 1 is small at checkpoint , then, at checkpoint
  , nodes that receive estimate values will conclude 0, which will get
  reinforced in subsequent checkpoints.  However, if the number of nodes with a
  1 at checkpoint  is large (in particular, larger than ), then,
  by suitable flooding, at least  nodes 
  will know that a support estimation is underway and will participate from
  checkpoint  onward.
\end{itemize}
}


\paragraph{Selection and Flooding Phase:} 
  In the very first round, each node  generates
  a uniform random number  from  and, if the random number is less
  than ,  becomes a \emph{candidate} and initiates a message  for flooding.
  The
  message  contains the random number  and the general value  (from domain ) assigned to  by the adversary.
  This phase ends after  rounds to ensure that no more than 
  nodes are suppressed (the precise bound on the number of rounds is given by  Corollary~\ref{cor:suppress}).
  The flooding of the generated messages, however, goes on ad infinitum.

\paragraph{Candidate Elimination Phase:}
  We  initiate  parallel
  iterations of \bc, whereby each iteration is associated with one of the   flooding messages, generated by the candidates in the first phase.
  More precisely, the -th instance of \bc\ for the -th candidate and its flooding message  is designed as follows:
  nodes that have received a flooded message , set
  their input bit (of the -th instance of \bc) to  and initiate \bc.
  We say that a flooded message  is
  a {\em survived candidate} message if the instance of \bc\ associated with it reached a
  decision value of . 
  

\paragraph{Confirmation Phase:} 
  Among the survived candidate messages, every node 
  chooses the message  among its received messages that has the smallest random number  (and associated general input value
  ), and initiates a support estimation for the number of nodes that have received .
  If the support estimation reveals a support of at least  for  then  decides on . 
  Nodes keep flooding their decision ad infinitum.


\begin{theorem}\label{thm:sa}
Consider the oblivious adversary and suppose that 
nodes can be subject to churn in every round. The 3-phase
algorithm is correct with high probability and reaches \sa\ in 
rounds.
\end{theorem}
\begin{proof}
  Validity follows immediately from the fact that nodes only decide on some value that was the input value of a (survived) candidate.

  We now argue Almost Everywhere Agreement:
  Since all nodes choose independently whether to become candidate, a simple application of a standard Chernoff bound shows that the number of candidates is in the range  with probability ; in the remainder of this proof, we condition on this event to be true.

  Consider the message  generated by some candidate  in the Selection and Flooding phase, and consider its associated instance of \bc:
If  has reached at least  nodes by flooding, it follows by the properties of the \bc\ algorithm that the decision value of \bc\ will be  with probability .
On the other hand, if  has a very small support (say, ), the consensus value will be
 with probability  (cf.\ Case A of the proof of Theorem~\ref{thm:ssd}),
  and, if the support of  is neither too small nor too large, the nodes
  will reach consensus on either  or . 
  Thus we can interpret a decision of  regarding the -th message, as a confirmation that the -th candidate had sufficiently large support.
  By taking a union bound, it follows that, with probability at least , at least  nodes agree
  on the set of survived candidate messages, since they reached agreement in each iteration of \bc.
Since the adversary is oblivious, each of
the  flooding messages generated by the candidates will not be suppressed with probability at least  (cf.\ Corollary~\ref{cor:random}).
Therefore, with probability , at least one candidate  will have  and thus the set of survived candidates  will be nonempty; let  be the candidate who generated the smallest random number.
When the support estimation is initiated in the third phase, a set of at least
 nodes will measure 's support to be at least 
for some  with probability  (cf.\
Theorem~\ref{thm:ObliviousSupport}) and decide on the value  of ,
whereas nodes that do not observe high support remain undecided.
This shows Almost Everywhere Agreement. 

Analogously to Algorithm~\ref{alg:bc}, nodes in  flood their decision
messages, which are adopted by newly incoming nodes. By virtue of
Lemma~\ref{lem:beta}, the stability property is maintained ad infinitum.

The additional running time overhead of the above three phases excluding 
Algorithm~\ref{alg:bc} is only in . This completes the proof of the Theorem.
\qed
\end{proof}











\section{\sa\ Under an Adaptive Adversary} \label{sec:adaptive}

In this section we consider the \sa\ problem while dealing with a more
powerful adaptive adversary. At the beginning of a round , this adversary
observes the entire state of the network and previous communication between
nodes (including even previous outcomes of random choices!), and thus can adapt
its choice of  to make it much more difficult for nodes to achieve
agreement. 


It is instructive to consider the algorithms  presented in
Section~\ref{sec:oblivious} in this context.  Both approaches are doomed to fail
in the presence of an adaptive adversary: For the \sa\ algorithm, the expected
number of nodes that initiate flooding in the flooding phase is . Even
though each of these nodes would have expanded its influence set to
some constant size by the end of the next round, the adaptive adversary can spot
and immediately churn out all these nodes before they can communicate with
anyone else, thus none of these values will gain any support. 


Algorithm~\ref{alg:bc} fails for the simple reason that the adversary can
selectively suppress the flooding of the smallest generated random value
 with attached bit  from ever reaching some 50\% of the
nodes, which instead might use a distinct minimum value  (with an attached
bit value ) to guide their output
changes.  


To counter the difficulties we have mentioned, we relax the model. Firstly, we
limit the order of the churn to . Secondly, we  allow messages of up
to a polynomial (in ) number of bits to be sent over a link in a single round. 
Under these relaxations, we can estimate the support of red nodes in the network
simply by flooding all the unique identifiers of the red and non-red
nodes\onlyLong{ (cf.\ Theorem~\ref{thm:AdaptiveSupport})}.

Similarly to Section~\ref{sec:oblivious}, we will first solve \bc\ under these
assumptions and then show how to implement \sa. In this section we assume that
the number of nodes in the network is sufficiently large, such that

Moreover, we assume that every node can send and process up to 
bits per round, where  is a constant and  is the size of the input domain.
\subsection{\bc} \label{sec:adaptiveBC}



\onlyLong{
\begin{algorithm}[h]
  \begin{algorithmic}[1]
  \footnotesize
\EMPTY Let  be initialized to .

  Let  be the current output bit of . If , then  is
  initialized to the input value of ; otherwise it is set to . 
  
  Let  be the first checkpoint round. Subsequent checkpoint rounds are
  given by  , for , with time between
  consecutive checkpoint rounds sufficient for unsuppressed nodes to reach a
  common influence\onlyLong{ (cf. Lemma~\ref{lem:UniversalInfluence2})}. 
  For the terminating checkpoint , we choose an , i.e.,
  .
\EMPTY
  \item[\bf At every checkpoint round  \underline{excluding} :] 
  \STATE Initiate support estimation (to be completed in checkpoint round ).
  \EMPTY 
  \item[\bf At every checkpoint round  \underline{excluding} , :] 
  \STATE Use the support estimation initiated at checkpoint round . Let  be the estimated support value for nodes that output 1. 
  \IF{support estimation is not accurate within }
    \STATE Do nothing. \label{line:doNothing}
  \ELSIF{}
    \STATE .
  \ELSIF{} \label{line:largeSupport}
    \STATE .
  \ELSE
    \IF{the outcome of an unbiased coin flip is heads}
      \STATE .
    \ELSE
      \STATE .
    \ENDIF
\ENDIF
  \EMPTY
  \item[\bf At terminating checkpoint round :]
  \IF{} \label{line:oneHigh}
   		\STATE . 
      \STATE Flood a 1-decision message ad infinitum.
  \ELSIF{}
   		\STATE . 
   		\STATE Flood a 0-decision message ad infinitum.
  \ENDIF
  \EMPTY
  \item[\bf If  receives a -decision message:]
    \STATE 
  \end{algorithmic}
  \caption{\bc\ under an adaptive adversary; code executed by node .}
  \label{alg:adaptive}
\end{algorithm}
\normalsize
}


We now\onlyShort{ briefly} describe an algorithm for solving \bc, which is
similar in spirit to Algorithm~\ref{alg:bc}. \onlyShort{The full statement of
the algorithm can be found in the attached full paper.} The main difference is
the handling of the case where the support of the nodes that output 1 is roughly
equal to the support of the nodes with output bit 0. In this case we rely on the
variance of random choices made by individual nodes to sway the balance of the
support towards one of the two sides with constant probability. 

First, we argue why this technique does not work when the churn limit is :
In our algorithm we handle the case where the support of  and  is roughly equal, by causing each node to update its current output bit to the outcome of a (private) unbiased coin flip.
The standard deviation that we get for the sum of these individual random variables is  and the event where the balance is swayed by  occurs with constant probability.
But since the adversary is adaptive and has  churn to play with, it can immediately undo this favourable imbalance by churning out nodes such that the support of  and  will yet again be roughly equal. 

\begin{theorem} \label{thm:adaptiveBC}
\onlyLong{Algorithm~\ref{alg:adaptive}}\onlyShort{There is an algorithm that}
solves \bc\ with high probability and reaches stability within  rounds, in the presence of
an adaptive adversary and up to  churn per round.
\end{theorem}
\onlyLong{
\begin{proof}
First consider the Validity property: Suppose that all nodes start with input value~1. 
Theorem~\ref{thm:AdaptiveSupport} guarantees that any node  that receives
insufficiently many identifiers for support estimation, will execute
Line~\ref{line:doNothing} and therefore never set its output to 0. On the other
hand, if  does receive sufficiently many samples, again
Theorem~\ref{thm:AdaptiveSupport} ensures that it will always pass the if-check
in Line~\ref{line:largeSupport}. Thus, no node can ever output 0. The case where
all nodes start with 0 can be argued analogously.


Next, we will show that Algorithm~\ref{alg:adaptive} satisfies Almost
Everywhere Agreement.
Let  be the number of vertices at checkpoint round  with output bit 1.
Let , , and , respectively, be the sets of nodes in
 for which , , and 
; note that nodes are placed in ,
, and  based on their  values, which are estimates of
, not . In a slight abuse of notation, we use , ,
and  to also refer to their respective cardinalities. Clearly, we have
that 

Furthermore, observe that either  or  will be 0. Otherwise, we will have two nodes such that one estimates  below , while the other estimates it above  --- a violation of Theorem~\ref{thm:AdaptiveSupport}.

Consider the following cases:
\begin{description}
	\item[Case A ():] From
    Theorem~\ref{thm:AdaptiveSupport},  and all
    nodes in  will set themselves to output 0. Once this case is reached
    in some checkpoint, it will be reached in all future checkpoints until 
    with high probability.
    Therefore, the algorithm guarantees Almost Everywhere Agreement on 0 in
    ; with high probability, nodes do not pass Line~\ref{line:oneHigh}
    in checkpoint , thus no node will ever decide on .
	\item[Case B  ():] This case is similar to
    Case A with the difference that almost all nodes decide on 1.
	\item[Case C ():] Notice that . Therefore, 

		We consider two subcases: 
\paragraph{1.} In this case, we assume that  is at least . This will set  putting the network in Case A in the next checkpoint.
\paragraph{2.} In this case, we assume that . This implies that 
  The nodes in  will choose 1 or 0 with equal probability. The number of nodes that choose 0 is a binomial distribution with mean  and standard deviation . Clearly, with some constant probability,  or more nodes in the set  will set themselves to output 0. Therefore, with constant probability,  

Clearly,  if 

We know that . Therefore,  if 

In other words, as long as 

it holds with constant probability that

which will put
the network in Case A at the next checkpoint round. Assumption~\eqref{eq:numberOfNodesAdaptive} guarantees that Condition~\eqref{eq:condition}
is easily met. 
\item[Case D ():] Using arguments
similar to Case C, we can show that with constant probability, 

thereby, putting the network in Case B. 
\end{description}
Clearly, after  checkpoint rounds the network
will reach either Case A or Case B\footnote{Due to Equation
\eqref{eq:numberOfNodesAdaptive} we know that Cases A and B exist.} with high probability and hence achieve Almost Everywhere
Agreement on either 0 or 1.

For property Stability, note that if a node has decided on some value 
in checkpoint , it continues to flood its decision message. Since at least
 have decided, it follows by Lemma~\ref{lem:beta} that any nodes
that have been churned in will also decide on this value within a constant
number of rounds, thus agreement will be maintained ad infinitum.
\qed
\end{proof}
}

\onlyLong{\subsection{\sa}} \label{sec:adaptiveSA}
Now that we have a solution for \bc, we will show how to use it to solve \sa\
where nodes have input values from some set , for .
Given some input value  we can write it in the base-2 number system as
 where , for . We
call  a \emph{general input value} and  a \emph{binary input value}.

The basic idea of the \sa\ algorithm is to run an instance of the \bc\ algorithm
for each  and then combine the agreed bits  to obtain agreement on the
general input values. We now describe our algorithm; the detailed pseudo code is
presented in Algorithm~\ref{alg:sa}.
Consider the -th iteration of Algorithm~\ref{alg:sa} and suppose that  are the first  decision values of the previous  iterations of the \bc\ algorithm.
We say that a node  \emph{knows a general input value matching the first  binary decision values}, if  has knowledge of a some  that was the input value of some node  and the first  bits of  are exactly .
We denote the -th bit value of a general value  by .
Recall that the \bc\ algorithm executes the support estimation routines developed in Section~\ref{sec:supportAdaptive}.
We slightly modify the support estimation routine by requiring each node  to also piggyback its current general value  onto the message it generates for support estimation.
Moreover, when  floods the decision message of the \bc\ algorithm, it also piggybacks .
Whenever a node  updates its current output bit value to , this guarantees that  has learned of a general value  that has  as its first bit.
Thus  sets  to the new value  and chooses its next input value for the -th iteration of the \bc\ algorithm to be the -th bit of .
This is formalized in the following lemma:
\begin{lemma} \label{lem:validity}
  Consider iteration  of the \bc\ subroutine executed in Algorithm~\ref{alg:sa}.
  If a node  has a current binary output value of , then the -th bit of  is .
\end{lemma}
\begin{proof}
  We will show the result by induction over the iterations of the \bc\ algorithm. 
  Initially, in the first iteration, node  uses the first bit of its input value .
  Now suppose that  sets its output bit to  at some point during the first iteration.
  We say that  violates \emph{general validity}.
  There are two possible cases: In the first case,  observed a sufficiently large support for  and thus received a support estimation message generated by a node  that had a current output bit , while in the second case  received a decision message generated by .
  In either case, it follows from the description of the algorithm that node  has piggybacked  on top of this message.
  If , then  has updated its own output bit without updating , due to receiving some message from another node , and both nodes,  and , violate general validity.
 By backwards traversing this chain of nodes that violate general validity, we eventually reach a node  which has set its output bit value to  but , without having received a message from a node that violates general validity.
  According to the \bc\ algorithm,  only sets its bit value to  if it has either observed sufficient support for  or received a decision message containing a value of .
  In both cases, it follows from the description of the algorithm that  updates  to the piggybacked general value, the -th bit of which is , providing a contradiction.
  \qed
\end{proof}
The above lemma guarantees that we can combine the decision bits of the \bc\ iterations to get a general decision value that satisfies validity. 
We can therefore show the following theorem:
\onlyLong{
\begin{algorithm}[h]
  \begin{algorithmic}[1]
  \footnotesize

	\STATE Suppose that node  starts with an initial general input value .

	\FOR{}
  \STATE Node  initiates \bc\ by proposing the -th bit of its current . Recall that \bc\ will be reached in  rounds.
		\STATE When participating in the support estimation that is part of \bc\, each node  piggybacks its  on top of the support estimation message. 
    


    \STATE Let  be the decision returned by \bc\ algorithm.
    If node  has decided on bit value , then  has learned of a general input value  where the -th bit is :
    Node  updates its current value  by setting it to  and floods  along the decision message according to the \bc\ algorithm.
    \STATE If  did not decide in the \bc\ algorithm, then   does not propose a value in the -th iteration.
  \ENDFOR
  \STATE If  did not decide in the last iteration, it remains undecided.
  Otherwise,  returns the  as its decision value and floods this value ad infinitum.
  \end{algorithmic}
  \caption{Solving \sa\ using \bc. Pseudo code for node .}
  \label{alg:sa}
\end{algorithm}
\normalsize
}


\begin{theorem} \label{thm:adaptiveSA}
  Suppose that the network is controlled by an adaptive adversary who can
  subject up to  nodes to churn in every round.
  There is an algorithm that solves \sa\ with high probability and reaches stability in .
\end{theorem}
\begin{proof}
  Almost-everywhere agreement follows almost immediately from the fact that the \bc\ algorithm satisfies almost-everywhere agreement; what remains to be shown is that all except  nodes decide:
  Note that it is possible that a set  of  nodes can remain undecided when running an instance of the \bc\ algorithm.
  The nodes in  will not propose any values in the next iteration but will participate in the support estimation and the propagation of messages.
  By the correctness of the \bc\ algorithm, all except  nodes eventually know the decision bit  of the -th iteration.
  In the next iteration, any node  that knows the decision bit , also knows a general value  such that  and thus can propose in the subsequent iteration.
  This holds regardless of whether  and thus all except  nodes participate in each iteration.

  For validity, we argue that Algorithm~\ref{alg:sa} maintains the following invariant at the end of every iteration : a node that is aware of the decision (bit) values of the first  runs of the \bc\ subroutine, has knowledge of a general value matching the first  binary decision values.
  By Lemma~\ref{lem:validity}, it follows that if a node  proposes a bit  in iteration , then  is the -th bit of some general input value .
  This guarantees that the sequence of decision bits correspond to some general input value and thus satisfies validity.

  Finally, we observe that the proof of stability is identical to the \bc\ algorithm, thus completing the proof.
  \qed
\end{proof}





\section{Impossibility of a Deterministic Solution} \label{sec:impossibility}



In this section we show that there is no deterministic algorithm to solve \sa\ even
when the churn is restricted to only a constant number of nodes per round. As a consequence, randomization is a necessity for solving \sa.  
\onlyLong{



We introduce some well known standard notations (see \cite[Chap.\ 5]{AW04})
used for showing impossibility results of agreement problems.
The \emph{configuration}  of the network at round  consists of 
\begin{compactitem}
  \item the graph of the network at that point in time, and 
  \item the local state of each node in the network. 
\end{compactitem}
A specific run  of some \sa\ algorithm  is entirely
determined by an infinite sequence of configurations  where 
contains the initial state of the graph before the first round.
Consider the input value domain .
A configuration  is \emph{1-valent} (resp.,
\emph{0-valent}) if all possible runs of  that share the common prefix
up to and including , lead to an agreement value of 1 (resp., 0). Note
that this decision value refers to the decision of the large majority of
nodes; strictly speaking, a small fraction of nodes might remain undecided on .
A configuration is
\emph{univalent} if it is either 1-valent or 0-valent.  Any configuration that
is not univalent is called a \emph{bivalent} configuration.  
The following observation follows immediately from the definition of the \sa\ problem.

\begin{observation} \label{lem:bivalent} Consider a bivalent configuration  in round  reached by an algorithm  that solves \sa\ and ensures Almost Everywhere Agreement.
No node in  can have decided on a value  by round .
\end{observation}
}
\begin{theorem}\label{thm:impossibility}
Suppose that the sequence of graphs  is an expander family with
maximum degree . Assume that the churn is limited to at most +1
nodes per round. There is no deterministic algorithm that solves \sa\ if the
network is controlled by an adaptive adversary.
\end{theorem}
\onlyLong{
\begin{proof}
We use an argument that is similar to the argument used in the proof that
 rounds are required for consensus in the presence of  faults (cf.\
\cite[Chap.\ 5]{AW04}).
For the purpose of this impossibility proof, we restrict the input domain of
nodes to  and allow arbitrary congestion on the communication
channnels.  Moreover, we assume that the topology of the network is fixed
throughout the run. Thus the adversary can only ``replace'' nodes at the same
position by some other nodes.

For the sake of contradiction, assume that such a deterministic algorithm 
exists that solves \sa\ under the assumed settings. 
We will prove our theorem by inductively constructing an
infinite run  of this algorithm consisting of a sequence of bivalent
configurations. By virtue of Observation~\ref{lem:bivalent} this allows us to conclude
that nodes do not reach almost everywhere agreement.












To establish the basis of our induction, we need to show that there is an
initial bivalent configuration  at the start of round 1. Assume
in contradiction that there is no bivalent starting configuration.
Let  (resp.\ ) be the configuration where all nodes
start with a value 0 (resp., 1); note that by validity the decision 
value must be on 0 (resp., 1).
Consider the sequence of configurations starting at  and ending at  where the only difference between any two configurations adjacent (in this sequence) is a single bit, i.e., exactly  node has a different input value.
Since  is -valent and  is -valent, this implies that there are two possible
starting configurations in this sequence,  and , in which (i) the input values are
the same for all but one node , but (ii)  is 0-valent whereas
 is 1-valent. Consider the respective one-round extension of  and
 where the adversary simply churns out node .
Both successor configurations  and  are indistinguishible for all
other nodes, in particular they have no way of knowing what initial value was
assigned to , since all witnesses have been removed by the adversary.
Therefore,  and  must both be either 0-valent or 1-valent, a
contradiction. This shows that there is an initial bivalent configuration,
thereby establishing the basis for our induction. 

For the inductive step, we assume that the network is in a bivalent
configuration  at the end of round . We will extend  by
one round (guided by the adversary) that yields another bivalent configuration
. Assume for the sake of a contradiction that every possible one-round
extension of  yields a univalent configuration.  Without loss of
generality, assume that the one-round extension  where no node is
churned out is 1-valent and yields configuration .  Since by assumption
 was bivalent, there is another one-round extension  that
yields a 0-valent configuration . Moreover,  we know that a nonempty
set  of size at most +1 nodes must have been subject to churn in
. (This is the only difference between  and  ---
recall that the edges of the graph are stable throughout the run.)

Let  be a subset of  and let
 be the one-round extension of  that we
get when only nodes in  are churned out. Clearly,
 and .
Consider the lattice of all such one-round extension bounded by  and
 that is given by the power set of . Starting at  and moving
towards  along some path, we must reach a one-round extension
 that yields a 1-valent configuration
, whereas the next point on this path is some one-round extension
 that ends in a 0-valent configuration
. The only difference between these two extensions is that node
 is churned out in the latter but not in the former extension. Now
consider the one-round extensions of  and  where  and all
its neighbors are churned out, yielding  and . For all
other nodes,  and  are indistinguishible and therefore they must
either both be 0-valent or both be 1-valent. This, however, is a contradiction.
\qed
\end{proof}
}
\onlyLong{
Considering that expander graphs usually are assumed to have constant degree,
Theorem~\ref{thm:impossibility} implies that even if we limit the churn to a
constant, the adaptive adversary can still beat any deterministic algorithm.
}




\section{Conclusion}
We have introduced a novel framework for analyzing highly dynamic
distributed systems with churn. We believe that our model
captures the core characteristics of such systems: a large amount of churn
per round and a constantly changing network topology.  Future work  involves extending our model to include Byzantine nodes and corrupted
communication channels. Furthermore, our work raises some key questions:
How much churn can we tolerate in an adaptive setting? Are there algorithms that
tolerate linear (in ) churn in an adaptive setting? We show that we can tolerate  
churn in an adaptive setting, but it takes a polynomial (in ) number of communication bits per round.
An intriguing problem is to reduce the number of bits to polylogarithmic in .

While the main focus of this paper was achieving agreement among nodes
which is one of the most important tasks in a distributed system, as a next step, it might be worthwhile to investigate whether the techniques presented in this paper can serve  as useful building blocks for tackling other important tasks
like aggregation or leader election in highly dynamic networks.

\bibliographystyle{plain}
\bibliography{papers}

\end{document}
