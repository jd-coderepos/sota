\pdfoutput=1 \documentclass[journal]{IEEEtran}

\usepackage{amsmath,graphicx}
\usepackage[ruled]{algorithm2e}
\usepackage{threeparttable}
\usepackage{dcolumn}
\usepackage{booktabs}
\usepackage{amsmath,graphicx}
\usepackage{url}
\usepackage{cite}
\usepackage{psfrag}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{graphicx}
\usepackage{array}
\usepackage{multirow}
\usepackage{bm}
\usepackage{subfigure}
\usepackage{cite}
\usepackage{verbatim}
\usepackage{colortbl}
\usepackage{algorithmic}
\hyphenation{op-tical net-works semi-conduc-tor}

\newcommand{\xx}{{\bf x}}
\newcommand{\yy}{{\bf y}}
\newcommand{\ffx}{{\bf f}_x}
\newcommand{\ffy}{{\bf f}_y}
\newcommand{\ccx}{{\bf c}_x}
\newcommand{\ccy}{{\bf c}_y}
\newcommand{\cc}{{\bf c}}
\newcommand{\bbx}{{\bf b}_x}
\newcommand{\bby}{{\bf b}_y}
\newcommand{\bb}{{\bf b}}

\begin{document}

\title{Image Deblurring Using Derivative Compressed Sensing for Optical Imaging Application}

\author{Mohammad~Rostami,~\IEEEmembership{Student~Member,~IEEE,}
        Oleg~Michailovich,~\IEEEmembership{Member,~IEEE,}
        and~Zhou~Wang,~\IEEEmembership{Member,~IEEE}\thanks{All the authors are with the Department of Electrical and Computer Engineering, University of Waterloo, N2L 3G1 Ontario.}}

\markboth{IEEE Transactions on Image Processing,~Vol.~XX, No.~XX, January~2012}{Shell \MakeLowercase{\textit{et al.}}: Manuscript submission.}

\maketitle

\begin{abstract}
Reconstruction of multidimensional signals from the samples of their partial derivatives is known to be a standard problem in inverse theory. Such and similar problems routinely arise in numerous areas of applied sciences, including optical imaging, laser interferometry, computer vision, remote sensing and control. Though being ill-posed in nature, the above problem can be solved in a unique and stable manner, provided proper regularization and relevant boundary conditions. In this paper, however, a more challenging setup is addressed, in which one has to recover an image of interest from its noisy and blurry version, while the only information available about the imaging system at hand is the amplitude of the generalized pupil function (GPF) along with partial observations of the gradient of GPF's phase. In this case, the phase-related information is collected using a simplified version of the Shack-Hartmann interferometer, followed by recovering the entire phase by means of derivative compressed sensing. Subsequently, the estimated phase can be combined with the amplitude of the GPF to produce an estimate of the point spread function (PSF), whose knowledge is essential for subsequent image deconvolution. In summary, the principal contribution of this work is twofold. First, we demonstrate how to simplify the construction of the Shack-Hartmann interferometer so as to make it less expensive and hence more accessible. Second, it is shown by means of numerical experiments that the above simplification and its associated solution scheme produce image reconstructions of the quality comparable to those obtained using dense sampling of the GPF phase.
\end{abstract}

\begin{IEEEkeywords}
Deconvolution, inverse problem, derivative compressive sampling, and Shack-Hartmann interferometer.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
\IEEEPARstart{T}{he} necessity to recover digital images from their distorted and noisy observations arises in a multitude of practical applications, with some specific examples including image denoising, super-resolution, image restoration, and watermarking, just to name a few \cite{30, 28, 29, 40}. In such cases, it is standard to assume that the observed image  is formed by subjecting the original image  to convolution with a point spread function\footnote{Note that, in optical imaging, this function is also referred to as an impulse transfer function \cite{39}.} (PSF) , followed by contamination by white Gaussian noise (WGN) . Thus, formally,

While  and  can be regarded as general members of the signal space  of real-valued functions on , the PSF  is normally a much smoother function, with effectively band-limited spectrum. As a result, the convolution with  has a destructive effect on the informational content of , in which case  typically has a substantially reduced set of features with respect to . This makes the problem of reconstruction of  from  a problem of significant practical importance \cite{41}.

Reconstruction of the original image  from  can be carried out within the framework of image deconvolution, which is a specific instance of a more general class of inverse problems \cite{42}. Most of such methods are Bayesian in nature, in which case the information lost in the process of convolution with  is recovered by requiring the optimal solution to reside within a predefined functional class \cite{26, 27}. Thus, for example, in the case when  is known to be an image of bounded variation, the above regularization leads to the famous Rudin-Osher-Fatemi reconstruction scheme, in which  is estimated as a solution to the following problem \cite{0, 11}

where  is the regularization parameter. It should be noted that, if the PSF obeys , the problem (\ref{2}) is strictly convex and therefore admits a unique minimizer, which can be computed by a spectrum of available algorithms \cite{0, 11}.

A particularly non-trivial version of deconvolution is commonly referred to as blind. In this case, the original image  is to be estimated without the knowledge of the PSF \cite{42}. In this paper, however, we follow the philosophy of {\em hybrid deconvolution} \cite{Oleg07}, which takes advantage of any partial information on the PSF to improve the image reconstruction. Thus, in the algorithm described in this paper, the original image  will be recovered from  and some partial information on .

Optical (and, in particular, turbulent) imaging is unarguably the field of applied sciences from which the notion of deconvolution has originally emanated \cite{Richardson72, Lucy74, 44}. In short-exposure imaging, however, computational methods of image restoration are still superseded by adaptive optics. As recently as a decade ago, the use of adaptive optics would have been considered as the only practical option. Nowadays, however, with the advent of distributed cluster computing and GPU-based image processing, it seems to be time to revisit the cost-to-performance characteristics of the existing tools of adaptive optics. Thus, in this work, our focus is on a specific tool of adaptive optics, known as the Shack-Hartmann interferometer \cite{12, 18}. Instead of completely excluding the interferometer from our measurement system, we propose to  modify its construction through reducing the number of its local wavefront lenses. Although the advantages of such a simplification are immediate to see, its main shortcoming is obvious as well: the smaller the number of lenses is, the stronger is the effect of undersampling and aliasing. Accordingly, to overcome this problem, we propose to augment the modified Shack-Hartmann interferometer by subjecting its output to the derivative compressed sensing (DCS) algorithm of \cite{14}. As it will be shown later in the paper, the PSF  is determined by a generalized pupil function , which can be expressed in a polar form as . While the amplitude  can be measured via calibration or computed as a function of the aperture geometry, the phase  is often influenced by environmental effects and hence it needs to be recovered from observations. It will be shown below that DCS is particularly well suited for reconstruction of  from incomplete measurements of its partial differences. Such an estimate can be subsequently combined with  to yield an estimate of the PSF , which can in turn be used by a deconvolution algorithm. Thus, the proposed method for estimation of the PSF and subsequent deconvolution of  can be regarded as a hybrid deconvolution technique, which comes to simplify the design and complexity of adaptive optics on the one hand, and to make the process of reconstruction of optical images as automatic as possible, on the other hand.

The rest of the paper is organized as follows. Section II summarizes basic technical preliminaries. In Section III, we describe the SH interferometer as well as phase measurements in optical imaging. In Section IV, we explain DCS and our new approach to solve it. In Section V we describe deconvolution process to recover the original image. Experimental results are presented in Section VI, while Section VII finalizes the paper with a discussion and main conclusions.

\section{Technical Preliminaries}
In short exposure imaging, due to aberrations in the imaging system induced by, e.g., atmospheric turbulence, the impulse response of an optical imaging system is often unknown \cite{15}. In order to better understand the setup under consideration, we first note that, in optical imaging, the PSF  is obtained from an amplitude spread function (ASF)  as . The ASF, in turn, is defined in terms of the generalized pupil function (GPF)  as given by \cite{17}

where  is the focal distance and  is the optical wavelength. Being a complex-valued quantity,  can be represented in terms of its amplitude  and phase  as

Here, the GPF amplitude  (which is sometimes simply referred to as the aperture function) is normally a function of the aperture geometry. Thus, for instance, in the case of a circular aperture,  can be defined as \cite{15}

where  denotes the pupil diameter. Thus, given , one could determine  and therefore . Unfortunately, the phase  does not have an analytic expression, and it has to be measured in practice using such tools as the Shack-Hartmann interferometer (SHI) \cite{12}.

As will be discussed later in the paper, the SHI is capable of sensing the partial derivatives of . Needless to say, in order to minimize the effect of aliasing on the estimation result, an accurate reconstruction of  requires taking a fairly large number of the samples of  \cite{Oleg2008}. In some applications, the number of sampling points (as defined by the number of local wavefront lenses) reaches as many as a few thousands. It goes without saying that reducing the number of lenses would have a positive impact on the SHI in terms of its cost and approachability. Alas, such a reduction is impossible without undersampling, which tends to have formidable effect on the overall quality of phase estimation.

In this paper, to minimize the effect of undersampling, we exploit DCS \cite{14}. As opposed to the classical compressed sensing (CCS) \cite{13}, in addition to the sparsifing constraints, DCS also uses constraints which are intrinsic in the definition of partial derivatives. Using these additional constraints -- which are called the cross-derivative constraints -- allows substantially improving the quality of reconstruction of , as compared to the case of CCS-based estimation.

\section{Shack-Hartmann Interferometer (SHI)}
As it was mentioned earlier, the SHI is typically used to measure the gradient  of the GPF phase , from which the values of the latter can be subsequently estimated. To this end, the unknown phase  is assumed to be expandable in terms of some basis functions , {\it viz.} \cite{18}

where the representation coefficients  are assumed to be unique and stably computable. Note that, in this case, the datum of  uniquely identifies , while the coefficients  can be estimated due to the linearity of (\ref{6}) which suggests


The most frequent choice of  in adaptive optics is the Zernike polynomials (aka Zernike functions) \cite{17}. These polynomials constitute an orthonormal basis in the space of square-integrable functions defined over the unit disk in . Zernike polynomials can be subdivided in two subsets of the even  and odd  Zernike polynomials which have very convenient analytical definitions as given by

where  and  are nonnegative integers with ,  is the azimuthal angle, and  is the radial distance. The radial polynomials  are defined as


Note that, since the Zernike polynomials above are defined using polar coordinates, it makes sense to re-express the phase  and its gradient in the polar coordinate system as well. (Technically, this would amount to replacing  and  in (\ref{6})-(\ref{6.1}) by  and , respectively.) Moreover, due to the property of the Zernike polynomials to be an orthonormal basis, the representation coefficients  in in (\ref{6})-(\ref{6.1}) can be computed by orthogonal projection, namely

In practice, however,  is unknown and therefore the coefficients  need to be estimated by other means. Thus, in the case of the SHI, the coefficients can be estimated from a finite set of discrete measurements of .

\begin{figure}[!t]
\begin{center}
\hspace{-5mm}\includegraphics[height=2.1in,width=2.1in]{fig1.pdf}
\caption{An example of a  SHI array on a circular aperture. The shading indicates those blocks (i.e., lenses) which are rendered idle.} \label{F1}
\end{center}
\end{figure}

The main function of the SHI is to acquire discrete measurements of  by means of linearization. The linearization takes advantage of subdividing a (circular) aperture into rectangular blocks with their sides formed by a uniform rectangular lattice. An example of such a subdivision is shown in Fig.~\ref{F1} for the case of a  lattice grid. Subsequently, it is assumed that the grid is sufficiently fine to approximate a restriction of the phase  to each of the above blocks by a linear function. This results in a piecewise linear approximation of , whose accuracy improves asymptotically when the lattice size goes to infinity\footnote{More rigorously, one can show that, as long as  is uniformly continuous, its piecewise linear approximation converges uniformly, as the grid size goes to infinity.}. Formally, let  be a circular aperture of radius  and  be a square subset of  such that . Then, for each polar coordinate  and an  grid of square blocks of size , the phase  can be expressed as

for all  in a neighbourhood of . The approximation in (\ref{8}) suggests that

where  denotes matrix transposition. While  in (\ref{8}) can be derived from boundary conditions, coefficients  and  should be determined via direct measurements. To this end, the SHI is endowed with an array of small focusing lenses, which are supported over each of the square blocks of the discrete grid. In the absence of phase aberrations, the focal points of the lenses are spatially identified and registered using a high-resolution CCD detector, whose imaging plane is aligned with the plane of the focal points. Then, when the wavefront gets distorted as a result of, e.g., atmospheric turbulence, the focal points are ``pushed" towards new spatial positions, which can also be pinpointed by the same detector. The resulting displacements can therefore be measured and subsequently related to the values of  at corresponding points.

To explain how the above procedure can be performed, additional notations are in order. Let  denote a finite set of spatial coordinates defined according to

Note that the set  can be thought of as a set of the spatial coordinates of the geometric centres of the wavefront lenses, restricted to the domain of aperture . Under some reasonable assumptions, one can then show \cite{19} that the focal displacement  measured at some  is related to the value of  as given by

where  is the focal distance of the wavefront lenses. Such a measurement setup is depicted in Fig.~\ref{fig222} along with an example of measured focal points.

\begin{figure}[!t]
\centering
\includegraphics[width = 7cm, height = 5cm]{SH.pdf}
\label{fig:tabsubfig21}
\caption{Basic structure of the SHI and a resulting pattern of the focal points.}
\label{fig222}
\end{figure}

Now, given a total of  measurements of  over , one can try to recover a useful approximation of  over the whole  in the form of a projection of  on the linear subspace spanned by all Zernike polynomials up to the order  inclusive. In this case, it is possible to estimate the representation coefficients  via solution of

subject to appropriate boundary conditions. It is worthwhile noting that (\ref{E1}) can be rewritten in a vector-matrix form as

where  is a  matrix composed of the values of the partial derivatives of the Zernike polynomials,  is a measurement (column) vector of length , and  is a vector of the representation coefficients of . The constraint  in (\ref{E2}) is supposed to further regularize the solution by forcing  to be  a member of some convex cone as well. Thus, for example, if the mean value of  can be assumed to be equal to zero, the solution to (\ref{E2}) can be computed as

where  denotes the pseudo-inverse of , whose definition is unique and stable as long as the row-rank of  is greater or equal to  (hence suggesting that ). Having estimated , the phase  can be approximated as


A higher accuracy of phase estimation requires using higher-order Zernike polynomials, which in turn necessitates a proportional increase in the number of wavefront lenses. Moreover, as required by the linearization procedure in the SHI, the lenses have to be of a relatively small sizes (sometimes, on the order of a few microns), which may lead to the use of a few thousand lenses per one interferometer. Accordingly, to simplify the construction and to reduce the cost of SHIs, we propose to substantially reduce the number of wavefront lenses, while compensating for the induced information loss through the use of DCS, which is detailed next.

\section{Derivative Compressive Sampling}
\subsection{Classical Compressed Sensing}
Central to signal processing is the Shannon-Nyquist theorem \cite{45}, which specifies conditions on which a band-limited signal can be stably and uniquely recovered from its discrete measurements. However, in around 2005, a different sampling theorem was formed which, in some cases, abrogates the fundamentals of its predecessor. This new theory, nowadays known as compressed sensing (aka compressive sampling), asserts that signals, which admit a sparse representation in a predefined basis/frame, can be recovered from their discrete measurements, whose number is proportional to the -norm of the coefficients of the sparse representation. In such a case, the sparser the representation of the signal is, the smaller can be the number of  measurements required for signal reconstruction. As a result, cases are numerous in which the conditions of compressed sensing far superseded those of the Shannon-Nyquist sampling \cite{4, 13}.

Despite its widespread success in countless applications, the theory of compressed sensing is not entirely free of limitations. One of such limitations stems from the necessity to use a non-linear decoder. Indeed, while in the case of classical sampling, the reconstruction of a time-domain signal is implemented through linear interpolation (i.e., linear filtering), in the case of compressed sensing, the reconstruction involves solution of a convex optimization problem. Specifically, let us consider a typical setup of {\em classical} compressed sensing (CCS), in which  represents an observed version of , related according to

where  is an observation (sampling) matrix with .

The recovery of  from  based on (\ref{13}) is impossible to implement in a unique and stable way, unless it is known that  is sparse and hence has a relatively low value of . In such a case, if the sampling matrix  satisfies the restricted isometry property (RIP) \cite{4, 13} with respect to a certain class of sparse signals to which  is assumed to belong, then CCS recovers  as a solution to \cite{20, 21}

which is a convex minimization problem, which is straightforward to reformulate in terms of linear programming. Moreover, in the case when the measurements  are error-prone, a more robust version of CCS is to recover  as given by

where  is a parameter controlling the size of the noise. Moreover, it was shown in \cite{4, 13}, that the estimation error in the signal reconstructed according to (\ref{E3}) can be bounded by a linear function of . This implies robustness of the CCS reconstruction towards the presence of measurement noise.

It should be finally noted that the optimization problem (\ref{E3}) can be reformulated in its alternative Lagrangian form, in which case one can find

where  is an optimal Lagrange multiplier \cite{20}. In what follows, it is assumed that an optimal value of  is known. (For more details on this subject, the reader is referred to \cite{20} as well as to the later sections of this paper).

\subsection{Derivative Compressed Sensing (DCS)}
Let the partial derivatives of  evaluated (by means of the SHI) at the points of set  be column-stacked into vectors  and  of length .  In what follows, the partial derivatives  and  are assumed to be sparsely representable by an orthonormal basis in . Representing such a basis by an  unitary matrix , the above assumption suggests the existence of two {\em sparse} vectors  and  such that  and  provide accurate approximations of the partial derivatives of the original phase  evaluated over .

Now, the simplification of the SHI proposed in the current paper amounts to reducing the number of wavefront lenses to a minimum. Formally, such a reduction can be described by two  sub-sampling matrices  and , where . Specifically, let  and  be incomplete (partial) observations of  and , respectively. Then, the noise-free counterparts of the partial derivatives can be approximated by  and , respectively, where

and

for some . Moreover, in the case when , the above estimates can be combined together. To this end, let , , and . Then,

where . In this form, the problem (\ref{E4}) is identical to (\ref{17}) and hence it can be solved by a variety of optimization algorithms \cite{20, 21}.

The DCS algorithm augments CCS by subjecting the minimization in (\ref{E4}) to an additional constraint which stems from the fact that \cite{14}

which is valid for any two times continuously differentiable . Thus, in particular, the constraint implies the existence of two partial differences matrices  and  which obey

Consequently, if  and  are the matrices satisfying  and , respectively, then (\ref{E5}) can be re-expressed in terms of  as

or

where . Thus, DCS solves the constrained minimization problem as given by


A solution to (\ref{E5}) can be found, for instance, by means of the Bregman algorithm \cite{22}, in which case  is approximated by a stationary point of the sequence of iterations produced by

where  is a vector of Bregman variables (or, equivalently, augmented Lagrange multipliers) and  is a user-defined parameter\footnote{In this work, we use .}.

The -update step in (\ref{E6}) has the format of a standard basis pursuit de-noising (BPDN) problem \cite{46}, which can be solved by a variety of optimization methods \cite{23}. In the present paper, we used the FISTA algorithm of \cite{33} due to the simplicity of its implementation as well as for its remarkable convergence properties. It should be noted that the algorithm does not require explicitly defining the matrices  and . Only the {\em operations} of multiplication by these matrices and their transposes need to be known, which can be implemented in an implicit and computationally efficient manner.

Once an optimal  is recovered, it can be used to estimate the noise-free versions of  and  as  and , respectively. These estimates can be subsequently passed on to the fitting procedure of Section III to recover the values of , which, in combination with a known aperture function , provide an estimate of the PSF  as an inverse discrete Fourier transform of the autocorrelation of . Algorithm 1 below summarizes our method of estimation of the PSF.

\begin{algorithm}
\setlength{\leftmargini}{0pt}
\caption{PSF estimation via DCS}
\begin{enumerate}
\item {\it Data:} , , and 
\item {\it Initialization:} For a given transform matrix  and matrices/operators , , , ,  and , preset the procedures of multiplication by , ,  and .
\item {\it Phase recovery:} Starting with an arbitrary  and , iterate (\ref{E6}) until convergence to result in an optimal . Use the estimated (full) partial derivatives  and  to recover the values of  over .
\item {\it PSF estimation:} Using a known aperture function , compute the inverse Fourier transform of  to result in a corresponding ASF . Estimate the PSF  as .
\end{enumerate}
\label{algo1}
\end{algorithm}

The estimated PSF can be used to recover the original image  from  through the process of deconvolution as explained in the section that follows.

\section{Deconvolution}
The acquisition model \eqref{1} can be rewritten in an equivalent operator form as given by

where  denote the operator of convolution with the estimated PSF . Note that, in this case, the noise term  accounts for both measurement noise as well as the inaccuracies related to estimation error in .

The deconvolution problem of finding a useful approximation of  given its distorted measurement  can be addressed in many way, using a multitude of different techniques \cite{31, 32, 33}. In this work, we use the ROF model and recover a regularized approximation as a solution of

where  denotes the total variation (TV) semi-norm of .

One computationally efficient way to solve \eqref{E7} is to substitute a direct minimization of the cost function in \eqref{E7} by recursively minimizing a sequence of its local quadratic majorizers \cite{33}. In this case, the optimal solution  can be approximated by the stationary point of a sequence of intermediate solutions produced by

where  is the adjoint of  and  is chosen to satisfy . In this paper, the TV denoising at the second step of \eqref{E8} has been performed using the fixed-point algorithm of Chambolle \cite{11}. The convergence of \eqref{E8} can be further improved by using the same FISTA algorithm of \cite{33}. The resulting procedure is summarized below in Algorithm 2.

\begin{algorithm}
\setlength{\leftmargini}{0pt}
\caption{TV deconvolution using FISTA}
\begin{enumerate}
\item {\it Initialize:} Select an initial value ; set  and \\
\item {\it Repeat until convergence:}
\begin{itemize}
\item 
\item 
\item 
\item 
\end{itemize}
\end{enumerate}
\label{algo2}
\end{algorithm}

In summary, Algorithms 1 and 2 represent the essence of the proposed algorithm for hybrid deconvolution of optical images. The next section provides experimental results which further support the value and applicability of the proposed methodology.

\section{Results}
To demonstrate the viability of the proposed approach, its performance has been compared against reference methods. The first reference method used a dense sampling of the phase (as it would have been the case with a regular SHI), thereby eliminating the need for a CS-based phase reconstruction. The resulting method is referred below to as the dense sampling (DS) approach. Second, to assess the importance of incorporation of the cross-derivative constraints, we have used both CCS and DCS for phase recovery. In what follows, comparative results for phase estimation and subsequent deconvolution are provided for all the above methods.

\subsection{Phase recovery}
To assess the performance of the proposed and reference methods under controllable conditions, simulation data was used. The random nature of atmospheric turbulence necessitated the use of statistical methods to model its effect on a wavefront propagation. Specifically, in this paper, the effect of atmospheric turbulence has been described using the modified Von Karman PSF model\cite{35}. A typical example of a GPF phase  is shown in subplot (a) of Fig.~\ref{F3}. In the shown case, the size of the phase screen was set to be equal to  cm, while the sampling was performed over a  uniform lattice (which would have corresponded to the use of 16384 lenses of a SHI). The partial derivatives  and  are shown in subplots (b) and (c) of Fig.~\ref{F3}, respectively.

In the present paper, the subsampling matrices  and  were obtained from an identity matrix  through a random subsampling of its rows to result in a required compression ratio  (to be specified below). To sparsely represent the partial derivatives of ,  was defined to correspond to a four-level orthogonal wavelet transform using the nearly symmetric wavelets of Daubechies with five vanishing moments \cite{92}.

\begin{figure}[t]
\centering
\subfigure[]
{
\includegraphics[width = 4cm]{orig-phase.pdf}
\label{fig:tabsubfig1}
}
\\
\subfigure[]{
\includegraphics[width = 4cm]{xderiv.pdf}
\label{fig:tabsubfig2}
}
\subfigure[]{
\includegraphics[width = 4cm]{yderiv.pdf}
\label{fig1:tabsubfig3}
}
\caption{An example of a simulated phase  (subplot (a)) along with its partial derivatives w.r.t.  (subplot (b)) and  (subplot (c)).}
\label{F3}
\end{figure}

To demonstrate the value of using the cross derivative constraint for phase reconstruction, the CCS and DCS algorithms have been compared in terms of the mean square errors (MSE) of their corresponding phase estimates. The results of this comparison are summarized in Fig.~\ref{F4} for different compression ratios (or, equivalently, (sub)sampling densities) and SNR = 40 dB.

\begin{figure}[!t]
\centering
\includegraphics[width = 9cm, height = 6cm]{density.pdf}
\label{fig1:tabsubfig2}
\caption{MSE of phase reconstruction obtained with different methods as a function of . Here, the dashed and solid lines correspond to CCS and DCS, respectively, and SNR is equal to 40 dB.}
\label{F4}
\end{figure}

As expected, one can see that DCS results in lower values of MSE as compared to CCS, which implies a higher accuracy of phase reconstruction. Moreover, the difference in the performances of  CCS and DCS appears to be more significant for lower sampling rates, while both algorithms tend to perform similarly when the sampling density approaches the DS case. Specifically, when the sampling density is equal to , DCS results in a ten times smaller MSE as compared to the case of CCS, whereas both algorithms have comparable performance for . This result characterizes DCS as a better performer than CCS in the case of relatively low (sub)sampling rates.

Some typical phase reconstruction results are shown in Fig.~\ref{F5}, whose left and right subplots depict the phase estimates obtained using the CCS and DCS algorithms, respectively, for the case of . To visualize the differences more clearly, error maps for both methods are shown in subplot (c) and (d)(note for better visualization error map values are multiplied by factor of 10.). A close comparison with the original phase (as shown in subplot (a) of Fig.~\ref{F3}) reveals that DCS provides a more accurate recovery of the original , which further supports the value of using the cross-derivative constraints. In fact, exploiting these constraints effectively amounts to using additional measurements, which are ignored in the case of CCS.

\begin{figure}[!t]
\subfigure[]{
\includegraphics[width = 4cm]{CS-phase.pdf}
\label{fig:tabsubfig21}
}
\subfigure[]{
\includegraphics[width = 4cm]{DCS-phase.pdf}
\label{fig:tabsubfig31}
}\\
\subfigure[]{
\includegraphics[width = 4cm]{CS-diff.pdf}
\label{fig:tabsubfig31}
}
\subfigure[]{
\includegraphics[width = 4cm]{DCS-diff.pdf}
\label{fig:tabsubfig31}
}
\caption{(Subplot (a)) Phase reconstructed obtained by means of CCS for SNR = 40 dB and ; (Subplot (b)) Phase reconstructed obtained by means of DCS for the same values of SNR and .; (Subplot (c) and (d)) Corresponding error maps for CCS and DCS.}
\label{F5}
\end{figure}

To investigate the robustness of the compared algorithms towards the influence of additive noises, their performances have been compared for a range of SNR values. The results of this comparison are summarized in Fig.~\ref{F6}. Since the cross-derivative constraints exploited by DCS effectively restrict the feasibility region for an optimal solution, the algorithm exhibits a substantially better robustness to the additive noise as compared to the case of CCS. This fact represents another beneficial outcome of incorporating the cross-derivative constraints in the process of phase recovery.

\begin{figure}[!t]
\centering
\includegraphics[width = 9cm, height = 6cm]{robust.pdf}
\caption{MSE of phase reconstruction obtained with different methods as a function of SNR. Here, the dashed and solid lines correspond to CCS and DCS, respectively, and .}
\label{F6}
\end{figure}

It should be taken into account that, although the shape of  does not change the energy of the PSF , it plays a crucial role in determining its spatial behaviour. In the section that follows, it will be shown that even small inaccuracies in reconstruction of  could be translated into dramatic  difference in the quality of image deconvolution.

\subsection{Image deconvolution}
As a next step, the phase estimates obtained using the CCS- and DCS-based methods for  were combined with the aperture function  to result in their respective estimates of the PSF . These estimates were subsequently used to deconvolve a number of test images such as ``Satellite", ``Saturn", ``Moon" and ``Galaxy". All the test images were blurred with an original PSF, followed by their contamination with additive Gaussian noise of different levels. As an example, Fig.~\ref{fig12} shows the ``Satellite" image (subplot (a)) along with its blurred and noisy version (subplot (b)).

Using the PSF estimates, the deconvolution was carried out using the method detailed in \cite{11}. For the sake of comparison, the deconvolution was also performed using the PSF recovered from dense sampling (DS) of . Note that this reconstruction is expected to have the best accuracy, since it neither involves undersampling nor requires a CS-based phase estimation. All the deconvolved images have been compared with their original counterparts in terms of PSNR as well as of the structural similarity index (SSIM) of \cite{36}, , which is believed to be a better indicator of perceptual image quality \cite{100}. The resulting values of the comparison metrics are summarized in Table 1, while Fig.~\ref{fig11} shows the deconvolution results produced by the CCS- and DCS-based methods.

\begin{figure}[!t]
\subfigure[]{
\includegraphics[width = 4cm]{orig-image.pdf}
\label{fig:tabsubfig2}
}
\subfigure[]{
\includegraphics[width = 4cm]{distorted-image.pdf}
\label{fig:tabsubfig3}
}
\caption{Satellite image (subplot (a)) and its blurred and noisy version (subplot (b)).}
\label{fig12}
\end{figure}

The above results demonstrate the importance of accurate phase recovery, where even a relatively small phase error can have a dramatic effect on the quality of image deconvolution. Under such conditions, the proposed method produces image reconstructions of a superior quality as compared to the case of CCS. Moreover, comparing the results of Table 1, one can see that DS only slightly outperforms DCS in terms of PSNR and SSIM, while in many practical cases, the difference between the performances of these methods are hard to detect visually.

\begin{figure}[t]
\subfigure[]{
\includegraphics[width = 4cm]{CS-result.pdf}
\label{fig:tabsubfig2}
}
\subfigure[]{
\includegraphics[width = 4cm]{DCS-result.pdf}
\label{fig:tabsubfig3}
}
\caption{(Subplot (a)) Image estimate obtained with the CCS-based method for phase recovery (SSIM = 0.917); (Subplot (b)) Image estimate obtained with the DCS-based method for phase recovery (SSIM = 0.781).}
\label{fig11}
\end{figure}

\begin{table*}[t]
\centering \caption{SSIM and PSNR comparisons of phase recovery
results}
\label{tab:denresults}
\begin{tabular}{l|cccc|cccc|cccc|cccc}
\hline
Image & \multicolumn{4}{c}{Satellite} & \multicolumn{4}{c}{Saturn} & \multicolumn{4}{c}{Moon} & \multicolumn{4}{c}{Galaxy}\\
\hline
Noise std &  &  &  &  &  &   &  &   &  &   &  &   &  &   &  &  \\
\hline
& \multicolumn{16}{c}{PSNR comparison (in dB)}\\
\hline
Blurred               & 14.06 & 14.06 & 14.06 & 14.05 & 17.78 & 17.78 & 17.78 &  17.78 & 19.98 & 19.97 & 19.97 &  19.97 & 18.79 & 18.79 & 18.78 &  18.78\\
DS                & 27.97 & 27.75 & 25.97 & 22.43 & 31.49 & 31.08 & 28.50 &  23.89 & 25.06 & 25.04 & 24.83 &  21.76 & 23.58 & 23.60 & 23.38 &  20.93\\
CS                    & 17.06 & 16.93 & 16.54 & 15.63 & 23.42 & 23.38 & 22.80 &  20.55 & 22.36 & 22.38 & 22.30 &  19.73 & 21.16 & 21.12 & 20.64 & 18.46\\
DCS                   & 27.42 & 27.22 & 25.56 & 22.22 & 31.02 & 30.65 & 28.30 &  23.72 & 25.00 & 24.99 & 24.78 &  21.73 & 23.52 & 23.54 & 23.32 & 20.86\\
\hline
& \multicolumn{16}{c}{SSIM comparison}\\
\hline
Blurred               & 0.200 & 0.200 & 0.199 & 0.197 & 0.226 & 0.226 & 0.226 & 0.175 & 0.512 & 0.512 & 0.509 & 0.504 & 0.257 & 0.257 & 0.257 & 0.254\\
DS               & 0.730 & 0.720 & 0.554 & 0.269 & 0.688 & 0.660 & 0.506 & 0.228 & 0.645 & 0.642 & 0.607 & 0.552 & 0.493 & 0.495 & 0.501 & 0.397\\
CS                    & 0.349 & 0.344 & 0.306 & 0.206 & 0.424 & 0.416 & 0.348 & 0.212 & 0.539 & 0.538 & 0.493 & 0.488 & 0.348 & 0.347 & 0.326 & 0.224\\
DCS                   & 0.674 & 0.667 & 0.519 & 0.263 & 0.656 & 0.641 & 0.483 & 0.223 & 0.643 & 0.640 & 0.604 & 0.549 & 0.490 & 0.491 & 0.501 & 0.393\\
\hline
\end{tabular}
\end{table*}

\section{Discussion and conclusions}
In the present paper, the applicability of DCS to the problem of reconstruction of optical images has been demonstrated. It was shown that, in the presence of atmospheric turbulence, the phase  of the GPF  is a random function, which needs to be measured using adaptive optics. To simplify the complexity of the latter, a CS-based approach has been proposed. As opposed to CCS, however, the proposed method performs phase reconstruction subject to an additional constraint, which stems from the property of  to be a potential field. The resulting algorithm (referred to as the DCS method) has been shown to yield phase estimates of substantially better quality as compared to the case of CCS.

In this paper, our main focus has been on simplifying the structure of the SHI through reducing the number of its wavefront lenses, while compensating for the effect of undersampling by using the theory of CS augmented by the cross-derivative constraint. The solution was computed using the Bregman algorithm, which provides a computationally efficient framework to carry out the constrained phase recovery. Moreover, the resulting phase estimates were used to recover their associated PSF, which was subsequently used for image deconvolution. It was shown that the DCS-based estimation of  with  results in image reconstructions of the quality comparable to that of DS, while substantially outperforming the results obtained with CCS.

While the proposed method offers a practical solution to the problem of phase estimation in adaptive optics, some interesting questions about the theoretical aspects of DCS still lay open. In particular, the question of theoretical performance of CS in the presence of side information on the source signal needs to be addressed through future research.

\section*{Acknowledgment}
This work was supported in part by the Natural Sciences and Engineering Research Council of Canada and in part by Ontario Early Researcher Award program, which are gratefully acknowledged. The authors would also like to acknowledge Sudipto Dolui for his helpful comments as well as for providing deconvolution codes.

\bibliography{DCS}
\bibliographystyle{IEEEbib}
\end{document}
