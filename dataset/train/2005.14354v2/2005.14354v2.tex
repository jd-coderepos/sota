









\documentclass[journal]{IEEEtran}
\usepackage{amsmath,bm}
\DeclareMathOperator*{\argmax}{arg\,max}


\usepackage{cite}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{lipsum}
\usepackage{graphicx}

\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{hyperref}
\usepackage{color,soul}
\usepackage[dvipsnames]{xcolor}

\usepackage[ruled,linesnumbered]{algorithm2e}



\ifCLASSOPTIONcompsoc
\usepackage[caption=false,font=normalsize,labelfon
t=sf,textfont=sf]{subfig}
\else
\usepackage[caption=false,font=footnotesize]{subfi
g}
\fi
\usepackage{alphalph}
\usepackage{tabularx,ragged2e}
\usepackage{threeparttable}







\usepackage{hyperref} 


















\ifCLASSINFOpdf
\else
\fi



















































\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
\title{UGC-VQA: Benchmarking Blind Video Quality Assessment for User Generated Content}
\author{Zhengzhong~Tu,
        Yilin~Wang,
        Neil~Birkbeck,
        Balu~Adsumilli,
        and~Alan~C.~Bovik,~\IEEEmembership{Fellow,~IEEE}\thanks{Z. Tu and A. C. Bovik are with Laboratory for Image and Video Engineering (LIVE), Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, 78712, USA (email: zhengzhong.tu@utexas.edu, bovik@utexas.edu).}\thanks{Y. Wang, N. Birkbeck, and B. Adsumilli are with YouTube Media Algorithms Team, Google LLC, Mountain View, CA, 94043, USA. (email: yilin@google.com, birkbeck@google.com, 
badsumilli@google.com)}\thanks{This work is supported by Google.}}

















\maketitle

\begin{abstract}
Recent years have witnessed an explosion of user-generated content (UGC) videos shared and streamed over the Internet, thanks to the evolution of affordable and reliable consumer capture devices, and the tremendous popularity of social media platforms. Accordingly, there is a great need for accurate video quality assessment (VQA) models for UGC/consumer videos to monitor, control, and optimize this vast content. Blind quality prediction of in-the-wild videos is quite challenging, since the quality degradations of UGC videos are unpredictable, complicated, and often commingled. Here we contribute to advancing the UGC-VQA problem by conducting a comprehensive evaluation of leading no-reference/blind VQA (BVQA) features and models on a fixed evaluation architecture, yielding new empirical insights on both subjective video quality studies and objective VQA model design. By employing a feature selection strategy on top of efficient BVQA models, we are able to extract 60 out of 763 statistical features used in existing methods to create a new fusion-based model, which we dub the \textbf{VID}eo quality \textbf{EVAL}uator (VIDEVAL), that effectively balances the trade-off between VQA performance and efficiency. Our experimental results show that VIDEVAL achieves state-of-the-art performance at considerably lower computational cost than other leading models. Our study protocol also defines a reliable benchmark for the UGC-VQA problem, which we believe will facilitate further research on deep learning-based VQA modeling, as well as perceptually-optimized efficient UGC video processing, transcoding, and streaming. To promote reproducible research and public evaluation, an implementation of VIDEVAL has been made available online: \url{https://github.com/vztu/VIDEVAL}.
\end{abstract}

\begin{IEEEkeywords}
Video quality assessment, image quality assessment, no-reference/blind, user-generated content
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle



\section{Introduction}
\IEEEPARstart{V}{ideo} dominates the Internet. In North America, Netflix and YouTube alone account for more than fifty percent of downstream traffic, and there are many other significant video service providers. Improving the efficiency of video encoding, storage, and streaming over communication networks is a principle goal of video sharing and streaming platforms. One relevant and essential research direction is the perceptual optimization of rate-distortion tradeoffs in video encoding and streaming, where distortion (or quality) is usually modeled using video quality assessment (VQA) algorithms that can predict human judgements of video quality. This has motivated years of research on the topics of perceptual video and image quality assessment (VQA/IQA).


VQA research can be divided into two closely related categories: subjective video quality studies and objective video quality modeling. Subjective video quality research usually requires substantial resources devoted to time- and labor-consuming human studies to obtain valuable and reliable subjective data. The datasets obtained from subjective studies are invaluable for the development, calibration, and benchmarking of objective video quality models that are consistent with subjective mean opinion scores (MOS).
\begin{table*}[!t]
\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{1.1}
\centering
\begin{threeparttable}
\caption{Evolution of popular public video quality assessment databases: from legacy lab studies of synthetically distorted video sets to large-scale crowdsourced user-generated content (UGC) video datasets with authentic distortions}
\label{table:db_comp}
\begin{tabular}{llllllllp{3.5cm}lllll}
\toprule
\textsc{Database}          & \textsc{Year} & \textsc{\#Cont} & \textsc{\#Total} & \textsc{Resolution} & \textsc{FR} & \textsc{Len} & \textsc{Format}      & \textsc{Distortion Type}                                                                  & \textsc{\#Subj} & \textsc{\#Rates}  & \textsc{Data}          & \textsc{Env}   \\
\hline\-1.em]
Number of contents & 1200 & 585  & 1380 \\
Video sources & YFCC100m (Flickr) & Captured (mobile devices) & YouTube \\
Video resolutions & 540p & 1080p,720p,480p,etc. & 4k,1080p,720p,480p,360p \\
Video layouts & Landscape & Landscape,portrait & Landscape,portrait \\
Video framerates & 24,25,30 fr/sec & 20,24,25,30 fr/sec & 15,20,24,25,30,50,60 fr/sec \\
Video lengths & 8 seconds & 10 seconds & 20 seconds \\
Audio track included & Yes (97\%) & Yes & No \\
Testing methodology & Crowdsourcing (CrowdFlower) & Crowdsourcing (AMT) & Crowdsourcing (AMT) \\
Number of subjects & 642 & 4,776 & 8,000 \\
Number of ratings & 136,800 (114 votes/video) & 205,000 (240 votes/video) & 170,159 (123 votes/video) \\
Rating scale & Absolute Category Rating 1-5 & Continuous Rating 0-100 & Continuous Rating 1-5 \\
\multirow[t]{6}{*}{Content remarks} & \multirow[t]{6}{4.6cm}{Videos sampled from YFCC100m via a feature space of blur, colorfulness, contrast, SI, TI, and NIQE; Some contents irrelevant to quality research; Content was clipped from the original and resized to 540p.}  & \multirow[t]{6}{4.6cm}{Videos manually captured by certain people; Content including many camera motions; Content including some night scenes that are prone to be outliers; Resolutions not uniformly distributed.} & \multirow[t]{6}{4.6cm}{Videos sampled from YouTube via a feature space of spatial, color, temporal, and chunk variation; Contents categorized into  classes, including HDR, screen content, animations, and gaming videos.}  \\ \\ \\ \\ \\ \\ 
\multirow[t]{5}{*}{Study remarks} & \multirow[t]{5}{4.6cm}{Study did not account for or remove videos on which stalling events occurred when viewed; test methodology prone to unreliable individual scores.}  & \multirow[t]{5}{4.6cm}{Distribution of MOS values slightly skewed towards higher scores; standard deviation statistics of MOS were not provided.} & \multirow[t]{5}{4.6cm}{Distribution of MOS values slightly skewed towards higher values; three additional chunk MOS scores with standard deviation were provided.} \\ \\ \\ \\ \\ 
\bottomrule

\end{tabular}
\end{table*}









\section{UGC-VQA Databases}
\label{sec:ugc_db}

The first UGC-relevant VQA dataset containing authentic distortions was introduced as the Camera Video Database (CVD2014) \cite{nuutinen2016cvd2014}, which consists of videos with in-the-wild distortions from 78 different video capture devices, followed by the similar LIVE-Qualcomm Mobile In-Capture Database \cite{ghadiyaram2017capture}. These two databases, however, only modeled (camera) capture distortions on small numbers of not very diverse unique contents. Inspired by the first successful massive online crowdsourcing study of UGC picture quality \cite{ghadiyaram2015massive}, the authors of \cite{hosu2017konstanz} created the KoNViD-1k video quality database, the first such resource for UGC videos. It consists of 1,200 public-domain videos sampled from the YFCC100M dataset \cite{thomee2015yfcc100m}, and was annotated by 642 crowd-workers. LIVE-VQC \cite{sinno2018large} was another large-scale UGC-VQA database with 585 videos, crowdsourced on Amazon Mechanical Turk to collect human opinions from 4,776 unique participants. The most recently published UGC-VQA database is the YouTube-UGC Dataset \cite{wang2019youtube} comprising 1,380 20-second video clips sampled from millions of YouTube videos, which were rated by more than 8,000 human subjects. Table \ref{table:ugc_db_comp} summarizes the main characteristics of the three large-scale UGC-VQA datasets studied, while Figure \ref{fig:snapshot} shows some representative snapshots of the source sequences for each database, respectively.  


\subsection{Content Diversity and MOS Distribution}

As a way of characterizing the content diversity of the videos in each database, Winkler \cite{winkler2012analysis} suggested three quantitative attributes related to spatial activity, temporal activity, and colorfulness. Here we expand the set of attributes to include six low-level features including brightness, contrast, colorfulness \cite{hasler2003measuring}, sharpness, spatial information (SI), and temporal information (TI), thereby providing a larger visual space in which to plot and analyze content diversities of the three UGC-VQA databases. To reasonably limit the computational cost, each of these features was calculated on every 10th frame, then was averaged over frames to obtain an overall feature representation of each content. For simplicity, we denote the features as . Figure \ref{fig:ind_feat_dis} shows the fitted kernel distribution of each selected feature. We also plotted the convex hulls of paired features, to show the feature coverage of each database, in Figure \ref{fig:feat_cvx_hull}. To quantify the coverage and uniformity of these databases over each defined feature space, we computed the relative range and uniformity of coverage \cite{winkler2012analysis}, where the relative range is given by:

where  denotes the feature distribution of database  for a given feature dimension , and  specifies the maximum value for that given dimension across all databases. 

Uniformity of coverage measures how uniformly distributed the videos are in each feature dimension. We computed this as the entropy of the -bin histogram of  over all sources for each database indexed :

where  is the normalized number of sources in bin  at feature  for database . The higher the uniformity the more uniform the database is. Relative range and uniformity of coverage are plotted in Figure \ref{fig:relative_range} and Figure \ref{fig:uniformity}, respectively, quantifying the intra- and inter-database differences in source content characteristics.

We also extracted 4,096-dimensional VGG-19 \cite{simonyan2014very} deep features and embedded these features into 2D subspace using t-SNE \cite{maaten2008visualizing} to further compare content diversity, as shown in Figure \ref{fig:tsne}. Apart from content diversity expressed in terms of visual features, the statistics of the subjective ratings are another important attribute of each video quality database. The main aspect considered in the analysis here is the distributions of mean opinion scores (MOS), as these are indicative of the quality range of the subjective judgements. The analysis of standard deviation of MOS is not presented here since it is not provided in LIVE-VQC. Figure \ref{fig:mos_dist} displays the histogram of MOS distributions for the three UGC-VQA databases. 

\begin{figure*}[!t]
\captionsetup[subfigure]{justification=centering}
\centering
\def\xwidth{0.133}
\def\hswidth{-0.4em}
\subfloat[Brightness][{Brightness}]{\includegraphics[height=\xwidth\textwidth]{figs/brightness_mu_dist.pdf} 
\label{fig:ind_feat_disa}} 
\hspace{\hswidth}
\subfloat[Contrast][{Contrast}]{\includegraphics[height=\xwidth\textwidth]{figs/contrast_mu_dist.pdf} 
\label{fig:ind_feat_disb}} 
\hspace{\hswidth}
\subfloat[Colorfulness][{Colorfulness}]{\includegraphics[height=\xwidth\textwidth]{figs/colorfulness_mu_dist.pdf} 
\label{fig:ind_feat_disc}}
\hspace{\hswidth}
\subfloat[Sharpness][{Sharpness}]{\includegraphics[height=\xwidth\textwidth]{figs/sharpness_mu_dist.pdf} 
\label{fig:ind_feat_disd}}
\hspace{\hswidth}
\subfloat[SI][{SI}]{\includegraphics[height=\xwidth\textwidth]{figs/si_mu_dist.pdf} 
\label{fig:ind_feat_dise}}
\hspace{\hswidth}
\subfloat[TI][{TI}]{\includegraphics[height=\xwidth\textwidth]{figs/ti_std_mu_dist.pdf} 
\label{fig:ind_feat_disf}}
\caption{Feature distribution comparisons among the three considered UGC-VQA databases: KoNViD-1k, LIVE-VQC, and YouTube-UGC.}
\label{fig:ind_feat_dis}
\end{figure*}


\begin{figure}[!t]
\centering
\def\xwidth{0.12}
\def\hswidth{-0.em}
\def\xlinewidth{0.255}
\def\xem{1pt}
\def\yem{3pt}
\footnotesize
\setlength{\tabcolsep}{1.5pt}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{ccc}

  \includegraphics[ height=\xlinewidth\linewidth, keepaspectratio]{figs/KONVID_1K_BRxCT.pdf} &
  \includegraphics[ height=\xlinewidth\linewidth, keepaspectratio]{figs/KONVID_1K_CFxSR.pdf} &
  \includegraphics[ height=\xlinewidth\linewidth, keepaspectratio]{figs/KONVID_1K_SIxTI.pdf} \\yem]
  \includegraphics[ height=\xlinewidth\linewidth, keepaspectratio]{figs/LIVE_VQC_BRxCT.pdf} &
  \includegraphics[ height=\xlinewidth\linewidth, keepaspectratio]{figs/LIVE_VQC_CFxSR.pdf} &
  \includegraphics[ height=\xlinewidth\linewidth, keepaspectratio]{figs/LIVE_VQC_SIxTI.pdf}  \\yem]
    \includegraphics[ height=\xlinewidth\linewidth, keepaspectratio]{figs/YOUTUBE_UGC_BRxCT.pdf} &
  \includegraphics[ height=\xlinewidth\linewidth, keepaspectratio]{figs/YOUTUBE_UGC_CFxSR.pdf} &
  \includegraphics[ height=\xlinewidth\linewidth, keepaspectratio]{figs/YOUTUBE_UGC_SIxTI.pdf}  \-1.em]
BRISQUE\textsubscript{avg}  &   & 36 & 3   \\
BRISQUE\textsubscript{std}  &    &  36  & 1 \\
GM-LOG\textsubscript{avg} &    & 40  &  4  \\
GM-LOG\textsubscript{std} &      &  40  & 5 \\
HIGRADE-GRAD\textsubscript{avg} &      &  36  & 8 \\
HIGRADE-GRAD\textsubscript{std} &      &   36 &  1 \\
FRIQUEE-LUMA\textsubscript{avg}  &        &  74  & 4  \\
FRIQUEE-LUMA\textsubscript{std}  &        & 74   & 8  \\
FRIQUEE-CHROMA\textsubscript{avg}  &        &  80  & 10 \\
FRIQUEE-CHROMA\textsubscript{std}  &        &  80  &  1 \\
FRIQUEE-LMS\textsubscript{avg}  &        &  74  & 1 \\
FRIQUEE-LMS\textsubscript{std}  &        & 74   & 0  \\
FRIQUEE-HS\textsubscript{avg}  &        &  4 & 0  \\
FRIQUEE-HS\textsubscript{std}  &        & 4   & 0  \\
TLVQM-LCF\textsubscript{avg}  &        & 22   &  5 \\
TLVQM-LCF\textsubscript{std}  &        & 23   &  3 \\
TLVQM-HCF  &        &  30 & 6 \\ \hline\-1.em]
\multirow{3}{*}{KoNViD} & NIQE (1 fr/sec)    & 0.5417 & 0.3790  & 0.5530  & 0.5336  \\
& ILNIQE (1 fr/sec)  & 0.5264  & 0.3692  & 0.5400  & 0.5406  \\
& VIIDEO  & 0.2988 & 0.2036  & 0.3002 & 0.6101  \\
\hline\-1.em]
\multirow{3}{*}{YT-UGC}
& NIQE (1 fr/sec) & 0.2379 & 0.1600 & 0.2776 & 0.6174  \\
& ILNIQE (1 fr/sec) & 0.2918 & 0.1980 & 0.3302 & 0.6052  \\
& VIIDEO  & 0.0580 & 0.0389  & 0.1534  & 0.6339 \\
\hline\-1.em]

\textsc{Model} \textbackslash\ \textsc{Metric} & \textsc{SRCC (std)}    & \textsc{KRCC (std)}    & \textsc{PLCC (std)}     & \textsc{RMSE (std)} & & \textsc{SRCC (std)}    & \textsc{KRCC (std)}    & \textsc{PLCC (std)}     & \textsc{RMSE (std)}   \\ 
\hline\-1.em]

\textsc{Model} \textbackslash\ \textsc{Metric} & \textsc{SRCC (std)}    & \textsc{KRCC (std)}    & \textsc{PLCC (std)}     & \textsc{RMSE (std)} & & \textsc{SRCC (std)}    & \textsc{KRCC (std)}    & \textsc{PLCC (std)}     & \textsc{RMSE (std)}   \\
\hline\-2ex]

\subfloat[HOSA]{\includegraphics[width=\xwidth\textwidth]{figs/ALL_COMBINED_HOSA_kfCV_corr.pdf}
\label{fig1f}}
\hspace{\hswidth}
\subfloat[VGG-19]{\includegraphics[width=\xwidth\textwidth]{figs/ALL_COMBINED_vgg19_kfCV_corr.pdf}
\label{fig1g}}
\hspace{\hswidth}
\subfloat[ResNet-50]{\includegraphics[width=\xwidth\textwidth]{figs/ALL_COMBINED_resnet50_kfCV_corr.pdf}
\label{fig1h}} 
\hspace{\hswidth}
\subfloat[KonCept512]{\includegraphics[width=\xwidth\textwidth]{figs/ALL_COMBINED_KONCEPT512_kfCV_corr.pdf}
\label{fig1i}} 
\hspace{\hswidth}
\subfloat[PaQ-2-PiQ]{\includegraphics[width=\xwidth\textwidth]{figs/ALL_COMBINED_PAQ2PIQ_kfCV_corr.pdf}
\label{fig1j}} \\ [-2ex]

\subfloat[V-BLIINDS]{\includegraphics[width=\xwidth\textwidth]{figs/ALL_COMBINED_VBLIINDS_org_kfCV_corr.pdf}
\label{fig1k}}
\hspace{\hswidth}
\subfloat[TLVQM]{\includegraphics[width=\xwidth\textwidth]{figs/ALL_COMBINED_TLVQM_kfCV_corr.pdf}
\label{fig1l}}
\hspace{\hswidth}
\subfloat[{VIDEVAL}]{\includegraphics[width=\xwidth\textwidth]{figs/ALL_COMBINED_FFVIQE_release_kfCV_corr.pdf}
\label{fig1m}}
\caption{Scatter plots and nonlinear logistic fitted curves of VQA models versus MOS trained with a grid-search SVR using -fold cross-validation on the All-Combined\textsubscript{c} set. (a) BRISQUE (1 fr/sec), (b) GM-LOG (1 fr/sec), (c) HIGRADE (1 fr/sec), (d) FRIQUEE (1 fr/sec), (e) CORNIA (1 fr/sec), (f) HOSA (1 fr/sec), (g) VGG-19 (1 fr/sec), (h) ResNet-50 (1 fr/sec), (i) KonCept512 (1 fr/sec), (j) PaQ-2-PiQ (1 fr/sec), (k) V-BLIINDS, (l) TLVQM, and (m) VIDEVAL.}
\label{fig:kvcv_draw}
\end{figure*}

\section{Experimental Results}
\label{sec:exp}

\subsection{Evaluation Protocol}
\label{ssec:eval_proto}
\textbf{UGC Dataset Benchmarks.} To conduct BVQA performance evaluation, we used the three UGC-VQA databases: KoNViD-1K \cite{hosu2017konstanz}, LIVE-VQC \cite{sinno2018large}, and YouTube-UGC \cite{wang2019youtube}. We found that the YouTube-UGC dataset contains 57 grayscale videos, which yield numerical errors when computing the color model FRIQUEE. Therefore, we extracted a subset of 1,323 color videos from YouTube-UGC, which we denote here as the YouTube-UGC\textsubscript{c} set, for the evaluation of color models. In order to study overall model performances on all the databases, we created a large composite benchmark, which is referred to here as All-Combined\textsubscript{c}, using the iterative nested least squares algorithm (INLSA) suggested in \cite{pinson2003objective}, wherein YouTube-UGC is selected as the anchor set, and the objective MOS from the other two sets, KoNViD-1k and LIVE-VQC, are linearly mapped onto a common scale (). Figure \ref{fig:inlsa} shows scatter plots of MOS versus NIQE scores before (Figure \ref{fig:inlsa-a}) and after (Figure \ref{fig:inlsa-b}) INLSA linear mapping, calibrated by NIQE \cite{mittal2012making} scores. The All-Combined\textsubscript{c} (3,108) dataset is simply the union of KoNViD-1k (1,200), LIVE-VQC (575), and YouTube-UGC\textsubscript{c} (1,323) after MOS calibration:


where (\ref{eq:mos_cal_kon}) and (\ref{eq:mos_cal_live}) are for calibrating KoNViD-1k and LIVE-VQC, respectively.  denotes the adjusted scores, while  is the original MOS.

\textbf{BVQA Model Benchmarks.} We include a number of representative BVQA/BIQA algorithms in our benchmarking evaluation as references to be compared against. These baseline models include NIQE \cite{mittal2012making}, ILNIQE \cite{zhang2015feature}, VIIDEO \cite{mittal2015completely}, BRISQUE \cite{mittal2012no}, GM-LOG \cite{xue2014blind}, HIGRADE \cite{kundu2017no}, FRIQUEE \cite{ghadiyaram2017perceptual}, CORNIA \cite{ye2012unsupervised}, HOSA \cite{xu2016blind}, KonCept512 \cite{hosu2020koniq}, PaQ-2-PiQ \cite{ying2019patches}, V-BLIINDS \cite{saad2014blind}, and TLVQM \cite{korhonen2019two}. Among these, NIQE, ILNIQE, and VIIDEO are ``completely blind'' (opinion-unaware (OU)), since no training is required to build them. The rest of the models are all training-based (opinion-aware (OA)) and we re-train the models/features when evaluating on a given dataset. We also utilized the well-known deep CNN models VGG-19 \cite{simonyan2014very} and ResNet-50 \cite{he2016deep} as additional CNN-based baseline models, where each was pretrained on the ImageNet classification task. The fully-connected layer (4,096-dim) from VGG-19 and average-pooled layer (2,048-dim) from ResNet-50 served as deep feature descriptors, by operating on 25 227227 random crops of each input frame, then average-pooled into a single feature vector representing the entire frame \cite{kim2017deep}. Two SOTA deep BIQA models, KonCept512 \cite{hosu2020koniq} and PaQ-2-PiQ \cite{ying2019patches}, were also included in our evaluations. We implemented the feature extraction process for each evaluated BVQA model using its initial released implementation in MATLAB R2018b, except that VGG-19 and ResNet-50 were implemented in TensorFlow, while KonCept512\footnote{\url{https://github.com/ZhengyuZhao/koniq-PyTorch}} and PaQ-2-PiQ\footnote{\url{https://github.com/baidut/paq2piq}} were implemented in PyTorch. All the feature-based BIQA models extract features at a uniform sampling rate of one frame per second, then temporally average-pooled to obtain the overall video-level feature.

\textbf{Regression Models.} We used a support vector regressor (SVR) as the back-end regression model to learn the feature-to-score mappings, since it achieves excellent performance in most cases \cite{korhonen2019two, saad2014blind, kim2017deep, ghadiyaram2017perceptual, mittal2012no, xu2014no}. The effectiveness of SVR, however, largely depends on the selection of its hyperparameters. As recommended in \cite{chang2011libsvm}, we optimized the SVR parameter values  by a grid-search of  exponentially growing sequences (in our experiments, we used a grid of  ) using cross-validation on the training set. The pair  yielding the best cross-validation performance, as measured by the root mean squared error (RMSE) between the predicted scores and the MOS, is picked. Afterward, the selected model parameters are applied to re-train the model on the entire training set, and we report the evaluation results on the test set. This kind of cross-validation procedure can prevent over-fitting, thus providing fair evaluation of the compared BVQA models. We chose the linear kernel for CORNIA, HOSA, VGG-19, and ResNet-50, considering their large feature dimension, and the radial basis function (RBF) kernel for all the other algorithms. We used Python 3.6.7 with the scikit-learn toolbox to train and test all the evaluated learning-based BVQA models.

\textbf{Performance Metrics.} Following convention, we randomly split the dataset into non-overlapping training and test sets (), where the regression model was trained on the training set, and the performance was reported on the test set. This process of random split was iterated 100 times and the overall median performance was recorded. For each iteration, we adopted four commonly used performance criteria to evaluate the models: The Spearman Rank-Order Correlation Coefficient (SRCC) and the Kendall Rank-Order Correlation Coefficient (KRCC) are non-parametric measures of prediction monotonicity, while the Pearson Linear Correlation Coefficient (PLCC) with corresponding Root Mean Square Error (RMSE) are computed to assess prediction accuracy. Note that PLCC and RMSE are computed after performing a nonlinear four-parametric logistic regression to linearize the objective predictions to be on the same scale of MOS \cite{seshadrinathan2010study}.

\subsection{Performance on Individual and Combined Datasets}
\label{ssec:performance_diff_datasets}

Table \ref{table:compete-blind} shows the performance evaluation of the three ``completely blind'' BVQA models, NIQE, ILNIQE, and VIIDEO on the four UGC-VQA benchmarks. None of these methods performed very well, meaning that we still have much room for developing OU ``completely blind'' UGC video quality models.

Table \ref{table:eval_svr} shows the performance evaluation of all the learning-based BVQA models trained with SVR on the four datasets in our evaluation framework. For better visualization, we also show box plots of performances as well as scatter plots of predictions versus MOS on the All-Combined\textsubscript{c} set, in Figures \ref{fig:all_boxplot} and \ref{fig:kvcv_draw}, respectively. Overall, VIDEVAL achieves SOTA or near-SOTA performance on all the test sets. On LIVE-VQC, however, TLVQM outperformed other BVQA models by a notable margin, while it significantly underperformed on the more recent YouTube-UGC database. We observed in Section \ref{ssec:observation} that LIVE-VQC videos generally contain more (camera) motions than KoNViD-1k and YouTube-UGC, and TLVQM computes multiple motion relevant features. Moreover, the only three BVQA models containing temporal features (V-BLIINDS, TLVQM, and VIDEVAL) excelled on LIVE-VQC, which suggests that it is potentially valuable to integrate at least a few, if not many, motion-related features into quality prediction models, when assessing on videos with large (camera) motions.

\begin{table}[!t]
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.1}
\centering
\caption{Performances on different resolution subsets: 1080p (427), 720p (566), and 480p (448).}
\label{table:resolution_breakdown}
\begin{tabular}{lccccccccccccccccccccccccc}
\toprule
\textsc{Subset} & \multicolumn{2}{c}{1080p} & & \multicolumn{2}{c}{720p}  & & \multicolumn{2}{c}{480p} \\ \cline{2-3}\cline{5-6}\cline{8-9}\-1.em]
BRISQUE   & 0.4597 & 0.4637 & & 0.5407 & 0.5585 & & 0.3812 & 0.4065  \\
GM-LOG    & 0.4796 & 0.4970 & & 0.5098 & 0.5172 & & 0.3685 & 0.4200  \\
HIGRADE   & 0.5142 & 0.5543 & & 0.5095 & 0.5324 & & 0.4650 & 0.4642  \\
FRIQUEE   & 0.5787 & 0.5797 & & 0.5369 & 0.5652 & & 0.5042 & 0.5363  \\
CORNIA    & 0.5951 & \textbf{0.6358} & & 0.6212 & 0.6551 & & 0.5631 & 0.6118  \\
HOSA      & 0.5924 & 0.6093 & & \textbf{\underline{0.6651}} & \textbf{0.6739} & & \textbf{0.6514} & \textbf{0.6652}  \\
VGG-19     & \textbf{0.6440} & 0.6090 & & 0.6158 & \textbf{0.6568} & & \textbf{0.5845} & \textbf{0.6267}  \\
ResNet-50  & \textbf{\underline{0.6615}} & \textbf{\underline{0.6644}} & & \textbf{0.6645} & \textbf{\underline{0.7076}} & & \textbf{\underline{0.6570}} & \textbf{\underline{0.6997}}  \\
{KonCept512} & \textbf{0.6332} & \textbf{0.6336} & & 0.6055 & 0.6514 & & 0.4271 & 0.4612 \\
{PaQ-2-PiQ} & 0.5304  & 0.5176 & & 0.5768 & 0.5802 & &  0.3646 & 0.4748  \\
V-BLIINDS & 0.4449 & 0.4491 & & 0.5546 & 0.5719 & & 0.4484 & 0.4752  \\
TLVQM     & 0.5638 & 0.6031 & & \textbf{0.6300} & 0.6526 & & 0.4318 & 0.4784 \\
VIDEVAL    & 0.5805 & 0.6111 & & {0.6296} & {0.6393} & & 0.5014 & 0.5508 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!t]
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.1}
\centering
\caption{Performances on different content subsets: screen content (163), animation (81), and gaming (209).}
\label{table:content_breakdown}
\begin{tabular}{lccccccccccccccccccccccccc}
\toprule
\textsc{Subset} & \multicolumn{2}{c}{Screen Content} & & \multicolumn{2}{c}{Animation}  & & \multicolumn{2}{c}{Gaming} \\ \cline{2-3}\cline{5-6}\cline{8-9}\-1.em]
BRISQUE   & 0.2573 & 0.3954 & & 0.0747 & 0.3857 & & 0.2717 & 0.3307  \\
GM-LOG    & 0.3004 & 0.4244 & & 0.2009 & 0.4129 & & 0.3371 & 0.4185  \\
HIGRADE   & 0.4971 & 0.5652 & & 0.1985 & 0.4140 & & 0.6228 & 0.6832  \\
FRIQUEE   & \textbf{0.5522} & \textbf{0.6160} & & 0.2377 & 0.4574 & & \textbf{0.6919} & \textbf{0.7193}  \\
CORNIA    & 0.5105 & 0.5667 & & 0.1936 & 0.4627 & & 0.5741 & 0.6502  \\
HOSA      & 0.4667 & 0.5255 & & 0.1048 & 0.4489 & & 0.6019 & \textbf{0.6998}  \\
VGG-19     & 0.5472 & 0.6229 & & 0.1973 & 0.4700 & & 0.5765 & 0.6370  \\
ResNet-50  & \textbf{\underline{0.6199}} & \textbf{\underline{0.6676}} & & \textbf{0.2781} & \textbf{0.4871} & & \textbf{0.6378} & 0.6779  \\
{KonCept512} & 0.4714 & 0.5119 & & \textbf{0.2757} & \textbf{0.5229} & & 0.4780 & 0.6240  \\
{PaQ-2-PiQ} & 0.3231 & 0.4312 & & 0.0208 & 0.4630 & & 0.2169 & 0.3874 \\
V-BLIINDS & 0.3064 & 0.4155 & & 0.0379 & 0.3917 & & 0.5473 & 0.6101  \\
TLVQM     & 0.3843 & 0.4524 & & 0.2708 & 0.4598 & & 0.5749 & 0.6195 \\
VIDEVAL    & \textbf{{0.6033}} & \textbf{{0.6610}} & & \textbf{\underline{0.3492}}  & \textbf{\underline{0.5274}} & & \textbf{\underline{0.6954}} & \textbf{\underline{0.7323}}  \\
\bottomrule
\end{tabular}
\end{table}

It is also worth mentioning that the deep CNN baseline methods (VGG-19 and ResNet-50), despite being trained as picture-only models, performed quite well on KoNViD-1k and All-Combined\textsubscript{c}. This suggests that transfer learning is a promising technique for the blind UGC-VQA problem, consistent with conclusions drawn for picture-quality prediction \cite{kim2017deep}. Deep models will perform even better, no doubt, if trained on temporal content and distortions. 

{The two most recent deep learning picture quality models, PaQ-2-PiQ, and KonCept512, however, did not perform very well on the three evaluated video datasets. The most probable reason would be that these models were trained on picture quality datasets \cite{ying2019patches, hosu2020koniq}, which contain different types of (strictly spatial) distortions than UGC-VQA databases. Models trained on picture quality sets do not necessarily transfer very well to UGC video quality problems. In other words, whatever model should be either trained or fine-tuned on UGC-VQA datasets in order to obtain reasonable performance. Indeed, if temporal distortions (like judder) are present, they may severely underperform if the frame quality is high \cite{madhusudana2020subjective}.}

\begin{table}[!t]
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.1}
\centering
\caption{Performances on different quality subsets: low quality (1558) and high quality (1550).}
\label{table:quality_breakdown}
\begin{tabular}{lccccccccccccccccccccccccc}
\toprule
\textsc{Subset} & \multicolumn{2}{c}{Low Quality} & & \multicolumn{2}{c}{High Quality} \\ \cline{2-3}\cline{5-6}\-1.em]
BRISQUE   & 0.4312 & 0.4593 & & 0.2813 & 0.2979   \\
GM-LOG    & 0.4221 & 0.4715 & & 0.2367 & 0.2621   \\
HIGRADE   & 0.5057 & 0.5466 & & 0.4714 & 0.4799   \\
FRIQUEE   & \textbf{0.5460} & \textbf{0.5886} & & \textbf{0.5061} & \textbf{0.5152}   \\
CORNIA    & 0.4931 & 0.5435 & & 0.3610 & 0.3748   \\
HOSA      & \textbf{0.5348} & \textbf{0.5789} & & 0.4208 & 0.4323   \\
VGG-19     & 0.3710 & 0.4181 & & 0.3522 & 0.3614   \\
ResNet-50  & 0.3881 & 0.4250 & & 0.2791 & 0.3030   \\
{KonCept512} & 0.3428 & 0.4497 & & 0.2245 & 0.2597 & \\
{PaQ-2-PiQ} & 0.2438 & 0.2713 & & 0.2013 & 0.2252 \\
V-BLIINDS & 0.4703 & 0.5060 & & 0.3207 & 0.3444   \\
TLVQM     & 0.4845 & 0.5386 & & \textbf{0.4783} & \textbf{0.4860}   \\
VIDEVAL   & \textbf{\underline{0.5680}} & \textbf{\underline{0.6056}} & & \textbf{\underline{0.5546}} & \textbf{\underline{0.5657}} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!t]
\setlength{\tabcolsep}{3.5pt}
\renewcommand{\arraystretch}{1.1}
\centering
\caption{Best model in terms of SRCC for cross dataset generalization evaluation.}
\label{table:cross_dataset_srcc}
\begin{tabular}{lcccccccc}
\toprule
\textsc{Train}\textbackslash\textsc{Test}  & LIVE-VQC &  KoNViD-1k &  YouTube-UGC\textsubscript{c} \\
\hline\-1.em]
 LIVE-VQC & - &   ResNet-50 (0.70)  & VIDEVAL (0.35)    \\
 KoNViD-1k & ResNet-50 (0.75) & - & VIDEVAL (0.39)  \\
 YouTube-UGC\textsubscript{c} & HOSA (0.50)  &  VIDEVAL (0.62)  & -    \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Performance Evaluation on Categorical Subsets}

We propose three new categorical evaluation methodologies - resolution, quality, and content-based category breakdown. These will allow us to study the compared BVQA models from additional and practical aspects in the context of real-world UGC scenarios, which have not been, nor can it be accounted in previous legacy VQA databases or studies.

\begin{table*}[!t]
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.1}
\centering
\caption{{Performance comparison of a total of eleven temporal pooling methods using TLVQM and VIDEVAL as testbeds on KoNViD-1k, LIVE-VQC, and YouTube-UGC. The three best results along each column are \textbf{boldfaced}.}}
\label{table:pooling}
\begin{tabular}{lccccccccccccccccccccccc}
\toprule
\textsc{Database} & \multicolumn{5}{c}{KoNViD-1k} & & \multicolumn{5}{c}{LIVE-VQC} & & \multicolumn{5}{c}{YouTube-UGC} \\ \cline{2-6}\cline{8-12}\cline{14-18}\-1.em]

\textsc{Pooling} & SRCC & PLCC & & SRCC &  PLCC & & SRCC & PLCC & & SRCC & PLCC & & SRCC & PLCC & & SRCC & PLCC \\ \hline\-1.em]
\multirow{15}{*}{\shortstack{IQA}} & NIQE (1 fr/sec) & Spatial NSS & 1 &  : window size  &  6.3 \\
 & ILNIQE (1 fr/sec) & {Spatial NSS, gradient, log-Gabor, and color statistics} & 1 & { : window size; : filter size; : log-Gabor filter size} & 23.3 \\
 & BRISQUE (1 fr/sec) & Spatial NSS & 36 &  : window size  &  1.7 \\
& GM-LOG (1 fr/sec) & Joint statistics of gradient magnitude and laplacian of gaussian coefficients & 40 & { : window size; : probability matrix size}  &  2.1 \\
& HIGRADE (1 fr/sec) & Spatial NSS, and gradient magnitude statistics in LAB color space & 216 & { : window size; : gradient kernel size} & 11.6 \\
& FRIQUEE (1 fr/sec) & Complex streerable pyramid wavelet, luminance, chroma, LMS, HSI, yellow channel, and their transformed domain statistics & 560 &  : window size; : number of color spaces; : neighborhood size in DNT  & 701.2 \\
 & CORNIA (1 fr/sec) & Spatially normalized image patches and max min pooling & 10k &  : window size : codebook size  & 14.3 \\
  & HOSA (1 fr/sec) & Local normalized image patches based on high order statistics aggregation & 14.7k &  : window size : codebook size  & 1.2 \\ \-1.em]
\multirow{10}{*}{\shortstack{VQA}}    & VIIDEO  & Frame difference spatial statistics, inter sub-band statistics  & 1 &  & 674.8 \\
  & V-BLIINDS  & Spatial NSS, frame difference DCT coefficient statistics, motion coherency, and egomotion & 47 &  : window size; : block size; : motion vector tensor size  & 1989.9 \\
& TLVQM & Captures impairments computed at two computation levels: low complexity and high complexity features & 75 &  : filter size; : motion estimation block size; : number of key points & 183.8  \\
& VIDEVAL & Selected combination of NSS features in multiple perceptual spaces and using visual impairment features from TLVQM & 60 &  : filter size; : number of color spaces; : motion estimation block size; : number of key points & 305.8 \\

\bottomrule
\end{tabular}
\begin{tablenotes}[para,flushleft]
\item : number of pixels per frame; : number of frames computed for feature extraction. Note that for VIIDEO and V-BLIINDS,  is the total number of frames, whereas for IQA models,  equals the total number of frames sampled at 1 fr/sec. For TLVQM and VIDEVAL,  is total number of frames divided by 2, while  is the number of frames sampled at 1 fr/sec.
\end{tablenotes}
\end{threeparttable}
\end{table*}

We also divided the All-Combined\textsubscript{c} into subsets based on content category: Screen Content (163), Animation (81), Gaming (209), and Natural (2,667) videos. We only reported the evaluation results on the first three subsets in Table \ref{table:content_breakdown}, since we observed similar results on the Natural subset with the entire combined set. The proposed VIDEVAL model outperformed over all categories, followed by ResNet-50 and FRIQUEE, suggesting that VIDEVAL features are robust quality indicatives across different content categories.

The third categorical division is based on quality scores: we partitioned the combined set into Low Quality (1,558) and High Quality (1,550) halves, using the median quality value 3.5536 as the threshold, to see the model performance only  on high/low quality videos. Performance results are shown in Table \ref{table:quality_breakdown}, wherein VIDEVAL still outperformed the other BVQA models on both low and high quality partitions.

\subsection{Cross Dataset Generalizability}

We also performed a cross dataset evaluation to verify the generalizability of BVQA models, wherein LIVE-VQC, KoNViD-1k, and YouTube-UGC\textsubscript{c} were included. That is, we trained the regression model on one full database and report the performance on another. To retain label consistency, we linearly scaled the MOS values in LIVE-VQC from raw  to , which is the scale for the other two datasets. We used SVR for regression and adopted -fold cross validation using the same grid-search as in Section \ref{ssec:eval_proto} for hyperparameter selection. The selected parameter pair were then applied to re-train the SVR model on the full training set, and the performance results on the test set were recorded. Table \ref{table:cross_dataset_srcc} and \ref{table:cross_dataset_plcc} show the best performing methods with cross domain performances in terms of SRCC and PLCC, respectively.

We may see that the cross domain BVQA algorithm generalization between LIVE-VQC and KoNViD-1k was surprisingly good, and was well characterized by pre-trained ResNet-50 features. We also observed better algorithm generalization between KoNViD-1k and YouTube-UGC than LIVE-VQC, as indicated by the performances of the best model, VIDEVAL. This might be expected, since as Figure \ref{fig:tsne} shows, YouTube-UGC and KoNViD-1k share overlapped coverage of content space, much larger than that of LIVE-VQC. Therefore, we may conclude that VIDEVAL and ResNet-50 were the most robust BVQA models among those compared in terms of cross domain generalization capacities.

\subsection{Effects of Temporal Pooling}

Temporal pooling is one of the most important, unresolved problems for video quality prediction \cite{park2012video, tu2020comparative,seshadrinathan2011temporal, korhonen2019two, bampis2018recurrent}. In our previous work \cite{tu2020comparative}, we have studied the efficacy of various pooling methods using scores predicted by BIQA models. Here we extend this to evaluate on SOTA BVQA models. For practical considerations, the high-performing TLVQM and VIDEVAL were selected as exemplar models. Since these two models independently extract features on each one-second block, we applied temporal pooling of chunk-wise quality predictions. A total of eleven pooling methods were tested: three Pythagorean means (arithmetic, geometric, and harmonic mean), median, Minkowski () mean, percentile pooling () \cite{moorthy2009visual}, VQPooling \cite{park2012video}, primacy and recency pooling \cite{murdock1962serial}, hysteresis pooling \cite{seshadrinathan2011temporal}, and our previously proposed ensemble method, EPooling \cite{tu2020comparative}, which aggregates multiply pooled scores by training a second regressor on top of mean, Minkowski, percentile, VQPooling, variation, and hysteresis pooling. We refer the reader to \cite{tu2020comparative} for detailed algorithmic formulations and parameter settings thereof.

It is worth noting that the results in Table \ref{table:pooling} are only \textit{self-consistent}, meaning that they are not comparable to any prior experiments - since we employed chunk-wise instead of previously adopted video-wise quality prediction to be able to apply temporal quality pooling, which may affect the base performance. Here we observed yet slightly different results using BVQA testbeds as compared to what we observed on BIQA \cite{tu2020comparative}. Generally, we found the mean families and ensemble pooling to be the most reliable pooling methods. Traditional sample mean prediction may be adequate in many cases, due to its simplicity. Pooling strategies that more heavily weight low-quality parts, however, were not observed to perform very well on the tested BVQA, which might be attributed to the fact that not enough samples () can be extracted from each video to attain statistically meaningful results.

\subsection{Complexity Analysis and Runtime Comparison}

The efficiency of a video quality model is of vital importance in practical commercial deployments. Therefore, we also tabulated the computational complexity and runtime cost of the compared BVQA models, as shown in Tables \ref{table:complexity}, \ref{table:time_dl}. The experiments were performed in MATLAB R2018b and Python 3.6.7 under Ubuntu 18.04.3 LTS system on a Dell OptiPlex 7080 Desktop with Intel Core i7-8700 CPU@3.2GHz, 32G RAM, and GeForce GTX 1050 Graphics Cards. The average feature computation time of MATLAB-implemented BVQA models on 1080p videos are reported in Table \ref{table:complexity}. The proposed VIDEVAL method achieves a reasonable complexity among the top-performing algorithms, TLVQM, and FRIQUEE. We also present theoretical time complexity in Table \ref{table:complexity} for potential analytical purposes.

{We also provide in Table \ref{table:time_dl} an additional runtime comparison between MATLAB models on CPU and deep learning models on CPU and GPU, respectively. It may be observed that top-performing BVQA models such as TLVQM and VIDEVAL are essentially slower than deep CNN models, but we expect orders-of-magnitude speedup if re-implemented in pure . Simpler NSS-based models such as BRISQUE and HIGRADE (which only involve several convolution operations) still show competitive efficiency relative to CNN models even when implemented in MATLAB. We have also seen a  times speedup switching from CPU to GPU for the CNN models, among which KonCept512 with PyTorch-GPU was the fastest since it requires just a single pass to the CNN backbone, while the other three entail multiple passes for each input frame.}

Note that the training/test time of the machine learning regressor is approximately proportional to the number of features. Thus, it is not negligible compared to feature computation given a large number of features, regardless of the regression model employed. The feature dimension of each model is listed in Table \ref{table:complexity}. As may be seen, codebook-based algorithms (CORNIA (10) and HOSA (14.7)) require significantly larger numbers of features than other hand-crafted feature based models. Deep ConvNet features ranked second in dimension (VGG-19 (4,080) and ResNet-50 (2,048)). Our proposed VIDEVAL only uses 60 features, which is fairly compact, as compared to other top-performing BVQA models like FRIQUEE (560) and TLVQM (75).

\begin{table}[!t]
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.1}
\centering
\caption{{Run time comparison of feature-based and deep learning BVQA models (in seconds evaluated on twenty  videos from LIVE-VQC). Model loading time for deep models are excluded}.}
\label{table:time_dl}
\begin{tabular}{lrrccccccccccccccccccccccc}
\toprule
\textsc{Model} &  & \textsc{Time (Sec)} \\ \hline\-1.em]
VGG-19 (1 fr/sec) & TensorFlow-CPU  & 27.8 \\
                & TensorFlow-\textit{GPU} & 5.7 \\
ResNet-50 (1 fr/sec) & TensorFlow-CPU  & 9.6 \\
                & TensorFlow-\textit{GPU} & 1.9 \\
\hline\-1.em]
\multirow{5}{*}{KoNViD} & 
VIDEVAL  & 0.7832 & 0.5845 & 0.7803 & 0.4024 \\
& VIDEVALVGG-19 & 0.7827 & 0.5928 & 0.7913 & 0.3897   \\
& VIDEVALResNet-50 & 0.8129 & 0.6212 & \textbf{0.8200} & \textbf{0.3659} \\
& VIDEVALKonCept512  & \textbf{0.8149} & \textbf{0.6251} & 0.8169 & 0.3670  \\
& VIDEVALPaQ-2-PiQ  & 0.7844 & 0.5891 & 0.7793 & 0.4018  \\
\hline\-1.em]
\multirow{5}{*}{YT-UGC} & 
VIDEVAL  & 0.7787 & 0.5830 & 0.7733 & 0.4049 \\
& VIDEVALVGG-19 & 0.7868 & 0.5930 & 0.7847 & 0.3993  \\
& VIDEVALResNet-50 & \textbf{0.8085} & 0.6128 & \textbf{0.8033} & \textbf{0.3837} \\
& VIDEVALKonCept512 & 0.8083 & \textbf{0.6139} & 0.8028 & 0.3859 \\
& VIDEVALPaQ-2-PiQ & 0.7981 & 0.6015 & 0.7941 & 0.3959  \\

\hline\-1.em]
\multirow{5}{*}{All-Comb} &
VIDEVAL  & 0.7960 & 0.6032 & 0.7939 & 0.4268 \\
& VIDEVALVGG-19 & 0.7859 & 0.5912 & 0.7962 & 0.4202   \\
& VIDEVALResNet-50 & 0.8115 & \textbf{0.6207} & \textbf{0.8286} & \textbf{0.3871} \\
& VIDEVALKonCept512 & \textbf{0.8123} & 0.6193  & 0.8168  & 0.4017  \\
& VIDEVALPaQ-2-PiQ & 0.7962  & 0.5991 & 0.7934  & 0.4229 \\

\toprule
\end{tabular}
\end{table}

\subsection{Summarization and Takeaways}

Finally, we briefly summarize the experimental results and make additional observations:

\begin{enumerate}
    \item Generally, spatial distortions dominated quality prediction on Internet UGC videos like those from YouTube and Flickr, as revealed by the remarkable performances of picture-only models (e.g., HIGRADE, FRIQUEE, HOSA, ResNet-50) on them. Some motion-related features (as in TLVQM) may not apply as well in this scenario.
    \item On videos captured with mobile devices (e.g., those in LIVE-VQC), which often present larger and more frequent camera motions, including temporal- or motion-related features can be advantageous (e.g., V-BLIINDS, TLVQM, VIDEVAL).
    \item Deep CNN feature descriptors (VGG-19, ResNet-50, etc.) pre-trained for other classical vision tasks (e.g. image classification) are transferable to UGC video quality predictions, achieving very good performance, suggesting that using transfer learning to address the general UGC-VQA problem is very promising.
    \item It is still a very hard problem to predict UGC video quality on non-natural or computer-generated video contents: screen contents, animations, gaming, etc. Moreover, there are no sufficiently large UGC-VQA datasets designed for those kinds of contents.
    \item A simple feature engineering and selection implementation built on top of current effective feature-based BVQA models is able to obtain excellent performance, as exemplified by the compact new model (VIDEVAL).
    \item Simple temporal mean pooling of chunk-wise quality predictions by BVQA models yields decent and robust results. Furthermore, an ensemble pooling approach can noticeably improve the quality prediction performance, albeit with higher complexity.
    \item Ensembling scene statistics-based BVQA models with additional deep learning features (e.g., VIDEVAL plus KonCept512) could further raise the performance upper bound, which may be a promising way of developing future BVQA models.
\end{enumerate}

















\section{Conclusion}
\label{sec:conclud}
We have presented a comprehensive analysis and empirical study of blind video quality assessment for user-generated content (the \textbf{\textsf{UGC-VQA problem}}). We also proposed a new fusion-based BVQA model, called the VIDeo quality EVALuator (VIDEVAL), which uses a feature ensemble and selection procedure on top of existing efficient BVQA models. A systematic evaluation of prior leading video quality models was conducted within a unified and reproducible evaluation framework and accordingly, we concluded that a selected fusion of simple distortion-aware statistical video features, along with well-defined visual impairment features, is able to deliver state-of-the-art, robust performance at a very reasonable computational cost. The promising performances of baseline CNN models suggest the great potential of leveraging transfer learning techniques for the UGC-VQA problem. We believe that this benchmarking study will help facilitate UGC-VQA research by clarifying the current status of BVQA research and the relative efficacies of modern BVQA models. To promote reproducible research and public usage, an implementation of VIDEVAL has been made available online: \textbf{\url{https://github.com/vztu/VIDEVAL}}. In addition to the software, we are also maintaining an ongoing performance leaderboard on Github: \textbf{\url{https://github.com/vztu/BVQA_Benchmark}}.

















\ifCLASSOPTIONcaptionsoff
  \newpage
\fi










\bibliographystyle{IEEEtran}
\bibliography{ref}

























\end{document}
