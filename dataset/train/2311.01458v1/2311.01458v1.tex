
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{amsmath}       \usepackage{amsthm}
\usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{csquotes}
\usepackage{graphicx}
\usepackage{arydshln}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{epsfig}
\usepackage{setspace}
\usepackage{csquotes}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\title{Detecting Deepfakes Without Seeing Any}



\author{Tal Reiss  $\quad$  Bar Cavia $\quad$ Yedid Hoshen\\
School of Computer Science and Engineering\\
The Hebrew University of Jerusalem, Israel\\ 
{\tt\small tal.reiss@mail.huji.ac.il}\\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle
\begin{abstract}    
Deepfake attacks, malicious manipulation of media containing people, are a serious concern for society. Conventional deepfake detection methods train supervised classifiers to distinguish real media from previously encountered deepfakes. Such techniques can only detect deepfakes similar to those previously seen, but not zero-day (previously unseen) attack types. As current deepfake generation techniques are changing at a breathtaking pace, new attack types are proposed frequently, making this a major issue. Our main observations are that: i) in many effective deepfake attacks, the fake media must be accompanied by false facts i.e. claims about the identity, speech, motion, or appearance of the person. For instance, when impersonating Obama, the attacker explicitly or implicitly claims that the fake media show Obama; ii) current generative techniques cannot perfectly synthesize the false facts claimed by the attacker. We therefore introduce the concept of “fact checking”, adapted from fake news detection, for detecting zero-day deepfake attacks. Fact checking verifies that the claimed facts (e.g. identity is Obama), agree with the observed media (e.g. is the face really Obama’s?), and thus can differentiate between real and fake media. Consequently, we introduce FACTOR, a practical recipe for deepfake fact checking and demonstrate its power in critical attack settings: face swapping and audio-visual synthesis. Although it is training-free, relies exclusively on off-the-shelf features, is very easy to implement, and does not see any deepfakes, it achieves better than state-of-the-art accuracy.  Our code is available at \url{https://github.com/talreiss/FACTOR}.
\end{abstract}

\section{Introduction}
\vspace{0.5em}
\begin{quote}
\textit{The ability to disseminate large-scale disinformation to undermine
scientifically established facts poses an existential risk to humanity and
endangers democratic institutions and fundamental human rights.}
\begin{flushright}
    \small{-------- UN Report}
  \end{flushright}
\end{quote}

Deepfakes have been universally acknowledged to pose a grave threat to society. Bad actors can use fake information for various malicious purposes, including disinformation, societal polarization, embarrassment, and privacy violations. Detecting and flagging deepfakes can be an effective way of overcoming this threat. The astonishing rise in quality and prevalence of deep generative models allows fast and credible deepfake production, making it essential to develop automatic means for detecting them. This challenge has been taken up by the machine learning community, which has developed many detection methods. Despite significant progress, current methods are insufficient for tackling this formidable challenge. For example, approaches based on detecting high-frequency artifacts created by generative models, can be overcome by removing the artifacts or slightly changing model architectures. Today's most effective approaches use a supervised classifier trained to discriminate between real images and previously seen deepfake media. These methods assume that future deepfake attacks will be similar to previously observed ones, thus often do not generalize well to previously-unseen “zero-day\footnote{Zero-day attack: An attack that exploits a previously unknown vulnerability}” deepfake attacks. This problem will only get worse, as generative models, which are the key technology behind deepfakes, are advancing at an unprecedented pace. There is therefore a clear and immediate need for new methods that overcome this generalization gap.

This paper suggests using the concept of \textit{fact checking} for detecting zero-day deepfake attacks. This idea is adapted from fake news detection \citep{fake_news_1,fake_news_2,fake_news_3}. In some of the most important deepfake attack scenarios, the fake media (image, video, etc.) must be accompanied by false facts e.g. claims about facial identity, speech, activity or appearance. Here, we deal with several critical scenarios illustrated in \cref{fig:conditional_df}: i) face manipulation attacks, wherein the identity in a video is manipulated to that of the impersonated person. This is usually accompanied by a fact, namely, a claim about the identity of this person, e.g., “A video of Barack Obama playing the harp”; ii) audio-visual attacks, where a video is manipulated to appear as if a person utters some speech or the audio is manipulated to align with some video. The claimed fact is that the video and audio describe the same event; iii) we explore the extension of this concept beyond facial attacks, analyzing a simplified text-to-image attack scenario where the fake image is augmented with a prompt. The false fact is that the fake image and prompt describe the same content.

The key assumption made in this work is that current generative models are not yet accurate enough to encode the false fact into fake media with sufficient accuracy. For example, when manipulating the facial identity in a video, the observed fake identity will be distinguishable from the identity claimed by the attacker. By checking the false fact provided by the attacker (claimed facial identity), we can distinguish between real and fake images and videos. For instance, we can detect fake images when they do not match Obama's facial identity. We present FACTOR, a general recipe for implementing fact checking, illustrated in \cref{fig:factor}. In our formulation, false facts assert that the fake media describe the same content as some other media. FACTOR computes the \textit{truth score} (similarity function) between the media. In all cases shown here, we use off-the-shelf features pretrained on real data. If the truth score is low, we predict that the fact is false, and the media are fake sources of information.

\begin{figure}[t]
  \begin{center}
    \begin{tabular}{c}
    \includegraphics[scale=1.075]{figures/alignment.png} 
    \end{tabular}
  \end{center}
  \vspace{-0.5em}
    \caption{False facts in deepfake attacks. \textit{(a)} Face forgery: the claimed identity is seamlessly blended into the original image. The observed image is accompanied by a false fact i.e., “an image of Barack Obama”. \textit{(b)} Audio-Visual (AV): fake audio is generated to align with the original video or fake video is generated to align with the original audio. Fake media are accompanied by a false fact, that the video and audio describe the same event. \textit{(c)} Text-to-Image (TTI): the textual prompt is used by a generative model e.g. Stable Diffusion, to generate a corresponding image. The fake image is accompanied by a false fact, that the caption and the image describe the same content.}
    \label{fig:conditional_df}
    \vspace{-0.25em}
\end{figure}

Despite being training-free and very simple to implement, not using any fake data for pretraining and relying entirely on off-the-shelf features, we demonstrate the superiority of our approach across many competitive benchmarks. \textit{Our main contributions are:}
\begin{enumerate}
    \item Proposing the concept of fact checking for deepfake detection.
    \item Introducing FACTOR, a practical recipe for deepfake fact checking and demonstrating its power in critical attack settings: face swapping and audio-visual synthesis.
    \item Exploring the applicability of fact checking beyond facial attacks.
\end{enumerate}

\section{Related Work}

\textbf{Image synthesis.} Fake images are created either by manipulating parts of existing images or by generating them from scratch. Examples of the former include techniques that modify attributes in a source image or those that replace the original face in an image or video with a target face \citep{faceswap1,faceswap2,faceswap3,faceswap4}. The other class of methods, however, involves generating all pixels from scratch, whether from random noise \citep{stylegan} or text prompts \citep{stable_diffusion,dalle-2,imagen}.

\textbf{General deepfake detection.} As deepfake technology advances, significant efforts have been devoted to identifying manipulated media. Traditional approaches focus on examining image statistics changes, detecting cues such as compression artifacts \citep{compression_artifacts}. Learning-based methods have also been employed, with an initial emphasis on whether classifiers could effectively distinguish images from the same generative model \citep{learning1,learning2_freq,learning3_ff++}. Recent studies \citep{efros_paper,generalization_isola} shift towards classifiers capable of generalizing to different generative models, demonstrating the efficacy of neural networks trained on real and fake images from one GAN model for detecting images from other GAN models. However, \cite{df_knn_paper} emphasizes the non-generalizability of neural networks to unknown families of generative models when trained for fake image detection.

\textbf{Face forgery detection.} Early approaches relied on supervised learning to transform cropped face images into feature vectors for binary classification \citep{face_forgery1_ffd,face_forgery2,learning3_ff++}. However, it became evident that relying solely on classification methods had limitations, often leading to overfitting of training data and potentially missing subtle distinctions between real and fake images. Incorporating frequency information proved invaluable for face forgery detection, enabling the identification of specific artifacts associated with manipulation \citep{learning2_freq,frequency1,high_freq_eccv_2020_f3net,srm_high_freq,spsl_high_freq}. However, it is noteworthy that these cues can sometimes be overcome by techniques such as artifact removal or slight alterations to model architectures. Recent research efforts have increasingly prioritized the improvement of generalization in forgery detection models, recognizing the significance of detecting previously unseen forgeries \citep{generalization1_recce,generalization2,generalization3}. \citet{identity_cvpr23} uses face identity and face recognition features for supervised training of deepfake detectors. In contrast, our approach does not need training, and therefore enjoys better generalization.

\textbf{Audio-Visual deepfake detection.} In the domain of identifying manipulated speech videos, prior research focused on exploiting audio-visual inconsistencies as a crucial cue. Many approaches, rooted in supervised learning, have been devised to directly train audio-visual networks, enabling them to discern video authenticity \citep{av_related1,av_related2}. Recently, attention has shifted towards audio-visual self-supervision as a pretraining strategy. This entails self-supervised training, followed by fine-tuning with real/fake labels \citep{byol,real_forensics}. Some methods incorporate lip-reading data for this purpose \citep{lip_forensics}, while others implicitly integrate it into audio-visual synchronization signals \citep{AD_DFD}. \citet{owens} proposed AVAD, probably the most related method, which adapts ideas from anomaly detection for AV deepfakes, and does not use fake data for training. The method requires training a multimodal transformer using multi-objective terms. Our method achieves higher accuracy while being far simpler, using only off-the-shelf feature extractors and not requiring training.

\section{Fact Checking for Deepfake Detection}
\label{sec:factchecking}

This paper introduces the concept of fact checking for detecting deepfake attacks. Previous approaches train supervised classifiers to discriminate real from previously seen deepfake data. This is an effective approach for detecting attacks similar to those seen before, but it does not generalize well to zero-day attacks. Instead, we introduce the concept of fact checking for deepfake detection, which we adapt from fake news detection \citep{fake_news_1,fake_news_2,fake_news_3}. Many effective deepfake attacks are accompanied by false facts; facts are claims about the fake media such as their facial identity, speech, actions or appearance. Claiming these facts is essentially the core of the attack. For example, a face swapping attack must be joined with a claim such as “the identity of the person in this image is Obama”. This can be stated in the caption provided by the attacker e.g. “President Obama smiling”, or it is implicitly inferred from the context.

Due to inherent imperfections in generative models, the false facts are not seamlessly embedded within the deepfake; for example, the manipulated face does not precisely correspond to Obama. This motivates the idea of fact checking for deepfake detection. We determine if the media are fake by verifying if the claimed facts are false e.g. the face does not belong to Obama. As fact checking models can be based entirely on real media, they do not depend on which deepfakes were previously observed. This gives fact checking the ability to generalize to arbitrary deepfake attacks.

\begin{figure}[t]
  \begin{center}
    \begin{tabular}{c}
    \includegraphics[scale=1.05]{figures/fact_verification.png} 
    \end{tabular}
  \end{center}
  \vspace{-0.75em}
    \caption{Illustration of FACTOR, our proposed deepfake detection method. FACTOR leverages the discrepancy between false facts and their imperfect synthesis within deepfakes. By quantifying the similarity using the truth score, computed via cosine similarity, FACTOR effectively distinguishes between real and fake media, enabling robust detection of zero-day deepfake attacks.}
    \label{fig:factor}
\end{figure}


\section{FACTOR: A Practical Recipe for Fact Checking}
\label{sec:framework}

We propose a practical recipe for implementing fact checking for deepfake detection. We formulate facts as statements that can be parsed as “$x$ describes the same content as $y$”. This simple formulation covers many interesting cases such as: i) “image $x$ has identity $f$” which can be parsed to “image $x$ describes the same identity as image $y$: for every image $y$ with true identity $f$”; ii) “the video $x$ depicts the same event as the audio $y$”; iii) “the content in image $x$ is being described by the text caption $y$”.    

We first obtain off-the-shelf encoders for $x$ and $y$ denoted by $\phi_X, \phi_Y$. The encoders are required to return similar values when $x$ and $y$ describe the same content and distinct values otherwise. We measure the similarity using the cosine similarity, name it the \textit{truth score} and denote it by $s$:
\begin{equation}
s(x,y) = \frac{\phi_X(x) \cdot \phi_Y(y)}{\|\phi_X(x)\|_2 \cdot \|\phi_Y(y)\|_2}
\end{equation}
Claimed facts with low truth scores are classified as false. The encoders can be obtained off-the-shelf (as is the case for all experiments in this paper) and their pretraining procedure does not require any fake images, rendering implementation remarkably straightforward. We illustrate FACTOR in \cref{fig:factor}.

FACTOR improves deepfake detection by effectively addressing zero-day attacks. In the following sections, we showcase the effectiveness of our method in key deepfake scenarios, providing comprehensive analysis and validation.

\section{Face Swapping Detection}
\label{sec:identity}
Face forgery detection involves identifying instances of manipulated facial features. This task has significant real-world implications, as most deepfakes involve human faces. Face swapping replaces the original face in an image or video with the claimed identity. The ultimate goal is to generate a fake face that is indistinguishable from a real one to the human eye. 

\subsection{Method}
Our approach relies on a key observation: in most face swapping attacks, the attacker states the claimed identity of the generated face as a (false) fact e.g. “Barack Obama smiling”. Due to the limitations of current generative methods, the claimed identity is not perfectly transferred to the fake image. We thus propose to distinguish between real and fake images by verifying that the facial identity claimed by the user matches that of the observed image. We assume the availability of a reference set (with as few as a single image) containing authentic facial images of the claimed identity. Such a set of images is very easy to obtain; for example by downloading from the web or by asking the attacked identity looking to clear their name.

Our method takes as input a face image $x$ and its claimed identity $f$. We also require a face recognition model, denoted by $\phi_{id}(.)$, to compute facial features (we use the open-source model \citet{attention_face_recognition}). We construct a reference set $R_f$ consisting of real images of the stated identity $f$. Subsequently, we measure the similarity between the test image $x$ and each image within our reference set using cosine similarity over $\phi_{id}(.)$ features. Note that in this scenario, both $x$ and $y \in R_f$ are facial images, thus $\phi_{id}=\phi_{X}=\phi_{Y}$. The truth score $s(x)$ of the image $x$ is the similarity to the nearest face in the reference set. Low truth scores indicate that the fact is false and the image is fake. Formally, the truth score is given by:
\begin{equation*}
    s(x) = \max_{y\in R_f} \{sim(\phi_{id}(x), \phi_{id}(y))\}
\end{equation*}
\begin{table}[t]
\caption{Performance comparison (average ROC-AUC \%) of baseline methods on the DFDC dataset, evaluating their ability to detect zero-day deepfake attacks. Our method outperforms supervised baselines, underscoring its robustness to previously unseen manipulation techniques. A and B are two distinct deepfake generation methods.}
\vspace{-0.75em}
\label{tab:faces_generalization}
\begin{center}
\resizebox{1.0\linewidth}{!}{\begin{tabular}{l|ccccccccc}
\toprule
 Scenario &  Train/Test    &  Xception & EffNetB4 & FFD & F3NET & SPSL & 
 RECCE & UCF &  Ours \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-10}
\multirow{2}{*}{In-Method} & A/A & 95.7 & 83.5 & 96.0 & 97.4 & 96.4 & 95.0 & 97.0 & \textbf{99.9}\\ 
& B/B & 93.2 & 64.4 & 85.7 & 89.4 & 81.7 & 88.6 & 94.9 & \textbf{100.}\\ 
\cmidrule(lr){1-10}
\multirow{2}{*}{Zero-Day} & A/B & 74.0 & 70.7 & 77.9 & 79.8 & 84.4 & 74.2 & 81.3 & \textbf{100.}\\ 
& B/A & 65.9 & 41.6 & 56.6 & 87.9 & 48.7 & 62.0 & 67.1 & \textbf{99.9}\\ 
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.25em}
\end{table}

\begin{table}[t]
\caption{Performance comparison of baseline methods and our fact checking based FACTOR. The supervised models were trained on FF++(C23) and evaluated on Celeb-DF, DFD, and DFDC datasets.}
\vspace{-0.75em}
\label{tab:cross_vs_ref}
\begin{center}
\begin{tabular}{lccccccc|c}
\toprule
\multirow{3}{*}{Dataset} & \multicolumn{7}{c|}{Cross-Dataset} & Ref. Set \\
\cmidrule(lr){2-8} \cmidrule(lr){9-9}
& Xception & EffNetB4 & FFD & F3NET & SPSL & 
 RECCE & UCF &  Ours \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9}
Celeb-DF & 73.7 & 73.9 & 74.4 & 73.5 & 76.5 & 73.2 & 75.3 & 97.0\\
DFD & 81.6 & 81.5 & 80.2 & 79.8 & 81.2 & 81.2 & 80.7 & 96.3 \\
DFDC & 73.7 & 72.8 & 74.3 & 73.5 & 74.1 & 74.2 & 75.9 & 99.7 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\vspace{-2em}
\subsection{Experiments}
\textbf{Datasets.} We conducted experiments on three face swapping datasets that provide identity-related information: Celeb-DF \citep{celeb_df}, DFD \citep{dfd}, and DFDC \citep{dfdc}. Other standard face swapping datasets do not include identity information. The Celeb-DF dataset was generated through face swapping involving $59$ pairs of distinct identities, comprising $590$ real videos and $5,639$ fake videos. DFD, on the other hand, is a deepfake dataset characterized by $363$ real videos and $3,068$ synthetically generated fake videos. The DFDC dataset stands out as the largest publicly available collection of face-swapped videos, featuring $1,133$ real videos and $4,080$ manipulated videos for testing. This dataset poses a substantial challenge for existing forgery detection methods, due to the diverse and previously unseen manipulation techniques it contains. To ensure uniformity in our experiments, we uniformly subsampled each video (train or test and for all datasets) into $32$ frames. Furthermore, we split each identity's authentic videos into a 50/50 train-test split. FACTOR uses the 50\% subset of training videos as its reference set (it does not require training), denoted by $R_f$. In contrast, the supervised baselines use this subset exclusively for training their models. The test set for claimed identity $f$ consisted of 50\% of the real videos of this identity and fake video clips of with $f$ being the claimed identity. We followed the literature in using frame-level evaluation, computing ROC-AUC scores across all frames. Our reported results are an average of performance computed for all identities within the dataset. Full implementation details are in \cref{app:face_forgery}.

\textbf{Results.}
In order to assess the robustness of our proposed method against zero-day deepfake attacks, we conducted experiments on the DFDC dataset, which contains real and fake images. The fakes were created by two distinct deepfake generation methods. To establish a comparative baseline, we selected a range of classic and contemporary state-of-the-art methods, including Xception \citep{learning3_ff++}, EfficientNetB4 \citep{efficientnet}, FFD \citep{face_forgery1_ffd}, F3Net \citep{high_freq_eccv_2020_f3net}, SPSL \citep{spsl_high_freq}, RECCE \citep{generalization1_recce}, and UCF \citep{ucf}. Each baseline model was trained to classify real vs. deepfakes generated by method A and subsequently evaluated on real images vs. method B or vice versa. This ensured that no baseline model had prior exposure to the test-time deepfake generation method. In contrast, our encoders were exclusively pretrained on real data and not on fake data from methods A or B. The results, presented in \cref{tab:faces_generalization}, show that while the supervised baselines performed poorly on zero-day attack scenarios, our method achieved near-perfect accuracy. Additionally, to showcase the generalization challenges of supervised methods, we also tested them on real vs. fake data from the \textit{same} method as for training (e.g. A). We can see in \cref{tab:faces_generalization} that the baselines perform much better in this case. Note that our method outperforms even in this case, as DFDC does not suffer from serious artifacts making it challenging for methods that use visual artifacts rather than identity.

\begin{figure}
     \centering
      \begin{subfigure}[b]{0.29\textwidth}
     \centering
     \includegraphics[width=\textwidth]{figures/reference_set_ablation.png}
     \vspace{-1.5em}
     \caption{}
     \label{fig:reference_set_ablation}
     \end{subfigure}
     \hfill
\begin{subfigure}[b]{0.325\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/ablation.png}
         \vspace{-1.5em}
         \caption{}
         \label{fig:av_ablation}
     \end{subfigure}
     \hfill
\begin{subfigure}[b]{0.36\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/TTI_similarity_vivid.png}
         \vspace{-1.5em}
         \caption{}
         \label{fig:tti}
     \end{subfigure}
\caption{(\textit{a}) Ablation study over our reference set size (ROC-AUC \%). Even a single image reference set results in performance close to the full set performance. (\textit{b}) Ablation study on the percentile $\lambda$ for truth score selection. Performance is robust across a wide range of values. (\textit{c}) Comparison of truth scores for different text-image encoders. We report the percentage of image captions whose fake images had a higher truth score than the original image and vice versa for CLIP, BLIP2, and BLIP2-CLIP. We can see that CLIP and BLIP2 truth scores have opposite effects while the proposed rule BLIP2-CLIP is the most effective.}
        \vspace{-0.5em}
\end{figure}


To simulate a common zero-day setting, where the dataset does not have previously observed deepfake attack data, we compared our method against state-of-the-art approaches that are trained on a large external dataset (here, FF++(C23) \citep{learning3_ff++}), and evaluated on the zero-day attack data in another dataset (here evaluated on Celeb-DF, DFD, and DFDC). As our method does not require training, we only used a reference set of real images from the claimed identity. We did not need to train on FF++. The results can be seen in \cref{tab:cross_vs_ref}, our method is far more effective on such zero-day attacks. It is clear that supervised methods struggle to generalize well across both datasets and attack types. Fact checking removes this strong requirement for in-method, in-dataset training data, resulting in improved performance.    

\textbf{Ablation.} We conducted sensitivity analysis on the effect of reference set size on performance. The results in \cref{fig:reference_set_ablation} demonstrate the method's robustness to reference set variations. A minimal reference set containing only a single image results in only a slight decrease in accuracy. Note that videos were uniformly subsampled to $32$ frames in our experiments.

\textbf{Limitations.} i) If an attacker simply copies the claimed face onto the observed image, it will correspond to the claimed face identity, although this would result in an unrealistic appearance. To mitigate this, we recommend ensembling our method with a simple image realism-based approach which will easily catch such crude attacks. ii) Our method does not deal with cases where the original and claimed identities are identical, but other attributes are manipulated, such as changes in facial expressions, age or other non-identity-related features. These tasks are left for future work.

\section{Audio-Visual Deepfake Detection}
\label{sec:audio-visual}

\subsection{Method}
Audio-visual (AV) deepfakes involve synthesizing video to match given audio, audio to match given video, or synthesizing both. AV deepfakes are typically also accompanied by a false fact about the claimed identity of the speaker, which can be dealt with using the tools of \cref{sec:identity}. Here, we focus on another false fact, the fact that the two media indeed correspond to the same event. Audio-visual synthesis is very hard for current generative models and we expect them to achieve only partial results. We adapt our method, FACTOR, to AV data by using powerful off-the-shelf audio-visual encoders (we use AV-Hubert \citep{avhubert}) to extract features for each modality. The audio and video encoders are denoted by $\phi_A$ and $\phi_V$, respectively. We then use the cosine similarity between them as the truth score. There is an added complication in this case, as AV deepfakes are scored at a video level, while the truth score is calculated for every temporal frame. We opt for a simple but effective solution, using the truth score with the $\lambda\%$ lowest value in the video (we choose $\lambda=3\%$, but a wide range of values is successful, see \cref{fig:av_ablation}). Formally, for a clip of length $T$, we denote the visual frame at time $t$ by $v_t$ and the audio frame by $a_t$. The truth score for frame $t$ (denoted $s_t$) is given by:
\begin{equation}
s_t = sim(\phi_V(v_t), \phi_A(a_t))
\end{equation}
Where $sim$ denotes cosine similarity. We choose the frame value with the $\lambda\%$ percentile as the overall clip truth score $s$. This is given by:
\begin{equation}
s = perc(\{s_1,s_2..s_T\},\lambda)
\end{equation}
Where $perc(\cdot, \lambda)$ calculates the $\lambda\%$ percentile of the set. AV data with some misaligned frames will obtain a low truth score indicating a high likelihood of being fake. Real data will not have mismatches and will achieve high truth scores. Note that misalignment between audio-visual data has been detected by several previous deepfake detection methods including \citet{owens}. The novelty here lies in demonstrating that our universal fact checking approach outperforms the previous methods, using a simpler, streamlined method.

\begin{table}[t]
\caption{AP and AUC (\%) FakeAVCeleb results, following the AVAD \citep{owens} evaluation protocol. Supervised methods are evaluated on unseen fake types. Best results are in bold.}
\label{tab:av}
\centering
\resizebox{1.0\linewidth}{!}{\begin{tabular}{clcccccccccccccc}
\toprule
  & \multirow{4}{*}{Method}  &  \multirow{4}{*}{Mode}     & \multirow{4}{*}{\begin{tabular}[c]{c}Pretrained\\ Dataset\end{tabular}} &  \multicolumn{12}{c}{Category}\\
  \cmidrule(lr){5-16}
  & & & & \multicolumn{2}{c}{RVFA} & \multicolumn{2}{c}{FVRA-WL} & \multicolumn{2}{c}{FVFA-WL} &  \multicolumn{2}{c}{FVFA-FS} &  \multicolumn{2}{c}{FVFA-GAN} & \multicolumn{2}{c}{AVG-FV}  \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14} \cmidrule(lr){15-16}
  & &  &  & AP & AUC & AP & AUC  & AP & AUC & AP & AUC & AP & AUC & AP & AUC  \\
\midrule 
\parbox[t]{4mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Supervised}}} 
 & Xception &     $\mathcal{V}$                    &            ImageNet                                                           &   --         &     --     &      88.2&  88.3     &  92.3     &     93.5     & 67.6       &    68.5    &91.0     & 91.0  &   84.8  &  85.3       \\
& LipForensics                         &     $\mathcal{V}$                    &            LRW                                                              &   --         &     --  &       \textbf{97.8}       &       \textbf{97.7}         &       99.9      &      99.9         &        61.5      &     68.1      &        98.6   &     98.7  & 89.4 & 91.1  \\
& AD DFD              &     $\mathcal{A} \mathcal{V}$                                           &     Kinetics                                                                         &       \textbf{74.9}       &       \textbf{73.3} &      97.0         &     97.4          &      99.6       &     99.7         &         58.4   &   55.4       &        \textbf{100.}    &      \textbf{100.}    & 88.8 & 88.1     \\
& FTCN                                  &        $\mathcal{V}$                      &          --                                                                     &    --        &   --        &   96.2    &      97.4       &    \textbf{100.}          &      \textbf{100.}        &      77.4       &       78.3    &    95.6       &    96.5    & 92.3 &   93.1    \\
& RealForensics   &  $\mathcal{V}$  & LRW   &   --   &  --   &    88.8      &     93.0&      99.3       &   99.1    &           \textbf{99.8}     &       \textbf{99.8}      &   93.4  &     96.7  & \textbf{95.3} & \textbf{97.1}       \\
\cmidrule(lr){1-16}
\parbox[t]{4mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Unsupervised}}} & AVBYOL & $\mathcal{A} \mathcal{V}$ & LRW & 50.0 & 50.0 & 73.4 & 61.3 & 88.7 & 80.8 & 60.2 & 33.8 & 73.2 & 61.0 & 73.9 & 59.2 \\
& VQ-GAN & $\mathcal{V}$ & LRS2 & - & - & 50.3 & 49.3 & 57.5 & 53.0 & 49.6 & 48.0 & 62.4 & 56.9 & 55.0 & 51.8 \\
& AVAD & $\mathcal{A} \mathcal{V}$ & LRS2 & 62.4 & 71.6  & 93.6 & 93.7 & 95.3 & 95.8 & 94.1 & 94.3 & 93.8 & 94.1 & 94.2 & 94.5  \\
& AVAD & $\mathcal{A} \mathcal{V}$ & LRS3 & 70.7 & 80.5  & 91.1 & 93.0 & 91.0 & 92.3 & 91.6 & 92.7 & 91.4 & 93.1 & 91.3 & 92.8 \\
& Ours & $\mathcal{A} \mathcal{V}$ & LRS3 & \textbf{98.6} & \textbf{98.7} & \textbf{94.4} & \textbf{95.7} & \textbf{97.4} & \textbf{97.7} & \textbf{97.8} & \textbf{98.1} & \textbf{97.6} & \textbf{97.9} & \textbf{96.8} & \textbf{97.4}\\
\bottomrule
\end{tabular}
}
\vspace{-0.25em}
\end{table}

\subsection{Experiments}
\textbf{Datasets.} 
We evaluated our method on the FakeAVCeleb video forensics dataset \citep{fakeavceleb}. This dataset contains a diverse range of manipulations that alter both human speakers' speech and facial features, reflecting real-world deepfake scenarios. Specifically, FakeAVCeleb is derived from the VoxCeleb2 dataset and consists of 500 authentic videos, and 19,500 manipulated videos. These manipulations are generated through various techniques, including Faceswap \citep{faceswap}, FSGAN \citep{faceswap4}, Wav2Lip \citep{wav2lip}, and the incorporation of synthetic sounds generated by SV2TTS \citep{sv2tts}. The dataset features examples that exhibit different combinations of these manipulations, capturing the diverse nature of deepfake content. Full implementation details and further evaluation can be found at \cref{app:av}.

\textbf{Settings and baselines.} We conducted experiments on the FakeAVCeleb dataset \citep{fakeavceleb} following the protocol established by AVAD \citep{owens}. We report numbers by SOTA supervised methods: Xception \citep{learning3_ff++}, LipForensics \citep{lip_forensics}, AD DFD \citep{AD_DFD}, FTCN \citep{ftcn}, and RealForensics \citep{real_forensics}. We also compared to other self-supervised methods: AVBYOL \citep{byol,real_forensics}, VQGAN \citep{vqgan} and AVAD \citep{owens}. We use the same categorization of the FakeAVCeleb dataset as in \citet{owens}: (i) RVFA: real video with fake audio by SV2TTS; (ii) FVRA-WL: fake video by Wav2Lip with real audio; (iii) FVFA-WL: fake video by Wav2Lip, and fake audio by SV2TTS; (iv) FVFA-FS: fake video by Faceswap and Wav2Lip, and fake audio by SV2TTS; (v) FVFA-GAN: fake video by FSGAN and Wav2Lip, and fake audio by SV2TTS. For supervised methods, we omitted the evaluated category during training and used the remaining ones.

\textbf{Results.} The results presented in \cref{tab:av} underscore the superior performance of our method across all categories, surpassing self-supervised approaches (AVBYOL, VQGAN, and AVAD) by a significant margin. Our method consistently demonstrates comparable or superior performance to supervised methods in all categories, despite not relying on labeled supervision or fake data. Notably, our method outperforms all supervised baselines in terms of average AP and ROC-AUC. Although supervised baselines excel in certain categories, their performance deteriorates in others demonstrating poor generalization skills. This highlights the robustness and effectiveness of our method for identifying fake videos manipulated by diverse and previously unseen zero-day attacks. A further evaluation of FACTOR on the KoDF \citep{kodf} dataset is provided in \cref{app:av}.

\textbf{Ablation.} We ablate the effect of the percentile $\lambda$ in \cref{fig:av_ablation}. The results are robust to this hyperparamer, with a mere 2\% decrease in fake video average performance when comparing $\lambda=0-90\%$. 


\section{Text-to-Image Deepfake Detection}
\label{sec:text-to-image}

Text-to-image synthesis is another important deepfake attack technique. The meteoric rise of text-to-image (TTI) generators such as DALLE-2 \citep{dalle-2}, Imagen \citep{imagen} and Stable Diffusion \citep{stable_diffusion} has made detecting such images a high-importance task. TTI models create fake images that are designed to seamlessly align with their accompanying textual prompt, in some cases creating synthetic images that humans find hard to distinguish from real ones. Along with many benevolent use cases, they can be deployed for malicious purposes.

Here, we study a simplified setting, where the attacker first synthesizes a fake image using a text prompt and a TTI model. The attacker then presents the fake image with the input prompt as its caption. In this case, the false fact is that the text prompt and the image describe the same content. As TTI models are known not to be perfectly aligned with the input prompt \citep{imagereward,attend_and_excite}, fact checking can be used to detect fake images generated by them. We note that this setting is simplified, as the attacker can choose to use another caption to describe the fake image which differs from the input prompt. This caption can be generated post-hoc (after the fake image was synthesized), making this caption potentially very accurate and a true fact. However, we choose to analyze the case of the caption being identical to the initial prompt as it showcases the interesting behavior of fact checking methods.   

\textbf{Dataset.} Our evaluation is performed on a random sample of $1000$ images from the COCO \citep{coco} dataset. Each image has $5$ corresponding captions written by different people. We use the popular TTI model Stable Diffusion (SD) to generate an image for each caption. This yields $5000$ synthetic images and $1000$ real images.   

\textbf{Truth score paradox.} We begin by using the CLIP \citep{clip} encoders to encode the caption and image respectively; the truth score is the cosine similarity between them. We compute truth scores for all real and fake images in COCO. We begin with a simple analysis - we compare the CLIP truth scores for each real image with its fake counterparts (fake images with the same caption). We denote classification accuracy, as the number of fake images whose truth score was lower than that of their real counterparts. The results are shown in \cref{fig:tti}. Surprisingly, we see that this method's deepfake detection accuracy is lower than chance! To further explore this phenomenon, we perform the same experiment, but now using BLIP2 \citep{blip2} as the text and image encoder. This experiment results are as expected; real images have higher truth scores than their fake counterparts. Full implementation details are in \cref{app:tti}.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.325\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/CLIP_histograms.png}
         \vspace{-1.5em}
         \caption{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.325\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/BLIP2_histograms.png}
         \vspace{-1.5em}
         \caption{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.325\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/BLIP2_CLIP_histograms.png}
         \vspace{-1.5em}
         \caption{}
     \end{subfigure}
\caption{Comparison of truth scores distributions of real images and fake images with respect to the claimed caption using (\textit{a}) CLIP, (\textit{b}) BLIP2, and (\textit{c}) $BLIP2-CLIP$ scoring rules. (\textit{a}) CLIP truth scores are surprisingly higher for fake images than for real ones. (\textit{b}) BLIP2 truth scores achieve weak separation between real and fake data. (\textit{c}) $BLIP2-CLIP$ truth scores achieve a stronger separation between real and fake data.}
        \label{fig:Real_vs_Fakes_Histograms}
        \vspace{-0.5em}
\end{figure}


\textbf{Resolution of the paradox.} Why do BLIP2 truth scores behave as expected, but CLIP scores do exactly the opposite? We recall that SD was trained with CLIP text features, but not with BLIP2 features. We therefore hypothesize that SD overfits to CLIP scores, making the fake images better aligned with their input caption than real images. On the other hand, the generated image does not fully correspond to the input prompt, due to imperfections in the TTI model. Therefore, an objective multi-modal encoder, i.e. one not used for training the TTI, is able to perform fact checking and identify that fake images do not fit with the claimed prompts. This explains why CLIP truth scores support the claimed fact, but BLIP2 rejects it.

\textbf{Method.} We compute the CLIP and BLIP2 truth score for each image and prompt pair $(x, y)$. The final score is the BLIP2 minus the CLIP scores:
\begin{equation}
s(x, y) = sim(\phi^{BLIP2}_{X}(x), \phi^{BLIP2}_{Y}(y)) - sim(\phi^{CLIP}_{X}(x), \phi^{CLIP}_{Y}(y))
\end{equation}
\textbf{Results.} We present the truth score histograms for CLIP, BLIP2, and BLIP2-CLIP in \cref{fig:Real_vs_Fakes_Histograms}. We find that the CLIP truth score is negative correlated with deepfakes, and BLIP2 is positively correlated. Using BLIP2 - CLIP achieves the best of both worlds. Numerically, CLIP truth score achieves around 30\% ROC-AUC, while BLIP2 obtains around 60\%. Using the difference between the truth scores yields a much better result of 79.2\%. While we do not claim that this setting is realistic (as attackers may not disclose the exact input prompt) or that these results are better than the state-of-the-art on COCO, this scenario demonstrates how overfitting may play an important part in fact checking.

\section{Discussion and Limitations}

\textbf{Facts must be falsifiable.} As our method detects deepfakes by detecting false facts, it is necessary that the false fact will be falsifiable. For example, a fact that carries no information will not help deepfake detection e.g. the tautological prompt “An image”. On the other hand, facial identity, which can distinguish one person out of a billion is certainly helpful. We conducted an investigation of the effect of fact information complexity vs. detection accuracy in \cref{app:cap_complexity}. The results showed that higher fact informativeness resulted in higher detection accuracy.  

\textbf{Unconditional deepfakes do not include facts.} Our method is not designed for unconditional deepfakes, e.g. a generated image without added information such as a caption, as there are no attacker provided facts to be falsified. We stress that face swapping and audio-visual deepfakes are of sufficient practical and important to make our approach valuable. 

\textbf{Supervised approaches work well on previously seen attacks.} The primary benefit of our approach is generalizing to unseen, zero-day attacks. Existing supervised techniques are effective on attacks similar to those seen before, for which sufficient training data can be obtained. We note that in some cases, our method outperforms supervised techniques even for previously seen attacks.

\textbf{No pretrained encoders for non-standard facts.} FACTOR used off-the-shelf feature encoders to do fact checking in important attack scenarios. It is possible that other facts may require specialized encoders that are not available off-the-shelf. In this case, the user would need to train the encoders rather than using off-the-shelf ones. Still, this will not require using any fake data. 

\textbf{Hazards of future progress.} To overcome FACTOR, generative models must synthesize false facts significantly better than is currently possible. Specifically, they would have to replicate not only the visual appearance but also the finer details, nuances, and contextual cues of the claimed facts. When generative methods indeed progress to this level, our method would need to be re-evaluated.

\section{Conclusion}
This paper proposes the concept of fact checking to address the challenge of detecting unseen, zero-day attacks. We propose FACTOR for implementing this, and showcase it in three important settings. FACTOR outperforms the state-of-the-art without seeing any fake data, using only pretrained feature encoders and being simple to implement.

\section*{Acknowledgements}
Tal Reiss and Yedid Hoshen are partially supported by funding from the Israeli Science Foundation. Tal Reiss is also supported by the Israeli Council for Higher Education. We also thank Shira Bar-On for creating our figures.


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\newpage
\clearpage
\appendix

\centerline{\textbf{\LARGE Appendix}}
\vspace{1.5em}
\centerline{\textbf{\Large Detecting Deepfakes Without Seeing Any}}
\vspace{1em}


\section{Experimental Details \& Analysis}

\subsection{Face Swapping Detection}
\label{app:face_forgery}

\textbf{Implementation details.} Our implementation relies on a pretrained Attention-92(MX) model from Face-X-Zoo \citep{facex}, denoted by $\phi_{id}(.)$, originally trained on the MS-Celeb dataset \citep{ms_celeb}. To ensure uniformity and consistency in our data, we incorporated robust data processing techniques. These encompassed critical steps such as face detection, precise cropping, and alignment, all facilitated by the DLIB library \citep{dlib}. The preprocessing ensured that all face images were uniformly cropped and normalized to compact dimensions of $112 \times 112$. We computed the features of all images (observed and referenced) using the above feature encoder and calculated their similarity using cosine similarity. For evaluation, we used Receiver Operating Characteristic Area Under the Curve (ROC-AUC).

\subsection{Audio-Visual Deepfake Detection}
\label{app:av}

\textbf{Implementation details.}
In our implementation, we use the AV-HuBERT Large model as our feature encoder. This model was pretrained on real, unlabeled speech videos from the LRS3 dataset. No fake videos at all or any real videos from the evaluation dataset were used in pretraining. We follow the official AV-HuBERT implementation\footnote{\url{https://github.com/facebookresearch/av_hubert}} for video preprocessing. Specifically, we use an off-the-shelf landmark detector to identify Regions of Interest (ROIs) within each video clip. Both video and audio components are transformed into feature matrices represented in $\mathbb{R}^{T \times d}$, where $T$ represents the number of frames, and $d$ is the AV-HuBERT feature space dimension ($d=1024$). Accordingly, we choose $\lambda = 3\%$ for our choice of $\lambda$. However, an ablation study is presented in \cref{fig:av_ablation} which indicates that performance is not sensitive to the choice of $\lambda$. In accordance with standard practice, we used two evaluation metrics: (i) average precision (AP) and (ii) Receiver Operating Characteristic Area Under the Curve (ROC-AUC). 

\textbf{KoDF Evaluation.} To further assess the cross-domain applicability of our fact checking approach, we conducted an evaluation on the Korean Deepfake Detection (KoDF) dataset \citep{kodf}, following the established protocol outlined by AVAD \citep{owens}. For comparative analysis, we benchmarked our method against several state-of-the-art supervised and self-supervised baselines, including Xception \citep{learning3_ff++}, LipForensics \citep{lip_forensics}, AD DFD \citep{AD_DFD}, FTCN \citep{ftcn}, VBYOL \citep{byol,real_forensics}, VQGAN \citep{vqgan}, and AVAD \citep{owens}. The supervised baselines were trained on the FakeAVCeleb dataset \citep{fakeavceleb}, which uses similar synthesis techniques to KoDF, such as FaceSwap \citep{faceswap}, FS-GAN \citep{faceswap4}, and Wav2Lip \citep{wav2lip}.

The results, summarized in \cref{tab:kodf}, provide evidence for our method's cross-generalization capability. FACTOR achieves performance levels comparable to many state-of-the-art supervised baselines, and surpasses all unsupervised methods by a large margin. This highlights the adaptability and effectiveness of our method to scenarios with distinct linguistic and cultural attributes.


\begin{table}[t!]
\caption{AP and AUC (\%) KoDF results, following the AVAD \citep{owens} evaluation protocol. Best results are in bold.}
\label{tab:kodf}
\centering
\begin{tabular}{llccc}
\toprule
& \multirow{2}{*}{Method} & \multirow{2}{*}{Modality} &\multicolumn{2}{c}{KoDF} \\
\cmidrule(lr){4-5}
\multicolumn{2}{c}{}           &             & AP          & AUC         \\
\midrule
\multirow{5}{*}{\shortstack[c]{Supervised \\ (transfer)}}
 & Xception   & $\mathcal{V}$  &   76.9  &  77.7\\
                            & LipForensics   &  $\mathcal{V}$ & 89.5       &    86.6       \\
                            & AD DFD   & $\mathcal{AV}$      &   79.6         &   82.1       \\
                            & FTCN & $\mathcal{V}$           &     66.8       &     68.1       \\
                            & RealForensics &        $\mathcal{V}$ &  \textbf{95.7}    &          \textbf{93.6}   \\
\midrule
\multirow{4}{*}{Unsupervised}                  
& AVBYOL &$\mathcal{AV}$   & 74.9     & 78.9 \\

& VQ-GAN & $\mathcal{V}$   &   46.8      &   45.5     \\

& AVAD       & $\mathcal{AV}$    &        87.6    &      86.9       \\
& Ours       & $\mathcal{AV}$    &        \textbf{92.0}    &      \textbf{93.1}        \\

\bottomrule
\end{tabular}
\end{table}



\subsection{Text-to-Image Deepfake Detection}
\label{app:tti}
\textbf{Implementation details.}
In our implementation, we employed two multi-modal feature encoders, CLIP \citep{clip} and BLIP2 \citep{blip2}, to encode textual prompts and images. Specifically, for CLIP's architecture, we leveraged the ViT-B/16 pretrained on the LAION-2B dataset, following OPENCLIP specifications \citep{Ilharco_OpenCLIP_2021}. Furthermore, the checkpoint version of Stable Diffusion we used was v1-5 \citep{sd_version}.
In order to evaluate the performance of our approach, we used two evaluation metrics: (i) average precision (AP) and (ii) Receiver Operating Characteristic Area Under the Curve (ROC-AUC). 


\subsection{Caption Complexity and Truth Scores}
\label{app:cap_complexity}
We hypothesize that more complex facts are more falsifiable and therefore improve deepfake detection accuracy. To test this hypothesis, we tested the correlation between textual prompt complexity and their truth scores for real and fake images. We hypothesized that as the complexity of the prompt increased, it would exhibit greater similarity to the real image compared to the fake image. Caption complexity, in this context, refers to the level of detail of describing the image content. To systematically explore this hypothesis, we evaluated a dataset consisting of $1000$ randomly selected images from the COCO \citep{coco} dataset. For each of these real images, we selected two captions: one with the minimum length and another with the maximum length. The minimum length caption represented a simple textual prompt, while the maximum length caption was considered more complex due to its larger word count. 

We paired each real image with its corresponding minimum and maximum length captions. For each caption, we generated corresponding fake images, using Stable Diffusion. For each pairing, we calculated truth scores, between the real image and its respective caption, as well as between the fake image and the same caption. This analysis was conducted within the feature spaces of both CLIP \citep{clip} and BLIP2 \citep{blip2}, which notably, exhibited disagreements in their prompt similarity trends. Our findings, presented in \cref{fig:cap_complex}, underscore a consistent pattern. With increasing complexity of the prompts, as measured through the maximum length captions, more prompts achieved a higher truth score on the real images than on the fakes. This aligns with our hypothesis, demonstrating that more complex prompts contribute to a shift in similarity to the real image, thereby enhancing deepfake detection accuracy. Additionally, we can observe the same phenomenon we witnessed in \cref{sec:text-to-image}, wherein CLIP truth scores support the claimed fact (caption), while BLIP2 rejects it. So the effectiveness of BLIP2 truth scores increases with prompt complexity, while using minus CLIP truth scores becomes less effective as prompts become more complex, as the overfitting contrasts with the generative model's failure to synthesize the image corresponding to the complex caption.


\begin{figure}[t]
  \begin{center}
    \begin{tabular}{c}
\includegraphics[scale=0.35]{figures/min_max_truth_score.png} 
    \end{tabular}
  \end{center}
\caption{Impact of prompt complexity on truth score. We paired real images with both their minimum and maximum length captions, generating fake versions of those captions. Truth scores were calculated for these pairs. They revealed that as prompt complexity increased, measured through maximum length captions, more prompts achieved higher truth scores with real images, enhancing deepfake detection accuracy. We report the percentage of image captions whose real images had a higher truth score than the fake image for CLIP, BLIP2, and BLIP2-CLIP.}
    \label{fig:cap_complex}
\end{figure}


\end{document}
