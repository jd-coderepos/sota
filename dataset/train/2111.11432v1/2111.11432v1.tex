

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}

\usepackage[hyphens]{url}
\usepackage{hyperref}
\hypersetup{breaklinks=true}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\eg}{{\it{e.g.}~}}
\newcommand{\ie}{{\it{i.e.}~}}



\usepackage[accepted]{icml2021}

\icmltitlerunning{Florence: A New Foundation Model for Computer Vision}


\usepackage[british,UKenglish,USenglish,english,american]{babel}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\renewcommand{\comment}[1]{}
\newcommand{\celiu}[1]{\textcolor{magenta}{[Ce: #1]}}
\newcommand{\highlight}[1]{\textcolor{red}{[#1]}}
\newcommand{\Florence}{\emph{Florence~}}


\begin{document}

\twocolumn[
\icmltitle{Florence: A New Foundation Model for Computer Vision}





\icmlsetsymbol{team}{*}

\begin{icmlauthorlist}
\icmlauthor{Lu Yuan}{cogs}
\icmlauthor{Dongdong Chen}{team,cogs}
\icmlauthor{Yi-Ling Chen}{team,cogs}
\icmlauthor{Noel Codella}{team,cogs}
\icmlauthor{Xiyang Dai}{team,cogs}
\icmlauthor{Jianfeng Gao}{team,msr}
\icmlauthor{Houdong Hu}{team,cogs}
\icmlauthor{Xuedong Huang}{team,cogs}
\icmlauthor{Boxin Li}{team,cogs}
\icmlauthor{Chunyuan Li}{team,msr}
\icmlauthor{Ce Liu}{team,cogs}
\icmlauthor{Mengchen Liu}{team,cogs}
\icmlauthor{Zicheng Liu}{team,cogs}
\icmlauthor{Yumao Lu}{team,cogs}
\icmlauthor{Yu Shi}{team,cogs}
\icmlauthor{Lijuan Wang}{team,cogs}
\icmlauthor{Jianfeng Wang}{team,cogs}
\icmlauthor{Bin Xiao}{team,cogs}
\icmlauthor{Zhen Xiao}{team,cogs}
\icmlauthor{Jianwei Yang}{team,msr}
\icmlauthor{Michael Zeng}{team,cogs}
\icmlauthor{Luowei Zhou}{team,cogs}
\icmlauthor{Pengchuan Zhang}{team,msr}
\end{icmlauthorlist}

\icmlaffiliation{cogs}{Microsoft Cloud and AI}
\icmlaffiliation{msr}{Microsoft Research Redmond}

\icmlcorrespondingauthor{Lu Yuan}{luyuan@microsoft.com}



\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}

Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP~\cite{radford2021learning}, ALIGN~\cite{jia2021scaling}, and Wu Dao 2.0~\cite{Wudao2} focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, \emph{Florence}, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our \Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, \Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. \Florence achieves new state-of-the-art results in majority of  representative benchmarks, \eg ImageNet-1K zero-shot classification with top-1 accuracy of
{} and the top-5 accuracy of {},  mAP on COCO fine tuning,  on VQA, and  on Kinetics-600.
\end{abstract}

\section{Introduction}
\label{sec:intro}


Human-like AI is not achieved by designing specific models to solve specific problems, but by holistic, joint models that can simultaneously solve diverse, real-world problems without too much human involvement. It is thus desirable to have new AI architectures that learn joint, fundamental representations to support a broad range of downstream AI tasks with limited additional domain knowledge, similar to what humans would do. One such proposal is XYZ-code~\cite{XYZ-code}, where monolingual text (X), audio and visual sensory signals (Y), and multilingual (Z) are organically integrated to create AI models that can speak, hear, see, and understand. Another approach is Pathways~\cite{pathways}, a single model that can generalize across millions of tasks.

\begin{figure}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{figure/ProblemSpace1.pdf}
  \caption{Common computer vision tasks are mapped to a \emph{Space-Time-Modality} space. A computer vision foundation model should serve as general purpose vision system for all of these tasks.}
  \label{fig:problem}
\end{figure}

\begin{figure*}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{figure/overview1.pdf}
  \caption{Overview of building Florence. Our workflow consists of data curation, unified learning,
  Transformer architectures and adaption. It shows the foundation model can be adapted to various
  downstream tasks and finally integrated into modern computer vision system to power real-world
  vision and multimedia applications. Compared with existing image-text pretraining models ~\cite{radford2021learning,jia2021scaling,Wudao2}, mainly limited on cross-modal shared representation for classification and retrieval (illustrated by \emph{light-green} adaptation module), Florence expands the representation to support object level, multiple modality, and videos respectively.}
  \label{fig:overview}
\end{figure*}

A concrete step towards this direction is the development of \emph{foundation} models. The term of \emph{foundation} model was first introduced in~\cite{bommasani2021opportunities} to refer to any model that is trained from broad data at scale that is capable of being adapted (\eg fine-tuned) to a wide range of downstream tasks. Foundation models become promising due to their impressive performance and generalization capabilities. They are quickly integrated and deployed into real-world AI systems by many researchers and developers.

Although foundation models have already demonstrated huge impact in NLP, \eg, BERT~\cite{devlin2019bert}, GPT-3~\cite{brown2020language}, in computer vision it is still standard practice to pre-train models on labeled data sets such as ImageNet~\cite{deng2009imagenet}. More recently, large-scale pre-training methods such as CLIP~\cite{radford2021learning}, ALIGN~\cite{jia2021scaling}, and Wu Dao 2.0~\cite{Wudao2}, which learn directly from Web-scale image-text pairs, show very encouraging progress for efficient transfer learning, and zero-shot capability. However, such models are restricted to image to text mapping only tasks such as classification, retrieval, and tagging.

We raise the question: ``\emph{What is the foundation model for computer vision}?". But first, in order to better define what ``foundation'' means in computer vision, we capture the spectrum of tasks in a problem space  (Figure~\ref{fig:problem}) with three orthogonal axes: {1) \emph{Space}:} from coarse (\eg ~scene-level classification) to fine-grained (\eg ~object detection), {2) \emph{Time}:} from static (\eg ~images) to dynamic (\eg ~videos), and {3) \emph{Modality}:} from RGB only to multiple senses (\eg ~captioning and depth). Due to the diversity nature of visual understanding, we redefine {\bf{\emph{foundation models for computer vision}}} to be \emph{a pre-trained model and its adapters} for solving all vision tasks in this Space-Time-Modality space, with transferability such as zero-/few-shot learning and fully fine tuning, etc. The adaptation for transferability is restricted to minimum customization for the pre-trained foundation models, such as continuing training, few epochs or few layers for fine tuning without significantly increasing or changing model parameters.

In this paper, we present an emerging paradigm for building a \emph{vision foundation model}, called \emph{Florence}. We use the name of \emph{Florence} as the origin of the trail for exploring \emph{vision foundation} models, as well as the birthplace of Renaissance. \emph{Florence} is trained on noisy Web-scale data end-to-end with a unifying objective, allowing the model to achieve best-in-class performance across a wide range of benchmarks.

The ecosystem of constructing \emph{Florence} consists of \emph{data curation}, \emph{model pretraining}, \emph{task adaptations} and \emph{training infrascturue}, as shown in Figure~\ref{fig:overview}.
\begin{itemize}
    \item \textbf{Data curation}. Diverse, large-scale data is the lifeblood of foundation models. Enabled by large amounts of publicly available images on the Internet with natural language weak supervision, we curate a new dataset of  million image-text pairs for training. As Web-crawled data is usually noisy free-form texts (\eg, word, phrase or sentence), to attain more effective learning, we consider {\emph{UniCL}}, a unified image-text contrastive learning objective recently proposed in~\cite{Jianwei_UNICL2022}, which has demonstrated improvements over contrastive and supervised learning approaches.

    \item \textbf{Model pretraining} (representation learning). To learn a good representation from image-text pairs, we used a two-tower \emph{architecture} including an image encoder and a language encoder, as commonly used in CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling}. For the image encoder, we chose hierarchical Vision Transformers (\eg, Swin~\cite{liu2021Swin}, CvT~\cite{Wu_2021_ICCV}, Vision Longformer~\cite{zhang2021multi}, Focal Transformer~\cite{yang2021focal}, and CSwin~\cite{dong2021cswin}). While inheriting performance benefits of the transformer self-attention operations~\cite{dosovitskiy2021image}, these hierarchical architectures model the scale invariance nature of images and have linear computational complexity with respect to image size, a property that is essential to dense prediction tasks such as object detection and segmentation.

    \item \textbf{Task adaptations}. As we have defined computer vision foundation models to adapt to various downstream tasks, it is vital for \Florence to be \emph{extensible} and \emph{transferable} for this purpose. We extended the learned feature representation along space (from scene to objects) using the dynamic head adapter ~\cite{Dai_2021_CVPR}, time (from static image to videos) via proposed video CoSwin adapter, and modality (from images to language) via METER adapter~\cite{dou2021empirical}. \Florence is designed to effectively adapted in the open world via few-shot and zero-shot transfer learning, with the ability of efficient deployment by extra training with few epochs (\eg ~in retrieval). Our model can be customized for various domains that application-developers can use.

    \item \textbf{Training infrastructure}. For both energy and cost concerns, it is critical to build foundation models with as low cost as possible. We developed scalable training infrastructure to improve training efficiency. It consists of several key techniques such as ZeRO~\cite{DBLP:journals/corr/abs-1910-02054}, activation checkpointing, mixed-precision training, gradient cache~\cite{gao2021scaling} to greatly reduce the memory consumption and thus improves the training throughput.

\end{itemize}



\Florence significantly outperforms previous large-scale pre-training methods and achieves new state-of-the-art results on a wide range of vision and vision-language benchmarks. It showed strength in zero-shot transfer in  classification downstream tasks (win , SOTA in ImageNet-1K zero-shot with top-1 accuracy of {} and the top-5 accuracy of {}), linear probe in  classification downstream tasks (win ), image retrieval zero-shot ({} R on Flickr30K image-to-text / text-to-image, {} R on MSCOCO image-to-text / text-to-image) and fine-tuning ({} R on Flickr30K image-to-text / text-to-image, {} R on MSCOCO image-to-text/ text-to-image), object detection ({} mAP on COCO, {} mAP on Object365, {} AP50 on Visual Genome), VQA ({}), text-to-video retrieval zero-shot ({} R on MSR-VTT), and video action recognition (top-1 accuracy {} on Kinetics-400 / Kinetics-600).


\section{Approach}

\subsection{Dataset Curation}

We leverage large quantities of image-text data available publicly on the internet. Specifically,
we construct a  million image-text-pair dataset, called FLD-900M (FLD stands for {\bf{FL}}orence{\bf{D}}ataset), using a programmatic data curation pipeline that processes around  billion Internet images and their raw descriptions in parallel. Selection and post-filtering is employed to ensure data relevance
and quality while respecting legal and ethical constraints. To improve data quality, we
performed rigorous data filtering, similar to ALIGN~\cite{jia2021scaling}, including a simple
hash-based near-duplicate image removal, small-size image removal, image-text relevance, etc. In addition, we follow the sampling strategy introduced in~\cite{radford2021learning, ramesh2021zeroshot} with the
goal of achieving improved balance, informativeness, and learnability of the sampled dataset. The final form of the FLD-900M dataset consists of  images with  free-form texts (ranging from one word, phase to sentences),  unique queries, and  tokens in total.

\subsection{Unified Image-Text Contrastive Learning}
\label{sect:UniC}

CLIP~\cite{radford2021learning} implicitly assumes that each image-text pair has its unique caption, which allows other captions to be considered negative examples. However, in web-scale data, multiple images can be associated with identical captions. For example, in FLD-900M, there are  image-text pairs where there are more than one images corresponding to one identical text, and all images associated with the same text can be treated as positive pairs in contrastive learning.

To address this issue, we utilize a unified image-text contrastive learning
(\emph{UniCL})~\cite{Jianwei_UNICL2022}, where \emph{Florence} is pre-trained in an
image-label-description space. Given an image-text pair, we generate a triplet
 via a text hash-table, where  is
the image,  is the language description (\ie, hash value), and  is
the language label (\ie, hash key) indicating the index of unique language description in the
dataset. Note that we only map identical language description to the same hash key, \ie, language
label. Thus, all image-text pairs mapped to the same label  are regarded as positive
in our universal image-text contrastive learning. Others are still regarded as negative. The unified
learning objective in the common image-label-description space unifies two popular learning
paradigms -- mapping images to the label for learning discriminative representations (\ie,
supervised learning) and assigning each description with a unique label for language-image
pre-training (\ie, contrastive learning).

Our empirical experiments indicate that long language descriptions with rich content would be more
beneficial for image-text representation learning than short descriptions (\eg, one or two words).
We have to enrich the short description by generating prompt templates such as ``A photo of the
\texttt{[WORD]}", ``A cropped photo of \texttt{[WORD]}", as data augmentation. During
training, we randomly select one template to generate  for each short language
description.

Following UniCL~\cite{Jianwei_UNICL2022}, we denote  and  as the image encoder and text encoder, respectively.
 and  are the normalized visual feature vector and language feature
vector, respectively, where , and .  is a learnable temperature.
Given a mini-batch , we use a bi-directional supervised contrastive learning objective
between images and language descriptions to train the model as:

This objective contains two contrastive terms: the supervised image-to-language contrastive loss

where , and the supervised
language-to-image contrastive loss

where .

The generated language prompt is not a precise description of an image, typically not as informative as the associated text descriptions from the Internet. Although including generated language prompt might not affect classification accuracy, it hurts the performance in retrieval and vision-language tasks. To mitigate the negative effect from augmented prompts, our training is separated into two stages. In the first stage, we use
all data including augmented texts for training; while in the second stage, we exclude all augmented
data for continuing training. We trained  iterations in the first stage, and continuously
trained  iterations in the second stage. The Adam optimizer with decoupled weight decay
regularization is utilized for model training. The image size is  and the maximum
language description length is truncated at . The batch size is . We further trained
 iterations at a higher resolution of  to boost the performance, which follows
existing pre-training approaches.


\subsection{Transformer-based Florence Pretrained Models}
\label{sect:architecture}

Our \emph{Florence} pretrained model uses a two-tower architecture: a 12-layer transformer~\cite{NIPS2017_3f5ee243} as language encoder, similar to CLIP~\cite{radford2021learning}, and a
hierarchical Vision Transformer as the image encoder. The hierarchical Vision Transformer is
a modified Swin Transformer~\cite{liu2021Swin} with convolutional embedding, called \emph{CoSwin} Transformer. Specifically, we replace the patch embedding and patch merging
modules in the Swin Transformer~\cite{liu2021Swin} with the convolutional embedding layers as
described in CvT~\cite{Wu_2021_ICCV}. We use the \emph{CoSwin} Transformer with global average
pooling to extract image features. Two linear projection layers are added on top of the image
encoder and language encoder to match the dimensions of image and language features. Our \emph{Florence} pretrained model has
in total  parameters, including the language transformer with  parameters and the
\emph{CoSwin}-H transformer with  parameters. The model takes  days to train on 
NVIDIA-A100 GPUs with 40GB memory per GPU.

\subsection{Object-level Visual Representation Learning}

We extend the \emph{Florence} pretrained model to learn fine-grained (\ie, object-level) representation, which is fundamental to
dense prediction tasks such as object detection. For this goal, we add an adaptor \emph{Dynamic Head}~\cite{Dai_2021_CVPR} (or Dynamic DETR~\cite{Dai_2021_ICCV}), a unified attention
mechanism for the detection head, to the pretrained image encoder (\ie, \emph{CoSwin}). We can continue visual representation learning from coarse (scene) to fine (object).

Based on the hierarchical structure of the image encoder \emph{CoSwin}-H, we can get the output
feature pyramids from the different scale levels. The feature pyramid scale levels can be concatenated and
scaled-down or scaled-up into a 3-dimensional tensor with dimensions . The key idea of \emph{Dynamic Head}~\cite{Dai_2021_CVPR} is to deploy three attention
mechanisms, each on one of the orthogonal dimensions of the tensor, \ie, level-wise, spatial-wise, and
channel-wise. Compared with building a single self-attention mechanism over this tensor, \emph{Dynamic
Head} makes the computation more affordable and enables more efficient learning. The above three
attention mechanisms are applied sequentially, and we can effectively stack multiple blocks
consisting of such three attention layers together. Figure~\ref{fig:dyhead} shows the \emph{Dynamic
Head} building blocks. In this work, \emph{Dynamic Head} is trained with the one-stage ATSS
framework and losses.

\begin{figure}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{figure/dyhead.pdf}
  \caption{Dynamic Head~\cite{Dai_2021_CVPR} adapter is used for object-level visual representation learning.}
  \label{fig:dyhead}
\end{figure}


We have constructed a large-scale object detection dataset, called FLOD-9M (for {\bf{FL}}orence {\bf{O}}bject detection {\bf{D}}ataset), for object detection
pre-training. We merge several well-known object detection datasets, including
COCO~\cite{lin2015microsoft}, LVIS~\cite{Gupta_2019_CVPR}, OpenImages~\cite{openimages},
Object365~\cite{Shao_2019_ICCV}. In addition, we generate pseudo bounding boxes on ImageNet-22K
dataset~\cite{deng2009imagenet} by following~\cite{ZophGLCLC020_nips}, which further enlarges our
data. In the end, FLOD-9M consists of  images,  object categories, and
 bounding boxes including annotations and pseudo labels. We then pre-train our
\emph{Dynamic Head} model for  epochs with batch size , which takes  days on 
NVIDIA-A100 GPUs.


\subsection{Fine-Grained V+L Representation Learning}

We use METER~\cite{dou2021empirical} adapter to expand to fine-grained vision-language representation. In the vision-language area, \eg ~visual question answering (VQA) and image captioning, fine-grained representation (\ie, object-level) is indispensable. Thus, the object detector has been a de facto tool for image feature extraction,
followed by a fusion network for prediction in many works ~\cite{00010BT0GZ18,li:oscar,Zhang_2021_CVPR,abs-2012-06946,abs-2104-02096,chen:uniter}.
Recently, there is an increasing trend~\cite{abs-2104-03135,xue2021probing,wang2021simvlm,KimSK21,dou2021empirical} of end-to-end approaches to reduce dependency on the object bounding box, which instead consider grid-based feature representations as the fine-grained features for V+L tasks.

In the \emph{Florence} V+L adaptation model, we replace the image encoder of METER~\cite{dou2021empirical} with \emph{Florence} pretrained model \emph{CoSwin}, and use a pretrained  Roberta~\cite{liu2019roberta} as the language encoder, shown in Figure~\ref{fig:florence_meter}. The \emph{Florence} pretrained language encoder can be used for this adapter as it utilizes BERT-based architecture. Then, the two modalities are fused together to learn the contextual representation with a transformer network based on co-attention. The co-attention model
(Figure~\ref{fig:florence_meter}) allows feeding the text and visual features to two -layer transformers separately, and each top transformer encoding layer consists of one self-attention block, one cross-attention block, and one feed-forward network block. We first train the model with the image-text matching loss and the masked-language modeling loss. Then, we fine-tune the model on the downstream task, such as VQA~\cite{GoyalKSBP16} task.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/meter.pdf}
    \caption{METER~\cite{dou2021empirical} is used as Florence V+L adaptation model, trained with
    the image-text matching (ITM) loss
    and the masked language modeling (MLM) loss.}
    \label{fig:florence_meter}
\end{figure}

\subsection{Adaption to Video Recognition}
\label{sect:adapt_video}

The self-attention based design in Transformer makes it possible to unify the systems of image and video recognition. Our Video \emph{CoSwin} adapter can borrow the image encoder from \emph{CoSwin} for the video domain with minimum changes, similar to prior work~\cite{liu2021video}. First, the image tokenization layer is replaced with a video tokenization layer. Accordingly, video \emph{CoSwin} replaces the tokenization layer of \emph{CoSwin} (in Section~\ref{sect:architecture}) from 2D convolutional layers to 3D convolutional layers, which converts each 3D tube into one token. As the initialization to 3D convolutional weights, the pre-trained 2D convolutional weights of \emph{CoSwin} are duplicated along the temporal dimension and divided by the temporal kernel size to keep the mean and
variance of the output unchanged. Second,  video \emph{CoSwin} uses the 3D convolution-based patch merging operator instead of the 2D patch merging operator used in~\cite{liu2021video}. Such overlapped token merging
can enhance spatial and temporal interactions among tokens. Third, we follow prior work~\cite{liu2021video} to replace the 2D shifted window design with 3D shifted local windows in self-attention layers. We duplicate the 2D relative positional embedding matrix from the pre-trained \emph{CoSwin} along the temporal dimension to initialize the 3D positional embedding matrix. In this way, the 2D relative positional embedding is the same for each temporal shift. In addition, all other layers and weights
(including self-attention, FFN) can be inherited directly from the pre-trained \emph{CoSwin}. To mitigate memory issues in the video training, we adopt the dynamic window size strategy, i.e., a relatively small window size in early stages of \emph{CoSwin}, and large window sizes in its later stages.

\begin{table*}[ht]
\centering
\setlength{\tabcolsep}{7.1pt}
\small
\renewcommand{\arraystretch}{1.35}
\begin{tabular}{l|cccccccccccc}
\toprule
 & \rotatebox{90}{Food101} & \rotatebox{90}{CIFAR10} & \rotatebox{90}{CIFAR100} &
 \rotatebox{90}{SUN397} & \rotatebox{90}{Stanford Cars} & \rotatebox{90}{FGVC Aircraft} &
 \rotatebox{90}{VOC2007} & \rotatebox{90}{DTD} & \rotatebox{90}{Oxford Pets} &
 \rotatebox{90}{Caltech101} & \rotatebox{90}{Flowers102} & \rotatebox{90}{ImageNet}\\ \midrule
CLIP-ResNet-50x64 & 91.8 & 86.8 & 61.3 & 48.9 & 76.0 & 35.6 & 83.8 & 53.4
& 93.4 & 90.6 & 77.3 & 73.6 \\
CLIP-ViT-L/14 (pix) & 93.8 & \underline{\bf{95.7}} & 77.5 & 68.4 &
78.8 & 37.2 & 84.3 & 55.7 & 93.5 & 92.8 & 78.3 & 76.2 \\
FLIP-ViT-L/14 & 92.2 & \underline{\bf{95.7}} & 75.3 & 73.1 & 70.8 &
\underline{\bf{60.2}} & - & 60.7 & 92.0 & 93.0 & \underline{\bf{90.1}} & 78.3 \\
Florence-CoSwin-H (pix) & \underline{\bf{95.1}} & 94.6 & \underline{\bf{77.6}} &
\underline{\bf{77.0}} & \underline{\bf{93.2}} & 55.5 & \underline{\bf{85.5}} & \underline{\bf{66.4}}
& \underline{\bf{95.9}} & \underline{\bf{94.7}} & 86.2 & \underline{\bf{83.7}} \\ \bottomrule
\end{tabular}
\caption{Zero-shot transfer of image classification comparisons on 12 datasets: CLIP-ResNet-50x64~\cite{radford2021learning}, FLIP-ViT-L/14~\cite{yao2021filip}.}
\label{tab:zero-shot}
\end{table*}

\subsection{Scalable Training Infrastructure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/profiling.pdf} \vspace{-0.7em}
    \caption{GPU memory reduction for various batch sizes. We compared the profiling between Torch (w/o optimization) and Florence (w/ optimization) on various number of GPUs.}
    \label{fig:florence_profiling}
\end{figure}

To train the \emph{Florence} model on our large-scale dataset, our scalable training infrastructure
faces two main challenges: reducing memory cost on each GPU and increasing the throughput. Reducing
the memory cost allows us to feed more data into each GPU and use a larger batch size, which has
been proved to be effective for contrastive learning.
Increasing the throughput can significantly speed up the whole training process and thus reduce
carbon emissions. We have developed several techniques that can be combined to achieve the two
goals:

\begin{description}
  \item[Zero Redundancy Optimizer (ZeRO)] The ZeRO
      technique~\cite{DBLP:journals/corr/abs-1910-02054} partitions the optimizer states,
      gradients and parameters across the GPUs and each partition is only updated locally. Thus,
      the memory consumption is largely reduced.
  \item[Activation Checkpointing] For a checkpointed model component, \eg, multi-head attention,
      it reruns a forward pass during backward pass. In this way, the internal gradients in the
      component do not need to be stored in the forward pass and then reduce the memory cost in
      the training.
  \item[Mixed-precision Training] In mixed-precision training, various operations are trained with
      different numerical precision (\ie, float-32 or float-16). Float-32 is used for numerically
      less stable operations, such as layer normalization; while float-16 is used for the other
      operations. Such a combination improves the training throughput and maintains the model
      performance.
  \item[Gradient Cache] The gradient cache technique~\cite{gao2021scaling} is able to increase the
      total batch size in a training step. A large batch size is shown to be beneficial to learn better
      representations in previous works. However, it is bounded by available GPU memory. To
      resolve this problem, we factor the contrastive loss by breaking the large batch gradient
      update into several sub-updates that can fit into GPU memory. It enables us to train big
      models with a large batch size.
\end{description}

Thanks to these above optimizations, we can achieve consistent improvement in reducing GPU memory for variable batch sizes on various numbers of NVIDIA-A100s, shown in Figure~\ref{fig:florence_profiling}.


\section{Experiments}

\begin{table*}[ht]
\centering
\setlength{\tabcolsep}{8.3pt}
\small
\renewcommand{\arraystretch}{1.35}
\begin{tabular}{l|ccccccccccc}
\toprule
 & \rotatebox{90}{Food101} & \rotatebox{90}{CIFAR10} & \rotatebox{90}{CIFAR100} &
 \rotatebox{90}{SUN397} & \rotatebox{90}{Stanford Cars} & \rotatebox{90}{FGVC Aircraft} &
 \rotatebox{90}{VOC2007} & \rotatebox{90}{DTD} & \rotatebox{90}{Oxford Pets} &
 \rotatebox{90}{Caltech101} & \rotatebox{90}{Flowers102} \\ \midrule
SimCLRv2-ResNet-152x3 & 83.6 & 96.8 & 84.5 & 69.1 & 68.5 & 63.1 & 86.7 & 80.5 &
92.6 & 94.9 & 96.3  \\
ViT-L/16 (pix) & 87.4 & 97.9 & 89.0 & 74.9 & 62.5 & 52.2 & 86.1 &
75.0 & 92.9 & 94.7 & 99.3  \\
EfficientNet-L2 (pix) & 92.0 & \underline{\bf{98.7}} &
\underline{\bf{89.0}} & 75.7 & 75.5 & 68.4 & 89.4 & 82.5 & 95.6 & 94.7 & 97.9  \\
CLIP-ResNet-50x64 & 94.8 & 94.1 & 78.6 & 81.1 & 90.5 & 67.7 & 88.9 & 82.0
& 94.5 & 95.4 & 98.9 \\
CLIP-ViT-L/14 (pix) & 95.9 & 97.9 & 87.4 & 82.2 & 91.5 & 71.6 &
89.9 & 83.0 & 95.1 & 96.0 & 99.2 \\
Florence-CoSwin-H (pix) & \underline{\bf{96.2}} & 97.6 & 87.1 & \underline{\bf{84.2}} &
\underline{\bf{95.7}} & \underline{\bf{83.9}} & \underline{\bf{90.5}} & \underline{\bf{86.0}} &
\underline{\bf{96.4}} & \underline{\bf{96.6}} & \underline{\bf{99.7}} \\  \bottomrule
\end{tabular}
\caption{Comparisons of image classification linear probing on 11 datasets with existing state-of-the-art models, including SimCLRv2~\cite{chen2020big}, ViT~\cite{dosovitskiy2020vit}, EfficientNet~\cite{Xie_2020_CVPR}, and CLIP~\cite{radford2021learning}.}
\label{tab:linear}
\end{table*}


\subsection{Zero-shot Transfer in Classification}
\label{sec:zero-shot}

In computer vision, zero-shot learning usually refers to the study of predicting classes that are
defined via descriptive text. As a vision foundation model, \emph{Florence} can be directly used to
predict if an image and a text snippet are semantically matched together in the task dataset. We
follow the same method of CLIP~\cite{radford2021learning} to perform zero-shot classification. For each
dataset, we use the names of all the classes in the dataset as the set of potential text pairings
and predict the most probable (image, text) pair according to \emph{Florence}. We compute the
feature embedding of the image for \emph{CoSwin} and the feature embedding of the set of possible
texts by the language encoder. The cosine similarities among these embeddings are then calculated,
and then we rank the similarity scores over all the classes to select the Top-1 or Top-5 classes as
the predicted classes. Here, we do not need to compute the normalized cosine similarity as done
in~\cite{radford2021learning}, since it won't affect the ranking order of final results.

We evaluate our \emph{Florence} model on the ImageNet-1K dataset and 11 downstream datasets from the
well-studied evaluation suit introduced by~\cite{kornblith2019better}. Note that our benchmarks
exclude the Birdsnap~\cite{6909656} dataset from 12 original classification datasets introduced
in~\cite{kornblith2019better}, because  of the image URLs provided by the authors
are invalid. We follow the same prompt templates and engineering, and ensembling
as previously proposed in~\cite{radford2021learning} for evaluating zero-shot performance. For all zero-shot tasks in this
paper, we follow the setup in CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling} to
remove near-duplicate test images from our training data. Table~\ref{tab:zero-shot} shows the
results over these 12 datasets, in comparison with the best performance achieved by both CLIP ResNet
and Vision Transformer models, and the concurrent work FILIP~\cite{yao2021filip}. \emph{Florence} outperforms on  tasks compared with state-of-the-art methods. We achieved a remarkable improvement in the zero-shot transfer on ImageNet-1K -- the top-1 accuracy of  ( over SOTA result),  and the top-5 accuracy of
.

\subsection{Linear Probe in Classification}

Linear probe as another main metric for evaluating representation quality has been used in most
recent studies, including self-supervised learning~\cite{pmlr-v119-chen20j, chen2020big},
self-training with noisy student~\cite{Xie_2020_CVPR} and contrastive
learning~\cite{radford2021learning}. We follow the same setting and implementation of
CLIP~\cite{radford2021learning} for linear evaluation, where the image encoder (or vision backbone)
is frozen, and only the appended linear layers can be fine-tuned on the downstream datasets. We use public available models (shown in Table 10~\cite{radford2021learning}) to verify the correctness of our own implementation. The variance between our reproduced results and their reported results is  for each task. Our linear evaluation considers 11 classification
benchmarks which are also used for our zero-shot transfer of classification. We compared our results with state-of-the-art methods with their best performance models, including SimCLRv2~\cite{chen2020big},
ViT~\cite{dosovitskiy2020vit}, Noisy Student~\cite{Xie_2020_CVPR} and
CLIP~\cite{radford2021learning} on Table~\ref{tab:linear}. Our results are consistently  better than
existing state-of-the-art results, expect for two datasets: CIFAR10, CIFAR100. On the two datasets, the input image
resolution is quite low (\ie, ). Training with higher resolution definitely boosts the performance,such as  Efficient-L2~\cite{Xie_2020_CVPR} which achieves the best accuracy compared with all other approaches trained on lower-resolution images.

\subsection{ImageNet-1K Fine-tune Evaluation}
\label{sect:imagenet_finetune}

\emph{Florence} can be easily adapted to support continual fine-tuning on target classification
tasks. We do not change or add anything into our architecture, but continue the training on
task-specific data using the same pre-training loss (shown in Equation~\ref{eq:obj_bicon}). We feed the class name to the text
encoder of \emph{Florence} to get the text feature embedding. We use the same prompt templates as in
~\cite{radford2021learning, jia2021scaling} to expand the descriptions of
ImageNet~\cite{deng2009imagenet} class names.

We evaluate the performance of continual fine-tuning on ImageNet ILSVRC-2012
benchmark~\cite{deng2009imagenet}. Our image encoder \emph{CoSwin}-H is
fine-tuned at the resolution of  with a batch size of  for  epochs. We use a cosine learning rate decay scheduler with  warmup steps and a peak learning rate of
. The comparisons with state-of-the-art results are shown in
Table~\ref{tab:imagenet_result}. Our model outperforms BiT~\cite{kolesnikov2020big} with larger model size and ALIGN~\cite{jia2021scaling} trained from more data in terms of Top-1 and Top-5 accuracy. Our result is slightly worse than SOTA~\cite{dai2021coatnet}, but their model and data scale are both  larger.


\begin{table}[t]
\centering
\setlength{\tabcolsep}{6.3pt}
\small
\renewcommand{\arraystretch}{1.35}
\begin{tabular}{l|rrcc}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Params} & \multirow{2}{*}{Data\;} &
\multicolumn{2}{c}{Accuracy} \\
& & & Top-1 & Top-5\\ \midrule
BiT-L-ResNet152x4 & 928M & 300M & 87.54 & 98.46  \\
ALIGN-Efficient-L2 & 480M & 1800M & 88.64 & 98.67  \\
ViT-G/14 & 1843M & 3000M & 90.45 & -  \\
CoAtNet-7 & 2440M & 3000M & \underline{\bf{90.88}} & - \\
Florence-CoSwin-H & 637M & 900M & 90.05 & \underline{\bf{99.02}}  \\ \bottomrule
\end{tabular}
\caption{Classification fine tuning on ImageNet-1K. Florence is compared with: BiT-L-ResNet152x4~\cite{kolesnikov2020big}, ALIGN-Efficient-L2~\cite{jia2021scaling}, ViT-G/14~\cite{zhai2021scaling}, CoAtNet-7~\cite{dai2021coatnet} in terms of model scale, data scale and Top-1/Top-5 accuracy.}
\label{tab:imagenet_result}
\end{table}


\subsection{Few-shot Cross-domain Classification}

The Cross-Domain Few-Shot learning benchmark \cite{cdfsl} is used to measure an algorithm's capability to adapt to downstream few-shot target tasks, containing domains with varying levels of dissimilarity to typical consumer photographs. The datasets in the benchmark include: CropDisease~\cite{plantdisease} (plant leaf images, 38 disease states over 14 plant species), EuroSAT~\cite{eurosat} (RGB satellite images, 10 categories), ISIC 2018~\cite{isic2018,ham10000} (dermoscopic images of skin lesions, 7 disease states), and ChestX~\cite{chestx} (Chest X-rays, 16 conditions). Exemplar image for each dataset is shown on the top of Table~\ref{tab:cdfsl}. The evaluation protocol involves 5-way classification across 5-shot, 20-shot, and 50-shot. The classes and shots are randomly sampled for each episode, for 600 episodes per way and shot. Average accuracy over all episodes is reported.

To predict the class, we append a single linear layer as an adapter head to our image encoder \emph{CoSwin}. Training occurs over 100 epochs per episode. We use SGD with momentum, with learning rate and momentum values of , respectively, for \emph{CoSwin}, and , respectively, for the adapter head. Horizontal data flip augmentation is used for training and test, and dropout of  is used between the image encoder and the classifier head.

Table \ref{tab:cdfsl} shows the results of adapting our model to the CD-FSL benchmark, in comparison to the winner of the challenge benchmark~\cite{cdfsltop}, which employs ensembes and transductive learning. By comparison, we employ a single model and no transduction on the test data is performed, yet we achieve higher results without any ``bells and whistles''.

\begin{table}[t]
\centering
\begin{minipage}[t]{1.0\linewidth}
  \centering
\includegraphics[width=1.0\linewidth]{figure/CD_DSL.pdf}
\end{minipage}\vspace{0.3em}
\setlength{\tabcolsep}{3.8pt}
\small
\renewcommand{\arraystretch}{1.35}
\begin{tabular}{r|l|cccc|c}
\toprule
 & Model & ISIC & EuroSAT & CropD & ChestX & mean \\ \midrule
\multirow{2}{*}{5-shot} & CW & 57.4 & 88.1 & 96.6 & 29.7 & 68.0 \\
 & Florence & 57.1 & 90.0 & 97.7 & 29.3 & \underline{\bf{68.5}} \\ \midrule
\multirow{2}{*}{20-shot} & CW & 68.1 & 94.7 & 99.2 & 38.3 & 75.1 \\
 & Florence & 72.9 & 95.8 & 99.3 & 37.5 & \underline{\bf{76.4}} \\ \midrule
\multirow{2}{*}{50-shot} & CW & 74.1 & 96.9 & 99.7 & 44.4 & 78.8 \\
 & Florence & 78.3 & 97.1 & 99.6 & 42.8 & \underline{\bf{79.5}} \\ \bottomrule
\end{tabular}
\caption{Comparison with CW~\cite{cdfsltop} (CD-FSL Challenge 2020 Winner) on CD-FSL benchmark. The average result comparison is  (Florence) vs.  (CW).}
\label{tab:cdfsl}
\end{table}


\subsection{Image-Text Retrieval}

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{9.5pt}
\small
\renewcommand{\arraystretch}{1.35}
\begin{tabular}{cl|cccc|cccc}
\toprule
& & \multicolumn{4}{c|}{Flickr30K ( test set)} & \multicolumn{4}{c}{MSCOCO ( test set)} \\
& Method & \multicolumn{2}{c}{Image  Text} & \multicolumn{2}{c|}{Text  Image} &
\multicolumn{2}{c}{Image  Text} & \multicolumn{2}{c}{Text  Image} \\
& & R@1 & R@5  & R@1 & R@5  & R@1 & R@5  & R@1 & R@5 \\
\midrule
\multirow{5}{*}{\it{Zero-shot}} & ImageBERT~\cite{qi:imagebert} & 70.7 & 90.2 & 54.3 & 79.6 & 44.0 &
71.2 & 32.3 & 59.0  \\
& UNITER~\cite{chen:uniter} & 83.6 & 95.7 & 68.7 & 89.2 & - & - & - & - \\
& CLIP~\cite{radford2021learning} & 88.0 & 98.7 & 68.7 & 90.6 & 58.4 & 81.5 & 37.8 & 62.4 \\
& ALIGN~\cite{jia2021scaling} & 88.6 & 98.7  & 75.7 & \underline{\bf{93.8}}  & 58.6 & 83.0  & 45.6 &
69.8  \\
& FLIP~\cite{yao2021filip} & 89.8 & \underline{\bf{99.2}}  & 75.0 & 93.4  & 61.3 & 84.3  & 45.9 &
70.6  \\
& Florence & \underline{\bf{90.9}} & \underline{\bf{99.1}} &  \underline{\bf{76.7}} &
\underline{\bf{93.6}} & \underline{\bf{64.7}} & \underline{\bf{85.9}} & \underline{\bf{47.2}} &
\underline{\bf{71.4}} \\
\hline
\multirow{7}{*}{\it{Fine-tuned}} & GPO~\cite{chen:vsepooling} & 88.7 & 98.9  & 76.1 & 94.5& 68.1 &
90.2 & 52.7 & 80.2  \\
& UNITER~\cite{chen:uniter} & 87.3 & 98.0 & 75.6 & 94.1 & 65.7 & 88.6 &  52.9 & 79.9  \\
& ERNIE-ViL~\cite{yu:ernie-vil} & 88.1 & 98.0 & 76.7 & 93.6   & - & - & - & - \\
& VILLA~\cite{gan:villa} & 87.9 & 97.5 & 76.3 & 94.2 & - & - & - & - \\
& Oscar~\cite{li:oscar} & - & - & - & - & 73.5 & 92.2 & 57.5 & 82.8 \\
& ALIGN~\cite{jia2021scaling} & {95.3} & {99.8} & {84.9} & {97.4} & {77.0} & {93.5} & {59.9} &
{83.3} \\
& FLIP~\cite{yao2021filip} & 96.6 & \underline{\bf{100.0}}  & 87.1 & 97.7  & 78.9 & 94.4  & 61.2 &
84.3  \\
& Florence & \underline{\bf{97.2}} & \underline{\bf{99.9}} & \underline{\bf{87.9}} &
\underline{\bf{98.1}} & \underline{\bf{81.8}} & \underline{\bf{95.2}}& \underline{\bf{63.2}} &
\underline{\bf{85.7}} \\
\bottomrule
\end{tabular}
\caption{Image-text retrieval comparisons on Flickr30K and MSCOCO datasets (zero-shot and
fine-tuned).}
\label{tab:flickr30k_mscoco_result}
\end{table*}

Table~\ref{tab:flickr30k_mscoco_result} presents the zero-shot transfer and fine-tuning performance
of \emph{Florence} for both text and image retrieval on the Flickr30k~\cite{plummer2016flickr30k}
and MSCOCO~\cite{lin2015microsoft} datasets.

For zero-shot retrieval, we feed the input text (or image) to the language (or image) encoder of
\emph{Florence} to get the feature embeddings, and also compute the feature embeddings of the set of
possible images (or texts) by the image (or language) encoder. Then we compute cosine similarity of
these embeddings and rank the similarity scores over the testing set to select the Top-1 or Top-5
results. Zero-shot \emph{Florence} matches or outperforms all prior zero-shot results on these two
datasets.

For fine-tuning retrieval, we continuously train our language and text encoders on the target
image-text pair data, as well as classification fine-tuning (shown in
Section~\ref{sect:imagenet_finetune}). We fine-tune our model with a batch size of  for 
epochs. We use the cosine learning rate decay scheduler with  warmup steps and a peak learning
rate of . Our results are superior to all previous fine-tuning results on the two
datasets. Moreover, our fine tuning on retrieval is more efficient, with only roughly  and  fine-tuning epochs of ALIGN~\cite{jia2021scaling} on Flickr30k and MSCOCO respectively.


\subsection{Object Detection and Zero-shot Transfer}

Object detection is one of the most prominent applications in computer vision. Compared with
existing large-scale pre-trained models (\eg, CLIP~\cite{radford2021learning},
ALIGN~\cite{jia2021scaling}, Wu Dao 2.0~\cite{Wudao2}), \emph{Florence} is more desirable for object
detection since its adaptation helps learn visual representation at the object level.  We evaluate
its performance of object-level visual representations via fine-tuned object
detection and zero-shot transfer tasks.

\paragraph{Fine-tuning} We evaluate fine-tuning on three popular object detection datasets:
COCO~\cite{lin2015microsoft}, Object365~\cite{Shao_2019_ICCV}, and Visual
Genome~\cite{krishnavisualgenome}. For COCO, we increase the maximum image side to  and
fine-tune with multi-scale training for  epochs. We follow the same multi-scale testing strategy
widely used in existing state-of-the-art approaches. For Object365, we use the same input resolution
of images (\ie, the maximum image side ) as the Multi-dataset Detection\footnote{This work was ranked 1-st
in the object detection track of ECCV 2020 Robust Vision Challenge.}~\cite{zhou2021simple} for
fine-tuning. For Visual Genome, we increase the maximum side of input resolution to  and
fine-tune with multi-scale training for  epochs. To leverage attributes annotations in Visual
Genome, we insert an  ROI pool on the final stage of \emph{CoSwin} backbone to extract
features for attribute learning, which allows the object detection adapter being optimized for multi-task
learning.


\begin{table}[t]
\centering
\setlength{\tabcolsep}{12.5pt}
\small
\renewcommand{\arraystretch}{1.35}
\begin{tabular}{c|l|c}
\toprule
Benchmark & Model & AP  \\ \midrule
\multirow{3}{*}{\it{COCO miniVal}} & DyHead & 60.3 \\
& Soft Teacher & 60.7 \\
& Florence & \underline{\bf{62.0}} \\ \midrule
\multirow{3}{*}{\it{COCO test-Dev}} & DyHead & 60.6 \\
& Soft Teacher & 61.3 \\
& Florence & \underline{\bf{62.4}} \\ \midrule
\multirow{2}{*}{\it{Object365}} & Multi-dataset Detection & 33.7 \\
& Florence & \underline{\bf{39.3}} \\ \midrule
\multirow{2}{*}{\it{Visual Genome}} & VinVL & 13.8 \\
& Florence & \underline{\bf{16.2}} \\
\bottomrule
\end{tabular}
\caption{Object detection fine tuning comparisons with state-of-the-art methods, including DyHead~\cite{Dai_2021_CVPR}, Soft Teacher~\cite{Xu_2021_ICCV}, Multi-dataset Detection~\cite{zhou2021simple}, VinVL~\cite{Zhang_2021_CVPR}.}
\label{tab:od_result}
\end{table}

\begin{table*}[ht]
\centering
\setlength{\tabcolsep}{6.5pt}
\small
\renewcommand{\arraystretch}{1.35}
\begin{tabular}{cl|ccccccccccc}
\toprule
 & & \rotatebox{90}{Aquarium} & \rotatebox{90}{BCCD} & \rotatebox{90}{Chess Pieces} &
 \rotatebox{90}{Mask Wearing} & \rotatebox{90}{Oxford Pets} & \rotatebox{90}{Packages} &
 \rotatebox{90}{Pistols} & \rotatebox{90}{PKLot} & \rotatebox{90}{Pothole} & \rotatebox{90}{Thermal}
 & \rotatebox{90}{Wildfire Smoke} \\ \midrule
 &Images & 638 & 364 & 292 & 149 & 3680 & 26 & 2986 & 12416 & 665 & 203 & 737\\
 &Categories& 7 & 3 & 12 & 2 & 37 & 1 & 1 & 2 & 1 & 2 & 1\\
 \midrule\midrule
\multirow{2}{*}{\it{Fine-tuned}} & DyHead-Swin-L (full) & 53.1 & 62.6 & 80.7 &
52.0 & 85.9 & 52.0 & 74.4 & 98.0 & 61.8 & 75.9 & 58.7 \\
 & DyHead-Swin-L (5-shot) & 39.0 & 40.6 & 57.3 & 26.8 & 47.5 & 32.8 & 20.0 &
 22.1 & 10.8 & 54.9 & 14.2 \\  \midrule
\multirow{2}{*}{\it{Zero-shot}} & ZSD & 16.0 & 1.2 & 0.1 & 0.6 & 0.3 & 58.3
& 31.5 & 0.2 & 2.4 & 37.4 & 0.002 \\
 &Florence & 43.1 & 15.3 & 13.4 & 15.0 & 68.9 & 79.6 & 41.4 & 31.4 & 53.3 & 46.9 & 48.7 \\
 \bottomrule
\end{tabular}
\caption{Zero-shot transfer in object detection, in comparison with previous state-of-the-art
model DyHead~\cite{Dai_2021_CVPR} (on COCO) fine tuning results on full-set or 5-shot respectively and zero-shot detection baseline model ZSD~\cite{bansal2018zero}.}
\label{tab:zero_od_result}
\end{table*}
\begin{figure*}[ht]
  \centering
\includegraphics[width=1.0\linewidth]{figure/Detection.pdf} \vspace{-1.5em}
\caption{Our fine-tuned detection results on COCO (sparse object boxes), Object365 (dense object boxes), Visual Genome (w/ object attributes), and zero-shot transfer results on 11 downstream detection tasks. Boxes with different colors denote different object categories.}
\end{figure*}\vspace{0.3em}

We compare \emph{Florence} with state-of-the-art results on these three benchmarks in
Table~\ref{tab:od_result}. In object detection, the standard mean average precision (AP) metric is used to report results under different IoU thresholds and object scales for all datasets. We follow the metrics used in existing state-of-the-art methods. For COCO, Object365 and zero-shot transfer benchmarks, we use mAP, \ie, average over multiple IoUs (). For Visual Genome, we use AP50 at IoU threshold . As we can see, \emph{Florence} establishes new results in these main benchmarks of object detection.

\paragraph{Zero-shot Transfer} Zero-shot object detection is more challenging than zero-shot
classification, since neither object proposal classification nor location (\ie, bounding box
regression) in downstream tasks is seen during training. In our zero-shot transfer setting, object proposal and object classification are decoupled into two tasks. Object proposal discriminates object from
background, ignoring semantics of object categories. Classification, on the other hand, focuses on
object semantics for each bounding box proposal.
In spirit, this setup is similar to the behavior of R-CNN model~\cite{RCNN2014} which has been widely used for object detection before. Using this approach, we can follow existing work on zero-shot image classification to zero-shot
transfer in object detection, to evaluate the \emph{Florence} for novel object recognition. As mentioned in ZSD~\cite{bansal2018zero}, it more approaches real world settings.

For zero-shot transfer, the training of the detection adapter can be different from fine-tuning.
Specifically, we freeze the \emph{CoSwin} backbones and pre-train the \emph{Dynamic Head} on FLOD-9M
by neglecting semantics from each object bounding box. We treat the object detection pre-training as
general-purpose object proposal training. Note that the detection pre-training only updates the object adapter, and does not affect the fused
feature representations learned from large-scale image-text pairs. In inference, we apply the pre-trained \emph{CoSwin} and \emph{Dynamic Head} on
downstream datasets, and obtain the object proposals for every image. For each object proposal, we
apply zero-shot classification, as described in Section~\ref{sec:zero-shot}.

To evaluate \emph{Florence}'s transferability to novel, diverse and application-oriented tasks, following \cite{harold_GLIP2021}, we curate an ``open-set oject detection benchmark" which aggregates   public datasets from
Roboflow\footnote{https://public.roboflow.com/object-detection}, spanning scenarios including fine-grained fishes/chess detection, drone-view detection, and thermal object detection. We use their split test datasets for evaluation. Table~\ref{tab:zero_od_result} shows that our \emph{Florence} model effectively zero-shot transfers to these tasks. We use the results of the baseline approach ZSD~\cite{bansal2018zero}, which considers a similar setting, for reference. In our implementation\footnote{We refer to \cite{harold_GLIP2021} for details.}, we replace their supervised object detector FasterRCNN with the recent SOTA detector~\cite{Dai_2021_CVPR} and use pre-trained BERT as the language encoder. Both are pre-trained end-to-end on the Objects365 dataset. Thanks to large-scale image-text pretraining, \emph{Florence} shows remarkable
gains on all tasks. Zero-shot in object detection still has a long way to be applied to
real-world tasks.
We further compare \emph{Florence} zero-shot with previous state-of-the-art
detector\footnote{It is pre-trained on ImageNet and COCO in supervised way.}~\cite{Dai_2021_CVPR} (on
COCO) fine-tunning on these tasks. We can observe noticeable performance gap between zero-shot and
supervised learning, especially for novel scenarios whose concepts/classes may not be covered by the pre-training dataset, such as ``BCCD" (blood cells photos), ``Chess Pieces" (Chess board photos and
various pieces). However, the results are  encouraging when compared with few-shot fine-tuning
results. \emph{Florence} outperforms in  tasks over 5-shot fine tuning, and outperforms full-set fine-tuning on the ``Packages" dataset, consisting of only  images for training. It demonstrates
the foundation models' great potential of improving data efficiency and reducing deployment cost for new tasks or domains.

\subsection{V+L Representation Learning}
The vision-langauge pretraining (VLP) is performed on
MSCOCO~\cite{lin2015microsoft}, Conceptual Captions (CC)~\cite{sharma2018conceptual},
CC12M~\cite{changpinyo2021cc12m},
SBU~\cite{OrdonezKB11}, and Visual Genome (VG)~\cite{krishnavisualgenome}.
These datasets result in  million images with  million associated captions.
Beyond replacing the image encoder with \emph{CoSwin}-H of
our \emph{Florence} model on~\cite{dou2021empirical}, we
remove the weight decay on the text embedding layer and
the modality-specific embedding.
ITM and MLM are applied for VLP with  epochs with the image input size as .

To evaluate the performance, we fine-tune the pre-trained model on the
challenging VQA~\cite{GoyalKSBP16} task, which is
to answer a question based on the image context.
The dataset consists of K training images and K validation images.
Only K validation images are reserved and the rest are merged with
the training data for fine-tuning.
As a common practice, the problem is cast as a classification task
where each class corresponds to an answer.
The final pooling representations
are fed into a
randomly-initialized multilayer perceptron (MLP) network to predict the answer
over  answers.
The loss is the binary cross-entropy loss, and the inference is to select the answer with the
highest confidence.
The model is fine-tuned for  epochs with the learning rate as  and is evaluated
on the \texttt{test-dev} and \texttt{test-std}. The final accuracy is calculated on
the public server \footnote{http://evalai.com}.

Figure~\ref{tab:vqa} shows the comparison results with the existing methods.
As we can see, we achieve the new state-of-the-art performance.
Compared with SimVLM~\cite{wang2021simvlm}, which uses B image-text pairs,
we only use M data to pre-train the image encoder and M
for VLP, but achieve better results.
This also demonstrates the data efficiency of our approach.


\begin{table}[t]
\centering
\setlength{\tabcolsep}{10.5pt}
\small
\renewcommand{\arraystretch}{1.35}
\begin{tabular}{l|cc}
\toprule
    Model                                 & test-dev            & test-std \\ \midrule
    UNITER~\cite{chen:uniter}           & 73.82               & 74.02 \\
    Visual Parsing~\cite{xue2021probing}  & 74.00               & 74.17 \\
    PixelBERT~\cite{huang2020pixel}       & 74.45               & 74.55 \\
    VILLA~\cite{gan:villa}             & 74.69               & 74.87 \\
    UNIMO~\cite{li2020unimo}              & 75.06               & 75.27 \\
    ALBEF~\cite{li2021align}              & 75.84               & 76.04 \\
    VinVL~\cite{Zhang_2021_CVPR}           & 76.52               & 76.60 \\
    CLIP-ViL~\cite{shen2021much}          & 76.48               & 76.70 \\
    METER~\cite{dou2021empirical}         & 77.68               & 77.64 \\
    SimVLM~\cite{wang2021simvlm}          & 80.03               & 80.34 \\
    \midrule
    Florence                            & \underline{\bf{80.16}}      & \underline{\bf{80.36}} \\
    \bottomrule
\end{tabular}
\caption{Compare our model with the existing state-of-the-art methods on VQA.}
  \label{tab:vqa}
\end{table}

\begin{table*}[ht]
\centering
\setlength{\tabcolsep}{10.5pt}
\small
\renewcommand{\arraystretch}{1.35}
\begin{tabular}{l|r|r|rrr}
\toprule
Method & Pre-training Type & Pre-training Data & R@1 & R@5 & R@10 \\
\midrule
MIL-NCE~\cite{miech2020end} & Video & HowTo100M & - & - & 32.4 \\
MMV~\cite{alayrac2020self} & Video & HowTo100M, AudioSet & - & - & 31.1 \\
VideoCLIP~\cite{xu2021videoclip} & Video & HowTo100M & 10.4 & 22.2 & 30.0 \\
VATT~\cite{akbari2021vatt} & Video & HowTo100M, AudioSet & - & - & 29.7 \\
MCN~\cite{chen2021multimodal} & Image and Video & HowTo100M & - & - & 33.8 \\
Frozen-in-Time~\cite{bain2021frozen} & Image and Video & ImageNet,
CC, WebVid-2M & 18.7 & 39.5 & 51.6 \\
\midrule
CLIP-ViT-B/16~\cite{radford2021learning} & Image & WIT400M & 26.0 & 49.4
& 60.7 \\
Florence & Image & FLD-900M & \underline{\bf{37.6}} & \underline{\bf{63.8}} & \underline{\bf{72.6}}
\\
\bottomrule
\end{tabular}
\caption{Zero-shot text-to-video retrieval results on MSR-VTT 1K-A test set. (: Feature extracted from the pre-trained model~\cite{miech2020end}, followed by another stage of video-and-language pre-training) The pretraining data used in these existing methods include HowTo100M~\cite{miech2019howto100m}, AudioSet~\cite{gemmeke2017audio},  ImageNet~\cite{deng2009imagenet}, CC~\cite{sharma2018conceptual},  WebVid-2M~\cite{bain2021frozen}, WIT400M~\cite{radford2021learning}}
\label{tab:t2v_retrieval_results}
\end{table*}

\begin{table*}[ht]
\centering
\setlength{\tabcolsep}{12pt}
\small
\renewcommand{\arraystretch}{1.35}
\begin{tabular}{l|r|cc|cc|c|c}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Pretraining Data} & \multicolumn{2}{c|}{Kinetics-400} &
\multicolumn{2}{c|}{Kinetics-600} & \multirow{2}{*}{Views} & \multirow{2}{*}{Params}\\
& & Top-1 & Top-5 & Top-1 & Top-5 & & \\
\midrule
ViViT-H/16x2 & JFT-300M & 84.8 & 95.8 & 85.8 & 96.5 & 4  3 & 648M\\
VideoSwin-L & ImageNet-22K & 84.6 & 96.5 & 85.9 & 97.1 & 4  3 & 200M \\
VideoSwin-L & ImageNet-22K & 84.9 & 96.7 & 86.1 & 97.3 & 10  5 & 200M
\\
TokenLearner 16at18+L/10 & JFT-300M & 85.4 & 96.3 & 86.3 & 97.0 & 4
 3 & 460M \\
Florence & FLD-900M & \underline{\bf{86.5}} & \underline{\bf{97.3}} & \underline{\bf{87.8}} &
\underline{\bf{97.8}} & 4  3 & 647M \\
\bottomrule
\end{tabular}
\caption{Comparison to state-of-the-art methods, including ViViT~\cite{arnab2021vivit}, VideoSwin~\cite{liu2021video}, TokenLearner~\cite{ryoo2021tokenlearner}, on Kinetics-400 and Kinetics-600. Views indicate . }
\label{tab:k400}
\end{table*}


\subsection{Zero-Shot Text-to-Video Retrieval}

Although \emph{Florence} is pre-trained on image-text pairs, it can be easily adapted to video tasks
(shown in Section~\ref{sect:adapt_video}), such as text-video retrieval. We expand the input 2D
patch embeddings and positional embeddings to 3D so that the encoder can process video inputs,
following~\cite{arnab2021vivit}. Then, we perform zero-shot text-to-video evaluation on the
MSR-VTT~\cite{xu2016msr} dataset. We report results on the 1K-A test~\cite{yu2018joint}, which
contains 1K video and caption pairs. We use the standard recall metrics for evaluation and compare
with existing state-of-the-art methods in Table~\ref{tab:t2v_retrieval_results}. As we can see, these
two image-text pre-trained models CLIP\footnote{We use a public available CLIP checkpoint for
comparison}~\cite{radford2021learning} and \emph{Florence} outperform all the state-of-the-art
methods by a large margin in terms of the  metric. It reveals that the video data used for
pretraining in these state-of-the-art methods may not be so rich or diverse as image-text data used
in \emph{Florence} or CLIP.


\subsection{Video Action Recognition}

We evaluate \emph{Florence} on fine-tuned video action recognition tasks. On the Kinectics-400 and
Kinectics-600 datasets, we follow the typical fine-tuning setting~\cite{liu2021video} and fine tune
the model (Section~\ref{sect:adapt_video}) with  resolution for  epochs. We use
the label smoothing, rand augmentation, a small learning rate  and a relatively large drop
path rate  to avoid over-fitting the target video datasets. We compare with existing
state-of-the-art methods in Table~\ref{tab:k400}. Our results are better than the state-of-the-art
by  and  on Kinectics-400 and Kinectics-600, respectively.




\section{Conclusion and Future Work}

In this paper we investigated a new paradigm of building a computer vision foundation model, \emph{Florence}, as a general-purpose vision system. Our attempt is a step towards building XYZ-code \cite{XYZ-code}, an integrative AI system that makes progress toward human-like AI. Although the model size is still below several other existing billion-scale models, \emph{Florence} successfully extends to different tasks along space, time, and modality, with great transferbility, and achieves new SOTA results on a wide range of vision benchmarks.

For the future work, we plan to include more vision tasks and applications, such as depth/flow estimation, tracking, and additional vision+language tasks. \emph{Florence} is designed to pave the way for building vision foundation models to power millions of real-world vision tasks and applications. In addition, the preliminary progress on zero-shot classification and object detection may motivate more research to close the performance gap to supervised learning.

\noindent\paragraph{\textsc{Acknowledgment}}\mbox{}\\

We would like to thank the following people involved in the discussion for their valuable feedback including Xiaowei Hu, Yen-Chun Chen, Lin Liang, Yinpeng Chen, Li Dong, Furu Wei, Han Hu, Yue Cao, Zheng Zhang, Hao Yang, Jianmin Bao, Dong Chen, Fang Wen, Jianlong Fu, Houwen Peng, Chong Luo, Baining Guo. We would also thank Qingfen Lin, Cha Zhang for their thoughtful feedback on the broader impacts of the paper. Thanks Mei Gao, Ping Jin for helping run evaluations on benchmark infrastructure. We are also grateful to the developers
of software toolkits used throughout this project, including Liyang Lu, Robert Gmyr, Felipe Cruz Salinas, Canrun Li, Steven Tsai, Min Gao, Kevin Pan, Shohei Ono, Christina Sun. Additionally, we would like to thank the entire Deepspeed, AI Frameworks, and ITP teams for making it possible to train models at this scale.


\bibliography{egbib}
\bibliographystyle{icml2021}


\end{document}
