\subsection{Experimental Setup}
\textbf{Dataset.}
To demonstrate the effectiveness and generalization of our method, we perform experiments on human and animal datasets.
We conduct experiments on human datasets according to DPC's~\cite{lang2021dpc} scheme.
For the large-scale dataset, we randomly downsample the SURREAL~\cite{groueix20183d} dataset, which contains 230000 training shapes, into 2000 shape pairs as the training set.
For the test set, we use the SHREC'19~\cite{melzi2019shrec} dataset, which contains 44 real human models, and pair them into 430 annotated test examples.
To further verify the ability of our method to learn discriminative feature expression with a small data size, we train SE-ORNet on the pairs randomly sampled from 44 SHREC instances, and the testing is still conducted on the official 430 SHREC'19 pairs.


For animal datasets, we also conduct experiments with different dataset scales.
We use the large-scale SMAL~\cite{zuffi20173d} dataset and TOSCA~\cite{bronstein2008numerical} dataset as the training set and test set, respectively.
SMAL dataset consists of parameterized models of various animals, and we randomly sample SMAL to obtain the corresponding shape pairs as the training set.
TOSCA is generated by deforming three template meshes (human, dog, and horse) into multiple poses.
We pair the 41 animal figures in TOSCA from the same category to
form a training set of 260 samples and a test set of 286 samples. 
Because the number of points in different shapes varies, we make a random downsample of the original point cloud to a fixed number  , as done in  CorrNet3D~\cite{zeng2021corrnet3d}.


\textbf{Evaluation Metrics.}
The evaluation metrics include the average correspondence error and the correspondence accuracy.
Based on the Euclidean-based measure, the average correspondence error is defined for a pair of source and target shapes  as follows:

where  is the ground-truth matching point to . 
The unit is centimeter(cm).
And the correspondence accuracy can be formulated as:

where  is the indicator function,  is the maximal Euclidean distance between points in , and  is an error tolerance. 



\textbf{Implementation Details.}
For a fair comparison with existing methods~\cite{lang2021dpc, zeng2021corrnet3d},
we use the same DGCNN~\cite{wang2019dynamic} backbone with
four EdgeConv blocks as the feature extractor in the self-ensembling framework.
The standard deviation  in Equation~\eqref{Gaussian_noise} is set as 0.1 for human datasets and 0.15 for animal datasets.
In the Orientation Estimation Module,
the feature encoding module is a simplified DGCNN with
three EdgeConv~\cite{wang2019dynamic} blocks whose layer
output sizes are 64, 128, and 256.
The  of k-NN is set as 24,
and the slope of all LeakyReLU is 0.2.
We feed the output of the last layer into
the proposed feature interaction module
with the output size 256.
Then we refine the feature by an EdgeConv layer with the same output size.
The angle classification head consists of three Linear-BN-ReLU and an output Linear. 
The channels are 256, 128, and 128 for the three Linear-BN-ReLU.
The last Linear outputs the probability for classification.
We set the number of bins  as 8 in the angle classification head, where each bin represents a range of .
The domain discriminator is a PointNet-like module consisting of two MLPs in Equation~\eqref{PointNet}. 
The channels of  are 512, 256, and 128, while the channels of  are 256, 128, and 256.
We follow~\cite{lang2021dpc} and use a neighborhood size  in Equation~\eqref{construction} and~\eqref{regularization}. 
 and  in Equation~\eqref{construction_loss} are set as 1 and 10, respectively. , , , , and  in Equation~\eqref{overall_loss} are set as 0.1, 0.1, 1.0, 0.8, and 1.0, respectively.



\subsection{Comparison on Human Datasets}
For a fair comparison with existing methods, we do not use any post-processing or additional connectivity information.
In addition, we follow DPC~\cite{lang2021dpc} and train our model on the SURREAL and SHREC datasets, respectively. Then we test our model on the official 430 SHREC'19 pairs. 





\textbf{Evaluation on SHREC dataset.}
\begin{table}[!t]
  \begin{center}
    \footnotesize
    \setlength\tabcolsep{6pt}
\caption{\textbf{Comparison on SHREC and SURREAL benchmarks.} Here, acc means the correspondence accuracy at an error tolerance of 0.01, while err refers to the average correspondence error. Higher accuracy and lower error reflect a better result. 
}
    \label{table:Human_dataset}
    \vspace{-0.8em}
\begin{tabular}{c|c|cc|cc}
      \toprule
      \multirow{2}*{Method} & \multirow{2}*{Input} & \multicolumn{2}{c|}{SHREC} & \multicolumn{2}{c}{SURREAL} \\
      \cline{3-4}\cline{5-6}
      & & acc  & err  & acc  & err  \\
      \midrule
Diff-FMaps\cite{marin2020correspondence}  & Point & / & / & 4.0\% & 7.1 \\
      3D-CODED\cite{groueix20183d}              & Point & / & / & 2.1\% & 8.1 \\
      Elementary\cite{deprelle2019learning}     & Point & / & / & 2.3\% & 7.6 \\
      CorrNet3D\cite{zeng2021corrnet3d}         & Point & 0.4\%  & 33.8 & 6.0\% & 6.9  \\
      DPC\cite{lang2021dpc}                     & Point & 15.3\%  & 5.6 & 17.7\% & 6.1 \\
      \textbf{Ours}                             & Point & \textbf{17.5\%}  & \textbf{5.1} & \textbf{21.5\%} & \textbf{4.6} \\
      \bottomrule
    \end{tabular}
    \vspace{-2em}
  \end{center}
\end{table}
\begin{figure}[!t]
  \begin{center}
      \includegraphics[width=0.37\textwidth]{figure/human.png}
      \vspace{-1em}
      \caption{\textbf{Correspondence accuracy at various error tolerances for human datasets.} The methods are trained on the SHREC or SURREAL dataset and evaluated on SHREC'19 test pairs. Compared with other methods, our approach achieves an impressive performance improvement.}
      \vspace{-2.5em}
      \label{human}
  \end{center}
\end{figure}
As shown in Table~\ref{table:Human_dataset}, our approach shows significant performance improvements
on the SHREC benchmark and achieves new SOTA performance by 2.2\% improvements in accuracy and 0.5 reductions in error.
To show the improvement under different error tolerances, we present the correspondence accuracy for point-based methods trained on SHREC and evaluated on the SHREC'19 test set.
As shown in Figure~\ref{human}, our method achieves better results with different error tolerances. 


\textbf{Cross-dataset Generalization.}
In Table~\ref{table:Human_dataset} and Figure~\ref{human}, we also report the comparison with other methods on the SURREAL benchmark.
The models are trained on the SURREAL dataset and evaluated on the SHREC'19 test set.
The large-scale training set of the SURREAL dataset helps the deep learning-based methods perform better, even though there is a domain gap between the SURREAL and SHREC datasets.
With our proposed method, the correspondence accuracy reaches 21.5\% at an error tolerance of 0.01, and the average correspondence error is reduced to 4.6 on the SHREC'19 test set.


\subsection{Comparison on Animal Datasets}


To verify the adaptability of our method to point clouds of different shapes, we conduct experiments on two animal benchmarks.
Similar to human datasets, we train our method on TOSCA and SMAL datasets respectively, and test on the TOSCA test dataset.
Table~\ref{table:Animal_dataset} and Figure~\ref{animal} show the competitive results on TOSCA and SMAL benchmarks.
Our method achieves a 3.5\% accuracy improvement on the TOSCA benchmark and a 3.2\% accuracy improvement on the SMAL benchmark.
Compared with the human datasets, the animal datasets have various shapes with aligned orientations.
Thus, the above performance gains come mainly from our self-ensembling framework, which can learn reliable features on complex data.
To verify the effect of the Orientation Estimation Module, we test our method and baseline using different augmented test sets in Section~\ref{Robustness_Analysis}.

\begin{table}[!t]
  \begin{center}
    \footnotesize
    \setlength\tabcolsep{6pt}
\caption{\textbf{Comparison on TOSCA and SMAL benchmarks.} Here, acc means the correspondence accuracy at an error tolerance of 0.01, while err refers to the average correspondence error. Higher accuracy and lower error reflect a better result.}
    \label{table:Animal_dataset}
    \vspace{-0.8em}
\begin{tabular}{c|cc|cc}
      \toprule
      \multirow{2}*{Method}  & \multicolumn{2}{c|}{TOSCA} & \multicolumn{2}{c}{SMAL} \\
      \cline{2-3}\cline{4-5}
      & acc  & err  & acc  & err  \\
      \midrule
      3D-CODED\cite{groueix20183d}               & / & / & 0.5\% & 19.2 \\
      Elementary\cite{deprelle2019learning}      & / & / & 0.5\% & 13.7 \\
      CorrNet3D\cite{zeng2021corrnet3d}          & 0.3\%  & 32.7 & 5.3\% & 9.8  \\
      DPC\cite{lang2021dpc}                      & 34.7\%  & 2.8 & 33.2\% & 5.8 \\
      \textbf{Ours}                              & \textbf{38.2\%}  & \textbf{2.7} & \textbf{36.4\%} & \textbf{3.9} \\
      \bottomrule
    \end{tabular}
    \vspace{-2.5em}
  \end{center}
\end{table}
\begin{figure}[!t]
  \begin{center}
      \includegraphics[width=0.37\textwidth]{figure/animal.png}
      \vspace{-1em}
      \caption{\textbf{Correspondence accuracy at various error tolerances for animal datasets.} The methods are trained on the TOSCA or SMAL dataset and evaluated on the official TOSCA test pairs. Compared with other methods,  our method achieves a desirable performance improvement. 
      }
      \vspace{-2.5em}
      \label{animal}
  \end{center}
\end{figure}



\subsection{Comparison on Real-world Dataset}
CMU Panoptic~\cite{joo2015panoptic} is a dataset of scanned point clouds of human subjects in various poses, containing noise, outliers, occlusions, and clutter. 
Meanwhile, SHRECâ€™20~\cite{dyke2020shrec} dataset contains real scans of various four-legged animal models. 
As shown in Table~\ref{table:panoptic} and Table~\ref{table:shrec20}, we provide the results under two real scan datasets, demonstrating the remarkable robustness of our model to noise.

\begin{table}[!t]
  \begin{center}
    \footnotesize
    \setlength\tabcolsep{2.5pt}
\caption{\textbf{Comparison on CMU-Panoptic benchmark.} Here, err means average Euclidean keypoint error (cm). }
    \label{table:panoptic}
    \vspace{-1.3em}
\begin{tabular}{c|ccccc}
~  & 3D-CODED~\cite{groueix20183d} & DIF-Net~\cite{ren2018continuous} & CorrNet~\cite{zeng2021corrnet3d}  & IFMatch~\cite{sundararaman2022implicit} & \textbf{Ours} \\ \hline
      err & 17.1     & 15.3    & 14.8     & 8.5                                    & \textbf{3.2}  \\
\end{tabular}
\vspace{-1.5em}
  \end{center}
\end{table}

\begin{table}[!t]
  \begin{center}
    \footnotesize
    \setlength\tabcolsep{8pt}
    \vspace{-1.1em}
    \caption{
        \textbf{Comparison on SHREC'20 benchmark.} The training dataset is indicated in the bracket.
    }
    \label{table:shrec20}
    \vspace{-1.5em}
\begin{tabular}{c|cc|cc}
\multirow{2}*{Method} & \multicolumn{2}{c|}{SHREC'20 [SURREAL]} & \multicolumn{2}{c}{SHREC'20 [SMAL]}                                      \\
      \cline{2-3}\cline{4-5}
                            & acc                           & err                     & acc   & err  \\
\hline
      DPC\cite{lang2021dpc}                   & 25.0\%                                  & 3.2                                 & 24.5\%          & 7.5              \\
      \textbf{Ours}         & \textbf{29.9\%}                         & \textbf{1.2}                        & \textbf{25.4\%} & \textbf{2.9}     \\
\end{tabular}
    \vspace{-2.5em}
  \end{center}
\end{table}



\subsection{Ablation Study}

\begin{table}[!t]
  \begin{center}
    \footnotesize
    \setlength\tabcolsep{5pt}
    \centering
    \caption{\textbf{Evaluation of the model with different designs on SURREAL.}  is the consistency loss of the self-similarities,  is the consistency loss of the cross-similarities,  means the stochastic transform, OEM means we use the Orientation Estimation Module, FIM is the Feature Interaction Module, and DAM is the Domain Adaptation Module.}
    \vspace{-0.8em}
    \begin{tabular}{cccccc|cc}
      \toprule
\multirow{2}*{} &\multirow{2}*{} &  \multirow{2}*{} &   \multirow{2}*{OEM} & \multirow{2}*{FIM} & \multirow{2}*{DAM} & \multicolumn{2}{c}{SURREAL} \\
      \cline{7-8}
      & & & & & & acc  & err  \\
      \midrule
      \ding{55}&\ding{55}&\ding{55}&\ding{55}&\ding{55}&\ding{55}&17.7\%&6.1\\
\ding{51}&\ding{51}&\ding{55}&\ding{55}&\ding{55}&\ding{55}&18.8\%&5.7\\
\ding{51}&\ding{51}&\ding{51}&\ding{55}&\ding{55}&\ding{55}&19.2\%&5.6\\
\ding{51}&\ding{51}&\ding{51}&\ding{51}&\ding{55}&\ding{55}&19.5\%&5.5\\
\ding{51}&\ding{51}&\ding{51}&\ding{51}&\ding{51}&\ding{55}&20.4\%&5.1\\
      \ding{51}&\ding{51}&\ding{51}&\ding{51}&\ding{51}&\ding{51}&\textbf{21.5\%}&\textbf{4.6}\\
\bottomrule
    \end{tabular}
    \label{table:ablation}
    \vspace{-2.6em}
  \end{center}
\end{table}

\textbf{Evaluation of the model  with different designs.}
In this section, we perform extensive ablation studies on the SURREAL dataset to evaluate the effectiveness of each design.
Table~\ref{table:ablation} demonstrates the performance of the model with different designs.
Specifically, the first line is the results of DPC~\cite{lang2021dpc}, which is our baseline model. 
The second row indicates that the self-ensembling framework with a stochastic transform achieves a better performance than the original model.
By using  to constrain the consistency of source features before and after augmentation, the correspondence accuracy can be improved by 0.4\%, as shown in the third row.
As shown in the fourth row, adding the Orientation Evaluation Module without the Feature Interaction Module and the Domain Adaptation Module, the performance has a slight improvement.
In the fifth row, by introducing the Feature Interaction Module into the Orientation Evaluation Module, the correspondence accuracy can be improved by 0.9\%.
Finally, after utilizing the Domain Adaptation Module, the correspondence accuracy is improved by 1.1\%.
\begin{table}[!t]
\begin{center}
      \footnotesize
      \setlength\tabcolsep{6pt}
      \caption{\textbf{Effect of the Self-Ensembling Framework on SURREAL.} Here, we modify the self-ensembling framework to show the effect of each designed component.  means Gaussian noise, and  means random rotation along the vertical z-axis.}
      \label{table:Self-Ensemble}
      \vspace{-0.8em}
      \begin{tabular}{c|c|c|c|c}
          \toprule
          SURREAL &w/o & w/o & w/o &w/o \\
          \midrule
          acc  & 19.9\% & 20.3\% & 20.6\% &18.8\% \\
          err  & 5.3 & 5.1 & 4.9 &5.7 \\
          \bottomrule
      \end{tabular}
      \vspace{-2em}
  \end{center}
\end{table}

\begin{figure}[!t]
  \vspace{-0.5em}
  \begin{center}
      \includegraphics[width=0.36\textwidth]{figure/orientation_est.pdf}
      \vspace{-1.4em}
      \caption{\textbf{Effect of the Orientation Estimation Module.} We visualize of the point clouds before and after orientation rectification on SHREC'19 test set. We denote the raw source point cloud as Source and the rectified one as Source(R). Besides, we provide the probability distributions of the relative rotation angle prediction.}
      \vspace{-2.0em}
      \label{orientation_est}
  \end{center}
\end{figure}


\textbf{Effect of the Self-Ensembling Framework.}
As shown in Table~\ref{table:Self-Ensemble}, we modify the self-ensembling framework to show the effect of each designed component.
Removing either of the consistency losses leads to a drop in performance, which indicates that constraining the student network with soft labels facilitates the consistency of the point cloud representations. 
When Gaussian noise and rotation augmentation are removed, the model performances show different degrees of degradation. 
The above experiments illustrate our self-ensemble method can obtain a more robust feature representation of the point cloud through data augmentation and consistency losses.





\textbf{Effect of the Orientation Estimation Module.}
To verify the effect of the proposed Orientation Estimation Module, we visualize the point clouds after orientation rectification and the probability distributions of the relative rotation angle predictions.
As shown in Figure~\ref{orientation_est}, our method can accurately estimate the relative rotation angle and align the point cloud orientation of the source with that of the target.
More results refer to the supplementary
materials.


\subsection{Robustness Analysis}
\label{Robustness_Analysis}

\begin{table}[!t]
  \begin{center}
    \footnotesize
    \setlength\tabcolsep{2pt}
    \centering
    \caption{\textbf{Robustness Analysis.}
To verify the robustness of our method, we test our method and baselines using different augmented test sets.  means Gaussian noise with standard deviation  of 0.1 and  means random rotation along the vertical z-axis.}
    \vspace{-0.9em}
    \begin{tabular}{cc|cc|cc|cc|cc}
      \toprule
\multirow{2}*{} &  \multirow{2}*{} & \multicolumn{2}{c|}{SURREAL(B)} & \multicolumn{2}{c|}{SURREAL} & \multicolumn{2}{c|}{SMAL(B)} & \multicolumn{2}{c}{SMAL} \\
      \cline{3-10}
      & &acc  & err  & acc  & err  &acc  & err  & acc  & err \\
      \midrule
      \ding{55}&\ding{55}&17.47\%&6.30&21.55\%&4.65&33.79\%&5.78&36.39\%&3.88\\
\ding{51}&\ding{55}&14.38\%&8.74&21.55\%&4.66&30.21\%&6.06&36.15\%&3.92\\
\ding{55}&\ding{51}&8.99\%&9.33&17.59\%&5.81&9.98\%&12.24&28.16\%&5.63\\
\ding{51}&\ding{51}&7.80\%&11.28&17.58\%&5.79&9.54\%&12.80&27.60\%&5.89\\
\bottomrule
    \end{tabular}
    \label{table:robustness}
    \vspace{-2.5em}
  \end{center}
\end{table}

\begin{figure}[!t]
  \begin{center}
    \vspace{-1.8em}
      \includegraphics[width=0.36\textwidth]{figure/rotation_samples.pdf}
      \vspace{-0.8em}
      \caption{\textbf{Visualization of the correspondence results with rotation augmentation. }
With rotation augmentation on point cloud pairs, the baseline shows regrettable performances, while our method still retains desirable performances.
      }
      \vspace{-1.8em}
      \label{rotation_samples}
  \end{center}
\end{figure}

\begin{figure}[!t]
  \begin{center}
      \includegraphics[width=0.38\textwidth]{figure/real_dataset.pdf}
      \vspace{-1.8em}
      \caption{\textbf{Visualization of point cloud pairs with deformations on real scanned Owlii dataset.}
In each pair, the left one is the source and the right one is the target.
      }
      \vspace{-2.5em}
      \label{real_dataset}
  \end{center}
\end{figure}

To verify the robustness of our method, we use different augmentations on the test set. As shown in Table~\ref{table:robustness}, we use Gaussian noise and random rotation for data augmentation of the test sets. 
Compared to our method, the baseline shows a significant performance decrease for test sets with Gaussian noise.
For test sets with random rotation, the baseline performance is greatly degraded, while our method retains an acceptable performance.
Figure~\ref{rotation_samples} shows that our SE-ORNet can handle the orientation inconsistency issue of source and target well.
To further verify the robustness and generalization of our method, we conduct experiments on the real scanned Owlii dataset~\cite{xu2017owlii} and present the visualization in Figure~\ref{real_dataset}.
The results show that our SE-ORNet trained on the synthetic SURREAL dataset still produces impressive performance on the real scanned dataset, demonstrating strong generalization and robustness. 




