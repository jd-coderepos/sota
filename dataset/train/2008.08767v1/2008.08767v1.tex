

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{enumerate}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]
{geometry}
\usepackage[letterpaper=true,colorlinks,urlcolor=blue,citecolor=blue,linkcolor=red,bookmarks=false]{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subfigure}
\begin{document}




\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{1520}  

\title{ Single Image Super-Resolution via \\ a Holistic Attention Network}



\titlerunning{Single Image SR via a Holistic Attention Network}




\author{Ben Niu\inst{1,\footnotemark[1]} \and
	Weilei Wen\inst{2,3,\footnotemark[1]} \and
	Wenqi Ren\inst{3} \and
	Xiangde Zhang\inst{1} \and
	Lianping Yang\inst{1,\footnotemark[4]} \and
	Shuzhen Wang\inst{2} \and
	Kaihao Zhang\inst{5} \and
	Xiaochun Cao\inst{3,4} \and
	Haifeng Shen\inst{6}}

\authorrunning{B. Niu et al.}
\institute{Northeastern University ~
Xidian University ~
SKLOIS, IIE, CAS ~ \\
 Peng Cheng Laboratory, Cyberspace Security Research Center, China \\
ANU ~ AI Labs, Didi Chuxing, China}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contribution}
\footnotetext[4]{Corresponding author}
\maketitle

\begin{abstract}
	Informative features play a crucial role in the single image super-resolution task. Channel attention has been demonstrated to be effective for preserving information-rich features in each layer.
However, channel attention treats each convolution layer as a separate process that misses the correlation among different layers. 
To address this problem, we propose a new holistic attention network (HAN), which consists of a layer attention module (LAM) and a channel-spatial attention module (CSAM), to model the holistic interdependencies among layers, channels, and positions.
Specifically, the proposed LAM adaptively emphasizes hierarchical features by considering correlations among layers.
Meanwhile, CSAM learns the confidence at all the positions of each channel to selectively capture more informative features.
Extensive experiments demonstrate that the proposed HAN performs favorably against the state-of-the-art single image super-resolution approaches.


	\keywords{Super-Resolution, Holistic Attention, Layer Attention, Channel-Spatial Attention}
\end{abstract}


\section{Introduction}

Single image super-resolution (SISR) is an important task in computer vision and image processing. Given a low-resolution image, the goal of super-resolution (SR) is to generate a high-resolution (HR) image with necessary edge structures and texture details. 
The advance of SISR will immediately benefit many application fields, such as video surveillance and pedestrian detection. 


SRCNN~\cite{dong2014learning} is an unprecedented work to tackle the SR problem by learning the mapping function from LR input to HR output using convolutional neural networks (CNNs). Afterwards, numerous deep CNN-based methods~\cite{ren2019face,ren2018deep} have been proposed in recent years and generate a significant progress.
The superior reconstruction performance of CNNs based methods are
mainly from deep architecture and residual learning \cite{he2016deep}.
Networks with very deep layers have larger receptive fields and are able to provide a powerful capability to learn a complicated mapping between the LR input and the HR counterpart.
Due to the residual learning, the depth of the SR networks are going to deeper since residual learning could  efficiently alleviate the gradient vanishing and exploding problems.


Though significant progress have been made, we note that the texture details of the LR image often tend to be smoothed in the super-resolved result since most existing CNN-based SR methods neglect the feature correlation of intermediate layers.
Therefore, generating detailed textures is still a non-trivial problem in the SR task.
Although the results obtained by using channel attention \cite{zhang2018image,dai2019second} retain some detailed information, these channel attention-based approaches struggle in preserving informative textures
and restoring natural details since they treat the feature maps at different layers equally and result in lossing some detail parts in the reconstructed image. 

To address these problems, we present a novel approach termed as holistic attention network (HAN) that is capable of exploring the correlations among hierarchical layers, channels of each layer, and all positions of each channel. Therefore, HAN is able to stimulate the representational power of CNNs.
Specifically, we propose a layer attention module (LAM) and a channel-spatial attention module (CSAM) in the HAN for more powerful feature expression and correlation learning. 
These two sub-attention modules are inspired by channel attention \cite{zhang2018image} which weighs the internal features of each layer to make the network pay more attention to information-rich feature channels.
However, we notice that channel attention cannot weight the features from multi-scale layers. Especially the long-term information from the shallow layers are easily weakened. Although the shallow features can be recycled via skip connections, they are treated equally with deep features across layers after long skip connection, hence hindering the representational ability of CNNs.
To solve this problem, we consider exploring the interrelationship among features at hierarchical levels, and propose a layer attention module (LAM). 
On the other hand, channel attention neglects that the importance of different positions in each feature map varies significantly. Therefore, we also propose a channel-spatial attention module (CSAM) to collaboratively improve the discrimination ability of the proposed SR network.

Our contributions in this paper are summarized as follows:

\begin{enumerate}[]

	\item We propose a novel super-resolution algorithm named Holistic Attention Network (HAN), which enhances the representational ability of feature representations for super-resolution.


	\item We introduce a layer attention module (LAM) to learn the weights for hierarchical features by considering correlations of multi-scale layers.
	Meanwhile, a channel-spatial attention module (CSAM) is presented to learn the channel and spatial interdependencies of features in each layer. 

\item The proposed two attention modules collaboratively improve the SR results by modeling informative features among hierarchical layers, channels, and positions. Extensive experiments demonstrate that our algorithm performs favorably against the state-of-the-art SISR approaches.


\end{enumerate}




\section{Related Work}
Numerous algorithms and models have been proposed to solve the problem of image SR, which can be roughly divided into two categories. One is the traditional algorithm~\cite {yang2008image,huang2018robust,huang2015single}, the other one is the deep learning model based on neural network \cite{kim2016accurate,dong2016accelerating,lai2017deep,lim2017enhanced,zhang2018residual,kim2016deeply,tai2017image,tai2017memnet}. Due to the limitation of space, we only introduce the SR algorithms based on deep CNN.\par   
\textbf{Deep CNN for super-resolution.} Dong et al.~\cite{dong2014learning} proposed a CNN architecture named SRCNN, which was the pioneering work to apply deep learning to single image super-resolution. 
Since SRCNN successfully applied deep learning network to SR task, various efficient and deeper architectures have been proposed for SR.
Wang et al. \cite{wang2015deep}combined the domain knowledge of sparse coding with a
deep CNN and trained a cascade network to recover images progressively.
To alleviate the phenomenon of gradient explosion and reduce the complexity of the model, DRCN~\cite{kim2016deeply} and DRRN~\cite{tai2017image} were proposed by using a recursive convolutional network.
Lai et al.~\cite{lai2017deep} proposed a LapSR network which employs a pyramidal framework to progressively generate  images by three sub-networks. 
Lim et al.~\cite{lim2017enhanced} modified the ResNet~\cite{he2016deep} by removing batch normalization (BN) layers, which greatly improves the SR effect.


In addition to above MSE minimizing based methods, perceptual constraints are proposed to achieve better visual quality \cite{sajjadi2017enhancenet}. 
SRGAN~\cite{ledig2017photo} uses a generative adversarial networks (GAN) to predict high-resolution outputs by introducing a multi-task loss including a MSE loss, a perceptual loss~\cite{johnson2016perceptual}, and an adversarial loss~\cite{NIPS2014_5423}.
Zhang et al. \cite{zhang2019image} further transferred textures from reference images according to the textural similarity to enhance textures.
However, the aforementioned models either result in the loss of detailed textures in intermediate features due to the very deep depth, or produce some unpleasing artifacts or inauthentic textures. 
In contrast, we propose a holistic attention network consists of a layer attention and a channel-spatial attention to investigate the interaction of different layers, channels, and positions.




\textbf{Attention mechanism.} Attention mechanisms direct the operational focus of deep neural networks to areas where there is more information. In short, they help the network ignore irrelevant information and focus on important information \cite{hu2018squeeze,hu2019channel}. 
Recently, attention mechanism has been successfully applied into deep CNN based image enhancement methods. 
Zhang et al.~\cite{zhang2018image} proposed a residual channel attention network (RCAN) in which residual channel attention blocks (RCAB) allow the network to focus on the more informative channels. 
Woo et al.~\cite{woo2018cbam} proposed channel attention (CA) and spatial attention (SA) modules to exploit both inter-channel and inter-spatial relationship of feature maps. 
Kim et al.~\cite{kim2018ram} introduced a residual attention module for SR which is composed of residual blocks and spatial channel attention for learning the inter-channel and intra-channel correlations. More recently, Dai et al.~\cite{dai2019second} presented a second-order channel attention (SOCA) module to adaptively refine features using second-order feature statistics.
\begin{figure}[t]\footnotesize
	\begin{center}
		\includegraphics[width = 0.8\textwidth]{img/network/Holistic_Attention_Network.pdf}
	\end{center}


	\caption{Network architecture of the proposed holistic attention network(HAN). Given a low-resolution image, the first convolutional layer of the HAN extracts a set of shallow feature maps. Then a series of residual groups further extract deeper feature representations of the low-resolution input. We propose a layer attention module (LAM) to learn the correlations of each output from RGs and a channel-spatial attention module (CSAM) to investigate the interdependencies between channels and pixels. Finally, an upsampling block produces the high-resolution image
	}

	\label{fig-Holistic_Attention_Network}
\end{figure}

However, these attention based methods only consider the channel and spatial correlations while ignore the interdependencies between multi-scale layers. To solve this problem, we propose a layer attention module (LAM) to
exploit the nonlinear feature interactions among hierarchical layers.  





\section{Holistic Attention Network (HAN) for SR}
\label{sec-att}
In this section, we first present the overview of HAN network for SISR. Then we give the detailed configurations of the proposed layer attention module (LAM) and channel-spatial attention module (CSAM).  




\subsection{Network Architecture}
As shown in Figure \ref{fig-Holistic_Attention_Network}, our proposed HAN consists of four parts: feature extraction, layer attention module, channel-spatial attention module, and the final reconstruction block.


\textbf{Features extraction.} Given a LR input , a convolutional layer is used to extract the shallow feature  of the LR input

Then we use the backbone of the RCAN \cite{zhang2018image} to extract the intermediate features  of the LR input

where  represents the -th residual group (RG) in the RCAN,  is the number of the residual groups. Therefore, except  is the final output of RCAN network backbone, all other feature maps are intermediate outputs.



\textbf{Holistic attention.} After extracting hierarchical features  by a set of residual groups, we further conduct a holistic feature weighting, which includes: \textit{i}) layer attention of hierarchical features, and \textit{ii}) channel-spatial attention of the last layer of RCAN. 

The proposed layer attention makes full use of features from all the preceding layers and can be represented as

where  represents the LAM which learns the feature correlation matrix of all the features from RGs' output and then weights the fused intermediate features  capitalized on the correlation matrix (see Section \ref{sec-lam}). As a results, LAM enables the high contribution feature layers to be enhanced and the redundant ones to be suppressed.


	In addition, channel-spatial attention aims to modulate features for adaptively capturing more important information of inter-channel and intra-channel for the final reconstruction, which can be written as

\noindent where  represents the CSAM to produce channel-spatial attention for discriminately abtaining feature information,  denotes the filtered features after channel-spatial attention (details can be found in Section \ref{sec-csam}). 
Although we can filter all the intermediate features of  using CSAM, we only modulate the last feature layer of  as a trade-off between accuracy and speed.







\textbf{Image reconstruction.} After obtaining features from both LAM and CSAM, we integrate the layer attention and channel-spatial attention units by element-wise summation.
Then, we employ the sub-pixel convolution \cite{shi2016real} as the last upsampling module,
which converts the scale sampling with a given magnification factor
by pixel translation.  We perform the sub-pixel convolution operation to aggregate low-resolution feature maps and simultaneously impose projection to high dimensional space to reconstruct the HR image. We formulate the process as follows

where  represents the operation of sub-pixel convolution, and  is the reconstructed SR result.
The long skip connection is introduced in HAN to stabilize the training of the proposed deep network, \textit{i.e.,} the sub-pixel upsampling block takes  as input.




\begin{figure*}[t]\footnotesize
	\begin{center}
		\includegraphics[width = 1\textwidth]{img/attention/LAM.pdf}
	\end{center}


	\caption{Architecture of the proposed layer attention module
}

	\label{fig-Layer_attention}
\end{figure*}


\textbf{Loss function.} Since we employ the RCAN network as the backbone of the proposed method, only  distance is selected as our loss function as in \cite{zhang2018image} for a fair comparison

where , , and  denote the function of the proposed HAN, the learned parameter of the HAN, and the number of training pairs, respectively.
	Note that we do not use other sophisticated loss functions such as adversarial loss \cite{NIPS2014_5423} and perceptual loss~\cite{johnson2016perceptual}. We show that simply using the
	naive image intensity loss  can already achieve
	competitive results as demonstrated in Section \ref{sec-exp}.



\subsection{Layer Attention Module}
\label{sec-lam}
Although dense connections~\cite{huang2017densely} and skip connections \cite{he2016deep} allow shallow information to be bypassed to deep layers, 
these operations do not exploit interdependencies between the different layers. 
In contrast, we treat the feature maps from each layer as a response to a specific class, and the responses from different layers are related to each other. 
By obtaining the dependencies between features of different depths, the network can allocate different attention weights to features of different depths and automatically improve the representation ability of extracted features. Therefore, we propose an innovative LAM that learns the relationship between features of different depths, which automatically improve the feature representation ability.


The structure of the proposed layer attention is shown in Figure \ref{fig-Layer_attention}. The input of the module is the extracted intermediate feature groups s, with the dimension of , from  residual groups. 
Then, we reshape the feature groups s into a 2D matrix with the dimension of , and apply matrix multiplication with the corresponding transpose
to calculate the correlation  between different layers

\begin{figure*}[t]\footnotesize
	\begin{center}
		\includegraphics[width = 0.8\textwidth]{img/attention/CSAM.pdf}
	\end{center}


	\caption{Architecture of the proposed channel-spatial attention module}


	\label{fig-CSAM}
\end{figure*}
where  and  denote the softmax and reshape operations,  represents the correlation index between -th and -th feature groups. 
Finally, we multiply the reshaped feature groups s by the predicted correlation matrix with a scale factor , and add the input features 

where  is initialized to  and is automatically assigned by the network in the following epochs. 
As a result, the weighted sum of features allow the main parts of network to focus on more informative layers of the intermediate LR features.





\subsection{Channel-Spatial Attention}
\label{sec-csam}
The existing spatial attention mechanisms~\cite{woo2018cbam,kim2018ram} mainly focuse on the scale dimension of the feature, with little uptake of channel dimension information, while the recent channel attention mechanisms~\cite{zhang2018image,zhang2018residual,dai2019second} ignore the scale information. 
To solve this problem, we propose a novel channel-spatial attention mechanism (CSAM) that contains responses from all dimensions of the feature maps. Note that although we can perform the CSAM for all the feature groups s extracted from RCAN, we only modulate the last feature group of  for a trade-off between accuracy and speed as shown in Figure \ref{fig-Holistic_Attention_Network}.




The architecture of the proposed CSAM is shown in Figure \ref{fig-CSAM}. 
Given the last layer feature maps , 
we feed  to a 3D convolution layer~\cite{ji20123d} to generate attention map by capturing joint
channel and spatial features.
We operate the 3D convolution via convolving 3D kernels with the cube constructed from multiple neighboring channels of . Specifically, we perform 3D convolutions with kernel size of  with step size of 1 (\textit{i.e.,} three groups of consecutive channels are convolved with a set of 3D kernels respectively), resulting
in three groups of channel-spatial attention maps .
By doing so, our CSAM can extract powerful representations to describe inter-channel and intra-channel information in continuous channels.

In addition, we perform element-wise multiplication with the attention map  and the input feature . Finally, multiply the weighted result by a scale factor , and then add the input feature  to obtain the weighted features

where  is the sigmoid function,  is the element-wise product, the scale factor  is initialized as 0 and progressively improved in the follow iterations. 
As a results,  is the weighted sum of all channel-spatial position features as well as the original features. Compared with conventional spatial attention and channel attention, our CSAM adaptively learns the inter-channel and intra-channel feature responses by explicitly modelling channel-wise and spatial feature interdependencies.









\begin{figure}[t]\tiny
	\begin{center}
		\tabcolsep 1pt
		\begin{tabular}{@{}ccccccccc@{}}
			HR&
			Bicubic & 
			VDSR\cite{kim2016accurate} & 
			EDSR\cite{lim2017enhanced} & 
			RDN\cite{zhang2018residual} & 
			RCAN\cite{zhang2018image}  &
			SRFBN\cite{li2019feedback} &
			SAN\cite{dai2019second} &
			HAN(our) \\
			
			
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/HR.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/BI.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/VD.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/ED.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/RD.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/RC.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/SR.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/SA.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/HA.jpg} \\ 


			\includegraphics[width = 0.1\textwidth]{results/test/img_002/HR_crop.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/BI_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/VD_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/ED_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/RD_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/RC_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/SR_crop.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/SA_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_002/HA_crop.jpg} \\


			PSNR/SSIM & 24.18/0.678  & 25.63/0.763 & 27.66/0.849 & 27.12/0.832 & 27.95/0.857 & 27.43/0.843 & 27.99/0.857 & \textbf{28.05}/\textbf{0.859} \\	


			\includegraphics[width = 0.1\textwidth]{results/test/img_003/HR.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/BI.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/VD.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/ED.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/RD.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/RC.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/SR.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/SA.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/HA.jpg} \\ 


			\includegraphics[width = 0.1\textwidth]{results/test/img_003/HR_crop.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/BI_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/VD_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/ED_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/RD_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/RC_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/SR_crop.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/SA_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_003/HA_crop.jpg} \\ 


			PSNR/SSIM & 22.97/0.636 &24.59/0.741 & 24.00/0.695 &24.00/0.698 &24.26/ \textbf{0.711} &24.13/0.706 &24.20/0.709 & \textbf{24.87}/ 0.710 \\
			
			
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/HR.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/BI.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/VD.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/ED.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/RD.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/RC.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/SR.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/SA.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/HA.jpg}\\ 


			\includegraphics[width = 0.1\textwidth]{results/test/img_078/HR_crop.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/BI_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/VD_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/ED_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/RD_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/RC_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/SR_crop.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/SA_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_078/HA_crop.jpg} \\ 
PSNR/SSIM & 25.71/0.680 &26.62/0.725 & 27.96/0.795 &27.53/0.782 &28.63/0.805 &27.74/0.789 &28.40/0.800 &\textbf{28.67}/\textbf{0.805} \\
			
			
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/HR.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/BI.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/VD.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/ED.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/RD.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/RC.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/SR.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/SA.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/HA.jpg} \\ 


			\includegraphics[width = 0.1\textwidth]{results/test/img_096/HR_crop.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/BI_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/VD_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/ED_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/RD_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/RC_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/SR_crop.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/SA_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/img_096/HA_crop.jpg} \\ 
PSNR/SSIM & 21.32/0.686 &23.07/0.783 &26.33/0.895 &25.62/0.880 & 26.46/0.897  &26.57/0.897 &26.87/0.900 & \textbf{26.98}/\textbf{0.900} \\	
			


			


		\end{tabular}
	\end{center}
	
	\caption{Visual comparison for 4 SR with BI degradation model on the Urban100 datasets. The best results are highlighted. Our method obtains better visual quality and recovers more image details compared with other state-of-the-art SR methods
	}

	\label{fig-BI}
\end{figure}

\begin{table}[t]\scriptsize
	\tabcolsep 6pt
	\caption{Effectiveness of the proposed LAM and CSAM for image super-resolution}
	
	\begin{center}
		\begin{tabular}{ccccc}
			\toprule
			& baseline & w/o CSAM & w/o LAM & Ours \\
			\midrule
			PSNR/SSIM  & 31.22/0.9173  & 31.38/0.9175  & 31.28/0.9174 & \textbf{31.42}/\textbf{0.9177} \\
			\bottomrule
		\end{tabular}
		\label{tab-psnr-ssim}
	\end{center}
	
\end{table}
\begin{table}[!t]\scriptsize
	
	\tabcolsep 10pt
	\caption{Ablation study about using different numbers of RGs}
	
	\begin{center}\scriptsize{
			\begin{tabular}{cccccc}
				\toprule
				&Set5 & Set14 & B100 & Urban100 & Manga100 \\
				\midrule
				RCAN       &32.63  & 28.87   & 27.77  & 26.82  &31.22   \\
				HAN 3RGs      &32.63  & 28.89   & 27.79  & 26.82  &31.40   \\
				HAN 6RGs      &\textbf{32.64}  & \textbf{28.90}    & 27.79  & 26.84  &\textbf{31.42}   \\
				HAN 10RGs      &\textbf{32.64}  & \textbf{28.90}   & \textbf{27.80}  & \textbf{26.85}  &\textbf{31.42}   \\
				\bottomrule
		\end{tabular}}
		\label{tab4}
	\end{center} 
\end{table}

\begin{table}[!ht]
	\scriptsize 
	\centering
	\caption{Quantitative results with BI degradation model. The best and second best results are highlighted in \textbf{bold} and \underline{underlined}}
	\begin{tabular}{|p{7em}|p{2.5em}|p{2.5em}|p{3em}|p{2.5em}|p{3em}|p{2.5em}|p{3em}|p{2.5em}|p{3em}|p{2.5em}|p{3em}|} 
		\hline
		\multicolumn{1}{|c|}{\multirow{2}{*}{{ Methods} }} & \multicolumn{1}{c|}{\multirow{2}{*}{Scale}} & \multicolumn{2}{c}{Set5} & \multicolumn{2}{c}{Set14} & \multicolumn{2}{c}{B100} & \multicolumn{2}{c}{Urban100} & \multicolumn{2}{c|}{ Manga109} \\
		\cline{3-12}   \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} &  PSNR  & SSIM  & PSNR   & SSIM  & PSNR  & SSIM  & PSNR  & SSIM  & PSNR  & SSIM \\
		\hline
		
		Bicubic \newline{}SRCNN~\cite{dong2014learning} \newline{} FSRCNN~\cite{dong2016accelerating} \newline{} VDSR~\cite{kim2016accurate} \newline{} LapSRN~\cite{lai2017deep} \newline{} MemNet~\cite{tai2017memnet} \newline{} EDSR~\cite{lim2017enhanced} \newline{} SRMDNF~\cite{zhang2018learning}  \newline{} D-DBPN~\cite{haris2018deep} \newline{} RDN~\cite{zhang2018residual} \newline{} RCAN~\cite{zhang2018image} \newline{} SRFBN~\cite{li2019feedback} \newline{} SAN~\cite{dai2019second} \newline{} HAN(ours) \newline{} HAN+(ours) &
		 \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{}  \newline{}  \newline{}  \newline{}  & 33.66 \newline{}36.66 \newline{}37.05 \newline{}37.53 \newline{}37.52 \newline{}37.78 \newline{}38.11 \newline{}37.79 \newline{}38.09 \newline{}38.24 \newline{}38.27 \newline{}38.11 \newline{}\underline{38.31} \newline{}38.27 \newline{}\bfseries{38.33} & 0.9299 \newline{}0.9542 \newline{}0.9560 \newline{}0.9590 \newline{}0.9591 \newline{}0.9597 \newline{}0.9602 \newline{}0.9601 \newline{}0.9600 \newline{}0.9614 \newline{}0.9614 \newline{}0.9609 \newline{}\textbf{0.9620} \newline{}0.9614 \newline{}\underline{0.9617}  & 30.24 \newline{}32.45 \newline{}32.66 \newline{}33.05 \newline{}33.08 \newline{}33.28 \newline{}33.92 \newline{}33.32 \newline{}33.85 \newline{}34.01 \newline{}34.12 \newline{}33.82 \newline{}34.07 \newline{}\underline{34.16} \newline{}\bfseries{34.24}  & 0.8688 \newline{}0.9067 \newline{}0.9090 \newline{}0.9130 \newline{}0.9130 \newline{}0.9142 \newline{}0.9195 \newline{}0.9159 \newline{}0.9190 \newline{}0.9212 \newline{}0.9216 \newline{}0.9196 \newline{}0.9213 \newline{}\underline{0.9217} \newline{}\bfseries{0.9224} & 29.56 \newline{}31.36 \newline{}31.53 \newline{}31.90 \newline{}31.08 \newline{}32.08 \newline{}32.32 \newline{}32.05 \newline{}32.27 \newline{}32.34 \newline{}32.41 \newline{}32.29 \newline{}\underline{32.42} \newline{}32.41 \newline{}\bfseries{32.45} & 0.8431 \newline{}0.8879 \newline{}0.8920 \newline{}0.8960 \newline{}0.8950 \newline{}0.8978 \newline{}0.9013 \newline{}0.8985 \newline{}0.9000 \newline{}0.9017 \newline{}0.9027 \newline{}0.9010 \newline{}\underline{0.9028} \newline{}0.9027 \newline{}\bfseries{0.9030} & 26.88 \newline{}29.50 \newline{}29.88 \newline{}30.77 \newline{}30.41 \newline{}31.31 \newline{}32.93 \newline{}31.33 \newline{}32.55 \newline{}32.89 \newline{}33.34 \newline{}32.62 \newline{}33.10 \newline{}\underline{33.35} \newline{}\bfseries{33.53} & 0.8403 \newline{}0.8946 \newline{}0.9020 \newline{}0.9140 \newline{}0.9101 \newline{}0.9195 \newline{}0.9351 \newline{}0.9204 \newline{}0.9324 \newline{}0.9353 \newline{}0.9384 \newline{}0.9328\newline{}0.9370 \newline{}\underline{0.9385} \newline{}\bfseries{0.9398}  & 30.80 \newline{}35.60 \newline{}36.67 \newline{}37.22 \newline{}37.27 \newline{}37.72 \newline{}39.10 \newline{}38.07 \newline{}38.89 \newline{}39.18 \newline{}39.44 \newline{}39.08 \newline{}39.32 \newline{}\underline{39.46} \newline{}\bfseries{39.62} & 0.9339 \newline{}0.9663 \newline{}0.9710 \newline{}0.9750 \newline{}0.9740 \newline{}0.9740 \newline{}0.9773 \newline{}0.9761 \newline{}0.9775 \newline{}0.9780 \newline{}\underline{0.9786} \newline{}0.9779 \newline{}0.9792 \newline{}0.9785 \newline{}\bfseries{0.9787} \\
		\hline
		\hline
		Bicubic \newline{}SRCNN~\cite{dong2014learning} \newline{} FSRCNN~\cite{dong2016accelerating} \newline{} VDSR~\cite{kim2016accurate} \newline{} LapSRN~\cite{lai2017deep} \newline{} MemNet~\cite{tai2017memnet} \newline{} EDSR~\cite{lim2017enhanced} \newline{} SRMDNF~\cite{zhang2018learning}  \newline{} RDN~\cite{zhang2018residual} \newline{} RCAN~\cite{zhang2018image} \newline{} SRFBN~\cite{li2019feedback} \newline{} SAN~\cite{dai2019second} \newline{} HAN(ours) \newline{} HAN+(ours)&
		 \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} & 30.39 \newline{}32.75 \newline{}33.18 \newline{}33.67 \newline{}33.82 \newline{}34.09 \newline{}34.65 \newline{}34.12 \newline{}34.71 \newline{}34.74 \newline{}34.70 \newline{}34.75 \newline{}\underline{34.75} \newline{}\bfseries{34.85} & 0.8682 \newline{}0.9090 \newline{}0.9140 \newline{}0.9210 \newline{}0.9227 \newline{}0.9248 \newline{}0.9280 \newline{}0.9254 \newline{}0.9296 \newline{}0.9299 \newline{}0.9292 \newline{}\underline{0.9300} \newline{}0.9299 \newline{}\bfseries{0.9305} & 27.55 \newline{}29.30 \newline{}29.37 \newline{}29.78 \newline{}29.87 \newline{}30.00 \newline{}30.52 \newline{}30.04 \newline{}30.57 \newline{}30.65 \newline{}30.51 \newline{}30.59 \newline{}\underline{30.67} \newline{}\bfseries{30.77} & 0.7742 \newline{}0.8215 \newline{}0.8240 \newline{}0.8320 \newline{}0.8320 \newline{}0.8350 \newline{}0.8462 \newline{}0.8382 \newline{}0.8468 \newline{}0.8482 \newline{}0.8461 \newline{}0.8476 \newline{}\underline{0.8483} \newline{}\bfseries{0.8495} & 27.21 \newline{}28.41 \newline{}28.53 \newline{}28.83 \newline{}28.82 \newline{}28.96 \newline{}29.25 \newline{}28.97 \newline{}29.26 \newline{}29.32 \newline{}29.24 \newline{}\underline{29.33} \newline{}29.32 \newline{}\bfseries{29.39}&  0.7385 \newline{}0.7863 \newline{}0.7910 \newline{}0.7990 \newline{}0.7980 \newline{}0.8001 \newline{}0.8093 \newline{}0.8025 \newline{}0.8093 \newline{}0.8111 \newline{}0.8084 \newline{}\underline{0.8112} \newline{}0.8110 \newline{}\bfseries{0.8120} & 24.46 \newline{}26.24 \newline{}26.43 \newline{}27.14 \newline{}27.07 \newline{}27.56 \newline{}28.80 \newline{}27.57 \newline{}28.80 \newline{}29.09 \newline{}28.73 \newline{}28.93 \newline{}\underline{29.10} \newline{}\bfseries{29.30} & 0.7349 \newline{}0.7989 \newline{}0.8080 \newline{}0.8290 \newline{}0.8280 \newline{}0.8376 \newline{}0.8653 \newline{}0.8398 \newline{}0.8653 \newline{}0.8702 \newline{}0.8641 \newline{}0.8671 \newline{}\underline{0.8705} \newline{}\bfseries{0.8735} & 26.95 \newline{}30.48 \newline{}31.10 \newline{}32.01 \newline{}32.21 \newline{}32.51 \newline{}34.17 \newline{}33.00 \newline{}34.13 \newline{}34.44 \newline{}34.18 \newline{}34.30 \newline{}\underline{34.48} \newline{}\bfseries{34.80} & 0.8556 \newline{}0.9117 \newline{}0.9210 \newline{}0.9340 \newline{}0.9350 \newline{}0.9369 \newline{}0.9476 \newline{}0.9403 \newline{}0.9484 \newline{}0.9499 \newline{}0.9481 \newline{}0.9494 \newline{}\underline{0.9500} \newline{}\bfseries{0.9514}\\
		\hline
		\hline
		Bicubic \newline{}SRCNN~\cite{dong2014learning} \newline{} FSRCNN~\cite{dong2016accelerating} \newline{} VDSR~\cite{kim2016accurate} \newline{} LapSRN~\cite{lai2017deep} \newline{} MemNet~\cite{tai2017memnet} \newline{} EDSR~\cite{lim2017enhanced} \newline{} SRMDNF~\cite{zhang2018learning}  \newline{} D-DBPN~\cite{haris2018deep} \newline{} RDN~\cite{zhang2018residual} \newline{} RCAN~\cite{zhang2018image} \newline{} SRFBN~\cite{li2019feedback} \newline{} SAN~\cite{dai2019second} \newline{} HAN(ours) \newline{} HAN+(ours) &
		 \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{}  & 28.42 \newline{}30.48 \newline{}30.72 \newline{}31.35 \newline{}31.54 \newline{}31.74 \newline{}32.46 \newline{}31.96 \newline{}32.47 \newline{}32.47 \newline{}32.63 \newline{}32.47 \newline{}32.64 \newline{}\underline{32.64} \newline{}\bfseries{32.75} & 0.8104 \newline{}0.8628 \newline{}0.8660 \newline{}0.8830 \newline{}0.8850 \newline{}0.8893 \newline{}0.8968 \newline{}0.8925 \newline{}0.8980 \newline{}0.8990 \newline{}0.9002 \newline{}0.8983 \newline{}\underline{0.9003} \newline{}0.9002 \newline{}\bfseries{0.9016} & 26.00 \newline{}27.50 \newline{}27.61 \newline{}28.02 \newline{}28.19 \newline{}28.26 \newline{}28.80 \newline{}28.35 \newline{}28.82 \newline{}28.81 \newline{}28.87 \newline{}28.81 \newline{}\underline{28.92} \newline{}28.90 \newline{}\bfseries{28.99} & 0.7027 \newline{}0.7513 \newline{}0.7550 \newline{}0.7680 \newline{}0.7720 \newline{}0.7723 \newline{}0.7876 \newline{}0.7787 \newline{}0.7860 \newline{}0.7871 \newline{}0.7889 \newline{}0.7868 \newline{}0.7888 \newline{}\underline{0.7890} \newline{}\bfseries{0.7907} & 25.96 \newline{}26.90 \newline{}26.98 \newline{}27.29 \newline{}27.32 \newline{}27.40 \newline{}27.71 \newline{}27.49 \newline{}27.72 \newline{}27.72 \newline{}27.77 \newline{}27.72 \newline{}27.78 \newline{}\underline{27.80} \newline{}\bfseries{27.85} & 0.6675 \newline{}0.7101 \newline{}0.7150 \newline{}0.0726 \newline{}0.7270 \newline{}0.7281 \newline{}0.7420 \newline{}0.7337 \newline{}0.7400 \newline{}0.7419 \newline{}0.7436 \newline{}0.7409 \newline{}0.7436 \newline{}\underline{0.7442} \newline{}\bfseries{0.7454} & 23.14 \newline{}24.52 \newline{}24.62 \newline{}25.18 \newline{}25.21 \newline{}25.50 \newline{}26.64 \newline{}25.68 \newline{}26.38 \newline{}26.61 \newline{}26.82 \newline{}26.60 \newline{}26.79 \newline{}\underline{26.85} \newline{}\bfseries{27.02} & 0.6577 \newline{}0.7221 \newline{}0.7280 \newline{}0.7540 \newline{}0.7560 \newline{}0.7630 \newline{}0.8033 \newline{}0.7731 \newline{}0.7946 \newline{}0.8028 \newline{}0.8087 \newline{}0.8015 \newline{}0.8068 \newline{}\underline{0.8094} \newline{}\bfseries{0.8131} & 24.89 \newline{}27.58 \newline{}27.90 \newline{}28.83 \newline{}29.09 \newline{}29.42 \newline{}31.02 \newline{}30.09 \newline{}30.91 \newline{}31.00 \newline{}31.22 \newline{}31.15 \newline{}31.18 \newline{}\underline{31.42} \newline{}\bfseries{31.73} & 0.7866\newline{} 0.8555\newline{} 0.8610\newline{} 0.8870\newline{} 0.8900\newline{} 0.8942\newline{} 0.9148\newline{} 0.9024\newline{} 0.9137\newline{} 0.9151\newline{} 0.9173 \newline{}0.9160 \newline{}0.9169 \newline{}\underline{0.9177} \newline{}\bfseries{0.9207} \\
		\hline
		\hline
		Bicubic \newline{}SRCNN~\cite{dong2014learning} \newline{} FSRCNN~\cite{dong2016accelerating} \newline{} SCN~\cite{wang2015deep} \newline{} VDSR~\cite{kim2016accurate} \newline{} LapSRN~\cite{lai2017deep} \newline{} MemNet~\cite{tai2017memnet} \newline{} MSLapSRN\cite{lai2017deep} \newline{} EDSR~\cite{lim2017enhanced} \newline{}  D-DBPN~\cite{haris2018deep} \newline{}  RCAN~\cite{zhang2018image} \newline{}  SAN~\cite{dai2019second} \newline{} HAN(ours) \newline{} HAN+(ours) &
		 \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{}  & 24.40 \newline{}25.33 \newline{}20.13 \newline{}25.59 \newline{}25.93 \newline{}26.15 \newline{}26.16 \newline{}26.34 \newline{}26.96 \newline{}27.21 \newline{}27.31 \newline{}27.22 \newline{}\underline{27.33} \newline{}\bfseries{27.47}& 0.6580 \newline{}0.6900 \newline{}0.5520 \newline{}0.7071 \newline{}0.7240 \newline{}0.7380 \newline{}0.7414 \newline{}0.7558 \newline{}0.7762 \newline{}0.7840 \newline{}0.7878 \newline{}0.7829 \newline{}\underline{0.7884} \newline{}\bfseries{0.7920} & 23.10 \newline{}23.76 \newline{}19.75 \newline{}24.02 \newline{}24.26 \newline{}24.35 \newline{}24.38 \newline{}24.57 \newline{}24.91 \newline{}25.13 \newline{}25.23 \newline{}25.14 \newline{}\underline{25.24} \newline{}\bfseries{25.39} & 0.5660 \newline{}0.5910 \newline{}0.4820 \newline{}0.6028 \newline{}0.6140 \newline{}0.6200 \newline{}0.6199 \newline{}0.6273 \newline{}0.6420 \newline{}0.6480 \newline{}\underline{0.6511} \newline{}0.6476 \newline{}0.6510 \newline{}\bfseries{0.6552} & 23.67 \newline{}24.13 \newline{}24.21 \newline{}24.30 \newline{}24.49 \newline{}24.54 \newline{}24.58 \newline{}24.65 \newline{}24.81 \newline{}24.88 \newline{}24.98 \newline{}24.88 \newline{}\underline{24.98} \newline{}\bfseries{25.04} & 0.5480 \newline{}0.5660 \newline{}0.5680 \newline{}0.5698 \newline{}0.5830 \newline{}0.5860 \newline{}0.5842 \newline{}0.5895 \newline{}0.5985 \newline{}0.6010 \newline{}0.6058 \newline{}0.6011 \newline{}\underline{0.6059} \newline{}\bfseries{0.6075} & 20.74 \newline{}21.29 \newline{}21.32 \newline{}21.52 \newline{}21.70 \newline{}21.81 \newline{}21.89 \newline{}22.06 \newline{}22.51 \newline{}22.73 \newline{}\underline{23.00} \newline{}22.70 \newline{}22.98 \newline{}\bfseries{23.20} & 0.5160 \newline{}0.5440 \newline{}0.5380 \newline{}0.5571 \newline{}0.5710 \newline{}0.5810 \newline{}0.5825 \newline{}0.5963 \newline{}0.6221 \newline{}0.6312 \newline{}\underline{0.6452} \newline{}0.6314 \newline{}0.6437 \newline{}\bfseries{0.6518} & 21.47 \newline{}22.46 \newline{}22.39 \newline{}22.68 \newline{}23.16 \newline{}23.39 \newline{}23.56 \newline{}23.90 \newline{}24.69 \newline{}25.14 \newline{}\underline{25.24} \newline{}24.85 \newline{}25.20 \newline{}\bfseries{25.54} & 0.6500\newline{} 0.6950\newline{} 0.6730\newline{} 0.6963\newline{} 0.7250\newline{} 0.7350\newline{} 0.7387\newline{} 0.7564\newline{} 0.7841\newline{} 0.7987\newline{} \underline{0.8029} \newline{}0.7906 \newline{}0.8011 \newline{}\bfseries{0.8080}  \\
		\hline
	\end{tabular}\label{table-BI}\end{table}











\section{Experiments}
\label{sec-exp}
In this section, we first analyze the contributions of the proposed two attention modules. We then compare our HAN with state-of-the-art algorithms on five benchmark datasets. 
The implementation code will be made available to the public. Results on more images can be found in the supplementary material.

\subsection{Settings}
\noindent\textbf{Datasets.} We selecte DIV2K~\cite{timofte2017ntire} as the training set as like in \cite{zhang2018image,dai2019second,zhang2018residual,lim2017enhanced}.
For the testing set, we choose five standard datasets: Set5~\cite{bevilacqua2012low}, Set14~\cite{zeyde2010single}, B100~\cite{martin2001database}, Urban100~\cite{huang2015single}, and Manga109~\cite{matsui2017sketch}. Degraded data was obtained by bilinear interpolation and Blur-downscale Degradation model. 
Following \cite{zhang2018image}, the reconstruct RGB results by the proposed HAN are first converted to YCbCr space, and then we only consider the luminance channel to calculate PSNR and SSIM in our experiments.






\begin{figure}[t]\tiny
	\begin{center}
		\tabcolsep 1pt
		\begin{tabular}{@{}ccccccccc@{}}
			HR&
			Bicubic & 
			VDSR~\cite{kim2016accurate} & 
			DBPN~\cite{haris2018deep}   &
			EDSR~\cite{lim2017enhanced} & 
			RCAN~\cite{zhang2018image}  &
			SRFBN~\cite{li2019feedback} &
			DBPNLL~\cite{haris2018deep}   &
			HAN(our) \\
\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/HR.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/BI.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/VD.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/DB.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/ED.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/RC.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/SR.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/DD.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/HA.jpg} \\ 


			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/HR_crop.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/BI_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/VD_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/DB_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/ED_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/RC_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/SR_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/DD_crop.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/MariaSamaNihaNaisyo/HA_crop.jpg} \\ 
PSNR/SSIM & 21.22/ 0.737 &21.20/0.733 &24.92/ 0.881 & 24.54/0.873 &25.08/0.886 &24.26/0.866 &25.25 0.889 &\textbf{25.78}/\textbf{0.902} \\
			
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/HR.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/BI.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/VD.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/DB.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/ED.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/RC.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/SR.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/DD.jpg}& 
			
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/HA.jpg} \\ 


			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/HR_crop.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/BI_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/VD_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/DB_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/ED_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/RC_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/SR_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/DD_crop.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/TapkunNoTanteisitsu/HA_crop.jpg} \\


			PSNR/SSIM & 22.88/0.768 & 24.86/0.845 &27.52/0.913 & 27.01/0.900 &27.56/0.914 &26.69/0.893 &27.75/0.918 &\textbf{27.77}/\textbf{0.935} \\
			
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/HR.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/BI.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/VD.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/DB.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/ED.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/RC.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/SR.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/DD.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/HA.jpg} \\ 


			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/HR_crop.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/BI_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/VD_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/DB_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/ED_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/RC_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/SR_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/DD_crop.jpg}& 
			
			\includegraphics[width = 0.1\textwidth]{results/test/AkkeraKanjinchou/HA_crop.jpg} \\ 


			PSNR/SSIM & 20.09/0.525 &21.07/0.523 &23.79/0.700 &23.47/0.688 &23.87/0.703 &23.12/ 0.673 &24.00/0.708 &\textbf{24.24}/\textbf{0.746} \\
			
			
		\end{tabular}
	\end{center}


	\caption{Visual comparison for 8 SR with BI model on the Manga109 dataset.The best results are highlighted 
	}

	\label{fig-BD8}
\end{figure}









\begin{figure}[t]\tiny
	\begin{center}
		\tabcolsep 1pt
		\begin{tabular}{@{}ccccccccc@{}}
			
			HR&
			Bicubic & 
			VDSR~\cite{kim2016accurate} & 
			EDSR~\cite{lim2017enhanced} & 
			RCAN~\cite{zhang2018image}  &
			SRFBN~\cite{li2019feedback} &
			SAN~\cite{dai2019second} &
			HAN(our) &
			HAN+(our) \\
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/HR.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/BI.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/VD.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/ED.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/RC.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/SR.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/SA.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/HA.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/Hp.jpg} \\
			
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/HR_crop.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/BI_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/VD_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/ED_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/RC_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/SR_crop.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/SA_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/HA_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img044/Hp_crop.jpg} \\
			
			PSNR/SSIM & 27.70/ 0.774 &30.10/0.854 &30.64/0.878 & 36.39/0.951 &30.75/.879 &34.31/0.930 &36.44/ 0.955 &\textbf{36.62}/\textbf{0.956}\\
			
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/HR.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/BI.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/VD.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/ED.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/RC.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/SR.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/SA.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/HA.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/Hp.jpg} \\
			
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/HR_crop.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/BI_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/VD_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/ED_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/RC_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/SR_crop.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/SA_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/HA_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img076/Hp_crop.jpg} \\
			
			PSNR/SSIM & 22.17/0.674 &23.39/ 0.747 &24.19/0.785 & 27.18/0.882 &24.20/0.788 &26.56/0.873 &27.40/0.889 & \textbf{27.67}/\textbf{0.893}\\
			
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/HR.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/BI.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/VD.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/ED.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/RC.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/SR.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/SA.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/HA.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/Hp.jpg} \\
			
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/HR_crop.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/BI_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/VD_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/ED_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/RC_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/SR_crop.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/SA_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/HA_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img098/Hp_crop.jpg} \\
			
			
			PSNR/SSIM & 19.93/0.425 &20.66/ 0.508 &20.89/0.531 & 22.34/ 0.675 &20.92/ 0.534 &22.07/0.656 &22.35/.677 &\textbf{22.49}/\textbf{0.681}\\
			
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/HR.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/BI.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/VD.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/ED.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/RC.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/SR.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/SA.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/HA.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/Hp.jpg} \\
			
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/HR_crop.jpg}&
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/BI_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/VD_crop.jpg} &
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/ED_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/RC_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/SR_crop.jpg}& 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/SA_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/HA_crop.jpg} & 
			\includegraphics[width = 0.1\textwidth]{results/test/BD/img083/Hp_crop.jpg} \\
			PSNR/SSIM & 20.85/0.590 & 21.92/0.671 &22.17/ 0.692 & 24.26/ 0.814 &23.98/ 0.802 &24.20/ 0.805 &24.28/0.819 &\textbf{24.65}/\textbf{0.828}\\
			
		\end{tabular}
	\end{center}


	\caption{Visual comparison for 3 SR with BD model on the Urban100 dataset. The best results are highlighted
	}


	\label{fig-BD}
\end{figure}







\textbf{Implementation Details.} We implement the proposed network using PyTorch platform and use the pre-trained RCAN (), (), (), () model to initialize the corresponding holistic attention networks, respectively. 
In our network, patch size is set as . We use
ADAM \cite{kingma2014adam} optimizer with a batch size 16 for training. The
learning rate is set as .
Default values of  and  are used, which are 0.9 and 0.999, respectively, and we set .
We do not use any regularization operations such as batch normalization and group normalization in our network. In addition to random rotation and translation, we do not apply other data augmentation methods in the training.
The input of the LAM is selected as the outputs of all residual groups of RCAN, we use  residual groups in out network. 
For all the results reported in the paper, we train the network for  
250 epochs, which takes about two days on an Nvidia GTX 1080Ti GPU.




\begin{table}[t]
	\centering
	\scriptsize
	\caption{Quantitative results with BD degradation model. The best and second best results are highlighted in \textbf{bold} and \underline{underlined}}
	\begin{tabular}{|p{6.5em}|p{2.5em}|p{2.5em}|p{3em}|p{2.5em}|p{3em}|p{2.5em}|p{3em}|p{2.5em}|p{3em}|p{2.5em}|p{3em}|}
		
		\hline
		\multicolumn{1}{|c|}{\multirow{2}{*}{{ Method} }} & \multicolumn{1}{c|}{\multirow{2}{*}{Scale}} & \multicolumn{2}{c}{Set5} & \multicolumn{2}{c}{Set14} & \multicolumn{2}{c}{B100} & \multicolumn{2}{c}{Urban100} & \multicolumn{2}{c|}{ Manga109} \\
		\cline{3-12}  &  &  PSNR  & SSIM  & PSNR   & SSIM  & PSNR  & SSIM  & PSNR  & SSIM  & PSNR  & SSIM \\
		\hline
		
		Bicubic \newline{}SPMSR~\cite{peleg2014statistical} \newline{}SRCNN~\cite{dong2014learning}  \newline{}FSRCNN~\cite{dong2016accelerating} \newline{}VDSR~\cite{kim2016accurate} \newline{}IRCNN~\cite{zhang2017learning}  \newline{}SRMDNF~\cite{zhang2018learning} \newline{}RDN~\cite{zhang2018residual} \newline{}RCAN~\cite{zhang2018image} \newline{} SRFBN~\cite{li2019feedback} \newline{} SAN~\cite{dai2019second} \newline{} HAN(ours) \newline{} HAN+(ours)	& 
		 \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{} \newline{}  \newline{} \newline{} \newline{} 
& 28.78 \newline{}32.21 \newline{}32.05 \newline{}26.23 \newline{}33.25 \newline{}33.38 \newline{}34.01 \newline{}34.58 \newline{}34.70 \newline{}34.66 \newline{}34.75 \newline{}\underline{34.76} \newline{}\bfseries{34.85}  & 0.8308 \newline{}0.9001 \newline{}0.8944 \newline{}0.8124 \newline{}0.9150 \newline{}0.9182 \newline{}0.9242 \newline{}0.9280 \newline{}0.9288 \newline{}0.9283 \newline{}0.9290 \newline{}\underline{0.9294} \newline{}\bfseries{0.9300}  & 26.38 \newline{}28.89 \newline{}28.80 \newline{}24.44 \newline{}29.46 \newline{}29.63 \newline{}30.11 \newline{}30.53 \newline{}30.63 \newline{}30.48 \newline{}30.68 \newline{}\underline{30.70} \newline{}\bfseries{30.79}& 0.7271 \newline{}0.8105 \newline{}0.8074 \newline{}0.7106 \newline{}0.8244 \newline{}0.8281 \newline{}0.8364 \newline{}0.8447 \newline{}0.8462 \newline{}0.8439 \newline{}0.8466 \newline{}\underline{0.8475} \newline{}\bfseries{0.8487} & 26.33 \newline{}28.13 \newline{}28.13 \newline{}24.86 \newline{}28.57 \newline{}28.65 \newline{}28.98 \newline{}29.23 \newline{}29.32 \newline{}29.21 \newline{}29.33 \newline{}\underline{29.34} \newline{}\bfseries{29.41} & 0.6918 \newline{}0.7740 \newline{}0.7736 \newline{}0.6832 \newline{}0.7893 \newline{}0.7922 \newline{}0.8009 \newline{}0.8079 \newline{}0.8093 \newline{}0.8069 \newline{}0.8101 \newline{}\underline{0.8106} \newline{}\bfseries{0.8116}& 23.52 \newline{}25.84 \newline{}25.70 \newline{}22.04 \newline{}26.61 \newline{}26.77 \newline{}27.50 \newline{}28.46 \newline{}28.81 \newline{}28.48 \newline{}28.83 \newline{}\underline{28.99} \newline{}\bfseries{29.21}  & 0.6862 \newline{}0.7856 \newline{}0.7770 \newline{}0.6745 \newline{}0.8136 \newline{}0.8154 \newline{}0.8370 \newline{}0.8582 \newline{}0.8647 \newline{}0.8581 \newline{}0.8646 \newline{}\underline{0.8676} \newline{}\bfseries{0.8710}  &25.46 \newline{}29.64 \newline{}29.47 \newline{}23.04 \newline{}31.06 \newline{}31.15 \newline{}32.97 \newline{}33.97 \newline{}34.38 \newline{}34.07 \newline{}34.46 \newline{}\underline{34.56} \newline{}\bfseries{34.87} &0.8149\newline{} 0.9003\newline{} 0.8924\newline{} 0.7927\newline{} 0.9234\newline{} 0.9245\newline{} 0.9391\newline{} 0.9465\newline{} 0.9483 \newline{} 0.9466 \newline{} 0.9487 \newline{}\underline{0.9494} \newline{}\bfseries{0.9509} \\
		\hline
	\end{tabular}\label{tab-BD}\end{table}



\subsection{Ablation Study about the Proposed LAM and CSAM}
The proposed LAM and CSAM ensure that the proposed SR method generate the feature correlations between hierarchical layers, channels, and locations. One may wonder whether the LAM and CSAM help SISR.
To verify the performance of these two attention mechanisms, we compare the method without using LAM and CSAM in Table \ref{tab-psnr-ssim}, where we conduct experiments on the Manga109 dataset with the magnification factor of . 


Table \ref{tab-psnr-ssim} shows the quantitative evaluations. Compared with the baseline method which is identical to the proposed network except for the absence of these two modules LAM and CSAM. CSAM achieves better results by up to 0.06 dB in terms of PSNR, while LAM promotes 0.16 dB on the test dataset. In addition, the improvement of using both LAM and CSAM is significant as the proposed algorithm improves 0.2 dB, which demonstrates the effectiveness of the proposed layer attention and channel-spatial attention blocks. 
Figure \ref{fig-BI} further shows that using the LAM and CSAM is able to
generate the results with clearer structures and details.


\begin{table}[!t]
	\tabcolsep 8pt
	\caption{Ablation study about using different numbers of CSAMs}
	
	\begin{center}\scriptsize{
			\begin{tabular}{cccccc}
				\toprule
				&Set5 & Set14 & B100 & Urban100 & Manga100 \\
				\midrule
				HAN(1 CSAM)       &32.64  & 28.90   & 27.80  & 26.85  &31.42   \\
				HAN(3 CSAM)       &32.67 & 28.91   & 27.80  & 26.89  &\textbf{31.46}   \\
				HAN(5 CSAM)       &\textbf{32.69}  & 28.91   & 27.80  & 26.89  &31.43   \\
				HAN(10 CSAM)      &32.67  & \textbf{28.91}   & \textbf{27.80}  & \textbf{26.89} &31.43  \\
				\bottomrule
		\end{tabular}}
		\label{tabR2}
	\end{center}
	
\end{table}

\subsection{Ablation Study about the Number of Residual Group}
We conduct an ablation study about feeding different numbers of RGs to the proposed LAM. Specifically, we apply severally three, six, and ten RGs to the LAM, and we evaluate our model on five standard datasets. As shown in Table \ref{tab4}, we compare our three models with RCAN, although using fewer RGs, our algorithm still generates higher PSNR values than the baseline of RCAN. This ablation study demonstrates the effectiveness of the proposed LAM.






\subsection{Ablation Study about the Number of CSAM}
In the paper, the channel-spatial attention module (CSAM) can extract powerful representations to describe inter-channel and intra-channel information in continuous channels. We conduct an ablation study about using different numbers of CSAM. We use one, three, five, and ten CSAMs in RGs. As shown in Table\ref{tabR2}, with the increase of CSAM, the values of PSNR are increasing on the testing datasets. 
This ablation study demonstrates the effectiveness of the proposed CSAM.




\subsection{Results with Bicubic (BI) Degradation Model}
We compare the proposed algorithm with 11 state-of-the-art methods: SRCNN~\cite{dong2014learning}, FSRCNN~\cite{dong2016accelerating}, VDSR~\cite{kim2016accurate}, LapSRN~\cite{lai2017deep}, MemNet~\cite{tai2017memnet}, SRMDNF~\cite{zhang2018learning}, D-DBPN~\cite{haris2018deep}, RDN~\cite{zhang2018residual}, EDSR~\cite{lim2017enhanced}, SRFBN~\cite{li2019feedback} and SAN~\cite{dai2019second}. We provide more comparisons in supplementary material.
Following~\cite{lim2017enhanced,dai2019second,zhang2018image},  we also propose self-ensemble model and donate it as HAN+.



\textbf{Quantitative results.} Table \ref{table-BI} shows the comparison of 2, 3, 4, and 8 SR quantitative results. Compared to existing methods, our HAN+ performs best on all the scales of reconstructed test datasets. Without using self-ensemble, our network HAN still obtains great gain compared with the recent SR methods. 
In particular, our model is much better than SAN which also uses the same backbone network of RCAN and has more computationally intensive attention module.
Specifically, when we compare the reconstruction results at 8 scale on the Set5 dataset, the proposed HAN advances 0.11 dB in terms of PSNR than the competitive SAN.





To further evaluate the proposed HAN, we conduct experiments on the large test sets of B100, Urban100, and Manga109. Our algorithm still performs favorably against the state-of-the-art methods. For example, the super-resolved results by the proposed HAN is 0.06 dB and 0.35 dB higher than the very recent work of SAN for the 4 and 8 scales, respectively.


\textbf{Visual results.} We also show visual comparisons of various methods on the Urban100 dataset for 4 SR in Figure \ref{fig-BI}. As shown, most compared SR networks cannot recover the grids of buildings accurately and suffer from unpleasant blurring artifacts. In contrast, the proposed HAN obtains clearer details and reconstructs sharper high-frequency textures.


Take the first and fourth images in Figure \ref{fig-BI} as example, 
VDSR and EDSR fail to generate the clear structures. The results generated by the recent work of RCAN, SRFBN, and SAN still contain noticeable artifacts caused by spatial aliasing. 
In contrast, our approach effectively suppresses such artifacts through the proposed two attention modules. As shown, our method accurately reconstructs the grid patterns on windows in the first row and the parallel straight lines on the building in the fourth image.


For 8 SR, we also show the super-resolved results by different SR methods in Figure \ref{fig-BD8}. As show, it is challenging to predict HR images from bicubic-upsampled input by VDSR and EDSR. Even the state-of-the-art methods of RCAN and SRFBN cannot super-resolve the fine structures well.
In contrast, our HAN reconstructs high-quality HR images for 8 results by using cross-scale layer attention and channel-spatial attention modules on the limited information.





\subsection{Results with Blur-downscale Degradation (BD) Model}
\noindent \textbf{Quantitative results.} Following the protocols of \cite{zhang2018learning,zhang2017learning,zhang2018residual}, we further compare the SR results on images with blur-downscale degradation model. We compare the proposed method with nine state-of-the-art super-resolution methods: SPMSR~\cite{peleg2014statistical}, SRCNN~\cite{dong2014learning}, FSRCNN~\cite{dong2016accelerating}, VDSR~\cite{kim2016accurate}, IRCNN~\cite{zhang2017learning}, SRMD~\cite{zhang2006edge}, RDN~\cite{zhang2018residual}, RCAN~\cite{zhang2018image},SRFBN~\cite{li2019feedback} and SAN~\cite{dai2019second}. Quantitative results on the 3 SR are reported in Table \ref{tab-BD}. 
As shown, both the proposed HAN and HAN+ perform favorably against existing methods. 
In particular, our HAN+ yields the best quantitative results and HAN obtains the second best scores for all the datasets, 0.06-0.2 dB PSNR better than the attention-based methods of RCAN and SAN and 0.2-0.8 dB better than the recently proposed SRFBN.


\textbf{Visual quality.} In Figure \ref{fig-BD}, we show visual results on images from the Urban 100 dataset with blur-downscale degradation model by a scale factor of 3. Both the full images and the cropped regions are shown for comparison. 
We find that our proposed HAN is able to recover structured details that were missing in the LR image by properly exploiting the layer, channel, and spatial attention in the feature space.


As shown, VDSR and EDSR suffer from unpleasant blurring artifacts and some results even are out of shape. RCAN alleviate it to a certain extent, but still misses some details and structures.
SRFBN and SAN also fail to recover these structured details. 
In contrast, our proposed HAN effectively suppresses artifacts and exploits the scene details and the internal natural image statistics to super-resolve the high-frequency contents.





\section{Conclusions}
In this paper, we propose a holistic attention network for single image super-resolution, which adaptively learns the global dependencies among different depths, channels, and positions using the self-attention mechanism. 
Specifically, the layer attention module captures the long-distance dependencies among hierarchical layers. Meanwhile, the channel-spatial attention module incorporates the
channel and contextual information in each layer. 
These two attention modules are collaboratively applied to multi-level features and then more informative features can be captured.
Extensive experimental results on benchmark datasets demonstrate that the proposed model performs favorably against the state-of-the-art SR algorithms in terms of accuracy and visual quality.


\subsubsection{Acknowledgements:}
This work is supported by the National Key R\&D Program of China under Grant 2019YFB1406500, National Natural Science Foundation of China (No. 61971016, U1605252, 61771369), Fundamental Research Funds of Central Universities (Grant No. N160504007), Beijing Natural Science Foundation (No. L182057), Peng Cheng Laboratory Project of Guangdong Province PCL2018KP004, and the Shaanxi Provincial Natural Science Basic Research Plan (2019JM-557). 
\clearpage



\bibliographystyle{splncs04}
\bibliography{sr2020_}
\end{document}
