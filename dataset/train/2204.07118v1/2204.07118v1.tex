\section{Experiments}
This section includes multiple experiments in image classification, with a special emphasis on Imagenet1k~\cite{deng2009imagenet,Recht2019ImageNetv2,Russakovsky2015ImageNet12}. We also report results for  downstream tasks in fine-grained classification and segmentation. 
We include a large number of ablations to better analyze different effects, such as the importance of the training resolution and longer training schedules. We provide additional results in the appendices. 

\subsection{Baselines and default settings}
 
The main task that we consider in this paper for the evaluation of our training procedure is image classification. 
We train on Imagenet1k-train and evaluate on Imagenet1k-val, with results on ImageNet-V2 to control  overfitting. 
We also consider the case where we can pretrain  on ImageNet-21k, 
Finally, we report transfer learning results on 6 different datasets/benchmarks. 

\paragraph{Default setting. }
When training on ImageNet-1k only,  by default we train during 400 epochs with a batch size 2048, following prior works~\cite{touvron2021going,xiao2021early}. 
Unless specified otherwise, both the training and evaluation are carried out at resolution  (even though we recommend to train at a lower resolution when targeting  at inference time). 

When pre-training on ImageNet-21k, we pre-train by default during 90 epochs at resolution , followed by a finetuning of 50 epochs on on ImageNet-1k. In this context we consider two fine-tuning resolutions:  and . 

\subsection{Ablations}


\subsubsection{Impact of training duration}

In Figure~\ref{fig:epochs} we provide an ablation on the number of epochs, which show that ViT models do not saturate as rapidly as the DeiT training procedure~\cite{Touvron2020TrainingDI} when we increase the number of epochs beyond the 400 epochs adopted for our baseline.

For ImageNet-21k pre-training, we use 90 epochs for pre-training as in a few works~\cite{liu2021swin,Touvron2021AugmentingCN}. We finetune during 50 epochs on ImageNet-1k~\cite{Touvron2021AugmentingCN} and marginally adapt the stochastic depth parameter. We point out that this choice is mostly for the sake of consistency across models: we observe that training 30 epochs also provides similar results. 




\begin{figure}
    \centering
    \begin{minipage}{0.48 \linewidth}
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/imnet_epochs_square.pdf}
    \caption{Top-1 accuracy on ImageNet-1k only at resolution  with our training recipes and a different number of epochs \label{fig:epochs}}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49 \linewidth}
    \centering
    \includegraphics[width=0.85\linewidth]{Figs/imnet_crop_square.pdf}
    \caption{Transfer learning performance on 6 datasets with different test-time crop ratio. ViT-B pre-trained on ImageNet-1k at resolution . 
    \label{fig:crop}}
    \end{minipage}
\end{figure}

\subsubsection{Data-Augmentation}
In Table~\ref{tab:da_comp} we compare our handcrafted data-augmentation 3-Augment with existing learned augmentation methods. 
With the ViT architecture, our data-augmentation is the most effective while being simpler than the other approaches. 
Since previous augmentations were introduced on convnets, we also provide results for a ResNet-50. In this case previous augmentation policies have similar (RandAugment, Trivial-Augment) or better results (Auto-Augment) on the validation set. This is no longer the case when evaluating on the independent set V2, for which the Auto-Augment better accuracy is not significant. 

\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{c|cc|c|ccc}
    \toprule
        \multirow{2}{*}{Method} & Learned augm. &  \# Nb of   &  \multirow{2}{*}{Model} & \multicolumn{3}{c}{ImageNet-1k} \\
        & methods & DA & & Val & Real & V2\\
        \midrule
         \multirow{3}{*}{Auto-Augment~\cite{Ekin2018AutoAugment}} & \multirow{3}{*}{\cmark} & \multirow{3}{*}{14}
         & \grtext{ResNet50}   & \grtext{\textbf{79.7}} & \grtext{\textbf{85.6}} & \grtext{\textbf{67.9}}\\
         & &  & ViT-B & 82.8 & 87.5 & 71.9\\
         & &  & ViT-L & 84.0 & \textbf{88.6} & 74.0 \\
         \midrule
        \multirow{3}{*}{RandAugment~\cite{Cubuk2019RandAugmentPA}}  & \multirow{3}{*}{\cmark} & \multirow{3}{*}{14}
        & \grtext{ResNet50}   & \grtext{79.5}  & \grtext{85.5} & \grtext{67.6} \\
        & &  & ViT-B &  82.7 & 87.4 & 72.2 \\
        &  & &  ViT-L & 84.0 & 88.3 & 73.8 \\
        \midrule
        \multirow{3}{*}{Trivial-Augment~\cite{Mller2021TrivialAugmentTY}} & \multirow{3}{*}{\cmark} & \multirow{3}{*}{14} 
        & \grtext{ResNet50}    & \grtext{79.5}  & \grtext{85.4} & \grtext{67.6} \\
        & &  & ViT-B  & 82.3 & 87.0 & 71.2 \\
        &  & & ViT-L & 83.6 & 88.1 & 73.7 \\
        \midrule
        \rowcolor{Goldenrod}
         &  & & \grtext{ResNet50} & \grtext{79.4}  & \grtext{85.5} & \grtext{67.8} \\
        \rowcolor{Goldenrod}
        &  & &ViT-B & \textbf{83.1} & \textbf{87.7} & \textbf{72.6} \\
        \rowcolor{Goldenrod}
        \multirow{-3}{*}{3-Augment (Ours)}  & \multirow{-3}{*}{\xmark} & \multirow{-3}{*}{3} &  ViT-L & \textbf{84.2} & \textbf{88.6} & \textbf{74.3} \\

    \bottomrule
    \end{tabular}}
    \caption{Comparison of some existing data-augmentation methods with our simple 3-Augment proposal inspired by data-augmentation used with self-supervised learning.
    \label{tab:da_comp}}
\end{table}


\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{c|cccc|ccc}
    \toprule
         \multirow{2}{*}{Model} & \multirow{2}{*}{Loss} & \multirow{2}{*}{LayerScale} &\multirow{2}{*}{Data Aug.} &\multirow{2}{*}{Epochs} & \multicolumn{3}{c}{ImageNet-1k} \\
         &  & &  &  & val & real & v2 \\
        \midrule
         \multirow{5}{*}{\rotatebox{90}{ViT-S}}
         & CE & \xmarkg & RandAugment & 300 & 79.8 & 85.3 & 68.1\\
         & BCE & \xmarkg & \textcolor{lightgray}{RandAugment} & \textcolor{lightgray}{300} & 79.8 & 85.9 & 68.2 \\
          & \textcolor{lightgray}{BCE} & \cmark & \textcolor{lightgray}{RandAugment} & \textcolor{lightgray}{300} &  80.1 & \textbf{86.1} & 69.1\\
          & \textcolor{lightgray}{BCE} & \cmarkg & \textcolor{lightgray}{RandAugment} & 400 & \textbf{80.7} & 86.0 & 69.3\\
          \rowcolor{Goldenrod}
          & \textcolor{lightgray}{BCE} & \cmarkg & 3-Augment & \textcolor{lightgray}{400} & 80.4 & \textbf{86.1} & \textbf{69.7}\\         
         
         \midrule
         \multirow{5}{*}{\rotatebox{90}{ViT-B}}
         & CE & \xmarkg & RandAugment & 300 & 80.9 & 85.5 & 68.5\\
         & BCE & \xmarkg & \textcolor{lightgray}{RandAugment} & \textcolor{lightgray}{300} & 82.2 & 87.2 & 71.4\\
          & \textcolor{lightgray}{BCE} & \cmark & \textcolor{lightgray}{RandAugment} & \textcolor{lightgray}{300} & 82.5 & 87.5 & 71.4 \\
          & \textcolor{lightgray}{BCE} & \cmarkg & \textcolor{lightgray}{RandAugment} & 400 & 82.7 & 87.4 & 72.2 \\
          \rowcolor{Goldenrod}
          & \textcolor{lightgray}{BCE} & \cmarkg & 3-Augment & \textcolor{lightgray}{400} & \textbf{83.1} & \textbf{87.7} & \textbf{72.6} \\
          \midrule
          \multirow{4}{*}{\rotatebox{90}{ViT-L}}
          & BCE & \xmarkg & RandAugment & 300 & 83.0 & 87.9 & 72.4\\
          & \textcolor{lightgray}{BCE} & \xmarkg & \textcolor{lightgray}{RandAugment} & 400 & 83.3 & 87.7 & 72.5\\
          & \textcolor{lightgray}{BCE} & \cmark & \textcolor{lightgray}{RandAugment} & \textcolor{lightgray}{400} & 84.0 & 88.3 & 73.8\\
          \rowcolor{Goldenrod}
           & \textcolor{lightgray}{BCE} & \cmarkg & 3-Augment  & \textcolor{lightgray}{400} & \textbf{84.2} & \textbf{88.6} & \textbf{74.3}\\
           \bottomrule
          
    \end{tabular}}
    \caption{Ablation on different training component with training at resolution  on ImageNet-1k. We perform avlations with ViT-S, ViT-B and ViT-L. We report top-1 accuracy (\%) on ImageNet validation set , ImageNet real and ImageNet v2. 
    \label{tab:ablation}}
\end{table}



\begin{table}
    \centering
    \scalebox{0.75}{
    \begin{tabular}{@{\ }ccccccccc|cccc@{\ }}
    \toprule
         \multirow{2}{*}{Crop.} & \multirow{2}{*}{LS} &  \multirow{2}{*}{Mixup} &  Aug. & \#Imnet21k & finetuning & \multicolumn{3}{c}{Imagenet-1k val top-1 }  & \multicolumn{3}{c}{Imagenet-1k v2 top-1 }\\
         & & & policy & epochs & resolution & ViT-S & ViT-B & ViT-L  &  ViT-S & ViT-B & ViT-L \\
         \midrule
         RRC                   & \xmark  & 0.8                      & RA  & \pzo 90                    & 224                   & 81.6 & 84.6 & 86.0 & 70.7 & 74.7 & 76.4  \\ 
         SRC                   & \xmarkg & \textcolor{gray}{0.8}    & RA  & \pzo \textcolor{gray}{90}  & \textcolor{gray}{224} & 82.1 & 84.8 & 86.3 & 71.8 & 75.0 &  76.7 \\
         \textcolor{gray}{SRC} & \cmark  & \textcolor{gray}{0.8}    & RA  & \pzo \textcolor{gray}{90}  & \textcolor{gray}{224} & 82.4 & 85.0 & 86.4 & 72.4 & 75.7 & 77.4  \\
         \textcolor{gray}{SRC} & \cmarkg & \xmark                   & RA  & \pzo \textcolor{gray}{90}  & \textcolor{gray}{224} & 82.3 & 85.1 & 86.5 & 72.4 & 75.6 &  77.2  \\
         \textcolor{gray}{SRC} & \cmarkg & \xmarkg                  & 3A  & \pzo \textcolor{gray}{90}  & \textcolor{gray}{224} & 82.6 & 85.2 & 86.8 &  72.6 & 76.1 & 78.3 \\
         \textcolor{gray}{SRC} & \cmarkg & \xmarkg & \textcolor{gray}{3A}  & 240                       & \textcolor{gray}{224} & \textbf{83.1} & \textbf{85.7} & \textbf{87.0} &  \textbf{73.8}  & \textbf{76.5} & \textbf{78.6}  \\
\midrule
         \textcolor{gray}{SRC} & \cmarkg & \xmarkg & \textcolor{gray}{3A}  & \textcolor{gray}{240}    & 384 & \emph{84.8} & \emph{86.7} & \emph{87.7} &  75.1  & 77.9  & 79.1  \\
    \bottomrule
    \end{tabular}}
    \caption{Ablation path: \textbf{augmentation and regularization} with ImageNet-21k pre-training (at resolution 224224) and ImageNet-1k fine-tuning. We measure the impact of changing Random Resize Crop (RRC) to Simple Random Crop (SRC), adding LayerScale (LS), removing Mixup, replacing RandAugment (RA) by 3-Augment (3A), and finally employing a longer number of epochs during the pre-training phase on ImageNet-21k. All experiments are done with Seed 0 with fixed hparams except the drop-path rate of stochastic depth, which depends on the model and is increased by 0.05 for the longer pre-training. We report 2 digits top-1 accuracy but note that the standard standard deviation is around  on our ViT-B baseline. 
    Note that all these changes are neutral w.r.t. complexity except in the last row, where the fine-tuning at resolution 384384 significantly increases the complexity. }
    \label{tab:rrc_ablation}
\end{table}



\subsubsection{Impact of training resolution}

In Table~\ref{tab:fixRes} we report the evolution of the performance according to the training resolution. 
We observe that we benefit from the FixRes~\cite{Touvron2019FixRes} effect. 
By training at resolution 192192 (or 160160) we get a better performance at 224 after a slight fine-tuning than when training from scratch at 224224. 

We observe that the resolution has a regularization effect. While it is known that it is best to use a smaller resolution at training time~\cite{Touvron2019FixRes}, we also observe in the training curves that this show reduces the overfitting of the larger models. This is also illustrated by our results Table~\ref{tab:fixRes} with ViT-H and ViT-L. This is especially important with longer training, where models overfit without a stronger regularisation.
This smaller resolution implies that there are less patches to be processed, and therefore it reduces the training cost and increases the performance. In that respect it effect is comparable to that of MAE~\cite{He2021MaskedAA}.
We also report results with ViT-H 52 layers and ViT-H 26 layers parallel~\cite{Touvron2022ThreeTE} models with 1B parameters. Due to the lower resolution training it is easier to train these models.

\begin{table}[]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{c|cc|cc|ccc}
    \toprule
        \multirow{2}{*}{Model} & \multicolumn{2}{c}{epochs} & \multicolumn{2}{c}{Resolution}  & \multicolumn{3}{c}{ImageNet top-1 acc} \\
        & Train. & FT & Train. & FT & val & real & v2 \\
        \midrule
        \multirow{8}{*}{ViT-B} & 
        \multirow{4}{*}{400} & \multirow{3}{*}{20} & 
        & \multirow{4}{*}{} & 83.2 & \textbf{88.1} & \underline{73.2} \\ & & & &  & \underline{83.3} & \underline{88.0} & \textbf{73.4} \\
        & & &  &  & \textbf{83.5} & \underline{88.0} & 72.8 \\
        & & \_ & & & 83.1 & 87.7 & 72.6 \\
        \cmidrule{2-8}
         &\multirow{4}{*}{800} & \multirow{3}{*}{20} & 
        & \multirow{4}{*}{} & 83.5 & \textbf{88.3} & 73.4  \\
        & & & &  & 83.6 & \underline{88.2} & \underline{73.5} \\
        & & &  &  & \textbf{83.8} & \underline{88.2} & \textbf{73.6}\\ & & \_ &  & & \underline{83.7} & 88.1 & 73.1   \\
        \midrule
        \multirow{8}{*}{ViT-L} & 
        \multirow{4}{*}{400} & \multirow{3}{*}{20} &
        & \multirow{4}{*}{} & 83.9 & \textbf{88.8} &  \underline{74.3} \\
        & & & &  & \underline{84.4} & \textbf{88.8} & \underline{74.3} \\ & & &   & & \textbf{84.5} & \textbf{88.8} & \textbf{75.1} \\
        & & \_  &  & & 84.2 & 88.6  & \underline{74.3} \\
         \cmidrule{2-8}
        &\multirow{4}{*}{800} & \multirow{3}{*}{20} &
        & \multirow{4}{*}{} & 84.5 &  \textbf{88.9} & 74.7 \\& & & &  & \underline{84.7} & \textbf{88.9} &  \textbf{75.2}\\ & & &   & & \textbf{84.9} & 88.7 & \underline{75.1} \\
        & & \_  &  & & 84.5 & \underline{88.8}  & 75.0 \\
        \midrule
        \multirow{8}{*}{ViT-H} & 
        \multirow{4}{*}{400} & \multirow{3}{*}{20} &
        & \multirow{4}{*}{} & 84.7  & \underline{89.2} & 75.2  \\
        & & & &  & \textbf{85.1} & \textbf{89.3} & \underline{75.3} \\ & & &   &  & \textbf{85.1} & \underline{89.2} & \textbf{75.4}  \\
        & & \_  &  &  & 84.8 & 89.1 & \underline{75.3} \\
         \cmidrule{2-8}
        &\multirow{4}{*}{800} & \multirow{3}{*}{20} &
        & \multirow{4}{*}{} & \underline{85.1} & \textbf{89.2}  & 75.6 \\& & & &  & \textbf{85.2} & \textbf{89.2} & \textbf{75.9} \\ & & &   & & \underline{85.1}  & 88.9 & \textbf{75.9} \\ & & \_  &  & & 84.6 & 88.5  & 74.9 \\    
        \midrule
        ViT-H-52 & 400 & 20 &  &  & 84.9 & 89.2 & 75.6\\
        \midrule
        ViT-H-262 & 400 & 20 &  &  & 84.9 & 89.1 & 75.3 \\
    \bottomrule
    \end{tabular}}
    \caption{We compare ViT architectures pre-trained on ImageNet-1k only with different training resolution followed by a fine-tuning at resolution . We benefit from the FixRes effect~\cite{Touvron2019FixRes} and get better performance with a lower training resolution (e.g resolution  with patch size 16 represent 100 tokens vs 196 for . This represents a reduction of ~50\% of the number of tokens).}
    \label{tab:fixRes}
\end{table}


\subsubsection{Comparison with previous training recipes for ViT}
In Figure~\ref{fig:method_fig}, we compare training procedures used to pre-train the ViT architecture either on ImageNet-1k and ImageNet-21k.
Our procedure outperforms existing recipes with a large margin. For instance, with ImageNet-21k pre-training we have an improvement of  with ViT-L in comparison to the best approach.
Similarly, when training from scratch on ImageNet-1k we improve the accuracy by  for ViT-H  compared to the previous best approach, and by  with the best approach that does not use EMA. See also detailed results in our appendices. 


\subsection{Image Classification}

\paragraph{\textbf{ImageNet-1k.}}

In Table~\ref{tab:mainres} we compare ViT architectures trained with our training recipes on ImageNet-1k with other architectures. We include a comparison with the recent SwinTransformers~\cite{liu2021swin} and ConvNeXts~\cite{Liu2022convnext}.



\paragraph{\textbf{Overfitting evaluation.}}

The comparison between Imagenet-val and -v2 is a way to quantify overfitting~\cite{Touvron2020FixingTT}, or at least the better capability to generalize in a nearby setting without any fine-tuning\footnote{Caveat: The measures are less robust with -V2 as the number of test images is 10000 instead of 50000 for Imagenet-val. This translates to a higher standard deviation ().}.   
In Figure~\ref{fig:imagenet_v2} we plot ImageNet-val top-1 accuracy  vs ImageNet-v2 top-1 accuracy in order to evaluate how the models performed when evaluated on a test set never seen at validation time. 
Our models overfit significantly than all other models considered, especially on ImageNet-21k. This is a good behaviour that validates the fact that our restricted choice of hyper-parameters and variants in our recipe does not lead to (too much) overfitting. 


\begin{figure}
\begin{minipage}{0.48 \linewidth}
    \centering
    ImageNet-1k
    \includegraphics[width=\linewidth]{Figs/imnet_v2_v2.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48 \linewidth}
    \centering
    ImageNet-21k
    \includegraphics[width=\linewidth]{Figs/imnet_v2_22k_v2.pdf}
    \end{minipage}
    \caption{Generalization experiment: top-1 accuracy on ImageNet1k-val versus ImageNet-v2 for models in Table~\ref{tab:mainres} and Table~\ref{tab:mainres_22k}. We display a linear interpolation of all points in order to compare the generalization capability (or level of overfitting) for the different models.
    \label{fig:imagenet_v2}}
\end{figure}


\begin{table}
    \caption{
\textbf{Classification with Imagenet1k training.} 
We compare architectures with comparable FLOPs and number of parameters. All models are trained on ImageNet1k only without distillation nor self-supervised pre-training.
We report Top-1 accuracy on the validation set of ImageNet1k and ImageNet-V2 with different measure of complexity: throughput, FLOPs, number of parameters and peak memory usage. 
The throughput and peak memory are measured on a single V100-32GB GPU with batch size fixed to 256 and mixed precision. 
For ResNet~\cite{He2016ResNet} and RegNet~\cite{Radosavovic2020RegNet} we report the improved results from Wightman et al.~\cite{wightman2021resnet}. Note that different models may have received a different optimization effort. R indicates that the model is fine-tuned at the resolution  and -R indicates that the model is trained at resolution R.
\label{tab:mainres}}
\centering
\scalebox{0.78}{
    \begin{tabular}{@{\ }l@{}c@{\ \ }c@{\ \ \ }r@{\ \ }r|cc@{\ }}
        \toprule
        Architecture        & nb params & throughput & FLOPs & Peak Mem & Top-1  & V2 \\
                      & () & (im/s) & () & (MB)\ \ \ \  & Acc.  & Acc. \3pt]
     ResNet-50~\cite{He2016ResNet,wightman2021resnet} &  25.6    & 2587  &  4.1      & 2182 & 80.4  & 68.7 \\
    ResNet-101~\cite{He2016ResNet,wightman2021resnet}&  44.5    & 1586  &  7.9      & 2269 & 81.5  & 70.3  \\
    ResNet-152~\cite{He2016ResNet,wightman2021resnet}&  60.2    &  1122 & 11.6      & 2359 & 82.0  & 70.6 \\
    \midrule
	 RegNetY-4GF~\cite{Radosavovic2020RegNet,wightman2021resnet}       & 20.6  & 1779  & \tzo4.0 & 3041 & 81.5  & 70.7 \\
	 RegNetY-8GF~\cite{Radosavovic2020RegNet,wightman2021resnet}       & 39.2  & 1158 & \tzo8.0 & 3939 & 82.2 & 71.1 \\
	 RegNetY-16GF~\cite{Radosavovic2020RegNet,Touvron2020TrainingDI}      & 83.6  & \pzo714 & \dzo16.0 & 5204   & 82.9  & 72.4 \\

    \midrule
	 EfficientNet-B4~\cite{tan2019efficientnet} & 19.0  & \pzo573 & \tzo4.2 &  10006  & 82.9  & 72.3\\
	 EfficientNet-B5~\cite{tan2019efficientnet} & 30.0  & \pzo268 & \tzo9.9 &  11046  & 83.6  & 73.6\\
	 
	 \midrule
	 EfficientNetV2-S~\cite{Tan2021EfficientNetV2SM}& 21.5 & 874 & 8.5 & 4515 & 83.9 & 74.0 \\
    EfficientNetV2-M~\cite{Tan2021EfficientNetV2SM}& 54.1 & 312 & 25.0 & 7127 & 85.1 & 75.5 \\
    EfficientNetV2-L~\cite{Tan2021EfficientNetV2SM}& 118.5 & 179 & 53.0 & 9540 & 85.7 & 76.3 \\
	
\toprule

\multicolumn{5}{c}{\textbf{Vision Transformers derivative}} \\ [5pt]
    PiT-S-224~\cite{Heo2021RethinkingSD} & 23.5 & 1809\pzo & 2.9 &  3293 & 80.9 & \_\\
    PiT-B-224~\cite{Heo2021RethinkingSD} & 73.8 & 615 & 12.5  &  7564 & 82.0 & \_\\
	Swin-T-224~\cite{liu2021swin} & 28.3 & 1109\pzo & 4.5 & 3345 & 81.3 &  69.5 \\
    Swin-S-224~\cite{liu2021swin} & 49.6 & 718 & 8.7 & 3470 &  83.0 &   71.8 \\
    Swin-B-224~\cite{liu2021swin} & 87.8  & 532 & 15.4 & 4695 & 83.5 &   \_ \\
    Swin-B-384~\cite{liu2021swin} & 87.9 &   160       &   47.2    & 19385 & 84.5 & \_ \\
    \toprule
 \multicolumn{7}{c}{\textbf{Vision MLP \& Patch-based ConvNets}} \3pt]

\toprule
\multicolumn{7}{c}{\textbf{``Traditional'' ConvNets}} \3pt]
 
    ConvNeXt-B~\cite{Liu2022convnext} & 88.6 & 563 & 15.4 &3029 &  85.8 & 75.6 \\     
    ConvNeXt-B384~\cite{Liu2022convnext} & 88.6 & 190 & 45.1 &7851 &  86.8 & 76.6\\     
    
    ConvNeXt-L~\cite{Liu2022convnext} & 197.8 & 344 & 34.4 & 4865 &  86.6 & 76.6\\     
    ConvNeXt-L384~\cite{Liu2022convnext} & 197.8 & 115 & 101 &11938 &  87.5 & 77.7 \\  
    
    ConvNeXt-XL~\cite{Liu2022convnext} & 350.2 & 241 & 60.9 & 6951 &  87.0 & 77.0\\     
    ConvNeXt-XL384~\cite{Liu2022convnext} & 350.2 & \pzo80 & 179.0 & 16260 &  87.8 & 77.7 \\  

    
    \toprule

\multicolumn{7}{c}{\textbf{Vision Transformers derivative}} \\ [5pt]
     Swin-B~\cite{liu2021swin}& 87.8 & 532 & 15.4 & 4695 &  85.2 & 74.6 \\
     Swin-B384~\cite{liu2021swin}& 87.9 & 160 & 47.0 & 19385 &  86.4 & 76.3 \\
     
     Swin-L~\cite{liu2021swin}& 196.5 & 337 & 34.5 & 7350 &  86.3 & 76.3 \\
     Swin-L384~\cite{liu2021swin}& 196.7 & 100 & 103.9 & 33456 &  87.3 & 77.0 \\     

\toprule
\multicolumn{7}{c}{\textbf{Vanilla Vision Transformers}} \\ [5pt]
ViT-B/16~\cite{Steiner2021HowTT}  & 86.6 & 831 & 17.6 & 2078 & 84.0 &  \_\\
ViT-B/16384~\cite{Steiner2021HowTT}  & 86.7 & 190 & 55.5 & 8956 & 85.5 & \_\\

ViT-L/16~\cite{Steiner2021HowTT}  & 304.4 & 277 & 61.6 & 3789 &  84.0 & \_\\
ViT-L/16384~\cite{Steiner2021HowTT}  & 304.8 & \pzo67 & 191.1 & 12866 & 85.5 &\_ \\

    \toprule
    

    \rowcolor{Goldenrod!75}
    \multicolumn{7}{c}{\textbf{Our Vanilla Vision Transformers}} \\ [5pt]
    \rowcolor{Goldenrod}
    \rowcolor{Goldenrod}
    ViT-S & 22.0  & 1891\pzo & \tzo4.6 & 987 & 83.1 & 73.8 \\

    \rowcolor{Goldenrod}
    ViT-B & 86.6  & 831  & \dzo17.6 & 2078 & 85.7 & 76.5 \\
    \rowcolor{Goldenrod}
    ViT-B384 & 86.9  & 190 & \dzo55.5 &  8956 & 86.7 & 77.9 \\
    \rowcolor{Goldenrod}
    ViT-L & 304.4 & 277 & 61.6 & 3789  & 87.0 &  78.6 \\
    \rowcolor{Goldenrod}
    ViT-L384 & 304.8 & \pzo67 & 191.2 & 12866 & 87.7 & 79.1  \\
    \rowcolor{Goldenrod}
    ViT-H & 632.1 & 112 & 167.4 & 6984 & 87.2 & 79.2 \\
    


    \bottomrule
    \end{tabular}}
\end{table}


\paragraph{\textbf{Comparison with BerT-like pre-training.}}

In Table~\ref{tab:comp_ssl} we compare ViT models trained with our training recipes with ViT trained with different BerT-like approaches. We observe that for an equivalent number of epochs our approach gives comparable performance on ImageNet-1k and better on ImageNet-v2 as well as in segmentation on Ade. For BerT like pre-training we compare our method with MAE~\cite{He2021MaskedAA} and BeiT~\cite{bao2021beit} because they remain  relatively simple approaches with very good performance. As our approach does not use distillation or multi-crops we have not made a comparison with approaches such as PeCo~\cite{Dong2021PeCoPC} which use an auxiliary model as a psycho-visual loss and iBoT~\cite{Zhou2021iBOTIB}, which uses multi-crop and an exponential moving average of the model.
\begin{table}[t]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{c|c|ccc|ccc}
    \toprule
        Pretrained & \multirow{2}{*}{Model} & 
        \multirow{2}{*}{Method}  &
        \# pre-training & \# finetuning & \multicolumn{3}{c}{ImageNet}   \\
        data & &  & epochs & epochs & val & Real & V2  \\
        \midrule
        \multirow{12}{*}{INET-1k} & \multirow{5}{*}{ViT-B}
         & \multirow{2}{*}{BeiT} & 300 &  & 82.9  & \_ & \_ \\
        & &  &    800 &  & 83.2  & \_ & \_\\
          \cmidrule{3-8}
        & & MAE   & 1600 &  & \underline{83.6}  & \underline{88.1} &  \underline{73.2}  \\
         \cmidrule{3-8}
        & &  \cellcolor{Goldenrod}  & \cellcolor{Goldenrod}   & \cellcolor{Goldenrod}  & \cellcolor{Goldenrod} 83.5  & \cellcolor{Goldenrod} 88.0 & \cellcolor{Goldenrod} 72.8  \\
        
        & &  \multirow{-2}{*}{\cellcolor{Goldenrod} Ours} & \cellcolor{Goldenrod}  & \cellcolor{Goldenrod}  & \cellcolor{Goldenrod} \textbf{83.8}  & \cellcolor{Goldenrod} \textbf{88.2} & \cellcolor{Goldenrod} \textbf{73.6}  \\
        \cmidrule{2-8}
        
        & \multirow{7}{*}{ViT-L}
        & BeiT   & 800 &  & \underline{85.2}  & \_ & \_  \\
        \cmidrule{3-8}
      &  & \multirow{3}{*}{MAE} & 400 &  & 84.3  & \_ & \_ \\
       &  &  & 800 &  & 84.9  & \_ & \_ \\
      &  &  & 1600 &  & 85.1  & \_ & \_ \\
      \cmidrule{3-8}
      & & MAE   & 1600 &  & \textbf{85.9}  & \textbf{89.4} & \textbf{76.5} \\
        \cmidrule{3-8}
       & & \cellcolor{Goldenrod}   &  \cellcolor{Goldenrod}  & \cellcolor{Goldenrod}  & \cellcolor{Goldenrod} 84.5  & \cellcolor{Goldenrod} \underline{88.8} &  \cellcolor{Goldenrod} \underline{75.1}  \\  
       & & \multirow{-2}{*}{\cellcolor{Goldenrod} Ours}   & \cellcolor{Goldenrod}  & \cellcolor{Goldenrod}  & \cellcolor{Goldenrod} 84.9   & \cellcolor{Goldenrod} 88.7  &  \cellcolor{Goldenrod} \underline{75.1}    \\  
     \midrule
     
     \multirow{8}{*}{INET-21k} & \multirow{4}{*}{ViT-B}
      &  \multirow{2}{*}{BeiT}  &  150 &   & 83.7  & 88.2 & 73.1  \\
         &  &  &  150 +  &    & \underline{85.2}  &  \underline{89.4} & 75.4  \\
    \cmidrule{3-8}
& &  \cellcolor{Goldenrod}   &  \cellcolor{Goldenrod}  &   \cellcolor{Goldenrod}  &  \cellcolor{Goldenrod} \underline{85.2}  &  \cellcolor{Goldenrod} \underline{89.4} &   \cellcolor{Goldenrod} \underline{76.1}  \\
     & & \multirow{-2}{*}{\cellcolor{Goldenrod} Ours}   &  \cellcolor{Goldenrod}  &   \cellcolor{Goldenrod}  &  \cellcolor{Goldenrod} \textbf{85.7}  &  \cellcolor{Goldenrod} \textbf{89.5} &   \cellcolor{Goldenrod} \textbf{76.5}  \\
    \cmidrule{2-8}
     & \multirow{4}{*}{ViT-L}
     & \multirow{2}{*}{BeiT} &  150 &   & 86.0  & 89.6 & 76.7  \\
    &  &  &  150 +  &    & \textbf{87.5}  &  \textbf{90.1} & \textbf{78.8}  \\
    \cmidrule{3-8}
    &  &  \cellcolor{Goldenrod}   &  \cellcolor{Goldenrod}  &  \cellcolor{Goldenrod}  &   \cellcolor{Goldenrod} 86.8  &  \cellcolor{Goldenrod} 89.9 &  \cellcolor{Goldenrod} 78.3  \\
    &  & \multirow{-2}{*}{\cellcolor{Goldenrod} Ours}   &  \cellcolor{Goldenrod}  &  \cellcolor{Goldenrod}  &   \cellcolor{Goldenrod} \underline{87.0}  &  \cellcolor{Goldenrod} \underline{90.0} &  \cellcolor{Goldenrod} \underline{78.6} \\
\bottomrule
    \end{tabular}}
    \caption{Comparison of self-supervised pre-training with our approach. As our approach is fully supervised, this table is given as an indication. All models are evaluated at resolution . We report Image classification results on ImageNet val, real and v2 in order to evaluate overfitting.  indicate a finetuning with labels on ImageNet-21k and  indicate a finetuning with labels on ImageNet-1k.  design the improved setting of MAE using pixel (w/ norm) loss.
}
    \label{tab:comp_ssl}
\end{table}
 

\subsection{Downstream tasks and other architectures} 

\subsubsection{Transfer Learning}


In order to evaluate the quality of the ViT models learned through our training procedure we evaluated them  with transfer learning tasks. We focus on the performance of ViT models pre-trained on ImageNet-1k only at resolution  during 400 epochs on the 6 datasets shown in Table~\ref{tab:dataset}. Our results are presented in Table~\ref{tab:sota_tl}. In Figure~\ref{fig:crop} we measure the impact of the crop ratio at inference time on transfer learning results. We observe that on iNaturalist this parameter has a significant impact on the performance. As recommended in the paper Three Things~\cite{Touvron2022ThreeTE} we finetune only the attention layers for transfer learning experiments on Flowers, this improves performance by .


\begin{table}[t]
    \caption{We compare Transformers based models on different transfer learning tasks with ImageNet-1k pre-training. We report results with our default training on ImageNet-1k (400 epochs at resolution ). We also report results with convolutional architectures for reference. For consistency we keep our crop ratio equal to 1.0 on all datasets. Other works use 0.875, which is better for iNat-19 and iNat-18, see Figure~\ref{fig:crop}.\label{tab:sota_tl}}
    \smallskip
    \centering
    \scalebox{0.8}{
    \begin{tabular}{l|cccccc}
    \toprule
    Model    & CIFAR-10 & CIFAR-100  & Flowers & Cars & iNat-18 & iNat-19  \\
    \midrule                                                                          
    Grafit ResNet-50~\cite{Touvron2020GrafitLF}  & \_   & \_ & 98.2 & 92.5 & 69.8 & 75.9 \\
    ResNet-152~\cite{Chu2020FeatureSA}  & \_ & \_ & \_ & \_ & 69.1 & \_ \\
    \midrule                                                                          
    ViT-B/16~\cite{dosovitskiy2020image}         & 98.1 & 87.1 & 89.5 & \_   & \_ & \_  \\
    ViT-L/16~\cite{dosovitskiy2020image}         & 97.9 & 86.4 & 89.7 & \_   & \_ & \_  \\
    \midrule
    ViT-B/16~\cite{Steiner2021HowTT}         & \_ & 87.8 & 96.0 & \_   & \_ & \_  \\
    ViT-L/16~\cite{Steiner2021HowTT}         & \_ & 86.2 & 91.4 & \_   & \_ & \_  \\
    \midrule
    DeiT-B & 99.1 & 90.8 & 98.4  & 92.1 & 73.2 &  77.7 \\
    \midrule 
    \rowcolor{Goldenrod}
    Ours ViT-S & 98.9 & 90.6 & 96.4 & 89.9 & 67.1 & 72.7 \\
    \rowcolor{Goldenrod}
    Ours ViT-B & 99.3 & 92.5 & 98.6 & 93.4 & 73.6 & 78.0\\ 
    \rowcolor{Goldenrod}
    Ours ViT-L & \textbf{99.3} & \textbf{93.4} & \textbf{98.9} & \textbf{94.5} & \textbf{75.6} & \textbf{79.3} \\ 
    \bottomrule
    \end{tabular}}
\end{table} 


\subsubsection{Semantic segmentation}
We evaluate our ViT baselines models (400 epochs schedules for ImageNet-1k models and 90 epochs for ImageNet-21k models) with semantic segmentation experiments on  ADE20k dataset~\cite{Zhou2017ScenePT}.
This dataset consists of 20k training and 5k validation images with labels over 150 categories. 
For the training, we adopt the same schedule as in Swin:~160k iterations with UperNet~\cite{xiao2018unified}. 
At test time we evaluate with a single scale and multi-scale.
Our UperNet implementation is based on the XCiT~\cite{el2021xcit} repository.
By default the UperNet head uses an embedding dimension of 512. In order to save compute, for  small and tiny models we set it to the size of their working dimension, i.e. 384 for small and 192 for tiny. We keep the 512 by default as it is done in XCiT for other models.
Our results are reported in Table~\ref{tab:sem_seg}.
We observe that vanilla ViTs trained with our training recipes have a better FLOPs-accuracy trade-off than recent architectures like XCiT or Swin.
\begin{table}[t]
       \caption{\textbf{ADE20k semantic segmentation} performance using UperNet \cite{xiao2018unified} (in comparable settings~\cite{Dong2021CSWinTA,el2021xcit,liu2021swin}). All models are pre-trained on ImageNet-1k  except models with  symbol that are pre-trained on ImageNet-21k. We report the pre-training resolution used on ImageNet-1k and ImageNet-21k.
       \smallskip
       \label{tab:sem_seg}}
        \centering
        \scalebox{0.8}{
        \begin{tabular}{l|l|cc|cc}
        \toprule
             \multirow{2}{*}{Backbone} & Pre-training & \multicolumn{4}{c}{UperNet}  \\
             & \multirow{2}{*}{resolution} & \#params & FLOPs & Single scale & Multi-scale  \\
& & () & () & mIoU  & mIoU \\
            \midrule 
             ResNet50 &  & 66.5 & \_ & 42.0 & \_ \\
             DeiT-S  &  & 52.0 & 1099 & \_ & 44.0 \\
             XciT-T12/16 &  & 34.2 & \pzo874  & 41.5  & \_\\
             XciT-T12/8  &  & 33.9 & 942 & 43.5 & \_ \\
             Swin-T   &  & 59.9 & \pzo945 & 44.5 & 46.1 \\

             \rowcolor{Goldenrod}
            Our ViT-T &  & 10.9 & 148 & 40.1 & 41.8 \\
            \rowcolor{Goldenrod}
            Our ViT-S &  & 41.7 & 588 & \textbf{45.6} & \textbf{46.8}\\
            \midrule
             XciT-M24/16  &  & 112.2 & 1213 & 47.6 & \_ \\
             XciT-M24/8  &  & 110.0 & 2161  & 48.4 & \_ \\

             PatchConvNet-B60 &  & 140.6 & 1258  & 48.1  & 48.6 \\


             PatchConvNet-B120 &  & 229.8 & 1550  & 49.4  & 50.3\\
             MAE ViT-B &  & 127.7 & 1283 & 48.1 &  \_ \\
             Swin-B  &  & 121.0 & 1188  & 48.1 & 49.7 \\
             \rowcolor{Goldenrod}
            Our ViT-B &  & 127.7 & 1283 & 49.3 &  50.2\\
            \rowcolor{Goldenrod}
            Our ViT-L &  & 353.6 & 2231 & \textbf{51.5} & \textbf{52.0}  \\            
            \midrule 

            

            
            
            PatchConvNet--B60 &  & 140.6 & 1258  & 50.5  & 51.1 \\

             PatchConvNet-L120 &  & 383.7 & 2086  &  52.2 & 52.9   \\
            Swin-B () &  & 121.0 & 1841  & 50.0 & 51.6 \\
            Swin-L () &  & 234.0 & 3230  & \_ & 53.5 \\
            \rowcolor{Goldenrod}
            Our ViT-B &  & 127.7 & 1283 & 51.8 & 52.8\\
            \rowcolor{Goldenrod}
            Our ViT-B &  & 127.7 & 1283 & 53.4 & 54.1\\
            \rowcolor{Goldenrod}
            Our ViT-L &  & 353.6 & 2231 &  53.8 & 54.7 \\
            \rowcolor{Goldenrod}
            Our ViT-L &  & 353.6 & 2231 &  \textbf{54.6} & \textbf{55.6} \\
        \bottomrule     
        \end{tabular}
        } 
            \bigskip 
\end{table}

\subsubsection{Training with others architectures}

In Table~\ref{tab:comp_arch_train} we measure the top-1 accuracy on ImageNet-val, ImageNet-real and ImageNet-v2 with different architecture train with our training procedure at resolution  on ImageNet-1k only. 
We can observe that for some architectures like PiT or CaiT our training method will improve the performance. For some others like TNT our approach is neutral and for architectures like Swin it decreases the performance. This is consistent with the findings of Wightman et al.~\cite{wightman2021resnet} and illustrates the need to improve the training procedure in conjunction to the architecture to obtain robust conclusions.
Indeed, adjusting these architectures while keeping the training procedure fixed can probably have the same effect as keeping the architecture fixed and adjusting the training procedure. 
That means that with a fixed training procedure we can have an overfitting of an architecture for a given training procedure.
In order to take overfitting into account we perform our measurements on the ImageNet val and ImageNet-v2 to quantify the amount of overfitting.

\begin{table}
    \centering
    \scalebox{0.85}{
    \begin{tabular}{l|cc|c|ccc}
    \toprule
         \multirow{2}{*}{Model} & Params & Flops & \multicolumn{4}{c}{ImageNet-1k}  \\
         & () &  () & orig. &  val & real & v2 \\
        \midrule
        \textcolor{lightgray}{ViT-S~\cite{Touvron2020TrainingDI}}                      & \textcolor{lightgray}{22.0} & \textcolor{lightgray}{4.6} &\textcolor{lightgray}{79.8} & \textcolor{lightgray}{80.4} & \textcolor{lightgray}{86.1} & \textcolor{lightgray}{69.7} \\
        \textcolor{lightgray}{ViT-B~\cite{dosovitskiy2020image,Touvron2020TrainingDI} }& \textcolor{lightgray}{86.6} & \textcolor{lightgray}{17.6} &\textcolor{lightgray}{81.8} & \textcolor{lightgray}{83.1} & \textcolor{lightgray}{87.7} & \textcolor{lightgray}{72.6} \\
         \midrule
         PiT-S~\cite{Heo2021RethinkingSD}  & 23.5 & 2.9 & \textcolor{lightgray}{80.9} & 80.4 & 86.1 & 69.2 \\
         PiT-B~\cite{Heo2021RethinkingSD}  & 73.8 & 12.5 & \textcolor{lightgray}{82.0} & 82.4 & 86.8 & 72.0 \\
         \midrule
         TNT-S~\cite{Yuan2021TokenstoTokenVT} & 23.8 & 5.2 & \textcolor{lightgray}{81.5} & 81.4 & 87.2 & 70.6 \\
         TNT-B~\cite{Yuan2021TokenstoTokenVT} & 65.6 & 14.1 & \textcolor{lightgray}{82.9} & 82.9 & 87.6 & 72.2\\
         \midrule
         
         ConViT-S~\cite{dAscoli2021ConViTIV} & 27.8 & 5.8 & \textcolor{lightgray}{81.3} & 81.3 & 87.0 & 70.3 \\
         ConViT-B~\cite{dAscoli2021ConViTIV} & 86.5 & 17.5 & \textcolor{lightgray}{82.4} & 82.0 & 86.7 & 71.3 \\
         \midrule
         Swin-S~\cite{liu2021swin} & 49.6 & 8.7 &\textcolor{lightgray}{83.0} & 82.1 & 86.9  & 70.7 \\
         Swin-B~\cite{liu2021swin} & 87.8 & 15.4 &\textcolor{lightgray}{83.5} & 82.2 & 86.7 & 70.7  \\
          \midrule
CaiT-B12~\cite{touvron2021going} & 100.0 & 18.2 &\textcolor{lightgray}{\_} & 83.3 & 87.7 & 73.3  \\

        \bottomrule
    \end{tabular}}
    \caption{We report the performance reached with our training recipe with 400 epochs at resolution  for other transformers architectures. We have not performed an extensive grid search to adapt the hyper-parameters to each architecture. Our results are overall similar to the ones achieved in the papers where these architectures were originally published (reported in column 'orig.'), except for Swin Transformers, for which we observe a drop on ImageNet-val.
    \label{tab:comp_arch_train}}
\end{table}

