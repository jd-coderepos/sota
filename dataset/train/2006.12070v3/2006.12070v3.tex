
\documentclass{article} \usepackage{iclr2021_conference,times}
\usepackage{natbib}




\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}       \usepackage{nicefrac}       

\usepackage{hyperref}
\usepackage{url}

\usepackage{overpic}

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{float}
\usepackage{empheq}
\usepackage{booktabs}

\usepackage{upgreek}
\usepackage{enumitem}


\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}


\newtheorem{thm}{Theorem}[subsection]

\newcommand {\omri}[1]{{\color{red}\sf{[omri: #1]}}}

\newcommand {\ale}[1]{{\color{red}\sf{[Alejandro: #1]}}}


\newcommand{\sym}{\mathrm{sym}}


\title{Lipschitz Recurrent Neural Networks}



\author{N. Benjamin Erichson \\
	ICSI and UC Berkeley\\
	\texttt{erichson@berkeley.edu} \\
	\And
	Omri Azencot \\
	Ben-Gurion University \\
	\texttt{azencot@cs.bgu.ac.il} \\
	\And
	Alejandro Queiruga \\
	Google Research \\
	\texttt{afq@google.com} \\
	\AND
	Liam Hodgkinson \\
	ICSI and UC Berkeley \\
	\texttt{liam.hodgkinson@berkeley.edu} \\
	\And	
	Michael W. Mahoney \\
	ICSI and UC Berkeley\\
	\texttt{mmahoney@stat.berkeley.edu} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\newcommand{\changed}[1]{{\textcolor{red}{#1}}}


\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Viewing recurrent neural networks (RNNs) as continuous-time dynamical systems, we propose a recurrent unit that describes the hidden state's evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. 
This particular functional form facilitates stability analysis of the long-term behavior of the recurrent unit using tools from nonlinear systems theory. In turn, this enables architectural design decisions before experimentation.
Sufficient conditions for global stability of the recurrent unit are obtained, motivating a novel scheme for constructing hidden-to-hidden matrices.
Our experiments demonstrate that the Lipschitz RNN can outperform existing recurrent units on a range of benchmark tasks, including computer vision, language modeling and speech prediction tasks.
Finally, through Hessian-based analysis we demonstrate that our Lipschitz recurrent unit is more robust with respect to input and parameter perturbations as compared to other continuous-time RNNs.
\end{abstract}


\section{Introduction}

Many interesting problems exhibit temporal structures that can be modeled with recurrent neural networks (RNNs), including problems in robotics, system identification, natural language processing, and machine learning control. In contrast to feed-forward neural networks, RNNs consist of one or more recurrent units that are designed to have dynamical (recurrent) properties, thereby enabling them to acquire some form of internal memory. This equips RNNs with the ability to discover and exploit spatiotemporal patterns, such as symmetries and periodic structures~\citep{hinton1986learning}. However, RNNs are known to have stability issues and are notoriously difficult to train, most notably due to the vanishing and exploding gradients problem~\citep{bengio1994learning,pascanu2013difficulty}. 

Several recurrent models deal with the vanishing and exploding gradients issue by restricting the hidden-to-hidden weight matrix to be an element of the orthogonal group~\citep{arjovsky2016unitary,wisdom2016full,mhammedi2017efficient,vorontsov2017orthogonality,lezcano2019cheap}. 
While such an approach is advantageous in maintaining long-range memory, it limits the expressivity of the model. To address this issue, recent work suggested to construct hidden-to-hidden weights which have unit norm eigenvalues and can be nonnormal~\citep{kerg2019non}. 
Another approach for resolving the exploding/vanishing gradient problem has recently been proposed by \citet{Kag2020RNNs}, who formulate the recurrent units as a differential equation and update the hidden states based on the difference between predicted and previous states.

In this work, we address these challenges by viewing RNNs as dynamical systems whose temporal evolution is governed by an abstract system of differential equations with an external input. The data are formulated in continuous-time where the external input is defined by the function , and the target signal is defined as . Based on insights from dynamical systems theory, we propose a continuous-time Lipschitz recurrent neural network with the functional form
[left=\empheqlbrace]{align} 
		\,\, \dot{h} \,\,\, & = \,\,\, {A_{\beta_A,\gamma_A}}h +  \tanh({W_{\beta_W,\gamma_W}}h + Ux + b) \ ,  \\
		\,\, y  \,\,\, & = \,\,\, Dh \ , \label{eq:rnn_de2}
	
{where the hidden-to-hidden matrices  and  are of the form
[left=\empheqlbrace]{align}
\,\, A_{\beta_A,\gamma_A} \,\,\, & =  (1-\beta_A) (M_A + M_A^T) +  \beta_A(M_A - M_A^T) - \gamma_A I \\
\,\, W_{\beta_W,\gamma_W} \,\,\, & =  (1-\beta_W) (M_W + M_W^T) +  \beta_W(M_W - M_W^T) - \gamma_W I,
\,\,

where ,  are tunable parameters and  are trainable matrices.
}
Here,  is a function of time  that represents an internal (hidden) state, and  is its time derivative. The hidden state represents the  memory that the system has of its past.  The function in Eq.~(\ref{eq:rnn_de}) is parameterized by the hidden-to-hidden weight matrices  and , the input-to-hidden encoder matrix , and an offset . The function in Eq.~(\ref{eq:rnn_de2}) is parameterized by the hidden-to-output decoder matrix . Nonlinearity is introduced via the 1-Lipschitz  activation function. 
While RNNs that are governed by differential equations with an additive structure have been studied before~\citep{zhang2014comprehensive}, the specific formulation that we propose in (\ref{eq:rnn_de}) and our theoretical analysis are distinct. 


Treating RNNs as dynamical systems enables studying the long-term behavior of the hidden state with tools from stability analysis. From this point of view, an unstable unit presents an exploding gradient problem, while a stable unit has well-behaved gradients over time~\citep{miller2018stable}. However, a stable recurrent unit can suffer from vanishing gradients, leading to catastrophic forgetting~\citep{hochreiter1997long}. Thus, we opt for a stable model whose dynamics do not (or only slowly do) decay over time. Importantly, stability is also a statement about the robustness of neural units with respect to input perturbations, \ie, stable models are less sensitive to small perturbations compared to unstable models. Recently, \citet{chang2018antisymmetricrnn} explored the stability of linearized RNNs and provided a {\em local} stability guarantee based on the Jacobian. In contrast, the particular structure of our unit~(\ref{eq:rnn_de}) allows us to obtain guarantees of {\em global exponential stability} using control theoretical arguments.
In turn, the sufficient conditions for global stability motivate a novel symmetric-skew decomposition based scheme for constructing hidden-to-hidden matrices.
This scheme alleviates exploding and vanishing gradients, while remaining highly expressive. 

In summary, the main contributions of this work are as follows:
\begin{itemize}\vspace{-0.15cm}
	\item First, in Section~\ref{sec:Stability}, using control theoretical arguments in a direct Lyapunov approach, we provide sufficient conditions for \emph{global exponential stability} of the Lipschitz RNN unit (Theorem \ref{thm:StabilityMain}). Global stability is advantageous over local stability results since it guarantees non-exploding gradients regardless of the state. In the special case where  is symmetric, we find that these conditions agree with those in classical theoretical analyses (Lemma \ref{lem:CGH}).


	\item Next, in Section~\ref{sec:hidden_scheme}, drawing from our stability analysis, we propose a novel scheme based on the \emph{symmetric-skew decomposition} for constructing hidden-to-hidden matrices. This scheme \emph{mitigates the vanishing and exploding gradients problem}, while obtaining \emph{highly expressive} hidden-to-hidden matrices.
	
	\item In Section~\ref{sec:experiments}, we show that our Lipschitz RNN has the \emph{ability to outperform state-of-the-art recurrent units} on computer vision, language modeling and speech prediction tasks.
Further, our results show that the \emph{higher-order explicit midpoint time integrator} improves the predictive accuracy as compared to using the simpler one-step forward Euler scheme.  

	\item 
	Finally, in Section~\ref{sec:sensitivity}), we study our Lipschitz RNN via the lens of the Hessian and show that it is \emph{robust with respect to parameter perturbations}; we also show that our model is \emph{more robust with respect to input perturbations}, compared to other continuous-time RNNs.
	
\end{itemize}



\section{Related Work}

The problem of vanishing and exploding gradients (and stability) have a storied history in the study of RNNs.
Below, we summarize two particular approaches to the problem (constructing unitary/orthogonal RNNs and the dynamical systems viewpoint) that have gained significant attention.


\textbf{Unitary and orthogonal RNNs.}
Unitary recurrent units have received attention recently, largely due to \citet{arjovsky2016unitary} showing that unitary hidden-to-hidden matrices alleviate the vanishing and exploding gradients problem. Several other unitary and orthogonal models have also been proposed~\citep{wisdom2016full,mhammedi2017efficient,jing2017tunable,vorontsov2017orthogonality,jose2018kronecker}. While these approaches stabilize the training process of RNNs considerably, they also limit their expressivity and their prediction accuracy. Further, unitary RNNs are expensive to train, as they typically involve the computation of a matrix inverse at each step of training. Recent work by \citet{lezcano2019cheap} overcame some of these limitations. By leveraging concepts from Riemannian geometry and Lie group theory, their recurrent unit exhibits improved expressivity and predictive accuracy on a range of benchmark tasks while also being efficient to train. Another competitive recurrent design was recently proposed by \citet{kerg2019non}. Their approach is based on the Schur decomposition, and it enables the construction of general nonnormal hidden-to-hidden matrices with unit-norm eigenvalues.

\textbf{Dynamical systems inspired RNNs.}
The continuous time view of RNNs has a long history in the neurodynamics community as it provides higher flexibility and increased interpretability~\citep{pineda1988dynamics,pearlmutter1995gradient,zhang2014comprehensive}.
In particular, RNNs that are governed by differential equations with an additive structure have been extensively studied from a theoretical point of view~\citep{funahashi1993approximation,kim1996nonlinear,chow2000modeling,hu2002global,li2005approximation,trischler2016synthesis}. See \cite{zhang2014comprehensive} for a comprehensive survey of continuous-time RNNs and their stability properties.

Recently, several works have adopted the dynamical systems perspective to alleviate the challenges of training RNNs which are related to the vanishing and exploding gradients problem. For non-sequential data, \cite{NEURIPS2018_7bd28f15} proposed a negative-definite parameterization for enforcing stability in the RNN during training. \cite{chang2018antisymmetricrnn} introduced an antisymmetric hidden-to-hidden weight matrix and provided guarantees for local stability.
\citet{Kag2020RNNs} have proposed a differential equation based formulation for resolving the exploding/vanishing gradients problem by updating the hidden states based on the difference between predicted and previous states.
\citet{niu2019recurrent} employed numerical methods for differential equations to study the stability of RNNs.

Another line of recent work has focused on continuous-time models that deal with irregular sampled time-series, missing values and multidimensional time series. \citet{rubanova2019latent} and \citet{NIPS2019_8957} formulated novel recurrent models based on the theory of differential equations and their discrete integration. \cite{lechner2020learning} extended these ordinary differential equation (ODE) based models and addresses the issue of vanishing and exploding gradients by designing an ODE-model that is based on the idea of long short-term memory (LSTM). This ODE-LSTM outperforms the continuous-time LSTM~\citep{mei2017neural} as well as the GRU-D model~\citep{che2018recurrent} that is based on a gated recurrent unit (GRU).

The link between dynamical systems and models for forecasting sequential data also provides the opportunity to incorporate physical knowledge into the learning process which improves the generalization performance, robustness, and ability to learn with limited data~\citep{chen2019symplectic}.


\section{Stability Analysis of Lipschitz Recurrent Units}
\label{sec:Stability}

One of the key contributions in this work is that we prove that model~(\ref{eq:rnn_de}) is \emph{globally exponentially stable} under some mild conditions on  and . Namely, for \emph{any} initial hidden state we can guarantee that our Lipschitz unit converges to an equilibrium if it exists, and therefore, gradients can never explode. We improve upon recent work on stability in recurrent models, which provide only a local analysis, see e.g.,~\citep{chang2018antisymmetricrnn}. In fact, global exponential stability is among the strongest notions of stability in nonlinear systems theory, implying all other forms of Lyapunov stability about the equilibrium ~\cite[Definitions 4.4 and 4.5]{khalil2002nonlinear}. 
\begin{definition}
A point  is an \emph{equilibrium point} of  if  for all . Such a point is \emph{globally exponentially stable} if there exists some  and  such that for any choice of initial values ,

\end{definition}
The presence of a Lipschitz nonlinearity in~(\ref{eq:rnn_de}) plays an important role in our analysis. While we focus on  in our experiments, our proof is more general and is applicable to models whose nonlinearity  is an -Lipschitz function. Specifically, we consider the general model

for which we have the following stability result. In the following, we let  and  denote the smallest and largest singular values of the hidden-to-hidden matrices, respectively.

\begin{theorem}
\label{thm:StabilityMain}
Let  be an equilibrium point of a differential equation of the form (\ref{eq:RNNODEMain}) for some . The point  is globally exponentially stable if the eigenvalues of  are strictly negative,  is non-singular, and either (a) ; or (b)   is monotone non-decreasing,  is negative definite, and  is positive definite.
\end{theorem}
The two cases show that global exponential stability is guaranteed if either (a) the matrix  has eigenvalues with real parts sufficiently negative to counteract expanding trajectories in the nonlinearity; or (b) the nonlinearity is monotone, both  and  yield stable linear systems , , and  have sufficiently similar eigenvectors. In practice, case (b) occasionally holds, but is challenging to ensure without assuming specific structure on , . Because such assumptions could limit the expressiveness of the model, the next section will develop a tunable formulation for  and  with the capacity to ensure that case (a) holds. 

In Appendix~\ref{sxn:proof_of_first_thm}, we provide a proof of Theorem \ref{thm:StabilityMain} using a direct Lyapunov approach. One advantage of this approach is that the driving input  is permitted to evolve in time arbitrarily in the analysis. The proof relies on the classical Kalman-Yakubovich-Popov lemma and circle criterion from control theory --- to our knowledge, these tools have not been applied in the modern RNN literature, and we hope our proof can illustrate their value to the community.

In the special case where  is symmetric and  constant, we show that we can also inherit criteria for both local and global stability from a class of well-studied \emph{Cohen--Grossberg--Hopfield models}.
\begin{lemma}
\label{lem:CGH}
Suppose that  is symmetric and  is nonsingular. There exists a diagonal matrix , and nonsingular matrices  such that an equilibrium of (\ref{eq:RNNODEMain}) is (globally exponentially) stable if and only if there is a corresponding (globally exponentially) stable equilibrium for the system

\end{lemma}
For a thorough review of analyses of (\ref{eq:CGH}), see~\citep{zhang2014comprehensive}. In this special case, the criteria in Theorem \ref{thm:StabilityMain} coincide with those obtained for the corresponding model (\ref{eq:CGH}). However, in practice, we will not choose  to be symmetric.


\section{Symmetric-Skew Hidden-to-Hidden Matrices}
\label{sec:hidden_scheme}

In this section we propose a novel scheme for constructing hidden-to-hidden matrices.
Specifically, based on the successful application of skew-symmetric hidden-to-hidden weights in several recent recurrent architectures, and our stability criteria in Theorem \ref{thm:StabilityMain}, we propose an effective \emph{symmetric-skew decomposition} for hidden matrices. Our decomposition allows for a simple control of the matrix spectrum while retaining its wide expressive range, enabling us to satisfy the spectral constraints derived in the previous section on both  and . The proposed scheme also accounts for the issue of vanishing gradients by reducing the magnitude of large negative eigenvalues.

Recently, several methods used skew-symmetric matrices, \ie,  to parameterize the recurrent weights , see \eg,~\citep{wisdom2016full,chang2018antisymmetricrnn}. From a stability analysis viewpoint, there are two main advantages for using skew-symmetric weights: these matrices generate the orthogonal group whose elements are isometric maps and thus preserve norms \citep{lezcano2019cheap}; and the spectrum of skew-symmetric matrices is purely imaginary which simplifies stability analysis \citep{chang2018antisymmetricrnn}.
The main shortcoming of this parametrization is its reduced expressivity, as these matrices have fewer than half of the parameters of a full matrix~\citep{kerg2019non}. The latter limiting aspect can be explained from a dynamical systems perspective: skew-symmetric matrices can only describe oscillatory behavior, whereas a matrix whose eigenvalues have nonzero real parts can also encode viable growth and decay information.

To address the expressivity issue, we aim for hidden matrices which on the one hand, allow to control the expansion and shrinkage of their associated trajectories, and on the other hand, will be sampled from a \emph{superset} of the skew-symmetric matrices. Our analysis in Theorem~\ref{thm:StabilityMain} guarantees that Lipschitz recurrent units maintain non-expanding trajectories under mild conditions on  and . Unfortunately, this proposition does not provide any information with respect to the shrinkage of paths. Here, we opt for a system whose expansion and shrinkage can be easily controlled. Formally, the latter requirement is equivalent to designing hidden weights  with small , where  denotes the real part of . A system of the form (\ref{eq:RNNODEMain}) whose matrices  and  exhibit small spectra and satisfy the conditions of Theorem~\ref{thm:StabilityMain}, will exhibit dynamics with moderate decay and growth behavior and alleviate the problem of exploding and vanishing gradients. To this end, we propose the following symmetric-skew decomposition for constructing hidden matrices:

where  is a weight matrix, and ,  are tuning parameters. In the case , we recover a skew-symmetric matrix, \ie, . The construction  is useful as we can easily bound its spectrum via the parameters  and , as we show in the next proposition. 
\begin{proposition}
\label{thm:our_second_thm}
Let  satisfy (\ref{eq:engergy_preserving_scheme}), and let . The real parts  of the eigenvalues of , as well as the eigenvalues of , lie in the interval
\end{proposition}
A proof is provided in Appendix~\ref{sxn:proof_of_second_thm}. We infer that   controls the width of the spectrum, while increasing  shifts the spectrum to the left along the real axis, thus enforcing eigenvalues with non-positive real parts. Choosing our hidden-to-hidden matrices to be  and  of the form (\ref{eq:engergy_preserving_scheme}) for different values of  and , we can ensure small spectra and satisfy the conditions of Theorem~\ref{thm:StabilityMain} as desired. 
Note, that different tuning parameters  and  affect the stability behavior of the Lipschitz recurrent unit. 
This is illustrated in Figure~\ref{fig:stability_simulation}, where different values for  and  are used to construct both  and  and applied to learning simple pendulum dynamics.


\begin{figure}[!t]
	\centering
	\begin{subfigure}[t]{0.23\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/sim_beta05.pdf}
\end{overpic}\vspace{-0.1cm}		
		\caption{, }
	\end{subfigure}~
	\begin{subfigure}[t]{0.23\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/sim_beta075.pdf} 
\end{overpic}\vspace{-0.1cm}			
		\caption{, }
	\end{subfigure}~
	\begin{subfigure}[t]{0.23\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/sim_beta090.pdf} 
\end{overpic}\vspace{-0.1cm}			
		\caption{, }
	\end{subfigure}~
	\begin{subfigure}[t]{0.23\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/sim_beta1.pdf} 
\end{overpic}\vspace{-0.1cm}			
		\caption{, }
	\end{subfigure}

	\caption{Vector fields of hidden states that are governed by Eq.~(\ref{eq:rnn_de}) trained for simple pendulum dynamics. In (a), an unstable model is shown. In (b) and (c), it can be seen that we yield models that are asymptotically stable,i.e., all trajectories are attracted by an equilibrium point. In contrast, in (d), a skew-symmetric parameterization leads to a stable model without an attracting equilibrium.}	
	
	\label{fig:stability_simulation}
\end{figure}


One cannot guarantee that model parameters will remain in the stability region during training. However, we can show that when  is taken to be close to one, the eigenvalues of  and  (which dictate the stability of the RNN) change slowly during training. Let  denote the change in a function  depending on the parameters of the RNN (\ref{eq:rnn_de}) after one step of gradient descent with step size  with respect to some loss . For a matrix , we let  denote the -th singular value of . We have the following lemma.
\begin{lemma}
\label{lem:Training}
As , .
\end{lemma}
Therefore, provided both the initial and optimal parameters lie within the stability region, the model parameters will remain in the stability region for longer periods of time with high probability as . Further empirical evidence of parameters often remaining in the stability region during training are provided alongside the proof of Lemma \ref{lem:Training} in the Appendix (see Figure \ref{fig:eigs}).

\section{Training Continuous-time Lipschitz Recurrent Units}
\label{sxn:lipshitz_recurrent_unit}

ODEs such as Eq.~(\ref{eq:rnn_de}) can be approximately solved by employing numerical integrators. In scientific computing, numerical integration is a well studied field that provides well understood techniques~\citep{LeVeque}. Recent literature has also introduced new approaches which are designed with neural network frameworks in mind~\citep{chen2018neural}. 

To learn the weights  and , we discretize the continuous model using one step of a numerical integrator between sequence entries. 
In what follows, a subscript  denotes discrete time indices,  represents the time difference between a pair of consecutive data points. Letting  so that , the exact and approximate solutions for  given  are given by

where  represents one step of a numerical integration scheme whose application yields an approximate solution for  given  using one or more evaluations of .

We consider both the explicit (forward) Euler scheme, 

as well as the midpoint method which is a two-stage explicit Runge-Kutta scheme (RK2),

where  is an intermediate hidden state.
The RK2 scheme can potentially improve the performance since the scheme is more accurate, however, this scheme also requires twice as many function evaluations as compared to the forward Euler scheme. 
Given a  and  that yields a globally exponentially stable continuous model,  can always be chosen so that the model remains in the stability region of forward Euler and RK2~\citep{LeVeque}.

\section{Empirical Evaluation}\label{sec:experiments}

In  this  section,  we  evaluate the  performance  of the Lipschitz RNN and compare it to other state-of-the-art methods.
The model is applied to ordered and permuted pixel-by-pixel MNIST classification, as well as to audio data using the TIMIT dataset.
We show the sensitivity with respect to to random initialization in Appendix~\ref{sec:app_results}. 
Appendix~\ref{sec:app_results} also contains additional results for: pixel-by-pixel CIFAR-10 and a noise-padded version of CIFAR-10; as well as for character level and word level prediction using the Penn Tree Bank (PTB) dataset.
All of these tasks require that the recurrent unit learns long-term dependencies: that is, the hidden-to-hidden matrices need to have sufficient memory to remember information from far in the past.

\begin{table}[!b]
	\caption{Evaluation accuracy on ordered and permuted pixel-by-pixel MNIST.}
	\label{tab:mnist-table}
	\centering
	\scalebox{0.92}{
		\begin{tabular}{l c c c c c c}
			\toprule
			Name                  &  ordered & permuted  & N & \# params\\
			\midrule 
			
			LSTM baseline by~\citep{arjovsky2016unitary} & 97.3\% & 92.7\% &128 & 68K\\        

			MomentumLSTM~\citep{nguyen2020momentumrnn} & 99.1\% & 94.7\% & 256 & 270K\\ 			
			
			Unitary RNN~\citep{arjovsky2016unitary} & 95.1\% & 91.4\% &512 & 9K\\  		
			
			Full Capacity Unitary RNN~\citep{wisdom2016full} & 96.9\% & 94.1\% &512 & 270K\\  
			
			Soft orth. RNN~\citep{vorontsov2017orthogonality} & 94.1\% & 91.4\% &128 & 18K\\    		
			
			
			Kronecker RNN~\citep{jose2018kronecker} & 96.4\% & 94.5\% & 512 & 11K\\
			
			
			Antisymmteric RNN~\citep{chang2018antisymmetricrnn}  & 98.0\% & 95.8\% &128 & 10K\\
			
			Incremental RNN~\citep{Kag2020RNNs} & 98.1\% & 95.6\% & 128 & 4K/8K\\
			
			
			Exponential RNN~\citep{lezcano2019cheap} & 98.4\% & 96.2\% & 360 & 69K\\
			
			Sequential NAIS-Net~\citep{NEURIPS2018_7bd28f15}     &    94.3\%     &  90.8\%         & 128              &  18K  \\      
			
\midrule
			Lipschitz RNN using Euler (ours)  & 99.0\% & 94.2\% & 64 & 9K\\
			Lipschitz RNN using RK2 (ours)  &  99.1\% & 94.2\% & 64 & 9K\\	
			\midrule
			Lipschitz RNN using Euler (ours)  & \textbf{99.4\%} & \textbf{96.3\%} & 128 & 34K\\
			Lipschitz RNN using RK2 (ours)  & 99.3\% & 96.2\% & 128 & 34K\\			
			\bottomrule
	\end{tabular}}
\end{table}


\subsection{Ordered and Permuted Pixel-by-Pixel MNIST}\label{sec:mnist}

The pixel-by-pixel MNIST task tests long range dependency by
sequentially presenting  pixels to the recurrent unit, \ie, the RNN processes one pixel at a time~\citep{le2015simple}. 
At the end of the sequence, the learned hidden state is used to predict the class membership probability of the input image. This task requires that the RNN has a sufficient long-term memory in order to discriminate between different classes. A more challenging variation to this task is to operate on a fixed random permutation of the input sequence.


Table~\ref{tab:mnist-table} provides a summary of our results.
The Lipschitz RNN, with hidden dimension of  and trained with the forward Euler and RK2 scheme, achieves  and  accuracy on the ordered pixel-by-pixel MNIST task. 
For the permuted task, the model trained with forward Euler achieves  accuracy, whereas the model trained with RK2 achieves  accuracy.
Hence, our Lipschitz recurrent unit outperforms state-of-the-art RNNs on both tasks and is competitive even when a hidden dimension of  is used, however, it can be seen that a larger unit with more capacity is advantageous for the permuted task.
Our results show that we significantly outperform the Antisymmetric RNN~\citep{chang2018antisymmetricrnn} on the ordered tasks, while using fewer weights. That shows that the antisymmetric weight paramterization is limiting the expressivity of the recurrent unit.
The exponential RNN is the next most competitive model, yet this model requires a larger hidden-to-hidden unit to perform well on the two considered tasks. 









\begin{table}[!t]
	\caption{Evaluation on TIMIT using 1 layer models.  The mean squared error (MSE) is computes the distance between the predicted and actual log-magnitudes of each predicted frame in the sequence. }
	\label{tab:timit}
	\centering
	\scalebox{0.91}{
		\begin{tabular}{l c c c c c c}
			\toprule
			Name                  & val. MSE & test MSE  & N & \# params\\
			\midrule 
			
			LSTM~\citep{helfrich2018orthogonal} & 13.66 & 12.62 & 158 & 200K\\ 
			
			LSTM~\citep{nguyen2020momentumrnn}  & 9.33 & 9.37 & 158 & 200K\\ 
			
			MomentumLSTM~\citep{nguyen2020momentumrnn} & 5.86 & 5.87 & 158 & 200K\\ 
			
			SRLSTM~\citep{nguyen2020momentumrnn} & 5.81 & 5.83 & 158 & 200K\\ 
			
			
			Full-capacity Unitary RNN~\citep{wisdom2016full} & 14.41 & 14.45 & 256 & 200K\\  		
			
			Cayley RNN~\citep{helfrich2018orthogonal} & 7.97 & 7.36 & 425 & 200K\\  	
			
			
			Exponential RNN~\citep{lezcano2019cheap}  & 5.52 & 5.48 & 425 & 200K\\  									 						
			\midrule
			Lipschitz RNN using Euler (ours) &  2.95 &  2.82   & 256 & 198K\\

			Lipschitz RNN using RK2 (ours) & \textbf{2.86} & \textbf{2.76} & 256 & 198K\\			
			\bottomrule
	\end{tabular}}
\end{table}


\subsection{TIMIT}

Next, we consider the TIMIT dataset~\citep{Garofolo} to study the capabilities of the Lipschitz RNN for speech prediction using audio data.
For our experiments, we used the publicly available implementation of this task by~\citet{lezcano2019cheap}.
This implementation applies the preprocessing steps suggested by \citet{wisdom2016full}: (i) downsample each audio sequence to 8kHz; (ii) process the downsampled sequences with a short-time Fourier transform using a Hann window of 256 samples and a window hop of 128 samples; and (iii) normalize the log-magnitude of the Fourier amplitudes.
We obtain a set of frames that each have 129 complex-valued Fourier amplitudes and the task is to predict the log-magnitude of future frames. 
To compare our results with those of other models, we used the common train / validation / test split: 3690 utterances from 462 speakers for training, 192 utterances for validation, and 400 utterances for testing.  

Table~\ref{tab:timit} lists the results for the Lipschitz recurrent unit as well as for several benchmark models. It can be seen that the Lipschitz RNN outperforms other state-of-the-art models for a fixed number of parameters (K). In particular, LSTMs do not perform well on this task, however, the recently proposed momentum based LSTMs~\citep{nguyen2020momentumrnn} have improvemed performance.
Interestingly, the RK2 scheme leads to a better performance since this scheme provides more accurate approximations for the intermediate states. 




\section{Robustness with Respect to Perturbations}\label{sec:sensitivity}



An important consideration beyond accuracy is robustness with respect to input and parameter perturbations. We consider a Hessian-based analysis and noise-response analysis of different continuous-time recurrent units and train the models on MNIST. Here, we reshape each MNIST thumbnail into sequences of length  so that each input has dimension .
We consider this simpler problem so that all models obtain roughly the same training loss. Here we use stochastic gradient decent (SGD) with momentum to train the models.

Eigenanalysis of the Hessian provides a tool for studying various aspects of neural networks~\citep{hochreiter1997flat,sagun2017empirical,ghorbani2019investigation}.
Here, we study the Hessian  spectrum with respect to the model parameters of the recurrent unit using PyHessian~\citep{YGKM19_pyhessian_TR}. 
The Hessian provides us with insights about the curvature of the loss function .  
This is because the Hessian is defined as the derivatives of the gradients, and thus the Hessian eigenvalues describe the change in the gradient of  as we take an infinitesimal step into a given direction. 
The eigenvectors span the (local) surface of the loss function at a given point, and the corresponding eigenvalue determines the curvature in the direction of the eigenvectors. 
This means that larger eigenvalues indicate a larger curvature, \ie, greater sensitivity, and the sign of the eigenvalues determines whether the curvature will be positive or negative.


\begin{table}[!b]
\caption{Summary of Hessian-based robustness metrics and resilience to adversarial attacks.
		\label{tab:sensitifity}}
	\centering
	\scalebox{0.91}{
		\begin{tabular}{lcccccccccc} \toprule
			Model                  & PGD  &DF & DF &   &  &  \\
			\midrule


			Neural ODE RNN            & 88.5\%  & 69.6\%   & 44.5\%  & 0.30 & 4.7 & 37.6\\
			Antisymmetric  RNN       & 84.7\%  & 83.4\%   & 44.3\%  & 0.24 & 4.8 & 35.5\\
			Lipschitz RNN (ours)    & \textbf{93.0}\%  & \textbf{89.2}\%   & \textbf{54.1}\%   &  \textbf{0.14} & \textbf{3.1} & \textbf{23.2}\\
			\bottomrule 
	\end{tabular}}
\end{table}

\begin{figure}[!b]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/pertubation_whitenoise.pdf}
			\put(-6,15){\rotatebox{90}{\footnotesize test accuracy}}			
			\put(42,-3){\footnotesize {amount of noise}}  	
		\end{overpic}\vspace{+0.2cm}		
		\caption{White noise perturbations.}\label{fig:perturb_a}
	\end{subfigure}~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/pertubation_sp.pdf} 
\put(42,-3){\footnotesize {amount of noise}}  		
		\end{overpic}\vspace{+0.2cm}			
		\caption{Salt and pepper perturbations.}\label{fig:perturb_b}
	\end{subfigure}	
\caption{Sensitivity with respect to different input perturbations.}
	\label{fig:perturb}
\end{figure}

To demonstrate the advantage of the additional linear term and our weight parameterization, we compare the Lipschitz RNN to two other continuous-time recurrent units. First, we consider a simple neural ODE RNN~\citep{rubanova2019latent} that takes the form

where  is a simple hidden-to-hidden matrix. As a second model we consider the antisymmetric RNN~\citep{chang2018antisymmetricrnn}, that takes the same form as (\ref{eq:odernn}), but uses a skew-symmetric scheme to parameterize the hidden-to-hidden matrix as 
,
where  is a trainable weight matrix and  is a tunable parameter. 

Table~\ref{tab:sensitifity} reports the largest eigenvalue  and the trace of the Hessian .The largest eigenvalue being smaller indicates that our Lipschitz RNN found a flatter minimum, as compared to the simple neural ODE and Antisymmetric RNN.
It is known that such flat minima can be perturbed without significantly changing the loss value~\citep{hochreiter1997flat}. 
Table~\ref{tab:sensitifity} also reports the condition number  of the Hessian.
The condition number  provides a measure for the spread of the eigenvalues of the Hessian.
It is known that first-order methods can slow down in situations where  is large~\citep{bottou2008tradeoffs}. 
The condition number and trace of our Lipshitz RNN being smaller also indicates improved robustness properties.


Next, we study the sensitivity of the response  at time  in terms of the test accuracy with respect to a sequence of perturbed inputs . 
We consider three different perturbations. 
The results for the artificially constructed perturbations are presented in Table~\ref{tab:sensitifity}, showing that the Lipschitz RNN is more resilient to adversarial perturbation.
Here, we have considered the projected gradient decent (PGD)~\citep{goodfellow2014explaining} method with , and the DeepFool method~\citep{moosavi2016deepfool} with  and  norm ball perturbations.
We construct the adversarial examples with full access to the models, using  iterations. The step size for PGD is set to .


Further, Figure~\ref{fig:perturb} shows the results for white noise and salt and pepper noise. It can be seen that the Lipschitz unit is less sensitive to input perturbations, as compared to the simple neural ODE RNN, and the antisymmetric RNN. In addition, we also show the results for an unitary RNN here.


\subsection{Ablation Study}\label{sec:ablation}


The performance of the Lipschitz recurrent unit is due to two main innovations: (i) the additional linear term; and (ii) the scheme for constructing the hidden-to-hidden matrices  and  in Eq.~(\ref{eq:engergy_preserving_scheme}).
Thus, we investigate the effect of both innovations, while keeping all other conditions fixed. More concretely, we consider the following ablation recurrent unit

where  controls the effect of the linear hidden unit. 
Both  and  depend on the parameters , .


Figure~\ref{fig:ablation_a} studies the effect of the linear hidden unit, with  for the ordered task and  for the permuted task. In both cases we use . It can be seen that the test accuracies of both the ordered and permuted pixel-by-pixel MNIST tasks clearly depend on the linear hidden unit. For , our models reduces to simple neural ODE recurrent units (Eq.~(\ref{eq:odernn})). The recurrent unit degenerates for , since the external input is superimposed by the hidden state. 
Figure~\ref{fig:ablation_b} studies the effect of the hidden-to-hidden matrices with respect to . 
It can be seen that  achieves peak performance for the ordered task, and  does so for the permuted task. 
Note that  recovers an skew-symmetric hidden-to-hidden matrix. 

\begin{figure}[!t]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/ablation_alpha.pdf}
			\put(-6,15){\rotatebox{90}{\footnotesize test accuracy}}			
			\put(31,-3){\footnotesize {ablation parameter, }}  	
		\end{overpic}\vspace{+0.3cm}		
		\caption{Effect of the linear term.}\label{fig:ablation_a}
	\end{subfigure}~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/ablation_beta.pdf} 
\put(31,-3){\footnotesize {ablation parameter, }}  		
		\end{overpic}\vspace{+0.3cm}			
		\caption{Effect of Eq.~(\ref{eq:engergy_preserving_scheme}).}\label{fig:ablation_b}
	\end{subfigure}\caption{The ablation study examines the effect of the linear term  (in (a)) and the importance of the Skew-Symmetric Decomposition for constructing the hidden-to-hidden matrices (in (b)).} 
	\label{fig:ablation}
\end{figure}



\section{Conclusion}

Viewing RNNs as continuous-time dynamical systems with input, we have proposed a new Lipschitz recurrent unit that excels on a range of benchmark tasks. 
The special structure of the recurrent unit allows us to obtain guarantees of global exponential stability using control theoretical arguments. 
In turn, the insights from this analysis motivated the symmetric-skew decomposition scheme for constructing hidden-to-hidden matrices, which mitigates the vanishing and exploding gradients problem.
Due to the nice stability properties of the Lipschitz recurrent unit, we also obtain a model that is more robust with respect to input and parameter perturbations as compared to other continuous-time units. 
This behavior is also reflected by the Hessian analysis of the model. 
We expect that the improved robustness will make Lipschitz RNNs more reliable for sensitive applications.
The theoretical results for our symmetric-skew decomposition of parameterizing hidden-to-hidden matrices also directly extend to the convolutional setting. Future work will explore this extension and study the potential advantages of these more parsimonious hidden-to-hidden matrices in combination with our parameterization in practice.
Research code is shared via \href{https://github.com/erichson/LipschitzRNN}{github.com/erichson/LipschitzRNN}.


\clearpage
\subsubsection*{Acknowledgments}
We would like to thank Ed H. Chi for fruitful discussions about physics-informed machine learning and the Antisymmetric RNN.
We are grateful to the generous support from Amazon AWS and Google Cloud.
NBE and MWM would like to acknowledge IARPA (contract W911NF20C0035), NSF, ONR and CLTC for providing partial support of this work. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.


\bibliography{rnn}
\bibliographystyle{iclr2021_conference}

\clearpage
\appendix


\section{Proofs}

\subsection{Proofs of Theorem \ref{thm:StabilityMain} and Lemma \ref{lem:CGH}}
\label{sxn:proof_of_first_thm}

There are numerous ways that one can analyze the global stability of (\ref{eq:RNNODEMain}) through the related model (\ref{eq:CGH}), many of which are discussed in \cite{zhang2014comprehensive}. Instead, here we shall conduct a direct approach and avoid appealing to diagonalization in order to obtain cleaner conditions, and a more straightforward proof that readily applies in the time-inhomogeneous setting.

Our method of choice relies on Lyapunov arguments summarized in the following theorem, which can be found as \cite[Theorem 4.10]{khalil2002nonlinear}. For more details on related Lyapunov theory, see also~\cite{hahn1967stability, sastry2013nonlinear}.
\begin{theorem}
\label{thm:Lyapunov}
	An equilibrium  for  is globally exponentially stable if there exists a continuously differentiable function  such that for all  and ,
	
	for some constants .
	and  for . 
\end{theorem}

To simplify matters, we shall choose a Lyapunov function  that is independent of time. The most common type of Lyapunov function satisfying the conditions of Theorem \ref{thm:Lyapunov} is of the form , where  is a positive definite matrix. One need only show that  for some other positive definite matrix  to guarantee global exponential stability.

The construction of the Lyapunov function  that satisfies the conditions of Theorem \ref{thm:Lyapunov} is accomplished using the Kalman-Yakubovich-Popov lemma, which is a statement regarding \emph{strictly positive real transfer functions}. We use the following definition, equivalent to other standard definitions by \cite[Lemma 6.1]{khalil2002nonlinear}.
\begin{definition}
A function  is \emph{strictly positive real} if it satisfies the following:
\begin{enumerate}[label=(\roman*)]
    \item The poles of  have negative real parts.
    \item  is positive definite for all , where .
    \item Either  is positive definite or it is positive semidefinite and  is positive definite for any  full-rank matrix  such that , where .
\end{enumerate}
\end{definition}
The following is presented in \cite[Lemma 6.3]{khalil2002nonlinear}. 
\begin{lemma}[Kalman-Yakubovich-Popov]
\label{lem:KYP}
Let  be full-rank square matrices. There exists a symmetric positive-definite matrix  and matrices  and a constant  such that

if and only if the \emph{transfer function}  is strictly positive real. In this case, we may take , where  is chosen so that  remains strictly positive real. 
\end{lemma}
A shorter proof for case (a) is available to us through the (multivariable) \emph{circle criterion} --- the following theorem is a corollary of \cite[Theorem 7.1]{khalil2002nonlinear} suitable for our purposes.
\begin{theorem}[Circle Criterion]
\label{thm:CircleCriterion}
The system of differential equations

is globally exponentially stable towards an equilibrium at the origin if  for some  and

is strictly positive real, where . 
\end{theorem}

Both the Kalman-Yakubovich-Popov lemma and the circle criterion are classical results in control theory, and are typically discussed in the setting of feedback systems \cite[Chapter 6, 7]{khalil2002nonlinear}. Our presentation here is less general than the complete formulation, but makes clearer the connection to RNNs. With these tools, we state our proof of Theorem \ref{thm:StabilityMain}.

\begin{proof}[Proof of Theorem \ref{thm:StabilityMain}]
To begin, we shall center the differential equation about the equilibrium. By assumption, there exists  such that . Letting , we find that

It will suffice to show that (\ref{eq:CenterODE}) is globally exponentially stable at the origin.

Let us begin with case (a). The proof follows arguments analogous to \cite[Example 7.1]{khalil2002nonlinear}. Let  denote the transfer function for the system (\ref{eq:CenterODE}). Letting

since  is -Lipschitz, we know that  for any . Therefore, let  denote the transfer function in the circle criterion. Our objective is to show that  is strictly positive real --- by Theorem \ref{thm:CircleCriterion}, this will guarantee the desired global exponential stability of (\ref{eq:RNNODEMain}). First, we need to show that the poles of  have negative real parts. This can only occur when  itself has poles or  is singular. The former case occurs precisely where  is singular, which occurs when  is an eigenvalue of . Since  is assumed to be negative definite,  must have eigenvalues with negative real part by Lemma \ref{lem:SymEigs}, and so the poles of  also have negative real parts. The latter case is more difficult to treat. First, since  and , 

Therefore, we observe that

From the Fan-Hoffman inequality~\citep[Proposition III.5.1]{bhatia2013matrix}, we have that

and since  is negative definite, for any  with ,

Since , it follows that  whenever  has non-negative real part, and so the poles of  must have negative real parts.

Next, we need to show that  is positive definite for all . Observe that

From Sylvester's law of inertia, we may infer that  is positive definite if and only if  is positive definite, where . If we can show that the eigenvalues of  lie strictly within the unit circle, that is,  for all , then  will necessarily be positive definite. From (\ref{eq:TransferSigmaMax}) and (\ref{eq:SigmaMinImag}), we may verify that

Therefore,

by assumption. Finally, since  is positive definite,  is strictly positive real and Theorem \ref{thm:CircleCriterion} applies. 

Now, consider case (b). The proof proceeds in two steps. First, we verify that the transfer function  satisfies the conditions of the Kalman-Yakubovich-Popov lemma. Then, using the matrices , , , and the constant  inferred from the lemma, a Lyapunov function is constructed which satisfies the conditions of Theorem \ref{thm:Lyapunov}, guaranteeing global exponential stability. Once again, condition (i) of Lemma \ref{lem:KYP} is straightforward to verify:  exhibits poles when  is an eigenvalue of , and so the poles of  also have negative real parts. Furthermore, condition (iii) is easily satisfied with  since . To show that condition (ii) holds, observe that for any , letting  for brevity,

Since the inner matrix factor is Hermitian, Sylvester's law of inertia implies that  is positive definite if and only if 

is positive definite. Since  is a Hermitian matrix, it has real eigenvalues, with minimal eigenvalue given by the infimum of the Rayleigh quotient:

By assumption,  has strictly positive eigenvalues, and hence  and  are positive definite. Therefore, Lemma \ref{lem:KYP} applies, and we obtain matrices  and a constant  with the corresponding properties.

Now we may construct our Lyapunov function . Let  and

so that . 
Since  is monotone non-decreasing,  for any . This implies that for each ,  and  have the same sign. In particular, . Now, let  be our Lyapunov function, noting that  is independent of . Taking the derivative of the Lyapunov function over (\ref{eq:CenterODE}) and using the properties of ,

Since  and , it follows that , and hence global exponential stability follows from Theorem \ref{thm:Lyapunov} and positive-definiteness of .
\end{proof}

To finish off discussion regarding the results from Sec. 3, we provide a quick proof of Lemma \ref{lem:CGH} using a simple diagonalization argument.

\begin{proof}[Proof of Lemma \ref{lem:CGH}]
Since  is symmetric and real-valued, by \cite[Theorem 4.1.5]{horn2012matrix}, there exists an orthogonal matrix  and a real diagonal matrix  such that . Letting  where  satisfies (\ref{eq:RNNODEMain}), since , we see that

Therefore,  satisfies (\ref{eq:CGH}) with  and , both of which are nonsingular by orthogonality of . By the same argument, for any equilibrium , taking ,

implying that  is an equilibrium of (\ref{eq:CGH}). Furthermore, since

from orthogonality of . Because every form of Lyapunov stability, both local and global, including global exponential stability, depend only on the norm  \cite[Definitions 4.4 and 4.5]{khalil2002nonlinear},  is stable under any of these forms if and only if  is also stable.
\end{proof}

We remark that the proof of Lemma \ref{lem:CGH} can extend to matrices  which have real eigenvalues and are diagonalizable. These attributes are implied for symmetric matrices. However, they can be difficult to ensure in practice for nonsymmetric matrices without imposing difficult structural constraints.

\subsection{Proof of Proposition \ref{thm:our_second_thm}}
\label{sxn:proof_of_second_thm}

The proof of Proposition \ref{thm:our_second_thm} relies on the following lemma, which we also have made use of several times throughout this work. 
\begin{lemma}
\label{lem:SymEigs}
For any matrix , the real parts of the eigenvalues  are contained in the interval , where . 
\end{lemma}
\begin{proof}
Recall by the min-max theorem, for , where  is the conjugate transpose of , the upper and lower eigenvalues of  satisfy

Let  be an eigenvalue of  with corresponding eigenvector  satisfying . Since ,

Hence, .
\end{proof}


\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/projection_64.pdf}
			\put(-6,16){\rotatebox{90}{\footnotesize magnitude}}			
			\put(31,-3){\footnotesize {tuning parameter, }}  	
		\end{overpic}\vspace{+0.4cm}		
		
		\caption{N=64}
	\end{subfigure}~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/projection_128.pdf} 
\put(31,-3){\footnotesize {tuning parameter, }}  	
		\end{overpic}\vspace{+0.4cm}			
		\caption{N=128}
	\end{subfigure}\vspace{-0.2cm}	
	
	
	\caption{Empirical evaluation of the theoretical bounds (\ref{eq:IntervalContainer}). The red lines track the largest real part and the blue lines track the smallest real part of the eigenvalues of the hidden-to-hidden matrix . Each line corresponds to a different hidden-to-hidden matrix of dimension  in (a) and  in (b). The dashed black lines indicate the theoretical bound for each trial.  }
	\label{fig:projection}
\end{figure}

\begin{proof}[Proof of Proposition \ref{thm:our_second_thm}]
By construction,

and so from Lemma \ref{lem:SymEigs}, both the real parts  of the eigenvalues of  as well as the eigenvalues of  lie in the interval

If , for any eigenvalue  of  with corresponding eigenvector ,

implying that  is an eigenvalue of , and therefore contained in . 
In particular, we find that

as required. Finally, if , then (\ref{eq:IntervalContainer}) still holds, since both intervals collapse to the single point . 
\end{proof}


Figure~\ref{fig:projection} illustrates the effect of  onto the eigenvalues of  with the largest and smallest real parts. 
It can  be seen, both empirically and theoretically, that the real part of the eigenvalues converges towards zero as  tends towards one, \ie, we yield a skew-symmetric matrix with purely imaginary eigenvalues in the limit.
Thus, for a sufficiently large parameter  we yield a system that approximately preserves an ``energy'' for a limited time-horizon 



\subsection{Proof of Lemma \ref{lem:Training}}
First, it follows from Gronwall's inequality that the norm of the final hidden state  is bounded uniformly in . From Weyl's inequalities and the definition of ,

By the chain rule, for each element  of the matrix ,

Now, for any collection of parameters ,

and from Gronwall's inequality,

Since ,

Since , it follows that

and so , and therefore . Similarly, for the matrix ,

and hence . \qedsymbol

In Figure \ref{fig:eigs}, we plot the most positive real part of the eigenvalues of  and  during training for the ordered MNIST task. As  increases, the eigenvalues change less during training, remaining in the stability region provided by case (b) of Theorem \ref{thm:StabilityMain} for more of the training time.


\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/eigs_smnist.pdf}
			\put(-6,9){\rotatebox{90}{\footnotesize real part of eigenvalue}}			
			\put(31,-3){\footnotesize {number of epoch}}  	
		\end{overpic}\vspace{+0.4cm}
		\caption{}
\end{subfigure}~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/eigs_smnist_95.pdf} 
\put(31,-3){\footnotesize {number of epoch}}  	
		\end{overpic}\vspace{+0.4cm}
		\caption{}
\end{subfigure}\vspace{-0.2cm}	
	
	
	\caption{The red lines track the largest real part of the eigenvalues of the hidden-to-hidden matrix  and the blue lines track the largest real part of the eigenvalues of  . We show results for two models trained on the ordered MNIST task with varying .\label{fig:eigs}}
	
\end{figure}

\section{Additional Experiments}\label{sec:app_results}




\subsection{Sensitivity to Random Initialization for MNIST and TIMIT}\label{sec:init}

The hidden matrices are initialized by sampling weights from the normal distribution , where  is the variance, which can be treated as a tuning parameter. In our experiments we typically chose a small ; see the Table~\ref{tab:tuning} for details.
To show that the Lipschitz RNN is insensitive to random initialization, we have trained each model with 10 different seeds. Table~\ref{tab:minmax} shows the maximum, average and minimum values obtained for each task. Note that higher values indicate better performance on the ordered and permuted MNIST tasks, while lower values indicate better performance on the TIMIT task.


\begin{table}[!t]
	\caption{Sensitivity to random initialization evaluated over 10 runs.}
	\label{tab:minmax}
	\centering
	\scalebox{0.91}{
		\begin{tabular}{l c c c c c c c c}
			\toprule
			Solver         & Task     & Minimum  & Average  & Maximum  & N & \# params\\
			\midrule 
			
			Euler & ordered MNIST  & 98.9\% & 99.0\%  & 99.0\% & 64 & 9K\\
			RK2 & ordered MNIST    & 98.9\% & 99.0\%  & 99.1\%  & 64 & 9K\\	
			Euler & ordered MNIST  & 99.0\% & 99.2\%  & 99.4\% & 128 & 34K\\
			RK2 & ordered MNIST    & 98.9\% & 99.1\%  & 99.3\% & 128 & 34K\\
			
			\midrule
			
			Euler & permuted MNIST   & 93.5\% & 93.8\%  & 94.2\% & 64 & 9K\\
			RK2 & permuted MNIST     & 93.5\% & 93.9\%  & 94.2\%  & 64 & 9K\\	
			
			Euler & permuted MNIST  & 95.6\% & 95.9\%  & 96.3\% & 128 & 34K\\
			RK2 & permuted MNIST    & 95.4\% & 95.8\%  & 96.2\%  & 128 & 34K\\
			
			
			\midrule


			Euler  & TIMIT (test MSE)  & 2.82 & 2.98   & 3.10 & 256 & 198K\\
			


			RK2  & TIMIT (test MSE)  & 2.76 & 2.81  & 2.84 & 256 & 198K\\			
			
			\bottomrule
	\end{tabular}}
\end{table}


\subsection{Ordered Pixel-by-Pixel and Noise-Padded CIFAR-10}\label{sec:cifar10}

The pixel-by-pixel CIFAR-10 benchmark problem that has recently been proposed by~\citep{chang2018antisymmetricrnn}. This task is similar to the pixel-by-pixel MNIST task, yet more challenging due to the increased sequence length and the more difficult classification problem.  
Similar to MNIST, we flatten the CIFAR-10 images to construct a sequence of length  in scanline order, where each element of the sequence consists of three pixels (one from each channel).

A variation of this problem is the noise-padded CIFAR-10 problem~\citep{chang2018antisymmetricrnn}, where we consider each row of an image as input at time step . 
The rows from each channel are stacked so that we obtain an input of dimension .
Then, after the  time step which process the 32 row, we start to feed the recurrent unit with independent standard Gaussian noise for  time steps. 
At the final point in , we use the learned hidden state for classification. 
This problem is challenging because only the first  time steps contain signals. Thus, the recurrent unit needs to recall information from the beginning of the process.

Table~\ref{tab:cifar-table} provides a summary of our results. 
Our Lipschitz recurrent unit outperforms both the incremental RNN~\citep{Kag2020RNNs} and the antisymmetric RNN~\citep{chang2018antisymmetricrnn} by a significant margin. 
This impressively demonstrates that the Lipschitz unit enables the stable propagation of signals over long time~horizons.



\begin{table}[!t]
	\caption{Evaluation accuracy on pixel-by-pixel CIFAR-10 and noise padded CIFAR-10.}
	\label{tab:cifar-table}
	\centering
	\scalebox{0.85}{
		\begin{tabular}{l c c c c c c}
			\toprule
			Name                  &  ordered & noise padded  & N & \# params\\
			\midrule 
			
			LSTM baseline by~\citep{chang2018antisymmetricrnn} & 59.7\% & 11.6\% &128 & 69K\\        
			
			Antisymmetric RNN~\citep{chang2018antisymmetricrnn}  & 58.7\% & 48.3\% &256 & 36K\\
			
			Incremental RNN~\citep{Kag2020RNNs} & - & 54.5\% & 128 & -\\

			\midrule
			
			Lipschitz RNN using Euler (ours)  & {60.5\%} & {57.4\%} & 128 & 34K/46K\\			

			Lipschitz RNN using RK2 (ours)  & {60.3\%} & {57.3\%} & 128 & 34K/46K\\	
			
			\midrule		
			
			Lipschitz RNN using Euler (ours)  & \textbf{64.2\%} & \textbf{59.0\%} & 256 & 134K/158K\\

			Lipschitz RNN using RK2 (ours)  & {64.2\%} & {58.9\%} & 256 & 134K/158K\\
			
			\bottomrule
	\end{tabular}}
\end{table}



\subsection{Penn Tree Bank (PTB)}

\subsubsection{Character Level Prediction}\label{sec:ptb_character}

Next, we consider a character level language modeling task using the Penn Treebank Corpus (PTB)~\citep{marcus1993building}. Specifically, this task studies how well a model can predict the next character in a sequence of text.
The dataset is composed of a train / validation / test set, where K characters are used for training, K characters are used for validation and K characters are used for testing.  
For our experiments, we used the publicly available implementation of this task by~\citet{kerg2019non}, which computes the performance in terms of mean bits per character (BPC).

Table~\ref{tab:PTB_character} shows the results for back-propagation through time (BPTT) over 150 and 300 time steps, respectively. The Lipschitz RNN performs slightly better then the exponential RNN and the non-normal RNN on this task. \citep{kerg2019non} notes that orthogonal hidden-to-hidden matrices are not particular well-suited for this task. Thus, it is not surprising that the Lipschitz unit has a small advantage here. 

For comparison, we have also tested the Antisymmetric RNN~\citep{chang2018antisymmetricrnn} on this task. The performance of this unit is considerably weaker as compared to our Lipschitz unit. 
This suggests that the Lipschitz RNN is more expressive and improves the propagation of meaningful signals over longer time scales.


\begin{table}[!h]
	\caption{Evaluation accuracy on PTB for character-level prediction for different sequence lengths .  The * indicate results that were adopted from \citet{kerg2019non}.}
	\label{tab:PTB_character}
	\centering
	\scalebox{0.85}{
		\begin{tabular}{l c c c c c c}
			\toprule
			Name                  &   &   & \# params\\
			\midrule 
			
			RNN baseline by~\citep{arjovsky2016unitary} & 2.89 & 2.90 & 1.32M\\  
			
			RNN-orth~\citep{pmlr-v48-henaff16} (*) & 1.62 & 1.66 & 1.32M\\ 			
			
			EURNN~\citep{jing2017tunable} (*) & 1.61 & 1.62 & 1.32M\\ 
			


			Exponential RNN~\citep{lezcano2019cheap} (*) & 1.49 & 1.52  & 1.32M\\ 
			
			Non-normal RNN~\citep{kerg2019non} & 1.47 & 1.49  & 1.32M\\ 
			
			Antisymmteric RNN  & 1.60 & 1.64 & 1.32M\\ 
			
			Lipschitz RNN using Euler (ours)  & \textbf{1.43} & \textbf{1.46}  & 1.32M\\
			
			\bottomrule
	\end{tabular}}
\end{table}



\subsubsection{Word-Level Prediction}\label{sec:ptb_word}

In addition to character-level prediction, we also consider word-level prediction using the PTB corpus.
For comparison with other state-of-the-art units, we consider the setup by \citet{kusupati2018fastgrnn}, who use a sequence length of .
Table~\ref{tab:PTB_word} shows results for back-propagation through time (BPTT) over 300 time steps. The Lipschitz RNN performs slightly better than the other RNNs on this task and the baseline LSTM for the test perplexity metric reported by \citet{kusupati2018fastgrnn}. 


\begin{table}[!h]
	\caption{Evaluation accuracy on PTB for word-level prediction. The * indicate results adopted from \citet{kusupati2018fastgrnn}. Note that here the parameters for the hidden-to-hidden units are reported.}
	\label{tab:PTB_word}
	\centering
	\scalebox{0.85}{
		\begin{tabular}{l c c c c c c}
			\toprule
			Name                  & validation perplexity & test perplexity  & N & \# params\\
			\midrule 
			
			LSTM (*) & - & 117.41 & - & 210K\\  
			
			SpectralRNN (*)  & - & 130.20  & - & 24.8K\\	
			
			FastRNN (*)   & - & 127.76  & - & 52.5K\\	
			
			FastGRNN-LSQ (*) & - & 115.92  & - & 52.5K\\
			
			FastGRNN (*)  & - & 116.11  & - & 52.5K\\															
			
			Incremental RNN~\citep{Kag2020RNNs} & - & 115.71 & - & 29.5K\\
			
			Lipschitz RNN using Euler (ours) & 124.55  & \textbf{115.36} & 160 & 50K\\
			
			\bottomrule
	\end{tabular}}
\end{table}





\section{Tuning Parameters}

For tuning we utilized a standard training procedure using a non-exhaustive random search within the following plausible ranges for the our weight parameterization , . For Adam we explored learning rates between 0.001 and 0.005, and for SGD we considered 0.1. For the step size we explored values in the range 0.001 to 1.0. We did not perform an automated grid search and thus expect that the models can be further fine-tuned.

The tuning parameters for the different tasks that we have considered are summarized in Table~\ref{tab:tuning}. 

For pixel-by-pixel MNIST and CIFAR-10, we use Adam for minimizing the objective. We train all our models for  epochs, with scheduled learning rate decays at epochs . We do not use gradient clipping during training.
Figure~\ref{fig:mnist_testacc} shows the test accuracy curves for our Lipschitz RNN for the ordered and permuted MNIST classification tasks.

For TIMIT we use Adam with default parameters for minimizing the objective. We also tried Adam using betas (0.0, 0.9) as well as RMSprop with , however, Adam with default values worked best in our experiments. We train the model for  epochs without learning-rate decay. Similar to \cite{kerg2019non} we train our model with gradient clipping, however, we observed that the performance of our model is relatively insensitive to the clipping value. 

For the character level prediction task, we use Adam with default parameters for minimizing the objective, while we use RMSprop with  for the word level prediction task. We train the model for  epochs for the character-level task, and for 500 epochs for the word-level task.


\begin{table}[!t]
	\caption{Tuning parameters used for our experimental results and the performance evaluated with 12 different seed values for the parameter initialization of the model.}
	\label{tab:tuning}
	\centering
	\scalebox{0.85}{
		\begin{tabular}{l c c c c c c c c }
			\toprule
			Name           &  N & lr  & decay &  &  &  &  &  \\
			\midrule 
			Ordered MNIST  & 64 & 0.003  & 0.1 & 0.75 &  0.001 & 0.001 & 0.03 &   \\
			Ordered MNIST  & 128 & 0.003 & 0.1  & 0.75 &  0.001 & 0.001 & 0.03 &   \\
			\midrule 
			Permuted MNIST & 64 & 0.0035 & 0.1 & 0.75 & 0.001 & 0.001 & 0.03 &  \\		
			Permuted MNIST & 128 & 0.0035 & 0.1 & 0.75 & 0.001 & 0.001 & 0.03 &   \\
			\midrule
			Ordered CIFAR10 & 256 & 0.1 & 0.2 & 0.65 & 0.001 & 0.001 & 0.01 &   \\
			Noise-padded CIFAR10 & 256 & 0.1 & 0.2 & 0.75 & 0.001 & 0.001 & 0.01 &  \\
			\midrule
			TIMIT & 256 & 0.001 & - & 0.8 & 0.8 & 0.001 & 0.9 &   \\
			\midrule
			PTB character-level 150 & 750 & 0.005 & - & 0.8 & 0.5 & 0.001 & 0.1 &  \\
			PTB character-level 300 & 750 & 0.005 & - & 0.8 & 0.5 & 0.001 & 0.1 &   \\			
			\midrule
			PTB word-level & 160 & 0.1 & - & 0.8 & 0.9 & 0.001 & 0.01 &   \\			
			\bottomrule
	\end{tabular}}
\end{table}



\begin{figure}[!t]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/mnist_testacc.pdf}
			\put(-4,18){\rotatebox{90}{\footnotesize test accuracy}}			
			\put(46,-3){\footnotesize {epochs}}  	
		\end{overpic}\vspace{+0.2cm}		
		
		\caption{Ordered pixel-by-pixel MNIST}
	\end{subfigure}\hspace{+0.3cm}
	~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/pmnist_testacc.pdf} 
			\put(-4,18){\rotatebox{90}{\footnotesize test accuracy}}			
			\put(46,-3){\footnotesize {epochs}} 		
		\end{overpic}\vspace{+0.2cm}			
		\caption{Permuted pixel-by-pixel MNIST.}
	\end{subfigure}

	\caption{Test accuracy for the Lipschitz RNN for different classification tasks.}
	\label{fig:mnist_testacc}
\end{figure}
 


\end{document}
