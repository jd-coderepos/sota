
\documentclass{article} \usepackage{iclr2021_conference,times}
\usepackage{natbib}




\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}       \usepackage{nicefrac}       

\usepackage{hyperref}
\usepackage{url}

\usepackage{overpic}

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{float}
\usepackage{empheq}
\usepackage{booktabs}

\usepackage{upgreek}
\usepackage{enumitem}


\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}


\newtheorem{thm}{Theorem}[subsection]

\newcommand {\omri}[1]{{\color{red}\sf{[omri: #1]}}}

\newcommand {\ale}[1]{{\color{red}\sf{[Alejandro: #1]}}}


\newcommand{\sym}{\mathrm{sym}}


\title{Lipschitz Recurrent Neural Networks}



\author{N. Benjamin Erichson \\
	ICSI and UC Berkeley\\
	\texttt{erichson@berkeley.edu} \\
	\And
	Omri Azencot \\
	Ben-Gurion University \\
	\texttt{azencot@cs.bgu.ac.il} \\
	\And
	Alejandro Queiruga \\
	Google Research \\
	\texttt{afq@google.com} \\
	\AND
	Liam Hodgkinson \\
	ICSI and UC Berkeley \\
	\texttt{liam.hodgkinson@berkeley.edu} \\
	\And	
	Michael W. Mahoney \\
	ICSI and UC Berkeley\\
	\texttt{mmahoney@stat.berkeley.edu} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\newcommand{\changed}[1]{{\textcolor{red}{#1}}}


\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Viewing recurrent neural networks (RNNs) as continuous-time dynamical systems, we propose a recurrent unit that describes the hidden state's evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. 
This particular functional form facilitates stability analysis of the long-term behavior of the recurrent unit using tools from nonlinear systems theory. In turn, this enables architectural design decisions before experimentation.
Sufficient conditions for global stability of the recurrent unit are obtained, motivating a novel scheme for constructing hidden-to-hidden matrices.
Our experiments demonstrate that the Lipschitz RNN can outperform existing recurrent units on a range of benchmark tasks, including computer vision, language modeling and speech prediction tasks.
Finally, through Hessian-based analysis we demonstrate that our Lipschitz recurrent unit is more robust with respect to input and parameter perturbations as compared to other continuous-time RNNs.
\end{abstract}


\section{Introduction}

Many interesting problems exhibit temporal structures that can be modeled with recurrent neural networks (RNNs), including problems in robotics, system identification, natural language processing, and machine learning control. In contrast to feed-forward neural networks, RNNs consist of one or more recurrent units that are designed to have dynamical (recurrent) properties, thereby enabling them to acquire some form of internal memory. This equips RNNs with the ability to discover and exploit spatiotemporal patterns, such as symmetries and periodic structures~\citep{hinton1986learning}. However, RNNs are known to have stability issues and are notoriously difficult to train, most notably due to the vanishing and exploding gradients problem~\citep{bengio1994learning,pascanu2013difficulty}. 

Several recurrent models deal with the vanishing and exploding gradients issue by restricting the hidden-to-hidden weight matrix to be an element of the orthogonal group~\citep{arjovsky2016unitary,wisdom2016full,mhammedi2017efficient,vorontsov2017orthogonality,lezcano2019cheap}. 
While such an approach is advantageous in maintaining long-range memory, it limits the expressivity of the model. To address this issue, recent work suggested to construct hidden-to-hidden weights which have unit norm eigenvalues and can be nonnormal~\citep{kerg2019non}. 
Another approach for resolving the exploding/vanishing gradient problem has recently been proposed by \citet{Kag2020RNNs}, who formulate the recurrent units as a differential equation and update the hidden states based on the difference between predicted and previous states.

In this work, we address these challenges by viewing RNNs as dynamical systems whose temporal evolution is governed by an abstract system of differential equations with an external input. The data are formulated in continuous-time where the external input is defined by the function $x = x(t) \in \mathbb{R}^{p}$, and the target signal is defined as $y = y(t) \in \mathbb{R}^{d}$. Based on insights from dynamical systems theory, we propose a continuous-time Lipschitz recurrent neural network with the functional form
\begin{subequations}
\label{eq:rnn_de}
	\begin{empheq}[left=\empheqlbrace]{align} 
		\,\, \dot{h} \,\,\, & = \,\,\, {A_{\beta_A,\gamma_A}}h +  \tanh({W_{\beta_W,\gamma_W}}h + Ux + b) \ ,  \\
		\,\, y  \,\,\, & = \,\,\, Dh \ , \label{eq:rnn_de2}
	\end{empheq}
\end{subequations}
{where the hidden-to-hidden matrices $A_{\beta,\gamma} \in \mathbb{R}^{N\times N}$ and $W_{\beta,\gamma} \in \mathbb{R}^{N\times N}$ are of the form
\begin{subequations}
\begin{empheq}[left=\empheqlbrace]{align}
\,\, A_{\beta_A,\gamma_A} \,\,\, & =  (1-\beta_A) (M_A + M_A^T) +  \beta_A(M_A - M_A^T) - \gamma_A I \\
\,\, W_{\beta_W,\gamma_W} \,\,\, & =  (1-\beta_W) (M_W + M_W^T) +  \beta_W(M_W - M_W^T) - \gamma_W I,
\,\,
\end{empheq}
\end{subequations}
where $\beta_A,\beta_W \in [0,1]$, $\gamma_A,\gamma_W > 0$ are tunable parameters and $M_A,M_W \in \mathbb{R}^{N\times N}$ are trainable matrices.
}
Here, $h = h(t) \in \mathbb{R}^{N}$ is a function of time $t$ that represents an internal (hidden) state, and $\dot{h} = \frac{\partial h(t)}{\partial t}$ is its time derivative. The hidden state represents the  memory that the system has of its past.  The function in Eq.~(\ref{eq:rnn_de}) is parameterized by the hidden-to-hidden weight matrices $A \in \mathbb{R}^{N\times N}$ and $W \in \mathbb{R}^{N\times N}$, the input-to-hidden encoder matrix $U \in \mathbb{R}^{N\times p}$, and an offset $b$. The function in Eq.~(\ref{eq:rnn_de2}) is parameterized by the hidden-to-output decoder matrix $D \in \mathbb{R}^{d\times N}$. Nonlinearity is introduced via the 1-Lipschitz $\mathrm{tanh}$ activation function. 
While RNNs that are governed by differential equations with an additive structure have been studied before~\citep{zhang2014comprehensive}, the specific formulation that we propose in (\ref{eq:rnn_de}) and our theoretical analysis are distinct. 


Treating RNNs as dynamical systems enables studying the long-term behavior of the hidden state with tools from stability analysis. From this point of view, an unstable unit presents an exploding gradient problem, while a stable unit has well-behaved gradients over time~\citep{miller2018stable}. However, a stable recurrent unit can suffer from vanishing gradients, leading to catastrophic forgetting~\citep{hochreiter1997long}. Thus, we opt for a stable model whose dynamics do not (or only slowly do) decay over time. Importantly, stability is also a statement about the robustness of neural units with respect to input perturbations, \ie, stable models are less sensitive to small perturbations compared to unstable models. Recently, \citet{chang2018antisymmetricrnn} explored the stability of linearized RNNs and provided a {\em local} stability guarantee based on the Jacobian. In contrast, the particular structure of our unit~(\ref{eq:rnn_de}) allows us to obtain guarantees of {\em global exponential stability} using control theoretical arguments.
In turn, the sufficient conditions for global stability motivate a novel symmetric-skew decomposition based scheme for constructing hidden-to-hidden matrices.
This scheme alleviates exploding and vanishing gradients, while remaining highly expressive. 

In summary, the main contributions of this work are as follows:
\begin{itemize}\vspace{-0.15cm}
	\item First, in Section~\ref{sec:Stability}, using control theoretical arguments in a direct Lyapunov approach, we provide sufficient conditions for \emph{global exponential stability} of the Lipschitz RNN unit (Theorem \ref{thm:StabilityMain}). Global stability is advantageous over local stability results since it guarantees non-exploding gradients regardless of the state. In the special case where $A$ is symmetric, we find that these conditions agree with those in classical theoretical analyses (Lemma \ref{lem:CGH}).


	\item Next, in Section~\ref{sec:hidden_scheme}, drawing from our stability analysis, we propose a novel scheme based on the \emph{symmetric-skew decomposition} for constructing hidden-to-hidden matrices. This scheme \emph{mitigates the vanishing and exploding gradients problem}, while obtaining \emph{highly expressive} hidden-to-hidden matrices.
	
	\item In Section~\ref{sec:experiments}, we show that our Lipschitz RNN has the \emph{ability to outperform state-of-the-art recurrent units} on computer vision, language modeling and speech prediction tasks.
Further, our results show that the \emph{higher-order explicit midpoint time integrator} improves the predictive accuracy as compared to using the simpler one-step forward Euler scheme.  

	\item 
	Finally, in Section~\ref{sec:sensitivity}), we study our Lipschitz RNN via the lens of the Hessian and show that it is \emph{robust with respect to parameter perturbations}; we also show that our model is \emph{more robust with respect to input perturbations}, compared to other continuous-time RNNs.
	
\end{itemize}



\section{Related Work}

The problem of vanishing and exploding gradients (and stability) have a storied history in the study of RNNs.
Below, we summarize two particular approaches to the problem (constructing unitary/orthogonal RNNs and the dynamical systems viewpoint) that have gained significant attention.


\textbf{Unitary and orthogonal RNNs.}
Unitary recurrent units have received attention recently, largely due to \citet{arjovsky2016unitary} showing that unitary hidden-to-hidden matrices alleviate the vanishing and exploding gradients problem. Several other unitary and orthogonal models have also been proposed~\citep{wisdom2016full,mhammedi2017efficient,jing2017tunable,vorontsov2017orthogonality,jose2018kronecker}. While these approaches stabilize the training process of RNNs considerably, they also limit their expressivity and their prediction accuracy. Further, unitary RNNs are expensive to train, as they typically involve the computation of a matrix inverse at each step of training. Recent work by \citet{lezcano2019cheap} overcame some of these limitations. By leveraging concepts from Riemannian geometry and Lie group theory, their recurrent unit exhibits improved expressivity and predictive accuracy on a range of benchmark tasks while also being efficient to train. Another competitive recurrent design was recently proposed by \citet{kerg2019non}. Their approach is based on the Schur decomposition, and it enables the construction of general nonnormal hidden-to-hidden matrices with unit-norm eigenvalues.

\textbf{Dynamical systems inspired RNNs.}
The continuous time view of RNNs has a long history in the neurodynamics community as it provides higher flexibility and increased interpretability~\citep{pineda1988dynamics,pearlmutter1995gradient,zhang2014comprehensive}.
In particular, RNNs that are governed by differential equations with an additive structure have been extensively studied from a theoretical point of view~\citep{funahashi1993approximation,kim1996nonlinear,chow2000modeling,hu2002global,li2005approximation,trischler2016synthesis}. See \cite{zhang2014comprehensive} for a comprehensive survey of continuous-time RNNs and their stability properties.

Recently, several works have adopted the dynamical systems perspective to alleviate the challenges of training RNNs which are related to the vanishing and exploding gradients problem. For non-sequential data, \cite{NEURIPS2018_7bd28f15} proposed a negative-definite parameterization for enforcing stability in the RNN during training. \cite{chang2018antisymmetricrnn} introduced an antisymmetric hidden-to-hidden weight matrix and provided guarantees for local stability.
\citet{Kag2020RNNs} have proposed a differential equation based formulation for resolving the exploding/vanishing gradients problem by updating the hidden states based on the difference between predicted and previous states.
\citet{niu2019recurrent} employed numerical methods for differential equations to study the stability of RNNs.

Another line of recent work has focused on continuous-time models that deal with irregular sampled time-series, missing values and multidimensional time series. \citet{rubanova2019latent} and \citet{NIPS2019_8957} formulated novel recurrent models based on the theory of differential equations and their discrete integration. \cite{lechner2020learning} extended these ordinary differential equation (ODE) based models and addresses the issue of vanishing and exploding gradients by designing an ODE-model that is based on the idea of long short-term memory (LSTM). This ODE-LSTM outperforms the continuous-time LSTM~\citep{mei2017neural} as well as the GRU-D model~\citep{che2018recurrent} that is based on a gated recurrent unit (GRU).

The link between dynamical systems and models for forecasting sequential data also provides the opportunity to incorporate physical knowledge into the learning process which improves the generalization performance, robustness, and ability to learn with limited data~\citep{chen2019symplectic}.


\section{Stability Analysis of Lipschitz Recurrent Units}
\label{sec:Stability}

One of the key contributions in this work is that we prove that model~(\ref{eq:rnn_de}) is \emph{globally exponentially stable} under some mild conditions on $A$ and $W$. Namely, for \emph{any} initial hidden state we can guarantee that our Lipschitz unit converges to an equilibrium if it exists, and therefore, gradients can never explode. We improve upon recent work on stability in recurrent models, which provide only a local analysis, see e.g.,~\citep{chang2018antisymmetricrnn}. In fact, global exponential stability is among the strongest notions of stability in nonlinear systems theory, implying all other forms of Lyapunov stability about the equilibrium $h^*$~\cite[Definitions 4.4 and 4.5]{khalil2002nonlinear}. 
\begin{definition}
A point $h^\ast$ is an \emph{equilibrium point} of $\dot{h} = f(h,t)$ if $f(h^\ast, t) = 0$ for all $t$. Such a point is \emph{globally exponentially stable} if there exists some $C > 0$ and $\lambda > 0$ such that for any choice of initial values $h(0) \in \mathbb{R}^N$,
\begin{equation}
\label{eq:ExpStable}
\|h(t) - h^\ast\| \leq C e^{-\lambda t} \|h(0) - h^\ast\|,\quad \mbox{for any }t \geq 0.
\end{equation}
\end{definition}
The presence of a Lipschitz nonlinearity in~(\ref{eq:rnn_de}) plays an important role in our analysis. While we focus on $\tanh$ in our experiments, our proof is more general and is applicable to models whose nonlinearity $\sigma(\cdot)$ is an $M$-Lipschitz function. Specifically, we consider the general model
\begin{equation}
\label{eq:RNNODEMain}
\dot{h} = Ah + \sigma(W h + U x + b) \ ,
\end{equation}
for which we have the following stability result. In the following, we let $\sigma_{\min}$ and $\sigma_{\max}$ denote the smallest and largest singular values of the hidden-to-hidden matrices, respectively.

\begin{theorem}
\label{thm:StabilityMain}
Let $h^\ast$ be an equilibrium point of a differential equation of the form (\ref{eq:RNNODEMain}) for some $x \in \mathbb{R}^p$. The point $h^\ast$ is globally exponentially stable if the eigenvalues of $A^{\sym} \coloneqq \frac12(A + A^T)$ are strictly negative, $W$ is non-singular, and either (a) $\sigma_{\min}(A^{\sym}) > M \sigma_{\max}(W)$; or (b)  $\sigma$ is monotone non-decreasing, $W + W^T$ is negative definite, and $A^T W + W^T A$ is positive definite.
\end{theorem}
The two cases show that global exponential stability is guaranteed if either (a) the matrix $A$ has eigenvalues with real parts sufficiently negative to counteract expanding trajectories in the nonlinearity; or (b) the nonlinearity is monotone, both $A$ and $W$ yield stable linear systems $\dot{u} = A u$, $\dot{v} = W v$, and $A,W$ have sufficiently similar eigenvectors. In practice, case (b) occasionally holds, but is challenging to ensure without assuming specific structure on $A$, $W$. Because such assumptions could limit the expressiveness of the model, the next section will develop a tunable formulation for $A$ and $W$ with the capacity to ensure that case (a) holds. 

In Appendix~\ref{sxn:proof_of_first_thm}, we provide a proof of Theorem \ref{thm:StabilityMain} using a direct Lyapunov approach. One advantage of this approach is that the driving input $x$ is permitted to evolve in time arbitrarily in the analysis. The proof relies on the classical Kalman-Yakubovich-Popov lemma and circle criterion from control theory --- to our knowledge, these tools have not been applied in the modern RNN literature, and we hope our proof can illustrate their value to the community.

In the special case where $A$ is symmetric and $x(t)$ constant, we show that we can also inherit criteria for both local and global stability from a class of well-studied \emph{Cohen--Grossberg--Hopfield models}.
\begin{lemma}
\label{lem:CGH}
Suppose that $A$ is symmetric and $W$ is nonsingular. There exists a diagonal matrix $D \in \mathbb{R}^{N \times N}$, and nonsingular matrices $L,V \in \mathbb{R}^{N \times N}$ such that an equilibrium of (\ref{eq:RNNODEMain}) is (globally exponentially) stable if and only if there is a corresponding (globally exponentially) stable equilibrium for the system
\begin{equation}
\label{eq:CGH}
\dot{z} = Dz + L\sigma(V z + U x + b).
\end{equation}
\end{lemma}
For a thorough review of analyses of (\ref{eq:CGH}), see~\citep{zhang2014comprehensive}. In this special case, the criteria in Theorem \ref{thm:StabilityMain} coincide with those obtained for the corresponding model (\ref{eq:CGH}). However, in practice, we will not choose $A$ to be symmetric.


\section{Symmetric-Skew Hidden-to-Hidden Matrices}
\label{sec:hidden_scheme}

In this section we propose a novel scheme for constructing hidden-to-hidden matrices.
Specifically, based on the successful application of skew-symmetric hidden-to-hidden weights in several recent recurrent architectures, and our stability criteria in Theorem \ref{thm:StabilityMain}, we propose an effective \emph{symmetric-skew decomposition} for hidden matrices. Our decomposition allows for a simple control of the matrix spectrum while retaining its wide expressive range, enabling us to satisfy the spectral constraints derived in the previous section on both $A$ and $W$. The proposed scheme also accounts for the issue of vanishing gradients by reducing the magnitude of large negative eigenvalues.

Recently, several methods used skew-symmetric matrices, \ie, $S + S^T = 0$ to parameterize the recurrent weights $W \in \mathbb{R}^{N \times N}$, see \eg,~\citep{wisdom2016full,chang2018antisymmetricrnn}. From a stability analysis viewpoint, there are two main advantages for using skew-symmetric weights: these matrices generate the orthogonal group whose elements are isometric maps and thus preserve norms \citep{lezcano2019cheap}; and the spectrum of skew-symmetric matrices is purely imaginary which simplifies stability analysis \citep{chang2018antisymmetricrnn}.
The main shortcoming of this parametrization is its reduced expressivity, as these matrices have fewer than half of the parameters of a full matrix~\citep{kerg2019non}. The latter limiting aspect can be explained from a dynamical systems perspective: skew-symmetric matrices can only describe oscillatory behavior, whereas a matrix whose eigenvalues have nonzero real parts can also encode viable growth and decay information.

To address the expressivity issue, we aim for hidden matrices which on the one hand, allow to control the expansion and shrinkage of their associated trajectories, and on the other hand, will be sampled from a \emph{superset} of the skew-symmetric matrices. Our analysis in Theorem~\ref{thm:StabilityMain} guarantees that Lipschitz recurrent units maintain non-expanding trajectories under mild conditions on $A$ and $W$. Unfortunately, this proposition does not provide any information with respect to the shrinkage of paths. Here, we opt for a system whose expansion and shrinkage can be easily controlled. Formally, the latter requirement is equivalent to designing hidden weights $S$ with small $\mathcal{R}\lambda_i(S),\; i=1,2,\dots,N$, where $\mathcal{R}(z)$ denotes the real part of $z$. A system of the form (\ref{eq:RNNODEMain}) whose matrices $A$ and $W$ exhibit small spectra and satisfy the conditions of Theorem~\ref{thm:StabilityMain}, will exhibit dynamics with moderate decay and growth behavior and alleviate the problem of exploding and vanishing gradients. To this end, we propose the following symmetric-skew decomposition for constructing hidden matrices:
\begin{align} \label{eq:engergy_preserving_scheme}
	S_{\beta,\gamma} &\coloneqq (1-\beta) \cdot (M+M^T) + \beta \cdot (M-M^T) - \gamma I ,
\end{align}
where $M$ is a weight matrix, and $\beta \in [0.5,1]$, $\gamma > 0$ are tuning parameters. In the case $(\beta,\gamma) = (1,0)$, we recover a skew-symmetric matrix, \ie, $S_{1,0} + S_{1,0}^T = 0$. The construction $S_{\beta, \gamma}$ is useful as we can easily bound its spectrum via the parameters $\beta$ and $\gamma$, as we show in the next proposition. 
\begin{proposition}
\label{thm:our_second_thm}
Let $S_{\beta,\gamma}$ satisfy (\ref{eq:engergy_preserving_scheme}), and let $M^{\sym} = \frac12(M + M^T)$. The real parts $\Re\lambda_i(S_{\beta,\gamma})$ of the eigenvalues of $S_{\beta,\gamma}$, as well as the eigenvalues of $S_{\beta,\gamma}^{\sym} = S_{\beta,\gamma} + S_{\beta,\gamma}^T$, lie in the interval\[[(1 - \beta)\lambda_{\min}(M^{\sym}) - \gamma, (1 - \beta)\lambda_{\max}(M^{\sym}) - \gamma].\]
\end{proposition}
A proof is provided in Appendix~\ref{sxn:proof_of_second_thm}. We infer that $\beta$  controls the width of the spectrum, while increasing $\gamma$ shifts the spectrum to the left along the real axis, thus enforcing eigenvalues with non-positive real parts. Choosing our hidden-to-hidden matrices to be $A_{\beta_A,\gamma_A}$ and $W_{\beta_W,\gamma_W}$ of the form (\ref{eq:engergy_preserving_scheme}) for different values of $\beta_A,\beta_W$ and $\gamma_A,\gamma_W$, we can ensure small spectra and satisfy the conditions of Theorem~\ref{thm:StabilityMain} as desired. 
Note, that different tuning parameters $\beta$ and $\gamma$ affect the stability behavior of the Lipschitz recurrent unit. 
This is illustrated in Figure~\ref{fig:stability_simulation}, where different values for $\beta$ and $\gamma$ are used to construct both $A_{\beta,\gamma}$ and $W_{\beta,\gamma}$ and applied to learning simple pendulum dynamics.


\begin{figure}[!t]
	\centering
	\begin{subfigure}[t]{0.23\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/sim_beta05.pdf}
\end{overpic}\vspace{-0.1cm}		
		\caption{$\beta=0.5$, $\gamma=0$}
	\end{subfigure}~
	\begin{subfigure}[t]{0.23\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/sim_beta075.pdf} 
\end{overpic}\vspace{-0.1cm}			
		\caption{$\beta=0.75$, $\gamma=0.01$}
	\end{subfigure}~
	\begin{subfigure}[t]{0.23\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/sim_beta090.pdf} 
\end{overpic}\vspace{-0.1cm}			
		\caption{$\beta=0.85$, $\gamma=0.01$}
	\end{subfigure}~
	\begin{subfigure}[t]{0.23\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/sim_beta1.pdf} 
\end{overpic}\vspace{-0.1cm}			
		\caption{$\beta=1.0$, $\gamma=0$}
	\end{subfigure}

	\caption{Vector fields of hidden states that are governed by Eq.~(\ref{eq:rnn_de}) trained for simple pendulum dynamics. In (a), an unstable model is shown. In (b) and (c), it can be seen that we yield models that are asymptotically stable,i.e., all trajectories are attracted by an equilibrium point. In contrast, in (d), a skew-symmetric parameterization leads to a stable model without an attracting equilibrium.}	
	
	\label{fig:stability_simulation}
\end{figure}


One cannot guarantee that model parameters will remain in the stability region during training. However, we can show that when $\beta$ is taken to be close to one, the eigenvalues of $A_{\beta,\gamma}^\sym$ and $W_{\beta,\gamma}^\sym$ (which dictate the stability of the RNN) change slowly during training. Let $\Delta_\delta F$ denote the change in a function $F$ depending on the parameters of the RNN (\ref{eq:rnn_de}) after one step of gradient descent with step size $\delta$ with respect to some loss $L(y)$. For a matrix $A$, we let $\lambda_k(A)$ denote the $k$-th singular value of $A$. We have the following lemma.
\begin{lemma}
\label{lem:Training}
As $\beta \to 1^-$, $\max_k |\Delta_\delta \lambda_k(A_{\beta,\gamma}^{\sym})| + \max_k |\Delta_\delta \lambda_k(W_{\beta,\gamma}^{\sym})| = \mathcal{O}(\delta(1-\beta)^2)$.
\end{lemma}
Therefore, provided both the initial and optimal parameters lie within the stability region, the model parameters will remain in the stability region for longer periods of time with high probability as $\beta \to 1$. Further empirical evidence of parameters often remaining in the stability region during training are provided alongside the proof of Lemma \ref{lem:Training} in the Appendix (see Figure \ref{fig:eigs}).

\section{Training Continuous-time Lipschitz Recurrent Units}
\label{sxn:lipshitz_recurrent_unit}

ODEs such as Eq.~(\ref{eq:rnn_de}) can be approximately solved by employing numerical integrators. In scientific computing, numerical integration is a well studied field that provides well understood techniques~\citep{LeVeque}. Recent literature has also introduced new approaches which are designed with neural network frameworks in mind~\citep{chen2018neural}. 

To learn the weights $A, W, U$ and $b$, we discretize the continuous model using one step of a numerical integrator between sequence entries. 
In what follows, a subscript $t$ denotes discrete time indices, $\Delta t$ represents the time difference between a pair of consecutive data points. Letting $f(h, t) = A h + \tanh(W h + U x(s) + b)$ so that $\dot{h}(t) = f(h,t)$, the exact and approximate solutions for $h_{t+1}$ given $h_t$ are given by
\begin{align}
	h_{t+1} &= h_t + \int_t^{t+\Delta t} f(h(s), s) \mathrm{d} s \coloneqq h_t + \int_t^{t+\Delta t} Ah(s) + \tanh(Wh(s) + Ux(s) + b)\,\mathrm{d}s \\
&\approx h_t + \Delta t \cdot \mathtt{scheme} \left[f,\, h_t,\, \Delta t\right] \ ,
\end{align}
where $\mathtt{scheme}$ represents one step of a numerical integration scheme whose application yields an approximate solution for $\frac{1}{\Delta t} \int_t^{t+\Delta t} f(h(s), s) \mathrm{d} s$ given $h_t$ using one or more evaluations of $f$.

We consider both the explicit (forward) Euler scheme, 
\begin{equation} \label{eq:Lipschitz_unit}
	h_{t+1} = h_{t} + \Delta t \cdot Ah_{t} + \Delta t \cdot \tanh(z_t),
\end{equation}
as well as the midpoint method which is a two-stage explicit Runge-Kutta scheme (RK2),
\begin{align} \label{eq:rk2}
	h_{t+1} = h_{t} + \Delta t \cdot A\tilde{h} + \Delta t \cdot \tanh(W \tilde{h} + Ux_{t} + b), 
\end{align}
where $\tilde{h} = h_{t} + \Delta t/2 \cdot Ah_{t} + \Delta t/2 \cdot \tanh(z_t)$ is an intermediate hidden state.
The RK2 scheme can potentially improve the performance since the scheme is more accurate, however, this scheme also requires twice as many function evaluations as compared to the forward Euler scheme. 
Given a $\beta$ and $\gamma$ that yields a globally exponentially stable continuous model, $\Delta t$ can always be chosen so that the model remains in the stability region of forward Euler and RK2~\citep{LeVeque}.

\section{Empirical Evaluation}\label{sec:experiments}

In  this  section,  we  evaluate the  performance  of the Lipschitz RNN and compare it to other state-of-the-art methods.
The model is applied to ordered and permuted pixel-by-pixel MNIST classification, as well as to audio data using the TIMIT dataset.
We show the sensitivity with respect to to random initialization in Appendix~\ref{sec:app_results}. 
Appendix~\ref{sec:app_results} also contains additional results for: pixel-by-pixel CIFAR-10 and a noise-padded version of CIFAR-10; as well as for character level and word level prediction using the Penn Tree Bank (PTB) dataset.
All of these tasks require that the recurrent unit learns long-term dependencies: that is, the hidden-to-hidden matrices need to have sufficient memory to remember information from far in the past.

\begin{table}[!b]
	\caption{Evaluation accuracy on ordered and permuted pixel-by-pixel MNIST.}
	\label{tab:mnist-table}
	\centering
	\scalebox{0.92}{
		\begin{tabular}{l c c c c c c}
			\toprule
			Name                  &  ordered & permuted  & N & \# params\\
			\midrule 
			
			LSTM baseline by~\citep{arjovsky2016unitary} & 97.3\% & 92.7\% &128 & $\approx$68K\\        

			MomentumLSTM~\citep{nguyen2020momentumrnn} & 99.1\% & 94.7\% & 256 & $\approx$270K\\ 			
			
			Unitary RNN~\citep{arjovsky2016unitary} & 95.1\% & 91.4\% &512 & $\approx$9K\\  		
			
			Full Capacity Unitary RNN~\citep{wisdom2016full} & 96.9\% & 94.1\% &512 & $\approx$270K\\  
			
			Soft orth. RNN~\citep{vorontsov2017orthogonality} & 94.1\% & 91.4\% &128 & $\approx$18K\\    		
			
			
			Kronecker RNN~\citep{jose2018kronecker} & 96.4\% & 94.5\% & 512 & $\approx$11K\\
			
			
			Antisymmteric RNN~\citep{chang2018antisymmetricrnn}  & 98.0\% & 95.8\% &128 & $\approx$10K\\
			
			Incremental RNN~\citep{Kag2020RNNs} & 98.1\% & 95.6\% & 128 & $\approx$4K/8K\\
			
			
			Exponential RNN~\citep{lezcano2019cheap} & 98.4\% & 96.2\% & 360 & $\approx$69K\\
			
			Sequential NAIS-Net~\citep{NEURIPS2018_7bd28f15}     &    94.3\%     &  90.8\%         & 128              &  $\approx$18K  \\      
			
\midrule
			Lipschitz RNN using Euler (ours)  & 99.0\% & 94.2\% & 64 & $\approx$9K\\
			Lipschitz RNN using RK2 (ours)  &  99.1\% & 94.2\% & 64 & $\approx$9K\\	
			\midrule
			Lipschitz RNN using Euler (ours)  & \textbf{99.4\%} & \textbf{96.3\%} & 128 & $\approx$34K\\
			Lipschitz RNN using RK2 (ours)  & 99.3\% & 96.2\% & 128 & $\approx$34K\\			
			\bottomrule
	\end{tabular}}
\end{table}


\subsection{Ordered and Permuted Pixel-by-Pixel MNIST}\label{sec:mnist}

The pixel-by-pixel MNIST task tests long range dependency by
sequentially presenting $784$ pixels to the recurrent unit, \ie, the RNN processes one pixel at a time~\citep{le2015simple}. 
At the end of the sequence, the learned hidden state is used to predict the class membership probability of the input image. This task requires that the RNN has a sufficient long-term memory in order to discriminate between different classes. A more challenging variation to this task is to operate on a fixed random permutation of the input sequence.


Table~\ref{tab:mnist-table} provides a summary of our results.
The Lipschitz RNN, with hidden dimension of $N=128$ and trained with the forward Euler and RK2 scheme, achieves $99.4\%$ and $99.3\%$ accuracy on the ordered pixel-by-pixel MNIST task. 
For the permuted task, the model trained with forward Euler achieves $96.3\%$ accuracy, whereas the model trained with RK2 achieves $96.2\%$ accuracy.
Hence, our Lipschitz recurrent unit outperforms state-of-the-art RNNs on both tasks and is competitive even when a hidden dimension of $N=64$ is used, however, it can be seen that a larger unit with more capacity is advantageous for the permuted task.
Our results show that we significantly outperform the Antisymmetric RNN~\citep{chang2018antisymmetricrnn} on the ordered tasks, while using fewer weights. That shows that the antisymmetric weight paramterization is limiting the expressivity of the recurrent unit.
The exponential RNN is the next most competitive model, yet this model requires a larger hidden-to-hidden unit to perform well on the two considered tasks. 









\begin{table}[!t]
	\caption{Evaluation on TIMIT using 1 layer models.  The mean squared error (MSE) is computes the distance between the predicted and actual log-magnitudes of each predicted frame in the sequence. }
	\label{tab:timit}
	\centering
	\scalebox{0.91}{
		\begin{tabular}{l c c c c c c}
			\toprule
			Name                  & val. MSE & test MSE  & N & \# params\\
			\midrule 
			
			LSTM~\citep{helfrich2018orthogonal} & 13.66 & 12.62 & 158 & $\approx$200K\\ 
			
			LSTM~\citep{nguyen2020momentumrnn}  & 9.33 & 9.37 & 158 & $\approx$200K\\ 
			
			MomentumLSTM~\citep{nguyen2020momentumrnn} & 5.86 & 5.87 & 158 & $\approx$200K\\ 
			
			SRLSTM~\citep{nguyen2020momentumrnn} & 5.81 & 5.83 & 158 & $\approx$200K\\ 
			
			
			Full-capacity Unitary RNN~\citep{wisdom2016full} & 14.41 & 14.45 & 256 & $\approx$200K\\  		
			
			Cayley RNN~\citep{helfrich2018orthogonal} & 7.97 & 7.36 & 425 & $\approx$200K\\  	
			
			
			Exponential RNN~\citep{lezcano2019cheap}  & 5.52 & 5.48 & 425 & $\approx$200K\\  									 						
			\midrule
			Lipschitz RNN using Euler (ours) &  2.95 &  2.82   & 256 & $\approx$198K\\

			Lipschitz RNN using RK2 (ours) & \textbf{2.86} & \textbf{2.76} & 256 & $\approx$198K\\			
			\bottomrule
	\end{tabular}}
\end{table}


\subsection{TIMIT}

Next, we consider the TIMIT dataset~\citep{Garofolo} to study the capabilities of the Lipschitz RNN for speech prediction using audio data.
For our experiments, we used the publicly available implementation of this task by~\citet{lezcano2019cheap}.
This implementation applies the preprocessing steps suggested by \citet{wisdom2016full}: (i) downsample each audio sequence to 8kHz; (ii) process the downsampled sequences with a short-time Fourier transform using a Hann window of 256 samples and a window hop of 128 samples; and (iii) normalize the log-magnitude of the Fourier amplitudes.
We obtain a set of frames that each have 129 complex-valued Fourier amplitudes and the task is to predict the log-magnitude of future frames. 
To compare our results with those of other models, we used the common train / validation / test split: 3690 utterances from 462 speakers for training, 192 utterances for validation, and 400 utterances for testing.  

Table~\ref{tab:timit} lists the results for the Lipschitz recurrent unit as well as for several benchmark models. It can be seen that the Lipschitz RNN outperforms other state-of-the-art models for a fixed number of parameters ($\approx 200$K). In particular, LSTMs do not perform well on this task, however, the recently proposed momentum based LSTMs~\citep{nguyen2020momentumrnn} have improvemed performance.
Interestingly, the RK2 scheme leads to a better performance since this scheme provides more accurate approximations for the intermediate states. 




\section{Robustness with Respect to Perturbations}\label{sec:sensitivity}



An important consideration beyond accuracy is robustness with respect to input and parameter perturbations. We consider a Hessian-based analysis and noise-response analysis of different continuous-time recurrent units and train the models on MNIST. Here, we reshape each MNIST thumbnail into sequences of length $98$ so that each input has dimension $x\in \mathbb{R}^8$.
We consider this simpler problem so that all models obtain roughly the same training loss. Here we use stochastic gradient decent (SGD) with momentum to train the models.

Eigenanalysis of the Hessian provides a tool for studying various aspects of neural networks~\citep{hochreiter1997flat,sagun2017empirical,ghorbani2019investigation}.
Here, we study the Hessian $H$ spectrum with respect to the model parameters of the recurrent unit using PyHessian~\citep{YGKM19_pyhessian_TR}. 
The Hessian provides us with insights about the curvature of the loss function $\mathcal{L}$.  
This is because the Hessian is defined as the derivatives of the gradients, and thus the Hessian eigenvalues describe the change in the gradient of $\mathcal{L}$ as we take an infinitesimal step into a given direction. 
The eigenvectors span the (local) surface of the loss function at a given point, and the corresponding eigenvalue determines the curvature in the direction of the eigenvectors. 
This means that larger eigenvalues indicate a larger curvature, \ie, greater sensitivity, and the sign of the eigenvalues determines whether the curvature will be positive or negative.


\begin{table}[!b]
\caption{Summary of Hessian-based robustness metrics and resilience to adversarial attacks.
		\label{tab:sensitifity}}
	\centering
	\scalebox{0.91}{
		\begin{tabular}{lcccccccccc} \toprule
			Model                  & PGD  &DF$_2$ & DF$_\infty$ &  $\lambda_{\max}(H)$ & $\mathrm{tr}(H)$ & $\kappa(H)$ \\
			\midrule


			Neural ODE RNN            & 88.5\%  & 69.6\%   & 44.5\%  & 0.30 & 4.7 & 37.6\\
			Antisymmetric  RNN       & 84.7\%  & 83.4\%   & 44.3\%  & 0.24 & 4.8 & 35.5\\
			Lipschitz RNN (ours)    & \textbf{93.0}\%  & \textbf{89.2}\%   & \textbf{54.1}\%   &  \textbf{0.14} & \textbf{3.1} & \textbf{23.2}\\
			\bottomrule 
	\end{tabular}}
\end{table}

\begin{figure}[!b]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/pertubation_whitenoise.pdf}
			\put(-6,15){\rotatebox{90}{\footnotesize test accuracy}}			
			\put(42,-3){\footnotesize {amount of noise}}  	
		\end{overpic}\vspace{+0.2cm}		
		\caption{White noise perturbations.}\label{fig:perturb_a}
	\end{subfigure}~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/pertubation_sp.pdf} 
\put(42,-3){\footnotesize {amount of noise}}  		
		\end{overpic}\vspace{+0.2cm}			
		\caption{Salt and pepper perturbations.}\label{fig:perturb_b}
	\end{subfigure}	
\caption{Sensitivity with respect to different input perturbations.}
	\label{fig:perturb}
\end{figure}

To demonstrate the advantage of the additional linear term and our weight parameterization, we compare the Lipschitz RNN to two other continuous-time recurrent units. First, we consider a simple neural ODE RNN~\citep{rubanova2019latent} that takes the form
\begin{equation}
    \label{eq:odernn}
    \dot{h} = \tanh(Wh + Ux + b),\qquad \qquad y = Dh,
\end{equation}
where $W$ is a simple hidden-to-hidden matrix. As a second model we consider the antisymmetric RNN~\citep{chang2018antisymmetricrnn}, that takes the same form as (\ref{eq:odernn}), but uses a skew-symmetric scheme to parameterize the hidden-to-hidden matrix as 
$W := (M - M^T) - \gamma I$,
where $M$ is a trainable weight matrix and $\gamma$ is a tunable parameter. 

Table~\ref{tab:sensitifity} reports the largest eigenvalue $\lambda_{\max}(H)$ and the trace of the Hessian $\mathrm{tr}(H)$.The largest eigenvalue being smaller indicates that our Lipschitz RNN found a flatter minimum, as compared to the simple neural ODE and Antisymmetric RNN.
It is known that such flat minima can be perturbed without significantly changing the loss value~\citep{hochreiter1997flat}. 
Table~\ref{tab:sensitifity} also reports the condition number $\kappa(H) := \frac{\lambda_{\max}(H)}{\lambda_{\min}(H)}$ of the Hessian.
The condition number $\kappa(H)$ provides a measure for the spread of the eigenvalues of the Hessian.
It is known that first-order methods can slow down in situations where $\kappa$ is large~\citep{bottou2008tradeoffs}. 
The condition number and trace of our Lipshitz RNN being smaller also indicates improved robustness properties.


Next, we study the sensitivity of the response $y_T$ at time $T$ in terms of the test accuracy with respect to a sequence of perturbed inputs $\{\tilde{x}_1,\dots,\tilde{x}_T\} \in \mathbb{R}^{8}$. 
We consider three different perturbations. 
The results for the artificially constructed perturbations are presented in Table~\ref{tab:sensitifity}, showing that the Lipschitz RNN is more resilient to adversarial perturbation.
Here, we have considered the projected gradient decent (PGD)~\citep{goodfellow2014explaining} method with $l_\infty$, and the DeepFool method~\citep{moosavi2016deepfool} with $l_2$ and $l_\infty$ norm ball perturbations.
We construct the adversarial examples with full access to the models, using $7$ iterations. The step size for PGD is set to $0.01$.


Further, Figure~\ref{fig:perturb} shows the results for white noise and salt and pepper noise. It can be seen that the Lipschitz unit is less sensitive to input perturbations, as compared to the simple neural ODE RNN, and the antisymmetric RNN. In addition, we also show the results for an unitary RNN here.


\subsection{Ablation Study}\label{sec:ablation}


The performance of the Lipschitz recurrent unit is due to two main innovations: (i) the additional linear term; and (ii) the scheme for constructing the hidden-to-hidden matrices $A$ and $W$ in Eq.~(\ref{eq:engergy_preserving_scheme}).
Thus, we investigate the effect of both innovations, while keeping all other conditions fixed. More concretely, we consider the following ablation recurrent unit
\begin{equation}\
	h_{t+1} = h_{t} + \alpha \cdot \epsilon \cdot Ah_{t} + \epsilon \cdot \tanh(z_t), \quad \text{with} \quad z_t = Wh_{t} + Ux_t + b,
\end{equation}
where $\alpha$ controls the effect of the linear hidden unit. 
Both $A$ and $W$ depend on the parameters $\beta$, $\gamma$.


Figure~\ref{fig:ablation_a} studies the effect of the linear hidden unit, with $\beta=0.65$ for the ordered task and $\beta=0.8$ for the permuted task. In both cases we use $\gamma=0.001$. It can be seen that the test accuracies of both the ordered and permuted pixel-by-pixel MNIST tasks clearly depend on the linear hidden unit. For $\alpha=0$, our models reduces to simple neural ODE recurrent units (Eq.~(\ref{eq:odernn})). The recurrent unit degenerates for $\alpha>1.6$, since the external input is superimposed by the hidden state. 
Figure~\ref{fig:ablation_b} studies the effect of the hidden-to-hidden matrices with respect to $\beta$. 
It can be seen that $\beta=\{0.65,0.70\}$ achieves peak performance for the ordered task, and $\beta=\{0.8,0.85\}$ does so for the permuted task. 
Note that $\beta=1.0$ recovers an skew-symmetric hidden-to-hidden matrix. 

\begin{figure}[!t]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/ablation_alpha.pdf}
			\put(-6,15){\rotatebox{90}{\footnotesize test accuracy}}			
			\put(31,-3){\footnotesize {ablation parameter, $\alpha$}}  	
		\end{overpic}\vspace{+0.3cm}		
		\caption{Effect of the linear term.}\label{fig:ablation_a}
	\end{subfigure}~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/ablation_beta.pdf} 
\put(31,-3){\footnotesize {ablation parameter, $\beta$}}  		
		\end{overpic}\vspace{+0.3cm}			
		\caption{Effect of Eq.~(\ref{eq:engergy_preserving_scheme}).}\label{fig:ablation_b}
	\end{subfigure}\caption{The ablation study examines the effect of the linear term $Ah$ (in (a)) and the importance of the Skew-Symmetric Decomposition for constructing the hidden-to-hidden matrices (in (b)).} 
	\label{fig:ablation}
\end{figure}



\section{Conclusion}

Viewing RNNs as continuous-time dynamical systems with input, we have proposed a new Lipschitz recurrent unit that excels on a range of benchmark tasks. 
The special structure of the recurrent unit allows us to obtain guarantees of global exponential stability using control theoretical arguments. 
In turn, the insights from this analysis motivated the symmetric-skew decomposition scheme for constructing hidden-to-hidden matrices, which mitigates the vanishing and exploding gradients problem.
Due to the nice stability properties of the Lipschitz recurrent unit, we also obtain a model that is more robust with respect to input and parameter perturbations as compared to other continuous-time units. 
This behavior is also reflected by the Hessian analysis of the model. 
We expect that the improved robustness will make Lipschitz RNNs more reliable for sensitive applications.
The theoretical results for our symmetric-skew decomposition of parameterizing hidden-to-hidden matrices also directly extend to the convolutional setting. Future work will explore this extension and study the potential advantages of these more parsimonious hidden-to-hidden matrices in combination with our parameterization in practice.
Research code is shared via \href{https://github.com/erichson/LipschitzRNN}{github.com/erichson/LipschitzRNN}.


\clearpage
\subsubsection*{Acknowledgments}
We would like to thank Ed H. Chi for fruitful discussions about physics-informed machine learning and the Antisymmetric RNN.
We are grateful to the generous support from Amazon AWS and Google Cloud.
NBE and MWM would like to acknowledge IARPA (contract W911NF20C0035), NSF, ONR and CLTC for providing partial support of this work. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.


\bibliography{rnn}
\bibliographystyle{iclr2021_conference}

\clearpage
\appendix


\section{Proofs}

\subsection{Proofs of Theorem \ref{thm:StabilityMain} and Lemma \ref{lem:CGH}}
\label{sxn:proof_of_first_thm}

There are numerous ways that one can analyze the global stability of (\ref{eq:RNNODEMain}) through the related model (\ref{eq:CGH}), many of which are discussed in \cite{zhang2014comprehensive}. Instead, here we shall conduct a direct approach and avoid appealing to diagonalization in order to obtain cleaner conditions, and a more straightforward proof that readily applies in the time-inhomogeneous setting.

Our method of choice relies on Lyapunov arguments summarized in the following theorem, which can be found as \cite[Theorem 4.10]{khalil2002nonlinear}. For more details on related Lyapunov theory, see also~\cite{hahn1967stability, sastry2013nonlinear}.
\begin{theorem}
\label{thm:Lyapunov}
	An equilibrium $h^*$ for $\dot{h}=f(t, h)$ is globally exponentially stable if there exists a continuously differentiable function $V:[0,\infty)\times \mathbb{R}^N \to [0,\infty)$ such that for all $h \in \mathbb{R}^N$ and $t \geq 0$,
	\[
	k_1 \|h - h^\ast\|^{\alpha} \leq V(t,h) \leq k_2 \|h - h^\ast\|^{\alpha}, \quad\mbox{and}\quad \frac{\partial V}{\partial t} + \frac{\partial V}{\partial h} \leq -k_3 \|h - h^\ast\|^{\alpha},
	\]
	for some constants $k_1,k_2,k_3,\alpha > 0$.
	and $\dot{V}(h) < 0$ for $h \neq h^\ast$. 
\end{theorem}

To simplify matters, we shall choose a Lyapunov function $V:\mathbb{R}^N\to[0,\infty)$ that is independent of time. The most common type of Lyapunov function satisfying the conditions of Theorem \ref{thm:Lyapunov} is of the form $V(h) = (h - h^\ast)^T P (h - h^\ast)$, where $P$ is a positive definite matrix. One need only show that $\dot{V}(h) \leq -(h-h^\ast)^T Q (h-h^\ast)$ for some other positive definite matrix $Q$ to guarantee global exponential stability.

The construction of the Lyapunov function $V$ that satisfies the conditions of Theorem \ref{thm:Lyapunov} is accomplished using the Kalman-Yakubovich-Popov lemma, which is a statement regarding \emph{strictly positive real transfer functions}. We use the following definition, equivalent to other standard definitions by \cite[Lemma 6.1]{khalil2002nonlinear}.
\begin{definition}
A function $G:\mathbb{C} \to \mathbb{C}^{N\times N}$ is \emph{strictly positive real} if it satisfies the following:
\begin{enumerate}[label=(\roman*)]
    \item The poles of $G(s)$ have negative real parts.
    \item $G(i\omega) + G(-i\omega)^T$ is positive definite for all $\omega \in \mathbb{R}$, where $i = \sqrt{-1}$.
    \item Either $G(\infty) + G(\infty)^T$ is positive definite or it is positive semidefinite and $\lim_{\omega\to\infty} \omega^2 M^T [G(i\omega)+G(-i\omega)^T]M$ is positive definite for any $N \times (N - q)$ full-rank matrix $M$ such that $M^T[G(\infty)+G(\infty)^T] M = 0$, where $q = \mathrm{rank}[G(\infty)+G(\infty)^T]$.
\end{enumerate}
\end{definition}
The following is presented in \cite[Lemma 6.3]{khalil2002nonlinear}. 
\begin{lemma}[Kalman-Yakubovich-Popov]
\label{lem:KYP}
Let $A,W:\mathbb{R}^N \to \mathbb{R}^N$ be full-rank square matrices. There exists a symmetric positive-definite matrix $P$ and matrices $L,U$ and a constant $\epsilon > 0$ such that
\begin{align*}
PA + A^T P &= -L^T L - \epsilon P \\
P &= L^T U - W^T \\
U^T U &= 0,
\end{align*}
if and only if the \emph{transfer function} $G(s) = W(sI-A)^{-1}$ is strictly positive real. In this case, we may take $\epsilon = 2\mu$, where $\mu > 0$ is chosen so that $G(s - \mu)$ remains strictly positive real. 
\end{lemma}
A shorter proof for case (a) is available to us through the (multivariable) \emph{circle criterion} --- the following theorem is a corollary of \cite[Theorem 7.1]{khalil2002nonlinear} suitable for our purposes.
\begin{theorem}[Circle Criterion]
\label{thm:CircleCriterion}
The system of differential equations
\[
\dot{h} = A h + \psi(t, W h)
\]
is globally exponentially stable towards an equilibrium at the origin if $\|\psi(t,y)\| \leq M\|y\|$ for some $M > 0$ and
$
Z(s) = [I + M G(s)][I - M G(s)]^{-1}
$
is strictly positive real, where $G(s) = W(sI - A)^{-1}$. 
\end{theorem}

Both the Kalman-Yakubovich-Popov lemma and the circle criterion are classical results in control theory, and are typically discussed in the setting of feedback systems \cite[Chapter 6, 7]{khalil2002nonlinear}. Our presentation here is less general than the complete formulation, but makes clearer the connection to RNNs. With these tools, we state our proof of Theorem \ref{thm:StabilityMain}.

\begin{proof}[Proof of Theorem \ref{thm:StabilityMain}]
To begin, we shall center the differential equation about the equilibrium. By assumption, there exists $h^\ast$ such that $A h^\ast = -\sigma(W h^\ast + Ux(t) + b)$. Letting $\bar{h} = h - h^\ast$, we find that
\begin{align}
\dot{\bar{h}}&=Ah+\sigma(Wh+Ux(t)+b)\nonumber\\
&=A\bar{h}+Ah^{\ast}+\sigma(W\bar{h}+Wh^{\ast}+Ux(t)+b)\nonumber\\
&=A\bar{h}+\sigma(W\bar{h}+Wh^{\ast}+Ux(t)+b)-\sigma(Wh^{\ast}+Ux(t)+b).\label{eq:CenterODE}
\end{align}
It will suffice to show that (\ref{eq:CenterODE}) is globally exponentially stable at the origin.

Let us begin with case (a). The proof follows arguments analogous to \cite[Example 7.1]{khalil2002nonlinear}. Let $G(s) = W(A - sI)^{-1}$ denote the transfer function for the system (\ref{eq:CenterODE}). Letting
\[
\psi(t, x) = \sigma(x + W h^\ast + Ux(t) + b) - \sigma(Wh^\ast + Ux(t)+b),
\]
since $\sigma$ is $M$-Lipschitz, we know that $\|\psi(t, x)\| \leq M \|x\|$ for any $x \in \mathbb{R}^N$. Therefore, let $Z(s) = [I + M G(s)][I - M G(s)]^{-1}$ denote the transfer function in the circle criterion. Our objective is to show that $Z(s)$ is strictly positive real --- by Theorem \ref{thm:CircleCriterion}, this will guarantee the desired global exponential stability of (\ref{eq:RNNODEMain}). First, we need to show that the poles of $Z(s)$ have negative real parts. This can only occur when $G(s)$ itself has poles or $I - M G(s)$ is singular. The former case occurs precisely where $A - sI$ is singular, which occurs when $s$ is an eigenvalue of $A$. Since $A + A^T$ is assumed to be negative definite, $A$ must have eigenvalues with negative real part by Lemma \ref{lem:SymEigs}, and so the poles of $G(s)$ also have negative real parts. The latter case is more difficult to treat. First, since $\sigma_{\max}(AB) \leq \sigma_{\max}(A)\sigma_{\max}(B)$ and $\sigma_{\max}(B^{-1}) = \sigma_{\min}(B)^{-1}$, 
\begin{equation}
\label{eq:TransferSigmaMax}
\sigma_{\max}(G(s)) \leq \frac{\sigma_{\max}(W)}{\sigma_{\min}(A - s I)}.
\end{equation}
Therefore, we observe that
\begin{align*}
\sigma_{\min}(I - M G(s)) &\geq 1 - \sigma_{\max}(M G(s)) \\
&\geq 1 - M \sigma_{\max}(G(s)) \\
&\geq 1 - \frac{M \sigma_{\max}(W)}{\sigma_{\min}(A - sI)}.
\end{align*}
From the Fan-Hoffman inequality~\citep[Proposition III.5.1]{bhatia2013matrix}, we have that
\[
\sigma_{\min}(A - sI) = \sigma_{\min}(s I - A) \geq \lambda_{\min}\left(\Re(s) I - \frac{A+A^T}{2}\right) = \Re(s) + \lambda_{\min}\left(-\frac{A + A^T}{2}\right),
\]
and since $A + A^T$ is negative definite, for any $s$ with $\Re(s) \geq 0$,
\begin{equation}
\sigma_{\min}(A - sI)  \geq \Re(s) + \sigma_{\min}\left(\frac{A+A^T}{2}\right) \geq \sigma_{\min}(A^{\sym}).
\label{eq:SigmaMinImag}
\end{equation}
Since $\sigma_{\min}(A^{\sym}) > M \sigma_{\max}(W)$, it follows that $\sigma_{\min}(I - M G(s)) > 0$ whenever $s$ has non-negative real part, and so the poles of $Z(s)$ must have negative real parts.

Next, we need to show that $Z(i \omega) + Z(-i \omega)^T$ is positive definite for all $\omega \in \mathbb{R}$. Observe that
\begin{align*}
Z(i \omega) + Z(-i \omega)^T &= [I + M G(i \omega)][I - M G(i \omega)]^{-1}
+ [I - M G(-i \omega)^T]^{-1}[I + M G(-i \omega)^T] \\
&= 2[I - M G(-i \omega)^T]^{-1}[I - M^2 G(-i\omega)^T G(i\omega)][I - M G(i \omega)]^{-1}.
\end{align*}
From Sylvester's law of inertia, we may infer that $Z(i\omega) + Z(-i\omega)^T$ is positive definite if and only if $I + Y_\omega$ is positive definite, where $Y_\omega = M^2 G(-i\omega)^T G(i \omega)$. If we can show that the eigenvalues of $Y_\omega$ lie strictly within the unit circle, that is, $\sigma_{\max}(Y_\omega) < 1$ for all $\omega \in \mathbb{R}$, then $I + Y_\omega$ will necessarily be positive definite. From (\ref{eq:TransferSigmaMax}) and (\ref{eq:SigmaMinImag}), we may verify that
\[
\sup_{\omega \in \mathbb{R}} \sigma_{\max}(G(i\omega)) \leq \sup_{\omega \in \mathbb{R}} \frac{\sigma_{\max}(W)}{\sigma_{\min}(A - i \omega I)} \leq \frac{\sigma_{\max}(W)}{\sigma_{\min}(A^{\sym})}.
\]
Therefore,
\[
\sigma_{\max}(Y_\omega) \leq M^2 \sigma_{\max}(G(-i\omega)^T) \sigma_{\max}(G(i\omega)) \leq \left(\frac{M \sigma_{\max}(W)}{\sigma_{\min}(A^{\sym})}\right)^2 < 1,
\]
by assumption. Finally, since $Z(\infty) + Z(\infty)^T = 2I$ is positive definite, $Z(s)$ is strictly positive real and Theorem \ref{thm:CircleCriterion} applies. 

Now, consider case (b). The proof proceeds in two steps. First, we verify that the transfer function $G(s) = W(A - sI)^{-1}$ satisfies the conditions of the Kalman-Yakubovich-Popov lemma. Then, using the matrices $P$, $L$, $U$, and the constant $\epsilon$ inferred from the lemma, a Lyapunov function is constructed which satisfies the conditions of Theorem \ref{thm:Lyapunov}, guaranteeing global exponential stability. Once again, condition (i) of Lemma \ref{lem:KYP} is straightforward to verify: $G(s)$ exhibits poles when $s$ is an eigenvalue of $A$, and so the poles of $G(s)$ also have negative real parts. Furthermore, condition (iii) is easily satisfied with $M = I$ since $G(\infty)+G(\infty)^T = 0$. To show that condition (ii) holds, observe that for any $\omega \in \mathbb{R}$, letting $A^{-T} = (A^{-1})^T$ for brevity,
\begin{align*}
G(i\omega) + G(-i\omega)^T &= W(A - i\omega I)^{-1} + (A + i\omega I)^{-T} W^T \\
&= (A + i\omega I)^{-T} [(A+i\omega I)^T W + W^T (A - i\omega I)](A - i\omega I)^{-1}.
\end{align*}
Since the inner matrix factor is Hermitian, Sylvester's law of inertia implies that $G(i\omega) + G(-i\omega)^T$ is positive definite if and only if 
\[
B_\omega \coloneqq (A + i\omega I)^T W + W^T (A - i\omega I).
\]
is positive definite. Since $B_\omega$ is a Hermitian matrix, it has real eigenvalues, with minimal eigenvalue given by the infimum of the Rayleigh quotient:
\begin{align*}
\lambda_{\min}(B_\omega) &= \inf_{\|v\| = 1} v^T B_\omega v \\
&= \inf_{\|v\| = 1} v^T (A^T W + W^T A)v + i\omega v^T(W - W^T) v \\
&= \inf_{\|v\| = 1} v^T (A^T W + W^T A)v \\
&= \lambda_{\min}(A^T W + W^T A).
\end{align*}
By assumption, $A^T W + W^T A$ has strictly positive eigenvalues, and hence $B_\omega$ and $G(i\omega) + G(-i\omega)^T$ are positive definite. Therefore, Lemma \ref{lem:KYP} applies, and we obtain matrices $P,L,U$ and a constant $\epsilon > 0$ with the corresponding properties.

Now we may construct our Lyapunov function $V$. Let $v = W \bar{h}$ and
\[
u(t) = \sigma(v(t) + W h^\ast + U x(t) + b) - \sigma(W h^\ast + U x(t) + b),
\]
so that $\dot{\bar{h}} = A \bar{h} + u$. 
Since $\sigma$ is monotone non-decreasing, $\sigma(x) - \sigma(y) \geq 0$ for any $x \geq y$. This implies that for each $i = 1,\dots,N$, $v_i$ and $u_i$ have the same sign. In particular, $v^T u \geq 0$. Now, let $V(h) = h^T P h$ be our Lyapunov function, noting that $V$ is independent of $t$. Taking the derivative of the Lyapunov function over (\ref{eq:CenterODE}) and using the properties of $P,L,U,\epsilon$,
\begin{align*}
\dot{V}(\bar{h}) &= \bar{h}^T P \dot{\bar{h}} + \dot{\bar{h}}^T P \bar{h} \\
&= \bar{h}^T (P A + A^T P) \bar{h} + 2 \bar{h}^T P u \\
&= \bar{h}^T (-L^T L - \epsilon P) \bar{h} + 2 \bar{h}^T (L^T U - W^T) u \\
&= -(L\bar{h})^T (L\bar{h}) + (L \bar{h})^T U u + (U u)^T (L \bar{h}) - u^T U^T U u - 2 v^T u\\
&= -(L \bar{h} + U u)^T(L\bar{h} + U u) - \epsilon \bar{h}^T P \bar{h} - 2 v^T u. 
\end{align*}
Since $v^T u \geq 0$ and $(L \bar{h}+Uu)^T(L\bar{h}+Uu) \geq 0$, it follows that $\dot{V}(\bar{h}) \leq -\epsilon \lambda_{\min}(P)\|h\|^2$, and hence global exponential stability follows from Theorem \ref{thm:Lyapunov} and positive-definiteness of $P$.
\end{proof}

To finish off discussion regarding the results from Sec. 3, we provide a quick proof of Lemma \ref{lem:CGH} using a simple diagonalization argument.

\begin{proof}[Proof of Lemma \ref{lem:CGH}]
Since $A$ is symmetric and real-valued, by \cite[Theorem 4.1.5]{horn2012matrix}, there exists an orthogonal matrix $P$ and a real diagonal matrix $D$ such that $A = PDP^T$. Letting $z = P^T h$ where $h$ satisfies (\ref{eq:RNNODEMain}), since $h = P z$, we see that
\begin{align*}
\dot{z} &= P^T P D P^T h + P^T \sigma(W h + U x + b) \\
&= D z + P^T \sigma(W P z + U x + b).
\end{align*}
Therefore, $z$ satisfies (\ref{eq:CGH}) with $L = P^T$ and $V = W P$, both of which are nonsingular by orthogonality of $P$. By the same argument, for any equilibrium $h^\ast$, taking $z^\ast = P^T h^\ast$,
\begin{align*}
D z^\ast + P^T \sigma(W P z^\ast + U x + b)
&= P^T ( P D P^T h^\ast + \sigma(W h^\ast + U x + b) ) \\
&= P^T ( A h^\ast + \sigma(W h^\ast + U x + b) ) = 0,
\end{align*}
implying that $z^\ast$ is an equilibrium of (\ref{eq:CGH}). Furthermore, since
\begin{align*}
\|z - z^\ast\|^2 &= (P^T h - P^T h^\ast)^T (P^T h - P^T h^\ast) \\
&= (h - h^\ast)^T P P^T (h - h^\ast) = \|h - h^\ast\|^2,
\end{align*}
from orthogonality of $P$. Because every form of Lyapunov stability, both local and global, including global exponential stability, depend only on the norm $\|h - h^\ast\|$ \cite[Definitions 4.4 and 4.5]{khalil2002nonlinear}, $h^\ast$ is stable under any of these forms if and only if $z^\ast$ is also stable.
\end{proof}

We remark that the proof of Lemma \ref{lem:CGH} can extend to matrices $A$ which have real eigenvalues and are diagonalizable. These attributes are implied for symmetric matrices. However, they can be difficult to ensure in practice for nonsymmetric matrices without imposing difficult structural constraints.

\subsection{Proof of Proposition \ref{thm:our_second_thm}}
\label{sxn:proof_of_second_thm}

The proof of Proposition \ref{thm:our_second_thm} relies on the following lemma, which we also have made use of several times throughout this work. 
\begin{lemma}
\label{lem:SymEigs}
For any matrix $A \in \mathbb{R}^{N \times N}$, the real parts of the eigenvalues $\Re \lambda_i(A)$ are contained in the interval $[\lambda_{\min}(A^{\sym}),\lambda_{\max}(A^{\sym})]$, where $A^{\sym} = \frac12(A + A^T)$. 
\end{lemma}
\begin{proof}
Recall by the min-max theorem, for $\langle u, v \rangle = u^\ast v$, where $u^\ast$ is the conjugate transpose of $u$, the upper and lower eigenvalues of $A + A^T$ satisfy
\begin{align*}
\lambda_{\min}(A + A^T) &= \inf_{v \in \mathbb{C}^N,\, \|v\| = 1} \langle v, (A + A^T) v\rangle = \inf_{v \in \mathbb{C}^N,\, \|v\| = 1} \langle v, A v\rangle + \langle A v, v\rangle,\\
\lambda_{\max}(A + A^T) &= \sup_{v \in \mathbb{C}^N,\, \|v\| = 1} \langle v, (A + A^T) v\rangle = \sup_{v \in \mathbb{C}^N,\, \|v\| = 1} \langle v, A v\rangle + \langle A v, v\rangle.
\end{align*}
Let $\lambda_i(A) = u + i \omega$ be an eigenvalue of $A$ with corresponding eigenvector $v$ satisfying $\|v\| = 1$. Since $A v = (u + i \omega) v$,
\[
\langle v, A v \rangle + \langle A v, v \rangle = \langle v, A v \rangle + \overline{\langle v, A v \rangle} = 2 \Re \langle v, A v \rangle = 2u\|v\|^2 = 2u.
\]
Hence, $\lambda_{\min}(A + A^T) \leq u \leq \lambda_{\max}(A+A^T)$.
\end{proof}


\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/projection_64.pdf}
			\put(-6,16){\rotatebox{90}{\footnotesize magnitude}}			
			\put(31,-3){\footnotesize {tuning parameter, $\beta$}}  	
		\end{overpic}\vspace{+0.4cm}		
		
		\caption{N=64}
	\end{subfigure}~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/projection_128.pdf} 
\put(31,-3){\footnotesize {tuning parameter, $\beta$}}  	
		\end{overpic}\vspace{+0.4cm}			
		\caption{N=128}
	\end{subfigure}\vspace{-0.2cm}	
	
	
	\caption{Empirical evaluation of the theoretical bounds (\ref{eq:IntervalContainer}). The red lines track the largest real part and the blue lines track the smallest real part of the eigenvalues of the hidden-to-hidden matrix $A_\beta$. Each line corresponds to a different hidden-to-hidden matrix of dimension $N=64$ in (a) and $N=128$ in (b). The dashed black lines indicate the theoretical bound for each trial.  }
	\label{fig:projection}
\end{figure}

\begin{proof}[Proof of Proposition \ref{thm:our_second_thm}]
By construction,
$
S_{\beta,\gamma}^{\sym} = S_{\beta,\gamma} + S_{\beta,\gamma}^T = (1-\beta)M^{\sym} - \gamma I,
$
and so from Lemma \ref{lem:SymEigs}, both the real parts $\Re\lambda_i(S_{\beta,\gamma})$ of the eigenvalues of $S_{\beta,\gamma}$ as well as the eigenvalues of $S_{\beta,\gamma}^{\sym}$ lie in the interval
\[
[\lambda_{\min}(S_{\beta,\gamma}^{\sym}), \lambda_{\max}(S_{\beta,\gamma}^{\sym})] = [\lambda_{\min}((1-\beta)M^{\sym} -\gamma I), \lambda_{\max}((1-\beta)M^{\sym} - \gamma I)].
\]
If $\beta < 1$, for any eigenvalue $\lambda$ of $S_{\beta,\gamma}^{\sym}$ with corresponding eigenvector $v$,
\[
(1-\beta)M^{\sym} v - \gamma v = \lambda v,\quad \text{and so} \quad  M^{\sym} v = \frac{\lambda + \gamma}{1 - \beta} v
\]
implying that $\frac{\lambda + \gamma}{1-\beta}$ is an eigenvalue of $M^{\sym}$, and therefore contained in $[\lambda_{\min}(M^{\sym}),\lambda_{\max}(M^{\sym})]$. 
In particular, we find that
\begin{equation}
\label{eq:IntervalContainer}
[\lambda_{\min}(S_{\beta,\gamma}^{\sym}),\lambda_{\max}(S_{\beta,\gamma}^{\sym})] \subseteq [(1-\beta)\lambda_{\min}(M^{\sym}) - \gamma, (1-\beta)\lambda_{\max}(M^{\sym})],
\end{equation}
as required. Finally, if $\beta = 1$, then (\ref{eq:IntervalContainer}) still holds, since both intervals collapse to the single point $\{-\gamma\}$. 
\end{proof}


Figure~\ref{fig:projection} illustrates the effect of $\beta$ onto the eigenvalues of $A_{\beta,\gamma}$ with the largest and smallest real parts. 
It can  be seen, both empirically and theoretically, that the real part of the eigenvalues converges towards zero as $\beta$ tends towards one, \ie, we yield a skew-symmetric matrix with purely imaginary eigenvalues in the limit.
Thus, for a sufficiently large parameter $\beta$ we yield a system that approximately preserves an ``energy'' for a limited time-horizon 
\begin{equation}
\mathcal{R}\lambda_i(A_{\beta,\gamma}) \approx 0, \quad \text{for} \quad i=1,2,\dots,N.
\end{equation}


\subsection{Proof of Lemma \ref{lem:Training}}
First, it follows from Gronwall's inequality that the norm of the final hidden state $\|h(T)\|$ is bounded uniformly in $\beta$. From Weyl's inequalities and the definition of $A_{\beta,\gamma}$,
\[
\max_k |\Delta_\delta \lambda_k(A_{\beta,\gamma}^\sym)| \leq \| \Delta_{\delta}A_{\beta,\gamma}^{\sym}\| =(1-\beta)\| \Delta_{\delta}M_{A}^{\sym}\|.
\]
By the chain rule, for each element $M_A^{ij}$ of the matrix $M_A$,
\[
\frac{\partial L}{\partial M_{A}^{ij}}=\frac{\partial L}{\partial y(T)}\frac{\partial y(T)}{\partial h(T)}\frac{\partial h(T)}{\partial M_{A}^{ij}}=\frac{\partial L}{\partial y(T)}D\frac{\partial h(T)}{\partial M_{A}^{ij}}.
\]
Now, for any collection of parameters $\theta_i$,
\[
\frac{d}{dt}\sum_{i}\frac{\partial h}{\partial\theta_{i}}=A\sum_{i}\frac{\partial h}{\partial\theta_{i}}+\sum_{i}\frac{\partial A}{\partial\theta_{i}}h+\mathrm{sech}^2\left(Wh+Ux+b\right)\left(W\sum_{i}\frac{\partial h}{\partial\theta_{i}}+\sum_{i}\frac{\partial W}{\partial\theta_{i}}h\right),
\]
and from Gronwall's inequality,
\[
\left\lVert \sum_{i}\frac{\partial h(T)}{\partial\theta_{i}}\right\rVert \leq\left(\left\lVert \sum_{i}\frac{\partial A_{\beta,\gamma}}{\partial\theta_{i}}\right\rVert +\left\lVert \sum_{i}\frac{\partial W_{\beta,\gamma}}{\partial\theta_{i}}\right\rVert \right)\left\lVert h\right\rVert e^{(\left\lVert A_{\beta,\gamma}\right\rVert +\left\lVert W_{\beta,\gamma}\right\rVert )T}.
\]
Since $\Delta_{\delta}M_{A}^{\sym}=\delta\frac{\partial L}{\partial M_{A}}+\delta\left(\frac{\partial L}{\partial M_{A}}\right)^{T}$,
\begin{align*}
\left\lVert \Delta_{\delta}M_{A}^{\sym}\right\rVert &\leq\left\lVert \Delta_{\delta}M_{A}^{\sym}\right\rVert _{F}\\
&\leq\delta\sqrt{\sum_{i,j}\left(\frac{\partial L}{\partial M_{A}^{ij}}+\frac{\partial L}{\partial M_{A}^{ij}}\right)^{2}}\\
&\leq\delta\left\lVert \frac{\partial L}{\partial y}\right\rVert \left\lVert D\right\rVert \left\lVert h\right\rVert e^{(\left\lVert A_{\beta,\gamma}\right\rVert +\left\lVert W_{\beta,\gamma}\right\rVert )T}\sqrt{\sum_{i,j}\left\lVert \frac{\partial A_{\beta,\gamma}}{\partial M_{A}^{ij}}+\frac{\partial A_{\beta,\gamma}}{\partial M_{A}^{ji}}\right\rVert ^{2}}.
\end{align*}
Since $\frac{\partial(M_{A}h)}{\partial M_{A}^{ij}}=\frac{\partial(M_{A}^{T}h)}{\partial M_{A}^{ji}}$, it follows that
\[
\frac{\partial A_{\beta,\gamma}}{\partial M_{A}^{ij}}+\frac{\partial A_{\beta,\gamma}}{\partial M_{A}^{ji}}=2(1-\beta)\left(\frac{\partial(M_{A}h)}{\partial M_{A}^{ij}}+\frac{\partial(M_{A}^{T}h)}{\partial M_{A}^{ji}}\right),
\]
and so $\|\Delta_\delta M_A^\sym\| = \mathcal{O}(\delta(1-\beta))$, and therefore $\max_k |\Delta_\delta \sigma_k(A_{\beta,\gamma}^\sym)| = \mathcal{O}(\delta(1-\beta)^2)$. Similarly, for the matrix $M_W$,
\begin{multline*}
\max_{k}\left|\Delta_{\delta}\lambda_{k}(W_{\beta,\gamma}^{\sym})\right|\leq(1-\beta)\left\lVert \Delta_{\delta}M_{W}^{\sym}\right\rVert \\
\leq\delta(1-\beta)\left\lVert \frac{\partial L}{\partial y}\right\rVert \left\lVert D\right\rVert \left\lVert h\right\rVert e^{(\| A_{\beta,\gamma}\| +\| W_{\beta,\gamma}\| )T}\sqrt{\sum_{i,j}\left\lVert \frac{\partial W_{\beta,\gamma}}{\partial M_{W}^{ij}}+\frac{\partial W_{\beta,\gamma}}{\partial M_{W}^{ji}}\right\rVert ^{2}}\\
=2\delta(1-\beta)^{2}\left\lVert \frac{\partial L}{\partial y}\right\rVert \left\lVert D\right\rVert \left\lVert h\right\rVert e^{(\| A_{\beta,\gamma}\| +\| W_{\beta,\gamma}\| )T}\sqrt{\sum_{i,j}\left(\frac{\partial(M_{W}h)}{\partial M_{W}^{ij}}+\frac{\partial(M_{W}^{T}h)}{\partial M_{W}^{ji}}\right)^{2}},
\end{multline*}
and hence $\max_k |\Delta_\delta \lambda_k(W_{\beta,\gamma}^\sym)| = \mathcal{O}(\delta(1-\beta)^2)$. \qedsymbol

In Figure \ref{fig:eigs}, we plot the most positive real part of the eigenvalues of $A_{\beta,\gamma}$ and $W_{\beta,\gamma}$ during training for the ordered MNIST task. As $\beta$ increases, the eigenvalues change less during training, remaining in the stability region provided by case (b) of Theorem \ref{thm:StabilityMain} for more of the training time.


\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/eigs_smnist.pdf}
			\put(-6,9){\rotatebox{90}{\footnotesize real part of eigenvalue}}			
			\put(31,-3){\footnotesize {number of epoch}}  	
		\end{overpic}\vspace{+0.4cm}
		\caption{$\beta = 0.65$}
\end{subfigure}~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/eigs_smnist_95.pdf} 
\put(31,-3){\footnotesize {number of epoch}}  	
		\end{overpic}\vspace{+0.4cm}
		\caption{$\beta = 0.95$}
\end{subfigure}\vspace{-0.2cm}	
	
	
	\caption{The red lines track the largest real part of the eigenvalues of the hidden-to-hidden matrix $A_{\beta,\gamma}$ and the blue lines track the largest real part of the eigenvalues of  $W_{\beta,\gamma}$. We show results for two models trained on the ordered MNIST task with varying $\beta$.\label{fig:eigs}}
	
\end{figure}

\section{Additional Experiments}\label{sec:app_results}




\subsection{Sensitivity to Random Initialization for MNIST and TIMIT}\label{sec:init}

The hidden matrices are initialized by sampling weights from the normal distribution $\mathcal{N}(0,\sigma)$, where $\sigma$ is the variance, which can be treated as a tuning parameter. In our experiments we typically chose a small $\sigma$; see the Table~\ref{tab:tuning} for details.
To show that the Lipschitz RNN is insensitive to random initialization, we have trained each model with 10 different seeds. Table~\ref{tab:minmax} shows the maximum, average and minimum values obtained for each task. Note that higher values indicate better performance on the ordered and permuted MNIST tasks, while lower values indicate better performance on the TIMIT task.


\begin{table}[!t]
	\caption{Sensitivity to random initialization evaluated over 10 runs.}
	\label{tab:minmax}
	\centering
	\scalebox{0.91}{
		\begin{tabular}{l c c c c c c c c}
			\toprule
			Solver         & Task     & Minimum  & Average  & Maximum  & N & \# params\\
			\midrule 
			
			Euler & ordered MNIST  & 98.9\% & 99.0\%  & 99.0\% & 64 & $\approx$9K\\
			RK2 & ordered MNIST    & 98.9\% & 99.0\%  & 99.1\%  & 64 & $\approx$9K\\	
			Euler & ordered MNIST  & 99.0\% & 99.2\%  & 99.4\% & 128 & $\approx$34K\\
			RK2 & ordered MNIST    & 98.9\% & 99.1\%  & 99.3\% & 128 & $\approx$34K\\
			
			\midrule
			
			Euler & permuted MNIST   & 93.5\% & 93.8\%  & 94.2\% & 64 & $\approx$9K\\
			RK2 & permuted MNIST     & 93.5\% & 93.9\%  & 94.2\%  & 64 & $\approx$9K\\	
			
			Euler & permuted MNIST  & 95.6\% & 95.9\%  & 96.3\% & 128 & $\approx$34K\\
			RK2 & permuted MNIST    & 95.4\% & 95.8\%  & 96.2\%  & 128 & $\approx$34K\\
			
			
			\midrule


			Euler  & TIMIT (test MSE)  & 2.82 & 2.98   & 3.10 & 256 & $\approx$198K\\
			


			RK2  & TIMIT (test MSE)  & 2.76 & 2.81  & 2.84 & 256 & $\approx$198K\\			
			
			\bottomrule
	\end{tabular}}
\end{table}


\subsection{Ordered Pixel-by-Pixel and Noise-Padded CIFAR-10}\label{sec:cifar10}

The pixel-by-pixel CIFAR-10 benchmark problem that has recently been proposed by~\citep{chang2018antisymmetricrnn}. This task is similar to the pixel-by-pixel MNIST task, yet more challenging due to the increased sequence length and the more difficult classification problem.  
Similar to MNIST, we flatten the CIFAR-10 images to construct a sequence of length $1024$ in scanline order, where each element of the sequence consists of three pixels (one from each channel).

A variation of this problem is the noise-padded CIFAR-10 problem~\citep{chang2018antisymmetricrnn}, where we consider each row of an image as input at time step $t$. 
The rows from each channel are stacked so that we obtain an input of dimension $x\in \mathbb{R}^{96}$.
Then, after the $32$ time step which process the 32 row, we start to feed the recurrent unit with independent standard Gaussian noise for $968$ time steps. 
At the final point in $T=1000$, we use the learned hidden state for classification. 
This problem is challenging because only the first $32$ time steps contain signals. Thus, the recurrent unit needs to recall information from the beginning of the process.

Table~\ref{tab:cifar-table} provides a summary of our results. 
Our Lipschitz recurrent unit outperforms both the incremental RNN~\citep{Kag2020RNNs} and the antisymmetric RNN~\citep{chang2018antisymmetricrnn} by a significant margin. 
This impressively demonstrates that the Lipschitz unit enables the stable propagation of signals over long time~horizons.



\begin{table}[!t]
	\caption{Evaluation accuracy on pixel-by-pixel CIFAR-10 and noise padded CIFAR-10.}
	\label{tab:cifar-table}
	\centering
	\scalebox{0.85}{
		\begin{tabular}{l c c c c c c}
			\toprule
			Name                  &  ordered & noise padded  & N & \# params\\
			\midrule 
			
			LSTM baseline by~\citep{chang2018antisymmetricrnn} & 59.7\% & 11.6\% &128 & 69K\\        
			
			Antisymmetric RNN~\citep{chang2018antisymmetricrnn}  & 58.7\% & 48.3\% &256 & 36K\\
			
			Incremental RNN~\citep{Kag2020RNNs} & - & 54.5\% & 128 & -\\

			\midrule
			
			Lipschitz RNN using Euler (ours)  & {60.5\%} & {57.4\%} & 128 & 34K/46K\\			

			Lipschitz RNN using RK2 (ours)  & {60.3\%} & {57.3\%} & 128 & 34K/46K\\	
			
			\midrule		
			
			Lipschitz RNN using Euler (ours)  & \textbf{64.2\%} & \textbf{59.0\%} & 256 & 134K/158K\\

			Lipschitz RNN using RK2 (ours)  & {64.2\%} & {58.9\%} & 256 & 134K/158K\\
			
			\bottomrule
	\end{tabular}}
\end{table}



\subsection{Penn Tree Bank (PTB)}

\subsubsection{Character Level Prediction}\label{sec:ptb_character}

Next, we consider a character level language modeling task using the Penn Treebank Corpus (PTB)~\citep{marcus1993building}. Specifically, this task studies how well a model can predict the next character in a sequence of text.
The dataset is composed of a train / validation / test set, where $5017$K characters are used for training, $393$K characters are used for validation and $442$K characters are used for testing.  
For our experiments, we used the publicly available implementation of this task by~\citet{kerg2019non}, which computes the performance in terms of mean bits per character (BPC).

Table~\ref{tab:PTB_character} shows the results for back-propagation through time (BPTT) over 150 and 300 time steps, respectively. The Lipschitz RNN performs slightly better then the exponential RNN and the non-normal RNN on this task. \citep{kerg2019non} notes that orthogonal hidden-to-hidden matrices are not particular well-suited for this task. Thus, it is not surprising that the Lipschitz unit has a small advantage here. 

For comparison, we have also tested the Antisymmetric RNN~\citep{chang2018antisymmetricrnn} on this task. The performance of this unit is considerably weaker as compared to our Lipschitz unit. 
This suggests that the Lipschitz RNN is more expressive and improves the propagation of meaningful signals over longer time scales.


\begin{table}[!h]
	\caption{Evaluation accuracy on PTB for character-level prediction for different sequence lengths $T$.  The * indicate results that were adopted from \citet{kerg2019non}.}
	\label{tab:PTB_character}
	\centering
	\scalebox{0.85}{
		\begin{tabular}{l c c c c c c}
			\toprule
			Name                  &  $T_{PTB}=150$ & $T_{PTB}=300$  & \# params\\
			\midrule 
			
			RNN baseline by~\citep{arjovsky2016unitary} & 2.89 & 2.90 & $\approx$1.32M\\  
			
			RNN-orth~\citep{pmlr-v48-henaff16} (*) & 1.62 & 1.66 & $\approx$1.32M\\ 			
			
			EURNN~\citep{jing2017tunable} (*) & 1.61 & 1.62 & $\approx$1.32M\\ 
			


			Exponential RNN~\citep{lezcano2019cheap} (*) & 1.49 & 1.52  & $\approx$1.32M\\ 
			
			Non-normal RNN~\citep{kerg2019non} & 1.47 & 1.49  & $\approx$1.32M\\ 
			
			Antisymmteric RNN  & 1.60 & 1.64 & $\approx$1.32M\\ 
			
			Lipschitz RNN using Euler (ours)  & \textbf{1.43} & \textbf{1.46}  & $\approx$1.32M\\
			
			\bottomrule
	\end{tabular}}
\end{table}



\subsubsection{Word-Level Prediction}\label{sec:ptb_word}

In addition to character-level prediction, we also consider word-level prediction using the PTB corpus.
For comparison with other state-of-the-art units, we consider the setup by \citet{kusupati2018fastgrnn}, who use a sequence length of $300$.
Table~\ref{tab:PTB_word} shows results for back-propagation through time (BPTT) over 300 time steps. The Lipschitz RNN performs slightly better than the other RNNs on this task and the baseline LSTM for the test perplexity metric reported by \citet{kusupati2018fastgrnn}. 


\begin{table}[!h]
	\caption{Evaluation accuracy on PTB for word-level prediction. The * indicate results adopted from \citet{kusupati2018fastgrnn}. Note that here the parameters for the hidden-to-hidden units are reported.}
	\label{tab:PTB_word}
	\centering
	\scalebox{0.85}{
		\begin{tabular}{l c c c c c c}
			\toprule
			Name                  & validation perplexity & test perplexity  & N & \# params\\
			\midrule 
			
			LSTM (*) & - & 117.41 & - & 210K\\  
			
			SpectralRNN (*)  & - & 130.20  & - & 24.8K\\	
			
			FastRNN (*)   & - & 127.76  & - & 52.5K\\	
			
			FastGRNN-LSQ (*) & - & 115.92  & - & 52.5K\\
			
			FastGRNN (*)  & - & 116.11  & - & 52.5K\\															
			
			Incremental RNN~\citep{Kag2020RNNs} & - & 115.71 & - & 29.5K\\
			
			Lipschitz RNN using Euler (ours) & 124.55  & \textbf{115.36} & 160 & 50K\\
			
			\bottomrule
	\end{tabular}}
\end{table}





\section{Tuning Parameters}

For tuning we utilized a standard training procedure using a non-exhaustive random search within the following plausible ranges for the our weight parameterization $\beta={0.65,0.7,0.75,0.8}$, $\gamma=[0.001, 1.0]$. For Adam we explored learning rates between 0.001 and 0.005, and for SGD we considered 0.1. For the step size we explored values in the range 0.001 to 1.0. We did not perform an automated grid search and thus expect that the models can be further fine-tuned.

The tuning parameters for the different tasks that we have considered are summarized in Table~\ref{tab:tuning}. 

For pixel-by-pixel MNIST and CIFAR-10, we use Adam for minimizing the objective. We train all our models for $100$ epochs, with scheduled learning rate decays at epochs $\{90\}$. We do not use gradient clipping during training.
Figure~\ref{fig:mnist_testacc} shows the test accuracy curves for our Lipschitz RNN for the ordered and permuted MNIST classification tasks.

For TIMIT we use Adam with default parameters for minimizing the objective. We also tried Adam using betas (0.0, 0.9) as well as RMSprop with $\alpha=0.9$, however, Adam with default values worked best in our experiments. We train the model for $1200$ epochs without learning-rate decay. Similar to \cite{kerg2019non} we train our model with gradient clipping, however, we observed that the performance of our model is relatively insensitive to the clipping value. 

For the character level prediction task, we use Adam with default parameters for minimizing the objective, while we use RMSprop with $\alpha=0.9$ for the word level prediction task. We train the model for $200$ epochs for the character-level task, and for 500 epochs for the word-level task.


\begin{table}[!t]
	\caption{Tuning parameters used for our experimental results and the performance evaluated with 12 different seed values for the parameter initialization of the model.}
	\label{tab:tuning}
	\centering
	\scalebox{0.85}{
		\begin{tabular}{l c c c c c c c c }
			\toprule
			Name           &  N & lr  & decay & $\beta$ & $\gamma_a$ & $\gamma_w$ & $\epsilon$ & $\sigma$ \\
			\midrule 
			Ordered MNIST  & 64 & 0.003  & 0.1 & 0.75 &  0.001 & 0.001 & 0.03 & $0.1 / 64$  \\
			Ordered MNIST  & 128 & 0.003 & 0.1  & 0.75 &  0.001 & 0.001 & 0.03 & $0.1 / 128$  \\
			\midrule 
			Permuted MNIST & 64 & 0.0035 & 0.1 & 0.75 & 0.001 & 0.001 & 0.03 & $0.1 / 128$ \\		
			Permuted MNIST & 128 & 0.0035 & 0.1 & 0.75 & 0.001 & 0.001 & 0.03 & $0.1 / 128$  \\
			\midrule
			Ordered CIFAR10 & 256 & 0.1 & 0.2 & 0.65 & 0.001 & 0.001 & 0.01 & $6 / 256 $  \\
			Noise-padded CIFAR10 & 256 & 0.1 & 0.2 & 0.75 & 0.001 & 0.001 & 0.01 & $6 / 256$ \\
			\midrule
			TIMIT & 256 & 0.001 & - & 0.8 & 0.8 & 0.001 & 0.9 & $12 / 256 $  \\
			\midrule
			PTB character-level 150 & 750 & 0.005 & - & 0.8 & 0.5 & 0.001 & 0.1 & $12 / 256 $ \\
			PTB character-level 300 & 750 & 0.005 & - & 0.8 & 0.5 & 0.001 & 0.1 & $12 / 256 $  \\			
			\midrule
			PTB word-level & 160 & 0.1 & - & 0.8 & 0.9 & 0.001 & 0.01 & $10 / 256 $  \\			
			\bottomrule
	\end{tabular}}
\end{table}



\begin{figure}[!t]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/mnist_testacc.pdf}
			\put(-4,18){\rotatebox{90}{\footnotesize test accuracy}}			
			\put(46,-3){\footnotesize {epochs}}  	
		\end{overpic}\vspace{+0.2cm}		
		
		\caption{Ordered pixel-by-pixel MNIST}
	\end{subfigure}\hspace{+0.3cm}
	~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{overpic}[width=1\textwidth]{figs/pmnist_testacc.pdf} 
			\put(-4,18){\rotatebox{90}{\footnotesize test accuracy}}			
			\put(46,-3){\footnotesize {epochs}} 		
		\end{overpic}\vspace{+0.2cm}			
		\caption{Permuted pixel-by-pixel MNIST.}
	\end{subfigure}

	\caption{Test accuracy for the Lipschitz RNN for different classification tasks.}
	\label{fig:mnist_testacc}
\end{figure}
 


\end{document}
