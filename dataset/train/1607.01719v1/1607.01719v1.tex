 \section{Experiments}
\label{sec:exp}

We evaluate our method on a standard domain adaptation benchmark -- the Office dataset~\cite{saenko2010adapting}. The Office dataset contains 31 object categories from an office environment in 3 image domains: $Amazon$, $DSLR$,  and $Webcam$. 

We follow the standard protocol of~\cite{gfk,dan_long15,decaf,tzeng_arxiv15,reversegrad} and use all the labelled source data and all the target data without labels. 
Since there are 3 domains, we conduct experiments on all 6 shifts, taking one domain as the source and another as the target. 

In this experiment, we apply the CORAL loss to the last classification layer
as it is the most general case--most deep classifier architectures (e.g., convolutional, recurrent) contain a fully connected layer for classification. Applying the CORAL loss to other layers or other network architectures should be straightforward. 

The dimension of last fully connected layer ($fc8$) was set to the number of categories (31) and initialized with $\mathcal{N}(0,0.005)$. The learning rate of $fc8$ is set to 10 times the other layers as it was training from scratch. We initialized the other layers with the parameters pre-trained on ImageNet~\cite{imagenet} and kept the original layer-wise parameter settings. In the training phase, we set the batch size to 128, base learning rate to $10^{-3}$, weight decay to $5\times10^{-4}$, and momentum to 0.9. The weight of the CORAL loss ($\lambda$) is set in such way that at the end of training the classification loss and CORAL loss are roughly the same. It seems be a reasonable choice as we want to have a feature representation that is both discriminative and also minimizes the distance between the source and target domains. We used Caffe~\cite{caffe} and BVLC Reference CaffeNet for all of our experiments.

We compare to 7 recently published methods: CNN~\cite{alexnet} (no adaptation), GFK~\cite{gfk}, SA~\cite{sa}, TCA~\cite{tca}, CORAL~\cite{coral}, DDC~\cite{tzeng_arxiv15}, DAN~\cite{dan_long15}. GFK, SA, and TCA are manifold based methods that project the source and target distributions into a lower-dimensional manifold and are not end-to-end deep methods. DDC adds a domain confusion loss to AlexNet and fine-tunes it on both the source and target domain. DAN is similar to DDC but utilizes a multi-kernel selection method for better mean embedding matching and adapts in multiple layers. For direct comparison, DAN in this paper uses the hidden layer $fc8$. For GFK, SA, TCA, and CORAL, we use the $fc7$ feature fine-tuned on the source domain ($FT7$ in~\cite{coral}) as it achieves better performance than generic pre-trained features, and train a linear SVM~\cite{sa,coral}. To have a fair comparison, we use accuracies reported by other authors with exactly the same setting or conduct experiments using the source code provided by the authors.

From Table~\ref{tab:result_office31} we can see that Deep CORAL (D-CORAL) achieves better average performance than CORAL and the other 6 baseline methods. In three 3 out of 6 shifts, it achieves the highest accuracy. For the other 3 shifts, the margin between D-CORAL and the best baseline method is very small ($\leqslant0.7$).

\begin{table}
\begin{center}
\resizebox{0.98\columnwidth}{!}{
\begin{tabular}{|l||c|c|c|c|c|c|c|}
\hline
~ & A$\rightarrow$D&	A$\rightarrow$W	&D$\rightarrow$A&	D$\rightarrow$W	&W$\rightarrow$A	&W$\rightarrow$D&AVG\\ 
\hline
GFK	&52.4\scriptsize{$\pm$0.0}	&54.7\scriptsize{$\pm$0.0}	&43.2\scriptsize{$\pm$0.0}	&92.1\scriptsize{$\pm$0.0}	&41.8\scriptsize{$\pm$0.0}	&96.2\scriptsize{$\pm$0.0}  &63.4\\
\hline
SA	&50.6\scriptsize{$\pm$0.0}	&47.4\scriptsize{$\pm$0.0}	&39.5\scriptsize{$\pm$0.0}	&89.1\scriptsize{$\pm$0.0}	&37.6\scriptsize{$\pm$0.0}	&93.8\scriptsize{$\pm$0.0}  &59.7\\
\hline
TCA	&46.8\scriptsize{$\pm$0.0}	&45.5\scriptsize{$\pm$0.0}	&36.4\scriptsize{$\pm$0.0}	&81.1\scriptsize{$\pm$0.0}	&39.5\scriptsize{$\pm$0.0}	&92.2\scriptsize{$\pm$0.0}  &56.9\\
\hline
CORAL	&65.7\scriptsize{$\pm$0.0}	&64.3\scriptsize{$\pm$0.0}	&48.5\scriptsize{$\pm$0.0}	&\textbf{96.1}\scriptsize{$\pm$0.0}	&48.2\scriptsize{$\pm$0.0}	&\textbf{99.8}\scriptsize{$\pm$0.0}  &70.4\\
\hline
CNN	 &63.8\scriptsize{$\pm$0.5}	&61.6\scriptsize{$\pm$0.5}	&51.1\scriptsize{$\pm$0.6}	&95.4\scriptsize{$\pm$0.3}	&49.8\scriptsize{$\pm$0.4}	&99.0\scriptsize{$\pm$0.2}  &70.1\\
\hline
DDC	 &64.4\scriptsize{$\pm$0.3}	&61.8\scriptsize{$\pm$0.4}	&52.1\scriptsize{$\pm$0.8}	&95.0\scriptsize{$\pm$0.5}	&\textbf{52.2}\scriptsize{$\pm$0.4}	&98.5\scriptsize{$\pm$0.4}  &70.6\\
\hline
DAN	 &65.8\scriptsize{$\pm$0.4}	&63.8\scriptsize{$\pm$0.4}	&\textbf{52.8}\scriptsize{$\pm$0.4}	&94.6\scriptsize{$\pm$0.5}	&51.9\scriptsize{$\pm$0.5}	&98.8\scriptsize{$\pm$0.6}  &71.3\\
\hline
D-CORAL	&\textbf{66.8}\scriptsize{$\pm$0.6}	&\textbf{66.4}\scriptsize{$\pm$0.4}	
&\textbf{52.8}\scriptsize{$\pm$0.2}	
&95.7\scriptsize{$\pm$0.3}	
&51.5\scriptsize{$\pm$0.3}	
&99.2\scriptsize{$\pm$0.1}  &\textbf{72.1}\\
\hline
\end{tabular}
}
\end{center}
\caption{Object recognition accuracies for all 6 domain shifts on the standard Office dataset with deep features, following the standard unsupervised adaptation protocol. }
\label{tab:result_office31}
\vspace{-0.1in}
\end{table}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{images/figure_all}
\caption{Detailed analysis of shift A$\rightarrow$W for training w/ vs. w/o CORAL loss. (a): training and test accuracies for training w/ vs. w/o CORAL loss. We can see that adding CORAL loss helps achieve much better performance on the target domain while maintaining strong classification accuracy on the source domain. (b): classification loss and CORAL loss for training w/ CORAL loss. As the last fully connected layer is randomly initialized with $\mathcal{N}(0,0.005)$, CORAL loss is very small while classification loss is very large at the beginning. After training for a few hundred iterations, these two losses are about the same. (c): CORAL distance for training w/o CORAL loss  (setting the weight to 0). The distance is getting much larger ($\geqslant100$ times larger compared to training w/ CORAL loss).}
\label{fig:a_w}
\end{figure}

To get a better understanding of Deep CORAL, we generate three plots for domain shift A$\rightarrow$W. In Figure~\ref{fig:a_w}(a) we show the training (source) and testing (target) accuracies for training with vs. without CORAL loss. We can clearly see that adding the CORAL loss helps achieve much better performance on the target domain while maintaining strong classification accuracy on the source domain. 

In Figure~\ref{fig:a_w}(b) we visualize both the classification loss and the CORAL loss for training w/ CORAL loss. As the last fully connected layer is randomly initialized with $\mathcal{N}(0,0.005)$,  in the beginning the CORAL loss is very small while the classification loss is very large. After training for a few hundred iterations, these two losses are about the same. In Figure~\ref{fig:a_w}(c) we show the CORAL distance between the domains for training w/o CORAL loss (setting the weight to 0). We can see that the distance is getting much larger ($\geqslant100$ times larger compared to training w/ CORAL loss). Comparing Figure~\ref{fig:a_w}(b) and Figure~\ref{fig:a_w}(c), we can see that even though the CORAL loss is not always decreasing during training, if we set its weight to 0, the distance between source and target domains becomes much larger. This is reasonable as fine-tuning without domain adaptation is likely to overfit the features to the source domain. Our CORAL loss constrains the distance between source and target domain during the fine-tuning process and helps to maintain an~\emph{equilibrium} where the final features work well on the target domain.
