\documentclass{article}


\PassOptionsToPackage{numbers, sort&compress, comma, square}{natbib}
\bibliographystyle{unsrt}








\usepackage[preprint]{neurips_2023}








\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{longtable}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow} \usepackage{makecell}
\usepackage{subcaption}
\usepackage{listings}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


\lstset{ basicstyle=\tiny\ttfamily,  lineskip=-1pt  }
\usepackage{float} \floatstyle{plain} \newfloat{Code}{H}{myc}

\usepackage{tcolorbox}
\definecolor{lightcyan}{rgb}{0.88, 1.0, 1.0}
\colorlet{mythmback}{lightcyan!40!white}
\newtcolorbox{boxEnv}{
colback=mythmback,coltitle=blue,colframe=mythmback,
center,
width=\linewidth,
boxrule=0.5pt,
left=5pt,right=0pt,
top=2pt,bottom=2pt,
before skip=10pt, after skip=10pt
}

\newcommand{\name}{\emph{Evol-Instruct}}
\newcommand{\modelname}{\emph{WizardCoder}}





\title{\modelname{}: Empowering Code Large Language Models with Evol-Instruct}


\author{ \quad  Ziyang Luo$^2$\thanks{\quad Equal contribution. Work done during the internship at Microsoft.}  \quad Can Xu$^1$\footnotemark[1]  \quad Pu Zhao$^1$ \quad Qingfeng Sun$^1$   \quad Xiubo Geng$^1$ \\  {\bf Wenxiang Hu}$^1$ \quad {\bf Chongyang Tao}$^1$ \quad {\bf Jing Ma}$^2$ \quad {\bf Qingwei Lin}$^1$ \quad{\bf Daxin Jiang}$^1$\thanks{\quad  Corresponding author.
 }\\
      $^1$Microsoft\\ $^2$Hong Kong Baptist University \\ 
      \texttt{\{caxu,puzhao,qins,xigeng,wenxh,chongyang.tao,qlin,djiang\}@microsoft.com}\\
      \texttt{\{cszyluo, majing\}@comp.hkbu.edu.hk}
      }
     




\begin{document}
\maketitle
\newcommand{\todo}[1]{\textcolor{brown}{{[#1]}}}






\begin{abstract}

Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce \modelname{}, which empowers Code LLMs with complex instruction fine-tuning, by adapting the \name{} method to the domain of code.
Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic’s Claude and Google’s Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at \url{https://github.com/nlpxucan/WizardLM}.

\end{abstract} \section{Introduction}

Recently, Large Language Models (LLMs)~\cite{GPT3,GPT4,PaLM,palm2,Chinchilla,gopher,GLM-130B,llama,opt} have garnered significant attention and demonstrated impressive success. Notably, OpenAI's ChatGPT stands out as a prominent example. Leveraging extensive pre-training on vast amounts of internet data and further fine-tuning with detailed instruction data~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22}, these models have achieved state-of-the-art (SOTA) zero-shot performance across diverse tasks. This trend is also observed in the domain of code understanding and generation. Numerous Code LLMs~\cite{li2023starcoder,AlphaCode,codegen,CodeGeeX,incoder,codex,codet5,CodeT5+} have been proposed to tackle the challenges associated with code-related tasks. These Code LLMs undergo pre-training using substantial amounts of code data, enabling them to excel in various code-related tasks, showcasing impressive performance.

In contrast to most previous Code LLMs that primarily emphasize the pre-training process, there has been limited exploration of fine-grained instruction tuning in the Code domain. The introduction of instruction tuning initially aimed to enhance the generalization capabilities of LMs across different tasks~\cite{t5,DBLP:conf/iclr/WeiBZGYLDDL22,flan-t5,ExT5,T0,ZeroPrompt,UnifiedQA}. OpenAI's InstructGPT~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22}, for instance, involved soliciting human annotators to provide explicit instructions to ensure alignment with users' intentions. Similarly, recent works such as Alpaca~\cite{alpaca} employed the self-instruct~\cite{wang2022self} method, where ChatGPT generated the instruction data. Vicuna~\cite{vicuna2023} utilized user-shared conversations collected from ShareGPT.com. WizardLM~\cite{xu2023wizardlm} introduced the \name{} method, which involved evolving existing instruction data to generate more complex and diverse datasets. However, it is worth noting that all these approaches primarily focused on the general domain and lacked specific design considerations for the code domain.

Motivated by the \name{} method, this study aims to enhance the capabilities of the SOTA open-source Code LLM, StarCoder~\cite{li2023starcoder}, by generating intricate code instruction data through code-specific \name{}. To achieve this, we have made several adaptations to the evolutionary prompt process tailored specifically for code-related tasks. These modifications include refining the evolutionary instructions, simplifying the form of evolutionary prompts, and incorporating code debugging and time-space complexity constraints. Initially, our method is applied to evolve the basic code instruction data, Code Alpaca~\cite{codealpaca}. Subsequently, we conduct fine-tuning of StarCoder using our newly created code instruction-following training set and obtain our \modelname{}.

The experimental results obtained from four code generation benchmarks, namely HumanEval~\cite{humeval}, HumanEval+~\cite{humanevalp}, MBPP~\cite{MBPP}, and DS-100~\cite{DS1000}, demonstrate that our \modelname{} outperforms all other open-source Code LLMs, achieving state-of-the-art (SOTA) performance. Specifically, we observe a substantial improvement in pass@1 scores, with an increase of +22.3 (57.3 vs. 35.0) in HumanEval and +8.2 (51.8 vs. 43.6) in MBPP. Remarkably, despite its much smaller size, our \modelname{} even surpasses Anthropic's Claude and Google's Bard in terms of pass rates on HumanEval and HumanEval+.

The contributions of this work can be summarized as follows:
\begin{itemize}
    \item We introduce \modelname{}, which enhances the performance of the open-source Code LLM, StarCoder, through the application of Code \name{}.
    \item \modelname{} surpasses all other open-source Code LLMs by a substantial margin in terms of code generation, including StarCoder, CodeGen, CodeGee, CodeT5+, InstructCodeT5+, StarCoder-GPTeacher, and Instruct-Codegen-16B.
    \item \modelname{} achieves superior results in code generation compared to the largest closed-source LLMs, such as Claude, Bard, PaLM, PaLM-2, and LaMDA, despite being considerably smaller in size.
\end{itemize}

 \section{Related Work}

\paragraph{Large Language Models.} Recently, LLMs have demonstrated remarkable achievements across a broad spectrum of tasks. Prominent tech companies have made significant strides in developing highly proficient LLMs. These include OpenAI's GPT3\&4~\cite{GPT3,GPT4}, Google's PaLM~\cite{PaLM,palm2}, and Bard\footnote{\url{https://bard.google.com/}}, DeepMind's Chinchilla~\cite{Chinchilla}, and Gopher~\cite{gopher}, as well as Anthropic's Claude\footnote{\url{https://www.anthropic.com/index/introducing-claude}}. However, it is important to note that these models are closed-source and can only be accessed through specific APIs or may not be accessible at all.

The AI community has witnessed the release of several open-source LLMs, where the model weights are made publicly available. EleutherAI has contributed GPT-NeoX-20B~\cite{GPT-NeoX-20B} and GPT-J-6B~\cite{gpt-j}. Google has released UL2-20B~\cite{UL2}. Tsinghua University has introduced GLM-130B~\cite{GLM-130B}. Meta has released OPT~\cite{opt} and LLaMA~\cite{llama}. It is worth noting that while these open-source models have made valuable contributions, they generally do not exhibit the same level of performance as their closed-source counterparts.

\paragraph{Large Language Models for Code.} Recent studies have introduced a significant number of LLMs for code-related tasks to address the challenges of code understanding and generation. OpenAI has unveiled Codex~\cite{codex} and Code-Davinci~\cite{Azure}. Google has proposed PaLM-Coder~\cite{PaLM}. They perform outstandingly on the popular code completion benchmarks, like HumanEval~\cite{humeval} and MBPP~\cite{MBPP}. However, these models are closed-source. 

On the other hand, there are several open-source Code LLMs available. Salesforce has introduced CodeGen~\cite{codegen}, CodeT5~\cite{codet5}, and CodeT5+~\cite{CodeT5+}. Tsinghua University has contributed CodeGeeX~\cite{CodeGeeX}, and the BigCode Project has developed StarCoder~\cite{li2023starcoder}. These models have demonstrated notable advancements in code-related tasks. However, when compared to the SOTA closed-source models, they still lag behind significantly. In contrast to the aforementioned models without instruction fine-tuning, our work demonstrates that further training Code LLMs with Code \name{} can substantially enhance performance.

\paragraph{Instruction Fine-Tuning.} 

The primary objective of instruction fine-tuning in its early stages was to enhance the cross-task generalization capabilities of LMs. This was achieved by fine-tuning LMs with a substantial corpus of public NLP tasks. T5~\cite{t5} was among the first models to explore this approach, training on a multitude of supervised text-to-text tasks. Subsequent works such as FLAN~\cite{DBLP:conf/iclr/WeiBZGYLDDL22}, ExT5~\cite{ExT5}, T0~\cite{T0}, and UnifiedQA~\cite{UnifiedQA} further expanded the range of tasks to bolster the overall generalization ability of LMs. Notably, ZeroPrompt~\cite{ZeroPrompt} and FLAN-T5~\cite{flan-t5} pushed the envelope by incorporating thousands of tasks in their training pipelines. Across these studies, a consistent finding emerges: fine-tuning LMs with diverse NLP task instructions yields significant performance improvements when applied to new tasks.

While fine-tuning LMs with diverse NLP tasks has shown promising results, it often falls short in aligning with the intentions of real-world users. OpenAI has pursued a different approach by soliciting human annotators to provide a large corpus of human instructions, encompassing diverse forms and a wide range of task types. Building upon this dataset, OpenAI trained its GPT3~\cite{GPT3} model to create InstructGPT~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22}, which better aligns with users' inputs. This line of development has even led to the impressive work known as ChatGPT. However, it is important to note that the dataset and model weights associated with these advancements are not publicly available. Alpaca~\cite{alpaca} takes a different route by adopting the self-instruct method~\cite{wang2022self}, leveraging ChatGPT to generate data for training. Vicuna~\cite{vicuna2023} utilizes user-shared conversations collected from ShareGPT.com to train its models. WizardLM~\cite{xu2023wizardlm} introduces the \name{} method, which involves evolving existing instruction data to generate more complex and diverse datasets. In contrast to these general instruction fine-tuning approaches, our \modelname{} successfully applies the \name{} method specifically in the domain of Code LLMs. \section{Approach}
In this section, we elaborate on the methodological details of \modelname{}. Following WizardLM, we apply the \name{} method to evolve Code Alpaca generated using self-instruct and fine-tune the pre-trained Code LLM StarCoder with the evolved data. 


\subsection{Evol-Instruct Prompts for Code}
Inspired by the Evol-Instruct~\cite{xu2023wizardlm} method proposed by WizardLM, this work also attempts to make code instructions more complex to enhance the fine-tuning effectiveness of code pre-trained large models. To adapt Evol-Instruct to the realm of code, we made the following modifications to the evolutionary prompt:
\begin{enumerate}
\item Streamlined the evolutionary instructions by removing deepening, complicating input, and In-Breadth Evolving.
\item Simplified the form of evolutionary prompts by unifying the evolutionary prompt template.
\item Addressing the specific characteristics of the code domain, we added two evolutionary instructions: code debugging and code time-space complexity constraints.
\end{enumerate}
The unified code evolutionary prompt template is as follows:
\begin{boxEnv}
\texttt{Please increase the difficulty of the given programming test question a bit. \\\\You can increase the difficulty using, but not limited to, the following methods:\\ \{method\}\\\\ \{question\}}
\end{boxEnv}
Here, $\{$question$\}$ represents the current code instruction awaiting evolution, and $\{$method$\}$ is the type of evolution. The five types we used are listed as follows:
\begin{boxEnv}
\texttt{Add new constraints and requirements to the original problem, adding approximately 10 additional words.\\\\Replace a commonly used requirement in the programming task with a less common and more specific one.\\\\If the original problem can be solved with only a few logical steps, please add more reasoning steps.\\\\Provide a piece of erroneous code as a reference to increase misdirection.\\\\Propose higher time or space complexity requirements, but please refrain from doing so frequently.}
\end{boxEnv}

\subsection{Training \modelname{}}
We employ the following procedure to train \modelname{}. Initially, we utilize StarCoder 15B~\cite{li2023starcoder} as the foundation and proceed to fine-tune it using the code instruction-following training set, which was evolved through \name{}. The prompt format for fine-tuning is outlined as follows:
\begin{boxEnv}
\texttt{Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \\\\\#\#\# Instruction:\\ \{instruction\}\\ \\\#\#\# Response:}
\end{boxEnv}
To construct the training dataset, we initialized it with the 20K instruction-following dataset called Code Alpaca\footnote{\url{https://github.com/sahil280114/codealpaca}}. We iteratively employ the Evol-Instruct technique on this dataset consisting of 20,000 samples to produce evolved data. After each round of data evolution, we merge the evolved data from all previous rounds with the original dataset to finetune StarCoder and assess the pass@1 metric on HumanEval~\cite{humeval}. Once we observe a decline in the pass@1 metric, we will discontinue the usage of Evol-Instruct and choose the model with the highest pass@1 as the ultimate model. \begin{figure}
\centering
     \includegraphics[width=\textwidth]{Coder_Figures/pass1.pdf}
     \caption{The percentage of pass rates on the HumanEval (164 problems) with a single attempt. All baseline scores are retrieved from the LLM-Humaneval-Benchmarks~\cite{llm-humaneval-benchmarks}. Our \modelname{} generates an answer with greedy decoding.}
     \label{fig:pass1}
\end{figure}

\section{Experiment}

This section begins by providing a comprehensive overview of the baseline models in our experiments. Subsequently, we present the performance of our models on four code generation benchmarks: HumanEval~\cite{humeval}, HumanEval+~\cite{humanevalp}, MBPP~\cite{MBPP}, and DS-1000~\cite{DS1000}.

\subsection{Baselines}

\paragraph{Closed-Source Models.} Multiple technology companies have successfully developed highly proficient LLMs while choosing not to publicly release them. These models are referred to as closed-source models. For our research, we incorporate a substantial number of these models as our baselines. Specifically, our baselines encompass the following: (i) OpenAI's GPT3.5\&4~\cite{GPT4}, Code-Davinci-002~\cite{Azure}, Code-Cushman-001~\cite{Azure}, and Codex~\cite{codex}; (ii) Google's Bard, PaLM 2~\cite{palm2}, PaLM~\cite{PaLM}, and LaMDA~\cite{LaMDA}; (iii) Google DeepMind's AlphaCode~\cite{AlphaCode}; and (iv) Anthropic's Claude.

\paragraph{Open-Source Models.} Several open-source LLMs have been made available to the AI community, although their performance generally lags behind the closed-source models a lot. As part of our research, we incorporate a significant number of these open-source models as our baselines. Our baselines encompass the following models: StarCoder~\cite{li2023starcoder}, LLaMa~\cite{llama}, CodeGen~\cite{codegen}, CodeGeeX~\cite{CodeGeeX}, CodeT5+\cite{CodeT5+}, and InCoder\cite{incoder}. In addition, we also include several models with instructions fine-tuning, including StarCoder-GPTeacher,\footnote{\url{https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct}} Instruct-Codegen-16B,\footnote{\url{https://huggingface.co/sahil2801/instruct-codegen-16B}} Guanaco-65B,\footnote{\url{https://huggingface.co/TheBloke/guanaco-65B-HF}} and Falcon-40B-Instruct.\footnote{\url{https://huggingface.co/tiiuae/falcon-40b-instruct}}

\subsection{Implementation Details}

The StarCoder~\cite{li2023starcoder} serves as our basic foundation model. The evolved dataset consists of approximately 78k samples. To fine-tune the basic models, we employ specific configurations, including a batch size of 512, a sequence length of 2048, 200 fine-tuning steps, 30 warmup steps, a learning rate of 2e-5, a Cosine learning rate scheduler, and fp16 mixed precision.

\subsection{Evaluation on HumanEval, HumanEval+, and MBPP}

HumanEval~\cite{humeval}, HumanEval+~\cite{humanevalp} and MBPP~\cite{MBPP} are extensively utilized benchmarks within the field of Code LLMs. These benchmarks encompass a vast collection of Python programming problems, employing test cases to validate the code generated by Code LLMs. HumanEval consists of 164 original programming problems, with an average of 9.6 test cases allocated to each problem. To ensure a thorough assessment of the functional correctness of LLM-synthesized code, HumanEval+ extends the number of test cases significantly, averaging at 774.8 test cases per problem. On the other hand, MBPP offers a set of 500 test programming problems, accompanied by three automated test cases per problem. The prompt format for these tasks is as follows:
\begin{boxEnv}
\texttt{Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \\\\\#\#\# Instruction:\\
Create a Python script for this problem:\\\{Question\}\\ \\\#\#\# Response:}
\end{boxEnv}

\paragraph{Comparing with the Closed-Source Models.} The SOTA LLMs for code generation, such as GPT4, Claude, and Bard, are predominantly closed-source. Acquiring access to the APIs of these models proves challenging. In this study, we adopt an alternative approach by retrieving the scores for HumanEval and HumanEval+ from the LLM-Humaneval-Benchmarks~\cite{llm-humaneval-benchmarks}. Notably, all the mentioned models generate code solutions for each problem utilizing a single attempt, and the resulting pass rate percentage is reported. To maintain consistency, we employ the same experimental setup by generating answers using greedy decoding and evaluate our \modelname{} using the provided evaluation codes. By adhering to these standardized procedures, we aim to ensure fair and comparable evaluations of our model against existing benchmarks.

As depicted in Figure~\ref{fig:pass1}, our \modelname{} attains the third position in this benchmark, surpassing Claude-Plus (59.8 vs. 53.0) and Bard (59.8 vs. 44.5). Notably, our model exhibits a substantially smaller size compared to these models. Furthermore, our \modelname{} demonstrates a remarkable superiority over other open-source LLMs that undergo instruction fine-tuning, showcasing a significant performance margin.

\begin{table}
    \centering
    \caption{Results of pass@1(\%) on HumanEval and MBPP. Most scores are retrieved from the papers of StarCoder~\cite{li2023starcoder} and CodeT5+~\cite{CodeT5+}. We follow the previous works~\cite{humeval} to generate n samples to estimate the pass@1 score with the same set of hyper-parameters: temperate=0.2, and top\_p=0.95. *: we evaluate this model by ourselves.}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{Params} & \textbf{HumanEval} & \textbf{MBPP} \\
        \midrule
        \multicolumn{4}{c}{Closed-source models}\\
        \midrule
        LaMDA~\cite{LaMDA} & 137B & 14.0 & -\\
        AlphaCode~\cite{AlphaCode} & 1.1B & 17.1 & -\\
        PaLM~\cite{PaLM} & 540B & 26.2 & 36.8\\
        PaLM-Coder~\cite{PaLM} & 540B & 36.0 & 47.0\\
        PaLM 2-S~\cite{palm2} & - & 37.6 & 50.0\\
        Codex~\cite{codex} & 2.5B & 21.4 & -\\
        Codex~\cite{codex} & 12B & 28.8 & -\\
        Code-Cushman-001~\cite{Azure} & - & 33.5 & 45.9\\
        Code-Davinci-002~\cite{Azure} & - & 47.0 & 58.1\\
        GPT-3.5~\cite{GPT4} & - & 48.1 & -\\
        GPT-4~\cite{GPT4} & - & 67.0 & -\\
        \midrule
        \multicolumn{4}{c}{Open-source models}\\
        \midrule
        LLaMa~\cite{llama} & 33B & 21.7 & 30.2\\
        LLaMa~\cite{llama} & 65B & 23.7 & 37.7\\
        CodeGen-Multi~\cite{codegen} & 16B & 18.3 & 20.9\\
        CodeGen-Mono~\cite{codegen} & 16B & 29.3 & 35.3\\
        CodeGeeX~\cite{CodeGeeX} & 13B & 22.9 & 24.4\\
        StarCoder~\cite{li2023starcoder} & 15B & 33.6 & 43.6$^*$\\
        CodeT5+~\cite{CodeT5+} & 16B & 30.9 & -\\
        InstructCodeT5+~\cite{CodeT5+} & 16B & 35.0 & -\\
        \midrule
        \modelname & 15B & \textbf{57.3} (\textcolor{red}{+22.3}) & \textbf{51.8} (\textcolor{red}{+8.2})\\
        \bottomrule
    \end{tabular}
    \label{tab:humaneval_mbpp}
\end{table} 
\paragraph{Comparing with the Open-Source Models.} In Table~\ref{tab:humaneval_mbpp}, we conduct a comprehensive comparison of our \modelname{} with other open-source models on the HumanEval and MBPP benchmarks. In contrast to the results presented in Figure~\ref{fig:pass1}, we adhere to the approach outlined in previous studies~\cite{humeval} by generating n samples for each problem to estimate the pass@1 score. The findings presented in Table~\ref{tab:humaneval_mbpp} clearly demonstrate that our \modelname{} exhibits a substantial performance advantage over all the open-source models.

From the experimental results in Figure~\ref{fig:pass1} and Table~\ref{tab:humaneval_mbpp}, we have the following conclusions:
\begin{enumerate}
    \item \modelname{} outperforms the largest closed-source LLMs, including Claude, Bard, PaLM, PaLM-2, and LaMDA, despite being significantly smaller.
    \item \modelname{} outperforms all the open-source Code LLMs by a large margin (+22.3 on HumanEval), including StarCoder, CodeGen, CodeGee, and CodeT5+.
    \item \modelname{} significantly outperforms all the open-source Code LLMs with instructions fine-tuning, including InstructCodeT5+, StarCoder-GPTeacher, and Instruct-Codegen-16B.
\end{enumerate}

\subsection{Evaluation on DS-1000}

\begin{table}
    \centering
    \caption{Performance of \modelname{} and baseline models on DS-1000. All models are evaluated with the same set of hyper-parameters: temperature=0.2, top\_p=0.5, max\_length=1024. Scores are average pass@1 accuracy over 40 samples. Matplotlib (plt) task does not have the right context, so insertion and completion scores are identical.}
    \begin{tabular}{lccccccccc}
        \toprule
        \textbf{Format} & \textbf{Model} & \textbf{plt} & \textbf{np} & \textbf{pd} & \textbf{py} & \textbf{scp} & \textbf{sk} & \textbf{tf} & \textbf{All}\\
        \midrule
        & \# of problems: & 155 & 220 & 291 & 68 & 106 & 115 & 45 & 1,000\\
        \midrule
        Completion & InCoder-6B & 28.3 & 4.4 & 3.1 & 4.4 & 2.8 & 2.8 & 3.8 & 7.4\\
        Completion & CodeGen-mono & 31.7 & 10.9 & 3.4 & 7.0 & 9.0 & 10.8 & 15.2 & 11.7\\
        Completion & Code-Cushman-001 & 40.7 & 21.8 & 7.9 & 12.4 & 11.3 & 18.0 & 12.2 & 18.1\\
        Completion & StarCoder & 51.7 & 29.7 & 11.4 & 21.4 & 20.2 & \textbf{29.5} & 24.5 & 26.0\\
        Completion & \modelname & \textbf{55.2} & \textbf{33.6} & \textbf{16.7} & \textbf{26.2} & \textbf{24.2} & 24.9 & \textbf{26.7} & \textbf{29.2}\\
        \midrule
        Insertion & InCoder-6B & 28.3 & 4.6 & 2.9 & 4.4 & 2.8 & 3.1 & 7.8 & 7.5\\
        Insertion & StarCoder & 51.7 & 30.8 & 10.3 & 21.0 & 20.2 & 27.4 & 20.0 & 25.4\\
        Insertion & \modelname & \textbf{55.2} & \textbf{35.1} & \textbf{20.4} & \textbf{30.4} & \textbf{28.9} & \textbf{32.3} & \textbf{37.8} & \textbf{32.8}\\
        \bottomrule
    \end{tabular}
    \label{tab:ds}
\end{table} 
The DS-1000 benchmark~\cite{DS1000} comprises 1,000 distinct data science workflows spanning seven libraries. It assesses the performance of code generations against test cases and supports two evaluation modes: completion and insertion. In our experiments, we only report insertion scores for models that support. The DS-1000 benchmark further classifies problems based on the libraries employed, including Matplotlib (plt), NumPy (np), Pandas (pd), SciPy (scp), Scikit-Learn (sk), PyTorch (py), and TensorFlow (tf). We follow the same prompt format as StarCoder. In Table~\ref{tab:ds}, we present pass@1 (n=40) results for each library, along with an overall score. Based on these results, our conclusion is that \modelname{} demonstrates a significant superiority over all other models when tackling data science problems on the DS-1000 benchmark. This observation holds true across nearly all data science libraries.

\subsection{Ablation Study}



\begin{figure}
\centering
     \includegraphics[width=\textwidth]{Coder_Figures/ablation.pdf}
     \caption{Ablation study on the number of data evolution rounds.}
     \label{fig:ablation}
\end{figure} 
Figure~\ref{fig:ablation} presents an ablation study investigating the impact of the number of data evolution rounds. The first round of evolved data contains 38k samples. The second round contains 58k. The third round contains 78k. The fourth round contains 98k. For consistency, all models undergo fine-tuning with 200 steps. The results reveal that the highest pass@1 score on humaneval is achieved after three rounds of data evolution. Based on this observation, we select the data that evolved during the third round as the ultimate dataset.

\begin{table}[!htb]
    \centering
    \caption{Examples of interaction with our \modelname{}.}
    \begin{tabular}{p{2.5cm}|p{10cm}}
        \toprule
        \textbf{Instruction} & \textbf{Response}\\
        \midrule
        Write a Python function to tell me what the date is today. &
        \begin{minipage}{29em}
            Here's a Python function that tells you what the date is today:

\begin{lstlisting}[language=Python]
import datetime

def get_date():
    now = datetime.datetime.now()
    return now.strftime("

print(get_date())
\end{lstlisting}

This function uses the datetime module to get the current date and time. It then uses the strftime() method to format the date as a string in the format "Day, Month Date, Year". Finally, it returns the formatted string.\\

You can call this function from your Python code to get the current date and time.
        \end{minipage}\\
        \midrule
        I do not know how to integrate a Python function numerically. Please write a code for me. &
        \begin{minipage}{29em}
        Sure, here's an example code for integrating a Python function numerically using the trapezoidal rule:

\begin{lstlisting}[language=Python]
def integrate(f, a, b, n):
    h = (b - a) / n
    s = (f(a) + f(b)) / 2
    for i in range(1, n):
        s += f(a + i * h)
    return h * s
\end{lstlisting}

This code takes in a function f, the lower bound a, the upper bound b, and the number of subintervals n. It then calculates the width of each subinterval h, and uses the trapezoidal rule to approximate the integral of f between a and b. The result is returned as a float.

To use this function, you would call it with the function you want to integrate, the lower bound, the upper bound, and the number of subintervals you want to use. For example:

\begin{lstlisting}[language=Python]
def f(x):
    return x**2

result = integrate(f, 0, 1, 1000)
print(result)
\end{lstlisting}

This would integrate the function f(x) = x**2 between 0 and 1 using 1000 subintervals, and print the result.
        \end{minipage}\\
        \midrule
        List the prime numbers between 20 and 30 with Java.&
        \begin{minipage}{29em}
Here's the Java code to list the prime numbers between 20 and 30:

\begin{lstlisting}[language=Java]
public class PrimeNumbers {
    public static void main(String[] args) {
        for (int i = 20; i <= 30; i++) {
            boolean isPrime = true;
            for (int j = 2; j < i; j++) {
                if (i isPrime = false;
                    break;
                }
            }
            if (isPrime) {
                System.out.print(i + " ");
            }
        }
    }
}
\end{lstlisting}

Output:

\begin{lstlisting}[language=Java]
23 29
\end{lstlisting}

Explanation:

- We use a nested for loop to iterate through all the numbers between 20 and 30.

- For each number, we use another for loop to check if it is prime or not.

- If a number is divisible by any number other than 1 and itself, then it is not prime.

- If a number is not divisible by any number other than 1 and itself, then it is prime.

- We print the prime numbers to the console.
        \end{minipage}\\
        \bottomrule
    \end{tabular}
    \label{tab:example}
\end{table} 
\subsection{Examples}

Table~\ref{tab:example} showcases examples of interactions with our \modelname{}. The examples demonstrate that our model consistently generates accurate responses accompanied by clear explanations. \section{Conclusion and Future Work}

This paper introduces \modelname{}, a Code \name{} fine-tuned Code LLM. The experimental results demonstrate that \modelname{} achieves SOTA performance surpassing all existing open-source Code LLMs on four widely recognized code generation benchmarks: HumanEval, HumanEval+, MBPP, and DS-1000. Furthermore, \modelname{} exhibits superior performance compared to the largest closed LLMs, including Anthropic's Claude and Google's Bard.

\paragraph{Future Work.} Although our \modelname{} demonstrates impressive coding performance, as depicted in Figure~\ref{fig:pass1}, our model still falls significantly behind the SOTA LLM, GPT4. Therefore, future work will prioritize the enhancement of the Code \name{} method to further augment the performance of our model.

\paragraph{Broader Impact.} Similar to the other LLMs, our \modelname{} could also generate unethical, harmful, or misleading information. Therefore, future research to address the ethical and societal implications is needed. 
\clearpage

{\small
\bibliography{nips2023}
}



\end{document}