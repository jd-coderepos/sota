\section{More Implementation Details}
\label{sec:more_imple}
\textbf{Training Details.}
In OSTrack-256, the input sizes of templates and search regions are  pixels and  pixels respectively, corresponding to  and  times of the target bounding box area. In OSTrack-384, the input sizes of templates and search regions are  pixels and  pixels, corresponding to  and  times of the target bounding box area.
For the GOT-10k test benchmark~\cite{got10k}, which requires training the models with only the training split of GOT-10k  (one-shot setting), we set the total training epoch to 100 with 60k image pairs per epoch, and we decrease the learning rate by a factor of 10 after 80 epochs. The other settings are kept consistent with the models trained with all datasets.

\textbf{Classification Loss.} 
We adopt the weighted focal loss~\cite{cornernet} for classification. Specifically, for each ground truth target center   and its corresponding low-resolution equivalent  , the ground truth heatmap can be generated using a Gaussian kernel as , where  is an object size-adaptive standard deviation~\cite{cornernet}. The Gaussian weighted focal loss can be formulated as:

where  and  are hyper-parameters and we set  and  as in ~\cite{cornernet, centernet}.

\textbf{Position Embeddings.}
The length of the position embeddings in the pre-trained ViT is different from the length of the input template and search region embeddings. Therefore, the pre-trained positional embeddings are interpolated (2D bicubic interpolation is adopted) to the sizes of the template and search region embeddings separately, which are further added to the patch embeddings.

\textbf{Model Details.}
In Sec.~\ref{subsec:ablation}, we compare our OSTrack (without the early candidate elimination module) with aligned two-stream trackers (\ie, STARK-aligned and SwinTrack-aligned), and we further present the detailed structures in this section. The proposed one-stream framework, as shown in Fig.~\ref{fig:arch_comapre}(a), combines feature extraction and relation modeling modules into a single ViT backbone. The aligned two-stream framework,  as shown in Fig.~\ref{fig:arch_comapre}(b), first extracts features of the template and the search region separately with the same ViT backbone and then models the feature relation with several extra Transformer encoder layers. As presented in Sec.~\ref{subsec:ablation}, this relation modeling module is instantiated with the encoder structure proposed in STARK~\cite{stark} (STARK-aligned) and SwinTrack~\cite{swintrack} (SwinTrack-aligned) separately.

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{0.05cm}
\includegraphics[width=0.8\columnwidth, keepaspectratio]{figures/compare.pdf}
\caption{\textbf{(a)} Our proposed one-stream framework without the early candidate elimination module, which combines feature extraction and relation modeling modules. \textbf{(b)} The aligned  two-stream tracking framework, which extracts features of the template and search region separately and then models the feature relation with extra Transformer encoder layers.}
\label{fig:arch_comapre}
\end{figure}

\textbf{Discriminative Regions Visualization.}
We show the method used to obtain the visualization of the discriminative regions in Fig.~\ref{fig:motivation}. Zagoruyko \etal~\cite{zagoruyko2017paying} show that the importance of a hidden neuron activation can be indicated by its absolute value. In this work, we adopt a similar approach to obtain the discriminative regions of each feature map . We first calculate the absolute mean values of each pixel along the channel dimension: 


where  is the number of channels. Then, the relative importance of each pixel is then calculated by:


where  and  return the maximum and minimum values of all pixels, respectively.

\section{More Ablation Studies}

\subsection{The Effect of Different Template Token Choices.}
As pointed out in Sec.~\ref{subsec:token_drop}, the goal of the early candidate elimination module is to identify and discard candidates belonging to background regions based on the ranking of similarity between the target and each candidate. However, the input template also contains background regions, which introduces noisy information when calculating the similarity score. Therefore,  different choices of template parts (tokens) used for the similarity calculation may influence the candidate elimination results and consequently affect the tracking performance. We compare four different template token choices (the similarity scores of all chosen template tokens are summed up for the final ranking): 1) all template tokens; 2) all template tokens within the ground truth target bounding box; 3) template tokens within a 4x4 region around the center of the template image; 4) the template token corresponding to the center of the template image. The result comparison of these template choices is shown in Tab.~\ref{tab:token_sec}. The results demonstrate that different template token choices do affect the quality of identifying background candidates. Since the input template contains background regions, directly using ``All Template Tokens'' clearly degrades the tracking performance compared with the baseline (``No Early Candidate Elimination''), \ie, 0.6\% lower in LaSOT AUC. Compared to other choices, using the central template token shows better performance, probably because the central token does not contain any background region and has aggregated the entire target information through self-attention.

\begin{table}[t]
\centering
\caption{Ablation study on different choices of template tokens used to identify candidates belonging to background.}
\label{tab:token_sec}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{c|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Template   Token Selection} & \multicolumn{3}{c|}{LaSOT} & \multicolumn{3}{c|}{TrackingNet} & \multicolumn{3}{c}{GOT-10k} \\ \cline{2-10} 
                                       & Success & P & P    & Success & P & P    & AO    & SR & SR \\ \hline
No Early Candidate Elimination         & 68.7    & 78.1   & 74.6 & 82.9    & 87.5   & 81.6 & \best{73.6} & 83.0   & 71.7   \\
All Template Tokens                & 68.1    & 77.4   & 73.5 & 82.8    & 87.5   & 81.6 & 72.9 & 82.3   & 70.1   \\
All Template Tokens within GT Box & 68.5    & 78.1   & 74.2 & \best{83.1}    & \best{87.8}   & 81.7 & \best{73.6} & \best{83.4}   & \best{72.0}   \\
Center 4x4 Template Tokens         & 68.3    & 77.6   & 73.9 & 82.9    & 87.7   & \best{82.0} & 73.5 & 83.0   & 71.5   \\
Center Template Token              & \best{69.1}    & \best{78.7}   & \best{75.2} & \best{83.1}    & \best{87.8}   & \best{82.0} & \best{73.6} & 82.8   & 71.4   \\ \hline
\end{tabular}}
\end{table} 
\subsection{Identity Embeddings and Relative Positional Embeddings}
We additionally verify the effect of adding identity embeddings and relative positional embeddings. Specifically, for the identity embeddings, we add learnable identity embeddings (to indicate a token belonging to the template or search region as in BERT~\cite{bert}) to template tokens and search region tokens separately. For the relative positional embeddings, the same method as in SwinTrack~\cite{swintrack} is adopted. The results are presented in Tab.~\ref{tab:rpe}, these two components do not bring performance gain compared to the original design, thus not adopted in our model.

\begin{table}[t]
\caption{The effect of adding additional identity embeddings to the template and search region embeddings and adding relative positional embeddings to the OSTrack-256 (without the early candidate elimination module). The results on LaSOT~\cite{lasot}, TrackingNet~\cite{trackingnet} and GOT10k~\cite{got10k} benchmarks are presented.}
\centering
\label{tab:rpe}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{l|ccc|ccc|ccc}
\hline
\multirow{2}{*}{}              & \multicolumn{3}{c|}{LaSOT} & \multicolumn{3}{c|}{TrackingNet} & \multicolumn{3}{c}{GOT-10k} \\ \cline{2-10} 
                    & Success & P & P    & Success & P & P    & AO    & SR & SR \\ \hline
Ours                & \best{68.7}    & \best{78.1}   & \best{74.6} & 82.9    & 87.5   & 81.6 & 73.6 & 83.0   & \best{71.7}   \\
+ Identity Embeddings & 68.0    & 77.3   & 73.6 & \best{83.3}    & \best{88.0}   & \best{82.2} & 73.6 & 82.9   & \best{71.7}   \\
+ Relative Positional Embeddings & 68.5    & 77.8    & 74.1   & 83.2      & 87.8      & 82.0     & \best{73.7}    & \best{83.3}    & 71.2   \\ \hline
\end{tabular}}
\end{table}

\subsection{Additional Relation Modeling Module}
To investigate whether our one-stream framework does not require an extra feature relation module, we add an additional transformer-based feature fusion module proposed in~\cite{swintrack}, which consists of 4 self-attention layers and 1 cross-attention layer, to further fusion the extracted template and search region features. As the results in Tab.~\ref{tab:add_relation} show, adding such a relation modeling module instead degrades the tracking performance, indicating that the output search region features of the ViT backbone have been sufficiently fused with the template features.

\begin{table}[t]
\caption{Add additional relation modeling module to our OSTrack-256 (without the early candidate elimination module).}
\centering
\label{tab:add_relation}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{l|ccc|ccc|ccc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{}} & \multicolumn{3}{c|}{LaSOT} & \multicolumn{3}{c|}{TrackingNet} & \multicolumn{3}{c}{GOT-10k} \\ \cline{2-10} 
\multicolumn{1}{c|}{} & Success & P & P    & Success & P & P    & AO    & SR & SR \\ \hline
Ours                  & \best{68.7}    & \best{78.1}   & \best{74.6} & \best{82.9}    & \best{87.5}   & \best{81.6} & \best{73.6} & \best{83.0}   & \best{71.7}   \\
+ Relation Modeling     &  68.5    & 78.0   & 74.1 & \best{82.9}    & 87.4   & 81.5 & 72.7 & 82.2   & 70.5   \\ \hline
\end{tabular}}
\end{table}

\subsection{Fewer Relation Modeling Layers}
In the implementation of vanilla OSTrack, all encoder layers in ViT-Base (12 layers in total) are used for simultaneous feature extraction and relation modeling. In this subsection, we try to decrease the number of layers used for relation modeling. Specifically, only the last  encoder layers are used for  simultaneous feature extraction and relation modeling, and the first  layers are only used for the template and search region feature extraction.  is set to be 6 and 3 separately and the results are presented in Tab.~\ref{tab:fusion_ratio}. The results show that using fewer encoder layers for simultaneous feature extraction and relation modeling will degrade the tracking performance, showing the necessity of sufficient feature fusion.

\begin{table}[t]
\caption{Ablation studies on the number of encoder layers used for relation modeling.}
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{c|ccc|ccc|ccc}
\hline
\multirow{2}{*}{} & \multicolumn{3}{c|}{LaSOT} & \multicolumn{3}{c|}{TrackingNet} & \multicolumn{3}{c}{GOT-10k} \\ \cline{2-10} 
    & Success & P & P    & Success & P & P    & AO    & SR & SR \\ \hline
12 (Ours) & \best{68.7}    & \best{78.1}   & \best{74.6} & 82.9    & \best{87.5}   & \best{81.6} & \best{73.6} & \best{83.0}   & \best{71.7}   \\
6  & 67.9    & 77.3   & 73.6 & \best{83.0}    & \best{87.5}   & 81.5 & 73.3 & 82.9   & 71.4   \\
3  & 67.8    & 77.0   & 73.5 & 82.7    & 87.4   & 80.7 & 72.8 & 82.5   & 70.7   \\ \hline
\end{tabular}}
\label{tab:fusion_ratio}
\end{table}

\subsection{Different Token Drop Rate}
We also try to apply a different keeping ratio  for the early candidate elimination module. As the results in Tab.~\ref{tab:keep_ratio} show, using  leads to performance drop on the LaSOT~\cite{lasot} tracking benchmark since small  may cause a significant information loss. However, the reduction in computational cost that comes with large  is limited. Setting  shows a decent decrease in computational cost with a slight improvement in tracking performance. Therefore, we use  in our experiments.

\begin{table}[t]
\caption{Different keeping ratio  used in the early candidate elimination module ( means the early candidate elimination module is not adopted).}
\centering
\label{tab:keep_ratio}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{c|ccc|ccc|ccc|c}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Keeping \\ Ratio\end{tabular}} & \multicolumn{3}{c|}{LaSOT} & \multicolumn{3}{c|}{TrackingNet} & \multicolumn{3}{c|}{GOT-10k} & \multirow{2}{*}{MACs (G)} \\ \cline{2-10}
    & Success & P & P    & Success & P & P    & AO    & SR & SR &      \\ \hline
1   & 68.7    & 78.1   & 74.6 & 82.9    & 87.5   & 81.6 & 73.6 & 83.0   & 71.7   & 29.0   \\ \hline
0.9 & 68.7    & 78.2   & 74.6 & 83.2    & 87.8   & 82.0 & \best{74.1} & \best{83.6}   & \best{71.8}   & 26.2 \\
0.8 & 68.9    & 78.4   & 74.9 & \best{83.3}    & \best{88.0}   & \best{82.3} & 73.4 & 82.7   & 71.4   & 23.6 \\
0.7 & \best{69.1}    & \best{78.7}   & \best{75.2} & 83.1    & 87.8   & 82.0 & 73.6 & 82.8   & 71.4   & 21.5 \\
0.6 & 68.4    & 77.9   & 74.3 & 83.1    & 87.6   & 81.8 & 73.5 & 82.9   & 71.7   & 19.6 \\
0.5 & 67.8    & 77.2   & 73.4 & 82.7    & 87.4   & 81.3 & 71.8 & 81.3   & 68.4   & \best{18.0}   \\ \hline
\end{tabular}}
\end{table}

\section{Results on VOT2020}
VOT2020~\cite{vot2020} is a challenging short-term tracking benchmark that is evaluated by target segmentation results. To evaluate OSTrack on VOT2020, we use AlphaRefine~\cite{alpha_refine} to generate segmentation masks, and the results are shown in Tab.~\ref{tab:vot20_results}. Since the wide existence of distractors in VOT2020, updating the template during the tracking process has become a common practice to avoid tracking drift, which can bring significant performance gain (\eg, STARK-ST50~cite{stark} raises the EAO from 0.462 to 0.505 by simply adding a dynamic template).  
OSTrack-256 obtains an EAO of 0.518, which already outperforms the STARK-ST50 with an online template updating mechanism. This demonstrates the great potential of OSTrack which serves as a neat and strong baseline model. 

\begin{table}[t!]
\caption{Comparison on VOT2020 benchmark. The left part of the trackers adopts an online template update mechanism, while the right part of the trackers does not. The best two results are shown in \rankfirst{red} and \ranksecond{blue} fonts.}
\centering
\label{tab:vot20_results}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccccc|cccc}
\hline
 &
  \begin{tabular}[c]{@{}c@{}}Ocean\\ ~\cite{ocean}\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}ATOM\\ ~\cite{atom}\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}D3S\\ ~\cite{d3s}\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}AlphaRef\\ ~\cite{alpha_refine}\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}STARK-\\ ST50~\cite{stark}\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}STARK -\\ ST101~\cite{stark}\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}SiamMask\\ ~\cite{siammask}\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}STARK-\\ S50~\cite{stark}\end{tabular} &
  OSTrack-256 &
  OSTrack-384 \\ \hline
EAO ()        & 0.43  & 0.271 & 0.439 & 0.482 & 0.505 & 0.497 & 0.321 & 0.462 & \ranksecond{0.518} & \rankfirst{0.524} \\
Accuracy ()   & 0.693 & 0.462 & 0.699 & 0.754 & 0.759 & \ranksecond{0.763} & 0.624 & 0.761 & 0.762 & \rankfirst{0.767} \\
Robustness () & 0.754 & 0.734 & 0.769 & 0.777 & 0.817 & 0.789 & 0.648 & 0.749 & \ranksecond{0.814} & \rankfirst{0.816} \\ \hline
\end{tabular}
}
\end{table}

\section{Results on ITB}
ITB~\cite{itb} benchmark is a newly collected benchmark with 9 representative scenarios and 180 diverse videos, which contains more informative tracking sequences. Tab.~\ref{tab:itb_results} shows the results of OSTrack compared with other SOTA tackers. Our OSTrack-384 achieves 64.8\% in mIoU, surpassing the previous best tracker STARK~\cite{stark} by a large margin (7.2\%).


\begin{table}[t!]
\centering
\caption{Comparison with state-of-the-arts on ITB~\cite{itb} benchmark. mIoU(\%) scores are reported. The best two results are shown in  \rankfirst{red} and \ranksecond{blue} fonts.}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccccccccc|cc}
\hline
 &  \begin{tabular}[c]{@{}c@{}}SiamRPN++\\ \cite{siamrpn++}\end{tabular}
 & \begin{tabular}[c]{@{}c@{}} Ocean\\ \cite{ocean}\end{tabular}
 & \begin{tabular}[c]{@{}c@{}}GAT\\ \cite{gat}\end{tabular}
 & \begin{tabular}[c]{@{}c@{}}ATOM\\ \cite{atom}\end{tabular}
 & \begin{tabular}[c]{@{}c@{}}DiMP\\ \cite{dimp}\end{tabular} 
 & \begin{tabular}[c]{@{}c@{}}PrDiMP\\ \cite{prdimp}\end{tabular} 
 & \begin{tabular}[c]{@{}c@{}}KYS\\ \cite{kys}\end{tabular} 
 & \begin{tabular}[c]{@{}c@{}}TrDiMP\\ \cite{trdimp}\end{tabular} 
 & \begin{tabular}[c]{@{}c@{}}TransT\\ \cite{transt}\end{tabular}
  & \begin{tabular}[c]{@{}c@{}}STARK\\ \cite{stark}\end{tabular}  
 & \begin{tabular}[c]{@{}c@{}}OSTrack\\ -246\end{tabular} 
 & \begin{tabular}[c]{@{}c@{}}OSTrack\\ -384\end{tabular} \\ \hline
mIoU    & 44.1 & 47.7 & 44.9 & 47.2  & 53.7 & 54.4 & 52.0    & 56.1 & 54.7  & 57.6 & \ranksecond{61.2} & \rankfirst{64.8} \\ \hline

\end{tabular}}

\label{tab:itb_results}
\end{table}

\section{More Visualization}
We first provide more visualization results for attention weights of the search region corresponding to the center part of the template (which can be seen as the target) in Fig.~\ref{fig:vis_attn_more}. The results show that the model attends to the foreground objects at an early stage (see ``Layer 4'' in Fig.~\ref{fig:vis_attn_more}) and finally shows great discriminative power between the target and distractors (see ``Layer 12'' in Fig.~\ref{fig:vis_attn_more}). These phenomenons demonstrate that the proposed OSTrack can extract target-oriented features with strong target-distractor discriminability.

In Fig.~\ref{fig:vis_drop_more}, more visualization results of the early candidate elimination module are presented. The results validate that the proposed method can effectively identify and discard background regions under various target categories and challenge scenarios (\eg, target deformation, occlusion, motion blur, \etc).

\begin{figure}[b!]
\centering
\setlength{\abovecaptionskip}{0.05cm}
\includegraphics[width=0.7\columnwidth, keepaspectratio]{figures/vis_attn_more.pdf}
\caption{Extended visualization for attention weights of the search region corresponding to the center part of the template after different ViT layers, the \textcolor{green}{green} rectangles indicate target objects. The results show that our one-stream framework is able to distinguish between targets and distractors and progressively focus on targets.}
\label{fig:vis_attn_more}
\end{figure}


\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{0.05cm}
\includegraphics[width=\columnwidth, keepaspectratio]{figures/vis_drop_more.pdf}
\caption{Extended visualization results of the progressive candidate elimination process. The main body of ``Input'' is the search region image, and the upper left corner shows the corresponding template image. The \textcolor{green}{Green} rectangles indicate target objects and the masked regions represent the discarded tokens. Our early candidate elimination module can effectively deal with different tracking targets and scenarios.}
\label{fig:vis_drop_more}
\end{figure}
