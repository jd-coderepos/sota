\documentclass[10pt]{article} \usepackage[preprint]{tmlr}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{color}
\usepackage{longtable}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage[noend]{algpseudocode}
\usepackage{colortbl}


\title{MVSFormer: Multi-View Stereo by Learning Robust Image Features and Temperature-based Depth}



\author{\name Chenjie Cao \email 20110980001@fudan.edu.cn \\
        \addr School of Data Science, Fudan University \\
        \AND
        \name Xinlin Ren \email 20110240015@fudan.edu.cn \\
        \addr School of Data Science, Fudan University \\
        \AND
        \name Yanwei Fu\thanks{Corresponding author.} \email yanweifu@fudan.edu.cn\\
        \addr School of Data Science, Fudan University}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{12}  \def\year{2022} \def\openreview{\url{https://openreview.net/forum?id=2VWR6JfwNo}} 


\begin{document}


\maketitle

\begin{abstract}
Feature representation learning is the key recipe for learning-based Multi-View Stereo (MVS). As the common feature extractor of learning-based MVS, vanilla Feature Pyramid Networks (FPNs) suffer from discouraged feature representations for reflection and texture-less areas, which limits the generalization of MVS. Even FPNs worked with pre-trained Convolutional Neural Networks (CNNs) fail to tackle these issues. On the other hand, Vision Transformers (ViTs) have achieved prominent success in many 2D vision tasks. Thus we ask whether ViTs can facilitate feature learning in MVS? In this paper, we propose a pre-trained ViT enhanced MVS network called MVSFormer, which can learn more reliable feature representations benefited by informative priors from ViT. 
The finetuned MVSFormer with hierarchical ViTs of efficient attention mechanisms can achieve prominent improvement based on FPNs.
Besides, the alternative MVSFormer with frozen ViT weights is further proposed. This largely alleviates the training cost with competitive performance strengthened by the attention map from the self-distillation pre-training.
MVSFormer can be generalized to various input resolutions with efficient multi-scale training strengthened by gradient accumulation. 
Moreover, we discuss the merits and drawbacks of classification and regression-based MVS methods, and further propose to unify them with a temperature-based strategy. MVSFormer achieves state-of-the-art performance on the DTU dataset. Particularly, MVSFormer ranks as Top-1 on both intermediate and advanced sets of the highly competitive Tanks-and-Temples leaderboard. Codes and models are released in \url{https://github.com/ewrfcas/MVSFormer}.
\end{abstract}


\section{Instruction}

Multi-View Stereo (MVS) aims to reconstruct highly detailed 3D representations with multi-view images, with the key step of estimating depth maps with known camera poses~\citep{furukawa2015multi}.
Many traditional methods~\citep{barnes2009patchmatch,furukawa2009accurate,Galliani_2015_ICCV,schonberger2016pixelwise} successfully make use of matching low-level features of images;
unfortunately they may be negatively affected by  various occlusions and different illumination conditions. 
To this end, learning-based methods enhanced by Deep Neural Networks (DNNs)~\citep{ji2017surfacenet,yao2018mvsnet,gu2020cascade,giang2021curvature,wei2021aa} have developed recently.  
Typically, there are three steps for learning-based MVS methods, \emph{i.e.}, feature extraction, cost volume construction, and cost volume regularization~\citep{yao2018mvsnet}.

Many works devote to formulating better cost volumes with efficient multi-stage models~\citep{gu2020cascade,yang2020cost,mi2021generalized} and visibility information~\citep{zhang2020visibility,xu2020pvsnet}. Meanwhile, learning a superior cost volume regularization with hybrid 3D U-Net structures~\citep{luo2019p,sormann2020bp} or Recurrent Neural Networks (RNNs)~\citep{yao2019recurrent,wei2021aa,wei2022bidirectional} is also shown to improve performance. Generally, the cost volume regularization is  designed to \textit{refine} noise-contaminated cost volumes with non-Lambertian surfaces or object occlusions~\citep{yao2018mvsnet} to smooth feature correlations as shown in Fig.~\ref{fig:teaser}(c)(e). 
Such regularization can not completely rectify ambiguous feature matchings from reflections or texture-less regions with unreliable 2D image features. Therefore, it is still of great significance to learn good representative features during the feature extraction to improve the generalization of MVS.

As a common solution for feature extraction, Feature Pyramid Network (FPN)~\citep{lin2017feature} learns multi-scale image features in most MVS networks. 
Some works leverage deformable convolutions~\citep{wang2021patchmatchnet,wei2021aa,ding2021transmvsnet}, attention mechanisms~\citep{yi2020pyramid,zhu2021multi,ding2021transmvsnet}, and normal curvatures~\citep{Xu_2020_CVPR,giang2021curvature} to learn more reliable features for FPNs.
Nevertheless, these works still suffer from poor generalization in modeling the reflection and texture-less regions as visualized in Fig.~\ref{fig:teaser}(c)(d). On the other hand, few previous efforts have explicitly exploring the features from the Convolutional Neural Networks (CNNs) pre-trained on extra image data~\citep{ding2022kd}, \emph{e.g.}, ResNet~\citep{he2016identity}, as such pre-trained CNNs may have some  problems in MVS:
1) \textit{low-level features of CNNs only consider limited receptive fields}, which lack the holistic image understanding, and fail to tackle with reflections and texture-less areas;
2) \textit{high-level features of CNNs are of highly semantic abstract}, thus best for the classification rather than the fine-grained visual feature matching.
We empirically validate that pre-trained CNN models fail to achieve significant improvement in MVS as in Tab.~\ref{tab:ablations}.

\begin{figure}
\begin{centering}
\includegraphics[width=0.9\linewidth]{pics/teaser_v2.pdf} 
\par\end{centering}
\caption{Hard cases in DTU~\citep{aanaes2016large} with reflection and texture-less regions. (c)(d) indicate Winner-Take-All (WTA) depth from the feature correlation (1/8 scale) achieved by the dot products among reference and source features (Eq.~\ref{eq:correlation}) before the 3D CNN cost volume regularization. WTA depth enhanced with pre-trained ViT (Twins-small~\citep{chu2021twins}) contains less noise compared with one without ViT priors. (f) can also get better final depth predictions compared with (d).\label{fig:teaser}}
\vspace{-0.1in}
\end{figure}


Recent Vision Transformers (ViTs) have achieved impressive performance on various image understanding tasks~\citep{dosovitskiy2020image,caron2021emerging,wang2021pyramid,liu2021swin,chu2021twins,he2021masked}. 
Thus, a nature question is \textit{whether we can significantly strengthen the feature representation learning of MVS  with pre-trained transformers from external 2D image dataset?} 
For the issues of reflections and texture-less regions in MVS, ViTs equipped with long-range attention modules can provide global understanding for MVS models rather than the low-level textures. Moreover, the patch-wise feature encoding of ViTs works reasonably well for feature matching~\citep{sun2021loftr,jiang2021cotr}. Since the depth prediction is intrinsically a 1D feature matching problem along epipolar lines~\citep{furukawa2015multi}, ViTs shall be the recipe for learning-based MVS.
Unfortunately, to the best of our knowledge, there is no work explicitly exploiting pre-trained ViTs for MVS. 

To this end, we thoroughly explore how to make pre-trained ViTs boost the MVS performance. 
As shown in Fig.~\ref{fig:teaser}(e)(f), features from pre-trained ViT are complementary to those from FPN, and facilitate better modeling reflection and texture-less regions in MVS. Formally, we propose using the pre-trained ViTs to enhance FPN for feature extraction in MVS, and further formulate a novel MVS Transformer (MVSFormer). 
Specifically, we employ the hierarchical ViT Twins~\citep{chu2021twins} as the backbone of MVSFormer. Benefited by the pyramid architecture and the efficient attention mechanism, MVSFormer can be trained in high-resolution and typically achieve better results compared with FPNs.
Moreover, we also extend the alternative MVSFormer to plain-ViT with vanilla attention~\citep{dosovitskiy2020image,caron2021emerging,he2021masked}, called MVSFormer-P. 
Compared with using efficient and hierarchical ViTs, we freeze the \textit{off-the-shelf} ViT backbone pre-trained by the self-distillation method -- DINO~\citep{caron2021emerging} in MVSFormer-P\footnote{\scriptsize{Intuitively Twins can also be pre-trained by the self-supervised task as DINO. However, it is non-trivial to implement and train it, due to technical difficulty and expensive running costs on ImageNet (taking 1 month on 2 V100 GPUs). Thus the self-supervised Twins is beyond the scope of this paper.}} during the training to reduce the computation as in Fig.~\ref{fig:overview}(A). 
Note that the MVSFormer-P still enjoys superior performance compared with other state-of-the-art MVS methods. 

Further, we present an efficient multi-scale strategy to train  MVSFormer, as it is non-trivial to directly train ViTs for MVS. In MVS tasks, the models should be tested on various high-resolution images~\citep{aanaes2016large,Knapitsch2017}, while they have to be trained on low-resolution to save the training computations. It is thus not amenable to directly extending pre-trained ViTs without scale invariance to MVS tasks. Our empirical experiments in Sec.~\ref{sec:quantitative} show that the proposed multi-scale strategy is sufficient to generalize MVSFormer to test images with much higher resolutions (1536 or 1920) compared with the largest one during the training (1280).

Furthermore, we technically unify the advantages of both regressive depth (regression), and \emph{argmax} depth (classification) for MVSFormer.
Different from previous works~\citep{wang2022itermvs,peng2022rethinking,wang2022mvster}, we find that optimizing the MVS network with cross-entropy loss can achieve much more reliable confidence maps but slightly worse depth predictions. Because the \emph{argmax} operation can not provide exact depth results, which harms the depth performance.
To address this issue, MVSFormer predicts the temperature-based depth expectation instead of the \emph{argmax} during the inference and achieves smooth depth predictions and superior final point clouds.

Our contributions can be highlighted as: (1) To the best of our knowledge, it is the first work that systematically explores the influence of pre-trained ViTs on MVS. Learning a better feature representation by the feature extractor is important to set up a bridge between 2D vision and 3D MVS tasks. (2) We propose a novel ViT enhanced MVS network -- MVSFormer, which is further trained with the efficient multi-scale training strategy to be generalized for various resolutions. The proposed Twins-small pre-trained MVSFormer remarkably reduces the overall error of point cloud reconstruction in DTU from 0.312 to 0.289 compared with the CNN-based pre-trained ResNet with competitive computations and all other model settings unchanged as in Tab.~\ref{tab:ablations}.
(3) We analyze the merits and limitations of regression and classification-based MVS, and propose a simple but effective way to unify both.Empirical qualitative evidences in Fig.~\ref{fig:cls_reg} and Fig.~\ref{fig:tnt_cla_reg} show that classification-based confidence can filter outliers for the real-world reconstruction. And quantitative results in Tab.~\ref{tab:ablation_cls_t} indicate that our temperature-based depth predictions also enjoy superior point cloud metrics. (4) The proposed methods can achieve the state-of-the-art performance in both DTU dataset~\citep{aanaes2016large} and Tanks-and-Temples~\citep{Knapitsch2017}.

\section{Related Works}

\noindent\textbf{Learning-based MVS methods.} 
Learning-based MVS methods enhanced with DNNs have been extensively studied for MVS tasks~\citep{ji2017surfacenet} .
MVSNet~\citep{yao2018mvsnet} proposes an end-to-end pipeline based on a 2D CNN image feature extraction, cost volumes formulated by homography warping, and a cost volume regularization by 3D CNN.
Many works are devoted to reducing the heavy computation of 3D CNN-based cost volume regularization with coarse-to-fine strategies~\citep{gu2020cascade,yang2020cost,mi2021generalized} and RNNs~\citep{yao2019recurrent,wei2021aa,wei2022bidirectional}. 
Meanwhile, some researches formulate a more reliable cost volume, such as the visibility of ViS-MVSNet~\citep{zhang2020visibility}, and the epipolar aggregation of MVSTER~\citep{wang2022mvster}.
Besides, many works try to learn a better cost regularization by hybrid 3D U-Net~\citep{luo2019p,sormann2020bp}, RNN-3DCNN~\citep{wei2021aa}, and epipolar attention~\citep{ma2021epp,yang2021mvs2d}.
Although many efforts are paid to achieve better cost volumes, we advocate that learning superior feature representations are more effective for a generalized MVS method. Our method explores using pre-trained ViTs to improve  MVS feature learning, which is orthogonal to these works based on cost volumes.


\noindent\textbf{Feature learning in MVS.}
Since 2D image features are critical in MVS learning, powerful FPN~\citep{lin2017feature} is the common solution. FPN is designed as a U-Net  to fuse multi-scale features. Deformation convolutions~\citep{dai2017deformable} are widely used in MVS to improve the receptive fields flexibly~\citep{wang2021patchmatchnet,wei2021aa,mi2021generalized,ding2021transmvsnet}. Furthermore,~\citet{Xu_2020_CVPR} and~\citet{giang2021curvature} leverage fixed and learnable normal curvatures to dynamically select kernel sizes for FPN, which can learn robust features for various image resolutions.
Besides, the attention mechanism is also utilized for MVS feature learning. Many works leverage intra- or inter-attention during the feature learning~\citep{yi2020pyramid,zhu2021multi,ding2021transmvsnet} for long-range feature dependencies.
As these efforts exactly improve the performance of the FPN, we should notice that such features still have inductive biases from CNNs, thus leading to inferior performance to patch-wise ViTs~\citep{dosovitskiy2020image}. There is no systematical exploration of pre-trained ViT models to MVS feature learning.


\noindent\textbf{Vision Transformers.}
Self-supervise pre-trained transformers have achieved prominent success in Natural Language Processing (NLP)~\citep{vaswani2017attention,devlin2018bert,brown2020language}. Inspired by these achievements, transformers are also introduced into the computer vision~\citep{dosovitskiy2020image,liu2021swin,he2021masked}. 
ViTs achieve state-of-the-art performance in many vision tasks, which include image classification~\citep{dosovitskiy2020image,liu2021swin}, detection and segmentation~\citep{zheng2021rethinking,li2022exploring}, and so on. 
Moreover, task specific transformers also achieve successes in optical flow~\citep{huang2022flowformer} and point cloud processing~\citep{guo2021pct,zhao2021point}.
Compared with CNNs, ViTs enjoy much more long-range modelling capacity. Since ViTs lack some inductive biases inherent to CNNs, such as translation equivalence and locality, they are much more data-hungry to generalize to unseen data~\citep{d2021convit,dosovitskiy2020image,xu2021vitae}. Therefore, pre-training is necessary for ViTs. 
For different pre-training tasks, ViTs can be categorized into self-supervised ones~\citep{bao2021beit,he2021masked,caron2021emerging} and supervised ones~\citep{dosovitskiy2020image,liu2021swin,chu2021twins}. Supervised ViTs are usually pre-trained for the classification task, while self-supervised ones are implemented with masked prediction~\citep{he2021masked,bao2021beit} or contrastive learning~\citep{caron2021emerging,Chen_2021_ICCV}. Furthermore, to reduce notorious computations and memory costs of the vanilla attention, many ViTs use multi-scale architectures~\citep{liu2021swin,wang2021pyramid,chu2021twins} instead of single-scale backbones. Particularly, many works tried to finetune pre-trained ViTs with task specific components, such as FPNs or CNNs, for various downstream tasks~\citep{li2022exploring,huang2022flowformer,chen2021pre}.
Although pre-trained ViTs are appealing in many fields, releasing their potential in MVS is non-trivial, which will be discussed in the following sections.

\noindent\textbf{Multi-scale training.}
It is well known that CNNs can be generalized to higher resolutions for high-level vision tasks such as classification~\citep{chu2021conditional}, which is benefited from their scale invariance. 
Some classical object detection methods~\citep{cai2016unified,redmon2017yolo9000} emphasize the importance to use multi-scale training for better robustness to various image scales.
Further, some recent generation works~\citep{dong2022incremental,chai2022any,cao2022learning} show that multi-scale training is also a data and computation efficient way to improve the generation quality at high resolution.
For the MVS learning, dynamic kernels enhanced by the curvature clues~\citep{giang2021curvature} are leveraged to tackle the image scale gaps for CNN-based models.
Unfortunately, the attention-based ViTs generally do not have the translation equivalence and locality; and these make ViTs more easily overfitting to specific resolution or sequence length than CNNs~\citep{d2021convit,dosovitskiy2020image,xu2021vitae}. Although Conditional Positional Encoding (CPE) and zero-padding are utilized to alleviate the spatial overfitting of ViT~\citep{chu2021conditional}, multi-scale training is still adopted in many ViTs for high-resolution detection tasks~\citep{liu2021swin,chu2021twins,carion2020end}. 
In this paper, we empirically evaluated that multi-scale training is the key strategy to release the potential of ViTs for MVS in Sec.~\ref{sec:aba}. Technically, we repurpose the gradient accumulation to improve GPU utilization for better efficiency of the multi-scale training.


\section{Method}

\noindent\textbf{Overview.}
The architecture of MVSFormer is overviewed in Fig.~\ref{fig:overview}, as the seminal MVS pipeline in \citet{yao2018mvsnet}. 
Given a group of  images with different views which contain reference image  and source images , as well as the their camera intrinsics and extrinsics, MVSFormer  learns  feature representation in feature extraction (Sec.~\ref{sec:feature_extraction}), enhanced by the hierarchical ViT-- Twins~\citep{chu2021twins} (Fig.~\ref{fig:overview}(a)) or the plain ViT-- DINO~\citep{caron2021emerging} (Fig.~\ref{fig:overview}(b)) with several novel training strategies (Sec.~\ref{sec:multi_scale_training}). Then the multi-stage cost volume formulation and regularization are presented to compute the probabilities of coarse-to-fine depth hypotheses (Sec.~\ref{sec:correlation_volume}).
Finally, cross-entropy loss is employed to optimize the MVSFormer, while the inference is made on depth expectation (Sec.~\ref{sec:loss}).

\noindent\textbf{Preliminaries}.
(1) \textit{Twins}~\citep{chu2021twins} is pre-trained in a supervised way by the hierarchical-ViT model as in Fig.~\ref{fig:overview}(a).
To further reduce the complexity, Twins employs the separable locally-grouped self-attention and global sub-sampled attention to construct each attention block. Such a global and local design outperforms the classic pyramid ViT~\citep{wang2021pyramid}. 
Twins also leverages CPE~\citep{chu2021conditional} with 2D depthwise convolutions instead of the absolute positional encoding in other ViTs~\citep{dosovitskiy2020image}.
(2) \textit{DINO}~\citep{caron2021emerging} is pre-trained in a self-supervised way by the self-distillation of plain-ViT as in Fig.~\ref{fig:overview}(b).
The prominent characteristic of DINO is that attention maps of its last layer can learn class-specific features leading to unsupervised object segmentations as discussed in ~\citet{caron2021emerging}, and shown in Fig.~\ref{fig:dino_att}. 
Thanks to the unsupervised training and the multi-crop strategy, the feature representations of DINO can be well generalized to various environments, illuminations, and resolutions.

\begin{figure}
\begin{centering}
\includegraphics[width=0.8\linewidth]{pics/plain-pyramid} 
\par\end{centering}
\caption{The overview of MVSFormer. (A) Feature extractors of hierarchical ViT (a) and plain ViT (b). Inputs for ViTs are downsampled to the 1/2 resolution. (B) Multi-scale cost volume formulation and regularization.
`Warping': warping source features with upsampled depth hypotheses (Eq.~\ref{eq:warp}) for cost volumes (Eq.~\ref{eq:correlation}). `Volume Fusion': fusing cost volumes from all source views with respective visibility (Eq.~\ref{eq:fusion}).}
\label{fig:overview}
\vspace{-0.1in}
\end{figure}


\subsection{Feature Extraction}
\label{sec:feature_extraction}

As many MVS works, we also use FPN~\citep{lin2017feature} as the main feature extractor, which is enhanced with pre-trained ViTs. 
In MVSFormer, ViTs work to formulate global feature correlations, while FPN is devoted to learning detailed ones. 
Before taking reference and source images to the ViT, we first downsample them into  to save the computation and memory costs. We resize the absolute position encoding of pre-trained ViTs with bicubic interpolation to fit different image scales~\citep{dosovitskiy2020image}.
Then hierarchical-ViT output  or plain-ViT output  is directly added to the highest level feature of the FPN encoder.
Thus we can get coarse-to-fine features  from the FPN decoder scaled from () to () of the origin resolution as shown in Fig.~\ref{fig:overview}(A). These features contain priors from both ViT and CNN, and will be leveraged to formulate more reliable cost volumes.
We have tried other feature fusion strategies in Sec.~\ref{sec:appendix_fusion_ways} of the Appendix, but the difference is negligible. So the simple but effective feature addition is adopted in MVSFormer.

\noindent\textbf{MVSFormer with trainable Twins.}
The Twins is used as the backbone in our default MVSFormer without additional explanations, because it enjoys the best reconstruction performance as in Sec.~\ref{sec:quantitative}.
To finetune MVSFormer in various resolutions, the ViT backbone should meet two conditions, \emph{i.e.}, the efficient attention mechanism and the robust position encoding for different scales, which are both solved by Twins elegantly. Except for the pyramid architecture, CPE in Twins can learn positional cues from the zero-padding~\citep{islam2020much}, and breaks the permutation-equivalent of ViTs with proper CNN inductive biases.
As shown in Fig.~\ref{fig:overview}(b), MVSFormer encodes 4 multi-scale features  with  of the origin resolution respectively. We use another FPN to upsample these multi-scale features as

Benefited by the efficient attention design, we can finetune the pre-trained Twins during the training phase with a relatively low learning rate in various resolutions. More details are discussed in Sec.~\ref{sec:implementation_details}.


\noindent\textbf{MVSFormer-P with frozen DINO.}
We also explore learning MVSFormer based on plain-ViT with vanilla attention, \emph{i.e.}, MVSFormer-P. Although plain ViTs suffer from heavy computations for the high-resolution MVS learning, we find that the self-supervised ViT -- DINO enjoys good properties to improve the feature representation with even frozen ViT backbones.
Particularly, attention maps of [\emph{CLS}] token from the last layer of DINO as in Fig.~\ref{fig:dino_att} are utilized to strengthen the feature learning.
Since MVS is a feature matching task essentially, the priors of object or scene segmentations are useful to avoid confused depth predictions of foreground and background.
As inputs to ViT have been halved yet, after the patch-wise embedding with kernel and stride size 16, DINO performs the attention-based learning for feature maps with the resolution of .
To better utilize the good segmentation property of DINO, we use a trainable Gated Linear Unit (GLU)~\citep{shazeer2020glu} to reduce the dimension of DINO features . We assume that   indicates attention maps of the [\emph{CLS}] token with  attention heads from the last layer of DINO.  is averaged along  heads from . Thus GLU can be written as

where ~\citep{ramachandran2017searching};  means the element-wise multiplication;  indicates the concat operation;  is the reduced channel of DINO. GLU helps to protect important features benefited from the segmentation attention maps during the dimension reduction, which can effectively improve the MVS performance. 
Then we use two transpose convolutions to upsample  to  for the feature addition to the FPN encoder with channels . Although the plain-ViT based DINO suffers from heavy memory and computation costs, we find that MVSFormer-P can still work well with trainable GLU and upsample convolutions when DINO is frozen. Therefore, the alternative MVSFormer-P only demands a little more memory cost compared with the vanilla MVS FPN training; and achieves competitive results as the full trainable MVSFormer.



\begin{figure}
\begin{centering}
\includegraphics[width=0.9\linewidth]{pics/dino_att.pdf} 
\par\end{centering}
\caption{Mean attention maps of [\emph{CLS}] token from DINO~\citep{caron2021emerging} with various resolutions. Note that 576768 and 544960 are max input sizes for DTU and Tank-and-Temples to our MVSFormer-P.}
\label{fig:dino_att}
\vspace{-0.1in}
\end{figure}

\subsection{Efficient Multi-scale Training}
\label{sec:multi_scale_training}
Although ViTs has large capacity, missing translation equivalence and locality make them vulnerable to handling various input resolutions~\citep{xu2021vitae}. Unfortunately, most MVS tasks should be tested in different High-Resolution (HR) (from 12001600~\citep{aanaes2016large} to 10801920~\citep{Knapitsch2017}). CNN-based methods can largely solve this problem with dynamic kernels~\citep{giang2021curvature} and random cropping~\citep{mi2021generalized}. Most importantly, CNNs can process arbitrary input sizes benefited by their inductive biases, \emph{i.e.}, translation equivalence, and locality.
For the trainable Twins in MVSFormer, training with the same resolution tends to overfit one input size, and fails to be generalized to HR cases.
\footnote{\scriptsize{For multi-crop augmented DINO frozen in MVSFormer-P, the scale issue is alleviated; but it still limits the performance (Tab.~\ref{tab:ablations}).}}


Thus, we repurpose learning features with multi-scale training,  originated from ViT-based detection tasks. Particularly, for efficient multi-scale training, we have to ensure that 1) image sizes for each batch should be the same; 2) dynamically changing the batch size according to image sizes, which aims to make the full usage of limited memory. 
Specifically, we train our models with dynamic resolutions from 512 to 1280, while the aspect ratios are randomly sampled from 0.8 to 0.67.
Instead of the compromise by a minimum batch size, we keep the multi-scale training with the largest batch size, assisted by the gradient accumulation. 
The gradient accumulation splits a batch into several sub-batches and accumulates their gradients to update the model. 
All instances are grouped into different pairs of \emph{resolution and sub-batch size} at the start of each epoch randomly.
Note that a larger image should have a smaller sub-batch to balance the memory cost and vice versa.
Training with a larger batch size contributes to faster convergence with lower variances and better performance for BatchNorm layers~\citep{ioffe2017batch}. Therefore, the gradient accumulation significantly improves the efficiency of  multi-scale training of MVSFormer. We find that dynamic training sizes from 512 to 1280 are sufficient to generalize MVSFormer to at least 2K resolution of Tanks-and-Temples~\citep{Knapitsch2017} as compared in Tab.~\ref{tab:quant_TnT} and Fig.~\ref{fig:TNT_depth}. 
More details about multi-scale training are  in Sec.~\ref{sec:appendix_multiscale_impl} of Appendix.



\subsection{Correlation Volume Construction}
\label{sec:correlation_volume}

We elaborate the classical components of cost volume construction and regularization from~\citet{gu2020cascade,zhang2020visibility,giang2021curvature}. Note that such components are orthogonal to our key contributions, as the primary focus of this paper is on better feature extraction for MVS.
To achieve the multi-stage cost volume, we first initialize a group of inverse depth ranges  for each stage. 
Here we omit the superscript of -th stage for the simplification.
Features of source views are warped to the reference view. Given a 2D pixel  of the reference image  with known camera intrinsics  of reference and source views, as well as their rotation  and translation ,  warped  with the -th depth hypothesis in source image  can be written as

Then the group-wise pooling~\citep{guo2019group} is leveraged to split features into  groups along the channel dimension. And the feature correlation  can be formulated by the inner production of group-wise reference features
 and warped source features  as

where  is the channel of  and .
Then the feature correlation from Eq.(\ref{eq:correlation}) is further averaged for each group to  for an efficient cost volume formulation. 
We also train a 2D CNN to learn pixel-wise weight visibility  for each source view through the entropy of normalized correlations~\citep{zhang2020visibility,giang2021curvature}. Thus  source feature correlations can be fused with their visibility as

which is the input for the 3D U-Net cost volume regularization. After being regularized by the 3D U-Net, we can achieve pixel-wise output 3D cost volume  for each stage.


\subsection{Temperature-based Depth Prediction}
\label{sec:loss}
Given the cost volume  from the 3D U-Net, the probability volume of depth hypotheses can be achieved by the  along the depth dimension as .
REGression-based depth (REG) utilizes \emph{soft-argmin}~\citep{kendall2017end} to softly weighting for each depth hypothesis, \emph{i.e.}, the expectation of  with probability . 
For the CLAssification-based depth (CLA), the predicted depth  is simply selected from all depth hypotheses with the maximum probability, \emph{i.e.} the \emph{argmax} depth hypothesis.
Thus the regressive depth  and the classification depth  are

 is optimized with the  loss with the ground-truth depth, while  is optimized with the Cross-Entropy (CE) with the one-hot ground-truth depth volume. 

\begin{figure}
\begin{centering}
\includegraphics[width=0.95\linewidth]{pics/cls-reg.pdf} 
\par\end{centering}
\caption{(a) Multi-stage confidence of regression and classification.
Both depth ranges of regression and classification based methods miss ground truth depth (8.8) in stage-1. Regression still provides high probability for the depth upper bound. (b) An DTU example: the kettle handle can be kept by confidence map of classification-based method. 
Confidence maps are got by Eq.(\ref{eq:get_conf}) of the Appendix.
}
\label{fig:cls_reg}
\vspace{-0.1in}
\end{figure}

\noindent \textbf{Remark}.
\citet{peng2022rethinking} think that REGs suffer from the overfitting issue, and lead to ambiguous depth prediction, whilst CLAs are more robust but fail to achieve exact depth results.
In contrast, we empirically find a different conclusion that confidence maps from CLAs are better than the REGs; and this should not be neglected especially for the widely used multi-stage MVS models~\citep{gu2020cascade,yang2020cost,mi2021generalized}. Particularly, MVS networks can not ensure all predicted depth maps are correct, due to reflection, occlusion, or missing reliable source views. Thus, providing solid confidence (uncertainty) maps of depth~\citep{kendall2017uncertainties} is also important for MVS to reconstruct good point clouds as empirically evaluated in Fig.~\ref{fig:tnt_cla_reg} and Fig.~\ref{fig:real-world}.
The way to get confidence maps of REGs and CLAs are summarized in Sec.~\ref{sec:appendix_conf}.
However, as shown in Fig.~\ref{fig:cls_reg}(a), the REG maintains high-confidence values even for out-of-range depth hypotheses in stage-2,3,4. 
It is difficult for REGs to filter outliers without hurting other points of correct depth as illustrated in Fig.~\ref{fig:cls_reg}(b). 
Since CE can not tackle out-of-range depth labels, we advocate masking all depth outliers in training as~\citet{mi2021generalized}. Note that we also tried to optimize the MVS with masked  loss, but its performance is inferior to regular regression.

Although CLA has many good properties for MVS, REGs can achieve superior performance of depth and point cloud compared with CLAs in our early experiments.
So we target on the issue mentioned in~\citet{peng2022rethinking}, \emph{i.e.}, inexact depth predictions. 
UniMVSNet~\citep{peng2022rethinking} designs a Unified Focal Loss (UFL) to solve it, which regards CE as multiple Binary Cross-Entropy (BCE). And the focal loss~\citep{lin2017focal} controlled by several hyper-parameters is used to solve the imbalance problem in BCE. Different from UFL, we propose a simple way to unify both REGs and CLAs, which only adjusts the inference process without re-training the model. We first multiply a temperature  to the cost volume  before the , and rewrite  to the temperature-based depth expectation  as

Obviously, when  or ,   is equivalent to  , or , respectively. 
The core idea is to adjust the temperature  during the inference to unify CLAs and REGs. For early stages with low-resolution, we set larger  to make the model work as a CLA for a better global distinguishing ability. And for later stages with high-resolution, our model tends to use lower  as a REG to smooth local details. In practice, we set \footnote{\scriptsize{Such a temperature setting is always fixed for all datasets and instances without data dependency. As analyzed in Sec.~\ref{sec:appendix_analysis_of_depth}, this setting may not be the best one, but the difference is not obvious. And the critical idea of `classify first, then regress' is the same.}} and achieve better performance than classification (), regression (), and other consistent settings of  as evaluated in Sec.~\ref{sec:aba}.
Note that  is only used during testing, as the masked CLA optimized with CE is robust enough for MVS learning. Thus,  is adopted in MVSFormer for the training phase. 
Although adjusting the temperature during the testing may suffer from some implications of the discrepancy between the train and test stages, we just tend to regress in the latter stages with only a few nearby depth hypotheses. Thus such gap is largely narrowed. And our temperature setting is generalized and effective enough for various datasets. More details are discussed in Sec.~\ref{sec:appendix_analysis_of_depth}.

\section{Experiments}
\label{sec:implementation_details}
\noindent\textbf{Settings.}
Our methods are evaluated on DTU~\citep{aanaes2016large}, Tanks-and-Temples~\citep{Knapitsch2017} and ETH3D~\citep{schops2017multi}.
Since DTU data is collected in an indoor environment with fixed camera poses, our model is finetuned on the BlendedMVS dataset~\citep{yao2020blendedmvs} with various scenes and objects to generalize more complex environments in Tanks-and-Temples and ETH3D, as standard practice in \citet{giang2021curvature,ding2021transmvsnet}.
MVSFormer is trained by the view number  of 4 coarse-to-fine stages of 32-16-8-4 depth hypotheses. 
CNN parts in MVSFormer are trained by Adam with a learning rate of 1e-3.
The part of Twins-small in MVSFormer is trained with learning rate 3e-5 and  weights decay, while DINO-small is frozen in MVSFormer-P.
Our models are trained by 10 epochs on DTU and finetuned with another 10 epochs on BlendedMVS. The learning rate is warmed up with 500 steps and then decayed with the cosine scheduler. For the multi-scale training, we dynamically change the sub-batch from 8 to 2 according to the scales from 512 to 1280, with a maximum batch size of 8. 
More details are in Appendix Sec.~\ref{sec:appendix_impl}.


\subsection{Quantitative Results}
\label{sec:quantitative}

\begin{table}
\vspace{-0.1in}
\small 
\caption{Quantitative point cloud results (mm) on DTU (lower is better). Best results are in bold, and second ones are underlined. * denotes that GBiNet is re-tested with the same post-processing threshold to all scans for fair comparisons with other methods. 
\label{tab:quant_DTU}}
\centering
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{cccc}
\toprule 
Methods & Accuracy & Completeness  & Overall\tabularnewline
\midrule 
Gipuma~\citep{Galliani_2015_ICCV} & \textbf{0.283} & 0.873 & 0.578\tabularnewline
COLMAP~\citep{schonberger2016pixelwise} & 0.400 & 0.664 & 0.532\tabularnewline
\midrule 
R-MVSNet~\citep{yao2019recurrent} & 0.385 & 0.459 & 0.422\tabularnewline
AA-RMVSNet~\citep{wei2021aa} & 0.376 & 0.339 & 0.357\tabularnewline
CasMVSNet~\citep{gu2020cascade} & 0.325 & 0.385 & 0.355\tabularnewline
CDS-MVSNet~\citep{giang2021curvature} & 0.352 & 0.280 & 0.316\tabularnewline
UniMVSNet~\citep{peng2022rethinking} & 0.352 & 0.278 & 0.315\tabularnewline
TransMVSNet~\citep{ding2021transmvsnet} & 0.321 & 0.289 & 0.305\tabularnewline
GBiNet{*}~\citep{mi2021generalized}  & \underline{0.312} & 0.293 & 0.303 \tabularnewline
\midrule 
MVSFormer (ours) & 0.327 & \textbf{0.251} & \textbf{0.289}\tabularnewline
MVSFormer-P (ours) & 0.327 & \underline{0.265} & \underline{0.296}\tabularnewline
\bottomrule 
\end{tabular}}
\vspace{-0.1in}
\end{table}

\textbf{DTU.} Our MVSFormer is evaluated on DTU with the official evaluation metrics of point clouds, \emph{i.e,} accuracy, completeness, and the overall error. The testing resolution is fixed in  and the view number . We use the depth fusion of Gipuma~\citep{Galliani_2015_ICCV} with a consistent confidence threshold  for the point clouds. Quantitative results of DTU are shown in Tab.~\ref{tab:quant_DTU}, and qualitative ones are shown in Fig.~\ref{fig:dtu_depth_qualitative} and Fig.~\ref{fig:dtu_ply_compare} of the Appendix.
Note that the post-processing hyper-parameters of all scans are fixed for learning-based methods. Traditional methods~\citep{Galliani_2015_ICCV,schonberger2016pixelwise} fail to get good completeness, which means that they have missed many points with sparse results. For learning-based methods, Our full trainable MVSFormer can achieve the best completeness and overall error. MVSFormer-P is the second-best in overall error which enjoys faster efficiency. Therefore, our methods can get more complete point clouds compared with other competitors. Note that results reported in GBiNet~\citep{mi2021generalized} need to use different hyper-parameters for the post-processing. Our methods can outperform GBiNet with fixed hyper-parameter settings for all scans. With the impressive improvements achieved by MVSFormer, we think that pre-trained ViTs have the potential to push the limits of MVS.  


\noindent\textbf{Tanks-and-Temples.} 
Our submission of the full trainable MVSFormer has \emph{ranked Top-1 on both intermediate and advanced sets of the official Tanks-and-Temples leaderboard} compared with other published works since May/2022.
We show quantitative results on both intermediate and advanced sets in Tab.~\ref{tab:quant_TnT}. 
All instances are inferred with the original  image size, and more detailed settings are in the Appendix.
The metric is officially evaluated by the F-score based on precision and recall of submitted point clouds~\citep{Knapitsch2017}. 
MVSFormer outperforms all other state-of-the-art methods with mean F-scores of 66.37 and 40.87 for intermediate and advanced sets respectively.
As shown in Tab.~\ref{tab:quant_TnT}, our method can achieve the best or second-best results in almost all cases except `Auditorium', which demonstrates its good generalization and impressive performance. Furthermore, confidence maps from CLA can filter outliers and get more precise point clouds as shown in the Appendix. Besides, the proposed multi-scale strategy can generalize MVSFormer to fit larger resolutions, such as 2K. More qualitative results are  in Fig.~\ref{fig:TNT_depth} and Fig.~\ref{fig:tnt_qual} of the Appendix.

\begin{table}
\vspace{-0.1in}
\caption{Quantitative results of F-score on Tanks-and-Temples. Higher F-score means a better reconstruction quality. Best results are in bold, while the second ones are underlined.
\label{tab:quant_TnT}}
\centering
\setlength{\tabcolsep}{0.8mm}{
{\scriptsize{}}{\scriptsize{}}\begin{tabular}{c|ccccccccc|ccccccc}
\hline 
\multirow{2}{*}{{\scriptsize{}Methods}} & \multicolumn{9}{c|}{{\scriptsize{}Intermediate}} & \multicolumn{7}{c}{{\scriptsize{}Advanced}}\tabularnewline
\cline{2-17} \cline{3-17} \cline{4-17} \cline{5-17} \cline{6-17} \cline{7-17} \cline{8-17} \cline{9-17} \cline{10-17} \cline{11-17} \cline{12-17} \cline{13-17} \cline{14-17} \cline{15-17} \cline{16-17} \cline{17-17} 
 & {\scriptsize{}Mean} & {\scriptsize{}Fam.} & {\scriptsize{}Fra.} & {\scriptsize{}Hor.} & {\scriptsize{}Lig.} & {\scriptsize{}M60} & {\scriptsize{}Pan.} & {\scriptsize{}Pla.} & {\scriptsize{}Tra.} & {\scriptsize{}Mean} & {\scriptsize{}Aud.} & {\scriptsize{}Bal.} & {\scriptsize{}Cou.} & {\scriptsize{}Mus.} & {\scriptsize{}Pal.} & {\scriptsize{}Tem.}\tabularnewline
\hline 
{\scriptsize{}COLMAP} & {\scriptsize{}42.14} & {\scriptsize{}50.41} & {\scriptsize{}22.25} & {\scriptsize{}26.63} & {\scriptsize{}56.43} & {\scriptsize{}44.83} & {\scriptsize{}46.97} & {\scriptsize{}48.53} & {\scriptsize{}42.04} & {\scriptsize{}27.24} & {\scriptsize{}16.02} & {\scriptsize{}25.23} & {\scriptsize{}34.70} & {\scriptsize{}41.51} & {\scriptsize{}18.05} & {\scriptsize{}27.94}\tabularnewline
{\scriptsize{}CasMVSNet} & {\scriptsize{}56.84} & {\scriptsize{}76.37} & {\scriptsize{}58.45} & {\scriptsize{}46.26} & {\scriptsize{}55.81} & {\scriptsize{}56.11} & {\scriptsize{}54.06} & {\scriptsize{}58.18} & {\scriptsize{}49.51} & {\scriptsize{}31.12} & {\scriptsize{}19.81} & {\scriptsize{}38.46} & {\scriptsize{}29.10} & {\scriptsize{}43.87} & {\scriptsize{}27.36} & {\scriptsize{}28.11}\tabularnewline
{\scriptsize{}CDS-MVSNet} & {\scriptsize{}61.58} & {\scriptsize{}78.85} & {\scriptsize{}63.17} & {\scriptsize{}53.04} & {\scriptsize{}61.34} & {\scriptsize{}62.63} & {\scriptsize{}59.06} & {\scriptsize{}62.28} & {\scriptsize{}52.30} & {\scriptsize{}--} & {\scriptsize{}--} & {\scriptsize{}--} & {\scriptsize{}--} & {\scriptsize{}--} & {\scriptsize{}--} & {\scriptsize{}--}\tabularnewline
{\scriptsize{}TransMVSNet} & {\scriptsize{}63.52} & {\scriptsize{}80.92} & {\scriptsize{}65.83} & {\scriptsize{}\underline{56.94}} & {\scriptsize{}62.54} & {\scriptsize{}63.06} & {\scriptsize{}60.00} & {\scriptsize{}60.20} & {\scriptsize{}\underline{58.67}} & {\scriptsize{}37.00} & {\scriptsize{}24.84} & {\scriptsize{}\underline{44.59}} & {\scriptsize{}34.77} & {\scriptsize{}46.49} & {\scriptsize{}\underline{34.69}} & {\scriptsize{}36.62}\tabularnewline
{\scriptsize{}UniMVSNet} & {\scriptsize{}\underline{64.36}} & {\scriptsize{}\underline{81.20}} & {\scriptsize{}66.43} & {\scriptsize{}53.11} & {\scriptsize{}\underline{63.46}} & \textbf{\scriptsize{}66.09} & \textbf{\scriptsize{}64.84} & \textbf{\scriptsize{}62.23} & {\scriptsize{}57.53} & {\scriptsize{}\underline{38.96}} & {\scriptsize{}\underline{28.33}} & {\scriptsize{}44.36} & \textbf{\scriptsize{}39.74} & \textbf{\scriptsize{}52.89} & {\scriptsize{}33.80} & {\scriptsize{}34.63}\tabularnewline
{\scriptsize{}GBiNet} & {\scriptsize{}61.42} & {\scriptsize{}79.77 } & {\scriptsize{}\underline{67.69}} & {\scriptsize{}51.81} & {\scriptsize{}61.25} & {\scriptsize{}60.37} & {\scriptsize{}55.87} & {\scriptsize{}60.67} & {\scriptsize{}53.89} & {\scriptsize{}37.32} & \textbf{\scriptsize{}29.77} & {\scriptsize{}42.12} & {\scriptsize{}36.30} & {\scriptsize{}47.69} & {\scriptsize{}31.11} & {\scriptsize{}\underline{36.93}}\tabularnewline
{\textbf{\scriptsize{}MVSFormer}} & \textbf{\scriptsize{}66.37} & \textbf{\scriptsize{}82.06} & \textbf{\scriptsize{}69.34} & \textbf{\scriptsize{}60.49} & \textbf{\scriptsize{}68.61} & {\scriptsize{}\underline{65.67}} & {\scriptsize{}\underline{64.08}} & {\scriptsize{}\underline{61.23}} & \textbf{\scriptsize{}59.53} & \textbf{\scriptsize{}40.87} & {\scriptsize{}28.22} & \textbf{\scriptsize{}46.75} & {\scriptsize{}\underline{39.30}} & {\scriptsize{}\underline{52.88}} & \textbf{\scriptsize{}35.16} & \textbf{\scriptsize{}42.95}\tabularnewline
\hline 
\end{tabular}{\scriptsize\par}}
\end{table}

\noindent\textbf{ETH3D.}
To show the robustness of the proposed method in scene data, we additionally evaluate MVSFormer on both training and test set of the high-resolution ETH3D~\citep{schops2017multi} without re-training. 
ETH3D contains 13 training and 12 test scenes, which include both indoor and outdoor challenging scenes. We evaluate the ETH3D with the model trained on DTU and finetuned on BlendedMVS. Input images are resized into 10881920 as Tanks-and-Temples and the view number is 7. Other settings are the same as~\citet{wang2022itermvs}. MVSFormer is quantitatively compared with both traditional methods~\citep{Galliani_2015_ICCV,schonberger2016pixelwise} and other state-of-the-art learning-based ones~\citep{wang2021patchmatchnet,wang2022itermvs,wang2022mvster} in Tab.~\ref{tab:eth3d}. MVSFormer achieves the best F1-score in both the training and test set of ETH3D, which shows good robustness and generalization of our method on scene datasets. 

\begin{table}[h]
\small
\centering
\caption{Comparisons of Accuracy (Acc.), Completeness (Cop.) and F1-score (F1) on the ETH3D benchmark.}\label{tab:eth3d}
\centering
\begin{tabular}{c|ccc|ccc}
\hline 
\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Training set} & \multicolumn{3}{c}{Test set}\tabularnewline
\cline{2-7} \cline{3-7} \cline{4-7} \cline{5-7} \cline{6-7} \cline{7-7} 
 & Acc. & Cop. & F1 & Acc. & Cop. & F1\tabularnewline
\hline 
Gipuma~\citep{Galliani_2015_ICCV} & 84.44 & 34.91 & 36.38 & 86.47 & 24.91 & 45.18\tabularnewline
PMVS~\citep{furukawa2009accurate} & 90.23 & 32.08 & 46.06 & 90.08 & 31.84 & 44.16\tabularnewline
COLMAP~\citep{schonberger2016pixelwise} & \textbf{91.85} & 55.13 & 67.66 & \textbf{91.97} & 62.98 & 73.01\tabularnewline
ACMH~\citep{xu2019multi} & 88.94 & 61.59 & 70.71 & 89.34 & 68.62 & 75.89\tabularnewline
\hline 
PatchMatchNet~\citep{wang2021patchmatchnet} & 64.81 & 65.43 & 64.21 & 69.71 & 77.46 & 73.12\tabularnewline
PatchMatch-RL~\citep{lee2021patchmatch} & 76.05 & 62.22 & 67.78 & 74.48 & 72.89 & 72.38\tabularnewline
CDS-MVSNet~\citep{giang2021curvature} & 73.07 & 64.22 & 67.65 & 77.32 & 81.43 & 79.07\tabularnewline
MVSTER~\citep{wang2022mvster} & 76.92 & 68.08 & 72.06 & 77.09 & 82.47 & 79.01\tabularnewline
IterMVS~\citep{wang2022itermvs} & 79.79 & 66.08 & 71.69 & 84.73 & 76.49 & 80.06\tabularnewline
\hline
MVSFormer (ours) & 73.62 & \textbf{74.64} & \textbf{73.44} & 82.23  &\textbf{83.75}  & \textbf{82.85} \tabularnewline
\hline 
\end{tabular}
\end{table}


\subsection{Ablation studies}
\label{sec:aba}

\begin{table} \small
\vspace{-0.1in}
\caption{Ablations in DTU on different pre-trained models, augmentation strategies, and loss types. Metrics are depth error ratios of 2mm (), 4mm (), 8mm () and  Overall error (Ovl.) of point clouds. Red line denotes our \textcolor{red!75}{baseline} results; green and blue rows refer to our \textcolor{green!99}{MVSFormer-P} and \textcolor{blue!75}{MVSFormer} respectively.
\label{tab:ablations}}
\small
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
\hline 
\multirow{2}{*}{Pre-trained} & \multicolumn{3}{c|}{Augmentation} & \multicolumn{2}{c|}{Loss} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{Ovl.}\tabularnewline
\cline{2-6} \cline{3-6} \cline{4-6} \cline{5-6} \cline{6-6} 
 & {\scriptsize{}Cropping} & {\scriptsize{}HR-FT} & {\scriptsize{}Multi-scale} & {\scriptsize{}REG} & {\scriptsize{}CLA} &  &  &  & \tabularnewline
\hline 
\rowcolor{red!25}
-- &  &  &  &  &  & 22.81 & 18.12 & 14.85 & 0.321\tabularnewline
ResNet50 &  &  &  &  &  & 20.09 & 15.11 & 11.78 & 0.323\tabularnewline
ResNet50 &  &  &  &  &  & 20.38 & 13.87 & 10.37 & 0.312\tabularnewline
DINO-small &  &  &  &  &  & 22.06 & 16.63 & 12.78 & 0.309\tabularnewline
DINO-small &  &  &  &  &  & \textbf{17.06} & \textbf{11.60} & \textbf{7.97} & 0.301\tabularnewline
DINO-small &  &  &  &  &  & 20.98 & 15.13 & 10.73 & 0.309\tabularnewline
\rowcolor{green!25}
DINO-small &  &  &  &  &  & 17.18 & 11.96 & 8.53 & 0.296\tabularnewline
MAE-base &  &  &  &  &  & 18.45 & 13.07 & 9.36 & 0.307\tabularnewline
Twins-small &  &  &  &  &  & 22.62 & 17.51 & 13.75 & 0.312\tabularnewline
Twins-small &  &  &  &  &  & 17.70 & 12.47 & 8.92 & 0.293\tabularnewline
\rowcolor{blue!25}
Twins-small &  &  &  &  &  & 17.50 & 12.48 & 9.14 & \textbf{0.289}\tabularnewline
\hline 
\end{tabular}
\vspace{-0.1in}
\end{table}

\begin{figure}
\begin{centering}
\includegraphics[width=0.9\linewidth]{pics/dtu_qua1.pdf} 
\par\end{centering}
\caption{Qualitative results compared with our baseline worked with different pre-trained models.}
\label{fig:dtu_qua1}
\vspace{-0.1in}
\end{figure}

\noindent\textbf{Different pre-trained models.}
We have tested different pre-trained models for MVS in Tab.~\ref{tab:ablations}, which include ResNet50~\citep{he2016identity}, DINO~\citep{caron2021emerging}, MAE~\citep{he2021masked}, and Twins~\citep{chu2021twins}. Note that both ResNet50 and Twins are trainable, while DINO and MAE are frozen. Our baseline method is a 4-stage cascaded MVS model with visibility modules~\citep{zhang2020visibility} and the random cropping~\citep{mi2021generalized}. From Tab.~\ref{tab:ablations}, ResNet50 in row2 improves the depth but fails to reduce the overall error of point clouds. Because CNN-based pre-training can not learn proper features from reflection and texture-less areas, which causes discouraged metrics for these scans and leads to even worse results in point clouds as shown in Fig.~\ref{fig:dtu_qua1}. Our MVSFormers (row4 and row9 of Tab.~\ref{tab:ablations}) can improve the baseline even without the multi-scale training.
For the comparisons among frozen pre-trained ViTs with the multi-scale training strategy,
DINO-small achieves better performance with fewer parameters compared with MAE-base, which is benefited by the multi-crop~\citep{caron2020unsupervised} and the proposed attention based GLU. Although REG based methods enjoy slightly better depth, they fail to reconstruct proper point clouds with proper confidence.
The Twins and DINO based MVSFormers achieve the best results in point cloud and depth respectively, while the pre-trained ResNet50 based model with all other tricks (row3 of Tab.~\ref{tab:ablations}) fails to produce competitive results. Detailed comparison among MVSFormers and ResNet are discussed in Sec.~\ref{sec:appendix_resnet50_discussion}. The computation and GPU memory cost of different pre-trained MVS methods are discussed in Sec.~\ref{sec:appendix_efficiency}.

\noindent\textbf{Multi-scale training.}
From Tab.~\ref{tab:ablations}, both Twins and DINO based MVSFormers achieve considerable improvements from the multi-scale training. Especially, the trainable Twins enjoys much more benefits (0.19 in the overall error).
Because of the spatial invariance, CNN-based ResNet50 basically can not be benefited from  multi-scale training (row3).
Besides, we also compare  multi-scale strategy with high-resolution finetuning (HR-FT), \emph{i.e.}, finetuning the model with fixed 10241280 images for another 5 epochs. But HR-FT produce inferior results compared with the multi-scale one. 
We think that finetuning on a specific resolution still suffers from the spatial overfitting to attention blocks, which limits the generalization of ViTs for various image scales.

\begin{table} 
\caption{Left table: Ablation study about pre-training of Twins in MVSFormer. Right table: The ablation study of DINO attention map and GLU in MVSFormer-P. `MS' means the multi-scale training. \textcolor{blue}{Blue} and \textcolor{green}{green} rows show the performance of the full model of \textcolor{blue}{MVSFormer} and \textcolor{green}{MVSFormer-P} respectively.}\label{tab:pretrain_and_glu}
\vspace{-0.1in}
\begin{centering}
\begin{tabular}{cc}
\begin{tabular}{c}
{\small{}}\setlength{\tabcolsep}{1.0mm}{
\begin{tabular}{ccccc}
\toprule 
{\small{}Pre-trained} & {\small{}} & {\small{}} & {\small{}} & {\small{}Ovl.}\tabularnewline
\midrule 
{\small{}} & {\small{}20.16} & {\small{}14.91} & {\small{}11.02} & {\small{}0.300}\tabularnewline
\rowcolor{blue!25}{\small{}} & \textbf{\small{}17.50} & \textbf{\small{}12.48} & \textbf{\small{}9.14} & \textbf{\small{}0.289}\tabularnewline
\bottomrule
\end{tabular}}\tabularnewline
\end{tabular} & \begin{tabular}{c}
{\small{}}\setlength{\tabcolsep}{1.0mm}{
\begin{tabular}{ccccccc}
\toprule 
{\small{}GLU} & {\small{}Concat+2Conv} & MS+CLA & {\small{}} & {\small{}} & {\small{}} & {\small{}Ovl.}\tabularnewline
\midrule 
{\small{}--} & {\small{}--} &  & {\small{}24.12} & {\small{}18.29} & {\small{}13.66} & {\small{}0.312}\tabularnewline
 & {\small{}} &  & {\small{}22.78} & {\small{}17.25} & {\small{}13.30} & {\small{}0.310}\tabularnewline
{\small{}} &  &  & {\small{}22.06} & {\small{}16.63} & {\small{}12.78} & {\small{}0.309}\tabularnewline
\rowcolor{green!25}{\small{}} &  & {\small{}} & \textbf{\small{}17.18} & \textbf{\small{}11.96} & \textbf{\small{}8.53} & \textbf{\small{}0.296}\tabularnewline
\bottomrule
\end{tabular}}\tabularnewline
\end{tabular}\tabularnewline
\end{tabular}
\par\end{centering}
\end{table}
\vspace{-0.1in}

\noindent\textbf{The effect of pre-training for ViTs.} 
Although training a transformer (with CNN) for MVS is feasible~\citep{zhu2021multi}, we think that the pre-training is still important for MVS learning, especially for the feature learning in our work. In particular, we train our MVSFormer with Twins-small from the scratch as shown in Tab.~\ref{tab:pretrain_and_glu}(left). We increase the learning rate of no pre-trained Twins-small from 3e-5 to 1e-4, while all other settings are unchanged. Results from Tab.~\ref{tab:pretrain_and_glu}(left) show that pre-training is critical to our proposed MVSFormer. The Twins-small without pre-training is not as good as pre-trained one.
Without the pre-training, our methods are close to those attention-based MVS methods~\citep{ding2021transmvsnet,zhu2021multi} with only intra-view attention except the temperature based depth. Actually, no pre-trained MVSFormer performs similarly as TransMVSNet~\citep{ding2021transmvsnet}. So the pre-training is important for ViTs to model proper feature representations to tackle the essential feature matching problem in MVS.



\noindent\textbf{Effects of DINO attention maps from [\emph{CLS}] and GLU.}
In Tab.~\ref{tab:pretrain_and_glu}(right), we evaluate the performance of MVSFormer-P with and without the [\emph{CLS}] attention map. Furthermore, we compare GLU based attention map fusion with the simple concatenation and  convolutions to balance the parameters. 
From Tab.~\ref{tab:pretrain_and_glu}(right), both attention fusion strategies enjoy improvements compared with the baseline without such attention map. And the GLU block can achieve better depth with the same computation.



\noindent\textbf{REG vs CLA in MVS.} 
To show the importance of effect confidence map from CLA models during  point cloud reconstruction.
In Fig.~\ref{fig:tnt_cla_reg}, we further show the difference between the REG and CLA on the real-world Tanks-and-Temples reconstruction.
Note that CLA can produce more reliable confidence maps to distinguish certain foregrounds and uncertain backgrounds (\emph{e.g.}, sky). Thus CLA based methods reconstruct more valid points with much less outliers, which is crucial for the real-world MVS practice. More real-world cases are shown in Fig.~\ref{fig:real-world} of Appendix.
 
\begin{figure}
\begin{centering}
\includegraphics[width=0.9\linewidth]{pics/CLA-REG.pdf} 
\par\end{centering}
\caption{`Francis' and `Horse' in Tanks-and-Temples compared between REG and CLA based MVSFormer. The number of total points are listed in the brackets of (b).
}
\label{fig:tnt_cla_reg}
\vspace{-0.1in}
\end{figure}


\begin{table}  
\centering  
\caption{Depth (, , ) and point cloud  ablations of the Twins based MVSFormer trained with REG and CLA, while the CLA model is further inferenced with different temperatures . \label{tab:ablation_cls_t}}
\vspace{-0.1in}
\begin{tabular}{c|c|c|ccc|ccc}
\hline 
{\small{}REG} & {\small{}CLA} &  & {\small{}} & {\small{} } & {\small{}} & {\small{}Accuracy} & {\small{}Completeness} & {\small{}Overall}\tabularnewline
\hline 
{\small{}} &  & {\small{}--} & {\small{}17.70} & {\small{}12.47} & \textbf{\small{}8.92} & {\small{}0.336} & {\small{}0.249} & {\small{}0.293}\tabularnewline
\hline 
 & {\small{}} & {\small{}} & {\small{}18.03} & \textbf{\small{}12.32} & {\small{}8.94} & {\small{}0.349} & {\small{}0.248} & {\small{}0.298}\tabularnewline
 & {\small{}} & {\small{}} & {\small{}17.62} & {\small{}12.48} & {\small{}9.18} & {\small{}0.342} & \textbf{\small{}0.247} & {\small{}0.295}\tabularnewline
 & {\small{}} & {\small{}} & {\small{}17.78} & {\small{}12.72} & {\small{}9.37} & {\small{}0.336} & {\small{}0.250} & {\small{}0.293}\tabularnewline
 & {\small{}} & {\small{}} & {\small{}19.06} & {\small{}13.97} & {\small{}10.37} & {\small{}0.323} & {\small{}0.258} & {\small{}0.291}\tabularnewline
 & {\small{}} & {\small{}} & {\small{}19.99} & {\small{}14.88} & {\small{}11.11} & \textbf{\small{}0.321} & {\small{}0.270} & {\small{}0.296}\tabularnewline
 & {\small{}} &  & \textbf{\small{}17.50} & {\small{}12.48} & {\small{}9.14} & {\small{}0.327} & {\small{}0.251} & \textbf{\small{}0.289}\tabularnewline
\hline 
\end{tabular}
\vspace{-0.1in}
\end{table}

\noindent\textbf{Temperature-based depth prediction.}
Quantitative ablations about REG and CLA of MVSFormer are shown in Tab.~\ref{tab:ablation_cls_t}. The vanilla CLA () based model can not achieve better results compared with the one trained with REG. As reducing the temperature , depth is smoothed and models tend to reconstruct more accurate point clouds. Because of the decrease in accuracy distance and the increase in completeness distance. Since the trade-off of the accuracy and completeness,  achieves a worse overall result. Simply reducing  for early stages causes over-smoothing results, which harms the completeness of point clouds.
Our  setting can get a good trade-off in both depth and point clouds, which is better than any consistent ; it also enjoys better accuracy compared with REG, which indicates that CLA provides better confidence maps to filter outliers. Therefore, the idea of making early stages work as CLA and latter stages work as REG is reasonable. 
More qualitative (Sec.~\ref{sec:appendix_quali_temp}) and quantitative (Sec.~\ref{sec:appendix_analysis_of_depth}) analysis about the temperature are discussed in the Appendix.



\section{Conclusion}
In this paper, we discuss the influence of pre-trained models on MVS learning, and propose a ViT enhanced MVS architecture called MVSFormer. MVSFormer achieves prominent improvements with a better feature encoding enhanced by pre-trained hierarchical-ViT Twins.
Furthermore, we propose an alternative MVSFormer-P with the plain-ViT DINO, which can also achieve competitive results with a frozen backbone.
The efficient multi-scale training is used to generalize MVSFormer to various resolutions. Besides, an effective temperature-based depth prediction is proposed to unify both REG and CLA in the MVS learning, which produces smooth depth maps and clear point clouds without outliers. Our method can achieve state-of-the-art results in DTU, and rank top-1 on Tanks-and-Temples.





\subsubsection*{Acknowledgments}
This work was supported by NSFC under Grant (No.62076067).

\bibliography{main}
\bibliographystyle{tmlr}

\appendix


\section{More Implementation Details and Discussions}
\label{sec:appendix_impl}

\subsection{Multi-scale Training} 
\label{sec:appendix_multiscale_impl}
The PyTorch pseudo-code of the multi-scale training is summarized in Alg.~\ref{alg:multi-scale}.
The training scales are randomly selected from 25 patterns, whose height is ranged from 512 to 1024, while width is ranged from 640 to 1280. We also randomly crop images from [0.83,1.0] of the original scale. The global batch size is 8, while the relation of sub-batch size and resolution is shown in Tab.~\ref{tab:multi-scale}. Thanks for the mixed-precision, it only takes about 22 and 15 hours for our proposed MVSFormer to be trained with 10 epochs in DTU~\citep{aanaes2016large} and BlendedMVS~\citep{yao2020blendedmvs} respectively with two V100 32GB NVIDIA Tesla GPUs. 

\begin{algorithm}
  \caption{PyTorch pseudo code for efficient multi-scale training with gradient accumulation}
  
  \label{alg:multi-scale}
    \definecolor{codeblue}{rgb}{0.25,0.5,0.5}
    \lstset{
      basicstyle=\fontsize{7.5pt}{7.5pt}\ttfamily\bfseries,
      commentstyle=\fontsize{7.5pt}{7.5pt}\color{codeblue},
      keywordstyle=\fontsize{7.5pt}{7.5pt},
    }
\vspace{-0.1in}
\begin{lstlisting}[language=python]
# B: maximal batch size
# scale_batch_dict: a mapping dict, key is resolution; value is related sub-batch size
# optimizer: Adam optimizer
for (x, y) in data_loader: # load a batch consisted of inputs x and reference depth y 
    b = scale_batch_dict[shape(x)] # get related sub-batch size from the dict
    n = B // b # get the step number n for accumulation 
    for i in range(n): # accumulate gradients for n steps
        y_pred = MVSFormer(x[i*b:(i+1)*b]) # model inference with a sub-batch
        loss = LossFunction(y_pred, y[i*b:(i+1)*b]) # calculate loss for sub-batch
        loss.backward() # back-propagate and accumulate gradients
    optimizer.step() # optimize model parameters
    optimizer.zero_grad() # clear accumulated gradients
\end{lstlisting}
\vspace{-0.1in}
\end{algorithm}

\begin{table}[h]
\small
\centering
\caption{The relation of sub-batch size and resolution in the multi-scale training of MVSFormer-P and MVSFormer, which can be trained in two 32GB GPUs with maximum batch size 8.\label{tab:multi-scale}}
\begin{tabular}{c|c|c}
\hline
\multirow{2}{*}{Resolution} & \multicolumn{2}{c}{sub-batch size}\tabularnewline
\cline{2-3} \cline{3-3} 
 & MVSFormer-P & MVSFormer\tabularnewline
\hline 
512x640\textasciitilde 768 & 8 & 8\tabularnewline
576x704\textasciitilde 832 & 8 & 8\tabularnewline
640x832\textasciitilde 960 & 8 & 8\tabularnewline
704x896\textasciitilde 1024 & 8 & 4\tabularnewline
768x960\textasciitilde 1088 & 4 & 4\tabularnewline
896x1152\textasciitilde 1280 & 4 & 4\tabularnewline
960x1216\textasciitilde 1344 & 4 & 2\tabularnewline
1024x1280 & 4 & 2\tabularnewline
\hline
\end{tabular}
\end{table}

\subsection{Data Augmentation} 
We augment input images for ViT in MVSFormer with the same parameter for all reference and source views, which can slightly improve the depth performance as shown in Tab.~\ref{tab:color_aug}.
Specifically, the data augmentation is performed by random gamma, color jitters with brightness, contrast, saturation, and hue. The augmented parameters are sampled from uniform distributions in the ranges [0.9,1.1] for gamma, contrast and saturation, [0.8,1.2] for brightness, [0.95,1.05] for hue.

\begin{table}[h]
\small
\centering
\caption{Depth metrics on DTU () with error ratios of 2mm (), 4mm (), 8mm (), and 14mm () with/without data augmentation.\label{tab:color_aug}}
\begin{tabular}{ccccc}
\toprule 
 &  &  &  & \tabularnewline
\midrule
w./o. aug & 17.15 & 10.85 & 7.29 & 5.38\tabularnewline
with aug & \textbf{16.52} & \textbf{10.46} & \textbf{7.07} & \textbf{5.22}\tabularnewline
\bottomrule
\end{tabular}
\end{table}

\subsection{Post-processing}
For DTU, we get the final point clouds with the depth fusion tool from Gipuma~\citep{Galliani_2015_ICCV} with consistent hyper-parameters, \emph{i.e.}, disparity threshold 0.1, number consistent 2, and probability threshold 0.5. 
For Tanks-and-Temples, to avoid adjusting hyper-parameters for each case, we follow the dynamic consistency checking proposed in \citet{yan2020dense}.

\subsection{Confidence Maps}
\label{sec:appendix_conf}
We achieve confidence maps for REGs and CLAs of each stage as

where  and  are predicted depth from Eq.~\ref{eq:reg_and_cla};  is the probability after the softmax along depth hypotheses;  indicates the kernel size for the depth pooling.
For CLAs we simply use the max probability of all depth hypotheses as the confidence map. REGs follow~\citet{yao2018mvsnet} that averages the probability along the depth before gathering the confidence. For the cascaded depth hypotheses, we set the pooling kernel size  for 4 stages as \{4,3,2,1\} to 32-16-8-4 hypotheses instead of fixing in 4 of~\citet{yao2018mvsnet} for slightly better reconstruction results.
Then confidence maps of all stages are averaged for the final confidence output after the nearest resizing to the original image size.

\subsection{Additional Discussions}

\noindent\textbf{Why completely separate the FPN and ViT in MVSFormer?}
The FPN used in MVS can efficiently integrate the information from different input scales. And FPN is not redundant to our ViT in MVSFormer for three reasons.
1) We use 1/2 input scale for ViT, while FPN takes full resolution inputs. So FPN can learn more detailed information. In Sec.~\ref{sec:appendix_vit_and_fpn}, we have discussed the performance of ViTs without FPNs and pre-trained ResNet. ViTs can work complementarily with FPNs for both global understanding and local details in MVS.
2) Since the inputs to ViT are downsampled to 1/2 to balance the computation, the valid largest feature scale of Hierarchical ViT is 1/8, while the one of Plain ViT is smaller (1/32). Our feature scales of FPN are 1, 1/2, 1/4, 1/8 respectively. So only one feature layer (1/8) can be completely replaced with the ViT feature. For the reason of the model integrity, we remain all layers of the FPN.
3) Because the inputs to FPN are 2x larger than ones to ViT, the highest level feature (1/8) of FPN also contains informative features learned from high-resolution inputs which are not included in ViTs.


\noindent\textbf{Why using DINO attention maps from [\emph{CLS}] rather than other tokens?}
We use the attention map attached from the [\emph{CLS}] token for two reasons. 
1) DINO~\citep{caron2021emerging} is pre-trained with self-distillation. The [\emph{CLS}] token is directly supervised during the pre-training. So the [\emph{CLS}] token needs to understand the global semantics in the image. Thus the attention map of the [\emph{CLS}] token is more significant to decide the final prediction. 
2) Moreover, the attention map from all tokens () is much larger than ones from [\emph{CLS}] token (). Thus choosing or getting a reasonable attention map from  relations is also a difficult issue.

\noindent\textbf{Relation of CLA/REG confidence and aleatoric/epistemic uncertainty.}
As mentioned in~\citet{kendall2017uncertainties}, aleatoric uncertainty captures noise inherent in the observations, while epistemic uncertainty indicates the uncertainty in the model parameters.
In terms of the definitions from~\citet{kendall2017uncertainties}, both CLA and REG should be categorized as aleatoric uncertainty rather than epistemic uncertainty. Because we have not tested these methods with model uncertainty (\emph{e.g.}, dropout inference). More importantly, both CLA and REG only show the uncertainty of noise in the training distribution, rather than the Out-of-data examples. For example, most learning based MVS methods (both CLA and REG) demand a certain depth range before the prediction; and they cannot provide proper results with wrong depth ranges. However, given a certain depth range, CLA based cascade MVS enjoys much more reasonable confidence as discussed in Sec.~\ref{sec:loss}. Moreover, the uncertainty estimation of MVS is an interesting issue as the future work.


\section{Inference Memory and Time Costs}
\label{sec:appendix_efficiency}

We test the inference memory and time costs with input resolution  in Tab.~\ref{tab:param_cost} compared with CNN-based pre-training, pure FPN, and other MVS methods. All comparisons are based on a V100 NVIDIA Tesla GPU. From Tab.~\ref{tab:param_cost}, although the parameter scale is increased, ViT enhanced MVSFormers only cost a little more GPU memory and time compared with the baseline method. Except for parameters contained by ViT itself, most other trainable parameters are worked for the dimension reduction in MVSFormer. MVSFormer can infer faster compared with MVSFormer-P, which is benefited from the efficient multi-scale attention designed in Twins~\citep{chu2021twins}.
Note that the pre-trained CNN model --ResNet50 also costs a lot in GPU memory and inference time.
For other MVS algorithms, they are all forwarded with 1 view at once. CasMVSNet~\citep{gu2020cascade} costs most memory without the group-wise pooling~\citep{guo2019group} for the cost volume. The sophisticated TransMVSNet~\citep{ding2021transmvsnet} suffers from slow inference speed and large memory costs. CDS-MVSNet~\citep{giang2021curvature} is slightly more efficient than MVSFormer, but it is discouraged in its performance.

\begin{table}[h]
\small
\centering
\caption{Illustration of model parameters (Params.) and memory/time-cost during the inference phase of  images. Results in brackets are forwarded with 5 views at once, while ones outside are forwarded 1 view at once. We also provide the efficiency of other MVS methods with their official codes (1-view).}\label{tab:param_cost}
\begin{tabular}{ccccc}
\toprule 
 & Memory (MB) & Time (s/img) & Params. (all) & Params. (trainable)\tabularnewline
\midrule 
Baseline & 4262(8997) & 0.4125(0.4055) & 1.35M & 1.35M\tabularnewline
Baseline+ResNet50 & 4882(9523) & 0.4920(0.4704) & 37.27M & 37.27M\tabularnewline
MVSFormer-P & 4842(9251) & 0.5530(0.5230) & 26.45M & 4.79M\tabularnewline
MVSFormer & 4970(9431) & 0.4831(0.4398) & 28.01M & 28.01M\tabularnewline
\midrule 
CasMVSNet & 6672 & 0.4747 & 0.93M & 0.93M \tabularnewline
CDS-MVSNet & 4760 & 0.4006 & 0.98M & 0.98M\tabularnewline
TransMVSNet & 6320 & 1.4975 & 1.15M & 1.15M\tabularnewline
\bottomrule
\end{tabular}
\end{table}







\section{More Experiment Results}

\subsection{Pre-trained ViTs without FPNs}
\label{sec:appendix_vit_and_fpn}
To explore the learning ability of ViTs, some quantitative results about ViTs (DINO-small~\citep{caron2021emerging}, MAE-base~\citep{he2021masked}, Twins-small~\citep{chu2021twins}) trained without FPNs for MVS are shown in Tab.~\ref{tab:CNN_compare_VIT}, which are also compared with CNN-based pre-trained ResNet34 and ResNet50 and the vanilla FPN. Note that these experiments are only based on low-resolution cases in DTU of , because we want to save computation with all trainable ViT weights. The learning rates of MAE and DINO are 1e-5; the learning rate of Twins is 3e-5, and the ones of all CNNs are 1e-3. 
From Tab.~\ref{tab:CNN_compare_VIT}, ViTs cannot achieve as good details as CNNs (2mm, 4mm), but results from ViTs are more robust in large depth error metrics (8mm, 14mm). Therefore, we think that ViTs can tackle some serious mistakes caused by reflection and texture-less areas as mentioned in the main paper. 
Interestingly, FPN trained from scratch achieves better depth results in low-resolution cases compared with other pre-trained CNNs, which demonstrates the dilemma of using pre-trained CNNs in MVS again. Twins-small can get better depth than FPN except for the 2mm error benefited by its pyramid architecture.
So ViTs can work complementarily with CNN-based FPNs for both global understanding and local details in MVS. 

 \begin{table}[h]
 \caption{Depth metrics for DTU () compared with FPN, pre-trained CNNs and VITs~\citep{he2021masked,caron2021emerging,chu2021twins}. Note that FPN is trained from scratch without pre-training.
 \label{tab:CNN_compare_VIT}}
 \small
 \centering
\begin{tabular}{c|c|cccccc}
\toprule 
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & FPN & ResNet34 & ResNet50 & MAE-base & DINO-small & Twins-small\tabularnewline
\midrule 
\multirow{5}{*}{\begin{turn}{90}
Error(\%)
\end{turn}} & 2mm & \textbf{19.53} & 20.65 & 20.55 & 21.85 & 26.66 & 19.65\tabularnewline
 & 4mm & 11.85 & 12.43 & 12.32 & 12.42 & 14.42 & \textbf{11.53}\tabularnewline
 & 8mm & 8.25 & 8.55 & 8.45 & 8.17 & 8.87 & \textbf{7.92}\tabularnewline
 & 14mm & 6.48 & 6.75 & 6.60 & 6.33 & 6.60 & \textbf{6.24}\tabularnewline
 & mean & 11.52 & 12.09 & 11.98 & 12.19 & 14.14 & \textbf{11.34}\tabularnewline
\bottomrule 
\end{tabular}
 \end{table}

\subsection{Different Feature Fusion Strategies of MVSFormer}
\label{sec:appendix_fusion_ways}
We pay more attention to the essential improvements gained from pre-trained ViTs in this paper. Thus we tend to use simple feature fusion strategies in MVSFormer. Both the Direct Feature Addition (DFA) (used in the main paper) and the Multi-scale Feature Addition (MFA) is considered in Tab.~\ref{tab:feature_fusion}. For the multi-scale addition, we use extra convolution blocks to further upsample ViT features to 1/4 and 1/2, and add them to related feature maps in FPN. Since inputs to ViTs are halved, we do not try to upsample ViT features to 1/1. From Tab.~\ref{tab:feature_fusion}, MFA can achieve slightly better depth predictions but worse point cloud metrics. We think that ViT features are not suitable for high-resolution features, and adopt DFA as our solution.

\begin{table}[h]
 \caption{Ablations about different feature fusion strategies of Direct Feature Addition (DFA) and Multi-scale Feature Addition (MFA) in MVSFormer.
 \label{tab:feature_fusion}}
 \small
 \centering
\begin{tabular}{ccccccc}
\toprule 
 &  &  &  & Acc. & Cop. & Ovl.\tabularnewline
\midrule
DFA & \textbf{17.50} & 12.48 & 9.14 & \textbf{0.327} & \textbf{0.251} & \textbf{0.289}\tabularnewline
MFA & 17.53 & \textbf{12.24} & \textbf{8.62} & 0.329 & 0.253 & 0.291\tabularnewline
\bottomrule
\end{tabular}
\end{table}

\subsection{Qualitative Results of Temperature-based Depth Prediction}
\label{sec:appendix_quali_temp}

We show additional qualitative results of CLA with different temperature  in Fig.~\ref{fig:tmp_exps}.  tends to output depth maps with jagged boundaries, while depth maps with  and based on regression suffer from uncertain and ambiguous predictions.
Although the visual difference is not obvious between Fig.~\ref{fig:tmp_exps}(e) and Fig.~\ref{fig:tmp_exps}(f),
our setting of  can get more exact depth predictions compared with  or . 
Because if we carefully compare Fig.~\ref{fig:tmp_exps}(i) and Fig.~\ref{fig:tmp_exps}(j), our depth enjoys less error on the surface (<2mm). On the other hand, the `side effect' brought by our temperature setting is much less than the setting of . Thus, the idea of `CLA in the early stages, REG in the latter stages' is reasonable and leads to better depth details and point cloud results as discussed in the main paper.

 \begin{figure}[h]
 \begin{centering}
 \includegraphics[width=0.95\linewidth]{pics/tmp_exps.pdf} 
 \par\end{centering}
\caption{Qualitative depth comparisons among REG, and CLA with different  settings. Our depth is based on . (g)-(j) indicate the depth error (mm).}
 \label{fig:tmp_exps}
 \vspace{-0.15in}
 \end{figure}

\subsection{Comparison of Different ViT Capacities}
\label{sec:appendix_vit_capacity}

We further evaluate the performance of MVSFormer-P and MVSFormer with larger ViT backbones (base model) in Tab.~\ref{tab:model_capacity}. Interestingly, DINO-base achieves worse performance compared with DINO-small. We think that smaller DINO~\citep{caron2021emerging} used in MVSFormer-P enjoys better generalization, because the DINO backbone is fixed in MVSFormer-P due to the costly plain-ViT design. On the other hand, Twins-base can achieve full depth improvements compared with the small one, but improvements of point clouds are negligible. Obviously, point cloud metrics are more difficult to be improved. But we can still expect for good performance from larger trainable pre-trained ViTs. Besides, both DINO-base and Twins-base make training be converged faster compared with small ones.

\begin{table}[h]
 \caption{Ablations about different capacities of MVSFormer-P and MVSFormer.
 \label{tab:model_capacity}}
 \small
 \centering
\begin{tabular}{ccccccc}
\toprule 
 &  &  &  & Acc. & Cop. & Ovl.\tabularnewline
\midrule
DINO-small & 17.18 & 11.96 & \textbf{8.53} & 0.327 & 0.265 & 0.296\tabularnewline
DINO-base & 17.41 & 12.22 & 8.67 & 0.334 & 0.268 & 0.301\tabularnewline
Twins-small & 17.50 & 12.48 & 9.14 & 0.327 & \textbf{0.251} & \textbf{0.289}\tabularnewline
Twins-base & \textbf{16.78} & \textbf{11.94} & 8.82 & \textbf{0.326} & 0.252 & \textbf{0.289}\tabularnewline
\bottomrule
\end{tabular}
\end{table}

\subsection{The Performance of Pre-trained ResNet50 with All Other Techniques}
\label{sec:appendix_resnet50_discussion}

To further ensure the effectiveness of pre-trained ViTs for MVS, we provide more results in Tab.~\ref{tab:resnet50_ablation} about pre-trained ResNet50 with all other techniques used in our MVSFormer including the multi-scale training.
Since CNNs enjoy good spatial invariance, the multi-scale training for ResNet50 is not as important as one for ViT based MVSFormers. The external experiment in Tab.~\ref{tab:resnet50_ablation} shows that multi-scale training can not improve ResNet50 a lot.

\begin{table}[h]
 \caption{Results of ResNet50 with all other proposed components. `T-CLA' indicates temperature based depth with CLA.
 \label{tab:resnet50_ablation}}
 \small
 \centering
\begin{tabular}{ccccccc}
\toprule 
Pre-trained & Multi-scale & T-CLA &  &  &  & Overall\tabularnewline
\midrule 
ResNet50 &  &  & 20.09 & 15.11 & 11.78 & 0.323\tabularnewline
ResNet50 &  &  & 20.38 & 13.87 & 10.37 & 0.312\tabularnewline
DINO-small &  &  & \textbf{17.18} & \textbf{11.96} & \textbf{8.53} & 0.296\tabularnewline
Twins-small &  &  & 17.50 & 12.48 & 9.14 & \textbf{0.289}\tabularnewline
\bottomrule
\end{tabular}
\end{table}

\subsection{More Detailed Quantitative Analysis of REG, CLA, and Temperature-based CLA}
\label{sec:appendix_analysis_of_depth}

\noindent\textbf{Quantitative analysis.}
Here we first re-list these important results in Tab.~\ref{tab:quant_reg_cla2} from Tab.~\ref{tab:ablation_cls_t} for a clear and concrete illustration.
The Accuracy (Acc) of point clouds indicates the mean value of reconstructed points whose closest distance to the ground-truth points. So ‘Acc’ can be seen as the precision of reconstructed points from the model. On the other hand, ‘Cop’ shows how complete the proposed method reconstructs. 
From Tab.~\ref{tab:quant_reg_cla2}, compared with REG, our temperature based depth from CLA can achieve better Acc and Ovl, with similar Cop, which means that our CLA enjoys better confidence maps that can filter outliers near the boundaries successfully with more precise point clouds. Note that vanilla CLA fails to get good Acc (0.349 vs 0.336) with similar Cop (0.248 vs 0.249). It means that the points reconstructed by inexact argmax from vanilla CLA are not precise (vanilla CLA does not produce more points because Cop are similar), which can be greatly solved by our temperature based depth.

\begin{table}[h]
 \caption{Quantitative point cloud results of REG, CLA, and our temperature-based CLA of MVSFormer.
 \label{tab:quant_reg_cla2}}
 \small
 \centering
\begin{tabular}{cccc}
\toprule 
 & Accuracy (Acc) mm & Completeness (Cop) mm & Overall (Ovl) mm\tabularnewline
\midrule
REG & 0.336 & 0.249 & 0.293\tabularnewline
vanilla CLA & 0.349 & \textbf{0.248} & 0.298\tabularnewline
ours & \textbf{0.327(-0.009)} & 0.251(+0.002) & \textbf{0.289(-0.004)}\tabularnewline
\bottomrule
\end{tabular}
\end{table}

\noindent\textbf{More ablations about temperatures.}
We provide more ablations for the temperature setting in Tab.~\ref{tab:appendix_ablation_t} for a deeper discussion about REG and CLA. In Tab.~\ref{tab:appendix_ablation_t}, the 1mm depth error is further compared, which shows that decreasing the temperature for the last stage can effectively get the exact depth with lower 1,2mm errors, while the training and inference disparity of the last stage is negligible. Moreover, the setting of  achieves the best depth metric and overall point cloud reconstruction, but the gap of accuracy and completeness is larger than . Thus slightly regressing in the early stages is still useful for the reconstruction with higher precision. Note that  outperforms  in most metrics except the completeness, but the improvement is not obvious. This phenomenon indicates that our critical idea of making early stages work as CLA and latter stages work as REG is validated; and adjusting the depth prediction for the last stage enjoys much more benefits and less cost compared with other stages.


\begin{table}[h]
 \caption{Quantitative comparisons for different temperature settings of MVSFormer.}
 \label{tab:appendix_ablation_t}
 \small
 \centering
\begin{tabular}{cccccccc}
\toprule 
 & {\small{}} & {\small{}} & {\small{}} & {\small{}} & {\small{}Accuracy} & {\small{}Completeness} & {\small{}Overall}\tabularnewline
\midrule 
{\small{}\{,,,\}} & {\small{}32.65} & {\small{}18.03} & {\small{}12.32} & {\small{}8.94} & {\small{}0.3488} & {\small{}0.2480} & {\small{}0.2984}\tabularnewline
{\small{}} & {\small{}31.75} & {\small{}18.73} & {\small{}13.34} & {\small{}9.83} & {\small{}0.3401} & {\small{}0.2511} & {\small{}0.2956}\tabularnewline
{\small{}} & {\small{}31.49} & {\small{}18.07} & {\small{}12.53} & {\small{}9.08} & {\small{}0.3475} & {\small{}0.2483} & {\small{}0.2979}\tabularnewline
{\small{}} & {\small{}32.39} & {\small{}17.99} & {\small{}12.37} & {\small{}8.98} & {\small{}0.3529} & {\small{}0.2546} & {\small{}0.3069}\tabularnewline
{\small{}} & {\small{}28.62} & {\small{}17.32} & {\small{}12.22} & {\small{}8.93} & {\small{}0.3465} & \textbf{\small{}0.2364} & {\small{}0.2915}\tabularnewline
{\small{}} & {\small{}27.87} & {\small{}17.26} & {\small{}12.22} & {\small{}8.94} & {\small{}0.3427} & {\small{}0.2376} & {\small{}0.2902}\tabularnewline
{\small{}} & {\small{}26.99} & {\small{}17.23} & {\small{}12.24} & {\small{}8.95} & \textbf{\small{}0.3265} & {\small{}0.2513} & {\small{}0.2889}\tabularnewline
{\small{}} & \textbf{\small{}26.67} & \textbf{\small{}17.18} & \textbf{\small{}12.19} & \textbf{\small{}8.91} & {\small{}0.3381} & {\small{}0.2390} & \textbf{\small{}0.2886}\tabularnewline
{\small{}} & {\small{}27.28} & {\small{}17.50} & {\small{}12.48} & {\small{}9.14} & {\small{}0.3270} & {\small{}0.2512} & {\small{}0.2891}\tabularnewline
\bottomrule 
\end{tabular}
\end{table}


\subsection{Ablations about More Source Views}
\label{sec:appendix_aba_of_view}

 \begin{figure}[h]
 \begin{centering}
 \includegraphics[width=0.95\linewidth]{pics/more_view.pdf} 
 \par\end{centering}
\caption{The view number ablation. All visualized depths are filtered by the confidence map .}
 \label{fig:more_view}
\vspace{-0.15in}
 \end{figure}
During the testing on Tanks-and-Temples~\citep{Knapitsch2017}, we find that camera poses from the advanced set are very challenging.
Therefore we try to expand source views with candidates of the selected source views which have not been included according to the view selection from \citet{yao2018mvsnet}.
We find that increasing the number of source views can effectively improve the performance in complex scenes of Tanks-and-Temples. Thanks to the visibility normalization~\citep{zhang2020visibility,giang2021curvature} used in MVSFormer, our method can be generalized to , and achieve better results on Tanks-and-Temples.
As shown in Fig.~\ref{fig:more_view}, depth maps predicted by 20-view inputs are more reliable than ones from 10-view. 
Quantitative results from Tab.~\ref{tab:quant_TnT} show that the state-of-the-art results on Tanks-and-Temples from our main paper can be further improved with more source views.


\begin{table}[!h]
\caption{Ablation studies on the number of total views (reference and source views).
\label{tab:quant_TnT}}
\centering
\setlength{\tabcolsep}{1.1mm}{
{\scriptsize{}}\begin{tabular}{c|ccccccccc|ccccccc}
\hline 
\multirow{2}{*}{} & \multicolumn{9}{c|}{{\scriptsize{}Intermediate}} & \multicolumn{7}{c}{{\scriptsize{}Advanced}}\tabularnewline
\cline{2-17} \cline{3-17} \cline{4-17} \cline{5-17} \cline{6-17} \cline{7-17} \cline{8-17} \cline{9-17} \cline{10-17} \cline{11-17} \cline{12-17} \cline{13-17} \cline{14-17} \cline{15-17} \cline{16-17} \cline{17-17} 
 & {\scriptsize{}Mean} & {\scriptsize{}Fam.} & {\scriptsize{}Fra.} & {\scriptsize{}Hor.} & {\scriptsize{}Lig.} & {\scriptsize{}M60} & {\scriptsize{}Pan.} & {\scriptsize{}Pla.} & {\scriptsize{}Tra.} & {\scriptsize{}Mean} & {\scriptsize{}Aud.} & {\scriptsize{}Bal.} & {\scriptsize{}Cou.} & {\scriptsize{}Mus.} & {\scriptsize{}Pal.} & {\scriptsize{}Tem.}\tabularnewline
\hline
{\scriptsize{}10-view} & {\scriptsize{}64.90} & {\scriptsize{}81.85} & {\scriptsize{}66.29} & {\scriptsize{}59.47} & {\scriptsize{}65.03} & {\scriptsize{}64.69} & {\scriptsize{}62.18} & \textbf{\scriptsize{}61.44} & {\scriptsize{}58.22} & {\scriptsize{}39.55} & \textbf{\scriptsize{}28.28} & {\scriptsize{}45.60} & {\scriptsize{}36.90} & {\scriptsize{}50.05} & {\scriptsize{}33.99} & {\scriptsize{}42.46}\tabularnewline

{\scriptsize{}15-view} & {\scriptsize{}65.89} & {\scriptsize{}81.32} & {\scriptsize{}68.54} & \textbf{\scriptsize{}60.59} & {\scriptsize{}67.82} & {\scriptsize{}64.59} & {\scriptsize{}63.70} & {\scriptsize{}61.19} & {\scriptsize{}59.34} & {\scriptsize{}40.19} & {\scriptsize{}28.19} & {\scriptsize{}45.59} & {\scriptsize{}	38.64} & {\scriptsize{}51.91} & {\scriptsize{}33.79} & \textbf{\scriptsize{}43.01}\tabularnewline

{\scriptsize{}20-view} & \textbf{\scriptsize{66.37}} & \textbf{\scriptsize{}82.06} & \textbf{\scriptsize{}69.34 } & {\scriptsize{}60.49} & \textbf{\scriptsize{}68.61 } & \textbf{\scriptsize{}65.67 } & \textbf{\scriptsize{}64.08} & {\scriptsize{}61.23} & \textbf{\scriptsize{}59.53} & \textbf{\scriptsize{}40.87} & {\scriptsize{}28.22} & \textbf{\scriptsize{}46.75}&\textbf{\scriptsize{}39.30 } & \textbf{\scriptsize{}52.88} & \textbf{\scriptsize{}35.16} & {\scriptsize{}42.95}  \tabularnewline
\hline 
\end{tabular}{\scriptsize\par}}
\vspace{-0.15in}
\end{table}

\subsection{Qualitative Results of DTU}
\label{sec:appendix_qualitative_dtu}

We provide qualitative results of DTU compared with CDS-MVSNet~\citep{giang2021curvature} and GBiNet~\citep{mi2021generalized}. Qualitative depth and confidence comparisons are shown in Fig.~\ref{fig:dtu_depth_qualitative}, while point clouds are compared in Fig.~\ref{fig:dtu_ply_compare}.
From Fig.~\ref{fig:dtu_depth_qualitative}, our MVSFormer can achieve more robust depth maps compared with others. 
Notably, depth maps from MVSFormer are as smooth as the regressive depth got from CDS-MVSNet. Besides, the \emph{argmax} operation used in GBiNet fails to achieve stable depth predictions, and relies heavily on confidence maps to filter invalid depth. But our MVSFormer can get not only good depth predictions but also reliable confidence maps, which is benefited from the proposed temperature-based depth prediction.
From Fig.~\ref{fig:dtu_ply_compare}, our method can faithfully reconstruct some challenging point clouds, which are usually omitted by other competitors.

\begin{figure}[h]
\begin{centering}
\includegraphics[width=0.99\linewidth]{pics/DTU_depth.pdf} 
\par\end{centering}
\caption{Qualitative DTU depth and confidence compared with CDS-MVSNet~\citep{giang2021curvature}, GBiNet~\citep{mi2021generalized}, and our MVSFormer.}
\label{fig:dtu_depth_qualitative}
\vspace{-0.15in}
\end{figure}

\begin{figure}[h]
\begin{centering}
\includegraphics[width=0.95\linewidth]{pics/dtu_ply_compare.pdf} 
\par\end{centering}
\caption{Qualitative DTU point clouds compared with CDS-MVSNet~\citep{giang2021curvature}, GBiNet~\citep{mi2021generalized}, and our MVSFormer. Please zoom-in for details.}
\label{fig:dtu_ply_compare}
\vspace{-0.15in}
\end{figure}

\subsection{Qualitative Results of Tanks-and-Temples}
\label{sec:appendix_qualitative_tanks}
Our qualitative depth results of Tanks-and-Temples are shown in Fig.~\ref{fig:TNT_depth}. Benefited by the proposed temperature-based depth prediction, MVSFormer can achieve not only good depth predictions but also reliable confidence maps, which leads to high-quality filtered depth in Fig.~\ref{fig:TNT_depth}(d).

We also provide qualitative results of Tanks-and-Temples compared with TransMVSNet~\citep{ding2021transmvsnet} and UniMVSNet~\citep{peng2022rethinking}. Fig.~\ref{fig:tnt_qual} shows qualitative results of `Horse' and `Lighthouse' in the Tanks-and-Temples intermediate set. From Fig.~\ref{fig:tnt_qual}, our MVSFormer can reconstruct more details (better Recall) and generate point clouds with more accurate positions (better Precision). But TransMVSnet misses some structures in `Horse' and predicts biased points in `Lighthouse'. On the other hands, UniMVSNet suffers from many outliers in `Horse', and fails to reconstruct a correct lighthouse.

\begin{figure}[h]
\begin{centering}
\includegraphics[width=0.85\linewidth]{pics/TNT_depth.pdf} 
\par\end{centering}
\caption{Depth prediction, depth confidence, and filtered depth of our MVSFormer on Tanks-and-Temples. Depth maps are filtered by confidence  in (d).}
\label{fig:TNT_depth}
\vspace{-0.1in}
\end{figure}

 \begin{figure}[h]
 \begin{centering}
 \includegraphics[width=0.85\linewidth]{pics/tnt_qual.pdf} 
 \par\end{centering}
\caption{Qualitative results of Tanks-and-Temples (Horse and  	Lighthouse), compared with TransMVSNet~\citep{ding2021transmvsnet},  UniMVSNet~\citep{peng2022rethinking}, and our MVSFormer.  is the distance threshold provided officially. mm and mm for `Horse' and `Lighthouse' respectively.
 }
 \label{fig:tnt_qual}
 \vspace{-0.15in}
 \end{figure}
 
\subsection{Training Randomness of MVSFormer}
To evaluate the training randomness of MVSFormer, we additionally train the other 4 Twins based MVSFormers and DINO based MVSFormer-Ps as in Tab.~\ref{tab:randomness} with the same settings except for random seeds. The reconstruction results of MVSFormer and MVSFormer-P are stable.

\begin{table}[!h]
\caption{Training randomnesses of MVSFormer and MVSFormer-P. The std deviations are after the .}
\label{tab:randomness}
\centering
\begin{tabular}{cccc}
\toprule 
Methods & Accuracy(mm) & Completeness(mm) & Overall(mm)\tabularnewline
\midrule 
MVSFormer & 0.32746.36e-4 & 0.25185.31e-4 & 0.28963.73e-4\tabularnewline
MVSFormer-P & 0.32757.93e-4 & 0.26528.26e-4 & 0.29643.15e-4\tabularnewline
\bottomrule 
\end{tabular}
\end{table}
 
 
\section{Real-World Results}

We show some challenging real-world cases in Fig.~\ref{fig:real-world} compared with CasMVSNet~\citep{gu2020cascade}. Colmap~\citep{schonberger2016pixelwise} is utilized for camera poses. We use the same setting of Gipuma fusion~\citep{Galliani_2015_ICCV} with disparity threshold 0.2, number consistent 3 for both CasMVSNet and MVSFormer. The confidence thresholds are set as 0.9 and 0.5 for CasMVSNet and MVSFormer respectively, which follow their DTU settings. 
From Fig.~\ref{fig:real-world}, CasMVSNet suffers from messy outliers without segmentation masks, while our MVSFormer can achieve good results. Besides, our method can get robust results for texture-less and reflective objects.

\begin{figure}[h]
\begin{centering}
\includegraphics[width=0.8\linewidth]{pics/real_world.pdf}
\par\end{centering}
\caption{Point Clouds of two real-world cases without masking compared with CasMVSNet~\citep{gu2020cascade}. These cases are challenging in complex structures (the first row), hairy, texture-less and reflective objects (the second row).}
\label{fig:real-world}
 \vspace{-0.1in}
\end{figure}

\section{More Point Cloud Results}

We show all point clouds of DTU test set generated by our MVSFormer in Fig.~\ref{fig:dtu_ply}. And point clouds of both intermediate and advanced sets of Tanks-and-Temples are shown in Fig.~\ref{fig:tnt_ply}. 
Moreover, ETH3D point clouds are shown in Fig.~\ref{fig:eth3d_ply}.

\begin{figure}
\begin{centering}
\includegraphics[width=0.8\linewidth]{pics/eth3d_test.pdf} 
\par\end{centering}
\caption{Point Clouds of scene dataset ETH3D~\citep{schops2017multi} reconstructed by MVSFormer.}
\label{fig:eth3d_ply}
\vspace{-0.1in}
\end{figure}

\begin{figure}
\begin{centering}
\includegraphics[width=1.0\linewidth]{pics/DTU_ply.png} 
\par\end{centering}
\caption{Point Clouds of all test set in DTU~\citep{aanaes2016large} reconstructed by MVSFormer.}
\label{fig:dtu_ply}
\end{figure}


\begin{figure}
\begin{centering}
\includegraphics[width=1.0\linewidth]{pics/tnt.pdf} 
\par\end{centering}
\caption{Point Clouds of Tanks-and-Temples~\citep{Knapitsch2017} reconstructed by MVSFormer.}
\label{fig:tnt_ply}
\end{figure}

\section{Limitations and Future Works}

We discuss the limitations and potential future works. 
In particular, 
1) we utilize the recent ViTs pre-trained with self-supervised (MAE~\citep{he2021masked}, DINO~\citep{caron2021emerging}) and supervised (Twins~\citep{chu2021twins}) tasks, while it is an interesting future work of exploring the influence of different pre-training tasks on MVS. 
The involved methods -- MAE, DINO, and Twins are relatively representative. Specifically, MAE and DINO are based on the vanilla plain-ViT, while Twins is based on the hierarchical-ViT. Additionally, note that due to the memory cost limitation, we have to freeze ViT weights in MVSFormer-P, which loses some advantages over another variant -- Twins based MVSFormer.
Therefore, these ViTs are not ensured to be pre-trained with the same architecture and training settings in our paper.
So it is interesting to use the same ViT architecture for different pre-training tasks, and further explore how these pre-training tasks (supervised and self-supervised) affect MVS. 
However, re-training all of these ViTs with different pre-training tasks is very non-trivial, and demands extraordinary computing resources. Thus we take it as future work. 
Critically, the performance of our proposed models has already outperformed all existing methods; this demonstrates the efficacy of our models.
2) Despite being simple, the fusion method used in this paper is good enough to make our models competitive. 
Here, we only consider the single and multi-scale feature addition. 
On the other hand, more technical and informative fusion strategies would be much advisable, such as cross-attention~\citep{vaswani2017attention} and GRU modules~\citep{cho2014learning}. This however would also be a very interesting future work that may inspire the community.

\section{Broader Impact}
Our methods can take the 3D reconstruction based on 2D images. Since learning-based MVS methods can be generalized to various real-world datasets, the proposed method may cause some societal impacts with controversial 2D images. Note that we only provide technical methods in this paper, but the real-world practice with potential negative societal impacts should be further considered. 

\end{document}
