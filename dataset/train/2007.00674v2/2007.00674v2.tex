

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} \usepackage[T1]{fontenc}
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2021}

\usepackage{amsfonts}
\usepackage{array}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{amsmath, amsthm, amssymb}
\DeclareMathOperator*{\argmax}{arg\,max}

\newtheorem{prop}{Proposition}



\icmltitlerunning{Sliced Iterative Normalizing Flows}

\begin{document}

\twocolumn[
\icmltitle{Sliced Iterative Normalizing Flows}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Biwei Dai}{ucb}
\icmlauthor{Uro{\v s} Seljak}{ucb,lbl}
\end{icmlauthorlist}

\icmlaffiliation{ucb}{Department of Physics, University of California, Berkeley, California, USA}
\icmlaffiliation{lbl}{Lawrence Berkeley National Laboratory, Berkeley, California, USA}

\icmlcorrespondingauthor{Biwei Dai}{biwei@berkeley.edu}

\icmlkeywords{Generative Models, Normalizing Flow, Optimal Transport, Sliced Wasserstein Distance}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
We develop an iterative (greedy) deep learning (DL) algorithm
which is able to transform an arbitrary probability distribution function (PDF) into the target PDF. The model is based on iterative Optimal Transport of a series of 1D slices, matching on each slice the marginal PDF to the target. 
The axes of the orthogonal slices are chosen to maximize the PDF difference using Wasserstein distance at each iteration, which enables the algorithm to scale well to high dimensions. 
As special cases of this algorithm, we introduce two sliced iterative Normalizing Flow (SINF) models, which map from the data to the latent space (GIS) and vice versa (SIG). 
We show that SIG is able to generate high quality samples of image datasets, which match the GAN benchmarks. GIS obtains competitive results on density estimation tasks compared to the density trained NFs, and is more stable, faster, and achieves higher  when trained on small training sets. SINF
approach deviates significantly from the current DL paradigm, as it is greedy and does not use concepts such as mini-batching, stochastic gradient descent and gradient back-propagation through deep layers. 


\end{abstract}

\section{Introduction}

\label{sec:introduction}

Latent variable generative models such as Normalizing Flows (NFs) \citep{rezende2015variational,dinh2014nice,dinh2016density,kingma2018glow}, Variational Auto-Encoders (VAEs) \citep{kingma2013auto,rezende2014stochastic} and Generative Adversarial Networks (GANs) \citep{goodfellow2014generative, radford2015unsupervised} aim to model the distribution  of high-dimensional input data  by introducing a mapping from a latent variable  to , where  is assumed to follow a given prior distribution . These models usually parameterize the mapping using neural networks, and the training of these models typically consists of minimizing a dissimilarity measure between the model distribution and the target distribution. For NFs and VAEs, maximizing the marginal likelihood is equivalent to minimizing the Kullback–Leibler (KL) divergence. While for GANs, the adversarial training leads to minimizations of the Jenson-Shannon (JS) divergence \citep{goodfellow2014generative}. The performance of these models largely depends on the following aspects: \newline\ \ \ \textbf{1)} The parametrization of the mapping (the architecture of the neural network) should match the structure of the data and be expressive enough. Different architectures have been proposed \citep{kingma2018glow, van2017neural,karras2017progressive, karras2019style}, but to achieve the best performance on a new dataset one still needs extensive hyperparameter explorations \citep{lucic2018gans}.\newline \ \ \  \textbf{2)} The dissimilarity measure (the loss function) should be appropriately chosen for the tasks. For example, in high dimensions the JS divergence is more correlated with the sample quality than KL divergence \citep{huszar2015not, theis2015note}, which is believed to be one of the reasons that GANs are able to generate higher quality samples than VAEs and NFs. However, JS divergence is hard to directly work with, and the adversarial training could bring many problems such as vanishing gradient, mode collapse and non-convergence \citep{arjovsky2017towards, wiatrak2019stabilizing}.

To avoid these complexities, in this work we adopt a different approach to build the map from latent variable  to data . We approach this problem from the Optimal Transport (OT) point of view. OT studies whether the transport maps exist between two probability distributions, and if they do, how to construct the map to minimize the transport cost. Even though the existence of transport maps can be proved under mild conditions \citep{villani2008optimal}, it is in general hard to construct them in high dimensions. We propose to decompose the high dimensional problem into a succession of 1D transport problems, where the OT solution is known. The mapping is iteratively augmented, and it has a NF structure that allows explicit density estimation and efficient sampling. We name the algorithm Sliced Iterative Normalizing Flow (SINF). Our objective function is inspired by the Wasserstein distance, which is defined as the minimal transport cost and has been widely used in the loss functions of generative models \citep{arjovsky2017towards, tolstikhin2017wasserstein}. We propose a new metric, max K-sliced Wasserstein distance, which enables the algorithm to scale well to high dimensions. 

In particular, SINF algorithm has the following properties: \newline
    \textbf{1)} The performance is competitive compared to state-of-the-art (SOTA) deep learning generative models. We show that if the objective is optimized in data space, the model is able to produce high quality samples similar to those of GANs; and if it is optimized in latent space, the model achieves comparable performance on density estimation tasks compared to NFs trained with maximum likelihood, and achieves highest performance on small training sets.\newline
    \textbf{2)} Compared to generative models based on neural networks, this algorithm has very few hyperparameters, and the performance is insensitive to their choices. \newline
    \textbf{3)} The model training is very stable and insensitive to random seeds. In our experiments we do not observe any cases of training failures. \newline 


\section{Method}



Flow-based models provide a powerful framework for density estimation \citep{dinh2016density, papamakarios2017masked}
and sampling \citep{kingma2018glow}. These models map the data  to latent variables  through a sequence of invertible transformations , such that  and  is mapped to a base distribution , which is normally chosen to be a standard Normal distribution. The probability density of data  can be evaluated using the change of variables formula:

The Jacobian determinant  must be easy to compute for evaluating the density, and the transformation  should be easy to invert for efficient sampling.

Before discussing our method we introduce two useful concepts, Radon transform and max sliced Wasserstein distance, in Section \ref{subsec:radon} and \ref{subsec:SWD}. We then introduce our objective max K-sliced Wasserstein distance in Section \ref{subsec:max-K-SWD}, and introduce SINF algorithm in Section \ref{subsec:algorithm}. Two iterative NF models are discussed as special cases of the algorithm in Section \ref{subsec:SIG} and \ref{subsec:GIS}. Finally we talk about our strategy for modeling high dimensional image datasets in Section \ref{subsec:patch}.

\subsection{Radon transform}
\label{subsec:radon}

Let  be the space of absolute integrable functions on . The Radon transform  is defined as 

where  denotes the unit sphere  in ,  is the Dirac delta function, and  is the standard inner product in . For a given , the function  is essentially the slice (or projection) of  on axis .

Note that the Radon transform  is invertible. Its inverse, also known as the filtered back-projection formula, is given by \citep{helgason2010integral, kolouri2019generalized}

where  is the convolution operator, and the convolution kernel  has the Fourier transform . The inverse Radon transform provides a practical way to reconstruct the original function  using its 1D slices , and is widely used in medical imaging. This inverse formula implies that if the 1D slices of two functions are the same in all axes, these two functions are identical. This is also known as Carm{\'e}r-Wold theorem \citep{cramer1936some}.

\subsection{Sliced and maximum sliced Wasserstein distance}
\label{subsec:SWD}

The p-Wasserstein distance, , between two probability distributions  and  is defined as:

where  is the set of all possible joint distributions  with marginalized distributions  and . In 1D the Wasserstein distance has a closed form solution via Cumulative Distribution Functions (CDFs), but this evaluation is intractable in high dimension. An alternative metric, the Sliced p-Wasserstein Distance (SWD), is defined as:

The SWD can be calculated by approximating the high dimensional integral with Monte Carlo samples. However, in high dimensions a large number of projections is required to accurately estimate SWD. This motivates to use the maximum Sliced p-Wasserstein Distance (max SWD):

which is the maximum of the Wasserstein distance of the 1D marginalized distributions of all possible directions. SWD and max SWD are both proper distances \citep{kolouri2015radon, kolouri2019generalized}.

\subsection{Maximum K-sliced Wasserstein distance}

\label{subsec:max-K-SWD}

We generalize the idea of maximum SWD and propose maximum K-Sliced p-Wasserstein Distance (max K-SWD):

In this work we fix . The proof that max K-SWD is a proper distance is in the supplementary document.
If , it becomes max SWD. For , the idea of finding the subspace with maximum distance is similar to the subspace robust Wasserstein distance \citep{paty2019subspace}. \citet{wu2019sliced} proposed to approximate SWD with orthogonal projections, similar to max K-SWD with . max K-SWD will be used as the objective in our proposed algorithm. It also defines K orthogonal axes  where the marginal distributions of  and  are most different, providing a natural choice for performing 1D marginal matching in our algorithm (see Section \ref{subsec:algorithm}).

The optimization in max K-SWD is performed under the constraints that  are orthonormal vectors, or equivalently,  where  is the matrix whose i-th column vector is . Mathematically, the set of all possible  matrices is called Stiefel Manifold , and we perform the optimization on the Stiefel Manifold following \citet{tagare2011notes}. The details of the optimization is provided in the supplementary document, and the procedure for estimating max K-SWD and  is shown in Algorithm \ref{alg:KmaxSWD}.

\begin{algorithm}[tb]
   \caption{max K-SWD}
   \label{alg:KmaxSWD}
\begin{algorithmic}
   \STATE {\bfseries Input:} , , , order , max iteration 
   \STATE Randomly initialize 
   \FOR{ {\bfseries to} }
   \STATE Initialize 
   \FOR{ {\bfseries to} }
   \STATE  
   \STATE Compute  and  for each  
   \STATE Sort  and  in ascending order s.t.  and 
   \STATE 
   \ENDFOR
   \STATE ,  , 
   \STATE Determine learning rate  with backtracking line search  
   \IF{ has converged}
   \STATE Early stop
   \ENDIF
   \ENDFOR
   \STATE {\bfseries Output:} , 
\end{algorithmic}
\end{algorithm}

\subsection{Proposed SINF algorithm}

\label{subsec:algorithm}

In this section we consider the general problem of building a NF that maps an arbitrary PDF  to another arbitrary PDF  of the same dimensionality. We will consider the special cases of  and  being standard Normal distributions in Section \ref{subsec:SIG} and Section \ref{subsec:GIS}, respectively. The propose algorithm is based on iteratively matching the 1D marginalized distribution of  to . This is motivated by the inverse Radon Transform (Equation \ref{eq:inverseRadon}) and Cram{\'e}r-Wold theorem, which suggest that matching the high dimensional distributions is equivalent to matching the 1D slices on all possible directions, decomposing the high dimensional problem into a series of 1D problems. Given a set of i.i.d. samples  drawn from , in each iteration, a set of 1D marginal transformations  ( where  is the dimensionality of the dataset) are applied to the samples on orthogonal axes  to match the 1D marginalized PDF of  along those axes. Let  be the weight matrix (), the transformation at iteration  of samples  can be written as
\footnote{{\bf Notation definition}: In this paper we use , ,  and  to represent different iterations of the algorithm, different axes , different gradient descent iterations of max K-SWD calculation (see Algorithm \ref{alg:KmaxSWD}), and different knots in the spline functions of 1D transformation, respectively.}

where  contains the components that are perpendicular to  and is unchanged in iteration .  is the marginal mapping of each dimension of , and its components are required to be monotonic and differentiable. The transformation of Equation \ref{eq:forward} can be easily inverted:

where . The Jacobian determinant of the transformation is also efficient to calculate (see supplementary document for the proof):


The weight matrix  and the marginal transformations  are determined by iteratively minimizing the max K-SWD (Equation \ref{eq:maxKSWp}) between the transformed  and . Specifically, we propose to iteratively solving for the orthogonal axes  in max K-SWD, and then apply 1D marginal matching on those axes to minimize max K-SWD. At iteration , the objective can be written as:

The algorithm firstly optimize  to maximize the objective, with  fixed to identical transformations (equivalent to Equation \ref{eq:maxKSWp}). Then the axes  are fixed and the objective is minimized with  marginal matching . The samples are updated, and this process repeats until convergence.  

\begin{algorithm}[tb]
   \caption{Sliced Iterative Normalizing Flow}
   \label{alg:NF}
\begin{algorithmic}
   \STATE {\bfseries Input:} , , , number of iteration 
   \FOR{ {\bfseries to} }
   \STATE 
   \FOR{ {\bfseries to} }
   \STATE  
   \STATE Compute  and  for each  
   \STATE \\ 
   \STATE 
   \ENDFOR
   \STATE 
   \STATE Update  
   \ENDFOR
\end{algorithmic}
\end{algorithm}

Let  be the transformed  at iteration . The th component of , , maps the 1D marginalized PDF of  to  and has an OT solution:

where  and  are the CDFs of  and  on axis , respectively. The CDFs can be estimated using the quantiles of the samples (in SIG Section \ref{subsec:SIG}), or using Kernel Density Estimation (KDE, in GIS Section \ref{subsec:GIS}). Equation \ref{eq:1D} is monotonic and therefore invertible. We choose to parametrize it with monotonic rational quadratic splines \citep{gregory1982piecewise, durkan2019neural}, which are continuously-differentiable and allows analytic inversion. Details about the splines are shown in the supplementary document.

The proposed algorithm iteratively minimizes the max K-SWD between the transformed  and . The orthonomal vectors  specify  axes along which the marginalized PDF between  and  are most different, thus maximizes the gain at each iteration and improves the efficiency of the algorithm. In the supplementary document we show empirically that the model is able to converge with two orders of magnitude fewer iterations than random axes, and it also leads to better sample quality. This is because as the dimensionality  grows, the number of slices  required to approximate  using inverse Radon formula scales as  \citep{kolouri2015radon}, where  is the number of slices needed to approximate a similar smooth 2D distribution. Therefore, if  are randomly chosen, it takes a large number of iterations to converge in high dimensions due to the curse of dimensionality. Our objective function reduces the curse of dimensionality in high dimensions by identifying the most relevant directions first. 


 is a free hyperparameter in our model. In the supplementary document we show empirically that the convergence of the algorithm is insensitive to the choice of , and mostly depends on the total number of 1D transformations .

Unlike KL-divergence which is invariant under flow transformations, max K-SWD is different in data space and in latent space. Therefore the direction of building the flow model is of key importance. In the next two sections we discuss two different ways of building the flow, which are good at sample generation and density estimation, respectively.

\subsection{Sliced Iterative Generator (SIG)}

\label{subsec:SIG}

For Sliced Iterative Generator (SIG)  is a standard Normal distribution, and  is the target distribution. The model iteratively maps the Normal distribution to the target distribution using 1D slice transformations. 
SIG directly minimizes the max K-SWD between the generated distribution and the target distribution, and is able to generate high quality samples.

Specifically, one firstly draw a set of samples from the standard Normal distribution, and then iteratively update the samples following Equation \ref{eq:forward}. Note that in the NF framework, Equation \ref{eq:forward} is the inverse of transformation  in Equation \ref{eq:flow}. The  transformation and weight matrix  are learned using Equation \ref{eq:1D} and Algorithm \ref{alg:KmaxSWD}. In Equation \ref{eq:1D} we estimate the CDFs using the quantiles of the samples.

\subsection{Gaussianizing Iterative Slicing (GIS)}

\label{subsec:GIS}

For Gaussianizing Iterative Slicing (GIS)
 is the target distribution and  is a standard Normal distribution. The mapping is learned in the reverse direction of SIG. 
In GIS the max K-SWD between latent data and the Normal distribution is minimized, thus the model performs well in density estimation, even though its learning objective is not .

We add regularization to GIS for density estimation tasks to further improve the performance and reduce overfitting. The regularization is added in the following two aspects:\newline
1) The weight matrix  is regularized by limiting the maximum number of iteration  (see Algorithm \ref{alg:KmaxSWD}). We set , where  is the number of training data and  is the dimensionality. Thus for very small datasets () the axes of marginal transformation are almost random. This has no effect on datasets of regular size. \newline
2) The CDFs in Equation \ref{eq:1D} are estimated using KDE, and the 1D marginal transformation is regularized with:

where  is the regularization parameter.  is the regularized transformation. In the supplements we show that as  increases, the performance improves, but more iterations are needed to converge. Thus  controls the trade-off between performance and speed.







\subsection{Patch-based hierarchical approach}

\label{subsec:patch}

Generally speaking, the neighboring pixels in images have stronger correlations than pixels that are far apart. This fact has been taken advantage by convolutional neural networks, which outperform Fully Connected Neural Networks (FCNNs) and have become standard building blocks in computer vision tasks. Like FCNNs, vanilla SIG and GIS make no assumption about the structure of the data and cannot model high dimensional images very well. Recently, \citet{meng2020gaussianization} proposed a patch-based approach, providing a different way to improve the modeling of the local correlations for NF models. The patch-based approach decomposes an  image into  patches, with  neighboring pixels in each patch (). In each iteration the marginalized distribution of each patch is modeled separately without considering the correlations between different patches. This approach effectively reduces the dimensionality from  to , at the cost of ignoring the long range correlations.

To reduce the effects of ignoring long range correlations, we propose a hierarchical model. In SIG, we start from modeling the entire images, which corresponds to  and . After some iterations the samples show correct structures, indicating the long range correlations have been modeled well. We then gradually decrease the patch size  until , which allows us to gradually focus on the smaller scales. Assuming a periodic boundary condition, we let the patches randomly shift in each iteration. If the patch size  does not divide , we set  and the rest of the pixels are kept unchanged.

\section{Related work}

\label{sec:related}

Iterative normalizing flow models called 
RBIG \citep{chen2001gaussianization, laparra2011iterative} are simplified versions of GIS, as they are based on a succession of rotations followed by 1D marginal Gaussianizations. Iterative Distribution Transfer (IDT) \citep{pitie2007automated} is a similar algorithm but does not require the base distribution to be a Gaussian. These models do not scale well to high dimensions because they do not have a good way of choosing the 
axes, and they are not competitive against modern NFs trained on  \cite{meng2020gaussianization}. 
A DL, non-iterative version of these models is Gaussianization Flow (GF) \cite{meng2020gaussianization}, which trains on  and achieves good density estimation results in low dimensions, but does not have good sampling properties in high dimensions. RBIG, GIS and GF have similar architectures but are trained differently. We compare their density estimation performance in Section \ref{subsec:density}.

Another iterative generative model is Sliced Wasserstein Flow (SWF) \citep{liutkus2018sliced}. Similar to SIG, SWF tries to minimize the SWD between the distributions of samples and the data, and transforms this problem into solving a d dimensional PDE. The PDE is solved iteratively by doing a gradient flow in the Wasserstein space, and they show SWF works well for low dimensional bottleneck features. However, in each iteration the algorithm requires evaluating an integral over the  dimensional unit sphere approximated with Monte Carlo integration, which does not scale well to high dimensions. Another difference with SIG is that SWF does not have a flow structure, cannot be inverted, and does
not provide the likelihood. We compare the sample qualities between SWF and SIG in Section \ref{subsec:samples}.

SWD, max SWD and other slice-based distance (e.g. Cram{\'e}r-Wold distance) have been widely used in training generative models \citep{deshpande2018generative, deshpande2019max, wu2019sliced, kolouri2018sliced, knop2018cramer}. \citet{wu2019sliced} propose a differentiable SWD block composed of a rotation followed by marginalized Gaussianizations, but unlike RBIG, the rotation matrix is trained in an end-to-end DL fashion. They add SWD blocks to the generator of an AE to regularize the latent variables, and show that its sample quality outperforms VAE and AE + RBIG. 

\citet{grover2018flow} propose Flow-GAN using a NF as the generator of a GAN, so the model can perform likelihood evaluation, and allows both maximum likelihood and adversarial training. Similar to our work they find that adversarial training gives good samples but poor , while training by maximum likelihood results in bad samples. Similar to SIG, the adversarial version of Flow-GAN minimizes the Wasserstein distance between samples and data, and has a NF structure. We compare sample qualities in Section \ref{subsec:samples}.

\begin{table*}[htb]
  \caption{Negative test log-likelihood for tabular datasets measured in nats, and image datasets measured in bits/dim (lower is better). }
  \label{tab:density}
  \vskip 0.15in
  \centering
  \begin{tabular}{>{\centering}c|>{\centering}c|>{\centering}c>{\centering}c>{\centering}c>{\centering}c>{\centering}c|>{\centering}c>{\centering\arraybackslash}c}
    \toprule
    & Method & POWER & GAS & HEPMASS & MINIBOONE & BSDS300 & MNIST & Fashion\\
    \midrule\midrule
    \multirow{2}{*}{iterative}
    & RBIG & 1.02 & 0.05 & 24.59 & 25.41 & -115.96 & 1.71 & 4.46\\
    & GIS (this work) & -0.32 & -10.30 & 19.00 & 14.26 & -155.75 & 1.34 & 3.22\\
    \midrule
    \multirow{6}{*}{\shortstack{maximum\\likelihood}}
    & GF & -0.57 & -10.13 & 17.59 & 10.32 & -152.82 & 1.29 & 3.35 \\
    & Real NVP & -0.17 & -8.33 & 18.71 & 13.55 & -153.28 & 1.06 & 2.85\\
    & Glow & -0.17 & -8.15 & 18.92 & 11.35 & -155.07 & 1.05 & 2.95\\
    & FFJORD & -0.46 & -8.59 & 14.92 & 10.43 & -157.40 & 0.99 & - \\
    & MAF & -0.30 & -10.08 & 17.39 & 11.68 & -156.36 & 1.89 & -\\
    & RQ-NSF (AR) & -0.66 & -13.09 & 14.01 & 9.22 & -157.31 & - & -\\
    \bottomrule
  \end{tabular}
  \vskip -0.1in
\end{table*}

\section{Experiments}

\subsection{Density estimation  of tabular datasets}
\label{subsec:density}

\begin{figure}[htb]
     \centering
     \begin{subfigure}[t]{0.49\linewidth}
         \includegraphics[width=\textwidth]{POWER_logp_smalldataset}
         \caption{POWER (6D)}
     \end{subfigure}
     \begin{subfigure}[t]{0.49\linewidth}
         \includegraphics[width=\textwidth]{GAS_logp_smalldataset}
         \caption{GAS (8D)}
     \end{subfigure}
     \begin{subfigure}[t]{0.49\linewidth}
         \includegraphics[width=\textwidth]{HEPMASS_logp_smalldataset}
         \caption{HEPMASS (21D)}
     \end{subfigure}
     \begin{subfigure}[t]{0.49\linewidth}
         \includegraphics[width=\textwidth]{MINIBOONE_logp_smalldataset}
         \caption{MINIBOONE (43D)}
     \end{subfigure}
     \begin{subfigure}[t]{0.49\linewidth}
         \includegraphics[width=\textwidth]{BSDS300_logp_smalldataset}
         \caption{BSDS300 (63D)}
     \end{subfigure}
        \caption{Density estimation on small training sets. The legend in panel (a) applies to other panels as well. Higher is better: at 100 training data GIS has the best performance in all cases.
        }
        \label{fig:density}
      \vskip -0.2in
\end{figure}

We perform density estimation with GIS on four UCI datasets \citep{lichman2013uci} and BSDS300  \citep{martin2001database}, as well as image datasets MNIST \citep{lecun1998gradient} and Fashion-MNIST \citep{xiao2017online}. The data preprocessing of UCI datasets and BSDS300 follows \citet{papamakarios2017masked}. In Table \ref{tab:density} we compare our results with RBIG \citep{laparra2011iterative} and GF \citep{meng2020gaussianization}. The former can be seen as GIS with random axes to apply 1D gaussianization, while the latter can be seen as training non-iterative GIS with MLE training on . We also list other NF models Real NVP \citep{dinh2016density}, Glow \citep{kingma2018glow}, FFJORD \citep{grathwohl2018ffjord}, MAF \citep{papamakarios2017masked} and RQ-NSF (AR)\citep{durkan2019neural} for comparison. 

We observe that RBIG performs significantly worse than current SOTA. GIS outperforms RBIG and is the first iterative algorithm that achieves comparable performance compared to maximum likelihood models. This is even more impressive given that GIS is not trained on , yet it outperforms GF on  on GAS, BSDS300 and Fashion-MNIST.

The transformation at each iteration of GIS is well defined, and the algorithm is very stable even for small training sets. To test the stability and performance we compare the density estimation results with other methods 
varying the size of the training set  (from  to ). 
For other NFs we use settings recommended by their original paper, and set the batch size to , where  is the batch size used by the original paper. All the models are trained until the validation  stops improving, and for KDE the kernel width is chosen to maximize . Some non-GIS NF models collapsed during training or used more memory than our GPU, and are not shown in the plot. The result in Figure \ref{fig:density} show that GIS is more stable compared to other NFs and outperforms them on small training sets. This highlights that GIS is less sensitive to hyper-parameter optimization and achieves good performance out of the box. Training time varies with data size, but is generally lower than other NFs for small training sets, and can be as low as 0.2s on an everyday laptop CPU if using a few iterations only, which often suffices for simple distributions.

\subsection{Generative modeling of images}

\label{subsec:samples}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.495\linewidth}
         \centering
         \includegraphics[width=\linewidth]{SIG_MNISTsample_085}
         \caption{MNIST (T=0.85)}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.495\linewidth}
         \centering
         \includegraphics[width=\linewidth]{SIG_FashionMNISTsample_085}
         \caption{Fashion-MNIST (T=0.85)}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.495\linewidth}
         \centering
         \includegraphics[width=\linewidth]{SIG_CIFAR10sample_1}
         \caption{CIFAR-10 (T=1)}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.495\linewidth}
         \centering
         \includegraphics[width=\linewidth]{SIG_CelebAsample_085}
         \caption{CelebA (T=0.85)}
     \end{subfigure}
     \caption{Random samples from SIG.}
     \label{fig:sample}
     \vskip -0.2in
\end{figure}

\begin{table*}[htb]
  \caption{FID scores on different datasets (lower is better). The errors are generally smaller than the differences.}
  \label{tab:FID}
  \vskip 0.15in
  \centering
  \begin{threeparttable}
  \begin{tabular}{>{\centering}c|>{\centering}c|>{\centering}c>{\centering}c>{\centering}c>{\centering\arraybackslash}c}
    \toprule
    & Method & MNIST & Fashion & CIFAR-10 & CelebA\\ 
    \midrule\midrule
    \multirow{2}{*}{iterative}
    & SWF &  &  & - & -\\
    & SIG () (this work) &  &  &  & \\\midrule
    \multirow{4}{*}{\shortstack{adversarial\\training}}
    & Flow-GAN (ADV) &  &  &  & -\\
    & WGAN &  &  &  & \\
    & WGAN GP &  &  &  & \\
    & Best default GAN &  &  &  &  \\\midrule
    \multirow{3}{*}{\shortstack{AE based}}
    & SWAE &  &  &  & \\
    & CWAE &  &  &  & \\
    & PAE & - &  & - & \\
    & two-stage VAE &  &  & & \\
    & Best default VAE & 16.6 & 43.6 & -& 53.3\\
    \bottomrule
  \end{tabular}
\end{threeparttable}
  \vskip -0.1in
\end{table*}

\begin{figure}
     \centering
     \begin{subfigure}[]{\linewidth}
         \centering \includegraphics[width=\linewidth]{FashionMNIST_samples_niter_2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[]{\linewidth}
         \centering \includegraphics[width=\linewidth]{CelebA_samples_niter_2}
     \end{subfigure}
     \caption{Gaussian noise (first column), Fashion-MNIST (top panel) and CelebA (bottom) samples at different iterations.}
     \label{fig:iteration}
     \vskip -0.1in
\end{figure}


\begin{figure}
     \centering
     \begin{subfigure}[]{0.08\linewidth}
         \centering \includegraphics[width=\linewidth]{CelebA_NN1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[]{0.8\linewidth}
         \centering \includegraphics[width=\linewidth]{CelebA_interpolation}
     \end{subfigure}
     \hfill
     \begin{subfigure}[]{0.08\linewidth}
         \centering \includegraphics[width=\linewidth]{CelebA_NN2}
     \end{subfigure}
     \caption{Middle: interpolations between CelebA samples from SIG. Left and right: the corresponding nearest training data.}
     \label{fig:interpolation}
     \vskip -0.1in
\end{figure}



We evaluate SIG as a generative model of images using the following 4 datasets: MNIST, Fashion-MNIST, CIFAR-10 \citep{krizhevsky2009learning} and Celeb-A (cropped and interpolated to  resolution) \citep{liu2015faceattributes}.

In Figure \ref{fig:sample} we show samples of these four datasets. For MNIST, Fashion-MNIST and CelebA dataset we show samples from the model with reduced temperature  (i.e., sampling from a Gaussian distribution with standard deviation  in latent space), which slightly improves the sample quality \citep{parmar2018image, kingma2018glow}. We report the final FID score (calculated using temperature T=1) in Table \ref{tab:FID}, where we compare our results with similar algorithms SWF and Flow-Gan (ADV). We also list the FID scores of some other generative models for comparison, including models using sliced-based distance Sliced Wasserstein Auto-Encoder (SWAE) \citep{kolouri2018sliced} and Cramer-Wold Auto-Encoder (CWAE) \citep{knop2018cramer}, Wasserstein GAN models \citep{arjovsky2017wasserstein, gulrajani2017improved}, and other GANs and AE-based models Probablistic Auto-Encoder (PAE) \citep{bohm2020probabilistic} and two-stage VAE \citep{dai2019diagnosing,xiao2019generative}.
The scores of WGAN and WGAN-GP models are taken from \citet{lucic2018gans}, who performed a large-scale testing protocol over different GAN models. The "Best default GAN" is extracted from Figure 4 of \citet{lucic2018gans}, indicating the lowest FID scores from different GAN models with the hyperparameters suggested by original authors. The "Best default VAE" shows the lowest FID scores of the three VAE models considered in \citet{dai2019diagnosing}, with the default settings. NF models usually do not report FID scores. PAE combines AEs with NFs and we expect it to outperform most NF models in terms of sample quality due to the use of AEs. We notice that previous iterative algorithms are unable to produce good samples on high dimensional image datasets (see Table \ref{tab:FID} and Figure \ref{fig:improve} for SWF samples; see Figure 6 and 7 of \citet{meng2020gaussianization} for RBIG samples). However, SIG obtains the best FID scores on MNIST and Fashion-MNIST, while on CIFAR-10 and CelebA it also outperforms similar algorithms and AE-based models, and gets comparable results with GANs.
In Figure \ref{fig:iteration} we show the samples at different iterations. 
In Figure \ref{fig:interpolation} we display interpolations between SIG samples, and the nearest training data, to verify we are not memorizing the training data.


\subsection{Improving the samples of other generative models}

\label{subsec:improve}



\begin{figure}
     \centering
     \begin{subfigure}[]{0.495\linewidth}
         \centering \includegraphics[width=\linewidth]{SWF_FashionMNISTsample}
     \end{subfigure}
     \hfill
     \begin{subfigure}[]{0.495\linewidth}
         \centering \includegraphics[width=\linewidth]{SWF_FashionMNISTsample_improve30}
     \end{subfigure}
     \hfill
     \begin{subfigure}[]{0.495\linewidth}
         \centering \includegraphics[width=\linewidth]{FlowGAN_FashionMNISTsample}
     \end{subfigure}
     \hfill
     \begin{subfigure}[]{0.495\linewidth}
         \centering \includegraphics[width=\linewidth]{FlowGAN_FashionMNISTsample_improve30}
     \end{subfigure}
     \hfill
     \begin{subfigure}[]{0.495\linewidth}
         \centering \includegraphics[width=\linewidth]{MAF_FashionMNISTsample}
     \end{subfigure}
     \hfill
     \begin{subfigure}[]{0.495\linewidth}
         \centering \includegraphics[width=\linewidth]{MAF_FashionMNISTsample_improve30}
     \end{subfigure}
    \caption{Fashion-MNIST samples before (left panel) and after SIG improvement (right panel). Top: SWF. Middle: Flow-GAN (ADV). Bottom: MAF.}
    \label{fig:improve}
    \vskip -0.15in
\end{figure}

Since SIG is able to transform any base distribution to the target distribution, it can also be used as a "Plug-and-Play" tool to improve the samples of other generative models. To demonstrate this, we train SWF, Flow-GAN(ADV) and MAF(5) on Fashion-MNIST with the default architectures in their papers, and then we apply 240 SIG iterations ( of the total number of iterations in Section \ref{subsec:samples}) to improve the sample quality. In Figure \ref{fig:improve} we compare the samples before and after SIG improvement. Their FID scores improve from ,  and  to ,  and , respectively. These results can be further improved by adding more SIG iterations.


\subsection{Out of Distribution (OoD) detection}

\label{subsec:ood}

\begin{table}[htb]
  \caption{OoD detection accuracy quantified by the AUROC of data  trained on Fashion-MNIST.  }
  \label{tab:auroc}
  \vskip 0.15in
  \centering
  \begin{tabular}{>{\centering}c|>{\centering}c>{\centering\arraybackslash}c}
  \toprule
    Method          & MNIST  & OMNIGLOT \\
    \midrule\midrule
    SIG (this work)               &  \textbf{0.980} & \textbf{0.993}  \\
    GIS (this work) & 0.824 & 0.891 \\
    PixelCNN++ & 0.089 & -\\
IWAE          &  0.423   & 0.568   \\
    \bottomrule
  \end{tabular}
\end{table}

OoD detection with generative models has recently attracted a lot of attention, since the  estimates of NF and VAE have been shown to be poor OoD detectors:  different generative models can assign higher probabilities to OoD data than to In Distribution 
(InD) training data \citep{nalisnick2018deep}. One combination of datasets for which this has been observed is Fashion-MNIST and MNIST, where a model trained on the former assigns higher density to the latter. 



SINF does not train on the likelihood , which is an advantage for OoD. 
Likelihood is sensitive to the smallest variance directions \citep{ren2019likelihood}: for example, a zero variance pixel leads to an infinite , and noise must be added to regularize it. But zero variance directions contain little or no information on the global structure of the image. SINF objective is more sensitive to the meaningful global structures that can separate between OoD and InD. Because the patch based approach ignores the long range correlations and results in bad OoD, we use vanilla SINF without patch based approach. We train the models on F-MNIST, and then evaluate anomaly detection on test data of MNIST and OMNIGLOT \citep{lake2015human}. 
In Table~\ref{tab:auroc} we compare our results to maximum likelihood  models PixelCNN++\citep{salimans2017pixelcnn++, ren2019likelihood}, 
and IWAE \citep{choi2018waic}. Other models that perform well 
include VIB and WAIC \citep{choi2018waic}, which achieve 0.941, 0.943 and 0.766, 0.796, for MNIST and OMNIGLOT, respectively (below our SIG results). 
For the MNIST case \citet{ren2019likelihood} obtained 0.996 using the likelihood ratio between the model and its perturbed version, but they require fine-tuning on some additional OoD dataset, which may not be available in OoD applications. Lower dimensional latent space PAE \citep{bohm2020probabilistic} achieves 0.997 and 0.981 for MNIST and OMNIGLOT, respectively, while VAE based 
likelihood regret \citep{xiao2020likelihood} achieves
0.988 on MNIST, but requires additional (expensive)
processing. 

\section{Conclusions}

We introduce sliced iterative normalizig flow (SINF) that iteratively transform data distribution to a Gaussian (GIS) or the other way around (SIG) using OT. To the best of our knowledge, SIG is the first greedy deep learning algorithm that is competitive with the SOTA generators in high dimensions, while GIS achieves comparable results on density estimation with current NF models, but is more stable, faster to train, and achieves higher  when trained on small training sets even though it does not train on . It also achieves better OoD performance. SINF is very stable to train, has very few hyperparameters, and is very insensitive to their choice (see supplements).
SINF has deep neural network architecture, but its approach deviates significantly from the current DL paradigm, as it does not use concepts such as mini-batching, stochastic gradient descent and gradient back-propagation through deep layers. 
SINF is an existence proof that greedy DL without these ingredients can be SOTA for modern high dimensional ML applications, especially for small data applications. Such approaches thus deserve more detailed investigations that may have an impact on the theory and practice of DL. 




\section*{Acknowledgements}

We thank He Jia for providing his code on Iterative Gaussianization, and for his helpful discussions. We thank Jascha Sohl-Dickstein and Vanessa Boehm for comments on the manuscript. This material is based upon work supported by the National Science Foundation under Grant Numbers 1814370 and NSF 1839217, and by NASA under Grant Number 80NSSC18K1274. 



\bibliography{SIGGIS_reference}
\bibliographystyle{icml2021}





\newpage
\ 
\newpage
\appendix
\section{Proofs}
\begin{prop}
\label{prop:max-K-SWD}
Let  be the set of Borel probability measures with finite p’th moment on metric space . The maximum K-sliced p-Wasserstein distance is a metric over .
\end{prop}

\begin{proof}
We firstly prove the triangle inequality.
Let ,  and  be probability measures in  with probability density function ,  and , respectively. 
Let ; then

where the first inequality comes from the triangle inequality of Wasserstein distance, and the second inequality follows Minkowski inequality. Therefore  satisfies the triangle inequality.

Now we prove the identity of indiscernibles. For any probability measures  and  in  with probability density function  and , let \\
, and\\
, we have

On the other hand, let  be a set of orthonormal vectors in  where the first element is , we have 

Therefore we have . Thus , where we use the non-negativity and identity of indiscernibles of .

Finally, the symmetry of  can be proven using the fact that p-Wasserstein distance is symmetric:

\end{proof}

\begin{proof}[Proof of Equation \ref{eq:jacobian}]
Let  be a set of orthonormal basis in  where the first  vectors are , respectively. Let  be an orthogonal matrix whose i-th column vector is , . Since , we have  (the concatenation of columns of  and ). Let  be a marginal transformation that consists of  1D identity transformation,  , we have


Since  is an orthogonal matrix with determinant , and the Jacobian of the marginal transformation  is diagonal, the Jacobian determinant of the above equation can be written as

\end{proof}


\section{Monotonic Rational Quadratic Spline}
\label{sec:RQspline}

Monotonic Rational Quadratic Splines \citep{gregory1982piecewise, durkan2019neural} approximate the function in each bin with the quotient of two quadratic polynomials. They are monotonic, contineously differentiable, and can be inverted analytically. The splines are parametrized by the coordinates and derivatives of  knots: , with ,  and . Given these parameters, the function in bin  can be written as \citep{durkan2019neural}

where ,  and . The derivative is given by

Finally, the inverse can be calculated with

where , ,  and . The derivation of these formula can be found in Appendix A of \citet{durkan2019neural}.

In our algorithm the coordinates of the knots are determined by the quantiles of the marginalized PDF (see Algorithm \ref{alg:NF}). The derivative   is determined by fitting a local quadratic polynomial to the neighboring knots , , and :

The function outside  is linearly extrapolated with slopes  and . In SIG,  and  are fixed to 1, while in GIS they are fitted to the samples that fall outside .

We use  knots in SIG to interpolate each , while in GIS we set . The performance is insensitive to these choices, as long as  is large enough to fully characterize the 1D transformation .

\section{Optimization on the Stiefel Manifold}
\label{sec:stiefel}

The calculation of max K-SWD (Equation \ref{eq:maxKSWp}) requires optimization under the constraints that  are orthonormal vectors, or equivalently,  where  is the matrix whose i-th column vector is . As suggested by \citet{tagare2011notes}, the optimization of matrix  can be performed by doing gradient ascent on the Stiefel Manifold:

where  is the weight matrix at gradient descent iteration  (which is different from the iteration  of the algorithm),  is the learning rate, which is determined by backtracking line search, , and  is the negative gradient matrix . Equation \ref{eq:Cayley1} has the properties that , and that the tangent vector  is the projection of gradient  onto  (the tangent space of  at ) under the canonical inner product \citep{tagare2011notes}. 

However, Equation \ref{eq:Cayley1} requires the inversion of a  matrix, which is computationally expensive in high dimensions. The matrix inversion can be simplified using the Sherman-Morrison-Woodbury formula, which results in the following equation \citep{tagare2011notes}:

where  (the concatenation of columns of  and ) and . Equation \ref{eq:Cayley2} only involves the inversion of a  matrix, where  is the number of axes to apply marginal transformation in each iteration. For high dimensional data (e.g. images), we use a relatively small  to avoid the inversion of large matrices. A large  leads to faster training, but one would converge to similar results with a small  using more iterations. In Appendix \ref{sec:ablation} we show that the convergence is insensitive to the choice of .

\section{Hyperparameter study and ablation analysis}
\label{sec:ablation}
Here we study the sensitivity of SINF to hyperparameters and perform 
ablation analyses. 
\subsection{Hyperparameter , objective function, and patch based approach}



\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{SWD}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{maxSWD}
     \end{subfigure}
     \caption{Sliced Wasserstein Distance (SWD, top panel) and Max-Sliced Wasserstein Distance (max SWD, bottom panel) between the MNIST test data and model samples as a function of total number of marginal transformations. The legend in the top panel also applies to the bottom panel. The SWD and max SWD between the training data and test data is shown in the horizontal solid black lines. The lines with "random" indicate that the axes are randomly chosen (like RBIG) instead of using the axes of max K-SWD. We also test  and . Their curves overlap with  and  and are not shown in the plot.}
     \label{fig:convergence}
     \vskip -0.10in
\end{figure}

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{MNIST_samples_niter_random}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{MNIST_samples_niter}
     \end{subfigure}
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{MNIST_samples_niter_hierarchy}
     \end{subfigure}
    \caption{Top panel: SIG samples with random axes (). Middle panel: SIG samples with optimized axes (). Bottom panel: SIG samples with optimized axes and patch based hierarchical approach. The numbers above each panel indicate the number of marginal transformations.}
    \label{fig:ablation}
\end{figure}

\begin{figure}[htb]
     \centering
     \includegraphics[width=\linewidth]{maxSWD_nsample}
     \caption{The measured maximum sliced Wasserstein distance between two Gaussian datasets as a function of number of samples. 10 different starting points are used to find the global maximum.}
    \label{fig:maxSWD_nsample}
    \vskip -0.10in
\end{figure}

We firstly test the convergence of SIG on MNIST dataset with different  choices. We measure the SWD (Equation \ref{eq:SWp}) and max SWD (Equation \ref{eq:maxSWp}) between the test data and model samples for different iterations (without patch based hierarchical modeling). The results are presented in Figure \ref{fig:convergence}. The SWD is measured with 10000 Monte Carlo samples and averaged over 10 times. The max SWD is measured with Algorithm \ref{alg:KmaxSWD} () using different starting points in order to find the global maximum. We also measure the SWD and max SWD between the training data and test data, which gives an estimate of the noise level arising from the finite number of test data. For the range of  we consider (), all tests we perform converges to the noise level, and the convergence is insensitive to the choice of , but mostly depends on the total number of 1D transformations (). As a comparison, we also try running SIG with random orthogonal axes per iteration, and for MNIST, our greedy algorithm converges with two orders of magnitude fewer marginal transformations than random orthogonal axes (Figure \ref{fig:convergence}).

For , the objective function (Equation \ref{eq:minimax}) is the same as max SWD, so one would expect that the max SWD between the data and the model distribution keep decreasing as the iteration number increases. For , the max K-SWD is bounded by max SWD (Equation \ref{eq:max-K-SWD-bound1} and \ref{eq:max-K-SWD-bound2}) so one would also expect similar behavior. However, from Figure \ref{fig:convergence} we find that max SWD stays constant in the first 400 iterations. This is because SIG fails to find the global maximum of the objective function in those iterations, i.e., the algorithm converges at some local maximum that is almost perpendicular to the global maximum in the high dimensional space, and therefore the max SWD is almost unchanged. This suggests that our algorithm does not require global optimization of  at each iteration: even if we find only a local maximum, it can be compensated with subsequent iterations. Therefore our model is insensitive to the initialization and random seeds. This is very different from the standard non-convex loss function optimization in deep learning with a fixed number of layers, where the random seeds often make a big difference in the results \citep{lucic2018gans}.

In Figure \ref{fig:ablation} we show the samples of SIG of random axes, optimized axes and hierarchical approach. On the one hand, the sample quality of SIG with optimized axes is better than that of random axes, suggesting that our proposed objective max K-SWD improves both the efficiency and the accuracy of the modeling. On the other hand, SIG with optimized axes has reached the noise level on both SWD and max SWD at around 2000 marginal transformations (Figure \ref{fig:convergence}), but the samples are not good at that point, and further increasing the number of 1D transformations from 2000 to 200000 does not significantly improve the sample quality. At this stage the objective function of Equation \ref{eq:minimax} is dominated by the noise from finite sample size, and the optimized axes are nearly random, which significantly limits the efficiency of our algorithm. To better understand this noise, we do a simple experiment by sampling two sets of samples from the standard normal distribution  and measuring the max SWD using the samples. The true distance should be zero, and any nonzero value is caused by the finite number of samples. In Figure \ref{fig:maxSWD_nsample} we show the measured max SWD as a function of sample size and dimensionality. For small number of samples and high dimensionality, the measured max SWD is quite large, suggesting that we can easily find an axis where the marginalized PDF of the two sets of samples are significantly different, while their underlying distribution are actually the same. Because of this sample noise, once the generated and the target distribution are close to each other (the max K-SWD reached the noise level), the optimized axes becomes random and the algorithm becomes inefficient. To reduce the noise level, one needs to either increase the size of training data or decrease the dimensionality of the problem. The former can be achieved with data augmentation. In this study we adopt the second approach, i.e., we effectively reduce the dimensionality of the modeling with a patch based hierarchical approach. The corresponding samples are shown in the bottom panel of Figure \ref{fig:ablation}. We see that the sample quality keeps improving after 2000 marginal transformations, because the patch based approach is used and the effective noise level is reduced.

\subsection{Effects of regularization parameter  in density estimation}

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.495\linewidth}
         \centering
         \includegraphics[width=\linewidth]{logp_alpha}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.495\linewidth}
         \centering
         \includegraphics[width=\linewidth]{Niter_alpha}
     \end{subfigure}
     \caption{Test log-likelihood (left panel) and number of iterations (right panel) as a function of regularization parameter  on POWER dataset.}
     \label{fig:alpha}
\end{figure}

To explore the effect of regularization parameter , we train GIS on POWER dataset with different . We keep adding iterations until the log-likelihood of validation set stops improving. The final test  and the number of iterations are shown in Figure \ref{fig:alpha}. We see that with a larger , the algorithm gets better density estimation performance, at the cost of taking more iterations to converge. Setting the regularization parameter  is a trade-off between performance and computational cost.

\section{Experimental details}
\label{sec:detail}

\begin{table*}[htb]
  \caption{GIS hyperparameters for density-estimation results in Table \ref{tab:density}.}
  \label{tab:hyper_GIS}
  \vskip 0.15in
  \centering
  \begin{tabular}{>{\centering}c|>{\centering}c>{\centering}c>{\centering}c>{\centering}c>{\centering}c|>{\centering}m{0.09\linewidth}>{\centering\arraybackslash}m{0.09\linewidth}}
    \toprule
    Hyperparameter & POWER & GAS & HEPMASS & MINIBOONE & BSDS300 & MNIST & Fashion\\
    \midrule\midrule
 & 6 & 8 & 8 & 8 & 8 & 8\ ()\ 4\ () & 8\ ()\ 4\ ()\\
     & (0.9,0.9) & (0.9,0.9) & (0.95, 0.99) & (0.95, 0.999) & (0.95, 0.95) & (0.9, 0.99) & (0.9, 0.99)\\
     & 2 & 1 & 1 & 2 & 5 & 1 & 1\\
    \bottomrule
  \end{tabular}
  \vskip -0.1in
\end{table*}

\begin{table*}[htb]
  \caption{The architectures of SIG for modeling different image datasets in Section \ref{subsec:samples}. The architecture is reported in the format of , where  is the side length of the patch,  is the depth of the patch,  is the number of marginal transformations per patch, and  is the number of iterations for that patch size. MNIST and Fashion-MNIST share the same architecture.}
  \label{tab:hyper_SIG}
  \vskip 0.15in
  \centering
  \begin{tabular}{>{\centering}c|>{\centering}c>{\centering}c>{\centering\arraybackslash}c}
    \toprule
    & MNIST / Fashion-MNIST & CIFAR-10 & CelebA\\ 
    \midrule\midrule
     \multirow{16}{*}{architecture} &  &  & \\
    &  &  & \\
    &  &  & \\
    &  &  & \\
    &  &  & \\
    &  &  & \\
    &  &  & \\
    &  &  & \\
    & &  & \\
    & &  & \\
    & &  & \\
    & &  & \\
    & &  & \\
    & &  & \\
    & &  & \\
    & &  & \\
    \midrule
    Total number of iterations  & 800 & 2500 & 2500\\
    \bottomrule
  \end{tabular}
  \vskip -0.1in
\end{table*}

The hyperparameters of GIS include the number of axes per iteration , the regularization , and the KDE kernel width factor . We have two different  values: , where  regularizes the rational quadratic splines, and  regularizes the linear extrapolations. The KDE kernel width  is determined by the Scott's rule \citep{scott2015multivariate}:

where  is the number of training data, and  is the standard deviation of the data marginalized distribution.

The hyperparameters for density-estimation results in Table \ref{tab:density} are shown in Table \ref{tab:hyper_GIS}.  is determined by . For BSDS300 we first whiten the data before applying GIS. For high dimensional image datasets MNIST and Fashion-MNIST, we add patch-based iterations with patch size  and  alternately. Logit transformation is used as data preprocessing. For all of the datasets, we keep adding iterations until the validation  stops improving.

For density estimation of small datasets, we use the following hyperparameter choices: , ,\\ . The results shown in Figure \ref{fig:density} are averaged over 5 different realizations of training sets.

The hyperparameters of SIG include the number of axes per iteration , and the patch size for each iteration, if the patch-based approach is adopted. We show the SIG hyperparameters for modeling image datasets in Table \ref{tab:hyper_SIG}. As discussed in Section \ref{subsec:patch}, the basic idea of setting the architecture is to start from the entire image, and then gradually decrease the patch size until . We set  or , depending on the datasets and the depth of the patch. For each patch size we add  or  iterations.

For OOD results in Section \ref{subsec:ood}, we train SIG and GIS on Fashion-MNIST with . GIS is trained with  and  (the results are insensitive to all these hyperparameter choices). We do not use logit transformation preprocessing, as it overamplifies the importance of pixels with low variance. The number of iterations are determined by optimizing the validation . For SIG, which cannot produce good , 
the results shown in Table \ref{tab:auroc} use 100 iterations, but we verify they do not depend on this choice and are stable up to thousands of iterations. 




\end{document}