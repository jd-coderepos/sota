\definecolor{ForestGreen}{rgb}{0.0, 0.66, 0.47}
\definecolor{RubineRed}{rgb}{1.0, 0.0, 0.31}

\newcommand{\green}[1]{{\textcolor{ForestGreen}{{#1}}}}
\newcommand{\red}[1]{{\textcolor{RubineRed}{{#1}}}}

\vspace{-1mm}
\section{Experiments}\label{sec:experiments}
We present the experiment results on benchmark datasets for EM:
the ER Benchmark datasets~\cite{kopcke2010evaluation},
the Magellan datasets~\cite{Konda:2016:Magellan} and the WDC product data corpus~\cite{Primpeli:2019:WDC}.
\system\ achieves new SOTA results on all these datasets and outperforms the previous best results
by up to \rev{31\%} in F1 score.
The results show that \system\ is more robust to dirty data and performs well when the training set is small.
\system\ is also more label-efficient as it achieves the previous SOTA results using 
only 1/2 or less of the training data across multiple subsets of the WDC corpus.
Our ablation analysis shows that 
(1) using pre-trained LMs contributes to over 50\% of \system's performance gain and
(2) all 3 optimizations, domain knowledge (DK), summarization (SU) and data augmentation (DA),
are effective.
For example, SU improves the performance on a text-heavy dataset by \rev{52\%},
DK leads to \rev{1.2\%} average improvement on the ER-Magellan datasets and
DA improves on the WDC datasets by 2.53\% on average. 
\rev{In addition, we show in Appendix \ref{sec:time} that although \system\ leverages deeper
neural nets, its training and prediction time is comparable to the SOTA EM systems.}





\vspace{-1mm}
\subsection{Benchmark datasets}

We experimented with all the 13 publicly available datasets used for evaluating 
DeepMatcher~\cite{Mudgal:2018:DeepMatcher}. These datasets
are from the ER Benchmark datasets~\cite{kopcke2010evaluation} and 
the Magellan data repository~\cite{magellandata}.
We summarize the datasets in Table \ref{tab:magellan-dataset}
and refer to them as ER-Magellan.
These datasets are for training and evaluating matching models for various domains
including products, publications, and businesses.
Each dataset consists of candidate pairs from two structured tables of entity records of the same schema. 
The pairs are sampled from the results of blocking and manually labeled.
The positive rate (i.e., the ratio of matched pairs) ranges from 9.4\% (Walmart-Amazon) to 25\% (Company).
The number of attributes ranges from 1 to 8.

Among the datasets, the Abt-Buy and Company datasets are text-heavy meaning that
at least one attributes contain long text. 
Also, following \cite{Mudgal:2018:DeepMatcher},
we use the dirty version of the DBLP-ACM, DBLP-Scholar, iTunes-Amazon, and Walmart-Amazon datasets
to measure the robustness of the models against noise. 
These datasets are generated from the clean version by
randomly emptying attributes and appending their values
to another randomly selected attribute.

Each dataset is split into the training, validation, and test sets using the ratio of \textsf{3:1:1}.
\rev{The same split of the datasets is also used in the evaluation of other EM 
solutions~\cite{Fu:2019:End2End,Kasai:2019:LowResourceER,Mudgal:2018:DeepMatcher}.}
We list the size of each dataset in Table \ref{tab:magellan}.

\begin{table}[t]
\small
    \centering
    \caption{\small The 13 datasets divided into 4 categories of domains. 
    The datasets marked with $\dag$ are text-heavy (Textual).
    Each dataset with $*$ has an additional dirty version to test the models' robustness against
    noisy data. }
    \vspace{-3mm}
    \label{tab:magellan-dataset}
\begin{tabular}{cc}
\toprule
\textbf{Datasets}                              & \textbf{Domains}                \\ \midrule
Amazon-Google, Walmart-Amazon$^*$         & software / electronics \\
Abt-Buy$^\dag$, Beer                         & product                \\
DBLP-ACM*, DBLP-Scholar*, iTunes-Amazon* & citation / music       \\
Company$^\dag$, Fodors-Zagats                & company / restaurant   \\ \bottomrule
\end{tabular}
\vspace{-3mm}\end{table}




The WDC product data corpus~\cite{Primpeli:2019:WDC} contains 26 million product offers and descriptions collected from e-commerce 
websites~\cite{wdc}. The goal is to find product offer pairs that refer to the same product.
To evaluate the accuracy of product matchers, the dataset provides 4,400 manually created 
golden labels of offer pairs from 4 categories: computers, cameras, watches, and shoes.
Each category has a fixed number of 300 positive and 800 negative pairs.
For training, the dataset provides for each category 
pairs that share the same product ID such as GTINs or MPNs mined from the product's webpage.
The negative examples are created by selecting pairs that have high textual similarity but different IDs.
These labels are further reduced to different sizes to test the models' label efficiency.
We summarize the different subsets in Table \ref{tab:wdc-dataset}.
We refer to these subsets as the WDC datasets.


\setlength{\tabcolsep}{4.5pt}
\begin{table}[!ht]
\vspace{-3mm}
\small
\centering
\caption{\small Different subsets of the WDC product data corpus. Each subset (except Test) 
is split into a training set and a validation set with a ratio of \textsf{4:1}
\rev{according to the dataset provider~\protect\cite{Primpeli:2019:WDC}}. 
The last column shows the positive rate (\%POS) of each category in the xLarge set.
The positive rate on the test set is 27.27\% for all the categories.} \label{tab:wdc-dataset}
\vspace{-3mm}
\begin{tabular}{ccccccc}\toprule
\textbf{Categories} & \textbf{Test} & \textbf{Small}   & \textbf{Medium}  & \textbf{Large}   & \textbf{xLarge}  & \textbf{\%POS} \\ \midrule
Computers & 1,100 & 2,834 & 8,094  & 33,359  & 68,461  & 14.15\% \\
Cameras   & 1,100 & 1,886 & 5,255  & 20,036  & 42,277  & 16.98\% \\
Watches   & 1,100 & 2,255 & 6,413  & 27,027  & 61,569  & 15.05\% \\
Shoes     & 1,100 & 2,063 & 5,805  & 22,989  & 42,429  & 9.76\%  \\
All       & 4,400 & 9,038 & 25,567 & 103,411 & 214,736 & 14.10\% \\ \bottomrule
\end{tabular}
\vspace{-3mm}
\end{table}

\rev{
Each entry in this dataset has 4 attributes: \textsf{title}, \textsf{description}, 
\textsf{brand}, and \textsf{specTable}.
Following the setting in \cite{Primpeli:2019:WDC} for DeepMatcher, 
we allow \system\ to use any subsets of attributes 
to determine the best combination.
We found in our experiments that \system\ achieves the best performance when it uses only the 
\textsf{title} attribute. We provide further justification of this choice in Appendix \ref{sec:wdc}.
}

\subsection{Implementation and experimental setup}\label{sec:setup}

We implemented \system\ in PyTorch~\cite{paszke2019pytorch} and 
the Transformers library~\cite{wolf2019transformers}.
\rev{
We currently support 4 pre-trained models:
DistilBERT~\cite{sanh2019distilbert},
BERT~\cite{Devlin:2019:BERT}, 
RoBERTa~\cite{liu2019roberta}, and XLNet~\cite{yang2019xlnet}.
We use the base uncased variant of each model in all our experiments.
We further apply the half-precision floating-point (fp16) optimization 
to accelerate the training and prediction speed.}
In all the experiments, we fix the max sequence length to be 256 and the learning rate to be 3e-5 
\rev{with a linearly decreasing learning rate schedule}.
The batch size is 32 if MixDA is used and 64 otherwise.
The training process runs a fixed number of epochs 
(10, 15, or 40 depending on the dataset size) and returns
the checkpoint with the highest F1 score on the validation set.
We conducted all experiments on a \textsf{p3.8xlarge} AWS EC2 machine 
with 4 V100 GPUs (1 GPU per run). 

\smallskip
\noindent
\textbf{Compared methods. } We compare \system\ with the SOTA EM solution DeepMatcher.
\rev{We also consider other baseline methods
including Magellan~\cite{Konda:2016:Magellan}, DeepER~\cite{Ebraheem:2018:DeepER},
and follow-up works of DeepMatcher~\cite{Fu:2019:End2End,Kasai:2019:LowResourceER}.}
We also compare with variants of \system\ without the data augmentation (DA) and/or domain knowledge (DK) 
optimization to evaluate the effectiveness of each component.
We summarize these methods below. We report the average F1 of 5 repeated runs in all the settings.
\vspace{-1mm}
\begin{itemize}\itemsep=0pt\parskip=0.5pt
\item \textbf{DeepMatcher: } DeepMatcher~\cite{Mudgal:2018:DeepMatcher} is the SOTA matching solution. 
Compared to \system, DeepMatcher customizes the RNN architecture to
aggregate the attribute values, then compares/aligns the aggregated representations of 
the attributes. DeepMatcher leverages FastText~\cite{fasttext} to train the word embeddings.
When reporting DeepMatcher's F1 scores, we use the numbers in \cite{Mudgal:2018:DeepMatcher}
for the ER-Magellan datasets and numbers in \cite{Primpeli:2019:WDC} for the WDC datasets.
We also reproduced those results using the open-sourced implementation.
\item \textbf{DeepMatcher+: } Follow-up work \cite{Kasai:2019:LowResourceER} slightly outperforms DeepMatcher in the DBLP-ACM dataset
and \cite{Fu:2019:End2End} achieves better F1 in the Walmart-Amazon and Amazon-Google datasets.
According to \cite{Mudgal:2018:DeepMatcher}, the Magellan system 
(\cite{Konda:2016:Magellan}, based on classical ML models) outperforms DeepMatcher in 
the Beer and iTunes-Amazon datasets. \rev{We also implemented and ran DeepER~\cite{Ebraheem:2018:DeepER}, 
which is another RNN-based EM solution.}
We denote by DeepMatcher+ (or simply DM+)
the best F1 scores among DeepMatcher and these works aforementioned.
\rev{We summarize in Appendix \ref{sec:breakdown} the implementation details 
and performance of each method.}
\item \textbf{\system: } This is the full version of our system with all 3 optimizations,
domain knowledge (DK), TF-IDF summarization (SU), and data augmentation (DA) turned on. See the details below.
\item \textbf{\system (DA): } This version only turns on the DA (with MixDA)
and SU but does not have the DK optimization.
We apply one of the span-level or attribute-level DA operators listed in Table \ref{tab:da} with
the entry\_swap operator. 
We compare the different combinations and report the best one.
Following \cite{miao2020snippext}, we apply MixDA with the interpolation parameter 
$\lambda$ sampled from a Beta distribution $\text{Beta}(0.8, 0.8)$.
\item \textbf{\system (DK): } With only the DK and SU optimizations on, this version
of \system\ is expected to have lower F1 scores but train much faster.
We apply the span-typing to datasets of each domain according to Table \ref{tab:spans} and 
apply the span-normalization on the number spans.
\item \textbf{Baseline: } This base form of \system\ corresponds
simply to fine-tuning a pre-trained LM on the EM task. We did not apply any optimizations on the baseline.
\rev{
For each ER-Magellan dataset, we tune the LM for the baseline and
found that RoBERTa generally achieves the best performance. Thus, we use
RoBERTa in the other 3 \system\ variants (\system, \system (DA), and \system (DK)) by default
across all datasets. The Company dataset is the only exception, where we found that the BERT model performs the best.
For the WDC benchmark, since the training sets are large,
we use DistilBERT across all settings for faster training. }
\end{itemize}
There is a concurrent work~\cite{brunner2020entity}, which also applies pre-trained LM to the entity matching
problem. The proposed method is similar to the baseline method above, but due to the difference in the
evaluation methods (\cite{brunner2020entity} reports the best epoch on the test set, instead of the validation set),
the reported results in \cite{brunner2020entity} is not directly comparable.
We summarize in Appendix \ref{sec:concurrent} the difference between \system\ and \cite{brunner2020entity}
and explain why the reported results are different.

\vspace{-1mm}
\subsection{Main results}

\iffalse Table \ref{tab:magellan} shows the results on the ER-Magellan datasets.
Overall, \system\ achieves significantly higher F1 scores than the SOTA results (DeepMatcher+).
\system\ outperforms DeepMatcher+ in 10/13 cases and by up to 25\% (Dirty, Walmart-Amazon).
On the 3 cases that \system\ performs slightly worse than DeepMatcher+,
as we show in Table \ref{tab:albert}, these small gaps can be filled
by replacing DistilBERT with larger pre-trained LMs such as BERT or ALBERT.
\fi 

Table~\ref{tab:magellan} shows the results of the ER-Magellan datasets. Overall, \system{} (with optimizations) achieves significantly higher F1 scores than the SOTA results (DM+). \system{} without optimizations (i.e., the baseline) achieves comparable results with DM+.
\system\ outperforms DM+ in  \rev{all 13} cases and by up to \rev{31\%} (Dirty, Walmart-Amazon) while the baseline outperforms DM+ in \rev{12/13 cases except for the Company dataset with long text}.


In addition, we found that \system\ is better at datasets with small training sets.
Particularly, the average improvement on the 7 smallest datasets is \rev{15.6\%} vs. \rev{1.48\%} on average on the rest of datasets. 
\system\ is also more robust against data noise than DM+. In the 4 dirty datasets,
the performance degradation of \system\ is only \rev{0.57} on average
while the performance of DM+ degrades by 8.21.
These two properties make \system\ more attractive in practical EM settings.

\rev{
Moreover, in Appendix \ref{sec:label_efficiency}, we show an evaluation of \system's
label efficiency on 5 of the ER-Magellan medium-size datasets. In 4/5 cases, 
when trained on less than 20\% of the original training data,
\system\ is able to achieve close or even better performance than DM+ when the full
training sets are in use.}

\system\ also achieves promising results on the WDC datasets (Table \ref{tab:wdc}).
\system\ achieves the highest F1 score of 94.08 when using all the 215k training data, outperforming
the previous best result by 3.92. Similar to what we found in the ER-Magellan datasets,
the improvements are higher on settings with fewer training examples (to the right of Table \ref{tab:wdc}).
The results also show that \system\ is more \emph{label efficient} than DeepMatcher.
For example, when using only 1/2 of the data (Large), \system\ already outperforms DeepMatcher with all the training data (xLarge) by 2.89 in All. When using only 1/8 of the data (Medium), the performance is within 1\% close to DeepMatcher's F1 when 1/2 of the data (Large) is in use.
The only exception is the shoes category. This may be caused by the large gap of the positive label ratios 
between the training set and the test set (9.76\% vs. 27.27\% according to Table \ref{tab:wdc-dataset}).


\setlength{\tabcolsep}{2.3pt}
\begin{table}[!ht]
\small
\centering
\caption{\small F1 scores on the ER-Magellan EM datasets. 
The numbers of DeepMatcher+ (DM+) are the highest available found in \protect\cite{Fu:2019:End2End,Kasai:2019:LowResourceER,Mudgal:2018:DeepMatcher} \rev{or re-produced by us}.
}\label{tab:magellan}
\vspace{-3mm}
\resizebox{0.47\textwidth}{!}{ 
\begin{tabular}{ccccccc}
\toprule
Datasets           & DM+ & 
\system & 
\begin{tabular}{c}
\system     \\
(DA)
\end{tabular}
 & \begin{tabular}{c}
\system     \\
(DK)
\end{tabular}
&
Baseline & Size    \\ \midrule
Structured         &              &                &            & &          &         \\
Amazon-Google      & 70.7         & 75.58 \green{(+4.88)}       & 75.08  & 74.67      & 74.10    & 11,460  \\
Beer               & 78.8         & 94.37 \green{(+15.57)}       & 87.21  & 90.46      & 84.59    & 450     \\
DBLP-ACM           & 98.45         & 98.99 \green{(+0.54)}       &  99.17 & 99.10      & 98.96    & 12,363  \\
DBLP-Google & 94.7         & 95.6 \green{(+0.9)}    &   95.73   & 95.80      & 95.84    & 28,707  \\
Fodors-Zagats      & 100          & 100.00 \green{(+0.0)}     &  100.00   & 100.00      & 98.14    & 946     \\
iTunes-Amazon      & 91.2         & 97.06 \green{(+5.86)}    &  97.40    & 97.80      & 92.28    & 539     \\
Walmart-Amazon     & 73.6         & 86.76 \green{(+13.16)}    &  85.50    & 83.73      & 85.81    & 10,242  \\ \midrule
Dirty              &              &                &          &  &          &         \\
DBLP-ACM           & 98.1         & 99.03 \green{(+0.93)}     & 98.94    & 99.08      & 98.92    & 12,363  \\
DBLP-Google & 93.8         & 95.75 \green{(+1.95)}  &  95.47      & 95.57      & 95.44    & 28,707  \\
iTunes-Amazon      & 79.4         & 95.65 \green{(+16.25)} &  95.29       & 94.48      & 92.92    & 539     \\
Walmart-Amazon     & 53.8         & 85.69 \green{(+31.89)} &  85.49  & 80.67      & 82.56    & 10,242  \\ \midrule
Textual            &              &                &    &         &          &         \\ 
Abt-Buy            & 62.8         & 89.33 \green{(+26.53)} &  89.79        & 81.69      & 88.85    & 9,575   \\
Company            & 92.7         & 93.85 \green{(+1.15)} &    93.69     & 93.15      & 41.00    & 112,632 \\ \bottomrule
\end{tabular}}
\vspace{-2mm}
\end{table}





\setlength{\tabcolsep}{3.4pt}
\begin{table}[!ht]
\centering
\small
\caption{\small F1 scores on the WDC product matching datasets. The numbers for DeepMatcher (DM) are taken
from~\protect\cite{Primpeli:2019:WDC}.}\label{tab:wdc}
\vspace{-2mm}
\begin{tabular}{ccccccccc} \toprule
Size                       & \multicolumn{2}{c}{xLarge (1/1)} & \multicolumn{2}{c}{Large (1/2)} & \multicolumn{2}{c}{Medium (1/8)} & \multicolumn{2}{c}{Small (1/20)} \\
Methods                    & DM             & Ditto         & DM             & Ditto          & DM              & Ditto          & DM              & Ditto          \\ \midrule
\multirow{2}{*}{Computers} & 90.80          & 95.45         & 89.55          & 91.70          & 77.82           & 88.62          & 70.55           & 80.76          \\
                           & \multicolumn{2}{c}{\green{+4.65}}       & \multicolumn{2}{c}{\green{+2.15}}        & \multicolumn{2}{c}{\green{+10.80}}        & \multicolumn{2}{c}{\green{+10.21}}        \\ \midrule
\multirow{2}{*}{Cameras}   & 89.21          & 93.78         & 87.19          & 91.23          & 76.53           & 88.09          & 68.59           & 80.89          \\
                           & \multicolumn{2}{c}{\green{+4.57}}       & \multicolumn{2}{c}{\green{+4.04}}        & \multicolumn{2}{c}{\green{+11.56}}        & \multicolumn{2}{c}{\green{+12.30}}        \\ \midrule
\multirow{2}{*}{Watches}   & 93.45          & 96.53         & 91.28          & 95.69          & 79.31           & 91.12          & 66.32           & 85.12          \\
                           & \multicolumn{2}{c}{\green{+3.08}}       & \multicolumn{2}{c}{\green{+4.41}}        & \multicolumn{2}{c}{\green{+11.81}}        & \multicolumn{2}{c}{\green{+18.80}}        \\ \midrule
\multirow{2}{*}{Shoes}     & 92.61          & 90.11         & 90.39          & 88.07          & 79.48           & 82.66          & 73.86           & 75.89          \\
                           & \multicolumn{2}{c}{\red{-2.50}}      & \multicolumn{2}{c}{\red{-2.32}}       & \multicolumn{2}{c}{\green{+3.18}}         & \multicolumn{2}{c}{\green{+2.03}}         \\ \midrule
\multirow{2}{*}{All}       & 90.16          & 94.08         & 89.24          & 93.05          & 79.94           & 88.61          & 76.34           & 84.36          \\
                           & \multicolumn{2}{c}{\green{+3.92}}       & \multicolumn{2}{c}{\green{+3.81}}        & \multicolumn{2}{c}{\green{+8.67}}         & \multicolumn{2}{c}{\green{+8.02}} \\ \bottomrule   
\end{tabular}
\vspace{-5mm}
\end{table}

\begin{figure*}[]
    \centering
    \includegraphics[width=0.98\textwidth]{wdc.pdf}
    \vspace{-3mm}
    \caption{\small F1 scores on the WDC datasets of different versions of \system. \textbf{DM}: DeepMatcher.}
    \label{fig:wdc}
    \vspace{-3mm}
\end{figure*}













\vspace{-1mm}
\subsection{Ablation study}


Next, we analyze the effectiveness of each component (i.e., LM, SU, DK, and DA)
by comparing \system\ with its variants without these optimizations.
The results are shown in Table \ref{tab:magellan} and Figure \ref{fig:wdc}.

The use of a pre-trained LM contributes to a large portion of the performance gain.
In the ER-Magellan datasets (excluding Company), the average improvement of the baseline 
compared to DeepMatcher+ is \rev{7.75}, which accounts for \rev{78.5\%} of the improvement of the full \system\ (\rev{9.87}).
While DeepMatcher+ and the baseline \system\ (essentially fine-tuning DistilBERT) 
are comparable on the Structured datasets, the baseline performs much better on all the Dirty datasets
and the Abt-Buy dataset. This confirms our intuition that the language understanding
capability is a key advantage of \system\ over existing EM solutions.
The Company dataset is a special case because the length of the company articles (3,123 words on average)
is much greater than the max sequence length of 256. 
The SU optimization increases the F1 score of this dataset from 41\% to over \rev{93\%}.
In the WDC datasets, across the 20 settings, LM contributes to 3.41 F1 improvement on average, which explains 55.3\% of improvement of the full \system\ (6.16).

The DK optimization is more effective on the ER-Magellan datasets.
Compared to the baseline, the improvement of \system(DK) is \rev{1.08} on average and 
is up to \rev{5.88} on the Beer dataset
while the improvement is only 0.22 on average on the WDC datasets.
We inspected the span-typing output and found that only 66.2\% of entry pairs have spans of the same type. This is caused by the current NER module not extracting product-related spans with the correct types. We expect DK to be more effective if we use
an NER model trained on the product domain.

DA is effective on both datasets and more significantly on the WDC datasets.
The average F1 score of the full \system\ improves upon \system(DK) (without DA) by \rev{1.39} and 2.53
respectively in the two datasets.
In the WDC datasets, we found that the span\_del operator always performs the best
while the best operators are diverse in the ER-Magellan datasets. We list the best operator for each dataset in Table \ref{tab:bestda}. We note that there is a large space
of tuning these operators
(e.g., the MixDA interpolation parameter, maximal span length, etc.)
and new operators to further improve the performance.


\begin{table}[!htb]
\vspace{-2mm}
\small
\centering
\caption{\small Datasets that each DA operator achieves the best performance.
The suffix (S)/(D) and (Both) denote the clean/dirty version of the dataset or both of them. All operators are applied with the entry\_swap operator.}
 \vspace{-2mm}
\label{tab:bestda}
\begin{tabular}{cc}
\toprule
\textbf{Operator} & \textbf{Datasets} \\ \midrule
span\_shuffle & DBLP-ACM (Both), DBLP-Google (Both), Abt-Buy      \\
span\_del     & Walmart-Amazon(D), Company, all of WDC          \\
attr\_del     & Beer, iTunes-Amazon(S), Walmart-Amazon(S) \\
attr\_shuffle & Fodors-Zagats, iTunes-Amazon(D) \\ \bottomrule
\end{tabular}
\vspace{-2mm}
\end{table}











%
