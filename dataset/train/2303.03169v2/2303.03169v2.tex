\documentclass{article} \usepackage{iclr2023_conference,times}
\usepackage{amsmath,amsthm,amssymb,amsfonts,bbm,bm,math}
\usepackage{enumitem}
\usepackage[bookmarks=false,hidelinks]{hyperref}
\usepackage[small,bf]{caption}
\usepackage{xspace}
\usepackage{longtable}
\usepackage{bbm}
\usepackage{natbib}
\usepackage{array}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{array}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{wrapfig}

\newcommand\citesingle[1]{\AtNextCite{\defcounter{maxnames}{1}}\citeauthor{#1}, \citeyear{#1}}


\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\SC}{\mathbf{S}}
\newcommand{\cvx}{\mathcal{F}}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\RH}{\field{RH}_\infty}
\newcommand{\RHmin}{\field{RH}_\infty^-}   
\newcommand{\RL}{\field{RL}_\infty}
\newcommand{\CF}{\field{C}}
\newcommand{\F}{\field{F}}
\newcommand{\Ha}{\field{H}_2}
\newcommand{\Haperp}{\field{H}_2^\perp}

\newcommand{\bmtx}{\begin{bmatrix}}
\newcommand{\emtx}{\end{bmatrix}}
\newcommand{\bsmtx}{\left[ \begin{smallmatrix}} 
\newcommand{\esmtx}{\end{smallmatrix} \right]} 
\newcommand{\bmatarray}[1]{\left[\begin{array}{#1}}
\newcommand{\ematarray}{\end{array}\right]} 

\newcommand{\Dt}{\mathcal{D}_\tau}
\newcommand{\St}{\mathcal{S}_\tau}
\newcommand{\A}{\mathcal{T}}
\newcommand{\D}{\mathbf{D}}

\newcommand{\cin}{{c_{\text{in}}}}
\newcommand{\cout}{{c_{\text{out}}}}


\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\regret}{regret}
\newcommand{\0}{\mathbf{0} }

\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

\newcommand\EnumPrefix{}

\newlist{senenum}{enumerate}{10}
\setlist[senenum]{label=\arabic*.,ref=\EnumPrefix,leftmargin=*}




\def \endprf{\hfill {\vrule height6pt width6pt depth0pt}\medskip}


\newcommand{\note}[1]{{\color{red}\bf [{\em Note:} #1]}}
\newcommand{\iprod}[2]{\left\langle #1 , #2 \right\rangle}

\newcommand{\SGD}{SGM}
\newcommand{\SGM}{SGM}
\newcommand{\sgd}{{stochastic gradient descent} }
\newcommand{\sgm}{{stochastic gradient methods} }

\gdef\issubmit{1}
\newcommand{\submit}[2]{\ifdefined\issubmit{#1}\else{#2} \fi }

\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}

\newcommand{\bin}[1]{\textbf{{\color{blue}Bin: #1}}}
\newcommand{\alex}[1]{\textbf{{\color{orange}Alex: #1}}}
\newcommand{\blaise}[1]{\textbf{{\color{purple}Blaise: #1}}}


\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}


\title{A Unified Algebraic Perspective on \\ Lipschitz Neural Networks}



\author{Alexandre Araujo$^{*1}$, Aaron Havens$^{*2}$, Blaise Delattre$^{3,4}$, Alexandre Allauzen$^{3,5}$ and Bin Hu$^2$ \\[0.2cm]
$^1$ INRIA, Ecole Normale Supérieure, CNRS, PSL University, Paris, France \\
$^2$ CSL \& ECE, University of Illinois Urbana-Champaign, IL, USA \\
$^3$ Miles Team, LAMSADE, Université Paris-Dauphine, PSL University, Paris, France \\
$^4$ Foxstream, Vaulx-en-Velin, France \\
$^5$ ESPCI PSL, Paris, France
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle
\blfootnote{* Equal contribution.}

\begin{abstract}
Important research efforts have focused on the design and training of neural networks with a controlled Lipschitz constant. The goal is to increase and sometimes guarantee the robustness against adversarial attacks. Recent promising techniques draw inspirations from different backgrounds to design 1-Lipschitz neural networks, just to name a few: convex potential layers derive from the discretization of continuous dynamical systems, Almost-Orthogonal-Layer proposes a tailored method for matrix rescaling.  However, it is today important to consider the recent and promising contributions in the field under a common theoretical lens to better design new and improved layers. This paper introduces a novel algebraic perspective unifying various types of 1-Lipschitz neural networks, including the ones previously mentioned, along with methods based on orthogonality and spectral methods. Interestingly, we show that many existing techniques can be derived and generalized via finding analytical solutions of a common semidefinite programming (SDP) condition.  We also prove that AOL biases the scaled weight to the ones which are close to the set of orthogonal matrices in a certain mathematical manner. Moreover, our algebraic condition, combined with the Gershgorin circle theorem, readily leads to new and diverse parameterizations for 1-Lipschitz network layers. Our approach, called SDP-based Lipschitz Layers (SLL), allows us to design non-trivial yet efficient generalization of convex potential layers.  Finally, the comprehensive set of experiments on image classification shows that SLLs outperform previous approaches on  certified robust accuracy. Code is available at \href{https://github.com/araujoalexandre/Lipschitz-SLL-Networks}{\texttt{github.com/araujoalexandre/Lipschitz-SLL-Networks}}. \\[0.2cm]
\emph{(10/26/2023): Erratum is added in Appendix D. This is an updated version that fixes an implementation issue in the previous version. Due to that implementation issue, the original numerical results in our original ICLR paper are not accurate. We elaborate on the issue and provides some fix in Appendix D.
}
\end{abstract}


\section{Introduction}
The robustness of deep neural networks is nowadays a great challenge to establish confidence in their  decisions for real-life applications.  Addressing this challenge requires guarantees on the stability of the prediction, with respect to adversarial attacks. In this context, the Lipschitz constant of neural networks is a key property at the core of many recent advances. Along with the margin of the classifier, this property allows us to certify the robustness against worst-case adversarial perturbations.  This certification is based on a sphere of stability within which the decision remains the same for any perturbation inside the sphere~\citep{tsuzuku2018lipschitz}.

The design of 1-Lipschitz layers provides a successful approach to enforce this property for the whole neural network. For this purpose, many different techniques have been devised such as spectral normalization~\citep{miyato2018spectral,farnia2018generalizable}, orthogonal parameterization~\citep{trockman2021orthogonalizing,li2019preventing,skew2021sahil,yu2022constructing,xulot2022}, Convex Potential Layers (CPL)~\citep{meunier2022dynamical}, and Almost-Orthogonal-Layers (AOL)~\citep{prach2022almost}. While all these techniques share the same goal, their motivations, and derivations can greatly differ, delivering different solutions. Nevertheless, their raw experimental comparison fails to really gain insight into their peculiar performance, soundness, and in the end their possible complementarity. Therefore a question acts as a barrier for an in-depth analysis and future development:
\begin{center}
\textbf{\textit{Are there common principles underlying the developments of 1-Lipschitz Layers?}}
\end{center}
In this paper, we propose a novel perspective to answer this question
based on a unified Semidefinite Programming (SDP) approach.
We introduce a common algebraic condition underlying various types of
methods like spectral normalization, orthogonality-based methods, AOL,
and CPL. Our key insight is that this condition can be formulated as a
unifying and simple SDP problem, and that the development of 1-Lipschitz
architectures systematically arise by finding ``analytical solutions"
of this SDP. Our main contributions are summarized as follows.
\begin{itemize}[leftmargin=10pt,parsep=0pt,topsep=0pt,itemsep=2pt]
    \item We provide a unifying algebraic perspective for 1-Lipschitz network layers by showing that existing techniques such as spectral normalization, orthogonal parameterization, AOL, and CPL can all be recast as a solution of the same simple SDP condition (Theorem \ref{thm:main1} and related discussions). Consequently, any new analytical solutions of our proposed SDP condition will immediately lead to new 1-Lipschitz network structures. 
    \item Built upon the above algebraic viewpoint, we give a rigorous mathematical interpretation for AOL explaining how this method promotes ``almost orthogonality" in training (Theorem \ref{thm:main2}).
    \item Based on our SDPs, a new family of 1-Lipschitz network structures termed as SDP-based Lipschitz layers (SLL) has been developed. Specifically, we apply the Gershgorin circle theorem to obtain some new SDP solutions, leading to non-trivial extensions of CPL (Theorem \ref{thm:main3}). We derive new SDP conditions to characterize SLL in a very general form (Theorem \ref{thm1}).
    \item Finally, we show, by a comprehensive set of experiments, that our new SDP-based Lipschitz layers outperform previous approaches on certified robust accuracy.
\end{itemize}
Our work is inspired by \cite{fazlyab2019efficient} that develops SDP conditions for numerical estimation of Lipschitz constants of given neural networks. A main difference is that we focus on ``analytical SDP solutions" which can be used to characterize 1-Lipschitz network structures. 


\section{Related Work}

In recent years, certified methods have been central to the development of trustworthy machine learning and especially for deep learning. 
\textit{Randomized Smoothing}~\citep{cohen2019certified,salman2019provably} is one of the first defenses to offer provable robustness guarantees. The method simply extends a given classifier by the smart introduction of  random noise to enhance the robustness of the classifier.
Although this method offers an interesting level of certified robustness, it suffers from important downsides such as the high computational cost of inference and some impossibility results from information-theory perspective~\citep{yang2020randomized,kumar2020curse}.

Another approach to certify the robustness of a classifier is to control its Lipschitz constant \citep{hein2017formal,tsuzuku2018lipschitz}. The main idea is to derive a certified radius in the feature space by upper bounding the margin of the classifier. See Proposition~1 of~\cite{tsuzuku2018lipschitz} for more details. This radius, along with the Lipschitz constant of the network can certify the robustness. 
In order to reduce the Lipschitz constant and have a non-trivial certified accuracy, \cite{tsuzuku2018lipschitz} and \cite{leino2021globally} both upper bound the margin via computing a bound on the global Lipschitz constant, however, these bounds have proved to be loose.
Instead of upper bounding the global Lipschitz constant, \cite{huang2021training} leverages {\em local} information to get tighter bound on the Lipschitz constant.  
On the other hand, other works, instead of upper bounding the local or global Lipschitz, devised neural networks architecture that are provably 1-Lipschitz. 
One of the first approaches in this direction consists of  normalizing each layer with its spectral norm~\citep{miyato2018spectral,farnia2018generalizable}. Each layer is, by construction,  1-Lipschitz.
Later, a body of research replaces the normalized weight matrix by an orthogonal matrix. It improves  upon the spectral normalization method by  adding  the gradient preservation~\citep{li2019preventing,trockman2021orthogonalizing,skew2021sahil,yu2022constructing,xulot2022}.
These methods constrain the parameters by orthogonality during training.
Specifically, the Cayley transform can be used to constrain the weights~\citep{trockman2021orthogonalizing} and,  
in a similar fashion, SOC~\citep{skew2021sahil}  parameterizes their layers with the exponential of a skew symmetric matrix making it orthogonal.
To reduce cost, \cite{trockman2021orthogonalizing}, \cite{yu2022constructing}, and \cite{xulot2022} orthogonalize their convolutional kernel in the Fourier domain.

More recently, a work by~\cite{meunier2022dynamical} has studied Lipschitz networks from a dynamical system perspective. Starting from the continuous view of a residual network, they showed that the parameterization with the Cayley transform \citep{trockman2021orthogonalizing} and SOC \citep{skew2021sahil} correspond respectively to two specific discretization schemes of the continuous flow. 
Furthermore, a new layer is derived from convex potential flows to ensure the 1-Lipschitz~property\footnote{We reverse the transposition from the original layer to have a consistent notation in the rest of the article.}:
\begin{equation} \label{eq:cpl}
  z = x-\frac{2}{\norm{W}_2^2} W \sigma(W^\top x+b),
\end{equation}
where $\norm{W}_2$ is the spectral norm of the weight matrix $W$ and $\sigma$ is the ReLU activation function. In general, the training of orthogonal layers can be expensive. The Cayley approach involves a matrix inversion, and the implementation of SOC requires either an SVD or an iterative Taylor expansion. The CPL approach can be more efficient, although the computation of $\norm{W}_2$ is still needed.

A recent work, \textit{Almost-Orthogonal-layer} (AOL) \citep{prach2022almost} came up with a middle ground: a new normalization which makes the layer 1-Lipschitz by favoring orthogonality.  
The fully-connected AOL layer is defined as $z = W D x + b$ where $D$ is a diagonal matrix given by\footnote{For simplicity, we assume all the columns of $W$ have at least one non-zero entry. Then \eqref{eq:DAOL} is well defined.}:
\begin{equation}\label{eq:DAOL}
  D = \textstyle \mathrm{diag}\left( \sum_j | W^\top W |_{ij} \right)^{-\frac12}
\end{equation}
They demonstrated that this layer is 1-Lipschitz and they empirically show that, after training, the Jacobian of the layer (with respect to $x$) is almost orthogonal, hence facilitating the training.

Another source of inspiration is the application of convex programs for robustness certification of neural networks~\citep{wong2018provable,raghunathan2018semidefinite,fazlyab2019efficient,revay2020lipschitz,fazlyab2020safety,wang2022}.
The most relevant work is 
 \cite{fazlyab2019efficient}, which leverages the quadratic constraint approach from control theory \citep{Megretski1997} to~formulate SDPs for estimating  the global Lipschitz constant of neural networks numerically.  It is possible to solve such SDPs numerically for training relatively small Lipschitz networks \citep{pauli2021training}.
However, due to the restrictions of existing SDP solvers, scalability has been one issue when deploying such approaches to deep learning problems with large data sets. 
Our focus is on the design of Lipschitz network structures, and we avoid the scalability issue via solving SDPs analytically.



\section{Background}

\paragraph{Notation.} The $n\times n$ identity matrix and the $n \times n$ zero matrix are denoted as $I_n$ and $0_n$, respectively. The subscripts will be omitted when the dimension is clear from the context. 
 When a matrix $P$ is negative semidefinite (definite), we will use the notation $P\preceq (\prec) 0$.  When a matrix $P$ is positive semidefinite (definite), we will use the notation $P\succeq (\succ) 0$.
Let $e_i$ denote the vector whose $i$-entry is $1$ and all other entries are $0$. Given a collection of scalars $\{a_i\}_{i=1}^n$,  we use the notation $\diag(a_i)$ to denote the $n\times n$ diagonal matrix whose $(i,i)$-th entry is $a_i$. For a matrix $A$, the following notations \(A^\tp\), \( \norm{A}_2 \),  \(\tr(A)\), \(\sigma_{\min}(A)\), \(\norm{A}_F\), and \(\rho(A) \) stand for its transpose, largest singular value, trace, smallest singular value, Frobenius norm, and spectral radius, respectively. 

\paragraph{Lipschitz functions.} A function $f:\R^n\rightarrow \R^m$ is $L$-Lipschitz with respect to the $\ell_2$ norm iff it satisfies $\norm{f(x)-f(y)}\le L \norm{x-y}$ for all $x, y\in \R^n$, where $\norm{\cdot}$ stands for the $\ell_2$ norm.
An important fact is that the robustness of a neural network can be certified based on its Lipschitz constant \citep{tsuzuku2018lipschitz}. 
In this paper, we are interested in the case where $L=1$. Specifically, we consider the training of 1-Lipschitz neural networks. If each layer of a neural network is 1-Lipschitz, then the entire neural network is also 1-Lipschitz. The Lipschitz constant also satisfies the triangle inequality, and hence convex combination will preserve the 1-Lipschitz property.

\paragraph{Matrix cones: Positive semidefiniteness and diagonal dominance.} 
Let $\SC^n$ denote the set of all $n\times n$ real symmetric matrices. Let $\SC_+^n\subset\SC^n$ be the set of all $n\times n$ symmetric positive semidefinite matrices. It is well known that $\SC_+^n$ is a closed-pointed convex cone in $\SC^n$. With the trace inner product, $\SC_+^n$ is also self-dual. Consider two symmetric matrices $A$ and $B$ such that $A\succeq B\in \SC^n$, then we have $A-B\in \SC_+^n$, and $\tr(A-B)$ provides a distance measure between $A$ and $B$. In addition, we have $\norm{A-B}_F\le \tr(A-B)$. 
Finally, the set of all $n\times n$ real symmetric diagonally dominant matrices with non-negative diagonal entries is represented by $\D^n$. It is known that $\D^n$ forms a closed, pointed, full cone \citep{barker1975cones}. Based on the Gershgorin circle theorem \citep{horn2012matrix}, we know $\D^n\subset \SC_+^n$. It is also known that $\D^n$ is smaller than $\SC_+^n$ \citep{barker1975cones}. 
For any $A\in \D^n$, we have $A_{ii}\ge \sum_{j:j\neq i}|A_{ij}|$. 
It is important to require $A_{ii}\ge 0$, and the set of real symmetric diagonally dominant matrices is not  a cone by itself. 



\section{An Algebraic Unification of 1-Lipschitz Layers}

In this section, we present a unified algebraic perspective for various 1-Lipschitz layers (Spectral Normalization, Orthogonalization, AOL, and CPL) via developing a common SDP condition characterizing the Lipschitz property.  Built upon our algebraic viewpoint, we also present a new mathematical interpretation explaining how AOL promotes orthogonality in training.

\subsection{The unifying  Algebraic Condition}
First, we present an algebraic condition which can be used to unify the developments of existing techniques such as SN, AOL, and CPL. Our main theorem is formalized below.

\begin{theorem}\label{thm:main1}
For any weight matrix $W\in \R^{m\times n}$, if there exists a nonsingular diagonal matrix $T$ such that $W^\tp W-T\preceq 0$, then the two following statements hold true. 
\begin{enumerate}
  \item The mapping $g(x)=WT^{-\frac{1}{2}}x+b$  is $1$-Lipschitz.
  \item The mapping $h(x)=x-2W T^{-1} \sigma (W^\tp x+b)$ is $1$-Lipschitz if $\sigma$ is ReLU, $\tanh$ or sigmoid.
\end{enumerate}
\end{theorem}

The proof of the above theorem and some related control-theoretic interpretations are provided in the appendix.
This theorem allows us to design different 1-Lipschitz layers just with various choices of $T$, in two important cases: for a linear transformation with Statement 1, as well as for a residual and non-linear block with Statement 2. Moreover, for any given weight matrix $W$, the condition $W^\tp W\preceq T$ is linear in $T$, and hence can be viewed as an SDP condition with decision variable $T$. 
To emphasize the significance of this theorem, we propose to derive existing methods used for designing 1-Lipschitz layers by choosing specific $T$ for the SDP condition $W^\tp W\preceq T$. The 1-Lipschitz property is then automatically obtained. 


\begin{itemize}
    \item {Spectral Normalization (SN)} corresponds to an almost trivial choice if we notice that $W^\tp W\preceq \norm{W^\tp W}_2 I\preceq \norm{W}_2^2 I$. Hence with $T=\norm{W}_2^2 I$, we build the SN layer $g(x)=WT^{-\frac{1}{2}}x+b=\frac{1}{\norm{W}_2}Wx+b$.
    \item The Orthogonality-based parameterization is obtained by setting $T=I$ and enforcing the equality $W^\tp W=T=I$. Then obviously $g(x)=Wx+b$ is 1-Lipschitz.
    \item AOL formula can be derived by letting  $T=\diag(\sum_{j=1}^n |W^\tp W|_{ij})$. With this choice, we have $T-W^\tp W \in \D^n \subset \SC_+^n$,  hence  $W^\tp W\preceq T$. Then Statement 1 in Theorem~\ref{thm:main1} implies that the AOL layer, written as $g(x)=WT^{-\frac{1}{2}}x+b$, is 1-Lipschitz.\footnote{For ease of exposition, our main paper always assumes that all the columns of $W$ have at least one non-zero entry such that \eqref{eq:DAOL} is well defined. To drop this assumption, we can use a variant of Theorem \ref{thm:main1} which replaces the SDP condition with a bilinear matrix inequality condition. We will discuss this point in the appendix.}
    \item CPL follows the same SN choice $T=\norm{W}_2^2 I$, but with Statement 2 of Theorem \ref{thm:main1}. Hence we derive a different function $h(x)=x-\frac{2}{\norm{W}_2^2}W\sigma(W^\tp x+b)$ which is also 1-Lipschitz.
\end{itemize}

The above discussion illustrates the benefit of expressing all these methods within the same theoretical framework, offering us a new tool to characterize the similarity  between different methods.
For instance, SN and CPL share the same choice of $T=\norm{W}_2^2 I$. The difference between them is which statement is used. Hence CPL can be viewed as the ''residual version`` of SN. Clearly, the residual network structure allows CPL to address the gradient vanishing issue more efficiently than SN.
With the same approach, we can readily infer from our unified algebraic condition what are the  "residual" counterparts for orthogonality-based parameterization and AOL. 
For orthogonality-based parameterization, if we enforce $W^\tp W=T=I$ via methods such as SOC and ECO, then the function $h(x)=x-2W\sigma(W^\tp x+b)$ is 1-Lipschitz (by Statement 2 in Theorem \ref{thm:main1}). Finally,  if we choose $T=\diag\left(\sum_{j=1}^n |W^\tp W|_{ij}\right)$, then the function $h(x)=x-2W\diag\left(\sum_{j=1}^n |W^\tp W|_{ij}\right)^{-1}\sigma(W^\tp x+b)$ is also 1-Lipschitz. Therefore it is straightforward to create  new classes of 1-Lipschitz network structures from existing ones. 

Another important consequence of Theorem \ref{thm:main1} is about new layer development. Any  new nonsingular diagonal solution $T$ for the SDP condition $W^\tp W-T\preceq 0$  immediately leads to new 1-Lipschitz network structures in the form of $g(x)=WT^{-\frac{1}{2}}x+b$ or $h(x)=x-2WT^{-1}\sigma(W^\tp x+b)$. Therefore, the developments of 1-Lipschitz network structures can be reformulated as finding analytical solutions of the matrix inequality $W^\tp W\preceq T$ with nonsingular diagonal $T$. 
As a matter of fact,  the Gershgorin circle theorem can help to improve the existing choices of $T$ in a systematic way. In Section~\ref{sec:ECPL}, we will discuss such new choices of $T$ and related applications to improve CPL. At this point, it is worth noticing that to develop deep Lipschitz networks, it is important to have analytical formulas of $T$. The analytical formula of $T$ will enable a fast computation of  $WT^{-\frac{1}{2}}$ or $WT^{-1}$.  

Theorem \ref{thm:main1} is powerful in building a connection between 1-Lipschitz network layers and the algebraic condition $W^\tp W\preceq T$. Next, we will look closer at this algebraic condition and provide a new mathematical interpretation explaining how AOL generates ``almost orthogonal" weights.  


\begin{remark}
The proof of Statement 2 in Theorem \ref{thm:main1} relies on \cite[Lemma 1]{fazlyab2019efficient}, which requires the activation function $\sigma$ to be slope-restricted on $[0,1]$. Therefore, Statement 2 cannot be applied to the case with $\sigma$ being the GroupSort activation function \citep{anil2019sorting}. In contrast, Statement 1 can be used to build neural networks with any activation functions which are 1-Lipschitz. 
\end{remark}

\subsection{A New Mathematical Interpretation for AOL}

In \cite{prach2022almost}, it is observed that AOL can learn "almost orthogonal" weights and hence overcome the gradient vanishing issue. 
As a matter of fact, 
the choice of $T$ used in AOL is optimal in a specific mathematical sense as formalized with the next theorem. 

\begin{theorem}\label{thm:main2}
Given any $W \in \mathbb{R}^{m \times n}$ which does not have zero columns, define the set $\mathbf{T}=\left\{T: T\,\mbox{is nonsingular diagonal, and}\,\, T-W^\tp W\in \D^n\right\}$. 
Then the choice of $T$ for the AOL method    actually satisfies
\begin{align*}
    T=\diag(\sum_{j=1}^n |W^\tp W|_{ij})=\argmin_{T\in \mathbf{T}} \tr(I-T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}})=\argmin_{T\in \mathbf{T}} \norm{T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}}-I}_F.
\end{align*}
\end{theorem}
We defer the proof for the above result to the appendix.
Here we provide some interpretations for the above result.  Obviously, the quantity $\norm{T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}}-I}_F$ provides a measure for the distance between the scaled weight matrix $WT^{-\frac{1}{2}}$ and the set of $n\times n$ orthogonal matrices. If $\norm{T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}}-I}_F=0$, then the scaled weight $W T^{-\frac{1}{2}}$ is orthogonal. If $\norm{T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}}-I}_F$ is small, it means that  $W T^{-\frac{1}{2}}$ is ``almost orthogonal" and close to the set of orthogonal matrices.  Since we require $W^\tp W-T\preceq 0$, we know that  $I-T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}}$ is a positive semidefinite matrix, and its trace provides an alternative metric quantifying the distance between $WT^{-\frac{1}{2}}$
and the set of orthogonal matrices. Importantly, we have the following inequality:  
\begin{align*}
\norm{T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}}-I}_F\le \tr(I-T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}}).
\end{align*}
If $\tr(I-T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}})$ is small, then $\norm{T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}}-I}_F$ is also small, and $WT^{-\frac{1}{2}}$ is close to the set of orthogonal matrices. Therefore, one interpretation for Theorem \ref{thm:main2} is that among all the nonsingular diagonal scaling matrices $T$ satisfying $T-W^\tp W\in \D^n$, the choice of $T$ used in AOL makes the scaled weight matrix $WT^{-\frac{1}{2}}$ the closest to the set of orthogonal matrices. This provides a new mathematical explanation of how AOL can generate ``almost orthogonal" weights. 


One potential issue for AOL is that $\D^n$ is typically much smaller than $\SC_+^n$, and the condition $T-W^\tp W\in \D^n$ may be too conservative compared to the original condition $T-W^\tp W\in \SC_+^n$ in Theorem \ref{thm:main1}. 
If we denote the set $\hat{\mathbf{T}}=\left\{T: T\,\mbox{is nonsingular diagonal, and}\,\, T-W^\tp W\in \SC_+^n\right\}$, then we  have 
 $\argmin_{T\in \hat{\mathbf{T}}} \tr(I-T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}})\le   \argmin_{T\in \mathbf{T}} \tr(I-T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}})$,
 and 
 $\argmin_{T\in \hat{\mathbf{T}}} \norm{T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}}-I}_F\le  \argmin_{T\in \mathbf{T}} \norm{T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}}-I}_F$.
This leads to interesting alternative choices of $T$ which can further promote orthogonality:
\begin{align}\label{eq:AOLplus}
T=\argmin_{T\in \hat{\mathbf{T}}} \norm{T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}}-I}_F\quad \mbox{or}\quad T=\argmin_{T\in \hat{\mathbf{T}}} \tr(I-T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}})
\end{align}
Although \eqref{eq:AOLplus} may be solved as convex programs on small toy examples, it is not practical to use such choice of $T$ for large-scale problems. It is our hope that our theoretical discussion above will inspire
more future research on developing new practical choices of $T$ for promoting orthogonality.


\section{Extensions of CPL: The Power of Gershgorin circle theorem}
\label{sec:ECPL}

In this section, we extend the original CPL layer \eqref{eq:layer} to a new family of 1-Lipschitz network structures via providing new analytical solutions to our condition $W^\tp W\preceq T$. We term this general family of layers as SDP-based Lipschitz layers (SLL), since the condition $W^\tp W\preceq T$ can be viewed as an SDP for the decision variable $T$.
First of all, we extend the existing CPL (Eq. \eqref{eq:cpl}) via applying more general choices of $T$ with Theorem \ref{thm:main1}. From the discussion after Theorem \ref{thm:main1}, we already know that we can use the choice of $T=\diag(\sum_{j=1}^n |W^\tp W|_{ij})$ to replace the original choice $T=\norm{W}_2^2 I$. In this section, we will strengthen CPL via an even more general choice of $T$, which is based on a special version of Gershgorin circle theorem. Specifically, we will apply \cite[Corollary 6.1.6]{horn2012matrix} to show the following result. 
\begin{theorem}\label{thm:main3}
Let $W$ be the weight matrix. 
Suppose $T$ is a nonsingular diagonal matrix. 
If there exists some  diagonal matrix $Q$ with all positive diagonal entries such that $(T-QW^\tp W Q^{-1})$ is a real diagonally dominant matrix with diagonal entries being all positive, then $T\succeq W^\tp W$, and the function $h(x)=x-2WT^{-1}\sigma(W^\tp x+b)$ is 1-Lipschitz for $\sigma$ being ReLU, $\tanh$ or sigmoid. 
\end{theorem}

We defer the proof of this result to the appendix. If we choose $Q=I$, the above theorem just recovers the choice of $T$ used in AOL, i.e. $T=\diag(\sum_{j=1}^n |W^\tp W|_{ij})$. However, it is expected that the use of more general $Q$ will allow us to train a less conservative 1-Lipschitz neural network due to the increasing expressivity brought by these extra variables. We will present numerical results to demonstrate this. We also emphasize that $(T-QW^\tp W Q^{-1})$ is typically not a symmetric matrix and hence is not in $\D^n$ even when it only has non-negative eigenvalues. However, this does not affect our proof on the positive-semidefiniteness of $(T-W^\tp W)$. 

\paragraph{Application of Theorem \ref{thm:main3}.} We can parameterize $Q^{-1}=\diag(q_i)$ with $q_i>0$. Then the $(i,j)$-th entry of $QW^\tp W Q^{-1}$ is equal to $(W^\tp W)_{ij} q_j/q_i$.
Hence we can just set the diagonal entry of $T$ as \begin{align}\label{eq:SLLT}
T_{ii}=\sum_{j=1}^n |(W^\tp W)_{ij}q_j/q_i|=\sum_{j=1}^n |W^\tp W|_{ij} \frac{q_j}{q_i}.
\end{align}
This leads to our new choice of $T=\diag(\sum_{j=1}^n |W^\tp W|_{ij}q_j/q_i)$.
Notice that the layer function $h(x)=x-2WT^{-1}\sigma(W^\tp x+b)$ has a residual network structure. Hence it is expected that vanishing gradient will not be an issue. Therefore, we can simultaneously optimize the training loss over $W$ and $\{q_i\}$. We will present a numerical study to demonstrate that such a training approach will allow us to generate competitive results on training certifiably robust classifiers.  


\paragraph{SDP conditions for more general network structures.} It is also worth mentioning that the SDP condition in Theorem \ref{thm:main1} can be generalized to address the following more general structure:
\begin{align}\label{eq:layer}
h(x)=H x+ G \sigma(W^\tp x+b),
\end{align}
where $H$ and $G$ will be determined by the weight $W$ in some manner, and the matrix dimensions are assumed to be compatible. If we choose $H=I$ and $G=-2WT^{-1}$, then \eqref{eq:layer} reduces to the residual network structure considered in Theorem \ref{thm:main1}. There are many other choices of $(H,G)$ which can also ensure \eqref{eq:layer} to be 1-Lipschitz. Our last theoretical result is a new SDP condition which generalizes Theorem \ref{thm:main1} and provides a more comprehensive characterization of such choices of $(H,G)$. 


\begin{theorem}\label{thm1}
Let $n$ be the neuron number. 
For any non-negative scalars $\{\lambda_i\}_{i=1}^n$, define 
\begin{align}
\Lambda=\diag(\lambda_{1},\lambda_{2},\ldots, \lambda_{n}).
\end{align}
Suppose the activation function $\sigma$ is ReLU or $\tanh$ or sigmoid. 
 If there exist non-negative scalars $\{\lambda_{i}\}_{i=1}^n$ such that the following matrix inequality holds
\begin{align}\label{eq:LMI2}
  \bmat{I-H^\tp H & -H^\tp G-W \Lambda \\ -G^\tp H-\Lambda W^\tp & 2\Lambda-G^\tp G}\succeq 0
\end{align}
then the network layer \eqref{eq:layer} is $1$-Lipschitz, i.e., $\norm{h(x)-h(y)}\le \norm{x-y}$ for all $(x,y)$. 
\end{theorem}
The above theorem can be proved via modifying the argument used in \citet[Theorem~1]{fazlyab2019efficient}\footnote{As commented in \cite{pauli2021training}, such a modification works as long as $\Lambda$ is diagonal.} and we defer the detailed proof to the appendix. 
On one hand, if we choose $H=0$, then our condition \eqref{eq:LMI2} reduces to a variant of Theorem~1 in  \citet{fazlyab2019efficient}.\footnote{To see this connection, set $(\alpha,\beta,W^0,W^1)=(0,1,W^\tp, G)$ in Theorem~1 of \citet{fazlyab2019efficient}.}
On the other hand, for residual network structure with $H=I$, we can choose $T=2\Lambda^{-1}$ and $G=-W\Lambda=-2WT^{-1}$ to reduce \eqref{eq:LMI2} to our original algebraic condition $T\succeq W^\tp W$.  Therefore, Theorem \ref{thm1} provides a connection between the SDP condition in \cite{fazlyab2019efficient} and our proposed simple algebraic condition in Theorem \ref{thm:main1}. It is possible to obtain new 1-Lipschitz network layers via providing new analytical solutions to \eqref{eq:LMI2}. It is our hope that our proposed SDP condition \eqref{eq:LMI2} can lead to many more 1-Lipschitz network structures in the future.
 
 
\section{Experiments}
\label{section:experiments}


\begin{table}[t]
  \centering
  \caption{This table presents the natural, provable accuracy as well as the number of parameters and training time of several concurrent work and our SLL networks on CIFAR10 dataset. All results for SLL networks are the result of the average of 3 trainings.}
    {\footnotesize
    \begin{tabular}{lrrrrrrr}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Models}}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Natural} \\ \textbf{Accuracy}}}}} & \multicolumn{4}{c}{\boldmath{}\textbf{Provable Accuracy ($\varepsilon$)}\unboldmath{}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Number of} \\ \textbf{Parameters}}}}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Time by} \\ \textbf{Epoch (s)}}}}} \\
   \cmidrule{3-6}
     &   & $\frac{36}{255}$ & $\frac{72}{255}$ & $\frac{108}{255}$ & \multicolumn{1}{c}{$1$} \\
    \midrule
      \textbf{GloRo} {\scriptsize \citep{leino2021globally}} & 77.0 & 58.4 & \multicolumn{1}{r}{-} & \multicolumn{1}{r}{-} & \multicolumn{1}{r}{-} & 8M & 6 \\
      \textbf{Local-Lip-B} {\scriptsize \citep{huang2021training}} & 77.4 & 60.7 & 39.0 & 20.4 & \multicolumn{1}{r}{-} & 2.3M & 8 \\
      \textbf{Cayley Large} {\scriptsize \citep{trockman2021orthogonalizing}} & 74.6 & 61.4 & 46.4 & 32.1 & \multicolumn{1}{r}{-} & 21M & 30 \\
      \textbf{SOC 20} {\scriptsize \citep{skew2021sahil}} & 78.0 & 62.7 & 46.0 & 30.3 & \multicolumn{1}{r}{-} & 27M & 52 \\
      \textbf{SOC+ 20} {\scriptsize \citep{singla2022improved}} & 76.3 & 62.6 & 48.7 & 36.0 & \multicolumn{1}{r}{-} & 27M & 52 \\
      \textbf{CPL XL} {\scriptsize \citep{meunier2022dynamical}} & 78.5 & 64.4 & 48.0 & 33.0 & \multicolumn{1}{r}{-} & 236M & 163 \\
      \textbf{AOL Large} {\scriptsize \citep{prach2022almost}} & 71.6 & 64.0 & 56.4 & 49.0 & 23.7 & 136M & 64 \\
    \midrule
      \textbf{SLL Small}   & 71.2 & 62.6 & 53.8 & 45.3 & 20.4 &  41M & 20 \\
      \textbf{SLL Medium}  & 72.2 & 64.3 & 56.0 & 48.3 & 23.9 &  78M & 35 \\
      \textbf{SLL Large}   & 72.7 & 65.0 & 57.3 & 49.7 & 25.4 & 118M & 55 \\
      \textbf{SLL X-Large} & 73.3 & 65.8 & 58.4 & 51.3 & 27.3 & 236M & 105 \\
    \bottomrule
    \end{tabular}}
  \label{table:main_results}\end{table}



\begin{table}[t]
  \centering
  \vspace{-0.2cm}
  \caption{This table presents the natural and provable accuracy of several concurrent works and our SLL networks on CIFAR100 and TinyImageNet datasets. SLL networks are averaged of 3 trainings.}
    {\footnotesize
    \begin{tabular}{llrrrrr}
    \toprule
     \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Datasets}}}  & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Models}}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Natural} \\ \textbf{Accuracy}}}}} & \multicolumn{4}{c}{\boldmath{}\textbf{Provable Accuracy ($\varepsilon$)}\unboldmath{}} \\
   \cmidrule{4-7}      &   &   & $\frac{36}{255}$ & $\frac{72}{255}$ & $\frac{108}{255}$ & \multicolumn{1}{c}{$1$} \\
    \midrule
    \multirow{9}[4]{*}{CIFAR100}
      & \textbf{Cayley Large} {\scriptsize \citep{trockman2021orthogonalizing}} & 43.3 & 29.2 & 18.8 & 11.0 & \multicolumn{1}{r}{-} \\
      & \textbf{SOC 20} {\scriptsize \citep{skew2021sahil}} & 48.3 & 34.4 & 22.7 & 14.2 & \multicolumn{1}{r}{-} \\
      & \textbf{SOC+ 20} {\scriptsize \citep{singla2022improved}} & 47.8 & 34.8 & 23.7 & 15.8 & \multicolumn{1}{r}{-} \\
      & \textbf{CPL XL} {\scriptsize \citep{meunier2022dynamical}} & 47.8 & 33.4 & 20.9 & 12.6 & \multicolumn{1}{r}{-} \\
      & \textbf{AOL Large} {\scriptsize \citep{prach2022almost}} & 43.7 & 33.7 & 26.3 & 20.7 & 7.8 \\
    \cmidrule{2-7}
      & \textbf{SLL Small}   & 44.9 & 34.7 & 26.8 & 20.9 &  8.1 \\
      & \textbf{SLL Medium}  & 46.0 & 35.5 & 27.9 & 22.2 &  9.1 \\
      & \textbf{SLL Large}   & 46.4 & 36.2 & 28.4 & 22.7 &  9.6 \\
      & \textbf{SLL X-Large} & 46.5 & 36.5 & 29.0 & 23.3 & 10.4 \\
    \midrule
    \multirow{6}[3]{*}{TinyImageNet}
      & \textbf{GloRo} {\scriptsize \citep{leino2021globally}} & 35.5 & 22.4 & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-}  \\
      & \textbf{Local-Lip-B (+MaxMin)} {\scriptsize \citep{huang2021training}} & 36.9 & 23.4 & 12.7 & 6.1 & 0.0 \\
      \cmidrule{2-7}
      & \textbf{SLL Small}   & 26.6 & 19.5 & 14.2 & 10.4 & 2.9 \\
      & \textbf{SLL Medium}  & 30.4 & 22.3 & 15.9 & 11.6 & 3.0 \\
      & \textbf{SLL Large}   & 31.3 & 23.0 & 16.9 & 12.3 & 3.3 \\
      & \textbf{SLL X-Large} & 32.1 & 23.2 & 16.8 & 12.0 & 3.2 \\
    \bottomrule
    \end{tabular}}
  \label{table:main_results2}\vspace{-0.2cm}
\end{table}




In this section, we present a comprehensive set of experiments with 1-Lipschitz neural networks based on our proposed \textit{SDP-based Lipschitz Layer}.
More specifically, we build 1-Lipschitz neural networks based on the following layer:
\begin{equation}\label{equation:lipschitz_layer}
  h(x) = x - 2W\diag\left(\sum_{j=1}^n |W^\tp W|_{ij}q_j/q_i \right)
  ^{-1}\sigma(W^\tp x+b),
\end{equation}
where $W$ is a parameter matrix being either dense or a convolution, $\{q_i\}$ forms a diagonal scaling matrix as described by Theorem \ref{thm:main3},  and $\sigma(\cdot)$ is the ReLU nonlinearity function.
We use the same architectures proposed by~\cite{meunier2022dynamical} with \textit{small}, \textit{medium}, \textit{large} and \textit{xlarge} sizes.
The architecture consists of several Conv-SLL and Linear-SLL.
For CIFAR-100, we use the Last Layer Normalization proposed by~\cite{singla2022improved} which improves the certified accuracy when the number of classes becomes large. 
Note that the layer presented in Equation~\eqref{equation:lipschitz_layer} can be easily implemented with convolutions following the same scaling as in ~\cite{prach2022almost}. Our experiments focus on the impact of the Lipschitz layer structures on certified robustness.
This complements a recent study on other aspects (e.g. projection pooling) of  robust networks~\citep{singla2022}.

\begin{wraptable}{r}{5.8cm}
    \centering
    \vspace{-0.0cm}
    \caption{The SLL architecture used for the experiments is inspired by~\citeauthor{meunier2022dynamical}.}
    \label{table:models_sizes}
    {\scriptsize
    \vspace{-0.3cm}
    \begin{tabular}{lcccc}
    \toprule
     & \textbf{S} & \textbf{M} & \textbf{L} & \textbf{XL} \\
    \midrule
    \textbf{Conv-SLL}        &   20 &   30 &   90 &  120 \\
    \textbf{Channels}        &   45 &   60 &   60 &   70 \\ 
    \textbf{Linear-SLL}      &    7 &   10 &   15 &   15 \\
    \textbf{Linear Features} & 2048 & 2048 & 4096 & 4096 \\
    \bottomrule
    \end{tabular}}
    \vspace{-0.1cm}
\end{wraptable}

\paragraph{Details on the architectures \& Hyper-parameters.}
Table~\ref{table:models_sizes} describes the detail of our Small, Medium, Large and X-Large architectures.
We trained our networks with a batch size of 256 over 1000 epochs with the data augmentation used by~\shortcites{prach2022almost}.
We use an Adam optimizer~\citep{diederik2014adam} with $0.01$ learning rate and parameters $\beta_1$ and $\beta_2$ equal to $0.5$ and $0.9$ respectively and no weight decay.
We use a piecewise triangular learning rate scheduler to decay the learning rate during training.
We use the CrossEntropy loss as in~\cite{prach2022almost} with a temperature of $0.25$ and an offset value $\frac32 \sqrt{2}$.


\vspace{-0.2cm}
\paragraph{Results in terms of  Natural and Certified Accuracy on CIFAR10/100.}
First, we evaluate our networks (SLL) on CIFAR10 and CIAFR100 and compare the results against recent 1-Lipschitz neural network structures: Cayley, SOC, SOC+, CPL and AOL.
We also compare SLL with two other Lipschitz training approaches \citep{leino2021globally,huang2021training},  which do not guarantee prescribed global Lipschitz bounds during the training stage.
Table~\ref{table:main_results} presents the natural and certified accuracy with different radius of certification on CIFAR10. 
For a fair comparison, parameter number and training time per epoch for each method are also added to Table \ref{table:main_results}. Results on CIFAR100 are included in Table \ref{table:main_results2}.
We can see that our approach outperforms existing 1-Lipschitz architectures including AOL and CPL on certified accuracy for all values of $\varepsilon$.
We also observe that SLL-based 1-Lipschitz neural networks offer a good trade-off among previous approaches with respect to natural and certified accuracy. A detailed comparison is given below.

\vspace{-0.2cm}
\paragraph{Advantages of SLL over Cayley/SOC.} In general, it is difficult to compare the expressive power of non-residual and residual networks.
Hence we do not claim that with the same model size, SLL is more representative than Cayley or SOC which are not residual networks in the first place.
However, we believe that the current choice of $T$ in SLL is very easy to calculate and hence leads to a scalable approach that allows us to train very large models with a reasonable amount of time.
For illustrative purposes, consider the comparison between SLL and Cayley in Table~\ref{table:main_results}.
We can see that SLL Small has more parameters than Cayley Large (41M vs. 21M) while being faster to train.
Indeed, the Cayley approach involves computing an expensive orthogonal projection (with a matrix inverse), while SOC requires to the computation of several convolutions at training and inference (from 6 to 12) to compute the exponential of a convolution up to a desired precision.
Hence the training time per epoch for Cayley Large and SOC is actually longer than SLL Small.
While being faster to train SLL Small still outperforms Cayley Large and SOC for all three values of $\varepsilon$.
In general, we think it is fair to claim that our approach is more scalable than previous approaches based on orthogonal layers, and allows the use of larger networks which leads to improvements in certified robustness. 

\vspace{-0.0cm}
\paragraph{Advantages of SLL over AOL/CPL.} 
With careful tuning of the offset value, SLL outperforms AOL for all values of $\varepsilon$. 
We experiment with several offset values: $\sqrt{2}$, $\frac32 \sqrt{2}$ and $2\sqrt{2}$. The detailed results for all these different offset values are deferred to Table~\ref{table:offset_results} in the appendix.
In general, the offset value offers a trade-off between natural accuracy and robustness, thus, by choosing the offset value properly, SLL Large already achieves better results than AOL Large (notice that the training time per epoch for these two is roughly the same). SLL X-Large has even more improvements.
We can also see that SLL Large outperforms CPL XL for all values of $\varepsilon$ while being faster to train. 
For larger value of $\varepsilon$, the gain of SLL over CPL is remarkable (over 10\%).


\begin{wraptable}{r}{5.0cm}
    \centering
    \centering
    \vspace{-0.3cm}
    \caption{Inference time for Local-Lip-B and SLL X-Large on the full TinyImageNet validation with 4 GPUs.}
    \label{table:inference_time}
    {\footnotesize
    \begin{tabular}{lr}
    \toprule
    \textbf{Models} & \textbf{Inference Time} \\ 
    \midrule
    \textbf{Local-Lip-B} & 41 min \\
    \textbf{SLL X-Large} & 8 sec \\
    \bottomrule
    \end{tabular}}
\end{wraptable}

\vspace{-0.0cm}
\paragraph{Results on TinyImageNet.}
We have also implemented SLL on TinyImageNet (see Table \ref{table:main_results2}).
Previously, other 1-Lipschitz network structures including SOC, Cayley, AOL, and CPL have not been tested on TinyImageNet, and the state-of-the-art approach on TinyImageNet is the local Lipschitz bound approach \citep{huang2021local}.
We can see that SLL significantly outperforms this local Lipschitz approach for larger values of $\epsilon$ (while generating similar results for the small $\varepsilon$ case).
Notice that the local Lipschitz approach \citep{huang2021local} is quite different from other 1-Lipschitz network methods in the sense that it has no guarantees on the Lipschitz constant of the resultant network and hence does not generate 1-Lipschtiz networks in the first place.
Furthermore, given that this approach does not guarantee a Lipschitz bound during training, a lot more computation needs to be performed during inference, making the certification process very time consuming. 
Table~\ref{table:inference_time} describes the inference time on TinyImageNet for this local Lipschitz approach and SLL X-large. 


\begin{table}[t]
  \centering
  {
  \caption{The table describes the empirical robustness of our SLL-based classifiers on CIFAR10 ans CIFAR100 datasets. The empirical robustness is measured with  \textit{AutoAttacks}. All results are
the average of 3 models.}
  \label{tab:autoattack}\begin{tabular}{lrccccccccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[4]{*}{\textbf{Models}}} &   & \multicolumn{4}{c}{\textbf{CIFAR10 -- \textit{AutoAttack} ($\varepsilon$)}} &   & \multicolumn{4}{c}{\textbf{CIFAR100 -- \textit{AutoAttack} ($\varepsilon$)}} \\
\cmidrule{3-6}\cmidrule{8-11}      &   & $\frac{36}{255}$ & $\frac{72}{255}$ & $\frac{108}{255}$ & $1$ &   & $\frac{36}{255}$ & $\frac{72}{255}$ & $\frac{108}{255}$ & $1$ \\
    \midrule
    \textbf{SLL Small}   &   & 68.1 & 62.5 & 56.8 & 35.0 &   & 40.7 & 35.2 & 30.4 & 17.0 \\
    \textbf{SLL Medium}  &   & 69.1 & 63.8 & 58.4 & 37.0 &   & 41.5 & 36.4 & 31.5 & 17.9 \\
    \textbf{SLL Large}   &   & 69.8 & 64.5 & 59.1 & 37.9 &   & 42.1 & 37.1 & 32.6 & 18.7 \\
    \textbf{SLL X-Large} &   & 70.3 & 65.4 & 60.2 & 39.4 &   & 42.7 & 37.8 & 33.2 & 19.5 \\
    \bottomrule
    \end{tabular}}
\end{table}

\vspace{-0.0cm}
\paragraph{Results on Empirical Robustness.}
We also provide results of our approach on empirical robustness against an ensemble of diverse parameter-free attacks (\ie, \textit{AutoAttacks}) developed by~\cite{croce2020reliable}.
Table~\ref{tab:autoattack} reports the empirical robustness accuracy for different levels of perturbations.
Although \textit{AutoAttacks} is a strong empirical attack consisting of an ensemble of several known attacks:  APGD\textsubscript{CE}, APGD\textsubscript{DLR}, FAB~\citep{croce2020minimally} and Square~\citep{andriushchenko2020square}.
We can observe that the measure robustness is high and well above the certified radius. Indeed, on CIFAR10, we observe a robustness ``gain'' of up to 4.5\%, 9.6\%, 14.1\% and 21.7\% for respectively, 36, 72, 108 and 255 $\varepsilon$-perturbations.


\vspace{-0.1in}
\section{Conclusion}
In this paper, we present a unifying framework for designing Lipschitz layers. Based on a novel algebraic perspective, we identify a common SDP condition underlying the developments of spectral normalization, orthogonality-based methods, AOL, and CPL.
Furthermore, we have shown that AOL and CPL can be re-derived and generalized using our theoretical framework.
From this analysis, we introduce a family of SDP-based Lipschitz layers (SLL) that outperforms previous work.
In the future, it will be interesting to investigate 
more expressive structures of $T$ and extending our contributions to address multi-layer neural networks.

\subsubsection*{Acknowledgments}
 This work was performed using HPC resources from GENCI–IDRIS (Grant 2021-AD011013259) and funded by the French National Research Agency (ANR SPEED-20-CE23-0025). A. Havens and B. Hu are generously supported by the NSF award CAREER-2048168. We also thank Kai Hu for emailing us about an issue in our previous implementation of SLL, which is now addressed in Appendix D.


\bibliography{bibliography,IQCandSOS}
\bibliographystyle{iclr2023_conference}

\newpage
\appendix


\section{Proofs}
In this section, we present the proofs for the theorems presented in our paper.

\subsection{Proof of Theorem \ref{thm:main1}}
To prove the first statement in Theorem \ref{thm:main1}, notice that we have
\begin{align*}
    \norm{g(x)-g(y)}^2=\norm{WT^{-\frac{1}{2}}(x-y)}^2=(x-y)^\tp T^{-\frac{1}{2}} W^\tp W T^{-\frac{1}{2}} (x-y).
\end{align*}
Based on our algebraic condition $W^\tp W\preceq T$, we immediately have
\begin{align*}
    \norm{g(x)-g(y)}^2\le (x-y)^\tp T^{-\frac{1}{2}}TT^{-\frac{1}{2}} (x-y)=\norm{x-y}^2.
\end{align*}
Therefore, Statement 1 is true. 

To prove Statement 2 in Theorem \ref{thm:main1}, we need to use the property of the nonlinear activation function~$\sigma$. Notice that the condition $W^\tp W\preceq T$ ensures that all the diagonal entries of the nonsingular matrix $T$ are positive. Therefore, $T^{-1}$ is also a diagonal matrix whose diagonal entries are all positive.
For all the three activation functions listed in the above theorem, $\sigma$ is slope-restricted on $[0,1]$, and the following inequality holds for any $\{x',y'\}$ \cite[Lemma 1]{fazlyab2019efficient}:
\begin{align*}
 \bmat{x' - y' \\ \sigma(x')-\sigma(y')}^\tp  \bmat{0 & -T^{-1}\\ -T^{-1} & 2T^{-1}}\bmat{x' - y' \\ \sigma(x')-\sigma(y')}\le 0.
\end{align*}
We can set $x'=W^\tp x+b$ and $y'=W^\tp y+b$, and the above inequality becomes
\begin{align*}
 \bmat{W^\tp (x - y) \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp  \bmat{0 & -T^{-1}\\ -T^{-1} & 2T^{-1}}\bmat{W^\tp (x-y) \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}\le 0.
\end{align*}
We can rewrite the above inequality as
\begin{align}\label{eq:slope}
 \bmat{x - y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp  \bmat{0 & -W T^{-1} \\ -T^{-1}W^\tp & 2T^{-1}}\bmat{x-y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}\le 0.
\end{align}

Now we can apply the following argument:
\begin{align*}
    &\norm{h(x)-h(y)}^2\\
    =&\norm{x-y-2\left(W T^{-1}\sigma(W^\tp x+b)-W T^{-1} \sigma(W^\tp y+b)\right)}^2\\[0.5cm]
    =&\bmat{x-y \\ 2W T^{-1} \left(\sigma(W^\tp x+b)- \sigma(W^\tp y+b)\right)}\bmat{I & -I\\ -I & I}\bmat{x-y \\ 2W T^{-1}\left(\sigma(W^\tp x+b)- \sigma(W^\tp y+b)\right)}\\[0.5cm]
    =& \bmat{x-y\\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp  \bmat{I & -2WT^{-1}\\ -2T^{-1}W^\tp & 4T^{-1}W^\tp W T^{-1}}\bmat{x-y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}\\[0.5cm]
    \le & \bmat{x-y\\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp  \bmat{I & -2WT^{-1}\\ -2T^{-1}W^\tp & 4T^{-1}}\bmat{x-y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)},
\end{align*}
where the last step follows from the fact that our condition $W^\tp W\preceq T$ implies $T^{-1}W^\tp W T^{-1}\preceq T^{-1}$. 
Finally, we can combine the above inequality with \eqref{eq:slope} to show 
\begin{align*}
    \norm{h(x)-h(y)}^2&\le \bmat{x-y\\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp  \bmat{I & 0\\ 0 & 0}\bmat{x-y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}\\
    &=\norm{x-y}^2,
\end{align*}
which is the desired conclusion. Our proof is complete.

\subsection{Proof of Theorem \ref{thm:main2}}
Since $T$ is nonsingular diagonal and $T-W^\tp W\in \D^n$, then we must have $T_{ii}\ge \sum_{j} |W^\tp W|_{ij}$. Given the following key relation:
\begin{align*}
    \tr(I-T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}})=\sum_i \left(1-\frac{|W^\tp W|_{ii}}{T_{ii}}\right), 
\end{align*}
it becomes clear that we need to choose the smallest value of $T_{ii}$ for all $i$ to minimize $\tr(I-T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}})$. Therefore the choice of $T$ for AOL minimizes $\tr(I-T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}})$ over $T\in \mathbf{T}$. 
The proof for the last equality in Theorem~\ref{thm:main2} is similar.
Let us denote $X=I-T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}}$. For any $(i,j)$, the quantity $X_{ij}^2$  is always monotone non-decreasing in $T_{ii}$ and $T_{jj}$. To minimize $\norm{X}_F$, we just need to choose the smallest value for all $T_{ii}$ under the constraint
 $T_{ii}\ge \sum_{j} |W^\tp W|_{ij}$. 
This completes the proof. 
\qed

\subsection{The Gershgorin Circle Theorem and Proof of Theorem \ref{thm:main3}}
Before stating the proof of Theorem~\ref{thm:main3}, we will state the Gershgorin circle theorem, a useful result from matrix analysis which locates the eigenvalues of a real (or complex) matrix~\cite[Theorem 6.1.1]{horn2012matrix}.
\begin{theorem}[Gershgorin]
Let $A \in \mathbb{R}^{n\times n}$ and define the $n$ Gershgorin discs of $A$ by
\begin{align*}
    \left\{z \in \mathbb{C}: |z-A_{ii}| \leq \sum_{j\neq i}|A_{ij} | \right\},\quad i \in \{1,\ldots, n\}.
\end{align*}
Then the eigenvalues of $A$ are contained in the union of Gershgorin discs
\begin{align*}
  \bigcup_{i=1}^n \left\{z \in \mathbb{C}: |z-A_{ii}| \leq \sum_{j\neq i}|A_{ij} | \right\}
\end{align*}
\end{theorem}
A useful consequence of this theorem is that whenever $A$ is diagonally dominant (i.e. $|A_{ii}| \geq \sum_{j\neq i}|A_{ij}|$) with positive diagonal entries, then the eigenvalues of $A$ must be non-negative. With this fact, we now proceed to the proof of Theorem~\ref{thm:main3}.
\paragraph{Proof of Theorem~\ref{thm:main3}}
Given  nonsingular matrix $Q$, clearly the eigenvalues of $Q(T-W^\tp W)Q^{-1}$ and $(T-W^\tp W)$ are the same. 
If $Q(T-W^\tp W)Q^{-1}$ is diagonally dominant and only has positive diagonal entries, then we can apply Gershgorin circle theorem \cite[Corollary 6.1.6]{horn2012matrix}  to show that
all the eigenvalues of $Q(T-W^\tp W) Q^{-1}$ (which is the same as $T-QW^\tp W Q^{-1})$ are non-negative. Therefore, we know that all the eigenvalues of $(T-W^\tp W)$ are non-negative. Since $(T-W^\tp W)$ is symmetric, we have $T\succeq W^\tp W$. Then we can apply Theorem \ref{thm:main1} to reach our desired conclusion.
\qed


\subsection{Proof of Theorem \ref{thm1}}

A detailed proof for Theorem \ref{thm1} is presented here. Our proof is based on modifying the arguments used in \cite[Theorem 1]{fazlyab2019efficient}, and mainly relies on the quadratic constraint technique developed in the control field \citep{Megretski1997}.

First, notice that \eqref{eq:LMI2} is equivalent to the following condition:
\begin{align}\label{eq:LMI1}
\bmat{H^\tp H & H^\tp G \\ G^\tp H & G^\tp G} \preceq  \bmat{I & -W \Lambda \\ -\Lambda W^\tp & 2\Lambda}.
\end{align}
Suppose \eqref{eq:LMI1} holds. Next we will show that $h(x)=Hx+G\sigma(W^\tp x+b)$ is 1-Lipschitz.

For all the three activation functions listed in the above theorem, $\sigma$ is slope-restricted on $[0,1]$, and the following inequality holds for any $\{x',y'\}$ \cite[Lemma 1]{fazlyab2019efficient}:
\begin{align*}
 \bmat{x' - y' \\ \sigma(x')-\sigma(y')}^\tp  \bmat{0 & -\Lambda\\ -\Lambda & 2\Lambda}\bmat{x' - y' \\ \sigma(x')-\sigma(y')}\le 0.
\end{align*}
We can set $x'=W^\tp x+b$ and $y'=W^\tp y+b$, and the above inequality becomes
\begin{align*}
 \bmat{W^\tp (x - y) \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp  \bmat{0 & -\Lambda\\ -\Lambda & 2\Lambda}\bmat{W^\tp (x-y) \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}\le 0.
\end{align*}
We can rewrite the above inequality as
\begin{align}\label{eq:slope_Gen}
 \bmat{x - y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp  \bmat{0 & -W \Lambda  \\ -\Lambda W^\tp & 2\Lambda}\bmat{x-y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}\le 0.
\end{align}

Now we can apply the following argument:
\begin{align*}
  \norm{h(x)-h(y)}^2 =& \norm{H(x-y)+\left(G\sigma(W^\tp x+b)-G \sigma(W^\tp y+b)\right)}^2\\[0.4cm]
    =&\bmat{H(x-y) \\ G \left(\sigma(W^\tp x+b)- \sigma(W^\tp y+b)\right)}\bmat{I & I\\ I & I}\bmat{H(x-y) \\ G\left(\sigma(W^\tp x+b)- \sigma(W^\tp y+b)\right)}\\[0.4cm]
    =& \bmat{x-y\\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp \bmat{H^\tp H & H^\tp G \\ G^\tp H & G^\tp G}\bmat{x-y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}\\[0.4cm]
    \le & \bmat{x-y\\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp   \bmat{I & -W \Lambda \\ -\Lambda W^\tp & 2\Lambda}\bmat{x-y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)},
\end{align*}
where the last step follows from the condition \eqref{eq:LMI1}.
Finally, we can combine the above inequality with \eqref{eq:slope_Gen} to show 
\begin{align*}
    \norm{h(x)-h(y)}^2&\le \bmat{x-y\\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp  \bmat{I & 0\\ 0 & 0}\bmat{x-y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}\\
    &=\norm{x-y}^2,
\end{align*}
which is the desired conclusion. 


\section{Additional Results}
In this section, we will present some additional results and discuss the effect of the offset value on training.
The choice of 
the offset value will affect the performance of SLL significantly. Larger offset values will lead to decrease in natural accuracy and increase in certified robust accuracy. The details are documented in Table \ref{table:offset_results}.


\begin{table}[t]
  \centering
  {\scriptsize
  \caption{Additional results for CIFAR10 and CIFAR100 datasets with different offset values.}
  \label{table:offset_results}\begin{tabular}{clcrrrrlcrrrr}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Offset}}}  & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Models}}} &  \multicolumn{5}{c}{\textbf{CIFAR10}} &  & \multicolumn{5}{c}{\textbf{CIFAR100}} \\
   \cmidrule{3-7} \cmidrule{9-13} 
   & & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Natural} \\ \textbf{Accuracy}}}}} & \multicolumn{4}{c}{\textbf{Provable Accuracy ($\varepsilon$)}} &  & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Natural} \\ \textbf{Accuracy}}}}}  & \multicolumn{4}{c}{\boldmath{}\textbf{Provable Accuracy ($\varepsilon$)}\unboldmath{}} \\
  \cmidrule{4-7} \cmidrule{10-13}  
& & & $\frac{36}{255}$ & $\frac{72}{255}$ & $\frac{108}{255}$ & $1$ & & & $\frac{36}{255}$ & $\frac{72}{255}$ & $\frac{108}{255}$ & $1$ \\
      \midrule
      \multirow{4}[0]{*}{$\sqrt{2}$} 
    & SLL small  & 73.3 & 63.7 & 53.8 & 44.5 & 15.3  & & 46.7 & 35.2 & 26.4 & 20.1 & 5.9 \\
    & SLL medium & 74.0 & 64.7 & 54.9 & 45.3 & 16.0  & & 47.2 & 36.1 & 27.1 & 20.7 & 6.5 \\
    & SLL large  & 74.6 & 65.3 & 55.2 & 45.8 & 16.2  & & 47.9 & 36.7 & 27.9 & 21.3 & 6.7 \\
    & SLL xlarge & 75.3 & 65.7 & 55.8 & 46.1 & 16.3  & & 48.3 & 37.2 & 28.3 & 21.8 & 6.9 \\
    \midrule
      \multicolumn{1}{r}{\multirow{4}[0]{*}{$\frac32 \sqrt{2}$}} 
    & SLL small  & 71.2 & 62.6 & 53.8 & 45.3 & 20.4  & & 44.9 & 34.7 & 26.8 & 20.9 &  8.1 \\
    & SLL medium & 72.2 & 64.3 & 56.0 & 48.3 & 23.9  & & 46.0 & 35.5 & 27.9 & 22.2 &  9.1 \\
    & SLL large  & 72.7 & 65.0 & 57.3 & 49.7 & 25.4  & & 46.4 & 36.2 & 28.4 & 22.7 &  9.6 \\
    & SLL xlarge & 73.3 & 65.8 & 58.4 & 51.3 & 27.3  & & 46.5 & 36.5 & 29.0 & 23.3 & 10.4 \\
    \midrule
      \multirow{4}[0]{*}{$2 \sqrt{2}$} 
    & SLL small  & 70.0 & 61.5 & 53.4 & 45.7 & 22.7 & & 44.6 & 34.5 & 26.5 & 21.0 &  8.6 \\
    & SLL medium & 70.8 & 63.1 & 55.4 & 48.3 & 25.8 & & 45.4 & 35.5 & 27.9 & 22.1 &  9.8 \\
    & SLL large  & 71.4 & 63.9 & 56.7 & 49.8 & 27.8 & & 45.9 & 36.0 & 28.2 & 22.7 & 10.3 \\ 
    & SLL xlarge & 71.6 & 64.6 & 57.7 & 50.8 & 29.6 & & 46.1 & 36.3 & 29.0 & 23.6 & 11.0 \\ 
    \bottomrule
  \end{tabular}
  }
\end{table}


\section{Further Discussions}

In this section, we provide some extra discussions on control-theoretic interpretations and possible extensions of our main results. 

\subsection{Control-theoretic Interpretations for Our Main Results}

Our work is inspired by the quadratic constraint approach \citep{Megretski1997} and the Lur'e system theory~\citep{Lure1944} developed in the control community. Specifically, the general network layer structure \eqref{eq:layer} can be viewed as a Lur'e system, which is a feedback interconnection of a linear dynamical system and a static nonlinearity. In this section, we try to make this connection more transparent.

Specifically, we can denote $x'=h(x)$ and rewrite \eqref{eq:layer} as follows
\begin{align*}
x'&=Hx+Gw\\
v&=W^\tp x+b\\
w&=\sigma(v)
\end{align*}
which is exactly a shifted version of the Lur'e system. Therefore, it is not surprising that one can tailor the Lur'e system theory to study the properties of \eqref{eq:layer}. As a matter of fact, the previous developments in \cite{fazlyab2019efficient}
and \cite{revay2020lipschitz} were based on similar ideas. The main difference is that our paper requires solving SDPs analytically. In the controls literature, the formulated SDP conditions are typically solved numerically.

\subsection{A Variant of Theorem \ref{thm:main1}}

When discussing AOL and SLL, our main paper makes the assumption that  all the columns of $W$ have at least one non-zero entry such that \eqref{eq:DAOL} is well defined. To drop this assumption, we can use the following variant of Theorem \ref{thm:main1}. 

\begin{theorem}\label{thm:main6}
For any weight matrix $W\in \R^{m\times n}$, if there exists a diagonal matrix $\Gamma\in \SC^n$ such that $\Gamma W^\tp W\Gamma\preceq \Gamma$, then the two following statements hold true. 
\begin{enumerate}
  \item The mapping $g(x)=W\Gamma^{\frac{1}{2}} x+b$  is $1$-Lipschitz.
  \item The mapping $h(x)=x-2W \Gamma \sigma (W^\tp x+b)$ is $1$-Lipschitz if $\sigma$ is ReLU, $\tanh$ or sigmoid.
\end{enumerate}
\end{theorem}

The proof is omitted here, since we can use exactly the same argument as before. If $\Gamma$ happens to be nonsingular, then we can set $T=\Gamma^{-1}$, and the above theorem exactly reduces to Theorem \ref{thm:main1}. However, the above result allows $\Gamma$ to be singular. This is useful for designing AOL and SLL in the case where $W$ has some zero columns. Suppose the $(i_0,j)$-entry of $W^\tp W$ is equal to $0$ for all $j$. Then we can set the $(i_0,i_0)$-th entry of $\Gamma$ as $0$ and still use \eqref{eq:DAOL} or \eqref{eq:SLLT} for other entries. It is straightforward to verify that the resultant $\Gamma$ is still a feasible solution to $\Gamma W^\tp W \Gamma\preceq \Gamma$, and then we can implement AOL or SLL accordingly. 


\subsection{A Variant of Theorem \ref{thm:main3}}

We can also modify Theorem \ref{thm:main3} for the non-residual network layer case. The following variant of Theorem \ref{thm:main3} is useful.


\begin{theorem}\label{thm:main7}
Let $W$ be the weight matrix. 
Suppose $T$ is a nonsingular diagonal matrix. 
If there exists some  diagonal matrix $Q$ with all positive diagonal entries such that $(T-QW^\tp W Q^{-1})$ is a real diagonally dominant matrix with diagonal entries being all positive, then $T\succeq W^\tp W$, and the function $g(x)=W T^{-\frac{1}{2}} x+b$ is 1-Lipschitz. 
\end{theorem}
The proof is trivial and hence omitted. Based on the above result, it is possible that one can use \eqref{eq:SLLT} to construct a non-residual layer that can still improve upon AOL.

\newpage

\section{Erratum -- Correction of SLL implementation}

The authors of \cite{hu2023recipe} discovered an issue in the original implementation of the SLL layers. We are grateful to them for bringing this to our attention. Now we elaborate on this issue and provide the corrected implementation of SLL.  

Recall that our paper proposed the following SLL layer to build 1-Lipschitz networks:
\begin{equation}\label{eq:reSLL}
  h(x) = x - 2W\diag\left(\sum_{j=1}^n |W^\tp W|_{ij} \frac{q_j}{q_i} \right)
  ^{-1}\sigma(W^\tp x+b),
\end{equation}
where $\sigma(\cdot)$ is the ReLU nonlinearity function.
The parameters of this layer consist of $W$, $\{q_i\}_{i=1}^n$ and $b$. 
One can prove that this layer is 1-Lipschitz.
However, the division on the parameter $q_j/q_i$ can make the training process unstable.
Previously, SLL was implemented as follows:
\begin{equation}\label{eq:wrong1}
  h(x) = x - 2W\diag\left(\sum_{j=1}^n |W^\tp W|_{ij} \frac{q_j}{q_i + \epsilon} \right)
  ^{-1}\sigma(W^\tp x+b),
\end{equation}
where $\epsilon = 10^{-6}$ is added to the denominator to avoid dividing by $0$. 
However, Equation~\eqref{eq:wrong1} is not 1-Lipschitz anymore due to the appearance of $\epsilon$. 




\begin{table}[t]
  \centering
  \caption{This table presents the natural and corrected provable accuracy of our SLL networks on CIFAR10 and CIFAR100 datasets. SLL networks are averaged of 3 trainings.}
    {\footnotesize
    \begin{tabular}{lllrrrrr}
    \toprule
     \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Datasets}}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Training}}}  & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Models}}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Natural} \\ \textbf{Accuracy}}}}} & \multicolumn{4}{c}{\boldmath{}\textbf{Provable Accuracy ($\varepsilon$)}\unboldmath{}} \\
   \cmidrule{5-8}      &   &  & & $\frac{36}{255}$ & $\frac{72}{255}$ & $\frac{108}{255}$ & \multicolumn{1}{c}{$1$} \\
    \midrule
    \multirow{8}[0]{*}{CIFAR10} &
    \multirow{4}[0]{*}{\textbf{Surrogate Training}} 
      & \textbf{SLL Small} & 71.3 & 62.7 & 53.8 & 45.4 & 20.4 \\
    & & \textbf{SLL Medium} & 72.0 & 63.6 & 54.7 & 46.4 & 21.0 \\
    & & \textbf{SLL Large} & 72.6 & 64.1 & 55.4 & 46.9 & 21.3 \\
    & & \textbf{SLL X-Large} & 73.2 & 64.6 & 55.8 & 47.3 & 21.5  \\
    \cmidrule{2-8}
    & \multirow{4}[0]{*}{\textbf{Exponential Scaling}} 
      & \textbf{SLL Small}   & 71.5 & 62.8 & 53.7 & 45.2 & 19.4 \\
    & & \textbf{SLL Medium}  & 72.2 & 63.7 & 54.7 & 46.1 & 20.1 \\
    & & \textbf{SLL Large}   & 72.6 & 64.2 & 55.1 & 46.6 & 20.3 \\
    & & \textbf{SLL X-Large} & 73.3 & 64.8 & 55.7 & 47.1 & 20.6 \\
    \midrule
    \multirow{8}[0]{*}{CIFAR100} & 
    \multirow{4}[0]{*}{\textbf{Surrogate Training}} 
      & \textbf{SLL Small} & 45.8 & 34.7 & 26.5 & 20.4 & 7.2 \\
    & & \textbf{SLL Medium} & 46.5 & 35.6 & 27.3 & 21.1 & 7.7 \\
    & & \textbf{SLL Large} & 46.9 & 36.2 & 27.9 & 21.6 & 7.9 \\
    & & \textbf{SLL X-Large} & 47.6 & 36.5 & 28.2 & 21.8 & 8.2 \\
    \cmidrule{2-8}
    & \multirow{4}[0]{*}{\textbf{Exponential Scaling}} 
       & \textbf{SLL Small}   & 45.8 & 34.8 & 26.5 & 20.2 & 7.2 \\
     & & \textbf{SLL Medium}  & 46.8 & 35.8 & 27.3 & 21.0 & 7.7 \\
     & & \textbf{SLL Large}   & 47.2 & 36.2 & 27.8 & 21.5 & 7.9 \\
     & & \textbf{SLL X-Large} & 47.8	& 36.7 & 28.3 & 22.2 & 8.3 \\
    \bottomrule
    \end{tabular}}
  \label{table:corrected_results}\end{table}

There are several ways to fix the above issue.
For example, we can still use \eqref{eq:wrong1} for training, and then substitute the resultant values of  $(W, \{q_i\}, b)$ to \eqref{eq:reSLL} for evaluating the certified robust accuracy. In other words, we can use \eqref{eq:wrong1} as a surrogate for stable training of \eqref{eq:reSLL}.  The results of \emph{surrogate training} are presented in Table~\ref{table:corrected_results}.
One other way to address the above issue is to use the exponential scaling:
\begin{equation} \label{eq:exp}
  h(x) = x - 2W\diag\left(\sum_{j=1}^n |W^\tp W|_{ij} \frac{\exp(q_j)}{\exp(q_i)} \right)
  ^{-1}\sigma(W^\tp x+b),
\end{equation}
The training of the above layer is stable, and the resultant network is indeed 1-Lipschitz. Table~\ref{table:corrected_results} also presents the corrected results obtained using the above exponential scaling form.
We can see that the results from surrogate training in Table \ref{table:corrected_results} are similar to those obtained using the exponential scaling. 
We can also observe from Table \ref{table:corrected_results} that SLL outperforms AOL on CIFAR100. For CIFAR10 with $\epsilon=\frac{36}{255}$, SLL still outperforms AOL. However, For CIFAR10 with $\epsilon=\frac{72}{255}$ or $\frac{108}{255}$, AOL achieves better results than SLL.
The Github repo \href{https://github.com/araujoalexandre/Lipschitz-SLL-Networks}{\texttt{https://github.com/araujoalexandre/Lipschitz-SLL-Networks}} has been corrected accordingly.




Finally, it is worth mentioning that Kai Hu's email has pointed out that the following layer is also 1-Lipschitz and can be trained in a stable manner:
\begin{equation*}
  h(x) = x - 2W\diag\left(\sum_{j=1}^n |W^\tp W|_{ij} \frac{q_j + \epsilon}{q_i + \epsilon} \right)
  ^{-1}\sigma(W^\tp x+b).
\end{equation*}
In comparison to our current results in Table \ref{table:corrected_results}, the certified robustness results from the above variant (reported in Kai Hu's email) are worse on CIFAR100 and similar on CIFAR10.







\end{document}
