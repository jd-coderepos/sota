\documentclass{article} \usepackage{iclr2023_conference,times}
\usepackage{amsmath,amsthm,amssymb,amsfonts,bbm,bm,math}
\usepackage{enumitem}
\usepackage[bookmarks=false,hidelinks]{hyperref}
\usepackage[small,bf]{caption}
\usepackage{xspace}
\usepackage{longtable}
\usepackage{bbm}
\usepackage{natbib}
\usepackage{array}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{array}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{wrapfig}

\newcommand\citesingle[1]{\AtNextCite{\defcounter{maxnames}{1}}\citeauthor{#1}, \citeyear{#1}}


\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\SC}{\mathbf{S}}
\newcommand{\cvx}{\mathcal{F}}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\RH}{\field{RH}_\infty}
\newcommand{\RHmin}{\field{RH}_\infty^-}   
\newcommand{\RL}{\field{RL}_\infty}
\newcommand{\CF}{\field{C}}
\newcommand{\F}{\field{F}}
\newcommand{\Ha}{\field{H}_2}
\newcommand{\Haperp}{\field{H}_2^\perp}

\newcommand{\bmtx}{\begin{bmatrix}}
\newcommand{\emtx}{\end{bmatrix}}
\newcommand{\bsmtx}{\left[ \begin{smallmatrix}} 
\newcommand{\esmtx}{\end{smallmatrix} \right]} 
\newcommand{\bmatarray}[1]{\left[\begin{array}{#1}}
\newcommand{\ematarray}{\end{array}\right]} 

\newcommand{\Dt}{\mathcal{D}_\tau}
\newcommand{\St}{\mathcal{S}_\tau}
\newcommand{\A}{\mathcal{T}}
\newcommand{\D}{\mathbf{D}}

\newcommand{\cin}{{c_{\text{in}}}}
\newcommand{\cout}{{c_{\text{out}}}}


\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\regret}{regret}
\newcommand{\0}{\mathbf{0} }

\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

\newcommand\EnumPrefix{}

\newlist{senenum}{enumerate}{10}
\setlist[senenum]{label=\arabic*.,ref=\EnumPrefix,leftmargin=*}




\def \endprf{\hfill {\vrule height6pt width6pt depth0pt}\medskip}


\newcommand{\note}[1]{{\color{red}\bf [{\em Note:} #1]}}
\newcommand{\iprod}[2]{\left\langle #1 , #2 \right\rangle}

\newcommand{\SGD}{SGM}
\newcommand{\SGM}{SGM}
\newcommand{\sgd}{{stochastic gradient descent} }
\newcommand{\sgm}{{stochastic gradient methods} }

\gdef\issubmit{1}
\newcommand{\submit}[2]{\ifdefined\issubmit{#1}\else{#2} \fi }

\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}

\newcommand{\bin}[1]{\textbf{{\color{blue}Bin: #1}}}
\newcommand{\alex}[1]{\textbf{{\color{orange}Alex: #1}}}
\newcommand{\blaise}[1]{\textbf{{\color{purple}Blaise: #1}}}


\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}


\title{A Unified Algebraic Perspective on \\ Lipschitz Neural Networks}



\author{Alexandre Araujo, Aaron Havens, Blaise Delattre, Alexandre Allauzen and Bin Hu \0.2cm]
\emph{(10/26/2023): Erratum is added in Appendix D. This is an updated version that fixes an implementation issue in the previous version. Due to that implementation issue, the original numerical results in our original ICLR paper are not accurate. We elaborate on the issue and provides some fix in Appendix D.
}
\end{abstract}


\section{Introduction}
The robustness of deep neural networks is nowadays a great challenge to establish confidence in their  decisions for real-life applications.  Addressing this challenge requires guarantees on the stability of the prediction, with respect to adversarial attacks. In this context, the Lipschitz constant of neural networks is a key property at the core of many recent advances. Along with the margin of the classifier, this property allows us to certify the robustness against worst-case adversarial perturbations.  This certification is based on a sphere of stability within which the decision remains the same for any perturbation inside the sphere~\citep{tsuzuku2018lipschitz}.

The design of 1-Lipschitz layers provides a successful approach to enforce this property for the whole neural network. For this purpose, many different techniques have been devised such as spectral normalization~\citep{miyato2018spectral,farnia2018generalizable}, orthogonal parameterization~\citep{trockman2021orthogonalizing,li2019preventing,skew2021sahil,yu2022constructing,xulot2022}, Convex Potential Layers (CPL)~\citep{meunier2022dynamical}, and Almost-Orthogonal-Layers (AOL)~\citep{prach2022almost}. While all these techniques share the same goal, their motivations, and derivations can greatly differ, delivering different solutions. Nevertheless, their raw experimental comparison fails to really gain insight into their peculiar performance, soundness, and in the end their possible complementarity. Therefore a question acts as a barrier for an in-depth analysis and future development:
\begin{center}
\textbf{\textit{Are there common principles underlying the developments of 1-Lipschitz Layers?}}
\end{center}
In this paper, we propose a novel perspective to answer this question
based on a unified Semidefinite Programming (SDP) approach.
We introduce a common algebraic condition underlying various types of
methods like spectral normalization, orthogonality-based methods, AOL,
and CPL. Our key insight is that this condition can be formulated as a
unifying and simple SDP problem, and that the development of 1-Lipschitz
architectures systematically arise by finding ``analytical solutions"
of this SDP. Our main contributions are summarized as follows.
\begin{itemize}[leftmargin=10pt,parsep=0pt,topsep=0pt,itemsep=2pt]
    \item We provide a unifying algebraic perspective for 1-Lipschitz network layers by showing that existing techniques such as spectral normalization, orthogonal parameterization, AOL, and CPL can all be recast as a solution of the same simple SDP condition (Theorem \ref{thm:main1} and related discussions). Consequently, any new analytical solutions of our proposed SDP condition will immediately lead to new 1-Lipschitz network structures. 
    \item Built upon the above algebraic viewpoint, we give a rigorous mathematical interpretation for AOL explaining how this method promotes ``almost orthogonality" in training (Theorem \ref{thm:main2}).
    \item Based on our SDPs, a new family of 1-Lipschitz network structures termed as SDP-based Lipschitz layers (SLL) has been developed. Specifically, we apply the Gershgorin circle theorem to obtain some new SDP solutions, leading to non-trivial extensions of CPL (Theorem \ref{thm:main3}). We derive new SDP conditions to characterize SLL in a very general form (Theorem \ref{thm1}).
    \item Finally, we show, by a comprehensive set of experiments, that our new SDP-based Lipschitz layers outperform previous approaches on certified robust accuracy.
\end{itemize}
Our work is inspired by \cite{fazlyab2019efficient} that develops SDP conditions for numerical estimation of Lipschitz constants of given neural networks. A main difference is that we focus on ``analytical SDP solutions" which can be used to characterize 1-Lipschitz network structures. 


\section{Related Work}

In recent years, certified methods have been central to the development of trustworthy machine learning and especially for deep learning. 
\textit{Randomized Smoothing}~\citep{cohen2019certified,salman2019provably} is one of the first defenses to offer provable robustness guarantees. The method simply extends a given classifier by the smart introduction of  random noise to enhance the robustness of the classifier.
Although this method offers an interesting level of certified robustness, it suffers from important downsides such as the high computational cost of inference and some impossibility results from information-theory perspective~\citep{yang2020randomized,kumar2020curse}.

Another approach to certify the robustness of a classifier is to control its Lipschitz constant \citep{hein2017formal,tsuzuku2018lipschitz}. The main idea is to derive a certified radius in the feature space by upper bounding the margin of the classifier. See Proposition~1 of~\cite{tsuzuku2018lipschitz} for more details. This radius, along with the Lipschitz constant of the network can certify the robustness. 
In order to reduce the Lipschitz constant and have a non-trivial certified accuracy, \cite{tsuzuku2018lipschitz} and \cite{leino2021globally} both upper bound the margin via computing a bound on the global Lipschitz constant, however, these bounds have proved to be loose.
Instead of upper bounding the global Lipschitz constant, \cite{huang2021training} leverages {\em local} information to get tighter bound on the Lipschitz constant.  
On the other hand, other works, instead of upper bounding the local or global Lipschitz, devised neural networks architecture that are provably 1-Lipschitz. 
One of the first approaches in this direction consists of  normalizing each layer with its spectral norm~\citep{miyato2018spectral,farnia2018generalizable}. Each layer is, by construction,  1-Lipschitz.
Later, a body of research replaces the normalized weight matrix by an orthogonal matrix. It improves  upon the spectral normalization method by  adding  the gradient preservation~\citep{li2019preventing,trockman2021orthogonalizing,skew2021sahil,yu2022constructing,xulot2022}.
These methods constrain the parameters by orthogonality during training.
Specifically, the Cayley transform can be used to constrain the weights~\citep{trockman2021orthogonalizing} and,  
in a similar fashion, SOC~\citep{skew2021sahil}  parameterizes their layers with the exponential of a skew symmetric matrix making it orthogonal.
To reduce cost, \cite{trockman2021orthogonalizing}, \cite{yu2022constructing}, and \cite{xulot2022} orthogonalize their convolutional kernel in the Fourier domain.

More recently, a work by~\cite{meunier2022dynamical} has studied Lipschitz networks from a dynamical system perspective. Starting from the continuous view of a residual network, they showed that the parameterization with the Cayley transform \citep{trockman2021orthogonalizing} and SOC \citep{skew2021sahil} correspond respectively to two specific discretization schemes of the continuous flow. 
Furthermore, a new layer is derived from convex potential flows to ensure the 1-Lipschitz~property\footnote{We reverse the transposition from the original layer to have a consistent notation in the rest of the article.}:

where  is the spectral norm of the weight matrix  and  is the ReLU activation function. In general, the training of orthogonal layers can be expensive. The Cayley approach involves a matrix inversion, and the implementation of SOC requires either an SVD or an iterative Taylor expansion. The CPL approach can be more efficient, although the computation of  is still needed.

A recent work, \textit{Almost-Orthogonal-layer} (AOL) \citep{prach2022almost} came up with a middle ground: a new normalization which makes the layer 1-Lipschitz by favoring orthogonality.  
The fully-connected AOL layer is defined as  where  is a diagonal matrix given by\footnote{For simplicity, we assume all the columns of  have at least one non-zero entry. Then \eqref{eq:DAOL} is well defined.}:

They demonstrated that this layer is 1-Lipschitz and they empirically show that, after training, the Jacobian of the layer (with respect to ) is almost orthogonal, hence facilitating the training.

Another source of inspiration is the application of convex programs for robustness certification of neural networks~\citep{wong2018provable,raghunathan2018semidefinite,fazlyab2019efficient,revay2020lipschitz,fazlyab2020safety,wang2022}.
The most relevant work is 
 \cite{fazlyab2019efficient}, which leverages the quadratic constraint approach from control theory \citep{Megretski1997} to~formulate SDPs for estimating  the global Lipschitz constant of neural networks numerically.  It is possible to solve such SDPs numerically for training relatively small Lipschitz networks \citep{pauli2021training}.
However, due to the restrictions of existing SDP solvers, scalability has been one issue when deploying such approaches to deep learning problems with large data sets. 
Our focus is on the design of Lipschitz network structures, and we avoid the scalability issue via solving SDPs analytically.



\section{Background}

\paragraph{Notation.} The  identity matrix and the  zero matrix are denoted as  and , respectively. The subscripts will be omitted when the dimension is clear from the context. 
 When a matrix  is negative semidefinite (definite), we will use the notation .  When a matrix  is positive semidefinite (definite), we will use the notation .
Let  denote the vector whose -entry is  and all other entries are . Given a collection of scalars ,  we use the notation  to denote the  diagonal matrix whose -th entry is . For a matrix , the following notations , ,  , , , and  stand for its transpose, largest singular value, trace, smallest singular value, Frobenius norm, and spectral radius, respectively. 

\paragraph{Lipschitz functions.} A function  is -Lipschitz with respect to the  norm iff it satisfies  for all , where  stands for the  norm.
An important fact is that the robustness of a neural network can be certified based on its Lipschitz constant \citep{tsuzuku2018lipschitz}. 
In this paper, we are interested in the case where . Specifically, we consider the training of 1-Lipschitz neural networks. If each layer of a neural network is 1-Lipschitz, then the entire neural network is also 1-Lipschitz. The Lipschitz constant also satisfies the triangle inequality, and hence convex combination will preserve the 1-Lipschitz property.

\paragraph{Matrix cones: Positive semidefiniteness and diagonal dominance.} 
Let  denote the set of all  real symmetric matrices. Let  be the set of all  symmetric positive semidefinite matrices. It is well known that  is a closed-pointed convex cone in . With the trace inner product,  is also self-dual. Consider two symmetric matrices  and  such that , then we have , and  provides a distance measure between  and . In addition, we have . 
Finally, the set of all  real symmetric diagonally dominant matrices with non-negative diagonal entries is represented by . It is known that  forms a closed, pointed, full cone \citep{barker1975cones}. Based on the Gershgorin circle theorem \citep{horn2012matrix}, we know . It is also known that  is smaller than  \citep{barker1975cones}. 
For any , we have . 
It is important to require , and the set of real symmetric diagonally dominant matrices is not  a cone by itself. 



\section{An Algebraic Unification of 1-Lipschitz Layers}

In this section, we present a unified algebraic perspective for various 1-Lipschitz layers (Spectral Normalization, Orthogonalization, AOL, and CPL) via developing a common SDP condition characterizing the Lipschitz property.  Built upon our algebraic viewpoint, we also present a new mathematical interpretation explaining how AOL promotes orthogonality in training.

\subsection{The unifying  Algebraic Condition}
First, we present an algebraic condition which can be used to unify the developments of existing techniques such as SN, AOL, and CPL. Our main theorem is formalized below.

\begin{theorem}\label{thm:main1}
For any weight matrix , if there exists a nonsingular diagonal matrix  such that , then the two following statements hold true. 
\begin{enumerate}
  \item The mapping   is -Lipschitz.
  \item The mapping  is -Lipschitz if  is ReLU,  or sigmoid.
\end{enumerate}
\end{theorem}

The proof of the above theorem and some related control-theoretic interpretations are provided in the appendix.
This theorem allows us to design different 1-Lipschitz layers just with various choices of , in two important cases: for a linear transformation with Statement 1, as well as for a residual and non-linear block with Statement 2. Moreover, for any given weight matrix , the condition  is linear in , and hence can be viewed as an SDP condition with decision variable . 
To emphasize the significance of this theorem, we propose to derive existing methods used for designing 1-Lipschitz layers by choosing specific  for the SDP condition . The 1-Lipschitz property is then automatically obtained. 


\begin{itemize}
    \item {Spectral Normalization (SN)} corresponds to an almost trivial choice if we notice that . Hence with , we build the SN layer .
    \item The Orthogonality-based parameterization is obtained by setting  and enforcing the equality . Then obviously  is 1-Lipschitz.
    \item AOL formula can be derived by letting  . With this choice, we have ,  hence  . Then Statement 1 in Theorem~\ref{thm:main1} implies that the AOL layer, written as , is 1-Lipschitz.\footnote{For ease of exposition, our main paper always assumes that all the columns of  have at least one non-zero entry such that \eqref{eq:DAOL} is well defined. To drop this assumption, we can use a variant of Theorem \ref{thm:main1} which replaces the SDP condition with a bilinear matrix inequality condition. We will discuss this point in the appendix.}
    \item CPL follows the same SN choice , but with Statement 2 of Theorem \ref{thm:main1}. Hence we derive a different function  which is also 1-Lipschitz.
\end{itemize}

The above discussion illustrates the benefit of expressing all these methods within the same theoretical framework, offering us a new tool to characterize the similarity  between different methods.
For instance, SN and CPL share the same choice of . The difference between them is which statement is used. Hence CPL can be viewed as the ''residual version`` of SN. Clearly, the residual network structure allows CPL to address the gradient vanishing issue more efficiently than SN.
With the same approach, we can readily infer from our unified algebraic condition what are the  "residual" counterparts for orthogonality-based parameterization and AOL. 
For orthogonality-based parameterization, if we enforce  via methods such as SOC and ECO, then the function  is 1-Lipschitz (by Statement 2 in Theorem \ref{thm:main1}). Finally,  if we choose , then the function  is also 1-Lipschitz. Therefore it is straightforward to create  new classes of 1-Lipschitz network structures from existing ones. 

Another important consequence of Theorem \ref{thm:main1} is about new layer development. Any  new nonsingular diagonal solution  for the SDP condition   immediately leads to new 1-Lipschitz network structures in the form of  or . Therefore, the developments of 1-Lipschitz network structures can be reformulated as finding analytical solutions of the matrix inequality  with nonsingular diagonal . 
As a matter of fact,  the Gershgorin circle theorem can help to improve the existing choices of  in a systematic way. In Section~\ref{sec:ECPL}, we will discuss such new choices of  and related applications to improve CPL. At this point, it is worth noticing that to develop deep Lipschitz networks, it is important to have analytical formulas of . The analytical formula of  will enable a fast computation of   or .  

Theorem \ref{thm:main1} is powerful in building a connection between 1-Lipschitz network layers and the algebraic condition . Next, we will look closer at this algebraic condition and provide a new mathematical interpretation explaining how AOL generates ``almost orthogonal" weights.  


\begin{remark}
The proof of Statement 2 in Theorem \ref{thm:main1} relies on \cite[Lemma 1]{fazlyab2019efficient}, which requires the activation function  to be slope-restricted on . Therefore, Statement 2 cannot be applied to the case with  being the GroupSort activation function \citep{anil2019sorting}. In contrast, Statement 1 can be used to build neural networks with any activation functions which are 1-Lipschitz. 
\end{remark}

\subsection{A New Mathematical Interpretation for AOL}

In \cite{prach2022almost}, it is observed that AOL can learn "almost orthogonal" weights and hence overcome the gradient vanishing issue. 
As a matter of fact, 
the choice of  used in AOL is optimal in a specific mathematical sense as formalized with the next theorem. 

\begin{theorem}\label{thm:main2}
Given any  which does not have zero columns, define the set . 
Then the choice of  for the AOL method    actually satisfies

\end{theorem}
We defer the proof for the above result to the appendix.
Here we provide some interpretations for the above result.  Obviously, the quantity  provides a measure for the distance between the scaled weight matrix  and the set of  orthogonal matrices. If , then the scaled weight  is orthogonal. If  is small, it means that   is ``almost orthogonal" and close to the set of orthogonal matrices.  Since we require , we know that   is a positive semidefinite matrix, and its trace provides an alternative metric quantifying the distance between 
and the set of orthogonal matrices. Importantly, we have the following inequality:  

If  is small, then  is also small, and  is close to the set of orthogonal matrices. Therefore, one interpretation for Theorem \ref{thm:main2} is that among all the nonsingular diagonal scaling matrices  satisfying , the choice of  used in AOL makes the scaled weight matrix  the closest to the set of orthogonal matrices. This provides a new mathematical explanation of how AOL can generate ``almost orthogonal" weights. 


One potential issue for AOL is that  is typically much smaller than , and the condition  may be too conservative compared to the original condition  in Theorem \ref{thm:main1}. 
If we denote the set , then we  have 
 ,
 and 
 .
This leads to interesting alternative choices of  which can further promote orthogonality:

Although \eqref{eq:AOLplus} may be solved as convex programs on small toy examples, it is not practical to use such choice of  for large-scale problems. It is our hope that our theoretical discussion above will inspire
more future research on developing new practical choices of  for promoting orthogonality.


\section{Extensions of CPL: The Power of Gershgorin circle theorem}
\label{sec:ECPL}

In this section, we extend the original CPL layer \eqref{eq:layer} to a new family of 1-Lipschitz network structures via providing new analytical solutions to our condition . We term this general family of layers as SDP-based Lipschitz layers (SLL), since the condition  can be viewed as an SDP for the decision variable .
First of all, we extend the existing CPL (Eq. \eqref{eq:cpl}) via applying more general choices of  with Theorem \ref{thm:main1}. From the discussion after Theorem \ref{thm:main1}, we already know that we can use the choice of  to replace the original choice . In this section, we will strengthen CPL via an even more general choice of , which is based on a special version of Gershgorin circle theorem. Specifically, we will apply \cite[Corollary 6.1.6]{horn2012matrix} to show the following result. 
\begin{theorem}\label{thm:main3}
Let  be the weight matrix. 
Suppose  is a nonsingular diagonal matrix. 
If there exists some  diagonal matrix  with all positive diagonal entries such that  is a real diagonally dominant matrix with diagonal entries being all positive, then , and the function  is 1-Lipschitz for  being ReLU,  or sigmoid. 
\end{theorem}

We defer the proof of this result to the appendix. If we choose , the above theorem just recovers the choice of  used in AOL, i.e. . However, it is expected that the use of more general  will allow us to train a less conservative 1-Lipschitz neural network due to the increasing expressivity brought by these extra variables. We will present numerical results to demonstrate this. We also emphasize that  is typically not a symmetric matrix and hence is not in  even when it only has non-negative eigenvalues. However, this does not affect our proof on the positive-semidefiniteness of . 

\paragraph{Application of Theorem \ref{thm:main3}.} We can parameterize  with . Then the -th entry of  is equal to .
Hence we can just set the diagonal entry of  as 
This leads to our new choice of .
Notice that the layer function  has a residual network structure. Hence it is expected that vanishing gradient will not be an issue. Therefore, we can simultaneously optimize the training loss over  and . We will present a numerical study to demonstrate that such a training approach will allow us to generate competitive results on training certifiably robust classifiers.  


\paragraph{SDP conditions for more general network structures.} It is also worth mentioning that the SDP condition in Theorem \ref{thm:main1} can be generalized to address the following more general structure:

where  and  will be determined by the weight  in some manner, and the matrix dimensions are assumed to be compatible. If we choose  and , then \eqref{eq:layer} reduces to the residual network structure considered in Theorem \ref{thm:main1}. There are many other choices of  which can also ensure \eqref{eq:layer} to be 1-Lipschitz. Our last theoretical result is a new SDP condition which generalizes Theorem \ref{thm:main1} and provides a more comprehensive characterization of such choices of . 


\begin{theorem}\label{thm1}
Let  be the neuron number. 
For any non-negative scalars , define 

Suppose the activation function  is ReLU or  or sigmoid. 
 If there exist non-negative scalars  such that the following matrix inequality holds

then the network layer \eqref{eq:layer} is -Lipschitz, i.e.,  for all . 
\end{theorem}
The above theorem can be proved via modifying the argument used in \citet[Theorem~1]{fazlyab2019efficient}\footnote{As commented in \cite{pauli2021training}, such a modification works as long as  is diagonal.} and we defer the detailed proof to the appendix. 
On one hand, if we choose , then our condition \eqref{eq:LMI2} reduces to a variant of Theorem~1 in  \citet{fazlyab2019efficient}.\footnote{To see this connection, set  in Theorem~1 of \citet{fazlyab2019efficient}.}
On the other hand, for residual network structure with , we can choose  and  to reduce \eqref{eq:LMI2} to our original algebraic condition .  Therefore, Theorem \ref{thm1} provides a connection between the SDP condition in \cite{fazlyab2019efficient} and our proposed simple algebraic condition in Theorem \ref{thm:main1}. It is possible to obtain new 1-Lipschitz network layers via providing new analytical solutions to \eqref{eq:LMI2}. It is our hope that our proposed SDP condition \eqref{eq:LMI2} can lead to many more 1-Lipschitz network structures in the future.
 
 
\section{Experiments}
\label{section:experiments}


\begin{table}[t]
  \centering
  \caption{This table presents the natural, provable accuracy as well as the number of parameters and training time of several concurrent work and our SLL networks on CIFAR10 dataset. All results for SLL networks are the result of the average of 3 trainings.}
    {\footnotesize
    \begin{tabular}{lrrrrrrr}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Models}}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Natural} \\ \textbf{Accuracy}}}}} & \multicolumn{4}{c}{\boldmath{}\textbf{Provable Accuracy ()}\unboldmath{}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Number of} \\ \textbf{Parameters}}}}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Time by} \\ \textbf{Epoch (s)}}}}} \\
   \cmidrule{3-6}
     &   &  &  &  & \multicolumn{1}{c}{} \\
    \midrule
      \textbf{GloRo} {\scriptsize \citep{leino2021globally}} & 77.0 & 58.4 & \multicolumn{1}{r}{-} & \multicolumn{1}{r}{-} & \multicolumn{1}{r}{-} & 8M & 6 \\
      \textbf{Local-Lip-B} {\scriptsize \citep{huang2021training}} & 77.4 & 60.7 & 39.0 & 20.4 & \multicolumn{1}{r}{-} & 2.3M & 8 \\
      \textbf{Cayley Large} {\scriptsize \citep{trockman2021orthogonalizing}} & 74.6 & 61.4 & 46.4 & 32.1 & \multicolumn{1}{r}{-} & 21M & 30 \\
      \textbf{SOC 20} {\scriptsize \citep{skew2021sahil}} & 78.0 & 62.7 & 46.0 & 30.3 & \multicolumn{1}{r}{-} & 27M & 52 \\
      \textbf{SOC+ 20} {\scriptsize \citep{singla2022improved}} & 76.3 & 62.6 & 48.7 & 36.0 & \multicolumn{1}{r}{-} & 27M & 52 \\
      \textbf{CPL XL} {\scriptsize \citep{meunier2022dynamical}} & 78.5 & 64.4 & 48.0 & 33.0 & \multicolumn{1}{r}{-} & 236M & 163 \\
      \textbf{AOL Large} {\scriptsize \citep{prach2022almost}} & 71.6 & 64.0 & 56.4 & 49.0 & 23.7 & 136M & 64 \\
    \midrule
      \textbf{SLL Small}   & 71.2 & 62.6 & 53.8 & 45.3 & 20.4 &  41M & 20 \\
      \textbf{SLL Medium}  & 72.2 & 64.3 & 56.0 & 48.3 & 23.9 &  78M & 35 \\
      \textbf{SLL Large}   & 72.7 & 65.0 & 57.3 & 49.7 & 25.4 & 118M & 55 \\
      \textbf{SLL X-Large} & 73.3 & 65.8 & 58.4 & 51.3 & 27.3 & 236M & 105 \\
    \bottomrule
    \end{tabular}}
  \label{table:main_results}\end{table}



\begin{table}[t]
  \centering
  \vspace{-0.2cm}
  \caption{This table presents the natural and provable accuracy of several concurrent works and our SLL networks on CIFAR100 and TinyImageNet datasets. SLL networks are averaged of 3 trainings.}
    {\footnotesize
    \begin{tabular}{llrrrrr}
    \toprule
     \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Datasets}}}  & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Models}}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Natural} \\ \textbf{Accuracy}}}}} & \multicolumn{4}{c}{\boldmath{}\textbf{Provable Accuracy ()}\unboldmath{}} \\
   \cmidrule{4-7}      &   &   &  &  &  & \multicolumn{1}{c}{} \\
    \midrule
    \multirow{9}[4]{*}{CIFAR100}
      & \textbf{Cayley Large} {\scriptsize \citep{trockman2021orthogonalizing}} & 43.3 & 29.2 & 18.8 & 11.0 & \multicolumn{1}{r}{-} \\
      & \textbf{SOC 20} {\scriptsize \citep{skew2021sahil}} & 48.3 & 34.4 & 22.7 & 14.2 & \multicolumn{1}{r}{-} \\
      & \textbf{SOC+ 20} {\scriptsize \citep{singla2022improved}} & 47.8 & 34.8 & 23.7 & 15.8 & \multicolumn{1}{r}{-} \\
      & \textbf{CPL XL} {\scriptsize \citep{meunier2022dynamical}} & 47.8 & 33.4 & 20.9 & 12.6 & \multicolumn{1}{r}{-} \\
      & \textbf{AOL Large} {\scriptsize \citep{prach2022almost}} & 43.7 & 33.7 & 26.3 & 20.7 & 7.8 \\
    \cmidrule{2-7}
      & \textbf{SLL Small}   & 44.9 & 34.7 & 26.8 & 20.9 &  8.1 \\
      & \textbf{SLL Medium}  & 46.0 & 35.5 & 27.9 & 22.2 &  9.1 \\
      & \textbf{SLL Large}   & 46.4 & 36.2 & 28.4 & 22.7 &  9.6 \\
      & \textbf{SLL X-Large} & 46.5 & 36.5 & 29.0 & 23.3 & 10.4 \\
    \midrule
    \multirow{6}[3]{*}{TinyImageNet}
      & \textbf{GloRo} {\scriptsize \citep{leino2021globally}} & 35.5 & 22.4 & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-}  \\
      & \textbf{Local-Lip-B (+MaxMin)} {\scriptsize \citep{huang2021training}} & 36.9 & 23.4 & 12.7 & 6.1 & 0.0 \\
      \cmidrule{2-7}
      & \textbf{SLL Small}   & 26.6 & 19.5 & 14.2 & 10.4 & 2.9 \\
      & \textbf{SLL Medium}  & 30.4 & 22.3 & 15.9 & 11.6 & 3.0 \\
      & \textbf{SLL Large}   & 31.3 & 23.0 & 16.9 & 12.3 & 3.3 \\
      & \textbf{SLL X-Large} & 32.1 & 23.2 & 16.8 & 12.0 & 3.2 \\
    \bottomrule
    \end{tabular}}
  \label{table:main_results2}\vspace{-0.2cm}
\end{table}




In this section, we present a comprehensive set of experiments with 1-Lipschitz neural networks based on our proposed \textit{SDP-based Lipschitz Layer}.
More specifically, we build 1-Lipschitz neural networks based on the following layer:

where  is a parameter matrix being either dense or a convolution,  forms a diagonal scaling matrix as described by Theorem \ref{thm:main3},  and  is the ReLU nonlinearity function.
We use the same architectures proposed by~\cite{meunier2022dynamical} with \textit{small}, \textit{medium}, \textit{large} and \textit{xlarge} sizes.
The architecture consists of several Conv-SLL and Linear-SLL.
For CIFAR-100, we use the Last Layer Normalization proposed by~\cite{singla2022improved} which improves the certified accuracy when the number of classes becomes large. 
Note that the layer presented in Equation~\eqref{equation:lipschitz_layer} can be easily implemented with convolutions following the same scaling as in ~\cite{prach2022almost}. Our experiments focus on the impact of the Lipschitz layer structures on certified robustness.
This complements a recent study on other aspects (e.g. projection pooling) of  robust networks~\citep{singla2022}.

\begin{wraptable}{r}{5.8cm}
    \centering
    \vspace{-0.0cm}
    \caption{The SLL architecture used for the experiments is inspired by~\citeauthor{meunier2022dynamical}.}
    \label{table:models_sizes}
    {\scriptsize
    \vspace{-0.3cm}
    \begin{tabular}{lcccc}
    \toprule
     & \textbf{S} & \textbf{M} & \textbf{L} & \textbf{XL} \\
    \midrule
    \textbf{Conv-SLL}        &   20 &   30 &   90 &  120 \\
    \textbf{Channels}        &   45 &   60 &   60 &   70 \\ 
    \textbf{Linear-SLL}      &    7 &   10 &   15 &   15 \\
    \textbf{Linear Features} & 2048 & 2048 & 4096 & 4096 \\
    \bottomrule
    \end{tabular}}
    \vspace{-0.1cm}
\end{wraptable}

\paragraph{Details on the architectures \& Hyper-parameters.}
Table~\ref{table:models_sizes} describes the detail of our Small, Medium, Large and X-Large architectures.
We trained our networks with a batch size of 256 over 1000 epochs with the data augmentation used by~\shortcites{prach2022almost}.
We use an Adam optimizer~\citep{diederik2014adam} with  learning rate and parameters  and  equal to  and  respectively and no weight decay.
We use a piecewise triangular learning rate scheduler to decay the learning rate during training.
We use the CrossEntropy loss as in~\cite{prach2022almost} with a temperature of  and an offset value .


\vspace{-0.2cm}
\paragraph{Results in terms of  Natural and Certified Accuracy on CIFAR10/100.}
First, we evaluate our networks (SLL) on CIFAR10 and CIAFR100 and compare the results against recent 1-Lipschitz neural network structures: Cayley, SOC, SOC+, CPL and AOL.
We also compare SLL with two other Lipschitz training approaches \citep{leino2021globally,huang2021training},  which do not guarantee prescribed global Lipschitz bounds during the training stage.
Table~\ref{table:main_results} presents the natural and certified accuracy with different radius of certification on CIFAR10. 
For a fair comparison, parameter number and training time per epoch for each method are also added to Table \ref{table:main_results}. Results on CIFAR100 are included in Table \ref{table:main_results2}.
We can see that our approach outperforms existing 1-Lipschitz architectures including AOL and CPL on certified accuracy for all values of .
We also observe that SLL-based 1-Lipschitz neural networks offer a good trade-off among previous approaches with respect to natural and certified accuracy. A detailed comparison is given below.

\vspace{-0.2cm}
\paragraph{Advantages of SLL over Cayley/SOC.} In general, it is difficult to compare the expressive power of non-residual and residual networks.
Hence we do not claim that with the same model size, SLL is more representative than Cayley or SOC which are not residual networks in the first place.
However, we believe that the current choice of  in SLL is very easy to calculate and hence leads to a scalable approach that allows us to train very large models with a reasonable amount of time.
For illustrative purposes, consider the comparison between SLL and Cayley in Table~\ref{table:main_results}.
We can see that SLL Small has more parameters than Cayley Large (41M vs. 21M) while being faster to train.
Indeed, the Cayley approach involves computing an expensive orthogonal projection (with a matrix inverse), while SOC requires to the computation of several convolutions at training and inference (from 6 to 12) to compute the exponential of a convolution up to a desired precision.
Hence the training time per epoch for Cayley Large and SOC is actually longer than SLL Small.
While being faster to train SLL Small still outperforms Cayley Large and SOC for all three values of .
In general, we think it is fair to claim that our approach is more scalable than previous approaches based on orthogonal layers, and allows the use of larger networks which leads to improvements in certified robustness. 

\vspace{-0.0cm}
\paragraph{Advantages of SLL over AOL/CPL.} 
With careful tuning of the offset value, SLL outperforms AOL for all values of . 
We experiment with several offset values: ,  and . The detailed results for all these different offset values are deferred to Table~\ref{table:offset_results} in the appendix.
In general, the offset value offers a trade-off between natural accuracy and robustness, thus, by choosing the offset value properly, SLL Large already achieves better results than AOL Large (notice that the training time per epoch for these two is roughly the same). SLL X-Large has even more improvements.
We can also see that SLL Large outperforms CPL XL for all values of  while being faster to train. 
For larger value of , the gain of SLL over CPL is remarkable (over 10\%).


\begin{wraptable}{r}{5.0cm}
    \centering
    \centering
    \vspace{-0.3cm}
    \caption{Inference time for Local-Lip-B and SLL X-Large on the full TinyImageNet validation with 4 GPUs.}
    \label{table:inference_time}
    {\footnotesize
    \begin{tabular}{lr}
    \toprule
    \textbf{Models} & \textbf{Inference Time} \\ 
    \midrule
    \textbf{Local-Lip-B} & 41 min \\
    \textbf{SLL X-Large} & 8 sec \\
    \bottomrule
    \end{tabular}}
\end{wraptable}

\vspace{-0.0cm}
\paragraph{Results on TinyImageNet.}
We have also implemented SLL on TinyImageNet (see Table \ref{table:main_results2}).
Previously, other 1-Lipschitz network structures including SOC, Cayley, AOL, and CPL have not been tested on TinyImageNet, and the state-of-the-art approach on TinyImageNet is the local Lipschitz bound approach \citep{huang2021local}.
We can see that SLL significantly outperforms this local Lipschitz approach for larger values of  (while generating similar results for the small  case).
Notice that the local Lipschitz approach \citep{huang2021local} is quite different from other 1-Lipschitz network methods in the sense that it has no guarantees on the Lipschitz constant of the resultant network and hence does not generate 1-Lipschtiz networks in the first place.
Furthermore, given that this approach does not guarantee a Lipschitz bound during training, a lot more computation needs to be performed during inference, making the certification process very time consuming. 
Table~\ref{table:inference_time} describes the inference time on TinyImageNet for this local Lipschitz approach and SLL X-large. 


\begin{table}[t]
  \centering
  {
  \caption{The table describes the empirical robustness of our SLL-based classifiers on CIFAR10 ans CIFAR100 datasets. The empirical robustness is measured with  \textit{AutoAttacks}. All results are
the average of 3 models.}
  \label{tab:autoattack}\begin{tabular}{lrccccccccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[4]{*}{\textbf{Models}}} &   & \multicolumn{4}{c}{\textbf{CIFAR10 -- \textit{AutoAttack} ()}} &   & \multicolumn{4}{c}{\textbf{CIFAR100 -- \textit{AutoAttack} ()}} \\
\cmidrule{3-6}\cmidrule{8-11}      &   &  &  &  &  &   &  &  &  &  \\
    \midrule
    \textbf{SLL Small}   &   & 68.1 & 62.5 & 56.8 & 35.0 &   & 40.7 & 35.2 & 30.4 & 17.0 \\
    \textbf{SLL Medium}  &   & 69.1 & 63.8 & 58.4 & 37.0 &   & 41.5 & 36.4 & 31.5 & 17.9 \\
    \textbf{SLL Large}   &   & 69.8 & 64.5 & 59.1 & 37.9 &   & 42.1 & 37.1 & 32.6 & 18.7 \\
    \textbf{SLL X-Large} &   & 70.3 & 65.4 & 60.2 & 39.4 &   & 42.7 & 37.8 & 33.2 & 19.5 \\
    \bottomrule
    \end{tabular}}
\end{table}

\vspace{-0.0cm}
\paragraph{Results on Empirical Robustness.}
We also provide results of our approach on empirical robustness against an ensemble of diverse parameter-free attacks (\ie, \textit{AutoAttacks}) developed by~\cite{croce2020reliable}.
Table~\ref{tab:autoattack} reports the empirical robustness accuracy for different levels of perturbations.
Although \textit{AutoAttacks} is a strong empirical attack consisting of an ensemble of several known attacks:  APGD\textsubscript{CE}, APGD\textsubscript{DLR}, FAB~\citep{croce2020minimally} and Square~\citep{andriushchenko2020square}.
We can observe that the measure robustness is high and well above the certified radius. Indeed, on CIFAR10, we observe a robustness ``gain'' of up to 4.5\%, 9.6\%, 14.1\% and 21.7\% for respectively, 36, 72, 108 and 255 -perturbations.


\vspace{-0.1in}
\section{Conclusion}
In this paper, we present a unifying framework for designing Lipschitz layers. Based on a novel algebraic perspective, we identify a common SDP condition underlying the developments of spectral normalization, orthogonality-based methods, AOL, and CPL.
Furthermore, we have shown that AOL and CPL can be re-derived and generalized using our theoretical framework.
From this analysis, we introduce a family of SDP-based Lipschitz layers (SLL) that outperforms previous work.
In the future, it will be interesting to investigate 
more expressive structures of  and extending our contributions to address multi-layer neural networks.

\subsubsection*{Acknowledgments}
 This work was performed using HPC resources from GENCIâ€“IDRIS (Grant 2021-AD011013259) and funded by the French National Research Agency (ANR SPEED-20-CE23-0025). A. Havens and B. Hu are generously supported by the NSF award CAREER-2048168. We also thank Kai Hu for emailing us about an issue in our previous implementation of SLL, which is now addressed in Appendix D.


\bibliography{bibliography,IQCandSOS}
\bibliographystyle{iclr2023_conference}

\newpage
\appendix


\section{Proofs}
In this section, we present the proofs for the theorems presented in our paper.

\subsection{Proof of Theorem \ref{thm:main1}}
To prove the first statement in Theorem \ref{thm:main1}, notice that we have

Based on our algebraic condition , we immediately have

Therefore, Statement 1 is true. 

To prove Statement 2 in Theorem \ref{thm:main1}, we need to use the property of the nonlinear activation function~. Notice that the condition  ensures that all the diagonal entries of the nonsingular matrix  are positive. Therefore,  is also a diagonal matrix whose diagonal entries are all positive.
For all the three activation functions listed in the above theorem,  is slope-restricted on , and the following inequality holds for any  \cite[Lemma 1]{fazlyab2019efficient}:

We can set  and , and the above inequality becomes

We can rewrite the above inequality as


Now we can apply the following argument:
0.5cm]
    =&\bmat{x-y \\ 2W T^{-1} \left(\sigma(W^\tp x+b)- \sigma(W^\tp y+b)\right)}\bmat{I & -I\\ -I & I}\bmat{x-y \\ 2W T^{-1}\left(\sigma(W^\tp x+b)- \sigma(W^\tp y+b)\right)}\0.5cm]
    \le & \bmat{x-y\\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp  \bmat{I & -2WT^{-1}\\ -2T^{-1}W^\tp & 4T^{-1}}\bmat{x-y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)},

    \norm{h(x)-h(y)}^2&\le \bmat{x-y\\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp  \bmat{I & 0\\ 0 & 0}\bmat{x-y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}\\
    &=\norm{x-y}^2,

    \tr(I-T^{-\frac{1}{2}}W^\tp W T^{-\frac{1}{2}})=\sum_i \left(1-\frac{|W^\tp W|_{ii}}{T_{ii}}\right), 

    \left\{z \in \mathbb{C}: |z-A_{ii}| \leq \sum_{j\neq i}|A_{ij} | \right\},\quad i \in \{1,\ldots, n\}.

  \bigcup_{i=1}^n \left\{z \in \mathbb{C}: |z-A_{ii}| \leq \sum_{j\neq i}|A_{ij} | \right\}
\label{eq:LMI1}
\bmat{H^\tp H & H^\tp G \\ G^\tp H & G^\tp G} \preceq  \bmat{I & -W \Lambda \\ -\Lambda W^\tp & 2\Lambda}.

 \bmat{x' - y' \\ \sigma(x')-\sigma(y')}^\tp  \bmat{0 & -\Lambda\\ -\Lambda & 2\Lambda}\bmat{x' - y' \\ \sigma(x')-\sigma(y')}\le 0.

 \bmat{W^\tp (x - y) \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp  \bmat{0 & -\Lambda\\ -\Lambda & 2\Lambda}\bmat{W^\tp (x-y) \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}\le 0.
\label{eq:slope_Gen}
 \bmat{x - y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp  \bmat{0 & -W \Lambda  \\ -\Lambda W^\tp & 2\Lambda}\bmat{x-y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}\le 0.

  \norm{h(x)-h(y)}^2 =& \norm{H(x-y)+\left(G\sigma(W^\tp x+b)-G \sigma(W^\tp y+b)\right)}^2\0.4cm]
    =& \bmat{x-y\\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}^\tp \bmat{H^\tp H & H^\tp G \\ G^\tp H & G^\tp G}\bmat{x-y \\ \sigma(W^\tp x+b)-\sigma(W^\tp y+b)}\
where the last step follows from the condition \eqref{eq:LMI1}.
Finally, we can combine the above inequality with \eqref{eq:slope_Gen} to show 

which is the desired conclusion. 


\section{Additional Results}
In this section, we will present some additional results and discuss the effect of the offset value on training.
The choice of 
the offset value will affect the performance of SLL significantly. Larger offset values will lead to decrease in natural accuracy and increase in certified robust accuracy. The details are documented in Table \ref{table:offset_results}.


\begin{table}[t]
  \centering
  {\scriptsize
  \caption{Additional results for CIFAR10 and CIFAR100 datasets with different offset values.}
  \label{table:offset_results}\begin{tabular}{clcrrrrlcrrrr}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Offset}}}  & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Models}}} &  \multicolumn{5}{c}{\textbf{CIFAR10}} &  & \multicolumn{5}{c}{\textbf{CIFAR100}} \\
   \cmidrule{3-7} \cmidrule{9-13} 
   & & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Natural} \\ \textbf{Accuracy}}}}} & \multicolumn{4}{c}{\textbf{Provable Accuracy ()}} &  & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Natural} \\ \textbf{Accuracy}}}}}  & \multicolumn{4}{c}{\boldmath{}\textbf{Provable Accuracy ()}\unboldmath{}} \\
  \cmidrule{4-7} \cmidrule{10-13}  
& & &  &  &  &  & & &  &  &  &  \\
      \midrule
      \multirow{4}[0]{*}{} 
    & SLL small  & 73.3 & 63.7 & 53.8 & 44.5 & 15.3  & & 46.7 & 35.2 & 26.4 & 20.1 & 5.9 \\
    & SLL medium & 74.0 & 64.7 & 54.9 & 45.3 & 16.0  & & 47.2 & 36.1 & 27.1 & 20.7 & 6.5 \\
    & SLL large  & 74.6 & 65.3 & 55.2 & 45.8 & 16.2  & & 47.9 & 36.7 & 27.9 & 21.3 & 6.7 \\
    & SLL xlarge & 75.3 & 65.7 & 55.8 & 46.1 & 16.3  & & 48.3 & 37.2 & 28.3 & 21.8 & 6.9 \\
    \midrule
      \multicolumn{1}{r}{\multirow{4}[0]{*}{}} 
    & SLL small  & 71.2 & 62.6 & 53.8 & 45.3 & 20.4  & & 44.9 & 34.7 & 26.8 & 20.9 &  8.1 \\
    & SLL medium & 72.2 & 64.3 & 56.0 & 48.3 & 23.9  & & 46.0 & 35.5 & 27.9 & 22.2 &  9.1 \\
    & SLL large  & 72.7 & 65.0 & 57.3 & 49.7 & 25.4  & & 46.4 & 36.2 & 28.4 & 22.7 &  9.6 \\
    & SLL xlarge & 73.3 & 65.8 & 58.4 & 51.3 & 27.3  & & 46.5 & 36.5 & 29.0 & 23.3 & 10.4 \\
    \midrule
      \multirow{4}[0]{*}{} 
    & SLL small  & 70.0 & 61.5 & 53.4 & 45.7 & 22.7 & & 44.6 & 34.5 & 26.5 & 21.0 &  8.6 \\
    & SLL medium & 70.8 & 63.1 & 55.4 & 48.3 & 25.8 & & 45.4 & 35.5 & 27.9 & 22.1 &  9.8 \\
    & SLL large  & 71.4 & 63.9 & 56.7 & 49.8 & 27.8 & & 45.9 & 36.0 & 28.2 & 22.7 & 10.3 \\ 
    & SLL xlarge & 71.6 & 64.6 & 57.7 & 50.8 & 29.6 & & 46.1 & 36.3 & 29.0 & 23.6 & 11.0 \\ 
    \bottomrule
  \end{tabular}
  }
\end{table}


\section{Further Discussions}

In this section, we provide some extra discussions on control-theoretic interpretations and possible extensions of our main results. 

\subsection{Control-theoretic Interpretations for Our Main Results}

Our work is inspired by the quadratic constraint approach \citep{Megretski1997} and the Lur'e system theory~\citep{Lure1944} developed in the control community. Specifically, the general network layer structure \eqref{eq:layer} can be viewed as a Lur'e system, which is a feedback interconnection of a linear dynamical system and a static nonlinearity. In this section, we try to make this connection more transparent.

Specifically, we can denote  and rewrite \eqref{eq:layer} as follows

which is exactly a shifted version of the Lur'e system. Therefore, it is not surprising that one can tailor the Lur'e system theory to study the properties of \eqref{eq:layer}. As a matter of fact, the previous developments in \cite{fazlyab2019efficient}
and \cite{revay2020lipschitz} were based on similar ideas. The main difference is that our paper requires solving SDPs analytically. In the controls literature, the formulated SDP conditions are typically solved numerically.

\subsection{A Variant of Theorem \ref{thm:main1}}

When discussing AOL and SLL, our main paper makes the assumption that  all the columns of  have at least one non-zero entry such that \eqref{eq:DAOL} is well defined. To drop this assumption, we can use the following variant of Theorem \ref{thm:main1}. 

\begin{theorem}\label{thm:main6}
For any weight matrix , if there exists a diagonal matrix  such that , then the two following statements hold true. 
\begin{enumerate}
  \item The mapping   is -Lipschitz.
  \item The mapping  is -Lipschitz if  is ReLU,  or sigmoid.
\end{enumerate}
\end{theorem}

The proof is omitted here, since we can use exactly the same argument as before. If  happens to be nonsingular, then we can set , and the above theorem exactly reduces to Theorem \ref{thm:main1}. However, the above result allows  to be singular. This is useful for designing AOL and SLL in the case where  has some zero columns. Suppose the -entry of  is equal to  for all . Then we can set the -th entry of  as  and still use \eqref{eq:DAOL} or \eqref{eq:SLLT} for other entries. It is straightforward to verify that the resultant  is still a feasible solution to , and then we can implement AOL or SLL accordingly. 


\subsection{A Variant of Theorem \ref{thm:main3}}

We can also modify Theorem \ref{thm:main3} for the non-residual network layer case. The following variant of Theorem \ref{thm:main3} is useful.


\begin{theorem}\label{thm:main7}
Let  be the weight matrix. 
Suppose  is a nonsingular diagonal matrix. 
If there exists some  diagonal matrix  with all positive diagonal entries such that  is a real diagonally dominant matrix with diagonal entries being all positive, then , and the function  is 1-Lipschitz. 
\end{theorem}
The proof is trivial and hence omitted. Based on the above result, it is possible that one can use \eqref{eq:SLLT} to construct a non-residual layer that can still improve upon AOL.

\newpage

\section{Erratum -- Correction of SLL implementation}

The authors of \cite{hu2023recipe} discovered an issue in the original implementation of the SLL layers. We are grateful to them for bringing this to our attention. Now we elaborate on this issue and provide the corrected implementation of SLL.  

Recall that our paper proposed the following SLL layer to build 1-Lipschitz networks:

where  is the ReLU nonlinearity function.
The parameters of this layer consist of ,  and . 
One can prove that this layer is 1-Lipschitz.
However, the division on the parameter  can make the training process unstable.
Previously, SLL was implemented as follows:

where  is added to the denominator to avoid dividing by . 
However, Equation~\eqref{eq:wrong1} is not 1-Lipschitz anymore due to the appearance of . 




\begin{table}[t]
  \centering
  \caption{This table presents the natural and corrected provable accuracy of our SLL networks on CIFAR10 and CIFAR100 datasets. SLL networks are averaged of 3 trainings.}
    {\footnotesize
    \begin{tabular}{lllrrrrr}
    \toprule
     \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Datasets}}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Training}}}  & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Models}}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{\makecell{\textbf{Natural} \\ \textbf{Accuracy}}}}} & \multicolumn{4}{c}{\boldmath{}\textbf{Provable Accuracy ()}\unboldmath{}} \\
   \cmidrule{5-8}      &   &  & &  &  &  & \multicolumn{1}{c}{} \\
    \midrule
    \multirow{8}[0]{*}{CIFAR10} &
    \multirow{4}[0]{*}{\textbf{Surrogate Training}} 
      & \textbf{SLL Small} & 71.3 & 62.7 & 53.8 & 45.4 & 20.4 \\
    & & \textbf{SLL Medium} & 72.0 & 63.6 & 54.7 & 46.4 & 21.0 \\
    & & \textbf{SLL Large} & 72.6 & 64.1 & 55.4 & 46.9 & 21.3 \\
    & & \textbf{SLL X-Large} & 73.2 & 64.6 & 55.8 & 47.3 & 21.5  \\
    \cmidrule{2-8}
    & \multirow{4}[0]{*}{\textbf{Exponential Scaling}} 
      & \textbf{SLL Small}   & 71.5 & 62.8 & 53.7 & 45.2 & 19.4 \\
    & & \textbf{SLL Medium}  & 72.2 & 63.7 & 54.7 & 46.1 & 20.1 \\
    & & \textbf{SLL Large}   & 72.6 & 64.2 & 55.1 & 46.6 & 20.3 \\
    & & \textbf{SLL X-Large} & 73.3 & 64.8 & 55.7 & 47.1 & 20.6 \\
    \midrule
    \multirow{8}[0]{*}{CIFAR100} & 
    \multirow{4}[0]{*}{\textbf{Surrogate Training}} 
      & \textbf{SLL Small} & 45.8 & 34.7 & 26.5 & 20.4 & 7.2 \\
    & & \textbf{SLL Medium} & 46.5 & 35.6 & 27.3 & 21.1 & 7.7 \\
    & & \textbf{SLL Large} & 46.9 & 36.2 & 27.9 & 21.6 & 7.9 \\
    & & \textbf{SLL X-Large} & 47.6 & 36.5 & 28.2 & 21.8 & 8.2 \\
    \cmidrule{2-8}
    & \multirow{4}[0]{*}{\textbf{Exponential Scaling}} 
       & \textbf{SLL Small}   & 45.8 & 34.8 & 26.5 & 20.2 & 7.2 \\
     & & \textbf{SLL Medium}  & 46.8 & 35.8 & 27.3 & 21.0 & 7.7 \\
     & & \textbf{SLL Large}   & 47.2 & 36.2 & 27.8 & 21.5 & 7.9 \\
     & & \textbf{SLL X-Large} & 47.8	& 36.7 & 28.3 & 22.2 & 8.3 \\
    \bottomrule
    \end{tabular}}
  \label{table:corrected_results}\end{table}

There are several ways to fix the above issue.
For example, we can still use \eqref{eq:wrong1} for training, and then substitute the resultant values of   to \eqref{eq:reSLL} for evaluating the certified robust accuracy. In other words, we can use \eqref{eq:wrong1} as a surrogate for stable training of \eqref{eq:reSLL}.  The results of \emph{surrogate training} are presented in Table~\ref{table:corrected_results}.
One other way to address the above issue is to use the exponential scaling:

The training of the above layer is stable, and the resultant network is indeed 1-Lipschitz. Table~\ref{table:corrected_results} also presents the corrected results obtained using the above exponential scaling form.
We can see that the results from surrogate training in Table \ref{table:corrected_results} are similar to those obtained using the exponential scaling. 
We can also observe from Table \ref{table:corrected_results} that SLL outperforms AOL on CIFAR100. For CIFAR10 with , SLL still outperforms AOL. However, For CIFAR10 with  or , AOL achieves better results than SLL.
The Github repo \href{https://github.com/araujoalexandre/Lipschitz-SLL-Networks}{\texttt{https://github.com/araujoalexandre/Lipschitz-SLL-Networks}} has been corrected accordingly.




Finally, it is worth mentioning that Kai Hu's email has pointed out that the following layer is also 1-Lipschitz and can be trained in a stable manner:

In comparison to our current results in Table \ref{table:corrected_results}, the certified robustness results from the above variant (reported in Kai Hu's email) are worse on CIFAR100 and similar on CIFAR10.







\end{document}
