\documentclass[paper_2425.tex]{subfiles}

\renewcommand \thepart{}
\renewcommand \partname{}

\begin{document}

\appendix


\twocolumn[{\centering
\Large\textbf{Omni-GAN and Omni-INR-GAN \\Supplementary Material}
\\
[1.5em]
}]

\begin{center}
  \doparttoc \faketableofcontents \part{}
  \parttoc
  \vspace{2cm}
\end{center}




\begin{figure*}[t]
\begin{subfigure}{0.5\textwidth}
    \centering
\includegraphics[width=0.49\linewidth]{figures/save_grad_unified_loss_snsp/spsn_dsn.pdf}
    \includegraphics[width=0.49\linewidth]{figures/save_grad_unified_loss_snsp/spsn_dsp.pdf}
\caption{}
\end{subfigure}
  \hspace{0.1cm}
  \begin{subfigure}{.5\textwidth}
    \centering
\includegraphics[width=0.49\linewidth]{figures/save_grad_unified_loss_spsp/spsp_dsp0.pdf}
    \includegraphics[width=0.49\linewidth]{figures/save_grad_unified_loss_spsp/spsp_dsp1.pdf}
\caption{}
\end{subfigure}
  \vspace{-0.3cm}
  \caption{Gradients of the omni-loss. (a) Gradients \wrt $s^{(n)}$ and $s^{(p)}$ are independent. (b) Gradients \wrt $s^{(p)}_{k}$, $\{k=0,1,\dots\}$, are automatically balanced. Please see the text in Sec.~\ref{sec:gradients} for details. This figure is inspired by \cite{sun2020Circle}.}
  \vspace{-0.3cm}
  \label{fig:gradient}
\end{figure*}

\section{Derivation from Unified Loss to Omni-loss.}
\label{apx:derivation_omniloss}


\subsection{Derivation of Omni-loss}

The unified loss~\cite{sun2020Circle} is defined as
\begin{equation}
  \resizebox{\hsize}{!}{$
      \begin{aligned}
        \mathcal{L}_{\text {uni}} & =\log \left[1+\sum_{s_{i}^{(n)} \in \sS_{\text{neg}}} \sum_{s_{j}^{(p)} \in \sS_{\text{pos}}} e^ {\left(\gamma\left(s_{i}^{(n)}-s_{j}^{(p)}+m\right)\right)} \right]                                         \\
                                  & =  \log \left[1+\sum_{s_{i}^{(n)} \in \sS_{\text{neg}}} e^ {\left(\gamma\left(s_{i}^{(n)}+m\right)\right)} \sum_{s_{j}^{(p)} \in \sS_{\text{pos}}} e^ {\left(\gamma\left(-s_{j}^{(p)}\right)\right)}\right],
      \end{aligned}
    $}
  \label{apx:equ:unified_loss}
\end{equation}
where $\gamma$ stands for a scale factor, and $m$ for a margin between positive and negative scores. $\sS_{\text{pos}}=\{\evs_1^{(p)}, \cdots, \evs_K^{(p)}\}$ and $\sS_{\text{neg}}=\{\evs_1^{(n)}, \cdots, \evs_L^{(n)}\}$ denote positive score set and negative score set, respectively.
\myeqref{apx:equ:unified_loss} aims to maximize $s^{(p)}$ and to minimize $s^{(n)}$.


The omni-loss is defined as
\begin{equation}
  \resizebox{\hsize}{!}{$
      \begin{aligned}
        \mathcal{L}_{\text {omni}}\left(\vx, \vy\right)
        = & \log \left(1 + \sum_{i \in \sI_{\text{neg}}} e^{\evs_i(\vx)} \right)
        + \log \left(1 + \sum_{j \in \sI_{\text{pos}}} e^{-\evs_j(\vx)} \right),
      \end{aligned}
    $}
  \label{apx:equ:multi_label_loss}
\end{equation}
where $\sI_{\text{neg}}$ is a set consisting of indexes of negative scores ($|\sI_{\text{neg}}|=L$), and $\sI_{\text{pos}}$ consists of indexes of positive scores ($|\sI_{\text{pos}}|=K$).

\myeqref{apx:equ:multi_label_loss} is a special case of \myeqref{apx:equ:unified_loss}, which has been proved by ~\cite{su2020multilabelloss}. For the convenience of readers in the English community, we provide our proof here. Let $\gamma$ be $1$ and $m$ be $0$, then
\begin{align}
  \mathcal{L}_{\text {uni}}                                     = & \log \left[1+\sum_{s_{i}^{(n)} \in \sS_{\text{neg}}} e^ {s_{i}^{(n)}} \sum_{s_{j}^{(p)} \in \sS_{\text{pos}}} e^ {-s_{j}^{(p)}}\right]                       \label{apx:equ:unified_loss_r1_m0}                                                                                                                  \\
  =                                                               & \log \left[ 1 + e                                                                                                                      ^ { \log \left( \sum_{s_{i}^{(n)} \in \sS_{\text{neg}}} e^ {s_{i}^{(n)}} \sum_{s_{j}^{(p)} \in \sS_{\text{pos}}} e^ {-s_{j}^{(p)}} \right) } \right]   \nonumber          \\
  =                                                               & \text{softplus} \left[    {                                                                                                                 \log \left( \sum_{s_{i}^{(n)} \in \sS_{\text{neg}}} e^ {s_{i}^{(n)}} \sum_{s_{j}^{(p)} \in \sS_{\text{pos}}} e^ {-s_{j}^{(p)}} \right) } \right]           \nonumber \\
  =                                                               & \text{softplus} \left[    {                                                                                                                 \log \left( \sum_{s_{i}^{(n)} \in \sS_{\text{neg}}} \sum_{s_{j}^{(p)} \in \sS_{\text{pos}}} e^ {s_{i}^{(n)} - s_{j}^{(p)}} \right) } \right]               \nonumber \\
  \approx                                                         & \left[    {                                                                                                                 \log \left( \sum_{s_{i}^{(n)} \in \sS_{\text{neg}}} \sum_{s_{j}^{(p)} \in \sS_{\text{pos}}} e^ {s_{i}^{(n)} - s_{j}^{(p)}} \right) } \right]_{+}               \nonumber  ,
\end{align}
where $[\cdot]_+$ means $\max(\cdot, 0)$.

According to $\log \sum_{i=1}^{n} e^{x_i} \approx \max(x_1, x_2, \dots, x_n) $, we get
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\text {uni}}                                     \approx & \left[    {                                                                                                                 \max_{s_{i}^{(n)} \in \sS_{\text{neg}}, s_{j}^{(p)} \in \sS_{\text{pos}}} {s_{i}^{(n)} - s_{j}^{(p)}}  } \right]_{+}      ,
  \end{aligned}
\label{apx:equ:unified_loss_approx}
\end{equation}
where minimizing \myeqref{apx:equ:unified_loss_approx} makes the smallest $s_{j}^{(p)}$ greater than the largest $s_{i}^{(n)}$.

Let $\sS^{(1)}_{\text{pos}}=\{0\}$ and $\sS^{(1)}_{\text{neg}}=\{\evs_1^{(n)}, \cdots, \evs_L^{(n)}\}$. According to \myeqref{apx:equ:unified_loss_r1_m0}, we get
\begin{equation}
\begin{aligned}
    \mathcal{L}^{(1)}_{\text {uni}}                                     = & \log \left[1+\sum_{s_{i}^{(n)} \in \sS^{(1)}_{\text{neg}}} e^ {s_{i}^{(n)}} \sum_{s_{j}^{(p)} \in \{0\}} e^ {-s_{j}^{(p)}}\right] \\
    =                                                                     & \log \left[1+\sum_{s_{i}^{(n)} \in \sS^{(1)}_{\text{neg}}} e^ {s_{i}^{(n)}} e^ {0}\right]                                         \\
    =                                                                     & \log \left[1+\sum_{s_{i}^{(n)} \in \sS^{(1)}_{\text{neg}}} e^ {s_{i}^{(n)}}\right],
  \end{aligned}
\label{apx:equ:omni_loss_l1}
\end{equation}
where from \myeqref{apx:equ:unified_loss_approx} we know that minimizing \myeqref{apx:equ:omni_loss_l1} makes $s_{i}^{(n)}$ less than $0$.

Let $\sS^{(2)}_{\text{pos}}=\{\evs_1^{(p)}, \cdots, \evs_K^{(p)}\}$ and $\sS^{(2)}_{\text{neg}}=\{0\}$. According to \myeqref{apx:equ:unified_loss_r1_m0}, we get
\begin{equation}
\begin{aligned}
    \mathcal{L}^{(2)}_{\text {uni}}
    = & \log \left[1+\sum_{s_{i}^{(n)} \in \{0\} } e^ {s_{i}^{(n)}} \sum_{s_{j}^{(p)} \in \sS^{(2)}_{\text{pos}}} e^ {-s_{j}^{(p)}}\right] \\
    = & \log \left[1+ e^ {0} \sum_{s_{j}^{(p)} \in \sS^{(2)}_{\text{pos}}} e^ {-s_{j}^{(p)}}\right]                                        \\
    = & \log \left[1+ \sum_{s_{j}^{(p)} \in \sS^{(2)}_{\text{pos}}} e^ {-s_{j}^{(p)}}\right]                                               \\
  \end{aligned}
\label{apx:equ:omni_loss_l2}
\end{equation}
where minimizing \myeqref{apx:equ:omni_loss_l2} makes $s_{j}^{(p)}$ greater than $0$.

Adding \myeqref{apx:equ:omni_loss_l1} and \myeqref{apx:equ:omni_loss_l2}, we get
\begin{equation}
  \resizebox{\hsize}{!}{$
      \begin{aligned}
        \mathcal{L}_{\text {omni}}
        = & \log \left[1+\sum_{s_{i}^{(n)} \in \sS^{(1)}_{\text{neg}}} e^ {s_{i}^{(n)}}\right] + \log \left[1+ \sum_{s_{j}^{(p)} \in \sS^{(2)}_{\text{pos}}} e^ {-s_{j}^{(p)}}\right],
      \end{aligned}
    $}
  \label{apx:equ:omni_loss}
\end{equation}
where minimizing \myeqref{apx:equ:omni_loss} makes $s_{i}^{(n)}$ less than $0$ and $s_{j}^{(p)}$ greater than $0$.
We finish the derivation.


\subsection{Gradient Analysis}
\label{sec:gradients}

The gradients of omni-loss have two properties: on one hand, the gradients \wrt $s^{(n)}$ and $s^{(p)}$ are independent; on the other hand, the gradients \wrt $s^{(p)}_{k}$ (or $s^{(n)}_k$), $\{k=0,1,\dots\}$, are automatically balanced. To illustrate these properties, we visualize the gradients of omni-loss. Fig.~\ref{fig:gradient}a shows a case that only contains one $s^{(n)}$ and one $s^{(p)}$. $A$, $B$, and $C$ have the same $s^{(p)}$, which is $0$, but different $s^{(n)}$ (\ie, $4, 0, -4$, respectively). As a result, the gradients \wrt $s^{(p)}$ at these three points are the same (\ie, $0.5$). Nevertheless, the gradients \wrt $s^{(n)}$ at these three points are different. For example, the gradient \wrt $s^{(n)}$ at $A$ is largest (equal to $0.98$).  The reason for this is that the objective of omni-loss is to minimize $s^{(n)}$. Thus the larger the $s^{(n)}$, the larger the gradient \wrt $s^{(n)}$.

In Fig.~\ref{fig:gradient}b, we show the ability of omni-loss to automatically balance gradients. We consider a case with only two positive labels, namely $s^{(p)}_0$ and $s^{(p)}_1$. We can observe that for $A$, its $s^{(p)}_0$ is smaller than $s^{(p)}_1$ (\ie, -2 vs. 0). As a result, the gradients \wrt $s^{(p)}_0$ is larger than that \wrt $s^{(p)}_1$ (\ie, 0.79 vs. 0.11), meaning that the omni-loss try to increase $s^{(p)}_0$ with higher superiority. A similar analysis applies to $C$ as well. For $B$, since $s^{(p)}_0$ and $s^{(p)}_1$ are equal, the gradients of them are also equal ($0.33$).


\section{Gradient Penalty for Classification-based cGANs}
\label{apx:sec:gp}

We investigate whether gradient penalty will alleviate early collapse. We chose AC-GAN~\cite{odena2017Conditional}, the currently widely known classification-based cGAN, as the testbed, and evaluate three gradient penalty methods: WGAN-GP~\cite{gulrajani2017Improved}, WGAN-div~\cite{mescheder2018Which}, and R1 regularization~\cite{wu2018Wasserstein}. Because cGANs are more likely to collapse when the number of categories is large, we evaluate them on CIFAR100 instead of CIFAR10. As shown in Fig.~\ref{apx:fig:acgan_gp}, none of the three gradient penalty methods can prevent AC-GAN from collapsing. We emphasize that computing gradient penalties will introduce additional computational overhead during GAN's training, which is very unfriendly to large-scale datasets such as ImageNet. However, weight decay effectively alleviates the collapse problem without adding any additional training overhead.


\begin{figure}[t]
\centering
  \begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth,height=3.3cm]{figures/ACGAN_GP/ACGAN_gradient_penalty_c100_FID.pdf}
    \caption{FID on CIFAR100}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth,height=3.3cm]{figures/ACGAN_GP/ACGAN_gradient_penalty_c100_IS.pdf}
    \caption{IS on CIFAR100}
  \end{subfigure}
  \vspace{-0.2cm}
  \caption{FID and IS on CIFAR100. We test three gradient penalty methods (\ie, WGAN-GP, WGAN-div, and R1 regularization), none of which can alleviate the collapse issue of AC-GAN.}
  \label{apx:fig:acgan_gp}
\end{figure}


\section{Improved AC-GAN (ImAC-GAN)}
\label{apx:sec:imacgan}

Auxiliary classifier GAN (AC-GAN)~\cite{odena2017Conditional} uses an auxiliary classifier to enhance the standard GAN model. Its objective function consists of tow parts: the GAN loss, $\mathcal{L}_{\text{GAN}}$, and the classification loss, $\mathcal{L}_{\text{cls}}$:
\begin{equation}
  \begin{aligned}
    \mathcal{L}_{\text{GAN}}=
     & \E \left[ \log P\left(\rg=\text{real} \mid \vx_{\text{real}}\right)\right] + \\
     & \E \left[ \log P\left(\rg=\text{fake} \mid \vx_{\text{fake}}\right)\right],
  \end{aligned}
\end{equation}
\begin{equation}
  \begin{array}{r}
    \mathcal{L}_{\text{cls}}=
    \E \left[ \log P\left(\rg=c \mid \vx_{\text{real}}\right) \right]+
    \E \left[ \log P\left(\rg=c \mid \vx_{\text{fake}}\right) \right],
  \end{array}
\end{equation}
where $\rg$ is a random variable denoting the class label and $c$ is the ground truth label of $\vx$. $\vx_{\text{real}}$ and $\vx_{\text{fake}}$ represent a real image and a generated image respectively. The discriminator $D$ of AC-GAN is trained to maximize $\mathcal{L}_\text{GAN}+\mathcal{L}_\text{cls}$, and the generator is trained to maximize $\mathcal{L}_{\text{cls}}-\mathcal{L}_\text{GAN}$.

The discriminator loss of AC-GAN is not optimal. We give a slightly modified version below. Suppose the dataset owns $C$ categories, then the discriminator is trained to maximize
\begin{equation}
  \begin{aligned}
    \mathcal{L}_{{D}} = & \mathcal{L}_{\text{GAN}} +                                         \\
                        & \E \left[ \log P\left(\rg=c \mid \vx_{\text{real}}\right) \right]+
    \E \left[ \log P\left(\rg=C \mid \vx_{\text{fake}}\right) \right],
  \end{aligned}
\end{equation}
where $c \in \{0, 1, \dots, C-1\}$ is the ground truth class label of $\vx_{\text{real}}$, and $\rg=C$ means that $\vx_{\text{fake}}$ belongs to the fake class. To sum up, we use an additional class to represent the generated image. In practice, this is achieved by setting the dimension of the fully connected layer of the auxiliary classification layer to be $C+1$ rather than $C$.

The objective function of the generator is consistent with that of the original AC-GAN, \ie, maximizing
\begin{equation}
  \begin{aligned}
    \mathcal{L}_{{G}} =  - \mathcal{L}_{\text{GAN}} +
    \E \left[ \log P\left(\rg=c_{\text{fake}} \mid \vx_{\text{fake}}\right) \right],
  \end{aligned}
\end{equation}
where $c_{\text{fake}}$ is the class label used by the generator to generate $\vx_{\text{fake}}$.

We name this improved version of AC-GAN ImAC-GAN. As shown in the paper, ImAC-GAN is comparable to Omni-GAN, both of which achieve superior performance compared to projection-based cGANs. However, because ImAC-GAN uses cross-entropy as the loss function for classification, it can only handle the case where the sample has a positive label. Omni-GAN uses omni-loss, essentially a multi-label classification loss, which naturally supports handling samples with one positive label or multiple positive labels. We will give an example of generating images with multiple positive labels in Sec.~\ref{apx:sec:multi_label_D}.

\section{Technical Details of Omni-INR-GAN}
\label{apx:sec:omni_inr_gan}


Learning image prior model is helpful for image restoration and manipulation, such as denoising, inpainting, and harmonizing. Deep generative prior (DGP)~\cite{pan2020Exploiting} showed the potential of employing the generator prior captured by a pre-trained GAN model (\ie, a BigGAN model trained on a large-scale image dataset, ImageNet). However, BigGAN can only output images with a fixed aspect ratio, limiting the practical application of DGP. To make the pre-trained GAN model more flexible for downstream tasks, we propose a new GAN named Omni-INR-GAN, which can output images with any aspect ratio and any resolution.

Images are usually represented by a set of pixels with fixed resolution. A popular method named implicit neural representation (INR) is prevalent in the 3D field~\cite{park2019DeepSDF,mescheder2019Occupancy,chen2019Learninga}. Recently, people introduced the INR method to 2D images~\cite{chen2020Learning,skorokhodov2020Adversarial}. As shown in Fig.~\ref{apx:fig:omni_inr_gan} (a), the INR of an image directly maps ($x$, $y$) coordinates to image's RGB pixel values. Since the coordinates are continuous, once we get the INR of an image, we can get images of arbitrary resolutions by sampling different numbers of coordinates.

Inspired by the local implicit image function (LIIF)~\cite{chen2020Learning}, we use INR to enhance Omni-GAN, with the goal of enabling the generator to output images with any aspect ratios and any resolution. We name our method Omni-INR-GAN.
As shown in Fig.~\ref{apx:fig:omni_inr_gan} (b), we keep the backbone of the generator network unchanged and employ an INR network for the output layer. Let $\mM \in \mathbb{R}^{C \times H \times W}$ represent the output feature map of the backbone, $f_\theta$ be the implicit neural function. Then the RGB signal at ($x$, $y$) coordinate is given by $s=f_\theta(\mM_{x, y}, x, y)$, where $\mM_{x, y}$ stands for the feature vector at ($x$, $y$). Note that since $x$ and $y$ can be any real numbers, $\mM_{x, y}$ may not exist in $\mM$. In such a case, we adopt the bilinear interpolation of the four feature vectors near ($x$, $y$) as the feature at ($x$, $y$).

Omni-INR-GAN can generate images with any aspect ratio, so as to be more friendly to downstream tasks like image restoration and manipulation. After trained on the large-scale dataset ImageNet, Omni-INR-GAN can be combined with DGP to do restoration tasks. Omni-INR-GAN eliminates cropping operations before image restoration, making it possible to repair the entire image directly. Since the generator has seen considerable natural images, utilizing the generator prior can facilitate downstream tasks significantly.


\begin{figure}[t]
  \begin{center}
\includegraphics[width=\linewidth]{./figures/omni_inr_gan.pdf}
  \end{center}
  \vspace{-0.5cm}
  \caption{(a) An example of an image represented in INR form. A fully connected network maps coordinates $(x, y)$ to pixel values $(r, g, b)$. (b) Using an INR network to enhance the generator so that the generator can output images with any resolution and any aspect ratio.}
  \label{apx:fig:omni_inr_gan}
\end{figure}



\section{An Example of Multi-label Discriminator}
\label{apx:sec:multi_label_D}


Omni-loss is essentially a multi-label classification loss and naturally supports classification with multiple positive labels. To verify the ability of Omni-GAN for generating samples with multiple positive labels, we construct a mixed dataset containing images of digits from two distinct domains, namely MNIST~\cite{lecun1998gradient} of handwritten digits and SVHN~\cite{netzer2011reading} of house numbers. Some example images from the datasets are shown in Fig.~\ref{apx:fig:mnist_svhn}. In this setting, the discriminator needs to predict three attributes, class (recognizing digits), domain, and reality.

Let us take images of MNIST as an example, and show how to set the loss for the discriminator. As for SVHN, the case is analogous. Suppose $\vx_{\text{real}}$ is an image sampled from MNIST, its multi-label vector is given by
\begin{equation}
  \vy_{\text{real}} = [\underbrace{-1, \dots, 1_{\text{gt}}, \dots, -1}_{\text{class}}, \underbrace{1_{\text{mnist}}, -1}_{\text{domain}}, \underbrace{1_{\text{real}}, -1}_{\text{reality}}],
  \label{apx:equ:oneside-multi_label_real}
\end{equation}
where $-1$ means the corresponding score belongs to the negative set, and $1$ to the positive set. As can be seen, $\vy_{\text{real}}$ possesses three positive labels. The multi-label vector for $\vx_{\text{fake}}$ is then given by
\begin{equation}
  \vy_{\text{fake}} = [\underbrace{-1, \dots, -1, \dots, -1}_{\text{class}}, \underbrace{-1, -1}_{\text{domain}}, \underbrace{-1, 1_{\text{fake}}}_{\text{reality}}],
  \label{apx:equ:oneside-multi_label_fake}
\end{equation}
which is a one-hot vector with the last element being $1$. The discriminator loss is given by
\begin{equation}
\begin{aligned}
    \mathcal{L}_{D}
    = & \E_{\vx_{\text{real}} \sim p_{\text{d}}} \left[\mathcal{L}_{\text {omni}}\left(\vx_{\text{real}}, \vy_{\text{real}}\right)\right]    \\
      & + \E_{\vx_{\text{fake}} \sim p_{\text{g}}} \left[\mathcal{L}_{\text {omni}}\left(\vx_{\text{fake}}, \vy_{\text{fake}}\right)\right].
  \end{aligned}
\label{apx:equ:D_loss}
\end{equation}

For generator, its goal is to cheat the discriminator. The multi-label vector for $\vx_{\text{fake}}$ is given by
\begin{equation}
  \vy_{\text{fake}}^{(\text{G})} = [\underbrace{-1, \dots, 1_{\text{G}}, \dots, -1}_{\text{class}}, \underbrace{1_{\text{mnist}}, -1}_{\text{domain}}, \underbrace{1_{\text{real}}, -1}_{\text{reality}}],
  \label{apx:equ:oneside-multi_label_G}
\end{equation}
where $1_{\text{G}}$ is $1$ if its index in the vector is equal to the label adopted by the generator to generate $\vx_{\text{fake}}$, otherwise $-1$. The generator loss is given by.
\begin{equation}
\begin{aligned}
    \mathcal{L}_{G}
    = \E_{\vx_{\text{fake}} \sim p_{\text{g}}} \left[\mathcal{L}_{\text {omni}}\left(\vx_{\text{fake}}, \vy_{\text{fake}}^{(\text{G})}\right)\right].
  \end{aligned}
\label{apx:equ:G_loss}
\end{equation}

We experimentally found that this multi-label discriminator can instruct the generator to generate images from different domains. Some generated images are shown in Fig.~\ref{apx:fig:generated_mnist_svhn}. We must emphasize that this is only a preliminary experiment to verify the function of the multi-label discriminator. We look forward to applying the multi-label discriminator to other tasks in the future, such as translation between images in different domains, domain adaptation, \etc.



\begin{figure}[tbp]
  \centering
  \begin{minipage}[t]{\linewidth}
    \begin{subfigure}{0.49\linewidth}
      \centering
\includegraphics[width=\linewidth]{figures/save_mnist_svhn_generated/mnist.png}
\caption{MNIST}
\end{subfigure}
    \begin{subfigure}{0.49\linewidth}
      \centering
\includegraphics[width=\linewidth]{figures/save_mnist_svhn_generated/svhn.png}
\caption{SVHN}
\end{subfigure}
    \vspace{-0.3cm}
    \caption{Real images sampled from the dataset.}
    \vspace{0.5cm}
    \label{apx:fig:mnist_svhn}
  \end{minipage}\\
  \begin{minipage}[t]{\linewidth}
    \begin{subfigure}{0.49\linewidth}
      \centering
\includegraphics[width=\linewidth]{figures/save_mnist_svhn_generated/Gz_mnist.png}
\caption{MNIST}
\end{subfigure}
    \begin{subfigure}{0.49\linewidth}
      \centering
\includegraphics[width=\linewidth]{figures/save_mnist_svhn_generated/Gz_svhn.png}
\caption{SVHN}
\end{subfigure}
    \vspace{-0.3cm}
    \caption{Images generated by a generator which is guided by a multi-label discriminator.}
    \vspace{0.5cm}
    \label{apx:fig:generated_mnist_svhn}
  \end{minipage}
\end{figure}





\section{Additional Results on CIFAR}
\label{apx:sec:results_cifar}

\subsection{Over-fitting of the Discriminator}

\begin{figure}[!t]
  \begin{subfigure}{0.49\linewidth}
    \centering
\includegraphics[width=\linewidth]{figures/save_early_collapse_Dx_on_cifar100/D_logits_biggan_cifar100.pdf}
\caption{}
  \end{subfigure}
\begin{subfigure}{.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/save_early_collapse_Dx_on_cifar100/D_logits_biggan_weight_decay_cifar100.pdf}
    \caption{}
  \end{subfigure}
  \vspace{-0.3cm}
  \caption{The raw logits of $D(\vx)$ and the corresponding FID score of a projection-based cGAN are plotted in the same figure. The black dashed line indicates where the minimum FID is reached. (a) training without weight decay. (b) training with weight decay.  The figures of $D(\vx)$ are inspired by~\cite{karras2020Training}.}
  \label{apx:fig:early_collapse}
\end{figure}


Karras \etal~\cite{karras2020Training} found that the discriminator overfits the training dataset, which will lead to incorrect gradients provided to the generator. Thus the training diverges. To verify that the collapse of the projection-based cGAN is due to the over-fitting of the discriminator, we plotted the scalar output of the discriminator, $D(\vx)$, over the course of training. We utilized the test set of CIFAR100 containing $10,000$ images as the verification set, which was not used in the training.

As shown in Fig.~\ref{apx:fig:early_collapse}a, obviously, as training progresses, the $D(\vx)$ of the validation set tends to that of the generated images, substantiating that the discriminator overfits the training data. We also plotted the FID curve in the same figure. We can see that the training commences diverging when showing about $20$M real images (\ie, around $400$ epoch) to the discriminator. The best FID is obtained when approximately $15$M real images are shown to the discriminator.

In Fig.~\ref{apx:fig:early_collapse}b, we show the $D(\vx)$ and FID after applying weight decay to the projection-based discriminator. We can find that although the discriminator still overfits the training data, the training dose not collapse during the whole training process (the minimum FID, $9.74$, is reached at the end of the training).




\subsection{Comparison of One-sided Omni-GAN and Projection-based GAN on CIFAR10}
\label{apx:sec:one_side}

\begin{figure}[!t]
  \begin{subfigure}{0.49\linewidth}
    \centering
\includegraphics[width=\linewidth]{figures/save_oneside_cifar10_FID/cGAN_oneside_FID_c10.pdf}
\caption{}
\end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \centering
\includegraphics[width=\linewidth]{figures/save_oneside_cifar10_IS/cGAN_oneside_IS_c10.pdf}
\caption{}
\end{subfigure}
  \caption{One-sided Omni-GAN is on par with projection-based BigGAN on CIFAR10, proving that one-sided Omni-GAN indeed belongs to projection-based cGANs. Both of them are inferior to the classification-based cGAN, Omni-GAN.}
  \label{apx:fig:oneside_c10}
\end{figure}

We provide the results of one-sided Omni-GAN and projection-based BigGAN on CIFAR10. As shown in Fig.~\ref{apx:fig:oneside_c10}, one-sided Omni-GAN is comparable to the projection-based BigGAN in terms of both FID and IS. This proves that one-sided Omni-GAN indeed belongs to projection-based cGANs. Both one-sided Omni-GAN and projection-based BigGAN are inferior to Omni-GAN. Because the only difference between one-sided Omni-GAN and Omni-GAN is whether the supervision is fully utilized, we conclude that the superiority of Omni-GAN lies in the full use of supervision.


\subsection{Comparison with Multi-hinge GAN}
\label{sec:comparison_multihinge}

Multi-hinge GAN belongs to classification-based cGANs, and also suffers from the early collapse issue. We study whether weight decay is effective for Multi-hinge GAN. As shown in Fig.~\ref{apx:fig:multihinge_wd_c100}, original Multi-hinge GAN suffers a severe early collapse issue. After equipped with weight decay, Multi-hinge GAN enjoys a safe optimization and its FID is even comparable to that of Omni-GAN. However, its IS is worse than that of Omni-GAN.

Multi-hinge GAN combined with weight decay does not always perform well. The results on CIFAR10 are shown in Fig.~\ref{apx:fig:multihinge_wd_c10}. Weight decay deteriorates Multi-hinge GAN in terms of both FID and IS. However, Omni-GAN outperforms Multi-hinge GAN. In addition, omni-loss is more flexible than multi-hinge loss. It supports implementing a multi-label discriminator. As a result, we suggest first considering using Omni-GAN when choosing cGANs.



\begin{figure}[t]
  \centering
  \begin{subfigure}{0.49\linewidth}
\includegraphics[width=\linewidth]{figures/save_multihinge_weight_decay_cifar100_FID/cGAN_multihinge_wd_FID.pdf}
    \caption{}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/save_multihinge_weight_decay_cifar100_IS/cGAN_multihinge_wd_IS.pdf}
    \caption{}
  \end{subfigure}
  \caption{FID and IS on CIFAR100. Weight decay can eliminate the early collapse problem of Multi-hinge GAN on CIFAR100.}
  \label{apx:fig:multihinge_wd_c100}
\end{figure}

\begin{figure}[t]
  \begin{subfigure}{0.49\linewidth}
    \centering
\includegraphics[width=\linewidth]{figures/save_multihinge_weight_decay_cifar10_FID/cGAN_multihinge_wd_FID_c10.pdf}
\caption{}
\end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \centering
\includegraphics[width=\linewidth]{figures/save_multihinge_weight_decay_cifar10_IS/cGAN_multihinge_wd_IS_c10.pdf}
\caption{}
\end{subfigure}
  \caption{FID and IS on CIFAR10. Weight decay deteriorates Multi-hinge GAN.}
  \label{apx:fig:multihinge_wd_c10}
\end{figure}

\subsection{Applying Weight Decay to the Generator}
\label{apx:sec:weight_decay_generator}

\begin{figure}[t]
  \centering
  \centering
  \includegraphics[width=0.7\linewidth]{figures/save_G_weight_decay_cifar100_IS/cGAN_G_wd_IS.pdf}
  \caption{Applying weight decay to the generator. Applying weight decay to the discriminator helps alleviate the collapse issue, but the IS gradually decreases as the training progresses. Applying weight decay to the generator simultaneously solves this problem. Experiments are conducted on CIFAR100.}
  \label{apx:fig:generator_wd}
\end{figure}


We found empirically that applying weight decay also to the generator can make training more stable. As shown in Fig.~\ref{apx:fig:generator_wd}, although only applying weight decay to the discriminator can avoid the risk of collapse earlier, the IS has a trend of gradually decreasing as the training progresses. Fortunately, applying weight decay (set to be $0.001$ in our most experiments) to the generator can solve this problem. This phenomenon seems to indicate that the generator is also at a risk of over-fitting.



\subsection{How to Set the Weight Decay?}

We did a grid search for the weight decay on CIFAR and found that its value is related to the size of the training dataset. For CIFAR100, there are only $500$ images per class, and the weight decay is set to be $0.0005$. For CIFAR10, there are $5000$ images per class, and the weight decay is set to be $0.0001$. For ImageNet, it is a large dataset with a considerable number of training data (approximate $1.2$M). The weight decay is set to 0.00001. The conclusion is that the smaller the dataset, the higher the risk of over-fitting for the discriminator. Then weight decay should be larger.


\section{Additional Results on ImageNet}
\label{apx:sec:curves_imagenet}

We provide convergence curves on ImageNet $256\times256$. As shown in Fig.~\ref{apx:fig:imagenet256}, both Omni-GAN and Omni-INR-GAN converge faster than BigGAN, proving the effectiveness of combining strong supervision and weight decay. Omni-INR-GAN clearly outperforms Omni-GAN, showing its significant potential for future applications. In Fig.~\ref{apx:fig:truncation_imagenet256.}, we show the tradeoff curve of these methods using the truncation trick on ImageNet $256\times256$. Omni-INR-GAN is consistently superior to Omni-GAN and BigGAN.

\begin{figure}[t]
\centering
\begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth,height=3.3cm]{figures/imagenet/save_OmniGAN_ImageNet256_FID.pdf}
    \caption{FID on ImageNet $256\times256$}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth,height=3.3cm]{figures/imagenet/save_OmniGAN_ImageNet256_IS.pdf}
    \caption{IS on ImageNet $256\times256$}
  \end{subfigure}
  \vspace{-0.2cm}
  \caption{FID and IS on ImgeNet $256\times256$. Omni-GAN and Omni-INR-GAN converge faster than the projection-based BigGAN. Omni-INR-GAN clearly outperforms Omni-GAN, showing its significant potential for future applications.}
  \label{apx:fig:imagenet256}
\end{figure}

\begin{figure}[t]
  \centering
  \centering
  \includegraphics[width=0.7\linewidth]{figures/imagenet/truncation_imagenet256.pdf}
  \caption{Tradeoff curves using truncation trick on ImageNet $256\times256$. We show truncation values from $\sigma=0.05$ to $\sigma=1$ with step being $0.05$. Omni-INR-GAN outperforms Omni-GAN and BigGAN.}
  \label{apx:fig:truncation_imagenet256.}
\end{figure}


\section{Application to Image-to-Image Translation}
\label{apx:sec:imagetoimage}


\begin{table*}[t]
  \centering
  \resizebox{0.9\textwidth}{!}{\begin{tabular}{ccccccccccc}
      \toprule
      \multirow{4}{*}{SPADE~\cite{park2019Semantic}} & \textit{road} & \textit{sidewalk} & \textit{building} & \textit{wall} & \textit{fence} & \textit{pole} & \textit{traffic light} & \textit{traffic sign} & \textit{vegetation} & \textit{terrain} \\
& $97.44$       & $79.89$           & $87.86$           & $50.57$       & $47.21$        & $35.90$       & $38.97$                & $44.67$               & $88.15$             & $66.14$          \\
      \cmidrule{2-11}
                                                     & \textit{sky}  & \textit{person}   & \textit{rider}    & \textit{car}  & \textit{truck} & \textit{bus}  & \textit{train}         & \textit{motorcycle}   & \textit{bicycle}    & mIoU             \\
                                                     & $91.61$       & $62.27$           & $38.67$           & $88.68$       & $64.96$        & $70.17$       & $41.42$                & $28.58$               & $58.86$             & $62.21$          \\
      \midrule
      \multirow{4}{*}{+ Omni-GAN}                    & \textit{road} & \textit{sidewalk} & \textit{building} & \textit{wall} & \textit{fence} & \textit{pole} & \textit{traffic light} & \textit{traffic sign} & \textit{vegetation} & \textit{terrain} \\
                                                     & $97.57$       & $81.62$           & $88.58$           & $53.39$       & $50.47$        & $35.88$       & $41.08$                & $46.75$               & $89.31$             & $67.00$
      \\
      \cmidrule{2-11}
                                                     & \textit{sky}  & \textit{person}   & \textit{rider}    & \textit{car}  & \textit{truck} & \textit{bus}  & \textit{train}         & \textit{motorcycle}   & \textit{bicycle}    & mIoU             \\
                                                     & $92.14$       & $63.97$           & $41.99$           & $89.91$       & $71.06$        & $74.21$       & $56.16$                & $33.99$               & $61.23$             & $\mathbf{65.07}$ \\
      \bottomrule
    \end{tabular}}
  \vspace{-0.2cm}
  \caption{Semantic image synthesis using SPADE. Replacing the GAN used by SPADE with Omni-GAN can improve the quality of synthesized images.}
  \vspace{-0.4cm}
  \label{tab:spade}
\end{table*}


Omni-GAN can be used for image-to-image translation tasks. We verify the effectiveness of Omni-GAN on semantic image synthesis~\cite{wang2018VideotoVideo,qi2018Semiparametric}. In particular, we replace the GAN loss of SPADE~\cite{park2019Semantic} with Omni-GAN's loss, and keep other hyper-parameters unchanged.
The discriminator is a fully convolutional network, which is widely adopted by image-to-image translation tasks~\cite{park2019Semantic,isola2017ImagetoImage,wang2018HighResolution}. As shown in Fig.~\ref{apx:fig:per_pixel_omni_loss}, the discriminator takes images as input and outputs feature maps with the number of channels being $C+2$. $C$ represents the number of classes which is analogous to that of the semantic segmentation task. $2$ indicates there are two extra feature maps representing to what extent the input image is real or fake. We adopt nearest neighbor downsampling to downsample the label map to the same resolution as the output feature maps of the discriminator. Then we use the downsampled label map as the ground truth label, and apply a per-pixel omni-loss to the output feature maps of the discriminator.

We use Cityscapes dataset~\cite{cordts2016Cityscapes} as a testbed, and train models on the training set with size of $2,975$. The images is resized to $256\times512$. Models are evaluated by the mIoU of the generated images on the test set with $500$ images. We use a pre-trained DRN-D-105~\cite{yu2017Dilated} as the segmentation model for the sake of evaluation. As shown in Table~\ref{tab:spade}, Omni-GAN improves the mIoU score of SPADE from $62.21$ to $65.07$, substantiating that the synthesized images possess more semantic information. We believe that the improvement comes from the improved ability of the discriminator in distinguishing different classes, so that the generator receives better guidance and thus produces images with richer semantic information.

\begin{figure}[t]
  \begin{center}
\includegraphics[width=\linewidth]{./figures/per_pixel_unified_loss.pdf}
  \end{center}
  \vspace{-0.5cm}
  \caption{Combine omni-loss with a fully convolutional discriminator whose outputs are feature maps. In the figure, the green and red feature maps represent scores that the input images are real and fake, respectively. Omni-loss is applied to the output feature maps pixel-by-pixel. }
  \label{apx:fig:per_pixel_omni_loss}
\end{figure}

\section{Application to Downstream Tasks}
\label{apx:sec:downstream_tasks}


\subsection{Colorization and Super-resolution}

Deep generative prior (DGP)~\cite{pan2020Exploiting} showed the potential of employing image prior captured by a pre-trained GAN model. Our colorization and super-resolution schemes are based on DGP. We first introduce the preliminary knowledge of DGP.

Suppose $\vx$ is a natural image and $\phi$ is a degradation function, \eg, gray transform for colorization and down-sampling for super-resolution. Then $\hat{\vx}=\phi{(\vx)}$ represents the degraded image, \ie, a partial observation of the original image, $\vx$. The goal of image restoration is to recover $\vx$ from $\hat{\vx}$ with the help of some statistical image prior of $\vx$. DGP proposes employing the image prior stored in a pre-trained GAN's generator. The objective is defined as
\begin{equation}
\begin{aligned}
    \boldsymbol{\theta}^{*}, \vz^{*}=
    \underset{\boldsymbol{\theta}, \vz}{\arg \min } \mathcal{L}(\hat{\vx}, \phi(G(\vz ; \boldsymbol{\theta}))),
  \end{aligned}
\label{apx:equ:dgp_objective}
\end{equation}
where $\vz$ is a noise vector. $G$ represents the generator in GAN and is parameterized by $\boldsymbol{\theta}$. $\mathcal{L}$ is a discriminator-based distance metric: $\mathcal{L}\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)=\sum_{i \in \mathcal{I}}\left\|D\left(\mathbf{x}_{1}, i\right), D\left(\mathbf{x}_{2}, i\right)\right\|_{1}$. $D$ is the discriminator coupled with $G$. $\mathcal{I}$ is a index set for feature maps of different blocks of $D$. Note that both $G$ and $D$ have been trained on a large-scale natural image dataset. DGP employs the prior of $G$ by fine-tuning $\boldsymbol{\theta}$ and $\vz$. After fine-tuning, we get the restored image $\vx^{*}=G(\vz^{*} ; \boldsymbol{\theta}^{*})$.


Although DGP has achieved noteworthy results in image restoration and manipulation, it has limitations due to the inflexibility of the pre-trained GAN model. For example, if DGP adopts a $128\times128$ BigGAN model, DGP must first crop the original image into image patch of size $128\times128$ before restoration, restricting its practical application. However, because Omni-INR-GAN can output images of any resolution, combining it with DGP can directly restore the original image.

We use Omni-INR-GAN pre-trained on ImageNet $256\times256$ for colorization and super-resolution. \myeqref{apx:equ:dgp_objective} is the objective. For colorization, $\hat{\vx}$ is a grayscale image, and for super-resolution, $\hat{\vx}$ is a low-resolution image. We resize the input image's short edge to $256$ and keep the aspect ratio of the image unchanged.  After fine-tuning, $\vx^{*}=G(\vz^{*} ; \boldsymbol{\theta}^{*})$ is the restored image. Because $G(\vz^{*}; \boldsymbol{\theta}^{*})$ represents $\vx^{*}$ in the INR form, we can get the restored image at any resolution through $G(\vz^{*}; \boldsymbol{\theta}^{*})$. Therefore, Omni-INR-GAN is more friendly to downstream tasks.



\subsection{Reconstruction}

We compare pre-trained GAN models for image reconstruction tasks. Specifically, we finetune the parameters of the generator to make it reconstruct given images. Note that we do not use mse or L1 loss, because these loss functions make it easy for the generator to overfit the given image, as long as the training iterations are enough. Instead, we only use the discriminator feature loss, because it has been proven to be very effective for utilizing the prior of the generator. For the dataset, we use 1k images sampled from the ImageNet validation set, which is the same as DGP's choice. Note that these data have not been used in GAN's training. We adopt the progressive reconstruction strategy of DGP~\cite{pan2020Exploiting}, and finetune each GAN model for the same number of iterations.


\section{Implementation Details}

We adopt BigGAN architectures of $128\times128$ and $256\times256$ in our experiments. Table~\ref{apx:tab:omnigan_imagenet128}, \ref{apx:tab:omnigan_imagenet256}, \ref{apx:tab:omniinrgan_imagenet128}, and \ref{apx:tab:omniinrgan_imagenet256} show the architectural details. Each experiment is conducted on eight v100 GPUs. Training Omni-GAN on ImageNet $128\times128$ and $256\times256$ took $25$ days and $60$ days, respectively. Training Omni-INR-GAN on ImageNet $128\times128$ and $256\times256$ took $27$ days and $87$ days, respectively. No collapse occurred during the entire training process. We have found experimentally that classification-based cGANs cannot set a large batch size like projection-based BigGAN. For all experiments of Omni-GAN and Omni-INR-GAN, the batch size is set to $256$. We adopt the ADAM optimizer in all experiments, with betas being $0$ and $0.999$. The learning rates of the generator and discriminator are set to $0.0001$ and $0.0004$, respectively.

For $128\times128$ experiments, the generator and discriminator use non-local block at $64\times64$ resolution. The generator is updated once every time the discriminator is updated. The weight decay of the generator and discriminator are set to $0.001$ and $0.00001$, respectively. For Omni-INR-GAN, we removed the non-local block at $64\times64$ resolution of the discriminator. Because when Omni-INR-GAN is used for downstream tasks, the input image of the discriminator may be of any size, so the middle layer of the discriminator may not output $64\times64$ resolution features. Moreover, although Omni-INR-GAN can generate images of any resolution, we did not adopt a multi-scale training strategy. We found that multi-scale training led to training collapse. We think that a possible reason is that multi-scale training enhances the discriminator, resulting in the ability of the generator and the discriminator to be out of balance. Thus we only generate $128\times128$ images during training, and the real images are also resized to $128\times128$.

For $256\times256$ experiments, the weight decay of the generator and discriminator are set to $0.0001$ and $0.00001$, respectively. The generator is updated once every time the discriminator is updated twice. We have found experimentally that this will make training more stable. The generator and discriminator use non-local block at $64\times64$ resolution rather than $128\times128$ due to limited GPU memory. For Omni-INR-GAN, in order to support downstream tasks friendly, we do not use non-local block in the discriminator. Moreover, due to GPU memory limitation, we reduce the batch size to $128$ and accumulate the gradient twice to approximate the gradient when the batch size is $256$. We did not adopt a multi-scale training strategy. Only $256\times256$ images are generated during training, and the real images are also resized to $256\times256$.

\begin{table*}[htbp]
  \begin{subtable}[t]{0.48\linewidth}
    \centering
    \resizebox{0.8\linewidth}{!}{\centering
      \begin{tabular}{c}
        \toprule
        $\vz\in \mathbb{R}^{120} \sim \mathcal{N}(0, I), \text{embed}(y) \in \mathbb{R}^{128}$ \\
        \midrule
        Linear $20$ $\rightarrow$ $4\times4\times16 ch$                                        \\         \midrule
        ResBlock up $16ch \rightarrow 16ch$                                                    \\         \midrule
        ResBlock up $16ch \rightarrow 8ch$                                                     \\         \midrule
        ResBlock up $8ch \rightarrow 4ch$                                                      \\         \midrule
        ResBlock up $4ch \rightarrow 2ch$                                                      \\         \midrule
        Non-local Block  ($64 \times 64$)                                                      \\         \midrule
        ResBlock up $2ch \rightarrow ch$                                                       \\         \midrule
        BN, ReLU, $3\times3$ Conv $ch \rightarrow 3$                                           \\         \midrule
        Tanh                                                                                   \\
        \bottomrule
      \end{tabular}}
    \caption{Generator}
  \end{subtable}
  \centering
  \begin{subtable}[t]{0.48\linewidth}
    \centering
    \resizebox{.6\linewidth}{!}{\begin{tabular}{c}
        \toprule
        RGB image $\vx \in \mathbb{R}^{128 \times 128 \times 3}$ \\ \midrule
        ResBlock down $3 \rightarrow ch$                         \\ \midrule
        Non-local Block ($64 \times 64$)                         \\ \midrule
        ResBlock down $ch \rightarrow 2ch$                       \\ \midrule
        ResBlock down $2ch \rightarrow 4ch$                      \\ \midrule
        ResBlock down $4ch \rightarrow 8ch$                      \\ \midrule
        ResBlock down $8ch \rightarrow 16ch$                     \\ \midrule
        ResBlock $16ch \rightarrow 16ch$                         \\ \midrule
        ReLU, global sum pooling                                 \\ \midrule
        Linear $\rightarrow 1002$                                \\
        \bottomrule
      \end{tabular}}
    \caption{Discriminator}
  \end{subtable}
  \caption{Omni-GAN architecture on ImageNet $128\times128$. $ch$ is set to be $96$.}
  \label{apx:tab:omnigan_imagenet128}
\end{table*}


\begin{table*}[htbp]
  \begin{subtable}[t]{0.48\linewidth}
    \centering
    \resizebox{0.8\linewidth}{!}{\centering
      \begin{tabular}{c}
        \toprule
        $\vz\in \mathbb{R}^{120} \sim \mathcal{N}(0, I), \text{embed}(y) \in \mathbb{R}^{128}$ \\
        \midrule
        Linear $17$ $\rightarrow$ $4\times4\times16 ch$                                        \\         \midrule
        ResBlock up $16ch \rightarrow 16ch$                                                    \\         \midrule
        ResBlock up $16ch \rightarrow 8ch$                                                     \\         \midrule
        ResBlock up $8ch \rightarrow 8ch$                                                      \\         \midrule
        ResBlock up $8ch \rightarrow 4ch$                                                      \\         \midrule
        Non-local Block  ($64 \times 64$)                                                      \\         \midrule
        ResBlock up $4ch \rightarrow 2ch$                                                      \\         \midrule
        ResBlock up $2ch \rightarrow ch$                                                       \\         \midrule
        BN, ReLU, $3\times3$ Conv $ch \rightarrow 3$                                           \\         \midrule
        Tanh                                                                                   \\
        \bottomrule
      \end{tabular}}
    \caption{Generator}
  \end{subtable}
  \centering
  \begin{subtable}[t]{0.48\linewidth}
    \centering
    \resizebox{.6\linewidth}{!}{\begin{tabular}{c}
        \toprule
        RGB image $\vx \in \mathbb{R}^{256 \times 256 \times 3}$ \\ \midrule
        ResBlock down $3 \rightarrow ch$                         \\ \midrule
        ResBlock down $ch \rightarrow 2ch$                       \\ \midrule
        Non-local Block ($64 \times 64$)                         \\ \midrule
        ResBlock down $2ch \rightarrow 4ch$                      \\ \midrule
        ResBlock down $4ch \rightarrow 8ch$                      \\ \midrule
        ResBlock down $8ch \rightarrow 8ch$                      \\ \midrule
        ResBlock down $8ch \rightarrow 16ch$                     \\ \midrule
        ResBlock $16ch \rightarrow 16ch$                         \\ \midrule
        ReLU, global sum pooling                                 \\ \midrule
        Linear $\rightarrow 1002$                                \\
        \bottomrule
      \end{tabular}}
    \caption{Discriminator}
  \end{subtable}
  \caption{Omni-GAN architecture on Imagenet $256\times256$. $ch$ is set to be $96$.}
  \label{apx:tab:omnigan_imagenet256}
\end{table*}


\begin{table*}[htbp]
  \begin{subtable}[t]{0.48\linewidth}
    \centering
    \resizebox{0.8\linewidth}{!}{\centering
      \begin{tabular}{c}
        \toprule
        $\vz\in \mathbb{R}^{120} \sim \mathcal{N}(0, I), \text{embed}(y) \in \mathbb{R}^{128}$ \\
        \midrule
        Linear $20$ $\rightarrow$ $4\times4\times16 ch$                                        \\         \midrule
        ResBlock up $16ch \rightarrow 16ch$                                                    \\         \midrule
        ResBlock up $16ch \rightarrow 8ch$                                                     \\         \midrule
        ResBlock up $8ch \rightarrow 4ch$                                                      \\         \midrule
        ResBlock up $4ch \rightarrow 2ch$                                                      \\         \midrule
        Non-local Block  ($64 \times 64$)                                                      \\         \midrule
        ResBlock up $2ch \rightarrow ch$                                                       \\         \midrule
        Unfold(kernel\_size=3) $ch \rightarrow 9ch$                                            \\         \midrule
        Grid\_sample($x, y$), Concat feature and $(x, y)$                                      \\         \midrule
        Linear, Relu $9ch + 2 \rightarrow ch$                                                  \\         \midrule
        Linear, Relu $ch \rightarrow ch$                                                       \\         \midrule
        Linear $ch \rightarrow 3$                                                              \\         \midrule
        Tanh                                                                                   \\
        \bottomrule
      \end{tabular}}
    \caption{Generator}
  \end{subtable}
  \centering
  \begin{subtable}[t]{0.48\linewidth}
    \centering
    \resizebox{.6\linewidth}{!}{\begin{tabular}{c}
        \toprule
        RGB image $\vx \in \mathbb{R}^{128 \times 128 \times 3}$ \\ \midrule
        ResBlock down $3 \rightarrow ch$                         \\ \midrule
        ResBlock down $ch \rightarrow 2ch$                       \\ \midrule
        ResBlock down $2ch \rightarrow 4ch$                      \\ \midrule
        ResBlock down $4ch \rightarrow 8ch$                      \\ \midrule
        ResBlock down $8ch \rightarrow 16ch$                     \\ \midrule
        ResBlock $16ch \rightarrow 16ch$                         \\ \midrule
        ReLU, global sum pooling                                 \\ \midrule
        Linear $\rightarrow 1002$                                \\
        \bottomrule
      \end{tabular}}
    \caption{Discriminator}
  \end{subtable}
  \caption{Omni-INR-GAN architecture on ImageNet $128\times128$. $ch$ is set to be $96$.}
  \label{apx:tab:omniinrgan_imagenet128}
\end{table*}


\begin{table*}[htbp]
  \begin{subtable}[t]{0.48\linewidth}
    \centering
    \resizebox{0.8\linewidth}{!}{\centering
      \begin{tabular}{c}
        \toprule
        $\vz\in \mathbb{R}^{120} \sim \mathcal{N}(0, I), \text{embed}(y) \in \mathbb{R}^{128}$ \\
        \midrule
        Linear $17$ $\rightarrow$ $4\times4\times16 ch$                                        \\         \midrule
        ResBlock up $16ch \rightarrow 16ch$                                                    \\         \midrule
        ResBlock up $16ch \rightarrow 8ch$                                                     \\         \midrule
        ResBlock up $8ch \rightarrow 8ch$                                                      \\         \midrule
        ResBlock up $8ch \rightarrow 4ch$                                                      \\         \midrule
        Non-local Block  ($64 \times 64$)                                                      \\         \midrule
        ResBlock up $4ch \rightarrow 2ch$                                                      \\         \midrule
        ResBlock up $2ch \rightarrow ch$                                                       \\         \midrule
        Unfold(kernel\_size=3) $ch \rightarrow 9ch$                                            \\         \midrule
        Grid\_sample($x, y$), Concat feature and $(x, y)$                                      \\         \midrule
        Linear, Relu $9ch + 2 \rightarrow ch$                                                  \\         \midrule
        Linear, Relu $ch \rightarrow ch$                                                       \\         \midrule
        Linear $ch \rightarrow 3$                                                              \\         \midrule
        Tanh                                                                                   \\
        \bottomrule
      \end{tabular}}
    \caption{Generator}
  \end{subtable}
  \centering
  \begin{subtable}[t]{0.48\linewidth}
    \centering
    \resizebox{.6\linewidth}{!}{\begin{tabular}{c}
        \toprule
        RGB image $\vx \in \mathbb{R}^{256 \times 256 \times 3}$ \\ \midrule
        ResBlock down $3 \rightarrow ch$                         \\ \midrule
        ResBlock down $ch \rightarrow 2ch$                       \\ \midrule
        ResBlock down $2ch \rightarrow 4ch$                      \\ \midrule
        ResBlock down $4ch \rightarrow 8ch$                      \\ \midrule
        ResBlock down $8ch \rightarrow 8ch$                      \\ \midrule
        ResBlock down $8ch \rightarrow 16ch$                     \\ \midrule
        ResBlock $16ch \rightarrow 16ch$                         \\ \midrule
        ReLU, global sum pooling                                 \\ \midrule
        Linear $\rightarrow 1002$                                \\
        \bottomrule
      \end{tabular}}
    \caption{Discriminator}
  \end{subtable}
  \caption{Omni-INR-GAN architecture on Imagenet $256\times256$. $ch$ is set to be $96$.}
  \label{apx:tab:omniinrgan_imagenet256}
\end{table*}





\section{Additional Results}

\subsection{Generated Images on CIFAR}

In Fig.~\ref{apx:fig:cifar10} and \ref{apx:fig:cifar100}, we show generated images from Omni-GAN on CIFAR10, CIFAR100 respectively. Due to limited space, we only show images of some categories on CIFAR100.

\begin{figure}[t]
  \centering
\includegraphics[width=\linewidth]{figures/save_generated_images_cifar10/cifar10.png}
\caption{Randomly generated image by Omni-GAN for CIFAR10}
  \label{apx:fig:cifar10}
\end{figure}

\begin{figure}[t]
  \centering
\includegraphics[width=\linewidth]{figures/save_generated_images_cifar100/cifar100.png}
\caption{Randomly generated image by Omni-GAN for CIFAR100}
  \label{apx:fig:cifar100}
\end{figure}





\subsection{Generated Images on ImageNet}

Omni-INR-GAN inherently supports generating images of arbitrary resolution. We adopt the Omni-INR-GAN $256\times256$ model to generate some images with different resolutions, \eg, Fig.~\ref{apx:fig:omniinrgan_sample_c1}, ~\ref{apx:fig:omniinrgan_sample_c10}, etc.



\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0001_20210324_113415_932/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
  \label{apx:fig:omniinrgan_sample_c1}
  \vspace{-5pt}
\end{figure*}



\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0010_20210324_114434_254/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
  \label{apx:fig:omniinrgan_sample_c10}
  \vspace{-5pt}
\end{figure*}

\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0011_20210324_114622_559/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0014_20210324_114836_556/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0143_20210324_142740_544/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0153_20210324_131040_858/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0156_20210324_131419_171/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0278_20210324_132057_656/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0323_20210324_132407_993/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0326_20210324_132706_954/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0331_20210324_133147_310/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0358_20210324_133427_825/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}



\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0425_20210324_140054_456/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0449_20210324_140507_216/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0946_20210324_141318_570/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0970_20210324_141828_735/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0980_20210324_142034_652/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.8}
  \graphicspath{{figures/results/OmniINGAN_samples/class_0985_20210324_142348_264/}}
  \resizebox{0.7\linewidth}{!}{\begin{tabular}{cccc}
\includegraphics[width=0.25\linewidth]{0032.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0064.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0128.jpg}                    &
      \includegraphics[width=0.25\linewidth]{0256.jpg}                                                                                             \\
      $32\times32$                                                        & $64\times64$                         & $128\times128$ & $256\times256$ \\
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{0512.jpg}} &
      \multicolumn{2}{c}{\includegraphics[width=0.5\linewidth]{1024.jpg}}                                                                          \\
      \multicolumn{2}{c}{$512\times512$}                                  & \multicolumn{2}{c}{$1024\times1024$}
    \end{tabular}
  }
  \vspace{-5pt}
  \caption{Samples generated by our Omni-INR-GAN $256\times256$ model. Omni-INR-GAN has the ability to generate images of any resolution.}
\vspace{-5pt}
\end{figure*}


\begin{figure*}[!t]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.4}
  \centering
  \resizebox{\linewidth}{!}{\centering
    \begin{tabular}{cc|ccc}
\includegraphics[width=.19\linewidth]{figures/DGP_SR/val_1955/OmniINRGAN/ILSVRC2012_val_00001955_target_origin.png}                         &
      \includegraphics[width=.19\linewidth]{figures/DGP_SR/val_1955/OmniINRGAN/ILSVRC2012_val_00001955_target_input.png}          \hspace{0.01cm} &
      \hspace{0.01cm}
      \includegraphics[width=.19\linewidth]{figures/DGP_SR/val_1955/liif/ILSVRC2012_val_00001955_target_input_x1_hw_32x42.jpg}                    &
      \includegraphics[width=.19\linewidth]{figures/DGP_SR/val_1955/liif/ILSVRC2012_val_00001955_target_input_x4.6_hw_147x193.jpg}                &
      \includegraphics[width=.19\linewidth]{figures/DGP_SR/val_1955/liif/ILSVRC2012_val_00001955_target_input_x63.5_hw_2032x2667.jpg}
      \\
      Size ($256\times336$)                                                                                                                       & Size ($32\times42$)          & $\times 1$ ($32\times42$)                          & $\times 4.6$ ($147\times193$) & $\times 63.5$ ($2032\times2667$)
      \\
      (a) Ground truth                                                                                                                            & (b) Input                    & \multicolumn{3}{c}{(c) LIIF}
      \\ \midrule
\includegraphics[width=.19\linewidth]{figures/DGP_SR/val_1955/dip/ILSVRC2012_val_00001955_target_origin_factor_8.jpg}                       &
      \includegraphics[height=2.51cm]{figures/DGP_SR/val_1955/BigGAN/00999.png}                                                                   &
      \hspace{0.01cm}
      \includegraphics[width=.19\linewidth]{figures/DGP_SR/val_1955/OmniINRGAN/000000_hw_32_42_r_1.0.png}                                         &
      \includegraphics[width=.19\linewidth]{figures/DGP_SR/val_1955/OmniINRGAN/000072_hw_147_193_r_4.6.png}                                       &
      \includegraphics[width=.19\linewidth]{figures/DGP_SR/val_1955/OmniINRGAN/000395_hw_2032_2667_r_63.5.png}
      \\
      $\times 8$ ($256\times 320$)                                                                                                                & $\times 8$ ($256\times 256$) & $\times 1$ ($32\times42$)                          & $\times 4.6$ ($147\times193$) & $\times 63.5$ ($2032\times2667$)
      \\
      (d) DIP                                                                                                                                     & (e) DGP w/ BigGAN            & \multicolumn{3}{c}{(f) DGP w/ Omni-INR-GAN (ours)}                                                                    \\
      \midrule
    \end{tabular}
  }
  \vspace{-0.3cm}
  \caption{Super-resolution using Omni-INR-GAN's prior, at any scale ($\times1$-$\times60+$). (b) input image with low resolution. (c) LIIF~\cite{chen2020Learning} can extrapolate the input image to any scale, but it cannot add semantic details, so the result is still blurred. (d) DIP~\cite{ulyanov2018Deep} also failed because the input image resolution is too low. (e) DGP~\cite{pan2020Exploiting} with BigGAN must crop the input and upsamples the cropped patch to a fixed size, which is inflexible. (f) Omni-INR-GAN has the ability to upsample the input image to any scale and also adds rich semantic details.}
\label{apx:fig:dgp_SR}
  \vspace{-.5cm}
\end{figure*}


\subsection{Results of Semantic Image Synthesis}

In Fig.~\ref{apx:fig:cityscapes}, we show several results of Omni-GAN as well as those of SPADE for semantic image synthesis. The label maps and the ground truth images are from the first ten items in the test set of Cityscapes dataset, without cherry-picking.

\begin{figure*}[htbp]
  \footnotesize
  \centering
  \renewcommand{\tabcolsep}{1pt} \renewcommand{\arraystretch}{0.4}
  \begin{tabular}{cccc}
    Label                                                                                                                                & Ground Truth                                                                                                                & SPADE                                                                                                                                      & Ours                                                                                                                                          \\
\includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/input_label/000.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/gt/000.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/synthesized_image/000.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_omni_GAN/spade/cityscapes_pretrained/test_latest/images/synthesized_image/000.png} \\
    \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/input_label/001.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/gt/001.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/synthesized_image/001.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_omni_GAN/spade/cityscapes_pretrained/test_latest/images/synthesized_image/001.png} \\
    \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/input_label/002.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/gt/002.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/synthesized_image/002.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_omni_GAN/spade/cityscapes_pretrained/test_latest/images/synthesized_image/002.png} \\
    \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/input_label/003.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/gt/003.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/synthesized_image/003.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_omni_GAN/spade/cityscapes_pretrained/test_latest/images/synthesized_image/003.png} \\
    \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/input_label/004.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/gt/004.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/synthesized_image/004.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_omni_GAN/spade/cityscapes_pretrained/test_latest/images/synthesized_image/004.png} \\
    \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/input_label/005.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/gt/005.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/synthesized_image/005.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_omni_GAN/spade/cityscapes_pretrained/test_latest/images/synthesized_image/005.png} \\
    \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/input_label/006.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/gt/006.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/synthesized_image/006.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_omni_GAN/spade/cityscapes_pretrained/test_latest/images/synthesized_image/006.png} \\
    \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/input_label/007.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/gt/007.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/synthesized_image/007.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_omni_GAN/spade/cityscapes_pretrained/test_latest/images/synthesized_image/007.png} \\
    \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/input_label/008.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/gt/008.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/synthesized_image/008.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_omni_GAN/spade/cityscapes_pretrained/test_latest/images/synthesized_image/008.png} \\
    \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/input_label/009.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/gt/009.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_spade/spade/cityscapes_pretrained/test_latest/images/synthesized_image/009.png} & \includegraphics[width=0.23\linewidth]{figures/save_images_omni_GAN/spade/cityscapes_pretrained/test_latest/images/synthesized_image/009.png} \\
\end{tabular}
\caption{Results of semantic image synthesis on Cityscapes.}
  \label{apx:fig:cityscapes}
  \vspace{-5pt}
\end{figure*}

\clearpage

\end{document}