\documentclass[11pt,letterpaper]{article}


\usepackage{fullpage,amsmath,amssymb,amsthm}
\usepackage{xspace}
\usepackage{multirow}
\usepackage[hypertex]{hyperref}

\usepackage{amsmath}

\usepackage{comment}
\usepackage{color}
\usepackage{paralist}

\usepackage{graphicx}
\usepackage{ifpdf}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{obs}[theorem]{Observation}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{property}{Property}
\newtheorem{notation}{Notation}[section]
\newtheorem{example}{Example}[section]


\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}


\newtheorem*{thmu}{Theorem}{\bfseries}{\rmfamily}
\newtheorem*{lemu}{Lemma}{\bfseries}{\rmfamily}

\newcommand\remove[1]{}




\newcommand{\e}{\epsilon}
\newcommand{\prob}[1]{\ensuremath{\text{{\bf Pr}}}}
\newcommand{\expct}[1]{\ensuremath{\text{{\bf E}}}}
\newcommand{\ef}{\frac{1}{\e}}
\newcommand{\half}{\frac{1}{2}}

\newcounter{note}[section]
\renewcommand{\thenote}{\thesection.\arabic{note}}
\newcommand{\mdnote}[1]{\refstepcounter{note}{\bf Mike's Comment~\thenote:}
{\sf #1}\marginpar{\tiny\bf MD~\thenote}}

\def\polylog{\operatorname{polylog}}

\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{url}

\makeatletter
\def\url@leostyle{\@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
\makeatother
\urlstyle{leo}

\renewcommand{\theequation}{\thesection.\arabic{equation}}


\begin{document}


\title{iBGP and Constrained Connectivity}
\author{Michael Dinitz\\Weizmann Institute of Science \and Gordon Wilfong\\Bell Labs}

\begin{titlepage}
\maketitle
\thispagestyle{empty}

\begin{abstract}
We initiate the theoretical study of the problem of minimizing the size of an iBGP overlay in an Autonomous System (AS) in the Internet subject to a natural notion of correctness derived from the standard ``hot-potato'' routing rules.  For both natural versions of the problem (where we measure the size of an overlay by either the number of edges or the maximum degree) we prove that it is NP-hard to approximate to a factor better than  and provide approximation algorithms with ratio .  This algorithm is based on a natural LP relaxation and randomized rounding technique inspired by the recent work on approximating directed spanners by Bhattacharyya et al.~[SODA 2009], Dinitz and Krauthgamer [STOC 2011], and Berman et al.~[ICALP 2011].  In addition to this theoretical algorithm, we give a slightly worse -approximation based on primal-dual techniques that has the virtue of being both fast (in theory and in practice) and good in practice, which we show via simulations on the actual topologies of five large Autonomous Systems.

The main technique we use is a reduction to a new connectivity-based network design problem that we call \emph{Constrained Connectivity}.  In this problem we are given a graph , and for every pair of vertices  we are given a set  called the \emph{safe set} of the pair.  The goal is to find the smallest subgraph  of  in which every pair of vertices  is connected by a path contained in .  We show that the iBGP problem can be reduced to the special case of Constrained Connectivity where  and safe sets are defined geometrically based on the IGP distances in the AS.  Indeed, our algorithmic upper bounds generalize to Constrained Connectivity on , and our -lower bound for the special case of iBGP implies hardness for the general case.  Furthermore, we believe that Constrained Connectivity is an interesting problem in its own right, so provide stronger hardness results (-hardness of approximation based on reductions from Label Cover) and integrality gaps ( based on random instances of Unique Games) for the general case.  On the positive side, we show that Constrained Connectivity turns out to be much simpler for some interesting special cases other than iBGP: when safe sets are symmetric and hierarchical, we give a polynomial time algorithm that computes an optimal solution.
\end{abstract}

\end{titlepage}

\section{Introduction}


The Internet consists of a number of interconnected subnetworks
called Autonomous Systems (ASes).
As described in~\cite{basu}, the way that routes to a given
destination are chosen by routers within an AS can be viewed as follows.
Routers have a ranking of routes based on economic considerations of the AS.
Without loss of generality, in what follows
we assume that all routes are equally ranked.
Thus routers must use some tie-breaking scheme in order to choose a route
from amongst the equally ranked routes.
Tie-breaking is based on traffic engineering considerations and
in particular, the goal is to get packets out of the AS as quickly
as possible (called {\em hot-potato routing}).

An AS attempts to achieve hot-potato routing using iBGP, the version of
the interdomain routing protocol BGP~\cite{stewart:99} used by routers within
a subnetwork to announce routes to each other that have been
learned from outside the subnetwork.  An iBGP configuration is defined by a \emph{signaling graph}, which is supposed to enforce hot-potato routing.  Unfortunately, while iBGP has many nice properties that make it useful in practice, constructing a good signaling graph turns out to be a computationally difficult problem.  For example, it is not clear \emph{a priori} that it is even possible to check in polynomial time that a signaling graph is correct, i.e.~it is not obvious that the problem is even in NP!  In this paper we study the problem of constructing small and correct signaling graphs, as well as a natural extension to a more general problem that we call \emph{Constrained Connectivity}.


\subsection{iBGP}
At a high level, iBGP works as follows.
The routers that initially know of a route are called {\em border routers}.
(These initial routes are those learned by the border routers from
routers outside the AS.)
The border router that initially knows of a route
is said to be the {\em egress router} of that route.
Each border router knows of at most one route.
Thus an initial set of routes  defines a set of egress routers  where
there is a one-to-one relationship between routes in  and routers
in .
The AS has an underlying physical network with edge weights (e.g., IGP
distances or OSPF weights).
The {\em distance} between two routers is then defined to be the
length of the shortest path (according to the edge weights) between them.
Given a set of routes, a router will rank highest the one whose
egress router is closest according to this definition of distance.
The {\em signaling graph}  is an overlay network whose nodes represent
routers and whose edges represent the fact that
the two routers at its endpoints use iBGP to inform one another of
their current chosen route.
The endpoints of an edge in  are called {\em iBGP neighbors}.
A path in  is called a {\em signaling path}.  Note that iBGP neighbors are not necessarily neighbors in the underlying graph, since  is an overlay and can include any possible edge.

Finally, iBGP can be thought of as working as follows:
in an asynchronous fashion, each router considers all the latest routes
it has heard about from its iBGP neighbors,
chooses the one with the closest egress router and tells its
iBGP neighbors about the route it has chosen.
This continues until no router learns of a route whose egress router is
closer than that of its currently chosen route.
When this process ends the route chosen by router  is denoted by .
Let  be
the shortest path from  to , the egress router of .
When a packet arrives at , it sends it to the next router  on ,
 in turn sends the packet to the next router on  and so on.
Thus if  is not the subpath of  starting at  then the
packet will not get routed as  expected.

A signaling graph  has the \emph{complete visibility property} for a set of egress routers  if each router  hears about (and hence
chooses as ) the route in  whose egress router  is closest to
 from amongst all routers in .  It is easy to see that  will achieve hot-potato routing for  if and only if it has the complete visibility property for .  So we say that a signaling graph is \emph{correct} if it has the complete visibility property for all possible .

Clearly if  is the complete graph then  is correct.  Because of this, the default configuration of iBGP and the original standard was to maintain a complete graph, also called a full mesh~\cite{stewart:99}.
However the complete graph is not practical and so network managers have
adopted various configuration techniques to reduce the size of the signaling
graph~\cite{rfc:2796,rfc:3065}.
Unfortunately these methods do not guarantee correct signaling
graphs~\cite{basu,griffinwilfong:2002a}.
Thus our goal is to determine correct signaling graphs with fewer
edges than the complete graph.  Slightly more formally, two natural questions are to minimize the number of edges in the signaling
graph or to minimize the maximum number of iBGP neighbors for any router
while guaranteeing correctness.
We define {\sc iBGP-Sum} to be the problem of finding
a correct signaling graph with the fewest edges, and similarly we define {\sc iBGP-Degree} to be the problem of finding
a correct signaling graph with the minimum possible maximum degree.

\subsection{Constrained Connectivity}
All we know \emph{a priori} about the complexity of {\sc iBGP-Sum} and {\sc iBGP-Degree} is that they are in  (the second existential level of the polynomial hierarchy), since the statement of correctness is that ``there exists a small graph  such that for all possible subsets   each router hears about the route with the closest egress router".  In particular, it is not obvious that these problems are in NP, i.e.~that there is a short certificate that a signaling graph is correct.  However, it turns out that these problems are actually in NP (see Section~\ref{sec:iBGP_CC}), and the proof of this fact naturally gives rise to a more general network design problem that we call \emph{Constrained Connectivity}.  In this problem we are given a graph  and for each pair of nodes  we are given a set .
Each such  is called a {\em safe set} and it is assumed that
.  We say that a subgraph 
of  is {\em safely connected} if for each pair of nodes  there
is a path in  from  to  where each node in the path is in .  As with iBGP, we are interested in two optimization versions of this problem:
\begin{enumerate}
\item {\sc Constrained Connectivity-Sum}: compute a safely
connected subgraph  with the minimum number of edges, and
\item {\sc Constrained Connectivity-Degree}: compute a safely
connected subgraph  that minimizes the maximum degree over all nodes.
\end{enumerate}

It turns out (see Theorem~\ref{thm:iBGP_safe_sets}) that the iBGP problems can be viewed as Constrained Connectivity problems with  and safe sets defined in a particular geometric way.  While the motivation for studying Constrained Connectivity comes from iBGP, we believe that it is an interesting problem in its own right.  It is an extremely natural and general network design problem that, somewhat surprisingly, seems to have not been considered before.  While we only provide negative results for the general problem (hardness of approximation and integrality gaps), a better understanding of Constrained Connectivity might lead to a better understanding of other network design problems, both explicitly via reductions and implicitly through techniques.  For example, many of the techniques used in this paper come from recent literature on directed spanners~\cite{BGJRW09,DK11,BBMRY11}, and given these similarities it is not unreasonable to think that insight into Constrained Connectivity might provide insight into directed spanners.

For a more direct example, there is a natural security application of Constrained Connectivity.  Suppose we have  players who wish to communicate with each other
but they do not all trust one another with messages they send to others.
That is, when  wishes to send a message to  there is a subset
 of players that it trusts to see the messages that it sends to .
Of course, if for every pair of players there were direct communication
channels between the two players, then there would be no problem.
But suppose there is a cost to protect communication channels from
eavesdropping or other such attacks.
Then a goal would be to have a network of fewer than 
communication channels that would still allow a route from
each  to each  with the route completely contained within .
Thus this problem defines a {\sc Constrained Connectivity-Sum} problem.




\subsection{Summary of Main Results}

In Section \ref{sec:CC_Kn} we
give a polynomial approximation for the iBGP problems, by giving the same approximations for the more general problem of Constrained Connectivity on .

\vspace{.1in}
{\bf Theorem~\ref{thm:approx}.}\hspace{.01in}
  There is an -approximation to the Constrained
  Connectivity problems on .
\vspace{.1in}

{\bf Corollary.}\hspace{.01in}
  There is an -approximation to {\sc iBGP-Sum} and
  {\sc iBGP-Degree}.
\vspace{.1in}

To go along with these theoretical upper bounds, we design a different (but related) algorithm for {\sc Constrained Connectivity-Sum} on  that provides a worse theoretical upper bound (a -approximation) but is faster in both practice and theory, and show by simulation on five real AS topologies (Telstra, Sprint, NTT, TINET, and Level 3) that in practice it provides an extremely good approximation.  Details of these simulations are in Section~\ref{sec:Simulations}

To complement these upper bounds, in Section~\ref{sec:iBGP_hardness} we show that the iBGP problem is hard to approximate, even with the extra power afforded us by the geometry of the safe sets:
\vspace{.1in}

{\bf Theorems~\ref{thm:iBGP_deghard} and~\ref{thm:iBGP_sumhard}.}\hspace{.01in}
  It is NP-hard to approximate {\sc iBGP-Sum} or {\sc iBGP-Degree} to
  a factor better than .
\vspace{.1in}

We then study the more general Constrained Connectivity problems, and in Section~\ref{sec:CC} we show that the fully general
constrained connectivity problems are hard to approximate:

\vspace{.1in}
{\bf Theorem~\ref{thm:ccsumhard}.}\hspace{.01in}
  {\sc The Constrained Connectivity-Sum} and \textsc{Constrained
    Connectivity-Degree} problems do not admit a -approximation algorithm for any constant  unless
  
\vspace{.1in}

This is basically the same inapproximability factor as for Label
Cover, and in fact our reduction is from a minimization version of
Label Cover known as {\sc Min-Rep}.  Moreover, we show that the
natural LP relaxation has a polynomial integrality gap of
.

Finally, in Section~\ref{sec:hierarchical}
 we consider some other special cases of Constrained
Connectivity that turn out to be easier.
In particular, we say that a collection of
safe sets is \emph{symmetric} if  for all  and that it is \emph{hierarchical} if for all , if  then  and .  It turns out that all of our hardness results and integrality gaps also hold for symmetric instances, but adding the hierarchical property makes things easier:
\vspace{.1in}

{\bf Theorem~\ref{lem:hierarchy_main}.}\hspace{.01in}
  {\sc Constrained Connectivity-Sum} with symmetric and hierarchical safe sets can
  be solved optimally in polynomial time.

\subsection{Related Work}
Issues involving eBGP, the version of BGP that routers in different ASes use
to announce routes to one another, have recently received
significant attention from the theoretical computer science community,
especially stability and game-theoretic issues
(e.g., ~\cite{griffin:02,levin:08,fabrikant:08}).
However, not nearly as much work has
been done on problems related to iBGP which distributes routes
internally in an AS.
There has been some work on the problem of guaranteeing
hot-potato routing in any AS with a route reflector
architecture~\cite{rfc:2796}.
These earlier papers did not consider the issue of
finding small signaling graphs that achieved the hot-potato goal.
Instead they either provided sufficient conditions for correctness
relating the underlying physical network with the route
reflector configuration~\cite{griffinwilfong:2002a} or they showed
that by allowing some specific extra routes to be announced (rather
than just the one chosen route) they would guarantee a version of
hot-potato routing~\cite{basu}.
The first people to consider the problem of
designing small iBGP overlays subject to achieving hot-potato
correctness were Vutukuru et al.~\cite{vutukuru:06}, who used graph
graph partitioning schemes to give such configurations.  But while
they proved that their algorithm gave correct configurations, they
only gave simulated evidence that the configurations it produced were
small.  Buob et
al.~\cite{buob:08} considered the problem of designing small correct
solutions and gave a mathematical programming formulation, but
then simply solved the integer program user super-polynomial time
algorithms.

\section{Preliminaries}

\subsection{Relationship between iBGP and Constrained Connectivity} \label{sec:iBGP_CC}

We will now show that the iBGP problems are just special
cases of {\sc Constrained Connectivity-Sum} and
{\sc Constrained Connectivity-Degree}.  This will be a natural consequence of the proof that {\sc iBGP-Sum} and {\sc iBGP-Degree} are in NP.



  To see this we will need the following definitions.  We
will assume that there are no ties, i.e.\ all distances are distinct.
For two routers  and , let  be
the set of routers that are farther from  than  is.  Let  be the set of
routers that are closer to  than to any router not in the ball
around  of radius .  We will refer to  as ``safe"
routers for the pair .
A path from  to  in a signaling graph is said to be a
{\em safe signaling path} if it is contained in .
It turns out that these safe sets characterize correct signaling graphs:

\begin{theorem} \label{thm:iBGP_safe_sets} An iBGP signaling graph 
  is correct if and only if for every pair 
  there is a signaling path from  to  that uses only routers in
  .
\end{theorem}
\begin{proof}
  We first show that if every pair has a safe signaling path then every
  node hears about the route that has the
  closest egress router no matter what the set of
  egress routers  is.  This is simple: let  be a router, and let  be its
  closest egress router. Let  be the route whose egress router is .
  By assumption there is a signaling path from  to 
  that uses only routers in .  By definition, every one of these
  routers is closer to  than to any router farther from  than 
  is.  Since  is the closest egress to , this means that for all of
  the routers in ,  will be the closest egress router.
  A simple induction then shows that the routers in a safe signaling
  path will each choose  and hence tell their iBGP neighbor in the
  path about .
  That is,  hears about .

  For the other direction we need to show that if a signaling graph
  is correct then every pair has a safe signaling path.  For
  contradiction, suppose that there is no safe signaling path from 
  to .  Let , the set of egress routers, be .  Let  be the route whose egress router is .
  Since every router in  is farther from  than
   is, this means that for this set of egress routers  is closer to 
  than any other egress.  By correctness we know that  does hear about
  .  Let  be the (or at least a)
  signaling path from  to  through which  hears about .
  Since there are no safe signaling paths from  to , we know that
  there exists some  such that .  This means that
  there is some  such that .  Since
  we assumed correctness we know that  heard about the route with
  the closest egress router  to , and 
  (since  in particular is closer).
  So  will not tell its iBGP neighbors about ,
  which is a contradiction since  is
  on the signaling path from which  heard about .  Thus a safe
  signaling path must exist.
\end{proof}

Note that this condition is easy to check in polynomial time, so we
have shown membership in NP.
Also this characterization shows that the problems
{\sc iBGP-Sum} and {\sc iBGP-Degree} are Constrained Connectivity
problems where the underlying graph  is  and the safe sets are defined by certain geometric properties.  While the proof of this is obviously relatively simple, we believe that it is an important contribution of this paper as it allows us to characterize the behavior of a protocol (iBGP) using only the static information of the signaling graph and the network distances.

\subsection{Linear Programming Relaxations}

There are two obvious linear programming relaxations of the {\sc
  Constrained Connectivity} problems (and thus the iBGP problems): the \emph{flow LP} and the
\emph{cut LP}.  For every pair  let
 be the collection of  paths that are contained
in .  The flow LP has a variable  for every edge  (called the \emph{capacity} of edge ) and a variable  for
every  path in  for every  (called the \emph{flow} assigned to path ).  The flow LP simply
requires that at least one unit of flow is sent between all pairs
while obeying capacity constraints:


 This is obviously a valid relaxation of {\sc
  Constrained Connectivity-Sum}: given a valid solution to
{\sc Constrained Connectivity-Sum}, let  denote the required
safe  path for every .  For every edge 
in some  set  to , and set  to  for
every .  This is clearly a valid
solution to the linear program with the exact same value.  To change
the LP for {\sc Constrained Connectivity-Degree} we can just introduce
a new variable , change the objective function to , and add the extra constraints  for all .  And while this LP can be exponential in size (since there is a variable for every path), it is also easy to design a compact representation that has only  variables and constraints.  This compact representation has variables  instead of , where  represents the amount of flow from  to  along edge  for the demand .    Then we can write the normal flow conservation and capacity constraints for every demand  independently, restricted to .  Indeed, this compact representation is one of the main reasons to prefer the flow LP over the cut LP.

The cut LP is basically equivalent to the flow LP, except that instead of requiring flow to be sent, it requires the min-cut to be large large enough. Given a pair , let

be the collection of safe set cuts that separate  and .
Furthermore, given a set  let  be the set
of safe edges that cross .  The cut LP has a variable  for
every edge  (equivalent to  in the flow LP), and is quite simple:


This LP simply minimizes the sum of the edge variables subject to the
constraint that for every cut between two nodes there must be at least
one safe edge crossing it.  While the flow LP and the cut LP are not technically duals of each other (since capacities are variables), it is easy to see from the max flow-min cut theorem that they do in fact describe the same polytope (with respect to the capacity variables).  Thus integrality gaps for one automatically hold for the other, as do approximations achieved by LP rounding.

\section{Algorithms for iBGP and Constrained Connectivity on } \label{sec:CC_Kn}


\subsection{-approximation}
In this section we show that there is a -approximation algorithm for both Constrained Connectivity problems as long as the underlying graph is the complete graph .  This algorithm is inspired by the recent progress on directed spanners by Bhattacharyya et al.~\cite{BGJRW09}, Dinitz and Krauthgamer~\cite{DK11}, and Berman et al.~\cite{BBMRY11}.  In particular, we use the same two-component framework that they do: a randomized rounding of the LP and a separate random tree-sampling step.  The randomized rounding we do is simple independent rounding with inflated probabilities.  The next lemma implies that this works well when the safe sets are small.

\begin{lemma} \label{lem:sample}
Let  be obtained by adding every edge   to  independently with probability at least .  Then with probability at least ,  will have a path between  and  contained in .
\end{lemma}
\begin{proof}
Let  be a partition of  so that  and , i.e.~ is an  cut of .  Note that there are only  such cuts, and by standard arguments if at least one edge from every cut is chosen to be in  then  contains an  path in .  Since in any LP solution at least one unit of flow is sent from  to  in , every cut has capacity at least .  Let  be the set of edges that cross the cut .  If  for any  then  is selected with probability , and thus  is spanned.  Otherwise, the probability that no edge from  is chosen is at most .  Thus by a simple union bound the probability that we fail on \emph{any} cut is at most 
\end{proof}


Another important part of our algorithm will be random sampling that is independent of the LP.  We
will use two different types of sampling: star sampling for the sum version and edge sampling for the degree version.  First we
consider star sampling, in which we independently sample nodes with
probability , and every sampled node becomes the center of a star that spans the vertex set.

\begin{lemma} \label{lem:star_sample}
  All pairs with safe sets of size at least  will be satisfied by
  random star sampling with high probability if .
\end{lemma}
\begin{proof}
  Consider some pair  with .  If some node
  (say ) from  is sampled then the pair is satisfied, since
  the creation of a star at  would create a path  that would
  satisfy .  The probability that no node from  is
  sampled is
  
  Since there are less than  pairs, we can take a union bound
  over all pairs  with , giving us that all
  such pairs are satisfied with probability at least .
\end{proof}

For edge sampling, we essentially consider the Erd\H{o}s-R\'{e}nyi
graph , i.e.\ we just sample every edge independently with
probability .  We will actually consider the union of 
independent  graphs, where 
for some small .  Let  be this random graph.

\begin{lemma} \label{lem:edge_sample} With probability at least
  , all pairs with safe sets of size at least  will be
  connected by a safe path in .
\end{lemma}
\begin{proof}
  Let  be a pair with .  Obviously  is
  satisfied if the graph induced on  is connected.  It is
  known \cite{bollobas:01} that there is some small  with  so that  is connected with probability at
  least .  Since  is the union of  instantiations of
  , we know that the probability that the subgraph of 
  induced on  is not connected is at most .  We can now
  take a union bound over all such  pairs, giving us that the
  probability that there is some unsatisfied  pairs with
   is at most .
\end{proof}


We will now combine the randomized rounding of the LP and the random sampling into a single approximation algorithm.  Our algorithm is
divided into two phases: first, we solve the LP and randomly include every edge  with probability .  By Lemma~\ref{lem:sample} this takes care of safe sets of size at most .  Second, if the objective is to
minimize the number of edges we do star sampling with probability
, and if the objective is to minimize the maximum
degree we do edge sampling using the construction of Lemma
\ref{lem:edge_sample} with .  It is easy to see that this algorithm with high probability results in a valid solution that is a -approximation.

\begin{theorem} \label{thm:approx}
  This algorithm is a -approximation to both {\sc Constrained Connectivity-Sum} and {\sc Constrained Connectivity-Degree} on .
\end{theorem}
\begin{proof}
  We first argue that the algorithm does indeed give a valid solution
  to the problem.  Let  be an arbitrary pair.  If , then Lemma \ref{lem:sample} implies that the first phase of
  the algorithm results in a safe path.  If ,
  then Lemma \ref{lem:star_sample} or Lemma \ref{lem:edge_sample}
  imply that the second phase of the algorithm results in a safe path.
  So every pair has a safe path, and thus the solution is valid.

  We now show that the cost of this algorithm is at most
  .  We first consider the objective
  function of minimizing the number of edges.  In the LP rounding step we only
  increase capacities by at most a factor of , so since the
  LP is a relaxation of the problem we know that the expected cost cost of
  the rounding is at most .  For phase 2, in
  expectation we chose  stars, for a total of at most
   edges.  But since there is a demand for every pair
  we know that , so phase 2 has total cost at most
  .

  If instead our objective function is to minimize the maximum degree,
  then since phase 1 only increases capacities by  we know
  that after phase 1 the maximum degree is at most  (by a Chernoff bound, with high probability every vertex has degree at most  times its fractional degree in the LP).  In phase 2, a simple Chernoff bound implies that with high
  probability every node gets  new edges, and thus
  the node with maximum degree still has degree at most
  .
\end{proof}

\subsection{Primal-Dual Algorithm} \label{sec:PD}

We also have a primal-dual algorithm that gives a slightly worse result for the {\sc Constrained Connectivity-Sum} problem.  While this algorithm and its analysis is slightly more complicated and only works for the Sum version, by not solving the linear program we get a faster algorithm.  In particular, the best known algorithms for solving linear programs with  variables take  time on general LPs, so since there are  variables in the compact version of the flow LP this takes  time.  The primal-dual algorithm, on the other hand, is significantly faster: a na\"{i}ve analysis shows that it takes  time.

In this algorithm we use the cut LP rather than the
flow LP (in fact, the algorithm is very similar to the primal-dual
algorithm for Steiner Forest, which uses a similar cut LP but doesn't
have to deal with safe sets).  Since this is a primal-dual algorithm,
instead of solving and rounding the cut LP we will consider the dual,
which has a variable  for every pair  and .  We say the an edge  if both endpoints of  are in .


Unfortunately we will not be able to use a pure primal-dual
approximation, but will have to trade off with a random sampling
scheme as in the rounding algorithm.  So instead of this primal, we
will only have constraints for  with  for
some parameter  that we will set later.  Thus in the dual we will
only have variables   for  with .
This clearly preserves the property that the primal is a valid
relaxation of the actual problem.  Let .

Our primal-dual algorithm, like most primal-dual algorithms,
maintains a set of \emph{active} dual variables that it increases
until some dual constraint becomes tight.  Once that happens we buy an
edge (i.e.\ set some  to  in the primal), change the set of
active dual variables, and repeat.  We do this until we have a
feasible primal.

Initially our primal solution  is empty and the active dual
variables are  for every , i.e.\ every
node  has an active dual variable for every other  that it has a
demand with corresponding to the cut in  that is the singleton
.  We raise these variables uniformly until some constraint
(say the one for ) becomes tight.  At this point we add
 to our current primal solution .  We now change the active dual
variables by ``merging'' moats that cross .  In particular, there are some active variables  where  (which implies
that  as well).  Let 
denote the subgraph of  induced on
.  Without loss of generality we can assume that  and
.  Let  be the connected component of
 containing .  We now make  inactive, and
make  active.  We do this for all such active
variables, and then repeat this process (incrementing all dual
variables until some dual constraint becomes tight, adding that edge
to , and then merging moats that cross it) until all pairs  have a safe path in .

\begin{lemma} \label{lem:PD_feasible} This algorithm always maintains
  a feasible dual solution and an active set that does not contribute
  to any tight constraint.
\end{lemma}
\begin{proof}
  We will show this by induction, where the inductive hypothesis is
  that the dual solution is feasible and that no dual variables that
  contribute to a tight constraint are active.  Initially all dual
  variable are , so it is obviously a feasible solution and no
  constraints are tight.  Now suppose this is true after we add some
  edge .  We need to show that it is also true after we add the
  next edge .  By induction the dual solution after we
  added  is feasible and none of the active dual variables
  contribute to any tight constraints.  Thus raising the active dual
  variables until some constraint becomes tight maintains dual
  feasibility.

  To prove that no active variables contribute to a tight constraint,
  note that the only new tight constraint is the one corresponding to
  .  The only variables contributing to that constraint are of the
  form  where .  But our algorithm
  made all of these variables inactive, and only added new active
  variables for sets  that contain both  and  and thus do
  not contribute to the newly tight constraint.  Furthermore, these
  sets  are formed by the union of  and the connected component
  in  containing the other endpoint, so no newly active
  variable contributes to a constraints that became tight previously
  (since they correspond to edges in ).
\end{proof}

\begin{theorem} \label{thm:PD_main}
  The primal-dual algorithm returns a graph  with at most  edges in which every pair  with 
  has a safe path.
\end{theorem}
\begin{proof}
  After every iteration of the algorithm all of the tight constraints
  are added to , which together with Lemma \ref{lem:PD_feasible}
  implies that the algorithm never gets stuck.  Thus it will run until
  every pair  with  has a safe path.  It just
  remains to show that the total number of edges returned is at most
  .  To see this, note that every edge in 
  corresponds to a tight constraint in the feasible dual solution we
  constructed, so if  then .  Thus
  we have that
  
  where the last inequality is by duality, and the next to last
  inequality is because  (since ).
\end{proof}

\begin{lemma} \label{lem:PD_time}
The primal-dual algorithm takes at most  time.
\end{lemma}
\begin{proof}
The primal-dual algorithm adds at least one new edge per iteration, so there can be at most  iterations.  In each iteration we have to figure out the current value of every dual constraint and the number of active variables in each constraint, which together will imply what the next tight constraint is and how much to raise the  variables.  We then need to raise the active variables by that amount and merge moats.  Note that for every demand there are at most two active moats, so the total number of active variables is at most .  Thus each iteration can be done in time , where the dominant term is the time taken to calculate the value of each dual constraint.  So the total time is , where there are extra poylogarithmic terms due to data structure overhead.
\end{proof}


Now we can trade this off with the random sampling solution for large
safe sets to get an actual approximation algorithm:

\begin{theorem} \label{thm:PD_total}
  There is a  approximation algorithm for the
  {\sc Constrained Connectivity-Sum} problem on  that runs in time .
\end{theorem}
\begin{proof}
Our algorithm first runs the primal-dual algorithm with .  By Theorem~\ref{thm:PD_main}, this returns a graph  with at most  edges in which there is a safe path for every  with .  We then use the random star sampling of Lemma~\ref{lem:star_sample} with  and thus .  By Lemma~\ref{lem:star_sample} this satisfies the rest of the demands (the pairs  with ) with high probability, and the number of edges added is with high probability at most  as desired.

The time bound follows from Lemma~\ref{lem:PD_time} together with the trivial fact that star sampling can be done in  time.
\end{proof}


\subsection{Simulations} \label{sec:Simulations}

In this section we discuss some the results of simulations using our
algorithms.  While we believe that the main contribution of this work
is theoretical, it is interesting that the algorithms are fast enough
to be practical and give solutions that are in practice far superior
to the worst case  bound.

We implemented both the LP rounding and the primal-dual algorithm for
the {\sc iBGP-Sum} problem.  However, the rounding algorithm turned
out to be impractical, mainly due to memory constraints.  Recall that
in the compact version of the flow LP there is a flow variable
 for every pair  and .  This variable
denotes the amount of flow from  to  along the edge  for
the demand .  There are also  capacity
constraints.  So on even a modest size AS topology, say one with 
nodes, the linear program has over six million variables and
constraints.  Running on a commodity desktop, the memory used by CPLEX
merely to create and store this LP results in an extremely large
running time, even without attempting to solve it.  Our primal-dual
algorithm, on the other hand, only needs to keep track of 
active dual variables and the current values of the  dual
constraints.  So we can actually run this algorithm on reasonably
sized graphs.

One change that we make from the theoretical algorithm is the tradeoff
with random sampling.  In the theoretical analysis we are only able to
get a nontrivial approximation bound by using the primal-dual
algorithm to handle small safe sets and random sampling to handle
large safe sets, but experimentation revealed that the simpler
algorithm of using the primal-dual technique to handle all safe sets
was sufficient.

\begin{table}
\begin{center}
\begin{tabular}{|c|l|c|c|}
\hline
AS & Name & Number of PoPs & Number of links\\
\hline
1221 & Telstra & 44 & 88 \\
1239 & Sprint & 52 & 168 \\
2914 & NTT & 70 & 222 \\
3257 & TINET & 41 & 174 \\
3356 & Level 3 & 63 & 570 \\
\hline
\end{tabular}
\end{center}
\caption{ISP Topologies Used}
\label{tab:topologies}
\end{table}

To test out this algorithm we ran it on five real-world ISP topologies
with link weights given by the Rocketfuel project~\cite{Rocketfuel04}.
Our implementation is still relatively slow, so we consider
Point-of-Presence level topologies rather than router-level
topologies.  We feel that this is not unrealistic, though, since in
practice the routers at a given PoP would probably just use a single
router at that PoP as a route reflector~\cite[Section
3.1]{POAMZ10}. The topologies we used are summarized in Table
\ref{tab:topologies}.

We compare the number of iBGP sessions used by a full mesh to the
number of edges in the overlay produced by the primal-dual algorithm.
We assume (conservatively) that all the nodes in the topology are
external BGP routers.  Our results are shown in Table
\ref{tab:results} and in Figure \ref{fig:main}.  These results show
that the primal-dual algorithm gives graphs that are much smaller than
the default full mesh.  Of course, we do not model additional
requirements such as fault-tolerance and stability, but the massive
gap suggests that even if adding extra requirements results in
doubling or tripling the size of the overlay we will still see a large
benefit over the full mesh.  Moreover, these results show that the
 upper bound on the approximation ratio that we
proved in Section~\ref{sec:PD} is extremely pessimistic.  On these
actual topologies the primal-dual algorithm gives results that are
only slightly larger than  (the worst case is for Level 3, in which
the primal-dual algorithm gives an overlay with about 
edges).  Since  is an obvious lower bound (the overlays clearly
must be connected), this means that in practice our algorithm gives a
-approximation.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
AS & full-mesh & Primal-Dual & Fraction of full-mesh \\
\hline
1221 & 946 & 44 & 4.65\% \\
1239 & 1326 & 83 & 6.26\% \\
2914 & 2415 & 109 & 4.5\% \\
3257 & 820 & 75 & 9.15\% \\
3356 & 1953 & 173 & 8.86\% \\
\hline
\end{tabular}
\end{center}
\caption{Primal-Dual vs.\ full-mesh}
\label{tab:results}
\end{table}

\begin{figure}
\begin{center}
\includegraphics[scale=0.65]{plot.eps}
\end{center}
\caption{Primal-Dual vs.\ full-mesh}
\label{fig:main}
\vspace*{-.2in}
\end{figure}




\section{Complexity of {\sc iBGP-Sum} and {\sc iBGP-Degree}} \label{sec:iBGP_hardness}

In this section we will show that the iBGP problems are -hard to approximate by a reduction from {\sc Hitting Set} (or
equivalently from {\sc Set Cover}).  This is a much weaker hardness
than the  hardness that we prove for the general
Constrained Connectivity problems in Section~\ref{sec:CC}, but the iBGP problems are much more
restrictive.  We note that this  hardness is easy to prove for Constrained Connectivity on ; the main difficulty is constructing a metric so that the geometrically defined safe sets of iBGP have the structure that we want.

We begin by giving a useful gadget that
encodes a {\sc Hitting Set} instance as an instance of an iBGP problem
in which all we care about is minimizing the degree of a particular
vertex.  We will then show how a simple combination of these gadgets
can be used to prove that {\sc iBGP-Degree} is hard to approximate,
and how more complicated modifications to the gadget can be used to
prove that {\sc iBGP-Sum} is hard to approximate.

Suppose we are given an instance of hitting set with elements  (note that we are overloading these as both integers and
elements) and sets .  Our gadget will contain
a node  whose degree we want to minimize, a node  for all
elements , and a node  for each set
 in the instance.  We will also have four extra ``dummy" nodes:
, and .  The following table specifies some of the distances
between points.  All other distances are the shortest path graph
distances given these. Let  be some large value (e.g.~), and let 
be some extremely small value larger than .

\begin{center}
\begin{tabular}{c|ccccccc}
 & x & z & y &  &  & u & h \\
\hline
x &  & M &  &  &  & &  \\
z & M & &   &  &  &  &  \\
y & &  & & \\
 & & & & &  (if ) & &  \\
 &  & & &  (if ) & & &  \\
 & &  & & \\
 &  & & & & 
\end{tabular}
\end{center}

It is easy to check that this is indeed a metric space.  Informally,
we want to claim that any solution to the iBGP problems on this
instance must have an edge from  to  nodes such that the
associated elements  form a hitting set.  Here , and  are nodes that force the safe sets into the form we want, and  is used to guarantee the existence of a small solution.

\begin{lemma} \label{lem:gadget_necessary} Let  be any feasible
  solution to the above iBGP instance.  For every vertex 
  there is either an edge  or an edge  where 
\end{lemma}
\begin{proof}
  We will prove this by analyzing .  If we can show that
   then we
  will be finished.  Note that ,
  so the vertices outside  are  (distance
   from ),  (distance  from ),  (distance at least
   from ), and  with  (distance
   from ).  The vertices inside the ball are
  , all  nodes, and  with .

Obviously  and  are in  by definition.  Let
 be a vertex with .  It is easy to verify that 
is closer to  than to any vertex outside of the ball: it has
distance  from , distance 
from  with , distance  from ,
distance  from , and distance greater than  from .  So
 as required.  On the other hand, suppose .  Then , while , so
.  Similarly, any vertex  with  is closer to  (distance ) than to 
(distance at least ) and  is closer to  (distance ) than
to  (distance at least ).  Thus , so  must include an edge from
 to either  or an  with .
\end{proof}

We now want to use this gadget to prove logarithmic hardness for {\sc
  iBGP-Sum}.  We will use the basic gadget but will duplicate .  So there
will be  copies of , which we will call , and their distances are defined to be  and  with all
other distances defined to be the shortest path.  Note that all we did
was modify the gadget to ``break ties'' between the 's.  Also
note that the shortest path between  and  is through , for
a total distance of .  As before, let  be the
smallest hitting set.

\begin{lemma} \label{lem:ibgp-sum-necessary}
  Any feasible {\sc iBGP-Sum} solution has at least  edges.
\end{lemma}
\begin{proof}
  It is easy to see that Lemma \ref{lem:gadget_necessary} still holds,
  i.e.~that .  Intuitively this is because all other  nodes are outside
  of  and all distances from  to the gadget
  are the same as before except with an additional .  This
  implies that the number of  and  nodes adjacent to
   in any feasible solution must be at least , since if
  there were fewer such adjacent nodes it would imply the existence of
  a smaller hitting set (any  nodes adjacent to  could
  just be covered using an arbitrary element in  at the same cost
  as using the set itself).  Thus the total number of edges must be at
  least .
\end{proof}

\begin{lemma} \label{lem:ibgp-sum-sufficient}
  There is a feasible {\sc iBGP-Sum} solution with at most  edges.
\end{lemma}
\begin{proof}
  The solution is simple: create a clique on the 
  nodes (which obviously has size at most ), include an
  edge from every  to  (another  edges) and include an
  edge from every  to every  with  (another  edges).  Obviously there are the right number of edges in this
  solution, so it remains to prove that it is feasible.  To show this
  we partition the pairs into types and show that every pair in every
  type is satisfied.  The types are
  \begin{inparaenum}[\itshape 1\upshape)]
  \item ,
  \item ,
  \item ,
  \item  (where  is any other node in the gadget
    not included in a previous type), and
  \item 
  \end{inparaenum}
  This is clearly an exhaustive partitioning, so we can just
  demonstrate that each type is satisfied in turn.

  For the first type we already showed that  includes
  all  where .  Since  is a valid hitting set 
  must be adjacent to one such , which in turn is adjacent to
  , forming a valid safe path.  For the second type the only
  vertices outside  are  with , and
   is closer to  than to any such .  Thus 
  so the path  in our solution is a valid safe path.  For
  the third type the vertices outside  are .  Because of the tie-breaking we
  introduced,  while , and thus  and so the
  path  in our solution is a valid safe path.  The
  fourth type is even simpler, since  must be either ,
  or an  node and the shortest path from  to any of these is
  through .  So  and  is a
  valid safe path.  Finally, for the last type the vertices outside
   are , and  is closer
  to  (distance ) than any such  (distance
  ).  So again  and thus  is a valid safe path.
\end{proof}


\begin{theorem}
\label{thm:iBGP_sumhard}
  It is NP-hard to approximate {\sc iBGP-Sum} to a factor better than
  , where  is the number of vertices in the
  metric.
\end{theorem}
\begin{proof}
  It is known that there is some  for which it is NP-hard to
  distinguish hitting set instances with a hitting set of size at most
   from instances in which all hitting sets have size at least
  .  In the first case we know from Lemma
  \ref{lem:ibgp-sum-sufficient} that there is a valid {\sc iBGP-Sum}
  solution of size at most .  In the
  second cast we know from Lemma \ref{lem:ibgp-sum-necessary} that any
  valid {\sc iBGP-Sum} solution must have size at least .  If we set  =  this gives a gap of .  The number of vertices  in the {\sc iBGP-Sum}
  instance is  so , and thus we
  get  hardness of approximation.
\end{proof}

It is also fairly simple to modify the basic gadget to prove the same logarithmic hardness for {\sc iBGP-Degree}.  We do this by duplicating everything \emph{other} than , instead of duplicating .  This will force  to have the largest degree.

\begin{theorem}
\label{thm:iBGP_deghard}
  It is NP-hard to approximate {\sc iBGP-Degree} to a factor better than
  , where  is the number of vertices in the
  metric.
\end{theorem}
\begin{proof}
  We will use multiple copies of the above gadget.  Let  be some
  large integer that we will define later.  We create  copies
  of the gadget but identify all of the  vertices, so there is
  still a unique  but for all other nodes  in the original there
  are now  copies .  The distance
  between two nodes in the same copy is exactly as in the original
  gadget, and the distance between two nodes in different copies (say
   and ) is the distance implied by forcing them to go
  through  (i.e.~).  Call this
  metric .  Every vertex in copy  is closer to the rest
  of copy  than to any vertex in copy , so Lemma
  \ref{lem:gadget_necessary} holds for every copy.  Thus if the
  smallest hitting set is  the degree of  in any feasible
  solution to {\sc iBGP-Degree} on  must be at least .

  Conversely, we claim that there is a feasible solution to {\sc
    iBGP-Degree} in which every vertex has degree at most
  .  Consider the solution in which  is adjacent to
   and to  for all  and , and all
  nodes (other than ) in copy  are adjacent to all other nodes
  (other than ) in copy  for all .  By the above
  analysis of  we know that this solution satisfies
  these safe sets (via the safe path  where 
  is an element in ).  It also obviously satisfies pairs not
  involving  in the same copy, since there is an edge directly
  between them.  It remains to show that pairs involving  are
  satisfied and that pairs involving two different copies are
  satisfied.

  For the first of these we will show that  is in all safe sets of
  the form  where  is not a  node.  This is easy to
  verify exhaustively.  It is also true that  is
  in all safe sets of the form  even when  is a  node,
  since all vertices outside the ball  are in
  different copies and the shortest path from  to any node in a
  different copy must go through .  Thus the path  in our
  solution satisfies both of these safe sets.  Finally, it is again easy to verify that
  pairs in different copies are also satisfied.

  Now by setting  appropriately we are finished.  Each copy
  has  nodes, so in the feasible solution we have constructed
  the degree of any node other than  is at most .
  If we set  to some value larger than this, say ,
  we know that the degree of  has to be at least .
  It is known that it is hard to distinguish between hitting set
  instances with hitting sets of size at most  and
  those in which every hitting set has size at least  for
  some value .  Suppose that we are in the first case, where
  there is a hitting set of size at most .  Then we constructed
  a feasible solution to the {\sc iBGP-Degree} problem with maximum
  degree at most .  In the second case, where
  every hitting set has size at least , we showed that
  the degree of  (and thus the maximum degree) must be at least
  .  This gives a gap of , which is clearly .  Since the number of
  vertices in the {\sc iBGP-Degree} instance is polynomial in ,
  this implies -hardness.
  \end{proof}





\section{Constrained Connectivity} \label{sec:CC}
In this section we consider the hardness of the Constrained Connectivity problems and the integrality gaps of the natural LP relaxations.

\subsection{Hardness}
We now show that the {\sc Constrained Connectivity-Sum} and {\sc
  Constrained Connectivity-Degree} problems are both hard to approximate to better than
 for any constant .  We do this via a reduction from {\sc
  Min-Rep}, a problem that is known to be impossible to approximate to
better than  unless ~\cite{Kortsarz:99}. An instance of {\sc
  Min-Rep} is a bipartite graph  in which  is
partitioned into groups  and  is partitioned
into group .  There is a \emph{super-edge}
between  and  if there is an edge  such that
 and .  The goal is to find a minimum set  of
vertices such that for all super-edges  there is some
edge  with  and  and .  Vertices from a group that are in  are called the \emph{representatives} of the group.
It is easy to prove by a reduction from {\sc Label Cover} that {\sc
  Min-Rep} is hard to approximate to better than , and in particular it is hard to distinguish the case when 
vertices are enough (one from each part in the partition for each side
of the graph) from the case when 
vertices are necessary~\cite{Kortsarz:99}.

Given an instance of {\sc Min-Rep}, we want to convert it into an
instance of {\sc Constrained Connectivity-Sum}.  We will create a graph
with five types of vertices:  for  and ; ; ;  for  and ; and .
Here the  nodes represent  copies of the groups of  and the
 nodes represent  copies of the groups of , where  is some
parameter that we will define later.   is a dummy node that we will
use to connect pairs that are not crucial to the analysis.  Given
this vertex set, there will be four types of edges:  for all  and  and ;  for all edges  in the original {\sc Min-Rep}
  instance;  for all  and  and ; and  for all vertices .

\begin{figure}
\centerline{\scalebox{.75}{\begin{picture}(0,0)\includegraphics{hardness.pstex}\end{picture}\setlength{\unitlength}{2901sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(5767,2511)(211,-3570)
\put(226,-1242){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(226,-3042){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(226,-1692){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(226,-3492){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture} }}
\caption{Basic hardness construction.}
\label{fig:CC_hard}
\end{figure}

This construction is shown in Figure \ref{fig:CC_hard}, except in the actual construction there are  copies of each node in the top and bottom layer and there is a  node that is adjacent to all other nodes.  In Figure \ref{fig:CC_hard} the middle two layers are identical to the original {\sc Min-Rep} problem, and the large ellipses represent the groups.  In the figure we have simply added a new vertex for each group, and in the construction there are  such new vertices per group as well as a  vertex.

Now that we have described the constrained connectivity graph, we need
to define the safe sets.  There are two types of safe sets: if in the
original instance there is a super-edge between  and  then
 for all .  All other safe sets consist of the two
endpoints and .  Let  denote the number of super-edges in
the {\sc Min-Rep} instance, let  denote the number of
vertices.

The following theorem shows that this reduction works.  The intuition behind it is that a safe path between an  node and a  node corresponds to using the intermediate nodes in the path as the representatives of the groups corresponding to the  and  nodes, so minimizing the number of labels is like minimizing the number of edges incident on  and  nodes.

\begin{theorem} \label{thm:Subgraph_Reduce}
  The original {\sc Min-Rep} instance has a solution of size at most
   if and only if there is a solution to the reduced Constrained
  Connectivity problem of size at most .
\end{theorem}
\begin{proof}
  We first prove the only if direction by showing that if there is a
  {\sc Min-Rep} solution of size  then there is a Constrained
  Connectivity solution of size .  Let
   be the set of vertices in a Min-Rep solution of size .
  Our constrained connectivity solution includes all edges of type
  , i.e.\ we include a star centered at .  For each 
  and  we also include all edges of the form 
  where  and all edges of the form  where .  Finally, for each super-edge
  in the Min-Rep instance we include the edge between the pair from
   that satisfies it (if there is more than one such pair we
  choose one arbitrarily).  The star clearly has  edges,
  there are  edges from  and  nodes to nodes in ,
  and there are clearly  of the third type of edges, so the
  total number of edges in our solution is  as required.  To prove that it is a valid solution, we first
  note that for all pairs except those of the form  or
   where  is a super-edge are satisfied
  via the star centered at .  For pairs  and
   with an associated super-edge, since  is
  a valid solution there must be some  and  that have an edge between them, and the above
  solution would include that edge as well as the edge from  to
   and from  to , thus forming a safe path of length .

  For the if direction we need to show that if there is a Constrained
  Connectivity solution of size  then
  there is a Min-Rep solution of size at most .  Let  be
  a constrained connectivity solution with  edges.  Since  for all vertices ,  of those edges must be a star centered at , so only  edges are between other vertices.  Obviously there need to
  be at least  edges between  and , since otherwise it
  would be impossible to satisfy all of the demands between  and
   nodes corresponding to super-edges.  Thus there are at most 
  edges incident on either  or  nodes.  We can partition these
  edges into  parts, where the edges in the th part are those
  incident on an  or  node.  So there must be one part of
  size at most ; let  be this part.  But since this is a valid
  constrained connectivity solution there is a safe path between
   and  for all  such that there is a
  super-edge between  and , and thus the nodes in 
  and  that are incident to edges in this th part must form a
  valid Min-Rep solution of size at most .
\end{proof}

We can now set , which gives the following theorem:

\begin{theorem} \label{thm:ccsumhard}
  {\sc Constrained Connectivity-Sum} cannot be approximated better
  than  for any  unless 
\end{theorem}
\begin{proof}
  We know that it is hard to distinguish between an instance of
  {\sc Min-Rep} with a solution of size at most  and an instance in
  which every solution is of size at least .  Let .  Then Theorem
  \ref{thm:Subgraph_Reduce} implies that it is hard to distinguish
  between an instance of constrained connectivity with a solution of
  size at most  and an instance in which every solution has size at least
  .  This
  gives an inapproximability gap of .  Since  the number of vertices  in our
  constrained connectivity instances is , and thus .  To get this to  we can simply use a smaller .
\end{proof}

We will now prove that {\sc Constrained Connectivity-Degree} has the same hardness of approximation of {\sc Constrained Connectivity-Sum}.  The reduction from {\sc Min-Rep} to the degree problem is basically the same as the reduction to the sum problem, except there are also  additional copies of the gadget other than the  and  nodes.  More formally, now the nodes are  and  for  and ,  for  and ,  for  and , and  for .  Now intuitively each copy  of the original , and  is hooked together exactly like in the original construction, and is hooked up to the nodes  and  exactly as if they were one copy of the outer  and  nodes of the original construction.

More formally, the edges are the same as before, except now each of the  new copies is independent.  In other words, there is an edge between  and  for all  and  and , an edge between  and  for all  and  and , an edge between  and  for all  and edges  in the original {\sc Min-Rep} instance, an edge between  and  for all  and , an edge between  and  for all  and , an edge between  and  for all  and , and an edge between  and  for all  and .  Similarly, the safe sets are as before but defined by the copies.  That is, .  All safe sets between nodes in the same copy  are the two endpoints together with , and the safe set of vertices in different copies is just all vertices.

\begin{theorem} \label{thm:ccdeghard}
{\sc Constrained Connectivity-Degree} cannot be approximated better than  for any constant  unless 
\end{theorem}
\begin{proof}
Every vertex in  or  can have degree at most , since there are only  other nodes in its copy, and it can in addition be adjacent to  and and the node  or  corresponding to its group  and  respectively.  Every node  has degree at most , since it can be adjacent to  nodes in  and  as well as  nodes from  and  nodes from .  On the other hand, every  node and every  node must be adjacent to at least   or  node respectively for all  possibilities for .  So every such  or  nodes has degree at least , so if we set  we know that the node with maximum degree must be an  or a  node.

Recall that it is hard to distinguish {\sc Min-Rep} instances with solutions of size at most  from those in which all solutions have size at least .  Suppose that there is a solution of size , i.e.\ there is a solution with one representative from each group.  Then there is a solution to the corresponding {\sc Constrained Connectivity-Degree} instance with max degree at most : every  and  is connected to its corresponding representative in each of the  copies corresponding to it as well as to the  node for that copy, and in each copy  we include all edges between  and  and all edges between those nodes and .  It is easy to see that this is a valid solution: by the analysis of Theorem \ref{thm:Subgraph_Reduce} we know that it is valid inside of each copy, and to get between copies nodes  and  can use the safe path , where  and  are arbitrary nodes in the copy , and  is an arbitrary index in .

On the other hand, suppose that every solution to the {\sc Min-Rep} instance has size at least .  Then as in the analysis of Theorem \ref{thm:Subgraph_Reduce} for every copy  there must be at least  edges that are either between  and  or between  and .  Thus there are at least  such edges.  Since there are only  vertices in , at least one such vertex must have degree at least .

This shows that it is hard to approximate {\sc Constrained Connectivity-Degree} to better than .  Since the number of vertices  in our instances is polynomial in , this means that it is hard to approximate to better than .  We can then get this to  by just using a smaller .
\end{proof}



\subsection{Integrality Gap for Constrained Connectivity}


We claim that the integrality gap of the flow and cut LP relaxations is large for both {\sc
  Constrained Connectivity-Sum} and {\sc Constrained Connectivity-Degree}.  The intuition is that we use a {\sc Min-Rep} instance in which the edges between each group form a matching (allowing the LP to cheat by breaking up the flow) but many representatives are needed for a valid solution.  This instance is then changed into a Constrained Connectivity problem as in the hardness reduction.  These results are in many ways similar to the  integrality gap for {\sc Min-Rep} recently proved by Charikar et al.~\cite{CHK09}, but the reduction to Constrained Connectivity adds extra complications.

  The instances for
which we will show a large integrality gap are derived from instances
of the \emph{Unique Games problem}, in which we are given a graph
 and a set of permutations  on some alphabet
 (one constraint for every edge ) and are asked
to assign a value  from  to each vertex  so as to satisfy
the maximum number of constraints of the form .
This problem was first considered by Khot~\cite{khot:02}, who
conjectured that it was NP-hard to distinguish instances on which
 fraction of the constraints can be satisfied from instances
on which at most  fraction of the constraints can be
satisfied (for sufficiently small  and ).  For our
purposes we will consider a minimization version of the Unique Games
problem in which we can assign multiple labels to vertices and the
goal is to assign as few labels as possible so that for every edge
 there is some label  assigned to  with 
assigned to .  We first show that there exist instances that
require many labels:

\begin{lemma} \label{lem:Unique} For any constant ,
  there are instances of Unique Games with alphabet size
   and  edges that
  require  labels for any valid solution.
\end{lemma}
\begin{proof}
  We will prove this by the probabilistic method, i.e.\ we will
  analyze a \emph{random} Unique Games instance with the given
  parameters and show that the probability that it has a solution of
  size at most  is strictly less than .  This then implies
  the existence of such an instance.  For our random instance, the
  underlying graph will be , so there is a permutation constraint
  on every pair of vertices.  Let  be the size of the
  alphabet (we will later set this to the value claimed in the lemma, but for now we will leave it
  as a parameter).  For each pair of vertices we will then select a
  permutation uniformly at random from .

  Now consider some fixed set  of  labels (so the average
  number of labels per node is ).  What is the probability
  that  is a valid solution?  By Markov's inequality, we know that
  at most  vertices have more than  labels, so there are
  at least  vertices with at most  labels.  Call these
  vertices \emph{light}, and call an edge \emph{light} if both of its
  endpoints are light.  Let  be a light edge.  We claim that
  the probability that  satisfies  is at most
  .  To see this, let  be one of
  the labels assigned to  by .  Since the permutation for
   was chosen uniformly at random, the probability that
   is matched to one of the labels assigned to  by  is at
  most .  Now we can do a union bound over all such
  labels , of which there are at most , to get that the
  probability that edge  is satisfied by  is at most
  .  Since the permutations for each edge are
  chosen independently, the event that edge  is satisfied is
  independent of the event that edge  is satisfied for all .  Thus the probability that  satisfies \emph{every} edge
  is at most the product of the probabilities that it satisfies each
  fixed edge, i.e.\ the probability that  is a valid solution is at
  most  (for sufficiently large ).

  By the trivial union bound, we know that the probability that there
  is \emph{some} valid solution of size  for our random
  instance is at most the sum over all possible solutions of size
   of the probability that the solution is valid, which by
  the above analysis we know is at most .  So we
  will now bound , which is easy to do
  by a simple counting argument.  In particular, it is obvious that , since there are exactly  total labels
  and we are just choosing  of them.  Now standard bounds
  for binomial coefficients imply that .  Combining this with the previous analysis and setting
  , we get that the probability that there is
  some valid solution of size  is at most

  

  The final inequality is true as long as  is sufficiently large.  If
  we set  then this expression is
  less than .  Since this is the probability that the random Unique
  Games instance we selected has a satisfying solution of size , this implies that for the given parameters there is \emph{some}
  unique games instance that requires more than  labels.
  \end{proof}

  Now that we have found a Unique Games instance that requires many
  labels we would like to use it to construct a {\sc Constrained
  Connectivity-Sum} instance on which the flow LP has large integrality
  gap.  We will basically use the same transformation that we used in
  the reduction of {\sc Min-Rep} to {\sc Constrained Connectivity-Sum}.  Let
   be the vertex set of the above Unique Games instance, and
  let  be the alphabet.  Then our {\sc Constrained Connectivity-Sum}
  instance will have vertex set  equal to the disjoint union of
  , , and a special node ,
  where  is a duplication parameter that we will set later.  For
  ease of notation, we will let  denote the 'th copy of vertex
   in , i.e.\ .  For all  and  there is an edge from  to every vertex
  in .  For every  and  there is an edge between  and 
  if and only if assigning  to  and  to  is
  sufficient to satisfy the  edge in the Unique Games
  instance (i.e.\ the permutation for that edge matches them up).
  There is also an edge between every vertex and .  For  and  we set , and we set all other
  safe sets to the two endpoints and .

  \begin{lemma} \label{lem:LP_gap} The value of the flow LP on the
    above {\sc Constrained Connectivity-Sum} instance is at most .
  \end{lemma}
  \begin{proof}
    We prove this by constructing an LP solution of the required size.
    We first set the capacity of every edge incident on  to ,
    for a total cost of .  This is
    enough capacity to satisfy all pairs other than those of the form
     or , since for any other pair  is in
    the safe set so we can send one unit of flow on the edge from one
    endpoint to  and then one unit of flow on the edge from  to
    the other endpoint.

    Now we set the capacity of every other edge to .
    Since the number of other edges is  this costs us  more, which when added to the cost of the edges to
     gives us the claimed total LP value.  So we just need to prove
    that this is enough capacity to satisfy demands between  and
     for all  and .  But this is easy to
    see:  can send  flow to every node in  (for a total flow of ), and each of these nodes will
    forward its incoming flow to its neighbor in .
    Since this is a Unique Games instance this neighbor will be
    unique, and each node in  will have exactly  incoming flow, which it can then forward along its edge
    to .  Thus we have enough capacity to send one unit of flow
    from  to .  And  can send flow to  the same
    way, just in reverse.
  \end{proof}

  \begin{lemma} \label{lem:IP_gap} Any integral solution to the above
    {\sc Constrained Connectivity-Sum} instance must have size at least  where  is the minimum number of labels
    needed to satisfy the original Unique Games instance.
  \end{lemma}
  \begin{proof}
    The safe set of any node and  is only that node and , so all
    edges incident to  need to be present in any integral solution
    for a cost of .  Furthermore, for
    every pair  at least one edge must be present from
     to  since if no such edge
    existed there would be no way of connecting  and 
    through  for any .  This adds  to the total cost, so now we just need to prove that
    there must be at least  edges between  and .

    To show this, we will consider some arbitrary integral solution
    and partition the edges between  and  into  parts where the th part consists of
    those edges incident on nodes .  If every
    part has size at least  then we are finished.  To prove
    that this is indeed the case, we will prove that for every part,
    the endpoints that are in  actually form a
    valid solution to the Unique Games instance.  So consider the
    th part of the partition.  Suppose that the associated label
    assignment does not form a valid solution to the Unique Games
    instance.  Then there is some pair  such that none of
    the labels assigned to  and none of the labels assigned to 
    are matched to each other in the permutation corresponding to edge
    .  But this clearly implies that there is no safe path
    from  to , as any such path must be of length  and
    pass through a label for  and a label for  that are matched
    to each in the permutation corresponding to edge .  This
    is a contradiction since the integral solution must be a valid
    solution.
  \end{proof}



  \begin{theorem} \label{thm:gap} The flow LP for {\sc Constrained Connectivity-Sum} has an integrality gap
    of  for any constant .
  \end{theorem}

  \begin{proof}
    We will use the Unique Games instance of Lemma \ref{lem:Unique}
    in the above reduction.  Lemma \ref{lem:LP_gap} implies that the
    flow LP has value at most  and Lemma
    \ref{lem:IP_gap} implies that any integral solution has size at
    least .  If we let  then this gives us an
    integrality gap of
  
  It is easy to see that the number of nodes  in our reduction
  equals  which in this case is
  .  Thus the integrality
  gap is , which is
  sufficient since we can set  to be arbitrarily small.
\end{proof}

We can modify this construction to show a polynomial integrality gap for the flow LP for {\sc Constrained Connectivity-Degree} also.  We will need Unique Games instances with the same parameters as in Lemma \ref{lem:Unique} but on the complete bipartite graph rather than the complete graph.  It is easy to see that Lemma \ref{lem:Unique} can be modified to prove the existence of these instance.  Now the modification is basically the same as the modification we made to show hardness: we just make  copies of the inner Unique Games instance and connect them up to the  copies of the outer  and  nodes in the obvious way.

\begin{theorem} \label{thm:degree_gap}
The flow LP for {\sc Constrained Connectivity-Degree} has an integrality gap of  for any constant .
\end{theorem}
\begin{proof}
The maximum degree of any node other than the outer  copies of the  and  nodes is at most , so if we set  equal to that value we know that the maximum degree must be achieved by some copy of an  or .  By splitting up the flow equally as in the proof of Lemma \ref{lem:LP_gap} we know that there is an LP solution in which the maximum degree is at most  (where the extra  factor is due to being adjacent to all associated  copies).  On the other hand, we know that any valid integer solution must use at least  edges incident on copies of  or  nodes for each of the  instances.  Thus there are at least  edges incident on these nodes in total, and since there are  such nodes there must be at least one with degree at least .  Thus the integrality gap is at least .  The total number of nodes in our {\sc Constrained Connectivity-Degree} instance is , so this means the integrality gap is .  By setting  small enough this gives us the claimed gap of .
\end{proof}






\section{Hierarchical and Symmetric Safe Sets} \label{sec:hierarchical}

While the constraint the  gave us some extra power for the iBGP problems, we did not leverage the structure of the safe sets in any way.  In this section we get rid of the requirement on , but show that if the safe sets have an extremely nice structure then {\sc Constrained Connectivity-Sum} can actually be solved optimally in polynomial time.  In the
hierarchical and symmetric safe set version of {\sc Constrained Connectivity-Sum},  for all  and
if some node  then  and .  We show that a simple greedy algorithm solves this
version optimally.

We say that a pair  is an \emph{easy} pair if there is some
node  such that  and .  The pair  is \emph{hard} otherwise.  Note
that in a hard pair , every node  in  has either
 or  by the hierarchy property.

\begin{lemma} \label{lem:hierarchy_easy}
  Let  be a graph that has a safe path for all hard pairs.  Then
  all easy pairs also have a safe path in , i.e.\  is a feasible
  solution.
\end{lemma}
\begin{proof}
  We prove that every pair  has a safe path in  by
  induction on the size of safe sets.  For the base case, all pairs
   with  are hard, so by assumption they have a
  safe path in .  For the inductive step, suppose that there are
  safe paths for all pairs  with , and let
   be a pair with .  If  is hard then
  by assumption there is a safe path.  If it is easy, then there is
  some node  such that  and
  .  Since these two subsets are strictly
  smaller, by induction there is an  path contained in  and there is a  path contained in .  Concatenating these paths give an  path
  contained in .
\end{proof}


This lemma means that we don't have to worry about satisfying easy
pairs, just hard ones.  We now prove a few structural lemmas that will be useful when designing an algorithm.


\begin{lemma} \label{lem:hierarchy_strong_containment} Let 
  be a hard pair.  Then  for all .
\end{lemma}
\begin{proof}
  Since  is hard either  or .  Without loss of generality we assume that .  This implies that , so by the hierarchy
  property we know that .
\end{proof}

This clearly implies that if  is a feasible solution and 
is a hard pair then  is connected and all pairs  have a safe path contained in .  We now prove some
lemmas about the structure of the optimal solution.

\begin{lemma} \label{lem:hierarchy_OPT_hard}
  Every edge  is a hard pair.
\end{lemma}
\begin{proof}
  Suppose  is an edge in  that is an easy pair.  Then
  there is some  such that  and
  .  Note that , since if it
  was then by the hierarchy property we would have that , so  contradicting 
  being an easy pair.  Similarly, we know that .
  Since  is feasible there is an  path in  and a  path in , and by the
  previous observation neither of them use the  edge.  So
  there is an  safe path in  that does not use the 
  edge.  Any hard pair  that use the  edge in a safe
  path can just use the path we found through , since by Lemma
  \ref{lem:hierarchy_strong_containment} .
  Thus if we remove  all of the hard pairs still have a safe
  path, so by Lemma \ref{lem:hierarchy_easy} so do all of the easy
  pairs.  This contradicts  being optimal.
\end{proof}

Order all hard pairs in nondecreasing order, breaking ties
arbitrarily.  We say  if  comes before
 in this ordering.  We partition the edges of  as
follows.  Let  be an edge in , and let  be
the first hard pair in the ordering such that  and , and assign  to part .  By Lemma
\ref{lem:hierarchy_OPT_hard} all edges in  are hard pairs so this
is a valid partition.  Let , and let  be defined
analogously.

\begin{lemma} \label{lem:hierarchy_OPT_connected}
  Let  be a hard pair.  Then 
  is connected.
\end{lemma}
\begin{proof}
  Let  be an edge in .  Then since 
  is a hard pair (by Lemma \ref{lem:hierarchy_OPT_hard}) and 
  is a hard pair with both  and  in , by the definition
  of the partition the part  containing  must
  have .  Thus .
\end{proof}

We now finally give our algorithm.  First we construct the above
ordering.  We then consider hard pairs in this order, and when
considering a pair  we add the minimum number of edges
required to make our current graph restricted to  connected.
This algorithm clearly returns a feasible solution, since for any hard
pair  at some point we consider it and make sure that its
safe set is connected and that is sufficient by Lemma
\ref{lem:hierarchy_easy}.  For every hard pair , let
 by the edges added by the algorithm when considering
, and define  and  analogously.  Now we will
prove that .

\begin{lemma} \label{lem:hierarchy_OPT_ALG}
  The endpoints of any edge in  are
  connected in .
\end{lemma}
\begin{proof}
  Let  be an edge in .  Then
   for some .  By
  definition, this means that  is the first pair in the
  ordering with a safe set that contains both  and .  By Lemma
  \ref{lem:hierarchy_strong_containment} we know that .  We also know that  is a hard pair by
  Lemma \ref{lem:hierarchy_OPT_hard}, so if 
  then  would be before  in the ordering and would
  contain both  and , contradicting the definition of .
  Thus .  After considering  the algorithm
  guarantees that  is connected, and
  therefore there is a safe  path in  after considering
  .  We also know from Lemma
  \ref{lem:hierarchy_strong_containment} that , so this safe path is entirely present in  and thus  and  are connected in .
\end{proof}

\begin{theorem} \label{lem:hierarchy_main}

\end{theorem}
\begin{proof}
  We will prove that  for all
  hard pairs .  Since these form a partition of the edges of
   and of , this is sufficient to prove that .  Consider some such hard pair .  We know from Lemma
  \ref{lem:hierarchy_OPT_connected} that  is connected, so  must contain
  enough edges to connect the components of .  By the definition of the algorithm,
   has the minimum number of edges necessary to connect
  the components of .  Now since the number
  of components in  is at most the number
  of components of  (by Lemma
  \ref{lem:hierarchy_OPT_ALG}), this implies that .
\end{proof}


\newpage

\bibliographystyle{abbrv}
\bibliography{cc}





\end{document}
