

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage[dvipsnames]{xcolor}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}


 \usepackage{url}
\usepackage{amsmath,graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color}
\usepackage{multicol}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}

\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

\def\paperID{1688} \def\confName{CVPR}
\def\confYear{2024}

\title{Boosting Audio-visual Zero-shot Learning with Large Language Models}

\author{Haoxing Chen$^{1}$, Yaohui Li$^{2}$, Yan Hong$^{1}$, Zizheng Huang$^{1,2}$, Zhuoer Xu$^{1}$\\  Zhangxuan Gu$^{1}$, Jun Lan$^{1}$, Huijia Zhu$^{1}$, Weiqiang Wang$^{1}$\\
$^{1}$Ant Group, $^{2}$Nanjing University\\
{\tt\small hx.chen@hotmail.com, yaohuili@smail.nju,edu.cn}}



\begin{document}
\maketitle

\begin{abstract}
Audio-visual zero-shot learning aims to recognize unseen categories based on paired audio-visual sequences. Recent methods mainly focus on learning aligned and discriminative multi-modal features to boost generalization towards unseen categories. However, these approaches ignore the obscure action concepts in category names and may inevitably introduce complex network structures with difficult training objectives. In this paper, we propose a simple yet effective framework named Knowledge-aware Distribution Adaptation (KDA) to help the model better grasp the novel action contents with an external knowledge base. Specifically, we first propose using large language models to generate rich descriptions from category names, which leads to a better understanding of unseen categories. Additionally, we propose a distribution alignment loss as well as a knowledge-aware adaptive margin loss to further improve the generalization ability towards unseen categories. Extensive experimental results demonstrate that our proposed KDA can outperform state-of-the-art methods on three popular audio-visual zero-shot learning datasets. Our code will be avaliable at \url{https://github.com/chenhaoxing/KDA}.

\end{abstract}

\section{Introduction}
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{fig1_av.pdf}
	\caption{Inspired by the fact that detailed descriptions can help people understand novel concepts and distinguish similar action contents, we propose to improve model generalization ability based on external knowledge.}
	\label{motivation}
\end{figure}
Visual and audio signals often coincide in videos. For example, in cinemas, high-quality displays and sound systems are installed to create an immersive experience. 
Humans also make integrated decisions by perceiving through multiple senses. 
In interpersonal communication, people often rely on observing facial expressions and body language while listening to others to understand their intentions.
Combining audio and visual signals is beneficial for many applications because acoustic features can help analyze people's emotions, and visual features can help locate the speaking objects in a video.
Consequently, there has been growing interest in learning more comprehensive audio-visual representations for multiple tasks~\cite{CASP-Net,egoav,AVFace,MIR-GAN}, such as robotic navigation~\cite{avms,AVLEN}, action recognition~\cite{CAAV}, highlight detection~\cite{JVAVHD}, and speech recognition~\cite{MIR-GAN}.

Learning audio-visual representations for specific tasks typically requires many annotated samples, which is time-consuming, expensive, and often impractical in certain applications. Additionally, the classes present in the existing datasets are limited, making it challenging for models to make accurate predictions when encountering unseen categories. This is due to their lack of necessary knowledge and context to make informed decisions about unfamiliar concepts. Zero-shot recognition~\cite{AVGZSLNet,AVCA,TCaF,HyperbolicAV} task has been proposed to enhance the ability of deep model to recognize unseen categories. In zero-shot recognition tasks, models are required to acquire transferable knowledge to handle data from unfamiliar classes effectively. In this paper, we focus on audio-visual zero-shot learning.

Previous work~\cite{TCaF,HyperbolicAV} has adopted the framework proposed in ACVA~\cite{AVCA} to solve zero-shot video classification tasks using paired audio-visual inputs. Specifically, AVCA learns to map audio-visual features to textual embeddings of category labels to classify samples from unseen categories. However, due to the limitations of text label representations with coarse granularity and weak representation ability, directly mapping audio-visual features to label feature space is not efficient and may introduce understanding bias.
Inspired by how humans utilize prior knowledge to learn novel visual concepts, we propose to overcome these problems by using large language models~\cite{brown2020language,touvron2023llama} as external knowledge bases to generate detailed descriptions of action concepts.
As a motivating example, we can see in Figure~\ref{motivation} that it can be difficult to distinguish between several similar actions that have not encountered before (e.g., Basketball Dunk V.S. Basketball Dunk).
However, once the model is exposed to detailed descriptions from an external knowledge base, it becomes easier to identify actions and correspondences between different categories.
As is shown in Figure~\ref{fig2}, a preliminary experiment verifies that more detailed text improves performance of model on several action datasets. 

To better utilize descriptions of action concepts, we propose Knowledge-aware Distribution Adaptation (KDA) method. 
KDA maps audio-visual and label knowledge features into a common space and learns mapping relationships between audio-visual and knowledge features through distribution alignment.
Specifically, we constrain the alignment of audio-visual features with knowledge features by the 2-Wasserstein distance~\cite{berthelot2017began,he2018wasserstein,chen2022multi}, which ensures that the distribution of samples belonging to the same category is similar.
To improve the inter-class separability of features, we propose a knowledge-aware adaptive margin loss.  
By adding an adaptive margin to the classification losses, knowledge-aware adaptive margin loss can effectively pull each class apart from the others. The adaptive margin is generated according to the knowledge distribution similarity of each pair of classes.
Our experiments show that KDA achieves state-of-the-art performance on the three action recognition datasets. 

\begin{figure}[t]
\centering
\includegraphics[width=0.99\linewidth]{fig2_av.pdf}
	\caption{We perform audio-visual zero-shot classification experiments on three benchmark datasets. We can find that textual descriptions with richer knowledge improve the generalization ability of models.}
	\label{fig2}
\end{figure}

Our main contributions are summarized as follows:
\begin{itemize}
    \item We propose a novel audio-visual zero-shot learning framework by leveraging knowledge from large language models, which greatly improves the generalization ability on unseen action categories.
    \item We propose a distribution alignment loss and a knowledge-aware adaptive margin loss to further separate different categories in the common embedding space according to their description similarities.
    \item Extensive experiments demonstrate that, the proposed KDA outperform existing models. And we perform a detailed analysis of the different proposed distribution alignment methods, demonstrating the benefits of our proposed model architecture.
    
\end{itemize}




\section{Related Works}
\subsection{Audio-visual Zero-shot Learning}
Parida \textit{et al.}~\cite{CJME} proposed the Audio-Visual Zero-Shot Learning (AVZSL) task and introduced the Coordinated Joint Multimodal Embedding (CJME) model to map video, audio, and text into the same feature space for comparison. To ensure proximity between video or audio features and their corresponding class features, the triplet loss was utilized.
Mazumder \textit{et al.}~\cite{AVGZSLNet} introduced the Audio-Visual Generalized Zero-shot Learning Network (AVGZSLNet) to tackle audio-visual zero-shot learning. The AVGZSLNet includes a module that reconstructs text features using visual and audio features.
The Audio-Visual Cross-Attention (AVCA) framework~\cite{AVCA} is specifically devised to facilitate the exchange of information between video and audio representations. This enables informative representations that contribute to achieving state-of-the-art performance in audio-visual zero-shot classification.
Mercea \textit{et al.}~\cite{TCaF} proposed a multi-modal and Temporal Cross-attention Framework (TCaF).
Hong \textit{et al.}~\cite{HyperbolicAV} incorporated hyperbolic learning with a hierarchical structure into AVZSL and achieved promising results.
Different from the above methods, this paper maps audio-visual representations and knowledge representations to the same feature space, and improves the generalization of the model through distribution alignment.

\subsection{Margin Loss in Visual Recognition}
Softmax loss is widely used in training CNN to extract features for object recognition tasks. 
Previous works have observed that the weights of the last fully connected layer of a classification CNN trained based on Softmax loss are conceptually similar to the centroid of each category. 
Consequently, some margin loss approaches have been proposed~\cite{Arcface,SphereFace,CosFace} to improve the discriminability of features. 
Liu \textit{et al}.~\cite{SphereFace} introduced the angular margin, but their loss function requires approximate computation, which leads to unstable network training. 
In contrast, Wang \textit{et al}.~\cite{CosFace} added the cosine margin directly to the objective function and obtained better results than~\cite{SphereFace}. In addition, Deng et al~\cite{Arcface} proposed to add corner margin loss to further enhance the discriminability of the feature space.
Although the above margin losses achieve exciting results on visual understanding tasks, they are not suitable for AVZSL, where AVZSL is a multimodal task and no sample is provided for novel classes.
To ensure an appropriate margin, we propose a principle for generating adaptive margins guided by knowledge, taking into account the differences in knowledge distribution.
By training the AVZSL method with our proposed knowledge-aware adaptive margin loss, the model can successfully align audio-visual representations with knowledge representations, resulting in enhanced generalization performance on unseen categories.

\begin{figure*}[t]
	\centering	\includegraphics[width=0.97\linewidth]{AV_KAML.pdf}
	\caption{Overview of our proposed knowledge-aware distribution adaptation method (KDA). KDA takes the audio and visual features extracted from the video data as input, and obtains multi-modal audio-visual features $\rho_{av}$ for classification through the cross-attention module and embedding layer. To get better classification features, we promote feature learning through two knowledge-aware distribution adaptation methods. The knowledge description is obtained through the interpretation of the action name by ChatGPT, and then the knowledge representation $\rho_t$ is obtained by using the CLIP text encoder and embedding layer. We use distribution alignment loss $\mathcal{L}_{align}$ to enhance inter-class separability learning, utilizing knowledge-aware adaptive loss $\mathcal{L}_{kdam}$ to promote intra-class compactness learning.
}
	\label{KAML}
\end{figure*}

\subsection{Large Language Models for Downstream Tasks}
Recently, there have been various approaches in recent literature that utilize text generated from Large Language Models (LLMs) in different ways~\cite{santurkar2022caption,KP4cr,hu2022knowledgeable,yu2022multimodal,su2022language,yang2022empirical}. CLIP$_{\rm s}$~\cite{santurkar2022caption} leverages LLMs to interpret existing image captions, utilizing them for data augmentation in the CLIP framework. Liu \textit{et al.}~\cite{KP4cr} introduced a method called generated knowledge prompting, which involves eliciting and integrating knowledge from GPT-3 to enhance performance on commonsense reasoning tasks.
Su \textit{et al.}~\cite{su2022language} proposed MAGIC, a novel decoding scheme that incorporates visual controls into the generation process of a LLM. MAGIC is a training-free framework that enables the LLM to handle complex multimodal tasks in a zero-shot manner without sacrificing decoding speed. Similarly, Yang \textit{et al.}~\cite{yang2022empirical} employed GPT-3 along with text descriptions of images for the Visual Question Answering (VQA) task.
In our work, we specifically focus on leveraging LLMs to enhance the zero-shot generalization of models. Our aim is to utilize the knowledge stored within LLMs to improve the ability of models to recognize and understand unseen action categories, thereby enhancing their generalization performance in the context of audio-visual zero-shot learning.

\section{Knowledge-aware Distribution Adaptation}
\subsection{Preliminary}
We first introduce the problem definition of audio-visual zero-shot learning.
Audio-visual zero-shot learning aims to recognize videos from classes that have not been encountered during training, referred to as unseen classes. This task is particularly challenging as it requires the model to generalize knowledge from seen classes to recognize and classify samples from unseen classes.
In the generalized zero-shot learning (GZSL) task, the test set not only contains samples from unseen classes, but also samples from seen classes, which makes it more realistic and challenging. 




Formally, let $S={(a^s_i, v^s_i, w^s_i, y^s_i)}_{i=1}^{N}$ represent the training set consisting solely of samples from known classes. Here, $a^s_i$, $v^s_i$, and $w^s_i$ denote the audio, visual, and class-level text embeddings, respectively, and $y^s_i$ corresponds to the ground-truth label.
The goal is to train a model $f$: $f(v^s_i, a_s^i)\longmapsto w_s^i$ that can later be applied to samples from unseen classes, such that $f(v^u_i, a_u^i)\longmapsto w_u^i$. Here, $U={(a^u_i, v^u_i, w^u_i, y^u_i)}_{i=1}^{M}$ represents the set of test samples from unseen classes. The objective is to develop an effective model that can successfully transfer knowledge learned from seen classes to recognize and classify samples from unseen classes.


\subsection{Distribution Alignment}
In the following, we describe the architecture of our proposed KDA (see Figure \ref{KAML}).

\noindent
\textbf{Extra knowledge.} 
Inspired by how humans utilize knowledge bases to gain detailed understanding of new visual concepts, we propose using ChatGPT to generate more detailed descriptions for action concepts. As shown in Figure \ref{motivation}, humans often struggle to imagine the performance of uncommon action categories, and it becomes even more challenging to recognize them when encountering action nouns for the first time. Additionally, it is also easy to confuse visually similar action categories, such as "playing basketball" and "slam dunk". However, once we provide more detailed explanations of action nouns through a knowledge base, it becomes much easier to identify the correspondences between different videos and different action categories. 
Specifically, we let ChatGPT~\footnote{https://chat.openai.com/\label{gpt}} explain action class names in few sentences to provide more knowledge. We encode the generated language description through CLIP~\cite{clip} text encoder to obtain the knowledge representation $t$. Thus, the composition of the data set $S$ and $U$ becomes $S=\{(a^s_i, v^s_i, t^s_i, y^s_i)\}_{i=1}^{N}$ and $U={(a^u_i, v^u_i, t^u_i, y^u_i)}_{i=1}^{M}$ respectively. This enhancement allows for more informative and detailed knowledge representation, enabling the model to better understand and discriminate between different action categories.



\noindent
\textbf{Knowledge-aware distribution alignment.} Instead of using separate visual (audio) and text feature spaces for classification~\cite{AVCA,HyperbolicAV}, we map audio-visual features and knowledge representations into a common space and then classify them. 
To achieve this, we first extract multi-modal features, denoted as $\theta_a$ and $\theta_v$, using encoder blocks $A_{enc}$ and $V_{enc}$, a cross-attention module $F_c$, and projectors $A_{proj}$ and $V_{proj}$, respectively.
Subsequently, we utilize two distinct embedding layers, namely $E_{av}$ and $E_{t}$, to map the concatenated multi-modal features $\theta_{av}$ and knowledge representations $t$ into the common space.:
\begin{equation}
    \rho_{av} = E_{av}(\theta_{av})\quad \text{and} \quad \rho_{t} = E_{t}(t).
\end{equation}



To facilitate the learning of a distribution-aligned common space for audio-visual and knowledge feature representations, we employ the minimization of the 2-Wasserstein distance~\cite{olkin1982distance} between their respective latent multivariate Gaussian distributions. The formulation of this distance is defined as follows:
\begin{align}
    \mathcal{L}_{align} &= (||\mu_{\rho_{av}}-\mu_{\rho_t}||^2_2+ \\
    &{\rm trace}(\Sigma_{\rho_{av}}+\Sigma_{\rho_{t}}-2(\Sigma_{\rho_{av}}^{\frac{1}{2}}
    \Sigma_{\rho_{t}}
    \Sigma_{\rho_{av}}^{\frac{1}{2}}
    )^{\frac{1}{2}})
)^{\frac{1}{2}},
\end{align}
where $||\cdot||_2^2$ represents the squared Euclidean distance. However, it should be noted that the calculation of the above distance function involves square roots of matrices, which can be computationally expensive and make the optimization process challenging. To address this, we employ an approximation function~\cite{berthelot2017began,he2018wasserstein}:
\begin{equation}
\mathcal{L}_{align} = (||\mu_{\rho_{av}}-\mu_{\rho_t}||^2_2+||\Sigma_{\rho_{av}}^{\frac{1}{2}}-\Sigma_{\rho_t}^{\frac{1}{2}}||^2_{\rm F})^{\frac{1}{2}},
\end{equation}
where $||\cdot||_F^2$ denotes the squared 
matrix Frobenuis norm.

By utilizing the alignment loss $\mathcal{L}{align}$, we are able to obtain better aligned feature representations $\rho{av}$ and $\rho_{t}$. To classify an audio-visual sample, we compute the dot product between $\rho_{av}$ and each class knowledge representation, resulting in class logits $s^k = (\rho^k_{t})^\top \rho_{av}$. These logits serve as compatibility scores, indicating how well the audio-visual representation aligns with the corresponding knowledge representation for each class. To encourage high compatibility scores, we apply the cross-entropy loss, which penalizes discrepancies between predicted class probabilities and true labels. By optimizing this loss, we encourage the audio-visual representation to have a strong compatibility score with its associated knowledge representation, leading to improved classification performance:
\begin{equation}
\mathcal{L}_{cls} = -\frac{1}{N} \sum_{{a,v,k,y}\in S} {\rm log}\,\frac{e^{s^y}}{\sum\limits_{k\in C_t} e^{s^k}},
\end{equation}
where $C_t$ is the seen classes in $S$ and $N$ denotes the sample number of training set. 


\begin{table*}[t]
    \centering
    \caption{Experimental results of audio-visual zero-shot learning on three datasets (main feature). The mean class accuracy for GZSL is reported on the seen (S) and unseen (U) test classes, and their harmonic mean (HM). For the ZSL performance, only the test subset of unseen classes is considered.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccccccccccc}
    \toprule
       \multirow{2}{*}{Model} & \multirow{2}{*}{Venue} & \multicolumn{4}{c}{UCF-GZSL} & \multicolumn{4}{c}{VGGSound-GZSL} & \multicolumn{4}{c}{ActivityNet-GZSL}\\ \cline{3-14}&&S&U&HM&ZSL&S&U&HM&ZSL&S&U&HM&ZSL\\
       \midrule
       DEVISE~\cite{DEVISE}&NeurIPS'13&55.59&14.94&23.56&16.09&36.22 &1.07&2.08& 5.59&3.45&8.53&4.91&8.53\\
        ALE~\cite{ALE}&T-PAMI'15&57.59&14.89&23.66&16.32&0.28&5.48& 0.53& 5.48&2.63&7.87&3.94& 7.90\\
        SJE~\cite{SJE}&CVPR'20&63.10&16.77&26.50&18.93&48.33& 1.10 &2.15 &4.06&4.61& 7.04& 5.57& 7.08\\
         f-VAEGAN-D2~\cite{fVAEGAND2}&CVPR'19&17.29&8.47 & 11.37 &11.11&12.77 &0.95& 1.77 &1.91&4.36 &2.14& 2.87 &2.40\\
         CJME~\cite{CJME}&WACV'20&26.04& 8.21&12.48 & 8.29&8.69& 4.78& 6.17& 5.16& 5.55& 4.75& 5.12& 5.84\\
         AVGZSLNet~\cite{AVGZSLNet}&WACV'21&52.52& 10.90 &18.05& 13.65&18.05 &3.48& 5.83& 5.28&8.93& 5.04& 6.44& 5.40\\
         APN~\cite{APN}&IJCV'22&28.46 &16.16&20.61 &16.44&7.48& 3.88 &5.11 &4.49 &9.84 &5.76& 7.27& 6.34\\
         AVCA~\cite{AVCA}&CVPR'22&51.53&18.43& 27.15 & 20.01&14.90& 4.00& 6.31& 6.00 &24.86& 8.02& 12.13 &9.13\\
         TCaF~\cite{TCaF}&ECCV'22&58.60&21.74& 31.72& 24.81 &9.64& 5.91& 7.33& 6.06& 18.70& 7.50 &10.71& 7.91\\
         VIB-GZSL~\cite{VIB}&ICME'23&90.35&21.41&34.62& 22.49&18.42& 6.00& 9.05& 6.41& 22.12& 8.94& 12.73 &9.29\\
         ACFS~\cite{acfs}&IJCNN'23&54.87& 16.49& 25.36& 22.37&15.20& 5.13& 7.67& 6.20& 29.00& 9.13& 13.89& 11.18\\
         Hyper-multiple~\cite{HyperbolicAV}&ICCV'23&63.08& 19.10& 29.32& 22.24&15.02& 6.75& 9.32& 7.97& 23.38& 8.67 &12.65 &9.50\\
        \midrule
        KDA&Ours&83.98&27.21&\textbf{41.10}	&\textbf{28.05}&19.13&7.86&\textbf{10.45}&\textbf{8.43}&42.27&12.82&\textbf{19.67}&\textbf{14.00}	\\
        \bottomrule
    \end{tabular}}
    \label{table1}
\end{table*}

The alignment loss $\mathcal{L}{align}$ ensures that the feature representations $\rho{av}$ and $\rho_{t}$ belonging to the same class are as close as possible in the common feature space. However, it does not guarantee that $\rho_{av}$ and $\rho_{t}$ from different classes are sufficiently separated. To address this, we propose a novel knowledge-aware adaptive margin loss $\mathcal{L}_{kaml}$, which leverages the Gaussian distribution similarities between class knowledge representations to adjust the margin. Specifically, for each pair of classes $i$ and $j$, we utilize their corresponding knowledge representations $\rho_{t}^i$ and $\rho_{t}^j$ to compute the margin $m_{i,j}$ using the following formulation::
\begin{equation}
    m_{i,j} = \alpha \cdot (||\mu^{\rho^i_{t}}-\mu^{\rho^j_{t}}||^2_2+||{(\delta^{\rho^i_{t}})}^{\frac{1}{2}}-{(\delta^{\rho^j_{t}})}^{\frac{1}{2}}||^2_{\rm F})^{\frac{1}{2}} + \beta,
\end{equation}
where $\alpha$ and $\beta$ denotes the scale and bias parameters respectively. By introducing the knowledge-aware margin into the classification loss, we can obtain the $\mathcal{L}_{kaml}$:
\begin{equation}
    \mathcal{L}_{cls} = -\frac{1}{N} \sum_{{a,v,k,y}\in S} {\rm log}\,\frac{e^{s^y}}{e^{s^y}+\sum\limits_{k\in C_t \backslash \{y\} } e^{s^k+m}}.
\end{equation}
Indeed, our proposed knowledge-aware adaptive margin loss takes advantage of the knowledge similarity between classes to enhance the separability of samples from similar classes in the common embedding space. By adjusting the margin based on the similarities between class knowledge representations, we can create a more discriminative embedding space. This enhanced separability and discriminability of features contribute to improved recognition of unseen test classes, enabling the model to effectively generalize to new and unseen classes.



\noindent
\textbf{Inference.} 
During test time, we determine the class prediction $c$ by identifying the knowledge representation that is closest to the multi-modal representation $\rho_{av}$. This is achieved by measuring the distance or similarity between $\rho_{av}$ and each knowledge representation, and selecting the class whose knowledge representation has the smallest distance or highest similarity to $\rho_{av}$. The predicted class $c$ corresponds to the class with the most similar knowledge representation to $\rho_{av}$:
\begin{equation}
    c = \mathop{\arg\min}\limits_{i}(||\rho^j_{t}-\rho_{av}||_2).
\end{equation}

\subsection{Optimization}
Our full model optimizes the cross-attention module, encoders, projectors, embedding layer, and decoders simultaneously. This is achieved by minimizing the following objective function:
\begin{equation}
    \mathcal{L}_{\rm KDA} = \mathcal{L}_{kaml} + \lambda \cdot \mathcal{L}_{align},
\end{equation}
where $\lambda$ is the weight that control the importance of the alignment loss. It is worth noting that our model consistently achieves significant results across all datasets, demonstrating its robustness and ease of training. These consistent and impressive results validate the effectiveness and reliability of our proposed method.



\begin{table*}[t]
    \centering
    \caption{Experimental results of audio-visual zero-shot learning on three datasets (classification feature). The mean class accuracy for GZSL is reported on the seen (S) and unseen (U) test classes, and their harmonic mean (HM). For the ZSL performance, only the test subset of unseen classes is considered.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccccccccccc}
    \toprule
       \multirow{2}{*}{Model} & \multirow{2}{*}{Venue} & \multicolumn{4}{c}{UCF-GZSL} & \multicolumn{4}{c}{VGGSound-GZSL} & \multicolumn{4}{c}{ActivityNet-GZSL}\\ \cline{3-14}&&S&U&HM&ZSL&S&U&HM&ZSL&S&U&HM&ZSL\\
       \midrule
       DEVISE~\cite{DEVISE}&NeurIPS'13&29.58& 34.80& 31.98& 35.48 &29.96& 1.94& 3.64& 4.72& 0.17& 5.84& 0.33& 5.84\\
        SJE~\cite{SJE}&CVPR'20&19.39& 32.47& 24.28& 32.47& 16.94& 2.72& 4.69& 3.22 &37.92 &1.22& 2.35& 4.35\\
         CJME~\cite{CJME}&WACV'20&33.89 &24.82& 28.65& 29.01& 10.86 &2.22 &3.68& 3.72& 10.75& 5.55 &7.32&6.29\\
         AVGZSLNet~\cite{AVGZSLNet}&WACV'21&74.79 &24.15& 36.51& 31.51& 15.02 &3.19 &5.26& 4.81 &13.70& 5.96& 8.30& 6.39\\
         APN~\cite{APN}&IJCV'22&13.54& 28.44& 18.35& 29.69& 6.46& 6.13& 6.29& 6.50& 3.79& 3.39 &3.58 &3.97\\
         AVCA~\cite{AVCA}&CVPR'22&63.15& 30.72& 41.34& 37.72& 12.63& 6.19& 8.31& 6.91& 16.77& 7.04& 9.92& 7.58\\
         TCaF~\cite{TCaF}&ECCV'22&67.14& 40.83& 50.78& 44.64& 12.63 &6.72 &8.77 &7.41& 30.12& 7.65& 12.20 &7.96\\
         ACFS~\cite{acfs}&IJCNN'23&54.57&36.94&44.06&41.55& 12.87&5.22& 7.43& 6.03& 14.41& 8.91&11.01&9.15\\
         Hyper-multiple~\cite{HyperbolicAV}&ICCV'23&74.26& 35.79& 48.30& 52.11& 15.62& 6.00& 8.67& 7.31& 36.98& 9.60& 15.25& 10.39\\
        \midrule
        KDA&Ours&75.88& 42.97& \textbf{54.84} &\textbf{52.66} &13.30&7.74&\textbf{9.78}&\textbf{8.32}&37.55&10.25&\textbf{17.95}&\textbf{11.85}
        
        \\
        \bottomrule
    \end{tabular}}
    \label{table2}
\end{table*}



\section{Experiments}
In this section, we validate the effectiveness of KDA and analyze its components empirically. We first detail of our experimental settings, then present our experimental results and compare KDA with previous state-of-the-art models. Finally, we present an ablation study which shows the benefits of using our proposed methods.

\subsection{Experimental Setup}

\textbf{Datasets}. We perform extensive experiments to verify the effectiveness of our method on three audio-visual zero-shot learning datasets: VGGSound-GZSL~\cite{AVCA}, UCF-GZSL~\cite{AVCA}, and ActivityNet-GZSL~\cite{AVCA}. 
VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL are modified versions of existing audio-visual and action recognition datasets~\cite{Vggsound,UCF101,Activitynet}. VGGSound-GZSL consists of 42 seen and 234 unseen classes. UCF-GZSL includes 21 seen and 30 unseen classes. ActivityNet-GZSL is based on the action recognition dataset ActivityNet and includes 200 categories, with 99 seen categories and 101 unseen categories.
We perform two types of experiments depending on the network used to extract the features, i.e., main features and classification features.
Specifically, we use the self-supervised SeLaVi~\cite{Asano_Patrick_Rupprecht_Vedaldi_2020} to obtain main features, and use C3D~\cite{C3D} and VGGish~\cite{Vggish} to obtain classification features.
To distinguish between the two settings, we add superscript ${cls}$ to the data set names, e.g., UCF-GZSL$^{cls}$.

\noindent
\textbf{Training Details}.
We use the Adam optimizer~\cite{adam} to train all our models. The optimizer has running average coefficients $\beta_1=0.5$, $\beta_1=0.999$, and an initial learning rate of 0.001. We reduce the learning rate by a factor of 0.1 when the GZSL performance plateaus with a patience of 3 epochs. For all datasets, we set the batch size to 2048.
To avoid overfitting, we used dropout rates $r_{dec}/r_{enc}/r_{proj}$ of 0.5/0.2/0.3 for UCF-GZSL/UCF-GZSL$^{cls}$, 0.1/0.2/0.2 for Activity-GZSL/Activity-GZSL$^{cls}$, and 0.1/0/0 for VGGSound-GZSL/VGGSound-GZSL$^{cls}$.
All experiments are conducted on a single Tesla A100 GPU.

\noindent
\textbf{Evaluation}.
Following~\cite{AVCA}, we use the metrics S, U, ZSL and ${\rm HM = \frac{2US}{U+S}}$ to evaluate the performance of the model in the seen and unseen categories. Specifically, ZSL is obtained by considering only a subset of test samples obtained from the unseen test classes, and HM is the harmonic mean of averaged performance on the unseen and seen classes. All these metrics are evaluated on "main feature" and "cls feature".

\noindent
\textbf{Compared Methods.}
We compare our KDA to five ZSL and seven current state-of-the-art audio-visual ZSL frameworks. The ZSL approaches
include ALE~\cite{ALE}, SJE~\cite{SJE}, DEVISE~\cite{DEVISE}, APN\cite{APN}, and
f-VAEGAN-D2~\cite{fVAEGAND2}. The first four approaches are image-based ones, and f-VAEGAN-D2 is a generative method for
ZSL. For these approaches, we concatenate image and audio features as input instead of using only image features.
The compared audio-visual GZSL approaches are CJME~\cite{CJME},
AVGZSLNet~\cite{AVGZSLNet}, AVCA~\cite{AVCA} TCaF~\cite{TCaF}, VIB-GZSL~\cite{VIB}, ACFS~\cite{acfs} and Hyper-multiple~\cite{HyperbolicAV}.


\begin{figure*}[t]
	\centering\includegraphics[width=0.99\linewidth]{av-tsne.pdf}
	\caption{t-SNE visualisation for five seen and two unseen test classes from the UCF-GZSL dataset, showing audio and visual input embeddings extracted with SeLaVi~\cite{Asano_Patrick_Rupprecht_Vedaldi_2020}, and learned audio-visual embeddings in the common space. Knowledge embeddings are visualised with a square. KDA facilitates pulling together features from the same parent class while pushing away features belonging to different parent classes.}
	\label{tsne}
\end{figure*}

\subsection{Experimental Results}

\textbf{Comparison with state-of-the-art.}
To validate the effectiveness of our model, we have compared it with the current state-of-the-art audio-visual ZSL methods on
three benchmark datasets. The main results are presented in Table~\ref{table1} and Table~\ref{table2}. 

For the main feature setting, KDA achieves state-of-the-art performance in all cases.
For instance, on UCF-GZSL, KDA obtains a HM of 41.10\% and a ZSL performance of 28.05\% compared to 29.32\% HM and ZSL performance for the most recent ICCV'23 method Hyper-multiple~\cite{HyperbolicAV}.
On VGGSound-GZSL, KDA obtains a HM of 10.45\% for GZSL and a ZSL performance of 8.43\% compared to 7.33\% HM and 6.06\% ZSL for TCaF. On ActivityNet-GZSL, KDA outperforms AVCA, with an HM/ZSL performance of 19.67\%/14.00\% compared to 12.13\%/9.13\%.


And for the classification feature setting, KDA also achieves state-of-the-art performance in all datasets.
For UCF-GZSL$^{cls}$, our proposed KDA is significantly better than its strongest competitor TCaF, with a HM of 54.84\% compared to 50.78\% and a ZSL performance of 52.66\% compared to 44.64\%. Similar patterns are exhibited for the VGGSound-GZSL$^{cls}$ and ActivityNet-GZSL$^{cls}$ datasets.

The significant performance improvements observed in our study provide strong evidence for the effectiveness of more detailed knowledge descriptions, knowledge perception distribution adaptations in public Spaces where learning represents audiovisual and knowledge features. By leveraging shared information between patterns, our approach successfully adjusts the distribution of features from different patterns to better integrate and represent audiovisual and knowledge-based information.

\noindent
\textbf{Qualitative results.}
We present a qualitative analysis of the learnt audio-visual embeddings in Figure~\ref{tsne}. For this, we conduct t-SNE visualization~\cite{hinton_tsne} of audio-visual/knowledge embeddings mapped by our KDA on 7 classes from UCF-GZSL test set.
As shown in Figure~\ref{tsne}, audio and visual input features are poorly clustered, while audio-visual features are well clustered for seen and unseen classes with clear boundaries. This observation shows that the audio-visual features learned by our KDA improve over the clustering of input features for both, seen and unseen classes. 
In addition, knowledge representations lie inside the corresponding audio-visual clusters. This confirms that the learned audio-visual embeddings are mapped close to the corresponding knowledge representation, indicating that our audio-visual embeddings and knowledge embeddings have an excellent distribution alignment.



\begin{table*}[t]
    \centering
    \caption{Ablation study on KDA. The mean class accuracy for GZSL is reported on the seen (S) and unseen (U) test classes, and their harmonic mean (HM). For the ZSL performance, only the test subset of unseen classes is considered.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccccccccccc}
    \toprule
       \multirow{2}{*}{Model} &  \multicolumn{4}{c}{UCF-GZSL} & \multicolumn{4}{c}{VGGSound-GZSL} & \multicolumn{4}{c}{ActivityNet-GZSL}\\ \cline{2-13}&S&U&HM&ZSL&S&U&HM&ZSL&S&U&HM&ZSL\\
       \midrule
       Baseline&29.04&16.32&20.88 &16.72&
       7.35& 4.05 &5.22 &4.87 
       &12.09 &5.95& 7.97& 6.67\\
       Baseline+ $k$&85.14&22.54&35.64&22.88& 7.93&5.89 &6.76 &6.42 &37.02&11.98&18.10&13.58\\
       Baseline+ $k$+$\mathcal{L}_{align}$&80.14&26.66&40.01&27.24&11.33&7.20&8.79&7.77&36.99&12.79&19.01&13.50\\
       Baseline+ $k$ +$\mathcal{L}_{kdam}$&90.46&23.78&37.66&	25.13&9.47&6.57&7.76&7.13&36.81&12.55&18.72&13.74\\
        \midrule
        KDA&83.98&27.21&\textbf{41.10}&\textbf{28.05}&13.30& 7.74& \textbf{9.78} &\textbf{8.32}&42.27&12.82&\textbf{19.67}&\textbf{14.00}	\\
        \bottomrule
    \end{tabular}}
    \label{ablation}
\end{table*}


\subsection{Ablation Study}
Here, we analyse different components of our proposed KDA. We first compare the performance of the model when trained using different loss functions and different text representations. We then investigated the impact of $\lambda$ used in $\mathcal{L}_{\rm KDA}$ on (G)ZSL performance. Finally, we study the effect of different text encoder on performance.

\noindent
\textbf{Ablation study on key components.}
In our framework, LLMs generated descriptions and adaptation losses are core components. 
To investigate the effectiveness of each component, we perform ablation experiments to reveal how the combination of three modules improves the overall performance of KDA, especially on unseen classes.
Specifically, we perform experiments by dding them one by one and observing changes in overall performance.
The baseline method means that none of these three components are used.
Our results shown in Table~\ref{ablation}.
We discovered that incorporating generated description by LLMs alternatives instead of action names led to a 70.7\%/29.5\%/127.1\% improvement in HM for UCF-GZSL/VGGSound-GZSL/ActivityNet-GZSL, respectively. These findings provide evidence that utilizing detailed semantic descriptions enables the model to better comprehend actions and establish mappings between audio-visual and text features.
The addition of $\mathcal{L}_{align}$ and $\mathcal{L}_{kdam}$ further enhances the performance of the model. 
Specifically, $\mathcal{L}_{kdam}$increases the HM of the model on the UCF-GZSL/VGGSound-GZSL/ActivityNet-GZSL dataset by 5.7\%/14.8\%/3.4\%.
These results demonstrate that incorporating knowledge-aware distribution alignment enhances the generalizability of model on unseen classes.


\noindent
\textbf{Influence of different rate $\lambda$.}
We conducted experiments to analyze the 
diffrent rate $\lambda$ to banlance inter class and intra class distribution learning.
The experimental results are presented in Table~\ref{lamda}. 
From the table, it can be observed that when $\lambda=10$, the model achieves the highest performance in UCF-GZSL, while when $\lambda=1$, the model achieves the highest performance in ActivityNet-GZSL.
This implies that at this specific value of $\lambda$, there is a well-balanced trade-off between intra-class and inter-class distribution learning. This finding suggests that selecting an appropriate value of $\lambda$ is crucial for achieving optimal performance in our model.



\begin{table}[t]
    \centering
    \caption{Ablation study: Influence of different $\lambda$.}
    \label{lamda}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccccccc}
    \toprule
       \multirow{2}{*}{Model} & \multicolumn{3}{c}{UCF-GZSL} & \multicolumn{3}{c}{ActivityNet-GZSL} \\ \cline{2-7}&U&HM&ZSL&U&HM&ZSL\\
       \midrule
       0.1&25.97&39.52&26.32&13.23&18.70&13.69\\
       1& 27.15&40.04& 27.38&12.82& \textbf{19.67}& \textbf{14.00}\\
       5&26.93&40.61&27.36&12.00&18.32&13.21\\
       10&27.21&\textbf{41.10}&\textbf{28.05}&12.50& 18.16& 13.57\\
       20&25.42&38.89&26.07&11.68	&17.51&12.88\\
       100&24.53&37.14&25.83&12.09& 16.73& 12.94\\
        \bottomrule
    \end{tabular}}
\end{table}

\begin{table}[t]
    \centering
    \caption{Ablation study: Influence of different text encoder.}
    \label{text}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccccccc}
    \toprule
       \multirow{2}{*}{Model} & \multicolumn{3}{c}{UCF-GZSL} & \multicolumn{3}{c}{ActivityNet-GZSL} \\ \cline{2-7}&U&HM&ZSL&U&HM&ZSL\\
       \midrule
       BERT &13.62 &7.06 &14.21&6.75&4.87&5.05\\
       GPT&22.32&32.85&23.14&9.55&11.22&10.65\\
       Instructor&26.57&38.55&27.14&12.55&17.94&12.88\\
       CLAP&26.55&39.12&27.65&12.33&16   
                                         
                          .75&12.32\\
       CLIP&27.21& \textbf{41.10} &\textbf{28.05}&12.82&\textbf{19.67}&\textbf{14.00}\\
        \bottomrule
    \end{tabular}}
\end{table}

\noindent
\textbf{Influence of text encoder.}
In our study, we consider several text encoders for our Knowledge-aware Discriminative Alignment (KDA) framework. These text encoders include CLIP~\cite{clip}, GPT-3~\cite{yang2022empirical}, Instructor~\cite{instructor}, and CLAP~\cite{clap}. The performance of these encoders on the UCF-GZSL and ActivityNet-GZSL datasets is presented in Table~\ref{text}. From the table, we observe that CLIP achieves the best performance. This can be attributed to the inherent capability of CLIP to align images and text, which proves advantageous for our audio-visual zero-shot learning task. The alignment between images and text representations in CLIP enables better cross-modal understanding and alignment, leading to improved performance in recognizing unseen action categories.


\noindent
\textbf{Evaluating different modalities.}
In Table~\ref{modality}, we compared our multi-modal KDA model with training our architecture using only unimodal inputs. In this case, we excluded the cross-modal attention block and trained each unimodal branch separately. The visual branch outperformed the audio branch, achieving a GZSL performance (HM) of 16.96\% compared to 2.93\% on the ActivityNet-GZSL dataset. A similar trend was observed for the ZSL performance, with the visual branch achieving 38.77\% compared to 20.24\% for the audio branch. This pattern was also seen on the UCF-GZSL and VGGSound-GZSL datasets, suggesting that visual input features contain more comprehensive information about video content than audio inputs. This supports the notion that incorporating complementary information from both audio and visual inputs is highly advantageous for GZSL and ZSL in video classification.


\subsection{Limitations}
We propose an audio-visual zero-shot learning framework enabled by large language models. KDA achieves state-of-the-art performance on all data sets. However, KDA uses time-averaged audio-visual input information and therefore does not consider fine semantic details. In addition, KDA uses the same description for all videos of the same class, which can be biased.

\begin{table}[t]
    \centering
    \caption{Ablation study: Influence of training KDA with different modalities.}
    \label{modality}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccccccc}
    \toprule
       \multirow{2}{*}{Model} & \multicolumn{3}{c}{UCF-GZSL} & \multicolumn{3}{c}{ActivityNet-GZSL} \\ \cline{2-7}&U&HM&ZSL&U&HM&ZSL\\
       \midrule
       Visual only &26.55&38.77&26.80&11.39&16.96&12.73	\\
       Audio only&12.62&20.24&12.93&4.53&2.93&3.61\\
       KDA&27.21&\textbf{41.10}&\textbf{28.05}&12.82 &\textbf{19.67}& \textbf{14.00}\\
        \bottomrule
    \end{tabular}}
\end{table}

\section{Conclusion}
In this work, we develop a audio-visual zero-shot learning framework with large languange models, i.e., knowledge-aware distribution adaptation (KDA), to learn an intrinsic common space for audio-visual and semantic feature representations. By introducing extra knowledge generated by ChatGPT and conducting distribution alignment, our model effectively tackles the problem of heterogeneous feature representations and bridges the gap between the audio-visual and semantic domains.
We demonstrate that KDA achieves consistent improvement over the current state-of-the-art methods on three action recognition benchmarks. 



{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}



\end{document}
