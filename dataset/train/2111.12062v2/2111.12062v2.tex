\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[final]{neurips_data_2021}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{bbm}
\usepackage{mwe}

\newif\ifcomments
\commentsfalse
\ifcomments
\newcommand{\art}[1]{\textbf{\textcolor{blue}{ART: #1}}}
\newcommand{\ndg}[1]{\textbf{\textcolor{magenta}{NDG: #1}}}
\else
\newcommand{\art}[1]{}
\newcommand{\ndg}[1]{}
\fi

    \makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
    \makeatother


\title{DABS: A Domain-Agnostic Benchmark for Self-Supervised Learning}



\author{Alex Tamkin\thanks{\url{atamkin@stanford.edu}} \\ Stanford University
   \And
   Vincent Liu \\ Stanford University \\
   \And
   Rongfei Lu \\ Stanford University \\
   \AND
   Daniel Fein \\ Stanford University \\
   \And
   Colin Schultz \\ Stanford University \\
   \And
   Noah Goodman \\ Stanford University \\
}

\begin{document}

\maketitle



\begin{abstract}
Self-supervised learning algorithms, including BERT and SimCLR, have enabled significant strides in fields like natural language processing, computer vision, and speech processing. However, these algorithms are domain-specific, meaning that new self-supervised learning algorithms must be developed for each new setting, including myriad healthcare, scientific, and multimodal domains. To catalyze progress toward domain-agnostic methods, we introduce DABS: a \textbf{D}omain-\textbf{A}gnostic \textbf{B}enchmark for \textbf{S}elf-supervised learning. To perform well on DABS, an algorithm is evaluated on seven diverse domains: natural images, multichannel sensor data, English text, speech recordings, multilingual text, chest x-rays, and images with text descriptions. Each domain contains an unlabeled dataset for pretraining; the model is then scored based on its downstream performance on a set of labeled tasks in the domain. We also present e-Mix and ShED: the first domain-agnostic algorithms evaluated on such a wide range of modalities. While e-Mix and ShED outperform a no-pretraining baseline, these improvements are uneven, demonstrating that significant progress is needed before self-supervised learning is an out-of-the-box solution for arbitrary domains. Code for the benchmark datasets and algorithms is available at \url{https://github.com/alextamkin/dabs}.
\end{abstract}

\begin{figure}[h!]
    \centering
    \includegraphics[height=140pt]{img/dabs.png}
    \caption{\textbf{The DABS Benchmark.} A domain-agnostic self-supervised algorithm consists of 1) a model architecture, 2) an objective used to pretrain the model on unlabeled data, and 3) a transfer method used to deploy it on a downstream task (bolded items). A successful algorithm will achieve high performance on downstream tasks \textbf{while holding these components constant across domains}.}
    \label{fig:dabs}
\end{figure}

\section{Introduction and Motivation}
\label{sec:intro}

Self-supervised learning (SSL) is on the rise across machine learning (ML), with notable recent successes in computer vision \citep{Chen2020ASF, He2020MomentumCF, Grill2020BootstrapYO}, natural language processing (NLP), \citep{Devlin2019BERTPO, Yang2019XLNetGA, Clark2020ELECTRAPT} and speech processing \citep{Oord2018RepresentationLW, Baevski2020wav2vec2A}. SSL enables a model to acquire useful capabilities from unlabeled data; these capabilities can then be leveraged to drastically reduce the amount of labeled data needed to achieve high performance in a domain---a crucial advance given the time and expense needed to annotate datasets of millions of labels.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{img/tail.pdf}
    \caption{\textbf{Domain-agnostic SSL could reduce the need for labeled data across a long tail of domains and application areas.} Currently, developing an SSL algorithm requires considerable domain-specific trial-and-error, limiting it to domains with the most active ML communities. Advances in domain-agnostic methods could make SSL available to all domains, as well as provide scientific insights into principles underlying the success of SSL across modalities. (Figure is illustrative, not based on real data.)}
    \label{fig:longtail}
\end{figure}

However, the potential impact of SSL is arguably greatest outside of the fields where it has currently seen the most success. Medical and scientific domains, for example, are rich in unlabeled data, yet the time and expertise needed for annotation far exceeds that for computer vision or NLP. This means that methods which reduce the need for labeled data are especially impactful in these settings. 

Unfortunately, the most popular SSL methods are currently domain-specific---for example, the color jitter distortions used in SimCLR \citep{Chen2020ASF} are inapplicable to black-and-white chest x-rays, and the masked language modeling task used in BERT \citep{Devlin2019BERTPO} is not directly applicable to spoken language, which is untokenized. Furthermore, these algorithms are challenging to develop, requiring costly trial-and-error by ML experts \citep{Chen2020ASF}. Unfortunately, while a great number of domains may benefit from SSL, this distribution exhibits a long tail where the vast majority of domains lack the ML expertise and resources to develop custom SSL solutions.

We argue that an appealing alternative to developing domain-specific SSL methods is to develop domain-agnostic techniques which work across a wide range of settings without extensive modification. Such domain-agnostic SSL algorithms could benefit the field in multiple ways:

\begin{enumerate}
    \item \textbf{Making SSL work out-of-the box.} The most important impact of domain-agnostic SSL would be turning SSL into an out-of-the-box technology capable of being used in any domain of interest without significant ML expertise  (Figure \ref{fig:longtail}, right). Aside from medical and scientific domains, this would also benefit the combinatorial number of multimodal settings which currently require novel algorithms to learn the relationships between modalities \citep{Lu2019ViLBERTPT, Chen2020UNITERUI, Tan2019LXMERTLC}. 
    \item \textbf{Improving handcrafted SSL methods.} Several works have investigated how more general SSL methods can be combined with domain-specific knowledge (e.g. image augmentations) to provide gains  \citep{Lee2021iMIXAD, Tamkin2020ViewmakerNL, Verma2020TowardsDC}. This suggests that advances in domain-agnostic SSL could benefit popular ML domains as well  (Figure \ref{fig:longtail}, left), through combination with domain-specific methods.
    \item \textbf{Uncovering fundamental principles of SSL across domains.} Communities such as computer vision and NLP currently have relatively disjoint investigations into SSL methods; this may obscure common scientific principles underlying the success of algorithms across modalities. Research on domain-agnostic methods may discover these general principles, which could benefit all domains.
\end{enumerate} 


However, despite the promise of domain-agnostic SSL, there has been no standardized way to evaluate or drive progress in a cross-cutting way among different communities. To fill this need, we propose DABS, a \textbf{D}omain \textbf{A}gnostic \textbf{B}enchmark for \textbf{S}elf-supervised learning. DABS measures how well a single SSL algorithm works on many different domains, as opposed to just one. The benchmark is composed of seven domains representing different kinds of data:  natural images, English text, speech, chest x-rays, multichannel sensor data, multilingual text, and images with text descriptions. Each domain contains one unlabeled dataset for self-supervised learning, and at least one labeled dataset to assess \emph{transfer}: how well the SSL model can adapt its abilities to downstream tasks. Models are assessed by their average transfer learning performance on downstream tasks across domains. 

We anticipate a few common questions about DABS:


\paragraph{Why do we need a benchmark for domain-agnostic SSL?}

Benchmarks catalyze progress by providing a common set of tasks, rules, and evaluation criteria for research towards a particular goal.  In this case, DABS provides a standardized way to evaluate the performance of domain-agnostic methods. Fixing the choice and preprocessing of datasets allows for clean comparisons over a range of diverse domains, enabling researchers to pinpoint what specific changes contribute to the success of different methods. Furthermore, the provided infrastructure for data processing, training, transfer, and evaluation significantly reduces the barrier to entry for other researchers interested in these questions. Without a low-friction way to evaluate algorithms in a standard way, many researchers may not bother with the significant effort needed to gather and process 25+ different datasets across distinct domains, impeding cumulative progress as a field towards domain-agnostic SSL.

\paragraph{Why might we expect there to be a good domain-agnostic method?}

Many kinds of naturally-occurring and artificial data exhibit structure which humans can exploit to learn transferable skills  \citep{Carey2004OnTO, spelke2007core, Bonawitz2011TheDS, Dubey2018InvestigatingHP}. Human-relevant data (as opposed to white noise) is often generated by some complex generative process. For example, the PAMAP2 wearable sensor dataset \citep{Reiss2012IntroducingAN} is produced by a cascade of latent factors including human interpretation of an activity command, the bodily mechanics of the activity's execution, and the physical properties of different kinds of sensors that produce measurements. Domain-agnostic pretraining objectives may enable models to capture these latent factors if they are useful for compressing the data (e.g. via density estimation objectives like language modeling \citep{Shin2006AMT}) or distinguishing examples from one another (e.g. via contrastive learning objectives \citep{Hadsell2006DimensionalityRB}).  Furthermore, studies on transfer learning of deep networks suggests there exist useful and general "subroutines" learned by SSL models which enable the model to transfer well to new datasets \citep{Yosinski2014HowTA, Tamkin2020InvestigatingTI}.  Empirically, the recent progress of existing domain-agnostic methods \citep{Tamkin2020ViewmakerNL, Lee2021iMIXAD, Verma2020TowardsDC} is cause for optimism about the future success of this research direction.

\paragraph{What does domain-agnostic mean?}
The goal of DABS is to catalyze the creation of SSL algorithms which are useful out-of-the-box across different domains. We operationalize this goal by evaluating algorithms on a suite of seven diverse domains crossing many different fields where machine learning is used. We also propose several constraints on submissions, described in Section \ref{sec:rules}, to prevent ``overfitting'' to these domains. For example, algorithms must use a set of provided dataloaders and keep their architecture and pretraining objective constant across domains (Section \ref{sec:rules}). However, we also rely on a degree of pragmatism and collaborative ethos from users of DABS to abide by the spirit of the benchmark; for example, a ``domain agnostic'' algorithm that uses an if-statement to select domain-specific methods for each domain would likely not generalize to new domains. To this end, we will add new domains in the future as an ultimate test of the generalizability of proposed algorithms.



To summarize, our \textbf{contributions} are:
\begin{enumerate}
    \item We propose and motivate the task of domain-agnostic self-supervised learning.
    \item We present a benchmark for measuring domain-agnostic self-supervised learning, including standardized data loaders and rules for ensuring fair comparisons across submissions
    \item We present two domain-agnostic baseline algorithms and evaluate them on our benchmark, showing relatively modest improvements over baselines that were not pretrained. This suggests ample room for future methods to drive progress.
\end{enumerate}




\section{Related Work}
\paragraph{Single-domain transfer learning benchmarks}
Several works have created benchmarks from multiple datasets in a single domain, often with the aim of measuring the general understanding capabilities of a single model by measuring its performance across those tasks.  Such datasets have been developed in  natural language processing \citep{Wang2018GLUEAM, Wang2019SuperGLUEAS}, computer vision \citep{triantafillou2019meta, Zhai2019ALS, Zamir2018TaskonomyDT}, speech processing \citep{Shor2020TowardsLA, Yang2021SUPERBSP}, molecular machine learning \citep{Wu2017MoleculeNetAB}, robotics \citep{Yu2019MetaWorldAB}, graphs \citep{Hu2020OpenGB}, and reinforcement learning \citep{bellemare2013arcade}, among others.

While these datasets often focus on how a single model can adapt to multiple downstream tasks in a domain, they are typically agnostic to the specifics of the pretraining process---encouraging a ``no holds barred'' setting where larger models, datasets, and domain-specific assumptions are all utilized to increase downstream accuracy. By contrast, our goal here is to develop general techniques that can be used out-of-the-box for acquiring transferrable capabilities from unlabeled data in any domain. Thus, we hold the pretraining data fixed, allowing researchers to improve only the (domain-agnostic) pretraining algorithm, model architecture, and transfer procedure. 


\paragraph{Modality-agnostic architectures}
In order for an SSL method to be usable out-of-the-box, the model architecture must be applicable in new domains without much customization. Transformers \citep{Vaswani2017AttentionIA}, originally developed for text, have recently shown promise as a more general architecture for SSL through successful extensions to computer vision \citep{Dosovitskiy2020AnII}, molecular data \citep{Schwaller2019MolecularTA, rothchild2021c5t5}, speech processing \citep{Gong2021ASTAS},  and multimodal data \citep{Lu2019ViLBERTPT, Tan2019LXMERTLC, Chen2020UNITERUI}. These approaches typically use locality assumptions about continuous data (e.g. breaking the input into patches) to map the data into a sequence of embeddings, which are then processed by the transformer. Our baseline algorithms, e-Mix and ShED, leverage similar ideas to train transformer models across all seven of our domains, however we expect and encourage future work to explore other flexible architectures, such as Perceiver \citep{Jaegle2021PerceiverGP} which relaxes these locality assumptions at the cost of increased computational demands.

\paragraph{Domain-agnostic self-supervised algorithms}
Several streams of work have recently developed more general SSL methods. Recent work in contrastive learning has sought to reduce the reliance of the objective on domain-specific augmentation functions. The most common approach seeks to find heuristic augmentations which are applicable across a wider range of domains \citep{Lee2021iMIXAD, Verma2020TowardsDC, You2020GraphCL}, while other work seeks to develop generative models which learn data-dependent distortions during training with a suitable objective \citep{Tamkin2020ViewmakerNL}. Outside of contrastive learning, masked language modeling \citep{Devlin2019BERTPO} or replaced token detection \citep{Clark2020ELECTRAPT}, have been applied to other kinds of tokenized or discrete data \citep{Iida2021TABBIEPR, Hu2020StrategiesFP, Choromanski2020MaskedLM}, but require modification when applied to continuous domains \citep{Dosovitskiy2020AnII, Liu2020MockingjayUS}. However, none of these algorithms are applicable out-of-the-box on the DABS datasets, so in this work we propose simple domain-agnostic extensions of algorithms in both of these families. 

\paragraph{Transfer learning methods} Evaluating a self-supervised learning algorithm requires a method to transfer model's abilities to other tasks of interest \citep{AbuMostafa1990LearningFH, Caruana1994LearningMR, Bengio2012DeepLO}. These strategies are typically quite domain-agnostic, but involve tradeoffs between various properties, including complexity, downstream performance, and the degree to which they modify the original model. The two most common transfer strategies have historically been training simple linear classifiers on activations extracted from these pretrained models \citep{Donahue2014DeCAFAD, McCann2017LearnedIT, Peters2018DeepCW}, or finetuning, where one can often achieve higher performance by specializing the entire model to the downstream task via end-to-end training \citep{Sermanet2013PedestrianDW, Girshick2014RichFH, Radford2018ImprovingLU, Devlin2019BERTPO}. However, recently, other transfer methods have shown initial success in capturing the benefits of both these extremes, including directly specifying the task in natural language \citep{Brown2020LanguageMA}, as well as approaches that train only a small subset of parameters in the original model \citep{Frankle2020TrainingBA, BenZaken2020BitFitSP} or that inject trainable features into the input \citep{Elsayed2019AdversarialRO,Liu2021GPTUT, Lester2021ThePO} or hidden states \citep{Li2021PrefixTuningOC} of the model.
We permit and encourage users of DABS to investigate different domain-agnostic transfer methods in order to understand their tradeoffs and performance across different domains.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{img/datasets.png}
    \caption{\textbf{Different domains and datasets used in this work.} Examples from pretraining datasets of different domains (clockwise from top-left): CIFAR-10, LibriSpeech, MS COCO, PAMAP2, CheXpert, and WikiText-103.  For sensor data, each line is a reading from a different sensor.}
    \label{fig:domains}
\end{figure}


\section{Evaluating Domain-Agnostic SSL Algorithms with DABS}
\label{sec:rules}

How should we evaluate a domain-agnostic SSL method? In DABS, the ultimate goal is to produce a general out-of-the-box solution for SSL across domains---one that generalizes without much modification to arbitrary desired applications. However, one challenge is that SSL methods are comprised of many factors, including the data, pretraining objective, model architecture, and transfer method. Here we describe how the rules of DABS ensure fair comparisons across each of these: 

\paragraph{Datasets} DABS consists of multiple datasets spread across seven domains (detailed in Section \ref{sec:domains}). Each domain contains an unlabeled dataset used for pretraining, and one or more labeled datasets to evaluate transfer. These transfer datasets include both labeled subsets of the pretraining dataset as well as different labeled datasets in the same domain. To establish fair comparisons across algorithms, we standardize the data loading process, ensuring the same train/test splits, resolutions, tokenizers, and other details. As our primary aim is to measure the performance of methods in a domain-agnostic setting, as opposed to competing with domain-specific methods, we also prohibit the use of data augmentations which vary between domains (e.g. cropping-and-resizing used in natural images). While this may result in lower performance on transfer metrics relative to domain-specific approaches, past work suggests that many domain-agnostic methods can be combined with domain-specific techniques to provide gains \citep{Tamkin2020ViewmakerNL, Lee2021iMIXAD, Verma2020TowardsDC}. 

\paragraph{Pretraining method} The goal of an SSL objective is to enable a model to acquire general capabilities from unlabeled data. To evaluate this, a single pretraining method is used to train a model on each pretraining dataset (Figure \ref{fig:dabs}). Crucially, this method may not be changed by hand between modalities (e.g. adding auxiliary losses for text). However, we do allow adaptive methods that alter the pretraining task in a general way based on the model's interaction with the unlabeled data, e.g. by learning a generative model to produce input-dependent distortions \citep{Tamkin2020ViewmakerNL}.

\paragraph{Model architecture} While the main model architecture should be kept constant, a key challenge is that different datasets have different input types and dimensions. Some recent works, such as \citet{Jaegle2021PerceiverGP}, attempt to build more data-agnostic model architectures capable of handling different kinds of inputs, however this comes at a compute cost. We take a more permissive stance, allowing different types of data to have specialized embedding modules that convert an example from the dataset into a series of vectors. These vectors then serve as the input to a model which is otherwise held constant across the datasets. 

We provide a starter set of embedding modules compatible with sequence modeling architectures such as transformers \citep{Vaswani2017AttentionIA}. However, we encourage development of other general input modules as long as their tradeoffs are made clear when comparing against other methods---including running ablation studies to isolate the effects of changing the embedding module from the effects of proposed training strategies. We also emphasize that these embedding modules should encode as few assumptions about the data as possible: the goal is to produce a general, out-of-the-box strategy for SSL. For continuous 1D and 2D inputs, we use patch embeddings \citep{Dosovitskiy2020AnII} with standardized patch sizes (see Table \ref{table:datasets}), and for text we use a standard token embedding lookup table, where the text is tokenized using a standard tokenizer.\footnote{We use the common HuggingFace \href{https://huggingface.co/transformers/model_doc/bert.html}{BertTokenizer} and \href{https://huggingface.co/transformers/model_doc/xlmroberta.html}{XLMRobertaTokenizer} for English and multilingual text, respectively.} For multimodal data, we apply the appropriate embedding modules to each input, then concatenate the resulting embedding sequences in the same order each time. These embedding modules are the only parts of the model which differ across domains, enabling the main architecture to operate identically on each embedding sequence.


\paragraph{Transfer method} The ultimate measure of an SSL model is how well it performs when its capabilities are adapted to new tasks. Crucially, transfer methods are distinct from pretraining objectives, and must be compared in their own right as first-class components of an SSL method. Like pretraining techniques, transfer methods exhibit tradeoffs beyond task performance: for example, finetuning a model may produce high accuracy, but requires a separate copy of the model for each use case. Other methods, such as linear evaluation \citep{Zhang2016ColorfulIC}, in-context learning \citep{Brown2020LanguageMA}, and p-, prefix-, and prompt tuning \citep{Liu2021GPTUT, Lester2021ThePO, Li2021PrefixTuningOC} enable the same model to be reused across tasks, but may achieve worse performance in some settings. We allow any transfer method, including adaptive techniques such as active learning \citep{tamkin2022active}, as long as it is held constant across domains and downstream tasks.\footnote{Note that a transfer method does not presuppose a particular loss function, which may in general vary across tasks. For example, one can finetune a model for both regression and classification tasks.}
\paragraph{Final evaluation metric} There are many metrics one might use to compare SSL algorithms, including downstream accuracy, speed, fairness, and cost \citep{Ethayarajh2020UtilityII}. In this work, we focus on absolute performance of the model on the given data, for a given number of pretraining steps. However, participants may also be interested in other factors, including compute or data efficiency, or the scaling coefficient of techniques \citep{Kaplan2020ScalingLF, Henighan2020ScalingLF}. We encourage users of the benchmark to consider any of these, as long as they make clear what previous work is comparable and perform ablations to identify which specific changes impacted the metric being measured.





\section{Domains and Datasets}
\label{sec:domains}

Here, we describe the domains and datasets that comprise DABS. Domains were chosen to span a mix of impactful areas, including domains with both large ML research communities (natural images, text, speech) as well as domains where methods are more nascent (medical imaging, sensor recordings, vision-and-language). Dataloading and preprocessing within each dataset has been standardized to ensure fair comparisons; more information about data processing for each modality may be found in the Appendix.

\paragraph{Natural images} Two-dimensional color images of the natural world is a deeply-studied domain in machine learning. For an unlabeled pretraining dataset, we use ImageNet \citep{Russakovsky2015ImageNetLS}, a pervasive image classification benchmark in machine learning consisting of 1.3M images from 1000 classes, including meerkat, streetcar, and chocolate sauce. We measure the average transfer accuracy on several image recognition tasks commonly used to assess transfer of pretrained vision models \citep{Chen2020ASF, Grill2020BootstrapYO}: 
the FGVC-Aircraft dataset \citep{maji13fine-grained},
the Caltech-UCSD Birds Dataset \citep{WelinderEtal2010},
the German Traffic Sign Recognition Benchmark \citep{Houben-IJCNN-2013},
the Describable Textures Dataset \citep[][DTD]{cimpoi14describing}, the VGG Flower Dataset \citep{Nilsback2008AutomatedFC}, and the CIFAR-10 dataset \citep{Krizhevsky2009LearningML}.

\paragraph{Speech recordings} Speech processing is another large community with significant ML presence. We pretrain using the LibriSpeech corpus \citep{Panayotov2015LibrispeechAA}, a large English-language audiobook corpus commonly used for pretraining. We evaluate transfer to several datasets, including the VoxCeleb \citep{Nagrani17} and LibriSpeech \citep{Panayotov2015LibrispeechAA} speaker recognition datasets, and the Fluent Speech Commands (action, object, and location classification) \citep[]{Lugosch2019SpeechMP}, Google Speech Commands \citep{speechcommandsv2}, and AudioMNIST \citep{becker2018interpreting} utterance classification tasks. To prepare inputs for models, we preprocess examples into log-mel spectrograms---a format which differs significantly from natural image data, and thus may pose challenges for natural image-specific SSL approaches.

\paragraph{Monolingual English Text} The discrete, tokenized nature of text data makes it very different in form from the two previous continuous domains. Historically, monolingual English text has been a dominant focus shaping the development of self-supervised pretraining in NLP \citep{Dai2015SemisupervisedSL, Howard2018UniversalLM, Peters2018DeepCW, Radford2018ImprovingLU, Devlin2019BERTPO, Brown2020LanguageMA}. To assess whether domain-agnostic approaches can match the performance of methods tailored to this well-studied domain, we consider pretraining on WikiText-103 \citep{Merity2017PointerSM}, a 100-million token English-language dataset collected from the set of \textit{Good} and \textit{Featured} Wikipedia articles. For transfer, we evaluate on the GLUE benchmark \citep{Wang2018GLUEAM}, a suite of English language tasks including natural language inference, sentiment classification, and paraphrase classification, commonly used to measure transfer of pretrained models.\footnote{The GLUE benchmark tasks are CoLA \citep{warstadt2018neural}, SST-2 \citep{Socher2013RecursiveDM}, MRPC \citep{Dolan2005AutomaticallyCA}, QQP \citep{WinNT}, STS-B \citep{cer2017semeval}, MNLI \citep{N18-1101, bowman2015large}, QNLI \citep{rajpurkar2016squad, Wang2018GLUEAM}, RTE \citep{dagan2005pascal, bar2006second, giampiccolo2007third, bentivogli2009fifth}, and WNLI \citep{levesque2012winograd}.}

\paragraph{Multilingual Text} Unfortunately, machine learning learning approaches designed with only an individual language in mind are unlikely to perform equally well across the broader range of human languages \citep{Bender2009LinguisticallyN, Bender2011OnAA, bender2019rule, ruder2020beyondenglish, Wu2020AreAL, wu2022oolong}. To assess the generality of pretraining approaches on multilingual, typologically diverse data, we consider the mC4 dataset \citep{2019t5}, a filtered multilingual  web crawl corpus.\footnote{However, we note that multilingual data is not sufficient to ensure inclusion of multicultural perspectives, and we encourage future work conducting deeper analysis and documentation of mC4, similar to \citet{Dodge2021DocumentingTE}.} For pretraining, we interleave the mC4 subsets for English, Spanish, French, German, Chinese, Korean, and Japanese, meaning that the fraction of examples seen for each language during pretraining is constant (though the kind and number of unique tokens for each dataset may differ---reflecting the heterogeneity of data availability across languages). For transfer, we evaluate on the PAWS-X tasks \citep{pawsx2019emnlp}, a set of seven adversarial paraphrase identification datasets in English, Spanish, French, German, Chinese, Korean, and Japanese.



\paragraph{Medical imaging} Medical image understanding encompasses a rich set of domains which often possess ample unlabeled data yet limited labeled data, making them ideal targets for SSL. However, the statistics of medical images can differ significantly from natural images, including lower variation across many inputs and subtler task-relevant features that indicate presence of a pathology \citep{Raghu2019TransfusionUT}. Medical imaging boasts less of an ML presence than natural images, despite the fact that many techniques developed for the former may not apply---e.g. color transformations for black-and-white scans. We focus on chest x-rays as a representative medical imaging domain. We pretrain on the large CheXpert \citep{Irvin2019CheXpertAL} dataset of chest x-rays, and assess how well the pretrained model adapts to binary multiclass classification of five observations: atelectasis, cardiomegaly, consolidation, edema, and pleural effusion.\footnote{These are known as the CheXpert ``competition tasks'' \citep{Irvin2019CheXpertAL}.} To assess transfer to other chest x-ray datasets, we also measure multiclass binary classification of eight observations in the ChestX-ray8 \citep{Wang2017ChestXRay8HC} dataset: atelectasis, cardiomegaly, effusion, infiltration, mass, nodule, pneumonia, pneumothorax.

\paragraph{Multi-channel sensor data} Many scientific applications are data-rich and show significant promise for SSL. However, many such domains have a very scarce ML presence compared to domains like natural images. As an example, we consider multi-channel sensor data from wearable devices. We use the PAMAP2 dataset \citep{Reiss2012IntroducingAN}, consisting of 52-channel sensor recordings (including accelerometer, gyroscope, and magnetometer data) recorded from different body parts as participants perform varied physical activities. We measure transfer to the labeled PAMAP2 task of human-activity recognition: classifying the activity captured in a given recording snippet (e.g. cycling or walking). Examples are arranged into 1-dimensional time series of 52-channel measurements. Thus, this modality contributes to the benchmark's coverage both in terms of shape and content.

\paragraph{Images with text descriptions} Multimodal models are an increasingly important area in machine learning. As an example domain with two modalities, we consider natural images with paired English-language text descriptions. We pretrain on image-description pairs from the COCO dataset \citep{Lin2014MicrosoftCC}. We then assess the model's ability to adapt to 1) detecting whether an MS COCO image-text pair is matching or mismatched, and 2) the Visual Question Answering task \citep{Agrawal2015VQAVQ}, reformulated as a binary task to predict whether a question-answer pair correctly describes an image.

\section{Domain-Agnostic Baseline Algorithms}
\label{sec:baselines}


A domain agnostic self-supervised algorithm is comprised of a domain-agnostic encoder, pretraining objective, and transfer method for learning downstream tasks. However, to the best of our knowledge no previously-proposed method is compatible off-the-shelf with all of the domains in DABS. To establish some baseline approaches, we propose two simple, domain-agnostic algorithms that we evaluate on DABS, which we describe below and hope will serve as useful starting points for future research on domain-agnostic SSL. The core idea behind these algorithms is simple: use a small set of domain-specific embeddings modules to map inputs into an embedding space, and then define the pretraining task directly on those embeddings as opposed to the original inputs.

\subsection{Transformer Architecture}
Our algorithms use a generalized architecture based on transformers \citep{Vaswani2017AttentionIA}. These transformers take as input the sequence of embeddings obtained from the DABS embedding modules, then process them through a series of self-attention and feed-forward layers. We use a 12-layer transformer with hidden size 256, 8 attention heads, and dropout with probability 0.1. To obtain a feature vector for the input, the activations from the final layer are averaged and projected to a 128-dimensional vector. The sequence lengths vary in length depending on the dataset input dimensions and patch sizes, listed in Table \ref{table:datasets}. The same Transformer architecture is used across all experiments and is optimized with the AdamW optimizer with learning rate 1e-4 and weight decay 1e-4.


\subsection{Pretraining Objectives}
Given this common architecture, the models are then optimized with respect to a pretraining objective, which enables them to learn useful capabilities and representations from the data. We propose two baseline domain-agnostic SSL objectives, which generalize existing domain-specific methods:

\subsubsection{e-Mix: A Contrastive Embedding-Mixup Objective}
Contrastive learning and other view-matching objectives have made great strides establishing themselves as competitive or even superior alternatives to supervised pretraining in computer vision \citep{Wu2018UnsupervisedFL, Chen2020ASF, He2020MomentumCF, Grill2020BootstrapYO}. Several works have identified the reliance of these algorithms on hand-designed augmentation or ``view'' functions as a crucial impediment to applying these methods in a more domain-agnostic method. These works either \emph{learn} these view functions from scratch using a generative model \citep{Tamkin2020ViewmakerNL}, or rely on handcrafted augmentations which are more domain-general such as mixup \citep{Lee2021iMIXAD, Verma2020TowardsDC}. However, these works make assumptions about the structure of the inputs (e.g. that they are continuous) and use domain-specific encoders, leaving room for a more general solution.

We propose \emph{e-Mix}, a generalization of the i-Mix approach proposed in \citet{Lee2021iMIXAD}. In i-Mix, a batch of inputs $x_{1 \ldots N}$ is perturbed with mixup noise \citep{Zhang2018mixupBE}, where examples are additively mixed with other examples in the dataset. This produces mixed inputs $\tilde x_{1 \ldots N}$, where $\tilde x_{i} = \lambda x_i + (1 - \lambda) x_{\pi(i)}$ for some random permutation $\pi : \{1 \ldots N\} \rightarrow \{1 \ldots N\}$ and mixing coefficient $\lambda \sim \textrm{Uniform}(0.5, 1)$, chosen for each example. Then, the task is to learn an encoder $f$ such that the vector $f(\tilde x_i)$ is close to both $f(x_i)$ and $f(x_{\pi(i)})$ in proportion to their respective mixing coefficients. Formally, the loss is:
\begin{align}
    \ell_{\textrm{i-Mix}}(x, \pi, \lambda) = 
    - \sum_{n=1}^{N} v_{i,n} \log \frac
    {\exp(\text{sim}(f(x_i), f(\tilde x_n)) / \tau))}
    {\sum_{k=1}^N \exp(\text{sim}(f(x_i),  f(\tilde x_k)) / \tau)}
\end{align}
where $\text{sim}$ denotes the cosine similarity, $\tau > 0$ is the temperature, and $v_{i,n}$ is a virtual label given by \begin{equation}
    v_{i,n}=
    \begin{cases}
      \lambda, & \text{if } n=i \\
      1-\lambda, & \text{if } n=\pi(i) \\
      0, & \text{otherwise}
    \end{cases}
  \end{equation}
 The main generalization provided by e-Mix is that the mixup noise is applied to the outputs of the embedding modules (i.e. the patch or token embeddings), as opposed to the inputs directly, which may not in general be continuous. This enables e-Mix to be applied without changes to each of the seven domains in the benchmark. To obtain $f(x_i)$ for a given example $x_i$, we process an input with the transformer, then mean pool the outputs along the sequence length dimension, and finally pass the resulting vector through a fully-connected layer with output size 128.

\subsubsection{SHED: A Shuffled Embedding Prediction Objective}

In contrast to the contrastive objectives that have become common in continous domains such as images and speech, token-level objectives have been more common in natural language processing. Perhaps the most paradigmatic example is masked language modeling \citep{Devlin2019BERTPO}, where random tokens in the input are either redacted or modified and the goal of the main network is to denoise the input. While this approach has been successfully applied to text, as well as other domains such as images \citep{Dosovitskiy2020AnII} and audio \citep{Liu2020MockingjayUS}, domain-specific modules are still needed to predict the inputs, which may be downsampled, averaged, or otherwise processed to improve performance.

To avoid this domain-specific complexity, we generalize another family of objectives based on the ELECTRA \citep{Clark2020ELECTRAPT} method for pretraining on text. Rather than reconstruct noised tokens as BERT does \citep{Devlin2019BERTPO}, ELECTRA involves replacing a subset of tokens in the input with substitutes, then training a detector network to predict which tokens were replaced. Substitute tokens can be chosen randomly, or generated by a BERT network. Similar replacement-detection methods have also recently been applied successfully to tabular data \citep{Iida2021TABBIEPR}, suggesting this objective is not text-specific.

We generalize ELECTRA by applying replacements at the embedding level, instead of the level of input tokens. This enables us to apply the method equally across all modalities, without domain-specific adjustments. To perform the replacements, we select 15\% of the embedding positions per input, then shuffle those embeddings among each other according to a random permutation. See the Appendix for more details. The task of the network is then to predict which of the embeddings were shuffled; this is instantiated as a binary prediction task performed by passing each output embedding through a fully-connected layer. We call this method ShED: Shuffled Embedding Detection.

\subsection{Linear Classification}

We transfer our trained models to downstream tasks with linear classifiers, a simple approach which enables the same base model to be adapted to many downstream tasks without storing a separate copy of the model for each task. We use the Adam optimizer \citep{kingma2014adam} with learning rate of 1e-4, $\beta_1 = 0.9, \beta_2 = 0.999$ for 100 epochs. We also compare against a randomly-initialized model which has not undergone training, to quantify the gains attributable to pretraining.

\subsection{Results}
We report average metrics by domain in Table \ref{table:results}, and full results for each transfer task in Table \ref{table:results-all}.  Our pretrained models broadly show gains over models that were not pretrained, although the gains are uneven and often quite modest compared to state-of-the-art domain-specific approaches. While the gains from pretraining are clear across transfer tasks in natural images, speech, text, sensors, and medical imaging, pretraining appears to hurt in the multimodal image-text domain, leaving a clear need for future work.  Interestingly, the relative gains for these algorithms also seems to reflect their communities of origin: e-Mix performs best on natural images, while ShED performs better on text-based tasks. Investigating the principles underlying these differences is an interesting avenue for future work, as is discovering methods that work better across all domains. 


\begin{table}
\centering
\begin{tabular}{cccccccc}
\toprule
Method & Natural Img & Eng Text & Speech & Mul Text  & Sensors & Medical & Img \& Text \\
\midrule
None   & 9.9 & 42.3 & 24.9 & 58.1 & 69.8 & 68.1 & 51.4  \\
e-Mix  & \textbf{25.6} & 44.1 & \textbf{41.8} &	\textbf{59.9} & 79.5 & 72.4 & \textbf{54.3}  \\
ShED   & 17.9 & \textbf{48.4} & 36.5 & 56.2 & \textbf{88.7} & \textbf{74.5} & 53.4  \\
\bottomrule
\end{tabular}
\vspace{.2cm}
\caption{\textbf{Downstream linear classifier performance of baseline domain-agnostic methods across domains.} Reported numbers are average evaluation metrics across transfer tasks within a domain. Metrics are percent accuracy, with the exception of Medical Imaging (average percent AUC across the five pathologies), and two Text tasks: CoLA (Pearson correlation) and STS-B (Spearman correlation). ``None'' refers to a randomly-reinitialized model that has not been trained.}
\label{table:results}
\end{table}

\section{Limitations and Conclusion}
\label{sec:conclusion}

DABS also has limitations. For example, a tradeoff exists between keeping a benchmark reasonably compact so it can be run easily and representing the full range of domains one might care about. Our choice of seven diverse domains represents a middle ground, but DABS is also a ``living benchmark,'' and we plan in the future to introduce domains spanning an even broader range of fields, data types, and applications to drive further progress towards domain-general SSL methods.\footnote{As of January 2023, two such extensions are DABS 2.0 \citep{tamkin2022dabs}, focusing on STEM domains, and BenchMD \citep{BenchMD}, focusing on healthcare domains.} 

In addition, DABS does not capture how well domain-agnostic methods can be combined with domain-specific methods in a hybrid manner, which may be of greater relevance to domains like natural images where many domain-specific augmentations have already been developed. This is an important yet challenging-to-frame problem, and we encourage future work in this direction.

We have presented DABS: a Domain-Agnostic Benchmark for Self-Supervised Learning. Algorithms that perform well on DABS may have significant practical impact, unlocking the benefits of pretraining for a wide array of domains without a significant ML presence. We also hope DABS enables researchers to better understand the general principles underlying self-supervised learning across different domains, especially as the technology matures and becomes more broadly deployed.

\section*{Acknowledgments and Funding Disclosures}
We would like to thank Shyamal Buch, Jesse Mu, Jared Davis, Daniel Rothchild, and Mike Wu for useful discussions and feedback. AT is supported by an Open Phil AI Fellowship.

\bibliographystyle{plainnat}

\bibliography{references}


\newpage
\appendix

\section{Changelog}
\paragraph{Jan 2023} Fixed a bug in the captioned images domain. Reran pretraining and transfer for this domain and updated the paper and codebase.

\section{ShED Permutations}

In ShED, embeddings are shuffled with a derangement: a permutation where no element is placed in its original position. To efficiently compute derangements, we restrict ourselves to a set of cyclic derangements $\pi : \{0 \ldots N-1\} \to \{0 \ldots N-1\}$ where $\pi(i) = {i + 1 \mod N}$. Here, the inputs to this derangement $i \in \{0 \ldots N-1\}$ index the set of randomly chosen embeddings to shuffle, which in general may not appear in the same order as in the original sequence.

\section{Compute requirements}
\label{appendix:compute}
All runs were performed on an internal cluster with single Titan X GPUs. Most pretraining jobs required approximately 1 GPU-day, while the transfer jobs ranged from several minutes (e.g. CoLA) to over 1 GPU-day (VQA).

\section{Dataset Licenses}
\label{appendix:licenses}
Below we list each dataset's license, as provided either in the paper proposing the dataset or on the dataset website. For datasets where we were unable to find a license, we list ``No License.''
\begin{itemize}
    \item \textbf{Natural Images:} ImageNet (ImageNet Terms of Access\footnote{\url{https://image-net.org/download}}), CIFAR-10 (MIT License), Describable Textures (``This data is made available to the computer vision community for research purposes.''\footnote{\url{https://www.robots.ox.ac.uk/~vgg/data/dtd/}}), FGVC-Aircraft (``the images are made available exclusively for non-commercial research purposes''\footnote{\url{https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/\#ack}}), Caltech-UCSD Birds (``Their use is restricted to non-commercial research and educational purposes.''\footnote{\url{http://www.vision.caltech.edu/visipedia/CUB-200-2011.html}}), German Traffic Sign Recognition Benchmark (CC0: Public Domain), VGG Flower (GNU General Public License, version 2).
    \item \textbf{Speech:} Librispeech (CC-BY 4.0), VoxCeleb (CC-BY 4.0), AudioMNIST (MIT License), Google Speech (CC-BY 4.0), Fluent Speech Commands (CC BY-NC-ND 4.0) 
    \item \textbf{Monolingual English Text:} WikiText103 (CC BY-SA 4.0), COLA (``We expect that research use within the US is legal under fair use''\footnote{\url{https://nyu-mll.github.io/CoLA/}}), MNLI (OANC's license), MRPC (No License), QNLI (CC BY-SA 4.0), QQP (``This data is subject to Quora's Terms of Service, allowing for non-commercial use''\footnote{\url{https://www.quora.com/about/tos}}), RTE (No License), SST2 (CC0: Public Domain), STSB (CC BY-SA 3.0), WNLI (No License)
    \item \textbf{Medical Imaging:} CheXpert (Stanford University School of Medicine CheXpert Dataset Research Use Agreement\footnote{\url{https://stanfordmlgroup.github.io/competitions/chexpert/}}),  ChestX-ray 8 (CC0: Public Domain)
    \item \textbf{Wearable Sensors:} PAMAP2 (CC-BY 4.0)
    \item \textbf{Images \& Text:} MSCOCO (CC-BY 4.0), VQA (CC-BY 4.0)
    \item \textbf{Multilingual Text:} mC4 (ODC-BY), PAWS-X(``The dataset may be freely used for any purpose''\footnote{\url{https://github.com/google-research-datasets/paws/blob/master/LICENSE}})
\end{itemize}

\section{Origins and Collection of the Datasets in DABS}
\label{appendix:consent}
DABS makes use of a diverse array of kinds of data. Here, we detail to the best of our knowledge how these datasets were collected, including whether consent was explicitly obtained from humans providing the data.

\begin{itemize}
    \item For the Describable Textures Dataset, \citet{cimpoi14describing}  set forth that ``images [were] downloaded from Google and Flickr by entering the attributes and related terms as search queries.''
    \item For VGG flowers, \citet{Nilsback2008AutomatedFC} note that they gathered “public images from various websites, with some supplementary images from our own photographs.”
    \item For CIFAR-10, \citet{Krizhevsky2009LearningML} used several search engines, including Google, Flickr, and Altavista to collect images.
    \item For ImageNet, \citet{Russakovsky2015ImageNetLS} state that ``We collect candidate images from the Internet by querying several image search engines.''
    \item For the Birds dataset, \citet{WelinderEtal2010} state that ``The images were downloaded from the website Flickr and filtered by workers on Amazon Mechanical Turk.''
    \item For the PAMAP2 dataset, \citet{Reiss2012IntroducingAN} explicitly note that participants consented to having their data be used for scientific purposes. 
    \item For CheXpert, \citet{Garbin2021StructuredDD} note ``Individual patient consent is waived for de-identified data in compliance with institutional IRB and federal guidelines.''
    \item For ChestX-ray 8, \citet{Wang2017ChestXRay8HC} state that the data was retrieved from an NIH collection of radiology reports and images pulled ``with IRB approval (OHSRP \#5357)'' from the Indiana Network for Patient Care. ``The images and reports were de-identified automatically and then the automatic de-identification was manually verified.''
    \item For LibriSpeech, \citet{Panayotov2015LibrispeechAA} note that the LibriSpeech corpus is a read speech dataset based on LibriVox’s audiobooks. ``The LibriVox project, a volunteer effort, is currently responsible for the creation of approximately 8000 public domain audiobooks. Most of the recordings are based on texts from Project Gutenberg, also in the public domain.”
    \item For Google Speech Commands, \citet{speechcommandsv2} note that ``The dataset has 65,000 one-second long utterances of 30 short words, by thousands of different people, contributed by members of the public through the AIY website.''
    \item For VoxCeleb1, \citet{Nagrani17} state that ``VoxCeleb contains over 100,000 utterances for 1,251 celebrities, extracted from videos uploaded to YouTube.''
    \item For Fluent Speech Command, \citet{Lugosch2019SpeechMP} state that ``The data was collected using crowdsourcing. Participants consented to data being released and provided demographic information about themselves.''
    \item For AudioMNIST,  \citet{becker2018interpreting} state that ``All speakers were informed about the intent of the data collection and have given written declarations of consent for their participation prior to their recording session.''
    \item For WikiText-103,  \citet{Merity2017PointerSM} note that ``The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.''
    \item For German Traffic Sign Benchmark, \citet{Houben-IJCNN-2013} note that ``The dataset was created from approx. 10 h of video that were recorded while driving on different road types in Germany during daytime. The sequences were recorded in March, October and November 2010.''
    \item For FGVC-Aircraft, \citet{maji13fine-grained} state that ``the photographers kindly made available their images for research purposes.''
    \item For MS COCO, \citet{Lin2014MicrosoftCC} state that they collected images from Flickr, and used Amazon Mechanical Turk for crowdsourcing image annotations.
    \item For VQA, \citet{Agrawal2015VQAVQ} note that  ``We use the 123,287 training and validation images and 81,434 test images from the newly-released Microsoft Common Objects in Context (MS COCO) dataset,'' as well as crowdsourcing questions and answers through Amazon Mechanical Turk.
    \item For mC4, \citet{2019t5} note that the dataset ``is generated from 71 Common Crawl dumps.''
    \item For PAWS-X, \citet{pawsx2019emnlp} note that ``The PAWS dataset contains 108,463 human-labeled pairs in English, sourced from Quora Question Pairs (QQP) and Wikipedia pages. PAWS-X contains 23,659 human translated PAWS evaluation pairs and 296,406 machine translated training pairs.''
    
    
\end{itemize}


\section{PII and Offensive Content}
\label{appendix:pii-offense}

To the best of our knowledge, none of the DABS datasets contains information directly identifying people involved in the creation of the data. However, some kinds of data, most notably  x-ray, speech, and wearable sensor data, may contain enough information about a person such that it could be used to identify them given appropriate information from other sources. 

It is quite likely that offensive content exists in some of our datasets, including Wikipedia (the source of WikiText-103), LibriVox (the repository of public-domain ebooks from which LibriSpeech was derived), YouTube (the origin of the celebrity voice snippets that comprise VoxCeleb), and especially Common Crawl (the origin of the mC4 dataset).


\section{Potential Negative Impacts}
\label{appendix:negative-impacts}

Domain-agnostic self-supervised learning is a very general technology, as a single SSL model can be applied to many different end tasks, and domain-agnosticity means that we must consider potential SSL models across many different domains. Thus, it is challenging to forecast potential negative impacts with certainty; however, we can delineate two potential kinds of negative impacts from domain-agnostic SSL research:
\begin{itemize}
    \item Domain-agnostic SSL may be used for a wide range of purposes, including both bad and good actors. As with good actors, domain-agnostic SSL may enable bad actors to create SSL algorithms for particular domains and applications where they were previously unable to do so, magnifying potential threats. Policies and norms, both formal and informal, may be useful tools for furthering positive impacts while preventing negative applications.
    \item An increased focus on domain-agnostic methods at the expense of domain-specific methods could have negative impacts. For example, the inductive bias afforded by domain-specific assumptions may enable the development of SSL algorithms in domains where less unlabeled data exists, including lower-resource languages. Furthermore, domain-specific knowledge and expertise are crucial tools both in the curation of data for SSL models and in their informed and ethical deployment to end tasks of interest. Thus, while we hope DABS alleviates the need for developing some domain-specific machine learning techniques through trial and error, domain-specific concerns should still guide the use and deployment of SSL.
\end{itemize}

\section{Additional Reproducibility Details}
\label{sec:repro}
In this section, we describe additional details regarding the processing and use of each dataset.

\subsection{Images and Descriptions}
The datasets in this domain are both based on the MS COCO dataset, which contains images that vary in size on the order of magnitude of \textasciitilde{600} x \textasciitilde{400}. We resize all images to 640 x 480, take a center crop of size 480 x 480, and resize to 224 x 224. The resulting image is divided into patches of size 16 x 16 that are passed into the embedding layer. All descriptions are tokenized and padded or truncated to sequences of 32 tokens.

In order to create the binary classification task for VQA, we create a sequence of tokens the form \texttt{<tokenized question>[SEP]<tokenized answer>} and then encode it with the image as during pretraining. \texttt{<tokenized answer>} is randomly chosen from the set of incorrect multiple choice answers in VQA 50\% of the time, and is left as the correct answer the remaining fraction. The binary classification task is to determine whether the correct or incorrect answer was chosen.  

\subsection{Medical Imaging}
CheXpert contains x-ray images that vary in size on the order of magnitude of \textasciitilde{320} x \textasciitilde{320}. We simply resize all images to 224 x 224. The resulting image is divided into patches of size 16 x 16 that are passed into the embedding layer.

\subsection{Natural Images}
All natural images in our datasets consist of images that are 32 x 32, so we do not apply any preprocessing transforms. The resulting image is divided into patches of size 4 x 4 that are passed into the embedding layer.

\subsection{Sensor}
Each measurement in PAMAP2 consists of 52 sensor signals from different parts of the body. We first take random subsamples of length 320. The resulting size 52 x 320 examples are then divided into 1-dimensional segments that each contain sensor readings from 5 measurements (a 1D analogue to the patch embeddings proposed by \citep{Dosovitskiy2020AnII}). These segments are ultimately passed into the embedding layer to produce the inputs for the transformer.

\subsection{Speech}
For all speech data, we take a random subsegment of 150526 audio samples (at 16kHz), compute its mel-spectrogram with hop length 672 and 224 mel bins, convert from decibels to power scale, and normalize with a fixed mean and standard deviation of the corresponding speech dataset. The spectrum is treated as a single-channel image and divided into patches of size 16 x 16 that are passed into the embedding layer.

\subsection{English Monolingual Text}
All text data is tokenized with the Hugging Face BertTokenizer,\footnote{\url{https://huggingface.co/transformers/model_doc/bert.html}} and the resulting token sequences are padded or truncated to 128 tokens.

\subsection{Multilingual Text}
All text data is tokenized with the Hugging Face XLMRobertaTokenizer,\footnote{\url{https://huggingface.co/transformers/model_doc/xlmroberta.html}} and the resulting token sequences are padded or truncated to 128 tokens.

\begin{small}
\begin{table}
\centering
\begin{tabular}{cccccccc}
\toprule
Dataset & Domain & Phase & Examples (train/val) & Patch Size & Dimensions & Batch \\
\midrule
ImageNet   & Image & Pretrain & 1,281,167/50,000 & 16 x 16 & 3 x 32 x 32 & 64 \\
CIFAR-10   & Image & Transfer & 50,000/10,000 & 16 x 16 & 3 x 32 x 32 & 64 \\
Textures   & Image & Transfer & 3,760/1,880 & 16 x 16 & 3 x 32 x 32 & 64 \\
Aircrafts   & Image & Transfer & 6,667/3,333 & 16 x 16 & 3 x 32 x 32 & 64 \\
Birds   & Image & Transfer & 5,994/5,794 & 16 x 16 & 3 x 32 x 32 & 64 \\
Traffic-signs   & Image & Transfer & 600/300 & 16 x 16 & 3 x 32 x 32 & 64 \\
Flowers   & Image & Transfer & 6,507/1,682 & 16 x 16 & 3 x 32 x 32 & 64 \\
Librispeech   & Speech & Both & 145,265/8,251 & 16 x 16 & 224 x 224 & 64 \\
VoxCeleb   & Speech & Transfer & 2,148/555 & 16 x 16 & 224 x 224 & 64 \\
Fluent Speech   & Speech & Transfer & 26,250/3,793 & 16 x 16 & 224 x 224 & 64 \\
Google Speech   & Speech & Transfer & 115,816/11,005 & 16 x 16 & 224 x 224 & 64 \\
AudioMNIST   & Speech & Transfer & 24,000/6,000 & 16 x 16 & 224 x 224 & 64 \\
WikiText-103   & Eng. Text & Pretrain & 1,165,029/2,461 & --- & 128 & 128 \\
CoLA   & Eng. Text & Transfer & 8,551/1,043 & --- & 128 & 128 \\
SST-2   & Eng. Text & Transfer & 67,349/872 & --- & 128 & 128 \\
MRPC   & Eng. Text & Transfer & 3,668/408 & --- & 128 & 128 \\
QQP   & Eng. Text & Transfer & 363,846/40,430 & --- & 128 & 128 \\
STS-B   & Eng. Text & Transfer & 5,749/1,500 & --- & 128 & 128 \\
MNLI   & Eng. Text & Transfer & 392,702/19,647 & --- & 128 & 128 \\
QNLI   & Eng. Text & Transfer & 104,743/5,463 & --- & 128 & 128 \\
RTE   & Eng. Text & Transfer & 2,490/277 & --- & 128 & 128 \\
WNLI   & Eng. Text & Transfer & 635/71 & --- & 128 & 128 \\
mC4   & Mul. Text & Pretrain & 26TB+ & --- & 128 & 128 \\
PAWS-X (all)   & Mul. Text & Transfer & 296,406/23,659 & --- & 128 & 128 \\
CheXpert   & X-Rays & Both & 223,414/234 & 16 x 16 & 3 x 224 x 224 & 64 \\
PAMAP2   & Sensor & Both & 50,000/10,000 & 5 & 52 x 320 & 256 \\
MSCOCO   & V+L & Pretrain & 117,266/4,952 & 16 x 16 & 3 x 224 x 224, 32 & 64 \\
Mismatched Caption   & V+L & Transfer & 117,266/4,952 & 16 x 16 & 3 x 224 x 224, 32 & 64 \\
VQA   & V+L & Transfer & 248,349/121,512 & 16 x 16 & 3 x 224 x 224, 32 & 64 \\
\bottomrule
\end{tabular}
\vspace{.2cm}
\caption{\textbf{Statistics of different pretraining and transfer datasets used in DABS.}}
\label{table:datasets}
\end{table}
\end{small}


\begin{table}
\centering
\begin{tabular}{ccccccc}
\toprule
Dataset          & Domain   & Metric        & None    & e-Mix   & ShED   \\
\midrule
CIFAR-10         & Images   & Accuracy       & 24.20 & 39.43 & \textbf{39.63}  \\
Birds            & Images   & Accuracy       & 1.62  & \textbf{3.86}  & 2.95   \\
VGG Flower          & Images   & Accuracy    & 9.03  & \textbf{25.96} & 13.03  \\
DTD (Textures)        & Images   & Accuracy  & 7.39  & 8.83  & \textbf{18.35}  \\
GTSRB (Traffic)    & Images   & Accuracy     & 14.33 & \textbf{65.07} & 27.51  \\
FGVC-Aircraft        & Images   & Accuracy   & 2.70  & \textbf{10.15} & 5.60   \\

LibriSpeech Sp. ID     & Speech   & Accuracy    & 17.12 & \textbf{60.18} & 34.77  \\
VoxCeleb Sp. ID        & Speech   & Accuracy       & 0.59  & 2.43  & \textbf{2.81}   \\
AudioMNIST       & Speech   & Accuracy       & 33.13 & \textbf{80.35} & 67.33  \\
Google Speech    & Speech   & Accuracy       & 4.87  & 19.22 & \textbf{20.73}  \\
Fluent Locations & Speech   & Accuracy       & \textbf{62.09} & 60.93 & 60.24  \\
Fluent Actions   & Speech   & Accuracy       & 26.15 & 29.87 & \textbf{30.53}  \\
Fluent Objects   & Speech   & Accuracy       & 30.13 & \textbf{39.89} & 39.36  \\
COLA             & English Text     & Pearson Corr.       & 0.00     & 8.40  & \textbf{19.00}  \\
MNLI\_Matched    & English Text     & Accuracy       & 35.80 & 37.80 & \textbf{43.10}  \\
MNLI\_Mismatched & English Text     & Accuracy       & 36.60 & 37.50 & \textbf{44.20}  \\
MRPC             & English Text     & Accuracy       & 68.40 & 66.20 & \textbf{70.10}  \\
QNLI             & English Text     & Accuracy       & 57.70 & 57.90 & \textbf{65.50}  \\
QQP              & English Text     & Accuracy       & 65.10 & 64.30 & \textbf{68.60}  \\
RTE              & English Text     & Accuracy       & \textbf{54.50} & 51.30 & 52.70  \\
SST2             & English Text     & Accuracy       & 57.00 & 58.10 & \textbf{59.30}  \\
STSB             & English Text     & Accuracy       & 4.20  & 11.40 & \textbf{17.60}  \\
WNLI             & English Text     & Accuracy       & 43.60 & \textbf{47.90} & 43.60  \\
PAWS-X EN  & Multilingual Text  & Accuracy  & 57.85 & 54.85 & \textbf{61.50}  \\
PAWS-X FR  & Multilingual Text  & Accuracy  & 57.80 & 55.90 & \textbf{60.90}  \\
PAWS-X ES  & Multilingual Text  & Accuracy  & 58.55 & 55.50 & \textbf{63.65}  \\
PAWS-X DE  & Multilingual Text  & Accuracy  & 58.85 & 56.50 & \textbf{62.20}  \\
PAWS-X ZH  & Multilingual Text  & Accuracy  & 57.35 & 55.35 & \textbf{58.55}  \\
PAWS-X JP  & Multilingual Text  & Accuracy  & \textbf{57.55} & 57.35 & 52.00  \\
PAWS-X KO  & Multilingual Text  & Accuracy  & 58.80 & 57.70 & \textbf{60.60}  \\
PAMAP2           & Sensor   & Accuracy       & 69.81 & 79.48       & \textbf{88.69}  \\
CheXpert    & Chest X-Rays    & Avg. AUROC & 68.14 & 72.40 & \textbf{74.50}        \\
ChestX-ray8    & Chest X-Rays    & Avg. AUROC & 57.00 & 63.00 & \textbf{63.70}        \\
VQA              & Vision/Language & Accuracy & 53.38 & \textbf{58.77}       & 54.25  \\
MSCOCO Mismatched              & Vision/Language & Accuracy & 49.41 & 49.86       & \textbf{52.6}  \\
\bottomrule
\end{tabular}
\vspace{.2cm}
\caption{\textbf{Downstream linear classifier performance of baseline domain-agnostic methods across all datasets.} ``None'' refers to a randomly-initialized model that has not been pretrained.}
\label{table:results-all}
\end{table}



\end{document}