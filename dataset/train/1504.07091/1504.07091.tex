\documentclass[english]{llncs}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}
\usepackage{todonotes}
\usepackage{fancyhdr}
\usepackage{tabularx,caption}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{babel}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{wrapfig}
 \usepackage{subcaption}

\makeatletter





\makeatother

\clearpage{}\newcommand{\centre}[1]{z_{#1}}
\newcommand{\centres}{Z}
\newcommand{\degree}{\operatorname{maxdeg}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\ord}{\operatorname{ord}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\LL}{\mathbf{L}}
\newcommand{\f}{\hat{f}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\dhat}{\overline{d}}
\newcommand{\llminimal}{-minimal\xspace}

\newcommand{\ie}{i.\,e.\xspace}
\newcommand{\eg}{e.\,g.\xspace}
\newcommand{\etal}{et al.\xspace}
\newcommand{\Wlog}{w.\,l.\,o.\,g.\ }
\newcommand{\wrt}{w.\,r.\,t.\xspace}
\newcommand{\st}{s.\,t.\xspace}
\newcommand{\vd}{\xspace}
\newcommand{\bms}{\textsf{BMS}\xspace}
\newcommand{\rk}{\textsf{RK}\xspace}
\newcommand{\ia}{\textsf{IA}\xspace}
\newcommand{\iaw}{\textsf{IAW}\xspace}
\newcommand{\da}{\textsf{DA}\xspace}
\newcommand{\daw}{\textsf{DAW}\xspace}
\newcommand{\vda}{\xspace}
\newcommand{\upvd}{\textsf{updateApprVD-W}\xspace}
\newcommand{\sssp}{\textsf{updateSSSP-W}\xspace}
\newcommand{\upvdu}{\textsf{updateApprVD-U}\xspace}
\newcommand{\ssspu}{\textsf{updateSSSP-U}\xspace}

\newcommand{\didic}{\textsc{DiDiC}\xspace}
\newcommand{\dibap}{\textsc{DibaP}\xspace}
\newcommand{\bubble}{\textsc{Bubble}\xspace}
\newcommand{\bubfosc}{\textsc{Bubble-FOS/C}\xspace}
\newcommand{\kappart}{\textsc{KaPPa}\xspace}
\newcommand{\metis}{\textsc{METIS}\xspace}
\newcommand{\kmetis}{\textsc{kMETIS}\xspace}
\newcommand{\parmetis}{\textsc{ParMETIS}\xspace}
\newcommand{\jostle}{\textsc{JOSTLE}\xspace}
\newcommand{\psiconsol}{\textsc{TruncCons}\xspace}
\newcommand{\consol}{\texttt{Consolidation}\xspace}
\newcommand{\consols}{\texttt{Consolidations}\xspace}
\newcommand{\asspart}{\texttt{AssignPartition}\xspace}
\newcommand{\compcen}{\texttt{ComputeCenters}\xspace}
\newcommand{\trunccons}{\textsc{TruncCons}\xspace}
\newcommand{\graclus}{\textsc{Graclus}\xspace}
\newcommand{\disjcup}{\mathop{\dot{\cup}}}

\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\Pro}[1]{\mathbf{Pr} \left[\,#1\,\right]}
\newcommand{\pro}[1]{\mathbf{Pr} [\,#1\,]}
\newcommand{\Ex}[1]{\mathbb{E} \left[\,#1\,\right]}
\newcommand{\ex}[1]{\mathbb{E} [\,#1\,]}
\newcommand{\hit}{- [\pi]_s (H[v,s]-H[u,s])}


\clearpage{}


\newcommand{\vertical}[1]{\begin{sideways}#1\end{sideways}}
\newcommand{\TODO}[1]{\todo[size=\small, inline]{#1}}
\newcommand{\tinytodo}[1]{\todo[size=\tiny, inline]{#1}}


\newcommand{\hmey}[1]{\textcolor{orange}{[HM: #1]}\xspace}
\newcommand{\ebe}[1]{\textcolor{green}{[EB: #1]}\xspace}
\newcommand{\cut}[1]{\textcolor{lightgray}{#1}}



\pagestyle{fancy}



\include{header}

\begin{document}

\author{Elisabetta Bergamini \and Henning Meyerhenke}
\institute{Institute of Theoretical Informatics \\ Karlsruhe Institute of Technology (KIT), Germany \\
  Email: \email{\{elisabetta.bergamini, meyerhenke\}\,@\,kit.edu}
}

\title{Fully-dynamic Approximation \\ of Betweenness Centrality}

\date{}

\maketitle

\begin{abstract}
Betweenness is a well-known centrality measure that ranks the nodes of a network according to their participation in shortest paths.
Since an exact computation is prohibitive in large networks, several approximation algorithms have been proposed. Besides that, recent years
have seen the publication of dynamic algorithms for efficient recomputation of betweenness in evolving networks. In previous work  we
proposed the first semi-dynamic algorithms that recompute an \emph{approximation} of betweenness in connected graphs after batches of edge insertions. 

In this paper we propose the first fully-dynamic approximation algorithms (for weighted and unweighted undirected graphs that need not to be connected) with a provable guarantee on the maximum approximation error.
The transfer to fully-dynamic and disconnected graphs implies additional algorithmic problems that could be of independent interest. In particular, we propose a new upper bound on the vertex diameter for weighted undirected graphs. For both weighted and unweighted graphs, we also propose the first fully-dynamic algorithms that keep track of this upper bound.
In addition, we extend our former algorithm for semi-dynamic BFS to batches of both edge insertions and deletions. 

Using approximation, our algorithms are the first to make in-memory computation of betweenness in fully-dynamic networks with millions of edges feasible. Our experiments show that they can achieve substantial speedups compared to recomputation, up to 
several orders of magnitude.\0.25ex]
\noindent \textbf{Keywords:}  betweenness centrality, algorithmic network analysis, fully-dynamic graph algorithms, approximation algorithms, shortest paths
\end{abstract}





\section{Introduction}
\label{sec:intro}
The identification of the most central nodes of a network is a fundamental problem in network analysis. \emph{Betweenness
centrality} (BC) is a well-known index that ranks the importance of nodes according to their participation in \textit{shortest paths}. Intuitively, a node has 
high BC when it lies on many shortest paths between pairs of other nodes.
Formally, BC of a node  is defined as , where  is the number of nodes,
 is the number of shortest paths between two nodes  and  and  is the number of these paths that go through node .
Since it depends on \textit{all} shortest paths, the exact computation of BC is expensive: the best known 
algorithm~\cite{Brandes01betweennessCentrality} is quadratic in the number of nodes for sparse networks and cubic for dense networks, prohibitive for networks with hundreds of thousands of nodes. Many graphs of interest, however, such as web 
graphs or social networks, have millions or even billions of nodes and edges. 
For this reason, approximation algorithms~\cite{DBLP:journals/ijbc/BrandesP07,DBLP:conf/alenex/GeisbergerSS08,DBLP:conf/waw/BaderKMM07} must be used in practice. In addition, many
large graphs of interest evolve continuously, making the efficient recomputation of BC a necessity.
In a previous work, we proposed the first two approximation algorithms~\cite{DBLP:conf/alenex/BergaminiMS15} (\ia for unweighted and \iaw for weighted graphs) 
that can efficiently recompute the approximate BC scores after batches of edge insertions or weight decreases. 
\ia and \iaw are the only semi-dynamic algorithms that can actually be applied to large networks. 
The algorithms build on \rk~\cite{DBLP:conf/wsdm/RiondatoK14}, a static algorithm with a theoretical guarantee on the quality of the approximation, and inherit this guarantee from \rk. 
However, \ia and \iaw target a relatively restricted configuration: only connected graphs and edge insertions/weight decreases.
\vspace{-1ex}
\paragraph{Our contributions.}
In this paper we present the first fully-dynamic algorithms (handling edge insertions, deletions and arbitrary weight updates) for BC approximation in weighted and unweighted undirected graphs. Our algorithms extend the semi-dynamic ones we presented in~\cite{DBLP:conf/alenex/BergaminiMS15}, while keeping the theoretical guarantee on the maximum approximation error.
The transfer to fully-dynamic and disconnected graphs implies several additional problems compared to the restricted case we considered previously~\cite{DBLP:conf/alenex/BergaminiMS15}.
Consequently, we present the following intermediate results, all of which could be of independent interest.
(i) We propose a new upper bound on the vertex diameter \vd (\ie number of nodes in the shortest path(s) with the maximum number of nodes) for weighted undirected graphs. This can improve significantly the one used in the \rk algorithm~\cite{DBLP:conf/wsdm/RiondatoK14} if the network's weights vary in relatively small ranges (from the size of the largest connected component to at most twice the vertex diameter times the ratio between the maximum and the minimum edge weights).
(ii) For both weighted and unweighted graphs, we present the first fully-dynamic algorithm for updating an approximation of \vd, which is equivalent to the diameter in unweighted graphs.
(iii) We extend our previous semi-dynamic BFS algorithm~\cite{DBLP:conf/alenex/BergaminiMS15} to batches of both edge insertions and deletions.
In our experiments, we compare our algorithms to recomputation with \rk on both synthetic and real dynamic networks. Our results show that our algorithms can achieve substantial speedups, often several orders of magnitude on single-edge updates and are always faster than recomputation on batches of more than 1000 edges. 
\section{Related work}
\label{sec:related_work}
\subsection{Overview of algorithms for computing BC}
The best static exact algorithm for BC (\textsf{BA}) is due to Brandes~\cite{Brandes01betweennessCentrality} and requires  operations for unweighted graphs
and  for graphs with positive edge weights. The algorithm computes a single-source shortest path (SSSP) search from every node  in the graph and adds to the BC score of each node  the fraction of shortest paths that go through . Several static approximation algorithms have been proposed that compute an SSSP search from a set of randomly chosen nodes and extrapolate the BC scores of the other nodes~\cite{DBLP:journals/ijbc/BrandesP07,DBLP:conf/alenex/GeisbergerSS08,DBLP:conf/waw/BaderKMM07}. The static approximation algorithm by Riondato and Kornaropoulos (\rk)~\cite{DBLP:conf/wsdm/RiondatoK14} samples a set of shortest paths and adds a contribution to each node in the sampled paths. This approach allows a theoretical guarantee on the quality of the approximation and will be described in Section~\ref{sec:rk}.
Recent years have seen the publication of a few dynamic exact algorithms~\cite{DBLP:conf/www/LeeLPCC12,DBLP:conf/socialcom/GreenMB12,kourtellis2014scalable,DBLP:journals/snam/KasCC14,DBLP:conf/mfcs/NasrePR14,DBLP:conf/waw/GoelSIS13}. Most of them
store the previously calculated BC values and additional information, like the distance
of each node from every source, and try to limit the recomputation to the nodes whose
BC has actually been affected. All the dynamic algorithms perform better than
recomputation on certain inputs. Yet, none of them
is in general better than \textsf{BA}. In fact, they
all require updating an all-pairs shortest paths (APSP) search, for which no algorithm has an improved worst-case complexity compared to the best static algorithm~\cite{DBLP:journals/algorithmica/RodittyZ11}. 
Also, the scalability of the dynamic exact BC algorithms is strongly compromised by their memory requirement of . To overcome these problems, we presented two algorithms that efficiently recompute an approximation of the BC scores instead of their exact values~\cite{DBLP:conf/alenex/BergaminiMS15}. 
The algorithms have shown significantly high speedups compared to recomputation with \rk and a good scalability, but they are limited to connected graphs and batches of edge insertions/weight decreases (see Section~\ref{sec:bms}).


\subsection{\rk algorithm}
\label{sec:rk}
The static approximation algorithm \rk~\cite{DBLP:conf/wsdm/RiondatoK14} is
the foundation for the incremental approach we presented in~\cite{DBLP:conf/alenex/BergaminiMS15} and our new fully-dynamic approach. \rk samples a set  of  shortest paths between randomly-chosen source-target pairs .
Then, \rk computes the approximated
betweenness  of a node  as the
fraction of sampled paths  that go through , by adding  to 's score for each of these paths.
In each of the  iterations, the probability of a shortest path  to be sampled is . The number  of samples required to approximate the BC scores with the given error guarantee is , where  and  are constants in  and .
Then, if  shortest paths
are sampled according to , with probability at least  the approximations
 are within 
from their exact value: 
To sample the shortest paths according to , \rk first chooses
a source-target node pair  uniformly at random and performs a shortest-path search (Dijkstra or BFS) from  to , keeping also track of the number 
of shortest paths between  and  and of the list of predecessors
 (\ie the nodes that immediately precede  in the shortest paths between  and ) for any node  between  and . Then one shortest path is selected: 
starting from , a predecessor 
is selected with probability .
The sampling is repeated iteratively
until node  is reached.
\paragraph{Approximating the vertex diameter.}
\rk uses two upper bounds on \vd that can be both computed in . For unweighted undirected graphs, it samples a source node  for each connected component of , computes a BFS from each  and sums the two shortest paths with maximum length starting in . The \vd approximation is the maximum of these sums over all components. For weighted graphs, \rk approximates \vd with the size of the largest connected component, which can be a significant overestimation for complex networks, possibly of orders of magnitude. In this paper, we present a new approximation for weighted graphs, described in Section~\ref{sec:new_vd_approx}.
\subsection{\textsf{IA} and \textsf{IAW} algorithms}
\label{sec:bms}
\ia and \iaw are the incremental approximation algorithms (for unweighted and weighted graphs, respectively) that we presented previously~\cite{DBLP:conf/alenex/BergaminiMS15}. The algorithms are based on the observation that if only edge insertions are allowed and the graph is connected, \vd cannot increase, and therefore also the number  of samples required by \rk for the theoretical guarantee. Instead of recomputing  new shortest paths after a batch of edge insertions, \ia and \iaw \textit{replace} each old shortest path  with a new shortest path between the same node pair . In \iaw the paths are recomputed with a slightly-modified \textsf{T-SWSF}~\cite{DBLP:conf/wea/BauerW09}, whereas \ia uses a new semi-dynamic BFS algorithm.
The BC scores are updated by subtracting  to the BC of the nodes in the old path and adding  to the BC of nodes in the new shortest path.

\subsection {Batch dynamic SSSP algorithms}
\label{sssp_update}
Dynamic SSSP algorithms recompute distances from a source node after a single edge update or a batch of edge updates.
Algorithms for the batch problem have been published~\cite{Ramalingam92anincremental,Frigioni_semi-dynamicalgorithms,DBLP:conf/wea/BauerW09} and compared in experimental studies~\cite{DBLP:conf/wea/BauerW09,DBLP:conf/wea/DAndreaDFLP14}.
The experiments show that the tuned algorithm \textsf{T-SWSF} presented in~\cite{DBLP:conf/wea/BauerW09} performs well on many types of graphs and edge updates. For batches of only edge insertions in unweighted graphs, we developed an algorithm asymptotically faster than \textsf{T-SWSF}~\cite{DBLP:conf/alenex/BergaminiMS15}. The algorithm is in principle similar to \textsf{T-SWSF}, but has an improved complexity thanks to different data structures.


\section{New \vd approximation for weighted graphs}
\label{sec:new_vd_approx}
Let  be an undirected graph. For simplicity, let  be connected for now. If it is not, we compute an approximation for each connected component and take the maximum over all the approximations. Let  be an SSSP tree from any source node . Let  denote a shortest path between  and  in  and let  denote a shortest path between  and  in . Let  be the number of nodes in  and  be the distance between  and  in , and analogously for  and . Let  and   be the maximum and minimum edge weights, respectively. Let  and  be the nodes with maximum distance from , \ie . 

We define the \vd approximation . Then:
\begin{proposition}
\label{lem:vd2}
. (Proof in Section~\ref{sub:proof_vd2}, Appendix)
\end{proposition}
To obtain the upper bound \vda, we can simply compute an SSSP search from any node , find the two nodes with maximum distance and perform the remaining calculations.
Notice that \vda extends the upper bound proposed for \rk~\cite{DBLP:conf/wsdm/RiondatoK14} for unweighted graphs: When the graph is unweighted and thus , \vda becomes equal to the approximation used by \rk.
Complex networks are often characterized by a small diameter and in networks like coauthorship, friendship, communication networks, \vd and  can be several order of magnitude smaller than the size of the largest component. This translates into a substantially improved \vd approximation.


\section{New fully-dynamic algorithms}
\paragraph{Overview.}
We propose two fully-dynamic algorithms, one for unweighted (\da, dynamic approximation) and one for weighted (\daw, dynamic approximation weighted) graphs. Similarly to \textsf{IA} and \textsf{IAW}, our new fully-dynamic algorithms keep track of the old shortest paths and substitute them only when necessary. However, if  is not connected or edge deletions occur, \vd can grow and a simple substitution of the paths is not sufficient anymore. 
Although many real-world networks exhibit a shrinking-diameter behavior~\cite{DBLP:conf/kdd/LeskovecKF05}, to ensure our theoretical guarantee, we need to keep track of \vda over time and sample new paths in case \vda increases.
The need for an efficient update of \vda augments significantly the difficulty of the fully-dynamic problem, as well as the necessity to recompute the SSSPs after batches of both edge insertions and deletions. 
The building block for the BC update are basically two: a fully-dynamic algorithm that updates distances and number of shortest paths from a certain source node (SSSP update) and an algorithm that keeps track of a \vd approximation for each connected component of . The following paragraphs give an overview of such building blocks, which could be of independent interest. The last paragraph outlines the dynamic BC approximation algorithm. \textbf{Due to space constraints, a detailed description of the algorithms as well as the pseudocodes and the omitted proofs can be found in the Appendix}.
\paragraph{SSSP update in weighted graphs.}
Our SSSP update is based on \textsf{T-SWSF}~\cite{DBLP:conf/wea/BauerW09}, which recomputes distances from a source node  after a batch  of weight updates (or edge insertions/deletions). For our BC algorithm, we need two extensions of \textsf{T-SWSF}: an algorithm that also recomputes the number of shortest paths between  and the other nodes (\sssp) and one that also updates a \vd approximation for the connected component of  (\upvd). 
The \vd approximation is computed as described in Section~\ref{sec:new_vd_approx}. Thus, \upvd keeps track of the two maximum distances  and  from  and the minimum edge weight .
We call \textit{affected nodes} the nodes whose distance (or also whose number of shortest paths, in \sssp) from  has changed as a consequence of . Basically, the idea is to put the set  of affected nodes  into a priority queue  with priority  equal to the candidate distance of . When  is extracted, if there is actually a path of length  from  to , the new distance of  is set to , otherwise  is reinserted into  with a higher candidate distance. In both cases, the affected neighbors of  are inserted into . In \upvd,  and  are recomputed while updating the distances and  is updated while scanning .  In \sssp, the number  of shortest paths of  is recomputed as the sum of the  of the new predecessors  of . 

Let  represent the cardinality of  and let  represent the sum of the nodes in  and of the edges that have at least one endpoint in . Then, the following complexity derives from feeding  with the batch and inserting into/extracting from  the affected nodes and their neighbors.
\begin{lemma}
\label{thm:complexity}
The time required by \upvd (\sssp) to update the distances and \vda (the number of shortest paths) is .
\end{lemma}
\paragraph{SSSP update in unweighted graphs.}
For unweighted graphs, we basically replace the priority queue  of \upvd and \sssp with a list of queues, as the one we used in~\cite{DBLP:conf/alenex/BergaminiMS15} for the incremental BFS. Each queue represents a \emph{level} from 0 (which only the source belongs to) to the maximum distance . The levels replace the priorities and also in this case represent the candidate distances for the nodes. In order not to visit a node multiple times, we use colors to distinguish the unvisited nodes from the visited ones. The replacement of the priority queue with the list of queues decreases the complexity of the SSSP update algorithms for unweighted graphs, that we call \upvdu and \ssspu, in analogy with the ones for weighted graphs.
\begin{lemma}
\label{thm:complexity2}
The time required by \upvdu (\ssspu) to update the distances and \vda (the number of shortest paths) is , where  is the maximum distance from  reached during the update.
\end{lemma}
\paragraph{Fully-dynamic \vd approximation.}
The algorithm keeps track of a \vd approximation for the whole graph , \ie for each connected component of . It is composed of two phases. In the initialization, we compute an SSSP from a source node  for each connected component . During the SSSP search from , we also compute a \vd approximation  for , as described in Sections~\ref{sec:rk} and~\ref{sec:new_vd_approx}. In the update, we recompute the SSSPs and the \vd approximations with \upvd (or \upvdu). Since components might split or merge, we might need to compute new approximations, in addition to update the old ones. To do this, for each node, we keep track of the number of times it has been visited. This way we discard source nodes that have already been visited and compute a new approximation for components that have become unvisited. The complexity of the update of the \vd approximation derives from the \vda update in the single components, using \upvd and \upvdu.
\begin{theorem}
\label{thm:complexity_vd}
The time required to update the \vd approximation is  in weighted graphs and  in unweighted graphs, where  is the number of components in  before the update and  is the sum of affected nodes in  and their incident edges.
\end{theorem}
\paragraph{Dynamic BC approximation.}
Let  be an undirected graph with  connected components. Now that we have defined our building blocks, we can outline a fully-dynamic BC algorithm: we use the fully dynamic \vd approximation to recompute \vda after a batch, we update the  sampled paths with \textsf{updateSSSP} and, if \vda (and therefore ) increases, we sample new paths. However, since \textsf{updateSSSP} and \textsf{updateApprVD} share most of the operations, we can ``merge'' them and update at the same time the shortest paths from a source node  and the \vd approximation for the component of . We call such hybrid function \textsf{updateSSSPVD}. Instead of storing and updating  SSSPs for the \vd approximation and  SSSPs for the BC scores, we 
recompute a \vd approximation for each of the  samples while recomputing the shortest paths with \textsf{updateSSSPVD}. This way we do not need to compute an additional SSSP for the components covered by  sampled paths (\ie in which the paths lie), saving time and memory. Only for components that are not covered by any of them (if they exist), we compute and store a separate \vd approximation. We refer to such components as  (and to  as ). 


\begin{algorithm}[h]
\begin{small}
\LinesNumbered
\SetKwData{B}{}
\SetKwFunction{dynamicSSSP}{updateSSSP}
\SetKwFunction{updateApproxVD}{updateApprVD}
\SetKwFunction{updateApproxVDP}{updateSSSPVD}
\SetKwFunction{computeApproxVD}{initApprVD}
\SetKwFunction{replacePath}{replacePath}
\SetKwFunction{sampleNewPaths}{sampleNewPaths}
\SetKwFunction{applyBatch}{applyBatch}
\applyBatch{}\; \label{bc:batch}
\For{ \KwTo }
{\label{bc:update1}
	 \updateApproxVDP{}\;\label{bc:dynsssp}
\replacePath{} \tcc*{update of BC scores}
}\label{bc:update2}
\ForEach{}
{\label{bc:rprime1}
	 \updateApproxVD{}\;
}\label{bc:rprime2}
\ForEach{unvisited }
{\label{bc:unvisited1}
	add  to \;
	 \computeApproxVD{}\;
}\label{bc:unvisited2}
\vda \; \label{bc:recompute1}
\; \label{bc:recompute2}
\If{}
{\label{bc:norm1}
	\sampleNewPaths{} \tcc*{update of BC scores}
	\ForEach{}
	{
		 \label{bc:norm1} \tcc*{renormalization of BC scores}
	}
\;
}\label{bc:norm2}
\Return{}
\end{small}
\caption{BC update after a batch  of edge updates}
\label{bc_overview}
\end{algorithm}
The high-level description of the update after a batch  is shown as Algorithm~\ref{bc_overview}. 
After changing the graph according to  (Line~\ref{bc:batch}), we recompute the previous  samples and the \vd approximations for their components (Lines~\ref{bc:update1}~-~\ref{bc:update2}). Then, similarly to \ia and \iaw, we update the BC scores of the nodes in the old and in the new shortest paths. 
Thus, we update a \vd approximation for the components in  (Lines~\ref{bc:rprime1}~-~\ref{bc:rprime2}) and compute a new approximation for new components that have formed applying the batch (Lines~\ref{bc:unvisited1}~-~\ref{bc:unvisited2}). Then, we use the results to update the number of samples (Lines~\ref{bc:recompute1}~-~\ref{bc:recompute2}). If necessary, we sample additional paths and normalize the BC scores (Lines~\ref{bc:norm1}~-~\ref{bc:norm2}). The difference between \da and \daw is the way the SSSPs and the \vd approximation are updated: in \da we use \upvdu and in \daw \upvd. Differently from \rk and our previous algorithms \ia and \iaw, in \da and \daw we scan the neighbors every time we need the predecessors instead of storing them. This allows us to use  memory per sample (\ie,  in total) instead of  per sample, while our experiments show that the running time is hardly influenced. The number of samples depends on , so in theory this can be as large as . However, the experiments conducted in~\cite{DBLP:conf/alenex/BergaminiMS15} show that relatively large values of  (\eg ) lead to good ranking of nodes with high BC and for such values the number of samples is typically much smaller than , making the memory requirements of our algorithms significantly less demanding than those of the dynamic exact algorithms () for many applications.
 \begin{theorem}
\label{thm:correctness_bc}
Algorithm~\ref{update} preserves the guarantee on the maximum absolute error, \ie naming  and  the new exact and approximated BC values, respectively, .

\end{theorem}
\begin{theorem}
\label{thm:complexity_bc}
Let  be the difference between the value of  before and after the batch and let  be the sum of affected nodes and their incident edges in the -th SSSP. The time required for the BC update in unweighted graphs is 
. 
In weighted graphs, it is .
\end{theorem}
Notice that, if \vda does not increase,  and the complexities are the same as the only-incremental algorithms \textsf{IA} and \textsf{IAW} we proposed in~\cite{DBLP:conf/alenex/BergaminiMS15}. Also, notice that in the worst case the complexity can be as bad as recomputing from scratch. However, no dynamic SSSP (and so probably also no BC approximation) algorithm exists that is faster than recomputation. 


\section{Experiments}
\label{sec:experimental}
\paragraph{Implementation and settings.} We implement our two dynamic approaches \textsf{DA} and \textsf{DAW} in C++, building on the open-source \textit{NetworKit} framework~\cite{DBLP:journals/corr/StaudtSM14}, which also contains
the static approximation \textsf{RK}.
In all experiments we fix  to 0.1 and  to 0.05, as a good tradeoff between running time and accuracy~\cite{DBLP:conf/alenex/BergaminiMS15}. 
This means that, with a probability of at least , the computed BC values deviate at most
 from the exact ones. In our previous experimental study~\cite{DBLP:conf/alenex/BergaminiMS15}, we showed that for such values of  and , the ranking error (how much the ranking computed by the approximation algorithm differs from the rank of the exact algorithm) is low for nodes with high betweenness. Since our algorithms simply update the approximation of \rk, our accuracy in terms or ranking error does not differ from that of \rk (see~\cite{DBLP:conf/alenex/BergaminiMS15} for details). Also, our experiments in~\cite{DBLP:conf/alenex/BergaminiMS15} have shown that dynamic exact algorithms are not scalable, because of both time and memory requirements, therefore we do not include them in our tests.
The machine used has 2 x 8 Intel(R) Xeon(R) E5-2680 cores at 2.7 GHz, of which we use only one core, and 256 GB RAM.
  \vspace{-2ex}
\begin{table*}[t]
\begin{center}
\begin{scriptsize}
  \begin{tabular}{ | l | l | r | r | r |}
    \hline
    Graph 						& Type 				& Nodes 			& Edges 			&  Type\\ \hline
   
    \texttt{repliesDigg}				& communication	& 30,398			& 85,155 			& Weighted	\\
    \texttt{emailSlashdot}			& communication	& 51,083 			& 116,573			& Weighted	\\ 
    \texttt{emailLinux}				& communication	& 63,399 			& 159,996			& Weighted	\\
    \texttt{facebookPosts}		& communication	& 46,952			& 183,412 		& Weighted	\\
    \texttt{emailEnron}				& communication	& 87,273 			& 297,456			& Weighted	\\
    \texttt{facebookFriends}			& friendship		& 63,731			& 817,035 		& Unweighted	\\
    \texttt{arXivCitations} 			& coauthorship 		& 28,093			& 3,148,447		& Unweighted	\\
    \texttt{englishWikipedia}		& hyperlink		& 1,870,709		& 36,532,531 		& Unweighted	\\

    \hline
  \end{tabular}
  \end{scriptsize}
\end{center}
  \caption{Overview of real dynamic graphs used in the experiments.}
  \label{table:graphs}
   \vspace{-4ex}
\end{table*}
\paragraph{Data sets and experiments.} We concentrate on two types of graphs: synthetic and real-world graphs with real edge dynamics. The real-world networks are taken from The Koblenz Network Collection (KONECT)~\cite{DBLP:conf/www/Kunegis13} and are summarized in Table~\ref{table:graphs}. All the edges of the KONECT graphs are characterized by a time of arrival. In case of multiple edges between two nodes, we extract two versions of the graph: one unweighted, where we ignore additional edges, and one weighted, where we replace the set   of edges between two nodes with an edge of weight . 
In our experiments, we let the batch size vary from 1 to 1024 and for each batch size, we average the running times over 10 runs.
Since the networks do not include edge deletions, we implement additional simulated dynamics. In particular, we consider the following experiments. (i) \textit{Real dynamics.} We remove the  edges with the highest timestamp from the network and we insert them back in batches, in the order of timestamps. (ii) \textit{Random insertions and deletions.} We remove  edges from the graph, chosen uniformly at random. To create batches of both edge insertions and deletions, we add back the deleted edges with probability  and delete other random edges with probability . (iii) \textit{Random weight changes.} In weighted networks, we choose  edges uniformly at random and we multiply their weight by a random 
value in the interval .

For synthetic graphs we use a generator based on a unit-disk graph model in hyperbolic geometry~\cite{DBLP:journals/corr/LoozSMP15}, where edge insertions and deletions are obtained by moving the nodes in the hyperbolic plane. The networks produced by the model were shown to have many properties of real complex networks, like small diameter and power-law degree distribution (see~\cite{DBLP:journals/corr/LoozSMP15} and the references therein). We generate seven networks, with  ranging from about  to about  and  approximately equal to .
\paragraph{Speedups.}
\begin{figure}[th]
\begin{center}
\includegraphics[width = 0.8\textwidth]{speedups_colors_Unweighted.pdf}
\caption{Speedups of \da on \rk in real unweighted networks using real dynamics.}
\label{fig:unweighted}
 \vspace{-4ex}
\end{center}
\end{figure} 
 \begin{table*}
\begin{center}
\begin{scriptsize}
  \begin{tabular}{  l | r | r | r | r | r | r | r | r |}
\cline{2-9}  
 & \multicolumn{4}{  c |  }{Real}& \multicolumn{4}{  c |  }{Random} \\ \cline{2-9}
 &\multicolumn{2}{  c |  }{Time [s]}& \multicolumn{2}{  c |  }{Speedups} &\multicolumn{2}{  c |  }{Time [s]}& \multicolumn{2}{  c |  }{Speedups} \\ \cline{1-9}
\multicolumn{1}{| l|}{Graph} &  &  &  &   &  &  &  &  \\\cline{1-9}
\multicolumn{1}{| l|}{\texttt{repliesDigg}} 		& 0.078	& 1.028	& 76.11	& 5.42	& 0.008	& 0.832	& 94.00	& 4.76	\\ \cline{1-9}		
\multicolumn{1}{| l|}{\texttt{emailSlashdot}} 	& 0.043	& 1.055	& 219.02	& 9.91	& 0.038	& 1.151	& 263.89	& 28.81	\\ \cline{1-9}		
\multicolumn{1}{| l|}{\texttt{emailLinux}} 		& 0.049	& 1.412	& 108.28	& 3.59	& 0.051	& 2.144	& 72.73	& 1.33	\\ \cline{1-9}				
\multicolumn{1}{| l|}{\texttt{facebookPosts}} 	& 0.023	& 1.416	& 527.04	& 9.86	& 0.015	& 1.520	& 745.86	& 8.21	\\ \cline{1-9}	
\multicolumn{1}{| l|}{\texttt{emailEnron}} 		& 0.368	& 1.279	& 83.59	& 13.66	& 0.203	& 1.640	& 99.45	& 9.39	\\ \cline{1-9}				
\multicolumn{1}{| l|}{\texttt{facebookFriends}} 	& 0.447	& 1.946	& 94.23	& 18.70	& 0.448	& 2.184	& 95.91	& 18.24	\\ \cline{1-9}			
\multicolumn{1}{| l|}{\texttt{arXivCitations}} 	& 0.038	& 0.186	& 2287.84	& 400.45	& 0.025	& 1.520	& 2188.70	& 28.81	\\ \cline{1-9}			
\multicolumn{1}{| l|}{\texttt{englishWikipedia}} 	& 1.078	& 6.735	& 3226.11	& 617.47	& 0.877	& 5.937	& 2833.57	& 703.18	\\ \cline{1-9}		
  \end{tabular}
  \end{scriptsize}
\end{center}
  \caption{Times and speedups of \da on \rk in unweighted real graphs under real dynamics and random updates, for batch sizes of 1 and 1024.}
  \label{table:speedups}
   \vspace{-6ex}
\end{table*} 

\begin{figure}[htb]
\begin{center}
\includegraphics[width = 0.8\textwidth]{speedups_colors_Hyperbolic.pdf}
\caption{Speedups of \da on \rk in hyperbolic unit-disk graphs.}
\label{fig:hyp}
\end{center}
\end{figure}
Figure~\ref{fig:unweighted} reports the speedups of \da on \rk in real graphs using real dynamics. Although some fluctuations can be noticed, the speedups tend to decrease as the batch size increases. We can attribute fluctuations to two main factors: First, different batches can affect areas of  of varying sizes, influencing also the time required to update the SSSPs. Second, changes in the \vd approximation can require to sample new paths and therefore increase the running time of \da (and \daw). Nevertheless, \da is significantly faster than recomputation on all networks and for every tested batch size. Analogous results are reported in Figure~\ref{fig:random_unweighted} of the Appendix for random dynamics. Table~\ref{table:speedups} summarizes the running times of \da and its speedups on \rk with batches of size 1 and 1024 in unweighted graphs, under both real and random dynamics. Even on the larger graphs (\texttt{arXivCitations} and \texttt{englishWikipedia}) and on large batches, \da requires at most a few seconds to recompute the BC scores, whereas \rk requires about one hour for \texttt{englishWikipedia}. 
The results on weighted graphs are shown in Table~\ref{table:weighted} in Section~\ref{sec:plots} in the Appendix. In both real dynamics and random updates, the speedups vary between  and  for single-edge updates and between  and  for batches of size 1024.
On hyperbolic graphs (Figure~\ref{fig:hyp}), the speedups of \da on \rk increase with the size of the graph. Table~\ref{table:hyperbolic} in the Appendix contains the running times and speedups on batches of 1 and 1024 edges. The speedups vary between  and  for single-edge updates and between  and  for batches of 1024 edges. 
The results show that \da and \daw are faster than recomputation with \rk in all the tested instances, even when large batches of 1024 edges are applied to the graph. With small batches, the algorithms are always orders of magnitude faster than \rk, often with running times of fraction of seconds or seconds compared to minutes or hours. Such high speedups are made possible by the efficient update of the sampled shortest paths, which limit the recomputation to the nodes that are actually affected by the batch. Also, processing the edges in batches, we avoid to update multiple times nodes that are affected by several edges of the batch.

\section{Conclusions}
Betweenness is a widely used centrality measure, yet expensive if computed exactly.
In this paper we have presented the first fully-dynamic algorithms for betweenness approximation  
(for weighted and for unweighted undirected graphs). 
The consideration of edge deletions and disconnected graphs is made possible by the efficient solution
of several algorithmic subproblems (some of which may be of independent interest).
Now BC can be approximated with an error guarantee for a much wider 
set of dynamic real graphs compared to previous work. 


Our experiments show significant speedups over the static algorithm \rk. In this context it is interesting
to remark that dynamic algorithms require to store additional memory and that this can be a limit to the size 
of the graphs they can be applied to. 
By not storing the predecessors in the shortest paths, we reduce the memory requirement from  per 
sampled path to  -- and are still often more than 100 times faster than \rk despite rebuilding the paths.


Future work may include the transfer of our concepts to approximating other centrality measures in a fully-dynamic
manner, \eg closeness, and the extension to directed graphs, for which a good \vd approximation is the only obstacle.
Moreover, making the betweenness code run in parallel will further accelerate the
computations in practice.
Our implementation will be made available as part of a future release of the network analysis tool suite 
\href{http://networkit.iti.kit.edu}{\textit{NetworKit}}~\cite{DBLP:journals/corr/StaudtSM14}.


\bigskip

\renewcommand{\baselinestretch}{0.25}
\begin{scriptsize}
\textbf{Acknowledgements.}
This work is partially supported by DFG grant FINCA (ME-3619/3-1) within the SPP 1736 \emph{Algorithms for Big Data}.
We thank Moritz von Looz for providing the synthetic dynamic networks and the numerous contributors to 
the \textit{NetworKit} project. We also thank Matteo Riondato (Brown University) and anonymous reviewers for their constructive comments.
\end{scriptsize}
\renewcommand{\baselinestretch}{1.0}

\bibliographystyle{abbrv}
\bibliography{references}


\pagebreak
\appendix

\section{Description of the fully-dynamic algorithms}
\label{sec:pseudocodes}

\renewcommand{\floatpagefraction}{0.99}
\subsection{Dynamic \vd approximation}
Algorithm~\ref{algo:vd_approx} describes the initialization. Initially, we put all the nodes in a queue and compute an SSSP from the nodes we extract. During the SSSP search, we mark as visited all the nodes we scan. When extracting the nodes, we skip those that have already been visited: this avoids us to compute multiple approximations for the same component. 
In the update (Algorithm~\ref{algo:dyn_vd_approx}), we recompute the SSSPs and the \vd approximations with \upvd (or \upvdu). Since components might split, we might need to add \vd approximations for some new subcomponents, in addition to recompute the old ones. Also, if components merge, we can discard the superfluous approximations. To do this, we keep track, for each node, of the number of times it has been visited. Let  denote this number for node . Before the update, all the nodes are visited exactly once. While updating an SSSP from , we increase (decrease) by one  of the nodes  that become reachable (unreachable) from . This way we can skip the update of the SSSPs from nodes that have already been visited. After the update, for all nodes  that have become unvisited (), we compute a new \vd approximation from scratch.

\begin{algorithm}[h]
 \begin{footnotesize}
\LinesNumbered
\SetKwFunction{initDynamicSSSP}{initApprVD}
\;
\ForEach{node }
{
		; insert  into \; \label{initvd:queue}
}
\;
\While{}
{\label{initvd:newsamples1}
	extract  from \;
	\If{}
	{
		 \;
		 \tcp{\initDynamicSSSP adds 1 to  of the nodes it visits}
		  \initDynamicSSSP{}\;
		\;
	}
}\label{initvd:newsamples2}
\;
\vda \;
\Return{\vda}
\end{footnotesize}
\caption{Dynamic \vd approximation (initialization)}
\label{algo:vd_approx}
\end{algorithm}

\begin{algorithm}[h]
 \begin{footnotesize}
\LinesNumbered
\SetKwFunction{dynamicSSSP}{updateApprVD}
\SetKwFunction{initDynamicSSSP}{initApprVD}
\; 
\ForEach{}
{\label{vdup:loop1}
	\If{vis(}
	{	
		remove  and ; decrease \; \label{vdup:skip}
	}
	\Else{
		 \tcp{\dynamicSSSP updates , inserts all  for which  into  and computes a \vd approximation }
		   \dynamicSSSP{} \;
	}
}\label{vdup:loop2}
 \;
\While{}
{\label{vdup:ext1}
	extract  from \; \label{vdup:extract}
	\If{}
	{
		 \;
		  \initDynamicSSSP{}\;
		; \;
	}
}\label{vdup:ext2}
reset  to 1 for nodes  such that \;
\vda \;
\Return{\vda}
\end{footnotesize}
\caption{Dynamic \vd approximation (\textsf{updateApprVD})}
\label{algo:dyn_vd_approx}
\end{algorithm}

\subsection{Dynamic SSSP update for weighted graphs}
Algorithm~\ref{algo:weighted} describes the SSSP update for weighted graphs. The pseudocode updates both the \vd approximation for the connected component of  and the number of shortest paths from , so it basically includes both \sssp and \upvd.
Initially, we scan the edges  in  and, for each , we insert the endpoint with greater distance from  into  (w.l.o.g., let  be such endpoint). The priority  of  represents the \textit{candidate} new distance of . This is the minimum between the  and  plus the weight of the edge . Notice that we use the expression "insert  into " for simplicity, but this can also mean update  if  is already in  and the new priority is smaller than . 
When we extract a node  from , we have two possibilities: (i) there is a path of length  and  is actually the new distance or (ii) there is no path of length  and the new distance is greater than . In the first case (Lines~\ref{update_weighted:part1-1}~-~\ref{update_weighted:part1-2}), we set  to  and insert the neighbors  of  such that  into  (to check if new shorter paths to  that go through  exist). In the second case (Lines~\ref{update_weighted:part2-1}~-~\ref{update_weighted:part2-2}), we assume there is no shortest path between  and  anymore, setting  to . We compute  as  (the new candidate distance for ) and insert  into . Also its neighbors could have lost one (or all of) their old shortest paths, so we insert them into  as well. The update of  can be done while scanning the batch and of  and  when we update . When updating , we also increase  in case the old  was equal to  (\ie w has become reachable) and we decrease  when we set  to  (\ie  has become unreachable). We update the number of shortest paths after updating , as the sum of the shortest paths of the predecessors of  (Lines~\ref{update_weighted:sp1}~-~\ref{update_weighted:sp2}).
\begin{algorithm}[H]
\begin{footnotesize}
\LinesNumbered
\SetKwFunction{insertOrDecreaseKey}{insertOrDecreaseKey}
\SetKwFunction{extractMin}{extractMin}
 empty priority queue\;
\ForEach{}
{\label{update_weighted:init1}
	 \insertOrDecreaseKey{}\;
}\label{update_weighted:init2}
\;
\While{there are nodes in }
{
		 \extractMin{}\;
		\;
		\If{}
		{\label{update_weighted:part1-1}
			update  and \;
			\If{} 
			{
				\; \label{update_weighted:visinc}
			}
			; \;
			\ForEach{incident edge }
			{
				\If{}
				{\label{update_weighted:sp1}
					\;
				}\label{update_weighted:sp2}
				\If{}
				{\label{update_w:neigh1}
					 \insertOrDecreaseKey{}\;
				}\label{update_w:neigh2}
			}
		}\label{update_weighted:part1-2}
		\Else
		{\label{update_weighted:part2-1}
			\If{}
			{
				\; \label{update_weighted:visdec}
				\If{vis(w)=0}{
				insert  into \;
				}
				\If{}
				{
					\insertOrDecreaseKey{}\;
					\ForEach{incident edge }
					{
						\If{}
						{
							\insertOrDecreaseKey{}\;
						}
					}
					\;
				}
			}
		}\label{update_weighted:part2-2}
}
\end{footnotesize} 
\caption{SSSP update for weighted graphs (\textsf{updateSSSP-W})}
\label{algo:weighted}
\end{algorithm}
 \vspace{4ex}
\begin{algorithm}[H]
\begin{footnotesize}
\LinesNumbered
\textbf{Assumption:} \;
 array of empty queues\;
\ForEach{}
{\label{update:batch1}
	; enqueue \;
}\label{update:batch2}
\; \label{update:queues1}
\While{there are nodes in }
{\label{lst2:line:start2}
	\While{}
	{
		dequeue \;
		\textbf{if}  \textbf{then}			continue\; \label{update:black2}
	
		\;
		\If{}
		{\label{update:if}
			update  and \;
			\textbf{if}   \textbf{then}   \; \label{update:vis}
			; ; \; \label{update:black1}
			\ForEach{incident edge }
			{
				\If{}
				{\label{update:sp}
					\;
				}\label{update:sp2}
				\If{}
				{\label{update:neigh1}
					enqueue \;
				}\label{update:neigh2}
			}
		}\label{update:if2}
		\Else
		{\label{update:else}
			\If{}
			{
				\;
				\; \label{update:vis3}
				\If{vis(w)=0}{
				insert  into \;
				}
				\If{}
				{
					enqueue \;
					\ForEach{incident edge }
					{
						\If{}
						{
							enqueue \;
						}
					}
				}
			}
		}\label{update:else2}
	}
	\;
}\label{update:queues2}
Set to white all the nodes that have been in \;
\end{footnotesize}
\caption{SSSP update for unweighted graphs (\textsf{updateSSSP-U})}
\label{algo:unweighted}
\end{algorithm}
\subsection{Dynamic SSSP update for unweighted graphs}
Algorithm~\ref{algo:unweighted} shows the pseudocode. As in Algorithm~\ref{algo:weighted}, we
first scan the batch (Lines~\ref{update:batch1} -~\ref{update:batch2}) and insert the nodes
in the queues. Then (Lines~\ref{update:queues1} -~\ref{update:queues2}), we scan the queues in order of increasing distance from , in a fashion similar to that of a priority queue. 
In order not to insert a node in the queues multiple times, we use colors: Initially we set all the nodes to white and then we set a node  to black only when we find the final distance of  (\ie when we set  to ) (Line~\ref{update:black1}). Black nodes extracted from a queue are then skipped (Line~\ref{update:black2}). At the end we reset all nodes to white.

\subsection{Fully-dynamic BC approximation} 
\label{sec:fully_dyn_bc}
Similarly to \textsf{IA} and \textsf{IAW}, we replace the  sampled paths between vertex pairs  with new shortest paths between the same vertex pairs. However, here we also check whether  (and consequently the number  of samples) has increased after the batch of edge updates. If so, we sample additional paths (computing new SSSPs from scratch) according to the new value of . Instead of updating  and then the paths in two successive steps, we use the SSSPs from the  source nodes  to compute and update also , computing new SSSPs only for the components that are not covered by any of the source nodes. In the initialization (Algorithm~\ref{algo:init}), we first compute the  SSSP, like in \rk (Lines~\ref{init:sampling1}~-~\ref{init:sampling2}). However, we also check which nodes have been visited, as in Algorithm~\ref{algo:vd_approx}. While we compute the  SSSPs, in addition to the distances and number of shortest paths, we also compute a \vd approximation for each of the  source nodes and increase  of all the nodes we visit during the sources with \textsf{initSSSPVD} (Line~\ref{init:dynsssp}). Since it is possible that the  shortest paths do not cover all the components of , we compute an additional VD approximation for nodes in the unvisited components, like in Algorithm~\ref{algo:vd_approx} (Lines~\ref{init:newsamples1}~-~\ref{init:newsamples2}). Basically we can divide the SSSPs into two sets: the set  of SSSPs used to compute the  shortest paths and the set  of SSSPs used for a \vd approximation in the components that were not scanned by the initial  SSSPs. We call  the number of the SSSPs in . The BC update after a batch is described in Algorithm~\ref{update}. First (Lines~\ref{up:update1} -~\ref{up:update2}), we recompute the shortest paths like in our incremental algorithms \textsf{IA} and \textsf{IAW}~\cite{DBLP:conf/alenex/BergaminiMS15}: we update the SSSPs from each source node  in  and we replace the old shortest path with a new one (subtracting  to the nodes in the old shortest path and adding  to those in the new shortest path). Notice that here we do not store the predecessors so we need to recompute them (Lines~\ref{up:pred1} and~\ref{up:pred2}).
Instead of using an incremental SSSP algorithm like in \textsf{IA}-\textsf{IAW}, here we use the fully-dynamic \textsf{updateSSSPVD} that updates also the \vd approximation and updates and keeps track of the nodes that become unvisited.
Then (Lines~\ref{up:ext1} -~\ref{up:ext2}), we add a new SSSP to  for each component that has become unvisited (by both  and ). After this, we have at least a \vd approximation for each component of . We take the maximum over all these approximations and recompute the number of samples  (Lines~\ref{up:recompute1} -~\ref{up:recompute2}). If  has increased, we need to sample new paths and therefore new SSSPs to add to . Finally, we normalize the BC scores, \ie we multiply them by the old value of  divided by the new value of  (Line~\ref{up:norm}).


\begin{algorithm}[h]
 \begin{footnotesize}
\LinesNumbered
\SetKwData{B}{}\SetKwData{VD}{VD}
\SetKwFunction{getVertexDiameter}{getApproxVertexDiameter}
\SetKwFunction{sampleUniformNodePair}{sampleUniformNodePair}
\SetKwFunction{computeExtendedSSSP}{computeExtendedSSSP}
\SetKwFunction{initDynamicSSSP}{initApprVD}
\SetKwFunction{initSSSPVD}{initSSSPVD}
\ForEach{node }
{
	\B; \; \label{init:init}
}
\vda\getVertexDiameter{}\; \label{init:sampling1}
\;

\For{ \KwTo }{
	\label{sampling1}
	 \sampleUniformNodePair{}\;
	 \initSSSPVD{}\; \label{init:dynsssp}
	\;\label{paths1}
	 empty list\;
	\;
	\While{}
	{
		\mbox{sample  with probability }\;
		\;
		add ;
		\;
		\;
	}\label{paths2}
}\label{sampling2} \label{init:sampling2}
\;
\;
\While{}
{\label{init:newsamples1}
	extract  from \;
	\If{}
	{
		 \;
		 \initDynamicSSSP{}\;
		\;
	}
}\label{init:newsamples2}
\;
\Return{}
\end{footnotesize}
\caption{BC initialization}
\label{algo:init}
\end{algorithm}


\begin{algorithm}[H]
\begin{footnotesize}
\LinesNumbered
\SetKwData{B}{}
\SetKwFunction{dynamicSSSP}{updateApprVD}
\SetKwFunction{dynamicSSSPVD}{updateSSSPVD}
\SetKwFunction{initDynamicSSSP}{initApprVD}
\;
\For{ \KwTo }
{\label{up:update1}
	\;
	\;
	 \tcp{\dynamicSSSPVD updates , inserts all  into  and updates the \vd approximation}
	 \dynamicSSSPVD{}\; \label{up:dynsssp}
\tcp { we replace the shortest path between  and }
\ForEach{}
			{
				\B() \; \label{up:sub}
			}
			\;
			 empty list\;
			\;\label{up:pred1}
			\While{}
			{\label{up:new1}
				\mbox{sample  with probability }\;
				\;
				add  to \;
				\;
				\;\label{up:pred2}
			}\label{up:new2}
}\label{up:update2}
\For{ \KwTo }
{\label{up:update3}
		 \dynamicSSSP{}\;
}\label{up:update4}
\;\label{ext1}
\While{}
{\label{up:ext1}
	extract  from \;
	\If{}
	{
		 \;
		 \initDynamicSSSP{}\;
		; \;
	}
}\label{up:ext2}
\tcp{compute the maximum over all the  computed by \dynamicSSSP}
\vda \; \label{up:recompute1}
\; \label{up:recompute2}
\If{}
{
	sample new paths\; \label{up:new_paths}
	\ForEach{}
	{
		 \label{up:norm}
	}
\;
}\label{ext4}
\Return{}
\end{footnotesize}
\caption{Dynamic update of BC approximation (\textsf{DA})}
\label{update}
\end{algorithm}

\section{Omitted proofs}
\label{sec:proofs}
\subsection{Proof of Proposition~\ref{lem:vd2}}
\label{sub:proof_vd2}
\begin{proof}
To prove the first inequality, we can notice that  for all , since all the edges of  are contained in those of . Also, since every edge has weight at least , . Therefore, , which can be rewritten as , for all . Thus, , where the last expression equals \vda by definition.

To prove the second inequality, we first notice that ,
 and analogously . Consequently, , supposing that  without loss of generality. By definition of \vd, . Therefore, .
 \qed
\end{proof}

\subsection{Proof of Lemma~\ref{thm:complexity}}
\label{sub:proof_complexity}
\begin{proof}
In the initial scan of the batch (Lines~\ref{update_weighted:init1}-\ref{update_weighted:init2}), we scan the nodes of the batch and insert the affected nodes into  (or update their value). This requires at most one heap operation (insert or decrease-key) for each element of , therefore  time.
When we extract a node  from , we have two possibilities: (i)  (Lines~\ref{update_weighted:part1-1}~-~\ref{update_weighted:part1-2}) or (ii)  (Lines~\ref{update_weighted:part2-1}~-~\ref{update_weighted:part2-2}). In the first case, we scan the neighbors of  and perform at most one heap operation for each of them (Lines~\ref{update_w:neigh1}~-~\ref{update_w:neigh2}). In the second case, this happens only if . Therefore, we can perform up to one heap operation per incident edge of , for each extraction of  in which  or .
How many times can an affected node  be extracted from  with  or ? If the first time we extract ,  is equal to  (case (i)), then the final value of  is reached and  is not inserted into  anymore. If the first time we extract ,  is greater than  (case (ii)),  can be inserted into the queue again. However, his distance is set to  and therefore no additional operations are performed, until  becomes less than . But this can happen only in case (i), after which  reaches its final value. To summarize, each affected node  can be extracted from  with  or  at most twice and, every time this happens, at most one heap operation per incident edge of  is performed. The complexity is therefore . \qed
\end{proof}

\subsection{Proof of Lemma~\ref{thm:complexity2}}
\label{sub:proof_complexity2}
\begin{proof}
The complexity of the initialization (Lines~\ref{update:batch1}~-~\ref{update:batch2}) of Algorithm~\ref{algo:unweighted}
is , as we have to scan the batch. In the main loop (Lines~\ref{update:queues1}~-~\ref{update:queues2}), we scan all the list
of queues, whose final size is . 
Every time we extract a node  whose color is not black, we scan all the 
incident edges, therefore this operation is linear in the number of neighbors of . 
If the first time we extract  (say at level )  is equal to , then  will be set to black and will not be scanned anymore.
If the first time we extract ,  is instead greater than ,  will be inserted into the queue at level  (if ). 
Also, other inconsistent neighbors of  might insert  in one of the queues. However, after the first time  is extracted,
its distance is set to , so its neighbors will not be scanned unless , in which case 
they will be scanned again, but for the last time, since  will be set to black. To summarize, each affected node and its neighbors can 
be scanned at most twice. The complexity of the
algorithm is therefore . \qed
\end{proof}

\subsection{Correctness of Algorithm~\ref{algo:vd_approx} and Algorithm~\ref{algo:dyn_vd_approx}}
\begin{lemma}
\label{thm:vd_correctness}
At the end of Algorithm~\ref{algo:vd_approx},  and exactly one \vd approximation is computed for each connected component of .
\end{lemma}
\begin{proof}
Let  be any node. Then  must be scanned by \emph{at least} one source node  in the while loop (Lines~\ref{initvd:newsamples1}~-~\ref{initvd:newsamples2}): In fact, either  is visited by some  before  is extracted from , or  at the moment of the extraction and  becomes a source node itself. This implies that .
On the other hand,  cannot be greater than . In fact, let us assume by contradiction that . This means that there are at least two source nodes  and  (, w.l.o.g.) that are in the same connected component as . Then also  and  are in the same connected component and  is visited during the SSSP search from . Then  before  is extracted from  and  cannot be a source node.
Therefore,  is exactly equal to 1 for each , which means that exactly one \vd approximation is computed for the connected component of each , \ie exactly one \vd approximation is computed for each connected component of .
 \qed
\end{proof}

\begin{lemma}
\label{thm:dyn_vd_correctness}
Let  be the set of connected components of  after the update. Algorithm~\ref{algo:dyn_vd_approx} updates or computes exactly one \vd approximation for each .
\end{lemma}
\begin{proof}
Let  be the set of connected components before the update. Let us consider three basic cases (then it is straightforward to see that the proof holds also for combinations of these cases): (i)  is also a component of , (ii)  and  merge into one component  of , (iii)  splits into two components  and  of . In case (i), the \vd approximation of  is updated exactly once in the for loop (Lines~\ref{vdup:loop1}~-~\ref{vdup:loop2}). In case (ii), (assuming , w.l.o.g.) the \vd approximation of  is updated in the for loop from the source node . In its SSSP search,  visits also , increasing . Therefore,  is skipped and exactly one \vd approximation is computed for . In case (iii), the source node  belongs to one of the components (say ) after the update. During the for loop, the \vd approximation is computed for  via . Also, for all the nodes  in ,  is set to 0 and  is inserted into . Then some source node  must be extracted from  in Line~\ref{vdup:extract} and a \vd approximation is computed for . Since all the nodes in  are set to visited during the search, no other \vd approximations are computed for .
 \qed
\end{proof}



\subsection{Proof of Theorem~\ref{thm:complexity_vd}}
\label{sub:proof_complexity_vd}
\begin{proof}
In the first part (Lines~\ref{vdup:loop1}~-~\ref{vdup:loop2} of Algorithm~\ref{algo:dyn_vd_approx}), we update an SSSP with \upvd or \upvdu for each source node  such that  is not greater than 1. Therefore the complexity of the first part is  in weighted graphs and  in unweighted, for Lemmas~\ref{thm:complexity} and~\ref{thm:complexity2}. Only some of the affected nodes (those whose distance from a source node becomes equal to ) are inserted into the queue . Therefore the cost of scanning  in Lines~\ref{vdup:ext1}~-~\ref{vdup:ext2} is . New SSSP searches are computed for new components that are not covered by the existing source nodes anymore. However, also such searches involve only the affected nodes and each affected node (and its incident edges) is scanned at most once during the search. Therefore, the total cost is  for weighted graphs and  for unweighted graphs. \qed
\end{proof}

\subsection{Proof of Theorem~\ref{thm:correctness_bc}}
\label{sub:proof_correctness_bc}
\begin{proof}
Let  be the old graph and  the modified graph after the batch of edge updates. Let  be a shortest path of  between nodes  and
. To prove the theoretical guarantee, we need to prove that the probability of any sampled path  is equal to  (\ie that the algorithms adds  to the nodes in ) is .
Algorithm~\ref{update} replaces the first  shortest paths with other shortest paths  between the same node pairs (Lines~\ref{up:new1}~-~\ref{up:new2}) using Algorithm 4.1 of~\cite{DBLP:conf/alenex/BergaminiMS15}, for which it was already proven that ~\cite[Theorem 4.1]{DBLP:conf/alenex/BergaminiMS15}. The additional  shortest paths (Line~\ref{up:new_paths}) are recomputed from scratch with \rk, therefore also in this case  by Lemma 7 of~\cite{DBLP:conf/wsdm/RiondatoK14}.
 \qed
\end{proof}

\subsection{Proof of Theorem~\ref{thm:complexity_bc}}
\label{sub:proof_complexity_bc}
\begin{proof}
Let  be the difference between the values of  before and after the batch. Let us start from the simplest case: the graph  is such that there is (before and after the update) one sample in each component and \vd does not increase after the update. This case includes, for example, connected graphs subject to a batch of only edge insertions, or any batch that neither splits the graph into more components nor increases \vd. In this case,  and  and we only need to update the  old shortest paths. Then, the total complexity is , where  is the set of nodes affected in the th SSSP, and  is the maximum distance in the th SSSP. In general graphs, we might need to sample new paths for the betweenness approximation () and/or sample paths in new components that are not covered by any of the sampled paths (). Then, the complexity for the betweenness approximation update is . The \vd update requires  to update the \vd approximation in the already covered components and  for the new ones, where  and  are nodes and edges of the th component, respectively. \qed
\end{proof}


\section{Additional Experimental Results}
 \vspace{-4ex}
\label{sec:plots}
\begin{figure}[h]
 \vspace{-2ex}
\begin{center}
\includegraphics[width = 0.8\textwidth]{speedups_colors_Random.pdf}
\caption{Speedups on \rk in real unweighted graphs under random updates.}
\label{fig:random_unweighted}
\end{center}
 \vspace{-2ex}
\end{figure}
  \vspace{-2ex}
 \begin{table*}[h]
  \vspace{-4ex}
\begin{center}
\begin{scriptsize}
  \begin{tabular}{  l | r | r | r | r | r | r | r | r |}
\cline{2-9}  
 & \multicolumn{4}{  c |  }{Real}& \multicolumn{4}{  c |  }{Random} \\ \cline{2-9}
 &\multicolumn{2}{  c |  }{Time [s]}& \multicolumn{2}{  c |  }{Speedups} &\multicolumn{2}{  c |  }{Time [s]}& \multicolumn{2}{  c |  }{Speedups} \\ \cline{1-9}
\multicolumn{1}{| l|}{Graph} &  &  &  &   &  &  &  &  \\\cline{1-9}
\multicolumn{1}{| l|}{\texttt{repliesDigg}} 		& 0.053	& 3.032	& 605.18	& 14.24	& 0.049	& 3.046	& 658.19	& 14.17	\\ \cline{1-9}		
\multicolumn{1}{| l|}{\texttt{emailSlashdot}} 	& 0.790	& 5.387	& 50.81	& 16.12	& 0.716	& 5.866	& 56.00	& 14.81	\\ \cline{1-9}		
\multicolumn{1}{| l|}{\texttt{emailLinux}} 		& 0.324	& 24.816	& 5780.49	& 75.40	& 0.344	& 24.857	& 5454.10	& 75.28	\\ \cline{1-9}				
\multicolumn{1}{| l|}{\texttt{facebookPosts}} 	& 0.029	& 6.672	&  2863.83& 11.42	& 0.029	& 6.534	& 2910.33	& 11.66	\\ \cline{1-9}	
\multicolumn{1}{| l|}{\texttt{emailEnron}} 		& 0.050	& 9.926	& 3486.99	& 24.91	& 0.046	& 50.425	& 3762.09	& 4.90	\\ \cline{1-9}					
  \end{tabular}
  \end{scriptsize}
\end{center}
  \caption{Times and speedups of \daw on \rk in weighted real graphs under real dynamics and random updates, for batch sizes of 1 and 1024.}
  \label{table:weighted}
  \vspace{-4ex}
\end{table*} 
  \vspace{-2ex}
 \begin{table*}[h]
   \vspace{-4ex}
\begin{center}
\begin{scriptsize}
  \begin{tabular}{  l | r | r | r | r |}
\cline{2-5}  
 & \multicolumn{4}{  c |  }{Hyperbolic} \\ \cline{2-5}
 &\multicolumn{2}{  c |  }{Time [s]}& \multicolumn{2}{  c |  }{Speedups} \\ \cline{1-5}
\multicolumn{1}{| l|}{Number of edges} &  &  &  &  \\\cline{1-5}
\multicolumn{1}{| l|}{} 				& 0.005	& 0.195	& 99.83	& 2.79	\\ \cline{1-5}		
\multicolumn{1}{| l|}{} 				& 0.002	& 0.152	& 611.17	& 10.21	\\ \cline{1-5}
\multicolumn{1}{| l|}{} 				& 0.015	& 0.288	& 422.81	& 22.64	\\ \cline{1-5}		
\multicolumn{1}{| l|}{} 				& 0.012	& 0.339	& 1565.12	& 51.97	\\ \cline{1-5}		
\multicolumn{1}{| l|}{} 			& 0.049	& 0.498	& 2419.81	& 241.17	\\ \cline{1-5}		
\multicolumn{1}{| l|}{} 			& 0.083	& 0.660	& 4716.84	& 601.85	\\ \cline{1-5}			
\multicolumn{1}{| l|}{} 			& 0.006	& 0.401	& 304338.86	& 5296.78	\\ \cline{1-5}							
  \end{tabular}
  \end{scriptsize}
\end{center}
  \caption{Times and speedups of \da on \rk in hyperbolic unit-disk graphs, for batch sizes of 1 and 1024.}
  \label{table:hyperbolic}
\end{table*} 

\end{document}
