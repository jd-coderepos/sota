\documentclass{article}

\PassOptionsToPackage{square,numbers,compress,sort}{natbib}






\usepackage[final]{neurips_data_2021}





\newcommand{\TODO}[1]{\textcolor{red}{#1}}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{comment}
\usepackage{multirow}



\graphicspath{{neurips_data_2021}}

\title{A realistic approach to generate masked faces applied on two novel masked face recognition data sets}



\author{Tudor~Mare \\
SecurifAI\\
  \texttt{tudor.mare@securifai.ro} \\
\And
  Georgian~Duta\\
  SecurifAI\\
  \texttt{georgian.duta@securifai.ro} \\
  \And
  Mariana-Iuliana~Georgescu \\
  SecurifAI\\
  University of Bucharest\\
  \texttt{georgescu\_lily@yahoo.com} \\
  \And
  Adrian~Sandru \\
  SecurifAI\\
  \texttt{adrian.sandru@securifai.ro} \\
  \And
  Bogdan~Alexe\\
  SecurifAI\\
  University of Bucharest\\
  \texttt{bogdan.alexe@fmi.unibuc.ro} \\
  \And
  Marius~Popescu\\
  SecurifAI\\
  University of Bucharest\\
  \texttt{popescunmarius@gmail.com} \\
  \And
  Radu~Tudor~Ionescu\\
  SecurifAI\\
  University of Bucharest\\
  \texttt{raducu.ionescu@gmail.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
The COVID-19 pandemic raises the problem of adapting face recognition systems to the new reality, where people may wear surgical masks to cover their noses and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for training these systems were released before the pandemic, so they now seem unsuited due to the lack of examples of people wearing masks. We propose a method for enhancing data sets containing faces without masks by creating synthetic masks and overlaying them on faces in the original images. Our method relies on SparkAR Studio, a developer program made by Facebook that is used to create Instagram face filters. In our approach, we use 9 masks of different colors, shapes and fabrics. We employ our method to generate a number of 445,446 (90\%) samples of masks for the CASIA-WebFace data set and 196,254 (96.8\%) masks for the CelebA data set, releasing the mask images at {\small \url{https://github.com/securifai/masked_faces}}. We show that our method produces significantly more realistic training examples of masks overlaid on faces by asking volunteers to qualitatively compare it to other methods or data sets designed for the same task. We also demonstrate the usefulness of our method by evaluating state-of-the-art face recognition systems (FaceNet, VGG-face, ArcFace) trained on our enhanced data sets and showing that they outperform equivalent systems trained on original data sets (containing faces without masks) or competing data sets (containing masks generated by related methods), when the test benchmarks contain masked faces.
\end{abstract}

\section{Introduction}

State-of-the-art systems~\cite{Schroff15CVPR,Deng19CVPR,Parkhi15BMVC,Liu2017CVPR} for face recognition, verification or clustering have achieved impressive results in recent years. These systems are trained on data sets whose dimensions have rapidly increased in the last years. Indeed, the number of images of faces of different identities has grown from a few thousands in 2007 (e.g., LFW~\cite{Huang07Tech} contains $\sim\!13,000$ images) to hundreds of thousands in 2015 (e.g., CASIA-WebFace~\cite{yi2014learning} and CelebA~\cite{Liu2015ICCV} contain $\sim\!500,000$ images and $\sim\!200,000$ images, respectively) or even millions of images in 2021 (e.g., WebFace260M~\cite{Zhu2021CVPR} and WebFace42M~\cite{Zhu2021CVPR} have $\sim\!260$ million and $\sim\!42$ million images, respectively). All these data sets have been released in the past 15 years, with much effort being put into collecting and cleaning them. Remarkably, some recognition models perform so well that they can even spot errors in the annotations (see \cite{Schroff15CVPR}). However, these models are trained based on the well-known machine learning assumption that training and testing examples follow the same distribution. With the COVID-19 pandemic hitting the entire world in early 2020, we have witnessed a sharp change in the way people across the globe live. People started to wear masks to cover their noses and mouths, keeping them safe from getting infected with the SARS-CoV-2 virus. Hence, the new safety measures generated a change in the distribution of face appearances, which is no longer the same as the one before the pandemic. Thus, the practical use in real scenarios of face recognition systems trained on data lacking masked faces is currently questioned. Intuitively, the discriminative power of such recognition systems, which might reside in analyzing some primary features localized around the mouth, nose or cheeks, could significantly degrade as these regions are now masked. Face masks vary in shape, color, texture or fabrics, and convey almost no information about the covered face.

The success of modern algorithms for face recognition relies heavily on training complex deep learning models~\cite{Schroff15CVPR,Deng19CVPR,Liu2017CVPR} on large-scale data sets~\cite{Liu2017CVPR,yi2014learning,Liu2015ICCV}. Unfortunately, current data sets \cite{wang2020arxiv,anwar2020arxiv,deng2021arxiv,zhu2021arxiv} with masked faces are a few orders of magnitude smaller than the ones containing regular faces, without masks. To the best of our knowledge, the largest data sets~\cite{wang2020arxiv,deng2021arxiv} containing masked faces contain less than $10,000$ examples. 

We address the lack of data sets containing images with masked faces by proposing a fully automatic method to enhance data sets containing faces without masks. Our method relies on SparkAR Studio~\cite{spark-ar-studio}, a developer program made by Facebook that is typically used to create Instagram face filters. We use SparkAR Studio to create synthetic masks and overlay them on faces in the original images. In our approach, we consider $9$ different types of masks of various colors, shapes and textures (see Figure~\ref{fig:mask-database}). Our approach can be run on any data set containing images of faces without masks. In particular, we have applied our method for generating images with synthesized masks for the CelebA~\cite{Liu2015ICCV} and CASIA-WebFace~\cite{yi2014learning} data sets. We release the images with mask overlays at {\small \url{https://github.com/securifai/masked_faces}} along with a script that superimposes the masks over the original CelebA and CASIA-WebFace images. 

We conduct a human evaluation study for which we designed a protocol to assess how realistic the generated masks look, comparing the output of different methods that synthetically generate masks and overlay them on faces. The results show that our method produces significantly more realistic images with synthesized masked faces when compared to competing methods~\cite{wang2020arxiv,anwar2020arxiv,huang2021icassp}. 
We present an in-depth analysis in Section~\ref{sec:comparison}. 

Our enhanced data sets (containing masked faces) can be used in training state-of-the-art face recognition systems in order to capture the new distribution in appearance of masked faces. We show that face recognition systems trained on the enhanced data sets attain superior performance in the matter of analyzing images with masked faces. Moreover, we present empirical evidence indicating that, when it comes to recognizing faces covered by real masks, our synthetically generated masks provide superior results compared with a baseline based on blacking out pixels, as well as the most realistic mask generation competitor~\cite{anwar2020arxiv}.

To summarize, the main contributions of our paper are the following: 
\begin{itemize}
    \item We propose an automatic method for synthesizing masks, applying a series of tools and algorithms (SparkAR Studio, Dlib-ml, ColorMatcher, custom verification scripts) to ensure the realism of the generated masks.
    \item We release two enhanced data sets containing a total of $\sim\!640,000$ examples of masked faces.
    \item We qualitatively compare the output of our method with the output of several competing methods in terms of how realistic the generated masked faces look.
    \item We show that state-of-the-art face recognition systems can perform better in recognizing people with masked faces when trained on our enhanced data sets.
\end{itemize}



\begin{figure}[t] 
\begin{center}
   \includegraphics[width=1\linewidth]{3x3_grid_mask_textures.png}
\end{center}
\vspace{-0.3cm}
   \caption{{\bf Mask database. }\emph {Our mask database contains 9 masks that vary in color, shape and texture. Best viewed in color.}}
\label{fig:mask-database} 
\end{figure}

\section{Related work}

The masked face recognition task is quite new in literature as it emerged as a natural problem to be solved after the COVID-19 pandemic broke out. 
Publicly available data sets containing real masked faces are rare~\cite{wang2020arxiv,anwar2020arxiv,zhu2021arxiv} and their sizes are usually by a few orders of magnitude smaller than the large-scale data sets with regular faces. 
This is due to the complex pipeline of face pre-processing, cleaning the data set, and also carefully selecting images containing real masked faces such that the distribution of collected examples reflects the diversity in race, ethnicity, gender, age, pose, mask types, and so on.

The work of Wang et al.~\cite{wang2020arxiv} was one of the earliest to propose a data set with real masked faces. Their data set, entitled the Real-World Masked Face Recognition Dataset (RMFRD), includes $5,000$ images of $525$ subjects wearing masks and $90,000$ images of the same $525$ subjects without masks. 
Anwar et al.~\cite{anwar2020arxiv} proposed the MFR2 data set, which contains a total of $269$ images with $53$ identities of celebrities and politicians collected from the Internet. Each identity has $5$ images on average, with both masked and unmasked faces. 
The authors of~\cite{ding2020icm} employed several data sets for testing, among them introducing the PKU-Masked-Face data set. This data set contains $10,301$ face images of $1,018$ different identities. For each identity, there are $5$ images with regular faces and $5$ images with masked faces with various orientations, lighting conditions and mask types. 
The Masked Face Recognition Challenge and Workshop organized in conjunction with ICCV 2021 proposes several benchmarks~\cite{deng2021arxiv,zhu2021arxiv} for deep face recognition methods under the existence of facial masks. In the InsightFace track~\cite{deng2021arxiv}, the organizers provided a test set with $6,964$ real masked facial images and $13,928$ non-masked facial images of $6,964$ identities. However, in order to avoid data privacy problems, the test set is not publicly available yet. In the WebFace track~\cite{zhu2021arxiv}, the organizers provided a test benchmark of a total of $60,926$ images with masked and unmasked faces. However, there are only $3,211$ images containing masked faces of $862$ identities. 




With algorithms relying heavily on large-scale data sets for training and with the lack of publicly available data sets with real masked faces, researchers have tried to find alternative ways for solving this issue. A simple alternative is to synthetically generate masks and overlay them on faces. This approach holds the promise of producing large-scale synthesized masked face data sets based on automatic algorithms. 
Wang et al.~\cite{wang2020arxiv} proposed a simple algorithm based on the Dlib-ml library~\cite{dlib-ml} for overlaying synthesized masks on faces. They generated the Simulated Masked Face Recognition Dataset (SMFRD) containing $500,000$ faces of $10,000$ subjects, by applying their algorithm on the LFW~\cite{Huang07Tech} and CASIA-WebFace~\cite{yi2014learning} data sets. 
Anwar et al.~\cite{anwar2020arxiv} proposed the open-source tool entitled MaskTheFace to mask faces in images. MaskTheFace relies on the face landmark detector from the same library, Dlib-ml, in order to estimate the face tilt and identify six key facial features needed for applying the mask. The face tilt is used to select a template mask with a pre-defined orientation, from a database of $7$ mask types, $3$ orientations and $27$ textures. The six key features of the face are needed at aligning the selected mask such that it fits on the face. 
The work of Huang et al.~\cite{huang2021icassp} introduced Webface-OCC, a simulated occluded face recognition data set with $804,704$ face images of $10,575$ subjects. This data set is built on the CASIA-WebFace data set, combining faces with simulated occlusions  with their original unmasked counterparts. The occluded faces are obtained by covering the original faces with a range of occluding objects (e.g., glasses, masks) that vary in texture and color. 
Ding et al.~\cite{ding2020icm} applied a similar data augmentation method for training data, automatically generating synthetic masked face images on LFW. They used the Delaunay triangulation algorithm to overlay different masks on the faces based on the automatically detected facial landmarks. 

In Section~\ref{sec:comparison}, we compare our approach with competing methods~\cite{wang2020arxiv,anwar2020arxiv,huang2021icassp} in terms of how realistic the synthetically generated face masks are, showing that our approach generates significantly more realistic images of masked faces. This is the main differentiating factor from our competitors.







\begin{figure}[b]

\begin{center}
\begin{tabular}{@{~}c@{~}c@{~}c@{~}c}
   \includegraphics[width=0.24\linewidth]{figure_2a.png} &
   \includegraphics[width=0.24\linewidth]{figure_2b.png} &
   \includegraphics[width=0.24\linewidth]{figure_2c.png} &
   \includegraphics[width=0.24\linewidth]{figure_2d.png} \\
   (a) & (b) & (c) & (d) 
   \end{tabular}
\end{center}
\vspace{-0.3cm}
   \caption{{\bf Overview of our approach. }\emph {For the original image (a), we select a random mask and employ SparkAR to transform it (b) based on the 3D face model. We adjust the color of the mask (c) and thus obtain the output of our method. Further, we can overlay the mask (c) over the original image to obtain the masked face image (d). Best viewed in color.}}
\label{fig:green-background} 
\end{figure}

\section{Synthetic generation of masked face data sets}

Our method for synthetic generation of masks relies on SparkAR Studio~\cite{spark-ar-studio}. SparkAR Studio is a developer program made by Facebook that enables users to create augmented reality effects. Its practical use resides in creating Instagram face filters. In particular, SparkAR can be used to detect a 3D face in an input image and then apply highly realistic shadings and shadow effects on the detected face. Based on these features, we consider SparkAR to be an useful tool in creating realistic synthesized masks and overlaying them on real faces. We employ it to enhance the CASIA-WebFace~\cite{yi2014learning} and CelebA~\cite{Liu2015ICCV} data sets with masked faces, as detailed below. 




\subsection{Synthetic mask generation}

For a given input image containing a face, our goal is to create a synthesized mask that looks as realistic as possible when overlaid on the face, as illustrated in Figure~\ref{fig:green-background}(d). 
Our mask database consists of $9$ masks which vary in color, shape and texture (see Figure~\ref{fig:mask-database}). 

We employ the SparkAR software to detect the 3D face in the input image and randomly select a mask out of the existing $9$ masks in the database to be mapped over the 3D face.
Next, we use SparkAR to transform the selected mask such that it fits well on the 3D face model and then save the processed mask using a green screen background (see Figure~\ref{fig:green-background}(b)). This step is necessary to avoid publicly sharing any data from the original CelebA and CASIA-WebFace data sets, which are protected by license agreements that do not permit data redistribution. In order to overlay the mask on the original face, we subtract the green background and compute color matching between the segmented mask and the face. For this step, we use the Color Matcher plugin provided by After Effects. The transformed mask with transparent background and adjusted colors is the output of our method. An output example can be visualized in Figure~\ref{fig:green-background}(c). 
By overlaying the mask on the original image, we obtain the masked face image, as shown in Figure~\ref{fig:green-background}(d). To make sure that the masks generated with SparkAR are positioned well, we employ a verification step based on a method that detects facial landmarks, which is implement in Dlib-ml \cite{dlib-ml}. Having the facial landmarks, we derive a polygon that should roughly coincide with the region covered by the mask. Then, we compute the Intersection over Union (IoU) between this polygon and the created mask. If the computed IoU has a value lower than a fixed threshold ($0.3$ for CelebA and $0.5$ for CASIA-WebFace), we reprocess the image by employing the Dlib-ml library to apply the mask over the input image. 


Even though SparkAR is very powerful, there are images on which SparkAR cannot recognize the 3D face. In this case, we also fallback to the Dlib-ml library to obtain the synthesized mask.

\begin{figure}[t]
\begin{center}
\begin{tabular}{@{~}c@{~}c@{~}c}
   \includegraphics[width=0.33\linewidth]{figure_3a.png} &
   \includegraphics[width=0.33\linewidth]{figure_3b.png} &
   \includegraphics[width=0.33\linewidth]{figure_3c.png} \\
\includegraphics[width=0.33\linewidth]{figure_3d.png} &
   \includegraphics[width=0.33\linewidth]{figure_3e.png} &
   \includegraphics[width=0.33\linewidth]{figure_3f.png} 
   \end{tabular}
\end{center}
\vspace{-0.3cm}
   \caption{{\bf Examples showcasing key features of our method. }\emph{Our method relies on SparkAR to correctly predict face positions for various poses of the subjects, even when they are looking in a direction perpendicular to the camera. In such cases, the corresponding parts of the mask are automatically occluded. SparkAR can also recognize facial expressions and modify the 3D mask accordingly, e.g.~stretching the mask when appropriate.  Color matching the original image helps to blend in the mask over the context image, thus creating a more realistic sample. Best viewed in color.}}
\label{fig:key-features} 
\end{figure}





{\bf CelebA+masks.} For CelebA, we consider the keypoints provided with the data set to verify if each mask produced by SparkAR (or Dlib-ml) is correctly placed over the corresponding face. Following the described protocol, we generate the {\emph {CelebA+masks}} data set with $196,254$ synthesized masks, which represents $96.8\%$ of the total number of images in the CelebA data set. About $5.4\%$ of these masks are obtained using the Dlib-ml library. We underline that there are some cases when both SparkAR and Dlib-ml fail to produce a mask and this accounts for the remaining $3.2\%$ of all images in the CelebA data set.

{\bf CASIA-WebFace+masks.} We use Dlib-ml to generate the face keypoints for the verification step and proceed in the same way (as for CelebA) to remove poor examples of generated masks. We employ our method to generate the {\emph {CASIA-WebFace+masks}} data set containing $445,446$ synthesized masks, which represents about $90\%$ of the total number of images in the CASIA-WebFace data set. Roughly $11.5\%$ of the masks are obtained using the Dlib-ml library, which is almost double the percentage of images generated with Dlib-ml for the CelebA data set. We believe that CASIA-WebFace is much more diverse context-wise, presenting a higher difficulty for SparkAR in detecting the 3D face in the input image.

\subsection{Key features}

Our approach presents a few key features that enable synthetic generation of realistic masks.
SparkAR proves to be a powerful tool, as it correctly recognizes the 3D position and rotation of faces in almost $90\%$ of the processed images. This makes our approach robust and flexible compared to other approaches~\cite{anwar2020arxiv} that are
bounded to using a number of predefined directions of the head, by design.
From the samples illustrated in Figure~\ref{fig:key-features}, we can infer that SparkAR can predict the face position even when the subject is looking in a direction perpendicular to the camera. 
SparkAR also recognizes facial expressions and modifies the 3D mask accordingly. Therefore, we can expect the mask to cover the subject's face while they are laughing, smiling, yawning, etc.
The 3D face mask can also produce occlusions on itself. Thus, when someone turns the head at an angle, a certain part of the mask may get occluded if it is not visible from the camera's viewpoint, creating a realistic image.
We place the masks over the context using a smoothed alpha channel. Through this procedure, we eliminate the white jagged edges that appear when applying the mask over the original image. Color matching the images helps to blend in the mask over the context image, thus creating a more realistic sample.
Because our mask images have 4 channels (RGBA), we can apply any changes to the color channels and we can use the alpha channel as a segmentation mask when training machine learning models. 

\begin{figure}[t]
\begin{center}
\begin{tabular}{@{~}c@{~}c@{~}c@{~}c}
   \includegraphics[width=0.24\linewidth]{figure_4a.png} &
   \includegraphics[width=0.24\linewidth]{figure_4b.png} &
   \includegraphics[width=0.24\linewidth]{figure_4c.png} &
   \includegraphics[width=0.24\linewidth]{figure_4d.png} \\
   (a) & (b) & (c) & (d) 
   \end{tabular}
\end{center}
    \vspace{-0.2cm}
   \caption{{\bf Failure cases. }\emph{In less than $3\%$ of the images, the data sets might exhibit the following errors: (a) mask applied on wrong person, (b) mask covering the face only partially, (c) mask not covering the face, (d) compression artifacts. Best viewed in color.}}
\label{fig:limitations} 
\end{figure}

\subsection{Limitations}

Even though we employ a verification step for our generated masks using Dlib-ml, failure mask generation cases can still be present in our generated data sets, as shown in Figure~\ref{fig:limitations}. The poor samples can be caused by multiple factors: (i) SparkAR might detect another face than the one of interest (see Figure~\ref{fig:limitations}(a)); (ii) SparkAR gets an inaccurate 3D face model and the transformed mask covers the face of interest only partially (see Figure~\ref{fig:limitations}(b)) or not at all (see Figure~\ref{fig:limitations}(c)); (iii) while the SparkAR software generates masks, we employ a third-party recording software (OBS) to record the screen, which might introduce unwanted artifacts, such as the one illustrated in Figure~\ref{fig:limitations}(d). We manually inspected a randomly sampled subset that accounts for about $1\%$ of the generates images from each of the two data sets. Based on the manual annotation of two of the paper co-authors, we estimate that {CASIA-WebFace+masks} contains about $2.83\%$ failure cases, while {CelebA+masks} contains about $2.90\%$ failure cases. We underline that this problem is also prevalent in related data sets of masked faces, although not mentioned in the introductory papers. In our case, we tried to minimize this percentage by employing SparkAR and Dlib-ml, in a cascaded fashion. We would like to add that the faulty images illustrated in Figure~\ref{fig:limitations} do not represent the actual distribution of fault types. Indeed, most faults (around $90\%$) are caused by masks not covering the entire face, this being the most acceptable type of error, in our opinion. 

\section{Quantitative and qualitative evaluation of mask generation methods}
\label{sec:comparison}

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Comparison                  &                Annotator ID & Votes for ours & Votes for competitor \\
\hline\hline
\multirow{7}{*}{Ours versus~\cite{wang2020arxiv}}  &  Person $\#1$  & $200$ & $0$ \\
                                             &  Person $\#2$  & $196$ & $4$ \\
                                             &  Person $\#3$  & $197$ & $3$ \\         
                                             &  Person $\#4$  & $191$ & $9$ \\  
                                             &  Person $\#5$  & $189$ & $11$ \\ 
                                             &  Person $\#6$  & $191$ &  $9$ \\  
                                             \cline{2-4}
                                             & Overall in \%  & $97.00\%$ & $3.00\%$ \\
\hline                                             
\multirow{7}{*}{Ours versus~\cite{anwar2020arxiv}} &  Person $\#1$  & $137$ & $66$ \\
                                             &  Person $\#2$  & $191$ & $9$ \\
                                             &  Person $\#3$  & $190$ & $10$ \\         
                                             &  Person $\#4$  & $177$ & $23$ \\  
                                             &  Person $\#5$  & $177$ & $23$ \\ 
                                             &  Person $\#6$  & $182$ & $18$ \\  
                                             \cline{2-4}
                                             & Overall in \%  & $87.83\%$ & $12.17\%$ \\
\hline 
\multirow{7}{*}{Ours versus~\cite{huang2021icassp}} &  Person $\#1$  & $194$ & $6$ \\
                                               &  Person $\#2$  & $199$ & $1$ \\
                                               &  Person $\#3$  & $200$ & $0$ \\         
                                               &  Person $\#4$  & $197$ & $3$ \\  
                                               &  Person $\#5$  & $192$ & $8$ \\ 
                                               &  Person $\#6$  & $200$ & $0$ \\  
                                               \cline{2-4}
                                               & Overall in \%  & $98.50\%$ & $1.50\%$ \\
  
\hline
\end{tabular}
\end{center}
\caption{Comparative results showing the number of votes for our approach versus the number of votes for each of the three state-of-the-art methods~\cite{wang2020arxiv,anwar2020arxiv,huang2021icassp}. Each comparative study is based on $200$ sample pairs and it was completed by $6$ human annotators.} 
\label{tab:quality_assessment}
\end{table} 

Automatically generating realistic images with synthesized masks overlaid on faces holds the promise to address the lack of large-scale data sets of masked faces.


We qualitatively compare our approach to the methods presented in~\cite{wang2020arxiv, anwar2020arxiv, huang2021icassp} in terms of how realistic the generated images with synthesized masks overlaid on faces are. For each of the three comparisons, we select a random subset of $200$ image pairs. Each image pair consists of two images with synthesized masks overlaid on faces. One image is the output of our method, while the other image is the output of one of the three methods~\cite{wang2020arxiv, anwar2020arxiv, huang2021icassp} that we compare with. The position of the images displayed in the application interface (left-hand or right-hand side) is randomly picked every time, such that the annotator does not know which method is shown in the left-hand side or in the right-hand side. 

When presented with an image pair, annotators are asked to select the image that looks more realistic. 
For our method, we consider a fixed set of $200$ images containing masked faces for all three comparisons. The set is randomly chosen from {CelebA+masks}. For Anwar et al.~\cite{anwar2020arxiv}, we run their open-source software, MaskTheFace, on the randomly chosen subset of $200$ images. For the other two competing methods~\cite{wang2020arxiv,huang2021icassp}, which do not share code to produce masked faces, we randomly select $200$ images from their released data sets. 
More precisely, we take images from the Simulated Masked Face Recognition~\cite{wang2020arxiv} and the Webface-OCC~\cite{huang2021icassp} data sets.



We collect data from $6$ impartial annotators and obtain a total of $3,600$ annotations ($6$ annotators $\times$ $200$ image pairs $\times$ $3$ comparative studies). Table~\ref{tab:quality_assessment} shows quantitative results of our human evaluation study. For each comparison and annotator, we show the number of votes for each of the two methods involved in the comparison.
When compared to~\cite{wang2020arxiv,huang2021icassp}, the annotators picked our method in more than $97\%$ of the times (on average), which is remarkable. Among our three competitors, Anwar et al.~\cite{anwar2020arxiv} gained the highest number of votes. Still, our method was voted in the majority of cases ($87.83\%$) when it is was compared to that of Anwar et al.~\cite{anwar2020arxiv}. According to the annotators, we conclude that our method produces significantly more realistic examples of faces with masks.

In Figure~\ref{fig:realistic-comparison}, we illustrate a set of typical cases when our method is considered superior in producing more realistic images than the competitors, along with some rare cases when our method is inferior to one of the competitors, especially Anwar et al.~\cite{anwar2020arxiv}. From the displayed examples, we observe that the method of Wang et al.~\cite{wang2020arxiv} does a very poor job at aligning the masks to the corresponding faces, while the method of Huang et al.~\cite{huang2021icassp} fails to properly blend in the mask, leaving clear patterns indicating that the overlaid masks are not realistic. Confirming the results in Table~\ref{tab:quality_assessment}, the masked faces shown in Figure~\ref{fig:realistic-comparison} indicate that Anwar et al.~\cite{anwar2020arxiv} is the strongest competitor. Nevertheless, our method is able to more realistically align and blend in the masks. In summary, we underline that the qualitative results shown in Figure~\ref{fig:realistic-comparison} are in perfect agreement with the quantitative results presented in Table~\ref{tab:quality_assessment}.






\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{examples.pdf}


    
















 \vspace{-0.2cm}   
    \caption{{\bf Comparing the degree of realism of images with synthetically masked faces. } \emph {We show six favorable cases and two failure cases for our comparative study. Our images (last row) are compared with the images produced by the competitors~\cite{wang2020arxiv,anwar2020arxiv,huang2021icassp} (first three rows). Image pairs are shown column-wise.  Examples shown on the first six columns represent the usual cases, when our method is superior to the other methods (see Table~\ref{tab:quality_assessment}). Examples shown on the last two columns represent rare cases, when our method is inferior to one of the competitors. Best viewed in color.}}
    \label{fig:realistic-comparison}
\end{figure}


\section{Experimental evaluation of face recognition systems}

We evaluate the practical usage of our method by fine-tuning three face recognition systems from the literature, namely FaceNet~\cite{Schroff15CVPR}, VGG-face~\cite{Parkhi15BMVC} and ArcFace~\cite{Deng19CVPR}, on masked face data sets generated by our method.  
We then test the face recognition models on data sets with faces covered by synthesized or real masks. We conduct our evaluation in the scenario of face verification: given a pair of two faces ($i$,$j$), the task is to decide whether faces $i$ and $j$ are of the same identity or not. 

{\bf Data sets.} As training data, we consider the training splits of the CelebA and CASIA-WebFace data sets, but the images are replaced with our masked faces from CelebA+masks and CASIA-WebFace+masks. Similarly, for testing, we consider the test splits of CelebA and CASIA-WebFace, replacing the original images with the corresponding ones from our data sets. Additionally, we report results on the MFR2 real masked face data set introduced by Anwar et al.~\cite{anwar2020arxiv}.

{\bf Evaluation protocol.} We follow the standard protocol for {\emph {unrestricted}}, {\emph{labeled outside data}}, as mentioned in~\cite{Huang07Tech}, to evaluate the image pairs. The $L_2$ distance threshold between two embeddings is calculated on the nine folds of the underlying test set and used on the tenth test fold to report the accuracy, in a 10-fold cross-validation manner.

{\bf Performance metrics.} We adopt the notation in~\cite{Schroff15CVPR} and consider, for a given test set of image pairs, $\mathcal{P}_{\mbox{\small{same}}}$ to be the set of all face pairs ($i$,$j$) of the {\emph {same}} identity and $\mathcal{P}_{\mbox{\small{diff}}}$ to be the set of all face pairs of {\emph {different}} identities. The set of pairs correctly classified as {\emph {same}} at threshold $d$ is the set of {\emph {true accepts}} defined as: $\mbox{TA}(d) = \{(i,j) \in {\mathcal{P}}_{\mbox{\small{same}}}, D(x_i,x_j ) \leq d \}$, where $x_i$ and $x_j$ are the neural embeddings of the faces $i$ and $j$ and $D$ is the squared $L_2$ distance. The set of pairs incorrectly classified as {\emph {same}} at threshold $d$ represents the set of {\emph {false accepts}}, which is defined as: $\mbox{FA}(d) = \{(i,j) \in {\mathcal{P}}_{\mbox{\small{diff}}}, D(x_i,x_j ) \leq d \}$. For the threshold $d$, we select the value that provides the maximum accuracy on the validation folds, considering values in the range $[0,4]$ with a step of $0.01$. We compute the accuracy on the test fold for the optimum threshold.

{\bf Implementation details.} Prior to the fine-tuning and evaluation of the face recognition systems, we apply the MTCNN~\cite{zhang2016spl} face detector, choosing the largest bounding box whenever there are multiple face detections in an image. We apply face registration only for ArcFace. The face recognition systems are trained using the original hyperparameters reported in the introductory works~\cite{Schroff15CVPR,Deng19CVPR,Parkhi15BMVC}. For FaceNet \cite{Schroff15CVPR}, we select the Inception-ResNet-v1 backbone, while for ArcFace~\cite{Deng19CVPR}, we select the ResNet-50 backbone.



{\bf Evaluation on the CelebA+masks data set.} After applying our method to enhance the data set with masks, we obtain $158,256$ training faces of $8,190$ identities and $19,494$ test faces of $1,000$ identities. We train the face recognition systems on the train split in two scenarios: on original images when no face masks are provided, and on images modified by our method with synthesized masks. We test the face recognition systems on the test split of images with synthesized masks.

{\bf Evaluation on the CASIA-WebFace+masks data set.} After applying our method to enhance the data set with masks, we obtain $264,168$ training faces of $6,345$ identities and $93,235$ test faces of $2,115$ identities. We train the face recognition systems in an analogous manner to the CelebA+masks case. We evaluate the face recognition systems on the test split of images with generated masks.

{\bf Evaluation on the MFR2 data set.}
We apply the CelebA models on the real images from the MFR2 \cite{anwar2020arxiv} data set. On this data set, we consider two additional baselines for each face recognition model, one that is fine-tuned on CelebA+MaskTheFace \cite{anwar2020arxiv}, and one that is fine-tuned on our mask segments with black pixels instead of textures.

{\bf Quantitative results.} 
The results of the face recognition systems are presented in Table~\ref{tab:quantitative-results}. While there is a clear ranking between the evaluated face recognition models, the best one being ArcFace and the least competitive one being VGG-face, we observe that fine-tuning on realistically generated faces is beneficial in each and every case. The improvements brought by our approach are always above $2\%$. We thus conclude that our approach is useful in increasing the performance of face recognition systems on masked faces.

\begin{table}
\begin{center}
\setlength\tabcolsep{4.5pt}
\begin{tabular}{|l|c|c|c|}
    \hline
    \multirow{2}{*}{\bf Train set} & \multicolumn{3}{|l|}{{\bf Test set}: CelebA-test+masks (synthetic)} \\
\cline{2-4}
    & FaceNet & VGG-face & ArcFace\\
    \hline
    \hline
    CelebA-train (no masks) & $90.96\%\pm1.13\%$ & $84.56\%\pm1.40\%$ & $91.78\%\pm0.56\%$\\
    \hline
    CelebA-train+masks (ours)  & $93.58\%\pm0.82\%$ & $91.51\%\pm0.97\%$ & $95.43\%\pm0.78\%$\\
    \hline
     & \multicolumn{3}{|l|}{{\bf Test set}: CASIA-WebFace-test+masks (synthetic)} \\
     \hline
    CASIA-WebFace-train (no masks) & $84.21\%\pm1.49\%$ & $79.65\%\pm1.81\%$ & $87.95\%\pm1.44\%$\\
    \hline
    CASIA-WebFace-train+masks (ours)  & $88.06\%\pm1.27\%$ & $86.85\%\pm1.11\%$ & $91.47\%\pm0.85\%$\\
    \hline
    & \multicolumn{3}{|l|}{{\bf Test set}: MFR2 (real)} \\
     \hline
     CelebA-train (no masks) & $92.20\%\pm1.92\%$ & $84.29\%\pm4.00\%$ & $91.39\%\pm3.64\%$\\
     \hline
    CelebA-train+black pixel masks  & $93.04\%\pm2.33\%$ & $88.31\%\pm3.08\%$ & $94.22\%\pm2.31\%$\\
    \hline
    CelebA-train+MaskTheFace \cite{anwar2020arxiv}  & $95.16\%\pm1.88\%$ & $88.35\%\pm2.61\%$ & $94.33\%\pm1.55\%$\\
    \hline
    CelebA-train+masks (ours)  & $96.22\%\pm1.91\%$ & $91.26\%\pm2.19\%$ & $95.16\%\pm2.90\%$\\
    \hline
\end{tabular}
\end{center}
\caption{Face recognition results of FaceNet, VGG-face and ArcFace on masked faces, when the models are trained on original and masked face images. The accuracy rates are reported for both synthetically generated as well as real masked faces.} 
\label{tab:quantitative-results}
\end{table} 






To assess the usefulness of our mask generation approach with respect to existing masked face data sets in training state-of-the-art masked face recognition systems, we conduct an additional experiment on MFR2. Accordingly, we use the MaskTheFace open-source tool of Anwar et al.~\cite{anwar2020arxiv} (which is the second-most realistic according to Table~\ref{tab:quality_assessment}) to generate synthetic masks for the CelebA data set, obtaining a set of $194,614$ masked face images. We train the three face recognition models on the resulting data set (CelebA-train+MaskTheFace) and report the accuracy on the MFR2 test set (containing real faces). The VGG-face trained on CelebA-train+MaskTheFace yields an accuracy of $88.35\%$, which is nearly $3\%$ lower than the accuracy of the VGG-face trained on our masked faces. We observe a similar pattern when training FaceNet and ArcFace on CelebA-train+MaskTheFace instead of our version of CelebA-train, the only difference being that the performance gap is around $1\%$ for both models. This clearly indicates that having more realistic face masks is useful.

We also carry out an experiment by training the face recognition models on a version of CelebA-train that uses our mask overlays, but replaces all pixels inside the mask with the black color. We test the resulting models on MFR2, observing that there are significant performance drops (around $3\%$) for two face recognition models, namely FaceNet and VGG-face. We believe that this observation can be explained by the fact that the actual masks reveal some information about the generic shape of the lower part of the face, which is useful for face recognition systems.



{\bf Discussion.} 
A relevant aspect to discuss in the context of masked face recognition is the performance level of models fine-tuned on masked faces, when the evaluation is performed on normal (unmasked) faces. Most likely, the performance on normal faces will not be maintained to the level attained by pre-trained face recognition models. However, we do not consider this as a problem, since we can employ a binary classifier to distinguish between normal and masked faces, and then use the appropriate face recognition system. Our preliminary results based on a MobileNetV2 architecture \cite{SandlerCVPR18} show that we can achieve an accuracy of $99.44\%$ for classifying normal faces versus masked faces (synthesized or real). We believe this guarantees the same level of performance, regardless of the occlusion level (mask or no mask).

In addition to the utility for face recognition systems, we underline that our masked face data sets can be used to train models for other tasks as well, e.g.~masked face detection or facial landmark detection of masked faces. This significantly broadens the applicability of our work.

\section{Conclusion}

In this paper, we proposed an automatic method for the synthetic generation of masks using SparkAR Studio for the goal of enhancing data sets with masked faces. 
We showed that our approach is superior in producing more realistic images than competitors~\cite{wang2020arxiv, anwar2020arxiv, huang2021icassp}. For the face verification task, we demonstrated that face recognition systems~\cite{Schroff15CVPR,Deng19CVPR,Parkhi15BMVC} fine-tuned on masked faces perform better than the ones trained on regular faces, when the evaluation is conducted on masked faces. We also showed that the usefulness of masked faces for face recognition systems is proportional to the degree of realism of the synthesized masks.

In future work, we aim to propose more elaborate ways of taking advantage of our synthetically generated masked face data sets to improve face recognition models. One approach could be to consider knowledge distillation, as in~\cite{Georgescu-ICPR-2020}.
 


\bibliographystyle{neurips_natbib}
\bibliography{bibliography}








\end{document}
