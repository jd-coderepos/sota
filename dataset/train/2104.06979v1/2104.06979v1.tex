

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage[normalem]{ulem}
\usepackage{vcell}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{float}
\usepackage{todonotes}
\usepackage{microtype}

\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning}

\author{Kexin Wang, Nils Reimers, Iryna Gurevych \\
	Ubiquitous Knowledge Processing Lab (UKP-TUDA)\\
	Department of Computer Science, Technical University of Darmstadt\\
	\url{www.ukp.tu-darmstadt.de}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Learning sentence embeddings often requires large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-the-art unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1\% of the performance of in-domain supervised approaches. Further, we show that TSDAE is a strong pre-training method for learning sentence embeddings, significantly outperforming other approaches like Masked Language Model.\footnote{Code available at: \url{https://github.com/UKPLab/sentence-transformers/}}

A crucial shortcoming of previous studies is the narrow evaluation: Most work mainly evaluates on the single task of Semantic Textual Similarity (STS), which does not require any domain knowledge. It is unclear if these proposed methods generalize to other domains and tasks. We fill this gap and evaluate TSDAE and other recent approaches on four different datasets from heterogeneous domains.
\end{abstract}

\section{Introduction}
Sentence embedding techniques encode sentences into a fixed-sized, dense vector space such that semantically similar sentences are close. The most successful previous approaches like InferSent~\citep{conneau-EtAl:2017:EMNLP2017}, Universial Sentence Encoder (USE)~\citep{DBLP:conf/emnlp/CerYKHLJCGYTSK18} and SBERT~\citep{DBLP:conf/emnlp/ReimersG19} heavily relied on labeled data to train sentence embedding models. However, for most tasks and domains, labeled data is not available and data annotation is expensive. To overcome this limitation, unsupervised approaches have been proposed which learn to embed sentences just using an unlabeled corpus for training.

We propose a new approach: Transformer-based Sequential Denoising Auto-Encoder (TSDAE). It significantly outperforms previous methods via an encoder-decoder architecture. During training, TSDAE encodes corrupted sentences into fixed-sized vectors and requires the decoder to reconstruct the original sentences from this sentence embedding. For good reconstruction quality, the semantics must be captured well in the sentence embedding from the encoder. Later, at inference, we only use the encoder for creating sentence embeddings.

A crucial shortcoming of previous unsupervised approaches is the evaluation. Often, approaches are only evaluated on the Semantic Textual Similarity (STS) task from SemEval ~\citep{DBLP:conf/emnlp/LiZHWYL20,carlsson2021semantic,DBLP:journals/corr/abs-2006-03659}. As we argue in Section~\ref{sec:STS_results}, we perceive this as an insufficient evaluation. The STS datasets do not include sentences with domain specific knowledge. Further, unsupervised sentence embeddings approaches achieve rather inferior results on this tasks compared to methods like USE or SBERT that use labeled data from other tasks like NLI. It remains unclear, how well unsupervised sentence embedding methods will perform on domain specific tasks.

To answer this question, we compare TSDAE with previous unsupervised sentence embeddings approaches on three different tasks (Information Retrieval, Re-Ranking and Paraphrase Identification), for heterogeneous domains and different text styles. We show that TSDAE can outperform other state-of-the-art unsupervised approaches by up to 6.4 points. TSDAE is able to perform on-par or even outperform existent supervised models like USE-large, which had been trained with a lot of labeled data from various datasets.

Further, we demonstrate in Section~\ref{sec:analysis} that TSDAE works well as a pre-training task for sentence embedding models. We observe a significant performance improvement compared to other pre-training tasks like Masked Language Model (MLM) or using the pretrained BERT without domain specific pre-training.


Our contributions are three-fold:
\begin{itemize}
    \item We propose a novel unsupervised method, TSDAE based on denoising auto-encoders. We show that it outperforms the previous best approach by up to 6.4 points on diverse datasets.
    \item To the best of our knowledge, we are the first to compare the unsupervised sentence embedding methods based on pre-trained Transformers for various tasks on heterogeneous domains.
    \item TSDAE outperforms other methods including MLM by a large margin as a pre-training method for sentence embedding learning on multiple datasets.
\end{itemize}

\section{Related Work}
In this work, we view sentence embedding methods as supervised if the training process requires labels for sentence pairs and otherwise as unsupervised ones.

\textbf{Supervised sentence embeddings} utilize training signals for sentence pairs which provide the information about the relation between the sentences. Since sentence embeddings are usually applied to measure the similarity of a sentence pair, the most direct way is to label this similarity for supervised training~\citep{DBLP:journals/corr/HendersonASSLGK17}. Many studies also find that natural language inference (NLI), question answering and conversational context datasets can successfully be used to train sentence embeddings~\citep{conneau-EtAl:2017:EMNLP2017,DBLP:conf/emnlp/CerYKHLJCGYTSK18}. The recently proposed Sentence-Transformers~\citep{DBLP:conf/emnlp/ReimersG19} introduce pre-trained Transformers to the field of sentence embeddings with the above-mentioned supervision like NLI labels~\citep{DBLP:conf/emnlp/BowmanAPM15,DBLP:conf/naacl/WilliamsNB18}. Although high-quality sentence embeddings can be derived via supervised training, the labeling cost is a major obstacle for practical usage, especially for specialized domains.

\textbf{Unsupervised sentence embeddings} utilize only unlabeled corpus during training. Recent work combines pre-trained Transformers with different training objectives to achieve state-of-the-art results. Among them,  Contrastive Tension (CT)~\citep{DBLP:journals/corr/abs-2006-03659} simply views the identical and different sentences as positive and negative examples, resp.\ and train two independent encoders; BERT-flow~\citep{DBLP:conf/emnlp/LiZHWYL20} trains model via debiasing embedding distribution towards Gaussian. For more details, please refer to Section~\ref{sec:exps}. Both of them requires only independent sentences. By contrast, DeCLUTR~\citep{DBLP:journals/corr/abs-2006-03659} utilizes sentence-level contexts and requires long documents (2048 tokens at least) for training. This requirement is hardly met for many cases, e.g.\ Tweets or dialogues. Thus, in this work we only consider methods which uses only single sentences during training.

Most previous work mainly evaluate only on Semantic Textual Similarity (STS) from the SemEval shared tasks. As we show in Section~\ref{sec:STS_results}, the unsupervised approaches perform much worse than the out-of-the-box supervised pre-trained models even though they were not specifically trained for STS. Further, a good performance on STS does not necessarily correlate with the performance on down-stream tasks~\citep{DBLP:conf/coling/ReimersBG16}. It remains unclear how these methods perform on specific tasks and domains. To answer this, we compare three powerful unsupervised methods based on pre-trained Transformers including CT, BERT-flow and our proposed TSDAE on different tasks of heterogeneous domains. 


\section{Sequential Denoising Auto-Encoder} 
Although Sequential Denoising Auto-Encoder (SDAE)~\citep{DBLP:journals/jmlr/VincentLLBM10,Goodfellow-et-al-2016,DBLP:conf/naacl/HillCK16} is a popular unsupervised method in machine learning, how to combine it with pre-trained Transformers remains unclear. In this section, we first introduce the training objective of TSDAE and then give the optimal configuration of TSDAE.

\begin{figure}[t]
  \centering
  \includegraphics[width=40mm]{imgs/architecture/architecture.pdf}
  \caption{Architecture of TSDAE.}
  \label{fig:architecture}
\end{figure}


\subsection{Training Objective}
Figure~\ref{fig:architecture} illustrates the architecture of TSDAE. TSDAE train sentence embeddings by adding a certain type of noise (e.g.\ deleting or swapping words) to input sentences, encoding the damaged sentences into fixed-sized vectors and then reconstructing the vectors into the original input. Formally, the training objective is:

where  is the training corpus,  is the input sentence with  tokens,  is the corresponding damaged sentence,  is the word embedding of ,  is the vocabulary size and  is the hidden state at decoding step . 

An important difference to original transformer encoder-decoder setup presented in~\citet{DBLP:conf/nips/VaswaniSPUJGKP17} is the information available to the decoder: Our decoder decodes only from a fixed-size sentence representation produced by the encoder. It does not have access to all contextualized word embeddings from the encoder. This modification introduces a bottleneck, that should force the encoder to produce a meaningful sentence representation. 


\subsection{TSDAE}

The model architecture of TSDAE is a modified encoder-decoder Transformer where the key and value of the cross-attention are both confined to the sentence embedding only. Formally, the formulation of the modified cross-attention is:

where  is the decoder hidden states within  decoding steps at the -th layer,  is the size of the sentence embedding,  is a one-row matrix including the sentence embedding vector and Q, K and V are the query, key and value, respectively. 
By exploring different configurations on the Semantic Textual Similarity (STS) datasets~\citep{cer-etal-2017-semeval}, we discover that the best combination is: (1) adopting deletion as the input noise and setting the deletion ratio to 0.6, (2) using the output of the \texttt{[CLS]} token as fixed-sized sentence representation (3) tying the encoder and decoder parameters during training. For the detailed tuning process, please refer to Appendix~\ref{sec:TSDAE_config}.


\section{Datasets and Evaluation}
In most of the previous work, the sentence embedding methods are mainly evaluated on the STS task from the SemEval shared tasks, which is a rather generic task. How the methods perform on specific tasks and domains remains unclear. To fill this gap, different tasks from heterogeneous domains are used for evaluation. The tasks include Re-Ranking (RR), Information Retrieval (IR) and Paraphrase Identification (PI). In detail, the datasets used are as follows:

\textbf{AskUbuntu} (RR task) is a collection of user posts from the technical forum AskUbuntu~\citep{DBLP:conf/naacl/LeiJBJTMM16}. Models are required to re-rank 20 candidate questions according to the similarity given an input post. The candidates are obtained via BM25 term-matching~\citep{DBLP:conf/trec/RobertsonWJHG94}. The evaluation metric is Mean Average Precision (MAP).

\textbf{CQADupStack} (IR task) is a question retrieval dataset of forum posts on various topics from StackExchange \citep{DBLP:conf/adcs/HoogeveenVB15}. In detail, it has 12 forums including Android, English, gaming, geographic information system, Mathematica, physics, programmers, statistics, Tex, Unix, Webmasters and WordPress. Models are required to retrieve duplicate questions from a large candidate pool. The metric is MAP@100.

\textbf{TwitterPara} (PI task) consists of two similar datasets: the Twitter Paraphrase Corpus (PIT-2015)~\citep{DBLP:conf/semeval/XuCD15} and the Twitter News URL Corpus (noted as TURL)~\citep{DBLP:conf/emnlp/LanQHX17}. The dataset consists of pairs of tweets together with a crowd-annotated score if the pair is a paraphrase. The evaluation metric is Average Precision (AP) over the gold confidence scores and the similarity scores from the models.

\textbf{SciDocs} (RR task) is a benchmark consisting of multiple tasks about scientific papers~\citep{DBLP:conf/acl/CohanFBDW20}. In our experiments, we use the tasks of \textit{Cite}: Given a paper title, identify the titles the paper is citing;  \textit{Co-Cite (CC)}, \textit{Co-Read (CR)}, and \textit{Co-View (CV)}, for which we must find papers that are frequently co-cited/-read/-viewed for a given paper title. For all these tasks, given one query paper title, models are required to identify up to 5 relevant papers titles from up to 30 candidates. The negative examples were selected randomly. The evaluation metric is MAP.

For evaluation, sentences are first encoded into fixed-sized vectors and then the cosine similarity of the vectors is adopted as the sentence similarity. Since sentence-level embeddings are the target, for AskUbuntu, CQADupStack and SciDocs, we remove the body texts and only keep the titles for evaluation. For the datasets with sub-datasets or sub-tasks including CQADupStack, TwitterPara and SciDocs, the final score is derived by averaging the scores from each sub-dataset or sub-task.

For unsupervised training, we use all sentences from the training split without any labels. For in-domain supervised training, we extract all the relevant pairs as the training set from the original training splits for IR and RR tasks. For the PI task, we keep the original labeling scheme, i.e., the confidence scores. The statistics for each dataset are in Appendix~\ref{sec:data_details}.



\begin{table*}[t]
\centering
\resizebox{15.5cm}{!}{
\begin{tabular}{|l|c|c|ccc|ccccc|c|} 
\hline
\textbf{Method}            & \textbf{AskU.}                              & \textbf{CQADup.}                & \multicolumn{3}{c|}{\textbf{TwitterP.}}                                              & \multicolumn{5}{c|}{\textbf{SciDocs}}                                                                                    & \textbf{Avg.}                   \\ 
\cline{4-11}
\textbf{Sub-task/-dataset} & \multicolumn{1}{l|}{}              & \multicolumn{1}{l|}{}  & TURL                   & PIT                    & \multicolumn{1}{l|}{Avg.} & Cite           & CC             & CR                             & CV             & \multicolumn{1}{l|}{Avg.} & \multicolumn{1}{l|}{}  \\ 
\hline
\multicolumn{12}{|l|}{ \textit{Proposed unsupervised method} }                                                                                                                                                                                                                                         \\ 
\hline
TSDAE             & \multicolumn{1}{c}{\textbf{59.4} } & 14.5                   & 76.8                   & 69.2                   & 73.0                      & \textbf{71.4}  & \textbf{73.9}  & \textbf{75.0}                  & \textbf{75.6}  & \textbf{74.0}             & \textbf{55.0}          \\ 
\hline
\multicolumn{12}{|l|}{ \textit{Previous unsupervised approaches based on BERT-base} }                                                                                                                                                                                                                  \\ 
\hline
CT                & {56.3}                       & {13.3}           & 74.6                   & {\textbf{70.4}}  & {72.5}              & {63.4}   & {67.1}   & {70.1}  & {69.7}   & {67.6}              & {52.4}           \\
BERT-flow         & 53.7                               & 9.2                    & 72.8                   & 65.7 & 69.2                      & 61.3           & 62.8           & 66.7                           & 67.1           & 64.5                      & 49.1                   \\ 
\hline
\multicolumn{12}{|l|}{ \textit{Other previous unsupervised approaches} }                                                                                                                                                                                                                               \\ 
\hline
BM25              & {53.4}                       & 9.7                    & 65.2                   & 63.2                   & 64.2                      & 47.4           & 49.7           & 56.1                           & 55.4           & 52.1                      & 44.9                   \\
Avg. GloVe        & 51.0                               & {10.0}           & {70.1}           & {52.1}           & {61.1}              & 58.8           & 60.6           & 64.2                           & 65.4           & 62.2                      & {46.1}           \\
Sent2Vec          & 49.0                               & 3.2                    & 47.5                   & 39.9                   & 43.7                      & {61.6}   & {66.0}   & {66.1}                   & {66.7}   & {65.1}              & 40.2                   \\ 
\hline
\multicolumn{12}{|l|}{ \textit{Out-of-the-box supervised pre-trained models} }                                                                                                                                                                                                                         \\ 
\hline
SBERT-base-NLI    & 52.5                               & 8.7                    & 71.5                   & 60.7                   & 66.1                      & 60.7           & 62.5           & 64.2                           & 65.7           & 63.3                      & 47.6                   \\
SDRoBERTa-para    & 56.5                               & 12.9                   & 74.8                   & 69.1                   & 72.2                      & 64.5           & 66.2           & 68.7                           & 69.5           & 67.2                      & 52.2                   \\
USE-large         & \textit{59.3}                       & \textit{\textbf{15.9}}  & \textbf{77.1}  & 69.8           & \textbf{73.5}     & 67.1   & 69.5   & 71.4                   & 72.6   & {70.2}              & {54.7}           \\ 
\hline
\multicolumn{12}{|l|}{ \textit{In-domain supervised training (upper bound)} }                                                                                                                                                                                                                          \\ 
\hline
SBERT-supervised  & 63.8                               & 16.3                   & 81.6                   & 75.8                   & 78.7                      & 90.4           & 91.2           & 86.2                           & 83.6           & 87.9                      & 61.6                   \\
\hline
\end{tabular}}
\caption{Evaluation using average precision. Results are averaged over 5 random seeds. The best results excluding the upper bound are bold. USE-large was trained with in-domain training data for AskUbuntu and CQADupStack (scores in italic). Our proposed TSDAE significantly outperforms both other unsupervised and supervised out-of-the-box approaches.}
\label{tbl:main_results}
\end{table*}

\section{Experiments}
\label{sec:exps}
In this section, we compare our proposed TSDAE with other unsupervised counterparts and  out-of-the-box supervised pre-trained models on the above mentioned tasks. We include two recent state-of-the-art unsupervised approaches, CT and BERT-flow, in the comparison. We use the proposed hyper-parameters from the respective paper. Without other specification, \textit{BERT-base-uncased} is used as the base Transformer model. To eliminate the influence of randomness, we report the scores averaged over 5 random seeds for all the in-domain models. For other details, please refer to Appendix~\ref{sec:experiment_settings}.

\subsection{Baseline Methods}
For the baselines of shallow methods, we include avg.\ GloVe embeddings~\citep{pennington2014glove} and Sent2Vec~\citep{DBLP:conf/naacl/PagliardiniGJ18}. The former generates sentence embeddings by averaging word embeddings trained on a large corpus from the general domain; the latter is also a bag-of-words model but trained on the in-domain unlabeled corpus. For deep models, we report the scores of two Sentence-Transformer models including the one trained on the supervised NLI data from the \textit{BERT-base-uncased}\footnote{Results for other checkpoints is reported in Appendix~\ref{sec:other_checkpoints}} checkpoint (noted as SBERT-base-NLI) and the one trained on large-scale supervised paraphrase data from the \textit{DistilRoBERTa} checkpoint (noted as SDRoBERTa-para). We also report the scores of the Google's Universial Sentence Embedding (USE)~\citep{DBLP:conf/acl/YangCAGLCAYTSSK20} which is trained on multiple supervised datasets including NLI and community question answering. Since there are IR and RR tasks, we additionally include the popular baseline BM25 for comparison.

To better understand the relative performance of these unsupervised methods, we also train SBERT models in an in-domain supervised manner and view their scores as the upper bound. For AskUbuntu, CQADupStack and SciDocs, where the relevant sentence pairs are labeled, the in-domain SBERT models are trained with the Multiple-Negative Ranking Loss (MNRL)~\citep{DBLP:journals/corr/HendersonASSLGK17} from the \textit{BERT-base-uncased} Transformer checkpoint. For a batch of relevant sentences pairs , MNRL views the labeled pairs as positive and the other in-batch combinations as negative. Formally, the training objective for each batch is:
where  is a certain similarity function for vectors and  is the sentence encoder that embeds sentences. For TwitterPara, whose relevant scores are labeled, the Mean Square Error (MSE) loss is adopted to train the in-domain models.


\subsection{Contrastive Tension (CT)}
CT~\citep{carlsson2021semantic} finetunes pre-trained Transformers in a contrastive-learning fashion. For each sentence, it construct a binary cross-entropy loss by viewing the same sentence as the relevant and samples  random sentences as the irrelevant.  To make the training process stable, for each sentence pair , CT uses two independent encoders  and  from the same initial parameter point to encode the sentence  and , respectively. Formally, the learning objective is:

where  represents whether sentence  is identical to sentence  and  is the Logistic function. Despite its simplicity, CT achieves state-of-the-art unsupervised performance on the Semantic Textual Similarity (STS) datasets.

\subsection{BERT-flow}
In stead of fine-tuning the parameters of the pre-trained Transformers, BERT-flow~\citep{DBLP:conf/emnlp/LiZHWYL20} aims at fully exploiting the semantic information encoded by these pre-trained models themselves via distribution debiasing. The paper of BERT-flow claims that the BERT word embeddings are highly relevant to the word frequency, which in turn influences the hidden states via the Masked Language Modeling (MLM) pre-training. This finally leads to biased sentence embeddings generated by the pooling over these hidden states. To solve this problem, BERT-flow inputs the biased sentence embedding into a trainable flow network ~\citep{DBLP:conf/nips/KingmaD18} for debiasing via fitting a standard Gaussian distribution, while keeping the parameters of the BERT model unchanged. Formally, the training objective is:

where  is the biased embedding of sentence  and  is the debiased sentence embedding which follows a standard Gaussian distribution. Equation~\ref{eq:berflow2} is derived by applying the change-of-variables theorem to Equation~\ref{eq:berflow1}.


\section{Results}
\label{sec:results}
The results are shown in Table~\ref{tbl:main_results}. Among the unsupervised methods, TSDAE outperforms the previous best approach (CT) by up-to 6.4 points and 2.6 points on average over all tasks.

Another feasible option for a task without training data is to use an out-of-the-box pre-trained model (SBERT-base-NLI, SDRoBERTa-para, USE-large).\footnote{USE-large was trained using question-answer pairs from StackExchange, i.e.\ it used in-domain training data for AskUbuntu and CQADupStack, likely resulting in inflated scores for these tasks. SBERT and SDRoBERTa was trained on data which is completely different from our evaluation datasets.} We note that out-of-the-box models achieve quite strong performances and outperform most unsupervised approaches. TSDAE is able to outperform SDRoBERTa-para on all tasks and outperforms USE-large on the AskUbuntu and SciDocs tasks. We observe that TSDAE yields the largest performance increase for SciDocs (3.8 points vs.\ USE and 6.8 points vs. SDRoBERTa-para), the dataset that is the most distinct from the training datasets of USE / SDRoBERTa-para. 


Training with in-domain labeled data can, as expected, further improve the performance. TSDAE is on average 6.6 points behind the in-domain supervised system. This indicates substantial room for future work. 


\section{Analysis}
\label{sec:analysis}
We further analyze unsupervised sentence embedding learning methods. In particular, we analyze the required size of the unlabeled corpus and using the methods as pre-training task for fine-tuning with labeled data. 

For all the datasets except TwitterPara, the analysis is carried out on the development set. For TwitterPara, the test set is used, as it has no development split released by the original paper. All the hyper-parameters are chosen up-front without tuning to a particular dataset.


\begin{figure}[t]
  \centering
  \subfloat[AskUbuntu]{\includegraphics[width=38mm]{imgs/training-amount/askubuntu-did.pdf}}
  \subfloat[CQADupStack]{\includegraphics[width=36mm]{imgs/training-amount/cqadupstack-did.pdf}} \\
  \subfloat[TwitterPara]{\includegraphics[width=36mm]{imgs/training-amount/twitter-did.pdf}} 
  \subfloat[SciDocs]{\includegraphics[width=36mm]{imgs/training-amount/scidocs-did.pdf}}
  \caption{The influence of the number of training sentences (in thousands) on the model performance.}
  \label{fig:training_size}
\end{figure}

\subsection{Influence of Corpus Size} 
In certain domains, getting a sufficiently high number of (unlabeled) sentences can be challenging. Hence, data efficiency and deriving good sentence embeddings even with little unlabeled training data can be important. 

In order to study this, we train the unsupervised approaches with different corpus sizes: Between 128 and 65,536 sentences. For each experiment, we train a BERT-base-uncased model with 10 epochs up to 100k training steps. The models are evaluated at the end of each epoch and the best score on the development set is reported. 

The results are shown in Figure \ref{fig:training_size}. We observe that TSDAE is outperforming previous unsupervised learning methods often with as little as 1000 unlabeled sentences. With 10k unlabeled sentences, the downstream performance usually stagnates for all tested unsupervised sentence embedding methods. The only exception where more training data is helpful is for the CQADupStack task. This is expected, as the CQADupStack consists of 12 vastly different StackExchange forums, hence, requiring more unlabeled data to represent all domains well.

We conclude that comparatively little unlabeled data of 10k sentences is needed to tune pre-trained transformers to a specific domain.


\begin{figure}[t]
  \centering
  \includegraphics[width=80mm]{imgs/weighting-scheme/overall.pdf}
  \caption{POS tag for the most relevant content word in a sentence, i.e.\ the word that mostly influences if a sentence pair is considered as similar.}
  \label{fig:weighting_scheme_top1_overall}
\end{figure}



\subsection{Relevant Content Words}

Not all word types play an equal role in determining the semantics of a sentence. Often, nouns are the critical content words in a sentence, while e.g.\ prepositions are less important and can be add / removed from a sentences without changing the content too much.

In this section, we investigate which word types are the most relevant for the different sentence embedding methods, i.e., which words (part-of-speech tags) mainly influence if a sentence pair is perceived as similar or not. We are especially interested if we observe differences between in-domain supervised approaches (SBERT-sup.), out-of-the-box pre-trained approaches (USE, SDRoBERTa), and unsupervised approaches (TSDAE, CT, BERT-flow).  

To measure this, we select a sentence pair  that is labeled as relevant and find the word that maximally reduces the cosine-similarity score for the pair (a, b):

among all words  that appear in either a or b. Then, we record the POS tag for  and compute the distribution of POS tags across all sentence pairs. POS-tags are determined using CoreNLP~\citep{manning-etal-2014-stanford}.

The result averaged over the four datasets is shown in Figure~\ref{fig:weighting_scheme_top1_overall}. For the result on each dataset, please refer to Appendix~\ref{sec:pos_tags}. We find that nouns (NN) are by far the most relevant content words in a sentence for all methods. We do not perceive significant differences between the three types of approaches. This is good news for the unsupervised methods (TSDAE, CT, BERT-flow) and show that these can learn which words types are critical in a sentence without having labeled data. On the down side, unsupervised approaches might have issues for tasks where nouns are not the most critical content words.

When analyzing the differences between the approaches in more detail, we notice that unsupervised approaches have challenges to pick-up fine nuances for specific tasks. For example, for the AskUbuntu task, version numbers of e.g.\ the operating system plays a critical role but is missed by the unsupervised and supervised out-of-box approaches. We think it will be hard for approaches to learn such fine nuances without having labeled data.




\begin{table}[t]
\centering
\resizebox{6cm}{!}{
\begin{tabular}{|c|c|c|c|c|} 
\hline
AskU. & CQADup. & TwitterP. & SciDocs & Avg.  \\ 
\hline
140       & 2661        & 6067        & 464     & 2333  \\
\hline
\end{tabular}}
\caption{Intersection point (number of labeled sentence pairs) between unsupervised TSDAE and in-domain supervised SBERT.}
\label{tbl:equivalent_labeling_work}
\end{table}

\subsection{Equivalent Labeling Work} 
The goal of unsupervised sentence embedding learning methods is to eliminate the need of labeled training data, which can be expensive in the creation. However, as shown in Section~\ref{sec:results}, approaches with sufficient in-domain labeled data significantly outperform unsupervised approaches. 

As far as we know, previous work did not study the point of intersection between unsupervised and supervised approaches: If you only need few labeled examples to outperform unsupervised approaches, annotating those might be the more viable solution.

To find this intersection point, we train the in-domain supervised SBERT approach with varying size of labeled training data. Results are shown in Figure~\ref{fig:pretraining_effect}. To estimate the intersection with more precision, we apply binary search. We set the search precision to the standard deviation of the target score over 5 random seeds.


The results are shown in Table~\ref{tbl:equivalent_labeling_work}. To match the performance of TSDAE, 140 - 6k annotated examples are required. CQADupStack and the TwitterParaphrase corpus, which compromise various domains, require more labeled data than AskUbuntu (1 domain). Surprisingly, SciDocs, which includes data from all type of scientific domains, the in-domain supervised approach outperforms unsupervised approaches with just 464 labeled examples. This dataset appears to be especially challenging for unsupervised approaches, as we observe a large performance gap between in-domain supervised and unsupervised approaches.


In an annotation experiment on the Twitter dataset, we measured that annotating 100 Tweet pairs takes about 20 minutes for an (experienced) annotator. Hence, the state-of-the-art unsupervised TSDAE approach achieves the same performance as a supervised approach with  0.5 - 20 hours of annotation work for one annotator (2.5h - 100h for 5 crowd annotators).  




\begin{figure}[t]
  \centering
  \subfloat[AskUbuntu]{\includegraphics[width=36mm]{imgs/pretraining-effect/pretraining-effect-askubuntu-map.pdf}}
  \subfloat[CQADupStack]{\includegraphics[width=38mm]{imgs/pretraining-effect/pretraining-effect-cqadupstack-map.pdf}}
  \\
  \subfloat[TwitterPara]{\includegraphics[width=36mm]{imgs/pretraining-effect/pretraining-effect-twitter-ap.pdf}}
  \subfloat[SciDocs]{\includegraphics[width=36mm]{imgs/pretraining-effect/pretraining-effect-scidocs-map.pdf}}
  \caption{Comparison of different pre-training approaches (TSDAE/MLM/CT+SBERT) with increasing sizes of labeled training data (in thousands). SBERT: Training from the standard bert-base-uncased checkpoint. TSDAE: Unsupervised baseline.}
  \label{fig:pretraining_effect}
\end{figure}

\subsection{Usage for Pre-Training}
Another important application of unsupervised methods is the usage as pre-training method before in-domain supervised training. Previous work~\citep{DBLP:conf/acl/GururanganMSLBD20} showed that MLM pre-training on the in-domain data can significantly improve the performance for classification tasks. This lead to the question if these unsupervised approaches can be used as pre-training methods for later in-domain supervised sentence embedding training? 

We analyze the pre-training effect of TSDAE and CT\footnote{We don't include BERT-flow as it does not update the Transformer parameters.} and compare it with MLM pre-training. All approaches are trained on the full unlabeled corpus. The results are shown in Figure~\ref{fig:pretraining_effect}. 

TSDAE outperforms MLM by a significant margin for all datasets except for AskUbuntu. There, MLM works slightly better. For the other datasets, TSDAE shows a clear out-performance to other pre-training strategies. The difference is quite consistent also for larger labeled training sets. We conclude, that TSDAE works well as pre-training method and can significantly improve the performance for later supervised training even for larger training datasets.

For CT, we observe mixed results: For TwitterPara, we observe a strong improvement, for CQaDupStack a weak improvement and for AskUbuntu \& SciDocs it actually weakens the performance compared to no-domain specific pre-training. Results for CT show that a good unsupervised approach must not necessarily be a good pre-training approach.   




\begin{table}[t]
\centering
\resizebox{6cm}{!}{
\begin{tabular}{|l|c|c|} 
\hline
\textbf{Method}         & \textbf{Spearman}               & \textbf{AP}                                     \\ 
\hline
\multicolumn{3}{|l|}{ \textit{Unsupervised method} }                             \\ 
\hline
TSDAE          & 65.8                   & 59.1                                   \\
CT             & \uline{71.5}           & 59.3                                   \\
BERT-flow      & 54.0                   & 48.1                                   \\ 
DeCLUTR-base   & 70.0                  & \uline{60.1}                            \\
\hline
\multicolumn{3}{|l|}{ \textit{Out-of-the-box supervised pre-trained models} }    \\ 
\hline
SBERT-base-NLI & 74.9                   & \uline{\textbf{70.8}}                  \\
SDRoBERTa-para & \uline{\textbf{77.5}}  & 66.7                                   \\
USE-large      & 76.3                   & 68.9                                   \\
\hline
\end{tabular}}
\caption{Evaluation on the STS datasets.}
\label{tbl:sts_results}
\end{table}

\section{Semantic Textual Similarity}
\label{sec:STS_results}

Previous unsupervised sentence embedding learning approaches~\citep{DBLP:journals/corr/abs-2006-03659,carlsson2021semantic,DBLP:conf/emnlp/LiZHWYL20,su2021whitening} primarily evaluated on the task of Semantic Textual Similarity (STS) with data from SemEval \citep{DBLP:conf/semeval/AgirreCDG12,DBLP:conf/starsem/AgirreCDGG13,DBLP:conf/semeval/AgirreBCCDGGMRW14,DBLP:conf/semeval/AgirreBCCDGGLMM15,DBLP:conf/semeval/AgirreBCDGMRW16}, the STS benchmark~\citep{cer-etal-2017-semeval}, and from the SICK-Relatedness dataset~\citep{DBLP:conf/lrec/MarelliMBBBZ14} using Pearson or Spearman's rank correlation. 

We find the (sole) evaluation on STS problematic for several reasons. First, the STS datasets consists of sentences which don't requiring domain specific knowledge, they are primarily from news and image captions. Pre-trained models like USE and SBERT, which use labeled data from other tasks, significantly outperform unsupervised approaches on this general domain. It remains unclear if the proposed unsupervised approach will work better for specific domains. Second, the STS datasets have an artificial score distribution that dissimilar and similar pairs appear roughly equally. For most real-word tasks, there is an extreme skew and only a tiny fraction of pairs are considered similar. Third, to perform well on the STS datasets, a method must rank dissimilar pairs and similar pairs equally well. A method that identifies perfectly similar pairs, but has issues to differentiate between various types of dissimilar pairs, would score badly on the STS datasets. In contrast, most real-world tasks, like duplicate questions detection, related paper finding, or paraphrase mining, only require to identify the few similar pairs out of a pool of millions of possible combinations. Overall, we think that the performance on the STS datasets does not correlate well with downstream task performance. This has also been previously shown in \newcite{DBLP:conf/coling/ReimersBG16} for various unsupervised approaches.

In order to make our work still comparable to previous work, we conduct experiments on the STS datasets. We sample sentences from Wikipedia as done by \citet{carlsson2021semantic} and train unsupervised approaches based on pre-trained transformers on it. Following previous work, we use Spearman's rank correlation. As mentioned, correlation has the downside that it emphasizes the whole score range of 0 to 5 equally, but most tasks are only interested in the highly similar pairs. To address this, we also report Average Precision (AP) and view the labeled scores larger than 4 as positive and the other as negative. 

The results\footnote{Averaged scores over all the datasets are reported. For detailed results on each STS dataset, please refer to Appendix~\ref{sec:detailed_sts}.} are shown in Table~\ref{tbl:sts_results}. The AP score of our proposed TSDAE method is on par with CT and falls behind it for Spearman correlation. This indicates TSDAE works well on finding similar sentence pairs, while ignoring how dissimilar the negative pairs are. 

Unsupervised approaches perform much worse than the out-of-the-box supervised pre-trained models (SBERT, SDRoBERTa and USE), even though those did not use any STS specific training data. Thus, we argue that more emphasis of the evaluation should be put on specific tasks and domains as we do in this work. 


\section{Conclusion}
In this work, we study unsupervised sentence embeddings based on pre-trained Transformers on different tasks from heterogeneous domains. Our proposed TSDAE approach significantly outperforms other, state-of-the-art unsupervised methods by up-to 6.4 points (2.6 points on average across all tasks). Further, we show that TSDAE works well as pre-training method, significantly outperforming the popular MLM on multiple datasets.





\section*{Acknowledgments}
This work has been supported by the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1 and grant GU 798/17-1) and has been funded by the  German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE.



\bibliography{anthology,emnlp2020}
\bibliographystyle{acl_natbib}

\onecolumn
\appendix

\section{Optimal Configuration of TSDAE}
\label{sec:TSDAE_config}
To obtain the optimal configuration, we compare TSDAE models trained and evaluated on the general domain without bias towards any specific domain. The greedy search is applied by sequentially finding the best (1) noise type and ratio (2) pooling method and (3) weight tying scheme. Similar to the choice of CT and BERT-flow, we train the models on the combination of SNLI and MultiNLI without labels and evaluate the models on the STS Benchmark (STS-B) with the metric of Spearman rank correlation. The maximum number of training steps is 30K and the models are evaluated every 1.5K training steps, reporting the best validation performance. Scores are obtained by calculating the average over 5 random seeds.

We first compare the scores of different noise types, fixing the noise ratio as 0.3 (i.e. 30\% tokens are influenced) and the pooling method as CLS pooling. The results are show in Table~\ref{tbl:noise_type}. This indicates deletion is the best noise type. We then tune the noise ratio of the deletion noise and the results are shown in Table~\ref{tbl:noise_ratio}. This indicates 0.6 is the best noise ratio. 

\begin{table}[H]
\centering
\resizebox{6cm}{!}{
\begin{tabular}{|c|c|c|c|c|} 
\hline
 Delete         & Swap  & Mask  & Replace & Add    \\ 
\hline
 \textbf{78.33} & 76.85 & 76.56 & 74.01~  & 72.65  \\
\hline
\end{tabular}}
\caption{Results with different noise types}
\label{tbl:noise_type}
\end{table}




\begin{table}[H]
\centering
\resizebox{12cm}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|} 
\hline
Ratio & 0.1   & 0.2   & 0.3   & 0.4   & 0.5   & 0.6   & 0.7   & 0.8   & 0.9    \\ 
\hline
Score & 77.81 & 77.70 & 77.75 & 78.02 & 78.25 & \textbf{78.77} & 78.19 & 77.69 & 75.67  \\
\hline
\end{tabular}}
\caption{Results with different noise ratio.}
\label{tbl:noise_ratio}
\end{table}

We then compare different pooling methods with the best setting so far. The results are shown in Table~\ref{tbl:pooling}. Since there is little difference between CLS and mean pooling and mean pooling loses the position information, the CLS pooling is chosen. Finally, we find that tying the encoder and the decoder can further improve the validation score to 79.15.

\begin{table}[H]
\centering
\resizebox{3.3cm}{!}{
\begin{tabular}{|c|c|c|} 
\hline
 CLS   & Mean           & Max     \\ 
\hline
 78.77 & \textbf{78.84} & 78.17~  \\
\hline
\end{tabular}}
\caption{Results with different pooling methods.}
\label{tbl:pooling}
\end{table}



\section{Data Statistics}
\label{sec:data_details}
The statistics of the four datasets used in this work are shown in Table~\ref{tbl:data_statistics}. Multiple sub-datasets are included in CQADupStack, SciDocs and TwitterPara. CQADupStack has one sub-dataset for each of the 12 forums. The avg. \#relevant, avg. \#candidates and avg. length in Table~\ref{tbl:data_statistics} are all general statistics without distinguishing the sub-datasets.

\begin{table}[H]
\centering
\resizebox{14cm}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|c|} 
\hline
Dataset             & Task          & \#queries     & \begin{tabular}[c]{@{}c@{}}Avg.\\\#relevant \end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg.\\\#candidates \end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\length \end{tabular} & \begin{tabular}[c]{@{}c@{}}Size of \\unsupervised\\training set \end{tabular} & \begin{tabular}[c]{@{}c@{}}Size of \\supervised \\training set \end{tabular}  \\ 
\hline
\vcell{AskUbuntu}   & \vcell{RR}    & \vcell{200}   & \vcell{5.9/5.4}                                           & \vcell{20}                                                  & \vcell{9.2}                                            & \vcell{165K}                                                                  & \vcell{23K}                                                                   \-\rowheight]
\printcelltop       & \printcelltop & \printcelltop & \printcelltop                                             & \printcelltop                                               & \printcelltop                                          & \printcelltop                                                                 & \printcelltop                                                                 \\
\vcell{SciDocs}     & \vcell{RR}    & \vcell{4K}    & \vcell{5}                                                 & \vcell{30}                                                  & \vcell{12.5}                                           & \vcell{312K}                                                                  & \vcell{380K}                                                                  \-\rowheight]
\printcelltop       & \printcelltop & \printcelltop & \printcelltop    & \printcelltop                                          & \printcelltop                                                                 & \printcelltop                                                                 \\
\hline
\end{tabular}}

\caption{Dataset statistics. The slash symbol `/' separates the numbers for development and test.}


\label{tbl:data_statistics}
\end{table}

\section{Experiment Settings}
\label{sec:experiment_settings}
We implement TSDAE, CT and BERT-flow based on Pytorch and Huggingface's Transformers\footnote{\url{https://github.com/huggingface/transformers}}. For these three unsupervised methods, following the original papers, the number of training steps is 100K; the batch size is 8; the optimizers are AdamW, RMSProp and AdamW, respectively; the initial learning rates are 3e-5, 1e-5 and 1e-6, resp. The weight decay for BERT-flow is 0.01. The learning rate for CT follows a segmented-constant scheduling scheme: 1e-5 for step 1 to 500; 8e-6 for step 501 to 1000; 6e-6 for step 1001 to 1500; 4e-6 for step 1501 to 2000; 2e-6 for others. The pooling method for CT and BERT-flow is both mean pooling. Since CT trains two independent encoders and we find the second encoder has better performance, we use the second encoder for evaluation. 

We use the repository of sentence-transformers\footnote{\url{https://github.com/UKPLab/sentence-transformers}} to train the in-domain supervised models. For them, the number of training epochs is 10; the maximum number of training steps is 20K; the batch size is 64; the similarity function  is set to cosine similarity; early-stopping is applied by checking the validation performance. To eliminate the influence of randomness, we report the scores averaged over 5 random seeds for all the in-domain unsupervised and supervised models. All the pre-trained checkpoints used are listed in Table~\ref{tbl:checkpoints}.



\begin{table}[H]
\centering
\resizebox{14.5cm}{!}{
\begin{tabular}{|l|l|} 
\hline
Model Name         & URL                                                                                    \\ 
\hline
DeCLUTR-base      & https://huggingface.co/johngiorgi/declutr-base 
   \\
ELECTRA-base       & https://huggingface.co/google/electra-base-discriminator                      \\
DistilRoBERTa-base & https://huggingface.co/distilroberta-base                                    \\
RoBERTa-base       & https://huggingface.co/roberta-base                                           \\
DistilBERT-base    & https://huggingface.co/distilbert-base-uncased                                 \\
BERT-base          & https://huggingface.co/bert-base-uncased                                       \\
SBERT-base-NLI     & https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens         \\
SDRoBERTa-para     & https://huggingface.co/sentence-transformers/paraphrase-distilroberta-base-v1  \\
USE-large       & https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3       \\
\hline
\end{tabular}}
\caption{Model checkpoints used in this work.}
\label{tbl:checkpoints}
\end{table}

\section{Results of Other Checkpoints}
\label{sec:other_checkpoints}

The results of other checkpoints besides \textit{BERT-base-uncased} are shown in Table~\ref{tbl:other_checkpoints}. For all the methods, better results are achieved by using BERT checkpoints, which also makes TSDAE significantly outperforms others. We suppose this advantage comes from the additional pre-training task, next sentence prediction of the BERT models, which guides the model to learn from sentence-level contexts.




\begin{table}
\centering
\resizebox{13cm}{!}{
\begin{tabular}{|l|c|c|c|c|c|} 
\hline
Method    & AskUbuntu                     & CQADupStack                   & TwitterPara                   & SciDocs                       & Avg.                           \\ 
\hline
\multicolumn{6}{|l|}{ \textit{ELECTRA-base} }                                                                                                                              \\ 
\hline
CT        & 50.3 +/- 0.4                  & 5.0 +/- 0.2                   & 66.5 +/- 0.7                  & 46.1 +/- 0.6                  & 41.6 +/- 0.8                   \\
TSDAE     & \uline{56.6 +/- 1.1}          & \uline{8.0 +/- 0.3}           & \uline{69.0 +/- 1.6}          & \uline{66.2 +/- 5.6}          & \uline{49.9 +/- 1.1}           \\
BERT-flow & 51.3 +/- 0.3                  & 5.2 +/- 0.0                   & 62.4 +/- 0.1                  & 41.2 +/- 0.1                  & 38.4 +/- 3.0                   \\ 
\hline
\multicolumn{6}{|l|}{ \textit{DistilRoBERTa-base} }                                                                                                                        \\ 
\hline
CT        & 57.9 +/- 0.8                  & \uline{13.8 +/- 0.3}          & 63.6 +/- 1.4                  & \uline{62.7 +/- 0.5}          & 49.8 +/- 0.4                   \\
TSDAE     & \uline{58.9 +/- 0.5}          & 12.5 +/- 0.1                  & \uline{68.5 +/- 0.5}          & 59.3 +/- 0.6                  & \uline{49.9 +/- 0.4}           \\
BERT-flow & 56.0 +/- 0.2                  & 11.1 +/- 0.1                  & 68.5 +/- 0.1                  & 53.0 +/- 0.2                  & 46.9 +/- 0.1                   \\ 
\hline
\multicolumn{6}{|l|}{ \textit{RoBERTa-base} }                                                                                                                              \\ 
\hline
CT        & 56.7 +/- 0.5                  & \uline{14.2 +/- 0.3}          & 69.4 +/- 1.3                  & \uline{63.1 +/- 0.3}          & \uline{50.5 +/- 0.4}           \\
TSDAE     & \uline{58.3 +/- 0.7}          & 12.2 +/- 0.3                  & \uline{70.0 +/- 0.8}          & 61.4 +/- 0.5                  & 50.3 +/- 0.3                   \\
BERT-flow & 54.5 +/- 0.2                  & 10.5 +/- 0.1                  & 69.0 +/- 0.1                  & 53.5 +/- 0.2                  & 46.6 +/- 0.1                   \\ 
\hline
\multicolumn{6}{|l|}{ \textit{DistilBERT-base} }                                                                                                                           \\ 
\hline
CT        & 57.7 +/- 0.8                  & 14.0 +/- 0.3                  & 66.4 +/- 0.4                  & 72.2 +/- 0.7                  & 52.3 +/- 0.3                   \\
TSDAE     & \uline{59.2 +/- 0.3}          & \uline{\textbf{14.6 +/- 0.1}} & \uline{\textbf{73.9 +/- 0.3}} & \uline{72.3 +/- 0.9}          & \uline{54.9 +/- 0.2}           \\
BERT-flow & 55.0 +/- 0.2                  & 11.0 +/- 0.0                  & 65.9 +/- 0.0                  & 70.5 +/- 0.1                  & 50.5 +/- 0.1                   \\ 
\hline
\multicolumn{6}{|l|}{ \textit{BERT-base} }                                                                                                                                 \\ 
\hline
CT        & 56.3 +/- 0.7                  & 13.3 +/- 0.3                  & 72.5 +/- 0.5                  & 67.6 +/- 0.4                  & 52.4 +/- 0.3                   \\
TSDAE     & \uline{\textbf{59.4 +/- 0.3}} & \uline{14.5 +/- 0.1}          & \uline{73.0 +/- 0.4}          & \uline{\textbf{74.0 +/- 0.4}} & \uline{\textbf{55.0 +/- 0.2}}  \\
BERT-flow & 53.7 +/- 0.2                  & 9.2 +/- 0.1                   & 69.2 +/- 0.2                  & 64.5 +/- 0.1                  & 49.1 +/- 0.1                   \\
\hline
\end{tabular}}
\caption{Evaluation of different checkpoints using average precision. `+/-' separates the mean value and standard deviation over scores of 5 random seeds.}
\label{tbl:other_checkpoints}
\end{table}

\section{Influence of Number of Training Steps}
\label{sec:training_dynamics}
The influence of the number of training steps for AskUbuntu, CQADupStack and TwitterPara is shown in Figure~\ref{fig:training_dynamics_lg}.

\begin{figure}[H]
  \centering
  \subfloat[AskUbuntu]{\includegraphics[width=57mm]{imgs/training-dynamics/training-dynamics-askubuntu.pdf}}
  \quad\quad
  \subfloat[CQADupStack]{\includegraphics[width=57mm]{imgs/training-dynamics/training-dynamics-cqadupstack.pdf}} \\
  \subfloat[TwitterPara]{\includegraphics[width=57mm]{imgs/training-dynamics/training-dynamics-twitter.pdf}} 
  \quad\quad
  \subfloat[SciDocs]{\includegraphics[width=57mm]{imgs/training-dynamics/training-dynamics-scidocs.pdf}}
  \caption{The influence of the number of training steps on the model performance.}
  \label{fig:training_dynamics_lg}
\end{figure}

\section{Influence of Corpus Size}
\label{sec:training_size}
The influence of corpus size for AskUbuntu, CQADupStack and TwitterPara is shown in Figure~\ref{fig:training_size_lg}.

\begin{figure}[H]
  \centering
  \subfloat[AskUbuntu]{\includegraphics[width=57mm]{imgs/training-amount-lg/askubuntu-did.pdf}}
  \quad\quad
  \subfloat[CQADupStack]{\includegraphics[width=57mm]{imgs/training-amount-lg/cqadupstack-did.pdf}} \\
  \subfloat[TwitterPara]{\includegraphics[width=57mm]{imgs/training-amount-lg/twitter-did.pdf}} 
  \quad\quad
  \subfloat[SciDocs]{\includegraphics[width=57mm]{imgs/training-amount-lg/scidocs-did.pdf}}
  \caption{The influence of the number of training sentences on the model performance.}
  \label{fig:training_size_lg}
\end{figure}

\section{Usage for Pre-Training}
\label{sec:training_size}
The pre-training performance on AskUbuntu, CQADupStack and TwitterPara is shown in Figure~\ref{fig:pretraining_effect_lg}.

\begin{figure}[H]
  \centering
  \subfloat[AskUbuntu]{\includegraphics[width=57mm]{imgs/pretraining-effect-lg/pretraining-effect-askubuntu-map.pdf}}
  \quad\quad
  \subfloat[CQADupStack]{\includegraphics[width=57mm]{imgs/pretraining-effect-lg/pretraining-effect-cqadupstack-map.pdf}} \\
  \subfloat[TwitterPara]{\includegraphics[width=57mm]{imgs/pretraining-effect-lg/pretraining-effect-twitter-ap.pdf}} 
  \quad\quad
  \subfloat[SciDocs]{\includegraphics[width=57mm]{imgs/pretraining-effect-lg/pretraining-effect-scidocs-map.pdf}}
  \caption{The influence of the number of training sentences on the model performance.}
  \label{fig:pretraining_effect_lg}
\end{figure}

\section{Influence of Different POS Tags}
\label{sec:pos_tags}
The influence of different POS tags on the output similarity scores for AskUbuntu, CQADupStack and TwitterPara is shown in Figure~\ref{fig:pos_tags_influence}.

\begin{figure}[H]
  \centering
  \subfloat[AskUbuntu]{\includegraphics[width=75mm]{imgs/weighting-scheme/askubuntu.pdf}}
  \subfloat[CQADupStack]{\includegraphics[width=75mm]{imgs/weighting-scheme/cqadupstack.pdf}} \\
  \subfloat[TwitterPara]{\includegraphics[width=75mm]{imgs/weighting-scheme/twitter.pdf}} 
  \subfloat[SciDocs]{\includegraphics[width=75mm]{imgs/weighting-scheme/scidocs.pdf}}
  \caption{The influence of different POS tags on the output similarity scores.}
  \label{fig:pos_tags_influence}
\end{figure}

\section{Detailed Results of Semantic Textual Similarity}
\label{sec:detailed_sts}

The detailed results of STS on each dataset are shown in Table~\ref{tbl:sts_spearman} and Table~\ref{tbl:sts_ap} with the evaluation metric of Spearman's rank correlation and average precision, respectively.

\begin{table}[H]
\centering
\resizebox{13cm}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|} 
\hline
Method         & STS12                  & STS13                  & STS14                  & STS15                  & STS16                  & STSb                   & SICK-R                 & Avg.                    \\ 
\hline
\multicolumn{9}{|l|}{ \textit{Unsupervised method based on BERT-base} }                                                                                                                                                 \\ 
\hline
TSDAE          & 55.2                   & 67.4                   & 62.4                   & 74.3                   & 73.0                   & 66.0                   & 62.3                   & 65.8                    \\
CT             & \uline{60.0}           & \uline{76.3}           & \uline{68.2}           & \uline{77.3}           & 75.8                   & \uline{73.9}           & \uline{69.4}           & \uline{71.5}            \\
BERT-flow      & 34.1                   & 60.7                   & 48.8                   & 61.9                   & 64.8                   & 48.9                   & 58.4                   & 54.0                    \\
DeCLUTR-base   & 52.4                   & 75.2                   & 65.5                   & 77.1                   & \uline{78.6}           & 72.4                   & 68.6                   & 70.0                    \\ 
\hline
\multicolumn{9}{|l|}{ \textit{Out-of-the-box supervised pre-trained models} }                                                                                                                                           \\ 
\hline
SBERT-base-NLI & 71.0                   & \uline{\textbf{76.5}}  & \uline{\textbf{73.2}}  & 79.1                   & 74.3                   & 77.0                   & 72.9                   & 74.9                    \\
SDRoBERTa-para & 69.2                   & 79.9                   & 72.9                   & \uline{\textbf{83.3}}  & \uline{\textbf{80.3}}  & \uline{\textbf{82.2}}  & 74.8                   & \uline{\textbf{77.5}}   \\
USE-large      & \uline{\textbf{74.3}}  & 71.8                   & 71.4                   & 82.5                   & 77.5                   & 80.9                   & \uline{\textbf{75.8}}  & 76.3                    \\
\hline
\end{tabular}}
\caption{Evaluation on the task of STS using Spearman's rank correlation.}
\label{tbl:sts_spearman}
\end{table}

\begin{table}[H]
\centering
\resizebox{13cm}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|} 
\hline
Method         & STS12                 & STS13                 & STS14                 & STS15                 & STS16                 & STSb                  & SICK-R                & Avg.                   \\ 
\hline
\multicolumn{9}{|l|}{ \textit{Unsupervised method based on BERT-base} }                                                                                                                                         \\ 
\hline
TSDAE          & 70.4                  & \uline{55.7}          & \uline{54.5}          & 64.3                  & 56.2                  & 54.0                  & 58.4                  & 59.1                   \\
CT             & \uline{71.3}          & 51.9                  & 51.6                  & 61.4                  & 59.5                  & 54.2                  & 65.0                  & 59.3                   \\
BERT-flow      & 59.8                  & 41.9                  & 40.6                  & 48.1                  & 51.9                  & 36.0                  & 58.1                  & 48.1                   \\
DeCLUTR-base   & 68.0                  & 51.6                  & 50.4                  & \uline{65.6}          & \uline{62.3}          & \uline{54.9}          & \uline{67.6}          & \uline{60.1}           \\ 
\hline
\multicolumn{9}{|l|}{ \textit{Out-of-the-box supervised pre-trained models} }                                                                                                                                   \\ 
\hline
SBERT-base-NLI & 77.8                  & \textbf{\uline{65.0}} & \textbf{\uline{63.7}} & 75.3                  & \textbf{\uline{66.2}} & 67.4                  & \textbf{\uline{80.1}} & \textbf{\uline{70.8}}  \\
SDRoBERTa-para & 76.7                  & 55.1                  & 57.3                  & 73.8                  & 63.3                  & 66.4                  & 74.3                  & 66.7                   \\
USE-large      & \textbf{\uline{80.3}} & 55.4                  & 58.0                  & \textbf{\uline{76.4}} & 62.1                  & \textbf{\uline{73.7}} & 76.2                  & 68.9                   \\
\hline
\end{tabular}}
\caption{Evaluation on the task of STS using average precision.}
\label{tbl:sts_ap}
\end{table}

\end{document}
