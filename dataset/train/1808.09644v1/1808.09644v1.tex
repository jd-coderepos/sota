\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2018}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{paralist}
\usepackage{url}
\usepackage{booktabs}
\usepackage{bm}


\aclfinalcopy 

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP 2018}
\newcommand\conforg{SIGDAT}

\newcommand{\haoyue}[1]{\textcolor{red}{[Haoyue: #1]}}
\newcommand{\TODO}[1]{\textcolor{red}{[zhouh: #1]}}

\title{On Tree-Based Neural Sentence Modeling}

\author{Haoyue Shi \thanks{~Now at Toyota Technological Institute at Chicago, {\tt freda@ttic.edu}. This work was done when HS was an intern researcher at ByteDance AI Lab. } ~~~~~~ Hao Zhou ~~~~~~ Jiaze Chen ~~~~~~ Lei Li \\
: School of EECS, Peking University, Beijing, China \\
{\tt hyshi@pku.edu.cn}
\\
: ByteDance AI Lab, Beijing, China \\
{\tt \{zhouhao.nlp, chenjiaze, lileilab\}@bytedance.com}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Neural networks with tree-based sentence encoders have shown better results on many downstream tasks.
Most of existing tree-based encoders adopt 
syntactic parsing trees as the explicit structure prior. 
To study the effectiveness of different tree structures, we replace the parsing trees with trivial trees (\textit{i.e.}, binary balanced tree, left-branching tree and right-branching tree) in the encoders. 
Though trivial trees contain no syntactic information, 
those encoders get competitive or even better results on all of the ten downstream tasks we investigated. 
This surprising result indicates that explicit syntax guidance may not be the main contributor to the superior performances of tree-based neural sentence modeling.
Further analysis show that tree modeling gives better results when crucial words are closer to the final representation.
Additional experiments give more clues on how to design an effective tree-based encoder.
Our code is open-source and available at \url{https://github.com/ExplorerFreda/TreeEnc}. 
\end{abstract}


\section{Introduction}

Sentence modeling is a crucial problem in natural language processing~(NLP).
Recurrent neural networks with long short term memory \cite{hochreiter1997long} or gated recurrent units  \cite{cho2014learning}
are commonly used sentence modeling approaches. 
These models embed sentences into a vector space and the resulting vectors can be used for classification or sequence generation in the downstream tasks.

In addition to the plain sequence of hidden units, recent work on sequence modeling proposes to impose tree structure 
in the encoder \cite{socher2013recursive,tai2015improved,zhu2015long}. 
These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling.
They have already obtained promising results in many NLP tasks, such as natural language inference \cite{bowman2016fast,chen2017enhanced} and machine translation \cite{eriguchi2016tree,chen2017improved,chen2017neural,P17-2092}.
\newcite{li2015tree} empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features.

On the other hand, some works propose to abandon the syntax tree 
but to adopt the latent tree for sentence modeling \cite{choi2017learning,yogatama2017learning,maillard2017jointly,williams2018latent}. 
Such latent trees are directly learned from the downstream task with reinforcement learning \cite{williams1992simple} or Gumbel Softmax \cite{jang2016categorical,maddison2016concrete}.
However, \newcite{williams2018latent} empirically show that, Gumbel softmax produces unstable latent trees with the same hyper-parameters but different initializations, while reinforcement learning \cite{williams2018latent} even tends to generate left-branching trees.
Neither gives meaningful latent trees in syntax, but each method still obtains considerable improvements in performance.
This indicates that syntax may not be the main contributor to the performance gains.

With the above observation, we bring up the following questions: 
What does matter in tree-based sentence modeling?
If tree structures are necessary in encoding the sentences, what mostly contributes to the improvement in downstream tasks?
We attempt to investigate the driving force of the improvement by latent trees without syntax. 

In this paper, we empirically study the effectiveness of tree structures in sentence modeling. 
We compare the performance of bi-LSTM and five tree LSTM encoders with different tree layouts, including the syntax tree, latent tree~(from Gumbel softmax) and three kinds of designed trivial trees~(binary balance tree, left-branching tree and right-branching tree).
Experiments are conducted on 10 different tasks, which are grouped into three categories, namely the single sentence classification (5 tasks), sentence relation classification (2 tasks), and sentence generation (3 tasks).
These tasks depend on different granularities of features, and the comparison among them can help us learn more about the results.
We repeat all the experiments 5 times and take the average to avoid the instability caused by random initialization of deep learning models.

We get the following conclusions:
\begin{compactitem}
\item Tree structures are helpful to sentence modeling on classification tasks, especially for tasks which need global~(long-term) context features, which is consistent with previous findings \cite{li2015tree}.
\item Trivial trees outperform syntactic trees, indicating that syntax may not be the main contributor to the gains of tree encoding, at least on the ten tasks we investigate.
\item Further experiments shows that, given strong priors, tree based methods give better results when crucial words are closer to the final representation. If structure priors are unavailable, balanced tree is a good choice, as it makes the path distances between word and sentence encoding to be roughly equal, and in such case, tree encoding can learn the crucial words itself more easily.  
\end{compactitem}

\begin{figure}[t!]
\centering
\subfloat[Encoder-decoder framework for sentence generation.]{
\includegraphics[width=0.2\textwidth]{generation-upd}
}
~~~~~
\subfloat[Encoder-classifier framework for sentence classification.] {
\includegraphics[width=0.2\textwidth]{classification-upd}
}
\\
\subfloat[Siamese encoder-classifier framework for sentence relation classification.]{
\includegraphics[width=0.35\textwidth]{sentence-relation-upd}
}
\caption{\label{fig:model-structures} The encoder-classifier/decoder framework for three different groups of tasks. We apply multi-layer perceptron (MLP) for classification, and left-to-right decoders for generation in all experiments. \-0.8cm]}
\end{table*}

In this section, we present our experimental results and analysis. 
Section~\ref{sec:set-up} introduces our set-up for all the experiments.
Section~\ref{sec:main-results} shows the main results and analysis on ten downstream tasks grouped into three classes, which can cover a wide range of NLP applications. 
Regarding that trivial tree based LSTMs perform the best among all models, we draw two hypotheses, which are i) right-branching tree benefits a lot from strong structural priors; ii) balanced tree wins because it fairly treats all words  so that crucial information could be more easily learned by the LSTM gates automatically. We test the hypotheses in Section~\ref{sec:trivial}. 
Finally, we compare the performance of linear and tree LSTMs with three widely applied pooling mechanisms in Section~\ref{sec:pooling}.


\subsection{Set-up}
\label{sec:set-up}
In experiments, we fix the structure of the classifier as a two-layer MLP with ReLU activation, and the structure of decoder as GRU-based recurrent neural networks \cite{cho2014learning}. \footnote{We observe that ReLU can significantly boost the performance of Bi-LSTM on SNLI.} 
The hidden-layer size of MLP is fixed to 1024, while that of GRU is adapted from the size of sentence encoding. 
We initialize the word embeddings with 300-dimensional {\tt GloVe} \cite{pennington2014glove} vectors.\footnote{\url{http://nlp.stanford.edu/data/glove.840B.300d.zip}}
We apply 300-dimensional bidirectional (600-dimensional in total) LSTM as leaf RNN when necessary.
We use Adam \cite{kingma2014adam} optimizer to train all the models, with the learning rate of 1e-3 and batch size of 64. 
In the training stage, we drop the samples with the length of either source sentence or target sentence larger than 64. 
We do not apply any regularization or dropout term in all experiments except the task of WSR, on which we tune dropout term with respect to the development set. 
We generate the binary parsing tree for the datasets without parsing trees using ZPar \cite{zhang2011syntactic}.\footnote{\url{https://www.sutd.edu.sg/cmsresource/faculty/yuezhang/zpar.html}}
More details are summarized in supplementary materials. 

\subsection{Main Results}
\label{sec:main-results}
In this subsection, we aim to compare the results from different encoders. We do not include any attention \cite{wang2016attention,lin2017astructured} or pooling \cite{collobert2008unified,socher2011dynamic,zhou2016text} mechanism here, in order to avoid distractions and make the encoder structure affects the most. 
We will further analyze pooling mechanisms in Section \ref{sec:pooling}.

Table~\ref{table:main-result} presents the performances of different encoders on a variety of downstream tasks, which lead to the following observations:
\paragraph{Tree encoders are useful on some tasks.} We get the same conclusion with \newcite{li2015tree} that tree-based encoders perform better on tasks requiring long-term context features.
Despiting the linear structured left-branching and right-branching tree encoders, we find that, tree-based encoders generally perform better than Bi-LSTMs on tasks of \textit{sentence relation} and \textit{sentence generation}, which may require relatively more long term context features for obtaining better performances.
However, the improvements of tree encoders on \textbf{NLI} and \textbf{Para} are relatively small, which may be caused by that sentences of the two tasks are shorter than others, and the tree encoder does not get enough advantages to capture long-term context in short sentences.


\paragraph{Trivial tree encoders outperform other encoders.}
Surprisingly, binary balanced tree encoder gets the best results on most tasks of classification and right-branching tree encoder tends to be the best on sentence generation.
Note that binary balanced tree and right-branching tree are only trivial tree structures, but outperform syntactic tree and latent tree encoders.
The latent tree is really competitive on some tasks, as its  structure is directly tuned by the corresponding tasks.
However, it only beats the binary balanced tree by very small margins on NLI and ARP.
We will give analysis about this in Section \ref{sec:trivial}.


\paragraph{Larger quantity of parameters is not the only reason of the improvements.}
Table \ref{table:main-result} shows that tree encoders benefit a lot from adding leaf-LSTM, which brings not only sentence level information to leaf nodes, but also more parameters than the bi-LSTM encoder.
However, left-branching tree LSTM has a quite similar structure with linear LSTM, and it can be viewed as a linear LSTM-on-LSTM structure. 
It has the same amounts of parameters as other tree-based encoders, but still falls behind the balance tree encoder on most of the tasks.
This indicates that larger quantity of parameters is at least not the only reason for binary balance tree LSTM encoders to gain improvements against bi-LSTMs.

 
\subsection{Why Trivial Trees Work Better?}
\label{sec:trivial}


Binary balanced tree and right-branching are trivial ones, hardly containing syntax information.
In this section, we analyze why these trees achieve high scores in deep. 


\subsubsection{Right Branching Tree Benefits from Strong Structural Prior}
\label{sec:right-better}
\begin{figure*}[t]
    \centering
    \subfloat[Balanced tree, MT.]{
    \includegraphics[width=0.24\textwidth]{mt-balance.pdf}
    }
    \subfloat[Left-branching tree, MT.]{
    \includegraphics[width=0.24\textwidth]{mt-left.pdf}
    }
      \subfloat[Right-branching, MT.]{
    \includegraphics[width=0.24\textwidth]{mt-right.pdf}
    } 
    \subfloat[Bi-LSTM, MT.]{
    \includegraphics[width=0.24\textwidth]{mt-bilstm.pdf}
    }
    
    \subfloat[Balanced tree, AE.]{
    \includegraphics[width=0.24\textwidth]{ae-balance.pdf}
    }
        \subfloat[Left-branching tree, AE.]{
    \includegraphics[width=0.24\textwidth]{ae-left.pdf}
    }
          \subfloat[Right-branching, AE.]{
    \includegraphics[width=0.24\textwidth]{ae-right.pdf}
    }
    \subfloat[Bi-LSTM, AE.]{
     \includegraphics[width=0.24\textwidth]{ae-bilstm.pdf}
    }
    \caption{\label{fig:saliency} Saliency visualization of words in learned MT and AE models. Darker means more important to the sentence encoding. }
\end{figure*}

We argue that right-branching trees benefit from its strong structural prior.
In sentence generation tasks, models generate sentences from left to right, which makes words in the left of the source sentence more important~\cite{sutskever2014sequence}.
If the encoder fails to memorize the left words, the information about right words would not help due to the error propagation.
In right-branching trees, left words of the sentence are closer to the final representation, which makes the left words are more easy to  be memorized, and we call this structure prior.
Oppositely, in the case of  left-branching trees, right words of the sentence are closer to the representation.

To validate our hypothesis, we propose to visualize the Jacobian as word-level saliency \cite{shi2018learning}, which can be viewed as the contribution of each word to the sentence encoding: 

where  denotes the embedding of a sentence, and  denotes embedding of a word.
We can compute the saliency score using backward propagation.
For a word in a sentence, higher saliency score means more contribution to  sentence encoding. 

We present the visualization in Figure~\ref{fig:saliency} using the visualization tool from \newcite{lin2017astructured}.
It shows that right-branching tree LSTM encoders tend to look at the left part of the sentence, which is very helpful to the final generation performance, as left words are more crucial.
Balanced trees also have this feature and we think it is because balance tree treats these words fairly, and crucial information could be more easily learned by the LSTM gates automatically.

However, bi-LSTM and left-branching tree LSTM also pay much attention to words in the right~(especially the last two words), which maybe caused by the short path from the right words to the root representation, in the two corresponding tree structures. 

Additionally, Table~\ref{table:pearson} shows that models trained with the same hyper-parameters but different initializations have strong agreement with each other. 
Thus, ``looking at the first words'' is a stable behavior of balanced and right-branching tree LSTM encoders in sentence generation tasks. So is ``looking at the first and the last words'' for Bi-LSTMs and left-branching tree LSTMs.


\begin{table}[t]
    \centering
    \begin{tabular}{lrr}
         \toprule
         \textbf{Model} & \textbf{MT} & \textbf{AE} \\
         \midrule
         Balanced (BiLRNN)  & 93.1  & 96.9 \\
         Left-Branching (BiLRNN) & 94.2 & 95.4 \\
         Right-Branching (BiLRNN) & 92.3 & 95.1 \\
         Bi-LSTM            & 96.4  & 96.1 \\
         \bottomrule
    \end{tabular}
    \caption{\label{table:pearson} Mean average Pearson correlation  across five models trained with same hyper-parameters. For each testing sentence, we compute the saliency scores of words. Cross-model Pearson correlation can show the agreement of two models on one sentence, and average Pearson correlation is computed through all sentences. We report mean average Pearson correlation of the  model pairs. }
\end{table}

\subsubsection{Binary Balanced Tree Benefits from Shallowness}
\label{sec:balanced-better}
\begin{figure}[t!]
    \centering
    \subfloat[-depth line for WSR.]{
    \includegraphics[width=0.21\textwidth]{semrel-depth.pdf}
    }
    ~~\subfloat[-Acc. line for WSR.]{
    \includegraphics[width=0.21\textwidth]{semrel-performance.pdf}
    }
    \\
    \subfloat[-depth line for MT.]{
    \includegraphics[width=0.21\textwidth]{MT-depth.pdf}
    }
    ~~\subfloat[-BLEU line for MT.]{
    \includegraphics[width=0.21\textwidth]{MT-performance.pdf}
    }
    \\
    \subfloat[-depth line for AE.]{
    \includegraphics[width=0.21\textwidth]{AE-depth.pdf}
    }
    ~~\subfloat[-BLEU line for AE.]{
    \includegraphics[width=0.21\textwidth]{AE-performance.pdf}
    }
    \caption{\label{fig:tree-depth-performance} -depth and -performance lines for three tasks. 
    There is a trend that the depth drops and the performance raises with the growth of . }
\end{figure}   

\begin{figure*}[t!]
    \centering
    \subfloat[Length-Accuracy lines for WSR.]{
    \includegraphics[width=0.3\textwidth]{semrel-gap.pdf}
    }
    \subfloat[Length-BLEU lines for MT.]{
    	\includegraphics[width=0.3\textwidth]{MT-gap.pdf}
    }
    \subfloat[Length-BLEU lines for AE.]{
        \includegraphics[width=0.3\textwidth]{AE-gap.pdf}
    }
    \caption{\label{fig:length-performance} Length-performance lines for the further investigated tasks. We divide test instances into several groups by length, and report the performance on each group respectively. Sentences with length in  are put to the first group, and the group  covers the range of  in length. ]}
\end{figure*}

Compared to syntactic and latent trees, the only advantage of balanced tree we can hypothesize is that, it is shallower and more balanced than others.
Shallowness may lead to shorter path for information propagation from leafs to the root representation, and makes the representation learning more easy due to the reduction of errors in the propagation process.
Balance makes the tree fairly treats all leaf nodes, which makes it more easily to automatically select the crucial information over all words in a sentence.  

To test our hypothesis, we conduct the following experiments. 
We select three tasks, on which binary balanced tree encoder wins Bi-LSTMs with a large margin~(WSR, MT and AE).
We generate random binary trees for sentences, while controlling the depth using a hyper-parameter . 
We start by a group with all words (nodes) in the sentence.
At each time, we separate  nodes to two continuous groups sized ,  with probability , while those sized  with probability .
Trees generated with  are exactly left-branching trees, and those generated with  are binary balanced trees. 
The expected node depth of the tree turns smaller with  varies from 0 to 1.

Figure~\ref{fig:tree-depth-performance} shows that, in general, trees with shallower node depth have better performance on all of the three tasks (for binary tree, shallower also means more balanced), which validates our above hypothesis that binary balanced tree gains the reward from its shallow and balanced structures. 

Additionally, Figure~\ref{fig:length-performance} demonstrates that binary balanced trees work especially better with relative long sentences. 
As desired, on short-sentence groups, the performance gap between Bi-LSTM and binary balanced tree LSTM is not obvious, while it grows with the test sentences turning longer. 
This explains why tree-based encoder gives small improvements on NLI and Para, because sentences on these two tasks are much shorter than others.




\subsection{Can Pooling Replace Tree Encoder?}
\label{sec:pooling}
\begin{figure}[t]
\centering
\subfloat[Balanced tree.]{
\includegraphics[width=0.4\textwidth]{attention-tree.pdf}
}

\subfloat[Bi-LSTM.]{
\includegraphics[width=0.4\textwidth]{attention-lstm.pdf}
}
\caption{\label{fig:attention} An illustration of the investigated self-attentive pooling mechanism. \
\begin{aligned}
\bm{f}_t & = \sigma(\bm{W}_f\cdot[\bm{h}_{t-1}, \bm{x}_t] + \bm{b}_f) \\
\bm{i}_t & = \sigma(\bm{W}_i\cdot[\bm{h}_{t-1}, \bm{x}_t] + \bm{b}_i) \\
\tilde{\bm{c}}_t & = \tanh(\bm{W}_c\cdot[\hm{h}_{t-1}, \bm{x}_t] + \bm{b}_c) \\
\bm{o}_t & = \sigma(\bm{W}_o\cdot[\bm{h}_{t-1}, \bm{x}_t] + \bm{b}_o) \\
\bm{c}_t & = \bm{f}_t \bm{c}_{t-1} + \bm{i}_t\tilde{\bm{c}}_{t} \\
\bm{h}_t & = \bm{o}_t \tanh(\bm{c}_t)
\end{aligned}

\begin{aligned}
\bm{f}_l & = \sigma(\bm{W}_l\cdot[\bm{h}_{l}, \bm{h}_r] + \bm{b}_l) \\
\bm{f}_r & = \sigma(\bm{W}_r\cdot[\bm{h}_{l}, \bm{h}_r] + \bm{b}_r) \\
\bm{i}_t & = \sigma(\bm{W}_i\cdot[\bm{h}_{l}, \bm{h}_r] + \bm{b}_i) \\
\tilde{\bm{c}}_t & = \tanh(\bm{W}_c\cdot[\hm{h}_{l}, \bm{h}_r] + \bm{b}_c) \\
\bm{o}_t & = \sigma(\bm{W}_o\cdot[\bm{h}_{t-1}, \bm{x}_t] + \bm{b}_o) \\
\bm{c}_t & = \bm{f}_l \bm{c}_{l} + \bm{f}_r \bm{c}_r + \bm{i}_t\tilde{\bm{c}}_{t} \\
\bm{h}_t & = \bm{o}_t \tanh(\bm{c}_t)
\end{aligned}

\centering
\bm{z} = \left(
\begin{aligned} 
&\bm{s}_1 \\
&\bm{s}_2 \\
\bm{s}_1 &- \bm{s}_2 \\
\bm{s}_1 &\odot \bm{s}_2
\end{aligned}
\right)

\begin{aligned}
\bm{H} &= (\bm{h}_1, \bm{h}_2, \cdots, \bm{h}_{m}) \\
\bm{s}_i &= \max_{j=1}^m h_{j,i} ~~~~~  i=1, 2, \cdots, d \\
\bm{s} &= (\bm{s}_1, \bm{s}_2, \cdots, \bm{s}_d)^T
\end{aligned}

\begin{aligned}
\bm{H} &= (\bm{h}_1, \bm{h}_2, \cdots, \bm{h}_{m}) \\
\bm{s} &= \frac{1}{m} \sum_{i=1}^m \hm{h}_i
\end{aligned}

\begin{aligned}
\bm{H} & = (\bm{h}_1, \bm{h}_2, \cdots, \bm{h}_{m}) \\
\bm{a} & = \mathrm{softmax}(\mathbf{w}_\beta^T \tanh (\mathbf{W}_\alpha \bm{H})) \\ 
\bm{s} & = \bm{H} \bm{a}^T
\end{aligned}

where  denotes attention weights computed by learned parameters  and .
In all experiments,  is a 128-d vector. 


\end{document}
