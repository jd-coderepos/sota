Text-to-video retrieval is to retrieve videos \wrt to an ad-hoc textual query from many \emph{unlabeled} videos. Both video and text have to be embedded into one or more cross-modal common spaces for  text-to-video matching. The state-of-the-art tackles the task in different approaches, including novel networks for query representation learning \cite{wray2019fine_r1,sigir20-yang-vr}, multi-modal Transformers for video representation learning \cite{gabeur2020multi,bain2021frozen_r6}, hybrid space learning for interpretable cross-modal matching \cite{Dong2021DE_hybrid,WuN2020}, and more recently CLIP2Video \cite{fang2021clip2video} that learns text and video representations in an end-to-end manner. Differently, we look into \emph{feature fusion}, an important yet largely underexplored topic for text-to-video retrieval.

Given video/text samples represented by diverse features, feature fusion aims to answer a basic research question of \emph{what is the optimal way to combine these features?} By optimal we mean the fusion shall maximize the retrieval performance. Meanwhile, the fusion process shall be explainable to interpret the importance of the individual features. As the use of each feature introduces extra computational and storage overheads, the explainability is crucial for the fusion process to be selective to balance the performance and the cost.

Feature fusion is not new by itself. In fact, the topic has been extensively studied in varied contexts such as \xredit{multimedia content analysis \cite{mmsurvey-2005,mmsurvey-2010} and multimodal or multi-view image classification \cite{aaai17-multi-view,mmsurvey-2018}}. These earlier efforts focus on combining hand-crafted features, because such kinds of features are known to be domain-specific, suffering from the semantic gap problem \cite{pami00-cbir}, and thus insufficient for content representation when used alone. While current deep learning features are already more powerful than their predecessors, no single feature appears to rule all. Dark knowledge about objects and scenes is better carried in pre-trained 2D convolutional neural networks (2D-CNNs) \cite{2021clip_icml}, while 3D-CNNs are more suited for representing actions and motions \cite{ircsn}. For text-to-video retrieval, there are some initial efforts on combining diverse deep video features, \eg JE \cite{mithun2018learning_r2,mithun2019joint_r3}, CE~\cite{liu2019use} and MMT \cite{gabeur2020multi}, whilst W2VV++ \cite{LiXirong2019W2VVPP} and SEA \cite{LiXirong2020SEA} show the potential of combining different text features for better query representation. The recent CLIP series \cite{luo2021clip4clip,fang2021clip2video}, due to their end-to-end learning paradigm, actually lacks the ability of exploiting existing features. Therefore, even in the era of deep learning, the need for feature fusion remains strong.

Concerning approaches to feature fusion, vector concatenation is commonly used when combining features at an early stage \cite{LiXirong2019W2VVPP,Dong2021DE_hybrid}. As for late fusion, multiple feature-specific common spaces are learned in parallel, with the resultant similarities combined either by averaging \cite{mithun2018learning_r2,LiXirong2020SEA}, \xredit{empirical weighting \cite{mithun2019joint_r3}} or by Mixture of Experts (MoE) ensembles \cite{liu2019use}. As the number of features grows, vector concatenation suffers from the curse of dimensionality, while constructing common spaces per feature lacks inter-feature interactions. Moreover, the prior works focus either on the video end or on the text end.
To the best of our knowledge, no attempt is made to develop a unified learning-based approach that works for both ends in the context of text-to-video retrieval, see \cref{tab:related_work}.


One might consider feature fusion by Multi-head Self-Attention (MHSA), the cornerstone of Transformers \cite{vaswani2017attention}. As \cref{fig:mhsa} shows, MHSA transforms a specific feature by blending it with information from all other features, with the blending weights produced by a self-attention mechanism termed QKV. Note that the module was initially developed for NLP tasks, for which exploiting element-wise correlations is crucial for resolving semantic ambiguity. However, as video features extracted by distinct 2D-CNNs and 3D-CNNs are meant for describing the video content from different aspects, we conjecture that optimizing their combination is preferred to modeling their correlations. \xredit{Moreover, the self-attention in MHSA, computed by , depends largely on inter-feature correlations. It thus tends to have a group effect that features related to each other will be more attended. Consequently, the related yet relatively weak features will be over-emphasized.
Hence, despite its high prevalence in varied contexts, we consider MHSA suboptimal for the current task.} 



\newlength{\twosubht}
\newsavebox{\twosubbox}

\begin{figure}[htp]

\sbox\twosubbox{\resizebox{\dimexpr\textwidth-1em}{!}{\includegraphics[height=3cm]{fig/fig1a.pdf} \includegraphics[height=3cm]{fig/fig1b.pdf}\includegraphics[height=3cm]{fig/fig1c.pdf}
  }}
\setlength{\twosubht}{\ht\twosubbox}



\centering

\subcaptionbox{\label{fig:mhsa} MHSA}{\includegraphics[height=\twosubht]{fig/fig1a.pdf}}\hspace{-1pt}
\subcaptionbox{\label{fig:atten-free} Attention-free}{\includegraphics[height=\twosubht]{fig/fig1b.pdf}}\hspace{8pt}
\subcaptionbox{\label{fig:laff} LAFF}{\includegraphics[height=\twosubht]{fig/fig1c.pdf}}
\caption{\textbf{Three distinct blocks for feature fusion}: (a) Multi-Head Self-Attention (MHSA), (b) Attention-free, and (c) our proposed LAFF.}

\label{Figure:attention_block_vs_multi_head_attention}

\end{figure}

 
We propose in this paper a much simplified feature fusion block, termed Lightweight Attention Feature Fusion (LAFF), see \cref{fig:laff}. LAFF is generic, working for both video and text ends. Video/text features are combined in a convex manner in a specific LAFF block, with the combination weights learned to optimize cross-modal text-to-video matching. Performing fusion at the feature level, LAFF can thus be viewed as an early fusion method. Meanwhile, with the multi-head trick as used in MHSA, multiple LAFFs can be deployed within a single network, with their resultant similarities combined in a late fusion manner. The ability to perform feature fusion at both early and late stages and at both video and text ends makes LAFF a powerful method for exploiting diverse, \xredit{multi-level} (off-the-shelf) features for text-to-video retrieval. In sum, our main contributions are as follows: \\
 We are the first to study both video-end and text-end feature fusion for text-to-video retrieval. Given the increasing availability of deep vision/language models for feature extraction, this paper presents an effective mean to harness such dark knowledge for tackling the task. \\
 We propose LAFF, a lightweight feature fusion block, capable of performing fusion at both early and late stages. Compared to MHSA, LAFF is much more compact yet more effective. Its attentional weights can also be used for selecting fewer features,  with the retrieval performance mostly preserved. \\
 Experiments on five benchmarks, \ie MSR-VTT, MSVD, TGIF, VATEX and TRECVID AVS 2016-2020, show that the LAFF-based video retrieval model (\cref{Figure:overall_framework}) compares favorably against the state-of-the-art, resulting in a  strong baseline for text-to-video retrieval. Code is available at GitHub\footnote{\url{https://github.com/ruc-aimc-lab/laff}}.
