\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{subcaption}


\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{color}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{tabularx}

\newcommand{\jt}[1]{\textcolor{red}{JT: #1}}
\newcommand{\geri}[1]{\textcolor{olive}{Geri: #1}}

\title{Graph-level Representation Learning with Joint-Embedding Predictive Architectures}

\author{Geri Skenderi\textsuperscript{1} \quad Hang Li \textsuperscript{2} \quad Jiliang Tang\textsuperscript{2} \quad Marco Cristani\textsuperscript{1} \\
\textsuperscript{1} University of Verona \quad \textsuperscript{2} Michigan State University
\\
\texttt{\{name.surname\}@univr.it} \\
\texttt{\{lihang4, tangjili\}@msu.edu}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
   Joint-Embedding Predictive Architectures (JEPAs) have recently emerged as a novel and powerful technique for self-supervised representation learning. They aim to learn an energy-based model by predicting the latent representation of a target signal $y$ from a context signal $x$. JEPAs bypass the need for data augmentation and negative samples, which are typically required by contrastive learning, while avoiding the overfitting issues associated with generative-based pretraining. In this paper, we show that graph-level representations can be effectively modeled using this paradigm and propose Graph-JEPA, the first JEPA for the graph domain. In particular, we employ masked modeling to learn embeddings for different subgraphs of the input graph. To endow the representations with the implicit hierarchy that is often present in graph-level concepts, we devise an alternative training objective that consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2D plane. Extensive validation shows that Graph-JEPA can learn representations that are expressive and competitive in both graph classification and regression problems.
\end{abstract}

\section{Introduction}

Graph data is ubiquitous in the real world due to its ability to universally abstract various concepts and entities \citep{ma_tang_2021,velivckovic2023everything}. To deal with this peculiar data structure, Graph Neural Networks (GNNs)~\citep{scarselli2008graph,kipf2016semi,gilmer2017neural,velivckovic2017graph} have established themselves as a staple solution. Nevertheless, most applications of GNNs usually rely on ground-truth labels for training. The growing amount of graph data in fields such as bioinformatics, chemoinformatics, and social networks has made manual labeling laborious, sparking significant interest in unsupervised graph representation learning.

A particularly emergent area in this line of research is self-supervised learning (SSL). In SSL, alternative forms of supervision are created stemming from the input signal. This process is then typically followed by invariance-based or generative-based pretraining \citep{liu2023sslsurvey,assran2023self}. Invariance-based approaches optimize the model to produce comparable embeddings for different views of the input signal. A common term associated with this procedure is contrastive learning \citep{tian2020makes}. Typically, these alternative views are created by a data augmentation procedure. The views are then passed through their respective encoder networks (which may share weights), as shown in Figure \ref{fig:jepa-diagram}a. Finally, an energy function, usually framed as a distance, acts on the latent embeddings. In the graph domain, several works have applied contrastive learning by designing graph-specific augmentations \citep{you2020graph}, using multi-view learning \citep{hassani2020contrastive} and even adversarial learning \citep{suresh2021adversarial}. This pretraining strategy is effective but comes with several drawbacks i.e., the necessity to augment the data and process negative samples. In order to learn embeddings that are useful for downstream tasks, the augmentations must also be non-trivial.

Generative methods on the other hand typically remove or corrupt portions of the input and predict them using an autoencoding procedure \citep{vincent2010stacked,he2022masked}, or rely on autoregressive modeling \citep{brown2020language,hu2020gpt}. Figure \ref{fig:jepa-diagram}b depicts the typical instantiation of these methods: The input signal $x$ is fed into an encoder network that constructs the latent representation and a decoder generates $\hat{y}$, the data corresponding to the target signal $y$. The energy function is then applied in data space, often as a reconstruction term \citep{bengio2013representation}. Generative models generally display strong overfitting tendencies since they have to estimate the data distribution (implicitly or explicitly), so the latent representations must be directly descriptive of the whole data space. This can be very problematic for graphs given that they live in a non-Euclidean and inhomogenous data space. Nevertheless, masked autoencoding has recently shown promising results in the graph domain with appropriately designed models \citep{hou2022graphmae,tan2023s2gae}. 


\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\includegraphics{figures/ssl_architectures.pdf}}
    \caption{Illustration of the three main SSL approaches: (a) Joint-Embedding Architectures learn to create similar embeddings for inputs x and y that are compatible with each other and dissimilar embeddings for inputs that are not compatible. This compatibility is implemented in practice by creating different views of the input data. (b) Generative Architectures reconstruct a signal $y$ from a compatible signal $x$ by conditioning the decoder network on additional (potentially latent) variables $z$. (c) Joint-Embedding Predictive Architectures act as a bridge: They utilize a predictor network that processes the context $x$ and is conditioned on additional (potentially latent) factors, to predict the embedding of the target $y$ in latent space.
    }
    \label{fig:jepa-diagram}
\end{figure}

Inspired by the innovative Joint-Embedding Predictive Architecture (JEPA) \citep{lecun2022path,assran2023self}, we propose Graph-JEPA to learn graph-level representations by bridging contrastive and generative models. As illustrated in Figure \ref{fig:jepa-diagram}c, a JEPA has two encoder networks that receive the input signals and produce the corresponding representations. Notably, the two encoders can be different models and don't need to share weights. A predictor module outputs a prediction of the latent representation of one signal based on the other, possibly conditioned on another variable. Graph-JEPA does not require any negative samples or data augmentation and by operating in the latent space it avoids the pitfalls associated with learning high-level details needed to fit the data distribution. However, the graph domain presents us with additional challenges, namely: context and target extraction; designing a latent prediction task that is optimal for graph-level concepts; and learning expressive representations. In response to these questions, we design Graph-JEPA  with a specific masked modeling objective. The input graph is first divided into several subgraphs, and then the latent representation of randomly chosen target subgraphs is predicted given a context subgraph. The subgraph representations are consequently pooled to create a graph-level representation. 

The nature of graph-level concepts is often assumed to be hierarchical \citep{ying2018hierarchical}, thus we conjecture that the typical latent reconstruction objective used in current JEPA formulations is not enough to provide optimal downstream performance. To this end, we design a prediction objective that starts by expressing the target subgraph encoding as a high-dimensional description of the hyperbolic angle. The predictor module is then tasked with predicting the location of the target in the 2D unit hyperbola. This prediction is compared with the target coordinates, obtained by using the aforementioned hyperbolic angle. Graph-JEPA outperforms popular contrastive and generative graph-level SSL methods on different datasets, while maintaining efficiency and ease of training. Notably, we observe from our experiments that Graph-JEPA can run up to 1.45x faster than Graph-MAE \citep{hou2022graphmae} and 8x faster than MVGRL \citep{hassani2020contrastive}. Finally, we empirically demonstrate Graph-JEPA's ability to learn highly expressive graph representations, showing it almost perfectly distinguishes pairs of non-isomorphic graphs that the 1-WL test cannot differentiate. 

\section{Related work}
\subsection{Self-Supervised Graph Representation Learning}
Graph Neural Networks (GNNs) \citep{scarselli2008graph, kipf2016semi, velivckovic2017graph, hamilton2017inductive, xu2018powerful, wu2019simplifying} are now established solutions to different graph machine learning problems such as node classification, link prediction, and graph classification. Nevertheless, the cost of labeling graph data is quite high given the immense variability of graph types and the information they can represent. To alleviate this problem, SSL on graphs has become a research frontier. SSL methods on graphs can be divided into two major groups \citep{liu2023sslsurvey}:

\textbf{Contrastive Methods.} Contrastive learning methods usually consist of minimizing an energy function \citep{hinton2002training,gutmann2010noise} between different views of the same data. InfoGraph \citep{sun2019infograph} maximizes the mutual information between the graph-level representation and the representations of substructures at different scales. GraphCL \citep{you2020graph} works similarly to distance-based contrastive methods in the imaging domain. The authors first propose four types of graph augmentations and then perform contrastive learning based on them. The work of \citet{hassani2020contrastive} goes one step further by contrasting structural views of graphs. They also show that a large number of views or multiscale training does not seem to be beneficial, contrary to the image domain. Another popular research direction for contrastive methods is learning graph augmentations \citep{suresh2021adversarial} and AutoSSL \citep{jin2021automated}, where the goal is to learn how to automatically leverage multiple pretext tasks effectively. Contrastive learning methods typically require a lot of memory due to data augmentation and negative samples. Graph-JEPA is much more efficient than typical formulations of these architectures given that it does not require any augmentations or negative samples. Another major difference is that the prediction in latent space in JEPAs is done through a separate predictor network, rather than using the common Siamese structure \citep{bromley1993signature}(Figure \ref{fig:jepa-diagram}a vs c).


\textbf{Generative Methods.} The goal of generative models is to recover the data distribution, an objective that is typically implemented through a reconstruction process. In the graph domain, most generative architectures that are also used for SSL are extensions of Auto-Encoder (AE) \citep{hinton1993autoencoders} and Variational Auto-Encoder (VAE) \citep{kingma2013auto} architectures. These models learn an embedding from the input data and then use a reconstruction objective with (optional) regularization in order to maximize the data evidence. \citet{kipf2016variational} extended the framework of AEs and VAEs to graphs by using a GNN as an encoder and the reconstruction of the adjacency matrix as a training objective. However, the results on node and graph classification benchmarks with these embeddings are often unsatisfactory compared with contrastive learning methods, a tendency also observed in other domains \citep{liu2023sslsurvey}. A recent and promising direction is masked autoencoding (MAE) \citep{he2022masked}, which has proved to be a very successful framework for the image and text domains. GraphMAE \citep{hou2022graphmae} is an instantiation of MAEs in the graph domain, where the node attributes are perturbed and then reconstructed, providing a paradigm shift from the structure learning objective of GAEs. S2GAE \citep{tan2023s2gae} is one of the latest GAEs, which focuses on reconstructing the topological structure but adds several auxiliary objectives and additional designs. Our architecture differs from generative models in that it learns to predict directly in the latent space, thereby bypassing the necessity of remembering and overfitting high-level details that help maximize the data evidence (Figure \ref{fig:jepa-diagram}b vs c).

\subsection{Joint-Embedding Predictive Architectures}
Joint-Embedding Predictive Architectures \citep{lecun2022path} are a recently proposed design for SSL. The idea is similar to both generative and contrastive approaches, yet JEPAs are non-generative since they cannot directly predict $y$ from $x$, as shown in Figure \ref{fig:jepa-diagram}c. The energy of a JEPA is given by the prediction error in the embedding space, not the input space. These models can therefore be understood as a way to capture abstract dependencies between $x$ and $y$, potentially given another latent variable $z$. It is worth noting that the different models comprising the architecture may differ in terms of structure and parameters. An in-depth explanation of Joint-Embedding Predictive Architectures and their connections to human representation learning is provided by \citet{lecun2022path}. The most well-known works to use particular instantiations of JEPAs before the term was officially coined are BYOL \citep{grill2020bootstrap} and SimSiam \citep{chen2021exploring}. There have recently been two works that implement JEPAs in practice, both in the vision domain: I-JEPA \citep{assran2023self} and MC-JEPA \citep{bardes2023mc}. I-JEPA provides an instantiation in the context of images, where the goal is to predict the latent embeddings of different image patches given a context patch. MC-JEPA instantiates the architecture in the context of learning optical flow and content features within a shared encoder. In this work, we propose the first JEPA for the graph domain and use it to learn graph-level representations in a self-supervised manner.

\section{Method}
\textbf{Preliminary.} Let a graph $G$ be defined as $G = (V, E)$ where $V = \{v_1 \ldots v_N\}$ is the set of nodes, with a cardinality $|V| = N$, and $E = \{e_1 \ldots e_M\}$ is the set of edges, with a cardinality $|E| = M$. We consider symmetric, unweighted graphs, although our method can be generalized to directed or weighted graphs. In this setting, $G$ can be represented by an adjacency matrix $A \in \{0,1\}^{N \times N}$, with $A_{ij} = 1$ if nodes $v_i$ and $v_j$ are connected and $A_{ij} = 0$ otherwise.

\textbf{A general overview.} Figure \ref{fig:graphjepa-diagram} provides an overview of the proposed architecture. The high-level idea of Graph-JEPA is to first divide the input graph into subgraphs (patches) \citep{he2023generalization} and then predict the representation of a randomly chosen target subgraph from a context subgraph \citep{assran2023self}. We would like to stress that this masked modeling objective is realized in latent space, without the need for augmentations or negative samples. The subgraph representations are then pooled to create a vectorized graph-level representation. Therefore Graph-JEPA can be described through a sequence of operations, namely: 1. Spatial Partitioning; 2. Subgraph Embedding; 3. Context and Target Encoding; 4. Latent Target Prediction.

\begin{figure}
    \centering
    \resizebox{.9\textwidth}{!}{\includegraphics{figures/graphjepa_complete.pdf}}
    \caption{An overview of Graph-JEPA. We first extract non-overlapping subgraphs (patches) (a.), perform a 1-hop neighborhood expansion (b.), and encode the subgraphs with a GNN (c.). After the subgraph encoding, one is randomly picked as the context and $m$ others as the targets (d.) and they are fed into their respective encoders (e.). The embeddings generated from the target encoder are used to produce the target subgraphs' coordinates $\psi_y$. Finally, the predictor network is tasked with directly predicting the coordinates $\hat{\psi}_y$ for each target subgraph based on the context embedding and the positional embedding of each target subgraph (f.). A regression loss acts as the energy function $D$ between the predicted and target coordinates.} 
    \label{fig:graphjepa-diagram}
\end{figure}


\subsection{Spatial Partitioning} \label{met:partitioning}
We base the initial part of the Graph-JEPA architecture on the recent work of \citet{he2023generalization}, but similar ideas consisting of graph partitioning have been proposed before for Graph SSL \citep{jin2020self}. This step consists of creating different subgraphs (patches) of a graph, similar to how Vision Transformers (ViT) \citep{dosovitskiy2020image} operate on images. We rely on the METIS \citep{karypis1998fast} graph clustering algorithm, which partitions a graph into a pre-defined, non-overlapping number of clusters $p$ (Figure \ref{fig:graphjepa-diagram}a), such that the number of within-cluster links is much higher than between-cluster links. Note that having non-overlapping subgraphs can be problematic since edges can be lost in this procedure and it is possible to end up with empty "subgraphs". We would also like to preserve the notion of locality in each subgraph while relating it to others that are close in terms of shared nodes. To create this overlap and avoid completely empty subgraphs, a one-hop neighborhood expansion of all the nodes belonging to a subgraph is performed (Figure \ref{fig:graphjepa-diagram}b).

\subsection{Subgraph Embedding} \label{met:rwse}
After partitioning the graph, we learn a representation for each subgraph through a GNN (Figure \ref{fig:graphjepa-diagram}c.). The specific choice of GNN architecture is arbitrary and depends on what properties we wish to induce in the representation. The learned node embeddings are mean pooled to create a vector representation for each subgraph: $\{h_1 ... h_p\}, h \in \mathbb{R}^d$. Given that these embeddings will be used as context or target variables, it is important to provide additional information regarding the subgraphs to help guide the predictor network. Thus, we propose to use a positional embedding for each subgraph, which is implemented as the maximum Random Walk Structural Embedding (RWSE) of all the nodes in that subgraph. In this way, the position is characterized in a global and consistent manner for each patch. Formally, a RWSE \citep{dwivedi2021graph} for a node $v$ can be defined as:
\begin{gather} \label{eq:rwse}
    P_v = (M_{ii}, M^2_{ii}, \ldots, M^k_{ii}), P_v \in \mathbb{R}^k,~~~~~M^k=(D^{-1}A)^k
\end{gather}
where $M^k$ is the random-walk diffusion matrix of order $k$ and $i$ is the index of node $v$ in the adjacency matrix. Therefore, $M^k_{ii}$ encodes the probability of node $v$ landing to itself after a $k$-step random walk. Given a subgraph $l$, let $V_l$ denote the set of nodes in $l$. The subgraph RWSE is then defined as:
\begin{gather}
    P_l = \max\limits_{v \in V_l}  P_v
\end{gather} 

\subsection{Context and Target Encoding} \label{met:context_target}
Given the subgraph representations and their respective positional embeddings, we frame the Graph-JEPA prediction task in a similar manner to I-JEPA \citep{assran2023self}. The goal of the network is to predict the latent embeddings of randomly chosen target subgraphs, given one random context subgraph. The prediction is conditioned on positional information regarding each subgraph. At each training step, we choose one random subgraph as the context $x$ and $m$ others as targets $Y = \{y_1, \ldots, y_m\}$ (Figure \ref{fig:graphjepa-diagram}d). These subgraphs are processed by the context and target encoders (Figure \ref{fig:graphjepa-diagram}e) which are parametrized by Transformer encoder blocks (without self-attention for the context) where normalization is applied at first \citep{xiong2020layer}. The target encoder uses Hadamard self-attention \citep{he2023generalization}, but other choices, such as the standard self-attention mechanism \citep{vaswani2017attention} are perfectly viable. We can summarize this step as:
\begin{equation}
    z^x = E_{context}(x), z^x \in \mathbb{R}^d, ~~~Z^y = E_{target}(Y), Z^y  \in \mathbb{R}^{m \times d}
\end{equation}
At this stage, we could use the predictor network to directly predict $Z^y$ from $z^x$. This is the typical formulation of JEPAs, also followed by \citet{assran2023self}. We argue that learning how to organize concepts for abstract objects such as graphs or networks directly in Euclidean space is suboptimal. In the following section, we propose a simple trick to bypass this problem using the encoding and prediction mechanisms in Graph-JEPA. The following subsections and the ablations studies in Section \ref{par:latent_space_ablations} will provide additional insights.

\subsection{Latent Target Prediction} \label{met:latent_task}
Different works in Neuroscience have shown the importance of learning hierarchically consistent concepts \citep{Deco2021Revisiting}, especially during infancy and young age \citep{Rosenberg2013Infants}. Networks in the real world often conform to some concept of hierarchy \citep{moutsinas2021graph} and this assumption is frequently used when learning graph-level representations \citep{ying2018hierarchical}. Thus, we conjecture that Graph-JEPA should operate in a hyperbolic space, where learned embeddings implicitly organize hierarchical concepts\citep{nickel2017poincare,zhao2023modeling}. This gives rise to another issue: Hyperbolic (Poincarè) embeddings are known to have several tradeoffs related to dimensionality \citep{pmlr-v80-sala18a,guo2022co}, which severely limits the expressive ability of the model. Given that graphs can describe very abstract concepts, high expressivity in terms of model parameters is preferred. In summary, we would ideally like to have a high-dimensional latent code that has a concept of hyperbolicity built into it. 

To achieve this, we think of the target embedding as a high-dimensional representation of the hyperbolic angle, which allows us to describe each target patch through its position in the 2D unit hyperbola. Formally, given a target patch $l$, its embedding $Z^y_l$ and positional encoding $P_l$, we first express the latent target as:
\begin{equation} \label{eq:highdcode}
    \alpha^y_l = \frac{1}{N} \sum_{n=1}^d {Z^y_l}_n, ~~~\psi^y_l = \begin{pmatrix} \; cosh(\alpha^y_l) \; \\ \\ \;  sinh(\alpha^y_l) \; \end{pmatrix} 
\end{equation}
where $cosh(\cdot)$ and $sinh(\cdot)$ are the hyperbolic cosine and sine functions respectively. The predictor module is then tasked with directly locating the target in the unit hyperbola given the context embedding and the target patch's positional encoding. 
\begin{equation} \label{eq:prediction}
    \hat{\psi}^y_l = MLP(LayerNorm(z^x + P_l)), \hat{\psi}^y_l \in \mathbb{R}^2
\end{equation}

This allows us to frame the learning procedure as a simple regression problem and the whole network can be trained end-to-end (Figure \ref{fig:graphjepa-diagram}f). In practice, we use the smooth L1 loss as the distance function, as it is less sensitive to outliers compared to the L2 loss \citep{girshick2015fast}:
\begin{gather} \label{eq:loss-fn}
    L(y, \hat{y}) = \frac{1}{N} \sum_{n=1}^N S_n,~~~S_n = \begin{cases}
    0.5(y_n - \hat{y}_n)^2 / \beta, & \text{if } |y - \hat{y}| < \beta \\
    |y - \hat{y}| - 0.5\beta, & \text{otherwise}
\end{cases}
\end{gather}

Thus, we are effectively measuring how far away the context and target patches are in the unit hyperbola of the plane, but the targets are actually described through a high dimensional latent code (Eq. \ref{eq:highdcode}). We explicitly show the differences between this choice and using the Euclidean or Hyperbolic distances as energy functions (in the latent space) in Section \ref{par:latent_space_ablations}. Our proposed pretraining objective forces the context encoder to understand the differences in the hyperbolic angle between the target patches, which can be thought of as establishing a hierarchy. 

An asymmetric design of the predictor and target networks (in terms of parameters) is used since it is reported to prevent representation collapse in self-supervised contrastive techniques \citep{chen2020simple, baevski2022data2vec}. We also utilize stop-gradient for the target encoder and update its weights using an Exponential Moving Average of the context encoder, as done in other works that utilize JEPAs \citep{assran2023self,grill2020bootstrap,chen2021exploring}. In the next section, we provide insights into these choices and why they are important. Finally, to produce the unique graph-level representation when fine-tuning, we simply feed all the subgraphs through the trained target encoder and then use mean pooling, obtaining a single feature vector $z_G \in \mathbb{R}^d$ that represents the whole graph.

\subsection{Why does Graph-JEPA work?} \label{met:formal_discussion}
In this subsection, we aim to provide a discussion regarding the design choices for Graph-JEPA and also why our formulation makes sense in the graph domain. We make the following assumptions for our analysis: i) The predictor network is linear; ii) We consider the encoded context features $z^x$ and target coordinates $\psi^y_l$. Without loss of generality, we assume that there is a one-to-one correspondence between context and target patches. (This holds in practice due to Eq. \ref{eq:prediction}; iii) The problem is treated as a least-squares problem in finite-dimensional vector spaces over the field of reals $\mathbb{R}$. Based on our assumptions, we can rewrite the context features as $X \in \mathbb{R}^{n \times d}$, the target coordinates as $Y \in \mathbb{R}^{n \times 2}$, and the weights of the linear model as $W \in \mathbb{R}^{d \times 2}$. The objective of the predictor is:
\begin{equation} \label{eq:lsq_objective}
   \argmin\limits_{W} \left\| XW - Y \right\|^2
\end{equation} 
where $\left\| . \right\|$ indicates the Frobenius norm. The solution to this system can be given by the (multivariate) least squares estimator:
\begin{equation} \label{eq:lsq_estimator}
    W = (X^TX)^{-1}X^TY
\end{equation}
By plugging Eq. \ref{eq:lsq_estimator} into Eq. \ref{eq:lsq_objective} and rewriting the difference by factorizing $Y$ we get:
\begin{equation} \label{eq:ortho_estimator}
   \argmin\limits_{W} \left\|  (X(X^TX)^{-1}X^T - I_n)Y \right\|^2
\end{equation}
Thus, to find an optimal (linear) predictor we must find the orthogonal projection of $Y$ onto the orthogonal complement of a subspace of $Col(X)$. This simply translates into finding the linear combination that is closest (in terms of $\left\| \cdot \right\|^2$) to $Y$ in a particular subspace. Similarly to what was shown in \citet{richemond2023edge}, we argue that this provides a key intuition behind the JEPA design. The target encoder which produces $Y$ must not share weights or be optimized with the context encoder. If that were the case, the easiest solution to Eq. \ref{eq:ortho_estimator} would be using a vector that is orthogonal to itself, i.e., the $\mathbf{0}$ vector, leading to representation collapse. Note that in practice, even though the predictor network doesn't have to be linear, it would still fail to perfectly learn a linear relationship (as shown above). This would render the architecture suboptimal. 

We now argue about the existence of hyperbolicity due to our latent prediction task and the necessity of the asymmetric predictor. The predictor network is linearly approximating the behavior of the unit hyperbola such that it best matches with the generated target coordinates (Eq \ref{eq:loss-fn}). Thus, the network is actually trying to estimate a space that can be considered a particular section of the hyperboloid model \citep{reynolds1993hyperbolic}. In the hyperboloid model, hyperbolas appear as hyperbolic geodesics. We are therefore evaluating our energy in a (very) restricted part of hyperbolic space. As mentioned before, we find this task to offer great flexibility as it is straightforward to implement, works well in practice (Section \ref{sec:downstream}), and is computationally efficient compared to the hyperbolic distance used to typically learn hyperbolic embeddings \citep{nickel2017poincare}. In our current setting, this approximation cannot perfectly capture the dynamics of the unit hyperbola, given the linearity of the predictor. In practice, with a non-linear predictor network, we might immediately run into a degenerate solution where the target encoder and predictor output collapse representations by immediately minimizing the training objective. It is therefore important to parametrize the predictor using a simpler network that is less expressive but can capture the correct dynamics over the training process.



\section{Experiments}
The experimental section introduces the empirical evaluation of the Graph-JEPA model in terms of downstream performance on different graph datasets and tasks, as well as various ablation studies that provide more insights into our design choices. 

\subsection{Experimental setting}
We use the TUD datasets \citep{Morris2020TUD} as commonly done for graph-level SSL \citep{suresh2021adversarial,tan2023s2gae}. We utilize seven different graph-classification datasets: PROTEINS, MUTAG, DD, REDDIT-BINARY, REDDIT-MULTI-5K, IMDB-BINARY and IMDB-MULTI. For all classification experiments, we report the accuracy over five runs (with different seeds) of ten-fold cross validation. It is worth noting that we retrain the Graph-JEPA model for each fold, without ever having access to the testing partition both in the pretraining and fine-tuning stage. For graph regression, we use the ZINC dataset and report the Mean Squared Error over ten runs (with different seeds), given that the testing partition is already provided. More details regarding the training procedure are available in Appendix \ref{app:exp-setting}.

\subsection{Downstream performance} \label{sec:downstream}
For the experiments on downstream performance, we follow \citet{suresh2021adversarial} and also report the results of a fully supervised Graph Isomorphism Network (GIN) \citep{xu2018powerful}, denoted F-GIN. As can be seen in Table \ref{tab:main-res}, Graph-JEPA achieves competitive results on all datasets, setting the state-of-the-art as a pretrained backbone on five different datasets and coming second on one. Overall, our proposed framework learns semantic embeddings that work well on different types of graphs, showing that Graph-JEPA can indeed be utilized as a general pretraining method for graph-level SSL. Notably, Graph-JEPA works well for both classification and regression and performs better than F-GIN on all classification datasets.

We further explore the performance of our model on the synthetic EXP dataset \citep{abboud2020surprising}. The goal behind this experiment is to empirically verify if Graph-JEPA can learn highly expressive graph representations, in terms of graph isomorphisms. The results in Table \ref{tab:wl-res} show that our model is able to perform much better than commonly used GNNs. Given its local and global exchange of information, this result is to be expected. Most importantly, Graph-JEPA closely rivals the flawless performance of Graph-MLP-Mixer \citep{he2023generalization}, which is trained in a supervised manner. 
\begin{table}
    \centering
     \caption{Performance of different graph SSL techniques ordered by pretraining type. The results of the competitors are taken as the best values from \citep{hassani2020contrastive,suresh2021adversarial,tan2023s2gae}. "-" indicates missing values from the literature.  The \textbf{best results} are reported in boldface, the \textit{second best} in italics.}
    \resizebox{\textwidth}{!}{\begin{tabular}{|lccc|cccc||c|}
        \hline
        Model & PROTEINS $\uparrow$ & MUTAG $\uparrow$ & DD $\uparrow$ & REDDIT-B $\uparrow$ & REDDIT-M5 $\uparrow$ & IMDB-B $\uparrow$ & IMDB-M $\uparrow$ & ZINC $\downarrow$ \\ \hline
        F-GIN \citep{xu2018powerful} & 72.39 $\pm$ 2.76 & 90.41 $\pm$ 4.61 & 74.87 $\pm$ 3.56 & 86.79 $\pm$ 2.04 & 53.28 $\pm$ 3.17 & 71.83 $\pm$ 1.93 & 48.46 $\pm$ 2.31 & 0.254 $\pm$ 0.005 \\ \hline\hline
        RU-GIN \citep{xu2018powerful} & 69.03 $\pm$ 0.33 & 87.61 $\pm$ 0.39 & 74.22 $\pm$ 0.30 & 58.97 $\pm$ 0.13 & 27.52 $\pm$ 0.61 & 51.86 $\pm$ 0.33 & 32.81 $\pm$ 0.57 & 0.809 $\pm$ 0.022 \\ \hline
        InfoGraph \citep{sun2019infograph} & 72.57 $\pm$ 0.65 & 87.71 $\pm$ 1.77 & 75.23 $\pm$ 0.39 & 78.79 $\pm$ 2.14 & 51.11 $\pm$ 0.55 & 71.11 $\pm$ 0.88 & 48.66 $\pm$ 0.67 & 0.890 $\pm$ 0.017 \\ \hline
        GraphCL \citep{you2020graph} & 72.86 $\pm$ 1.01 & 88.29 $\pm$ 1.31 & 74.70 $\pm$ 0.70 & 82.63 $\pm$ 0.99 & 53.05 $\pm$ 0.40 & 70.80 $\pm$ 0.77 & 48.49 $\pm$ 0.63 & 0.627 $\pm$ 0.013 \\ \hline
        AD-GCL-FIX \citep{suresh2021adversarial} & 73.59 $\pm$ 0.65 & 89.25 $\pm$ 1.45 & 74.49 $\pm$ 0.52 & 85.52 $\pm$ 0.79 & 53.00 $\pm$ 0.82 & 71.57 $\pm$ 1.01 & 49.04 $\pm$ 0.53 & 0.578 $\pm$ 0.012 \\ \hline
        AD-GCL-OPT \citep{suresh2021adversarial} & 73.81 $\pm$ 0.46 & $\textit{89.70}$ $\pm$ $\textit{1.03}$ & $\textit{75.10}$ $\pm$ $\textit{0.39}$ & 85.52 $\pm$ 0.79 & $\textit{54.93}$ $\pm$ $\textit{0.43}$ & 72.33 $\pm$ 0.56 & 49.89 $\pm$ 0.66 & $\textit{0.544}$ $\pm$ $\textit{0.004}$ \\ \hline
        MVGRL \citep{hassani2020contrastive} & - & - & - &  84.5 $\pm$ 0.6 & - & 74.2 $\pm$ 0.7 & 51.2 $\pm$ 0.5 & - \\ \hline \hline
        GraphMAE \citep{hou2022graphmae} & 75.30 $\pm$ 0.39 & 88.19 $\pm$ 1.26  & - & $\textit{88.01}$ $\pm$ $\textit{0.19}$ & - & $\textit{75.52}$ $\pm$ $\textit{0.66}$ & $\textit{51.63}$ $\pm$ $\textit{0.52}$ & - \\ \hline
        S2GAE \citep{tan2023s2gae} & $\textbf{76.37}$ $\pm$ $\textbf{0.43}$ & 88.26 $\pm$ 0.76 & - & 87.83 $\pm$ 0.27 & - & $\textbf{75.76}$ $\pm$ $\textbf{0.62}$ & $\textbf{51.79}$ $\pm$ $\textbf{0.36}$ & - \\ \hline \hline         
        Graph-JEPA & $\textit{75.67}$ $\pm$ $\textit{3.78}$ & $\textbf{91.25}$ $\pm$ $\textbf{5.75}$ & $\textbf{78.64}$ $\pm$ $\textbf{2.35}$ & $\textbf{91.99}$ $\pm$ $\textbf{1.59}$ & $\textbf{56.73}$ $\pm$ $\textbf{1.96}$ & 73.68 $\pm$ 3.24 & 50.69 $\pm$ 2.91 & $\textbf{0.434}$ $\pm$ $\textbf{0.014}$ \\ \hline
    \end{tabular}}
    \label{tab:main-res}  
\end{table}

\begin{table}
    \centering
    \caption{Classification accuracy on the synthetic EXP dataset \citep{abboud2020surprising}, which contains 600 pairs of non-isomorphic graphs that are indistinguishable by the 1-WL test. The competitor models are proposed in \citep{kipf2016semi,bresson2017residual,xu2018powerful,dwivedi2020generalization,he2023generalization}}.  
    \resizebox{\textwidth}{!}{
    \begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        Model & GCN & GatedGCN & GINE & GraphTransformer & Graph-MLP-Mixer & Graph-JEPA \\ \hline
        Accuracy & 51.90 $\pm$ 1.96 & 51.73 $\pm$ 1.65 & 50.69 $\pm$ 1.39 & 52.35 $\pm$ 2.32 & \textbf{100.00 $\pm$ 0.00} &  \textit{98.77 $\pm$ 0.99}\\ \hline
    \end{tabular}}
    \label{tab:wl-res}
\end{table}

\subsection{Exploring the Graph-JEPA latent space} \label{par:latent_space_ablations}
As discussed in Section \ref{met:latent_task}, the choice of energy function has a big impact on the learned representations. We argued against the use of simple Euclidean or Poincarè embeddings previously and now provide empirical evidence for this conjecture in Table \ref{tab:ablation_embspace}. Our results reveal that learning the distance between patches in the 2D unit hyperbola provides a simple way to get the advantages of both embedding types. Hyperbolic embeddings must be learned in lower dimensions due to stability issues \citep{tao2021hyperFloatErrors}, while Euclidean ones do not properly reflect the dependencies between subgraphs. Our results suggest that the hyperbolic distance seems to be a better choice than the Euclidean distance in lower dimensions, but it is very unstable and expensive computationally in high dimensions. The proposed approach provides the best overall results, with the Euclidean distance performing negligibly better in a single case. We provide a qualitative example that describes how the embedding space is altered when using our proposed latent objective in Appendix \ref{app:latent-fig}.
\begin{table}
    \centering
    \caption{Comparison of Graph-JEPA performance for different distance functions. We used a latent space of dimension 128 for Poincarè embeddings due to optimization issues, which was still problematic for the IMDB-B dataset. LD stands for Lower Dimension, where we use a much smaller embedding size (32).}
    \resizebox{\textwidth}{!}{\begin{tabular}{|l|c|c|c|c|c|}
    \hline
    Distance function & Ours & Euclidean & Hyperbolic & Euclidean (LD) & Hyperbolic (LD) \\ \hline
    MUTAG    & \textbf{91.25 $\pm$ 5.75} & 87.04 $\pm$ 6.01 & 89.43 +- 5.67 & 86.63 $\pm$ 5.9 & 86.32 $\pm$ 5.52 \\ \hline
    REDDIT-M & \textbf{56.73 $\pm$ 1.96} & 56.55 $\pm$ 1.94 & 56.19 +- 1.95 & 54.84 $\pm$ 1.6 & 55.07 $\pm$ 1.83 \\ \hline
    IMDB-B   & 73.68 $\pm$ 3.24 & \textbf{73.76 $\pm$ 3.46} & NaN & 72.5 $\pm$ 3.97 & 73.4 $\pm$ 4.07 \\ \hline
    ZINC     & \textbf{0.434 $\pm$ 0.01} & 0.471 $\pm$ 0.01 & 0.605 +- 0.01 & 0.952 $\pm$ 0.05 & 0.912 $\pm$ 0.04 \\ \hline
    \end{tabular}}
    \label{tab:ablation_embspace}
\end{table}


\subsection{Ablation studies}
In the following, we present various ablation studies to get a better understanding of the different design choices of Graph-JEPA. For all ablations, we consider 4 out of the 8 datasets initially presented in Table \ref{tab:main-res}, making sure to choose different graph types for a fair comparison. All results are summarized in Figure \ref{fig:ablations}, but we also provide the results in tabular format in Appendix \ref{app:ablation-tab} for additional clarity.

\textbf{Positional embedding.} Following \citet{he2023generalization}, it is possible to use the RWSE of the patches as conditioning information. Formally, let $B \in \{0,1\}^{p \times N}$ be the patch assignment matrix, such that $B_{ij} = 1$ if $v_j \in p_i$. We can calculate a coarse patch adjacency matrix $A' = BB^T \in \mathbb{R}^{p \times p}$, where each $A_{ij}'$ contains the node overlap between $p_i$ and $p_j$. The RWSE can be calculated as described in Eq. \ref{eq:rwse} (considering $A'$). We test Graph-JEPA with these relative positional embeddings and find that they still provide good performance, but consistently fall behind the node-level (global) RWSE that we employ in our formulation (Figure \ref{fig:ablations}a). Indeed, a great issue for the relative embeddings is that the number of shared neighbors obscures the local peculiarities of each patch and this adds more variance to the results. 

\textbf{Using different Self-Attention mechanisms.} As stated in Section \ref{met:context_target}, Graph-JEPA uses Hadamard self-attention \citep{he2023generalization}, which provides a strong inductive bias for graphs. It is possible to make no assumptions about the data and render the prediction task more challenging, by using the standard self-attention mechanism \citep{vaswani2017attention}. We show the results of this change in Figure \ref{fig:ablations}b. The results reveal that the performance is slightly better but more unstable, i.e., additional variance in the results. This is to be expected since, as mentioned previously, using the standard self-attention mechanism emphasizes a lack of inductive bias regarding the input data.

\textbf{Random subgraphs.} A natural question that arises in our framework is how to design the spatial partitioning procedure. Using a structured approach like METIS is intuitive and leads to favorable results. Another option would be to simply extract random, non-empty subgraphs as context and target. As can be seen in Figure \ref{fig:ablations}c, the random patches provide strong performance as well but do not show any improvement. Even though our results show that it might not be necessary to use a structured way to extract the patches, not all graphs are equally easy to sample randomly from. Thus, we advocate extracting subgraphs with METIS as it is a safer option in terms of generalizability across different graphs and what inductive biases it provides.



\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\includegraphics{figures/all_ablas.pdf}}
    \caption{Ablation studies: a. Performance when using absolute (node-level) vs relative (patch-level) RWSEs. b. Results when using the standard self-attention mechanism vs a graph-specific one. c. Performance when using METIS subgraphs vs using random subgraphs. We remind the reader that the MAE is reported on ZINC and Accuracy on the other datasets. The error bars represent the average standard deviation values over the runs in order to report results similarly to the tables in previous sections. Best viewed in color.} 
    \label{fig:ablations}
\end{figure}


\section{Conclusion}
In this work, we introduce the first Joint Embedding Predictive Architecture (JEPA) for graph-level Self-Supervised Learning (SSL). A proper design of the model both in terms of data preparation and pretraining objective reveals that it is possible for a neural network model to self-organize the semantic knowledge embedded in a graph, demonstrating competitive performance in both graph classification and regression. Future research directions include extending the method to node and edge-level learning, exploring the expressiveness of Graph-JEPA, and gaining insights into the optimal geometry of the embedding space for graph SSL.

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}



\appendix
\section{Appendix}
\subsection{Experimental setting} \label{app:exp-setting}
For the datasets that do not natively have node and edge features, we use a simple constant (0) initialization. The subgraph embedding GNN (Figure \ref{fig:graphjepa-diagram}c) consists of the GIN operator with support for edge features \citep{hu2019strategies} a.k.a GINE. For fine-tuning, we feed the high-dimensional representation of the hyperbolic angle produced by the target encoder to a linear model with L2 regularization. For the classification datasets, that corresponds to a Logistic Regression with L2 regularization, while for ZINC it corresponds to a Ridge Regression. We also provide details regarding the hyperparameters of the JEPA design for Graph-JEPA in Table \ref{tab:hyperparams-app}. The neural network is trained using the Adam optimizer \citep{kingma2014adam}. All experiments were performed on a single Nvidia RTX 3090 GPU.

\begin{table}
    \centering
    \caption{Hyperparameters regarding the Graph-JEPA design for the TUD datasets.}
    \resizebox{\textwidth}{!}{\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
        \hline
        Hyperparameter & PROTEINS & MUTAG  & DD & REDDIT-B & REDDIT-M5 & IMDB-B  & IMDB-M  & ZINC \\ \hline
        \# Subgraphs  & 32 & 32 & 32 & 128 & 128 & 32 & 32 & 32 \\ \hline
        \# GNN Layers & 2 & 2 & 3 & 2 & 2 & 2 & 2 & 2 \\ \hline
        \# Encoder Blocks & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 \\ \hline
        Embedding size & 512 & 512 & 512 & 512 & 512 & 512 & 512 & 512\\ \hline
        RWSE size & 20 & 15 & 30 & 40 & 40 & 15 & 15 & 20\\ \hline
        \# context - \# target & 1 - 2 & 1 - 3 & 1 - 4 & 1 - 4 & 1 - 4 & 1- 4 & 1- 4 & 1- 4 \\ \hline
    \end{tabular}}
    \label{tab:hyperparams-app}  
\end{table}

\begin{table}
    \centering
    \caption{Performance when using absolute (node-level) vs relative (patch-level) RWSEs.}
    \begin{tabular}{|l|c|c|}
    \hline
    Patch PE & Node-level RWSE & Patch-level RWSE \\ \hline
    MUTAG    & \textbf{91.25 $\pm$ 5.75} & 91.23 $\pm$ 5.86 \\ \hline
    REDDIT-M & \textbf{56.73 $\pm$ 1.96} & 56.01 $\pm$ 2.1 \\ \hline
    IMDB-B   & \textbf{73.68 $\pm$ 3.24} & 73.58 $\pm$ 4.47 \\ \hline
    ZINC     & \textbf{0.434 $\pm$ 0.01} & 0.505 $\pm$ 0.005 \\ \hline
    \end{tabular}
    \label{tab:ablation_posemb}
\end{table}

\begin{table}
    \centering
    \caption{Performance when using METIS subgraphs vs. using random subgraphs.}
    \begin{tabular}{|l|c|c|}
    \hline
    Partitioning method & Metis & Random \\    \hline
    MUTAG    & 91.25 $\pm$ 5.75  &  \textbf{91.58 $\pm$ 5.82}\\ \hline
    REDDIT-M & \textbf{56.73 $\pm$ 1.96}  &  56.08 $\pm$ 1.69\\ \hline
    IMDB-B   & \textbf{73.68 $\pm$ 3.24}  &  73.52 $\pm$ 3.08\\ \hline
    ZINC     & 0.434 $\pm$ 0.01  &  \textbf{0.43  $\pm$ 0.01}\\ \hline
    \end{tabular}
    \label{tab:ablation_patches}
\end{table}

\begin{table}
    \centering
    \caption{Results when using the standard self-attention mechanism vs. a graph-specific one.}
    \begin{tabular}{|l|c|c|}
    \hline
    Attention mechanism & Standard & Hadamard \\ \hline
    MUTAG    & \textbf{91.46 $\pm$ 6.1}  & 91.25 $\pm$ 5.75\\ \hline
    REDDIT-M & \textbf{56.92 $\pm$ 2.09} & 56.73 $\pm$ 1.96\\ \hline
    IMDB-B   & \textbf{73.92 $\pm$ 3.85} & 73.68 $\pm$ 3.24\\ \hline
    ZINC     & 0.465 $\pm$ 0.01 & \textbf{0.434 $\pm$ 0.01}\\ \hline
    \end{tabular}
    \label{tab:ablation_attn}
\end{table}

\subsection{Ablation results} \label{app:ablation-tab}
Tables \ref{tab:ablation_posemb}, \ref{tab:ablation_patches}, and \ref{tab:ablation_attn} present the results from Figure \ref{fig:ablations} in tabular format, providing additional clarity.

\subsection{Visualizations} \label{app:latent-fig}
In this section, we provide two qualitative examples that give a more visual understanding of how and what Graph-JEPA learns. Firstly, we plot a low-dimensional representation of the high-dimensional latent code obtained from the target encoder when using the Euclidean objective vs using our objective in Figure \ref{fig:latentspace}. The modified prediction task greatly alters the geometry and distribution of the latent codes, avoiding the formation of a blob-like structure like in the Euclidean distance task. 

Figure \ref{fig:hyperspace} depicts how the Graph-JEPA prediction task is framed. In \ref{fig:hyperspace}a we can see a particular molecule from the ZINC dataset and the corresponding context and target nodes. The target encoder learns a high-dimensional embedding which then can represent the nodes in the 2D unit hyperbola, as shown in \ref{fig:hyperspace}b. The predictor network tries to guess these coordinates and this training procedure gives rise to the representations learned in Figure \ref{fig:latentspace}b.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\includegraphics{figures/jepa_latents.pdf}}
    \caption{3D representation (via t-SNE) of the latent codes used to finetune the linear classifier for the DD dataset. Best viewed in color.} 
    \label{fig:latentspace}
\end{figure}

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\includegraphics{figures/jepa_encoding.pdf}}
    \caption{Visualization of a partitioned graph from the ZINC dataset and its corresponding embedding in the 2D unit hyperbola, as detailed in Eq. \ref{eq:highdcode}} 
    \label{fig:hyperspace}
\end{figure}

\end{document}
