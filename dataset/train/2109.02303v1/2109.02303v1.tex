\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[accsupp]{axessibility}  \usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{autobreak}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{afterpage}
\usepackage{multicol}
\usepackage{caption}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{3582} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}



\makeatletter
\newcommand\email[2][]{\newaffiltrue\let\AB@blk@and\AB@pand
      \if\relax#1\relax\def\AB@note{\AB@thenote}\else\def\AB@note{\relax}\setcounter{Maxaffil}{0}\fi
      \begingroup
        \let\protect\@unexpandable@protect
        \def\thanks{\protect\thanks}\def\footnote{\protect\footnote}\@temptokena=\expandafter{\AB@authors}{\def\\{\protect\\\protect\Affilfont}\xdef\AB@temp{#2}}\xdef\AB@authors{\the\@temptokena\AB@las\AB@au@str
         \protect\
    L=L_{3D}+L_{2D}+L_{SMPL}+L_{NORM}

\begin{gathered}
L_{3D} =\sum_{k=1}^{K}\left\|J_{3d}^{k}-J_{3dgt}^{k}\right\|_{2}, \\ 
L_{2D} =\sum_{k=1}^{K}\left\|J_{2d}^{k}-J_{2dgt}^{k}\right\|_{2} \\
L_{SMPL} = \|\vec{\theta}-\vec{\theta}_{gt}\|_{2} + \|\vec{\beta}-\vec{\beta}_{gt}\|_{2} \\
L_{NORM} = \|\vec{\beta}\|_{2}+\|\vec{\theta}\|_{2}
\end{gathered}

    G_{k}(\mathcal{R}, \mathcal{T})=\prod_{i \in A(k)}\left(\begin{array}{cc}
\mathbf{R}_i & \mathbf{t}_{i} \\
\mathbf{0} & \mathbf{1}
\end{array}\right)
\label{equ:transformation}

    \vec{\beta}=\mathbf{W}_{\text{shape}} \cdot \mathbf{x}, \quad \vec{\phi}=\mathbf{W}_{\text{cam}} \cdot \mathbf{x}
\label{eq:shape_cam}

where , , and  is the image feature extracted by the STE.

Then we iteratively generate the pose parameter for each joint in hierarchical order according to the structure of kinematic tree.
Take joints \{0, 2, 5\} in Figure \ref{fig:body_with_joints} as an example. We first predict the pose parameters of root, namely the global body orientation, using the output feature of STE and a learnable linear regressor , \textit{i.e.}, . Here, following \cite{spin}, we use the 6D rotation representation proposed in \cite{6d_rotation} for faster convergence. Then, for its child joint 2, we take the image feature  and  as the input of another linear regressor  which outputs the pose parameters , \textit{i.e.}, , where  is the concatenate operation. Similarly for the grandson joint 5, . This regression process is shown in Figure \ref{fig:net}.

By KTD, we establish the dependence between the parent joint and its children, which is consistent with kinematic tree structure. In traditional regressor, the error of the parent joint's pose estimation only affects itself. While in KTD, the error will be propagated to its children as well. This encourages the model to learn an attention at the joint level and pay more attention to parent joints, so as to achieve more accurate estimation results.




\section{Experiments}

\subsection{Datasets}



\textbf{Training.} Following previous works \cite{hmr}\cite{spin}\cite{vibe}, we use mixed datasets for training, including 3D video datasets, 2D video datasets and 2D image datasets. For 3D video datasets, Human3.6M \cite{human3.6m} and MPI-INF-3DHP \cite{mpii3d} provide 3D keypoints and SMPL parameters in indoor scene. For 2D video datasets, PennAction \cite{pennaction} and PoseTrack \cite{posetrack} provide ground-truth 2D keypoints annotation, while InstaVariaty \cite{insta} provides pseudo 2D keypoints annotation using a keypoint detector \cite{detector1,multiposenet}. For image-based datasets, COCO \cite{coco}, MPII \cite{mpii} and LSP-Extended \cite{lspet} are adopted, providing in-the-wild 2D keypoints annotation. Meanwhile, we conduct ablation study on the 3DPW \cite{3dpw} dataset.

\textbf{Evaluation.} We report the experiments results on Human3.6M \cite{human3.6m}, MPI-INF-3DHP \cite{mpii3d} and 3DPW \cite{3dpw} evaluation set. We adopt the widely used evaluation metrics following previous works \cite{hmr}\cite{spin}\cite{vibe}, including Procrustes-Aligned Mean Per Joint Position Error (PA-MPJPE), Mean Per Joint Position Error (MPJPE), Per
Vertex Error (PVE) and ACCELeration error (ACCEL). We report the results with and without 3DPW \cite{3dpw} training set for fair comparison with previous methods.

\subsection{Training Details}

\textbf{Data Augmentation.} Horizontal flipping, random cropping, random erasing \cite{randomerase} and color jittering are employed to augment the training samples. 
Different frames of the same video input share consistent augmentation parameters. 




\textbf{Model Details.} Following \cite{vit}, we use a modified ResNet-50 \cite{resnet} as the CNN backbone to extract the basic feature of an input image.
For STE, 6 STE Parallel Blocks are stacked, and each block has 12 heads. We adopt the weights from \cite{vit} to initialize the ResNet-50 and STE.

The whole training process is divided into two stages. In the first stage, the model aims at accumulating sufficient spatial prior knowledge, and thus is trained with all image-based datasets and frames from Human3.6M and MPI-INF-3DHP.
We fix the number of epochs as 100 and the mini-batch size as 512 for this stage. In the second stage, we use both video and image datasets for temporal modeling. For video datasets, 
we sample 16-frame clips at a interval of 8 as training instances.
We train another 100 epochs with a mini-batch size of 32 for this stage. The model is optimized by Adam optimizer with an initial learning rate of  which is decreased by 10 at the 60-th and 90-th epochs. Finally, each term in the loss function has different weighting coefficients. Refer to Sup. Mat. for further details. All experiments are conducted on 16 Nvidia GTX1080ti GPUs.



\subsection{Comparison to state-of-the-art results}

In this section, we compare our method with the state-of-the-art models on 3DPW, MPI-INF-3DHP and Human3.6M, and the results are summarized in Table \ref{fig:compared_with_sota}. On the 3DPW and MPI-INF-3DHP datasets, our method outperforms other competitors including image- and video-based methods by a large margin, whether or not using 3DPW training set. On Human3.6M, our method achieves results on-par with I2LMeshNet \cite{I2LMeshNet}. We also observe MEVA \cite{meva}, an two-stage method that aims at producing both smooth and accurate results, ranks best in ACCEL metric on 3DPW. However, considering all indicators, our method achieves better performance overall.


These results validate our hypothesis that the exploitation of the attentions at spatial-temporal level and human joint level greatly helps to achieve more accurate estimation. The leading performance on these three datasets (especially the in-the-wild dataset 3DPW) demonstrates the robustness and the potential to real-world applications of our method.









\begin{figure*}
\centering     \subfigure
{\includegraphics[width=0.95\linewidth]{figs/visual1.pdf}}
\subfigure
{\includegraphics[width=0.95\linewidth]{figs/visual2.pdf}}
\vspace{-0.1in}
\caption{Qualitative visualization of MAED. More visualization results will be shown in Sup.Mat. }
\label{fig:visual}
\end{figure*}

\subsection{Ablation Study} \label{ablation}



\begin{table}[]
\begin{center}
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{|l|l|cc|}
\hline
\multirow{2}{*}{Encoder}        & \multirow{2}{*}{Decoder}       & \multicolumn{2}{c|}{3DPW}                                 \\ \cline{3-4} 
                                &                                & \multicolumn{1}{l}{PA-MPJPE} & \multicolumn{1}{l|}{MPJPE} \\ \hline \hline
CNN                             & Iterative                      & 52.2                         & 87.5                       \\
CNN                             & KTD                            & 50.9                         & 88.0                       \\
CNN+STE                         & Iterative                      & 47.5                         & 80.2                       \\
CNN+STE                         & KTD                            & \textbf{45.7}                & \textbf{79.1}              \\ \hline
CNN                             & \multirow{7}{*}{Iterative}     & 52.2                         & 87.5                       \\
CNN+TE                          &                                & 51.1                         & 84.5                       \\
CNN+SE                          &                                & 49.8                         & 84.5                       \\
CNN+STE       &                                & 48.5                         & 83.6                       \\ 
CNN+STEv1   &                                & 48.1                         & 81.6                       \\
CNN+STEv2   &                                & \textbf{47.5}                & \textbf{80.2}              \\
CNN+STE     &                                & 49.3                         & 82.6                       \\ \hline
\multirow{5}{*}{CNN+STE}        & Iterative                      & 47.5                         & 80.2                       \\
                                & Decoder     & 47.7                         & 80.7                       \\
                                & KTD                            & \textbf{45.7}                & \textbf{79.1}              \\
                                & KTD          & 47.7                         & 82.5                       \\
                                & KTD         & 47.6                         & 79.7                       \\ \hline
\end{tabular}
}
\end{center}
\caption{Analytical experiment results with different encoders and decoders. CNN represents ResNet-50. "Iterative" represents the iterative feedback regressor.}
\label{tab:ablation}
\end{table}







\subsubsection{The effectiveness of STE and KTD}

The upper part of Table \ref{tab:ablation} verifies the effectiveness of our proposed STE and KTD. Compared with CNN encoder+Iterative decoder, STE and KTD brings 4.7 and 1.3 mm improvement in PA-MPJPE metric respectively. Moreover, STE and KTD together further improves the performance by 6.5 mm. This proves the attention at different levels extracted by STE and KTD effectively complement rather than conflict each other.


We can also observe that when using CNN encoder, the gain of KTD in PA-MPJPE metric is smaller than that when using CNN+STE encoder. Even there is a small decline in MPJPE metric. This is because the CNN loses too much spatial information due to the global pooling operation, and fails to provide detailed human body clue for KTD. However, with hard downsampling removed, STE not only preserves more spatial information, but also pay more attention to more informative locations, which makes KTD capture more precise attention between joints.


\subsubsection{Influence of different encoders} \label{ablation_ste}
In the middle part of Table \ref{tab:ablation}, we compare the performance of various forms of STE. SE denotes the encoder with only MSA-S. TE denotes the encoder with only MSA-T and CNN global pooling layer kept. STEv1 and STEv2 denote the Parallel Block w/o and w/ attentive addition respectively. We conclude that all the variants of STE benefit the model, while STEv2 yields the most significant gain. This is because the attentive weights dynamically computed
in the Parallel Block effectively act as a valve which adjusts the proportion of temporal and spatial information passing through the network.
When it comes to occlusion or ambiguity, the valve will allow more temporal information to pass through to complement the lack of information in current frame, and do otherwise when the current frame is clear. Surprisingly, STE yields only modest improvement over encoder with only MSA-S (49.849.3), which has no temporal modeling capability. We also observe that STE converges more slowly compared to other STE variants. We argue that flattening the spatial and temporal dimension together may harm human pose estimation mainly due to the extremely long sequence. 
Tremendous irrelevant patches (such as background and joints that are too far apart) 
overwhelm valid information, making it challenging for the current patch to allocate reasonable attention.


\subsubsection{Influence of different decoders} 
We choose CNN+STE as the encoder and report the results with different decoders in the lower part of Table \ref{tab:ablation}. KTD denotes the KTD on a randomly generated kinematic tree. KTD denotes the KTD on the reverse kinematic tree, namely, exchange the relationship between parent joint and its children. Decoder denotes the standard decoder in \cite{transformer} with 6 layers. It takes as input the zero sequence of length 37 (24 for pose, 10 for shape and 3 for camera) and outputs SMPL parameters. We observe that KTD outperforms Iterative by a large margin. While KTD and KTD have no obvious improvement, even are slightly worse, proving unreasonable kinematic tree is useless prior knowledge, which brings difficulties to the optimization of the network. We also observe that Decoder brings no improvement. Although it can capture the relation between different joints with the self-attention mechanism, the predictions of all joints are generated simultaneously, not in the sequential way as KTD. As a result, it can not pay more attention to the parent joints.





\subsection{Visualization Analysis}

Figure \ref{fig:visual} includes qualitative results of MAED from two representative scenarios. 
For these challenging cases including extreme pose in Figure \ref{fig:visual}(a) and cluttered background and occlusion in Figure \ref{fig:visual}(b), our model predicts reasonable spatial and temporal attention maps and further produce proper estimations.




\section{Conclusion}
This paper describes MAED, an approach that utilizes multi-level attentions at spatial-temporal level and human joint level for 3D human shape and pose estimation. We design multiple variants of MSA and STE Block to construct STE to learn spatial-temporal attention from the output feature of CNN backbone. In addition, we propose KTD, which simulates the process of joint rotation based on SMPL kinematic tree to decode human pose. 
MAED makes significant accuracy improvement on multiple datasets but also brings non-negligible computation overhead, which we explore further in the Sup. Mat.
Thus, future work could consider reducing computation overhead or extending this method to capture the relation between multiple people. 



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
