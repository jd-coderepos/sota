

Our evaluation is organized as follows. Firstly in Section \ref{ssec:exp-sota}, we compare the proposed Dual Encoding model (with its best setup) against the state-of-the-art on four datasets, \ie MSR-VTT~\cite{xu2016msr}, TRECVID AVS 2016-2018~\cite{AwadTRECVID16,AwadTRECVID17,AwadTRECVID18}, VATEX~\cite{wang2019vatex} and MPII-MD~\cite{rohrbach2015dataset}. Constructed independently by the dataset developers, the first three datasets consist of short web videos with very diverse content, while the last dataset contains video clips from 72 movies. 
Second, in order to verify the influence of major components in the proposed model, Section \ref{ssec:ablation} presents an ablation study on the MSR-VTT dataset.  Additional experiments concerning the effect of the hyper-parameter  and the concept annotation quality on the performance and the efficiency of our model are provided in the appendix.



Before proceeding to the experiments, we detail common implementations regarding text preprocessing, video features, concept vocabulary extraction, and model training. 
For sentence preprocessing, we first convert all words to the lowercase and then replace words that occurring less than five times in the training set with a special token. 
For video features, on VATEX we adopt 1,024-d I3D \cite{carreira2017quo} video features provided by the dataset developers~\cite{wang2019vatex}.
As for the other datasets, we extract frame-level ResNeXt-101~\cite{xie2017aggregated,mettes2020shuffled} and ResNet-152~\cite{cvpr2016-resnet} using an open-source toolbox\footnote{\url{https://github.com/xuchaoxi/video-cnn-feat}}. The two feature vectors are concatenated to obtain a combined 4,096-d CNN feature, which we refer to as ResNeXt-ResNet.


To obtain the concept vocabulary, we conduct part-of-speech tagging by NLTK toolkit on all training sentences, and only keep the nouns, verbs and adjectives. All the English stopwords also removed. Besides, we also lemmatize the words, making \textit{dog} and \textit{dogs} to be a same concept. Finally, the top  frequent words are selected as the final concept vocabulary.

The proposed model is implemented using PyTorch (http://pytorch.org). Following~\cite{faghri2017vse}, the parameter  for the improved triplet ranking loss is set to . The weight  in the combined similarity is empirically set to . 
We learn from our earlier studies~\cite{li2019w2vv++,cvpr2019-dual-dong} that setting the dimensionality of the common space to  is a good practice. Hence, we let the overall dimensionality of the hybrid space be . Recall that the concept space is 512-dimensional. Accordingly, the dimensionality of the latent space is .
We use stochastic gradient descent with Adam \cite{kingma2014adam}. The mini-batch size is 128. With an initial learning rate of 0.0001, we take an adjustment schedule similar to \cite{dong2018predicting}. That is, once the validation loss does not decrease in three consecutive epochs, we divide the learning rate by 2. Early stop occurs if the validation performance does not improve in ten consecutive epochs. The maximal number of epochs is 50. In practice, early stop occurs typically after 15 epochs. 



\subsection{Comparison with the State-of-the-art} \label{ssec:exp-sota}


\subsubsection{Experiments on MSR-VTT}

\textbf{Data}. 
The MSR-VTT dataset \cite{xu2016msr}, originally developed for video captioning, consists of 10k web video clips and 200k natural sentences describing the visual content of the clips. The number of sentences per clip is 20. 
For this dataset, we notice there are three distinct editions of data partition in the literature~\cite{xu2016msr,miech2018learning,yu2018joint}.
The official partition~\cite{xu2016msr} uses 6,513 clips for training, 497 clips for validation, and the remaining 2,990 clips for testing.
For the partition by \cite{miech2018learning}, there are 6,656 clips for training and 1,000 clips for testing. The partition of \cite{yu2018joint} uses 7,010 and 1,000 clips for training and testing, respectively. 
As the last two data partitions provide no validation set, we build a validation set by randomly sample 1,000 clips from MSR-VTT with \cite{miech2018learning,yu2018joint} excluded, respectively.
For a comprehensive evaluation, our experiments are performed on all the three data partitions.


\textbf{Performance Metrics}.  We use rank-based metrics, namely  (), Median rank (Med r) and mean Average Precision (mAP) to evaluate the performance.  is the percentage of test queries for which at least one relevant item is found among the top- retrieved results. Med r is the median rank of the first relevant item in the search results. Higher , mAP and lower Med r mean better performance. 
For overall comparison, we report the Sum of all Recalls (SumR).


\begin{table*} [tb!]
\renewcommand{\arraystretch}{1}
\caption{\textbf{State-of-the-art on MSR-VTT}. Larger R@\{1,5,10\}, mAP and smaller Med r indicate better performance. Symbol asterisk (*) indicates numbers directly cited from the original papers, and the others are obtained by our re-training given the same ResNeXt-ResNet feature. On all the three distinct editions of data partition, the proposed Dual Encoding model obtains the best overall performance.
}
\label{tab:sota-msrvtt}
\centering 
\scalebox{0.93}{
\begin{tabular}{@{}l*{12}{r}c @{}}
\toprule
\multirow{2}{*}{\textbf{Method}}   & \multicolumn{5}{c}{\textbf{Text-to-Video Retrieval}} && \multicolumn{5}{c}{\textbf{Video-to-Text Retrieval}} & \multirow{2}{*}{\textbf{SumR}} \\
 \cmidrule{2-6}  \cmidrule{8-12} 
& R@1 & R@5 & R@10 & Med r & mAP && R@1 & R@5 & R@10 & Med r & mAP & \\
\cmidrule{1-13}
\textbf{Official full-size test set \cite{xu2016msr}}  \\
Francis \etal* \cite{iccv2019-francis}     & 6.5 & 19.3 & 28.0 & 42 & - && - & - & - & - & - & -  \\
Mithun \etal* \cite{mithun2018learning}    & 7.0 & 20.9 & 29.7 & 38 & - &&    12.5 & 32.1 & 42.4 & 16 & - &     144.6 \\
TCE* \cite{sigir2020tree}                  & 7.7 & 22.5 & 32.1 & 30 & - && - & - & - & - & - & -  \\
HGR* \cite{chen2020fine}            & 9.2  & 26.2 & 36.5 & 24 & - &&          15.0 & 36.7 & 48.8 & 11 & - &     172.4 \\
CE* \cite{liu2019use}               & 10.0 & 29.0  & 41.2  & \textbf{16}  & -   & &    15.6  & 40.9  & 55.2   & 8.3  & - & 191.9 \\
\cmidrule{2-13}
W2VV \cite{dong2018predicting}    & 1.1  & 4.7  & 8.1 & 236 & 3.7 &&      17.0 & 37.9 & 49.1 & 11 & 7.6 &     117.9 \\
MEE \cite{miech2018learning}      & 6.8 & 20.7 & 31.1 & 28 & 14.7  &&   13.4 & 32.0 & 44.0 & 14 & 6.6  & 148.0 \\
CE \cite{liu2019use}              & 7.9 & 23.6 & 34.6 & 23 & 16.5 &&       11.0 & 31.9 & 46.1 & 13  & 6.8 & 155.1 \\
VSE++ \cite{faghri2017vse}        & 8.7  & 24.3 & 34.1 & 28 & 16.9 &&      15.6 & 36.6 & 48.6 & 11 & 7.4 & 167.9 \\
TCE \cite{sigir2020tree}          & 9.3 & 27.3 & 38.6 & 19 & 18.7 &&       15.1 & 36.8 & 50.2 & 10 & 8.0 & 177.3 \\
W2VV++ \cite{li2019w2vv++}        & 11.1 & 29.6 & 40.5 & 18 & 20.6 &&      17.5 & 40.2 & 52.5 & 9 & 8.5 & 191.4 \\
HGR \cite{chen2020fine}           & 11.1 &	\textbf{30.5} &	\textbf{42.1} & \textbf{16} & 20.8 &&    18.7 & 44.3 & 57.6 & \textbf{7} & 9.9 & 204.4 \\
\textit{Dual Encoding}         & \textbf{11.6} & 30.3 & 41.3 & 17 & \textbf{21.2} && \textbf{22.5} & \textbf{47.1} & \textbf{58.9} & \textbf{7} & \textbf{10.5} & \textbf{211.7} \\  [3pt]


\hline
\textbf{Test1k-Miech \cite{miech2018learning}}  \\
JPoSE*  \cite{wray2019fine}                & 14.3 & 38.1 & 53.0 & 9 & - && 16.4 & 41.3 & 54.4 & 8.7 & - & 217.5 \\
MEE* \cite{miech2018learning}               & 16.8 & 41.0 & 54.4 & 9 & - && - & - & - & - & - & -  \\
TCE* \cite{sigir2020tree}                  & 17.1 & 39.9 & 53.7 & 9 & - && - & - & - & - & - & -  \\
CE* \cite{liu2019use}                      & 18.2 & 46.0 & 60.7 & 7 & - && 18.0 & 46.0  & 60.3 & 6.5 & - & 249.2 \\
\cmidrule{2-13}
W2VV \cite{dong2018predicting}            & 2.7 & 12.5 & 17.3 & 83 & 7.9 &&      17.3 & 42.0 & 53.5 & 9 & 29.3 & 145.3 \\
MEE \cite{miech2018learning}             & 15.7 & 39.0 & 52.3 & 9 & 27.1  && 15.3 & 41.9 & 54.5& 8 & 28.1 & 218.7 \\
VSE++ \cite{faghri2017vse}               & 17.0 & 40.9 & 52.0 & 10 & 16.9 &&     18.1 & 40.4 & 52.1 & 9 & 29.2 & 220.5 \\
CE \cite{liu2019use}                     & 17.8 & 42.8 & 56.1 & 8 & 30.3  &&   17.4 & 42.9 & 56.1 & 8 & 29.8 & 233.1 \\
TCE \cite{sigir2020tree}                 & 17.0 & 44.7 & 58.3 & 7 & 30.0  &&   15.1 & 43.3 & 58.2 & 7 & 28.3 & 236.6 \\
W2VV++ \cite{li2019w2vv++}               & 21.7 & 48.6 & 60.9 & 6 & 34.4 &&    18.6 & 46.4 & 59.1 & 6 & 31.7 & 255.3 \\
HGR \cite{chen2020fine}                  & 22.9 & 50.2 & \textbf{63.6} & \textbf{5} & 35.9  &&   20.0 & 48.3 & 60.9 & 6 & 33.2 & 265.9 \\
\textit{Dual Encoding}         & \textbf{23.0} & \textbf{50.6} & 62.5 & \textbf{5} & \textbf{36.1} && \textbf{25.1} & \textbf{52.1} & \textbf{64.6} & \textbf{5} & \textbf{37.7} & \textbf{277.9} \\ [3pt]



\hline
\textbf{Test1k-Yu \cite{yu2018joint}} \\
CT-SAN*   \cite{yu2017end}                 & 4.4 & 16.6 & 22.3 & 35 & - && - & - & - & - & - & -  \\
JSFusion* \cite{yu2018joint}               & 10.2 & 31.2 & 43.2 & 13 & - && - & - & - & - & - & -  \\
TCE* \cite{sigir2020tree}                  & 16.1 & 38.0 & 51.5 & 10 & - && - & - & - & - & - & -  \\
Miech \etal*  \cite{miech2019howto100m}    & 14.9 & 40.2 & 52.8 & 9 & - && - & - & - & - & - & - \\
CE* \cite{liu2019use}                      & 20.9  & \textbf{48.8}  & \textbf{62.4}  & \textbf{6} & - && 20.6  & \textbf{50.3}  & \textbf{64.0}  & \textbf{5.3}  & - & \textbf{267.0} \\
\cmidrule{2-13}
W2VV \cite{dong2018predicting}            & 1.9 & 9.9 & 15.2 & 79 & 6.8 &&    17.3 & 39.3 & 50.2 & 10 & 27.8 & 133.8 \\
VSE++ \cite{faghri2017vse}               & 16.0 & 38.5 & 50.9 & 10 & 27.4 &&      16.2 & 39.3 & 51.2 & 10 & 27.4 & 212.1 \\
MEE \cite{miech2018learning}             & 14.6 & 38.4 & 52.4 & 9 & 26.1 && 15.2 & 40.9 & 53.8 & 9 & 27.9 & 215.3 \\
W2VV++ \cite{li2019w2vv++}                & 19.0 & 45.0 & 58.7 & 7 & 31.8 &&   16.9 & 42.7 & 54.6 & 8 & 29.0 & 236.9 \\
CE \cite{liu2019use}                     & 17.2	& 46.2 & 58.5 & 7  & 30.3 && 15.8 & 44.9 & 59.2 & 7 & 30.4 & 241.8 \\
TCE \cite{sigir2020tree}                 & 17.8 & 46.0 & 58.3 & 7 & 31.1 && 18.9 & 43.5 & 58.8 & 7 & 31.4 & 243.3 \\
HGR \cite{chen2020fine}                  & \textbf{21.7} & 47.4 & 61.1 & 6 & \textbf{34.0} && 20.4 & 47.9 & 60.6 & 6 & 33.4 & 259.1 \\
\textit{Dual Encoding}                      & 21.1 & 48.7 & 60.2 & \textbf{6} & 33.6 &&      \textbf{21.7} & 49.4 & 61.6 & 6 & \textbf{34.7} & 262.7 \\
\bottomrule
\end{tabular}
 }\end{table*}
 
\textbf{Baselines}. The following thirteen state-of-the-art models are compared:\\
 VSE++ \cite{faghri2017vse}: A state-of-the-art text-image retrieval model, which is commonly used as the strong baseline model for text-video retrieval. We replace its image-side branch with mean pooling on frame-level feature followed by a FC layer. \\ 
 W2VV~\cite{dong2018predicting}: Learn to project text into a visual feature space by minimizing the distance of relevant video-text pairs in the visual space. Multiple text encoding strategies including BoW, word2vec and GRU are jointly used for text encoding.  \\
 MEE~\cite{miech2018learning}: Use four different features to represent videos, and learn one latent space for each video feature. The weighted sum of similarities in four latent spaces is regarded as the final video-text similarity. \\
 W2VV++~\cite{li2019w2vv++}: An improved version of W2VV, it employs a better sentence encoding strategy and an improved triplet ranking loss.  \\
 CE~\cite{liu2019use}: Use a collaborative gating to fuse multiple features to obtain a strong video representation. \\
 TCE \cite{sigir2020tree}: Utilize a latent semantic tree \cite{tai2015improved} augmented encoder to represent text, and a GRU with multi-head self-attention mechanism \cite{vaswani2017attention} to encode videos. \\
 HGR \cite{chen2020fine}: Utilize graph convolutional network to model the connection between words, and project text and videos into three latent spaces. \\
 Mithun \etal \cite{mithun2018learning}: Project videos and text into two latent spaces, and a weighted triplet ranking loss is used for training.  \\
 Francis \etal \cite{iccv2019-francis}: Fuse multimodal features, \ie counting, activity and concpet features to obtain stronger video and text representations. \\
 JPoSE \cite{wray2019fine}: Decompose text into nouns and non-noun words, and respectively project them into two different latent spaces. \\
 CT-SAN \cite{yu2017end}: Learn to directly predict the similarity based on the fused video-text features without learning a common space, and a concept word detector is used for enhancing the video representation.    \\
 JSFusion \cite{yu2018joint}: With the same idea of \cite{yu2017end} that direly predicts the video-text similarity, a stronger joint sequence fusion is employed to fuse video and text features. \\
 Miech \etal \cite{miech2019howto100m}: Project videos and text into a common space by a gated embedding module respectively. The model is pre-trained on large-scale video-text dataset HowTo100M \cite{miech2019howto100m} and fine-tuned on MSR-VTT.

For a direct comparison, we cite numbers from the original papers whenever applicable. Meanwhile, we notice that video features used by specific papers vary. So, to make the comparison fairer, we have re-trained the following seven models which have been open-sourced, \ie W2VV, MEE, VSE++, W2VV++, CE, TCE and HGR, using the same ResNeXt-ResNet feature\footnote{MEE and CE employ a separated branch to handle the two video features respectively, while others utilize the concatenated feature as the whole input. }.



\textbf{Results}.
Table \ref{tab:sota-msrvtt} summarizes the performance comparison on three different data partitions of MSR-VTT. Though our goal is video retrieval by text, which corresponds to text-to-video retrieval in the table, video-to-text retrieval is also included for completeness.
Note that the number of the candidate videos/sentences to be retrieved in the official partition is larger than that in the other two partitions. Hence, the official partition is more challenging. As a consequence, for all the models, their performance scores on the official partition are lower than their counterparts on the other partitions. 
Consider the results using the same video features, our dual encoding model achieves the best overall performance. 


Among the results marked with asterisks (*), CE* which utilizes seven video features performs the best.
Still, the proposed Dual Encoding model using only two visual features outperforms CE* on the first two data partitions, while slightly worse on Test1k-Yu. Note that on this test set, Dual Encoding was trained on the corresponding training set of 7,010 videos as specified by \cite{yu2018joint}, whilst CE* was trained on an enlarged set of 9,000 videos. The results demonstrate the effectiveness of our proposed model.


\subsubsection{Experiments on TRECVID AVS 2016-2018}

\textbf{Data}. 
IACC.3 dataset is the largest test bed for video retrieval by text to this date, which developed for TRECVID (Ad-hoc Video Search) AVS 2016, 2017 and 2018 task \cite{AwadTRECVID16,AwadTRECVID17,AwadTRECVID18}. 
The dataset contains 4,593 Internet Archive videos with duration ranging from 6.5 minutes to 9.5 minutes and a mean duration of almost 7.8 minutes. Shot boundary detection results in 335,944 shots in total. 
Given an ad-hoc query, \eg \emph{Find shots of military personnel interacting with protesters}, the task is to return for the query a list of 1,000 shots from the test collection ranked according to their likelihood of containing the given query. Per year TRECVID specifies 30 distinct queries of varied complexity.

As TRECVID does not specify training data for the AVS task, we train the dual encoding network using the joint collection of MSR-VTT and the TGIF \cite{tgif} which contains 100K animated GIFs and 120K sentences describing visual content of the GIFs. Although animated GIFs are a very different domain, TGIF was constructed in a way to resemble user-generated video clips, \eg with cartoon, static, and textual content removed. For IACC.3, MSR-VTT and TGIF, we use the ResNeXt-ResNet video feature.


\textbf{Performance Metrics}. 
We utilize inferred Average Precision (infAP), the official performance metric used by the TRECVID AVS task. The overall performance is measured by averaging infAP scores over the queries. Their values are reported in percentage (\%).
Note that the TRECVID ground truth is partially available at the shot-level. The task organizers employ a pooling strategy to collect the ground truth, \ie a pool of candidate shots are formed by collecting the top-1000 shots from each submission and a random subset is selected for manual verification. The ground truth thus favors official participants.
As the top ranked items found by our method can be outside of the subset, infAP scores of our method are likely to be underestimated.



\textbf{Baselines}.
We include the top 3 entries of each year, \ie \cite{tv16-nii,tv16-certh,tv16-inf} for 2016, \cite{tv17-uva,tv17-waseda,tv17-vireo} for 2017 and \cite{li2018renmin,tv18-informedia,tv18-ntu} for 2018. Besides we include publications on the tasks, \ie \cite{pami2017-videostory,icmr2017-certh-avs}. 
The most of above methods are concept based, except \cite{li2018renmin,tv18-informedia,tv18-ntu}. 
Among them, \cite{li2018renmin} fuses three W2VV++ variants with different settings, 
\cite{tv18-informedia} uses two attention networks, besides the classical concept-based representation, while \cite{tv18-ntu} is based on VSE++.
Notice that visual features and training data used by these methods vary, meaning the comparison and consequently conclusions drawn from this comparison is at a system level. So for a more conclusive comparison, we re-train VSE++~\cite{faghri2017vse}, W2VV~\cite{dong2018predicting}, W2VV++~\cite{li2019w2vv++} and CE~\cite{liu2019use} using the same training data and the ResNeXt-ResNet feature.

\textbf{Results}.
Table \ref{tab:avs-perf} shows the performance of different methods on the TRECVID AVS 2016, 2017 and 2018 tasks, and the overall performance is the mean score of the three years. The proposed method again performs the best, with infAP of 15.2, 23.1 and 12.1 respectively. While \cite{li2018renmin} has a same infAP of 12.1 on the TRECVID AVS 2018 task, their solution ensembles three models. Their best single model, \ie W2VV++ \cite{li2019w2vv++}  which uses the same training data and the same ResNeXt-ResNet feature, has a lower infAP of 10.6. 
Given the same training data and feature, the proposed method outperforms VSE++, W2VV, CE with a clear margin. 
These results confirm the effectiveness of our dual encoding for large-scale video retrieval by text.


\begin{table} [tb!]
\renewcommand{\arraystretch}{1.2}
\caption{\textbf{State-of-the-art on the TRECVID AVS 2016 / 2017 / 2018}.
Symbol asterisk (*) indicates numbers directly cited from the original papers. The proposed dual encoding model consistently perform the best. 
}
\label{tab:avs-perf}
\centering
 \scalebox{0.93}{
 \begin{tabular}{@{} l lllr @{}}
\toprule
         & \multicolumn{3}{c}{\textbf{TRECVID edition}} \\
         \cmidrule{2-4}
         & \emph{2016} & \emph{2017} & \emph{2018} & \textbf{OVERALL} \\
        \midrule
\textbf{Top-3 TRECVID finalists}: \\        
Rank 1* & 5.4 \cite{tv16-nii}   & 20.6 \cite{tv17-uva}    & \textbf{12.1} \cite{li2018renmin} & -- \\
Rank 2* & 5.1 \cite{tv16-certh} & 15.9 \cite{tv17-waseda} & 8.7 \cite{tv18-informedia} & -- \\
Rank 3* & 4.0 \cite{tv16-inf}   & 12.0 \cite{tv17-vireo}  & 8.2 \cite{tv18-ntu} & -- \\
\cmidrule{1-1}
\textbf{Literature methods}: \\
VideoStory* \cite{pami2017-videostory,tv2017-uva-avs} &  8.7 & 15.0 & -- & -- \\
Markatopoulou \etal* \cite{icmr2017-certh-avs}        &  6.4 & -- & -- & -- \\
CE \cite{liu2019use}                                &  7.4 & 14.5 & 8.6 & 10.2 \\
VSE++ \cite{faghri2017vse}                          &  13.5 & 16.3 & 10.6 & 13.5 \\
W2VV \cite{dong2018predicting}                      &  14.9 & 19.8 & 10.3 & 15.0 \\  
W2VV++ \cite{li2019w2vv++}                          &  15.1 & 21.3 & 10.6 & 15.7 \\
\textit{Dual Encoding}      & \textbf{15.2} & \textbf{23.1} & \textbf{12.1} & \textbf{16.8}  \\
\bottomrule
\end{tabular}
 }\end{table}



\subsubsection{Experiments on VATEX}


\begin{table} [tb!]
\renewcommand{\arraystretch}{1.2}
\caption{\textbf{State-of-the-art on VATEX}. Our proposed model performs the best.}
\label{tab:vatex_perf}
\centering 
\scalebox{0.93}{
\begin{tabular}{@{}l*{8}{r} @{}}
\toprule
\multirow{2}{*}{\textbf{Method}}   & \multicolumn{3}{c}{\textbf{Text-to-Video}} && \multicolumn{3}{c}{\textbf{Video-to-Text}} & \multirow{2}{*}{\textbf{SumR}} \\
 \cmidrule{2-4}  \cmidrule{6-8} 
& R@1 & R@5 & R@10 && R@1 & R@5 & R@10 &  \\
\cmidrule{1-9}
W2VV   \cite{dong2018predicting}     & 14.6 & 36.3 & 46.1   &&   39.6 & 69.5 & 79.4 & 285.5 \\
VSE++  \cite{faghri2017vse}          & 31.3 & 65.8 & 76.4   &&   42.9 & 73.9 & 83.6 & 373.9   \\
CE     \cite{liu2019use}             & 31.1 & 68.7 & 80.2   &&   41.3 & 71.0 & 82.3 & 374.6 \\
W2VV++ \cite{li2019w2vv++}           & 32.0 & 68.2 & 78.8   &&   41.8 & 75.1 & 84.3 & 380.2   \\
HGR    \cite{chen2020fine}           & 35.1 & 73.5 & 83.5   &&   - & - & - & - \\
\textit{Dual Encoding}  & \textbf{36.8} & \textbf{73.6} & \textbf{83.7}   &&   \textbf{46.8} & \textbf{75.7} & \textbf{85.1} & \textbf{401.7}  \\
\bottomrule
\end{tabular}
 }\end{table}


\textbf{Data}. 
VATEX \cite{wang2019vatex} a large-scale multilingual video description dataset. Each video, collected for YouTube, has a duration of 10 seconds. Per video there are 10 English sentences and 10 Chinese sentences to describe the corresponding video content. Here, we only use the English sentences. 
We adopt the dataset partition provided by \cite{chen2020fine}, \ie 25,991 video clips for training, 1,500 clips for validation and 1,500 clips for testing, where validation and test set are obtained by randomly split the official validation set of 3,000 clips into two equal parts.


\textbf{Performance Metrics}. 
 () and SumR are used as the performance metrics.

\textbf{Baselines}. For method comparison, we consider HGR~\cite{chen2020fine}, the first work reporting video retrieval performance on VATEX. We also compare the VSE++~\cite{faghri2017vse}, W2VV~\cite{dong2018predicting}, W2VV++~\cite{li2019w2vv++} and CE~\cite{liu2019use}.

\textbf{Results}.
Table \ref{tab:vatex_perf} summarizes the performance, where all the models use the same I3D \cite{carreira2017quo} video features. Among them, VSE, VSE++ and W2VV++ are all use mean pooling over the frame-level feature to encode videos, while our dual encoding model explores multi-level features to represent videos, consistently achieving better performance. The result shows the benefit of using the multi-level feature for video representation.
Although HGR utilizes the extra semantic role annotation of sentences for text representation, our dual encoding model still slightly outperforms HGR. 



\subsubsection{Experiments on MPII-MD}


\begin{table} [tb!]
\renewcommand{\arraystretch}{1.2}
\caption{\textbf{State-of-the-art on MPII-MD}. Our proposed model performs the best.}
\label{tab:mpii_perf}
\centering 
\scalebox{0.93}{
\begin{tabular}{@{}l*{8}{r} @{}}
\toprule
\multirow{2}{*}{\textbf{Method}}   & \multicolumn{3}{c}{\textbf{Text-to-Video}} && \multicolumn{3}{c}{\textbf{Video-to-Text}} & \multirow{2}{*}{\textbf{SumR}} \\
 \cmidrule{2-4}  \cmidrule{6-8} 
& R@1 & R@5 & R@10 && R@1 & R@5 & R@10  \\
\cmidrule{1-9}
W2VV++ \cite{li2019w2vv++}           & 0.3 & 1.0 & 1.7   &&   0.1 & 0.7 & 1.3 & 5.1 \\
VSE++  \cite{faghri2017vse}          & 0.2 & 0.9 & 1.6   &&   0.8 & 2.2 & 3.6 & 9.3 \\
W2VV   \cite{dong2018predicting}     & 0.1 & 0.3 & 0.5   &&   1.3 & 4.0 & 6.1 & 12.3 \\
CE     \cite{liu2019use}             & 0.9 & 3.1 & 5.7   &&   1.1 & 3.6 & 5.8 & 20.2 \\
\textit{Dual Encoding}  & \textbf{1.7} & \textbf{4.8} & \textbf{7.0}   &&   \textbf{1.4} & \textbf{4.7} & \textbf{7.0} & \textbf{26.6} \\
\bottomrule
\end{tabular}
 }\end{table}




\begin{figure}[tb!]
\centering
\includegraphics[width=\columnwidth]{fig/fig_5}
\caption{\textbf{Selected examples of movie retrieval by text on MPII-MD}. The top retrieved shots, though not being ground truth, appear to be correct.}
\vspace{-4mm}
\label{fig:mpii-md}
\end{figure}



\begin{table*} [tb!]
\renewcommand{\arraystretch}{1.2}
\caption{\textbf{Effectiveness of Dual Encoding}. The overall performance, as indicated by Sum of Recalls, goes up as more encoding layers are added. Dual encoding exploiting all the three levels is the best.}
\label{tab:ablation-encoding}
\centering 
\scalebox{0.93}{
\begin{tabular}{l*{12}{r}c}
\toprule
\multirow{2}{*}{\textbf{Encoding strategy}} & \multicolumn{5}{c}{\textbf{Text-to-Video Retrieval}} && \multicolumn{5}{c}{\textbf{Video-to-Text Retrieval}} &  \multirow{2}{*}{\textbf{SumR}}\\
\cmidrule(l){2-6} \cmidrule(l){8-12}
  &  R@1 & R@5 & R@10  & Med r & mAP && R@1 & R@5 & R@10  & Med r & mAP & \\
\cmidrule(l){1-13}
Level 1 (Mean pooling)    &   9.7 & 26.8 & 37.0 & 23 & 18.5  &&     17.7 & 40.0 & 51.7 & 10 & 8.3 &     182.9 \\
Level 2 (biGRU)           &  10.3 & 28.5 & 39.4 & 19 & 19.7  &&     18.4 & 41.9 & 54.3 & 8  & 9.3 &     192.7 \\
Level 3 (biGRU-CNN)       &  11.1 & 30.1 & 41.6 & 17 & 20.9  &&     18.1 & 41.6 & 55.3 & 8  & 9.6 &     197.9 \\
Level 1 + 2               &  10.6 & 28.8 & 39.2 & 20 & 19.9  &&     19.1 & 43.1 & 54.5 & 8  & 9.2 &     195.3 \\
Level 1 + 3               &  11.5 & 30.0 & 40.8 & 18 & 20.9  &&     19.8 & 42.7 & 55.2 & 8  & 9.5 &     200.1 \\
Level 2 + 3               &  11.4 & \textbf{30.6} & \textbf{41.7} & \textbf{17} & \textbf{21.2} &&      19.9 & 44.3 & 55.8 & 8  & 10.1 &     203.8 \\ 
Level 1 + 2 + 3           & \textbf{11.6} & 30.3 & 41.3 & \textbf{17} & \textbf{21.2} && \textbf{22.5} & \textbf{47.1} & \textbf{58.9} & \textbf{7} & \textbf{10.5} & \textbf{211.7} \\ 
\bottomrule
\end{tabular}
 }\end{table*}



\begin{table*} [tb!]
\renewcommand{\arraystretch}{1.2}
\caption{\textbf{Performance of Dual encoding with distinct common spaces}. Dataset: MSR-VTT.}
\label{tab:ablation-space}
\centering 
\scalebox{0.9}{
\begin{tabular}{l*{12}{r}c}
\toprule
\multirow{2}{*}{\textbf{Common Space}} & \multicolumn{5}{c}{\textbf{Text-to-Video Retrieval}} && \multicolumn{5}{c}{\textbf{Video-to-Text Retrieval}} &  \multirow{2}{*}{\textbf{SumR}}\\
\cmidrule(l){2-6} \cmidrule(l){8-12}
  &  R@1 & R@5 & R@10  & Med r & mAP && R@1 & R@5 & R@10  & Med r & mAP & \\
\cmidrule(l){1-13}
2,048-d Latent Space (Conference version~\cite{cvpr2019-dual-dong})  & 11.0 & 29.2 & 39.8 & 19 & 20.2  &&      18.8 & 42.7 & 56.2 & 8 & 9.3 &  197.7 \\
1,536-d Latent Space  & 11.0 & 29.3 & 39.9 & 19 & 20.3 && 19.7 & 43.6 & 55.6 & 8 & 9.3  & 199.0   \\
512-d Concept Space   & 9.9 & 26.8 & 37.4 & 23 & 18.7 && 17.9 & 41.5 & 53.9 & 8 & 9.0 &  187.4 \\
2,048-d Hybrid, without  & 9.8 & 26.3 & 36.0 & 25 & 18.2 && 17.2 & 40.7 & 53.3 & 9 & 8.7 &  183.3 \\
2,048-d Hybrid: 1,536-d Latent Space + 512-d Concept Space       & \textbf{11.6} & \textbf{30.3} & \textbf{41.3} & \textbf{17} & \textbf{21.2} && \textbf{22.5} & \textbf{47.1} & \textbf{58.9} & \textbf{7} & \textbf{10.5} & \textbf{211.7} \\ 
\bottomrule
\end{tabular}
 }\end{table*}


\textbf{Data}. 
To evaluate the effectiveness of our proposed methods for a specific video domain, we conduct the experiment on MPII-MD \cite{rohrbach2015dataset} a movie description dataset.
We use the official data partition, that is, 56,828, 4,929 and 6,580 movie clips for training, evaluation and testing, respectively. 
Each movie clip is associated with one or two textual descriptions.

\textbf{Performance Metrics}. 
Performance of , ,  and SumR are reported.

\textbf{Baselines}. In this experiments, we compare the VSE++~\cite{faghri2017vse}, W2VV~\cite{dong2018predicting}, W2VV++~\cite{li2019w2vv++} and CE~\cite{liu2019use}. All the models are trained using the  ResNeXt-ResNet feature.

\textbf{Results}. 
Table \ref{tab:mpii_perf} summarizes the performance on the MPII-MD dataset.
Our proposed dual encoding model outperforms the other counterparts. 
It is worth noting that the performance of all models are lower on MPII-MD than that on MSR-VTT, we attribute it to the challenging nature of movie retrieval by vague descriptions on MPII-MD.
See Fig.~\ref{fig:mpii-md} for some qualitative results.





\subsection{Ablation Study} \label{ssec:ablation}



In this section, we evaluate the viability of each component on MSR-VTT, with performance reported on the full-size test set unless otherwise stated. A comparison with recent multi-modal Transformer methods is also conducted. In addition, we investigate the effectiveness of our multi-level text encoding for image-text retrieval.




\subsubsection{Multi-level Encoding versus Single-Level Encoding}
To exam the usefulness of each encoding component in the dual encoding network, we conduct an ablation study as follows. Given varied combinations of the encoding components, seven models are trained. Table \ref{tab:ablation-encoding} summarizes the choices of video and text encodings and the corresponding performance.

Among the individual encoders, biGRU-CNN is found to be the most effective. 
As more encoding layers are included, the overall performance goes up. For the last four models which combine the output from previous layers, they all outperform the corresponding counterpart using the output of a specific layer. E.g., the model with Level 1 + 2 encoding strategy outperforms the ones with Level 1 or Level 2. The results suggest that features of different levels are complementary to each other. The full multi-level encoding setup, \ie Level 1 +2 + 3, performs the best.


We also investigate single-side encoding, that is, video-side multi-level encoding with mean pooling on the text side, and text-side multi-level encoding with mean pooling on the video side. These two strategies obtain SumR of 194.5 and 191, respectively. The lower scores justify the necessity of dual encoding. The result also suggests that video-side multi-level encoding is more beneficial. 





\begin{figure*}[tb!]
\centering\includegraphics[width=2.0\columnwidth]{fig/fig_6}
\caption{\textbf{Selected examples of text-to-video retrieval by our model on MSR-VTT}. For each query, the top 3 ranked videos and the ground-truth video (marked with red ticks) are shown. In case the ground-truth video is among the top three, the fourth video will be included as well. By definition, each query has only one ground-truth video. Number on the left hand side of each video indicates the video's rank in the retrieval result. Below a specific query are its predicted concepts, visualized in the form of a tag cloud, bigger font meaning larger predicted scores. Next to the videos are their predicted concepts. Putting these tag clouds together helps us better understand the video retrieval results.}\label{fig:wordcloud}
\end{figure*}

\subsubsection{Hybrid Space versus Single Space}\label{sssec:hybrid-space}

In order to verify the effectiveness of the hybrid space, we have re-trained Dual Encoding with two alternative spaces, \ie fully latent space (the CVPR version~\cite{cvpr2019-dual-dong}) and fully concept space, respectively. Table \ref{tab:ablation-space} summarizes their performance on MSR-VTT.

Our model with the hybrid space consistently outperforms the other two counterparts with a clear margin, which shows the effectiveness of hybrid space for video-text retrieval.
Among them, although the model with the concept space is able to give some interpretation of the retrieval model, its performance is the worst. The latent space counterpart gives better performance than the concept space, but lacks interpretability. 
Moreover, simply increasing the dimensionality of the latent space, from 1,536 to 2,048, does not improve the performance. 
By contrast, the hybrid space strikes a proper balance between the retrieval performance and the interpretability.


To justify the necessity of the combined loss for concept space learning, we also report the performance of the hybrid space that excludes the triplet ranking loss from Eq. \ref{eq:p_loss}. This variant suffers a noticeable performance decrease in terms of SumR, from 211.7 to 183.3. The result shows the importance of considering the triplet ranking loss for learning a concept space that is beneficial for video-text matching. 


\begin{table*} [tb!]
\renewcommand{\arraystretch}{1.2}
\caption{\textbf{Comparison with Transformer-based multi-modal methods for video retrieval by text}. Dataset: MSR-VTT. Even though Dual Encoding uses relatively simple 2D CNN video features, it outperforms MMT on Test1k-Miech \cite{miech2018learning}, while less effective on Test1k-Yu \cite{yu2018joint}. The inclusion of Transformer features as shown in Fig. \ref{fig:with_transformer} makes Dual Encoding top the performance table consistently on all the three test sets.}
\label{tab:dual_transformer}
\centering 
\scalebox{0.78}{
\begin{tabular}{@{}l*{18}{r} @{}}
\toprule
\multirow{2}{*}{\textbf{Method}}   && \multicolumn{5}{c}{\textbf{Official full-size test set \cite{xu2016msr}}} && \multicolumn{5}{c}{\textbf{Test1k-Miech \cite{miech2018learning}}} && \multicolumn{5}{c}{\textbf{Test1k-Yu~\cite{yu2018joint}}}  \\
 \cmidrule{3-7}  \cmidrule{9-13} \cmidrule{15-19} 
&& R@1 & R@5 & R@10 & Med r & mAP && R@1 & R@5 & R@10 & Med r & mAP && R@1 & R@5 & R@10 & Med r & mAP \\
\cmidrule{1-1}  \cmidrule{3-7}  \cmidrule{9-13} \cmidrule{15-19}
ActBERT \cite{zhu2020actbert}, HowTo100M pre-training   && - & - & - & - & - && - & - & - & - & - &&     8.6 & 23.4 & 33.1 & 36 & -\\
MMT \cite{gabeur2020multi}               && - & - & - & - & - && 20.3 & 49.1 & 63.9 & 6 & - &&     24.6 & 54.0 & 67.1 & 4 & -   \\
MMT \cite{gabeur2020multi}, HowTo100M pre-training  && - & - & - & - & - && - & - & - & - & - &&     26.6 & 57.1 & 69.6 & 4 & -  \\
\cmidrule{3-7}  \cmidrule{9-13} \cmidrule{15-19}
\textit{Dual Encoding}                   && 11.6 & 30.3 & 41.3 & 17 & 21.2 && 23.0 & 50.6 & 62.5 & \textbf{5} & 36.1 &&     21.1 & 48.7 & 60.2 & 6 & 33.6 \\
\textit{Dual Encoding}, with Transformer features only && 8.5 & 25.1 & 36.1 & 22 & 17.4 && 16.8 & 41.6 & 55.8 & 8 & 28.7 &&     15.0 & 40.7 & 54.2 & 8 & 27.6  \\
\textit{Dual Encoding}, with Transformer features included  && \textbf{13.3} & \textbf{34.0} & \textbf{45.7} & \textbf{13} & \textbf{23.8} && \textbf{25.7} & \textbf{52.9} & \textbf{66.4} & \textbf{5} & \textbf{38.9} && \textbf{32.4} & \textbf{62.3} & \textbf{72.8} & \textbf{3} & \textbf{46.0}  \\
\bottomrule
\end{tabular}
 }\end{table*}




\textbf{Interpreting retrieval results with predicted concepts}. 
Fig. \ref{fig:wordcloud} shows some examples returned by our proposed dual encoding. 
Although only one correct video is annotated for each query, the top retrieved videos for Q1 (Query 1), Q2 and Q3 are typically relevant to the given query to some extent. 
In Q4, as the word \textit{beach} is used to describe the object \textit{ball}, the former is less important than the latter in this query. However, the predicted concept vector of Q4 shows that the model over emphasizes \textit{beach}, as visualized in the tag cloud. This explains that the top 2 retrieved videos are all about activities on beach. Meanwhile, for the truly relevant video, which is ranked at the position of 32, the predicted concepts are \textit{dance} and \textit{group}. Although these concepts are semantically relevant to the video content, they are irrelevant for the query.  
For Q5, concepts predicted our model, \eg \textit{cartoon} and \textit{tree}, are not precise enough to capture \textit{santa claus} the key role in the query. So our model also fails to answer this query. 
In general, we find concepts predicted by our dual encoding model  reasonable, and useful for understanding the retrieval model.



\begin{table} [tb!]
\renewcommand{\arraystretch}{1}
\caption{\textbf{Comparison with MMT in terms of model size and computation overhead at the inference stage}.  For each model, we measure the amount of FLOPs it takes to encode a given video-text pair. The computational cost of video feature extraction is excluded as that step is typically performed once in an offline mode. Numbers in parentheses indicate relative changes against MMT.}
\vspace{-3mm}
\label{tab:dual_transformer_complex}
\centering 
\scalebox{0.9}{
\begin{tabular}{@{}l*{2}{l} @{}}
\toprule
\multirow{2}{*}{\textbf{Model}}   & \multicolumn{2}{c}{\textbf{Model Complexity}} \\
 \cmidrule{2-3} 
 &  Parameters (M) & FLOPs (G) \\
\cmidrule{1-3} 
MMT               &  133.4 & 12.64 \\
\textit{Dual Encoding}            &   \textbf{~~95.9} ( 28.1\%) & \textbf{~~3.64} ( 71.2\%) \\
\bottomrule
\end{tabular}
 }\end{table}



\subsubsection{Dual Encoding versus-and-with Transformers}\label{sssec:trans}
\textbf{Dual Encoding versus multi-modal Transformers}. We compare with two recent multi-modal Transformers, \ie ActBERT~\cite{zhu2020actbert} and MMT~\cite{gabeur2020multi}, that have been evaluated on specific test sets of MSR-VTT. 
As Table \ref{tab:dual_transformer} shows, Dual Encoding clearly outperforms ActBERT and is comparable to MMT in terms of the overall performance (better on Test1k-Miech and worse on Test1k-Yu). It is worth pointing out that MMT uses a diverse set of seven video features that describe varied aspects of the video content including motion, audio, scene, OCR, face, speech, and visual appearance features. By contrast, Dual Encoding uses only two regular 2D-CNN features. 
Moreover, on Test1k-Yu \cite{yu2018joint}, Dual Encoding was trained on the corresponding training set of 7,010 videos, whilst MMT was trained on an enlarged set of 9,000 videos.
We consider in this context that Dual Encoding compares favorably to MMT. 
In addition, Table~\ref{tab:dual_transformer_complex} shows a comparison concerning model complexity. Note that ActBERT is not included, as that method remains closed-source, making a precise estimation of its model complexity impossible. Dual Encoding is smaller (28.1\% less parameters) and much faster (71.2\% less FLOPs) than MMT. 



\textbf{Dual Encoding with Transformers}. Dual Encoding is a generic framework that allows pre-trained Transformers to be included with ease. As illustrated in Fig. \ref{fig:with_transformer},  we extend the Dual Encoding network by adopting MMT trained on the HowTo100M dataset \cite{miech2019howto100m} as the visual Transformer and BERT~\cite{bert-toolbox} trained on Wikipedia and book corpora~\cite{zhu2015aligning} as the textual Transformer. The two Transformers are integrated as is without fine tuning. Their outputs, \ie a 3,584-d video feature vector and a 1,024-d sentence vector, are concatenated with the video-side and text-side multi-level encodings, respectively, in advance to hybrid space learning.
As shown in Table \ref{tab:dual_transformer}, the inclusion of the Transformer features clearly and consistently boosts the retrieval performance on all the three test sets. Meanwhile, using the Transformer features alone is less effective than the previous Dual Encoding model. Hence, our multi-level encoding modules, while built upon the relatively simple mean feature pooling, GRU and 1D-CNN encoders, remain competitive and can be used together with the Transformers to maximize the retrieval performance. 


\begin{figure}[tb!]
\centering\includegraphics[width=0.7\columnwidth]{fig/fig_7}
\vspace{-3mm}
\caption{\textbf{Integrating pre-trained visual / textual Transformers into the Dual Encoding network}. The outputs of the two transformers are respectively concatenated with the video-side and text-side multi-level encodings in advance to hybrid space learning. }\label{fig:with_transformer}
\end{figure}




\subsubsection{Multi-level Encoding for Image-Text Retrieval}


\begin{table} [tb!]
\renewcommand{\arraystretch}{1.2}
\caption{\textbf{Performance of image-text retrieval on Flickr30k and MSCOCO}. The proposed text-side multi-level encoding (MLE) is beneficial for VSE++~\cite{faghri2017vse}.}\vspace{-3mm}
\label{tab:image-text}
\centering 
\scalebox{0.9}{
\begin{tabular}{@{}l*{8}{r} @{}}
\toprule
\multirow{2}{*}{\textbf{Method}}   & \multicolumn{3}{c}{\textbf{Text-to-Image}} && \multicolumn{3}{c}{\textbf{Image-to-Text}} & \multirow{2}{*}{\textbf{SumR}}\\
 \cmidrule{2-4}  \cmidrule{6-8} 
& R@1 & R@5 & R@10 && R@1 & R@5 & R@10 & \\
\cmidrule{1-9}
\textit{On Flickr30k}                       &&& & &  &    &   \\
VSE++  & 23.1 & 49.2 & 60.7 &&     31.9 & 58.4 & 68.0 & 291.3 \\
VSE++, MLE & \textbf{24.7} & \textbf{52.3} & \textbf{65.1} &&      \textbf{35.1} & \textbf{62.2} & \textbf{71.3} & \textbf{310.7} \\
\cmidrule{1-9}
\textit{On MSCOCO}                         &&&  & &  &    &   \\
VSE++  & 33.7 & 68.8 & 81.0 && 43.6 & 74.8 & 84.6  &  389.6   \\
VSE++, MLE  &  \textbf{34.8} & \textbf{69.6} & \textbf{82.6} &&   \textbf{46.7} & \textbf{76.2} & \textbf{85.8} & \textbf{395.7}  \\
\bottomrule
\end{tabular}
 }\end{table}


\textbf{Setup}.
We investigate if the VSE++ model \cite{faghri2017vse} can be improved in its original context of image-text retrieval, when replacing its textual encoding module, which is a GRU, by the proposed multi-level encoding module. To that end, we fix all other choices, adopting the exact evaluation protocol of \cite{faghri2017vse}. That is, we use the same data split, where the training / validation / test test has 30,000 / 1,000 / 1,000 images for Flickr30K, and 82,783 / 5,000 / 5,000 images for MSCOCO. We also use the same VGGNet feature provided by \cite{faghri2017vse}. Performance of ,  and  are reported. On MSCOCO, the results are reported by averaging over 5 folds of 1,000 test images.


\textbf{Results}.
Table \ref{tab:image-text} shows the performance of image-text retrieval on Flickr30k and MSCOCO. Integrating text-side multi-level encoding into VSE++ brings improvements on both datasets.  The results suggest that the proposed text-side multi-level encoding is also beneficial for VSE++ in its original context.

