\documentclass{article}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\m}{\hspace{0.25mm}}
\newcommand{\mm}{\hspace{5mm}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}
\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{hyperref}
\usepackage{url}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}       \usepackage{arydshln}
\usepackage{amssymb}
\usepackage{cuted}
\usepackage{flushend}
\setlength\dashlinegap{2.5pt}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{rotating}
\usepackage{ifthen}
\usepackage{mathtools}
\usepackage{afterpage}
\graphicspath{ {Images/} }

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{proof}[theorem]{Proof}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{method}[theorem]{Method}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}
\newtheorem{remark}[theorem]{Remark}

\usepackage{ifthen}
\usepackage{intcalc}
\usepackage{tikz}
\usepackage{multirow}
\usetikzlibrary{decorations.pathreplacing,calligraphy}
\usetikzlibrary{arrows.meta, decorations.text, decorations.markings, decorations.pathreplacing, angles, quotes, calligraphy}
\definecolor{bittersweet}{rgb}{1.0, 0.44, 0.37}

\pgfmathsetmacro{\pathyshift}{0.8}
\global\def\rs{{0, 1, 2.1, 3.3, 4.5, 5.5, 6.5}}
\global\def\lsone{{2.15, 2.3, 2.1, 2.4, 2.1, 2.4}}
\global\def\lstwo{{2.1, 2.4, 2.2, 2.5, 2.2, 2.1}}
\global\def\lsthree{{2.3, 2.1, 2.15, 2.35, 2.5, 2.2}}
\global\def\lspathraise{0.7}
\global\def\data{{0.4, 0.7, 0.9, 0.5, 0.6, 0.9, 0.8, 0.5, 0.3, 0.3, 0.8, 1.0, 0.2, 0.4, 0.7, 0.5, 1.0, 1.0, 0.8, 0.4, 0.2, 0.6, 0.8, 0.5, 0.3, 0.5, 0.2, 0.1, 0.5, 0.2, 0.4, 0.6, 0.3, 0.2, 0.1, 0.7, 0.6, 0.2, 0.5, 0.4, 0.4, 0.1}}

\definecolor{colorls1}{HTML}{66C2A5}
\definecolor{colorls2}{HTML}{8DA0CB}
\definecolor{colorls3}{HTML}{FC8D62}

\newcommand\ncdediagram[1]{
    \begin{tikzpicture}
\pgfmathsetmacro{\logsigversion}{#1}
        
\ifthenelse{\logsigversion=0}
            {\pgfmathsetmacro{\hreduce}{1.8}}
            {\pgfmathsetmacro{\hreduce}{0}}
        
\draw[black, thick, ->] (0, 0) -- (7, 0);
        
        \ifthenelse{\logsigversion=0}
            {
                \foreach \i in {0, 6} {
                    \node at (\rs[\i], 0)[circle,fill, inner sep=1.5pt] {};
                }
                \node at (\rs[0], -0.25) {\tiny };
                \node at (\rs[6], -0.25) {\tiny };
            }
            {
                \foreach \i in {0, 1, 2, 3, 4, 5, 6} {
                    \node at (\rs[\i], 0)[circle,fill, inner sep=1.5pt] {};
                }
                \node at (\rs[0], -0.25) {\tiny };
                \node at (\rs[1], -0.25) {\tiny };
                \node at (\rs[2], -0.25) {\tiny };
                \node at (\rs[3], -0.25) {\tiny };
                \node at (\rs[4], -0.25) {\tiny };
                \node at (\rs[5], -0.25) {\tiny };
                \node at (\rs[6], -0.25) {\tiny };
            }
        \node at (3.85, -0.25) {\tiny };
    
        
    
        
        \pgfmathsetmacro{\dataix}{0}
        \pgfmathsetmacro{\midwayprev}{0}
        \pgfmathsetmacro{\xprev}{0}
        \pgfmathsetmacro{\yprev}{0}
        \foreach \i in {0, 1, 2, 3, 4, 5} {
\pgfmathsetmacro{\frac}{\rs[\i + 1] - \rs[\i]}
            \pgfmathsetmacro{\midway}{\rs[\i] + (\rs[\i + 1] - \rs[\i]) * 0.5}
            
\foreach \d in {0, 1, 2, 3, 4, 5, 6} {
                \ifthenelse{\i>0 \AND \d=0}{\pgfmathsetmacro{\y}{\data[\dataix - 1] * 0.5 + 0.1}}{\pgfmathsetmacro{\y}{\data[\dataix] * 0.5 + 0.1}}
                \pgfmathsetmacro{\x}{\rs[\i] + \d * \frac * 0.1667}
                \pgfmathsetmacro{\dx}{\frac * 0.1667}
                \ifthenelse{\i=3}
                    {
\node at (\x, \y)[circle, draw=black, fill=green!50, inner sep=1.5pt, opacity=0.3] {};
\node at (\x, \y + \pathyshift)[circle, draw=black, fill=green!50, inner sep=0.5pt, densely dashed] {};
                        \draw[green!50, thick, cap=round, dashed] (\xprev, \yprev + \pathyshift) -- (\x, \y + \pathyshift);
                    }
                    {
\ifthenelse{\logsigversion=0}{\draw (\x, -0.05) -- (\x, 0.05);}{}
\node at (\x, \y)[circle, draw=black, fill=green!50, inner sep=1.5pt] {};
\node at (\x, \y + \pathyshift)[circle, draw=black, fill=green!50, inner sep=0.5pt] {};
                        \ifthenelse{\i=0 \AND \d=0}{}{\draw[green!50, thick, cap=round] (\xprev, \yprev + \pathyshift) -- (\x, \y + \pathyshift);}
\ifthenelse{\logsigversion=0}
                            {
                                \ifthenelse{\d=6 \AND \i=5}
                                    {}
                                    {
                                        \node at (\x, 3.8 - \hreduce)[circle, draw=black, fill=orange!50, inner sep=1.5pt] {};
                                    }
                            }
                            {}
                    }
\pgfmathsetmacro{\dataixnew}{\dataix + 1}
                \global\let\dataix=\dataixnew
                \global\let\yprev=\y
                \global\let\xprev=\x
                
            }
            
            \ifthenelse{\logsigversion=1}{
                \ifthenelse{\i=3}
                    {
\node at (\rs[\i], 3.8)[circle, draw=black, fill=orange!50, inner sep=1.5pt] {};
                        \draw[->, dashed] (\rs[\i] + 0.08, 3.8) -- (\rs[\i+1] - 0.08, 3.8);
}
                    {
\draw[decoration={calligraphic brace, amplitude=3pt}, decorate, line width=1pt] (\rs[\i] + 0.05, 1.65) node {} -- (\rs[\i+1] - 0.05, 1.65);
\draw[->, dashed] (\midway, 1.8) -- (\midway, 2.0);
\node at (\midway, \lsone[\i])[circle, draw=black, fill=colorls1, inner sep=1pt] {};
                        \node at (\midway, \lstwo[\i])[circle, draw=black, fill=colorls2, inner sep=1pt] {};
                        \node at (\midway, \lsthree[\i])[circle, draw=black, fill=colorls3, inner sep=1pt] {};
\node at (\rs[\i], 3.8)[circle, draw=black, fill=orange!50, inner sep=1.5pt] {};
                        \draw[->] (\rs[\i] + 0.08, 3.8) -- (\rs[\i+1] - 0.08, 3.8);
                    }
\ifthenelse{\i=3}
                    {
                        
                    }
                    {
                        \node at (\midway, \lsone[\i] + \lspathraise)[circle, draw=black, fill=colorls1, inner sep=.5pt] {};
                        \node at (\midway, \lstwo[\i] + \lspathraise)[circle, draw=black, fill=colorls2, inner sep=.5pt] {};
                        \node at (\midway, \lsthree[\i] + \lspathraise)[circle, draw=black, fill=colorls3, inner sep=.5pt] {};
                    }
\ifthenelse{\i>0}
                    {   
\ifthenelse{\i=3 \OR \i=4}
                        {  
                            \ifthenelse{\i=3}
                                {
                                    \pgfmathsetmacro{\midmidway}{\midwayprev + (\midway - \midwayprev) * 0.5}
                                    \draw[colorls1, thick, cap=round] (\midwayprev, \lsone[\i-1] + \lspathraise) -- (\midmidway, \lsone[\i-1]*0.5 + \lsone[\i]*0.5 + \lspathraise);
                                    \draw[colorls1, thick, cap=round, densely dashed] (\midmidway, \lsone[\i-1]*0.5 + \lsone[\i]*0.5 + \lspathraise) -- (\midway, \lsone[\i] + \lspathraise);
                                    \draw[colorls2, thick, cap=round] (\midwayprev, \lstwo[\i-1] + \lspathraise) -- (\midmidway, \lstwo[\i-1]*0.5 + \lstwo[\i]*0.5 + \lspathraise);
                                    \draw[colorls2, thick, cap=round, densely dashed] (\midmidway, \lstwo[\i-1]*0.5 + \lstwo[\i]*0.5 + \lspathraise) -- (\midway, \lstwo[\i] + \lspathraise);
                                    \draw[colorls3, thick, cap=round] (\midwayprev, \lsthree[\i-1] + \lspathraise) -- (\midmidway, \lsthree[\i-1]*0.5 + \lsthree[\i]*0.5 + \lspathraise);
                                    \draw[colorls3, thick, cap=round, densely dashed] (\midmidway, \lsthree[\i-1]*0.5 + \lsthree[\i]*0.5 + \lspathraise) -- (\midway, \lsthree[\i] + \lspathraise);
                                }
                                {
                                    \pgfmathsetmacro{\midmidway}{\midwayprev + (\midway - \midwayprev) * 0.5}
                                    \draw[colorls1, thick, cap=round, densely dashed] (\midwayprev, \lsone[\i-1] + \lspathraise) -- (\midmidway, \lsone[\i-1]*0.5 + \lsone[\i]*0.5 + \lspathraise);
                                    \draw[colorls1, thick, cap=round] (\midmidway, \lsone[\i-1]*0.5 + \lsone[\i]*0.5 + \lspathraise) -- (\midway, \lsone[\i] + \lspathraise);
                                    \draw[colorls2, thick, cap=round, densely dashed] (\midwayprev, \lstwo[\i-1] + \lspathraise) -- (\midmidway, \lstwo[\i-1]*0.5 + \lstwo[\i]*0.5 + \lspathraise);
                                    \draw[colorls2, thick, cap=round] (\midmidway, \lstwo[\i-1]*0.5 + \lstwo[\i]*0.5 + \lspathraise) -- (\midway, \lstwo[\i] + \lspathraise);
                                    \draw[colorls3, thick, cap=round, densely dashed] (\midwayprev, \lsthree[\i-1] + \lspathraise) -- (\midmidway, \lsthree[\i-1]*0.5 + \lsthree[\i]*0.5 + \lspathraise);
                                    \draw[colorls3, thick, cap=round] (\midmidway, \lsthree[\i-1]*0.5 + \lsthree[\i]*0.5 + \lspathraise) -- (\midway, \lsthree[\i] + \lspathraise);
                                }
                        }
                        {
                            \draw[colorls1, thick, cap=round] (\midwayprev, \lsone[\i-1] + \lspathraise) -- (\midway, \lsone[\i] + \lspathraise);
                            \draw[colorls2, thick, cap=round] (\midwayprev, \lstwo[\i-1] + \lspathraise) -- (\midway, \lstwo[\i] + \lspathraise);
                            \draw[colorls3, thick, cap=round] (\midwayprev, \lsthree[\i-1] + \lspathraise) -- (\midway, \lsthree[\i] + \lspathraise);
                        }
                    }
                    {
                        \draw[colorls1, thick, cap=round] (\midwayprev, \lsone[\i] + \lspathraise) -- (\midway, \lsone[\i] + \lspathraise);
                        \draw[colorls2, thick, cap=round] (\midwayprev, \lstwo[\i] + \lspathraise) -- (\midway, \lstwo[\i] + \lspathraise);
                        \draw[colorls3, thick, cap=round] (\midwayprev, \lsthree[\i] + \lspathraise) -- (\midway, \lsthree[\i] + \lspathraise);
                    }
                \ifthenelse{\i=5} 
                    {
                        \draw[colorls1, thick, cap=round] (\midway, \lsone[\i] + \lspathraise) -- (\rs[\i+1], \lsone[\i] + \lspathraise);
                        \draw[colorls2, thick, cap=round] (\midway, \lstwo[\i] + \lspathraise) -- (\rs[\i+1], \lstwo[\i] + \lspathraise);
                        \draw[colorls3, thick, cap=round] (\midway, \lsthree[\i] + \lspathraise) -- (\rs[\i+1], \lsthree[\i] + \lspathraise);
                    }
                    {}
                }
                {
                }
                
\global\let\midwayprev=\midway
        }
        
\ifthenelse{\logsigversion=1}{}{\draw[->, dashed] (\rs[3] + 0.08, 3.8 - \hreduce) -- (\rs[4] - 0.08, 3.8 - \hreduce);}
        
\draw[blue!50, thick, cap=round] (\rs[0], 4.8 - \hreduce) .. controls (0.5, 3.5 - \hreduce) and (2, 5.3 - \hreduce) .. (\rs[3], 4.8 - \hreduce);
        \draw[blue!50, thick, cap=round, dashed] (\rs[3], 4.8 - \hreduce) .. controls (\rs[3] + 0.2, 4.7 - \hreduce) and (\rs[4] - 0.2, 4.3 - \hreduce) .. (\rs[4], 4.3 - \hreduce);
        \draw[blue!50, thick, cap=round] (\rs[4], 4.3 - \hreduce) .. controls (\rs[4] + 0.3, 4.2 - \hreduce) and (\rs[6] - 0.4, 4.4 - \hreduce) .. (\rs[6], 4.7 - \hreduce);
    
\ifthenelse{\logsigversion=1}
            {\node at (4, 1.7) {};}
            {}
        
\node at (8, 0) {\footnotesize Time};
        \node (data) at (8, 0.4) {\footnotesize Data };
        \node (path) at (8, 1.15) {\footnotesize Path };
        \ifthenelse{\logsigversion=1}{
            \node (logsig) at (8, 2.1) {\footnotesize }; 
            \node (logsig_path) at (8, 2.95) {\footnotesize Log-signature path}; 
        }{}
        \node (hidden) at (8, 4.7 - \hreduce) {\footnotesize Hidden state }; 
    
\draw[->] (data) -- (path);
        \ifthenelse{\logsigversion=1}
            {
                \draw[->] (path) -- (logsig);
                \draw[->] (logsig) -- (logsig_path);
                \draw [decorate, decoration={calligraphic brace,amplitude=1.5pt, mirror}] (6.5,3.6) -- node[midway, right, xshift=-5.5pt] {\scriptsize \begin{tabular}{l}Integration\\steps\end{tabular}} (6.5,4 - \hreduce);
                \draw[->] (logsig_path) -- (hidden);
            }
            {
                \draw [decorate, decoration={calligraphic brace,amplitude=1.5pt, mirror}] (6.5, 3.6 - \hreduce) -- node[midway, right, xshift=-5.5pt] {\scriptsize \begin{tabular}{l}Integration\\steps\end{tabular}} (6.5, 4 - \hreduce);
                \draw[->] (path) -- (hidden);
            }

    \end{tikzpicture}
} \def\arrnocomma {
    (0.0, 0.5903005787234183)
    (0.15853658536585366, 0.31548220294099943)
    (0.3170731707317073, 0.5723798869840881)
    (0.47560975609756095, 0.7592559931584444)
    (0.6341463414634146, 0.46000436695787333)
    (0.7926829268292683, 0.8268714937427233)
    (0.9512195121951219, 0.8165360374858748)
    (1.1097560975609757, 0.40954054026875836)
    (1.2682926829268293, 0.7450130964733412)
    (1.4268292682926829, 0.3631221102604148)
    (1.5853658536585367, 0.7482435510764813)
    (1.7439024390243902, 0.917384718293228)
    (1.9024390243902438, 0.5573926930403175)
    (2.0609756097560976, 0.7663748137356283)
    (2.2195121951219514, 0.6694620736634)
    (2.3780487804878048, 1.1176346402925919)
    (2.5365853658536586, 0.949415028614423)
    (2.6951219512195124, 0.3177597410442012)
    (2.8536585365853657, 0.6693951873464801)
    (3.0121951219512195, 0.9294900828561168)
    (3.1707317073170733, 0.9328267954184619)
    (3.3292682926829267, 0.663301293119932)
    (3.4878048780487805, 1.256064569672367)
    (3.6463414634146343, 0.472180216368223)
    (3.8048780487804876, 1.2501090850305931)
    (3.9634146341463414, 0.6636785039153019)
    (4.121951219512195, 0.650984855539094)
    (4.280487804878049, 0.7248161591552023)
    (4.439024390243903, 0.5481046159182468)
    (4.597560975609756, 1.0235344968993427)
    (4.7560975609756095, 1.0369378751495655)
    (4.914634146341464, 1.1262114265391723)
    (5.073170731707317, 0.3962487903451824)
    (5.2317073170731705, 0.9389413773558675)
    (5.390243902439025, 1.2096540128425803)
    (5.548780487804878, 1.079908936992647)
    (5.7073170731707314, 1.2109400195109703)
    (5.865853658536586, 0.37773305265847396)
    (6.024390243902439, 0.5808720577974682)
    (6.182926829268292, 0.5390596533228442)
    (6.341463414634147, 0.6221988967136689)
    (6.5, 0.7650561612631395)
}

\newcommand{\arrcomma}{
    (0.0, 0.5903005787234183), (0.15853658536585366, 0.31548220294099943), (0.3170731707317073, 0.5723798869840881), (0.47560975609756095, 0.7592559931584444), (0.6341463414634146, 0.46000436695787333), (0.7926829268292683, 0.8268714937427233), (0.9512195121951219, 0.8165360374858748), (1.1097560975609757, 0.40954054026875836), (1.2682926829268293, 0.7450130964733412), (1.4268292682926829, 0.3631221102604148), (1.5853658536585367, 0.7482435510764813), (1.7439024390243902, 0.917384718293228), (1.9024390243902438, 0.5573926930403175), (2.0609756097560976, 0.7663748137356283), (2.2195121951219514, 0.6694620736634), (2.3780487804878048, 1.1176346402925919), (2.5365853658536586, 0.949415028614423), (2.6951219512195124, 0.3177597410442012), (2.8536585365853657, 0.6693951873464801), (3.0121951219512195, 0.9294900828561168), (3.1707317073170733, 0.9328267954184619), (3.3292682926829267, 0.663301293119932), (3.4878048780487805, 1.256064569672367), (3.6463414634146343, 0.472180216368223), (3.8048780487804876, 1.2501090850305931), (3.9634146341463414, 0.6636785039153019), (4.121951219512195, 0.650984855539094), (4.280487804878049, 0.7248161591552023), (4.439024390243903, 0.5481046159182468), (4.597560975609756, 1.0235344968993427), (4.7560975609756095, 1.0369378751495655), (4.914634146341464, 1.1262114265391723), (5.073170731707317, 0.3962487903451824), (5.2317073170731705, 0.9389413773558675), (5.390243902439025, 1.2096540128425803), (5.548780487804878, 1.079908936992647), (5.7073170731707314, 1.2109400195109703), (5.865853658536586, 0.37773305265847396), (6.024390243902439, 0.5808720577974682), (6.182926829268292, 0.5390596533228442), (6.341463414634147, 0.6221988967136689), (6.5, 0.7650561612631395)
}

\newcommand{\arrarr}{
    {0.0, 0.5903005787234183}, {0.15853658536585366, 0.31548220294099943}
}

\global\def\xarrm{{0.0, 0.15853658536585366, 0.3170731707317073, 0.47560975609756095, 0.6341463414634146, 0.7926829268292683, 0.9512195121951219, 1.1097560975609757, 1.2682926829268293, 1.4268292682926829, 1.5853658536585367, 1.7439024390243902, 1.9024390243902438, 2.0609756097560976, 2.2195121951219514, 2.3780487804878048, 2.5365853658536586, 2.6951219512195124, 2.8536585365853657, 3.0121951219512195, 3.1707317073170733, 3.3292682926829267, 3.4878048780487805, 3.6463414634146343, 3.8048780487804876, 3.9634146341463414, 4.121951219512195, 4.280487804878049, 4.439024390243903, 4.597560975609756, 4.7560975609756095, 4.914634146341464, 5.073170731707317, 5.2317073170731705, 5.390243902439025, 5.548780487804878, 5.7073170731707314, 5.865853658536586, 6.024390243902439, 6.182926829268292, 6.341463414634147, 6.5}}

\global\def\yarrm{{0.5903005787234183, 0.31548220294099943, 0.5723798869840881, 0.7592559931584444, 0.46000436695787333, 0.8268714937427233, 0.8165360374858748, 0.40954054026875836, 0.7450130964733412, 0.3631221102604148, 0.7482435510764813, 0.917384718293228, 0.5573926930403175, 0.7663748137356283, 0.6694620736634, 1.1176346402925919, 0.949415028614423, 0.3177597410442012, 0.6693951873464801, 0.9294900828561168, 0.9328267954184619, 0.663301293119932, 1.256064569672367, 0.472180216368223, 1.2501090850305931, 0.6636785039153019, 0.650984855539094, 0.7248161591552023, 0.5481046159182468, 1.0235344968993427, 1.0369378751495655, 1.1262114265391723, 0.3962487903451824, 0.9389413773558675, 1.2096540128425803, 1.079908936992647, 1.2109400195109703, 0.37773305265847396, 0.5808720577974682, 0.5390596533228442, 0.6221988967136689, 0.7650561612631395}}

\global\def\xarrlen{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41}


\newcommand\mypath[1]{
\draw [green!50, line width=0.5mm] plot [#1] coordinates {
        \arrnocomma
    };
}

%
 
\newcommand\ncdenrdediagram[1]{
    \begin{tikzpicture}
\pgfmathsetmacro{\logsigversion}{#1}
    
\draw[black, thick, ->] (0, 0) -- (7, 0);
        
\pgfmathsetmacro{\summaryh}{1.7}
        \pgfmathsetmacro{\summaryeps}{0.01}
        \pgfmathsetmacro{\summaryd}{0.5}
        \pgfmathsetmacro{\cdeh}{2.5}
        \pgfmathsetmacro{\cded}{1}
        
\draw (\rs[0], -0.05) -- (\rs[0], 0.05);
        \draw (\rs[6], -0.05) -- (\rs[6], 0.05);
        \node at (\rs[0], -0.25) {\small };
        \node at (\rs[6], -0.25) {\small };

        \ifthenelse{\logsigversion=1}
            {
                \foreach \d in {1, 2, 3, 4, 5, 6} {
\draw [fill=red!10, draw=red!50, rounded corners, dashed] (\rs[\d - 1] + \summaryeps, \summaryh) rectangle (\rs[\d] - \summaryeps, \summaryh + \summaryd) node[pos=.5] {};
                    
\draw [black!50, ->] (0.5 * \rs[\d - 1] + 0.5 * \rs[\d], \summaryh + \summaryd) -- (0.5 * \rs[\d - 1] + 0.5 * \rs[\d], \cdeh);
                    
\draw [black!50, ->] (0.5 * \rs[\d - 1] + 0.5 * \rs[\d], \cdeh + \cded) -- (0.5 * \rs[\d - 1] + 0.5 * \rs[\d], \cded + \cdeh + 0.5);
                }
            }
            {
            }
            
        \pgfmathsetmacro{\cdeeps}{0.}
        \ifthenelse{\logsigversion=0}
            {
\draw [fill=orange!10, draw=orange!50, rounded corners, dashed] (\rs[0], \cdeh - \cdeeps) rectangle (\rs[6], \cdeh + \cded - \cdeeps) node[pos=.5] {Neural CDE};
            }
            {
\draw [fill=orange!10, draw=orange!50, rounded corners, dashed] (\rs[0], \cdeh) rectangle (\rs[6], \cdeh + \cded) node[pos=.5] {Neural RDE};
            }
        
\ifthenelse{\logsigversion=1}{
                \mypath{}
            }
            {
                \mypath{smooth}
            }
\foreach \coord [count=\i] in \arrcomma {
            \node at \coord [circle, draw=black, fill=green!50, inner sep=0.5pt] {};
        }
        \foreach \i in \xarrlen {
            \ifthenelse{\logsigversion=1}{
                \draw [black!50, ->] (\xarrm[\i], \yarrm[\i]) -- (\xarrm[\i], \summaryh);
            }
            {
                \draw [black!50, ->] (\xarrm[\i], \yarrm[\i]) -- (\xarrm[\i], \cdeh);
                \draw [black!50, ->] (\xarrm[\i], \cdeh + \cded) -- (\xarrm[\i], \cdeh + \cded + 0.5);
            }
        }
        
\pgfmathsetmacro{\hiddenh}{1.5}
        \draw[blue!50, line width=0.5mm, cap=round] (\rs[0], 3 + \hiddenh) .. controls (1.5, 1.6 + \hiddenh) and (\rs[4], 4 + \hiddenh) .. (\rs[6], 2.5 + \hiddenh);
        
\node at (8, 0) {\footnotesize Time};
        \ifthenelse{\logsigversion=0}{
                \node (cde) at (8, 3) {CDE}; 
                \node (path) at (8, 0.8) {\footnotesize \begin{tabular}{c}Path, \
    Z_a = \xi,\quad Z_t = Z_a + \int^{t}_{a} f(Z_s)\,\dby X_s \quad \text{for } t \in (a, b].
    \label{eq:kidger_cde}

        \int_a^t f(Z_s) \dby X_s = \int_a^t f(Z_s) \dot X_s \dby s,\\ \text{with} \; \dot X_s = \frac{\dby X_r}{\dby r}(s).

    Z_{t_0} = \xi_\theta(t_0, x_0), \text{ with } Z_t = Z_{t_0} + \int^{t}_{t_0} f_\theta(Z_s)\,\dby X_s, \\
    \text{and } Y_t = \ell_\theta(Z_t) \text{ for } t \in (t_0, t_n]
    \label{eq:kidger_ncde}

     Z_t = Z_{t_0} + \int^{t}_{t_0} g_{\theta, X}(Z_s, s)\,\dby s,
     \label{eq:kidger_cde_evaluation}

    g_{\theta, X}(Z, s) = f_\theta(Z) \dot X_s. 
    \label{eq:kidger_g_X}

    S^{i_1,...i_k}_{a, b}(X) = \underset{\scriptscriptstyle a < t_1 < ... < t_k < b}{\int ... \int} \prod^k_{j=1} \frac{\dby X^{i_j}}{\dby t} (t_j) \dby t_j,
    \label{eq:signature}

    \mathrm{Sig}^N_{a, b}(X) = \Big(\big\{S^i_{a, b}(X)^{(i)}\big\}_{i = 1}^{d}, \big\{S^{i, j}_{a, b}(X)\big\}_{i, j = 1}^{d}, \\ 
    \ldots, \big\{S^{i_1,\ldots, i_N}_{a,b}(X)\big\}_{i_1, \ldots, i_N = 1}^{d}\Big).
    \label{eq:truncated_path_signature}

    Z_t &\approx Z_a + \int^{t}_{a} \Big(f(Z_a) + D_f(Z_a)(Z_s - Z_a)\Big) \frac{\dby X}{\dby t}(s)\m\dby s\nonumber\2pt]
    &\approx Z_a + f(Z_a)\int^{t}_{a} \frac{\dby X}{\dby t}(s)\,\dby s\nonumber
     + D_f(Z_a)f(Z_a)\int^{t}_{a}\int^{s}_{a} \frac{\dby X}{\dby t}(u)\,\dby u\frac{\dby X}{\dby t}(s)\m\dby s\nonumber\
    \vspace{-1.5em}
    \caption{Signature (Taylor) expansion of a CDE. The action of the vector field f on the depth-N signature is a matrix-vector product and is fully described, for any N, in \cite{logode2014estimate}.}\label{eq:simple_taylor}
\end{figure*}

\paragraph{(Log-)Signatures and CDEs} In Figure \ref{eq:simple_taylor} we give the equations for how log-signatures arise in the solution of CDEs. Begin by letting  denote the Jacobian of a function . Now expand equation (\ref{eq:kidger_cde}) by linearising the vector field  and neglecting higher order terms.

This is simply the Taylor Expansion of the CDE. The Taylor coefficients are precisely these signature terms, thus demonstrating how signatures are intrinsically linked to the solutions of CDEs. Higher order Taylor expansions results in corrections using higher order signature terms.





\subsection{The Log-ODE Method}
Recall for  that . The log-ODE method states that  where

and . Here  is the same as in equation (\ref{eq:kidger_ncde}), and the relationship between  to  is given in Appendix \ref{apx:logode}.

That is, the solution of the CDE may be approximated by the solution to an ODE. This is typically applied locally: pick some points  such that , split up the CDE of equation (\ref{eq:kidger_cde}) into an integral over , an integral over , and so on, and apply the log-ODE method to each interval separately. A CDE treated in this way is, for the purposes of this exposition, termed a \textit{rough differential equation}.

See Appendix \ref{apx:logode} for the precise details and Appendix \ref{apx:logodeconv} for a proof of convergence. For the reader familiar with the Magnus expansion for linear differential equations \citep{magnus2008expansion}, then the log-ODE method is a generalisation.



 %
 
\begin{figure*}[!hbtp]
    \begin{subfigure}[t]{.5\linewidth}
        \centering
            \resizebox{\linewidth}{!}{
                \ncdediagram{0}
            }
    \end{subfigure}\begin{subfigure}[t]{.5\linewidth}
        \centering
        \resizebox{\linewidth}{!}{
            \ncdediagram{1}
        }
    \end{subfigure}
    \caption{An overview of the log-ODE method applied to Neural RDEs. \textbf{Left:} A single step (CDE or RDE) model. The path  is quickly varying, meaning a lot of integration steps are needed to resolve it. \textbf{Right:} The Neural RDE utilising the log-ODE method with integration steps larger than the discretisation of the data. The path of log-signatures is more slowly varying (in a higher dimensional space), and needs fewer integration steps to resolve.}
    \label{fig:ncde_plots}
\end{figure*}

\section{Method}\label{sec:method}
We move on to introducing the neural rough differential equation.

Recall that we observe some time series , and have constructed a piecewise linear interpolation  such that .

We now pick points  such that . In principle these can be variably spaced but in practice we will typically space them equally far apart. The total number of points  should be much smaller than . The choice and spacing of  will be a hyperparameter.

We also pick a depth hyperparameter . In section \ref{sec:theory} we introduced the depth- log-signature transform. For  and  the log-signature of  over the interval  was defined to be a particular collection of statistics ; specifically those statistics that best describe how  drives the CDE equation (\ref{eq:kidger_cde}).

\subsection{The Rough Hidden State Update}

Recall how the Neural CDE formulation of equation (\ref{eq:kidger_ncde}) was solved via equations (\ref{eq:kidger_cde_evaluation}), (\ref{eq:kidger_g_X}). For the rough approach we begin by replacing (\ref{eq:kidger_g_X}) with the piecewise

where  is an arbitrary neural network, and the right hand side denotes a matrix-vector product between  and the log-signature. Equation (\ref{eq:kidger_cde_evaluation}) then becomes

This may now be solved as a (neural) ODE using standard ODE solvers.

We give an overview of this process in figure \ref{fig:ncde_plots}. The left hand side represents a single step method, as in the existing approach to Neural CDEs. The right hand side depicts a rough approach that takes steps larger than the discretisation of the data in exchange for additional terms of the log-signature.

\subsection{Neural RDEs Generalise Neural CDEs}\label{section:generalise}
Suppose we happened to choose  and . Then the log-signature term is 

Recall that the depth 1 log-signature is just the increment of the path over the interval. So this becomes

that is to say the same as obtained via the original method if using linear interpolation. In this way the Neural RDE approach generalises the existing Neural CDE approach.
    
\subsection{Discussion}\label{subsec:tradeoff}

\paragraph{Length/Channel Trade-Off} 
The sequence of log-signatures is now of length , which was chosen to be much smaller than . As such, it is much more slowly varying over the interval  than the original data, which was of length . The differential equation it drives is better behaved, and so larger integration steps may be used in the numerical solver. This is the source of the speed-ups of this method; we observe typical speed-ups by a factor of about 10.



\paragraph{Memory Efficiency} Long sequences need large amounts of memory to perform backpropagation-through-time (BPTT). As with the original Neural CDEs, the log-ODE approach supports memory-efficient backpropagation via the adjoint equations. If the vector field  requires  memory, and the time series is of total length , then backpropagating through the solver requires  memory whilst the adjoint method requires only ; see \citet{kidger2020neural}.

\paragraph{The Log-signature as a Preprocessing Step} When training a model in practice, the log-signatures need only be computed once and thus the computation can be performed as part of data preprocessing. Log-signatures can also be easily computed in an online fashion, making the model suitable for such problems. 

\paragraph{Structure of } The description here aligns with the log-ODE scheme described in equation (\ref{eq:log-ode}). There is one discrepancy: we do not attempt to model the specific structure of . This is in principle possible, but is computationally expensive. Instead, we model  as a neural network directly. This need \emph{not} necessarily exhibit the requisite structure, but as neural networks are universal approximators \citep{pinkus, deepandnarrow} then this approach is at least as general from a modelling perspective.

\paragraph{Ease of Implementation}
This method is straightforward to implement using pre-existing tools.

There are standard libraries available for computing the log-signature transform: we use Signatory \citep{signatory}. As equation (\ref{eq:log_ode_ncde}) is an ODE, it may be solved directly using tools such as \texttt{torchdiffeq} \citep{torchdiffeq}.

As an alternative, we note that the form of equation (\ref{eq:log_ode_g}) is that of equation (\ref{eq:kidger_g_X}), with the driving path taken to be piecewise linear in log-signature space. Computation of the log-signatures can therefore be considered as a preprocessing step, producing a sequence of log-signatures. From this we may construct a path in log-signature space, and apply existing tools for neural CDEs. (Rather than tools for neural ODEs.) This idea is summarised in figure \ref{fig:ncde_plots}. We make this approach available in the [redacted] open source project.





\paragraph{Applications} In principle, a Neural RDE may be applied to solve any Neural CDE problem. However, we typically observe limited benefit on relatively short time series: the original Neural CDE formulation works well enough, and there is little room to see either speed or loss/accuracy improvements via this approach.

The situation changes for long time series. Here, the existing approach struggles as the length of the time series grows. Performance worsens, and speed drops due to the sheer number of forward evaluations. This is the same behaviour as for RNNs.

Now, the reduction in length (from  to ) is highly beneficial. Moreover, the compression performed by the log-signature is also of benefit: closely-sampled points will be typically be strongly correlated, and there is little to be gained by treating them all individually.

In addition, there are two advantages shared by both Neural CDEs and Neural RDEs, that make them suitable for long time series. The first is the sharply reduced memory requirements of the adjoint method. For example (chosen arbitrarily without cherry-picking) in one experiment we see a reduction in memory usage from GB to just MB.

The second is that as both operate in continuous time, the steps in the numerical solver may be decoupled from the sampling rate of the data: steps are taken with respect to the complexity of the data, not just its sampling rate. In particular a slowly-varying but densely-sampled path would still be fast without requiring many integration steps.

\paragraph{The Depth and Step Hyperparameters}To solve a Neural RDE accurately via the log-ODE method, we should be prepared to take the depth  suitably large, or the intervals  suitably small. Accomplishing this would often require that they are taken relatively large or relatively small, respectively. Instead, we treat these as hyperparameters. This makes use of the log-ODE method a modelling choice rather than an implementation detail.

Increasing step size will lead to faster (but less informative) training by reducing the number of operations in the forward pass. Increasing depth will lead to slower (but more informative) training, as more information about each local interval is used in each update.






 
\section{Experiments} \label{sec:experiments}
We run experiments applying Neural RDEs to four real-world datasets. Every problem was chosen for its long length. The lengths are sufficiently long that adjoint-based backpropagation \citep{neural2018ode} was often needed simply to avoid running out of memory at any reasonable batch size. Every problem is regularly sampled, so we take .

Recall that the Neural RDE approach features two hyperparameters, corresponding to log-signature depth and step size. Good choices will turn out to have a dramatic positive effect on performance. Accordingly for every experiment we run Neural RDEs for all depths in  and all step sizes in . Depth 1 and step 1 are not considered as both reduce onto the Neural CDE model, as discussed in section \ref{section:generalise}. In practice, when choosing a final model, one would choose that with depth and step values that minimise the validation loss, as in any hyperparamter value selection.

We compare against two baseline models. The first is a Neural CDE; as the model we are extending then comparisons to this are our primary concern. For context we also additionally include a baseline against the ODE-RNN introduced in \citet{rubanova2019latent}. For both of these models, we also run experiments on the full range of step sizes described above.

For the Neural CDE model, increased step sizes correspond to na{\"i}ve subsampling of the data (in accordance with section \ref{section:generalise}). For the ODE-RNN model, we instead fold the time dimension into the feature dimension, so that at each step the ODE-RNN model sees several adjacent time points. This represents an alternate technique for dealing with long time series, so as to provide a reasonable benchmark.





For each model, and each hyperparameter combination, we run the experiment three times and report the mean and standard deviation of the test metrics. We additionally report mean training times and memory usages. 



Precise details of hyperparameter selection, optimisers, normalisation, and so on can be found in Appendix \ref{apx:experiments}. For brevity, we provide results for only some of the step sizes here. The full results are described in Appendix \ref{apx:results}.

\subsection{Classifying EigenWorms}
\begin{table}[t]
    \small
    \setlength{\tabcolsep}{4.pt}
    \begin{center}
        \begin{tabular}{ccccc}
        \toprule
        \textbf{Model} & \textbf{Step} & \textbf{Accuracy (\%)} & \textbf{Time (Hrs)} & \textbf{Mem (Mb)} \\
        \midrule
          & 1    &  -- & -- & -- \\
        ODE-RNN  & 4    &   35.0  1.5 &           0.8 &        3629.3 \\
        (folded) & 32   &   32.5  1.5 &           0.1 &         532.2 \\
          & 128  &   47.9  5.3 &           0.0 &         200.8 \\
          
        \hdashline\noalign{\vskip 0.5ex}
          & 1    &  62.4  12.1 &          22.0 &         176.5 \\
        \multirow{2}{*}{NCDE}    & 4    &  66.7  11.8 &           5.5 &          46.6 \\
          & 32   &  64.1  14.3 &           0.5 &           8.0 \\
          & 128  &   48.7  2.6 &           0.1 &           3.9 \\
          
        \midrule
        \multirow{3}{*}{\begin{tabular}{c} NRDE\depth 3)\end{tabular}}  & 4    &   76.9  9.2 &           2.8 &         856.8 \\
         & 32   &   \textbf{75.2  3.0} &           0.6 &         134.7 \\
          & 128  &   68.4  8.2 &           0.1 &          53.3 \\
        \bottomrule
        \end{tabular}
    \end{center}
    \caption{EigenWorms dataset: mean  standard deviation of test set accuracy measured over three repeats. Also reported are the mean memory usage and training time. For all models a variety of step sizes are considered. For the Neural RDE we additionally investigate varying depths. (Recalling that the NCDE is a depth-1 NRDE.) `--' denotes that the model could not be run within GPU memory. Bold denotes the best model score for a given step size, and  denotes that the score was the best achieved over all models and step sizes.}
    \label{tab:eigenworms}
\end{table} 

Our first example uses the EigenWorms dataset from the UEA archive from \citet{bagnall16bakeoff}. This consists of time series of length 17\,984 and 6 channels (including time), corresponding to the movement of a roundworm. The goal is to classify each worm as either wild-type or one of four mutant-type classes.



Results are shown in Table \ref{tab:eigenworms}. We begin by seeing that the step-1 Neural CDE model takes roughly a day to train. Switching to Neural RDEs speeds this up by an order of magnitude, to roughly two hours. Moreover doing so dramatically improves accuracy, by up to , reflecting the classical difficulty of learning from long time series.

Meanwhile na{\"i}ve subsampling approaches for the Neural CDE method only achieve speed-ups without performance improvements. The folded ODE-RNN model performs poorly, attaining the worst score for any step size whilst imposing a significantly higher memory burden. 





Results across all step sizes may be found in Appendix \ref{apx:results}.


\subsection{Estimating Vitals Signs from PPG and ECG data}
\begin{table*}[t]
    \small
    \begin{center}
        \begin{tabular}{cccccccccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} &
        &
        \multirow{2}{*}{\textbf{Step}} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{\textbf{Time (Hrs)}} & \multirow{2}{*}{\textbf{Memory (Mb)}} \\
        \cmidrule(lr){4-6} \cmidrule(lr){7-9}
        & & & RR & HR & SpO & RR & HR & SpO & \\
         
        \midrule
        
         & & 1   &  -- & 13.06    0.0 & -- & -- & 10.5 & -- & 3653.0 \\
        ODE-RNN (folded) & \multirow{2}{*}{} & 8   & 2.47  0.35 & 13.06  0.00 & 3.3  0.00 & 1.5 & 1.2 & 0.9 & 917.2 \\
         & & 128  &  1.62  0.07 &  13.06  0.00 &    3.3  0.00 &           0.2 &           0.1 &           0.1 &               81.9 \\
         & & 512  &  1.66  0.06 &   6.75  0.9 &  1.98  0.31 &           0.0 &           0.1 &           0.1 &               40.4 \\
         
        \hdashline\noalign{\vskip 0.5ex}
        
        & & 1   &  2.79  0.04 &   9.82  0.34 &  2.83  0.27 &          23.8 &          22.1 &          28.1 &               56.5 \\
        \multirow{2}{*}{NCDE} & \multirow{2}{*}{} & 8   &   2.80  0.06 &  10.72  0.24 &  3.43  0.17 &           3.0 &           2.6 &           4.8 &               14.3 \\
        &  & 128 &  2.64  0.18 &  11.98  0.37 &  2.86  0.04 &           0.2 &           0.2 &           0.3 &                8.7 \\
        &  & 512 &  2.53  0.03 &  12.22  0.11 &  2.98  0.04 &           0.1 &           0.0 &           0.1 &                8.4 \\
          
        \midrule
        & & 8   &  2.63  0.12 &   8.63  0.24 &  2.88  0.15 &           2.1 &           3.4 &           3.3 &               21.8 \\
        NRDE (depth 2) &  & 128  &  1.86  0.03 &   6.77  0.42 &  1.95  0.18 &           0.3 &           0.4 &           0.7 &               10.9 \\
        &  & 512 &  1.81  0.02 &   5.05  0.23 &  2.17  0.18 &           0.1 &           0.2 &           0.4 &               10.3 \\
        \hdashline\noalign{\vskip 0.5ex}
        & & 8   &  \textbf{2.42  0.19} &    \textbf{7.67  0.40} &  \textbf{2.55  0.13} &           2.9 &           3.2 &           3.1 &               43.3 \\
        NRDE (depth 3) &   & 128  &  \textbf{1.51  0.08} &   \textbf{2.97  0.45} &  \textbf{1.37  0.22} &           0.5 &           1.7 &           1.7 &               17.3 \\
        &  & 512  &  \textbf{1.49  0.08} &   \textbf{3.46  0.13} &  \textbf{1.29  0.15} &           0.3 &           0.4 &           0.4 &               15.4 \\
        \bottomrule
        \end{tabular}
    \end{center}
    \caption{The three experiments on BIDMC datasets: mean  standard deviation of test set  loss, measured over three repeats, over each of three different vital signs prediction tasks (RR, HR, SpO). Also reported are the memory usage and training time. Only mean times are shown for space. For all models a variety of step sizes are considered. For the Neural RDE we additionally investigate varying depths. (Recalling that the NCDE is a depth-1 NRDE.) `--' denotes that the model could not be run within GPU memory. Bold denotes the best model score for a given step size, and  denotes that the score was the best achieved over all models and step sizes.}
    \label{tab:bidmc}
\end{table*} \begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Images/BIDMC_cmaps.png}
    \caption{Heatmap depicting normalised losses on the three BIDMC datasets for differing step sizes and depths. We can see that the point of lowest MSE (deepest red) has step  and depth , and that performance worsens for very long steps. This represents the depth/step tradeoff for long length time series. }
    \label{fig:bidmc_heat}
\end{figure*}

Next we consider three separate problems, using data from the TSR archive \citep{MonashTSRegressionArchive}, coming originally from the Beth Israel Deaconess Medical Centre (BIDMC).

We aim to predict a person's respiratory rate (RR), their heart rate (HR), or their oxygen saturation (SpO2) at the end of the sample, having observed PPG and ECG data over the length of the sample. The data is sampled at 125Hz and each series has length 4\,000. There are 3 channels (including time). We evaluate performance with the  loss.

The results are shown in table \ref{tab:bidmc}.

We find that the depth  Neural RDE is the top performer for every task at every step size, reducing test loss by -- versus the Neural CDE. Moreover, it does so with roughly an order of magnitude less training time.

We attribute the improved test loss to the Neural RDE model being better able to learn long-term dependencies due to the reduced sequence length. Note that the performance of the rough models actually improves as the step size is increased. This is in contrast to Neural CDE, which sees a degradation in performance.

The ODE-RNN model, besides using significantly more memory, struggles to train effectively when the sequence length is long. Training improves as the sequence size is shortened, but still produces results substantially worse than those achieved by the Neural RDE.

As a visual summary of these results, including the full range of step sizes, we also provide heatmaps in Figure \ref{fig:bidmc_heat}.

The full results across the full range of step sizes may be found in Appendix \ref{apx:results}. 
\section{Limitations}

\paragraph{Number of hyperparameters} Two new hyperparameters -- truncation depth and step size -- with substantial effects on training time and memory usage must now also be tuned.

\paragraph{Number of input channels} The log-ODE method is most feasible with few input channels, as the number of log-signature channels  grows exponentially in . For larger  then the available parallelism may become saturated. 
\section{Related Work}
CNNs and Transformers have been shown to offer improvements over RNNs for modelling long-term dependencies \citep{bai2018empirical, li2019enhancing}, although the latter in particular have typically focused on language modelling. On a more practical note, Transformers are famously  in the length of the time series . Several approaches have been introduced to reduce this, for example \citet{li2019enhancing} reduce this to . Extensions specifically to long sequences do exist \citep{sourkov2018igloo}, but again these typically focus on language modelling rather than multivariate time series data. 

There has also been some work on long time series for classic RNN (GRU/LSTM) models.

\citet{wisdom2016full, jing2019gated} show that unitary or orthogonal RNNs can mitigate the vanishing/exploding gradients problem. However, they are expensive to train due to the need to compute a matrix inversion at each training step. \citet{chang2017dilated} introduce dilated RNNs with skip connections between RNN states, which help improve training speed and learning of long-term dependencies. \citet{campos2017skip} introduce the `Skip-RNN' model, which extend the RNN by adding an additional learnt component that skips state updates. \citet{li2018independently} introduce the `IndRNN' model, with particular structure tailored to learning long time series.

One meaningful comparison is to hierarchical subsampling as in \citet{graves2012supervised, de2015survey}. There the data is split into windows, an RNN is run over each window, and then an additional RNN is run over the first RNN's outputs; we may describe this as an RNN/RNN pair. \citet{liao2019learning} then perform the equivalent operation with a log-signature/RNN pair. In this context, our use of log-ODE method is analogous to an log-signature/NCDE pair.

In comparison to \citet{liao2019learning}, this means moving from an inspired choice of pre-processing to an actual implementation of the log-ODE method. In doing so the differential equation structure is preserved. Moreover this takes advantage of the synergy between log-signatures (which extract statistics on how data drives differential equations), and the controlled differential equation it then drives. Broadly speaking these connections are natural: at least within the signature/CDE/rough path community, it is a well-known but poorly-published fact that RNNs, (log-)signatures, and (Neural) CDEs are all related; see for example \citet{kidger2020neural} for a little exposition on this.

\citet{de2019gru, lechner2020learning} amongst others consider continuous time modifications to GRUs and LSTMs, improving the learning of long-term dependencies.

\citet{lmu, hippo} consider links with ODEs and approximation theory, with the goal of improving the long-term memory capacity of RNNs. Given the differential equation structure both they and we consider, a hybridisation of these techniques seems like a promising line of future inquiry. 
\section{Conclusion}
We have introduced \textit{neural rough differential equations} as an approach to continuous-time time series modelling. These extend Neural CDEs, driving the hidden state not by point evaluations but by interval summarisations of the underlying time series or control path. Neural RDEs may still be solved via ODE methods, and thus retain both adjoint backpropagation and continuous dynamics. As they additionally reduce the effective length of the control path, we observe substantial practical benefits in applying Neural RDEs to long time series. In this regime we report significant training speed-ups, model performance improvements, and reduced memory requirements, on problems of length up to 17\,000. 
\section*{Acknowledgements} JM was supported by the EPSRC grant EP/L015803/1 in collaboration with Iterex Therapuetics. PK was supported by the EPSRC grant EP/L015811/1. CS was supported by the EPSRC grant EP/R513295/1. JF was supported by the EPSRC grant EP/N509711/1. JM, CS, PK, JF were supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1.

\bibliography{references}
\bibliographystyle{style/icml2021}




\newpage
\onecolumn
\appendix
\begin{center}
	\huge Supplementary material
\end{center}
	
In sections \ref{apx:logode} and \ref{apx:logodeconv}, we give a more thorough introduction to solving CDEs via the log-ODE method.

In section \ref{apx:experiments} we discuss the experimental details such as the choice of network structure, computing infrastructure and hyperparameter selection approach.

In section \ref{apx:results} we give a full breakdown of every experimental result.

\section{An introduction to the log-ODE method for controlled differential equations}\label{apx:logode}

The log-ODE method is an effective method for approximating the controlled differential equation:
3pt]
Y_0 & = \xi,\nonumber

\boldsymbol{a} + \boldsymbol{b} & =\big(a_{0} + b_{0}, a_{1} + b_{1}, \cdots\big),\label{eq:tensoradd}\\
\boldsymbol{a} \otimes \boldsymbol{b} & =\big(c_{0}, c_{1}, c_{2}, \cdots\big),\label{eq:tensormult}
\label{eq:tproduct2}
c_{n} := \sum_{k=0}^{n}a_{k}\otimes b_{n-k}.
\label{eq:fullsig}
S_{s,t}\big(X\big) := \Big(1\hspace{0.5mm}, X_{s,t}^{(1)}, X_{s,t}^{(2)}, X_{s,t}^{(3)},\ldots\Big)\in T\big(\big(\R^d\big)\big),

X_{s,t}^{(n)} := \idotsint\displaylimits_{s<u_{1}<\cdots<u_{n}<t}\dby X_{u_{1}}\otimes\cdots\otimes \dby X_{u_{n}}\in\big(\R^d\big)^{\otimes n}.\nonumber
\label{eq:truncsig}
S_{s,t}^{N}\big(X\big) := \Bigg(\hspace{0.5mm}1\hspace{0.5mm}, X_{s,t}^{(1)}, X_{s,t}^{(2)},\ldots,X_{s,t}^{(N)}\Bigg)\in T^N\big(\R^d\big),

\int_0^t\int_0^s \dby X_u^{i} \dby X_s^{j} + \int_0^t\int_0^s \dby X_u^{j} \dby X_s^{i} = X_t^{i}X_t^{j},
\nonumber

\log(\boldsymbol{a}) & := \log(a_{0}) + \sum_{n=1}^{\infty}\frac{(-1)^{n}}{n}\bigg(\boldsymbol{1} - \frac{\boldsymbol{a}}{a_{0}}\bigg)^{\otimes n},\label{eq:fulltensorlogseries}

\log^N(\boldsymbol{a})  & := P_N\big(\log(\boldsymbol{\widetilde{a}})\big),\label{eq:trunctensorlogseries}

    \beta(d, N) = \sum_{k = 1}^N \frac{1}{k} \sum_{i | k} \mu\left(\frac{k}{i}\right) d^i
    \label{eq:logsig-dim}

f^{\circ (0)}(y) & := y,\\
f^{\circ (1)}(y) & := f(y),\\
f^{\circ (k + 1)}(y) & := D\big(f^{\circ k}\big)(y)f(y),

\mathrm{Taylor}(Y_s, f, S_{s,t}^N(X)) := \sum_{k=0}^N f^{\circ k}(Y_s)\pi_k \big(S_{s,t}^N(X)\big),
\label{eq:taylor_def}

\frac{\dby z}{\dby u} & = \widehat{f}(z)\logsig_{s,t}^N(X),\label{eq:first_logode_def}\
Then the log-ODE approximation of  given  and  is defined as

\end{definition}
\begin{remark}
Our assumptions of  ensure that  is either globally bounded and Lipschitz continuous or linear. Hence both the Taylor and log-ODE methods are well defined.
\end{remark}
\begin{remark}
It is well known that the log-signature of a path  lies in a certain free Lie algebra (this is detailed in section 2.2.4 of \citet{roughpath2007notes}). Furthermore, it is also a theorem that the Lie bracket of two vector fields is itself a vector field which doesn't depend on choices of basis.
By expressing  using a basis of the free Lie algebra, it can be shown that only the vector field  and its (iterated) Lie brackets are required to construct the log-ODE vector field . In particular, this leads to our construction of the log-ODE (\ref{eq:log-ode}) using the Lyndon basis of the free Lie algebra (see \cite{reizenstein2017logsig} for a precise description of the Lyndon basis). We direct the reader to \citet{lyons2014streams} and \citet{logode2014estimate} for further details on this Lie theory.
\end{remark}



\begin{figure*}
    \centering
    \resizebox{\textwidth}{!}{

    \begin{tikzpicture}
\node (initial) at (0, 0) [black, thick, rounded corners, rectangle, draw=black!50, fill=green!30, minimum height=0.8cm] {\scriptsize  };
\node (taylor) at (5, 0) [black, thick, rounded corners, rectangle, draw=black!50, fill=red!30, minimum height=0.8cm] {\scriptsize };
\node (logsig) at (0, -2) [black, thick, rounded corners, rectangle, draw=black!50, fill=yellow!30, minimum height=0.8cm] {
            \scriptsize 
            \begin{tabular}{l}
                \\
                    
            \end{tabular}
        };
\node (logode) at (5, -2) [black, thick, rounded corners, rectangle, draw=black!50, fill=red!30, minimum height=0.8cm, label={[shift={(0, -0.08)}]\tiny Log-ODE method}] {\scriptsize };
\node (approx) at (5, -0.85) [rotate=90] {\Large };
        
\draw[->] (initial) -- (taylor) node[midway, above, yshift=-0.4ex] {\tiny \begin{tabular}{c}Action of  on\\signature of \end{tabular}};
        \draw[->] (initial) -- (logsig) node[midway, left, xshift=1.5ex] {\tiny \begin{tabular}{c}Action of  on\\log-signature of \end{tabular}};
        \draw[->] (logsig) -- (logode) node[midway, above, yshift=-0.4ex] {\tiny Solve ODE on };

    \end{tikzpicture}
}     \caption{Illustration of the log-ODE and Taylor methods for controlled differential equations.} 
    \label{fig:logode_taylor}
\end{figure*}

To illustrate the log-ODE method, we give two examples:
\begin{example}[The ``increment-only'' log-ODE method]
When , the ODE (\ref{eq:first_logode_def}) becomes
3pt]
z(0) & = Y_s.
\label{eq:IGBM}
\dby Y_t & = a(b-y_t)\,\dby t + \sigma\m y_t\circ \dby W_t,\
where  are the mean reversion parameters,  is the volatility and  denotes a standard real-valued Brownian motion. The  means that this SDE is understood in the Stratonovich sense.
The SDE (\ref{eq:IGBM}) is known in the literature as Inhomogeneous Geometric Brownian Motion (or IGBM).
Using the control path  and setting , the log-ODE (\ref{eq:first_logode_def}) becomes
3pt]
z(0) & = Y_s.

A_{s,t} & := \int_s^t W_{s,r}\,\dby r - \frac{1}{2}hW_{s,t},\3pt]
L_{s,t}^{(2)} & := \int_s^t\int_s^r W_{s,v}\,\dby v\,\dby r - \frac{1}{2}h A_{s,t} - \frac{1}{6}h^2W_{s,t}.

\dby Y_t & = f(Y_t)\,\dby X_t,\label{eq:RDE}\\
Y_0 & = \xi,\nonumber

X_{s,t} & = \Big(1, X_{s,t}^{(1)}, X_{s,t}^{(2)}, \cdots, X_{s,t}^{(\floor{p})}\Big),\label{eq:roughpathincrements}\\
X_{s,t}^{(k)} & := \pi_k\big(X_{s,t}\big),\nonumber

d_p\Big(S^{\floor{p}}(x_n), X\Big) \rightarrow 0,
\label{eq:rpconvege}

d_p\big(Z^1, Z^2\big) := \max_{1\leq k\leq \floor{p}}\sup_{\D}\bigg(\sum_{t_i\in\D}\Big\|\pi_k\big(Z_{t_i, t_{i+1}}^1\big) - \pi_k\big(Z_{t_i, t_{i+1}}^2\big)\Big\|^\frac{p}{k}\bigg)^\frac{k}{p},
\label{eq:rpmetric}

\|a\otimes b\| \leq \|a\|\|b\|,\nonumber

\|f\|_{\mathrm{Lip}(\gamma)} := \max_{0 \leq k\leq \floor{\gamma}}\big\|D^k f\big\|_{\infty} \vee \big\|D^{\floor{\gamma}} f\big\|_{(\gamma - \floor{\gamma})-\text{H\"{o}l}}\,,
\label{eq:lipgamma}

\frac{\dby z^{s,t}}{\dby u} & = F\big(z^{s,t}\big),\label{eq:standardlogode}\\
z_0^{s,t} & = Y_s,\nonumber

F(z) := \sum_{k=1}^{\floor{\gamma}}f^{\circ k}(z)\pi_k\Big(\logsig_{s,t}^{\floor{\gamma}}(X)\Big).
\label{eq:logodevectfield}

\big\|Y_t - z_1^{s,t}\big\| \leq C_{p,\gamma}\|f\|_{\mathrm{Lip}(\gamma)}^\gamma\|X\|_{p\text{-var};[s,t]}^\gamma,
\label{eq:local_logodeestimate}

\|X\|_{p\text{-var};[s,t]} := \max_{1\leq k\leq \floor{p}}\sup_{\D}\bigg(\sum_{t_i\in\D}\big\|X_{t_i, t_{i+1}}^k\big\|^\frac{p}{k}\bigg)^\frac{k}{p},
\label{eq:rpnorm}

\dby Y_t & = f(Y_t)\,\dby X_t,\\
Y_0 & = \xi,

\|Y_t - Y_s\| \leq C_p\big(1+\|\xi\|\big)K\|X\|_{p\text{-var};[s,t]}\exp\Big(C_p K^p \|X\|_{p\text{-var};[s,t]}^p\Big),
\label{eq:linearRDEbound}

\dby Y_t & = f(Y_t)\,\dby X_t,\\
Y_0 & = \xi.\nonumber

\|z_u^{s,t}\| \leq \|Y_s\|\exp\bigg(\sum_{m=1}^{\floor{\gamma}}K^m \Big\|\pi_m\Big(\logsig_{s,t}^{\floor{\gamma}}(X)\Big)\Big\|\bigg),
\label{eq:linearODEbound}

z_u^{s,t} = \exp(uF)z_0^{s,t},\nonumber

\frac{\dby z^{t_k,t_{k+1}}}{\dby u} & := F\big(z^{t_k,t_{k+1}}\big),\label{eq:standardlogode2}\\
z_0^{t_k,t_{k+1}} & := Y_k^{\log},\nonumber

\big\|Y_{t_k} - Y_k^{\log}\big\| \leq C\sum_{i=0}^{k-1}\|X\|_{p\text{-var};[t_i,t_{i+1}]}^\gamma,
\label{eq:global_logodeestimate}

for .
\end{theorem}
\begin{remark}
The above error estimate also holds when the vector field  is linear (by Remark \ref{rmk:linear_rmk})).
\end{remark}

Since  is the truncation depth of the log-signatures used to construct each log-ODE vector field, we see that high convergence rates can be achieved through using more terms in each log-signature.
It is also unsurprising that the error estimate (\ref{eq:global_logodeestimate}) increases with the ``roughness'' of the control path.
So just as in our experiments, we see that the performance of the log-ODE method can be improved by choosing an appropriate step size and depth of log-signature.
 
\section{Experimental details} \label{apx:experiments}

\paragraph{Code} The code to reproduce the experiments is available at \url{https://github.com/jambo6/neuralRDEs}. 

\paragraph{Data splits} Each dataset was split into a training, validation, and testing dataset with relative sizes 70\%/15\%/15\%.

\paragraph{Hyperparameter selection} Hyperparameters were selected for the Neural CDE model by performing a grid search, with a step size chosen so that the length of the sequence was 500 steps. This was found to create a reasonable balance between training time and sequence length. We additionally performed a separate hyperparameter selection for the ODE-RNN model. The Neural RDE models then use the same hyperparameters as the Neural CDE model.

\paragraph{Normalisation} The training splits of each dataset were normalised to zero mean and unit variance. The statistics from the training set were then used to normalise the validation and testing datasets.

\paragraph{Architecture} We give a graphical description of the architecture used for updating the Neural CDE hidden state in figure \ref{fig:network_diagram}. The input is first run through a multilayer perceptron with  layers of size , with with  being hyperparameters. ReLU nonlinearities are used at each layer except the final one, where we instead use a tanh nonlinearity. The goal of this is to help prevent term blow-up over the long sequences.

Note that this is a small inconsistency between this work and the original model proposed in \citet{kidger2020neural}. Here, we applied the tanh function as the final hidden layer nonlinearity, whilst in the original paper the tanh nonlinearity is applied after the final linear map. Both methods are used to constrain the rate of change of the hidden state; we do not know of a reason to prefer one over the other.

Note that the final linear layer in the multilayer perceptron is reshaped to produce a matrix-valued output, of shape . (As  is matrix-valued.) A matrix-vector multiplication with the log-signature then produces the vector field for the ODE solver.


\begin{figure*}
    \centering
    \resizebox{\textwidth}{!}{

    \begin{tikzpicture}
        \node[input, rotate=90, minimum width=2.8cm] (input) at (0, 0) {Input, };
        \node[hidden, rotate=90, minimum width=5cm] (hidden1) at (2,0) {Hidden layer 1};
        \node[hidden, rotate=90, minimum width=5cm] (hidden_end) at (6,0) {Hidden layer n};
        \node[hidden_square, minimum width=4cm] (hidden_square) at (10,0) {};
        \node[logsig, rotate=90, minimum width=3.5cm] (logsig) at (12.6, 0) {};
        \node[input, rotate=90, minimum width=2.8cm] (output) at (15.5, 0) {Output, };
        
\node[below, rotate=90] at (input.north east) [xshift=-2.4ex, yshift=0.4ex] {\footnotesize };
        \node[below, rotate=90] at (hidden1.north east) [xshift=-2.4ex, yshift=0.4ex] {\footnotesize };
        \node[below, rotate=90] at (hidden_end.north east) [xshift=-2.4ex, yshift=0.4ex] {\footnotesize };
        \node[below, rotate=0] at (hidden_square.north east) [xshift=-2.4ex, yshift=0.4ex] {\footnotesize };
        \node[below, rotate=90] at (logsig.north east) [xshift=-2.4ex, yshift=0.4ex] {\footnotesize };
        \node[below, rotate=90] at (output.north east) [xshift=-2.4ex, yshift=0.4ex] {\footnotesize };

\node (dots) at (4, 0) {\ldots};
        
\draw[middlearrow={>}] (12.6, -3) -- (logsig) node[midway, above, rotate=90] {Logsig} node[midway, below, rotate=90] {factor}; 
        
\draw[middlearrow={>}] (input) -- (hidden1) node[midway, above] {ReLU};
        \draw[middlearrow={>}] (hidden1) -- (dots) node[midway, above] {ReLU};
        \draw[middlearrow={>}] (dots) -- (hidden_end) node[midway, above] {Tanh};
        \draw[middlearrow={>}] (hidden_end) -- (hidden_square) node[midway, above] {Linear} node[midway, below] {+ reshape};
        \draw[middlearrow={>}] (logsig) -- (output) node[midway, above] {ODE Solve};
    
\draw[decoration={brace,raise=5pt},decorate] (8, 1.8) -- node[above=6pt] {Matrix multiplication} (13, 1.8);
        \draw[decoration={brace,raise=5pt},decorate] (2, 2.5) -- node[above=6pt] {n layers} (6, 2.5);
        \draw[decoration={brace,mirror,raise=5pt},decorate] (0.5, -2.5) -- node[below=6pt] {} (8, -2.5);

    \end{tikzpicture}
}     \caption{Overview of the hidden state update network structure. We give the dimensions at each layer in the top right hand corner of each box.} 
    \label{fig:network_diagram}
\end{figure*}


\paragraph{ODE Solver} All problems used the `rk4' solver as implemented by \texttt{torchdiffeq} \citep{torchdiffeq} version 0.0.1.

\paragraph{Computing infrastructure} All EigenWorms experiments were run on a computer equipped with three GeForce RTX 2080 Ti's. All BIDMC experiments were run on a computed with two GeForce RTX 2080 Ti's and two Quadro GP100's.

\paragraph{Optimiser} All experiments used the Adam optimiser. The learning rate was initialised at  divided by batch size. The batch size used was 1024 for EigenWorms and 512 for the BIDMC problems. If the validation loss failed to decrease after 15 epochs the learning rate was reduced by a factor of 10. If the validation loss did not decrease after 60 epochs, training was terminated and the model was rolled back to the point at which it achieved the lowest loss on the validation set. 

\paragraph{Hyperparameter selection} Hyperparameters were selected to optimise the score of the NCDE model on the validation set. For each dataset the search was performed with a step size that meant the total number of hidden state updates was equal to 500, as this represented a good balance between length and speed that allowed us to complete the search in a reasonable time-frame. In particular, this was short enough that we could train using the non-adjoint training method which helped to speed this section up. The hyperparameters that were considered were:
\begin{itemize}
    \item Hidden dimension: [16, 32, 64] - The dimension of the hidden state .
    \item Number of layers: [2, 3, 4] - The number of hidden state layers.
    \item Hidden hidden multiplier: [1, 2, 3] - Multiplication factor for the hidden hidden state, this being the `Hidden layer ' in figure \ref{fig:network_diagram}. The dimension of each of these `hidden hidden' layers with be this value multiplied by `Hidden dimension'.
\end{itemize}
We ran each of these 27 total combinations for every dataset and the parameters that corresponded were used as the parameters when training over the full depth and step grid. The full results from the hyperparameter search are listed in tables (\ref{tab:eigenworms_hyper}, \ref{tab:bidmc_hyper}) with bolded values to show which values were eventually selected. 


\begin{table*}[t]
    \scriptsize
    \begin{center}
        \begin{tabular}{ccccc}
        \toprule
        \textbf{Validation accuracy} & \textbf{Hidden dim} & \textbf{Num layers} & \textbf{Hidden hidden multiplier} & \textbf{Total params} \\
        \midrule
        33.3 &         16 &                        2 &          3 &        5509 \\
        43.6 &         16 &                        2 &          2 &        5509 \\
        56.4 &         16 &                        2 &          1 &        4453 \\
        64.1 &         16 &                        3 &          2 &        8869 \\
        38.5 &         16 &                        3 &          3 &        8869 \\
        51.3 &         16 &                        3 &          1 &        6517 \\
        82.1 &         16 &                        4 &          2 &       12741 \\
        35.9 &         16 &                        4 &          3 &       12741 \\
        53.8 &         16 &                        4 &          1 &        8581 \\
        35.9 &         32 &                        2 &          3 &       21253 \\
        74.4 &         32 &                        2 &          2 &       21253 \\
        43.6 &         32 &                        2 &          1 &       17093 \\
        53.8 &         32 &                        3 &          3 &       34629 \\
        \textbf{87.2} &         \textbf{32} & \textbf{3} & \textbf{2} & \textbf{34629} \\
        64.1 &         32 &                        3 &          1 &       25317 \\
        35.9 &         32 &                        4 &          3 &       50053 \\
        71.8 &         32 &                        4 &          1 &       33541 \\
        79.5 &         32 &                        4 &          2 &       50053 \\
        41.0 &         64 &                        2 &          3 &       83461 \\
        64.1 &         64 &                        2 &          2 &       83461 \\
        48.7 &         64 &                        3 &          3 &      136837 \\
        59.0 &         64 &                        3 &          2 &      136837 \\
        51.3 &         64 &                        2 &          1 &       66949 \\
        56.4 &         64 &                        4 &          2 &      198405 \\
        64.1 &         64 &                        4 &          3 &      198405 \\
        64.1 &         64 &                        3 &          1 &       99781 \\
        51.3 &         64 &                        4 &          1 &      132613 \\
        \bottomrule
        \end{tabular}
    \end{center}
    \caption{Hyperparamter selection results for the EigenWorms dataset. The blue values denote the selected hyperparameters.}
    \label{tab:eigenworms_hyper}
\end{table*}  
\begin{table*}[t]
    \scriptsize
    \begin{center}
        \begin{tabular}{ccc}
        \toprule
        \textbf{Validation accuracy} & \textbf{Hidden dim} & \textbf{Total params} \\
        \midrule
        61.5 &               32 &       11299 \\
        53.8 &               64 &       24611 \\
        \textbf{64.1} &              \textbf{128} &       \textbf{57379} \\
        59.0 &              192 &       98339 \\
        61.5 &              256 &      147491 \\
        59.0 &              320 &      204835 \\
        64.1 &              388 &      274739 \\
        \bottomrule
        \end{tabular}
    \end{center}
    \caption{Hyperparamter selection results for the ODE-RNN model on the EigenWorms dataset}
    \label{tab:eigenworms_hyper_odernn}
\end{table*}

 
\begin{table*}[t]
    \scriptsize
    \begin{center}
        \begin{tabular}{ccccccc}
        \toprule
        \multicolumn{3}{c}{\textbf{Validation loss}} & \multirow{2}{*}{\textbf{Hidden dim}} & \multirow{2}{*}{\textbf{Num layers}} & \multirow{2}{*}{\textbf{Hidden hidden multiplier}} & \multirow{2}{*}{\textbf{Total params}} \\
        \cmidrule(lr){1-3}
        RR & HR & SpO2 & & & & \\
        \midrule
        1.72 &      6.10 &      2.07 &         16 &                        2 &          1 &        2209 \\
        1.57 &      5.58 &      1.97 &         16 &                        2 &          2 &        3265 \\
        1.55 &      6.10 &      1.33 &         16 &                        2 &          3 &        3265 \\
        1.80 &      5.16 &      2.05 &         16 &                        3 &          1 &        3249 \\
        1.61 &      5.22 &      1.62 &         16 &                        3 &          2 &        5601 \\
        1.56 &      3.34 &      1.18 &         16 &                        3 &          3 &        5601 \\
        1.57 &      3.86 &      1.97 &         16 &                        4 &          1 &        4289 \\
        1.45 &      3.54 &      1.25 &         16 &                        4 &          2 &        8449 \\
        1.54 &      3.93 &      1.09 &         16 &                        4 &          3 &        8449 \\
        1.56 &      6.81 &      1.87 &         32 &                        2 &          1 &        8513 \\
        1.42 &      3.11 &      1.43 &         32 &                        2 &          2 &       12673 \\
        1.54 &      3.60 &      1.11 &         32 &                        2 &          3 &       12673 \\
        1.54 &      3.52 &      1.57 &         32 &                        3 &          1 &       12641 \\
        1.39 &      2.96 &      1.03 &         32 &                        3 &          2 &       21953 \\
        1.47 &      2.95 &      1.05 &         32 &                        3 &          3 &       21953 \\
        1.55 &      3.00 &      2.00 &         32 &                        4 &          1 &       16769 \\
        1.38 &      3.20 &      1.07 &         32 &                        4 &          2 &       33281 \\
        1.43 &      2.58 &      1.01 &         32 &                        4 &          3 &       33281 \\
        1.51 &      3.21 &      1.10 &         64 &                        2 &          1 &       33409 \\
        1.43 &     \textbf{2.22} & 1.00 &         \textbf{64} &                        \textbf{2} &          \textbf{2} &       \textbf{49921} \\
        1.51 &      3.34 &      0.94 &         64 &                        2 &          3 &       49921 \\
        1.55 &      3.24 &      2.09 &         64 &                        3 &          1 &       49857 \\
        1.32 &      2.53 &      0.88 &         64 &                        3 &          2 &       86913 \\
        \textbf{1.25} &      2.57 &      \textbf{0.73} &         \textbf{64} &                        \textbf{3} &         \textbf{3} &       \textbf{86913} \\
        1.43 &      5.78 &      1.43 &         64 &                        4 &          1 &       66305 \\
        1.28 &      2.26 &      0.93 &         64 &                        4 &          2 &      132097 \\
        1.32 &      2.46 &      1.15 &         64 &                        4 &          3 &      132097 \\
        \bottomrule
        \end{tabular}
    \end{center}
    \caption{Hyperparameter selection results for each problem of the BIDMC dataset. The bold values denote the selected hyperparameters for each vitals sign problem. Note that RR and SpO2 had the same parameters selected, hence why only two lines are given in bold.}
    \label{tab:bidmc_hyper}
\end{table*} 
\begin{table*}[t]
    \scriptsize
    \begin{center}
        \begin{tabular}{ccccc}
        \toprule
        \multicolumn{3}{c}{\textbf{Validation loss}} & \multirow{2}{*}{\textbf{Hidden dim}} & \multirow{2}{*}{\textbf{Total params}} \\
        \cmidrule(lr){1-3}
        RR & HR & SpO2 & & \\
        \midrule
        3.00 &     \textbf{12.82} &     \textbf{3.37} &               32 &        3871 \\
        3.00 &     12.82 &      3.37 &               64 &        9759 \\
        2.82 &     12.82 &      3.37 &              128 &       27679 \\
        \textbf{2.49} &     12.82 &      3.37 &              192 &       53791 \\
        2.52 &     12.82 &      3.37 &              256 &       88095 \\
        2.50 &     12.82 &      3.37 &              320 &      130591 \\
        2.83 &     12.82 &      3.37 &              388 &      184719 \\
        \bottomrule
        \end{tabular}
    \end{center}
    \caption{Hyperparameter selection results for the folded ODE-RNN model on the BIDMC problem. Bold values indicate selected hyperparamter values. The ODE-RNN model failed to train effectively for the HR and SpO2 problems which is why the validation losses are the same (to 2dp).}
    \label{tab:bidmc_hyper_odernn}
\end{table*}  
\section{Experimental Results} \label{apx:results}
Here we include the full breakdown of all experimental results. Tables \ref{tab:eigenworms_all} and \ref{tab:bidmc_all} include all results from the EigenWorms and BIDMC datasets respectively.

\begin{table*}[t]
    \small
    \begin{center}
        \begin{tabular}{ccccc}
        \toprule
        \textbf{Model} & \textbf{Step} & \textbf{Test Accuracy} & \textbf{Time (Hrs)} & \textbf{Memory (Mb)} \\
        \midrule
          & 1    &  Memory Error & Memory Error & Memory Error \\
          & 2    &   36.8  1.5 &           1.6 &        7170.1 \\
          & 4    &   35.0  1.5 &           0.8 &        3629.3 \\
          & 6    &   36.8  1.5 &           0.5 &        2448.6 \\
          & 8    &   36.8  1.5 &           0.4 &        1858.8 \\
          & 16   &   32.5  3.0 &           0.2 &         973.5 \\
        ODE-RNN  & 32   &   32.5  1.5 &           0.1 &         532.2 \\
        (folded)  & 64   &   41.0  4.4 &           0.1 &         311.2 \\
          & 128  &   47.9  5.3 &           0.0 &         200.8 \\
          & 256  &   46.2  0.0 &           0.0 &         147.0 \\
          & 512  &  47.9  10.4 &           0.0 &         124.5 \\
          & 1024 &   44.4  7.4 &           0.0 &         122.4 \\
          & 2048 &   48.7  6.8 &           0.0 &         137.2 \\
        
        \midrule
          & 1    &  62.4  12.1 &          22.0 &         176.5 \\
          & 2    &   69.2  4.4 &          14.6 &          90.6 \\
          & 4    &  66.7  11.8 &           5.5 &          46.6 \\
          & 6    &  65.8  12.9 &           2.6 &          31.5 \\
          & 8    &  64.1  13.3 &           3.1 &          24.3 \\
          & 16   &  64.1  16.8 &           1.5 &          13.4 \\
        \multirow{2}{*}{NCDE}  & 32   &  64.1  14.3 &           0.5 &           8.0 \\
          & 64   &   56.4  6.8 &           0.4 &           5.2 \\
          & 128  &   48.7  2.6 &           0.1 &           3.9 \\
          & 256  &   42.7  3.0 &           0.1 &           3.2 \\
          & 512  &   44.4  5.3 &           0.0 &           2.9 \\
          & 1024 &  41.9  14.6 &           0.0 &           2.7 \\
          & 2048 &   38.5  5.1 &           0.0 &           2.6 \\
          
        \midrule
          & 2    &  \textbf{76.1  13.2} &           9.8 &         354.3 \\
          & 4    &   \textbf{83.8  3.0} &           2.4 &         180.0 \\
          & 6    &   \textbf{76.9  6.8} &           2.0 &          82.2 \\
          & 8    &   \textbf{77.8  5.9} &           2.1 &          94.2 \\
          & 16   &   \textbf{78.6  3.9} &           1.3 &          50.2 \\
        NRDE & 32   &  67.5  12.1 &           0.7 &          28.1 \\
          & 64   &   73.5  7.8 &           0.4 &          17.2 \\
          & 128  &   \textbf{76.1  5.9} &           0.2 &           7.8 \\
          & 256  &  \textbf{72.6  12.1} &           0.1 &           8.9 \\
          & 512  &  \textbf{69.2  11.8} &           0.0 &           7.6 \\
          & 1024 &   \textbf{65.0  7.4} &           0.0 &           6.9 \\
          & 2048 &   \textbf{67.5  3.9} &           0.0 &           6.5 \\
          
        \hdashline\noalign{\vskip 0.5ex}
          & 2    &   66.7  4.4 &           7.4 &        1766.2 \\
          & 4    &   76.9  9.2 &           2.8 &         856.8 \\
          & 6    &   70.9  1.5 &           1.4 &         606.1 \\
          & 8    &   70.1  6.5 &           1.3 &         460.7 \\
          & 16   &   73.5  3.0 &           1.4 &         243.7 \\
        NRDE & 32   &   \textbf{75.2  3.0} &           0.6 &         134.7 \\
          & 64   &  \textbf{74.4  11.8} &           0.3 &          81.0 \\
          & 128  &   68.4  8.2 &           0.1 &          53.3 \\
          & 256  &   60.7  8.2 &           0.1 &          40.2 \\
          & 512  &  62.4  10.4 &           0.0 &          33.1 \\
          & 1024 &   59.8  3.9 &           0.0 &          29.6 \\
          & 2048 &   61.5  4.4 &           0.0 &          27.7 \\
        \bottomrule
        \end{tabular}
    \end{center}
    \caption{Mean and standard deviation of test set accuracy (in \%) over three repeats, as well as memory usage and training time, on the EigenWorms dataset for depths 1--3 and a small selection of step sizes. The bold values denote that the model was the top performer for that step size.}
    \label{tab:eigenworms_all}
\end{table*} 
\begin{table*}[t]
    \small
    \begin{center}
        \begin{tabular}{ccccccccc}
        \toprule
        \multirow{2}{*}{\textbf{Depth}} & \multirow{2}{*}{\textbf{Step}} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{\textbf{Time (H)}} & \multirow{2}{*}{\textbf{Memory (Mb)}} \\
        \cmidrule(lr){3-5} \cmidrule(lr){6-8}
        & & RR & HR & SpO & RR & HR & SpO & \\
        \midrule
         & 1    &    Error &  13.06  0.0 & Error &           Error &          10.4 &           Error &             3654.0 \\
          & 2    &    Error &  13.06  0.0 &  Error &           Error &           5.5 &           Error &             1840.4 \\
          & 4    &  2.76  0.14 &  13.06  0.0 &    3.3  0.0 &           3.0 &           2.7 &           2.1 &             1809.0 \\
          & 8    &  2.47  0.35 &  13.06  0.0 &    3.3  0.0 &           1.5 &           1.2 &           0.9 &              917.2 \\
          & 16   &  2.21  0.75 &  13.06  0.0 &    3.3  0.0 &           2.2 &           0.7 &           0.4 &              471.9 \\
        ODE-RNN   & 32   &  1.82  0.64 &  13.06  0.0 &    3.3  0.0 &           0.7 &           0.3 &           0.2 &              249.4 \\
        (folded)  & 64   &   1.6  0.22 &  13.06  0.0 &    3.3  0.0 &           0.5 &           0.1 &           0.1 &              137.0 \\
          & 128  &  1.62  0.07 &  13.06  0.0 &    3.3  0.0 &           0.2 &           0.1 &           0.1 &               81.9 \\
          & 256  &  1.57  0.04 &  7.04  1.04 &  \textbf{1.43  0.11} &           0.1 &           0.1 &           0.1 &               53.8 \\
          & 512  &  1.66  0.06 &   6.75  0.9 &  1.98  0.31 &           0.0 &           0.1 &           0.1 &               40.4 \\
          & 1024 &  \textbf{1.69  0.02} &   8.4  0.28 &  2.05  0.14 &           0.0 &           0.0 &           0.0 &               36.2 \\
          & 2048 &  1.75  0.03 &   9.2  0.27 &  \textbf{2.24  0.11} &           0.0 &           0.0 &           0.0 &               39.6 \\
        \midrule
          & 1    &  2.79  0.04 &   9.82  0.34 &  2.83  0.27 &          23.8 &          22.1 &          28.1 &               56.5 \\
          & 2    &  2.87  0.03 &  11.69  0.38 &   \textbf{3.36  0.2} &          19.3 &           9.6 &           8.8 &               32.6 \\
          & 4    &  \textbf{2.92  0.08} &  11.15  0.49 &  3.69  0.06 &           5.3 &           5.7 &           3.2 &               20.2 \\
          & 8    &   2.8  0.06 &  10.72  0.24 &  3.43  0.17 &           3.0 &           2.6 &           4.8 &               14.3 \\
          & 16   &  2.22  0.07 &   7.98  0.61 &   2.9  0.11 &           1.7 &           1.4 &           1.8 &               11.8 \\
        \multirow{2}{*}{NCDE}  & 32   &  2.53  0.23 &  12.23  0.43 &  2.68  0.12 &           1.9 &           0.9 &           2.2 &                9.8 \\
          & 64   &  2.63  0.11 &  12.02  0.09 &  2.88  0.06 &           0.2 &           0.3 &           0.4 &                9.1 \\
          & 128  &  2.64  0.18 &  11.98  0.37 &  2.86  0.04 &           0.2 &           0.2 &           0.3 &                8.7 \\
          & 256  &  2.53  0.04 &   12.29  0.1 &   3.08  0.1 &           0.1 &           0.1 &           0.1 &                8.3 \\
          & 512  &  2.53  0.03 &  12.22  0.11 &  2.98  0.04 &           0.1 &           0.0 &           0.1 &                8.4 \\
          & 1024 &  2.67  0.12 &  11.55  0.03 &  2.91  0.12 &           0.1 &           0.1 &           0.1 &                8.4 \\
          & 2048 &  2.48  0.03 &   12.03  0.2 &  3.25  0.01 &           0.0 &           0.1 &           0.0 &                8.2 \\
        \midrule
          & 2    &   2.91  0.1 &  11.11  0.23 &  3.89  0.44 &          12.7 &           9.3 &           8.2 &               58.3 \\
          & 4    &  \textbf{2.92  0.04} &   11.14  0.2 &  4.23  0.57 &          18.1 &           5.0 &           3.4 &               34.0 \\
          & 8    &  2.63  0.12 &   8.63  0.24 &  2.88  0.15 &           2.1 &           3.4 &           3.3 &               21.8 \\
          & 16   &   1.8  0.07 &   5.73  0.45 &  1.98  0.21 &           2.2 &           1.4 &           2.5 &               16.0 \\
          & 32   &   1.9  0.02 &     7.9  1.0 &   1.69  0.2 &           1.2 &           1.1 &           2.0 &               13.1 \\
        NRDE  & 64   &  1.89  0.04 &   5.54  0.45 &  2.04  0.07 &           0.3 &           0.3 &           1.7 &               11.6 \\
          & 128  &  1.86  0.03 &   6.77  0.42 &  1.95  0.18 &           0.3 &           0.4 &           0.7 &               10.9 \\
          & 256  &  1.86  0.09 &   5.64  0.19 &   2.1  0.19 &           0.1 &           0.1 &           0.5 &               10.5 \\
          & 512  &  1.81  0.02 &   5.05  0.23 &  2.17  0.18 &           0.1 &           0.2 &           0.4 &               10.3 \\
          & 1024 &  1.93  0.11 &    6.0  0.19 &  2.41  0.07 &           0.1 &           0.1 &           0.2 &               10.2 \\
          & 2048 &  \textbf{2.03  0.03} &    \textbf{7.7  1.46} &  2.55  0.03 &           0.1 &           0.1 &           0.1 &               10.2 \\
        \hdashline\noalign{\vskip 0.5ex}
          & 2    &  \textbf{2.82  0.08} &  \textbf{11.01  0.28} &   4.1  0.72 &           8.8 &           9.4 &           6.9 &              125.2 \\
          & 4    &  2.97  0.23 &  \textbf{10.13  0.62} &  \textbf{3.56  0.44} &           3.2 &           4.1 &           2.6 &               71.6 \\
          & 8    &  \textbf{2.42  0.19} &    \textbf{7.67  0.4} &  \textbf{2.55  0.13} &           2.9 &           3.2 &           3.1 &               43.3 \\
          & 16   &  \textbf{1.74  0.05} &   \textbf{4.11  0.61} &   \textbf{1.4  0.06} &           1.4 &           1.4 &           6.5 &               29.1 \\
          & 32   & \textbf{1.67  0.01} &     \textbf{4.5  0.7} &  \textbf{1.61  0.05} &           1.3 &           1.8 &           7.3 &               20.5 \\
        NRDE  & 64   &  \textbf{1.53  0.08} &   \textbf{3.05  0.36} &  \textbf{1.48  0.14} &           0.4 &           1.9 &           3.3 &               17.9 \\
          & 128  &  \textbf{1.51  0.08} &   \textbf{2.97  0.45} &  \textbf{1.37  0.22} &           0.5 &           1.7 &           1.7 &               17.3 \\
          & 256  &  \textbf{1.51  0.06} &    \textbf{3.4  0.74} &  1.47  0.07 &           0.3 &           0.7 &           0.6 &               16.6 \\
          & 512  &  \textbf{1.49  0.08} &   \textbf{3.46  0.13} &  \textbf{1.29  0.15} &           0.3 &           0.4 &           0.4 &               15.4 \\
          & 1024 &  1.83  0.33 &    \textbf{5.58  2.5} &  \textbf{1.72  0.31} &           0.2 &           0.1 &           0.1 &               15.7 \\
          & 2048 &  2.31  0.27 &   9.77  1.53 &  2.45  0.18 &           0.1 &           0.1 &           0.1 &               15.6 \\
        \bottomrule
        \end{tabular}
    \end{center}
    \caption{Mean and standard deviation of the  losses on the test set for each of the vitals signs prediction tasks (RR, HR, SpO) on the BIDMC dataset, across three repeats. Only mean times are shown for space. The memory usage is given as the mean over all three of the tasks as it was approximately the same for any task for a given depth and step. Error denotes that the model could not be run within GPU memory. The bold values denote the algorithm with the lowest test set loss for a fixed step size for each task.}
    \label{tab:bidmc_all}
\end{table*} 












  








\end{document}