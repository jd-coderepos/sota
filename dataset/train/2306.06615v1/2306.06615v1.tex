

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{ICML/icml2023}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{color}
\newcommand{\red}[1]{\textcolor{red}{#1}}\newcommand{\blue}[1]{\textcolor{blue}{#1}}\newcommand{\green}[1]{\textcolor{green}{#1}}\definecolor{brown}{RGB}{139,64,0}
\definecolor{pink}{RGB}{255,170,182}
\definecolor{purple}{RGB}{160,32,240}
\newcommand{\tyler}[1]{\textcolor{brown}{#1}}

\newcommand{\yq}[1]{{\color{cyan}#1}} \newcommand{\jt}[1]{{\color{pink}(#1)}} \newcommand{\wq}[1]{{\color{blue}#1}} \newcommand{\jl}[1]{{\{\color{purple}#1\}}} \newcommand{\wei}[1]{{\{\color{orange}#1\}}} 

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Submission and Formatting Instructions for ICML 2023}

\begin{document}

\twocolumn[
\icmltitle{Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jiatong Li}{comp,equal}
\icmlauthor{Yunqing Liu}{comp,equal}
\icmlauthor{Wenqi Fan}{comp}
\icmlauthor{Xiao-Yong Wei}{comp,scu}
\icmlauthor{Hui Liu}{msu}
\icmlauthor{Jiliang Tang}{msu}
\icmlauthor{Qing Li}{comp}
\end{icmlauthorlist}

\icmlaffiliation{comp}{Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR China}
\icmlaffiliation{scu}{Dept. of computer science, Sichuan University, China}
\icmlaffiliation{msu}{Michigan State University, Michigan, USA}

\icmlcorrespondingauthor{Wenqi Fan}{wenqifan03@gmail.com}
\icmlcorrespondingauthor{Qing Li}{qing-prof.li@polyu.edu.hk}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}

Molecule discovery plays a crucial role in various scientific fields, advancing the design of tailored materials and drugs. Traditional methods for molecule discovery follow a trial-and-error process, which are both time-consuming and costly, while computational approaches such as artificial intelligence (AI) have emerged as revolutionary tools to expedite various tasks, like molecule-caption translation. Despite the importance of molecule-caption translation for molecule discovery, most of the existing methods heavily rely on domain experts, require excessive computational cost, and suffer from poor performance. On the other hand, Large Language Models (LLMs), like ChatGPT, have shown remarkable performance in various cross-modal tasks due to their great powerful capabilities in natural language understanding, generalization, and reasoning, which provides unprecedented opportunities to advance molecule discovery. To address the above limitations, in this work, we propose a novel LLMs-based framework (\textbf{MolReGPT}) for  molecule-caption translation, where a  retrieval-based prompt paradigm is introduced to empower molecule discovery with LLMs like ChatGPT without fine-tuning. More specifically, MolReGPT leverages the principle of molecular similarity to retrieve similar molecules and their text descriptions from a local database to ground the generation of LLMs through in-context few-shot molecule learning. We evaluate the effectiveness of MolReGPT via molecule-caption translation, which includes molecule understanding and text-based molecule generation. Experimental results show that MolReGPT outperforms fine-tuned models like MolT5-base without any additional training. To the best of our knowledge, MolReGPT is the first work to leverage LLMs in molecule-caption translation for advancing molecule discovery. Our implementation is available at: \url{https://github.com/phenixace/MolReGPT} 
\end{abstract}
 
  \section{Introduction}

Molecules are the fundamental building blocks of matter, comprising the intricate fabric of the world around us.
As the foundation of all chemical compounds, molecules are composed of two or more atoms that are chemically bonded together, and they retain the unique chemical properties dictated by their specific structures \cite{xu2023plasma}.
With a comprehensive understanding of molecules, scientists can effectively design materials, drugs, and products with tailored characteristics and functionalities, impacting a variety of crucial fields such as chemistry \cite{wang2023advances,cuzzucoli2023predictive,weng2021late}, pharmacology \cite{patani1996bioisosterism,anderson2003process,ding2019selective}, material science \cite{curtarolo2013high,higuchi2023material}, and environmental science \cite{ali2023new,lv2023autonomous}.
One notable example is in the pharmaceutical industry during the COVID-19 pandemic, where the discovery of new molecules has the potential to revolutionize not only the development of groundbreaking treatments, therapies \cite{gupta2023protein}, and vaccines against viruses but also a wide range of other diseases in the coming decade \cite{osamor2023covid}.

However, traditional molecule discovery lies in the long, expensive, and failure-prone process that requires navigating a complex landscape of molecule structures and biological interactions, with limitations in scalability, precision, and data management \cite{hajduk2007decade}.
To overcome these challenges, computational technologies such as artificial intelligence (AI) have emerged as powerful tools to expedite the discovery of new molecules \cite{urbina2022commoditization}. 
Specifically, molecules can be represented as simplified molecular-input line-entry system (\textbf{SMILES}) strings \cite{weininger1988smiles,cao2022identifying}. As shown in Figure \ref{fig:bot} (a), the structure of Phenol can be represented as a SMILES sequence, which is made of a Benzene ring and a Hydroxy. 
Such SMILES representations can be effectively processed by deep sequence models like Recurrent Neural Networks \cite{arus2019randomized,grisoni2020bidirectional} and Transformers \cite{honda2019smiles,yoshikai2023difficulty}.
These AI-powered models enable researchers to understand molecular properties and functionalities and create promising compounds in a more efficient and cost-effective manner. 
For example, in order to generate new molecules and better understand them, a novel task that translates between molecules and natural language has been proposed by using language models like Text2Mol ~\cite{edwards2021text2mol} and MolT5 ~\cite{edwards-etal-2022-translation}. It consists of two sub-tasks: molecule captioning (Mol2Cap) and text-based molecule generation (Cap2Mol). 
More specifically, as shown in Figure~\ref{fig:bot} (b-c), the goal of \emph{molecule captioning} is to generate a text caption to describe a SMILES string of the molecule for providing humans with a better understanding of molecule, while \emph{text-based molecule generation} aims to generate the corresponding molecule (i.e., SMILES string) based on a given natural language description (e.g., properties and functional groups).
Despite the impressive progress that has been made in the molecule-caption translation task, the majority of existing advanced approaches suffer from several limitations \cite{edwards2021text2mol, edwards-etal-2022-translation, su2022molecular}.
First, the design of such model architecture in molecule-caption translation heavily relies on domain experts, which can significantly limit the development and deployment of AI-powered molecule discovery. Second, most existing methods follow the ``pre-train\&fine-tuning'' paradigm for molecule-caption translation, which requires excessive computational costs.
Third, existing approaches such as Text2Mol and MolT5 fall short in their inability to reason on complex tasks and generalize to unseen examples. Therefore, it is desired to design a novel paradigm for molecule-caption translation in molecule discovery.

\begin{figure}[t]
\centering
\begin{minipage}{3cm}
\vspace{1cm}
\subfigure[Molecule Representations.]{ 
\includegraphics[scale=0.34]{figures/molecule.pdf}
}
\subfigure[Molecule Captioning.]
{ 
\includegraphics[scale=0.34]{figures/mol2cap_intro.pdf}
}
\subfigure[Text-based Molecule Generation.]
{   
\includegraphics[scale=0.34]{figures/cap2mol_intro.pdf}
}
\end{minipage}
\subfigure[Empowering ChatGPT with molecule captioning and text-based molecule generation abilities.]{ 
\begin{minipage}{4cm} 
\centering 
\includegraphics[width=1\textwidth]{figures/bot.pdf}
\end{minipage}
}
\caption{An illustration on translation between molecule and natural language in molecule discovery. (a) A molecule can be denoted as a chemical formula, SMILES string, and 2D molecule graph.
(b) Molecule captioning aims to generate a text caption to describe a molecule for humans' better understanding. 
(c) Given a text description, text-based molecule generation is used to generate a corresponding molecule.
(d) Large language models (e.g., ChatGPT) can perform molecule captioning and text-based molecule generation with corresponding well-designed prompts.}
\label{fig:bot} 
\end{figure}

Recently, Large Language Models (\textbf{LLMs}), scaling up their weights to the billion level, have achieved tremendous success not only in the field of Natural Language Processing (NLP) but also in some cross-modal areas like computer vision \cite{zhu2023minigpt}, recommender systems \cite{bao2023tallrec}, and molecule discovery~\cite{edwards-etal-2022-translation}. 
For example, ChemGPT~\cite{frey2022neural}, a variant of GPT model with more than one billion parameters, is introduced to understand and generate small molecules in chemistry. 
Meanwhile, in addition to the impressive capabilities in  natural language understanding and generation,  LLMs also demonstrate their powerful generalization and reasoning capabilities \cite{rubin2022learning, min2022metaicl}, which can  generalize to other unseen tasks  by  specific task context (In-Context Learning, ICL) without being fine-tuned and largely reduce computational cost. 
Therefore, LLMs provide unprecedented potential to advance molecule discovery, specifically the task of molecule-caption translation.

Although building specific LLMs in molecule discovery has immense potential for advancing scientific research, we also face significant challenges. First, due to privacy and security concerns, many advanced large language models (e.g., ChatGPT and GPT4.0) are not publicly available, where LLMs' architectures and parameters are not released publicly for fine-tuning in downstream tasks.
Second, owing to their complex architectures and the extensive data required, training advanced LLMs requires significant computing resources, leading to high costs and substantial energy consumption. 
For instance, it has been reported that the cost of \emph{one single training session} for GPT-3 exceeds 1 million.
As a result, it is very challenging for us to re-design our own LLMs with pre-training and fine-tuning in specific downstream tasks. 
At last, it is crucial to design proper guidelines/prompts with high-quality few-shot examples to improve LLMs' generalization and reasoning capabilities for molecule discovery.  


To address such challenges, as the early exploration attempt to take advantage of the powerful capabilities of LLMs in the molecule discovery field, in this work, we propose a novel solution to teach LLMs with prompts for translating between molecules and natural language, as illustrated in Figure~\ref{fig:bot} (d).
More specifically, inspired by the latest ChatGPT, a retrieval-based prompt paradigm through in-context learning (ICL) is developed to conduct two sub-tasks (i.e., molecule captioning and text-based molecule generation) without fine-tuning the LLMs, where n-shot examples are retrieved to augment the prompt instances via BM25-based caption ranking and Morgan Fingerprints-based molecule ranking. 
Experiments show that MolReGPT can achieve Text2Mol scores of 0.560 in Mol2Cap generation and 0.571 in Cap2Mol generation, which surpasses the \emph{fine-tuned} MolT5-base in both sub-tasks of molecule-caption translation. 
Notably, MolReGPT even outperforms MolT5 largely in text-based molecule generation, increasing the Text2Mol metric by 3\%. Note that all of these improvements from our proposed method are achieved without any fine-tuning steps. 

Our major contributions are summarized as follows:
\begin{itemize}
    \item We introduce a principle strategy based on LLMs to perform translation between molecules and natural language for molecule discovery. To the best of our knowledge, we are the first to investigate molecule-caption translation by employing LLMs.

    \item We develop a novel framework (MolReGPT) to empower LLMs like ChatGPT to perform molecule captioning and text-based molecule generation without being fine-tuned, where a retrieval-based prompt paradigm through in-context learning is developed to explicitly guide the generation process.
    
    \item Comprehensive experiments on a real-world dataset demonstrate the effectiveness of the proposed method on molecule captioning and text-based molecule generation tasks, surpassing even fine-tuned models such as T5-base and MolT5-base.
\end{itemize} 
 \section{Related Work}
\label{sec:relatedwork}
In this section, we briefly review related work about molecule-caption translation tasks in molecule discovery as well as the advanced LLMs techniques. 
 
\subsection{Molecule Discovery}


Molecule discovery plays a pivotal role across numerous scientific fields, driving advancements in the development of drug discovery and material design~\cite{du2022molgensurvey}.
In recent decades, AI-powered approaches have emerged as mainstream techniques to revolutionize the process of molecule discovery~\cite{hu2023deep,fan2023generative}.
For instance, SMILES-based Variational Autoencoders (VAEs) methods such as ChemVAE~\cite{gomez2018automatic}, SD-VAE~\cite{dai2018syntax}, and GrammarVAE~\cite{kusner2017grammar}, employ a VAE-based model that encodes and decodes SMILES strings, generating new molecules (strings) by decoding from a Gaussian prior.
In terms of molecular string representation, existing studies have explored advanced deep representation methods from other fields, including Convolutional Neural Network (CNN)~\cite{peng2019convolutional,le2019imotor}, Recurrent Neural Network (RNN)~\cite{grisoni2020bidirectional,amabilino2020guidelines}, and Transformer~\cite{bagal2021molgpt,wang2021multi}.

More recently, as a new task in molecule discovery, Text2Mol~\cite{edwards2021text2mol} is introduced to retrieve molecules using natural language descriptions as search queries, in which a paired dataset of molecules and their corresponding text descriptions are constructed, enabling the learning of a shared semantic embedding space for retrieval.
KV-PLM~\cite{zeng2022deep} develops a knowledgeable machine reading system pre-trained on a domain corpus, in which SMILES strings are inserted and link molecule structures with biomedical text.
What's more, a self-supervised learning framework MolT5~\cite{edwards-etal-2022-translation} is proposed to pre-train on a substantial volume of unlabeled language text and SMILES strings, enhancing the molecule-caption translation task, such as molecule captioning and text-based molecule generation.
MoMu~\cite{su2022molecular} bridges molecular graphs and natural language by pre-training molecular graphs and their semantically related text data through comparative learning. 



\subsection{Large Language Models} 

Large Language models (LLMs) have been a trending topic in recent years, with numerous studies exploring their capabilities and potential applications. 
One of the most well-known LLMs is the GPT family~\cite{radford2018improving,radford2019language,brown2020language,ouyang2022training}, which has played a pivotal role in advancing the field of generative language models. As a representative of the GPT family, ChatGPT is specifically fine-tuned for conversational purposes, which can generate impressively human-like responses~\cite{leiter2023chatgpt}. In addition, other LLMs, such as LaMDA~\cite{thoppilan2022lamda}, PaLM~\cite{chowdhery2022palm}, and Vicuna~\cite{chiang2023vicuna}, also show a decent performance. 

The power of LLMs is far beyond language generation but also lies in their ability to learn from context, namely the ability of in-context learning. Several works have explored the utilization of in-context learning from various tasks, such as KATE~\cite{liu2021makes} and AutoCoT~\cite{zhang2022automatic}. These works show that through in-context learning, LLMs can adapt to new tasks based on the context provided in the input, eliminating the need for explicit fine-tuning on specific tasks. 


In addition to NLP tasks, LLMs have also shown remarkable potential in various molecule discovery tasks, such as molecule understanding~\cite{bran2023chemcrow,white2023future}. For instance, ChemBERTa~\cite{chithrananda2020chemberta} leverages pre-training on an extensive corpus of chemical texts, enabling it to comprehend the structure and properties of chemical compounds. Another notable example is MoleculeSTM~\cite{liu2022multi}, which employs in-context learning in conjunction with LLMs. This approach facilitates a deeper understanding of the relationships between chemical structures and their corresponding textual descriptions. 
Furthermore, ChemGPT~\cite{frey2022neural} represents a variant of the GPT model specifically trained on chemical data. Through the application of in-context learning, ChemGPT is capable of generating novel chemical structures and accurately predicting their properties.
MolT5~\cite{edwards-etal-2022-translation} shows that LLMs can perform the cross-modal transition task between molecule and text (i.e. molecule captioning task and text-based molecule generation task), which is one of the most closely related attempts to ours. 
Note that MolT5 needs to pre-train and fine-tune LLMs for translating between molecules and natural language, leading to huge computational costs. In this paper, we propose a novel framework to empower LLMs like ChatGPT to perform molecule captioning and text-based molecule generation without being fine-tuned.  \section{MolReGPT}
\label{sec:methodlogy}

In this section, we aim to introduce our proposed method (MolReGPT) as a novel solution to empower molecule discovery for molecule-caption translation with LLMs like ChatGPT.
We will first introduce an overview of the proposed framework, and then detail each model component.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\textwidth]{figures/model_structure.pdf}
    \caption{This diagram shows the workflow of MolReGPT. MolReGPT consists of four main stages. 
    In stage 1, Molecule-Caption Retrieval is employed to find  best-matched examples from the local database. 
    Then in stage 2, Prompt Management helps construct the system prompt with the retrieved molecule-caption pairs. Following this, LLMs perform In-Context Few-Shot Molecule Learning based on the provided system prompt and user input prompt. 
    Finally, Generation Calibration is conducted to ensure desired output.}
    \label{fig:prompt}
\end{figure*}


\subsection{An Overview}

Due to the huge computation costs, training or fine-tuning LLMs on the domain-specific corpus from the molecule discovery field is often infeasible in practice.
To address such limitations, we investigate leveraging the great capabilities of LLMs without changing the LLMs, where we propose a novel framework (\textbf{MolReGPT}) to empower ChatGPT with the ability of molecule-caption translation for molecule discovery. 
More specifically, in order to improve the quality of guidelines/prompts, a retrieval-based prompt paradigm under in-context learning is introduced to teach ChatGPT to conduct two molecule-related tasks: \textit{molecule captioning} (Mol2Cap) and \textit{text-based molecule generation} (Cap2Mol).
The framework of MolReGPT is shown in Figure \ref{fig:prompt}, consisting of four main stages: Molecule-Caption Retrieval, Prompt Management, In-Context Few-Shot Molecule Learning, and Generation Calibration, following the workflow of pre-processing, querying, and post-processing. The first stage, Molecule-Caption Retrieval, is used to retrieve the  most similar examples (i.e., \emph{few-shot examples}) from human-annotated datasets (i.e., molecule-caption pairs database) for augmenting the prompt instances.
The second stage is Prompt Management, which is executed to construct the system prompt as evidence for successive in-context learning.
After that, both the system and the user input prompt are sent to query LLMs such as ChatGPT to perform In-Context Few-Shot Molecule Learning without fine-tuning LLMs for the molecule-caption translation task in the molecule discovery field. 
Valid responses are expected in the pre-defined JSON format, while there may be instances where the language models generate unexpected outputs. 
The last stage is Generation Calibration, which is deployed to calibrate the original responses for the validity of the outputs.



\subsection{Molecule-Caption Retrieval}

In order to teach LLMs to handle the molecule-caption translation task (i.e., Mol2Cap and Cap2Mol) without fine-tuning LLMs, we propose to perform in-context learning with few-shot examples to prompt LLMs.
In general,  examples are randomly selected from human-annotated datasets (i.e., molecule-caption pair database), providing a general task instruction to LLMs. 
However, such a naive solution often provides insufficient knowledge regarding the associations between natural language and molecules.
To mitigate this issue, we propose incorporating retrieval methods into the selection of examples to complement the lack of domain-specific knowledge of LLMs in molecule discovery, specifically through the stage of Molecule-Caption Retrieval.
These retrieval strategies are motivated by the similar property principle, in which molecules similar in structures tend to exhibit similar characteristics ~\cite{wang2016improving}.
Thus, similar captions containing the descriptions of molecule structures and properties are used to describe similar molecules.
Therefore, by retrieving the most similar molecules or captions, we can utilize the corresponding molecule-caption pairs as examples to prompt LLMs. 


However, the SMILES representation of molecules as a sequence structure can not reveal the actual 2-D graph topology of molecules. Hence, domain-specific methods are required for better molecular similarity calculation during the retrieval stage.
Specifically, given a SMILES string representation for \emph{molecule captioning} task, we introduce to use of Morgan Fingerprints (i.e., molecular structures representations)~\cite{butina1999unsupervised}, to calculate molecular similarity using Dice similarity for molecule retrieval.
In \emph{text-based molecule generation task} for caption retrieval, BM25, which is widely used in information retrieval~\cite{robertson2009probabilistic}, is proposed to compute similarity scores between captions of molecules, which mainly contain functional groups and properties of molecules. 
In both scenarios, top-n molecule-caption pair examples are retrieved to serve as examples in the system prompt.
Next, we will detail Morgan Fingerprints-based molecule retrieval and BM25-based caption retrieval.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/similarity.pdf}
    \caption{Illustrations of Morgan Fingerprints and Dice Similarity. 
    The two molecules will first be transformed into the Morgan Fingerprints. Then, Dice similarity will be calculated.
    The green colour corresponds to sub-structures that contribute positively to the similarity score between the molecules, 
    while the purple colour represents sub-structures that contribute negatively or have differences between the molecules. 
    }
    \label{fig:similarity}
\end{figure}

\subsubsection{Morgan Fingerprints-based Molecule Retrieval}
Molecular fingerprints are numerical representations of the chemical structures of molecules, which can be used for various computational objectives~\cite{butina1999unsupervised}, such as similarity searching, property prediction, virtual screening, and cluster analysis.
One of the most representative molecular fingerprints is the Morgan Fingerprints (Morgan FTS), which is also known as circular fingerprints or extended-connectivity fingerprints (ECFP). 

The key idea behind Morgan FTS is to capture the presence or absence of specific sub-structures or chemical fragments in a molecule. Morgan FTS follows a variant of the Morgan algorithm ~\cite{butina1999unsupervised}, which encodes the structural information of a molecule by representing its connectivity patterns in a circular manner. Morgan FTS is then generated by iteratively expanding a set of atoms from a central atom in the molecule, capturing the neighbouring atoms and their bond types at each expansion step. The process continues until a pre-defined radius is reached. The resulting fingerprint is a binary bit vector, where each bit represents the presence or absence of a particular substructure.

What's more, Morgan FTS has several advantages over other types of fingerprints, including their ability to handle molecules of varying sizes, resistance to small structural changes, and effectiveness in capturing structural similarities between molecules.
To extract the Morgan Fingerprints, the SMILES representations of the molecules are converted into rdkit objects using the rdkit library\footnote{https://www.rdkit.org/}. 
Subsequently, we apply Dice similarity ~\cite{dice1945measures}, also known as the Dice coefficient, to measure the similarity between the input molecule and the molecules in the local database. Mathematically, it can be expressed as:

where  and  are the Morgan Fingerprints of the two molecules.  and  represent the cardinality (i.e., number of sub-structures) of  and .  denotes the number of sub-structures that are common to both  and . Dice similarity ranges from 0 to 1, where the value of 0 indicates no overlap or similarity between the molecules, and the value of 1 represents complete overlap.
As shown in Figure \ref{fig:similarity}, Dice similarity can be calculated by comparing the Morgan fingerprints of the molecules. The similarity map shows the similarities and differences between the two molecules.
The Dice similarity is particularly useful when dealing with imbalanced datasets or focusing on the agreement between positive instances (i.e., sub-structures present in both sets) rather than the overall agreement. 

Compared to existing molecule embedding methods~\cite{coupry2022application}, Morgan FTS together with Dice similarity provides a distinctive advantage by explicitly indicating the similarities in detailed molecular structures, as these structures are usually directly stated in the molecule captions.



\subsubsection{BM25-based Caption Retrieval} 
BM25 is one of the most representative ranking approaches in information retrieval for calculating the relevance of the documents to the given query ~\cite{robertson2009probabilistic}. 
The idea is based on the term frequency-inverse document frequency (TF-IDF), which measures how often a term appears in a document (i.e., caption) and how rare it is in the corpus of documents (i.e., the local database) ~\cite{aizawa2003information}. In addition, BM25 considers the caption's length and the position of the query terms in the caption.

In the Cap2Mol task, we use the input caption as the query sentence, while the captions in the local database (i.e., the training set), are served as the corpus of documents, where each caption represents a document.
Mathematically, the formula of BM25 can be defined as follow:

where  is the caption corpus and  is the query caption.  is the number of query terms in the query caption,  is the -th query term,  is the inverse document frequency of ,  is the term frequency of  in ,  and  are tuning parameters,  is the length of , and  is the average caption length in the corpus.

In caption retrieval, BM25 is applied to calculate the similarity scores between captions so that the relevant molecule structures described by captions can be learnt through retrieved molecule-caption pairs.


\subsection{Prompt Management}
System prompts and user input prompts are two important parts to form the task context. User prompts are usually more complex and contain essential instructions for task solving and format formalization, where user prompts are defined to formalize the user inputs. To help LLMs understand the task and generate desired outputs, Prompt Management is proposed to design the system prompt templates, which are further completed with the retrieved examples. 
As shown in stage 2 of Figure \ref{fig:prompt}, the system prompts consist of four parts: Role Identification, Task Description, Retrieved Examples, and Output Instruction.

\textbf{Role Identification} aims to help LLMs identify the role of experts in the chemistry and molecule discovery domain.
By establishing this role, the LLMs are encouraged to generate responses that align with the expertise expected in the specific domain.

\textbf{Task Description} provides a comprehensive explanation of the task's content, ensuring that LLMs have a clear understanding of the specific task they need to address.
It also includes critical definitions to clarify terms or concepts that are specialized in the molecule-caption translation task.

The next component of the system prompt is designed to define the user input prompt and incorporate the \textbf{Retrieved Examples}, which serve as the evidence for the molecule-caption translation task, allowing LLMs to leverage the information contained within few-shot examples to generate better responses.

Finally, \textbf{Output Instruction} specifies the desired format for the response. Here, we restrict the output to a JSON format. The choice of JSON format enables a quick and efficient validation of the LLMs' response, ensuring that it adheres to the expected structure and facilitates further processing or analysis.

\subsection{In-Context Few-Shot Molecule Learning}

Since ChatGPT is treated as a \emph{black-box} system, it is impossible for us to fine-tune the model's parameters on task-specific datasets for translation between molecules and natural language captions. Besides, as the weights of LLMs continue to scale, it is infeasible to train and fine-tune these foundation models with huge computational resources.
To address the above limitations, recently, as an alternative to fine-tuning, in-context learning techniques provide great opportunities to teach ChatGPT to make predictions based on a few examples.
In this work, we introduce in-context few-shot molecule learning to perform the Mol2Cap task and Cap2Mol task without fine-tuning ChatGPT. 
This stage is to utilize both the system prompt and user input prompt to query the LLMs after Molecule-Caption Retrieval and Prompt Management.
In particular, the combination of the system prompt and user input prompt provides ChatGPT with a clear guideline (i.e., Mol2Cap and Cap2Mol prompts with a few examples) via in-context learning, The system prompt establishes the task framework of molecule-caption translation and molecule domain expertise, while the user prompt narrows the focus and directs the model's attention to the specific user input. 
As a result, ChatGPT can learn how to perform the molecule-caption translation from the given task context, without the necessity to modify its parameters.


The formula below describes the differences between fine-tuning and in-context learning. 
Let  be the model of ChatGPT,  be the molecule,  be the molecule caption, and  be the parameters of . The fine-tuning process can be formulated as: 

where  and  are the updated parameters after being fine-tuned on the entire training set ( for Mol2Cap and  for Cap2Mol).

In contrast, the In-Context Few-Shot Molecule Learning process can be defined as:


where  and  are the Prompt Management templates that transform the original user input (molecules  or captions ) into system prompts with the user input prompts for querying ChatGPT, and  is the original parameters without being fine-tuned.

It is apparent that the fine-tuning methods require additional model training for the sub-tasks of molecule-caption translation.
In contrast, in-context few-shot molecule learning only needs to switch the prompt templates, which is much more efficient for deployment. Through the way of in-context few-shot molecule learning, valid and meaningful responses are expected, which contain the generated captions or molecules in our pre-defined JSON formats.


\subsection{Generation Calibration}
Despite specifying the desired output format, LLMs (e.g., ChatGPT) can occasionally produce unexpected responses, including incorrect output formats and denial of answering. 
To address these issues, a generation calibration mechanism is introduced to validate the response from ChatGPT.

In Generation Calibration, we first check the format of original responses by parsing them into JSON objects.
If the parsing process fails, indicating a deviation from the expected format, 
several pre-defined format correction strategies, such as Regular Matching ~\cite{thompson1968programming}, are introduced to correct the format and extract the desired output from the response. 
If the original response successfully passes the format check or can be calibrated using the format correction strategies, it is considered valid and accepted as a final response. 
However, if the original response fails the format check and cannot be corrected within the predefined strategies, we initiate re-queries. 
Notably, there is a special case for re-queries. When the original response reports the "Exceed Maximum Input Length Limitation" error, we will remove the longest example in the re-query phase until the query length meets the length limitation.
The re-query process involves making additional queries to the LLMs until a valid response is obtained or until the maximum error allowance is reached. This maximum error allowance is set to ensure that the system does not get stuck in an endless loop and instead provides a suitable response to the user within acceptable bounds.

By employing the generation calibration stage, we can mitigate unexpected deviations from the desired output format and ensure that the final responses align with the expected format and requirements. \section{Experiment}
\label{sec:Experiments}
In this section, we aim to evaluate the feasibility and effectiveness of the proposed method MolReGPT by conducting comprehensive experiments on the molecule-caption translation task. 
Additionally, ablation studies are conducted to investigate the impact of different retrieval methods and the number of selected examples. These investigations aim to provide deeper insights into the performance and capabilities of MolReGPT.


\subsection{Experimental Settings}
We first introduce the basic experimental settings. In this work, we use ChatGPT through the OpenAI API \footnote{https://openai.com/blog/openai-api} with backend model \textbf{GPT-3.5-turbo}, which can not be fine-tuned in our tasks. 
Besides, we will provide an overview of the data and metrics employed in this section.

\subsubsection{Dataset}
The research on molecule-caption translation is still in the early stage, and there is only one public dataset ChEBI-20 \cite{edwards2021text2mol}, which contains 33,010 molecule-caption pairs. To ensure consistency, we adhere to the data split process as used in MolT5 ~\cite{edwards-etal-2022-translation}, dividing the dataset into 80/10/10\% train/validation/test splits. 
For our method evaluation, we focus on the test split while utilizing the training set as the local database to retrieve n-shot examples through in-context learning. 

\subsubsection{Evaluation Metrics}
In terms of evaluation metrics, we align with the metrics adopted in MolT5 for comparison ~\cite{edwards-etal-2022-translation}. By adopting these metrics, we ensure consistency and enable a meaningful and fair assessment of the performance of our method.
\begin{itemize}

\item \textbf{Mol2Cap Metrics}. 
In the Mol2Cap task, natural language generation metrics like BLEU, ROUGE, and METEOR scores are applied to assess the proximity of the generated output to the ground truth. 
Here, BLEU and ROUGE scores evaluate the n-gram precision, measuring the alignment between the generated structures and the reference structures, while METEOR is a recall-oriented metric that accounts for both exact matches and paraphrases between the generated and reference structures.
Additionally, we incorporate \emph{Text2Mol}, a task-specific metric that employs pre-trained models to quantify the structural similarity between the generated and reference molecules using their SMILES representations \cite{edwards2021text2mol}.
This metric provides further insights into the quality and relevance of the generated output in terms of the underlying molecular structures.

\item \textbf{Cap2Mol Metrics}. 
Since the SMILES representation of molecules exhibits a sequence structure, natural language metrics can be directly applied for evaluation. Thus, BLEU and the Exact Match scores are calculated as initial assessments. 
Furthermore, molecule-specified metrics are also reported, including Levenshtein distance, validity, and three molecule fingerprints scores - MACCS FTS, RDK FTS, and Morgan FTS. 
These metrics provide valuable insights into the quality, validity, and structural characteristics of the generated molecules. Finally, the \emph{Text2Mol} metric is also discussed here to highlight the relevance between the generated molecules and the input molecule captions. 
\end{itemize}


Note that smaller values of Levenshtein score and FCD indicate better generation performance in the molecule generation task, while other evaluation metrics positively correlate to the performance.

\subsubsection{Baselines}
It is worth mentioning that there are limited baselines for translating between molecule captioning and text-based molecule generation.
Specifically, the following baselines are selected for performance evaluation:
\begin{itemize}
    \item \textbf{Transformer}~\cite{vaswani2017attention}. This method is the most representative language architecture to process natural language. 
    A vanilla Transformer model with six encoder and decoder layers, directly trained on ChEBI-20. Note that this model is not pre-trained, making it simple and easy to implement. 
    
    \item \textbf{T5-base}~\cite{raffel2020exploring}. 
    T5 is pre-trained on the Colossal Clean Crawled Corpus (C4), but no domain knowledge is specifically fed for pre-training.
    In this work, the base version of T5 is directly fine-tuned on ChEBI-20 for molecule discovery. 
    
    \item \textbf{MolT5-base}~\cite{edwards-etal-2022-translation}. 
     This model is pre-trained on a large corpus with both language texts and SMILES strings so that it can have a prior understanding of the two domains.
    More specifically, the base version of MolT5 was pre-trained on the Colossal Clean Crawled Corpus (C4) and ZINC-15 datasets and further fine-tuned on task-specific dataset ChEBI-20.
\end{itemize}
Note that these baselines are required to fine-tune the model on the public dataset ChEBI-20, specifically tailored to the molecule-caption translation task.

\subsection{Performance Comparison of Molecule-Caption Translation}
We present the results of each sub-task within the molecule-caption translation task, incorporating both quantitative analysis and detailed examples for comparison. 
In addition, Figures \ref{fig:case study}, \ref{fig:caption2smiles_diffmodels} and \ref{fig:smiles2caption_diffmodels} illustrate specific examples that demonstrate the differences among various models, providing a visual understanding of their performance.

\subsubsection{Molecule Captioning (Mol2Cap)}
Given a molecule's SMILES representation, Mol2Cap aims to generate natural language for describing the molecule to enable humans to understand molecular structure, properties, and functionalities in a more efficient and cost-effective manner. 
Table \ref{tab:m2c} illustrates the performance comparison of 10-shot MolReGPT (GPT-3.5-turbo) with other advanced methods for the Mol2Cap task, offering an overview of the results. Notably, our method can achieve comparable ROUGE scores to MolT5-base and T5-base while surpassing all selected baselines in the remaining metrics without being fine-tuned on ChEBI-20 dataset. Furthermore, we obtain the following observations. 

\begin{table*}[htb]
    \centering
        \caption{The performance of molecule captioning on ChEBI-20 dataset. Experimental results for Transformer, T5-base, and MolT5-base are retrieved from ~\cite{edwards-etal-2022-translation}. The \textbf{best} scores are in bold, and the \underline{second-best} scores are underlined.}
    \label{tab:m2c}
    \resizebox{2.0\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    Methods & BLEU-2 & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & METETOR & Text2Mol \\
    \midrule
    Transformer \cite{edwards-etal-2022-translation} & 0.061 & 0.027 & 0.204 & 0.087 & 0.186 & 0.114 & 0.057 \\ 
    T5-base \cite{edwards-etal-2022-translation} & 0.511 & 0.423 & 0.607 & \underline{0.451} & \underline{0.550} & 0.539 & 0.523 \\ 
    MolT5-base \cite{edwards-etal-2022-translation} & \underline{0.540} & \underline{0.457} & \textbf{0.634} & \textbf{0.485} & \textbf{0.578} & \underline{0.569} & \underline{0.547} \\ 
    GPT-3.5-turbo (zero-shot) & 0.103 & 0.050 & 0.261 & 0.088 & 0.204 & 0.161 & 0.352 \\ 
    GPT-3.5-turbo (10-shot MolReGPT) & \textbf{0.565} & \textbf{0.482} & \underline{0.623} & 0.450 & 0.543 & \textbf{0.585} & \textbf{0.560} \\ 
    \bottomrule
    \end{tabular}
    }
\end{table*}

\begin{table*}[htb]
    \centering
        \caption{Text-based molecule generation results on CheBI-20. Experimental results for Transformer, T5-base, and MolT5-base are retrieved from \cite{edwards-etal-2022-translation}. The \textbf{best} scores are in bold, and the \underline{second-best} scores are underlined.}
    \label{tab:c2m}
    \resizebox{2.0\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule
    Method & BLEU & EM & Levenshtein & MACCS FTS & RDK FTS & Morgan FTS & FCD & Text2Mol & Validity \\
    \midrule
    Transformer \cite{edwards-etal-2022-translation} & 0.499 & 0.000 & 57.66 & 0.480 & 0.320 & 0.217 & 11.32 & 0.277 & \textbf{0.906} \\ 
    T5-base \cite{edwards-etal-2022-translation} & 0.762 & 0.069 & 24.950 & \underline{0.731} & \underline{0.605} & \underline{0.545} & 2.48 & \underline{0.499} & 0.660 \\ 
    MolT5-base \cite{edwards-etal-2022-translation} & \underline{0.769} & \underline{0.081} & \textbf{24.458} & 0.721 & 0.588 & 0.529 & \underline{2.18} & 0.496 & 0.772 \\
    GPT-3.5-turbo (zero-shot) & 0.489 & 0.019 & 52.13 & 0.705 & 0.462 & 0.367 & 2.05 & 0.479  & 0.802 \\ 
    GPT-3.5-turbo (10-shot MolReGPT) & \textbf{0.790} & \textbf{0.139} & \underline{24.91} & \textbf{0.847} & \textbf{0.708} & \textbf{0.624} & \textbf{0.57} & \textbf{0.571} & \underline{0.887} \\ 
    \bottomrule
    \end{tabular}
    }

\end{table*}

First, GPT-3.5-turbo is not explicitly trained or fine-tuned for molecule-caption translation tasks so it has poor zero-shot performance. 
However, with the instruction of 10-shot MolReGPT, GPT-3.5-turbo achieves significantly improved results that gain an improvement of 60\% to the zero-shot case and 2.4\% to MolT5-base under the Text2Mol metric, indicating that our proposed method can teach ChatGPT to effectively learn the Mol2Cap task from the system prompt. 

Second, limited by the number of examples, MolReGPT only gains limited insights from the distribution of molecule captions.
The model's predictions for captions heavily rely on its internal factual knowledge and the contextual information provided by the system prompt, which means that common patterns may not be as apparent and can not be captured from the selected  examples. 
As a result, although our 10-shot MolReGPT achieves a 0.560 Text2Mol score, which is higher than MolT5's 0.547, MolReGPT in turn gets lower ROUGE scores compared to MolT5.
However, it is crucial to note that the captions generated by 10-shot MolReGPT with lower ROUGE scores are not entirely incorrect. 
In fact, the highest Text2Mol score serves as a reliable indicator of the generation quality and highlights the better relevance between the generated molecules and the molecule captions.

Figure \ref{fig:smiles2caption_diffmodels} lists examples of molecule captioning results to compare the performance among different models. From the given examples, we note that our MolReGPT can generate captions that contain key information about the input molecule. And more importantly, the generated captions are better in grammar and easy for humans to understand.
\begin{figure*}[htb]
   \centering
   \includegraphics[width=0.9\textwidth]{figures/mol2cap_ex.pdf} 
   \caption{Examples of molecule captions generated by different models, where SMILES strings are converted to molecule graphs for better visualization. 
   Based on the same input molecule graph, our MolReGPT can generate accurate and natural captions to describe the structure, properties, and even the functions of the molecule. 
   In contrast, Transformer generates meaningless captions that are far from the ground truth. Captions generated by MolT5-base seem better but still have many typo errors.}
   \label{fig:smiles2caption_diffmodels}
\end{figure*}





\subsubsection{Text-based Molecule Generation (Cap2Mol)}
Given a natural language description (e.g., properties and functional groups), the goal of Cap2Mol is to generate the corresponding molecule (i.e., SMILES string) for molecule discovery.
Results of the text-based molecule generation task are presented in Table \ref{tab:c2m}. 
Comparing all these baselines, 10-shot MolReGPT significantly enhances the capabilities of GPT-3.5-turbo, leading to the best overall performance.
In molecular evaluation metrics like MACCS FTS, RDK FTS, and Morgan FTS, MolReGPT helps GPT-3.5-turbo gain a significant 15\% increase in Text2Mol score compared to MolT5-base. 
Considering the molecule fingerprints scores, our 10-shot MolReGPT also gets an average of 18\% improvement compared to MolT5-base.
Besides, MolReGPT also achieves the highest exact match score, generating 13.9\% molecules that are completely correct to the ground truth. 
Remarkably, these impressive results are achieved without additional training or fine-tuning steps.

Furthermore, it is worth noting that the original weights of T5 are primarily for natural language generation, which means it has to be fine-tuned separately to fit the two sub-tasks in this study. 
Unfortunately, MolT5 does not tackle this issue, as it continues to treat the two sub-tasks of the molecule-caption translation task as separate tasks.
Switching between the two sub-tasks in MolT5 requires using a different model class and reloading the weights, which makes it technically inefficient. 
Besides, treating these sub-tasks as independent overlooks the potential knowledge transfer between them. In contrast, MolReGPT enables a single foundation LLMs to solve both the two sub-tasks simultaneously, providing a comprehensive solution for LLMs to address molecule-related tasks. 

Figure \ref{fig:caption2smiles_diffmodels} lists examples of text-based molecule generation results to compare the performance among different models. From the given examples, it is clear that our MolReGPT can generate structures more similar to the ground truth.
\begin{figure*}[htb]
   \centering
   \includegraphics[width=0.95\textwidth]{figures/cap2mol_ex.pdf}
   \caption{Examples of molecules generated by different models, where SMILES strings are converted to molecule graphs for better visualization. Based on the same input caption, our MolReGPT can generate accurate molecule graphs described by the caption. In contrast, Transformer generates quite different molecules compared to the ground truth. Compared to Transformer, molecules generated by MolT5-base are closer to the ground truth but still miss so many details.}
   \label{fig:caption2smiles_diffmodels}
\end{figure*}

\subsection{Ablation Study}
In addition to the experiments above, we also perform ablation studies to analyze the critical factors that influence the performance of MolReGPT. We first examine the impact of different retrieval strategies employed for retrieving n-shot examples.
Subsequently, we investigate the influence of the number of selected examples, denoted as , including zero-shot results to ensure that GPT-3.5-turbo is not already trained to handle the molecule-caption translation task. These ablation studies shed light on the key aspects contributing to the performance of MolReGPT.

\subsubsection{Impact of Retrieval Strategies}
Retrieval strategies play a key role in guiding LLMs to perform molecule-caption translation tasks for MolReGPT. 
More similar examples are retrieved, and more valuable information could be contained for in-context few-shot molecule learning.
For each sub-task in the molecule-caption translation task, we choose three retrieval strategies for comparison. The detailed results are shown in Table \ref{tab:m2c_all} and Table \ref{tab:c2m_all}. 

\textbf{Molecule Captioning (Mol2Cap)}. 
In the Mol2Cap task, we compare the performance of three retrieval strategies: Random, BM25, and Morgan FTS (adopted in MolReGPT),. 
The Random strategy involves retrieving  random examples, while BM25 applies a character-level BM25 algorithm to the molecule SMILES representations.

As shown in Table \ref{tab:m2c_all}, among the three retrieval strategies, Morgan FTS shows the best performance with the same value of , outperforming BM25 by 37\% in the Text2Mol metric. Besides, the ROUGE-L score achieved by Morgan FTS is almost doubled compared to the Random or BM25 retrieval strategies. 
The use of Morgan FTS with Dice similarity shows a better estimation of the structural similarity between molecules by comparing unique structural features like functional groups. These features are usually revealed in molecule captions with detailed descriptions. In this case, retrieving similar molecules by Morgan FTS could effectively guide the LLM to learn the associations between molecule structures and caption descriptions, resulting in more accurate and desired outputs.

\textbf{Text-based Molecule Generation (Cap2Mol)}. 
In the Cap2Mol task, we also employ three retrieval strategies: Random, SentenceBert, and BM25 (adopted in MolReGPT). The Random strategy still retrieves  random examples, while SentenceBert encodes captions as numerical vectors to compute their semantic similarity.

As shown in Table \ref{tab:c2m_all}, we find that BM25 is better in the Cap2Mol task, despite the fact that SentenceBert has outperformed BM25 in many classical NLP text retrieval datasets. 
When  changes from 1 to 10, n-shot BM25 always achieves better BLEU, Exact Match, Levenshtein, and fingerprints scores than n-shot SentenceBert.
As shown in the input caption (stage\#1) of Figure \ref{fig:prompt}, the input molecule captions tend to use phrases with dashes (-) like "2-methylphenyl" to connect the structure details of the molecule. 
Understanding such phrases plays a crucial role in generating correct molecule structures. 
In this case, retrieving similar texts while precisely matching these details significantly contributes to performance improvement. In contrast, SentenceBert, as a neural method, encodes an entire caption into a 1-D embedding vector, focusing more on semantic similarity rather than specific details.
Consequently, BM25 is chosen as the retrieval strategy of MolReGPT in text-based molecule generation.

All in all, in both sub-tasks, compared to random selection, the other retrieval strategies used in this paper can help improve n-shot generation results. 
These strategies enhance the overall performance metrics, underscoring the importance of thoughtful retrieval strategy design for achieving performance improvement in MolReGPT.

\subsubsection{Impact of the Number of Examples for In-Context Learning}
In this subsection, we study how the number of examples contained in the system prompt through in-context learning affects the performance.

\begin{table*}[htbp]
    \centering
    \caption{N-shot Molecule Captioning results on ChEBI-20 dataset. The \textbf{best} scores are in bold, and the \underline{second-best} scores are underlined.}
    \label{tab:m2c_all}
    \resizebox{1.6\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    Method & BLEU-2 & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & METETOR & Text2Mol \\
    \midrule
    zero-shot & 0.103 & 0.050 & 0.261 & 0.088 & 0.204 & 0.161 & 0.352 \\ 
    \hline
    1-shot (random) & 0.236 & 0.131 & 0.335 & 0.135 & 0.257 & 0.253 & 0.372 \\ 
    1-shot (BM25) & 0.243 & 0.150 & 0.350 & 0.156 & 0.278 & 0.262 & 0.394\\ 
    1-shot (Morgan FTS) & 0.506 & 0.416 & 0.547 & 0.372 & 0.473 & 0.499 & 0.529\\ 
    \hline
    2-shot (random) & 0.273 & 0.158 & 0.357 & 0.154 & 0.278 & 0.284 & 0.371 \\ 
    2-shot (BM25) & 0.287 & 0.188 & 0.380 & 0.185 & 0.307 & 0.297 & 0.397 \\ 
    2-shot (Morgan FTS) & 0.547 & 0.460 & 0.592 & 0.425 & 0.520 & 0.559 & 0.548 \\ 
    \hline
    5-shot (random) & 0.297 & 0.178 & 0.376 & 0.173 & 0.300 & 0.305 & 0.366 \\ 
    5-shot (BM25) & 0.311 & 0.213 & 0.398 & 0.205 & 0.327 & 0.317 & 0.405 \\ 
    5-shot (Morgan FTS) & \underline{0.562} & \underline{0.478} & \underline{0.609} & \underline{0.446} & \underline{0.540} & \underline{0.583} & \underline{0.559(6)} \\ 
    \hline
    10-shot (random) & 0.295 & 0.181 & 0.389 & 0.185 & 0.310 & 0.329 & 0.369 \\ 
    10-shot (BM25) & 0.326 & 0.227 & 0.413 & 0.221 & 0.342 & 0.333 & 0.408 \\ 
    10-shot (Morgan FTS) & \textbf{0.565} & \textbf{0.482} & \textbf{0.623} & \textbf{0.450} & \textbf{0.543} & \textbf{0.585} & \textbf{0.559(8)} \\ 
    \bottomrule
    \end{tabular}
    }
\end{table*}

\begin{table*}[htbp]
    \centering
    \caption{N-shot Molecule Generation results on ChEBI-20 dataset. The \textbf{best} scores are in bold, and the \underline{second-best} scores are \underline{underlined}.}
    \label{tab:c2m_all}
    \resizebox{1.6\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule
    Method & BLEU & EM & Levenshtein & MACCS FTS & RDK FTS & Morgan FTS & FCD & Text2Mol & Validity \\
    \midrule
    zero-shot & 0.489 & 0.019 & 52.13 & 0.705 & 0.462 & 0.367 & 2.05 & 0.479 & 0.802 \\ 
    \hline
    1-shot (random) & 0.525 & 0.027 & 51.86 & 0.716 & 0.475 & 0.373 & 1.67 & 0.482 & 0.821\\ 
    1-shot (SentenceBert) & 0.687 & 0.066 & 35.89 & 0.796 & 0.609 & 0.511 & 0.85 & 0.541 & 0.839\\ 
    1-shot (BM25) & 0.706 & 0.074 & 33.38 & 0.799 & 0.620 & 0.526 & 0.84 & 0.540 & 0.842\\ 
    \hline
    2-shot (random) & 0.529 & 0.026 & 49.87 & 0.720 & 0.479 & 0.380 & 1.71 & 0.483 & 0.824\\ 
    2-shot (SentenceBert) & 0.642 & 0.048 & 40.98 & 0.770 & 0.560 & 0.463 & 1.01 & 0.557 & 0.841\\ 
    2-shot (BM25) & 0.748 & 0.101 & 28.89 & 0.827 & 0.668 & 0.578 & 0.67 & 0.519 & 0.860\\ 
    \hline
    5-shot (random) & 0.552 & 0.028 & 49.26 & 0.720 & 0.476 & 0.382 & 1.60 & 0.481 & 0.832\\ 
    5-shot (SentenceBert) & 0.758 & 0.095 & 28.34 & 0.824 & 0.659 & 0.568 & 0.71 & 0.558 & 0.871\\ 
    5-shot (BM25) & \underline{0.771} & \underline{0.121} & \underline{26.78} & \underline{0.836} & \underline{0.686} & \underline{0.599} & \underline{0.60} & \underline{0.564} & 0.882\\ 
    \hline
    10-shot (random) & 0.564 & 0.029 & 49.11 & 0.723 & 0.486 & 0.386 & 1.46 & 0.484 & 0.846\\ 
    10-shot (SentenceBert) & 0.767 & 0.098 & 27.46 & 0.831 & 0.672 & 0.585 & 0.63 & 0.562 & \textbf{0.890}\\ 
    10-shot (BM25) & \textbf{0.790} & \textbf{0.139} & \textbf{24.91} & \textbf{0.847} & \textbf{0.708} & \textbf{0.624} & \textbf{0.57} & \textbf{0.571} & \underline{0.887} \\ 
    \bottomrule
    \end{tabular}
    } 
\end{table*}


\textbf{Zero-shot Performance}. 
In the zero-shot scenario, where no extra examples are included in the prompt for guiding LLMs for learning molecule-caption translation tasks, we utilize two special spans, `[MOLECULE\_MASK]' and `[CAPTION\_MASK]', to inform the LLMs of the desired output format, as shown in Figure \ref{fig:zero_shot} (in Appendix). In this case, the output of the LLMs can be conveniently filtered and further processed to satisfy the desired output specifications for molecule discovery.

After analyzing the zero-shot results of GPT-3.5-turbo in Tables \ref{tab:m2c_all} and \ref{tab:c2m_all}, we can observe that OpenAI did include SMILES strings in their training corpus because it can generate basically valid SMILES representations of molecules based on zero-shot prompts, achieving a 0.802 validity score and a 0.479 Text2Mol score in molecule generation.
However, it is important to note that the zero-shot results exhibit a performance level similar to a vanilla Transformer model. This observation provides evidence that GPT-3.5-turbo is not specifically trained on the molecule-caption translation task, thereby alleviating concerns regarding potential information leakage.




\textbf{Few-shot Performance}. 
 Table \ref{tab:m2c_all} and Table \ref{tab:c2m_all} list the comprehensive details of the experimental results, while Figure \ref{fig:trend} (a) and Figure \ref{fig:trend} (b) illustrate how the Text2Mol score changes when the number of examples increases.


Normally, the performance should improve as the number of examples, denoted as , increases, as more examples provide additional knowledge for the task at hand. 
However, due to the input length limitation of LLMs, it is impossible to contain a large number of examples in the system prompt. 
Therefore, for few-shot scenarios, we choose four different values 1, 2, 5, and 10. 

Tables \ref{tab:m2c_all} and \ref{tab:c2m_all} illustrate that performance generally improves as n increases  in the system prompt through in-context learning. Significant performance enhancements are observed as n changes from 0 to 10. 
Taking Morgan FTS and BM25 as examples, in caption generation, we see remarkable increases from 0.050 to 0.482, 0.204 to 0.543, and 0.352 to 0.560 in BLEU-4, ROUGE-L, and Text2Mol scores, respectively.
Besides, BM25 improves molecule generation from 0.489 to 0.790 in the BLEU score and 0.479 to 0.571 in the Text2Mol score.

Besides, it is also interesting to notice that when  increases from 5 to 10, the Text2Mol metrics almost keep the same.
This could be the problem of the maximum input length limitation of LLMs. To fit the input length limitation, we would remove the longest examples to degrade the n-shot generation to (n-1)-shot generation. As  increases, there is a higher possibility of exceeding the input length limitation. In this case, unless the maximum input length of the LLM is expanded, the performance will finally converge when  continues to grow.




 
\section{Conclusion}
\label{sec:conclusion}
In this work, we propose MolReGPT, a general retrieval-based prompt paradigm that empowers molecule discovery with LLMs like ChatGPT under In-Context Few-Shot Molecule Learning. MolReGPT leverages the molecular similarity principle to retrieve examples from a local database, guiding LLMs in generating n-shot outputs without fine-tuning. Our method is focused and evaluated on the task of molecule-caption translation, including molecule captioning (Mol2Cap) and text-based molecule generation (Cap2Mol). Specifically, BM25 is applied to retrieve similar molecule captions, while Morgan Fingerprints and Dice similarity are adopted to retrieve similar molecules. 
Experimental results show that our proposed MolReGPT can empower ChatGPT to achieve 0.560 and 0.571 Text2Mol scores in molecule captioning and molecule generation, respectively.
The performance surpasses fine-tuned models like MolT5-base in both molecule understanding and text-based molecule generation aspects and is even comparable to the fine-tuned MolT5-large.
To conclude, MolReGPT provides a novel and versatile paradigm to deploy LLMs in molecule discovery through in-context learning, which greatly reduces the cost of domain transfer and explores the potential of LLMs in molecule discovery. 
\bibliography{main}
\bibliographystyle{ICML/icml2023}


\newpage
\appendix
\onecolumn
\section{Appendix}

\begin{figure*}[htbp]
\centering
   \includegraphics[width=0.8\textwidth]{figures/zero_shot.pdf}
    \caption{System Prompt for zero-shot Molecule-Caption translation.  The main structure of zero-shot prompts is almost the same as that of few-shot prompts. The main difference lies in that the \textbf{Example} part in few-shot prompts is changed to \textbf{Task Format} to pre-define the input and output format. To avoid information leaks, we use "[CAPTION\_MASK]" and "[MOLECULE\_MASK]" to denote the position of captions and molecules.}
   \label{fig:zero_shot}
\end{figure*}



\begin{figure*}[htbp]
   \centering
   \includegraphics[width=0.950\textwidth]{figures/generation.pdf}   
   \caption{
   Illustrations of molecule graphs generated by MolT5 and our MolReGPT, given  customized inputs.
   Notably, the key points in Example 1 highlight the \underline{\textbf{five}} \textbf{\red{benzene rings}} and \textbf{\green{hydrophobic groups}} in the structure, which are correctly generated by our MolReGPT. 
   In contrast, the results of MolT5 generate the incorrect number of \red{\textbf{benzene rings}} and contain a few \textbf{\blue{hydrophilic groups}}. 
   In example 2, both generations give the correct number of benzene rings, while MolReGPT generates more hydrophilic groups, which are closer to our input caption.}
   \label{fig:case study}
\end{figure*}


\begin{figure*}[htbp]
\centering  
\subfigure[Text2Mol metric comparison of caption retrieval strategies with respect to the change of the number of selected examples in the \emph{Cap2Mol} task.]
{   
\begin{minipage}{8cm}
\centering    
\includegraphics[width=\textwidth]{figures/cap_re.pdf}  
\end{minipage}
}
\hspace{0.2cm}
\subfigure[Text2Mol metric comparison of molecule retrieval strategies with respect to the change of the number of selected examples in the \emph{Mol2Cap} task.]
{ 
\begin{minipage}{8cm}
\centering    
\includegraphics[width=\textwidth]{figures/mol_re.pdf}
\end{minipage}
}
\caption{The trend of the Text2Mol metric with respect to the number of examples (i.e., ). Basically, as  increases, the n-shot performance is also improved. However, when  increases from 1 to 2, we see a clear performance drop in molecule generation, which is possibly the reason that the noise brought by the added examples exceeds the information gain they could bring. 
Besides, in caption generation, we see a remarkable increase by comparing Morgan Fingerprints to other retrieval strategies, showing the superiority of Morgan Fingerprints-based molecule retrieval. It is also interesting to notice that when  increases from 5 to 10, the Text2Mol metrics almost keep the same. This is the problem of the maximum input length limitation of LLMs.
To fit the input length limitation, we would remove the longest examples to degrade the n-shot generation to (n-1)-shot generation. As  increases, there is a higher possibility of exceeding the input length limitation. In this case, unless the maximum input length of the LLMs is expanded, the performance will finally converge when  continues to grow.}    
\label{fig:trend}   
\end{figure*}




 


\end{document}
