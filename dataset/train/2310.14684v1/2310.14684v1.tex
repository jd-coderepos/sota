\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{acl}


\usepackage{times}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{soul}
\usepackage{adjustbox}
\usepackage{fontawesome}

\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}



\title{\textsc{SpEL}: Structured Prediction for Entity Linking}


\author{Hassan S. Shavarani \\
  School of Computing Science \\
  Simon Fraser University \\
  BC, Canada \\
  \texttt{sshavara@sfu.ca} \\\And
  Anoop Sarkar \\
  School of Computing Science \\
  Simon Fraser University\\
  BC, Canada \\
  \texttt{anoop@sfu.ca} \\}

\begin{document}
\maketitle
\begin{abstract}
Entity linking is a prominent thread of research focused on structured data creation by linking spans of text to an ontology or knowledge source. We revisit the use of structured prediction for entity linking which classifies each individual input token as an entity, and aggregates the token predictions. Our system, called \textsc{SpEL} (Structured prediction for Entity Linking) is a state-of-the-art entity linking system that uses some new ideas to apply structured prediction to the task of entity linking including: two refined fine-tuning steps; a context sensitive prediction aggregation strategy; reduction of the size of the model's output vocabulary, and; we address a common problem in entity-linking systems where there is a training vs.\ inference tokenization mismatch. Our experiments show that we can outperform the state-of-the-art on the commonly used AIDA benchmark dataset for entity linking to Wikipedia. Our method is also very compute efficient in terms of number of parameters and speed of inference.

\faGithub\ \href{https://github.com/shavarani/SpEL}{https://github.com/shavarani/SpEL}

\end{abstract}

\section{Introduction}


Knowledge bases, such as Wikipedia and Yago \cite{YAGO4}, are valuable resources that facilitate structured information extraction from textual data. 
Entity Linking \cite{TKDE14} involves identifying text spans (mentions) and disambiguating the concept or knowledge base entry to which the mention is linked. 

\begin{figure}[!ht]

    \scalebox{0.7}{
        \trimbox{1.3cm 0cm 0cm 0cm}{ 
            


\tikzset{every picture/.style={line width=0.8pt}} 

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=0.85]


\draw  [color={rgb, 255:red, 189; green, 16; blue, 224 }  ,draw opacity=1 ][fill={rgb, 255:red, 189; green, 16; blue, 224 }  ,fill opacity=0.2 ] (253,359.22) .. controls (253,353.78) and (257.42,349.36) .. (262.87,349.36) -- (610.4,349.36) .. controls (615.84,349.36) and (620.26,353.78) .. (620.26,359.22) -- (620.26,388.82) .. controls (620.26,394.27) and (615.84,398.69) .. (610.4,398.69) -- (262.87,398.69) .. controls (257.42,398.69) and (253,394.27) .. (253,388.82) -- cycle ;
\draw    (273.75,430.18) -- (273.05,400.74) ;
\draw [shift={(273,398.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (308.75,430.18) -- (308.05,400.74) ;
\draw [shift={(308,398.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (363.75,430.18) -- (363.05,400.74) ;
\draw [shift={(363,398.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (411.75,430.18) -- (411.05,400.74) ;
\draw [shift={(411,398.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (461.75,430.18) -- (461.05,400.74) ;
\draw [shift={(461,398.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (491.75,430.18) -- (491.05,400.74) ;
\draw [shift={(491,398.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (543.75,430.18) -- (543.05,400.74) ;
\draw [shift={(543,398.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (603.75,430.18) -- (603.05,400.74) ;
\draw [shift={(603,398.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw   (260,450.26) .. controls (260.03,454.93) and (262.37,457.25) .. (267.04,457.23) -- (283.54,457.14) .. controls (290.21,457.11) and (293.55,459.42) .. (293.57,464.09) .. controls (293.55,459.42) and (296.87,457.07) .. (303.54,457.04)(300.54,457.06) -- (320.04,456.96) .. controls (324.71,456.93) and (327.03,454.59) .. (327,449.92) ;
\draw   (343.64,450.5) .. controls (343.66,455.17) and (346,457.49) .. (350.67,457.47) -- (353.31,457.47) .. controls (359.98,457.44) and (363.32,459.76) .. (363.34,464.43) .. controls (363.32,459.76) and (366.64,457.42) .. (373.31,457.39)(370.31,457.4) -- (375.96,457.38) .. controls (380.63,457.37) and (382.95,455.03) .. (382.93,450.36) ;
\draw   (448.64,450.5) .. controls (448.61,455.17) and (450.92,457.52) .. (455.59,457.55) -- (466.77,457.64) .. controls (473.44,457.69) and (476.75,460.04) .. (476.72,464.71) .. controls (476.75,460.04) and (480.1,457.74) .. (486.77,457.79)(483.77,457.76) -- (497.95,457.87) .. controls (502.62,457.9) and (504.97,455.59) .. (505,450.92) ;
\draw   (516.5,450.93) .. controls (516.59,455.6) and (518.96,457.89) .. (523.63,457.8) -- (533.63,457.61) .. controls (540.3,457.49) and (543.67,459.76) .. (543.76,464.43) .. controls (543.67,459.76) and (546.96,457.37) .. (553.63,457.24)(550.63,457.3) -- (563.63,457.06) .. controls (568.3,456.97) and (570.59,454.6) .. (570.5,449.93) ;
\draw  [color={rgb, 255:red, 144; green, 19; blue, 254 }  ,draw opacity=1 ][fill={rgb, 255:red, 144; green, 19; blue, 254 }  ,fill opacity=0.2 ] (253,334.1) .. controls (253,332.35) and (254.42,330.93) .. (256.17,330.93) -- (617.09,330.93) .. controls (618.84,330.93) and (620.26,332.35) .. (620.26,334.1) -- (620.26,343.6) .. controls (620.26,345.35) and (618.84,346.77) .. (617.09,346.77) -- (256.17,346.77) .. controls (254.42,346.77) and (253,345.35) .. (253,343.6) -- cycle ;
\draw   (391.64,450.5) .. controls (391.66,455.17) and (394,457.49) .. (398.67,457.47) -- (401.31,457.47) .. controls (407.98,457.44) and (411.32,459.76) .. (411.34,464.43) .. controls (411.32,459.76) and (414.64,457.42) .. (421.31,457.39)(418.31,457.4) -- (423.96,457.38) .. controls (428.63,457.37) and (430.95,455.03) .. (430.93,450.36) ;
\draw    (273.75,331.18) -- (273.05,301.74) ;
\draw [shift={(273,299.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (308.75,331.18) -- (308.05,301.74) ;
\draw [shift={(308,299.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (363.75,331.18) -- (363.05,301.74) ;
\draw [shift={(363,299.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (411.75,331.18) -- (411.05,301.74) ;
\draw [shift={(411,299.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (462.75,331.18) -- (462.05,301.74) ;
\draw [shift={(462,299.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (492.75,331.18) -- (492.05,301.74) ;
\draw [shift={(492,299.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (543.75,331.18) -- (543.05,301.74) ;
\draw [shift={(543,299.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw    (603.75,331.18) -- (603.05,301.74) ;
\draw [shift={(603,299.74)}, rotate = 88.63] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (256.33,119.38) .. controls (256.33,117.54) and (257.83,116.04) .. (259.67,116.04) -- (289.46,116.04) .. controls (291.3,116.04) and (292.8,117.54) .. (292.8,119.38) -- (292.8,129.38) .. controls (292.8,131.22) and (291.3,132.72) .. (289.46,132.72) -- (259.67,132.72) .. controls (257.83,132.72) and (256.33,131.22) .. (256.33,129.38) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (256.33,136.05) .. controls (256.33,134.21) and (257.83,132.72) .. (259.67,132.72) -- (289.46,132.72) .. controls (291.3,132.72) and (292.8,134.21) .. (292.8,136.05) -- (292.8,146.06) .. controls (292.8,147.9) and (291.3,149.39) .. (289.46,149.39) -- (259.67,149.39) .. controls (257.83,149.39) and (256.33,147.9) .. (256.33,146.06) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (256.33,152.73) .. controls (256.33,150.88) and (257.83,149.39) .. (259.67,149.39) -- (289.46,149.39) .. controls (291.3,149.39) and (292.8,150.88) .. (292.8,152.73) -- (292.8,162.73) .. controls (292.8,164.57) and (291.3,166.07) .. (289.46,166.07) -- (259.67,166.07) .. controls (257.83,166.07) and (256.33,164.57) .. (256.33,162.73) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (292.33,119.38) .. controls (292.33,117.54) and (293.83,116.04) .. (295.67,116.04) -- (325.46,116.04) .. controls (327.3,116.04) and (328.8,117.54) .. (328.8,119.38) -- (328.8,129.38) .. controls (328.8,131.22) and (327.3,132.72) .. (325.46,132.72) -- (295.67,132.72) .. controls (293.83,132.72) and (292.33,131.22) .. (292.33,129.38) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (292.33,136.05) .. controls (292.33,134.21) and (293.83,132.72) .. (295.67,132.72) -- (325.46,132.72) .. controls (327.3,132.72) and (328.8,134.21) .. (328.8,136.05) -- (328.8,146.06) .. controls (328.8,147.9) and (327.3,149.39) .. (325.46,149.39) -- (295.67,149.39) .. controls (293.83,149.39) and (292.33,147.9) .. (292.33,146.06) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (292.33,152.73) .. controls (292.33,150.88) and (293.83,149.39) .. (295.67,149.39) -- (325.46,149.39) .. controls (327.3,149.39) and (328.8,150.88) .. (328.8,152.73) -- (328.8,162.73) .. controls (328.8,164.57) and (327.3,166.07) .. (325.46,166.07) -- (295.67,166.07) .. controls (293.83,166.07) and (292.33,164.57) .. (292.33,162.73) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (345.33,119.38) .. controls (345.33,117.54) and (346.83,116.04) .. (348.67,116.04) -- (378.46,116.04) .. controls (380.3,116.04) and (381.8,117.54) .. (381.8,119.38) -- (381.8,129.38) .. controls (381.8,131.22) and (380.3,132.72) .. (378.46,132.72) -- (348.67,132.72) .. controls (346.83,132.72) and (345.33,131.22) .. (345.33,129.38) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (345.33,136.05) .. controls (345.33,134.21) and (346.83,132.72) .. (348.67,132.72) -- (378.46,132.72) .. controls (380.3,132.72) and (381.8,134.21) .. (381.8,136.05) -- (381.8,146.06) .. controls (381.8,147.9) and (380.3,149.39) .. (378.46,149.39) -- (348.67,149.39) .. controls (346.83,149.39) and (345.33,147.9) .. (345.33,146.06) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (345.33,152.73) .. controls (345.33,150.88) and (346.83,149.39) .. (348.67,149.39) -- (378.46,149.39) .. controls (380.3,149.39) and (381.8,150.88) .. (381.8,152.73) -- (381.8,162.73) .. controls (381.8,164.57) and (380.3,166.07) .. (378.46,166.07) -- (348.67,166.07) .. controls (346.83,166.07) and (345.33,164.57) .. (345.33,162.73) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (393.33,119.38) .. controls (393.33,117.54) and (394.83,116.04) .. (396.67,116.04) -- (426.46,116.04) .. controls (428.3,116.04) and (429.8,117.54) .. (429.8,119.38) -- (429.8,129.38) .. controls (429.8,131.22) and (428.3,132.72) .. (426.46,132.72) -- (396.67,132.72) .. controls (394.83,132.72) and (393.33,131.22) .. (393.33,129.38) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (393.33,169.4) .. controls (393.33,167.56) and (394.83,166.07) .. (396.67,166.07) -- (426.46,166.07) .. controls (428.3,166.07) and (429.8,167.56) .. (429.8,169.4) -- (429.8,179.4) .. controls (429.8,181.25) and (428.3,182.74) .. (426.46,182.74) -- (396.67,182.74) .. controls (394.83,182.74) and (393.33,181.25) .. (393.33,179.4) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (393.33,185.34) .. controls (393.33,183.5) and (394.83,182.01) .. (396.67,182.01) -- (426.46,182.01) .. controls (428.3,182.01) and (429.8,183.5) .. (429.8,185.34) -- (429.8,195.35) .. controls (429.8,197.19) and (428.3,198.68) .. (426.46,198.68) -- (396.67,198.68) .. controls (394.83,198.68) and (393.33,197.19) .. (393.33,195.35) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (441.33,201.77) .. controls (441.33,199.93) and (442.83,198.44) .. (444.67,198.44) -- (474.46,198.44) .. controls (476.3,198.44) and (477.8,199.93) .. (477.8,201.77) -- (477.8,211.77) .. controls (477.8,213.62) and (476.3,215.11) .. (474.46,215.11) -- (444.67,215.11) .. controls (442.83,215.11) and (441.33,213.62) .. (441.33,211.77) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (441.33,218.44) .. controls (441.33,216.6) and (442.83,215.11) .. (444.67,215.11) -- (474.46,215.11) .. controls (476.3,215.11) and (477.8,216.6) .. (477.8,218.44) -- (477.8,228.45) .. controls (477.8,230.29) and (476.3,231.78) .. (474.46,231.78) -- (444.67,231.78) .. controls (442.83,231.78) and (441.33,230.29) .. (441.33,228.45) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (441.33,234.63) .. controls (441.33,232.79) and (442.83,231.3) .. (444.67,231.3) -- (474.46,231.3) .. controls (476.3,231.3) and (477.8,232.79) .. (477.8,234.63) -- (477.8,244.63) .. controls (477.8,246.48) and (476.3,247.97) .. (474.46,247.97) -- (444.67,247.97) .. controls (442.83,247.97) and (441.33,246.48) .. (441.33,244.63) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (477.33,201.77) .. controls (477.33,199.93) and (478.83,198.44) .. (480.67,198.44) -- (510.46,198.44) .. controls (512.3,198.44) and (513.8,199.93) .. (513.8,201.77) -- (513.8,211.77) .. controls (513.8,213.62) and (512.3,215.11) .. (510.46,215.11) -- (480.67,215.11) .. controls (478.83,215.11) and (477.33,213.62) .. (477.33,211.77) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (477.33,218.44) .. controls (477.33,216.6) and (478.83,215.11) .. (480.67,215.11) -- (510.46,215.11) .. controls (512.3,215.11) and (513.8,216.6) .. (513.8,218.44) -- (513.8,228.45) .. controls (513.8,230.29) and (512.3,231.78) .. (510.46,231.78) -- (480.67,231.78) .. controls (478.83,231.78) and (477.33,230.29) .. (477.33,228.45) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (477.33,234.63) .. controls (477.33,232.79) and (478.83,231.3) .. (480.67,231.3) -- (510.46,231.3) .. controls (512.3,231.3) and (513.8,232.79) .. (513.8,234.63) -- (513.8,244.63) .. controls (513.8,246.48) and (512.3,247.97) .. (510.46,247.97) -- (480.67,247.97) .. controls (478.83,247.97) and (477.33,246.48) .. (477.33,244.63) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (525.33,119.38) .. controls (525.33,117.54) and (526.83,116.04) .. (528.67,116.04) -- (558.46,116.04) .. controls (560.3,116.04) and (561.8,117.54) .. (561.8,119.38) -- (561.8,129.38) .. controls (561.8,131.22) and (560.3,132.72) .. (558.46,132.72) -- (528.67,132.72) .. controls (526.83,132.72) and (525.33,131.22) .. (525.33,129.38) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (525.33,185.34) .. controls (525.33,183.5) and (526.83,182.01) .. (528.67,182.01) -- (558.46,182.01) .. controls (560.3,182.01) and (561.8,183.5) .. (561.8,185.34) -- (561.8,195.35) .. controls (561.8,197.19) and (560.3,198.68) .. (558.46,198.68) -- (528.67,198.68) .. controls (526.83,198.68) and (525.33,197.19) .. (525.33,195.35) -- cycle ;
\draw  [draw opacity=0][fill={rgb, 255:red, 155; green, 155; blue, 155 }  ,fill opacity=0.2 ] (525.33,267.98) .. controls (525.33,266.14) and (526.83,264.64) .. (528.67,264.64) -- (558.46,264.64) .. controls (560.3,264.64) and (561.8,266.14) .. (561.8,267.98) -- (561.8,277.98) .. controls (561.8,279.82) and (560.3,281.32) .. (558.46,281.32) -- (528.67,281.32) .. controls (526.83,281.32) and (525.33,279.82) .. (525.33,277.98) -- cycle ;
\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][dash pattern={on 5.63pt off 4.5pt}][line width=1.5]  (254.1,79.77) -- (384.1,79.77) -- (384.1,299.77) -- (254.1,299.77) -- cycle ;
\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (384.1,66.02) .. controls (384.11,61.35) and (381.78,59.02) .. (377.11,59.01) -- (328.56,58.91) .. controls (321.89,58.9) and (318.57,56.56) .. (318.58,51.89) .. controls (318.57,56.56) and (315.23,58.88) .. (308.56,58.87)(311.56,58.87) -- (260.01,58.76) .. controls (255.34,58.75) and (253.01,61.08) .. (253,65.75) ;
\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][dash pattern={on 5.63pt off 4.5pt}][line width=1.5]  (390.1,79.77) -- (433.33,79.77) -- (433.33,299.77) -- (390.1,299.77) -- cycle ;
\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][dash pattern={on 5.63pt off 4.5pt}][line width=1.5]  (439.08,80.77) -- (516.51,80.77) -- (516.51,299.77) -- (439.08,299.77) -- cycle ;
\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (514.76,65.7) .. controls (514.76,61.03) and (512.43,58.7) .. (507.76,58.71) -- (487.38,58.72) .. controls (480.71,58.73) and (477.38,56.4) .. (477.37,51.73) .. controls (477.38,56.4) and (474.05,58.73) .. (467.38,58.73)(470.38,58.73) -- (447,58.75) .. controls (442.33,58.75) and (440,61.08) .. (440,65.75) ;
\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (433.01,65.7) .. controls (432.9,61.03) and (430.52,58.75) .. (425.85,58.86) -- (420.85,58.98) .. controls (414.18,59.13) and (410.8,56.87) .. (410.69,52.2) .. controls (410.8,56.87) and (407.52,59.28) .. (400.86,59.43)(403.85,59.36) -- (395.85,59.54) .. controls (391.18,59.65) and (388.9,62.03) .. (389.01,66.7) ;
\draw  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ] (613.26,65.77) .. controls (613.31,61.1) and (611.01,58.75) .. (606.34,58.7) -- (577.71,58.38) .. controls (571.04,58.3) and (567.74,55.93) .. (567.79,51.26) .. controls (567.74,55.93) and (564.38,58.22) .. (557.71,58.15)(560.71,58.19) -- (529.08,57.83) .. controls (524.41,57.78) and (522.05,60.08) .. (522,64.75) ;
\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=2.25]    (263,140.93) -- (375.75,140.94) ;
\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=0.75 ][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=0.5 ] (377.58,283.79) -- (263.1,283.79) -- (263.1,179.87) -- (377.58,179.87) -- cycle -- (377.58,283.79) -- cycle ; \draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=0.75 ] (377.58,283.79) -- (377.58,283.79) -- cycle ;
\draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=0.75 ][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=0.5 ] (511.58,191.79) -- (442.1,191.79) -- (442.1,88.87) -- (511.58,88.87) -- cycle -- (511.58,191.79) -- cycle ; \draw  [color={rgb, 255:red, 126; green, 211; blue, 33 }  ,draw opacity=0.75 ] (511.58,191.79) -- (511.58,191.79) -- cycle ;
\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 5.63pt off 4.5pt}]  (521.76,80.72) -- (609.76,80.72) ;
\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 5.63pt off 4.5pt}]  (521.76,80.72) -- (521.76,300.02) ;
\draw [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 5.63pt off 4.5pt}]  (521.76,300.02) -- (609.76,300.02) ;

\draw (436.63,374.02) node   [align=left] {\begin{minipage}[lt]{175.67pt}\setlength\topsep{0pt}
\begin{center}
fine-tuned pre-trained language model
\end{center}

\end{minipage}};
\draw (273.48,438.57) node   [align=left] {\begin{minipage}[lt]{14.06pt}\setlength\topsep{0pt}
\begin{center}
Gr
\end{center}

\end{minipage}};
\draw (308.24,440) node   [align=left] {\begin{minipage}[lt]{19.17pt}\setlength\topsep{0pt}
\begin{center}
ace
\end{center}

\end{minipage}};
\draw (363.4,439) node   [align=left] {\begin{minipage}[lt]{24.83pt}\setlength\topsep{0pt}
\begin{center}
Kelly
\end{center}

\end{minipage}};
\draw (411.06,438.57) node   [align=left] {\begin{minipage}[lt]{13.49pt}\setlength\topsep{0pt}
\begin{center}
by
\end{center}

\end{minipage}};
\draw (461.39,438.57) node   [align=left] {\begin{minipage}[lt]{11.22pt}\setlength\topsep{0pt}
\begin{center}
M
\end{center}

\end{minipage}};
\draw (490.74,438.57) node   [align=left] {\begin{minipage}[lt]{15.77pt}\setlength\topsep{0pt}
\begin{center}
ika
\end{center}

\end{minipage}};
\draw (543.25,438.57) node   [align=left] {\begin{minipage}[lt]{39.59pt}\setlength\topsep{0pt}
\begin{center}
reached
\end{center}

\end{minipage}};
\draw (593,438) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{11.23pt}\setlength\topsep{0pt}
\begin{center}
...
\end{center}

\end{minipage}};
\draw (294.07,475) node   [align=left] {\begin{minipage}[lt]{15.77pt}\setlength\topsep{0pt}
\begin{center}
w1
\end{center}

\end{minipage}};
\draw (364.07,476) node   [align=left] {\begin{minipage}[lt]{15.77pt}\setlength\topsep{0pt}
\begin{center}
w2
\end{center}

\end{minipage}};
\draw (411.07,475) node   [align=left] {\begin{minipage}[lt]{15.77pt}\setlength\topsep{0pt}
\begin{center}
w3
\end{center}

\end{minipage}};
\draw (477.07,475) node   [align=left] {\begin{minipage}[lt]{15.77pt}\setlength\topsep{0pt}
\begin{center}
w4
\end{center}

\end{minipage}};
\draw (544.07,475) node   [align=left] {\begin{minipage}[lt]{15.77pt}\setlength\topsep{0pt}
\begin{center}
w5
\end{center}

\end{minipage}};
\draw (436.63,338.85) node   [align=left] {\begin{minipage}[lt]{206.27pt}\setlength\topsep{0pt}
\begin{center}
classification head of fixed candidate set size
\end{center}

\end{minipage}};
\draw (235,290.92) node   [align=left] {\begin{minipage}[lt]{11.23pt}\setlength\topsep{0pt}
\begin{flushright}
...
\end{flushright}

\end{minipage}};
\draw (189,272.92) node   [align=left] {\begin{minipage}[lt]{70.18pt}\setlength\topsep{0pt}
\begin{flushright}
Single\_(music)
\end{flushright}

\end{minipage}};
\draw (184,255.92) node   [align=left] {\begin{minipage}[lt]{78.14pt}\setlength\topsep{0pt}
\begin{flushright}
United\_Kingdom
\end{flushright}

\end{minipage}};
\draw (210,238.92) node   [align=left] {\begin{minipage}[lt]{43.53pt}\setlength\topsep{0pt}
\begin{flushright}
FC\_Mika
\end{flushright}

\end{minipage}};
\draw (205,222.92) node   [align=left] {\begin{minipage}[lt]{50.35pt}\setlength\topsep{0pt}
\begin{flushright}
Mika\_Solo
\end{flushright}

\end{minipage}};
\draw (193,205.92) node   [align=left] {\begin{minipage}[lt]{64.5pt}\setlength\topsep{0pt}
\begin{flushright}
Mika\_(singer)
\end{flushright}

\end{minipage}};
\draw (175,189.92) node   [align=left] {\begin{minipage}[lt]{86.64pt}\setlength\topsep{0pt}
\begin{flushright}
UK\_Singles\_Chart
\end{flushright}

\end{minipage}};
\draw (191,174.92) node   [align=left] {\begin{minipage}[lt]{66.61pt}\setlength\topsep{0pt}
\begin{flushright}
Malika\_Ayane
\end{flushright}

\end{minipage}};
\draw (170,157.92) node   [align=left] {\begin{minipage}[lt]{92.86pt}\setlength\topsep{0pt}
\begin{flushright}
Grace\_Kelly\_(song)
\end{flushright}

\end{minipage}};
\draw (162,140.92) node   [align=left] {\begin{minipage}[lt]{103.63pt}\setlength\topsep{0pt}
\begin{flushright}
Grace\_Kelly\_(actress)
\end{flushright}

\end{minipage}};
\draw (235,124.92) node   [align=left] {\begin{minipage}[lt]{10.66pt}\setlength\topsep{0pt}
\begin{flushright}

\end{flushright}

\end{minipage}};
\draw (411.56,174.4) node   [align=left] {\begin{minipage}[lt]{16.9pt}\setlength\topsep{0pt}
\begin{center}
0.0
\end{center}

\end{minipage}};
\draw (411.56,190.34) node   [align=left] {\begin{minipage}[lt]{16.9pt}\setlength\topsep{0pt}
\begin{center}
0.0
\end{center}

\end{minipage}};
\draw (411.56,124.38) node   [align=left] {\begin{minipage}[lt]{16.9pt}\setlength\topsep{0pt}
\begin{center}
1.0
\end{center}

\end{minipage}};
\draw (274.56,124.38) node   [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.01
\end{center}

\end{minipage}};
\draw (274.56,141.05) node   [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.83
\end{center}

\end{minipage}};
\draw (274.56,157.73) node   [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.16
\end{center}

\end{minipage}};
\draw (310.56,124.38) node   [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.01
\end{center}

\end{minipage}};
\draw (310.56,141.05) node  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,opacity=1 ] [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.80
\end{center}

\end{minipage}};
\draw (310.56,157.73) node   [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.19
\end{center}

\end{minipage}};
\draw (363.56,124.38) node   [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.01
\end{center}

\end{minipage}};
\draw (363.56,141.05) node   [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.85
\end{center}

\end{minipage}};
\draw (363.56,157.73) node   [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.14
\end{center}

\end{minipage}};
\draw (543.56,124.38) node   [align=left] {\begin{minipage}[lt]{16.9pt}\setlength\topsep{0pt}
\begin{center}
1.0
\end{center}

\end{minipage}};
\draw (543.56,190.34) node   [align=left] {\begin{minipage}[lt]{16.9pt}\setlength\topsep{0pt}
\begin{center}
0.0
\end{center}

\end{minipage}};
\draw (543.56,272.98) node   [align=left] {\begin{minipage}[lt]{16.9pt}\setlength\topsep{0pt}
\begin{center}
0.0
\end{center}

\end{minipage}};
\draw (459.56,239.63) node   [align=left] {\begin{minipage}[lt]{16.9pt}\setlength\topsep{0pt}
\begin{center}
0.0
\end{center}

\end{minipage}};
\draw (495.56,239.63) node   [align=left] {\begin{minipage}[lt]{16.9pt}\setlength\topsep{0pt}
\begin{center}
0.0
\end{center}

\end{minipage}};
\draw (459.56,223.45) node   [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.01
\end{center}

\end{minipage}};
\draw (495.56,223.45) node   [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.52
\end{center}

\end{minipage}};
\draw (459.56,206.77) node   [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.99
\end{center}

\end{minipage}};
\draw (495.56,206.77) node   [align=left] {\begin{minipage}[lt]{22.58pt}\setlength\topsep{0pt}
\begin{center}
0.48
\end{center}

\end{minipage}};
\draw (316.27,26.75) node  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ] [align=left] {\begin{minipage}[lt]{92.86pt}\setlength\topsep{0pt}
\begin{center}
pred: \\Grace\_Kelly\_(song)
\end{center}

\end{minipage}};
\draw (483.27,27.75) node  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,opacity=1 ] [align=left] {\begin{minipage}[lt]{64.5pt}\setlength\topsep{0pt}
\begin{center}
pred:\\Mika\_(singer)
\end{center}

\end{minipage}};
\draw (411.27,27.75) node  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {\begin{minipage}[lt]{25.98pt}\setlength\topsep{0pt}
\begin{center}
pred:\\
\end{center}

\end{minipage}};
\draw (568.27,27.75) node  [color={rgb, 255:red, 208; green, 2; blue, 27 }  ,opacity=1 ] [align=left] {\begin{minipage}[lt]{25.98pt}\setlength\topsep{0pt}
\begin{center}
pred:\\
\end{center}

\end{minipage}};
\draw (320.34,231.83) node   [align=left] {{\small Grace\_Kelly\_}\\{\small (actress) is not}\\{\small in the candidate }\\{\small set for this phrase,}\\{\small so will be ignored!}};
\draw (476.84,140.33) node   [align=left] {{\small prediction}\\{\small based on}\\{\small the highest}\\{\small average}\\{\small probability}};
\draw (595.14,175.07) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{11.23pt}\setlength\topsep{0pt}
\begin{center}
...
\end{center}

\end{minipage}};
\draw (235,105.92) node   [align=left] {\begin{minipage}[lt]{11.23pt}\setlength\topsep{0pt}
\begin{flushright}
...
\end{flushright}

\end{minipage}};

\end{tikzpicture}         }
    }
    \caption{\textsc{SpEL} (Structured prediction for Entity Linking). In this example, we demonstrate top 3 most probable entities for each tokenized subword. In the first phrase, the most likely entity identified for \texttt{Grace Kelly} is incorrect. We use a candidate set for this mention to filter out irrelevant entity links with a high probability. In the second phrase, \texttt{Mika} is correctly narrowed down to the top 3 potential related entities. However, subword predictions do not consistently agree on the most probable prediction. In such cases, we calculate the average predicted probability for each entity across subwords and select the entity with the highest average probability.}
    \label{fig:spel}
    \vspace{-0.5cm}
\end{figure}

\noindent Entity linking can be viewed as three interlinked tasks \citep{K19-1063,2020.findings-emnlp.71, REL_EL}:
\begin{enumerate}
    \vspace{-0.5em}
    \itemsep0em 
    \item[(1)] \textit{Mention Detection} \cite{NER_SURVEY} to scan the raw text and identify the potential spans that may contain entity links.
    \item[(2)] \textit{Candidate Generation} (e.g. \citealp{fang2020high}) to match each potential span with a number of potential entity records in the knowledge base.
    \item[(3)] \textit{Mention Disambiguation} \cite{P11-1138, 2022.naacl-main.238} to select one of the potential entity records for each detected mention.
    \vspace{-0.5cm}
\end{enumerate}



\noindent An end-to-end entity linking system does all three tasks and links text spans to concepts. The system can either have independently modelled components \citep{TagmeToWAT, REL_EL} or jointly modelled components \citep{K18-1050,2021.emnlp-main.604}. 

Similar to almost all NLP tasks, recent entity linking models use pre-trained representation learning methods that are based on Transformers \cite{Transformer}. These methods commonly utilize bidirectional language models like BERT \cite{N19-1423} or auto-regressive causal language models such as GPT \cite{GPT3} or BART \cite{BART}, which are then fine-tuned on specific entity linking training datasets. In a number of such techniques, entity linking is framed as another well-studied problem, such as sequence-to-sequence translation \cite{GENRE} or question answering \cite{EntQA}.

Entity linking can be viewed as sequence tagging using structured prediction\footnote{Structured prediction has shown successful enhancements in various NLP tasks, including Dependency Parsing \cite{P15-1117}, Question Answering \cite{2020.emnlp-main.248}, and Machine Translation \cite{2021.eacl-main.241}.}, aiming to assign one of the finitely many classes to every input utterance. 
This approach involves using a pre-trained model to encode each input subword token into a multi-layer context-aware dense vector representation. A classifier head calibrates each token representation to predict the entity for each subword token.
Due to the large number of possible entities and issues with consistency of entity prediction across multiple subword tokens, structured prediction for entity linking (surprisingly) has not been studied in-depth. Our contributions in this paper are as follows:

\begin{enumerate}
    \vspace{-0.8em}
    \itemsep0em 
    \item[(1)] A new structured prediction framework for entity linking called \textsc{SpEL} (Figure \ref{fig:spel}). We demonstrate that \textsc{SpEL} establishes a new state-of-the-art for Wikipedia entity linking on the commonly used AIDA~\cite{D11-1072} dataset.
    \item[(2)] Two separate and refined fine-tuning steps. One for general knowledge about Wikipedia concepts and one for specifically tuning on the AIDA dataset.
    \item[(3)] A \textit{context sensitive} prediction aggregation strategy that enables subword-level token classification while enforcing word-level and span-level prediction coherence. This does not incur additional inference time but drastically enhances the quality of the predicted annotation spans.
    \item[(4)] We use the in-domain mention vocabulary to create a \textit{fixed candidate set}. We use this to improve the efficiency and accuracy of entity linking.
    \item[(5)] Addressing the training/inference tokenization mismatch challenge in previous works which arises when differences in tokenization between training and testing phases lead to discrepancies. To address this challenge, we introduce an additional fine-tuning step where the model is fine-tuned on tokenized sequences without explicit mention location information. This encourages the model to learn robust representations that are not dependent on specific tokenization patterns, improving its generalization to the mention-agnostic tokenization at inference.
    \item[(6)] New pre-training and inference ideas that can achieve a new state of the art with much better compute efficiency (fewer parameters) and much faster inference speed than previous methods.
    \item[(7)] We have annotated and released \texttt{AIDA/testc}, a new entity linking test set for the AIDA dataset.
    \item[(8)] Simplify and speed up the evaluation process of entity linking systems using the GERBIL platform \cite{GERBIL} by providing a Python equivalent of its required Java middleware.
    \vspace{-0.4em}
\end{enumerate}

We introduce recent entity linking methods in Section \ref{sec:lit_rev}; explain our approach to structured prediction for entity linking and our \textit{context sensitive} prediction aggregation strategy in Section \ref{sec:entity_linking} with comprehensive experiments in Section \ref{sec:experiments}.

\vspace{-0.1cm}
\section{Related Work}\label{sec:lit_rev}

Entity linking can be framed as another well-studied task and the best solution for that task is applied. Autoregressive encoder-decoder sequence-to-sequence translation is one such approach. \citet{GENRE} consider the input text as the source for translation and the text is annotated with Wikimedia markup containing the mention spans and the entity for each mention. Instead of mapping the entity identifiers into a single id (this is the default in many techniques including this work because the approach can be easily ported to ontologies other than Wikipedia), their model generates the entity label in a token-by-token basis (it generates the Wikipedia URL one subword at a time). The generation process follows a constrained decoding schema that prevents the model from producing invalid entity URLs while limiting the generated output to the entities in a predefined candidate set (discussed in Section \ref{sec:candidateset}). 

\citet{2021.emnlp-main.604} use a BERT-style bidirectional model fine-tuned to identify potential spans (mention detection) by learning spans using a \textit{begin} probability and an \textit{end} probability for each subword in the input text. For each potential span, they use a generative LSTM-based \cite{LSTM} language model to generate the entity identifiers (token-by-token), and limit the generation process to pre-defined candidate sets. 

\citet{2022.findings-acl.156} frame entity linking as a sequence-to-sequence translation task using BART \cite{BART}. They duplicate the BART decoder three times to fine-tune the model in a multi-task setting. The two additional decoder modules are trained using auxiliary objectives of mention detection and re-ranking. While this training method increases the model size during training, they mitigate increased model size and speed at inference time by excluding the auxiliary decoder modules and employing sampling and re-ranking techniques on the generated target sequences.

\citet{EntQA} use Question Answering as a way to frame the entity linking task. They suggest a two-step entity linking model in which they use a fine-tuned Transformer-based BLINK \citep{2020.emnlp-main.519} model to find all the potential entity records that might exist in the text and then utilize a fine-tuned question answering ELECTRA \cite{ELECTRA} model to identify the matching occurrences of the potential entities discovered in the first step. This approach obtains high accuracy; however, it is very resource intensive and inference is slow.

Structured prediction (subword token multi-label classification) is the other well-studied problem. 
\citet{K19-1063} proposes a very simple entity linking model which places a classification head on top of a BERT language model and directly classifies each subword representation using a softmax over all the entities known to the model. 

Our work also uses structured prediction because it is one of the most lightweight techniques in terms of compute cost and inference speed. We extend the structured prediction model in this work by (1) utilizing a \textit{context sensitive} prediction aggregation strategy to form meaningful span annotations (Section \ref{sec:sp_for_el}), (2) addressing a training/inference tokenization mismatch issue (Section \ref{sec:finetuning}), (3) examining the role of different types of candidate sets (Section \ref{sec:candidateset}) in curating the predicted results, and (4) optimizing the implementation of the structured prediction model. We obtain more than 6.1\% points Micro-F1 improvement as well as more than 10 times reduction in required disk space, and close to 4 times reduction in the required GPU memory (in \texttt{base} case) compared to \citet{K19-1063}. Our approach is also faster at inference time than previous Transformer-based methods.

A number of recent techniques focus on enhancing the entity linking knowledge in BERT (or one of its variations) and utilize one or more such \textit{knowledge-enhanced} models to perform the task of entity linking. \citet{D19-1005} inject Wikipedia and Wordnet information into the last few layers of BERT, \citet{2020.findings-emnlp.71} inject pre-trained Wikipedia2Vec \cite{Wikipedia2Vec} entity embeddings into the input layer of the language model while freezing the rest of its parameters, and \citet{P19-2026} leverage a Stack-LSTM \cite{P15-1033} Named Entity Recognition model to enhance entity linking performance using multi-task learning to improve entity linking. These approaches are Wikipedia-centric and while we also experiment on Wikipedia exclusively, our approach can be used on any entity-linking dataset that has entities from other ontologies such as the MedMentions dataset \cite{MedMentions} which links to concepts in UMLS ontology \cite{UMLS}.

\citet{K18-1050} jointly model mention detection and mention disambiguation using an LSTM-based architecture while reusing the candidate sets created by \citet{D17-1277} as a replacement for the candidate generation step, and \citet{CHOLAN} follow a similar framework while modeling each of mention detection and mention disambiguation using separate BERT models. \citet{2022.findings-aacl.1} compute entity embeddings (instead of using pre-trained ones) using the average of the subword embeddings of the candidates and compare them to the average of the subword embeddings for the potential span (training a Siamese network; \citealp{SiameseNetwork}). \citet{AKBC2020} investigate pre-training strategies specifically tailored for Transformer models to perform entity linking, diverging from the conventional use of pre-trained BERT models. And, \citet{REL_EL} propose a modular configuration that composes mention detection, candidate generation, and mention disambiguation in a pipeline approach, incorporating the most promising components from prior research. 

In Section \ref{sec:experiments_and_results}, we conduct experiments to compare \textsc{SpEL} to the methods discussed in this section.


\section{Entity Linking}\label{sec:entity_linking}
Formally, entity linking receives a passage () containing words \{, ..., \} and produces a list containing  span annotations. Each span annotation is expected to be a triplet of the form (\textit{span start}, \textit{span end}, \textit{entity identifier}).
The \textit{span start} and \textit{span end} values are expected to be character positions on the raw input , and the \textit{entity identifier} values are selected from a predefined vocabulary of entities (e.g. there would be approximately 6 million entities to choose from when entity linking to Wikipedia). The massive entity vocabulary size increases the model's hardware requirements and in some cases renders the task intractable. 

\subsection{Candidate Sets}\label{sec:candidateset}
To solve the entity vocabulary size problem, a common approach is to limit to  most frequent entities in the knowledge base\footnote{For Wikipedia, we can define an entity frequency as the number of times a title is hyperlinked in the other pages.}. This vocabulary can be simply considered as the \textit{fixed candidate set} for linking each mention to the knowledge base. Where no more information is available, the model will have to choose one entity from this \textit{fixed candidate set}. 

The selected \textit{fixed candidate set} may lack many of the expected entity annotations at inference. Consequently, even if the model is highly capable, it may perform poorly during inference due to its inability to suggest the expected entities. Recognizing this challenge, there is a consensus among existing literature \cite{K18-1050, K19-1063, D19-1005, 2020.findings-emnlp.71} to augment the \textit{fixed candidate set} by including the expected entities necessary for inference. We adopt a similar approach to create the \textit{fixed candidate set}, following the same line of reasoning as previous studies.
Nonetheless, it's important to underscore that our adherence to this approach is driven by the desire for consistency with prior research; our framework, however, does not necessitate this specific method for effective functioning. In practical scenarios, one straightforward approach to construct the \textit{fixed candidate set} is to base it on the anticipated entities (any subset of knowledge base entities that are pertinent to the task at hand) to be detected by \textsc{SpEL}.

An alternative is to use \textit{mention-specific candidate sets} \cite{K18-1050, D19-1005, CHOLAN, GENRE, 2021.emnlp-main.604}. \textit{Mention-specific candidate sets} can be divided into two groups:
\begin{enumerate}
    \vspace{-0.5em}
    \itemsep0em 
    \item[(1)] \textit{context-agnostic mention-specific sets} which are usually generated over large amounts of annotated text and try to model the probability of each mention span to all possible entity identifiers without assuming a specific context in which the mention would appear. KB+Yago\footnote{\href{https://github.com/yifding/deep_ed_PyTorch}{https://github.com/yifding/deep\_ed\_PyTorch}} \cite{D17-1277} contains candidate lists for approximately 200K mentions created over the entire English Wikipedia combined with the Yago dictionary of \cite{D11-1072}.
    \item[(2)] \textit{context-aware mention-specific sets} can be constructed if there is a method for identifying mentions and a set of candidates for those mentions. For example, \citet{N15-1026} have built such candidate sets, called PPRforNED\footnote{\href{https://github.com/masha-p/PPRforNED}{https://github.com/masha-p/PPRforNED}}. Such lists have been primarily suggested for the task of entity disambiguation where the mention is provided in the input. As gold mentions are not available for real-world use cases of entity linking, this type of candidate sets have fallen out of favor.
    \vspace{-0.2em}
\end{enumerate}

\textit{Mention-specific} candidate sets consist of many entity identifiers and the correct entity identifier is not guaranteed to exist given the mention span. 

\subsection{Structured Prediction for Entity Linking}\label{sec:sp_for_el}
For a sequence of subwords \footnote{When feeding a long text in training and inference, we split the text into smaller overlapping chunks.}, we employ RoBERTa \citep{RoBERTa}, in both \texttt{base} and \texttt{large} sizes, as our underlying model  to encode  into  where  is the hidden representation dimension of . Each representation ,  is then transformed into a distribution over the \textit{fixed candidate set} of size  using a transformation matrix . This results in , where  represents the probability distribution for the  subword in the input sequence.

When we set up fine-tuning for this task, we use hard negative mining \citep{K19-1049} to find the most probable incorrect predictions in the batch\footnote{We add random negative examples in addition to hard negatives to make sure we get to 5K negative examples for each batch when fine-tuning on CoNLL/AIDA and 10K negatives for general fine-tuning.}. In each fine-tuning step, we update the network based on the subword classification probabilities of the hard negative examples as well as the expected prediction. To increase inference speed, the classification head does not normalize the predicted scores. 

We employ binary cross-entropy with logits, Equation (\ref{eq:fine-tune_loss}), as our loss function, which is calculated over many factors. Let  represent the total number of selected examples () comprising the one positive example corresponding to the expected prediction and the negative examples. Additionally,  takes a value of 1 when the  member of  correctly points to the entity identifier for subword ,  denotes model's predicted score for linking the  member of the selected examples to the  subword, and  is the sigmoid function, which maps the scores to .



During inference we collect the top  predictions for each subword  based on the predicted probabilities in . We then collect subwords that belong to the same word into a single group, which we call the \textit{word annotation}. For each word annotation, we generate an aggregated entity identifier prediction set by taking the union of the entity identifiers predicted for the subwords. We then compute the weighted average of the prediction probabilities for each entity identifier to obtain the word-level probability score over entities.

Consecutive word-level entity labels when they refer to the same concept are joined into a single mention span over that phrase. 

When a \textit{mention-specific} candidate set is available, and the mention surface form matches one of the mentions in the candidate set, we filter out any predictions from the phrase annotation that are not present in the candidate set, regardless of their probability\footnote{The presence of a \textit{mention-specific} candidate set is \textit{not} a prerequisite for this technique to be effective.}. The final prediction for an entity span is generated based on the most probable prediction in the phrase annotations, excluding the ones annotated with \texttt{O} (which means the phrase is not an entity). As an additional post-processing cleanup step, we reject phrase annotations that span over a single punctuation subword (e.g. a single period or comma) or a single function (sub)word (e.g. \texttt{and}, \texttt{by}, ...). In such cases, we manually override the model's prediction to \texttt{O}. 

This \textit{context sensitive} prediction aggregation strategy leads to improved performance and enhances prediction results in inference. Our strategy ensures that annotation spans do not begin or end inside a word\footnote{For instance, in the word \texttt{U.S.}, if in the \texttt{U.S} part, the subwords have high likelihood for the concept \texttt{The United States} and the ending \texttt{.} refers to an \texttt{O}, the conflict is resolved so that the entire word \texttt{U.S.} is linked to \texttt{The United States}.}, and the conflicts between the subword predictions within a word are resolved by the average prediction probability for each entity identifier. 

This simpler method to ensure label consistency does better than using a CRF layer \cite{CRF}. 
Although our experiments show that a CRF layer does not improve our model, our readers can think of the suggested strategy as a domain-tailored, non-parametric, and rule-driven version of a CRF layer which guides the model to unify the predicted subword-level entity predictions considering the local context. 
Based on our experiments (Table \ref{tab:spel_mention_detection}), although we do not explicitly model Mention Detection (as predicting the \textit{span start} and \textit{span end} probability scores or separate \texttt{BIO} tags) for each subword in inference time, we observe a high in-domain accuracy in distinguishing \texttt{O} spans from non-\texttt{O} spans in predictions as a result of using the \textit{context sensitive} prediction aggregation strategy.

Our modelling framework, \textsc{SpEL}, stands for \textit{Structured Prediction for Entity Linking}.

\subsection{Fine-tuning Procedure}\label{sec:finetuning}
\citet{2021.eacl-main.153} argue that pre-trained language models can produce better representations when they are first fine-tuned on a much larger entity-linked training data (almost like a further pre-training step) and then subsequently fine-tuned for the entity-linking task. We perform such a multi-step fine-tuning procedure: first fine-tuning on a large dataset encompassing general knowledge on the set of linked concepts and then fine-tuning on an in-domain dataset specific to the target domain over which we aim to perform entity linking.


\textbf{General knowledge fine-tuning.} In the first step, we fine-tune the pre-trained language model using text that includes links to the knowledge base (in our experiments, we use a large subset of English Wikipedia\footnote{Limited to the articles that contain some presence of the entities in our selected \textit{fixed candidate set}.}). As mentioned in \cite{K19-1063}, it helps if this data is aware of the mentions (using special space character subwords before and after each span that is linked to an entity). This helps the model learn the identification of the starting and ending subwords in entity mention spans. However, this imposes a mismatch in the distributions of the data in fine-tuning compared to inference, where the model does not have access to the entity mentions to perform the customized tokenization. To address this issue, as a subsequent fine-tuning step, we iterate again through the large entity-linked dataset which is re-tokenized \textit{without} the knowledge of the mention spans. 

\textbf{Domain specific fine-tuning.} In the third and last fine-tuning step, we refocus the model's attention to the in-domain dataset annotated with a \textit{fixed candidate set} which usually is a subset of all the knowledge base entities that the model has observed in the previous two fine-tuning steps. Similar to the second fine-tuning step, we tokenize the in-domain dataset \textit{without} the knowledge of the mention spans. 

\section{Experiments}\label{sec:experiments}

\subsection{Data}
For our experiments, we focus on Wikipedia as the knowledge base and we use the following datasets for the fine-tuning steps mentioned in Section \ref{sec:finetuning}.

\textbf{Wikipedia} we use the 20230820 dump of Wikipedia (with approximately 238K documents), and we use the script from \cite{K19-1063} to handle incomplete annotations, perform mention-aware customized tokenization, and compute the average probability of linking to no entity (called the \textit{Nil} probability) for the 1000 most frequent entities. The \textit{Nil} probability is used to modify the Wikipedia training data annotations in such a way that the chance of linking a surface form referring to a frequent entity to \texttt{O} is almost 0. We construct the Wikipedia \textit{fixed candidate set} using the union of the 500K most frequent mentions in the Wikipedia dump and the \textit{fixed candidate set} of AIDA and the test datasets. We split the content of Wikipedia pages into chunks consisting of 254 subwords with a 20 subword overlap between consecutive chunks. After the split, our dataset contains 3,055,221 training instances with 1000 instances for validation. 
We also create a mention-agnostic re-tokenized version of this dataset with the same exact mentions to perform the second step of general knowledge fine-tuning as explained above.

\textbf{AIDA} \cite{D11-1072} contains manual Wikipedia annotations for the 1393 Reuters news stories originally published for the CoNLL-2003 Named Entity Recognition Shared Task \cite{CoNLL2003}. 
Its \texttt{train}, \texttt{testa}, and \texttt{testb} splits contain 946, 216, and 231 documents, respectively.
It has a \textit{fixed candidate set} size of 5600 (including \texttt{O} tag) and for evaluation on the AIDA test sets, we shrink the classification head in the model to these 5600 candidates and disregard the rest of the 500K candidates\footnote{Another implementation idea can revolve around multiplying the predicted output vector into a mask vector that masks all the candidates not in the expected 5600 entities.}.


\subsection{Evaluation using GERBIL}
The GERBIL platform \cite{GERBIL} is an evaluation toolkit (publicly available online) that eliminates any mistakes and allows for a fair comparison between methods.
However, GERBIL is a Java toolkit, while most of modern entity linking work is done in Python. GERBIL developers recommend using \texttt{SpotWrapNifWS4Test} (a middleware tool written in Java) to connect Python entity linkers to GERBIL. Because of the complexity of this setup, we have not been able to directly evaluate some of the earlier publications due to software version mismatches and communication errors between Python and Java. This is a drawback that discourages researchers from using GERBIL. 
To address this issue, we provide a Python equivalent of \texttt{SpotWrapNifWS4Test} to encourage entity linking researchers to use GERBIL for fair repeatable comparisons.
We evaluate all \textsc{SpEL} models using GERBIL in the \texttt{A2KB} experiment type, and report InKB strong annotation matching scores for entity linking. Only five of the publications to which we compare use GERBIL, however, all report InKB strong Micro-F1 scores allowing a direct comparison to our work.

\subsection{Experimental Setup}

For the first general knowledge fine-tuning step (Section \ref{sec:finetuning}), as a warm-up to full fine-tuning, we freeze the entire \textsc{RoBERTa} model and only modify the classification head parameters on top of the encoder. We fine-tune with this configuration for 3 epochs and subsequently continue with fine-tuning all model parameters. We stop the fine-tuning process in this phase when the subword-level entity linking F1 score on the validation set shows no improvement for 2 consecutive epochs. Following this, we proceed to the second phase of full fine-tuning, where we fine-tune all model parameters using the mention-agnostic re-tokenized Wikipedia fine-tuning data. Just like phase one, we stop this phase based on the same criteria. We implement \textsc{SpEL} using \texttt{pytorch}, utilize \texttt{Adam} optimizer with a learning rate of  to fine-tune the encoder parameters, and use \texttt{SparseAdam} optimizer with a learning rate of  to fine-tune the classification head. We run fine-tuning phases one and two on the large subset of Wikipedia using two Nvidia Titan RTX GPUs.

For the last phase of fine-tuning on the AIDA dataset (Section \ref{sec:finetuning}), we freeze the first four layers of the encoder (including the embedding layer) as well as the shrunk classification head parameters, and we fine-tune the rest of the model parameters for 30 epochs (over the \texttt{train} part of AIDA dataset). We run this step using one Nvidia 1060 with 6 GBs of GPU memory, and accumulate gradients \cite{W18-6301} for 4 batches before updating model parameters. We perform redirect normalization\footnote{See Appendix \ref{sec:appendix_redirect_normalization} for more on this standard normalization technique.} on the final predicted spans.

\begin{table*}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{l|c|c|r|r}
\multicolumn{1}{c|}{\multirow{2}{*}{Approach}} & \multicolumn{2}{c|}{EL Micro-F1} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\#params\\on GPU\end{tabular}}} &  \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}speed\\ sec/doc\end{tabular}}} \\ \cline{2-3}
 \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\texttt{testa}} & \multicolumn{1}{c|}{\texttt{testb}}  & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{} \\ \hline\hline
 \citet{D11-1072}\hfill(Linear)                     & 72.4 & 72.8 & -      & - \\
 \citet{K18-1050}\hfill(LSTM)                       & 89.4 & 82.4 & 330.7M & 0.097 \\ \citet{K19-1063}\hfill(BERT)                       & 86.0  & 79.3  & 495.1M & 0.613 \\
 \citet{D19-1005}\hfill(BERT)                       & 82.1 & 73.7  &  -      & - \\ \citet{P19-2026}\hfill(Stack-LSTM)                 & 85.2 & 81.9  & - &  - \\
\citet{REL_EL}\hfill(LSTM)                         & 83.3 & 82.4 & 19.0M  & 0.337 \\ \citet{AKBC2020}\hfill(Transformer)                & 79.7 & 76.7 & -      & - \\ 
 \citet{2020.findings-emnlp.71}\hfill(BERT)         & 90.8 & 85.0  &  131.1M & - \\ \citet{CHOLAN}\hfill(BERT)                         & -    & 83.1 & -      & - \\
 \citet{GENRE}\hfill(BART)                          & -    & 83.7 & 406.3M & 40.969 \\ 
 \citet{2021.emnlp-main.604}\hfill(RoBERTa+LSTM) &      &       &       &        \\
 \ \ \ \ \ \ \ (no mention-specific candidate set)  & 61.9 & 49.4 & 124.8M & 0.268  \\  \ \ \ \ \ \ \ (using PPRforNED candidate set)      & 90.1 & 85.5  & 124.8M & 0.194 \\ \citet{2022.findings-acl.156}\hfill(BART)          & -    & 85.7  & \begin{tabular}[r]{@{}r@{}}(train) 811.5M\test) 406.2M\end{tabular} & -\\ \citet{EntQA}\hfill(BLINK+ELECTRA)                 & -    & 85.8  & 1004.3M & - \\ \citet{2022.findings-aacl.1}\hfill(BERT)           & -    & 86.3  &  157.3M & -\\ \hline
 \textsc{SpEL}-base (no mention-specific candidate set)              & 91.3 & 85.5 & 128.9M & 0.084  \\ \textsc{SpEL}-base (KB+Yago candidate set)       & 90.6 & 85.7 & 128.9M & 0.158 \\ \textsc{SpEL}-base (PPRforNED candidate set)  &      &       &       &        \\
 \ \ \ \ \ \ \ context-agnostic     & 91.7 & 86.8 & 128.9M & 0.156  \\ \ \ \ \ \ \ \ context-aware        & 92.7 & 88.1 & 128.9M & 0.156 \\ \hline 
\textsc{SpEL}-large (no mention-specific candidate set)              & 91.6 & 85.8 & 361.1M & 0.273  \\ \textsc{SpEL}-large (KB+Yago candidate set)       & 90.8 & 85.7 & 361.1M & 0.267 \\ \textsc{SpEL}-large (PPRforNED candidate set)  &      &       &       &        \\
 \ \ \ \ \ \ \ context-agnostic     & 92.0 & 87.3 & 361.1M & 0.268  \\ \ \ \ \ \ \ \ context-aware        & \textbf{92.9} & \textbf{88.6} & 361.1M & 0.267 \\ \hline\hline
\end{tabular}
}
\caption{Entity Linking evaluation results of \textsc{SpEL} compared to that of the literature over AIDA test sets.\\\textit{\#params on GPU} only considers the total number of parameters that will directly effect the cost of GPU acquisition and does not reflect upon the total amount of data loaded into/from main memory and disk.}
\label{tab:in_domain_results}
\vspace{-0.4cm}
\end{table*} 
\begin{table*}
\centering
\scalebox{0.9}{
\begin{tabular}{l|ccc|ccc}
\multicolumn{1}{c|}{\multirow{3}{*}{Approach}} & \multicolumn{6}{c}{MD Micro Scores} \\ \cline{2-7} 
                                    & \multicolumn{3}{c|}{\texttt{testa}} & \multicolumn{3}{c}{\texttt{testb}} \\ \cline{2-7} 
                                    & P & R & F1                           & P & R & F1 \\ \hline
\begin{tabular}[l]{@{}l@{}}\citet{2021.emnlp-main.604}\\\ \ \ \ \ \ \ \ \ \ \ (using PPRforNED c. set)\end{tabular} & 93.9 & 96.7 & 95.2                   & 92.2 & 94.8 & 93.5 \\\hline
 \textsc{SpEL}-base (no mention-specific c. set)          & 94.6 & 94.4 & 94.5                   & 92.5 & 90.1 & 91.2 \\ 
\textsc{SpEL}-base (using PPRforNED c. set - context-agnostic)      & 98.3 & 91.6 & 94.8                   & 98.3 & 86.4 & 92.0 \\
 \textsc{SpEL}-base (using PPRforNED c. set - context-aware)        & 99.4 & 90.9 & 95.0                   & 99.4 & 84.9 & 91.6 \\ \hline\hline
\end{tabular}
}
\caption{Mention Detection evaluation results of \textsc{SpEL} in comparison to the work of \citet{2021.emnlp-main.604} using their released evaluation code (from \href{https://github.com/nicola-decao/efficient-autoregressive-EL/blob/master/src/utils.py}{utils.py}). As \citet{2021.emnlp-main.604} use PPRforNED candidate sets, we only compare the \textsc{SpEL} results calculated using these candidate sets.}
\label{tab:spel_mention_detection}
\vspace{-0.4cm}
\end{table*}

\subsection{Experiments and Results}\label{sec:experiments_and_results}
In this section, we conduct experiments to evaluate the performance of both \textsc{SpEL}-base and \textsc{SpEL}-large (referring to the size of the underlying \textsc{RoBERTa} model) in different configurations concerning the use of candidate sets (Section \ref{sec:candidateset}), and report our experimental results over the AIDA test datasets in Table \ref{tab:in_domain_results}.

In the first configuration, we examine our model without any \textit{mention-specific} candidate sets. Our results show a minimum of 5.3 Micro-F1 score improvement in AIDA test sets compared to \cite{K19-1063} while significantly reducing the required parameter size on GPU by fourfold, resulting in a 7.2 times increase in inference speed in \texttt{base} case.

Next, we run \textsc{SpEL} in three other configurations: (1) utilizing the KB+Yago \cite{D17-1277} context-agnostic candidate set, (2) employing the PPRforNED \cite{N15-1026} context-aware candidate set, and (3) adapting PPRforNED to aggregate the candidate information for each mention surface form, resulting in a context-agnostic candidate set, excluding context-specific information.

Candidate sets help reject many over-generated spans. If a mention's candidate set is empty, the mention span is excluded from further consideration. While this approach typically leads to improved precision and subsequent enhancement in F1 score, instances may arise where the model correctly predicts mentions that are not encompassed within the candidate sets. This can lead to lower recall in the evaluation. The observed Micro-F1 score drop when employing KB+Yago candidate sets compared to the scenario where no \textit{mention-specific} candidate set is utilized, can be attributed to these cases.

\textsc{SpEL}-large using context-aware candidate sets achieves the highest boost, reporting 2.1 and 2.3 Micro-F1 scores improvement over \texttt{testa} and \texttt{testb} sets of AIDA, respectively, and establishes a new state-of-the-art for AIDA dataset. It is noteworthy to consider that the proposed model by \citet{EntQA} demands significant computational resources, including tens of gigabytes of RAM and over 7 and 2.7 times the number of parameters on GPU compared to \textsc{SpEL}-base and \textsc{SpEL}-large, respectively. Despite these resource-intensive requirements, \textsc{SpEL} outperforms \citet{EntQA}. The comparison between our results and that of \citet{2021.emnlp-main.604, GENRE} demonstrates that generating entity descriptions (which can share lexical information with the mention text) is not necessary even for high accuracy Wikipedia entity linking. Our approach can be easily extended to ontologies without textual concept descriptions, while methods that generate entity descriptions cannot.

Lastly, in Table \ref{tab:spel_mention_detection}, we compare \textsc{SpEL}-base, which utilizes the \textit{context sensitive} prediction aggregation strategy to convert subword-level predicted entity identifiers into span-level predictions, to the model proposed by \citet{2021.emnlp-main.604}. The latter model explicitly models the start and end positions of the spans for mention detection. We employ the evaluation script released by \citet{2021.emnlp-main.604} for our assessment. The results confirm that, despite not using \texttt{BIO} tags or explicitly modeling span boundaries, \textsc{SpEL} demonstrates strong performance in mention detection, with a high level of accuracy. Its near-perfect precision scores indicate its ability to minimize over-generated predictions, contributing to its state-of-the-art entity linking performance.

\begin{table}
\centering
    \addtolength{\tabcolsep}{-0.3em}
    \scalebox{0.9}{
    \begin{tabular}{l|c|c|r}
        \multicolumn{1}{c|}{\multirow{2}{*}{Approach}}      & \multicolumn{2}{c|}{EL Micro-F1}    & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}USu, v)uvuv$.

\section{More on Comparison to OpenAI GPT Models}\label{sec:appendix_gpt}


In this section, we provide more information on our experimental procedure which can help replicating our results.

When utilizing a pre-trained GPT language model, it is common practice to structure the task description as a prompt, which is then passed to the model. The model leverages its linguistic understanding to generate a solution in the form of a completion, based on the given prompt. However, a crucial limitation arises in the process of identifying the appropriate prompt, as its selection greatly influences the successful completion of the task. 


\subsection{Zero-shot experiments}
Our best performing prompt was as follows:\\\texttt{You are a Wikipedia annotator. Annotate the Wikipedia entities in the following paragraph, and produce the output in markup using the <mark> element and the data-entity attribute:}\\
In each query\footnote{\citet{2022.emnlp-main.638} employ GPT for entity linking by implementing a process that involves a sequence of summarization and multiple-choice queries to GPT. However, we have found this approach to be rather costly. Furthermore, it necessitates prior knowledge of the target mention to condition the summary accordingly. Additionally, it relies on a set of candidates generated through heuristics which undermines the feasibility of utilizing GPT for end-to-end entity linking.}, in the line after the prompt, we add the AIDA document received from GERBIL and pass it to GPT using \texttt{openai.ChatCompletion.create} API\footnote{We used python's \texttt{openai} package version "0.27.6" and we set \texttt{temperature=0}, \texttt{top\_p=1}, \texttt{frequency\_p enalty=0.}, \texttt{presence\_penalty=0.}}. 
In cases where the documents exceed the maximum subword limit of GPT, we employ \texttt{spacy} python library, to divide the document, and create query prompts consisting of approximately 1000 tokens each. Subsequently, we concatenate the received responses to these queries to form a comprehensive result. 
Following this, we parse the generated markup and associate each annotation with its corresponding segment in the original text. We proceed by forwarding the predicted annotations that match entries in the Wikipedia \textit{fixed candidate set}, along with the extracted spans, back to GERBIL.

During the experiments, we analyzed the validation set results and observed consistent patterns that shed light on the challenges posed by generative language models in entity linking. One notable observation was the presence of annotations from a mixture of knowledge bases and domains, indicating that the model possesses an excessive amount of knowledge, leading to \textit{distractions} in the annotation process focused on entity linking over Wikipedia. With this regards, we observed a lack of stability in the model's output even when setting the \texttt{temperature} parameter to 0. Despite using the same prompt, the model occasionally confused entity linking (EL) with named entity recognition (NER) and reported mentions annotated with NER tags such as \texttt{Person} or \texttt{Location}. In our experiments, we removed all predicted spans with such tags and did not relay them back to GERBIL.

Furthermore, due to the nature of generative models, there were instances where the model failed to generate the complete entity, resulting in incomplete predictions (for example it generated \texttt{Leicestershire} instead of the full entity identifier \texttt{Leicestershire County Cricket Club} or \texttt{Pohang} instead of \texttt{Pohang Steelers}). In these instances, if an exact match to an entity in the knowledge base was not found, we collected all entities in the \textit{fixed candidate set} that included the full prediction from the generative language model. From this collection\footnote{In the majority of cases, the candidate set contains only one element if it is not empty.}, we randomly selected one of those mentions and reported it back to GERBIL instead of the original prediction generated by the model. 

\subsection{Few-shot experiments}
In light of the growing popularity of few-shot prompting techniques with GPT language models, we conducted a survey of some of the leading approaches for experimenting with few-shot examples. Given the promising outcomes associated with the chain-of-thought (CoT; \citealp{CoT}) prompting technique, we chose to conduct our few-shot experiments using this particular method. To construct our best performing few-shot CoT prompt, we retained our zero-shot prompt and added the following lines as an extension:\\\texttt{Document: "EU rejects German call to boycott British lamb."\\Answer: <p> <chain-of-thought> Considering EU, German, and British are shown in the text together with the word boycott, this is a political document, I should annotate EU with "European Union",  German with the country "Germany", and British with the country "United Kingdom". I make sure I do not mistake Wikipedia identifiers with entity type identifiers, for example I choose "United Kingdom" instead of the incorrect general entity type "country". I make sure to annotate all entities even if there is a large number of entities. </chain-of-thought> <result> <mark data-entity="European Union"> EU </mark> rejects <mark data-entity="Germany"> German </mark> call to boycott <mark data-entity="United Kingdom"> British</mark> lamb.</result></p>}

\noindent Adding more examples in the same format did not significantly improve performance, but it substantially increased the prompting cost to GPT-4. 

We maintained the rest of the configurations and setups for the few-shot experiments the same as we had in the zero-shot experiments.

\begin{table*}
    \centering
    \scalebox{0.9}{
    \begin{tabular}{l||c|c|c|c|c|c|c}
        \multicolumn{1}{c||}{Approach}                & MSNBC & Derczynski & KORE & N\textsuperscript{3} Reuters & N\textsuperscript{3} RSS & OKE2015 & OKE2016 \\ \hline\hline
        \citet{D11-1072}\textsuperscript{\textdagger} & 65.1  & 32.6 & 55.4 & 46.4 & \underline{42.4} & \underline{63.1} &  0.0 \\
        \citet{K18-1050}                              & 72.4  & 34.1 & 35.2 & \underline{50.3} & 38.2 & 61.9 & 52.7 \\ \citet{REL_EL}                                & \textbf{74.4}  & 41.2 & \underline{61.6} & 49.7 & 34.3 & \textbf{64.8} & \textbf{58.8} \\ \citet{GENRE}                                 & \underline{73.7}  & \underline{54.1} & 60.7 & 46.7 & 40.3 & 56.1 & 50.0 \\
        \citet{EntQA}                                 & 72.1  & 52.9 & \textbf{64.5} & \textbf{54.1} & 41.9 & 61.1 & 51.3 \\\hline
        \textsc{SpEL}-base                            & 64.5  & 50.7 & 48.7 & 47.9 & 41.9 & 55.9 & \underline{57.4} \\
        \textsc{SpEL}-large                           & 63.1  & \textbf{59.1} & 53.7 & 47.1 & \textbf{44.4} & 59.5 & 56.6 \\
        \hline Oracle\textsuperscript{\ddag}          & 93.2  & 91.4 & 99.6 & 99.7 & 98.0 & 88.2 & 91.4 \\ 
        \hline\hline
    \end{tabular}
    }
    \caption{Comparison of \textsc{SpEL} (with a \textit{fixed candidate set} size of 500k) evaluation results with the literature on out-of-domain datasets. The best score is shown as \textbf{bold} and the second best is shown as \underline{underlined}.\\ \textdagger Results from (\citealp{K18-1050} - Table 2).\\ \ddag The ``Oracle'' results are calculated through feeding the gold annotations of each dataset to GERBIL, and depicts the In-KB annotation quality of each dataset.}
    \label{tab:out_of_domain_experimental_results}
\end{table*}

\section{Out-of-domain Experiments and Results}

Few of the publications listed in Table \ref{tab:in_domain_results} recommend assessing entity linking models on \textit{out-of-domain} testing datasets. These datasets typically lack associated training sets and are often annotated with entity links to variations or subsets of the \texttt{DBpedia} \cite{Dbpedia} knowledge base. Out-of-domain annotation typically operates under the assumption that the knowledge base entry identifiers remain consistent between in-domain and out-of-domain scenarios. While this assumption may hold true to a certain extent, as DBpedia's primary focus has been on information extraction from Wikipedia, it's important to note that the temporal evolution of both knowledge bases has introduced discrepancies. These datasets, which are between 8 to 16 years old at the time of writing this paper, have been affected by temporal changes, and the two knowledge bases are not always perfectly aligned.
The following offers a concise overview of some of the most commonly utilized out-of-domain datasets for evaluation:

\textbf{MSNBC} \cite{MSNBCDataset} contains 20 MSNBC news stories (annotated with Wikipedia) from different categories including Health, Technology, Sports, etc. 

\textbf{KORE} \cite{KORE50} contains 50 sentences annotated with DBpedia and chosen from five domains: celebrities, music, business, sports, and politics. It was created to examine the disambiguation functionality in the older entity disambiguation models.

\textbf{N\textsuperscript{3} Reuters} and \textbf{N\textsuperscript{3} RSS} \cite{N3Dataset} contain mentions referring to persons, places and organizations (DBpedia annotations). The Reuters dataset contains 128 news stories from Reuters news agency and the RSS dataset contains 500 RSS feed messages from worldwide newspapers (in English).

\textbf{Derczynski} \cite{DerczynskiDataset} contains 182 tweets annotated with DBpedia knowledge base entities.

\textbf{OKE challenge 2015 and 2016} evaluation sets \cite{OKEChallenge} contain 101 and 55 sentences from Wikipedia articles (reporting biographies of scholars), respectively, annotated using a mixture of annotations from DBpedia and the OKE entity identifiers.

We provided the \textit{out-of-domain} data sets to \textsc{SpEL}, using a \textit{fixed candidate set} of 500K entities, and compared its performance against other methods that have reported results on these datasets. The comparative results can be found in Table \ref{tab:out_of_domain_experimental_results}. 

Please note that \textsc{SpEL}'s tokenization procedure does not allow the generation of annotations that start or end within a single word (separated by space characters). For instance, in \textsc{SpEL}, the token \texttt{washington-based} is considered a single word, whereas out-of-domain datasets contain several annotations that commence or conclude within a single word. Additionally, each dataset necessitates a specific redirect normalization schema; for example, \texttt{China} is annotated as \texttt{People's\_Republic\_of\_China} in KORE, but in N\textsuperscript{3} RSS, it is annotated as \texttt{China}.

Nevertheless, \textsc{SpEL}-large delivers the best results on two out of seven and the second-best result on one out of seven test sets. It doesn't significantly underperform the other models in terms of performance on the remaining four test sets.

\end{document}
