\section{Result and discussion}
\label{sec:result}

\subsection{Overall Evaluation of Fraud Detection Task}\label{sec:fraud-overall}
\begin{sidewaystable}[thp]
    \setlength{\abovecaptionskip}{-16.cm}
\centering
    \scalebox{0.92}{
        \begin{tabular}{c|cccc|cccc||cccc|cccc}
            \hline
            \multirow{4}*{Models}&\multicolumn{8}{c||}{\multirow{2}*{\textbf{Yelp}}}&\multicolumn{8}{c}{\multirow{2}*{\textbf{Amazon}}}\\
            &&&&\multicolumn{1}{c}{}&&&&&&&&\multicolumn{1}{c}{}&&&&\\
            \cline{2-17}
            &\multicolumn{4}{c|}{\textbf{AUC}}&\multicolumn{4}{c||}{\textbf{Recall}}&\multicolumn{4}{c|}{\textbf{AUC}}&\multicolumn{4}{c}{\textbf{Recall}}\\
            &5\%&10\%&20\%&40\%&5\%&10\%&20\%&40\%&5\%&10\%&20\%&40\%&5\%&10\%&20\%&40\%\\
            \hline
            \multirow{2}*{\textbf{GCN}}&\multirow{2}*{54.98}&\multirow{2}*{50.94}&\multirow{2}*{53.15}&\multirow{2}*{52.47}&\multirow{2}*{53.12}&\multirow{2}*{51.10}&\multirow{2}*{53.87}&\multirow{2}*{50.81}&\multirow{2}*{74.44}&\multirow{2}*{75.25}&\multirow{2}*{75.13}&\multirow{2}*{74.34}&\multirow{2}*{65.54}&\multirow{2}*{67.81}&\multirow{2}*{66.15}&\multirow{2}*{67.45}\\
            \multirow{2}*{\textbf{GAT}}&\multirow{2}*{56.23}&\multirow{2}*{55.45}&\multirow{2}*{57.69}&\multirow{2}*{56.24}&\multirow{2}*{54.68}&\multirow{2}*{52.34}&\multirow{2}*{53.20}&\multirow{2}*{54.52}&\multirow{2}*{73.89}&\multirow{2}*{74.55}&\multirow{2}*{72.10}&\multirow{2}*{72.16}&\multirow{2}*{63.22}&\multirow{2}*{65.84}&\multirow{2}*{67.13}&\multirow{2}*{65.51}\\
            \multirow{2}*{\textbf{GraphSAGE}}&\multirow{2}*{53.82}&\multirow{2}*{54.20}&\multirow{2}*{56.12}&\multirow{2}*{54.00}&\multirow{2}*{54.25}&\multirow{2}*{52.23}&\multirow{2}*{52.69}&\multirow{2}*{52.86}&\multirow{2}*{70.71}&\multirow{2}*{73.97}&\multirow{2}*{73.97}&\multirow{2}*{75.27}&\multirow{2}*{69.09}&\multirow{2}*{69.36}&\multirow{2}*{70.30}&\multirow{2}*{70.16}\\
            &&&&&&&&&&&&&&&&\\
            \hline
            \multirow{2}*{\textbf{RGCN}}&\multirow{2}*{50.21}&\multirow{2}*{55.12}&\multirow{2}*{55.05}&\multirow{2}*{53.38}&\multirow{2}*{50.38}&\multirow{2}*{51.75}&\multirow{2}*{50.92}&\multirow{2}*{50.43}&\multirow{2}*{75.12}&\multirow{2}*{74.13}&\multirow{2}*{75.58}&\multirow{2}*{74.68}&\multirow{2}*{64.23}&\multirow{2}*{67.22}&\multirow{2}*{65.08}&\multirow{2}*{67.68}\\
            \multirow{2}*{\textbf{GeniePath}}&\multirow{2}*{56.33}&\multirow{2}*{56.29}&\multirow{2}*{57.32}&\multirow{2}*{55.91}&\multirow{2}*{52.33}&\multirow{2}*{54.35}&\multirow{2}*{54.84}&\multirow{2}*{50.94}&\multirow{2}*{71.56}&\multirow{2}*{72.23}&\multirow{2}*{71.89}&\multirow{2}*{72.65}&\multirow{2}*{65.56}&\multirow{2}*{66.63}&\multirow{2}*{65.08}&\multirow{2}*{65.41}\\
            \multirow{2}*{\textbf{Player2Vec}}&\multirow{2}*{51.03}&\multirow{2}*{50.15}&\multirow{2}*{51.56}&\multirow{2}*{53.65}&\multirow{2}*{50.00}&\multirow{2}*{50.00}&\multirow{2}*{50.00}&\multirow{2}*{50.00}&\multirow{2}*{76.86}&\multirow{2}*{75.73}&\multirow{2}*{74.55}&\multirow{2}*{56.94}&\multirow{2}*{50.00}&\multirow{2}*{50.00}&\multirow{2}*{50.00}&\multirow{2}*{50.00}\\
            \multirow{2}*{\textbf{SemiGNN}}&\multirow{2}*{53.73}&\multirow{2}*{51.68}&\multirow{2}*{51.55}&\multirow{2}*{51.58}&\multirow{2}*{52.28}&\multirow{2}*{52.57}&\multirow{2}*{52.16}&\multirow{2}*{50.59}&\multirow{2}*{70.25}&\multirow{2}*{76.21}&\multirow{2}*{73.98}&\multirow{2}*{70.35}&\multirow{2}*{63.29}&\multirow{2}*{63.32}&\multirow{2}*{61.28}&\multirow{2}*{62.89}\\
            \multirow{2}*{\textbf{GraphConsis}}&\multirow{2}*{61.58}&\multirow{2}*{62.07}&\multirow{2}*{62.31}&\multirow{2}*{62.07}&\multirow{2}*{62.60}&\multirow{2}*{62.08}&\multirow{2}*{62.35}&\multirow{2}*{62.08}&\multirow{2}*{85.46}&\multirow{2}*{85.29}&\multirow{2}*{85.50}&\multirow{2}*{85.50}&\multirow{2}*{85.49}&\multirow{2}*{85.38}&\multirow{2}*{85.59}&\multirow{2}*{85.53}\\
            \multirow{2}*{\textbf{GAS}}&\multirow{2}*{54.43}&\multirow{2}*{52.58}&\multirow{2}*{52.51}&\multirow{2}*{52.60}&\multirow{2}*{53.40}&\multirow{2}*{53.26}&\multirow{2}*{53.37}&\multirow{2}*{51.61}&\multirow{2}*{71.40}&\multirow{2}*{77.49}&\multirow{2}*{74.51}&\multirow{2}*{71.03}&\multirow{2}*{64.31}&\multirow{2}*{64.57}&\multirow{2}*{62.08}&\multirow{2}*{63.74}\\
            \multirow{2}*{\textbf{FdGars}}&\multirow{2}*{61.77}&\multirow{2}*{62.15}&\multirow{2}*{62.81}&\multirow{2}*{62.66}&\multirow{2}*{62.83}&\multirow{2}*{62.16}&\multirow{2}*{62.73}&\multirow{2}*{62.40}&\multirow{2}*{85.58}&\multirow{2}*{85.41}&\multirow{2}*{85.88}&\multirow{2}*{85.81}&\multirow{2}*{85.83}&\multirow{2}*{85.73}&\multirow{2}*{85.84}&\multirow{2}*{85.93}\\
            &&&&&&&&&&&&&&&&\\
            \hline
            \multirow{2}*{\textbf{GraphNAS}}&\multirow{2}*{52.93}&\multirow{2}*{54.69}&\multirow{2}*{56.73}&\multirow{2}*{54.46}&\multirow{2}*{52.40}&\multirow{2}*{54.15}&\multirow{2}*{55.69}&\multirow{2}*{56.16}&\multirow{2}*{71.01}&\multirow{2}*{72.48}&\multirow{2}*{73.52}&\multirow{2}*{76.05}&\multirow{2}*{69.17}&\multirow{2}*{69.48}&\multirow{2}*{70.35}&\multirow{2}*{70.16}\\
            \multirow{2}*{\textbf{GraphNAS}}&\multirow{2}*{53.26}&\multirow{2}*{55.31}&\multirow{2}*{57.15}&\multirow{2}*{55.59}&\multirow{2}*{53.69}&\multirow{2}*{55.47}&\multirow{2}*{56.04}&\multirow{2}*{57.00}&\multirow{2}*{72.41}&\multirow{2}*{73.04}&\multirow{2}*{73.58}&\multirow{2}*{76.25}&\multirow{2}*{70.36}&\multirow{2}*{70.53}&\multirow{2}*{71.73}&\multirow{2}*{71.88}\\
            \multirow{2}*{\textbf{Policy-GNN}}&\multirow{2}*{54.04}&\multirow{2}*{55.73}&\multirow{2}*{59.30}&\multirow{2}*{60.60}&\multirow{2}*{53.08}&\multirow{2}*{55.35}&\multirow{2}*{58.75}&\multirow{2}*{59.99}&\multirow{2}*{72.20}&\multirow{2}*{73.30}&\multirow{2}*{74.11}&\multirow{2}*{77.20}&\multirow{2}*{70.10}&\multirow{2}*{71.20}&\multirow{2}*{73.08}&\multirow{2}*{74.44}\\
            \multirow{2}*{\textbf{Policy-GNN}}&\multirow{2}*{55.75}&\multirow{2}*{56.29}&\multirow{2}*{60.01}&\multirow{2}*{61.52}&\multirow{2}*{54.15}&\multirow{2}*{56.16}&\multirow{2}*{58.95}&\multirow{2}*{60.33}&\multirow{2}*{73.69}&\multirow{2}*{74.06}&\multirow{2}*{75.29}&\multirow{2}*{78.85}&\multirow{2}*{71.34}&\multirow{2}*{72.46}&\multirow{2}*{74.55}&\multirow{2}*{76.70}\\
            &&&&&&&&&&&&&&&&\\
            \hline
            \multirow{2}*{\textbf{CARE-GNN}}&\multirow{2}*{71.26}&\multirow{2}*{73.31}&\multirow{2}*{74.45}&\multirow{2}*{75.70}&\multirow{2}*{67.53}&\multirow{2}*{67.77}&\multirow{2}*{68.60}&\multirow{2}*{71.92}&\multirow{2}*{89.54}&\multirow{2}*{89.44}&\multirow{2}*{89.45}&\multirow{2}*{89.73}&\multirow{2}*{88.34}&\multirow{2}*{88.29}&\multirow{2}*{88.27}&\multirow{2}*{88.48}\\
            &&&&&&&&&&&&&&&&\\
            \hline
            \multirow{2}*{\textbf{\RioGNN}}&\multirow{2}*{\textbf{81.97}}&\multirow{2}*{\textbf{83.72}}&\multirow{2}*{\textbf{82.31}}&\multirow{2}*{\textbf{83.54}}&\multirow{2}*{\textbf{75.33}}&\multirow{2}*{\textbf{75.78}}&\multirow{2}*{\textbf{75.51}}&\multirow{2}*{\textbf{76.19}}&\multirow{2}*{\textbf{95.44}}&\multirow{2}*{\textbf{95.41}}&\multirow{2}*{\textbf{95.63}}&\multirow{2}*{\textbf{96.19}}&\multirow{2}*{\textbf{90.17}}&\multirow{2}*{\textbf{89.48}}&\multirow{2}*{\textbf{89.51}}&\multirow{2}*{\textbf{89.82}}\\
            &&&&&&&&&&&&&&&&\\            
            \hline
        \end{tabular}
    }
    \caption{Fraud Detection results () compared to the baselines.}\label{tab:fraud_baseline}
\end{sidewaystable}





\subsubsection{Accuracy Analysis}\label{sec:fraud-accuracy}
In this section, we conduct experiments to evaluate the accuracy of the fraud detection task on Yelp and Amazon datasets.
We report the best test results of \RioGNN, baselines and variants in five hundred epochs.
It can be observed from the results that \RioGNN performs better than other baselines and variants under most training ratios or indicators. 
This indicates the feasibility of \RioGNN in fraud detection scenarios. 

\textbf{Single-relations vs. Multi-relations. }
Table~\ref{tab:fraud_baseline} shows the results of baseline experiments built on different types of graphs for the fraud detection task.
To solve the diversity and heterogeneity of complex networks in actual fine-grained applications, we consider introducing a neural network with a multi-relational graph structure instead of a single relation structure.
However, from the results of some of the baselines in the table, although GCN, GAT, GraphSAGE and GeniePath models run on a single relation graph, they are better than RGCN, Player2Vec and SemiGNN in terms of accuracy of the Yelp dataset, which are run on the multi-relational graphs.
The observation above shows that the previous multi-relational GNNs are not suitable for  constructing multi-relational graphs in the fraud detection task.
Similar phenomena manifest in the Amazon dataset.
Besides, GraphConsis, FdGars, CARE-GNN and \RioGNN significantly outperform other models by 8.60\%-32.78\% over Yelp and Amazon datasets. 
This is because these four models sample the neighbors according to node features before aggregating them, indicating that the impurity neighbors will interfere with the aggregation process and the fraud detection task has a strong demand for neighbor sampling optimization.
CARE-GNN and \RioGNN have improved the AUC of 3.51\%-21.65\%, compared with the performance of GraphConsis and FdGars.
This is because \RioGNN can better use internal relations to solve downstream application problems through parameterized similarity measures and adaptive sampling thresholds, which shows that automated sampling has a significant improvement effect on fraud detection tasks.
The more remarkable result is that the proposed \RioGNN model improves the accuracy of 5.90\% and 10.41\% compared with CARE-GNN.
It verifies the advantage of combining the label-aware neighbor similarity measure and the Recursive and Scalable Reinforcement Learning framework, which can effectively break through the limitations of the CARE-GNN state observation range and manually specified strategies.
Meanwhile, \RioGNN has a promising effectiveness on fraud detection tasks.

\textbf{Heterogeneous vs. Multi-relation. }
In order to further analyze the accuracy of the multi-relational graph, we conduct heterogeneous graph experiments and multi-relational graph experiments on the latest GNN model guided by reinforcement learning. 
From the results of the heterogeneous graph model GraphNAS, Policy-GNN and the multi-relationship graph model GraphNAS and Policy-GNN in Table~\ref{tab:fraud_baseline}, it can be found that the multi-relational graph compared with the heterogeneous graph brings an AUC improvement of - in the Yelp and Amazon datasets. 
This confirms that in other similar models, the multi-relational graph we construct still has obvious advantages. 
In addition, we found that GraphNAS and Policy-GNN, which are also based on reinforcement learning guidance and use the multi-relational graph, have no significant advantages in AUC and Recall. 
This is because GraphNAS and Policy-GNN do not adaptively sample different relations, which causes them to be limited by the complexity of the relationship between Yelp and Amazon datasets. 
And for Policy-GNN, since the one-hop neighbor information of Yelp and Amazon is already rich enough, more multi-hop strategies cannot bring significant benefits. 


\textbf{Training Percentage. }
To measure the impact of the training ratio on the classification accuracy, we use four different ratios of 5\% to 40\% for experiments.
It can be seen from Table~\ref{tab:fraud_baseline} that most of the baseline performance changes are not necessarily related to the increase in training percentage.
It indicates that the semi-supervised learning approach leveraging a small number of supervised signals is enough to train a good model.
Moreover, in the four different training ratios of the Yelp dataset, the AUC fluctuation range of \RioGNN is only within 1.57\% compared to the 4.44\% of CARE-GNN.
In Amazon, both models have good stability.
This is because the Amazon node features provide enough information to distinguish fraudsters, which is of higher quality than the Yelp dataset.
It also verifies from another result that \RioGNN has better stability and adaptability under complicated environments.




\begin{table}[t]
    \setlength{\abovecaptionskip}{0.cm}
    \setlength{\belowcaptionskip}{-0.cm}
    \caption{Fraud Detection classification results () compared to \RioGNN variants.}\label{tab:fraud_variants}
    \centering
    \scalebox{1}{
        \begin{tabular}{p{0.5cm}p{2.5cm}|p{1.5cm}<{\centering}p{1.5cm}<{\centering}|p{1.5cm}<{\centering}p{1.5cm}<{\centering}}
            \hline
            &\multicolumn{1}{p{2.5cm}|}{\multirow{2}*{Models}}&\multicolumn{2}{c|}{\textbf{Yelp}}&\multicolumn{2}{c}{\textbf{Amazon}}\\
            \cline{3-6}
            &\multicolumn{1}{p{2.5cm}|}{}&\textbf{AUC}&\textbf{Recall}&\textbf{AUC}&\textbf{Recall}\\
            \hline
            &\multicolumn{1}{p{2.5cm}|}{\RioGNN}&76.01&63.15&91.28&72.46\\
            \hline
            &\multicolumn{1}{p{2.5cm}|}{BIO-GNN}&78.67&71.21&95.47&88.35\\
            &\multicolumn{1}{p{2.5cm}|}{ROO-GNN}&\textbf{83.59}&    \textbf{75.56}&95.58&89.22\\
            \hline
            &\multicolumn{1}{p{2.5cm}|}{RIO-Att}&78.65&71.69&93.97&83.78\\
            &\multicolumn{1}{p{2.5cm}|}{RIO-Weight}&80.40&72.83&\textbf{96.25}&\textbf{89.61}\\
            &\multicolumn{1}{p{2.5cm}|}{RIO-Mean}&77.84&71.43&94.57&89.47\\
            \hline
            &\multicolumn{1}{p{2.5cm}|}{\RioGNN}&83.54&75.55&96.19&88.66\\
            \hline
        \end{tabular}
    }
\end{table}



\textbf{\RioGNN Variants in Classification. }
To measure the positive impact of the newly added mechanism on the classification accuracy of fraud detection tasks, we compare several variants of \RioGNN under the discrete strategy.
The experiment sets the training data ratio to 40\%, and the other settings are the same as Section~\ref{sec:hardware-software}.
We show the experimental results of spam review classification in the Yelp dataset and suspicious user classification in the Amazon dataset as shown in Table~\ref{tab:fraud_variants}.
From the results, the performance of all variants is better than the baseline model. Next, we will discuss the effects of different variants from three aspects.


Firstly, in the two datasets, except for the \RioGNN variant, all other variants only use the Label-aware Similarity Measure with a one-layer structure.
From the results in Table~\ref{tab:fraud_variants}, the \RioGNN variant is lower than all other variants, but it performs better than all baselines.
It can be found that in the fraud detection task, the increase in the number of layers does not bring about a significant increase in classification accuracy.
This is limited by the dataset size of Yelp and Amazon, and the importance of information in multi-hop neighbors is low.
We will continue to explore more multi-layer effects in Section~\ref{sec:fraud-efficiency}.



Secondly, for the similarity-aware neighbor selector part, we observe the comparison results of the variant BIO-GNN without adaptive strategy optimization reinforcement learning algorithm and the single-depth structure variant ROO-GNN without the recursive framework for the \RSRL framework.
The second part of Table~\ref{tab:fraud_variants} gives evidence of partial optimization. 
The results of the BIO-GNN variant on the \RioGNN model show that the automatic strategy optimization in Yelp and Amazon effectively improves the classification accuracy of 4.65\% and 0.89\%, and the BIO-GNN variant is also far better than most baseline models.
This shows that the Markov Decision Process has a positive effect on searching for the filtering threshold of the aggregation process.
Moreover, the reinforcement learning algorithm with dynamic iterative function and full action space learning process breaks the limitation of fixed strategy and observation range and obtains a better threshold selection effect, which also confirms the conjecture in Section~\ref{sec:rsrl}. 
Besides, combining Figure~\ref{fig:fraud-efficiency} with Table~\ref{tab:fraud_variants}, the accuracy of the ROO-GNN variant has little change compared with \RioGNN, that is, the multi-depth structure of \RioGNN can converge much quicker than the single-depth variant ROO-GNN whilst maintaining a higher accuracy rate.
This also implies the stability of the recursive framework in terms of classification accuracy.


Finally, from the results of the variation of the aggregation methods between the different relations in the third part of the Table~\ref{tab:fraud_variants}, \RioGNN has apparent advantages over the other three on the Yelp dataset, and both RIO-Weight and \RioGNN on the Amazon dataset have good results.
It confirms that \RioGNN does not need to train additional attention weights, and using the filtering threshold as an inter-relation aggregation weight can improve the performance of GNN and reduce the complexity of the model.
Therefore, it can get the best performance compared with other variants.
For the three variants, RIO-Weight has better results than the other two, but \RioGNN can maintain better accuracy in the dataset of different quality and structure and has a certain degree of adaptability.



\begin{table}[t]
    \setlength{\abovecaptionskip}{0.cm}
    \setlength{\belowcaptionskip}{-0.cm}
    \caption{Fraud detection clustering results () compared to \RioGNN variants.}\label{tab:fraud_cluster}
    \centering
    \scalebox{1}{
        \begin{tabular}{c|c|c|cc|ccc|c}
            \hline
            \multicolumn{1}{c|}{\multirow{1}*{Dataset}}&\multicolumn{1}{c|}{\multirow{1}*{Metric}}&\multirow{1}*{\RioGNN}&BIO-GNN&ROO-GNN&RIO-Att&RIO-Weight&RIO-Mean&\multirow{1}*{\RioGNN}\\
            \hline
            \multirow{2}*{\textbf{Yelp}}&\textbf{NMI}&3.18&9.36&\textbf{12.39}&9.80&12.05&8.39&12.22\\
            &\textbf{ARI}&6.12&11.84&\textbf{16.61}&11.88&15.88&8.80&16.45\\
            \hline
            \multirow{2}*{\textbf{Amazon}}&\textbf{NMI}&58.87&59.83&57.81&55.76&58.76&58.72&\textbf{61.26}\\
            &\textbf{ARI}&76.53&77.38&76.09&76.54&76.73&76.51&\textbf{78.40}\\
            \hline
        \end{tabular}
    }
\end{table}



\textbf{\RioGNN Variants in Clustering. }
In order to explore the effectiveness of \RioGNN in clustering tasks, we conduct clustering experiments on \RioGNN and its variant models. 
The experiment set a fixed training rate of . 
We cluster the node representations learned by \RioGNN through K-Means. 
The results are shown in Table~\ref{tab:fraud_cluster}. 
We respectively count the best values of NMI and ARI indicators within 500 epochs. 
It can be seen from the results that compared with RIO-GNN and BIO-GNN, \RioGNN's NMI and ARI indicators in the Yelp dataset increase at least  and  respectively. 
Similarly, the Amazon dataset has risen by at least  and . 
This phenomenon is the same as the classification result, and is affected by the limitation of the size of the dataset, the choice of the action space, and the dynamic iterative function. 
In addition, the NMI and ARI of ROO-GNN in the Yelp dataset achieved the best results, while the \RioGNN in the Amazon dataset exceeded the NMI and ARI of ROO-GNN by  and . 
This is because the Yelp dataset is smaller than Amazon, so the optimization space of the recursive framework is also smaller. 
It shows that the recursive framework has better advantages in dense datasets. 
What is more noteworthy is that \RioGNN in the clustering experiment exceeds the effect of all GNN aggregation variants. 
NMI and ARI exceed - and - in the Yelp dataset, and exceed - and - in the Amazon dataset. 
This shows that directly using the filtering threshold as the weight of the aggregation has obvious advantages in clustering tasks.


\begin{figure}[t]
\centering
\subfigure[Scores of \RioGNN on Yelp.]{\label{fig:fraud-training-RioGNN-a}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=8.3cm]{fig/scores_yelp_discrete.pdf}
\end{minipage}}\subfigure[Thresholds of \RioGNN on Yelp.]{\label{fig:fraud-training-RioGNN-b}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=8.3cm]{fig/thresholds_yelp_discrete.pdf}
\end{minipage}}

\centering
\subfigure[Scores of \RioGNN on Amazon.]{\label{fig:fraud-training-RioGNN-c}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=8.3cm]{fig/scores_amazon_discrete.pdf}
\end{minipage}}\subfigure[Thresholds of \RioGNN on Amazon.]{\label{fig:fraud-training-RioGNN-d}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=8.3cm]{fig/thresholds_amazon_discrete.pdf}
\end{minipage}}\centering
\caption{The training scores and thresholds of \RioGNN on Yelp and Amazon.}\label{fig:fraud-training-RioGNN}
\end{figure}


\begin{figure}[h]
\centering
\subfigure[Scores of ROO-GNN on Yelp.]{\label{fig:fraud-training-varitants-a}
\begin{minipage}[h]{0.5\linewidth}
\centering
\includegraphics[width=8.3cm]{fig/scores_yelp_no_recursion.pdf}
\end{minipage}}\subfigure[Thresholds of ROO-GNN on Yelp.]{\label{fig:fraud-training-varitants-b}
\begin{minipage}[h]{0.5\linewidth}
\centering
\includegraphics[width=8.3cm]{fig/thresholds_yelp_no_recursion.pdf}
\end{minipage}}

\subfigure[Scores of BIO-GNN on Yelp.]{\label{fig:fraud-training-varitants-c}
\begin{minipage}[h]{0.5\linewidth}
\centering
\includegraphics[width=8.3cm]{fig/scores_yelp_recursion_BMAB.pdf}
\end{minipage}}\subfigure[Thresholds of BIO-GNN on Yelp.]{\label{fig:fraud-training-varitants-d}
\begin{minipage}[h]{0.5\linewidth}
\centering
\includegraphics[width=8.3cm]{fig/thresholds_yelp_recursion_BMAB.pdf}
\end{minipage}}\centering
\caption{The training scores and thresholds of \RioGNN variants on Yelp.}\label{fig:fraud-training-varitants}
\end{figure}



\subsubsection{Explainable RSRL Training Process}\label{sec:fraud-explainable}
This section focuses on the \RSRL framework and discusses the explainability of the reinforcement learning process in detail.
We show the change process of the \RioGNN's filtering threshold and similarity score of the different relations before convergence during the training process on the Yelp and Amazon datasets.
For a better comparison, we also conduct a similar analysis on ROO-GNN and BIO-GNN variants.


\textbf{Filter Thresholds. }
In Table~\ref{tab:dataset}, we observe that the average feature similarity for most relations in Yelp and Amazon are very high.
However, there is a relation such as R-T-R, which has a low label similarity of , which indicates that fraudsters successfully disguised themselves.
For this reason, we propose to filter the lower-ranked neighbors in the Top-p sampling through the filtering threshold.
It can be seen from Figure~\ref{fig:fraud-training-RioGNN-b} and Figure~\ref{fig:fraud-training-RioGNN-c} that the filtering thresholds of the three relations of the Yelp dataset are stable at , and the Amazon dataset converges to .
It shows that the filtering thresholds for different relations eventually converge to different values.
The reason is that the label similarity and feature similarity of different relations are different in the same dataset.
This result also can be verified from Table~\ref{tab:dataset}.
For instance, the label similarity difference between R-U-R and R-T-R is , but the feature similarity difference is only .
The proposed framework builds a reinforcement learning tree for each relation, uses the similarity between the relations as a reward, and independently finds the appropriate filtering threshold.
Due to the mutual influence between relations, a set of Nash equilibrium filtering thresholds will eventually be obtained, which is a sampling scheme that can best eliminate the interference of fraudsters~\cite{dou2020robust}.
In the Nash equilibrium, it is impossible for all agents to obtain greater rewards only by changing their own strategies.
In addition, in Figure~\ref{fig:fraud-training-RioGNN-b} and Figure~\ref{fig:fraud-training-varitants-b}, Figure~\ref{fig:fraud-training-varitants-d}, we also observe that different models are used under the same dataset, and they converge and combine with different filtering thresholds , , , respectively.
Since the result of reinforcement learning is obtained by the connection strategy of all agents, there may be many different filter threshold combinations at each time the game between relations reaches the Nash equilibrium~\cite{junling1998rlmultinash,pozo2011multinash}.


Figure~\ref{fig:fraud-training-RioGNN-a} and Figure~\ref{fig:fraud-training-RioGNN-c} show the changes in rewards obtained during the filtering threshold learning process for different relations.
We observe that in the Yelp dataset, the relations R-S-R and R-U-R achieve better reward growth in the interest competition of the three relations. 
In contrast, the relation R-T-R eventually stabilizes at a relatively low reward. 
This is consistent with the actual scenario. 
Comments with the same star rating in the same product are important considerations for dividing spam comments, and comments by the same user usually have the same tendency. 
However, reviews published in the same month have a lower impact factor. 
The U-P-U and U-S-U of the similar Amazon dataset are more meaningful than the relation U-V-U. 
This means that users who post similar content are more closely connected, and are considered an important observation factor when making user judgments.



\textbf{Recursive Framework.}
In the right column of Figure~\ref{fig:fraud-training-varitants}, we show the changes in the filtering thresholds of the two variants of \RSRL, ROO-GNN and BIO-GNN on the Yelp dataset.
Comparing \RioGNN in Figure~\ref{fig:fraud-training-RioGNN-b} with ROO-GNN in Figure~\ref{fig:fraud-training-varitants-b}, it can be seen that \RioGNN with the recursive framework of Figure~\ref{fig:fraud-training-RioGNN-b} performs a more accurate filtering threshold search for each depth.
For example, the relation R-U-R is first explored in the interval  with an accuracy of  to obtain a convergence of .
Then it searches for the convergence between  to obtain the highest accuracy  of the relation R-U-R, and get the final convergence . 
The difference from \RioGNN is the ROO-GNN model in Figure~\ref{fig:fraud-training-varitants-b}, where only one deep reinforcement learning is performed.
For example, R-U-R directly explores the  interval with the highest accuracy of  and obtains the final convergence value of .
In addition to the difference mentioned above, we can also see whether there is a recursive reward change shown in Figure~\ref{fig:fraud-training-RioGNN-a} and Figure~\ref{fig:fraud-training-varitants-a}.
For \RioGNN with recursion, all relations converge fully at  epochs, while ROO-GNN without recursion converges at  epochs.
Furthermore, the rewards obtained by the two methods are basically the same.
This solves the challenge we pose in Section~\ref{sec:challenges}, and explains that reducing the number of actions for each reinforcement learning can effectively accelerate the convergence of the model.
The experimental results also show that the addition of recursion does not bring about a significant loss of accuracy, which is significant.

\begin{comment}
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/scores_yelp_2layer.pdf}
  \caption{Scores of Multi-Layer Rio-GNN on Yelp.}
  \label{fig:fraud-2layer-scores}
\end{figure}
\end{comment}


\begin{figure}[t]
\centering
\subfigure[R-U-R.]{\label{fig:fraud-2layer-a}
\begin{minipage}[h]{0.3333\linewidth}
\centering
\includegraphics[width=5.7cm]{fig/scores_yelp_2layer_1.pdf}
\end{minipage}}\subfigure[R-T-R.]{\label{fig:fraud-2layer-b}
\begin{minipage}[h]{0.3333\linewidth}
\centering
\includegraphics[width=5.7cm]{fig/scores_yelp_2layer_2.pdf}
\end{minipage}}\subfigure[R-S-R.]{\label{fig:fraud-2layer-c}
\begin{minipage}[h]{0.3333\linewidth}
\centering
\includegraphics[width=5.7cm]{fig/scores_yelp_2layer_3.pdf}
\end{minipage}}\centering
\caption{Scores of Multi-Layer \RioGNN on Yelp.}\label{fig:fraud-2layer-scores}
\end{figure}


\textbf{Action Space and Iterative Function. }
In Figure~\ref{fig:fraud-training-RioGNN-b} and Figure~\ref{fig:fraud-training-varitants-d}, we present the BIO-GNN variant and \RioGNN with dynamic iterative function and full action space learning process.
It can be seen that, similar to \RioGNN, BIO-GNN also recursively converges to  with an accuracy of  for the R-U-R relation and then converges to  with an accuracy of .
But the difference from \RioGNN is that this method has an average similarity score of  for the three relations, which is lower than \RioGNN's .
This means that the maximum filtering effect has not been achieved. 
In other respects, compared with the Figure~\ref{fig:fraud-training-RioGNN-a} and Figure~\ref{fig:fraud-training-varitants-a} that have obtained better performance, the R-T-R relation of Figure~\ref{fig:fraud-training-varitants-c} has obtained a higher reward ratio in the competition. That is, the behaviors published in the same month are considered by BIO-GNN to be more important, which is contrary to the reality. So this explains the reason for the lower performance of BIO-GNN.
Meanwhile, these results demonstrates that dynamic learning can be observed globally, thereby obtaining more effect, which proves the conjecture in Section~\ref{sec:rsrl}.





\begin{figure}[t]
\centering
\subfigure[AUC of Rio-GNN, BIO-GNN and ROO-GNN on Yelp.]{\label{fig:fraud-efficiency-a}
\begin{minipage}[t]{1\linewidth}
\centering
\includegraphics[width=15.3cm]{fig/calculation_efficiency_yelp.pdf}
\end{minipage}}

\subfigure[AUC of \RioGNN and \RioGNN on Amazon.]{\label{fig:fraud-efficiency-b}
\begin{minipage}[t]{1\linewidth}
\centering
\includegraphics[width=15.3cm]{fig/calculation_efficiency_amazon.pdf}
\end{minipage}}\centering
\caption{The impact of recursive framework on computational efficiency.}\label{fig:fraud-efficiency}
\end{figure}


\subsubsection{Effectiveness and Efficiency Evaluation}\label{sec:fraud-efficiency}
Next, we introduce the effectiveness and efficiency of multi-layer similarity perception modules and recursive neighbor selectors in fraud detection tasks on the two datasets.
We use \RioGNN with a two-layer perception structure to train for  epochs on the Yelp dataset, and select the first  epochs to show the changes in the scores of each layer in Figure~\ref{fig:fraud-2layer-scores}.
For the recursive framework, we analyze the AUC change trend of ROO-GNN, BIO-GNN variants and \RioGNN on two datasets within 500 epochs (Figure~\ref{fig:fraud-efficiency}).
The dotted line in the figure indicates the average AUC of each variant after reaching stability.
The specially marked points in the figure show the epoch numbers for different models to reach a certain AUC.

\textbf{Multi-layer Analysis.}
We provide a multi-layer label-aware similarity neighbor measurement scheme to deal with data collections with more complex structures in the future. 
For the datasets in this article, the increase of multiple layers is limited by the datasets (one-hop neighbor information is enough for sampling), and the AUC gain brought by it is limited. 
However, we notice in Figure~\ref{fig:fraud-2layer-scores} that as the number of layers increases, Layer  has a significant improvement in the similarity score of each relation compared to Layer . 
Among them, the similarity score of the relation R-U-R is  in the second level compared with the first level. 
In addition, the R-T-R and R-S-R are improved in  and .
This is because we take the embedding of the neighbors of the previous layer as the input of the second layer, embedding more hops of neighbor relations. 
Rich neighbor information makes the performance of the similarity module better. This also shows that multi-layer joining can be a new deployment scheme in some datasets with insufficient embedded information of one-hop neighbors.

\textbf{Multi-depth Analysis.}
In order to effectively speed up the optimization speed of the filtering threshold of each relation while ensuring the accuracy rate, we propose a recursive reinforcement learning learning framework. 
It can be seen from Figure~\ref{fig:fraud-efficiency} that the \RioGNN model has obvious advantages over ROO-GNN and BIO-GNN in both datasets. 
First of all, we observe the final convergence AUC size.
In the Yelp dataset, \RioGNN and BIO-GNN models are about  higher than ROO-GNN. 
In the Amazon dataset, \RioGNN is better than the other two models by about . 
And BIO-GNN finally converged AUC is generally low. 
In addition, in terms of computational efficiency, compared with the ROO-GNN model without recursion, \RioGNN maintains a stable and rapid increase in AUC in both datasets. 
However, ROO-GNN has greater fluctuations. 
In the first  epochs of Yelp and the first  epochs of Amazon, there is no significant difference between them. 
In the Yelp dataset, the speedup of \RioGNN compared to ROO-GNN is  when AUC reaches ,  when AUC reaches , and  when AUC reaches . 
In Amazon, the speedup ratio is  when the AUC reaches , and the speedup ratio is  when AUC reaches . 
Compared with the BIO-GNN with limited action space and fixed strategy, Yelp and Amazon are also observed  and  times time savings at  and  AUC. 
This shows that the proposed recursive framework can achieve good efficiency while maintaining accuracy. And generally, the higher the AUC demand, the better the efficiency. 
The broader and flexible action space and the iterative function that automatically updates have more significant advantages in terms of efficiency and accuracy. 
Finally, we find that different optimization structures have different impact factors for different datasets. 
Due to the smaller scale and lower accuracy requirements of the Yelp dataset, whether it has a recursive structure has little effect on the final convergence AUC. 
But in Amazon, which has a larger scale and higher precision requirements, placing all actions at one depth causes a loss of accuracy. 
This also confirms the conjecture in Section~\ref{sec:rsrl} about the loss of accuracy caused by the excessively large action space, and confirms the advantages of the recursive structure in large-scale datasets.







\subsection{Overall Evaluation of Diagnosis of Diabetes Mellitus Task}\label{sec:diabetes-overall}

\subsubsection{Accuracy Analysis}\label{sec:diabetes-accuracy}
In this section, we conduct experiments to evaluate the accuracy of diagnosis of diabetes mellitus on the MIMIC-III dataset.
As presented in Table~\ref{tab:diabetes_base}, we report the best test results of \RioGNN and various baselines and variants in seven hundred epochs.
It can be observed from the results that \RioGNN performs better than other baselines and variants under most training ratios and indicators.



\textbf{Single-relation vs. Multi-relation.}
In order to further prove the effectiveness of \RioGNN in processing the multi-relational graph, we perform a baseline comparison on the more challenging task of diagnosis of diabetes mellitus.
Table~\ref{tab:diabetes_base} shows the comparative results of \RioGNN and the three types of baselines. 
Compared with the first type of baseline running on a single-relational graph: GCN, GAT and GraphSAGE, the model \RioGNN is significantly better than them on the MIMIC-III dataset by 13.32\%-18.30\%.
This result fully confirms that the fine-grained division of multi-relational graph and hierarchical aggregation based on different relations are very conducive to the completion of the node classification task. 
This point is completely consistent with the experimental conclusions on the fraud detection task. 
Furthermore, in order to show the performance of \RioGNN when dealing with the multi-relational graph, we carry out the second type of baseline comparison experiment.
Although GCT, HSGNN and HAN all run on heterogeneous graphs, and propose different ideas for processing heterogeneous relations, their accuracy rates are at least 8.77\% lower than \RioGNN. 
Due to the high density of neighbor nodes under each relation, the inability to effectively filter interfering nodes also brings difficulties to the final diagnosis task. 
Compared with \RioGNN, it is obvious that they fail to filter out interfering neighbor nodes and produce a sufficiently strong positive effect on the final diagnosis. 
Interestingly, comparing the two types of baselines, we find that although the second type of baselines divides the heterogeneous relations to a certain extent, they are in most cases even worse than the first type of baselines running on a single-relational graph. 
The above results show that when dealing with the multi-relational graph, how to effectively select the appropriate neighbor nodes is particularly important, while \RioGNN achieves this well through parameterized similarity measures and adaptive sampling thresholds. 
When it comes to the third type of baseline, CARE-GNN, we find that its accuracy is significantly higher than the first two types of baselines by 11.73\%-17.70\%, which means that fine-grained and multi-relational division of heterogeneous graphs is very necessary.
Although CARE-GNN realizes automatic filtering and sampling of neighbor nodes to a certain extent, its diagnostic effect is lower than \RioGNN due to the inability to adaptively select the best filtering threshold under each relation. 
The above shows that the proposed RSRL framework finds the optimal filtering threshold under each relation in the recursive process successfully, thus it performs outstandingly in downstream tasks.





\begin{table}[t]
    \setlength{\abovecaptionskip}{0.cm}
    \setlength{\belowcaptionskip}{-0.cm}
    \caption{Diabetes Detection results () compared to the baselines.}\label{tab:diabetes_base}
    \centering
    \scalebox{1}{
        \begin{tabular}{p{3cm}<{\centering}|p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}|p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}}
            \hline
            \multirow{4}*{Models}&\multicolumn{8}{c}{\multirow{2}*{\textbf{MIMIC-III}}}\\
            &&&&\multicolumn{1}{c}{}&&&&\\
            \cline{2-9}
            &\multicolumn{4}{c|}{\textbf{AUC}}&\multicolumn{4}{c}{\textbf{Recall}}\\
            &5\%&10\%&20\%&40\%&5\%&10\%&20\%&40\%\\
            \hline
            \multirow{1}*{\textbf{GCN}}&\multirow{1}*{65.37}&\multirow{1}*{65.39}&\multirow{1}*{64.93}&\multirow{1}*{65.13}&\multirow{1}*{60.85}&\multirow{1}*{61.24}&\multirow{1}*{60.31}&\multirow{1}*{60.99}\\
             \multirow{1}*{\textbf{GAT}}&\multirow{1}*{63.67}&\multirow{1}*{63.09}&\multirow{1}*{63.11}&\multirow{1}*{64.26}&\multirow{1}*{62.13}&\multirow{1}*{62.75}&\multirow{1}*{63.12}&\multirow{1}*{63.45}\\
             \multirow{1}*{\textbf{GraphSAGE}}&\multirow{1}*{65.91}&\multirow{1}*{66.28}&\multirow{1}*{65.82}&\multirow{1}*{65.34}&\multirow{1}*{63.80}&\multirow{1}*{63.88}&\multirow{1}*{63.82}&\multirow{1}*{63.99}\\
             \hline
             \multirow{1}*{\textbf{GCT}}&\multirow{1}*{62.97}&\multirow{1}*{63.45}&\multirow{1}*{64.33}&\multirow{1}*{65.14}&\multirow{1}*{60.08}&\multirow{1}*{61.23}&\multirow{1}*{61.39}&\multirow{1}*{62.25}\\
             \multirow{1}*{\textbf{HSGNN}}&\multirow{1}*{63.27}&\multirow{1}*{65.68}&\multirow{1}*{65.03}&\multirow{1}*{67.87}&\multirow{1}*{61.24}&\multirow{1}*{63.87}&\multirow{1}*{64.08}&\multirow{1}*{65.39}\\
            \multirow{1}*{\textbf{HAN}}&\multirow{1}*{62.79}&\multirow{1}*{63.26}&\multirow{1}*{64.94}&\multirow{1}*{65.13}&\multirow{1}*{61.15}&\multirow{1}*{61.27}&\multirow{1}*{62.38}&\multirow{1}*{63.02}\\
            \hline
             \multirow{1}*{\textbf{GraphNAS}}&\multirow{1}*{64.03}&\multirow{1}*{65.28}&\multirow{1}*{65.08}&\multirow{1}*{65.59}&\multirow{1}*{62.18}&\multirow{1}*{63.94}&\multirow{1}*{64.05}&\multirow{1}*{65.14}\\
             \multirow{1}*{\textbf{GraphNAS}}&\multirow{1}*{65.76}&\multirow{1}*{67.40}&\multirow{1}*{67.79}&\multirow{1}*{68.05}&\multirow{1}*{64.28}&\multirow{1}*{66.88}&\multirow{1}*{67.31}&\multirow{1}*{67.93}\\
            \multirow{1}*{\textbf{Policy-GNN}}&\multirow{1}*{66.30}&\multirow{1}*{67.19}&\multirow{1}*{67.70}&\multirow{1}*{67.98}&\multirow{1}*{63.28}&\multirow{1}*{66.05}&\multirow{1}*{67.12}&\multirow{1}*{68.18}\\
            \multirow{1}*{\textbf{Policy-GNN}}&\multirow{1}*{67.45}&\multirow{1}*{68.55}&\multirow{1}*{69.01}&\multirow{1}*{69.59}&\multirow{1}*{64.73}&\multirow{1}*{67.11}&\multirow{1}*{68.32}&\multirow{1}*{69.02}\\
            \hline
            \multirow{1}*{\textbf{CARE-GNN}}&\multirow{1}*{77.64}&\multirow{1}*{80.22}&\multirow{1}*{80.81}&\multirow{1}*{81.26}&\multirow{1}*{69.17}&\multirow{1}*{71.58}&\multirow{1}*{72.08}&\multirow{1}*{72.68}\\
            \hline
            \multirow{1}*{\textbf{\RioGNN}}&\multirow{1}*{\textbf{79.23}}&\multirow{1}*{\textbf{80.92}}&\multirow{1}*{\textbf{81.23}}&\multirow{1}*{\textbf{82.56}}&\multirow{1}*{\textbf{71.23}}&\multirow{1}*{\textbf{72.64}}&\multirow{1}*{\textbf{72.93}}&\multirow{1}*{\textbf{74.01}}\\
            \hline
        \end{tabular}
    }
\end{table}



\textbf{Heterogeneous vs. Multi-relation. }
In synchronization with the fraud detection task, we also analyze the performance of GraphNAS and Policy-GNN models in heterogeneous graph and multi-relational graph in the disease detection task. 
From Table~\ref{tab:diabetes_base}, the same phenomenon as the fraud detection task can be obtained, that is, the introduction of the multiple relational graph on the Mimic dataset also has certain advantages compared with the heterogeneous graph. 
However, unlike Yelp and Amazon, GraphNAS and Policy-GNN are generally better than other baselines in terms of accuracy. 
We believe that the cause is the balanced features and labels of the Mimic dataset with different relations, and the higher aggregation requirements of the large-scale dataset for neighbor information.



\textbf{Training Percentage.}
In order to measure the impact of the training ratio on the classification accuracy in the diagnosis of diabetes mellitus task, we still set four different training ratios ranging from 5\% to 40\%. 
It can be seen from Table~\ref{tab:diabetes_base} that the classification accuracy of \RioGNN shows a steady upward trend as the training ratio increases. 
This shows that the training process of \RioGNN has a very positive effect on the final classification accuracy. 
It can be seen intuitively, \RioGNN has successfully achieved a good performance improvement in the process of reinforcement learning recursion by supervised signals, which is also in line with expectations. 
However, the accuracy of some baselines changes unrelated to the training percentage, which means that their training methods have great limitations in this task and fail to improve the performance of their models by increasing the supervised signal learning process. 
It also verifies that \RioGNN has better explainability and can continuously improve the accuracy of diagnosis through learning more node features. 
Consistent with its performance under the previous fraud detection task, \RioGNN still maintains strong stability and explainability and its accuracy rate surpasses the others at each training ratio. 




\begin{table}[t]
    \setlength{\abovecaptionskip}{0.cm}
    \setlength{\belowcaptionskip}{-0.cm}
    \caption{Diabetes diagnosis classification results () compared to \RioGNN variants.}\label{tab:diabetes-variants}
    \centering
    \scalebox{1}{
        \begin{tabular}{p{3cm}<{\centering}|p{2cm}<{\centering}p{2cm}<{\centering}}
            \hline
            \multicolumn{1}{p{3cm}<{\centering}|}{\multirow{2}*{Models}}&\multicolumn{2}{p{4cm}<{\centering}}{\textbf{MIMIC-III}}\\
            \cline{2-3}
            \multicolumn{1}{p{3cm}<{\centering}|}{}&\textbf{AUC}&\textbf{Recall}\\
            \hline
            \multicolumn{1}{p{3cm}<{\centering}|}{\RioGNN}&81.06&72.28\\
            \hline
            \multicolumn{1}{p{3cm}<{\centering}|}{BIO-GNN}&81.29&72.75\\
            \multicolumn{1}{p{3cm}<{\centering}|}{ROO-GNN}&81.01&72.34\\
            \hline
            \multicolumn{1}{p{3cm}<{\centering}|}{RIO-Att}&80.96&72.16\\
            \multicolumn{1}{p{3cm}<{\centering}|}{RIO-Weight}&81.04&72.58\\
            \multicolumn{1}{p{3cm}<{\centering}|}{RIO-Mean}&80.31&77.42\\
            \hline
            \multicolumn{1}{p{3cm}<{\centering}|}{\RioGNN}&\textbf{81.36}&\textbf{72.84}\\
            \hline
        \end{tabular}
    }
\end{table}


\textbf{\RioGNN Variants in Classification.}
In order to further verify the impact of the proposed new mechanism on different tasks, we compare different performances of several variants of \RioGNN under the discrete strategy in the context of diabetes diagnosis. 
We show the experimental results of diagnosis of diabetes mellitus on the MIMIC-III dataset in Table~\ref{tab:diabetes-variants}.
First of all, we compare three variants based on different aggregation methods on the MIMIC-III dataset. 
The results show that \RioGNN is better than the other three variants, while RIO-Weight has better results than the other two, which is consistent with the results on Yelp dataset.
We find that the choice of aggregation method is usually based on a specific dataset. 
As for different dataset, different relations have different ways of influencing the results, which lead to different aggregation methods.
However, whether it is for Yelp, Amazon or MIMIC-III, the results show that \RioGNN is superior to all aggregation variants, while RIO-Weight is second only.
This point has a certain degree of universality.
Next, we also focus on the performance of the \RioGNN variant with two-layer structure, \RioGNN.
Consistent with the previous results on Yelp and Amazon, the two-layer architecture doesn't bring very good performance improvements to the model on MIMIC-III. 
Compared with \RioGNN, it can be found that the increase in the number of layers does not improve the final accuracy, indicating that the use of a single-layer structure based on the label-aware similarity measure for neighbor selection is optimal. 
However, in conjunction with Table~\ref{tab:dataset}, it is noted that MIMIC-III dataset is denser than Yelp and Amazon, thus the effect of \RioGNN is very close to that of \RioGNN. 
This shows that for denser relations, the second-order neighbors found by \RioGNN are more effective in the final result. 
To some extent, for too dense multi-relation graphs, second-order neighbors can be used as supplementary information for first-order neighbors.
Finally, for the Label-aware Similarity Measure section, we observe the variant BIO-GNN without adaptive strategy optimization reinforcement learning algorithm and the single-depth structure variant ROO-GNN without the recursive framework for the RSRL framework again on MIMIC-III. 
It can be found from Table~\ref{tab:diabetes-variants} and Table~\ref{tab:diabetes_base} that ROO-GNN performs significantly better than most baselines and variants, second only to \RioGNN. 
This shows that Bernoulli Multi-armed Bandit (BMAB) algorithm of discrete strategy has strong adaptability in the process of recursively selecting the filtering threshold of relations. 
In contrast, the accuracy rate of ROO-GNN is 0.28\% lower than BIO-GNN. 
Apart from that, Figure~\ref{fig:RSRL provement_mic} shows that as the training epoch increases, ROO-GNN fluctuates greatly in the range of 79.48\%-81.43\%, and can never be stable in a fixed smaller interval as \RioGNN or BIO-GNN. 
That is to say, the multi-depth structure of RioGNN brings better convergence speed and excellent stability than the single-depth variant ROO-GNN while maintaining a higher accuracy rate. 


\begin{table}[h]
    \setlength{\abovecaptionskip}{0.cm}
    \setlength{\belowcaptionskip}{-0.cm}
    \caption{Diabetes diagnosis clustering results () compared to \RioGNN variants.}\label{tab:diabetes-cluster}
    \centering
    \scalebox{1}{
        \begin{tabular}{c|c|c|cc|ccc|c}
            \hline
            \multicolumn{1}{c|}{\multirow{1}*{Dataset}}&\multicolumn{1}{c|}{\multirow{1}*{Metric}}&\multirow{1}*{\RioGNN}&BIO-GNN&ROO-GNN&RIO-Att&RIO-Weight&RIO-Mean&\multirow{1}*{\RioGNN}\\
            \hline
            \multirow{2}*{\textbf{MIMIC-III}}&\textbf{NMI}&19.01&19.81&19.13&17.17&\textbf{20.22}&19.86&20.10\\
            &\textbf{ARI}&7.15&8.27&8.11&6.24&9.01&7.51&\textbf{10.03}\\
            \hline
        \end{tabular}
    }
\end{table}

\textbf{\RioGNN Variants in Clustering. }
Similar to fraud detection, we also perform cluster analysis in the task of disease diagnosis. 
The best results of NMI and ARI within  epochs are recorded in Table~\ref{tab:diabetes-cluster}. 
It can be seen that compared with BIO-GNN and ROO-GNN, \RioGNN brings at least  and  increase in NMI and ARI respectively. 
This proves that the RSRL framework also brings accuracy optimization for dense datasets in the clustering task. 
In addition, RioGNN has better performance on ARI indicators for variants that use different methods for aggregation. 
For the NMI indicator, the RIO-Weight effect has been improved. 
We believe this is because the MIMIC-III dataset has a smaller difference in weight between the relationships compared with Yelp and Amazon. 
Weight can distinguish different classes better than \RioGNN, which directly uses the filtering threshold as the aggregation parameter.


\begin{figure}[h]
\centering
\subfigure[Scores of \RioGNN on MIMIC-III.]{\label{fig:MIMIC-training-RioGNN-a}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=8.3cm]{fig/scores_mic.pdf}
\end{minipage}}\subfigure[Thresholds of \RioGNN on MIMIC-III.]{\label{fig:MIMIC-training-RioGNN-b}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=8.3cm]{fig/Thresholds_mic.pdf}
\end{minipage}}

\subfigure[Scores of BIO-GNN on MIMIC-III.]{\label{fig:MIMIC-training-RioGNN-c}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=8.3cm]{fig/scores_mimic_BMAB.pdf}
\end{minipage}}\subfigure[Thresholds of BIO-GNN on MIMIC-III.]{\label{fig:MIMIC-training-RioGNN-d}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=8.3cm]{fig/Thresholds_mimic_BMAB.pdf}
\end{minipage}}\centering
\caption{The training scores and thresholds of \RioGNN vs BIO-GNN on MIMIC-III.}\label{fig:MIMIC-training-RioGNN}
\end{figure}





\begin{figure}[t]
\centering
\subfigure[AUC of \RioGNN,ROO-GNN and BIO-GNN on MIMIC-III.]{
\begin{minipage}[t]{1\linewidth}
\centering
\includegraphics[width=15.3cm]{fig/Rem_vs_CARE.pdf}
\end{minipage}}\centering
\caption{The effectiveness and necessity of the RSRL framework.}\label{fig:RSRL provement_mic}
\end{figure}

\subsubsection{Explainable Reinforcement Learning Training Process}
This section focuses on the process of reinforcement learning and explains the convergence process of proposed \RioGNN on the MIMIC-III dataset.
We also present a comparative analysis of different variants to further explain the applicability of \RioGNN. 


\textbf{The Effectiveness and Necessity of the \RSRL Framework.}
This part demonstrates the validity and necessity of the proposed framework by comparing \RioGNN with variants and its preliminary version CARE-GNN.
It can be seen from the Figure~\ref{fig:RSRL provement_mic} that \RioGNN performs better than CARE-GNN in almost every epoch, which proves that the \RSRL framework has a positive effect on the final classification results on MIMIC-III.
\RioGNN can effectively filter suspected nodes by building a reinforcement learning tree for each relation and identify suspected nodes more accurately. 
Compared with \RioGNN, the accuracy of ROO-GNN in Figure~\ref{fig:MIMIC-training-RioGNN} fluctuates significantly, and it is difficult to converge to a stable range, which proves the necessity to establish a recursive process.
Through the \RSRL process, the classification accuracy can be maintained in a relatively stable range. 
The experimental results show that \RioGNN with the recursive framework performs better through a more accurate filtering threshold search for each depth.
While ROO-GNN without recursion not only fails to converge within a finite number of epochs but also causes a loss of accuracy.



\textbf{Filter Thresholds. }
To further test the proposed model's filtering performance against suspected neighbors, we deliberately extract four denser relations when designing the MIMIC-III dataset.
Each relation is at least an order of magnitude higher than the Yelp or Amazon dataset, which means that the MIMIC-III dataset is more challenging for \RioGNN's filtering performance.
It can be seen from Figure~\ref{fig:MIMIC-training-RioGNN-b} and Figure~\ref{fig:MIMIC-training-RioGNN-d} that the filtering thresholds of the four relations of \RioGNN on the MIMIC-III dataset are stable at [0.88, 0.96, 0.32, 0.26], while BIO-GNN are stable at [0.35, 0.37, 0.36, 0.37].
Considering that the label similarity and feature similarity of different relations are different, the model's filtering strength for different relations is not the same. 
It is worth noting that there is a certain commonality between the relation filtering strength on \RioGNN and BIO-GNN.
Under RioGNN, the convergence thresholds of V-A-V and V-M-V are relatively similar, while V-P-V and V-D-V are relatively similar. 
Generally speaking, relations with high filtering thresholds can bring more positive guidance to the diagnosis result and are more explainable. 
From another perspective, a higher filtering threshold can prove the explainability and correctness of the selected relation, which is more conducive to us intuitively judging whether the choice of the relation is appropriate enough. 
It is clear from Figure~\ref{fig:MIMIC-training-RioGNN} that these four relations of \RioGNN converge to a stable value within 100 epochs.
This indicates that the algorithm can obtain a set of Nash equalization filter thresholds in a very limited epoch through RSRL framework, and thus has good stability and high efficiency. 






\begin{table}[t]
    \setlength{\abovecaptionskip}{0.cm}
    \setlength{\belowcaptionskip}{-0.cm}
    \caption{Results () compared to different RL algorithms and strengthening strategies.}\label{tab:RL_algorithms}
    \centering
    \scalebox{1}{
        \begin{tabular}{p{0.5cm}<{\centering}|p{2cm}<{\centering}||p{1.5cm}<{\centering}p{1.5cm}<{\centering}|p{2cm}<{\centering}}
            \hline
            \multicolumn{2}{c||}{Methods}&\multirow{1}*{\textbf{Yelp}}&\multirow{1}*{\textbf{Amazon}}&\multirow{1}*{\textbf{MIMIC-III}}\\
            \hline
            \multirow{3}*{\rotatebox{90}{Discrete}}&AC~\cite{konda2000actor}&83.54&\textbf{96.19}&81.36\\
            &DQN~\cite{mnih2015dqn}&84.08&95.13&80.96\\
            &PPO~\cite{schulman2017proximal}&80.52&94.99&80.98\\
            \hline
            \multirow{4}*{\rotatebox{90}{Continuous}}&AC~\cite{konda2000actor}&81.31&94.72&80.98\\
            &DDPG~\cite{lillicrap2019ddpg}&83.80&95.39&81.17\\
            &SAC~\cite{haarnoja2018soft}&80.42&94.76&80.87\\
            &TD3~\cite{scott2018td3}&\textbf{84.18}&95.11&\textbf{81.51}\\
            \hline
        \end{tabular}
    }
\end{table}

\begin{figure}[t]
\centering
\subfigure[Yelp.]{\label{fig:alapha-a}
\begin{minipage}[t]{0.333\linewidth}
\centering
\includegraphics[width=5.3cm]{fig/bar_yelp.pdf}
\end{minipage}}\subfigure[Amazon.]{\label{fig:alapha-b}
\begin{minipage}[t]{0.333\linewidth}
\centering
\includegraphics[width=5.3cm]{fig/bar_amazon.pdf}
\end{minipage}}\centering
\subfigure[MIMIC-III.]{\label{fig:alapha-c}
\begin{minipage}[t]{0.333\linewidth}
\centering
\includegraphics[width=5.3cm]{fig/bar_mimic.pdf}
\end{minipage}}\centering
\caption{Depth and Width for Different Task Scenarios.}\label{fig:alapha}
\end{figure}


\subsection{Versatility Analysis of \RSRL Framework}\label{sec:versatility}
To better adapt to many task-driven scenarios, we implement a general \RSRL reinforcement learning framework. 
In dealing with datasets of different sizes and types, different reinforcement learning algorithms and action space types can be flexibly matched. 
The depth and width of the reinforcement learning tree can be adaptively estimated for each relation. 


\textbf{Algorithms and Action Space for Different Task Scenarios. }
In Section~\ref{sec:fraud-overall} and Section~\ref{sec:diabetes-overall}, we analyze the function variants (Table~\ref{tab:fraud_variants} and Table~\ref{tab:diabetes-variants}) and the difference between reinforcement learning algorithms under \RSRL framework and traditional reinforcement learning (Figure~\ref{fig:fraud-efficiency} and Figure~\ref{fig:RSRL provement_mic}) in terms of accuracy and efficiency through a classical reinforcement learning algorithm, Actor-Critic.
Here, in order to better explore the versatility of \RioGNN for different reinforcement learning algorithms and the applicability of different reinforcement learning algorithms for different tasks, in Table~\ref{tab:RL_algorithms} we select the current mainstream reinforcement learning algorithms for experiments, and record the best AUC value within  epochs. 
In addition, for the \RSRL framework, we proposed two different action spaces for the construction of reinforcement learning forests in Section~\ref{sec:rsrl} to adapt to different task requirements.
Different from the discrete action space, the reinforcement learning framework with continuous action space has continuous precision (that is, the highest floating-point number of the processor) in every action selection of reinforcement learning. 
This difference makes it have a better exploration effect in large-scale dataset.
From the experimental results, PD3 algorithm continuous action space and two sets of networks to update the Q value achieves the best results in both Yelp and MIMIC-III dataset. 
In the Amazon dataset, the best result is obtained from the more basic discrete action space AC. SAC and PPO are at a low level in the three datasets. 
In general, \RioGNN is well-adapted to most reinforcement learning algorithms, and is a versatile framework for different types of dataset and task scenarios.




\textbf{Depth and Width for Different Task Scenarios. }
In Section~\ref{sec:rsrl}, we define a depth and width adaptive parameter  to adjust the size of the action space of each layer of the relation and the depth of the entire relation tree. 
In the previous experiment, we fixedly chose 10 as .
In this section, in order to discuss the impact of the depth and width adaptive parameter on the accuracy and efficiency of the \RioGNN model, we compare and analyze the AUC and convergence epoch sizes of the three dataset under different settings.
As shown in Figure~\ref{fig:alapha}, we set the six  values of , , , , , , and respectively record the maximum AUC and the corresponding epoch serial number obtained in  epochs. 
In the Yelp dataset, AUC achieves the maximum value when  is , which is at least  better than other parameters. 
But in this case, it takes longer to reach this value. 
Therefore, we suggest that Yelp can be adaptively chosen  to  or  for efficiency priority and accuracy priority.
On the other hand, Amazon achieves better accuracy when  is  or , and achieves better efficiency when  is  or . 
The difference from the previous two is that the MIMIC-III dataset obtains a loss of accuracy and efficiency when  is .
Judging from these situations, \RioGNN can be adapted to a dataset of different scales and different accuracy and efficiency biases by adjusting . 
This represents the versatility of the dataset size and task requirements.



\begin{table}[h]
    \setlength{\abovecaptionskip}{0.cm}
    \setlength{\belowcaptionskip}{-0.cm}
    \caption{Inductive learning results () compared to \RioGNN variants.}\label{tab:inductive}
    \centering
    \scalebox{1}{
        \begin{tabular}{p{3cm}<{\centering}|p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}|p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}|p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}}
            \hline
            \multicolumn{1}{c|}{\multirow{2}*{Models}}&\multicolumn{3}{c|}{\textbf{Yelp}}&\multicolumn{3}{c|}{\textbf{Amazon}}&\multicolumn{3}{c}{\textbf{MIMIC-III}}\\
            \cline{2-10}
            \multicolumn{1}{c|}{}&\textbf{AUC}&\textbf{Recall}&\textbf{F1}&\textbf{AUC}&\textbf{Recall}&\textbf{F1}&\textbf{AUC}&\textbf{Recall}&\textbf{F1}\\
            \hline
            \multicolumn{1}{c|}{GAT}&55.94&51.79&47.25&72.33&65.86&60.17&63.89&59.13&56.78\\
            \multicolumn{1}{c|}{GraphSAGE}&53.85&51.78&44.36&74.91&70.02&65.32&63.89&69.99&59.24\\
            \hline
            \multicolumn{1}{c|}{\RioGNN}&79.45&71.86&63.58&92.01&83.65&86.24&79.01&69.77&69.64\\
            \hline
            \multicolumn{1}{c|}{BIO-GNN}&79.49&71.86&63.58&\textbf{95.07}&88.19&86.51&81.21&\textbf{72.81}&\textbf{72.64}\\
            \multicolumn{1}{c|}{ROO-GNN}&82.15&74.23&\textbf{67.73}&94.79&87.43&\textbf{88.67}&81.01&72.39&72.23\\
            \hline
            \multicolumn{1}{c|}{RIO-Att}&78.72&71.78&62.38&93.79&\textbf{88.71}&83.72&79.84&71.31&71.28\\
            \multicolumn{1}{c|}{RIO-Weight}&81.06&72.79&65.59&94.67&88.58&85.12&\textbf{81.25}&72.72&72.28\\
            \multicolumn{1}{c|}{RIO-Mean}&78.17&71.41&62.12&93.53&87.32&85.75&80.29&71.92&71.74\\
            \hline
            \multicolumn{1}{c|}{\RioGNN}&\textbf{82.38}&\textbf{75.08}&65.26&94.03&88.58&86.46&81.23&72.63&72.53\\
            \hline
        \end{tabular}
    }
\end{table}






\begin{figure}[t]
\centering
\subfigure[Yelp (pos:neg).]{\label{fig:sample-yelp}
\begin{minipage}[t]{0.333\linewidth}
\centering
\includegraphics[width=5.3cm]{fig/Yelp_upsample.pdf}
\end{minipage}}\subfigure[Amazon (pos:neg).]{\label{fig:sample-amazon}
\begin{minipage}[t]{0.333\linewidth}
\centering
\includegraphics[width=5.3cm]{fig/Amazon_upsample.pdf}
\end{minipage}}\centering
\subfigure[MIMIC-III (pos:neg).]{\label{fig:sample-mimic}
\begin{minipage}[t]{0.333\linewidth}
\centering
\includegraphics[width=5.3cm]{fig/MIMIC_upsample.pdf}
\end{minipage}}\centering

\subfigure[Yelp.]{\label{fig:backtracking-yelp}
\begin{minipage}[t]{0.333\linewidth}
\centering
\includegraphics[width=5.3cm]{fig/Yelp_backtracking.pdf}
\end{minipage}}\subfigure[Amazon.]{\label{fig:backtracking-amazon}
\begin{minipage}[t]{0.333\linewidth}
\centering
\includegraphics[width=5.3cm]{fig/Amazon_backtracking.pdf}
\end{minipage}}\centering
\subfigure[MIMIC-III.]{\label{fig:backtracking-mimic}
\begin{minipage}[t]{0.333\linewidth}
\centering
\includegraphics[width=5.3cm]{fig/MIMIC_backtracking.pdf}
\end{minipage}}\centering

\subfigure[Yelp.]{\label{fig:stopnum-yelp}
\begin{minipage}[t]{0.333\linewidth}
\centering
\includegraphics[width=5.3cm]{fig/Yelp_stopnumber.pdf}
\end{minipage}}\subfigure[Amazon.]{\label{fig:stopnum-amazon}
\begin{minipage}[t]{0.333\linewidth}
\centering
\includegraphics[width=5.3cm]{fig/Amazon_stopnumber.pdf}
\end{minipage}}\centering
\subfigure[MIMIC-III.]{\label{fig:stopnum-mimic}
\begin{minipage}[t]{0.333\linewidth}
\centering
\includegraphics[width=5.3cm]{fig/MIMIC_stopnumber.pdf}
\end{minipage}}\centering
\caption{Parameter sensitivity of under-sampling ratio, with \& without backtracking, deep switching number on Yelp, Amazon and MIMIC-III.}\label{fig:parameter}
\end{figure}


\subsection{Inductive Learning Analysis}\label{sec:inductive}
In this section, we perform inductive learning on \RioGNN, some representative baselines and variant models of \RioGNN.
In the previous experiment, we use transductive learning, that is, the graph passed into the model contains the test nodes. 
In inductive learning, we only pass the adjacency matrix of the nodes that need to be trained into the model, and record the best AUC, Recall and F1 indicators of Yelp and Amazon within  epochs and MIMIC-III within  epochs.
From the results in Table~\ref{tab:inductive}, it can be seen that \RioGNN still has obvious advantages compared with GAT and GraphSAGE, where AUC, Recall and F1 increase by -, -, -.
In addition, \RioGNN has a relatively stable evaluation index among many variants. 
Among them, the results on Yelp dataset are compared with those with transductive learning in Table~\ref{tab:fraud_variants}.
In the inductive learning, the AUC and Recall rate of \RioGNN constantly surpasses ROO-GNN variants although \RioGNN is slightly lower than ROO-GNN in the transductive learning shown in Table~\ref{tab:inductive}.
This represents the stability of the performance of the recursive framework in challenging tasks and illustrates its advantages in small-scale scenarios.
In the Amazon dataset, due to the expansion of the data scale, some variants are better evaluated in some aspects, but \RioGNN is stable at a relatively high level from the comprehensive situation of AUC, Recall and F1. 
This situation similarly appears in MIMIC-III. 
It is worth noting that the BIO-GNN variants in the Amazon and MIMIC-III datasets achieve a good performance improvement compared with their situation in the transductive learning task. 
We believe this is because BMAB has a relatively weak learning ability compared to Actor-Critic, which reduces the dependence of the learned model on the training set, so it is more compatible with newly added nodes.
Overall, \RioGNN has good applicability in both transductive learning and inductive learning.




\subsection{Hyper-parameter Sensitivity}\label{sec:hyper-parameter}
Figure~\ref{fig:parameter} shows the test performance of the three hyper-parameters we introduce in Section~\ref{sec:model-training} in three datasets.
The first row of Figure~\ref{fig:parameter} shows the AUC and Recall of the training set of \RioGNN at different sampling ratios (note that the test results come from an unbalanced test set). 
It can be seen that when the sampling ratio is , that is, when the negative samples are much smaller than the positive samples, overfitting occurs in all three datasets. 
Compared with  and  sampling ratios,  sampling show higher AUC and Recall indicators in all three datasets.
The second row of Figure~\ref{fig:parameter} studies the backtracking structure we set in Section~\ref{sec:rsrl}. 
From Figure~\ref{fig:backtracking-yelp}, Figure~\ref{fig:backtracking-amazon} and Figure~\ref{fig:backtracking-mimic}, models with backtracking settings bring stable performance in all datasets compared to models without backtracking.
In the third row of Figure~\ref{fig:parameter}, we test different depth switching conditions. 
When the deep switching number is set to , AUC and Recall achieve good and balanced performance.





