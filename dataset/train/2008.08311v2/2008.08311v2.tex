

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}



\usepackage{subfig}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{dblfloatfix}    \usepackage{array}
\usepackage[font=small,labelfont=bf]{caption}

\newcommand{\drule}{\specialrule{0.2pt}{1pt}{1pt}\specialrule{0.2pt}{0pt}{\belowrulesep}}
\newenvironment{myindentpar}[1]{\begin{list}{}{\setlength{\leftmargin}{#1}}\item[]}
  {\end{list}}
\usepackage{kotex}
\newcommand{\todoc}[2]{{\textcolor{#1} {\textbf{[[#2]]}}}}
\newcommand{\todoblue}[1]{\todoc{blue}{\textbf{[[#1]]}}}
\newcommand{\modblue}[1]{\todoc{blue}{[[#1]]}}
\newcommand{\todored}[1]{\todoc{red}{\textbf{[[#1]]}}}
\newcommand{\jg}[1]{\todored{JG: #1}}
\newcommand{\sh}[1]{\todoblue{SH: #1}}
\newcommand{\sw}[1]{\todoblue{SW: #1}}
\newcommand{\tr}[1]{\todoblue{TR: #1}}
\newcommand{\shmod}[1]{\modblue{#1}}
\newcommand{\az}[1]{\todoblue{AZ: #1}}
\newcommand{\todo}[1]{\textcolor{blue}{#1}}
\newcommand{\todogreen}[1]{\textcolor{cyan}{#1}}
\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}



\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{100}  

\title{Towards Lightweight Lane Detection by Optimizing Spatial Embedding} 

\begin{comment}
\titlerunning{ECCV-20 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-20 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{Towards Lightweight Lane Detection}
\author{Seokwoo Jung\inst{\text{*}1} \and
Sungha Choi\inst{\text{*}1} \and
Mohammad A. Khan\inst{2} \and
Jaegul Choo\inst{2}}
\authorrunning{S. Jung et al.}
\institute{A\&B Center, LG Electronics,\, Seoul, South Korea \\
\email{\{seokwoo.jung, sungha.choi\}@lge.com}\\
\and
KAIST,\, Daejeon, South Korea\\
\email{muhammedazamkhan@gmail.com, jchoo@kaist.ac.kr}}
\maketitle
\blfootnote{\vspace*{-0.3cm}* indicates equal contribution}

\begin{abstract}
\vspace*{-1.0cm}
A number of lane detection methods depend on a proposal-free instance segmentation because of its adaptability to flexible object shape, occlusion, and real-time application. This paper addresses the problem that pixel embedding in proposal-free instance segmentation based lane detection is difficult to optimize. A translation invariance of convolution, which is one of the supposed strengths, causes challenges in optimizing pixel embedding. In this work, we propose a lane detection method based on proposal-free instance segmentation, directly optimizing spatial embedding of pixels using image coordinate. Our proposed method allows the post-processing step for center localization and optimizes clustering in an end-to-end manner. The proposed method enables real-time lane detection through the simplicity of post-processing and the adoption of a lightweight backbone. Our proposed method demonstrates competitive performance on public lane detection datasets.
\vspace*{-0.3cm}
\keywords{Lane detection, Proposal-free instance segmentation, Spatial embedding, Translation invariance}
\end{abstract}

\vspace*{-0.8cm}
\section{Introduction}
\vspace*{-0.3cm}
Lane marker detection, a fundamental task in autonomous driving, has accomplished substantial advancements through the adoption of convolutional neural networks (CNNs). A few studies formulate the lane marker detection task into semantic segmentation~\cite{chen2018encoder} or instance segmentation~\cite{he2017mask,neven2019instance} problem in terms of segmenting the lane area and classifying the lane to which each pixel belongs. 
Several studies have recently suggested lane detection methods~\cite{hsu2018learning,neven2018towards} based on proposal-free instance segmentation~\cite{neven2019instance,de2017semantic} and demonstrated remarkable performance.
Although the current dominant methods for instance segmentation are proposal-based (i.e., detect-then-segment approach)~\cite{he2017mask},
proposal-free methods are more suitable for lane detection. The proposal-free approaches map pixels into an embedding space utilizing CNNs, and learn embedding to close points if the pixels belong to the same instance. 
These approaches have strengths in long, thin shapes and complex occlusion, which are not suitable for bounding box, and real-time application.
Additionally, several proposal-free approaches~\cite{de2017semantic,neven2018towards} utilize a discriminative loss in the form of triplet loss~\cite{Schroff_2015_CVPR} that pulls samples belonging to the same instance closer and pushes clusters away from each other to facilitate the learning of the pixel embedding.

The proposal-free method can be considered as a coordinate transform problem~\cite{liu2018intriguing}, which requires learning a mapping between coordinates in image pixel and another embedding space. However, CoordConv~\cite{liu2018intriguing} shows that CNNs fail to transform spatial representations between two different types of space. This phenomenon is caused by the translation-invariant nature of convolution, which in turn makes pixel embedding difficult to optimize. Recent studies~\cite{liu2018intriguing,neven2019instance} alleviate the complexity in learning the pixel embedding by exploiting coordinate information of input.

Inspired by these work~\cite{liu2018intriguing,neven2019instance,de2017semantic,neven2018towards}, we propose a novel lane marker detection method based on instance segmentation approach with making cluster-friendly spatial embedding. The contributions of this paper include:

\begin{myindentpar}{0.2cm}
\vspace*{-0.1cm}
\noindent \,To the best of our knowledge, this is the first endeavor that adopts spatial embedding of pixels using image coordinate that overcomes the limitation of translation invariant nature of convolution to solve lane detection task.
\vspace*{-0.1cm}
\ \label{eq:Loss_e}
\nonumber
\mathcal{L}_e = \frac{1}{K}\sum_{k=1}^{K}\mathcal{L}_h(\left\{y \right\}, \left\{ \phi_k(e_i) \right\}), \quad
y =\begin{cases}
 1, & \text{if }\\
 0, & \text{otherwise}
\end{cases} \quad
\forall e_i \in \text{fg}
\vspace*{-0.2cm}
 \label{eq:Loss_e}
\nonumber
\mathcal{L}_b = \frac{1}{K}\sum_{k=1}^{K} \text{max}\left( \sqrt{-2\sigma_k^{2}\ \ln\Pr} - \delta_m , \  0 \right)
\vspace*{-0.2cm}


Additionally, the loss  for inter-cluster push force~\cite{de2017semantic} is adopted to minimize interference between adjacent clusters in embedding space, and the loss ~\cite{neven2019instance} is applied for sampling instance center at inference time.











\begin{table}[b!]
\vspace*{-0.65cm}
\begin{center}
\footnotesize
\renewcommand{\tabcolsep}{1.0mm}
\begin{tabular}{c|c|c|c|c|c}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Acc(\%)} & \multirow{2}{*}{FP} & \multirow{2}{*}{FN} & \multicolumn{2}{c}{Inference time (msec)}\\
& & & & Networks & Post-pro. \\ 
\drule
Baseline (Push + Pull)~\cite{neven2018towards} & 96.42 & 0.0654 & 0.0195 & 6.62 & 498\\
\midrule
Baseline (SE)~\cite{neven2019instance} & 95.96 & 0.0799 & 0.0261 & N/A & N/A\\
\midrule
Ours (SE + FG) & 96.20 & 0.0598 & 0.0256 & N/A & N/A \\
\midrule
Ours (SE + FG + BS + Push) & \textbf{96.58} & \textbf{0.0540} & \textbf{0.0177} & 6.64 & \textbf{3.20} \\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.2cm}
\caption{TuSimple benchmark results for an image size of 1280720. Push, Pull, SE, FG and BS are abbreviations for inter-cluster push, intra-cluser pull, spatial embedding, the embedding loss only for foreground pixels and bandwidth saturation.  denotes our re-implementation of the baseline.  denotes adoption of DBSCAN for post-processing. The inference time is measured on NVIDIA V100 and Xeon Silver 4114.}
\label{tab_best_model}
\end{table}

\vspace*{-0.4cm}
\section{Experiments}
\vspace*{-0.25cm}
In this section, we quantitatively and qualitatively compare the performance of our model with the baseline models~\cite{neven2018towards,neven2019instance} to verify the effectiveness of the proposed model. Our model is superior to the baseline models in terms of accuracy and inference speed (Table~\ref{tab_best_model}). Especially, the proposed method is much faster than the baseline due to simple post-processing. Finally, Fig.~\ref{fig:qualitative} shows the qualitative results compared to the baseline in a real road environment.




\begin{figure*}[ht!]
\vspace*{-0.55cm}
  \centering\includegraphics[width=0.9\linewidth]{eccv2020kit/figures/fig_lane_results2.pdf}
\vspace*{-0.15cm}
  \caption{Qualitative comparison between baseline~\cite{neven2018towards} and ours on real road environment. From top to bottom: input image with predicted lane marker, magnifying the blue boxes in the image of the first row, spatial embedding of the pixels in the input images. Red boxes denote the incorrectly predicted regions.
}
\label{fig:qualitative}
\vspace*{-0.65cm}
\end{figure*}
\vspace*{-0.42cm}
\section{Conclusions}
\vspace*{-0.18cm}
We proposed novel methods for applying spatial embedding to lane detection, and experimentally verified that our methods is efficient compared to state-of-the-arts models. Furthermore, we will enhance our methods through further extensive experiments and for real-time applications in an embedded device.
\vspace*{-0.28cm}
{\footnotesize
\bibliographystyle{splncs04}
\bibliography{egbib}
}
\end{document}
