\documentclass[sigconf]{acmart}
\setcopyright{none}
\settopmatter{printacmref=true} \renewcommand\footnotetextcopyrightpermission[1]{} 

\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}



\acmConference[ACM Multimedia '23]{Ottawa '23: ACM Multimedia}{October 29 – November 3}{Ottawa, Canada}


\acmSubmissionID{898}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}


\usepackage{algorithm, algpseudocode}
\usepackage{multirow}
\usepackage{caption}
\usepackage{comment}
\definecolor{citecolor}{RGB}{119,185,0} 
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\usepackage{pifont}
\usepackage{color}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage[export]{adjustbox}
\usepackage{bm}
\usepackage{stfloats}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}

\def\eg{\emph{e.g.}} 
\def\ie{\emph{i.e.}} 
\def\etal{\emph{et~al.}} 



\begin{document}
\title{Relieving Triplet Ambiguity: Consensus Network for Language-Guided Image Retrieval}

\author{  
Xu Zhang} 
\affiliation{\institution{CCAI, Zhejiang University}
  \city{Hangzhou}
  \country{China}
  }
\email{xu.zhang@zju.edu.cn}

\author{Zhedong Zheng}
\affiliation{
    \institution{Sea-NExT Joint Lab, National University of Singapore}
  \city{Singapore}
  \country{Singapore}
  }
  \email{zdzheng12@gmail.com}
\author{Xiaohan Wang}
    \affiliation{\institution{CCAI, Zhejiang University}
        \city{Hangzhou}
        \country{China}
    }
    \email{wxh1996111@gmail.com}
\author{Yi Yang}
\affiliation{\institution{CCAI, Zhejiang University}
  \city{Hangzhou}
  \country{China}
  }
  \email{yangyics@zju.edu.cn}



\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010224.10010225.10010231</concept_id>
       <concept_desc>Computing methodologies~Visual content-based indexing and retrieval</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010224.10010240.10010241</concept_id>
       <concept_desc>Computing methodologies~Image representations</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010179</concept_id>
       <concept_desc>Computing methodologies~Natural language processing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}
\ccsdesc[500]{Computing methodologies~Visual content-based indexing and retrieval}
\ccsdesc[500]{Computing methodologies~Image representations}
\ccsdesc[500]{Computing methodologies~Natural language processing}

\keywords{Representation Learning, Multi-modal Retrieval, Image Retrieval with Text Feedback, Triplet Ambiguity.}

\begin{abstract}
    Language-guided image retrieval enables users to search for images and interact with the retrieval system more naturally and expressively by using a reference image and a relative caption as a query. Most existing studies mainly focus on designing image-text composition architecture to extract discriminative visual-linguistic relations. Despite great success, we identify an inherent problem that obstructs the extraction of discriminative features and considerably compromises model training: \textbf{triplet ambiguity}. This problem stems from the annotation process wherein annotators view only one triplet at a time. As a result, they often describe simple attributes, such as color, while neglecting fine-grained details like location and style. This leads to multiple false-negative candidates matching the same modification text. We propose a novel Consensus Network (Css-Net) that self-adaptively learns from noisy triplets to minimize the negative effects of triplet ambiguity. Inspired by the psychological finding that groups perform better than individuals, Css-Net comprises 1) a consensus module featuring four distinct compositors that generate diverse fused image-text embeddings and 2) a Kullback-Leibler divergence loss, which fosters learning among the compositors, enabling them to reduce biases learned from noisy triplets and reach a consensus. The decisions from four compositors are weighted during evaluation to further achieve consensus. Comprehensive experiments on three datasets demonstrate that Css-Net can alleviate triplet ambiguity, achieving competitive performance on benchmarks, such as $+2.77\%$ R@10 and $+6.67\%$ R@50 on FashionIQ.
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}
Image retrieval is a fundamental task in computer vision and proves to be valuable in many applications, such as product search \cite{guo2019attentive,sharma2019retrieving,guo2018multi}, internet search \cite{noh2017large} and fashion retrieval \cite{liu2016deepfashion,liao2018interpretable}. Prevalent image retrieval approaches include image-to-image retrieval \cite{wang2018cosface,deng2019arcface,fan2019spherereid,sheng2020mining,feng2020learning} and text-to-image retrieval \cite{ji2017cross,zhen2019deep,zheng2020dual,chun2021probabilistic,guerrero2021cross,wang2022point}, which endeavor to locate the image of interest using a single image or descriptive texts as a query. 
Despite significant progress in image retrieval, users often lack a precise search target in advance but instead seek categories, such as shoes or clothing. Therefore, an interactive system is highly desirable to assist users to reconsider their intentions, as depicted in Fig.~\ref{fig:task}. Hence, Language-guided image retrieval, which aims to search the target image of interest given the composed query consisting of a reference image and the relative caption describing the desired modification, has recently attracted great attention \cite{vo2019composing,chen2020image,lee2021cosmo,kim2021dual,wen2021comprehensive}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/task3.pdf}
    \vspace{-.1in}
    \caption{Illustration of the language-guided image retrieval system. Using a reference image and an associated descriptive sentence, the system endeavors to accurately retrieve the intended target image from candidate images for user convenience.}
    \label{fig:task}
\end{figure}

Recent studies addressing the task of language-guided image retrieval primarily concentrate on extracting discriminative representations from image-text-image triplets. For example, TIRG \cite{vo2019composing}, VAL \cite{chen2020image}, and CoSMo \cite{lee2021cosmo} propose different ways to modify the visual features of the reference image conditioned on the relative caption. TIRG uses a simple gating and residual module, VAL devises a visual-linguistic attention learning framework, and CoSMo introduces the content and style modulators. Additionally, CLVC-Net \cite{wen2021comprehensive} and CLIP4cir \cite{baldrati2022conditioned} devise more intricate multi-modal fusion modules to accentuate the modifications of the reference image. CLVC-Net uses local-wise and global-wise composition modules, while CLIP4cir finetunes the CLIP \cite{radford2021learning} text encoder and trains a combiner network to fuse the visual and textual features.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/intro12.pdf}
    \caption{Illustration of the triplet ambiguity problem. Triplet ambiguity denotes multiple false-negative samples in the dataset. It is due to the annotator usually seeing one triplet with true match (\!~\protect\includegraphics[scale=0.12,valign=t]{figures/right.png}) at a time, while neglecting other candidates (\protect\includegraphics[scale=0.12,valign=t]{figures/question.jpg}) in the whole dataset. Triplet ambiguity largely compromise the traditional metric learning on pushing away other negatives.
    }
    \label{fig:intro}
\end{figure}

Despite the significant success, these works fail to address an inherent problem of the language-guided image retrieval task: the ambiguity of the training data triplets, \ie, \textbf{triplet ambiguity}. Triplet ambiguity originates from the annotation process in which annotators, focusing on single data triplet, frequently describe simple properties such as color, while neglecting more fine-grained details, such as location and style. Consequently, many noisy triplets exist where candidate images meet the requirement of the composed query but are not annotated as the desired ground-truth target image, especially when the relative caption is brief. Fig.~\ref{fig:intro} shows examples that apart from the true match marked with\!~\protect\includegraphics[scale=0.12,valign=t]{figures/right.png}, the other two candidate images marked with \protect\includegraphics[scale=0.12,valign=t]{figures/question.jpg} could also serve as the target image of the composed query. 
The noisy triplets lead to multiple false-negative candidates capable of fulfilling the same modification text, compromising the representation learning of the model. It is because the metric learning objective in this task aims to push away these false-negative samples from the composed query. 
We empirically verify that triplet ambiguity does exist in the language-guided image retrieval task in Sec.~\!\ref{subsec:ambiguity}. Specifically, we compare the batch-based classification (mostly used in previous works) with a global-wise classification.
We find that such a global-wise classification significantly degrades the performance, validating our assumption on triplet ambiguity.

To address the triplet ambiguity problem, we propose a straightforward and effective Consensus Network (Css-Net) for language-guided image retrieval, as illustrated in Fig.~\!\ref{fig:overview}(a). 
The key idea underpinning our method to alleviate the triplet ambiguity is ``two heads are better than one'' in short. To be more specific, an individual often errs due to inherent biases, but groups are less susceptible to making similar mistakes, thereby circumventing sub-optimal solutions. This is known as the psychological finding \cite{hinsz1990cognitive} that groups perform better than individuals on the memory task. Consequently, our goal is to (1)~\!develop a consensus module composed of compositors possessing diverse knowledge to jointly make decisions during evaluation and (2)~\!encourage learning among the compositors to minimize their biases learned on noisy triplets by employing an additional Kullback Leibler divergence loss (KL loss) \cite{kullback1951information}. 

To ensure that the compositors possess distinct knowledge, we differentiate them in two ways: $\bullet$~Motivated by the finding \cite{lin2017feature,miech2021thinking} that the image features of high-resolution are semantically weak, while the image features of low-resolution are semantically strong, we first employ two image-text compositors at different depths of the same image encoder, (\ie, block3 and block4 of the ResNet \cite{he2016deep}). The former focuses more on detailed change like ``has a purple star pattern'', while the latter emphasizes more overall change such as ``is modern and fashional''. $\bullet$~Unlike the image-text compositor that uses relative caption to describe \textbf{what should change} on the reference image, we devise the text-image compositor to capture the textual cues based on text-to-image retrieval, where the reference image implies \textbf{what should preserve}. 
Specifically, we denote the reference image feature as $\bm{f_r}$, the text feature as $\bm{f_s}$, and the composed feature as $\bm{\hat{g}}$. The image-text compositors primarily devised by previous works \cite{vo2019composing,chen2020image,lee2021cosmo,wen2021comprehensive} are in the residual form of $\bm{\hat{g}_{IT}} = \bm{f_r} + comp(\bm{f_r},\bm{f_s})$, where $comp$ represents a function to fuse $\bm{f_r}$ and $\bm{f_s}$. The proposed text-image compositors are in the form of $\bm{\hat{g}_{TI}} = \bm{f_s} + comp(\bm{f_s},\bm{f_r})$ for capturing the textual cues of the query. We incorporate two symmetric text-image compositors at the same depths of the image encoder as image-text compositors. These four compositors share the same image and text encoders but exhibit distinct feature representations based on their respective knowledge. They collaboratively make decisions during evaluation to mitigate the individual biases learned on noisy triplets, which enhances the language-guided image retrieval performance.

To further reduce the negative impact of triplet ambiguity, we impose an additional KL loss between two image-text compositors. The KL loss enables two compositors to learn from each other and reach a consensus. This \textbf{soft label} combined with the respective \textbf{knowledge} from two compositors is more effective than the supervision from one-hot labels, as it helps each compositor to mitigate its own bias learned on noisy triplets and thus prevents the overfitting to the annotated target image. To demonstrate that KL loss provides additional information for compositors, we employ an intuitive label-smoothing approach as the soft label. However, we find that the uniform distribution of soft labels without knowledge does not address the triplet ambiguity due to the high false positive rate problem. In comparison, the KL loss bridges two image-text compositors more flexibly and feasibly.
The experimental results show that Css-Net has achieved competitive performance on three benchmarks, empirically validating the effectiveness of our method.

In summary, our contributions are as follows:

    $\bullet$ We identify an inherent problem in the language-guided image retrieval task and further verify the phenomenon through the preliminary experiments. We observe that the triplet ambiguity leads to sub-optimal model learning (\textit{see Fig.~\ref{fig:twolosses}}). 
    
    $\bullet$ To address triplet ambiguity, we introduce a Consensus Network (Css-Net) featuring a consensus module with four unique compositors for joint inference (\textit{see Table~\ref{tab:joint_infer}}). Moreover, we employ KL loss to facilitate learning among compositors and reduce their biases learned on noisy triplets, making Css-Net more robust to triplet ambiguity. \textit{See results in Table~\ref{tab:diagnostic}.}
    
    $\bullet$ Extensive experiments show that the proposed method minimizes the negative impacts of noisy triplets. On three prevalent public benchmarks, we observe that Css-Net significantly surpasses the current state-of-the-art competitive methods, \eg, with $+2.77\%$ Recall@10 on Shoes, and $+6.67\%$ Recall@50 on FashionIQ (\textit{see Table~\ref{tab:fashioniq},~\ref{tab:shoes},~and~\ref{tab:fashion200k}}).

\section{Related Work}

\subsection{Cross-modal Image Retrieval}

Cross-modal image retrieval is a fundamental task in computer vision that has attracted wide attention from researchers. The most popular patterns of image retrieval are image-to-image matching \cite{wang2018cosface,deng2019arcface,sun2020circle,wu2017RGB,dai2018cross,liu2022learning} and text-to-image matching \cite{lee2018stacked,zhang2020context,zheng2020dual}, which allow users to search for images of interest with a similar image or some descriptive texts as queries. Although these paradigms have made great progress, they do not provide enough convenience for users to express their search intention. Therefore, more forms of image retrieval with flexible queries such as sketch-based image retrieval \cite{guo2017sketch,deng2020progressive,wang2021tcn,tian2021sketch} have emerged. In this work, we focus on the language-guided image retrieval task which involves a composed query of a reference image and a corresponding caption. To tackle this task, recent works \cite{vo2019composing,chen2020learning,yang2021cross,chen2020image,zhang2021heterogeneous,lee2021cosmo,wen2021comprehensive,gu2021image,zhao2022progressive} aim to devise a composition architecture to capture the visual-linguistic relation. For example,  TIRG \cite{vo2019composing} uses a simple gating and residual module, VAL \cite{chen2020image} devises a visual-linguistic attention learning framework, and CoSMo \cite{lee2021cosmo} introduces the content and style modulators. Besides, CLVC-Net \cite{wen2021comprehensive} devises local-wise and global-wise composition modules, resembling model ensemble.
Unlike the methods described above, our Css-Net does not rely on complicated composition modules for learning. Instead, our Css-Net mainly focuses on alleviating the triplet ambiguity problem that leads to a sub-optimal solution for a single compositor. To address this problem, Css-Net trains a consensus module to infer during evaluation and leverage KL loss to reduce individual bias during training. 

\subsection{Attention Mechanism}
The attention mechanism is widely used in language and vision tasks in machine learning to capture the long-range dependencies and the relations between features. This mechanism is also inspired by a psychological finding \cite{corbetta2002control} that humans observe and pay attention to specific parts as needed. In the language-guided image retrieval task, many works use the attention mechanism to design the image-text compositor. For example, VAL \cite{chen2020image} employs self-attention by concatenating the text feature to each location of the image features. CoSMo \cite{lee2021cosmo} adopts the disentangled multi-modal non-local block to stabilize the training procedure of the content modulator. Besides, CLVC-Net \cite{wen2021comprehensive} proposes a complex cross-attention between the feature of each word in the sentence and each spatial location of the image feature. 
In our work, we focus on utilizing several compositors with different knowledge. Without loss of generalizability, we deploy the widely-used CoSMo \cite{lee2021cosmo} as our image-text compositor, which takes the feature map of the reference image and the pooled text feature (sentence-level feature) as input. Moreover, we propose a unique text-image compositor to fully utilize the attention mechanism to capture the relation of the average pooled reference image feature and the word-level text feature, which is orthogonal with existing attention-based models and could further improve the performance.

\subsection{Co-training}
Co-training is a semi-supervised learning technique that exploits two classifiers to acquire complementary information on two views of the data \cite{blum1998combining}. It has been extensively utilized in various research fields such as image recognition \cite{qiao2018deep}, semantic segmentation \cite{peng2020deep} and domain adaptation \cite{saito2018maximum,zheng2019unsupervised,luo2019taking}. For instance, in domain adaptation, these co-training works explicitly maximize the discrepancies of the classifiers by utilizing extra losses such as adversarial loss \cite{saito2018maximum} or weight discrepancy loss \cite{luo2019taking}. In contrast, our work adopts a co-training paradigm that leverages four compositors with different knowledge to jointly make decisions for the language-guided image retrieval task. We do not introduce extra loss to explicitly maximize the discrepancy of the four compositors, as they inherently possess various knowledge due to their different designs. 
For example, the two image-text compositors focus on the detailed and overall changes to the reference images based on the perspective of finding ``what should change'' in the reference image, and two text-image compositors are in view of the text-to-image retrieval with the reference image implying ``what should preserve''. Instead, we explicitly encourage the consensus between compositors and leverage the consensus to rectify the single prediction, which is aligned with this work \cite{guan2021multimodal} exploring the consistent and complementary correlations of multi-modal data. 
Refer to Sec.~\ref{subsec:consensus} for more details.

\section{Method} \label{method}

This section describes the Consensus Network in detail. Sec.~\ref{subsec:overview} introduces the overall framework of the network. Sec.~\ref{subsec:consensus} elaborates on the consensus module consisting of four distinct compositors with diverse knowledge, and the triplet ambiguity resolution. Sec.~\ref{subsec:discussion} discusses Css-Net and some recent and relevant works.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/overview10.pdf}
    \vspace{-.1in}
    \caption{The overview of the Consensus Network. Given a reference image and a relative caption, we first extract the mid-level image feature $\bm{f_r^m}$ and high-level image feature $\bm{f_r^h}$ with the image encoder $F_{img}$, and the text representation $\bm{f_s}$ with the text encoder $F_{text}$. Then, we fuse the text representation with either the mid-level or high-level image feature using compositors. The solid \textcolor{blue!80!black}{blue} lines represent mid-level image features, while dotted \textcolor{blue!80!black}{blue} lines indicate high-level image features and solid \textcolor{green!80!black}{green} lines denote text features. The text-image compositor $F_{TI}$ has the residual form of $\bm{f_s}+F(\bm{f_s},\bm{f_r})$, which takes the word-level text representation $\bm{f_s}$ and the average pooled reference image feature $pool(\bm{f_r})$ as input. The image-text compositor $F_{IT}$ has the residual form of $\bm{f_r}+F(\bm{f_r},\bm{f_s})$ taking the intermediate feature map of the reference image $\bm{f_r}$ and the average pooled sentence-level text feature $pool(\bm{f_s})$ as input. Each compositor generates its own composed feature. We use a simple attention-based multi-modal non-local block for the text-image compositor and employ CoSMo \cite{lee2021cosmo} for the image-text compositor. Finally, we match the composed features with the target image feature to train the model. The projector block consists of an averaging pooling layer (Avgpool) and a multilayer perceptron (MLP).}
    \label{fig:overview}
\end{figure*}

\subsection{Overview of Consensus Network}
\label{subsec:overview}
  
As illustrated in Fig.~\ref{fig:overview} (a), the Consensus Network consists of three components: the image encoder, the text encoder, and the consensus module. The image encoder, $F_{img}$, extracts mid-level and high-level representations of the input images as:

\begin{equation}
    \bm{f^m_r}, \bm{f^h_r} = F_{img}(I_r),
\end{equation}
where $I_r$  is the reference image, and $\bm{f^m_r}, \bm{f^h_r} \in \mathbb{R}^{C_{in} \times (H \times W)}$ refer to the mid-level and high-level image feature, respectively (\ie, output from block3 and block4 of the ResNet \cite{he2016deep}). Note that, since $\bm{f^m_r}$ and $\bm{f^h_r}$ are not used in the same compositor, the symbols with the superscript $m$ in the subsequent formulas correspond to $\bm{f^m_r}$ rather than $\bm{f^h_r}$, and vice versa. $C_{in} \times (H \times W)$ represents the shape of the feature maps. For brevity, we do not distinguish between different shapes of the image feature maps. 
The text encoder, denoted as $F_{text}$, extracts the features of the relative caption as follows:
\begin{equation}
    \bm{f_s} = F_{text}(S),
\end{equation}
where $S$ denotes the relative caption, $\bm{f_s} \in \mathbb{R}^{C_{in}' \times L}$ refers to the word-level representation, and $L$ is the number of words in the caption.

\iffalse
Then, we use average pooling layer followed by a distinct MLP layer, denoted as $pool$ to project the mid-level and high-level feature maps of the reference image onto an embedding space with the dimension of $C$:
\begin{equation}
\label{eq:img_proj}
    \begin{cases}
    &f^m_{x_r} = pool(x_r^m) \\
    &f^h_{x_r} = pool(x_r^h),
    \end{cases}
\end{equation}
where $f^m_{x_r}, f^h_{x_r} \in \mathbb{R}^{C}$. 
Similarly, the text features are projected onto the embedding space with the same dimension:
\begin{equation}
    \begin{cases}
    &f_s^m = pool(s) \\
    &f_s^h = pool(s),
    \end{cases}
\end{equation}
where $f_s^m, f_s^h \in \mathbb{R}^{C}$ are sentence-level representations. To note that the MLP layer are distinguished from each other, although they share the same notation $pool$.
\fi
 
After extracting the image and text features, the consensus module transforms the reference image features with the corresponding text features into the composed features. It consists of four distinct compositors possessing different knowledge. These compositors at different depths of the image encoder can be grouped into two types. Specifically, given the reference image feature $\bm{f_r}$ and the text feature $\bm{f_s}$, the composed query $\bm{\hat{g}}$ can be obtained by either an image-text compositor or a text-image compositor. The image-text compositor has the residual form of $\bm{\hat{g}_{IT}} = \bm{f_r} + comp(\bm{f_r},\bm{f_s})$, which focuses on ``what should change'' to the reference image, while the text-image compositor has the residual form of $\bm{\hat{g}_{TI}} = \bm{f_s} + comp(\bm{f_s},\bm{f_r})$ and emphasizes ``what should preserve'' based on the text-to-image retrieval. Here, $comp$ represents a function to fuse $\bm{f_r}$ and $\bm{f_s}$. Considering both the performance and computational efficiency, the two text-image compositors $F_{TI}^{m}, F_{TI}^{h}$, shown in Fig.~\ref{fig:overview} (b), take the word-level representation $\bm{f_s}$ along with the average pooled reference image features $pool(\bm{f^m_r}), pool(\bm{f^h_r})$ as input, respectively, which are given by:
\begin{equation}
\label{eq:text_comp}
    \begin{cases}
    &\bm{\hat{g}^m_{TI}} = F_{TI}^{m}(\bm{f_s}, pool(\bm{f^m_r})) \\
    &\bm{\hat{g}^h_{TI}} = F_{TI}^{h}(\bm{f_s}, pool(\bm{f^h_r})),
    \end{cases}
\end{equation}
where $\bm{\hat{g}^m_{TI}},~\bm{\hat{g}^h_{TI}}\in\mathbb{R}^C$ are the composed features from text-image compositors. Similarly, the image-text compositors $F_{IT}^m, F_{IT}^h$, shown in Fig.~\ref{fig:overview} (c) take the intermediate image feature maps, $\bm{f_r^m}, \bm{f_r^h}$ along with the pooled sentence-level text representation $pool(\bm{f_s})$ as input, which are given by:
\begin{equation}
\label{eq:img_comp}
    \begin{cases}
    &\bm{\hat{g}_{IT}^m} = F_{IT}^m(\bm{f^m_r}, pool(\bm{f_s})) \\
    &\bm{\hat{g}_{IT}^h} = F_{IT}^h(\bm{f^h_r}, pool(\bm{f_s})),
    \end{cases}
\end{equation}
where $\bm{\hat{g}_{IT}^m},~\bm{\hat{g}_{IT}^h} \in \mathbb{R}^C$ are the composed features from image-text compositors.
The target image features $\bm{f^m_t}, \bm{f^h_t}$ are obtained from the same image encoder $F_{img}$ as the reference image features. Then the projector blocks (composed of an average pooling layer and a multilayer perceptron (MLP)) are employed to acquire the target features: $\bm{g_{TI}^m}$, $\bm{g_{TI}^h}$, $\bm{g_{IT}^m}$, and $\bm{g_{IT}^h}$. The four compositors are trained by reducing the distance between the composed features and their corresponding projected target features within the embedding space.

In the next section, we will explain how these diverse compositors with different knowledge in the consensus module learn to relieve the triplet ambiguity problem.

\subsection{Consensus Module}
\label{subsec:consensus}

To address the triplet ambiguity, we propose the consensus module that consists of four distinct compositors with different knowledge. These compositors are trained to generate the composed query $\bm{\hat{g}}$ that is close to the corresponding target image feature $\bm{g}$ in the feature space. At the evaluation stage, compositors independently compute the similarity between each composed query and all candidate target images in the gallery and collaboratively rank the whole gallery by aggregating the given similarities with different weights. We initially discuss the design of ensuring each compositor acquires distinct knowledge, followed by elucidating how compositors learn from each other to reduce their biases learned on noisy triplets.

\subsubsection{Pyramid Training for Image-Text Compositor}\label{subsubsec:pyramid}

We develop a pyramid training paradigm for image-text compositors, which is inspired by the finding \cite{lin2017feature,miech2021thinking} that the image features of high-resolution are semantically weak, while the image features of low-resolution are semantically strong. Through exploring the different spatial information of the reference image, the two image-text compositors $F^m_{IT}$ and $F^h_{IT}$ independently learn unique knowledge by leveraging the batch-based classification loss, as given by:

\begin{equation}
\label{eq:lossitm}
    \mathcal{L}_{IT}^m = -\log\frac{\exp(\bm{\hat{g}_{IT}^m} \cdot \bm{g_{IT,+}^m)}}{\sum_{j=1}^{B}\exp(\bm{\hat{g}_{IT}^m} \cdot \bm{g_{IT, j}^m} )}
\end{equation}
and
\begin{equation}
\label{eq:lossith}
    \mathcal{L}_{IT}^h = -\log\frac{\exp(\bm{\hat{g}_{IT}^h} \cdot \bm{g_{IT,+}^h)}}{\sum_{j=1}^{B}\exp(\bm{\hat{g}_{IT}^h} \cdot \bm{g_{IT, j}^h} )},
\end{equation}
where $\bm{\hat{g}_{IT}^m}$ and $\bm{\hat{g}_{IT}^h}$ are mid-level and high-level composed features from two image-text compositors (Eq.~\ref{eq:img_comp}). $\bm{g_{IT,+}^m}$ and $\bm{g_{IT,+}^h}$ are corresponding target features obtained from different projectors. The independent batch-based classification loss makes each image-text compositor learn from the interactions between text and different spatial information of the image, which enables these compositors to hold unique knowledge. 

\subsubsection{Auxiliary knowledge from Text-Image Compositor}

The text-image compositor is a brand-new framework for generating the composed feature from the reference image and text, which is seldom referred to in previous works. It offers additional knowledge due to its distinct design from the image-text compositor. The text-image compositor mainly focuses on the text-to-image retrieval with the reference image implying ``what should preserve'', while the image-text compositor finds ``what should change'' in the reference image. We use two symmetric text-image compositors at the same depths of the image encoder to capture different knowledge. The compositors reuse the features from the image encoder $F_{img}$ and the text encoder $F_{text}$with minimal cost. We also apply a batch-based classification loss for each compositor $F^m_{TI}$ and $F^h_{TI}$, as follows:
\begin{equation}
\label{eq:losstim}
    \mathcal{L}_{TI}^m = -\log\frac{\exp(\bm{\hat{g}_{TI}^m} \cdot \bm{g_{TI,+}^m)}}{\sum_{j=1}^{B}\exp(\bm{\hat{g}_{TI}^m} \cdot \bm{g_{TI, j}^m} )}
\end{equation}
and
\begin{equation}
\label{eq:losstih}
    \mathcal{L}_{TI}^h = -\log\frac{\exp(\bm{\hat{g}_{TI}^h} \cdot \bm{g_{TI,+}^h)}}{\sum_{j=1}^{B}\exp(\bm{\hat{g}_{TI}^h} \cdot \bm{g_{TI, j}^h} )},
\end{equation}
where $\bm{\hat{g}_{TI}^m}$ and $\bm{\hat{g}_{TI}^h}$ are composed features from two text-image compositors (Eq.~\ref{eq:text_comp}), respectively. $\bm{g_{TI,+}^m}$ and $\bm{g_{TI,+}^h}$ are corresponding target features obtained from different projector blocks. 

\subsubsection{Collaborative Consensus Learning}
The triplet ambiguity problem causes the compositors to learn from noisy triplets and introduces biases. To mitigate this problem, we use the Kullback Leibler divergence loss (KL loss) for two image-text compositors. The KL loss enables the compositors to learn collaboratively from each other, reducing biases and reaching a consensus. This approach balances the preservation of distinct knowledge and the achievement of consensus. By enhancing cooperation and knowledge sharing, our method is more robust to the triplet ambiguity problem. 
Specifically, we denote the resulting posterior probability of $F_{IT}^m$ as $\bm{p^m}$ and that of $F_{IT}^h$ as $\bm{p^h}$. We set a target probability $\bm{p^w}$ as the weighted sum of both $\bm{p^m}$ and $\bm{p^h}$, which is given by:
\begin{equation}
    \bm{p^w} = \lambda_1 \cdot \bm{p^m} + \lambda_2 \cdot \bm{p^h},
\end{equation}
where $\lambda_{1}$ and $\lambda_{2}$ are two weight coefficients, and thus the KL loss is formulated as:
\begin{equation}
\label{eq:losskl}
    \mathcal{L}_{KL} = D_{KL}(\bm{p^m}||\bm{p^w}) + D_{KL}(\bm{p^h}||\bm{p^w}),
\end{equation}
where $D_{KL}$ is the KL divergence distance. The batch-based classification loss and KL loss play complementary roles in our approach. The application of KL loss minimizes individual biases of compositors with distinct knowledge. It is not essential to incorporate additional KL loss for the two text-image compositors, given their similarities in input. Specifically, both text-image compositors receive pooled reference image features with identical dimensions and share the same text representations. Consequently, the primary function of these text-image compositors is to act as auxiliary decision-makers during joint inference, addressing the triplet ambiguity issue.
The final loss for training is the sum of the above loss functions:
\begin{equation}
    \mathcal{L} = \mathcal{L}^m_{IT}+\mathcal{L}^h_{IT}+\mathcal{L}^m_{TI}+\mathcal{L}^h_{TI}+\mathcal{L}_{KL}
\end{equation}

\subsubsection{Joint Inference}
\label{subsubsec:joint_infer}
We train four distinct compositors to independently learn different knowledge from the data triplets and enable the knowledge transfer to reduce biases learned on noisy triplets. At the evaluation step, we involve each compositor in decision-making to further minimize individual bias. Specifically, we use each compositor to independently generate composed features and measure the similarity between any composed feature and target feature. The resulting similarity matrices are denoted as $P_{IT}^m$, $P_{IT}^h$, $P_{TI}^m$, $P_{TI}^h \in \mathbb{R}^{n_1 \times n_2}$, where $n_1$ and $n_2$ are the number of queries and target images in the gallery. The final similarity matrix for ranking the gallery is the weighted sum of four similarity matrices from distinct compositors:
\begin{equation}
\label{eq:jointinfer}
    P = \alpha_1 \cdot P_{IT}^m + \alpha_2 \cdot P_{IT}^h + \alpha_3 \cdot P_{TI}^m + \alpha_4 \cdot P_{TI}^h,
\end{equation}
where $\alpha_{1} \dots \alpha_{4}$ are weight coefficients. Note that a common practice that concatenates multiple composed features as one query is a special case that all $\alpha$ are equal to $1$.

\subsection{Discussions}
\label{subsec:discussion}

VAL \cite{chen2020image} and CLVC-Net \cite{wen2021comprehensive} are most relevant to our Css-Net. Although VAL employs hierarchical matching strategies, our Css-Net diverges fundamentally in three respects: 1) Facilitating knowledge sharing among compositors at various depths for consensus, as opposed to independent compositors of VAL; 2) Omitting the low-level compositor to enhance performance and efficiency; 3) Implementing a weighted sum during evaluation, enabling adjustable influence of compositors. CLVC-Net incorporates global-wise and local-wise learning through two distinct models, akin to a model ensemble. Conversely, compositors in Css-Net utilize the same encoders but acquire unique knowledge from data triplets, employing a co-training strategy that renders it both effective and efficient. In conclusion, Css-Net represents a novel language-guided image retrieval approach that leverages compositors to learn diverse knowledge from noisy triplets, shares knowledge across compositors to minimize biases, and distinguishes itself from VAL and CLVC-Net.

\section{Experiments} \label{experiments}

This section consists of four parts. Sec.~\ref{subsec:implement} describes the experimental setup. Sec.~\ref{subsec:ambiguity} presents the empirical evidence of the triplet ambiguity problem. Sec.~\ref{subsec:diagnostic} conducts some diagnostic experiments for our Consensus Network. Sec.~\ref{subsec:effective} evaluates the performance of our method and compares it with the state-of-the-art methods. More qualitative results are provided in Supplementary Material.

\subsection{Experimental Setup}
\label{subsec:implement}
\subsubsection{Datasets} 

We evaluate our method on three large-scale language-guided image retrieval datasets: Shoes \cite{berg2010automatic}, FashionIQ \cite{wu2021fashion}, and Fashion200k \cite{vo2019composing}.

The Shoes dataset \cite{berg2010automatic} is originally crawled from $like.com$ for attribute discovery. It is then annotated in the form of a triplet for dialog-based interactive retrieval. We follow VAL \cite{chen2020image} to use $10,000$ training samples and $4,658$ evaluation samples.

The FashionIQ dataset \cite{wu2021fashion} is a language-based interactive fashion retrieval dataset with $77,684$ images across three categories: Dresses, Tops\&Tees, and Shirts. It includes $18,000$ triplets from $46,609$ training images, each containing a reference image, a target image, and two descriptive natural language captions. The evaluation procedure follows VAL  \cite{chen2020image} and CoSMo \cite{lee2021cosmo} for a fair comparison. 

The Fashion200k dataset \cite{han2017automatic} contains over $200k$ fashion images from various websites and is for attribute-based product retrieval. With descriptive attributes for each product, $172k$ images are used for training and $33,480$ test queries for evaluation, following VAL and CoSMo methods. Attributes generate relative descriptions using an online-processing pattern. As shown in Figure~\ref{fig:intro}, we observe that Fahsion200k also meets the triplet ambiguity problem. 

\subsubsection{Implementation Details} 

We modify CoSMo \cite{lee2021cosmo} as our baseline by replacing LSTM \cite{graves2012long} with RoBERTa \cite{liu2019RoBERTa} as the text encoder. ResNet-50 \cite{he2016deep} serves as the image encoder for Shoes and FashionIQ datasets, while ResNet-18 \cite{he2016deep} is used for Fashion200k. Image encoders are pretrained on ImageNet. Embedding space dimension $C$ is $512$, and the output sizes of image feature maps $C_{in} \times (H \times W)$ for ResNet50 are $1024 \times (14 \times 14)$ and $2048 \times (7 \times 7)$. Text feature shape is $C_{in}' \times L$, with $C_{in}'$ being $768$ and $L$ is the sentence length. During training, we set $\lambda_1 = 10$ and $\lambda_2 = 1$, while evaluation uses $\alpha_{1} \dots \alpha_4 = 1, 0.5, 0.5, 0.5$. We adopt the standard evaluation metric in retrieval, \ie, Recall@K, denoted as R@K for short.

We use Adam \cite{kingma2014adam} as the optimizer with $\beta_1 = 0.9$ and $\beta_2 = 0.999$. On Shoes and FashionIQ datasets, the batch size is $30$ and the base learning rate of the text encoder and other modules are $2e-6$ and $2e-5$, respectively. On Fashion200k, the batch size is $126$ and the base learning rate for the text encoder and other modules are $2e-6$ and $2e-4$, respectively. We adopt the warm-up scheme for the first $5$ epochs. The learning rate decays at epoch $35$ and epoch $45$ by a factor of $10$, and the total number of epochs is $50$. 

\subsection{Triplet Ambiguity Verification}
\label{subsec:ambiguity}

\subsubsection{Global-wise \emph{v.s.} Batch-based Optimization}

To verify the negative impacts from the noisy triplets as shown in Fig.~\ref{fig:intro}, we quantitatively compare between global-wise and batch-based optimization objectives. In particular, we mainly adopt $\bullet$ Batch-based Classification (BBC): Limited negatives in the current batch are involved, and $\bullet$ Global-wise Classification (GWC): Mining more negative samples in the training set for comparison.

If the data triplet does \textbf{NOT} have ambiguity, the global-wise classification has the potential to be a comparable even better choice since it uses more negative samples in the training set and potentially learns a better metric, which is consistent with some findings in metric learning \cite{hermans2017defense,sheng2020mining,wang2020cross} and self-supervised learning \cite{chen2020simple,he2020momentum}. Specifically, Consider a composed query $q$ and a set of features/prototypes of the candidate target images  $\{k_0, k_1, ...\}$, there is one true match denoted $k_+$ in the candidates. The two losses are given by:
\begin{equation}
    \mathcal{L}_{BBC} = -\log\frac{\exp(q\cdot k_+))}{\sum_{i=1}^{B}\exp(q\cdot k_i))}
\end{equation}
and
\begin{equation}
    \mathcal{L}_{GWC} =  -\log\frac{\exp(q\cdot k_+))}{\sum_{i=1}^{N}\exp(q\cdot k_i))},
\end{equation}
where $B$ is the batch size, and $N$ is the number of IDs (classes) in the training set. The only difference between them is that $\mathcal{L}_{GWC}$ involves more negative counterparts, which results in high false negative rates if the triplet ambiguity does exist. We conduct experiments on the Shoes dataset \cite{berg2010automatic} using two losses, respectively, under the same settings of CoSMo \cite{lee2021cosmo}. We observe that batch-based methods outperform global-wise methods by a large margin, as shown in Fig.~\ref{fig:twolosses}. The experimental results confirm our triplet ambiguity assumption: the training data contains many noisy triplets (\ie, false negative samples) as Sec.~\ref{sec:intro} discusses, which makes learning on noisy triplets challenging. Although batch-based classification suffers less from triplet ambiguity, the single compositor still faces some noisy negative triplets in the batch and produces a sub-optimal solution.

\subsubsection{Label Smoothing}
\label{subsubsec:smoothing}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/twoloss5.pdf}
    \caption{Comparison between the batch-based classification and the global-wise classification on the Shoes dataset. Batch-based classification discriminates different objects within the batch, while global-wise classification distinguishes all categories, introducing more ambiguous negatives. Obviously, the global-wise classification significantly degrades the performance since more false negative samples from triplet ambiguity are involved.}
    \label{fig:twolosses}
\end{figure}

We first briefly illustrate one intuitive way we consider to alleviate the triplet ambiguity problem: label smoothing. The motivation is that there are many false negative samples due to the triplet ambiguity and label smoothing could alleviate the overfitting to the annotated true match. In label smoothing, the label $\bm{y} = [y_1, \dots y_n]$ is not a hard one-hot label rather than a soft one-hot label, which is given by:
\begin{equation}
y_i = 
\begin{cases}
&1~(if~i =  c) \\
&0~(if~i\ne c)
\end{cases}
\Longrightarrow 
y_i = 
\begin{cases}
&1-\varepsilon ~(if~i =  c) \\
&\frac{\varepsilon}{B-1} ~(if~i\ne c),
\end{cases}
\end{equation}
where $y_i$ is the label for class $i$, $c$ is the corresponding class of the query, $B$ is the batch size, and $\varepsilon$ is a hyperparameter for label smoothing and is set to be $0.1$. We use label smoothing for both the batch-based classification and the global-wise classification, and perform the experiments on the Shoes dataset, which are presented in Fig.~\ref{fig:twolosses}. The experimental results indicate that label smoothing deteriorates the performance of batch-based classification but enhances the performance of global-wise classification. This is because $\bullet$~global-wise classification is severely affected by triplet ambiguity since there are always false negative samples during learning, while batch-based classification is affected only when noisy negative triplets are in the batch; $\bullet$~Label smoothing could alleviate the triplet ambiguity but also introduce another problem that many true negative target samples are assigned weights to learn, which impairs the model training for batch-based classification. The experimental results also verify the effectiveness of KL loss used in this work.


\subsection{Diagnostic Experiments}
\label{subsec:diagnostic}


\begin{table}[t]
\caption{Comparison of various pyramid training methods on the Shoes dataset, which are trained and evaluated independently.}
\label{tab:pyramid}
\setlength\tabcolsep{20pt}
  \resizebox{\linewidth}{!}{
\begin{tabular}{@{}lccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{Shoes} \\ \cmidrule(l){2-4} 
       & R@1     & R@10   & R@50   \\ \midrule
$F_{IT}^h$  & 17.27   &52.26   &77.35   \\
$F_{IT}^l+F_{IT}^h$    & 18.24 & 52.14  & 78.12  \\ 
$F_{IT}^l+F_{IT}^m+F_{IT}^h$   & 18.81   & 54.21  & 79.55  \\ 
$F_{IT}^m+F_{IT}^h$    & 19.10   &54.69   &79.63   \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[t]
\centering
  \caption{Efficacy of model designs. The experiments are conducted on the Shoes dataset under the same setting.}
  \label{tab:diagnostic}
  \setlength\tabcolsep{2pt}
  \resizebox{\linewidth}{!}{
\begin{tabular}{@{}lccccc}
\toprule
$\mathcal{L}_{IT}^m$ &  $\mathcal{L}_{TI}^h+L_{TI}^m$ &  $\mathcal{L}_{KL}$  & \multicolumn{3}{c}{Shoes} \\ \cmidrule(l){4-6} 
Eq.~\ref{eq:lossith} &Eq.~\ref{eq:losstim}\&\ref{eq:losstih} &Eq.~\ref{eq:losskl} & R@1     & R@10   & R@50   \\ \midrule
\multicolumn{2}{l}{\hspace*{-4pt}Baseline: only $\mathcal{L}_{IT}^h$}(Eq.~\ref{eq:lossitm}) &  & 17.27    & 52.26  & 77.35  \\ \midrule
$\bm{\checkmark}$& &     & 19.10\textcolor{green!70!black}{(+1.83)}  & 54.69\textcolor{green!70!black}{(+2.43)}  & 79.63\textcolor{green!70!black}{(+2.28)}  \\ 
$\bm{\checkmark}$&$\bm{\checkmark}$ &   & 19.47\textcolor{green!70!black}{(+2.20)}   & 54.63\textcolor{green!70!black}{(+2.37)}  & 80.46\textcolor{green!70!black}{(+3.11)}  \\
$\bm{\checkmark}$&$\bm{\checkmark}$ & $\bm{\checkmark}$  & 20.13\textcolor{green!70!black}{(+2.86)}       & 56.81\textcolor{green!70!black}{(+4.55)}  & 81.32\textcolor{green!70!black}{(+3.97)}  \\ \bottomrule
\end{tabular}
}
\end{table}

\begin{table}[t]
\caption{Effect of joint inference. We train the whole model on the Shoes dataset and separately evaluate each component.}
\label{tab:joint_infer}
\setlength\tabcolsep{16pt}
  \resizebox{\linewidth}{!}{
\begin{tabular}{@{}lccc}
\toprule
\multirow{2}{*}{Inference Method} & \multicolumn{3}{c}{Shoes} \\ \cmidrule(l){2-4} 
       & R@1     & R@10   & R@50   \\ \midrule
$F_{IT}^m$    & 15.72   &51.17   &78.89   \\
$F_{IT}^h $   & 18.35  & 55.15            &80.52   \\
$F_{TI}^m$     & 17.06  & 53.35            &78.92    \\
$F_{TI}^h$     & 16.58  & 52.17            &77.77    \\ \midrule
Joint Inference (Eq.~\ref{eq:jointinfer})     & 20.13  & 56.81            &81.32    \\
\bottomrule
\end{tabular} }
\end{table}

\begin{table*}[ht]
\caption{Quantitative results of language-guided image retrieval on the FashionIQ dataset. The best results are in \textbf{bold}. They symbol * marks an updated version by the same authors. The symbol $\dagger$ indicates that this method deploys model ensemble.}
\label{tab:fashioniq}
\centering
\setlength\tabcolsep{16pt}
  \resizebox{\linewidth}{!}{
\begin{tabular}{@{}lccccccccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Image Encoder} & \multicolumn{2}{c|}{Dress}        & \multicolumn{2}{c|}{Shirt}          & \multicolumn{2}{c|}{Toptee}         & \multicolumn{2}{c}{Average}        \\ \cmidrule(l){3-10} 
       & & \multicolumn{1}{c}{R@10} & \multicolumn{1}{c|}{R@50} & \multicolumn{1}{c}{R@10} & \multicolumn{1}{c|}{R@50} & \multicolumn{1}{c}{R@10} & \multicolumn{1}{c|}{R@50} & \multicolumn{1}{c}{R@10} & \multicolumn{1}{c}{R@50} \\ \midrule
MRN \cite{kim2016multimodal}&ResNet-152 & 12.32   & 32.18    & 15.88   & 34.33    & 18.11   & 36.33    & 15.44   & 34.28   \\
FiLM \cite{perez2018film}&ResNet-50   & 14.23   & 33.34    & 15.04   & 34.09    & 17.30   & 37.68    & 15.52  & 35.04   \\
TIRG \cite{vo2019composing}&ResNet-17   & 14.87   & 34.66    & 18.26   & 37.89    & 19.08   & 39.62    & 17.40   & 37.39   \\
VAL \cite{chen2020image} &ResNet-50   & 21.12   & 42.19    & 21.03   & 43.44    & 25.64   & 49.49    & 22.60    & 45.04   \\
DCNet \cite{kim2021dual}&ResNet-50  & 28.95   & 56.07    & 23.95   & 47.30     & 30.44   & 58.29    & 27.78   & 53.89   \\
CoSMo* \cite{lee2021cosmo}& ResNet-50 & 26.45   & 52.43    & 26.94   & 52.99    & 31.95   & 62.09    & 28.45   & 55.84   \\
CLVC-Net$\dagger$  \cite{wen2021comprehensive}&ResNet-50$\times$2  & 29.85   & 56.47    & 28.75   & 54.76    & 33.50    & 64.00    & 30.70    & 58.41   \\
ARTEMIS \cite{delmas2022artemis}&ResNet-50 &27.16   & 52.40    & 21.78   & 54.83    & 29.20   & 43.64    & 26.05   & 50.29   \\ 
CLIP4Cir \cite{baldrati2022conditioned} &ResNet-50 &31.73 &56.02 &35.77 &57.02 &36.46 &62.77 &34.65 &58.60 \\ \midrule
Baseline &ResNet-50 &30.95 	&56.98	&31.48	&59.98  &36.97	&67.31		&33.13	&61.42 \\
Css-Net &ResNet-50 & \textbf{33.65}   & \textbf{63.16}    & \textbf{35.96}   & \textbf{61.96}    & \textbf{42.65}   & \textbf{70.70}     & \textbf{37.42}   & \textbf{65.27}   \\ \bottomrule
\end{tabular}
}
\end{table*}

\begin{table}[ht]
\caption{Quantitative results on the Shoes dataset. The best results are in \textbf{bold}. The symbol $\dagger$ denotes model ensemble method.}
\label{tab:shoes}
\small
\setlength\tabcolsep{18pt}
  \resizebox{\linewidth}{!}{
\begin{tabular}{@{}lccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{Shoes} \\ \cmidrule(l){2-4} 
       & R@1     & R@10   & R@50   \\ \midrule
MRN \cite{kim2016multimodal}    & 11.74    & 41.70  & 67.01  \\
FiLM \cite{perez2018film}   & 10.19    & 38.89  & 68.30  \\
TIRG \cite{vo2019composing}   & 12.6    & 45.45  & 69.39  \\
VAL \cite{chen2020image}    & 16.49   & 49.12  & 73.53  \\
CoSMo \cite{lee2021cosmo}  & 16.72   & 48.36  & 75.64  \\
DCNet \cite{kim2021dual}  & -       & 53.82  & 79.33  \\
CLVC-Net$\dagger$ \cite{wen2021comprehensive}  & 17.64   & 54.39  & 79.47  \\
ARTEMIS \cite{delmas2022artemis} & 18.72   & 53.11  & 79.31  \\ \midrule
Baseline                & 17.27   &52.26   &77.35   \\
Css-Net& \textbf{20.13}   & \textbf{56.81}  & \textbf{81.32}  \\ \bottomrule
\end{tabular}
}
\end{table}


\subsubsection{Pyramid Training}
\label{subsec:layer2}

In Sec.~\ref{subsubsec:pyramid}, we present the design of the pyramid training, which exploits the image features from the mid-level and high-level blocks of the image encoder. In this section, we verify its effectiveness by comparing it with different designs. Table \ref{tab:pyramid} reports the experimental results. Our base model is $F_{IT}^m+F_{IT}^h$ used in Css-Net, which applies pyramid training on mid-level and high-level features. We conduct experiments on two variants for pyramid training: 1) $F_{IT}^l+F_{IT}^h$, which uses the image features from block2 and block4 of the ResNet, and 2) $F_{IT}^l+F_{IT}^m+F_{IT}^h$, which uses three image-text compositors at three blocks to generate the composed query. Both variants perform worse than Css-Net, e.g., $-2.55\%$ and $-0.48\%$ on the R@10 evaluation metric. However, they both surpass $F_{IT}^h$ using only one image-text compositor at block4. These results indicate that 1) the low-level image feature is too semantically weak for pyramid training, and 2) groups perform better than individuals. 

\subsubsection{Efficacy of Model Designs}

Table~\ref{tab:diagnostic} shows the effectiveness of our core idea, which uses four different compositors with KL loss to address the triplet ambiguity problem. We make three observations from the table. First, employing image-text compositors at other layers of the image encoder (\ie, $\mathcal{L}_{IT}^m$ in Eq.~\ref{eq:lossith}) can reduce the triplet ambiguity problem and improve the performance significantly ($77.35\% \rightarrow 79.63\%$ at R@50 metric). This indicates that two image-text compositors can benefit from the interactions between the relative caption and different spatial information of the reference image. Second, adding a new compositor framework, text-image compositor, to this task (\ie, $\mathcal{L}_{TI}^m+\mathcal{L}_{TI}^h$ in Eq.~\ref{eq:losstim}\&\ref{eq:losstih}) can further improve the performance ($79.63\% \rightarrow 80.46\%$ at R@50 metric). This demonstrates the advantage of the novel text-image compositors. Third, applying an extra KL loss for the posterior probability from two image-text compositors ($\mathcal{L}_{KL}$ in Eq.~\ref{eq:losskl})  can enhance the performance notably ($80.46\% \rightarrow 81.32\%$ at R@50 metric). This suggests that the KL loss enables two image-text compositors to share and learn from their respective knowledge, thus minimizing the biases.

\subsubsection{Effect of Joint Inference}

At the evaluation stage, Css-Net makes compositors jointly make the decision as introduced in Sec.~\ref{subsubsec:joint_infer}. Table~\ref{tab:joint_infer} shows the experimental results. It is observed that joint inference surpasses every single compositor and verifies our motivation that groups perform better than individuals and could be used to reduce their own biases mainly caused by triplet ambiguity. 

\begin{table}[t]
\caption{Quantitative results on Fashion200k dataset. The best results are in \textbf{bold}. The symbol $\dagger$ denotes model ensemble method.}
\label{tab:fashion200k}
\setlength\tabcolsep{18pt}
\small
  \resizebox{\linewidth}{!}{
\begin{tabular}{@{}lccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{Fashion200k} \\ \cmidrule(l){2-4} 
       & R@1       & R@10     & R@50     \\ \midrule
MRN \cite{kim2016multimodal}    & 13.4    & 40.0  & 61.9  \\
FiLM \cite{perez2018film}   & 12.9    & 39.5  & 61.9  \\
TIRG \cite{vo2019composing}   & 14.1      & 42.5     & 63.8     \\
VAL \cite{chen2020image}    & 21.2      & 49       & 68.8     \\
DCNet \cite{kim2021dual}  & -         & 46.9     & 67.6     \\
CoSMo \cite{lee2021cosmo}  & 23.3      & 50.4     & 69.3     \\
CLVC-Net$\dagger$ \cite{wen2021comprehensive}                & 22.6      & \textbf{53.0}      & \textbf{72.2}     \\
ARTEMIS \cite{delmas2022artemis}     & 21.5      & 51.1     & 70.5     \\ \midrule
Baseline                & 20.9      & 47.7     & 67.8     \\
Css-Net& 22.2      & 50.5     & 69.7                        \\
Css-Net$\dagger$ & \textbf{23.4}     & 52.0     & 72.0                       \\ \bottomrule
\end{tabular}
}
\end{table}

\subsection{The Effectiveness of Our Method}
\label{subsec:effective}

We present the experimental results in Table~\ref{tab:fashioniq}, Table~\ref{tab:shoes}, and Table~\ref{tab:fashion200k}. 
We could make two observations: \textbf{(1) We adopt a competitive baseline with few modifications.} As mentioned in Sec.~\ref{subsec:implement}, we adopt the CoSMo as our baseline and replace the LSTM with a more robust text encoder: RoBERTa, and observe consistent improvement. For example, on the FashionIQ dataset, our baseline improves CoSMo by $4.68\%$ R@10 on average, and surpasses CoSMo by $3.90\%$ R@10 on the Shoes dataset. We infer that RoBERTa is more robust than LSTM \cite{hochreiter1997long} to more accurately capture the textual information. However, our baseline is slightly lower than the reported results of CoSMo on Fashion200k, as the authors do not provide sufficient implementation details for reproducing. This also limits comparing our method with CQBIR \cite{zhang2022comprehensive}, whose baseline uses faster RCNN \cite{girshick2015fast} as a different image encoder. Nevertheless, our method is more effective than CQBIR on FashionIQ and Shoes, where the triplet ambiguity problem is more serious.
\textbf{(2) The proposed Css-Net could further improve and advances the state of the art on such a strong baseline, verifying the effectiveness of Css-Net.} For example, Table \ref{tab:fashioniq} shows Css-Net improves retrieval accuracy on all FashionIQ subsets. Compared to the baseline, it gains $+2.70\%$ R@10 on Dress, $+4.48\%$ R@10 on Shirt, and $+5.68\%$ R@10 on TopTee. Compared to previous works, our method brings overall improvements (e.g., $+2.77\%$ R@10 and $+6.67\%$ R@50 on average by CLIP4Cir). The improvements are significant and empirically validate the effectiveness of Css-Net for handling the triplet ambiguity problem. Besides in Table \ref{tab:shoes}, Css-Net surpasses the state-of-the-art (CLVC-Net) on the Shoes dataset, achieving improvements of $+2.49\%$ R@1 and $+2.42\%$ R@10, which further demonstrates that Css-Net is robust across different datasets. Table~\ref{tab:fashion200k} presents Fashion200k results. Although our baseline is below the reported results of CosMo because of insufficient implementation details for reproduction, Css-Net brings a considerable improvement (\eg, $+2.8\%$ R@10 over the baseline ) and is still competitive with many state-of-the-art works especially when applying the model ensemble (\eg, $+4.3\%$ R@10 by the baseline).


\section{Conclusion} \label{conclusion}

We present a Consensus Network (Css-Net) for language-guided image retrieval. Css-Net aims to relieve the inherent triplet ambiguity problem, which arises when the dataset contains multiple false-negative candidates that match the same query text. This problem stems from annotators overlooking fine-grained details of the images and describing only simple properties. The resulting noisy triplets significantly compromise the metric learning objective. To alleviate this problem, Css-Net employs a consensus module with four diverse compositors that possess different knowledge and can learn mutually during training and infer collaboratively when evaluation. Specifically, Css-Net adopts a pyramid training paradigm and auxiliary text-image compositor design that endow each compositor with unique knowledge. Css-Net also utilizes a KL loss that facilitates the learning among the compositors and reduces their biases learned on noisy triplets. Our experiments show that Css-Net is a competitive method on three benchmarks, demonstrating its effectiveness and robustness. Moreover, Css-Net is orthogonal and complementary to most existing methods, and can further enhance their performance. As future work, we plan to extend our method to real-world applications that involve learning from noisy triplets.


{\small
\bibliographystyle{ACM-Reference-Format}
\bibliography{egbib}
}

\clearpage
\appendix
\section*{Appendix}
\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}
\begin{figure*}
    \centering
    \vspace{-.15in}
    \includegraphics[width=0.95\linewidth]{figures/retrieve3.pdf}
    \vspace{-.15in}
    \caption{Top-10 retrieval results on three datasets. The queries consist of a reference image and a relative caption that describes the desired modification. The \textcolor{blue}{blue}/\textcolor{green!70!black}{green} boxes refer to the reference image and the true match(es) in the database.}
    \label{fig:top10}
\end{figure*}

\section{Further Qualitative Analysis}
\label{subsec:qualitative}
Figure~\ref{fig:top10} shows the top-10 retrieval results on three datasets. We make three key observations from these results: (1) Css-Net can capture the information of the reference image and the relative caption for both coarse-grained and fine-grained queries. For example, the first query of Shoes and the third query of FashionIQ retrieve the correct matches easily, and the first and second queries of FashionIQ also find the correct matches. (2) The model sometimes fails to retrieve the correct matches due to the triplet ambiguity problem, \eg, the first query of Fashion200K retrieves some negative samples but are highly related to the query. (3) Css-Net is less sensitive to some detailed information such as location. For example, the third query in Shoes retrieves a shoe that is visually similar but has a wrong paid location, because the dataset has few similar training samples. Improving the model’s sensitivity to the detailed information is a direction for our future work. \clearpage

\end{document}
