\documentclass[twoside,11pt]{article}



\usepackage[preprint]{jmlr2e}

\usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{thm-restate}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}

\usepackage{setspace}
\usepackage{xspace}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{wrapfig}
\usepackage{array} \usepackage{multirow}
\usepackage{enumitem}
\usepackage{mdframed}
\usepackage{relsize}
\usepackage{caption}
\usepackage{verbatim}



\newcommand{\blue}[1]{\textcolor{MidnightBlue}{#1}}



\usepackage[nameinlink]{cleveref}

\newcommand{\lui}[1]{{\color{red}#1}}
\renewcommand{\figurename}{Fig.}

\newcommand{\eps}{\varepsilon}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\hat}{\widehat}
\renewcommand{\tilde}{\widetilde}
\renewcommand{\bar}{\overline}

\newcommand{\cO}{\mathcal{O}}
\newcommand{\tcO}{\widetilde{\cO}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\tOO}{\wt{\OO}}

\let\originalleft\left
\let\originalright\right




\def\:#1{\protect \ifmmode {\mathbf{#1}} \else {\textbf{#1}} \fi}
\newcommand{\CommaBin}{\mathbin{\raisebox{0.5ex}{,}}}

\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\wo}[1]{\overline{#1}}
\newcommand{\wb}[1]{\overline{#1}}

\newcommand{\bPhi}{\mathbf{\Upphi}}
\newcommand{\bphi}{\mathbf{\upphi}}
\newcommand{\bPsi}{\mathbf{\Uppsi}}
\newcommand{\bpsi}{\mathbf{\uppsi}}
\newcommand{\bSigma}{\bm{\Upsigma}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPi}{\bm{\Uppi}}
\newcommand{\bpi}{\bm{\uppi}}
\newcommand{\bGamma}{\bm{\Upgamma}}
\newcommand{\bgamma}{\bm{\upgamma}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bupsilon}{\boldsymbol{\upsilon}}

\newcommand{\bzero}{\mathbf{0}}


\newcommand{\abx}{\wt{\bx}}

\newcommand{\bA}{{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\widehat{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{{Z}}



\newcommand{\nystrom}{Nystr\"{o}m\xspace}
\newcommand{\lowner}{L\"{o}wner\xspace}
\newcommand{\squeak}{\textsc{SQUEAK}\xspace}
\newcommand{\disqueak}{\textsc{DISQUEAK}\xspace}
\newcommand{\kors}{\textsc{KORS}\xspace}

\newcommand{\oraclerls}{\textsc{Oracle-RLS}\xspace}
\newcommand{\exactrls}{\textsc{Exact-RLS}\xspace}
\newcommand{\twosteprls}{\textsc{Two-Step-RLS}\xspace}
\newcommand{\manysteprls}{\textsc{Many-Step-RLS}\xspace}
\newcommand{\recursiverls}{\textsc{Recursive-RLS}\xspace}
\newcommand{\streamoraclerlsnorem}{\textsc{SOracle-RLS-NoRem}\xspace}
\newcommand{\streamoraclerlsrem}{\textsc{SOracle-RLS-Rem}\xspace}
\newcommand{\uniformsample}{\textsc{Uniform}\xspace}
\newcommand{\uniformdist}{Uniform}
\newcommand{\binomialdist}{Bin}
\newcommand{\multinomialdist}{Multinomial}


\newcommand{\batchsample}{\textsc{Batch-Sample}\xspace}
\newcommand{\inkoracle}{\textsc{INK-Oracle}\xspace}
\newcommand{\inkestimate}{\textsc{INK-Estimate}\xspace}
\newcommand{\shrinkexpandop}{\textsc{Shrink-Expand}\xspace}
\newcommand{\updatedictop}{\textsc{Dict-Update}\xspace}
\newcommand{\mergedictop}{\textsc{Dict-Merge}\xspace}
\newcommand{\shrinkop}{\textsc{Shrink}\xspace}
\newcommand{\expandop}{\textsc{Expand}\xspace}
\newcommand{\skipop}{\textsc{Skip}\xspace}

\newcommand{\hfssparse}{\textsc{Sparse-HFS}\xspace}
\newcommand{\hfsstable}{\textsc{Stable-HFS}\xspace}
\newcommand{\hfs}{\textsc{HFS}\xspace}
\newcommand{\simplehfs}{\textsc{Simple-HFS}\xspace}
\newcommand{\subs}{\textsc{SubSampling}\xspace}
\newcommand{\fergus}{\textsc{EigFun}\xspace}
\newcommand{\onenn}{\textsc{1-NN}\xspace}

\newcommand{\onlinesqueak}{\textsc{KORS}\xspace}
\newcommand{\kons}{\textsc{KONS}\xspace}
\newcommand{\ons}{\textsc{ONS}\xspace}
\newcommand{\conskons}{\textsc{Con-KONS}\xspace}
\newcommand{\bskons}{\textsc{B-KONS}\xspace}
\newcommand{\batch}{\textsc{BATCH}\xspace}
\newcommand{\prosnkons}{\textsc{PROS-N-KONS}\xspace}
\newcommand{\icmlskons}{\textsc{Sketched-KONS}\xspace}
\newcommand{\fixskons}{\textsc{Fixed-SKONS}\xspace}
\newcommand{\nysgd}{\textsc{NOGD}\xspace}
\newcommand{\fogd}{\textsc{FOGD}\xspace}
\newcommand{\dualgd}{\textsc{Dual-SGD}\xspace}

\newcommand{\exactkons}{\kons} \newcommand{\sketchkons}{\icmlskons}

\newcommand{\dictpool}{\mathcal{S}}
\newcommand{\coldict}{\mathcal{I}}
\newcommand{\epsaccurate}{-accurate\xspace}
\newcommand{\epsgamaccurate}{-accurate\xspace}

\newcommand{\coherence}{\mu}
\newcommand*{\MyDef}{\mathrm{\tiny def}}
\newcommand*{\eqdefU}{\ensuremath{\mathop{\overset{\MyDef}{=}}}}\newcommand*{\eqdef}{\mathop{\overset{\MyDef}{\resizebox{\widthof{\eqdefU}}{\heightof{=}}{=}}}}
\newcommand{\projid}{\scalebox{1.15}{\ensuremath{\mathtt{I}}}}
\newcommand{\pmat}{\bP}
\newcommand{\mmat}{\bM}
\newcommand{\pimat}{\bPi}
\newcommand{\pivec}{\bpi}
\newcommand{\gammamat}{\bGamma}
\newcommand{\gammavec}{\bgamma}
\newcommand{\phimat}[1]{{{\Phi}(\bX_{#1})}}
\newcommand{\aphivec}{\wt{\phivec}}
\newcommand{\psimat}{\bPsi}
\newcommand{\psivec}{\bpsi}
\newcommand{\vmat}{\bC}
\newcommand{\vvec}{\bc}
\newcommand{\amat}{\bA}
\newcommand{\avec}{\ba}
\newcommand{\nhl}{\nu}
\newcommand{\qbar}{\wb{q}}
\newcommand{\alphacc}{\rho}



\newcommand{\mergebadevent}{ \mathcal{E}}



\newcounter{cnt-lem-quad-variation}
\setcounter{cnt-lem-quad-variation}{1}

\newcommand{\krrlambda}{\lambda}
\newcommand{\hfslambda}{\eta}

\newcommand{\skonsuniform}{\beta}

\newcommand{\bwavec}{\bomega}
\newcommand{\buavec}{\boldsymbol{\upsilon}}
\newcommand{\bvavec}{\boldsymbol{\theta}}
\newcommand{\feasibleset}{\mathcal{S}}
\newcommand{\dictentry}{\bx}
\newcommand{\loco}{OL\xspace}
\newcommand{\koco}{OKL\xspace}

\newcommand{\oD}{\overline{D}}
\newcommand{\oX}{\overline{X}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calP}{\mathcal{P}}



\newcommand{\wrt}{w.r.t.\@\xspace}
\newcommand{\whp}{w.h.p.\@\xspace}
\newcommand{\ie}{i.e.,\@\xspace}
\newcommand{\eg}{e.g.,\@\xspace}
\newcommand{\wlogen}{w.l.o.g.\@\xspace}
\newcommand{\defeq}{\stackrel{\mathclap{\normalfont\mbox{\tiny def}}}{=}}
\newcommand{\argmin}[1]{\mathop{\operatorname{argmin}}_{#1}}
\newcommand{\argmax}[1]{\mathop{\operatorname{argmax}}_{#1}}
\newcommand{\maxund}[1]{\max\limits_{#1}}
\newcommand{\minund}[1]{\min\limits_{#1}}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\bigotime}{\mathcal{O}}
\newcommand{\abigotime}{\wt{\mathcal{O}}}
\newcommand{\bigomegatime}{\Omega}

\newcommand{\nor}[1]{\left\Vert #1 \right\Vert}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normsmall}[1]{\Vert #1 \Vert}
\newcommand{\transpose}{^\mathsf{\scriptscriptstyle T}}
\newcommand{\transp}{\mathsf{\scriptscriptstyle T}}
\newcommand{\comp}{\complement}
\newcommand{\probability}{\mathbb{P}}
\newcommand{\probdist}{Pr}
\newcommand{\expectedvalueover}[1]{\expectedvalue\limits_{#1}}
\newcommand{\condbar}{\;\middle|\;}
\newcommand{\gaussdistr}{\mathcal{N}}
\newcommand{\uniformdistr}{\mathcal{U}}
\newcommand{\bernoullidist}{Bernoulli}
\newcommand{\F}{\mathcal{F}}
\newcommand{\learningalg}{\mathcal{L}}
\newcommand{\indvec}{J}
\newcommand{\indfunc}{\mathbb{I}}
\newcommand{\E}{{\mathbb E}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Natural}{\mathbb{N}}
\newcommand{\statespace}{\mathcal{X}}
\newcommand{\funcspace}{\mathcal{F}}
\newcommand{\dataset}{\mathcal{D}}
\newcommand{\trainingset}{\mathcal{S}}
\newcommand{\tarset}{\mathcal{T}}
\newcommand{\laplacian}{L}
\newcommand{\adjacency}{A}
\newcommand{\Gg}{\mathcal{G}}
\newcommand{\Hg}{\mathcal{H}}
\newcommand{\Ag}{\mathcal{A}}
\newcommand{\vertexset}{\mathcal{V}}
\newcommand{\Ex}{\mathcal{E}}
\newcommand{\hh}{\mathcal{H}}
\newcommand{\shh}{\widetilde{\mathcal{H}}}
\newcommand{\hhone}{{\mathcal H}_1}
\newcommand{\phh}{{{\mathcal H}_\theta}}
\newcommand{\zph}{Z_\phh}
\newcommand{\hhtwo}{{\mathcal H}_2}
\newcommand{\hhthree}{{\mathcal H}_3}
\newcommand{\func}{f}
\newcommand{\whfunc}{\wh{\mathsf{f}}}
\newcommand{\wtfunc}{\wt{\mathsf{f}}}
\newcommand{\hilbprod}[2]{\left\langle{#1},{#2}\right\rangle_\rkhs}
\newcommand{\scal}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\Lt}{{L^2(\X, \rhox)}}
\newcommand{\Ltwo}{{L^2(\X, \rhox)}}
\newcommand{\rhox}{{\rho_{\X}}}
\newcommand{\fh}{f_\hh}
\newcommand{\frho}{f_\rho}


\newcommand{\kerfunc}{k}
\newcommand{\kermatrix}{{\bK}}
\newcommand{\featkermatrix}{\boldsymbol{\Phi}}
\newcommand{\featvec}{\boldsymbol{\phi}}
\newcommand{\deff}{{\wh{\cal N}}}
\newcommand{\deffs}{{\cal N}}
\newcommand{\deffsm}{{{\cal N}_M}}
\newcommand{\deffsinfty}{{{\cal N}_\infty}}
\newcommand{\deffinfty}{{{\wh {\cal N}}_\infty}}
\newcommand{\donl}{d_{\text{onl}}}
\newcommand{\bdeff}[1]{\wb{d}_{\text{eff}}(#1)}
\newcommand{\adeff}[1]{\wt{d}_{\text{eff}}(#1)}
\newcommand{\atau}{\wt{\tau}}
\newcommand{\axi}{\wt{\xi}}
\newcommand{\akermatrix}{\wt{\mathbf{K}}}
\newcommand{\bkermatrix}{\mathbf{\wb{K}}}
\newcommand{\selmatrix}{{\bS}}
\newcommand{\selrmatrix}{{\bR}}





\newcommand{\wolog}{w.l.o.g.\xspace}
\newcommand{\amu}{\wt{\mu}}
\newcommand{\asigma}{\wt{\sigma}}
\newcommand{\abw}{\wt{\bw}}
\newcommand{\abomega}{\wt{\bomega}}
\newcommand{\abA}{\wt{\bA}}
\newcommand{\taumin}{\tau}
\newcommand{\cset}{\mathcal{I}}
\newcommand{\srigamma}{\alpha}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\la}{\lambda}
\newcommand{\dimension}{d}

\newcommand{\sampdist}{\rho}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\aphimat}[1]{\wt{{\Phi}}_{#1}(\bX_{#1})}
\newcommand{\afeatvec}{\widetilde{\boldsymbol{\phi}}}
\newcommand{\armset}{\mathcal{A}}
\newcommand{\featmap}{{\phi}}
\newcommand{\embfunc}{\bz}
\newcommand{\embvec}{\bz}
\newcommand{\embmat}{\bZ}

\newcommand{\fnorm}{F}
\newcommand{\Narm}{A}
\newcommand{\history}{\mathcal{D}}
\newcommand{\batchsize}{B}
\newcommand{\fb}[1]{\normalfont \texttt{fb}[#1]}

\newcommand{\bkb}{\textsc{BKB}\xspace}
\newcommand{\bbkb}{\textsc{BBKB}\xspace}
\newcommand{\oful}{\textsc{OFUL}\xspace}
\newcommand{\gpucb}{\textsc{GP-UCB}\xspace}
\newcommand{\bgpucb}{\textsc{GP-BUCB}\xspace}
\newcommand{\agpucb}{\textsc{GP-AUCB}\xspace}
\newcommand{\agpucbloc}{\textsc{GP-AUCB-Local}\xspace}
\newcommand{\kernelucb}{\textsc{KernelUCB}\xspace}
\newcommand{\linrel}{\textsc{LinRel}\xspace}


\newcommand{\cond}{\nu}
\newcommand{\lspanc}[2]{\overline{\operatorname{span}\{#1~|~#2\}}}




\newcommand{\initset}{\mathcal{\dataset}_{\text{init}}}
\newcommand{\sizeinitset}{\Tinit}
\newcommand{\scalefact}{S}
\newcommand{\Tinit}{T_{\text{init}}}
\newcommand{\tepsfrac}{\big(\tfrac{1 + \varepsilon}{1 - \varepsilon}\big)}
\newcommand{\epsfrac}{\left(\frac{1 + \varepsilon}{1 - \varepsilon}\right)}





\newcommand{\K}{{\wh K}}
\newcommand{\Km}{\wh K_{MM}}
\newcommand{\Knm}{\wh K_{nM}}
\newcommand{\Kim}{\wh K_{iM}}
\newcommand{\Kbm}{\wh K_{BM}}
\newcommand{\KJi}{\widehat K_{J,i}}
\newcommand{\KJJ}{\widehat K_{J,J}}
\newcommand{\KJ}{K_{J}}
\newcommand{\KJhJh}{\widehat K_{J_h,J_h}}
\newcommand{\Kii}{\widehat K_{i,i}}
\newcommand{\yn}{\widehat{y}}
\newcommand{\yni}{\widehat{y}_i}
\newcommand{\ynb}{\widehat{y}_b}
\newcommand{\iter}{t}
\newcommand{\HS}{{\textrm{HS}}}
\newcommand{\Cn}{\widehat{\C}_n}
\newcommand{\Cl}{\C_\la}
\newcommand{\Cnl}{\widehat{\C}_{n\lambda}}
\newcommand{\Cnlz}{\widehat{\C}_{n\lambda_0}}
\newcommand{\Cm}{\widehat{\C}_M}
\newcommand{\Cml}{\widehat{\C}_{M\la}}
\newcommand{\Gm}{\widehat{G}_M}
\newcommand{\Gml}{\widehat{G}_{M\la}}
\newcommand{\Sm}{\widehat{S}_M}
\newcommand{\Zm}{\widehat{Z}_M}
\newcommand{\Sn}{\widehat{S}}
\newcommand{\Pm}{P_M}



\newcommand{\alglev}{\widetilde{\ell}}
\newcommand{\emplev}{\widehat{\ell}}
\newcommand{\barlev}{\overline{\ell}}
\newcommand{\lev}{\ell}
\newcommand{\prweight}{A}
\newcommand{\lossf}{\mathcal{L}}
\newcommand{\pen}{\textit{Pen}}
\newcommand{\tmax}{{t_{max}}}

\newcommand{\rkhs}{\hh}
\newcommand{\phivec}[1]{\featmap(\bx_{#1})}
\newcommand{\supkernelucb}{\textsc{SupKernelUCB}\xspace}
\newcommand{\soful}{\textsc{SOFUL}\xspace}
\newcommand{\pvec}{\mathbf{\psi}}
\newcommand{\normempty}[1]{\left\Vert #1 \right\Vert}
\newcommand{\filtration}{\mathcal{F}}

\newcommand{\A}{A}
\newcommand{\B}{B}
\newcommand{\Tla}{T_\la}
\newcommand{\Z}{Z}
\newcommand{\zr}{bohZ}
\newcommand{\En}{\widehat L}
\newcommand{\Enr}{\widehat L_{M}}
\newcommand{\enri}{\widehat \ell_{i}}
\newcommand{\Enh}{\widehat L_\hh}
\newcommand{\Ent}{\widehat L_{\theta}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\data}{\widehat z}
\newcommand{\kf}{\kerfunc}
\newcommand{\pp}{\widetilde P}

\newcommand{\xmix}{{\widetilde x_{i,j}}}
\newcommand{\ymix}{{\widetilde y_{i,j}}}
\newcommand{\xmess}{{\bar x_{i,j}}}
\newcommand{\ymess}{{\bar y_{i,j}}}

\newcommand{\nt}[1]{{\bf \color{red} #1}}

\newcommand{\Hcal}{\mathcal{Z}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Scal}{\mathcal{S}}
 \newcommand{\rev}[1]{{#1}}








\usepackage{lastpage}


\firstpageno{1}

\begin{document}

\title{On Mixup Regularization}

\author{\name Luigi Carratino\thanks{This work was done in part at Google Brain, Paris.} \email luigi.carratino@dibris.unige.it \\
       \addr MaLGa - University of Genova, Italy
       \AND
       \name Moustapha Ciss\'e \email moustaphacisse@google.com \\
       \addr Google Research - Brain team, Accra
       \AND
       \name Rodolphe Jenatton \email rjenatton@google.com \\
       \addr Google Research - Brain team, Berlin
       \AND
       \name Jean-Philippe Vert\thanks{Now at Owkin, Paris, France} \email jean-philippe.vert@m4x.org \\
       \addr Google Research - Brain team, Paris
       }



\maketitle

\begin{abstract}Mixup is a data augmentation technique that creates new examples as convex combinations of training points and labels. 
  This simple technique has empirically shown to improve the accuracy of many state-of-the-art models in different settings and 
  applications, but the reasons behind this empirical success remain poorly understood. 
  In this paper we take a substantial step in explaining the theoretical foundations of Mixup, 
  by clarifying its regularization effects. 
  We show that Mixup can be interpreted as standard empirical risk minimization estimator subject to 
  a combination of data transformation and random perturbation of the transformed data. 
  We gain two core insights from this new interpretation.
  First, the data transformation suggests that, at test time, a model trained with Mixup should also be applied to transformed data, a one-line change in code that we show empirically to improve both accuracy and calibration of the prediction.
  Second, we show how the random perturbation of the new interpretation of Mixup
  induces multiple known regularization schemes, 
  including label smoothing and reduction of the Lipschitz constant of the estimator. 
  These schemes interact synergistically with each other, 
  resulting in a self calibrated and effective regularization effect that prevents overfitting 
  and overconfident predictions. 
  We corroborate our theoretical analysis with experiments that support our conclusions.
\end{abstract}



\section{Introduction}

Regularization is an essential component of machine learning models and 
plays an even more important role in deep learning \citep{goodfellow2016deep}. 
Regularization mechanisms can take various forms. They can be \emph{explicitly} enforced by: 
(i) applying various penalties to the parameters of the models~\citep{hinton1987learning, krogh1992simple, bartlett2017spectrally,neyshabur2015path, sedghi2018singular,arjovsky2017wasserstein}, (ii) injecting noise to the internal representations of the network~\citep{srivastava2014dropout, gal2016dropout} and/or to its outputs~\citep{szegedy2016rethinking,Muller2019When},
or (iii) normalizing the activations~\citep{he2016deep,salimans2016weight}.
Or they can be \emph{implicit} thanks to: (j) parameter sharing in architectures such as convolutional 
networks~\citep{lecun1998gradient}, (jj) the choice of the optimization algorithm~\citep{neyshabur2017implicit}, e.g., stochastic gradient descent converging to small norm solutions~\citep{arora2019implicit}, 
or (jjj) through data augmentation and transformation \citep{goodfellow2016deep}.
There is a large body of work explaining the effects of the numerous explicit and implicit 
regularization procedures existing in the literature. 
For instance, explicit regularization schemes usually proceed from analysis aiming to control specific 
characteristics of a model such as robustness~\citep{hein2017formal,cisse2017parseval} or 
calibration~\citep{guo2017calibration,Muller2019When}, while the forms of implicit regularization are 
often understood through the angle of generalization~\citep{neyshabur2017implicit, arora2019implicit}. 
However, the regularization effects of modern data augmentation procedures are less theoretically understood. 

Data augmentation is a core ingredient for successful deep learning pipelines. 
It helps to alleviate sample size issues and prevent overfitting. 
In simple cases, there are known equivalences between data augmentation and other existing explicit 
regularization procedures, e.g., training with additional noisy points in least-squares regression 
is equivalent to Tikhonov regularization~\citep{bishop1995training}. 
Similar analysis have recently been performed to explain the regularization effect of 
dropout~\citep{srivastava2014dropout,wager2013dropout,wei2020implicit}.
In this work, we focus on \textit{Mixup}~\rev{\citep{Zhang2018mixup,tokozume2018between}}, a recently introduced data-augmentation 
technique that consists in generating examples as random convex combinations of data points and labels 
from the training set (as illustrated in Figure~\ref{fig:overview}). 
Despite its simplicity, Mixup has been shown to substantially improve generalization on a broad 
range of tasks ranging from computer vision~\rev{\citep{Zhang2018mixup,tokozume2018between}} to natural language 
processing~\citep{guo2020nonlinear} and semi-supervised learning~\citep{berthelot2019mixmatch}. 
The success of Mixup has triggered several variations such as adaptive Mixup~\citep{guo2019mixup}, 
manifold Mixup~\citep{verma2018manifold} and Cutmix~\citep{yun2019cutmix}, but the reasons why Mixup and 
its variants work so well in practice remain poorly understood.

Mixup's primary motivation was to alleviate overfitting in training deep neural networks~\citep{Zhang2018mixup}. 
However, previous studies have also empirically noticed other desirable regularization effects it induces. 
These include improved calibration~\citep{thulasidasan2019mixup}, robustness to input adversarial noise~\citep{Zhang2018mixup}, 
and robustness to label corruption~\citep{Zhang2018mixup}. \citet{Zhang2018mixup} also 
showed it helps stabilize notoriously difficult learning problems such as generative adversarial networks. 
Traditionally, separate regularization methods are applied to induce the above effects. 
For example, label smoothing~\citep{szegedy2016rethinking,Muller2019When} leads to better calibration, 
while dropout improves generalization~\citep{srivastava2014dropout,wager2013dropout} and robustness to 
label corruption~\citep{arpit2017closer}.  Lipschitz regularization helps stabilize the training of generative 
adversarial networks~\citep{arjovsky2017wasserstein,gulrajani2017improved}. It also leads to increased robustness to 
adversarial perturbations~\citep{hein2017formal,cisse2017parseval}. 
\Cref{tab:comparisons} shows a comparison of various regularization procedure proposed in the literature, 
and the effect they are known to induce on the model. 
Although all these desirable regularization effects have been observed empirically, 
no theoretical explanation has been given yet.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{./imgs/overview.pdf}
    \caption{\small
        Illustration of how training a model with Mixup (second plot) differs from training 
        a model on original data (first plot), the fourth plot highlighting the discrepancy between 
        the Bayes classifiers in both situations (black vs red). 
        To explain this difference, we show in this paper that the model trained with Mixup 
        can be interpreted as a regularized version of a model trained on modified data 
        (third plot, blue curve on the zoom plot), and characterize both the data modification 
        (from black to blue)  and the regularization effect (from blue to red). 
        Both effects interact synergistically to confer Mixup strong regularization properties, 
        which may explain its good empirical behavior in a variety of tasks.
        }
    \label{fig:overview}
    \vspace{-1\baselineskip}
\end{figure}
In this work, we propose the first theoretical analysis of Mixup\footnote{\rev{After we published a first version of this work~\citep{Carratino2020Mixup}, \citet{Zhang2021HowDM} independently derived a similar and complementary analysis of Mixup; we summarize in \Cref{sec:discussion} the main differences between both works.}} to better understand the reasons for its empirical success. 
We show that Mixup can be analyzed through the lenses of empirical risk minimization with random perturbations, 
and exploit ingredients from previous analysis of dropout~\citep{wager2013dropout, khalfaoui2019asni, wei2020implicit} 
to derive a regularized objective function that sharply captures the regularization effects of Mixup. 
In particular, our analysis sheds some light on the multiple effects that Mixup borrows from the popular 
regularization mechanisms listed above such as label smoothing~\citep{pereyra2017regularizing} (output noise) or  
dropout~\citep{srivastava2014dropout} (input noise), and how it uniquely combines them to improve calibration and 
smooth the Jacobian of the model.
We further show that this analysis points out a missing step in learning with Mixup, and we present 
how applying a simple transformation when evaluating at test time the function learned with Mixup
can improves accuracy and calibration.
More precisely, we make the following contributions (illustrated in Figure~\ref{fig:overview}):
\begin{itemize}\item We show that Mixup can be reinterpreted as a standard empirical risk minimization procedure, applied to a transformation of the original data perturbed by random perturbations, and give explicit formulas for the data transformation and the perturbations.
\item In particular, we show that the Mixup transformations shrinks both the inputs and the outputs towards their mean, the later creating a form of regularization by label smoothing. We notably give a formal description of the effect of label smoothing in the case of the cross-entropy loss where it translates into an increase in the entropy of the predictions.
\item We show that Mixup learns functions from a modified version of the input space of the training points to a modified version of the output space of the training points.
Thus, we present how to properly evaluate the learned functions to further improve accuracy and calibration. 
\item We characterize the random perturbations induced by Mixup on both the inputs and the outputs, as well as their dependency and their correlation structure.
\item We deduce an approximation of the regularization induced by Mixup, and highlight in particular how it regularizes both the model and its derivatives. We discuss in details the specific cases of classification with cross-entropy loss, and least squares regression.
\item We provide empirical support for our interpretation of Mixup regularization.
\end{itemize}


\begin{table}[!htbp]
\begin{center}
\begin{scriptsize}
\setlength\tabcolsep{4pt}
\begin{tabular}{@{}l|c|c|c|c|c|@{}}
\toprule
Method & Calibration  & Jacobian Reg.  & Robustness Label Noise  & Input Normaliz.  \\
\midrule
Label smooth.~\citep{szegedy2016rethinking}       &             &   &     &    \\
Spectral Reg.~\citep{cisse2017parseval}   &           &             &      &\\
Dropout~\citep{wager2013dropout}       &   &    &      &      \\
Temperat. scaling~\citep{guo2017calibration} &    &   &    & \\
Mixup~\citep{Zhang2018mixup} &    &   &      & \\
\bottomrule
\end{tabular}
\end{scriptsize}
\end{center}
\caption{Summary of the effects induced by various regularizers. Absence of checkmark means the corresponding effect is not known for this regularizer. \nt{}}
\label{tab:comparisons}
\end{table}

The rest of the paper is organized as follows. In \Cref{sec:notations}, we introduce notations used throughout the paper and describe the setting of empirical risk minimization and learning with Mixup. In \Cref{sec:erm}, we show how Mixup can be interpreted as an empiricial risk minimization on modified data with random perturbations. In \Cref{sec:regu} we analyze the regularization effect of Mixup through a quadratic Taylor approximation of the formulation derived in \Cref{sec:erm}. In \Cref{sec:discussion}, we discuss in detail several aspects of Mixup that the theoretical analysis in \Cref{sec:regu} and \Cref{sec:erm} suggest, and confront them to experimental validations. The proofs of all results are detailed in the Annex, together with additional experimental results.











\section{Notations and setting}\label{sec:notations}

\paragraph{Notations.}
For any ,  is the set of nonzero integers up to ,
 is the -dimensional vector of ones, 
and  and  are the -dimensional null and identity matrices, respectively.
For any two matrices  of equal size we note  their 
Frobenius inner product, and with  the Frobenius norm.
For any vector , matrix  and positive semi-definite matrix 
, we denote by  
the squared semi-norm of  with metric , and with 
the squared Frobenius norm with metric . 
For any function  and vector ,
we denote respectively by  and  the Jacobian and Hessian of  at , i.e., if , then  and , for . Note in particular that if , then the gradient of  is a row vector . When  has several arguments and we wish to take partial derivatives with respect to some of the arguments, we explicitly name the different arguments as  and then indicate as a subscript to the  sign the argument(s) according to which we take derivatives, e.g., if  and , then  is the Jacobian of  with respect to , and  is the tensor of second derivatives of  of the form  for . We recall that if  is twice continuously differentiable, then , by Schwarz's theorem. For any random variable  and measurable function , we denote by  the expectation of , or simply  when no confusion is possible. For any shape parameters , and any interval ,  denotes the truncated Beta distribution on , i.e., the distribution of a random variable with values in  and density proportional to  on . We simply write  for the usual Beta distribution. For any ,  denotes the Bernoulli distribution with parameter . For any , we denote by 

the simplex in , and for any , we denote by

the entropy of a categorical distribution with parameter .

\paragraph{Learning problem.}
We consider a training set  made of  input/output pairs, where for each pair ,  and . This covers in particular the regression or binary classification settings, where , or the multivariate regression and multiclass classification setting, where  is an embedding of the class of  in , e.g., the one-hot encoding by taking  equal to the total number of classes and letting  be the binary vector with all entries equal to zero except for the one corresponding to the class of . We further denote the mean input and output as

and the empirical variance and covariance matrices 
or inputs and outputs as

Our goal is to learn from  a function \rev{} to predict the output corresponding to any new input  via \rev{, where  maps an -valued prediction to an element of ; standard mappings include the identity  for regression problems, and the softmax operator  for multiclass classification problems}. For that purpose, we formulate the inference problem as an optimization problem:

where  is a class of candidate functions, such as linear functions or deep neural networks, and  is a risk functional that depends on . The most standard risk used in machine learning is the empirical risk, defined for any loss function \rev{} by:

Solving (\ref{eq:minrisk}) with the empirical risk (\ref{eq:erm}) is often called \emph{empirical risk minimization} (ERM), and is typically performed in practice by first-order numerical optimization such as stochastic gradient descent~\citep{Bottou2008Tradeoffs}. Standard losses  include the squared error (in regression) and the cross-entropy loss \rev{applied to the softmax mapping} (in classification, assuming that , which is true for one-hot encoded classes and their convex combinations):


\paragraph{Mixup.}
Instead of minimizing the empirical risk (\ref{eq:erm}),  Mixup \citep{Zhang2018mixup}  creates new random input/output samples by taking convex combinations of pairs of training samples, and minimizes the corresponding empirical risk. With our notations, Mixup therefore minimizes the following \emph{Mixup risk} over :

where , and  is a parameter of Mixup. The minimization of (\ref{eq:mixup}) is typically performed by stochastic gradient descent, where  is sampled at each iteration to obtain a stochastic gradient. In practice, \citet{Zhang2018mixup} suggest to sample minibatches of training pairs, and generate Mixup random pairs within the minibatch, which also produces a stochastic gradient of (\ref{eq:mixup}). \section{Mixup as a perturbed ERM}\label{sec:erm}

The Mixup risk (\ref{eq:mixup}) is defined as a sum over pairs of samples, making a comparison with standard ERM approaches (\ref{eq:erm}) not direct. The following result shows that the Mixup risk can be equivalently rewritten as a standard empirical risk, over modified input/output pairs (as in the third plot of Figure~\ref{fig:overview}), subject to random perturbations.

\begin{theorem}\label{thm:mixupAsErm}
Let  and  
be two random variables with ,  and let . For any training set , let  for any  be the modified input/output pair given by

and  be the random perturbations given by:

Then for any , , and for any function ,

\end{theorem}
Both  and  are random vectors because they are functions of  and  in (\ref{eq:perturbations}), which are themselves random variables. We hence use the notation  in (\ref{eq:mixupERM}). Note also  meaning that the transformation from  to  in (\ref{eq:modif}) shrinks the inputs and the outputs towards their mean.

\Cref{thm:mixupAsErm}
and the expression (\ref{eq:mixupERM}) of the Mixup risk allow us to re-interpret Mixup as a combination of two standard techniques: (i) transforming each input/output pair  into , and (ii) adding zero-mean random perturbations  to each transformed pair, before minimizing the empirical risk. 
This helps us to understand the effects of training a model with Mixup by studying each technique and their interaction.
In particular, perturbing input data is a classical approach to regularize ERM estimators~\citep{bishop1995training,srivastava2014dropout,wager2013dropout,wei2020implicit}, and we study in detail in the next section the particular regularization induced by the Mixup perturbations on both inputs and outputs, before interpreting the resulting regularization aspects of Mixup due to both data transformation and perturbation in~\Cref{sec:discussion}. \section{The regularization effects of Mixup}\label{sec:regu}

We now study the effect of the random perturbations  for  in the Mixup risk (\ref{eq:mixupERM}). While perturbing inputs with additive or multiplicative noise (e.g., dropout), and independently perturbing outputs (resulting, e.g., in label smoothing) have been widely studied, the Mixup perturbation (\ref{eq:mixupERM}) is unique in the sense that it is applied to both inputs and outputs simultaneously, and that the input and output perturbations are not independent from each other by (\ref{eq:perturbations}). 
In order to study the regularization effect of these perturbations, we first characterize the covariance structure among the input and output perturbations.
\begin{lemma}\label{lem:corr}
Let  and  be respectively the mean and variance of a  distributed random variable, and . For any , let

Then, for any , the random perturbations defined in (\ref{eq:perturbations}) satisfy

\end{lemma}

Following recent lines of work that interpret various random perturbations such as dropout as regularization \citep{wager2013dropout,wei2020implicit}, we can now introduce and study an approximate Mixup risk:

obtained by replacing the loss function  by a second-order quadratic Taylor approximation near each modified input/output training pairs , namely, for any  and :

assuming both  and  are twice continuously differentiable. Due to its quadratic form as a function of input and output perturbations, the approximate Mixup risk (\ref{eq:mixupQ}) can be re-expressed as a regularized ERM risk, as shown in the next result. We note that the expression we derive is in fact valid for \emph{any} joint perturbation of the inputs and outputs with covariance structure given in (\ref{eq:cov}).

\begin{theorem}\label{thm:mixupreg}
For any twice continuously differentiable loss , the approximate Mixup risk at any twice differentiable  satisfies

where

and

\end{theorem}

\Cref{thm:mixupreg} captures the effect of the random perturbations in Mixup as a sum of four penalty terms  for . 
They regularize the simple ERM risk applied on the modified inputs  and smoothed outputs . 
\rev{Before discussing the accuracy and practical consequences of this reformulation of Mixup as regularized empirical risk minimization on modified data in the next Section, we now derive the details of this approximation for the cross-entropy, logistic and squared error losses.}
We begin by presenting the results for the cross-entropy loss:

\begin{corollary}\label{cor:CEloss}
Let  be the softmax operator, i.e., 
for any  and , , 
and let . The approximate Mixup risk for the cross-entropy loss satisfies

where

with

\end{corollary}

In the binary classification setting, minimizing the empirical cross-entropy risk over  after one-hot encoding of the two possible classes in  as  and  is equivalent to minimizing the following well-known logistic loss over  after encoding the two classes in  as  and :

The regularization effect of Mixup in that case is detailed in the following result:
\begin{corollary}\label{cor:logisticloss}
Let  be the sigmoid operator, i.e., 
for any , , and 
let . The approximate Mixup risk for the logistic regression loss satisfies

where

with

\end{corollary}

The next result summarizes the form of the approximate Mixup risk in the case of the squared error loss, 
and shows in particular that Mixup has no effect for linear least-squares regression models.
\begin{corollary}\label{cor:SEloss}
The approximate Mixup risk for the squared error loss satisfies

where  is a constant independent of  and

with

In particular, when we consider linear models with intercept of the form  for , then the exact Mixup risk satisfies 
where  is a constant that does not depend on  and . Consequently, the linear model that minimizes  is the standard multivariate ordinary least squares (MOLS) predictor that minimizes  on the original data, i.e., Mixup has no effect on linear least-squares regression.
\end{corollary}


 


 \section{Discussion and experiments}\label{sec:discussion}

Let us now discuss \rev{how our analysis relates to a recent similar study by \citet{Zhang2021HowDM}}, and empirically assess the validity of our analysis and the regularization properties of Mixup it suggests.
To support our discussion, we provide empirical results on CIFAR-10/100 and ImageNet for different networks
(LeNet, ResNet-34/50). 
For each experimental result we report mean and 95\% confidence interval using 10 repetitions (unless stated otherwise). 
All details about experiments are provided in \Cref{sec:app_exp}, together with other experiments on 
the simpler setting of learning on the two-moon dataset with random features. 

\rev{\paragraph{Comparison with related work.}
\citet{Zhang2021HowDM} independently published a similar analysis of the regularization effect of Mixup. Both works provide complementary and coherent views of the effect of Mixup on generalization and robustness, and differ in a few technical aspects that we clarify here. First, we provide an analysis valid for any loss , where  and  are respectively the predicted and true outputs, while \citet{Zhang2021HowDM} restrict their analysis to the losses of the form  for some function . Second, while both works use a second-order Taylor expansion to approximate the loss at a Mixup example, the expansions are different since we do a Taylor expansion near the expected value of the Mixup example, while \citet[Lemma 3.1]{Zhang2021HowDM} do a Taylor expansion near a non-Mixup example. One consequence is that our Taylor approximation converges to the exact Mixup risk when the amount of Mixup reduces (i.e., when  in the  distribution of the mixing parameter), while the one used by \citet{Zhang2021HowDM} does not\footnote{\rev{More precisely, the Taylor approximation remainder in \citet[Lemma 3.1]{Zhang2021HowDM} does not vanish since it is a Taylor expansion near 0 of a random variable that follows a  distribution, i.e., that takes values close to  with probability  when  goes to .}}. Third, the generalization analysis of \citet{Zhang2021HowDM} for generalized linear models (GLM) is performed for a variant of Mixup where the mixed input is , while we focus on the standard Mixup . A consequence of this difference is that we identify the importance of rescaling data at test time for standard Mixup (see below), while no such rescaling is needed in the variant considered by \citet{Zhang2021HowDM}. Finally, \citet{Zhang2021HowDM} explore the link between Mixup-induced regularization and adversarial robustness and generalization through Rademacher complexity analysis, which we do not explore in this work but could be done in a similar manner.}
\\


\begin{figure*}[ht]
\hfill
    \minipage{0.32\textwidth}
    \includegraphics[width=\linewidth]{./imgs/train_loss_taylor_approx_lenet.pdf}
    \endminipage\hfill
    \minipage{0.32\textwidth}
    \includegraphics[width=\linewidth]{./imgs/test_loss_taylor_approx_lenet.pdf}
    \endminipage\hfill 
    \minipage{0.32\textwidth}
    \includegraphics[width=\linewidth]{./imgs/test_acc_taylor_approx_lenet.pdf}
    \endminipage\hfill
    \caption{\small{From left to right: train loss, test loss and accuracy during optimization of 
    LeNet on CIFAR-10 with Mixup and approximate Mixup.}
    }\label{fig:mess_vs_approx_acc}
\end{figure*} 
\paragraph{Validity of the Taylor approximation.} 
To analyze the regularization effect of Mixup, we used a quadratic approximation of the loss 
function (\ref{eq:taylor}). We note that compared to similar approximations that have been proposed 
to study the regularization effect of input perturbation only, such as 
dropout~\citep{wager2013dropout,wei2020implicit}, we must include in the Taylor expansion all 
second-order terms involving the input perturbation only (term with ), 
the output perturbation only (term with ), and their interaction 
(term with ). In the absence of output perturbation (e.g., in the case of dropout), 
only the term in  matters, and in the absence of correlation between input and output 
perturbation (e.g., dropout combined with independent label smoothing), then the term in  
does not matter either. Mixup is unique in the correlation it creates between input and output perturbations, 
which is captured by the interaction term with  in (\ref{eq:taylor}). 
Regarding the validity of the Taylor approximation, we note that, as for similar work on input perturbation, 
the approximate Mixup risk (\ref{eq:mixupQ}) is only a good approximation to the Mixup risk for ``small'' 
perturbations; as noted by \citet[Annex A.2]{wei2020implicit}, though, this often remains valid even for ``large'' 
input perturbation followed by a linear transformation layer. 
To support empirically the validity of the 
approximation, \Cref{fig:mess_vs_approx_acc} shows the training and test 
performance of training a LeNet on CIFAR-10 using
Mixup (minimizing \eqref{eq:mixup}), and using the approximate Mixup formulation (minizing \eqref{eq:mixapprox}), 
where we dropped the term  in the regularization 
since it empirically induces numerical instability due to it non-convexity \citep[see also][for a discussion about 
discarding the Hessian regularization]{wei2020implicit}. 
\begin{figure}[ht]
\centering
    \minipage{0.39\textwidth}
    \includegraphics[width=\linewidth]{./imgs/regularization_taylor.pdf}
    \endminipage
\hspace{1cm}
    \minipage{0.39\textwidth}
    \includegraphics[width=\linewidth]{./imgs/erm_tild_taylor.pdf}
    \endminipage
\caption{\small{Evaluations of the regularization terms of the mixup approximation (left) and of the loss on modified data (right) for functions learned with Mixup, ERM and ERM on modified data.}}\label{fig:regularization_taylor}
\end{figure}
We can see how when training using the approximate Mixup formulation, 
we learn functions which mimic, both in training and test, 
the performance of functions learned when training with the standard Mixup procedure.
To further mark the validity of the approximation and decouple the contributions of 
the data transformation \eqref{eq:modif} and the input perturbation \eqref{eq:perturbations},
we evaluate for functions learned with Mixup \eqref{eq:mixup}, ERM \eqref{eq:erm} and ERM on modified data: 

the regularization terms of the approximation \eqref{eq:mixapprox} and the loss on modified data \eqref{eq:ermtild}.
From \Cref{fig:regularization_taylor} we see that the functions learned with Mixup are the ones with the smallest values
of the regularization terms but not the smallest loss on modified data, which confirms that the model trained with Mixup finds a trade-off between empirical risk and the regularization we study.


\paragraph{Data modification at test time.} 
By \Cref{thm:mixupAsErm}, we see that Mixup implicitly shrinks inputs towards their 
mean since the Mixup risk involves the empirical risk over modified inputs  and outputs . 
In particular, this means that the functions that the standard Mixup procedure learns are not functions
from the space of input points  to the output points , but it learns functions from  to ,
which are spaces defined by the training points and the hyperparameter  of Mixup as  

where  and  are respectively the average training inputs and outputs points, 
and  as defined in \Cref{thm:mixupAsErm}.
An important consequence is that the function  estimated by Mixup, 
should ideally be applied at test time to transformed data, to 
map the test point  to , 
and the evaluation of the function  should be mapped back to .
In details, the prediction for point  should be
 



\begin{small}
    \begin{algorithm}[H]\small
        \caption{{\tt python} code to evaluate according to \eqref{eq:testmodif} functions learned with mixup
            \label{algo:pred-python}}
\vspace{-0.35cm}
        \begin{flushleft}
            {\bf Input:}  \texttt{X\_test} \textit{Tensor} of test points to evaluate, 
            \texttt{trained\_mode} \textit{Model} trained using mixup,
            \texttt{alpha} \textit{float} hyperparameter used by mixup during training, 
            \texttt{X\_train}, \texttt{Y\_train} \textit{Tensor} of points used for training with one-hot encoded labels.\\
        \end{flushleft}
        
     {\footnotesize
        \begin{center}
\begin{verbatim}
    import scipy.special as sc
    import torch
    
    X_bar = torch.mean(X_train, dim=0, keepdim=True)
    Y_bar = torch.mean(Y_train, dim=0, keepdim=True)
    
    # expectation of a truncated beta distribution in [0.5, 1]
    theta_bar = 1. - sc.betainc(alpha + 1., alpha, .5) 
    
    def predict_mixup(X_test, trained_model):
        f_X = trained_model.forward((1. - theta_bar) * X_bar + theta_bar * X_test)
        return Y_bar * (1. - 1. / theta_bar) + f_X / theta_bar
            \end{verbatim}
        \end{center}
    }
    \vspace{-0.35cm}
    
    \end{algorithm}
    \end{small}
For centered training data () and homogeneous functions 
( for any , e.g., linear models 
or neural networks with ReLU activation and linear transformations), 
this has no impact as  in that case. 
For more general models, however,  (\ref{eq:testmodif}) may be a better predictor than . 
For example, we clearly see in \Cref{fig:overview} that the asymptotically Bayes optimal classifier 
under the Mixup distribution matches the one under the empirical distribution of the modified data 
(up to regularization effects), and not of the original data. 
Interestingly, when the classes are balanced, 
i.e., , the transformation in~(\ref{eq:testmodif}) adds the same 
constant to each of the  entries of . In particular, in the multi-class setting, 
since the softmax is invariant to a constant in the logits, (\ref{eq:testmodif}) becomes equivalent 
to a scaling of the logits, commonly referred to as temperature scaling~\citep{guo2017calibration}. 
While temperature scaling is traditionally tuned with a validation set~\citep{guo2017calibration}, 
mixup automatically sets this value, according to the distribution of .
\begin{figure}[ht]
    \includegraphics[width=\linewidth]{./imgs/cifar_imagenet/cifar_imagenet.pdf}
    \caption{\small{Test accuracy, test cross-entropy loss and test expected calibration error (first, second and third column respectively) on the CIFAR-10, CIFAR-100, ImageNet datasets (first, second and third row respectively). 
    We report the mean and the standard error over 10 repetitions for CIFAR-10 and CIFAR-100 and 5 repetitions for ImageNet. 
    The training was done with ResNet-34 for CIFAR-10, CIFAR-100, and with ResNet-50 for ImageNet.}
    }\label{fig:data_transf_mixup}
\end{figure} 
To point out the advantages of using \eqref{eq:testmodif}, we compare in \Cref{fig:data_transf_mixup} the performance
of ERM, standard Mixup (for different  values), the same Mixup but with the proper 
data transformation \eqref{eq:testmodif} at test time and the same data transformation applied to the ERM estimator. 
The trained networks are ResNet-34 for CIFAR-10 and CIFAR-100, and ResNet-50 for ImageNet.
For CIFAR-10 and 100, we observe overall benefits of using the data transformation for evaluating the functions learned with Mixup:
higher test accuracy, lower test loss, and lower expected calibration error (ECE). For ImageNet we have the same benefits, 
with the only exception of the test loss and the ECE for very low values of . 
Notice indeed that for small values of  the data transformation \eqref{eq:testmodif} has a smaller impact
than for bigger values of , as  
while . Thus, as observed empirically, the 
``correction'' \eqref{eq:testmodif} is more important, as it brings bigger improvements, 
when training Mixup with bigger . Finally, we can observe that when 
the data transformation \eqref{eq:testmodif} is applied to ERM, performance always deteriorate.
This supports that \eqref{eq:testmodif} is a mixup specific improvement. 
\Cref{algo:pred-python} shows the few lines of codes that implement the new prediction procedure \eqref{eq:testmodif}.
\begin{figure}[ht]
    \includegraphics[width=\linewidth]{./imgs/cifar_C/cifar_C.pdf}
    \caption{\small{Test accuracy, test cross-entropy loss and test expected calibration error (first, second and third column respectively) 
    on the CIFAR-10-C, CIFAR-100-C (first, and second row respectively). 
    We report the mean and the standard error over 10 repetitions.
    The training was done with ResNet-34 on the standard CIFAR-10, CIFAR-100.}
    }\label{fig:data_transf_mixup-OOD}
\end{figure}
\paragraph{Data modification for out-of-distribution data.} 
Even though our theoretical analysis holds only for in-distribution data, we now empirically investigate whether the benefits of rescaling data at test time that we observe for models trained with Mixup also hold when the test time data come from a different distribution, a setting called out-of-distribution (OOD) data \citep{hendrycks2016baseline}. Indeed, standard Mixup is known to provide some benefits in out-of-distribution settings compared to models simply trained by ERM, so it is interesting to assess whether the data modification scheme we propose can further boost the accuracy and calibration of models trained with Mixup in that setting.
In particular we consider CIFAR-10-C, CIFAR-100-C, ImageNet-C \citep{hendrycks2019robustness}, ImageNet-A \citep{hendrycks2019natural}, 
ImageNetV2 \citep{recht2019imagenet}, ImageNet-Vid-Robust, YTBB-Robust \citep{shankar2019image}, 
ObjectNet \citep{barbu2019objectnet}. 
These benchmark datasets are designed by systematically perturbing in a controlled way 
the in-distribution data (CIFAR-10, CIFAR-100 and ImageNet), e.g., 
by adding noise or applying transformations to the images.
\begin{figure}[ht]
    \includegraphics[width=\linewidth]{./imgs/cifar_C_diff/cifar_10_C_diff.pdf}
    \caption{\small{Difference in test performance between Mixup and rescaled Mixup on CIFAR-10-C, CIFAR-100-C for the different corruption intensities. 
    The training was done with ResNet-34 on the standard CIFAR-10, CIFAR-100 dataset and the performance metrics are accuracy, cross-entropy loss, expected calibration error.}}\label{fig:diff-rescaling_mixup}
\end{figure} 
\Cref{tab:imagenet-ood} details the performance of models trained with Mixup for  and , with or without data modification at test time, on out-of-distribution datasets derived from ImageNet, while \Cref{fig:data_transf_mixup-OOD} shows similar results for benchmarks derived from CIFAR-10 and CIFAR-100. We see that for most datasets the rescaling improves accuracy and some of the other metrics, 
but for others and in particular CIFAR-10-C, CIFAR-100-C and ImageNet-C, the rescaling worsen almost all metrics with respect of the standard Mixup.
We now investigate how the performance of Mixup with or without rescaling differs with respect to the intensity of the noise.
\begin{table}[H]
    \begin{center}
    \begin{small}
    \setlength\tabcolsep{4pt}
    \begin{tabular}{l l c c c c }
        \toprule    
        \multirow{2}{*}{ImageNet} & \multirow{2}{*}{Metric} &  mixup(rescaled) &  mixup &  mixup(rescaled) &  mixup  \\
         \cmidrule(lr){3-4} \cmidrule(lr){5-6} 
        {} & {} &  \multicolumn{2}{c}{}  &  \multicolumn{2}{c}{} \\
        \midrule
        \multirow{3}{*}{standard} & Acc                 &                       &                         &                        &                       \\
        & CE-loss                      &                       &                         &                        &                       \\
        & ECE                      &                       &                         &                        &                       \\
        \midrule
        \multirow{3}{*}{A} & Acc               &                       &                         &                        &                       \\
        & CE-loss                    &                       &                         &                        &                       \\
        & ECE                    &                       &                         &                        &                       \\
        \midrule
        \multirow{4}{*}{C} & Acc          &                       &                         &                        &                       \\
        & CE-loss               &                       &                         &                        &                       \\
        & MCE                   &                       &                         &                        &                       \\
        & ECE               &                       &                         &                        &                       \\
        \midrule
        \multirow{3}{*}{V2} & Acc              &                       &                         &                        &                       \\
        & CE-loss                   &                       &                         &                        &                       \\
        & ECE                   &                       &                         &                        &                       \\
        \midrule
        Vid-Robust & Acc (pm-k)  &                       &                         &                        &                       \\
        \midrule
        YTBB-Robust & Acc (pm-k)          &                       &                         &                        &                       \\
        \midrule
        ObjectNet & Acc &                       &                         &                        &                       \\
        \bottomrule
    \end{tabular}
    \end{small}
    \end{center}
    \caption{\small Test performance of Mixup and rescaled Mixup on different OOD versions of ImageNet.
    The training was done with ResNet-50 on the standard ImageNet dataset and the performance metrics are accuracy, cross-entropy loss, expected calibration error and mean corruption error \citep{hendrycks2019robustness}.
    For each metric means and standard errors over 10 repetitions are reported.}
    \label{tab:imagenet-ood}
\end{table}
In details, the CIFAR-10-C and CIFAR-100-C data are 95 different corrupted versions each of the original CIFAR-10, CIFAR-100 test sets: 
19 different corruption types, for 5 different growing corruption intensities. 
The results that we reported in \Cref{fig:data_transf_mixup-OOD} are the averages of the performance of 
these 95 test sets.
In \Cref{fig:diff-rescaling_mixup} instead, for Mixup and rescaled Mixup, we compute the average of the different metrics 
(test accuracy, test cross-entropy loss, ECE) over the 19 corruption types for each corruption intensity, and we 
report the difference in performance between Mixup and rescaled Mixup.
We can see that as the noise intensity grows the gap between the two methods always grows in favor of standard Mixup, 
and that only for a few metrics and settings with low noise the rescaled version is better than the standard one.
This observations encourage future investigations of the effect of Mixup on OOD data\footnote{Codes for reproducing results for ImageNet and various OOD variants are available at \url{https://github.com/google/uncertainty-baselines/tree/master/baselines/imagenet}}.


\paragraph{Label smoothing.}
The transformation that modifies the original labels  onto  acts as some form of \textit{label smoothing}, a technique known to often improve accuracy and calibration~\citep{szegedy2016rethinking,Muller2019When}. The transformed labels  are indeed pulled towards the average label . Recall from \citet{szegedy2016rethinking}
that label smoothing consists in training a model on the perturbed version of the training labels defined as , where  is a fixed
scalar in  and  is a fixed distribution over the labels. 
It is easy to see that for  and  the two formulations coincide. 
This implies that Mixup implicitly performs label smoothing, and can benefit from this technique in terms of accuracy or calibration.
In the following Proposition~\ref{prop:label_smoothing_logistic_reg}, we formally prove that, 
in the case of the cross entropy and linear models, 
label smoothing translates into an \textit{increase} in the average entropy of the predictions, or, 
in other words, that predictions become less certain, as observed in practice. Like in~\Cref{cor:CEloss}, we use the softmax operator  defined for  by :
\begin{proposition}\label{prop:label_smoothing_logistic_reg}
Let us consider the following two classification problems with a cross-entropy loss and linear model  parameterized by  

and

defined without and with label smoothing respectively, i.e., with  for . Let us denote by  and  a solution of~(\ref{eq:primal_logistic_reg_without_ls}) and (\ref{eq:primal_logistic_reg_with_ls}) respectively, together with 

It holds that the average entropy of the predictions of  is lower bounded as follows

If predicting with  also reduces the entropy of the average predictor, i.e., , then label smoothing increases the average entropy of the predictions:

\end{proposition}
To illustrate how both Mixup and label smoothing increase the entropy of predictions compared to ERM, we show on \Cref{fig:confidence-lenet} the histograms of 
the confidence of the estimators' predictions on test points, for a LeNet neural network trained on CIFAR-10. 
From the first plot, we notice how standard ERM produces very confident predictions, 
how label smoothing helps decreasing ERM confidence at test time, 
and how Mixup naturally produces even less confident predictions. 
From the second plot, we see that that approximate Mixup, like Mixup, produces less confident prediction, which confirms that the Mixup approximation we study captures well this behavior of Mixup.

\begin{figure*}[ht]
\centering
    \includegraphics[width=0.49\textwidth]{./imgs/hist_EEM.pdf}
    \includegraphics[width=0.49\linewidth]{./imgs/hist_EAM.pdf}
    \caption{
        \small{Histograms of confidence of predictions for models trained with different techniques.}
        }\label{fig:confidence-lenet}
\end{figure*}



\paragraph{Jacobian regularization.} The first implicit regularization term  in \Cref{thm:mixupreg} penalizes the discrepancy between  and  given by (\ref{eq:Ji}). We recognize in  the Jacobian of the standard MOLS model trained in the input space on the modified training set, with an increased weight for sample  in 
. Compared to, e.g., dropout regularization with penalizes the norm of  at the training points, we therefore see that Mixup also regularizes the Jacobian of  but with a different and more informative implicit bias, namely, to mimic a good linear model in the input space. Furthermore, we note from the proof of \Cref{thm:mixupreg} that this implicit bias results from the correlation between input and output noise, which may explain why independent Mixup in the input and output performs more poorly than standard Mixup~\citep{Zhang2018mixup}. While this regularization is similar across all points in the squared loss setting (Corollary~\ref{cor:SEloss}), it is weighted by the Hessian  in the cross-entropy loss (Corollary~\ref{cor:CEloss}). Similar to dropout, this implies that this regularization vanishes when the prediction  is confidently near  or . In the Mixup case, though, the label smoothing effect discussed in the previous paragraph tends to prevent over-confident predictions on the training point (see Proposition~\ref{prop:label_smoothing_logistic_reg} for a formal description of that property), therefore ensuring that the Jacobian regularization in  remains active even for ``easy'' points. This interaction between label smoothing (due to output Mixup) and Jacobian regularization (due to input Mixup) may explain why Mixup on inputs only performs poorly compared to Mixup on both inputs and outputs \citep{thulasidasan2019mixup}.


\section{Conclusions}
In this paper we have proposed the first theoretical analysis that explains the multiple regularization effects of Mixup. We have proved that training with Mixup is equivalent to learning on modified data with the injection of structured noise.
Through a Taylor approximation, we have further shown that 
Mixup
amounts to
empirical risk minimization on modified points plus multiple regularization terms. Fascinatingly,
our derivation shows that Mixup induces varied and complex effects, e.g., calibration, Jacobian regularization, label noise and normalization, 
while being a simple and cheap data augmentation technique.
Further, we have shown how this analysis points out a missing rescaling procedure required to evaluate functions learned with
Mixup, and we brought empirical evidence that implementing it improves accuracy and calibration.
More broadly, we have studied how a specific combination of data modification and noise injection leads to certain regularizers. An interesting research question is whether we can \textit{reverse-engineer} this process, namely starting from possibly expensive regularizers and design the corresponding data augmentation technique emulating their effects at a lower computational cost.

 
\section*{\bf Acknowledgments\ \ }
{The authors thank Jeremiah Zhe Liu for his feedback on an early version of this work.}


\bibliography{biblio}
 \newpage
\appendix
\section*{Appendix}
\section{Proofs}\label{sec:annex}

\subsection{Proof of \Cref{thm:mixupAsErm}}

\begin{proof}
To simplify notations, let us denote, for any  and ,

The Mixup risk (\ref{eq:mixup}) can then be written as

We now separate the values of  depending on whether or not they are above  by expressing it as

By symmetry of the  distribution around , it is clear that  defined in (\ref{eq:betatrick}) follows a  distribution, and furthermore that  follows, like , a  distribution. For any , we therefore get

where we used the fact that  to get the third equality. Plugging this equality back into (\ref{eq:mixupshort}), we finally get

where

We now easily see that  and  defined in (\ref{eq:modif}) satisfy

and furthermore that  and  defined in (\ref{eq:perturbations}) satisfy

from which we deduce that  and

Plugging this equality back into (\ref{eq:technical}) concludes the proof.
\end{proof}


\subsection{Proof of Lemma~\ref{lem:corr}}

\begin{proof}
From the definition of  in (\ref{eq:perturbations}), we easily get:

where we used the independence between  and  in the first equality; for the second, the facts that , , , and ; for the third, we reorganized the terms and used the equality  by definition of the empirical covariance matrix ; the last equality is obtained by reorganizing the terms and using the definition of . In order to write this covariance matrix in terms of modified inputs, we notice that by definition (\ref{eq:modif}) we have  and , which implies that the empirical covariance matrix of the modified inputs is . Combining these equalities with (\ref{eq:technical2}) gives the first equality in (\ref{eq:cov}). The two other equalities can be proved exactly the same way.
\end{proof}

\subsection{Proof of \Cref{thm:mixupreg}}

\begin{proof}
Given a modified input/output pair  and a function , the second-order Taylor approximation of the loss  is, for any :

Using this quadratic approximation at each training point  in (\ref{eq:mixupERM}), and using the fact that , we get

which we can rewrite as follows by expressing the derivatives of  in terms of derivatives of  and :

Replacing the expectations in this equation by their values given by Lemma~\ref{lem:corr} gives:

We now use the following fact, true for any square symmetric and invertible matrices  and  and rectangular matrices  and  (such that the matrix multiplications below make sense):

where , to combine the second and fifth terms together. Indeed, the fifth term (\ref{eq:technical3}) can be rewritten as

so plugging into (\ref{eq:matrixtrick}) the following matrices:

gives

where  is defined in (\ref{eq:Ji}).
Theorem~\ref{thm:mixupreg} then follows by merging the second and fifth terms in (\ref{eq:technical3}) using (\ref{eq:technical4}), and summing over .
\end{proof}

\subsection{Proof of Corollary~\ref{cor:CEloss}}

\begin{proof}
For the definition (\ref{eq:loss}) of the cross-entropy loss , we easily get:

Plugging these results back in the four regularization terms in \Cref{thm:mixupreg} we conclude the proof.
\end{proof}

\subsection{Proof of Corollary~\ref{cor:logisticloss}}

\begin{proof}
For the definition (\ref{eq:lrloss}) of the logistic regression loss , we easily get:

Plugging these results back in the four regularization terms in \Cref{thm:mixupreg} we conclude the proof.
\end{proof}

\subsection{Proof of Corollary~\ref{cor:SEloss}}

\begin{proof}
For the definition (\ref{eq:loss}) of the squared error loss , we easily get:

Plugging these results back in the 4 regularization terms in \Cref{thm:mixupreg} proves (\ref{eq:QmixupSE}).

When  is a linear function with intercept of the form , then we first note that  is a quadratic function of , so the second-order Taylor approximation (\ref{eq:taylor}) is exact in that case:
 for any  and , and consequently:


Applying (\ref{eq:QmixupSE}) to the case of a linear function  gives us immediately , because . For the first regularization term, we compute

As for the empirical risk term, we can also rewrite it as

Plugging (\ref{eq:technical10}) and (\ref{eq:technical11}) into (\ref{eq:QmixupSE}) finally gives (\ref{eq:mixupOLS}).

To see that the minimizer of (\ref{eq:mixupOLS}) is the standard MOLS solution, we notice that the obvious solution for  is , which is the intercept of MOLS, while the solution for  should minimize the sum of squared errors over centered points, which is exactly what MOLS does.
\end{proof}

\subsection{Proof of Proposition~\ref{prop:label_smoothing_logistic_reg}}

\begin{proof}
We first start by deriving a dual formulation for~(\ref{eq:primal_logistic_reg_with_ls}). The derivation for~(\ref{eq:primal_logistic_reg_without_ls}) follows along the same lines, up to the replacement of  by .

Introducing primal variables  for  with equality constraints  and the dual variables , we obtain the following Lagrangian (see~\citet{boyd2004convex}):

We recall that the entropy is concave and is the Fenchel conjugate, up to a sign flip, of the log-sum-exp function

as for instance detailed in Example 5.5 of~\citep{boyd2004convex}. We therefore derive the dual function using~(\ref{eq:fenchel_log_sum_exp})

and the dual problem

where we have made the change of variables . The dual problem for (\ref{eq:primal_logistic_reg_without_ls}) is identical to~(\ref{eq:dual_logitstic_reg}) up to the replacement of  by .

Recalling the definitons in~(\ref{eq:prediction_softmax_without_with_ls}) and exploiting the first-order optimality conditions of~(\ref{eq:primal_logistic_reg_without_ls}) and~(\ref{eq:primal_logistic_reg_with_ls}), we have

so that  is feasible for the dual problem~(\ref{eq:dual_logitstic_reg}).
Since strong duality applies~\citep{boyd2004convex}, it also holds that  maximize~(\ref{eq:dual_logitstic_reg}).

Let us consider  defined for  by

We can easily observe that  as convex combination of  and that

This implies that  is feasible for (\ref{eq:dual_logitstic_reg}) and we have

We get the advertised result by using the concavity of  so that .
\end{proof}




\section{Experiments}\label{sec:app_exp}
\subsection{CIFAR-10 and CIFAR-100}
To learn on CIFAR-10, CIFAR-100 we use ResNet-34 and LeNet.
For both architectures the optimizer used for training is SGD with momentum  for  epochs 
with mini-batch size , weight-decay .
For ResNet-34 the learning rate is  reduced by a factor  at epoch .
For LeNet the learning rate is  reduced by a factor  at epoch .

\subsection{ImageNet}
To learn on the ImageNet we use ResNet-50.
The optimizer used for training is SGD with Nesterov and momentum  for  epochs 
with mini-batch size  ( numbers of cores  per core batch-size), weight-decay .
The learning rate is  reduced by a factor  at epoch .

\subsection{Two Moons with Random Features}
We report in \Cref{fig:mess_vs_approx_acc_twomoons} and \Cref{fig:hist_twomoon} some more results on a synthetic binary classification problem (noisy two-moon problem),
where we train a logistic regression model with random Fourier features~\citep{rahimi2008random}.
This allowed us to get rid of convergence issues due to the convexity of the problem, but still work with nonlinear models of the input points. 
For each experimental result we report mean and 95\% confidence interval using 30 repetitions over 30 different instances of the data.

What we notice from these results is similar behaviors as reported for CIFAR-10, CIFAR-100, ImageNet in the main paper. 
\begin{figure*}[ht]
\hfill
    \minipage{0.33\textwidth}
    \includegraphics[width=\linewidth]{./imgs/train_err_mixup_vs_approx.pdf}
    \endminipage\hfill
    \minipage{0.33\textwidth}
    \includegraphics[width=\linewidth]{./imgs/test_acc_mixup_vs_approx.pdf}
    \endminipage\hfill 
    \minipage{0.33\textwidth}
    \includegraphics[width=\linewidth]{./imgs/test_err_mixup_vs_approx.pdf}
    \endminipage\hfill
\caption{
        \small{From left to right: train loss, 
        train and test accuracy during optimization of a 
        logistic regression model trained on the noisy two-moon problem with Mixup, Mixup (rescaled), 
        approximate Mixup and ERM risks.}
        }\label{fig:mess_vs_approx_acc_twomoons}
\end{figure*} 
\begin{figure*}[h]
\centering
    \minipage{0.42\textwidth}
    \includegraphics[width=\linewidth]{./imgs/test_hist_LS_mixup_erm_ermLS.pdf}
    \endminipage
    \hspace{1cm}
\minipage{0.42\textwidth}
    \includegraphics[width=\linewidth]{./imgs/test_hist_LS_approx_vs_mixup_vs_mixup_scaled.pdf}
    \endminipage\hfill
    \caption{\small{Histograms of confidence of predictions on test points, for models trained with different techniques.}}\label{fig:hist_twomoon}
\end{figure*}

\paragraph{Data generation.} 
To generate the data we use the \texttt{sklearn.datasets.make\_moons} function from the \texttt{scikit-learn} library.
We create  points with \emph{noise}, and split them in  for train and  for test. We then randomly flip  of the training labels to make the learning task more difficult. We repeat this pipeline 30 times for 30 different random seeds.

\paragraph{Function space.}
Let ,  and  be the feature map defined as , where  is the element-wise \emph{cosine} function,  is the random matrix s.t.  with , and  s.t. .
The space of candidate solutions  we consider is the class of functions of the form .

\paragraph{Optimization.}
To minimize any functional we use stochastic gradient descent with mini-batching, with mini-batch size  and step-size .

\paragraph{Mixup hyperparameter.} We consider the Beta distribution in Mixup and its approximation to be Beta with .


 

\end{document}