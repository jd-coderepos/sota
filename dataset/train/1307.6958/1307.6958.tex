\documentclass{article}

\usepackage{ra4}


\usepackage{etex}

\newcommand{\TL}[1]{\tag{#1}\label{#1}}

\pagestyle{plain}
\newcommand{\tab}[1]{\phantom{#1}}
\newcommand{\taba}{\phantom{\wedge}}
\newcommand{\tabo}{\phantom{\vee}}
\newcommand{\tabi}{\phantom{\imp}}

\usepackage{times}
\usepackage{mine}
\usepackage{yhmath}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{oz}
\usepackage{zed-csp}
\usepackage{graphicx}
\renewcommand*\ttdefault{txtt}
\usepackage[T1]{fontenc}

\usepackage{mathrsfs}
\usepackage{rcp}
\usepackage{color}		
\usepackage{epsfig}
\usepackage{stmaryrd}

\def \hasgn{\asgn}
\def\cdota{\!\cdot\!}
\def \Eval{{\sf eval}} 
\def \Holds{{\sf holds}} 
\def \Update{{\sf update}}

\def \reachable{{\sf RE}}


\newcommand{\AG}[1]{\aang{#1}} 
\newcommand{\Idle}{{\sf Idle}}
\newcommand{\FinIdle}{{\sf fin\_Idle}}
\newcommand{\InfIdle}{{\sf inf\_Idle}}
\newcommand{\False}{{\sf false}} 
\newcommand{\Fin}{{\sf fin}}
\newcommand{\Inf}{{\sf inf}}

\renewcommand{\qed}{\ensuremath{{}_\Box}}
\newcommand{\lub}{{\sf lub}}
\newcommand{\glb}{{\sf glb}}

\newcommand{\True}{{\sf True}}
\renewcommand{\Chaos}{{\sf Chaos}}

\def\Read{\mathop{\textbf{read}}}
\def\Write{\mathop{\textbf{write}}}
\def\Return{\mathop{\textbf{return}}}
\newcommand{\st}{荏泸轲趔泸轲趔豉戾捃怩祆弭

\newcommand{\field}{荏泸轲趔泸轲趔豉戾捃镳祯簖

\def\figrule{\rule{\columnwidth}{0.5pt}}
\def\uop{\mathop{\ominus}}
\def\bop{\mathbin{\oplus}}
\def \rely {\mathop{\textsc{Rely}}}
\def \Init {\mathop{\textsc{Init}}}
\def \intfree {\mathop{\textsc{IntFree}}}
\def \enf {\mathop{\textsc{Enf}}}
\def\tb {\dagger}

\def\Free{FN}
\newcommand{\lv}[1]{{\sf #1}}
\def\piref{\mathbin{{}_\pi\!\!\sqsupseteq}}

\def\llpar{\llparenthesis}
\def\rrpar{\rrparenthesis}


\newcommand{\NoteEnv}[3]{\newenvironment{#1}{\color{#3}#2 }{}}
\definecolor{brijeshcolor}{rgb}{1,0.2,0}
\definecolor{johncolor}{cmyk}{1,0.3,0.4,0.3}

\NoteEnv{brijesh}{Brijesh says:}{brijeshcolor}
\NoteEnv{modified}{}{blue}
\NoteEnv{ian}{Ian}{iancolor}
\NoteEnv{lindsay}{Lindsay}{lindsaycolor}
\NoteEnv{john}{John}{johncolor}

\let\ddot=\widehat 

\DeclareMathSymbol{\Diamond}{\mathord}{lasy}{"33}



\usepackage{amsthm}
\theoremstyle{plain}
\newcounter{thm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{example}{Example}[section]


\newcommand\notesb[4]{\todo[disable,linecolor=red,backgroundcolor=yellow!50,size=\small]{#4}{#3}
}
\newcommand\notesbin[4]{\todo[disable,inline,linecolor=orange!80!black,backgroundcolor=yellow!50,size=\small]{#4}{#3}
}






\newcommand{\modsb}[2]{\notesb{}{blue}{#1}{#2}}
\newcommand{\modsbin}[2]{\notesbin{}{blue}{#1}{#2}}
\newcommand{\brijeshsb}[2]{\notesb{BD}{brijeshcolor}{#1}{#2}}
\newcommand{\iansb}[2]{\notesb{IJH}{iancolor}{#1}{#2}}

\newcommand{\lindsaysb}[2]{\notesb{LG}{lindsaycolor}{#1}{#2}}
\newcommand{\johnsb}[2]{\notesb{JD}{johncolor}{#1}{#2}}


\def \llb {\llbracket}
\def \rrb {\rrbracket}
\def\abssynt{\mathop{:\joinrel:\joinrel=}}

\newcommand{\Frame}[2]{\llb #1 \cdota #2 \rrb}
\newcommand{\Constant}[2]{\textsc{Con}#1 \st #2}
\newcommand{\Context}[2]
{\!\llb #1 
    \begin{array}[c]{@{~}|@{~}l@{}}
      #2
    \end{array}\rrb } \newcommand{\Var}[2]{\textsc{Var}#1 \st #2}
\newcommand{\Intfree}[2]{\mathop{\intfree} #1 \st #2}

\newcommand{\prev}{\varominus}

\newcommand{\Rely}[2]
{\rely #1 \st
    \begin{array}[c]{@{}l@{}}
      #2
    \end{array}}

\newcommand{\Enf}[2]
{\enf #1 \st
    \begin{array}[c]{@{}l@{}}
      #2
    \end{array}}

\newcommand{\Code}[1]
{\begin{array}[c]{@{}l@{}}
      #1
    \end{array}}


\makeatletter
\newcommand*\wt[1]{\mathpalette\wthelper{#1}}
\newcommand*\wthelper[2]{\hbox{\dimen@\accentfontxheight#1\accentfontxheight#11.5\dimen@
                \accentfontxheight#1\dimen@
        }}

\newcommand*\accentfontxheight[1]{\fontdimen5\ifx#1\displaystyle
                \textfont
        \else\ifx#1\textstyle
                \textfont
        \else\ifx#1\scriptstyle
                \scriptfont
        \else
                \scriptscriptfont
        \fi\fi\fi3
}
\makeatother

\newcommand{\EFrame}[1]{\llb #1 \rrb}
\newcommand{\Par}{\textstyle\mathop{\|}}
\newcommand{\Mod}{{\sf mod}}
\newcommand{\Empty}{{\sf empty}}
\newcommand{\Always}{\textstyle\mathord{\boxdot}}
\newcommand{\Sometime}{\raisebox{0.1em}{\rotatebox[origin=c]{45}{}}}
\newcommand{\Def}{\textstyle\mathord{\boxast}}
\newcommand{\Pos}{\raisebox{0.1em}{\rotatebox[origin=c]{45}{}}}

\newcommand{\Defp}{\mathop{\Def_p}}
\newcommand{\Posp}{\mathop{\Pos\!_p}}

\newcommand{\HAlways}{\widetilde{\Always}}
\newcommand{\HSometime}{\wt{\Sometime}}
\newcommand{\HDef}{\widetilde{\Def}}
\newcommand{\HPos}{\wt{\mbox{\Pos}}}

\def\TOT{{\sf T}}
\def\adjoins{\mathbin{\varpropto}}
\def\simpadjoins{\mathbin{\underline\varpropto}}
\def\ch{\mathbin{;}}
\def \kout {\mathop{{\sf Out}}}
\def \kwin {\mathop{{\sf in}}}
\def \kif{\mathop{\mathsf{if}}}
\def \kthen{\mathbin{\mathsf{then}}}
\def \kelse{\mathbin{\mathsf{else}}}
\def \kelseif{\mathbin{\mathsf{elseif}}}
\def \klet {\mathop{{\sf let}}}
\def \kin {\mathbin{{\sf in}}}

\def \bs {\backslash}


\def \deref{\mathop{*}}





\def \dom {\mathrm{dom}}
\def \ran {\mathrm{ran}}


\def \seq {\mathrm{seq}}


\newcommand{\Add}[1]{{\tt add(#1)} }
\newcommand{\Remove}[1]{{\tt remove(#1)} }
\newcommand{\Contains}[1]{{\tt contains(#1)} }

\newcommand{\olh}[1]{\overleftarrow{#1}}
\newcommand{\orh}[1]{\overrigharrow{#1}}





\title{Simplifying proofs of linearisability using layers of
  abstraction} 

\author{Brijesh Dongol\ and\ John Derrick \\
  \\
  \small{Department of Computer Science} \\
  \small{The University of Sheffield, S1 4DP, UK} \\
  \small{\texttt{B.Dongol@sheffield.ac.uk, J.Derrick@dcs.shef.ac.uk}}}

\date{}



\begin{document}




\maketitle

\begin{abstract}
  Linearisability has become the standard correctness criterion for
  concurrent data structures, ensuring that every history of
  invocations and responses of concurrent operations has a matching
  sequential history. Existing proofs of linearisability require one
  to identify so-called linearisation points within the operations
  under consideration, which are atomic statements whose execution
  caus\-es the effect of an operation to be felt. However,
  identification of linearisation points is a non-trivial task,
  requiring a high degree of expertise. For sophisticated algorithms
  such as Heller et al's lazy set, it even is possible for an
  operation to be linearised by the concurrent execution of a
  statement outside the operation being verified. This paper
  proposes an alternative method for verifying linearisability that
  does not require identification of linearisation points. Instead,
  using an interval-based logic, we show that every behaviour of
  each concrete operation over any interval is a possible behaviour
  of a corresponding abstraction that executes with coarse-grained
  atomicity. This approach is applied to Heller et al's lazy set to
  show that verification of linearisability is possible without
  having to consider linearisation points within the program code.
\end{abstract}





\section{Introduction}
\label{sec:intro}
Development of correct fine-grained concurrent data structures has
received an increasing amount of attention over the past few years as
the popularity of multi/many-core architectures has increased. An
important correctness criterion for such data structures is
\emph{linearisability} \cite{Herlihy90}, which guarantees that every
history of invocations and responses of the concurrent operations on
the data structure can be rearranged without violating the ordering
within a process such that the rearranged history is a valid
sequential history. A number of proof techniques developed over the years match concurrent
and sequential histories by identifying an atomic \emph{linearising
  statement} within the concrete code of each operation, whose
execution corresponds to the effect of the operation taking place.
However, due to the subtlety and complexity of concurrent data
structures, identification of linearising statements within the
concrete code is a non-trivial task, and it is even possible for an
operation to be linearised by the execution of other concurrent
operations. An example of such behaviour occurs in Heller et al's lazy
set algorithm, which implements a set as a sorted linked list
\cite{HHLMSS07} (see \reffig{fig:lazyset}). In particular, its {\tt
  contains} operation may be linearised by the execution of a
concurrent {\tt add} or {\tt remove} operation and the precise
location of the linearisation point is dependent on how much of the
list has been traversed by the {\tt contains} operation.  This paper
presents a method for simplifying proofs of linearisability using
Heller et al's lazy set as an example.




An early attempt at verifying linearisability of Heller et al's lazy
set is that of Vafeiadis et al, who extend each linearising statement
with code corresponding to the execution of the abstract operation so
that execution of a linearising statement causes the corresponding
abstract operation to be executed \cite{VHHS06}. However, this
technique is incomplete and cannot be used to verify the {\tt
  contains} operation, and hence, its correctness is only treated
informally \cite{VHHS06}. These difficulties reappear in more recent
techniques: ``In [Heller et al's lazy set] algorithm, the correct
abstraction map lies outside of the abstract domain of our
implementation and, hence, was not found.''  \cite{Vaf10}.  The first
complete linearisability proof of the lazy set was given by Colvin et
al \cite{CGLM06}, who map the concrete program to an abstract set
representation using simulation to prove data refinement. To verify
the {\tt contains} operation, a combination of forwards and backwards
simulation is used, which
involves the development of an intermediate program  such that
there is a backwards simulation from the abstract representation to
, and a forwards simulation from  to the concrete
program. More recently, O'Hearn et al use a so-called hindsight lemma
(related to backwards simulation) to verify a variant of Heller's lazy
set algorithm \cite{OHea10}.  Derrick et al use a method based on
\emph{non-atomic} refinement, which allows a single atomic step of the
concrete program to be mapped to several steps of the abstract
\cite{DSW11}.

Application of the proof methods in \cite{VHHS06,CGLM06,OHea10,DSW11}
remains difficult because one must acquire a high degree of expertise
of the program being verified to correctly identify its linearising
statements. For complicated proofs, it is difficult to determine
whether the implementation is erroneous or the linearising statements
have been incorrectly chosen. Hence, we propose an approach that
eliminates the need for identification of linearising statements in
the concrete code by establishing a refinement between the
fine-grained implementation and an abstraction that executes with
coarse-grained atomicity \cite{DD12}. The idea of mapping fine-grained
programs to a coarse-grained abstraction has been proposed by Groves
\cite{Gro08} and separately Elmas et al \cite{EQSST10}, where the
refinements are justified using \emph{reduction}
\cite{Lip75}. However, unlike our approach, their methods must
consider each pair of interleavings, and hence, are not
compositional. Turon and Wand present a method of abstraction in a
compositional rely/guarantee framework with separation logic
\cite{TW11}, but only verify a stack algorithm that does not require
backwards reasoning.





Capturing the behaviour of a program over its interval of execution is
crucial to proving linearisability of concurrent data structures. In
fact, as Colvin et al point out: ``The key to proving that [Heller et
al's] lazy set is linearisable is to show that, for any failed {\tt
  contains(x)} operation, {\tt x} is absent from the set at some point
during its execution.''  \cite{CGLM06}.  Hence, it seems
counter-intuitive to use logics that are only able to refer to the pre
and post states of each statement (as done in
\cite{VHHS06,CGLM06,DSW11,Vaf10}). Instead, we use a framework based
on \cite{DDH12} that allows reasoning about the fine-grained atomicity
of pointer-based programs over their intervals of execution. By
considering complete intervals, i.e., those that cover both the
invocation and response of an operation, one is able to determine the
future behaviour of a program, and hence, backwards reasoning can
often be avoided. For example, B{\"a}umler et al \cite{BSTR11} use an interval-based
approach to verify a lock-free queue without resorting to backwards
reasoning, as is required by frameworks that only consider the
pre/post states of a statement \cite{DGLM04}. However, unlike our
approach, B{\"a}umler et al must identify the linearising statements
in the concrete program, which is a non-trivial step.

An important difference between our framework and those mentioned
above is that we assume a truly concurrent execution model and only
require interleaving for conflicting memory accesses
\cite{DD12,DDH12}. Each of the other frameworks mentioned above assume
a strict interleaving between program statements. Thus, our approach
captures the behaviour of program in a multicore/multiprocesor
architecture more faithfully.  

The main contribution of this paper is the use of the techniques in
\cite{DD12} to simplify verification of a complex set algorithm by
Heller et al. This algorithm presents a challenge for linearisability
because the linearisation point of the {\tt contains} operation is
potentially outside the operation itself \cite{DSW11}. We propose a
method in which the proof is split into several layers of abstraction
so that linearisation points of the fine-grained implementation need
not be identified. As summarised in \reffig{fig:steps}, one must
additionally prove that the coarse-grained abstraction is
linearisable, however, due to the coarse granularity of atomicity, the
linearising statements are straightforward to identify and the
linearisability proof itself is simpler \cite{DD12}. Other
contributions of this paper include a method for reasoning about truly
concurrent program executions and an extension of the framework in
\cite{DDH12} to enable reasoning about pointer-based programs, which
includes methods for reasoning about expressions non-deterministically
\cite{HBDJ13}.





\section{A list-based concurrent set}  
\label{sec:list-based-conc}
\begin{figure}[t]
  \centering\small

  \fbox{\begin{minipage}[t]{.3\textwidth}
    \tt 
  add(x): 
    
  \ A1: n1, n3:= 

    \vspace{-0.3em}    
    \ \ \ \ \ \ \ \ locate(x); 

    \ A2: {\bf if} n3.val != x 


    \ A3: \ \ n2:= 

    \vspace{-0.3em}    
    \ \ \ \ \ \ \ \ \ {\bf new} Node(x); 

    \ A4: \ \ n2.nxt := n3; 

    \ A5: \ \ n1.nxt := n2; 

    \ A6: \ \ res := true 

    \ A7: {\bf else} res := false 
    
    \ \ \ \ \ {\bf endif};

    \ A8: n1.unlock(); 

    \ A9: n3.unlock(); 

    A10:\ {\bf return} res
  \end{minipage}}
\fbox{
  \begin{minipage}[t]{.3\textwidth}
    \tt 
    remove(x): 
    
    \ R1:\  n1, n2 := 

    \vspace{-0.3em}    
    \ \ \ \ \ \ \ \ locate(x); 

    \ R2:\  {\bf if} n2.val = x 

    \ R3:\ \ \ n2.mrk := true;

    \ R4:\  \ \ n3 := n2.nxt;

    \ R5:\  \ \ n1.nxt := n3;

    \ R6:\  \ \ res := true

    \ R7:\ {\bf else} res := false 

    \ \ \ \ \ {\bf endif};

    \ R8:\ n1.unlock();

    \ R9:\ n2.unlock();

    R10:\ {\bf return} res    
    \

    \vspace{-.3em}
    \
  \end{minipage}
  }
  \fbox{
  \begin{minipage}[t]{.3\textwidth}
    \tt 
    contains(x): 
    
    C1: n1 := Head; 

    C2: {\bf while} (n1.val < x) 

    C3: \ \ n1 := n1.nxt
    
    \ \ \ \ {\bf enddo};

    C4: res :=  (n1.val = x) 

    \vspace{-0.3em}
    \ \ \ \ \ \ \ \ \ \ \ {\bf and} !n1.mrk 
    
    C5: {\bf return} res
    \ 
    
    \

    \ 
    
    \
    
    \
     
    \
    

    \vspace{-.3em}
    \
  \end{minipage}
  }
  
  \fbox{
    \begin{minipage}[t]{0.475\textwidth}
    \tt 
    locate(x):

    \ \ \ \ \ {\bf while} (true) {\bf do}
    
    \ L1:\ \ \ pred := Head; 

    \ L2:\ \ \ curr := pred.nxt; 

    \ L3:\ \ \ {\bf while} (curr.val < x) {\bf do} 

    \ L4:\ \ \ \ \ pred := curr; 

    \ L5:\ \ \ \ \ curr := pred.nxt {\bf enddo};

    \ L6:\ \ \ pred.lock();
  \end{minipage}
    \begin{minipage}[t]{0.47\textwidth}
    \tt 
    \
    
    \

    \ L7:\ \ \ curr.lock();

    \ L8:\ \ \ {\bf if} !pred.mrk 
    {\bf and} !curr.mrk 

    \ \ \ \ \ \ \ \ \ \ 
    {\bf and} pred.nxt = curr
    
    \ L9:\ \ \ \ \ {\bf return} pred, curr

    L10:\ \ \ {\bf else} pred.unlock(); 
    
    L11:\ \ \ \ \ curr.unlock() {\bf endif} {\bf enddo}
  \end{minipage}
  }
  \caption{Heller et al's lazy set algorithm}
  \label{fig:lazyset}
\end{figure}



Heller et al \cite{HHLMSS07} implement a set as a concurrent algorithm
operating on a shared data structure (see \reffig{fig:lazyset}) with
operations {\tt add} and {\tt remove} to insert and delete elements
from the set, and an operation {\tt contains} to check whether an
element is in the set. The concurrent implementation uses a shared
linked list of node objects with fields , and ,
where  stores the value of the node,  is a pointer to the
next node in the list,  denotes the marked bit and  stores
the identifier of the process that currently holds the lock to the
node (if any) \cite{HHLMSS07}.  The list is sorted in strictly
ascending values order (including marked nodes).

Operation {\tt locate(x)} is used to obtain pointers to two nodes
whose values may be used to determine whether or not {\tt x} is in the
list --- the value of the predecessor node {\tt pred} must always be
less than {\tt x}, and the value of the current node {\tt curr} may
either be greater than {\tt x} (if {\tt x} is not in the list) or
equal to {\tt x} (if {\tt x} is in the list). Operation {\tt add(x)}
calls {\tt locate(x)}, then if {\tt x} is not already in the list
(i.e., value of the current node {\tt n3} is strictly greater than
{\tt x}), a new node {\tt n2} with value field {\tt x} is inserted
into the list between {\tt n1} and {\tt n3} and {\tt true} is
returned. If {\tt x} is already in the list, the {\tt add(x)}
operation does nothing and returns {\tt false}. Operation {\tt
  remove(x)} also starts by calling {\tt locate(x)}, then if {\tt x}
is in the list the current node {\tt n2} is removed and {\tt true} is
returned to indicate that {\tt x} was found and removed. If {\tt x} is
not in the list, the {\tt remove} operation does nothing and returns
{\tt false}. Note that operation {\tt remove(x)} distinguishes between
a logical removal, which sets the marked field of {\tt n2} (the node
corresponding to {\tt x}), and a physical removal, which updates the
{\tt nxt} field of {\tt n1} so that {\tt n2} is no longer
reachable. Operation {\tt contains(x)} iterates through the list and
if a node with value greater or equal to {\tt x} is found, it returns
{\tt true} if the node is unmarked and its value is equal to {\tt x},
otherwise returns {\tt false}.



\begin{figure}[!t]
  \begin{minipage}[b]{0.65\textwidth}
    \scalebox{0.8}{\begin{picture}(0,0)\includegraphics{exec_pspdftex}\end{picture}\setlength{\unitlength}{4144sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(4773,2326)(931,-4490)
\put(1666,-3886){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3106,-3796){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}{\tt add(x)}}}}}}
\put(3961,-3571){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4726,-3481){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}{\tt add(y)}}}}}}
\put(2296,-3481){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}{\tt remove(x)}}}}}}
\put(1441,-3571){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3016,-4111){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}{\tt LazySet}}}}}}
\put(1351,-4201){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(946,-4426){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1936,-3211){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3466,-3121){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}{\tt contains(x)}}}}}}
\put(3601,-2581){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2341,-2806){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}{\tt C1}}}}}}
\put(3106,-2806){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}{\tt C2}  {\tt C3}}}}}}
\put(3736,-2806){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}{\tt C4}}}}}}
\put(4501,-2806){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}{\bf return} }}}}}
\put(4501,-2356){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}{\bf return} }}}}}
\put(3106,-2356){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture} }      
    \caption{Execution of {\tt contains(x)} over  that
      returns }
    \label{fig:tf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.32\textwidth}
    \scalebox{0.85}{\begin{picture}(0,0)\includegraphics{steps_pspdftex}\end{picture}\setlength{\unitlength}{4144sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(2569,2139)(1649,-3403)
\put(2926,-1501){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}Abstract sequential program}}}}}
\put(2296,-2851){\makebox(0,0)[lb]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}Behaviour refinement}}}}}
\put(2926,-3301){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}Fine-grained implementation}}}}}
\put(2926,-2401){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}Coarse-grained abstraction}}}}}
\put(2296,-1951){\makebox(0,0)[lb]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}Linearisability proof}}}}}
\end{picture} }
    \caption{Proof steps}
    \label{fig:steps}
  \end{minipage}
\end{figure}

The complete specification consists of a number of processes, each of
which may execute its operation on the shared data structure. For the
concrete implementation, therefore, the set operations can be executed
concurrently by a number of processes, and hence, the intervals in
which the different operations execute may overlap. Our basic
semantic model uses \emph{interval predicates} (see
\refsec{sec:an-interval-based}), which allows formalisation of a
program's behaviour with respect to an \emph{interval} (which is a
contiguous set of times), and an infinite \emph{stream} (that maps each
time to a state). For example, consider \reffig{fig:tf}, which depicts
an execution of the lazy set over interval  in stream , a
process  that executes a {\tt contains(x)} that returns  over
, a process  that executes {\tt remove(x)} and {\tt
  add(y)} over intervals  and , respectively, and
a process  that executes {\tt add(x)} over interval
. Hence, the shared data structure may be changing over
 while process  is checking to see whether  is in the
set.

Correctness of such concurrent executions is judged with respect to
\emph{linearisability}, the crux of which requires the existence of an
atomic \emph{linearisation point} within each interval of an
operation's execution, corresponding to the point at which the effect
of the operation takes place \cite{Herlihy90}. The ordering of
linearisation points defines a sequential ordering of the concurrent
operations and linearisability requires that this sequential ordering
is valid with respect to the data structure being implemented.  For
the execution in \reffig{fig:tf}, assuming that the set is initially
empty, because {\tt contains(x)} returns , a valid linearisation
corresponds to a sequential execution  obtained by picking linearisation
points within , ,  and  in
order. Note that a single concurrent history may be linearised by more
than one valid sequential history, e.g., the execution in
\reffig{fig:tf} can correspond to the sequential execution . The abstract
sets after completion of  and  are  and
, respectively. Unlike , operation {\tt remove(x)} in
 returns . Note that a linearisation of 
cannot occur before  because {\tt remove(x)} responds before
the invocation of {\tt add(y)}.

Herlihy and Wing formalise linearisability in terms of histories of
invocation and response events of the operations on the data structure
in question \cite{Herlihy90}. Clearly, reasoning about such histories
directly is infeasible, and hence, existing methods (e.g.,
\cite{CGLM06,DSW11,VHHS06}) prove linearisability by identifying an
atomic {\em linearising statement} within the operation being verified
and showing that this statement can be mapped to the execution of a
corresponding abstract operation.  However, due to the fine
granularity of the atomicity and inherent non-determinism of
concurrent algorithms, identification of such a statement is
difficult. The linearising statement for some operations may actually
be outside the operation, e.g., none of the statements {\tt C1}-{\tt
  C5} are valid linearising statements of {\tt contains(x)}; instead
{\tt contains(x)} is linearised by the execution of a statement within
{\tt add(x)} or {\tt remove(x)} \cite{DSW11}.


As summarised in \reffig{fig:steps}, we decompose proofs of
linearisability into two steps, the first of which proves that a
fine-grained implementation refines a program that executes the same
operations but with coarse-grained atomicity. The second step of the
proof is to show that the abstraction is linearisable. The atomicity
of a coarse-grained abstraction cannot be guaranteed in hardware
(without the use of contention inducing locks), however, its
linearisability proof is much simpler \cite{DDH12}. Because we prove
behaviour refinement, any behaviour of the fine-grained implementation
is a possible behaviour of the coarse-grained abstraction, and hence,
an implementation is linearisable whenever the abstraction is
linearisable. Our technique does not require identification of the
linearising statements in the implementation.

A possible coarse-grained abstraction of {\tt contains(x)} is an
operation that is able to test whether {\tt x} is in the set in a
single atomic step (see \reffig{fig:labs}), unlike the implementation
in \reffig{fig:lazyset}, which uses a sequence of atomic steps to
iterate through the list to search for a node with value {\tt x}.
Therefore, as depicted in \reffig{fig:tf}, an execution of {\tt
  contains} that returns , i.e., , is
required to refine a coarse-grained abstraction , where {\tt C1} - {\tt C4} are the labels of
{\tt contains} in \reffig{fig:lazyset} and  is a
guard that is atomically able to test whether  is in the abstract
set.  In particular,  holds in an interval
 and stream  iff there is a time  in  such that
. Streams are formalised in
\refsec{sec:interval-predicates}. Note that both 
and  may hold within ; the
refinement in \reffig{fig:tf} would only be invalid if for all ,  holds. 


Proving refinement between a coarse-grained abstraction and an
implementation is non-trivial due to the execution of other
(interfering) concurrent processes. Furthermore, our execution model
allows non-conflicting statements (e.g., concurrent writes to
different locations) to be executed in a truly concurrent manner. We
use compositional rely/guarantee-style reasoning \cite{Jon83} to
formalise the behaviour of the environment of a process and allow the
execution of an arbitrary number of processes in the environment. Note
that unlike Jones \cite{Jon83}, who assumes rely conditions are
two-state relations, rely conditions in our framework are interval
predicates that are able to refer to an arbitrary number of states.
 





\section{Interval-based framework}
\label{sec:an-interval-based}

To simplify reasoning about the linked list structure of the lazy
list, the domain of each state distinguishes between variables and
addresses. We use a language with an abstract syntax that closely
resembles program code, and use interval predicates to formalise
interval-based behaviour. Fractional permissions are used to control
conflicting accesses to shared locations.


\paragraph{Commands.}
We assume variable names are taken from the set , values have
type , addresses have type ,  and . A \emph{state} over  has type 
and a \emph{state predicate} has type .

The objects of a data structure may contain fields, which we assume
are of type . We assume that every object with  fields is
assigned  contiguous blocks of memory and use  to obtain the offset of  within this block
\cite{Vaf07}, e.g., for the fields of a node object, we assume that
, ,  and .

We assume the existence of a function  that evaluates a given
expression in a given state. The full details of expression evaluation
are elided. To simplify modelling of pointer-based programs, for an
address-valued expression , we introduce expressions ,
which returns the value at address , , which returns
the address of  with respect to . For a state , we
define  and
. We also define
shorthand , which returns
the value at  in state .



\begin{figure}[!t]
  \centering
  \figrule \small

  
  \smallskip
  
  
  \figrule
  \caption{Formal model of the lazy set operations}
  \label{fig:formal-lazyset}
\end{figure}



Assuming that  denotes the set of process ids, for a set of
variables , state predicate , variable or address-valued
expression , expression , label , and set of processes , the abstract syntax of a command is given by 
below, where .

Hence a command is either , a
guard , an atomically evaluated guard , an assignment , a
sequential composition , a non-deterministic choice , a possibly infinite
iteration , a parallel composition , a
command  within a context  (denoted ), or a labelled 
command . 

A formalisation of part of Heller et al's lazy list using the syntax
above is given in \reffig{fig:formal-lazyset}, where . Operations {\tt add(x)}, {\tt remove(x)} and {\tt contains(x)}
executed by process  are modelled by commands ,
 and , respectively. We
assume that  denotes . Details of  and
 are elided and the  construct is
formalised in \refsec{sec:behaviour-refinement-1}.\footnote{The
  formalisation is given in Appendix A.}  Note that unlike the methods
in \cite{CGLM06,DSW11}, where labels identify the atomicity, we use
labels to simplify formalisation of the rely conditions of each
process, and may correspond to a number of atomic steps.
Furthermore, guard evaluation is formalised with respect to the set of states
apparent to a process (see \refsec{sec:eval-state-pred}), and hence,
unlike \cite{VHHS06,CGLM06,DSW11}, we need not split complex
expressions into their atomic components. For example, in \cite{VHHS06,CGLM06,DSW11}, the expression at {\tt C4}
(\reffig{fig:lazyset}) must be split into two expressions {\tt
  curr.val = x} and {\tt !curr.mrk} to explicitly model the fact that
interference may occur between accesses to {\tt curr.val} and {\tt
  curr.mrk}. 

























\paragraph{Interval predicates.}
\label{sec:interval-predicates}
A (discrete) {\em interval} (of type ) is a contiguous set of
time (of type ), i.e., .  Using `.' for function application, we
let  and  denote the \emph{least upper} and
\emph{greatest lower} bounds of an interval , respectively,
where  and . We define ,  and .  For a set  and , we let  denote the closed interval from  to
 containing elements from .  One must often reason about two
\emph{adjoining} intervals, i.e., intervals that immediately precede
or follow a given interval. We say  adjoins  iff
, where 

\noindent
Note that adjoining intervals  and  must be disjoint,
and by conjunct , the union of 
and  must be contiguous. Note that both  and  hold trivially for any
interval .


A \emph{stream} of behaviours over  is
given by a total function of type , which maps each time to a state over . To reason
about specific portions of a stream, we use \emph{interval
  predicates}, which have type . Note that because a stream encodes the
behaviour over all time, interval predicates may be used to refer to
the states outside a given interval.
Like Interval Temporal Logic
\cite{Mos00}, we may define a number of operators on interval
predicates. For example, if ,
 and , we define:

\noindent
We assume pointwise lifting of operators on stream and interval
predicates in the normal manner, define \emph{universal implication}  for interval predicates  and , and say
 holds iff both  and  hold.

We
define two operators on interval predicates: \emph{chop}, which is
used to formalise sequential composition, and -{\em
  iteration}, which is used to formalise a possibly infinite iteration
(e.g., a while loop). The \emph{chop} operator `;' is a basic operator
on two interval predicates \cite{Mos00,DDH12,DH12MPC}, where  holds iff either interval  may be split into two
parts so that  holds in the first and  holds in the second,
or the least upper bound of  is  and  holds in
. The latter disjunct allows  to formalise an execution
that does not terminate. Using chop, we define the possibly infinite
iteration (denoted ) of an interval predicate  as the
greatest fixed point of , where the
interval predicates are ordered using `' (see \cite{DHMS12}
for details).  We define

\noindent
In the definition of , interval  may be empty,
in which case , and similarly  may empty,
in which case . Hence, both  and  trivially hold. An iteration
 of  may iterate  a finite (including zero) number of
times, but also allows an infinite number of iterations \cite{DHMS12}.











\paragraph{Permissions and interference.}  
To model true concurrency, the behaviour of the parallel composition
between two processes in an interval  is modelled by the
conjunction of the behaviours of both processes executing within
. Because this potentially allows conflicting accesses to
shared variables, we incorporate fractional permissions into our
framework \cite{Boy03,DDH12}.  We assume the existence of a {\em
  permission variable} in every state  of type
, where  and
 denotes the set of rationals.
A process  has {\em write-permission} to location  in  iff ; has {\em
  read-permission} to  in  iff ;
and has {\em no-permission} to access  in  iff
.


We define  and
 and  to be state predicates on permissions.
In the context of a stream , for any time , process
 may only write to and read from  in the transition step from
 to  if  and  hold,
respectively. Thus,  does not give  permission to write to 
in the transition from  to  (and similarly ).
For example, to state that process  updates variable  to value
 at time  of stream , the effect of the update should imply
.

One may introduce healthiness conditions on streams that formalise our
assumptions on the underlying hardware. We assume that at most one
process has write permission to a location  at any time, which is
guaranteed by ensuring the sum of the permissions of the processes on
 at all times is at most , i.e.,  Other
conditions may be introduced to model further restrictions as required
\cite{DDH12}.

Fractional permissions may also be used to characterise interference
within a process . For a set of variables, we define . Such notions are particularly useful because we aim to develop
rely/guarantee-style reasoning, where we use rely conditions to
characterise the behaviour of the environment. One may introduce rely
conditions that refer to  to characterise the interference
on  by the environment of .



\section{Evaluating state predicates over intervals}
\label{sec:eval-state-pred}

The set of times within an interval corresponds to a set of states
with respect to a given stream. Hence, if one assumes that expression
evaluation is non-atomic (i.e., takes time), one must consider
evaluation with respect to a set of states, as opposed to a single
state. It turns out that there are a number of possible ways in which
such an evaluation can take place, with varying degrees of
non-determinism \cite{HBDJ13}. In this paper, we consider \emph{actual
  states evaluation}, which evaluates an expression with respect to
the set of actual states that occur within an interval and
\emph{apparent states evaluation}, which considers the set of states
apparent to a given process.

Actual states evaluation allow one to reason about the true state of a
system, and evaluates an expression instantaneously at a single point
in time. However, a process executing with fine-grained atomicity can
only read a single variable at a time, and hence, will seldom be able
to view an actual state because interference may occur between two
successive reads. For example, a process  evaluating  (the
expression at ) cannot read both  and
 in a single atomic step, and hence, may obtain a
value for  that is different from any actual value of 
because interference may occur between reads to  and
. Therefore, we define an apparent states evaluator
that models fine-grained expression evaluation over intervals. Our
definition of apparent states evaluation does not fix the order in
which  and  are read. We see this as
advantageous over frameworks that must make the atomicity explicit
(e.g., \cite{VHHS06,CGLM06,DSW11}), which require an ordering to be
chosen, even if an evaluation order is not specified by the
corresponding implementation (e.g., \cite{HHLMSS07}). In
\cite{VHHS06,CGLM06,DSW11}, if the order of evaluation is modified,
the linearisability proof must be redone, whereas our proof is more
general because it shows that any order of evaluation is valid.










\smallskip

\noindent{\bf Evaluation over actual states.}
To formalise evaluators over actual states, for an interval 
and stream , we define . Two useful operators for a sets of actual states of a state
predicate  are  and , which specify that
 holds in \emph{some} and \emph{all} actual state of the given
stream within the given interval, respectively. 


\begin{example} \label{ex:uv} Suppose  is a variable,  and 
  are fields, and  is a stream such that the expression  always evaluates to ,  and  within intervals ,  and
  , respectively, i.e., for example . Thus, both  and  may be deduced.
\end{example}

Using , we define  and , which hold iff 
holds at the beginning and end of the given interval, respectively. 

Operators  and  cannot accurately model
fine-grained interleaving in which processes are able to access at
most one location in a single atomic step. However, both  and
 are useful for modelling the actual behaviour of the
system as well as the behaviour of the coarse-grained abstractions
that we develop. We may use  to define \emph{stability} of a
variable , and \emph{invariance} of a state predicate  as
follows:

\noindent
Such definitions of stability and invariance are necessary because
adjoining intervals are assumed to be disjoint, i.e., do not share a
point of overlap. Therefore, one must refer to the values at the end
of some immediately preceding interval. \smallskip

\noindent{\bf Evaluation over states apparent to a process.}
Assuming the same setup as \refex{ex:uv}, if  is only able to
access at most one location at a time, evaluating  using the states \emph{apparent} to process  over the
interval  may result in , e.g., if the value at  is read within interval  and the value at  read within . 

Reasoning about the apparent states with respect to a process 
using function  is not always adequate because it is not
enough for an apparent state to exist; process  must also be able
to read the relevant variables in this apparent state. Typically, it
is not necessary for a process to be able to read all of the state
variables to determine the apparent value of a given state
predicate. In fact, in the presence of local variables (of other
processes), it will be impossible for  to read the value of each
variable. Hence, we define a
function , where  is the set of
locations whose values process  needs to determine to evaluate the
given state predicate.

\noindent
Using this function, we are able to determine whether state predicates
definitely and possibly hold with respect the apparent states of a
process. For a state predicate , interval , stream  and
state , we let  denote the smallest set of
locations (variables and addresses) that must be accessed in order to
evaluate  in state  and define . For a process , this is
used to define , which states that  holds in
all states apparent to  in  within . (Similarly
.)

\noindent 
Continuing \refex{ex:uv}, if , we have  holds, i.e.,  even though  holds
(cf. \cite{DDH12,HBDJ13}).
One may establish a number of properties on , ,
 and  \cite{HBDJ13}, for example
 holds.
The following lemma relates apparent and states evaluation.
\begin{lemma}
  \label{lem:stable}
  For any process , variable , field  and constant ,

  
\end{lemma}








\section{Behaviours and refinement}
\label{sec:behaviour-refinement-1}



The \emph{behaviour} of a command  executed by a non-empty set of
processes  in a context  is given by interval
predicate , which is defined inductively in
\reffig{fig:beh-def}. We use  to denote  and
assume the existence of a program counter variable  for each
process . We define shorthand  and
 to denote finite and infinite
idling, respectively and use the interval predicates below to
formalise the semantics of the commands in
\reffig{fig:beh-def}.

\noindent
To enable compositional reasoning, for interval predicates  and
, and command , we introduce two additional constructs
 and , which denote a command  with a
\emph{rely condition}  and an \emph{enforced condition} ,
respectively \cite{DDH12}.

\begin{figure}[t]
  \figrule \centering \small

  vae \in VarP = \emptysetP = \{p\}

  \figrule
  \caption{Formalisation of behaviour function}
  \label{fig:beh-def}
\end{figure}


We say that a concrete command  is a refinement of an abstract
command  iff every possible behaviour of  is a possible
behaviour of . Command  may use additional variables to those in
, hence, we define refinement in terms of sets of variables
corresponding to the contexts of  and . In particular, we say
 with context  is \emph{refined} by  with context  with
respect to a set of processes  (denoted ) iff
 holds. Thus, any behaviour of the
concrete command  is a possible behaviour of the abstract command
. This is akin to operation refinement \cite{deRoever98}, however,
our definition is with respect to the intervals over which the
commands execute, as opposed to their pre/post states. We write  for , write  for , write  iff both  and , and write 
for . There are numerous theorems and lemmas
for behaviour refinement \cite{DDH12,DD12}. We present a selection of
results that are used to verify correctness of the lazy set. The
following results may be proved using monotonicity of the
corresponding interval predicate operators.



The next lemma states that an assignment of state predicate  to a
variable  may be decomposed to a guard  followed by an
assignment of  to  and a guard  followed by an
assignment of  to .
\begin{lemma}
  \label{lem:bool-exp}
  For a state predicate , variable , process , and
  , we have

  .
\end{lemma}
Note that a property like \reflem{lem:bool-exp} is difficult to
formalise in interleaved frameworks such as action systems
\cite{BvW99} because interference may occur between guard evaluation
and assignment to  at the concrete level, which is not possible in
the abstract. The lemma below allows one to move the frame of a
command into the refinement relation.
\begin{lemma}
  \label{lem:add-context}
  Suppose  and  are commands, ,  and  such that  and . If , then .
\end{lemma}

The following theorem allows one to turn a rely condition at the
abstract level to an enforced condition at the concrete level,
establishing a Galois connection between rely and enforced conditions
\cite{DDH12}.
\begin{theorem}
  \label{thm:rely-enf-gc}
  
\end{theorem}
When modelling a lock-free algorithm \cite{CGLM06,DSW11,VHHS06}, one
assumes that each process repeatedly executes operations of the data
structure, and hence the processes of the system only differ in terms
of the process ids. For such programs, a proof of the parallel
composition may be decomposed using the following theorem \cite{DD12}.
\begin{theorem}
  \label{thm:decompose}
  If , , and
   and  are commands parameterised by , then  holds if
  for some interval predicate  and some  and  both of the following hold.
  
\end{theorem}

\begin{figure}[!t]
  \centering
  \figrule\small

    
    \smallskip
  
    
    \figrule
  \caption{A coarse-grained abstraction of {\tt contains}}
  \label{fig:labs}
\end{figure}


\section{Verification of the lazy set}
\label{sec:verif-lazy-set}


As already mentioned, we focus on a proof {\tt contains}, which
highlights the advantages of interval-based reasoning over frameworks
that only reason about the pre/post states.\footnote{A verification of
  the {\tt add} and {\tt remove} operations are presented in Appendix
  A.}  Verification of linearisability of \texttt{contains} is known
to be difficult using frameworks that only consider the pre/post
states \cite{Vaf10,VHHS06,CGLM06,DSW11}. A coarse-grained abstraction
of  in \reffig{fig:formal-lazyset} is given by
 in \reffig{fig:labs}, where the  and
 operations are unmodified, but 
is replaced by , which tests to see if  is in the
set using an atomic (coarse-grained) guard, then updates the return
value to  or  depending on the outcome of the test.

State predicates ,  and , which are used
our refinement proof, are defined in \reffig{fig:labs}. A location
 is \emph{reachable} from  in state  iff
 holds, hence, for example,
 holds iff it is possible to traverse the
list starting from  and reach node  in . The abstract
set of node addresses corresponding to each list data structure in
 is given by  and the set of values of these nodes is
given by . Although  is always reachable from
,  will not contain  because .

\begin{figure}[t]
  \centering
  \scalebox{0.65}{\begin{picture}(0,0)\includegraphics{refine2_pspdftex}\end{picture}\setlength{\unitlength}{3947sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(8799,1734)(2014,-4573)
\put(5551,-3136){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5551,-3361){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5551,-3586){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(9278,-3361){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(9278,-3136){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(7126,-3211){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}Lemma \ref{lem:add-context}}}}}}
\put(9278,-3586){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(9301,-4036){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(9301,-4261){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(9301,-4486){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3976,-3211){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\refthm{thm:decompose}}}}}}
\put(3976,-2986){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}Lemma \ref{lem:add-context}}}}}}
\put(2701,-3661){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2701,-3436){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2701,-3211){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5701,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture} }
  \caption{Proof decomposition for the lazy set verification}
  \label{fig:refsteps}
\end{figure}

An overview of the proof decomposition is given in
\reffig{fig:refsteps}. 
To prove that  refines , using
\refthm{thm:decompose} we show that  refines
 for a single process  under a yet to be
determined rely condition  (condition \refeq{eq:17}), provided that
the behaviour of the rest of the program implies the  that is
derived (condition \refeq{eq:18}). Then, using monotonicity of 
and \reflem{lem:add-context}, we further decompose the proof that
 refines  to the level of each
operation. The proofs for  and  are trivial
because they are unmodified in . To prove , we use \reflem{lem:bool-exp} to perform case analysis on
executions that return  and . The refinement proof is
hence localised as much as possible.  Furthermore, the structure of
 is elucidated as part of the correctness proof.








We are required to prove 
for an arbitrarily chosen set of processes . Using
\reflem{lem:add-context}, we transfer the context  of  and  into the
refinement relation. Then, using monotonicity of  followed by
\refthm{thm:decompose}, we decompose the specifications into the
following proof obligations, where  and 
and the rely condition  is yet to be developed.


\noindent \textbf{Proof of .}
Using \reflem{lem:add-context} to expand the context followed by
monotonicity of  and , assuming  and , condition 
decomposes as follows.
  





Condition \refeq{eq:3} is trivial by \reflem{lem:add-context} and
reflexivity of . To prove \refeq{eq:7}, must ensure that
if  is assigned , then there must have been an actual
state, say , in the interval preceding the assignment to
 such that . Similarly, if  is
assigned , there must have been an actual state  within
the interval of execution such that . Note
that in the proof, we use the states apparent to process  to deduce
a property of an actual state of the system. Using
\reflem{lem:bool-exp},  is equivalent to the
following, where\smallskip

\noindent
\quad
\hfill\smallskip

\noindent
and split the label  into  and  --- the true and
false cases of .

\noindent
We distribute  within the `', use monotonicity to match
the abstract and concrete  and  branches, then use
monotonicity again to remove the assignments to  from both
sides of the refinement. Thus, we are required to prove the following
properties.

Condition \refeq{eq:4} (i.e., the branch that assigns ) states that there must be an actual state  within the
interval in which  executes, such that  holds, which indicates that there is a point at which
the abstract set contains . It may be the case that a process  has removed  from the set by the time process  returns
from the contains operation. In fact,  may be added and removed
several times by concurrent add and remove operations before process
 completes execution of . However, this
does not affect linearisability of  because a
state for which  holds has been found.  An execution of
 that returns  would only be incorrect
(not linearisable) if  is returned and  holds for the interval in which 
executes. Similarly, we prove correctness of \refeq{eq:8} by showing
that is impossible for there to be an execution that returns 
if  holds in the interval of execution.
\smallskip

\noindent{\bf Proof of \refeq{eq:4}.} Using \refthm{thm:rely-enf-gc},
we transfer the rely condition  to the right hand side as an
enforced property. We define state predicate , which
states that  with value  is in the abstract set, i.e.,
.  We require
that  implies the following.

The behaviour of the right hand side of \refeq{eq:4} then simplifies as follows. 
\begin{derivation*}
  \step{ beh_{p,M}.(\enf r \st CL
    \ch cl_3: [IN])}

  \trans{\equiv}{definition of }

  \step{r \land  (beh_{p,M}.CL \ch  beh_{p,M}.(cl_3: [IN]))}

  \trans{\entails}{definition of  and  is local to }

  \step{r \land  (beh_{p,M}.CL \ch (stable.n1_p \land beh_{p,M}.(cl_3: [IN])))}

  \trans{\entails}{ and \reflem{lem:stable}}



  \step{r \land \left(
      \begin{array}[c]{@{}l@{}}
        beh_{p,M}.(cl_1: n1_p \asgn Head) \!\ch\! beh_{p,M}.(cl_2: {\sf
          CLoop}(p,x)) \!\ch \! \Sometime \neg 
        (n1_p\mapsto mrk) 
        \land \Sometime ((n1_p\mapsto val) = x)
      \end{array}
    \right)}

  \trans{\entails}{first chop: change context , second
    chop: assumption \refeq{eq:5}} 

  \step{r \land \left(
    \begin{array}[c]{@{}l@{}}
      beh_{p,L}.\Idle \ch \Always (\reachable.Head.n1_p \lor (n1_p \mapsto
      mrk)) \ch {}  (\Sometime \neg (n1_p\mapsto mrk) \land
      \Sometime ((n1_p\mapsto val) = x))
    \end{array}
  \right)}
\end{derivation*}
Focusing on just the second and third parts of the chop, because
 is not modified after , and  is assumed to
split, we obtain the following calculation.
\begin{derivation*}

  \step{\exists a : Addr \st r \land 
    \left(
      \begin{array}[c]{@{}l@{}}
        \left(\Always (\reachable.Head.n1_p \lor (n1_p \mapsto mrk)) \land \ora{n1_p = a}\right) \ch \qquad \\
        \hfill (\Always (n1_p = 
        a) \land \Sometime \neg (a \mapsto mrk) \land \Sometime (a \mapsto
        val) = x))
      \end{array}\right)
  }

  \trans{\entails}{, then by assumption
    \refeq{eq:10}, disjunct  in LHS of chop }




  \trans{}{implies  in RHS, which contradicts }

  \step{\exists a : Addr \st r \land \left(
      \begin{array}[c]{@{}l@{}}
        \left(\ora{\reachable.Head.a \land
          \neg (a\mapsto mrk) }\right)
    \ch 
    (\Always (n1_p = 
      a) \land \Sometime \neg (a \mapsto mrk) \land \Sometime ((a \mapsto
      val) = x))
    \end{array}\right)
  }


  \trans{\entails}{case analysis and assumption \refeq{eq:10},
    disjunct  in LHS of chop }

  \trans{}{implies  in RHS, 
    contradicting }

  \step{\exists a : Addr \st r \land \left(
    \begin{array}[b]{@{}l@{}}
\ora{inSet(Head,a,x)}\ch {} 
       (\Always (n1_p = 
      a) \land \Sometime \neg (a \mapsto mrk) \land \Sometime ((a
      \mapsto 
      val) = x))
    \end{array}\right)
  }

  \trans{\entails}{definition of }

  \step{\Sometime (x \in absSet)}

\end{derivation*}
Having shown that the behaviour of the implementation implies the
behaviour of the abstraction, it is now straightforward to show that
the refinement for case \refeq{eq:4} holds. 


\smallskip

\noindent{\bf Proof of \refeq{eq:8}.} As with \refeq{eq:4}, we use
\refthm{thm:rely-enf-gc} to transfer the rely condition  to the
right hand side as an enforced property. By logic, the right hand side
of the \refeq{eq:8} is equivalent to command .  The  case is trivially true. For
case , we require that  satisfies:

By \refeq{eq:11}, in any interval, if the value  is in the set
throughout the interval, there is an address that can be reached from
, the marked bit corresponding to the node at this address is
unmarked and the value field contains . By \refeq{eq:13} the
reachable nodes of the list (including marked nodes) must be sorted in
strictly ascending order and by \refeq{eq:24} the  node must be
reachable from . Conditions \refeq{eq:11}, \refeq{eq:13} and
\refeq{eq:24} together imply that there cannot be a terminating
execution of  such that  holds,
i.e., the behaviour is equivalent to .


\smallskip 
\noindent{\bf Proof of .}
The final rely condition  must imply each of \refeq{eq:5}, \refeq{eq:10}, \refeq{eq:11}, \refeq{eq:13} and \refeq{eq:24}. We choose to take the
weakest possible instantiation and let  be the conjunction
. These
properties are straightforward to verify by expanding the definitions
of the behaviours. The details of this proof are elided.





\section{Conclusions}

We have developed a framework, based on \cite{DDH12}, for reasoning
about the behaviour of a command over an interval that enables
reasoning about pointer-based programs where processes may refer to
states that are apparent to a process \cite{HBDJ13}. Parallel
composition is defined using conjunction and conflicting access to
shared state is disallowed using fractional permissions, which models
truly concurrent behaviour.  We formalise behaviour refinement in our
framework, which can be used to show that a fine-grained
implementation is a refinement of a coarse-grained abstraction. One is
only required to identify linearising statements of the abstraction
(as opposed to the implementation) and the proof of linearisability
itself is simplified due to the coarse-granularity of commands. For
the coarse-grained contains operation in \ref{fig:labs}, the guard
 is the linearising statement for an execution
that returns  and  the linearising statement of
an execution that returns .

Our proof method is compositional (in the sense of rely/guarantee) and
in addition, we develop the rely conditions necessary to prove
correctness incrementally. As an example, we have shown refinement
between the \texttt{contains} operation of Heller et al's lazy set and
an abstraction of the contains operation that executes with
coarse-grained
atomicity. 

Behaviour refinement is defined in terms of implication, which makes
this work highly suited to mechanisation. However, we consider full
mechanisation to be future work. 


\noindent\textbf{Acknowledgements.}
  This research is supported by EPSRC Grant EP/J003727/1. We thank
  Gerhard Schellhorn and Bogdan Tofan for useful discussions, and
  anonymous reviewers for their insightful comments.

\bibliographystyle{plain}
\bibliography{thesis}

\newpage
\appendix

\section{Proofs of Add/Remove}
\newcommand{\spec}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\NE}[1]{\underline{ #1}}

In this appendix, we complete the proofs of abstraction for the
\texttt{add} and \texttt{remove} operations. Compared to the proofs of
the \texttt{contains} operation, these proofs are simpler due to the
locking that occurs during the main portion of each
operation. However, because we assume a truly concurrent semantics,
the coarse-grained abstraction is more difficult to specify. In
particular, it is possible for a number of concurrent add/remove
operations to take effect as part of a single state transition. 




\subsection{Formal model of \texttt{locate}, \texttt{add} and
  \texttt{remove}} 

In this section, we formalise the  and 
operations in our framework, which requires that we also formalise
.

We define a predicate , which formalises the
guard at \texttt{L8}. Operation 
 
formalises lines \texttt{L1}-\texttt{L7} and  formalises an execution of \texttt{L8} in which guard
 evaluates to . The two unlock statements
within  correspond to \texttt{L10}
and \texttt{L11}. The  operation
models an execution of the main loop body within \texttt{locate} that
that loops again. Operation  models a
successful execution of the loop body (where 
evaluates to .

As the names imply,  and  model the
successful and failed executions of the add operation, and
 operation behaves as locate, then non-deterministically
chooses between an successful or failed operation, then unlocks the
locks on  and  held after the termination of
. Operation  is similar, and is formalised
below. 


\subsection{The \texttt{add} operation}

In this section, we verify the coarse-grained abstraction of the
\texttt{add} operation.  Unlike the \texttt{contains} operation, this
abstraction cannot be defined using the standard language constructs,
because the standard constructs are not precise enough to describe the
abstract behaviour.  Hence, we introduce a specification command,
which turns an interval predicate to into a command, whose behaviour
is given by the interval predicate. Thus, for an interval predicate
, process  and set of variables , the behaviour of a
specification command is given by:


We also introduce two further interval predicates, namely ,
which states that interval predicate  holds and the interval under
consideration is non-empty, and  which states that 
holds in some subinterval of the given interval, i.e., for an interval
 and stream , we define: 

We define a state predicate  which holds if
process  writes to any of the fields in  of the data structure
at address .

We further define interval predicate  that is used to
determine whether  ever writes to the addresses corresponding to
the ,  and  fields of the nodes reachable from ,
, which holds if no other process different from 
writes to fields of the node , and  that restricts the
values that are modified by  with respect to node .

Thus,  holds iff there is a point in the interval such that
 writes to the ,  or  fields of the node at address
 and  holds iff there is no interference by the
environment of  to any of the fields of node .

The insertion of a node into the set is modelled as follows, where
 denotes the precondition of an insertion,  models the
insertion, and  models the full operation, including the
possible interference from other processors. 

State predicate  states that  is reachable from
, the  predicate holds, node  has value is less
than  and node  has value greater than . Thus,  is not in
the abstract set. State predicate  states that  is updated with value , and node  has value ,
points to  is not marked and is not locked. The 
predicate states that there are addresses ,  and  such that
 holds as a precondition, behaves as 
and furthermore,  and  are interference free and  does not
write to any other set address.

The coarse-grained abstraction of the \texttt{add} operation is then
defined as follows.

A successful execution of the \texttt{add} operation behaves as
 then sets the return value  to . A failed
execution of the \texttt{add} operation never adds  to the set,
but detects that  is in the set and sets  to . The
 operation performs some idling at the start modelled as
 because the concrete operation has the possibility of
not terminating, and at the end (to allow the concrete program time to
unlock the held locks).

Like the decomposition depicted in \reffig{fig:refsteps} for the
\texttt{contains} operation, we may decompose the proof so that we
consider the execution of \texttt{add} by a single process under a
rely condition  that we assume splits, provided that the rest of
the program satisfies the rely condition that we derive. Given that
 is the program derived from  by
replacing  by , the refinement holds if
we prove both of the following:


\subsubsection{Proof of (\ref{eq:22}).}

We define the following state predicate, which formalises the
postcondition of .

Thus, operation {\sf Locate} ensures that  and  satisfy
, that the value of  is less than , the value of
 is above or equal to , that both  and  are
locked, and that both  and  are reachable from .  We
now have the following refinement, where .
\begin{derivation}
  \step{\Enf{r}{\mathsf{Add}(p,x)}}

  \trans{\srefeq_p^M}{definition of }

  \step{\enf r \st \mathsf{Locate}(p,x,n1_p, n3_p) \ch ({\sf AddOK}(p,x) \sqcap {\sf
      AddFail}(p,x)) \ch {\sf U}(p,n1_p, n3_p)}

  \trans{\srefeq_p^M}{logic}

  \step{\enf r \st
    \begin{array}[t]{@{}l@{}}
      \left(\Enf{\Inf}{\mathsf{Locate}(p,x,n1_p, n3_p)}) \sqcap
        (\Enf{\Fin}{\mathsf{Locate}(p,x,n1_p, n3_p)})\right) \ch {} 
      \\
      ({\sf AddOK}(p,x) \sqcap {\sf
        AddFail}(p,x)) \ch {\sf U}(p,n1_p, n3_p)
    \end{array}
  }
  

  \trans{\sqsupseteq_p^M}{behaviour of }
  \trans{}{distribute `' over `;',  is a right
    annihilator}


  \step{\enf r \st \spec{\Inf \land \neg ModSet.p} \sqcap {}}
  \step{\enf r \st
    \begin{array}[t]{@{}l@{}}
      (\Enf{\Fin}{\mathsf{Locate}(p,x,n1_p, n3_p)}) \ch
      {} \\
      ({\sf AddOK}(p,x) \sqcap  {\sf AddFail}(p,x)) \ch {\sf U}(p,n1_p,n3_p)
    \end{array}
  }
  
  \trans{\srefeq_p^M}{distribute `'}
  \step{\enf r \st \spec{\Inf \land \neg ModSet.p} \sqcap {}
    \hfill (A_1)}

  \step{\enf r \st (\Enf{\Fin}{\mathsf{Locate}(p,x,n1_p, n3_p)}) \ch {\sf AddOK}(p,x) \ch {\sf
      U}(p,n1_p,n3_p) \sqcap {} \hfill (A_2)}
   
  \step{\enf r \st (\Enf{\Fin}{\mathsf{Locate}(p,x,n1_p, n3_p)}) \ch {\sf AddFail}(p,x) \ch {\sf
      U}(p,n1_p,n3_p) \hfill (A_3)}
  
\end{derivation}

Splitting the behaviour of  into finite and
infinite executions, and distributing the `;' through `', it
is possible to show that 
where ,  , and   are defined below. 
\begin{derivation*}
  \step{\spec{\Inf \land \neg ModSet.p} \hfill (CA_1)}

  \step{\spec{\Fin \land \neg ModSet.p}  \ch \mathsf{CGAOK}(p, x) \ch
    \spec{\neg ModSet.p} \hfill (CA_2)}
  
  \step{\spec{\Fin \land \neg ModSet.p}  \ch  \mathsf{CGAFail}(p, x) \ch
    \spec{\neg ModSet.p} \hfill (CA_3)}
\end{derivation*}


\paragraph{Proof of .}
It is trivial to verify . 

\paragraph{Proof of .} To prove this case, we strengthen
condition  and require that it satisfies both of the following.

Assuming that  holds, we now have the following calculation.
\begin{derivation*}
  \step{beh_{p,M}.
    \left(\Enf{\Fin}{\mathsf{Locate}(p,x,n1_p, n3_p)} \ch {\sf AddOK}(p,x) \ch {\sf
        U}(p,n1_p,n3_p)\right)}





  \trans{\entails}{expand behaviour and use \refeq{eq:21} }

  \step{beh_{p,M}.(\Enf{\Fin}{\mathsf{Locate}(p,x,n1_p, n3_p)}) \ch{}} 
  
  \step{(IntFree.n1_p \land IntFree.n3_p \land beh_{p,M}.{\sf
      AddOK}(p,x)) \ch {} }

  \step{beh_{p,M}.{\sf U}(p,n1_p,n3_p)}

  \trans{\entails}{first chop: behaviour of }
  \trans{}{second chop: expand , use
    , behaviour of  and  conditions}

  \step{
    (\neg ModSet.p \land \ora{postLocate(p,n1_p,n3_p)})
    \ch{}}
  \step{
    \left(\begin{array}[c]{@{}l@{}}
        \exists 
        a: Addr, k:Val \st 
          \begin{array}[t]{@{}l@{}}
            \left(
              \begin{array}[c]{@{}l@{}}
                \Always (n1_p \mapsto
                val < x) \land \Always(n3_p \mapsto val > x) \land\\
                \left(
                  \begin{array}[c]{@{}l@{}}
                    beh_{p,M}.(alt_2 \ch al_3 \ch al_4)
                    \ch \\
                    \Always(pc_p = al_5) \land \Eval_{p,M}.(n1_p \cdota nxt = a
                      \land n2_p = k)
                  \end{array}
                \right)
              \end{array}
            \right) \ch \!\!
            \\
            (\Always(pc_p =
            al_5) \land \Update_{p,M}.(a,k))
          \end{array}
        \end{array}\right)
    \ch {}}

  \step{ beh_{p,M}.al_6 \ch beh_{p,M}.{\sf U}(p,n1_p,n3_p)}


  \trans{\entails}{first chop: logic, second chop: expand behaviour
    use (\ref{eq:20})} 

  \step{ \neg ModSet.p
    \ch{}}
  \step{
    \left(\begin{array}[c]{@{}l@{}}
        \exists 
        a: Addr, k:Val \st 
          \begin{array}[t]{@{}l@{}}
            \left(
              \begin{array}[c]{@{}l@{}}
                    \neg ModSet.p \land \ora{preIns(n1_p, n3_p, x)}
                    \land (\ora{n2_p \mapsto (x, n3_p, false,null)}) \land \\
                    \ora{(n1_p \cdota nxt = a
                      \land n2_p = k)}
              \end{array}
            \right)
            \ch \!\!
            \\
            (\Always(pc_p = al_5) \land \Update_{p,M}.(a,k))
          \end{array}
        \end{array}\right)
    \ch {}}

  \step{ beh_{p,M}.al_6 \ch beh_{p,M}.{\sf U}(p,n1_p,n3_p)}

  \trans{\equiv}{logic,  both splits and joins}

  \step{
    \neg ModSet.p
    \ch{}}
  \step{
    \left(\begin{array}[c]{@{}l@{}}
        \exists 
        a: Addr, k:Val \st 
          \begin{array}[t]{@{}l@{}}
            \prev\left(
              \begin{array}[c]{@{}l@{}}
                    \ora{preIns(n1_p, n3_p, x)}
                    \land (\ora{n2_p \mapsto (x, n3_p, false,null)}) \land \\
                    \ora{(n1_p \cdota nxt = a
                      \land n2_p = k)}
              \end{array}
            \right)
            \land
            \\
            (\Always(pc_p = al_5) \land \Update_{p,M}.(a,k))
          \end{array}
        \end{array}\right)
    \ch {}}

  \step{ beh_{p,M}.al_6 \ch beh_{p,M}.{\sf U}(p,n1_p,n3_p)}

  \trans{\entails}{ holds locks on  and , use
    (\ref{eq:20}), definition of }

  \step{
    \neg ModSet.p
    \ch{}}
  \step{
    \left(\begin{array}[c]{@{}l@{}}
            \prev\ \ora{preIns(n1_p, n3_p, x)}
            \land 
            \NE{\Always\ doIns(n1_p, n2_p, n3_p, x)} \land \\
            IntFree.n1_p \land IntFree.n3_p \land 
            \all ua : Addr \bs \{a\cdota nxt\} \st \Always \neg \mcW.ua.p  
        \end{array}\right)
    \ch {}}

  \step{ beh_{p,M}.al_6 \ch beh_{p,M}.{\sf U}(p,n1_p,n3_p)}








  





















  \trans{\entails}{change context}

  \step{beh_{p,L}.(CA_2)}

\end{derivation*}

\paragraph{Proof of .} Once again assuming  holds, we obtain: 
\begin{derivation*}
  \step{beh_{p,M}.\left(\Enf{\Fin}{\mathsf{Locate}(p,x,n1_p, n3_p)} \ch {\sf
        AddFail}(p,x) \ch {\sf U}(p,n1_p,n3_p)\right)}

  \trans{\entails}{definition of  and behaviour of
    }

  \step{(\neg ModSet.p \land \ora{postLocate(p,n1_p, n3_p)}) \ch beh_{p,M}.{\sf AddFail}(p,x) \ch beh_{p,M}.{\sf U}(p,n1_p,n3_p)}

  \trans{\entails}{definition of  }

  \step{\left(
    \begin{array}[c]{@{}l@{}}
      beh_{p,L}.\Idle \land\\ 
      \ora{\reachable.Head.n3_p \land \neg (n3_p \mapsto
        mrk)}
    \end{array}
  \right) \ch  beh_{p,M}.{\sf AddFail}(p,x) 
    \ch beh_{p,M}.{\sf U}(p,n1_p,n3_p)}


  \trans{\entails}{use \refeq{eq:21} and guard , change context}

  \step{beh_{p,L}.\Idle   \ch  (beh_{p,L}.\Idle \land \NE{\Always (\reachable.Head.n3_p
      \land \neg (n3_p \mapsto mrk) \land (n3_p \mapsto val = x))})
    \ch {} }
  \step{beh_{p,M}.(al_7 \ch {\sf U}(p,n1_p,n3_p))}

  \trans{\entails}{definition of }

  \step{beh_{p,L}.\Idle
    \ch (beh_{p,L}.\Idle \land \NE{\Always (x \in absSet)})
    \ch  beh_{p,M}.(al_7 \ch {\sf U}(p,n1_p,n3_p))}

  \trans{\entails}{change context}

  \step{beh_{p,M}.CA_3}


\end{derivation*}

\subsubsection{Proof of (\ref{eq:23}).}

We strengthen the rely condition with additional conjunct
. This proof is straightforward due
to the locks held by process .

\subsection{The \texttt{remove} operation}

A coarse-grained abstraction of the \texttt{remove} operation is given
below. 

The proof of refinement between \texttt{remove} and the abstraction
above proceeds in a similar manner to the \texttt{add} operation.
In particular,  holds, where:
\begin{derivation*}
  \step{\spec{\Inf \land \neg ModSet.p}\hfill (CR_1)}
  
  \step{\spec{\Fin \land \neg ModSet.p} \ch \mathsf{CGROK}(p, x) \ch \spec{\neg ModSet.p}\hfill (CR_2)}
  
  \step{\spec{\Fin \land \neg ModSet.p} \ch \mathsf{CGRFail}(p, x) \ch
    \spec{\neg ModSet.p}\hfill (CR_3)}
\end{derivation*}
Furthermore,  holds, where:
\begin{derivation*}
  \step{(\Enf{\Inf}{\mathsf{Locate}(p,x, n1_p, n2_p)}) \hfill (R_1) }

  \step{(\Enf{\Fin}{\mathsf{Locate}(p,x, n1_p, n2_p)}) \ch
    \mathsf{RemOK}(p,x) \ch \mathsf{U}(p, n1_p, n2_p)\hfill (R_2)}

  \step{(\Enf{\Fin}{\mathsf{Locate}(p,x, n1_p, n2_p)}) \ch
    \mathsf{RemFail}(p,x) \ch \mathsf{U}(p, n1_p, n2_p) \hfill (R_3)}
\end{derivation*}
Thus, to complete the proof, we must show:  for , and the proof of  is similar to
the failed case of the {\tt add}. The proof of  is trivial. For the
proof of case  we assume the following.

Hence, assuming , the proof proceeds as follows.
\begin{derivation*}
  \step{beh_{p,M}.(R_2)}
  
  \trans{\equiv}{expanding definitions}

  \step{beh_{p,M}.(\Enf{\Fin}{\mathsf{Locate}(p,x, n1_p, n2_p)}) \ch  {}}

  \step{\left(
      \begin{array}[c]{@{}l@{}}
        \exists a: Addr, k : Val \st
        \begin{array}[t]{@{}l@{}}
          beh_{p,M}.rlt_2 \ch (\Always (pc_p = rl_3) \land \Eval_{p,M}.(k \land
          (a = (n2_p \cdota mrk)))) \ch {} \\
          (\Always (pc_p = rl_3) \land \Update_{p,M}.(a, k)) \ch beh_{p,M}.(rl_4 \ch rl_5
          \ch rl_6)
        \end{array}
      \end{array}
    \right)  \ch {}}

  \step{beh_{p,M}.{\sf U}(p, n1_p, n2_p)}

  \trans{\entails}{behaviour of {\sf Locate}, then assuming (\ref{eq:27})}

  \step{(\neg ModSet.p \land \ora{postLocate(p, n1_p, n2_p)}) \ch  {}}

  \step{\left(
      \begin{array}[c]{@{}l@{}}
        \exists a: Addr, k : Val \st
        \begin{array}[t]{@{}l@{}}
          beh_{p,M}.rlt_2 \ch (\Always (pc_p = rl_3) \land \Eval_{p,M}.(k \land
          a = (n2_p \cdota mrk))) \ch {} \\
          (\Always (pc_p = rl_3) \land \Update_{p,M}.(a,  k)) \ch beh_{p,M}.(rl_4 \ch rl_5
          \ch rl_6)
        \end{array}
      \end{array}
    \right)  \ch {}}

  \step{beh_{p,M}.{\sf U}(p, n1_p, n2_p)}

  \trans{\entails}{logic, expand behaviours, use (\ref{eq:27})}

  \step{\neg ModSet.p \ch  {}}

  \step{\left(
      \begin{array}[c]{@{}l@{}}
        \exists b: Addr \st
        \begin{array}[t]{@{}l@{}}
          \neg ModSet.p \land  \ora{postLocate(p, n1_p, n2_p)} \land
          \ora{(n2_p \longmapsto (x, b, false, p))} \ch {} \\
          (\Always (pc_p = rl_3) \land \Update_{p,M}.(n2_p \cdota mrk, true)) \ch beh_{p,M}.(rl_4 \ch rl_5
          \ch rl_6)
        \end{array}
      \end{array}
    \right)  \ch {}}

  \step{beh_{p,M}.{\sf U}(p, n1_p, n2_p)}

  \trans{\equiv}{ splits and joins, logic}

  \step{\neg ModSet.p \ch  {}}

  \step{\left(
      \begin{array}[c]{@{}l@{}}
        \exists b: Addr \st
        \begin{array}[t]{@{}l@{}}
          \prev(\ora{postLocate(p, n1_p, n2_p)} \land
          \ora{(n2_p \longmapsto (x, b, false, p))}) \land \\
          (\Always (pc_p = rl_3) \land \Update_{p,M}.(n2_p \cdota mrk, true)) \ch beh_{p,M}.(rl_4 \ch rl_5
          \ch rl_6)
        \end{array}
      \end{array}
    \right)  \ch {}}

  \step{beh_{p,M}.{\sf U}(p, n1_p, n2_p)}

  \trans{\entails}{behaviour definitions, (\ref{eq:27})}

  \step{beh_{p,M}.(CR_2)}
\end{derivation*}

\noindent
Finally, we are left with a proof requirement that the rest of the
program implies the rely condition \refeq{eq:27}. This proof is
straightforward due to the locks held by process . 

\end{document}
