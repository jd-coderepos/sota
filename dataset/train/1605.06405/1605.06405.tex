\documentclass{sig-alternate-05-2015}
\pdfoutput=1 

\pagenumbering{arabic}

\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{subfigure}

\usepackage{url} 

\newcommand{\indicator}[1]{\mathds{1}_{\left[ {#1} \right] }}
\newcommand{\Fig}[1]{Fig.~\ref{fig:#1}}
\newcommand{\Sec}[1]{Sec.~\ref{sec:#1}}
\newcommand{\Tab}[1]{Tab.~\ref{tab:#1}}
\newcommand{\Eq}[1]{(\ref{eq:#1})}
\newcommand{\Alg}[1]{Alg.~\ref{alg:#1}}

\let\tempone\itemize
\let\temptwo\enditemize
\renewenvironment{itemize}{\tempone\addtolength{\itemsep}{-0.7\baselineskip}}{\temptwo}
\let\tempthree\enumerate
\let\tempfour\endenumerate
\renewenvironment{enumerate}{\tempthree\addtolength{\itemsep}{-0.7\baselineskip}}{\tempfour}


\usepackage{etoolbox}
\makeatletter
\patchcmd{\maketitle}{\@copyrightspace}{}{}{}
\makeatother

\begin{document}













\title{The Price of Fog: a Data-Driven Study on\\
Caching Architectures in Vehicular Networks
}

\numberofauthors{2} 

\author{
\alignauthor
Francesco Malandrino, Carla Chiasserini\\
       \affaddr{Politecnico di Torino, Italy}
\alignauthor Scott Kirkpatrick\\
       \affaddr{The Hebrew University of Jerusalem, Israel}
}

\maketitle
\begin{abstract}

Vehicular users are expected to consume large amounts of data, for both entertainment and navigation purposes. This will put a strain on cellular networks, which will be able to cope with such a load only if proper caching is in place; this in turn begs the question of which caching architecture is the best-suited to deal with vehicular content consumption. In this paper, we leverage a large-scale, crowd-sourced trace to (i) characterize the vehicular traffic demand, in terms of overall magnitude and content breakup; (ii) assess how different caching approaches perform against such a real-world load; (iii) study the effect of recommendation systems and local content items. We define a {\em price-of-fog} metric, expressing the additional caching capacity to deploy when moving from traditional, centralized caching architectures to a ``fog computing'' approach, where caches are closer to the network edge. We find that for location-specific items, such as the ones that vehicular users are most likely to request, such a price almost disappears.
Vehicular networks thus make a strong case for the adoption of mobile-edge caching, as we are able to reap the benefit thereof -- including a reduction in the distance travelled by data, within the core network -- with little or none of the associated disadvantages.
\end{abstract}

\section{Introduction}

Back in 2010, the traffic demand of newly-introduced iPhones briefly disrupted some cellular networks~\cite{arnaud}. It is uncertain whether such disruptions are likely to happen again; however, there is no doubt that {\em if} they do happen, vehicular users will be among the main culprits.

The reason for this trend is multifold. First, vehicles carry people, and people carry multiple, data-hungry mobile devices. Second, vehicles themselves are increasingly often equipped with entertainment devices, which only add to the problem. Third, vehicles download navigation data, e.g., map updates: while this is a minor component of the overall traffic today, it is expected to increase by orders of magnitude with the introduction of self-driving vehicles, which will need much more detailed and up-to-date maps.

To make things worse, virtually {\em all} such data demand will be served by cellular networks. Indeed, most offloading solutions target pedestrian users, because their position changes relatively slowly over time and because they are more likely to be covered by such networks as Wi-Fi.

{\em Caching} is a primary way in which cellular network operators
plan to react to this demand surge. One of the most popular solutions
is to move caches as close as possible to the users, in the context of
an approach known as {\em fog computing} (a term created by
Cisco~\cite{fog}). It is expected that doing so will increase the
cache hit ratio while reducing the service latency as well as the
traffic load on the cellular core network. On the negative side, it will require deploying multiple, smaller caches. Additional help is expected from recommendation systems, whose effect is to shape the demand concentrating it around the most popular content items. Intuitively, having fewer, popular items to serve will improve caching performance.

In this context, our paper targets three main questions.

\noindent{\bf Vehicular demand.} What is the data demand generated by today's vehicular users? Which apps and services represent the most significant contributions thereto?

\noindent{\bf Caching architectures.} Given a target hit ratio, what
is the relationship between  caching architecture and  size of the
caches we need to deploy? What is the impact of moving  caches from
core-level switches to individual base stations, on the total cache size,
the distance data must travel within the core network, and the load thereof?
What changes if a recommendation system is in place?

\noindent{\bf Location-specific content.} Content items consumed by
future vehicular networks are expected to strongly depend on the
location -- augmented maps for self-driving vehicles being the most
obvious example. What will be the impact of this kind of content on caching?

We answer these questions using a set of real-world, large-scale measurement data, coming from users of the WeFi app~\cite{wefi}. Due to its crowd-sourced nature, our dataset includes data for:
(i) multiple apps, including video (e.g., YouTube) and maps;
(ii) multiple types of users, from pedestrian to vehicular ones;
(iii) multiple network technologies, including 3G, LTE, and Wi-Fi;
(iv) multiple operators.

We describe our dataset, as well as the additional processing we need
to perform in order to enhance the information it provides, in \Sec{dataset}. Then, in \Sec{caching} we explain how we model caching and caching architectures in our vehicular scenario. \Sec{results} presents numerical results and some insights we obtain from them. Finally, \Sec{conclusion} concludes the paper and sketches future work directions.

\section{Input data}
\label{sec:dataset}

We describe the WeFi dataset we have access to in \Sec{wefi}. Then in \Sec{process} we detail the processing steps we need, in order to extract further information that is not directly included therein. Finally, \Sec{complement} explains how we complement the available information using other datasets and well-known information.

\subsection{The WeFi dataset}
\label{sec:wefi}

Our data comes from the users of an app called WeFi~\cite{wefi}.  WeFi
provides its users with information on the safest and fastest Wi-Fi
access points available at the user's location. At the same time, it collects information about the user's
location, connectivity and activity. 
WeFi  reports over seven million downloads of the app globally, and over three billion daily records. In this work, we use the dataset relative to the city of Los Angeles --  a vehicle-dominated environment. Its main features are summarized in \Tab{dataset}.

\begin{table}[]
\caption{
The Los Angeles dataset
\label{tab:dataset}
} \centering
\begin{tabularx}{.8\columnwidth}{|X|X|}
\hline
Metric & Value \\
\hline\hline
Time of collection & Oct. 2015\\
\hline
Total traffic & 35 TByte\\
\hline
Number of records & 81 million\\
\hline
Unique users & 64,386\\
\hline
Unique cell IDs & 47,928\\
\hline
Mobile operators & AT\&T (16,992)\\
(number of cells) & Sprint (2,764)\\
& T-Mobile (24,290)\\
& Verizon (3,882)\\
\hline
\end{tabularx}
\vspace{-4mm}
\end{table}

\begin{figure*}[h!]
\centering
\subfigure[\label{fig:distance-cdf}]{
\includegraphics[width=.3\textwidth]{img/distance_cdf.eps}
} \subfigure[\label{fig:frac-veh}]{
\includegraphics[width=.3\textwidth]{img/frac_veh.eps}
} \subfigure[\label{fig:cat-pie}]{
\includegraphics[width=.3\textwidth]{img/cat_pie.eps}
} \caption{Distribution of the distance covered by users in the dataset (a); fraction of vehicular users as a function of time (b); share of data volume for the most popular app categories used by vehicular users (c).
\vspace*{-3mm}
} \end{figure*}


Each record contains the following information:
\begin{itemize}
\item day, hour (a coarse-grained timestamp);
\item anonymized user identifier and GPS position;
\item network operator, cell ID, cell technology and local area (LAC) the user is connected to (if any);
\item Wi-Fi network (SSID) and access point (BSSID) the user is connected to (if any);
\item active app and amount of downloaded/uploaded data.
\end{itemize}
If the location of the user or the networks she is connected to change within a one-hour period, multiple records are generated. Similarly, one record is generated for each app that is active during the same period.
The fact that location changes trigger the creation of multiple records allows us to assess whether, and how much, each user moves during each one-hour period. As we will see in \Sec{process}, this is instrumental in distinguishing between static and vehicular users. Combining this knowledge with network technology information allows us to ascertain which types of traffic cellular networks ought to worry about.

\begin{figure*}[]
\centering
\includegraphics[width=.23\textwidth]{img/deployment_att.png}
\includegraphics[width=.23\textwidth]{img/deployment_sprint.png}
\includegraphics[width=.23\textwidth]{img/deployment_tmobile.png}
\includegraphics[width=.23\textwidth]{img/deployment_verizon.png}
\caption{
Left to right: deployment for AT\&T, Sprint, T-Mobile, Verizon. Each dot represents a cell, and the size of dots is proportional to the coverage area thereof, as estimated from the location of users reporting the same cell ID.
\label{fig:deployment}
\vspace*{-3mm}
} \end{figure*}

\Fig{deployment} shows the cell deployment of the four main operators present in our trace. We can see that all operators cover the whole geographical area we consider, but using radically different strategies. T-Mobile and, to a lesser extent, AT\&T, deploy a large number of cells, each covering a comparatively small area. Sprint and, especially, Verizon, follow the opposite approach: their networks are composed of relatively few cells, each covering a fairly large area.

This fundamental difference reflects on the topologies of each operator's core network, and potentially on the effectiveness of different caching architectures. It is worth to stress that using a real-world, crowd-sourced trace such as ours, we are able to properly account for these factors, which are typically neglected by more abstract models.

\subsection{Further data processing steps}
\label{sec:process}

From the WeFi dataset we easily identify several types of users and the content they consume.

\noindent{\bf User type.}
The WeFi app can be installed on a variety of mobile devices. The users carrying them can be static (e.g., sitting in a caf\'{e}), pedestrian (e.g., walking or jogging), or vehicular. We discriminate among these cases by looking at the {\em distance} covered by each user during each one-hour period. \Fig{distance-cdf} shows the distribution thereof: we have almost 40\% of static users, which do not move at all, a large number of pedestrian users covering moderate distance, and some users covering larger ones.

We conservatively we label as vehicular those users that travel more than 5~km in any one-hour period\footnote{Notice that the same user can be vehicular in some time periods and static in others.}. \Fig{frac-veh} shows the fraction of vehicular users as a function of time, and exhibits the familiar morning and afternoon peaks.

\begin{table}[]
\caption{
Content categories
\label{tab:categories}
} \centering\footnotesize
\begin{tabularx}{1\columnwidth}{|c|X|}
\hline
Category & Description \\
\hline\hline
YouTube & All class names pertaining to YouTube\\
\hline
OnDemand & On-demand video services such as Netflix, Time Warner, and ShowTime\\
\hline
RealTime & Real-time streaming, e.g., Periscope and DirectTV\\
\hline
Players & Player apps such as VLC and HTC Video\\
\hline
Weather & Most notably Weather.com\\
\hline
Maps & Most notably Google Maps\\
\hline
News & Including CNN and NBC\\
\hline
Sports & NFL, Fox Sports and the like\\
\hline
\end{tabularx}
\vspace{-4mm}
\end{table}

\noindent{\bf Content type.}
As recalled in \Sec{dataset}, records contain an \path{app} field,
containing the class name of the active application, e.g.,
\path{COM.GOOGLE.ANDROID.APPS.YOUTUBE.KIDS}. However, we cannot use
this information directly for two main reasons. First and foremost,
different class names may correspond to the same app, e.g., both
\path{COM.GOOGLE.ANDROID.APPS.YOUTUBE.KIDS} and
\path{COM.GOOGLE.ANDROID.YOUTUBE} correspond to YouTube. Furthermore,
we are not only interested in individual apps, but also in the {\em category} they belong to, as summarized in \Tab{categories}.

It is important to point out that different content categories lend themselves to caching to radically different extents. Caches are virtually useless for real-time streaming content (while LTE broadcasting~\cite{noi-broadcasting} represents a more promising alternative). On-demand video content can be successfully cached, especially if popular. Sport and news content is even easier to cache, as there is a limited number of items that are likely to be requested (e.g., the highlights of yesterday's games). Finally, weather and map content is highly local, as users are very likely to need information about their current location.

The relative importance of the aforementioned categories is summarized in \Fig{cat-pie}. YouTube and other on-demand content dominate the vehicular traffic, while real-time streaming represents much of the rest. This is good news from the caching viewpoint, as much of the vehicular traffic is represented by content that can be successfully cached.

Finally, it is important to stress that over 70\% of vehicular traffic in our dataset is served by cellular networks, compared to 11\% of the global demand. This further confirms the importance of making cellular networks able to withstand an increase in the vehicular traffic, for which fewer offloading options are available.

\subsection{Network topology and content demand}
\label{sec:complement}

There are two types of information that are altogether missing in our WeFi dataset: network topology (both access and core), and content demand. In the following, we explain how we reconstruct this information using other existing datasets and/or common knowledge.

\noindent{\bf Network topology.}
In order to study the effectiveness of different caching architectures, we need information about how base stations are connected to each other. Sadly, such information is not only absent from the WeFi dataset, but virtually impossible to obtain for any network. Indeed, this is highly sensitive information for network operators.
We estimate the position of base stations from the users' locations:
\begin{enumerate}
\item from each record, we extract the ID of the cell the user is connected to and her latitude/longitude coordinates;
\item the convex hull of these locations corresponds to the cell coverage area (notice that such areas can and do overlap);
\item we assume base stations sit at the baricenter of each convex hull.
\end{enumerate}

As for the core network, we assume, as in~\cite{softcell}, a tree topology where:
\begin{itemize}
\item base stations are grouped into {\em rings} of ten;
\item rings are connected to aggregation-layer {\em pods};
\item pods are connected to {\em core}-level switches.
\end{itemize}
Finally, we assume completely separate network topologies for each operator.

\noindent{\bf Per-content item demand.}
Our dataset tells us how many users use, e.g., YouTube, and how much data they consume. However, it contains no information about {\em which} of the countless YouTube videos they are watching, which is crucial to study the effectiveness of caching schemes.
We cope with this limitation through different approaches, depending on the content category:
\begin{itemize}
\item RealTime, Players: each request refers to a different content ID, modeling the fact that no caching is possible;
\item YouTube, OnDemand: the content ID is extracted from the YouTube
  measurement~\cite{youtube}, with a probability that is proportional
  to the number of each video's views;
\item News, Sports: with probability 0.9, the content item is selected
  from 50 popular ones, otherwise, a new content ID is generated;
\item Meteo, Maps: with probability 0.9 the item is selected from 10
  location-specific ones, otherwise, a new content ID is generated.

\end{itemize}
The above assignment policy reproduces the qualitative differences
between content categories, and therefore the different ways each
lends itself to caching. Also, note that content items belonging to
different applications are always considered to be different items.

\section{Caching architectures}
\label{sec:caching}

Our purpose is to evaluate not caching {\em policies}, i.e., how to choose the content to cache, rather cache {\em architectures}, i.e., at which level of the network topology caches should be deployed.
Four options are possible:
\begin{itemize}
\item individual {\em base stations}: each base station has its own cache, bringing the fog-computing vision to its extreme;
\item base station {\em rings}: caches are shared among the base stations (typically around ten) connected by the same ring, reducing the number of caches to deploy;
\item {\em aggregation-layer pods}: they typically serve hundreds of base stations within a fairly wide area; this represents a more centralized caching architecture;
\item {\em core-layer switches}: the most centralized caching architecture.
\end{itemize}

Given the user demand information, we consider a {\em target} hit ratio, and seek to determine the cache capacity needed to achieve such a ratio under different architectures.
More precisely, we proceed as follows:
\begin{enumerate}
\item we keep track of the popularity (i.e., number of requests) of each content item within each cell;
\item we sort the item/cell pairs by decreasing popularity;
\item we mark as {\em cache-worthy} enough pairs to guarantee the target hit ratio, starting from the most popular ones;
\item we identify the location at which cache-worthy content items should be stored;
\item we add at most one copy of the cache-worthy content item at said location;
\item we evaluate the total cache size needed.
\end{enumerate}
The network node at which content copies are stored (as per item 4 above) depends on the current caching architecture: if caches are deployed at base stations, it is the base station itself; otherwise, it is the core network entity (ring, aggregation pod, core-layer switch) serving that base station.

\begin{figure}[]
\centering
\includegraphics[width=.8\columnwidth]{img/archi.eps}
\caption{In this simplified network architecture, base stations are
  connected to rings, rings to aggregation pods, and the latter to a
  core switch. Shapes correspond to cache-worthy content items. The
  total cache capacity is 6 if caches are deployed at the base
  stations or at the rings; it decreases to 5 if caches are moved to
  aggregation pods, 
and to 3 if they are located at the core switch.\label{fig:archi}
\vspace{-3mm}} \end{figure}

\begin{figure*}[]
\centering
\subfigure[\label{fig:cachecdf-bs}]{
\includegraphics[width=.3\textwidth]{img/caches_cell.eps}
} \subfigure[\label{fig:cachecdf-pod}]{
\includegraphics[width=.3\textwidth]{img/caches_ring.eps}
} \subfigure[\label{fig:cache-totsize}]{
\includegraphics[width=.3\textwidth]{img/cache_totsize.eps}
} \caption{Distribution of the cache size when they are deployed at base stations (a) and rings (b); total cache size for different architectures (c).
\vspace*{-3mm}
} \end{figure*}

\Fig{archi} exemplifies the relationship between caching architecture
and cache size. The closer caches are to base stations and end-users,
the more likely we are to cache multiple copies of the same content
item (at different locations), thus increasing the total cache
size. On the other hand, caches that are closer to the end-users tend to be smaller, which can result in significant cost reduction.

\subsection{Performance metrics}
\label{sec:metrics}

\noindent{\bf Price-of-fog.}
We can formally define the price-of-fog metric as the ratio of the cache size to deploy under a given architecture to the cache size to deploy at the core switches. In the example case of \Fig{archi}, the price-of-fog is~$\frac{5}{3}\approx 1.67$ when placing caches at aggregation pods, and~$\frac{6}{3}=2$ when placing them at base stations or at the rings. Clearly, content popularity distribution and content locality have a major impact on the price-of-fog.

Suppose that exactly the same set of content items were deemed cache-worthy at all base stations -- perhaps as a consequence of an effective recommendation system. In the network of \Fig{archi}, the price-of-fog would raise as high as~$4$ -- and much higher in real networks, where core nodes have more descendants. At the other extreme, if the set of cache-worthy content items at every base station were disjoint, the price-of-fog would drop to~$1$, the lowest possible value. Indeed, one of the main contributions of our paper is to assess to which of these extreme cases current {\em and} future vehicular networks are closer.

\noindent{\bf Distance travelled by data.}
Fog computing essentially means moving data closer to the users, thus reducing the load on the core network. We quantify this effect by measuring the physical distance between the network node at which content items are cached (e.g., aggregation-layer pods or core-layer switches) and the base station serving it.

\subsection{Recommendation systems and local content}
\label{sec:recloc}

We study two factors that can alter the content demand and the distribution thereof: recommendation systems and the presence of location-specific content. The latter is expected to become a dominant factor in the near future, especially for vehicular applications.

\noindent
{\em Recommendation systems} have the high-level effect of concentrating the demand towards the most popular items. To model this, we first track the top 5\% most popular content items for each app; then, for each request, we switch the requested content to one of those popular ones with a probability~$p$. The higher~$p$, the stronger the bias towards popular content.

\noindent
In the case of {\em location-specific content}, we create~$5$ new content items specific to each cell; then, for each request, we switch the requested content to one of those local ones with a probability~$q$. The higher~$q$, the stronger the correlation between user location and content demand.

\section{Numerical results}
\label{sec:results}

\begin{figure*}[]
\centering
\subfigure[\label{fig:rec-pof}]{
\includegraphics[width=.3\textwidth]{img/rec_pof_cell.eps}
} \subfigure[\label{fig:rec-cachesize}]{
\includegraphics[width=.3\textwidth]{img/rec_cachesize.eps}
} \subfigure[\label{fig:rec-bars}]{
\includegraphics[width=.3\textwidth]{img/rec_bars.eps}
} \caption{Recommendation system: price-of-fog (a); total cache size averaged
over the different operators, as a function of~$p$ (b); per-operator breakdown when~$p=0$ (solid bars) and~$p=0.5$ (bars with pattern) (c).
\vspace*{-3mm}
} \end{figure*}

A first aspect we are interested into is cache size. For each cache architecture, we are interested in (i) the distribution of cache sizes, and (ii) the total size thereof.

Comparing the distributions in \Fig{cachecdf-bs} and
\Fig{cachecdf-pod}, we can see that the closer caches are to the end users, the smaller their size becomes -- consistently with what one might intuitively expect.
Interestingly,
there are major differences between operators: as shown in
\Fig{deployment}, Verizon has fewer cells with larger coverage areas,
therefore, it would need to deploy larger caches. T-Mobile, on the other hand, has many smaller cells, and therefore smaller caches.

Moving to the total cache size, \Fig{cache-totsize} highlights that both the total cache size {\em and} how it changes across caching architectures strongly depend on the operator and its network. T-Mobile, with its numerous small cells, has to deploy the most caches, followed by Verizon with its few bigger ones. The other operators follow intermediate approaches, and have smaller total cache sizes.

As for the price-of-fog metric defined in \Sec{metrics}, it is actually quite modest, ranging between 1.15 for Verizon and 1.25 for AT\&T. In other words, even considering the {\em current} demand of {\em current} networks, mobile operators (and their users) can reap the benefits of fog at the cost of a moderate increase in the total cache capacity they need to deploy.

\noindent{\bf Recommendation system.}
We now assume that there is a recommendation system in place, as described in \Sec{recloc}, and study the effect of the $p$-value modeling its effectiveness. Somehow surprisingly, the price-of-fog depicted in \Fig{rec-pof} {\em increases} as~$p$ grows; in other words, an effective recommendation system makes the fog computing approach more onerous in terms of required caching capacity.

Recall, however, that the price-of-fog is a ratio between two size values. As we can see from \Fig{rec-cachesize}, cache capacity {\em decreases} as~$p$ grows, for {\em all} caching architectures. However, the size of core-level caches decreases faster, hence the growing price-of-fog.

It is also interesting to point out that, as we can see from \Fig{rec-bars}, both the decrease in cache size and the price-of-fog strongly depend on the operator and its network topology. As an example, T-Mobile reaps significant benefits when caches are deployed at the core level and negligible ones otherwise, while Verizon experiences a decrease in cache size under all architectures.
This is again due to the differences in network deployments, shown in \Fig{deployment}. Cells covering very small areas, such as in the case of T-Mobile, are unlikely to be a good location to place a cache.

\begin{figure*}[]
\centering
\subfigure[\label{fig:loc-pof}]{
\includegraphics[width=.3\textwidth]{img/loc_pof_pod.eps}
} \subfigure[\label{fig:loc-cachesize}]{
\includegraphics[width=.3\textwidth]{img/loc_cachesize.eps}
} \subfigure[\label{fig:loc-bars}]{
\includegraphics[width=.3\textwidth]{img/loc_bars.eps}
} \caption{Location-specific content: price-of-fog (a); total cache size averaged
over the different operators, as a function of~$q$ (b); per-operator breakdown when~$q=0$ (solid bars) and~$q=0.5$ (bars with pattern) (c).
\vspace*{-3mm}
} \end{figure*}

\begin{figure}[]
\centering
\includegraphics[width=.35\textwidth]{img/distance_bars.eps}
\caption{
Distance travelled by data for different operators and cache architectures. Solid bars correspond to the default scenario, lines pattern to local content with~$p=0.5$, grid pattern to recommendation system with~$p=0.5$.
\label{fig:distance}
\vspace{-6mm}
} \end{figure}

\noindent{\bf Location-specific content.}
Recall that, as mentioned in \Sec{recloc}, the $q$-value expresses how strong the correlation between location and content demand is. Comparing \Fig{loc-pof} to \Fig{rec-pof} above, we can clearly see that the price-of-fog is (i) much lower, and (ii) virtually constant for all values of~$q$. At a high level, this tells us that if demand and location are strongly correlated, then embracing  fog computing-based caching comes at virtually no penalty.

Consistently, \Fig{loc-cachesize} shows that cache sizes steadily decrease as~$q$ grows, for all caching architectures. Also notice, from \Fig{loc-bars}, that the effect has roughly the same magnitude for all operators.

Finally, \Fig{distance} explores how caching architectures, recommendation systems and content locality influence the average distance travelled by data, as defined in \Sec{metrics}. We can clearly see the benefit of fog computing, as to more decentralized architectures invariably correspond shorter distances. Furthermore, such a benefit strongly depends on the operator -- and their deployment, as laid out in \Fig{deployment} -- and changes little if a recommendation system is in place or content is location-specific.

The reason for the latter is that we keep the target hit ratio fixed, and deploy the minimum amount of cache necessary to achieve it. In other words, we exploit recommendation systems and content locality to reduce the cache size (i.e., the price of the fog) rather than to enhance the benefits thereof (i.e., data travelling shorter distances).

\section{Related work}
\label{sec:relwork}

Our paper falls in the general area of caching for mobile cellular networks. The most significant recent trend in this field is {\em fog computing}, also called {\em mobile edge computing}. Compared to traditional cloud computing, the emphasis is to move processing and caching capabilities as close to the access networks (and users) as possible, so as to reduce the load on the core network.

A first body of works deal with the fundamental problem of {\em where} to locate the cached content items, given some degree of knowledge about user demand. For example, the authors of~\cite{commag-air,icn15} exploit concepts from information-centric and content-centric networking to maximize the cache hit ratio, while~\cite{mobaware} leverages mobility information for the same purpose. Other works~\cite{moving} take a more holistic approach, moving both caches and virtual machines around the network as the load changes.

An especially relevant application of caching is video streaming. As an example,~\cite{multiop-caching} accounts for layered video coding techniques, and addresses the problem of placing the right layers at the right cache, also accounting for cooperation between operators. Other works~\cite{proactive-caching,proseed} aim at {\em foreseeing} the content demand, in order to proactively populate caches~\cite{proactive-caching} or to serve users~\cite{proseed}.
\vspace{-2mm}

\section{Conclusion and current work}
\label{sec:conclusion}

Traffic demand from vehicular users is set to rapidly grow in the next
years, and cellular networks are expected to bear most of the
burden. In this context, we compared different caching
architectures from the viewpoint of the total cache size that operators need to deploy to reach a target hit ratio.

Leveraging a real-world, large-scale, crowd-sourced dataset coming
from the WeFi app, we found that fog computing pairs remarkably well
with highly localized content, such as navigation information for
future self-driving vehicles. On the other hand, more centralized
caching approaches perform better along with traditional
recommendation systems, that make globally-popular content even more popular.

We are currently extending our work by including caching policies, e.g., least-recently-used, into the picture. This would allow us to model the interaction between caching policies and caching architectures.
\vspace{-2mm}
\bibliographystyle{IEEEtran}
\bibliography{refs}
\balancecolumns 

\end{document}
