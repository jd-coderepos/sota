

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2018}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\usepackage{booktabs}
\usepackage{amsbsy}
\usepackage{footmisc}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{cleveref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{todonotes}

\usepackage[normalem]{ulem}

\aclfinalcopy \def\aclpaperid{1562} 




\usepackage{algorithmicx} 
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage[T1]{fontenc}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\algnewcommand\algorithmicdefinitions{\textbf{Definitions:}}
\algnewcommand\Definitions{\item[\algorithmicdefinitions]}
\algnewcommand\algorithmicindices{\textbf{Indices:}}
\algnewcommand\Indices{\item[\algorithmicindices]}
\newlength{\commentindent}
\setlength{\commentindent}{.5\textwidth}
\renewcommand{\algorithmiccomment}[1]{\unskip\hfill\makebox[\commentindent][l]{//~#1}\par}
\captionsetup[algorithm]{font=footnotesize}
\algrenewcommand\alglinenumber[1]{\footnotesize #1:}



\newcommand{\dline}{\hdashline[0.5pt/1pt]}
\newcommand{\ddline}[1]{\cdashline{#1}[0.5pt/1pt]}

\newcommand{\xxcomment}[4]{\textcolor{#1}{[ #4]}}
\newcommand{\tl}[1]{\xxcomment{blue}{T}{L}{#1}}
\newcommand{\ya}[1]{\xxcomment{red}{Y}{A}{#1}}
\newcommand{\eat}[1]{}

\newcommand{\edit}[3]{\textcolor{#1}{#2\sout{#3}}}
\newcommand{\tle}[2]{\edit{green}{#1}{#2}}
\newcommand{\yae}[2]{{#1}{#2}}

\newcommand{\x}{\mathbf{x}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\cc}{\mathbf{c}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\rr}{\mathbf{r}}
\newcommand{\bb}{\mathbf{b}}

\title{Simple Recurrent Units for Highly Parallelizable Recurrence}

\author{
Tao Lei  Yu Zhang  Sida I. Wang  Hui Dai  Yoav Artzi\0.1em]
\begin{tabular}{c@{~~~~~~~~~~~~~~~~~~}c}
{\tt \{tao, hd\}@asapp.com } & {\tt ngyuzh@google.com}\\
{\tt sidaw@cs.princeton.edu} & {\tt yoav@cs.cornell.edu}
\end{tabular}
}


\date{}

\begin{document}
\maketitle

\begin{abstract}
Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. 
SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models.
We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. 
We also obtain an average of 0.7 BLEU improvement over the Transformer model~\citep{vaswani2017attention} on translation by incorporating SRU into the architecture.\footnote{Our code is available at \url{https://github.com/taolei87/sru}.}


%
 \end{abstract}


\section{Introduction}
\label{sec:intro}






Recurrent neural networks (RNN) are at the core of state-of-the-art approaches for a large number of natural language tasks, including machine translation~\citep{cho-al-emnlp14,Bahdanau:14neuralmt,jean15,luong2015:EMNLP}, language modeling~\citep{zaremba2014recurrent,Gal2016Theoretically,zoph2016neural}, opinion mining~\citep{Irsoy:14opinion}, and situated language understanding~\citep{Mei:16neuralnavi,Misra:17instructions,Suhr:18context,Suhr:18situated}.
Key to many of these advancements are architectures of increased capacity and computation. 
For instance, the top-performing models for semantic role labeling and translation use eight recurrent layers, requiring days to train~\citep{he2017deep,wu2016gnmt}.
The scalability of these models has become an important problem that impedes NLP research.

The difficulty of scaling recurrent networks arises from the time dependence of state computation.
In common architectures, such as Long Short-term Memory~\cite[LSTM;][]{hochreiter1997long} and Gated Recurrent Units~\cite[GRU;][]{cho-al-emnlp14}, the computation of each step is suspended until the complete execution of the previous step. 
This sequential dependency makes recurrent networks significantly slower than other operations, and limits their applicability. 
\yae{For example, recent translation models consist of non-recurrent components only, such as attention and convolution, to scale model training~\citep{gehring2017convolutional,vaswani2017attention}.}{}


\begin{figure*}[!ht]
\centering
\includegraphics[width=5.3in]{figures/test.pdf}
\vspace{-0.1in}
\caption{Average processing time in milliseconds of a batch of 32 samples using cuDNN LSTM, word-level convolution \texttt{conv2d} (with filter width  and ), and the proposed SRU. We vary the number of tokens per sequence () and feature dimension ().}
\label{fig:overview}
\end{figure*}

In this work, we introduce the Simple Recurrent Unit (SRU), a unit with light recurrence that offers both high parallelization and sequence modeling capacity.
The design of SRU is inspired by previous efforts, \yae{such as Quasi-RNN~\cite[QRNN;][]{bradbury2016quasi} and Kernel NN~\citep[KNN;][]{lei2017deriving}}{}, but enjoys additional benefits:
\begin{itemize}
\item SRU exhibits the same level of parallelism as convolution and feed-forward nets. This is achieved by balancing sequential dependence and independence: while the state computation of SRU is time-dependent, each state dimension is independent. \yae{This simplification enables CUDA-level optimizations that parallelize the computation across hidden dimensions and time steps, effectively using the full capacity of modern GPUs.}{}
\yae{Figure~\ref{fig:overview} compares our architecture's runtimes to common architectures.}{} 
\item \yae{SRU replaces the use of convolutions (i.e., n-gram filters), as in QRNN and KNN, with more recurrent connections}{}.
This retains modeling capacity, while using less computation (and hyper-parameters).
\item SRU improves the training of deep recurrent models by employing highway connections~\citep{srivastava2015training} \yae{and a parameter initialization scheme tailored for gradient propagation}{} in deep architectures.
\end{itemize}


We evaluate SRU on a broad set of problems, including text classification, question answering, translation and character-level language modeling.
Our experiments demonstrate that light recurrence is sufficient for various natural language tasks, offering a good trade-off between scalability and representational power.
On classification and question answering datasets, SRU outperforms common recurrent and non-recurrent architectures, while achieving 5--9x speed-up compared to cuDNN LSTM.
Stacking additional layers further improves performance, while incurring relatively small costs owing to the cheap computation of a single layer. 
We also obtain an average improvement of 0.7 BLEU score on the English to German translation task by incorporating SRU into Transformer~\citep{vaswani2017attention}.















 
\section{Related Work}
\label{sec:related}

Improving on common architectures for sequence processing has recently received significant attention~\citep{greff2015lstm,BalduzziG16,miao2016simplifying,zoph2016neural,lee2017recurrent}. 
One area of research involves incorporating word-level convolutions (i.e. n-gram filters) into recurrent computation~\citep{lei:2015:EMNLP,bradbury2016quasi,lei2017deriving}.
For example, Quasi-RNN~\citep{bradbury2016quasi} proposes to alternate convolutions and a minimalist recurrent pooling function and achieves significant speed-up over LSTM.
\yae{While \citet{bradbury2016quasi} focus on the speed advantages of the network, \citet{lei2017deriving} study the theoretical characteristics of such computation and possible extensions.}{}
\yae{Their results suggest that simplified recurrence retains strong modeling capacity through layer stacking.}{}
\yae{This finding motivates the design of SRU for both high parallelization and representational power.}{}
SRU also relates to IRNN~\citep{le2015simple}, which uses an identity diagonal matrix to initialize hidden-to-hidden connections.
SRU uses point-wise multiplication for hidden connections, which is equivalent to using a diagonal weight matrix.
This can be seen as a constrained version of diagonal initialization.

Various strategies have been proposed to scale network training~\citep{goyal2017accurate} and to speed up recurrent networks~\citep{diamos2016persistent,shazeer2017outrageously,kuchaiev2017factorization}.
For instance, \citet{diamos2016persistent} utilize hardware infrastructures by stashing RNN parameters on cache (or fast memory).
\citet{shazeer2017outrageously} and \citet{kuchaiev2017factorization} improve the computation via conditional computing and matrix factorization respectively.
Our implementation for SRU is inspired by the cuDNN-optimized LSTM~\citep{cudnnlstm}, but enables more parallelism -- while cuDNN LSTM requires six optimization steps, SRU achieves more significant speed-up via two optimizations.


The design of recurrent networks, such as SRU and related architectures, raises questions about representational power and interpretability~\citep{chen2017recurrent,peng2018emnlp}.
\citet{BalduzziG16} applies type-preserving transformations to discuss the capacity of various simplified RNN architectures. 
Recent work~\citep{anselmi2015deep,DanielyFS16,zhang16l1,lei2017deriving} relates the capacity of neural networks to deep kernels.
We empirically demonstrate SRU can achieve compelling results by stacking multiple layers.
 
\section{Simple Recurrent Unit}
\label{sec:sru}
We present and explain the design of Simple Recurrent Unit (SRU) in this section. 
A single layer of SRU involves the following computation:
0.3em]
\rr_t\ &=\ \sigma\left(\W_r\x_t + \vv_r\odot \cc_{t-1} + \bb_r\right) \\
\h_t\ &=\ \rr_t\odot \cc_t + (1-\rr_t)\odot \x_t 

\nonumber 	\U^\top &= \left(\begin{array}{l}\W \\ \W_f \\ \W_r \end{array}\right) [\x_1, \x_2, \cdots, \x_L] \;\;,

\frac{1}{3}\ \leq\ \frac{\text{Var}[\h_t]}{\text{Var}[\x_t]}\ \leq\ \frac{1}{2}\;\;,

\h_t\ &=\ \rr_t\odot \cc_t\, +\, (1-\rr_t)\odot \x_t \cdot \alpha \;\;,

\alpha\ &=\ \sqrt{1+\exp(b)\times 2}\;\;.
0.3em]
\citet{wang2013fast} & & 82.1 & 93.6 & 79.1 & - & 86.3 & - & -\\
    \citet{kalchbrenner:2014} & & - & - & - & 93.0 & - & 86.8 & -\\
    \citet{Kim14} & & 85.0 & 93.4 & 81.5 & 93.6 & 89.6 & 88.1  & -\\
    \citet{zhang2015sensitivity} & & 84.7 & 93.7 & 81.7 & 91.6 & 89.6 & 85.5 & - \\
    \citet{zhao2015self} & & 86.3 & \textbf{95.5} & \textbf{83.1} & 92.4 & \textbf{93.3} & - & - \\
    \midrule
    \multicolumn{7}{@{}l}{\bf Our setup (default Adam, fixed word embeddings):}\0.5em]
   	\textbf{SRU} (2 layers) & 204k & 84.91.6 & 93.50.6 & 82.31.2 & 94.00.5& 90.10.7 & 89.20.3 & \textbf{320}\\
    \textbf{SRU} (4 layers) & 303k & 85.91.5 & 93.80.6 & 82.91.0 & \textbf{94.8}0.5& 90.10.6 & \textbf{89.6}0.5 & 510\\
    \textbf{SRU} (8 layers) & 502k & \textbf{86.4}1.7 & 93.70.6 & \textbf{83.1}1.0 & 94.70.5& 90.20.8 & 88.90.6 & 879\\
    \bottomrule
   \end{tabular}
\caption{\label{table:clf} Test accuracies on classification benchmarks (Section~\ref{sec:exp:class}).
The first block presents best reported results of various methods. 
The second block compares SRU and other baselines given the same setup.
   For the SST dataset, we report average results of 5 runs.
   For other datasets, we perform 3 independent trials of 10-fold cross validation (310 runs).
   The last column compares the wall clock time (in seconds) to finish 100 epochs on the SST dataset.
}
\end{table*}

\section{Experiments}
\label{sec:exp}

We evaluate SRU on several natural language processing tasks and perform additional analyses of the model.
The set of tasks includes text classification, question answering, machine translation, and character-level language modeling. 
Training time on these benchmarks ranges from  minutes (classification) to  days (translation), providing a variety of computation challenges.




The main question we study is the performance-speed trade-off SRU provides in comparison to other architectures. 
We stack multiple layers of SRU to directly substitute other recurrent, convolutional or feed-forward modules. We minimize hyper-parameter tuning and architecture engineering for a fair comparison. Such efforts have a non-trivial impact on the results, which are beyond the scope of our experiments. 
Unless noted otherwise, the hyperparameters are set identical to prior work. 


\subsection{Text Classification}
\label{sec:exp:class}

\paragraph{Dataset} We use six sentence classification benchmarks: movie review sentiment~\citep[MR;][]{pang2005seeing}, sentence subjectivity ~\citep[SUBJ;][]{pang2004sentimental}, customer reviews polarity~\citep[CR;][]{hu2004mining}, question type~\citep[TREC;][]{li2002learning}, opinion polarity~\citep[MPQA;][]{wiebe2005annotating}, and the Stanford sentiment treebank~\citep[SST;][]{socher2013sentiment}.\footnote{We use the binary version of SST dataset.}


Following~\citet{Kim14}, we use word2vec embeddings trained on  billion Google News tokens. 
For simplicity, all word vectors are normalized to unit vectors and are fixed during training.

\begin{figure*}[!th!]
\centering
\includegraphics[width=5.9in]{figures/clf_new.pdf}
\vspace{-0.05in}
\caption{\label{fig:clf}Mean validation accuracies (y-axis) and standard deviations of the CNN, 2-layer LSTM and 2-layer SRU models. We plot the curves of the first 100 epochs. X-axis is the training time used (in seconds). Timings are performed on NVIDIA GeForce GTX 1070 GPU, Intel Core i7-7700K Processor and cuDNN 7003.}
\end{figure*}

\paragraph{Setup} We stack multiple SRU layers and use the last output state to predict the class label for a given sentence.
We train for 100 epochs and use the validation (i.e., development) set to select the best training epoch.
We perform 10-fold cross validation for datasets that do not have a standard train-evaluation split.
The result on SST is averaged over five independent trials.
We use Adam~\citep{Kingma:14adam} with the default learning rate 0.001, a weight decay 0 and a hidden dimension of 128.

We compare SRU with a wide range of methods on these datasets, including various convolutional models~\citep{kalchbrenner:2014,Kim14,zhang2015sensitivity} and a hierarchical sentence model~\citep{zhao2015self} reported as the state of the art on these datasets~\citep{conneau17}.
Their setups are not exactly the same as ours, and may involve more tuning on word embeddings and other regularizations.
We use the setup of~\citet{Kim14} but do not fine-tune word embeddings and the learning method for simplicity.
In addition, we directly compare against three baselines trained using our code base: a re-implementation of the CNN model of~\citet{Kim14}, a two-layer LSTM model and Quasi-RNN~\citep{bradbury2016quasi}.
We use the official implementation of Quasi-RNN and also implement a version with highway connection for a fair comparison.
These baselines are trained using the same hyper-parameter configuration as SRU.


\paragraph{Results} 
Table~\ref{table:clf} compares the test results on the six benchmarks. 
We select the best number reported in previous methods when multiple model variants were explored in their experiments.
Despite our simple setup, SRU outperforms most previous methods and achieves comparable results compared to the state-of-the-art but more sophisticated model of~\citet{zhao2015self}.
Figure~\ref{fig:clf} shows validation performance relative to training time for SRU, cuDNN LSTM and the CNN model.
Our SRU implementation runs 5--9 times faster than cuDNN LSTM, and 6--40\% faster than the CNN model of~\citet{Kim14}.
On the movie review (MR) dataset for instance, SRU completes 100 training epochs within 40 seconds, while LSTM takes over 320 seconds.



\subsection{Question Answering}
\label{sec:exp:qa}

\paragraph{Dataset}
We use the Stanford Question Answering Dataset~\citep[SQuAD;][]{rajpurkar2016squad}. 
SQuAD is a large machine comprehension dataset that includes over 100K question-answer pairs extracted from Wikipedia articles.
We use the standard train and development sets.

\newcommand\Bstrut{\rule[-1.3ex]{0pt}{0pt}}   \begin{table*}[!ht!]
  \centering
  \begin{tabular}{@{~~}lccc@{~~~~~~}ccc@{~~}}
    \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{\# layers} & \multirow{2}{*}{{~~~~}Size{~~~~}} & Dev & Dev & \multicolumn{2}{c}{Time per epoch{~}}\\
    & & & EM & F1 & {~}RNN{~} & {~}Total{~} \\
    \midrule
LSTM & \multirow{2}{*}{3} & \multirow{2}{*}{4.1m} & \multirow{2}{*}{69.5} & \multirow{2}{*}{78.8} & \multirow{2}{*}{316s} & \multirow{2}{*}{431s} \\
\citep{chen2017reading} & &  &  &  &  & \\
\midrule
\multirow{2}{*}{QRNN (k=1) + highway} & 4 & 2.4m & 70.10.1& 79.40.1& 113s & 214s \\
& 6 & 3.2m & 70.60.1 & 79.60.2 & 161s & 262s \\
\midrule
SRU & 3 & 2.0m & \textbf{70.2}0.3 & \textbf{79.3}0.1 & 58s & 159s\\
SRU & 4 & 2.4m & \textbf{70.7}0.1 & \textbf{79.7}0.1 & 72s & 173s\\
SRU & 6 & 3.2m & \textbf{71.4}0.1 & \textbf{80.2}0.1 & 100s & 201s\\
    \bottomrule
   \end{tabular}
\caption{\label{table:squad}Exact match (EM) and F1 scores of various models on SQuAD (Section~\ref{sec:exp:qa}). 
   We also report the total processing time per epoch and the time spent in RNN computations.
   SRU outperforms other models, and is more than five times faster than cuDNN LSTM. }
\end{table*}

\paragraph{Setup}
We use the Document Reader model of \citet{chen2017reading} as our base architecture for this task.
The model is a combination of word-level bidirectional RNNs and attentions, providing a good testbed to compare our bidirectional SRU implementation with other RNN components.\footnote{The current state-of-the-art models~\citep{seo2016bidirectional,wang2017gated} make use of additional components such as character-level embeddings, which are not directly comparable to the setup of~\citet{chen2017reading}.
However, these models can potentially benefit from SRU since RNNs are incorporated in the model architecture.}

We use the open source implementation of Document Reader in our experiments.\footnote{\url{https://github.com/hitvoice/DrQA}}
We train models for up to 100 epochs, with a batch size of 32 and a hidden dimension of 128.
Following the author suggestions, we use the Adamax optimizer~\citep{Kingma:14adam} and variational dropout~\citep{Gal2016Theoretically} during training.
We compare with two alternative recurrent components:
the bidirectional LSTM adopted in the original implementation of \citet{chen2017reading} and Quasi-RNN with highway connections for improved performance.


\paragraph{Results}
Table~\ref{table:squad} summarizes the results on SQuAD.
SRU achieves 71.4\% exact match and 80.2\% F1 score, outperforming the bidirectional LSTM model by 1.9\% (EM) and 1.4\% (F1) respectively. 
SRU also exhibits over 5x speed-up over LSTM and 53--63\% reduction in total training time.
In comparison with QRNN, SRU obtains 0.8\% improvement on exact match and 0.6\% on F1 score, and runs 60\% faster.
\yae{This speed improvement highlights the impact of the fused kernel (Algorithm 1).
While the QRNN baseline involves a similar amount of computation, assembling all element-wise operations of both directions in SRU achieves better GPU utilization.}{}

\subsection{Machine Translation}
\label{sec:exp:mt}

\paragraph{Dataset}
We train translation models on the WMT EnglishGerman dataset, a standard benchmark for translation systems~\citep{peitz-EtAl,li-EtAl:2014,jean15}.
The dataset consists of 4.5 million sentence pairs.
We obtain the pre-tokenized dataset from the OpenNMT project~\citep{opennmt17}.
The sentences were tokenized using the word-piece model~\citep{wu2016gnmt}, which generates a shared vocabulary of about 32,000 tokens.
Newstest-2014 and newstest-2017 are provided and used as the validation and test sets.\footnote{\url{https://github.com/OpenNMT/OpenNMT-tf/tree/master/scripts/wmt}}


\paragraph{Setup}
We use the state-of-the-art Transformer model of~\citet{vaswani2017attention} as our base architecture.
In the base model, a single Transformer consists of a multi-head attention layer and a bottleneck feed-forward layer.
We substitute the feed-forward network using our SRU implementation:

The intuition is that SRU can better capture sequential information as a recurrent network, and potentially achieve better performance while requiring fewer layers.


We keep the model configuration the same as \citet{vaswani2017attention}:
the model dimension is , the feed-forward and SRU layer has inner dimensionality , and positional encoding~\cite{gehring2017convolutional} is applied on the input word embeddings.
The base model without SRU has 6 layers, while we set the number of layers to 4 and 5 when SRU is added.
Following the original setup, we use a dropout probability 0.1 for all components, except the SRU in the 5-layer model, for which we use a dropout of 0.2 as we observe stronger over-fitting in training.

We use a single NVIDIA Tesla V100 GPU for each model.
The published results were obtained using 8 GPUs in parallel, which provide a large effective batch size during training.
To approximate the setup, we update the model parameters every 55120 tokens and use 16,000 warm-up steps following OpenNMT suggestions.
We train each model for 40 epochs (250,000 steps), and perform 3 independent trials for each model configuration. A single run takes about 3.5 days with a Tesla V100 GPU.

\begin{table*}[!ht!]
\fontsize{10.5}{12.2}\selectfont
\centering
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{\# layers} & \multirow{2}{*}{~~~Size~~~} & \multicolumn{2}{c}{BLEU score} & Speed & Hours \\
& & & Valid & Test & (toks/sec) & per epoch\\
\midrule
Transformer (base) & 6 & 76m & 26.60.2 (26.9) & 27.60.2 (27.9) & 20k & 2.0\0.3em]
Transformer (+SRU) & 5 & 90m & \textbf{27.1}0.0 (\textbf{27.2}) & \textbf{28.3}0.1 (\textbf{28.4}) & 19k & 2.1 \0.2em]
MI-LSTM~\cite{wu2016milstm} & 17m & 1 & 100 & - & 1.44 & - \\
HM-LSTM~\citep{ChungAB16} & 35m & 3 & 100 & - & 1.32 & - \\
LSTM~\citep{melis2017state} & 46m & 4 & 50 & 1.28 & 1.30 & - \\
RHN~\citep{ZillySKS17} & 46m & 10 & 50 & - & 1.27 & - \\
FS-LSTM~\citep{mujika2017fast} & 47m & 4 & 100 & - & 1.25 & - \\
QRNN~\citep{merity2018analysis} & 26m & 4 & 200 & - & 1.33 & -\\
LSTM~\citep{merity2018analysis} & 47m & 3 & 200 & - & 1.23 & -\\\midrule
\multicolumn{7}{@{~~}l}{\bf Our setup:} \0.22em]
SRU & 37m & 6 & 100 & 1.29 & 1.30 & 28min \\
SRU & 37m & 10 & 100 & 1.26 & 1.27 & 29min \\
SRU (with projection) & 37m & 6 & 100 & 1.25 & 1.26 & 29min \\
SRU (with projection) & 47m & 8 & 100 & 1.21 & 1.21 & 39min \\
SRU (with projection) & 49m & 12 & 256 & 1.19 & 1.19 & 41min \\
\bottomrule
\end{tabular}
\caption{Validation and test BPCs of different recurrent models on Enwik8 dataset.
The last column presents the training time per epoch.
For SRU with projection, we set the projection dimension to 512.
\label{table:clm}
}
\end{table*}

\paragraph{Results}
Table~\ref{table:clm} presents the results of SRU and other recurrent models. 
The 8-layer SRU model achieves validation and test bits per character (BPC) of 1.21, outperforming previous best reported results of LSTM, QRNN and recurrent highway networks (RHN).
Increasing the layer of SRU to 12 and using a longer context of 256 characters in training further improves the BPC to 1.19

\subsection{Ablation Analysis}
\label{sec:exp:ablation}


We perform ablation analyses on SRU by successively disabling different components:
\begin{itemize}
\item[(1)] Remove the point-wise multiplication term  in the forget and reset gates. The resulting variant involves less recurrence and has less representational capacity.
\item[(2)] Disable the scaling correction by setting the constant .
\item[(3)] Remove the skip connections.\end{itemize}
We train model variants on the classification and question answering datasets.
Table~\ref{table:ablation} and Figure~\ref{fig:ablation} confirm the impact of our design decisions -- 
removing these components result in worse classification accuracies and exact match scores.

\begin{table}[!t!]
\centering
\begin{tabular}{lc@{~~~~~}c@{~~~}}
\toprule
Model & 4 layers & 6 layers \\
\midrule
SRU (full) &  70.7 & 71.4\\
   remove  & 70.6 & 71.1\\
   remove -scaling & 70.3 & 71.0\\
   remove highway & 69.4 & 69.1\\
\bottomrule
\end{tabular}
\caption{Ablation analysis on SQuAD. Components are successively removed and the EM scores are averaged over 4 runs.}
\label{table:ablation}
\end{table}

\begin{figure}[!t!]
\centering
\vspace{0.2in}
\includegraphics[width=3.1in]{figures/ablation_2.pdf}
\caption{Ablation analysis on the classification datasets.
Average validation results are presented. 
We compare the full SRU implementation (left blue), the variant without  multiplication (middle green) and the variant without highway connection (right yellow).}
\label{fig:ablation}
\end{figure}
 \section{Discussion}
This work presents Simple Recurrent Unit (SRU), a scalable recurrent architecture that operates as fast as feed-forward and convolutional units.
We confirm the effectiveness of SRU on multiple natural language tasks ranging from classification to translation.
We open source our implementation to facilitate future NLP and deep learning research.

\paragraph{Trading capacity with layers}
SRU achieves high parallelization by simplifying the hidden-to-hidden dependency.
This simplification is likely to reduce the representational power of a single layer and hence should be balanced to avoid performance loss. 
However, unlike previous work that suggests additional computation (e.g., n-gram filters) within the layer~\citep{BalduzziG16,bradbury2016quasi}, we argue that increasing the depth of the model suffices to retain modeling capacity.
Our empirical results on various tasks confirm this hypothesis.

 \section*{Acknowledgement}
We thank Alexander Rush and Yoon Kim for help with machine translation experiments, and Danqi Chen for help with SQuAD experiments.
We thank Adam Yala, Howard Chen, Jeremy Wohlwend, Lili Yu, Kyle Swanson and Kevin Yang for providing useful feedback on the paper and the SRU implementation.
A special thanks to Hugh Perkins for his support on the experimental environment setup and Runqi Yang for answering questions about his code.%
 
\bibliography{emnlp2018}
\bibliographystyle{acl_natbib_nourl}

\clearpage
\newpage
\appendix
\section{Parameter Initialization Derivation}
\label{sec:appendix:init}

Following the derivation of Glorot and Kaiming initialization~\cite{glorot2010understanding,he2015delving}, we assume the values of each input vector  are i.i.d with zero mean and a small variance:

We initialize each weight matrix with zero mean and a variance of .
After a matrix multiplication , each value  would have

which means the scale of the values after matrix multiplication remains the same.

\subsection{Computing Var[]}
Let  be the -th entry of the forget gate :

The pre-activation value will be sufficiently close to 0 because the parameters are initialized with zero mean and small variance and the bias value is initially 0.
As a result,

The state value  is computed according to

Substituting the expectation of  in, we get:\footnote{We are ignoring the correlation between  and  here because their variance is small.}

Therefore,  as .
The variance of  however depends on the correlation between input vectors.
When the input vectors are independent: 

However, the two vectors in the input sequence, for instance  and , are not necessarily independent, for example because two words in an input sentence are often correlated.
When the input vectors are perfectly correlated , on the other hand, 


In practice, multiple SRU layers are stacked to construct a deep network.
The internal state  and  would be a weighted combination of inputs , which will increase the correlation of the state vectors at different steps.
These state vectors are again fed into the next layer, and keep increasing the correlation. 
As a result, we expect the actual ratio between the variance of  and that of the input of the current layer  lies between the two derived values,

and would finally converge to the upper bound value of 1.
Figure~\ref{fig:var} confirms our expectation by computing the empirical value of  in deep SRU networks.
\begin{figure}[!t!]
\vspace{0.1in}
\includegraphics[width=3.1in]{figures/var_ratio.pdf}
\caption{Empirical estimation of the variance ratio  at each layer in a randomly initialized SRU model.
We use the pre-trained word2vec embeddings as input, resulting an initial ratio slightly higher than .
As expected, the ratio increases to 1 in deep layers.
}
\label{fig:var}
\end{figure}

\subsection{Computing Var[]}
Given the result in Equation (5), we  proceed to compute .
The -th entry of  is similarly computed as

The highway reset gate is not necessarily initialized with a zero bias.
Let the initial bias be  and  denote the rest of terms in the sigmoid function.
We have  and  because  and  have small variance.

We approximate the value of  using its Taylor expansion at :

We can ignore the term with  since , which gives us

Substituting this result in ,
0.5em]
&\ =\ \frac{e^{2b}\cdot \text{Var}[c]}{(e^b+1)^2} + \frac{\text{Var}[x]}{(e^b+1)^2}

\frac{e^{2b}+3}{3(e^b+1)^2}\ \leq\ \frac{\text{Var}[h]}{\text{Var}[x]}\ \leq\ \frac{e^{2b}+1}{(e^b+1)^2}

\frac{1}{3}\ \leq\ \frac{\text{Var}[h]}{\text{Var}[x]}\ \leq\ \frac{1}{2}

\text{Var}[h_{t,i}] &\ =\ \frac{e^{2b}\cdot \text{Var}[c]}{(e^b+1)^2} + \frac{\alpha^2\cdot \text{Var}[x]}{(e^b+1)^2} \
as  according to Equation (5) and the empirical evaluation (Figure~\ref{fig:var}).
This implies  if we want .
By solving for  we have

and  when .

\section{Experimental Details}
\label{sec:appendix:experiments}
We include additional experimental setup and results in this section.

\subsection{Classification}
The data and pre-processing code are obtained from the code repository of Harvard NLP.\footnote{\url{https://github.com/harvardnlp/sent-conv-torch}}

We use a batch size of 32 and a dropout probability of 0.5 for all models.
In addition, we increment the dropout to 0.55 or 0.6 for the 8-layer SRU model.
Following the implementation of~\citep{Kim14}, out-of-vocabulary words that are not in the pre-trained embeddings are initialized with random vectors with values from .

\subsection{Question Answering}
We use a word embedding dropout of 0.5 and a recurrent dropout of 0.2.
In the setup of~\citet{chen2017reading}, the bi-LSTM models concatenates the output of each layer and feed it to subsequent layers.
This helps the gradient propagation and improves the final performance.
With highway connection, this is no longer necessary.
In SRU and Q-RNN (with highway), only the output of the last layer is given to subsequent layers.

\subsection{Machine Translation}
\label{sec:appendix:mt}
We use the OpenNMT PyTorch implementation for the translation experiments.Table~\ref{table:option} shows the list of configuration options used for training.
For evaluation, we use beam size 5 and length penalty 0.6. 

\begin{table}[!h!]
\centering
\begin{tabular}{|@{~~}ll|l|}
\hline
-layers & 4 to 6 & -share\_embedding \\
-rnn\_size & 512 & -position\_encoding \\
-word\_vec\_size & 512 & -param\_init 0\\
-batch\_type & tokens & -max\_grad\_norm 0 \\
-normalization & tokens & -dropout 0.1\\
-batch\_size & 5120  & -label\_smoothing 0.1\\
-accum\_count & 5 & -epoch 40\\
-optim & adam & -param\_init\_glorot \\
-learning\_rate & 2 & \\
-adam\_beta2 & 0.998 & \\
-decay\_method & noam & \\
-warmup\_steps & 16000 & \\
\hline
\end{tabular}
\caption{Translation training configuration.}
\label{table:option}
\end{table}
\begin{table*}[!t!h!]
\fontsize{10.5}{12.2}\selectfont
\centering
\begin{tabular}{@{~~~}c@{~~~}|rc|rc|rc}
\toprule
\multirow{2}{*}{Epoch} & \multicolumn{2}{c|}{{~~~~}Transformer base{~~~~}} & \multicolumn{2}{c|}{{~~~~}w/ SRU (4 layer){~~~~}} & \multicolumn{2}{c}{{~~~~}w/ SRU (5 layer){~~~~}} \\
& {~~}Valid & Test & {~~}Valid & Test & {~~}Valid & Test \\
\midrule
20 & 26.1 & 27.3 & 26.2 & 27.6 & 26.6 & 27.9\\
21 & 26.2 & 27.3 & 26.3 & 27.7 & 26.6 & 28.1\\
22 & 26.1 & 27.4 & 26.3 & 27.8 & 26.7 & 28.0\\
23 & 26.2 & 27.4 & 26.4 & 27.7 & 26.8 & 28.1\\
24 & 26.2 & 27.4 & 26.4 & 27.8 & 26.7 & 28.0\\
25 & 26.3 & 27.4 & 26.4 & 27.7 & 26.6 & 28.1\\
26 & 26.5 & 27.5 & 26.5 & 27.7 & 26.7 & 28.1\\
27 & 26.4 & 27.6 & 26.4 & 27.6 & 26.8 & 28.1\\
28 & 26.4 & 27.6 & 26.4 & 27.7 & 26.7 & 28.2\\
29 & 26.4 & 27.5 & 26.4 & 27.8 & 26.8 & 28.2\\
30 & 26.5 & 27.7 & 26.4 & 27.8 & 26.9 & 28.1\\
31 & 26.4 & 27.6 & 26.6 & 27.7 & 26.9 & 28.3\\
32 & 26.5 & 27.5 & 26.5 & 27.8 & 26.9 & 28.3\\
33 & 26.5 & 27.5 & 26.5 & 27.8 & 27.1 & 28.3\\
34 & 26.4 & 27.6 & 26.5 & 27.9 & 26.9 & 28.2\\
35 & 26.4 & 27.6 & 26.5 & 27.9 & 26.9 & 28.2\\
36 & 26.5 & 27.6 & 26.5 & 27.8 & 26.9 & 28.3\\
37 & 26.5 & 27.5 & 26.5 & 27.8 & 26.9 & 28.2\\
38 & 26.5 & 27.6 & 26.5 & 28.0 & 27.0 & 28.2\\
39 & 26.5 & 27.6 & 26.7 & 27.8 & 27.0 & 28.2\\
40 & 26.6 & 27.6 & 26.6 & 27.9 & 27.0 & 28.2\\
\bottomrule
\end{tabular}
\caption{Average BLEU scores after each epoch.}
\label{table:full_mt}
\end{table*}
\begin{figure}[ht]
\includegraphics[width=3in]{figures/mt_ppl.pdf}
\caption{Training and validation perplexity curves of the base model and two SRU models.}
\label{fig:mtppl}
\end{figure}


Table~\ref{table:full_mt} shows the averaged BLEU score of each model from 20th to 40th epoch.
The improvement over the Transformer base model is consistent across different epochs.

Figure~\ref{fig:mtppl} plots the training and validation perplexity of three models. 
With a higher dropout (0.2) used for the SRU, the 5-layer model gets consistent lower validation perplexity over the base model and the 4-layer model.
We also see that models with SRU exhibit much faster training progress with much lower training perplexity, suggesting the models could be tuned better with further training regularization.

\subsection{Character-level Language Modeling}
We train all models using a weight decay of  and a gradient clipping of .
We set the learning rate factor of Noam scheduling to  and the warmup steps to .
We tune the dropout probability from .

The projection (bottleneck) trick is implemented as follows.
Recall that the batched multiplication of SRU is computed as

The stacked parameter matrices on the left is re-parameterized by a low-rank factorization,

where  and  are two new parameter matrices to be learned, and  is the projection dimension that is much smaller than the input and output dimension of the SRU. 
\end{document}
