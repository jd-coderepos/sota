\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}      


\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{algorithm, algpseudocode}
\usepackage{multirow}
\usepackage{caption}
\usepackage{comment}
\usepackage[dvipsnames,svgnames,x11names]{xcolor}
\definecolor{citecolor}{RGB}{119,185,0} 
\usepackage[pagebackref=false,breaklinks=true,letterpaper=true,colorlinks,citecolor=citecolor,bookmarks=false]{hyperref}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\usepackage{pifont}
\usepackage{color}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}



\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
  
\newcommand{\zznote}[1]{\textcolor{orange}{ZZ: #1}}
\newcommand{\yynote}[1]{\textcolor{red}{YY: #1}}
\newcommand{\jwnote}[1]{\textcolor{green}{JW: #1}}
\newcommand{\lgnote}[1]{\textcolor{orange}{[LG: #1]}}

\def\eg{\emph{e.g.}} 
\def\ie{\emph{i.e.}} 
\def\etal{\emph{et~al.}} 

\iccvfinalcopy
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Composed Image Retrieval with Text Feedback\\ via Multi-grained Uncertainty Regularization}
\author{
    Yiyang Chen ${^{1,2}}$\thanks{Work done during an internship at NUS.} \quad Zhedong Zheng${^1}$ \quad Wei Ji${^1}$ \quad Leigang Qu${^1}$ \quad Tat-Seng Chua${^1}$
    \\
     $^1$ Sea-NExT Joint Lab, National University of Singapore \quad $^2$Tsinghua University 
    }

\maketitle

\begin{abstract}
We investigate composed image retrieval with text feedback.
Users gradually look for the target of interest by moving from coarse to fine-grained feedback.  
However, existing methods merely focus on the latter, \ie, fine-grained search, by harnessing positive and negative pairs during training. This pair-based paradigm only considers the one-to-one distance between a pair of specific points, which is not aligned with the one-to-many coarse-grained retrieval process and compromises the recall rate. 

In an attempt to fill this gap, we introduce a unified learning approach to simultaneously modeling the coarse- and fine-grained retrieval by considering the multi-grained uncertainty. 
The key idea underpinning the proposed method is to integrate fine- and coarse-grained retrieval as matching data points with small and large fluctuations, respectively.
Specifically, our method contains two modules: uncertainty modeling and uncertainty regularization. 
(1) The uncertainty modeling simulates the multi-grained queries by introducing identically distributed fluctuations in the feature space. 
(2) Based on the uncertainty modeling, we further introduce uncertainty regularization to adapt the matching objective according to the fluctuation range.
Compared with existing methods, the proposed strategy explicitly prevents the model from pushing away potential candidates in the early stage, and thus improves the recall rate.  
On the three public datasets, \ie, FashionIQ, Fashion200k, and Shoes, the proposed method has achieved +4.03\%, + 3.38\%, and + 2.40\% Recall@50 accuracy over a strong baseline, respectively. 
\end{abstract}

\begin{figure}[t]
\begin{center}\vspace{-.1in}
     \includegraphics[width=1\linewidth]{images/motivation.pdf}
\end{center} \vspace{-.25in}
      \caption{ The typical retrieval process contains two steps, \ie, the coarse-grained retrieval and fine-grained retrieval.
      The coarse-grained retrieval harnesses the brief descriptions or imprecise query images, while the fine-grained retrieval requires more details for one-to-one mapping. 
      The existing approaches usually optimize the strict pair-wise distance during training, which is different from the one-to-many coarse-grained test setting. Overwhelming one-to-one metric learning compromises the model to recall potential candidates. 
      }
      \label{fig:motivation}\vspace{-.1in}
\end{figure}


\section{Introduction}\label{sec:introduction}
Despite the great success of recent deeply-learned retrieval systems~\cite{radenovic2018fine, zheng2017sift, zheng2020dual}, obtaining accurate queries remains challenging. Users usually cannot express clearly their intention at first glance or describe the object of interest with details at the very beginning. 
Considering such an obstacle, the retrieval system with feedback is preferable since it is similar to the human recognition process, which guides the users to provide more discriminative search conditions. These conditions are used to narrow down the search scope effectively and efficiently, such as \emph{``I want similar shoes but with red color''}. In this work, without loss of generality, we study a single-round real-world scenario, \ie, composed image retrieval with text feedback. This task is also named text-guided image retrieval. Given one query image and one feedback, the retrieval model intends to spot the image, which is similar to the query but with modified attributes according to the text. 
Such an ideal text-guided image retrieval system can be widely applied to shopping and tourism websites to help users find the target products / attractive destinations without the need to expressing the clear intention at the start~\cite{liu2012street}. 
Recent developments in this field are mainly attributed to two trends: 1) the rapid development of deeply-learned methods in both computer vision and natural language processing communities, \eg, the availability of pre-trained large-scale cross-modality models, such as CLIP~\cite{clip};
and 2) the fine-grained metric learning in biometric recognition, such as triplet loss with hard mining ~\cite{hermans2017defense, oh2016deep}, contrastive loss~\cite{zheng2017discriminatively}, and infoNCE loss~\cite{oord2018representation,han2022fashionvil}, which mines one-to-one relations among pair- or triplet-wise inputs.

However, one inherent problem remains: how to model the coarse-grained retrieval? The fine-grained metric learning in biometrics \emph{de facto} is designed for strict one-to-one fine-grained matching, which is not well aligned with the multi-grained image retrieval with text feedback. 
As shown in Figure~\ref{fig:motivation},  we notice that there exists multiple true matchings or similar candidates.  
If we still apply pair-wise metric learning, it will push away these true positives,  compromising the training on coarse-grained annotations.

\begin{figure}[t]
\begin{center}\vspace{-.2in}
     \includegraphics[width=1\linewidth]{images/space.png}
\end{center} \vspace{-.3in}
      \caption{ Our intuition. We notice that there exist two typical matching types for the fine- and coarse-grained retrieval. Here we show the difference between one-to-one matching (left) and one-to-many matching (right). }
      \label{fig:space}\vspace{-.15in}
\end{figure}

As an attempt to loosen this restriction, we introduce a matching scheme to model an uncertainty range, which is inspired by the human retrieval process from coarse-grained range to fine-grained range. 
As shown in Figure~\ref{fig:space}, we leverage uncertain fluctuation to build the multi-grained area in the representation space. 
For the detailed query, we still apply one-to-one matching as we do in biometric recognition. On the other hand, more common cases are one-to-many matching. We conduct matching between one query point and a point with an uncertain range. The uncertain range is a feature space, including multiple potential candidates due to the imprecise query images or the ambiguous textual description. Jointly considering the two kinds of query, we further introduce a unified uncertainty-based approach for both one-to-one matching and one-to-many matching, in the multi-grained image retrieval with text feedback. 
In particular, the unified uncertainty-based approach consists of two modules, \ie, uncertainty modeling and uncertainty regularization. 
The uncertainty modeling simulates the real uncertain range within a valid range. The range is estimated based on the feature distribution within the mini-batch. 
Following the uncertainty modeling, the model learns from these noisy data with different weights. The weights are adaptively changed according to the fluctuation range as well. In general, we will not punish the model, if the query is ambiguous. In this way, we formulate the fine-grained matching with the coarse-grained matching in one unified optimization objective during training. Different from only applying one-to-one matching, the uncertainty regularization prevents the model from pushing away potential true positives, thus improving the recall rate.  
Our contributions are as follows. 
\begin{itemize}
\vspace{-2mm}
    \item We identify a training/test misalignment between the fine-grained metric learning and the demand on coarse-grained inference in the real-world image retrieval with text feedback. 
\vspace{-2mm}
    \item  We introduce a new unified method to learn both fine- and coarse-grained matching during training. In particular, we leverage the uncertainty-based matching, which consists of uncertainty modeling and uncertainty regularization.  
\vspace{-2mm}
    \item Albeit simple, the proposed method has achieved competitive recall rates, \ie, 61.39\%, 79.84\% and 70.2\% Recall@50 accuracy on three large-scale datasets, \ie, FashionIQ, Fashion200k, and Shoes, respectively. Since our method is orthological to existing methods, it can be combined with existing works to improve performance further.
\end{itemize}


\section{Related work}


\subsection{Composed Image Retrieval with Text Feedback}
Traditional image retrieval systems utilize one relevant image as a query~\cite{philbin2007object, zheng2020university}. It is usually challenging to acquire such an accurate query image in advance. 
The multimodal query involves various input queries of different modalities, such as image and text, which eases the difficulty in crafting a query and provides more diverse details.  
In this work,  we focus on image retrieval with text feedback, also called text-guided image retrieval. 
Specifically, the input query contains a reference image and textual feedback describing the modifications between the reference image and the target image.
The critical challenge is how to compose the two modality inputs properly~\cite{he-2022-CPL,yang2016stacked,saito2023pic2word,shin2021rtic}, and align visual and language space~\cite{clip,norelli2022asif,saito2023pic2word}.
The existing methods~\cite{2021CoSMo,CLIP4Cir} usually extract the textual and visual features of the query separately through the text encoder and image encoder. These two types of features are composited as the final query embeddings to match the visual features of the target image.
Generally, there are two families of works on image retrieval with text feedback based on whether using the pre-trained model. 
The first line of works mainly studies how to properly combine the features of the two modalities~\cite{qu2021dynamic,han2022fashionvil}.
Content-Style Modulation (CosMo)~\cite{2021CoSMo} proposes a new image-based compositor containing two independent modulators. 
Given the extracted visual feature from the reference image, the content modulator first performs local updates on the visual feature map according to text features.
The style modulator then normalizes the feature with the original visual feature mean and std as the global modification, which recovers the visual feature distribution for matching. 
Taking one step further, CLVC-Net~\cite{CLVC-Net} introduces two fine-grained compositors:
a local-wise image-based compositor and a global-wise text-based compositor. 
Following the spirit of mutual learning~\cite{deep_mutual_learning}, two compositors are learned from each other considering the prediction consistency. Since the backbone network shares weights, the model enhances the local-wise and global-wise composition.  
With the rapid development of the big model, another line of works is to leverage the model pre-trained on large-scale cross-modality datasets, and follow the pretrain-finetune paradigm. 
For instance, the recent work CLIP4Cir~\cite{CLIP4Cir} applies CLIP~\cite{clip} as the initial network to integrate text and image features. CLIP4Cir adopts a two-stage training strategy. Similar to CLIP, CLIP4Cir fine-tunes the CLIP text encoder and CLIP visual encoder for linear feature matching in the first stage. 
In the second stage, the two encoders are fixed and a new non-linear compositor is added to fuse the well-trained text and visual feature in an end-to-end manner. Since the two-stage strategy fine-tunes one part in one stage, it eases the training difficulty on both compositor and encoders.  Different from these existing works, in this work, we do not focus on the network structure or pretraining weights. Instead, we take a closer look at the multi-grained matching, especially the coarse-grained one-to-many matching during training. We explicitly introduce the uncertainty range to simulate the intra-class jittering.

\subsection{Uncertainty Learning} 
With the rapid development of data-driven methods, the demands on the model reliability rise. For instance, one challenging problem still remains how to measure the ``confidence" of a prediction. Therefore, some researchers resort to uncertainty. 
Kendall~\etal~\cite{uncertainty} divide the uncertainty into two major categories, \ie, epistemic uncertainty and aleatoric uncertainty. The former epistemic uncertainty denotes model uncertainty that the models, even trained on the same observation (dataset), learn different weights. The typical work of this direction is Bayesian networks \cite{bnn_2007, gal2016dropout}, which does not learn any specific weights but the distribution of weights.  
In a similar spirit, Monte Carlo Dropout~\cite{gal2016dropout} is proposed to simulate the Bayesian networks during inference, randomly dropping the network weights. 
Another family of works deals with the inherent data uncertainty, usually caused by device deviations or ambiguous annotations. This line of uncertainty approaches has been developed in many fields, including image retrieval~\cite{Warburg2021BayesianTL}, image classification~\cite{DBLP:journals/corr/abs-2107-00649}, image segmentation~\cite{zheng2021rectifyingijcv}, 3D reconstruction~\cite{DBLP:journals/corr/abs-2109-13912}, and person re-id~\cite{Robust_uncertainty,zhang2022implicit}. 
In term of representation learning, \cite{chun2021probabilistic,pishdad2022uncertainty} directly regress the mean and variance of the input data and use the probability distribution similarity instead of the cosine similarity. 
Similar to \cite{chun2021probabilistic,pishdad2022uncertainty}, Oh~\etal~\cite{oh2018modeling} use Monte Carlos sampling to obtain the averaged distribution similarity.
Besides, Warburg~\etal~\cite{Warburg2021BayesianTL} directly consider the loss variance instead of the feature variance. In particular, they re-formulate the original triplet loss to involve the consideration of loss variance.
Another line of works \cite{chang2020data,dou2022reliability} directly adds noise to features to simulate the uncertainty. The variance is also from the model prediction like \cite{chun2021probabilistic,pishdad2022uncertainty} and a dynamic uncertainty-based loss is deployed as \cite{uncertainty,zheng2021rectifyingijcv}. 
Similarly, in this work, we also focus on enabling the model learning from multi-grained annotations, which can be viewed as an annotation deviation. 
Differently, there are two primary differences from existing uncertainty-based works: (1) We explicitly introduce Gaussian Noise to simulate the data uncertainty in terms of the coarse-grained retrieval. For instance, we simulate the ``weak'' positive pairs by adding intra-class jittering.  
(2) We explicitly involve the noisy grade into optimization objectives, which unifies the coarse- and fine-grained retrieval. If we add zero noise, we still apply the strict one-to-one metric learning objective. If we introduce more noise, we give the network less punishments. It is worth noting that unifying the fine- and coarse-grained retrieval explicitly preserve the original representation learning, while prevents over-fitting to ambiguous coarse-grained samples in the dataset.
 

\section{Method} \label{sec:method}
We first introduce some notations and assumptions. Next, we illustrate the uncertainty modeling, followed by the details of uncertainty regularization. Finally, we discuss the differences from existing works and the advantages in implementation. 

\begin{figure*}[t]
\begin{center}\vspace{-.1in}
     \includegraphics[width=0.95\linewidth]{images/pipeline.pdf}
     \vspace{-.15in}
     \caption{The overview of our network.  
     Given the source image $I_s$ and the text $T_s$ for modification, we obtain the composed features $f_s$ by combining $f^T_s$ and $f^I_s$ via compositor. The compositor contains a content module and a style module.
     Meanwhile, we extract the visual features $f_t$ of the target image $I_t$ via the same image encoder as the source image.  
     Our main contributions are the uncertainty modeling via augmenter, and the uncertainty regularization for coarse matching. 
     (1) The proposed augmenter applies feature-level noise to $f_t$, yielding $\hat{f_t}$ with identical Gaussian Noise $N(1,\sigma_t)$ and $N(\mu_t,\sigma_t)$, respectively. Albeit simple, it is worth noting that the augmented feature $\hat{f_t}$ simulates the intra-class jittering of the target image, following the original feature distribution. 
     (2) The commonly-used InfoNCE loss focuses on the fine-grained one-to-one mapping between the original target feature $f_t$ and the composited feature $f_s$.
     Different from InfoNCE loss, the proposed method harnesses the augmented feature $\hat{f_t}$ and $f_s$ to simulate the one-to-many mapping, considering different fluctuations during training. 
     Our model applies both the fine-grained matching and the proposed coarse-grained uncertainty regularization, facilitating the model training. 
     }
     \label{fig:pipeline} \vspace{-.2in}
\end{center} 
\end{figure*}

\subsection{Problem Definition.} 
We show the brief pipeline in Figure ~\ref{fig:pipeline}. \textbf{In this paper, we do not pursue a sophisticated network structure but a new learning strategy. Therefore, we mainly follow existing works~\cite{2021CoSMo,Chen_2020_CVPR} to build the network for a fair comparison, and more structure details are provided in Implementation Details.} Given a triplet input, \ie, one source image input $I_s$, one text sentence $T_s$ and the target image $I_t$, the model extracts the visual feature $f^I_s$ of $I_s$, the target feature $f_t$ of $I_t$ and the textual feature  $f^T_s$ of $T_s$. Then we leverage a simple compositor to combine the source visual feature $f^I_s$ and the source text feature $f^T_s$ as the composed feature $f_s$.
We intend to increase the cosine similarity between $f_s$ and $f_t$ in the representation space.
During test time, we extract $f_s$ as the query feature to find the most similar $f_t$ in the candidate pool.  

We mainly consider the uncertainty in the triplet annotations. As shown in Figure~\ref{fig:motivation}, the dataset usually contains multi-grained matching triplets, considering descriptions and input images. 
If the description is ambiguous or the source image is relatively imprecise, the system should return multiple candidate images. 
In the training phase, if we still apply strong one-to-one matching supervision on such triplet, it will significantly compromise the model to recall potential true matches. 
Therefore, in this work, we mainly focus on the uncertainty in the triplet matching. If not specified, we deploy uncertainty to denote matching uncertainty.

\subsection{Uncertainty Modeling} 
We introduce a noise augmenter to simulate the intra-class jittering. As shown in Figure~\ref{fig:space}, instead of strict one-to-one matching, we impel the model to focus on one-to-many matching. Therefore, we need to generate the jittering via augmenter.   
The augmenter directly works on the final representation space. 
In particular, the augmenter adds Gaussian Noise of the original feature distribution to the target features $f_t$. 
The mean $\mu_t$ and standard deviation $\sigma_t$ of the Gaussian Noise are calculated from the original feature $f_t$. We then apply the generated Gaussian noise to the whitened feature. Therefore, the final jittered feature $\hat{f}_t$ can be formulated as follows:\vspace{-2mm}
\begin{equation} 
\hat{f}_t = \alpha \cdot \bar{f_t} + \beta \label{augment}
\vspace{-2mm}
\end{equation}
where $\alpha$ and $\beta$ are the noisy vectors with the same shape as the input target feature, $\alpha \sim N(1, \sigma_t), \beta \sim N(\mu, \sigma_t)$, and $\bar{f}_t$ is whitened feature $\bar{f}_t = \frac{f_t-\mu_t}{\sigma_t}$. We apply the element-wise multiplication to re-scale the input feature, so the Gaussian noise mean of $\alpha$ is set as 1, which is different from $\beta$. It is worth noting that, in this way, we make the feature fluctuate in a limited degree, which is close to the original distribution.   


\subsection{Uncertainty Regularization}
The existing methods usually adopt InfoNCE loss~\cite{NIPS2004_42fe8808, Movshovitz-Attias_2017_ICCV,NIPS2017_cb8da676, Gidaris_2018_CVPR, TIRG, 2021CoSMo}. Actually, InfoNCE loss can be viewed as a kind of batch-wise classification loss. It can be simply formulated as:\vspace{-2mm}
\begin{equation}
    \mathcal{L}_{\mathrm{info}} \left( f_s,\ f_t \right)=\frac{1}{B} \sum_{i=1}^{B}-\log \frac{\exp \left(\kappa\left(f^i_s, f^i_t\right)\right)}{\sum_{j=1}^{B} \exp \left(\kappa\left(f^i_s, f^j_t\right)\right)}.
    \label{equ1}\vspace{-1mm}
\end{equation}
Given the composed feature $f_s$ and the target feature $f_t$ of a mini-batch with $B$ samples, InfoNCE loss maximizes the self-similarity $\kappa\left(f^i_s, f^i_t\right)$ while minimizing the similarity with other samples $\kappa\left(f^i_s, f^j_t\right)$ $(i\neq j)$ in the batch. We usually adopt the cosine similarity as $\kappa$, which is defined as $\kappa(f^i, f^j) = \frac{f^i\cdot f^j}{|f^i||f^j|}$. 

We note that InfoNCE loss merely focuses on the one-to-one fine-grained matching. In this work, we intend to unify the fine- and coarse-grained matching. Inspired by the aleatoric uncertainty, we propose the uncertainty regularization. The basic InfoNCE loss is a special case of our loss. Given the augmented feature $\hat{f}_t$ and the composed feature $f_s$, our uncertainty regularization can be defined as follow: \vspace{-2mm}
\begin{equation}
    \mathcal{L}_{\mathrm{u}}\left( f_s,\ \hat{f_t}, \sigma \right)= \cfrac{\mathcal{L}_{\mathrm{info}}\left( f_s,\ \hat{f_t} \right)}{2 \sigma^{2}} +\cfrac{1}{2} \log \sigma^{2}.
\label{eq:uncertain}\vspace{-1mm}
\end{equation}
If the $\sigma$ is a constant, our $\mathcal{L}_{\mathrm{u}}$ regresses to a weighted $\mathcal{L}_{\mathrm{info}}$. 
The main difference from the InfoNCE loss is that we adaptively tune the optimization objective according to the jittering level in $\hat{f}_t$. If the jittering fluctuation is large (\ie, a large $\sigma$), the weight of first-term InfoNCE loss decreases. In contrast, if the feature has limited changes, the regularization is close to the original InfoNCE loss.  



To optimize the multi-grained retrieval performance, we adopt a combination of fine-grained InfoNCE loss $\mathcal{L}_{\mathrm{inf}} $ and the proposed uncertainty regularization $\mathcal{L}_{\mathrm{u}}$. Therefore, the total loss is as follows:\vspace{-2mm}
\begin{equation}
    \mathcal{L}_{\mathrm{total}}= \gamma \mathcal{L}_{\mathrm{u}}\left( f_s,\ \hat{f_t}, \sigma_t \right) + \left(1-\gamma\right)\mathcal{L}_{\mathrm{info}} \left( f_s,\ f_t \right), 
    \label{total_loss}\vspace{-1mm}
\end{equation}
where $\gamma $ is a dynamic weight hyperparameter to balance the ratio of the fine- and coarse-grained retrieval. \textbf{Here we ignore the constant term, which does not generate backward gradients.} We could rewrite this loss in a unified manner as:\vspace{-2mm}
\begin{equation}
    \mathcal{L}_{\mathrm{total}}= \gamma \mathcal{L}_{\mathrm{u}}\left( f_s,\ \hat{f_t}, \sigma_t \right) + \left(1-\gamma\right)\mathcal{L}_{\mathrm{u}} \left( f_s,\ f_t, \frac{1}{
    \sqrt 2} \right). 
    \vspace{-1mm}
\end{equation}
During training, we gradually decrease the coarse-grained learning, and increase the fine-grained learning by annealing $\gamma=\exp(-\gamma_0\cdot \frac{current\_epoch}{total\_epoch} ),$
where $\gamma_0$ is the initial weight. By setting the exponential function, we ensure $\gamma \in [0,1]$. 
If $\gamma = 0$, we only consider the fine-grained retrieval as existing works~\cite{2021CoSMo}. If $\gamma = 1$, we only consider the coarse-grained one-to-many matching between fluctuated features.







\noindent\textbf{Discussion.} 
\textbf{1). What is the advantage of uncertainty regularization in the feature learning?} The uncertainty regularization is to simulate the one-to-many matching case, which leaves space for multiple ground-truth candidates. It successfully invades the model to over-fitting the strict one-to-one matching as which in biometric recognition. As shown in the experiment, the proposed method significantly surpasses other competitive methods in terms of the recall rate, especially Recall@50.
\textbf{2). How about using uncertainty regularization alone?  Why adopt the dynamic weight?} Only coarse matching, which is easy to converge, leads the model to miss the challenging fine-grained matching, even if the description is relatively detailed. 
Therefore, when the model converges with the ``easy'' coarse-grained matching during training, we encourage the model to focus back on the fine-grained matching again. The ablation study on $\gamma$ in Section~\ref{dropout_vs_uncertainty} also verifies this point.
\textbf{3). Extensibility to the Ensembled Dataset.}  
For instance, in the real-world scenario, one common challenge is how to learn from the ensembled dataset. The ensembled dataset may contain fine-grained text descriptions as well as coarse-grained attribute annotations, like key words. 
For such a case, the proposed method could, in nature, facilitate the model to learn from multi-grained data. The experiment on the three different subsets of FashionIQ verifies the scalability of the proposed method.

\begin{table*}[]
\centering
\small
    \begin{tabular}{l|c|cccccc|cc}
    \shline
    \multirow{2}{*}{Method} & \multirow{2}{*}{Visual Backbone} & \multicolumn{2}{c}{Dress} & \multicolumn{2}{c}{Shirt} & \multicolumn{2}{c}{Toptee} & \multicolumn{2}{|c}{Average}\\\cline{3-10}
    \multirow{2}{*}{} & \multirow{2}{*}{} &  R@10 & R@50 & R@10 & R@50 & R@10 & R@50 & R@10 & R@50\\\shline
    MRN \cite{MRN} & ResNet-152 & 12.32 & 32.18 & 15.88 & 34.33 & 18.11 & 36.33 & 15.44 & 34.28 \\
    FiLM \cite{perez2018film} & ResNet-50 & 14.23 & 33.34 & 15.04 & 34.09 & 17.30 & 37.68 & 15.52 & 35.04 \\
    TIRG \cite{TIRG} & ResNet-17 & 14.87 & 34.66 & 18.26 & 37.89 & 19.08 & 39.62 & 17.40 & 37.39 \\
    VAL \cite{Chen_2020_CVPR} & ResNet-50 & 21.12 & 42.19 & 21.03 & 43.44 & 25.64 & 49.49 & 22.60 & 45.04 \\
    ARTEMIS \cite{delmas2022artemis} & ResNet-50 & 27.16 & 52.40 & 21.78 & 54.83 & 29.20 & 43.64 & 26.05 & 50.29 \\
    DCNet \cite{kim:2021:AAAI} & ResNet-50 & 28.95 & 56.07 & 23.95 & 47.30 & 30.44 & 58.29 & 27.78 & 53.89 \\
    CoSMo \cite{2021CoSMo} & ResNet-50 & 26.45 & 52.43 & 26.94 & 52.99 & 31.95 & 62.09 & 28.45 & 55.84 \\
\hline 
    Baseline & ResNet-50 & 24.80 & 52.35 & 27.70 & 55.71 & 33.40 & 63.64 & 28.63 & 57.23\\
    Ours & ResNet-50 & \textbf{{30.60}} & \textbf{{57.46}} & \textbf{31.54} & \textbf{58.29} & \textbf{{37.37}} & \textbf{{68.41}} & \textbf{{33.17}} & \textbf{{61.39}}\\
    \shline
    CLVC-Net \cite{CLVC-Net} & ResNet-50$\times$2 & 29.85 & 56.47 & 28.75 & 54.76 & 33.50 & 64.00 & 30.70 & 58.41 \\
    Ours & ResNet-50$\times$2 & \textbf{{31.25}} & \textbf{{58.35}} & \textbf{{31.69}} & \textbf{{60.65}} & \textbf{{39.82}} & \textbf{{71.07}} & \textbf{{34.25}} & \textbf{{63.36}}\\ 
    \shline
    CLIP4Cir \cite{CLIP4Cir} & ResNet-50$\times$4 & 31.63 & 56.67 & \textbf{{36.36}} & 58.00 & 38.19 & 62.42 & 35.39 & 59.03 \\
    Ours & ResNet-50$\times$4 & \textbf{32.61} & \textbf{61.34} & 33.23 & \textbf{62.55} & \textbf{41.40} & \textbf{72.51} & \textbf{35.75} & \textbf{65.47}\\
    \shline
    \end{tabular}
    \vspace{-.1in}
    \caption{Results on FashionIQ. The best performance is in \textbf{bold}. Here we show the recall rate R@K, which denotes Recall@K. Average denotes the mean of R@K on all subsets. It worth noting that our method with one ResNet-50 is competitive with CLIP4Cir~\cite{CLIP4Cir} of 4$\times$ResNet-50 in R@50. We train models with different initialization as model ensembles.} 
    \label{tab:Recall rates on fashionIQ}
    \vspace{-.2in}
\end{table*}

\section{Experiment}
\subsection{Implementation Details}   
We employ the pre-trained models as our backbone: ResNet-50~\cite{He2015resnet} on ImageNet as the image encoder and RoBERTa~\cite{liu2019roberta} using the BERT-base architecture as the text encoder. 
We remove the original final classification layer of ResNet-50 for ImageNet. Therefore, the output feature of ResNet-50 backbone is 2048 dimensions, and we deploy a linear layer for mapping the output to 512-dim feature for further comparison. Similarly, we harness RoBERTa, followed by a Linear Layer to reduce the feature channels from 768 to 512 dimensions.
Our work is trained with a mini-batch of 32. We deploy SGD optimizer \cite{sgd} with 50 training epochs and the base learning rate is set as $2 \times 10^{-2}$, which remains the same setting as several existing works~\cite{2021CoSMo}. 
We apply the one-step learning rate scheduler to decay the learning rate by 10 at the 45th epoch.
We empirically set $w_1, w_2$, which control the scale of augmenter generates Gaussian noise, and balance weight $\gamma_0$ between the fine- and coarse-grained retrieval to 1. During inference, we extract the composed feature $f_s$ to calculate the cosine similarity with the feature of gallery images. The final ranking list is generated according to the feature similarity.

\noindent\textbf{Reproducibility.} The code is based on Pytorch~\cite{NEURIPS2019_9015}. We will make our code open-source for reproducing all results.

\subsection{Datasets}
Without loss of generability, we verify the effectiveness of the proposed method on the fashion datasets, which collect the feedback from customers easily, including FashionIQ \cite{guo2019fashion}, Fashion200k \cite{fashion200k} and Shoes \cite{shoes}. 
Each image in these fashion datasets is tagged with descriptive texts as product description, such as ``similar style t-shirt but white logo print''. \textbf{FashionIQ.} We follow the training and test split of existing works \cite{Chen_2020_CVPR,2021CoSMo}.
Due to privacy changes and deletions, some links are out-of-the-date. We try our best to make up for the missing data by requesting other authors. As a result, we download 75,384 images and use 46,609 images in the original protocol for training. \textbf{Shoes.} Shoes \cite{shoes} crawls 10,751 pairs of shoe images with relative expressions which describe fine-grained visual differences. We use 10,000 samples for training and 4,658 samples for evaluation. \textbf{Fashion200k.} 
Fashion200k \cite{fashion200k} has five subsets: \emph{dresses}, \emph{jackets}, \emph{pants}, \emph{skirts}, and \emph{tops}. We deploy 172,000 images for training on all subsets and 33,480 test queries for evaluation. \textbf{Evaluation metric.}  Following existing works, we report the average Recall@1, Recall@10, and Recall@50 of all queries.

\begin{table}[]
\resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc|ccc}
    \shline
    \multirow{2}{*}{Method} & \multicolumn{3}{c}{Shoes} & \multicolumn{3}{|c}{Fashion200k} \\\cline{2-7}
    \multirow{2}{*}{} & R@1 & R@10 & R@50 & R@1 & R@10 & R@50\\
    \shline
    MRN\cite{MRN} & 11.74 & 41.70 & 67.01 & 13.4 & 40.0 & 61.9\\
    FiLM\cite{perez2018film} & 10.19 & 38.89 & 68.30 & 12.9 & 39.5 & 61.9\\
    TIRG\cite{TIRG} & 12.6 & 45.45 & 69.39 & 14.1 & 42.5 & 63.8\\
    VAL\cite{Chen_2020_CVPR} & 16.49 & 49.12 & 73.53 & 21.2 & 49.0 & 68.8\\
    CoSMo\cite{2021CoSMo} & 16.72 & 48.36 & 75.64 & \textcolor{red}{23.3} & 50.4 & 69.3\\
    DCNet\cite{kim:2021:AAAI} & - & \textcolor{red}{53.82} & \textcolor{blue}{79.33} & - & 46.9 & 67.6\\
    ARTEMIS\cite{delmas2022artemis} & \textcolor{red}{18.72} & 53.11 & 79.31 & 21.5 & \textcolor{blue}{51.1} & \textcolor{red}{70.5}\\
\shline
    Baseline & 15.26 & 49.48 & 76.46 & 19.5 & 46.7 & 67.8 \\    
    Ours & \textcolor{blue}{18.41} & \textcolor{blue}{53.63} & \textcolor{red}{79.84} & \textcolor{blue}{21.8} & \textcolor{red}{52.1} & \textcolor{blue}{70.2} \\
    \shline
    \end{tabular}}
    \vspace{-.15in}
    \caption{Results on Shoes and Fashion200k. Here we mainly compare ours with single model-based methods. The best performance is in \textbf{\textcolor{red}{red}}, while the second-best is in \textbf{\textcolor{blue}{blue}}. Similar to the phenomenon in FashionIQ, we could observe that the proposed method significantly improves the recall rate, which is aligned with our intuition on coarse-grained matching. 
}
    \label{tab:Recall rates on Shoes and fashion200k}
     \vspace{-.2in}
\end{table}

\subsection{Comparison with Competitive Methods}
We show the recall rate on FashionIQ in Table \ref{tab:Recall rates on fashionIQ}, including the three subsets and the average score. 
We could observe four points: (1) The method with uncertainty modeling has largely improved the baseline in both Recall@10 and Recall@50 accuracy, verifying the motivation of the proposed component on recalling more potential candidates. 
Especially for Recall@50, the proposed method improves the accuracy from $57.23\%$ to $61.39\%$.
(2) Comparing with the same visual backbone, \ie, one ResNet-50, the proposed method has arrived at a competitive recall rate in terms of both subsets and averaged score. (3) The proposed method with one ResNet-50 is competitive with the ensembled methods, such as CLIP4Cir~\cite{CLIP4Cir} with 4$\times$ ResNet-50. 
In particular, ours with one ResNet-50 has arrived at $61.39\%$ Recall@50, surpassing CLIP4Cir ($59.03\%$ Recall@50). 
(4) We train our model ensemble with different initialization, and simply concatenate features, which also surpasses both CLVC-Net~\cite{CLVC-Net} and CLIP4Cir~\cite{CLIP4Cir}. 





We observe similar performance improvement on Shoes and Fashion200k in Table~\ref{tab:Recall rates on Shoes and fashion200k}. (1) The proposed method surpasses the baseline, yielding $18.41\%$ Recall@1, $53.63\%$ Recall@10, and $79.84\%$ Recall@50 on Shoes, and $21.8\%$ Recall@1, $52.1\%$ Recall@10, and $70.2\%$ Recall@50 on Fashion200k. Especially, in terms of Recall@50 accuracy, the model with uncertainty regularization has obtained $+3.38\%$ and $+2.40\%$ accuracy increase on Shoes and Fashion200k, respectively. 
(2) Based on one ResNet-50 backbone, the proposed method is competitive with ARTEMIS~\cite{delmas2022artemis} in both Shoes and Fashion200k, but ours are more efficient, considering that ARTEMIS needs to calculate the similarity score for every input triplets. 
(3) Our method also achieves better Recall@50 $79.84\%$ than CLVC-Net~\cite{CLVC-Net} $79.47\%$ with 2$\times$ ResNet-50 backbone, and arrives at competitive Recall@10 accuracy on Fashion200k.  

\section{Further Analysis and Discussions}
\noindent\textbf{Can Dropout replace the uncertainty regularization?} \label{dropout_vs_uncertainty}
No. Similar to our method, the dropout function also explicitly introduces the noise during training. There are also two primary differences: (1) Our feature fluctuation is generated according to the original feature distribution, instead of a fixed drop rate in dropout. (2) The proposed uncertainty regularization adaptively punishes the network according to the noise grade. 
To verify this point, we compare the results between ours and the dropout regularization. In particular, we add a dropout layer after the fully connected layer of the text encoder, which is before the compositor. 
For a fair comparison, we deploy the baseline ($\mathcal{L}_{\mathrm{info}}$) without modeling uncertainty for evaluation and show the results of different dropout rates in Table~\ref{table:different_uncertainty}. 
We observe that the dropout does not facilitate the model recalling more true candidates. No matter whether the drop rate is set as $0.2$ or $0.5$, the performance is close to the baseline without introducing the dropout layer. In contrast, the uncertainty regularization significantly improves the Recall@10 rate by $+4.15\%$ accuracy.
Besides, we re-implement Dou~\etal~\cite{dou2022reliability} by adding two extra branches to regress the feature mean and variance for loss calculation. It has achieved 50.14\% Recall@10 and 77.89 \% Recall@50 on Shoes, which is inferior to our method. 


\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|ccc}
\shline
Uncertainty & Drop Rate & R@10 & R@50 & Average\\
\shline
Baseline ($\mathcal{L}_{\mathrm{info}}$) & - & 49.48 & 76.46 & 62.97 \\
        $\mathcal{L}_{\mathrm{info}}$ + Dropout  & 0.2 & 49.74 & 76.37  & 63.06   \\
        $\mathcal{L}_{\mathrm{info}}$ + Dropout  & 0.5 & 49.00 & 75.83  & 62.42   \\
        Dou~\etal~\cite{dou2022reliability} & - &  50.14 &   77.89 & 64.01 \\ 
         Augment Source Feature $f_s^I$ & - & 52.20 & 77.75 & 64.98 \\
         Ours ($\mathcal{L}_{\mathrm{info}}$ + $\mathcal{L}_{\mathrm{u}}$) & - & \textbf{53.63} & \textbf{79.84} & \textbf{66.74}   \\ 


\shline
\end{tabular}}
\vspace{-.15in}
\caption{Ablation study of dropout on the Shoes dataset. We compare the impact of dropout rate and the proposed uncertainty regularization $\mathcal{L}_{\mathrm{u}}$. The Average denotes (R@10+R@50)/2. We could observe that dropout has limited impacts on the final retrieval performance. In contrast, the proposed $\mathcal{L}_{\mathrm{u}}$ shows significant recall accuracy boost.} 
\label{table:different_uncertainty}\vspace{-.1in}
\end{table}

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc|ccc}
    \shline
    \multirow{2}{*}{Scale} & \multicolumn{3}{c}{$w_2=1, w_1$} & \multicolumn{3}{|c}{$w_1=1, w_2$} \\\cline{2-7}
     & R@10 & R@50 & Average & R@10 & R@50 & Average\\
    \shline
        0.1 & 48.42  & 75.57  & 62.00  & 51.03  & 79.10  & 65.07   \\
        0.2 & 51.35  & 76.35  & 63.85  & 50.49  & 79.15  & 64.82   \\
        0.5 & 51.03  & 78.84  & 64.94  & 50.69  & 77.75  & 64.22   \\
        0.7 & 51.95  & 78.95  & 65.45  & 50.43  & 78.38  & 64.41   \\
        1   & \textbf{53.63}  & \textbf{79.84}  & \textbf{66.74}  & \textbf{53.63}  & \textbf{79.84}  & \textbf{66.74}   \\
        2   & 48.71  & 76.80  & 62.76  & 49.83  & 78.41  & 64.12   \\
        5   & 51.29  & 78.06  & 64.68  & 48.20  & 76.83  & 62.52   \\
        7   & 48.91  & 76.23  & 62.57  & 48.60  & 77.18  & 62.89   \\
        10  & 48.71  & 76.80  & 62.76  & 47.02  & 74.71  & 60.87   \\
    \shline
    \end{tabular}}
    \vspace{-.1in}
    \caption{The influence of $w_1,w_2$ to the model on the Shoes dataset. The average is (R@10+R@50)/2. The left side of the table shows the R@Ks of different values of $w_1$ when we fix $w_2 = 1$. Meanwhile, the right side of the table shows the R@Ks of different values of $w_2$ when we fix $w_1 = 1$. $w_1=w_2=1$ has the best performance and the same results in two sides. }
    \label{tab:ratio}\vspace{-.15in}
\end{table}

\begin{table}[t]
    \centering{\scriptsize
    \begin{tabular}{c|ccc}
    \shline 
    $\gamma_0$& R@10 & R@50 & Average\\\shline
    0.1 & 50.80 & 78.64 & 64.72 \\
    0.5 & 51.20 & 79.12 & 65.76 \\
    1 & \textbf{53.63} & \textbf{79.84} & \textbf{66.74} \\
    2 & 49.66 & 77.63 & 63.65 \\
    3 & 50.83 & 77.55 & 64.19 \\
    5 & 50.14 & 76.32 & 63.23 \\
    10 & 49.91 & 76.43 & 63.17\\
    +$\infty$ (baseline) & 49.48 & 76.46 & 62.97\\
    \shline
    \end{tabular}}
    \vspace{-.15in}
    \caption{The influence of $\gamma_0$ to the model on the Shoes dataset. The average is (R@10+R@50)/2.}
    \label{tab:gamma}\vspace{-.1in}
\end{table}

\begin{table}[t]
\scriptsize
    \centering{
    \begin{tabular}{c|ccc}
    \shline
    $\gamma$& R@10 & R@50 & Average\\\shline
    0.2 & 30.90 & 63.12 & 47.01 \\
    0.5 & 41.87 & 73.11 & 57.49 \\
    0.8 & 41.87 & 72.39 & 57.13 \\
    \shline
    \end{tabular}}\vspace{-.15in}
    \caption{Ablation Study on fixed $\gamma$ on the Shoes dataset.}
    \label{tab:fixgamma}\vspace{-.15in}
\end{table}

\noindent\textbf{Impact of the noise fluctuation.}
We study the impact of the noise fluctuation in the uncertainty modeling. In particular, we change the noise scale in Eq.~\ref{augment} by adjusting the scale of $\alpha$ and $\beta$. The modified formulation is as follows:\vspace{-2mm}
\begin{equation}
\hat{f}_t =  \alpha' \bar{f_t} + \beta' 
\vspace{-2mm}
\end{equation}
where $\alpha'\in N(1, w_1 \sigma)$ and $\beta'\in N(\mu,w_2 \sigma)$. $w_1, w_2$ are the scalars to control the noise scale. 
We show the impact of different noise scales in Table \ref{tab:ratio}. We fix one of the $w$ to 1 and change the another parameter.
We could observe two points: (1) As we expected, the identical noise setting $(w_1=1, w_2=1)$ achieves the best performance. It is because such noise is close to the original feature distribution and simulates the fluctuation in the training set. 
(2) The experiment results also show that our uncertainty regularization can tolerate large amplitude noise changes. Even if the training data contains lots of noise, the network is still robust and achieves reasonable performance. It is attributed to uncertainty regularization that punishes the network according to the noise grade.

\noindent\textbf{Parameter sensitivity of the balance weight $\gamma$.} 
As shown in Eq. \ref{total_loss}, $\gamma$ is a dynamic weight to help balance the fine- and coarse-grained retrieval. During training, we encourage the model gradually paying more attention to the fine-grained retrieval. According to $\gamma=\exp(-\gamma_0\cdot \frac{current\_epoch}{total\_epoch} ),$ we set different initial values of $\gamma_0$ to change the balance of the two loss functions. 
We evaluate the model on the Shoes dataset and report results in Table \ref{tab:gamma}. 
If $\gamma_0$ is close to $0$, the model mostly learns the uncertainty loss on the coarse-grained retrieval, and thus recall rate is still high. In contrast, if $\gamma_0$ is close to $+\infty$, the model only focuses on the fine-grained learning, and thus the model will converge to the baseline. Therefore, when deploying the model to the unseen environment, $\gamma_0 = 1$ can be a good initial setting. Besides, results with fixed $\gamma$ are shown in Table~\ref{tab:fixgamma}. The constant uncertainty loss drives the model to focus on coarse-grained matching, resulting in low recall rates as well.



\begin{figure}[t]
\begin{center}\vspace{-.1in}
\includegraphics[width=0.85\linewidth]{images/visualization.jpg}
\vspace{-.2in}
\caption{Qualitative image retrieval result on FashionIQ, Fashion200k and Shoes. We mainly compare the top-5 ranking list of proposed method with the baseline. (Please zoom in for better visualization.)}
\label{fig:visualization}
\vspace{-.2in}
\end{center}
\end{figure}


\noindent\textbf{Qualitative visualization.}
We show the top-5 retrieval results on FashionIQ, Fashion200k, and Shoes in Figure~\ref{fig:visualization}. 
(1) Compared with the baseline, our model captures more fine-grained keywords, like ``shiner''.
(2) The proposed method also recalls more candidate images with a consistent style. It also reflects that the proposed method is robust and provides a better user experience, since most websites display not only top-1 but also top-5 products. 

\noindent\textbf{Training Convergence.} As shown in Figure~\ref{fig:loss}, the baseline model (blue line) is prone to over-fit all labels, including the coarse-grained triplets. Therefore, the training loss converges to zero quickly. In contrast, the proposed method (orange line) also converges, but does not force the loss to be zero. Because we provide the second term in uncertainty regularization as Eq.~\ref{eq:uncertain}, which could serve as a loose term. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=1\linewidth]{images/loss2.pdf}
\vspace{-.2in}
\caption{The training loss of the proposed method and the baseline. The baseline model is prone to over-fit all triplets in a one-to-one matching style. In contrast, the proposed method would converge to one non-zero constant.}
\label{fig:loss}\vspace{-.2in}
\end{center}
\end{figure}

\noindent\textbf{Only Coarse Retrieval Evaluation.} We design an interesting experiment only to consider query pairs with ambiguous coarse descriptions, \ie, less than 5 words in the FashionIQ dress dataset (about 6,246 of 10,942 queries). Compared with the baseline model, our method improves $1.46\%$ Recall@10 rate and $3.34\%$ Recall@50 rate. The result verifies that our model can significantly improve coarse retrieval performance over baseline.

\noindent\textbf{Augment the source feature $f_s^I$.} We modify and add feature augments to the source image, but the Recall@10 rate decreases
1.43\% and the Recall@50 rate decreases 2.09\% on Shoes (see Table~\ref{table:different_uncertainty}). It is due to the conflict with the one-to-many matching. If we conduct the source feature augmentation, it will become to many-to-one matching. As the visual intuition in Figure~\ref{fig:space}, it is better to apply such augmentation on the target feature instead.

\begin{table}[t]
\scriptsize
    \centering{
    \begin{tabular}{l|cccccc}
    \shline
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{Dress} & \multicolumn{2}{c}{Shirt} & \multicolumn{2}{c}{Toptee}\\\cline{2-7}
    \multirow{2}{*}{} &  R@10 & R@50 & R@10 & R@50 & R@10 & R@50\\\shline
    Clip4Cir$^*$ & 28.60 & 54.75 & 33.25 & 56.32 & 35.22 & 59.15 \\
    Clip4Cir$^*$ + Ours & 30.15 & 57.32 & 35.47 & 58.64 & 36.94 & 61.75 \\
    \shline
    \end{tabular}}\vspace{-.1in}
    \caption{Complementary to Clip4Cir on FashionIQ. $^*$: We re-implemented the method with one ResNet50.} 
    \label{tab:complementary}\vspace{-.2in}
\end{table}

\noindent\textbf{Complementary to other works?} Yes. We re-implement the competitive method Clip4Cir~\cite{CLIP4Cir} in Table~\ref{tab:complementary}, and show our method is complementary, further improving the recall. 

\section{Conclusion}
In this work, we provide an early attempt at a unified learning approach to simultaneously modeling coarse- and fine-grained retrieval, which could provide a better user experience in real-world retrieval scenarios.
Different from existing one-to-one matching approaches, the proposed uncertainty modeling explicitly considers the uncertainty fluctuation in the feature space. The feature fluctuation simulates the one-to-many matching for the coarse-grained retrieval. The multi-grained uncertainty regularization adaptively modifies the punishment according to the fluctuation degree during the entire training process, and thus can be combined with the conventional fine-grained loss to improve the performance further. Extensive experiments verify the effectiveness of the proposed method on the composed image retrieval with text feedback, especially in terms of recall rate. In the future, we will continue to explore the feasibility of the proposed method on common-object retrieval datasets, domain adaptation between multiple datasets, and involving the knowledge graph~\cite{liu2020hyperbolic,sun2021dynamic}.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{references}
}

\begin{comment}
\begin{table*}[t]
\centering
    \begin{tabular}{c|c|cccccc|cc|cc}
    \shline
    \multirow{3}{*}{Model} & \multirow{3}{*}{Joint Datasets} & \multicolumn{6}{c}{FashionIQ} & \multicolumn{2}{|c}{\multirow{2}{*}{Shoes}} & \multicolumn{2}{|c}{\multirow{2}{*}{Average}} \\\cline{3-8}
     & & \multicolumn{2}{c}{Dress} & \multicolumn{2}{c}{Shirt} & \multicolumn{2}{c|}{Toptee} & & \\\cline{3-12} 
     & & R@10 & R@50 & R@10 & R@50 & R@10 & R@50 & R@10 & R@50 & R@10 & R@50\\
     \shline
     $\mathcal{L}_{\mathrm{info}}$ & $\checkmark$ & 24.16 & 51.50 & 26.39 & 51.16 & 30.34 & 58.98 & 45.27 & 70.73 & 31.54 & 58.09\\
     $\mathcal{L}_{\mathrm{info}}$ & $\times$ & 24.80 & 52.35 & 27.70 & 55.71 & 33.40 & 63.64 & 49.48 & 76.46 & 33.85 & 62.04\\   
     $\mathcal{L}_{\mathrm{info}} +\mathcal{L}_{\mathrm{u}}$ & $\checkmark$ & 25.20 & 51.74 & 25.79 & 53.29 & 31.54 & 59.72 & 45.39 & 73.54 & 31.98 & 59.57\\ 
     $\mathcal{L}_{\mathrm{info}} + \mathcal{L}_{\mathrm{u}}$ & $\times$ & 30.60 & 57.46 & 31.54 & 58.29 & 37.37 & 68.41 & 53.63 & 79.84 & 38.29 & 66.00\\
    \shline
    \end{tabular}
    \caption{The result on joint FashionIQ and Shoes. The Average denotes the mean of the corresponding R@K on different datasets.}
    \label{tab:multiple datasets}
\end{table*}
\end{comment}

\end{document}
