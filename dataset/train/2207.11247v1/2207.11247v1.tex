

\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage[misc]{ifsym}

\usepackage{booktabs}
\usepackage{multirow}
\newcommand{\keypoint}[1]{\noindent\textbf{#1}\quad}
\usepackage{booktabs}
\usepackage{floatrow}
\usepackage{float}
\floatstyle{plaintop}
\restylefloat{table}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
\usepackage{blindtext}

\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\JY}[1]{{\color{magenta}#1}}

\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \textsuperscript{\Letter}\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\makeatletter
\renewcommand\paragraph{
  \@startsection{paragraph} {4} {\z@} {.5em \@plus1ex \@minus.2ex} {-1.5em} {\normalfont\normalsize\bfseries} }
\makeatother


\usepackage[pagebackref=false, breaklinks=true,letterpaper=true, colorlinks,citecolor=citecolor, linkcolor=linkcolor, bookmarks=false]{hyperref}
\definecolor{citecolor}{HTML}{0071BC}
\definecolor{linkcolor}{HTML}{ED1C24}
\newcommand{\subfig}[1]{\textcolor{linkcolor}{-#1}}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}



\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} \def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\iid{i.i.d\onedot} \def\wolog{w.l.o.g\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother


\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{222}  \title{Panoptic Scene Graph Generation}

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\author{Jingkang Yang\inst{1} \and
Yi Zhe Ang\inst{1} \and
Zujin Guo\inst{1} \and\\
Kaiyang Zhou\inst{1} \and
Wayne Zhang\inst{2} \and
Ziwei Liu\inst{1}~\textsuperscript{\Letter}}
\authorrunning{J. Yang et al.}
\institute{S-Lab, Nanyang Technological University, Singapore\\
\email{\{jingkang001,yizhe.ang,gu00008,kaiyang.zhou,ziwei.liu\}@ntu.edu.sg}\and
SenseTime Research, Shenzhen, China\\
\email{wayne.zhang@sensetime.com}}
\maketitle
\begin{abstract}
Existing research addresses scene graph generation (SGG)---a critical technology for scene understanding in images---from a detection perspective, \ie, objects are detected using bounding boxes followed by prediction of their pairwise relationships. We argue that such a paradigm causes several problems that impede the progress of the field. For instance, bounding box-based labels in current datasets usually contain redundant classes like hairs, and leave out background information that is crucial to the understanding of context. In this work, we introduce \emph{panoptic scene graph generation (PSG)}, a new problem task that requires the model to generate a more comprehensive scene graph representation based on panoptic segmentations rather than rigid bounding boxes. A high-quality \emph{PSG dataset}, which contains 49k well-annotated overlapping images from COCO and Visual Genome, is created for the community to keep track of its progress. For benchmarking, we build four two-stage baselines, which are modified from classic methods in SGG, and two one-stage baselines called PSGTR and PSGFormer, which are based on the efficient Transformer-based detector, \ie, DETR. 
While PSGTR uses a set of queries to directly learn triplets, PSGFormer separately models the objects and relations in the form of queries from two Transformer decoders, followed by a prompting-like relation-object matching mechanism. In the end, we share insights on open challenges and future directions. We invite users to explore the PSG dataset on our project page \url{https://psgdataset.org/}, and try our codebase \url{https://github.com/Jingkang50/OpenPSG}. 
\end{abstract} \section{Introduction}
\label{sec:intro}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/fig1_2x2.pdf}
    \caption{\textbf{Scene graph generation (a. SGG task) \vs panoptic scene graph generation (b. PSG task)}. The existing SGG task in (a) uses bounding box-based labels, which are often inaccurate---pixels covered by a bounding box do not necessarily belong to the annotated class---and cannot fully capture the background information. In contrast, the proposed PSG task in (b) presents a more comprehensive and clean scene graph representation, with more accurate localization of objects and including relationships with the background (known as stuff), \ie, the trees and pavement.
    }
    \label{fig:psg_vs_vg}
\end{figure} 
The goal of scene graph generation (SGG) task is to generate a graph-structured representation from a given image to abstract out objects---grounded by bounding boxes---and their pairwise relationships~\cite{sggsurvey21arxiv,sggsurvey20tnnls}. 
Scene graphs aim to facilitate the understanding of complex scenes in images and has potential for a wide range of downstream applications, such as image retrieval~\cite{johnson2015image,schuster2015generating,qi2017online}, visual reasoning~\cite{aditya2018image,shi2019explainable}, visual question answering~(VQA)~\cite{hildebrandt2020scene}, image captioning~\cite{gao2018image,chen2020say}, structured image generation and outpainting~\cite{johnson2018image,dhamo2020semantic,yang2022scene}, and robotics~\cite{gadre2022continuous,amiri2022reasoning}.

Since the introduction of SGG~\cite{johnson2015image}, this problem has been addressed from a detection perspective, \ie, using bounding boxes to detect objects followed by the prediction of their pairwise relationships~\cite{vg17ijcv,xu2017scene,liang2019vrr}. We argue that such a bounding box-based paradigm is not ideal for solving the problem, and would instead cause a number of issues that impede the progress of the field. Firstly, bounding boxes---as labeled in current datasets~\cite{vg17ijcv}---only provide a coarse localization of objects and often contain noisy/ambiguous pixels belonging to different objects or categories (see the bounding boxes of the two persons in Fig.~\ref{fig:psg_vs_vg}\subfig{-a}). Secondly, bounding boxes typically cannot cover the full scene of an image. For instance, the pavement region in Fig.~\ref{fig:psg_vs_vg}\subfig{-a} is crucial for understanding the context but is completely ignored. Thirdly, current SGG datasets often include redundant classes and information like \texttt{woman-has-hair} in Fig.~\ref{fig:psg_vs_vg}\subfig{-a}, which is mostly deemed trivial~\cite{liang2019vrr}. Furthermore, inconsistent and redundant labels are also observed in current datasets, \eg, the trees and benches in Fig.~\ref{fig:psg_vs_vg}\subfig{-a} are labeled multiple times, and some extra annotations do not contribute to the graph (see isolated nodes). Using such labels for learning might confuse the model.

Ideally, the grounding of objects should be clear and precise, and a scene graph should not only focus on salient regions and relationships in an image but also be comprehensive enough for scene understanding. We argue that as compared to bounding boxes, panoptic segmentation~\cite{panopticsegmentation} labels would be a better choice for constructing scene graphs. To this end, we introduce a new problem, \emph{panoptic scene graph generation}, or PSG, with a goal of generating scene graph representations based on panoptic segmentations rather than rigid bounding boxes.

To help the community keep track of the research progress, we create a new PSG dataset based on COCO~\cite{lin2014microsoft} and Visual Genome (VG)~\cite{vg17ijcv}, which contains 49k well-annotated images in total. We follow COCO's object annotation schema of 133 classes; comprising 80 thing classes and 53 stuff (background) classes. To construct predicates, we conduct a thorough investigation into existing VG-based datasets, \eg, VG-150~\cite{xu2017scene}, VrR-VG~\cite{liang2019vrr} and GQA~\cite{hudson2019gqa}, and summarize 56 predicate classes with minimum overlap and sufficient coverage of semantics. See Fig.~\ref{fig:psg_vs_vg}\subfig{-b} for an example of our dataset. From Fig.~\ref{fig:psg_vs_vg}, it is clear that the panoptic scene graph representation---including both panoptic segmentations and the scene graph---is much more informative and coherent than the previous scene graph representation.

For benchmarking, we build four two-stage models by integrating four classic SGG methods~\cite{xu2017scene,zellers2018neural,tang2018vctree,suhail2021energybased} into a classic panoptic segmentation framework~\cite{kirillov2019panoptic}. We also turn DETR~\cite{detr}, an efficient Transformer-based detector, into a one-stage PSG model dubbed as PSGTR, which has proved effective for the PSG task. We further provide another one-stage baseline called PSGFormer, which extends PSGTR with two improvements: 1) modeling objects and relations separately in the form of queries within two Transformer decoders, and 2) a prompting-like interaction mechanism. A comprehensive comparison of one-stage models and two-stage models is discussed in our experiments.

In summary, we make the following contributions to the SGG community:
\begin{itemize}
  \item \textbf{A New Problem and Dataset}: We discuss several issues with current SGG research, especially those associated with existing datasets. To address them, we introduce a new problem that combines SGG with panoptic segmentation, and create a large PSG dataset with high-quality annotations.
  \item \textbf{A Comprehensive Benchmark}: We build strong two-stage and one-stage PSG baselines, and evaluate them comprehensively on our new dataset, so that the PSG task is solidified in its inception.
  We find that one-stage models, despite having a simplified training paradigm, have great potential for PSG as it achieves competitive results on the dataset.
\end{itemize}

 \section{Related Work}
\label{sec:related_work}
\paragraph{Scene Graph Generation}
Existing scene graph generation (SGG) methods have been dominated by the two-stage pipeline that consists of object detection and pairwise predicate estimation. Given bounding boxes, early work predicts predicates using conditional random fields~\cite{johnson2015image,dai2017detecting} or casts predicate prediction into a classification problem~\cite{zhang2017relationship,kolesnikov2019detecting,qi2019attentive}. Inspired by knowledge graph embeddings, VTransE~\cite{zhang2017visual} and UVTransE~\cite{hung2020contextual} are proposed for explicit predicate modeling. Follow-up works have investigated various variants based on, \eg, RNN and graph-based modeling~\cite{xu2017scene,zellers2018neural,tang2018vctree,yang2018graph,lin2020gps,chen2019knowledge,li2018factorizable}, energy-based models~\cite{suhail2021energybased}, external knowledge~\cite{tang2018vctree,gu2019scene,zareian2020bridging,ZareianWYC20,lu2016visual}, and more recently language supervision~\cite{zhong2021learning,ye2021linguistic}. Recent research has shifted the attention to problems associated with the SGG datasets, such as the long-tailed distribution of predicates~\cite{tang2020unbiased,desai2021learning}, excessive visually-irrelevant predicates~\cite{liang2019vrr}, and inaccurate localization of bounding boxes~\cite{khandelwal2021segmentation}. In particular, a very recent study~\cite{khandelwal2021segmentation} shows that training an SGG model to simultaneously generate scene graphs and predict semantic segmentation masks can bring about improvements, which inspires our research. In our work, we study panoptic segmentation-based scene graph generation in a more systematic way by formulating a new problem and building a new benchmark.
We also notice that a closely-related topic human-object interaction (HOI)~\cite{gupta2015visual} shares a similar goal with SGG, \ie, to detect prominent relations from the image. However, the HOI task restricts the model to only detect human-related relations while ignoring the valuable information between objects that is often critical to comprehensive scene understanding. Nevertheless, many HOI methods are applicable to SGG tasks~\cite{gkioxari2018detecting,kato2018compositional,chao2018learning,wang2019deep,li2019transferable,zhou2020cascaded,wang2020learning,hou2020visual,li2020detailed,gao2020drg,kim2020uniondet,liu2020amplifying,tamura2021qpic,hou2021affordance,zhang2021mining,wang2022learning,zhang2022efficient}, and some of them have inspired our PSG baselines~\cite{kim2021hotr,zou2021end}.

\paragraph{Scene Graph Datasets}
\begin{figure*}[!t]\RawFloats
\captionsetup[table]{position=top}
\begin{minipage}{0.7\textwidth}
\captionof{table}{\textbf{Comparsion between classic SGG datasets and PSG dataset.} \#PPI counts predicates per image. DupFree checks whether duplicated object groundings are cleaned up. Spvn indicates whether the objects are grounded by bounding boxes or segmentations.
}
\label{tab:dataset_comparison}
\centering
\setlength\tabcolsep{2.5pt}

\resizebox{\textwidth}{!}{
\begin{tabular}{@{\hskip 0.05in}l@{\hskip 0.05in}|ccccccc@{\hskip 0.05in}}
\toprule
Dataset &\#Image &\#ObjCls &\#RelCls  & \#PPI & DupFree & Spvn & Source \\ \midrule
VG~\cite{vg17ijcv}         & 108K &  34K   &  40K     &  21.4   & \xmark  & BBox  & COCO \& Flickr\\ 
VG-150~\cite{xu2017scene}  & 88K  &  150   &   50      &  5.7   & \xmark  & BBox  & Clean VG \\ 
VrR-VG~\cite{liang2019vrr} & 59K  &  1,600 &  117      &  3.4   & \xmark  & BBox  & Clean VG \\ 
GQA~\cite{hudson2019gqa}   & 85K  &  1,703 &  310      &  50.6  & \cmark   & BBox & Re-annotate VG \\ 
\midrule
\textbf{PSG}               & 49K  & 133 &  56     &  5.7    & \cmark & Seg  & Annotate COCO\\ \bottomrule
\end{tabular}}
\end{minipage}
\hspace{6mm}
\begin{minipage}{0.23\textwidth}
\centering
\includegraphics[width=\linewidth]{figs/predicate_wordcloud.png}

\caption{Word Cloud for PSG Predicates.}
\label{fig:predicates}
\end{minipage}
\end{figure*}
 While early SGG works have constructed several smaller-size datasets such as RW-SGD~\cite{johnson2015image}, VRD~\cite{lu2016visual}, and UnRel-D~\cite{peyre2017weakly}, the large-scale Visual Genome (VG)~\cite{vg17ijcv} quickly became the standard SGG dataset as soon as it was released in 2017, prompting subsequent work to research in a more realistic setting. 
However, several critical drawbacks of VG were raised by the community, and therefore, some VG variants were gradually introduced to address some problems.
Notice that VG contains an impractically large number of 33,877 object classes and 40,480 predicate classes, leading VG-150~\cite{xu2017scene} to only keep the most frequent 150 object classes and 50 predicate classes for a more realistic setting. 
Later, VrR-VG~\cite{liang2019vrr} argued that many predicates in VG-150 can be easily estimated by statistical priors, hence deciding to re-filter the original VG categories to only keep visually relevant predicate classes.
However, by scrutinizing VrR-VG, we find many predicates are redundant (\eg, \texttt{alongside}, \texttt{beside}, \texttt{besides}) and ambiguous (\eg, \texttt{beyond}, \texttt{down}).
Similar drawbacks appeared in another large-scale dataset with scene graph annotations called GQA~\cite{hudson2019gqa}.
In summary, although relations play a pivotal role in SGG tasks, a systematic definition of relations is unfortunately overlooked across all the existing SGG datasets.
Therefore, in our proposed PSG dataset, we consider both comprehensive coverage and non-overlap between words, and carefully define a predicate dictionary with 56 classes to better formulate the scene graph problem. Fig.~\ref{fig:predicates} shows the word cloud of the predicate classes, where font size indicates frequency.

Apart from the problem of predicate definition, another critical issue of SGG datasets is that they all adopt bounding box-based object grounding, which inevitably causes a number of issues such as coarse localization (bounding boxes cannot reach pixel-level accuracy), inability to ground comprehensively (bounding boxes cannot ground backgrounds), tendency to provide trivial information (current datasets usually capture objects like \texttt{head} to form the trivial relation of \texttt{person-has-head}), and duplicate groundings (the same object could be grounded by multiple separate bounding boxes). These issues together have caused the low-quality of current SGG datasets, which impede the progress of the field.
Therefore, the proposed PSG dataset tries to address all the above problems by grounding the images using accurate and comprehensive panoptic segmentations with COCO's appropriate granularity of object categories.
Table~\ref{tab:dataset_comparison} compares the statistics of the PSG dataset with classic SGG datasets.

\paragraph{Panoptic Segmentation}
The panoptic segmentation task unifies semantic segmentation and instance segmentation~\cite{panopticsegmentation} for comprehensive scene understanding, and the first approach is a simple combination of a semantic segmentation model and an instance segmentation model to produce stuff masks and thing masks respectively~\cite{panopticsegmentation}. Follow-up work, such as Panoptic FPN~\cite{kirillov2019panoptic} and UPSNet~\cite{Xiong2019UPSNetAU}, aim to unify the two tasks in a single model through multi-task learning to achieve gains in compute efficiency and segmentation performance. Recent approaches (e.g., MaskFormer~\cite{Cheng2021MaskFormer}, Panoptic Segformer~\cite{Li2021PanopticS} and K-Net~\cite{zhang2021knet}) have turned to more efficient architectures based on Transformers like DETR~\cite{detr}, which simplifies the detection pipeline by casting the detection task as a set prediction problem.
 \section{Problem and Dataset}
\label{sec:problem_dataset}

\keypoint{Recap: Scene Graph Generation}
We first briefly review the goal of the classic scene graph generation (SGG) task, which aims to model the distribution:

where  is the input image, and  is the desired scene graph which comprises the bounding boxes  and labels  that correspond to each of the  objects in the image, and their relations in the set . 
More specifically,  represents the box coordinates,  
and  
belong to the set of all object and relation classes.

\medskip

\keypoint{Panoptic Scene Graph Generation} Instead of localizing each object by its bounding box coordinates, the new task of panoptic scene graph generation (PSG task) grounds each object with the more fine-grained panoptic segmentation. For conciseness, we refer to both objects and background as objects.

Formally, with panoptic segmentation, an image is segmented into a set of masks , where . Each mask is associated with an object with class label . A set of relations  between objects are also predicted. The masks do not overlap, \ie, . Hence, PSG task models the following distribution:





\paragraph{PSG Dataset}
To address the PSG task, we build our PSG dataset following these three major steps. Readers can find more details in the Appendix.

\noindent\textbf{Step 1: A Coarse COCO \& VG Fusion:} 
To create a dataset with both panoptic segmentation and relation annotations, we use the 48,749 images in the intersection of the COCO and VG datasets with an automatic but coarse dataset fusion process.
Specifically, we use an object category matching procedure to match COCO's segmentations with VG's bounding boxes, so that part of VG's predicates are applied to COCO's segmentation pairs.
Due to the inherent mismatch between the label systems and localization annotations of VG and COCO, the auto-generated dataset is very noisy and requires further cleaning.

\noindent\textbf{Step 2: A Concise Predicate Dictionary:}
Inspired by the appropriate granularity of COCO categories~\cite{lin2014microsoft}, we carefully identify 56 salient relations by taking reference from common predicates in the initial noisy PSG dataset and all VG-based datasets including VG-150~\cite{xu2017scene}, VrR-VG~\cite{liang2019vrr} and GQA~\cite{hudson2019gqa}.
The selected 56 predicates are maximally independent (\eg, we only keep ``\texttt{over/on}'' and do not have ``\texttt{under}'') and cover most common cases in the dataset.

\noindent\textbf{Step 3: A Rigorous Annotation Process:}
Building upon the noisy PSG dataset, we require the annotators to
\emph{1) filter} out incorrect triplets, and
\emph{2) supplement} more relations between not only object-object, but also object-background and background-background pairs, using the predefined 56 predicates.
To prevent ambiguity between predicates, we ask the annotators strictly not to annotate using general relations like \texttt{on}, \texttt{in} when a more precise predicate like \texttt{parked on} is applicable.
With this rule, the PSG dataset allows the model to understand the scene more precisely and saliently.

\noindent\textbf{Quality Control:} The PSG dataset goes through a professional dataset construction process. The main authors first annotated 1000 images to construct a detailed documentation (available in project page), and then employed a professional annotation company (sponsored by SenseTime) to annotate the training set within a month (US\\mathcal{T}_i\in\mathbb{Q}^\text{T}\mathcal{G}\mathcal{T}_i\ddot{\mathcal{T}}_i^\text{S}\ddot{\mathcal{T}}_i^\text{R}\ddot{\mathcal{T}}_i^\text{O}\tilde{\mathcal{T}}_i^\text{S}\tilde{\mathcal{T}}_i^\text{O}\mathbf{C}_\text{tm}\mathbf{C}_\text{cls}\mathbf{C}_\text{seg}\sigma\mathcal{T}_i\in\mathbb{Q}^\text{T}\mathbb{Q}^\text{T}|\mathbb{Q}^\text{T}|\mathcal{L}_\text{total}\mathcal{L}_\text{cls}\mathcal{L}_\text{cls}\mathcal{R}_i\in\mathbb{Q}^\text{R}\mathcal{O}_i\in\mathbb{Q}^\text{O}\mathcal{R}_i\in\mathbb{Q}^{R}f_{\text{S}}f_{\text{O}}\mathbb{S}ii\mathcal{R}_i\mathbb{O}\mathbb{S}\mathbb{O}i\mathcal{S}_ii\mathcal{O}_ii\mathcal{R}_i\mathbb{T}\mathcal{L}_\text{total}-^\dagger^\dagger^\dagger^\dagger^\dagger10^{-4}10^{-4}10^{-5}$. For initialization, we directly use COCO pretrained DETR to initialize the weights of our backbone and transformer.  Besides, we also generally follow DETR’s data augmentation which does cropping and resizing operations with settings such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333. However, it should be noted that when cropping images, we also filter the ground truth of bounding boxes and relations pairs that might be cropped. We train our model for 60 epochs with a step scheduler at epoch 40, and it finally takes us around 2 days to train on eight V100 GPUs with batch size 1.




\subsection{PSGFormer}
PSGFormer is built on the baseline of PSGTR so that most of the training details are shared.
In detail, PSGFormer also implements each FFN by a 3-layer MLP, and each panoptic head by a multi-head attention layer and a 6-layer FPN-like CNN as DETR does. 
Besides, the number of object queries and relation queries are set as 100. 
We follow the training hyperparameters of PSGTR including optimizer, learning rate, data augmentation, \etc.
Notice that PSGFormer also has an auxiliary task of pure panoptic segmentation with object decoder, the ratio between the main task on triplet supervision and the auxiliary panoptic segmentation supervision is 5 to 1.
We train our model for 60 epochs, taking around 2.5 days to train on eight V100 GPUs with batch size 1.



\section{Visualization of PSGTR Result Triplet-by-Triplet}
\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/psgtr_vis.pdf}
    \caption{\textbf{Visualization of PSGTR Result Triplet-by-Triplet.} 
    PSGTR uses triplet queries to directly predict subject / object masks in the triplets, and the subject / object across triplets are not dependent, so the panoptic segmentation visualization is chaos in Fig.~\textcolor{linkcolor}{6}.
    However, if we visualize PSGTR result triplet-by-triplet, the result looks good.
    }
    \label{fig:psgtr_vis}
\end{figure}
Fig.~\ref{fig:psgtr_vis} shows PSGTR's predict results in a triplet-by-triplet fashion, as a complementary to the lower example in Fig.~\textcolor{linkcolor}{6}.
Notice that panoptic segmentation visualization is chaos in Fig.~\textcolor{linkcolor}{6}.
It is because PSGTR uses triplet queries to directly predict subject / object masks in the triplets, and the subject / object across triplets are not dependent, and the re-identification of each subject / object is no-trivial, and we use a simple post-processing method of pixel-wise argmax function to merge the segments, but it will still split one object into parts. However, it does not mean that PSGTR cannot segment objects well when predicting triplets. As we visualize the PSGTR result triplet-by-triplet in Fig.~\ref{fig:psgtr_vis}, the result looks good.


 \end{document}
