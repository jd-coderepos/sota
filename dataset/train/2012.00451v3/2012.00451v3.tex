In this Appendix, we start by giving additional  analysis and examples of our proposed \datasetname{} dataset in Section \ref{sec:sqadata}.
We, then,  provide additional architecture details for our VideoQA model in Section \ref{sec:mmt}.
Next, we present additional statistics and details of the collection procedure for our manually collected \smalldatasetname{} evaluation benchmark in Section \ref{sec:ivqadata}.
We describe additional implementation details in Section \ref{sec:detailsbis} and present experiments including cross-dataset transfer, results per answer quartile and per question type in Section \ref{sec:expbis}.



\section{Analysis of \datasetname{} dataset}
\label{sec:sqadata}

Figure \ref{fig:sqa_length} shows the statistics of the \datasetname{} dataset in terms of the question length, answer length and video clip duration. 
Overall, \datasetname{} contains longer answers than downstream open-ended VideoQA datasets like MSRVTT-QA, MSVD-QA or ActivityNet-QA.
The distribution of clip duration has a peak at around seven seconds with a long tail of longer clips.  These statistics demonstrate the diversity of our \datasetname{} dataset, both in terms of videos and answers.

Word clouds\footnote{To generate the word clouds, we used \url{https://github.com/amueller/word_cloud}.} for questions and answers in \datasetname{} are shown in Figure~\ref{fig:sqa_words} and illustrate the diverse vocabulary in \datasetname{} as well as the presence of speech-related words such as as \textit{okay}, \textit{right}, \textit{oh}. 
In Figure~\ref{fig:sqa_add} we illustrate the diversity and the noise in the automatically obtained annotations in the \datasetname{} dataset.

We show quantitative comparisons of our question-answer generation models with~\cite{heilman2010good} in Section \ref{sec:java}, and supplement it here with a qualitative comparison shown in Figure~\ref{fig:java}. 
We found that compared to~\cite{heilman2010good} our generation method provides higher quality as well as higher diversity of question-answer pairs when applied to the uncurated sentences extracted from speech in narrated videos.


In Section \ref{sec:\datasetname{}} we present a manual evaluation of the quality of the automatically generated video-question-answer triplets for our method and two other baselines. We complement this analysis here with inter-rater agreement statistics.
For the 300 generated video-question-answer triplets (100 for each generation method), 94 were in an agreement of all 5 annotators, 198 in an agreement of at least 4 annotators, and 299 in an agreement of at least 3 annotators. 
This high agreement of annotators demonstrates the reliability of the results in Table \ref{table:manual}. 

We further manually classify the 100 video-question-answer triplets obtained with our method by the question type (``Attribute", ``Object", ``Action", ``Counting", ``Place", ``People", or ``Other"), evaluate the quality of generated triplets for different question types and report results in Table~\ref{table:manualsplit}.
Out of the 6 most common categories, we observe that questions related to ``Action" lead to the best annotations, ``Counting" questions lead to the highest number of QAs unrelated to the video content, and questions related to ``Place" lead to the highest number of QA generation errors. 
Qualitatively, we found that actions are often depicted in the video, while counted quantities (\eg time, weight, length) mentioned in the speech are hard to guess from the video only. 

\begin{figure}[t]
\centering
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{figures/sqa_lengths.pdf}
\caption{Question and answer length}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{figures/sqa_durations.pdf}
\caption{Clip duration}
\end{subfigure}
\vspace{-0.3cm}
\caption{{\bf Statistics of the \datasetname{} dataset.} (a)~Distribution of length of questions and answers. (b)~Distribution of video clip duration in seconds.}
\label{fig:sqa_length}
\vspace{-0.3cm}
\end{figure}

\begin{table}[t]
\begin{center}
\setlength\tabcolsep{1.5pt}
\resizebox{0.75\linewidth}{!}{\begin{tabular}{ll|ccc}
\makecell{ \small{Question} \\ \small{Type}} & Total &
\makecell{ \small{Correct} \\ \small{Samples (\%)}} & \makecell{ \small{QA Generation} \\ \small{Failure (\%)}} & \makecell{ \small{QA unrelated} \\ \small{to video (\%)}} \\ \hline
Attribute & 25 & 28 & 32 & 40 \\
Object & 17 & 41 & 24 & 35 \\
Action & 16 & \textbf{69} & 19 & 13 \\
Counting & 13 & 23 & 15 & \textbf{62} \\
Place & 7 & 0 & \textbf{86} & 14 \\
People & 7 & 0 & 43 & 57 \\
Other & 15 & 13 & 27 & 60 \\
\end{tabular}}
\end{center}
\vspace{-0.4cm}
\caption{\small Manual evaluation of our video-question-answer generation method on 100 randomly chosen generated examples split by question type. Results are obtained by majority voting among 5 annotators.}
\vspace{-0.6cm}
\label{table:manualsplit}
\end{table} 

\begin{figure*}[t]
\centering
\begin{subfigure}{0.8\textwidth}
\includegraphics[width=\linewidth]{figures/sqa_answers_cloud.pdf}
\caption{Answers}
\end{subfigure}
\begin{subfigure}{0.8\textwidth}
\includegraphics[width=\linewidth]{figures/sqa_questions_cloud.pdf}
\caption{Questions}
\end{subfigure}
\caption{Word clouds extracted from the \datasetname{} dataset showing its diverse vocabulary and the words characteristic to speech such as \textit{okay}, \textit{right}, or \textit{ok}.}
\label{fig:sqa_words}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{figures/heilman.pdf}
\caption{Qualitative examples of video-question-answer triplets generated with our trained language models compared to Heilman \etal~\cite{heilman2010good}, illustrating the higher quality and diversity of triplets obtained with our generation method.}
\label{fig:java}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/sqasupmat.pdf}
\caption{Additional examples of videos, questions and answers from our automatically generated \datasetname{} dataset. These examples illustrate the large data diversity in \datasetname{}. {\color{green}The green color} indicates relevant examples, {\color{orange}the orange color} (penultimate row) indicates a failure of the question-answer generation, and {\color{red}the red color} (last row) indicates that the generated question-answer is unrelated to the visual content.}
\label{fig:sqa_add}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=.8\linewidth]{figures/modelsupmat.pdf}
\vspace{-0.2cm}
\caption{{\bf VideoQA architecture overview.} Our model is composed of a video-question module  based on a multi-modal transformer (top) and an answer module  based on DistilBERT \cite{sanh2019distilbert} encoder (bottom).}
\vspace{-0.5cm}
\label{fig:archappendix}
\end{figure*}



\section{VideoQA architecture}\label{sec:mmt}
Our architecture, shown in Figure \ref{fig:archappendix}, has two main modules: (i) a video-question multi-modal transformer (top) and (ii) an answer transformer (bottom). Details are given next, and further implementation details are given in Section \ref{sec:detailsbis}.

\begin{figure}[t]
\centering
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{figures/ivqa_acloud.pdf}
\caption{Answers}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{figures/ivqa_qcloud.pdf}
\caption{Questions}
\end{subfigure}
\vspace{-0.31cm}
\caption{Word clouds for our \smalldatasetname{} dataset illustrate a vocabulary related to the domains of cooking, hand crafting, or gardening. The frequent  occurrence of location and time-specific words (\textit{behind}, \textit{front}, \textit{right}, \textit{left}, \textit{first}, \textit{end}, \textit{beginning}) indicate the presence of the spatial and temporal context within \smalldatasetname{} questions.}
\vspace{-0.6cm}
\label{fig:words}
\end{figure}

\begin{figure}[t]
\centering
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{figures/ivqa_lengths.pdf}
\caption{Question and answer length}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{figures/ivqa_durations.pdf}
\caption{Clip duration}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{figures/ivqa_starts.pdf}
\caption{Clip start time in the original video}
\end{subfigure}
\vspace{-.3cm}
\caption{{\bf Statistics of the \smalldatasetname{} dataset.} (a)~Distribution of length of questions and answers. (b)~Distribution of video clip duration in seconds. (c)~Distribution of video clip relative start time in the original video.}
\label{fig:length}
\vspace{-.6cm}
\end{figure}

\noindent \textbf{Video-question multi-modal transformer.} The input video representation, obtained from a fixed S3D model~\cite{xie2018rethinking}, is composed of  features denoted   where  is the dimension of the video features, and  is the number of extracted features, one per second. 
The contextualized representation of the question, provided by the DistilBERT model~\cite{sanh2019distilbert}, is composed of  token embeddings denoted as  where  is the dimension of the DistilBERT embedding and  is the number of tokens in the question. 
The inputs to our video-question multi-modal transformer are then defined as a concatenation of question token embeddings and video features
 
where 
 
and 

where , , ,  and learnable parameters,  and  are learnt modality encodings for video and question, respectively, and  are fixed sinusoidal positional encodings.  is a Gaussian Error Linear Unit~\cite{hendrycks2016gaussian} followed by a Layer Normalization~\cite{ba2016layer} and  refers to Dropout~\cite{srivastava2014dropout}.

\begin{figure*}[t]
\centering
\begin{subfigure}{0.95\textwidth}
\includegraphics[width=\linewidth]{figures/qcollec.pdf}
\caption{Collection interface for questions. Note that the answer provided by the question annotator is only used to ensure that the provided question follows the given instructions, but is not included in \smalldatasetname{}. Answers are collected separately, see Figure \ref{fig:acollec}.}
\label{fig:qcollec}
\end{subfigure}
\begin{subfigure}{0.95\textwidth}
\includegraphics[width=\linewidth]{figures/acollec.pdf}
\caption{Collection interface for answers. Five different answer annotators provide an answer annotation for each collected question.}
\label{fig:acollec}
\end{subfigure}
\vspace{-0.2cm}
\caption{Amazon Mechanical Turk interfaces for collecting questions (Figure \ref{fig:qcollec}) and answers (Figure \ref{fig:acollec}) for the \smalldatasetname{} dataset. For readability, the videos shown in these Figures are shrinked, and only one annotation example is shown.}
\vspace{-0.5cm}
\label{fig:collec}
\end{figure*}

The multi-modal transformer is a transformer with  layers,  heads, dropout probability , and hidden dimension . The outputs of the multi-modal transformer  are contextualized representations over tokens in the question and temporal video representations. Finally, the fused video-question embedding  is obtained as 

where ,  are learnable parameters and  is the multi-modal contextualized embedding of the [CLS] token in the question, as shown in Figure~\ref{fig:archappendix}.

\noindent \textbf{Answer transformer.} The contextualized representation of the answer, provided by the DistilBERT model~\cite{sanh2019distilbert}, is composed of  token embeddings denoted as  where  is the dimension of the DistilBERT embedding and  is the number of tokens in the answer. 
Our answer embedding  is then obtained as

where ,  are learnable parameters and  is the contextualized embedding of the [CLS] token in the answer, as shown in Figure~\ref{fig:archappendix}.



\section{Details of the \smalldatasetname{} dataset}
\label{sec:ivqadata}

\subsection{Data Collection}
The Amazon Mechanical Turk interfaces used for collecting the question and answer annotations, are shown in Figure \ref{fig:collec}.  
An emphasis was placed on collecting visually grounded questions about objects and scenes that could not be easily guessed without watching the video, and collecting short answers in order to maximize the chance for consensus between annotators, \ie, having multiple annotators giving exactly the same answer.

\begin{table*}[t]
\setlength\tabcolsep{4.5pt}
\begin{center}
\resizebox{0.95\linewidth}{!}{	
\begin{tabular}{c|cccc|cccc}
Pretraining Data & \multicolumn{4}{c}{Zero-shot} & \multicolumn{4}{c}{Finetune} \\
& \smalldatasetname{} & \small{MSRVTT-QA} & \small{ActivityNet-QA} & How2QA 
& \smalldatasetname{} & \small{MSRVTT-QA} & \small{ActivityNet-QA} & How2QA \\ 
\hline
 & --- & --- & --- & ---
& 23.0 & 39.6 & 36.8 & 80.8 \\
MSRVTT-QA 
& 8.6 & --- & 1.7 & 42.5
& 25.2 & --- & 37.5 & 80.0 \\
ActivityNet-QA & 5.5 & 2.7 & --- & 40.8
& 24.0 & 39.9 & --- & 80.7 \\
\hline
\datasetname{} & \textbf{12.2} & \textbf{2.9} & \textbf{12.2} & \textbf{51.1}
& \textbf{35.4} & \textbf{41.5} & \textbf{38.9} & \textbf{84.4} \\
\end{tabular}
}
\vspace{-0.3cm}
\caption{\small Comparison of our training on \datasetname{} with cross-dataset transfer using the previously largest open-ended VideoQA dataset (MSRVTT-QA) and the largest manually annotated open-ended VideoQA dataset (ActivityNet-QA).}
\vspace{-0.5cm}
\label{table:transfer}
\end{center}
\end{table*}

\begin{table*}[t]
\centering
\setlength\tabcolsep{7pt}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{cc|cccc|cccc|cccc}
Pretraining Data & Finetuning
& \multicolumn{4}{c}{MSRVTT-QA} 
& \multicolumn{4}{c}{MSVD-QA} 
& \multicolumn{4}{c}{ActivityNet-QA} \\
& & Q1 & Q2 & Q3 & Q4 & Q1 & Q2 & Q3 & Q4 & Q1 & Q2 & Q3 & Q4 \\
\hline
& \cmark & 
\textbf{68.4} & 44.1 & 32.9 & 8.1 &
71.2 & 53.7 & 28.9 & 8.8 &
65.6 & 49.0 & 25.7 & 3.9 \\
HowTo100M & \cmark & 
65.2 & 46.4 & 34.9 & 10.6 &
\textbf{74.8} & 58.8 & 30.6 & 10.5 &
\textbf{67.5} & \textbf{53.3} & 25.9 & 4.1 \\
\datasetname{} & \xmark & 
0.2 & 6.4 & 2.4 & 3.0 &
9.3 & 9.0 & 6.9 & 4.8 &
36.3 & 5.7 & 3.7 & 1.5 \\
\datasetname{} & \cmark & 
66.9 & \textbf{46.9} & \textbf{36.0} & \textbf{11.5} &
74.7 & \textbf{59.0} & \textbf{35.0} & \textbf{14.1} &
66.3 & 53.0 & \textbf{28.0} & \textbf{5.0} \\
\end{tabular}
}
\vspace{-0.3cm}
\caption{\small Results of our \textit{\vqat{}} model with different training strategies, on subsets of MSRVTT-QA, MSVD-QA and ActivityNet-QA, corresponding to four quartiles with Q1 and Q4 corresponding to samples with the most frequent and the least frequent answers, respectively.}
\vspace{-0.5cm}
\label{table:rareall}
\end{table*}


\subsection{Statistical Analysis}
Word clouds for questions and answers in \smalldatasetname{}, shown in Figure~\ref{fig:words}, demonstrate the relation of \smalldatasetname{} to the domains of cooking, hand crafting and gardening.
These word clouds also indicate that questions in \smalldatasetname{} often require spatial reasoning (\textit{behind}, \textit{front}, \textit{right}, \textit{left}) and temporal understanding (\textit{first}, \textit{end}, \textit{left}, \textit{beginning}) of the video. 
The most frequent answer (\textit{spoon}) in \smalldatasetname{} corresponds to 2\% of all answers in the dataset. 
In contrast, the most frequent answers in other VideoQA datasets account for more than 9\% of all answers in these datasets (we have verified this for MSRVTT-QA, MSVD-QA and ActivityNet-QA). 
As a consequence, the \textit{most frequent answer baseline} is significantly lower for our \smalldatasetname{} dataset compared to other VideoQA datasets.
Figure \ref{fig:length} shows the distributions of question length, answer length, clip duration and clip relative start time in the original video. Clip duration and start time distributions are almost uniform because we randomly sampled them to obtain the clips, which results in a high video content diversity.
Answers are in great majority one or two words as a result of our collection procedure.

We observe that 27.0\% of questions lead to a perfect consensus among the five answer annotators, 48.4\% of questions lead to a consensus among at least four annotators, and 77.3\% lead to a consensus among at least three annotators, while only six questions do not lead to a consensus between at least two annotators, justifying the defined accuracy metric. 
Additionally, 27.5\% of questions have two different answers that had a consensus between at least two annotators. 

\section{Additional experimental details}\label{sec:detailsbis}
\noindent \textbf{VideoQA generation.} The input sequence to the answer extractor and question generation transformers are truncated and padded up to a maximum of 32 tokens. 
The question decoding is done with the beam search keeping track of the 4 most probable states at each level of the search tree. 
We have used the original captions (including stop words) from the HowTo100M dataset~\cite{miech19howto100m} and removed word repetitions from adjacent clips.

\noindent \textbf{VideoQA model.} We use the following hyperparameters: , , , , , , , , , .
The video features are sampled at equally spaced timestamps, and padded to length . 
Sequences of question and answer tokens are truncated and padded to length  and , respectively. 
Attention is computed only on non-padded sequential video and question features. 


\noindent \textbf{VideoQA datasets.} For MSRVTT-QA and MSVD-QA, we follow~\cite{le2020hierarchical} and use a vocabulary made of the top  training answers for MSRVTT-QA, and all  training answers for MSVD-QA. 
For our \smalldatasetname{} dataset and ActivityNet-QA, we consider all answers that appear at least twice in the training set, resulting in  answers for \smalldatasetname{} and  answers for ActivityNet-QA.

\begin{table*}[t]
\setlength\tabcolsep{3pt}
\begin{center}
\resizebox{.9\linewidth}{!}{	
\begin{tabular}{lc|cccccc|cccccc}
Pretraining Data & Finetuning & \multicolumn{6}{c}{MSRVTT-QA} & \multicolumn{6}{c}{MSVD-QA} 
\\ 
& & What & Who & Number & Color & When & Where
& What & Who & Number & Color & When & Where \\ \hline
& \cmark & 33.4 & 49.8 & 83.1 & 50.5 & 78.5 & 40.2 
& 31.5 & 54.9 & \textbf{82.7} & 50.0 & 74.1 & 46.4 \\
HowTo100M & \cmark & 34.3 & 50.2 & 82.7 & \textbf{51.8} & 80.0 & 41.5 
& 34.3 & 58.6 & 82.4 & \textbf{62.5} & \textbf{77.6} & \textbf{50.0} \\
\datasetname{} & \xmark & 1.8 & 0.7 & 66.3 & 0.6 & 0.6 & 4.5
& 7.8 & 1.7 & 74.3 & 18.8 & 3.5 & 0.0 \\
\datasetname{} & \cmark & \textbf{35.5} & \textbf{51.1} & \textbf{83.3} & 49.2 & \textbf{81.0} & \textbf{43.5} 
& \textbf{37.9} & \textbf{58.0} & 80.8 & \textbf{62.5} & \textbf{77.6} & 46.4 \\
\end{tabular}
}
\vspace{-0.3cm}
\caption{\small Effect of our pretraining per question type on MSRVTT-QA and MSVD-QA.}
\label{table:qtype}
\end{center}
\vspace{-0.5cm}
\end{table*}

\begin{table*}[t]
\setlength\tabcolsep{4.5pt}
\begin{center}
\resizebox{.9\linewidth}{!}{	
\begin{tabular}{lc|ccccccccc}
Pretraining Data & Finetuning & Motion & Spatial & Temporal & Yes-No & Color & Object & Location & Number & Other \\ \hline
& \cmark & 23.4 & 16.1 & 3.8 & 65.6 & 31.3 & 26.4 & 33.7 & 48.0 & 33.6 \\
HowTo100M & \cmark & 26.6 & \textbf{17.7} & 3.5 & \textbf{67.5} & 32.8 & 25.3 & 34.0 & \textbf{50.5} & 35.8 \\
\datasetname{} & \xmark & 2.3 & 1.1 & 0.3 & 36.3 & 11.3 & 4.1 & 6.5 & 0.2 & 4.7 \\
\datasetname{} & \cmark & \textbf{28.0} & 17.5 & \textbf{4.9} & 66.3 & \textbf{34.3} & \textbf{26.7} & \textbf{35.8} & 50.2 & \textbf{36.8} \\
\end{tabular}
}
\vspace{-0.3cm}
\caption{\small Effect of our pretraining per question type on ActivityNet-QA.}
\label{table:qtypeact}
\end{center}
\vspace{-0.5cm}
\end{table*}

\begin{table}[t]
\begin{center}
\setlength\tabcolsep{4pt}
\resizebox{\linewidth}{!}{	
\begin{tabular}{l|ccccc}
Method & \smalldatasetname{} & \makecell{ \small{MSRVTT} \\ \small{QA}} & \makecell{\small{MSVD} \\ \small{QA}} & \makecell{\small{ActivityNet} \\ \small{QA}} & How2QA \\
\hline
\qat{}
& 14.1 & 32.8 & 32.6 & 30.4 & 76.6 \\
\vqat{} & 23.0 & 39.6 & 41.2 & 36.8 & 80.8 \\ 
\end{tabular}
}
\end{center}
\vspace{-0.5cm}
\caption{\small Comparison of \textit{\qat{}} and \textit{\vqat{}} models trained from scratch (without pretraining) on downstream datasets.}
\vspace{-0.3cm}
\label{table:bias}
\end{table} 

\noindent \textbf{Training.} We use a cosine annealing learning rate schedule with initial values of  and  for pretraining and finetuning, respectively. For finetuning, we use the Adam optimizer with batch size of 256 and training runs for 20 epochs. The final model is selected by the best performance on the validation set.

\noindent \textbf{Masked Language Modeling.} For the masked language modeling objective, a token is corrupted with a probability 15\%, and replaced 80\% of the time with [MASK], 10\% of the time with the same token and 10\% of the time with a randomly sampled token. 
To guess which token is masked, each sequential question output  of the multi-modal transformer is classified in a vocabulary of 30,522 tokens, and we use a cross-entropy loss.


\noindent \textbf{Pretraining on HowTo100M.} For video-text cross-modal matching, we sample one video negative and one text negative per (positive) video-text pair, and use a binary cross-entropy loss. 
The cross-modal matching module is used to perform zero-shot VideoQA for the variant \textit{VQA-T} trained on HowTo100M, by computing scores for  for all possible answers , for each video-question pair .
We aggregate adjacent clips from HowTo100M to have at least 10 second clips and at least 10 narration words. 



\section{Additional experiments}\label{sec:expbis}

\subsection{Comparison to cross-dataset transfer}\label{sec:transfer}

We define cross-dataset transfer as a procedure where we pretrain our VideoQA model on a VideoQA dataset and then finetune and test it on another VideoQA dataset. 
The training follows the procedure described for finetuning in Section \ref{sec:training}. 
We report results for cross-dataset transfer in Table \ref{table:transfer}. Note that we do not use MSVD-QA as downstream dataset as its test set has been automatically generated with the same method \cite{heilman2010good} as MSRVTT-QA.
As can be observed, our approach with pretraining on \datasetname{} significantly outperforms cross-dataset transfer models using the previously largest VideoQA dataset (MSRVTT-QA), or the largest manually annotated VideoQA dataset (ActivityNet-QA), both for the zero-shot and finetuning settings, on all four downstream datasets.
We emphasize that our dataset is generated relying on text-only annotations, while MSRVTT-QA was generated using manually annotated video descriptions and ActivityNet-QA was manually collected.
These results further demonstrate the benefit of our \datasetname{} dataset.

\subsection{Results for rare answers and per question type}\label{sec:rarebis}
Results for different answers frequencies are presented for the \smalldatasetname{} dataset in Section \ref{sec:rare}. 
Here, we show results for MSRVTT-QA, MSVD-QA and ActivityNet-QA datasets in Table \ref{table:rareall}. 
As for \smalldatasetname{}, we observe that our model pretrained on our \datasetname{} dataset, after finetuning, shows the best results for quartiles corresponding to rare answers (Q3 and Q4), notably in comparison with the model trained from scratch or the model pretrained on HowTo100M.
We also find that our pretrained model, in the zero-shot setting, performs similarly across the different quartiles, with the exception of ActivityNet-QA, which includes in its most common answers \textit{yes}, \textit{no}.
Note that in order to have a consistent evaluation with other experiments, we keep the same train vocabulary at test time. 
This implies that a significant part of answers in the test set is considered wrong because the answer is not in the vocabulary. 
This represents 16\% of answers in \smalldatasetname{}, 3\% of answers in MSRVTT-QA, 6\% for MSVD-QA and 19\% for ActivityNet-QA. 
Note, however, that our joint embedding framework could allow for different vocabularies to be used at the training and test time. 

We also present results per question type for MSRVTT-QA, MSVD-QA and ActivityNet-QA in Tables \ref{table:qtype} and \ref{table:qtypeact}. Compared to the model trained from scratch or the model pretrained on HowTo100M, we observe consistent improvements for most categories.

\subsection{Comparison between \textit{\qat{}} and \textit{\vqat{}} on different datasets.}\label{sec:bias}

We show in Table~\ref{table:bias} that \textit{\qat{}} is a strong baseline compared to \textit{\vqat{}} on existing VideoQA datasets, when both are trained from scratch.
However, on \smalldatasetname{}, \textit{\vqat{}} improves more over \textit{\qat{}} than in other datasets, as measured by absolute improvement in top-1 accuracy.
This suggests that the visual modality is more important in \smalldatasetname{} than in other VideoQA datasets.  