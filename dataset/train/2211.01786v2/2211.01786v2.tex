\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{ACL2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{hyperref}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subfig} \usepackage{placeins} \usepackage{pdfpages}




\title{Crosslingual Generalization through Multitask Finetuning}



\author{
Niklas Muennighoff
\quad Thomas Wang 
\quad Lintang Sutawika 
\quad Adam Roberts
\quad Stella Biderman\\
\bf \quad Teven Le Scao
\bf \quad M Saiful Bari 
\bf \quad Sheng Shen 
\bf \quad Zheng-Xin Yong
\bf \quad Hailey Schoelkopf \\
\bf \quad Xiangru Tang  
\bf \quad Dragomir Radev 
\bf \quad Alham Fikri Aji
\bf \quad Khalid Almubarak\\
\bf \quad Samuel Albanie
\bf \quad Zaid Alyafeai
\bf \quad Albert Webson
\bf \quad Edward Raff
\bf \quad Colin Raffel \\
  Hugging Face \quad
  Datasaur.ai \quad
  EleutherAI \quad
  Google Research, Brain Team \quad
  Booz Allen Hamilton \quad\\
  Nanyang Technological University \quad
   UC Berkeley \quad
   Brown University \quad \\
   Yale University \quad
   MBZUAI \quad  
   PSAU \quad
   University of Cambridge 
   KFUPM\\
  {\tt \href{mailto:niklas@hf.co}{niklas@hf.co}}
}

\begin{document}
\maketitle
\begin{abstract}

Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at \url{https://github.com/bigscience-workshop/xmtf}.

\end{abstract}

\section{Introduction}

\begin{figure*}[ht]
    \centering
    \begin{center}
        \includegraphics[width=\textwidth]{assets/xp3_taxonomy.pdf}
        \caption{An overview of datasets in xP3. Datasets added to P3 in this work are marked \textbf{bold}. Yellow datasets are trained on. Green datasets are held out for evaluation.}
        \label{fig:xp3tasks}
    \end{center}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \begin{center}
        \includegraphics[width=\textwidth]{assets/xp3_languages.pdf}
        \caption{Language composition of xP3, ROOTS, and the corpus of mT5. All ROOTS and xP3 languages are depicted. The mT5 corpus covers additional languages that are not included in the graph.}
        \label{fig:xp3langs}
    \end{center}
\end{figure*}

\begin{figure*}[ht]
\includegraphics[width=\textwidth]{assets/xp3_variants.pdf}
\centering
\caption{Comparison of dataset variants P3, xP3, and xP3mt on a sample from PAWS for P3 \cite{zhang2019paws} and PAWS-X \cite{pawsx2019emnlp} for xP3 and xP3mt. P3 pairs English datasets with English prompts, xP3 pairs multilingual datasets with English prompts and xP3mt pairs multilingual datasets with prompts machine-translated from English to match the dataset language. Expressions in curly brackets are replaced, e.g. for xP3mt the target shown as \texttt{\{\{Choices[label]\}\}}) becomes  \texttt{SÃ­}.}\label{fig:dataset-diagram}
\end{figure*}

Large language models pretrained on vast amounts of text show some capability of solving tasks expressed in natural language, even without explicit training on these tasks \cite{gpt3}. Finetuning on groups of language tasks has been shown to significantly boost this zero-shot task generalization of language models~\cite{wei2021finetuned, sanh2022multitask, min2021metaicl}. For example, \citet{sanh2022multitask} finetune on tasks like summarization and question answering leading to better performance on unseen tasks like natural language inference. Previous work has focused on multitask finetuning in the context of large English language models and tasks.

Multilingual large language models show the same zero-shot learning capabilities for both monolingual and crosslingual tasks \cite{goyal2021larger,lin2021few,patel2022bidirectional,saleh2022}. However, zero-shot performance tends to be significantly lower than finetuned performance. Thus, task-specific or language-specific transfer learning via finetuning remains the predominant practice \cite{devlin2018bert,conneau2019unsupervised,ExT5}. This is particularly challenging for low-resource languages or tasks with limited data available, such as writing a fable that teaches a specified moral. In the spirit of multitask finetuning, it would be desirable to improve the zero-shot task generalization of multilingual models to make them usable on tasks from low-resource languages without requiring further finetuning.

To address this goal, we focus on crosslingual multitask finetuning. Due to the difficulty of collecting supervised task data in low-resource languages, previous work typically aims to transfer capabilities learned from finetuning on English data, which can improve performance on non-English language tasks \cite{wu-dredze-2019-beto,phang2020english,chalkidis2021multieurlex,vu2022overcoming}. We investigate whether English-only multitask finetuning also improves performance on non-English held-out tasks using the multilingual BLOOM~\cite{scao2022bloom} and mT5~\cite{xue2020mt5} models. We find that after finetuning on the English-only multitask mixture used for T0~\cite{sanh2022multitask} (P3), performance on a diverse set of non-English held-out tasks increases.

To investigate whether multilingual task data can further improve performance, we extend P3 to xP3 by adding datasets from 46 different languages that cover tasks previously not present in P3 (such as translation and program synthesis). Finetuning on xP3 leads to even better zero-shot task generalization in both English and non-English compared to the P3-trained baseline. Models finetuned on xP3 perform best on English prompts, even for non-English samples. Hypothesizing that better performance could be attained by training on non-English prompts, we construct a variant of xP3 with machine-translated prompts called xP3mt. We find that finetuning on machine-translated prompts is enough to significantly increase performance on held-out tasks with non-English human-written prompts. However, reducing the number of English prompts in the finetuning also worsens English prompt performance on multilingual tasks. 

Notably, we also find that models finetuned on xP3 generalize to held-out tasks in languages never intentionally seen during pretraining nor finetuning. We conduct a contamination analysis and find that only small amounts of these languages were included in the pretraining corpus. Thus, we hypothesize the models learn some language- and task-agnostic capabilities.

We publicly release all our datasets and models (URLs in Appendix \S\ref{sec:artifacts}).

\section{Related work}

\subsection{Multitask learning}

Multitask finetuning \cite{sanh2022multitask} (or instruction tuning \cite{wei2021finetuned}) has emerged as a recipe for improving the zero-shot task generalization of large language models. Typically, these works define a task as a collection of datasets that require a certain set of skills. To inform large language models which task to perform given an input, a prompt is used to add natural language instructions to dataset instances \cite{schick2020exploiting,scao2021many}. In this line of work, zero-shot task generalization refers to the ability to perform a held-out task based on prompted instructions alone. Our work builds on T0~\citep{sanh2022multitask}, a variant of T5~\citep{raffel2020exploring} that underwent MTF and was subsequently shown to have strong zero-shot task generalization capabilities.

Increasing the number and diversity of finetuning tasks and datasets has been shown to increase model performance \cite{min2021metaicl,fries2022bigbio,wang2022zemi,scialom2022continual,chung2022scaling,mishra2021}. PromptSource \cite{promptsource} is a software application that provides a framework for developing and applying prompts. PromptSource was used to construct P3, the training dataset of T0. While most prior work has focused on using English prompts on English datasets, \citet{wang2022super} trained both English and multilingual models on prompted datasets. Their multilingual model, called mTk-Instruct, attains strong crosslingual performance. In contrast with \citet{wang2022super}, our sole focus is crosslingual zero-shot generalization. Therefore, we consider a wider variety of prompting settings and perform a more detailed evaluation of multilingual capabilities. Separately, \citet{radford2019language} find that accidental inclusion of non-English text gave the GPT-2 model a limited ability to process and generate non-English text. We similarly discover that our finetuned models can process text in languages not intentionally trained on.

\subsection{Multilingual models}

Many language models are pretrained on English data only. Multilingual pretrained language models \cite{lample2019,conneau2019unsupervised,fan2021beyond} aim to enable processing a wide variety of non-English languages. Unlike monolingual models, multilingual models can also be used for crosslingual tasks, such as translation. For language generation, recent efforts have focused on two different model architectures based on the Transformer~\cite{vaswani2017attention}. On the one hand, encoder-decoder transformers trained with a denoising objective such as mBART \cite{liu2020multilingual} and mT5 \cite{xue2020mt5} learn to predict tokens masked out in the input sequence. Predicting masked tokens is only a pretraining task and these models are generally finetuned on downstream datasets before being used. On the other hand, decoder-only models pretrained on next token prediction such as mGPT \cite{shliazhko2022mgpt}, XGLM \cite{lin2021few} and BLOOM \cite{scao2022bloom} can be used to solve tasks expressed in natural language directly in a zero-shot or few-shot setting \cite{gpt3}. XGLM demonstrated competitive few-shot performance even when the model was prompted in a language different than the sample being processed. In particular, using English prompts for multilingual datasets provides better performance with XGLM than human-translating the English prompt to the dataset language. 

In this work, we use the BLOOM models~\cite{scao2022bloom,scao2022language}, which were pretrained on the ROOTS corpus \cite{laurencconbigscience} in 46 natural languages and 13 programming languages. We also finetune mT5~\cite{xue2020mt5} to compare encoder-decoder and decoder-only performance. mT5 is pretrained on a corpus sampled from mC4 covering 101 languages.


\section{Finetuning data and models}

To study crosslingual multitask prompted finetuning, we create xP3 by extending the P3 dataset collection with additional non-English tasks. We finetune both BLOOM and mT5 models on xP3. We refer to Appendix~\S\ref{sec:artifacts} for public links to released models and datasets. 

\subsection{Finetuning data}

We build on the P3~\cite{sanh2022multitask} task taxonomy and add 30 new multilingual datasets illustrated in Figure~\ref{fig:xp3tasks}. We define four task clusters previously not present in P3: translation, simplification, program synthesis, and miscellaneous code datasets. As 11\% of BLOOM's pretraining data is code, we add code datasets classified as program synthesis (text-to-code) or miscellaneous. The latter includes tasks such as estimating the computational complexity of a provided code snippet and generating a name for a given function. We extend the XWinograd dataset~\cite{tikhonov2021heads} with winograd schemas from CLUE~\cite{xu2020clue} to increase its Chinese samples from 16 to 504. Similar to P3, a fraction of our prompts invert the task at hand. For example, a prompt may invert a closed-book QA sample by asking the model to generate a question given an answer.

With xP3 we aim to replicate the language distribution of the ROOTS corpus \cite{laurencconbigscience} used to pretrain BLOOM. Thus, xP3 consists of the same 46 natural languages and code as ROOTS. ROOTS, xP3 and the mT5 corpus~\cite{xue2020mt5} language distributions are visualized in Figure~\ref{fig:xp3langs}. 39\% of xP3 data is English, slightly more than the 30\% of English data in ROOTS. Various African languages such as Twi (tw) and Bambara (bm) form the tail of xP3's language distribution. Many of them are not included in the mT5 pretraining corpus. In xP3, Twi and others are represented solely as a translation task using data from Flores-200~\cite{nllb2022}. 

To study the importance of non-English prompts, we construct a machine-translated variant of xP3, xP3mt. We translate prompts of monolingual datasets into the respective dataset language. For example, for the Chinese dataset C3~\cite{sun2020investigating} prompts in xP3mt are in Chinese instead of English in xP3. For crosslingual datasets prompts remain in English in xP3mt (such as Wiki-Lingua, which involves producing a summary in one language based on text in another language). We use the Google Cloud API for machine translation\footnote{\url{https://cloud.google.com/translate}}. Figure \ref{fig:dataset-diagram} compares the dataset variants we train on.


\subsection{Models}
\label{sec:models}

\begin{figure*}[ht]
\includegraphics[width=\textwidth]{assets/task_generalization_bar.pdf}
\centering
\caption{Zero-shot multilingual task generalization with English prompts. BLOOM models have 176 billion parameters. Scores are the language average for each task. Appendix~\S\ref{sec:taskgenlang} breaks down performance by language.}
\label{fig:taskgen}
\end{figure*}

We use publicly available pretrained BLOOM models ranging from 560 million to 176 billion parameters. BLOOM models are large decoder-only language models pretrained for around 350 billion tokens with an architecture similar to GPT-3 \cite{gpt3}. We finetune the models for an additional 13 billion tokens with loss only being computed on target tokens. For example, given the input ``Translate to English: Je t'aime." and a space-separated target ``I love you.", the model is trained to predict only the targets. As targets vary in length from just one to hundreds of tokens, we downscale the loss of each token by the length of the target it belongs to. This ensures short targets (e.g.\ for multiple-choice QA) get the same weight as long targets (e.g. for translation). We skip samples longer than 2048 tokens and use packing to train efficiently on multiple samples at a time \cite{kosec2021packing}. We select the final checkpoint based on validation performance.

For mT5 models, we finetune using the T5X~\cite{roberts2022t5x} framework on TPUs. mT5 uses the same encoder-decoder architecture, pretraining objective (masked language modeling), and pretraining length (1 trillion tokens) as T5~\cite{raffel2020exploring}. For finetuning mT5, we follow the same procedure as described above for BLOOM, except that inputs are fed into the encoder and thus are not space-separated from targets. 

We produce three core model variants available in different sizes:

\begin{itemize}
    \item \textbf{BLOOMZ-P3 / mT0-P3:} Models finetuned on the English-only P3.
    \item \textbf{BLOOMZ / mT0:} Models finetuned on xP3, which consists of multilingual datasets with English prompts.
    \item \textbf{BLOOMZ-MT / mT0-MT:} Models finetuned on xP3mt, which consists of multilingual datasets with English and machine-translated prompts.
\end{itemize}

We evaluate on three held-out tasks: coreference resolution, sentence completion and natural language inference (NLI) as depicted in Figure~\ref{fig:xp3tasks}. We also evaluate on HumanEval due to its popularity for code evaluations using the pass@k metric~\cite{chen2021evaluating}. For datasets that involve choosing the correct completion from several options, we follow prior work~\cite{sanh2022multitask,gpt3} and use rank classification: We compute the log-likelihood of each possible completion and select the highest scoring option. For each evaluation dataset, we select 5 prompts at random from PromptSource and use them for all language splits of the dataset. We report the median of the 5 prompts for results per language split. Thus, in constrast to XGLM~\cite{lin2021few}, we do not tune prompts based on performance on validation data. A selection of prompts can be found in Appendix~\S\ref{sec:prompts}. For evaluation on generative tasks, such as translation, we use lm-evaluation-harness \cite{eval-harness} and report BLEU scores~\cite{papineni2002bleu}.


\section{Results}

\begin{figure*}[ht]
\includegraphics[width=\textwidth]{assets/language_generalization.pdf}
\centering
\caption{Zero-shot task and language generalization using English prompts on tasks and languages not intentionally seen during pretraining nor finetuning. Language codes are ISO 639-1, except for JP (Japanese).}
\label{fig:langgen}
\end{figure*}

We first examine generalization to new tasks in languages included in finetuning in~\S\ref{sec:taskgen}. Then, in~\S\ref{sec:langgen}, we look at language generalization: Can models generalize to tasks in languages that \textbf{(a)} they have only seen during pretraining and \textbf{(b)} they have never seen intentionally? In~\S\ref{sec:multilingualprompting}, we investigate performance on multilingual prompts and finetuning on xP3mt. Scaling laws are analyzed in~\S\ref{sec:scaling}. Finally, \S\ref{sec:generation} looks at performance on generative tasks and \S\ref{sec:corpus} at the effect of language proportions on performance.


\subsection{Task generalization}
\label{sec:taskgen}

Previous work has shown that large language models finetuned on prompted multitask mixtures generalize to unseen tasks \cite{zhong2021adapting,wei2021finetuned,mishra2021,mishra2021cross,wang2022super}. In Figure \ref{fig:taskgen}, we show that the same applies to multilingual models: Finetuned BLOOMZ and BLOOMZ-P3 models significantly improve over BLOOM and XGLM on held-out tasks. Despite an order of magnitude fewer parameters, mT0 (13 billion parameters) is ahead of BLOOMZ (176 billion parameters). We attribute this to the encoder-decoder architecture paired with a masked language modeling pretraining objective \cite{wang2022language,tay2022unifying} as well as the longer pretraining of mT5 \cite{hoffmann2022training,su2022welm} (1 trillion tokens for mT5 vs.\ 366 billion for BLOOM). Despite also having gone through crosslingual multitask finetuning, mTk-Instruct performs significantly worse than the same-sized mT0. We attribute this to our prompting style, which aims to replicate natural human communication. mTk-Instruct is finetuned on more structured prompts with specific ``Definition", ``Input" and ``Output" fields. Similarly, \citet{wang2022super} find that T0 performs worse than Tk-Instruct on their prompts. We also find models finetuned on the 39\% English xP3 (BLOOMZ, mT0-13B) outperform models finetuned on the 100\% English P3 (BLOOMZ-P3, mT0-13B-P3) on \emph{English tasks} (Appendix~\S\ref{sec:taskgenlang}). Even the fully English T0-11B model \cite{sanh2022multitask} is outperformed by our mT0-13B model on entirely \emph{English tasks}. Ignoring embedding parameters T0-11B and mT0-13B have about the same size. This is likely due to xP3 adding additional tasks and prompts, which has been shown to help generalization~\cite{chung2022scaling,iyer2022opt}. mT0-13B beating T0-11B indicates that the benefit of scaling tasks is larger than the benefit of pre-training and finetuning on relatively more English tokens. 

\subsection{Language generalization}
\label{sec:langgen}

Here we add another layer of generalization: languages. Figure \ref{fig:taskgen} already shows that finetuning on English data only (P3) leads to better performance on non-English data: For example, BLOOMZ-P3 improves by over 50\% on multilingual sentence completion compared to BLOOM. Thus, zero-shot task performance in languages only seen during pretraining improves after finetuning on English. This has major practical benefits as it can be more difficult to collect data for low-resource languages.

Next, we investigate performance on languages the model has \textit{never intentionally seen}. Due to the scale of large language model pretraining, it is difficult to label tasks or languages as strictly unseen. It is likely that the training data unintentionally includes small fractions of these languages (just as many tasks might appear ``implicitly'' in the pretraining corpus~\cite{sanh2022multitask}). In Figure~\ref{fig:langgen} we show that after multitask finetuning on xP3, the models can perform unseen tasks in languages that were not intentionally trained on. After probing the pretraining corpus of BLOOM, we do find small amounts of these languages that were unintentionally included (Appendix~\S\ref{sec:contamination}). However, for XNLI, performance increases across all languages, many of which only show up in tiny fractions in our language contamination analysis, such as Thai with 0.006\%. If we extrapolate this proportion to the entire ROOTS corpus, the BLOOM models would have seen a mere 20 million tokens of Thai during pretraining. One possibility is that better-than-random XNLI performance can be attained with little or no language understanding. In Appendix~\S\ref{sec:levenshtein}, we investigate edit distances of XNLI samples and find that there are differences across labels, however, likely not significant enough to enable this kind of generalization.

\subsection{Multilingual prompting}
\label{sec:multilingualprompting}

\begin{table}[htbp]
    \begin{center}
        \resizebox{1\linewidth}{!}{
            \begin{tabular}{@{}ll|cc|cc@{}}
\toprule
Task & Prompt & \multicolumn{4}{c}{Average accuracy} \\
     &        & \small{BLOOMZ} & \small{BLOOMZ-MT} & \small{mT0-13B} & \small{mT0-13B-MT} \\
\midrule
XNLI & EN & \textbf{52.99} & 49.01 & 48.24 & \textbf{51.29} \\
 & MT & 37.56 & \textbf{41.16} & 39.31 & \textbf{41.66} \\
 & HT & 40.4 & \textbf{43.88} & 44.95 & \textbf{46.87} \\
\midrule
XCOPA & EN & 72.52 & \textbf{73.24} & \textbf{81.4} & 80.36 \\
 & MT & 70.04 & \textbf{71.84} & \textbf{81.16} & 79.64 \\
\midrule
XStoryCloze & EN & \textbf{81.73} & 81.39 & 81.99 & \textbf{82.3} \\
 & MT & 80.89 & \textbf{81.76} & \textbf{83.37} & 82.86 \\
\midrule
XWinograd & EN & \textbf{60.07} & 59.15 & 70.49 & \textbf{73.24} \\
 & MT & 58.48 & \textbf{60.14} & 66.89 & \textbf{72.33} \\
\bottomrule
            \end{tabular}
        }
    \caption{Comparison between EN (English), MT (machine-translated) and HT (human-translated) prompts for 176B BLOOMZ and 13B mT0 models finetuned on either only English or English and machine-translated multilingual prompts (-MT).}
    \label{tab:promptlangl1}
    \end{center}
\end{table}
 
Since all prompts in xP3 are in English (even for multilingual datasets), we created xP3mt, an extension with machine-translated prompts. To investigate performance on non-English prompts, we additionally human- and machine-translated the English evaluation prompts from Figure~\ref{fig:taskgen}. In Table~\ref{tab:promptlangl1}, we report performance on these. Results on machine-translated prompts in languages that are not part of the finetuning corpus, such as those in Figure \ref{fig:langgen}, are in Appendix \S\ref{sec:multiunseen}. Table~\ref{tab:promptlangl1} shows that BLOOMZ performs much better on English than on non-English prompts. BLOOMZ-MT, which is finetuned on xP3mt, significantly improves on multilingual prompts. On XNLI, BLOOMZ-MT raises the average performance on human-translated prompts from 41.13 to 45.55. This comes at the cost of a reduction in its performance on English prompts, from 53.58 to 49.74. For mT0, the MT version provides similar performance gains on XNLI and XWinograd non-English prompts, while results on XCOPA and XStoryCloze are mixed. Similar to~\citet{lin2021few}, we also find that models perform better on human-translated prompts than machine-translated ones for XNLI. 

\subsection{Scaling}
\label{sec:scaling}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{assets/scale.pdf}
    \caption{Aggregate performance vs. size. Transparent lines correspond to individual languages, while thick lines are average accuracy scores.}
    \label{fig:scale}
\end{figure}

In Figure \ref{fig:taskgen}, the average performance of BLOOM is near the random baselines of 0.50 for Sentence Completion and Coreference Resolution and 0.33 for NLI. We think this is due to all of our experiments being zero-shot and using untuned prompts~\cite{perez2021true}. We find in Figure~\ref{fig:scale} that even at 560M parameters, multitask finetuning improves zero-shot generalization. The gap between pretrained and multitask finetuned models grows significantly as parameters increase. Scaling up parameters benefits all languages evaluated.

\subsection{Generation tasks}
\label{sec:generation}

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\linewidth]{assets/validation.pdf}
        \caption{Validation performance during training on natural language understanding (NLU) and natural language generation (NLG) tasks. The former are scored using accuracy and the latter using BLEU \cite{papineni2002bleu}. The NLG tasks measured are translation and summarization. For BLOOMZ(-7.1B) the performance at 0 training tokens corresponds to the performance of BLOOM(-7.1B). For mT0 there is no data point at 0 tokens, as its base model, mT5, is not suitable for evaluation without finetuning. Performance on individual tasks is in Appendix \S\ref{sec:fullresults}.}
        \label{fig:validation}
\end{figure}



In this section, we investigate the impact of multitask finetuning on generative tasks. In Figure \ref{fig:validation}, we plot validation performance throughout the training process. We find that while performance on natural language understanding tasks continues to increase, generative performance jumps initially and then decreases. Relatedly, in Table~\ref{tab:humaneval}, we find that multitask finetuning does not improve performance on HumanEval~\cite{chen2021evaluating}. Only for small models, such as BLOOM-560M vs. BLOOMZ-560M, there are meaningful performance gains. When no code data is included in finetuning (BLOOMZ-P3) performance decreases significantly. mT0 models, which have not been pretrained on code, fail to solve any HumanEval problems (see full results in Appendix~\S\ref{sec:fullresults}). Given a Python docstring, HumanEval requires models to complete a function. Inspecting generations reveals that the multitask finetuned models are biased towards short generations. In Appendix~\S\ref{sec:codegen}, we show example solutions from HumanEval and compute average length statistics. BLOOMZ tries to solve problems with 70\% fewer characters than BLOOM.

\begin{table}[h!]
    \centering
    \footnotesize
\begin{tabular}{lccc}
    \toprule
    & \multicolumn{3}{c}{Pass@} \\
    &  &  &  \\
    \midrule
    GPT-Neo 1.3B & 4.79\% & 7.47\% & 16.30\% \\
    GPT-Neo 2.7B & 6.41\% & 11.27\% & 21.37\% \\
    GPT-J 6B & 11.62\% & 15.74\% & 27.74\% \\
    GPT-NeoX 20B & 15.4\% & 25.6\% & 41.2\% \\
    \midrule
    Codex-300M & 13.17\% & 20.37\% & 36.27\% \\
    Codex-679M & 16.22\% & 25.7\% & 40.95\% \\
    Codex-2.5B & 21.36\% & 35.42\% & 59.5\% \\
    Codex-12B & 28.81\% & 46.81\% & 72.31\% \\
    \midrule
    BLOOM-560M & 0.82\% & 3.02\% & 5.91\% \\
    BLOOM-1.1B & 2.48\% & 5.93\% & 9.62\% \\
    BLOOM-1.7B & 4.03\% & 7.45\% & 12.75\% \\
    BLOOM-3B & 6.48\% & 11.35\% & 20.43\% \\
    BLOOM-7.1B & 7.73\% & 17.38\% & 29.47\% \\
    BLOOM & 15.52\% & 32.20\% & 55.45\% \\
    \midrule
    BLOOMZ-560M & 2.18 \% & 4.11\% & 9.00\% \\
    BLOOMZ-1.1B & 2.63\% & 6.22\% & 11.68\% \\
    BLOOMZ-1.7B & 4.38\% & 8.73\% & 16.09\% \\
    BLOOMZ-3B & 6.29\% & 11.94\% & 19.06\% \\
    BLOOMZ-7.1B & 8.06\% & 15.03\% & 27.49\% \\
    BLOOMZ & 12.06\% & 26.53\% & 48.44\% \\
    BLOOMZ-P3 & 6.13\% & 11.79\% & 18.73\% \\
    \bottomrule
    \end{tabular}
\caption{
        Code continuation on HumanEval. Non-BLOOM results come from prior work~\cite{chen2021evaluating,fried2022incoder}. Codex is a language model finetuned on code, while the GPT models~\cite{black2021gpt,wang2021gpt,black2022gpt} are trained on a mix of code and text like BLOOM. Following \citet{chen2021evaluating} we generate 200 samples for each problem with top \emph{p} = 0.95 and compute pass rates. We perform this evaluation three times for temperatures 0.2, 0.6 and 0.8 and pick the best pass rate.
    }
    \label{tab:humaneval}
\end{table}

This bias towards short answers and the performance drop on generative tasks come from finetuning on short texts. Most tasks in our finetuning dataset, xP3, are single sentences. We show in Appendix~\S\ref{sec:generationlength} that finetuning on fewer short tasks via early stopping, adding long tasks or upweighting long tasks leads to longer generations and slightly better performance. We find it most effective, however, to force a minimum generation length at inference. This is done by ignoring any probability mass the model assigns to its end-of-sequence token for a desired number of tokens. Only after the generation has reached the desired length, can the model generate the end-of-sequence token, thus finishing the generation. Forcing a minimum generation length improves the BLEU score on a translation task by 9 points, see Appendix~\S\ref{sec:generationlength} for quantitative and Figure \ref{fig:qgen} for qualitative results.




















\subsection{Effect of language proportions}
\label{sec:corpus}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{assets/pretraining_sizes.pdf}
    \caption{Performance across languages by size in the BLOOM pretraining corpus, ROOTS.}
    \label{fig:langscale}
\end{figure}

In Figure~\ref{fig:langscale}, we find that finetuned BLOOM models perform better on languages seen extensively during pretraining. As the language distribution in the finetuning dataset, xP3, closely follows that of pretraining, these languages are also seen most frequently during finetuning. Specifically, XCOPA and XNLI show significantly better performance on these high-resource languages, such as English, Spanish or French, which all make up more than 10\% of pretraining individually. The trend is less consistent for XWinograd. This may be caused by the fact that XWinograd language subsets are not translations of each other and have a significantly different number of samples. Thus, some language subsets of XWinograd may be inherently more difficult than others.

\section{Conclusion}

In this work we investigated crosslingual multitask finetuning. We developed xP3, a corpus consisting of tasks in 46 languages. Further, we have extended xP3 to xP3mt with machine-translated prompts. We have finetuned pretrained BLOOM and mT5 models on the newly created corpora as well as the English-only P3 corpus to produce BLOOMZ and mT0 models.

We found that English-only finetuning suffices for a multilingual pretrained large language model to generalize to tasks in other pretrained languages. However, finetuning on multiple languages using xP3 provided even better performance. We have further observed finetuned models to be capable of generalization to new tasks in languages they have never intentionally seen. We investigated multilingual prompting and found performance after finetuning on English prompts only to be poor. However, finetuning on a corpus with machine-translated prompts (xP3mt) lead to significantly better performance on human-written non-English prompts. Comparing models from 560 million up to 176 billion parameters revealed that the performance gap between only pretraining and finetuning widens as parameters increase. Lastly, we found multitask finetuning on billions of short targets biases models to produce short answers, which can hurt performance on generative tasks. We proposed a simple workaround by forcing a minimum generation length at inference.

To contribute to future progress on improving zero-shot generalization, we release all datasets and models introduced in this work.

\section{Limitations}

We highlight several limitations of our work:

\paragraph{Unnatural prompting format} The choice to separate inputs and targets using a space character has proven effective to multitask finetune our decoder-only models. Nonetheless, poorly formatted prompts may result in undesirable behavior. For example, given the following prompt: ``Translate to English: Je t'aime", the model may continue the input with additional French content before starting to solve the task, i.e. translating the input from French to English. This can be mitigated by improving the prompts with a trailing full stop or a newline symbol. Encoder-decoder models, such as our mT0, do not suffer from this problem, as inputs and targets are fed into different parts of the model.

\paragraph{Limited languages in xP3} The pretraining corpus of mT0 contains more than 101 languages~\cite{xue2020mt5}, however, we finetune on only 46 languages. Likely, finetuning on the full 101 languages mT0 has seen during pretraining would lead to better performance. However, we decided to use only the languages of BLOOM in order to study language generalization (\S\ref{sec:langgen}). Similarly, one could likely attain better performance by enhancing xP3 with more datasets, such as via BIG-Bench \cite{srivastava2022beyond,suzgun2022challenging}, or more prompts, such as via NL-Augmenter \cite{dhole2021nl}. We have released an extended version of xP3 dubbed xP3x that covers 277 languages and is around ten times larger than xP3, but are yet to finetune models on it.

\paragraph{Performance} While our models show strong capabilities of performing tasks zero-shot, there remain numerous failure modes that are common in large language models~\cite{rae2021scaling,bommasani2021opportunities,zhang2022opt,smith2022using,ouyang2022training,taylor2022galactica,chowdhery2022palm,biderman2023pythia,allal2023santacoder,li2023starcoder}. In Figure \ref{fig:fable} of Appendix \S\ref{sec:gen}, BLOOMZ fails to understand the moral of a fable resulting in an undesirable generation. Similarly, in Figure \ref{fig:qgen}, mT0-13B is asked to provide an explanation, but answers with a question. We have made several modifications to the multitask finetuning recipe, such as loss weighting, mixing in long tasks, and various multilingual aspects, leading to the strong zero-shot performance of our models. However, there are many other changes to the multitask finetuning procedure that are worth exploring to get better models~\cite{honovich2022unnatural,wang2022self,longpre2023flan,liu2023zero,dettmers2023qlora}. Further, the pre-trained models we use, BLOOM and mT5, are suboptimal in many aspects such as compute allocation~\cite{hoffmann2022training,muennighoff2023scaling}, pre-training datasets~\cite{longpre2023pretrainer,touvron2023llama,chung2023unimax}, pre-training objective~\cite{tay2022ul2} and possibly model architecture~\cite{komatsuzaki2022sparse,shen2023flan}. Future work should investigate multitask finetuning better base models.

\paragraph{Learning new languages during finetuning} While we have investigated generalization to languages only seen during pretraining, we did not investigate generalization to languages only seen during finetuning. Our mT0 models are finetuned on several new languages not seen in pretraining (see Figure \ref{fig:xp3langs}). Out of those, we only evaluated on code (HumanEval), where mT0 performed at the random baseline (0.00 in Table \ref{tab:allres}). We point to follow-up work that has investigated the question of teaching BLOOMZ new languages ~\cite{yong2022bloom+,cahyawijaya2023instruct} and work investigating adaptation of BLOOM~\cite{ennen2023extending,yong2022adapting}.

\newpage

\section*{Acknowledgments}

This work was granted access to the HPC resources of Institut du d\'eveloppement et des ressources en informatique scientifique (IDRIS) du Centre national de la recherche scientifique (CNRS) under the allocation 2021-A0101012475 made by Grand \'equipement national de calcul intensif (GENCI). In particular, all the evaluations and data processing ran on the Jean Zay cluster of IDRIS, and we want to thank the IDRIS team for responsive support throughout the project, in particular R\'emi Lacroix.

We thank the XGLM team for providing access to XStoryCloze. We thank volunteers who human-translated XNLI prompts. We thank Noah Constant and Douwe Kiela for feedback on drafts of this paper. We thank Victor Sanh, Stephen Bach, Sasha Rush and Jordan Clive for support throughout the project.



\bibliography{anthology,acl2020,prompt_appendix_generation/multilingual_promptgen.bib}
\bibliographystyle{acl_natbib}

\appendix

\onecolumn

\tableofcontents

\clearpage

\section{Contributions}
\label{sec:contributions}

This research was conducted under the BigScience project for open research, a year-long initiative targeting the study of large models and datasets. The goal of the project is to research language models in a public environment. The project has hundreds of researchers from more than 50 countries and over 250 institutions. The BigScience project was initiated by Thomas Wolf at Hugging Face, and this collaboration would not have been possible without his effort. In the following, we list contributions made to this work. \\

{\setlength{\parindent}{0cm}
\textbf{Niklas Muennighoff} evaluated all models, created xP3 and wrote most of the paper.

\textbf{Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts and Hailey Schoelkopf} wrote the training and evaluation code.

\textbf{Niklas Muennighoff and Adam Roberts} trained the models.

\textbf{Niklas Muennighoff, Teven Le Scao, Hailey Schoelkopf, Zheng-Xin Yong, Thomas Wang, Khalid Almubarak, Alham Fikri Aji, M Saiful Bari and Zaid Alyafeai} contributed prompts or datasets.

\textbf{Lintang Sutawika, Stella Biderman, Zheng-Xin Yong, Khalid Almubarak, M Saiful Bari and Albert Webson} initiated the project.

\textbf{Sheng Shen} conducted the contamination analysis.

\textbf{Samuel Albanie} wrote the prompt appendix.

\textbf{Thomas Wang and Zheng-Xin Yong} converted checkpoints.

\textbf{Colin Raffel, Thomas Wang, Teven Le Scao, M Saiful Bari, Edward Raff and Dragomir Radev} advised the project.

\textbf{Niklas Muennighoff, Lintang Sutawika, Teven Le Scao, Colin Raffel, Stella Biderman, Alham Fikri Aji, Adam Roberts, Samuel Albanie, Sheng Shen, M Saiful Bari, Albert Webson, Xiangru Tang, Dragomir Radev and Edward Raff} contributed to the paper.
} 
\FloatBarrier

\section{Task generalization breakdown}
\label{sec:taskgenlang}

In Figure~\ref{fig:taskgeneng}, we compare performance on English held-out tasks. We find that \textbf{(a)} finetuning on xP3 outperforms P3 \textbf{(b)} multilingual mT0 is better than monolingual T0 on \emph{English tasks}. We think both improvements come from xP3 having more prompts and datasets than P3 \cite{chung2022scaling}.

\begin{figure*}[htbp]
\includegraphics[width=\textwidth]{assets/english_task_generalization.pdf}
\centering
\caption{Zero-shot English task generalization. Each dot represents performance on one English evaluation prompt.}
\label{fig:taskgeneng}
\end{figure*}

\newpage

In Figure \ref{fig:taskgenlang}, we visualize task generalization to multilingual datasets. The same data is aggregated in Figure \ref{fig:taskgen}. Performance by prompt varies substantially highlighting that prompt engineering may still be necessary after MTF. We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure \ref{fig:xp3langs} and \S\ref{sec:corpus}).

\begin{figure*}[htbp]
\includegraphics[width=\textwidth]{assets/task_generalization.pdf}
\centering
\caption{Zero-shot multilingual task generalization on languages seen during pretraining and finetuning. Each dot represents performance on one English evaluation prompt.}
\label{fig:taskgenlang}
\end{figure*}


\FloatBarrier
\newpage


\section{Artifacts}
\label{sec:artifacts}

Table \ref{tab:ckpts} lists all artifacts used or released in this work. We make all our work accessible under the most permissive licenses available to us.

\begin{table*}[htbp]
    \centering
    \tiny
    \resizebox{\textwidth}{!}{\begin{tabular}{@{}l|c|l@{}}
    \toprule
\multicolumn{1}{l|}{\textbf{Artifact}} &
\multicolumn{1}{c|}{\textbf{Explanation}} &\multicolumn{1}{c}{\textbf{Public link}} \\
\midrule
\midrule
ROOTS & Multilingual pretraining corpus of BLOOM & \url{https://huggingface.co/bigscience-data} \\
mC4 & Multilingual pretraining corpus used for mT5 & \url{https://huggingface.co/datasets/mc4} \\
P3 & Multitask finetuning dataset with English data \& English prompts & \url{https://huggingface.co/datasets/bigscience/P3} \\
xP3 & Multitask finetuning dataset with multilingual data \& English prompts & \url{https://huggingface.co/datasets/bigscience/xP3} 
\\
xP3all & Same as xP3 with held-out evaluation sets & \url{https://huggingface.co/datasets/bigscience/xP3all}
\\
xP3mt & Same as xP3 with English \& multilingual machine-translated prompts & \url{https://huggingface.co/datasets/bigscience/xP3mt} 
\\
xP3megds & Processed version of xP3 for easy usage with Megatron-DeepSpeed & \url{https://huggingface.co/datasets/bigscience/xP3megds}
\\
xP3x & Extension of xP3 to 277 languages & \url{https://huggingface.co/datasets/Muennighoff/xP3x}
\\
\midrule
XGLM-7.5B & 7.5B parameter pretrained multilingual transformer & \url{https://huggingface.co/facebook/xglm-7.5B} \\
T0-11B & 11B parameter model finetuned on P3 & \url{https://huggingface.co/bigscience/t0} \\
mTk-Instruct-3.7B & 3.7B parameter multitask finetuned multilingual transformer & \url{https://huggingface.co/allenai/mtk-instruct-3b-def-pos} \\
mTk-Instruct-13B & 13B parameter multitask finetuned multilingual transformer & \url{https://huggingface.co/allenai/mtk-instruct-11b-def-pos} \\
\midrule
BLOOM-560M & 560M parameter model pretrained on ROOTS & \url{https://huggingface.co/bigscience/bloom-560m} \\
BLOOM-1.1B & 1.1B parameter model pretrained on ROOTS & \url{https://huggingface.co/bigscience/bloom-1b1} \\
BLOOM-1.7B & 1.7B parameter model pretrained on ROOTS & \url{https://huggingface.co/bigscience/bloom-1b7} \\
BLOOM-3B & 3B parameter model pretrained on ROOTS & \url{https://huggingface.co/bigscience/bloom-3b} \\
BLOOM-7.1B & 7.1B parameter model pretrained on ROOTS & \url{https://huggingface.co/bigscience/bloom-7b1} \\
BLOOM & 176B parameter model pretrained on ROOTS & \url{https://huggingface.co/bigscience/bloom} \\
\midrule
BLOOMZ-560M & 560M parameter model finetuned on xP3 & \url{https://huggingface.co/bigscience/bloomz-560m} 
\\
BLOOMZ-1.1B & 1.1B parameter model finetuned on xP3 & \url{https://huggingface.co/bigscience/bloomz-1b1} 
\\
BLOOMZ-1.7B & 1.7B parameter model finetuned on xP3 & \url{https://huggingface.co/bigscience/bloomz-1b7} 
\\
BLOOMZ-3B & 3B parameter model finetuned on xP3 & \url{https://huggingface.co/bigscience/bloomz-3b} 
\\
BLOOMZ-7.1B & 7.1B parameter model finetuned on xP3 & \url{https://huggingface.co/bigscience/bloomz-7b1} 
\\
BLOOMZ-7.1B-MT & 7.1B parameter model finetuned on xP3mt & 
\url{https://huggingface.co/bigscience/bloomz-7b1-mt} 
\\
BLOOMZ-7.1B-P3 & 7.1B parameter model finetuned on P3 & \url{https://huggingface.co/bigscience/bloomz-7b1-p3} 
\\
BLOOMZ & 176B parameter model finetuned on xP3 & \url{https://huggingface.co/bigscience/bloomz} 
\\
BLOOMZ-MT & 176B parameter model finetuned on xP3mt & \url{https://huggingface.co/bigscience/bloomz-mt} 
\\
BLOOMZ-P3 & 176B parameter model finetuned on P3 & \url{https://huggingface.co/bigscience/bloomz-p3} 
\\
\midrule
mT5-300M & 300M parameter model pretrained on a sampled version of mC4 & \url{https://huggingface.co/google/mt5-small} \\
mT5-580M & 580M parameter model pretrained on a sampled version of mC4 & \url{https://huggingface.co/google/mt5-base} \\
mT5-1.2B & 1.2B parameter model pretrained on a sampled version of mC4 & \url{https://huggingface.co/google/mt5-large} \\
mT5-3.7B & 3.7B parameter model pretrained on a sampled version of mC4 & \url{https://huggingface.co/google/mt5-xl} \\
mT5-13B & 13B parameter model pretrained on a sampled version of mC4 & \url{https://huggingface.co/google/mt5-xxl} \\
\midrule
mT0-300M & 300M parameter model finetuned on xP3 & \url{https://huggingface.co/bigscience/mt0-small} 
\\
mT0-580M & 580M parameter model finetuned on xP3 & \url{https://huggingface.co/bigscience/mt0-base} 
\\
mT0-1.2B & 1.2B parameter model finetuned on xP3 & \url{https://huggingface.co/bigscience/mt0-large} 
\\
mT0-3.7B & 3.7B parameter model finetuned on xP3 & \url{https://huggingface.co/bigscience/mt0-xl} 
\\
mT0-13B & 13B parameter model finetuned on xP3 & \url{https://huggingface.co/bigscience/mt0-xxl} 
\\
mT0-13B-MT & 13B parameter model finetuned on xP3mt & \url{https://huggingface.co/bigscience/mt0-xxl-mt} 
\\
mT0-13B-P3 & 13B parameter model finetuned on P3 & \url{https://huggingface.co/bigscience/mt0-xxl-p3} 
\\
    \bottomrule
    \end{tabular}}
    \caption{Links to all models \& datasets used as part of this work. BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending ``-optimizer-states" to the respective URL. BLOOM(Z) models are released under the RAIL license, while mT5 / mT0 models are licensed under Apache 2.0}
    \label{tab:ckpts}
\end{table*}

\FloatBarrier

\section{ROOTS language contamination}
\label{sec:contamination}

\begin{figure*}[htbp]
    \centering
    \begin{center}
        \includegraphics[width=\textwidth]{assets/xp3_languages_identify.pdf}
        \caption{Language composition of ROOTS-IDENTIFY-1\%, ROOTS-1\% and the mT5 corpus. All mT5 languages are depicted. ROOTS-1\% is a random 1\% sample of ROOTS with its assigned meta-languages. ROOTS-IDENTIFY-1\% are the actual languages in ROOTS-1\% re-identified using \texttt{cld3}.}
        \label{fig:roots-identity-langs}
    \end{center}
\end{figure*}

While the BLOOM ROOTS corpus~\cite{laurencconbigscience} was collected from 46 natural languages and 13 programming languages, we find that sentences from the same document do not always belong to the collected (meta) language. Some sentences use languages like Russian or Japanese that were not the intentionally collected parts. This ``language contamination'' may stem from ``code-mixing'' or different languages being used in code comments. To investigate the extent of contamination, we randomly sample 1\% of the documents from ROOTS for a total of 51M documents. For each document, we use \texttt{cld3}\footnote{\url{https://github.com/google/cld3}}~\cite{xue2020mt5} to identify the languages used in each sentence and compare them with the meta language of the document. We summarize our results in Figure~\ref{fig:roots-identity-langs}. It shows that ROOTS contains unintentionally collected languages, such as Burmese (my: 0.00003\%), Thai (th: 0.006\%), Turkish (tr: 0.03\%), Greek (el: 0.03\%), Russian (ru: 0.03\%), Bulgarian (bg: 0.05\%),  Estonian (et: 0.06\%), Haitian (ht: 0.12\%), German (de: 0.21\%), Italian (it: 0.28\%) and Japanese (ja: 0.54\%). These ``unseen'' languages only have small sentence proportions in our subsample compared to English (en: 46.23\%), French (fr: 15.73\%) and Spanish (es: 13.38\%). Yet, they may help the language generalization of BLOOMZ models described in \S\ref{sec:langgen}. Japanese is mostly mixed in the meta English documents (47\%), meta Code documents (8\%) and meta Chinese documents (5\%). Meanwhile, Russian is mostly mixed in the meta English documents (52\%), meta Code documents (19\%) and meta French documents (11\%).

\FloatBarrier

\section{Code generations}
\label{sec:codegen}

Table \ref{tab:codelen} provides statistics on code generations and code data. We find that BLOOM generates on average 70\% more characters and 17x more comments than BLOOMZ for a given problem from HumanEval. Figure \ref{fig:codegen} compares an example solution from BLOOM and BLOOMZ. While both solutions are correct, BLOOMZ is biased towards short and concise answers.

\begin{figure*}[htbp]
    \centering
    \subfloat[\centering BLOOM]{{\includegraphics[width=0.45\textwidth]{assets/bloom_code_light.pdf}}}
    \qquad
    {\subfloat[\centering BLOOMZ]{\raisebox{0.85\height}{\includegraphics[width=0.45\textwidth]{assets/bloomz_code_light.pdf}}}}
    \caption{Code generations of BLOOM and BLOOMZ on HumanEval. The model is prompted to generate after the final . The generation is stopped after an end-of-sequence token or a return statement followed by a newline.}
    \label{fig:codegen}
\end{figure*}


\begin{table}[htbp]
    \small
    \begin{center}
        \resizebox{0.5\linewidth}{!}{
            \begin{tabular}{l|cc|c}
\toprule
Data () & \multicolumn{2}{c|}{HumanEval generations} & Fine-tuning data \\
& BLOOM & BLOOMZ & in xP3 (code data) \\
\midrule
Average characters & 247 & 144 & 531 \\
Average Python comments (\#) & 0.69 & 0.04 & 0.85 \\
\bottomrule
            \end{tabular}
        }
    \caption{Number of characters and comments for generations and fine-tuning data. For finetuning data, the statistics are computed for the targets that the model is tasked to generate, not the input.}
    \label{tab:codelen}
    \end{center}
\end{table}

\FloatBarrier

\section{Qualitative examples}
\label{sec:gen}

\begin{figure*}[htbp]
    \centering
    \subfloat[\centering English prompt]{{\includegraphics[width=0.45\textwidth]{assets/example_sentiment1.pdf}}}
    \qquad
    {\subfloat[\centering Non-English prompt]{{\includegraphics[width=0.45\textwidth]{assets/example_sentiment2.pdf}}}}
    \caption{Greedy generations for sentiment analysis, a task trained on. BLOOMZ and mT0-13B have not been trained on non-English prompts, but are still able to handle them. BLOOMZ, however, answers in English. The review is a five star review of Star Wars Episode IV.}
    \label{fig:sentgen}
\end{figure*}


\begin{figure*}[htbp]
    \centering
    \subfloat[\centering English prompt]{{\includegraphics[width=0.45\textwidth]{assets/example_query1.pdf}}}
    \qquad
    {\subfloat[\centering Non-English prompt]{{\includegraphics[width=0.45\textwidth]{assets/example_query2.pdf}}}}
    \caption{Greedy generations for zero-shot query expansion, a task not trained on. The models sometimes fail to output at least five terms as requested in the prompt.}
    \label{fig:querygen}
\end{figure*}


\begin{figure*}[htbp]
    \centering
    \subfloat[\centering English prompt]{{\includegraphics[width=0.45\textwidth]{assets/example_explanation1.pdf}}}
    \qquad
    {\subfloat[\centering English prompt]{{\includegraphics[width=0.45\textwidth]{assets/example_explanation2.pdf}}}}
    \caption{Greedy generations on question answering, a task trained on. \textbf{Left:} Specifying the language in the prompt is an effective way to force the output language. \textbf{Right:} Setting a minimum token length as a generation hyperparameter is an effective way to force long generations. The output of BLOOM is shortened~(marked with ).}
    \label{fig:qgen}
\end{figure*}


\begin{figure*}[htbp]
    \centering
    \subfloat[\centering English prompt]{{\includegraphics[width=0.45\textwidth]{assets/example_fable_spanish.pdf}}}
    \qquad
    {\subfloat[\centering English prompt]{{\includegraphics[width=0.45\textwidth]{assets/example_fable_hindi.pdf}}}}
    \caption{Non-greedy fable generations given a moral, a task not trained on. The generations are cherry-picked from 16 outputs with no minimum length, a temperature of 0.9 and top  of 40. \textbf{Left:} BLOOMZ generates an interesting fable with the desired moral. mT0 is significantly worse at writing stories likely due to its different pretraining objective. \textbf{Right:} BLOOMZ does not seem to understand the moral correctly.}
    \label{fig:fable}
\end{figure*}


\FloatBarrier

\section{Increasing generation length}
\label{sec:generationlength}

In \S\ref{sec:generation}, we found performance on generative tasks to worsen in later stages of training. To investigate this problem further, we study a 7.1 billion parameter BLOOM model that is finetuned for 13 billion tokens, which results in a low BLEU score of 0 and very short generations as shown in Table \ref{tab:genlength} (Default). We can solve this problem with two high-level strategies: \textbf{(a)} Reducing short tasks during finetuning and \textbf{(b)} Forcing a minimum generation length. 

For \textbf{(a)}, we do so by either early stopping, upweighting long tasks or adding new long tasks. As the majority of our finetuning data are single sentences, early stopping has the effect of finetuning on fewer short sentences. Upweighting long tasks is done by removing the loss normalization explained in \S\ref{sec:models}. This has the effect of each token getting equal weight regardless of the task, which upweights long tasks, as they have more tokens. Finally, for adding long tasks, we add tasks that require multi-sentence generations, such as generating an entire news article given a title. These long tasks collectively make up 10\% of finetuning data for this ablation. All three solutions result in longer average generations as shown in Table \ref{tab:genlength} and slightly better BLEU scores, albeit effects are still small.

For \textbf{(b)}, we force the model to generate a minimum number of tokens at inference. Our benchmarking task, MultiEURLEX~\cite{chalkidis2021multieurlex}, requires multi-sentence generations with an average target length of 1965 characters (about 491 tokens). By forcing the model to generate at least 768 tokens, we ensure that the generation is at least as long as the target. This boosts the BLEU score significantly to 9.05. This approach is thus an effective strategy to maintain long generations of good quality.

For our final models, we employ early stopping, adding of long tasks and recommend forcing a minimum generation length at inference for long generations. We do not upweight longer tasks, as it worsens accuracy on our NLU validation tasks by 10\%. The number of tokens our final models are fine-tuned for are displayed in Table~\ref{tab:earlystopping}.

\begin{table}[h]
    \small
    \begin{center}
        \resizebox{1\linewidth}{!}{
            \begin{tabular}{l|ccc}
\toprule
Model & Finetuning tokens & BLEU Score & Average generation length (characters) \\
\midrule
Default & 13 billion & 0.00 & 122 \\
\midrule
Early stopping & 6 billion & 0.00 & 155 \\
Upweight longer tasks & 13 billion & 0.06 & 364 \\
Add more long tasks & 13 billion & 0.06 & 136 \\
\midrule
Forcing 768 tokens at inference & 13 billion & 9.05 & 3072 \\
\bottomrule
            \end{tabular}
        }
    \caption{7.1 billion parameter BLOOMZ models with various modifications benchmarked on MultiEURLEX English-French translation~\cite{chalkidis2021multieurlex}. We benchmark three prompts on both English to French and French to English translation. We then take the median performance across the three prompts for each translation direction and average the two scores to arrive at the BLEU score reported.}
    \label{tab:genlength}
    \end{center}
\end{table}



\begin{table}[h]
    \small
    \begin{center}
\begin{tabular}{l|cccccccccc}
\toprule
Model & mT0-300M & mT0-560M & mT0-1.2B & mT0-3.7B & mT0-13B \\
\midrule
Tokens & 4.62 & 4.62 & 4.62 & 1.85 & 1.29 \\
\midrule
Model & BLOOMZ-560M & BLOOMZ-1.1B & BLOOMZ-1.7B & BLOOMZ-3B & BLOOMZ-7.1B & BLOOMZ \\
\midrule
Tokens & 3.67 & 0.502 & 8.39 & 8.39 & 4.19 & 2.09 \\
\bottomrule
            \end{tabular}
\caption{Tokens in billions that final models are finetuned for. We early-stop models based on validation performance. For -MT and -P3 variants we take the checkpoint after the same number of steps as for their default versions.}
    \label{tab:earlystopping}
    \end{center}
\end{table}


\FloatBarrier

\section{XNLI edit distances}
\label{sec:levenshtein}

\begin{table}[h]
    \small
    \begin{center}
        \resizebox{\linewidth}{!}{
            \begin{tabular}{lccc}
\toprule
Premise & Hypothesis & Lev. distance &
Label \\
\midrule
probably so probably so um-hum & probably yes so uh-huh & 13 & Entailment \\
equivalent to increasing national saving to 19 . & National savings are 18 now . & 34 & Neutral \\
The Inglethorps did not appear . & The Inglethorps were the first ones to turn up . & 26 & Contradiction \\
\bottomrule
            \end{tabular}
        }
    \caption{Three samples from the English XNLI split. To solve XNLI models need to classify whether the premise entails, is neutral to or contradicts the hypothesis. Samples are cherry-picked.}
    \label{tab:xnliex}
    \end{center}
\end{table}

As models are surprisingly capable of solving XNLI in languages they were never intentionally trained on (\S\ref{sec:langgen}), we investigate whether XNLI can be solved without any language understanding. To do so, we compute edit distances using the Levenshtein methodology~\cite{levenshtein1966binary} between premise and hypothesis. Table~\ref{tab:xnliex} shows three samples from the English XNLI and their edit distances. Our hypothesis is that entailment pairs generally need to cover similar content, and thus have similar distance. Contradiction pairs still need to cover similar content but differ in at least one major way. Meanwhile for neutral pairs, hypothesis and premise may be about completely different topics, hence they should have the highest distance. In Table~\ref{tab:levenshtein} we compute distances across all Thai, Turkish and Greek samples, three languages where we found language generalization to occur for BLOOMZ. Results confirm our hypothesis that distances are generally largest for neutral samples and smallest for entailment samples. However, the aggregate differences are very small with only a few edits difference. For example, Thai contradiction samples only have 2.5 edits more on average than entailment samples. Thus, comparing characters based on edit distance alone is likely not sufficient to fully explain the language generalization of models in \S\ref{sec:langgen}.

\begin{table}[h]
    \small
    \begin{center}
        \resizebox{0.5\linewidth}{!}{
            \begin{tabular}{lccc}
\toprule
Label () & Entailment & Neutral &
Contradiction \\
Language () & & & \\
\midrule
Thai (th) & 79.08 & 82.64 & 81.52 \\
Turkish (tr) & 76.93 & 80.59 & 80.24 \\
Greek (el) & 90.90 & 95.10 & 93.93 \\
\bottomrule
            \end{tabular}
        }
    \caption{Levenshtein distances between hypothesis and premise averaged across samples from different XNLI labels. Each label has 830 samples per language subset.}
    \label{tab:levenshtein}
    \end{center}
\end{table}

\FloatBarrier

\section{Multilingual prompting in unseen languages}
\label{sec:multiunseen}

Table \ref{tab:promptlangl2} shows aggregate performances on languages not intentionally seen during pretraining nor finetuning for BLOOMZ and only seen during pretraining for mT0. For BLOOMZ, performance drops significantly when translating the prompts to the respective unseen languages. Unlike on translated prompts for seen languages (\S\ref{sec:multilingualprompting}), BLOOMZ-MT performs worse than BLOOMZ for machine-translated prompts in unseen languages. This is likely because BLOOMZ-MT has not been finetuned on prompts in these languages. For mT0 differences are less significant.

\begin{table}[htbp]
    \small
    \begin{center}
            \begin{tabular}{ll|cc|cc}
\toprule
Task & Prompt & \multicolumn{4}{c}{Average accuracy} \\
     &        & BLOOMZ & BLOOMZ-MT & mT0-13B & mT0-13B-MT \\
\midrule
XNLI & EN & \textbf{45.65} & 43.2 & 48.52 & \textbf{51.33} \\
 & MT & \textbf{36.48} & 35.67 & \textbf{41.86} & 39.78 \\
\midrule
XCOPA & EN & \textbf{54.27} & 53.67 & \textbf{72.67} & 71.6 \\
 & MT & \textbf{53.2} & 53.0 & \textbf{71.57} & 70.87\\
\midrule
XStoryCloze & EN & \textbf{61.59} & 61.36 & 79.31 & \textbf{80.13} \\
 & MT & \textbf{60.5} & 59.91 & 80.21 & \textbf{80.28}\\
\midrule
XWinograd & EN & \textbf{55.98} & 54.54 & 70.81 & \textbf{72.0} \\
 & MT & \textbf{53.11} & 52.46 & 67.86 & \textbf{70.45} \\
\bottomrule
            \end{tabular}
    \caption{Comparison between EN (English) and MT (machine-translated) prompts for 176B BLOOMZ and 13B mT0 models finetuned on either only English or English and machine-translated multilingual prompts (-MT). For BLOOMZ the evaluation languages averaged are never intentionally seen, such as Japanese and Russian for XWinograd (see Figure \ref{fig:langgen}). For mT0 the evaluation languages are only seen during pretraining.}
    \label{tab:promptlangl2}
    \end{center}
\end{table}

 
\FloatBarrier

\section{Ideas that did not work}
\label{sec:failures}

We list several experiments that did not improve over baseline results:

\paragraph{Non-causal} In a non-causal or prefix language model, the model attends bidirectionally over input tokens and only causally over target tokens. Given a pretrained causal decoder, other work found that multitask finetuning in a non-causal setup performed better than causal finetuning \cite{wang2022language,tay2022transcending}. However, in our experiments, non-causal finetuning did not improve over causal finetuning.

\paragraph{Special tokens} Instead of separating inputs and targets with a space, we experimented with special tokens. Using the end-of-sequence token as a separator or a completely new token that the model would learn during finetuning significantly worsened results. The models may need to train on more tokens, possibly even during pretraining, to learn these new special tokens \cite{zeng2022glm}.

\paragraph{Fixing prompts} PromptSource has been written with encoder-decoder models in mind, where inputs and targets are fed into different models. As a consequence, human-written prompts in PromptSource often lack separators between input and target. For our decoder models, we decided to separate them with a space. We additionally experimented with leaving them as is or rewriting a significant amount of prompts, but neither improved significantly over space separation.

\paragraph{BitFit} Previous work has shown bias-only finetuning~\cite{zaken2021bitfit} of large language models to be sufficient for strong downstream performance~\cite{logan2021cutting,hu2021lora,muennighoff2022sgpt,liu2022few,ding2022delta,muennighoff2022mteb}. We found multitask finetuning of only biases to perform 15 absolute percentage points worse on the average of held-out tasks for BLOOMZ-7.1B.

\FloatBarrier

\FloatBarrier

\section{Full results}
\label{sec:fullresults}

Table~\ref{tab:allres} shows all evaluation results on test datasets. Table~\ref{tab:valres} displays evaluation results on validation datasets which we use for checkpoint selection.





\eject \pdfpagewidth=77cm \pdfpageheight=80cm

\begin{table*}[ht]
    \centering
    \begin{minipage}{\pdfpagewidth}
    \resizebox{0.95\textwidth}{!}{
    \footnotesize
            \begin{tabular}{llllll|c|cccccc|c|cc|ccccccc|cccccccccc}
\toprule
\multicolumn{6}{c|}{} & \multicolumn{7}{c|}{Pretrained} & \multicolumn{20}{c}{Pretrained + Multitask finetuned} \\
\cmidrule{7-13} \cmidrule{14-33}
Task & Dataset & Config & Split & Prompt & Metric & XGLM-7.5B & BLOOM-560M & BLOOM-1.1B & BLOOM-1.7B & BLOOM-3B & BLOOM-7.1B & BLOOM & T0-11B & mTk-Instruct-3.7B & mTk-Instruct-13B & mT0-300M & mT0-560M & mT0-1.2B & mT0-3.7B & mT0-13B & mT0-13B-MT & mT0-13B-P3 & BLOOMZ-560M & BLOOMZ-1.1B & BLOOMZ-1.7B & BLOOMZ-3B & BLOOMZ-7.1B & BLOOMZ-7.1B-MT & BLOOMZ-7.1B-P3 & BLOOMZ & BLOOMZ-MT & BLOOMZ-P3 \\
\midrule
\midrule
Coref. resolution & winogrande & xl & validation & EN & Median acc. & 49.25 & 49.88 & 50.99 & 49.57 & 49.96 & 49.41 & 48.62 & 60.46 & 50.99 & 52.33 & 49.57 & 51.62 & 50.51 & 52.01 & 62.27 & 62.51 & 56.91 & 49.80 & 51.07 & 50.75 & 51.78 & 55.41 & 55.88 & 51.78 & 58.41 & 58.64 & 55.64\\
Coref. resolution & winogrande & xl & validation & EN & Max acc. & 50.12 & 50.99 & 51.62 & 50.91 & 51.46 & 50.91 & 49.64 & 63.61 & 51.14 & 54.54 & 50.51 & 53.28 & 51.78 & 52.49 & 63.38 & 62.67 & 58.56 & 52.41 & 52.33 & 51.14 & 53.67 & 55.80 & 56.51 & 54.06 & 59.27 & 59.98 & 57.06\\
Coref. resolution & xwinograd & en & test & EN & Median acc. & 50.88 & 50.62 & 51.10 & 50.67 & 50.97 & 50.15 & 50.28 & 62.75 & 52.22 & 52.77 & 50.11 & 51.01 & 52.30 & 57.94 & 79.91 & 81.33 & 59.87 & 50.24 & 50.15 & 52.09 & 54.84 & 60.09 & 59.31 & 52.26 & 67.87 & 64.73 & 59.74\\
Coref. resolution & xwinograd & en & test & EN & Max acc. & 51.61 & 51.53 & 51.57 & 51.66 & 51.70 & 50.71 & 51.27 & 70.71 & 53.12 & 60.82 & 51.31 & 51.40 & 54.80 & 61.89 & 81.29 & 83.31 & 70.71 & 51.01 & 50.49 & 56.34 & 59.23 & 66.02 & 65.76 & 53.72 & 69.08 & 69.33 & 60.65\\
Coref. resolution & xwinograd & fr & test & EN & Median acc. & 50.60 & 46.99 & 48.19 & 50.60 & 46.99 & 50.60 & 51.81 & 54.22 & 50.60 & 53.01 & 50.60 & 51.81 & 49.40 & 56.63 & 77.11 & 73.49 & 55.42 & 49.40 & 53.01 & 51.81 & 49.40 & 53.01 & 53.01 & 53.01 & 65.06 & 59.04 & 53.01\\
Coref. resolution & xwinograd & fr & test & EN & Max acc. & 51.81 & 51.81 & 56.63 & 55.42 & 54.22 & 51.81 & 53.01 & 56.63 & 53.01 & 60.24 & 53.01 & 55.42 & 56.63 & 59.04 & 78.31 & 78.31 & 61.45 & 51.81 & 56.63 & 55.42 & 53.01 & 57.83 & 55.42 & 55.42 & 68.67 & 68.67 & 59.04\\
Coref. resolution & xwinograd & fr & test & MT & Median acc. & 46.99 & 48.19 & 53.01 & 48.19 & 46.99 & 50.60 & 49.40 & 54.22 & 50.60 & 53.01 & 49.40 & 53.01 & 53.01 & 56.63 & 68.67 & 75.90 & 53.01 & 48.19 & 50.60 & 50.60 & 50.60 & 51.81 & 55.42 & 51.81 & 56.63 & 57.83 & 53.01\\
Coref. resolution & xwinograd & fr & test & MT & Max acc. & 51.81 & 51.81 & 55.42 & 53.01 & 55.42 & 51.81 & 50.60 & 59.04 & 57.83 & 63.86 & 54.22 & 55.42 & 55.42 & 59.04 & 75.90 & 75.90 & 61.45 & 51.81 & 59.04 & 51.81 & 50.60 & 57.83 & 57.83 & 54.22 & 65.06 & 66.27 & 56.63\\
Coref. resolution & xwinograd & jp & test & EN & Median acc. & 49.22 & 50.36 & 50.89 & 51.62 & 51.41 & 50.89 & 50.26 & 51.51 & 52.66 & 53.18 & 50.89 & 50.68 & 51.41 & 56.93 & 74.35 & 77.37 & 60.27 & 49.74 & 49.84 & 49.95 & 50.26 & 50.68 & 49.64 & 50.36 & 57.46 & 55.47 & 51.09\\
Coref. resolution & xwinograd & jp & test & EN & Max acc. & 52.03 & 51.09 & 52.03 & 52.35 & 52.24 & 52.76 & 50.99 & 51.82 & 53.18 & 56.20 & 52.14 & 51.41 & 52.24 & 60.27 & 78.62 & 78.62 & 65.59 & 50.57 & 51.09 & 52.55 & 52.45 & 52.87 & 51.62 & 51.93 & 59.65 & 58.39 & 56.00\\
Coref. resolution & xwinograd & jp & test & MT & Median acc. & 48.91 & 50.89 & 50.26 & 50.78 & 51.93 & 49.53 & 51.72 & 51.51 & 51.20 & 53.28 & 51.41 & 50.05 & 50.26 & 55.27 & 73.31 & 78.42 & 61.00 & 50.78 & 50.57 & 49.64 & 50.68 & 49.95 & 50.26 & 50.36 & 52.87 & 52.66 & 50.89\\
Coref. resolution & xwinograd & jp & test & MT & Max acc. & 50.99 & 52.03 & 52.03 & 52.24 & 52.97 & 50.99 & 53.18 & 52.03 & 53.70 & 56.41 & 52.45 & 51.09 & 53.08 & 59.02 & 78.21 & 80.19 & 66.11 & 52.03 & 51.82 & 49.95 & 52.14 & 52.76 & 51.82 & 51.51 & 53.91 & 53.60 & 54.33\\
Coref. resolution & xwinograd & pt & test & EN & Median acc. & 50.57 & 51.33 & 51.71 & 51.71 & 50.19 & 48.67 & 50.95 & 52.47 & 52.09 & 56.27 & 49.81 & 49.81 & 53.61 & 58.17 & 72.24 & 76.05 & 56.27 & 50.19 & 50.19 & 50.95 & 52.47 & 53.99 & 54.37 & 51.33 & 63.50 & 60.08 & 53.99\\
Coref. resolution & xwinograd & pt & test & EN & Max acc. & 53.99 & 53.99 & 53.99 & 53.99 & 54.37 & 50.19 & 51.33 & 54.75 & 52.09 & 58.56 & 50.57 & 52.09 & 55.13 & 60.84 & 76.43 & 80.99 & 61.98 & 52.09 & 51.33 & 53.23 & 53.61 & 57.79 & 57.41 & 53.99 & 64.26 & 64.64 & 60.46\\
Coref. resolution & xwinograd & pt & test & MT & Median acc. & 50.95 & 52.09 & 50.57 & 49.81 & 50.57 & 50.57 & 53.23 & 52.47 & 53.23 & 52.47 & 49.81 & 47.15 & 52.47 & 54.75 & 71.48 & 75.67 & 55.89 & 52.47 & 50.57 & 49.81 & 50.19 & 52.85 & 53.61 & 51.33 & 60.46 & 59.70 & 54.75\\
Coref. resolution & xwinograd & pt & test & MT & Max acc. & 53.99 & 53.99 & 53.99 & 53.99 & 53.99 & 53.99 & 53.99 & 56.65 & 54.37 & 55.89 & 51.71 & 52.09 & 56.27 & 66.16 & 77.95 & 80.61 & 64.26 & 53.99 & 54.75 & 53.23 & 52.47 & 53.99 & 55.51 & 52.09 & 64.26 & 62.74 & 59.32\\
Coref. resolution & xwinograd & ru & test & EN & Median acc. & 53.33 & 51.43 & 52.38 & 54.29 & 52.70 & 54.29 & 54.29 & 51.43 & 53.97 & 56.83 & 49.52 & 51.11 & 52.38 & 56.83 & 74.29 & 73.97 & 56.51 & 52.06 & 49.52 & 51.75 & 52.38 & 53.97 & 53.02 & 48.57 & 57.78 & 56.51 & 52.70\\
Coref. resolution & xwinograd & ru & test & EN & Max acc. & 53.97 & 53.97 & 53.97 & 56.19 & 54.92 & 55.24 & 57.14 & 53.33 & 55.56 & 60.32 & 53.65 & 52.70 & 55.56 & 59.05 & 76.51 & 79.05 & 62.22 & 53.97 & 50.48 & 53.33 & 53.97 & 54.92 & 55.87 & 49.21 & 60.95 & 60.32 & 56.19\\
Coref. resolution & xwinograd & ru & test & MT & Median acc. & 53.33 & 51.75 & 52.38 & 53.97 & 52.06 & 53.97 & 52.70 & 50.16 & 53.33 & 54.29 & 52.06 & 51.75 & 52.70 & 52.38 & 66.98 & 71.43 & 55.87 & 51.43 & 51.43 & 53.02 & 49.52 & 52.06 & 52.70 & 47.62 & 54.29 & 55.87 & 54.92\\
Coref. resolution & xwinograd & ru & test & MT & Max acc. & 54.60 & 53.97 & 53.97 & 54.60 & 54.92 & 55.56 & 55.87 & 52.70 & 54.92 & 58.73 & 54.29 & 53.97 & 54.60 & 54.60 & 72.06 & 75.24 & 58.41 & 53.97 & 53.97 & 55.24 & 53.97 & 53.33 & 54.92 & 53.97 & 60.32 & 57.14 & 57.14\\
Coref. resolution & xwinograd & zh & test & EN & Median acc. & 49.01 & 49.21 & 48.81 & 50.20 & 50.00 & 50.60 & 49.21 & 49.21 & 52.18 & 56.75 & 52.78 & 52.18 & 51.59 & 57.54 & 69.25 & 76.19 & 58.53 & 54.17 & 53.97 & 51.39 & 55.16 & 57.94 & 54.37 & 52.18 & 68.65 & 62.10 & 51.59\\
Coref. resolution & xwinograd & zh & test & EN & Max acc. & 50.79 & 52.18 & 52.78 & 53.77 & 55.16 & 55.36 & 52.98 & 49.40 & 54.76 & 57.14 & 54.17 & 53.77 & 54.17 & 62.90 & 77.38 & 79.17 & 65.67 & 54.76 & 55.16 & 56.15 & 60.91 & 63.69 & 62.70 & 52.98 & 69.05 & 70.63 & 55.95\\
Coref. resolution & xwinograd & zh & test & HT & Median acc. & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 50.99 & - & 49.40 & - & - & -\\
Coref. resolution & xwinograd & zh & test & HT & Max acc. & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 59.72 & - & 52.18 & - & - & -\\
Coref. resolution & xwinograd & zh & test & MT & Median acc. & 48.02 & 49.01 & 49.01 & 49.40 & 49.60 & 50.79 & 49.60 & 49.21 & 53.17 & 53.17 & 51.19 & 51.79 & 50.60 & 56.35 & 67.86 & 72.42 & 57.74 & 50.79 & 51.19 & 51.79 & 52.98 & 52.38 & 57.94 & 50.40 & 62.70 & 67.46 & 57.14\\
Coref. resolution & xwinograd & zh & test & MT & Max acc. & 49.21 & 55.56 & 53.17 & 56.15 & 53.57 & 56.94 & 57.74 & 49.21 & 54.56 & 57.74 & 53.37 & 53.97 & 54.37 & 62.10 & 72.82 & 82.34 & 64.09 & 51.98 & 54.17 & 54.17 & 55.16 & 60.71 & 62.50 & 52.38 & 70.24 & 76.39 & 60.71\\
\midrule
NLI & anli & r1 & validation & EN & Median acc. & 33.30 & 33.60 & 33.50 & 33.40 & 32.90 & 33.40 & 36.20 & 44.50 & 29.90 & 34.20 & 33.30 & 31.30 & 30.70 & 37.50 & 48.00 & 48.50 & 44.90 & 29.60 & 29.10 & 33.10 & 38.60 & 40.90 & 40.10 & 34.50 & 46.00 & 45.60 & 40.60\\
NLI & anli & r1 & validation & EN & Max acc. & 33.50 & 34.40 & 33.70 & 33.80 & 33.40 & 33.70 & 37.60 & 45.00 & 34.80 & 35.40 & 34.70 & 33.30 & 33.30 & 38.20 & 49.50 & 49.50 & 47.30 & 33.40 & 33.30 & 34.00 & 40.10 & 42.10 & 42.60 & 35.10 & 48.60 & 49.70 & 41.70\\
NLI & anli & r2 & validation & EN & Median acc. & 33.40 & 33.20 & 33.10 & 33.30 & 33.20 & 33.30 & 33.70 & 39.30 & 32.40 & 32.50 & 33.20 & 33.30 & 32.50 & 34.40 & 41.70 & 40.60 & 37.90 & 32.00 & 33.20 & 34.30 & 34.60 & 38.20 & 37.60 & 33.90 & 41.90 & 41.00 & 37.80\\
NLI & anli & r2 & validation & EN & Max acc. & 35.00 & 33.70 & 33.50 & 36.00 & 34.90 & 33.40 & 34.80 & 39.60 & 33.20 & 34.20 & 34.00 & 33.50 & 34.70 & 34.80 & 43.00 & 42.00 & 40.20 & 33.40 & 33.50 & 36.10 & 36.80 & 39.50 & 39.40 & 35.40 & 44.10 & 45.00 & 39.30\\
NLI & anli & r3 & validation & EN & Median acc. & 32.92 & 33.50 & 33.42 & 33.17 & 33.33 & 33.08 & 34.58 & 41.33 & 32.83 & 33.33 & 33.00 & 33.00 & 33.50 & 37.42 & 44.83 & 46.25 & 40.50 & 33.25 & 33.08 & 35.42 & 37.75 & 38.00 & 38.92 & 34.08 & 42.67 & 41.33 & 40.08\\
NLI & anli & r3 & validation & EN & Max acc. & 34.25 & 35.58 & 33.50 & 33.67 & 33.58 & 33.58 & 36.33 & 43.75 & 33.00 & 34.83 & 33.83 & 33.33 & 34.75 & 39.00 & 46.08 & 48.17 & 44.17 & 33.50 & 34.50 & 37.08 & 40.00 & 41.00 & 42.00 & 37.58 & 45.50 & 45.58 & 42.83\\
NLI & super\_glue & cb & validation & EN & Median acc. & 39.29 & 42.86 & 42.86 & 28.57 & 32.14 & 44.64 & 33.93 & 76.79 & 44.64 & 26.79 & 44.64 & 46.43 & 50.00 & 69.64 & 82.14 & 87.50 & 67.86 & 51.79 & 53.57 & 58.93 & 67.86 & 57.14 & 71.43 & 53.57 & 76.79 & 76.79 & 75.00\\
NLI & super\_glue & cb & validation & EN & Max acc. & 41.07 & 60.71 & 48.21 & 42.86 & 51.79 & 57.14 & 42.86 & 78.57 & 51.79 & 57.14 & 50.00 & 50.00 & 51.79 & 85.71 & 85.71 & 87.50 & 76.79 & 53.57 & 58.93 & 71.43 & 75.00 & 80.36 & 83.93 & 62.50 & 82.14 & 87.50 & 85.71\\
NLI & super\_glue & rte & validation & EN & Median acc. & 52.71 & 53.07 & 47.65 & 49.46 & 54.15 & 52.35 & 50.18 & 83.39 & 56.68 & 51.26 & 58.84 & 65.70 & 62.09 & 76.90 & 83.03 & 83.75 & 80.14 & 64.26 & 53.07 & 73.29 & 72.56 & 79.06 & 79.06 & 67.15 & 81.95 & 81.23 & 73.65\\
NLI & super\_glue & rte & validation & EN & Max acc. & 53.07 & 54.15 & 52.71 & 53.79 & 57.40 & 55.96 & 54.15 & 84.84 & 61.37 & 55.23 & 61.01 & 66.43 & 64.26 & 78.70 & 85.56 & 84.84 & 83.03 & 67.15 & 65.70 & 76.17 & 76.17 & 84.12 & 82.67 & 78.70 & 85.56 & 85.92 & 85.20\\
NLI & xnli & ar & validation & EN & Median acc. & 33.33 & 33.82 & 33.57 & 33.98 & 35.94 & 33.82 & 33.78 & 33.90 & 35.10 & 35.10 & 33.90 & 40.84 & 38.15 & 49.72 & 56.51 & 57.63 & 54.82 & 39.80 & 41.33 & 46.99 & 48.23 & 51.20 & 49.28 & 47.99 & 54.38 & 51.85 & 46.39\\
NLI & xnli & ar & validation & EN & Max acc. & 34.98 & 36.95 & 34.78 & 35.90 & 36.59 & 37.99 & 34.46 & 34.22 & 39.72 & 38.31 & 37.43 & 41.85 & 42.61 & 51.85 & 57.91 & 58.03 & 56.06 & 44.46 & 46.59 & 50.04 & 53.29 & 53.25 & 55.58 & 50.64 & 60.68 & 58.03 & 55.22\\
NLI & xnli & ar & validation & HT & Median acc. & 34.30 & 33.37 & 33.33 & 34.06 & 33.37 & 33.33 & 33.37 & 33.33 & 33.65 & 35.62 & 32.57 & 33.37 & 34.34 & 39.32 & 42.93 & 49.16 & 47.55 & 34.78 & 35.30 & 35.82 & 39.36 & 41.85 & 39.00 & 36.35 & 38.63 & 37.31 & 50.32\\
NLI & xnli & ar & validation & HT & Max acc. & 37.31 & 33.45 & 33.41 & 35.38 & 35.26 & 37.79 & 34.18 & 33.65 & 35.50 & 37.79 & 36.02 & 34.14 & 35.14 & 49.56 & 50.04 & 55.22 & 54.82 & 36.47 & 38.27 & 45.06 & 47.39 & 48.07 & 46.10 & 43.21 & 42.29 & 43.01 & 56.71\\
NLI & xnli & ar & validation & MT & Median acc. & 33.33 & 33.33 & 33.33 & 33.33 & 33.49 & 35.06 & 33.61 & 33.33 & 33.25 & 33.78 & 33.29 & 33.37 & 33.33 & 33.41 & 33.33 & 35.14 & 33.37 & 33.25 & 33.33 & 33.53 & 33.90 & 33.49 & 34.66 & 33.53 & 33.33 & 41.97 & 33.37\\
NLI & xnli & ar & validation & MT & Max acc. & 34.22 & 33.45 & 35.42 & 33.69 & 34.54 & 36.67 & 36.95 & 33.45 & 34.10 & 36.27 & 33.33 & 33.53 & 34.22 & 33.94 & 34.18 & 42.85 & 39.48 & 36.18 & 40.32 & 35.94 & 41.89 & 49.12 & 48.55 & 42.89 & 35.50 & 45.42 & 47.51\\
NLI & xnli & bg & validation & EN & Median acc. & 33.37 & 33.33 & 33.33 & 33.41 & 33.33 & 33.37 & 33.13 & 34.66 & 34.30 & 34.50 & 33.86 & 40.44 & 41.49 & 52.65 & 59.24 & 59.80 & 56.79 & 37.27 & 35.46 & 38.59 & 39.36 & 43.49 & 41.20 & 41.65 & 47.19 & 43.69 & 41.16\\
NLI & xnli & bg & validation & EN & Max acc. & 37.23 & 33.45 & 34.66 & 34.78 & 34.62 & 34.66 & 33.90 & 35.66 & 39.92 & 36.59 & 37.55 & 42.33 & 43.94 & 54.18 & 59.88 & 59.92 & 58.23 & 39.76 & 40.40 & 42.17 & 43.82 & 43.61 & 44.90 & 43.98 & 48.43 & 46.75 & 46.63\\
NLI & xnli & bg & validation & MT & Median acc. & 33.33 & 33.33 & 33.33 & 33.37 & 33.33 & 33.09 & 33.05 & 33.33 & 34.34 & 36.63 & 33.33 & 33.69 & 34.10 & 40.72 & 38.39 & 46.67 & 41.93 & 33.33 & 34.74 & 33.33 & 33.33 & 33.49 & 33.65 & 33.86 & 34.10 & 33.41 & 33.41\\
NLI & xnli & bg & validation & MT & Max acc. & 33.33 & 33.73 & 33.33 & 33.45 & 34.62 & 34.70 & 33.49 & 33.33 & 37.19 & 39.44 & 33.65 & 35.54 & 38.31 & 48.55 & 54.70 & 52.53 & 48.23 & 33.33 & 39.80 & 34.10 & 36.27 & 35.02 & 34.66 & 34.58 & 39.20 & 37.43 & 43.98\\
NLI & xnli & de & validation & EN & Median acc. & 33.37 & 33.37 & 34.14 & 33.29 & 33.45 & 33.33 & 33.09 & 43.94 & 36.10 & 35.14 & 33.94 & 41.37 & 42.25 & 52.93 & 60.12 & 59.84 & 57.03 & 37.71 & 36.27 & 41.12 & 40.76 & 45.86 & 44.22 & 41.85 & 53.13 & 45.18 & 46.43\\
NLI & xnli & de & validation & EN & Max acc. & 36.10 & 34.02 & 35.02 & 33.65 & 35.18 & 34.26 & 33.65 & 45.90 & 40.52 & 35.50 & 35.78 & 42.41 & 44.18 & 54.78 & 60.64 & 60.16 & 58.59 & 39.36 & 40.12 & 42.73 & 45.26 & 46.83 & 48.92 & 47.03 & 54.38 & 53.69 & 50.16\\
NLI & xnli & de & validation & MT & Median acc. & 33.13 & 33.29 & 33.37 & 33.37 & 33.33 & 33.41 & 33.41 & 33.94 & 34.30 & 39.32 & 33.33 & 33.33 & 33.41 & 37.87 & 37.51 & 35.26 & 33.41 & 33.41 & 34.78 & 33.61 & 34.26 & 33.69 & 33.86 & 35.70 & 39.56 & 36.79 & 39.32\\
NLI & xnli & de & validation & MT & Max acc. & 36.06 & 33.45 & 33.45 & 33.65 & 34.54 & 34.58 & 36.39 & 40.08 & 35.74 & 41.45 & 33.86 & 33.41 & 34.54 & 47.47 & 51.37 & 45.82 & 50.56 & 35.50 & 35.58 & 38.27 & 37.63 & 36.22 & 37.99 & 36.47 & 42.13 & 38.96 & 44.70\\
NLI & xnli & el & validation & EN & Median acc. & 33.29 & 33.33 & 33.45 & 33.37 & 33.33 & 33.33 & 33.49 & 33.57 & 34.50 & 34.82 & 34.62 & 39.76 & 41.57 & 52.57 & 58.80 & 58.88 & 56.10 & 37.95 & 35.50 & 38.43 & 40.36 & 41.08 & 40.92 & 40.68 & 45.94 & 42.29 & 39.12\\
NLI & xnli & el & validation & EN & Max acc. & 36.83 & 33.90 & 34.02 & 34.58 & 35.54 & 34.42 & 33.73 & 34.10 & 40.08 & 37.31 & 37.43 & 40.92 & 43.94 & 53.78 & 59.00 & 59.20 & 57.35 & 40.96 & 39.32 & 41.81 & 42.61 & 41.53 & 42.89 & 41.89 & 47.43 & 46.55 & 43.05\\
NLI & xnli & el & validation & MT & Median acc. & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 & 33.37 & 33.33 & 33.33 & 33.53 & 34.70 & 33.21 & 33.33 & 33.33 & 33.29 & 45.06 & 34.78 & 33.49 & 33.33 & 33.33 & 33.41 & 33.33 & 33.78 & 33.33 & 33.37 & 34.82 & 34.74 & 33.61\\
NLI & xnli & el & validation & MT & Max acc. & 34.22 & 33.41 & 34.62 & 33.33 & 33.37 & 34.90 & 33.37 & 34.70 & 39.08 & 36.22 & 33.73 & 33.33 & 39.28 & 34.98 & 51.24 & 42.37 & 35.58 & 33.49 & 33.37 & 35.50 & 33.37 & 34.82 & 34.38 & 33.94 & 37.19 & 37.43 & 35.70\\
NLI & xnli & en & validation & EN & Median acc. & 33.49 & 34.38 & 33.61 & 33.61 & 33.57 & 33.29 & 33.49 & 59.12 & 36.79 & 35.66 & 34.10 & 43.13 & 40.60 & 55.18 & 61.24 & 61.93 & 60.36 & 44.62 & 39.04 & 49.84 & 52.21 & 55.38 & 54.38 & 46.35 & 60.92 & 57.47 & 55.02\\
NLI & xnli & en & validation & EN & Max acc. & 36.79 & 35.34 & 34.74 & 35.22 & 37.83 & 33.45 & 36.02 & 60.16 & 41.00 & 36.83 & 38.47 & 43.78 & 44.26 & 56.83 & 62.01 & 62.25 & 61.00 & 46.43 & 47.11 & 55.02 & 57.31 & 59.68 & 58.92 & 55.90 & 67.47 & 61.81 & 59.72\\
NLI & xnli & es & validation & EN & Median acc. & 33.33 & 33.37 & 33.33 & 33.49 & 33.90 & 33.86 & 34.10 & 45.70 & 34.26 & 35.58 & 34.26 & 40.36 & 42.49 & 51.97 & 60.32 & 60.72 & 58.03 & 43.90 & 41.24 & 46.91 & 50.32 & 52.53 & 46.10 & 45.34 & 58.51 & 43.98 & 52.09\\
NLI & xnli & es & validation & EN & Max acc. & 33.41 & 34.38 & 33.98 & 33.86 & 35.98 & 35.94 & 39.48 & 48.67 & 38.96 & 36.27 & 36.75 & 41.93 & 45.34 & 54.78 & 60.80 & 60.92 & 59.40 & 44.98 & 47.55 & 52.97 & 56.14 & 55.10 & 57.35 & 53.73 & 61.24 & 59.12 & 59.32\\
NLI & xnli & es & validation & HT & Median acc. & 33.37 & 33.33 & 33.33 & 33.45 & 33.33 & 33.33 & 33.33 & 33.33 & 34.66 & 36.75 & 33.33 & 33.41 & 33.37 & 35.66 & 38.76 & 54.74 & 33.33 & 33.37 & 34.10 & 33.33 & 33.45 & 33.33 & 37.63 & 33.33 & 33.37 & 46.55 & 33.37\\
NLI & xnli & es & validation & HT & Max acc. & 33.49 & 34.86 & 33.33 & 34.86 & 34.94 & 33.37 & 34.94 & 43.05 & 35.22 & 42.05 & 33.90 & 33.78 & 36.02 & 39.04 & 58.15 & 60.76 & 51.65 & 37.23 & 38.96 & 48.03 & 53.09 & 48.76 & 53.13 & 52.97 & 56.83 & 56.99 & 58.76\\
NLI & xnli & es & validation & MT & Median acc. & 33.45 & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 & 33.61 & 34.22 & 33.82 & 36.27 & 33.29 & 33.33 & 33.37 & 33.53 & 45.54 & 55.34 & 33.45 & 33.33 & 33.45 & 33.33 & 33.45 & 33.33 & 39.80 & 34.50 & 33.37 & 49.52 & 33.90\\
NLI & xnli & es & validation & MT & Max acc. & 34.22 & 35.38 & 34.10 & 34.90 & 34.02 & 35.38 & 36.14 & 37.27 & 40.08 & 39.00 & 33.53 & 33.57 & 34.14 & 47.67 & 55.38 & 60.12 & 47.23 & 43.21 & 34.46 & 43.01 & 51.24 & 55.10 & 53.86 & 52.21 & 57.71 & 53.82 & 58.96\\
NLI & xnli & fr & validation & EN & Median acc. & 33.33 & 33.82 & 33.90 & 33.45 & 34.50 & 34.46 & 33.45 & 45.38 & 35.78 & 35.26 & 34.94 & 40.36 & 40.36 & 52.37 & 59.52 & 59.56 & 58.15 & 44.22 & 42.65 & 48.59 & 50.52 & 52.41 & 51.00 & 47.39 & 57.71 & 53.69 & 51.81\\
NLI & xnli & fr & validation & EN & Max acc. & 35.46 & 35.26 & 34.86 & 33.90 & 37.47 & 37.27 & 36.95 & 46.71 & 36.75 & 35.94 & 37.15 & 42.45 & 42.01 & 54.22 & 59.88 & 59.88 & 58.47 & 45.54 & 48.51 & 52.21 & 55.78 & 55.26 & 56.67 & 53.37 & 61.37 & 59.12 & 57.99\\
NLI & xnli & fr & validation & HT & Median acc. & 33.49 & 33.82 & 33.53 & 34.82 & 34.22 & 33.98 & 33.90 & 33.61 & 35.26 & 36.06 & 33.41 & 33.98 & 34.58 & 35.90 & 39.76 & 55.86 & 33.94 & 33.33 & 35.14 & 37.59 & 34.58 & 46.55 & 48.92 & 42.73 & 49.52 & 51.53 & 48.59\\
NLI & xnli & fr & validation & HT & Max acc. & 36.27 & 34.02 & 35.70 & 35.50 & 35.82 & 34.34 & 35.06 & 47.59 & 41.45 & 36.83 & 34.14 & 38.92 & 37.79 & 47.31 & 57.07 & 58.80 & 50.92 & 43.53 & 47.75 & 46.83 & 51.89 & 53.21 & 53.45 & 47.11 & 58.47 & 56.95 & 55.50\\
NLI & xnli & fr & validation & MT & Median acc. & 34.10 & 33.33 & 33.33 & 33.90 & 32.69 & 33.45 & 33.49 & 33.61 & 37.07 & 35.58 & 33.53 & 34.78 & 34.38 & 35.10 & 37.79 & 55.66 & 34.98 & 33.73 & 34.26 & 37.83 & 36.67 & 41.41 & 47.23 & 41.85 & 46.95 & 52.01 & 49.00\\
NLI & xnli & fr & validation & MT & Max acc. & 34.98 & 34.54 & 35.22 & 34.98 & 33.49 & 34.70 & 35.66 & 34.46 & 43.37 & 35.90 & 35.26 & 38.27 & 36.51 & 44.02 & 57.83 & 58.71 & 50.32 & 40.48 & 46.35 & 42.13 & 51.41 & 48.84 & 52.41 & 47.91 & 57.91 & 54.82 & 55.86\\
NLI & xnli & hi & validation & EN & Median acc. & 33.33 & 33.41 & 33.49 & 34.54 & 33.78 & 33.33 & 33.53 & 33.33 & 34.18 & 34.02 & 34.18 & 38.88 & 38.19 & 47.87 & 56.14 & 57.31 & 54.94 & 39.08 & 37.59 & 44.54 & 46.18 & 48.35 & 44.70 & 41.41 & 53.53 & 44.38 & 45.78\\
NLI & xnli & hi & validation & EN & Max acc. & 34.86 & 34.94 & 34.34 & 35.10 & 37.07 & 35.38 & 36.75 & 33.65 & 39.48 & 34.86 & 35.38 & 39.76 & 41.89 & 50.24 & 57.23 & 57.47 & 55.46 & 41.81 & 42.89 & 48.07 & 51.49 & 50.88 & 53.45 & 49.84 & 56.83 & 52.53 & 55.02\\
NLI & xnli & hi & validation & HT & Median acc. & 33.41 & 33.33 & 33.29 & 33.33 & 33.33 & 33.33 & 34.18 & 33.33 & 34.22 & 34.22 & 33.29 & 36.79 & 37.59 & 39.24 & 47.99 & 44.78 & 34.62 & 37.27 & 35.50 & 36.59 & 34.50 & 40.56 & 37.31 & 43.90 & 44.54 & 49.64 & 47.15\\
NLI & xnli & hi & validation & HT & Max acc. & 34.62 & 34.46 & 34.14 & 33.33 & 33.37 & 33.41 & 35.98 & 35.22 & 38.67 & 37.23 & 34.94 & 37.99 & 41.69 & 48.67 & 56.06 & 56.39 & 53.41 & 40.32 & 41.12 & 41.24 & 43.41 & 42.61 & 47.03 & 49.28 & 60.20 & 52.37 & 52.21\\
NLI & xnli & hi & validation & MT & Median acc. & 33.29 & 33.37 & 33.33 & 33.33 & 33.45 & 33.37 & 34.22 & 33.33 & 35.22 & 33.33 & 33.37 & 34.86 & 34.26 & 41.61 & 47.59 & 36.39 & 33.45 & 34.54 & 37.39 & 36.71 & 33.33 & 33.94 & 34.50 & 33.90 & 38.51 & 44.14 & 38.07\\
NLI & xnli & hi & validation & MT & Max acc. & 33.73 & 33.98 & 33.45 & 34.14 & 33.61 & 33.45 & 36.22 & 33.33 & 38.31 & 36.91 & 34.30 & 36.79 & 39.36 & 47.23 & 50.24 & 48.59 & 33.69 & 37.15 & 39.04 & 39.08 & 36.18 & 36.27 & 36.55 & 39.88 & 41.61 & 49.32 & 43.94\\
NLI & xnli & ru & validation & EN & Median acc. & 33.33 & 33.33 & 33.37 & 33.33 & 33.61 & 33.33 & 33.29 & 38.15 & 33.86 & 35.30 & 33.73 & 41.16 & 41.37 & 49.84 & 57.95 & 57.67 & 55.10 & 36.99 & 35.78 & 41.45 & 43.73 & 46.67 & 44.62 & 43.90 & 52.33 & 46.75 & 44.74\\
NLI & xnli & ru & validation & EN & Max acc. & 36.10 & 34.34 & 35.02 & 33.82 & 35.10 & 35.26 & 34.66 & 41.24 & 38.84 & 37.99 & 37.35 & 41.93 & 42.13 & 53.09 & 58.88 & 58.67 & 56.55 & 39.64 & 42.81 & 45.10 & 47.11 & 47.75 & 50.24 & 46.55 & 54.02 & 52.85 & 50.12\\
NLI & xnli & ru & validation & MT & Median acc. & 33.41 & 33.33 & 33.33 & 33.37 & 33.41 & 34.14 & 33.37 & 33.37 & 34.10 & 36.10 & 35.26 & 33.57 & 33.29 & 39.44 & 39.72 & 33.94 & 34.14 & 33.33 & 33.49 & 33.73 & 33.29 & 33.78 & 34.46 & 33.94 & 42.89 & 38.59 & 41.49\\
NLI & xnli & ru & validation & MT & Max acc. & 33.98 & 33.53 & 33.61 & 33.82 & 34.86 & 36.79 & 33.45 & 33.65 & 37.63 & 38.27 & 36.67 & 35.86 & 35.42 & 42.93 & 56.71 & 48.03 & 38.11 & 33.86 & 33.57 & 36.63 & 33.57 & 38.96 & 37.31 & 34.94 & 45.34 & 46.31 & 43.49\\
NLI & xnli & sw & validation & EN & Median acc. & 33.25 & 33.82 & 33.45 & 33.53 & 33.98 & 33.33 & 33.21 & 33.73 & 34.82 & 34.14 & 33.94 & 38.23 & 39.64 & 45.78 & 55.46 & 55.70 & 53.25 & 37.19 & 35.50 & 41.20 & 41.77 & 43.90 & 42.29 & 36.51 & 50.36 & 43.98 & 37.27\\
NLI & xnli & sw & validation & EN & Max acc. & 34.82 & 34.94 & 35.46 & 34.46 & 36.75 & 36.55 & 33.73 & 33.78 & 37.79 & 35.46 & 35.18 & 39.68 & 40.08 & 49.60 & 55.66 & 56.79 & 53.73 & 38.35 & 41.29 & 44.34 & 47.83 & 46.63 & 48.27 & 43.49 & 52.09 & 50.36 & 50.04\\
NLI & xnli & sw & validation & HT & Median acc. & 33.45 & 33.33 & 33.41 & 33.33 & 33.33 & 33.37 & 34.54 & 33.94 & 35.02 & 34.58 & 33.41 & 33.33 & 33.57 & 37.75 & 41.73 & 46.95 & 43.25 & 34.54 & 33.53 & 34.02 & 33.41 & 35.70 & 33.61 & 34.10 & 34.98 & 39.40 & 34.54\\
NLI & xnli & sw & validation & HT & Max acc. & 35.54 & 34.50 & 33.45 & 33.41 & 33.37 & 35.02 & 35.94 & 34.58 & 35.42 & 37.19 & 34.02 & 35.46 & 36.59 & 46.31 & 52.37 & 49.60 & 49.68 & 35.14 & 33.98 & 34.94 & 36.10 & 35.94 & 35.58 & 37.19 & 37.71 & 42.85 & 35.02\\
NLI & xnli & sw & validation & MT & Median acc. & 33.57 & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 & 33.41 & 33.33 & 33.25 & 35.18 & 33.33 & 33.33 & 33.33 & 35.98 & 33.33 & 33.37 & 34.58 & 33.33 & 33.53 & 33.57 & 32.97 & 33.41 & 33.37 & 33.33 & 35.82 & 35.10 & 33.37\\
NLI & xnli & sw & validation & MT & Max acc. & 34.22 & 33.33 & 33.45 & 34.38 & 33.37 & 34.62 & 35.46 & 34.06 & 35.02 & 37.11 & 33.57 & 34.82 & 34.78 & 38.03 & 39.64 & 37.55 & 41.45 & 34.34 & 34.98 & 33.94 & 33.53 & 37.03 & 33.41 & 33.41 & 36.27 & 36.83 & 33.57\\
NLI & xnli & th & validation & EN & Median acc. & 33.37 & 33.73 & 33.37 & 33.41 & 33.33 & 33.41 & 33.33 & 33.69 & 34.74 & 34.58 & 33.41 & 40.92 & 39.68 & 45.34 & 56.31 & 57.19 & 54.98 & 34.14 & 33.61 & 38.88 & 38.19 & 39.00 & 39.48 & 41.65 & 41.45 & 41.33 & 37.31\\
NLI & xnli & th & validation & EN & Max acc. & 35.22 & 33.86 & 33.90 & 34.46 & 34.02 & 33.82 & 36.31 & 34.70 & 39.88 & 35.46 & 37.55 & 41.97 & 40.80 & 52.13 & 57.43 & 58.03 & 56.02 & 35.50 & 42.93 & 40.36 & 42.93 & 40.12 & 41.08 & 43.17 & 43.78 & 43.98 & 42.29\\
NLI & xnli & th & validation & MT & Median acc. & 33.53 & 33.33 & 33.37 & 33.33 & 33.33 & 33.33 & 34.58 & 33.33 & 34.06 & 35.54 & 33.25 & 35.34 & 33.57 & 35.78 & 38.55 & 39.20 & 40.32 & 33.57 & 33.33 & 33.29 & 33.65 & 32.53 & 33.78 & 33.94 & 33.29 & 34.10 & 34.02\\
NLI & xnli & th & validation & MT & Max acc. & 35.46 & 33.69 & 33.69 & 33.41 & 35.18 & 36.39 & 40.44 & 33.33 & 35.98 & 36.06 & 33.57 & 35.82 & 33.69 & 40.84 & 52.45 & 57.95 & 49.08 & 34.94 & 34.30 & 34.22 & 33.98 & 34.78 & 34.90 & 35.38 & 36.27 & 37.63 & 36.99\\
NLI & xnli & tr & validation & EN & Median acc. & 33.33 & 33.33 & 33.33 & 33.33 & 33.37 & 33.33 & 34.22 & 34.14 & 35.26 & 34.62 & 34.26 & 39.16 & 40.76 & 48.71 & 56.75 & 56.39 & 54.34 & 36.95 & 33.98 & 35.46 & 36.06 & 36.14 & 37.47 & 38.31 & 42.73 & 39.76 & 39.08\\
NLI & xnli & tr & validation & EN & Max acc. & 36.51 & 33.73 & 33.57 & 33.37 & 33.41 & 35.18 & 34.66 & 34.90 & 40.24 & 36.79 & 36.51 & 40.28 & 41.29 & 50.56 & 57.59 & 57.67 & 55.38 & 37.31 & 37.51 & 37.15 & 37.23 & 37.55 & 38.71 & 40.44 & 45.70 & 43.78 & 43.78\\
NLI & xnli & tr & validation & MT & Median acc. & 33.33 & 33.33 & 33.41 & 33.33 & 33.33 & 33.21 & 33.37 & 33.49 & 34.34 & 34.02 & 33.29 & 33.21 & 33.94 & 34.06 & 38.80 & 38.67 & 38.67 & 33.49 & 33.33 & 33.33 & 33.25 & 33.33 & 33.37 & 34.46 & 33.33 & 33.37 & 33.82\\
NLI & xnli & tr & validation & MT & Max acc. & 33.45 & 33.41 & 34.62 & 34.34 & 33.98 & 33.49 & 34.02 & 34.02 & 34.86 & 38.80 & 34.26 & 35.94 & 34.46 & 37.63 & 48.35 & 54.98 & 46.99 & 36.67 & 33.61 & 35.34 & 34.46 & 33.73 & 33.98 & 36.18 & 37.27 & 37.27 & 40.88\\
NLI & xnli & ur & validation & EN & Median acc. & 33.05 & 33.61 & 33.37 & 33.69 & 33.41 & 34.42 & 34.02 & 33.13 & 33.29 & 34.14 & 34.50 & 36.59 & 37.07 & 46.67 & 54.70 & 54.58 & 53.57 & 36.67 & 35.26 & 39.88 & 42.89 & 43.94 & 40.84 & 40.12 & 49.96 & 45.86 & 40.28\\
NLI & xnli & ur & validation & EN & Max acc. & 34.10 & 33.69 & 34.58 & 34.78 & 34.30 & 35.82 & 34.26 & 33.33 & 38.43 & 34.62 & 35.78 & 38.71 & 39.80 & 47.91 & 55.42 & 55.98 & 54.02 & 38.96 & 41.37 & 44.38 & 49.04 & 46.51 & 49.48 & 45.18 & 50.80 & 51.24 & 51.81\\
NLI & xnli & ur & validation & HT & Median acc. & 33.90 & 33.61 & 33.09 & 33.53 & 32.93 & 31.69 & 33.69 & 33.25 & 34.86 & 34.10 & 34.30 & 34.70 & 35.74 & 35.50 & 48.92 & 45.42 & 35.66 & 34.18 & 33.37 & 37.95 & 38.35 & 34.26 & 35.62 & 36.10 & 34.62 & 40.44 & 38.92\\
NLI & xnli & ur & validation & HT & Max acc. & 34.98 & 34.06 & 33.37 & 33.78 & 33.53 & 33.13 & 35.06 & 33.53 & 35.78 & 35.86 & 35.14 & 37.23 & 39.88 & 41.12 & 52.17 & 53.82 & 46.87 & 36.79 & 33.86 & 39.92 & 41.49 & 41.77 & 40.00 & 37.67 & 41.77 & 46.39 & 46.95\\
NLI & xnli & ur & validation & MT & Median acc. & 33.33 & 33.33 & 33.33 & 33.25 & 33.25 & 33.25 & 33.29 & 33.45 & 33.33 & 33.49 & 33.61 & 35.02 & 33.49 & 40.08 & 40.32 & 36.10 & 33.49 & 34.14 & 33.33 & 33.29 & 33.33 & 33.33 & 33.37 & 33.37 & 33.33 & 33.82 & 36.67\\
NLI & xnli & ur & validation & MT & Max acc. & 34.02 & 33.45 & 33.45 & 33.53 & 33.29 & 33.33 & 34.66 & 33.57 & 34.06 & 34.78 & 34.22 & 35.46 & 38.88 & 42.69 & 53.78 & 51.81 & 49.80 & 36.02 & 33.33 & 33.33 & 33.45 & 35.66 & 33.78 & 37.51 & 35.50 & 35.86 & 37.87\\
NLI & xnli & vi & validation & EN & Median acc. & 33.33 & 34.42 & 33.37 & 33.45 & 33.57 & 33.69 & 33.49 & 34.70 & 34.98 & 34.58 & 35.46 & 39.60 & 39.76 & 52.45 & 58.19 & 58.35 & 56.39 & 43.65 & 40.52 & 46.35 & 46.22 & 50.08 & 46.51 & 43.61 & 55.78 & 48.27 & 49.80\\
NLI & xnli & vi & validation & EN & Max acc. & 37.79 & 35.26 & 34.54 & 34.82 & 37.91 & 37.19 & 33.73 & 35.58 & 38.27 & 35.14 & 36.95 & 40.20 & 41.81 & 53.21 & 58.51 & 58.92 & 57.11 & 44.74 & 47.19 & 51.08 & 53.98 & 52.93 & 54.50 & 51.97 & 61.00 & 55.82 & 57.27\\
NLI & xnli & vi & validation & HT & Median acc. & 33.41 & 33.05 & 33.41 & 33.29 & 33.37 & 32.97 & 33.73 & 33.21 & 34.02 & 37.63 & 33.13 & 33.78 & 34.98 & 41.37 & 43.57 & 33.45 & 45.78 & 37.23 & 37.79 & 35.94 & 40.92 & 41.24 & 50.28 & 33.57 & 39.60 & 46.55 & 33.61\\
NLI & xnli & vi & validation & HT & Max acc. & 34.86 & 33.53 & 33.61 & 33.78 & 34.14 & 33.53 & 34.46 & 33.25 & 37.51 & 38.63 & 33.61 & 37.99 & 39.56 & 46.31 & 56.14 & 56.75 & 49.24 & 39.20 & 42.21 & 44.14 & 43.29 & 47.59 & 52.65 & 39.76 & 46.99 & 54.82 & 48.03\\
NLI & xnli & vi & validation & MT & Median acc. & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 & 33.41 & 33.98 & 33.33 & 33.33 & 33.33 & 33.33 & 33.78 & 33.33 & 33.73 & 33.57 & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 & 33.33 & 33.33\\
NLI & xnli & vi & validation & MT & Max acc. & 34.62 & 33.41 & 33.82 & 34.66 & 34.14 & 34.02 & 35.02 & 34.06 & 33.69 & 34.70 & 33.37 & 33.94 & 34.34 & 33.53 & 36.79 & 33.94 & 35.82 & 37.03 & 34.66 & 34.42 & 33.57 & 33.94 & 38.88 & 34.26 & 39.20 & 33.33 & 33.90\\
NLI & xnli & zh & validation & EN & Median acc. & 33.41 & 35.06 & 33.41 & 33.33 & 35.54 & 34.10 & 33.73 & 33.33 & 34.62 & 34.74 & 36.14 & 41.37 & 38.92 & 44.54 & 57.11 & 57.39 & 53.90 & 43.94 & 39.40 & 48.11 & 47.71 & 51.24 & 47.07 & 47.39 & 56.22 & 44.02 & 48.47\\
NLI & xnli & zh & validation & EN & Max acc. & 35.14 & 36.31 & 34.22 & 36.95 & 36.67 & 37.27 & 34.86 & 33.82 & 41.85 & 35.10 & 37.07 & 42.49 & 40.84 & 50.64 & 59.12 & 58.71 & 55.22 & 44.66 & 47.63 & 51.12 & 54.18 & 53.61 & 54.30 & 52.29 & 56.91 & 55.50 & 56.95\\
NLI & xnli & zh & validation & HT & Median acc. & 33.45 & 33.33 & 33.33 & 33.33 & 33.33 & 33.37 & 33.53 & 33.33 & 33.25 & 34.70 & 33.13 & 33.78 & 34.10 & 40.24 & 46.83 & 50.40 & 39.00 & 34.70 & 33.37 & 33.45 & 33.69 & 35.38 & 34.46 & 39.32 & 33.69 & 36.95 & 52.89\\
NLI & xnli & zh & validation & HT & Max acc. & 33.86 & 33.37 & 33.45 & 33.57 & 34.74 & 35.02 & 36.91 & 33.33 & 37.63 & 37.51 & 34.02 & 35.22 & 35.66 & 41.89 & 55.98 & 56.99 & 51.49 & 37.43 & 38.96 & 34.06 & 42.09 & 44.10 & 40.64 & 45.10 & 41.37 & 49.24 & 53.69\\
NLI & xnli & zh & validation & MT & Median acc. & 33.69 & 33.25 & 33.25 & 32.61 & 34.38 & 33.41 & 33.82 & 33.33 & 34.42 & 34.46 & 32.85 & 33.69 & 34.54 & 36.31 & 48.92 & 54.86 & 33.33 & 33.82 & 33.98 & 34.30 & 34.06 & 34.98 & 46.35 & 35.10 & 34.38 & 49.56 & 38.96\\
NLI & xnli & zh & validation & MT & Max acc. & 35.62 & 33.49 & 33.45 & 33.86 & 34.54 & 34.02 & 34.70 & 33.33 & 36.63 & 35.30 & 34.10 & 34.82 & 35.90 & 39.12 & 51.49 & 56.87 & 39.28 & 35.58 & 36.95 & 36.63 & 39.16 & 42.41 & 48.92 & 39.60 & 50.60 & 52.25 & 44.14\\
\midrule
Program synthesis & openai\_humaneval & None & test & EN & Pass@1 & - & 0.82 & 2.48 & 4.03 & 6.48 & 7.73 & 15.52 & - & - & - & - & - & - & 0.00 & - & - & - & 2.18 & 2.62 & 4.38 & 6.29 & 8.06 & 7.23 & 1.55 & 12.06 & 13.55 & 6.13\\
Program synthesis & openai\_humaneval & None & test & EN & Pass@10 & - & 3.02 & 5.93 & 7.45 & 11.35 & 17.38 & 32.20 & - & - & - & - & - & - & 0.00 & - & - & - & 4.11 & 6.22 & 8.73 & 11.94 & 15.03 & 14.46 & 4.12 & 26.53 & 26.26 & 11.79\\
Program synthesis & openai\_humaneval & None & test & EN & Pass@100 & - & 6.23 & 9.62 & 12.75 & 20.43 & 29.47 & 55.45 & - & - & - & - & - & - & 0.00 & - & - & - & 9.00 & 11.68 & 16.09 & 19.06 & 27.49 & 25.86 & 9.60 & 48.44 & 47.01 & 18.73\\
\midrule
Sent. completion & story\_cloze & 2016 & validation & EN & Median acc. & 51.68 & 50.08 & 47.25 & 48.48 & 47.51 & 49.44 & 50.99 & 94.71 & 46.71 & 48.21 & 52.38 & 57.14 & 58.69 & 77.61 & 95.40 & 93.85 & 96.31 & 58.52 & 59.01 & 79.64 & 85.20 & 89.10 & 88.51 & 84.66 & 95.67 & 95.83 & 94.01\\
Sent. completion & story\_cloze & 2016 & validation & EN & Max acc. & 66.27 & 59.43 & 62.05 & 64.30 & 66.44 & 70.92 & 76.22 & 94.92 & 52.27 & 57.08 & 54.36 & 57.83 & 59.49 & 79.10 & 96.04 & 94.66 & 96.63 & 60.29 & 62.75 & 82.90 & 87.33 & 90.43 & 89.58 & 87.07 & 96.26 & 96.69 & 94.66\\
Sent. completion & super\_glue & copa & validation & EN & Median acc. & 55.00 & 55.00 & 57.00 & 54.00 & 62.00 & 69.00 & 55.00 & 93.00 & 53.00 & 58.00 & 51.00 & 53.00 & 65.00 & 66.00 & 90.00 & 86.00 & 90.00 & 51.00 & 58.00 & 66.00 & 73.00 & 83.00 & 80.00 & 78.00 & 88.00 & 90.00 & 89.00\\
Sent. completion & super\_glue & copa & validation & EN & Max acc. & 67.00 & 67.00 & 62.00 & 65.00 & 66.00 & 78.00 & 75.00 & 94.00 & 54.00 & 66.00 & 57.00 & 55.00 & 65.00 & 72.00 & 93.00 & 88.00 & 91.00 & 52.00 & 63.00 & 69.00 & 76.00 & 86.00 & 84.00 & 81.00 & 91.00 & 91.00 & 91.00\\
Sent. completion & xcopa & et & validation & EN & Median acc. & 57.00 & 53.00 & 50.00 & 53.00 & 50.00 & 53.00 & 51.00 & 53.00 & 52.00 & 51.00 & 53.00 & 49.00 & 53.00 & 65.00 & 72.00 & 79.00 & 75.00 & 48.00 & 49.00 & 48.00 & 50.00 & 49.00 & 51.00 & 52.00 & 48.00 & 52.00 & 49.00\\
Sent. completion & xcopa & et & validation & EN & Max acc. & 58.00 & 58.00 & 56.00 & 57.00 & 57.00 & 57.00 & 52.00 & 55.00 & 56.00 & 61.00 & 57.00 & 51.00 & 56.00 & 70.00 & 75.00 & 81.00 & 79.00 & 53.00 & 55.00 & 50.00 & 51.00 & 50.00 & 51.00 & 57.00 & 50.00 & 54.00 & 53.00\\
Sent. completion & xcopa & et & validation & MT & Median acc. & 56.00 & 56.00 & 54.00 & 51.00 & 53.00 & 53.00 & 46.00 & 49.00 & 55.00 & 56.00 & 53.00 & 50.00 & 54.00 & 60.00 & 74.00 & 76.00 & 75.00 & 48.00 & 53.00 & 47.00 & 49.00 & 47.00 & 47.00 & 48.00 & 49.00 & 48.00 & 46.00\\
Sent. completion & xcopa & et & validation & MT & Max acc. & 57.00 & 60.00 & 58.00 & 55.00 & 61.00 & 56.00 & 54.00 & 54.00 & 58.00 & 64.00 & 56.00 & 52.00 & 55.00 & 69.00 & 79.00 & 79.00 & 77.00 & 50.00 & 54.00 & 48.00 & 53.00 & 48.00 & 52.00 & 52.00 & 51.00 & 52.00 & 53.00\\
Sent. completion & xcopa & ht & validation & EN & Median acc. & 52.00 & 48.00 & 48.00 & 52.00 & 53.00 & 52.00 & 56.00 & 47.00 & 53.00 & 47.00 & 55.00 & 55.00 & 59.00 & 60.00 & 76.00 & 76.00 & 75.00 & 42.00 & 46.00 & 48.00 & 52.00 & 49.00 & 51.00 & 51.00 & 55.00 & 51.00 & 51.00\\
Sent. completion & xcopa & ht & validation & EN & Max acc. & 53.00 & 53.00 & 56.00 & 56.00 & 57.00 & 59.00 & 57.00 & 51.00 & 63.00 & 54.00 & 60.00 & 59.00 & 62.00 & 66.00 & 79.00 & 79.00 & 77.00 & 49.00 & 52.00 & 51.00 & 62.00 & 54.00 & 52.00 & 55.00 & 58.00 & 55.00 & 56.00\\
Sent. completion & xcopa & ht & validation & MT & Median acc. & 53.00 & 52.00 & 54.00 & 50.00 & 55.00 & 53.00 & 57.00 & 49.00 & 58.00 & 45.00 & 54.00 & 55.00 & 54.00 & 60.00 & 72.00 & 75.00 & 73.00 & 45.00 & 44.00 & 47.00 & 49.00 & 50.00 & 51.00 & 54.00 & 53.00 & 50.00 & 56.00\\
Sent. completion & xcopa & ht & validation & MT & Max acc. & 57.00 & 53.00 & 62.00 & 57.00 & 59.00 & 56.00 & 66.00 & 56.00 & 58.00 & 52.00 & 60.00 & 60.00 & 58.00 & 61.00 & 81.00 & 78.00 & 80.00 & 47.00 & 51.00 & 54.00 & 64.00 & 52.00 & 54.00 & 56.00 & 56.00 & 54.00 & 58.00\\
Sent. completion & xcopa & id & validation & EN & Median acc. & 51.00 & 53.00 & 51.00 & 56.00 & 50.00 & 54.00 & 54.00 & 55.00 & 48.00 & 51.00 & 56.00 & 50.00 & 59.00 & 65.00 & 90.00 & 88.00 & 84.00 & 50.00 & 59.00 & 58.00 & 66.00 & 70.00 & 70.00 & 65.00 & 79.00 & 83.00 & 78.00\\
Sent. completion & xcopa & id & validation & EN & Max acc. & 54.00 & 56.00 & 61.00 & 58.00 & 53.00 & 59.00 & 62.00 & 58.00 & 52.00 & 53.00 & 58.00 & 54.00 & 59.00 & 70.00 & 92.00 & 90.00 & 86.00 & 57.00 & 60.00 & 61.00 & 70.00 & 76.00 & 73.00 & 67.00 & 86.00 & 87.00 & 82.00\\
Sent. completion & xcopa & id & validation & MT & Median acc. & 52.00 & 55.00 & 51.00 & 59.00 & 53.00 & 53.00 & 56.00 & 56.00 & 47.00 & 53.00 & 53.00 & 53.00 & 55.00 & 59.00 & 90.00 & 88.00 & 84.00 & 53.00 & 56.00 & 56.00 & 60.00 & 62.00 & 64.00 & 64.00 & 76.00 & 83.00 & 75.00\\
Sent. completion & xcopa & id & validation & MT & Max acc. & 58.00 & 60.00 & 60.00 & 61.00 & 58.00 & 57.00 & 61.00 & 59.00 & 51.00 & 57.00 & 59.00 & 55.00 & 61.00 & 71.00 & 91.00 & 89.00 & 87.00 & 54.00 & 57.00 & 59.00 & 64.00 & 67.00 & 71.00 & 70.00 & 82.00 & 84.00 & 87.00\\
Sent. completion & xcopa & it & validation & EN & Median acc. & 49.00 & 56.00 & 50.00 & 46.00 & 55.00 & 50.00 & 54.00 & 66.00 & 55.00 & 54.00 & 53.00 & 55.00 & 60.00 & 64.00 & 87.00 & 85.00 & 87.00 & 51.00 & 51.00 & 45.00 & 50.00 & 59.00 & 57.00 & 57.00 & 72.00 & 69.00 & 69.00\\
Sent. completion & xcopa & it & validation & EN & Max acc. & 52.00 & 59.00 & 55.00 & 55.00 & 60.00 & 55.00 & 56.00 & 68.00 & 58.00 & 55.00 & 57.00 & 61.00 & 61.00 & 69.00 & 90.00 & 88.00 & 90.00 & 52.00 & 55.00 & 48.00 & 53.00 & 61.00 & 62.00 & 60.00 & 74.00 & 72.00 & 74.00\\
Sent. completion & xcopa & it & validation & MT & Median acc. & 53.00 & 53.00 & 53.00 & 45.00 & 54.00 & 52.00 & 54.00 & 63.00 & 53.00 & 56.00 & 57.00 & 54.00 & 59.00 & 66.00 & 84.00 & 84.00 & 85.00 & 49.00 & 54.00 & 43.00 & 48.00 & 55.00 & 57.00 & 55.00 & 69.00 & 69.00 & 68.00\\
Sent. completion & xcopa & it & validation & MT & Max acc. & 55.00 & 58.00 & 55.00 & 48.00 & 57.00 & 55.00 & 57.00 & 72.00 & 59.00 & 57.00 & 59.00 & 56.00 & 63.00 & 70.00 & 88.00 & 86.00 & 88.00 & 52.00 & 56.00 & 49.00 & 51.00 & 57.00 & 60.00 & 58.00 & 73.00 & 74.00 & 71.00\\
Sent. completion & xcopa & qu & validation & EN & Median acc. & 59.00 & 54.00 & 48.00 & 56.00 & 59.00 & 61.00 & 56.00 & 52.00 & 48.00 & 48.00 & 47.00 & 52.00 & 54.00 & 53.00 & 58.00 & 54.00 & 48.00 & 54.00 & 44.00 & 52.00 & 51.00 & 45.00 & 48.00 & 50.00 & 49.00 & 51.00 & 51.00\\
Sent. completion & xcopa & qu & validation & EN & Max acc. & 61.00 & 56.00 & 56.00 & 58.00 & 59.00 & 65.00 & 59.00 & 58.00 & 55.00 & 53.00 & 51.00 & 53.00 & 57.00 & 55.00 & 58.00 & 56.00 & 49.00 & 55.00 & 50.00 & 56.00 & 56.00 & 60.00 & 60.00 & 54.00 & 56.00 & 53.00 & 54.00\\
Sent. completion & xcopa & qu & validation & MT & Median acc. & 60.00 & 49.00 & 50.00 & 55.00 & 52.00 & 60.00 & 51.00 & 57.00 & 47.00 & 53.00 & 49.00 & 53.00 & 54.00 & 54.00 & 56.00 & 54.00 & 53.00 & 53.00 & 47.00 & 50.00 & 45.00 & 51.00 & 46.00 & 50.00 & 49.00 & 50.00 & 51.00\\
Sent. completion & xcopa & qu & validation & MT & Max acc. & 63.00 & 60.00 & 58.00 & 57.00 & 55.00 & 63.00 & 60.00 & 60.00 & 51.00 & 55.00 & 54.00 & 55.00 & 56.00 & 56.00 & 59.00 & 55.00 & 56.00 & 55.00 & 56.00 & 54.00 & 50.00 & 60.00 & 61.00 & 51.00 & 53.00 & 56.00 & 57.00\\
Sent. completion & xcopa & sw & validation & EN & Median acc. & 50.00 & 51.00 & 52.00 & 53.00 & 48.00 & 52.00 & 58.00 & 57.00 & 45.00 & 47.00 & 50.00 & 54.00 & 53.00 & 48.00 & 70.00 & 76.00 & 71.00 & 52.00 & 56.00 & 53.00 & 55.00 & 47.00 & 54.00 & 48.00 & 60.00 & 64.00 & 56.00\\
Sent. completion & xcopa & sw & validation & EN & Max acc. & 56.00 & 59.00 & 61.00 & 61.00 & 58.00 & 59.00 & 65.00 & 58.00 & 58.00 & 50.00 & 53.00 & 59.00 & 54.00 & 53.00 & 73.00 & 81.00 & 74.00 & 55.00 & 64.00 & 55.00 & 66.00 & 63.00 & 60.00 & 58.00 & 64.00 & 66.00 & 58.00\\
Sent. completion & xcopa & sw & validation & MT & Median acc. & 53.00 & 49.00 & 49.00 & 49.00 & 53.00 & 53.00 & 60.00 & 57.00 & 46.00 & 47.00 & 50.00 & 54.00 & 51.00 & 52.00 & 77.00 & 76.00 & 72.00 & 53.00 & 57.00 & 53.00 & 54.00 & 59.00 & 59.00 & 56.00 & 63.00 & 62.00 & 58.00\\
Sent. completion & xcopa & sw & validation & MT & Max acc. & 57.00 & 60.00 & 62.00 & 59.00 & 57.00 & 55.00 & 62.00 & 59.00 & 54.00 & 52.00 & 55.00 & 58.00 & 54.00 & 53.00 & 79.00 & 78.00 & 75.00 & 56.00 & 61.00 & 57.00 & 62.00 & 60.00 & 61.00 & 62.00 & 64.00 & 67.00 & 61.00\\
Sent. completion & xcopa & ta & validation & EN & Median acc. & 52.00 & 48.00 & 54.00 & 53.00 & 58.00 & 54.00 & 53.00 & 55.00 & 57.00 & 50.00 & 57.00 & 60.00 & 59.00 & 60.00 & 84.00 & 78.00 & 79.00 & 48.00 & 54.00 & 52.00 & 55.00 & 55.00 & 59.00 & 69.00 & 67.00 & 66.00 & 66.00\\
Sent. completion & xcopa & ta & validation & EN & Max acc. & 61.00 & 55.00 & 54.00 & 59.00 & 61.00 & 56.00 & 63.00 & 62.00 & 61.00 & 59.00 & 59.00 & 63.00 & 61.00 & 62.00 & 84.00 & 79.00 & 84.00 & 50.00 & 57.00 & 56.00 & 59.00 & 57.00 & 62.00 & 71.00 & 69.00 & 70.00 & 69.00\\
Sent. completion & xcopa & ta & validation & MT & Median acc. & 54.00 & 44.00 & 55.00 & 53.00 & 57.00 & 56.00 & 59.00 & 55.00 & 50.00 & 52.00 & 57.00 & 60.00 & 61.00 & 55.00 & 77.00 & 74.00 & 71.00 & 46.00 & 52.00 & 50.00 & 54.00 & 61.00 & 56.00 & 63.00 & 62.00 & 63.00 & 63.00\\
Sent. completion & xcopa & ta & validation & MT & Max acc. & 58.00 & 55.00 & 60.00 & 55.00 & 62.00 & 57.00 & 68.00 & 58.00 & 60.00 & 62.00 & 58.00 & 61.00 & 62.00 & 64.00 & 80.00 & 81.00 & 82.00 & 58.00 & 55.00 & 54.00 & 57.00 & 64.00 & 60.00 & 66.00 & 64.00 & 64.00 & 69.00\\
Sent. completion & xcopa & th & validation & EN & Median acc. & 54.00 & 53.00 & 53.00 & 54.00 & 52.00 & 50.00 & 55.00 & 55.00 & 52.00 & 55.00 & 60.00 & 50.00 & 51.00 & 56.00 & 73.00 & 71.00 & 74.00 & 54.00 & 55.00 & 56.00 & 54.00 & 57.00 & 55.00 & 56.00 & 54.00 & 51.00 & 51.00\\
Sent. completion & xcopa & th & validation & EN & Max acc. & 55.00 & 57.00 & 53.00 & 56.00 & 54.00 & 52.00 & 59.00 & 55.00 & 56.00 & 57.00 & 65.00 & 51.00 & 53.00 & 60.00 & 74.00 & 74.00 & 77.00 & 58.00 & 59.00 & 60.00 & 55.00 & 57.00 & 61.00 & 63.00 & 58.00 & 53.00 & 59.00\\
Sent. completion & xcopa & th & validation & MT & Median acc. & 53.00 & 52.00 & 52.00 & 52.00 & 48.00 & 45.00 & 55.00 & 55.00 & 52.00 & 58.00 & 56.00 & 51.00 & 53.00 & 54.00 & 71.00 & 72.00 & 72.00 & 54.00 & 50.00 & 56.00 & 55.00 & 51.00 & 51.00 & 55.00 & 52.00 & 51.00 & 54.00\\
Sent. completion & xcopa & th & validation & MT & Max acc. & 55.00 & 54.00 & 59.00 & 55.00 & 52.00 & 54.00 & 57.00 & 55.00 & 58.00 & 63.00 & 59.00 & 55.00 & 57.00 & 58.00 & 77.00 & 76.00 & 76.00 & 57.00 & 58.00 & 59.00 & 63.00 & 56.00 & 56.00 & 56.00 & 57.00 & 53.00 & 61.00\\
Sent. completion & xcopa & tr & validation & EN & Median acc. & 48.00 & 56.00 & 54.00 & 51.00 & 52.00 & 49.00 & 49.00 & 49.00 & 48.00 & 49.00 & 51.00 & 55.00 & 47.00 & 55.00 & 73.00 & 73.00 & 74.00 & 55.00 & 47.00 & 55.00 & 53.00 & 49.00 & 55.00 & 53.00 & 54.00 & 51.00 & 54.00\\
Sent. completion & xcopa & tr & validation & EN & Max acc. & 56.00 & 60.00 & 55.00 & 55.00 & 55.00 & 52.00 & 50.00 & 51.00 & 51.00 & 56.00 & 58.00 & 58.00 & 49.00 & 57.00 & 79.00 & 76.00 & 76.00 & 58.00 & 55.00 & 59.00 & 58.00 & 53.00 & 56.00 & 55.00 & 57.00 & 54.00 & 55.00\\
Sent. completion & xcopa & tr & validation & MT & Median acc. & 53.00 & 55.00 & 50.00 & 50.00 & 48.00 & 42.00 & 51.00 & 49.00 & 50.00 & 49.00 & 52.00 & 56.00 & 50.00 & 55.00 & 77.00 & 73.00 & 73.00 & 55.00 & 52.00 & 58.00 & 54.00 & 44.00 & 48.00 & 54.00 & 51.00 & 50.00 & 55.00\\
Sent. completion & xcopa & tr & validation & MT & Max acc. & 56.00 & 57.00 & 56.00 & 58.00 & 54.00 & 49.00 & 55.00 & 53.00 & 56.00 & 58.00 & 57.00 & 60.00 & 57.00 & 58.00 & 79.00 & 76.00 & 74.00 & 61.00 & 54.00 & 59.00 & 61.00 & 48.00 & 51.00 & 58.00 & 54.00 & 53.00 & 56.00\\
Sent. completion & xcopa & vi & validation & EN & Median acc. & 51.00 & 60.00 & 50.00 & 59.00 & 54.00 & 59.00 & 56.00 & 49.00 & 51.00 & 54.00 & 43.00 & 51.00 & 57.00 & 62.00 & 85.00 & 83.00 & 82.00 & 56.00 & 58.00 & 68.00 & 73.00 & 78.00 & 71.00 & 65.00 & 84.00 & 84.00 & 74.00\\
Sent. completion & xcopa & vi & validation & EN & Max acc. & 56.00 & 62.00 & 59.00 & 64.00 & 57.00 & 62.00 & 59.00 & 53.00 & 52.00 & 61.00 & 54.00 & 52.00 & 63.00 & 68.00 & 87.00 & 85.00 & 84.00 & 61.00 & 63.00 & 70.00 & 77.00 & 79.00 & 72.00 & 67.00 & 87.00 & 91.00 & 79.00\\
Sent. completion & xcopa & vi & validation & MT & Median acc. & 52.00 & 61.00 & 52.00 & 57.00 & 59.00 & 62.00 & 57.00 & 61.00 & 50.00 & 57.00 & 48.00 & 50.00 & 57.00 & 60.00 & 87.00 & 84.00 & 81.00 & 54.00 & 57.00 & 63.00 & 64.00 & 68.00 & 72.00 & 61.00 & 82.00 & 85.00 & 76.00\\
Sent. completion & xcopa & vi & validation & MT & Max acc. & 57.00 & 66.00 & 65.00 & 65.00 & 69.00 & 67.00 & 68.00 & 65.00 & 54.00 & 61.00 & 52.00 & 52.00 & 63.00 & 64.00 & 88.00 & 84.00 & 83.00 & 57.00 & 57.00 & 67.00 & 65.00 & 74.00 & 77.00 & 64.00 & 84.00 & 89.00 & 81.00\\
Sent. completion & xcopa & zh & validation & EN & Median acc. & 55.00 & 51.00 & 49.00 & 57.00 & 51.00 & 60.00 & 56.00 & 55.00 & 51.00 & 53.00 & 53.00 & 53.00 & 56.00 & 63.00 & 85.00 & 83.00 & 79.00 & 55.00 & 55.00 & 62.00 & 72.00 & 76.00 & 77.00 & 72.00 & 86.00 & 84.00 & 80.00\\
Sent. completion & xcopa & zh & validation & EN & Max acc. & 58.00 & 61.00 & 63.00 & 73.00 & 66.00 & 68.00 & 72.00 & 55.00 & 52.00 & 65.00 & 54.00 & 58.00 & 58.00 & 65.00 & 89.00 & 86.00 & 79.00 & 61.00 & 61.00 & 66.00 & 73.00 & 80.00 & 80.00 & 77.00 & 90.00 & 86.00 & 82.00\\
Sent. completion & xcopa & zh & validation & HT & Median acc. & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 76.00 & - & 75.00 & - & - & -\\
Sent. completion & xcopa & zh & validation & HT & Max acc. & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 78.00 & - & 79.00 & - & - & -\\
Sent. completion & xcopa & zh & validation & MT & Median acc. & 54.00 & 52.00 & 49.00 & 57.00 & 52.00 & 61.00 & 53.00 & 55.00 & 50.00 & 56.00 & 48.00 & 51.00 & 55.00 & 57.00 & 83.00 & 83.00 & 77.00 & 54.00 & 54.00 & 59.00 & 57.00 & 72.00 & 74.00 & 72.00 & 86.00 & 83.00 & 82.00\\
Sent. completion & xcopa & zh & validation & MT & Max acc. & 63.00 & 62.00 & 58.00 & 67.00 & 66.00 & 67.00 & 73.00 & 55.00 & 52.00 & 58.00 & 56.00 & 52.00 & 58.00 & 59.00 & 88.00 & 87.00 & 79.00 & 59.00 & 55.00 & 67.00 & 61.00 & 81.00 & 80.00 & 76.00 & 90.00 & 86.00 & 83.00\\
Sent. completion & xstory\_cloze & ar & validation & EN & Median acc. & 51.69 & 49.44 & 49.57 & 49.57 & 50.23 & 50.36 & 52.42 & 48.11 & 47.98 & 49.97 & 48.11 & 53.61 & 54.53 & 65.92 & 85.37 & 87.23 & 88.29 & 52.75 & 52.08 & 69.49 & 78.23 & 81.47 & 82.13 & 75.18 & 91.26 & 91.59 & 91.86\\
Sent. completion & xstory\_cloze & ar & validation & EN & Max acc. & 52.95 & 51.36 & 52.08 & 54.20 & 56.25 & 59.17 & 65.52 & 49.64 & 49.04 & 51.62 & 48.71 & 54.53 & 56.59 & 67.50 & 90.93 & 90.60 & 88.95 & 53.67 & 53.54 & 73.33 & 80.61 & 83.26 & 83.79 & 77.50 & 92.65 & 92.92 & 92.12\\
Sent. completion & xstory\_cloze & ar & validation & MT & Median acc. & 51.62 & 48.91 & 49.11 & 49.24 & 51.49 & 49.37 & 52.61 & 51.69 & 48.71 & 51.36 & 47.58 & 53.08 & 54.86 & 66.71 & 90.54 & 90.14 & 88.29 & 52.55 & 52.28 & 62.48 & 67.24 & 80.01 & 82.46 & 75.12 & 91.13 & 91.73 & 91.79\\
Sent. completion & xstory\_cloze & ar & validation & MT & Max acc. & 53.01 & 51.75 & 52.48 & 54.20 & 55.72 & 58.37 & 65.12 & 53.47 & 49.90 & 53.01 & 48.78 & 54.20 & 55.06 & 70.09 & 91.07 & 91.00 & 89.15 & 54.40 & 53.08 & 70.48 & 78.03 & 83.06 & 83.85 & 78.69 & 92.79 & 94.04 & 92.46\\
Sent. completion & xstory\_cloze & es & validation & EN & Median acc. & 52.28 & 50.89 & 46.86 & 47.72 & 48.58 & 49.83 & 50.56 & 81.80 & 47.19 & 48.11 & 53.28 & 54.33 & 55.33 & 73.73 & 89.94 & 89.68 & 93.32 & 55.33 & 55.99 & 69.56 & 81.67 & 86.76 & 86.76 & 78.89 & 93.18 & 94.77 & 92.85\\
Sent. completion & xstory\_cloze & es & validation & EN & Max acc. & 59.36 & 55.20 & 58.77 & 60.95 & 63.07 & 65.25 & 72.34 & 83.32 & 49.97 & 52.35 & 55.20 & 54.93 & 55.72 & 74.39 & 92.52 & 93.32 & 93.58 & 55.86 & 58.04 & 77.96 & 85.90 & 88.88 & 88.62 & 82.93 & 94.31 & 95.23 & 93.91\\
Sent. completion & xstory\_cloze & es & validation & MT & Median acc. & 52.08 & 50.56 & 46.72 & 49.97 & 49.77 & 50.69 & 50.63 & 80.08 & 49.44 & 48.05 & 53.87 & 55.00 & 54.60 & 74.45 & 91.20 & 92.85 & 92.72 & 55.53 & 56.06 & 55.72 & 66.25 & 84.98 & 86.57 & 78.49 & 92.98 & 94.90 & 92.85\\
Sent. completion & xstory\_cloze & es & validation & MT & Max acc. & 60.56 & 54.93 & 57.84 & 60.89 & 62.81 & 65.78 & 72.47 & 82.13 & 50.63 & 53.47 & 54.86 & 55.39 & 55.33 & 77.17 & 92.52 & 93.38 & 93.98 & 56.45 & 58.37 & 72.53 & 84.51 & 88.95 & 88.82 & 81.60 & 94.37 & 95.50 & 94.44\\
Sent. completion & xstory\_cloze & eu & validation & EN & Median acc. & 51.56 & 49.31 & 48.25 & 50.30 & 50.36 & 48.38 & 50.96 & 49.11 & 45.14 & 49.31 & 51.82 & 51.82 & 50.50 & 64.13 & 86.04 & 84.38 & 90.87 & 49.44 & 46.72 & 57.05 & 67.44 & 70.81 & 70.02 & 67.17 & 85.24 & 85.04 & 85.04\\
Sent. completion & xstory\_cloze & eu & validation & EN & Max acc. & 54.86 & 52.95 & 54.14 & 54.00 & 55.00 & 56.52 & 62.61 & 51.36 & 50.83 & 53.01 & 52.95 & 52.88 & 52.55 & 66.64 & 89.48 & 89.68 & 91.33 & 50.56 & 52.22 & 60.49 & 70.95 & 73.33 & 72.67 & 70.42 & 86.90 & 85.90 & 86.70\\
Sent. completion & xstory\_cloze & eu & validation & MT & Median acc. & 49.83 & 50.69 & 48.64 & 50.50 & 49.31 & 48.44 & 50.89 & 50.83 & 46.46 & 50.30 & 51.42 & 52.55 & 51.29 & 65.92 & 89.54 & 87.16 & 91.20 & 47.39 & 45.14 & 49.83 & 50.17 & 59.89 & 65.59 & 66.38 & 81.40 & 82.59 & 82.73\\
Sent. completion & xstory\_cloze & eu & validation & MT & Max acc. & 55.13 & 52.22 & 52.95 & 54.20 & 54.60 & 56.32 & 62.67 & 52.15 & 50.96 & 53.47 & 52.28 & 53.67 & 52.61 & 69.03 & 90.60 & 91.13 & 91.66 & 47.52 & 52.35 & 57.91 & 64.13 & 70.81 & 73.26 & 68.63 & 86.76 & 86.83 & 84.25\\
Sent. completion & xstory\_cloze & hi & validation & EN & Median acc. & 50.96 & 48.11 & 47.65 & 50.17 & 51.42 & 49.44 & 52.42 & 49.70 & 46.33 & 52.22 & 51.09 & 52.55 & 50.89 & 67.84 & 89.81 & 87.76 & 88.68 & 53.34 & 51.82 & 68.03 & 75.98 & 75.31 & 74.85 & 68.17 & 87.03 & 87.43 & 87.09\\
Sent. completion & xstory\_cloze & hi & validation & EN & Max acc. & 55.33 & 52.95 & 54.53 & 56.78 & 56.39 & 59.70 & 63.80 & 50.23 & 52.02 & 53.61 & 51.62 & 53.87 & 52.15 & 70.68 & 92.32 & 90.73 & 89.41 & 53.74 & 55.20 & 72.87 & 78.89 & 79.48 & 78.82 & 72.20 & 87.89 & 88.68 & 88.35\\
Sent. completion & xstory\_cloze & hi & validation & MT & Median acc. & 49.64 & 46.99 & 47.72 & 50.23 & 52.35 & 50.56 & 51.09 & 48.78 & 47.12 & 52.08 & 51.89 & 54.60 & 50.17 & 69.89 & 91.79 & 88.95 & 87.82 & 54.80 & 50.89 & 55.46 & 65.06 & 73.33 & 75.84 & 68.83 & 86.70 & 87.36 & 86.23\\
Sent. completion & xstory\_cloze & hi & validation & MT & Max acc. & 56.59 & 53.87 & 54.40 & 56.78 & 57.31 & 60.23 & 65.39 & 50.76 & 53.14 & 54.86 & 53.01 & 55.00 & 51.16 & 71.08 & 92.19 & 90.14 & 89.15 & 55.79 & 55.92 & 70.75 & 75.25 & 80.61 & 80.41 & 71.61 & 88.42 & 89.15 & 88.09\\
Sent. completion & xstory\_cloze & id & validation & EN & Median acc. & 52.42 & 48.97 & 45.86 & 47.85 & 50.63 & 52.28 & 52.02 & 70.28 & 46.86 & 48.58 & 50.76 & 53.54 & 54.14 & 72.34 & 90.80 & 90.54 & 91.86 & 55.79 & 55.79 & 64.00 & 71.81 & 82.40 & 78.16 & 74.45 & 90.67 & 90.87 & 91.40\\
Sent. completion & xstory\_cloze & id & validation & EN & Max acc. & 59.36 & 55.00 & 57.51 & 58.77 & 60.29 & 63.53 & 69.03 & 73.06 & 49.90 & 51.03 & 52.35 & 54.40 & 54.67 & 73.86 & 93.25 & 93.05 & 92.46 & 57.25 & 57.97 & 74.92 & 82.99 & 84.25 & 83.32 & 77.10 & 92.12 & 92.06 & 92.59\\
Sent. completion & xstory\_cloze & id & validation & MT & Median acc. & 52.15 & 49.90 & 48.58 & 49.70 & 51.03 & 52.81 & 51.89 & 68.96 & 47.85 & 48.05 & 50.83 & 54.73 & 54.14 & 74.98 & 92.46 & 91.73 & 91.66 & 52.75 & 54.67 & 53.14 & 58.97 & 68.96 & 82.26 & 73.46 & 89.28 & 90.87 & 90.07\\
Sent. completion & xstory\_cloze & id & validation & MT & Max acc. & 59.63 & 55.33 & 57.97 & 60.29 & 60.89 & 63.60 & 68.76 & 70.15 & 49.97 & 51.49 & 53.08 & 57.38 & 54.27 & 75.71 & 93.51 & 92.19 & 92.65 & 57.84 & 56.98 & 69.29 & 78.89 & 83.32 & 84.58 & 75.12 & 91.46 & 92.79 & 91.40\\
Sent. completion & xstory\_cloze & my & validation & EN & Median acc. & 51.42 & 51.49 & 47.32 & 52.02 & 49.64 & 52.48 & 52.68 & 52.75 & 46.00 & 48.91 & 50.89 & 51.16 & 51.16 & 63.20 & 82.79 & 84.78 & 86.96 & 46.46 & 46.00 & 48.31 & 46.99 & 49.70 & 49.17 & 50.63 & 49.97 & 51.89 & 50.63\\
Sent. completion & xstory\_cloze & my & validation & EN & Max acc. & 53.21 & 52.61 & 48.44 & 52.95 & 50.56 & 52.61 & 52.95 & 54.80 & 50.89 & 50.56 & 50.96 & 51.42 & 51.69 & 65.65 & 87.49 & 86.70 & 88.35 & 47.05 & 46.39 & 51.03 & 49.90 & 50.43 & 51.42 & 51.03 & 52.35 & 52.35 & 52.68\\
Sent. completion & xstory\_cloze & my & validation & MT & Median acc. & 49.83 & 50.03 & 46.86 & 50.50 & 49.31 & 51.62 & 52.28 & 48.18 & 45.47 & 47.39 & 49.57 & 52.28 & 50.03 & 63.07 & 83.45 & 81.07 & 84.32 & 46.06 & 46.06 & 47.39 & 47.58 & 51.42 & 50.56 & 49.90 & 49.83 & 50.23 & 51.22\\
Sent. completion & xstory\_cloze & my & validation & MT & Max acc. & 52.48 & 51.69 & 47.58 & 52.61 & 50.56 & 52.68 & 53.41 & 50.17 & 50.50 & 50.83 & 51.82 & 52.75 & 50.23 & 64.66 & 85.57 & 85.90 & 85.44 & 46.59 & 47.05 & 51.09 & 49.04 & 52.55 & 51.56 & 51.49 & 50.56 & 51.42 & 51.95\\
Sent. completion & xstory\_cloze & ru & validation & EN & Median acc. & 50.23 & 49.70 & 50.63 & 52.88 & 50.89 & 50.10 & 50.36 & 65.12 & 46.19 & 49.04 & 48.38 & 51.82 & 52.75 & 69.49 & 87.49 & 86.30 & 83.65 & 51.29 & 48.44 & 54.00 & 57.78 & 63.27 & 62.08 & 64.06 & 79.02 & 77.56 & 79.42\\
Sent. completion & xstory\_cloze & ru & validation & EN & Max acc. & 60.09 & 50.69 & 51.22 & 54.14 & 52.08 & 52.35 & 56.06 & 67.50 & 51.42 & 56.39 & 49.24 & 53.87 & 53.08 & 71.14 & 90.80 & 90.87 & 84.51 & 53.14 & 50.23 & 56.39 & 61.42 & 65.32 & 64.26 & 66.45 & 81.73 & 79.09 & 79.62\\
Sent. completion & xstory\_cloze & ru & validation & MT & Median acc. & 49.50 & 50.23 & 51.09 & 52.02 & 52.35 & 50.50 & 49.83 & 61.95 & 47.52 & 49.24 & 48.97 & 50.63 & 53.01 & 71.28 & 89.54 & 90.01 & 82.86 & 51.03 & 49.11 & 50.17 & 50.10 & 58.31 & 58.77 & 61.02 & 78.42 & 74.19 & 75.98\\
Sent. completion & xstory\_cloze & ru & validation & MT & Max acc. & 60.42 & 50.30 & 51.42 & 53.14 & 53.41 & 52.15 & 55.46 & 63.93 & 51.82 & 55.39 & 49.70 & 53.01 & 53.74 & 74.85 & 91.40 & 91.66 & 84.91 & 52.22 & 50.30 & 55.13 & 56.65 & 63.40 & 62.41 & 64.13 & 79.09 & 78.16 & 76.57\\
Sent. completion & xstory\_cloze & sw & validation & EN & Median acc. & 52.08 & 49.90 & 49.64 & 49.83 & 53.08 & 51.89 & 49.31 & 49.31 & 46.59 & 49.04 & 53.61 & 53.21 & 53.94 & 67.11 & 86.17 & 87.76 & 89.15 & 49.24 & 49.24 & 55.59 & 67.44 & 67.70 & 66.31 & 58.84 & 77.83 & 79.42 & 75.71\\
Sent. completion & xstory\_cloze & sw & validation & EN & Max acc. & 56.32 & 50.03 & 50.30 & 51.62 & 55.06 & 53.94 & 60.42 & 51.49 & 49.31 & 53.21 & 54.53 & 53.34 & 54.73 & 68.83 & 88.82 & 89.61 & 89.61 & 51.36 & 49.24 & 61.28 & 69.69 & 71.67 & 71.01 & 60.82 & 79.81 & 81.14 & 77.76\\
Sent. completion & xstory\_cloze & sw & validation & MT & Median acc. & 50.69 & 50.83 & 49.83 & 50.76 & 51.49 & 50.30 & 48.84 & 51.56 & 46.46 & 47.92 & 52.81 & 53.14 & 53.81 & 69.69 & 87.36 & 88.15 & 89.08 & 48.64 & 49.17 & 49.24 & 50.23 & 53.28 & 58.37 & 55.79 & 70.81 & 73.00 & 70.28\\
Sent. completion & xstory\_cloze & sw & validation & MT & Max acc. & 55.33 & 51.62 & 50.69 & 51.69 & 53.01 & 53.67 & 60.56 & 52.28 & 49.37 & 53.94 & 54.20 & 54.40 & 55.53 & 71.14 & 89.41 & 89.15 & 89.34 & 50.50 & 49.97 & 58.44 & 63.73 & 68.43 & 69.69 & 57.18 & 78.36 & 80.01 & 72.60\\
Sent. completion & xstory\_cloze & te & validation & EN & Median acc. & 51.29 & 49.90 & 51.95 & 50.23 & 52.68 & 50.83 & 51.69 & 48.44 & 49.04 & 49.97 & 53.21 & 54.67 & 56.32 & 64.86 & 85.04 & 86.10 & 86.37 & 52.02 & 51.36 & 63.40 & 70.28 & 72.01 & 70.09 & 62.08 & 80.61 & 80.15 & 77.70\\
Sent. completion & xstory\_cloze & te & validation & EN & Max acc. & 57.38 & 55.33 & 55.92 & 55.79 & 57.58 & 57.91 & 61.61 & 49.17 & 54.00 & 53.34 & 53.34 & 55.72 & 57.18 & 68.70 & 89.54 & 90.40 & 87.29 & 53.61 & 55.26 & 66.25 & 73.66 & 74.72 & 73.06 & 63.14 & 81.20 & 82.40 & 79.88\\
Sent. completion & xstory\_cloze & te & validation & MT & Median acc. & 49.11 & 51.03 & 52.68 & 49.44 & 52.61 & 50.89 & 50.76 & 49.50 & 49.77 & 48.97 & 53.21 & 56.25 & 55.13 & 67.50 & 87.16 & 86.90 & 82.20 & 51.16 & 50.36 & 55.33 & 54.73 & 63.73 & 66.98 & 58.31 & 76.17 & 78.29 & 73.79\\
Sent. completion & xstory\_cloze & te & validation & MT & Max acc. & 57.05 & 55.79 & 56.12 & 56.32 & 58.11 & 57.64 & 62.41 & 49.83 & 53.61 & 54.00 & 53.67 & 56.92 & 56.85 & 68.89 & 90.54 & 89.41 & 85.51 & 54.86 & 55.86 & 61.88 & 66.38 & 73.59 & 71.54 & 59.30 & 80.28 & 80.54 & 77.37\\
Sent. completion & xstory\_cloze & zh & validation & EN & Median acc. & 52.08 & 49.83 & 47.39 & 47.85 & 49.17 & 50.30 & 51.36 & 49.70 & 47.65 & 53.21 & 56.25 & 54.00 & 56.32 & 68.17 & 91.20 & 91.79 & 92.92 & 54.53 & 57.18 & 76.84 & 82.59 & 84.91 & 83.85 & 76.70 & 91.99 & 92.32 & 91.13\\
Sent. completion & xstory\_cloze & zh & validation & EN & Max acc. & 55.59 & 54.47 & 56.45 & 58.04 & 59.89 & 61.22 & 65.65 & 50.89 & 49.04 & 53.94 & 56.78 & 54.60 & 58.84 & 71.74 & 92.72 & 93.05 & 93.18 & 56.52 & 58.17 & 78.69 & 84.32 & 85.04 & 85.84 & 79.62 & 93.12 & 92.85 & 92.26\\
Sent. completion & xstory\_cloze & zh & validation & HT & Median acc. & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 81.67 & - & 77.10 & - & - & -\\
Sent. completion & xstory\_cloze & zh & validation & HT & Max acc. & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 85.37 & - & 79.48 & - & - & -\\
Sent. completion & xstory\_cloze & zh & validation & MT & Median acc. & 52.15 & 49.24 & 47.45 & 47.65 & 50.23 & 51.89 & 53.01 & 48.05 & 46.99 & 52.02 & 55.00 & 54.27 & 57.71 & 72.01 & 92.59 & 91.79 & 91.79 & 55.59 & 56.45 & 70.88 & 74.26 & 81.20 & 84.65 & 78.42 & 91.86 & 91.40 & 90.40\\
Sent. completion & xstory\_cloze & zh & validation & MT & Max acc. & 56.12 & 54.33 & 56.59 & 57.38 & 60.09 & 61.22 & 66.64 & 50.03 & 48.97 & 54.20 & 57.78 & 55.72 & 59.50 & 72.93 & 93.85 & 93.05 & 93.58 & 56.45 & 56.85 & 77.17 & 80.87 & 85.11 & 85.90 & 80.34 & 92.39 & 92.52 & 91.93\\
\midrule
\bottomrule
            \end{tabular}
            }
    \caption{Evaluation results. Results per prompt can be found at \url{https://huggingface.co/datasets/bigscience/evaluation-results}
    \label{tab:allres}
    }
\end{minipage}
\end{table*} 
\clearpage

\eject \pdfpagewidth=50cm \pdfpageheight=40cm

\begin{table*}[ht]
    \centering
    \begin{minipage}{\pdfpagewidth}
    \resizebox{0.9\textwidth}{!}{
    \footnotesize
            \begin{tabular}{llllll|ccccc|ccccccc}
\toprule
Task & Dataset & Config & Split & Prompt & Metric & mT0-300M & mT0-560M & mT0-1.2B & mT0-3.7B & mT0-13B & BLOOMZ-560M & BLOOMZ-1.1B & BLOOMZ-1.7B & BLOOMZ-3B & BLOOMZ-7.1B & BLOOMZ \\
\midrule
\midrule
Extractive QA & craigslist\_bargains & bargains & validation & EN & Median acc. & 30.49 & 23.95 & 22.61 & 39.61 & 25.96 & 38.94 & 47.99 & 28.14 & 22.86 & 46.48 & 26.47\\
Extractive QA & craigslist\_bargains & bargains & validation & EN & Max acc. & 49.41 & 28.14 & 31.32 & 50.92 & 40.54 & 72.53 & 72.36 & 46.90 & 31.32 & 60.47 & 51.76\\
\midrule
Grammar Correction & blimp\_adjunct & island & validation & EN & Median acc. & 50.40 & 51.60 & 51.80 & 53.80 & 55.10 & 51.60 & 52.30 & 50.60 & 49.20 & 49.90 & 49.80\\
Grammar Correction & blimp\_adjunct & island & validation & EN & Max acc. & 50.90 & 57.00 & 58.00 & 59.10 & 56.80 & 77.10 & 60.90 & 62.30 & 59.90 & 57.60 & 51.60\\
Grammar Correction & glue & cola & validation & EN & Median acc. & 30.97 & 38.26 & 56.57 & 35.19 & 45.83 & 31.26 & 57.81 & 31.16 & 31.35 & 33.27 & 44.58\\
Grammar Correction & glue & cola & validation & EN & Max acc. & 64.33 & 51.01 & 62.80 & 47.17 & 58.29 & 41.71 & 67.98 & 46.40 & 65.39 & 56.86 & 63.37\\
\midrule
Multiple-Choice QA & aqua\_rat & raw & validation & EN & Median acc. & 27.95 & 25.20 & 24.80 & 20.47 & 16.14 & 19.29 & 22.83 & 22.05 & 22.44 & 24.41 & 27.56\\
Multiple-Choice QA & aqua\_rat & raw & validation & EN & Max acc. & 29.53 & 26.38 & 25.59 & 21.65 & 18.90 & 20.08 & 24.80 & 22.83 & 22.83 & 25.20 & 28.35\\
Multiple-Choice QA & codah & codah & train & EN & Median acc. & 25.25 & 25.43 & 26.48 & 55.04 & 75.58 & 24.93 & 24.35 & 57.17 & 64.12 & 73.60 & 80.66\\
Multiple-Choice QA & codah & codah & train & EN & Max acc. & 25.32 & 26.15 & 27.13 & 55.44 & 76.22 & 25.04 & 24.60 & 57.31 & 64.41 & 73.67 & 80.91\\
Multiple-Choice QA & commonsense\_qa & qa & validation & EN & Median acc. & 31.20 & 37.43 & 36.61 & 56.35 & 69.53 & 43.98 & 38.90 & 69.86 & 84.44 & 83.05 & 80.26\\
Multiple-Choice QA & commonsense\_qa & qa & validation & EN & Max acc. & 31.53 & 37.51 & 39.72 & 60.03 & 69.94 & 44.47 & 42.42 & 72.40 & 84.60 & 84.36 & 83.05\\
Multiple-Choice QA & head\_qa & en & validation & EN & Median acc. & 24.89 & 24.38 & 23.43 & 27.53 & 36.02 & 26.72 & 27.16 & 27.53 & 30.01 & 38.58 & 53.15\\
Multiple-Choice QA & head\_qa & en & validation & EN & Max acc. & 25.55 & 25.62 & 26.87 & 31.55 & 36.16 & 27.75 & 27.67 & 33.31 & 35.21 & 40.92 & 53.95\\
Multiple-Choice QA & head\_qa & es & validation & EN & Median acc. & 24.60 & 24.45 & 23.94 & 27.89 & 34.92 & 26.94 & 25.04 & 24.45 & 26.21 & 34.41 & 50.81\\
Multiple-Choice QA & head\_qa & es & validation & EN & Max acc. & 26.21 & 26.21 & 24.74 & 29.50 & 37.04 & 28.26 & 26.28 & 29.87 & 33.02 & 39.75 & 51.76\\
Multiple-Choice QA & math\_qa & qa & test & EN & Median acc. & 21.11 & 20.00 & 22.18 & 23.25 & 23.69 & 19.66 & 21.21 & 20.97 & 21.81 & 21.14 & 21.84\\
Multiple-Choice QA & math\_qa & qa & test & EN & Max acc. & 22.21 & 26.03 & 35.64 & 24.89 & 26.60 & 45.56 & 27.94 & 35.24 & 43.28 & 38.12 & 47.37\\
Multiple-Choice QA & mwsc & mwsc & validation & EN & Median acc. & 50.00 & 52.44 & 54.88 & 60.98 & 74.39 & 53.66 & 52.44 & 56.10 & 58.54 & 62.20 & 71.95\\
Multiple-Choice QA & mwsc & mwsc & validation & EN & Max acc. & 52.44 & 53.66 & 57.32 & 65.85 & 79.27 & 58.54 & 57.32 & 58.54 & 63.41 & 69.51 & 80.49\\
Multiple-Choice QA & pubmed\_qa & labeled & train & EN & Median acc. & 45.55 & 54.50 & 55.75 & 58.35 & 65.35 & 55.75 & 58.90 & 66.75 & 66.80 & 67.15 & 71.80\\
Multiple-Choice QA & pubmed\_qa & labeled & train & EN & Max acc. & 48.60 & 57.60 & 58.30 & 58.60 & 66.20 & 57.50 & 63.50 & 72.10 & 69.80 & 69.50 & 74.40\\
Multiple-Choice QA & riddle\_sense & sense & validation & EN & Median acc. & 24.39 & 22.04 & 23.41 & 29.63 & 43.14 & 22.87 & 24.53 & 30.02 & 35.11 & 39.47 & 50.64\\
Multiple-Choice QA & riddle\_sense & sense & validation & EN & Max acc. & 34.48 & 33.30 & 33.01 & 39.18 & 47.50 & 37.41 & 39.86 & 43.58 & 47.60 & 48.09 & 59.26\\
\midrule
Sentiment & amazon\_reviews\_multi & en & validation & EN & Median acc. & 40.60 & 50.80 & 51.12 & 49.00 & 53.24 & 46.52 & 42.46 & 50.48 & 49.88 & 51.00 & 50.90\\
Sentiment & amazon\_reviews\_multi & en & validation & EN & Max acc. & 41.34 & 53.88 & 54.18 & 55.92 & 57.04 & 50.44 & 47.74 & 55.94 & 53.74 & 55.08 & 54.16\\
Sentiment & amazon\_reviews\_multi & es & validation & EN & Median acc. & 39.56 & 48.70 & 49.02 & 47.56 & 52.30 & 37.60 & 38.92 & 45.08 & 45.32 & 44.44 & 43.26\\
Sentiment & amazon\_reviews\_multi & es & validation & EN & Max acc. & 42.66 & 51.00 & 50.42 & 50.68 & 53.58 & 39.10 & 40.24 & 47.98 & 46.28 & 47.76 & 44.48\\
Sentiment & amazon\_reviews\_multi & fr & validation & EN & Median acc. & 38.74 & 48.44 & 48.32 & 46.12 & 51.12 & 38.78 & 38.38 & 44.36 & 45.84 & 44.92 & 43.92\\
Sentiment & amazon\_reviews\_multi & fr & validation & EN & Max acc. & 40.66 & 49.64 & 49.70 & 49.30 & 52.40 & 41.16 & 40.04 & 46.66 & 46.80 & 47.42 & 44.90\\
Sentiment & amazon\_reviews\_multi & zh & validation & EN & Median acc. & 34.74 & 42.38 & 42.58 & 39.66 & 45.30 & 37.54 & 34.44 & 41.10 & 38.78 & 44.78 & 40.48\\
Sentiment & amazon\_reviews\_multi & zh & validation & EN & Max acc. & 37.88 & 44.36 & 44.74 & 43.66 & 47.14 & 39.48 & 35.24 & 43.52 & 39.64 & 47.12 & 42.10\\
Sentiment & financial\_phrasebank & allagree & train & EN & Median acc. & 18.33 & 28.98 & 28.09 & 25.44 & 35.25 & 31.10 & 29.28 & 34.76 & 35.91 & 34.89 & 24.82\\
Sentiment & financial\_phrasebank & allagree & train & EN & Max acc. & 22.22 & 57.51 & 52.25 & 68.15 & 37.77 & 44.79 & 34.81 & 54.37 & 59.23 & 37.15 & 37.23\\
Sentiment & glue & sst2 & validation & EN & Median acc. & 79.70 & 83.49 & 83.37 & 82.80 & 93.58 & 87.96 & 83.72 & 92.09 & 94.50 & 94.04 & 93.92\\
Sentiment & glue & sst2 & validation & EN & Max acc. & 81.88 & 87.96 & 86.81 & 91.51 & 94.84 & 92.89 & 89.79 & 94.15 & 95.87 & 94.61 & 95.07\\
Sentiment & lince & spaeng & validation & EN & Median acc. & 43.63 & 43.09 & 49.11 & 41.69 & 54.81 & 58.04 & 53.85 & 52.82 & 50.19 & 58.15 & 59.60\\
Sentiment & lince & spaeng & validation & EN & Max acc. & 56.91 & 56.05 & 56.37 & 55.78 & 56.80 & 58.53 & 55.35 & 56.37 & 54.60 & 58.47 & 60.09\\
Sentiment & movie\_rationales & rationales & validation & EN & Median acc. & 63.50 & 78.00 & 81.00 & 69.50 & 90.00 & 93.50 & 97.50 & 98.50 & 98.00 & 97.50 & 98.50\\
Sentiment & movie\_rationales & rationales & validation & EN & Max acc. & 94.50 & 95.50 & 98.50 & 99.50 & 100.00 & 98.50 & 97.50 & 100.00 & 99.50 & 99.00 & 99.50\\
Sentiment & poem\_sentiment & sentiment & validation & EN & Median acc. & 17.14 & 18.10 & 16.19 & 16.19 & 26.67 & 20.95 & 29.52 & 24.76 & 24.76 & 22.86 & 23.81\\
Sentiment & poem\_sentiment & sentiment & validation & EN & Max acc. & 18.10 & 23.81 & 20.00 & 27.62 & 27.62 & 22.86 & 33.33 & 29.52 & 31.43 & 29.52 & 24.76\\
\midrule
Summarization & mlsum & es & validation & EN & Median BLEU & 0.18 & 0.18 & 0.18 & 0.19 & 0.19 & 0.20 & 0.18 & 0.19 & 0.19 & 0.20 & 0.19\\
Summarization & mlsum & es & validation & EN & Max BLEU & 2.91 & 3.51 & 3.46 & 3.72 & 4.21 & 3.62 & 2.87 & 3.23 & 3.84 & 4.82 & 4.16\\
\midrule
Text Classification & art & art & validation & EN & Median acc. & 50.85 & 50.85 & 50.46 & 53.33 & 68.99 & 51.50 & 50.07 & 52.68 & 54.57 & 58.42 & 66.58\\
Text Classification & art & art & validation & EN & Max acc. & 51.04 & 51.83 & 51.76 & 56.07 & 69.71 & 52.68 & 50.65 & 54.24 & 57.31 & 61.10 & 67.43\\
Text Classification & climate\_fever & fever & test & EN & Median acc. & 10.62 & 25.28 & 10.94 & 26.78 & 29.97 & 45.34 & 10.36 & 51.92 & 10.81 & 43.97 & 18.63\\
Text Classification & climate\_fever & fever & test & EN & Max acc. & 42.41 & 43.78 & 20.98 & 43.32 & 51.01 & 63.97 & 30.94 & 65.54 & 32.12 & 47.69 & 36.61\\
Text Classification & conv\_ai\_3 & 3 & validation & EN & Median acc. & 35.15 & 38.52 & 37.79 & 39.04 & 39.04 & 39.04 & 39.04 & 39.04 & 39.04 & 39.04 & 39.04\\
Text Classification & conv\_ai\_3 & 3 & validation & EN & Max acc. & 60.35 & 60.96 & 55.69 & 60.96 & 60.96 & 60.96 & 60.96 & 60.96 & 60.96 & 60.96 & 60.96\\
Text Classification & emotion & emotion & test & EN & Median acc. & 20.75 & 23.83 & 42.20 & 32.38 & 31.35 & 34.72 & 35.57 & 29.93 & 39.77 & 33.05 & 36.70\\
Text Classification & emotion & emotion & test & EN & Max acc. & 32.40 & 24.65 & 46.25 & 33.05 & 34.65 & 46.70 & 42.40 & 49.20 & 49.35 & 50.25 & 45.20\\
Text Classification & health\_fact & fact & validation & EN & Median acc. & 31.59 & 27.27 & 31.10 & 43.67 & 54.78 & 42.04 & 45.63 & 44.00 & 32.41 & 31.51 & 47.92\\
Text Classification & health\_fact & fact & validation & EN & Max acc. & 50.61 & 43.02 & 42.53 & 44.16 & 59.59 & 54.78 & 56.82 & 63.76 & 62.53 & 57.55 & 61.31\\
Text Classification & hlgd & hlgd & validation & EN & Median acc. & 50.65 & 59.45 & 52.88 & 78.15 & 80.72 & 72.89 & 68.63 & 64.14 & 65.39 & 70.57 & 67.57\\
Text Classification & hlgd & hlgd & validation & EN & Max acc. & 63.80 & 73.71 & 65.83 & 79.36 & 84.92 & 74.92 & 72.50 & 73.37 & 68.15 & 81.83 & 78.44\\
Text Classification & hyperpartisan\_news\_detection & byarticle & train & EN & Median acc. & 46.20 & 49.15 & 52.87 & 52.87 & 43.26 & 62.95 & 63.10 & 63.10 & 63.10 & 63.10 & 63.10\\
Text Classification & hyperpartisan\_news\_detection & byarticle & train & EN & Max acc. & 49.15 & 50.39 & 54.57 & 53.64 & 44.96 & 63.10 & 63.26 & 63.10 & 63.41 & 63.10 & 63.72\\
Text Classification & liar & liar & validation & EN & Median acc. & 19.47 & 18.07 & 20.40 & 17.68 & 17.91 & 17.60 & 19.31 & 19.39 & 15.19 & 20.79 & 20.87\\
Text Classification & liar & liar & validation & EN & Max acc. & 19.47 & 18.07 & 20.40 & 17.68 & 17.91 & 17.60 & 19.31 & 19.39 & 15.19 & 20.79 & 20.87\\
Text Classification & onestop\_english & english & trsin & EN & Median acc. & 48.32 & 48.15 & 33.33 & 58.20 & 48.32 & 43.39 & 33.51 & 35.80 & 45.33 & 54.67 & 41.80\\
Text Classification & onestop\_english & english & trsin & EN & Max acc. & 56.26 & 58.73 & 46.74 & 65.61 & 56.44 & 55.56 & 34.57 & 41.80 & 63.32 & 64.02 & 53.09\\
Text Classification & scicite & scicite & validation & EN & Median acc. & 13.97 & 24.56 & 23.14 & 33.08 & 39.63 & 33.08 & 17.90 & 21.62 & 30.57 & 34.28 & 54.91\\
Text Classification & scicite & scicite & validation & EN & Max acc. & 25.11 & 37.23 & 30.57 & 66.16 & 66.16 & 54.91 & 25.98 & 44.10 & 57.21 & 50.33 & 63.43\\
\midrule
Topic Classification & banking77 & banking77 & test & EN & Median acc. & 11.30 & 11.53 & 16.27 & 19.51 & 30.10 & 14.38 & 19.29 & 20.81 & 24.19 & 25.39 & 28.57\\
Topic Classification & banking77 & banking77 & test & EN & Max acc. & 15.10 & 12.99 & 19.94 & 23.83 & 30.94 & 16.10 & 20.45 & 26.04 & 28.90 & 26.36 & 29.06\\
Topic Classification & blbooksgenre\_title\_genre & classifiction & validation & EN & Median acc. & 26.21 & 35.43 & 35.83 & 49.14 & 32.03 & 41.47 & 25.17 & 30.47 & 27.13 & 74.94 & 77.07\\
Topic Classification & blbooksgenre\_title\_genre & classifiction & validation & EN & Max acc. & 33.93 & 43.78 & 73.10 & 74.88 & 85.43 & 74.31 & 74.94 & 73.62 & 71.72 & 84.56 & 86.41\\
Topic Classification & selqa & analysis & validation & EN & Median acc. & 88.34 & 88.54 & 90.00 & 89.30 & 92.61 & 89.81 & 87.71 & 91.08 & 90.83 & 89.24 & 91.46\\
Topic Classification & selqa & analysis & validation & EN & Max acc. & 91.59 & 90.32 & 91.08 & 91.97 & 94.39 & 92.36 & 88.66 & 92.36 & 92.48 & 91.21 & 94.27\\
Topic Classification & snips\_built\_in\_intents & intents & train & EN & Median acc. & 35.37 & 45.73 & 34.15 & 62.20 & 82.62 & 27.13 & 39.63 & 11.89 & 25.91 & 39.94 & 70.12\\
Topic Classification & snips\_built\_in\_intents & intents & train & EN & Max acc. & 39.02 & 54.27 & 42.07 & 64.63 & 92.68 & 46.34 & 53.96 & 17.68 & 33.84 & 58.23 & 78.66\\
\midrule
Translation & wmt14\_fr\_en & en & validation & EN & Median BLEU & 5.47 & 11.33 & 17.00 & 23.92 & 29.87 & 4.70 & 4.28 & 6.10 & 12.29 & 8.03 & 26.07\\
Translation & wmt14\_fr\_en & en & validation & EN & Max BLEU & 10.84 & 19.23 & 23.15 & 29.63 & 33.65 & 21.24 & 25.38 & 26.19 & 27.93 & 29.54 & 33.71\\
Translation & wmt14\_hi\_en & en & validation & EN & Median BLEU & 1.02 & 3.35 & 5.11 & 9.14 & 18.43 & 1.36 & 0.39 & 1.11 & 1.84 & 3.62 & 10.05\\
Translation & wmt14\_hi\_en & en & validation & EN & Max BLEU & 2.47 & 7.57 & 12.96 & 19.80 & 26.13 & 9.02 & 11.09 & 12.02 & 14.82 & 17.02 & 21.18\\
\midrule
\bottomrule
            \end{tabular}
            }
    \caption{Evaluation results on validation datasets used for checkpoint selection. Results of the chosen checkpoint are shown. Results per prompt can be found at \url{https://huggingface.co/datasets/bigscience/evaluation-results}
    \label{tab:valres}
    }
\end{minipage}
\end{table*} 
\clearpage
\eject \pdfpagewidth=21cm \pdfpageheight=29.7cm

\FloatBarrier

\section{Version control}



\textbf{V1 â V2:}
\begin{itemize}
    \item Added evaluation results for the validation datasets used for checkpoint selection (Appendix~\S\ref{sec:fullresults})
    \item Added a section on the effect on generation length (Appendix~\S\ref{sec:generationlength}) and rewrote parts of \S\ref{sec:generation}
    \item Added a mention of xP3x, the extension of xP3 to 277 languages in Appendix~\S\ref{sec:artifacts}
    \item Added an example of XNLI to Appendix~\S\ref{sec:levenshtein}
\end{itemize}



\FloatBarrier

\section{Prompts used}
\label{sec:prompts}
This section describes the prompts used for training and evaluation.
\clearpage
\nocite{Artetxe:etal:2019}
\nocite{DBLP:conf/iclr/HellendoornSSMB20}
\nocite{DBLP:journals/corr/abs-2001-07676}
\nocite{N18-1101}
\nocite{austin2021program}
\nocite{cmrc2018-dataset}
\nocite{conneau2018xnli}
\nocite{goyal2021flores}
\nocite{gpt3}
\nocite{guzman2019two}
\nocite{hasan-etal-2021-xl}
\nocite{hendrycksapps2021}
\nocite{huggingface:dataset}
\nocite{kim-etal-2021-bisect}
\nocite{ladhak-wiki-2020}
\nocite{lewis2019mlqa}
\nocite{nllb2022}
\nocite{pawsx2019emnlp}
\nocite{ponti2020xcopa}
\nocite{raganato-etal-2020-xl-wic}
\nocite{roemmele2011choice}
\nocite{sanh2022multitask}
\nocite{sun2020investigating}
\nocite{tiedemann-2020-tatoeba}
\nocite{tikhonov2021heads}
\nocite{true-zero-shot}
\nocite{webson-pavlick-2021}
\nocite{xu2020clue}
\nocite{zhu2022xlcost}
 
\includepdf[pages=2-]{prompt-appendix.pdf}
    


\end{document}
