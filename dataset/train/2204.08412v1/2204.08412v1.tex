

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage[accsupp]{axessibility}  \usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}
\usepackage{colortbl}
\usepackage{tipa}
\definecolor{mygray}{gray}{0.93}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{Temporally Efficient Vision Transformer for Video Instance Segmentation}

\author{Shusheng Yang\thanks{This work was done while Shusheng Yang and Yu Li were at Applied Research Center (ARC), Tencent PCG.}, \ \  Xinggang Wang\thanks{Corresponding author, E-mail: {\tt xgwang@hust.edu.cn}.}, \ \ Yu Li, \ \ Yuxin Fang, \\
Jiemin Fang, \ \ Wenyu Liu, \ \ Xun Zhao, \ \ Ying Shan \\
\normalsize 
School of EIC, Huazhong University of Science \& Technology \\
\normalsize 
Institute of Artificial Intelligence, Huazhong University of Science \& Technology \\
\normalsize 
Applied Research Center (ARC), Tencent PCG \ \ International Digital Economy Academy (IDEA) \\
}
\maketitle

\begin{abstract}
Recently vision transformer has achieved tremendous success on image-level visual recognition tasks. To effectively and efficiently model the crucial temporal information within a video clip, we propose a \textbf{T}emporally \textbf{E}fficient \textbf{Vi}sion \textbf{T}ransformer (TeViT) for video instance segmentation (VIS). Different from previous transformer-based VIS methods, TeViT is nearly convolution-free, which contains a transformer backbone and a query-based video instance segmentation head. In the backbone stage, we propose a nearly parameter-free messenger shift mechanism for early temporal context fusion. In the head stages, we propose a parameter-shared spatiotemporal query interaction mechanism to build the one-to-one correspondence between video instances and queries. Thus, TeViT fully utilizes both frame-level and instance-level temporal context information and obtains strong temporal modeling capacity with negligible extra computational cost. On three widely adopted VIS benchmarks, \ie, YouTube-VIS-, YouTube-VIS-, and OVIS, TeViT obtains state-of-the-art results and maintains high inference speed, \eg,  AP with  FPS on YouTube-VIS-. Code is available at \url{https://github.com/hustvl/TeViT}.
\end{abstract}

\section{Introduction}

Video Instance Segmentation (VIS)~\cite{vis} is a representative and challenging video understanding task that requires detecting, segmenting and tracking video instances across frames simultaneously. Similar to other instance-level video recognition tasks, making full use of temporal context information is critical for building high-performance VIS systems. Vision transformer (ViT) \cite{vit}, which is based on self-attention~\cite{transformer}, has shown strong long-range context modeling ability and obtained great successes on image classification~\cite{vit, deit, halonet, swintransformer, pvt, msgtransformer, shuffletransformer}, object detection~\cite{detr, defdetr, conddetr, sparsercnn}, semantic segmentation~\cite{segformer, segmenter, maskformer}, instance segmentation~\cite{queryinst, solq, knet}, and video recognition~\cite{timesformer, videoswin, tokshift, vivit, vidtr, mvit, vtn}.

Recently, how to design ViTs for instance-level video understanding, especially VIS, becomes an emerging problem. Different from the detection transformers \cite{detr,defdetr,conddetr,sparsercnn, yolos}, semantic segmentation transformers~\cite{segformer, segmenter, maskformer}, and instance segmentation transformers \cite{queryinst, solq, knet}, which focus on D contextual information modeling, VIS transformers additionally require to perform temporal context modeling. To this end, VisTR~\cite{vistr} firstly proposes a transformer encoder to fuse patch features from a sequence of frames using a CNN backbone and leverages a query-based decoder to predict video instances, IFC~\cite{ifc} introduces memory tokens to store frame-level features and performs cross-frame feature interaction by computing self-attention among memory tokens, and then decodes instance-level results using a conditional mask head.

In this paper, we focus on the efficiency of modeling temporal information for ViT-based VIS. This is a very important problem since (1) computing self-attention among all video patches has extremely high time and space complexity \cite{vistr}, (2) additional multi-head self attention () layers for temporal modeling have extra parameters and are sensitive to pre-training \cite{ifc}, (3) the CNN or transformer backbones in these methods \cite{vistr, ifc, querytrack, tcis} only support single frame feature extraction and fail to capture temporal information in the backbone stage. To remedy the above issues, we present Temporally Efficient ViT (TeViT) to fully utilize temporal contextual information for efficient and effective video instance segmentation.

TeViT contains a transformer backbone and a series of query-based VIS heads. In the backbone stage, we use messenger tokens~\cite{msgtransformer} to extract intra-frame information via self-attention and propose a messenger shift mechanism for frame-level context modeling, in which messenger tokens are divided into several groups to perform temporal shift with various of time steps. Different from previous VIS methods, the messenger shift transformer enables early temporal feature fusion. 
In the head stages, we convert the QueryInst~\cite{queryinst} instance segmentation head into our VIS head by reusing the multi-head self-attention ()~\cite{transformer} parameters for instance-level temporal information interaction. The instance-level  fuses the features for a single video instance among input frames, thus it realizes the concept of a video instance as a query.

Experiments are conducted on three large-scale VIS datasets, \ie, YouTube-VIS-~\cite{vis}, YouTube-VIS-~\cite{vis2021}, and OVIS~\cite{ovis}. New state-of-the-art (SoTA) performance has been obtained, \eg, TeViT obtains  AP with  FPS on YouTube-VIS-. Our main contributions are summarized as follows.

\begin{itemize}
\item TeViT is the first video instance segmentation transformer that can efficiently capture temporal contextual information at both frame level and instance level.
\item Benefiting from the flexibility of self-attention, the proposed temporal modeling modules, \ie, messenger token shift and spatiotemporal query interaction, both are friendly to the image-level pre-trained models, cost marginal extra computation overhead and parameters.
\item TeViT is a nearly convolution-free framework and obtains SoTA VIS results. In TeViT, the concepts of ``early temporal feature fusion" and ``a video instance as a query" shield lights on how to build effective video transformers for instance-level recognition tasks.
\end{itemize}

\section{Related Work}

\noindent\textbf{Video instance segmentation.}
How to achieve efficiently temporal modeling is always the focus of video tasks, such as video object segmentation (VOS) \cite{uvos, uvos, stm}, multi-object tracking and segmentation (MOTS) \cite{mots} and VIS~\cite{vis}. Though VOS and MOTS are very related with VIS, MOTS mainly focuses on the urban scene understanding and VOS aims at tracking specific object by a given mask. Representative VIS works are reviewed as follows. MaskTrack R-CNN~\cite{vis} extends Faster R-CNN~\cite{fasterrcnn} and Mask R-CNN~\cite{maskrcnn} to VIS with a tracking branch and external memory that saves instance features across multiple frames. MaskProp~\cite{maskprop} builds on the Hybrid Task Cascade Network~\cite{htc} and propagates instance region features to adjacent frames to perform temporal modeling. STEm-Seg~\cite{stemseg} treats video clip as D spatiotemporal volume and captures temporal information by D convolutional backbone network. CompFeat~\cite{compfeat} refines temporal features at both frame-level and instance-level. CrossVIS~\cite{crossvis} introduces a crossover learning scheme upon~\cite{fcos, condinst} to make use of contextual information across video frames. SeqMask R-CNN~\cite{seqmaskrcnn} establishes temporal relation across frames by adding an extra sequence propagation head upon Mask R-CNN. Both VisRGNN~\cite{visrgnn} and VisSTG~\cite{visstg} model temporal information in VIS by a graph neural network. VisTR~\cite{vistr} proposes the first fully end-to-end VIS method upon DETR~\cite{detr}, temporal contexts are fused by the multi-head attention mechanism in transformer encoder layers. IFC~\cite{ifc} presents inter-frame communication to exchange frame-level information. In this paper, we present a temporally efficient framework to model temporal contexts at both frame-level and instance-level.

\noindent\textbf{Vision Transformer.}
Transformer~\cite{transformer} is firstly proposed to model long-range sequence data in natural language process (NLP). ViT~\cite{vit} firstly adopts transformer to image domain. After that various high-performance vision transformers \cite{swintransformer, halonet, deit, pvt, pvtv2, msgtransformer, shuffletransformer} have been proposed as backbones for image understanding. Beyond serving as backbone networks, transformer has motivated lots of novel object detection \cite{detr, defdetr, sparsercnn, conddetr, yolos}, instance segmentation \cite{queryinst, solq, knet}, and semantic segmentation \cite{segformer, segmenter, maskformer} frameworks. Recently, VisTR~\cite{vistr}, IFC~\cite{ifc}, QueryTrack~\cite{querytrack}, and TCIS~\cite{tcis} bring transformer to video instance segmentation and achieve excellent performance. In this paper, we investigate how to efficiently model temporal context across video frames and propose TeViT. TeViT is a nearly convolution-free transformer while VisTR and IFC both use ResNet~\cite{resnet} backbone.

\noindent\textbf{Temporal context modeling.}
Temporal context modeling is the key issue in video understanding. A lot of works \cite{c3d, i3d, p3d, s3d, r213d} model temporal context by D convolutional block. TSM~\cite{tsm} proposes an efficient temporal shift module by moving the convolutional feature map along the temporal dimension. Non-local network~\cite{nonlocal} applies self-attention to capture long-range spatiotemporal dependencies directly. Recently, TimeSformer~\cite{timesformer}, ViViT~\cite{vivit}, VidTR~\cite{vidtr}, and MViT~\cite{mvit} extend ViT to capture spatiotemporal context for video classification. Video Swin Transformer~\cite{videoswin} extends Swin Transformer~\cite{swintransformer} to video by conducting shift window  in both space and time. TokShift~\cite{tokshift} proposes a temporal shift mechanism on  tokens of ViT. Different from these video transformers focus on video classification, we target at building temporally efficient transformer for instance-level video understanding.


\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/overall_arch.pdf}
    \caption{The overall illustration of our TeViT framework. TeViT contains a messenger shift transformer backbone and a series of spatiotemporal query-driven instance heads. The messenger shift mechanism performs efficient frame-level temporal modeling by simply shifting messenger tokens along the temporal axis. Spatiotemporal query interaction conducts two successive and parameter-shared multi-head self attention () with feed forward network () upon video instance queries. The ``Dynamic Conv" design follows QueryInst~\cite{queryinst}. Best viewed in color.}
    \label{fig:overall_arch}
\end{figure*}

\section{Method}

\subsection{Overall Architecture}

The overall architecture of our VIS method TeViT is shown in Fig.~\ref{fig:overall_arch}, which contains a transformer-based backbone network and a query-driven head network. Given a sequence of video frames, the transformer backbone performs feature extraction and generates multi-scale pyramid features. The query-driven head network takes randomly initialized instance queries with backbone feature maps to predict video instances. Our whole network is end-to-end for both training and inference.

\subsection{Messenger Shift Transformer Backbone}
\label{sec:messenger_shift_transformer_backbone}

\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/messenger_shift.pdf}
    \caption{An illustration of the messenger shift mechanism. Messenger tokens are first segmented into several groups ( in the figure) and then shifted along the temporal axis with different stride () and direction () to exchange frame-level information. For every two successive messenger shift mechanisms, we apply an inverse shift operation. As shown in figure above, the shift direction () of each token group in the second shift mechanism (right) is exactly inverse to the first (medium). Messenger tokens are shown by green cubes, while blue cubes denote the patch tokens.}
    \label{fig:messenger_shift}
\end{figure}

In previous VIS methods, the backbone networks only perform feature extraction in per-frame fashion \cite{vistr, ifc} and neglect the rich contextual information inherent in video frames. In contrast, inspired by MSG-Transformer \cite{msgtransformer}, we propose messenger shift transformer (MsgShifT) which performs highly efficient temporal context modeling in a bottom-up manner, as shown in Fig.~\ref{fig:overall_arch} (left). Without loss of generality, we build MsgShifT based on the pyramid vision transformer (PVT)~\cite{pvt, pvtv2}.

To be specific, given an input video with  frames of resolution , denoted as , we first divide these frames into  patch tokens frame-by-frame, where  denotes the size of each patch. Then we feed the flattened patch tokens to a linear projection and get embedded patches  with size of ,  denotes the channel dimension.
Meanwhile, a group of randomly initialized learnable embeddings with size of  are introduced as messenger tokens, denoted as , where  indicates the number of messenger tokens.
Then we simply copy and concatenate messenger tokens with patch tokens:

where  indicates the copycat of messenger tokens . The concatenated joint tokens  are taken as inputs for our MsgShifT.

Our transformer architecture consists of  stages and each stage has the same architecture as in Fig.~\ref{fig:overall_arch} (left). The multi-head self attention () and feed forward network () act on the concatenated joint tokens in a per-frame manner:

Next, a messenger shift manipulation performs temporal information exchange across video frames.

In short, the messenger shift mechanism takes temporal messenger tokens  as inputs and builds temporal context modeling by shifting messenger tokens along the temporal axis.
Fig.~\ref{fig:messenger_shift} gives a detailed illustration. First, messenger tokens are divided into  groups and shifted along the temporal axis with different time steps ( or ) and direction (forward or backward).
With various time steps and directions, messenger tokens are able to achieve temporal context exchange with both past and future frames.
Moreover, for every two messenger shift operations, we apply an inverse operation to the second one, which implies the messenger tokens will be shifted back to their original corresponding frames after two contiguous messenger shift manipulations.
This design aims to maintain a stable temporal receptive field as the network goes deeper.

After the above process, the messenger tokens and patch tokens go through one of four stages, and the output tokens are reshaped to feature maps  which is  smaller than the original image. In the same way, using the output messenger tokens and patch tokens of prior stage as inputs, we obtain the following pyramid feature maps ,  and , whose strides are ,  and  pixels with respect to the input image. The pyramid feature maps  will be used to predict video instances in the head network.

MsgShifT performs early temporal fusion in the backbone network, while the previous transformer-based VIS approaches~\cite{vistr, querytrack, tcis, ifc} only perform temporal feature fusion using transformer encoders after image-level feature extraction. It is almost parameter-free, friendly to image-level pre-training models, and brings negligible computation costs. The messenger tokens are randomly initialized and the shift manipulation has no parameter, so this module is insensitive to the pre-training process, which will be further discussed in the experiments in Tab.~\ref{tab:shift_vs_attention}.

\subsection{Spatiotemporal Query Interaction Head}

MsgShifT achieves frame-level spatiotemporal context modeling. Meanwhile, in the VIS head network, our method still emphasizes temporally efficient spatiotemporal context modeling, but at the instance level. To this end, we propose a spatiotemporal query interaction (STQI) head network based on the recent SoTA query-based image-level instance segmentation method, \ie, QueryInst~\cite{queryinst}.

As shown in Fig.~\ref{fig:overall_arch} (right), our head network contains  STQI heads and takes a fixed-length instance queries  along with pyramid features extracted by MsgShifT  as inputs, and generates  instance predictions.  and  denotes the numbers and the channel dimensions of instance query respectively.
Instance queries  are randomly initialized and optimized during training. Additionally, our VIS network also contains a set of proposal boxes  as prior proposals, for more details about this, we refer readers to QueryInst~\cite{queryinst}.

Instance queries are firstly copied by  times to each frame. Two successive and parameter-shared  modules act on instance queries along spatial and temporal dimensions:



``:'' denotes ranging from  to . Enhanced instance queries  are fed into a dynamic convolution module and perform interactions with instance region features. Its output serves as the input queries of the next head.
Finally, task specific heads (\ie, classification head, box head and mask head) predict a sequence of video instances:

where ,  and  denotes predicted confidence scores, bounding boxes and instance foreground masks, respectively.

The advantages of STQI mainly stem from the minimum modifications on the still-image instance prediction head in~\cite{queryinst}. STQI achieves highly efficient temporal context modeling at instance-level by a parameter-shared  (Eq.~\ref{eq:spatial_query_mhsa} and Eq.~\ref{eq:temporal_query_mhsa}) while does not involve extra parameters.

\begin{table*}
\begin{center}
\setlength{\tabcolsep}{12.3 pt}
\resizebox{\linewidth}{!}{ 
\begin{small}
\begin{tabular}{llc|c|c|c|c|c|c}
\hline

\hline
\rowcolor{mygray}

Method & Backbone & MST & FPS & AP & AP & AP & AR & AR \\
\hline
\hline
MaskTrack R-CNN~\cite{vis} & ResNet- & &  &  &  &  &  &  \\
MaskTrack R-CNN~\cite{vis} & ResNet- & &  &  &  &  &  &  \\
SipMask~\cite{sipmask} & ResNet- & \checkmark &  &  &  &  &  &  \\
SG-Net~\cite{sgnet} & ResNet- & &  &  &  &  &  &  \\
SG-Net~\cite{sgnet} & ResNet- & &  &  &  &  &  &  \\
CrossVIS~\cite{crossvis} & ResNet- & \checkmark &  &  &  &  &  &  \\
CrossVIS~\cite{crossvis} & ResNet- & \checkmark &  &  &  &  &  &  \\
\hline
STEm-Seg~\cite{stemseg} & ResNet- & \checkmark &  &  &  &  &  &  \\
STEm-Seg~\cite{stemseg} & ResNet- & \checkmark &  &  &  &  &  &  \\
MaskProp~\cite{maskprop} & ResNet- & \checkmark &  &  &  &  &  &  \\
MaskProp~\cite{maskprop} & ResNet- & \checkmark &  &  &  &  &  &  \\
MaskProp~\cite{maskprop} & STSN-X- & \checkmark &  &  &  &  &  &  \\
SeqMask R-CNN~\cite{seqmaskrcnn} & ResNet- & &  &  &  &  &  &  \\
SeqMask R-CNN~\cite{seqmaskrcnn} & ResNet- & &  &  &  &  &  &  \\
\hline
VisTR~\cite{vistr} & ResNet- & &  &  &  &  &  &  \\
VisTR~\cite{vistr} & ResNet- & &  &  &  &  &  &  \\
EfficientVIS~\cite{efficientvis} & ResNet- & \checkmark &  &  &  &  &  &  \\
EfficientVIS~\cite{efficientvis} & ResNet- & \checkmark &  &  &  &  &  &  \\
IFC~\cite{ifc} & ResNet- & \checkmark &  &  &  &  &  &  \\
IFC~\cite{ifc} & ResNet- & \checkmark &  &  &  &  &  &  \\
\hline
TeViT (ours) & MsgShifT & &  &  &  &  &  &  \\
TeViT (ours) & MsgShifT & \checkmark &  &  &  &  &  &  \\
\hline

\hline
\end{tabular}
\end{small}
}
\caption{Comparisons on YouTube-VIS- dataset~\cite{vis}. ``\checkmark" under ``MST" indicates using multi-scale training strategy,
The FPS is measured with a single TESLA V GPU.
All methods in the figure are organized into four groups. According to their basic architectures, the first two groups of methods are built upon CNN architecture, while the last two are transformer-based. According to their training inference paradigms, the first group follows the online and track-by-detect fashion, while the rest all follow offline and sequence-in-sequence-out paradigm.
}
\label{tab:vis2019}
\end{center}
\end{table*}



\subsection{Matching and Loss Function}

The loss function is motivated by \cite{detr}. We first compute the one-to-one assignment between predicted instances and ground-truth annotations.
The ground-truth annotations are denoted as follows:

in which  indicates the number of ground-truth video instances, ,  and  indicates the category, bounding box and mask respectively.
We then perform sequence-level bipartite matching between predictions and annotations by Hungarian algorithm~\cite{hungarian}. The cost matrix with size of  between each predicted video instance and each annotation is defined as follows.

where  indicates the focal loss~\cite{focalloss} for classification,  and  indicates the L loss and GIoU loss~\cite{giou} respectively.  are hyper-parameters which we simply follow \cite{queryinst, sparsercnn, defdetr}. Besides we use dice coefficient~\cite{diceloss} to optimize mask predictions. For more details, please refer to \cite{queryinst}.

\subsection{Online and Offline Inference}
\label{sec:online_and_offline_inference}

Our method is flexible for both offline and online inference. Under the offline scenario, our TeViT takes the \textit{whole} video clips as inputs and then outputs all possible video instances with a single run. No post-tracking process is needed. When it comes to the near online \cite{stemseg} scenario, an entire video is split into several overlapping segments. TeViT takes clips in time order and generates predictions. A rule-based post-tracking procedure is applied to linking instances across different video clips. For instances from two overlapping video clips, we first compute the similarity score between each instance, and then a Hungarian matcher gives the assignment according to the similarity matrix. The similarity score is defined as a combination of box IoU and mask IoU.

\begin{table*}
\centering
\begin{minipage}[t]{0.49\linewidth}
\centering
\renewcommand\arraystretch{1.17}
\renewcommand\tabcolsep{3.5pt}
\begin{small}
\begin{tabular}{l|ccccc}
\hline

\hline
\rowcolor{mygray}
Methods & AP & AP & AP & AR & AR \\
\hline
\hline
MaskTrack R-CNN~\cite{vis, crossvis} &  &  &  &  &  \\
SipMask~\cite{sipmask, crossvis} &  &  &  &  &  \\
CrossVIS~\cite{crossvis} &  &  &  &  &  \\
IFC~\cite{ifc} &  &  &  &  &  \\
TeViT &  &  &  &  &  \\
\hline

\hline
\end{tabular}
\end{small}
\caption{Comparisons with previous VIS methods on YouTube-VIS- datasets. Methods with superscript ``" are reported in~\cite{crossvis}.
}
\label{tab:vis2021}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\renewcommand\arraystretch{1}
\renewcommand\tabcolsep{3.5pt}
\begin{small}
\begin{tabular}{l|ccccc}
\hline

\hline
\rowcolor{mygray}
Methods 
& AP 
& AP 
& AP 
& AR 
& AR 
\\
\hline
\hline
SipMask~\cite{sipmask, crossvis} &  &  &  &  &  \\
MaskTrack R-CNN~\cite{vis, crossvis} &  &  &  &  &  \\
STEm-Seg~\cite{stemseg, ovis} &  &  &  &  &  \\
CrossVIS~\cite{crossvis} &  &  &  &  &  \\
CMaskTrack R-CNN~\cite{ovis} &  &  &  &  &  \\
TeViT &  &  &  &  &  \\
\hline

\hline
\end{tabular}
\end{small}
\caption{Comparisons on OVIS dataset. Methods with superscript ``" and ``"  are reported in~\cite{crossvis} and~\cite{ovis} respectively.
}
\label{tab:ovis}
\end{minipage}
\end{table*}


\section{Experiments}

\subsection{Datasets and Evaluation Metrics}
We evaluate TeViT on three challenging video instance segmentation benchmarks, \ie, YouTube-VIS-~\cite{vis}, YouTube-VIS-~\cite{vis2021}, and OVIS~\cite{ovis}.
\textbf{YouTube-VIS-} is the first dataset that focuses on the VIS problem. It contains  common object categories,  unique video instances and about  high-quality instance-level annotations.
\textbf{YouTube-VIS-} dataset is the new version of YouTube-VIS- with  more video frames and  more annotations.
\textbf{OVIS} dataset aims to explore the VIS problem under high-occlusion scenarios. It consists of  high-quality instance masks and  instances per video from  semantic categories.
Following previous works, we report the performance on the validation set for all three datasets.
We follow the standard VIS evaluation metrics defined in~\cite{vis}.

\subsection{Implementation Details}
TeViT is built upon the  toolbox~\cite{mmdetection}.
Unless otherwise noted, hyper-parameters follow the settings of QueryInst~\cite{queryinst}.
We use  video instance queries as \cite{ifc, queryinst}.
Due to the temporal efficient designs in TeViT, we do not need to create pseudo video data, \eg \cite{stemseg, ifc}, to train the temporal modeling parameters, instead, we first train a transformer-based QueryInst for image-level instance segmentation on the COCO dataset~\cite{mscoco} and then initialize TeViT with the COCO pre-trained QueryInst weights. Besides, we provide a MindSpore \cite{mindspore} implementation of TeViT.

When training on the VIS datasets, we use the AdamW~\cite{adam} optimizer with an initial learning rate of , and a weight decay of . Especially, the backbone learning rate is slightly lower with a multiplier set to . We also apply gradient clipping with a maximal gradient norm of . TeViT is trained with a batch size of  and a clip length of . The total training process contains  epochs, and the learning rate is decreased by  at the -th and -th epoch respectively.
For example, our TeViT can be trained in about  hours with  Tesla V GPUs on YouTube-VIS-, which is much faster than previous transformer-based method (\ie, VisTR \cite{vistr}).
The number of instance queries  is set to  for all experiments. Following \cite{vis}, all input frames are resized to  in single-scale experiments. Settings of multi-scale training simply follow \cite{sipmask}. For inference, all frames are resized to  regardless of the training setups. During inference, we use  for most results and report the near online results in ablation study. For main results, we evaluate our framework on YouTube-VIS-, YouTube-VIS-, and OVIS datasets, with PVT-B~\cite{pvtv2} based MsgShifT as backbone.

It's noted that all reported results in main results and ablation studies are average performance from multiple runs (\ie, we choose five different random seeds and run each random seed for three times). The standard deviations (\ie,  in following tables) are calculated in the same way.

\subsection{Main Results}

\noindent\textbf{Main results on YouTube-VIS-2019 dataset.} We compare our TeViT to state-of-the-art methods on YouTube-VIS- dataset in Tab.~\ref{tab:vis2019}. The longest video in YouTube-VIS- dataset only contains  frames, so that our TeViT executes fully offline inference on this dataset. Without bells and whistles, our TeViT achieves  AP when using a single-scale training strategy and outperforms the previous state-of-the-art methods by a large margin. Multi-scale training strategy further boosts the performance to  AP. Meanwhile, our method also achieves competitive inference speed. With about  AP higher, our method is still faster than VisTR.

\noindent\textbf{Main results on YouTube-VIS-2021 dataset.}
Tab.~\ref{tab:vis2021} shows the final results of several VIS methods and ours on YouTube-VIS- dataset. Due to the video length in YouTube-VIS- is longer than our inference clip length (), TeViT performs near online tracking described in Sec.~\ref{sec:online_and_offline_inference} on this dataset. TeViT obtains  AP, outperforming the previous state-of-the-art method by  AP.

\noindent\textbf{Main results on OVIS dataset.}
The results on the OVIS dataset are shown in Tab.~\ref{tab:ovis}. Our method also performs near online inference on OVIS dataset. TeViT achieves a relatively higher performance of  AP on the  split, surpassing previous state-of-the-art methods. Compared to CMaskTrack R-CNN~\cite{ovis} which presents an elaborate-designed feature calibration plug-in to alleviate occlusion, our TeViT still gains  AP improvement, which shows that our temporal context modeling designs are helpful to segment occluded instances.
\begin{table*}
\centering
\begin{minipage}[t]{0.49\linewidth}
\centering
\renewcommand\arraystretch{1}
 \renewcommand\tabcolsep{1.3pt}
\begin{small}
\begin{tabular}{cc|cccccc}
\hline

\hline
\rowcolor{mygray}
MSM & STQI & GFLOPs & AP  & AP & AP & AR & AR \\
\hline
\hline
& &  &  &  &  &  &  \\
\checkmark & &  &  &  &  &  &  \\
& \checkmark &  &  &  &  &  &  \\
\checkmark & \checkmark &  &  &  &  &  &  \\
\hline

\hline
\end{tabular}
\end{small}
\caption{Component-wise analysis on TeViT. MSM denotes the messenger shift mechanism and STQI denotes spatiotemporal query interaction. Without applying STQI implies only one  is performed for query interaction within each frame (excluding Eq.~\ref{eq:temporal_query_mhsa}).}
\label{tab:component_wise}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\renewcommand\arraystretch{1.25}
 \renewcommand\tabcolsep{3.1pt}
\begin{small}
\begin{tabular}{c|ccccc}
\hline

\hline
\rowcolor{mygray}
Interaction & AP & AP & AP & AR & AR 
\\
\hline
\hline
Spatial Only~\cite{queryinst}&  &  &  &  &  \\
Fused Space-Time~\cite{vistr} &  &  &  &  &  \\
Ours &  &  &  &  &  \\
\hline

\hline
\end{tabular}
\end{small}
\caption{Variants of spatiotemporal query interaction. ``Spatial Only'' denotes the image-level instance segmentation heads in \cite{queryinst}, ``Fused Space-Time'' denotes applying  to all video instance queries at a single run, which is the same as in \cite{vistr}.}
\label{tab:variants_query_interaction}
\end{minipage}
\end{table*}


\begin{table*}
\centering
\begin{minipage}[t]{0.33\linewidth}
\centering
\renewcommand\arraystretch{1.25}
\renewcommand\tabcolsep{5pt}
\begin{small}
\scalebox{0.9}{
\begin{tabular}{c|c|ccc}
\hline

\hline
\rowcolor{mygray}
Manip. & AP  & AP & AP 
\\
\hline
\hline
 &  &  &  \\
 &  &  &  \\
 &  &  &  \\
\hline

\hline
\end{tabular}}
\caption{Study of the manipulations upon messenger tokens. Our method obtains the highest AP and a relatively stable performance () among all settings.}
\label{tab:shift_vs_attention}
\end{small}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\linewidth}
\centering
\renewcommand\arraystretch{1}
 \renewcommand\tabcolsep{5pt}
\begin{small}
\scalebox{0.9}{
\begin{tabular}{c|ccccc}
\hline

\hline
\rowcolor{mygray}
Manip. & AP & AP & AP
\\
\hline
\hline
 &  &  &  \\
 &  &  &  \\
 &  &  &  \\
 &  &  &  \\
\hline

\hline
\end{tabular}}
\caption{Study of frame-level feature aggregation. Compared to other frame-level feature manipulations, our messenger shift (Row ) obtains the best results.}
\label{tab:frame_level_agg}
\end{small}
\end{minipage}
\hfill
\begin{minipage}[t]{0.33\linewidth}
\centering
\renewcommand\arraystretch{1.25}
\renewcommand\tabcolsep{6pt}
\begin{small}
\scalebox{0.9}{
\begin{tabular}{c|ccccc}
\hline

\hline
\rowcolor{mygray}
 M & AP & AP & AP & AR  & AR  \\
\hline
\hline
 &  &  &  &  &  \\
 &  &  &  &  &  \\
 &  &  &  &  &  \\
\hline

\hline
\end{tabular}}
\caption{Impact of messenger token numbers. M indicates the number of messenger tokens. We increase M from  to  and observe the effects on final performance.}
\label{tab:messenger_token_number}
\end{small}
\end{minipage}
\end{table*}


\subsection{Ablation Study}

\noindent\textbf{Effect of frame-level \& instance-level temporal context modeling.}
We investigate the effects of messenger shift mechanism and spatiotemporal query interaction individually and simultaneously in Tab.~\ref{tab:component_wise}.
Using messenger shift mechanism and spatiotemporal query interaction individually brings  and  AP improvements respectively.
The results show that both frame-level and instance-level temporal context modeling can obviously improve VIS performance. In addition, the instance-level one brings more significant performance gain. The two designs together brings  () AP improvements over a high-performance baseline. Besides the remarkable performance improvements, our designs only bring  computation overhead on our baseline ( GFLOPs \vs  GFLOPs), which demonstrates our design is very efficient.

\noindent\textbf{Variants of spatiotemporal query interaction.}
In Tab.~\ref{tab:variants_query_interaction}, we investigate the effectiveness of our spatiotemporal query interaction comparing to its variants. A naive query interaction method in~\cite{queryinst} without using temporal interaction, denoted as ``Spatial Only'', serves as a baseline. ``Fused Space-Time'' in the Row~ denotes fusing video instance queries together and performing spatial and temporal interaction  within a single , which is the same as in \cite{vistr}. As the results show: (1) Our spatiotemporal query interaction achieves the best performance among three variants.
(2) Compared to the spatial-only query interaction, joint spatiotemporal query interaction brings only  AP improvements. However, our method achieves  AP gains. We argue this is because the one-to-one corresponding between instance queries are misaligned in the joint spatiotemporal attention, while ours is not.

\noindent\textbf{Different messenger token manipulation methods.}
We compare our messenger shift mechanism with two other optional manipulations in Tab.~\ref{tab:shift_vs_attention}.
 indicates there are no extra manipulations conducted on messenger tokens, thus no temporal information is employed.
 stands for the same operation in \cite{ifc} which performs extra  and  on messenger tokens, and  denotes our messenger shift mechanism.
Different from \cite{ifc}, we do not conduct any extra pre-training process so that both the messenger tokens and  layers with  are randomly initialized and trained from scratch.
As results have shown: (1) Our method achieves the best AP and outperforms  by  AP and  by  AP. 
(2) Compared to conducting  upon messenger tokens, our method obtains a more stable performance.  shows more fluctuating final results (see  in Tab.~\ref{tab:shift_vs_attention}) while ours is much more stable.
The results confirm that the randomized  is unable to capture temporal context while our parameter-free shift operation works.

\noindent\textbf{Different manipulations on frame-level feature aggregation.}
We also compare our messenger shift mechanism with other optional frame-level feature aggregation manipulations in Tab.~\ref{tab:frame_level_agg}.
 indicates no manipulation is conducted to aggregate frame-level temporal features.
 and  denotes using newly introduced convolution layers or transformer layers to achieve temporal feature aggregation.
As results have shown, our messenger shift mechanism achieves the best performance (\ie,  AP) compared to its all counterparts.
Meanwhile, we observe apparent performance decrease by using newly introduced  or  as aggregation layers.
We infer such a performance decrease comes from the enormous newly introduced parameters, while our messenger shift mechanism eliminates this performance decrease by the nearly parameter-free design.

\noindent\textbf{Number of messenger tokens.}
In Tab.~\ref{tab:messenger_token_number}, we test our method with number of messenger tokens increases from  to .
Compared to less messenger tokens (), more messenger tokens () achieves better results.
Unless specified, our experiments are conducted with  messenger tokens.

\begin{table*}
\centering
\begin{minipage}[t]{0.49\linewidth}
\centering
\renewcommand\arraystretch{1.4}
\renewcommand\tabcolsep{12.2pt}
\begin{small}
\begin{tabular}{c|ccccc}
\hline

\hline
\rowcolor{mygray}
T & AP & AP & AP & AR & AR 
\\
\hline
\hline
 &  &  &  &  &  \\
 &  &  &  &  &  \\
 &  &  &  &  &  \\
 &  &  &  &  &  \\
\hline

\hline
\end{tabular}
\end{small}
\caption{Effect of training clip length on AP. ``T'' indicates the number of frames for each video clip during training.}
\label{tab:train_clip_length}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\renewcommand\arraystretch{1}
\renewcommand\tabcolsep{9.5pt}
\begin{small}
\begin{tabular}{cc|ccccc}
\hline

\hline
\rowcolor{mygray}
T & S & AP & AP & AP & AR & AR 
\\
\hline
\hline
 &  &  &  &  &  &  \\
 &  &  &  &  &  &  \\
 &  &  &  &  &  &  \\
 &  &  &  &  &  &  \\
 &  &  &  &  &  &  \\
 &  &  &  &  &  &  \\
\hline

\hline
\end{tabular}
\end{small}
\caption{Study the impact of clip length and stride during inference phase. ``T'' and ``S'' indicates the clip length and overlapping stride respectively.}
\label{tab:inference_clip_length}
\end{minipage}
\end{table*}


\begin{table*}
\centering
\begin{minipage}[t]{0.49\linewidth}
\centering
\renewcommand\arraystretch{1.0}
\renewcommand\tabcolsep{4.1pt}
\begin{small}
\begin{tabular}{lcc|ccccc}
\hline

\hline
\rowcolor{mygray}
Method & MST & FPS & AP & AP & AP & AR & AR 
\\
\hline
\hline
VisTR~\cite{vistr} & &  &  &  &  &  &  \\
IFC~\cite{ifc} & \checkmark &  &  &  &  &  &  \\
\multicolumn{2}{l}{Ours w/o. Eq.~\ref{eq:temporal_query_mhsa}} &  &  &  &  &  &  \\
Ours & &  &  &  &  &  &  \\
Ours & \checkmark &  &  &  &  &  &  \\
\hline

\hline
\end{tabular}
\end{small}
\caption{Comparisons with ResNet- as backbone.}
\label{tab:resnet_backbone}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\renewcommand\arraystretch{1.5}
\renewcommand\tabcolsep{4.2pt}
\begin{small}
\begin{tabular}{cc|ccccc}
\hline

\hline
\rowcolor{mygray} Train & Inference & AP & AP & AP & AR & AR 
\\
\hline
\hline
learnable & learned &  &  &  &  &  \\
learnable & zero &  &  &  &  &  \\
learnable & random &  &  &  &  &  \\
\hline

\hline
\end{tabular}
\end{small}
\caption{Revisiting messenger tokens in inference phase.}
\label{tab:understanding_messenger_token}
\end{minipage}
\end{table*}


\noindent\textbf{Training and inference clip length.}
We also investigate the effects of clip length in both the training and testing phase.
From Tab.~\ref{tab:train_clip_length}, we find that: (1) Our method shows great tolerance to short length of training clip. Only trained with  or  frames, our method can effectively learn temporal context and obtains comparable results to previous methods. (2) The performance improvements by increasing the length of the training clip gradually gets saturated. Increasing training clip length from  to  and  to  brings  AP and  AP gains respectively while increasing training clip length from  to  only obtains slight  AP profit. Besides, a longer training clip requires more training computations and memory budgets. To this end, we set the training clip length of our method to  as a compromise between performance and training costs.

Tab.~\ref{tab:inference_clip_length} gives the results of TeViT under different inference settings. ``T'' indicates the input clip length during the inference phase, and ``S'' indicates strides. It shows that our TeViT obtains promising performance under various inference setups. Even with  and , TeViT still achieves  AP, which indicates TeViT can serve as a strong baseline for both offline and online video understanding scenarios.

\noindent\textbf{Performance under ResNet backbone.}
We compare our method with previous transformer-based methods using ResNet-~\cite{resnet} as backbone network in Tab.~\ref{tab:resnet_backbone}.
As results show, our method achieves  AP on YouTube-VIS- dataset with single-scale training (Row~), even outperforming VisTR and IFC with multi-scale training strategy.
The performance goes a step further to  AP when the multi-scale strategy is applied (Row~).
It's worth noting that with ResNet- as backbone network, the messenger tokens, and messenger shift mechanism are unable to proceed, so our method achieves such high performance with only the STQI mechanism.
We also investigate the improvements by our STQI head. Simply taking off the spatial  in Eq.~\ref{eq:temporal_query_mhsa}, the final performance drops from  AP to  AP (Row~), demonstrating the effectiveness of our STQI head directly.

\noindent\textbf{Revisiting messenger tokens.}
Inspired by MSG-Transformer~\cite{msgtransformer}, we re-initialize messenger tokens in inference phase and obvious the influence on performance in Tab.~\ref{tab:understanding_messenger_token}.
As the results show, when we re-initialize messenger tokens to zero, the performance merely drops  AP (compare Row~ to Row~).
Randomly initialize the messenger token in inference phase leads to a similar performance decrease (Row~).
We think this phenomenon implies that the messenger tokens contain only a few or not specific information in themselves.
On the contrary, they play the role of summarizing frame-level contexts, and exchanging them across adjacent frames.

\section{Conclusion}
\label{sec:conclusion}

In this paper, we provide lightweight and effective solutions to fully exploit temporal context for VIS. Based on existing ViTs and query-based image-level instance segmentation methods, we proposes the TeViT VIS method that contains the messenger shift and spatiotemporal query interaction mechanisms. TeViT performs both frame-level and instance-level temporal feature interactions while only bringing a few parameters and marginal extra computational costs. Experiments on YouTube-VIS-, YouTube-VIS-, and OVIS show that TeViT can obtain remarkably better results than previous SoTA methods, \eg, IFC, VisTR, MaskProp, and STEm-Seg. We believe the proposed temporal context modeling mechanisms have great potential to be extended to other video understanding tasks.

\noindent\textbf{Limitations.}
Although the extensive experiments have demonstrated the capacity and efficiency of our TeViT on temporal context modeling, it still suffers effects from occlusion, motion deformation and long time-span videos (\ie, results of TeViT in Tab.~\ref{tab:vis2021} and Tab.~\ref{tab:ovis} are far from satisfying). We leave these promising directions as future work.

\noindent\textbf{Broader impact.}
Although our research does not make direct negative impacts in society, it may be misused by illegal video applications, which could be a potential invasion to human privacy.

\noindent\textbf{Acknowledgement.} This work was in part supported by NSFC (No. 61876212 and No. 61733007) and CAAI-Huawei MindSpore Open Fund.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
