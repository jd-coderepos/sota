

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{overpic}
\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2023}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{amssymb}
\usepackage{amsthm}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    
\usepackage{hyperref}      
\usepackage{url}           
\usepackage{booktabs}       
\usepackage{amsmath}       
\usepackage{nicefrac}     
\usepackage{microtype}
\usepackage{multirow}      
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{amsfonts}      
\usepackage{nicefrac}
\usepackage{enumitem}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{mathtools}
\definecolor{codeblue}{rgb}{0.25,0.5,0.25}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{8pt}{8pt}\ttfamily\selectfont,
  columns=fullflexible,
  numbers=none,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{8pt}{8pt}\color{codeblue},
  keywordstyle=\fontsize{8pt}{8pt}\color{codekw},
}
\usepackage{tikz}
\usepackage{comment}
\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.93}
\definecolor{orange}{rgb}{0.9,0.5,0}

\newcommand{\cocominivalboxmapms}{\textbf{58.7}}
\newcommand{\cocotestdevboxmapms}{\textbf{58.9}}
\newcommand{\adevalss}{\textbf{54.0}}

\newcommand{\PM}[1]{\textcolor{green}{\textbf{[PM: #1]}}}
\newcommand{\yin}[1]{\textcolor{blue}{\textbf{[Yin: #1]}}}
\newcommand{\AH}[1]{\textcolor{red}{\textbf{[AH: #1]}}}
\newcommand{\JK}[1]{\textcolor{cyan}{\textbf{[JK: #1]}}}
\usepackage{hyperref}
\hypersetup{colorlinks, citecolor={teal}, urlcolor=black, urlbordercolor=black}


\def\vs{{\em vs.~}}
\def\Softmax{\textsf{Softmax}} 
\def\Sigmoid{\textsf{Sigmoid}} 
\def\ReLU{\textsf{ReLU}} 
\def\GeLU{\textsf{GeLU}} 
\def\Conv{\textsf{Conv}}
\def\FMBConv{\textsf{F-MBConv}}
\def\SE{\textsf{SE}}
\def\AvgPool{\textsf{Avg Pool}}
\def\MaxPool{\textsf{Max-Pool}}
\newcommand{\blue}{\color{blue}}
            
\usepackage[margin=4pt,font=small,labelfont=bf,labelsep=endash,tableposition=bottom]{caption}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
\newcommand{\adevalms}{\textbf{55.4}}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Global Context Vision Transformers}

\begin{document}

\twocolumn[
\icmltitle{Global Context Vision Transformers}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ali Hatamizadeh}{comp}
\icmlauthor{Hongxu Yin}{comp}
\icmlauthor{Greg Heinrich}{comp}
\icmlauthor{Jan Kautz}{comp}
\icmlauthor{Pavlo Molchanov}{comp}
\end{icmlauthorlist}
\icmlaffiliation{comp}{NVIDIA}

\icmlcorrespondingauthor{Ali Hatamizadeh}{ahatamizadeh@nvidia.com}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{} 

\begin{abstract}
We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in ViTs, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the variants of GC ViT with M, M and M parameters achieve ,  and  Top-1 accuracy, respectively, at  image resolution and without any pre-training, hence surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based MaxViT and Swin Transformer by a large margin. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation using MS COCO and ADE20K datasets outperform prior work consistently. Specifically, GC ViT with a 4-scale DINO detection head achieves a box AP of  on MS COCO dataset. Code is available at
\href{https://github.com/NVlabs/GCViT}{\color{magenta}{https://github.com/NVlabs/GCViT}}.
\end{abstract}

\section{Introduction}

\begin{figure}[!ht] 
\small
\centering
    \begin{minipage}[c]{\linewidth}
    \tiny
    \begin{overpic}[width=\textwidth]
{figures/teaser_flops.pdf}
    \end{overpic}
    \end{minipage}\hfill
    \begin{minipage}{0.328\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/1_img.png}\\
        Input Image
    \end{minipage}\hfill
    \begin{minipage}{0.328\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/1_mask.png}\\
        Global Attention
    \end{minipage}\hfill
    \begin{minipage}{0.328\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/1_grad.png}\\
        GradCAM 
    \end{minipage}
\caption{GC ViT achieves a new Pareto-front with respect to ImageNet Top-1 vs number of parameters trade-off. For fair comparison, models that are trained and evaluated with input image size of  on ImageNet-1K dataset and without pre-training are considered. GC ViT is capable of capturing both short and long-range information using its global attention mechanism. We visualize corresponding attention and GradCAM maps from GC ViT to demonstrate the effectiveness of the proposed global attention mechanism.  
}
\label{fig:benchmark_fig}
\end{figure}


During the recent years, Transformers~\citep{vaswani2017attention} have achieved State-Of-The-Art (SOTA) performance in Natural Language Processing (NLP) benchmarks and became the de facto model for various tasks. A key element in the success of Transformers is the self-attention mechanism which allows for capturing contextual representations via attending to both distant and nearby tokens~\citep{yin2021adavit}. Following this trend, Vision Transformer (ViT)~\citep{dosovitskiy2020image} proposed to utilize image patches as tokens in a monolithic architecture with minor differences comparing to encoder of the original Transformer. Despite the historic dominance of Convolutional Neural Network (CNN) in computer vision, ViT-based models have achieved SOTA or competitive performance in various computer vision tasks. 

In essence, the self-attention mechanism in ViT allows for learning more uniform short and long-range information~\citep{raghu2021vision} in comparison to CNN. However, the monolithic architecture of ViT and quadratic computational complexity of self-attention baffle their swift application to high resolution images~\citep{yang2021nvit} in which capturing multi-scale long-range information is crucial for accurate representation modeling. 

\begin{figure*}[t!]
\centering
    \includegraphics[width=1.0\textwidth]{figures/model_architecture2.pdf}
  \caption{Architecture of the proposed GC ViT. At each stage, a query generator extracts global query tokens which captures long-range information by interacting with local key and value representations. We use alternating blocks of local and global context self attention layers. Best viewed in color.}
  \label{fig:model_architecture}
\end{figure*}

\begin{figure*}[t!]
\centering
    \includegraphics[width=0.9\textwidth,trim={35pt 10pt 15pt 25pt},clip]{figures/Global_vs_local2.pdf}
  \caption{Attention formulation. Local attention is computed on feature patches within local window only (left). On the other hand, the global features are extracted from the entire input features and then repeated to form global query tokens. The global query is interacted with local key and value tokens, hence allowing to capture long-range information via cross-region interaction. Best viewed in color.   
  }
  \label{fig:local_global_att}
\end{figure*}

Several efforts~\citep{liu2021swin,dong2022cswin,chu2021twins,tu2022maxvit}, most notably Swin Transformer~\citep{liu2021swin}, have attempted to address the balance between short- and long-range spatial dependencies by proposing multi-resolution architectures in which the self-attention is computed in local windows. In this paradigm, cross-window connections such as window shifting are used for modeling the interactions across different regions. Despite the progress, the limited receptive field of local windows challenges the capability of self-attention to capture long-range information, and window-connection schemes such as shifting only cover a small neighborhood in the vicinity of each window. Subsequent efforts such as Focal Transformer~\citep{yang2021focal} attempted to address this issue by designing highly sophisticated self-attention modules with increased model complexity. 

In this work, we introduce the Global Context (GC) ViT network to address these limitations. Specifically, we propose a hierarchical ViT architecture consisting of local and global self-attention modules. At each stage, we compute global query tokens, using a novel fused inverted residual blocks, which we refer to as modified Fused-MBConv blocks, that encompass global contextual information from different image regions. While the local self-attention modules are responsible for modeling short-range information, the global query tokens are shared across all global self-attention modules to interact with local key and value representations. 

The design of our proposed framework for global query generator and self-attention is intuitive and simple and can be efficiently implemented using major deep learning framework. Hence, it eliminates sophisticated and computationally expensive operations and ensures the effectiveness of self-attention when applied to high-resolution images. In addition, we propose a novel downsampling block with a parameter-efficient fused-MBConv layer to address the lack of inductive bias in ViTs and enhancing the modeling of inter-channel dependencies.  

We have extensively validated the effectiveness of the proposed GC ViT using three publicly available datasets for various computer vision tasks. For image classification using ImageNet-1K dataset, GC ViT with M, M, M parameters achieve new SOTA benchmarks of , ,  Top-1 accuracy and without using extra data or pre-training. 

Hence, GC ViT consistently outperforms both ConvNeXt \citep{liu2022convnet}, MaxViT~\citep{tu2022maxvit} and Swin Transformer \citep{liu2021swin} models, sometimes by a significant margin (see Fig.~\ref{fig:benchmark_fig}). 

Using an ImageNet-1K pre-trained GC ViT base backbone with a Cascade Mask RCNN~\citep{he2017mask} head, our model achieves a box mAP of \textbf{52.9} for object detection and a mask mAP of \textbf{45.8} for instance segmentation on the MS COCO dataset and by using single-scale inference. We also used an ImageNet-21K GC ViT model as backbone with a 4-scale DINO detection head and achieved a box AP of . 

In addition, using an UPerNet \citep{xiao2018unified} head, our model achieves a mIoU of \textbf{49.2} on ADE20K for semantic segmentation by only using a single-scale inference scheme. Other variants of GC ViT with different learning capacities also demonstrate SOTA results when compared to similarly-sized models on both MS COCO and ADE20K datasets. Hence, GC ViT demonstrates great scalability for high-resolution images on various downstream tasks, validating the effectiveness of the proposed framework in capturing both short and long-range information.  


The main contributions of our work are summarized as follows:


\begin{itemize}[leftmargin=*,nosep]
\item We introduce a compute and parameter-optimized hierarchical ViT with reparametrization of the design space (\textit{e.g.}, embedding dimension, number of heads, MLP ratio).
\item We design an efficient CNN-like token generator that encodes spatial features at different resolutions for global query representations.

\item We propose global query tokens that can effectively capture contextual information in an efficient manner and model both local and global interactions.
\item We introduce a parameter-efficient downsampling module with modified Fused MB-Conv blocks that not only integrates inductive bias but also enables the modeling of inter-channel dependencies.

\item We demonstrate new SOTA benchmarks for : (1) ImageNet classification with Pareto fronts on ImageNet-1K for number of parameters and FLOPs (2) downstream tasks such as detection, instance segmentation and semantic segmentation on MS COCO and ADE20K, respectively.
\end{itemize} \section{GC ViT architecture}


\label{sec:arch}
\textbf{Architecture.} Fig.~\ref{fig:model_architecture} depicts the architecture of GC ViT. We propose a hierarchical framework to obtain feature representations at several resolutions (called stages) by decreasing the spatial dimensions while expanding the embedding dimension, both by factors of . 

At first, given an input image with resolution of , we obtain overlapping patches by applying a  convolutional layer with a stride of  and appropriate padding. Then patches are projected into a -dimensional embedding space with another  convolutional layer with stride . 

Every GC ViT stage is composed of alternating local and global self-attention modules to extract spatial features. Both operate in local windows like Swin Transformer~\citep{liu2021swin}, however, the global self-attention has access to global features extracted by the global query generator. The query generator is a CNN-like module that extracts features from the entire image only once at every stage. After each stage, the spatial resolution is decreased by  while the number of channels is increased by  via a downsampling block. Resulting features are passed through average pooling and linear layers to create an embedding for a downstream task. 


The GC ViT architecture benefits from novel blocks such as \textit{a downsampling operator}, \textit{a global query generator} and \textit{a global self-attention module} described in the next sections. 


\textbf{Downsampler.} We leverage an idea of spatial feature contraction from CNN models that imposes locality bias and cross channel interaction while reducing dimensions.  We utilize a modified Fused-MBConv block, followed by a max pooling layer with a kernel size of  and stride of  as a downsampling operator. The Fused-MBConv block in our work is similar to the one in EfficientNetV2~\citep{tan2021efficientnetv2} with modifications as in

where ,  and  denote Squeeze and Excitation block~\citep{hu2018squeeze}, Gaussian Error Linear Unit~\citep{hendrycks2016gaussian} and  depth-wise convolution, respectively. In our proposed architecture, the Fused-MBConv blocks provide desirable properties such as inductive bias and modeling of inter-channel dependencies.  It is ablated in Table~\ref{tab:abl-sup-downsampler}.

\begin{figure*}[t!]
\centering \includegraphics[width=0.9\textwidth,trim={20pt 12pt 0pt 15pt}, clip]{figures/global-token-danny.drawio.pdf}
  \caption{
Global query generator schematic diagram. It is designed to (i) transform an input feature map to the current stage of dimension  denoting height, width, and channel respectively, (ii) extract features via repeating the modified Fused MBConv block, joint with down-sampling,  times for dimension matching to local window size  (iii) output is reshaped and repeated to  number of local tokens that can attend to global contextual information.  denotes merged dimensions during reshaping.
  }
  \label{fig:global_query}
\end{figure*}

\subsection{Global Self-Attention}
\label{sec:global_sa}
Fig.~\ref{fig:local_global_att} demonstrates the main idea behind our contribution. Local self-attention can only query patches within a local window, whereas the global attention can query different image regions while still operating within the window. At each stage, the global query component is pre-computed.
The global self-attention utilizes the extracted global query tokens
and shared across all blocks, to interact with the local key and value representations. In addition, GC ViT employs alternating local and global self-attention blocks to effectively capture both local and global spatial information. Fig.~\ref{fig:attention_blocks} illustrates the difference between local and global self-attention. The global attention query  has a size of , wherein , ,  and  denote batch size, embedding dimension, local window height and width, respectively. Moreover,  is repeated along the batch dimension to compensate for the overall number of windows and aggregated batch size  where  is the number of local windows.  is further reshaped into multiple heads. The value and key are computed within each local window using a linear layer.


\begin{minipage}{0.48\textwidth}
\floatname{algorithm}{Algorithm.}
\begin{algorithm}[H]
\caption{Global Attention Pseudocode}
\label{alg:global_code}
\begin{lstlisting}[language=python]
# Input/output shape: (B*, N, C);
# B*: Aggregated Batch Size; H: Height;
# W: Width; C: dim; q_g: Global Token;
# F: Num Attention Head; N: H x W.
def init():
    f = nn.Linear(C, 2*C)
    softmax = nn.Softmax(dim=-1)

def forward(x, q_g):
    B*, N, C = x.shape
    B, C, h, w = q_g.shape
    kv = f(x).reshape(B*, N, 2, F, C // F)
    kv = kv.permute(2, 0, 3, 1, 4)
    k, v = split(kv, (1, 1), 0)
    q_g = q_g.repeat(1, B* // B, 1, 1)
    q_g = q_g.reshape(B*, F, N, C // F)
    qk = matmul(q_g,k.transpose(-2, -1))
    attn = softmax(qk)
    return matmul(attn, v).reshape(B*, N, C)
\end{lstlisting}
\end{algorithm}
\end{minipage}



Since the partitioned windows only contain local information, interaction with rich contextual information embedded in the global query tokens provides an effective way of enlarging the receptive field and attending to various regions in the input feature maps. The self-attention module is computed as in    






where  is scaling factor and  is a learnable relative position bias term. Assuming position change between  along horizontal and vertical axes,   is sampled from the grid . As shown in Sec.~\ref{sec:abl}, relative position bias improves the performance, especially for dense prediction downstream tasks. In Algorithm~\ref{alg:global_code}, we present a PyTorch-like pseudocode for computing global self-attention in GC ViT.

\subsection{Complexity Analysis}
Given an input feature map of  at each stage with a window size of , the computational complexity of GC ViT is as follows

The efficient design of global query token generator and other components allows to maintain a similar computational complexity in comparison to Swin Transformer~\cite{liu2021swin} while being able to capture long-range information and achieve better higher accuracy for classification and downstream tasks such as detection and segmentation. \section{Experiments}
\label{sec:exp}

\begin{table}[!t]
	\caption{Image classification benchmarks on \textbf{ImageNet-1K} dataset~\citep{deng2009imagenet}. Models that are trained on ImageNet-1K dataset and without any pre-training or usage of extra data are considered.}
	\label{tab:classfication_main}
	\centering
\resizebox{1\linewidth}{!}{
	\begin{tabular}[t]{ l |c|c|c|Hl}
		\toprule
		Model & Param (M) & FLOPs (G) & Image Size & Step (s)& Top-1 (\%)  \\
          \midrule
		\multicolumn{6}{c}{ConvNet} \\
   \midrule
   ResNet50~\citep{he2016deep}   &25 &4.1 &224 &-&76.1 \\
   ResNet-101~\citep{he2016deep}  &44 & 7.9 &224&-&77.4\\
   ResNet-152~\citep{he2016deep} &60 & 11.6 &224 &-&78.3 \\
EfficientNetV2-B2~\cite{tan2021efficientnetv2}  & 10 & 1.6 & 260&- & 80.2\\
EfficientNetV2-B3~\cite{tan2021efficientnetv2}  & 14 & 2.9 & 300 &- & 82.0\\
EfficientNetV2-S~\cite{tan2021efficientnetv2}  & 21 & 8.0 & 384 &- & 83.9\\
RegNetY-040~\cite{radosavovic2020designing}  & 20 & 6.6 & 288&-& 83.0\\
RegNetY-064~\cite{radosavovic2020designing}  & 30 & 10.5 & 288&- & 83.7\\
   ConvNeXt-T~\citep{liu2022convnet}  & 29& 4.5 & 224 & 0.90 & 82.1 \\
   ConvNeXt-S~\citep{liu2022convnet}  & 50& 8.7 & 224 & 0.90 & 83.1 \\
   ConvNeXt-B~\citep{liu2022convnet}  & 89& 15.4 & 224 & 0.90 & 83.8\\
   ConvNeXt-L~\citep{liu2022convnet}  & 198& 34.4 & 224 & 0.90 & 84.3\\
          \midrule
		\multicolumn{6}{c}{Transformer} \\
		\midrule
  ViT-B~\citep{dosovitskiy2020image} & 86 & 17.6 & 224& & 77.9 \\
  DeiT-S/16~\citep{touvron2021training}  & 22 & 4.6 & 224 && 79.9 \\
		DeiT-B~\citep{touvron2021training} & 86 & 17.6 & 224& & 81.8 \\
	
Swin-T \citep{liu2021swin} & 29 & 4.5& 224 & 0.48 &81.3 \\
		Swin-S~\citep{liu2021swin} & 50 & 8.7 & 224& 0.73 &83.0\\
  Swin-B \citep{liu2021swin} & 88 & 15.4& 224& 0.89 &83.3  \\
  Twins-S~\cite{chu2021twins}  & 24 & 2.8 & 224&- & 81.7\\
Twins-B~\cite{chu2021twins}  & 56 & 8.3 & 224&- & 83.1\\
Twins-L~\cite{chu2021twins}  & 99 & 14.8 & 224&- & 83.7\\
   Focal-T~\citep{yang2021focal}  & 29& 4.9 & 224 & 0.90 & 82.2
		\\
		Focal-S~\citep{yang2021focal}  & 51& 9.1 & 224 & 0.90 & 83.5 \\
  Focal-B~\citep{yang2021focal}  & 90& 16.0 & 224 & 0.90 & 83.8
		\\ 
 PoolFormer-S36~\cite{yu2022metaformer}  & 31 & 5.0 & 224&- & 81.4\\
PoolFormer-M36~\cite{yu2022metaformer}  & 56 & 8.8 & 224&- & 82.1\\ 
PoolFormer-M58~\cite{yu2022metaformer}  & 73 & 11.6 & 224&- & 82.4\\
 SwinV2-T~\cite{liu2022swin}  & 28 & 4.4 & 256&- & 81.8\\
SwinV2-S~\cite{liu2022swin}  & 49 & 8.5 & 256&- & 83.8\\
SwinV2-B~\cite{liu2022swin}  & 88 & 15.1 & 256&- & 84.6\\

 \midrule
		\multicolumn{6}{c}{Hybrid} \\
		\midrule
  CrossViT-S~\cite{chen2021crossvit}  & 27 & 5.1 & 224&- & 81.0\\
CrossViT-B~\cite{chen2021crossvit}  & 105 & 20.1 & 224&- & 82.2\\
CoAtNet-0~\citep{dai2021coatnet}  & 25& 4.2 & 224 & 0.90 & 81.6\\
  CoAtNet-1~\citep{dai2021coatnet}  & 42& 8.4 & 224 & 0.90 & 83.3\\
  CoAtNet-2~\citep{dai2021coatnet}  & 42& 8.4 & 224 & 0.90 & 83.3\\
		CoAtNet-3~\citep{dai2021coatnet}  & 168& 34.7 & 224 & 0.90 & 84.5\\
  PVT-v2-B2~\citep{wang2022pvt}   &25 &4.0 &224&-&82.0 \\
 		PVT-v2-B3~\citep{wang2022pvt}   &45 &6.9 &224&-&83.2 \\
		PVT-v2-B5~\citep{wang2022pvt}   &82 &11.8 &224&-&83.8 \\
    CSwin-T~\citep{dong2022cswin}  & 23& 4.3 & 224 & 0.90 & 82.7
 \\
		CSwin-S~\citep{dong2022cswin}  & 35& 6.9 & 224 & 0.90 & 83.6\\
  CSwin-B~\citep{dong2022cswin}  & 78& 15.0 & 224 & 0.90 & 84.2
		\\
    MaxViT-T~\citep{tu2022maxvit}  & 31& 5.6 & 224 & 0.90 & 83.6
    		\\
    MaxViT-S~\citep{tu2022maxvit}  & 69& 11.7 & 224 & 0.90 & 84.4
    		\\
    MaxViT-B~\citep{tu2022maxvit}  & 120& 74.2 & 224 & 0.90 & 84.9
    		\\
    MaxViT-L~\citep{tu2022maxvit}  & 212& 43.9 & 224 & 0.90 & 85.1
 \\
            \midrule
		\multicolumn{6}{c}{\textbf{GC ViT}} \\
   \midrule
  \rowcolor{Gray}
		\textbf{GC ViT-XXT}& 12& 2.1 & 224 & 0.90 & \textbf{79.9}  \\
  \rowcolor{Gray}
		\textbf{GC ViT-XT}& 20& 2.6 & 224 & 0.90 & \textbf{82.0}  \\ 
		\rowcolor{Gray}
		\textbf{GC ViT-T}& 28& 4.7 & 224 & 0.90 & \textbf{83.5}\\ 
  		\rowcolor{Gray}
		\textbf{GC ViT-T2}& 34& 5.5 & 224 & 0.90 & \textbf{83.7}\\\rowcolor{Gray}
		\textbf{GC ViT-S}& 51& 8.5 & 224 & 0.90 & \textbf{84.3} \\
  		\rowcolor{Gray}
		\textbf{GC ViT-S2}& 68& 10.7 & 224 & 0.90 & \textbf{84.8}\\
   \rowcolor{Gray}
		\textbf{GC ViT-B}& 90& 14.8 & 224 & 0.90 & \textbf{85.0} \\
		\rowcolor{Gray}
		\textbf{GC ViT-L}& 201& 32.6 & 224 & 0.90 & \textbf{85.7} \\
		\bottomrule
  

	\end{tabular}
	}
\label{tab:imgnet}
\end{table}

For image classification, we trained and tested our model on ImageNet-1K dataset \citep{deng2009imagenet}. To allow for a fair comparison, all GC ViT variants are trained by following training configurations of previous efforts \citep{liu2021swin,yang2021focal,chu2021twins}. Specifically, all models are trained with the AdamW \citep{kingma2014adam} optimizer for  epochs with an initial learning rate of , weight decay of , cosine decay scheduler and 20 warm-up and cool-down epochs, respectively.


For object detection and instance segmentation, we trained our model on MS COCO \citep{lin2014microsoft} with DINO~\citep{he2017mask} and a Mask-RCNN~\citep{he2017mask} heads, using  LR schedule with an initial learning rate of , a batch size of  and weight decay of . Following~\cite{liu2022convnet}, we compared against Tiny, Small and Base model variants using Cascade Mask-RCNN but only compared against Tiny variant using Mask-RCNN. For semantic segmentation, we used the ADE20K dataset \citep{zhou2017scene} with a UPerNet~\citep{xiao2018unified} segmentation head. Following previous efforts, we used a random crop size of  for the input images. 


\setlength{\tabcolsep}{8pt}
\begin{table*}[!t]
    \centering
    \caption{Object detection and instance segmentation benchmarks using Mask R-CNN and Cascade Mask R-CNN on \textbf{MS COCO} dataset~\citep{lin2014microsoft}. All models employ  schedule.}
    \label{tab:cascademaskrcnn}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{l|cc|cccccc}
        \toprule
        Backbone & Param (M) & FLOPs (G) &  &  &  &  &  &  \\ 
        \midrule
		\multicolumn{9}{c}{Mask-RCNN 3 schedule} \\
		\midrule

        Swin-T~\citep{liu2021swin} & 48 & 267 & 46.0 & 68.1 & 50.3 & 41.6 & 65.1 & 44.9 \\
        ConvNeXt-T~\citep{liu2022convnet} & 48 & 262 & 46.2 & 67.9 & 50.8 & 41.7 & 65.0 &44.9
        \\ \rowcolor{Gray}
        \textbf{GC ViT-T} & 48 & 291 & \textbf{47.9} & \textbf{70.1} & \textbf{52.8} & \textbf{43.2} & \textbf{67.0} & \textbf{46.7} \\
        \midrule
		\multicolumn{9}{c}{Cascade Mask-RCNN 3 schedule} \\
		\midrule
        DeiT-Small/16~\citep{touvron2021training} & 80 & 889 & 48.0 & 67.2 & 51.7 & 41.4 & 64.2 & 44.3 \\
		ResNet-50~\citep{he2016deep} & 82 & 739 & 46.3 & 64.3 & 50.5 & 40.1 & 61.7 & 43.4 \\
        Swin-T~\citep{liu2021swin} & 86 & 745 & 50.4 & 69.2 & 54.7 & 43.7 & 66.6 & 47.3 \\
        ConvNeXt-T~\citep{liu2022convnet} & 86 & 741 & 50.4 & 69.1 & 54.8 & 43.7 & 66.5 & 47.3
        \\ \rowcolor{Gray}
        \textbf{GC ViT-T} & 85 & 770 & \textbf{51.6} & \textbf{70.4} & \textbf{56.1} & \textbf{44.6} & \textbf{67.8} & \textbf{48.3} \\
		\midrule
		X101-32~\citep{xie2017aggregated} & 101 & 819 & 48.1 & 66.5 & 52.4 & 41.6 & 63.9 & 45.2 \\
        Swin-S~\citep{liu2021swin} & 107 & 838  & 51.9 & 70.7 & 56.3 & 45.0 & 68.2 & 48.8 \\
        ConvNeXt-S~\citep{liu2022convnet} & 108 & 827 & 51.9 & 70.8 & 56.5 & 45.0 & 68.4 & 49.1
        \\ \rowcolor{Gray}
        \textbf{GC ViT-S} & 108 & 866 & \textbf{52.4} & \textbf{71.0} & \textbf{57.1} & \textbf{45.4} & \textbf{68.5} & \textbf{49.3} \\
        \midrule
        X101-64~\citep{xie2017aggregated} & 140 & 972 & 48.3 & 66.4 & 52.3 & 41.7 & 64.0 & 45.1 \\
        Swin-B~\citep{liu2021swin} & 145 & 982  & 51.9 & 70.5 & 56.4 & 45.0 & 68.1 & 48.9
        \\
        ConvNeXt-B~\citep{liu2022convnet} & 146 & 964 & 52.7 & 71.3 & 57.2 & 45.6 & 68.9 & 49.5 \\
        \rowcolor{Gray}
        \textbf{GC ViT-B} & 146 & 1018 & \textbf{52.9} & \textbf{71.7} & \textbf{57.8} & \textbf{45.8} & \textbf{69.2} & \textbf{49.8} \\
        \bottomrule
    \end{tabular}
    }
\end{table*}



\subsection{Classification}
\label{sec:exp_imagenet_results}

We present the ImageNet-1K classification benchmarks in Table~\ref{tab:imgnet} and compare against CNN and ViT-based models across different model sizes. Our model achieves better performance when compared to other established benchmarks such as ConvNeXt~\citep{liu2022convnet}. Furthermore, as shown in Fig.~\ref{fig:benchmark_fig}, GC ViT models have better or comparable computational efficiency in terms of number FLOPsover the competing counterpart models.


\subsection{Detection and Instance Segmentation}
\label{sec:exp_mscoco_results}
In Table~\ref{tab:cascademaskrcnn}, we present object detection and instance segmentation benchmarks on MS COCO dataset. Using a Mask-RCNN head, the model with pre-trained GC ViT-T (47.9/43.2) backbone outperforms counterparts with pre-trained ConvNeXt-T \citep{liu2022convnet} (46.2/41.7) by +1.7 and +1.5 and Swin-T \citep{liu2021swin} (46.0/41.6) by +1.9 and +1.6 in terms of box AP and mask AP, respectively. Using a Cascade Mask-RCNN head, the models with pre-trained GC ViT-T (51.6/44.6) and GC ViT-S (52.4/45.4) backbones outperform ConvNeXt-T \citep{liu2022convnet} (50.4/43.7) by +1.2 and +0.9 and ConvNeXt-S \citep{liu2022convnet} (51.9/45.0) by +0.5 and +0.4 in terms of box AP and mask AP, respectively. Furthermore, the model with GC ViT-B (52.9/45.8) backbone outperforms the counterpart with ConvNeXt-B \citep{liu2022convnet} (52.7/45.6) by +0.2 and +0.2 in terms of box AP and mask AP, respectively. 

As shown in Table~\ref{tab:cascademaskrcnn}, we have also tested the performance of GC ViT-L model, pre-trained on ImageNet-21K dataset, with a 4-scale DINO~\citep{zhang2022dino} detection head and achieved a box AP of  on MS COCO dataset. Hence our model outperforms the counterpart with Swin-L backbone.


\subsection{Semantic Segmentation}
\label{sec:exp_ade_seg_results}
We present semantic segmentation benchmarks on ADE20K dataset in Table~\ref{tab:ade_segmentation}. The models using pre-trained GC ViT-T (47.0), GC ViT-S (48.3) and GC ViT-B (49.2) backbones outperform counterpart models with pre-trained Twins-SVT-S \citep{chu2021twins} (46.2), Twins-SVT-B \citep{chu2021twins} (47.7) and Twins-SVT-L \citep{chu2021twins} (48.8) by +0.8, +0.6 and +0.4 in terms of mIoU, respectively. In addition, models with GC ViT backbones significantly outperform counterparts with Swin Transformer backbones, hence demonstrating the effectiveness of the global self-attention.


\begin{table}
\centering
\resizebox{1.0\linewidth}{!}{
\footnotesize
\setlength{\tabcolsep}{2.5pt}
  \begin{tabular}{lccc}
    \toprule
    Backbone  & Head & Scale &  \\
    \midrule
    ResNet-50~\citep{he2016deep}&  DINO~\citep{zhang2022dino}&4 & 50.9 \\
    ResNet-50~\citep{he2016deep}&  DINO~\citep{zhang2022dino}&5 & 51.2 \\
Swin-L~\citep{liu2021swin}  & DINO~\citep{zhang2022dino} & 4 & 58.0\\
    \rowcolor{Gray}
    \textbf{GC ViT-L}  & DINO~\citep{zhang2022dino} & 4 & \textbf{58.3}\\
    \bottomrule

  \end{tabular} 
  }
\caption{Object detection benchmarks using DINO~\citep{zhang2022dino} network on \textbf{MS COCO} dataset~\citep{lin2014microsoft}.  denotes models that are pre-trained on ImageNet-21K dataset.}
\label{tab:dino}
\end{table}

\begin{table}
\centering
\resizebox{1.0\linewidth}{!}{
\footnotesize
\setlength{\tabcolsep}{2.5pt}
  \begin{tabular}{lccc}
    \toprule
    Backbone  & Param (M) & FLOPs (G) & mIoU \\
    \midrule	 
    DeiT-Small/16~\citep{touvron2021training}  & 52 & 1099 & 44.0\\
    Swin-T~\citep{liu2021swin}  & 60 & 945 & 44.5\\
    ResNet-101~\citep{he2016deep}  & 86 & 1029 & 44.9\\
    Focal-T~\citep{yang2021focal}  & 62 & 998 & 45.8\\
    Twins-SVT-S~\citep{chu2021twins}  & 55 & - & 46.2\\
    \rowcolor{Gray}
\textbf{GC ViT-T}  & 58 & 947 & \textbf{47.0}\\
    \midrule
    Swin-S~\citep{liu2021swin}  & 81 & 1038 & 47.6\\
    Twins-SVT-B~\citep{chu2021twins}  & 89 & - & 47.7\\
    Focal-S~\citep{yang2021focal}  & 85 & 1130 & 48.0\\
    \rowcolor{Gray}
    \textbf{GC ViT-S}  & 84 & 1163 & \textbf{48.3}\\
\midrule
    Swin-B~\citep{liu2021swin}  & 121 & 1188 & 48.1\\
    Twins-SVT-L~\citep{chu2021twins}  & 133 & - & 48.8\\
    Focal-B~\citep{yang2021focal}  & 126 & 1354 & 49.0\\
    \rowcolor{Gray}
    \textbf{GC ViT-B}  & 125 & 1348 & \textbf{49.2}\\
\bottomrule

  \end{tabular} 
  }
\caption{Semantic segmentation benchmarks \textbf{ADE20K} validation set with UPerNet~\citep{xiao2018unified} and pre-trained ImageNet-1K backbone. All models use a crop size of  and use single-scale inference.}
\label{tab:ade_segmentation}
\end{table}

\begin{figure*}[!t]
\centering

\resizebox{0.85\linewidth}{!}{
\begingroup
\renewcommand*{\arraystretch}{0.3}
\begin{tabular}{c}

  \includegraphics[width=1\linewidth]{figures/imgs.jpg} \\ 
  \small{(a) Original images from ImageNet-1K validation set.} \3pt]
 

\includegraphics[width=1\linewidth]{figures/query.jpg} \\
 \small{(b) Learned \textbf{global query} tokens.}
\end{tabular}
\endgroup
}
\caption{Visualization of : (a) input images (b) learned global query token feature maps.}
\vspace{-2mm}
\label{fig:att_maps_supp}
\end{figure*}

\setlength{\tabcolsep}{4pt}
\begin{table}[h]
\centering
\resizebox{.5\linewidth}{!}{
\setlength{\tabcolsep}{2.5pt}
  \begin{tabular}{lcccc}
    \toprule
    Model&Local Batch Size   & Global Batch Size&EMA&  Top-1 \\
    \midrule	 
    GC ViT-T&32  & 1024&No&83.45 \\
    GC ViT-T&128  & 4096&No&83.46 \\
    GC ViT-T&32  & 1024&Yes&83.47\\
    \hline
    \rowcolor{Gray}
   GC ViT-T&128  & 4096&Yes&83.48\\
    \bottomrule
    
  \end{tabular} 
  }
    \caption{Ablation study on the effect of EMA and batch size on GC ViT-T ImageNet Top-1 accuracy.}
    \label{tab:ema}
\end{table}

\subsection{Training Details}
For image classification, GC ViT models were trained using four computational nodes with 32 NVIDIA A100 GPUs. The total training batch size is  ( per GPU) for GC ViT-S, GC ViT-B, GC ViT-L and  ( per GPU) for GC ViT-XXT, GC ViT-XT and GC ViT-T. On average, each model required  hours of training with the specified hyper-parameters as indicated in the paper. All classification models were trained using the \verb|timm| package~\citep{rw2019timm}. Object detection and instance segmentation models as well as semantic segmentation models were trained using one computational node with 8 NVIDIA A40 GPUs using a total batch size of , hence a batch size of  per GPU. Detection and instance segmentation models were trained using \verb|mmdetection| \citep{chen2019mmdetection} package and on average required  hours of training.  Semantic segmentation models were trained using \verb|mmsegmentation|~\citep{mmseg2020} package, and on average required  hours of training.





\subsection{Interpretability}
In Fig.~\ref{fig:att_maps_supp}, we illustrate the learned global query token maps and demonstrate their effectiveness in capturing long-range contextual representations from different image regions. 





\end{document}
