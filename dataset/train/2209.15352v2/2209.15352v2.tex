\section{Experiments}
\label{sec:exp}
In this section we start by providing a detailed description of the experimental setup. Next, we present the main results for audio generation, and we conclude this section with an ablation study.

\subsection{Experimental Setup}
\label{sec:setup}
{\noindent \bf{Dataset.}} We use a set of several datasets: AudioSet~\citep{gemmeke2017audio}, BBC sound effects~\footnote{\url{https://sound-effects.bbcrewind.co.uk/}}, AudioCaps~\citep{kim2019audiocaps}, Clotho v2~\citep{drossos2020clotho}, VGG-Sound~\citep{chen2020vggsound}, FSD50K~\citep{fonseca2021fsd50k}, Free To Use Sounds~\footnote{\url{https://www.freetousesounds.com/all-in-one-bundle/}}, Sonniss Game Effects~\footnote{\url{https://sonniss.com/gameaudiogdc}}, WeSoundEffects~\footnote{\url{https://wesoundeffects.com/we-sound-effects-bundle-2020/}}, Paramount Motion - Odeon Cinematic Sound Effects~\footnote{\url{https://www.paramountmotion.com/odeon-sound-effects}}. All audio files were sampled at 16kHz.

For textual descriptions we use two types of annotations. The first one is multi-label annotations, available for the datasets: AudioSet, VGG-Sound, FSD50K, Sinniss Game Effects, WeSoundEffects, Paramount Motion - Odeon Cinematic Sound Effects. We form pseudo-sentences by concatenating lists of tags available per audio samples (e.g., ``dog, bark, park" is transformed to "dog bark park"). The second type of annotation is natural language captions available for the datasets: AudioCaps, Clotho v2, Free To Use Sounds, and BBC Sound Effects. A more elaborate description of the used datasets can be found in Appendix~\ref{tab:datasets}.
We apply a pre-processing step to better match the class-label annotation distribution. Specifically, we remove stop words and numbers, finally we lemmatize the remaining words (e.g. ``a dog is barking at the park" is transformed to "dog bark park") using the WordNet lemmatizer in NLTK~\citep{bird2009natural}. As speech is the dominant class in the data, we filter all samples where the tag or caption contains the word ``speech'' to generate a more balanced dataset. Overall we are left with $\sim$4k hours for training data.

{\noindent \bf{Data Augmentations.}} One of the most impressive capabilities of recently proposed generative models~\citep{ramesh2022hierarchical, imagen, gafni2022make} is their ability to create unseen object compositions (e.g., ``An astronaut riding a horse in space''). To achieve similar capabilities with regards to audio generation we propose an augmentation method that fuses pairs of audio samples and their respective text captions, thus creating new concept compositions during training. Formally, given two audio samples $\vx_1, \vx_2$ and their respective text captions $\vc_1, \vc_2$, we first randomly draw an temporal offset to merge the two audio samples. Next, we draw a random \ac{SNR} in the interval $[-5,5]$, and finally we mix the audio samples and concatenate the text captions $\vc_1, \vc_2$.

{\noindent \bf{Evaluation Methods.}}
We evaluate all models and baselines using both objective and subjective metrics. For the objective functions we compute the \ac{FAD}~\citep{Kilgour2019FrchetAD} over both real and generated samples. \ac{FAD}, adapted from the \ac{FID} to the audio domain, is a reference-free evaluation metric that closely correlates with human perception. Similarly to~\citet{yang2022diffsound} we additionally compute the KL-Divergence between the output of a state-of-the-art audio classification model~\citep{koutini2021efficient} while feeding it both the original samples and the generated audio.
The FAD was shown to correlate well with human perception in terms of audio quality. On the other hand, the KL measure is computed using the label distribution produced by a pre-trained classifier. Hence, it reflects more on the broader audio concepts occurring in the recording. As a result, the two metrics are complementary. 

For subjective methods, we follow the a similar setting to~\citep{yang2022diffsound}. We ask human raters to evaluate two main aspects of the audio signal (i) overall quality (OVL), and (ii) relevance to the text input. We follow the MUSHRA protocol~\citep{series2014method}, using both a hidden reference and a low anchor. For the overall quality test raters were asked to rate the perceptual quality of the provided samples in a range between 1 to 100. For the text relevance test, raters were asked to rate the match between audio and text on a scale between 1 to 100. Raters were recruited using the Amazon Mechanical Turk platform. We evaluate 100 randomly sampled files from the AudioCaps test set, where each sample was evaluated by at least 5 raters. We verified that the majority of the samples (85\%) contain at least two audio concepts (e.g., ``a dog barks while a bird chirps''). We use the CrowdMOS package\footnote{\url{http://www.crowdmos.org/download/}} to filter noisy annotations and outliers. We remove annotators who did not listen to the full recordings, annotators who rate the reference recordings less then 85, and the rest of the recommended recipes from the CrowdMOS~\citep{ribeiro2011crowdmos}. Participants in this study were paid at least the American minimum wage.

{\noindent \bf{Hyper-parameters.}} We trained two sets of \ac{ALM}s, one with 285M parameters (base) and the other with 1B parameters (large). In the smaller model we use a hidden-size of 768, 24 layers and 16 attention-heads, while for the large variant we use a hidden size 1280, 36 layers and 20 attention-heads. We use the Adam optimizer with a batch size of 256, a learning rate of 5e-4 and 3k steps of warm-up followed by inverse-square root decay. The small model was trained on 64 A100 GPUs for 200k steps ($\sim$5 days) and the large model was trained on 128 A100 GPUs for 200k steps ($\sim$1 week). For the small model we use T5-base and for the large model we use T5-large. For sampling, we employ top-$p$~\citep{holtzman2019curious} sampling with $p=0.25$. For the \ac{CFG} we use a $\gamma=3.0$. 

\begin{table}[t!]
\centering \small
\caption{\label{tab:main_res} Results are reported for DiffSound together with several versions of \audiogen. For DiffSound data augmentation, we follow the authors suggested mask-based text generation (MBTG) strategy. For subjective tests we report overall quality (OVL), and text relevenace (REL.) together with 95\% Confidence Interval. For the objective metrics we report FAD and KL. }
\resizebox{\columnwidth}{!}{\begin{tabular}{l|ccc|cc|cc}
\toprule
& & & & \multicolumn{2}{c}{\textsc{Subjective}} & \multicolumn{2}{c}{\textsc{Objective}} \\
 & \#params & \textsc{Aug.} & \textsc{Text-cond.} & \textsc{OVL}$\uparrow$ & \textsc{Rel.}$\uparrow$ & \textsc{FAD}$\downarrow$ & \textsc{KL}$\downarrow$  \\
\midrule
Reference & - & - & - & 92.08\pmr{1.16} & 92.97\pmr{0.85} & - & - \\
\midrule
DiffSound  & 400M & MBTG & CLIP &  65.68\pmr{1.58}  & 55.91\pmr{1.75} & 7.39 & 2.57 \\
\audiogen-base & 285M  & - & T5-base &  70.85\pmr{1.06}  & 63.23\pmr{1.65} & 2.84 & 2.14\\
\audiogen-base & 285M & Mix & T5-base & \bf{71.68\pmr{1.89}}  & 66.01\pmr{1.79} & 3.13 & 2.09\\
\audiogen-large & 1B & Mix & T5-large & \bf{71.85\pmr{1.07}}  & \bf{68.73\pmr{1.61}} & \bf{1.82} & \bf{1.69} \\
\bottomrule
\end{tabular}}
\end{table}


\begin{figure}[t!]
\centering
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/cont_1_1.pdf}
    \caption{\label{fig:first}}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/cont_1_2.pdf}
    \caption{\label{fig:mid}}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/cont_2.pdf}
    \caption{\label{fig:second}}
\end{subfigure}
\caption{\label{fig:audio_cont} Audio continuation results. In (a) we compare model generations against audio corresponding to \src~ and \rnd. In (b) we compare model generations with and without text conditioning against audio corresponding to \rnd~ text. In (c) we compare model generations with and without text conditioning against audio corresponding to \src~ text. In all settings we report KL results. Specific details can be found on Section~\ref{sec:res}.}
\end{figure}

\subsection{Results}
\label{sec:res}

We start by comparing \audiogen~~ to DiffSound. Objective metrics are reported on the AudioCaps test set. We use the official pre-trained model provided by DiffSound authors~\footnote{Pre-trained model can be found under https://github.com/yangdongchao/Text-to-sound-Synthesis}. The model was trained on AudioSet and fine-tuned on AudioCaps.
Results are presented in Table~\ref{tab:main_res}. 

\audiogen-base outperforms DiffSound considering all metrics while being smaller in terms of parameters count. As expected \audiogen-large, significantly outperforms both DiffSound and \audiogen-base. Notice, for \audiogen-base, the model trained without mixing obtained superior \ac{FAD} and comparable OVL to the model trained with mixing augmentations. However, when considering relevance to text, i.e., KL and \textsc{Rel.} the model with mixing augmentations reaches better performance. This is especially noticeable for the \textsc{Rel.} metric as it contains mostly complex compositions (see Table~\ref{tab:main_res}). We additionally compare \audiogen-base to the same dataset setup used by DiffSound (i.e., training on AudioSet and AudioCaps). \audiogen-base reached KL of 2.46 vs. 2.57 for DiffSound and FAD of 4.39 vs. 7.39 for DiffSound. These results suggest that \audiogen is still significantly superior to DiffSound.

Next, we experimented with pre-training the \ac{ALM} component on only audio tokens without conditioning on text (learning an audio prior). 
We did not observe a gain in doing \ac{ALM} pretraining. We hypothesize that is due to the pre-training process taking place on the same labeled data. Samples can be found under the following link: \url{https://felixkreuk.github.io/audiogen}.








{\noindent \bf{Audio Continuation.}} Similarly to~\citep{lakhotia2021generative}, we first encode an audio prompt into a sequence of audio tokens, feed it into the \ac{ALM}, and sample a continuation. Unlike previous work, we can additionally steer the generated audio towards textual captions.
We evaluate our model given audio prompts of lengths [0.5s, 1s, 2s, 3s, 4s] and different combinations of text prompts: (i) no-text; (ii) text corresponding to the audio prompt (\src); (iii) text taken from a randomly sampled audio file (\rnd). For each of the above settings we measure the KL between the generated audio and either the audio corresponding to the source text or the target text. 

In Figure~\ref{fig:audio_cont}(a) we input the model with an audio prompt together with \rnd as conditioning text. We evaluate KL between the output of the classification model using the generated audio against either the audio prompt or the audio corresponding to the \rnd. Results suggest that by using short audio prompts we could better steer the generation towards the conditioning text. In contrast, using long audio prompts leaves less room for textual guidance. Text and audio prompts have roughly the same impact at $\sim$1.5s. In Figure~\ref{fig:audio_cont}(b) we input the model with an audio prompt with and without \rnd text conditioning. We evaluate the KL between the output of the audio classification model using the generated audio against the audio corresponding to the \rnd. Although using longer audio prompts leaves less room for textual guidance, we still observe a gap between generations with and without text, even when using longer audio prompts ($\sim$4s). This suggests that text has a significant effect on the generation process. Lastly, In Figure~\ref{fig:audio_cont}(c) we condition the model on an audio prompt with and without \src text conditioning. We evaluate the KL between the output of the audio classification model using the generated audio against the input audio. Results suggest that when using short audio prompts, text has a major impact on the generated output. However as we feed longer sequences, the audio is sufficient to steer the generation towards the target concept classes with a minimal gain when adding text.




Full results together with FAD scores for all settings together with a visual depiction can be found in Table~\ref{tab:full_audio_cont} on the supplementary \ref{sec:app_res}.



\subsection{Ablation Study.}
\label{sec:abl}

{\noindent \bf{The effect of classifier-free guidance scale.}} As pointed out by~\citet{ho2021classifier}, the choice of the \ac{CFG} scale offers a trade-off between sample diversity and quality with respect to the conditioning text. To gauge the effect of the $\gamma$ parameter in our setting on Figure~\ref{fig:cfg} we report results for $\gamma \in \{1.0, 2.0, 3.0\}$. Notice, setting $\gamma=1$ is equivalent to a vanilla sampling procedure (no \ac{CFG}).


Removing the \ac{CFG} results in poor performance compared to $\gamma > 1.0$. The FAD score reaches its' minimum at $\gamma = 3.0$, while the KL monotonically decreases and converges at $\gamma=4.0$. This implies that using $\gamma = 3.0$ provides the best trade-off in the evaluated setting between quality and diversity.



\begin{figure}
\centering
\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/fad.pdf}
    \label{fig:first}
\end{subfigure}
\begin{subfigure}{0.46\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/kl.pdf}
    \label{fig:second}
\end{subfigure}
\caption{\label{fig:cfg} Results of FAD (left) and KL (right) as a function of the guidance scale.}
\label{fig:figures}
\end{figure}

\begin{table}[t!]
\centering \small
\caption{\label{tab:mst} Multi-stream results: we report two sets of results encoding and generation. For the encoding metrics we report Bitrate (kbps), SI-SNR (dBs), and ViSQOL. For the generation scores we report FAD, KL and inference speed-up. We additionally include the number of streams and the Down Sampling Factor (DSF). Notice, the encoding metrics are the same for the large and base model as the same audio representation model was used.
}
\resizebox{1.0\columnwidth}{!}{\begin{tabular}{l|cc|ccc|cccc}
\toprule
 & & & \multicolumn{3}{c}{\textsc{Encoding}} & \multicolumn{3}{c}{\textsc{Generation}} & \\
 & \textsc{\# streams} & \textsc{DSF} & Bitrate (kbps) & \textsc{SI-SNR}$\uparrow$ & \textsc{ViSQOL}$\uparrow$ & \textsc{FAD}$\downarrow$ & \textsc{KL}$\downarrow$ & \textsc{Speed-Up}\\
\midrule
\multirow{3}{*}{\rotatebox{90}{base}} & 1 & x32 & 5.37 & 5.1 & 3.95 & 3.13  & 2.09 & x1.0\\
& 2 & x64 & 4.88 & 4.5 & 3.94 & 10.35 & 2.17 & x2.0 \\
& 4 & x128 & 4.39 & 4.2 & 3.91 & 9.68  & 2.36 & x5.1 \\
\midrule
\multirow{3}{*}{\rotatebox{90}{large}} & 1 & x32 & 5.37 & 5.1 & 3.95 & 1.82 & 1.69 & x1.0\\
& 2 & x64 & 4.88 & 4.5 & 3.94 & 6.89  & 1.86 & x2.3 \\
& 4 & x128 & 4.39 & 4.2 & 3.91 & 10.89 & 2.59 & x3.6 \\
\bottomrule
\end{tabular}}
\end{table}

{\noindent \bf{Multi-stream processing.}} To better understand the benefit of using multiple streams over a single stream we optimized three different audio discrete representation models using Down Sampling Factors (DSF) of \{x32, x64, x128\} using \{1, 2, 4\} different codebooks respectively. For a fair comparison we kept all models at 2048 codes overall (e.g., 2 codebooks of size 1024 for the x64 model and 4 codebooks of size 512 for the x128 model). 

To assess the quality of the audio representation we report three encoding metrics namely \ac{SI-SNR}, ViSQOL~\citep{chinen2020visqol}, and bitrate. These metrics characterize the audio representation only while ignoring the \ac{ALM}. Note, both \ac{SI-SNR} and ViSQOL are reference based metrics hence are computed between the audio reconstructed from the learned representation and the original audio sequence. Results are reported on Table~\ref{tab:mst}. While a single stream encoder achieves better \ac{SI-SNR} results, all representations are comparable in terms of estimated perceptual quality as measured by ViSQOL. While the total amount of codes is the same across all settings, the effective bitrate of the multi-stream models was degraded, leading to lower \ac{SI-SNR} values. 

Next, we report the FAD, KL and inference speed-up for the \ac{ALM} on top of the learned representations. Results suggest that increasing the number of streams degrades the performance in both base and large models when compared to a single-stream. 
\audiogen-base improves the KL score over DiffSound (2.57) while yielding higher FAD. \audiogen-large with two streams improves over DiffSound in both FAD and KL (7.39 and 2.57 respectively). While showing inferior FAD and KL results when compared to the single-stream model, the multi-stream setting offers a trade-off between inference quality and speed.

