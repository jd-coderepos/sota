

\section{Results}
\label{sec:results}

In this section, we first describe the datasets we tested our approach on and the corresponding 
evaluation protocols. We then compare our approach against the state-of-the-art methods and provide 
a detailed analysis of our general framework.

\subsection{Datasets}
\label{ssec:datasets}

We evaluate our  approach on the Human3.6m~\cite{Ionescu14a}, HumanEva-I~\cite{Sigal06}, KTH Multiview
Football II~\cite{Burenius13} and Leeds Sports Pose (LSP)~\cite{Johnson10b} datasets described below.

\noindent{\bf Human3.6m} is a large and diverse motion capture dataset including  3.6 
million  images  with  their corresponding 2D  and 3D  poses. The  poses are viewed  from 4  different 
camera angles. The  subjects carry out  complex motions corresponding to  daily human activities.
 We  use the standard $17$ joint skeleton from Human3.6m as our pose representation.

\noindent{\bf HumanEva-I} comprises synchronized images and motion capture data and is a  standard 
benchmark for 3D human pose  estimation. The output pose is a vector of $15$ 3D joint coordinates.

\noindent{\bf KTH Multiview Football II} provides a benchmark to evaluate the performance of pose 
estimation algorithms in unconstrained outdoor settings. The camera follows a soccer player moving 
around the pitch. The videos are captured from 3 different camera viewpoints. The output pose is a vector of 14 3D joint coordinates.

\noindent{\bf LSP} is a standard benchmark for 2D human pose estimation and does
not contain any ground-truth 3D pose data. The images are captured in unconstrained
outdoor settings. 2D pose is represented in terms of a vector of $14$ joint coordinates.
We report qualitative 3D pose estimation results on this dataset.


\begin{table*}[tbph]
	\begin{center}
		\tabcolsep=0.17cm
		\scalebox{0.73}{
			\begin{tabular}[b]{llccccccccc}
				\toprule
				Input &Method							 &  Directions    &  Discussion     &  Eating        &  Greeting      &  Phone Talk   		  &  Posing          &  Buying        &  Sitting               & Sitting Down    \\
				\toprule
				\multirow{6}{*}{Single-Image}
				&Ionescu et al.~\cite{Ionescu14a}		 &  132.71        & 183.55          & 132.37         & 164.39 	      & 162.12	      		  & 150.61	         & 171.31         & 151.57                 & 243.03\\
				&Li \& Chan~\cite{Li14a}			     &  -	 	      & 148.79          & 104.01         & 127.17		  & -			  		  & -		         & -              & -                      & -	  	\\
				&Li et al.~\cite{Li15a}					 & -              & 134.13          & 97.37          & 122.33         & -             		  & -                & -              & -			           & -       \\
				&Li et al.~\cite{Li16b}					 & -              & 133.51          & 97.60          & 120.41         & -             		  & -                & -              & -			           & -        \\
				&Zhou et al.~\cite{Zhou16a}        		 &-               & -               & -              &-               &-              		  & -                &-               &- 			           & -         \\
				&Rogez \& Schmid~\cite{Rogez16}     	 & -              & -               & -              & -              & -             		  & -                & -              & -			           & - 		    \\
				&Tekin et al.~\cite{Tekin16b}			 & -              & 129.06          & 91.43          & 121.68         & -             		  & -             	 & -           	  & - 					   & -           \\
				&Park et al.~\cite{Park16}			     & 100.34         & 116.19          & 89.96          & 116.49         & 115.34       		  & 117.57       	 & 106.94         & 137.21    			   & 190.82      \\
				&Zhou et al.~\cite{Zhou16b}			     & 91.83          & 102.41          & 96.95          & 98.75          & 113.35       		  & 90.04       	 & 93.84          & 132.16    			   & 158.97      \\
				\midrule
				\multirow{3}{*}{Video}
				&Tekin et al.~\cite{Tekin16a}            & 102.41 	      & 147.72 	        & 88.83  	     & 125.28  	      & 118.02   	          &112.3   	  		 & 129.17         & 138.89    			   &224.90		\\
				&Zhou et al.~\cite{Zhou16a}			     & 87.36          & 109.31          & 87.05          & 103.16         & 116.18		          & 106.88       	 & 99.78          & 124.52    			   & 199.23      \\
				&Du et al.~\cite{Du16}                   & 85.07      	  & 112.68          & 104.90         & 122.05         & 139.08                & 105.93       	 & 166.16         &	117.49     			   & 226.94       \\
				\midrule
Single-Image & Ours						 & \textbf{54.23} & \textbf{61.41}  & \textbf{60.17} & \textbf{61.23} &\textbf{79.41}         & \textbf{63.14}   & \textbf{81.63} & \textbf{70.14}         & \textbf{107.31}             \\
				\bottomrule
				\toprule
			\end{tabular}
		}
	\end{center}
\begin{center}
		\tabcolsep=0.1cm
		\scalebox{0.75}{
			\begin{tabular}[b]{llcccccccc}
Input &Method:						&  Smoking  	&  Taking Photo  & Waiting     	   & Walking     		   &  Walking Dog    &  Walking Pair     	 & Avg. (All) & Avg. (6 Actions)        \\
				\toprule
				\multirow{6}{*}{Single-Image}
				&Ionescu et al.~\cite{Ionescu14a}  	& 162.14    	& 205.94     	 &  170.69	 	   & 96.60 				   & 177.13	         &  127.88       		 & 162.14 		   & 159.99					\\
				&Li \& Chan~\cite{Li14a}			& - 		 	& 189.08         & -               & 77.60				   & 146.59		     & -             		 & -      		   & 132.20 				 	\\
				&Li et al.~\cite{Li15a}             & -         	& 166.15         & -               & 68.51       		   & 132.51          & -                 	 & -			   & 120.17        					\\
				&Li et al.~\cite{Li16b}				& -           	& 163.33         & -               & 73.66       		   & 135.15          & -                  	 & -      	       & 121.55					\\
				&Zhou et al.~\cite{Zhou16a}         &-           	&-               &-                &-            		   & -             	 & -                     & 120.99 	       & - 						\\
				&Rogez \& Schmid~\cite{Rogez16}	    & -           	& -              & -               & -           		   & -               & -                     & 121.20    	   & -   					\\
				&Tekin et al.~\cite{Tekin16b}       & -      		& 162.17         & -               & 65.75       		   & 130.53          & -                 	 & -			   & 116.77             				\\
				&Park et al.~\cite{Park16}          & 105.78    	& 149.55         & 125.12          & 62.64       		   & 131.90          & 96.18                 & 117.34   	   & 111.12      			\\
				&Zhou et al.~\cite{Zhou16b}         & 106.91    	& 125.22         & 94.41           & 79.02       		   & 126.04          & 98.96                 & 107.26   	   & 104.73      			\\
				\midrule
				\multirow{3}{*}{Video}
				&Tekin et al.~\cite{Tekin16a}		& 118.42    	& 182.73         & 138.75          & 55.07 			 	   & 126.29          & 65.76			  	 & 124.97		   & 120.99     				\\
				&Zhou et al.~\cite{Zhou16a}         & 107.42    	& 143.32         & 118.09          & 79.39       		   & 114.23          & 97.70                 & 113.01   	   & 106.07      			\\
				&Du et al.~\cite{Du16}              & 120.02     	& 135.91         & 117.65          & 99.26       		   & 137.36       	 & 106.54            	 & 126.47    	   & 118.69 			\\
				\midrule
Single-Image &Ours 					& \textbf{69.29}& \textbf{78.31} & \textbf{70.27}  & \textbf{51.79}		   & \textbf{74.28}  & \textbf{63.24}		 & \textbf{69.73}  & \textbf{64.53}		\\
				\bottomrule
			\end{tabular}
		}
	\end{center}
	\vspace{-0.3cm}
	\caption{{\bf   Comparison  of   our   approach  with   state-of-the-art
            algorithms on \emph{Human3.6m}.} We  report 3D joint position errors
          in  mm,  computed  as  the  average  Euclidean  distance  between  the
          ground-truth  and  predicted   joint  positions. `-' indicates  that the
          results  were not  reported for  the  respective action  class in  the
          original  paper.  Note  that our  method consistently  outperforms the
          baselines.  }  \label{tab:overall}
	\vspace{-0.3cm}
\end{table*}

\subsection{Evaluation Protocol}
\label{ssec:eval}

On Human3.6m, we used the same data partition as in earlier work~\cite{Li14a,Li15a,Li16b,Tekin16a,Zhou16a}
for a fair comparison. The data from 5~subjects (S1, S5, S6, S7, S8) was used for training and
the data from 2 different subjects (S9, S11) was used for testing. We evaluate the accuracy
of 3D human pose estimation in terms of average Euclidean distance between the predicted
and ground-truth 3D joint positions, as in~\cite{Li14a,Li15a,Li16b,Tekin16a,Zhou16a}. Training and testing
were carried out monocularly in all camera views.

In~\cite{Bogo16}, \cite{Pavlakos16}\footnote{While~\cite{Pavlakos16} also reports results without Procrustes analysis, the authors confirmed to us by email that their evaluation assumes the ground-truth depth of the root joint to be known to go from their volumetric representation to 3D pose in metric space. Since this also sets the scale of the skeleton, we believe that a comparison using the full Procrustes transformation for both their approach and ours is the right one to perform here.} ,  and~\cite{Sanzari16}\footnote{This it is not explicitly stated in~\cite{Sanzari16}, but the authors confirmed this to us by email.}  the estimated skeleton was  first aligned to the  ground-truth one  by Procrustes transformation before measuring the joint distances. This is therefore what we also do when comparing against~\cite{Bogo16,Pavlakos16,Sanzari16}.

\comment{Testing  was
carried  out only  in the  frontal camera  ("cam3") from  trial $1$  using the
sequences from S9  and S11.} 




On HumanEva-I, following the standard evaluation protocol~\cite{Bogo16,Simo-Serra13,Tekin16a,Yasin16,Zhou16a}, we trained our model on  the training 
sequences of  subjects S1, S2 and  S3  and  evaluated on  the  validation sequences of all subjects.  
We pretrained  our network on Human3.6m  and  used  only  the  first  camera  view  for  further  
training  and validation.


On the KTH  Multiview Football II  dataset, we evaluate  our method on  the sequence
containing Player~2,  as in~\cite{Belagiannis14a,Burenius13,Pavlakos16,Tekin16a}. Following~\cite{Belagiannis14a,Burenius13,Pavlakos16,Tekin16a}, 
the first half of the sequence from camera~1 is used for training and the second half for
testing. To compare our results to those of~\cite{Belagiannis14a,Burenius13,Pavlakos16,Tekin16a},
we report accuracy using the percentage of correctly estimated parts (PCP) score. Since 
the training set   is    quite   small, we propose to   pretrain our network on the 
recent synthetic dataset~\cite{Chen16}, which contains images of sports players
with their corresponding 3D poses. We then fine-tuned it using the training data 
from KTH  Multiview Football II. We report results with and without this pretraining.


\subsection{Comparison to the State-of-the-Art}

We first compare our approach with state-of-the-art baselines on the \emph{Human3.6m~\cite{Ionescu14a}, 
	HumanEva~\cite{Sigal06} and KTH Multiview Football~\cite{Burenius13}} datasets. 
\comment{Here, \emph{Ours} refer to our \ms{trainable} fusion strategy, which, as
shown in Section~\ref{ssec:analysis}, yields the best results among \comment{our 
four} different \bt{fusion} strategies. \MS{Are we still going to show this?}
\BT{There is going to be a comparison against Early and Late-Late fusion.}}

\vspace{-4mm}

\paragraph{Human3.6m.} In Table~\ref{tab:overall},
we  compare the  results of our trainable fusion approach with  those of  the following  state-of-the-art
single image-based  methods: KDE regression  from HOG features  to 3D
poses~\cite{Ionescu14a}, jointly training a 2D  body part detector and a
3D pose  regressor~\cite{Li14a,Park16}, the maximum-margin  structured learning
framework of~\cite{Li15a,Li16b}, the deep structured prediction approach
of~\cite{Tekin16b}, pose regression with kinematic constraints~\cite{Zhou16b},
and  3D  pose  estimation  with  mocap  guided  data
augmentation~\cite{Rogez16}.   For  completeness,  we also  compare  our
approach  to  the  following  methods   that  rely  on  either  multiple
consecutive images or impose temporal consistency: regression from short
image sequences  to 3D poses~\cite{Tekin16a},  fitting a sparse  3D pose
model to  2D  confidence map predictions across  frames~\cite{Zhou16a}, and
fitting a  3D pose  sequence to  the 2D joints  predicted by  images and
height-maps that encode the height of each pixel in the image with respect 
to a reference plane~\cite{Du16}. 
	
As can be seen from the results in Table~\ref{tab:overall}, our approach
outperforms all the methods on all the action categories
by a large margin. In  particular,  we  outperform   the  image-based  
regression  methods of~\cite{Ionescu14a,Li14a,Li15a,Li16b,Tekin16b,Park16,Zhou16b},   as    
well   as   the model-fitting strategy of~\cite{Li15a,Li16b}.  This, we 
believe, clearly evidences the  benefits of fusing 2D joint location confidence maps with 3D 
image  cues, as done by our approach. Furthermore,  we also achieve lower error than
the  method  of~\cite{Rogez16},  despite  the fact  that  it  relies  on
additional training data. Even though our algorithm uses
only individual  images, it  also outperforms the  methods that  rely on
sequences~\cite{Du16,Tekin16a,Zhou16a}.  

\pf{
Since results are reported in~\cite{Bogo16,Sanzari16,Pavlakos16} for the average accuracy over all actions using 
	the Procrustes transformation, as explained in Section~\ref{ssec:eval}, 
	we do the same when comparing against these methods. Table~\ref{tab:procrustes} shows that we also outperform these baselines.
}

\comment{
As explained in Section~\ref{ssec:eval},~\cite{Bogo16} and~\cite{Sanzari16} report pose estimation results using 
Procrustes transformation. We carried out the same experiment and report results in Table~\ref{tab:procrustes}.
\bt{The parallel work of~\cite{Pavlakos16} adapts two different evaluation protocols.
The first protocol aligns the prediction to the ground-truth by Procrustes transformation. In the second one, 
as we confirmed through private communication, it is assumed that the depth of root joint position is known and 
pose of the person in the metric space is recovered from the volumetric representation using ground-truth z-axis 
of the root joint. By contrast, we do not assume that the ground-truth depth of the person is known. For a fair 
comparison, we also examine the performance of our approach against~\cite{Pavlakos16} using Procrustes transformation.}
}

\vspace{-0.4cm}
\paragraph{HumanEva.}

In Table~\ref{tab:humaneva}, we present the performance of our fusion approach on the HumanEva-I 
dataset~\cite{Sigal06}. We adopted the evaluation protocol described in~\cite{Bogo16,Simo-Serra13,Yasin16,Zhou16a} 
for a fair comparison. As in~\cite{Bogo16,Simo-Serra13,Yasin16,Zhou16a}, we measure 3D pose error as the average joint-to-joint 
distance after alignment by a rigid transformation.
Our approach also significantly outperforms the state-of-the-art on this dataset.

\begin{table}[t]
\centering
\tabcolsep=0.1cm
\scalebox{0.82}{
	\begin{tabular}[b]{lc}
		\toprule
		Method:												& 3D Pose Error \\
		\midrule
		Sanzari et al.~\cite{Sanzari16}		  				& 93.15		             \\
		Bogo et al.~\cite{Bogo16}			  				& 82.3		 		        \\
		Pavlakos et al.~\cite{Pavlakos16} 					& 53.2						\\
		\midrule
		Ours  						  						& \textbf{50.12}	 \\
		\bottomrule
	\end{tabular}
}
\caption{Comparison of our approach to the state-of-the-art methods that use
	Procrustes transformation on Human3.6m. We report 3D joint position errors (in mm).}
\label{tab:procrustes}
\end{table}

\begin{table}[tbph]
	\centering
	\tabcolsep=0.3cm
	\scalebox{0.82}{
		\begin{tabular}[b]{lcccc}
			\toprule
			Method 									& S1		 & S2         & S3     		& Average   \\
			\midrule
			Simo-Serra et al.~\cite{Simo-Serra13}  	& 65.1		 & 48.6		  &73.5	 	  	& 62.4			\\
			Bogo et al.~\cite{Bogo16}				& 73.3		 & 59.0		  &99.4			& 77.2				\\
			Zhou et al.~\cite{Zhou16a} 				& 34.2		 & 30.9		  &49.1			& 38.07				\\
			Yasin et al.~\cite{Yasin16}				& 35.8       & 32.4       &41.6         & 36.6              \\
			Tekin et al.~\cite{Tekin16a}			& 37.5       & 25.1       & 49.2        & 37.3           \\
			Ours					  			    & {\bf 27.24}& {\bf 14.26}&{\bf 31.74}  & {\bf 24.41}      		\\
			\bottomrule
		\end{tabular}
	}
	\caption{Quantitative results of our fusion approach on the Walking sequences of the HumanEva-I dataset~\cite{Sigal06}. S1, S2 and S3 correspond
		to Subject 1, 2, and 3, respectively. The accuracy is reported in terms of average Euclidean distance (in mm) between the
		predicted and ground-truth 3D joint positions.}
	\vspace{-4mm}
	\label{tab:humaneva}
\end{table}



\paragraph{KTH Multiview Football.}
In  Table~\ref{tab:kth}, we  compare our  approach  to~\cite{Belagiannis14a,Burenius13,Pavlakos16,Tekin16a} on the KTH  Multiview Football  II
dataset. Note that~\cite{Belagiannis14a} 
and~\cite{Burenius13}  rely on multiple views, and~\cite{Tekin16a} makes use of video data. 
As  discussed  in Section~\ref{ssec:eval}, we report the results of two instances of our 
model: one trained on the standard KTH training data, and one pretrained on the synthetic 3D 
human pose  dataset of~\cite{Chen16} and  fine-tuned on  the KTH  dataset. Note that, while 
working with a single input image, both instances outperform all the baselines. Note also that 
pretraining on synthetic data yields the highest accuracy. We believe that this further demonstrates    
the generalization ability of our method. 

In Fig.~\ref{fig:results}, we  provide representative poses predicted by our approach on
the Human3.6m, HumanEva and KTH Multiview Football datasets.


\begin{table}[t]
	\centering
	\tabcolsep=0.1cm
	\scalebox{0.62}{
	\begin{tabular}[b]{lccccccc}
		\toprule
		
		Method:         &\cite{Burenius13}    &\cite{Burenius13} &\cite{Belagiannis14a}  & \cite{Tekin16a} & \cite{Pavlakos16}& Ours-NoPretraining     & Ours-Pretraining   \\
		Input:			& Image				  &Image 		     & Image          		 & Video           & Image			  & Image    			   & Image    \\
		Num. of cameras:&1   				  &2 				 &2  					 &1  	    	   & 1				  & 1     			       & 1 \\
		\midrule
		Pelvis  	    &97    				  &97				 &-						 &99       		   & -			      &66					   & {\bf100}   \\
		Torso		    &87					  &90 				 &-						 &{\bf 100} 	   & -			      &{\bf100}				   & {\bf100}  \\
		Upper arms      &14 				  &53  				 &64     				 &74       		   & 94				  &74					   & {\bf100}  \\
		Lower arms      &06					  &28				 &50					 &49       		   & 80			      &{\bf100}			       & 88  		\\
		Upper legs      &63					  &88				 &75 					 &98  	  		   & 96				  &{\bf100}	    		   & {\bf100}  \\
		Lower legs      &41				   	  &82				 &66 					 &77	      	   & 84				  &77					   & {\bf88}  	\\
		All parts       &43					  &69				 &-						 &79  	  	       & -				  &83.2					   & {\bf95.2}   \\
		\bottomrule
	\end{tabular}}
	\caption{On KTH Multiview Football  II, we compare our method
        that uses a single image to those of~\cite{Burenius13,Pavlakos16,Tekin16a} 
        that use either one or two images, the one of~\cite{Belagiannis14a}  
        that  uses  two, and  the  one of~\cite{Tekin16a} that operates 
        on a sequence. As in~\cite{Belagiannis14a,Burenius13,Pavlakos16,Tekin16a}, we measure performance as the percentage of correctly estimated parts (PCP)
		score.
		A higher PCP score corresponds to better 3D pose estimation accuracy.}
	\label{tab:kth}
	\vspace{-2mm}
\end{table}


\subsection{Detailed Analysis}
\label{ssec:analysis}
\begin{table}[t]
	\centering
	\tabcolsep=0.1cm
	\scalebox{0.88}{
		\begin{tabular}[b]{lc}
			\toprule
			Method:  						 					& 3D Pose Error            \\
			\midrule
			Image-Only						  					& 124.13  	 	  			\\
			CM-Only 											& 79.28                          \\
			Early Fusion									    & 76.41   	  	 	            \\
			Late Fusion						  					& 74.12 		                    \\
			Trainable Fusion  						  			& {\bf 69.73}		                    \\
			\bottomrule
		\end{tabular}
	}\vspace{-1mm}
	\caption{Comparison of different fusion strategies and single-stream baselines on Human3.6m. We report the 3D joint position errors (in  mm).  The fusion
		networks perform better than those  that use only the image or
		only the confidence  map as input. Our trainable fusion achieves the
		best accuracy overall.}
	\vspace{-5mm}
	\label{tab:baseline_results}
\end{table}

\begin{figure}[t]
	\centering
	\scalebox{0.84}{
		\begin{tabular}{cc}
			\includegraphics[width=0.45\columnwidth]{alpbet.pdf}  
			& \includegraphics[width=0.45\columnwidth]{gate.pdf}  \\
			\includegraphics[width=0.45\columnwidth]{alpbet2.pdf}  
			& \includegraphics[width=0.45\columnwidth]{gate2.pdf}  \\
			\includegraphics[width=0.45\columnwidth]{alpbet9.pdf}  
			& \includegraphics[width=0.45\columnwidth]{gate9.pdf}  \\
			\multicolumn{1}{c}{\footnotesize (a)}&\multicolumn{1}{c}{\footnotesize (b)}
		\end{tabular}
	}\vspace{-4mm}
	\caption{Evolution of {\bf(a)}~$\alpha$ and $\beta$, and {\bf(b)} the fusion weights in Human3.6m as training progresses. 
		Top row: Directions; Middle row: Discussion; Bottom row: Sitting Down.}
	\label{fig:weights}
	\vspace{-6mm}
\end{figure}

\begin{figure*}[t]
	\centering
	\scalebox{0.8}{
		\begin{tabular}{cc}
			\includegraphics[width=0.45\linewidth]{buying_c_1} \hspace{6mm}
			&\includegraphics[width=0.45\linewidth,height=1.09in]{sittingdown_c_1} \\
			\includegraphics[width=0.45\linewidth,height=1in]{h36m_6.pdf} \hspace{6mm}
			& \includegraphics[width=0.45\linewidth,height=1in]{h36m_3.pdf} \\
			\includegraphics[width=0.45\linewidth]{kth_9.pdf} \hspace{6mm}
			& \includegraphics[width=0.45\linewidth]{kth_11.pdf} \\
			\includegraphics[width=0.45\linewidth]{he_6.pdf} \hspace{6mm}
			& \includegraphics[width=0.45\linewidth]{he_2.pdf} \\
			\footnotesize (a) Image \;\;\; (b) Confidence Map \;\;\; (c) Prediction \;\;\; (d) Ground-truth & \footnotesize \;\;\;\;\;\;\;\;\;\;\; (e) Image \;\;\; (f) Confidence Map \;\;\; (g) Prediction \;\;\; (h) Ground-truth \;\;\;
		\end{tabular}
	} \vspace{0mm}
	\caption{{\bf Pose  estimation  results  on  Human3.6m, HumanEva and KTH Multiview Football.} {\bf  (a, e)}  Input
		images. {\bf (b, f)} 2D joint  location confidence maps.
		{\bf (c, g)} Recovered pose. {\bf (d, h)} Ground truth.
		Note that our
		method can  recover  the  3D  pose  in these  challenging  scenarios,  which
		involve significant amounts of self occlusion  and orientation ambiguity. Best viewed in color.}
	\label{fig:results}
\end{figure*}





We now analyze two different aspects of our approach. First, we compare our trainable
fusion approach to early fusion, depicted in Fig.~\ref{fig:hardcoded}(a), and late fusion, depicted in
Fig.~\ref{fig:hardcoded}(c). Then, we analyze the benefits 
of leveraging both 2D joint locations with their corresponding uncertainty and additional image cues. 
To this end, we make use of two additional baselines. The first one consists of 
a single stream CNN regressor operating on the image only. We refer to this baseline as \emph{Image-Only}. 
The second is a CNN trained to predict 3D pose from only the 2D 
confidence map (CM) stream. We refer to this baseline as \emph{CM-Only}.

\comment{
\begin{figure*}[t]
	\centering
	\scalebox{0.79}{
		\begin{tabular}{cc}
			\includegraphics[width=0.45\linewidth]{walking_c_1} \hspace{6mm}
			&\includegraphics[width=0.45\linewidth,height=1.09in]{sittingdown_c_1}  \\
			\includegraphics[width=0.45\linewidth]{buying_c_1} \hspace{6mm}
			& \includegraphics[width=0.45\linewidth]{directions_c_2} \\
			\includegraphics[width=0.45\linewidth]{greeting_c_3} \hspace{6mm}
			&\includegraphics[width=0.45\linewidth]{eating_c_1}  \\
			\includegraphics[width=0.45\linewidth,height=1in]{posing_c_2} \hspace{6mm}
			& \includegraphics[width=0.45\linewidth]{walkingpair_c_1} \\
\footnotesize (a) Image \;\;\; (b) Confidence Map \;\;\; (c) Prediction \;\;\; (d) Ground-truth & \footnotesize \;\;\;\;\;\;\;\;\;\;\; (e) Image \;\;\; (f) Confidence Map \;\;\; (g) Prediction \;\;\; (h) Ground-truth \;\;\;
		\end{tabular}
	} \vspace{-3mm}
	\caption{Pose  estimation  results  on  Human3.6m. {\bf  (a,e)}  Input
		images. {\bf (b,f)} 2D joint  location confidence maps.
		{\bf (c,g)} Recovered pose. {\bf (d,h)} Ground truth.
		Note that our
		method can  recover  the  3D  pose  in these  challenging  scenarios,  which
		involve significant amounts of self occlusion  and orientation ambiguity. Best viewed in color.}
	\vspace{-8mm}
	\label{fig:results}
\end{figure*}

\begin{figure}[t]
	\centering
	\scalebox{0.81}{
		\begin{tabular}{cc}
			\includegraphics[width=\linewidth]{kth_n_1.pdf}  \\
			\includegraphics[width=\linewidth]{kth_n_4.pdf}  \\
\end{tabular}
	}
	\caption{Pose  estimation results  on  KTH Multiview Football II. In the first two columns, we show
		the input image and the predicted confidence maps. First skeleton depicts our prediction
		and the second one depicts the ground-truth 3D pose. Best viewed in color.}
	\label{fig:kth_examples}
	\vspace{-3mm}
\end{figure}	
}



In Table~\ref{tab:baseline_results}, we report the average pose estimation errors on Human3.6m
for all these methods. Our trainable fusion strategy yields the best results. Note 
also that, in general, all fusion strategies outperform the state-of-the-art methods in Table~\ref{tab:overall}. 
Importantly, the \emph{Image-Only} and \emph{CM-Only} baselines perform worse than our approach, 
and all fusion-based methods. This demonstrates the importance of fusing 2D joint location confidence
maps along with 3D cues in the image for monocular pose estimation.

\ms{In Fig.~\ref{fig:weights}, we depict the evolution throughout the training iterations of {\bf (a)}~the
parameters $\alpha$ and $\beta$ that define the weight vector in our trainable fusion framework
as given by Eq.~\ref{eq:sig}, 
and {\bf (b)} the weight vector itself.
An increasing value of $\alpha$, expected due 
to our regularizer, indicates that fusion becomes sharper throughout the training. An increasing $\beta$, which is 
the typical behavior, corresponds to fusion occurring in the later stages of the network.
We conjecture 
that this is due to the fact that features learned by the image and confidence map streams at later
layers become less correlated, and thus yield more discriminative power. \comment{To analyze this, we have measured 
the correlation between the features of the confidence map stream and those of the image stream after the 
last convolutional layer of the network for our trainable fusion strategy and compared it against the 
correlation of the feature maps of the early fusion scheme in terms of Pearson correlation coefficient values. 
\comment{\MS{What is this?}}  As can be seen in Fig.~\ref{fig:pearson}, where we visualize these values, the 
early fusion features are indeed more correlated than those obtained by our trainable fusion strategy, which 
typically tends to select a later layer to fuse the two streams.}}

\bt{
To analyze this further, we show in Fig.~\ref{fig:pearson}
the squared Pearson correlation coefficients between all pairs of features of the confidence map stream and
of the image stream at the 
last convolutional layer of our trainable fusion network. As can be seen in the figure, 
the image and confidence map streams produce decorrelated features that
are complementary to each other allowing to effectively account for different input
modalities.
}

\comment{
\MS{To be honest, I cannot see much from the figure...}}

\comment{
The sharp transitions of the weight vector decides where the fusion takes place. 
We have observed that fusion generally occurs later than earlier at the network. 
This might be due to the fact that high level features learned from the image and 
confidence map streams are more correlated at the early fusion, thus leading to more redundancy and less 
discriminative power. In order to analyze this further, we have measured 
the correlation between the feature maps of the confidence map stream and those of the image stream 
after the last convolutional layer of the network for our trainable fusion strategy
and compared it against the correlation of the feature maps of the early 
fusion in terms of $R^2$ values in Fig.~\ref{fig:r2}. While the feature maps of the early fusion are 
more correlated, the image and confidence map streams of trainable fusion scheme 
learns less correlated and complementary features that, when combined, improve
upon the general early fusion.
}

\subsection{Qualitative Results}

In Fig.~\ref{fig:leeds_examples}, we present qualitative pose estimation results on the Leeds Sports Pose
dataset. We trained our network on the synthetic dataset of~\cite{Chen16} and tested on images
acquired outdoors in unconstrained settings. The accurate 3D predictions of the challenging poses 
demonstrate the generalization ability and robustness of our method. 

\begin{figure}[t]
	\centering
	\scalebox{0.78}{
		\begin{tabular}{c}
			\includegraphics[width=\linewidth]{leeds.pdf}  \\
		\end{tabular}
	}
	\caption{{\bf Pose  estimation results  on the Leeds Sports Pose dataset.} We show
		the input image and the predicted 3D pose for four images. Best viewed in color.}
	\label{fig:leeds_examples}
	\vspace{-3mm}
\end{figure}

\begin{figure}[t]
	\centering
	\scalebox{0.77}{
	\begin{tabular}{c}
		\includegraphics[width=\columnwidth]{abs_pearson}  
	\end{tabular}}
	\caption{Squared Pearson correlation coefficients ($R^2$) between each pair of the features
		learned at the last convolutional layer of our trainable fusion network computed
		from $128$~randomly selected images in Human3.6m. As can be seen in the lower left and
		upper right submatrices, the feature maps of the image and the 
		confidence map streams are decorrelated.}
	\label{fig:pearson}
	\vspace{-6mm}
\end{figure}



\comment{
	\begin{figure*}[t]
\centering
		\scalebox{0.81}{
			\begin{tabular}{cc}
				\includegraphics[width=0.5\linewidth, height=1.3in]{im_to_3D_new.pdf}
				&\includegraphics[width=0.5\linewidth, height=1.3in]{hm_to_3D_new.pdf}   \\
\multicolumn{1}{c}{\footnotesize (a) Image to 3D pose regression}&\multicolumn{1}{c}{\footnotesize (b) Probability map to 3D pose regression} \\
			\end{tabular}
		}
		\vspace{0mm}
		\caption{Baseline architectures  we consider. {\bf (a)}  Regression from
			image to 3D human pose by a CNN, {\bf (b)} Regression from probability
			maps to 3D  human pose by a  CNN. \pf{Its input is  the joint location
				probability maps for $17$~joints in the human body.}}
		\label{fig:baseline}
	\end{figure*}
}
\comment{
	\begin{table}[t]
		\centering
		\tabcolsep=0.1cm
		\scalebox{0.88}{
			\begin{tabular}[b]{lc}
				\toprule
				Method:  						 					& 3D Pose Error            \\
				\midrule
				Image-Only						  					& 128.47  	 	  \\
				PM-Only 											& 120.07                 \\
				\hline
				EM-Optimization									    & 116.95	    	  	          \\
				\hline
				Early Fusion									    & 114.62   	  	  \\
				Average Fusion						  				& 112.07			      \\
				Linear Fusion						  				& 109.02 		      \\
				Late Fusion  						  				& {\bf 100.08}		          \\
				\bottomrule
			\end{tabular}
		}
		\caption{\pf{3D joint position errors (in  mm) for the baselines and
				fusion strategies introduced  in~\ref{sec:fusion}.  The fusion
				networks perform better than those  that use only the image or
				only the probability  map as input.  Late  fusion achieves the
				best accuracy overall.}}
		\label{tab:baseline_results}
	\end{table}
}
\comment{	
	We now analyze two different aspects of our approach. First, we compare the different 
	fusion strategies introduced in Section~\ref{sec:approach}. In addition to these strategies, 
	we report the results of the following model-fitting baseline that enforces consistency of 
	the projections of 3D poses and 2D joint uncertainties via an Expectation-Maximization (EM) 
	framework similar to that of~\cite{Zhou16a}: It consists of two different Deep Networks, one to predict 2D
	probability maps and one to predict 3D pose. The former is the same as our U-Net approach 
	discussed in Section~\ref{sec:heatmap}. The second one is a CNN with the same architecture 
	as the image stream in Fig.~\ref{fig:architecture}(b), from which we estimate a density in 
	3D using Gaussian distributions around the predicted joint locations. Given these two predictions, 
	we estimate the 3D pose by using an EM algorithm that couples 2D uncertainties and projection 
	of 3D joint distributions. We will refer to this baseline as \emph{EM-Optimization}.
	
	The second aspect of our approach we analyze is the benefits of leveraging both 2D uncertainty 
	and 3D cues. To this end, we make use of two additional baselines. The first one consists of 
	a direct CNN regressor operating on the image only, as depicted in Fig.~\ref{fig:baseline}(a). 
	We refer to this baseline as \emph{Image-Only}. By contrast, the second baseline corresponds 
	to a CNN trained to predict 3D pose from only the 2D probability maps (PM) obtained with our 
	U-Net method, as shown in Fig.~\ref{fig:baseline}(b). We refer to this baseline as \emph{PM-Only}.
}
\comment{
	During our experiments, we have observed that our U-Net-based 2D prediction network, depicted 
	in Fig.~\ref{fig:unet}, yields very accurate probability maps. Specifically, it achieves a 
	localization error of $7.14$ pixel on average over all actions, which outperforms the $10.85$ 
	error reported in~\cite{Zhou16a}. To verify that our better 3D results are not only due to 
	these better 2D results, we evaluated the method of~\cite{Zhou16a} using our probability maps 
	as input with their publicly available code. In Table~\ref{tab:heatmap_pred}, we compare these 
	results with ours. Note that we still outperform this approach even when it relies on our 2D 
	probability maps. This demonstrates that our better 3D predictions are truly the results
	of our fusion strategy.
}
\comment{	
\begin{table}[t]
	\centering
	\tabcolsep=0.1cm
	\scalebox{0.87}{
		\begin{tabular}[b]{llc}
			\toprule
			2D Prediction 				& 3D Prediction			& 3D Error        \\
			\midrule
			Zhou et al.~\cite{Zhou16a}  & Zhou et al.~\cite{Zhou16a}		& 133.91  	 	  		\\
			Ours 					    & Zhou et al.~\cite{Zhou16a}		& 129.15                \\
			Ours					    & Ours								& {\bf 116.96}	    	 \\
			\bottomrule
		\end{tabular}
	}\vspace{-1mm}
	\caption{Average Euclidean distance in millimeters with different 2D and 3D prediction methods.
		We evaluate the influence of 2D probability map prediction in the 3D pose accuracy by comparing the
		different stages of the method of~\cite{Zhou16a} to those of our method. We evaluated on the
		first 1966 frames of the sequence corresponding to Subject 9 performing \emph{Posing} action
		on camera 1 in trial 1 as was done in the online test code of~\cite{Zhou16a}.}
	\label{tab:heatmap_pred}
\end{table}
}

	
