\onecolumn
\begin{center}
  {\large {\bf\textsc{supplementary material : heavy-tailed representations, text polarity classification \& data augmentation}}}
\end{center}
\iffalse
\begin{table}[ht]
\scriptsize
    \centering
\begin{tabular}{|c|cc|cc|}
       \hline \multirow{2}{*}{} & \multicolumn{2}{c|}{\textit{Amazon}} & \multicolumn{2}{c|}{\textit{Yelp}}\\
        & dataset 1 & dataset 2        & dataset 1 & dataset 2 \\\hline
          $\vert \mathcal{D}_n^{Train} \vert$ &750  & 210k & 750 & 1400k\\
          $\vert \mathcal{D}_n^{Test} \vert$ & 250 & 22k & 250 & 22k \\
    $\vert\mathcal{T}_{\text{test}}\vert$ & 61 & 6.9k  & 97 & 6.1k\\\hline
\end{tabular}
    \caption{Sizes of the train  $\mathcal{D}_n^{Train}$, test $\mathcal{D}_n^{Test}$ and the extreme test set $\mathcal{T}_{\text{test}}$. The extreme train tests constitute $1/4$ of the  full train tests. These datasets are used to validate  {\fontfamily{qcr}\small\textbf{LHTR}}.}
\label{tab:my_label}
\end{table}
\fi

\section{Models}\label{sec:Model_sup} \subsection{Background on Adversarial Learning}\label{sec:adversarial_learning}
Adversarial networks, introduced in \cite{goodfellow2014generative}, form a system where two neural networks are competing. A first model $G$, called the generator, generates  samples as close as possible to the input dataset. A second model $D$, called the discriminator, aims at distinguishing samples produced by the generator from the input dataset. The goal of the generator is to maximize the probability of  the discriminator making a mistake. Hence, if $P_\text{input}$ is the distribution of the input dataset then the adversarial network intends to minimize the distance (as measured by the Jensen-Shannon divergence) between the distribution of the generated data $P_G$ and $P_\text{input}$. In short, the problem is a minmax game with value function $V(D, G)$
\begin{align*}
    \min_{G}\max_{D} V(D, G) =  &\mathbb{E}_{x \sim P_{\text{input}} }[\log D(x)] + \mathbb{E}_{z \sim P_{G}}[\log\big(1 - D(G(z))\big)].
\end{align*}
Auto-encoders and derivatives \cite{Goodfellow-et-al-2016, laforgue2018autoencoding, fard2018deep} form a subclass of neural networks whose purpose is to build a suitable representation by learning encoding and decoding functions which capture the core properties of the input data.
An adversarial auto-encoder (see \cite{makhzani2015adversarial}) is a specific kind of auto-encoders where the encoder plays the role of the generator of an adversarial network. Thus the latent code is forced to follow a given distribution while containing information relevant to reconstructing the  input. In the remaining of this paper, a similar adversarial encoder constrains the encoded representation to be heavy-tailed. 

\subsection{Models Overview}\label{sec:workflows} 
Figure~\ref{Fig:schema} provides an overview of the different algorithms proposed  in the paper. Figure~\ref{fig:two_heads} describes the pipeline for {\fontfamily{qcr}\small\textbf{LHTR}} detailed in Algorithm~\ref{alg:Orthrus}. Figure~\ref{fig:one_head} describes the pipeline for the comparative baseline~\HTalgo$_1$  where $C^\text{ext} = C^\text{bulk}$. Figure~\ref{fig:baseline} illustrates the pipeline for the baseline classifier trained on BERT. Figure~\ref{fig:generation} describes  {\fontfamily{qcr}\small\textbf{GENELIEX}} described in Algorithm~\ref{alg:hydra}, note that the hatched components are inherited from \HTalgo~ and are not used in the workflow.
\begin{figure}[h]{}
 \begin{subfigure}[t]{0.49\textwidth}
 \includegraphics[width=1\textwidth]{figures/two_heads.pdf}
 \caption{}
   \label{fig:two_heads}
 \end{subfigure}
 \begin{subfigure}[t]{0.49\textwidth}
\includegraphics[width=\textwidth]{figures/one_heads.pdf}
   \caption{}
   \label{fig:one_head}
 \end{subfigure}
 \begin{subfigure}[t]{0.49\textwidth}
   \includegraphics[width=1\textwidth]{figures/bert.pdf}
   \caption{}
   \label{fig:baseline}
 \end{subfigure}
 \hfill
 \begin{subfigure}[t]{0.49\textwidth}
   \includegraphics[width=1\textwidth]{figures/generation.pdf}
   \caption{}
   \label{fig:generation}
 \end{subfigure}
 \caption{Illustrative pipelines.}
\label{Fig:schema}
 \end{figure}

\subsection{\HTalgo\ and \geneliex\ algorithm}\label{sec:code} This subsection provides detailed algorithm for both models \HTalgo\ and \geneliex.
\begin{algorithm}[t]
\caption{LHTR}
\begin{algorithmic}\INPUT  Weighting coef. $\rho_1, \rho_2, \rho_3>0$, Training dataset $\mathcal{D}_n=\{(X_1,Y_1),\; \ldots,\; (X_n,Y_n)\}$, batch size $m$, proportion of extremes $\kappa$, heavy tailed prior $P_Z$. 
\Initialize parameters $(\tau, \theta,\theta', \gamma)$ of the encoder $\varphi_\tau$, classifiers $C_\theta^{ext}$, $C_{\theta^\prime}^{bulk}$ and  discriminator $D_\gamma$
\Optimization
\While{$(\tau, \theta, \theta^\prime,\gamma)$ not converged}
\Statex Sample $\{(X_1, Y_1) \ldots, (X_m, Y_m)\}$ from $\mathcal{D}_n$ 
and define $\tilde Z_i = \varphi(X_i), \, i\le m$. 
\Statex Sample $\{Z_1, \ldots, Z_m\}$ from the prior $P_Z$.
\Statex Update $\gamma$ by ascending:\\\Statex 
 $$ \frac{\rho_3}{m}\sum_{i=1}^m \log D_\gamma(Z_i) + \log(1 - D_\gamma(\tilde{Z_i})).$$
\Statex Sort $\{\tilde{Z_i}\}_{i \in \{1,\ldots, m\}}$ by decreasing order of magnitude $||\tilde{Z}_{(1)}||\geq \ldots \geq ||\tilde{Z}_{(m)}||.$
\Statex Update $\theta$ by descending:\\\Statex 
$$ \mathcal{L}^\text{ext} (\theta,\tau) \defeq \frac{\rho_1}{\lfloor\kappa m\rfloor}\sum_{i=1}^{\lfloor{\kappa m}\rfloor} \ell\big(Y_{(i)} ,  C^\text{ext}_\theta(\tilde{Z}_{(i)})\big).$$\Statex Update $\theta^\prime$ by descending:\\\Statex
$$\mathcal{L}^\text{bulk}(\theta^\prime,\tau) \defeq \frac{\rho_2}{m - \lfloor{\kappa m}\rfloor}\sum_{i=\lfloor{\kappa m}\rfloor+1}^m \ell\big( Y_{(i)} ,  C_{\theta^\prime}^\text{bulk}(\tilde{Z}_{(i)})\big).$$\Statex
\Statex Update $\tau$ by descending: 
\Statex
$$ \frac{1}{m}\sum_{i=1}^m - \rho_3 \log D_\gamma(\tilde{Z}_{i})
+ \mathcal{L}^\text{ext}(\theta,\tau) + \mathcal{L}^\text{bulk}(\theta^\prime,\tau).
$$
\EndWhile
\Statex Compute $\{\tilde{Z_i}\}_{i \in \{1,\ldots, n\}} = {\varphi(X_i)}_{i \in \{1,\ldots, n\}}$
\Statex Sort $\{\tilde{Z_i}\}_{i \in \{1,\ldots, n\}}$ by decreasing order of magnitude $||\tilde{Z}_{(1)}||\geq \ldots ||\tilde{Z}_{(\lfloor{\kappa n}\rfloor)}||\geq \ldots \geq ||\tilde{Z}_{(n)}||.$
\OUTPUT encoder $\varphi$, classifiers $C^\text{ext}$ for $\{x : ||\varphi(x)|| \geq  t:=||\tilde{Z}_{(\lfloor{\kappa n}\rfloor)}||  \}$ and $C^\text{bulk}$ on the complementary set.
\end{algorithmic}
\label{alg:Orthrus}
\end{algorithm} \begin{algorithm}[H]
\caption{GENELIEX: training step}
\begin{algorithmic}\INPUT 
input of LHTR, 
$\mathcal{D}_{g_{n}}=\{U_{1},\; \ldots,\; U_{n}\}$
\Initialize parameters of $\varphi_\tau$, $C_\theta^{\text{ext}}$, $C_{\theta^\prime}^{\text{bulk}}$, $D_\gamma$ and decoder $G_\psi^{\text{ext}}$ \Optimization  \Statex  $\varphi$, $C^{\text{ext}}$, $C^{\text{bulk}}$ = LHTR($\rho_1, \rho_2, \rho_3$, $\mathcal{D}_{n},\kappa, m$)
\While {$\psi$ not converged} 
\Statex Sample $\{U_{1} \ldots, U_{m}\}$ from the training set $\mathcal{D}_{g_n}$ and define
$\tilde{Z_i} = \varphi( X_{U,i})$ for $i \in \{1,\ldots, m\}$.
\Statex Sort $\{\tilde{Z_i}\}_{i \in \{1,\ldots, m\}}$ by decreasing order of magnitude
$\|\tilde{Z}_{(1)}\| \geq \ldots \geq \|\tilde{Z}_{(m)}\|.$
\Statex Update $\psi$ by descending:$$\mathcal{L}_g^\text{ext}(\psi) \defeq \frac{\rho_1}{\lfloor{\kappa m}\rfloor}\sum_{i=1}^{\lfloor{\kappa m}\rfloor} \ell_{gen.} \big(U_{(i)}, G^\text{ext}_\psi(\tilde{Z}_{(i)})\big).$$
\EndWhile
\Statex Compute $\{\tilde{Z_i}\}_{i \in \{1,\ldots, n\}} = {\varphi(X_i)}_{i \in \{1,\ldots, n\}}$
\Statex Sort $\{\tilde{Z_i}\}_{i \in \{1,\ldots, n\}}$ by decreasing order of magnitude $\|\tilde{Z}_{(1)}\|\geq \ldots \|\tilde{Z}_{(k)}\|\geq \ldots \geq \|\tilde{Z}_{(n)}\|.$
\OUTPUT encoder $\varphi$, decoder $G^\text{ext}$ applicable on the region $\{x : \|\varphi(x)\| \geq \|\tilde{Z}_{(\lfloor{\kappa n}\rfloor)}\|  \}$ 
\end{algorithmic}
\label{alg:hydra}
\end{algorithm}
%
 

\section{Extreme Value Analysis: additional material}
\label{sec:classif-supplem}

\subsection{Choice of~k}  
To the best of our knowledge, selection of $k$  in extreme value analysis (in particular in  Algorithm~\ref{alg:Orthrus} and Algorithm~\ref{alg:hydra}) is still a vivid problem in EVT for which no absolute answer exists. As $k$ gets large the number of extreme points increases including samples which are not large enough and deviates from the asymptotic distribution of extremes. Smaller values of $k$ increase the variance of the classifier/generator. This bias-variance trade-off is beyond the scope of this paper.

\subsection{Preliminary standardization for selecting extreme samples} In Figure~\ref{fig:classif_RawInput} selecting the extreme samples on the input space is not a straightforward step as the two components of the vector are not on the same scale, componentwise standardisation is a natural and necessary preliminary step. Following common practice in multivariate extreme value analysis it was decided to standardise the input data $(X_i)_{i \in \{1,\ldots, n\} }$ by applying the rank-transformation: $$\widehat{T}(x) =\bigg(1 / \Big(1 - \widehat{F}_j(x) \Big) \bigg)_{j=1, \ldots, d} $$ for all $x = (x^{1}, \ldots, x^{d}) \in \rset^d$ where $\widehat{F}_j (x) \defeq \frac{1}{n+1}\sum_{i=1}^n \mathds{1}\{X_i^j \leq x\}$ is the $j^{th}$ empirical marginal distribution.
Denoting by $V_i$ the standardized variables, $\forall i \in \{1, \ldots, n \}, V_i = \hat{T}(X_i)$.  The marginal distributions of $V_i$ are well approximated by  standard Pareto distribution, the approximation error comes from the fact that the empirical \emph{c.d.f}'s are used in $\widehat T$ instead of the genuine marginal \emph{c.d.f.}'s $F_j$. After this standardization step, the selected extreme samples are  $\{ V_i, \|V_i\| \geq  V_{(\lfloor \kappa n \rfloor)} \}$. 
\subsection{Enforcing regularity assumptions in Theorem~\ref{thm:main}}\label{sec:unifCV_app}
The methodology in the  present paper consists in learning a representation $Z$ for text data \emph{via} \HTalgo\ satisfying the regular variation  condition~(\ref{RV}). This condition   is weaker than the assumptions from Theorem~\ref{thm:main} for two reasons: first, it does not imply that  each class (conditionally to the label $Y$) is regularly varying, only that the distribution of $Z$  (unconditionally to the label) is. Second, 
in \citet{jalalzai2018binary}, it is additionally required that the regression function $\eta(z) =\PP{Y = +1 \given Z=z}$ converges uniformly as $\|z\|\to \infty$. Getting into details, one needs to introduce a limit random pair $(Z_\infty, Y_\infty)$ which distribution is the limit of $\PP{Y = \,\cdot\, ,  t^{-1} Z \in \, \cdot\, \given \|Z\| > t}$ as $t\to \infty$. Denote by $\eta_\infty$ the limiting regression function, $\eta_\infty(z)=  \PP{Y_\infty = +1 \given Z_\infty = z}$. The required assumption is that 
\begin{equation}
    \label{eq:uniformCV_regression}
\sup_{\{z \in \rset_+^d: \|z\| >t\}} \big| \eta(z) - \eta_\infty(z)\big| \xrightarrow[t\to\infty]{} 0. 
\end{equation}
Uniform convergence~(\ref{eq:uniformCV_regression}) is not enforced in \HTalgo\, and the question of how to enforce it together with regular variation of each class separately remains open.  However, our experiments in sections~\ref{sec:Exp} and~\ref{sec:Exp_g} demonstrate  that enforcing  Condition~(\ref{RV})  is enough for our purposes,  namely improved classification and label preserving data augmentation.  
 
 
\subsection{Logistic distribution} \label{logistic_Appendix}
The logistic distribution  with dependence  parameter $\delta \in (0, 1]$  is defined in $\rset^d$ by its  \textit{c.d.f.}
$F(x) = \exp\big\{ - (\sum_{j=1}^d {x^{(j)}}^\frac{1}{\delta})^{\delta} \big\}$.
Samples from the  logistic distribution can be  simulated according to the algorithm proposed in  \citet{stephenson2003simulating}. \autoref{fig:Logistic_Exemples} illustrates this distribution with various values of $\delta$. Values of $\delta$ close to~$1$ yield  non concomitant extremes, \emph{i.e.} the probability of a simultaneous excess of a high threshold by more than one vector component is negligible. Conversely, for small values of $\delta$, extreme values tend to occur simultaneously. These two distinct tail dependence structures are respectively called  `asymptotic independence' and `asymptotic dependence' in the EVT terminology. 



\begin{figure}[tbh]
\begin{subfigure}[t]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/logistic_alpha_09.pdf}
    \caption{ near tail independence}
    \label{fig:logistic_alpha_09}
    \end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/logistic_alpha_05.pdf}
    \caption{ moderate tail dependence}
    \label{fig:logistic_alpha_05}
        \end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/logistic_alpha_01.pdf}
    \caption{high tail dependence}
    \label{fig:logistic_alpha_01}
\end{subfigure}
\caption{Illustration of the distribution of the angle $\Theta(X)$ obtained with bivariate samples $X$ generated from a logistic model with different coefficients of dependence ranging from near asymptotic independence  \autoref{fig:logistic_alpha_09} ($\delta = 0.9$) to high asymptotic dependence \autoref{fig:logistic_alpha_01} ($\delta = 0.1$) including moderate dependence \autoref{fig:logistic_alpha_05} ($\delta = 0.5$). Non extreme samples are plotted in gray, extreme samples are plotted in black and the angles  $\Theta(X)$ (extreme samples projected on the sup norm sphere) are plotted in red. Note that not all extremes are shown since the plot was truncated for a better visualization. However all projections on the sphere are shown.}
\label{fig:Logistic_Exemples}
\end{figure}



\subsection{Scale invariance comparison of BERT and \HTalgo}\label{BERT_notRV}
In this section, we compare \HTalgo\ and BERT and show that the latter is not scale invariant. For this preliminary experiment we rely on labeled fractions of both \textit{Amazon} and \textit{Yelp} datasets respectively denoted as \textit{Amazon small dataset} and \textit{Yelp small dataset} detailed in \cite{kotzias2015group}, each of them containing $1000$ sequences from the large dataset. Both datasets are divided at random in a train set $\mathcal{T}_\text{train}$ and  $\mathcal{T}_\text{test}$. The train set represents \nicefrac{3}{4} of the whole dataset while the remaining samples represent the test set. We use the hyperparameters reported in Table
~\ref{tab:small_dataset_experiment}. 

\begin{table}[ht]
    \centering
    \begin{tabular}{c|ccc}\hline
     & NN model & $ \HTalgo_{1}$  & \HTalgo  \\ \hline
    Sizes of the layers $\varphi$   &  [768,384,200,50,8,1] & [768,384,200,100] & [768,384,200,150]   \\
    Sizes of the layers  $C_{\theta^\prime}^{bulk}$  & X  &  [100,50,8,1] &  [150,75,8,1]  \\
        Sizes of the layers  $C_{\theta}^{ext}$ & X &  X &  [150,75,8,1]\\
  $\rho_3$   & X  & X & 0.001 \\ 
\end{tabular}
    \caption{Network architectures for \textit{Amazon small dataset} and \textit{Yelp small dataset }.  The weight decay is set to $10^5$, the learning rate is set to $5*10^{-4}$, the number of epochs is set to 500 and the batch size is set to $64$.}
    \label{tab:small_dataset_experiment}
\end{table}

\textbf{BERT is not regularly varying.}
In order to show that $X$ is not regularly varying, independence between $\|X\|$ and a margin of $\Theta(X)$ can be tested   \cite{coles1994statistical}, which is easily  done \emph{via} correlation tests.  
Pearson correlation tests were run on the extreme samples  of BERT and \HTalgo\ embeddings of $\textit{Amazon small dataset}$ and $\textit{Yelp small dataset}$. The statistical tests were performed  between all margins of $\big(\Theta(X_i)\big)_{1 \geq i \geq n}$ and $\big(\|X_i\|\big)_{1 \geq i \geq n}$.  
\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/pearson_BERT_yelp.pdf}
    \caption{\textit{Yelp small dataset} - BERT}
    \label{fig:pearson_bert_yelp}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/pearson_BERT_amazon.pdf}
    \caption{\textit{Amazon small dataset} - BERT}
    \label{fig:pearson_bert_amazon}
\end{subfigure}

\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/pearson_LHTR_yelp.pdf}
    \caption{\textit{Yelp small dataset} - \HTalgo}
    \label{fig:pearson_lhtr_yelp}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/pearson_LHTR_amazon.pdf}
    \caption{\textit{Amazon small dataset} - \HTalgo}
    \label{fig:pearson_lhtr_amazon}
\end{subfigure}
\caption{Histograms of the $p$-values for the non-correlation test between $\big(\Theta(X_i)\big)_{1 \geq i \geq n}$ and $\big(\|X_i\|\big)_{1 \geq i \geq n}$ on embeddings provided by BERT (Figure~\ref{fig:pearson_bert_yelp} and Figure~\ref{fig:pearson_bert_amazon}) or \HTalgo\ (Figure~\ref{fig:pearson_lhtr_yelp} and Figure~\ref{fig:pearson_lhtr_amazon}).}
\label{fig:pearson}
\end{figure}
Each histogram in Figure~\ref{fig:pearson}  displays the distribution of the  $p$-values of  the correlation tests between the margins  $X_j$ and the angle $\Theta(X)$ for $j \in \{1,\ldots d\}$, in  a given representation (BERT or \HTalgo) for a given dataset. For both \textit{Amazon small dataset} and \textit{Yelp small dataset} the distribution of the $p$-values is shifted towards larger values in the representation of \HTalgo\ than in BERT, which means that the correlations are weaker in the former representation than in the latter. This phenomenon is more pronounced with \textit{Yelp small dataset} than with \textit{Amazon small dataset}. Thus, in BERT representation, even the largest data points exhibit a non negligible correlation between the radius and the angle and the regular variation condition does not seem to be satisfied. 
As a consequence, 
in a classification setup such as binary sentiment analysis  detailed in Section~\ref{sec:sentiment_analysisYelp}), classifiers trained on BERT embedding are not guaranteed to be scale invariant. In other words for a representation $X$ of a sequence $U$ with a given label $Y$, the predicted label $g(\lambda X)$ is not necessarily constant for varying values of $\lambda \geq 1$.
Figure~\ref{fig:barcode_bert} illustrates this fact on a particular example taken from \textit{Yelp small dataset}. The color (white or black respectively) indicates the predicted class (respectively $-1$ and $+1$). For values of $\lambda$ close to $1$, the predicted class is $-1$ but the prediction shifts to class $+1$  for larger values of $\lambda$. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/bar_code_BERT.pdf}
    \caption{Lack of scale invariance of the classifier trained on BERT: evolution of the predicted label $g(\lambda X)$ from $-1$ to $+1$ for increasing values of $\lambda$, for one particular example $X$.}
    \label{fig:barcode_bert}
\end{figure}{}

\textbf{Scale invariance of \HTalgo.} We provide here experimental evidence that \HTalgo's classifier $g^\text{ext}$ is scale invariant (as defined in Equation~(\ref{eq:invariance_dilation})). Figure~\ref{fig:barcodes_lhtr} displays the predictions $g^{\text{ext}}(\lambda Z_i)$ for increasing values of the scale factor $\lambda\ge 1$  and $Z_i$ belonging to $\mathcal{T}_{\text{test}}$, the set of samples considered as extreme in the learnt representation. For any such sample $Z$,
the predicted label remains constant as $\lambda$ varies, \emph{i.e.} it is  scale invariant, $g^\text{ext}(\lambda Z) = g^\text{ext}(Z)$, for all $\lambda\ge 1$. 
\begin{figure}[ht]
\centering
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/small_datasets/small_amazon/bar_code_extreme_amazon.pdf}
    \caption{\textit{Amazon small dataset}}
    \label{fig:bar_code_amazon}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/small_datasets/small_yelp/bar_code_extreme_yelp}
    \caption{\textit{Yelp small dataset}}
    \label{fig:bar_code_yelp}
\end{subfigure}
\caption{Scale invariance of $g^\text{ext}$ trained on LHTR: evolution of the predicted label   $g^\text{ext}(\lambda Z_i)$ (white or black for $-1 / +1$) for increasing values of $\lambda$, for samples $Z_i$ from the extreme test set $\mathcal{T}_\text{test}$ from \textit{Amazon small dataset} (Figure~\ref{fig:bar_code_amazon}) and  \textit{Yelp small dataset} (Figure~\ref{fig:bar_code_yelp})\label{fig:barcodes_lhtr}.}
\end{figure}

\subsection{Experimental settings (Classification): additional details}
\label{additional_exp_classif}

\textbf{Toy example. } For the toy example, we generate $3000$ points distributed as a mixture of two normal distributions in dimension two. For training \HTalgo, the number of epochs is set to $100$ with a  dropout rate equal to $0.4$, a  batch size of $64$ and a learning rate of $5*10^{-4}$. The weight parameter $\rho_3$ in the loss function (Jensen-Shannon divergence from the target) is set to $10^{-3}$. 
Each component $\varphi$, $C^{\text{bulk}}$ and $C^{\text{ext}} $ is made of $3$ fully connected layers, the sizes of which are reported in  Table~\ref{tab:toys_dataset_experiment}. \\
\textbf{Datasets.} For Amazon, we
  work with the video games subdataset from
  \url{http://jmcauley.ucsd.edu/data/amazon/}. For Yelp \cite{yelp_1,yelp_2}, we work with 1,450,000 reviews after that can be found at
  \url{https://www.yelp.com/dataset}. 




\begin{table}[ht]
    \centering
    \begin{tabular}{c|c}\hline
&     Layers' sizes \\ \hline
$\varphi$  & [2,4,2]   \\
$C_{\theta^\prime}^{bulk}$ &  [2,8,1]   \\
$C_{\theta}^{ext}$ &  [2,8,1] \\
\end{tabular}
    \caption{ Sizes of the successive layers in each component of \HTalgo\ used in the toy example. }
    \label{tab:toys_dataset_experiment}
\end{table}

\textbf{BERT representation for text data.} 
We use BERT pretrained models and code from the library \textit{Transformers} \footnote{\url{https://github.com/huggingface/transformers}}. All models were implemented using Pytorch and trained on a single Nvidia P100. The output of BERT is a $\rset^{768}$ vector. All parameters of the models have been selected using the same grid search. 

\textbf{Network architectures.}
Tables~\ref{tab:huge_dataset_experiment} report the architectures (layers sizes) chosen for each component of the three algorithms considered for performance comparison (Section~\ref{sec:Exp}), respectively for the moderate and large datasets used in our experiments.  We set $\rho_1 = (1 -  \hat{\mathbb{P}}(||Z|| \geq ||Z_{(\lfloor \kappa n \rfloor)}||))^{-1}$ and $\rho_2=\hat{\mathbb{P}}(||Z||\geq ||Z_{(\lfloor \kappa n \rfloor)}||)^{-1}$.




\begin{table}[ht]
    \centering 
    \begin{tabular}{c|ccc}\hline
     & NN model & $ \HTalgo_{1}$  & $ \HTalgo$    \\\hline
   Sizes of the layers $\varphi$   &  [768,384,200,50,8,1] & [768,384,200,100] & [768,384,200,150]   \\
    Sizes of the layers of  $C_{\theta^\prime}^{bulk}$ & [150,75,8,1]  &  [100,50,8,1] &  [150,75,8,1]  \\
        Sizes of the layers of  $C_{\theta}^{ext}$ & X &  X &  [150,75,8,1]\\
  $\rho_3$   & X  & X & 0.01 \\ 
\end{tabular}
    \caption{Network architectures for \textit{Amazon dataset} and \textit{Yelp dataset}. The weight decay is set to $10^5$, the learning rate is set to $1*10^{-4}$, the number of epochs is set to 500 and the batch size is set to $256$.}
    \label{tab:huge_dataset_experiment}
\end{table}

\subsection{Experiments for data generation}
\label{additional_exp_generation}
\subsubsection{Experimental setting}
As mentioned in Section~\ref{subsection:generation}, hyperparameters for dataset augmentation are detailed in Table~\ref{tab:generation_experiments}.
\begin{table}[ht]
    \centering
    \begin{tabular}{c|ccc}\hline
     &  \HTalgo  \\\hline Sizes of the layers $\varphi$  & [768,384,200,150]   \\ Sizes of the layers of  $C_{\theta^\prime}^{bulk}$ & [150,75,8,1]  \\ Sizes of the layers of  $C_{\theta}^{ext}$ & [150,75,8,1]\\ $\rho_3$ &0.01 \\ \end{tabular}
    \caption{ For \textit{Amazon}  and \textit{Yelp}, we follow \cite{classif} the weight decay is set to $10^5$, the learning rate is set to $1*10^{-4}$, the number of epochs is set to 100 and the batch size is set to $256$.}
    \label{tab:generation_experiments}
\end{table}
For the Transformer Decoder we use $2$ layers with $8$ heads, the dimension of the key and value is set to $64$ \cite{attention_is} and  the inner dimension is set to $512$. The architectures for the models proposed by \citet{eda} and \citet{naacl_baseline} are chosen according to the original papers. For a fair comparison with \citet{naacl_baseline}, we update the language model with a BERT model, the labels are embedded in $\rset^{10}$ and  fed to a single MLP layer (this dimension is chosen using the same procedure as in \cite{colombo2019affect,dinkar2020importance}). The new model is trained using AdamW \cite{adamW}.
\subsubsection{Influence of the scaling factor on the linguistic content } Table~\ref{tab:additional_example_sentences} gathers some extreme sequences generated by \geneliex\ for $\lambda$ ranging from $1$ to $1.5$.  No major linguistic change appears when $\lambda$ varies. The generated sequences  are grammatically correct and share the same polarity (positive or negative sentiment) as the input sequence. Note that for greater values of $\lambda$, a repetition phenomenon appears. The resulting sequences keep the label and polarity of the input sequence but repeat some words \cite{holtzman2019curious}.
\begin{table}[ht]
\centering
\resizebox{.9\textwidth}{!}{ \begin{tabular}[t]{c|p{16cm}}
\small Input	& very sloppy and slow service. when we arrived they told us to sit anywhere but all the tables were still dirty and haven't been cleaned. they didn't bother to ask if we wanted refills on our drinks. we needed an extra plate and didn't get one so my nephew decides to go up to the counter and ask for one because he's hungry. they gave our check when we were still eating. the list can go on and on. i wouldn't recommend this place. go somewhere else for faster and better service. very disappointed
\\
\small $\lambda = 1.1$	&very sloppy and sluggish service. when we got there, they told us to sit anywhere but all the tables were empyt full of dishes and were not cleaned at all. they didn't bother to ask if our drinks would be added. we needed an extra dish and didn't get one, so my cousin decided to go to the counter and ask one because he's hungry. they were going to watch while we were still eating. the list could go on and on. i would not recommend this place. go elsewhere for faster and better service. very very disappointed \\
\small $\lambda = 1.2$	&services and survivors. when he got there, he told us we were sitting everywhere but all the tables were full of dishes and we didn't wash everything. he never bothered to ask if our drinks would be added. we needed extra food and didn't get one, so my brother decided to go to the locker and ask because he was thirsty. they want to watch it while we eat. the list can be continuous and active. i would not recommend this place. go elsewhere for faster and better service. very disappointed \\
\small $\lambda = 1.3$	&services and survivors. when he got there, he told us that we were sitting everywhere, but all the tables were full of dishes and we didn't wash everything. he never bothered to ask if our drinks would be added. We needed more food and we didn't get it, so my brother decided to go to the locker and ask because he was thirsty. they want to watch it when we eat. the list can be continuous and active. i would not recommend this place. go faster and faster for better service. very disappointed \\\hline
\small Input	&visited today with my husband. we were in the firearms section. there were 3 employees in attendance with one customer. my husband ask a question and was ignored. he waited around for another 10 minutes or so. if it had been busy i could understand not receiving help. we left and went elsewhere for our purchases. \\
\small $\lambda = 1.1$	&visited today with my husband. we were in the firearms section. together with one customer there were 3 employees. my husband asked and was ignored. waited about another 10 minutes. if it was busy, i would understand that i wouldn't get help. we left and went somewhere else because of our purchases. \\
\small $\lambda = 1.2$	&today she visited with her husband. we were in the gun department. there were 3 employees together with one customer. my husband asked and was ignored. waited another 10 minutes. if he was busy, i would understand that i would not receive help. we went and went somewhere else because of our shopping. \\
\small $\lambda = 1.3$	&today, she went with her husband. we are in the gun department. there are 3 employees and one customer. my husband rejected me and ignored him. wait another minute. if he has a job at hand, i will understand that i will not get help. we went somewhere else because of our business. \\\hline
\small Input	&walked in on a friday and got right in. it was exactly what i expected for a thai massage. the man did a terrific job. he was very skilled, working on the parts of my body with the most tension and adjusting pressure as i needed throughout the massage. i walked out feeling fantastic and google eyed. \\
\small $\lambda = 1.1$	&walked in on a friday and got right in. it was exactly what i expected for a thai massage. the man did a terrific job. he was very skilled, working on the parts of my body with the most tension and adjusting pressure as needed throughout the massage. i walked out feeling fantastic and google eyed. \\
\small $\lambda = 1.2$	&climb up the stairs and get in. the event that i was expecting a thai massage. the man did a wonderful job. he was very skilled, dealing with a lot of stress and stress on my body parts. i walked out feeling lightly happy and tired. \\
\small $\lambda = 1.3$	&go up and up. this was the event i was expecting a thai massage. the man did a wonderful job. what this was was an expert, with a lot of stress and stress on my body parts. i walked out feeling lightly happy and tired. \\\hline
\small Input	&i came here four times during a 3 - day stay in madison. the first two was while i was working - from - home. this place is awesome to plug in, work away at a table, and enjoy a great variety of coffee. the other two times, i brought people who wanted good coffee, and this place delivered. awesome atmosphere. awesome awesome awesome. \\
\small $\lambda = 1.1$	&i came here four times during a 3-day stay in henderson. the first two were while i was working - from home. this place is great for hanging out, working at tables and enjoying the best variety of coffee. the other two times, i brought in people who wanted a good coffee, and it delivered a place. better environment. really awesome awesome. \\
\small $\lambda = 1.2$	&i came here four times during my 3 days in the city of henderson. the first two were while i was working - at home. this place is great for trying, working tables and enjoying the best variety of coffee. the other two times, i brought people who wanted good coffee, and it brought me somewhere. good environment. really amazing. \\
\small $\lambda = 1.3$	&i came here four times during my 3 days in the city of henderson. the first two are when i'm working - at home. this place is great for trying, working tables and enjoying a variety of the best coffees. the other two times, i bring people who want good coffee, and that brings me somewhere. good environment. very amazing.\\\hline
\end{tabular}}
\caption{Sequences generated by \geneliex\ for extreme embeddings implying label (sentiment polarity) invariance for generated Sequence. $\lambda$ is the scale factor. Two first reviews are negatives, two last reviews are positive.}
\label{tab:additional_example_sentences}
\end{table}


\iffalse
  \begin{table}[ht]
\centering
\begin{tabular}[t]{c|c}
\hline

\hline
\small input	&\small ( it wasn't busy either), the building was cold.  	\\\hline
\small $\lambda = 1$&	\small (it was not occupied either), the building was cold. 		\\
\small $\lambda = 1.1$	&\small (i wasn't busy either), the building was frozen. 	\\
\small $\lambda = 1.3$	&\small also, the building was freezing.  	\\
\small $\lambda = 1.5$	&\small plus, the building was colder than ice. \\\hline
\small input &\small food quality has been horrible.
\\\hline
\small $\lambda = 1$&\small 		food quality has been terrible.  	\\
\small $\lambda = 1.1$	&\small the quality of the food was horrible.  	\\
\small $\lambda = 1.3$& \small 	the quality of the food has been horrible. 	\\
\small $\lambda = 1.5$	& \small the quality of food was terrible.  \\
\hline
\small input	&\small  overall , i like there food and the service .	\\\hline
\small $\lambda = 1$&	\small 	i love food and the service. 	\\
\small $\lambda = 1.1$	&\small on the whole, i like food and service. 	 	\\
\small $\lambda = 1.3$&\small 	in general, i like the food and the service.  \\
\small $\lambda = 1.5$&\small 	in general, i like food and service. 	 \\
\hline

\small input	&\small  the desserts were a bit strange. 	\\\hline
\small $\lambda = 1$&	\small 	the desserts were a little weird. 	\\
\small $\lambda = 1.1$	&\small the desserts were very strange. 	\\
\small $\lambda = 1.3$&\small 	the desserts were terrible. \\
\small $\lambda = 1.5$&\small 	the desserts were terrible.
\\
\hline
\small input	&\small  we definately enjoyed ourselves.	\\\hline
\small $\lambda = 1$&	\small we enjoyed ourselves.  	\\
\small $\lambda = 1.1$	&\small we had a lot of fun. 		\\
\small $\lambda = 1.3$&\small 	we've really enjoyed each other. \\
\small $\lambda = 1.5$&\small we certainly had fun. \\
\hline
\small input &\small 	seriously killer hot chai latte. 	\\\hline
\small $\lambda = 1$&\small 	-it's a real killer. 	\\
\small $\lambda = 1.2$	&\small he is a real killer. 	\\
\small $\lambda = 1.3$& \small he likes to kill. 	\\
\small $\lambda = 1.5$	& \small i loves murders. \\
\hline
\small input	&\small all of the tapas dishes were delicious! 	 \\	\hline
\small $\lambda = 1$&\small all the tapas was delicious.  	\\
\small $\lambda = 1.1$\small	&all tapas dishes were delicious!  	\\
\small $\lambda = 1.3$&\small	all the tapas dishes were delicious! 	\\
\small $\lambda = 1.5$\small	&the tapas were great!  	\\
\hline
\small input	&\small  there was hardly any meat. 	 	\\\hline
\small $\lambda = 1$&	\small there was almost no meat. 	\\
\small $\lambda = 1.1$	&\small there was practically no meat. 	\\
\small $\lambda = 1.3$&\small 	there was almost no meat. \\
\small $\lambda = 1.5$&\small 	there was no meat.
\\
\hline
\small input	&\small  waiter was a jerk. 	\\\hline
\small $\lambda = 1$&	\small the waiter was a jerk. 	\\
\small $\lambda = 1.1$	&\small awaiter was a poor guy. \\
\small $\lambda = 1.3$&\small 	waiter was an idiot. \\
\small $\lambda = 1.5$&\small 	waiter was such an idiot. \\
\hline
\small input	&\small  i 'm not eating here! 	\\\hline
\small $\lambda = 1$&	\small i don't eat here. 	\\
\small $\lambda = 1.1$	&\small i don't eat here! 	 \\
\small $\lambda = 1.3$&\small 	i'm not going to eat here! 	 \\
\small $\lambda = 1.5$&\small 	i will never going to eat here! \\
\hline
\end{tabular}
\caption{Sequences generated by \geneliex\ for extreme embeddings implying label (sentiment polarity) invariance for generated sequence. $\lambda$ is the scale factor.}
\label{tab:additional_example_sentences}
\end{table}

\section{A linguistic point of view of extremes}\label{sec:linguistic_appendix}
In this section, we  investigate possible linguistic interpretations 
regarding the extreme or non extreme nature of text data in the heavy-tailed representation learnt by \HTalgo. 
We consider the number of words in each sequence of  the extreme region $\mathcal{T^\lambda}$ for increasing values of $\lambda$.  Figure~\ref{fig:violin_plot} displays boxplots of these quantities for 
\textit{Amazon small dataset} and \textit{Yelp small dataset}. In both cases one can notice that the median  of the number of words in extreme samples tends to increase  for increasing values of $\lambda$. \cccmt{que peut-on dire de ça? pourquoi à votre avis le nombre  de mots augmente avec lambda? }\addeda{on n'a pas de réponse et je ne suis pas sûre qu'il y en ait une. il serait peut etre preferable de supprimer la section D (et ce que j'ai rajouté en bleu ans la partie  3.1), si chloé  confirme qu'on ne nous reprochera pas forcéement de ne pas donner d'interprétation linguistique, tant que les performances augmentent.}\cccmt{Il faut ici discuter le tableau 7}
\cccmt{La phrase suivante est à supprimer ou à détailler. Sans avoir expliqué la méthodologie adoptée pour calculer les affects scores ce résultat n'est pas forcémenet convaincant }Further experiments through the affect scores provided by the lexicon for English words from \cite{mohammad2017word}, show that the affect score of samples in the extreme regions and in the bulk are similar. Therefore, extreme samples can not be considered as prototypical or different in terms of content.
\begin{figure}[ht]
\begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[width=\textwidth]{figures/small_datasets/boxplot_Nwords_amazon.pdf}
    \caption{}
    \label{fig:violin_plot_amazon}
    \end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}

    \includegraphics[width=\textwidth]{figures/small_datasets/boxplot_Nwords_yelp.pdf}
    \caption{}
    \label{fig:violin_plot_yelp}
\end{subfigure}
\caption{Boxplot of the number of words in each sequence of $\mathcal{T}^\lambda$   in \textit{Amazon small dataset}  (\autoref{fig:violin_plot_amazon}) and for \textit{Yelp small dataset }  (\autoref{fig:violin_plot_yelp}). }
\label{fig:violin_plot}
\end{figure}
\fi
