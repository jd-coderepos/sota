\onecolumn
\begin{center}
  {\large {\bf\textsc{supplementary material : heavy-tailed representations, text polarity classification \& data augmentation}}}
\end{center}
\iffalse
\begin{table}[ht]
\scriptsize
    \centering
\begin{tabular}{|c|cc|cc|}
       \hline \multirow{2}{*}{} & \multicolumn{2}{c|}{\textit{Amazon}} & \multicolumn{2}{c|}{\textit{Yelp}}\\
        & dataset 1 & dataset 2        & dataset 1 & dataset 2 \\\hline
           &750  & 210k & 750 & 1400k\\
           & 250 & 22k & 250 & 22k \\
     & 61 & 6.9k  & 97 & 6.1k\\\hline
\end{tabular}
    \caption{Sizes of the train  , test  and the extreme test set . The extreme train tests constitute  of the  full train tests. These datasets are used to validate  {\fontfamily{qcr}\small\textbf{LHTR}}.}
\label{tab:my_label}
\end{table}
\fi

\section{Models}\label{sec:Model_sup} \subsection{Background on Adversarial Learning}\label{sec:adversarial_learning}
Adversarial networks, introduced in \cite{goodfellow2014generative}, form a system where two neural networks are competing. A first model , called the generator, generates  samples as close as possible to the input dataset. A second model , called the discriminator, aims at distinguishing samples produced by the generator from the input dataset. The goal of the generator is to maximize the probability of  the discriminator making a mistake. Hence, if  is the distribution of the input dataset then the adversarial network intends to minimize the distance (as measured by the Jensen-Shannon divergence) between the distribution of the generated data  and . In short, the problem is a minmax game with value function 

Auto-encoders and derivatives \cite{Goodfellow-et-al-2016, laforgue2018autoencoding, fard2018deep} form a subclass of neural networks whose purpose is to build a suitable representation by learning encoding and decoding functions which capture the core properties of the input data.
An adversarial auto-encoder (see \cite{makhzani2015adversarial}) is a specific kind of auto-encoders where the encoder plays the role of the generator of an adversarial network. Thus the latent code is forced to follow a given distribution while containing information relevant to reconstructing the  input. In the remaining of this paper, a similar adversarial encoder constrains the encoded representation to be heavy-tailed. 

\subsection{Models Overview}\label{sec:workflows} 
Figure~\ref{Fig:schema} provides an overview of the different algorithms proposed  in the paper. Figure~\ref{fig:two_heads} describes the pipeline for {\fontfamily{qcr}\small\textbf{LHTR}} detailed in Algorithm~\ref{alg:Orthrus}. Figure~\ref{fig:one_head} describes the pipeline for the comparative baseline~\HTalgo  where . Figure~\ref{fig:baseline} illustrates the pipeline for the baseline classifier trained on BERT. Figure~\ref{fig:generation} describes  {\fontfamily{qcr}\small\textbf{GENELIEX}} described in Algorithm~\ref{alg:hydra}, note that the hatched components are inherited from \HTalgo~ and are not used in the workflow.
\begin{figure}[h]{}
 \begin{subfigure}[t]{0.49\textwidth}
 \includegraphics[width=1\textwidth]{figures/two_heads.pdf}
 \caption{}
   \label{fig:two_heads}
 \end{subfigure}
 \begin{subfigure}[t]{0.49\textwidth}
\includegraphics[width=\textwidth]{figures/one_heads.pdf}
   \caption{}
   \label{fig:one_head}
 \end{subfigure}
 \begin{subfigure}[t]{0.49\textwidth}
   \includegraphics[width=1\textwidth]{figures/bert.pdf}
   \caption{}
   \label{fig:baseline}
 \end{subfigure}
 \hfill
 \begin{subfigure}[t]{0.49\textwidth}
   \includegraphics[width=1\textwidth]{figures/generation.pdf}
   \caption{}
   \label{fig:generation}
 \end{subfigure}
 \caption{Illustrative pipelines.}
\label{Fig:schema}
 \end{figure}

\subsection{\HTalgo\ and \geneliex\ algorithm}\label{sec:code} This subsection provides detailed algorithm for both models \HTalgo\ and \geneliex.
\begin{algorithm}[t]
\caption{LHTR}
\begin{algorithmic}\INPUT  Weighting coef. , Training dataset , batch size , proportion of extremes , heavy tailed prior . 
\Initialize parameters  of the encoder , classifiers ,  and  discriminator 
\Optimization
\While{ not converged}
\Statex Sample  from  
and define . 
\Statex Sample  from the prior .
\Statex Update  by ascending:\\\Statex 
 
\Statex Sort  by decreasing order of magnitude 
\Statex Update  by descending:\\\Statex 
\Statex Update  by descending:\\\Statex
\Statex
\Statex Update  by descending: 
\Statex

\EndWhile
\Statex Compute 
\Statex Sort  by decreasing order of magnitude 
\OUTPUT encoder , classifiers  for  and  on the complementary set.
\end{algorithmic}
\label{alg:Orthrus}
\end{algorithm} \begin{algorithm}[H]
\caption{GENELIEX: training step}
\begin{algorithmic}\INPUT 
input of LHTR, 

\Initialize parameters of , , ,  and decoder  \Optimization  \Statex  , ,  = LHTR(, )
\While { not converged} 
\Statex Sample  from the training set  and define
 for .
\Statex Sort  by decreasing order of magnitude

\Statex Update  by descending:
\EndWhile
\Statex Compute 
\Statex Sort  by decreasing order of magnitude 
\OUTPUT encoder , decoder  applicable on the region  
\end{algorithmic}
\label{alg:hydra}
\end{algorithm}
%
 

\section{Extreme Value Analysis: additional material}
\label{sec:classif-supplem}

\subsection{Choice of~k}  
To the best of our knowledge, selection of   in extreme value analysis (in particular in  Algorithm~\ref{alg:Orthrus} and Algorithm~\ref{alg:hydra}) is still a vivid problem in EVT for which no absolute answer exists. As  gets large the number of extreme points increases including samples which are not large enough and deviates from the asymptotic distribution of extremes. Smaller values of  increase the variance of the classifier/generator. This bias-variance trade-off is beyond the scope of this paper.

\subsection{Preliminary standardization for selecting extreme samples} In Figure~\ref{fig:classif_RawInput} selecting the extreme samples on the input space is not a straightforward step as the two components of the vector are not on the same scale, componentwise standardisation is a natural and necessary preliminary step. Following common practice in multivariate extreme value analysis it was decided to standardise the input data  by applying the rank-transformation:  for all  where  is the  empirical marginal distribution.
Denoting by  the standardized variables, .  The marginal distributions of  are well approximated by  standard Pareto distribution, the approximation error comes from the fact that the empirical \emph{c.d.f}'s are used in  instead of the genuine marginal \emph{c.d.f.}'s . After this standardization step, the selected extreme samples are  . 
\subsection{Enforcing regularity assumptions in Theorem~\ref{thm:main}}\label{sec:unifCV_app}
The methodology in the  present paper consists in learning a representation  for text data \emph{via} \HTalgo\ satisfying the regular variation  condition~(\ref{RV}). This condition   is weaker than the assumptions from Theorem~\ref{thm:main} for two reasons: first, it does not imply that  each class (conditionally to the label ) is regularly varying, only that the distribution of   (unconditionally to the label) is. Second, 
in \citet{jalalzai2018binary}, it is additionally required that the regression function  converges uniformly as . Getting into details, one needs to introduce a limit random pair  which distribution is the limit of  as . Denote by  the limiting regression function, . The required assumption is that 

Uniform convergence~(\ref{eq:uniformCV_regression}) is not enforced in \HTalgo\, and the question of how to enforce it together with regular variation of each class separately remains open.  However, our experiments in sections~\ref{sec:Exp} and~\ref{sec:Exp_g} demonstrate  that enforcing  Condition~(\ref{RV})  is enough for our purposes,  namely improved classification and label preserving data augmentation.  
 
 
\subsection{Logistic distribution} \label{logistic_Appendix}
The logistic distribution  with dependence  parameter   is defined in  by its  \textit{c.d.f.}
.
Samples from the  logistic distribution can be  simulated according to the algorithm proposed in  \citet{stephenson2003simulating}. \autoref{fig:Logistic_Exemples} illustrates this distribution with various values of . Values of  close to~ yield  non concomitant extremes, \emph{i.e.} the probability of a simultaneous excess of a high threshold by more than one vector component is negligible. Conversely, for small values of , extreme values tend to occur simultaneously. These two distinct tail dependence structures are respectively called  `asymptotic independence' and `asymptotic dependence' in the EVT terminology. 



\begin{figure}[tbh]
\begin{subfigure}[t]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/logistic_alpha_09.pdf}
    \caption{ near tail independence}
    \label{fig:logistic_alpha_09}
    \end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/logistic_alpha_05.pdf}
    \caption{ moderate tail dependence}
    \label{fig:logistic_alpha_05}
        \end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/logistic_alpha_01.pdf}
    \caption{high tail dependence}
    \label{fig:logistic_alpha_01}
\end{subfigure}
\caption{Illustration of the distribution of the angle  obtained with bivariate samples  generated from a logistic model with different coefficients of dependence ranging from near asymptotic independence  \autoref{fig:logistic_alpha_09} () to high asymptotic dependence \autoref{fig:logistic_alpha_01} () including moderate dependence \autoref{fig:logistic_alpha_05} (). Non extreme samples are plotted in gray, extreme samples are plotted in black and the angles   (extreme samples projected on the sup norm sphere) are plotted in red. Note that not all extremes are shown since the plot was truncated for a better visualization. However all projections on the sphere are shown.}
\label{fig:Logistic_Exemples}
\end{figure}



\subsection{Scale invariance comparison of BERT and \HTalgo}\label{BERT_notRV}
In this section, we compare \HTalgo\ and BERT and show that the latter is not scale invariant. For this preliminary experiment we rely on labeled fractions of both \textit{Amazon} and \textit{Yelp} datasets respectively denoted as \textit{Amazon small dataset} and \textit{Yelp small dataset} detailed in \cite{kotzias2015group}, each of them containing  sequences from the large dataset. Both datasets are divided at random in a train set  and  . The train set represents \nicefrac{3}{4} of the whole dataset while the remaining samples represent the test set. We use the hyperparameters reported in Table
~\ref{tab:small_dataset_experiment}. 

\begin{table}[ht]
    \centering
    \begin{tabular}{c|ccc}\hline
     & NN model &   & \HTalgo  \\ \hline
    Sizes of the layers    &  [768,384,200,50,8,1] & [768,384,200,100] & [768,384,200,150]   \\
    Sizes of the layers    & X  &  [100,50,8,1] &  [150,75,8,1]  \\
        Sizes of the layers   & X &  X &  [150,75,8,1]\\
     & X  & X & 0.001 \\ 
\end{tabular}
    \caption{Network architectures for \textit{Amazon small dataset} and \textit{Yelp small dataset }.  The weight decay is set to , the learning rate is set to , the number of epochs is set to 500 and the batch size is set to .}
    \label{tab:small_dataset_experiment}
\end{table}

\textbf{BERT is not regularly varying.}
In order to show that  is not regularly varying, independence between  and a margin of  can be tested   \cite{coles1994statistical}, which is easily  done \emph{via} correlation tests.  
Pearson correlation tests were run on the extreme samples  of BERT and \HTalgo\ embeddings of  and . The statistical tests were performed  between all margins of  and .  
\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/pearson_BERT_yelp.pdf}
    \caption{\textit{Yelp small dataset} - BERT}
    \label{fig:pearson_bert_yelp}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/pearson_BERT_amazon.pdf}
    \caption{\textit{Amazon small dataset} - BERT}
    \label{fig:pearson_bert_amazon}
\end{subfigure}

\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/pearson_LHTR_yelp.pdf}
    \caption{\textit{Yelp small dataset} - \HTalgo}
    \label{fig:pearson_lhtr_yelp}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/pearson_LHTR_amazon.pdf}
    \caption{\textit{Amazon small dataset} - \HTalgo}
    \label{fig:pearson_lhtr_amazon}
\end{subfigure}
\caption{Histograms of the -values for the non-correlation test between  and  on embeddings provided by BERT (Figure~\ref{fig:pearson_bert_yelp} and Figure~\ref{fig:pearson_bert_amazon}) or \HTalgo\ (Figure~\ref{fig:pearson_lhtr_yelp} and Figure~\ref{fig:pearson_lhtr_amazon}).}
\label{fig:pearson}
\end{figure}
Each histogram in Figure~\ref{fig:pearson}  displays the distribution of the  -values of  the correlation tests between the margins   and the angle  for , in  a given representation (BERT or \HTalgo) for a given dataset. For both \textit{Amazon small dataset} and \textit{Yelp small dataset} the distribution of the -values is shifted towards larger values in the representation of \HTalgo\ than in BERT, which means that the correlations are weaker in the former representation than in the latter. This phenomenon is more pronounced with \textit{Yelp small dataset} than with \textit{Amazon small dataset}. Thus, in BERT representation, even the largest data points exhibit a non negligible correlation between the radius and the angle and the regular variation condition does not seem to be satisfied. 
As a consequence, 
in a classification setup such as binary sentiment analysis  detailed in Section~\ref{sec:sentiment_analysisYelp}), classifiers trained on BERT embedding are not guaranteed to be scale invariant. In other words for a representation  of a sequence  with a given label , the predicted label  is not necessarily constant for varying values of .
Figure~\ref{fig:barcode_bert} illustrates this fact on a particular example taken from \textit{Yelp small dataset}. The color (white or black respectively) indicates the predicted class (respectively  and ). For values of  close to , the predicted class is  but the prediction shifts to class   for larger values of . 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/bar_code_BERT.pdf}
    \caption{Lack of scale invariance of the classifier trained on BERT: evolution of the predicted label  from  to  for increasing values of , for one particular example .}
    \label{fig:barcode_bert}
\end{figure}{}

\textbf{Scale invariance of \HTalgo.} We provide here experimental evidence that \HTalgo's classifier  is scale invariant (as defined in Equation~(\ref{eq:invariance_dilation})). Figure~\ref{fig:barcodes_lhtr} displays the predictions  for increasing values of the scale factor   and  belonging to , the set of samples considered as extreme in the learnt representation. For any such sample ,
the predicted label remains constant as  varies, \emph{i.e.} it is  scale invariant, , for all . 
\begin{figure}[ht]
\centering
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/small_datasets/small_amazon/bar_code_extreme_amazon.pdf}
    \caption{\textit{Amazon small dataset}}
    \label{fig:bar_code_amazon}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/small_datasets/small_yelp/bar_code_extreme_yelp}
    \caption{\textit{Yelp small dataset}}
    \label{fig:bar_code_yelp}
\end{subfigure}
\caption{Scale invariance of  trained on LHTR: evolution of the predicted label    (white or black for ) for increasing values of , for samples  from the extreme test set  from \textit{Amazon small dataset} (Figure~\ref{fig:bar_code_amazon}) and  \textit{Yelp small dataset} (Figure~\ref{fig:bar_code_yelp})\label{fig:barcodes_lhtr}.}
\end{figure}

\subsection{Experimental settings (Classification): additional details}
\label{additional_exp_classif}

\textbf{Toy example. } For the toy example, we generate  points distributed as a mixture of two normal distributions in dimension two. For training \HTalgo, the number of epochs is set to  with a  dropout rate equal to , a  batch size of  and a learning rate of . The weight parameter  in the loss function (Jensen-Shannon divergence from the target) is set to . 
Each component ,  and  is made of  fully connected layers, the sizes of which are reported in  Table~\ref{tab:toys_dataset_experiment}. \\
\textbf{Datasets.} For Amazon, we
  work with the video games subdataset from
  \url{http://jmcauley.ucsd.edu/data/amazon/}. For Yelp \cite{yelp_1,yelp_2}, we work with 1,450,000 reviews after that can be found at
  \url{https://www.yelp.com/dataset}. 




\begin{table}[ht]
    \centering
    \begin{tabular}{c|c}\hline
&     Layers' sizes \\ \hline
  & [2,4,2]   \\
 &  [2,8,1]   \\
 &  [2,8,1] \\
\end{tabular}
    \caption{ Sizes of the successive layers in each component of \HTalgo\ used in the toy example. }
    \label{tab:toys_dataset_experiment}
\end{table}

\textbf{BERT representation for text data.} 
We use BERT pretrained models and code from the library \textit{Transformers} \footnote{\url{https://github.com/huggingface/transformers}}. All models were implemented using Pytorch and trained on a single Nvidia P100. The output of BERT is a  vector. All parameters of the models have been selected using the same grid search. 

\textbf{Network architectures.}
Tables~\ref{tab:huge_dataset_experiment} report the architectures (layers sizes) chosen for each component of the three algorithms considered for performance comparison (Section~\ref{sec:Exp}), respectively for the moderate and large datasets used in our experiments.  We set  and .




\begin{table}[ht]
    \centering 
    \begin{tabular}{c|ccc}\hline
     & NN model &   &     \\\hline
   Sizes of the layers    &  [768,384,200,50,8,1] & [768,384,200,100] & [768,384,200,150]   \\
    Sizes of the layers of   & [150,75,8,1]  &  [100,50,8,1] &  [150,75,8,1]  \\
        Sizes of the layers of   & X &  X &  [150,75,8,1]\\
     & X  & X & 0.01 \\ 
\end{tabular}
    \caption{Network architectures for \textit{Amazon dataset} and \textit{Yelp dataset}. The weight decay is set to , the learning rate is set to , the number of epochs is set to 500 and the batch size is set to .}
    \label{tab:huge_dataset_experiment}
\end{table}

\subsection{Experiments for data generation}
\label{additional_exp_generation}
\subsubsection{Experimental setting}
As mentioned in Section~\ref{subsection:generation}, hyperparameters for dataset augmentation are detailed in Table~\ref{tab:generation_experiments}.
\begin{table}[ht]
    \centering
    \begin{tabular}{c|ccc}\hline
     &  \HTalgo  \\\hline Sizes of the layers   & [768,384,200,150]   \\ Sizes of the layers of   & [150,75,8,1]  \\ Sizes of the layers of   & [150,75,8,1]\\  &0.01 \\ \end{tabular}
    \caption{ For \textit{Amazon}  and \textit{Yelp}, we follow \cite{classif} the weight decay is set to , the learning rate is set to , the number of epochs is set to 100 and the batch size is set to .}
    \label{tab:generation_experiments}
\end{table}
For the Transformer Decoder we use  layers with  heads, the dimension of the key and value is set to  \cite{attention_is} and  the inner dimension is set to . The architectures for the models proposed by \citet{eda} and \citet{naacl_baseline} are chosen according to the original papers. For a fair comparison with \citet{naacl_baseline}, we update the language model with a BERT model, the labels are embedded in  and  fed to a single MLP layer (this dimension is chosen using the same procedure as in \cite{colombo2019affect,dinkar2020importance}). The new model is trained using AdamW \cite{adamW}.
\subsubsection{Influence of the scaling factor on the linguistic content } Table~\ref{tab:additional_example_sentences} gathers some extreme sequences generated by \geneliex\ for  ranging from  to .  No major linguistic change appears when  varies. The generated sequences  are grammatically correct and share the same polarity (positive or negative sentiment) as the input sequence. Note that for greater values of , a repetition phenomenon appears. The resulting sequences keep the label and polarity of the input sequence but repeat some words \cite{holtzman2019curious}.
\begin{table}[ht]
\centering
\resizebox{.9\textwidth}{!}{ \begin{tabular}[t]{c|p{16cm}}
\small Input	& very sloppy and slow service. when we arrived they told us to sit anywhere but all the tables were still dirty and haven't been cleaned. they didn't bother to ask if we wanted refills on our drinks. we needed an extra plate and didn't get one so my nephew decides to go up to the counter and ask for one because he's hungry. they gave our check when we were still eating. the list can go on and on. i wouldn't recommend this place. go somewhere else for faster and better service. very disappointed
\\
\small 	&very sloppy and sluggish service. when we got there, they told us to sit anywhere but all the tables were empyt full of dishes and were not cleaned at all. they didn't bother to ask if our drinks would be added. we needed an extra dish and didn't get one, so my cousin decided to go to the counter and ask one because he's hungry. they were going to watch while we were still eating. the list could go on and on. i would not recommend this place. go elsewhere for faster and better service. very very disappointed \\
\small 	&services and survivors. when he got there, he told us we were sitting everywhere but all the tables were full of dishes and we didn't wash everything. he never bothered to ask if our drinks would be added. we needed extra food and didn't get one, so my brother decided to go to the locker and ask because he was thirsty. they want to watch it while we eat. the list can be continuous and active. i would not recommend this place. go elsewhere for faster and better service. very disappointed \\
\small 	&services and survivors. when he got there, he told us that we were sitting everywhere, but all the tables were full of dishes and we didn't wash everything. he never bothered to ask if our drinks would be added. We needed more food and we didn't get it, so my brother decided to go to the locker and ask because he was thirsty. they want to watch it when we eat. the list can be continuous and active. i would not recommend this place. go faster and faster for better service. very disappointed \\\hline
\small Input	&visited today with my husband. we were in the firearms section. there were 3 employees in attendance with one customer. my husband ask a question and was ignored. he waited around for another 10 minutes or so. if it had been busy i could understand not receiving help. we left and went elsewhere for our purchases. \\
\small 	&visited today with my husband. we were in the firearms section. together with one customer there were 3 employees. my husband asked and was ignored. waited about another 10 minutes. if it was busy, i would understand that i wouldn't get help. we left and went somewhere else because of our purchases. \\
\small 	&today she visited with her husband. we were in the gun department. there were 3 employees together with one customer. my husband asked and was ignored. waited another 10 minutes. if he was busy, i would understand that i would not receive help. we went and went somewhere else because of our shopping. \\
\small 	&today, she went with her husband. we are in the gun department. there are 3 employees and one customer. my husband rejected me and ignored him. wait another minute. if he has a job at hand, i will understand that i will not get help. we went somewhere else because of our business. \\\hline
\small Input	&walked in on a friday and got right in. it was exactly what i expected for a thai massage. the man did a terrific job. he was very skilled, working on the parts of my body with the most tension and adjusting pressure as i needed throughout the massage. i walked out feeling fantastic and google eyed. \\
\small 	&walked in on a friday and got right in. it was exactly what i expected for a thai massage. the man did a terrific job. he was very skilled, working on the parts of my body with the most tension and adjusting pressure as needed throughout the massage. i walked out feeling fantastic and google eyed. \\
\small 	&climb up the stairs and get in. the event that i was expecting a thai massage. the man did a wonderful job. he was very skilled, dealing with a lot of stress and stress on my body parts. i walked out feeling lightly happy and tired. \\
\small 	&go up and up. this was the event i was expecting a thai massage. the man did a wonderful job. what this was was an expert, with a lot of stress and stress on my body parts. i walked out feeling lightly happy and tired. \\\hline
\small Input	&i came here four times during a 3 - day stay in madison. the first two was while i was working - from - home. this place is awesome to plug in, work away at a table, and enjoy a great variety of coffee. the other two times, i brought people who wanted good coffee, and this place delivered. awesome atmosphere. awesome awesome awesome. \\
\small 	&i came here four times during a 3-day stay in henderson. the first two were while i was working - from home. this place is great for hanging out, working at tables and enjoying the best variety of coffee. the other two times, i brought in people who wanted a good coffee, and it delivered a place. better environment. really awesome awesome. \\
\small 	&i came here four times during my 3 days in the city of henderson. the first two were while i was working - at home. this place is great for trying, working tables and enjoying the best variety of coffee. the other two times, i brought people who wanted good coffee, and it brought me somewhere. good environment. really amazing. \\
\small 	&i came here four times during my 3 days in the city of henderson. the first two are when i'm working - at home. this place is great for trying, working tables and enjoying a variety of the best coffees. the other two times, i bring people who want good coffee, and that brings me somewhere. good environment. very amazing.\\\hline
\end{tabular}}
\caption{Sequences generated by \geneliex\ for extreme embeddings implying label (sentiment polarity) invariance for generated Sequence.  is the scale factor. Two first reviews are negatives, two last reviews are positive.}
\label{tab:additional_example_sentences}
\end{table}


\iffalse
  \begin{table}[ht]
\centering
\begin{tabular}[t]{c|c}
\hline

\hline
\small input	&\small ( it wasn't busy either), the building was cold.  	\\\hline
\small &	\small (it was not occupied either), the building was cold. 		\\
\small 	&\small (i wasn't busy either), the building was frozen. 	\\
\small 	&\small also, the building was freezing.  	\\
\small 	&\small plus, the building was colder than ice. \\\hline
\small input &\small food quality has been horrible.
\\\hline
\small &\small 		food quality has been terrible.  	\\
\small 	&\small the quality of the food was horrible.  	\\
\small & \small 	the quality of the food has been horrible. 	\\
\small 	& \small the quality of food was terrible.  \\
\hline
\small input	&\small  overall , i like there food and the service .	\\\hline
\small &	\small 	i love food and the service. 	\\
\small 	&\small on the whole, i like food and service. 	 	\\
\small &\small 	in general, i like the food and the service.  \\
\small &\small 	in general, i like food and service. 	 \\
\hline

\small input	&\small  the desserts were a bit strange. 	\\\hline
\small &	\small 	the desserts were a little weird. 	\\
\small 	&\small the desserts were very strange. 	\\
\small &\small 	the desserts were terrible. \\
\small &\small 	the desserts were terrible.
\\
\hline
\small input	&\small  we definately enjoyed ourselves.	\\\hline
\small &	\small we enjoyed ourselves.  	\\
\small 	&\small we had a lot of fun. 		\\
\small &\small 	we've really enjoyed each other. \\
\small &\small we certainly had fun. \\
\hline
\small input &\small 	seriously killer hot chai latte. 	\\\hline
\small &\small 	-it's a real killer. 	\\
\small 	&\small he is a real killer. 	\\
\small & \small he likes to kill. 	\\
\small 	& \small i loves murders. \\
\hline
\small input	&\small all of the tapas dishes were delicious! 	 \\	\hline
\small &\small all the tapas was delicious.  	\\
\small \small	&all tapas dishes were delicious!  	\\
\small &\small	all the tapas dishes were delicious! 	\\
\small \small	&the tapas were great!  	\\
\hline
\small input	&\small  there was hardly any meat. 	 	\\\hline
\small &	\small there was almost no meat. 	\\
\small 	&\small there was practically no meat. 	\\
\small &\small 	there was almost no meat. \\
\small &\small 	there was no meat.
\\
\hline
\small input	&\small  waiter was a jerk. 	\\\hline
\small &	\small the waiter was a jerk. 	\\
\small 	&\small awaiter was a poor guy. \\
\small &\small 	waiter was an idiot. \\
\small &\small 	waiter was such an idiot. \\
\hline
\small input	&\small  i 'm not eating here! 	\\\hline
\small &	\small i don't eat here. 	\\
\small 	&\small i don't eat here! 	 \\
\small &\small 	i'm not going to eat here! 	 \\
\small &\small 	i will never going to eat here! \\
\hline
\end{tabular}
\caption{Sequences generated by \geneliex\ for extreme embeddings implying label (sentiment polarity) invariance for generated sequence.  is the scale factor.}
\label{tab:additional_example_sentences}
\end{table}

\section{A linguistic point of view of extremes}\label{sec:linguistic_appendix}
In this section, we  investigate possible linguistic interpretations 
regarding the extreme or non extreme nature of text data in the heavy-tailed representation learnt by \HTalgo. 
We consider the number of words in each sequence of  the extreme region  for increasing values of .  Figure~\ref{fig:violin_plot} displays boxplots of these quantities for 
\textit{Amazon small dataset} and \textit{Yelp small dataset}. In both cases one can notice that the median  of the number of words in extreme samples tends to increase  for increasing values of . \cccmt{que peut-on dire de ça? pourquoi à votre avis le nombre  de mots augmente avec lambda? }\addeda{on n'a pas de réponse et je ne suis pas sûre qu'il y en ait une. il serait peut etre preferable de supprimer la section D (et ce que j'ai rajouté en bleu ans la partie  3.1), si chloé  confirme qu'on ne nous reprochera pas forcéement de ne pas donner d'interprétation linguistique, tant que les performances augmentent.}\cccmt{Il faut ici discuter le tableau 7}
\cccmt{La phrase suivante est à supprimer ou à détailler. Sans avoir expliqué la méthodologie adoptée pour calculer les affects scores ce résultat n'est pas forcémenet convaincant }Further experiments through the affect scores provided by the lexicon for English words from \cite{mohammad2017word}, show that the affect score of samples in the extreme regions and in the bulk are similar. Therefore, extreme samples can not be considered as prototypical or different in terms of content.
\begin{figure}[ht]
\begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[width=\textwidth]{figures/small_datasets/boxplot_Nwords_amazon.pdf}
    \caption{}
    \label{fig:violin_plot_amazon}
    \end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}

    \includegraphics[width=\textwidth]{figures/small_datasets/boxplot_Nwords_yelp.pdf}
    \caption{}
    \label{fig:violin_plot_yelp}
\end{subfigure}
\caption{Boxplot of the number of words in each sequence of    in \textit{Amazon small dataset}  (\autoref{fig:violin_plot_amazon}) and for \textit{Yelp small dataset }  (\autoref{fig:violin_plot_yelp}). }
\label{fig:violin_plot}
\end{figure}
\fi
