\documentclass[11pt]{article}


\usepackage{graphicx,amsfonts,latexsym,color}

\usepackage{eurosym}


\bibliographystyle{plain}

\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{graphicx}

\usepackage{mathrsfs}






\newcommand{\est}{{\hfill }}
\newcommand{\one}{{\mathbf{1}}}
\newcommand{\tran}{^{\top}}

\newcommand{\zbf}{{\mathbf z}}

\newcommand{\erem}{{\hfill }}
\newcommand{\qed}{{\hfill }}
\renewcommand{\d}{\mbox {\rm d}}
\newcommand{\Dset}{{\mathbf \Delta}}
\newcommand{\diag}{\mbox {\rm diag}}
\newcommand{\tr}{\mbox {\rm Tr }}
\newcommand{\lam}{\lambda}

\newcommand{\sign}{{\mbox{\rm sgn}\,}}
\newcommand{\vc}{{\mbox{\rm VC-dim}\,}}
\newcommand{\prob}{\mbox{\rm Prob}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Pz}{\mathbb{P}_z}
\newcommand{\Pu}{\mathbb{P}_u}

\newcommand{\Ev}{\mathbb{E}}

\newcommand{\rvet}{{\mathbf r}}
\newcommand{\xvet}{{\mathbf x}}

\newcommand{\ub}{{\mathbf u}}
\newcommand{\ubar}{\bar u}
\newcommand{\xhat}{\hat{x}}
\newcommand{\yvet}{{\mathbf y}}
\newcommand{\zvet}{{\mathbf z}}
\newcommand{\wvet}{{\mathbf w}}
\newcommand{\e}{{\mathrm e}}
\newcommand{\inv}{^{-1}}
\newcommand{\beq}{}
\newcommand{\bea}{}
\newcommand{\beas}{}
\newcommand{\ba}{\begin{array}}

\newcommand{\ea}{\end{array}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\dss}{\displaystyle}
\newcommand{\Real}[1]{ { {\mathbb R}^{#1} } }
\newcommand{\Realp}[1]{ { {\mathbb R}^{#1}_+ } }

\newcommand{\Complex}[1]{ { {\mathbb C}^{#1} } }
\newcommand{\Field}[1]{ {\mathbb F}^{#1} }
\newcommand{\pre}[1]{ { {\mbox{Re}}  ({#1})} }
\newcommand{\pim}[1]{ { {\mbox{Im}}  ({#1})} }
\newcommand{\unif}[1]{ {\mathcal U}\left[{#1}\right] }
\newcommand{\ped}[1]{{_{\mathrm{#1}}}}
\newcommand{\ap}[1]{{^{\mathrm{#1}}}}

\renewcommand{\binom}[2]{\left(\!\ba{c} {#1}\\#2 \ea \!\right)}

\newcommand{\rp}{ ^{\Re} }
\newcommand{\ip}{ ^{\Im} }
\newcommand{\Van}{ {\mathcal V} }
\newcommand{\X}{ {\mathcal X} }
\newcommand{\E}[1]{ { {\mbox{E}}  \left\{{#1}\right\}} }

\newtheorem{trule}{Rule}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}

\newcommand{\simax}{ {\bar{\sigma}} }
\newcommand{\calA}{{\mathcal A}}
\newcommand{\calB}{{\mathcal B}}
\newcommand{\calC}{{\mathcal C}}
\newcommand{\calD}{{\mathcal D}}
\newcommand{\calG}{{\mathcal G}}
\newcommand{\calM}{{\mathcal M}}
\newcommand{\calN}{{\mathcal N}}

\newcommand{\calP}{{\mathcal P}}
\newcommand{\calQ}{{\mathcal Q}}
\newcommand{\calR}{{\mathcal R}}
\newcommand{\calS}{{\mathcal S}}

\newcommand{\calI}{{\mathcal I}}
\newcommand{\calE}{{\mathcal E}}
\newcommand{\calF}{{\mathcal F}}
\newcommand{\calH}{{\mathcal H}}
\newcommand{\calL}{{\mathcal L}}
\newcommand{\calX}{{\mathcal X}}
\newcommand{\K}{{\mathcal K}}
\newcommand{\calK}{{\mathcal K}}
\newcommand{\calU}{{\mathcal U}}

\newcommand{\thetahat}{{\hat{\theta}}}
\newcommand{\tetahat}{{\hat{\theta}}}
\newcommand{\Sighat}{{\hat{\Sigma}}}
\newcommand{\Thv}{{\mathbf{\Theta}}}

\newcommand{\ahat}{{\hat{a}}}
\newcommand{\thbar}{{\bar{\vartheta}}}
\newcommand{\rhat}{{\hat{r}}}
\newcommand{\phat}{{\hat{p}}}
\newcommand{\bp}{{\underline{p}}}
\newcommand{\gammahat}{{\hat{\gamma}}}
\newcommand{\calMhat}{{\hat{\calM}}}
\newcommand{\calIhat}{{\hat{\mathcal I}}}
\newcommand{\Rbar}{{\bar{R}}}
\newcommand{\Vbar}{{\bar{V}}}
\newcommand{\fro}{ {F} }
\renewcommand{\t}{ ^{\top} }
\newcommand{\LPM}{ \mbox{LPM} }
\newcommand{\brho}{ \bar{\rho} }
\newcommand{\co}[1]{ {\mbox{co}\{#1\}} }
\newcommand{\vol}[1]{{\mbox{vol}\left(#1\right)}}
\newcommand{\partiz}{ {\; \raisebox{-.5ex}{\dashbox{1}(0,10){}} \;} }\begin{document}
\author{Giuseppe C. Calafiore\thanks{Giuseppe C. Calafiore, Dipartimento di Automatica e Informatica,
Politecnico di Torino, Italy.
Tel.: +39-011-564.7071; Fax:
+39-011-564.7099. E-mail: {\tt giuseppe.calafiore@polito.it}
},
Laurent El Ghaoui\thanks{Laurent El Ghaoui, EECS and IEOR, UC Berkeley, CA, USA. E-mail: {\tt  elghaoui@berkeley.edu}},
Carlo Novara\thanks{Carlo Novara, Dipartimento di Automatica e Informatica,
Politecnico di Torino, Italy.
Tel.: +39-011-564.7019. E-mail: {\tt carlo.novara@polito.it}}
}

\title{Sparse Identification of Posynomial Models}

\date{}
\maketitle


\begin{abstract}

Posynomials are nonnegative combinations of monomials with possibly fractional and both positive and negative exponents. 
Posynomial models are widely used in various engineering design endeavors,  such as circuits, aerospace and structural design, mainly due to the fact that design problems cast in terms of posynomial objectives and constraints can be solved efficiently by means of a convex optimization technique known as geometric programming (GP). However, while quite a vast literature exists on GP-based design, very few contributions can yet be found on the problem of identifying posynomial models from experimental data.  Posynomial identification amounts to determining not only the coefficients of the combination, but also the exponents in the monomials, which renders the identification problem numerically hard.  In this draft, we propose an approach to the identification of multivariate posynomial models, based on the expansion on a given large-scale basis of monomials. The  model is then identified  by seeking coefficients of the combination that minimize a mixed objective, composed by a term representing the fitting error and a term inducing sparsity in the representation, which results in a problem formulation of the ``square-root LASSO'' type, with nonnegativity constraints on the variables. We propose to solve the problem via a sequential coordinate-descent scheme, which is suitable for large-scale implementations.

\vspace{.1cm}

\noindent
{\em Key Words: } Posynomial models, Identification, Sparse optimization, Square-root LASSO, Coordinate-descent methods.
\end{abstract}


\section{Introduction}

A posynomial model is defined by a function  of the form 

where  (the positive orthant), ,
 are coefficients, 
are vectors of exponents with , and 
is defined as 

The term  is called a {\em monomial}. Note that, while
in polynomial models the exponents  are nonnegative
integers, in posynomial models these exponents may also be negative
and/or noninteger.

Posynomial models are of great importance in many fields of technology,
ranging from structural design, network flow, optimal control (see
\cite{BePh76,Wilde78}), to aerospace system design \cite{HoAb12},
circuit design \cite{bkph05,DaGiSa03,SaRaVaKa93}, antennas \cite{BaLaDoHa10}
and communication systems \cite{Chiang05}. The interest in posynomials
is motivated by the fact that they lead to computationally efficient
geometric programming models for optimal system design, see, e.g.,
\cite{gp_67,BePh76,Wilde78}.

Despite the fact that a  consistent number of papers is available in the literature
where posynomial models and geometric programming are used for design
purposes, very few works can be found to date addressing the relevant problem
of identifying a posynomial model from experimental data; see \cite{DaGiSa03}
for such an exception. Typically, the model is  assumed known (i.e.,
the coefficients  and the exponents  are assumed
known), and then it is processed by  geometric programming to obtain an optimal design.
However, in most real-world applications, the model is {\em not}
known a priori, and has to be identified from experimental data.

Identification of posynomial models can be performed following the
standard approach used for polynomials. In this approach, an heuristic
search finalized at finding a viable model structure, i.e., a suitable
set of exponent vectors  is first carried
out. Once the exponent vector set has been chosen, the coefficients
 are estimated by means of least-squares or other convex optimization
algorithms, see, e.g., \cite{SpPiLo06,PuPi07,DaGiSa03}. A critical
issue in this approach is that the model structure search may be extremely
time consuming and in most cases leads only to approximate model structures,
see \cite{MiNoAUT04}. An alternative approach is to assume (or estimate
by means of some heuristic) a value  for the basis cardinality
, and then estimate  and  by means of
nonlinear programming algorithms. However, these kind of algorithms
are non-convex and thus do not ensure convergence to the optimal parameter
estimate. A third approach, which overcomes the issues of the other
two, consists in considering an over-parametrized model and inserting
in the optimization problem a sparsity promoting term (or constraint),
given by the -norm of the coefficient vector. This term
allows one to efficiently select the model structure and, at the same
time, to avoid the problem of overfitting. This approach is based
on the well-known LASSO (least absolute shrinkage and selection operator)
or other similar algorithms (see, e.g., \cite{Tib96,KuLoBr06,BoSePi10,NoTAC12}
for applications of the approach to identification of polynomial models).
The optimization problem is in this case convex but, due to the over-parametrization,
it typically involves a very large number of decision variables.

In this paper, we follow this latter approach: we minimize a convex
objective, defined as the sum of a regularized accuracy term based
on the -norm of the estimation residual, and a sparsity-inducing
term given by a weighted -norm of the coefficient vector.
We name this approach \emph{nonnegative regularized square-root LASSO}
or nnrsqrt-LASSO, since it is similar to LASSO but presents three
differences which may give advantages in terms of computational efficiency
and model regularity. The first one is to use in the objective function
an accuracy objective that is the square-root of the one used in LASSO.
With this choice, we obtain an a-priori and easily checkable sufficient condition
that, if satisfied for a certain monomial, guarantees that that monomial
will not appear in the representation (i.e., it has a null coefficient). 
This condition (called \emph{feature elimination} condition) can be
verified very efficiently, and can thus be used in a pre-optimization
phase to eliminate all the monomials which have very low relevance
in explaining the data. The second difference is to include an 
regularization in the accuracy term, allowing us to implicitly account for uncertainty
in the data, and to improve the numerical conditioning of the problem. 
The third difference
consists in using a weighted -norm of the coefficient vector
in place of the standard -norm. This allows for more flexibility
in problems where the entries of  have different scales. Note
that in the nnrsqrt-LASSO the variables are constrained to be nonnegative,
as required for the identification of posynomial models.

In order to solve the nnrsqrt-LASSO problem, we propose a large-scale-capable
iterative algorithm based on sequential coordinate descent.


The remainder of the paper is organized as follows. In Section \ref{sec:id_pos}, the problem
of identifying a posynomial model is introduced and then formulated
in terms of a nnrsqrt-LASSO optimization problem. In Section \ref{sec:dual},
the dual formulation of this optimization problem is developed and
the feature elimination condition is derived. Section \ref{sec:univ}
shows how the univariate nnrsqrt-LASSO optimization problem can be
solved in closed form. Based on this result, in Section \ref{sec:cd},
a sequential coordinate descent scheme is proposed, allowing us
to solve the multivariate optimization problem. The computational
aspects of the proposed scheme are also discussed in this section.
Finally, in Section \ref{sec:numes}, two numerical examples are presented.
The first one regards identification of a posynomial with negative
and non integer exponents; the second one is about identification
of a posynomial model for a NACA 4412 airfoil.



\section{Identification of posynomial models}
\label{sec:id_pos}

\subsection{Model setup}
\label{pp_id}
Consider a posynomial

where the coefficients , the exponent vectors 
and the expansion cardinality  are not known. Suppose that
a set of noise-corrupted measurements is available:

where 

and  is a noise term. The problem considered in this
paper is to estimate from these data the unknown parameters ,
, , and the cardinality .

To this end, we define an over-parametrized posynomial family

where . In real-world situations, this over-parametrization
can be obtained from the available prior information on the exponents
. For example, a certain exponent may be unknown
but it can be known to be  integer and to belong to a given interval; another
one may be known to be fractional in another interval; another one can be known to be negative,
etc.

More formally, suppose that the following prior information is available
on the exponents:

where  is a set of exponents which, on the basis of the available
prior information, can be considered reasonable for the variable .
Then, the set of exponent vectors defining the over-parametrization
(\ref{eq:model}) can be constructed as

where  denotes the Cartesian product. Note that this approach
can be adopted also if an exponent is known to belong to a continuous
(finite) interval, in which case the set  can be obtained by properly discretizing
the interval.

If the information (\ref{eq:prior1}) is correct, then 
is guaranteed to contain the true exponent vectors:


\subsection{Square-root LASSO formulation of the identification problem}
Model identification is here performed by minimizing with respect to the coefficients 
in the expansion (\ref{eq:model})
an objective function defined as the sum of an accuracy objective
and a sparsity-promoting
term, allowing us to select, in the over-parametrized family, a parsimonious model structure. 
Define
 , ,
and 

The objective we consider is of the form
\beq
f(x)\doteq\left\Vert \left[\begin{array}{c}
\Phi x-y\\
\sigma x
\end{array}\right]\right\Vert _{2}+\lambda^{\top}\left|x\right|,
\label{eq:f:obj}
\eeq
where ,  
with  (component-wise), and  denotes a vector whose entries are
the absolute values of the entries in .
We define, for notational
compactness, 

where , , denotes the -th column
of , and  is the -th vector of the standard
basis of . The objective thus becomes
\beq
f(x)\doteq \|\tilde \Phi x -\tilde y\|_2 +\lambda^{\top}\left|x\right|.
\eeq

Note that  is  a weighted
-norm. Vector  is thus a penalty factor which
quantifies the tradeoff between the  accuracy objective
 and the term ,
which is a proxy for sparsity in the solution, see \cite{Fuchs05,Tropp06,Donoho06_2,Candes06_2}.
Clearly, for  (where 
is a vector with all entries equal to one), and , the rsqrt-LASSO
problem coincides with the standard sqrt-LASSO. 
The use of the sparsity
promoting term  instead of the standard
term   allows for more flexibility, in problems
where the entries of  have different scales. 
The regularization
parameter   is introduced to improve the numerical conditioning of the problem, guaranteeing
(if ) that  has full rank, and that the  term of the objective remains differentiable for all .

We hence consider the following two optimization problems, which
we name regularized square-root LASSO (rsqrt-LASSO)

and nonnegative regularized square-root LASSO (nnrsqrt-LASSO)

where  (the
inequality is component-wise). The first model can be used for polynomial model identification, and the second one for posynomial model identification (the focus in this paper is on this latter case).

As already mentioned, the solutions of the optimization problems (\ref{eq:rsqrtLASSO:primal})
and (\ref{eq:nnrsqrtLASSO:primal}) tend to be sparse, i.e., to have only a few non-zero components. This important feature is produced
by the  term, which is able to select among the large set
of monomials only those which are relevant to explain the data.
Indeed, the -norm is the convex envelope of the 
quasi-norm, a quantity defined as the number of vector non-zero elements,
which is commonly used to measure vector sparsity. 
Minimizing
the -norm allows one to approximately minimize the  quasi-norm,
and thus to maximize the coefficient sparsity \cite{Fuchs05,Tropp06,Donoho06_2,Candes06_2}.
While the  quasi-norm is non-convex and its minimization
is a NP-hard problem, the -norm is convex and its minimization
can be performed quite efficiently. Conditions under which the 
minimization problem provides a maximally sparse solution, i.e., a
solution of the corresponding  minimization problem, are
given, e.g., in \cite{NoTAC12}. Note that the sparsity property is
important also to allow an efficient implementation on real-time processors,
which may have limited memory and computational capacity \cite{NoFaMiAUT13}.

\begin{remark}\rm Notice that the cardinality  of the set , and hence the dimension of the decision vector , may be
very large, since it is given by the product of the cardinalities of , for .
For this reason, although the two previous problems are standard convex optimization problems, they may
not be practically solved using standard interior-point methods for convex optimization. Actually, in some cases, even just storing in memory the data matrix  may be unfeasible due to dimensionality issues.\qed

In the following sections, we describe a simple scheme for solving both the unconstrained and the constrained versions
of the regularized sqrt-LASSO problem, based on a two-phase procedure. 
In the first phase, we apply a feature elimination step to eliminate a-priori all variables that are guaranteed to be zero at optimum, thus possibly reducing the dimensionality of the problem. In the second phase, we apply a coordinate-descent scheme to the reduced problem, in order to find the optimal solution. This latter phase is based on the fact that we can find in ``closed form'' an optimal solution to the univariate restriction of the above problems. 
\end{remark}

\vspace{.2cm}
We shall assume throughout that , since for  the optimal solution of both problems 
(\ref{eq:rsqrtLASSO:primal}), (\ref{eq:nnrsqrtLASSO:primal})   is trivially .

\section{Dual  formulations and feature elimination}
\label{sec:dual}

We next derive dual formulations of the rsqrt-LASSO and nnrsqrt-LASSO problems,
and then show how a feature elimination condition is obtained from these dual formulations. 

\subsection{Dual of the rsqrt-LASSO problem}
We here derive a dual formulation for problem (\ref{eq:rsqrtLASSO:primal}).  To this end, we first recall the definition of dual norm: if  is a vector norm, then the corresponding dual norm is defined as

It is well known, for instance, that the dual of the  norm is the  norm itself, and that
the dual of the  norm is the  norm, and vice versa. Therefore,
\beas
\|\tilde \Phi x -\tilde y\|_2 &=& \max_{\|u\|_2\leq 1} \, u\tran (\tilde \Phi x - \tilde y).
\eeas
Also, one can readily verify that

We can  thus rewrite problem (\ref{eq:rsqrtLASSO:primal}) as
\beas
p^*  =\min_{x\in\Real{n}} & \displaystyle{\max_{\|u\|_2\leq 1,    |v| \leq \lam}} \;
u\tran (\tilde \Phi x - \tilde y) +  v\tran x.
\eeas
Then, a standard saddle-point result (see, for instance, Sion's theorem, \cite{Komiya:88,Sion:58}), 
prescribes that we may exchange the order of min and max in the previous expression without changing the optimal value, whence
\beas
p^*  & = & \displaystyle{\max_{\|u\|_2\leq 1,   |v| \leq \lam}} \;  \displaystyle{\min_{x\in\Real{n}}} \;
u\tran (\tilde \Phi x - \tilde y)  +  v\tran x.
\eeas
Notice further that the infimum over  of the term  is , unless the coefficient
 is zero, hence
\beas
p^*  = \displaystyle{\max_{u,v}} & 
-u\tran  \tilde y \\
\mbox{s.t.:} & \tilde \Phi\tran u   +  v = 0 \\
&\|u\|_2\leq 1 \\
& |v| \leq \lam.
\eeas
Eliminating the  variable, we obtain the following  formulation
for the dual of  problem (\ref{eq:rsqrtLASSO:primal})  
 \bea
p^*  = \displaystyle{\max_{u}} & 
-u\tran  \tilde y   \label{eq:rsqrtLASSO:dual} \\
\mbox{s.t.:} & \|u\|_2\leq 1 \nonumber \\
& |\tilde \phi_i\tran u | \leq \lam_i , &  i=1,\ldots, n. \label{eq:rsqrtLASSO:dual_cinf} 
\eea


\subsection{Dual of the nnrsqrt-LASSO problem}
The derivation of the dual for the nnrsqrt-LASSO    problem (\ref{eq:nnrsqrtLASSO:primal})
follows  similar lines, noticing that,
for , we have , hence 
\beas
p^*_+  & = & \displaystyle{\max_{\|u\|_2\leq 1}} \;  \displaystyle{\min_{x\geq 0}} \;
u\tran (\tilde \Phi x - \tilde y) +  \lam\tran x,
\eeas
and the infimum over  of the term  is , unless 
, thus
\bea
p^*_+  = \displaystyle{\max_{u}} & 
-u\tran  \tilde y  \label{eq:rsqrtLASSO:dual+} \\
\mbox{s.t.:} & \|u\|_2\leq 1 \nonumber \\
& \tilde \phi_i\tran u   +  \lam_i \geq  0, & i=1,\ldots,n. 
 \label{eq:rsqrtLASSO:dual_cinf+} 
\eea


\subsection{Safe feature elimination}
\label{sec:safelim}
In this section we analyze the dual formulations of problems (\ref{eq:rsqrtLASSO:primal}), (\ref{eq:nnrsqrtLASSO:primal}), in order to derive
a simple sufficient condition that permits to predict when an entry
 is zero  at optimum, and hence to
eliminate a priori some features (i.e., columns of ) from the problem.
This type of condition, first introduced by \cite{ElgViRa:12} in the context of the standard LASSO problem, is named {\em safe feature elimination}.
Observe that
\beas
\max_{ \|u\|_2\leq 1}\, |\tilde \phi_i\tran u | &=& \|\tilde\phi_i\|_2 = 
\left\|\left[\ba{c}\phi_i \\ \sigma e_i \ea\right]\right\|_2.
\eeas
Therefore, if for some  it holds that

then the corresponding constraint
in (\ref{eq:rsqrtLASSO:dual_cinf}), 
as well as in (\ref{eq:rsqrtLASSO:dual_cinf+}),
will certainly be satisfied with strict inequality, that is, it will be {\em inactive}
at the optimum.
This means that  it can be safely eliminated from the dual optimization problem, without changing the optimal objective value. Defining

we thus have that
 \bea
p^*  = \displaystyle{\max_{u}} & 
-u\tran  \tilde y   \label{eq:rsqrtLASSO:dual_red} \\
\mbox{s.t.:} & \|u\|_2\leq 1 \nonumber \\
& |\tilde \phi_i\tran u  | \leq \lam_i , &  i\in\calF(\lam) \label{eq:rsqrtLASSO:dual_cinf_red} \nonumber ,
\eea
which is the dual of the ``reduced'' primal problem
\bea
p^*  = \min_{\xi} &
\|\tilde \Phi_{\calF(\lam)} \xi  - \tilde y 
\|_2
 + \lam\tran  |\xi|,
\label{eq:rsqrtLASSO:primal_red}
\eea
where  is a matrix containing by columns vectors , ,
and  is a decision variable vector, having dimension equal to the cardinality of .
In other words, the features  in the primal problem (\ref{eq:rsqrtLASSO:primal})
corresponding to indexes  in the set   complementary to , defined as

are certainly zero at the optimum, that is
\beq
\|\phi_i\|_2^2 + \sigma^2 < \lam_i^2  \quad \Rightarrow \quad x_i^* = 0.
\label{eq:feature-elim}
\eeq
Similarly,  we have that
 \bea
p^* _+ = \displaystyle{\max_{u}} & 
-u\tran  \tilde y   \label{eq:nnrsqrtLASSO:dual_red} \\
\mbox{s.t.:} & \|u\|_2\leq 1   \nonumber \\
& \tilde \phi_i\tran u   + \lam_i \geq 0, &  i\in\calF(\lam) \label{eq:nnrsqrtLASSO:dual_cinf_red} \nonumber ,
\eea
is the dual of  the ``reduced'' primal problem
\bea
p^*_+  = \min_{\xi\geq 0} & \|\tilde \Phi_{\calF(\lam)} \xi  - \tilde y \|_2+ \lam\tran  |\xi|.
\label{eq:nnrsqrtLASSO:primal_red}
\eea

\subsubsection{When is  optimal?}
Point   is optimal 
 for problem (\ref{eq:rsqrtLASSO:primal}) if and only if , which is equivalent to   being optimal (hence feasible) for the dual problem. This happens if and only if

that is, since , , if an only if

Similarly, point   is optimal 
 for problem (\ref{eq:nnrsqrtLASSO:primal}) if and only if , which is equivalent to  being optimal (hence feasible) for the dual problem, which happens if and only if

or, equivalently,



\section{Univariate solution of rsqrt-LASSO and  nnrsqrt-LASSO}
\label{sec:univ}

Consider the following  rsqrt-LASSO problem with a single scalar variable :

where ,  ,  are given, and  is a vector of all zeros, except for an entry in generic position , which is equal to one, and correspondingly we postulate that
, thus it holds that .
We set for convenience 
\beq
\tilde\phi \doteq  \left[\ba{c}
\phi \\ \sigma e
\ea\right], \quad \tilde y \doteq \left[\ba{c}
y \\ \xi
\ea\right].
\label{eq:tildephiy}
\eeq
Thus, the problem rewrites to
\beq
\min_{x\in\Real{}}\,  f(x)\doteq
\|
\tilde \phi x - \tilde y \|_2 +  \lam |x|.
\label{eq:univariate_sqrtLASSO}
\eeq
We  assume that  and
,
otherwise the optimal solution is simply . Let us define

which corresponds to the solution of the problem for .
The following theorem holds.

\begin{theorem} 
\label{prop:univariate_sqrtLASSO}
Consider problem (\ref{eq:univariate_sqrtLASSO}), with , , .
\ben
\item  is an optimal solution for (\ref{eq:univariate_sqrtLASSO}) if and only if

(notice, in particular, that if , then the above condition is certainly satisfied, hence ).
\item If  (hence  ), then 
the optimal solution of (\ref{eq:univariate_sqrtLASSO}) is given by
\beq
x^* = x\ped{ls} -  \sign (x\ped{ls}) \frac{\lam }{\|\tilde \phi\|_2^2}  \sqrt{\frac{\|\tilde \phi\|_2^2 \|\tilde y\|_2^2 - (\tilde \phi\tran \tilde y)^2}
{\|\tilde \phi\|_2^2- \lam^2}}.
\label{eq:univariate_sqrtLASSO_nzsol}
\eeq
\een
\end{theorem}

\noindent
{\bf Proof.}
The problem is convex but nonsmooth, hence we write the optimality conditions in terms of the subdifferential of the objective:

where
\beas
\partial  \|\tilde \phi x - \tilde y\|_2 &=& \left\{\displaystyle{ \ba{ll} 
\displaystyle{ \frac{\tilde \phi\tran (\tilde \phi x -  \tilde y)}{\|\tilde \phi x - \tilde y\|_2 }} & \mbox{if } \tilde \phi x - \tilde y\neq 0 \\
\{\tilde \phi\tran g:\; \|g\|_2\leq 1\} &  \mbox{if } \tilde \phi x - \tilde y= 0 ,
\ea}\right. \\
\partial  |x| &=& \left\{\ba{ll} 
\sign(x) & \mbox{if } x \neq 0 \\
\{v:\;  |v| \leq 1\}  & \mbox{if } x= 0.
\ea\right.
\eeas
For point 1.\ we thus check under what conditions  is contained in the subdifferential of  at , that is

Since  the term  may take any value in the interval , it follows that the above condition is satisfied if and only if
, which proves the first part of the theorem.
Also, since by the Cauchy-Schwartz inequality it holds that

it is clear that  implies , hence the optimal solution is certainly zero when .

\vspace{.2cm}
Consider next the case when the optimal solution is nonzero, i.e., when ,
thus . We initially assume for simplicity that  and  are not collinear, so that
 for all ; later we show that the derived solution is still valid if this assumption is lifted.
With this assumption, and since , we have that

that is, since , for
\beq
\tilde \phi\tran (\tilde \phi x -  \tilde y) = -\lam  \|\tilde \phi x - \tilde y\|_2\sign(x).
\label{eq:univariate_sqrtLASSO_rooteq}
\eeq
All solution to this equation are also solutions of the squared equation
\beq
(\tilde \phi\tran \tilde \phi x - \tilde \phi\tran \tilde y)^2 = \lam^2  \|\tilde \phi x - \tilde y\|_2^2,
\label{eq:univariate_sqrtLASSO_rooteqsq}
\eeq
which is a quadratic equation in , equivalent to:

The roots of this equation are in

Observe that the term under the square root is nonnegative, since
\beas
\delta \doteq x\ped{ls}^2 - \frac{(\tilde \phi\tran \tilde y)^2-\lam^2\|\tilde y\|_2^2}
{\|\tilde \phi\|_2^2(\|\tilde \phi\|_2^2-\lam^2) }&=&
\frac{(\tilde \phi\tran \tilde y)^2}{\|\tilde \phi\|^4} - \frac{(\tilde \phi\tran \tilde y)^2-\lam^2\|\tilde y\|_2^2}
{\|\tilde \phi\|_2^2(\|\tilde \phi\|_2^2-\lam^2)}  \\
&=& \frac{\lam^2}{\|\tilde \phi\|_2^2}\cdot \frac{\|\tilde \phi\|_2^2\|\tilde y\|_2^2 - (\tilde \phi\tran \tilde y)^2}
{\|\tilde \phi\|_2^2(\|\tilde \phi\|_2^2-\lam^2) },
\eeas
where, under the conditions of point 2., , and 
, by the Cauchy-Schwartz inequality.
Further,   is smaller in magnitude than , since
the condition 
implies that .
It follows that the sign of 
is the same sign of  (since adding  to 
cannot change its sign).
Then, plugging  into equation (\ref{eq:univariate_sqrtLASSO_rooteq}), we have 
the left-hand side

and the right-hand side

Thus, sign consistency is obtained by choosing the solution with ``+'' when  is negative, and
with ``-'' when  is positive. In conclusion, the unique solution to eq.\ (\ref{eq:univariate_sqrtLASSO_rooteq}) is given by

which is the expression we wished to prove.

It only remains to be proved that the above expression is still valid also when  and  are collinear.
In this case, since , eq.\ (\ref{eq:univariate_sqrtLASSO_nzsol}) gives , and
we have that . Let us check that this solution is indeed optimal. The subdifferential of 
at  such that  is

and we see that  if , which is indeed the condition under which
the expression (\ref{eq:univariate_sqrtLASSO_nzsol}) for  holds.
\qed


\subsection{Univariate solution of nnrsqrt-LASSO}
The solution of the univariate nnrsqrt-LASSO problem in the scalar variable 
\beq
\min_{x\geq 0}\,  f(x)\doteq
\|\tilde \phi x - \tilde y \|_2 +  \lam |x|,
\label{eq:univariate_sqrtLASSO+}
\eeq
can be readily obtained from the solution of the corresponding unconstrained problem (\ref{eq:univariate_sqrtLASSO}), by the following reasoning. Since (\ref{eq:univariate_sqrtLASSO+}) is a convex optimization problem in one variable and one linear inequality constraint, its optimal solution is either on the boundary of the feasible set (in this case, at ), or it coincides with the solution of the unconstrained version of the problem.
Thus, we solve the unconstrained problem  (\ref{eq:univariate_sqrtLASSO}): if this solution is nonnegative, then it is also the optimal solution to  (\ref{eq:univariate_sqrtLASSO+}); if it is negative, then the optimal solution to (\ref{eq:univariate_sqrtLASSO+}) is . Since the sign of the solution of  (\ref{eq:univariate_sqrtLASSO}) is simply the sign of , we can state the following theorem.

\begin{theorem} 
\label{prop:univariate_sqrtLASSO+}
Consider problem (\ref{eq:univariate_sqrtLASSO+}), with , , .
\ben
\item  is an optimal solution for (\ref{eq:univariate_sqrtLASSO+}) if and only if

\item Otherwise, the optimal solution of (\ref{eq:univariate_sqrtLASSO+}) is given by
\beq
x^* = x\ped{ls} - \frac{\lam}{\|\tilde \phi\|_2^2}\sqrt{\frac{
\| \tilde \phi\|_2^2 \| \tilde y \|_2^2 - (\tilde  \phi\tran \tilde y )^2}
{\|\tilde \phi\|_2^2  -\lam^2 }}
.
\label{eq:univariate_sqrtLASSO+_nzsol}
\eeq
\een
\end{theorem}



\begin{remark}\rm
For the specific structure of  and  in (\ref{eq:tildephiy}), we have that

and the solutions in theorems~\ref{prop:univariate_sqrtLASSO} and \ref{prop:univariate_sqrtLASSO+} can be expressed accordingly in terms of
, 
, ,  , and , .
In particular, the condition for  being optimal becomes

which, in particular, is satisfied if .\newline 
 Notice further that   for , since we assumed , and that,
 for ,  also for , since the -th entry of  is zero by definition.
Therefore, for , the -norm part of the objective is always nonzero, and hence differentiable.\qed
\end{remark}



\section{Sequential coordinate descent scheme}
\label{sec:cd}

We next outline a sequential coordinate-descent scheme for the rsqrt-LASSO problem (\ref{eq:rsqrtLASSO:primal}).
Suppose all variables , , are fixed to some numerical values, and we wish
to minimize the objective in (\ref{eq:rsqrtLASSO:primal}) with respect to the scalar variable . We have that
\beas
f_i(x_i) &\doteq & \|\sum_{j=1}^n 
\tilde  \phi_j  x_j - 
\tilde y\|_2 + \sum_{j=1}^n \lam_j|x_j| \\
&=& 
 \|\tilde \phi_i x_i - 
\tilde y(i) \|_2 + \lam_i|x_i| + \sum_{j\neq i} \lam_j|x_j|,
\eeas
where we defined 
.
We thus have that

where  the minimizer  is readily computed by applying Theorem~\ref{prop:univariate_sqrtLASSO}.

A
 sequential coordinate-descent scheme  works by updating the variables  sequentially, according to
the above univariate minimization criterion. The scheme of the algorithm is as follows.
\ben
\item Initialize  (an -vector of zeros), ;
\item For , let

\item If stopping criterion is met, finish and return , else set , and goto 2.
\een
The detailed data management involved in applying this scheme to our specific problem is described in Section~\ref{sec:cd-iterate}.

\begin{remark}\rm 
As a stopping criterion, one may use a standard check on sufficient progress in objective reduction, or the approach described in Section~\ref{sec:dualbound}, based on the evaluation of a lower bound on the duality gap. \qed
\end{remark}

\begin{remark}\rm 
Observe that, due to Theorem~\ref{prop:univariate_sqrtLASSO}, all variables  for which
 are {\em never} updated by the algorithm, i.e., they remain fixed at their initial zero value.
The inner loop on  can thus be sped up by considering only the indices  such that
, which can be determined a priori (feature elimination). \qed
\end{remark}

\begin{remark}\rm 
The same coordinate-descent scheme can be
used also for solving the nnrsqrt-LASSO problem  (\ref{eq:nnrsqrtLASSO:primal}), by using the result in Theorem~\ref{prop:univariate_sqrtLASSO+}
for updating the -th coordinate. \qed
\end{remark}

Convergence of the proposed scheme is established in the following theorem, which is a
direct consequence of a result in \cite{Tseng:01}.

\begin{theorem}[Convergence]
For , ,  the sequential coordinate descent algorithm converges
to an optimal point,
for both the rsqrt-LASSO and the nnrsqrt-LASSO problems.
\end{theorem}

\noindent
{\bf Proof.}
	The function  in (\ref{eq:f:obj}) that we minimize using coordinate descent is convex and composite:
	
	where  are convex and nonsmooth. 
	In the unconstrained case, we have . The constrained case, where , can  also be tackled as an unconstrained one, by considering
	, where  is equal to zero if
	 and it is  otherwise. 	
	Further, the function  is convex and, for  and , 
	it is differentiable over all .
	Since the objective we minimize satisfies the hypotheses of
	Theorem~5.1 in \cite{Tseng:01}, convergence 
	of the sequential coordinate descent algorithm to an optimal point
	 is guaranteed for both the rsqrt-LASSO and the nnrsqrt-LASSO problems.
	  \qed


\subsection{Dual-bound based stopping criterion}
\label{sec:dualbound}
Inspecting the primal and dual problems (\ref{eq:rsqrtLASSO:primal}), (\ref{eq:rsqrtLASSO:dual}), we see that if
 is primal optimal, then the dual-optimal variable  must be

This suggests considering,
for the candidate solution  at iteration  of the algorithm,  an associated vector

where

Such  is, by construction, feasible for the dual problem  (\ref{eq:rsqrtLASSO:dual}), hence 

is a lower bound on the primal optimal value , that is ,
where .
As  converges to ,  should converge to  and  to .
Hence, if at iteration  it holds that

we can terminate the algorithm with a solution  that guarantees -suboptimality.

An analogous approach can be followed for determining a dual lower bound for the 
nnrsqrt-LASSO problem (\ref{eq:nnrsqrtLASSO:primal}). The only difference is in the choice of , which is now given by


\subsection{Data management and cost per iteration}
\label{sec:cd-iterate}
We next analyze in more detail 
the data management and
the computational cost per iteration of the coordinate-descent scheme.

\subsubsection{Variable update}
Suppose we have a current value of  
and we want to update the -th coordinate of . Suppose further that the following quantities are available:
\beas
h &\doteq & \tilde\Phi \tran r  \\
c &\doteq & \|r\|_2^2,
\eeas
where 

is the current value of the residual vector (as we shall see, we do not need to store : only  and  need be updated).
We set up the univariate minimization problem
 
where 
\beas
\tilde y(i) &=& \tilde y -\sum_{j\neq i} \tilde\phi_j x_j = \tilde \phi_i x_i - (\tilde\Phi x-\tilde y) \\
&=& \tilde \phi_i x_i - r.
\eeas
Notice that  all we need in order to compute the optimal coordinate , by applying Theorem~\ref{prop:univariate_sqrtLASSO} (or Theorem~\ref{prop:univariate_sqrtLASSO+}, in the nonnegative constrained case) is the following data:
\beas
\tilde\phi_i\tran \tilde y(i) &=& \|\tilde \phi_i\|_2^2 x_i - h_i\\
\|\tilde y(i) \|_2^2 &=& \|\tilde \phi_i\|_2^2 x_i^2 + c - 2x_i h_i.
\eeas
 Therefore, we find the optimal , and we update the solution  to

where . Also, we update the data necessary for the next iteration. Since
\beas
r_+ &\doteq &  \tilde \Phi x_+ -\tilde y  = r + \tilde\phi_i\delta_i,
\eeas
we have that
\beas
c_+ &\doteq & \|r_+\|_2^2 = c + \|\tilde\phi_i\|_2^2\delta_i^2 + 2\delta_i h_i, \\
h_+ &\doteq & \tilde\Phi \tran r_+ =  h + \tilde\Phi \tran \tilde\phi_i\delta_i.
\eeas
Then, we let ,  , ,  and iterate.
The whole process is initialized with , , .

\subsubsection{Storage and computational cost per iteration}
Let us define the {\em kernel} matrix  and the projected response vector 
\beas
\tilde K &\doteq & \tilde\Phi\tran \tilde \Phi  = K + \sigma^2 I_n \\
 q &\doteq & \tilde\Phi\tran \tilde y  = \Phi\tran y,
\eeas
where
.
Initialization of the  coordinate descent method  requires
, and , as described previously. 


For updating the -th variable, the method does not necessarily need to store or access the whole kernel matrix
. Indeed, computing the -th optimal update just requires access to , and  operations. Then, the update of the  vector requires access to the -th column
of , and then  operations for computing .

The storage requirement of the method is thus essentially given by keeping in memory  
and , so it is , if  is not stored. Evaluating the -th column of the kernel matrix requires  operations, unless the values of the kernel can be obtained directly (i.e., without actually performing the inner products ), as it is the case, for instance, for polynomial kernels.

\section{Numerical examples}
\label{sec:numes}

\subsection{Example 1: posynomial with negative and non-integer exponents}

\label{sub:toy_example}

As a first numerical experiment, we have considered the problem of
identifying the posynomial function 
defined as

which contains monomials with negative
and non-integer exponents. 

A set 
of  input-output data points has been generated from \eqref{eq:psi_es2},
for randomly chosen values of  in the interval ,
for . The sequence  has been generated as a Gaussian
noise with zero mean and a noise-to-signal standard deviation ratio
of 1\%. 

The exponent sets 

have been assumed. For  and for this exponent set, 
results to be a  matrix.

We set ,
, and . It has been
observed in several numerical experiments that this choice is effective
to penalize monomials with large powers. We considered several values
of , logarithmically spaced in the interval .
For each value of , the optimization problem (\ref{eq:nnrsqrtLASSO:primal})
has been solved using the approach described in Sections \ref{sec:dual}-\ref{sec:cd}.
Then, the following quantities have been recorded: 
\begin{itemize}
\item the cardinality (i.e., the number of nonzero entries) of the solution
 of the optimization problem (\ref{eq:nnrsqrtLASSO:primal}); 
\item the relative error . 
\end{itemize}
\noindent Figure~\ref{pareto:figure} shows the Pareto trade-off curve,
reporting the  values versus the solution cardinality. Based
on this curve, the parameter value  has been chosen,
since providing the best trade-off between the model complexity (measured
by the cardinality of ) and its accuracy (measured by the relative
error ). The model identified with this value of  is
given by

It can be noted that the identification algorithm has been able to
recover the ``true'' monomials and to accurately estimate the coefficients
of these monomials. The model is compared with the ``true'' posynomial
in Figure \ref{sezioni}, where some sections of the two functions
are shown.

\begin{figure}
\centering\includegraphics[scale=0.5]{pareto2}

\caption{Example 1. Pareto trade-off curve.\label{pareto:figure}}
\end{figure}


\begin{figure}
\centering\includegraphics[scale=0.5]{sezioni}

\caption{Example 1. Comparison between the ``true'' posynomial and the identified
model. Top: section for ,  and .
Bottom: section for ,  and .
\label{sezioni}}


\end{figure}


In order to validate the identified model (supposing that the ``true''
function was not known), a new set of  data has been randomly
generated from (\ref{eq:psi_es2}), where 
the same intervals for  and the same type of noise and have been considered (although using noise free data for the validation would have allowed
us to assess the quality of the model more accurately, here we used
noise corrupted data to be closer to a real situation). The relative
error obtained by the model on this validation data is .

Then, two Monte Carlo simulations have been performed, each consisting
of 100 repetitions of this data generation-identification-validation
procedure. Noise-to-signal standard deviation ratios of 1\% and 3\%
have been considered in the two simulations, respectively. In the
first one, the nnsqrt-LASSO algorithm has been able to find the ``true''
monomials 97\% of times; the average relative error 
on the validation data has been obtained. In the second one, the ``true''
monomials have been recovered 67\% of times; the average relative
error  on the validation data has been obtained.

We next discuss a few relevant aspects related to the identification
process.

The safe feature elimination discussed in Section \ref{sec:safelim},
reduced the number of columns of  from  to  (average
value obtained in the two Monte Carlo simulations), suggesting that
this elimination phase can be useful in practical large-scale problems.

The time taken for applying the safe elimination and solving the optimization
problem (\ref{eq:nnrsqrtLASSO:primal}) with the approach described
in Sections \ref{sec:dual}-\ref{sec:cd} is about  seconds on
a PC with a Core i7 processor and a RAM memory of 8GB (average time
obtained in the two Monte Carlo simulations).

\subsection{Example 2: identification of airfoil drag force}

As a second numerical experiment, we have considered the problem of identifying
a posynomial model for the drag force (per unit length) of a NACA
4412 airfoil. 

This force can be evaluated as a function of the air flow
density , the wing chord , the incidence angle 
and the flow velocity , that is 

where . No analytical expression is available for this function. The values 
can be obtained via simulations based on CFD (computational fluid dynamics),
by integration of the Navier-Stokes equations. Each evaluation is
numerically very costly, thus it is of interest to obtain a simple
model for , to be used, for instance, in a later stage of
system evaluation or design. 

In this example, we identified a posynomial model for the drag force
of the airfoil, from data obtained from the CFD simulations. The posynomial
form is important since it allows the application of geometric programming
algorithms, which in turn allow for efficient optimization of the
airfoil characteristics, see, e.g., \cite{HoAb12}.


A set 
of  input-output data points has been obtained, for randomly
chosen values of , ,  and  in the intervals
shown in Table~\ref{tab_param}. 

\begin{table}[htb]
\centering

\begin{tabular}{|c|c|c|c|}
\hline 
PARAM.  & Minimum  & Maximum  & Dimension \tabularnewline
\hline 
  & 0.039  & 1.2250  &  \tabularnewline
\hline 
  & 0.1  & 1  &  \tabularnewline
\hline 
  & -5  & 10  &  \tabularnewline
\hline 
  & 0  & 40  &  \tabularnewline
\hline 
\end{tabular}\caption{Parameter intervals considered in the CFD simulations.}


\label{tab_param} 
\end{table}


The exponent sets 

have been assumed, following the approach described in Section~\ref{pp_id}.
This choice has been made after a preliminary trial and error process.
For  and for the exponent sets (\ref{eq:exp_set}), 
results to be a  matrix.

We set for simplicity , , and
we considered several values of , logarithmically spaced
in the interval . For each value of , the optimization
problem (\ref{eq:nnrsqrtLASSO:primal}) has been solved using the
approach described in Sections~\ref{sec:dual}-\ref{sec:cd}. For
each value of , the following quantities have been recorded:
\begin{itemize}
\item the cardinality (i.e., the number of nonzero entries) of the solution
 of the optimization problem (\ref{eq:nnrsqrtLASSO:primal}); 
\item the relative error .
\end{itemize}
\noindent Figure~\ref{pareto:figure2} shows the Pareto trade-off curve,
reporting the  values versus the solution cardinality. Based
on this curve, the parameter value  has been chosen,
since providing the best trade-off between the model complexity (measured
by the cardinality of ) and its accuracy (measured by the relative
error ).

\begin{figure}
\centering
\includegraphics[scale=0.5]{pareto}
\caption{Example 2. Pareto trade-off curve.}
\label{pareto:figure2} 
\end{figure}


In order to verify the reliability of an identified model, we carried
out a leave-one-out (LOO) cross validation, on a subset of the available
data. In particular, we used for cross validation data points 
that lie within  from the boundary of the the hyperrectangle
defining the minimum and maximum deviation for each parameter (as
defined in Table \ref{tab_param}). This was done to avoid points
near the boundary of the  domain, which are too close to the non-explored
region.

For each pair  in the LOO validation set, a posynomial
model has been identified from the data set .
This model has then been tested on the single datum ,
and the relative error 
has been evaluated, where  is the output provided by
the model, and  is the Euclidean norm of the vector
with entries , for  in the validation set. The accumulated
relative error is given by . In our
experiment, with , we obtained . This value
appears to be quite low: a model identified using the proposed approach
is able to approximate the unknown function quite accurately, even
if only  points are used to explore its 4-dimensional domain.

The same LOO validation has been performed considering 
and , obtaining  and , respectively.
The model identified using  has thus the most advantageous
trade-off between complexity and accuracy. This model is given by

where , ,
, and 
(the units of these coefficient can be inferred from Table \ref{tab_param}).
It is interesting to note that a dependence of the drag force on the
square velocity has been found by the algorithm and this result is
consistent with the well-known drag equation. No significant dependence
on the incidence angle  has been observed. A possible interpretation
for this latter result is that the range considered for 
is not sufficiently large compared to the ranges considered for ,
 and  (see Table \ref{tab_param}) and, consequently, the
force variations due to  are negligible with respect to those
produced by the other three parameters.

We next discuss a few relevant aspects related to the identification
process.

The safe feature elimination discussed in Section \ref{sec:safelim},
reduced the number of columns of  from  to  (this
latter is the average value obtained in the LOO validation), suggesting
that this elimination phase can be quite useful in practical large-scale
problems.

The time taken for applying the safe elimination and solving the optimization
problem (\ref{eq:nnrsqrtLASSO:primal}) with the approach described
in Sections \ref{sec:dual}-\ref{sec:cd} is about  seconds
on a PC with a Core i7 processor and a RAM memory of 8GB (average
time obtained in the LOO validation).

{\bf Acknowledgments:}
We thank Valentina Dolci (Politecnico di Torino) for providing us
with the fluid dynamic simulation data used in the example.

\section{Conclusions}

An approach for the identification of posynomial models has been presented in this paper, based on the solution of a nonnegative regularized square-root LASSO  problem. In this approach, a large-scale expansion of monomials is considered and the model is identified by seeking coefficients of the expansion that minimize an objective composed by a fitting error term and a sparsity promoting term. A sequential coordinate-descent scheme has been proposed to solve the nnrsqrt-LASSO problem. This scheme guarantees convergence to a minimum of the objective function and is suitable for large-scale implementations. Two numerical examples have finally been shown to demonstrate the effectiveness of the approach. The first one regards identification of a posynomial with negative and non integer exponents; the second one is about identification of a posynomial model for a NACA 4412 airfoil.

\bibliographystyle{agsm}
\bibliography{poly_posy_nomial,references_1,mybiblio}

\end{document}
