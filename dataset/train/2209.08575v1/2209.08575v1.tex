\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}








\usepackage[final]{neurips_2022}



\usepackage{url}            \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[dvipsnames,table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{booktabs}

\usepackage{overpic}


\usepackage{multirow}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\usepackage{makecell}
\usepackage{wrapfig}


\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\text{\ding{55}}}




\usepackage{silence}
\hbadness=10000 \vbadness=10000 

\graphicspath{{./Images/}}

\newcommand{\gmh}[1]{\textcolor{blue}{#1}}
\newcommand{\lcz}[1]{\textcolor{green}{#1}}
\newcommand{\gzy}[1]{\textcolor{orange}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\hou}[1]{\textcolor{RoyalBlue}{[QB:#1]}}

\usepackage[pagebackref=false,breaklinks,colorlinks,linkcolor=blue,citecolor=RoyalBlue]{hyperref}

\def\etal{{\em et al.}}
\def\aka{\emph{a.k.a.,~}}
\def\wrt{\emph{w.r.t.,~}}
\def\sArt{state-of-the-art~}
\def\ie{\emph{i.e.,~}}
\def\eg{\emph{e.g.,~}}
\def\etc{{\em etc}}
\def\modelname{SegNeXt}

\newcommand{\addImgs}[1]{}


\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\tabref}[1]{Tab.~\ref{#1}}
\newcommand{\secref}[1]{Sec.~\ref{#1}}




\title{
SegNeXt: Rethinking Convolutional Attention Design  \\
for Semantic Segmentation
}




\author{Meng-Hao Guo$^{1}$ \quad Cheng-Ze Lu$^{2}$ \quad Qibin Hou$^{2}$ \quad Zheng-Ning Liu$^{3}$ \\ 
  \textbf{Ming-Ming Cheng$^{2}$ \quad Shi-Min Hu$^{1}$} \\ \\
$^{1}$BNRist, Department of Computer Science and Technology, Tsinghua University \\
  $^{2}$TMCC, CS, Nankai University \\
  $^{3}$Fitten Tech, Beijing, China \\
}




\begin{document}

\maketitle

\begin{abstract}

We present SegNeXt, a simple convolutional network architecture for
semantic segmentation.
Recent transformer-based models have dominated the field of 
semantic segmentation due to the efficiency of self-attention 
in encoding spatial information.
In this paper, we show that convolutional attention is 
a more efficient and effective way to encode contextual information 
than the self-attention mechanism in transformers.
By re-examining the characteristics owned by successful segmentation models, 
we discover several key components leading to the performance improvement 
of segmentation models.
This motivates us to design a novel convolutional attention network that 
uses cheap convolutional operations.
Without bells and whistles, our SegNeXt significantly improves the 
performance of previous state-of-the-art methods on popular benchmarks, 
including ADE20K, Cityscapes, COCO-Stuff, Pascal VOC, Pascal Context, and iSAID.
Notably, SegNeXt outperforms EfficientNet-L2 w/ NAS-FPN and achieves 
$90.6\%$ mIoU on the Pascal VOC 2012 test leaderboard using only 
\nicefrac{1}{10} parameters of it.
On average, SegNeXt achieves about $2.0\%$ mIoU improvements compared to the state-of-the-art methods on the ADE20K datasets with the same or fewer computations.
Code is available.



\end{abstract}


\newcommand{\cmplx}[1]{$\mathcal{O}(#1)$}

\begin{table}[htp!]
  \centering
  \setlength{\tabcolsep}{2.6mm}
  \caption{Properties we observe from the successful semantic segmentation methods 
  that are beneficial to the boost of model performance. Here, $n$ refers to the number
  of pixels or tokens. Strong encoder denotes strong backbones, like ViT~\cite{dosovitskiy2020image} and VAN~\cite{guo2022visual}. }
  \begin{tabular}{l|c|c|c|c|c}
    \toprule
    \textbf{Properties} & \textbf{DeepLabV3+} & \textbf{HRNet} & \textbf{SETR}  
      & \textbf{SegFormer} &  \textbf{SegNeXt} \\ \midrule
    Strong encoder          & \xmark & \xmark & \cmark & \cmark & \cmark \\ 
    Multi-scale interaction & \cmark & \cmark & \xmark & \xmark & \cmark \\
    Spatial attention       & \xmark & \xmark & \cmark & \cmark & \cmark \\ 
    \midrule
    Computational complexity&\cmplx{n}&\cmplx{n}&\cmplx{n^2}&\cmplx{n^2}&\cmplx{n} \\
    \bottomrule
  \end{tabular}
  \label{Tab.operation_properties}
\end{table}


\section{Introduction}
\label{sec:intro}
As one of the most fundamental research topics in computer vision, 
semantic segmentation, 
which aims at assigning each pixel a semantic category, 
has attracted great attention over the past decade.
From early CNN-based models, typified by FCN~\cite{long2015fully} and
DeepLab series~\cite{chen2014semantic,chen2017rethinking,chen2018encoder},
to recent transformer-based methods, 
represented by SETR~\cite{zheng2021rethinking}
and SegFormer~\cite{xie2021segformer}, 
semantic segmentation models have experienced
significant revolution in terms of network architectures.



By revisiting previous successful semantic segmentation works, 
we summarize several key properties different models possess as shown in
\tabref{Tab.operation_properties}.
Based on the above observation, 
we argue a successful semantic segmentation model should have 
the following characteristics:
(i) A strong backbone network as encoder. 
Compared to previous CNN-based models, 
the performance improvement of transformer-based models is mostly 
from a stronger backbone network.
(ii) Multi-scale information interaction. 
Different from the image classification task that
mostly identifies a single object, 
semantic segmentation is a dense prediction task and hence
needs to process objects of varying sizes in a single image.
(iii) Spatial attention.
Spatial attention allows models to perform segmentation
through prioritization of areas within the semantic regions.
(iv) Low computational complexity. 
This is especially crucial when dealing with high-resolution
images from remote sensing and urban scenes.










\begin{figure}[t]
  \centering
  \scriptsize
  \begin{overpic}[width=.47\textwidth]{data_figure_flops_city.pdf}
    \put(61, 19.2){SegFormer~\cite{xie2021segformer}}
    \put(61, 15.1){HRFormer \cite{yuan2021hrformer}}
    \put(61, 11.0){SegNeXt}
  \end{overpic} \hspace{2mm}
  \begin{overpic}[width=.47\textwidth]{data_figure_flops_ade.pdf}
    \put(61, 23.3){SegFormer \cite{xie2021segformer}}
    \put(61, 19.2){HRFormer \cite{yuan2021hrformer}}
    \put(61, 15.1){MaskFormer \cite{cheng2021maskformer}}
    \put(61, 11.0){SegNeXt}
  \end{overpic} \hspace{2mm}
  \caption{Performance-Computing curves on the Cityscapes (left) 
    and ADE20K (right) validation sets. FLOPs are calculated using an input size of 
    $2,048 \times 1,024$ for Cityscapes and $512 \times 512$ for ADE20K.
    The size of the circle indicates the number of parameters. Larger circles mean
    more parameters. We can see that our SegNeXt achieves the best trade-off between
    segmentation performance and computational complexity.
  }\label{fig:results}
  \vspace{-0.1cm}
\end{figure}

Taking the aforementioned analysis into account, in this paper, 
we rethink the design of convolutional attention and propose an efficient 
yet effective encoder-decoder architecture for semantic segmentation.
Unlike previous transformer-based models that use convolutions in decoders 
as feature refiners,
our method inverts the transformer-convolution encoder-decoder architecture.
Specifically, for each block in our encoder, 
we renovate the design of conventional
convolutional blocks and utilize multi-scale convolutional features 
to evoke spatial attention via a simple element-wise multiplication
following~\cite{guo2022visual}.
We found such a simple way to build spatial attention is more efficient 
than both the standard convolutions and self-attention 
in spatial information encoding.
For decoder, we collect multi-level features from different stages and
use Hamburger~\cite{geng2021attention} to further extract global context. Under this setting, our method can obtain multi-scale context from local to global, achieve adaptability in spatial and channel dimensions, and aggregate information from low to high levels.

Our network, termed SegNeXt, 
is mostly composed of convolutional operations except the decoder part, 
which contains a decomposition-based Hamburger module~\cite{geng2021attention} (Ham) for global information extraction.
This makes our SegNeXt much more efficient than previous segmentation methods that heavily rely on transformers.
As shown in \figref{fig:results},
SegNeXt outperforms recent transformer-based methods significantly.
In particular, our SegNeXt-S outperforms SegFormer-B2 (81.3\% vs. 81.0\%) 
using only about \nicefrac{1}{6} (124.6G vs. 717.1G) computational cost and 
\nicefrac{1}{2} parameters (13.9M vs. 27.6M)
when dealing with high-resolution urban scenes from the Cityscapes dataset.


Our contributions can be summarized as follows:

\begin{itemize}
\item We identify the characteristics that a good semantic segmentation model should own and present a novel tailored network architecture, termed SegNeXt, that evokes spatial attention via multi-scale convolutional features.
\item We show that an encoder with simple and cheap convolutions can still perform better than vision transformers, especially when processing object details, while it requires much less computational cost.
\item Our method improves the performance of \sArt semantic segmentation methods by a large margin on various segmentation benchmarks, including ADE20K, Cityscapes, COCO-Stuff, Pascal VOC, Pascal Context, and iSAID. 
\end{itemize} 




\section{Related Work}


\subsection{Semantic Segmentation}

Semantic segmentation is a fundamental computer vision task. 
Since FCN~\cite{long2015fully} was proposed, {convolutional neural networks (CNNs)~\cite{badrinarayanan2017segnet,ronneberger2015u,yu2015multi,zhao2017pyramid,fu2019dual,yuan2020object,wang2020deep,pami21Res2net,li2020gated} have 
achieved great success and become a popular architecture for semantic segmentation.
Recently, transformer-based methods~\cite{zheng2021rethinking,xie2021segformer,yuan2021hrformer,strudel2021segmenter,ranftl2021vision,li2022video,cheng2021maskformer,cheng2021mask2former} have shown 
great potentials and outperform CNN-based methods. }


In the era of deep learning, the architecture of segmentation models
can be roughly divided into two parts: encoder and decoder. 
For the encoder, researchers usually adopt popular classification networks
(\eg{ResNet~\cite{he2016deep}, ResNeXt~\cite{xie2017aggregated} 
and DenseNet~\cite{huang2017densely}}) instead of tailored architecture.
However, semantic segmentation is a kind of dense prediction task, 
which is different from image classification.
The improvement in classification may not appear in the challenging segmentation task~\cite{he2019bag}.
Thus, some tailored encoders appear, including 
Res2Net~\cite{pami21Res2net},
HRNet~\cite{wang2020deep}, SETR~\cite{zheng2021rethinking}, 
SegFormer~\cite{xie2021segformer}, HRFormer~\cite{yuan2021hrformer}, MPViT~\cite{lee2022mpvit}, DPT~\cite{ranftl2021vision}, \etc.
For the decoder, it is often used in cooperating with 
encoders to achieve better results.
There are different types of decoders for different goals,
including achieving multi-scale receptive fields~\cite{zhao2017pyramid,chen2016attention,xia2016zoom}, 
collecting multi-scale semantics~\cite{ronneberger2015u,xie2021segformer,chen2018encoder},
enlarging receptive field~\cite{chen2017deeplab,chen2014semantic,peng2017large}, 
strengthening edge features~\cite{zhen2020joint,bertasius2016semantic,ding2019boundary,li2020improving,yuan2020segfix}, and capturing global context~\cite{fu2019dual,huang2019ccnet,yuan2018ocnet,li2019expectation,guo2021beyond,he2019adaptive,zhang2018context}.


{In this paper, we summarize the characteristics of those successful models 
designed for semantic segmentation and present a CNN-based model, named SegNeXt.}
The most related work to our paper, is~\cite{peng2017large},
which decomposes a $k \times k$ convolution into a pair of $k\times1$ and 
$1\times k$ convolutions.
Though this work has shown large convolutional kernels matter in semantic segmentation, it ignores the importance of multi-scale receptive field
and does not consider how to leverage these multi-scale features extracted by large
kernels for segmentation in the form of attention.





\subsection{Multi-Scale Networks}

Designing multi-scale network is one of the popular directions in computer vision.
For segmentation models, multi-scale blocks appear in both the 
encoder~\cite{wang2020deep,pami21Res2net,szegedy2015going} and the 
decoder~\cite{zhao2017pyramid,yu2015multi,chen2017rethinking} parts.
GoogleNet~\cite{szegedy2015going} is one of the most related
multi-scale architectures to our method, which uses a multi-branch structure to
achieve multi-scale feature extraction.
Another work that is related to our method is HRNet~\cite{wang2020deep}.
In the deeper stages, HRNet also keeps high-resolution features, which are
aggregated with low-resolution features, to enable multi-scale feature extraction.

Different from previous methods, \modelname, besides capturing multi-scale
features in encoder, introduces an efficient attention mechanism and employs
cheaper and larger kernel convolutions.
These enable our model to achieve higher performance than the aforementioned segmentation
methods.



\subsection{Attention Mechanisms}

Attention mechanism is a kind of adaptive selection process,
which aims to make the network focus on the important part.
Generally speaking, it can be divided into two categories in semantic 
segmentation~\cite{guo2021attention_survey}, including channel attention and spatial attention. 
Different types of attentions play different roles.
For instance, spatial attentions mainly care about the important spatial regions~\cite{dosovitskiy2020image,dai2017deformable,mnih2014recurrent,liu2021swin,guo_pct}.
Differently, the goal of using channel attention is to make the network selectively attend to those important objects, which has been demonstrated important in previous works~\cite{hu2018squeeze,chen2017sca,wang2020ecanet}.
Speaking of the recent popular vision transformers~\cite{dosovitskiy2020image,liu2021swin,yang2021focal,wang2021pyramid,wang2021pvtv2,liu2021fuseformer,xie2021segformer,huang2022flowformer,liu2021decoupled,yuan2021hrformer}, they usually ignore adaptability in channel dimension. 



Visual attention network (VAN)~\cite{guo2022visual} is the most related work to SegNeXt, 
which also proposes to leverage the large-kernel attention (LKA) mechanism to build both channel and spatial attention.
Though VAN has achieved great performance in image classification,
it neglects the role of multi-scale feature aggregation during the network design, which
is crucial for segmentation-like tasks.




\section{Method}




In this section, we describe the architecture of the proposed SegNeXt in detail.
Basically, we adopt an encoder-decoder architecture following most previous works,
which is simple and easy to follow.










\subsection{Convolutional Encoder}
\label{sec:encoder}



We adopt the pyramid structure for our encoder following most previous work~\cite{xie2021segformer,chen2017deeplab,fu2019dual}.
For the building block in our encoder, we adopt a similar structure to
that of ViT~\cite{dosovitskiy2020image,xie2021segformer} but what is different
is that we do not use the self-attention mechanism but design a novel multi-scale
convolutional attention (MSCA) module.
As depicted in~\figref{fig:pipeline} (a), MSCA contains three parts: 
a depth-wise convolution to aggregate local information, multi-branch depth-wise strip convolutions to capture multi-scale context, and an $1\times1$ convolution to 
model relationship between different channels.
The output of the $1\times1$ convolution is used as attention weights directly
to reweigh the input of MSCA.
Mathematically, our MSCA can be written as:
\vspace{-5pt}
\begin{align}
  \mbox{Att} &= \text{Conv}_{1\times1}(\sum_{i=0}^{3}\mbox{Scale}_{i}(\text{DW-Conv}(F))), \\
  \mbox{Out} &= \mbox{Att} \otimes F.
\end{align}
\begin{figure}\vspace{-2pt}
\centering 

\begin{overpic}[width=0.9\linewidth]{MSCA_arch.pdf}
    \put(9,-0.2){ \footnotesize{(a) A stage of MSCAN }  }
    \put(63,-0.2){ \footnotesize{(b) MSCA} }
\end{overpic}
\caption{Illustration of the proposed MSCA and MSCAN. Here, $d, k_1 \times k_2$ means a depth-wise convolution ($d$) using a kernel size of $k_1 \times k_2$. We extract multi-scale features
using convolutions and then utilize them as attention weights to reweigh the input
of MSCA.}
\label{fig:pipeline}
\vspace{-0.2cm}
\end{figure}
where $F$ represents the input feature.
Att and Out are the attention map and output, respectively.
$\otimes$ is the element-wise matrix multiplication operation.
$\text{DW-Conv}$ denotes depth-wise convolution and Scale$_{i},~i \in \{0, 1, 2, 3\}$, 
denotes the $i$th branch in~\figref{fig:pipeline}(b).
Scale$_{0}$ is the identity connection.
Following~\cite{peng2017large}, in each branch, we use
two depth-wise strip convolutions to approximate standard depth-wise convolutions
with large kernels.
Here, the kernel size for each branch is set to 7, 11, and 21, respectively.
The reasons why we choose depth-wise strip convolutions are two-fold.
On one hand, strip convolution is lightweight.
To mimic a standard 2D convolution with kernel size $7\times7$, we only need
a pair of $7\times1$ and $1\times7$ convolutions.
On the other hand, there are some strip-like objects, such as human and telephone pole
in the segmentation scenes.
Thus, strip convolution can be a complement of grid convolutions
and helps extract strip-like features~\cite{peng2017large,hou2020strip}.
\newcommand{\MCols}[2]{\multicolumn{#1}{c|}{#2}}
\begin{table}[t]
\vspace{-0.2cm}
  \centering
  \renewcommand{\arraystretch}{1.2}  
  \setlength{\tabcolsep}{4.2pt}
  \caption{Detailed settings of different sizes of the proposed SegNeXt. In this table,
  `e.r.' represents the expansion ratio in the feed-forward network. `$C$' and `$L$' are
  the numbers of channels and building blocks, respectively.
  `Decoder dimension' denotes the MLP dimension in the decoder. `Parameters' are calculated on the ADE20K dataset~\cite{zhou2017scene}. Due to the different numbers of the categories in different datasets, the number of parameters may change slightly.
  }\label{tab.architecture}
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    stage & output size & e.r. & SegNeXt-T & SegNeXt-S & SegNeXt-B & SegNeXt-L \\ \midrule
    1 & $\frac{H}{4}\times \frac{W}{4} \times C$ & 8 & 
    \makecell{$C=32$, $L=3$} & \makecell{$C=64$, $L=2$} & \makecell{$C=64$, $L=3$} & \makecell{$C=64$, $L=3$}\\ \midrule
    2 & $\frac{H}{8}\times \frac{W}{8} \times C$ & 8 & 
    \makecell{$C=64$, $L=3$} & \makecell{$C=128$, $L=2$} & \makecell{$C=128$, $L=3$} &  \makecell{$C=128$ , $L=5$} \\ \midrule
    3 & $\frac{H}{16}\times \frac{W}{16} \times C$ & 4 & 
    \makecell{$C=160$, $L=5$} & \makecell{$C=320$,$L=4$} & \makecell{$C=320$,$L=12$} &  \makecell{$C=320$,$L=27$} \\ \midrule
    4 & $\frac{H}{32}\times \frac{W}{32} \times C$ & 4 & 
    \makecell{$C=256$,$L=2$} & \makecell{$C=512$,$L=2$} & \makecell{$C=512$,$L=3$} &  \makecell{$C=512$,$L=3$} \\ \midrule
    \MCols{3}{Decoder dimension} & 256 & 256 & 512 & 1,024 \\ \midrule
    \MCols{3}{{Parameters} (M)} & 4.3 & 13.9  & 27.6  & 48.9 \\ 
    \bottomrule
  \end{tabular}
}
\vspace{-0.2cm}
\end{table}

Stacking a sequence of building blocks yields the proposed convolutional encoder, named
MSCAN.
For MSCAN, we adopt a common hierarchical structure, 
which contains four stages with decreasing spatial 
resolutions $\frac{H}{4} \times \frac{W}{4}$, 
$\frac{H}{8} \times \frac{W}{8}$, 
$\frac{H}{16} \times \frac{W}{16}$ and 
$\frac{H}{32} \times \frac{W}{32}$.
Here, $H$ and $W$ are height and width 
of the input image, respectively.
Each {stage} contains a down-sampling block and a stack of building blocks as described above.
The down-sampling block has a convolution with stride 2 and kernel size $3\times3$, followed by
a batch normalization layer~\cite{ioffe2015batch}.
Note that, in each building block of MSCAN, we use batch normalization
instead of layer normalization as we found batch normalization gains more for
the segmentation performance.


We desgin four encoder models with different sizes, 
named MSCAN-T, MSCAN-S, MSCAN-B, and MSCAN-L, respectively.
The corresponding overall segmentation models are termed
SegNeXt-T, SegNeXt-S, SegNeXt-B, SegNeXt-L, respectively.
Detailed network settings are displayed in~\tabref{tab.architecture}.



\subsection{Decoder}
\label{sec:decoder}


In segmentation models~\cite{xie2021segformer,zheng2021rethinking,chen2017deeplab},
the encoders are mostly pretrained on the ImageNet dataset.
To capture high-level semantics, a decoder is usually necessary, which is applied upon the encoder.
In this work, we investigate three simple decoder structures, which have been
shown in~\figref{fig:ablation_fig}.
The first one, adopted in SegFormer~\cite{xie2021segformer}, is
a purely MLP-based structure.
The second one is mostly adopted CNN-based models.
In this kind of structure, the output of the encoder is directly used as the input
to a heavy decoder head, like ASPP~\cite{chen2017deeplab}, PSP~\cite{zhao2017pyramid},
and DANet~\cite{fu2019dual}.
The last one is the structure adopted in our SegNeXt.
We aggregate features from the last three stages and use a lightweight Hamburger~\cite{geng2021attention} to further model the global context.
Combined with our powerful convolutional encoder, we found that using a lightweight decoder improves performance-computation efficiency.

\begin{figure}[htp!]
  \centering \footnotesize
  \begin{overpic}[width=0.95\linewidth]{diff_arch.pdf}
    \put(14,1.0){(a)}
    \put(50,1.0){(b)}
    \put(80,1.0){(c)}
  \end{overpic}
\caption{ Three different decoder designs.}
\label{fig:ablation_fig}
\vspace{-0.1cm}
\end{figure}

It is worth nothing that unlike SegFormer whose decoder aggregates the features from Stage 1 to
Stage 4, our decoder only receives features from the last three stages.
This is because our SegNeXt is based on convolutions.
The features from Stage 1 contain too much low-level 
information and hurts the performance.
Besides, operations on Stage 1 bring heavy computational overhead.
In our experiment section, we will show that our convolutional SegNeXt performs much better than the recent state-of-the-art transformer-based SegFormer~\cite{xie2021segformer} and HRFormer~\cite{yuan2021hrformer}.


\section{Experiments}

\paragraph{Dataset.}
We evaluate our methods on seven popular datasets, including
ImageNet-1K~\cite{deng2009imagenet}, ADE20K~\cite{zhou2017scene}, Cityscapes~\cite{cordts2016cityscapes},
Pascal VOC~\cite{everingham2010pascal}, Pascal Context~\cite{mottaghi2014role}, COCO-Stuff~\cite{caesar2018coco}, and iSAID~\cite{waqas2019isaid}.  
ImageNet~\cite{deng2009imagenet} is the best-known dataset for image classification, 
which contains 1,000 categories.
Similar to most segmentation methods, we use it to pretrain our MSCAN encoder. 
ADE20K~\cite{zhou2017scene} is a challenging dataset which contains 150 semantic classes.
It consists of 20,210/2,000/3,352 images in the training, validation and test sets.
Cityscapes~\cite{cordts2016cityscapes} mainly focuses on urban scenes and 
contains 5.000 high-resolution images with 19 categories.
There are 2,975/500/1,525 images for  training, validation and testing, respectively.
Pascal VOC~\cite{everingham2010pascal} involves 20 foreground classes and a background class.
After augmentation, it has 10, 582/1, 449/1, 456 images for 
training, validation and testing, respectively.
Pascal Context~\cite{mottaghi2014role} contains 59 foreground classes and a background class.
The training set and validation set contain 4,996 and 5,104 images, respectively.
COCO-Stuff~\cite{caesar2018coco} is also a challenging benchmark, which 
contains 172 semantic categories and 164k images in total.
iSAID~\cite{waqas2019isaid} is a large-scale aerial image segmentation benchmark,
which includes 15 foreground classes and a background class.
Its training, validation and test sets separately involve
1,411/458/937 images.


\begin{table}[t]
\vspace{-0.2cm}
  \footnotesize
  \begin{minipage}{0.46 \textwidth}
  \centering
  \setlength{\tabcolsep}{2mm}
\caption{
Comparison with state-of-the-art methods on ImageNet validation set. `Acc.' denotes Top-1 accuracy.
}
\begin{tabular}{l|c|c}
\toprule
	Method & Params. (M)  & Acc. (\%)  \\
    \midrule
    MiT-B0~\cite{xie2021segformer} & 3.7 & 70.5 \\
	VAN-Tiny~\cite{guo2022visual} & 4.1  & 75.4 \\ 
	\textbf{MSCAN-T} & 4.2 & \textbf{\textcolor{ForestGreen}{75.9}} \\ 
	\midrule
	MiT-B1~\cite{xie2021segformer} & 14.0 & 78.7  \\
	{VAN-Small~\cite{guo2022visual} } & 13.9 & {81.1} \\ 
	\textbf{MSCAN-S } & 14.0 & \textbf{\textcolor{ForestGreen}{81.2}} \\ 
	\midrule
	MiT-B2~\cite{xie2021segformer}   &25.4  &81.6 \\
	Swin-T~\cite{liu2021swin} & 28.3 & 81.3 \\
	ConvNeXt-T~\cite{liu2022convnet} & 28.6  & 82.1 \\  
	{VAN-Base~\cite{guo2022visual} } & 26.6 & {82.8} \\
	\textbf{MSCAN-B} & 26.8 & \textbf{\textcolor{ForestGreen}{83.0}} \\
	\midrule
	MiT-B3~\cite{he2016deep}  & 45.2  & 83.1\\
	Swin-S ~\cite{liu2021swin} & 49.6 & 83.0 \\
	ConvNeXt-S ~\cite{liu2021swin} & 50.1  & 83.1 \\
    {VAN-Large~\cite{guo2022visual}} & 44.8 & \textbf{\textcolor{ForestGreen}{83.9}} \\
    \textbf{MSCAN-L} & 45.2 & \textbf{\textcolor{ForestGreen}{83.9}} \\
\bottomrule
\end{tabular}
\label{tab:imagenet_cls}
\end{minipage}
\hfill
  \begin{minipage}{0.5\textwidth}
  \centering
  \setlength{\tabcolsep}{1.8mm}
  \caption{Comparison with state-of-the-art methods on the remote sensing dataset iSAID.
  Single-scale (SS) test is applied by default. Our SegNeXt-T has achieved \sArt performance.
  }
  \begin{tabular}{l|c|c} 
    \toprule
	Method & Backbone & mIoU (\%)  \\
    \midrule
	DenseASPP~\cite{yang2018denseaspp} & ResNet50  & 57.3  \\ 
	PSPNet~\cite{zhao2017pyramid}  & ResNet50 & 60.3  \\ 
	SemanticFPN~\cite{kirillov2019panoptic} & ResNet50  & 62.1  \\ 
	RefineNet~\cite{lin2017refinenet}  & ResNet50 & 60.2  \\ 
	HRNet~\cite{wang2020deep} & HRNetW-18 & 61.5  \\ 
	GSCNN~\cite{takikawa2019gated} & ResNet50 & 63.4  \\ 
	SFNet~\cite{li2020semantic} & ResNet50 & 64.3  \\ 
	RANet~\cite{mou2019relation} & ResNet50  & 62.1  \\ 
	PointRend~\cite{kirillov2020pointrend} & ResNet50 & 62.8  \\ 
	FarSeg~\cite{zheng2020foreground} & ResNet50 & 63.7  \\ 
	UperNet~\cite{xiao2018unified} & Swin-T & 64.6   \\ 
	PointFlow~\cite{li2021pointflow} & ResNet50 & 66.9  \\ 
	\midrule
	SegNeXt-T & MSCAN-T & 68.3  \\ 
	SegNeXt-S & MSCAN-S & 68.8  \\ 
	SegNeXt-B & MSCAN-B & 69.9  \\ 
	SegNeXt-L & MSCAN-L & \textbf{\textcolor{ForestGreen}{70.3}}  \\ 
    \bottomrule
  \end{tabular}
  \label{tab:isaid}
  \end{minipage}
\vspace{-0.2cm}
\end{table}



\textbf{Implementation details.}
We conduct experiments by using Jittor~\cite{hu2020jittor} and Pytorch~\cite{paszke2019pytorch}.
Our implementation is based on timm (Apache-2.0)~\cite{rw2019timm} and mmsegmentation (Apache-2.0)~\cite{mmseg2020} libraries for classification and segmentation, respectively.
All encoders of our segmentation models are pretrained on the ImageNet-1K dataset~\cite{deng2009imagenet}.
We adopt Top-1 accuracy and mean Intersection over Union (mIoU) as 
our evaluation metrics for classification and segmentation, respectively.
All models are trained on a node with 8 RTX 3090 GPUs.



For ImageNet pretraining, our data augmentation method and training settings are the same as DeiT~\cite{touvron2021training}.
For segmentation experiments, we adopt some common data augmentation including random horizontal flipping, random scaling (from 0.5 to 2) and random cropping.
The batch size is set to
8 for the Cityscapes dataset and 16 for all the other datasets.
AdamW~\cite{loshchilov2017decoupled} is applied to train our models.
We set the initial learning rate as 0.00006 and employ the poly-learning rate decay policy.
We train our model 160K iterations for ADE20K, Cityscapes and iSAID datasets and 
80K iterations for COCO-Stuff, Pascal VOC and Pascal Context datasets.
During testing, we use both the 
single-scale (SS) and multi-scale (MS) flip test strategies  for a fair comparison.
More details can be found in our supplementary materials.





\subsection{Encoder Performance on ImageNet}
\label{sec:imagenet}

ImageNet pretraining is a common strategy for training segmentation models~\cite{zhao2017pyramid,chen2017rethinking,xie2021segformer,yuan2021hrformer,chen2017deeplab}.
Here, we compare the performance of our MSCAN with several recent popular CNN-based
and transformer-based classification models.
As shown in~\tabref{tab:imagenet_cls}, our MSCAN achieves better results than the recent \sArt CNN-based method, ConvNeXt~\cite{liu2022convnet} and outperforms popular transformer-based methods, like Swin Transformer~\cite{liu2021swin} and MiT, the encoder of SegFormer~\cite{xie2021segformer}.




\subsection{Ablation study}


\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1}
    \renewcommand{\tabcolsep}{3mm}
    \caption{Performance of different attention mechanisms in decoder. 
    SegNeXt-B w/ Ham means the MSCAN-B encoder plus the Ham decoder.
    FLOPs are calculated using the input size of 512$\times$512.
    }
    \begin{tabular}{l|cc|cc}
        \toprule
    	Architecture & Params. (M) & GFLOPs & mIoU (SS) & mIoU (MS)  \\ \midrule
    	SegNeXt-B w/ CC~\cite{huang2019ccnet} & 27.8 & 35.7 & 47.3 & 48.6 \\ 
    	SegNeXt-B w/ EMA~\cite{li2019expectation} & 27.4 & 32.3 & 48.0 & 49.1 \\ 
    	SegNeXt-B w/ NL~\cite{wang2018non} & 27.6 & 40.9 & 48.6 & 50.0 \\ 
    	SegNeXt-B w/ Ham~\cite{geng2021attention} & 27.6 & 34.9 & 48.5 & 49.9 \\ 
        \bottomrule
    \end{tabular}
    \label{tab:ablation_decoder}
\end{table}

\paragraph{Ablation on MSCA design.}
We conduct ablation study on MSCA design on both ImageNet and ADE20K dataset.
K $\times$ K branch contains a depth-wise 1 $\times$ K convolution and a K $\times$ 1 depth-wise convolution.
1 $\times$ 1 conv means the channel mixing operation.
Attention means the element-wise product,
which makes the network obtain adaptive ability.
Results are shown in~\tabref{Tab.ablation_msca}.  
We can find that each part contributes to the final performance.

\begin{table}[htp!]
  \centering
  \setlength{\tabcolsep}{2.6mm}
  \caption{Ablation study on the design of MSCA. Top-1 means Top-1 accuracy on ImageNet dataset and mIoU denotes mIoU on ADE20K benchmark. Br: Branch.}
  \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \textbf{7 $\times$ 7 Br} & \textbf{11 $\times$ 11 Br} & \textbf{21 $\times$ 21 Br} & \textbf{1 $\times$ 1 Conv}  
      & \textbf{Attention} &  \textbf{Top-1} &  \textbf{mIoU} \\ \midrule
       \cmark   & \xmark & \xmark & \cmark & \cmark & 74.7 & 39.6 \\ 
       \xmark   & \cmark & \xmark & \cmark & \cmark & 75.2 & 39.7 \\ 
       \xmark   & \xmark & \cmark & \cmark & \cmark & 75.3 & 40.0 \\ 
       \cmark   & \cmark & \cmark & \xmark & \cmark & 74.8 & 39.1 \\ 
       \cmark   & \cmark & \cmark & \cmark & \xmark & 75.5 & 40.5 \\ 
       \cmark   & \cmark & \cmark & \cmark & \cmark & 75.9 & 41.1 \\ 
    \bottomrule
  \end{tabular}
  \label{Tab.ablation_msca}
\end{table}



\paragraph{Global Context for Decoder.} 
Decoder plays an important role in integrating global context from multi-scale features for segmentation models.
Here, we investigate the influence of different global context modules on decoder.
As shown in most previous works~\cite{wang2018non,fu2019dual}, attention-based
decoders achieves better performance for CNNs than pyramid structures~\cite{zhao2017pyramid,chen2017deeplab}, we thus only show the results using attention-based decoders.
Specifically, we show results with 4 different types of attention-based decoders,
including non-local (NL) attention~\cite{wang2018non} with \cmplx{n^2} complexity
and CCNet~\cite{huang2019ccnet}, EMANet~\cite{li2019expectation}, and HamNet~\cite{geng2021attention} with \cmplx{n} complexity.
As shown in~\tabref{tab:ablation_decoder}, Ham achieves the best trade-off between complexity and performance.
Therefore, we use Hamburger~\cite{geng2021attention} in our decoder.


\begin{table}[ht]\centering
\renewcommand{\arraystretch}{1}
\renewcommand{\tabcolsep}{3mm}
\caption{Performance of different decoder structures. 
SegNeXt-T (a) means~\figref{fig:ablation_fig} (a) is used in decoder.
FLOPs are calculated using the input size of 512$\times$512.
SegNeXt-T (c) w/ stage 1 means the output of stage 1 is also sent into the decoder.
}
\begin{tabular}{l|cc|cc}
    \toprule
	Architecture & Params. (M) & GFLOPs & mIoU (SS) & mIoU (MS)  \\ \midrule
	SegNeXt-T (a)            & 4.4 & 10.0 & 40.3 & 41.1 \\ 
	SegNeXt-T (b)            & 4.2 & 4.9  & 30.9 & 40.6 \\ 
	SegNeXt-T (c)            & 4.3 & 6.6  & 41.1 & 42.2 \\ 
	SegNeXt-T (c) w/ stage 1 & 4.3 & 12.1 & 40.7 & 42.2 \\ 
    \bottomrule
\end{tabular}
\label{tab:ablation_architecture}
\end{table}


\textbf{Decoder Structure.} Unlike image classification, segmentation models need
high-resolution outputs. We ablate three different decoder designs for segmentation,
all of which have been shown in~\figref{fig:ablation_fig}.
The corresponding results are listed in \tabref{tab:ablation_architecture}.
We can see that SegNeXt (c) achieves the best performance and the computational cost
is also low.


\begin{table}[ht]\centering
\renewcommand{\arraystretch}{1}
\renewcommand{\tabcolsep}{3mm}
\caption{Importance of our multi-scale convolutional attention (MSCA). 
SegNeXt-T w/o MSCA means we use only a branch with a large kernel convolution
as done in \cite{guo2022visual} to replace the multiple branches in our MSCA.
FLOPs are calculated using the input size of 512$\times$512.
}
\begin{tabular}{l|cc|cc}
    \toprule
	Architecture & Params. (M) & GFLOPs & mIoU (SS) & mIoU (MS)  \\ \midrule
	SegNeXt-T w/o MSCA & 4.2 & 6.5 & 39.5 & 40.9 \\ 
	SegNeXt-T w/ MSCA & 4.3 & 6.6 & 41.0 & 42.5 \\  \midrule
	SegNeXt-S w/o MSCA & 13.8 & 15.8 & 43.5 & 45.2 \\ 
	SegNeXt-S w/ MSCA & 13.9 & 15.9 & 44.3 & 45.8 \\ 
    \bottomrule
\end{tabular}
\label{tab:multi_scale}
\vspace{-0.3cm}
\end{table}

\paragraph{Importance of Our MSCA.}
Here, we conduct experiments to demonstrate the importance of MSCA for segmentation.
As a comparison, we follow VAN~\cite{guo2022visual} and replace the multiple branches
in our MSCA with a single convolution with a large kernel.
As shown in~\tabref{tab:multi_scale} and ~\tabref{tab:imagenet_cls}, 
we can observe that though the performance of the two encoders is close in ImageNet
classification, SegNeXt w/ MSCA yields much better results than the setting w/o MSCA.
This indicates that aggregating multi-scale features is crucial in encoder for
semantic segmentation.

\renewcommand{\addImgs}[1]{\includegraphics[width=0.24\textwidth]{Results/#1}}
\begin{figure}[t]
  \centering
  \renewcommand{\tabcolsep}{2pt}
  \begin{tabular}{cccc}
      \addImgs{SegFormer_0.jpg} & \addImgs{SegNeXt_0.jpg} & \addImgs{SegFormer_1.jpg} & \addImgs{SegNeXt_1.jpg} \\
      \addImgs{SegFormer_2.jpg} & \addImgs{SegNeXt_2.jpg} & \addImgs{SegFormer_3.jpg} &  \addImgs{SegNeXt_3.jpg} \\
      SegFormer-B2 & SegNeXt-B & SegFormer-B2 & SegNeXt-B \\
  \end{tabular}
  \caption{Qualitative Comparison of SegNeXt-B and SegFormer-B2 on the Cityscapes dataset.
  More visual results can be found in our supplementary materials.
}\label{fig:results_show}
\end{figure}

\subsection{Comparison with state-of-the-art methods}
\begin{table}[t]
\begin{center}
\caption{Comparison with state-of-the-art methods on the ADE20K, 
Cityscapes and COCO-Stuff benchmarks.
The number of FLOPs (G) is calculated on the input size of 512$\times$512 for ADE20K and COCO-Stuff, and 2,048$\times$1,024 for Cityscapes. $^{\dagger}$ means models pretrained on ImageNet-22K.}
\vspace{-4pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|c|ccc|ccc|ccc}
\toprule
    \multirow{2}{*}{Model}  & Params & \multicolumn{3}{c|}{ADE20K} & \multicolumn{3}{c|}{Cityscapes} & \multicolumn{3}{c}{COCO-Stuff} \\
    & (M) & GFLOPs  & \multicolumn{2}{c|}{mIoU (SS/MS)}  & GFLOPs  & \multicolumn{2}{c|}{mIoU (SS/MS)}  & GFLOPs  & \multicolumn{2}{c}{mIoU (SS/MS)} \\ \midrule
     Segformer-B0~\cite{xie2021segformer} & 3.8  & 8.4 & 37.4 & 38.0 & 125.5 & 76.2 & 78.1 & 8.4 & 35.6 & -\\ 
     SegNeXt-T & 4.3  & 6.6 & \textbf{\textcolor{ForestGreen}{41.1}} & \textbf{\textcolor{ForestGreen}{42.2}} & 50.5 & \textbf{\textcolor{ForestGreen}{79.8 }} & \textbf{\textcolor{ForestGreen}{81.4}} & 6.6 & \textbf{\textcolor{ForestGreen}{38.7}} & \textbf{\textcolor{ForestGreen}{39.1}} \\  
\midrule
    Segformer-B1~\cite{xie2021segformer} & 13.7 & 15.9 & 42.2 & 43.1 & 243.7 & 78.5 & 80.0 & 15.9 & 40.2 & - \\  
     HRFormer-S~\cite{yuan2021hrformer} & 13.5 & 109.5 & 44.0 & 45.1 & 835.7 & 80.0 & 81.0 & 109.5 & 37.9 & 38.9 \\ 
     SegNeXt-S & 13.9 & 15.9 & \textbf{\textcolor{ForestGreen}{44.3}} & \textbf{\textcolor{ForestGreen}{45.8}} & 124.6 & \textbf{\textcolor{ForestGreen}{81.3}} & \textbf{\textcolor{ForestGreen}{82.7}} & 15.9 & 
     \textbf{\textcolor{ForestGreen}{42.2}} & \textbf{\textcolor{ForestGreen}{42.8}} \\  
\midrule
     Segformer-B2~\cite{xie2021segformer} & 27.5 & 62.4 & 46.5 & 47.5 & 717.1 & 81.0 & 82.2 & 62.4 & 44.6 & - \\
     MaskFormer~\cite{cheng2021maskformer} & 42 & 55 & 46.7 & 48.8 & - & - & - & - & - & - \\ 
     SegNeXt-B & 27.6 & 34.9 & \textbf{\textcolor{ForestGreen}{48.5}} & \textbf{\textcolor{ForestGreen}{49.9}} & 275.7 & \textbf{\textcolor{ForestGreen}{82.6}} & \textbf{\textcolor{ForestGreen}{83.8}} & 34.9 & \textbf{\textcolor{ForestGreen}{45.8}} & \
     \textbf{\textcolor{ForestGreen}{46.3}} \\  
\midrule
     SETR-MLA$^{\dagger}$\cite{zheng2021rethinking} & 310.6 & - & 48.6 & 50.1 & - & 79.3 & 82.2 & - & - & - \\  
     DPT-Hybrid~\cite{ranftl2021vision} & 124.0 & 307.9 & - & 49.0 & - & - & - & - & - & - \\
     Segformer-B3~\cite{xie2021segformer} & 47.3 & 79.0 & 49.4 & 50.0 & 962.9 & 81.7 & 83.3 & 79.0 & 45.5 & - \\  
Mask2Former~\cite{cheng2021mask2former} & 47 & 74 & 47.7 & 49.6 & - & - & - & - & - & - \\ 
     HRFormer-B~\cite{yuan2021hrformer} & 56.2 & 280.0 & 48.7 & 50.0 & 2223.8 & 81.9 & 82.6 & 280.0 & 42.4 & 43.3 \\ 
     MaskFormer~\cite{cheng2021maskformer} & 63 & 79 & 49.8 & 51.0 & - & - & - & -  & - & -  \\ 
     SegNeXt-L  & 48.9 & 70.0 & \textbf{\textcolor{ForestGreen}{51.0}} & \textbf{\textcolor{ForestGreen}{52.1}} & 577.5 & \textbf{\textcolor{ForestGreen}{83.2}} & \textbf{\textcolor{ForestGreen}{83.9}} & 70.0 & \textbf{\textcolor{ForestGreen}{46.5}} & \textbf{\textcolor{ForestGreen}{47.2}} \\  
\bottomrule
\end{tabular}
}
\label{tab:ade20k_city_stuff}
\end{center}
\vspace{-0.5cm}
\end{table}
\begin{table}[t]
  \footnotesize
  \begin{minipage}{0.48 \textwidth}
  \centering
  \setlength{\tabcolsep}{2mm}
  \caption{Comparison with state-of-the-art methods on Pascal VOC dataset. $^{*}$ means COCO~\cite{lin2014microsoft} pretraining.
  $^{\dagger}$ denotes JFT-300M~\cite{sun2017revisiting} pretraining.
  $^{\$}$ utilizes additional 300M unlabeled images for pretraining.
  }
  \begin{tabular}{l|c|c}
    \toprule
	Method & Backbone & mIoU  \\
    \midrule
	DANet~\cite{fu2019dual} & ResNet101 & 82.6  \\ 
OCRNet~\cite{yuan2020object} & HRNetV2-W48 & 84.5  \\ 
	HamNet~\cite{geng2021attention} & ResNet101 & 85.9  \\
	EncNet$^{*}$~\cite{zhang2018context} & ResNet101 & 85.9 \\
	EMANet$^{*}$~\cite{li2019expectation} & ResNet101 & 87.7 \\
	DeepLabV3+$^{*}$~\cite{chen2018encoder} & Xception-71 & 87.8 \\
	DeepLabV3+$^{\dagger}$~\cite{chen2018encoder} & Xception-JFT & 89.0 \\
    NAS-FPN$^{\$}$~\cite{zoph2020rethinking} & EfficientNet-L2 & 90.5 \\
	\midrule
	SegNeXt-T & MSCAN-T & 82.7  \\ 
	SegNeXt-S & MSCAN-S & 85.3 \\ 
	SegNeXt-B & MSCAN-B & 87.5 \\ 
SegNeXt-L$^{*}$ & MSCAN-L & \textbf{\textcolor{ForestGreen}{90.6}} \\ 
    \bottomrule
  \end{tabular}
  \label{Tab.pascal_voc}
  \end{minipage}
  \hfill
\begin{minipage}{0.48\textwidth}
  \centering
  \setlength{\tabcolsep}{3.5mm}
\caption{
Comparison with state-of-the-art real-time methods on Cityscapes test dataset.
We test our method with a single RTX-3090 GPU and AMD EPYC 7543 32-core processor CPU .
Without using any optimizations, SegNeXt-T can achieve 25 frames per second (FPS),
which meets the requirements of real-time applications.
}
\begin{tabular}{l|c|c}
\toprule
	Method & Input size  & mIoU  \\
    \midrule
ESPNet~\cite{mehta2018espnet} & 512$\times$1,024 &  60.3  \\ 
	ESPNetv2~\cite{mehta2019espnetv2} & 512$\times$1,024 & 66.2  \\ 
    ICNet~\cite{zhao2018icnet} &  1,024 × 2,048 &  69.5  \\ 
DFANet~\cite{li2019dfanet} & 1,024 × 1,024 & 71.3  \\ 
	BiSeNet~\cite{yu2018bisenet} & 768 × 1,536 &  74.6  \\ 
	BiSeNetv2~\cite{yu2021bisenet} & 512 × 1,024 &  75.3  \\ 
	DF2-Seg~\cite{li2019partial} &  1,024 × 2,048 & 74.8  \\ 
	SwiftNet~\cite{orsic2019defense} & 1,024 × 2,048 & 75.5  \\ 
	SFNet~\cite{li2020semantic} & 1,024 × 2,048 & 77.8  \\ 
	\midrule
	SegNeXt-T & 768 × 1,536 &  \textbf{\textcolor{ForestGreen}{78.0}} \\ 
\bottomrule
\end{tabular}
\label{tab:real_time_city}
\end{minipage}
\end{table}
In this subsection, we compare our method with \sArt 
CNN-based methods, such as HRNet~\cite{wang2020deep}, ResNeSt~\cite{zhang2020resnest}, and EfficientNet~\cite{tan2019efficientnet}, and
transformer-based methods, like Swin Transformer~\cite{liu2021swin}, 
SegFormer~\cite{xie2021segformer}, HRFormer~\cite{yuan2021hrformer}, 
MaskFormer~\cite{cheng2021maskformer}, and Mask2Former~\cite{cheng2021mask2former}. 



\textbf{Performance-computation trade-off.}
ADE20K and Cityscapes are two widely used benchmarks in semantic segmentation.
As shown in \figref{fig:results}, we plot the performance-computation curves of
different methods on the Cityscape and ADE20K validation set. 
Clearly, our method achieves the best trade-off between performance and computations
compared to other state-of-the-art methods, like SegFormer~\cite{xie2021segformer}, HRFormer~\cite{yuan2021hrformer}, and MaskFormer~\cite{cheng2021maskformer}.


\textbf{Comparison with state-of-the-art transformers.}
We compare SegNeXt with \sArt transformer models on the ADE20K, Cityscapes, COCO-Stuff and Pascal Context benchmarks.
As shown in~\tabref{tab:ade20k_city_stuff}, SegNeXt-L surpasses Mask2Former 
with Swin-T backbone by 3.3 mIoU (51.0 v.s. 47.7) with similar parameters and 
computational cost on he ADE20K dataset.
Moreover, SegNeXt-B yields 2.0 mIoU improvement (48.5 v.s. 46.5) compared to SegFormer-B2 
using only 56\% computations on the ADE20K dataset.
In particular, since the self-attention in SegFormer~\cite{xie2021segformer} is of quadratic complexity \wrt the input size while our method uses convolutions, this makes our method perform
greatly well when dealing with high-resolution images from the Cityscapes dataset.
For instance,  SegNeXt-B gains 1.6 mIoU (81.0 v.s. 82.6) over SegFormer-B2 
but uses 40\% less computations.
In \figref{fig:results_show}, we also show a qualitative comparison with SegFormer.
We can see that thanks to the proposed MSCA, our method recognizes well 
when processing object details.
\begin{table}[!t]\centering
\renewcommand{\arraystretch}{1}
\renewcommand{\tabcolsep}{3mm}
\small
\caption{Comparison on Pascal Context benchmark.
    	The number of FLOPs is calculated with the input size of 512$\times$512. 
    	$^{*}$ means ImageNet-22K pretraining.
    	$^{\dagger}$ denotes ADE20K pretraining.}
\begin{tabular}{l|c|c|c|cc}
\toprule
	Method & Backbone & Params.(M) & GFLOPs & \multicolumn{2}{c}{mIoU (SS/MS)}  \\
	\midrule
	PSPNet~\cite{zhao2017pyramid} & ResNet101 & - & - & - & 47.8  \\ 
	DANet~\cite{fu2019dual} & ResNet101 & 69.1 & 277.7 & - & 52.6  \\ 
    EMANet~\cite{li2019expectation} & ResNet101 & 61.1 & 246.1 & - & 53.1  \\ 
	HamNet~\cite{geng2021attention} & ResNet101 & 69.1 &  277.9 & - & 55.2  \\ 
	HRNet(OCR)~\cite{wang2020deep} & HRNetW48 & 74.5 & - & - & 56.2  \\ 
	DeepLabV3+~\cite{chen2018encoder} & ResNeSt-269 & - & - & - & 58.9  \\ 
	SETR-PUP$^{*}$~\cite{zheng2021rethinking} & ViT-Large & 317.8 & -  & 54.4 & 55.3  \\ 
	SETR-MLA$^{*}$~\cite{zheng2021rethinking} & ViT-Large & 309.5 & - & 54.9 & 55.8  \\ 
	HRFormer-B~\cite{yuan2021hrformer} & HRFormer-B  & 56.2 & 280.0 & 57.6 & 58.5  \\ 
	DPT-Hybrid$^{\dagger}$~\cite{ranftl2021vision} & ViT-Hybrid & 124.0 & - & - & 60.5  \\ 
	\midrule
    SegNeXt-T & MSCAN-T & 4.2 & 6.6 & 51.2 & 53.3  \\ 
    SegNeXt-S & MSCAN-S & 13.9 & 15.9 &  54.2 & 56.1 \\ 
	SegNeXt-B & MSCAN-B & 27.6 & 34.9 & 57.0 & 59.0 \\ 
	SegNeXt-L & MSCAN-L & 48.8 & 70.0 & 58.7 & 60.3  \\ \specialrule{0em}{0pt}{-1pt}
	SegNeXt-L$^{\dagger}$ & MSCAN-L & 48.8 & 70.0 &
	\textbf{\textcolor{ForestGreen}{59.2}} & \textbf{\textcolor{ForestGreen}{60.9}}  \\ 
\bottomrule
\end{tabular}
\label{tab:pascal_context}
\end{table}

\textbf{Comparison with state-of-the-art CNNs.}
As shown in ~\tabref{tab:isaid}, ~\tabref{Tab.pascal_voc}, and ~\tabref{tab:pascal_context},
we compare our SegNeXt with state-of-the-art CNNs 
such as ResNeSt-269~\cite{zhang2020resnest}, EfficientNet-L2~\cite{zoph2020rethinking},
and HRNet-W48~\cite{wang2020deep} on the Pascal VOC 2012, Pascal Context, and iSAID datasets.
SegNeXt-L outperforms the popular HRNet (OCR)~\cite{wang2020deep,yuan2020object} model (60.3 v.s. 56.3) using even less parameters and computations, which is elaborately designed for the segmentation task.
Moreover, SegNeXt-L performs even better than EfficientNet-L2 (NAS-FPN), which is
pretrained on additional 300 million unavailable images, on the \href{http://host.robots.ox.ac.uk:8080/anonymous/HRSC5B.html}{Pascal VOC 2012 test leaderboard}.
It is worth noting that EfficientNet-L2 (NAS-FPN) has 485M parameters, while SegNeXt-L has only 48.7M parameters.


\textbf{Comparison with real-time methods.} 
In addition to the \sArt performance, our method is also suitable for real-time deployments.
Even without any specific software or hardware acceleration, SegNeXt-T realizes 25 frames per second (FPS) using a single 3090 RTX GPU when dealing with an image of size 768$\times$1,536.
As shown in~\tabref{tab:real_time_city}, our method sets new \sArt results for real-time segmentation on the Cityscapes test set.




\section{Conclusions and Discussion}

In this paper, we analyze previous successful segmentation models and
find the good characteristics owned by them.
Based on the findings, we present a tailored convolutional attention module MSCA and a 
CNN-style network SegNeXt.
Experimental results demonstrate that 
SegNeXt surpasses current \sArt transformer-based methods by a considerable margin.


Recently, transformer-based models have dominated various segmentation leaderboards.
Instead, this paper shows that CNN-based methods can still perform better than transformer-based methods when using a proper design.
We hope this paper could encourage researchers to further investigate the potential of CNNs.

Our model also has its limitations, for example, extending this method to large-scale models with 100M+ parameters and the performance on other vision or NLP tasks.
These will be addressed in our future works.






















\bibliographystyle{eccv22.bst}
\bibliography{neurips_2022}





























\appendix


\end{document}