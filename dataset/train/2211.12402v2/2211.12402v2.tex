\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[preprint]{neurips_2022}






\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[hidelinks]{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         


\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amsmath,amssymb}
\usepackage{epsfig,graphicx,subfigure,caption}
\usepackage{algpseudocode}
\usepackage{multirow,booktabs, hhline}
\usepackage[ruled,noend]{algorithm2e}
\usepackage{amsmath, bm}
\usepackage{titlesec}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{caption}
\usepackage{footnote}
\usepackage[bottom]{footmisc}
\usepackage[toc,page]{appendix}
\usepackage{todonotes}


\newcommand{\baby}{X-VLM\xspace}
\newcommand{\babyx}{X-VLM}
\newcommand{\babyB}{X-VLM\xspace}
\newcommand{\babyL}{X-VLM\xspace}

\newcommand{\note}[4][]{\todo[author=#2,color=#3,size=\scriptsize,fancyline,caption={},#1]{#4}} \newcommand{\chunshu}[2][]{\note[#1]{chunshu}{yellow}{#2}\xspace}
\newcommand{\Chunshu}[2][]{\chunshu[inline,#1]{#2}}

\title{X-VLM: All-In-One Pre-trained Model For Vision-Language Tasks}





\author{
Yan Zeng\thanks{Correspondence to: <zengyan.yanne@bytedance.com>.} \\
ByteDance Research \\
\And
Xinsong Zhang \\
ByteDance Research \\
\And
Hang Li \\
ByteDance Research \\
\AND 
Jiawei Wang \\
ByteDance Research \\
\And
Jipeng Zhang \\
HKUST \\
\And
Wangchunshu Zhou \\
ETH Zurich
}


\begin{document}
\maketitle

\begin{abstract}
Vision language pre-training aims to learn alignments between vision and language from a large amount of data. Most existing methods only learn image-text alignments. Some others utilize pre-trained object detectors to leverage vision language alignments at the object level. In this paper, we propose to learn multi-grained vision language alignments by a unified pre-training framework that learns multi-grained aligning and multi-grained localization simultaneously. Based on it, we present \baby, an all-in-one model with a flexible modular architecture, in which we further unify image-text pre-training and video-text pre-training in one model. \baby is able to learn unlimited visual concepts associated with diverse text descriptions. Experiment results show that \baby performs the best on base and large scale for both image-text and video-text tasks, making a good trade-off between performance and model scale. Moreover, we show that the modular design of \baby results in high transferability for it to be utilized in any language or domain. For example, by simply replacing the text encoder with XLM-R, \baby outperforms state-of-the-art multilingual multi-modal pre-trained models without any multilingual pre-training. The code and pre-trained models are available at \url{github.com/zengyan-97/X2-VLM}. 
\end{abstract}
 \section{Introduction}
\label{sec:introduction}


\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{pics/x2vlm_tpami_intro.pdf}}
\caption{(a) Comparison of \baby with existing image-text pre-training methods on the visual reasoning task. (b) Comparison with existing video-text pre-training methods on video-text tasks. (c) Comparison with existing multilingual multi-modal pre-training methods.}
\vspace{-0.5cm}
\label{Fig:intro} 
\end{center}
\end{figure*}


Vision language pre-training aims to learn vision language alignments from a large number of image-text or video-text pairs. A pre-trained Vision Language Model (VLM) fine-tuned with a small amount of labeled data has shown state-of-the-art (SoTA) performances in many Vision Language (V+L) tasks, such as image-text retrieval and visual question answering (VQA).



Existing work learning vision language alignments typically falls into two categories: \textit{coarse-grained} and \textit{fine-grained}. Coarse-grained approaches use convolutional neural networks~\cite{he2016deep} or vision transformers~\cite{dosovitskiy2020image} to encode overall image features~\cite{huang2020pixel, kim2021vilt, li2021align}, which however have difficulties in learning fine-grained vision language alignments, e.g., at the object level, from noisy image-text pairs which are usually weak-correlated~\cite{huo2021wenlan}. To learn fine-grained vision language alignments, many approaches adopt pre-trained object detectors as the image encoder~\cite{tan2019lxmert, lu2019vilbert, li2019visualbert, gan2020large, chen2020uniter, li2020oscar, zhang2021vinvl}. However, object detectors output object-centric features unable to encode relations among multiple objects. Moreover, an object detector can only recognize a limited number of object categories.




Ideally, a VLM should simultaneously learn multi-grained alignments between vision and language in pre-training without being restricted to object-text alignments or image-text alignments. However, learning multi-grained alignments is challenging, and previous work has failed to handle this issue. The challenges come from four aspects: 1) what types of data to use to learn multi-grained vision language alignments; 2) how to aggregate the different types of data in a unified way for vision language pre-training; 3) how to represent multi-grained visual concepts, including objects, regions, and images, by a single vision encoder; 4) how to efficiently learn multi-grained vision language alignments from the data.





In this paper, we present an all-in-one VLM pre-trained with a unified framework to learn multi-grained vision language alignments, namely \baby. We leverage three types of data for vision language pre-training, including object labels on images~\cite{lin2014microsoft,shao2019objects365,kuznetsova2018open} such as ``man'' or ``backpack'', region annotations on images~\cite{krishna2016visual,kuznetsova2018open} such as ``boy wearing backpack'', and text descriptions for images such as ``The first day of school gives a mixed feeling to both students and parents.''. We assume that learning multi-grained vision language alignments can help VLMs better understand weak-correlated image-text pairs since the model has learned to align the components in images, e.g., objects or regions, to textual descriptions, e.g., words or phrases. We associate all visual concepts with text descriptions instead of class labels, including objects, regions, and images. By associating all visual concepts with language, the model can learn unlimited visual concepts described by diverse texts in a unified way.


\baby has a flexible modular architecture, with three modules for vision, text, and fusion, respectively. All modules are based on Transformer~\cite{vaswani2017attention}. We encode an image with vision transformer~\cite{dosovitskiy2020image}, and we utilize certain patch features to represent multi-grained visual concepts in the image that can be objects, regions, or the image itself. By doing so, \baby outputs vision features for objects, regions, and images in a unified form. Furthermore, we propose directly aligning the multi-grained vision features with the paired text features and simultaneously locating multi-grained visual concepts in the same image given different text descriptions for vision language pre-training. In fine-tuning and inference, \baby can leverage the learned multi-grained alignments to perform the downstream V+L tasks without object or region annotations in the input images.


\baby can be easily extended to video-text pre-training. For video encoding, we sample video frames and encode the frames with vision transformer respectively. Then, we use the average in the temporal dimension of patch features of frames to encode a video. The encoder parameters are shared between video-text pre-training and image-text pre-training. By doing so, we leverage video-text pairs to enable the model to understand visual concepts in temporal dimension and learn a more versatile VLM. 


Moreover, we show the flexibility of \baby with the modular architecture. We investigate whether the cross-modal ability can be transferred to other languages or domains after pre-training. This is an important problem in real-world applications because many multi-modal tasks exist in non-English languages. However, since collecting image-text pairs or video-text pairs in certain languages or domains can be costly, recent SoTA VLMs are trained with English data and only applicable to English texts, limiting their application scopes. We find that surprisingly \baby can effectively adapt to V+L tasks in different languages or domains by simply replacing the text module with a language-specific or domain-specific one without further pre-training. 






We conduct extensive experiments to verify the effectiveness of \baby. First, we compare \baby with SoTA image-text pre-training methods on base and large scale and find that \baby substantially outperforms all of them in the image-text tasks, including retrieval, VQA, reasoning, and grounding. Moreover, \baby outperforms SimVLM~\cite{wang2021simvlm} and BLIP~\cite{li2022blip}, which are designed for generative tasks, in image caption generation. \baby also outperforms MDETR~\cite{kamath2021mdetr} and OFA~\cite{wang2022ofa}, which also leverage image annotations of objects and regions, in cross-modal understanding tasks. \babyL with 590M parameters performs competitively to CoCa~\cite{yu2022coca} and BEiT-3~\cite{wang2022image} with 2B parameters, especially on image-text retrieval and visual reasoning. In summary, \baby makes a good trade-off between performance and model scale, as indicated in Figure \ref{Fig:intro} (a). Besides, we find that by training with large-scale image-text pairs, \baby learns to locate diverse fine-grained visual concepts in open-domain images, such as different sodas, cars, characters and celebrities. Second, \baby is also the new SoTA pre-trained model on video-text tasks, including video-text retrieval and video VQA, as shown in Figure \ref{Fig:intro} (b). Most existing VLMs only tackle image-text tasks, but \baby with a unified framework achieves SoTA performances on both types of tasks. Third, to verify the flexibility of the modular design, we replace the text encoder of \baby with XLM-R~\cite{conneau2020unsupervised}, a multilingual text encoder, after vision-language pre-training on English data. As indicated in Figure~\ref{Fig:intro} (c), \baby outperforms SoTA multilingual multi-modal pre-training methods that need multilingual image-text pairs~\cite{zhou2021uc2, jain2021mural} and multilingual sentence pairs~\cite{cclm} which are costly to collect. 


The contributions of this paper are as follows:
\begin{itemize}

\item We propose to learn multi-grained vision language alignments by a unified pre-training framework that learns multi-grained aligning and multi-grained localization simultaneously. Based on it, we present \baby, an all-in-one pre-trained VLM that can handle both image-text and video-text tasks. 

\item Experiment results show that \baby is the best model on base and large scale on both image-text and video-text benchmarks. Furthermore, the results confirm that the proposed framework for multi-grained vision language pre-training is scalable to massive data and larger model sizes.


\item  We reveal the potential of the modular design of \baby, showing that it can be utilized in other languages or domains. By replacing the text encoder with XLM-R after pre-training on English data, \baby outperforms SoTA methods on multi-lingual multi-modal tasks. 


\end{itemize}




 
\section{Related Work}
\label{sec:related}

\subsection{Image-Text Pre-training} 



The existing work on image-text pre-training typically falls into two categories: fine-grained and coarse-grained. Fine-grained approaches ~\cite{tan2019lxmert, lu2019vilbert, li2019visualbert, gan2020large, chen2020uniter, li2020oscar, zhang2021vinvl} utilize a pre-trained object detector~\cite{ren2015faster, anderson2018bottom} as the image encoder, which is trained on annotations of common objects, e.g. COCO~\cite{lin2014microsoft} and Visual Genome~\cite{krishna2016visual}. An object detector first identifies all regions that probably contain an object, then conducts object classification on each region. An image is then represented by dozens of object-centric features of the identified regions. However, object-centric features cannot represent relations among multiple objects in different regions. Therefore, it is difficult for this approach to effectively encode multi-grained visual concepts. Moreover, object detectors can only detect common objects, e.g. only 80 object categories for the COCO dataset. Thus, it is suboptimal to apply this approach to encode various visual concepts in real-world applications. For example, the approach cannot distinguish ``Pepsi'' from ``Coca Cola'' or ``Audi'' from ``BMW''. 

In contrast, the coarse-grained approaches build VLMs by extracting and encoding overall image features with convolutional network~\cite{jiang2020defense, huang2020pixel} or vision transformer~\cite{kim2021vilt, li2021align}. While being more efficient, the performance of the coarse-grained approach is usually not as good as the fine-grained approach since the latter leverages vision language alignments at the object level, which are shown to be critical for downstream V+L tasks. However, with advanced vision transformers, e.g. Swin-Transformer~\cite{liu2021swin} and BEiT-2~\cite{peng2022beit}, recent methods such as METER~\cite{dou2021empirical} and VL-BEiT~\cite{bao2022vl}, can outperform strongest fine-grained approach VinVL~\cite{zhang2021vinvl}. 


There also emerge some methods attempting to learn both object-level and image-level alignments. However, these approaches still rely on object detectors and thus suffer from the aforementioned problems. For example, E2E-VLP~\cite{xu2021e2e} adds an end-to-end object detection module (i.e. DETR~\cite{carion2020end}). KD-VLP~\cite{liu2021kd} relies on external object detectors to perform object knowledge distillation. Different from these approaches, our framework for multi-grained vision language pre-training does not rely on object detection, and it learns vision language alignments not restricted to object-level or image-level in a unified way. 



\subsection{Video-Text Pre-training}
Most existing VLMs only tackle image-text tasks. Only a few VLMs work on video-text pre-training. Since a video consists of multiple images, video-text models usually share many similarities with image-text models in both model architecture and training objectives. Though video-text pre-training shares similarities with image-text pre-training, no existing method can achieve SoTA performances on both types of tasks. Representative work on video-text pre-training including ClipBERT~\cite{lei2021less}, Frozen~\cite{bain2021frozen}, ALPRO~\cite{li2022align}, VIOLET~\cite{fu2021violet}, and All-in-one~\cite{wang2022all}. There are other methods optimized specifically for a downstream task, for either video-text retrieval~\cite{xue2022clip,min2022hunyuan_tvr} or video question answering~\cite{yang2022zero}. Recently, OmniVL~\cite{wang2022omnivl} is proposed to support both image-text tasks and video-text tasks. It utilizes 3D patch embeddings for videos and 2D patch embeddings for images, and adopts TimeSformer~\cite{bertasius2021space} for vision encoding. 


\subsection{Multilingual Multi-modal Pre-training}
Multilingual multi-modal pre-training aims to make multi-modal models applicable to non-English texts. While appealing, multi-lingual multi-modal pre-training has its own challenges. Unlike multi-lingual pre-training and multi-modal pre-training where a relatively large amount of parallel data is available, there exist only a few multi-lingual multi-modal corpora and their language coverage is also limited. Therefore, ~\cite{ni2021m3p} utilizes 101G texts covering 100 languages for pre-training. It makes English a pivot and alternates between English-only vision-language pre-training and multi-lingual masked language modeling. Differently, UC~\cite{zhou2021uc2} translates image-text pairs in English into five different languages and uses all the data for pre-training. MURAL~\cite{jain2021mural} collects large-scale image-text pairs in 110 languages. CCLM~\cite{cclm} utilizes parallel multilingual text pairs and proposes a simple framework that unifies cross-lingual and cross-modal pre-training with shared architecture and objectives. All these methods require extra data to perform multilingual multi-modal pre-training. In contrast, we show that \baby can adapt to multilingual V+L tasks without the need for multilingual multi-modal pre-training process by exploiting the potential of its modular architecture. \begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.6\columnwidth]{pics/x2vlm_tpami_visionE.pdf}}
\caption{\textbf{Unified vision encoding} For images, we extract the subset of patch features from the vision transformer to represent an image and objects/regions in the image. For videos, each frame is first encoded independently, and than a light-weight non-parametric temporal modeling layer is applied across frames. }
\vspace{-0.5cm}
\label{Fig:vision}
\end{center}
\end{figure}

\section{Method}

\subsection{Overview}




\noindent\textbf{Architecture}: \baby consists of vision, text, and multi-modal fusion modules. All modules are based on Transformer~\cite{vaswani2017attention}. The fusion module takes text features as input and fuses the vision features with the text features through cross-attention at each layer, where the text features work as the queries and the vision features work as the keys and values. In pre-training, the three modules work as encoders, while the text and fusion modules can also be adapted for generation tasks if applying left-to-right self-attention as shown in our experiments for image caption generation. Figure~\ref{Fig:model} illustrates the architecture of \baby and the way we perform multi-grained aligning and multi-grained localization. 


\noindent\textbf{Data}: \baby is a unified approach that associates all visual concepts with text descriptions, including image-text pairs, video-text pairs, and image annotations of objects and regions. That is to say, an image may contain more than one visual concept and each of them is associated with a text description, denoted as .  are the image annotations of objects or regions. Here,  is an object or region in a bounding box  represented by the normalized center coordinates, width, and height of the box. When the image itself represents a visual concept, .  for objects are originally object labels. If an object annotation contains object attributes, e.g. color, we concatenate the attribute with the object label as the text description.  for regions are phrases that describe the regions. Note that, as listed in Table~\ref{tbl:data}, some images do not have associated texts, i.e.,  is NaN, and some images do not have annotations, i.e., . \textbf{Nevertheless, we mix all types of data in a training batch, and thus for each training iteration, we optimize the model by multi-grained aligning and multi-grained localization simultaneously. }














\subsection{Unified Vision Encoding}  


\baby unifies image and video encoding, as illustrated in Figure~\ref{Fig:vision}. Irrespective of the inputs, the vision module of \baby produces hidden states in the latent feature space of the vision transformer. As a result, image-text pre-training and fine-grained pre-training mutually reinforce one another. Moreover, the capability of image understanding can be better transferred for video comprehension. 





\noindent\textbf{Visual Concept Representation} \baby proposes an efficient way to obtain all multi-grained visual concepts in an image with only one forward pass of the vision transformer. First, we process an image into patch features. Then, \baby represents an object or a region, e.g. , that corresponds to a set of patches in the bounding box, e.g. , by aggregating information among the patches as illustrated in Figure~\ref{Fig:vision}. Specifically, we flatten the corresponding patch features while keeping their original positions. Then, we calculate the average of the patch features as the [CLS] patch and prepend it. Accordingly, the representation of the entire image  is obtained by aggregating information among all the patches. 


\noindent\textbf{Video Representation} Since a video consists of multiple images, to leverage large-scale image-text pre-training for better video understanding, we unify video encoding and image encoding in a simple and efficient way. First, we sample one frame per second for videos. Then, for each training iteration, we randomly sample a few frames of a video. The vision encoder will encode the frames into patch features respectively. Finally, we add temporal information to the patch features of each frame and calculate the average in temporal dimension to represent the video. By doing so, a video is encoded by a sequence of patch features the same as an object/region/image, and thus we can apply a unified pre-training framework for both video-text pairs and object/region/image-text pairs. 





\subsection{Multi-Grained Vision Language Pre-training}
\label{sec:xvlm}


\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{pics/x2vlm_tpami_model_pic.pdf}}
\caption{\textbf{Illustration of the proposed multi-grained vision language pre-training.} \baby consists of vision, text, and fusion modules. After encoding visual concepts (a) and text inputs (b), multi-grained vision features are then paired with corresponding text features for multi-grained aligning (c). Besides, the image is paired with different textual descriptions for multi-grained localization to predict the bounding box for each visual concept (d). All the datasets we used are publicly available (see Section~\ref{sec:pretraindata}).
}
\vspace{-0.5cm}
\label{Fig:model}
\end{center}
\end{figure*}

We mix all types of data in a training batch, and thus for each training iteration, as shown in Figure~\ref{Fig:model}, we optimize \baby by two objectives simultaneously: 1) learning multi-grained alignments between visual concepts and texts; 2) locating multi-grained visual concepts in images given different text descriptions. 


\subsubsection{Multi-Grained Aligning}
Since we have associated all visual concepts with text descriptions, we propose to align the multi-grained visual concepts with the corresponding texts. Specifically, after encoding visual concepts by the aforementioned method, we align the vision features in multiple granularities with the corresponding text features in the same way. We simply choose three losses for optimization, including contrastive loss, matching loss, and MLM loss. These losses have been well-studied by previous work~\cite{chen2020uniter, radford2021learning, li2021align}, but we propose to employ them on the visual concept-to-text level. Note that  in this section represents a visual concept, including an object, region, image, or video. 


We apply contrastive loss to predict (visual concept, text) pairs from in-batch negatives. Given a pair ,  is the positive example for , and we treat the other  texts within the mini-batch as negative examples. First, we define the similarity by: 

where  and  are the output \texttt{[CLS]} embedding of the vision encoder and the text encoder respectively.  and  are transformations that map the \texttt{[CLS]} embeddings to normalized lower-dimensional representations. Based on it, we calculate the in-batch vision-to-text similarity as: 

Similarly, the text-to-vision similarity is: 

where  is a learnable temperature parameter. Let  and  denote the ground-truth one-hot similarity, in which only the positive pair has the probability of one. Finally, the contrastive loss is defined as the cross-entropy  between  and : 





We also utilize the matching loss to determine whether a pair of visual concept and text is matched. For each visual concept in a mini-batch, we sample an in-batch hard negative text by following  in Equation~\ref{eq:pi2t}. Texts that are more relevant to the concept are more likely to be sampled. We also sample one hard negative visual concept for each text. We then put the pairs as inputs for the fusion module, and then we use , the output \texttt{[CLS]} embedding of the fusion module, to predict the matching probability , and the loss is:

where  is a 2-dimensional one-hot vector representing the ground-truth label.


Furthermore, we apply masked language modeling loss to predict the masked words in the text based on the visual concept. We randomly mask out the input tokens with a probability of 40\%, and the replacements are 10\% random tokens, 10\% unchanged, and 80\% \texttt{[MASK]}. We use the fusion encoder's outputs and append a linear layer followed by softmax for prediction. Let  denote a masked text, and  denote the probability of the masked token  predicted by the fusion module. We minimize the cross-entropy loss:

where  is a one-hot distribution in which the ground-truth token  has the probability of one. 


\subsubsection{Multi-Grained Localization}
We have aligned visual concepts with texts in different granularity. We further optimize \baby by training it to locate different visual concepts in the same image given corresponding text descriptions. Specifically, we introduce bounding box prediction task into vision language pre-training, where the model is asked to predict the bounding box  of a visual concept : 

where Sigmoid is for normalization, MLP denotes multi-layer perceptron, and  is the output \texttt{[CLS]} embedding of the fusion module given the features of  (the entire image) and  (the description of the visual concept). 


For bounding box prediction,  is the most commonly-used loss. However, it has different scales for small and large boxes, even if their relative errors are similar. To mitigate this issue, we use a linear combination of the  loss and the generalized Intersection over Union (IoU) loss~\cite{rezatofighi2019generalized}, which is scale-invariant. The overall loss is defined as: 



Finally, the pre-training objective of \baby is defined as:



 
\section{Experiment}



\begin{table}[h]
\small
\centering	
\resizebox{0.7\columnwidth}{!}{\begin{tabular}	{ l | l |  l | l | l}
\toprule
Dataset & \# Images & \# Captions & \# Objects & \# Regions \\
\midrule
COCO & 0.11M & 0.55M & 0.45M & -\\
VG & 0.10M & - & 2.0M & 3.7M \\
SBU & 0.86M & 0.86M & - & -\\
CC-3M & 2.9M & 2.9M & - & - \\
\midrule
CC-12M & 11.1M & 11.1M & - & - \\
Objects365 & 0.58M  & - & 2.0M & - \\ 
OpenImages & 1.7M  & - & 4.2M & - \\
LAION & 1.3B & 1.3B & - & - \\
WebVid2.5M & 2.5M & 2.5M & - & - \\
Howto100M & 1.7M & 1.7M & - & - \\
YTT180M & 5.3M & 5.3M & - & - \\ 
 \bottomrule
\end{tabular}
}
\vspace{0.2cm}
\caption
{
\textbf{Statistics of the pre-training datasets.} We pre-train \baby with two sets of data: one contains COCO, VG, SBU, and CC-3M, where the total number of images is 4M; the other one includes more noisy image-text pairs and video-text pairs.  
}  
\label{tbl:data}
\end{table}


\begin{table}[t]
\centering
\resizebox{0.7\columnwidth}{!}{\begin{tabular}{@{\hskip1pt}l@{\hskip1pt} @{\hskip1pt}c@{\hskip2pt} @{\hskip2pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip2pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip2pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} }
\toprule
\multirow{3}{*}{Model} & \multirow{3}{*}{Hidden} & \multicolumn{2}{c}{Vision} & \multicolumn{2}{c}{Text} & \multicolumn{2}{c}{Fusion} \\ 
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
 &  & Layers & Params & Layers & Params & Layers & Params \\ 
\midrule
\babyB & 768 & 12 & 86M & 12 & 111M & 6 & 55M \\
\babyL & 1024 & 24 & 303M & 12 & 190M & 6 & 95M \\ 
\bottomrule
\end{tabular}
}
\vspace{0.2cm}
\caption{\textbf{Size variants of \babyx.} All modules consist of transformer layers.}
\label{tab:modelsize}
\end{table}


\begin{table*}[t]
\centering
\resizebox{1\columnwidth}{!}{\begin{tabular}{@{\hskip1pt}l@{\hskip1pt} @{\hskip1pt}c@{\hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} | @{ \hskip2pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} }
\toprule
\multirow{3}{*}{  Model} & \multirow{3}{*}{ \# Params} & \multicolumn{6}{c}{ MSCOCO (5K test set)} & \multicolumn{6}{c}{ Flickr30K (1K test set)} \\
 & & \multicolumn{3}{c}{ TR} & \multicolumn{3}{c}{  IR} & \multicolumn{3}{c}{ TR} & \multicolumn{3}{c}{ IR} \\
 & & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
\midrule
\multicolumn{14}{l}{\textit{Models Pretrained on COCO, VG, SBU and CC datasets (4M)}} \\
ALBEF & 210M & 73.1 & 91.4 & 96.0 & 56.8 & 81.5 & 89.2 & 94.3 & 99.4 & 99.8 &82.8 & 96.7 & 98.4 \\
VLMo & 175M &  74.8 & 93.1 & 96.9 &  57.2 &  82.6 & 89.8 & 92.3 & 99.4 & 99.9 & 79.3 & 95.7 & 97.8 \\
VL-BEiT & 175M & 79.5 & - & - & 61.5 & - & - & 95.8 & - & - & 83.9 & - & - \\
OmniVL & 288M & 76.8 & 93.6 & 97.3 & 58.5 & 82.6 & 89.5 & 94.9 & 99.6 & 99.9 & 83.4 & 97.0 & 98.6 \\


\bf \babyx & 255M & \bf 80.5 & \bf 95.5 & \bf 97.8 & \bf 62.7 & \bf 84.7 & \bf 90.7 & \bf 97.4 & \bf 99.9 & \bf 100 & \bf 90.0 & \bf 98.6 & \bf 99.3 \\

\midrule
VLMo & 562M & 78.2 & 94.4 & 97.4 & 60.6 & 84.4 & 91.0 & 95.3 & 99.9 & 100 &  84.5 &  97.3 &  98.6 \\

\bf \babyx & 593M & \bf 82.3 & \bf 96.2 & \bf 98.3 & \bf 65.2 & \bf 86.4 & \bf 91.9 & \bf 99.1 & \bf 100 & \bf 100 & \bf 91.1 & \bf 98.6 & \bf 99.4 \\
\midrule
\midrule
\multicolumn{14}{l}{\textit{ Models Pretrained on More Data}} \rule{0pt}{2.5ex} \\

BLIP & 240M  & 81.9 & 95.4 & 97.8 & 64.3 & 85.7 & 91.5 & 97.3 & 99.9 & 100 & 87.3 & 97.6 & 98.9 \\
OmniVL & 288M & 82.1 & 95.9 & 98.1 & 64.8 & 86.1 & 91.6 & 97.3 & 99.9 & 100 & 87.9 & 97.8 & 99.1 \\

\bf \babyx & 255M & \bf 83.5 & \bf 96.3 & \bf 98.5 & \bf 66.2 & \bf 87.1 & \bf 92.2 & \bf 98.5 & \bf 100 & \bf 100 & \bf 90.4 & \bf 98.2 & \bf 99.3 \\

\midrule
ALIGN & 490M & 77.0 & 93.5 & 96.9 & 59.9 & 83.3 & 89.8 & 95.3 & 99.8 & 100 & 84.9 & 97.4 & 98.6 \\
FLIP & 420M & 78.9 & 94.4 &  97.4 & 61.2 & 84.3 & 90.6 & 96.6 & 100 & 100 &  87.1 & 97.7 & 99.1 \\

BLIP & 452M & 82.4 & 95.4 & 97.9 & 65.1 & 86.3 & 91.8 & 97.4 & 99.8 & 99.9 & 87.6 & 97.7 & 99.0 \\

\bf \babyx & 593M & \bf 84.4 & \bf 96.5 & \bf 98.5 & \bf 67.7 & \bf 87.5 & \bf 92.5 & \bf 98.8 & \bf 100 & \bf 100 & \bf 91.8 & \bf 98.6 & \bf 99.5 \\



\bottomrule
\end{tabular}
}
\caption{\textbf{Results of image-to-text retrieval (TR) and
text-to-image retrieval (IR) on COCO and Flickr30K.}  denotes dual-encoder retrieval models, and others use a fusion module to re-rank top-k candidates following ALBEF~\cite{li2021align}. 
}
\label{tbl:retrieval}
\end{table*}



\subsection{Pre-training Datasets}
\label{sec:pretraindata}


We pre-train \baby with two sets of data. The 4M pre-training dataset consists of two in-domain datasets, COCO~\cite{lin2014microsoft} and Visual Genome (VG)~\cite{krishna2016visual}, and two out-of-domain datasets, SBU Captions~\cite{ordonez2011im2text} and Conceptual Captions (CC)~\cite{sharma2018conceptual}. This pre-training dataset is widely utilized by previous work, and thus we choose this setting to make a fair comparison with other methods. We also include annotations for COCO and VG images from RefCOCO~\cite{yu2016modeling}, GQA~\cite{hudson2019gqa}, and Flickr entities~\cite{plummer2015flickr30k} following OFA~\cite{wang2022ofa} and MDETR~\cite{kamath2021mdetr}.


Then, we scale up the pre-training dataset by including out-of-domain and much noisier image-text pairs from Conceptual 12M dataset (CC-12M)~\cite{changpinyo2021conceptual} and LAION~\cite{schuhmann2022laion}, and object annotations from Objects365~\cite{shao2019objects365} and OpenImages~\cite{kuznetsova2018open}. Besides, to support video-text downstream tasks, we include video-text pairs from WebVid2.5M~\cite{bain2021frozen}, Howto100M~\cite{miech2019howto100m}, and YT-Temporal 180M~\cite{zellers2021merlot} for pre-training. Note that all the datasets we used are public available and have been exploited in previous work~\cite{li2021align,zhang2021vinvl,li2022blip,wang2022ofa,wang2022all}. Besides, since most downstream tasks are built on top of COCO and VG, we exclude all images that also appear in the test sets of downstream tasks to avoid information leak. We give data filtering details in Appendix. 




\subsection{Implementation Details}
\label{sec:details}

Table~\ref{tab:modelsize} lists the parameters of \baby. Considering the trade-off between performance and model scale~\cite{wang2022efficientvlm}, \babyL also uses a 12L text encoder. The vision encoder is initialized with BEiT-2~\cite{peng2022beit}. The text encoder is initialized with BERT~\cite{devlin2019bert}. \baby is pre-trained at image resolution of  using  patch size. We mix all types of data in a training batch, and thus for each training iteration, we optimize the model by multi-grained aligning and multi-grained localization simultaneously. With 4M data, we pre-train \babyB for 500K steps with a batch size of 1024 on 8 A100 and \babyL for 250K steps on 16 A100, which takes  week. The learning rate of \babyB is warmed-up to  in the first 2500 steps and decayed following a linear schedule. The learning rate is  for \babyL. With large-scale data, training \baby takes 2-3 weeks on 32 A100 for the base model and 64 A100 for the large model. We describe the implementation details in Appendix. 


\begin{table}[t]
\centering
\resizebox{0.6\columnwidth}{!}{\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{ } & \multirow{2}{*}{\# Params} & \multicolumn{2}{c}{ MSCOCO} & \multicolumn{2}{c}{ Flickr30K} \\
 & & TR &  IR &  TR & IR \\
\midrule
BEiT-3 & 1.9B & \bf 84.8 & 67.2 & 98.0 & 90.3 \\
\bf \babyx & 593M & 84.4 & \bf 67.7 & \bf 98.8 & \bf 91.8 \\
\bottomrule
\end{tabular}
}
\vspace{0.2cm}
\caption{\baby compared with the SoTA giant model, BEiT-3, on image-text retrieval benchmarks. We report Recall@1 for both image-to-text retrieval (TR) and text-to-image retrieval (IR).
}
\label{tbl:beit3}
\end{table}




\begin{table*}[!t]


	\centering	
\resizebox{1\columnwidth}{!}{\begin{tabular}	{@{\hskip1pt}l@{\hskip1pt} @{\hskip1pt}c@{\hskip1pt} @{\hskip5pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip5pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip5pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip5pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt}} 
	\toprule
	 \multirow{2}{*}{Method} & \multirow{2}{*}{\# Params} & \multicolumn{2}{c}{VQA} & \multicolumn{2}{c}{NLVR2} & \multicolumn{3}{c}{RefCOCO+} & \multicolumn{2}{c}{COCO Caption} \\
	  & & test-dev & test-std & dev & test-P & val & testA & testB & BLEU@4 & CIDEr \\
\midrule
\multicolumn{10}{l}{\textit{Models Pretrained on COCO, VG, SBU and CC datasets (4M)}} \\
ALBEF & 210M & 74.5 & 74.7 & 80.2 & 80.5 & - & - & - & - & - \\
VLMo & 175M & 76.6 & 76.9 & 82.8 & 83.3 & - & - & - & - & - \\
METER & 341M & 77.7 & 77.6 & 82.3 & 83.1 & - & - & - & - & - \\
VL-BEiT & 175M & 77.5 & 77.8 & 81.9 & 82.7 & - & - & - & - & - \\



\bf \babyx & 255M & \bf 79.2 & \bf 79.3 & \bf 85.9 & \bf 86.1 & \bf 85.4 & \bf 89.2 & \bf 77.3 & \bf 41.0 & \bf 133.6 \\
\midrule

VLMo & 562M & 79.9 & 80.0 & 85.6 & 86.9 & - & - & - & - & -  \\

\bf \babyx & 593M & \bf 80.5 & \bf 80.5 & \bf 87.2 & \bf 87.6 & \bf 86.9 & \bf 90.1 & \bf 80.2 & \bf 42.0 & \bf 136.7 \\

\midrule
\midrule

\multicolumn{10}{l}{\textit{Models Pretrained on More Data}} \\
OmniVL & 288M & 78.3 & 78.4 & - & - & - & - & - & 39.8 & 133.9 \\
SimVLM & 273M  & 77.9 & 78.1 & 81.7 & 81.8 & - & - & - & 39.0 & 134.8 \\
OFA & 182M & 78.0 & 78.1 & - & - & 81.4 & 87.2 & 74.3 & 41.0 & \bf 138.2 \\
BLIP & 240M & 78.2 & 78.2 & 82.5 & 83.1 & - & - & - & 39.4 & 131.4 \\

\bf \babyx & 255M & \bf 80.4 & \bf 80.2 & \bf 86.2 & \bf 87.0 & \bf 85.2 & \bf 90.3 & \bf 78.4 & \bf 41.7 & 136.1 \\

\midrule

SimVLM & 783M & 79.3 & 79.6 & 84.1 & 84.8 & - & - & - & 40.3 & \bf 142.6 \\

OFA & 472M & 80.3 & 80.5 & - & - & 85.8 & 89.9 & 79.2 & 42.4 & 142.2 \\

\bf \babyx & 593M & \bf 81.9 & \bf 81.8 & \bf 88.7 & \bf 89.4 & \bf 87.6 & \bf 92.1 & \bf 81.8 & \bf 42.6 & 139.1 \\



\bottomrule  	  
\end{tabular}
}
\caption
{
\textbf{Results on downstream image-text tasks}, including visual question answering (VQA), visual reasoning (NLVR2), visual grounding (RefCOCO+), and image caption generation (COCO Caption). }
\vspace{-0.3cm}
\label{tbl:results}
\end{table*}






\subsection{Image-Text Downstream Tasks}
We compare \baby with the most well-known state-of-the-art approaches on five widely used image-text downstream tasks. In general, we follow the settings in the previous work on fine-tuning. We describe how we implement fine-tuning as follows. 


\subsubsection{Image-Text Retrieval}

We evaluate \baby on both MSCOCO and Flickr30K~\cite{plummer2015flickr30k} datasets. We adopt the widely used Karpathy split~\cite{karpathy2015deep} for both datasets. We optimize  and  for fine-tuning. We set the batch size to 1024. The resolution of input images is set to 384x384. Following the previous work~\cite{li2021align}, \baby first encodes images and texts separately and calculates in-batch text-to-image and image-to-text similarities to obtain the top- candidates, and then uses the fusion encoder to re-rank the candidates.  is set to 80 for the MSCOCO dataset and 32 for Flickr30K.


Table~\ref{tbl:retrieval} shows that \baby achieves SoTA results on image-text retrieval tasks especially on Flickr30K benchmark even though existing approaches either have more model parameters or more training data. Concretely, \babyB outperforms FLIP~\cite{yao2021filip}, BLIP and BLIP which also exploits large-scale image-text pairs from LAION, and \babyL further improves the image-text retrieval performance. Compared to OmniVL which also supports both image-text tasks and video-text tasks, \babyB substantially outperforms it when pre-trained with the 4M data or with more data. These results validate the advantage of learning multi-grained vision language alignments. 




We also compare \babyL with BEiT-3, a giant foundation model with 1.9B model parameters in Table~\ref{tbl:beit3}. Experimental results show that though being much smaller, \babyL has a comparable or even better performance compared with BEiT-3. Moreover, as shown in Table~\ref{tbl:retrieval}, \babyB substantially outperforms VL-BEiT which is the base version of BEiT-3 in the 4M setting. On the other hand, when comparing \baby's performances in different settings in Table~\ref{tbl:retrieval}, we can see that the proposed framework for multi-grained vision language pre-training has good scalability which can benefit from a larger model size and large-scale out-of-domain image-text pairs. 



\subsubsection{Visual Question Answering}
The task requires the model to predict an answer given an image and a question. We evaluate \baby on the VQA v2.0 dataset~\cite{goyal2017making}. Following existing methods~\cite{tan2019lxmert, chen2020uniter, li2021align}, we use both train and validation sets for training and include additional question-answer pairs from Visual Genome. Following ALBEF, we use a six-layer Transformer decoder to generate answers based on the outputs of the fusion module. Then, the model is fine-tuned by optimizing the auto-regressive loss. During inference, we constrain the decoder to only generate from the 3,129 candidate answers to make a fair comparison with existing methods. Note that there is a NULL answer. Thus, the actual number of candidate answers is 3,128. Following previous work~\cite{wang2022ofa,yu2022coca,wang2022image}, the resolution of input images is set to 768x768.


  
We report the experimental results of VQA in Table~\ref{tbl:results}. We can see that \babyB and \babyL outperforms other approaches with similar scale of model size. Specifically, \babyB substantially outperforms ALBEF, VLMo, METER, and VL-BEiT in the 4M setting. Besides, with more pre-training data, \babyB outperforms BLIP which also exploits large-scale image-text pairs from LAION. Compared to OmniVL which also supports both image-text tasks and video-text tasks, \babyB substantially outperforms it, achieving an absolute improvement of 2\%. \baby also substantially outperforms SimVLM and OFA on both base and large scales. SimVLM utilizes an in-house 1.8B image-text dataset. OFA also leverages image annotations of objects and regions the same as \baby. These results confirm the effectiveness of the proposed framework for multi-grained vision language pre-training. Furthermore, when comparing \baby's performances in different settings in Table~\ref{tbl:results}, we can see that the proposed framework has good scalability which can benefit from a larger model size. When pre-training a larger model with more data, the performance improvement is even more remarkable. 


\subsubsection{Visual Reasoning}
We evaluate \baby on widely used benchmark NLVR2~\cite{suhr2018corpus}. The task lets the model determine whether a text describes the relations between two images. Following previous work~\cite{wang2021vlmo, bao2022vl}, we formulate the triplet input to two image-text pairs, each containing the text description and one image. We then concatenate the final output [CLS] features of the fusion module of the two pairs to predict the label. The resolution of input images is set to 384x384. Given the results in Table~\ref{tbl:results}, we can observe that the visual reasoning task benefits more from the model size than the pre-training data scale. Comparing to other base-scale models, e.g. ALBEF, VLMo, VL-BEiT, SimVLM, and BLIP, \babyB has much better performance, achieving  3-4\% absolute improvement, no matter when pre-training with 4M data or with much more noisy data. \babyL also substantially outperforms other large-scale models, including VLMo and SimVLM. 


\subsubsection{Visual Grounding}
We evaluate \baby on RefCOCO+~\cite{yu2016modeling}. Given an image as the input and a text description as the query, the final output [CLS] features of the fusion module is utilized to predict the bounding box of the visual concept. The resolution of input images is set to 384x384. As indicated in Table~\ref{tbl:results}, \baby outperforms OFA~\cite{wang2022ofa} which also utilizes image annotations of objects and regions for pre-training. Differently, OFA with an encoder-decoder architecture formulates all the data in the form of sequence-to-sequence. Furthermore, \baby for general V+L purposes outperforms MDETR~\cite{kamath2021mdetr} specialized for visual grounding tasks, achieving absolute improvements of  (average on metrics). These results confirm the effectiveness of the proposed multi-grained vision language pre-training compared to other approaches that also leverage image annotations of objects and regions. 



\subsubsection{Image Captioning}
The task requires a model to generate text descriptions of input images. Though \baby is more for cross-modal understanding, we also evaluate its generation performance on the COCO Captioning dataset~\cite{chen2015microsoft}. Following UniLM~\cite{dong2019unified} and BEiT-3, we use left-to-right MLM for generation. Specifically, we employ the text module and fusion module as decoder with left-to-right self-attention and adopt the method~\cite{zeng-nie-2021-investigation} that decreases finetune-generation discrepancy in MLM generation. The resolution of input images is set to 480x480. We report BLEU-4 and CIDEr scores on the Karparthy test split. As shown in Table~\ref{tbl:results}, \babyB outperforms BLIP~\cite{li2022blip} and SimVLM~\cite{wang2021simvlm} which are designed for generative tasks. BLIP exploits large-scale image-text pairs from LAION the same as \baby. SimVLM utilizes an in-house 1.8B image-text dataset. \baby also outperforms OFA~\cite{wang2022ofa} in image captioning in terms of BLEU-4. OFA has an encoder-decoder architecture and formulates all downstream tasks into sequence-to-sequence form for pre-training. In general, though \baby is more for cross-modal understanding, it performs competitively or sometimes better compared with SoTA generative methods. 


\begin{table}[t]
\centering
\resizebox{0.8\columnwidth}{!}{\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{ } & \multicolumn{3}{c}{\textbf{Winoground}} & \multicolumn{4}{c}{\textbf{OVAD}}\\
 & Group & Text & Image & All & Head & Medium & Tail \\
\midrule

Random & 12.5 & 25.0 & 25.0 & 8.6 & 36.0 & 7.3 & 0.6 \\

\midrule
CLIP & 8.0 & 30.7 & 10.5 & 17.0 & 44.3 & 18.4 & 5.5 \\
ALBEF & 11.0 & 29.2 & 15.5 & 15.6 & 43.1 & 17.3 & 3.7 \\
BLIP & 11.7 & 35.5 & 15.0 & 24.3 & 51.0 & 28.5 & 9.7 \\
BLIP-2 & 18.2 & 43.0 & 22.0 & - & - & - & - \\
UNITER & 10.5 & 38.0 & 14.0 & - & - & - & - \\
PEVL & 12.2 & 33.2 & 15.7 & - & - & - & - \\
OVADetector & - & - & - & 21.4 & 48.0 & 26.9 & 5.2 \\ 
\midrule
\textbf{\babyx} & 22.5 & 46.3 & 25.3 & 24.0 & 51.9 & 29.7 & 7.2 \\
\textbf{\babyx} & 25.5 & 49.5 & 31.0 & 27.7 & 54.0 & 34.4 & 10.1 \\
\bf \babyx & 24.5 & 47.3 & 29.8 & 27.6 & 52.2 & 34.7 & 10.3 \\
\bf \babyx & \bf 25.8 & \bf 52.5 & \bf 32.5 & \bf 29.2 & \bf 55.1 & \bf 36.4 & \bf 11.3 \\

\bottomrule
\end{tabular}
}
\vspace{0.2cm}
\caption{\textbf{Zero-shot evaluation results on fine-grained downstream tasks}: Winoground, a fine-grained image-text matching task, and OVAD, Open-vocabulary Attribute Detection(mAP). 
}
\label{tbl:finegrained}
\end{table}



\subsubsection{Winoground}
Winoground~\cite{thrush2022winoground} presents a challenging task: given two images and two captions, the goal is to match them correctly, where the captions contain identical sets of words, but in a different order. Three metrics, namely Text (whether a model can match the correct caption for a given image), Image (vice versa), and Group (whether a model can match each pair), are used to evaluate the performance. Several competitive VLMs have been shown to perform close to or even below random chance. Experimental results in Table~\ref{tbl:finegrained} shows that even when trained on 4M data \baby substantially outperforms other models such as UNITER, which is based on a large pre-trained object detector, and BLIP-2, which consists of giant ViT and FlanT5 large language model~\cite{chung2022scaling} and is pre-trained on a much larger dataset with 129M images. Notably, the performance of \baby can be further improved by increasing the model size or pre-training dataset. 


\subsubsection{Open-vocabulary Attribute Detection}
Open-Vocabulary Attribute Detection (OVAD)~\cite{bravo2023open} aims to recognize an open set of objects in an image together with an open set of attributes for every object. We follow the benchmark and evaluate zero-shot performance of vision language models on attributes in the box-oracle setting. The experimental results are given in Table~\ref{tbl:finegrained}. \babyB pre-trained with 4M dataset has already been comparable to BLIP pre-trained with 129M dataset. \babyB also outperforms OVADetector which consists of a frozen CLIP text encoder and an object detector based on Faster-RCNN. Moreover, scaling \baby with larger pre-training datasets or larger model size consistently improve its performance as in other tasks. 




\begin{table}[t]
\centering
\resizebox{0.7\columnwidth}{!}{\begin{tabular}{@{\hskip2pt}l@{\hskip2pt} @{\hskip2pt}c@{\hskip2pt} @{\hskip5pt}c@{\hskip2pt} @{\hskip2pt}c@{\hskip5pt} @{\hskip2pt}c@{ \hskip2pt} @{\hskip2pt}c@{\hskip2pt} @{\hskip2pt}c@{\hskip2pt} }
\toprule
\multirow{2}{*}{\bf Model} & \multirow{2}{*}{\textbf{\# Params}} & \multicolumn{2}{c}{\bf Video-QA} & \multicolumn{3}{c}{\bf MSRVTT (1K test set)} \\
 & & MSRVTT & MSVD & R@1 & R@5 & R@10 \\
\midrule
ALPRO & 513M & 42.1 & 45.9 & - & - & - \\  VIOLET & 163M & 43.9 & 47.9 & - & - & - \\  All-in-one & 110M & 44.3 & 47.9 & 37.9 & 68.1 & 77.1  \\
OmniVL & 288M & 44.1 & 51.0 & 47.8 & 74.2 & 83.8  \\
\midrule
\bf \babyB & 255M & 45.0 & 52.8 & 47.6 & 74.1 & 84.2 \\
\bf \babyL & 593M & \bf 45.5 & \bf 54.6 & \bf 49.6 & \bf 76.7 & \bf 84.2 \\

\bottomrule
\end{tabular}
}
\vspace{0.2cm}
\caption{\textbf{Fine-tuning results on video-text tasks}, including video question answering on MSRVTT and MSVD datasets, and text-to-video retrieval on MSRVTT. We report classification accuracy for VQA and Recall@K for text-to-video retrieval. }
\label{tbl:video}
\end{table}








\subsection{Video-Text Downstream Tasks}

\baby unifies image-text and video-text pretraining. In this section, we evaluate \baby on three widely used video-text tasks, including both \textbf{Video-Text Retrieval} (MSRVTT~\cite{xu2016msr}) and \textbf{Video Question Answering} (MSRVTT-QA~\cite{xu2017video} and MSVD-QA~\cite{xu2017video}). We implement a text-to-video retrieval model the same as image-text retrieval by first calculating top- candidates and then re-ranking the candidates using the fusion module.  is set to 32. During training and inference, we sample five frames for each video. The image resolution is set to 384. Video question answering requires a model to generate an answer given a video and a question. Following previous work, we formulate it as a classification task given candidate answers. During training and inference, we sample five frames for each video in the MSRVTT dataset, and eight frames for the MSVD dataset. The image resolution is set to 320 for MSRVTT and 224 for MSVD. We compare with SoTA video-language foundation models: ALPRO~\cite{li2022align}, VIOLET~\cite{fu2021violet}, and All-in-one~\cite{wang2022all}. We also compare \baby with OmniVL which also supports both image-text tasks and video-text tasks. There are other methods optimized specifically for either video-text retrieval~\cite{xue2022clip,min2022hunyuan_tvr} or video question answering~\cite{yang2022zero}, which are not included in our comparison. 


The results are given in Table~\ref{tbl:video}. We can see that \babyB outperforms previous video-language foundation models on both video question answering and text-to-video retrieval, and \babyL further advance the performance, achieving new SoTA results of video-text pre-training. Besides, we compare \baby with OmniVL on both image-text (Table~\ref{tbl:retrieval} and Table~\ref{tbl:results}) and video-text benchmarks. In general, \babyB substantially outperforms OmniVL on all image-text downstream tasks, including image-text retrieval, visual question answering, image caption generation, and video question answering. 





\subsection{Multilingual Multi-modal Tasks}

\begin{table}[t]
\centering
\resizebox{0.7\columnwidth}{!}{\begin{tabular}{@{\hskip1pt}l@{\hskip5pt} @{\hskip5pt}c@{\hskip5pt} @{\hskip5pt}c@{ \hskip5pt} @{\hskip5pt}c@{ \hskip5pt} @{\hskip5pt}c@{ \hskip5pt} @{\hskip5pt}c@{ \hskip5pt} @{\hskip5pt}c@{ \hskip5pt} @{\hskip5pt}c@{ \hskip5pt}}

\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{ \textbf{Flickr30K}} & \multicolumn{3}{c}{\textbf{MSCOCO}} \\
& EN  & DE & FR & CS & EN & ZH & JA  \\ 
\midrule

\multicolumn{8}{l}{\textit{Models with Multilingual Multimodal Pretraining}} \\

 & 87.7 & 82.7 & 73.9 & 72.2 & 88.7 & 86.2 & 87.9  \\
UC & 88.2 & 84.5 & 83.9 & 81.2 & 88.1 & 89.8 & 87.5  \\ 
MURAL & 92.2 & 88.6 & 87.6 & 84.2 & 88.6 & - & 88.4 \\
MURAL & 93.8 & 90.4 & 89.9 & 87.1 & 92.3 & - & 91.6 \\
CCLM & 96.0 & 93.3 & 93.7 & 92.8 & 94.1 & 93.0 & 94.3 \\  
\midrule
\bf \babyB & 96.7 & 94.0 & 93.5 & 92.9 & 94.9 & 93.0 & 95.2 \\
\bf \babyL & \bf 97.1 & \bf 94.5 & \bf 95.1 & \bf 94.9 & \bf 95.3 & \bf 93.3 & \bf 95.6 \\
\bottomrule
\end{tabular}
}
\vspace{0.2cm}
\caption{\textbf{Results on multilingual multi-modal tasks}. All the methods except \baby rely on data that are costly to collect to perform multilingual multi-modal pre-training. We evaluate model performance in English (EN), German (DE), French (FR), Czech (CS), Chinese (ZH), and Japanese (JA). Following previous work, we report the average Recall@K for both image-to-text retrieval and text-to-image retrieval with K = 1, 5, 10. 
}
\label{tbl:mm}
\end{table}

In \baby architecture, text encoding, vision encoding and fusion are separated. Accordingly, the capabilities of vision encoding and fusion would be kept when replacing the text encoder, leading to an efficient adaptation of the new text encoder. Our study demonstrates that we can replace the text encoder after pre-training on English data with a language-specific or domain-specific one to support more applications in different languages or domains. Such a feature is hard to achieve with unified models like OFA and BEiT-3. For instance, BEiT-3 shares image, text, and fusion in a single Transformer, and thus replacing the text encoder can cause the capabilities of image encoding and fusion to be lost as well.


In this section, we replace the English text encoder of \baby with a multilingual text encoder XLM-R~\cite{conneau2020unsupervised}. Then, without a second step multilingual multi-modal pre-training, we simply finetune \baby on multilingual multi-modal downstream tasks. We choose Multi30K~\cite{young2014image} and multilingual MSCOCO~\cite{chen2015microsoft, yoshikawa2017stair, li2019coco} for evaluation since other multilingual multi-modal benchmarks such as IGLUE~\cite{bugliarello2022iglue} do not have a training set. Following previous work, we compute the average Recall@K for both image-to-text retrieval and text-to-image retrieval with K = 1, 5, 10, as the evaluation metric. 


We compare \baby with SoTA multilingual multi-modal pre-training methods. ~\cite{ni2021m3p} utilizes 101G texts covering 100 languages. UC~\cite{zhou2021uc2} translates image-text pairs in English into five different languages. MURAL~\cite{jain2021mural} collects large-scale image-text pairs in 110 languages. CCLM~\cite{cclm} utilizes parallel multilingual text pairs. All these methods rely on data that are costly to collect, while \baby relieves the multilingual multi-modal pre-training process. As shown in Table~\ref{tbl:mm}, \baby surprisingly outperforms all these methods in all six languages. The results indicate the potential of \baby being applicable to other domains or languages using a different text encoder without further pre-training. 





\subsection{Ablation Study} 

\begin{table}[t]

\centering	
\resizebox{0.8\columnwidth}{!}{\begin{tabular}	{l | c  c  c  c c c }
		\toprule	 	
	  & \multicolumn{2}{c}{\bf Flickr30K} & \bf VQA & \multicolumn{2}{c}{\bf RefCOCO+} & \bf OVAD \\
	 & TR & IR & test-dev & \multicolumn{1}{c}{testA} & \multicolumn{1}{c}{testB} & mAP \\
	\midrule
    Ours &  \bf 98.0 & 89.0 & \bf 78.4 & \bf 88.6 & \bf 76.7 & \bf 27.9 \\  

        w/o \baby & 96.0 & 85.9  & 77.6 & 78.6 & 59.0 & 20.6 \\ 

	\midrule
     
     w/o multi-grained align &  96.6  & 86.2 & 77.7 & 87.3 & 75.3 & 23.1 \\
     
     w/o bbox loss &  97.4 & \bf 89.6 & 78.2  & 83.6 & 66.0 & 26.8 \\

    \midrule
    
    w/o object data &  97.2 & 86.8 & 78.1 & 88.1 & 76.5 & 26.4 \\ 
    
    w/o region data &  97.8 & 89.0 & 78.0  & 84.8 & 69.3 & 22.3 \\ 

    
		\bottomrule
	\end{tabular}
	}
 \vspace{0.2cm}

	\caption
	{
		\textbf{Ablation study} of different components in the proposed framework and different types of data utilized. 
	}
	\label{tbl:ablation}

\end{table}

We conduct an in-depth ablation study and the results are given in Table~\ref{tbl:ablation}. We describe the experimental settings in Appendix. First, we investigate the role of different components in the proposed framework and conduct an ablation of multi-grained aligning and box prediction loss respectively. It should be noted that both object and region data are utilized in these two variants. The experimental results demonstrate that multi-grained aligning is more important for the model performance than the box prediction loss in all tasks, except the visual grounding task. The box prediction loss is critical to performance on visual grounding tasks, and combining the box prediction with multi-grained aligning further improves the model performances (Ours vs. w/o bbox loss). 


Second, we explore the impact of different types of annotation data used in \baby, and ablate object data and region data respectively. Both multi-grained aligning and box prediction loss are applied in these two variants. The results indicate that both types of annotations are important to performance. Object data improve image-text retrieval, while region data are critical to visual grounding and open-vocabulary attribute detection. Combining object and region data yields the best performances (Ours vs. w/o object and w/o region). The w/o \baby variant, which ablates both multi-grained aligning and box prediction loss, or both object and region data, has the worst performances in all the tasks. We also provide an ablation study on temporal modeling methods in Appendix. 





\subsection{Qualitative Study of Multi-Grained Alignments}

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{pics/x2vlm_capt_grounding.pdf}}
\caption{\textbf{Visualization of \baby generating image captions and locating visual concepts given manual input texts.} Only the image in the upper left corner is from the COCO dataset. Others are out-of-domain images from the internet. We give more examples in Appendix where we test \baby on images from robot grasping, e-commerce websites, and children's textbooks. }
\label{Fig:grounding}
\end{center}
\end{figure*}

In this section, we provide a qualitative study of what vision language alignments have been learned by \baby. To this end, we ask \baby to generate image captions to see if it can describe an image appropriately. We also ask \baby to locate visual concepts in an image given manual input descriptions to see whether it can understand fine-grained objects or regions in an image. We use \babyL fine-tuned on COCO Caption and RefCOCO+ dataset respectively for this evaluation. We visualize the results in Figure~\ref{Fig:grounding}, in which we choose some out-of-domain images from scientific posters, video games, cartoons, etc. 


The visualization examples show that \baby can describe all these images appropriately with a precise understanding of the main characters or objects and their relationships. When asking \baby to locate visual concepts in an image according to the descriptions we provided, we find that it can capture small objects in the background or objects which have been partially obscured. Moreover, \baby can recognize different brands of soda or cars or distinguish ``Luffy'' and ``Zoro'' from other cartoon characters. We give more examples in Appendix, where \baby can also recognize ``Albert Einstein'', ``Edison'', ``Ultraman'', and ``Doraemon''. It is surprising since the annotations of objects or regions we exploited in pre-training are only about common objects such as ``soda'', ``car'', or ``man''. The results indicate that \baby learns to localize diverse fine-grained visual concepts from large-scale noisy image-text pairs.  
\section{Conclusion and Discussion}
In this paper, we have proposed to learn multi-grained alignments between vision and language in pre-training. To this end, we have proposed a unified framework for multi-grained vision language pre-training that directly aligns the multi-grained vision features with the paired text features and simultaneously locates multi-grained visual concepts in the same image given different text descriptions. Based on it, we have presented \baby, an all-in-one pre-trained VLM with a flexible modular architecture, in which we have further unified image encoding and video encoding to make it able to handle both image-text tasks and video-text tasks. 


We have conducted extensive experiments to verify the effectiveness of \baby. The results have shown that \baby substantially outperforms SoTA image-text pre-training methods on base and large scale in many downstream image-text tasks, making a good trade-off between performance and model scale. \baby is also the new SoTA pre-trained model on video-text tasks, including video-text retrieval and video VQA. Experimental results also show that the proposed framework for multi-grained vision language pre-training is scalable to massive data and a larger model size. Moreover, we have revealed the potential of the modular design of \baby, showing it can be utilized in other languages or domains. By replacing the text encoder with XLM-R after pre-training on English data, \baby outperforms SoTA methods on multi-lingual multi-modal tasks. 


We also have provided an in-depth ablation study to investigate the role of different components in the proposed framework. Experimental results have shown that both multi-grained localization and multi-grained aligning are critical components of the proposed method. Furthermore, we have conducted a qualitative study of what vision language alignments have been learned by \baby. We have found that by training with large-scale image-text pairs, \baby learns to locate diverse fine-grained visual concepts in open-domain images, such as different brands of sodas, cars, and characters or celebrities.
 
\section*{Acknowledgements}
We sincerely thank our colleagues at ByteDance, Tao Kong, for his constructive and detailed feedback on this work, and Jiaze Chen for his generous assistance in the training of \baby.

\bibliographystyle{unsrtnat}
\bibliography{neurips_2022}



































\appendix

\newpage

\section{Appendix}


\subsection{Pre-training Datasets}

As follows, we give some data filtering details. Since LAION and the video-text datasets are too large, we have filtered the datasets to speed up the pre-training. Specifically, for LAION, we use English data only. Following BLIP~\cite{li2022blip}, we remove an image if the shorter edge is smaller than 224 pixels. We also remove an image if the ratio of height/width or width/height is larger than 3. For video clip-text pairs, we remove a pair if the number of words is less than 2. Following previous work, we use CLIP score to filter video data. We sample a frame for a video clip and we calculate the CLIP score between the frame and the text. We remove a video clip-text pair if the score is less than 0.25. For image annotations of objects and regions, we remove a sample because of: 1) invalid annotations (e.g. negative values for bounding boxes or boxes being outside of the images); 2) boxes being too small (less than a patch); 3) highly overlapped text descriptions of regions ( 75\%), etc. For an object annotation, if it contains an object attribute, e.g. color, we concatenate the attribute with the object label as the text description. Moreover, some images in the OpenImages dataset contain relationship annotations, indicating pairs of objects in particular relations (e.g. "woman playing guitar", "beer on table"), object properties (e.g. "table is wooden"), and human actions (e.g. "woman is jumping"). We also utilize this part of data.  


\subsection{Implementation Details}

\baby is pre-trained at image resolution of  using  patch size. Though, as indicated in previous work such as OFA~\cite{wang2022ofa} and CoCa~\cite{yu2022coca}, increasing resolution will improve model performance, we keep it small to accelerate pre-training. Besides, we apply mixed precision for training. For text input, we set the maximum number of tokens to 30. To further speed up pre-training with large-scale data, we divide the training process into two steps. First, we train \baby with large-scale image-text pairs. Then, we further train \baby on video-text pairs and the 4M dataset. The reason behind this is that training on video data is slow. Because of it, we randomly sample only three frames for a video clip in pre-training. We mix all types of data in a training batch, and thus for each training iteration, we optimize the model by multi-grained aligning and multi-grained localization simultaneously. 


With 4M data, we pre-train \babyB for 500K steps with a batch size of 1024 on 8 A100 and \babyL for 250K steps on 16 A100, which takes  week. The learning rate of \babyB is warmed-up to  in the first 2500 steps and decayed following a linear schedule. The learning rate is  for \babyL. With large-scale data, training \baby takes 2-3 weeks on 32 A100 for the base model and 64 A100 for the large model. 



\subsection{Ablation Study}
To ensure a fair comparison, all compared model variants are trained on 4M images for 100K steps. Following previous studies, we have shortened the training steps to compare different ablated variants more efficiently. We evaluate model performance on image-text retrieval (Recall@1), visual question answering, visual grounding, and zero-shot open-vocabulary attribute detection. It is worth noting that VQA has a large train and test set, which means that even a relatively small difference in performance is worth considering.



\begin{table}[h]
\centering
\resizebox{0.7\columnwidth}{!}{\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{ } & \multicolumn{2}{c}{\bf Flickr30K} & \bf VQA & \bf MSRVTT & \multicolumn{2}{c}{\bf Video-QA}\\
 & TR & IR & test-dev & IR & MSRVTT & MSVD \\
\midrule
w/ avg pool (ours) & \bf 98.5 & \bf 90.4 & \bf 80.4 & \bf 47.6 & \bf 45.0 & \bf 52.8 \\

w/ temporal attn & 98.2 & 89.6 & 80.0 & 45.6 & 44.4 & 52.1 \\

\bottomrule
\end{tabular}
}
\vspace{0.2cm}

\caption{\textbf{Ablation study} of temporal modeling methods. 
}
\label{tbl:ab_video}
\end{table}


Additionally, we investigate whether better temporal modeling could further improve video understanding capabilities while maintaining good image understanding, as presented in Table~\ref{tbl:ab_video}. We use an established method that adds temporal attention in ViT. The experimental results on image/video-text retrieval and image/video VQA show that simply averaging the features of each frame achieves better performances on all tasks. We suppose that our approach is more unified in modeling both image and video features, and thus strong image understanding capability is better transferred to video understanding. 



\subsection{Qualitative Study of Multi-Grained Alignments}

\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{pics/x2vlm_examples_app1.pdf}}
\caption{Visualization of \baby generating captions for images and locating visual concepts given manual input descriptions. }
\label{app:example1}
\end{center}
\end{figure*}


\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{pics/x2vlm_grounding1.pdf}}
\caption{Visualization of \baby locating visual concepts in robot grasping scene given text descriptions: 1) ``deep blue cup''; 2) ``light blue cup''; 3) ``blue cup at the bottom''; 4) ``four cups at the top''; 5) ``two small ducks''. }
\label{app:g1}
\end{center}
\end{figure*}

\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{pics/x2vlm_grounding2.pdf}}
\caption{Visualization of \baby locating celebrities and cartoon characters given text descriptions: 1) ``Albert Einstein''; 2)``Edison''; 3)``Ultraman''; 4)``Doraemon''. }
\label{app:g2}
\end{center}
\end{figure*}

\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{pics/x2vlm_grounding4.pdf}}
\caption{Visualization of \baby locating objects in an image from an e-commerce website in China. The input text descriptions are: 1) ``shoes''; 2)``vacuum cleaner''; 3)``lipstick''; 4)``dress''. }
\label{app:g3}
\end{center}
\end{figure*}


\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{pics/x2vlm_grounding3.pdf}}
\caption{Visualization of \baby locating visual concepts in an image from children's textbooks. The input text descriptions are: 1) ``flying kites in the park''; 2) ``watering flowers''; 3) ``well dressed girl''; 4) ``Tiananmen Tower''; 5) ``drive to work''; 6) ``sign''; 7) ``traffic lights''. }
\label{app:g4}
\end{center}
\end{figure*}


We provide a qualitative study of what vision language alignments have been learned by \baby. To this end, we ask \baby to generate image captions or to locate visual concepts. We visualize the results in Figure~\ref{app:example1}, where the first two images are from the in-domain COCO dataset. We find that \baby can capture small objects in the background or objects which have been partly masked. We also choose out-of-domain images for evaluation. As shown in Figure~\ref{app:g1}, Figure~\ref{app:g2}, Figure~\ref{app:g3}, and Figure~\ref{app:g4}, \baby can recognize many visual concepts from different domains. 



 
\end{document}