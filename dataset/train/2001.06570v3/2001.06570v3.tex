\documentclass[12pt,a4paper]{article}

\usepackage{lineno, hyperref, multirow, amsmath, amssymb, color, mathtools, ulem}
\usepackage[a4paper]{geometry}
\usepackage[ruled]{algorithm2e}

\bibliographystyle{elsarticle-num}

\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}


\begin{document}

\title{Harmonic Convolutional Networks based on Discrete Cosine Transform}

\author{Matej Ulicny, Vladimir A. Krylov, Rozenn Dahyot}
\date{
    ADAPT Centre\\
    School of Computer Science \& Statistics, Trinity College Dublin, Ireland\\
    School of Mathematical Sciences, Dublin City University, Ireland\\
    Department  of Computer Science, Maynooth University, Ireland\\
}

\maketitle

\begin{abstract}
\noindent Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. 
We propose to learn these filters as combinations of preset spectral filters defined by the Discrete Cosine Transform (DCT).
Our proposed DCT-based harmonic blocks  replace conventional convolutional layers to produce partially or fully harmonic versions of new or existing CNN architectures. 
Using DCT energy compaction properties, we demonstrate how the harmonic networks can be efficiently compressed  by truncating high-frequency information in harmonic blocks thanks to the redundancies in the spectral domain.
We report extensive experimental validation demonstrating benefits of the introduction of harmonic blocks into state-of-the-art CNN models in image classification, object detection and semantic segmentation applications.
\end{abstract}

\keywords{
Harmonic Network, Convolutional Neural Network, Discrete Cosine Transform, Image Classification, Object Detection, Semantic Segmentation
}

\section{Introduction}\label{sec:introduction}

CNNs have been designed to take advantage of implicit characteristics of natural images, specifically correlation in local neighborhood and feature equivariance.
Standard CNNs rely on learned convolutional filters hence finetuned to the data available. However, it can be advantageous to revert to preset filter banks: for instance, with limited training data ~\cite{Ulicny19}, using a  collection of preset filters can help in avoiding overfitting and in reducing the computational complexity of the system.
Scattering networks are an example of such networks with preset (wavelet based) filters  which have achieved state-of-the-art results in handwritten digit recognition and texture classification~\cite{Bruna13}.

We propose instead to replace the standard convolutional operations in CNNs by harmonic blocks that learn the weighted sums of responses to the Discrete Cosine Transform (DCT) filters, see Fig.~\ref{fig:harmlayer}.
\begin{figure}[!h]
\begin{center}
\begin{center}
   \includegraphics[width=0.27\linewidth]{graphics/hblock1.png}
   \includegraphics[width=0.72\linewidth]{graphics/harmonic_block_graphic.png}
\end{center}
\end{center}
\vspace{-.5\baselineskip}
\caption{Left: Design of the harmonic block. 
Boxes show operation type, size of filter (if applicable) and the number of output channels given the block filter size , number of input channels  and output channels . Batch normalization (BN) block is optional. Right: Visualization of the harmonic block applied to an input layer.}
\label{fig:harmlayer}
\end{figure}
DCT has been successfully used for JPEG encoding to transform image blocks into spectral representations to capture the most information with a small number of coefficients.
Motivated by frequency separation and energy compaction properties of DCT, the proposed harmonic networks rely on combining responses of window-based DCT with a small receptive field.
Our method learns how to optimally combine spectral coefficients at every layer to produce a fixed size representation defined as a weighted sum of responses to DCT filters. The use of DCT filters allows one to represent and regularize filter parameters directly in the DCT domain and easily address the task of model compression.
Other works that propose convolutional filters decomposition to particular basis functions~\cite{Qiu18,Tayyab19} have predominantly focused on network compression. In our study we demonstrate that prior information coming from well chosen filter basis can not only be used to compress but also speeds up training convergence and improves performance.

Based on our earlier works~\cite{Ulicny19b,Ulicny19},  
this paper  contributions are as follows. 
First we demonstrate that the theoretical computational overheads of the optimised formulation of a harmonic block are minimal (experimentally, within 3-7\%) whereas the memory footprint requirements are comparable to those of the benchmark architecture based on standard convolutional blocks (and are lower if harmonic blocks undergo compression). Second, we substantially expand experimental validation to demonstrate a consistent increase in performance due to the use of harmonic blocks. Specifically, on the small NORB dataset we achieve state-of-the-art results and demonstrate how DCT-based harmonic blocks allow one to efficiently generalise to unseen lighting conditions. We further report quantitative as well as qualitative results of application of harmonic blocks to a representative variety of vision tasks: object detection and instance/semantic segmentation. We observe a consistent average improvement of 1\% AP on these tasks, which demonstrates the practical appeal of using harmonic networks.
Section ~\ref{sec:soa} presents the relevant background research to   our harmonic network formulation (Sec.~\ref{sec:method}).
It is extensively validated against state-of-the-art alternatives  for image classification (Sec.~\ref{sec:experiments1}), for object detection, instance and semantic segmentation (Sec.~\ref{sec:experiments.detection}).
All our architectures in the reported results are denoted as \textit{Harm};
the PyTorch implementations for our harmonic networks are publicly available at \url{https://github.com/matej-ulicny/harmonic-networks}.


\section{Related work}
\label{sec:soa}

\subsection{DCT \& CNNs }

Networks trained on DCT coefficients are frequently used in forensics for detection of tampered parts in images. These parts are assumed to have different distribution of DCT coefficients from the rest of the image.
A common practice is to classify histograms of preselected JPEG-extracted DCT coefficients by 1-D or 2-D convolutional network~\cite{Zheng19}.
A number of studies have also investigated the use of spectral image representations for object and scene recognition. 
DCT features from an entire image were used to train Radial Basis Function Network for face recognition~\cite{Er05}. 
A DCT-based scene descriptor was used together with a CNN classifier~\cite{Farinella15}.
A significant convergence speedup and case-specific accuracy improvement have been achieved by applying DCT transform to early stage learned feature maps in shallow CNNs~\cite{Ghosh16} whereas the later stage convolutional filters were operating on a sparse spectral feature  representation. 
In~\cite{Ulicny17,Gueguen18} it was demonstrated how DCT coefficients can be efficiently used to train CNNs for classification, where the DCT coefficients can be computed or taken directly from JPEG image format. 
\textit{Wang et al.}~\cite{Wang16b} compresses CNN filters by separating cluster centers and residuals in their DCT representation. Weights in this form were quantized and transformed via Huffman coding (used for JPEG compression) for limiting storage. Lastly, DCT image representation has been used for calculation of more informative loss function in generative learning~\cite{Atapour19}. 

\subsection{ Wavelets \& CNNs}

\paragraph{Wavelet networks} 
As an alternative to DCT, scattering networks~\cite{Bruna13} are built on complex-valued wavelets.
The scattering network has its filters designed to extract translation and rotation invariant representations.
It was shown to effectively reduce the input representation while preserving discriminative information for training CNN on image classification and object detection task~\cite{Oyallon18b} achieving performance comparable to deeper models. \textit{Williams et al.}~\cite{Williams16} have advocated image preprocessing with wavelet transform, but used different CNNs for each frequency subband. Wavelet filters were also used as a preprocessing method prior to NN-based classifier~\cite{Said16}.

\paragraph{Spectral based CNNs}
Other works have used wavelets in CNN computational graphs. 
Low-frequency components of the Dual-Tree Complex Wavelet transform were used in a noise suppressing pooling operator~\cite{Duan17}. 
\textit{Ripperl et al.} have designed a spectral pooling~\cite{Rippel15} based on Fast Fourier Transform and truncation of high-frequency coefficients. 
The pooled features were recovered with Inverse Discrete Fourier Transform, thus the CNN still operates in the spatial domain.
They also proposed to parameterise filters in the Fourier domain to decrease their redundancy and speed up the convergence when training the network. 


A Wavelet Convolutional Network proposed by \textit{Lu et al.}~\cite{Lu18} learns from both spatial and spectral information that is decomposed from the first layer features. The higher-order coefficients are concatenated along with the feature maps of the same dimensionality. 
However, contrary to our harmonic networks, Wavelet CNNs decompose only the input features and not the features learned at intermediate stages. 
Robustness to object rotations was addressed by modulating learned filters by oriented Gabor filters~\cite{Luan18}. \textit{Worrall et al.} incorporated complex circular harmonics into CNNs to learn rotation equivariant representations~\cite{Worrall17}. Similarly to our harmonic block, the structured receptive field block~\cite{Jacobsen16} learns new filters by combining fixed filters, a set of Gaussian derivatives with considerably large spatial extent. Additionally, an orthogonal set of Gaussian derivative bases of small spatial extend have been used by \textit{Kobayashi} to express convolutional filters~\cite{Kobayashi18}. DCFNet~\cite{Qiu18} expresses filters by truncated expansion of Fourier-Bessel basis, maintaining accuracy of the original model while reducing the number of parameters. 



\section{Harmonic Networks}\label{sec:method}

A convolutional layer extracts correlation of input patterns with locally applied learned filters. The idea of convolutions applied to images stems from the observation that pixels in local neighborhoods of natural images tend to be strongly correlated. In many image analysis applications, transformation methods are used to decorrelate signals forming an image~\cite{Wang12}. In contrast with spatial convolution with learned kernels, this study proposes feature learning by weighted combinations of responses to predefined filters. The latter extracts harmonics from lower-level features in a region. The use of well selected predefined filters allows one to reduce the impact of overfitting and decrease computational complexity. We focus here on the use of DCT as the underlying transformation.

\subsection{Discrete Cosine Transform} \label{sec:dct}

DCT is an orthogonal transformation method that decomposes an image to its spatial frequency spectrum. A 2D signal is expressed as a sum of sinusoids with different frequencies. The contribution of each sinusoid towards the whole signal is determined by its coefficient calculated during the transformation.
DCT is also a separable transform and due to its energy compaction properties on natural images~\cite{Wang12} it is commonly used for image and video compression in widely used JPEG and MPEG formats. Note that Karhunen-Lo{\`e}ve transform (KLT) is considered to be optimal in signal decorrelation, however it transforms signal via unique basis functions that are not separable and need to be estimated from the data. On locally correlated signals such as natural images DCT was shown to closely approximate KLT~\cite{Wang12}.

We use the most common DCT formulation, noted DCT-II,  computed on a 2-dimensional grid of an image  of size  representing the image patch with 1 pixel discretisation step:

 is  the DCT coefficient  of the input  using a sinusoid with horizontal and vertical frequencies noted   and  respectively. Basis functions are typically normalized with factors  (resp. ) when  (resp. when )   and  (resp. ) otherwise to ensure their orthonormality. 

It is worth noting~\cite{Ulicny19}, that the sine transform of the signal  with  values at frequency  is equivalent to the cosine transform of the image shifted by  pixels, :

Hence by applying DCT with a certain stride (effectively resulting in {\it overlapping} DCT transform) it is possible to obtain a feature representation as rich as that obtained employing the full Fourier transform~\cite{Ulicny19}.


\subsection{Harmonic blocks}

We propose the harmonic block  to replace a conventional convolutional operation hence relying on processing the data in two stages (see Fig.~\ref{fig:harmlayer}). Firstly, the input features undergo harmonic decomposition using window-based DCT.
In the second stage, the transformed signals are combined by learned weights. 
The fundamental difference from standard convolutional network is that the optimization algorithm is not searching for filters that extract spatial correlation, rather learns the relative importance of preset feature extractors (DCT filters) at multiple layers.

Harmonic blocks are integrated as a structural element in the existing or new CNN architectures. We thus design harmonic networks that consist of one or more harmonic blocks and, optionally, standard learned convolutions and fully-connected layers, as well as any other structural elements of a neural net.
Spectral decomposition of input features into block-DCT representation is implemented as a convolution with DCT basis functions. A 2D kernel with size  is constructed for each basis function, comprising a filter bank of depth , which is separately applied to each of the input features. Convolution with the filter bank isolates coefficients of DCT basis functions to their exclusive feature maps, creating a new feature map per each channel and each frequency considered. The number of operations required to calculate this representation can be minimized by decomposing 2D DCT filter into two rank-1 filters and applying them as separable convolution to rows and columns sequentially. 

Each feature map  at depth  is computed as a weighted linear combination of DCT coefficients across all input channels :

where  is a  frequency selective DCT filter of size ,  the 2-dimensional convolution operator and  is learned weight for  frequency of the -th feature. The linear combination of spectral coefficients is implemented via a convolution with  filter that scales and sums the features, see Fig.~\ref{fig:harmlayer}.
In our implementation we use a fixed collection of DCT bases. Specifically, if we are to replace a  convolution layer, the DCT filter bank  has filters defined for every filter coordinate  as given in Eq.~\ref{eq:dct}.
Since the DCT is a linear transformation, backward pass through the transform layer is performed similarly to a backward pass through a convolution layer. Harmonic blocks are designed to learn the {\it same} number of parameters as their convolutional counterparts. Such blocks can be considered a special case of depth-separable convolution with predefined spatial filters. 

DCT is distinguished by its energy compaction capabilities which typically results in higher filter responses in lower frequencies. The behaviour of relative loss of high frequency information can be efficiently handled by normalizing spectrum of the input channels. This can be achieved via batch normalization that adjusts per frequency mean and variance prior to the weighted combination. The spectrum normalization transforms Eq.~\eqref{eq:feature} into: 

with parameters  and  estimated per input batch.

\subsection{Harmonic Network Compression} \label{sec:method.subsample}


The JPEG compression encoding relies on stronger quantisation of higher frequency DCT coefficients. This is motivated by the human visual system which often prioritises low frequency information over high frequencies. We propose to employ similar idea in the harmonic network architecture. Specifically, we limit the visual spectrum of harmonic blocks to only several most informative low frequencies, which results in a reduction of number of parameters and operations required at each block. The coefficients are (partially) ordered by their relative importance for the visual system in triangular patterns starting at the most important zero frequency at the top-left corner, see Fig.~\ref{fig:filters}. We limit the spectrum of considered frequencies by hyperparameter  representing the number of levels of coefficients included perpendicularly to the main diagonal direction starting from zero frequency: DC only for , three coefficients used for , and six coefficients used for .
Fig.~\ref{fig:filters} illustrates filters used at various levels assuming a  receptive field.
\begin{figure}[t]
\begin{center}
   \includegraphics[width=.8\linewidth]{graphics/dct_filters1.png}
\end{center}
\vspace{-.5\baselineskip}
   \caption{  DCT filter bank employed in the harmonic networks and its compression.}
\label{fig:filters}
\end{figure} 
Thus, reformulating convolutional layers as harmonic allows one  to take advantage of this natural approach to model compression, and in doing also introduce additional regularization into the model.

When compressing harmonic networks with multiple harmonic blocks a question what is the appropriate subset of coefficients arises. We propose 3 principles of selecting these subsets: uniform, progressive and adaptive. 

\noindent\textbf{Uniform selection} is the most simple compression approach that uses the same  at every layer. A pitfall of this method is that a specific subset of basis functions might not explain well enough filter banks at some layers, but might have some bases redundant when representing filters in other layers.

\noindent\textbf{Progressive selection} omits higher number of frequencies in deeper layers. We investigate two strategies for progressive coefficient selection. The first strategy employs the same subset of coefficients in all harmonic blocks applied to feature maps of a particular size, but this subset shrinks with the size of feature maps (smaller maps in deeper layers). 
The second strategy relies on the selection of compression level based on the depth of the layer. Specifically, the compression level is estimated as , where  is the lowest allowed value of , considering ,  the filter size ( corresponds to no compression), and  a constant that sets a linear relation between  and the depth of a particular layer and controls the overall compression rate when .
    
\noindent\textbf{Adaptive selection}: compression is estimated adaptively for each layer; a basis is excluded if proportion of the  norm of its corresponding weights compared to the norms of the other bases in the same layer is lower than a threshold . Specifically, if  then the coefficient is truncated. It should be noted that this approach, unlike the previous ones, needs the full model to be optimized prior to compression.

The empirical impact of harmonic model compression is  investigated experimentally in more detail in Sections \ref{sec:experiments1} and \ref{sec:experiments.detection}.

\subsection{Computational Requirements} \label{sec:methodrequirements}

Harmonic blocks are designed to learn the same number of parameters as their convolutional counterparts. Requirements for the DCT transform scale linearly with the number of input channels and result in a modest increase to the theoretical number of operations. Standard convolutional layer used in many popular architectures that has  input and  output channels with a kernel size  learns  parameters and performs  operations if the filter is applied  and  times in particular directions. Harmonic block with  transformation filters of size  upsamples representation to  features and then learns one weight for each upsampled-output feature pair hence  weights. Transformation of an  feature set costs  on top of weighted combination  that matches the number of multiply-add operations of  convolution. The total number of operations is thus . The theoretical number of multiply-add operations over the standard convolutional layer increases by a factor of . If we assume truncated spectrum (use of ) given by  filters, proportion of operations becomes .

While keeping the number of parameters intact, a harmonic block requires additional memory during training and inference to store transformed feature representation. In our experiments with WRN models (Sec.\ref{sec:experiments.cifar}), the harmonic network trained with full DCT spectrum requires almost 3 times more memory than the baseline.
This memory requirement can be reduced by using the DCT spectrum compression.

Despite the comparable theoretical computational requirements, the run time of harmonic networks is larger compared to the baseline models, at least twice slower (on GPU) in certain configurations. This effect is due to generally less efficient implementation of separable convolution and the design of harmonic block that replaces a single convolutional layer by a block of 2 sequential convolutions (with individual harmonic filters and 1x1 convolution). Most blocks do not need BN between the convolutions and thus represent a combined linear transformation. The associativity property of convolutions allows one to reformulate the standard harmonic block defined above so that the DCT transform and linear combination can be effectively merged into a single linear operation:

In other words, equivalent features can be obtained by factorizing filters as linear combinations of DCT basis functions. We thus propose a faster Algorithm~\ref{alg:mem_eff_harm_block} that is a more memory efficient alternative to the standard two-stage harmonic block formulation and uses dense convolution. 

\begin{algorithm}[t]
 \KwIn{}
 {Define updates} \;
 \For{}{
  \For{}{
   \;
  }
 }
 \;
 \KwOut{}
 \caption{Memory efficient harmonic block} 
 \label{alg:mem_eff_harm_block}
\end{algorithm}
The Algorithm~\ref{alg:mem_eff_harm_block} overhead in terms of multiply-add operations with respect to the standard convolutional layer is only , where the input image size for the block is . The experimental performance of the algorithm is evaluated in Section~\ref{sec:experiments.cifar}. 



\section{Image Classification } 
\label{sec:experiments1}

The performance of the harmonic networks is assessed for image classification  on small (NORB, Sec. \ref{sec:experiments.norb}), medium (CIFAR-10 and CIFAR-100, Sec \ref{sec:experiments.cifar}) and large (ImageNet-1K, Sec. \ref{sec:experiments.imagenet}) scale datasets. 



\subsection{Small NORB dataset} \label{sec:experiments.norb}

The small NORB dataset~\cite{Lecun04} is a synthetic set of  binocular images of 50 toys sorted into 5 classes (four-legged animals, human figures, airplanes, trucks, and cars), captured under different lighting and pose conditions (i.e. 18 angles, 9 elevations and 6 lighting conditions induced by combining different light sources). Training  and test sets used in our experiments are retained original~\cite{Lecun04}.
We show first that harmonic networks outperform standard and state-of-the-art CNNs in both accuracy and compactness (c.f. Section \ref{sec:norb:comparison}) and also illustrate how Harmonic networks can be naturally resilient to unseen illumination changes without resorting to using data augmentation (Sec. \ref{sec:norb:illumination}).

\subsubsection{Comparisons CNN vs. Harmonic Nets}
\label{sec:norb:comparison}

\vskip .1cm
\noindent{\bf Baseline architectures.} 
Our baseline CNN2 consists of 2 convolution and 2 fully-connected layers. Features are subsampled by convolution with stride and overlapping max-pooling. All hidden layer responses are batch normalized and rectified by ReLU. We also use a slightly deeper network CNN3 with an additional convolutional layer preceding the first pooling. Details of the architectures are summarised in Table~\ref{tab:norb_nn}. 

\noindent{\bf Optimisation.}
The baseline CNNs are trained with stochastic gradient descent for 200 epochs with momentum 0.9 and weight decay 0.0005. The initial learning rate 0.01 is decreased by factor 10 every 50 epochs. The network is trained with batches of 64 stereo image pairs, each pair is zero-padded 5 pixels and a random crop of 9696 pixels is fed to the network.


\begin{table}[!t]
\tabcolsep = 1.0mm
\begin{center}
\caption{Models used in NORB experiments.
Convolution and harmonic operation are denoted as \{conv, harm\}~M,KK/S with M output features, kernel size K and stride S; similarly for pooling~KK/S and fully connected layers~fc~M.}
\label{tab:norb_nn}
\vspace{0.3\baselineskip}
\footnotesize
\begin{tabular}{cccccc}
\hline
\textbf{Resol.} & \textbf{CNN2} & \textbf{CNN3} & \textbf{Harm-CNN2}& \textbf{Harm-CNN3 }& \textbf{Harm-CNN4}\\
\hline
96x96 & conv 32, 5x5/2 & conv 32, 5x5/2 & harm 32, 4x4/4 & harm 32, 4x4/4 & harm 32, 4x4/4\\
48x48 & pool 3x3/2 & conv 64, 3x3/2 & - & - & - \\
24x24 & conv 64, 3x3/2 & pool 2x2/2 & harm 64, 3x3/2 & harm 64, 3x3/2& harm 64, 3x3/2 \\
12x12 & pool 3x3/2 & conv 128, 3x3/2 &  pool 3x3/2&pool 3x3/2&pool 3x3/2\\
6x6 & fc 1024 & pool 2x2/2 & fc 1024 & harm 128, 3x3/2 & harm 128, 3x3/2 \\
3x3 & - & fc 1024 & - & fc 1024 & harm 1024, 3x3/3 \\
1x1 & dropout 0.5 & dropout 0.5 & dropout 0.5& dropout 0.5& dropout 0.5\\
1x1 & fc 5 & fc 5 & fc 5& fc 5& fc 5\\
\hline
\end{tabular}
\end{center}
\end{table}

\noindent{\bf Harmonic Networks architectures.} Several versions of harmonic networks are considered (Tab. \ref{tab:norb_nn}), by substituting the first, first two or all three of CNN2 and CNN3 convolution layers by harmonic blocks. Furthermore, the first fully-connected layer can be transformed to a harmonic block taking global DCT transform of the activations. The first harmonic block uses 4 DCT filters, the further blocks mimic their convolutional counterparts.

\vskip .1cm
\noindent{\bf Performance evaluation.} The baseline CNN architecture shows poor generalization performance in early stages of training, see Fig.~\ref{fig:norb_convergence}. Baseline CNN2 achieved mean error 3.48\%0.50 from 20 trials, while CNN2 utilizing harmonic blocks without explicit normalization of harmonic responses exhibits similar behavior resulting in lower mean error of 2.40\%0.39. Normalizing DCT responses at the first block prevents harmonic network from focusing too much on pixel intensity, allows using 10 higher learning rate, significantly speeds up convergence, improves performance and stability. 
\begin{figure}[t]
\begin{center}
   \includegraphics[width=.6\linewidth]{graphics/norb_convergence.png}
\end{center}
\vspace{-1.5\baselineskip}
\caption{Mean classification error on small NORB test set. Weak generalization of CNN (green) and harmonic network (blue) is observed during the early stages of training. Filled areas (best seen in color) show 50\% empirical confidence intervals from 20 runs. Batch normalization of DCT spectrum (first block) significantly speeds up convergence of harmonic network (red).}
\label{fig:norb_convergence}
\end{figure}
All variants of the harmonic network perform comparably.
Particularly we observe the overlapping average pooling to work well in combination with harmonic blocks. The best result was obtained by the Harm-CNN4 model with 4 harmonic blocks (the latter replaces the fully-connected layer), misclassifying only 1.10\%0.16 of test samples.

\vskip .1cm
\noindent{\bf Comparison with state-of-the-art.} Table~\ref{tab:norb_state_of_art} shows that these results surpass the best previously reported error rate for this dataset to the best of our knowledge. The capsule network~\cite{Hinton18} claims 1.4\% error rate, however estimated under a different evaluation protocol.

\begin{table}[h]
\begin{center}
\caption{Comparison with the state-of-the-art on small NORB dataset, showing the proposed method outperforms other reported results.} 
\label{tab:norb_state_of_art}
\vspace{0.3\baselineskip}
\tabcolsep = 1mm
\begin{tabular}{ l c c c c }
\hline
\textbf{Method} & \textbf{Parameters}  & \textbf{Error \%}  \\
\hline
CNN3 & 1.28M & 3.43  0.31 \\
CapsNet~\cite{Hinton18} multi-crop & 310K & 1.4* \\
\hline
Harm-CNN2 & 2.39M & 1.56  0.18 \\
Harm-CNN3 & 1.28M & 1.15  0.22 \\
Harm-CNN4 & 1.28M & \textbf{1.10  0.16} \\
\hline
\end{tabular}\.05cm]
*scores reported by~\cite{Luan18}.
\end{center}
\end{table}

\begin{figure}[!b]
\begin{center}
   \includegraphics[width=.9\linewidth]{graphics/freq_heatmap.png}
\end{center}
\vspace{-.25\baselineskip}
\caption{Distribution of weights (averaged in each layer) assigned to DCT filters in the first harmonic block (left-most) and the remaining blocks in the Harm-WRN-28-10 model trained on CIFAR-10. Vertical lines separate the residual blocks.}
\label{fig:freq_heatmap}
\end{figure}

\begin{figure*}[!t]
\begin{center}
   \includegraphics[width=.49\linewidth]{graphics/cifar10_graph.png}
   \includegraphics[width=.49\linewidth]{graphics/cifar100_graph.png}
\end{center}
\vspace{-1.25\baselineskip}
\caption{Decrease of classification error as a function of model size on CIFAR-10 (left) and CIFAR-100 (right). Parameters of harmonic networks are controlled by the compression parameter , the WRN baselines by the width multiplier {\it w}.}
\label{fig:compression_graph}
\end{figure*}

Analysis of fully harmonic WRN weights learned with 3x3 spectrum revealed that the deeper layers tend to favour low-frequency information over high frequencies when learning representations. Relative importance of weights corresponding to different frequencies shown in Fig.~\ref{fig:freq_heatmap} motivates truncation of high-frequency coefficients for compression purposes. While preserving the input image spectrum intact, we train the harmonic networks on limited spectrum of hidden features for =2 and =3 using 3 and 6 DCT bases respectively. 
To assess the loss of accuracy associated with parameter reduction we train baselines with reduced widths having comparable numbers of parameters: WRN-28-8 and WRN-28-6, see Fig.~\ref{fig:compression_graph}. Fully harmonic WRN-28-10 with =3 has comparable error to the network using the full spectrum and outperforms the larger baseline WRN-28-10, showing almost no loss in discriminatory information. On the other hand Harm-WRN-28-10 with =2 is better on CIFAR-100 and slightly worse on CIFAR-10 compared to the similarly sized WRN-28-6. The performance degradation indicates that some of the truncated coefficients carry important discriminatory information.

We further compare performance with the Gabor CNN 3-28~\cite{Luan18} that relies on modulating learned filters with Gabor orientation filters. To operate on a similar model we remove dropouts and reduce complexity by applying progressive : no compression for filters on 32x32 features, =3 for 16x16, and =2 for the rest. With a smaller number of parameters the Harm-WRN-28-10 performs similarly on CIFAR-10 and outperforms Gabor CNN on CIFAR-100.

\vskip .1cm
\noindent{\bf Harmonic block implementations.}
Here we compare the standard harmonic block implementation with its memory efficient version introduced in Algorithm~\ref{alg:mem_eff_harm_block}, see Table~\ref{tab:alg_req}. The comparison on CIFAR-10 dataset demonstrates that Algorithm~\ref{alg:mem_eff_harm_block} provides similar overall performance but reduces both the runtime and memory requirements nearly three times. We will therefore use solely this implementation of the harmonic block except for the root (first) layer due to the use of BN on that first layer.
\iffalse
\begin{table}[!t]
\centering
\caption{Computational requirements of harmonic block implementations on CIFAR-10 with accuracy shown averaged over 5 runs.} \label{tab:alg_req}
 \begin{tabular}{lccc}
  \hline
  \multirow{2}{*}{\textbf{Model}} & {\bf GPU}  & {\bf Epoch}  & \multirow{2}{*}{{\bf Error \%}} \\
 &  {\bf memory} & {\bf runtime} &  \\
  \hline
  WRN-16-8~\cite{Zagoruyko16} & 2.8GB & 45.0s & 4.390.14 \\
  Harm-WRN-16-8 (standard block) & 6.6GB & 123.4s & 4.440.04 \\
  Harm-WRN-16-8 (Alg.~\ref{alg:mem_eff_harm_block}) & 2.9GB & 56.8s & 4.380.09 \\
  \hline
 \end{tabular}
\end{table}

\begin{table}[!t]
\centering
\caption{Computational requirements of harmonic block implementations on CIFAR-10 with accuracy shown averaged over 5 runs.} \label{tab:alg_req}
 \begin{tabular}{lccc}
  \hline
  \multirow{2}{*}{\textbf{Model}} & {\bf GPU}  & {\bf Epoch}  & \multirow{2}{*}{{\bf Error \%}} \\
 &  {\bf memory} & {\bf runtime} &  \\
  \hline
  WRN-16-8~\cite{Zagoruyko16} & 2.1GB & 40.2s, 2.6s & 4.30.11 \\
  Harm-WRN-16-8 (standard block) & 6.0GB & 123.5s, 10.3s & 4.370.14 \\
  Harm-WRN-16-8 (Alg.~\ref{alg:mem_eff_harm_block}) & 2.2GB & 50.0s, 3.4s & 4.360.11 \\
  Harm-WRN-16-8-BN (standard block) & 6.0GB & 123.2s, 10.4s & 4.290.07 \\
  Harm-WRN-16-8-BN (Alg.~\ref{alg:mem_eff_harm_block}) & 2.2GB & 50.3s, 3.4s & 4.320.06 \\
  \hline
 \end{tabular}
\end{table}
\fi


\begin{table}[b]
\caption{Modifications of the WRN-16-4 baseline on CIFAR-100: mean classification errors and standard deviations from 5 runs when replacing particular layers by harmonic blocks.} \label{tab:ablation}
\vspace{0.3\baselineskip}
\centering
\begin{tabular}{ c c c c }
 \hline
 \textbf{Root block} & \textbf{Harmonic root BN} & \textbf{Residual blocks} & \textbf{Error \%}  \\
 \hline
 & & & 24.07  0.24 \\
 \checkmark & & & 23.79  0.24 \\
 \checkmark & \checkmark & & 23.67  0.12 \\
 & & \checkmark & 23.22  0.28 \\
 \checkmark & & \checkmark & 23.25  0.25 \\
 \checkmark & \checkmark & \checkmark & 23.21  0.11 \\
 \hline
\end{tabular}
\end{table}

\vskip .1cm
\noindent{\bf Ablation study.}
The effect of filter parametrisation by DCT bases is investigated by replacing particular layers of WRN-16-4 (w/o dropout) with harmonic blocks, see Table~\ref{tab:ablation}. 
We consider replacing the root convolution layer (with or without BN), or layers in residual blocks. Replacing each layer has provided the greatest improvement, while BN in the first block decreases the variance by half.
These observations correspond to the results obtained on NORB dataset. We will always be employing BN as part of the root harmonic block.

\vskip .1cm
\noindent{\bf Compressing existing models.}
Section~\ref{sec:method.subsample} described how convolutional filters in certain layer can be approximated with fewer parameters. So far we have only considered uniform coefficient truncation by truncating the same frequencies in all the layers, or a simple progressive compression. In this experiment we assess the effectiveness of our more elaborate coefficient selection schemes on already trained model. We start with the WRN-28-10 baseline trained without dropout, which has been converted to harmonic WRN-28-10 net (omitting BN in the first harmonic block) by re-expressing each 33 filter as a combination of DCT basis functions. The first harmonic block is kept intact (no compression in DCT representation), while all other blocks are compressed. We compare three different coefficient selection strategies: uniform, advanced progressive and adaptive selection  (cf. Sec \ref{sec:method.subsample}).

The results reported in Fig.~\ref{fig:compression_types} confirm the behavior observed in Fig.~\ref{fig:freq_heatmap}, i.e. the high frequencies appear to be more relevant in the early layers of the network compared to deeper layers. The uniform compression discards the same amount of information in all the layers, and is surpassed by the other compression strategies. By using progressive or adaptive coefficient selection a model can be compressed by over 20\% without a loss in accuracy. The best progressive method loses less than 1\% of accuracy when compressed by 45\% without a need for finetuning.

\begin{figure}
\begin{center}
  \includegraphics[width=.6\linewidth]{graphics/compression_types}
\end{center}
\vspace{-.5\baselineskip}
\caption{Accuracy of compressed WRN-28-10 on CIFAR-100 dataset using different coefficient truncation strategies.}
\label{fig:compression_types}
\end{figure}

\subsection{ImageNet dataset} 
\label{sec:experiments.imagenet}

We present here results obtained on ImageNet-1K classification task.
ResNet~\cite{He16} with 50 layers is adopted as the baseline. 
To reduce memory consumption maxpooling is not used, instead the first convolution layer employs stride 4 to produce equally-sized features; we refer to this modification as ResNet-50 (no maxpool). The following harmonic modifications refer to this baseline without maxpooling after the first layer. We investigate the performance of three harmonic modifications of the baseline: (i) replacing solely the initial 7x7 convolution layer with harmonic block (with BN) with 7x7 DCT filters, (ii) replacing all convolution layers with receptive field larger than 1x1 with equally-sized harmonic blocks, (iii) compressed version of the fully-harmonic network.
The models are trained as described in~\cite{Ulicny19b}, and here we report accuracy after 100 epochs.

\begin{table}[h]
\caption{Classification errors on ImageNet validation set using central crops.} 
\label{tab:imagenet_spec}
\vspace{0.3\baselineskip}
\centering
\begin{tabular}{ l c c c }
\hline
 \textbf{Model} & \textbf{Parameters}  & \textbf{Top-1 \%}  & \textbf{Top-5 \%}  \\
\hline
 VGG16-BN & 138.4M & 25.86 & 8.05 \\
 Harm-VGG16-BN & 138.4M & 25.55 & 8.01 \\
 ResNet-50 (no maxpool) & 25.6M & 23.83 & 7.01 \\
 Harm1-ResNet-50 & 25.6M & 23.01 & \textbf{6.47} \\
 Harm-ResNet-50 & 25.6M & \textbf{22.98} & 6.64 \\
 Harm-ResNet-50, progr.  & 19.7M & 23.21 & 6.67 \\
 Harm-ResNet-101 & 44.5M & 21.45 & 5.78 \\

\hline
{\bf Benchmarks}\\
 ResNet-50 (maxpool)* & 25.6M & 23.87 & 7.14 \\
 ScatResNet-50~\cite{Oyallon18b} & 27.8M & 25.5 & 8.0 \\
 JPEG-ResNet-50~\cite{Gueguen18} & 28.4M & 23.94 & 6.98 \\
 ResNet-101 (maxpool)* & 44.5M & 22.63 & 6.45 \\
\hline
\end{tabular}\.05cm]
*models trained on 600600 (EfficientNet) and 384384 (Swin) crops.
\end{table}

\vskip .1cm
\noindent{\bf Comparison with the cutting-edge techniques.}
Here we verify the use of DCT-based harmonic blocks in the more elaborate state-of-the-art models. To this end we modify ResNeXt architecture~\cite{Xie17}, which is similar to ResNets and uses wider bottleneck and grouped convolution to decrease the amount of FLOPS and the number of parameters. The model is further boosted using several state-of-the-art adjustments: {\it (i)} identity mappings that downsample features are extended by average pooling to prevent information loss; {\it (ii)} squeeze and excitation blocks~(SE)~\cite{Hu19} are used after every residual connection. The network is further regularized by stochastic depth and dropout on the last layer. Training is performed via stochastic gradient decent with learning rate 0.1 and batch size 256, with the former decayed according to one cosine annealing cycle. In addition to mirroring and random crops of size 224, images are augmented with rotations and random erasing. 

Our ResNeXt modification with 101 layers and 32 groups per 4 convolutional filters in a residual block is trained for 120 epochs. The use of DCT bases provides a subtle improvement over the standard bases. Furthermore, we upscale the network to use 64 groups of filters, replace max-pooling in the first layer by increased stride and train this network for 170 epochs. From Table~\ref{tab:resnext} we conclude that our model outperforms all other ``handcrafted'' architectures that do not use extra training images and performs comparably to the networks of similar complexity found via neural architecture search. Note that {these models were trained on larger image crops compared to our harmonic network, which typically leads to higher accuracies.}

Vision transformers~\cite{Liu21} process a sequence of patch embeddings and consider global relations within an image, which is distinct to the CNN approach. At the same time transformers lack some inductive biases such as translation equivariance, locality and have issues related to the features' scale. Recently proposed Swin transformers~\cite{Liu21} address some of these issues by hierarchical representation and strong augmentation to perform very well even without pre-training on larger datasets (cf. Tab. \ref{tab:resnext}). This excellent performance, surpassing that achieved by our model, is obtained by using more sophisticated augmentation techniques and more training epochs.



\section{Object Detection and Segmentation}
\label{sec:experiments.detection}

Representations learned from features expressed via harmonic basis are versatile and can serve well for transfer learning.  We demonstrate here that popular vision architectures relying on harmonic backbones provide a notable improvement in accuracy compared to the use of standard convolution-based backbone models. To this end we assess the performance of harmonic networks in object detection, instance and semantic segmentation tasks. For object detection, the popular single stage RetinaNet~\cite{Lin17b} and multistage Faster~\cite{Ren15} and Mask R-CNN~\cite{He17} frameworks are built upon our harmonic ResNet backbones. The semantic segmentation pipeline extends these backbones to DeepLabV3~\cite{Chen17rethinking} models. A set of experiments is conducted on the datasets Pascal VOC~\cite{pascal} (Sec. \ref{sec:segmentation:Pascal}) and MS COCO~\cite{mscoco} (Sec. \ref{sec:segmentation:coco}).

\begin{table}[b]
\caption{Mean average precision of Faster R-CNN models after 5 runs on Pascal VOC07 test set. ResNet-101-based models are trained once.} \label{tab:VOC}
\vspace{0.3\baselineskip}
\centering
\begin{tabular}{ l c c }
 \hline
 \textbf{Backbone} &  {\textbf{Box AP VOC07}} &  {\textbf{Box AP VOC07+12}} \\
\hline
ResNet-50 & 73.8  0.3  & 79.7  0.3 \\
Harm-ResNet-50 & \textbf{75.0  0.4} & \textbf{80.7  0.2} \\
\hline
ResNet-101 & 76.1 & 82.1 \\
Harm-ResNet-101 & \textbf{77.4} & \textbf{82.9} \\
 \hline
\end{tabular}
\end{table}

\subsection{Object Detection on Pascal VOC}\label{sec:segmentation:Pascal}

We extend PyTorch implementation provided by Chen et al.~\cite{mmdet} and train Faster R-CNN model based on our harmonic ResNets with 50 and 101 layers. 
Region proposal network (RPN) is applied on the feature pyramid~\cite{Lin17} constructed from the network layers. RPN layers as well as regression and classification heads are randomly initialized and use standard (non-harmonic) convolution/fully connected layers. Images are resized to set their shortest sides at 600 pixels. The Faster R-CNN is trained with the learning rate  dependant on a particular batch size . Models are trained on the union of VOC 2007 training and validation sets with about 5000 images for 17 epochs, decreasing the learning rate by a multiplicative factor of 0.1 after epoch 15. We train the networks with original and harmonic backbones using the same setting. Additionally, these models are also trained on the combination of training sets of VOC 2007 and VOC 2012, consisting of about 16 500 images, for 12 epochs with learning rate dropped at epoch 9. All models are tested on VOC 2007 test set and the official evaluation metric, the mean average precision (AP), is averaged over 5 runs. Final results are reported in Table~\ref{tab:VOC}  for different depths of ResNet backbones and configurations of the dataset.

From Table~\ref{tab:VOC} we conclude that the models built on our harmonic backbones surpass their conventional convolution-based counterparts in all configurations as well as on both training sets. 
We observe a consistent improvement due to the Harmonic architecture: by 1\% AP for ResNet-50 and 0.8\% AP in case of ResNet-101 using the Faster R-CNN architecture.



\subsection{Object Detection on MS COCO}\label{sec:segmentation:coco}
Common Objects in COntext (COCO) dataset poses a greater challenge due to a higher variety of target classes and generally smaller object sizes.
The networks are trained following the standard procedure, images resized so that their shortest side is 800 pixels. The learning rate is initialized by linear scaling method  using default hyperparameters set up by Chen et al.~\cite{mmdet}. 
All models are trained with standard 12 (24) epochs schedules with learning rate decreased by 10 after epochs 8 (16) and 11 (22). Table~\ref{tab:COCO} shows that the use of our harmonic backbones consistently improves both single-stage RetinaNet and multi-stage Faster and Mask R-CNN detectors by 0.7-1.3 AP with identical training procedures employed.

\begin{table}[t]
\caption{Mean average precision for different backbones and detector types on MS COCO 2017 validation set. All backbones are transformed to FPNs.} \label{tab:COCO}
\vspace{0.3\baselineskip}
\centering
\tabcolsep = .55mm
\begin{tabular}{ lccccc }
 \hline
 \multirow{2}{*}{\textbf{Backbone}} & \multirow{2}{*}{\textbf{Type}} & \multicolumn{2}{c}{\textbf{Box AP} } & \multicolumn{2}{c}{\textbf{Mask AP} } \\
 & & \textbf{12 epochs} & \textbf{24 epochs} & \textbf{12 epochs} & \textbf{24 epochs} \\
 \hline
 ResNet-50 & Faster & 36.4 & \hspace{.4ex} 37.7* & - & - \\
 Harm-ResNet-50 & Faster & \textbf{37.2} & \textbf{38.4} & - & - \\
 ResNet-50 & Retina & \hspace{.4ex} 35.6* & \hspace{.4ex} 36.4* & - & - \\
 Harm-ResNet-50 & Retina & 36.3 & 36.8 & - & - \\
 \hline
 ResNet-101 & Faster & 38.5 & 39.3 & - & - \\
 Harm-ResNet-101 & Faster & \textbf{39.7} & \textbf{40.3} & - & - \\
 ResNet-101 & Retina & \hspace{.4ex} 37.7* & \hspace{.4ex} 38.1* & - & - \\
 Harm-ResNet-101 & Retina & 39.0 & 39.2 & - & -\\
 \hline
 ResNet-50 & Mask & \hspace{.4ex} 37.3* & \hspace{.4ex} 38.5* & \hspace{.4ex} 34.2* & \hspace{.4ex} 35.1* \\
 Harm-ResNet-50 & Mask & \textbf{38.1} & \textbf{38.9} & \textbf{34.7} & \textbf{35.5} \\
 \hline
 ResNet-101 & Mask & \hspace{.4ex} 39.4* & \hspace{.4ex} 40.3* & \hspace{.4ex} 35.9* & \hspace{.4ex} 36.5* \\
 Harm-ResNet-101 & Mask & \textbf{40.7} & \textbf{41.5} & \textbf{36.8} & \textbf{37.3} \\
 \hline
\end{tabular}\.05cm]
*scores reported by~\cite{mmdet}.
\end{table}

These experiments on object detection and localization demonstrate that the harmonic versions of the backbones provide a meaningful improvement of about 1.0 AP in terms of both bounding boxes and masks to the state-of-the-art detection architectures. Our harmonic networks retain this improvement from the purely classification task through the transformation to the Feature Pyramid Networks (FPNs).

\subsection{Semantic Segmentation on Pascal VOC} \label{sec:experiments.semantic}

We now assess our proposed harmonic networks on the task of semantic segmentation using the Pascal VOC 2012 benchmark. Training images are augmented into a set of 10,582 samples as in~\cite{Chen17rethinking}. Performance is measured in terms of intersection over union (IoU) on a large validation set consisting of 1449 images. The segmentation is performed using the DeepLabV3 architecture~\cite{Chen17rethinking}. We extend  PyTorch implementation of this model\footnote{\url{https://github.com/VainF/DeepLabV3Plus-Pytorch}} and retrain baseline models with ResNet-50 and ResNet-101 backbones for 30,000 iterations with batch size 16, learning rate 0.1 and output stride parameter equal to 16.
We replace the backbone model of the segmentation network with harmonic ResNets using two settings for the backbone: (i) converting from the original backbone models or (ii) taking a harmonic models pre-trained on ImageNet (90~epochs), see Table~\ref{tab:imagenet_spec}. The results are summarized in Table~\ref{tab:semantic}. The DeepLabV3 models with harmonic backbones pretrained on ImageNet improve IoU scores by approximately 1.1\%. This experiment also validates the application of harmonic blocks with dilated convolution, which is dissimilar to the classical dense formulation in that the spatial correlation patterns may be weaker due to dilatation. 
DeepLabV3 with harmonic backbone has the strongest average improvement on classes ``chair'' (+9.5\%), ``sheep'' (+3.9\%), ``boat'' (+3.2\%), ``cow'' (+2.9\%) and ``tvmonitor'' (+2.9\%), and performs worse on ``pottedplant'' (-3.4\%) and ``aeroplane'' (-0.8\%). 
A selection of image samples from these classes is presented in Fig.~\ref{fig:segmentations} showing how harmonic networks impact image segmentation quality. We observe some non-trivial improvements of the segmentation masks due to the use of harmonic blocks.

\begin{table}[h]
\caption{Intersection over Union (IoU) of DeepLabV3 architecture semantic segmentation on Pascal VOC 2012 validation set. IoU shown is the median of 5 trials  empirical std. dev.} \label{tab:semantic}
\vspace{0.3\baselineskip}
\centering
\begin{tabular}{ lcccc }
 \hline
 \multirow{2}{*}{\textbf{Backbone}} & \multirow{2}{*}{\textbf{Baseline}} & \textbf{Harm} & \textbf{Harm} & \multirow{2}{*}{\textbf{Benchmark}} \\
 & & \textbf{converted} & \textbf{pre-trained} & \\
 \hline
 ResNet-50 & 76.31  0.07 & 76.65  0.07 & \textbf{77.40  0.08} & - \\
 ResNet-101 & 78.31  0.07 & 77.92  0.11 & \textbf{79.49  0.29} & 77.21~\cite{Chen17rethinking} \\
 \hline
\end{tabular}
\end{table}

\begin{figure}[!t]
\centering
   \includegraphics[width=0.24\linewidth]{graphics/segmentations/317_image.png}
   \includegraphics[width=0.24\linewidth]{graphics/segmentations/317_target.png}
   \includegraphics[width=0.24\linewidth]{graphics/segmentations/317_overlay_base.png}
   \includegraphics[width=0.24\linewidth]{graphics/segmentations/317_overlay_harm.png}\.05cm]
   \includegraphics[width=0.24\linewidth]{graphics/segmentations/363_image.png}
   \includegraphics[width=0.24\linewidth]{graphics/segmentations/363_target.png}
   \includegraphics[width=0.24\linewidth]{graphics/segmentations/363_overlay_base.png}
   \includegraphics[width=0.24\linewidth]{graphics/segmentations/363_overlay_harm.png}\.05cm]
   \includegraphics[width=0.24\linewidth]{graphics/segmentations/1094_image.png}
   \includegraphics[width=0.24\linewidth]{graphics/segmentations/1094_target.png}
   \includegraphics[width=0.24\linewidth]{graphics/segmentations/1094_overlay_base.png}
   \includegraphics[width=0.24\linewidth]{graphics/segmentations/1094_overlay_harm.png}\-.25cm]
   \setlength{\unitlength}{1cm}
   \begin{picture}(12,0.65)
     \put(0,0){Original}
     \put(3.2,0){Annotation}
     \put(6.9,0){DeepLabV3}
     \put(10.0,0){Harm-DeepLabV3}
   \end{picture}
\vspace{-.5\baselineskip}
   \caption{\small Examples of semantic segmentation on Pascal VOC 2012 validation images. The first 4 rows show where harmonic network is more successful than the baseline, while the last row displays case where it fails. DeepLabV3 with ResNet-101 backbone is used.}
\label{fig:segmentations}
\end{figure}

\section{Conclusion} \label{sec:conclusion}

We have presented a novel approach to explicitly incorporate spectral information extracted via DCT into CNN models. 
We have empirically evaluated the use of our harmonic blocks with the well-established state-of-the-art CNN architectures, and shown that our approach  improves results for a range of applications including image classification (0.7-1.2\% accuracy on ImageNet), object detection (0.7-1.1 AP on Pascal VOC and MS COCO) and semantic segmentation (1.1\% IoU on Pascal VOC).
We further establish that the memory footprint of harmonic nets is similar and the computational complexity increases only slightly when compared to the standard convolutional baseline architectures.
We ascertain that harmonic networks can be efficiently set-up by converting the pretrained CNN baselines. 
The use of DCT allows one to order the harmonic block parameters by their significance from the most relevant low frequency to less important high frequencies. This enables efficient model compression by parameter truncation with only minor degradation in the model performance. Current efforts aim at investigating robustness of harmonic networks and at compressing weights according to correlations across filters in depth direction.


\section*{Acknowledgements}
This research was conducted with the financial support of Science Foundation Ireland under Grant Agreement No. 13/RC/2106 and 13/RC/2106\_P2 at the ADAPT SFI Research Centre at Trinity College Dublin, Maynooth University and Dublin City University. ADAPT, the SFI Research Centre for AI-Driven Digital Content Technology, is funded by Science Foundation Ireland through the SFI Research Centres Programme.
We gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs.

\bibliography{biblio}

\end{document}