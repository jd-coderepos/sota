\documentclass{article}








\usepackage[preprint]{neurips_2023}








\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{cite}
\usepackage{amsfonts}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{booktabs}       

\usepackage{bm}
\usepackage{color}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{xspace}
\usepackage{soul}

\usepackage{enumitem}
\usepackage{textcomp}  \usepackage{scalerel}  

\usepackage[colorlinks,linkcolor=blue]{hyperref}

\def\logo{\scalerel*{\includegraphics{logo_new.png}}{\textrm{\textbigcircle}}}
\newcommand{\titledmodelname}{mPLUG-Owl\logo\xspace}
\newcommand{\modelname}{mPLUG-Owl\xspace}
\newcommand{\evalsetname}{OwlEval\xspace}

\title{\titledmodelname: Modularization Empowers Large Language Models with Multimodality}




\author{Qinghao Ye\thanks{Equal contribution}\hspace{1.5mm}, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan\thanks{Corresponding author}, Yiyang Zhou, \\ \textbf{Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu,} \\ \textbf{Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang} \\
DAMO Academy, Alibaba Group \\
{\small \texttt{\{yeqinghao.yqh, shuofeng.xhy, guohai.xgh, ym119608\}@alibaba-inc.com}} 
}


\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce \textbf{\modelname}, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of \modelname involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM, while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set \textbf{\evalsetname}. Experimental results show that our model outperform existing multi-modal models,  demonstrating \modelname's impressive instruction and visual understanding ability, multi-turn conversation ability and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension.
Our code, pre-trained model,  instruction-tuned models, and evaluation set are available at \href{https://github.com/X-PLUG/mPLUG-Owl}{https://github.com/X-PLUG/mPLUG-Owl}. The online demo is available at \href{https://www.modelscope.cn/studios/damo/mPLUG-Owl/}{https://www.modelscope.cn/studios/damo/mPLUG-Owl}.
\end{abstract}



\section{Introduction}
Large language models (LLMs) such as GPT-3 \citep{gpt3}, BLOOM \citep{bloom}, LLaMA \citep{llama} have experienced rapid development to make general artificial intelligence possible, which demonstrates impressive zero-shot abilities on various linguistic applications. However, except GPT-4 \citep{gpt4}, current general LLMs cannot support different modalities of input and develop impressive multimodal abilities. 

Although GPT-4 \citep{gpt4} has exhibited remarkable multimodal abilities, the methods behind its extraordinary abilities remain a mystery. Recently, researchers have been extending LLMs to understand visual inputs in two different paradigms: systematic collaboration and end-to-end trained models. However, systematic collaboration approaches, including Visual ChatGPT \citep{visualchatgpt}, MM-REACT \citep{mmreact}, and HuggingGPT \citep{hugginggpt}, are designed to facilitate the coordination of various vision models or tools to express visual information with text descriptions. However, these approaches may not be able to comprehend specific multimodal instructions due to their lack of alignment with different modalities. Additionally, these approaches may encounter challenges related to inference efficiency and cost. End-to-end models, such as BLIP-2 \citep{blip2}, LLaVA \citep{llava}, and MiniGPT-4 \citep{minigpt4} aim to use unified models to support different modalities. However, these models have some limitations as they take frozen visual models, which may lead to inadequate alignment due to the limited number of parameters. Moreover, they cannot unlock various abilities due to missing unimodal and multimodal instruction.

In this paper, we present \modelname with an innovative modularized training paradigm for large multi-modal language models that can support multiple modalities concurrently, drawing inspiration from the concept of modularization \citep{mplug2, mplug, e2evlp, hitea}. Our method harnesses the power of pre-trained LLM, visual knowledge module, and connected visual abstractor module to achieve effective alignment between images and text, and utilizes a two-stage training scheme to stimulate impressive unimodal and multimodal abilities. Our approach even enhances the strong generation abilities of LLM by modality collaboration between modalities. In the first step, we align the image and text to acquire comprehensive visual knowledge using text-image pairs, which is accomplished by training the  visual knowledge module and abstractor module with the frozen LLM module. Subsequently, we fine-tune mPLUG-Owl with language-only and multi-modal instructions to unlock a range of unimodal and multimodal abilities. We freeze the visual knowledge module and train low-rank adaption (LoRA) \citep{lora} on LLM and visual abstractor module jointly. This approach allows for the effective integration of textual and visual information, facilitating the development of versatile and robust cognitive abilities.

Our experiments on a carefully-built visually related instruction evaluation set \evalsetname shows that \modelname outperforms existing models such as MiniGPT-4 \citep{minigpt4} and LLaVA \citep{llava}. We separately verifies 
\modelname's remarkable abilities in instruction understanding, visual understanding, knowledge transfer, and multi-turn dialogue. Abundant ablation study is performed to show the effectiveness of our training paradigm. Furthermore, we find some unexpected emerging ability such as multi-image correlation, multilingual conversation and scene text understanding.



Our main contributions can be highlighted as follows:
\begin{itemize}
    \item We propose \modelname, a novel  training paradigm for large language models through modularization.
    \item We carefully construct an instruction evaluation set, dubbed \textbf{OwlEval}, to assess the capabilities of different models in the context of visual-related tasks.
    \item Experimental results demonstrate that \modelname excels in multi-modal instruction understanding and multi-turn dialogue, surpassing the performance of existing models.
\end{itemize}

\section{Related Work}
\subsection{Large Language Models}
In recent times, Large Language Models (LLMs) have garnered increasing attention for their exceptional performance in diverse natural language processing (NLP) tasks. Initially, transformer models such as BERT \citep{bert}, GPT \citep{gpt1}, and T5 \citep{t5} were developed with different pre-training objectives. However, the emergence of GPT-3 \citep{gpt3}, which scales up the number of model parameters and data size, showcases significant zero-shot generalization abilities, enabling them to perform commendably on previously unseen tasks. Consequently, numerous LLMs such as OPT \citep{opt}, BLOOM \citep{bloom}, PaLM \citep{palm}, and LLaMA \citep{llama} are created, ushering in the success of LLMs. Additionally, Ouyang et al. \citep{instructgpt} propose InstructGPT by aligning human instruction and feedback with GPT-3. Furthermore, it has been applied to ChatGPT \citep{chatgpt}, which facilitates conversational interaction with humans by responding to a broad range of diverse and intricate queries and instructions.

\subsection{Multi-Modal Large Language Models}
Despite the successful applications of LLMs in natural language processing, it is still struggling for LLMs to perceive other modalities such as vision and audio. Recently, researchers have been extending language models to understand visual inputs in two different paradigms: systematic collaboration and end-to-end trained models. Systematic collaboration approaches, such as Visual ChatGPT \citep{visualchatgpt}, MM-REACT \citep{mmreact}, and HuggingGPT \citep{hugginggpt}, leverage various vision experts or tools to express visual information with text descriptions. Subsequently, large language models, such as ChatGPT, can act as the agents, and be prompted to select the appropriate experts and tools for visual understanding. Finally, LLMs would summarize the output of these experts to answer user queries. 
On the other hand, some approaches \citep{blip2, flamingo, llava} leverage the pre-trained large language model to build unified models for multi-modality.
For example, Flamingo \citep{flamingo} freezes the pre-trained vision encoder and large language model and fuses vision and language modalities with gated cross-attention showing impressive few-shot capabilities. Additionally, BLIP-2 \citep{blip2} designs Q-Former to align the visual features from the frozen visual encoder and large language models with Flan-T5 \citep{flant5} and OPT \citep{opt}. Moreover, PaLM-E \citep{palm-e} directly inputs features from sensor modalities with PaLM \citep{palm}, which has 520 billion parameters, contributing to robust performance in real-world perceptions. Furthermore, some powerful instruction-tuned language models that built upon open-sourced foundation model LLaMA \citep{llama}, such as Alpaca \citep{alpaca} and Vicuna \citep{vicuna}, exhibit comparable performance to ChatGPT \citep{chatgpt} and GPT-4 \citep{gpt4}. MiniGPT-4 \citep{minigpt4} and LLaVA \citep{llava} align these finetuned models with extracted visual features from the frozen visual backbone. In contrast, \modelname not only aligns the representation between the vision and language foundation model (e.g. CLIP and LLaMA) in terms of knowledge acquisition and grounding to the real world but also can understand language and multi-modal instructions, showcasing strong zero-shot generalization and multi-turn conversation capabilities.


\section{mPLUG-Owl}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figs/compare.pdf}
    \caption{Comparison between different training paradigms. All of these methods are trained in a two-stage fashion. Stage 1 stands for pre-training and Stage 2 represents instruction tuning.}
    \label{fig:compare_method}
    \vspace{-2mm}
\end{figure}



\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figs/model.pdf}
    \caption{Our training paradigm and model overview.}
    \label{fig:model}
    \vspace{-2mm}
\end{figure*}

\subsection{Architecture Overview}


As illustrated in Figure \ref{fig:compare_method}, there exist mainly three types of end-to-end multimodal LLMs: 1) models that utilize limited parameters with frozen LLM and visual models during pretraining and instruction tuning, such as MiniGPT4; 2) models that incorporate trainable LLMs and frozen visual models, exemplified by Kosmos-1; and 3) models that involve trainable LLMs during instruction tuning and frozen visual models, as seen in LLaVA. Nevertheless, these models exhibit certain constraints since they depend on frozen visual models, which can lead to insufficient alignment due to the limited number of parameters. Furthermore, they fail to effectively stimulate a diverse set of abilities, as they lack both unimodal and multimodal instruction.

 
To this end, we propose \modelname, a multi-modal language model that is capable of perceiving various modalities while taking the visual context and information into account and generating corresponding outputs. Specifically, as illustrated in Figure \ref{fig:model}, \modelname consists of a vision foundation model  to encode the visual knowledge, a language foundation model , and a visual abstractor module . We first obtain dense image representations from the pre-trained visual foundation model . However, such dense features would fragment the fine-grained image information and bring large computation due to the lengthy sequence when feeding into . To mitigate this issue, we employ the visual abstractor module  to summarize visual information within several learnable tokens, thereby obtaining higher semantic visual representations and reducing computation, as illustrated in Figure \ref{fig:model}. The visual representations are combined with text queries and fed into the language model to generate the response.

\subsection{Training Scheme}

\paragraph{Multimodal Pretraining}
Large-scale language models, such as GPT-3 \citep{gpt3} and LLaMA \citep{llama}, are trained on extensive and diverse data collected from the internet, providing them with a comprehensive understanding of the world. This vast knowledge base endows these models with remarkable capabilities across a range of tasks. However, the utilization of visual information in such models remains underexplored. Previous approaches \citep{minigpt4, llava} have employed a limited number of additional parameters to learn the alignment between visual data and language models, constraining their capacity to comprehend complex visual information. To enhance the ability of large-scale language models to perceive visual information while integrating their internal abilities, we propose a novel training paradigm that incorporates a trainable visual backbone  and an additional visual abstractor , while maintaining the pre-trained language model  in a frozen state. This approach enables the model to effectively capture both low-level and higher semantic visual information and align it with the pre-trained language model without compromising its performance.


\paragraph{Joint Instruction Tuning}
Upon completion of the prior phase, the model acquires the ability to retain a considerable amount of knowledge and provide reasonable answers to human queries. Nonetheless, it continues to exhibit challenges in generating coherent linguistic responses. As posited in GPT-3 \citep{gpt3}, refining the model through instruction tuning is essential for accurately discerning user intentions.
Previous attempts \citep{mplug, mplug2} in multi-modal learning have demonstrated that joint learning from uni-modal and multi-modal sources can lead to significant improvements owing to the collaboration between different modalities. Building on this insight, we present a novel vision-language joint instruction tuning strategy to facilitate better alignment between \modelname and human instructions and intentions. 
Specifically, given that the model can comprehend the visual concepts and knowledge depicted in images through visual knowledge learning, we freeze the entire model and employ low-rank adaption (i.e., LoRA \citep{lora}) to adapt  by training multiple low-rank matrices for efficient alignment with human instructions.
For each data record, we unified them in a snippet of conversation following Vicuna \citep{vicuna}, and we compute the loss on the response. During the training, we accumulate the gradient for text-only instruction data and multi-modal instruction data for multiple batches and updated the parameters. Therefore, by joint training with both language and multi-modal instructions, \modelname can better understand a wide range of instructions and respond with more natural and reliable output. Moreover, our approach can easily handle various text and multi-modal instructions without the need for realignment of the vision and language models, as required by methods such as MiniGPT-4 \citep{minigpt4} and LLaVA \citep{llava}. 

\paragraph{Training Objective} 
The model is trained using the language modeling task, which entails learning to generate subsequent tokens based on the preceding context. The primary objective of the training process is to maximize the log-likelihood of the tokens. It is important to note that only discrete tokens, such as text tokens, are considered in the calculation of the training loss. Most significantly, the emergence of diverse capabilities resulting from the training task during the joint instruction tuning stage enhances the performance of \modelname in downstream applications.

\section{Experiment}
\subsection{Experimental Setup}
\paragraph{Model Settings.}We choose ViT-L/14 \citep{vit} as the visual foundation model  which has 24 layers with hidden dimension set as 1024 and patch size set as 14. For faster convergence, the ViT is initialized from CLIP ViT-L/14 model pre-trained via contrastive learning. Different with LLaVA \citep{llava} and MiniGPT-4 \citep{minigpt4}, to demonstrate the effectiveness and generalization ability, we utilize raw LLaMA-7B \citep{llama} rather than its instruction-tuned variants such as Alpaca \citep{alpaca} and Vicuna \citep{vicuna}. The total number of parameters of \modelname is about 7.2B. More details about hyper-parameters can be found in Appendix.

\paragraph{Data and Training Details.} For the first stage, we utilize the image-caption pairs from several datasets, including LAION-400M \citep{laion400m}, COYO-700M \citep{coyo700m}, Conceptual Captions \citep{conceptualcap} and MSCOCO \citep{cococap}. We use a batch size of 2.1 million tokens and train \modelname for 50k steps, corresponding to about 104 billion tokens. We adopt the AdamW optimizer with , and set the learning rate and weight decay to 0.0001 and 0.1 respectively. We warm up the training with 2k warm-up steps then decay the learning rate with the cosine schedule. The input image is randomly resized to . Besides, we tokenize the text input with SentencePiece \citep{sentencepiece} tokenizer. 
For the second stage, we gather pure text instruction data from three distinct sources: 102k data from the Alpaca \citep{alpaca}, 90k from the Vicuna \citep{vicuna}, and 50k from the Baize \citep{baize}. Additionally, we utilize 150k multi-modal instruction data from the LLaVA dataset \citep{llava}.
We train \modelname for 2k steps with the batch size 256, and the learning rate is set to 0.00002.


\paragraph{Baselines.} 
We compare our \modelname with end-to-end models and systematic collaboration approaches as follows:
\begin{itemize}
    \item \textit{OpenFlamingo} \citep{openflamingo} is an open-source version of Flamingo \citep{flamingo} model. We use the released code of OpenFlamingo-9B\footnote{\href{https://github.com/mlfoundations/open_flamingo}{https://github.com/mlfoundations/open\_flamingo}} to run zero-shot generation.
    \item \textit{BLIP-2} \citep{blip2} is pre-trained through bootstrapped learning from off-the-shelf frozen pre-trained image models and large language models using an efficient pre-training strategy. We use the released code of BLIP-2 ViT-G FlanT5\footnote{\href{https://github.com/salesforce/LAVIS/tree/main/projects/blip2}{https://github.com/salesforce/LAVIS/tree/main/projects/blip2}} to perform zero-shot generation.
    \item \textit{MiniGPT-4} \citep{minigpt4} utilizes a single projection layer to align visual information from a pre-trained vision encoder with LLM. Specifically, they employ the same visual encoder as used in BLIP-2, a ViT coupled with their pre-trained Q-Former, and Vicuna as LLM.
    We use the released demonstration\footnote{\href{https://huggingface.co/spaces/Vision-CAIR/minigpt4}{https://huggingface.co/spaces/Vision-CAIR/minigpt4}} to perform image-instruction generation.
    \item \textit{LLaVA} \citep{llava} applies a single projection layer to convert image features from pre-trained CLIP visual encoder ViT-L/14 into the language embedding space of Vicuna. We use their released demonstration\footnote{\href{https://llava.hliu.cc}{https://llava.hliu.cc}} to perform image-instruction generation.
    \item \textit{MM-REACT} \citep{mmreact} integrates ChatGPT/GPT-4 with various specialized vision experts to achieve multimodal reasoning and action. We use their released demonstration\footnote{\href{https://huggingface.co/spaces/microsoft-cognitive-service/mm-react}{https://huggingface.co/spaces/microsoft-cognitive-service/mm-react}} to get responses. 
\end{itemize}






 
\subsection{Quantitative analysis}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5 \textwidth]{figs/mPLUG_Owl_compare_result_nomm.pdf}
    \caption{The comparison between mPLUG-Owl and baselines on \evalsetname with manual evaluation metrics. The order of response quality ranking is as follows: A > B > C > D.}
    \label{fig:compare_result}
    \vspace{-2mm}
\end{figure}

In order to comprehensively evaluate various models, we construct a visually-related evaluation set \textbf{\evalsetname} by collecting 82 artificially constructed questions based on 50 images, where 21 from MiniGPT-4, 13 from MM-REACT, 9 from BLIP-2, 3 from GPT-4 and 4 collected by us. Partial images have multiple rounds of questions, refers to multi-turn conversation cases. These questions examine a variety of model capabilities including natural image understanding, diagram and flowchart comprehension, optical character recognition (OCR), multi-modal creation, knowledge-intensive QA, and referential interaction QA. As questions are open-ended, we employ manual evaluation metrics to rate the model's responses as A, B, C, or D following the rating method proposed in Self-Instruct~\citep{self-instruct}. 



We manually score 82 responses given by \modelname and baselines. The comparison results are shown in Figure~\ref{fig:compare_result}. First, \modelname gets 66  and , while the most competitive baseline MiniGPT-4 gets 54. Second, \modelname doesn't get any  scores, outperforming all the models. These results suggest that \modelname can better understand both instructions and images, which results in a stronger capability in generating satisfactory responses. For a fair comparison, we have excluded those cases in which MM-REACT failed to make predictions. The results are shown separately in Figure~\ref{fig:mm-react} and \modelname still exhibits superior performance.


To separately examine the single-turn and multi-turn conversation capabilities, we reorganize 82 questions into a single-turn conversation set and a multi-turn conversation set. The former contains the first question from 50 images. The latter contains 52 questions from multi-turn conversation cases. As shown in Figure~\ref{fig:compare_result_s_m}, the \modelname achieves outstanding performance in both single-turn and multi-turn conversations.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1 \textwidth]{figs/mPLUG_Owl_compare_result_s_mturn.png}
    \caption{The comparison results of 50 single-turn responses (left) and 52 multi-turn responses (right) among mPLUG-Owl and baselines on \evalsetname with manual evaluation metrics.}
    \label{fig:compare_result_s_m}
    \vspace{-2mm}
\end{figure}



\subsection{Ablation Study}

We ablate the two-stage training scheme and the data modality of instruction tuning. Six dimensions of abilities are defined to complete visually related tasks, as shown in Table~\ref{fig:mult-modle-level}. For each question, we manually label the required abilities and annotate which abilities are reflected in the model's response. Table~\ref{tb:ablation} shows the  ability accuracy of different variants of \modelname.


\begin{table*}
\centering
\scalebox{0.92}{
\begin{tabular}{c|c|p{8.5cm}}
\midrule
 & Meaning                       & \multicolumn{1}{c}{Definition}                                                                                                                                                                                           \\ \midrule
IU                           & Instruction Understanding     & \begin{tabular}[c]{@{}l@{}}1. Understand text instruction.\\ 2. Do not require a correct answer, but the response \\  \hspace{0.35cm} should be related to the instruction.\end{tabular}                                                                                                                                     \\ \midrule
VU                           & Visual Understanding          & \begin{tabular}[c]{@{}l@{}}1. Identify image information.\\ 2. The answer faithfully reflects over 60\% of  visual \\  \hspace{0.35cm} information in the image.\end{tabular}                                                                                                                              \\ \midrule
OCR                           & Optical Character Recognition & \begin{tabular}[c]{@{}l@{}}1. Recognize text information in the image.\\ 2. The answer faithfully reflects over 60\% of  text \\  \hspace{0.35cm}information in the image.\end{tabular}                                                                                                                                      \\ \midrule
KTA                           & Knowledge Transfer Ability     & \begin{tabular}[c]{@{}l@{}}1. Transfer knowledge between language and vision.\\       \hspace{0.35cm}(1) understand textual and visual content\\       \hspace{0.35cm}(2) align and transfer visual and language knowledge \\ 2. Answers are mostly accurate with accuracy rate over 80\%.\\ \end{tabular}                                                                                                          \\ \midrule
RA                           & Reasoning Ability             & \begin{tabular}[c]{@{}l@{}}1. Combine image and text for reasoning.\\     \hspace{0.35cm}(1) understand textual and visual content\\     \hspace{0.35cm}(2) conduct multi-step reasoning\\     \hspace{0.35cm}(3) generate answers based on multi-step reasoning process\\ 2. The final answer is essentially correct, but it lacks an \\     \hspace{0.35cm}explicit reasoning process. Alternatively, the final answer is \\     \hspace{0.35cm}mostly correct, and the reasoning process is over 80\% accurate.\\ 3. For example\\     \hspace{0.35cm}(1) Commonsense Knowledge Reasoning\\        \hspace{0.35cm}(2) Counterfactual Reasoning\\        \hspace{0.35cm}(3) Spatial Relation Reasoning  \\        \hspace{0.35cm}(4) Numerical Computation\\       \hspace{0.35cm}(5) Coding\\        \end{tabular} \\ \midrule
MDA                           & Multi-turn Dialogue Ability   & \begin{tabular}[c]{@{}l@{}}1. Understand instructions and handle multi-turn conversations.\\ 2. It includes clear references to multiple conversations and \\  \hspace{0.35cm}handles natural language semantics in context effectively.\\ 3. The semantics and references are mostly correct, with an \\  \hspace{0.35cm}accuracy rate of over 80\%.\end{tabular}                                                                                                                              \\ \midrule
\end{tabular}
}
\caption{The definition of 6 abilities to complete visually-related tasks.}
\vspace{-5mm}
\label{fig:mult-modle-level}
\end{table*}


\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{table*}[!ht]
\centering
\renewcommand{\arraystretch}{1}
\scalebox{0.75}{
\begin{tabular}{c  c  c  c  p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm}}
\midrule
&\multirow{2}{*}{\tabincell{c}{\textbf{Multimodal}\\\textbf{Pretraining}}}&\multirow{2}{*}{\tabincell{c}{\textbf{Pure Text}\\\textbf{Instruction}}}&\multirow{2}{*}{\tabincell{c}{\textbf{Multi-modal}\\\textbf{Instruction}}}&\multicolumn{6}{c}{\textbf{Ability}}\\
\cmidrule(lr){5-10}
&&&&\textbf{IU}&\textbf{VU}&\textbf{OCR}&\textbf{KTA}&\textbf{RA}&\textbf{MDA}\\
\midrule
r1&\checkmark&&&&&&&&\\
r2&&\checkmark&\checkmark&&&&&&\\
r3&\checkmark&\checkmark&&&&&&&\\
r4&\checkmark&&\checkmark&&&&&&\\
r5&\checkmark&\checkmark&\checkmark&\textbf{100.0}&\textbf{95.2}&\textbf{56.7}&\textbf{87.5}&\textbf{80.0}&\textbf{95.0}\\
\midrule
\midrule
\multicolumn{4}{c}{MiniGPT-4 \citep{minigpt4}}&97.6&81.0&40.0&83.3&65.7&75.0\\
\midrule
\end{tabular}
}
\caption{The ablation results. Each value represents the proportion of questions where the corresponding ability is correctly reflected in the model's response. IU: instruction understanding, VU: visual understanding, OCR: optical character recognition, KTA: knowledge transferability, RA: reasoning ability, MDA: multi-turn dialogue ability.}
\label{tb:ablation}
\end{table*}

\noindent\textbf{Training Strategy Ablation.}
As shown in Table~\ref{tb:ablation}, without joint instruction tuning, the model is 
 not good at instruction understanding and fail to generalize pre-training abilities to other tasks (r1 vs r5). With the instruction tuning alone, although the model can better comprehend instructions, the model is incapable of achieving promising performance in visual knowledge-related tasks due to lacking of visually-related knowledge pretraining (r2 vs r5). With both multimodal pretraining and joint instruction tuning, the model achieves the best performance and demonstrates the effectiveness of our two-stage training scheme.

\noindent\textbf{Instruction Data Ablation.}
By comparing r3 with r4, text-only instruction tuning brings more improvement in instruction understanding, while multi-modal instruction tuning achieves better knowledge and reasoning capabilities. This is due to that visual question answering mainly requires the alignment of vision and language knowledge, which is not optimized during text-only instruction tuning. Besides, we also verify that introducing multi-modal data during instruction tuning could further improve the model's performance on text-only tasks, as shown in Table \ref{tab:text-only result} (r5 vs r4). Concretely, following the evaluation setting as Vicuna\citep{vicuna}, for each question, we pair the response of each model with the one given by ChatGPT and prompt ChatGPT\footnote{Without access to the GPT-4, we use the ChatGPT as the suboptimal scorer.} to give two scores respectively for these two responses. Table \ref{tab:text-only result} shows the total score and the score ratio with the ChatGPT score as a reference. 







\begin{table*}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{lccccc}
\toprule
 Model & Tuning Strategy & Model Score & ChatGPT Score & Ratio  \\
\midrule
Alpaca-7B   & Full & 573 & 708 & 80.93\% \\
Vicuna-7B & Full  &612 & 684 & 89.47\%\\
mPLUG-Owl w/o multimodal tuning (r4) & LoRA  & 587 & 682 & 86.07\%\\
mPLUG-Owl (r5) & LoRA & 600 & 692 & 86.71\%\\
\bottomrule
\end{tabular}
}
\caption{The performance of 80 text-only questions from Vicuna\citep{vicuna} assessed by ChatGPT. }
\label{tab:text-only result}
\end{table*}




\subsection{Qualitative Analysis}



In this section, we show qualitative results from our evaluation set \evalsetname.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1 \textwidth]{figs/case_JT.jpg}
    \caption{A comparison of Knowledge-intensive QA.}
    \label{fig:case_JT}
    \vspace{-2mm}
\end{figure}

\paragraph{Knowledge-intensive QA}
As shown in Figure~\ref{fig:case_JT}, the instruction expects the model to identify the movie characters in the image. MM-REACT is unable to provide an effective response to the instruction, while MiniGPT-4 understands the instruction but failed to answer the movie characters. In contrast, \modelname answers four out of the five characters present in the image. This demonstrates that \modelname has a better understanding of the knowledge in the image.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1 \textwidth]{figs/case_Yao_Ming.jpg}
    \caption{A comparison of Multi-turn Conversation.}
    \label{fig:case_Yao_Ming}
    \vspace{-2mm}
\end{figure}


\paragraph{Multi-round Conversation}
The instruction in Figure~\ref{fig:case_Yao_Ming} requires the model to identify the content of the image based on the referential information. The baseline models often made mistakes when faced with referential expressions related to spatial orientation, human behavior, and target attributes in the questions, whereas \modelname provided the most accurate response. This capability stems from \modelname's fine-grained understanding of the image, allowing it to locate the corresponding part of the image based on the referential information in the instruction.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1 \textwidth]{figs/case_Final.jpg}
    \caption{A comparison of Reasoning QA.}
    \label{fig:case_Final}
    \vspace{-2mm}
\end{figure}

\paragraph{Reasoning}
Figure~\ref{fig:case_Final} shows an instruction asking models to give a prediction based on visual information and explain the reason. 
\modelname analyzes the characteristics of the two teams from the aspects of the lineup and tactics and uses them to reason for the outcome. Although MiniGPT-4 also performs well, its persuasiveness in reasoning is slightly inferior to \modelname.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1 \textwidth]{figs/OwlvsGPT4.jpg}
    \caption{A comparison of Joke Understanding.}
    \label{fig:case_GPT4}
    \vspace{-2mm}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{figs/Memes_and_Jokes_scoreA.pdf}
    \caption{More cases of Jokes Comprehension by \modelname.}
    \label{fig:Memes_and_Jokes_scoreA}
    \vspace{-2mm}
\end{figure}



\paragraph{Joke Comprehension}
The case in Figure~\ref{fig:case_GPT4} comes from the GPT-4\citep{gpt4}, which requires the model to understand and explain a visually related joke.
GPT-4 not only follows the instructions in performing analysis panel by panel but also almost perfectly understands the humor of the charging method. \modelname also understands this unusual humor, but it incorrectly identified the ``VGA'' to ``USB''. This is mainly due to the limitation of visual information in our training data. More cases about joke comprehension are shown in Figure~\ref{fig:Memes_and_Jokes_scoreA}.






\section{Discussion and Limitation}
In this section, we show some nascent abilities of \modelname that is not yet fully developed and discuss the limitation. Part of cases (without scores) in this section are not in \evalsetname.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1 \textwidth]{figs/multi-image.jpg}
    \caption{Multi-image correlation cases.}
    \label{fig:appendix_case_twoimg}
    \vspace{-2mm}
\end{figure}

\paragraph{Multi-image Correlation}
In Figure~\ref{fig:appendix_case_twoimg}, \modelname shows a emerging but not strong vision correlation capability across multiple images. In the left case, the model could identify an identical person in two images and correctly tell the difference of cloth color. But in the left case, the model fails to relate 4 images and produces some text hallucinations. 


\begin{figure}[!ht]
    \centering
    \includegraphics[width=1 \textwidth]{figs/multi-language.jpg}
    \caption{Example prompt of multilingual understanding which showcases the multilingual abilities across Chinese, French, and Japanese, respectively.}
    \label{fig:multilingual}
    \vspace{-2mm}
\end{figure}

\paragraph{Multilingual Conversation}
Besides English, we further test the model's multilingual ability. As shown in Figure~\ref{fig:multilingual}, although there is no multilingual data during our two-stage training, \modelname shows a promising multilingual understanding for Chinese, French and Japanese. We mainly attribute this ability to the raw text knowledge in LLaMa\citep{llama}. However, due to the lacking of  multilingual training, \modelname may fail to response in corresponding languages. 


\paragraph{Scene Text Understanding}
In Figure \ref{fig:OCR_1_scoreB}, mPLUG-Owl demonstrates its OCR ability in some simple scenes, but we can see that the model's perception of numbers in images is still limited.
However, for the OCR of complex scenes, as shown in Figure \ref{fig:OCR_1_scoreC_a}-\ref{fig:OCR_1_scoreC_b}, the performance of mPLUG-Owl is more general, mainly because the perception of numbers in images is weak, which affects the subsequent reasoning calculation.

\paragraph{Vision-only Document Comprehension}
Although we did not use any document annotation data for training, the model exhibited some text recognition and document understanding capabilities. Hence, we delved deeper into the combination of document understanding and functionality of our model. as illustrated in Figure \ref{fig:document_app}, we explored movie review writing, code generation, code explanation, chat summary, and application guidance. The model show decent performance in (a) and (b), but still, had some errors. Meanwhile, it was unable to provide usable responses in (d), (e), and (f). Therefore, there is further scope to explore our model's potential in document understanding and downstream applications.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1 \textwidth]{figs/document_cases.pdf}
    \caption{Examples about various document understanding and application.}
    \label{fig:document_app}
    \vspace{-2mm}
\end{figure}

\paragraph{Open-ended Creation}
mPLUG-Owl performs well in the creation of poetry, lyrics, advertisements and other works based on images. Its performance in some cases is shown in Figure \ref{fig:create_scoreA}-\ref{fig:copywriting}. However, further exploration is needed for more functional and practical creations.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1 \textwidth]{figs/create_scoreA.pdf}
    \caption{Open-ended creation cases.}
    \label{fig:create_scoreA}
    \vspace{-2mm}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1 \textwidth]{figs/create.png}
    \caption{Copywriting cases.}
    \label{fig:copywriting}
    \vspace{-2mm}
\end{figure}



\section{Conclusion}
We propose \modelname, a novel training paradigm that enhances the multi-modal abilities of large language models (LLMs). Our approach consists of modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module, which can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. We employ a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. Experimental results demonstrate the impressive capabilities of \modelname, indicating its potential for various applications in multi-modal generation.

\bibliographystyle{abbrvnat}
\clearpage
\bibliography{reference}



\clearpage
\newpage
\appendix
\section{Training Hyperparameters}
We report the detailed model training hyperparameters for visual knowledge learning in Table~\ref{tbl:hyperparam:pt} and vision-language joint instruction tuning in Table~\ref{tbl:hyperparam:ft}.


\begin{table}[ht]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Hyperparameters} & \\ \midrule
Training steps             &       50,000 \\
Warmup steps                      &       375 \\
Max length        &       512 \\
Batch size of image-caption pairs  &       4,096 \\
Optimizer & AdamW \\
Learning rate & 2e-4 \\
Learning rate decay & Cosine \\
Adam  & 1e-6 \\
Adam  & (0.9, 0.98) \\
Weight decay & 0.01 \\
\bottomrule
\end{tabular}
\vspace{1ex}
\caption{Training hyperparameters for multi-modal pre-training stage.}
\label{tbl:hyperparam:pt}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Hyperparameters} & \\ \midrule
Training steps                   &       2,000 \\
Warmup steps                      &       50 \\
Max length         &       1,024 \\
Batch size of text instruction data  &       128 \\
Batch size of multi-modal instruction data  &   128 \\
Optimizer & AdamW \\
Learning rate & 2e-5 \\
Learning rate decay & Cosine \\
AdamW  & 1e-6 \\
AdamW  & (0.9, 0.999) \\
Weight decay & 0.0001 \\
\bottomrule
\end{tabular}
\vspace{1ex}
\caption{Training hyperparameters for vision-language joint instruction tuning stage.}
\label{tbl:hyperparam:ft}
\end{table}

\section{Comparison with MM-REACT}
\begin{figure}[!ht]
     \centering
     \includegraphics[width=0.7 \textwidth]{figs/mm-react.jpg}
     \caption{The comparison results which exclude the cases that were generated unsuccessfully by MM-REACT.}
     \label{fig:mm-react}
     \vspace{-2mm}
\end{figure}




























\begin{figure}[!ht]
    \centering
    \includegraphics[width=1 \textwidth]{figs/OCR_1_scoreB.pdf}
    \caption{OCR of simple scenes (mostly scenes with few numbers and no calculation a).}
    \label{fig:OCR_1_scoreB}
    \vspace{-2mm}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1 \textwidth]{figs/OCR_1_scoreC.pdf}
    \caption{OCR of complex scenes (a).}
    \label{fig:OCR_1_scoreC_a}
    \vspace{-2mm}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figs/OCR_2_scoreC.pdf}
    \caption{OCR of complex scenes (b).}
    \label{fig:OCR_1_scoreC_b}
    \vspace{-2mm}
\end{figure}

\end{document}
