\section{Program Context Analysis}\label{sec:append_var}

\ours emphasizes the importance of program context for reasoning transferability, owing to the analogy between the program to program context and the sentence to natural context drawn in~\figref{overview}.
To investigate it, we explore the effect of different program context design choices on reasoning transferability by conducting experiments on well-designed \oursmath variants.

\subsection{The Necessairty of Program Context}

\begin{table}[tbp]
  \small
  \centering
  \scalebox{0.98}{
    \begin{tabular}{lcc}
    \toprule
    \textbf{Models} & \textbf{EM} & \textbf{F1} \\
    \midrule
     BART-Large  & $66.2$  & $69.2$  \\
     \midrule
     \oursbartmath \textit{without} program context & $67.4$ & $70.5$ \\ 
     \oursbartmath \textit{with} ~~$0$ irrelevant variable & $71.5$  & $74.5$  \\
     \oursbartmath \textit{with} $10$ irrelevant variables & $74.6$  & $77.5$  \\
     \oursbartmath \textit{with} $30$ irrelevant variables & $75.2$  & $78.1$  \\
    \bottomrule
    \end{tabular}}
  \caption{The DROP performance with different numbers of irrelevant variables in \oursbartmath pre-training.}
  \label{tab:irr}\end{table}

To verify the necessairty of program context, we experiment with \oursmath without program context, i.e. a variable-free \oursmath variant whose program context is empty.
Taking the example of \ours-Math in~\figref{program_context}, the program is transformed into \texttt{\small 152.0\,+\,99.0\,-\,70.3}.
The experimental results are shown in \tabref{irr}.
One can see that there is a dramatic performance drop in the variant compared to \oursmath, verifying the importance of program context.

\subsection{The Variables Design in Program Context}

In the pre-training task of \ours-Math, the program context is several floating-point variables.
These variables include necessary variables (i.e., variables required by the program) and irrelevant variables.
The irrelevant variables exist to make the program context closer to the natural context which generally contains irrelevant sentences.
For example, given the program \texttt{\small a + b} and the program context \texttt{\small a = 1; b = 2; c = 3; d = 4;}, variables \texttt{\small c} and \texttt{\small d} are what we refer to as irrelevant variables.
This is motivated by the fact that passages are usually full of irrelevant information regarding a specific question in NL downstream tasks.

In this section, we explore impacts on pre-training effectiveness brought by numbers of irrelevant variables.
Empirically, we experiment on pre-training with $0$, $10$, $30$ irrelevant variables.
The total length of $30$ irrelevant variables approaches the maximum input length of pre-trained LMs, and thus we do not try more settings.
The experimental results are shown in \tabref{irr}.
As observed, (i) models can still learn numerical reasoning during pre-training where the program context is free from irrelevant variables, though less effective. (ii) the setting of $30$ irrelevant variables brings BART-Large more performance improvement than the setting of $10$ irrelevant variables.
Considering there are plenty of lengthy passages in the DROP dataset, we therefore hypothesize that the noise level brought by irrelevant variables in the program context during pre-training should be made closer with the counterpart in the natural context during fine-tuning.

\section{Experimental Setup}\label{sec:append_dataset}

\subsection{Dataset Setup}

\tabref{dataset-data} presents some statistics about our experimental datasets.
Below we introduce each dataset in detail.

\paragraph{DROP} A reading comprehension benchmark to measure \textit{numerical reasoning} ability over a given passage~\cite{dua-etal-2019-drop}.
It contains three subsets of questions: \textit{span}, \textit{number}, and \textit{date}, each of which involves a lot of numerical operations. Unlike traditional reading comprehension datasets such as SQuAD \cite{rajpurkar-etal-2016-squad} where answers are always a single span from context, several answers in the \textit{span} subset of DROP contains multiple spans.
The \textit{number} and \textit{date} answers are mostly out of context and need generative-level expressiveness.

\paragraph{HotpotQA} An extractive reading comprehension dataset that requires models to perform \textit{multi-hop reasoning} over different passages~\cite{yang-etal-2018-hotpotqa}.
It contains two settings (i) \textit{Distractor}: reasoning over $2$ gold paragraphs along with $8$ similar distractor paragraphs and (ii) \textit{Full wiki}: reasoning over customized retrieval results from full Wikipedia passages.
We experiment with its distractor setting since retrieval strategy is beyond our focus in this work.

\begin{table}[t]
  \centering
  \small
  \scalebox{0.9}{
    \begin{tabular}{lcccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[4]{*}{\bf Dataset}} & \multicolumn{2}{c}{\bf Train} & \multicolumn{2}{c}{\bf Dev}
    \\
         \cmidrule(lr){2-3}
     \cmidrule(lr){4-5}
          & {\# Questions} & {\# Docs} & {\# Questions} & {\# Docs} \\
    \midrule
    DROP & $77,409$  & $5,565$  &  $9,536$ & $582$ \\
    HotpotQA & $90,564$  & $90,564$  & $7,405$ & $7,405$\\ 
    TAT-QA & $13,215$ & $2,201$ & $1,668$  & $278$ \\
    SVAMP   & --  & --  & $726$  & $726$ \\
    EQUATE & --  & -- & $9,606$ & $9,606$ \\
    LogiQA & $6,942$  & $6,942$ & $868$ & $868$ \\
    \bottomrule
    \end{tabular}
    }
  \caption{The statistics of our experimental datasets.}
  \label{tab:dataset-data}\end{table}

\paragraph{TAT-QA} A question answering benchmark to measure reasoning ability over \textit{hybrid} context, i.e., passages and tables~\cite{zhu-etal-2021-tat}.
It is curated by combing paragraphs and tables from real-world financial reports.
According to the source(s) the answers are derived from, the dataset can be divided into three subsets: \textit{Table}, \textit{Text} and \textit{Table-Text(both)}.

\paragraph{EQUATE} The first benchmark dataset to explore \textit{quantitative reasoning} under the task of natural language inference~\cite{ravichander-etal-2019-equate}.
As a test-only dataset, it requires fine-tuned models on MNLI to perform \textit{zero-shot} natural language inference tasks over quantitative statements described in (premise, hypothesis) pairs to reach final entailment decisions.

\paragraph{LogiQA} A multi-choice reading comprehension dataset that evaluates the \textit{logical reasoning} ability, whose questions are designed by domain experts~\cite{ijcai2020-501}.
It contains four types of logical reasoning, including categorical reasoning, disjunctive reasoning, conjunctive reasoning and conditional reasoning.


\paragraph{SVAMP} A challenging math word problem dataset~\cite{patel-etal-2021-nlp}. 
It is designed specifically to hack models who leverage spurious patterns to perform arithmetic operations without true understanding of context.
We only keep addition and subtraction problems in accordance with our pre-training coverage.

\subsection{Baseline Setup}

We summarize the baseline methods in short below, and refer readers to their papers for more details.
(i) On \textbf{DROP}, we include two families of models for comparison: specialized models such as NumNet(+)~\citep{ran-etal-2019-numnet}, MTMSN~\citep{hu-etal-2019-multi}, NeRd~\citep{Chen2020Neural}, QDGAT~\cite{chen-etal-2020-question} and language models such as GenBERT~\citep{geva-etal-2020-injecting} and  PReaM~\citep{DBLP:journals/corr/abs-2107-07261}.
(ii) Similarly, on \textbf{HotpotQA} (Distractor), specialized model baselines include DFGN~\citep{qiu-etal-2019-dynamically}, SAE~\citep{DBLP:conf/aaai/TuHW0HZ20}, C2F Reader~\citep{shao-etal-2020-graph} and the SOTA model HGN~\citep{fang-etal-2020-hierarchical}. The language model baselines consist of BERT~\citep{devlin-etal-2019-bert}, SpanBERT~\citep{joshi-etal-2020-spanbert} and ReasonBERT~\citep{deng2021reasonbert}.
(iii) On \textbf{TAT-QA}, we adopt the official baselines, including \tapas~\citep{herzig-etal-2020-tapas}, NumNet+ V2 and the SOTA model \tagop~\citep{zhu-etal-2021-tat}.
(iv) On \textbf{EQUATE}, we compare our methods with BERT~\citep{devlin-etal-2019-bert}, GPT~\citep{radford2019language} and Q-REAS~\citep{ravichander-etal-2019-equate}.
(v) On \textbf{LogiQA}, we compare our methods with Co-Matching Network~\citep{wang-etal-2018-co} and the SOTA model DAGN~\citep{huang-etal-2021-dagn}.


\section{Implementation Details}\label{sec:append_imp}

\subsection{\oursroberta on Different Datasets}

On \textbf{DROP}, we cast the span selection task as a sequence tagging problem following~\citet{segal-etal-2020-simple}.
On \textbf{TAT-QA}, we in-place substitute the RoBERTa-Large encoder in \tagop~\citep{zhu-etal-2021-tat} with our \oursroberta to verify its effectiveness, and keep the rest of the components unchanged.
On \textbf{HotpotQA}, we train two classifiers independently to predict the start and end positions of the answer span, as done in~\citet{devlin-etal-2019-bert}.
On \textbf{EQUATE}, we train a classifier to perform sequence classification on concatenated premise-hypothesis pairs.
Notably, we follow the official setup to train LMs on the MNLI dataset~\cite{williams-etal-2018-broad} and evaluate their zero-shot performance on EQUATE.
On \textbf{SVAMP}, the encoder-only model is not suitable since the answers are out-of-context.


\begin{table*}[t]
\centering
\scalebox{1.0}{
\begin{tabular}{lccccc}
\toprule
\textbf{Models} & \textbf{Number} & \textbf{Span} & \textbf{Spans} & \textbf{Date} & \textbf{Total}  \\
\toprule
\multicolumn{6}{c}{\textit{Previous Systems}} \\
MTMSN (BERT) & $81.1$ &  $82.8$ & $62.8$ & $69.0$  & $80.5$ \\
NumNet+ (RoBERTa) & $83.1$ &  $86.8^*$ & ${86.8}^*$ & $63.9$  & $84.4$ \\
QDGAT (RoBERTa) & ${86.2}$ &  $88.5^*$ & ${88.5}^*$ & $67.5$  & ${87.1}$ \\
GenBERT & $75.2$ & $74.5$ & $24.2$ & $56.4$ & $72.3$\\
PReasM &  $64.4$ & $86.6$ & $78.4$ & $77.7$ & $72.3$\\
\multicolumn{6}{c}{\textit{Original LMs}} \\
RoBERTa-Large & -- &  $86.4$ & $79.9$ & -- & --\\
BART-Large & $63.6$ & $79.6$ & $74.6$ & $62.1$ & $69.2$ \\ 
T5-11B & $83.2$ & ${90.2}$ & ${85.8}$ & ${84.9}$ & $85.8$\\
\multicolumn{6}{c}{\textit{\ours Models}} \\
\oursroberta & -- & $88.2$ & $83.1$ & -- & --\\
\oursbart & $78.9$ & $84.5$ & $79.6$ & $71.9$ & $80.6$\\
\ourstfive & ${85.2}$ & ${92.4}$ & ${86.6}$ & ${84.4}$ & ${87.6}$\\
\bottomrule
\end{tabular}
}
\caption{Breakdown of model \fone{} score by answer types on the dev set of DROP. Some works only report overall span type performance (marked by *), and single-span is non-separable from multi-span performance.
Bold and underlined numbers indicate the best and second-best results, respectively.
}
\label{tab:drop-by-type}    
\end{table*}

\subsection{Pre-training Details}

By default, we apply AdamW as pre-training optimizer with default scheduling parameters in fairseq.
The coefficient of weight decay is set as $0.05$ to alleviate over-fitting of pre-trained models.
Additionally, we employ fp16 to accelerate the pre-training.

\paragraph{\ours-Math} The pre-training procedure lasts for $10,000$ steps with a batch size of $512$. After the warm up in the first $2000$ steps, the learning rate arrives the peak at $3{\times}{10}^{-5}$ during pre-training.

\paragraph{\ours-Logic} The pre-training procedure lasts for $5,000$ steps with a batch size of $512$. After the warm up in the first $1000$ steps, the learning rate arrives the peak at $3{\times}{10}^{-5}$ during pre-training.

\paragraph{\ours-SQL} For \oursbart and \oursroberta, the pre-training procedure lasts for $50,000$ steps with a batch size of $512$. After the warm up in the first $5000$ steps, the learning rate arrives the peak at $3{\times}{10}^{-5}$ during pre-training. To save memory, each example in the pre-training corpus could at most contains $512$ tokens.
For \ourstfive, the pre-training procedure lasts for $20,000$ steps with a batch size of $512$. After the warm up in the first $2000$ steps, the learning rate arrives the peak at $1{\times}{10}^{-5}$ during pre-training. The maximum input length in each example is truncated to $384$ tokens to increase the batch size.

\begin{table*}[ht]
    \small
    \centering
    \begin{tabular}{lcccccc}
    \toprule
    \textbf{Models} &  \textbf{RTE-Q} &  \textbf{NewsNLI} &  \textbf{RedditNLI} &  \textbf{NR ST} &  \textbf{AWPNLI} &  \textbf{Average}  \\
    \midrule
    \multicolumn{7}{c}{\textit{Previous Systems}} \\
    MAJ & $57.8$ & $50.7$ & $58.4$ & $33.3$ & $50.0$ & $50.4$ \\
    BERT  & $57.2$ & $72.8$ & $49.6$ & $36.9$ & $42.2$ & $51.8$   \\
    GPT  & $68.1$ & $72.2$ & $52.4$ & $36.4$ & $50.0$ & $55.8$  \\
    Q-REAS & $56.6$ & $61.1$ & $50.8$ & $63.3$ & ${71.5}$ & $60.7$ \\
    \multicolumn{7}{c}{\textit{Original LMs}} \\
    BART-Large & $68.1$ & ${76.2}$ & $65.0$ & $53.7$ & $49.7$ & $62.6$   \\
    RoBERTa-Large & $69.3$ & ${75.5}$ & ${65.6}$ & $60.1$ & ${50.7}$ & $64.2$   \\
    \multicolumn{7}{c}{\textit{\ours Models}} \\
    \oursbart & ${72.3}$ & $75.2$ & $64.8$ & ${70.7}$ & $49.5$ & ${66.5}$   \\
    \oursroberta & ${75.3}$ & ${75.5}$ & ${68.1}$ & ${69.2}$ & $50.5$ & ${67.5}$   \\
    \bottomrule
    \end{tabular}
    \caption{The EM performance of different models on all subsets of the EQUATE benchmark. Bold and underlined numbers indicate the best and second-best results, respectively.}
    \label{tab:equate}   
\end{table*}

\begin{table*}[ht]
\centering
\small
\scalebox{1.0}{
\begin{tabular}{lcccc}
\toprule
&  \textbf{Table} &  \textbf{Text} &  \textbf{Table-Text} &  \textbf{Total} \\
\cmidrule{2-5} & EM\,\,/\,\,\fone{} & EM\,\,/\,\,\fone{} & EM\,\,/\,\,\fone{} & EM\,\,/\,\,\fone{}
\\
\midrule
Arithmetic & $50.1$\,\,/\,\,$50.1$ & $43.8$\,\,/\,\,$50.0$ & $55.6$\,\,/\,\,$55.6$ & $51.5$\,\,/\,\,$51.5$ \\
Counting & $66.7$\,\,/\,\,$66.7$ &  --\,\,\,/\,\,\,-- & $90.0$\,\,/\,\,$90.0$ & $81.3$\,\,/\,\,$81.3$\\
Spans & $67.4$\,\,/\,\,$80.6$ & $54.2$\,\,/\,\,$80.8$ & $79.2$\,\,/\,\,$84.8$ & $71.4$\,\,/\,\,$82.6$\\
Span &  $68.4$\,\,/\,\,$68.4$ & $51.2$\,\,/\,\,$76.0$ & $76.2$\,\,/\,\,$77.8$ & $61.9$\,\,/\,\,$74.6$\\
Total & $56.5\,\,/\,\,58.0$ & $51.1\,\,/\,\,75.0$ & $69.0\,\,/\,\,70.7$ & $59.1$\,\,/\,\,$65.9$ \\
\bottomrule
\end{tabular}}
\caption{The EM performance of \tagop (\oursroberta) with respect to answer types and sources on the dev set of TAT-QA.
}
\label{tab:tatqa}    
\end{table*}

\subsection{Fintuning Details}

We implement our models based on transformers~\cite{wolf-etal-2020-transformers}, fairseq~\cite{ott-etal-2019-fairseq} and DeepSpeed~\footnote{\url{http://github.com/microsoft/DeepSpeed}}.

\paragraph{Passage Retrieval in HotpotQA} Since the total length of the original passages in HotpotQA is too long to fit into memory, we train a classifier to filter out top-$3$ passages, as done in previous work~\citep{deng2021reasonbert}.
Specifically, a RoBERTa-Large model is fine-tuned to discriminate if an input passage is required to answer the question. 
The Hits@$3$ score of the classifier on HotpotQA is $97.2\%$.

\paragraph{Numerical Design in DROP and SVAMP}

As noticed by previous works, sub-word tokenization methods such as byte pair encoding \cite{DBLP:journals/corr/SennrichHB15} potentially undermines the arithmetic ability of models.
Instead, the character-level number representation is argued to be a more effective alleviation~\cite{wallace-etal-2019-nlp}.
Additionally, the reverse decoding of numbers is proposed as a better way of modelling arithmetic carry~\citep{geva-etal-2020-injecting}. 
Therefore, we employ these design strategies on DROP and SVAMP. 



\subsection{Fine-tuning Hyperpameters}

By default, we apply AdamW as fine-tuning optimizer with default scheduling parameters on all datasets. To ensure statistical significance, all fine-tuning procedures are run with three random seeds, except for T5-$11$B and \ourstfive due to the limit of computation budgets.

\paragraph{DROP}
\oursroberta and RoBERTa-Large are trained with the subset of questions marked as ``span'' from the DROP dataset.t
Since a gold answer may occur multiple times in the passage, we optimize over the sum of negative log probability for all possibly-correct \texttt{IO} sequences where each one of gold answers is included at least once, as done in \citet{segal-etal-2020-simple}.
The fine-tuning procedure runs up to $25,000$ steps with a batch size of $64$, with the learning rate of $7.5{\times}{10}^{-6}$.
As for BART-Large (and \oursbart, \oursbartmath, the same below) and T5-$11$B (and \ourstfive, the same below), they are trained with the whole DROP dataset.
For BART-Large, the fine-tuning procedure runs up to $20,000$ steps with a batch size as $128$ and a learning rate as $3{\times}{10}^{-5}$.
For T5-$11$B, due to the computational budget, the fine-tuning procedure only lasts for $10,000$ steps with a batch size of $32$, and the learning rate is $1{\times}{10}^{-5}$.


\paragraph{TAT-QA}

In the experiment of TAT-QA, we employ the official implementation and the default hyperparameters provided in \tagop~\footnote{https://github.com/NExTplusplus/TAT-QA}.
The fine-tuning procedure runs up to $50$ epochs with a batch size of $48$.
For modules introduced in \tagop, the learning rate is set as $5{\times}{10}^{-4}$, while for RoBERTa-Large (and \oursroberta), the learning rate is set as $1.5{\times}{10}^{-5}$.

\paragraph{HotpotQA}

The fine-tuning procedure runs up to $30,000$ steps with a batch size of $64$. The learning rate is $1{\times}{10}^{-5}$. Overlong inputs are truncated to $512$ tokens for both RoBERTa-Large (and \oursroberta), T5-$11$B (and \ourstfive) and BART-Large (and \oursbart). 

\paragraph{EQUATE}
The fine-tuning procedure runs up to $20,000$ steps on MNLI with a batch size of $128$ for both RoBERTa-Large (and \oursroberta) and BART-Large (and \oursbart), with learning rate is $1{\times}{10}^{-5}$. After fine-tuning, models are directly evaluated on EQUATE.


\paragraph{LogiQA}

In the experiment of LogiQA, we employ the open-source implementation and the default hyperparameters provided in ReClor~\footnote{https://github.com/yuweihao/reclor} \citep{yu2020reclor} to fine-tune RoBERTa-Large (and \oursroberta).
The fine-tuning procedure runs up to $10$ epochs with a batch size of $24$.
The learning rate is set as $1{\times}{10}^{-5}$.

\section{Fine-grained Analysis}\label{sec:fine_analysis}

\paragraph{DROP} In \tabref{drop-by-type} we report model \fone{} scores by question type on DROP. Comparing three \ours pre-trained models with their vanilla versions, we observe that: (i) \oursbart outperforms the vanilla BART-large with a wide margin in all types of questions, i.e. \textit{number} ($15.3\%$), \textit{date} ($9.8 \%$), \textit{span} (around $5\%$). (ii) \oursroberta only deals with span selection questions, and obtain $1.9\%$, $3.2\%$ gain on \textit{span, spans} questions, respectively.
(iii) For the giant \ourstfive, we also observe $2\%$ improvement on \textit{number} questions, $2.2\%$ on \textit{span} and $0.8\%$ on \textit{spans} questions. These model-agnostic performance boost on DROP reveals the extra numerical reasoning knowledge models learned from SQL program executors.

\paragraph{EQUATE}
\tabref{equate} presents performance break-down by subsets of EQUATE \cite{ravichander-etal-2019-equate}, where we compare \oursbart and \oursroberta with their vanilla versions and previous baselines. For both models, we observe around $10\%$ acc improvement on the \textit{NR ST} subset, where \textbf{numerical comparison and quantifiers} are especially emphasized. Stable performance improvement was also observed in both pre-trained models on  the \textit{RTE-Q} subset, where \textbf{arithmetics and ranges} are primary focus. Interestingly, \oursroberta alone demonstrate improvement on \textit{RedditNLI} (emphasizes approximation and verbal quantitative reasoning) subset. Performance on other subsets are approximately comparable between \ours pre-trained models and vanilla models, suggesting that \ours does not harm intrinsic abilities of language models.

\paragraph{TAT-QA}

\tabref{tatqa} shows the detailed experimental results of \tagop (\oursroberta). 
Considering that the pre-training of \oursroberta is only performed on table-like texts (i.e., the flatten sequence of databases), it is highly non-trivial for our model to generalize to such a hybrid scenario containing both tables and passages, again illustrating the transferability of reasoning capabilities.


\section{NL Understanding Performance}\label{sec:nl_understnad}

\begin{table}[t]
  \centering
  \small
  \scalebox{0.9}{
    \begin{tabular}{lcccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[4]{*}{\bf Dataset}} & \multicolumn{2}{c}{\bf Train} & \multicolumn{2}{c}{\bf Dev}
    \\
         \cmidrule(lr){2-3}
     \cmidrule(lr){4-5}
          & {\# Questions} & {\# Docs} & {\# Questions} & {\# Docs} \\
    \midrule
    SQuAD & $77,409$  & $5,565$  &  $9,536$ & $582$ \\
    MNLI & $392,702$  & $392,702$  & $9,815$ & $9,815$\\ 
    QuoRef & $19,399$ & $3,771$ & $2,418$  & $454$ \\
    \bottomrule
    \end{tabular}
    }
  \caption{\ours on NL understanding experiment dataset statistics.}
  \label{tab:mrc-stat}\end{table}

\paragraph{Dataset Details} We fine-tune \oursroberta on (i) SQuAD v1.0: \citep{rajpurkar-etal-2016-squad}: one of the most classical single-span selection RC benchmarks measuring understanding over natural language context; (ii) MNLI \citep{williams-etal-2018-broad}: a large-scale NLI dataset measuring cross-domain and cross-genre generalization of NLU. Notably, our model is evaluated on the \textit{matched} setting for the purpose of simplicity. (iii) QuoRef \cite{dasigi-etal-2019-quoref}: A Wikipedia-based multi-span selection RC benchmark with a special emphasis on coreference resolution. All dataset Statistics are shown in \tabref{mrc-stat}.

\paragraph{Implementation Details}
(i) On SQuAD, we cast the span selection task as a sequence tagging problem following~\citet{segal-etal-2020-simple}. 
(ii) On MNLI-matched, we train both models to perform sequence classification on concatenated premise-hypothesis pairs.
(iii) On {Quoref}, we cast the span(s) selection task as an \texttt{IO} sequence tagging problem following~\citet{segal-etal-2020-simple}. 




