\documentclass[11pt]{article}

\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}


\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{multicol}
\usepackage[all]{xy}
\usepackage{todonotes}

\setlength{\parindent}{0em}
\setlength{\parskip}{1ex plus .1ex minus .1ex}
\setlength{\itemsep}{0ex}

\usepackage{todonotes}
\newcommand{\kim}[1]{\todo[color=green]{#1}}
\newcommand{\lene}[1]{\todo[color=orange]{#1}}
\newcommand{\marie}[1]{\todo[color=yellow]{#1}}

\usepackage{calc,ifthen}
\newcounter{hours}
\newcounter{minutes}
\newcommand{\Printtime}{\setcounter{hours}{\time/60}\setcounter{minutes}{\time-\value{hours}*60}\thehours:\ifthenelse{\value{minutes}<10}{0}{}\theminutes}

\newtheorem{xdefinition}{Definition}
\newtheorem{xobservation}{Observation}
\newtheorem{xtheorem}{Theorem}
\newtheorem{xlemma}{Lemma}
\newtheorem{xproposition}{Proposition}
\newtheorem{xcorollary}{Corollary}
\newenvironment{definition}{\begin{xdefinition}\rm}{\hspace*{\fill}\raisebox{-1pt}{\boldmath}\end{xdefinition}}
\newenvironment{observation}{\begin{xobservation}\rm}{\hspace*{\fill}\raisebox{-1pt}{\boldmath}\end{xobservation}}
\newenvironment{theorem}{\begin{xtheorem}\rm}{\end{xtheorem}}
\newenvironment{lemma}{\begin{xlemma}\rm}{\end{xlemma}}
\newenvironment{proposition}{\begin{xproposition}\rm}{\end{xproposition}}
\newenvironment{corollary}{\begin{xcorollary}\rm}{\end{xcorollary}}
\newenvironment{proof}{\begin{trivlist}\item[]{\bf Proof }}{\hspace*{\fill}\raisebox{-1pt}{\boldmath}\end{trivlist}}

\newcommand{\A}{\ensuremath{\mathcal{A}}\xspace}
\newcommand{\Aalt}{\ensuremath{\mathbb{A}}\xspace}
\newcommand{\ALG}{\ensuremath{\operatorname{\textsc{A}}}\xspace}
\newcommand{\ALGB}{\ensuremath{\operatorname{\textsc{B}}}\xspace}
\newcommand{\OPT}{\ensuremath{\operatorname{\textsc{Opt}}}\xspace}
\newcommand{\DNF}{\ensuremath{\operatorname{\textsc{DNF}}}\xspace}
\newcommand{\nextfit}{\ensuremath{\operatorname{\textsc{Next-Fit}}}\xspace}
\newcommand{\dnextfit}{\ensuremath{\operatorname{\textsc{Dual Next-Fit}}}\xspace}
\newcommand{\harm}{{\ensuremath{\textsc{Harmonic}_k}}\xspace}
\newcommand{\dharm}{{\ensuremath{\textsc{Dual Harmonic}_k}}\xspace}
\newcommand{\Har}{{\ensuremath{\textsc{DHarmonic}_k}}\xspace}
\newcommand{\DHk}{{\ensuremath{\textsc{DH}_k}}\xspace}
\newcommand{\DHone}{{\ensuremath{\textsc{DH}_{1}}}\xspace}
\newcommand{\DHtwo}{{\ensuremath{\textsc{DH}_{2}}}\xspace}
\newcommand{\DHi}{{\ensuremath{\textsc{DH}_i}}\xspace}
\newcommand{\DHj}{{\ensuremath{\textsc{DH}_j}}\xspace}
\newcommand{\smallItems}{\ensuremath{\operatorname{small}}\xspace}
\newcommand{\bigItems}{\ensuremath{\operatorname{big}}\xspace}
\newcommand{\middleItems}{\ensuremath{\operatorname{middle}}\xspace}
\newcommand{\timer}{\ensuremath{\operatorname{times}}\xspace}

\newcommand{\SET}[1]{\left\{#1\right\}}
\newcommand{\SETOF}[2]{\{#1 \mid #2\}}
\newcommand{\WEHAVE}{\colon}
\newcommand{\SEQ}[1]{\langle #1 \rangle}
\newcommand{\BIGLANGLE}{\left\langle}
\newcommand{\BIGRANGLE}{\right\rangle}
\newcommand{\BIGSEQ}[1]{\BIGLANGLE #1 \BIGRANGLE}
\newcommand{\WR}{\ensuremath{\mathrm{WR}}\xspace} 
\newcommand{\CR}[1]{\ensuremath{\mathrm{CR}(#1)}\xspace}  
\newcommand{\CRu}{\ensuremath{\mathrm{CR}}\xspace} 
\newcommand{\CRab}[1]{\ensuremath{\mathrm{CR}_{a,b}(#1)}\xspace} 
\newcommand{\CRabu}{\ensuremath{\mathrm{CR}_{a,b}}\xspace} 
\newcommand{\RWOR}[2]{\ensuremath{\WR(#1,#2)}\xspace}
\newcommand{\PROB}[1]{\ensuremath{\operatorname{Prob}[#1]}}
\newcommand{\EXP}[1]{\ensuremath{\operatorname{E}[#1]}}
\newcommand{\EXPDIST}[2]{\ensuremath{\operatorname{E}_{#1}[#2]}}
\newcommand{\BIGEXPDIST}[2]{\ensuremath{\operatorname{E}_{#1}\left[#2\right]}}
\newcommand{\RO}[1]{\ensuremath{\mathrm{RR}(#1)}\xspace}
\newcommand{\MAX}[1]{\ensuremath{\mathrm{MR}(#1)}}
\newcommand{\MINV}[1]{\ensuremath{\mathrm{MR_{vol}}(#1)}}
\newcommand{\vol}[1]{\ensuremath{\textit{vol}(#1)}\xspace}
\newcommand{\BIGSET}[1]{\left\{#1\right\}}
\newcommand{\CEIL}[1]{\left\lceil#1\right\rceil}
\newcommand{\FLOOR}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\MIN}[1]{\min\left\{#1\right\}}
\newcommand{\MINinline}[1]{\min\{#1\}}
\newcommand{\SIZE}[1]{|#1|}
\newcommand{\PERM}[1]{\ensuremath{\operatorname{Perm}(#1)}}

\newcommand{\RDNF}{\ensuremath{R_{(0,\frac{1}{k})}}\xspace} 
\newcommand{\RHar}{\ensuremath{R_{[\frac{1}{k},1)}}\xspace}
\newcommand{\RDNFtwo}{\ensuremath{R_{(0,\frac{1}{2})}}\xspace} 
\newcommand{\RHartwo}{\ensuremath{R_{[\frac{1}{2},1)}}\xspace}
\newcommand{\gf}{\ensuremath{\psi_1}\xspace}
\newcommand{\ERU}[1]{\ensuremath{\mathrm{ER_U}(#1)}\xspace} 

\newcommand{\p}{p}
\newcommand{\bin}{\ensuremath{\beta}}
\newcommand{\eps}{\ensuremath{\varepsilon}\xspace}

\title{Online Bin Covering: Expectations vs.\ Guarantees\,\thanks{A
preliminary version of this paper appeared in the
proceedings of the Seventh Annual International Conference on
Combinatorial Optimization and Applications, 2013.
Supported in part by the Danish Council for Independent Research
and the Villum Foundation.}}

\author{Marie G. Christ \hspace{2em} Lene M. Favrholdt \hspace{2em} Kim S. Larsen \1ex]
        {\tt \{christm,lenem,kslarsen\}@imada.sdu.dk}}

\date{February 27, 2014}

\begin{document}

\maketitle

\begin{abstract}
Bin covering is a dual version of classic bin packing.
Thus, the goal is to cover as many bins as possible, where covering a bin
means packing items of total size at least one in the bin.


For online bin covering, competitive analysis fails to distinguish between
most algorithms of interest; 
all ``reasonable'' algorithms have a competitive ratio of .
Thus,
 in order to get a better understanding of the combinatorial difficulties in solving this problem,
 we turn to other
performance measures, namely relative worst order, random order, and
max/max analysis, as well as
analyzing input with restricted or uniformly distributed item sizes.
In this way, our study also supplements the ongoing systematic studies of
the relative strengths of various performance measures.

Two classic algorithms for online bin packing that have natural dual
 versions are \harm and \nextfit. 
Even though the algorithms are quite different in nature, 
the dual versions are
 not separated by competitive analysis.
We make the case that when guarantees are needed, even under restricted
input sequences, dual \harm is preferable. In addition, we
establish quite robust theoretical results showing that if items come
from a uniform distribution or even if just the ordering of items is uniformly
random, then dual \nextfit is the right choice.
\end{abstract}

\section{Introduction}
Bin covering~\cite{AJKL84j} is a dual version of classic bin packing.
As usual, bins have
size one and items with sizes between zero and one must be packed. However,
in bin covering, the objective is to cover as many bins as possible,
where a bin is covered if the sizes of items placed in the bin sum up
to at least one.
We are considering the online version of bin covering. A problem
is online if the input sequence is presented to the algorithm one item
at a time, and the algorithm must make an irrevocable decision regarding
the current item without knowledge of future items.

Bin covering algorithms have numerous important applications.
For instance, when packing or canning food items guaranteeing a
minimum weight or volume, reductions in the overpacking of even a
few percent may have a large economic impact.
If items arrive on a conveyor belt, for instance, the problem becomes online.

Classic algorithms for online bin packing are \nextfit and the
parameterized family \harm~\cite{HL85}.
\nextfit is a very simple and natural algorithm, and \harm was designed to
obtain a competitive ratio~\cite{ST85,KMRS88} better than any Any-Fit
algorithm (First-Fit and Best-Fit are examples of Any-Fit
algorithms for bin packing, and the competitive ratio of Next-Fit is
worse than both these algorithms).
\harm and variations of it have been analyzed extensively~\cite{RBLL89,W93,S02}.
We consider the obvious dual version of these,
\DNF~\cite{AJKL84j} and \DHk~\cite{CW98}.
These algorithms are quite different in nature
and the bin packing versions are clearly separated, 
having competitive ratios of  and approximately , respectively.
However, for bin covering,
competitive analysis does not distinguish between them!
In fact, for bin covering, competitive analysis categorizes both
algorithms as being optimal among deterministic algorithms, but also worst
possible among ``reasonable'' algorithms for the problem.
This is unlike the situation in bin packing, and in general, results
from bin packing do not transfer directly to bin covering.

To understand the algorithmic differences better,
it is therefore necessary to employ different techniques,
and we turn to other generally applicable performance measures, namely
relative worst order analysis, random order analysis, and
max/max analysis.
As for almost all performance measures, the idea is to abstract away
some details of the problem to enable comparisons. Without some abstraction,
it is hard to ever, analytically, claim that one algorithm
is better than another, since almost any algorithm performs better
than any other algorithm on at least one input sequence.
For all the measures considered here, the abstraction can be viewed
as being defined via first a partitioning of the set of input sequences of a given length and then
an aggregation of the results from each partition.
For each sequence length, competitive analysis, for instance,
considers all the ratios of the online performance to the optimal
offline performance obtained for each sequence of that length,
and then takes the worst ratio of all of these.
The measures above employ a less fine-grained partitioning of the
input space.
Worst order and random order analysis group permutations of the same
sequence together instead of considering each sequence separately,
deriving worst-case or average-case performance, respectively, within each partition.
With max/max analysis the partitioning of the input space is even
coarser: for each sequence length , the online worst-case behavior
over all sequences of length  is compared to the worst-case optimal
offline behavior over all sequences of length .
There is no one correct way to compare algorithms,
but since these measures focus on different aspects of algorithmic behavior,
considering all of the ones above lead to a very broad analysis of the problem.
Extensive motivational sections can be found in the papers introducing
these measures and in the survey~\cite{DL05}.
As a further supplement, we analyze restricted input sequences, where
items have similar size, which is likely to happen in practice if one
is packing products with an origin in nature, for instance.
Finally, we consider input sequences containing items having uniformly distributed sizes.

Relative worst order analysis~\cite{BF07j,BFL07j} has been applied to
many problems; a recent list can be found
in~\cite{EKL13j}.
In~\cite{EFK12}, bin covering was analyzed, but using
a version of the problem allowing items of size~1.
We analyze the more commonly studied version for bin covering,
where all items are
strictly smaller than 1. Since worst-case sequences from~\cite{EFK12}
contain items of size~1, this leads to slightly different results.
For completeness, we include these results.
Random order analysis~\cite{K96}
was introduced for classic bin packing, but has also been used
for other problems; a server problem, for instance~\cite{BIL09p}.
Max/max analysis~\cite{BB94} was introduced
as an early step towards refining the results from competitive
analysis for paging and a server problem.



Relative worst order analysis emphasizes the fact that there
exist multisets of input items where \DNF can perform
 times as poorly as \DHk. On the other hand,
\DHk's method of limiting the worst-case also means that it has less
of an opportunity to reach the best case, as opposed to \DNF.
This is reflected in the random order analysis, where \DNF
comes out at least as well as \DHk.
Another way of approaching randomness is to analyze a uniform distribution.
We establish new results on \DHk showing that its performance here
is slightly worse than that of \DNF, in line with the random order results.
With the max/max analysis, a distinction between the two algorithms 
can only be achieved, when the item sizes are limited, and
\DHk is the algorithm selected as best by this measure.
With respect to competitive analysis,
we also consider restricted input in the sense that item sizes may
only vary across one or two consecutive \DHk partitioning points. This is a
formal way of treating the case where items are of similar size,
while allowing greater variation when this size is large. We show
that with this restricted form of input, considering the
worst-case measures of competitive analysis, \DHk is deemed
better than \DNF, as \DNF is more vulnerable to
worst-case sequences. 

This study also contributes to the ongoing systematic studies of
the relative strengths of various performance measures,
initiated in~\cite{BIL09p}. Up until that paper, most performance
measures were introduced for a specific problem to 
overcome the limitations of
competitive analysis. In~\cite{BIL09p}, comparisons
of performance measures different from competitive analysis
were initiated, and this line of work has been continued
in~\cite{BGL12p,BLM12p,BGL13}, among others.
Our results supplement results in~\cite{C88},
showing that
no deterministic algorithm for the bin covering problem
can be better than -competitive and
giving an asymptotically optimal algorithm for the case of items
being uniformly distributed on .
For \DNF, \cite{CFGK91} established an expected competitive
ratio of  under the same conditions.

In the following, we formally define the bin covering problem and the
algorithms \DNF and \DHk, the performance of which we compare under
different performance measures. The performance measures themselves are
defined in each their section.
We conclude on our findings in the final section.

\subsection*{Bin Covering}
In the one dimensional bin covering problem, the algorithm gets an input
sequence  of item sizes, where for all , .
The items are to be packed in bins of size 1.
A bin is {\em covered}, if items of total size at least 1 have been
 packed in it, and
the goal is to cover as many bins as possible.

Requiring items to be strictly smaller than 1 corresponds to assuming
 that items of size 1 are treated separately.
This makes sense, since there is no advantage in combining an item of
 size 1 with any other items in a bin.
In other words, any algorithm not giving special treatment to items
of size 1 could trivially be improved by doing so.

For a bin covering algorithm \ALG, we let  denote the number
 of covered bins when given the sequence  of items. 
We let  denote an optimal offline algorithm.
Thus,  is the largest number of bins that can be covered
by any algorithm processing .

In algorithms for bin packing and covering, it is standard to use the
following terminology.
A bin that has received at least one item is \emph{open} if it may
receive more items, and \emph{closed}
if the algorithm will not consider
that bin again for future items.

\subsubsection*{The Dual Next-Fit algorithm}
Assmann, Johnson, Kleitman, and Leung~\cite{AJKL84j} introduced the
Dual \nextfit algorithm 
(\DNF), an adaptation of the \nextfit algorithm for bin packing.
\DNF always keeps at most one open bin. 
When a new item arrives, it is packed in the currently open bin, if
 any.
Otherwise, a new bin is opened.
A bin is closed when it has received items of total size at least one.

\subsubsection*{The Dual Harmonic algorithm}
The algorithm \harm was introduced for bin packing by Lee and Lee~\cite{HL85}. This algorithm
partitions the interval  into  subintervals,
with the partitioning points at ,
resulting in the different sized intervals
.
\harm packs items from each of
these  subintervals in separate bins.
This means that each closed bin for the interval~ 
contains exactly  items.
The natural adaptation to the bin covering problem is to use the intervals

The resulting algorithm, \Har (\DHk), uses exactly  items from the
interval~ to cover a bin.
All through the paper we assume that , since for ,
\DHk becomes \DNF.

\section{Competitive Analysis}
\label{competitive-analysis}
In competitive analysis~\cite{ST85,KMRS88}, the performance of an online
algorithm is compared to that of an optimal offline algorithm \OPT. 
An algorithm \ALG for a maximization problem is called
\emph{-competitive} if there exists a fixed 
constant~ such that for any input sequence ,
it holds that .
The supremum over all such  is the \emph{competitive ratio} \CR{\ALG} of \ALG.
Note that some authors reverse the order of the algorithm and \OPT to get
ratios larger than one.

For bin covering, Csirik and Totik~\cite{C88} showed that no deterministic
online algorithm can be better than -competitive.
\DNF was shown to be -competitive in~\cite{AJKL84j},
and the same result for \DHk was noted in~\cite{EFK12}.
For completeness, to show that this result is tight for a
large class of algorithms,
we define a \emph{reasonable} algorithm to be one that closes bins as
soon as they are covered, does not close bins before
they are covered, and does not have more than a constant number of open
bins at any point.
\begin{theorem}
\label{competitive-reasonable}
Any deterministic reasonable algorithm has a competitive ratio of
.
\end{theorem}
\begin{proof}
The upper bound follows from~\cite{C88}.
For the lower bound, note that
the only item that can overfill a bin is the last item to go into that
bin, by the definition of a reasonable algorithm. Since that item has
size less than one, all bins will contain items of total size
less than two. Thus, \OPT could not cover more than twice as many bins,
using items from the closed bins.
Being reasonable also means that there are only a constant number of
open bins, so the items in there can only enable \OPT to cover
an additive constant of further bins. Thus,
no reasonable algorithm can be worse than -competitive.
\end{proof}

\subsection{Limiting the item sizes}
In some applications of the bin covering problem it is likely that
the sizes of the items contained in an input sequence differ only
slightly, e.g., packing similar food items into a container, guaranteeing
the consumer a minimum weight.
In the following, we investigate the performance of \DNF and
\DHk on sequences with similar-sized items.
Since it seems reasonable to allow larger variance in size when the
considered sizes are large,
we consider sequences containing item sizes from two or three
consecutive \DHk intervals. 

We first consider intervals  that contain exactly one \DHk partitioning point.
Afterwards, we consider sequences with exactly two \DHk partitioning points.
We emphasize that there are no restrictions on the endpoints  and ,
which can be any real numbers,
as long as the interval between them contains exactly one or two \DHk partitioning points.
In both cases, \DHk turns out to have the better ratio.

\begin{proposition}
\label{proposition-basic-dnf}
For any , , and ,
, even if we
only consider items in the range
.
\end{proposition}
\begin{proof}
Consider the sequence 
.
For this sequence, \DNF covers only  bins, whereas \OPT can
place exactly one small and one large item in each bin, filling up with
items of size , to cover  bins.
\end{proof}

For any , we let
\CRabu denote the competitive ratio on
sequences where all item sizes are in .

If  does not contain at least one of the interval borders
used by \DHk, then \DHk packs exactly like \DNF.
If  contains a \DHk border, then we define 
 
and refer to  as the {\em maximal border in }.

Note that if  contains exactly one of the interval borders
used by \DHk, then .
The next two theorems and the corollary deal with this case.

\begin{theorem}\label{lemma-DNF-2int}
If , then 

\end{theorem}
\begin{proof}
The lower bound follows directly from the fact that it takes at 
least~ and at most  items to cover a bin,
and the upper bound follows from Proposition~\ref{proposition-basic-dnf}.
\end{proof}

\begin{theorem}\label{lemma-Har-2int}
If  and , then 

\end{theorem}
\begin{proof}
We consider the lower bound first.
Since , \DHk packs the items of size larger than or equal
 to  in separate bins.
For any sequence , let  denote the total number of items in  and
let  denote the number of items of size larger than or equal to
.
Then, \DHk covers at least

bins. 
Thus, letting , we obtain 


We treat this in two cases:

\textbf{Case :} At least  bins covered by \OPT contain more than ~items.
Hence, ,
and


\textbf{Case :}
Here we can only use , obtaining


For the upper bound,
we consider the sequence ,
where ,
ensuring that both item sizes belong to .
\OPT covers  bins, whereas
\DHk packs the different sized items in separate bins,
and covers  bins,
up to an additive constant independent of  which is due to rounding.
\end{proof}

It follows that if  contains exactly one \DHk partitioning
point, , and , then \DHk has a better competitive ratio than \DNF:

\begin{corollary}
\label{corollary-2int}
 If  and , then 
\end{corollary}
\begin{proof}
 The result follows from Theorems~\ref{lemma-DNF-2int} and~\ref{lemma-Har-2int},
since .
\end{proof}

We now consider intervals  that contain exactly two \DHk partitioning points,
  and .
Including the extra partitioning point, , results in a
 lower competitive ratio for \DNF, with an upper bound depending on whether
  is smaller or larger than .
The competitive ratio of \DHk becomes lower than with just one
partitioning point, only if .




\begin{theorem}\label{lemma-DNF-3int-small}
If , then
2ex]
  \displaystyle
  \frac{\p^2+\p}{\p^2+2\p+2}, & 
  \displaystyle
  \mbox{otherwise}
\end{array}
\right.
a < \frac{1}{\p+1} < \frac{1}{\p} < \frac{\p+2}{\p(\p+1)} < b < \frac{1}{\p-1},\DNF(I) = (\p+1)(\p-2)n+(\p+1)n+((\p+1)n-1)+1=\p(\p+1)n\,.\OPT(I) = (\p+1)(\p-2)n+2(\p+1)n+(\p+1)n+n=(\p^2+2\p+2)n\,.
\CRab{\DHk} = 
\left\{
\begin{array}{ll}
\displaystyle 
\frac{\p^3+2\p^2+\p+2}{\p(\p+1)(\p+2)} = \frac{\p^2+1}{\p(\p+1)}, &
               \displaystyle \mbox{if } \
\end{theorem}
\begin{proof}
 We prove the lower bound first.

Items of size less than  are called {\em
  small}, items of size 
at least  are called {\em large}, and the remaining items
are called {\em medium}.
Let , , and  denote the number of small, medium, and large
 items, respectively.

Consider an optimal packing.
For ,
let  denote the number of bins with exactly  items.
Then,
 is the number of bins covered by \OPT.
Since \DHk covers exactly  bins,
independent of the order of the items,  we can
consider items from the three types of bins separately.

{\bf Bins with  items:}
Let , , and  denote the number of small, medium, and
 large items, respectively, packed in these  bins by \OPT.
Further, let  denote the total
 number of items packed here.

In each of these bins, the items have an average size of at least
. This means that , since each bin has to contain at least one item of size at least . 


We now prove the inequality .
We do this by proving the stronger result that for each bin  with
exactly  items, , where  and 
denote the number of small and large items in , respectively.

Small items deviate from the average size with strictly more than

and large items deviate with at most

Thus, having an average item size of at least  within a
  bin  requires .
Assume that  contains more small items than large items, i.e.,
  .
Then, ,
which is equivalent to
,
implying that ,
using the equation above.
Since  is an integer, this means that , and
since  contains exactly  items, this proves that .


The contribution to the number of bins covered by \DHk from the 
items considered here is more than , where the  comes from
a possible fractional part in the three addends below.


 
If , 
 one large item is not large enough to compensate for the loss of contribution
to the average that a small item generates (recall that this loss is
strictly larger than ). Therefore,
additional to the  large 
items, there has to be at least one more large item for each small
item, i.e., .
In this case, we can strengthen the calculations above from a certain point:


{\bf Bins with  items:}
Let , , and  denote the number of small, medium, and
 large items, respectively, packed in these  bins.
Further, let  denote the total
 number of items packed here.
 
 In each of these bins, the items have an average size of at least
.
This means that , as each bin has to contain at least one item of size at least . 

The contribution to the number of bins covered by \DHk from the 
items considered here is more than , where


{\bf Bins with  items:}
Since \DHk cannot be forced to pack more than  items in each bin,
the contribution to the number of bins covered by \DHk from the
items considered here is exactly .
 

Now, we turn to the upper bound.
Assume first that .
Consider the sequence

for some ,
sufficiently small such that all the items in the sequence are in
the range .
Since ,
\OPT can cover  bins by combining one
item of size , one item of size 
, and  items of size
. 
\DHk packs each kind of item
separately, covering  bins. 
 
If ,
 we do not need small items to get this weaker upper bound.
 It is sufficient to consider the two larger intervals and use Theorem~\ref{lemma-Har-2int}, 
 since .
\end{proof}


It follows that if  contains exactly two \DHk partitioning
points, then \DHk has a better competitive ratio than \DNF:

\begin{corollary}
\label{cor:abcomp}
 If , then .
\end{corollary}
\begin{proof}
The result follows from Theorems~\ref{lemma-DNF-3int-small} and~\ref{lemma-Har-3int}, 
since if , then
2ex]
& > & \displaystyle\frac{\p+1}{\p+2} \geq \CRab{\DNF}
\end{array}

\begin{array}{rcl}
\CRab{\DHk} & = & \displaystyle \frac{\p^3+2\p^2+2}{\p(\p+1)(\p+2)} \2ex]
& > &  \displaystyle\frac{\p(\p+1)}{\p^2+2\p+2} \geq  \CRab{\DNF}
\end{array}
\RWOR{\ALG}{\ALGB} = 
 \sup \SETOF{c}{\exists b \, \forall I \colon \ALG_W(I) \geq c \ALGB_W(I) - b}\DHk_W(I) \geq \DNF_W(I)-(k-1){\DHk}_W(I_n)\geq\frac{3}{2}\cdot {\DNF}_W(I_n)-1\,.
 \RO{\ALG} = \liminf\limits_{\OPT(I)\rightarrow \infty} \frac{E_{\sigma}[\ALG(\sigma(I))]}{\OPT(I)}
S_i^n=\SETOF{I\in S^n}{\mbox{ contains  items of size  and  items of size }}\,.
        \frac{\EXPDIST{I \in S^n}{\DNF(I)}}{\EXPDIST{I \in S^n}{\OPT(I)}}
 & \geq \min_{0 \leq i \leq n} 
        \frac{\EXPDIST{I \in S^n_i}{\DNF(I)}}{\OPT(I^n_i)}, 
        \text{ where } I^n_i \in S^n_i\\
 & =    \min_{I \in S^n}
        \frac{\EXPDIST{\sigma}{\DNF(\sigma(I))}}{\OPT(I)} \geq \min_{I \in R^n} 
        \frac{\EXPDIST{\sigma}{\DNF(\sigma(I))}}{\OPT(I)}
\lim_{n\rightarrow\infty}\frac{\EXPDIST{I\in S^n}{\DNF(I)}}{\EXPDIST{I\in S^n}{\OPT(I)}}
  \geq
  \liminf_{\OPT(I)\rightarrow\infty}\frac{\EXPDIST{\sigma}{\DNF(\sigma(I))}}{\OPT(I)}
  =
  \RO{\DNF}.
 \xymatrix{ & *+[o][F-]{N} \ar@/^/[dl]^{\frac{1}{2}} \ar@/_/[dr]_{\frac{1}{2}}& \\ *+[o][F-]{L} \ar@/^/[ur]^{1} & & *+[o][F-]{S} \ar@/_/[ul]_{\frac{1}{2}} \ar@(dl,dr)_{\frac{1}{2}}}

  1 & = \PROB{N} + \PROB{L} + \PROB{S} \\
 \PROB{N} &= \PROB{L} + \frac{\PROB{S}}{2} \\
 \PROB{L} &= \frac{\PROB{N}}{2} \\
 \PROB{S} &= \frac{\PROB{N}}{2} + \frac{\PROB{S}}{2}

 \lim\limits_{n \rightarrow \infty}
    \frac{\EXPDIST{I\in S^n}{\DNF(I)}}{\EXPDIST{I\in S^n}{\OPT(I)}}
  = \lim\limits_{n \rightarrow \infty}\frac{\frac{2}{5}n}{\frac{n}{2}-O(\sqrt{n})}
  = \frac{4}{5}.

\MINV{\ALG} = \dfrac{\liminf_{v\rightarrow\infty}\min_{\vol{I} = v} \ALG(I)/v}{\liminf_{v\rightarrow\infty}\min_{\vol{I} = v} \OPT(I)/v}
\liminf\limits_{n\rightarrow\infty}\min\limits_{\vol{I} = n} \frac{\DNF(I_n)}{n}
=\frac{1}{1-(\p+1)\varepsilon+b}\,,\liminf\limits_{n\rightarrow\infty}\min\limits_{\vol{I} = n} \frac{\OPT(I_n)}{n} = \min\SET{\frac{1}{(\p+1)\frac{1}{\p}},\frac{1}{\p b}}
=\min\SET{\frac{1}{1+\frac{1}{\p}},\frac{1}{\p b}}\,.\ERU{\ALG} = \lim_{n \rightarrow \infty}
  \frac{\EXPDIST{I\in U_n(0,1)}{\ALG(I)}}{\EXPDIST{I\in U_n(0,1)}{\OPT(I)}}.
 & \ERU{\DHtwo} = \frac{1}{2}+\frac{1}{e^2-e}\approx 0.7141 \text{ and }\\
 & \lim_{k \rightarrow \infty} \ERU{\DHk} = \frac{12-\pi^2}{3} \approx 0.7101\,.
 
 \ERU{\DHk}
& = \lim_{n \rightarrow \infty}
  \frac{\EXPDIST{I\in U_n(0,1)}{\DHk(I)}}{\EXPDIST{I\in U_n(0,1)}{\OPT(I)}} \\
& = \lim_{n \rightarrow \infty}
  \frac{\EXPDIST{I\in U_{\frac{(k-1)n}{k}}[\frac{1}{k},1)}{\DHk(I)}
        +\EXPDIST{I\in U_{\frac{n}{k}}(0,\frac{1}{k})}{\DHk(I)}}
       {\EXPDIST{I\in U_n(0,1)}{\OPT(I)}} \\
& = \RHar + \RDNF \,,
 \RHar = \lim_{n \rightarrow \infty}
  \frac{\EXPDIST{I\in U_{\frac{(k-1)n}{k}}[\frac{1}{k},1)}{\DHk(I)}}
       {\EXPDIST{I\in U_n(0,1)}{\OPT(I)}}   \RDNF = \lim_{n \rightarrow \infty}
  \frac{\EXPDIST{I\in U_{\frac{n}{k}}(0,\frac{1}{k})}{\DHk(I)}}
       {\EXPDIST{I\in U_n(0,1)}{\OPT(I)}}\,.
   \RHar
 & = \lim_{n \rightarrow \infty} 
     \frac{\sum\limits_{i=2}^k \left(\frac{n}{i^2(i-1)}-1\right)}
          {n/2}
   = 2 \sum\limits_{i=2}^k \frac{1}{i^2(i-1)}\,.
 
   \RHar
 & = 2 \sum\limits_{i=2}^k  \left(\frac{1}{i-1} - \frac{1}{i} -
     \frac{1}{i^2}\right)\\ 
 & = 2 \left( \sum\limits_{i=1}^{k-1}\frac{1}{i} -  
     \sum\limits_{i=2}^k \frac{1}{i} -
     \sum\limits_{i=1}^k \frac{1}{i^2} + 1 \right) \\
 & = 2 \left( 1 - \frac{1}{k} - 
     \sum\limits_{i=0}^{k-1}\frac{1}{(i+1)^2} + 1 \right) \\
 & = 2 \left( 2 - \frac{1}{k} - 
     \sum\limits_{i=0}^\infty \frac{1}{(i+1)^2} +
     \sum\limits_{i=0}^\infty \frac{1}{(i+1+k)^2} \right)\\
 & = 2 \left( 2 - \frac{1}{k} - \gf(1) + \gf(k+1) \right)\,,
 
   \RHar
 & = 2\left(2 - \frac{1}{k} - \frac{\pi^2}{6} + \gf(k)-\frac{1}{k^2}\right)\\
 & = 2\left(\frac{12-\pi^2}{6} - \frac{1+k}{k^2} + \gf(k)\right).
 
     \lim_{k \rightarrow \infty} \ERU{\DHk}
 & = \frac{12-\pi^2}{3} \approx 0.7101\,.
 
\lim\limits_{n\rightarrow\infty}
  \frac{\EXPDIST{I\in U_n[0,\frac{1}{k})}{\DNF(I)}}{n}
=
\frac{1}{\mu(k)}
\mu(k) 
 = \lim\limits_{\varepsilon \rightarrow 0}
   \sum\limits_{l=0}^k (-1)^l \frac{1}{l!}
      \left( \frac{k}{1-\varepsilon} - l \right)^l
      e^{\frac{k}{1-\varepsilon} -l}
 = \sum\limits_{l=1}^{k}  \frac{e^l (-l)^{k-l}}{(k-l)!}\,,
   \RDNF 
 & = \frac{1}{k}
     \lim\limits_{n\rightarrow\infty}
     \frac{\EXPDIST{I\in U_n[0,\frac{1}{k})}{\DNF(I)}}{n/2}
   = \frac{2}{\mu(k) k}\,.
 
   \ERU{\DHtwo} 
 & = \RHartwo + \RDNFtwo \\
 & = 2 \left( \frac{12-\pi^2}{6} - \frac{1+2}{2^2} + \gf(2) \right) + 
     \frac{2}{2\mu(2)}\\
 & = \frac{12-\pi^2}{3} - \frac{3}{2} + \frac{\pi^2}{3} - 2 + 
     \frac{1}{e^2-e} \\
 & = \frac{1}{2} + \frac{1}{e^2-e} \approx 0.7141.
 
\end{proof}

This should be compared with a result from~\cite{CFGK91}, showing
that on a uniform distribution,
\DNF has an expected performance ratio of .
Thus, under this assumption, \DNF is a little better than \DHk.



\section{Concluding Remarks}
The starting point for this paper was the fact that bin covering
algorithms as different as \DNF and \DHk are not separated using
competitive analysis.
We are interested in the question of which algorithm to use in different
scenarios. \DHk was designed to guard against worst-case sequences,
and since these are often made up using pathological input, such as mixing very
large and very small items, we have carried out analyses using the
worst-case performance, but on restricted input of items of similar size.
The comparison is still in \DHk's favor, albeit less so.
Max/max analysis (under similar conditions) and relative worst order
analysis also point to \DHk.

In contrast, if input is not organized into worst-case sequences by
an adversary, we can show, by carrying out an analysis of the expected
results under a uniform distribution that \DNF performs a little
better than \DHk. This seems to be very robust, since adding a small
element of worst-case requirements in the form of random order analysis
also points to \DNF not being worse than \DHk. Thus, even if an adversary
gets to choose the worst sequence for the algorithm, just the fact that
the items are received in a random order removes
\DHk's advantage over \DNF.

The conclusion is that unless guarantees are desired or it is known that
items do not arrive in a random order, it is worth considering \DNF
as the algorithm of choice.

\DHk has a random order ratio of ,
which is worst possible, whereas the upper bound we have on
\DNF is . We conjecture that these two algorithms
can be separated, proving \DNF to be best.
Though it is not essential to the conclusion above,
we leave this as an interesting open problem we would like to see solved,
and use the rest of this section to discuss some relevant issues
regarding this.
It seems intuitively almost obvious that
\DNF would always get a ratio larger than .
The difficulty in establishing this formally stems from problems of
handling the size aspects using probability theory. In the hardest
case, there are a linear number of very large items such that if
they end up on top of each other pairwise, we get the ratio of . 
Thus, we need to prove that some fraction of these large items do not end up pairwise
on top of each other.
The small items that would be packed with the large items in an
optimal packing can be cut into very small pieces so there are
orders of magnitude more small items than large items---but still of
possibly dramatically varying size, relatively.
Whereas we have strong theoretical tools for
bounding the deviation from the expected
number of items in certain locations in the form of
Chebyshev's inequality, for instance, it is much harder to reason
regarding deviations from the expected size, and it is exactly the sum
of sizes of small items surrounding a large item that decides whether or not
two large items end up on top of each other.

Results on the random order ratio are often difficult to establish.
This is reflected in the rather small
number of obtained results and also in published
results being far from tight. In the paper~\cite{K96}
introducing the random order ratio, for example,
the random order ratio of the bin
packing algorithm Best-Fit is shown to lie between 1.08 and 1.5.
An exceptionally tight result appears in~\cite{CCRZ08}, where it is shown
that the random order ratio of Next-Fit for bin packing is exactly~.
Note, however, that this result does not give indication that the
random order ratio of \DNF for bin covering should be
. The sequence establishing the lower bound of 2 consists of
 items of size~ and  items of size~, for some large . For a random ordering of these
items, each item of size  has a high probability of
being combined with at least one of the small items, leaving too
little space in the bin for another large item. 
For bin covering, the problem is reversed;
to prove an upper bound of ,
we must prove that each large item has a
significant probability of being surrounded by a sufficiently small
volume of small items so that it will go into the same bin as a
neighboring large item.


\bibliography{refs}
\bibliographystyle{plain}

\end{document}
