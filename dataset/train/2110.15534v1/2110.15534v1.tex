\section{Results}
\label{sec:results}








\begin{table*}
    \centering
    \resizebox{\textwidth}{!}{\begin{tabular}{cccccccc}
        \toprule
            \multirow{3}{*}{Transition System} & \multicolumn{3}{c}{Features}  & \multicolumn{2}{c}{Model Results on AMR 2.0} & \multicolumn{2}{c}{Model Results on AMR 3.0} \\
            \cmidrule(lr){2-4}
            \cmidrule(lr){5-6}
            \cmidrule(lr){7-8}
            & \multirow{2}{*}{\begin{tabular}{@{}c@{}} \#Base \\ Actions \end{tabular}} & \multirow{2}{*}{\begin{tabular}{@{}c@{}} Distant \\ Edges \end{tabular}} & \multirow{2}{*}{\begin{tabular}{@{}c@{}} Special \\ Subgraph \end{tabular}} &  \multirow{2}{*}{\begin{tabular}{@{}c@{}} APT \\ \citep{zhou2021amr} \end{tabular}  } & \multirow{2}{*}{\begin{tabular}{@{}c@{}} StructBART \\ sep-voc \end{tabular} } &
            \multirow{2}{*}{\begin{tabular}{@{}c@{}} APT \\ \citep{zhou2021amr} \end{tabular}  } & \multirow{2}{*}{\begin{tabular}{@{}c@{}} StructBART \\ sep-voc \end{tabular} } \\
            & & & & & & & \\
        \midrule
            \citet{astudillo2020transition} & 12 & \textsc{Swap} & merge  & - & - & - & -\\
            \citet{zhou2021amr} & 10 & pointer & merge  & 81.5 \small{0.1} & 83.4 \small{0.1} & 79.8 \small{0.1} & 81.6 \small{0.0} \\
            Ours & 6 & pointer & no & 81.6 \small{0.1} & 84.0 \small{0.1} & 79.6 \small{0.0} & 82.3 \small{0.1} \\
        \bottomrule
    \end{tabular}
    } \caption{Transition system comparison, including their effects on different parsing models.
     we adopt the cited model without graph structure embedding to compare and run on our proposed oracle.}
    \label{tab:oracle_compare_wide_full}
\end{table*}





\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{ccc}
    \toprule
        Transition System & Avg. \#actions & Oracle  \\
    \midrule
        \citet{astudillo2020transition} & 76.2 & 98.0 \\
        \citet{zhou2021amr} & 41.6 & 98.9 \\
        Ours & 45.6 & 99.9 \\
    \bottomrule
    \end{tabular}
    } \caption{Average action sequence length and oracle coverage on AMR 2.0 training data from different transition systems.
    Average source sentence length is 18.9.
    }
    \label{tab:oracle_actions}
\end{table}

\paragraph{Main Results}

We present parsing performances of our model (StructBART) in comparison with previous approaches in Table~\ref{tab:main_results123}. For each model, we also list its features such as utilization of pre-trained language models and graph simplification methods such as re-categorization. This gives a comprehensive overview of how systems compare in terms of complexity aside from performance.

All recent systems rely on pre-trained language models, either as fixed features or through fine-tuning. 
The pre-trained BART is particularly beneficial due to its encoder-decoder structure. 
Among all the models, the graph linearization models \citep{xu2020improving, bevilacqua2021one} have the least number of extra dependencies when not using graph re-categorization. Our model only requires aligned training data, a trait common to all transition-based approaches. This bears the advantage of producing reliable alignments at decoding time, which are useful for downstream tasks and as explanation of the graph constructing process. 



Both our sep-voc and joint-voc model variations work well on all datasets.
Without using extra silver data, our model achieves the  score of 84.2 {\small } on AMR 2.0, which is the same as the previous best model \citep{bevilacqua2021one} with 200K silver data. With the input of only 47K silver data (consisting of 20K example sentences of propbank frames and randomly selected 27K SQuAD-2.0 context sentences\footnote{https://rajpurkar.github.io/SQuAD-explorer/.}), we achieve the highest score of 84.7 {\small } for AMR 2.0. We also attain the high score of 81.7 {\small } on the smallest AMR 1.0 benchmark, and the second best score of 82.7 {\small } on the largest AMR 3.0 benchmark. Ensemble of the 3 models from the silver training further improves the performances to 84.9 for AMR 2.0 and 83.1 for AMR 3.0.

\paragraph{Fine-grained Results}

We further examine the fine-grained parsing results on AMR 2.0 in Table~\ref{tab:finegrained_amr2.0}. We compare models not relying on extra data nor graph re-categorizationn since silver data sets differ across methods, and re-categorization comes with limitations outlined in Section \ref{sec:complexity}. Our models achieve the highest scores across most of the categories, except for negation and wikification. The former may be due to alignment errors and the latter is solved as a separate post-processing step independent of the parser. Compared with the closely related model from \citet{bevilacqua2021one} which also fine-tunes BART but directly on linearized graphs, we achieve significant gains on re-entrancy and SRL ( arcs), proving our
model generate AMR graphs more faithful to their topological structures.

\section{Analysis}
\label{sec:analysis}



\paragraph{Transition System}

Table~\ref{tab:oracle_compare_wide_full} and Table~\ref{tab:oracle_actions} compare different transition systems used by recent transition-based AMR parsers with strong performances. Our proposed system has the smallest set of base actions, utilizes the action-side pointer mechanism for flexible edge creation as in \citet{zhou2021amr}, but does not rely on special treatment of certain subgraphs such as named entities and dates. This results in slightly longer action sequences compared to \citet{zhou2021amr}, but with almost 100\% coverage\footnotemark\footnotetext{We can recover 100\% of AMR 2.0 training graphs excluding  with notation errors. Imperfect  is due to ambiguities of our parser in recovering Penman notation.} (Table~\ref{tab:oracle_actions}).
Our transition system and oracle can always find action sequences with full recovery of the original AMR graph, regardless of graph topology and alignments.

To assess whether our proposed transition system helps integration with pre-trained BART,
we train both the APT model from \citet{zhou2021amr} and our sep-voc model on the transition system of \citet{zhou2021amr} and the one introduced in this work (Table~\ref{tab:oracle_compare_wide_full} last 4 columns). The APT model, based on fixed RoBERTa features, does not benefit from the proposed transition system. However, our proposed model gains 0.6 on AMR 2.0 and 0.7 on AMR 3.0. This confirms the hypothesis that the proposed transitions are better able to exploit BART's powerful language generation ability.


\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{ccc}
    \toprule
         \multirow{2}{*}{\begin{tabular}{@{}c@{}} Model \\ Variation \end{tabular}} & \multicolumn{2}{c}{
         (\%)} \\
        & sep-voc & joint-voc \\
    \midrule
         Ours (hard attention) & 84.0 \small{0.1} & 84.2 \small{0.1} \\
    \midrule
         No align. modeling  & 83.5 \small{0.0} & 83.4 \small{0.2} \\
         Align. soft supervision  & 82.9 \small{0.0} & 83.0 \small{0.0} \\
         Align. add src emb.  & 83.9 \small{0.0} & 84.1 \small{0.0} \\
         No  action  & 83.1 \small{0.1} & 84.1 \small{0.0} \\
    \bottomrule
    \end{tabular}
    } \caption{Ablation study of structure modeling with transition alignments.
    Results are on AMR 2.0 test data.}
    \label{tab:structure_ablation_compact}
\end{table}









\paragraph{Structural Alignment Modeling}

In Table~\ref{tab:structure_ablation_compact}, we evaluate the effects of our structural modeling of parser states within BART during fine-tuning.
 Action-source alignments are natural byproduct of the parser state, providing structural information of where and how to generate the next graph component.
 Our default use of hard attention to encode such alignments works the best.
We explore two other strategies for modeling alignments. One is to supervise cross-attention distributions for the same heads with inferred alignments during training, inspired by \citet{strubell2018linguistically}. The other is to directly add the aligned source contextual embeddings from the encoder top layer to the decoder input at every generation step. The former hurts the model performance, indicating the model is unable to learn the underlying transition logic to infer correct alignments, while the latter does equally well as our default model. These results justify the modeling of structural constraints, even when fine-tuning strong pre-trained models such as BART.
 
We also ablate the use of  action in our transition system. The sep-voc model suffers but the joint-voc model is not affected. Without  action, the joint-voc model would rely more on BART's pre-trained subword embeddings to split node concepts more frequently, while the sep-vocab model would need to learn to generate more rare concepts from scratch. This indicates that BART's strong generation power is fully used to tackle concept sparsity problems with its subwords.


\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/share_voc_node_freq.pdf}
    \caption{Effect of special AMR node names added to the BART vocabulary in joint-voc model on AMR 2.0 dataset. Remaining AMR concepts are split and generated with BART subwords and sense numbers.}
    \label{fig:voc_min_node}
\end{figure}






\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{c|cccccc}
    \toprule
      \multirow{2}{*}{\#} & \multicolumn{4}{c}{Model Initialization}  &  \multicolumn{2}{c}{ (\%)} \\
& src emb & encoder & decoder & tgt emb & Base & Large \\
    \midrule
1 & & & & & 71.2 & 72.7 \\
       2 & \ding{51} & & & \ding{51} & 71.7  & 72.8 \\
       3 & \ding{51} & \ding{51} & & \ding{51} & 81.4 & 82.8 \\
       4 & \ding{51} &  & \ding{51} & \ding{51} & 69.2 & 9.5 \\
       5 & \ding{51} &  &  &  & 71.2 & 72.8 \\
       6 & \ding{51} & \ding{51} &  &  & 80.9 & 82.5 \\
       7 & \ding{51} & \ding{51} & \ding{51} &  & 82.2  & 83.9 \\
       8 & \ding{51} & \ding{51} & \ding{51} & \ding{51} & 82.7 & 84.1 \\
    \midrule
9 & \multicolumn{4}{c}{freeze BART src emb} & 82.6 & 84.0 \\
       10 & \multicolumn{4}{c}{freeze BART src emb + encoder} & 80.9 & 81.8 \\
\bottomrule
    \end{tabular}
    } \caption{Effects of pre-trained BART parameters. Results are with our sep-voc model on AMR 2.0 data.  failed to converge with a range of hyper-parameters.}
    \label{tab:bart_components}
\end{table}


\paragraph{Special Nodes in Joint-Voc}

In Figure~\ref{fig:voc_min_node},
we show the joint-voc model performance with different sized (joint) vocabularies. The vocabulary size is controlled by specifying the minimum frequency of occurrence needed for an AMR concept to be added to the vocabulary.
For instance, when the minimum frequency is 1, all 12475 AMR concepts from the training data are added onto the BART vocabulary. The number of added concepts decreases as we increase the minimum  frequency threshold. On model performance side, we only observe 0.2 score variations resulting from vocabulary expansion. More interestingly, the model can work equally well when no special concepts are added to the BART vocabulary (minimum node frequency is ) -- where all the node names are split and generated with BART subword tokens.
Although our default setup uses frequency threshold of 5 in joint-voc expansion, following \citet{bevilacqua2021one}, it seems unnecessary in terms of achieving good performance.
This highlights the efficacy of utilizing the pre-trained BART's language generation power for AMR concepts even with relatively small annotated training datasets.



\paragraph{Pre-trained Parameters}

We study the contribution of different pre-trained BART components in Table~\ref{tab:bart_components}. With our sep-voc model, we decompose the whole seq-to-seq Transformer into four components for BART initialization, i.e. the source embedding (mapped with BART shared embedding), encoder, decoder, and the separate target embedding (initialized with the average subword embeddings from BART shared embedding). We run both the StructBART base model and the StructBART large model with different combinations of parameter initialization, on the top part of Table~\ref{tab:bart_components}.
We can see that a randomly initialized model of the same size (\#1) performs badly.
There is an accumulative effect of BART initialization in helping the model performance, except that BART decoder can not work alone well without its encoder (\#4).
The encoder gives the largest performance gains (\#3 vs. \#2, \#6 vs. \#5) of about 10 points.
Adding the decoder further gives around 1.4 points on top (\#7 vs. \#6), justifying its importance as well.

We also experiment with freezing BART parameters during training in the bottom part of Table~\ref{tab:bart_components}.
Our results of freezing the BART encoder are on similar levels of previous best RoBERTa feature based models, which is behind the full fine-tuning.
Overall, full initialization from BART with structure-aware fine-tuning (\#8) works the best.
