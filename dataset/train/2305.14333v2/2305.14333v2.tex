\pdfoutput=1
\PassOptionsToPackage{prologue,dvipsnames}{xcolor}
\documentclass[11pt]{article}

\usepackage[]{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{xr}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage[normalem]{ulem}
\usepackage[titletoc,title]{appendix}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{cleveref}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage{tablefootnote}
\usepackage{bbding}

\usepackage{Definitions}
\usepackage{comment}
\allowdisplaybreaks

\newcommand{\bsigma}{\bar \sigma}
\newcommand{\tsigma}{\tilde \sigma}
\newcommand{\hLcal}{\hat \Lcal}
\newcommand{\hL}{\hat L}
\newcommand{\hf}{\hat f}
\newcommand{\htheta}{\hat \theta}
\newcommand{\bg}{\bar g}
\newcommand{\hg}{\hat g}
\newcommand{\baf}{\bar f}
\newcommand{\bW}{\bar W}
\newcommand{\uc}{\textit{\underbar c}}
\newcommand{\bc}{\bar c}
\newcommand{\bepsilon}{\bar \epsilon}
\newcommand{\tGcal}{\tilde \Gcal}
\newcommand{\nc}{the optimal stationary condition}


\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\bp}{\boldsymbol{p}}
\newcommand{\bth}{\boldsymbol{\theta}}
\newcommand{\bA}{\boldsymbol{A}}
\newcommand{\cH}{\pazocal{H}}
\newcommand{\cN}{\pazocal{N}}
\newcommand{\cP}{\pazocal{P}}
\newcommand{\cD}{\pazocal{D}}
\newcommand{\cO}{\pazocal{O}}
\newcommand{\cL}{\pazocal{L}}
\newcommand{\hy}{\hat y}
\newcommand{\hx}{\hat x}
\newcommand{\hell}{\hat \ell}

\newcommand{\by}{\bar y}
\newcommand{\bx}{\bar x}
\newcommand{\bell}{\bar \ell}


\newcommand{\hEE}{\hat \EE}
\newcommand{\oP}{\operatorname{P}}

\newcommand{\hp}{\hat p}
\newcommand{\bbp}{\bar p}
\newcommand{\bs}{\bar s}
\newcommand{\tmu}{\tilde \mu}
\newcommand{\bmu}{\bar \mu}
\newcommand{\tp}{\tilde p}
\newcommand{\tY}{\tilde Y}


\newcommand{\xo}{x_0}
\newcommand{\xoi}{x_0^{(i)}}
\newcommand{\tx}{\tilde x}
\newcommand{\hFcal}{\hat \Fcal}
\newcommand{\ty}{\tilde y}
\newcommand{\AcalO}{\Acal_{\mathrm{OPT}}}
\newcommand{\bE}{\bar E}

\newcommand{\err}{\mathrm{err}}
\newcommand{\acc}{\mathrm{acc}}
\newcommand{\crt}{\mathrm{correct}}
\newcommand{\tdelta}{\tilde \delta}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\usepackage{listings}

\lstdefinestyle{mystyle}{
commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=false,                                 
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}


\usepackage{Definitions}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\definecolor{xu-color}{rgb}{0.54, 0.17, 0.89}
\newcommand{\xu}[1]{\textcolor{xu-color}{[#1]}}
\newcommand{\yuxi}[1]{\textcolor{teal}{[#1]}}

\title{Automatic Model Selection with Large Language Models for Reasoning}


\author{James Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, Michael Qizhe Xie \\
   National University of Singapore \quad   The Hong Kong University of Science and Technology \\
  xu.zhao@u.nus.edu 
}

\begin{document}
\maketitle
\begin{abstract}

Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two distinct reasoning methods, each with its own strengths. CoT employs natural language, offering flexibility and interpretability, while PAL utilizes programming language, yielding more structured and rigorous logic. We introduce a model selection method to combine the best of both worlds by employing a large language model (LLM) to dynamically select between them. Our theoretical analysis underscores the feasibility of this method, which is further corroborated by empirical results. Our proposed method demonstrates significant performance improvements across eight reasoning datasets with Codex, ChatGPT, and GPT-4. Additionally, our method is complementary to self-consistency; when integrated, it can further enhance performance while significantly reducing computation costs. Moreover, we achieve new state-of-the-art results on GSM8K and SVAMP, with respective accuracies of 96.8\% and 93.7\%.\footnote{Our code, data and prompts are available at \href{https://github.com/XuZhao0/Model-Selection-Reasoning}{https://\\github.com/XuZhao0/Model-Selection-Reasoning}}

\end{abstract}

\section{Introduction}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=1.0\linewidth]{emnlp2023-latex/selection_model}
    \caption{We propose to perform model selection to combine two distinct methods, \colorbox[RGB]{227,240,237}{\makebox(20,4){CoT}} and \colorbox[RGB]{223,236,242}{\makebox(20,4){PAL}}. The figure illustrates an example where PAL makes mistakes about \textcolor{violet}{crucial information} and therefore \textcolor{red}{fails} to answer the question correctly. In contrast, CoT manages to \textcolor{cyan}{correctly} answer the same question. Our selection model successfully chooses the correct solution and provides a brief explanation to support its choice.}
    \label{fig:model_demo}
\end{figure*}

Large language models (LLMs) have made impressive progresses in numerous fields~\citep{Devlin2019BERTPO, Brown2020LanguageMA, OpenAI2023GPT4TR, Chowdhery2022PaLMSL, Bubeck2023SparksOA, Wei2022EmergentAO} and are often powerful enough to solve problems through a single unified method. While convenient, this approach tends to ignore the distinct structures and variations among the problems, which would benefit from using different methods. On the other hand, in human society, individuals with different skill sets excel in various roles, leading to a thriving world as a whole. 

In the case of reasoning, Chain-of-Thought (CoT)~\cite{Wei2022ChainOT} and Program-Aided Language Models (PAL)~\citep{Gao2022PALPL, Chen2022ProgramOT} have emerged as two effective methods that offer different strengths and weaknesses. Essentially, CoT decomposes a reasoning problem into a series of intermediate steps using natural language, making it more general, flexible, and comprehensible. On the other hand, PAL offers solutions via Python functions, with its step-by-step programming code ensuring a more rigorous and structured methodology. The external Python interpreter further guarantees the computation precision. Intuitively, combining the strengths of both models and selecting between them based on their solutions to a specific problem would lead to enhanced performance. However, without access to the ground truth, choosing a better method itself becomes a machine learning problem.

In order to select among multiple solutions, previous studies have suggested training a ranker~\citep{Uesato2022SolvingMW}. While training a dedicated model generally results in improved accuracy, it can also be somewhat cumbersome and entail significant costs. Conversely, large language models (LLMs) have demonstrated good calibration and have been used to assess the accuracy of their own outputs~\citep{Guo2017OnCO, Shinn2023ReflexionAA, Xie2023DecompositionER}. In light of this, we propose leveraging the in-context learning capabilities of LLMs for model selection. We direct LLMs to choose between two distinct reasoning models and elucidate their selection rationale.

We present a detailed theoretical analysis to validate our approach. The analysis highlights two primary factors impacting the effectiveness of our method: (1) the significance of the difference between the two models, i.e. the distinct distributions of their respective probabilities of correctly solving problems, and (2) the probability of selecting the correct model. Particularly, a higher overall performance can be attained when there is a substantial difference between the models being considered, and when there is a high probability of the correct model being selected. Furthermore, our analysis affirms that even without an exceptional model selector, we can still achieve improvement in certain cases. This reinforces our decision to simply employ an LLM for model selection. 

We evaluate our method across eight reasoning tasks, with CoT and PAL serving as the baseline methods. Our method consistently attains performance improvement, when employing Codex~\citep{Chen2021EvaluatingLL}, ChatGPT, and GPT-4 as backbone LLMs. Its broad applicability is further underscored by performance gains with open-source Llama 2 models~\citep{Touvron2023Llama2O}. In the context of multiple sample aggregations, our approach significantly reduces the computation costs while achieving notable performance enhancements. Moreover, our approach attains new state-of-the-art accuracies of 96.8\%  and 93.7\% on GSM8K~\citep{Cobbe2021TrainingVT} and SVAMP~\citep{Patel2021AreNM}, respectively.

\section{Automatic Model Selection with Large Language Models}
In this study, we examine reasoning tasks using two baseline models: CoT and PAL. To tackle complex reasoning tasks, CoT leverages an LLM to generate several intermediate reasoning steps before arriving at a final answer. Due to its reliance on natural language, the reasoning steps are clearly explained. Furthermore, natural language supports a broad range of reasoning that may involve common sense and confidence in the reasoning steps. But it might struggle with intricate logical relationships. In contrast, PAL takes a structured and accurate approach, breaking down reasoning problems using Python code. However, the deterministic program format of PAL constrains its generalizability and flexibility. We provide a detailed comparison of these models in Section~\ref{sec:case_difference}, drawing insights from instances where their results differ.

\subsection{Case Study on the Differences between CoT and PAL}
\label{sec:case_difference}
\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0\linewidth]{emnlp2023-latex/error_cases}
    \caption{Comparative analysis of error cases between CoT and PAL on GSM8K.}
    \label{fig:case_study}
\end{figure}

To delve deeper into the distinct strengths of CoT and PAL, we analyze 100 instances from GSM8K where CoT and PAL yield different results: 50 cases where CoT is correct and PAL is wrong, and another 50 where the opposite is true. All the CoT and PAL solutions are generated using ChatGPT. Then we divide the error reasons into 5 categories. Examples of each category can be found in Appendix~\ref{app:example_case}.

\begin{itemize}
    \item Computation Precision: The ability to perform accurate numerical computations.
    \item Logical Decomposition: Breaking down complex problems into manageable parts and handling logical relationships well. 
    \item Problem-Solving Flexibility: The adaptability in addressing problems using different reasoning paradigms, such as forward and backward reasoning. 
    \item Semantic Understanding: Grasping and interpreting the problem accurately without overlooking crucial information.
    \item Others: This encompasses miscellaneous error reasons such as redundant calculations and ambiguous questions. 
\end{itemize}

Figure~\ref{fig:case_study} depicts the error distribution across these categories. It reveals notable differences between CoT and PAL. PAL, with its external Python interpreter, ensures computation accuracy. It also excels in logical decomposition, effectively breaking down problems and employing variables to map logical relationships. On the other hand, CoT is more versatile and flexible, allowing for both forward and backward logical reasoning, while PAL is less adept at backward reasoning scenarios. CoT's strength also lies in its superior grasp of natural language, aiding in semantic understanding. This analysis highlights the distinct proficiencies of both models in reasoning, reinforcing our hypothesis.

\subsection{Method}
Given the distinct advantages of CoT and PAL, it would be beneficial to combine the strengths of both. We propose a systematic approach that combines these models through model selection. Specifically, it contains two stages: solution generation and model selection.

For a given reasoning question , an LLM is prompted to generate reasoning chains for CoT, symbolized as  along with its answer . Simultaneously, the LLM is prompted to derive reasoning chains for PAL, denoted as . Employing a Python interpreter, we then get its answer . When  and  yield different results, we move to the model selection phase. The LLM is prompted again, provided with , and tasked to select the better method, alongside generating a brief explanation. Specifically for Llama 2 selection prompts, the LLM is prompted with  pairs. This selection result, together with the explanation, is represented as . Ultimately, based on , the final answer is derived as .

During the selection phase, LLMs leverage their in-context learning abilities for model selection, i.e., we present the LLM with a few in-context exemplar pairs . An example of the proposed method is illustrated in Figure~\ref{fig:model_demo}, excluding the few-shot examples to save space. Specifically, we provide the LLM with two distinct reasoning solutions in a multiple-choice question format. We expect the LLM to identify errors or gaps within the chains and determine the correct method. Note that there are very few instances where the LLM fails to make a choice; in these cases, we randomly select a method.  The effect and qualitative analysis of the generated explanation can be found in Appendix~\ref{sec:effect_exp}. 

Owing to the in-context learning capabilities of LLMs, we find that they exhibit reasonable accuracy in selecting the appropriate method. Furthermore, as our subsequent theoretical analysis in Section~\ref{sec:theory_analysis} reveals, a highly accurate method selection is not actually necessary for the algorithm to perform well. Our empirical findings corroborate this, showing that even if there are situations where the model selection is poor, the overall improvement remains substantial.


\subsection{Theoretical Analysis} \label{sec:theory_analysis}
In this section, we conduct a theoretical analysis to determine under which condition the proposed method could work (and fail). 

\paragraph{Quantifying error rates} Let us denote the error rates of the two base methods,  and , by  and , respectively. Without loss of generality, let  be a better base method in the overall performance: i.e., .
For a given question , we define  as the probability of choosing a more accurate method, either  or , for the given  using the proposed approach. 
Define  where  represents the probability of outputting a correct prediction given input  with method . Then we can quantify the final error rate  of the proposed method as follows:
\begin{proposition}  \label{prop:1}
For any methods  and  with any combining probability function , 

\end{proposition}
We refer readers to Appendix \ref{app:proofs} for the full proof. Proposition \ref{prop:1} decomposes the possible improvement (or deterioration) over base methods in terms of  and . It quantitatively shows when and how we can expect improvement (or deterioration) based on these two factors. For example, to improve over the best base method , Proposition \ref{prop:1} suggests us to choose another base method  such that  is not too small and  is high when  is large. In other words, it discourages us from choosing too similar methods as base methods, because for similar methods,  tends to be small and it is challenging to increase  even when  is large due to the similarity. This provides a theoretical motivation for us to use CoT and PAL, instead of combining CoT with another CoT.

\paragraph{On the accuracy of selection} Define  to be the overall probability of selecting a better method:  . Theorem~\ref{thm:2} shows that  can be much worse than that of a random guess to achieve the improvement over the base methods  and ; i.e.,  and  can happen with : 
\begin{theorem} \label{thm:2}
For any , there exist data distribution over , two base methods (), and combining probability () such  that   , , and .  
\end{theorem}
We provide a stronger version of Theorem \ref{thm:2} in Appendix \ref{app:thm:2_detail} (that implies Theorem \ref{thm:2}) and its proof in Appendix \ref{app:proofs}. 

Theorem~\ref{thm:2} supports our proposition that, despite not training a new model for the selection process and with the in-context learning limited to a few-shot prompt, it is possible to achieve improvement, even if we do not achieve  in some instances. This theoretical analysis offers deeper insights into the conditions and strategies for effective performance improvement with the proposed methodology. 

\section{Experiments}

\subsection{Setup}\label{sec:setup}
\textbf{Datasets and backbones} 
We conduct experiments on eight datasets that span a range of arithmetic and symbolic reasoning tasks. 7 of these datasets, including GSM8K~\citep{Cobbe2021TrainingVT}, SVAMP~\citep{Patel2021AreNM}, ASDIV~\citep{Miao2020ADC}, SingleOP, SingleEQ, AddSub and MultiArith~\citep{KoncelKedziorski2016MAWPSAM}, are about arithmetic reasoning, while Date Understanding~\citep{Srivastava2022BeyondTI} focuses on symbolic reasoning. To comprehensively evaluate the effectiveness of our approach, we employ three proprietary LLMs as backbone systems: Codex (\texttt{code-davinci-002}), ChatGPT (\texttt{gpt-3.5-turbo-0301}) and GPT-4 (\texttt{gpt-4-0314}). We also evaluate our method with open-source Llama 2 models on GSM8K.\footnote{Codex results are obtained in February and March, ChatGPT in April and May, GPT-4 in May and June 9-13, Llama 2 in September 2023.} Unless stated otherwise, we always utilize the same LLM for both base models as well as for the model selector.

\textbf{Prompt design}
To effectively exploit the in-context learning abilities of the LLMs, we create a set of few-shot examples by manually creating an error in one model's reasoning chain. Few-shot examples in our prompts are based on those in previous works. For each backbone LLM, we use a fixed prompt for all the arithmetic benchmarks without specific customizations. Examples of the prompts can be found in Appendix~\ref{app:prompt_example}.

\textbf{Hyperparameters} 
For the results derived using greedy decoding, we use a temperature of 0 during both solution generation and model selection. For experiments with self-consistency in Section~\ref{sec:main_results} and \ref{sec:computation_cost}, we follow the settings in prior works by setting the temperature at 0.5 for CoT and 0.8 for PAL during solution generation. During model selection, the temperature is set to 0 for a deterministic result. We refrain from further hyperparameter tuning to emphasize the simplicity and efficacy of our method.


\begin{table*}[ht]
\centering
\small
    \begin{tabular}{cccccccccc}
    \toprule
    \multirow{2}{*}{Backbone} & \multirow{2}{*}{Method} &\multicolumn{7}{c}{Arithmetic} & \multicolumn{1}{c}{Symbolic}\\
      & & GSM8K & SVAMP & ASDIV & SingleOP & SingleEQ & AddSub & MultiArith & Date\\
    \midrule
    \multirow{3}{*}{\tabincell{c}{Codex}}
    & CoT & 64.4 & 77.6 & 80.2 & 92.7 & 93.5  & 88.4 & 95.7 & 64.5 \\
    & PAL & 71.5 & 79.6 & 79.1 & 95.4 & 96.5  & \textbf{91.9} & 99.7 & 77.5 \\ 
    & \textbf{Ours} & \textbf{74.7} & \textbf{82.2} & \textbf{81.6} & \textbf{96.3} & \textbf{96.9} & 91.6 & \textbf{99.7} & \textbf{79.4}\\

    \midrule
    \multirow{3}{*}{\tabincell{c}{ChatGPT}}
    & CoT & 80.8 & 83 & 89.3 & 94.8 & 97.4  & 90.4 & 98.7 & 69.1 \\
    & PAL & 79.2 & 80.3 & 83 & 90.7 & 97.6  & 89.4 & 96.3 & 68.3 \\ 
    & \textbf{Ours} & \textbf{82.6} & \textbf{84.3} & \textbf{89.4} & \textbf{94.8} & \textbf{97.8}  & \textbf{90.6} & \textbf{98.7} & \textbf{70.2}\\

    \midrule
    \multirow{3}{*}{\tabincell{c}{GPT-4}}
    & CoT & 94.6 & 91.9 & 92.7 & 97.2 & 97.2 & 93.9 & 98 & 90 \\
    & PAL & 94.0 & 92.2 & 90.2 & 95.2 & \textbf{98.8} & 94.9 & 98.5 & 88.1 \\ 
    & \textbf{Ours} & \textbf{95.6} & \textbf{93.7}& \textbf{93.5} & \textbf{97.3} & 98.6 & \textbf{95.7} & \textbf{99} & \textbf{90.5}\\

    \bottomrule         
    \end{tabular}
    \caption{Results comparison (Accuracy \%) on 7 arithmetic datasets and 1 symbolic dataset with greedy decoding. We evaluate our methods on Codex, ChatGPT, and GPT-4. The best results are highlighted in \textbf{bold}.}
    \label{tab:main_results}
\end{table*}



\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{cccc}
    \toprule
    Backbone & Method &  &  \\
     \midrule
     \multirow{3}{*}{\tabincell{c}{ChatGPT}}
     & CoT & 85.4 & 87.4 \\
     & PAL & 80.9 & 82.4 \\
     & \textbf{Ours} & \textbf{88.2}~\textcolor{ForestGreen}{(+2.8)} & \textbf{89.2}~\textcolor{ForestGreen}{(+1.8)} \\
     \midrule
     \multirow{3}{*}{\tabincell{c}{GPT-4}}
     & CoT & 95.6 & 95.8\\
     & PAL & 94.7 & 95.5 \\
     & \textbf{Ours} & \textbf{96.5}~\textcolor{ForestGreen}{(+0.9)} & \textbf{96.8}~\textcolor{ForestGreen}{(+1.0)}\\
     \bottomrule
    \end{tabular}
    \caption{Results comparison (Accuracy \%) on GSM8K with the integration of the Self-Consistency (SC).  and  represents  and  sampled paths respectively. The previous state-of-the-art on GSM8K is 95.5\%, achieved by \citet{Zheng2023ProgressiveHintPI}.}
    \label{tab:sc_results}
\end{table}

\subsection{Main Results}\label{sec:main_results}

The results of our experiments with greedy decoding are shown in Table~\ref{tab:main_results}.  
First, we find that our proposed method effectively and robustly enhances performance in most settings across various datasets of different difficulties and with different backbone LLMs, simply by combining two base models. For example, with GPT-4, we achieve an accuracy of 95.6\% on GSM8K and 93.7\% on SVAMP without self-consistency.

Second, our results show a considerable improvement even when one of the base models performs much worse than the other. For instance, we observe a significant 3.2\% improvement over PAL's 71.5\% accuracy on GSM8K, even though CoT has a lower accuracy of 64.4\%. 

Third, our model's general applicability is further underscored by its 1.9\% improvement on the symbolic date understanding task when utilizing Codex as the backbone. In fact, even if the accuracy difference between two base models is as large as 13\% on this task, our proposed method still improves the accuracy from 77.5\% to 79.4\%. Additionally, our method also achieves respectable performance gains with both ChatGPT and GPT-4 on this symbolic reasoning task. It serves as evidence of our method's applicability to a spectrum of reasoning tasks, extending beyond mathematical reasoning.


\begin{table}[t]
    \centering
    \small
    \begin{tabular}{ccccc}
    \toprule
    Method & Acc. & Cost & \# Tokens  & \# Generated \\
     \midrule
     CoT@15 & 87.4 & 33.69 & 21.56M  & 2.72M \\
     PAL@15 & 82.4 & 41.64 & 27.13M & 1.90M \\
     \textbf{Ours@5} & \textbf{88.2} & \textbf{29.29} & \textbf{18.99M}  & \textbf{1.61M} \\
     \midrule
     \midrule
     CoT@40 & 88.3 & 89.84  & 57.32M & 7.08M\\
     PAL@40 & 83.5 & 111.04 & 72.35M & 5.07M\\
    \textbf{Ours@15} & \textbf{89.2} & \textbf{87.87} & \textbf{56.97M}  & \textbf{4.83M} \\
     \midrule
     \midrule
     CoT@80 & 88.2 & 179.68 & 115.04M  & 14.48M\\
     PAL@80 & 83.3 & 222.08 & 144.64M  & 10.16M\\
     \textbf{Ours@15} & \textbf{89.5} & \textbf{117.16} & \textbf{75.96M} & \textbf{6.44M} \\
     \bottomrule
    \end{tabular}
    \caption{Computation cost comparison with ChatGPT on GSM8K (1319 data samples). CoT@15 represents CoT with self-consistency (SC) with 15 sampled paths. Cost in USD is calculated based on OpenAI Pricing\footnotemark. \# Tokens denotes the total number of tokens, including inputs and generation tokens. \# Generated denotes the number of generated tokens. }
    \label{tab:cost_comp}
\end{table}
\footnotetext{\href{https://openai.com/pricing}{https://openai.com/pricing}}



\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{cccccc}
    \toprule
    Baseline LLM & Selector LLM &  &  & Ours & Improvement\\
     \midrule
     Llama 2 7B & Llama 2 7B & 15.0 & 13.7  & \textbf{16.2} & \textcolor{ForestGreen}{+1.2}\\
     Llama 2 7B & Llama 2 13B  & 15.0 & 13.7 & \textbf{16.8} & \textcolor{ForestGreen}{+1.8}\\
     Llama 2 13B & Llama 2 13B & 29.0  & 33.3 & \textbf{35.3} &\textcolor{ForestGreen}{+2.0}\\
     \bottomrule
    \end{tabular}
    \caption{Results with Llama 2 7B/13B on a subset of GSM8K. The reference CoT accuracies in the  Llama 2 paper~\citep{Touvron2023Llama2O} with Llama 2 7B and 13B are 14.6\% and 28.7\% respectively. Baseline LLM refers to the backbone LLM used for both CoT and PAL solution generation. Selector LLM refers to the backbone LLM used for model selection.}
    \label{tab:llama2_result}
\end{table*}

\paragraph{Experiments with self-consistency} We aim to investigate the relationship between self-consistency~\citep{Wang2022SelfConsistencyIC} and model selection with multiple samples, and whether they complement each other. The results of CoT and PAL with self-consistency are obtained by sampling multiple times with the temperature at 0.5 and 0.8 respectively. For our method with self-consistency, we execute our algorithm repeatedly, using the hyperparameter settings specified in Section~\ref{sec:setup}. Subsequently, we aggregate results across all samples through majority voting, arriving at the final answer. As demonstrated in Table~\ref{tab:sc_results}, we achieve substantial improvements over the self-consistency results of both CoT and PAL.

Employing ChatGPT, we attain a pronounced performance improvement with only 5 samples, reflecting a 2.8\% leap in accuracy. With GPT-4, even though both base models already score around 95\%, integrating them with our method leads to a 96.5\% accuracy on GSM8K. Furthermore, we establish the new state-of-the-art results on GSM8K at 96.8\% using 15 samples.


\subsection{Computation Cost Comparison}\label{sec:computation_cost}
We extend our analysis to compare the computation costs of model selection with multiple samples versus using self-consistency alone. As depicted in Table~\ref{tab:cost_comp}, our method not only showcases notable performance improvement but also significantly reduces computation costs, underscoring its efficacy and efficiency.

Our method consistently outperforms both CoT and PAL with SC in terms of accuracy while demanding less token computation and cost. For instance, using only 5 paths, our method achieves comparable performance to CoT@80 while only costs 29 USD, which is merely \textbf{16 \%} of expense associated with CoT@80. Moreover, when aggregating 15 samples, our method surpasses the performance of CoT@80 at only half the cost.

Furthermore, our method also elevates the performance ceilings. While the result of CoT with SC plateaus around 88.2\% and that of PAL with SC saturates around 83.3\%, our method breaks through these ceilings,  achieving an impressive 89.5\% accuracy using just 20 samples. This result underscores the power and potential of integrating different reasoning models.


\subsection{Experiments with Llama 2 Models}

To explore the broad applicability of our method, we conduct experiments with open-source Llama 2 7B/13B models using a subset of the GSM8K, comprising 600 data samples. We generate CoT and PAL solutions with Llama 2 7B/13B models\footnote{\href{https://huggingface.co/meta-llama/Llama-2-7b-hf}{https://huggingface.co/meta-llama/Llama-2-7b-hf}\par \quad \href{https://huggingface.co/meta-llama/Llama-2-13b-hf}{https://huggingface.co/meta-llama/Llama-2-13b-hf}}. For model selection, we use Llama 2 7B/13B Chat models\footnote{
\href{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf}{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf} \par \quad \href{https://huggingface.co/meta-llama/Llama-2-13b-chat-hf}{https://huggingface.co/meta-llama/Llama-2-13b-chat-\\hf}}. The experiments are carried out by using greedy decoding without tuning hyperparameters.

As shown in Table~\ref{tab:llama2_result}, our method consistently yields performance enhancements when applied to Llama 2 models. Specifically, compared to the baseline results on the 13B model, our method achieves respectable 2\% performance gains, improving accuracy from 33.3\% to 35.3\%. On the smaller 7B model, we observe the performance improvement from 15.0\% to 16.2\% when utilizing the 7B model as the selector. Moreover, when employing the 13B model as the selector, the performance is further improved to 16.8\%. These results affirm the broad applicability of our method.

\section{Analysis}
In this section, we provide a few analyses to see when and how our method works.

\begin{table*}[t]
\centering
\small
    \begin{tabular}{cccccccccc}
    \toprule
    \multirow{2}{*}{Backbone} & \multirow{2}{*}{Metric} &\multicolumn{7}{c}{Arithmetic} & \multicolumn{1}{c}{Symbolic}\\
      & & GSM8K & SVAMP & ASDIV & SingleOP & SingleEQ & AddSub & MultiArith & Date\\
    \midrule
    \multirow{3}{*}{\tabincell{c}{Codex}}

    &  & 10 & 8.1 & 6.5 & 1.6 & 0.9 & 2.0 & 0.3 & 4.6 \\
    & Success Rate & 74.8 & 72.5 & 63.7 & 87.9 & 88  & 70 & 92.9 & 87.8 \\
    & Improvement & \textcolor{ForestGreen}{+3.2} & \textcolor{ForestGreen}{+2.6} & \textcolor{ForestGreen}{+1.4} & \textcolor{ForestGreen}{+0.9} & \textcolor{ForestGreen}{+0.4} & \textcolor{red}{-0.3} & \textcolor{ForestGreen}{0} & \textcolor{ForestGreen}{+1.9}\\

    \midrule
    \multirow{3}{*}{\tabincell{c}{ChatGPT}}
    &  & 8.6 & 6.0 & 3.4 & 2.4 & 1.4 & 2.5 & 0.6 & 9.8 \\
    & Success Rate & 60.4 & 66.4 & 69.8 & 58 & 57.1 & 60.9 & 75 & 53.6 \\
    & Improvement & \textcolor{ForestGreen}{+1.8} & \textcolor{ForestGreen}{+1.3} & \textcolor{ForestGreen}{+0.1} & \textcolor{ForestGreen}{0} & \textcolor{ForestGreen}{+0.4} & \textcolor{ForestGreen}{+0.2} & \textcolor{ForestGreen}{0} & \textcolor{ForestGreen}{+1.1}\\

    \midrule
    \multirow{3}{*}{\tabincell{c}{GPT-4}}
    &  & 2.5 & 3.6 & 2.2 & 2.4 & 0.6 & 1.3 & 0.5 & 1.1 \\

    & Success Rate & 72.6 & 68.7 & 64.2 & 57 & 69.2 & 85.7 & 100 & 86.7 \\
    & Improvement & \textcolor{ForestGreen}{+1.0} & \textcolor{ForestGreen}{+1.8} & \textcolor{ForestGreen}{+0.8} & \textcolor{ForestGreen}{+0.1} & \textcolor{red}{-0.2} & \textcolor{ForestGreen}{+0.8} & \textcolor{ForestGreen}{+0.5} & \textcolor{ForestGreen}{+0.5}\\

    \bottomrule         
    \end{tabular}
    \caption{We define the following terms: , where  is the upper bound accuracy where we assume a perfect model selection and   is the stronger one of the two base models.  reflects the expected performance difference between the two base models. Success Rate calculates the correct selection rates when either CoT is correct or PAL is correct, i.e., we ignore the cases where both methods are either correct or wrong. Improvement is the performance improvement achieved over the performance of the stronger base model.}
    \label{tab:upper_selection}
\end{table*}

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{c|cc|cc}
    \toprule
          Backbone & \multicolumn{2}{c|}{ChatGPT} & \multicolumn{2}{c}{GPT-4}\\
           & \multicolumn{2}{c|}{CoT} & \multicolumn{2}{c}{CoT} \\
           & PAL &  & PAL & CCoT\\
         \midrule
          & 80.8 & 80.8 & 94.6 & 94.6\\
          & 79.2 & 79.2 & 94.0 & 95.1 \\
\textbf{Ours} & \textbf{82.6} & 80.8 & \textbf{95.6} & 95.2 \\
          Improvement & \textbf{\textcolor{ForestGreen}{(+1.8)}} & \textcolor{ForestGreen}{(+0)} & \textbf{\textcolor{ForestGreen}{(+1.0)}} &\textcolor{ForestGreen}{(+0.1)}\\
         & \textbf{8.6} & 7.5 & \textbf{2.5} & 1.7 \\
         Success Rate & \textbf{60.4} & 52.2 & \textbf{72.6} & 58.8\\
         \bottomrule
    \end{tabular}
    \caption{Other model combinations results on GSM8K.  denotes the base CoT model with the temperature of . CCoT denotes ComplexCoT~\citep{Fu2022ComplexityBasedPF}. 
    }
    \label{tab:other_combinations}
\end{table}


\subsection{Influencing Factors}
To better understand the reasons for the performance improvement across various datasets and backbone LLMs, we present the performance improvement, and the associated influencing factors in Table~\ref{tab:upper_selection}.  

As demonstrated in our theoretical analysis, the performance improvement is jointly determined by the difference between the two combined methods, , and the effectiveness of the model selection, . The results from Table~\ref{tab:upper_selection} showcase a high expected performance difference between CoT and PAL, and decent selection accuracy with all backbone LLMs, which clarifies the significant performance gains across the datasets.  

Firstly, the  of CoT and PAL reflects how differently the two base models behave across questions.  A larger  signifies a larger room for potential improvement. Specifically, on GSM8K with ChatGPT,  stands at 8.6\%, even though the accuracies of CoT and PAL are closely matched at 80.8\% and 79.2\%, respectively. Similarly, with GPT-4,  is 2.5\% while the accuracies of the two base models are close (94.6\% vs 94.0\%). 

Secondly, across the eight datasets with varied complexities, each of the three LLMs exhibits commendable selection accuracy. Notably, both Codex and GPT-4 show a relatively high success selection rate. For example, their success rates on GSM8K surpass 70\%, and on the simpler MultiArith dataset, the selection accuracies exceed 90\%. 

Furthermore, as detailed in Theorem~\ref{thm:2}, performance improvements are attainable even without a high success selection rate. Our empirical results corroborate it. For instance, on the date understanding task employing ChatGPT, despite a modest success rate of 53.6\%, we still achieve 1.1\% performance enhancement with a large  of 9.8\%. 

\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{cccccccc}
    \toprule
    CoT LLM & PAL LLM & Selector LLM &  &  & Ours & Success Rate &  \\
     \midrule
      ChatGPT & ChatGPT & ChatGPT & 80.8 & 79.2 & \textbf{82.6} \textcolor{ForestGreen}{(+1.6)} & 60.4 & 8.6\\
     ChatGPT & Codex & ChatGPT & 80.8 & 71.5  & \textbf{81.2}  \textcolor{ForestGreen}{(+0.4)} & 70.6 & 7.0\\
     ChatGPT & Codex  & GPT-4 & 80.8 & 71.5 & \textbf{84.4} \textcolor{ForestGreen}{(+3.6)} & 84.8 & 7.0 \\
     \bottomrule
    \end{tabular}
    \caption{Results with the combination of different backbone LLMs on GSM8K. CoT LLM refers to the backbone LLM used for CoT solution generation.}
    \label{tab:different_backbones}
\end{table*}

\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{c|cc|cc|cc}
    \toprule
         \multirow{2}{*}{Metric} & \multicolumn{2}{c|}{Codex} & \multicolumn{2}{c|}{ChatGPT} & \multicolumn{2}{c}{GPT-4}\\
         & CoT-PAL & PAL-CoT & CoT-PAL & PAL-CoT & CoT-PAL & PAL-CoT\\
         \midrule
 & 64.4 & 64.4 & 80.8 & 80.8 & 94.6 & 94.6\\
          & 71.5 & 71.5 & 79.2 & 79.2 & 94.0 & 94.0\\
         \textbf{Ours} & 69.9\textcolor{red}{(-1.6)} & \textbf{74.7\textcolor{ForestGreen}{(+3.2)}} & \textbf{82.6\textcolor{ForestGreen}{(+1.8)}} & 81.6\textcolor{ForestGreen}{(+0.8)} & \textbf{95.6\textcolor{ForestGreen}{(+1.0)}} & 95.1\textcolor{ForestGreen}{(+0.5)}\\
         Success Rate & 56.7 & 75 & 60.4 & 54.6 & 72.6 & 63 \\
          & 71.9 & 17.3 & 89.9 & 79.7 & 60.3 & 53.4 \\
          & 28.1 & 82.7 & 10.1 & 20.3 & 39.7 & 46.6 \\
         \bottomrule
    \end{tabular}
    \caption{The effect of option order for different backbones on GSM8K. CoT-PAL represents CoT and PAL being placed as choice A and B respectively.  indicates the ratio of CoT selected out of all the selections. As stated previously, during analysis, we ignore the cases where both CoT and PAL are either correct or wrong.}
    \label{tab:order_effect}
\end{table*}

\subsection{Combination between Similar Methods}

We choose CoT and PAL as our two base models due to the motivation of combining different strengths of distinct models. We conduct experiments to examine whether the performance improves when we combine two similar base models. We use two variants of CoT:  where we set the temperature at , ComplexCoT~\citep{Fu2022ComplexityBasedPF} where we use more complex few-shot examples in the prompt. Both of these methods' accuracies are similar or higher than the accuracy of PAL. The results are shown in Table~\ref{tab:other_combinations}. 

From Table~\ref{tab:other_combinations}, we can find that model selection between CoT and , or CoT and ComplexCoT, does not lead to substantial performance gains, even though the accuracy of  and ComplexCoT is on par with PAL. On the other hand, model selection between CoT and PAL results in consistent performance improvements. To understand the reasons behind these outcomes, we further investigate the  and the success selection rate. 

Firstly,  of CoT-PAL exceeds that of other combinations, CoT- and ComplexCoT-CoT, despite their employing two stronger or equivalent two base models. This observation suggests a larger absolute value of the accuracy difference per question for CoT-PAL. It indicates that CoT and PAL perform more dissimilarly than other model combinations, which represents a larger . As Proposition~\ref{prop:1} highlights, without a substantial , it is unlikely to achieve significant performance gain since the improvement component is factored by .

Secondly, the success selection rate of CoT-PAL surpasses that of other model combinations. It means that the selector is more likely to select the correct choice when one solution derives from CoT and the other from PAL. In theory, this higher success rate implies that when  is high for a given question , the success selection probability  for CoT-PAL is higher than others.

These findings support our initial motivation and hypothesis. We choose CoT and PAL as our two base models because they represent distinct reasoning approaches using natural language and programming language. We expect these models to exhibit a significant difference in errors and accuracies, indicated by a high . Moreover, we posit that the considerable difference in errors for a particular question makes it easier for LLMs to select the correct option, leading to a higher success rate compared to selecting between two similar base models like CoT-. This holds true even when different prompts or temperature settings are used. 

\subsection{Combination between Different Backbone LLMs}
Considering that Codex is optimized for code generation and ChatGPT performs better in natural language reasoning, we embark on experiments integrating various backbone LLMs, to harness the strengths of distinct backbone LLMs and reasoning models.

As shown in Table~\ref{tab:different_backbones}, we achieve a 0.4\% performance improvement with ChatGPT as the selector, despite a notable 9.3\% accuracy disparity between PAL from Codex and CoT from ChatGPT. More impressively, when utilizing GPT-4 as the model selector, we get a significant 3.6\% performance boost, with an 84.8\% success selection rate. These results hint the potential benefits when integrating multiple models tailored for specialized domains or tasks.

\subsection{The Effect of Option Order}

To understand the influence of option order in a multiple-choice question format, we perform experiments by modifying the order of CoT and PAL options. The results in Table~\ref{tab:order_effect} show a performance impact from simply changing the choice order. We attribute this to the inherent bias within the backbone LLMs. A similar phenomenon is reported by~\citet{Zheng2023JudgingLW}.

Each of the three backbones seems to exhibit a preference for the first option. Particularly noticeable on Codex, in which when CoT is positioned first, it is selected in 71.9\% of all selections. However,  significantly drops to 17.3\% when putting CoT second. ChatGPT shows a preference for natural language reasoning as it always selects CoT solutions more frequently regardless of its position in the options. Moreover, when CoT is the second choice,  decreases from 89.9\% to 79.7\%, which we speculate demonstrates its preference for the first option. GPT-4 has similar biases akin to ChatGPT, though it appears fairer. The option order also affects the success selection rate and further influences the overall performance. 

While \citet{Zhao2021CalibrateBU} attempts to address the inherent bias through probability calibration, the task of mitigating the bias in the absence of underlying token probabilities remains a challenge. Therefore, despite the wide use of LLMs in data annotation and task evaluation\citep{Xu2023BaizeAO, Wang2023PandaLMAA, vicuna2023}, addressing and eliminating inherent biases is a critical area that deserves further exploration.

\section{Related Work}
\textbf{Ensemble learning.} In machine learning, the strategy of combining various models to address a single problem is exemplified in techniques such as bagging~\citep{breiman1996bagging}, boosting~\citep{freund1997decision, chen2016xgboost,ke2017lightgbm}, and random forest~\citep{ho1995random, breiman2001random}. The underlying idea in these methods is that a group of weak learners can collectively manifest as a strong learner. This concept has also found its place in deep learning through the use of ensembles. For reasoning, self-consistency samples diverse reasoning paths and chooses the most consistent answer through majority voting~\citep{Wang2022SelfConsistencyIC}. \citet{Wang2022RationaleAugmentedEI} takes it a step further by introducing rationale-augmented ensembles, emphasizing rationale sampling in the output space. However, typically, ensembling places equal weights on models through majority voting, which may restrict the full potential of the diverse strengths that each model offers.

\textbf{Reasoning.} 
The research community has made tremendous progress in the field of reasoning. Apart from CoT~\citep{Wei2022ChainOT} and PAL~\citep{Gao2022PALPL, Chen2022ProgramOT}, \citet{Zhou2022LeasttoMostPE} simplifies complex problems by breaking them down into a series of sub-problems. \citet{Kojima2022LargeLM} shows that by simply adding "Let's think step by step" before each answer, LLMs can be competent zero-shot reasoners. \citet{Creswell2022SelectionInferenceEL} alternates between selection and inference stages, generating causal reasoning steps to the final answer. \citet{Kazemi2022LAMBADABC} proposes a backward chaining algorithm that breaks reasoning down into four sub-models. 
\citet{Paul2023REFINERRF, Xie2023DecompositionER, Yao2023TreeOT} refine intermediate reasoning steps by leveraging another critic model or the self-evaluation ability. Recently, \citet{Zhou2023SolvingCM} utilizes GPT-4 Code Interpreter and code-based-self-verification, achieving a 97\% accuracy on GSM8K, but their work is concurrent with ours. More importantly, the contributions of these methods are distinct from our approach, and the progress made by them could potentially be seamlessly integrated using our method.

\textbf{Self-Evaluation.}
LLM calibration studies reveal that the probabilistic predictions made by current LLMs closely align with the actual frequencies of token occurrences, hence producing well-calibrated predictions for certain tasks~\citep{Guo2017OnCO, Kadavath2022LanguageM, Jiang2020HowCW}. 
As LLMs exhibit reliable calibration, there is a growing number of research emphasizing the use of self-evaluation for verification. \citet{Shinn2023ReflexionAA} proposes an approach to provide an agent with dynamic memory and self-reflection capabilities. \citet{Madaan2023SelfRefineIR} proposes a method to generate outputs from LLMs and refine its previously generated output given its own feedback. Different from these works where the underlying method is the same, in this work, we are interested in combining systems with different strengths and weaknesses through self-evaluation model selection.

\section{Conclusion}
We introduce a method that effectively combines two distinct base models, CoT and PAL, by enabling an LLM to make the selection. We provide a theoretical analysis that supports the feasibility of such a model combination, which is further validated by empirical results. Our method achieves performance improvements across eight datasets with various backbone LLMs. We reduce the computation cost while attaining notable performance improvement with self-consistency. We set new SOTA performance on GSM8K and SVAMP. This research represents a significant step towards tapping into the potential of diversity and collaboration among models in LLMs.

In our future work, we aim to expand this framework to more reasoning tasks and possibly other domains. An intriguing concept involves exploring the use of diverse system instructions to elicit varying model behaviors for model combinations. Furthermore, a genuine combination of specialized models, each with its strengths and expertise, offers a promising avenue for exploration.

\section{Limitation}
This work focuses on reasoning tasks, but we believe that extending the exploration of model selection into other domains could yield valuable insights. Further, due to the inherent bias of LLMs, our method is affected by the order of options. We encourage the community to explore solutions for bias mitigation. We also anticipate subsequent studies to identify more robust and potent model selectors. Finally, our combination strategy currently comprises only the CoT and PAL base models. Future research could investigate the incorporation of more base models with distinct strengths, with the aim of creating a more powerful synthesis that leverages their unique advantages.


\bibliography{anthology}
\bibliographystyle{acl_natbib}

\clearpage
\appendix
\label{sec:appendix}

\onecolumn


\section{A detailed version of Theorem \ref{thm:2}} \label{app:thm:2_detail}
In this appendix, we provide a detailed version of Theorem \ref{thm:2}. Whereas Theorem \ref{thm:2} only states the existence of problem instances, Theorem \ref{thm:1} constructs such instances concretely: i.e., Theorem \ref{thm:1} implies Theorem \ref{thm:2}. Define  to be the distribution for the expected errors: i.e., an expected error can be written by  for some function . Define . Let us denote  as the uniform distribution over .
 Given any , we write , ,  . Assume that .
\begin{theorem} \label{thm:1}
Let  and   be given such that . Let  and  such that  and  . Let  and  be set such that 
   for ,   for ,   , and  . Then, we have  that  , , and 

In particular, when , we have   as  and  (with   ); when  , we have  as  and  (with  ).
\end{theorem}
The proof of Theorem \ref{thm:1} is presented in Appendix \ref{app:proofs}. Theorem~\ref{thm:1} shows that the overall success probability of the selection process can be much worse than a random guess to achieve the improvement over the base methods  and ; i.e.,  and  can happen with . Indeed, it is possible to have  with the improvement ( and ) when the size of  is large: when , we can choose  with which   , , and  as  and . When  , we can choose  with which   , , and  as  and . This supports our proposition that, despite not training a new model for the selection process and with the in-context learning limited to a few-shot prompt, it is possible to achieve improvement, even if we do not achieve  in some instances.


Theorem~\ref{thm:1} also suggests that if the overall performance of two base methods is similar, captured by , the overall selection process can be weak to achieve some improvement, as long as the success selection probability is relatively high when the two methods have very different expected errors (or accuracies) for a given question. In essence, Theorem~\ref{thm:1} suggests a trade-off: we want  to be larger when deciding which two base methods  and  to choose, implying that we prefer base methods to perform dissimilarly on . On the other hand, if two base methods exhibit a substantial expected accuracy difference, then the selection process needs to be stronger to improve the performance (i.e.,  needs to be larger). However, if the expected accuracy difference between the two base methods is relatively small, increasing the power of the selection process is not that necessary to boost performance.



\section{Proofs} \label{app:proofs}
\subsection{Proof of Proposition \ref{prop:1}}
\begin{proof}
Define  and  for . Since , we have that

where  represents the event of the correct prediction. Similarly,
  
where  represents the probability of selecting method   given  via the proposed method. Thus, 
  
Since , 

Here, we notice that 

By plugging this into the above equation,
  
Since  and , we have that 
 
Since  for any , 
 

  
\end{proof}

\subsection{Proof of Theorem \ref{thm:1}}
\begin{proof}
We first confirm that  and  define valid probabilities under the condition of this statement.  For , since  and , it defines valid probabilities for methods  and . For  , since , it also defines valid probabilities for  the case of . For the case of , since , we need to show that . That is, 
   
which is satisfied by the condition on  that . Thus,  the condition on  defines the valid probabilities for both cases of   and  .

We now show that    . 
Invoking Proposition \ref{prop:1}, 
  
Thus, we have    if . This condition can be rewritten as

This  is satisfied by the condition on  that   for some : i.e., 

Therefore, 
we have that    . 

We now show that . Similarly to the proof of Proposition \ref{prop:1}, we define   for . Then, the inequality   holds if  . By using   to represent the event of the correct prediction, this condition can be rewritten as
   
This is satisfied by .
Thus, we have that  .

Using these, we now compute the  as 

Finally, we prove  the asymptotic behavior
using this equation. When , by setting , we have that 
as  and . When , by setting , we have that 

By defining , we have 

Here, 

Thus, 

as  and : e.g., by setting  and take , with which .   
\end{proof}

\clearpage
\section{The Effect and Qualitative Analysis of Explanation}\label{sec:effect_exp}

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{cccccc}
    \toprule
    Backbone &  & & Explanation? & Ours Acc. & \tabincell{c}{Success\\ Rate} \\
    \midrule
     \multirow{2}{*}{Codex} & 64.4 & 71.5 & w/o exp & \textbf{74.7}~\textcolor{ForestGreen}{(+3.2)} & 74.9 \\
     & 64.4 & 71.5 & w/ exp & 74.6 ~\textcolor{ForestGreen}{(+3.1)}& 74.2 \\
     \midrule
     \multirow{2}{*}{ChatGPT} & 80.8 & 79.2 & w/o exp & 81.8 ~\textcolor{ForestGreen}{(+1.0)} & 55.9 \\
     & 80.8 & 79.2 & w/ exp & \textbf{82.6} ~\textcolor{ForestGreen}{(+1.8)}& 60.4 \\
     \midrule
     \multirow{2}{*}{GPT-4} & 94.6 & 94.0 &w/o exp & 95.5~\textcolor{ForestGreen}{(+0.9)} & 69.9\\
     & 94.6 & 94.0 & w/ exp & \textbf{95.6}~\textcolor{ForestGreen}{(+1.0)} & 72.6\\
     \bottomrule
    \end{tabular}
    \caption{Accuracy and success selection rate with and without explanation on GSM8K.}
    \label{tab:exp_effect}
\end{table}

To perform model selection, we provide explanations in the few-shot examples and ask the LLMs to generate explanations after making a choice, as we expect to improve the model's selection ability by pointing out why the other choice is incorrect. To investigate the potential role of explanations in enhancing the model's selection capability, we conduct experiments on GSM8K by excluding explanations from the answer.

The results in Table~\ref{tab:exp_effect} reveal that for backbone LLMs with more powerful in-context learning abilities, such as Codex and GPT-4, the inclusion of explanations within the answer seems to have negligible influence on the model's performance. In other words, Codex and GPT-4 have a relatively high success selection rate even when explanations are not given. However, for ChatGPT, which possesses relatively low few-shot in-context learning capability, the integration of explanations yields a larger improvement in the success selection rate. This observation suggests that for backbone LLMs with weaker in-context learning abilities, explanations can serve as a valuable tool to improve the success selection rate.

\paragraph{Qualitative studies on the explanations}
We are also interested in gaining an intuitive understanding of the algorithm, so we conduct some qualitative analysis on the selection explanations. As displayed in Table \ref{tab:exp_case}, in the first example, both ChatGPT and GPT-4 can accurately identify that the PAL solution is correct. However, ChatGPT provides an incorrect explanation, while GPT-4 offers a valid one. 
In the second example, which is relatively simpler, both ChatGPT and GPT-4 successfully perform accurate model selection and provide valid explanations.
In the first example, we can see that GPT-4 actually possesses exceptional reasoning capabilities and provides reliable explanations.


\begin{table*}[t]
    \centering
    \begin{tabular}{p{0.46\textwidth}|p{0.46\textwidth}}
    \toprule
     \multicolumn{2}{p{0.92\textwidth}}{\footnotesize \textbf{Question}: Courtney attended a concert and reported that the audience was 48 in number. However, Kelly went to the same concert and said that Courtney had made the mistake of overstating the number of people in attendance by 20\%. If Kelly was right, how many people really attended the concert?} \\
    \midrule
    \footnotesize \textbf{(A)} 
    \newline
    \footnotesize Answer: \newline
     \footnotesize Courtney reported 48 people in attendance.\newline
     \footnotesize If this number is overstated by 20\%, then we need to find the actual number of people.\newline
     \footnotesize First, we need to find 20\% of 48.\newline
     \footnotesize 20\% of 48 is (20/100) * 48 = 9.6. \newline
     \footnotesize Now, we subtract this number from Courtney's reported number to find the actual number of people.\newline
     \footnotesize 48 - 9.6 = 38.4.\newline
     \footnotesize Since we cannot have a fraction of a person, we round the number to the nearest whole number.\newline
     \footnotesize So, the actual number of people in attendance was approximately 38.\newline
     \footnotesize So the answer is 38.
     & 
     \footnotesize \textbf{(B)}
\lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    courtney_report = 48
    overstatement_percentage = 0.20
    actual_attendance = courtney_report /(1 + overstatement_percentage)
    result = int(actual_attendance)
    return result
     \end{lstlisting}\\
     \midrule
\multicolumn{2}{p{0.92\textwidth}}{\footnotesize \textbf{ChatGPT}: (B) can correctly answer the math problem. \textcolor{red}{Because (A) rounds up the result to the nearest whole number, which is not necessary.}}\\
      \midrule
      \multicolumn{2}{p{0.92\textwidth}}{\footnotesize \textbf{GPT-4}: (B) can correctly answer the math problem. \textcolor{cyan}{Because (A) incorrectly calculated the actual number of people by subtracting the overstatement percentage from the reported number instead of dividing it.}}
     
\\
     \midrule
     \midrule
\multicolumn{2}{p{0.92\textwidth}}{\footnotesize \textbf{Question}: How much does it cost you for lunch today at Subway if you pay \40, then the six-inch cold-cut combo sub costs 3 * \120.\newline
     \footnotesize So the total cost for both subs is \120 = \160 for lunch today at Subway.
     & 
     \footnotesize \textbf{(B)}
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution(): 
    cost_footlong_fish = 40
    cost_sixinch_coldcut = cost_footlong_fish / 3
    total_cost = cost_footlong_fish + cost_sixinch_coldcut
    result = total_cost
    return result
     \end{lstlisting}\\
     \midrule
\multicolumn{2}{p{0.92\textwidth}}{\footnotesize \textbf{ChatGPT}: (A) can correctly answer the math problem. \textcolor{cyan}{Because (B) calculates the cost of the six-inch cold-cut combo sub as one-third of the cost of the foot-long fish sub instead of three times the cost of the foot-long fish sub.}}\\
      \midrule
      \multicolumn{2}{p{0.92\textwidth}}{\footnotesize \textbf{GPT-4}: (A) can correctly answer the math problem. \textcolor{cyan}{Because (B) incorrectly calculates the cost of the six-inch cold-cut combo sub as one-third of the foot-long fish sub instead of thrice as much.}}
\\
\bottomrule
     
     
    \end{tabular}
    \caption{In the first example, ChatGPT performs model selection correctly, but gives a \textcolor{red}{wrong} explanation, while GPT-4 gives a \textcolor{cyan}{correct} explanation. The second example shows where both ChatGPT and GPT-4 select correctly and give the \textcolor{cyan}{correct} explanation.}
    \label{tab:exp_case}
\end{table*}


\clearpage
\section{Examples of Prompts} \label{app:prompt_example}
We show examples of model selection prompts used on different tasks with different backbones. For arithmetic reasoning tasks, we employ an 8-shot example for both Codex and Llama 2, and a 5-shot example for ChatGPT and GPT-4. For the date understanding task, we use 6-shot examples for Codex, ChatGPT, and GPT-4. We only show a few examples for each case. Full prompts can be found in our code.
\begin{table*}[h]
    \centering
    \begin{tabular}{p{0.46\textwidth}|p{0.46\textwidth}}
    \toprule
     \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Math Problem}: Olivia has \3 each. How much money does she have left?} \\
      \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Question}:  Which of the following two choices can \textbf{correctly} answer the math problem?} \\
     
    \midrule

    \small \textbf{(A)} 
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    money_initial = 23
    bagels = 5
    bagel_cost = 3
    money_spent = bagels * bagel_cost
    money_left = money_initial - money_spent
    result = money_left
    return result
     \end{lstlisting} 
&
    \small \textbf{(B)} \newline
    \newline
    \small Answer: \newline
     \small  Olivia had 23 dollars. \newline
     5 bagels for 3 dollars each will be 5 * 3 = 15 dollars. \newline
     So she has 23 - 5 = 18 dollars left. \newline
     The answer is 18.\\
     \midrule

      \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Answer}: (A)}
\\
     \bottomrule
    \end{tabular}
    \caption{An example of 8-shot model selection prompts used on 7 arithmetic datasets with Codex.}

    \label{tab:codex_example_math}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{p{0.46\textwidth}|p{0.46\textwidth}}
    \toprule
     \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Date Understanding Problem}: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?} \\
      \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Question}: Which of the following two choices can \textbf{correctly} answer the date understanding problem?} \\
     
    \midrule

    \small \textbf{(A)}
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    # If 2015 is coming in 36 hours, then today is 36 hours before.
    today = datetime(2015, 1, 1) - relativedelta(hours=36)
    # One week from today,
    one_week_from_today = today + relativedelta(weeks=1)
    # The answer formatted with result = one_week_from_today.strftime('return result
     \end{lstlisting} 
&
    \small \textbf{(B)} \newline
    \newline
    \small A: \newline
     \small  If 2015 is coming in 36 hours, then it is coming in 2 days. \newline
     2 days before 01/01/2015 is 12/30/2014, so today is 12/30/2014. \newline
     So one week from today will be 01/06/2015.\newline
     So the answer is 01/06/2015. \\
     \midrule

      \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Answer}: (A)}
     
\\
     \bottomrule
    \end{tabular}
    \caption{An example of 6-shot model selection prompts used on Date Understanding task with Codex.}

    \label{tab:codex_example_date}
\end{table*}


\begin{table*}[ht]
    \centering
    \begin{tabular}{p{0.46\textwidth}|p{0.46\textwidth}}
    \toprule
     \multicolumn{2}{p{0.92\textwidth}}{\small 
     \textbf{System}: You are a helpful assistant that can identify the correct answer to the math problem.}\\
     \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small There are two choices to the same math problem. One uses natural language to answer the question, while the other uses Python program to answer it. Either of them can correctly answer the math problem. You need to identify which choice can correctly answer the math problem. Here is one example how to do it,} \\
      \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Math problem}: Olivia has \3 each. How much money does she have left?} \\
     
    \midrule

    \small \textbf{(A)} \newline
    \newline
    \small Answer: \newline
     \small Olivia had 23 dollars. \newline
     \small 5 bagels for 3 dollars each will be 5 * 3 = 15 dollars. \newline
     \small So she has 23 - 15 = 8 dollars left.\newline
     \small So the answer is 8.\newline
&
    \small \textbf{(B)} 
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    money_initial = 23
    bagels = 5
    bagel_cost = 3
    money_spent = bagels + bagel_cost
    money_left = money_initial - money_spent
    result = money_left
    return result
     \end{lstlisting} \\

     \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small Which of the above two choices can \textbf{correctly} answer the math problem? }\\
     \midrule
      \multicolumn{2}{p{0.92\textwidth}}{\small (A) can correctly answer the math problem. Because (B) adds the number of bagels to the cost of each bagel instead of multiplying them.}\\
\multicolumn{2}{p{0.92\textwidth}}{\small Now it's your turn. Here is another math problem and two choices. }\\
        \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Math Problem}: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday? }\\
        \midrule
        \small \textbf{(A)} \newline
    \newline
    \small Answer: \newline
     \small Michael started with 58 golf balls.\newline
     \small Then after losing 23 on tuesday, he had 58 -23 = 35. \newline
     \small After losing 2 more, he had 35 + 2 = 37 golf balls. \newline
     \small So the answer is 37. \newline
&
    \small \textbf{(B)} 
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    golf_balls_initial = 58
    golf_balls_lost_tuesday = 23
    golf_balls_lost_wednesday = 2
    golf_balls_left = golf_balls_initial - \
        golf_balls_lost_tuesday - 
            \golf_balls_lost_wednesday
    result = golf_balls_left
    return result
     \end{lstlisting} \\

     \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small Which of the above two choices can \textbf{correctly} answer the math problem? }\\
\multicolumn{2}{p{0.92\textwidth}}{\small (B) can correctly answer the math problem. Because (A) adds 2 more balls after losing 2 more on Wednesday instead of subtracting them.}\\
     \bottomrule
    \end{tabular}
    \caption{Two examples of 5-shot model selection prompts used on 7 arithmetic datasets with ChatGPT.}

    \label{tab:chatgpt_example_math}
\end{table*}




\begin{table*}[ht]
    \centering
    \begin{tabular}{p{0.46\textwidth}|p{0.46\textwidth}}
    \toprule
     \multicolumn{2}{p{0.92\textwidth}}{\small 
     \textbf{System}: You are a helpful assistant that can identify the correct answer to the math problem.}\\
     \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small There are two choices to the same math problem. One uses natural language to answer the question, while the other uses Python code to answer it. Either of them can correctly answer the math problem. You need to identify which choice can correctly answer the math problem. Here is one example how to do it,} \\
      \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Math problem}: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?} \\
     
    \midrule

    \small \textbf{(A)} \newline
    \newline
    \small Answer: \newline
     \small There were originally 9 computers. \newline
     \small For each of 4 days from monday to thursday, 5 more computers were added. \newline
     \small So 5 * 4 = 20 computers were added.\newline
     \small So there are 9 + 20 = 29 computers now.\newline
     \small So the answer is 29. \newline

     &
    \small \textbf{(B)} 
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    computers_initial = 9
    computers_added = 5
    computers_total = computers_initial + computers_added
    result = computers_total
    return result
     \end{lstlisting} \\

     \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small Which of the above two choices can \textbf{correctly} answer the math problem? }\\
\multicolumn{2}{p{0.92\textwidth}}{\small (A) can correctly answer the math problem. Because (B) missed the fact that computers were added each day from monday to thursday.}\\
          \midrule
        \multicolumn{2}{p{0.92\textwidth}}{\small Now it's your turn. Here is another math problem and two choices. }\\
        \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Math Problem}: A piece of square paper has a perimeter of 32 centimeters. Nicky's dog, Rocky, tore off 1/4 of the paper. What is the area of the remaining paper? }\\
        \midrule

    \small \textbf{(A)} \newline
    \newline
    \small Answer: \newline
     \small A square has 4 equal sides. \newline
     \small The perimeter of the square paper is 32 centimeters. \newline
     \small So each side of the square is 32 / 4 = 8 centimeters. \newline
     \small The area of the whole square paper is side * side = 8 * 8 = 64 square centimeters. \newline
     \small Rocky tore off 1/4 of the paper. \newline
     \small So The area of the remaining paper is 1/4 * 64 = 16 square centimeters. \newline
     \small So the answer is 16. \newline

     &
    \small \textbf{(B)} 
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    perimeter = 32
    fraction_torn = 1 / 4
    area_total = (perimeter / 4) ** 2
    area_remaining = (1 - fraction_torn) * area_total
    result = area_remaining
    return result
     \end{lstlisting} \\
     \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small Which of the above two choices can \textbf{correctly} answer the math problem? }\\
\multicolumn{2}{p{0.92\textwidth}}{\small (B) can correctly answer the math problem. Because (A) incorrectly calculated the area of the torn-off portion instead of the remaining portion.}\\
     \bottomrule
    \end{tabular}
    \caption{Two examples of 5-shot model selection prompts used on 7 arithmetic datasets with GPT-4.}

    \label{tab:gpt4_example_math}
\end{table*}


\begin{table*}[ht]
    \centering
    \begin{tabular}{p{0.46\textwidth}|p{0.46\textwidth}}
    \toprule
     \multicolumn{2}{p{0.92\textwidth}}{\small 
     \textbf{System}: You are a helpful assistant that can identify the correct answer to the date understanding problem.}\\
     \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small There are two choices to the same date understanding problem. One uses natural language to answer the question, while the other uses Python program to answer it. Either of them can correctly answer the date understanding problem. You need to identify which choice can correctly answer the problem. Here is one example how to do it,} \\
      \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Date Understanding Problem}: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? } \\
     
    \midrule

    \small \textbf{(A)} \newline
    \newline
    \small Answer: \newline
     \small If 2015 is coming in 36 hours, then it is coming in 2 days. \newline
     \small And 2 days before 01/01/2015 is 12/30/2014, so today is 12/30/2014. \newline
     \small So one week from today will be 01/05/2015. \newline
     \small So the answer is 01/05/2015.\newline
     &
    \small \textbf{(B)} 
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    # If 2015 is coming in 36 hours, then today is 36 hours before.
    today = datetime(2015, 1, 1) + relativedelta(hours=36)
    # One week from today,
    one_week_from_today = today + relativedelta(weeks=1)
    # The answer formatted with result = one_week_from_today.strftime('return result
     \end{lstlisting} \\

     \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small Which of the above two choices can \textbf{correctly} answer the date understanding problem?}\\
\multicolumn{2}{p{0.92\textwidth}}{\small (A) can correctly answer the date understanding problem. Because (B) incorrectly calculates the date 36 hours later instead of 36 hours before.}\\
          \midrule
        \multicolumn{2}{p{0.92\textwidth}}{\small Now it's your turn. Here is another date understanding problem and two choices. }\\
        \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Date Understanding Problem}: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY? }\\
        \midrule
        \small \textbf{(A)} \newline
    \newline
    \small Answer: \newline
     \small If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. \newline
     \small And today is the first monday, would be 5 days later. \newline
     \small So today is 01/06/2019. \newline
     \small So the answer is 01/06/2019.\newline

     &
    \small \textbf{(B)} 
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    # If the first day of 2019 is a Tuesday, and today is the first Monday of 2019, then today is 6 days later.
    today = datetime(2019, 1, 1) + relativedelta(days=6)
    # The answer formatted with result=today.strftime('return result
     \end{lstlisting} \\
    \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small Which of the above two choices can \textbf{correctly} answer the date understanding problem?}\\
\multicolumn{2}{p{0.92\textwidth}}{\small (B) can correctly answer the problem. Because (A) missed the fact that there are 6 days between the first day of 2019 and the first Monday of 2019.}\\
     \bottomrule
    \end{tabular}
    \caption{Two examples of 6-shot model selection prompts used on Date Understanding with ChatGPT and GPT-4.}

    \label{tab:chatgpt_example_date}
\end{table*}


\begin{table*}[ht]
    \centering
    \begin{tabular}{p{0.46\textwidth}|p{0.46\textwidth}}
    \toprule
     \multicolumn{2}{p{0.92\textwidth}}{\small 
     \textbf{System}: You are a helpful and brilliant assistant. You are a math expert who can identify the correct answer to the math problem.}\\
     \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small There are two choices to the same math problem. One uses natural language to answer the question, while the other uses Python program to answer it. *ONLY ONE* of them can correctly answer the math problem. 
You need to check these two solutions carefully and identify which choice can correctly answer the math problem. 

Here is the math problem and two solutions to it.} 
\\
      \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Math problem}: Olivia has \3 each. How much money does she have left?} \\
     
    \midrule

    \small \textbf{(A)}
        \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    money_initial = 23
    bagels = 5
    bagel_cost = 3
    money_spent = bagels * bagel_cost
    money_left = money_initial - money_spent
    result = money_left
    return result
     \end{lstlisting} 
The result is 8.
     &
    \small \textbf{(B)}  \newline
    \newline
    \small Answer: \newline
     \small Olivia had 23 dollars. \newline
     \small 5 bagels for 3 dollars each will be 5 + 3 = 8 dollars. \newline
     \small So she has 23 - 8 = 15 dollars left.\newline
     \small The answer is 15.\newline
     \\

     \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small Which of the above two choices can \textbf{correctly} answer the math problem? (A) or (B)? }\\
     \midrule
      \multicolumn{2}{p{0.92\textwidth}}{\small (A) can correctly answer the math problem. Because (B) adds the number of bagels to the cost of each bagel instead of multiplying them.}\\
     \bottomrule
    \end{tabular}
    \caption{One example of 8-shot model selection prompts on GSM8K with Llama 2 7B as the model selector.}

    \label{tab:llama2_7b_example_math}
\end{table*}


\begin{table*}[h]
    \centering
    \begin{tabular}{p{0.46\textwidth}|p{0.46\textwidth}}
    \toprule
     \multicolumn{2}{p{0.92\textwidth}}{\small 
     \textbf{System}: You are a helpful and brilliant assistant. You are a math expert who can identify the correct answer to the math problem.}\\
     \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small There are two choices to the same math problem. One uses natural language to answer the question, while the other uses Python program to answer it. *ONLY ONE* of them can correctly answer the math problem. 
You need to check these two solutions carefully and identify which choice can correctly answer the math problem. 

Here is the math problem and two solutions to it.} 
\\
      \multicolumn{2}{p{0.92\textwidth}}{\small \textbf{Math problem}: Olivia has \3 each. How much money does she have left?} \\
     
    \midrule

    \small \textbf{(A)}  \newline
    \newline
    \small Answer: \newline
     \small Olivia had 23 dollars. \newline
     \small 5 bagels for 3 dollars each will be 5 * 3 = 15 dollars. \newline
     \small She has 23 - 15 = 8 dollars left..\newline
     \small The answer is 8.\newline
     &
    \small \textbf{(B)} 
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    money_initial = 23
    bagels = 5
    bagel_cost = 3
    money_spent = bagels + bagel_cost
    money_left = money_initial - money_spent
    result = money_left
    return result
     \end{lstlisting} 
     The result is 15.
     \\

     \midrule
     \multicolumn{2}{p{0.92\textwidth}}{\small Which of the above two choices can \textbf{correctly} answer the math problem? (A) or (B)? }\\
     \midrule
      \multicolumn{2}{p{0.92\textwidth}}{\small (A) can correctly answer the math problem. Because (B) adds the number of bagels to the cost of each bagel instead of multiplying them.}\\
     \bottomrule
    \end{tabular}
    \caption{One example of 8-shot model selection prompts on GSM8K with Llama 2 13B as the model selector.}

    \label{tab:llama2_13b_example_math}
\end{table*}

\clearpage
\section{Examples of Failure Cases Comparison}
\label{app:example_case}
In Section~\ref{sec:case_difference}, we divide the error reasons of CoT and PAL on GSM8K into 5 categories. Detailed examples of 4 primary categories are presented in the following tables.

\begin{table*}[h]
    \centering
    \begin{tabular}{p{0.46\textwidth}|p{0.46\textwidth}}
    \toprule
     \multicolumn{2}{p{0.92\textwidth}}{\footnotesize \textbf{Question}: It costs \29100.\newline
     \footnotesize The cost per meter is the same for Lewis' street, so the cost to repave it is \textcolor{red}{194 * 490 = \94860 - \65760.\newline
     \footnotesize So it costs \2 each and packs of cloths will cost \92. Profit is the difference between total income and total expenses, so how much profit, in dollars, will Lucas make each week?} \\
    \midrule
    \footnotesize Answer: \newline
     \footnotesize Kim has 3 clients and is talking to 5 more potential clients, so he will have 3 + 5 = 8 clients in total.\newline
     \footnotesize Each client needs 2 bottles of bleach, so Kim will need 2 * 8 = 16 bottles of bleach each week. \newline
     \footnotesize \textcolor{red}{Each client also needs a pack of cloths, so Kim will need 8 * 5 = 40 packs of cloths each week.}\newline
     \footnotesize The cost of the bleach will be 16 * 2 = \200.\newline
      \footnotesize So Kim's total expenses will be 32 + 200 = \92. \newline
    \footnotesize So his profit will be 92 - 232 = -\140 each week. \textcolor{red}{\XSolidBrush}
     & 
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    num_clients = 8
    bleach_cost = 2
    cloths_cost = 5
    total_expenses = num_clients * (2 * bleach_cost + cloths_cost)
    total_income = 92
    profit = total_income - total_expenses
    result = profit
    return result
     \end{lstlisting}
   \colorbox{Black}{\makebox(25,4){\small \textcolor[RGB]{0,255,0}{>20.0}}} \textcolor{Green}{\Checkmark}
     \\
\bottomrule
     
    \end{tabular}
    \caption{A CoT failure case due to \textbf{Logical Decomposition}. PAL breaks down the problem into step-by-step Python code, and utilizes variables to map logical relationships.}
    \label{tab:comp_case_2}
\end{table*}



\begin{table*}[h]
    \centering
    \begin{tabular}{p{0.46\textwidth}|p{0.46\textwidth}}
    \toprule
     \multicolumn{2}{p{0.92\textwidth}}{\footnotesize \textbf{Question}:Seth is twice as old as Brooke. In 2 years, the sum of their ages will be 28. How old is Seth?} \\
    \midrule
    \footnotesize Answer: \newline
     \footnotesize Let's assume Brooke's age to be x.\newline
     \footnotesize Then Seth's age would be 2x (as he is twice as old as Brooke).\newline
     \footnotesize In 2 years, Brooke's age would be x+2 and Seth's age would be 2x+2.\newline
     \footnotesize The sum of their ages in 2 years would be (x+2) + (2x+2) = 3x+4.\newline
      \footnotesize We know that the sum of their ages in 2 years will be 28.\newline
      \footnotesize So, 3x+4 = 28.\newline
    \footnotesize Solving for x, we get x = 8. \newline
    \footnotesize Therefore, Brooke's age is 8 and Seth's age is 2x = 16.\newline
    \footnotesize So the answer is 16. \textcolor{Green}{\Checkmark}
     & 
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    sum_of_ages_in_two_years = 28
    age_difference = 2 * 2  # Seth is twice as old as Brooke
    age_of_seth_in_two_years = (sum_of_ages_in_two_years + age_difference) / 2
    age_of_seth = age_of_seth_in_two_years - 2
    result = age_of_seth
    return result
     \end{lstlisting}
    \colorbox{Black}{\makebox(25,4){\small \textcolor[RGB]{0,255,0}{>14.0}}} \textcolor{Red}{\XSolidBrush}
     \\
\bottomrule
     
    \end{tabular}
    \caption{A PAL failure case due to \textbf{Problem-Solving Flexibility}. CoT employs both forward and backward reasoning paradigms and solves this problem correctly.}
    \label{tab:comp_case_3}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{p{0.46\textwidth}|p{0.46\textwidth}}
    \toprule
     \multicolumn{2}{p{0.92\textwidth}}{\footnotesize \textbf{Question}: Gloria is shoe shopping when she comes across a pair of boots that fit her shoe budget. However, she has to choose between the boots and \textcolor{violet}{two pairs of high heels that together cost five dollars less than the boots}. If one pair of heels costs \33, and the other pair costs twice as much, which is \66.\newline
     \footnotesize So the total cost of the two pairs of high heels is \66 = \5 more than the cost of the two pairs of high heels. \newline
      \footnotesize Therefore, the cost of the boots is \5 = \104. \textcolor{Green}{\Checkmark}
     & 
    \lstset{style=mystyle}
     \begin{lstlisting}[language=Python]
def solution():
    heels1 = 33
    heels2 = 2 * heels1
    heels_total = heels1 + heels2
    boots = heels_total - 5
    result = boots
    return result
     \end{lstlisting}
     \colorbox{Black}{\makebox(25,4){\small \textcolor[RGB]{0,255,0}{>94.0}}} \textcolor{Red}{\XSolidBrush}
     \\
\bottomrule
     
    \end{tabular}
    \caption{A PAL failure case due to \textbf{Semantic Understanding}. PAL fails to understand the crucial information \textit{`five dollars less than boosts'}, whereas CoT exhibits a more advanced semantic understanding capability.}
    \label{tab:comp_case_4}
\end{table*}

\end{document}
