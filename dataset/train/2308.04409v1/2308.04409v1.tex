\documentclass[10pt,twocolumn,letterpaper]{article}


\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{diagbox}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pbox}
\usepackage{url}

\usepackage{algorithm}\usepackage{algpseudocode}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{enumerate}

\usepackage[misc]{ifsym}
\usepackage{bbding}


\usepackage{booktabs}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\bq}{\mathbf{q}}
\newcommand{\bqe}{\mathbf{q}^{e}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bo}{\mathbf{\Delta q}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\hbb}{\hat{\bb}}
\newcommand{\hbc}{\hat{\bc}}
\newcommand{\hba}{\hat{\ba}}
\newcommand{\hbd}{\hat{\bd}}
\newcommand{\hbs}{\hat{\bs}}
\newcommand{\hbl}{\hat{\bl}}
\newcommand{\hbp}{\hat{\bp}}
\newcommand{\bmatch}{\mathrm{match}}
\usepackage[table,x11names]{xcolor}
\def\methods{CASeg}
\usepackage{multicol}
\usepackage{multirow}

\newcommand\paperurl[1]{{\footnotesize{\color{blue}{\url{#1}}}}}

\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.0pt}{7.5pt}\fontfamily{lmtt}\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{8pt}{9pt}\color{gray},
  keywordstyle=\fontsize{8pt}{9pt}\color{blue},
  stringstyle=\fontsize{8pt}{9pt}\color{purple},
  frame=tb,
  otherkeywords = {self},
  upquote=true,
  escapeinside={<@}{@>},
}

\newcommand\our{\textsc{DeepNet}}
\newcommand\deepnorm{\textsc{DeepNorm}}
\newcommand\postln{Post-LN}
\newcommand\preln{Pre-LN}


\newcommand\todo[1]{\textcolor{red}{TODO: #1}}
\newcommand\toedit[1]{\textcolor{orange}{TOEDIT: #1}}
\newcommand{\comment}[1]{}
\newcommand{\tsdagger}{{\textsuperscript{\textdagger}}}

\newcommand{\mbf}[1]{\mathbf{#1}}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\definecolor{LightCyan}{rgb}{0.88,1,1}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}

\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \S\or \S\or
\mathsection\or \mathparagraph\or \|\or **\or \S\S
\or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\renewcommand{\ttdefault}{ptm}  \newcommand{\tablestyle}[2]{\ttfamily\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage{algorithm}
\usepackage[]{algpseudocode}

\newcommand{\norm}[1]{\|#1\|}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\cvprfinalcopy 

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

\title{\bf{V-DETR: DETR with Vertex Relative Position Encoding for 3D Object Detection}}



\author{
Yichao Shen\quad\;
Zigang Geng \quad
Yuhui Yuan \quad
Yutong Lin \quad
Ze Liu \quad \\
Chunyu Wang \quad\quad\;
Han Hu \quad\quad\;
Nanning Zheng \quad\quad\;
Baining Guo \\\hspace{-5mm}
Xi'an Jiaotong University \quad
University of Science and Technology of China \quad
Microsoft Research Asia
}


\maketitle


\begin{abstract}
We introduce a highly performant 3D object detector for point clouds using the DETR framework. The prior attempts all end up with suboptimal results because they fail to learn accurate inductive biases from the limited scale of training data. In particular, the queries often attend to points that are far away from the target objects, violating the locality principle in object detection. To address the limitation, we introduce a novel 3D Vertex Relative Position Encoding (3DV-RPE) method which computes position encoding for each point based on its relative position to the 3D boxes predicted by the queries in each decoder layer, thus providing clear information to guide the model to focus on points near the objects, in accordance with the principle of locality. In addition, we systematically improve the pipeline from various aspects such as data normalization based on our understanding of the task. We show exceptional results on the challenging ScanNetV2 benchmark, achieving significant improvements over the previous 3DETR in / from 65.0\%/47.0\% to 77.8\%/66.0\%, respectively. In addition, our method sets a new record on ScanNetV2 and SUN RGB-D datasets.
Code will be released at: {\url{{https://github.com/yichaoshen-MS/V-DETR}}}.
\end{abstract}

\begin{figure}[t]
\centering
\includegraphics[height=0.2\columnwidth]{img/rpe_local/1realworld1.png}
\hfill
\includegraphics[height=0.2\columnwidth]{img/rpe_local/1norpe1.png}
\hfill
\includegraphics[height=0.2\columnwidth]{img/rpe_local/1rpe1.png} \\
\includegraphics[height=0.2\columnwidth]{img/rpe_local/2realworld.png}
\hfill
\includegraphics[height=0.2\columnwidth]{img/rpe_local/2norpe.png}
\hfill
\includegraphics[height=0.2\columnwidth]{img/rpe_local/2rpe.png} \\
\begin{subfigure}[b]{0.3\linewidth}
\centering
\includegraphics[height=0.64\columnwidth]{img/rpe_local/3realworld.png}
\caption{}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\linewidth}
\centering
\includegraphics[height=0.64\columnwidth]{img/rpe_local/3norpe.png}
\caption{}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\linewidth}
\centering
\includegraphics[height=0.64\columnwidth]{img/rpe_local/3rpe.png} 
\caption{}
\end{subfigure}

\caption{\small{
(a) 3D scans from the ScanNetV2 in the rear/front/top-down view. 
We display one of the ground-truth bounding boxes with a green 3D box.
(b) The decoder cross-attention map based on plain DETR. Attention weights are distributed over many positions even outside the ground-truth box.
(c) The decoder cross-attention map based on plain DETR + 3DV-RPE. Attention weights focus on the sparse object boundaries of the object located in the ground-truth bounding box.
The color indicates the attention values: yellow for high and blue for low.
}}
\label{fig:attention_maps_intro}
\vspace{-5mm}
\end{figure}

\section{Introduction}
\begin{figure*}[t]
\centering
\hspace{-12mm}
\begin{subfigure}[b]{0.22\linewidth}
\centering
\includegraphics[height=0.32\columnwidth]{img/3DETR_intro_zg_fig1.pdf}
\caption{\small{voxelized sparse input}}
\end{subfigure}
\hspace{-2mm}
\begin{subfigure}[b]{0.22\linewidth}
\centering
\includegraphics[height=0.32\columnwidth]{img/3DETR_intro_zg_fig2.pdf}
\caption{\small{Voting-based method}}
\end{subfigure}\hspace{5mm}
\begin{subfigure}[b]{0.22\linewidth}
\centering
\includegraphics[height=0.32\columnwidth]{img/3DETR_intro_zg_fig3.pdf}
\caption{\small{Expansion-based method}}
\end{subfigure}\hspace{10mm}
\begin{subfigure}[b]{0.22\linewidth}
\centering
\includegraphics[height=0.32\columnwidth]{img/3DETR_intro_zg_fig4.pdf}
\caption{DETR-based method}
\end{subfigure}\caption{\small{
(a) A simplified sparse 3D voxel space from a top-down perspective. The curve shows the input surface and the small cubes show the voxelized input. The gray five-pointed star \FiveStar shows the object's center. (b) The voting scheme estimates offsets for each voxel and we color the voxels (nearer to the object's center) with yellow after voting. The dashed small cubes show the empty space after voting. (c) The generative sparse decoder (GSD) scheme enlarges the voxels around the surfaces, thus creating new voxels both inside and outside of the object (marked with yellow cubes).
(d) The DETR-based approach simply selects a small set of voxels (marked with yellow cubes) as the initial object query and iteratively predicts the boxes by refining (marked with the open yellow circles) the object query with multiple Transformer decoder layers. We follow the DETR-based path in this work.
}}
\label{fig:intro_compare_approach}
\vspace{-5mm}
\end{figure*}


3D object detection from point clouds is a challenging task that involves identifying and localizing the objects of interest present in a 3D space. This space is represented using a collection of data points that have been gleaned from the surfaces of all accessible objects and background in the scene.  The task has significant implications for various industries, including augmented reality, gaming, robotics, and autonomous driving.

Transformers have made remarkable advancement in 2D object detection, serving as both powerful backbones~\cite{Vaswani2017attention,liu2021swin} and detection architectures~\cite{carion2020end}. However, their performance in 3D detection~\cite{misra2021-3detr} is significantly worse than the state-of-the-art methods.  Our in-depth evaluation of~\cite{misra2021-3detr} revealed that the queries often attend to points that are far away from the target objects (Figure \ref{fig:attention_maps_intro} (b) shows three typical visualizations), which violates the principle of \emph{locality} in object detection. The principle of locality dictates that object detection should only consider subregions of data that contain the object of interest and not the entire space. 
Besides, the behavior is also in contrast with the success that Transformers have achieved in 2D detection, where they have been able to effectively learn the inductive biases, including locality. We attribute the discrepancy to the limited scale of training data available for 3D object detection, making it difficult for Transformers to acquire the correct inductive biases. 

In this paper, we present a simple yet highly performant method for 3D object detection using the transformer architecture DETR~\cite{carion2020end}. To improve locality in the cross-attention mechanism, we introduce a novel 3D Vertex Relative Position Encoding (3DV-RPE) method. It computes a position encoding for each point based on its relative offsets to the vertices of the predicted 3D boxes associated with the queries, providing clear positional information such as whether each point is inside the boxes. This information can be utilized by the model to guide cross-attention to focus on points inside the box, in accordance with the principle of locality. The prediction of these boxes is consistently refined as the decoder layers progress, resulting in increasingly accurate position encoding. 



To mitigate the impact of object rotation, we propose to compute 3DV-RPE in a canonical object space where all objects are consistently rotated. Particularly, for each query, we predict a rotated 3D box and compute the relative offsets between the 3D points rotated in the same way, and the eight vertices of the box.
This results in consistent position encoding for different instances of the same object regardless of their positions or orientations in the space, greatly facilitating the learning of the locality property in cross-attention even from limited training data. Figure \ref{fig:attention_maps_intro} (c) visualizes the attention weights obtained by our method. We can see that the query for detecting the chair nicely attends to the points on the chair. Our experiment demonstrates that 3DV-RPE boosts the performance. 



We also systematically enhance our pipeline from various aspects such as data normalization and network architectures based on our understanding of the task. For example, we propose object-based normalization, instead of the scene-based one used by the DETR series, to parameterize the 3D boxes. This is because the former is more stable for point clouds which differs from 2D detection where the sizes of the same object in images can be very different depending on the camera parameters, impelling them to use image size to coarsely normalize the boxes. Besides, we also evaluate and adapt some of the recent advancement in 2D DETR.



We conduct thorough experiments to empirically show that our simple DETR-based approach significantly outperforms the previous state-of-the-art fully convolutional 3D detection methods,
which helps to accelerate the convergence of the detection head architecture design for 2D and 3D detection tasks.
We report the results of our approach on two challenging indoor 3D object detection benchmarks including ScanNetV and SUN RGB-D. Overall, compared to the DETR baseline~\cite{misra2021-3detr}, our method with 3DV-RPE improves / from / to /, respectively, and reduces the training epochs by 50\%.
Particularly, on ScanNetV, our approach outperforms the very recent state-of-the-art CAGroup3D~\cite{wang2022cagroup3d} by +/+ measured by /, respectively.


\section{Related work}

\vspace{1mm}
\noindent\textbf{DETR-based Object Detection.}
DETR~\cite{carion2020end} is a groundbreaking work that applies transformers~\cite{Vaswani2017attention} to D object detection, eliminating many hand-designed components such as non-maximum suppression~\cite{neubeck2006efficient} or anchor boxes~\cite{girshick2015fast,ren2015faster,lin2017focal,liu2016ssd}. Many extensions of DETR have been proposed~\cite{meng2021CondDETR,gao2021fast,dai2021dynamic,wang2021anchor,jia2022detrs,zhang2022dino}, such as Deformable-DETR~\cite{zhu2020deformable}, which uses multi-scale deformable attention to focus on key sampling points and improve performance on small objects. DAB-DETR~\cite{liu2022dab} introduces a novel query formulation to enhance detection accuracy. Some recent works~\cite{li2022dn,zhang2022dino,jia2022detrs,chen2022group} achieve state-of-the-art results on object detection by using query denoising or one-to-many matching schemes, which addressed the training inefficiency of one-to-one matching. -DETR~\cite{jia2022detrs} shows that one-to-many matching can also speed up convergence on 3D object detection tasks. Following the DETR-based approach, GroupFree~\cite{liu2021group} and 3DETR~\cite{misra2021-3detr} built strong 3D object detection systems for indoor scenes. However, they are still inferior to other methods such as CAGroup3D~\cite{wang2022cagroup3d}. In this work, we propose several critical modifications to improve the DETR-based methods and achieve new records on two indoor 3D object detection tasks.

\vspace{1mm}
\noindent\textbf{3D Indoor Object Detection.}
We revisit the existing indoor 3D object detection methods that directly use raw point clouds to detect 3D boxes. We categorize them into three types based on their strategies:
(i) \emph{Voting-based methods}, such as VoteNet~\cite{qi2019deep}, MLCVNet~\cite{xie2020mlcvnet} and H3DNet~\cite{zhang2020h3dnet}, use a voting mechanism to shift the surface points toward the object centers and group them into object candidates.
(ii) \emph{Expansion-based methods}, such as GSDN\cite{gwak2020generative}, FCAFD\cite{rukhovich2022fcaf3d}, and CAGroupD\cite{wang2022cagroup3d}, which generate virtual center features from surface features using a generative sparse decoder and predict high-quality 3D region proposals.
(iii) \emph{DETR-based methods},
unlike these two types that require modifying the original geometry structure of the input 3D point cloud, we adopt the DETR-based approach~\cite{liu2021group,misra2021-3detr} for its simplicity and generalization ability. Our experiments show that DETR has great potential for indoor 3D object detection.
We show the differences between above-mentioned methods in Figure~\ref{fig:intro_compare_approach}.

\vspace{1mm}
\noindent\textbf{3D Outdoor Object Detection.}
We briefly review some methods for outdoor 3D object detection~\cite{yan2018second,zhou2018voxelnet,lang2019pointpillars,yin2021center}, which mostly transform 3D points into a bird-eye-view plane and apply 2D object detection techniques. For example, VoxelNet~\cite{zhou2018voxelnet} is a single-stage and end-to-end network that combines feature extraction and bounding box prediction. PointPillars~\cite{lang2019pointpillars} uses a 2D convolution neural network to process the flattened pillar features from a Bird’s Eye View (BEV). CenterPoint~\cite{yin2021center} first detects centers of objects using a keypoint detector and regresses to other attributes, then refines them using additional point features on the object. However, these methods still suffer from center feature missing issues, which FSD~\cite{fan2023super} tries to address. We plan to extend our approach to outdoor 3D object detection in the future, which could unify indoor and outdoor 3D detection tasks.


\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{img/vDETR_framework.pdf}
\caption{\small{\textbf{Illustrating the overall framework of V-DETR for 3D object detection}. We first use an encoder to extract 3D features and then we use a plain Transformer decoder to estimate the 3D object queries from a set of initialized 3D object queries. In the Transformer decoder multi-head cross-attention layer, we use a 3D vertex relative position encoding scheme for both locality and accurate position modeling.}}
\label{fig:vdetr_pipeline}
\end{figure}


\section{Our Approach}

\subsection{Baseline setup}

\vspace{1mm}
\noindent\textbf{Pipeline.}
We build our V-DETR baseline following the previous DETR-based 3D object detection methods~\cite{misra2021-3detr,liu2021group}.
The detailed steps are as follows: given a 3D point cloud  sampled from a 3D scan of an indoor scene, where the RGB values are in the first  dimensions and the position XYZ values are in the last  dimensions.
We first sample about {}K points from the original point cloud that typically has around {}K points.
Second, we use a feature encoder to process the raw sampled points and compute the point features .
Third, we construct a set of 3D object queries  send them into a plain Transformer decoder to predict a set of 3D bounding boxes .
We set  by default.
Figure~\ref{fig:vdetr_pipeline} shows the overall pipeline.
We present more details on the encoder architecture design, the 3D object query construction, the Hungarian matching, and loss function formulations as follows.


\vspace{1mm}
\noindent\textbf{Encoder architecture.}
We choose two different kinds of encoder architecture for experiments including:
(i) a PointNet followed by a shallow Transformer encoder adopted by~\cite{misra2021-3detr} or (ii) a sparse 3D modification of ResNet followed by an FPN neck adopted by~\cite{rukhovich2022fcaf3d}, where we replace the expensive generative transposed convolution with a simple transposed convolution within the FPN neck.

\vspace{1mm}
\noindent\textbf{3D object query.}
We construct the 3D object query by combining two kinds of representations as follows: first, we simply sample a set of  initial center positions over the entire encoder output space and select their representations to initialize a set of 3D content query . Then we use their XYZ coordinates in the input point cloud space to compute the 3D position query  with a simple  consisting of two linear layers.
We build the 3D object query by adding the 3D position query to the 3D content query.



\vspace{1mm}
\noindent\textbf{Hungarian matching and loss function.}
We choose the weighted combination of six terms including the bounding box localization regression loss, angular classification and regression loss, and semantic classification loss as the final matching cost functions and training loss functions.
We illustrate the mathematical formulations
as follows:

where we use , , , ,  (or , , , , ) to represent the predicted (or ground-truth) bounding box, box center, box size, classification score, and rotation angle respectively, e.g.,
 represents the ground-truth semantic category of .
 represents angle classification cross entropy loss and  represents the residual continuous angle regression loss.
 represents semantic classification focal loss.
We ablate the influence of hyper-parameter value choices in the ablation experiments.


\vspace{1mm}
\noindent\textbf{Object-normalized box parameterization.}
We propose an object-normalized box reparameterization scheme that differs from the original DETR~\cite{carion2020end}, which normalizes the box predictions by the scene scales.
We account for one key discrepancy between object size variation in 2D images and 3D point clouds, e.g., a chair's 2D box size may change depending on its distance to the cameras, but its 3D box size should remain consistent as the point cloud captures the real 3D world.
In the implementation, we simply reparameterize the prediction target of width and height from the original groud-truth  and  to  and , where  and  represent the coarsely predicted box height and width.



\begin{figure}[t]
\centering
\includegraphics[width=0.25\textwidth]{img/rotated_rpe_v4.pdf}
\caption{\small{\textbf{Canonical object space transformation for 3DV-RPE.}
The rectangle represents the box of the object, which defines an object coordinate system. The green line represents the offset from a point to the box vertex. The offset transformed to the object coordinate system is  where the exact values can be geometrically reasoned. Since there is no rotation along the z-axis on the current datasets, we only show the changes in the x-y plane.}}
\label{fig:rotatedRPE}
\vspace{-2mm}
\end{figure}


\vspace{3mm}
\subsection{3DV-RPE in Canonical Object Space}

Position Encoding (PE) is crucial for enhancing the ability of transformers to comprehend the spatial context of the tokens. The appropriate PE strategy depends on tasks. For 3D object detection, where geometry features are the primary focus, it is essential for PE to encode rich semantic positions for the points, \eg whether they are on/off the 3D shapes of interest. 
\vspace{0.5em}

To that end, we present 3D Vertex Relative Position Encoding (3DV-RPE), a novel solution specifically tailored for 3D object detection within the DETR framework. We modify the global plain Transformer decoder multi-head cross-attention maps as follows:

where  and  represent the sparse query points and the dense key-value points, respectively.
 represents the position encoding computed by our 3DV-RPE that carries accurate position information.

\begin{figure}[t]
\centering
\centering
\includegraphics[width=0.48\textwidth]{img/vDETR-details-final-v5.pdf}
\caption{\small{\textbf{Illustration of the proposed V-DETR framework.} We mark the modifications with yellow-colored regions and the other components, that are designed following the plain DETR, with gray-colored regions. 
}}
\label{fig:pipeline_details}
\end{figure}

\paragraph{3DV-RPE.}
Our key insight is that, encoding a point by its relative position to the target object, which is coarsely represented by a box, is sufficient for 3D object detection. It is computed as follows:

where  denotes the offsets between the  points and the -th vertex of the  boxes and  represents the relative position bias term.  is the number of heads.  is a non-linear function. We will evaluate several alternatives for  in the experiments.  represents an MLP based transformation that first projects the features to a higher dimension space, and then to the output features of dimension .

We obtain the final relative position bias term by adding the bias term of the eight vertices, respectively:

where 
, encodes the relations between the 3D boxes and the points. In the subsequent section, we will introduce how we compute  with the aid of the boxes predicted at current layer.


\begin{figure*}[t]
\centering
\includegraphics[height=0.32\columnwidth, trim={80 50 80 50},clip]{img/vis_3drpe/dense_layer7_small/layer7_corners_0_soft.pdf}
\includegraphics[height=0.32\columnwidth, trim={80 50 80 50},clip]{img/vis_3drpe/dense_layer7_small/layer7_corners_1_soft.pdf}
\includegraphics[height=0.32\columnwidth, trim={80 50 80 50},clip]{img/vis_3drpe/dense_layer7_small/layer7_corners_2_soft.pdf}
\includegraphics[height=0.32\columnwidth, trim={80 50 80 50},clip]{img/vis_3drpe/dense_layer7_small/layer7_corners_3_soft.pdf}
\includegraphics[height=0.32\columnwidth, trim={80 50 80 50},clip]{img/vis_3drpe/dense_layer7_small/layer7_all_presoft_all.pdf}
\includegraphics[height=0.32\columnwidth, trim={80 50 80 50},clip]
{img/vis_3drpe/dense_layer7_small/layer7_corners_4_soft.pdf}
\includegraphics[height=0.32\columnwidth, trim={80 50 80 50},clip]{img/vis_3drpe/dense_layer7_small/layer7_corners_5_soft.pdf}
\includegraphics[height=0.32\columnwidth, trim={80 50 80 50},clip]{img/vis_3drpe/dense_layer7_small/layer7_corners_6_soft.pdf}
\includegraphics[height=0.32\columnwidth, trim={80 50 80 50},clip]{img/vis_3drpe/dense_layer7_small/layer7_corners_7_soft.pdf}
\includegraphics[height=0.32\columnwidth, trim={80 50 80 50},clip]{img/vis_3drpe/dense_layer7_small/layer7_all_presoft.pdf}
\caption{\small{\textbf{Visualizing the learned spatial attention maps based on 3DV-RPE.} We use the small red-colored cube to represent the 3D bounding box of an object, the red five-pointed star to mark the eight vertices, and the entire colored cube as the input scene for simplicity. We average each  along head dimension according to Equation~\ref{eq.rpe_transform} and visualize eight vertices' learned spatial cross-attention maps (from column\#1 to column\#4). We visualize the merged spatial attention maps in column\#5 (from the cutaway view). The color indicates the attention values: yellow for high and blue for low. We can observe that (i) the learned spatial attention maps of each vertex can enhance the regions along the internal direction starting from each vertex position, and (ii) the combined spatial attention maps can accurately enhance the internal regions inside the red-colored cubes.}}
\label{fig:attention_maps_rpe}
\vspace{-2mm}
\end{figure*}


\paragraph{Canonical Object Spaces.}
It is worth noting that the direction of the offsets are dependent on the definition of the world coordinate system and the object orientation which complicates the learning of semantic position encoding. To address the limitation, we propose to transform it to a object coordinate system defined by the rotated bounding box.  As illustrated in~\Cref{fig:rotatedRPE}, an offset vector in the world coordinate system can be transformed to the object coordinate system  following:

where  is an element of . We use the other transformations in Equation~\ref{eq.rpe_transform} and Equation~\ref{eq.rpe_add} to get the final normalized relative position bias item that models the rotated 3D bounding box position information. We perform 3DV-RPE operations for different Transformer decoder layers by default.


\paragraph{Efficient implementation.} 
A naive implementation has high GPU memory consumption due to the large number of combinations between the object queries (each object query predicts a 3D bounding box) and the key-value points (output by the encoder), i.e. , which makes it hard to train and deploy.

To solve this challenge, we use a smaller pre-defined 3DV-RPE table of shape: . We apply the non-linear projection  on this 3DV-RPE table and do volumetric (5-D)  on the transformed 3DV-RPE table as follows:



\vspace{3mm}
\subsection{DETR with 3DV-RPE}
\noindent\textbf{Framework.} We extend the original plain Transformer decoder, which consists of a stack of decoder layers and was designed for 2D object detection, to detect 3D bounding boxes from the irregular 3D points. Our approach has two steps: (i) as the first decoder layer has no access to coarse 3D bounding boxes, we employ a light-weight FFN to predict the initial 3D bounding boxes and feed the top confident ones to the first Transformer decoder layer (e.g., ); and (ii) we update the bounding box predictions with the output of each Transformer decoder layer and use them to compute the modulation term in the multi-head cross-attention.

Figure~\ref{fig:pipeline_details} illustrates more details of the DETR with our 3DV-RPE. For instance, we employ only the 3D content query  as the input for the first decoder layer and use the decoder output embeddings  from the -th decoder layer as the input for the -th decoder layer. We also apply MLP projects to compute the absolute position encodings of the 3D bounding boxes by default. We set the number of decoder layers as  following~\cite{misra2021-3detr}.
We predict the 3D bounding box delta target based on the initial prediction such as  in all the Transformer decoder layers.



\vspace{1mm}
\noindent\textbf{Visualization.}
Figure~\ref{fig:attention_maps_rpe} shows the relative position attention maps learned with the 3DV-RPE scheme. We show the attention maps for  vertices in the first  columns and the merged ones in the last column. The visualization results show that (i) our 3DV-RPE can enhance the inner 3D box regions relative to each vertex position and (ii) combining the eight relative position attention maps can accurately localize the regions within the bounding box. We also show that 3DV-RPE can localize the extremity positions on the 3D object surface in the experiments.


\section{Experiment}
\subsection{Datasets and metrics}
\noindent \textbf{Datasets.} We evaluate our approach on two challenging 3D indoor object detection benchmarks including:

\noindent \emph{ScanNetV2}~\cite{dai2017scannet}: ScanNetV2 consists of 3D meshes recovered from RGB-D videos captured in various indoor scenes. It has about K training meshes and  validation meshes, each annotated with semantic and instance segmentation masks for around  classes of objects. We follow~\cite{qi2019deep} to extract the point clouds from the meshes.

\noindent \emph{SUN RGB-D}~\cite{song2015sun}: SUN RGB-D is a single-view RGB-D image dataset. It has about K images for both training and validation sets. Each image is annotated with oriented 3D bounding boxes for  classes of objects. We follow VoteNet~\cite{qi2019deep} to convert the RGB-D image to the point clouds using the camera parameters and evaluate our approach on the  most common classes of objects.


\vspace{1mm}
\noindent \textbf{Metrics.} We report the standard mean Average Precision (mAP) under different IoU thresholds, \ie AP for  IoU threshold and AP for  IoU threshold. 

\renewcommand{\arraystretch}{1.45}
\begin{table}[!t]
\centering
\footnotesize
\setlength{\tabcolsep}{12pt}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{@{}l|cccc}
\multirow{2}{*}{Method} & \multicolumn{2}{c}{ScanNetV2} & \multicolumn{2}{c}{SUN RGB-D} \\ 
& AP & AP & AP & AP \\
\shline
VoteNet~\cite{qi2019deep} &  &  &  & - \\

HGNet~\cite{chen20hgnet} &  &  &  & - \\

3D-MPA~\cite{engelmann203dmpa} &  &  & - & - \\

MLCVNet~\cite{xie2020mlcvnet} &  &  &  & - \\

GSDN~\cite{gwak2020generative} &  &  & - & - \\

H3DNet~\cite{zhang2020h3dnet} &  &  &  &  \\

BRNet~\cite{cheng21brnet} &  &  &  &  \\

3DETR~\cite{misra2021-3detr} &  &  &  &  \\

VENet~\cite{xie21venet} &  & - &  &  \\

Group-Free~\cite{liu2021group} &  &  &  &  \\

RBGNet~\cite{wang22rbgnet} &  &  &  &  \\

HyperDet3D~\cite{zheng22hyperdet3d} &  &  &  &  \\

FCAF3D~\cite{rukhovich2022fcaf3d} &  &  &  &  \\

TR3D~\cite{rukhovich23tr3d} &  &  &  &  \\

CAGroup3D~\cite{wang2022cagroup3d} &  &  &  &  \\
\rowcolor{gray!10}V-DETR &  &  &  &  \\
\rowcolor{gray!10}V-DETR (TTA) &  &  &  &  \\

\shline
\multicolumn{5}{c}{\emph{Average Results under  trials}}     \\
\hline
Group-Free~\cite{liu2021group}  &  &  &  &  \\
RBGNet~\cite{wang22rbgnet} &  &  &  &  \\
FCAF3D~\cite{rukhovich2022fcaf3d}  &  &  &  &  \\
TR3D~\cite{rukhovich23tr3d} &  &  &  &  \\
CAGroup3D~\cite{wang2022cagroup3d} &  &  &  &  \\
\rowcolor{gray!10}V-DETR  &  &  &  &  \\
\rowcolor{gray!10}V-DETR (TTA) &  &  &  &  \\
\end{tabular}
}
\caption{\small{System-level comparison with the state-of-the-art on ScanNetV2 and SUN RGB-D. TTA: test-time augmentation.}
}
\vspace{-5mm}
\label{tab:sota_comparison}
\end{table}

\subsection{Implementation details}

\vspace{1mm}
\noindent \textbf{Training.} We use the AdamW optimizer~\cite{Loshchilov2019adamw} with the base learning rate e-, the batch size , and the weight decay . The learning rate is warmed up for  epochs, then is dropped to e- using the cosine schedule during the entire training process.
We use gradient clipping to stabilize the training. We train for  epochs on ScanNetV2 and  epochs on SUN RGB-D in all experiments except for the system-level comparisons, where we train for  epochs on ScanNetV2.
We use the standard data augmentations including random cropping (at least K points), random sampling (K points), random flipping (p=0.5), random rotation along the z-axis (-, ), random translation (-, ), random scaling (, ).
We also use the one-to-many matching~\cite{jia2022detrs} to speed up the convergence speed with more rich and informative positive samples.


\vspace{1mm}
\noindent \textbf{Inference.}
We process the entire point clouds of each scene and generate the bounding box proposals. We use 3D NMS to suppress the duplicated proposals in the one-to-many matching setting, which is not needed in the one-to-one matching setting. We also use test-time augmentation, i.e., flipping, by default unless specified otherwise.


\subsection{Comparisons with Previous Systems}
In Table~\ref{tab:sota_comparison}, we compare our method with the state-of-the-art methods from previous works at the system level.
These methods use different techniques, so we cannot compare them in a controlled way. According to the results, we show that our method performs the best either measured by the highest performance or the average results under multiple trials. For example, on ScanNetV2 \texttt{val} set, our method achieves AP= and AP=, which surpasses the latest state-of-the-art CAGroup3D that reports AP= and AP=.
Notably, on ScanNetV2, we observe more significant gains on AP (+) that requires more accurate localization, i.e., under a higher IoU threshold.
We also observe consistent gains on both AP and AP on SUN RGB-D.

\subsection{3DV-RPE Ablation Experiments}
We conduct all the following ablation experiments on ScanNetV2 except for the ablation experiments on the coordinate system, where we report the results on SUN RGB-D.
\vspace{1mm}

\begin{figure}[t]
\centering
\includegraphics[width=0.475\textwidth]{img/curve_nonlinear.pdf}
\caption{\small{{Illustrating the curve of signed log transform function.}}}
\label{fig:curve_nonlinear}
\end{figure}

\begin{table}[t]
\footnotesize
\renewcommand{\arraystretch}{1.2}
\centering
\begin{minipage}{1\linewidth}
{\begin{center}
\tablestyle{20pt}{1.35}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{l|cc}
   &  &  \\
    \shline
     =  & & \\
     =  & & \\
     =  & & \\
     =  & & \\
    \rowcolor{gray!10} =  &  &  \\  
\end{tabular}
}
\end{center}}
\end{minipage}
\caption{\small{Effect of non-linear transform within 3DV-RPE}.}
\label{tab:3dv_rpe_nonlinear}
\end{table}

\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{32pt}
\footnotesize
\renewcommand{\arraystretch}{1.35}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|cc}
\# vertex  & &  \\
\shline
 &  &      \\
 &  &      \\
 &  &      \\
\rowcolor{gray!10} &  &      \\
\end{tabular}
}
\caption{\small{{
Effect of the number of vertex within 3DV-RPE.}}
}
\label{tab:3dv_rpe_num_vertex}
\end{minipage}
\end{table}

\noindent \textbf{Non-linear transform.}
Table~\ref{tab:3dv_rpe_nonlinear} shows the effect of different non-linear transform functions. The results show that the signed log function performs the best. Figure~\ref{fig:curve_nonlinear} illustrates the curve of the signed log function and shows how it magnifies small changes in smaller ranges.
Therefore, we choose the signed log function by default.


\vspace{1mm}
\noindent \textbf{Number of vertex.}
Table~\ref{tab:3dv_rpe_num_vertex} shows the effect of different numbers of vertices for computing the relative position bias term. The results show that using  vertices performs the best, so we use this setting by default.
We attribute their close performances to the fact that they essentially share the same minimal and maximal XYZ values when using fewer vertices such as  or , which is caused by the zero rotation angles on ScanNetV2.

\vspace{1mm}
\noindent \textbf{Coordinate system on SUN RGB-D.}
We evaluate the effect of the coordinate system on calculating the relative positions in our 3DV-RPE on SUN RGB-D, which requires predicting the rotation angle along the -axis. Table~\ref{tab:3dv_rpe_rotation} shows the results. We find that transforming the relative offsets from the world coordinate system to the object coordinate system significantly improves the performance, e.g.,  and  increase by + and , respectively.


\vspace{1mm}
\noindent \textbf{Comparison with 3D box mask.}
Table~\ref{tab:effect_cross_attn} compares our 3DV-RPE with a 3D box mask method, which sets the relative position bias term to  for positions outside the 3D bounding box and  otherwise. The results show that (i) the 3D box mask method achieves strong results on , and (ii) our 3DV-RPE significantly improves over the 3D box mask method on . We speculate that our 3DV-RPE performs better because the 3D box mask method suffers from error accumulation from the previous decoder layers and cannot be optimized end-to-end.
We also report the results of combining the 3D box mask and 3DV-RPE, which performs better than the 3D box mask scheme but worse than our 3DV-RPE. This verifies that our 3DV-RPE can learn to (i) exploit more accurate geometric structure information within the 3D bounding box and (ii) benefit from capturing useful long-range context information outside the box. Moreover, we report the results with longer training epochs and observe that the gap between the 3D box mask and 3DV-RPE remains, thus further demonstrating the advantages of our approach.

\begin{table}[t]
\footnotesize
\renewcommand{\arraystretch}{1.1}
\centering
\begin{minipage}{1\linewidth}
{\begin{center}
\tablestyle{24pt}{1.3}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|cc}
coordinate system  &  &  \\
    \shline
   world coord.&  &  \\ 
   \rowcolor{gray!10}object coord.&  &  \\ 
\end{tabular}
}
\end{center}}
\end{minipage}
\caption{\small Effect of the coordinate system on SUN RGB-D.}
\label{tab:3dv_rpe_rotation}
\end{table}


\begin{table}[t]
\footnotesize
\renewcommand{\arraystretch}{1.2}
\centering
\begin{minipage}{1\linewidth}{\begin{center}
\tablestyle{5pt}{1.3}
\setlength{\tabcolsep}{12pt}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{l|ccc}
   attention modulation & \#epochs &   &  \\
    \shline
    None &  &  &  \\ 
    3D box mask  &  &  &  \\ 
    \rowcolor{gray!10}3DV-RPE  &  &  &  \\
    3D box mask + 3DV-RPE  &  &  & \\ \hline
    None  &  &  &  \\ 
    3D box mask   &  &  &  \\ 
    \rowcolor{gray!10}3DV-RPE  &  &  & \\
    3D box mask + 3DV-RPE  &  & & \\ 
\end{tabular}
}
\end{center}}
\end{minipage}
\caption{\small Effect of the attention modulation choices.}
\label{tab:effect_cross_attn}
\end{table}


\subsection{Other Ablation Experiments}
We study the effect of the other components in the following experiments.

\vspace{1mm}
\noindent \textbf{Encoder choice.}
Table~\ref{tab:effect_backbone} compares the results of using different encoder architectures. We find that using a sparse 3D version of ResNet with an FPN neck achieves the best results. Therefore, we use ResNet + FPN as our default encoder.
\begin{table}[t]
\footnotesize
\renewcommand{\arraystretch}{1.2}
\centering
\begin{minipage}{1\linewidth}
{\begin{center}
\tablestyle{24pt}{1.3}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{l|cc}
   encoder  &  &  \\
    \shline
   PointNet + Tran.Enc. &  &  \\ 
   \rowcolor{gray!10}ResNet + FPN &  &  \\ 
\end{tabular}
}
\end{center}}
\end{minipage}
\caption{\small Effect of the encoder choice.}
\label{tab:effect_backbone}
\end{table}


\begin{table}[t]
\footnotesize
\renewcommand{\arraystretch}{1.2}
\centering
\begin{minipage}{1\linewidth}
{\begin{center}
\tablestyle{25pt}{1.3}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|cc}
  object-normalize  &  &  \\
    \shline
  \xmark &&  \\ 
  \rowcolor{gray!10}\cmark &  &  \\ 
\end{tabular}
}
\end{center}}
\end{minipage}
\caption{\small Effect of the object-normalized box parameterization.}
\label{tab:effect_reparam}
\end{table}


\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{15pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{l|c|cc}
method & voxel expansion  & &  \\
\shline
\multirow{2}{*}{FCAF3D} & \xmark &  &          \\
& \cmark &  &  \\ \hline
\multirow{2}{*}{Ours} & \cellcolor{gray!10}\xmark & \cellcolor{gray!10} & \cellcolor{gray!10}         \\
& \cmark &  &         \\
\end{tabular}
}
\caption{\small{{
Effect of voxel expansion.}}
}
\label{tab:voxel_expan}
\end{minipage}
\end{table}


\begin{figure*}[t]
\centering
\includegraphics[height=0.28\columnwidth]{img/results/46-gt.png}
\hfill
\includegraphics[height=0.28\columnwidth]{img/results/57-gt.png}
\hfill
\includegraphics[height=0.28\columnwidth]{img/results/68_gt.png}
\hfill
\includegraphics[height=0.28\columnwidth]{img/results/72-gt.png} \\
\includegraphics[height=0.28\columnwidth]{img/results/46-pre.png}
\hfill
\includegraphics[height=0.28\columnwidth]{img/results/57-pre.png}
\hfill
\includegraphics[height=0.28\columnwidth]{img/results/68_re.png}
\hfill
\includegraphics[height=0.28\columnwidth]{img/results/72-pre.png}

\caption{\small{
Qualitative results of 3D object detection on ScanNetV2. The ground-truth is shown in the first row and our method's detection results are shown in the second row.
}}
\label{fig:qualitative}
\end{figure*}

\vspace{1mm}
\noindent \textbf{Object-normalized box parameterization.}
In Table~\ref{tab:effect_reparam}, we show the effect of using object-normalized box parameterization.
We find using the object-normalized scheme significantly boosts the  from  to .

\vspace{1mm}
\noindent \textbf{Voxel expansion.}
Table~\ref{tab:voxel_expan} evaluates the effect of using voxel expansion in the FPN neck when the encoder is ResNet + FPN. We also compare our results with the recent FCAF3D method. The results show that (i) voxel expansion is crucial for FCAF3D, which relies on building virtual center features; and (ii) voxel expansion degrades the performance when using DETR, which might lose the original accurate 3D surface information. Therefore, we demonstrate an important advantage of using DETR-based approaches, i.e., they do not require complicated voxel expansion operations.

\vspace{1mm}
\noindent \textbf{Qualitative comparisons.} 
We show some examples of V-DETR detection results on ScanNet in Figure~\ref{fig:qualitative}, where the scenes are diverse and challenging with clutter, partiality, scanning artifacts, etc.
Our V-DETR performs well despite these challenges. For example, it detects most of the chairs in the scene shown in the -st column.
Figure~\ref{fig:qualitative_sunrgbd} shows some examples of our prediction results on SUN RGB-D. Accordingly, we find our V-DETR can handle the rotated bounding boxes under various challenging rotation angles.
We include more qualitative comparisons in the supplementary material.


\noindent \emph{More ablation experiments.} 
We provide more ablation studies on the effects of using different shapes for the pre-defined 3DV-RPE table, one-to-many matching, the number of points in training and testing, and other factors in the supplementary material.


\begin{figure}[t]
\centering
\includegraphics[height=0.23\columnwidth, trim={50 0 70 0},clip]{img/results/sunrgbd/gt_2.jpg}
\includegraphics[height=0.23\columnwidth, trim={50 0 50 0},clip]{img/results/sunrgbd/gt_5.jpg}
\includegraphics[height=0.23\columnwidth, trim={50 0 50 0},clip]{img/results/sunrgbd/gt_3.jpg}
\hfill \\
\includegraphics[height=0.23\columnwidth,trim={50 0 70 0},clip]{img/results/sunrgbd/pre_2.jpg}
\includegraphics[height=0.23\columnwidth,trim={50 0 50 0},clip]{img/results/sunrgbd/pre_5.jpg}
\includegraphics[height=0.23\columnwidth,trim={50 0 50 0},clip]{img/results/sunrgbd/pre_3.jpg}
\caption{\small{
Qualitative results of 3D object detection on SUN RGB-D. The ground-truth is shown in the first row and our method's detection results are shown in the second row.
}}
\label{fig:qualitative_sunrgbd}
\end{figure}







\section{Conclusion}
In this work, we have shown how to make DETR-based approaches competitive for indoor 3D object detection tasks. The key contribution is an effective 3D vertex relative position encoding (3DV-RPE) scheme that can model the accurate position information in the irregular sparse 3D point cloud directly. We demonstrate the advantages of our approach by achieving strong results on two challenging 3D detection benchmarks. We also plan to extend our approach to outdoor 3D object detection tasks, which differ from most existing methods that rely on modern 2D DETR-based detectors by converting 3D points to a 2D bird-eye-view plane.
We hope our approach can show the potential for unifying the object detection architecture design for indoor and outdoor 3D detection tasks.


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\clearpage
\section{Supplementary}

\section*{A. More Ablation Experiments and Analysis}



\begin{table}[t]
\footnotesize
\renewcommand{\arraystretch}{1.2}
\centering
\begin{minipage}{1\linewidth}
{\begin{center}
\tablestyle{25pt}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|cc}
  Light-weight FFN  &  &  \\
    \shline
  \xmark &  &  \\ 
  \rowcolor{gray!10}\cmark &  &       \\ 
\end{tabular}
}
\end{center}}
\end{minipage}
\caption{\small Effect of light-weight FFN.}
\label{tab:ablate_ffn}
\end{table}



\begin{table}[t]
\footnotesize
\renewcommand{\arraystretch}{1.2}
\centering
\begin{minipage}{1\linewidth}
{\begin{center}
\tablestyle{30pt}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|cc}
  \# of points  &  &  \\
    \shline
  K &  &  \\
  K &  &  \\ 
  \rowcolor{gray!10}K &  &  \\ 
\end{tabular}
}
\end{center}}
\end{minipage}
\caption{\small Effect of using more points during training and evaluation.}
\label{tab:ablate_point_num}
\end{table}

\begin{table}[t]
\footnotesize
\renewcommand{\arraystretch}{1.2}
\centering
\begin{minipage}{1\linewidth}
{\begin{center}
\tablestyle{10pt}{1.1}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{ccc|cc}
   \# points & \#query & \#repeat number &  &  \\
    \shline
 \multirow{3}{*}{K} &
    &  &  &  \\
   &  &  &  &  \\
   &  &  &  & 
    \\ \hline
   \multirow{3}{*}{K} &  &  &  &  \\
   &  &  &  &  \\
   & \cellcolor{gray!10} & \cellcolor{gray!10} & \cellcolor{gray!10} & \cellcolor{gray!10}          
\end{tabular}
}
\end{center}}
\end{minipage}
\caption{\small Effect of the one-to-many matching.}
\label{tab:effect_one2many}
\end{table}


\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{2mm}
\centering
\setlength{\tabcolsep}{23pt}
\footnotesize
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{c|cc}
3DV-RPE table shape  & &  \\
\shline
 &  &          \\
\rowcolor{gray!10} &  &         \\
 &  &          \\
 &  &         \\
\end{tabular}
}
\caption{\small{{
Effect of the pre-defined 3DV-RPE table shape.}}
}
\label{tab:ablate_shape}
\end{minipage}
\end{table}
\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\centering
\setlength{\tabcolsep}{1pt}
\footnotesize
\tablestyle{12pt}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{l|c|c}
method &  &  \\
\shline
Baseline (w/o RPE) &  &  \\
Baseline + CRPE (Stratrified Transformer)  &   &  \\
Baseline + CRPE (EQNet) &   &  \\
Baseline + Cond-CA &  &  \\
Baseline + DAB-CA  &   &  \\
 \rowcolor{gray!10}Baseline + 3DV-RPE  &  &  \\
\end{tabular}
}
\caption{\small
Comparison to other attention modulation methods.
We only change the decoder cross-attention scheme and keep all other settings the same for comparison fairness.
}
\label{tab:compare_crpe}
\end{minipage}
\end{table}



\begin{table}[t]
\begin{minipage}[t]{1\linewidth}
\vspace{-2mm}
\centering
\setlength{\tabcolsep}{5pt}
\footnotesize
\tablestyle{2pt}{1.2}
\resizebox{1.0\linewidth}{!}
{
\begin{tabular}{l|c|c|c|c|c}
method &  Scenes/second & Latency/scene & GPU Memory &  &  \\
\shline
FCAF3D &  & ms & M &   &    \\
CAGroup3D &  & ms & M &  &    \\
Ours (light) &   & ms & M &  &    \\
Ours &  & ms & M &  &    \\
\end{tabular}
}
\caption{\small
Inference cost comparison. We evaluate all numbers on a Tesla V100 PCIe 16 GB GPU with batch size as  for a fair comparison.
}
\label{tab:compare_inference}
\vspace{1mm}
\end{minipage}
\end{table}



\vspace{1mm}
\noindent \textbf{Light-weight FFN.}
Table~\ref{tab:ablate_ffn} reports the comparison results on the effect of proposed light FFN.
According to the results, we observe that using the light-weight FFN significantly boosts the  from  to , thus showing the advantages of using a set of adaptive predicted initial 3D bounding boxes over a set of pre-defined 3D bounding boxes of the same size.

\vspace{1mm}
\noindent \textbf{Number of points during training and evaluation.}
In Table~\ref{tab:ablate_point_num}, we report the comparison results when using different number of points during training. We observe that using K points achieves consistently better performance, thus we choose K points.


\vspace{1mm}
\noindent \textbf{One-to-many matching.}
Table~\ref{tab:effect_one2many} shows the comparison results when choosing different hyper-parameters for a one-to-many matching scheme.
For example, we find increasing the number of queries and the number of ground truth repeating times even hurts the performance when training with K points but improves the performance when training with K.


\vspace{1mm}
\noindent \textbf{Table shape.}
In Table~\ref{tab:ablate_shape}, we show the effect of different shapes for the pre-defined 3DV-RPE table. We find that  achieves the best results. Our approach is less sensitive to the shape of the 3DV-RPE table thanks to the signed log function, which improves the interpolation quality to some degree.

\vspace{1mm}
\noindent \textbf{Comparison with other attention modulation methods.}
We summarize the comparison results with other advanced related methods including contextual relative position encoding (CRPE)~\cite{lai2022stratified,yang2022unified}, conditional cross-attention (Cond-CA)~\cite{meng2021CondDETR}, dynamic anchor box cross-attention (DAB-CA)~\cite{liu2022dab} in Table~\ref{tab:compare_crpe}.
We report the comparison results under the most strong settings, i.e.,  training epochs.
Accordingly, we see that (i) both CRPE (Stratified Transformer~\cite{lai2022stratified}) and CRPE (EQNet~\cite{yang2022unified}) consistently improve the baseline; (ii) our 3DV-RPE achieves the best performance.
The reason is that the CRPE methods of Stratified-Transformer~\cite{lai2022stratified} and EQNet~\cite{yang2022unified} only consider the center point of the 3D box while our 3DV-RPE explicitly considers the  vertex points and rotated angle of the 3D box. Our method encodes the box size and the six faces, thus modeling the accurate position relations between all other points and the 3D bounding box (supported by the much larger gains on ).


\vspace{1mm}
\noindent \textbf{Inference complexity comparison.}
Table~\ref{tab:compare_inference} reports the comparison results to FCAFD and CAGroup3D. Accordingly, our method achieves a better performance-efficiency trade-off than CAGroup3D.
We also provide a light version by decreasing the number of 3D object query from  to .
Notably, the reported latency of CAGroup3D is close to the numbers in their \href{https://github.com/Haiyang-W/CAGroup3D#main-results}{official logs} but different from the numbers reported in the paper (179.3ms tested on RTX 3090 GPU). The authors of CAGroup3D have acknowledged this \href{https://github.com/Haiyang-W/CAGroup3D#todo}{issue} in their GitHub repository.



\section*{B. More Qualitative Results and Analysis}

We show more qualitative examples of our V-DETR detection on ScanNet and SUN RGB-D in Figure~\ref{fig:vis_scannet} and Figure~\ref{fig:vis_sunrgbd}, respectively.
We can observe that our method can find most of the target objects in various scenes.

Figure~\ref{fig:3dvrpe_visual_exp} shows the spatial cross-attention maps of our 3DV-RPE on three ScanNetV2 scenes. We see that (i) our 3DV-RPE can find the 3D bounding boxes accurately and (ii) each vertex's RPE can enhance the regions inside the boxes from that vertex.


\begin{figure*}[t]
\centering
\includegraphics[height=0.5\columnwidth]{img/results/new_scannet/1gt.jpg}
\hspace{5pt}
\includegraphics[height=0.5\columnwidth]{img/results/new_scannet/1pre.jpg} \\
\includegraphics[height=0.5\columnwidth]{img/results/new_scannet/2gt.jpg}
\hspace{5pt}
\includegraphics[height=0.5\columnwidth]{img/results/new_scannet/2pre.jpg} \\
\includegraphics[height=0.5\columnwidth]{img/results/new_scannet/3gt.jpg}
\hspace{5pt}
\includegraphics[height=0.5\columnwidth]{img/results/new_scannet/3pre.jpg} \\
\includegraphics[height=0.5\columnwidth]{img/results/new_scannet/4gt.jpg}
\hspace{5pt}
\includegraphics[height=0.5\columnwidth]{img/results/new_scannet/4pre.jpg} \\
\includegraphics[height=0.5\columnwidth]{img/results/new_scannet/5gt.jpg}
\hspace{5pt}
\includegraphics[height=0.5\columnwidth]{img/results/new_scannet/5pre.jpg} \\
\caption{\small{
More qualitative results of 3D object detection on ScanNetV2. The ground-truth is shown in the first column and our method's detection results are shown in the second column.
}}
\label{fig:vis_scannet}
\end{figure*}
\begin{figure*}[t]
\centering
\includegraphics[height=0.5\columnwidth]{img/results/new_sunrgbd/1gt.jpg}
\hspace{5pt}
\includegraphics[height=0.5\columnwidth]{img/results/new_sunrgbd/1pre.jpg} \\
\includegraphics[height=0.5\columnwidth]{img/results/new_sunrgbd/2gt.jpg}
\hspace{5pt}
\includegraphics[height=0.5\columnwidth]{img/results/new_sunrgbd/2pre.jpg} \\
\includegraphics[height=0.5\columnwidth]{img/results/new_sunrgbd/3gt.jpg}
\hspace{5pt}
\includegraphics[height=0.5\columnwidth]{img/results/new_sunrgbd/3pre.jpg} \\
\includegraphics[height=0.5\columnwidth]{img/results/new_sunrgbd/4gt.jpg}
\hspace{5pt}
\includegraphics[height=0.5\columnwidth]{img/results/new_sunrgbd/4pre.jpg} \\

\caption{\small{
More qualitative results of 3D object detection on SUN RGB-D. The ground truth is shown in the first column and our method's detection results are shown in the second column.
}}
\label{fig:vis_sunrgbd}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_1/pc.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_1/corner0.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_1/corner2.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_1/corner4.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_1/corner6.jpg}
\hfill
\\
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_1/cornerallnew.jpg} 
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_1/corner1.jpg}  
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_1/corner3.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_1/corner5.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_1/corner7.jpg}  \\
\vspace{10mm}

\includegraphics[height=0.26\columnwidth]{img/vis_3drpe/real_pc/example_0/pc.jpg}
\hfill
\includegraphics[height=0.26\columnwidth]{img/vis_3drpe/real_pc/example_0/corner0.jpg}
\hfill
\includegraphics[height=0.26\columnwidth]{img/vis_3drpe/real_pc/example_0/corner2.jpg}
\hfill
\includegraphics[height=0.26\columnwidth]{img/vis_3drpe/real_pc/example_0/corner4.jpg}
\hfill
\includegraphics[height=0.26\columnwidth]{img/vis_3drpe/real_pc/example_0/corner6.jpg}
\hfill
\\
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_0/cornerallnew2.jpg} 
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_0/corner1.jpg}  
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_0/corner3.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_0/corner5.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_0/corner7.jpg}  \\
\vspace{10mm}
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_2/pc.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_2/corner0.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_2/corner2.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_2/corner4.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_2/corner6.jpg}
\hfill
\\
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_2/cornerallnew2.jpg} 
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_2/corner1.jpg}  
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_2/corner3.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_2/corner5.jpg}
\hfill
\includegraphics[height=0.25\columnwidth]{img/vis_3drpe/real_pc/example_2/corner7.jpg}  \\
\vspace{5mm}
\caption{\small{
\textbf{Illustration of the spatial attention maps learned by our 3DV-RPE on ScanNetV2 scenes.} Each scene consists of two rows. We draw a green cube to mark the detected 3D bounding box and a red star at its eight vertices. We average the head dimension of each  and show the spatial cross-attention maps for eight vertices (columns -). Column  shows the input scene and the merged attention maps. The color shows the attention values: yellow is high and blue is low. We see that (i) each vertex's attention map highlights the regions inside the cube from that vertex, and (ii) the combined attention maps focus on the regions inside the red cubes.}}
\label{fig:3dvrpe_visual_exp}
\end{figure*}
\end{document}