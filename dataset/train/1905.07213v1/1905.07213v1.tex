\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage[sort, numbers]{natbib}

\bibliographystyle{apa}


\pagestyle{fancy}
\fancyhf{}
\lhead{Submitted to Dialogue 2019 conference}

\begin{document}

\begin{center}
  \LARGE Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language
\end{center}
\vskip 2em
\begin{center}
  \large \lineskip .75em\begin{tabular}[t]{c}
    Yuri Kuratov \textsuperscript{1} \and Mikhail Arkhipov \textsuperscript{1} \\ \\
 \and
  \textsuperscript{1}  Neural Networks and Deep Learning Lab, \\
  Moscow Institute of Physics and Technology \\
  \and
  {\tt\small yurii.kuratov@phystech.edu~~~arkhipov.mu@mipt.ru}
  \end{tabular}\par
\end{center}

\renewcommand{\abstractname}{Abstract}
\begin{abstract}
The paper introduces methods of adaptation of multilingual masked language models for a specific language. Pre-trained bidirectional language models show state-of-the-art performance on a wide range of tasks including reading comprehension, natural language inference, and sentiment analysis. At the moment there are two alternative approaches to train such models: monolingual and multilingual. While language specific models show superior performance, multilingual models allow to perform a transfer from one language to another and solve tasks for different languages simultaneously.
This work shows that transfer learning from a multilingual model to monolingual model results in significant growth of performance on such tasks as reading comprehension, paraphrase detection, and sentiment analysis. Furthermore, multilingual initialization of monolingual model substantially reduces training time. Pre-trained models for the Russian language are open sourced. 


\end{abstract} 

\selectlanguage{english}
\section{Introduction}

A large amount of work is devoted to unsupervised pre-training of neural networks on a variety of Natural Language Processing (NLP) tasks. Unsupervised pre-training shows significant improvements in almost every NLP task \cite{mikolov2013w2v, dai2015semi, peters2018elmo, devlin2018bert}. 

At the moment one of the best performing models for unsupervised pre-training is BERT \cite{devlin2018bert}. This model is based on Transformer \cite{vaswani2017attention} architecture and trained on a large number of unlabeled texts from Wikipedia to solve Masked Language Modelling task. It shows state-of-the-art results on a wide range of NLP tasks in English. 

Currently, there are publicly available two monolingual English and Chinese models and a single multilingual model. It was previously reported that monolingual models performance is significantly better than multilingual ones\footnote{\url{https://github.com/google-research/bert/blob/master/multilingual.md\#results}}. 

In the present work, we consider the possibility of multilingual to monolingual transfer. We use Russian as a target language for transfer. We show that it is possible to train the monolingual model using multilingual initialization.

To show this, we evaluated the multilingual model on a number of common NLP tasks from the target language. The model trained in a monolingual setting achieves substantially better performance compared to the multilingual model.

\section{Model Architecture}

In the present work, we use BERT \cite{devlin2018bert} model for all our experiments. The model is a Transformer \cite{vaswani2017attention} encoder. The basic building blocks of the model is Self-Attention. The model was trained on the Masked Language Modelling and next sentence prediction tasks. We refer readers to check BERT original paper for details about the model\cite{devlin2018bert}. 

We used 12 layers (Transformer blocks) version of BERT with self-attention hidden size 768, feed-forward hidden size 3072, and 12 self-attention heads. This setting corresponds to BERT\textsubscript{BASE} model from \cite{devlin2018bert}. Task-specific layers were trained according to the BERT paper.

\section{Language transfer}

In this work, we consider the transfer of multilingual BERT model to monolingual. Authors of \cite{devlin2018bert} showed that monolingual models show superior performance compared to multilingual one. 
Furthermore, the BERT model uses the subword segmentation algorithm\cite{sennrich2015subwordnmt} to cope with large vocabulary problem. Multilingual models use only a small part of the entire vocabulary for a single language. It results in much longer sequences after tokenization compared to the monolingual model. Since the Transformer model has quadratic computational complexity in terms of input sequence length, it is highly undesirable.

We investigated the possibility of using the multilingual model as initialization for the monolingual model. The basic idea is to use knowledge about target language that already captured during multilingual training. It also is known that training model using data from multiple languages can significantly improve the performance of the model \cite{mulcaire2018polyglot}.
We used the multilingual model from BERT repository\footnote{https://github.com/google-research/bert}.
This model was trained on one hundred languages with largest Wikipedias.
The target language is Russian. All parameters of the model except word embeddings were initialized from the multilingual model \cite{mulcaire2018polyglot}. 

The new subword vocabulary was obtained using subword-nmt\footnote{https://github.com/rsennrich/subword-nmt}.
Training of the subword vocabulary was performed on the Russian part of Wikipedia and news data.
The part of Wikipedia data was around 80\%. The result of this step is a new monolingual Russian subword vocabulary. This vocabulary contains longer Russian words and subwords compared to multilingual one.  

New word embeddings matrix was obtained by assembling monolingual embeddings from multilingual. Namely, embeddings of all tokens from the intersection of multilingual and monolingual vocabulary were left without any changes. The same for special tokens like [UNK] or [CLS]. We replaced all tokens from outside the intersection with tokens from the monolingual vocabulary. These tokens are mostly longer subword units which are combinations of shorter units present in the intersection. New tokens are initialized with the mean value of embeddings from the intersection. For example, there are tokens 'bi' and '\#\#rd' in the intersection of vocabularies, where '\#\#' stands for the continuation of the word. There is also a token 'bird' present in the monolingual vocabulary and absent in the multilingual vocabulary. The embedding of 'bird' is initialized as the mean value of the embeddings 'bi' and '\#\#rd'.

The model with reassembled vocabulary and embeddings matrix was trained on the same data that was used for building of the monolingual vocabulary. The following hyperparameters were used for training:
\begin{itemize}
    \item batch size: 256
    \item learning rate: 
    \item optimizer: Adam
    \item L2 regularization: 
\end{itemize}

The monolingual Russian model\footnote{\url{http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v1.tar.gz}} is available as a part of the \href{https://github.com/deepmipt/deeppavlov/}{DeepPavlov library}\footnote{https://github.com/deepmipt/deeppavlov/}.

\section{Tasks description}
We have chosen three tasks to evaluate our approach: paraphrase identification, sentiment analysis, and question answering. We briefly describe them in this section.

    \subsection{Paraphrase Identification with ParaPhraser}
    ParaPhraser \cite{pivovarova2017paraphraser} is a dataset for paraphrase detection in Russian language. Two sentences are paraphrases if they have the same meaning. This dataset consists of 7227/1924 train/test pairs of sentences which are labeled as precise paraphrases, near paraphrases or non-paraphrases. One approach for paraphrase identification is a binary classification: first class is precise and near paraphrases, second class - non-paraphrases.
    
    \subsection{Sentiment Analysis with RuSentiment}
    RuSentiment \cite{rogers2018rusentiment} is a dataset for sentiment analysis of posts from VKontakte (VK), the most popular social network in Russia. Realised in 2018, it became one of the largest sentiment datasets for Russian language with 30521 posts. Each post is labeled with one of the five classes. The informal language presented in RuSentiment dataset makes it more challenging for our model, trained on Wikipedia and news articles.
    
    \subsection{Question answering with SDSJ Task B}
    As part of Sberbank Data Science Journey\footnote{https://sdsj.sberbank.ai/} 2017 was held a competition with two tasks. Task B was inspired by Stanford Question Answering Dataset (SQuAD) \cite{rajpurkar2016squad}. Organizers collected about 50,000 (train and development set) questions and contexts, where the answer is always a span from corresponding context. SQuAD dataset encouraged community to develop sophisticated and effective neural architectures such as BiDAF \cite{seo2016bidirectional}, R-NET \cite{wang2017gated}, Mnemonic Reader \cite{hu2017reinforced}. All this models are based on attention mechanisms \cite{bahdanau2014neural,luong2015effective} and building joint context-question representation.

\section{Results}
We evaluated BERT multilingual model and BERT trained with our approach (RuBERT) on three tasks: paraphrase identification, sentiment analysis, and question answering. All reported results were obtained by averaging across 5 runs.

\begin{table}[!ht]
\centering
\begin{tabular}{l|l|l}
\hline
model                            & F-1   & Accuracy \\ \hline
Neural networks \cite{pivovarova2017paraphraser}  & 79.82 & 76.65    \\ \hline
Classifier + linguistic features \cite{pivovarova2017paraphraser} & 81.10 & 77.39    \\ \hline
Machine Translation + Semantic similarity \cite{kravchenko2017paraphrase} & 78.51 & 81.41    \\ \hline\hline
BERT multilingual                &  &     \\ \hline
RuBERT                           &  &     \\ \hline
\end{tabular}
\caption{ParaPhraser. We compare BERT based models with models in non-standard run setting, when all resources were allowed.}
\label{tab:paraphraser}
\end{table}

\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l|l}
\hline
model               & F-1   & Precision & Recall \\ \hline
Logistic Regression \cite{rogers2018rusentiment} & 68.84 & 69.53     & 69.46  \\ \hline
Linear SVC \cite{rogers2018rusentiment}         & 68.56 & 69.46     & 69.25  \\ \hline
Gradient Boosting \cite{rogers2018rusentiment}   & 68.48 & 69.63     & 69.19  \\ \hline
NN classifier \cite{rogers2018rusentiment}       & 71.64 & 71.99     & 72.15  \\ \hline \hline
BERT multilingual   &  & -         & -      \\ \hline
RuBERT              &  & -         & -      \\ \hline
\end{tabular}
\caption{RuSentiment. We used only randomly selected posts (21,268) subset for training.}
\label{tab:rusentiment}
\end{table}

\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l}
\hline
model                 & F-1 (dev) & EM (dev) \\ \hline
R-Net from DeepPavlov \cite{burtsev2018deeppavlov} & 80.04     & 60.62    \\ \hline \hline
BERT multilingual     &      &     \\ \hline
RuBERT                &      &     \\ \hline
\end{tabular}
\caption{Results on question answering with SDSJ Task B. Models performance was evaluated on development set (public leaderboard subset).}
\label{tab:squad}
\end{table}

SDSJ Task B and ParaPhraser datasets share the same domain with data, which we used for training RuBERT. The RuSentiment dataset is based on posts from a social network and shows to be more challenging for RuBERT. As result, we can see only 1 F-1 point improvement from previous state of the art for RuSentiment in Table \ref{tab:rusentiment}, comparing to 4-6 F-1 points improvement on SDSJ Task B and ParaPhraser (results in Table \ref{tab:paraphraser} and Table \ref{tab:squad}).

\subsection{Vocabulary comparison}
BERT multilingual and RuBERT have the same size of vocabulary (about 120k subtokens), but RuBERT vocabulary was built especially for Russian language. Figure \ref{fig:vocab_hist} shows that RuBERT model allows to reduce mean sequence length in  times in subtokens, what makes possible to increase batch size or feed longer texts to the model, comparing to BERT multilingual.

\begin{figure}[ht]\centering
    \subfloat[BERT multilingual]{{\includegraphics[width=5cm]{bert_multilingual_vocab.png} }}\qquad
    \subfloat[RuBERT]{{\includegraphics[width=5cm]{RuBERT_vocab.png}}}\caption{Distribution of lengths in subtokens of contexts with their questions (SDSJ Task B dataset). Red vertical lines represent mean values.}\label{fig:vocab_hist}\end{figure}

\subsection{Training dynamics}
In this section we compare training BERT model for Russian language from scratch (random initialization) and initialized with BERT multilingual. Figure \ref{fig:loss} shows that BERT multilingual initialization helps to converge faster: about 800 thousand steps is required for random initialized model to get the same loss as at 250 thousand step of multilingual initialization. It takes about two days to train for 250 thousand steps (on Tesla P100 x 8), so it helped us to save six days of computational time. Proposed averaging of new subtokens in vocabulary also has positive effect on the rate of convergence (instead of averaging we could take random initialization for new subtokens).

\begin{figure}[ht!]
\centering
\includegraphics[width=9cm]{loss_new.png}
\caption{Models training dynamics to get to the same value of loss.}
\label{fig:loss}
\end{figure}

\section{Conclusion}

In this work, we have shown that Transformer network pre-trained on the multilingual Masked Language Modelling task significantly improves performance on a number of Russian NLP tasks compared to existing solutions. Furthermore, language-specific unsupervised training with multilingual initialization results in even better improvements. Pre-trained models for the Russian language are open sourced, as well as code to reproduce our results as part of DeepPavlov library.

\section{Acknowledgments}
This work was supported by National Technology Initiative and PAO Sberbank project ID 0000000007417F630002.

\begin{thebibliography}{}

\bibitem[\protect\astroncite{Bahdanau et~al.}{2014}]{bahdanau2014neural}
Bahdanau, D., Cho, K., and Bengio, Y. (2014).
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473}.

\bibitem[\protect\astroncite{Burtsev et~al.}{2018}]{burtsev2018deeppavlov}
Burtsev, M., Seliverstov, A., Airapetyan, R., Arkhipov, M., Baymurzina, D.,
  Bushkov, N., Gureenkova, O., Khakhulin, T., Kuratov, Y., Kuznetsov, D.,
  et~al. (2018).
\newblock Deeppavlov: Open-source library for dialogue systems.
\newblock {\em Proceedings of ACL 2018, System Demonstrations}, pages 122--127.

\bibitem[\protect\astroncite{Dai and Le}{2015}]{dai2015semi}
Dai, A.~M. and Le, Q.~V. (2015).
\newblock Semi-supervised sequence learning.
\newblock In {\em Advances in neural information processing systems}, pages
  3079--3087.

\bibitem[\protect\astroncite{Devlin et~al.}{2018}]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018).
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}.

\bibitem[\protect\astroncite{Hu et~al.}{2017}]{hu2017reinforced}
Hu, M., Peng, Y., Huang, Z., Qiu, X., Wei, F., and Zhou, M. (2017).
\newblock Reinforced mnemonic reader for machine reading comprehension.
\newblock {\em arXiv preprint arXiv:1705.02798}.

\bibitem[\protect\astroncite{Kravchenko}{2017}]{kravchenko2017paraphrase}
Kravchenko, D. (2017).
\newblock Paraphrase detection using machine translation and textual similarity
  algorithms.
\newblock In {\em Conference on Artificial Intelligence and Natural Language},
  pages 277--292. Springer.

\bibitem[\protect\astroncite{Luong et~al.}{2015}]{luong2015effective}
Luong, M.-T., Pham, H., and Manning, C.~D. (2015).
\newblock Effective approaches to attention-based neural machine translation.
\newblock {\em arXiv preprint arXiv:1508.04025}.

\bibitem[\protect\astroncite{Mikolov et~al.}{2013}]{mikolov2013w2v}
Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013).
\newblock Efficient estimation of word representations in vector space.
\newblock {\em arXiv preprint arXiv:1301.3781}.

\bibitem[\protect\astroncite{Mulcaire et~al.}{2018}]{mulcaire2018polyglot}
Mulcaire, P., Swayamdipta, S., and Smith, N. (2018).
\newblock Polyglot semantic role labeling.
\newblock {\em arXiv preprint arXiv:1805.11598}.

\bibitem[\protect\astroncite{Peters et~al.}{2018}]{peters2018elmo}
Peters, M.~E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and
  Zettlemoyer, L. (2018).
\newblock Deep contextualized word representations.
\newblock In {\em Proc. of NAACL}.

\bibitem[\protect\astroncite{Pivovarova
  et~al.}{2017}]{pivovarova2017paraphraser}
Pivovarova, L., Pronoza, E., Yagunova, E., and Pronoza, A. (2017).
\newblock Paraphraser: Russian paraphrase corpus and shared task.
\newblock In {\em Conference on Artificial Intelligence and Natural Language},
  pages 211--225. Springer.

\bibitem[\protect\astroncite{Rajpurkar et~al.}{2016}]{rajpurkar2016squad}
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016).
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock {\em arXiv preprint arXiv:1606.05250}.

\bibitem[\protect\astroncite{Rogers et~al.}{2018}]{rogers2018rusentiment}
Rogers, A., Romanov, A., Rumshisky, A., Volkova, S., Gronas, M., and Gribov, A.
  (2018).
\newblock Rusentiment: An enriched sentiment analysis dataset for social media
  in russian.
\newblock In {\em Proceedings of the 27th International Conference on
  Computational Linguistics}, pages 755--763.

\bibitem[\protect\astroncite{Sennrich et~al.}{2015}]{sennrich2015subwordnmt}
Sennrich, R., Haddow, B., and Birch, A. (2015).
\newblock Neural machine translation of rare words with subword units.
\newblock {\em arXiv preprint arXiv:1508.07909}.

\bibitem[\protect\astroncite{Seo et~al.}{2016}]{seo2016bidirectional}
Seo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H. (2016).
\newblock Bidirectional attention flow for machine comprehension.
\newblock {\em arXiv preprint arXiv:1611.01603}.

\bibitem[\protect\astroncite{Vaswani et~al.}{2017}]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5998--6008.

\bibitem[\protect\astroncite{Wang et~al.}{2017}]{wang2017gated}
Wang, W., Yang, N., Wei, F., Chang, B., and Zhou, M. (2017).
\newblock Gated self-matching networks for reading comprehension and question
  answering.
\newblock In {\em Proceedings of the 55th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, volume~1, pages 189--198.

\end{thebibliography}
 
\end{document}
