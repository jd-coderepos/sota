\begin{table*}[ht!]
    \small
    \centering
    \begin{tabular}[t]{l|l|c c | c c c c c c c c}
         method & training data & $AP$ & $AP_{50}$ & person & rider & car & truck & bus & train & mcycle & bicycle \\
         \hline
         DIN~\cite{arnab2017pixelwise}          & \texttt{fine\,+\,coarse}  & 23.4 & 45.2 & 20.9 & 18.4 & 31.7 & 22.8 & 31.1 & 31.0 & 19.6 & 11.7 \\ 
         SGN~\cite{liu2017sgn}                  & \texttt{fine\,+\,coarse}  & 25.0 & 44.9 & 21.8 & 20.1 & 39.4 & 24.8 & 33.2 & 30.8 & 17.7 & 12.4 \\
         PolygonRNN++~\cite{acuna2018efficient} & \texttt{fine}             & 25.5 & 45.5 & 29.4 & 21.8 & 48.3 & 21.1 & 32.3 & 23.7 & 13.6 & 13.6 \\ 
         Mask R-CNN~\cite{he2017mask}           & \texttt{fine}             & 26.2 & 49.9 & 30.5 & 23.7 & 46.9 & 22.8 & 32.2 & 18.6 & 19.1 & 16.0 \\
         GMIS~\cite{liu2018affinity}            & \texttt{fine\,+\,coarse}  & 27.6 & 44.6 & 29.3 & 24.1 & 42.7 & 25.4 & 37.2 & 32.9 & 17.6 & 11.9 \\
         PANet~\cite{liu2018path}               & \texttt{fine}             & 31.8 & 57.1 & 36.8 & 30.4 & 54.8 & 27.0 & 36.3 & 25.5 & 22.6 & 20.8 \\
         Mask R-CNN~\cite{he2017mask}           & \texttt{fine\,+\,COCO}    & 31.9 & 58.1 & 34.8 & 27.0 & 49.1 & 30.1 & 40.9 & 30.9 & 24.1 & 18.7 \\   
         PANet~\cite{liu2018path}               & \texttt{fine\,+\,COCO}    & 36.4 & 63.1 & 41.5 & 33.6 & 58.2 & 31.8 & 45.3 & 28.7 & 28.2 & 24.1 \\
         \hline
         ours           & \texttt{fine}             & 27.6 & 50.9 & 34.5 & 26.1 & 52.4 & 21.7 & 31.2 & 16.4 & 20.1 & 18.9 \\
    \end{tabular}
    \vspace{3mm}
    \caption{Results on Cityscapes $test$ set. With a score of 27.6 AP we reach second place on the benchmark, compared with the \texttt{fine}-only methods.}
    \label{tab:results_cityscapes}
\end{table*}


In this section we evaluate the performance of our instance segmentation method on the Cityscapes dataset. To find the best settings of our loss function, we first analyze the different aspects in an ablation study. Afterwards we report results of our best model on the test set of Cityscapes and compare with other top performing methods. Since our method is optimized for fast instance segmentation, we also report a time comparison with other instance segmentation methods.

\subsection{Implementation details}

\paragraph{Network architecture} We use the ERFNet-architecture~\cite{romera2018erfnet} as base-network. ERFNet is a dense-prediction encoder-decoder network optimized for real-time semantic segmentation. We convert the model into a 2-branch network, by sharing the encoder part and having 2 separate decoders. The first branch predicts the sigma and offset values, with 3 or 4 output channels depending on sigma ($\sigma$ vs $\sigma_{xy}$). The other branch outputs N seed maps, one for each semantic class. The offset values are limited between [-1,1] with a tanh activation function, sigma is made strictly positive by using an exponential activation function, effectively letting the network predict $log(\frac{1}{2\sigma^2})$.

\paragraph{Coordinate map} Since the Cityscapes images are of size 2048x1024, we construct a pixel coordinate map so that the x-coordinates are within the range of [0,2] and the y-coordinates within the range of [0,1]. This way, the difference in coordinate between two neighboring pixel is 1/1024, both in x and y direction. Because the offset vectors can have a value between [-1,1], each pixel can point at most 1024 pixels away from its current location. 

\paragraph{Training procedure} We first pre-train our models on 500x500 crops, taken out of the original 2048x1024 train images and centered around an object, for 200 epochs with a batch-size of 12. This way, we don't spend to much computation time on background patches without any instances. Afterwards we finetune the network for another 50 epochs on 1024x1024 crops with a batch-size of 2 to increase the performance on the bigger objects who couldn't fit completely within the 500x500 crop. During this stage, we keep the batch normalization statistics fixed. We use the Adam optimizer and polynomial learning rate decay $(1 - \frac{\text{epoch}}{\text{max epoch}})^{0.9}$. During pre-training we use an initial learning rate of 5e-4, which we lower to 5e-5 for finetuning. Training takes roughly 24 hours on two NVIDIA 1080 Ti GPU's. Next to random cropping, we also apply random horizontal mirroring as data-augmentation.

\begin{figure}
    \begin{center}
    	\includegraphics[width=1\linewidth]{images/timing_graph.pdf}
    \end{center}
    \caption{Speed accuracy trade-off between instance segmentation methods on the Cityscapes benchmark. Our method is the first real-time method with high accuracies. Image adapted from ~\cite{uhrig2018box2pix}}
    \label{fig:timing_results}
\end{figure}

\subsection{Cityscapes dataset}
The Cityscapes dataset is high quality dataset for urban scene understanding. It consists out of 5,000 finely annotated images (\texttt{fine}) of 2048 by 1024 pixels, with both semantic and instance-wise annotations, and 20,000 coarsely annotated images (\texttt{coarse}) with only semantic annotations. The wide range in object size and the varying scene layout makes this a challenging dataset for instance segmentation methods. 

The instance segmentation task consists in detecting objects of 8 different semantic classes and generating a binary mask for each of them. The performance is evaluated by the average precision (AP) criterion on the region level and averaged over the different classes. Aside from AP, $\text{AP}_{50\%}$ for an overlap of 50\,\%, $\text{AP}_{100m}$ and $\text{AP}_{50m}$ for objects restricted to respectively 100m and 50m are also reported. 

In the following experiments we will only use the \texttt{fine} train set to train our models, which consists out of the following classes with their respective number of objects:

\vspace{3mm}

\begin{centering}
    \footnotesize
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c}
         person & rider & car & truck & bus & train & mcycle & bicycle \\
         \hline 
         17.9k & 1.8k & 26.9k & 0.5k & 0.4k & 0.2k & 0.7k & 3.7k \\
    \end{tabular}
\end{centering}

Note that some classes (truck, bus, train) are highly underrepresented, which will negatively effect the model's test performance on those specific classes.

\begin{table*}
    \small
    \centering
    \begin{tabular}{c|c|c|c c c c c c c c}
        $\sigma / \sigma_{xy}$  & CoA & $AP[\texttt{val}]_{gt}$ & person & rider & car & truck & bus & train & mcycle & bicycle \\
        \hline
        $\sigma_{fixed}$         & centroid    & 28.0 & 32.3 & 28.1 & 45.1 & 30.2 & 37.3 & 14.4 & 19.9 & 16.9 \\
        $\sigma$                  & centroid    & 38.7 & 36.4 & 33.6 & 54.5 & 42.7 & 56.0 & 36.7 & 24.9 & 24.5 \\
        $\sigma$                  & learnable   & 39.5 & 39.4 & 35.4 & 56.0 & 40.3 & 57.6 & 34.6 & 26.1 & 26.5 \\
        $\sigma_{xy}$           & centroid    & 39.1 & 38.0 & 33.9 & 54.5 & 42.0 & 59.4 & 37.8 & 23.0 & 24.5 \\
        $\sigma_{xy}$           & learnable   & 40.5 & 39.3 & 34.5 & 55.5 & 44.3 & 59.8 & 41.2 & 24.8 & 25.0 \\
    \end{tabular}
    \vspace{3mm}
    \caption{Ablation experiments evaluated on the Cityscapes validation set using a ground-truth sampling approach. We measure the performance of a fixed sigma, the difference in using a scalar vs. 2-dimensional sigma and the difference in using the centroid or learnable center as \textit{center of attraction}.}
    \label{tab:abl_exp_gt}
\end{table*}

\subsection{Ablation Experiments}

In this section we evaluate the influence of the different parameters of our loss function on the validation set of Cityscapes: we investigate the importance of a learnable sigma, the difference in using the instance centroid or a learnable center as the \textit{center of attraction}, and the difference in using a scalar or a 2 dimensional sigma. Since we want to measure the effect on the instance part, we remove the object detection and classification part from the equation by using the ground truth annotations to localize the objects and assigning the correct semantic class, which is indicated in the tables as $\text{AP}_{gt}$.

\paragraph{Fixed vs. learnable sigma}
In this experiment we evaluate the importance of a learnable, instance-unique sigma over a fixed one. As explained in section~\ref{sub:learnable_margin}, when using a fixed sigma, the value has to be selected based on the size of the smallest object we still want to be able to separate, and is therefore set to correspond with a margin of 20 pixels. The results can be seen in table~\ref{tab:abl_exp_gt}. The significant performance difference (28 AP vs. 38.7 AP) shows the importance of having a unique, learnable sigma for each instance. Notice also that for classes with relatively more small instances, the difference is less pronounced, as expected.

\paragraph{Fixed vs. learnable Center of Attraction}
As described in the method section, the \textit{center of attraction} (CoA) of an instance can be defined as either the centroid, or more general, as a learnable center calculated by taking the mean over all spatial embeddings belonging to the instance. Intuitively, by giving the network the opportunity to decide on the location of the CoA itself, it can learn a more optimal location than the standard centroid. In table~\ref{tab:abl_exp_gt} we evaluate the two different approaches on the Cityscapes validation set using a ground-truth sampling approach,both in the case of a scalar or a 2-dimensional sigma. As predicted, in both cases we achieve a higher AP-score when using a learnable center instead of the fixed centroid, with a noticeable improvement over all classes. 

\begin{figure}
    \begin{center}
    	\includegraphics[width=1\linewidth]{images/sigma_size_small_crop.png}
    \end{center}
    \caption{Learned margin against the object's size. Each dot represents an object in the dataset. As predicted, we notice a positive correlation between the margin and the object's size.}
    \label{fig:sigma_vs_size}
\end{figure}

\paragraph{Circular vs. elliptical margin}
The margin for each instance is defined by the learnable sigma parameter in the gaussian function. This sigma can either be a scalar ($\sigma$), which results in a circular margin, or a two-dimensional vector ($\sigma_{xy}$), resulting in an elliptical margin. For rectangular objects (e.g.~pedestrians) a circular margin is not optimal, since it can only expand until it reaches the shortest border. An elliptical margin however would have the possibility to stretch and adapt to the shape of an object, possibly resulting in a higher accuracy. In table~\ref{tab:abl_exp_gt} we compare both methods and verify that a 2-dimensional sigma (elliptical margin) indeed performs better than a scalar one (circular margin).

Since sigma is a learnable parameter, we have no direct control over its value. Intuitively, since sigma controls the clustering margin, we speculated that for big objects sigma will be bigger, resulting in a bigger margin, and smaller for small objects. To verify this, in figure~\ref{fig:sigma_vs_size} we plotted sigma in function of the object's size. As predicted, there is indeed a positive correlation between an object's size and sigma.

\begin{figure*}
    \begin{center}
    	\includegraphics[width=1\linewidth]{images/results_cityscapes.pdf}
    \end{center}
    \caption{Results on the Cityscapes dataset. From left to right: input image, ground-truth and our predictions. Notice that our method is very good at detecting small objects and often predicts more correct objects than annotated in the ground-truth.}
    \label{fig:results}
\end{figure*}

\subsection{Results on Cityscapes}
In table~\ref{tab:results_cityscapes} we report results on the Cityscapes test set and compare with other high performing methods. Note however that it is important to pay attention at the training data on which a method is trained. Since the truck, bus and train classes are highly underrepresented in the \texttt{fine} set, methods who only train on this set will perform less on these classes than methods who augment their dataset with the \texttt{coarse} or \texttt{COCO} set. 

Comparing our method against the other \texttt{fine}-only methods, we occupy the second place with an AP-score of 27.6, locating ourselves between between the popular Mask R-CNN (26.2) and PANet(31.8). Notice however that we do much better on the person (34.5 vs 30.5), rider (26.1 vs 23.7) and car class (52.4 vs 46.9) than Mask R-CNN. If we compare our method with GMIS, a method trained on both the \texttt{fine} and \texttt{coarse} set, we notice that although it has the same AP-score as our method, it only performs better on the truck, bus and train class (because of the extra \texttt{coarse} set) and performs worse on all other classes. 

Although it is not fair to compare our method against methods trained on \texttt{fine+COCO}, we do notice that we achieve similar results on person (34.5 vs 34.8) and rider (26.1 vs 27.0), and even perform better on car (52.4 vs 49.1) and bicycle (18.9 vs 18.7) with respect to Mask R-CNN.

\begin{table}
    \small
    \centering
    \begin{tabular}{l|c|c|c}
        Method & AP & $\text{AP}_{50}$ & FPS \\
        \hline 
        Deep Contours~\cite{van2016instance}     & 2.3  & 3.6  & 5 \\
        Box2Pix~\cite{uhrig2018box2pix}          & 13.1 & 27.2 & 10.9 \\
        BAIS~\cite{hayder2017boundary}           & 17.4 & 36.7 & $<$1 \\
        Discriminate Loss~\cite{de2017semantic}  & 17.5 & 35.9 & 5 \\
        DWT~\cite{bai2017deep}                   & 19.4 & 35.3 & $<3$ \\
        Dynamic Net~\cite{arnab2017pixelwise}    & 20.0 & 38.3 & $<3$ \\
        SGN~\cite{liu2017sgn}                    & 25.0 & 44.9 & 0.6 \\
        Mask-RCNN~\cite{he2017mask} (fine)       & 26.2 & 49.9 & 2.2 \\
        PANet~\cite{liu2018path}                 & 31.8 & 57.1 & $<$1 \\
        \hline
        ours                                     & 27.6 & 50.9 & \textbf{11} \\
    \end{tabular}
    \vspace{3mm}
    \caption{Approximate timing results of instance segmentation methods on a resolution of 2048x1024 with test set accuracy ~\cite{uhrig2018box2pix}. Methods which are either to slow or have a very low accuracy are left out.}
    \label{tab:timing_results}
\end{table}

\subsection{Timing}
In table~\ref{tab:timing_results} we compare the execution speed of different methods. This is also depicted in fig~\ref{fig:timing_results}. Up to this moment, most methods have put there focus on accuracy rather than on execution speed. Mask-RCNN (26.2 AP - 1fps) and derivatives have high accuracy, but slow execution speed. Other methods,like Discriminative loss (17.5 AP - 5fps) or Box2Pix (13.1 AP - 10.9fps) achieve higher frame rates by downsampling resolution or using single shot detection methods, but dramatically lack behind in accuracy compared to Mask R-CNN. Since our method is based on the ERFNet network and combined with a clustering loss function, we are the first ones to achieve high accuracy  combined with real time performance (27.6 AP - 11fps). More specifically, the forward pass at a resolution of 2MP takes 65ms and the clustering step requires 26ms. 


