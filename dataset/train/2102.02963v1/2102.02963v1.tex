\section{Experiment}
\subsection{Dataset}

We conduct experiments on the widely used VIST dataset \cite{huang2016visual}, which consists of 40101 samples for training 5050 test samples. Each sample contains 5 images and one corresponding gold story. 
For fair comparison, we follow the same experiment setting as \cite{jung2020hide} except that we set the vocabulary size to 28000.
All models use the same fixed random seed.

\subsection{Concept Detector}
In this paper, we use the Concept Detection API produced by clarifai \footnote{\label{footnote-1}www.clarifai.com}. This Detector can predict 11,000 concepts which is larger than any other detection model. This powerful pretrained detector helps us to precisely find out the concepts inside the images, so that the word in the gold sentences can be easily involved in our knowledge enhanced candidate concepts.


\subsection{Implementation Details}

When training SSM, since we assume that there is no order relationship between the concepts in one image, so during the training phase, we randomly shuffle the target concept in one image.
When training BART, we conduct a two-stage training: 1) freeze all BART parameters except for the image projection matrix. 2) finetune all parameters. We use Adam~\cite{kingma2014adam} optimizer with an initial learning rate of 4e-4 in the first stage, then the learning rate is decreased to 4e-5 in the fine tuning stage. Each stage is trained for 5 epochs.
All the other parts of our model are trained with an Adam Optimizer with learning rate 4e-4.
During training, we follow \citet{jung2020hide} to blind one of the images starting from the 50-th epoch and increase the blinding into two images from epoch 80. The training stops at epoch 100.
Our model uses gold concepts extracted from gold stories to train the concept to story model. This step is similar to the common auto-regressive models that use the target token as the input to generate the next token. As has been discussed in the previous sections, this kind of generation often meets with the problem caused by the train-test discrepancy that we cannot see the gold concepts in the testing phase. To alleviate, a simple and effective way is to add noise to the inputs. In this work, we add two kinds of noise into the inputs in the story generation module: masking and random replacement. We mask 30\% concepts and replace 20\% of them into other similar words in training.


\subsection{$\tau$ search in Maximal Clique Selection Module}
 $\tau$ is set as the threshold in pruning edges for the maximal clique selection. Larger $\tau$ leads to fewer concepts that can be selected and will further result in the lack of imagination (diversity) in the generated story. However, smaller $\tau$ would lead to too many concepts that may mislead the model to generate irrelevant stories. To make a trade-off, we initialize $\tau$ as 0.3 and continual decreasing the number until Bron Kerbosch algorithm \cite{10.1145/362342.362367} can produce at least 5 candidate cliques that contains 7 to 15 candidate concepts in each clique.
 


\begin{table}[t]
\centering
\begin{tabular}{l|c|c|c}
\hline
 Method&  Precision& Recall&  F measure \\\hline\hline
 Rand&  2.68&  2.45&  2.56 \\\hline
 C\_Attn&  30.38&  \textbf{43.37}&  35.86 \\\hline
 I2C&  31.32&  20.75&  24.96\\\hline
 SSM&  40.43&  40.30&  40.36\\\hline
 MCSM&  \textbf{45.30}&  40.90&  \textbf{42.99}\\
  \hline
\end{tabular}
\caption{Concept selection performance of different methods. The results show that our MCSM achieved the best f-score among all methods.}
\label{tab:acc}
\end{table}









\subsection{Decoding strategy}

During model inference, usually beam search is used in decoding the sentences from the decoder~\cite{wang2018no, jung2020hide, yu2017beam}. However, it has been proved that beam search can result in bland, incoherent stories, or gets stuck in repetitive outputs~\cite{holtzman2019curious}. In this paper, to further improve diversity in generated stories, we apply nucleus sampling~\cite{holtzman2019curious} which outperforms top-k sampling method. However, nucleus sampling does not perform well without a well-trained language model. We show this by an experiment where we train our model with RNN as the story generation module on the VIST dataset and decode using nucleus sampling. Note the VIST dataset is relatively small compared with other natural language datasets like Book Corpus~\cite{Zhu_2015_ICCV}. We conduct human evaluation and let the annotators to rank the overall story qualities (lower the better) generated with different softmax temperatures [0.5,0.6,0.7,0.8,0.9] for the same image sequence. We randomly pick 100 samples for this experiment. Figure~\ref{fig:rank} shows that with the temperature increases, the story quality would drop.
Especially, when the temperature is 0.9 which is recommended in \citet{holtzman2019curious}, the generated sentences are incomprehensible and full of grammatical errors, as shown in the example of Figure~\ref{fig:rank}. This result motivates us to use a large scale pre-trained language model in our experiment.
\begin{figure}[t]
\centering
\includegraphics[scale=0.45]{LaTeX/figure/rank.pdf}
\caption{The y axis shows the ranking score (lower the better) and Distinct-4 score (higher the better) while we change the temperature in nucleus sampling with the model trained from scratch on VIST dataset. 
We ask the workers to rank the overall score for five stories generated by 5 different temperatures.
As we can see, with the increasing of the temperature, the stories become more diverse, however, the quality of them become lower.}
\label{fig:rank}
\end{figure}

Thus, for fair comparison with previous works, the same RNN with beam search is used as the story generation module to validate the effectiveness of our concept planning model.
While, we use nucleus sampling (temperature=0.9, p=0.9) when BART \cite{lewis2019bart} is applied to demonstrate a higher upper bound of our model's ability.




\subsection{Experiment on Concept Selection}
We first test the ability to select appropriate concepts for different models.
Since each image sequence in the test sample corresponds to 3 to 5 gold stories, we report the highest performance of the output selection result with respect to all gold stories in each sample. 
Similar as Keyphrase generation tasks, we apply precision, recall and f measure to evaluate the efficiency of concept selection methods. The precision is defined as the number of correct concepts selected over the target concepts, and recall is defined as the number of correct selections over all candidates.



We compare among several methods:
\begin{itemize}[noitemsep]
    \item \textbf{Rand}: A simple baseline where we randomly pick 3 concepts from the candidates for each image. On average, each image contains 2.65 gold concepts.
    \item \textbf{C\_Attn}: We extract the attended concepts where the attention score is larger than a threshold from the model of \citet{yang2019knowledgeable}. This is an end-to-end model with sigmoid attention on concept words. We choose 0.8 as the threshold since this contributes the best f-score.
    \item Image to concept(\textbf{I2C}): This is a straightforward version of concept selection where the concepts are directly generated from the images. We simply add a projection layer on each hidden state to predict the concept words from the vocabulary size of the concepts, which is very similar to the model of \citet{hsu2019knowledge}.
    \item \textbf{SSM}: Our proposal which uses a copy mechanism in each step of selection.
    \item \textbf{MCSM}: Our proposal which calculates the correlation score for concept-concept and image-concept and uses maximal clique selection.
\end{itemize}

\begin{table}[t]
\centering
\begin{tabular}{l|c|c|c}
\hline
 Method& Dist-2& Dist-3 &Dist-4\\\hline\hline
 INet$\smwhitestar$ &8.36 &18.92 &31.02\\\hline
 KS$\smwhitestar$ &10.84 &22.90 &36.51\\\hline
 KG-Story\dag &18.73 &38.65 & 57.22\\ \hline
 Our(MCSM) &13.98 & 34.01& 54.11\\ \hline
 Image+BART\dag &21.63 & 46.23& 67.57\\ \hline
 Our(MCSM)+BART\dag & \textbf{34.95}& \textbf{69.88}& \textbf{88.74}\\ \hline
 \hline
 Gold                 & 47.76& 82.27 & 95.05\\ \hline
\end{tabular}
\caption{Diversity of generated stories by different methods. Two-stage generation methods can produce more diverse stories. Using BART, we can achieve the diversity close the human writing, while achieving same level story quality in other aspect. \dag denotes the story generation module is pre-trained with other dataset. $\smwhitestar$ denotes end-to-end methods.}
\label{tab:diverse}
\end{table}

Qualitative results are shown in Table~\ref{tab:acc}. We can see that our proposed SSM and MCSM can achieve significantly higher f-score than other methods. This helps our model to keep the story relevance to the input images while generating diverse stories.



\subsection{Experiment on Visual Storytelling}
Here we show the results of the visual storytelling. We use the following baselines for comparison:

\noindent\textbf{INet}~\cite{jung2020hide} This is a recent work which uses a ``hide-and-tell" strategy to train an end-to-end model. In this method no concept is used.

\noindent\textbf{{KS}}~\cite{yang2019knowledgeable} This method uses sigmoid attention to incorporate concept features into the model. We change the structure of the visual encoder and decoder the same as \textbf{INet} for fair comparison.

\noindent\textbf{KG-Story}\dag~\cite{hsu2019knowledge} is a strong baseline that use two stage plan-write strategy and pretrain the decoder on RocStories Corpora~\cite{mostafazadeh2017lsdsem}. \dag indicates the model introduces external large-scale pretraining data.

\noindent\textbf{Image+BART}\dag is an end-to-end baseline that uses BART on top of image features to directly generate the story. This baseline is one-stage that does not generate concepts.

We also change the concept selection module and story generation module in our model to validate the effectiveness of each component. Specifically, we compare: \textbf{Rand+RNN}, \textbf{C\_Attn+RNN}, \textbf{SSM+RNN}, \textbf{MCSM+RNN}, and \textbf{MCSM+BART}\dag~.

\subsubsection{Comparison on diversity}
We first compare the ability of generating diverse stories of different models. Quantitative comparison is shown in Table \ref{tab:diverse}. We report Distinct-n (Dist-n) scores~\cite{li2015dist} that calculate the percentage of unique n-gram in all generated stories in the test dataset. Higher score means less inter-story repetition. 
From the table, two stage generation methods (KG-Story and ours) can achieve significantly higher diversity scores. 
Our MCSM can generate the most diverse stories among all the methods without using external pretrained models.
When equipped with BART, we can even achieve diversity close to human writing.
We show in the following that the increased diversity also improves the overall quality of the story.

\begin{table}[t]
\centering
\begin{tabular}{l|c|c|c|c|c}
\hline
 Method& B-3& B-4 & R& M & C\\\hline\hline
 INet$\smwhitestar$& 23.5&14.4&29.7&35.3&9.5\\\hline
 KS$\smwhitestar$&\textbf{24.7}&\textbf{15.0}&\textbf{31.0}&35.0&9.8\\\hline
 \hline
 Rand+RNN&  13.3&6.1&27.2&31.1&2.2\\\hline
 C\_Attn+RNN&20.7&11.2&29.7&34.5&7.8\\\hline
 SSM+RNN &22.1&12.0&30.0&35.4&10.5\\\hline
 MCSM+RNN&23.1&13.0&30.7&\textbf{36.1}&\textbf{11.0}\\
 \hline
\end{tabular}
\caption{Automatic metric in story quality. We report BLEU (B), METEOR (M), ROUGH-L (R), and CIDEr (C) scores. The two-stage generation can achieve higher METEOR and CIDER scores.}
\label{tab:am}
\end{table}



\begin{table*}[!ht]
\footnotesize
\centering
\small
\begin{tabular}{l||c|c||c|c||c|c||c|c||c|c}
    \hline
    Choices(\%) & \multicolumn{2}{c||}{MCSM vs INet}& \multicolumn{2}{c||}{MCSM vs KS}& \multicolumn{2}{c||}{MCSM vs SSM}& \multicolumn{2}{c||}{MCSM+BART\dag ~vs KS}& \multicolumn{2}{c}{MCSM+BART\dag ~vs MCSM}\\\cline{2-11}
    & MCSM & INet & MCSM & KS & MCSM & SSM & MCSM+BART & KS & MCSM+BART & MCSM\\
    \hline
    Revelence               &\textbf{47.4} &35.6    &26.3&\textbf{31.6}     &\textbf{50.5} &40.0                 &28.8 &\textbf{33.6}                        &35.2&35.2\\
    Informativeness         &\textbf{51.0*} &31.6    &\textbf{46.3*} &28.9    &\textbf{44.7} &41.2                 &\textbf{62.5**} &18.8                       &\textbf{58.8**} &23.5\\
    Logicality                 &\textbf{35.5} &34.3    &\textbf{34.2} &29.0    &32.9 &\textbf{42.3}                 &\textbf{35.3} &33.3                        &\textbf{40.2} & 37.5\\
    Overall                 &\textbf{55.0**} &30.0    &\textbf{44.7} &34.2    &\textbf{48.3} &37.1                 &\textbf{43.5**} &23.0                        &\textbf{47.0*} & 31.6\\
    
    \hline
\end{tabular}
\caption{Human evaluation. Numbers indicate the percentage of annotators believe that a model outperforms its opponent. Methods without (+BART) means using RNN as the story generation module. Cohenâ€™s Kappa coefficients ($\kappa$) for all evaluations are in Moderate (0.4-0.6) or Fair (0.2-0.4) agreement, which ensures inter-annotator agreement. We also conduct a sign test to check the significance of the differences. The scores marked with * denotes $p < 0.05$ and ** indicates $p < 0.01$ in sign test.}

\label{tab:human}
\end{table*}


\subsubsection{Automatic evaluation}
In Table~\ref{tab:am}, for comparing the quality of generated stories, we use automatic metrics BLEU (B)~\cite{papineni2002bleu}, METEOR (M)~\cite{banerjee2005meteor},
ROUGH-L (R)~\cite{lin2004rouge}, and CIDEr (C)~\cite{vedantam2015cider}. Note that it remains tricky for automatic scores to appropriately evaluate story qualities. Using concept, KS can achieve better performance than INet that does not use concept. From the comparison of the variants of our model, we can see that better concept selection can lead to better automatic scores. With reasonable concept selection, our SSM and MCSM can achieve highest METEOR and CIDER scores.







\subsubsection{Human Evaluation}
To better evaluate the quality of generated stories, we conduct human evaluation to compare pair-wise outputs with several models via the Amazon Mechanical Turk (AMT). 
We randomly sample 200 image sequences from the test set and generate stories using each model. For each sample pair, two annotators participate in the judgement and decide their preference on either story (or tie) in terms of Relevance, Informativeness, Logicality and Overall. \textbf{Relevance} evaluates how relevant the stories and the images are. \textbf{Informativeness} assesses how much information can be achieved in the generated stories, and this score from one side reflects the diversity of stories. \textbf{Logicality} evaluates the logical coherence in the stories. \textbf{Overall} is a subjective criterion that shows the preference of workers. 








Table~\ref{tab:human} shows the human evaluation result. Since there exists randomness in human evaluation, we compute the Cohen's Kappa coefficient and found that all evaluations are in Moderate Agreement and Fair agreement, which indicates the evaluation result is reasonable as good inner agreement between evaluators is reached. We also conduct a Sign test to illustrate the significance of the evaluation difference: if p is below 0.05 it would indicate that the two compared models have a significant performance difference.
From the comparison between MCSM and INet and the comparison between MCSM and KS, we can see that our two-stage planning method greatly outperforms the end-to-end models, especially in the informativeness score. The MCSM module also outperforms the SSM module, which indicates positive correlation between the quality of concept selection and the overall quality of generated stories.
Finally, using BART with MCSM can help to achieve further informativeness and generate even better stories.


\begin{figure}[t]
\centering
\includegraphics[scale=0.3]{LaTeX/figure/correlation.png}
\caption{We calculate the Pearson Correlation Coefficient for each criteria in human evaluation. R, I, L, O denotes Relevance, Informativeness, Logicality and Overall, respectively. We can see that Informativeness is almost independent to Relevance and Logicality, while is highly correlated to Overall score.}
\label{fig:correlation}
\end{figure}



\subsection{Importance of informativeness in story quality}
We calculate the Pearson Correlation Coefficient on four criteria in human evaluation. 
In Figure~\ref{fig:correlation}, R,  I,  L,  O  denotes Relevance, Informativeness, Logicality and Overall, respectively. 
Ranged from -1 to 1, the Informativeness score has low correlation score with Logicality (0.011) and Relevance (0.08), while a high correlation score with Overall (0.47).
This indicates that Informativeness is almost independent on Relevance and Logicality, but highly dependent on the Overall score. This suggests that humans tend to choose stories with more interesting information. This phenomenon proves the significance of informativeness and diversity in visual storytelling.



\begin{figure}[!t]
\centering
\includegraphics[scale=0.33]{LaTeX/figure/demo.pdf}
\caption{The examples of generated stories by different methods. Our MCSM and SSM can generate better stories compared with other baselines that do not use BART. When using the pretrained BART, the concept selection with MCSM can produce vivid and informative story.}
\label{fig:demo}
\end{figure}


\subsection{Case Study}
We show a qualitative result of a random test sample in Figure~\ref{fig:demo}. This is a hard example because the last three images are very similar and the objects in all images are hard to recognize. We can see that INet generates monotonous and even irrelevant sentences. KS can generate better sentences but still low in lexical diversity. For the stories generated by two-stage strategy with RNN (SSM+RNN, MCSM+RNN), we can see that the story follows the selected concepts and the stories seem more reasonable than that of end-to-end training methods. 
When using BART, we compare three methods that represent no concept selection (Image+BART), bad concept selection (Rand+BART) and ours concept selection (MCSM+BART).
We can see that without using concepts or using randomly selected concepts, the generated stories are of low quality and to a certain extent irrelevant to the images. However, when guided by the selected concept, the story becomes vivid, relevant and logical. 

