\section{Experiments and Results}


In this section, we describe our experiments with the method described above to learn convolutional cells. In summary, all architecture searches are performed using the CIFAR-10 classification task \cite{krizhevsky2009learning}. The controller RNN was trained using Proximal Policy Optimization (PPO) \cite{SchulmanWDRK17} by employing a global workqueue system for generating a pool of child networks controlled by the RNN. In our experiments, the pool of workers in the workqueue consisted of 500 GPUs. 

The result of this search process over 4 days yields several candidate convolutional cells. We note that this search procedure is almost  faster than previous approaches \cite{zoph2017neural} that took 28 days.\footnote{In particular, we note that previous architecture search \cite{zoph2017neural} used 800 GPUs for 28 days resulting in 22,400 GPU-hours. The method in this paper uses 500 GPUs across 4 days resulting in 2,000 GPU-hours. The former effort used Nvidia K40 GPUs, whereas the current efforts used faster NVidia P100s. Discounting the fact that the we use faster hardware, we estimate that the current procedure is roughly about  more efficient.} Additionally, we demonstrate below that the resulting architecture is superior in accuracy.

Figure \ref{figure:cell_structure} shows a diagram of the top performing Normal Cell and Reduction Cell. Note the prevalence of separable convolutions and the number of branches compared with competing architectures \cite{simonyan2014very,szegedy2015going,he2015deep,szegedy2016rethinking,szegedy2016inception}. Subsequent experiments focus on this convolutional cell architecture, although we examine the efficacy of other, top-ranked  convolutional cells in ImageNet experiments (described in Appendix \ref{sec:othernasnet}) and report their results as well. We call the three networks constructed from the best three searches \emph{NASNet-A}, \emph{NASNet-B} and \emph{NASNet-C}.



\begin{figure*}[h!]
\begin{center}
\includegraphics[width=0.7\linewidth]{Cell-A-Version-2.pdf}
\caption{Architecture of the best convolutional cells (NASNet-A) with  blocks identified with CIFAR-10 . The input (white) is the hidden state from previous activations (or input image). The output (pink) is the result of a concatenation operation across all resulting branches.
Each convolutional cell is the result of  blocks.
A single block is corresponds to two primitive operations (yellow) and a combination operation (green). Note that colors correspond to operations in Figure \ref{figure:cell}. 
}
\label{figure:cell_structure}
\end{center}
\end{figure*}

We demonstrate the utility of the convolutional cells by employing this learned architecture on CIFAR-10 and a family of ImageNet classification tasks. The latter family of tasks is explored across a few orders of magnitude in computational budget.
After having learned the convolutional cells, several hyper-parameters may be explored to build a final network for a given task: (1) the number of cell repeats  and (2) the number of filters in the initial convolutional cell. After selecting the number of initial filters, we use a common heuristic to double the number of filters whenever the stride is 2.
Finally, we define a simple notation, e.g.,   @ , to indicate these two parameters in all networks, where  and  indicate the number of cell repeats and the number of filters in the penultimate layer of the network, respectively.

For complete details of of the architecture learning algorithm and the controller system, please refer to  Appendix \ref{section:appendix-details}. Importantly, when training NASNets, we discovered \emph{ScheduledDropPath}, a modified version of DropPath~\cite{larsson2016fractalnet}, to be an effective regularization method for NASNet. In DropPath~\cite{larsson2016fractalnet}, each path in the cell is stochastically dropped with some fixed probability during training.  In our modified version, ScheduledDropPath, each path in the cell is dropped out with a probability that is \emph{linearly increased over the course of training}. We find that DropPath does not work well for NASNets, while ScheduledDropPath  significantly improves the final performance of NASNets in both CIFAR and ImageNet experiments.

\subsection{Results on CIFAR-10 Image Classification}






For the task of image classification with CIFAR-10, we set  or 6 (Figure \ref{figure:mainnet}). The test accuracies of the best architectures are reported in Table~\ref{tab:cifar10} along with other state-of-the-art models. As can be seen from the Table, a large NASNet-A model with cutout data augmentation~\cite{devries2017improved} achieves a state-of-the-art error rate of 2.40\% (averaged across 5 runs), which is slightly better than the previous best record of 2.56\% by \cite{devries2017improved}. The best single run from our model achieves 2.19\% error rate.



\begin{table*}[h!]
\centering
\small
\begin{tabular}{l|cc|c}
\toprule
\multicolumn{1}{c|}{\bf model} & \multicolumn{1}{l}{\bf depth} & \multicolumn{1}{l|}{\bf \# params}  & \bf error rate (\%)  \\ \midrule
DenseNet  \cite{Huang2016Densely} & 40 & 1.0M &  5.24  \\
DenseNet \cite{Huang2016Densely} & 100 & 7.0M &  4.10  \\
DenseNet  \cite{Huang2016Densely} & 100 & 27.2M &  3.74  \\ 
DenseNet-BC  \cite{Huang2016Densely} & 190 & 25.6M &  3.46  \\
\midrule
Shake-Shake 26 2x32d \cite{gastaldi17shakeshake} & 26 & 2.9M & 3.55 \\
Shake-Shake 26 2x96d \cite{gastaldi17shakeshake} & 26 & 26.2M & 2.86 \\
Shake-Shake 26 2x96d + cutout \cite{devries2017improved} & 26 & 26.2M & 2.56 \\
\midrule
NAS v3 \cite{zoph2017neural}& 39 & 7.1M & 4.47  \\
NAS v3 \cite{zoph2017neural}& 39 & 37.4M & 3.65 \\
\midrule
NASNet-A \;(6 @ 768) & - & 3.3M & 3.41 \\
NASNet-A \;(6 @ 768) + cutout & - & 3.3M & 2.65 \\
NASNet-A \;(7 @ 2304) & - & 27.6M & 2.97 \\
NASNet-A \;(7 @ 2304) + cutout & - & 27.6M & 2.40 \\
NASNet-B \;(4 @ 1152) & - & 2.6M & 3.73 \\
NASNet-C \;(4 @ 640) & - & 3.1M & 3.59 \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{Performance of Neural Architecture Search and other state-of-the-art models on CIFAR-10. All results for NASNet are the mean accuracy across 5 runs.}
\label{tab:cifar10}
\end{table*}










\subsection{Results on ImageNet Image Classification}



We performed several sets of experiments on ImageNet with the best convolutional cells learned from CIFAR-10.
We emphasize that we merely transfer the architectures from CIFAR-10 but train all ImageNet models weights from scratch.

Results are summarized in Table \ref{tab:imagenet} and \ref{tab:imagenet-constrained} and Figure \ref{figure:flops_vs_accuracy}. In the first set of experiments, we train several image classification systems operating on 299x299 or 331x331 resolution images with different experiments scaled in computational demand to create models that are roughly on par in computational cost with Inception-v2 \cite{BatchNorm}, Inception-v3 \cite{szegedy2016rethinking} and PolyNet \cite{zhang2016polynet}.
We show that this family of models achieve state-of-the-art performance with fewer floating point operations and parameters than comparable architectures. Second, we demonstrate that by adjusting the scale of the model we can achieve state-of-the-art performance at smaller computational budgets, exceeding streamlined CNNs hand-designed for this operating regime \cite{howard2017mobilenets, shufflenet}.

Note we do not have residual connections between convolutional cells as the models learn skip connections on their own. We empirically found manually inserting residual connections between cells to not help performance.
Our training setup on ImageNet is similar to \cite{szegedy2016rethinking}, but please see Appendix \ref{section:appendix-details} for details.


\begin{figure*}[h!]
\begin{center}
\includegraphics[width=0.45\linewidth]{FLOPS-Diagram-With-SE-2.pdf}
\hfill
\includegraphics[width=0.45\linewidth]{Params-Diagram-With-SE-2.pdf}
\caption{Accuracy versus computational demand (left) and number of parameters (right) across top performing published CNN architectures on ImageNet 2012 ILSVRC challenge prediction task. Computational demand is measured in the number of floating-point multiply-add operations to process a single image. Black circles indicate previously published results and red squares highlight our proposed models.}
\label{figure:flops_vs_accuracy}
\end{center}
\end{figure*}

\begin{table*}[h!]
\centering
\small
\begin{tabular}{lc|cc|cc}
\toprule
\multicolumn{1}{c}{\bf Model} & {\bf image size} & \multicolumn{1}{l}{\bf \# parameters} & \bf Mult-Adds & \bf Top 1 Acc. (\%) & \bf Top 5 Acc. (\%) \\ \midrule
Inception V2~\cite{BatchNorm} & 224224 & 11.2\,M & 1.94\,B & 74.8 & 92.2 \\
\textbf{NASNet-A (5 @ 1538)} & \textbf{299299} & \textbf{10.9\,M} & \textbf{2.35\,B} & \textbf{78.6} & \textbf{94.2} \\
\midrule
Inception V3~\cite{szegedy2016rethinking} & 299299 & 23.8\,M & 5.72\,B & 78.8 & 94.4 \\
Xception~\cite{chollet2016xception}& 299299 & 22.8\,M & 8.38\,B & 79.0 & 94.5 \\
Inception ResNet V2~\cite{szegedy2016inception} & 299299 & 55.8\,M & 13.2\,B & 80.1 & 95.1 \\
\textbf{NASNet-A (7 @ 1920)} & \textbf{299299} & \textbf{22.6\,M} & \textbf{4.93\,B} & \textbf{80.8} & \textbf{95.3} \\
\midrule
ResNeXt-101 (64 x 4d)~\cite{xie2016aggregated} & 320320 & 83.6\,M & 31.5\,B & 80.9 & 95.6 \\
PolyNet~\cite{zhang2016polynet} & 331331 & 92\,M & 34.7\,B & 81.3 & 95.8 \\
DPN-131~\cite{dualpath} & 320320 & 79.5\,M & 32.0\,B & 81.5 & 95.8 \\
{\bf SENet~\cite{hu2017squeeze}}& {\bf 320320} & {\bf 145.8\,M} & {\bf 42.3\,B} & {\bf 82.7} & {\bf 96.2} \\

\textbf{NASNet-A (6 @ 4032)} & \textbf{331331} & \textbf{88.9\,M} & \textbf{23.8\,B} & \textbf{82.7} & \textbf{96.2} \\
\bottomrule
\end{tabular}

\vspace{0.2cm}
\caption{Performance of architecture search and other published state-of-the-art models on ImageNet classification. Mult-Adds indicate the number of composite multiply-accumulate operations for a single image. Note that the composite multiple-accumulate operations are calculated for the image size reported in the table. Model size for \cite{hu2017squeeze} calculated from open-source implementation.}
\label{tab:imagenet}
\end{table*}

\begin{table*}[h!]
\centering
\small
\begin{tabular}{l|cc|cc}
\toprule
\multicolumn{1}{c|}{\bf Model} & \multicolumn{1}{l}{\bf \# parameters} & \bf Mult-Adds & \bf Top 1 Acc. (\%) & \bf Top 5 Acc. (\%)  \\ \midrule
Inception V1~\cite{szegedy2015going} & 6.6M & 1,448\,M & 69.8 & 89.9 \\
MobileNet-224 \cite{howard2017mobilenets} & 4.2\,M & 569\,M & 70.6 & 89.5  \\
ShuffleNet (2x) \cite{shufflenet} &  5M & 524\,M & 70.9 & 89.8 \\
\midrule
\textbf{NASNet-A (4 @ 1056)} & \textbf{5.3\,M} & \textbf{564\,M} & \textbf{74.0} & \textbf{91.6} \\
NASNet-B (4 @ 1536) & 5.3M & 488\,M & 72.8 & 91.3  \\
NASNet-C (3 @ 960) & 4.9M & 558\,M & 72.5 & 91.0  \\
\bottomrule
\end{tabular}

\vspace{0.2cm}
\caption{Performance on ImageNet classification on a subset of models operating in a constrained computational setting, i.e., \,B multiply-accumulate operations per image. All models use 224x224 images.
 indicates top-1 accuracy not reported in \cite{szegedy2015going} but from open-source implementation.}
\label{tab:imagenet-constrained}
\end{table*}










Table~\ref{tab:imagenet} shows that the convolutional cells discovered with CIFAR-10 generalize well to ImageNet problems.
In particular, each model based on the convolutional cells exceeds the predictive performance of the corresponding hand-designed model.
Importantly, the largest model achieves a new state-of-the-art performance for ImageNet (82.7\%) based on single, non-ensembled predictions, surpassing previous best published result by 1.2\% \cite{dualpath}. Among the unpublished works, our model is on par with the best reported result of 82.7\%~\cite{hu2017squeeze}, while having significantly fewer floating point operations.
Figure \ref{figure:flops_vs_accuracy} shows a complete summary of our results in comparison with other published results. Note the family of models based on convolutional cells provides an envelope over a broad class of human-invented architectures.


Finally, we test how well the best convolutional cells may perform in a resource-constrained setting, e.g.,  mobile devices (Table \ref{tab:imagenet-constrained}). In these settings, the number of floating point operations is severely constrained and predictive performance must be weighed against latency requirements on a device with limited computational resources. MobileNet \cite{howard2017mobilenets} and ShuffleNet \cite{shufflenet} provide state-of-the-art results obtaining 70.6\% and 70.9 accuracy, respectively on 224x224 images using 550M multliply-add operations. An architecture constructed from the best convolutional cells  achieves superior predictive performance (74.0\% accuracy) surpassing previous models but with comparable computational demand. In summary, we find that the learned convolutional cells are flexible across model scales achieving state-of-the-art performance across almost 2 orders of magnitude in computational budget.



\subsection{Improved features for object detection}

\begin{table*}[h!]
\centering
\small
\begin{tabular}{lc|cc}
\toprule
{\bf Model} & {\bf resolution} & {\bf mAP (mini-val)} & {\bf mAP (test-dev)} \\
\midrule
MobileNet-224 \cite{howard2017mobilenets} &  & 19.8\% & -  \\
ShuffleNet (2x) \cite{shufflenet} &   & 24.5\% & - \\ 
\textbf{NASNet-A (4 @ 1056)} &  & \textbf{29.6\%} & - \\
\midrule
ResNet-101-FPN \cite{lin2016feature} & 800 (short side) & - & 36.2\% \\
Inception-ResNet-v2 (G-RMI) \cite{huang2016speed} &  & 35.7\% & 35.6\% \\
Inception-ResNet-v2 (TDM) \cite{shrivastava2016beyond} &  & 37.3\% & 36.8\% \\
\textbf{NASNet-A (6 @ 4032)} &  & 41.3\% & 40.7\% \\
\textbf{NASNet-A (6 @ 4032)} &  & \textbf{43.2\%} & {\bf 43.1\%} \\
\midrule
ResNet-101-FPN (RetinaNet) \cite{lin2017focal} & 800 (short side) & - & 39.1\%  \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{Object detection performance on COCO on {\it mini-val} and {\it test-dev} datasets across a variety of image featurizations. All results are with the Faster-RCNN object detection framework \cite{faster_rcnn} from a single crop of an image. Top rows highlight mobile-optimized image featurizations, while bottom rows indicate computationally heavy image featurizations geared towards achieving best results. All {\it mini-val} results employ the same 8K subset of validation images in \cite{huang2016speed}.}
\label{table:mscoco}
\end{table*}


Image classification networks provide generic image features that may be transferred to other computer vision problems \cite{donahue2014decaf}. One of the most important problems is the spatial localization of objects within an image. To further validate the  performance of the family of NASNet-A networks, we test whether object detection systems derived from NASNet-A lead to improvements in object detection \cite{huang2016speed}.

To address this question, we plug in the family of NASNet-A networks pretrained on ImageNet into the Faster-RCNN object detection pipeline \cite{faster_rcnn} using an open-source software platform \cite{huang2016speed}. We retrain the resulting object detection pipeline on the combined COCO training plus validation dataset excluding 8,000 mini-validation images.
We perform single model evaluation using 300-500 RPN proposals per image. In other words, we only
pass a single image through a single network. We evaluate the model on the COCO {\it mini-val} \cite{huang2016speed} and {\it test-dev} dataset and report the mean average precision (mAP) as computed with the standard COCO metric library \cite{lin2014microsoft}. We perform a simple search over learning rate schedules to identify the best possible model. Finally, we examine the behavior of two object detection systems employing the best performing NASNet-A image featurization (NASNet-A,  @ ) as well as the image featurization geared towards mobile platforms (NASNet-A,  @ ).

For the mobile-optimized network, our resulting system achieves a mAP of 29.6\% -- exceeding previous mobile-optimized networks that employ Faster-RCNN by over 5.0\% (Table \ref{table:mscoco}). For the best NASNet network, our resulting network operating on images of the same spatial resolution (800  800) achieves mAP = 40.7\%, exceeding equivalent object detection systems based off lesser performing image featurization (i.e. Inception-ResNet-v2) by 4.0\% \cite{huang2016speed,shrivastava2016beyond} (see Appendix for example detections on images and side-by-side comparisons). Finally, increasing the spatial resolution of the input image results in the best reported, single model result for object detection of 43.1\%, surpassing the best previous best by over 4.0\% \cite{lin2017focal}.\footnote{A primary advance in the best reported object detection system is the introduction of a novel loss \cite{lin2017focal}. Pairing this loss with NASNet-A image featurization may lead to even further performance gains. Additionally, performance gains are achievable through ensembling multiple inferences across multiple model instances and image crops (e.g., \cite{huang2016speed}).} These results provide further evidence that NASNet provides superior, generic image features that may be transferred across other computer vision tasks. Figure~\ref{figure:object-detection} and Figure~\ref{figure:object-detection-examples} in Appendix~\ref{sec:object_detection} show four examples of object detection results produced by NASNet-A with the Faster-RCNN framework.




\subsection{Efficiency of architecture search methods}
\label{sec:random_search}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=\columnwidth]{random_search-eps-converted-to.pdf}
\caption{Comparing the efficiency of random search (RS) to reinforcement learning (RL) for learning neural architectures. The x-axis measures the total number of model architectures sampled, and the y-axis is the validation performance on CIFAR-10 after 20 epochs of training. 
}
\label{figure:random-search}
\end{center}
\end{figure}

Though what search method to use is not the focus of the paper, an open question is how effective is the reinforcement learning search method. In this section, we study the effectiveness of reinforcement learning for architecture search on the CIFAR-10 image classification problem and compare it to brute-force random search (considered to be a very strong baseline for black-box optimization~\cite{bergstra2012random}) given an equivalent amount of computational resources. 



Figure \ref{figure:random-search} shows the performance of reinforcement learning (RL) and random search (RS) as more model architectures are sampled. Note that the best model identified with RL is significantly better than the best model found by RS by over 1\% as measured by on CIFAR-10. Additionally, RL finds an entire range of models that are of superior quality to random search. We observe this in the mean performance of the top-5 and top-25 models identified in RL versus RS.
We take these results to indicate that although RS may provide a viable search strategy, RL finds better architectures in the NASNet search space.


