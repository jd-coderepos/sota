\documentclass[11pt,a4paper]{article}

\usepackage{times}
\usepackage{latexsym}
\usepackage[hide links]{hyperref}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{stix}
\usepackage{comment}
\usepackage{naaclhlt2018}

\widowpenalty9000
\clubpenalty9000
  
\aclfinalcopy 




\title{Explainable Prediction of Medical Codes from Clinical Text}
   \author{James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, Jacob Eisenstein \\
     Georgia Institute of Technology \\
     \texttt{\{jmullenbach3, swiegreffe6, jon.duke\}@gatech.edu} \\ \texttt{jsun@cc.gatech.edu, jacobe@gatech.edu} \\
   }
\date{}

\newcommand{\bff}{\mathbf{f}}
\newcommand{\bfg}{\mathbf{g}}
\newcommand{\bfs}{\mathbf{s}}
\newcommand{\bfp}{\mathbf{p}}
\newcommand{\bfq}{\mathbf{q}}

\newcommand{\FIR}{\tilde F_{\textrm{IR}}}
\newcommand{\fIRx}{{\bf f}_{\textrm{IR},\textit{x}}}
\newcommand{\fIRy}{{\bf f}_{\textrm{IR},\textit{y}}}

\newcommand{\fM}{\mathbf{f}_{\rm M}}
\newcommand{\fMM}{\mathbf{f}_{\rm MM}}
\newcommand{\bft}{\mathbf{t}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\weight}{\boldsymbol{\theta}}


\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\demb}{d_{e}}
\newcommand{\dconv}{d_c}
\newcommand{\pate}{precision}
\newcommand{\numlabels}{\ensuremath |\mathcal{L}|}
\newcommand{\sumlabels}{\sum_{\ell = 1}^{\numlabels}}

\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}
\renewcommand{\subsubsectionautorefname}{\S}

\begin{document}
\maketitle

\begin{abstract}
Clinical notes are text documents that are created by clinicians for each patient encounter. They are typically accompanied by medical codes, which describe the diagnosis and treatment. Annotating these codes is labor intensive and error prone; furthermore, the connection between the codes and the text is not annotated, obscuring the reasons and details behind specific diagnoses and treatments. We present an attentional convolutional network that predicts medical codes from clinical text. Our method aggregates information across the document using a convolutional neural network, and uses an attention mechanism to select the most relevant segments for each of the thousands of possible codes. The method is accurate, achieving \pate{} of 0.71 and a Micro-F1 of 0.54, which are both better than the prior state of the art. Furthermore, through an interpretability evaluation by a physician, we show that the attention mechanism identifies meaningful explanations for each code assignment.
\end{abstract}

\section{Introduction}
Clinical notes are free text narratives generated by clinicians during patient encounters.
They are typically accompanied by a set of metadata codes from the International Classification of Diseases (ICD), which present a standardized way of indicating diagnoses and procedures that were performed during the encounter. ICD codes have a variety of uses, ranging from billing to predictive modeling of patient state~\cite{choi2016doctor, ranganath2015survival,denny2010phewas,avati2017improving}. Because manual coding is time-consuming and error-prone, automatic coding has been studied since at least the 1990s~\cite{de1998hierarchical}. The task is difficult for two main reasons. First, the label space is very high-dimensional, with over 15,000 codes in the ICD-9 taxonomy, and over 140,000 codes combined in the newer ICD-10-CM and ICD-10-PCS taxonomies \cite{ICD10}. Second, clinical text includes irrelevant information, misspellings and non-standard abbreviations, and a large medical vocabulary. These features combine to make the prediction of ICD codes from clinical notes an especially difficult task, for computers and human coders alike~\cite{birman2005accuracy}.

\begin{table*}
  \centering
  \small
\begin{tabular}{lp{4.8in}}

  \multicolumn{2}{l}{\textbf{934.1}: ``Foreign body in main bronchus''} \\
  \toprule
CAML (HI) & \textit{...line placed bronchoscopy performed showing} \textbf{large mucus plug on} \textit{the left on transfer to...} \\
Cosine Sim & \textit{...also needed medication to help} \textbf{your body maintain your} \textit{blood pressure after receiving iv...} \\
CNN & \textit{...found to have a large} \textbf{lll lingular pneumonia on} \textit{chest x ray he was...} \\
Logistic Regression & \textit{...impression confluent consolidation involving nearly} \textbf{the entire left lung} \textit{with either bronchocentric or vascular...} \1em]


\multicolumn{2}{l}{\textbf{428.20}: ``Systolic heart failure, unspecified''} \\ \toprule
CAML & \textit{...no mitral valve prolapse moderate} \textbf{to severe mitral regurgitation} \textit{is seen the tricuspid valve...} \\
Cosine Sim & \textit{...is seen the estimated pulmonary} \textbf{artery systolic pressure is} \textit{normal there is no pericardial...} \\
CNN & \textit{...and suggested starting hydralazine imdur} \textbf{continue aspirin arg admitted} \textit{at baseline cr appears patient...} \\
Logistic Regression (HI) & \textit{...anticoagulation monitored on tele pump} \textbf{systolic dysfunction with ef} \textit{of seen on recent echo...}


\end{tabular}
\caption{Presentation of example qualitative evaluations. In real evaluation, system names generating the 4-gram are not given. An `I' marking indicates a snippet evaluated as informative, and `HI' indicates that it is highly informative; see \autoref{sec:qual} for more details. }
\label{tab:qual_example}
\end{table*}
 
In this application paper, we develop convolutional neural network (CNN)-based methods for automatic ICD code assignment based on text discharge summaries from intensive care unit (ICU) stays. To better adapt to the multi-label setting, we employ a per-label attention mechanism, which allows our model to learn distinct document representations for each label. We call our method \textbf{C}onvolutional \textbf{A}ttention for \textbf{M}ulti-\textbf{L}abel classification (CAML). Our model design is motivated by the conjecture that important information correlated with a code's presence may be contained in short snippets of text which could be anywhere in the document, and that these snippets likely differ for different labels. To cope with the large label space, we exploit the textual descriptions of each code to guide our model towards appropriate parameters: in the absence of many labeled examples for a given code, its parameters should be similar to those of codes with similar textual descriptions. 

We evaluate our approach on two versions of MIMIC~\cite{johnson2016mimic}, an open dataset of ICU medical records. Each record includes a variety of narrative notes describing a patient's stay, including diagnoses and procedures.
Our approach substantially outperforms previous results on medical code prediction on both MIMIC-II and MIMIC-III datasets. 

We consider applications of this work in a decision support setting. Interpretability is important for any decision support system, especially in the medical domain. The system should be able to explain why it predicted each code; even if the codes are manually annotated, it is desirable to explain what parts of the text are most relevant to each code. These considerations further motivate our per-label attention mechanism, which assigns importance values to -grams in the input document, and which can therefore provide explanations for each code, in the form of extracted snippets of text from the input document. We perform a human evaluation of the quality of the explanations provided by the attention mechanism, asking a physician to rate the informativeness of a set of automatically generated explanations.\footnote{Our code, data splits, and pre-trained models are available at \url{github.com/jamesmullenbach/caml-mimic}.}
 \section{Method}

We treat ICD-9 code prediction as a multilabel text classification problem~\cite{mccallum1999multi}.\footnote{We focus on codes from the ICD-9 taxonomy, rather than the more recent ICD-10, for the simple reason that this is the version of ICD used in the MIMIC datasets.} Let  represent the set of ICD-9 codes; the labeling problem for instance  is to determine  for all . We train a neural network which passes text through a convolutional layer to compute a base representation of the text of each document~\cite{kim2014convolutional}, and makes  binary classification decisions. Rather than aggregating across this representation with a pooling operation, we apply an attention mechanism to select the parts of the document that are most relevant for each possible code. These attention weights are then applied to the base representation, and the result is passed through an output layer, using a sigmoid transformation to compute the likelihood of each code. 
We employ a regularizer to encourage each code's parameters to be similar to those of codes with similar textual descriptions. We now describe each of these elements in more detail.

\subsection{Convolutional architecture}
At the base layer of the model, we have {-dimensional} pre-trained embeddings for each word in the document, which are horizontally concatenated into the matrix , where  is the length of the document. 
Adjacent word embeddings are combined using a convolutional filter , where  is the filter width,  the size of the input embedding, and  the size of the filter output. At each step , we compute

where  denotes the convolution operator,  is an element-wise nonlinear transformation, and  is the bias. 
We additionally pad each side of the input with zeros so that the resulting matrix  has dimension .

\subsection{Attention}\label{sec:attn}
After convolution, the document is represented by the matrix . It is typical to reduce this matrix to a vector by applying pooling across the length of document, by selecting the maximum or average value at each row~\citep{kim2014convolutional}. However, our goal is to assign multiple labels (i.e., medical codes) for each document, and different parts of the base representation may be relevant for different labels. For this reason, we apply a per-label attention mechanism. An additional benefit is that it selects the -grams from the text that are most relevant to each predicted label.

Formally, for each label , we compute the matrix-vector product, , where  is a vector parameter for label . 
We then pass the resulting vector through a softmax operator, obtaining a distribution over locations in the document,

where , and  is the element-wise exponentiation of the vector . The attention vector  is then used to compute vector representations for each label,


As a baseline model, we instead use max-pooling to compute a single vector  for all labels,


\subsection{Classification}
Given the vector document representation , we compute a probability for label  using another linear layer and a sigmoid transformation:

where  is a vector of prediction weights, and  is a scalar offset. The overall model is illustrated in \autoref{fig:conv_attn}. 

\subsection{Training}
The training procedure minimizes the binary cross-entropy loss,

plus the L2 norm of the model weights, using the Adam optimizer~\cite{kingma2014adam}.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{img_final.pdf}
\caption{CAML architecture with per-label attention shown for one label. In a max-pooling architecture,  is mapped directly to the vector  by maximizing over each dimension.}
\label{fig:conv_attn}
\end{figure}

\subsection{Embedding label descriptions}\label{sec:desc_embed}

Due to the dimensionality of the label space, many codes are rarely observed in the labeled data.
To improve performance on these codes, we use text descriptions of each code from the \newcite{ICD10}.
Examples can be found in \autoref{tab:qual_example}, next to the code numbers.
We use these descriptions to build a secondary module in our network that learns to embed 
them as vectors. These vectors are then used as the target of regularization on the model parameters . If code  is rarely observed in the training data, this regularizer will encourage its parameters to be similar to those of other codes with similar descriptions.

The code embedding module consists of a max-pooling CNN architecture. Let  be a max-pooled vector, obtained by passing the description for code  into the module. Let  be the number of true labels in a training example. We add the following regularizing objective to our loss ,

where  is a tradeoff hyperparameter that calibrates the performance of the two objectives. We call this model variant Description Regularized-CAML (DR-CAML).
 
\section{Evaluation of code prediction}
This section evaluates the accuracy of code prediction, comparing our models against several competitive baselines.

\subsection{Datasets}
\begin{table*}
\centering
\begin{tabular}{llll}
\toprule
& \textbf{MIMIC-III full} & \textbf{MIMIC-III 50} & \textbf{MIMIC-II full} \\ \midrule
\# training documents                &  47,724               &    8,067                & 20,533                 \\
Vocabulary size             & 51,917                & 51,917                  & 30,688                 \\
Mean \# tokens per document & 1,485                 & 1,530                  & 1,138                  \\
Mean \# labels per document & 15.9                 & 5.7                   & 9.2                    \\
Total \# labels             & 8,922                   & 50                  & 5,031 \\ \bottomrule
\end{tabular}
\caption{Descriptive statistics for MIMIC discharge summary training sets.}
\label{tab:stats}
\end{table*}
 
MIMIC-III~\cite{johnson2016mimic} is an open-access dataset of text and structured records from a hospital ICU. Following previous work, we focus on discharge summaries, which condense information about a stay into a single document. In MIMIC-III, some admissions have addenda to their summary, which we concatenate to form one document.

Each admission is tagged by human coders with a set of ICD-9 codes, describing both diagnoses and procedures which occurred during the patient's stay. There are 8,921 unique ICD-9 codes present in our datasets, including 6,918 diagnosis codes and 2,003 procedure codes. Some patients have multiple admissions and therefore multiple discharge summaries; we split the data by patient ID, so that no patient appears in both the training and test sets.

In this full-label setting, we use a set of 47,724 discharge summaries from 36,998 patients for training, with 1,632 summaries and 3,372 summaries for validation and testing, respectively. 

\paragraph{Secondary evaluations}
For comparison with prior work, we also follow \newcite{shi2017towards} and train and evaluate on a label set consisting of the 50 most frequent labels. In this setting, we filter each dataset down to the instances that have at least one of the top 50 most frequent codes, and subset the training data to equal the size of the training set of \newcite{shi2017towards}, resulting in 8,067 summaries for training, 1,574 for validation, and 1,730 for testing.

We also run experiments with the MIMIC-II dataset, to compare with prior work by \newcite{baumel2017multi} and \newcite{perotte2013diagnosis}. We use the train/test split of \newcite{perotte2013diagnosis}, which consists of 20,533 training examples and 2,282 testing examples. Detailed  statistics for the three settings are summarized in \autoref{tab:stats}.


\paragraph{Preprocessing}
We remove tokens that contain no alphabetic characters (e.g., removing ``500'' but keeping ``250mg''), lowercase all tokens, and replace tokens that appear in fewer than three training documents with an `UNK' token. We pretrain word embeddings of size  using the word2vec CBOW method~\cite{mikolov2013efficient} on the preprocessed text from all discharge summaries. All documents are truncated to a maximum length of 2500 tokens.

\subsection{Systems}
We compare against the following baselines:
\begin{itemize}
\item a single-layer one-dimensional convolutional neural network \cite{kim2014convolutional};
\item a bag-of-words logistic regression model;
\item a bidirectional gated recurrent unit (Bi-GRU).\footnote{Our pilot experiments found that GRU was stronger than long short-term memory (LSTM) for this task.}
\end{itemize}
For the CNN and Bi-GRU, we initialize the embedding weights using the same pretrained word2vec vectors that we use for the CAML models. All neural models are implemented using PyTorch\footnote{\url{https://github.com/pytorch/pytorch}}. The logistic regression model consists of  binary one-vs-rest classifiers acting on unigram bag-of-words features for all labels present in the training data. If a label is not present in the training data, the model will never predict it in the held-out data. 

\paragraph{Parameter tuning}
We tune the hyperparameters of the CAML model and the neural baselines using the Spearmint Bayesian optimization package~\citep{NIPS2012_4522, NIPS2013_5086}.\footnote{\url{https://github.com/HIPS/Spearmint}}
We allow Spearmint to sample parameter values for the L2 penalty on the model weights  and learning rate , as well as filter size , number of filters , and dropout probability  for the convolutional models, and number of hidden layers  of dimension  for the Bi-GRU, using \pate{} on the MIMIC-III full-label validation set as the performance measure. We use these parameters for DR-CAML as well, and port the optimized parameters to the MIMIC-II full-label and MIMIC-III 50-label models, and manually fine-tune the learning rate in these settings. We select  for DR-CAML based on pilot experiments on the validation sets. Hyperparameter tuning is summarized in \autoref{tab:hyperparams}.
Convolutional models are trained with dropout after the embedding layer. We use a fixed batch size of 16 for all models and datasets. Models are trained with early stopping on the validation set; training terminates after the precision@8 does not improve for 10 epochs, and the model at the time of the highest precision@8 is used on the test set. 

\begin{table}
\centering
  \small
\begin{tabular}{lp{2.4cm}llll}
  \toprule
 & Range & CAML & CNN & Bi-GRU \\ \midrule
 & 50-500 & 50 & 500 & -- \\
 & 2-10 & 10 & 4 & -- \\
 & 0.2-0.8 & 0.2 & 0.2 & -- \\
 & 0, 0.001, 0.01, 0.1 & 0 & 0 & 0 \\
 & 0.0001, 0.0003, 0.001, 0.003 & 0.0001 & 0.003 & 0.003 \\
 & 1-4 & -- & -- & 1 \\
 & 32-512 & -- & -- & 512 \\ \bottomrule
\end{tabular}
\caption{Hyperparameter ranges and optimal values for each neural model selected by Spearmint.
  }
\label{tab:hyperparams}
\end{table} 
\subsection{Evaluation Metrics}
To facilitate comparison with both future and prior work, we report a variety of metrics, focusing on the micro-averaged and macro-averaged F1 and area under the ROC curve (AUC). Micro-averaged values are calculated by treating each (text, code) pair as a separate prediction. Macro-averaged values, while less frequently reported in the multi-label classification literature, are calculated by averaging metrics computed per-label. For recall, the metrics are distinguished as follows:

where TP denotes true positive examples and FN denotes false negative examples. Precision is computed analogously. The macro-averaged metrics place much more emphasis on rare label prediction. 

We also report precision at  (denoted as `P@n'), which is the fraction of the  highest-scored labels that 
are present in the ground truth. This is motivated by the potential use case as a decision support application, in which a user is presented with a fixed number of predicted codes to review. In such a case, it is more suitable to select a model with high precision than high recall. We choose  and  to compare with prior work~\cite{vani2017grounded,prakash2017condensed}. For the MIMIC-III full label setting, we also compute precision@15, which roughly corresponds to the average number of codes in MIMIC-III discharge summaries (\autoref{tab:stats}). 
 \begin{table*}
\centering
\begin{tabular}{lll|llll|ll}
\toprule
& \multicolumn{2}{c}{AUC} & \multicolumn{4}{c}{F1} & \multicolumn{2}{c}{P@n}  \\
Model & Macro & Micro & Macro & Micro & Diag & Proc & 8 & 15 \\ \midrule
Scheurwegs et. al \shortcite{scheurwegs2017selecting} & -- & -- & -- & -- & 0.428 & 0.555 & -- & --  \1em]
Logistic Regression & 0.829 &            0.864  &         0.477 & 0.533 & 0.546  \\
CNN                 & 0.876 &            0.907  & \textbf{0.576}* &             0.625  & \textbf{0.620} \\
Bi-GRU              & 0.828 & 0.868 & 0.484 & 0.549 & 0.591  \\ \midrule
CAML                & 0.875 & 0.909 & 0.532 & 0.614 & 0.609 \\
DR-CAML      & \textbf{0.884}* & \textbf{0.916} & \textbf{0.576}* & \textbf{0.633} &         0.618 \\
  \bottomrule
\end{tabular}
\caption{Results on MIMIC-III, 50 labels.}
\label{tab:mimic3-50}
\end{table*}
  
\section{Evaluation of Interpretability}
\label{sec:qual}

We now evaluate the explanations generated by CAML's attention mechanism, in comparison with three alternative heuristics. A physician was presented with explanations from four methods, using a random sample of 100 predicted codes from the MIMIC-III full-label test set. The most important -gram from each method was extracted, along with a window of five words on either side for context. We select  in this setting to emulate a span of attention over words likely to be given by a human reader. Examples can be found in \autoref{tab:qual_example}. Observe that the snippets may overlap in multiple words. We prompted the evaluator to select all text snippets which he felt adequately explained the presence of a given code, provided the code and its description, with the option to distinguish snippets as ``highly informative'' should they be found particularly informative over others. 

\subsection{Extracting informative text snippets}

\paragraph{CAML}
The attention mechanism allows us to extract -grams from the text that are most influential in the prediction of each label, by taking the argmax of the SoftMax output .

\paragraph{Max-pooling CNN}
We select the -grams that provide the maximum value selected by max-pooling at least once and weighting by the final layer weights. Defining an argmax vector  which results from the max-pooling step as

we can compute the importance of position  for label ,

We then select the most important -gram for a given label as .

\paragraph{Logistic regression}
The informativeness of each -gram with respect to label  is scored by the sum of the coefficients of the weight matrix for , over the words in the -gram. The top-scoring -gram is then returned as the explanation.

\paragraph{Code descriptions} Finally, we calculate a word similarity metric between each stemmed -gram and the stemmed ICD-9 code description. We compute the idf-weighted cosine similarity, with idf weights calculated on the corpus consisting of all notes and relevant code descriptions. We then select the argmax over -grams in the document, breaking ties by selecting the first occurrence. We remove those note-label pairs for which no -gram has a score greater than 0, which gives an ``unfair'' advantage to this baseline.


 \subsection{Results}
The results of the interpretability evaluation are presented in \autoref{tab:qual_results}. Our model selects the greatest number of ``highly informative'' explanations, and selects more ``informative'' explanations than both the CNN baseline and the logistic regression model. While the cosine similarity metric also performs well, the examples in \autoref{tab:qual_example} demonstrate the strengths of CAML in extracting text snippets in line with more intuitive explanations for the presence of a code. As noted above, there exist some cases, which we exclude, where the cosine similarity method is unable to provide any explanation, because no -grams in a note have a non-zero similarity for a given label description. This occurs for about 12 of all note-label pairs in the test set. 
\begin{table*}
\centering
\begin{tabular}{lll|ll|l}
\toprule
 & \multicolumn{2}{c}{AUC} & \multicolumn{2}{c}{F1} &   \\
Model & Macro & Micro & Macro & Micro & P@8   \\
\midrule
Flat SVM \cite{perotte2013diagnosis} & -- & -- & -- & 0.293 & -- \\
HA-GRU \cite{baumel2017multi} & -- & -- & -- & 0.366 & --   \1em]
Logistic Regression & 0.690 & 0.934 & 0.025 & 0.314 & 0.425  \\
CNN     &         0.742  &         0.941  &         0.030  &         0.332  &                 0.388   \\
Bi-GRU  &         0.780  &         0.954  &         0.024  &         0.359  &                 0.420  \\ \midrule
CAML    &         0.820  & \textbf{0.966}* &         0.048  &         0.442  & \textbf{0.523}* \\
DR-CAML & \textbf{0.826} & \textbf{0.966}* & \textbf{0.049} & \textbf{0.457}* &                 0.515  \\
\bottomrule
\end{tabular}
\caption{Results on MIMIC-II full, 5031 labels.}
\label{tab:mimic2}
\end{table*} 
\begin{table}
  \centering
\begin{tabular}{lll}
\toprule
& & Highly \\
  Method & Informative & informative \\
  \midrule
CAML & 46 & 22 \\
Code Descriptions & 48 & 20 \\
Logistic Regression & 41 & 18 \\
CNN & 36 & 13 \\
  \bottomrule
\end{tabular}
\caption{Qualitative evaluation results. The columns show the number of examples (out of 100) for which each method was selected as ``informative'' or ``highly informative''.}
\label{tab:qual_results}
\end{table}
  
\section{Related Work}

\paragraph{Attentional Convolution for NLP}

CNNs have been successfully applied to tasks such as sentiment classification~\cite{kim2014convolutional} and language modeling~\cite{dauphin2016language}. Our work combines convolution with attention~\cite{bahdanau2014neural,yang2016hierarchical} to select the most relevant parts of the discharge summary. Other recent work has combined convolution and attention~\cite[e.g.,][]{allamanis2016convolutional,yin2016abcnn,dos2016attentive,yin2017attentive}.
Our attention mechanism is most similar to those of \newcite{yang2016hierarchical} and \newcite{allamanis2016convolutional}, in that we use context vectors to compute attention over specific locations in the text. Our work differs in that we compute separate attention weights for each label in our label space, which is better tuned to our goal of selecting locations in a document which are most important for predicting specific labels.

\paragraph{Automatic ICD coding}
ICD coding is a long-standing task in the medical informatics community, which has been approached with machine learning and handcrafted methods~\cite{scheurwegs2015data}. Many recent approaches, like ours, use unstructured text data as the only source of information~\cite[e.g.,][]{kavuluru2015empirical,subotin2014system}, though some incorporates structured data as well~\cite[e.g.,][]{scheurwegs2017selecting,wang2016diagnosis}. 
Most previous methods have either evaluated only on a strict subset of the full ICD label space~\cite{wang2016diagnosis}, relied on datasets that focus on a subset of medical scenarios~\cite{zhang2017enhancing}, or evaluated on data that are not publicly available, making direct comparison difficult~\cite{subotin2016method}. A recent shared task for ICD-10 coding focused on coding of death certificates in English and French \cite{neveol2017clef}. This dataset also contains shorter documents than those we consider, with an average of 18 tokens per certificate in the French corpus. We use the open-access MIMIC datasets containing de-identified, general-purpose records of intensive care unit stays at a single hospital. 

\newcite{perotte2013diagnosis} use ``flat'' and ``hierarchical'' SVMs; the former treats each code as an individual prediction, while the latter trains on child codes only if the parent code is present, and predicts on child codes only if the parent code was positively predicted. \newcite{scheurwegs2017selecting} use a feature selection approach to ICD-9 and ICD-10 classification, incorporating structured and unstructured text information from EHRs. They evaluate over various medical specialties and on the MIMIC-III dataset. We compare directly to their results on the full label set of MIMIC-III.

Other recent approaches have employed neural network architectures. 
\newcite{baumel2017multi} apply recurrent networks with hierarchical sentence and word attention (the HA-GRU) to classify ICD9 diagnosis codes while providing insights into the model decision process. Similarly, \newcite{shi2017towards} applied character-aware LSTMs to generate sentence representations from specific subsections of discharge summaries, and apply attention to form a soft matching between the representations and the top 50 codes. \newcite{prakash2017condensed} use memory networks that draw from discharge summaries as well as Wikipedia, to predict top-50 and top-100 codes. Another recent neural architecture is the Grounded Recurrent Neural Network~\cite{vani2017grounded}, which employs a modified GRU with dimensions dedicated to predicting the presence of individual labels. We compare directly with published results from all of these papers, except \newcite{vani2017grounded}, who evaluate on only a 5000 code subset of ICD-9. Empirically, the CAML architecture proposed in this paper yields stronger results across all experimental conditions. We attribute these improvements to the attention mechanism, which focuses on the most critical features for each code, rather than applying a uniform pooling operation for all codes. We also observed that convolution-based models are at least as effective, and significantly more computationally efficient, than recurrent neural networks such as the Bi-GRU.

\paragraph{Explainable text classification}
A goal of this work is that the code predictions be explainable from features of the text. Prior work has also emphasized explainability. \newcite{lei2016rationalizing} model ``rationales'' through a latent variable, which tags each word as relevant to the document label. \newcite{li2016visualizing} compute the salience of individual words by the derivative of the label score with respect to the word embedding.
\newcite{ribeiro2016should} use submodular optimization to select a subset of features that closely approximate a specific classification decision (this work is also notable for extensive human evaluations). In comparison to these approaches, we employ a relatively simple attentional architecture; this simplicity is motivated by the challenge of scaling to multi-label classification with thousands of possible labels. Other prior work has emphasized the use of attention for highlighting salient features of the text~\cite[e.g.,][]{rush2015neural,rocktaschel2015reasoning}, although these papers did not perform human evaluations of the interpretability of the features selected by the attention mechanism.
\section{Conclusions and Future Work}
We present CAML, a convolutional neural network for multi-label document classification, which employs an attention mechanism to adaptively pool the convolution output for each label, learning to identify highly-predictive locations for each label. CAML yields strong improvements over previous metrics on several formulations of the ICD-9 code prediction task, while providing satisfactory explanations for its predictions. Although we focus on a clinical setting, CAML is extensible without modification to other multi-label document tagging tasks, including ICD-10 coding. We see a number of directions for future work. From the linguistic side, we plan to integrate the document structure of discharge summaries in MIMIC-III, and to better handle non-standard writing and other sources of out-of-vocabulary tokens. From the application perspective, we plan to build models that leverage hierarchy of ICD codes~\cite{choi2016doctor}, and to attempt the more difficult task of predicting diagnosis and treatment codes for \emph{future} visits from discharge summaries.  

\paragraph{Acknowledgments}
Helpful feedback was provided by the anonymous reviewers, and by the members of the Georgia Tech Computational Linguistics lab. The project was partially supported by project HDTRA1-15-1-0019 from the Defense Threat Reduction Agency, by the National Science Foundation under awards IIS-1418511 and CCF-1533768, by the National Institutes of Health under awards 1R01MD011682-01 and R56HL138415, by Children's Healthcare of Atlanta, and by UCB.

\bibliography{0-naaclhlt2018.bib}
\bibliographystyle{acl_natbib}

\end{document}
