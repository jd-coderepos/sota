[{'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'FRMT (Chinese - Mainland)', 'Metric': 'BLEURT', 'Score': '74.4'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'FRMT (Chinese - Mainland)', 'Metric': 'BLEURT', 'Score': '72.3'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'FRMT (Chinese - Mainland)', 'Metric': 'BLEURT', 'Score': '70.3'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'FRMT (Chinese - Taiwan)', 'Metric': 'BLEURT', 'Score': '72.0'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'FRMT (Chinese - Taiwan)', 'Metric': 'BLEURT', 'Score': '68.6'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'FRMT (Chinese - Taiwan)', 'Metric': 'BLEURT', 'Score': '68.5'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'FRMT (Portuguese - Brazil)', 'Metric': 'BLEURT', 'Score': '81.1'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'FRMT (Portuguese - Brazil)', 'Metric': 'BLEURT', 'Score': '80.2'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'FRMT (Portuguese - Brazil)', 'Metric': 'BLEURT', 'Score': '78.5'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'FRMT (Portuguese - Portugal)', 'Metric': 'BLEURT', 'Score': '78.3'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'FRMT (Portuguese - Portugal)', 'Metric': 'BLEURT', 'Score': '76.1'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'FRMT (Portuguese - Portugal)', 'Metric': 'BLEURT', 'Score': '75.3'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MGSM', 'Metric': 'Average (%)', 'Score': '87.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MGSM', 'Metric': 'Average (%)', 'Score': '72.2'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '81.2'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '78.3'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiRC', 'Metric': 'F1', 'Score': '88.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiRC', 'Metric': 'F1', 'Score': '84.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiRC', 'Metric': 'F1', 'Score': '84.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'PIQA', 'Metric': 'Accuracy', 'Score': '85.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'PIQA', 'Metric': 'Accuracy', 'Score': '83.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'PIQA', 'Metric': 'Accuracy', 'Score': '82.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'BoolQ', 'Metric': 'Accuracy', 'Score': '90.9'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'BoolQ', 'Metric': 'Accuracy', 'Score': '88.6'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'BoolQ', 'Metric': 'Accuracy', 'Score': '88.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'OpenBookQA', 'Metric': 'Accuracy', 'Score': '58.5'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'OpenBookQA', 'Metric': 'Accuracy', 'Score': '57.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'OpenBookQA', 'Metric': 'Accuracy', 'Score': '56.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'StrategyQA', 'Metric': 'Accuracy', 'Score': '90.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'DROP Test', 'Metric': 'F1', 'Score': '85.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TriviaQA', 'Metric': 'EM', 'Score': '86.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TriviaQA', 'Metric': 'EM', 'Score': '81.7'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TriviaQA', 'Metric': 'EM', 'Score': '75.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Story Cloze', 'Metric': 'Accuracy', 'Score': '87.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Story Cloze', 'Metric': 'Accuracy', 'Score': '86.7'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Story Cloze', 'Metric': 'Accuracy', 'Score': '85.6'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '37.5'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '32.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '25.3'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'COPA', 'Metric': 'Accuracy', 'Score': '96.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'COPA', 'Metric': 'Accuracy', 'Score': '90.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'COPA', 'Metric': 'Accuracy', 'Score': '89.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WebQuestions', 'Metric': 'EM', 'Score': '28.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WebQuestions', 'Metric': 'EM', 'Score': '26.9'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WebQuestions', 'Metric': 'EM', 'Score': '21.8'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Movie Recommendation)', 'Metric': 'Accuracy', 'Score': '94.4'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Movie Recommendation)', 'Metric': 'Accuracy', 'Score': '93.6'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Hyperbaton)', 'Metric': 'Accuracy', 'Score': '84.8'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Hyperbaton)', 'Metric': 'Accuracy', 'Score': '82.4'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Ruin Names)', 'Metric': 'Accuracy', 'Score': '90'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Ruin Names)', 'Metric': 'Accuracy', 'Score': '83.6'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Navigate)', 'Metric': 'Accuracy', 'Score': '91.2'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Navigate)', 'Metric': 'Accuracy', 'Score': '68.8'}}, {'LEADERBOARD': {'Task': 'Cross-Lingual Question Answering', 'Dataset': 'TyDiQA-GoldP', 'Metric': 'F1', 'Score': '73.6'}}, {'LEADERBOARD': {'Task': 'Cross-Lingual Question Answering', 'Dataset': 'TyDiQA-GoldP', 'Metric': 'F1', 'Score': '73.3'}}, {'LEADERBOARD': {'Task': 'Math Word Problem Solving', 'Dataset': 'MATH', 'Metric': 'Accuracy', 'Score': '48.8'}}, {'LEADERBOARD': {'Task': 'Math Word Problem Solving', 'Dataset': 'MATH', 'Metric': 'Accuracy', 'Score': '34.3'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'HumanEval', 'Metric': 'Pass@1', 'Score': '37.6'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'HumanEval', 'Metric': 'Pass@100', 'Score': '88.4'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Date Understanding)', 'Metric': 'Accuracy', 'Score': '91.2'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Date Understanding)', 'Metric': 'Accuracy', 'Score': '74.0'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Challenge)', 'Metric': 'Accuracy', 'Score': '95.1'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Challenge)', 'Metric': 'Accuracy', 'Score': '69.2'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Challenge)', 'Metric': 'Accuracy', 'Score': '64.9'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Challenge)', 'Metric': 'Accuracy', 'Score': '59.6'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'CommonsenseQA', 'Metric': 'Accuracy', 'Score': '90.4'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Causal Judgment)', 'Metric': 'Accuracy', 'Score': '62.0'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Causal Judgment)', 'Metric': 'Accuracy', 'Score': '58.8'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Easy)', 'Metric': 'Accuracy', 'Score': '89.7'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Easy)', 'Metric': 'Accuracy', 'Score': '88.0'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Easy)', 'Metric': 'Accuracy', 'Score': '85.6'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'WinoGrande', 'Metric': 'Accuracy', 'Score': '83.0'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'WinoGrande', 'Metric': 'Accuracy', 'Score': '79.2'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'WinoGrande', 'Metric': 'Accuracy', 'Score': '77.9'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Sports Understanding)', 'Metric': 'Accuracy', 'Score': '98'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Sports Understanding)', 'Metric': 'Accuracy', 'Score': '90.8'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Disambiguation QA)', 'Metric': 'Accuracy', 'Score': '78.8'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Disambiguation QA)', 'Metric': 'Accuracy', 'Score': '77.6'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ReCoRD', 'Metric': 'F1', 'Score': '93.8'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ReCoRD', 'Metric': 'F1', 'Score': '92.4'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ReCoRD', 'Metric': 'F1', 'Score': '92.1'}}, {'LEADERBOARD': {'Task': 'Word Sense Disambiguation', 'Dataset': 'Words in Context', 'Metric': 'Accuracy', 'Score': '66.8'}}, {'LEADERBOARD': {'Task': 'Word Sense Disambiguation', 'Dataset': 'Words in Context', 'Metric': 'Accuracy', 'Score': '52.0'}}, {'LEADERBOARD': {'Task': 'Word Sense Disambiguation', 'Dataset': 'Words in Context', 'Metric': 'Accuracy', 'Score': '50.6'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'CommitmentBank', 'Metric': 'Accuracy', 'Score': '87.5'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'CommitmentBank', 'Metric': 'Accuracy', 'Score': '82.1'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'CommitmentBank', 'Metric': 'Accuracy', 'Score': '80.4'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A1', 'Score': '73.1'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A2', 'Score': '63.4'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A3', 'Score': '67.1'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A1', 'Score': '58.1'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A2', 'Score': '49.5'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A3', 'Score': '54.5'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A1', 'Score': '53.1'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A2', 'Score': '48.8'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A3', 'Score': '53.2'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '81.9'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '79.3'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '78.7'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Accuracy', 'Score': '86.9'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Accuracy', 'Score': '83.7'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Accuracy', 'Score': '80.7'}}, {'LEADERBOARD': {'Task': 'Sarcasm Detection', 'Dataset': 'BIG-bench (SNARKS)', 'Metric': 'Accuracy', 'Score': '84.8'}}, {'LEADERBOARD': {'Task': 'Sarcasm Detection', 'Dataset': 'BIG-bench (SNARKS)', 'Metric': 'Accuracy', 'Score': '78.7'}}, {'LEADERBOARD': {'Task': 'Cross-Lingual Transfer', 'Dataset': 'XCOPA', 'Metric': 'Accuracy', 'Score': '94.4'}}, {'LEADERBOARD': {'Task': 'Coreference Resolution', 'Dataset': 'WSC', 'Metric': 'Accuracy', 'Score': '88.1'}}, {'LEADERBOARD': {'Task': 'Coreference Resolution', 'Dataset': 'WSC', 'Metric': 'Accuracy', 'Score': '86.9'}}, {'LEADERBOARD': {'Task': 'Coreference Resolution', 'Dataset': 'WSC', 'Metric': 'Accuracy', 'Score': '84.6'}}, {'LEADERBOARD': {'Task': 'Text Summarization', 'Dataset': 'X-Sum', 'Metric': 'ROUGE-2', 'Score': '23.2'}}, {'LEADERBOARD': {'Task': 'Text Summarization', 'Dataset': 'X-Sum', 'Metric': 'ROUGE-2', 'Score': '17.2'}}, {'LEADERBOARD': {'Task': 'Text Summarization', 'Dataset': 'X-Sum', 'Metric': 'ROUGE-2', 'Score': '16.9'}}, {'LEADERBOARD': {'Task': 'Toxic Comment Classification', 'Dataset': 'Civil Comments', 'Metric': 'AUROC', 'Score': '0.8535'}}, {'LEADERBOARD': {'Task': 'Toxic Comment Classification', 'Dataset': 'Civil Comments', 'Metric': 'AUROC', 'Score': '0.7596'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '87.4'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '86.7'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '85.6'}}, {'LEADERBOARD': {'Task': 'Arithmetic Reasoning', 'Dataset': 'GSM8K', 'Metric': 'Accuracy', 'Score': '91.0'}}, {'LEADERBOARD': {'Task': 'Arithmetic Reasoning', 'Dataset': 'GSM8K', 'Metric': 'Accuracy', 'Score': '80.7'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Formal Fallacies Syllogisms Negation)', 'Metric': 'Accuracy', 'Score': '64.8'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Formal Fallacies Syllogisms Negation)', 'Metric': 'Accuracy', 'Score': '57.2'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Temporal Sequences)', 'Metric': 'Accuracy', 'Score': '100'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Temporal Sequences)', 'Metric': 'Accuracy', 'Score': '96.4'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Reasoning About Colored Objects)', 'Metric': 'Accuracy', 'Score': '91.2'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Reasoning About Colored Objects)', 'Metric': 'Accuracy', 'Score': '61.2'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Logic Grid Puzzle)', 'Metric': 'Accuracy', 'Score': '42.4'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Logic Grid Puzzle)', 'Metric': 'Accuracy', 'Score': '36.5'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Penguins In A Table)', 'Metric': 'Accuracy', 'Score': '84.9'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Penguins In A Table)', 'Metric': 'Accuracy', 'Score': '65.8'}}]
