\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage{array}
\usepackage{color}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{4} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{DupNet: Towards Very Tiny Quantized CNN  with Improved Accuracy for Face Detection}

\author{Hongxing Gao, Wei Tao, Dongchao Wen, Junjie Liu\\
	Canon Information Technology (Beijing) Co., LTD\\
{\tt\small \{gaohongxing,taowei,wendongchao, liujunjie\}@canon-ib.com.cn}
\and
	Tse-Wei Chen, Kinya Osa, Masami Kato\\
	Device Technology Development Headquarters, Canon Inc.\\
{\tt\small twchen@ieee.org}
}

\maketitle

\ifcvprfinal\thispagestyle{empty}\fi

\begin{abstract}
		
Deploying deep learning based face detectors on edge devices is a challenging task due to the limited computation resources. Even though binarizing the weights of a very tiny network gives impressive compactness on model size (\eg 240.9 KB for IFQ-Tinier-YOLO), it is not tiny enough to fit in the embedded devices with strict memory constraints. In this paper, we propose DupNet which consists of two parts. Firstly, we employ weights with duplicated channels for the weight-intensive layers to reduce the model size. Secondly, for the quantization-sensitive layers whose quantization causes notable accuracy drop, we duplicate its input feature maps. It allows us to use more weights channels for convolving more representative outputs. Based on that, we propose a very tiny face detector, DupNet-Tinier-YOLO, which is 6.5 times smaller on model size and 42.0\% less complex on computation and meanwhile achieves 2.4\% higher detection than IFQ-Tinier-YOLO. Comparing with the full precision Tiny-YOLO, our DupNet-Tinier-YOLO gives 1,694.2 and 389.9 times savings on model size and computation complexity respectively with only 4.0\% drop on detection rate (0.880 vs. 0.920). Moreover, our DupNet-Tinier-YOLO is only 36.9 KB, which is the tiniest deep face detector to our best knowledge.
	
\end{abstract}

\section{Introduction}\label{sec:intro}




Deep neural networks have demonstrated impressive accuracy in many computer vision applications such as image classification, object detection and recognition, semantics segmentation, etc. However, their increasing computation cost leads to the requirement of high-end devices such as GPU for real-time inference. It has been a challenging task to deploy the deep network based face detector on the edge devices due to their limited resources (e.g. memory size and computation power). To deploy the deep models on the edge devices, lots of approaches have been proposed, such as network pruning~\cite{Prun1,Prun2}, efficient architecture design (e.g. MobileNet~\cite{MobileNetv1,MobileNetv2}) and quantized networks~\cite{XNOR,HWGQ}. Especially, for the embedded devices, quantized networks are particularly attractive because of their impressive compression ratio (\eg 32 times savings on model size) and easy conversion for fixed-point representation. For instance, IFQ-Net~\cite{IFQNet} designs a tiny fixed-point face detector (240.9 KB) through slimming, quantizing and fixed-point converting the layers of Tiny-YOLO network~\cite{TinyYOLO}. Even though the fixed-point converting is lossless,  the accuracy drop caused by slimming and quantizing is still notable.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=\linewidth]{pipeline.pdf}
	\end{center}
	\caption{Strategies for DupNet. We duplicate the input feature maps of the  quantization-sensitive layers which usually locate in the lower part of a network to improve the accuracy. Besides, to compress the overall model size, it employs duplicated weights for weight-intensive layers that usually locate in the upper part.}
	\label{fig:pipeline}
\end{figure}



In this paper, to compress the model size and meanwhile improve the accuracy of a quantized network, we propose DupNet as shown in Figure~\ref{fig:pipeline}. Firstly, to compress the model size, DupNet employs weights with duplicated channels for weight-intensive layers which usually are the upper layers. On the other hand, to improve the accuracy, DupNet duplicates the input feature maps and thus uses more weights channels for the quantization-sensitive layers (usually locate in the lower part of the network) whose quantization causes significant accuracy drop. In details, to further compress a quantized network, we force the weights of weight-intensive layers to have identical channels. As shown in Figure~\ref{fig:WXdup}, to generate such identical channels,  we employ template weights   which has less channels than input feature maps and duplicate it to  for proper convolution. During inference time,   can be restored from the template weights . Consequently, model size savings can be easily achieved by only storing .


Another major issue of the quantized network is that the accuracy is usually downgraded by a large margin. For example, in XNOR-Net~\cite{XNOR}, binarizing both weights and feature maps of AlexNet~\cite{AlexNet} leads to 12\% accuracy drop on ImageNet dataset~\cite{ImageNet}. In the case of face detection, we observe that the accuracy drop is mainly caused by the quantization of several specific layers, named as quantization-sensitive layers (see Section~\ref{subsec:exp-Xdup}). Usually, they are the lower layers which only have small amount of output feature maps. Thus, quantizing them into extremely low-bits severely harms the representative power of the output features. To address the problem, one may simply employ more feature maps or quantize them into higher bits, both of which would increase the memory usage on feature maps.

In this paper, we propose to further duplicate the input feature maps of the quantization-sensitive layers to improve its accuracy (Figure~\ref{fig:WXdup}). It is true that simply duplicating the feature maps does not introduce extra information. However, it allows us to use weights with more channels (not identical) for convolving more representative outputs. The advantage of our method is that it does not require extra memory on feature maps which is a critical issue for the embedded devices. Nevertheless, it does increase the memory usage on weights. However, as will be demonstrated in Section~\ref{subsec:exp-Xdup}, we experimentally found out that the quantization-sensitive layers are usually the lower layers of a network which only have small amount of weights. Consequently, such memory increase does not affect the network much. 

In summary, we propose DupNet which employs duplicated weights for the weight-intensive layers and duplicates the input feature maps for the quantization-sensitive layers of a quantized network. The benefits of our proposal are two-folds: 1) it reduces the model size of a quantized network by duplicated weights for weight-intensive layers; 2) it increases the accuracy through duplicating the input feature maps of its quantization-sensitive layers. Based on the DupNet, we design a very tiny quantized CNN with impressive improvement on accuracy for face detection. The model size of our network is only 36.9 KB which is the tiniest deep learning based face detector to our best knowledge.





\section{Related Work}\label{sec:RelatedWork}







\subsection{Face Detection}
Two main approaches, namely one-stage and two-stage methods,  have been successfully inherited from object detection domain for face detection. Two-stage methods follow a common two steps pipeline: 1) generates a set of region proposals with their local features; 2) pass them to a network for classifying detected objects and regressing their bounding boxes. For example, Faster-RCNN~\cite{FasterRCNN} proposes an efficient Region Proposal Network (RPN) to generate region proposals and then use Fast-RCNN network to refine the proposals. To improve the speed of Faster R-CNN,  RFCN~\cite{RFCN} proposes to share RPN network and Fast-RCNN network. In order to further improve the speed, Li~\etal~\cite{LightHeadRCNN} proposes Light Head RCNN, which employs light weight head network to reduce the computation complexity. To speedup the R-FCN network for detecting 3000 object classes, Singh~\etal ~\cite{RFCN-3000} propose to only employ position-sensitive feature maps for several predefined super-classes.

On the other hand, one-stage approaches usually employ a single network to classify and regress the objects~\cite{SSD,YOLO,YOLOv2}and thus usually can run faster. For example, YOLO~\cite{YOLO} predicts 2 bounding boxes in each of the  grids for VOC object detection~\cite{VOC}. Furthermore, YOLOv2~\cite{YOLOv2} employs fully convolution network that results in  grids (m, n are the width and height of the output feature) and uses predefined anchors to better predict the bounding boxes of the objects. In~\cite{DetNet}, Li~\etal propose a backbone network to improve the accuracy by maintaining high resolution for feature maps and reduce the computation complexity by decreasing the width of upper layers.


In spite of the enormous progresses for reducing the complexity of two-stage methods, such region proposal based frameworks may be expensive for embedded devices because they usually need to store the features from previous layers. Therefore, following~\cite{IFQNet}, we employ  the widely used one-stage pipeline YOLOv2 for our face detector.






\subsection{Deep Network Compression}

To reduce the computation cost of the deep models, many  approaches have been studied. One way is to design novel efficient architectures. For example, by replacing a standard convolution layer by the combination of a depth-wise and a point-wise (11) convolution layer, MobileNets~\cite{MobileNetv1} reduces the weights and computation by 89 times. Similarly, LBCNN~\cite{LBCNN} employs predefined binary patterns for the depthwise convolution and shares those patterns over multiple layers for further compression.

Another direction for designing a compact model is to compress the network through pruning, quantization, etc. Pruning methods eliminate the less important connections and fine-tune the pruned network to narrow down the accuracy drop. For example, in~\cite{QunatMimic}, Wei~\etal reduce the input and output channels of each layer of VGG by 32 times and design a  very small detector whose size is only 132KB. In contrast, quantization approaches aim to quantize the float data of a network into low-bits data. For example, XNOR-Net~\cite{XNOR} and HWGQ-Net~\cite{HWGQ} achieves 32 times savings on model size via binarizing (1-bit) the network weights. In addition to the quantized weights,  further quantizing feature maps into low-bits data can reduce the feature maps memory usage and meanwhile increase the inference speed. For example, XNOR-Net which quantizes both weights and feature maps into 1-bit is theoretically 64 times faster than its full precision counterpart. Furthermore, for embedded devices such as FPGA and ASIC, quantization network is particularly attractive because it leads to higher throughput and lower power consumption through converting the network into a fixed-point one.  

One interesting topic is about further exploring the redundancy and  compressing the quantized network. For example, in~\cite{ToCompress}, various networks (VGG16, MobileNet) are firstly quantized to 8-bit data and then further pruned by 24\%. Similarly, Li and Ren~\cite{BNNPrune} explores the redundancy of a Binarized Neural Network (BNN) and further compresses the model size by 3.9 times through bit-level data pruning. 

Different with methods that explore the redundancy through carefully tuned strategies, we propose to simply employ duplicated weights which contain lots of identical channels for the weight-intensive layers. Since the duplicated weights can be easily restored from the template weights which contain all the non-identical channels, it is sufficient to only store the template weights in the memory during inference time.


\subsection{Accuracy Improvement for Quantized Network}

Even though quantizing the network into low-bits data leads to promising reduction on computation cost, accuracy drop is usually observed. As demonstrated in~\cite{Google8bit,NvidiaInt8}, quantizing the network data into 8-bits  only leads to minor accuracy drop on ImageNet classification task~\cite{ImageNet}. Nevertheless, quantizing the network into lower bits usually results in notable accuracy degradation.  For example, XNOR-Net~\cite{XNOR} which quantizes both its weights and feature maps into 1-bit and thus observes a 12.6\% accuracy drop (56.8\% vs. 44.2\%). Based on that, HWGQ-Net~\cite{HWGQ} gains 8.2\% accuracy back through using 2-bits on its feature maps (52.4\% on ImageNet). Additionally, for object detection tasks, 3\%5\% drop is observed in~\cite{SQuantizer}.

To improve the accuracy of quantized networks, lots of efforts have been done on better strategies for training the networks. INQ~\cite{INQ} proposes to incrementally quantize the weights and achieves more accurate quantized networks through iterative fine-tuning. Similarly, in~\cite{PQ}, the weights and activations are firstly quantized to 16-bits, then to 4-bits and at the end to 2-bits. PACT~\cite{PACT} optimizes the clipping thresholds for better quantization on feature maps. Besides, knowledge distillation technology additionally uses the knowledge from teacher network to guide the training process of student network~\cite{QunatMimic}. 

To narrow down the accuracy drop caused by the network quantization, we propose to duplicate the feature maps of its quantization-sensitive layers  which allows us to use weights with more channels for convolving more representative features. The advantage of our method is that it gives significant accuracy improvement without increasing the feature maps memory usage.

\section{Our Approach: DupNet}\label{Sec:hwgq}
To further compress the model size and improve the accuracy of a quantized network for face detection, we propose to employ weights with duplicated channels in the weight-intensive layers and duplicate the input feature maps of its quantization-sensitive layers. 

\begin{figure*}[!htb]
	\centering
	\includegraphics[width=\linewidth]{WX_dup.pdf}
	
	\caption{Illustration of the forward and backward pass for both duplicated weights and feature maps duplication.}
	\label{fig:WXdup}
\end{figure*}


\subsection{Duplicated Weights for Model Compression}\label{subSec:Wdup}
As discussed in~\cite{BNNPrune}, even though the network data is quantized into very low-bits data, redundancy still exists. In this section, we will illustrate our method which employs template weights with less channels and thus less redundancy. During convolution process, we duplicate the template weights to get required channels to convolve with the input feature maps.

We assume the quantized network only employs  convolution which means that the input feature maps and weights are quantized to 2-bits and 1-bit respectively. We represent its weights and feature maps as  and , where , ,  are the number of channels, the width and height of input feature maps,  is the kernel size of the convolution\footnote{Even though we use  for explanation, our method is also able to compress the convolution with other kernel size (e.g.  and ). }. To compress the model size, we define a weights template  which has less channels (). However, it can not be used to convolve with  since they have different number of channels. To solve such problem, we duplicate the channels of template weights into required number and obtain duplicated weights .

During training process, it is straightforward to compute the gradient of duplicated weights  by employing standard convolution, where  represents the loss of the network for given training samples. As shown in Figure~\ref{fig:WXdup}, to compute the gradient of template weights , we average the corresponding channels of the . Specifically, we assume that  and  for 4 times compression, and the 0th, 128th, 256th and 384th channels of  are duplicated from the 0th channel of .  In order to compute the 0th channel of the gradient , we element-wisely average the 0th, 128th, 256th and 384th channels of . Such gradient averaging process is repeated for computing the 1127th channels of . At the end, the template weights  can be learned by iteratively updating it with its gradients  using SGD optimization.

Since the duplicated weights  can be easily restored from the templates weights , it is only necessary to store . Thus,   times model size reduction can be achieved. Nevertheless, our compression method may harm the accuracy because the duplicated weights contains less non-identical channels. Considering that the model size is usually dominated by the weight-intensive layers, we only apply our compression method on these layers to prevent significant accuracy drop.

Furthermore, given the fact that many channels of the duplicated weights are identical, we can reduce the computation complexity as follows. We split duplicated weights  into  while each of them is identical with the template weights . Similarly, the feature maps  can also be accordingly split into  which are non-identical. We use    and  to represent convolution operation and a function that concatenate its members along channel axis respectively. Then, . Consequently, the convolution  can be alternatively computed by ,  where .  The overall computation complexity of  and  is much smaller than .



\subsection{Duplicate Feature Maps to Improve Accuracy}\label{subSec:Xdup}

The quantized networks quantize their full precision data into low-bits data thus usually leads to notable accuracy drop. In the following, we further improve the degraded accuracy for a very tiny quantized face detector.


As discussed in Section~\ref{sec:intro}, the accuracy degradation of quantized face detector is mainly caused by the weak representation power of the quantized output feature maps of its quantization-sensitive layers. To enhance the representative power of their output features, one straightforward way is to simply make these layers wider (more input feature maps). However, it significantly increases the memory usage on feature maps.  Besides, observing that these layers usually locate in the lower part of a network, such memory increase may cause critical issue because their feature maps hold high resolution (). In contrast, the number of channels () is usually small and thus their weights () is small too. Consequently, we propose to  duplicate the input feature maps and employ more weights channels for better output features. As shown in Figure~\ref{fig:WXdup}, the input feature maps are duplicated 4 times and thus the weights size is also increased to . For the backward pass during training time, to obtain the gradients , we firstly compute the gradients of duplicated feature maps , and then average every 4 of its corresponding channels that are identical in . At the end, the gradients   are propagated back to its previous layers.

Comparing  with the strategy that simply uses more input feature maps, our method does not require extra memory for feature maps. Thanks to the increased channels of input feature maps, we can employ more channels of weights ( vs. ). Consequently, the input feature maps are convolved with more patterns and thus leads to more representative power on the resulted features. Even though our method increases the weights size, such cost increase has limited influence on the overall model cost because the weights size of these layers are usually very small (see Table~\ref{tab:IFQ_cost}). Similar with the theory that is explained in Section~\ref{subSec:Wdup}, one also can achieve speedup by replacing   with ,  where  can be obtained by summing the corresponding channels of .

\section{Experimental Results}

To design a very tiny CNN for face detection, we borrow the compression ideas from IFQ-Tinier-YOLO~\cite{IFQNet} which compresses Tiny-YOLO network by 260 times through halving the filter numbers of all convolution layers, replacing one 33 layer which contains massive parameters by 11 kernels and binarizing the weights in all layers. Moreover, we further halve their filter number and apply the proposed duplicated weights for its weight-intensive layers (Conv6Conv8) and achieve 6.7 times further savings on model size. Besides, we will demonstrate that duplicating the input feature maps of its quantization-sensitive layers can significantly improve the accuracy.

We employ WiderFace~\cite{widerface} training images to train our models using Darknet framework~\cite{darknet}. For fair comparison, all the models are trained with the same strategies which are: 1) training the models by 100k iterations with SGD optimization method; 2) the learning rate is initially set to 0.01 and downscaled by a factor of 0.1 at the -th, -th, -th and -th iteration respectively; 3) all the models are trained from scratch. Furthermore, we use FDDB~\cite{FDDB} benchmark which contains 5,171 faces within 2,845 test images to evaluate the accuracy of our face detectors. Inheriting from~\cite{IFQNet}, we use the detection rate when 284 false positive faces are reached (averagely allowing 1 false positive in every 10 images) as the evaluation metric.


\subsection{Model Compression} \label{subsec:exp-Wdup}

To further compress model size of IFQ-Tinier-YOLO, we analyze the weights size for each of its layer. Meanwhile, to measure the computation complexity, we borrow the term \#FLOPs\footnote{As stated in ~\cite{XNOR}, for the 64-bit based computing devices, 64 Multi-Adds of the  convolution are equivalent to 1 FLOP. Similarly, we assume that 32 Multi-Adds of the  convolution and 8 Multi-Adds of the  (Conv1) equal to 1 FLOP respectively.} (Floating-point operations) which is generally used for full precision networks~\cite{FLOPs}. Nevertheless, it is worthy to point out that our network can be lossless converted to fixed-point network and thus does not require any floating-point operation.


\begin{table}[!h]
	\centering
	\caption{IFQ-Tinier-YOLO inference costs in terms of weights size and  \#FLOPs (million) for measuring computation complexity.}
	\label{tab:IFQ_cost}
	\setlength\tabcolsep{1.5pt}
	\begin{tabular}{c|c|c|c|c}
		
		\hline
		\multirow{2}{*}{} &\multirow{2}{*}{\shortstack{Kernel \\size (W)}} & \multirow{2}{*}{\shortstack{Feature \\size (X)}}  &\multirow{2}{*}{\shortstack{\#FLOPs\million)}} &\multirow{2}{*}{\shortstack{Model \\size(KB)}} &\multirow{2}{*}{\shortstack{Detection \\rate}}\\
		\cline{1-4}
		{1st}    &{2nd-3rd}       &{4th-8th} &{9th} &  &  &  \\
		
		\hline \hline		
		&              &              &               &1,338.9    &2,637.3   &0.902\\
		&              &  &               &422.2      &366.6     &0.880\\
&  &  &               &215.9      &344.8     &0.858\\
		  &  &  &               &146.0      &344.0     &0.845\\
		  &  &  &   &49.3       &82.4      &0.837\\
		\hline
	\end{tabular}
\end{table}

As shown in Table~\ref{tab:exp_lwb}, we firstly quantize Conv4Conv8 convolution layers of a full precision counterpart of IFQ-Tinier-YOLO network but with halved filter number in Conv6Conv8. In this subsection, to demonstrate the accuracy improvement effect of duplicating the input feature maps, the duplicated weights based compression is not applied. As shown in Table~\ref{tab:exp_lwb}, quantizing Conv4Conv8 leads to 3.2 and 7.2 times reduction on MFLOPs (million of FLOPs) and model size respectively while the detection rate only drops by 2.2\%. Nevertheless, progressively quantizing the Conv3Conv2 and then Conv1 causes  2.2\%, 1.3\% accuracy drop respectively but gives much less reductions on inference cost.   Thus, we define the Conv1Conv3 as quantization-sensitive layers of the network. We think the reason is that they only contain limited number of feature maps. Consequently, quantizing them severely damages the representative power of their output features. At the end, quantizing Conv9, resulting in a fully quantized Tinier-YOLO model, further gives remarkable savings on computation cost while the accuracy is only decreased by 0.8\%. 

\begin{table}[!h]
	\centering
	\caption{Illustration of performance improvement for progressively duplicating the input feature maps of the Conv2-3, Conv1 and Conv9 of fully quantized Tinier-YOLO face detector.}
	\label{tab:exp_X_dup}
	\setlength\tabcolsep{1.5pt}
	\begin{tabular}{c|c|c|c|c|c|c}
		
		\hline
		\multicolumn{4}{c|}{Feature maps duplication?} &\multirow{2}{*}{\shortstack{\#FLOPs\million)}}     &{\shortstack{Model \\size(KB)}}   &\shortstack{Detection\\ rate} \\
		
		\hline \hline
		
		Tiny-YOLO                     &24,407         &62,516            &0.920            \\
		Tinier-YOLO                   &3,213          &7,707             &0.902            \\
		IFQ-Tinier-YOLO~\cite{IFQNet} &107.9          &240.9             &0.835           \\
		DupNet                       &62.6           &36.9              &{0.859}  \\
		DupNet+PACT                  &62.6           &36.9              &{0.880}  \\
DupNet-L                       &95.7           &45.4              &{0.884}  \\
		DupNet-L+PACT                  &95.7           &45.4              &{0.906}  \\
		
		


		\hline
	\end{tabular}
\end{table}

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{exp_ROC_curve.pdf}
	\end{center}
	\caption{Comparison on the performance of the face detectors  in terms of ROC curves of FDDB dataset~\cite{FDDB}.}
	\label{fig:exp_alg_comp}
\end{figure}

\begin{figure*}[!htb]
	\centering
	\includegraphics[width=\linewidth]{exp_wider_face.pdf}
	\caption{Qualitative results of the proposed DupNet-Tinier-YOLO-L face detector on Wider Face dataset~\cite{widerface}. }
	\label{fig:exp_widerFace}
\end{figure*}

As shown in Table~\ref{tab:exp_alg_compare}, comparing with the IFQ-Tinier-YOLO~\cite{IFQNet}, our DupNet-Tinier-YOLO (represented as ``DupNet") gives 6.5 times savings on model size and 42.0\%  less MFLOPs. Meanwhile, it also gives 2.4\% improvement on detection rate. To further improve the detection rate with acceptable cost increase, we design DupNet-Tinier-YOLO-L (marked as ``DupNet-L")  which additionally duplicates the input feature maps fo Conv1 and Conv9 by 4 and 2 respectively. As shown in Table~\ref{tab:exp_alg_compare},  DupNet-Tinier-YOLO-L further gives 2.5\% higher detection rate. Nevertheless, the model size and \#FLOPs are increased to 45.4 KB and 95.7 MFLOPs respectively, both of which are still smaller than IFQ-Tinier-YOLO.

Furthermore, we employ PACT~\cite{PACT} to train optimal clipping thresholds for feature maps quantization to improve the accuracy. As illustrated in Table~\ref{tab:exp_alg_compare}, PACT algorithm improves the DupNet-Tinier-YOLO and DupNet-Tinier-YOLO-L by 2.1\% and 2.2\% respectively. Comparing with Tiny-YOLO network, our DupNet-Tinier-YOLO achieves 389.9 and 1694.2 times reduction on \#FLOPs and model size respectively while the detection rate is only decreased by 4.0\%. On the other hand, the accuracy of DupNet-Tinier-YOLO-L is only decreased by 1.4\% while the inference cost reduction is kept impressive. Besides, we compare their accuracy in terms of ROC curves in Figure~\ref{fig:exp_alg_comp}.


To demonstrate the performance of our detector on more challenging faces, we also test DupNet-Tinier-YOLO-L on WiderFace testing dataset~\cite{widerface}. As shown in Figure~\ref{fig:exp_widerFace}, our model also gives excellent detection quality in various challenging scenarios such as tiny size, low-illumination, severe occlusion and degraded coloring, etc.

In summary, we proposed DupNet-Tinier-YOLO face detector, which is quantized, very tiny and accurate. By employing duplicated weights for the weight-intensive layers, we reduced the model size and \#FLOPs of IFQ-Tinier-YOLO by 6.5 times and 42.0\% respectively. Meanwhile, we increased its detection rate by 4.5\% by using the proposed DupNet and the PACT~\cite{PACT} technique.  Moreover, we demonstrated that our DupNet can be flexibly adjusted for different inference cost (\eg DupNet-Tinier-YOLO-L has higher cost and is more accurate).


\section{Conclusions}
In this paper, we proposed DupNet which employs duplicated weights for the weight-intensive layers of a quantized CNN to compress its model size. Furthermore, we observe that the degraded accuracy of the quantized CNN is mainly caused by quantization-sensitive layers which have poor representative power on their quantized output feature maps. Hence, DupNet also duplicates the input feature maps of these layers and employ more weights channels to improve their output features. Through the experiments on FDDB dataset, we demonstrated that our DupNet-Tinier-YOLO face detector can significantly compress the model size and meanwhile impressively improve the detection rate. Moreover, our DupNet-Tinier-YOLO face detector can be lossless converted into fixed-point network~\cite{IFQNet} and thus can be easily implemented on embedded devices.

Additionally, our DupNet can be combined with other algorithms that are proposed to improve the performance of compressed networks such as knowledge distillation. Moreover, despite we only test our method on face detection, it is also applicable for other tasks such as object detection or even face recognition or semantic segmentation, etc.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{paper_HG}
}

\end{document}
