\section{Experiments}
\label{sec:experiments}

\subsection{Setup and Baselines}
\label{sec:experiments-setup}

In this work, we conducted experiments on images from the \dataset and \ufpramr~\cite{laroca2019convolutional} datasets.
The \ufpramr dataset contains $2{,}000$ images acquired in relatively well-controlled environments, with a resolution between $2{,}340$~$\times$~$4{,}160$ and $3{,}120$~$\times$~$4{,}160$ pixels, and is split into three subsets: training ($800$ images), validation ($400$ images) and testing ($800$ images).
In order to train, validate and test our networks, we merge the respective subsets from both datasets (i.e., exactly the same networks are used regardless of which datasets we are running experiments on).
As the \ufpramr dataset does not have any annotations related to the corners of the counters, we manually labeled their positions in its $2{,}000$ images so that we can use images from both datasets to train/evaluate the \gls*{cdcc} model.
All annotations made by us are publicly available to the research community along with the \dataset~dataset. 

It is important to point out that in several recent works in the literature~\cite{gomez2018cutting,koscevic2018automatic,calefati2019reading,liao2019reading} the authors reported only the results obtained by the proposed methods or compared them exclusively with traditional approaches, overlooking deep learning-based approaches designed for \gls*{amr}.
This makes it difficult to make a fair comparison between recently published~works.
Taking this into account, in this work, the end-to-end results achieved by the proposed system are compared (both in terms of recognition rate and execution time) with those obtained by several baseline methods~\cite{gomez2018cutting,calefati2019reading,liao2019reading,laroca2019convolutional,bochkovskiy2020yolov4} trained by us on \textit{exactly} the same images as the proposed method\footnote{The architectures and weights of the baselines implemented/trained by us are also publicly available at \url{\supplementary}}.
These specific methods, described in Section~\ref{sec:related_work}, were chosen/implemented by us for two main reasons: 
(i)~they were recently employed in the context of \gls*{amr} (except~\cite{bochkovskiy2020yolov4}, which was recently introduced) with promising/impressive results being reported, and (ii)~we believe we have the knowledge necessary to train/adjust them in the best possible way in order to ensure fairness in our experiments, as the authors provided enough details about the  architectures used, and also because we designed/employed similar networks (even the same ones in certain cases) in recent works in the context of license plate recognition and related areas~\cite{goncalves2018realtime,goncalves2019multitask,laroca2021efficient}.
Note that, in our experiments, we adapted all networks so that their input layers have the same aspect ratio~($w$/$h$~=~$3$).

Regarding the baselines, GÃ³mez et al.\cite{gomez2018cutting} evaluated their recognition network on a dataset containing mostly images where the counter is well centered and occupies a good portion of the image; therefore, in our experiments we first detect the counter region in the input image with the \detnet model and then apply their network to the detected region.
The same was done with the recognition network proposed by Calefati et al.~\cite{calefati2019reading}, as they dealt with the counter detection stage using an \gls*{fcn} for semantic segmentation~\cite{long2015fully} that we do not have the knowledge necessary to train/adjust in the best possible way.
We consider reimplementing/retraining~it out of scope for this work since our detection model (i.e.,~\detnet) is able to achieve high F-measure rates in both datasets (see Section~\ref{sec:results:cd}) and also due to the fact that the segmentation task is generally more time-consuming than the detection~one.

The YOLO-based models were trained using the Darknet framework\footnote{\url{https://github.com/AlexeyAB/darknet/}}, while the other models were trained using Keras\footnote{\url{https://keras.io/}}. 
In Darknet, the following parameters were used:~$65K$ iterations (max batches), batch size~=~$64$, and learning rate~=~[$10$\textsuperscript{-$3$},~$10$\textsuperscript{-$4$},~$10$\textsuperscript{-$5$}] with decay steps at $26K$ and $45.5K$~iterations.
In Keras, we employed the following parameters: initial learning rate~=~$10$\textsuperscript{-$3$} (with \textit{ReduceLROnPlateau}'s patience = 3 and factor = $10$\textsuperscript{-$1$}), batch size~=~$128$, max epochs~=~100, and patience~=~$7$ (patience refers to the number of epochs with no improvement after which training will be stopped).
All networks were trained using the \gls*{sgd} optimizer, and all experiments were carried out on a computer with an AMD Ryzen Threadripper $1920$X $3.5$GHz CPU, $48$~GB of RAM, SSD~(read: $535$~MB/s; write: $445$~MB/s), and an NVIDIA Titan~V~GPU. 
We remark that all parameter values were defined based on experiments performed in the validation~set.

\major{In addition to the baselines mentioned above, we trained and evaluated all models for scene text recognition implemented in}~\cite{clovaai} (\major{i.e., CRNN}~\cite{shi2017endtoend}, \major{RARE}~\cite{shi2016robust}, \major{R2AM}~\cite{lee2016recursive}, \major{STAR-Net}~\cite{liu2016starnet}, \major{GRCNN}~\cite{wang2017deep}, \major{Rosetta}~\cite{borisyuk2018rosetta} \major{and TRBA}~\cite{baek2019what})\major{, which is the open source repository (PyTorch\footnote{\url{https://pytorch.org/}}) of Clova AI Research used to record the 1st place of ICDAR2013 focused scene text and ICDAR2019~ArT, and 3rd place of ICDAR2017COCO-Text and ICDAR2019 ReCTS~(task1).
For reasons of space and clarity, instead of reporting the results obtained by all these models, we included in our overall evaluation only the results obtained by the model that performed faster (CRNN}~\cite{shi2017endtoend}\major{) and by the one that obtained the best recognition rates~(TRBA}~\cite{baek2019what}\major{) in our~experiments.}


It is worth noting that, as stated in Section~\ref{sec:related_work}, recognition models trained exclusively on images from datasets for general robust reading (e.g., ICDAR 2013~\cite{karatzas2013icdar}) are likely to fail in AMR scenarios due to some domain-specific characteristics (e.g., rotating digits).
However, we believe that first pre-training them using images from large-scale scene text recognition datasets and then fine-tuning them on images from AMR datasets can enable  even better results to be~achieved.

\subsection{Results and Discussion}

In this section, we report the experiments carried out to verify the effectiveness of the proposed \gls*{amr} system.
We first assess the counter detection stage separately since the regions used in the following stages are extracted from the detection results, rather than cropped directly from the ground truth --~note that a detection failure probably leads to another failure in the subsequent stages.
Similarly, we then report the results reached by \gls*{cdcc} in both corner detection and counter classification tasks.
Finally, we evaluate the entire \gls*{amr} system in an end-to-end manner \major{(without any prior knowledge as to which dataset each test image belongs to or whether it is from a legible/operational or illegible/faulty meter)} and compare the reading results achieved in legible/operational images with those obtained by \numbaselines baseline methods.





\subsubsection{Counter Detection}
\label{sec:results:cd}

Detection tasks in the \gls*{amr} context are often evaluated by considering a predicted bounding box to be correct if its \gls*{iou} with the ground truth is greater than $50$\%~\cite{laroca2019convolutional,salomon2020deep}.
Nevertheless, such a low threshold~(\gls*{iou}~$> 0.5$) was deliberately defined by Everingham et al.~\cite{everingham2010pascalvoc} to account for inaccuracies in bounding boxes in the training data, as defining the bounding box for a highly non-convex object (e.g., a person with arms and legs spread) is somewhat subjective.
Taking into account that the counters are convex objects and that they were carefully labeled in both datasets, in Table~\ref{tab:results-counter-detection} we report the performance (in terms of F-measure) of the \detnet model over different \gls*{iou} thresholds, from $0.5$ to~$0.95$, similarly to the COCO~\cite{lin2014microsoft} primary metric (\gls*{map}@\gls*{iou}=[$0.5$:$0.95$]).
As we consider only one meter per image, the precision and recall rates are~identical.

\begin{table}[!htb]
\centering
\setlength{\tabcolsep}{5pt}
\caption{
\small F-measure values obtained over different \gls*{iou} thresholds, from $0.5$ to $0.95$, in the counter detection stage. 
Note that \detnet achieves considerably better results at higher \gls*{iou} thresholds (i.e., $0.8$-$0.95$), which indicates that its predictions are much better aligned with the ground~truth.
}
\label{tab:results-counter-detection}

\vspace{0.5mm}

\resizebox{0.99\linewidth}{!}{ \begin{tabular}{@{}lccccccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Model}}         & \multicolumn{7}{c}{\gls*{iou} Threshold}            \\                          & $0.5$     & $0.6$     & $0.7$    & $0.8$    & $0.9$ & $0.95$ & $0.5$:$0.95$    \\ \midrule
\small \textit{\textbf{\ufpramr~\cite{laroca2019convolutional}}} \normalsize &         &         &        &        & & &        \\
\multicolumn{1}{c}{\major{Fast-YOLOv4 ($608\times608$)}}                  & \major{$98.1$\%}  & \major{$97.8$\%}  & \major{$97.6$\%} & \major{$94.6$\%} & \major{$75.9$\%} & \major{$38.6$\%} & \major{$83.8$\%} \\
\multicolumn{1}{c}{\detnet}               & \major{$99.9$\%} & \major{$99.9$\%} & \major{$99.9$\%} & \major{$98.1$\%} & \major{$80.0$\%} & \major{$38.9$\%} & \major{$86.1$\%} \\ \midrule
\small \textit{\textbf{\dataset}} \normalsize &         &         &        &        &  & &       \\
\multicolumn{1}{c}{\major{Fast-YOLOv4 ($608\times608$)}}                    & \major{$99.0$\%}  & \major{$98.3$\%}  & \major{$95.3$\%} & \major{$84.2$\%} & \major{$52.2$\%} & \major{$17.0$\%} & \major{$74.3$\%} \\
\multicolumn{1}{c}{\detnet}             & \major{$99.7$\%}  & \major{$99.5$\%}  & \major{$98.8$\%} & \major{$96.1$\%} & \major{$75.3$\%} & \major{$28.8$\%} & \major{$83.0$\%} \\ \bottomrule
\end{tabular}
} \end{table}

For comparison purposes, we also list in Table~\ref{tab:results-counter-detection} the detection results obtained by the original Fast-YOLOv4 model. 
Observe that the results achieved by \detnet are considerably better at higher \gls*{iou} thresholds (i.e., $0.8$-$0.95$), which indicates that the bounding boxes predicted by the modified architecture are much better aligned with the ground~truth.
This is relevant since although our \gls*{amr} system can tolerate less accurate detections at this stage, such imprecise predictions may still impair counter rectification and, consequently, counter recognition because one or more corners may not be within the detected bounding box.
Considering the detections with \gls*{iou}~$> 0.5$ with the ground truth as correct, as in some previous works, the \detnet \major{ failed in only one image from  the UFPR-AMR's test set and in only $14$ of the $5{,}000$ test images of the Copel-AMR dataset.}
Nevertheless, we highlight that it is still possible to correctly perform the subsequent tasks in most cases where our network has failed at this stage, as the four corners are usually within the detected region (especially considering that we use as input to the next stage a counter region slightly larger than the one~detected).
\major{For the record, we tried to further improve the results achieved at this stage by adding attention modules to each model; however, Fast-YOLOv4's results improved only marginally whereas Fast-YOLOv4-SmallObj's results have not improved at~all.}

Some detection results are shown in Fig.~\ref{fig:results-counter-detection}.
As can be seen, well-located predictions were attained on meters of different models and on images acquired under unconstrained conditions, that is, with significant reflections, rotations, and scale~variations.

\begin{figure}[!htb]
    \centering
    
    \resizebox{0.995\linewidth}{!}{ \includegraphics[width=0.16\linewidth]{imgs/5-results/counter-detection/fast-yolov4-small/meter002832.jpg}
    \includegraphics[width=0.16\linewidth]{imgs/5-results/counter-detection/fast-yolov4-small/meter002936.jpg}
    \includegraphics[width=0.16\linewidth]{imgs/5-results/counter-detection/fast-yolov4-small/meter003631.jpg}
    \includegraphics[width=0.16\linewidth]{imgs/5-results/counter-detection/fast-yolov4-small/meter001242.jpg}
    \includegraphics[width=0.16\linewidth]{imgs/5-results/counter-detection/fast-yolov4-small/meter010663.jpg}
    \includegraphics[width=0.16\linewidth]{imgs/5-results/counter-detection/fast-yolov4-small/meter010206.jpg}
    } 

    \vspace{0.5mm}
    
    \resizebox{0.995\linewidth}{!}{ \includegraphics[width=0.19\linewidth]{imgs/5-results/counter-detection/fast-yolov4-small/meter003945.jpg}
    \includegraphics[width=0.19\linewidth]{imgs/5-results/counter-detection/fast-yolov4-small/meter003621.jpg}
    \includegraphics[width=0.19\linewidth]{imgs/5-results/counter-detection/fast-yolov4-small/meter001948.jpg}
    \includegraphics[width=0.19\linewidth]{imgs/5-results/counter-detection/fast-yolov4-small/meter003197.jpg}
    \includegraphics[width=0.19\linewidth]{imgs/5-results/counter-detection/fast-yolov4-small/meter010394.jpg}
    } 

    \vspace{-1mm}
    
    \caption{\small Representative samples of counters detected by the \detnet model -- images taken under unconstrained conditions, with significant reflections, rotations, and scale variations.}
    \label{fig:results-counter-detection}
\end{figure}

\subsubsection{Corner Detection and Counter Classification}
\label{sec:results:corner-detection}

To assess the performance of \gls*{cdcc} in the \textit{corner detection} task, following~\cite{yoo2020deep}, we report in Table~\ref{tab:results-corner-detection} the mean pixel distance between the predicted corner positions and the ground truth on each dataset, in addition to how many \gls*{fps} the proposed model is capable of processing (we report the average across $\nruns$ runs).
We normalize the distances by dividing them by the respective image dimensions.
To enable a comparative evaluation, we also list in Table~\ref{tab:results-corner-detection} the results obtained by \numbaselinescdcc \gls*{cnn} models recently proposed for the detection of corners on license plate~images: \smallerlocatenet~\cite{meng2018robust}, \locatenet~\cite{meng2018robust} and \yoohybrid~\cite{yoo2020deep}.
For a fair comparison and considering that the classification of the meters as legible/operational or illegible/faulty is essential in the proposed \gls*{amr} pipeline, we added an output layer (softmax) to each baseline so that they can also perform such classification.
We emphasize that, according to our experiments,  this additional layer does not significantly affect the results obtained in the corner detection task (the normalized mean pixel distance achieved with and without that layer varied slightly in the fourth decimal~place).

\begin{table}[!htb]
\setlength{\tabcolsep}{8pt}
\centering
\caption{\small Comparison of the corner detection results obtained by \gls*{cdcc} and \numbaselinescdcc baselines.
The proposed model presents similar accuracy to \yoohybrid but is twice as fast.
Also, it performs almost as fast as \smallerlocatenet, even though it predicts much more accurate corner~positions.}
\label{tab:results-corner-detection}

\vspace{0.5mm}

\resizebox{0.99\linewidth}{!}{ \begin{tabular}{@{}lcccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{1}{c}{\multirow{2}{*}{\gls*{fps}}} & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}\small Mean pixel distance between the predicted\\\small corners and the ground truth (normalized) \normalsize\end{tabular}} \\ \cmidrule(l){3-5} 
& & \ufpramr & \phantom{ii}\dataset\phantom{ii} & Average   \\ \midrule

\smallerlocatenet~\cite{meng2018robust} & $\mathbf{207}$ & $0.0059$ & $0.0173$ & $0.0116$ \\
\locatenet~\cite{meng2018robust} & $166$ & $0.0031$ & $0.0098$ & $0.0065$ \\
\gls*{cdcc} (Ours) & $\fpscdcc$ & $0.0017$ & $0.0055$ & $0.0036$ \\
\yoohybrid~\cite{yoo2020deep} & $97$ & $\mathbf{0.0016}$ & $\mathbf{0.0046}$ & $\mathbf{0.0031}$ \\\bottomrule
\end{tabular}
} \end{table}

As can be seen, \gls*{cdcc} presents the best balance between accuracy and speed among the evaluated models.
More specifically, (i)~\smallerlocatenet is considerably less accurate in predicting the corner positions than the other networks, even though it runs faster; (ii)~\gls*{cdcc} is both faster and more accurate in locating the corners than~\locatenet; and
(iii)~\gls*{cdcc} predicts the positions of the corners almost as precisely as \yoohybrid, despite being able to process twice as many~\gls*{fps}. 

\major{Fig.~}\ref{fig:cdcc-net-loss-plot}\major{ shows the evolution of the training and validation losses of CDCC-NET over time (we omitted the losses related to counter classification for better viewing).
As it can be seen, CDCC-NET learns to predict the position of the four corners simultaneously and converged after $14$/$15$~epochs.}


\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.99\linewidth]{imgs/5-results/loss-plots/plot-exp-39-corners.pdf}
    
    \vspace{-4mm}
    
    \caption{\small \major{Training and validation losses of CDCC-NET over time. For each corner, the loss plotted is the mean between the $x$ and $y$ coordinates, e.g., the `top-left corner' loss is the mean between the losses of the $x_0$ and $y_0$~tasks.}}
    \label{fig:cdcc-net-loss-plot}
\end{figure}

It should be noted that we evaluated deeper networks in place of \gls*{cdcc} (i.e., with more convolutional layers and/or more filters), however, the end-to-end reading results achieved by the proposed \gls*{amr} system improved only slightly --~not justifying the higher computational cost.
In fact, we carried out an experiment in which we rectified all counter regions using the ground-truth annotations, as if the four corners were detected perfectly on each image, and the results improved less than we expected (from~$\accaverage$\% to~$\accaveragegt$\%), implying that most reading errors made by our \gls*{amr} system were not caused by a poor rectification of the counter region, but by other challenging factors (see Section~\ref{sec:results:overall} for more information and qualitative results). 

Table~\ref{tab:results-counter-classification} shows the results achieved by \gls*{cdcc} in the \textit{counter classification} task, which is significantly less challenging than corner detection.
It is clear that \gls*{cdcc} handles counter classification very well \major{--~in images from both datasets~-- since it correctly filtered out $989$ of the $1{,}000$ images in the Copel-AMR's test set~($\accfaulty$\%)} where it is not possible to perform the meter reading due to occlusions or faulty meters, thereby reducing the overall cost of the proposed system by skipping the counter rectification and recognition tasks in such cases, \major{while correctly accepting $799$ of the $800$ images in the UFPR-AMR's test set ($99.88$\%) and $3{,}990$ of the $4{,}000$ legible/operational images in the Copel-AMR's test set~($99.75$\%).}


\begin{table}[!htb]
\centering
\caption{\small Results achieved by \gls*{cdcc} in the counter classification task.
It is able to filter out \accfaulty\% of the illegible/faulty meters, while correctly accepting \acclegible\% (on average of both datasets) of the legible/operational meters.}
\label{tab:results-counter-classification}

\vspace{0.5mm}

\resizebox{0.725\linewidth}{!}{ \begin{tabular}{@{}ccc@{}}
\toprule
\diagbox[trim=l,trim=r]{Dataset}{Class} & Legible/Operational & Illegible/Faulty \\ \midrule



\ufpramr               & $99.88$\%             & -                \\
\dataset                & \major{$99.75$\%}             & $\accfaulty0$\%          \\ \midrule
Average                  & \major{$\acclegible$\%}             & $\accfaulty0$\%          \\ \bottomrule
\end{tabular}
} \end{table}

According to Fig.~\ref{fig:results-cdcc}, \gls*{cdcc} is able to successfully predict the four corners of the counter and simultaneously classify it as legible/operational or illegible/faulty, regardless of the meter model and other factors that are common in images acquired in uncontrolled environments such as rotations, reflections and shadows.

\begin{figure}[!htb]
    \centering
    \resizebox{0.995\linewidth}{!}{ \includegraphics[width=0.16\linewidth]{imgs/5-results/samples-cdcc/correct/meter002316.jpg}
    \includegraphics[width=0.16\linewidth]{imgs/5-results/samples-cdcc/correct/meter000407.jpg}
    \includegraphics[width=0.16\linewidth]{imgs/5-results/samples-cdcc/correct/meter001629.jpg}
    \includegraphics[width=0.16\linewidth]{imgs/5-results/samples-cdcc/correct/meter003145.jpg}
    \includegraphics[width=0.16\linewidth]{imgs/5-results/samples-cdcc/correct/meter010218.jpg}
    \includegraphics[width=0.16\linewidth]{imgs/5-results/samples-cdcc/correct/meter010650.jpg}
    } 

    \vspace{0.45mm}
    
    \resizebox{0.995\linewidth}{!}{ \includegraphics[width=0.19\linewidth]{imgs/5-results/samples-cdcc/correct/meter003407.jpg}
    \includegraphics[width=0.19\linewidth]{imgs/5-results/samples-cdcc/correct/meter001615.jpg}
    \includegraphics[width=0.19\linewidth]{imgs/5-results/samples-cdcc/correct/meter001480.jpg}
    \includegraphics[width=0.19\linewidth]{imgs/5-results/samples-cdcc/correct/meter010259.jpg}
    \includegraphics[width=0.19\linewidth]{imgs/5-results/samples-cdcc/correct/meter010606.jpg}
    } 

    \vspace{-1.25mm}
    
    \caption{\small Some qualitative results achieved by \gls*{cdcc} in corner detection and counter classification. For better visualization, we draw a polygon from the predicted corner positions. Counters classified as legible/operational are outlined in green while those classified as illegible/faulty are outlined in~red. 
    }
    \label{fig:results-cdcc}
\end{figure}

\subsubsection{Overall Evaluation (end-to-end)}
\label{sec:results:overall}

In this section, for each dataset, we report the number of correctly recognized~counters divided by the number of legible/operational meters in the test set \major{(legible/operational meters classified as illegible/faulty in the previous stage are considered as a reading error of the proposed method)}.
A correctly recognized counter means that all digits on the counter were correctly recognized, as a single digit recognized incorrectly can result in a large reading/billing error.
As in the previous stage, to enable an accurate analysis regarding the speed/accuracy trade-off of the evaluated methods, we report how many \gls*{fps} each method is capable of processing (in the average of $\nruns$ runs), including the time required to load the respective models and~weights.

\major{Note that our end-to-end system classifies the counter regions as legible/operational or illegible/faulty in the UFPR-AMR dataset as well, even though it does not have images labeled as~illegible/faulty.
This procedure was adopted to better simulate a real-world scenario, where there are images of both legible/operational and illegible/faulty~meters.}



The results obtained by the proposed system and the baselines are shown in Table~\ref{tab:end-to-end-results}.
As can be seen, our system performed the correct reading of $\accufpr$\% of the meters in the \ufpramr's test set and $\accdataset$\% of the legible/operational meters in the \dataset's test set, considerably outperforming all baselines in terms of recognition rate.
It is remarkable that the proposed approach is able to process $\fps$~\gls*{fps} (i.e., it took $\approx\msperimage$~ms to process each image), meeting the efficiency requirements of real-world~applications.

\begin{table*}[!htb]
\centering
\caption{\small Recognition rates obtained by the proposed \gls*{amr} system, a version without counter rectification of our system, and \numbaselines baselines in both datasets used in our experiments. Note that exactly the same images were used for training the proposed methods and the~baselines.}
\label{tab:end-to-end-results}

\vspace{0.5mm}

\resizebox{0.725\linewidth}{!}{ \begin{tabular}{@{}lcccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Approach}} & \multicolumn{1}{c}{\multirow{2}{*}{\gls*{fps}}} & \multicolumn{3}{c}{Recognition Rate} \\ \cmidrule(l){3-5} 
& & \ufpramr & \dataset & Average   \\ \midrule

\detnet + GÃ³mez et al.~\cite{gomez2018cutting} & $66$ & \major{$88.25$\%} & \major{$83.93$\%} & \major{$86.09$\%} \\
\detnet \major{ + CRNN}~\cite{shi2017endtoend} & \major{$69$} & \major{$89.62$\%} & \major{$84.07$\%} & \major{$86.85$\%} \\
Laroca et al.~\cite{laroca2019convolutional} (Multi-task) & $58$ & $89.12$\% & $87.02$\% & $88.07$\% \\
Liao et al.~\cite{liao2019reading} & $61$ & $91.37$\% & $92.25$\% & $91.81$\% \\
\detnet \major{ + Baek et al.}~\cite{baek2019what} \major{(TRBA)} & \major{$31$} & \major{$93.87$\%} & \major{$90.80$\%} & \major{$92.34$\%} \\
\major{YOLOv4 ($416\times416$)}~\cite{bochkovskiy2020yolov4} & \major{$58$} & \major{$91.63$\%} & \major{$93.75$\%} & \major{$92.69$\%} \\
Liao et al.~\cite{liao2019reading} (larger input size) & $42$ & $92.25$\% & $94.27$\% & $93.26$\% \\ \detnet + Calefati et al.~\cite{calefati2019reading} & \major{$58$} & $93.87$\% & \major{$95.15$\%} & \major{$94.51$\%} \\
\major{YOLOv4 ($608\times608$)}~\cite{bochkovskiy2020yolov4} & \major{$40$} & \major{$94.00$\%} & \major{$95.33$\%} & \major{$94.67$\%} \\
Laroca et al.~\cite{laroca2019convolutional} (CR-NET) & $41$ & $94.25$\% & $95.20$\% & $94.73$\% \\[2pt] \cdashline{1-5} \\[-5pt]
Ours - unrectified (\detnet + \ocrnet) & $\textbf{\fpsmodified}$ & \major{$\unrectaccufpr$\%} & \major{$\unrectaccdataset$\%} & \major{$\unrectaccaverage$\%} \\
Ours (\detnet + \gls*{cdcc} + \ocrnet) & $\fps$ & $\textbf{\accufpr}$\textbf{\%} & $\textbf{\accdataset}$\textbf{\%} & $\textbf{\accaverage}$\textbf{\%} \\\bottomrule
\end{tabular}
} \end{table*}

At first, we were surprised by the fact that the recognition rates reached in the proposed dataset were higher than those obtained in the \ufpramr dataset~\cite{laroca2019convolutional}, where the images were acquired in relatively more controlled conditions.
However, through an inspection of the reading errors made by all methods, we noticed some inconsistencies in the way rotating digits were labeled in the \ufpramr dataset (not always the lowest digit was chosen as the ground truth).
We also observed that the \ufpramr's test set contains some images where it is very difficult to perform the correct reading, even for humans, due to factors such as water vapor, reflections and dirt on the meter glass, as well as the poor positioning of the camera by the person who took the photo, causing the digits to appear only partially in the image even though the counter region appears entirely --~we emphasize that the \ufpramr dataset was collected by one of its authors and not by employees of the service~company, unlike the \dataset dataset.
In fact, in a few cases, it is even difficult to verify if the labeled reading (i.e., the ground truth) is correct.
Although we believe that such images should be rejected by the system due to the great possibility of reading errors (some of these images are shown in Fig.~\ref{fig:samples-challenging-ufpr-amr}), we employed the original labels in our evaluations to enable fair comparisons with other works in the literature, since most authors tend to use the annotations originally provided as part of the dataset.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.24\linewidth]{imgs/5-results/samples-challenging-ufpr-amr/meter1406.jpg} 
    \includegraphics[width=0.24\linewidth]{imgs/5-results/samples-challenging-ufpr-amr/meter1426.jpg} 
    \includegraphics[width=0.24\linewidth]{imgs/5-results/samples-challenging-ufpr-amr/meter1445.jpg} 
    \includegraphics[width=0.24\linewidth]{imgs/5-results/samples-challenging-ufpr-amr/meter1766.jpg}
    
    \vspace{-1.25mm}
    
    \caption{\small Some images from the \ufpramr dataset in which all evaluated methods failed to correctly perform the reading.
    In the first three images, there is dirt or water vapor on the meter glass, while in the fourth image the rightmost digit is labeled as `$6$' and not `$5$', contrary to the protocol normally~adopted.}
    \label{fig:samples-challenging-ufpr-amr}
\end{figure}

To highlight the importance of rectifying the counter region prior to the recognition stage, we included in Table~\ref{tab:end-to-end-results} the results achieved by a modified version of our approach in which the detected counter region is fed directly into \ocrnet \major{ (i.e., without counter rectification)}.
As can be seen, the \textit{corner detection and counter classification} stage is \textbf{essential} for accomplishing outstanding results in unconstrained scenarios, as our system made~$\errorsavoided$\% fewer reading errors  (i.e., $(\accdataset\%-\unrectaccdataset\%)/(100\%-\unrectaccdataset\%)$) \major{in the legible/operational meters} of the \dataset dataset when feeding rectified counters into the recognition~network.



The end-to-end results achieved by the baselines ranged from $\minbaselines$\% to $\maxbaselines$\%.
As some of them were originally evaluated only on private datasets, it is now possible to assess their applicability --~both in terms of speed and accuracy~-- more accurately.
For example, GÃ³mez et al.~\cite{gomez2018cutting} reported a promising recognition rate of $94.17$\% on a proprietary dataset using their segmentation-free network.
Nevertheless, in our comparative assessment, this model reached the lowest recognition rate among the baselines.
In general, as observed in~\cite{laroca2019convolutional}, the recognition models  based on object detectors (e.g., CR-NET and \ocrnet~--~which are based on YOLO~\cite{redmon2016yolo}) performed better than those where counter recognition is done holistically (e.g.,~\cite{shi2017endtoend,gomez2018cutting,baek2019what,calefati2019reading}).

In terms of execution time, we noticed that all methods evaluated are relatively fast, i.e., they are all capable of processing more than $30$ \gls*{fps} on a high-end GPU, especially the version without counter rectification of our system, which is able to process $\fpsmodified$~\gls*{fps}.
\major{As can be seen clearly in Fig.~}\ref{fig:performance-graph}\major{, the proposed system outperformed all baselines in terms of average recognition rate while being relatively fast, and its version without counter rectification is much faster than the baselines that reached similar~results.}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{imgs/5-results/execution-time/results-no-refs.pdf}
    
    \vspace{-3mm}
    
    \caption{\small \major{Performance comparison of our AMR system with $\numbaselines$ deep learning-based baselines. 
    For better viewing, both the proposed method~(L) and its version without counter rectification~(K) are depicted as red diamonds, whereas all the baselines~(A-J) are depicted as blue~circles.}}
    \label{fig:performance-graph}
\end{figure}


\major{We highlight that if an AMR system can run at $30$+~FPS on a high-end~GPU, it probably also works well on cheaper hardware (this is relevant to the service company).}
In this sense, for simpler/constrained scenarios, we believe that the proposed AMR system can be employed in low-end setups or even in some mobile phones (taking a few~seconds).





Fig.~\ref{fig:end-to-end-results-correct} shows some meter readings performed correctly by the proposed system.
It is noticeable that our end-to-end system is able to generalize well, being robust to meters of different models and images captured in unconstrained conditions (e.g., with various lighting conditions, reflections, shadows, scale variations, considerable rotations,~etc.).

\begin{figure*}[!htb]
	\centering
 	\captionsetup[subfigure]{labelformat=empty,captionskip=1.25pt,font={footnotesize}} 
	
\resizebox{0.8\linewidth}{!}{ \subfloat[][\texttt{04241}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/correct-end-to-end/meter002989.jpg}}\subfloat[][\texttt{25464}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/correct-end-to-end/meter003728.jpg}}\subfloat[][\texttt{57599}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/correct-end-to-end/meter001213.jpg}}\subfloat[][\texttt{00841}]{
    \includegraphics[width=0.16\linewidth]{imgs/5-results/correct-end-to-end/ieee-access/meter000205.jpg}}\subfloat[][\texttt{44671}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/correct-end-to-end/meter000499.jpg}}\subfloat[][\texttt{04730}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/correct-end-to-end/meter003538.jpg}} \hspace{1.25mm}
	} 

	\vspace{1.1mm}
	
\resizebox{0.8\linewidth}{!}{ \subfloat[][\texttt{25725}]{
		\includegraphics[width=0.19\linewidth]{imgs/5-results/correct-end-to-end/meter001443.jpg}}\subfloat[][\phantom{a}\texttt{10651}]{
		\includegraphics[width=0.19\linewidth]{imgs/5-results/correct-end-to-end/meter002160.jpg}}\subfloat[][\phantom{a}\texttt{06578}]{
		\includegraphics[width=0.19\linewidth]{imgs/5-results/correct-end-to-end/meter000087.jpg}}\subfloat[][\texttt{00353}]{
		\includegraphics[width=0.19\linewidth]{imgs/5-results/correct-end-to-end/meter003666.jpg}}\subfloat[][\texttt{02059}]{
		\includegraphics[width=0.19\linewidth]{imgs/5-results/correct-end-to-end/meter003945.jpg}} \hspace{1.25mm}
	} 

	\vspace{1.1mm}
	
\resizebox{0.8\linewidth}{!}{ \subfloat[][\texttt{07059}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/correct-end-to-end/meter002727.jpg}}
	\subfloat[][\texttt{21270}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/correct-end-to-end/meter002012.jpg}}\subfloat[][\texttt{18101}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/correct-end-to-end/meter003479.jpg}}\subfloat[][\texttt{01710}]{
 		\includegraphics[width=0.16\linewidth]{imgs/5-results/correct-end-to-end/meter002744.jpg}}\subfloat[][\texttt{03953}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/correct-end-to-end/meter000379.jpg}}\subfloat[][\texttt{28382}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/correct-end-to-end/meter002051.jpg}} \hspace{1.25mm}
	} 

	\vspace{-1.1mm}

    \caption{\small Examples of meter readings performed correctly by the proposed system.
    It is remarkable that it performed well in images of meters of different models and captured in unconstrained conditions (e.g., with various lighting conditions, reflections, shadows, scale variations, and considerable~rotations).}
    \label{fig:end-to-end-results-correct}  
\end{figure*}

Some reading errors made by our \gls*{amr} system are shown in Fig.~\ref{fig:end-to-end-results-errors}.
As one may see, they occurred mainly in challenging cases, where one digit becomes very similar to another due to artifacts in the counter region.
Another portion of the errors occurred on rotating digits (also called half digits), which is known to be a major cause of errors in electromechanical~meters~\cite{gao2018automatic,laroca2019convolutional}, even when robust approaches/models are employed for digit/counter~recognition.
It should be noted that (i)~errors in the least significant digits are tolerable, as they do not significantly impact the amount charged to consumers; and (ii) reading errors in the most significant digits can be filtered by the service company through heuristic rules, for example, the reading must be greater than or equal to the reading taken in the previous~month.

\begin{figure*}[!htb]
	\centering
 	\captionsetup[subfigure]{labelformat=empty,captionskip=1.25pt,font={footnotesize},justification=centering} 
	
\resizebox{0.8\linewidth}{!}{ \subfloat[][\texttt{1874\textcolor{red}{0}}\hspace{\textwidth}\texttt{(18748)}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/wrong-end-to-end/meter1473.jpg}}\subfloat[][\texttt{18\textcolor{red}{3}27}\hspace{\textwidth}\texttt{(18527)}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/wrong-end-to-end/meter002940.jpg}}\subfloat[][\texttt{104\textcolor{red}{6}9}\hspace{\textwidth}\texttt{(10459)}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/wrong-end-to-end/meter000794.jpg}}\subfloat[][\texttt{09\textcolor{red}{3}79}\hspace{\textwidth}\texttt{(09979)}]{
	\includegraphics[width=0.16\linewidth]{imgs/5-results/wrong-end-to-end/ieee-access/meter000755.jpg}}

	\subfloat[][\texttt{00\textcolor{red}{2}64}\hspace{\textwidth}\texttt{(00864)}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/wrong-end-to-end/meter002718.jpg}}\subfloat[][\texttt{3412\textcolor{red}{5}}\hspace{\textwidth}\texttt{(34124)}]{
		\includegraphics[width=0.16\linewidth]{imgs/5-results/wrong-end-to-end/meter000592.jpg}} \hspace{1.25mm}
	} 

	\vspace{1.1mm}
	
\resizebox{0.8\linewidth}{!}{ \subfloat[][\texttt{0\textcolor{red}{1}815}\hspace{\textwidth}\texttt{(07815)}]{	\includegraphics[width=0.19\linewidth]{imgs/5-results/wrong-end-to-end/meter003382.jpg}}\subfloat[][\texttt{0393\textcolor{red}{1}}\hspace{\textwidth}\texttt{(03937)}]{
	\includegraphics[width=0.19\linewidth]{imgs/5-results/wrong-end-to-end/meter003174.jpg}}\subfloat[][\texttt{6305\textcolor{red}{4}}\hspace{\textwidth}\texttt{(63059)}]{
	\includegraphics[width=0.19\linewidth]{imgs/5-results/wrong-end-to-end/meter001129.jpg}}\subfloat[][\texttt{02\textcolor{red}{9}23}\hspace{\textwidth}\texttt{(02323)}]{
	\includegraphics[width=0.19\linewidth]{imgs/5-results/wrong-end-to-end/meter003878.jpg}}\subfloat[][\texttt{2409\textcolor{red}{1}}\hspace{\textwidth}\texttt{(24097)}]{
	\includegraphics[width=0.19\linewidth]{imgs/5-results/wrong-end-to-end/ieee-access/meter001854.jpg}}} 

	\vspace{-1.1mm}

    \caption{\small Examples of reading errors made by our system.
	The ground truth is shown in parentheses.
    Observe that most of the errors occurred in challenging cases, where even humans can make mistakes, as one digit becomes very similar to another due to rotating digits or artifacts in the counter~region.}
    \label{fig:end-to-end-results-errors}  
\end{figure*}

\begin{table}[!htb]
\centering
\setlength{\tabcolsep}{7pt}
\caption{\small Recognition rates reached by the proposed \gls*{amr} system when discarding/rejecting the readings returned with lower confidence values by the \ocrnet network. Our system achieves impressive recognition rates (i.e.,~$\ge$~$99$\% on average) when using a confidence threshold that rejects $15$\% of the~images.}
\label{tab:results-with-rejection}

\vspace{0.5mm}

\resizebox{0.975\linewidth}{!}{ \begin{tabular}{@{}cccccc@{}}
\toprule
\multirow{2}{*}{Dataset} & \multicolumn{5}{c}{Rejection Rate}            \\ \cmidrule(l){2-6} 
                                          & $0$\%     & $5$\%    & $10$\%    & $15$\%    & $20$\%    \\ \midrule

\ufpramr & $\accufpr$\% & $97.63$\% & $98.47$\% & $98.82$\% & $99.22$\% \\
\dataset                                 & $\accdataset$\% & $98.61$\% & $99.00$\% & $99.24$\% & $99.44$\% \\ \midrule
Average                                   & $\accaverage$\% & $98.12$\% & $98.74$\% & $99.03$\% & $99.33$\% \\ \bottomrule \\[-12.5pt]
\end{tabular}
}
\end{table}

Finally, considering that very few reading errors are tolerated by \gls*{copel}~\cite{copel} and other service companies, we present in Table~\ref{tab:results-with-rejection} the end-to-end results achieved by the proposed system when discarding/rejecting the readings returned with lower confidence values by the \ocrnet network (in practice, in a mobile application, the employee would have to capture another image).
It is noteworthy that our \gls*{amr} system achieved an average recognition rate above~$98$\% by rejecting only $5$\%~of the meter readings.
Moreover, recognition rates above $99$\%, which are acceptable to service companies, are achieved by setting a confidence threshold that rejects $15$\% of the~images.
In this way, we consider that the proposed approach can be reliably employed on real-world~applications.