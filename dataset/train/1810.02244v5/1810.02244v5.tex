\def\year{2019}\relax
\documentclass[letterpaper]{article}
\usepackage{aaai19}  
\usepackage[utf8]{inputenc} 
\usepackage{times}  
\usepackage{helvet}  
\usepackage{courier} 
\usepackage{graphicx}  
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{url}
\usepackage[capitalise,noabbrev]{cleveref}
\usepackage{mathtools}
\usepackage{mleftright}
\usepackage{thm-restate}
\usepackage{subcaption}
\usepackage{scrextend}
\usepackage{scrextend}

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in} 
 
\usepackage{booktabs}      
\usepackage{amsfonts}      
\usepackage{nicefrac}      
\usepackage[tracking=true, stretch=0,shrink=55]{microtype} 
\usepackage{xcolor}
\usepackage{multirow}

\usepackage{enumitem}
\setlist[enumerate]{noitemsep, topsep=0.5\topsep}
\setlist[description]{noitemsep, topsep=0.5\topsep}
\setlist[itemize]{noitemsep, topsep=0.5\topsep}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{remark}[theorem]{Remark}


\definecolor{green}{rgb}{0.12, 0.66, 0.0}
\definecolor{blue}{rgb}{0.0, 0.25, 0.65} 
\newcommand{\xhdr}[1]{{\noindent\bfseries #1}.}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\newcommand{\new}[1]{\emph{#1}}
\newcommand{\cA}{\ensuremath{{\mathcal A}}\xspace}
\newcommand{\cB}{\ensuremath{{\mathcal B}}\xspace}
\newcommand{\cC}{\ensuremath{{\mathcal C}}\xspace}
\newcommand{\cD}{\ensuremath{{\mathcal D}}\xspace}
\newcommand{\cF}{\ensuremath{{\mathcal F}}\xspace}
\newcommand{\cH}{\ensuremath{{\mathcal H}}\xspace}
\newcommand{\cN}{\ensuremath{{\mathcal N}}\xspace}
\newcommand{\cO}{\ensuremath{{\mathcal O}}\xspace}
\newcommand{\cP}{\ensuremath{{\mathcal P}}\xspace}
\newcommand{\cR}{\ensuremath{{\mathcal R}}\xspace}
\newcommand{\cS}{\ensuremath{{\mathcal S}}\xspace}
\newcommand{\cU}{\ensuremath{{\mathcal U}}\xspace}
\newcommand{\cV}{\ensuremath{{\mathcal V}}\xspace}
\newcommand{\cPn}{\ensuremath{{\mathcal P}_n}\xspace}

\newcommand{\fA}{\ensuremath{\mathfrak{A}}\xspace}
\newcommand{\fB}{\ensuremath{\mathfrak{B}}\xspace}
\newcommand{\fC}{\ensuremath{\mathfrak{C}}\xspace}

\newcommand{\fa}{\ensuremath{\mathfrak{a}}\xspace}
\newcommand{\fb}{\ensuremath{\mathfrak{b}}\xspace}
\newcommand{\fc}{\ensuremath{\mathfrak{c}}\xspace}
\newcommand{\fd}{\ensuremath{\mathfrak{d}}\xspace}

\newcommand{\bA}{\ensuremath{{\bf A}}\xspace}
\newcommand{\bB}{\ensuremath{{\bf B}}\xspace}
\newcommand{\bK}{\ensuremath{{\bf K}}\xspace}
\newcommand{\bE}{\ensuremath{{\bf E}}\xspace}
\newcommand{\bN}{\ensuremath{{\bf N}}\xspace}
\newcommand{\bG}{\ensuremath{{\bf G}}\xspace}

\newcommand{\ba}{\ensuremath{{\bf a}}\xspace}
\newcommand{\bb}{\ensuremath{{\bf b}}\xspace}
\newcommand{\bc}{\ensuremath{{\bf c}}\xspace}

\newcommand{\bbE}{\ensuremath{\mathbb{E}}}
\newcommand{\bbR}{\ensuremath{\mathbb{R}}}
\newcommand{\bbQ}{\ensuremath{\mathbb{Q}}}
\newcommand{\bbP}{\ensuremath{\mathbb{P}}}
\newcommand{\bbZ}{\ensuremath{\mathbb{Z}}}
\newcommand{\bbN}{\ensuremath{\mathbb{N}}}
\newcommand{\bbNn}{\ensuremath{\mathbb{N}_0}}

\newcommand{\bbRnp}{\ensuremath{\bbR_{\geq 0}}}
\newcommand{\bbQnp}{\ensuremath{\bbQ_{\geq}}}
\newcommand{\bbZnp}{\ensuremath{\bbZ_{\geq}}}

\newcommand{\bbRsp}{\ensuremath{\bbR_>}}
\newcommand{\bbQsp}{\ensuremath{\bbQ_>}}
\newcommand{\bbZsp}{\ensuremath{\bbZ_>}}

\newcommand{\cp}{\textsf{P}\xspace}
\newcommand{\cnp}{\textsf{NP}\xspace}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\GN}{\mathbb{G}_n}

\newcommand{\rb}{\right\}\xspace}
\newcommand{\lb}{\left\{\xspace}
\newcommand{\lbr}{\left(\xspace}
\newcommand{\rbr}{\right)\xspace}

\newcommand{\trans}{^T}
\renewcommand{\vec}[1]{\mathbf{#1}} 

\newcommand{\oms}{\{\!\!\{}
\newcommand{\cms}{\}\!\!\}}

\newcommand{\cl}{\operatorname{cl}}
\newcommand{\rk}{\operatorname{rk}}
\newcommand{\sgn}{\operatorname{sgn}}

\pdfinfo{
/Title (Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks)
/Author (Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, Martin Grohe)
}
\setcounter{secnumdepth}{0}  
\begin{document}

\title{Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks}
\author{
	Christopher Morris\textsuperscript{1},
	Martin Ritzert\textsuperscript{2},
	Matthias Fey\textsuperscript{1},
	William L. Hamilton\textsuperscript{3},\\\bf \Large
	Jan Eric Lenssen\textsuperscript{1},
	Gaurav Rattan\textsuperscript{2},
	Martin Grohe\textsuperscript{2}\\
	\textsuperscript{1}{TU Dortmund University}\\
	\textsuperscript{2}{RWTH Aachen University}\\
	\textsuperscript{3}{McGill University and MILA}\\\
	\{christopher.morris, matthias.fey, janeric.lenssen\}@tu-dortmund.de,\\
	\{ritzert, rattan, grohe\}@informatik.rwth-aachen.de,\\
	wlh@cs.mcgill.ca
}

\maketitle 

\begin{abstract}
	In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically---showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the -dimensional Weisfeiler-Leman graph isomorphism heuristic (-WL). We show that GNNs have the same expressiveness as the -WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called -dimensional GNNs (-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.
\end{abstract}



\section{Introduction}
Graph-structured data is ubiquitous across application domains ranging from chemo- and bioinformatics to image and social network analysis.
To develop successful machine learning models in these domains, we need techniques that can exploit the rich information inherent in graph structure, as well as the feature information contained within a graph's nodes and edges. In recent years, numerous approaches have been proposed for machine learning graphs---most notably, approaches based on graph kernels \cite{Vis+2010} or, alternatively, using graph neural network algorithms \cite{Ham+2017a}.

Kernel approaches typically fix a set of features in advance---e.g., indicator features over subgraph structures or features of local node neighborhoods.
For example, one of the most successful kernel approaches, the \new{Weisfeiler-Lehman subtree kernel}~\cite{She+2011}, which is based on the -dimensional Weisfeiler-Leman graph isomorphism heuristic \cite[pp.\,79\,ff.]{Gro2017}, generates node features through an iterative relabeling, or \emph{coloring}, scheme: 
First, all nodes are assigned a common initial color; the algorithm then iteratively recolors a node by aggregating over the multiset of colors in its neighborhood, and the final feature representation of a graph is the histogram of the resulting node colors.  
By iteratively aggregating over local node neighborhoods in this way, the WL subtree kernel is able to effectively summarize the neighborhood substructures present in a graph. 
However, while powerful, the WL subtree kernel---like other kernel methods---is limited because this feature construction scheme is fixed (i.e., it does not adapt to the given data distribution). Moreover, this approach---like the majority of kernel methods---focuses only on the graph structure and cannot interpret continuous node and edge labels, such as real-valued vectors which play an important role in applications such as bio- and chemoinformatics. 
 
Graph neural networks (GNNs) have emerged as a machine learning framework addressing the above challenges.
Standard GNNs can be viewed as a neural version of the -WL algorithm, where colors are replaced by continuous feature vectors and neural networks are used to aggregate over node neighborhoods \cite{Ham+2017,Kip+2017}. 
In effect, the GNN framework can be viewed as implementing a continuous form of graph-based ``message passing'', where local neighborhood information is aggregated and passed on to the neighbors~\cite{Gil+2017}. By deploying a trainable neural network to aggregate information in local node neighborhoods, GNNs can be trained in an end-to-end fashion together with the parameters of the classification or regression algorithm, possibly allowing for greater adaptability and better generalization 
compared to the kernel counterpart of the classical -WL algorithm. 

Up to now, the evaluation and analysis of GNNs has been largely empirical, showing promising results compared to kernel approaches, see, e.g.,~\cite{Yin+2018}. However, it remains unclear how GNNs are actually encoding graph structure information into their vector representations, and whether there are theoretical advantages of GNNs compared to kernel based approaches. 

\xhdr{Present Work}
We offer a theoretical exploration of the relationship between GNNs and kernels that are based on the -WL algorithm. 
We show that GNNs cannot be more powerful than the -WL in terms of distinguishing non-isomorphic (sub-)graphs, e.g., the properties of subgraphs around each node. 
This result holds for a broad class of GNN architectures and all possible choices of parameters for them.
On the positive side, we show that given the right parameter initialization GNNs have the same expressiveness as the -WL algorithm, completing the equivalence. 
Since the power of the -WL has been completely characterized, see, e.g.,~\cite{Arv+2015,kiefer2015graphs}, we can transfer these results to the case of GNNs, showing that both approaches have the same shortcomings.

Going further, we leverage these theoretical relationships to propose a generalization of GNNs, called -GNNs, which are neural architectures based on the -dimensional WL algorithm (-WL), which are strictly more powerful than GNNs. 
The key insight in these higher-dimensional variants is that they perform message passing directly between subgraph structures, rather than individual nodes.
This higher-order form of message passing can capture structural information that is not visible at the node-level.

Graph kernels based on the -WL have been proposed in the past \cite{Mor+2017}.
However, a key advantage of implementing higher-order message passing in GNNs---which we demonstrate here---is that we can design hierarchical variants of -GNNs, which combine graph representations learned at different granularities in an end-to-end trainable framework. 
Concretely, in the presented hierarchical approach the initial messages in a -GNN are based on the output of lower-dimensional -GNN (with ), which allows the model to effectively capture graph structures of varying granularity.  Many real-world graphs inherit a hierarchical structure---e.g., in a social network we must model both the ego-networks around individual nodes, as well as the coarse-grained relationships between entire communities, see, e.g.,~\cite{New2003}---and our experimental results demonstrate that these hierarchical -GNNs are able to consistently outperform traditional GNNs on a variety of graph classification and regression tasks. Across twelve graph regression tasks from the QM9 benchmark, we find that our hierarchical model reduces the mean absolute error by 54.45\% on average. For graph classification, we find that our hierarchical models leads to slight performance gains.

\xhdr{Key Contributions}
Our key contributions are summarized as follows:
\begin{enumerate}
	\item We show that GNNs are not more powerful than the -WL in terms of distinguishing non-isomorphic (sub-)graphs. Moreover, we show that, assuming a suitable parameter initialization, GNNs have the same power as the -WL.
	\item We propose -GNNs, which are strictly more powerful than GNNs. Moreover, we propose a hierarchical version of -GNNs, so-called --GNNs, which are able to work with the fine- and coarse-grained structures of a given graph, and relationships between those. 
	\item Our theoretical findings are backed-up by an experimental study, showing that higher-order graph properties are important for successful graph classification and regression.
\end{enumerate}

\section{Related Work}
Our study builds upon a wealth of work at the intersection of supervised learning on graphs, kernel methods, and graph neural networks. 

Historically, kernel methods---which implicitly or explicitly map graphs to elements of a Hilbert space---have been the dominant approach for supervised learning on graphs. 
Important early work in this area includes random-walk based kernels \cite{Gae+2003,Kas+2003}) and kernels based on shortest paths \cite{Borgwardt2005}.
More recently, developments in graph kernels have emphasized scalability, focusing on techniques that bypass expensive Gram matrix computations by using explicit feature maps.
Prominent examples of this trend include kernels based on graphlet counting~\cite{She+2009}, and, most notably, the Weisfeiler-Lehman subtree kernel~\cite{She+2011} as well as its higher-order variants~\cite{Mor+2017}. 
Graphlet and Weisfeiler-Leman kernels have been successfully employed within frameworks for smoothed and deep graph kernels~\cite{Yan+2015,Yan+2015a}. Recent works focus on assignment-based approaches~\cite{Kri+2016,Nik+2017,Joh+2015}, spectral approaches~\cite{Kon+2016}, and graph decomposition approaches~\cite{Nik+2018}.
Graph kernels were dominant in graph classification for several years, leading to new state-of-the-art results on many classification tasks. 
However, they are limited by the fact that they cannot effectively adapt their feature representations to a given data distribution, since they generally rely on a fixed set of features. More recently, a number of approaches to graph classification based upon neural networks have been proposed. 
Most of the neural approaches fit into the graph neural network framework proposed by~\cite{Gil+2017}. Notable instances of this model include \new{Neural Fingerprints}~\cite{Duv+2015}, \emph{Gated Graph Neural Networks}~\cite{Li+2016}, \emph{GraphSAGE}~\cite{Ham+2017}, \emph{SplineCNN}~\cite{Fey+2018}, and the spectral approaches proposed in~\cite{Bru+2014,Def+2015,Kip+2017}---all of which descend from early work in~\cite{Mer+2005} and~\cite{Sca+2009}.
Recent extensions and improvements to the GNN framework include approaches to incorporate different local structures around subgraphs \cite{Xu+2018} and novel techniques for pooling node representations in order perform graph classification \cite{Zha+2018,Yin+2018}.
GNNs have achieved state-of-the-art performance on several graph classification benchmarks in recent years, see, e.g.,~\cite{Yin+2018}---as well as applications such as protein-protein interaction prediction~\cite{Fou+2017}, recommender systems~\cite{Yin+2018a}, and the analysis of quantum interactions in molecules~\cite{Sch+2017}.
A survey of recent advancements in GNN techniques can be found in \cite{Ham+2017a}.

Up to this point (and despite their empirical success) there has been very little theoretical work on GNNs---with the notable exceptions of Li et\ al.'s \cite{Li+2018a} work connecting GNNs to a special form Laplacian smoothing and Lei et al.'s\@ \cite{Lei+2017} work showing that the feature maps generated by GNNs lie in the same Hilbert space as some popular graph kernels. Moreover, Scarselli et al.\ \cite{Sca+2009a} investigates the approximation capabilities of GNNs. 

\section{Preliminaries}\label{prelim}

We start by fixing notation, and then outline the Weisfeiler-Leman algorithm and the standard graph neural network framework.  

\subsection{Notation and Background}
A \new{graph}  is a pair  with a finite set of \new{nodes}  and a set of \new{edges} . We denote the set of nodes and the set of edges of  by  and , respectively. For ease of notation we denote the edge  in  by  or .
Moreover,  denotes the \new{neighborhood} of  in , i.e., . We say that two graphs  and  are \new{isomorphic} if there exists an edge preserving bijection , i.e.,  is in  if and only if  is in . We write  and call the equivalence classes induced by  \new{isomorphism types}. Let  then  is the \new{subgraph induced} by  with . A \new{node coloring} is a function  with arbitrary codomain . Then a  \new{node colored} or \new{labeled graph}   is a graph  endowed with a node coloring . We say that  is a \new{label} or \new{color} of . We say that a node coloring  \new{refines} a node coloring , written , if  implies  for every  in . 
Two colorings are \new{equivalent} if  and , and we write .
A \new{color class}  of a node coloring  is a maximal set of nodes with  for every  in . Moreover, let  for , let  be a set then the set of \new{-sets}   for , which is the set of all subsets with cardinality , and let  denote a multiset.

\subsection{Weisfeiler-Leman Algorithm}
We now describe the \textsc{-WL} algorithm for labeled graphs. Let  be a labeled graph. In each iteration, , the -WL computes a node coloring ,
which depends on the coloring from the previous iteration.
In iteration , we set . Now in iteration , we set 

where  bijectively maps the above pair to a unique value in , which has not been used in previous iterations. To test two graph  and  for isomorphism, we run the above algorithm in ``parallel'' on both graphs. Now if the two graphs have a different number of nodes colored  in , the \textsc{-WL} concludes that the graphs are not isomorphic. Moreover, if the number of colors between two iterations does not change, i.e., the cardinalities of the images of  and  are equal, the algorithm terminates. Termination is guaranteed after at most  iterations. It is easy to see that the algorithm is not able to distinguish all non-isomorphic graphs, e.g., see~\cite{Cai+1992}. Nonetheless, it is a powerful heuristic, which can successfully test isomorphism for a broad class of graphs~\cite{Bab+1979}.


The -dimensional Weisfeiler-Leman algorithm (-WL), for , is a generalization of the -WL which colors tuples from  instead of nodes. That is, the algorithm computes a coloring . In order to describe the algorithm, we define the -th neighborhood

of a -tuple  in . That is, the -th neighborhood  of  is obtained by replacing the -th component of  by every node from . In iteration , the algorithm labels each -tuple with its \new{atomic type}, i.e., two -tuples  and  in  get the same color if the map  induces a (labeled) isomorphism between the subgraphs induced from the nodes from  and , respectively. For iteration , we define 

and set 


Hence, two tuples  and  with  get different colors in iteration  if there exists  in  such that the number of -neighbors of  and , respectively, colored with a certain color is different. 
The algorithm then proceeds analogously to the \textsc{-WL}. 
By increasing , the algorithm gets more powerful in terms of distinguishing non-isomorphic graphs, i.e., for each , there are non-isomorphic graphs which can be distinguished by the ()-WL but not by the -WL~\cite{Cai+1992}.
We note here that the above variant is not equal to the \emph{folklore} variant of -WL described in~\cite{Cai+1992}, which differs slightly in its update rule. 
However, it holds that the -WL using~\cref{labelk} is as powerful as the folklore -WL \cite{GroheO15}.

\xhdr{WL Kernels}
After running the WL algorithm, the concatenation of the histogram of colors in each iteration can be used as a feature vector in a kernel computation. 
Specifically, in the histogram for every color  in  there is an entry containing the number of nodes or -tuples that are colored with .

\subsection{Graph Neural Networks}
Let  be a labeled graph with an initial node coloring  that is \emph{consistent} with .
This means that each node  is annotated with a feature  in  such that  if and only if .
Alternatively,  can be an arbitrary  real-valued feature vector associated with .
Examples include continuous atomic properties in chemoinformatic applications where nodes correspond to atoms, or vector representations of text in social network applications. 
A GNN model consists of a stack of neural network layers, where each layer aggregates local neighborhood information, i.e., features of neighbors, around each node and then passes this aggregated information on to the next layer. 

A basic GNN model can be implemented as follows~\cite{Ham+2017a}.
In each layer ,  we compute a new feature 

in   for , where 
 and  are parameter matrices from 
, and  denotes a component-wise non-linear function, e.g., a sigmoid or a ReLU.\footnote{For clarity of presentation we omit biases.}

Following~\cite{Gil+2017}, one may also replace the sum defined over the neighborhood in the above equation by a permutation-invariant, differentiable function, and one may substitute the outer sum, e.g., by a column-wise vector concatenation or LSTM-style update step.
Thus, in full generality a new feature  is computed as

where  aggregates over the set of neighborhood features and  merges the node's representations from step  with the computed neighborhood features.
Both  and  may be arbitrary differentiable, permutation-invariant functions (e.g., neural networks), and, by analogy to Equation \ref{eq:basicgnn}, we denote their parameters as  and , respectively. 
In the rest of this paper, we refer to neural architectures implementing~\cref{eq:gnngeneral} as \emph{-dimensional GNN architectures} (-GNNs).

A vector representation  over the whole graph can be computed by summing over the vector representations computed for all nodes, i.e.,  

where  denotes the last layer. More refined approaches use differential pooling operators based on sorting~\cite{Zha+2018} and soft assignments~\cite{Yin+2018}. 

In order to adapt the parameters  and  of~\cref{eq:basicgnn,eq:gnngeneral}, to a given data distribution, they are optimized in an end-to-end  fashion (usually via stochastic gradient descent) together with the parameters of a neural network used for classification or regression.

\section{Relationship Between 1-WL and 1-GNNs}

In the following we explore the relationship between the -WL and -GNNs. 
Let  be a labeled graph, and let  denote the GNN parameters given by \cref{eq:basicgnn} or~\cref{eq:gnngeneral} up to iteration . 
We encode the initial labels  by vectors  , e.g., using a -hot encoding. 

Our first theoretical result shows that the -GNN architectures do not have more power in terms of distinguishing between non-isomorphic (sub-)graphs than the -WL algorithm.
More formally, let  and  be any two functions chosen in \eqref{eq:gnngeneral}.
For every encoding of the labels  as vectors , and for every choice of , we have that the coloring  of -WL always refines the coloring  induced by a -GNN parameterized by .

\begin{theorem}\label{thm:refine}
	Let  be a labeled graph. Then for all  and for all choices of initial colorings  consistent with , and weights ,
	
\end{theorem}

Our second result states that there exist a sequence of parameter matrices  such that -GNNs have exactly the same power in terms of distinguishing non-isomorphic \mbox{(sub-)}graphs as the -WL algorithm.
This even holds for the simple architecture~\eqref{eq:basicgnn}, provided we choose the encoding of the initial labeling  in such a way that different labels are encoded by linearly independent vectors.

\begin{theorem}\label{equal}
	Let  be a labeled graph. Then for all \mbox{} there exists a sequence of weights , and a -GNN architecture such that 
	
\end{theorem}

Hence, in the light of the above results, -GNNs may viewed as an extension of the -WL which in principle have the same power but are more flexible in their ability to adapt to the learning task at hand and are able to handle continuous node features.

\subsection{Shortcomings of Both Approaches}

The power of -WL has been completely characterized, see, e.g.,~\cite{Arv+2015}.  
Hence, by using~\cref{thm:refine,equal}, this characterization is also applicable to -GNNs. 
On the other hand, -GNNs have the same shortcomings as the -WL. 
For example, both methods will give the same color to every node in a graph consisting of a triangle and a -cycle, although vertices from the triangle and the vertices from the -cycle are clearly different.
Moreover, they are not capable of capturing simple graph theoretic properties, e.g., triangle counts, which are an important measure in social network analysis~\cite{Mil+2002,New2003}.

\section{-dimensional Graph Neural Networks}

\begin{figure*}[t]
	\centering
	\begin{subfigure}[b]{0.65\linewidth}
		\centering
		\includegraphics[width=0.7\textwidth]{overview.pdf}
		\caption{Hierarchical 1-2-3-GNN network architecture}\label{fig:architecture}
	\end{subfigure}
	\hspace{-.5cm}
	\begin{subfigure}[b]{0.28\linewidth}
		\centering
		\includegraphics[width=0.7\textwidth]{pool.pdf}
		\caption{Pooling from - to -GNN.}\label{fig:pooling}
	\end{subfigure}
	\caption{Illustration of the proposed hierarchical variant of the -GNN layer. For each subgraph  on  nodes a feature  is learned, which is initialized with the learned features of all -element subgraphs of . Hence, a hierarchical representation of the input graph is learned.}\label{fig:overview}
\end{figure*}

In the following, we propose a generalization of -GNNs, so-called -GNNs, which are based on the -WL. Due to scalability and limited GPU memory, we consider a set-based version of the -WL. For a given , we consider all -element subsets  over . Let  be a -set in , then we define the \emph{neighborhood} of  as 

The \emph{local neighborhood}\,  consists of all  such that  for the unique  and the unique . The \emph{global neighborhood}  then is defined as .\footnote{Note that the definition of the local neighborhood is different from the the one defined in~\cite{Mor+2017} which is a superset of our definition. 
Our computations therefore involve sparser graphs.}

The set based -WL works analogously to the -WL, i.e., it computes a coloring  as in \cref{eq:wlColoring} based on the above neighborhood. 
Initially,  colors each element  in  with the isomorphism type of . 

Let  be a labeled graph. 
In each -GNN layer , we compute a feature vector  for each -set  in . 
For , we set   to  , a one-hot encoding  of the isomorphism type of  labeled by . In each layer ,  we compute new features by 

Moreover, one could split the sum into two sums ranging over  and  respectively, using distinct parameter matrices to enable the model to learn the importance of local and global neighborhoods. 
To scale -GNNs to larger datasets and to prevent overfitting, we propose \emph{local} -GNNs, where we omit the global neighborhood of  , i.e.,

The running time for evaluation of the above depends on ,  and the sparsity of the graph (each iteration can be bounded by the number of subsets of size  times the maximum degree). Note that we can scale our method to larger datasets by using sampling strategies introduced in, e.g.,~\cite{Mor+2017,Ham+2017}. We can now lift the results of the previous section to the -dimensional case. 
\begin{proposition}\label{pro:refines}
	Let  be a labeled graph and let . Then for all , for all choices of initial colorings  consistent with  and for all weights ,
	
\end{proposition}
Again the second result states that there exists a suitable initialization of the parameter matrices  such that -GNNs have exactly the same power in terms of distinguishing non-isomorphic (sub-)graphs as the set-based -WL.
\begin{proposition}\label{pro:equality}
	Let  be a labeled graph and let . Then for all  there exists a sequence of weights , and a -GNN architecture such that   
			
\end{proposition}


\subsection{Hierarchical Variant}

One key benefit of the end-to-end trainable -GNN frame\-work---compared to the discrete -WL algorithm---is that we can hierarchically combine representations learned at different granularities.
Concretely, rather than simply using one-hot indicator vectors as initial feature inputs in a -GNN, we propose a \emph{hierarchical} variant of -GNN that uses the features learned by a -dimensional GNN, in addition to the (labeled) isomorphism type, as the initial features, i.e.,

for some , where  is a matrix of appropriate size, and square brackets denote matrix concatenation. 

Hence, the features are recursively learned from dimensions  to  in an end-to-end fashion.
This hierarchical model also satisfies \cref{pro:refines,pro:equality}, so its representational capacity is theoretically equivalent to a standard -GNN (in terms of its relationship to -WL).
Nonetheless, hierarchy is a natural inductive bias for graph modeling, since many real-world graphs incorporate hierarchical structure, so we expect this hierarchical formulation to offer empirical utility. 
\begin{table*}[t]\
	\caption{Classification accuracies in percent on various graph benchmark datasets.}
	\label{fig:classification_results}
	\renewcommand{\arraystretch}{0.90}
	\centering
	\begin{tabular}{@{}clccccccc@{}}
		\toprule
		& \multirow{3}{*}{\vspace*{8pt}\textbf{Method}} & \multicolumn{7}{c}{\textbf{Dataset}} \\
		\cmidrule{3-9}
		&                                    & {\textsc{Pro}} & {\textsc{IMDB-Bin}} & \!{\textsc{IMDB-Mul}} & \!{\textsc{PTC-FM}} & \!{\textsc{NCI1}} & \!{\textsc{Mutag}} & \!{\textsc{PTC-MR}} \\
		\cmidrule{2-9}
		\multirow{6}{*}{\rotatebox{90}{\hspace*{-6pt}Kernel}}
		& \textsc{Graphlet}                  & 72.9           & 59.4                & 40.8                  & 58.3                & 72.1              & 87.7               & 54.7                \\
		& \textsc{Shortest-path}             & \textbf{76.4}  & 59.2                & 40.5                  & 62.1                & 74.5              & 81.7               & 58.9                \\
		& \textsc{-WL}                    & 73.8           & 72.5                & \textbf{51.5}         & 62.9                & 83.1              & 78.3               & 61.3                \\
		& \textsc{-WL}                    & 75.2           & 72.6                & 50.6                  & \textbf{64.7}       & 77.0              & 77.0               & 61.9                \\
		& \textsc{-WL}                    & 74.7           & 73.5                & 49.7                  & 61.5                & 83.1              & 83.2               & 62.5                \\
		& \textsc{WL-OA}                     & 75.3           & 73.1                & 50.4                  & 62.7                & \textbf{86.1}     & 84.5               & \textbf{63.6}       
		\\
		\cmidrule{2-9}
		\multirow{7}{*}{\rotatebox{90}{GNN}}
		& \textsc{DCNN}                      & 61.3           & 49.1                & 33.5                  & ---                 & 62.6              & 67.0               & 56.6                \\
		& \textsc{PatchySan}                 & 75.9           & 71.0                & 45.2                  & ---                 & 78.6              & \textbf{92.6}      & 60.0                \\
		& \textsc{DGCNN}                     & 75.5           & 70.0                & 47.8                  & ---                 & 74.4              & 85.8               & 58.6                \\     
		\cmidrule{2-9}   
		
		& \textsc{-Gnn No Tuning}         & 70.7           & 69.4                & 47.3                  & 59.0                & 58.6              & 82.7               & 51.2                \\
		& \textsc{-Gnn}                   & 72.2           & 71.2                & 47.7                  & 59.3                & 74.3              & 82.2               & 59.0                \\
		& \textsc{---Gnn No Tuning} & 75.9           & 70.3                & 48.8                  & 60.0                & 67.4              & 84.4               & 59.3                \\
		& \textsc{---Gnn}           & 75.5           & \textbf{74.2}       & 49.5                  & 62.8                & 76.2              & 86.1               & 60.9                \\
\bottomrule
	\end{tabular}
\end{table*}

\begin{table*}[t]\
	\caption{Mean absolute errors on the \textsc{Qm9} dataset. The far-right column shows the improvement of the best -GNN model in comparison to the -GNN baseline.
	}\label{fig:qm9_results}
	\renewcommand{\arraystretch}{1.0}
	\centering
	\begin{tabular}{lccccc}
		\toprule
		\multirow{3}{*}{\vspace*{8pt}\textbf{Target}} & \multicolumn{5}{c}{\textbf{Method}} \\
		\cmidrule{2-6}
		&   \textsc{-Gnn}   & \!\textsc{--Gnn} & \textsc{--Gnn} & \textsc{---Gnn}\! & Gain   \\
		\midrule
		                                      & 0.493              & 0.493                  &      & 0.476                      & 4.0\%  \\
		                                                   & 0.78               &         & 0.46                 &             & 65.3\% \\
		            &  & 0.00331                & 0.00328              & 0.00337                    & --     \\
		                  & 0.00355            &      & 0.00354              & 0.00351                    & 1.4\%  \\
		                           & 0.0049             & 0.0047                 &     & 0.0048                     & 6.1\%  \\
		                       & 34.1               & 21.5                   & 25.8                 & 22.9                       & 37.0\% \\
		\textsc{ZPVE}                      & 0.00124            &      & 0.00064              & 0.00019                    & 85.5\% \\
		                                         & 2.32               &       & 0.6855               & 0.0427                     & 98.5\% \\
		                                                 & 2.08               &        & 0.686                & 0.111                      & 94.9\% \\
		                                                      & 2.23               & 0.070                  & 0.794                &           & 98.1\% \\
		                                              & 1.94               & 0.140                  & 0.587                &           & 97.6\% \\
		                                & 0.27               & 0.0989                 & 0.158                &           & 65.0\% \\
		\bottomrule
	\end{tabular}
\end{table*}


\section{Experimental Study}

In the following, we want to investigate potential benefits of GNNs over graph kernels as well as the benefits of our proposed -GNN architectures over -GNN architectures. More precisely, we address the following questions:
\begin{description}
	\item[Q1] How do the (hierarchical) -GNNs perform in comparison to state-of-the-art graph kernels? 
	\item[Q2] How do the (hierarchical)  -GNNs perform in comparison to the -GNN in graph classification and regression tasks?
	\item[Q3]
	How much (if any) improvement is provided by optimizing the parameters of the GNN aggregation function, compared to just using random GNN parameters while optimizing the parameters of the downstream classification/regression algorithm?
\end{description}

\subsection{Datasets }
To compare our -GNN architectures to kernel approaches we use well-established benchmark datasets from the graph kernel literature~\cite{KKMMN2016}. The nodes of each graph in these dataset is annotated with (discrete) labels or no labels. 

To demonstrate that our architectures scale to larger datasets and offer benefits on real-world applications, we conduct experiments on the \textsc{Qm9} dataset~\cite{Ram+2014,Rud+2012,Wu+2018}, which consists of 133\,385 small molecules. The aim here is to perform regression on twelve targets representing energetic, electronic, geometric, and thermodynamic properties, which were computed using density functional theory.


\subsection{Baselines}

We use the following kernel and GNN methods as baselines for our experiments.

\xhdr{Kernel Baselines} We use the Graphlet kernel~\cite{She+2009}, the shortest-path kernel~\cite{Borgwardt2005}, the Weisfeiler-Lehman subtree kernel (\textsc{WL})~\cite{She+2011}, the Weisfeiler-Lehman Optimal Assignment kernel (\textsc{WL-OA})~\cite{Kri+2016}, and the global-local -WL~\cite{Mor+2017} with  in  as kernel baselines. 
For each kernel, we computed the normalized Gram matrix. 
We used the -SVM implementation of LIBSVM~\cite{Cha+2011} to compute the classification accuracies using 10-fold cross validation. 
The parameter  was selected from   by 10-fold cross validation on the training folds. 

\xhdr{Neural Baselines} To compare GNNs to kernels we used the basic -GNN layer of~\cref{eq:basicgnn}, DCNN~\cite{Wang2018}, PatchySan~\cite{Nie+2016}, DGCNN~\cite{Zha+2018}. For the \textsc{Qm9} dataset we used a -GNN layer similar to~\cite{Gil+2017}, where we replaced the inner sum of~\cref{eq:basicgnn} with a 2-layer MLP in order incorporate edge features (bond type and distance information).

\subsection{Model Configuration}

We always used three layers for -GNN, and two layers for (local) -GNN and -GNN, all with a hidden-dimension size of . 
For the hierarchical variant we used architectures that use features computed by -GNN as initial features for the -GNN (--GNN)  and -GNN (--GNN), respectively. 
Moreover, using the combination of the former we componentwise concatenated the computed features of the --GNN and the --GNN (---GNN).
For the final classification and regression steps, we used a three layer MLP, with binary cross entropy and mean squared error for the optimization, respectively.
For classification we used a dropout layer with  after the first layer of the MLP. 
We applied global average pooling to generate a vector representation of the graph from the computed node features for each . 
The resulting vectors are concatenated column-wise before feeding them into the MLP. 
Moreover, we used the Adam optimizer with an initial learning rate of  and applied an adaptive learning rate decay based on validation results to a minimum of .  We trained the classification networks for  epochs and the regression networks for  epochs.

\subsection{Experimental Protocol} 

For the smaller datasets, which we use for comparison against the kernel methods, we performed a 10-fold cross validation where we randomly sampled 10\% of each training fold to act as a validation set.
For the \textsc{Qm9} dataset, we follow the dataset splits described in~\cite{Wu+2018}.
We randomly sampled 10\% of the examples for validation, another 10\% for testing, and used the remaining for training. We used the same initial node features as described in~\cite{Gil+2017}. Moreover, in order to illustrate the benefits of our hierarchical -GNN architecture, we did not use a complete graph, where edges are annotated with pairwise distances, as input.
Instead, we only used pairwise Euclidean distances for connected nodes, computed from the provided node coordinates. The code was built upon the work of~\cite{Fey+2018} and is provided at~\url{https://github.com/chrsmrrs/k-gnn}.


\subsection{Results and Discussion}

In the following we answer questions \textbf {Q1} to \textbf{Q3}. \cref{fig:classification_results} shows the results for comparison with the kernel methods on the graph classification benchmark datasets. Here, the hierarchical -GNN is on par with the kernels despite the small dataset sizes (answering question \textbf{Q1}).
We also find that the 1-2-3-GNN significantly outperforms the 1-GNN on all seven datasets (answering \textbf{Q2}), with the 1-GNN being the overall weakest method across all tasks.\footnote{Note that in very recent work, GNNs have shown superior results over kernels when using advanced pooling techniques~\cite{Yin+2018}. Note that our layers can be combined with these pooling layers. However, we opted to use standard global pooling in order to compare a typical GNN implementation with standard off-the-shelf kernels.}
We can further see that optimizing the parameters of the aggregation function only leads to slight performance gains on two out of three datasets, and that no optimization even achieves better results on the \textsc{Proteins} benchmark dataset (answering \textbf{Q3}).
We contribute this effect to the one-hot encoded node labels, which allow the GNN to gather enough information out of the neighborhood of a node, even when this aggregation is not learned.

\cref{fig:qm9_results} shows the results for the \textsc{Qm9} dataset. On eleven out of twelve targets all of our hierarchical variants beat the -GNN baseline, providing further evidence for \textbf{Q2}. 
However, the additional structural information extracted by the -GNN layers does not serve all tasks equally, leading to huge differences in gains across the targets.

It should be noted that our -GNN models have more parameters than the -GNN model, since we stack two additional GNN layers for each . However, extending the -GNN model by additional layers to match the number of parameters of the -GNN did not lead to better results in any experiment.


\section{Conclusion}

We presented a theoretical investigation of GNNs, showing that a wide class of GNN architectures cannot be stronger than the -WL. On the positive side, we showed that, in principle, GNNs possess the same power in terms of distinguishing between non-isomorphic (sub-)graphs, while having the added benefit of adapting to the given data distribution. Based on this insight, we proposed -GNNs which are a generalization of GNNs based on the -WL. This new model is strictly stronger then GNNs in terms of distinguishing non-isomorphic (sub-)graphs and is capable of distinguishing more graph properties. Moreover, we devised a hierarchical variant of -GNNs, which can exploit the hierarchical organization of most real-world graphs. Our experimental study shows that -GNNs consistently outperform -GNNs. Future work includes designing task-specific -GNNs, e.g., devising -GNNs layers that exploit expert-knowledge in bio- and chemoinformatic settings. 

\section*{Acknowledgments}
This work is supported by the German research council (DFG) within the Research Training Group 2236 \emph{UnRAVeL} and the Collaborative Research Center
SFB 876, \emph{Providing Information by Resource-Constrained
	Analysis}, projects A6 and B2.



\fontsize{9.5pt}{10.5pt} \selectfont
\bibliographystyle{aaai}
\begin{thebibliography}{}
	
	\bibitem[\protect\citeauthoryear{Arvind \bgroup et al\mbox.\egroup
		}{2015}]{Arv+2015}
	Arvind, V.; K{\"{o}}bler, J.; Rattan, G.; and Verbitsky, O.
	\newblock 2015.
	\newblock On the power of color refinement.
	\newblock In {\em Symposium on Fundamentals of Computation Theory},  339--350.
	
	\bibitem[\protect\citeauthoryear{Babai and Kucera}{1979}]{Bab+1979}
	Babai, L., and Kucera, L.
	\newblock 1979.
	\newblock Canonical labelling of graphs in linear average time.
	\newblock In {\em Symposium on Foundations of Computer Science},  39--46.
	
	\bibitem[\protect\citeauthoryear{Borgwardt and Kriegel}{2005}]{Borgwardt2005}
	Borgwardt, K.~M., and Kriegel, H.-P.
	\newblock 2005.
	\newblock Shortest-path kernels on graphs.
	\newblock In {\em ICDM},  74--81.
	
	\bibitem[\protect\citeauthoryear{Bruna \bgroup et al\mbox.\egroup
		}{2014}]{Bru+2014}
	Bruna, J.; Zaremba, W.; Szlam, A.; and LeCun, Y.
	\newblock 2014.
	\newblock Spectral networks and deep locally connected networks on graphs.
	\newblock In {\em ICLR}.
	
	\bibitem[\protect\citeauthoryear{Cai, F{\"{u}}rer, and
		Immerman}{1992}]{Cai+1992}
	Cai, J.; F{\"{u}}rer, M.; and Immerman, N.
	\newblock 1992.
	\newblock An optimal lower bound on the number of variables for graph
	identifications.
	\newblock {\em Combinatorica} 12(4):389--410.
	
	\bibitem[\protect\citeauthoryear{Chang and Lin}{2011}]{Cha+2011}
	Chang, C.-C., and Lin, C.-J.
	\newblock 2011.
	\newblock {{LIBSVM}}: {A} library for support vector machines.
	\newblock {\em ACM Transactions on Intelligent Systems and Technology}
	2:27:1--27:27.
	
	\bibitem[\protect\citeauthoryear{Defferrard, X., and
		Vandergheynst}{2016}]{Def+2015}
	Defferrard, M.; X., B.; and Vandergheynst, P.
	\newblock 2016.
	\newblock Convolutional neural networks on graphs with fast localized spectral
	filtering.
	\newblock In {\em NIPS},  3844--3852.
	
	\bibitem[\protect\citeauthoryear{Duvenaud \bgroup et al\mbox.\egroup
		}{2015}]{Duv+2015}
	Duvenaud, D.~K.; Maclaurin, D.; Iparraguirre, J.; Bombarell, R.; Hirzel, T.;
	Aspuru-Guzik, A.; and Adams, R.~P.
	\newblock 2015.
	\newblock Convolutional networks on graphs for learning molecular fingerprints.
	\newblock In {\em NIPS},  2224--2232.
	
	\bibitem[\protect\citeauthoryear{Fey \bgroup et al\mbox.\egroup
		}{2018}]{Fey+2018}
	Fey, M.; Lenssen, J.~E.; Weichert, F.; and M{\"u}ller, H.
	\newblock 2018.
	\newblock {SplineCNN}: Fast geometric deep learning with continuous {B}-spline
	kernels.
	\newblock In {\em CVPR}.
	
	\bibitem[\protect\citeauthoryear{Fout \bgroup et al\mbox.\egroup
		}{2017}]{Fou+2017}
	Fout, A.; Byrd, J.; Shariat, B.; and Ben-Hur, A.
	\newblock 2017.
	\newblock Protein interface prediction using graph convolutional networks.
	\newblock In {\em NIPS},  6533--6542.
	
	\bibitem[\protect\citeauthoryear{G\"{a}rtner, Flach, and
		Wrobel}{2003}]{Gae+2003}
	G\"{a}rtner, T.; Flach, P.; and Wrobel, S.
	\newblock 2003.
	\newblock On graph kernels: Hardness results and efficient alternatives.
	\newblock In {\em Learning Theory and Kernel Machines}.
	\newblock  129--143.
	
	\bibitem[\protect\citeauthoryear{Gilmer \bgroup et al\mbox.\egroup
		}{2017}]{Gil+2017}
	Gilmer, J.; Schoenholz, S.~S.; Riley, P.~F.; Vinyals, O.; and Dahl, G.~E.
	\newblock 2017.
	\newblock Neural message passing for quantum chemistry.
	\newblock In {\em ICML}.
	
	\bibitem[\protect\citeauthoryear{Grohe and Otto}{2015}]{GroheO15}
	Grohe, M., and Otto, M.
	\newblock 2015.
	\newblock Pebble games and linear equations.
	\newblock {\em Journal of Symbolic Logic} 80(3):797--844.
	
	\bibitem[\protect\citeauthoryear{Grohe}{2017}]{Gro2017}
	Grohe, M.
	\newblock 2017.
	\newblock {\em Descriptive Complexity, Canonisation, and Definable Graph
	Structure Theory}.
	\newblock Lecture Notes in Logic. Cambridge University Press.
	
	\bibitem[\protect\citeauthoryear{Hamilton, Ying, and
		Leskovec}{2017a}]{Ham+2017}
	Hamilton, W.~L.; Ying, R.; and Leskovec, J.
	\newblock 2017a.
	\newblock Inductive representation learning on large graphs.
	\newblock In {\em NIPS},  1025--1035.
	
	\bibitem[\protect\citeauthoryear{Hamilton, Ying, and
		Leskovec}{2017b}]{Ham+2017a}
	Hamilton, W.~L.; Ying, R.; and Leskovec, J.
	\newblock 2017b.
	\newblock Representation learning on graphs: Methods and applications.
	\newblock {\em {IEEE} Data Engineering Bulletin} 40(3):52--74.
	
	\bibitem[\protect\citeauthoryear{Johansson and Dubhashi}{2015}]{Joh+2015}
	Johansson, F.~D., and Dubhashi, D.
	\newblock 2015.
	\newblock Learning with similarity functions on graphs using matchings of
	geometric embeddings.
	\newblock In {\em KDD},  467--476.
	
	\bibitem[\protect\citeauthoryear{Kashima, Tsuda, and Inokuchi}{2003}]{Kas+2003}
	Kashima, H.; Tsuda, K.; and Inokuchi, A.
	\newblock 2003.
	\newblock Marginalized kernels between labeled graphs.
	\newblock In {\em ICML},  321--328.
	
	\bibitem[\protect\citeauthoryear{Kersting \bgroup et al\mbox.\egroup
		}{2016}]{KKMMN2016}
	Kersting, K.; Kriege, N.~M.; Morris, C.; Mutzel, P.; and Neumann, M.
	\newblock 2016.
	\newblock Benchmark data sets for graph kernels.
	
	\bibitem[\protect\citeauthoryear{Kiefer, Schweitzer, and
		Selman}{2015}]{kiefer2015graphs}
	Kiefer, S.; Schweitzer, P.; and Selman, E.
	\newblock 2015.
	\newblock Graphs identified by logics with counting.
	\newblock In {\em MFCS},  319--330.
	\newblock Springer.
	
	\bibitem[\protect\citeauthoryear{Kipf and Welling}{2017}]{Kip+2017}
	Kipf, T.~N., and Welling, M.
	\newblock 2017.
	\newblock Semi-supervised classification with graph convolutional networks.
	\newblock In {\em ICLR}.
	
	\bibitem[\protect\citeauthoryear{Kondor and Pan}{2016}]{Kon+2016}
	Kondor, R., and Pan, H.
	\newblock 2016.
	\newblock The multiscale laplacian graph kernel.
	\newblock In {\em NIPS},  2982--2990.
	
	\bibitem[\protect\citeauthoryear{Kriege, Giscard, and Wilson}{2016}]{Kri+2016}
	Kriege, N.~M.; Giscard, P.-L.; and Wilson, R.~C.
	\newblock 2016.
	\newblock On valid optimal assignment kernels and applications to graph
	classification.
	\newblock In {\em NIPS},  1615--1623.
	
	\bibitem[\protect\citeauthoryear{Lei \bgroup et al\mbox.\egroup
		}{2017}]{Lei+2017}
	Lei, T.; Jin, W.; Barzilay, R.; and Jaakkola, T.~S.
	\newblock 2017.
	\newblock Deriving neural architectures from sequence and graph kernels.
	\newblock In {\em ICML},  2024--2033.
	
	\bibitem[\protect\citeauthoryear{Li \bgroup et al\mbox.\egroup
		}{2016}]{Li+2016}
	Li, W.; Saidi, H.; Sanchez, H.; Sch{\"{a}}f, M.; and Schweitzer, P.
	\newblock 2016.
	\newblock Detecting similar programs via the {W}eisfeiler-{L}eman graph kernel.
	\newblock In {\em International Conference on Software Reuse},  315--330.
	
	\bibitem[\protect\citeauthoryear{Li, Han, and Wu}{2018}]{Li+2018a}
	Li, Q.; Han, Z.; and Wu, X.-M.
	\newblock 2018.
	\newblock Deeper insights into graph convolutional networks for semi-supervised
	learning.
	\newblock In {\em AAAI},  3538--3545.
	
	\bibitem[\protect\citeauthoryear{Merkwirth and Lengauer}{2005}]{Mer+2005}
	Merkwirth, C., and Lengauer, T.
	\newblock 2005.
	\newblock Automatic generation of complementary descriptors with molecular
	graph networks.
	\newblock {\em Journal of Chemical Information and Modeling} 45(5):1159--1168.
	
	\bibitem[\protect\citeauthoryear{Milo \bgroup et al\mbox.\egroup
		}{2002}]{Mil+2002}
	Milo, R.; Shen-Orr, S.; Itzkovitz, S.; Kashtan, N.; Chklovskii, D.; and Alon,
	U.
	\newblock 2002.
	\newblock Network motifs: simple building blocks of complex networks.
	\newblock {\em Science} 298(5594):824--827.
	
	\bibitem[\protect\citeauthoryear{Morris, Kersting, and Mutzel}{2017}]{Mor+2017}
	Morris, C.; Kersting, K.; and Mutzel, P.
	\newblock 2017.
	\newblock Glocalized {W}eisfeiler-{L}ehman kernels: Global-local feature maps
	of graphs.
	\newblock In {\em ICDM},  327--336.
	
	\bibitem[\protect\citeauthoryear{Newman}{2003}]{New2003}
	Newman, M. E.~J.
	\newblock 2003.
	\newblock The structure and function of complex networks.
	\newblock {\em SIAM review} 45(2):167--256.
	
	\bibitem[\protect\citeauthoryear{Niepert, Ahmed, and Kutzkov}{2016}]{Nie+2016}
	Niepert, M.; Ahmed, M.; and Kutzkov, K.
	\newblock 2016.
	\newblock Learning convolutional neural networks for graphs.
	\newblock In {\em ICML},  2014--2023.
	
	\bibitem[\protect\citeauthoryear{Nikolentzos \bgroup et al\mbox.\egroup
		}{2018}]{Nik+2018}
	Nikolentzos, G.; Meladianos, P.; Limnios, S.; and Vazirgiannis, M.
	\newblock 2018.
	\newblock A degeneracy framework for graph similarity.
	\newblock In {\em IJCAI},  2595--2601.
	
	\bibitem[\protect\citeauthoryear{Nikolentzos, Meladianos, and
		Vazirgiannis}{2017}]{Nik+2017}
	Nikolentzos, G.; Meladianos, P.; and Vazirgiannis, M.
	\newblock 2017.
	\newblock Matching node embeddings for graph similarity.
	\newblock In {\em AAAI},  2429--2435.
	
	\bibitem[\protect\citeauthoryear{Ramakrishnan \bgroup et al\mbox.\egroup
		}{2014}]{Ram+2014}
	Ramakrishnan, R.; Dral, P., O.; Rupp, M.; and von Lilienfeld, O.~A.
	\newblock 2014.
	\newblock Quantum chemistry structures and properties of 134 kilo molecules.
	\newblock {\em Scientific Data} 1.
	
	\bibitem[\protect\citeauthoryear{Ruddigkeit \bgroup et al\mbox.\egroup
		}{2012}]{Rud+2012}
	Ruddigkeit, L.; van Deursen, R.; Blum, L.~C.; and Reymond, J.-L.
	\newblock 2012.
	\newblock Enumeration of 166 billion organic small molecules in the chemical
	universe database gdb-17.
	\newblock {\em Journal of Chemical Information and Modeling} 52 11:2864--75.
	
	\bibitem[\protect\citeauthoryear{Scarselli \bgroup et al\mbox.\egroup
		}{2009a}]{Sca+2009a}
	Scarselli, F.; Gori, M.; Tsoi, A.~C.; Hagenbuchner, M.; and Monfardini, G.
	\newblock 2009a.
	\newblock Computational capabilities of graph neural networks.
	\newblock {\em IEEE Transactions on Neural Networks} 20(1):81--102.
	
	\bibitem[\protect\citeauthoryear{Scarselli \bgroup et al\mbox.\egroup
		}{2009b}]{Sca+2009}
	Scarselli, F.; Gori, M.; Tsoi, A.~C.; Hagenbuchner, M.; and Monfardini, G.
	\newblock 2009b.
	\newblock The graph neural network model.
	\newblock {\em IEEE Transactions on Neural Networks} 20(1):61--80.
	
	\bibitem[\protect\citeauthoryear{Sch{\"{u}}tt \bgroup et al\mbox.\egroup
		}{2017}]{Sch+2017}
	Sch{\"{u}}tt, K.; Kindermans, P.~J.; Sauceda, H.~E.; Chmiela, S.; Tkatchenko,
	A.; and M{\"{u}}ller, K.~R.
	\newblock 2017.
	\newblock {SchNet}: A continuous-filter convolutional neural network for
	modeling quantum interactions.
	\newblock In {\em NIPS},  992--1002.
	
	\bibitem[\protect\citeauthoryear{Shervashidze \bgroup et al\mbox.\egroup
		}{2009}]{She+2009}
	Shervashidze, N.; Vishwanathan, S.~V.~N.; Petri, T.~H.; Mehlhorn, K.; and
	Borgwardt, K.~M.
	\newblock 2009.
	\newblock Efficient graphlet kernels for large graph comparison.
	\newblock In {\em AISTATS},  488--495.
	
	\bibitem[\protect\citeauthoryear{Shervashidze \bgroup et al\mbox.\egroup
		}{2011}]{She+2011}
	Shervashidze, N.; Schweitzer, P.; van Leeuwen, E.~J.; Mehlhorn, K.; and
	Borgwardt, K.~M.
	\newblock 2011.
	\newblock Weisfeiler-{L}ehman graph kernels.
	\newblock {\em JMLR} 12:2539--2561.
	
	\bibitem[\protect\citeauthoryear{Vishwanathan \bgroup et al\mbox.\egroup
		}{2010}]{Vis+2010}
	Vishwanathan, S. V.~N.; Schraudolph, N.~N.; Kondor, R.; and Borgwardt, K.~M.
	\newblock 2010.
	\newblock Graph kernels.
	\newblock {\em JMLR} 11:1201--1242.
	
	\bibitem[\protect\citeauthoryear{Wang \bgroup et al\mbox.\egroup
		}{2018}]{Wang2018}
	Wang, Y.; Sun, Y.; Liu, Z.; Sarma, S.~E.; Bronstein, M.~M.; and Solomon, J.~M.
	\newblock 2018.
	\newblock Dynamic graph {{CNN}} for learning on point clouds.
	\newblock {\em CoRR} abs/1801.07829.
	
	\bibitem[\protect\citeauthoryear{Wu \bgroup et al\mbox.\egroup
		}{2018}]{Wu+2018}
	Wu, Z.; Ramsundar, B.; Feinberg, E.~N.; Gomes, J.; Geniesse, C.; Pappu, A.~S.;
	Leswing, K.; and Pande, V.
	\newblock 2018.
	\newblock Moleculenet: a benchmark for molecular machine learning.
	\newblock {\em Chemical Science} 9:513--530.
	
	\bibitem[\protect\citeauthoryear{Xu \bgroup et al\mbox.\egroup
		}{2018}]{Xu+2018}
	Xu, K.; Li, C.; Tian, Y.; Sonobe, T.; Kawarabayashi, K.-i.; and Jegelka, S.
	\newblock 2018.
	\newblock Representation learning on graphs with jumping knowledge networks.
	\newblock In {\em ICML},  5453--5462.
	
	\bibitem[\protect\citeauthoryear{Yanardag and Vishwanathan}{2015a}]{Yan+2015}
	Yanardag, P., and Vishwanathan, S. V.~N.
	\newblock 2015a.
	\newblock Deep graph kernels.
	\newblock In {\em KDD},  1365--1374.
	
	\bibitem[\protect\citeauthoryear{Yanardag and Vishwanathan}{2015b}]{Yan+2015a}
	Yanardag, P., and Vishwanathan, S. V.~N.
	\newblock 2015b.
	\newblock A structural smoothing framework for robust graph comparison.
	\newblock In {\em NIPS},  2134--2142.
	
	\bibitem[\protect\citeauthoryear{Ying \bgroup et al\mbox.\egroup
		}{2018a}]{Yin+2018a}
	Ying, R.; He, R.; Chen, K.; Eksombatchai, P.; Hamilton, W.~L.; and Leskovec, J.
	\newblock 2018a.
	\newblock Graph convolutional neural networks for web-scale recommender
	systems.
	\newblock {\em KDD}.
	
	\bibitem[\protect\citeauthoryear{Ying \bgroup et al\mbox.\egroup
		}{2018b}]{Yin+2018}
	Ying, R.; You, J.; Morris, C.; Ren, X.; Hamilton, W.~L.; and Leskovec, J.
	\newblock 2018b.
	\newblock Hierarchical graph representation learning with differentiable
	pooling.
	\newblock In {\em NIPS}.
	
	\bibitem[\protect\citeauthoryear{Zhang \bgroup et al\mbox.\egroup
		}{2018}]{Zha+2018}
	Zhang, M.; Cui, Z.; Neumann, M.; and Yixin, C.
	\newblock 2018.
	\newblock An end-to-end deep learning architecture for graph classification.
	\newblock In {\em AAAI},  4428--4435.
	
\end{thebibliography}



\onecolumn

\fontsize{11.0pt}{14.0pt} \selectfont

\section{\LARGE Appendix}

In the following we provide proofs for Theorem 1, Theorem 2, Proposition 3, and Proposition 4.

\section{Proof of Theorem 1}

\begin{theorem}[Theorem 1 in the main paper]\label{thm:refine:restated}
	Let  be a labeled graph. Then for all  and for all choices of initial colorings  consistent with , and weights ,
	
\end{theorem}
For the theorem we consider a single iteration of the -WL algorithm and the GNN on a single graph.
\begin{proof}[Proof of Theorem 1]
	We show for an arbitrary iteration  and nodes , that  implies . In iteration  we have  as the initial node coloring  is chosen consistent with .
				
	Let  and  such that .
	Assume for the induction that  holds.
	As  we know from the refinement step of the -WL that the old colors  of  and  as well as the multisets  and  of colors of the neighbors of  and  are identical.
				
	Let  and  be the multisets of feature vectors of the neighbors of  and  respectively. By the induction hypothesis, we know that  and  such that independent of the choice of  and  we get . This holds as the input to both functions  and  is identical. This proves  and thereby the theorem.
\end{proof}


\section{Proof of Theorem 2}

\begin{theorem}[Theorem 2 in the main paper]\label{equal:restated}
	Let  be a labeled graph. Then for all  there exists a sequence of weights  and a -GNN architecture such that 
	
\end{theorem}
For the proof we start by giving the proof for graphs where all nodes have the same initial color and then extend it to colored graphs. 
In order to do that we use a slightly adapted but equivalent version of the -WL.
Note that the extension to colored graphs is mostly technical, while the important idea is already contained in the first case.

\subsection{Uncolored Graphs}
Let  be the refinement operator for the -WL, mapping the old coloring  to the updated one :

We first show that for uncolored graphs this is equivalent to the update rule :

We denote  as the all- matrix where the size will always be clear from the context.

\begin{lemma}\label{lem:1}
	Let  be a graph, , and  such that
	.
	Then  for all .
\end{lemma}

\begin{proof}
	Let  be minimal such that there are  with
	
	Then , because  as there are no initial colors.
	Let  be the color classes of . That
	is, for all  we have
	 if any only if there is an
	 such that . 
	Similarly, let  be the color classes of . 
	Observe that the partition  of  refines the partition . 
	Indeed, if there were ,  such that  and , then all ,  would satisfy  and , contradicting the minimality of .
				
	Choose  satisfying \eqref{eq:2} and \eqref{eq:1}. 
	By \eqref{eq:2}, there is an  such that . 
	Let  such that . 
	By \eqref{eq:1}, for all  we have . 
	As the  are disjoint, this implies  , which is a contradiction.
\end{proof}
Hence, the two update rules are equivalent.
\begin{corollary}
	For all  and all  we have .
\end{corollary}
Thus we can use the update rule  for the proof on unlabeled graphs. For the proof, it will be convenient to 
assume that  (although we still work with the notation ). It follows that .  A node coloring  defines a matrix  where the  row of  is defined by .
Here we interpret  as a node from . As colorings and matrices can be interpreted as one another, given a matrix  we write  (or ) for a Weisfeiler-Leman iteration on the coloring  induced by the matrix . For the GNN computation we provide a matrix based notation.
Using the adjacency matrix  of  and a coloring , we can write the update rule of the GNN layer as

where  is the refinement operator of GNNs corresponding to a single iteration of the -WL. For simplicity of the proof, we choose

and the bias as 

Note that we later provide a way to simulate the sign-function using ReLu operations to indicate that choosing the sign function is not really a hard restriction.
\begin{lemma}
	\label{lem:dist2lu}
	Let  be a matrix such that 
	for all  and the rows of  are pairwise distinct. 
	Then there is a matrix  such that the matrix
	 is non-singular.
\end{lemma}

\begin{proof}
	Let  where  is the upper bound on the matrix entries of   and . 
	Then the entries of  are nonnegative and pairwise distinct.
	Without loss of generality, we assume that  such that . Now we choose numbers  such that
	
	for all  as the  are ordered. 
	Let  and  and . 
	Then  has entries , and thus by \eqref{eq:25}, 
	
	Thus  is non-singular. Now we simply let . Then .
\end{proof}
Let us call a matrix \emph{row-independent modulo equality} if the set
of all rows appearing in the matrix is linearly independent.

\begin{example}
	The matrix
	
	is row-independent modulo equality.
\end{example}
Note that the all- matrix  is row-independent modulo equality in all dimensions.
\begin{lemma}\label{lem:equivalencePerStep}
	Let , and let   be row independent modulo equality. 
	Then there is a  such that the matrix  is row independent modulo equality and
	
\end{lemma}
\begin{proof}
	Let  be the color
	classes of  (that is, for all  it holds that
	). Let
	 be the matrix with rows  for all . Then the rows of  are linearly independent, and thus there is a matrix 
	 such that  is the  identity
	matrix. 
	It follows that  is the matrix with entries
		
	
	Let  be
	the matrix with entries . Note that
	
	because for all  and  we have
	
	where the second equality follows from Equation \eqref{eq:12}. By the definition of  as the -WL operator on uncolored graphs, we have
	
	if we view  as a coloring of .
				
	Let  be the color classes of , and let  be the matrix
	with rows  for all  and . Then  for all , and the rows of
	 are pairwise distinct. By Lemma~\ref{lem:dist2lu}, there is a
	matrix  such that the matrix
	 is non singular. This implies
	that the matrix  is row-independent modulo
	equality. Moreover,  by \eqref{eq:10}. We let  be the matrix of obtained from
	 by adding  all-0 columns. Then 
	
	is row-independent modulo equality and
	. 
\end{proof}
\begin{corollary}
	There is a sequence  with
	 such that for all ,
	
	where  is given by the -fold application of  on the initial uniform coloring .
\end{corollary}
\begin{remark}\label{rem:fixedSizeOutput}
	The construction in \cref{lem:equivalencePerStep} always outputs a matrix with as many columns as there are color classes in the resulting coloring.
	Thus we can choose  to be  and pad the matrix using additional -columns.
\end{remark}

\subsection{Colored Graphs}
We now extend the computation to colored graphs.
In order to do that, we again use an equivalent but slightly different variant of the Weisfeiler-Leman update rule leading to colorings  instead of the usual . 
We then start by showing that both update rules are equivalent.

We define  to be the refinement operator for the -WL, mapping a coloring  to the updated one  as follows:

Note that for  we use the initial color  of a node  whereas  used the color  from the previous round.
The idea of using those old colors is to make sure that any two nodes which got a different color in iteration , get different colors in iteration .
This is formalized by the following lemma.

\begin{lemma}\label{lem:2}
	Let  be a colored graph, , and  such that
	. 
	Then  for all .
\end{lemma}

\begin{proof}
	Let  be minimal such that there are  with
	
	Then , because by \eqref{eq:4},
	 implies . Let  be the color classes of , and let  be the color classes of . 
	Observe that the partition  of  refines the partition
	. The argument is the same as in the proof of
	Lemma~\ref{lem:1}.
		
	Choose  satisfying \eqref{eq:5} and \eqref{eq:6}. 
	By \eqref{eq:5}, either  or there is an  such that . 
	By \eqref{eq:4},  contradicts \eqref{eq:6}. 
	Thus  for some . 
	Let  such that . By \eqref{eq:5}, for all  we have . 
	As the  are disjoint, this implies , which is a contradiction.
\end{proof}
\begin{corollary}\label{cor:c-l-zero}
	For all graphs  and initial vertex colorings  of  we have
	 for all .
\end{corollary}
We now consider the slightly modified update rule for -GNNs which takes the initial colors  into account.
Let the matrix  be an encoding (such as the one-hot encoding) of  such that  is linearly independent modulo equality. Then



We now show that this version of GNNs, which can be implemented by the simple variant (1) of GNNs provided in the main paper, is equivalent to -WL on colored graphs, proving the main theorem.


\begin{proof}[Proof of Theorem 2 (sketch)]
	We prove the theorem by induction over the iteration .
	The initial colorings are chosen consistent with  providing  .
	For the induction step we assume  for iteration .
	We know by \cref{lem:equivalencePerStep} that the inner part  of the update rule results in color classes which are identical to the ones that  would produce.
	This implies that restricted to each color class  from  of iteration , the new color classes  restricted to  match the coloring  that is  restricted to .
	This holds as within one color class, the common color of the nodes contains no further information as shown in \cref{lem:1}.
	Observe that colors are represented by linearly independent row vectors.
	This especially holds for .
	In order to show that  represents the coloring  we have to prove two properties.
	\begin{enumerate}
		\item Given  and , we have .
		\item  is linearly independent modulo equality.
	\end{enumerate}
	In the first item we interpret  as a coloring function. Both properties follow directly from the definition of  as the concatenation of the matrices  and .
	The first property holds as the row vectors of  are clearly different, as otherwise .
	The second property follows from the fact that linear independence cannot be lost by extending a matrix.
	Thus within each old color class , all vectors are linearly independent modulo equality as  (without subscript) is linearly independent modulo equality.
	This then extends to all combinations of old color classes and colors from  as  is also linearly independent modulo equality.
	By induction this chows that  for all .
				
	Note that the width of the matrices  can be bounded by .
	The width of  can trivially be bounded by  (in the worst case every node has a different initial color).
	The width of  can also be bounded by  using \cref{rem:fixedSizeOutput} as it is the output of a computation using \cref{lem:equivalencePerStep}. Using \cref{cor:c-l-zero} for the step from  to  finishes the proof of 2.
\end{proof}

\subsection{Adaptation to the ReLu activation function.}
The above framework can be easily adapted to 
use the ReLu function as the activation function 
of our choice, as follows. Recall that the  activation function 
was applied to the matrix ,
where the matrix  is defined via Equation \ref{eq:25}.  
We claim that a two-fold application of ReLu, interspersed 
by some elementary matrix operations, also yields the matrix .
This ensures that we can use our GNN architecture to work with the ReLu activation function. 

The proof is as follows. 
The first application of ReLu to the matrix  yields the matrix ,
which is a lower triangular matrix satisfying

Let  be the smallest positive value occurring in .
Note that  is well defined as there are only a finite number of possible binary input matrices (with values ) for which  can deterministically be computed.
The matrix  then satisfies 

Another application of ReLu yields an upper triangular matrix ,
where every non-zero entry is equal to . Subtracting  from  then yields . We now show that those operations can be represented by the GNN architecture.
Recall that our GNN architecture was given by stacking a number of iterations of the form, in matrix notation

where the  term is the bias that is added in each iteration and  is a non-linear function such as  or ReLu written as .

To compute  we can choose  and  appropriate to reach ,  as the all- matrix and .
Correspondingly, for  we can choose , ,  and .
In the next round  can be chosen according to the regular choices of  and  to simulate a computation on  instead of  which works by the linearity of the inner computation. Every layer described by Equation \eqref{eq:matrixGNN} is followed by an adjustment layer plus a few modifications in both the previous and upcoming step.
As a result, we have a constant (two-fold) increase
in the number of layers in our GNN set-up. The following corollary 
summarizes the above discussion.

\begin{corollary}[ReLu activation]
	Let  be a labeled graph.
	Then there exists a sequence  and a -GNN architecture based on the simple architecture described in (5) in the main paper using ReLu as activation function, such that  for all .
\end{corollary}



\section{Proofs of Proposition 3 and Proposition 4}

\begin{proposition}[Proposition 3 in the main paper]
	Let  be a labeled graph and let . Then for all , for all choices of initial colorings  consistent with  and for all weights ,
	
\end{proposition}

\begin{proof}
	The proof follows the arguments in the proof of Theorem 1.
	We therefore only provide a brief proof by induction on the iteration . 
	For the base case, i.e. iteration , the statement holds because 
	the initial coloring  is chosen to be consistent with the 
	isomorphism types .
				
	For the inductive step, assume that the statement holds until iteration .
	Consider two tuples  and  
	which (i) are not distinguished in the first  iterations 
	and (ii) are not distinguished in the  iteration. 
	Therefore,  and  must have equal number of neighbors from 
	every color class. This implies that the -GNN update rule yields the same output for  and .
	Hence, if two such tuples are distinguished by the -GNN in the  iteration,
	they must be distinguished by the -WL as well. This finishes the induction and proves the proposition. 
\end{proof}

\begin{proposition}[Proposition 4 in the main paper]
	Let  be a labeled graph and let . Then for all  there exists a sequence of weights  and a -GNN architecture such that   
	
\end{proposition}


\begin{proof}
	Let us simulate the set-based -WL on a -vertex graph  
	via a -WL on a graph  on  vertices,
	defined as follows. The vertex set of  is the 
	set  of all -element subsets of . 
	The edge set of  is defined as follows: two 
	sets  and  are connected by an edge in  
	if and only if . Observe that the 
	neighborhood of a vertex  in this graph
	is exactly the set  defined earlier. 
	The initial labeling of the vertices of the graph  is determined as follows:
	For  the initial label of  is its isomorphism type.
				
	For the above construction, it immediately follows that 
	performing the -WL on the graph  yields the same 
	coloring, as the one obtained by performing -WL for the graph . It remains to define the sequence  such that -GNN simulates the set based -WL on . Applying Theorem 2 to the graph  results in a 
	sequence  such that the -GNN can simulate -WL on  using . 
	Hence, this sequence can be directly used in the -GNN to simulate -WL on . 
\end{proof}





\begin{table*}
	\caption{Details of the  \textsc{Qm9} dataset. : Including atomic energy/enthalpy.
	}\label{fig:qm9_details}
	\renewcommand{\arraystretch}{1.0}
	\centering
	\begin{tabular}{lcc}
		\toprule
		\textbf{Property} & \textbf{Unit} & \textbf{Description} \\
		\midrule
		                       & Debye   &     Dipole moment  \\
		                    &  Bohr   &    Isotropic polarizability \\
		 & Hartree &     Energy of highest occupied molecular
		orbital (HOMO)  \\
		 &  Hartree  &    Energy of lowest occupied molecular
		orbital (LUMO)     \\
		         &  Hartree  &    Gap, difference between LUMO and \\
		       &Bohr  &    Electronic spatial extent                \\
		\textsc{ZPVE}               &Hartree  &    Zero point vibrational energy    \\
		                      &Hartree &     Internal energy at 0 K  \\
		                         &Hartree   &   Internal energy at 298.15 K  \\
		                         &Hartree     & Enthalpy at 298.15 K  \\
		                         &Hartree     & Free energy at 298.15 K
		 \\
		              & cal/(mol K)&  Heat capacity at 298.15 K   \\
		\bottomrule
	\end{tabular}
\end{table*}

\end{document}
