\subsection{Setting}
\subsubsection{Datasets}We evaluate SparseTrack on the MOT17 \cite{mot16}, MOT20 \cite{mot20}, and DanceTrack \cite{dancetrack} benchmark datasets, and submit the evaluation results to compare the performance with other methods. For ablation experiments, we split half of the training sets of MOT17 and MOT20 (dense scene) as validation sets \lzl{and conduct experiments on the MOT17 validation set, MOT20 validation set and DanceTrack validation set to analyse the effects with different settings}.
\subsubsection{Metrics}We use the common CLEAR metrics \cite{CLEAR-metrics} (MOTA, FP, FN, IDs, \etc), IDF1 \cite{idf1}, HOTA \cite{hota}, AssA and DetA, to evaluate tracking performance. MOTA is computed based on FP, FN, and IDs. As the number of FP and FN is larger than that of IDs, MOTA prefers to reflect detection performance. Additionally, IDF1 and AssA focus more on association performance. DetA is used to measure detection accuracy. HOTA is a comprehensive measure used to evaluate the overall effectiveness of detection and association.
\subsubsection{Implementation details}During the data association stage, we set different numbers of pseudo-depth levels for different datasets according to the scene crowding level. For MOT17, we set the default number of pseudo-depth levels to 3 and keep the maximum length of lost tracks at 30 frames. For MOT20, we set the default number of depth levels to 8 and keep the maximum length of lost tracks at 60 frames. For DanceTrack, we set the number of depth levels to 12 and keep the lost tracks until 60 frames. \lzl{The high-score and low-score detection threshold is set to 0.6 and 0.1 by default, separately, which aligns with the baseline approach}. To align the positions of detections and tracks on depth levels as closely as possible, we implement a fast online version of global motion compensation (GMC). It is worth noting that all our experimental results rely on IoU distance association and do not use any appearance feature component.

\lzl{During the inference phase, we configure the input resolution as  for MOT17 and DanceTrack, while we utilize an input resolution of  for MOT20. To make a fair comparison with the baseline method \cite{bytetrack}, we adopt the pre-trained YOLOX \cite{yolox} detector from ByteTrack and use the same weights and non-maximal suppression (NMS) thresh as the baseline method. For the ablation experiments on the MOT20 validation set, we train SparseTrack on the CrowdHuman and the half of MOT20 train set. For the ablation experiments on the DanceTrack validation set, we adopt publicly available YOLOX detector from \cite{dancetrack}. It is worth mentioning that DanceTrack only provides pre-trained detectors of the YOLOX-x model.} All implementations are performed on NVIDIA GeForce RTX 3090 GPUs.
 
\subsection{Evaluation of Different Benchmark}
We evaluate SparseTrack on the test set of MOT17, MOT20 and DanceTrack to compare with other methods. All evaluation results are based on the private detection protocol and are shown in \cref{tab:17}, \cref{tab:20} and \cref{tab:dance}, respectively \footnote{The best results are shown with bold in \cref{tab:17}, \cref{tab:20} and \cref{tab:dance}. In \cref{tab:dance}, \textbf{} indicates that the tracker utilize extra training data.}.

\begin{table}[t]
\caption{The comparison of SparseTrack with other methods on the MOT17 test set.}
\label{tab:17}
\resizebox{1.0\linewidth}{!}{
\setlength{\tabcolsep}{1.73pt}
 
\begin{tabular}{ l | c c c c c c r}
\toprule
Tracker & HOTA  & MOTA & IDF1 &  FP & FN & IDs & FPS\\
\midrule
\textcolor{red}{\textbf{  :}}\\
Tube\_TK\cite{tubetk} & 48.0 & 63.0 & 58.6 & 27060 & 177483 & 4137 & 3.0\\
CenterTrack\cite{centertrack} &52.2 &  67.8 & 64.7 & 18498 & 160332 & 3039 & 17.5\\
TraDes\cite{TraDeS}& 52.7  & 69.1 & 63.9 & 20892 & 150060 & 3555 & 17.5\\
MAT\cite{mat} & 53.8 & 69.5 & 63.1 & 30660 & 138741 & 2844 & 9.0\\
PermaTrackPr\cite{PermaTrackPr}  & 55.5 & 73.8 & 68.9& 28998 & 115104 & 3699 & 11.9\\
OC-SORT\cite{oc-sort} & 63.2 & 78.0 & 77.5 & \textbf{15129} &	107055 & 1950 & 29.0\\ 
MotionTrack\cite{motiontrack} & \textbf{65.1} & 81.1 & 80.1 & 23802 &  \textbf{81660} & \textbf{1140} & 15.7 \\

\midrule
\textcolor{red}{\textbf{ :}}\\
DAN\cite{DAN} & 39.3 & 52.4 & 49.5 & 25423 & 234592 & 8431 & \textless 3.9\\
QuasiDense\cite{qdtrack} & 53.9 & 68.7 & 66.3 & 26589 & 146643 & 3378 & 20.3\\
SOTMOT\cite{SOTMOT} & - & 71.0 & 71.9 & 39537 & 118983 & 5184 & 16.0\\
Semi-TCL\cite{semi} & 59.8 & 73.3 & 73.2 & 22944 & 124980 & 2790 & -\\
FairMOT\cite{fairmot} & 59.3 & 73.7 & 72.3 & 27507 & 117477 & 3303 & 25.9\\
CSTrack\cite{CStrack} & 59.3 & 74.9 & 72.6 & 23847 & 114303 & 3567 & 15.8\\
SiamMOT\cite{siammot} & - & 76.3 & 72.3 & - & - & - & 12.8\\
ReMOT\cite{remot} & 59.7 & 77.0 & 72.0 & 33204 & 93612 & 2853 & 1.8\\
StrongSORT\cite{strongsort} & 64.4 & 79.6 & 79.5 & 27876 & 86205 & 1194 & 7.1\\
BoT-SORT-ReID\cite{BoT-SORT} & 65.0 & 80.5 & \textbf{80.2} & 22521 & 86037 & 1212 & 4.5\\

\midrule
\textcolor{red}{\textbf{ :}}\\
CTracker\cite{ctracker}  & 49.0 & 66.6 & 57.4& 22284 & 160491 & 5529 & 6.8\\
TransCenter\cite{transcenter} & 54.5 & 73.2 & 62.2 & 23112 & 123738 & 4614 & 1.0\\
RelationTrack\cite{relationtrack} & 61.0 & 73.8 & 74.7 & 27999 & 118623 & 1374 & 8.5\\
TransTrack\cite{transtrack} & 54.1 & 75.2 & 63.5 & 50157 & 86442 & 3603 & 10.0\\
MOTRv2\cite{motrv2} & 62.0 & 78.6 & 75.0 & 23409 & 94797 & 2619 & 7.5\\
P3AFormer\cite{p3aformer} & - & \textbf{81.2} & 78.1 & 17281 & 86861 &  1893 & -\\

\midrule
\textcolor{red}{\textbf{ \& }}\\
\textcolor{red}{\textbf{ :}}\\
GSDT\cite{GSDT} & 55.2 & 73.2 & 66.5 & 26397 & 120666 & 3891 & 4.9\\
FUFET\cite{FUFET} & 57.9 & 76.2 & 68.0 & 32796 & 98475 & 3237 & 6.8\\
CorrTracker\cite{CorrTrack}& 60.7  & 76.5 & 73.6 & 29808 & 99510 & 3369 & 15.6\\
TransMOT\cite{transmot} & 61.7 & 76.7 & 75.1 & 36231 & 93150 & 2346 & 9.6\\

\midrule
\textcolor{red}{\textbf{  :}}\\
ByteTrack\cite{bytetrack} & 63.1 & 80.3 & 77.3 & 25491 & 83721 & 2196 & \textbf{29.6}\\
BoT-SORT\cite{BoT-SORT} & 64.6 & 80.6 & 79.5 & 22524 & 85398 & 1257 & 6.6\\
\textbf{SparseTrack (ours)} & \textbf{65.1} & 81.0 & 80.1 & 23904 & 81927 & 1170 & 19.9\\
\bottomrule
\end{tabular}
}
\end{table}

\textbf{MOT17.} By performing the target set decomposition based on pseudo-depth, SparseTrack achieves impressive performance on the MOT17 test set. Compared to the baseline (ByteTrack) with the \lzl{same pre-trained detector}, SparseTrack achieves gains of \textbf{+0.7} MOTA, \textbf{+2.8} IDF1, \textbf{+2.0} HOTA and IDs decrease to almost twice the original level. Instead of other methods that use appearance features and graph convolutional networks, SparseTrack achieves comparable performance with SOTA using only simple IoU association, which demonstrates that SparseTrack is \lzl{simple} and strong.

\begin{table}[t]
\caption{The comparison of SparseTrack with other methods on the MOT20 test set.}
\label{tab:20}
\setlength{\tabcolsep}{1.0pt}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{ l | c c c c c c r}
\toprule
Tracker & HOTA & MOTA & IDF1 & FP & FN & IDs & FPS\\
\midrule
\textcolor{red}{\textbf{  :}}\\
MotionTrack\cite{motiontrack}  & 62.8 & 78.0 & 76.5& 28629 & 84152 & 1165 & 15.0\\

\midrule
\textcolor{red}{\textbf{ :}}\\
FairMOT\cite{fairmot}  & 54.6 & 61.8 & 67.3 & 103440 & 88901 & 5243 & 13.2\\
Semi-TCL\cite{semi}  & 55.3 & 65.2 & 70.1 & 61209 & 114709 & 4139 & -\\
CSTrack\cite{CStrack}  & 54.0 & 66.6 & 68.6 & 25404 & 144358 & 3196 & 4.5\\
SiamMOT\cite{siammot}  & - & 67.1 & 69.1 & - & - & - & 4.3\\
SOTMOT\cite{SOTMOT}  & - & 68.6 & 71.4 & 57064 & 101154 & 4209 & 8.5\\
BoT-SORT-ReID\cite{BoT-SORT} & 63.3 & 77.8 & \textbf{77.5} & 24638 & 88863 & 1257 & 2.4\\
UTM\cite{UTM} & 62.5 & \textbf{78.2} & 76.9 & 29964 & \textbf{81516} & 1228 & -\\

\midrule
\textcolor{red}{\textbf{ :}}\\
TransCenter\cite{transcenter}   & - & 61.9 & 50.4& 45895 & 146347 & 4653 & 1.0\\
TransTrack\cite{transtrack}   & 48.5 & 65.0 & 59.4& 27197 & 150197 & 3608 & 7.2\\
RelationTrack\cite{relationtrack}  & 56.5 & 67.2 & 70.5 & 61134 & 104597 & 4243 & 2.7\\
MOTR\cite{motr} & 57.8  & 73.4 & 68.6 & - & - & 2439 & -\\
P3AFormer\cite{p3aformer} & - & 78.1 & 76.4 &  25413   & 86510 & 1332 & -\\

\midrule
\textcolor{red}{\textbf{ \& }}\\
\textcolor{red}{\textbf{ :}}\\
MLT\cite{MLT}  & 43.2 & 48.9 & 54.6 & 45660 & 216803 & 2187 & 3.7\\
CorrTracker\cite{CorrTrack}  & - & 65.2 & 69.1 & 79429 & 95855 & 5183 & 8.5\\
GSDT\cite{GSDT}  & 53.6 & 67.1 & 67.5 & 31913 & 135409 & 3131 & 0.9\\

\midrule
\textcolor{red}{\textbf{  :}}\\
ByteTrack\cite{bytetrack} & 61.3 & 77.8 & 75.2 & 26249 & 87594 & 1223 & \textbf{17.5}\\
BoT-SORT\cite{BoT-SORT} & 62.6 & 77.7 & 76.3 & \textbf{22521} & 86037 & 1212 & 6.6 \\
\textbf{SparseTrack (ours)} & \textbf{63.4} & \textbf{78.2} & 77.3 & 25108 & 86720 & \textbf{1116} & 12.5\\
\bottomrule
\end{tabular}
}
\end{table}

\textbf{MOT20.} Compared to MOT17, MOT20 has denser scenes, more occlusions, and longer videos, which increases the difficulty of the tracker in handling occlusions. SparseTrack uses the same \lzl{pre-trained detector} as the baseline and achieves a gain of \textbf{+0.4} MOTA, \textbf{+2.1} IDF1, and \textbf{+2.1} HOTA. Compared to other methods that utilize appearance cues, attention mechanism \cite{attention, vit} and graph \cite{graph-mot,graph-mot1,Learnable_graph_matching}, SparseTrack provides a more convenient solution: the target set decomposition based on hierarchical depth. It is worth mentioning that SparseTrack achieves very low IDs and FP, as well as high HOTA, which indicates that the target set decomposition is helpful in associating dense occlusions.

\begin{table}[t]
\caption{The comparison of SparseTrack with other methods on the DanceTrack test set.}
\label{tab:dance}
\centering
\setlength{\tabcolsep}{1.0pt}
\begin{tabular}{ l | c c c c r}
\toprule
Tracker & HOTA & MOTA & IDF1 & AssA & DetA \\
\midrule
\textcolor{red}{\textbf{  :}}\\
CenterTrack\cite{centertrack} & 41.8 &  86.8 & 35.7 & 22.6 & 78.1\\
TraDes\cite{TraDeS}& 43.3 & 86.2 & 41.2 & 25.4 & 74.5\\
OC-SORT\cite{oc-sort} & 55.1 & \textbf{92.0} & 54.6 & 38.3 & \textbf{80.3}\\ 

\midrule
\textcolor{red}{\textbf{ :}}\\
FairMOT\cite{fairmot}  & 39.7 & 82.2 & 40.8 & 23.8 & 66.7 \\
FCG\cite{FCG} & 48.7 & 89.9 & 46.5 & 29.9 & 79.8 \\
QuasiDense\cite{qdtrack} & 54.2 & 87.7 & 50.4 & 36.8 & 80.1\\

\midrule
\textcolor{red}{\textbf{ :}}\\
TransTrack\cite{transtrack}   & 45.5 & 88.4 & 45.2 & 27.5 & 75.9 \\
GTR\cite{GTR} & 48.0 & 84.7 & 50.3 & 31.9 & 72.5\\
MOTR\cite{motr} & 54.2  & 79.7 & 51.5 & \textbf{40.2}& 73.5 \\

\midrule
\textcolor{red}{\textbf{  :}}\\
ByteTrack\cite{bytetrack} & 47.7 & 89.6 & 53.9 & 32.1 & 71.0 \\
BoT-SORT\cite{BoT-SORT} & 54.7 & 91.3 & 56.0 & 37.8 & 79.6 \\
\textbf{SparseTrack (ours)} & \textbf{55.5} & 91.3 & \textbf{58.3} & 39.1 & 78.9 \\
\bottomrule
\end{tabular}

\end{table}

\textbf{DanceTrack.} DanceTrack is a more challenging benchmark for multi-object tracking with frequent occlusions, non-rigid motions and similar appearances. \lzl{With the same pre-trained detector}, SparseTrack achieves significant improvements compared to the baseline with a gain of \textbf{+7.8} HOTA, \textbf{+1.7} MOTA, \textbf{+4.4} IDF1, \textbf{+7.0} AssA and \textbf{+7.9} DetA, which demonstrates the enormous potential of depth-based target set decomposition in handling occlusions. Even with simple IoU distance association, SparseTrack still achieves comparable or even better performance than other methods.

\subsection{Ablations}
\subsubsection{The impact of the number of pseudo-depth levels}
\lzl{As DCM focuses on addressing dense occlusions, we conduct experiments on the MOT20 validation set and DanceTrack validation set (More crowded scenario and more frequent occlusions.) to better observe the impact of the number of pseudo-depth levels on tracking performance. Specifically, we use the BYTE \cite{bytetrack} association method as the baseline and integrate DCM into the low-score association process within BYTE. We set the number of pseudo-depth levels for the process of associating low-score detections to 1, 2, 5, 7, 9, separately. It is worth noting that we lower the detection confidence score from the initial 0.1 to 0.01 to involve more low-scoring detections in the association process.} The experimental results are shown in \cref{ablation:levels-compare}. 

\lzl{After integrating DCM, the association performance of BYTE gradually improves with the increase in the number of pseudo-depth levels. It indicates that a higher number of pseudo-depth levels is more beneficial to associate low-score occlusions. However, it does not yield additional gains when the number of pseudo-depth levels is exorbitant (\eg the number of pseudo-depth levels exceeds 7). We speculate that excessively frequent layering schemes result in increased sparsity among both low-scoring detections and participating tracks, which can not yield additional benefits compared to an already sparse scheme.}
\begin{table}[t]
\caption{
\lzl{Comparison of the association performance with different numbers of the pseudo-depth level on MOT20 and DanceTrack validation sets.}
}
\label{ablation:levels-compare}
\renewcommand\arraystretch{1.1}
\centering
\setlength{\tabcolsep}{1.0pt}
\begin{tabular}{ c c |c c c |c c c}
\toprule
\multirow{2.5}{*}{Levels} & &\multicolumn{3}{c}{MOT20}  & \multicolumn{3}{c}{DanceTrack} \\
\cmidrule(r){3-5} \cmidrule(lr){6-8}
 & & HOTA & AssA  & IDF1 & HOTA & AssA & IDF1 \\
\midrule
1 & & 68.6 & 65.9 & 83.0 & 52.6 & 36.5 & 54.2\\
2 & & 68.7 & 66.1 & 83.1 & 53.4 & 37.7 & 55.4\\
5 & & 68.8 & 66.1 & 83.2 & \textbf{53.9} & \textbf{38.5} & \textbf{55.8}\\
7 & & \textbf{68.9} & \textbf{66.3} & \textbf{83.4} & 53.8 & 38.4 & 55.7\\
9 & & 68.9 & 66.3 & 83.3 & 53.8 & 38.3 & 55.7\\
\bottomrule
\end{tabular}

\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=0.99\linewidth, height=10cm]{imgs/resolution.pdf}
\caption{\lzl{The graph depicts the changes in the association performance of SparseTrack and ByteTrack \cite{bytetrack} as the resolution varies. While there is marginal improvement in detector performance with increasing resolution, the association performance of the trackers consistently strengthens. It is worth noting that SparseTrack demonstrates more significant enhancements compared to ByteTrack.}}
\label{resolution-compare}
\end{figure}

\subsubsection{The impact of the input with different resolutions}
\lzl{We investigate the impact of input resolution on the tracking performance of SparseTrack. Specifically, we maintain consistent detection settings and test the association performance of SparseTrack and ByteTrack on the MOT20 validation dataset with varying input resolutions. We set the pseudo-depth levels to 8 in SparseTrack for associating low-scoring detections. The results are shown in} \cref{resolution-compare}.

\lzl{As the increase in resolution, the performance of object detection shows a gradual improvement. However, beyond a certain threshold in resolution, the detector exhibits diminishing returns, with little to no substantial gains. Interestingly, both SparseTrack and ByteTrack show significant enhancements in their tracking performance as resolution continues to climb. This phenomenon can be attributed to the increased detection capability to recognize and involve more severely occluded targets within the tracking process. Notably, our observations suggest that SparseTrack outperforms ByteTrack in terms of association enhancements during the process.}

\subsubsection{The impact of the global motion compensation}
\lzl{We perform ablative analyses of the global motion compensation (GMC) module using SparseTrack on both the MOT17 and MOT20 validation datasets. During the low-score association stage, we fix the number of pseudo-depth levels at 8. We maintain the default input resolution settings for MOT17 and MOT20. The results are presented in} \cref{ablation:gmc-compare}.

\lzl{By incorporating GMC, substantial enhancements in the performance of SparseTrack are achieved on the MOT17 validation dataset. This improvement primarily stem from the more pronounced camera motion prevalent in the MOT17 dataset. Conversely, GMC yield little gains in tracking performance on the MOT20 dataset.}
\begin{table}[!t]
\caption{
\lzl{Comparison of the association performance with or without global motion compensation on MOT17 and MOT20 validation sets.}
}
\label{ablation:gmc-compare}
\renewcommand\arraystretch{1.1}
\centering
\setlength{\tabcolsep}{1.0pt}
\begin{tabular}{c c |c c c |c c c}
\toprule
\multirow{2.55}{*}{w/ GMC}& & \multicolumn{3}{c}{MOT17}  & \multicolumn{3}{c}{MOT20} \\
\cmidrule(r){3-5} \cmidrule(lr){6-8}
 & & HOTA & AssA  & IDF1 & HOTA & AssA & IDF1 \\
\midrule
           & & 67.9 & 69.9 & 79.1 & 69.9 & 67.8 & 84.5\\
\checkmark & & 69.2 & 72.3 & 81.4 & 69.9 & 67.7 & 84.4\\
\bottomrule
\end{tabular}
\end{table}

\begin{table*}[t]
\caption{
\lzl{Comparison of the association methods on MOT17, MOT20 and DanceTrack validation sets.}
}
\label{methods-compare}
\renewcommand\arraystretch{1.1}
\centering
\setlength{\tabcolsep}{2.0pt}
\begin{tabular}{ c |c c c c |c c c c |c c c c}
\toprule
\multirow{2.5}{*}{Methods} & \multicolumn{4}{c}{MOT17}  & \multicolumn{4}{c}{MOT20} & \multicolumn{4}{c}{DanceTrack}\\
\cmidrule(r){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}
 & HOTA & AssA  & IDF1 & MOTA & HOTA &AssA & IDF1 & MOTA & HOTA & AssA & IDF1 & MOTA \\
\midrule
SORT \cite{Bewley2016_sort} &62.9 &61.9  &70.5  &73.5  &65.2 &59.1 &74.8 &85.1 &43.3  &26.0  &40.8  &86.0 \\
ByteTrack \cite{bytetrack}   &68.0 &69.9  &79.1  &76.3  &69.4 &66.9 &83.3 &86.1 &52.2&36.2&54.3&87.5\\
BoT-SORT \cite{BoT-SORT}    &68.9 &71.2  &80.7  &\textbf{76.9}  &69.5 &67.1 &83.7 &86.1 &53.4&37.6&\textbf{56.1} & \textbf{87.7}\\
OC-SORT \cite{oc-sort}     &66.3 &68.8  &77.5  &74.2  &68.0 &64.9 &82.0 &85.1 &51.2 &34.8 &51.1 &85.1 \\
SparseTrack &\textbf{69.2} &\textbf{72.3}  &\textbf{81.4}  &76.8  &\textbf{69.9} &\textbf{67.8} &\textbf{84.5} &\textbf{86.1} & \textbf{53.8} & \textbf{38.5} & 56.0 & 87.1 \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Comparison with Other Simple Methods}
\lzl{We compare SparseTrack with other location-dependent association methods on the MOT17, MOT20, and DanceTrack validation datasets respectively. The results as shown in \cref{methods-compare}. To ensure a fair comparison, we remove all specific hyper-parameters for each video to prevent any bias towards any method and all experiments maintain the same YOLOX-x detector model weights and NMS \cite{girshick14CVPR} hyper-parameters. For the SparseTrack, the number of pseudo-depth layers during the low-score association phase is consistently set to 8 and the low-score detection threshold remains at the default value of 0.1. It is worth mentioning that SparseTrack without GMC module reports the results on the MOT20 and DanceTrack validation set.}
 
\lzl{Compared to previous methods that rely on position information and Kalman filter~\cite{kf}, the association approach of SparseTrack achieves the best tracking performance on the aforementioned three validation sets.}


\subsection{Visualization}
To exhibiting the target set decomposition process and the performance of associating crowded targets intuitively, we conduct several different visualization experiments, as shown in \cref{dense-occ}, \cref{decomposition} and \cref{vis_occlusion}, respectively.

\textbf{Visualization of the target set decomposition.} 
Although the target set decomposition is a sub-process of DCM, it is the key to decoupling dense occlusions. In fact, not all targets in the scene are occluded state, but occluded targets may gather together and form dense occlusions easier, as shown in \cref{dense-occ}. By decomposing dense occlusion set based on pseudo-depth information, overlapping targets can be assigned to different depth levels, effectively. We visualize the decomposition of a set of dense occlusions, as shown in \cref{decomposition}.

\textbf{Visualization of associating occlusive targets via SparseTrack and ByteTrack.} 
\lzl{We conduct visualizations using both SparseTrack and ByteTrack on four different videos from the MOT challenge, while maintaining consistent detection hyper-parameter settings for the publicly available detector. The comparative results are shown in}~\cref{vis_occlusion}. \lzl{Comparing to ByteTrack, SparseTrack demonstrates greater stability in handling challenging occlusion scenarios. It is attributed to its ability to differentiate between targets at various depths crowded together, leveraging discriminative pseudo-depth information originating from the targets.}

\subsection{Discussion}
\lzl{While SparseTrack demonstrates remarkable performance improvements on both the MOT datasets and the DanceTrack dataset, it has poor performance on the public detection benchmark of the MOT dataset. The association performance of SparseTrack is substantially contingent on the detector's capacity to accurately identify occluded targets. To be precise, SparseTrack places a higher reliance on the detection quality of partially obscured and low-confidence targets compared to conventional detection-based tracking methods. In addition, SparseTrack faces challenges in other demanding scenarios such as \cite{visdrone, uavdt} where targets exhibit rapid motion and deformation. In such cases, it becomes difficult for the pseudo-depth method to capture the relative depth relationships accurately, which affects the tracking performance of SparseTrack. For videos with low frame rates \cite{bdd100k}, simple Kalman filter motion model struggles to obtain accurate motion cues, which would result in even less accurate pseudo-depth information. In the future work, we will explore the different decomposition methods that can be adapted to various tracking scenarios and aim to make the implementation more elegant.}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.97\linewidth, height=5cm]{imgs/dense-occ.pdf}
\caption{Visualization of dense occlusions. We visualize ground truth instances with visibility less than 0.15. The occluded targets in the crowd are often clustered, resulting in dense occlusions.}
\label{dense-occ}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.97\linewidth, height=4.3cm]{imgs/decomposition.pdf}
\caption{Visualization of decomposing dense occlusions. We use different color of bounding boxes to distinguish different target subsets obtained via dividing pseudo-depth levels.}
\label{decomposition}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.97\linewidth, height=24cm]{imgs/vis_compare.pdf}
\caption{\lzl{Comparable visualization of associating occlusive targets. We visualize the partial tracking results of SparseTrack and ByteTrack on the MOT17 and MOT20 data set, while keep the same settings of hyper-parameters on pre-trained detector. The tracking results of SparseTrack are displayed on the video clips in the} \textcolor{blue}{\textbf{blue box}}, \lzl{while the tracking results of ByteTrack are displayed on the video clips in the} \textcolor{red}{\textbf{red box}}. \lzl{The identical yellow numbers signify that the image cropping regions originate from the same frame.}}
\label{vis_occlusion}
\end{figure*}