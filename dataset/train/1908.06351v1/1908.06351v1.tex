\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\DeclareMathOperator*{\argmax}{argmax}

\usepackage{tabularx} \newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage{adjustbox} 

\usepackage[numbers]{natbib}
\usepackage{multirow}



\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Anomaly Detection in Video Sequence with Appearance-Motion Correspondence}

\author{Trong-Nguyen Nguyen, Jean Meunier\\
DIRO, University of Montreal\\
{\tt\small \{nguyetn, meunier\}@iro.umontreal.ca}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
Anomaly detection in surveillance videos is currently a challenge because of the diversity of possible events. We propose a deep convolutional neural network (CNN) that addresses this problem by learning a correspondence between common object appearances (\eg pedestrian, background, tree, etc.) and their associated motions. Our model is designed as a combination of a reconstruction network and an image translation model that share the same encoder. The former sub-network determines the most significant structures that appear in video frames and the latter one attempts to associate motion templates to such structures. The training stage is performed using only videos of normal events and the model is then capable to estimate frame-level scores for an unknown input. The experiments on 6 benchmark datasets demonstrate the competitive performance of the proposed approach with respect to state-of-the-art methods.
\end{abstract}

\section{Introduction}

Anomaly detection in video sequences is a necessary functionality for surveillance systems. Because abnormal events rarely occur in real-world videos, this task is significantly time-consuming and may require a large amount of resource (\eg people) to perform manual checking. A method than can automatically determine potential frames of anomalous events is thus crucial.



Our model is a combination of a convolutional auto-encoder (Conv-AE) and a U-Net with skip connections~\cite{Olaf2015UNet} that share the same encoder sub-network. Other related works employed either an AE or a U-Net to perform the anomaly detection in different ways. Hasan~\etal~\cite{Hasan2016Learning} estimate regularity score for frames in video sequences according to reconstruction models. Their two AEs (with and without convolutional layers) work on two different inputs: hand-crafted features (HOG and HOF with trajectory-based properties~\cite{Wang2013Action}) and concatenation of 10 consecutive frames along the temporal axis. The reconstruction error is used to indicate their regularity score. Unlike that work, the input of our Conv-AE is a single frame and the temporal factor is considered in the other stream via U-Net. The purpose of our Conv-AE is to learn only regular appearance structures.

On the contrary, Ravanbakhsh~\etal~\cite{Ravanbakhsh2017Abnormal} employ the U-Net structure proposed in~\cite{Isola2017Image} to translate an input from video frame to a corresponding optical flow and vice versa. We argue that the use of two CNNs with the same structure may be redundant and an appropriate modification and/or combination would improve the model ability. Compared with~\cite{Ravanbakhsh2017Abnormal}, our network keeps the stream translating a video frame to an optical flow (but using our proposed structure instead of~\cite{Isola2017Image}) while replaces the other U-Net by a Conv-AE that shares the encoding flow.

Inspired by the good performance of the video prediction model in~\cite{Mathieu2015Deep}, Liu~\etal~\cite{Wen2018Future} present a model that uses a U-Net structure to predict a frame from a number of recent ones and then estimates the corresponding optical flow. The model is optimized according to the difference between the outputted and original versions of video frame as well as the optical flow together with an adversarial loss. Our work also predicts an optical flow but directly from a single frame in order to determine the association between a scene appearance and its typical motion. Since a fixed procedure of optical flow estimation (FlowNet~\cite{Dosovitskiy2015FlowNet}) is embedded inside the network in~\cite{Wen2018Future}, the selection of such method is thus limited because the estimator has to be fully differentiable to perform an end-to-end training. Our model, however, has a stream that directly estimates a mapping from input frame to optical flow. We only use a pretrained estimator for ground truth calculation and the model signal does not propagate through it during the training as well as inference stages.

Our main contributions are summarized as follows:
\begin{itemize}
	\item We design a CNN that combines a Conv-AE and a U-Net, in which each stream has its own contribution for the task of detecting anomalous frames. The model can be trained end-to-end.
	\item We integrate an Inception module modified from~\cite{Szegedy2016Rethinking} right after the input layer to reduce the effect of network's depth since this depth is considered as a hyperparameter that requires a careful selection.
	\item We propose a patch-based scheme estimating frame-level normality score that reduces the effect of noise which appears in the model outputs.
	\item Experiments on 6 benchmark datasets demonstrate the potential of our model with competitive performance compared with state-of-the-art methods. We also provide discussions for these datasets that should be useful for future works.
\end{itemize}

The remainder of this paper is organized as follows: a summary of related studies is given in Section~\ref{sec:related}; Section~\ref{sec:method} describes the details of our method; experiments and discussions for the 6 benchmark datasets are presented in Section~\ref{sec:experiment}; and Section~\ref{sec:conclusion} concludes this work.

\section{Related work}\label{sec:related}
We briefly describe the principal categories that lead to very different approaches for anomaly detection in video.

\subsection{Trajectory}
The diversity of possible anomalous events is the main challenge of the anomaly detection problem. Some researchers simplify this issue by explicitly specifying anomalies (\eg~\cite{Sultani2018}) or particular relevant attributes that can be used effectively for anomaly detection, in which the most common one is motion trajectory. These studies aim to learn patterns of object trajectories determined from normal events~\cite{Medioni2001Event,Basharat2008Learning,Piciarelli2008Trajectory,Zhang2009Learning}. There are four main stages in the methodology including object detection, tracking, trajectory-based feature extraction and classification/detection. The advantages of methods in this category are the simple implementation and fast execution. However, their effectiveness may significantly degrade when working on videos with cluttered background since the trajectory determination depends on the result of object detection and tracking. Moreover, trajectory anomalies do not cover the whole spectrum of anomalies in video surveillance.

\subsection{Sparse coding}
Instead of explicitly defining and estimating specific anomaly attributes, other researchers consider an input sequence of frames as a collection of small 3D patches. Concretely, a number of consecutive frames are concatenated along the temporal axis and then split into same-size 3D patches according to a window sliding on the image plane. In the inference stage, each 3D patch extracted from unknown inputs is represented as a sparse combination of training samples of normal events. The reconstruction error is considered as the score supporting the final decision. Such sparsity-based methods have achieved state-of-the-art performances~\cite{Cong2011Sparse,Zhao2011Online}. The main drawback is the high computational cost in finding combination coefficients due to sparse representation. Some studies thus attempt to reduce the complexity by modifying the learning algorithms and/or data structures~\cite{Lu2013Abnormal,Luo2017A}. Beside window-based split, 3D patches are also determined using keypoint detectors~\cite{Cheng2015Video} while other researchers attempt to learn the relation between training patches according to their distribution~\cite{Mahadevan2010Anomaly} or graph-based representation~\cite{Kim2009Observe}.

\subsection{Deep learning}
Since deep learning models currently achieve top performance in a wide range of vision applications such as image classification~\cite{Krizhevsky2012ImageNet,Szegedy2015Going,He2016Deep}, object detection~\cite{Shaoqing2015Faster,He2017Mask} and image captioning~\cite{Johnson2016DenseCap,Karpathy2017Deep}, many CNNs have been proposed to deal with the problem of anomaly detection in videos. Typical structures of image reconstruction and translation are usually employed and the difference between their output and ground truth is used to indicate the frame-level score~\cite{Hasan2016Learning,Ravanbakhsh2017Abnormal,Wen2018Future}. Some researchers apply pretrained classification models (such as VGG~\cite{Simonyan2014Very}) to extract useful features from input videos~\cite{Sorina2017Deep,Ionescu2017Unmasking}. Results of object detection and/or foreground estimation are also used for the determination of anomalous events in~\cite{Hinami2017Joint,Xu2017Detecting}.



\section{Proposed method}\label{sec:method}
\begin{figure}[t]
\begin{center}
\begin{picture}(250,160)
\put(0,-10){\includegraphics[width=\columnwidth]{figures/network_with_legend_2}}
\put(40,115){\includegraphics[scale=0.5]{figures/overview/connection}}
\put(8,119){\includegraphics[scale=0.35]{figures/overview/frame.png}}
\put(70,134){\includegraphics[scale=0.35]{figures/overview/pred_frame.png}}
\put(70,104){\includegraphics[scale=0.35]{figures/overview/pred_flow.png}}
\put(25,150){}\put(112,152){}\put(112,122){}
\end{picture}
\end{center}
\caption{Overview of our model structure together with the spatial resolution of feature maps in each block (\ie a sequence of layers with the same output shape). The number of channels corresponding to each layer in each block is also presented (in parentheses). The input and two output layers have the same size of . There are three clusters of layers: common encoder (left), appearance decoder (top right) and motion decoder (bottom right). Each concatenation is performed along the channel axis right before operating the next deconvolution. The model input is a single video frame  and the outputs from the two decoders are a reconstructed frame  and an optical flow  predicting the motion between  and . Best viewed in color.}
\label{fig:overview}
\end{figure}
An overview of our model is visualized in Figure~\ref{fig:overview}. The model includes two processing streams. The first one is performed via a Conv-AE to learn common appearance spatial structures in normal events. The second stream is to determine an association between each input pattern and its corresponding motion represented by an optical flow of 3 channels ( displacements and magnitude). The skip connections in U-Net are useful for image translation since it directly transforms low-level features (\eg edge, image patch) from original domains to the decoded ones. Such connections are not employed in the appearance stream because the network may let the input information go through these connections instead of emphasizing underlying attributes via the bottleneck.

Our model does not use any fully-connected layer, so it can theoretically work on images of any resolution. In order to simplify the model as well as make it be appropriate for possible further extensions, we fixed the size of input layer as . The image size is set to a ratio of 1:1.5 instead of 1:1 as in related works (\eg~\cite{Hasan2016Learning,Sorina2017Deep,Wen2018Future}) in order to preserve the aspect of objects in surveillance videos.

\subsection{Inception module}
The Inception module was originally proposed to let a CNN decide its filter size (in a few layers) automatically~\cite{Szegedy2015Going}. A number of convolutional operations with various filter resolutions are performed in parallel and the obtained feature maps are then concatenated along the channel axis. The use of this module in our work can be explained under an alternative perspective as follows. The proposed network has an encoder-decoder structure with bottleneck. A very deep architecture may eliminate the features that are helpful for decoding. On the contrary, a shallow network takes the risk of missing high-level abstractions. Therefore, we apply an Inception module to let the model select its appropriate convolutional operations.

This work focuses on surveillance videos acquired from a fixed position. Given a convolutional layer with a predefined receptive field (\ie filter size) right after the input layer, the information abstraction would be different for the same object captured at various distances. This property is propagated for next layers, we thus expect the model to early determine low-level features by putting the Inception module right after the input layer. We remove the max-pooling in this module since the input is a regular video frame instead of a collection of feature maps. Our Inception module is modified from~\cite{Szegedy2016Rethinking} including 4 streams of convolutions of filter sizes , ,  and . Each convolutional layer of filter larger than  is factorized into a sequence of layers with smaller receptive fields in order to reduce the computational cost as suggested in~\cite{Szegedy2016Rethinking}.

\subsection{Appearance convolutional autoencoder}\label{sec:convAE}
Our Conv-AE supports the detection of strange (abnormal) objects within input frames by learning common appearance templates in normal events. This sub-network consists of the encoder and the top decoder without any skip connection as shown in Figure~\ref{fig:overview}. The encoder is constructed by a sequence of blocks including triple layers: convolution, batch-normalization (BatchNorm) and leaky-ReLU activation~\cite{Maas2013Rectifier}. The first block (right after the Inception module) does not contain BatchNorm layer as suggested in~\cite{Isola2017Image} for our U-Net task in Section~\ref{sec:unet}. Instead of using pooling layer to reduce the resolution of feature maps, we apply strided convolution. Such parametric operation is expected to support the network finding an informative way to downsample the spatial resolution of feature maps as well as learning the further upsampling in decoding stage~\cite{Springenberg2014Striving}.

The decoder is also a sequence of layer blocks that increases the spatial resolution while reduces the number of feature maps after each deconvolution layer. A dropout layer (with ) is attached before the ReLU activation in each block as a regularization that reduces the risk of overfitting during the training stage~\cite{Srivastava2014Dropout}.

Since the Conv-AE is to learn common appearance patterns of normal events, we consider the  distance between the input image  and its reconstruction . The model thus forces to produce an image with similar intensity for each pixel. The intensity loss is estimated as

A drawback of using only  loss is the blur in the output, we thus add a constraint that attempts to preserve the original gradient (\ie the sharpness) in the reconstructed image. The gradient loss is defined as the difference between absolute gradients along the two spatial dimensions as

where  denotes the image gradient along the -axis. The final loss function of the appearance Conv-AE is formed as a summation of the intensity and gradient losses.

This loss combination has been reported to give good performance for the task of video prediction~\cite{Mathieu2015Deep,Wen2018Future}.

\subsection{Motion prediction U-Net}\label{sec:unet}
Beside the appearance of strange object structures, unusual motions of typical objects would also be appropriate to provide an assessment of a video frame. Recall that each block in the encoder is to emphasize spatial abstractions of common objects within training frames. Our U-Net sub-network thus focuses on learning the association between such patterns and corresponding motions. The ground truth optical flow employed in this work is estimated by a pretrained FlowNet2~\cite{Eddy2017Flownet2}. Compared with related models, the optical flow outputted from FlowNet2 is not only much smoother but also preserves motion discontinuities with sharper boundaries. The motion stream is expected to associate typical motions to common appearance objects while ignoring the static background patterns.

The decoder of our U-Net has the same structure as the Conv-AE except for the skip connections. These concatenations are to combine the feature maps upsampled from a higher level of abstraction with the ones containing low-level details. The use of leaky-ReLU activation in the encoder also keeps weak responses that may be informative for the translation in the decoder.

Unlike the Conv-AE in Section~\ref{sec:convAE}, the loss between an outputted optical flow and its ground truth is measured by  distance. There are two main reasons for this. First, the FlowNet2 model is formed as a fusion of multiple networks providing optical flows from coarse (noisy) to fine (smooth), the result might thus contain noise or even amplify noisy regions during the smoothing procedure. Second, because the selection of optical flow estimation is not limited to FlowNet2, the training ground truth obtained from other algorithms might therefore possibly have small patches of wrong and/or noisy motion measure. In order to reduce the effect of such outliers when learning the motion association, we apply  distance loss

where  is the ground truth optical flow estimated from two consecutive frames  and , and  is the output of our U-Net given . In summary, this stream attempts to predict instant motions of objects appearing in the video.

\subsection{Additional motion-related objective function}
Beside the distance-based loss , we also add another loss that penalizes the underlying distribution of predicted optical flow to be similar to ground truth. The generative adversarial network (GAN)~\cite{Goodfellow2014Generative} was originally introduced to allow a CNN learning an implicit distribution of patterns. The model consists of a generator that creates fake samples from noise and a discriminator that attempts to distinguish such outputs from the real patterns. Many modified GAN versions have been proposed for the task of data generation. The discriminator also plays the role of a regularization in many models. Inspired by~\cite{Mathieu2015Deep} where using a GAN loss is reported to provide better results compared with employing only distance-based ones, we apply such strategy as an additional objective function.
\begin{figure}\begin{center}
\begin{picture}(250,100)
\put(0,-12){\includegraphics[width=\columnwidth]{figures/discriminator}}
\end{picture}
\end{center}
\caption{The architecture of our discriminator. The input layer of shape  is fed by the concatenation of a video frame and its optical flow (that is either ground truth or outputted from the U-Net). The output layer is sigmoid activation of 512 feature maps of spatial resolution . Best viewed in color.}
\label{fig:discriminator}
\end{figure}

Our generator is the entire network in Figure~\ref{fig:overview} while the discriminator conditionally performs the classification on predicted optical flow. A visualization of our discriminator architecture is shown in Figure~\ref{fig:discriminator}. Notice that the discriminator is not employed in the inference stage. Although the recent study~\cite{Wen2018Future} employed a Least Square GAN~\cite{Mao2017Least} and achieved state-of-the-art performance in detecting anomalous video frames, our model follows the strategy of typical conditional GAN (cGAN) where both the ground truth video frame and its corresponding optical flow are fed into the discriminator. There are two reasons leading to this decision. First, the cGAN theoretically avoids the problem of mode collapse in vanilla GAN since ground truth information (\ie labels, real samples) is fed into the discriminator. The model is thus expected to efficiently learn the distribution of training samples. Second, cGAN is appropriate for a CNN of image translation as demonstrated in~\cite{Isola2017Image}.

Finally, the adversarial loss is directly computed on the last layer containing activated feature maps in the discriminator. This calculation is different from~\cite{Isola2017Image,Wen2018Future} where a convolutional layer is employed to collapse previous feature channels into a 2D map. The common sense of our model and the two others is the structural penalization where the classification is performed according to image patches instead of the whole image. However, we strictly constrain patches at feature-level so that each feature map must attempt to provide a classification result. This design is inspired from the study~\cite{Long2017SCA} demonstrating that each convolutional channel attends to particular semantic patterns.

Given an input video frame  and its associated optical flow  obtained from FlowNet2, the proposed network in Figure~\ref{fig:overview} (the generator denoted as ) produces a reconstructed frame  and a predicted optical flow , while the discriminator  estimates a probability that the optical flow associated to  is the ground truth . The GAN objective function consists of two loss functions:


where  and  respectively indicate the spatial position and the corresponding channel of a unit in the feature maps outputted from , and  values are the weights associated to partial losses within our proposed model. Our GAN is optimized by alternately minimizing the two GAN losses. In our experiments (see Section~\ref{sec:experiment}), we assigned 0.25 for , 1 for  and 2 for . This GAN aims to emphasize the efficiency of motion prediction.

\subsection{Anomaly detection}\label{sec:detection}
Our model aims to provide a score of normality for each frame. In related studies, such scores are usually quantities measuring the similarity between a ground truth and the reconstructed/predicted output. There are two common scores employed in CNN approaches:  distance and Peak Signal to Noise Ratio (PSNR). The normality of each video frame is decided by comparing its score with a threshold. It is obvious that an anomalous event occurring within a small image region may be missed due to the summation and/or average operations over all pixel positions. We hence propose another score estimation scheme considering only a small patch instead of the entire frame.

First, we define partial scores individually estimated on the two model streams sharing the same patch position as

where  indicates an image patch and  is its number of pixels. Our frame-level score is then computed as a weighted combination of the two partial scores as follows:

where  and  are the weights calculated according to the training data,  is to control the contribution of partial scores to the summation, and  is the patch providing the highest value of  in the considering frame, \ie

The weights  and  are estimated as the inverse of average scores obtained on the training data of  images:

This helps to normalize the two scores on the same scale. The size of  was set to  in our experiments. Typically, such patches are determined by a sliding window. In realistic implementation, it can be performed using a convolutional operation with a filter of size .  was empirically set to 0.2 since the model focuses on motion prediction efficiency.

Finally, we perform a normalization on frame-level scores in each evaluated video as suggested in related studies such as~\cite{Hasan2016Learning,Ravanbakhsh2017Abnormal,Wen2018Future}. Our final frame-level score is

where  is the frame index in a video containing  frames. The score estimated from a frame of abnormal event is expected to be higher compared with the ones of normal event.

\section{Experiments}\label{sec:experiment}
\begin{figure}\begin{center}
\includegraphics[width=\columnwidth]{figures/dataset_marked.png}
\end{center}
\caption{Examples of normal (top) and abnormal (bottom) frames in the CUHK Avenue, UCSD Ped2, Exit Gate, and Entrance Gate (from left to right) datasets. Anomalous events are highlighted including a man picking a bag, bicycle appearance, and loitering.}
\label{fig:dataset}
\end{figure}

We performed experiments on various benchmark datasets of anomaly detection including CUHK Avenue~\cite{Lu2013Abnormal}, UCSD Ped2~\cite{Li2014Anomaly}, Subway Entrance Gate and Exit Gate~\cite{Adam2008Robust}, Traffic-Belleview and Traffic-Train~\cite{Zaharescu2010Anomalous}. Their training data contain only normal events. Some examples of normal and abnormal frames in the first 4 datasets are shown in Figure~\ref{fig:dataset}. The first two datasets are provided with frame-level ground truth, we thus employ area under curve (AUC) of the receiver operating characteristic (ROC) curve measured according to frame-level scores outputted from the proposed model to indicate the performance. The next two Subway datasets are evaluated on event-level that requires some additional operations described below. The last two datasets are evaluated according to the average precision (AP) since the precision-recall (PR) curve was usually used for their assessment~\cite{Zaharescu2010Anomalous,Xu2017Detecting}. We used the FlowNet2 pretrained on FlyingThing3D~\cite{Mayer2016A} and ChairsSDHom~\cite{Eddy2017Flownet2} datasets as the ground truth optical flow estimator. The GAN was trained using Adam algorithm~\cite{Diederik2014Adam} where the initial learning rates were set to  for the generator  and  for the discriminator . The description, experimental results and a discussion corresponding to each evaluation are presented in the remaining of this section.

\subsection{CUHK Avenue and UCSD Ped2}

\begin{table}
\begin{center}
	\begin{tabularx}{\columnwidth}{ |l| *{2}{Y|} }\hline
	Method & Avenue & Ped2 \\
	\hline\hline
Conv-AE~\cite{Hasan2016Learning} & 0.702 & 0.900 \\
	Discriminative learning~\cite{Giorno2016A} & 0.783 & - \\
	Hashing filters~\cite{Zhang2016Video} & - & 0.910 \\
Unmask late fusion~\cite{Ionescu2017Unmasking} & 0.806 & 0.822 \\
AMDN (double fusion)~\cite{Xu2017Detecting} & - & 0.908 \\
	ConvLSTM-AE~\cite{Luo2017Remembering} & 0.770 & 0.881 \\
	DeepAppearance~\cite{Sorina2017Deep} & 0.846 & - \\
	FRCN action~\cite{Hinami2017Joint} & - & 0.922 \\
	TSC~\cite{Luo2017A} & 0.806 & 0.910 \\
	Stacked RNN~\cite{Luo2017A} & 0.817 & 0.922 \\
	AbnormalGAN~\cite{Ravanbakhsh2017Abnormal} & - & 0.935 \\
	GrowingGas~\cite{Sun2017Online} & - & 0.941 \\
	Future frame prediction~\cite{Wen2018Future} & 0.851 & 0.954 \\
	\hline
	Our proposed method & 0.869 & 0.962 \\\hline
	\end{tabularx}
\end{center}
\caption{Frame-level performance (AUC) of anomaly detection on the CUHK Avenue and UCSD Ped2 datasets. The methods are ordered according to the year of publication.}
\label{table:avenueped2}
\end{table}
\begin{figure}[t]
\begin{center}
\begin{picture}(250,482)
	\put(0,378){\includegraphics[width=\columnwidth]{figures/results/UCSDped2_3_124.png}}
	\put(16,368){(a) The appearance of a truck and a bicycle. (Ped2)}
\put(0,253){\includegraphics[width=\columnwidth]{figures/results/UCSDped2_10_83.png}}
	\put(6,243){(b) A bicycle is running in a low contrast region. (Ped2)}
\put(0,128){\includegraphics[width=\columnwidth]{figures/results/Avenue_2_600.png}}
	\put(55,118){(c) A man is running. (Avenue)}
\put(0,3){\includegraphics[width=\columnwidth]{figures/results/Avenue_19_95.png}}
	\put(40,-7){(d) A man is tossing papers. (Avenue)}
\end{picture}
\end{center}
\caption{(Best viewed in color) Results on the Ped2 and Avenue datasets. Each example consists of 3 image columns that are input frame and its optical flow (left), reconstructed frame and predicted motion (middle), and the frame superimposed by the motion error map below (right). The flow field color coding is the same as~\cite{Eddy2017Flownet2}.}
\label{fig:avenue_ped2}
\end{figure}

The Avenue dataset consists of 30652 frames that are split into 16 clips for training and 21 clips for testing. This dataset was captured in a campus avenue and contains various types of anomaly such as unusual action (\eg running), wrong moving direction and abnormal object (\eg bicycle). This also provides some challenges for evaluation such as slight camera shake and the occurrence of a few outliers.

The UCSD anomaly dataset includes two subsets Ped1 and Ped2 acquired from static cameras overlooking pedestrian walkways. The anomalies are the appearance of non-pedestrian object (\eg vehicle) and strange pedestrian motion. The difference between the two subsets is the walking direction (toward and away from the camera in Ped1, parallel to the camera plane in Ped2). We select only the Ped2 dataset for two reasons. First, our optical flow estimator (FlowNet2) does not work well on very small and thin pedestrians appearing too far from the camera. Nevertheless, examples of people walking towards and away from the camera are available in the CUHK Avenue dataset allowing to evaluate performance in this situation. Second, we observed that some events were labeled as normality in the training data but were considered as anomalous in the test data (\eg people walking on grass). Therefore, the Ped2 dataset (16 training and 12 testing clips) was used in our experiments.

The frame-level assessment results in Table~\ref{table:avenueped2} show that our model outperforms all other recent methods in the task of anomaly detection. Examples of reconstructed frames and predicted optical flows obtained from the appearance and motion streams are given in Figure~\ref{fig:avenue_ped2}. Considering the first example, the truck was reconstructed as a collection of pedestrian patterns since it is a new object observed by the model. The corresponding predicted motion was thus completely different from the ground truth. The processing of the bicycle on the right image edge was also similar. The second scene shows that the model still worked well on a crowded scene with many pedestrians and an anomalous object having similar intensities with the background. In the next two Avenue frames, the model expected slower moving speed and another motion direction as observed in the training data. In addition, notice that the reconstructed man's trouser color was slightly different from the input frame while the back ground was well restored. This demonstrates that the model reasonably determined the low-significance relation between the color of a pattern and its movement.

\subsection{Subway Entrance and Exit gates}
This dataset contains videos capturing the entrance gate and exit gate of a subway station. Their lengths are respectively 96 and 43 minutes. The anomalous events in these two videos are wrong direction (\eg passenger exits through the entrance gate), no payment, loitering, irregular interaction (\eg a person walks awkwardly to avoid another) and miscellaneous (\eg sudden changing of walking speed).

We performed the evaluation according to the ground truth of events with the training and test sets provided in~\cite{Kim2009Observe}, in which the normal events in the first 15 minutes of the Entrance Gate video and 5 minutes of the Exit Gate were used in training stage. Notice that the experiments were performed individually for the two videos.

Since the dataset does not provide the frame-level ground truth, we employ the assessment scheme in~\cite{Hasan2016Learning} to determine anomalous events in the experiments. In detail, the persistence algorithm~\cite{persistence1d} is applied on the sequence of scores to locate local maxima, in which each maximum point indicates an anomalous event. In order to reduce the effect of possible noisy detected extrema, nearby events are combined to provide only an anomalous one.

\begin{table}
\begin{center}
\begin{tabularx}{\columnwidth}{ |l| *{4}{Y|} }
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{Method}} & \multicolumn{2}{c|}{Entrance (66)} & \multicolumn{2}{c|}{Exit (19)} \\ \cline{2-5} 
\multicolumn{1}{|l|}{} & \multicolumn{1}{c|}{TP} & \multicolumn{1}{c|}{FA} & \multicolumn{1}{c|}{TP} & \multicolumn{1}{c|}{FA} \\
\hline\hline
Subspace~\cite{Elhamifar2009Sparse} & 46 & 7 & 14 & 4 \\
MPPCA~\cite{Kim2009Observe} & 57 & 6 & 19 & 3 \\
DSC~\cite{Zhao2011Online} & 60 & 5 & 19 & 2 \\
Sparse dict.~\cite{Lu2013Abnormal} & 57 & 4 & 19 & 2 \\
Conv-AE~\cite{Hasan2016Learning} & 61 & 15 & 17 & 5 \\
IT-AE~\cite{Hasan2016Learning} & 55 & 17 & 17 & 9 \\
Hashing filters~\cite{Zhang2016Video} & 61 & 4 & 19 & 2 \\
Early fusion~\cite{Xu2017Detecting} & 56 & 8 & 15 & 4 \\
Late fusion~\cite{Xu2017Detecting} & 58 & 6 & 17 & 2 \\
AMDN~\cite{Xu2017Detecting} & 61 & 4 & 19 & 1 \\
\hline
Our method & 61 & 18 & 17 & 5 \\
\hline
\end{tabularx}
\end{center}
\caption{Our results of anomaly detection on the Subway datasets. In the ground truth, the numbers of abnormal events in the Entrance and Exit are respectively 66 and 19. The term TP indicates the number of true positive detections while FA is the counting of false alarms. The methods are listed in temporal order.}\label{table:subway}
\end{table}
\begin{figure*}[t]
\begin{center}
\begin{picture}(520,130)
\put(250,2){\includegraphics[scale=0.25]{figures/results/Entrance_18_700.png}}
	\put(300,2){\includegraphics[scale=0.25]{figures/results/Entrance_42_260.png}}
	\put(350,2){\includegraphics[scale=0.25]{figures/results/Entrance_33_492.png}}
	\put(400,2){\includegraphics[scale=0.25]{figures/results/Entrance_54_356.png}}
	\put(450,2){\includegraphics[scale=0.25]{figures/results/Entrance_71_450.png}}
	\put(270,-8){(f)}\put(320,-8){(g)}\put(370,-8){(h)}\put(421,-8){(i)}\put(470,-8){(j)}
\put(0,2){\includegraphics[scale=0.25]{figures/results/Entrance_0_150.png}}
	\put(50,2){\includegraphics[scale=0.25]{figures/results/Entrance_6_50.png}}
	\put(100,2){\includegraphics[scale=0.25]{figures/results/Entrance_28_230.png}}
	\put(150,2){\includegraphics[scale=0.25]{figures/results/Entrance_16_280.png}}
	\put(200,2){\includegraphics[scale=0.25]{figures/results/Entrance_5_95.png}}
	\put(21,-8){(a)}\put(71,-8){(b)}\put(120,-8){(c)}\put(170,-8){(d)}\put(220,-8){(e)}
\end{picture}
\end{center}
\caption{Examples of missed detections (a)-(e) and false alarms (f)-(j) in our experiments on the Entrance dataset. Each example consists of 4 images that are (from top to bottom) the input frame, ground truth optical flow, predicted motion and the corresponding motion error map. The missed detections are: (a)-(c) movement stopping, (d) loitering, and (e) loitering (man) and movement stopping (woman). The false alarms are: (f)-(g) movement stopping, (h) loitering, (i) changing gate, and (j) passenger going near the railway. Best viewed in color.}
\label{fig:subway}
\end{figure*}
Our event-based assessment results are presented in Table~\ref{table:subway}. It shows that our model detected most anomalous events but also generated more false alarm than other recent studies. By taking a closer look at these false alarms, we determined that some events denoted as normal in the test set can be considered as anomaly under other circumstances. A visualization of some false alarms and missed anomaly detections in the Entrance dataset is given in Figure~\ref{fig:subway}.

Figure~\ref{fig:subway} shows that the normality decision of movement stopping and loitering was unstable since the cases (a)-(e) were missed while (f)-(h) were wrongly detected. There are two possible reasons: (1) the use of maximum localization as in~\cite{Hasan2016Learning} is not ideal when the anomaly score smoothly and/or slowly changes, and (2) the training set (according to~\cite{Kim2009Observe}) contains loitering event [caused by the man in (b) and (e)]. The ambiguity in ground truth annotation is also shown in the event (h) where a loitering man appeared on the right side but was not labeled as anomaly. In the event (i), the model predicted that the man would go through the left gate but he suddenly changed to the right one (the color indicates the motion direction). Since this action does not occur in the training data, the model determined it as an anomalous event. Regarding the last example (j), the motion stream expected the passenger to go to the train because most people at this location move to the left side in the training data. In other words, the model may \textit{forget} training patterns moving to the right side. In this case, using sparse coding approaches~\cite{Kim2009Observe,Zhao2011Online,Lu2013Abnormal} can be appropriate since the effect of the frequency of training patterns is reduced.

\subsection{Traffic-Belleview and Traffic-Train}
The Traffic-Belleview dataset was acquired by a surveillance camera looking at the traffic on a road intersection from a high viewpoint. In the training data (300 frames), vehicles only run on the main street. The appearance and movement of vehicles from/to left or right roads is defined as anomaly in the test set containing a total of 2618 frames. The video is gray-scale and has a low quality.

Unlike the previous benchmark datasets, the Traffic-Train can be considered as the most challenging dataset since the lighting conditions vary drastically together with camera jitter. The camera was mounted in a train and people movement is defined as anomaly. The training and test sets consist of 800 and 4160 frames, respectively.

Our average precision of frame-level assessment is presented in Table~\ref{table:belleview_train}. Figure~\ref{fig:belleview_train} shows examples of problems that the model encountered when dealing with the traffic datasets as well as illustrates the change of lighting conditions in the Train dataset. In Figure~\ref{fig:belleview_train}(b), the predicted motion was very noisy and the passenger at the frame center was missed in the error map. The effect of optical flow estimator is illustrated in Figure~\ref{fig:belleview_train}(c) where two cars were combined to be a big blob. This bad estimation significantly affected the error map though the three cars running on other way were correctly determined. The results may thus be improved by choosing another optical flow estimator or tuning the pretrained FlowNet2 by a more appropriate dataset.
\begin{table}
\begin{center}
	\begin{tabularx}{\columnwidth}{ |l| *{2}{Y|} }\hline
	Method & Belleview & Train \\
	\hline\hline
	GANomaly~\cite{Samet2018GANomaly} & 0.735 & 0.194 \\
	AEs + local feature~\cite{Narasimhan2018Dynamic} & 0.748 & 0.171 \\
	AEs + global feature~\cite{Narasimhan2018Dynamic} & 0.776 & 0.216 \\
	ALOCC ~\cite{Sabokrou2018Adversarially} & 0.734 & 0.182 \\
	ALOCC ~\cite{Sabokrou2018Adversarially} & 0.805 & 0.237 \\
	\hline
	Our proposed method & 0.751 & 0.490 \\SSIM on appearance stream & 0.830 & 0.798 \\\hline
	\end{tabularx}
\end{center}
\caption{The average precision of frame-level anomaly detection on the Traffic-Belleview and Traffic-Train datasets.}
\label{table:belleview_train}
\end{table}
\begin{figure}[t]
\begin{center}
\begin{picture}(250,303)
	\put(0,252){\includegraphics[width=\columnwidth]{figures/results/Train.png}}
	\put(10,242){(a) The change of lighting in the Traffic-Train dataset.}
\put(0,127){\includegraphics[width=\columnwidth]{figures/results/Train_0_2848.png}}
	\put(28.5,117){(b) Passengers moving in the stopping train.}
\put(0,2){\includegraphics[width=\columnwidth]{figures/results/Belleview_0_237.png}}
	\put(54,-8){(c) Cars turning to the left way.}
\end{picture}
\end{center}
\caption{(Best viewed in color) Some testing results on the two traffic datasets. Each example consists of 6 images as in Figure~\ref{fig:avenue_ped2}.}
\label{fig:belleview_train}
\end{figure}

As an attempt to reduce the effect of such factors, we estimated another frame-level score without the support of motion as in section~\ref{sec:detection}. Concretely, we used the Structural Similarity Index (SSIM)~\cite{Wang2004Image} to compute the similarity between an input frame and its reconstruction provided by the appearance stream. Compared with other common measures such as MSE or PSNR, SSIM can work well on jitter images where pixel by pixel comparison is not appropriate. Table~\ref{table:belleview_train} shows that this modification improved the anomaly detection results, especially with the Train dataset.

Further details including ROC and PR curves, visualization of some feature maps and evaluation results of each single stream are provided in the supplementary materials.

\section{Conclusion}\label{sec:conclusion}
This paper presents an anomaly detection approach that exploits the correspondence between pattern appearances and their motions. The model is designed as a combination of two streams. The first one attempts to reconstruct the appearance according to its auto-encoder architecture while the second stream uses a U-Net structure to predict the instant motion given an input video frame. By sharing the same encoder, the model is forced to learn the correspondence. A patch-based scheme of anomaly score estimation is proposed to reduce the effect of noise in model outputs. Experiments on 6 benchmark datasets demonstrated the potential of our method. Detailed discussions are also presented to provide improvement suggestions for further works.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
