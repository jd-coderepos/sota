



Our Semantic-Aware Local-Global Vision Transformer (\emph{SALG} Transformer) is designed as a general-purpose Transformer for learning visual features, which can be further utilized in various downstream tasks in Computer Vision. Given an input image, the proposed \emph{SALG} Transformer first partitions it into semantic regions in an unsupervised way, which explores the underlying semantic priors in the image. Then \emph{SALG} Transformer employs the designed Local-Global Transformer block iteratively to perform both local intra-region self-attention within each semantic region and global inter-region feature propagation between regions. Consequently, our \emph{SALG} Transformer is able to not only perform fine-grained feature learning locally for each region which can substantially reduce the computational complexity than the global learning manner of conventional Transformer, but also model global dependencies at the region level efficiently, thereby learning effective visual features for downstream tasks.

In this section we will first present the overall framework of the proposed \emph{SALG} Transformer. Then we will elaborate on the unsupervised semantic-aware region segmentation and the designed Local-Global Transformer block in our \emph{SALG} Transformer, respectively. 


\subsection{Overall Framework}
\label{sec:3.1}
Figure~\ref{fig:fig_sim} illustrates the overall architecture of our \emph{SALG} Transformer. It learns the visual representations for an input image in an iterative manner by four cascaded feature learning stages, including three specialized \emph{SALG} stages consisting of the designed region segmentation module and Local-Global Transformer block, and one normal stage based on typical Transformer block. 

Given an input image of size $H \times W \times 3$, the proposed \emph{SALG} Transformer first employs a patch embedding layer to project the input image to a feature map of size $\frac{H}{4} \times \frac{W}{4} \times C$, where $C$ is the feature dimension. Recent research~\cite{wang2021pyramid,wang2022scaled,ren2022shunted} has shown that using convolutional operations to perform feature transformation with overlapping of receptive fields between tokens rather than splitting the input image into non-overlapping patches (as ViT~\cite{dosovitskiy2020image} and Swin Transformer do) can obtain higher-quality tokens. Specifically, we use three convolutional layers in the patch embedding layer, among which the stride of the first two convolutional layers are set to 2 to downsample the input image into the size of $\frac{H}{4} \times \frac{W}{4}$. The third convolutional layer with stride 1 is used for feature transformation.

The obtained transformed feature map is then fed into three specialized \emph{SALG} stages and one normal stages sequentially for feature learning. Each \emph{SALG} stage comprises two steps: region segmentation and feature learning. The feature map is partitioned into semantic regions by the designed Semantic-Aware Region Segmentation module in an unsupervised way. Different from the typical way of equally partitioning the feature map into regular windows by most existing methods~\cite{liu2021swin,chen2021regionvit,chu2021twins,vaswani2021scaling,huang2021shuffle,fang2022msg}, our \emph{SALG} Transformer aims to segment the feature map into semantic regions that could be irregular and thus explore the underlying semantic priors. For instance, as shown in Figure~\ref{fig:fig_sim}, the pixels (tokens) from a same object or a same patch of background tend to be partitioned into the same region. 


The segmented regions are then processed by the Local-Global Transformer block to learn visual representations. In particular, it first performs local intra-region self-attention for learning fine-grained features for each segmented region. Then the Local-Global Transformer block models the global dependencies between different regions via global inter-region feature propagation. As a result, our model is able to obtain the global view when refining features for each token, which is the essential advantage of Transformer. Meanwhile, our model substantially reduces the computational complexity compared to the typical global self-attention operation of Transformer, benefiting from such Local-Global stepwise learning mechanism.

Similar to Swin Transformer, we adopt the patch merging operation, implemented by a linear transformation layer, to gradually reduce the size of feature maps between stages and thus learn a hierarchical representation. Three Patch Merging modules between four learning stages compress the feature map from $\frac{H}{4} \times \frac{W}{4}$ to $\frac{H}{32} \times \frac{W}{32}$, with increasing feature dimensions (channels) for each pixel. The obtained hierarchical representations can be further used for downstream tasks including image-level prediction like image classification and pixel-level prediction such as object detection or semantic segmentation.

\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{Images/figure3_5.pdf}
\caption{Unsupervised semantic-aware region segmentation. (a) Our model first splits the feature map into non-overlapping regular windows uniformly and calculate the average features for each window to approximate the mean representations for semantic regions to be segmented, dubbed `region means'. (b) We define the coverage area for each region mean by expanding its window with $p$ pixels on all four sides. (c) A token (pixel) is assigned to the region mean with maximal Cosine similarity among all region means covering the token.}
\label{fig:fig_seg}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{Images/figure5_6.pdf}
\caption{Visualization of hierarchical region segmentation by our \emph{SALG} Transformer for a randomly selected sample from Imagenet-1K validation set. The semantic segmentation results in three \emph{SALG} stages are presented. The deeper stages have less segmented regions which correspond to larger receptive fields in the input image.}
\label{fig:hie_segm}
\end{figure*}

\subsection{Unsupervised Semantic-Aware Region Segmentation}
\label{sec:3.2}
We perform semantic-aware region segmentation in an unsupervised way, which does not require the pixel-level annotation. An intuitive way is to perform token (pixel) clustering, for which K-means clustering algorithm is a straightforward method. However, K-means is inefficient due to its iteratively refining strategy. Moreover, performing k-means over the entire feature map ignores the positional continuity between tokens and tends to produce a lot of small disconnected regions belonging to a same cluster. To circumvent above two issues, we adapt K-means algorithm to our Semantic-Aware Region Segmentation module in both core steps: calculation of mean representations for clusters (regions) and token assignment for segmentation. Both steps only need to be conducted once in our Semantic-Aware Region Segmentation module, which is quite efficient.

\smallskip\noindent\textbf{Estimation of mean representations for regions.}
Instead of iteratively updating the cluster means in K-means algorithm, we estimate $K$ mean representations for total $K$ regions (clusters) in the feature map directly. Specifically, we divide the feature map into $n \times n$ non-overlapping rectangle windows uniformly, where $n \times n = K$, then we calculate the average features over all tokens for each window to approximate mean representations of $K$ regions, as shown in Figure~\ref{fig:fig_seg}(a). Estimating region means in such a way leads to two potential merits: 1) the positional continuity is taken into account since each region mean is estimated from a local window of tokens instead of scattered tokens; 2) all region means can be calculated in parallel to achieve high efficiency. 

\smallskip\noindent\textbf{Token assignment for region segmentation.} Similar to K-means algorithm, we assign each token to the region mean with the nearest semantic distance. To preserve the positional continuity, we only assign a token to its nearby region means. Formally, for each region mean, we define its coverage area by expanding its window with $p$ pixels on all four sides, resulting in an area of $(\frac{H'}{n}+2p) \times (\frac{W'}{n}+2p)$. Here $H'$ and $W'$ are the height and width of the feature map being processed, respectively. Consequently, a token can fall into the coverage area of more than one region means due to the overlapping between coverage areas of regions means, as shown in Figure~\ref{fig:fig_seg}(b). Then we calculate the Cosine similarity in the feature space between a token located at $<u,v>$ to all region means covering it, denoted as a set of region means $\mathcal{S}$, and select the region mean $r_{<u,v>}$ with the maximal similarity for assignment:
\begin{equation}
\begin{split}
& s_{<u,v>}^i = \frac{\mathbf{m}_i \cdot \mathbf{F}_{u,v}}{ \Vert \mathbf{m}_i \Vert \Vert \mathbf{F}_{u,v} \Vert}, \qquad \forall i \in \mathcal{S},\\
& r_{<u,v>} = \text{argmax}_{i}{s_{<u,v>}^i}.
\end{split}
\label{eqn:token_assign}
\end{equation}
Herein, $s_{<u,v>}^i$ is the Cosine similarity between the representation $\mathbf{m}_i$ of the $i$-th region mean and the token feature $\mathbf{F}_{u,v}$ located at $<u,v>$. In this way, the whole feature map is segmented into semantic regions in an unsupervised way while basically preserving the positional continuity, as illustrated in Figure~\ref{fig:fig_seg}(c). 

\smallskip\noindent\textbf{Hierarchical region segmentation.} We perform region segmentation in each of the  three \emph{SALG} stages while keeping the window size of region means consistent across different stages. Thus, deeper stages have less regions, each region corresponding to a larger receptive field in the input image, as shown in Figure~\ref{fig:hie_segm} . As a result, we obtain a hierarchical region segmentation, which enables our model to capture the object semantics in  different scales. It is particularly advantageous to tasks of pixel-level predictions like object detection or semantic segmentation, which is experimentally validated in Section~\ref{sec:4.2}.

\smallskip\noindent\textbf{Seamless integration into \emph{SALG} Transformer.} Besides the computational efficiency, another prominent advantage of the proposed Semantic-Aware Region Segmentation module is that it can be seamlessly integrated into the our \emph{SALG} Transformer, which allows for both the end-to-end inference and optimization for the whole model.


\subsection{Local-Global Transformer Block}
\label{sec:3.3}
We design the Local-Global Transformer block of our \emph{SALG} Transformer to learn visual features from segmented regions in two successive steps as shown in Figure~\ref{fig:local-global}: local feature learning within each region and global feature propagation between regions, which allows \emph{SALG} Transformer to learn features efficiently in a global view.


\begin{figure}[!t]
\centering
\includegraphics[width=1.0\linewidth]{Images/figure4_6.pdf}
\caption{Illustration of Local-Global Transformer block, which consists of local intra-region self-attention and global inter-region feature propagation. The local intra-region self-attention is performed in the coverage area of equal size for each semantic region while masking out the tokens (denoted as `null token' in the figure) that do not belong to current region for parallel computing. The inter-region feature propagation is performed on the ARG tokens which aggregate representative features for each region.}
\label{fig:local-global}
\end{figure}

\smallskip\noindent\textbf{Intra-Region Self-Attention for learning local object-wise semantics.}
The segmented semantic regions generally correspond to either a (part of) object or a patch of background. We first perform fine-grained feature learning within each region to extract object-wise semantics. To this end, we perform intra-region self-attention operations for each region individually. Different from the typical methods that partition the feature map equally, our \emph{SALG} Transformer obtains irregular regions of unequal size from the Semantic-Aware Region Segmentation module. To enable parallel intra-region self-attention for all regions, for each region we implement local self-attention operation in the coverage area of the corresponding region mean while masking out the tokens in the coverage area that do not belong to current region (denoted as null tokens in the figure). Such implementation is based on three observations: 1) all region means have the same size of coverage area, i.e., $(\frac{H'}{n}+2p) \times (\frac{W'}{n}+2p)$, 2) each segmented region is bounded by the coverage area of its region mean, 3) the spatial structure of all tokens in the entire feature map is preserved, which allows for the straightforward incorporation of the relative position encoding.

For a segmented region, apart from the normal tokens in the coverage area of its region mean, we also add a learnable token, termed as Aggregation (AGR) Token shown in Figure~\ref{fig:local-global}, to learn the aggregated representation for the entire region. The AGR token is responsible for feature interaction between regions during the inter-region feature propagation and also providing the global information of the whole feature map in the operation of intra-region self-attention. As a result, our \emph{SALG} Transformer is able to obtain the global view when refining features for each token locally via the intra-region self-attention. Formally, denoting the feature map of a segmented region $r$ as $\mathbf{F}_r$, the intra-region self-attention is modeled as:
\begin{equation}
\begin{split}
    &\mathbf{F}'_r = \mathbf{F}_r \odot \mathbf{M}_r, \\
    &\mathbf{x}_r = \text{Concat} (\mathbf{t}^\text{AGR}_r, \text{Flatten}(\mathbf{F}'_r)), \\
    &\mathbf{x}_{r} = \text{Local-MSA}(\mathrm{LN}(\mathbf{x}_{r}))+\mathbf{x}_{r},\\
    &\mathbf{x}_{r} = \text{MLP}(\text{LN}(\mathbf{x}_{r}))+\mathbf{x}_{r}.
\end{split}
\end{equation}
Here $\mathbf{M}_r$ denotes the mask to indicate whether a token in a coverage area belong to current region (1 for true and 0 for false) and $\mathbf{t}^\text{AGR}_r$ is the feature for AGR token for $r$. `Local-MSA' refers to the local operation of Multi-head Self-Attention within the coverage area of the region mean for $r$. The operation `Flatten' reshapes the coverage area $\mathbf{F}'_r$ into an array of tokens. As other Vision Transformers~\cite{liu2021swin} do, the relative position encoding~\cite{shaw2018self} is used to provide relative spatial information during feature learning.

\smallskip\noindent\textbf{Inter-Region Feature Propagation for modeling global dependencies.}
Our \emph{SALG} Transformer performs inter-region feature propagation to model the global dependencies between different regions. Specifically, we perform feature interaction between AGR tokens of different regions to improve the computational efficiency, as illustrated in Figure~\ref{fig:local-global}. The refined AGR tokens after current inter-region propagation are then fed to the intra-region self-attention in the next layer, which provides global information of the whole feature map for local feature learning for each tokens. Such local-global stepwise learning mechanism substantially reduce the computational complexity while preserving the essential advantage of Transformer, namely learning features for each token in a global view.

We investigate two optional implementations for inter-region feature propagation. The first way is to apply Multi-head Self-Attention (MSA) to total $n^2$ AGR tokens to propagate features, which is modeled by:
\begin{equation}
    \begin{split}
    &\mathbf{T}^\text{AGR} = \text{Concat}(\mathbf{t}^\text{AGR}_1, \dots, \mathbf{t}^\text{AGR}_{n^2}), \\
    &\mathbf{T}^\text{AGR} = \text{MSA}(\mathrm{LN}(\mathbf{T}^\text{AGR})+\mathbf{T}^\text{AGR},\\
    &\mathbf{T}^\text{AGR} = \text{MLP}(\text{LN}(\mathbf{T}^\text{AGR}))+\mathbf{T}^\text{AGR}.
    \end{split}
    \label{eqn:agr-msa}
\end{equation}
While such way of inter-region feature propagation is able to achieve deep feature interaction between AGR tokens of different regions, a potential limitation is that it introduces extra parameters and thus increases the computational complexity to some degree. A more simple and efficient solution is to perform average fusion on all AGR features via an averaging pooling (AP) operation:
\begin{equation}
\begin{split}
&\mathbf{T}^\text{AGR} = \text{Concat}(\mathbf{t}^\text{AGR}_1, \dots, \mathbf{t}^\text{AGR}_{n^2}), \\
&\mathbf{T}^\text{AGR} = \text{AP}(\mathbf{T}^\text{AGR}),
\end{split}
\label{eqn:agr-avg}
\end{equation}
where the obtained $\mathbf{T}^\text{AGR}$ is assigned to each of all AGR tokens. In this case, all AGR tokens are treated equally and no fine-grained distinctions between regions are modeled during the inter-region feature propagation. We conduct experimental comparison for ablation study in Section~\ref{sec:4.3}.

\subsection{Different Scales of Architecture Variants}
\label{sec:3.4}
Our \emph{SALG} Transformer first performs semantic region segmentation to explore the semantic priors, and then learns fine-grained features locally within each segmented region and models the inter-region dependencies globally. Compared to typical Vision Transformers that partition the feature map into equal windows without considering the semantic distributions of tokens, our model can potentially learn more effective features with the comparable model size and computational complexity due to the explicit modeling of the semantic priors. Our \emph{SALG} is particularly advantageous for small-scale models when the modeling capacity is not sufficient to learn semantics implicitly. 

To have a fair and extensive comparison with the existing Vision Transformers, especially in the small-scale modeling scenarios, we build our \emph{SALG} in various scales, to have comparable model size and computational complexity with the corresponding versions of DeiT and Swin Transformer. To be specific, we build tiny (\emph{SALG}-T) and small (\emph{SALG}-S) versions, which correspond to the tiny and small versions of Swin, as well as the small and base versions of DeiT respectively. Besides, we also introduce an extra super-tiny version (\emph{SALG}-ST), which is even smaller than the tiny version, to investigate the effectiveness of our model in extremely small scale. We also build the corresponding super-tiny version for both DeiT and Swin accordingly for comparison. Note that we do not consider the `base' and `large' versions of different models since we focuses on small-scale modeling scenarios. We adjust the feature dimension $C$ and the numbers of Local-Global Transformer block in each stage to achieve different scales of our model:
\begin{itemize}
    \item \emph{SALG}-ST: C = 48, block numbers = \{2, 2, 2, 2\};
    \item \emph{SALG}-T: C = 96, block numbers = \{2, 2, 6, 2\};
    \item \emph{SALG}-S: C = 96, block numbers = \{2, 2, 18, 2\}.
\end{itemize}

\begin{comment}
An image always consists of foreground objects and background. Vision Transformer divides the image into different tokens and  aims to learn focusing attention on foreground object tokens, which is important in tasks like classification and detection. So it is make sense to segment the image into different regions and the tokens in each region belong to the same object or the background. Then we perform self-attention within regions rather than regular local windows as Swin and Twins do. Moreover, information exchange between different regions enables the model to model global information of the image.  
Specifically, given a feature map, we show how to divide it  into irregular regions according to the semantic information of tokens in Sec 3.1,  and in Sec 3.2 we introduce the structure of Intra-Inter-region Block and the way of information exchange. In the last part, we show the overall architecture of our Intra-Inter-Region Transformer model.

\subsection{Region Segmentation}
\label{sec:3.1}
Given a feature map $x \in \mathbb{R}^{H \times W \times C}$ in each Intra-Inter-Transformer stage, where (H, W) represents the two-dimensional resolution of the input feature, C represents the feature dimension ,  we adopt the idea of clustering to segment it into different regions: we take two steps in the process of segmentation: 1) determine the center point of each region, and 2) divide the tokens of the feature map into non-overlapping regions based on the the semantic distance between the current token and different center points. 

In order to realize the parallel calculation of the model, the number of regions segmented by different feature maps in the same batch should be consistent. Therefore, a sliding window is used to obtain the center point of each region. As shown in Figure 2(a), we divide the two-dimensional feature map $x \in \mathbb{R}^{H \times W \times C}$ into non-overlapping windows of size $w \times w $, and the number of windows is equal to the number of divided regions. After average pooling within the tokens in each window, we obtain the feature of the initial center points $center \in \mathbb{R}^{{\frac{H}{w}} \times {\frac{W}{w}} \times C}$ , where ${\frac{H}{w}} \times \frac{W}{w}$ is the number of center points.



As we get ${\frac{H}{w}} \times \frac{W}{w}$  center points, we are supposed to segment the feature map into ${\frac{H}{w}} \times \frac{W}{w}$  regions based on the semantic information of tokens. The original clustering algorithm determines the clustering result based on the distance between the feature of each member and the center points. However, applied to the image domain, this approach is not only computationally intensive, but also generates many disconnected regions, which obviously violates the local characteristic of the image. So we adopt an overlapping window as the segmentation boundaries for each region. Specifically, $p$ tokens are padded around the non-overlapping windows, so that the size of the window becomes $w+2p$, and adjacent windows generate overlapping areas, as shown in Figure 2(b). Then, for a token $x_{u,v}$ whose spatial position is $(u, v)$ on the two-dimensional feature map , we denote that the set of center points of all overlapping windows where it is located is E, where $E \in \mathbb{R}^{N \times C}$ and N represent the the number of center points in set E.
Then we conduct region segmentation according to the cosine similarity of $x_{u,v}$ and every center point in set E:


\begin{equation}
\label{deqn_ex1a}
sim_{u,v}^i = \frac{\left<E_i,x_{u,v} \right>}{\Vert E_i \Vert \Vert x_{u,v} \Vert} \qquad i\in [0,N-1],
\end{equation}

\begin{equation}
\label{deqn_ex1a}
region_{u,v} = argmax(sim_{u,v}^0, sim_{u,v}^1,\cdots, sim_{u,v}^{N-1} ),
\end{equation}
where $sim_{u,v}^i$ is the cosine similarity of  $x_{u,v}$ and $E_{i}$, the i-th center point in set E , and the argmax function finds the center point with the highest similarity to the current token among the N candidates and  make $region_{u,v}$ to be the result of region assignment.

After region segmentation, each token is uniquely assigned to one of the ${\frac{H}{w}} \times \frac{W}{w}$ regions. The segmentation result of the sample image is shown in Figure 2(c), the feature map is divided into irregular regions. Tokens in each region either belong to the same foreground object or to the background, containing similar semantic information. 

\subsection{Intra-Inter-Region Transformer Block}
\label{sec:3.2}
\textbf{Intra-Region Self-Attention Modeling}\quad
After region segmentation for the two-dimensional feature map input to the model, we obtain non-overlapping ${\frac{H}{w}} \times \frac{W}{w}$ regions. Corresponding to the original image, tokens in the same region may have similar texture features, or belong to the same object in the image. So we perform self-attention within each region to model the local information of the image  better .


As shown in Figure 1(c), the feature map is divided into irregular regions in the process of region segmentation, which contains a different number of tokens. 
To perform parallel computation, we use the mask mechanism in intra-region self-attention modeling.
Due to the existence of segmentation boundaries in the process of segmentation, as mentioned in Sec A, the maximum number of tokens in each region is $(w+2p)^2$, which is the size of the overlapping window. Therefore, we mask the tokens that do not belong to this region in each overlapping window.
As indicated in Figure 3, the blank tokens in each region represent the masked tokens. 
In addition, after adding the mask tokens, the tokens of each region can maintain their own relative spatial position. Therefore, by adding the relative position encoding, the model can learn the spatial information and semantic information of the tokens at the same time.

As shown in Figure 3, in addition to the normal tokens on the feature map, we add an additional aggregation (AGR) token to each region as a representative token, which aggregate the  semantic information of the region and provide conditions for the information exchange between regions in Inter-Region Transformer Block.

Then we apply local multi-head self-attention to AGR  token and the normal tokens within each region. Concretely, after region segmentation for the input feature map $x \in \mathbb{R}^{H \times W \times C}$, for each region in all ${\frac{H}{w}} \times {\frac{W}{w}}$  regions, we obtain new regional features $x_r \in \mathbb{R}^{(w+2p)^2 \times C}$ , which is concatenated to ${\frac{H}{w}} \times {\frac{W}{w}}$ AGR tokens ,whose feature is denoted as $T_{AGR} \in \mathbb{R}^{{\frac{H}{w}} \times {\frac{W}{w}} \times  C}$. Then we get the input of Intra-Region Transformer block $x_r^{'} \in \mathbb{R}^{{\frac{H}{w}} \times {\frac{W}{w}} \times [(w+2p)^2+1] \times C}$. The Intra-Region self-attention process is as follows:

\begin{equation}
\label{deqn_ex1a}
x_{r}^{'} = \mathrm{{Concat}}(T_{AGR},x_r).
\end{equation}

\begin{equation}
\label{deqn_ex1a}
x_{r}^{'} = \mathrm{{Local\raisebox{0mm}{-} MSA}}(\mathrm{LN}(x_{r}^{'}))+x_{r}^{'}.
\end{equation}

\begin{equation}
\label{deqn_ex1a}
x_{r}^{'} = \mathrm{MLP}(\mathrm{LN}(x_{r}^{'}))+x_{r}^{'},
\end{equation}


\textbf{Inter-Region Self-Attention Modeling}\quad
Transformer can model the long-distance dependencies of tokens on the feature map, so after self-attention within each regions, it is necessary to exchange information between different regions.

Swin Transformer shifts the split windows in different layers and then stacks transformer blocks to build implicit connections between windows. We add an extra AGR token for each region to aggregate the information of the region. Through the interaction between AGR Tokens, we can exchange information between different regions.

In our implements, we try two different ways to exchange message between AGR tokens. First, we adopt the method of self-attention modeling between AGR tokens. After Intra-Region Transformer block, the AGR token of each region are sent to the Inter-Region Transformer block together for multi-head self-attention modeling, so that each AGR Token learns the information of other regions. This method enables the AGR token to adaptively learn information from other regions, but assigns more parameters to the model, as we add an extra Transformer block in each Transformer layer. As shown in Figure 3, after the Intra-Region self-attention calculation, the model outputs the feature with AGR tokens $x_{r}^{'} \in \mathbb{R}^{{\frac{H}{w}} \times {\frac{W}{w}} \times [(w+p)^2+1] \times C}$. Then we extract the features of AGR tokens $T_{AGR} \in \mathbb{R}^{{\frac{H}{w}} \times {\frac{W}{w}}\times C}$ and input it to the Inter-Region Interaction Block to exchange message between different regions. When we choose to conduct inter-regional information exchange with self-attention modeling, the calculation is as follows:

\begin{equation}
\label{deqn_ex1a}
T_{AGR} = \mathrm{Global\raisebox{0mm}{-}MSA}(\mathrm{LN}(T_{AGR}))+T_{AGR}.
\end{equation}

\begin{equation}
\label{deqn_ex1a}
T_{AGR} = \mathrm{MLP}(\mathrm{LN}(T_{AGR}))+T_{AGR}.
\end{equation}




Besides, we tried another simple yet effective method: applying a global average pooling(GAP) layer for the AGR tokens to fuse the information of different regions, and then repeat the obtained token for $r$ times as the initial AGR token of each region of the next Intra-Transformer block. 

\begin{equation}
\label{deqn_ex1a}
T_{AGR} = \mathrm{GAP}(T_{AGR})+T_{AGR}.
\end{equation}

\begin{figure*}[ht]
\centering
\includegraphics[width=5.5in]{Images/overall.png}
\caption{ Overall architecture of Intra-Inter-Region Transformer.}

\label{fig_sim}
\end{figure*}

\subsection{Overall Architecture And Model Variants}
\label{sec:3.4}
 As shown in Figure 1, we adopt a pyramid-shaped hierarchical Transformer structure like Swin\cite{liu2021swin} and PVT in our implementation. Given an image of size $H\times W \times 3$, it is firstly input to the Patch Embedding stage and converted to a feature map $x \in \mathbb{R}^{\frac{H}{4} \times \frac{W}{4} \times C}$ with feature dimension $C$. Recent research has shown that using convolution networks in Patch Embedding rather than directly splitting the image into non-overlap patches can form higher quality tokens. In our model, we take three convolution layers with overlapping in Patch Embedding stage. In the first two layers, we take two $3\times3$ convolution layers with stride of 2 and padding of 1 to gather the local information of the image and downsample the image to 1/16 of the original size. Then we take a $3\times3$ convolution layer with stride of 1 and padding of 1 to generate the input token sequence with size of $\frac{H}{4} \times \frac{W}{4} $ . 

Then, the feature map goes through three Intra-Inter-Region Transformer stages and one Normal Transformer stage. As shown in Figure 3, each Intra-Inter-Region Transformer stage consists of region segmentation, N-layer Intra-Inter-Region Transformer block, and feature map restoration. Assuming that the size of the feature map input to Intra-Inter-Region Transformer stage is $ {H^{'} \times W^{'} \times C^{'}}$, after region segmentation, we get the regional feature map  $x_r \in \mathbb{R}^{{\frac{H^{'}}{w}} \times {\frac{W^{'}}{w}} \times (w+2p)^2 \times C^{'}  }$, and in the recovery stage, $x_r$ is restored from the regional spatial structure to the corresponding original spatial structure, forming the output feature map $x \in \mathbb{R}^{H^{'}\times W^{'} \times C^{'}}$ of this Intra-Inter-Region Transformer stage. 

Then, to get hierarchical representations in our model, we reduce the resolution of the feature map and the number of tokens in patch merging layers between different stages. Given a feature map $x \in \mathbb{R}^{H \times W \times C}$, we adopt a 3 Ã— 3 2d-convolution layer with stride of 2 and padding of 1 to reduce the number of tokens by 4 and set the output dimension to 2C to form the input feature map $x \in \mathbb{R}^{{\frac{H}{4}} \times {\frac{W}{4}} \times 2C}$ of the next stage. The AGR token also performs downsampling operation like ordinary tokens in patch merging stage to convey regional information between successive stages. The fused AGR Token is used as the initial center point in the process of region segmentation in the next stage. In the last stage, there is only one AGR Token left, and no semantic segmentation is required. Instead, all tokens are input to normal Transformer blocks for global self-attention modeling.

We propose three different architecture variants called SALG-Mi, SALG-T and SALG-S to validate the performance of our SALG Transformer in different model size and computation complexity. Following Swin Transformer, we take the model configuration strategy that the feature dimension of each head is set to 32 and the expansion rate in MLP layers is set to 4. In the process of semantic-aware region segmentation, the size of non-overlapping patches is set to $7\times7$ and the number of padded token is set to 1, 2 and 1 in the first three SALG Transformer block. The other specific hyper-parameters of these architecture variants are shown as follows:

1) SALG-Mi: C = 48, block numbers = \{2, 2, 2, 2\}

2) SALG-T: C = 96, block numbers = \{2, 2, 6, 2\}

3) SALG-S: C = 96, block numbers = \{2, 2, 18, 2\}

where C represents the dimension of the feature in the first stage, and the block numbers display the number of SALG Blocks contained in each stage. 
\end{comment}