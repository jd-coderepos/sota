\documentclass{sig-alternate}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{upgreek}
\usepackage{graphicx}
\usepackage{float}
\usepackage[noend]{algpseudocode}
\usepackage{verbatim}
\usepackage{pbox}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{breakurl}



\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{inequality}[theorem]{Inequality}
\newtheorem{axiom}[theorem]{Axiom}

\newcommand{\us}{\ensuremath{\mathrm{\upmu}}s } 

\newcommand{\pFq}[2]{\ensuremath{\!\,_{#1}F_{#2}}}
\newcommand{\bigpFq}[5]{\ensuremath{\pFq{#1}{#2}\left(\begin{matrix}#3 \\ #4 \end{matrix} \;\; \vline  \;\; #5\right)}}


\newcommand   \NN                 {{\mathbb N}}
\newcommand   \NNN                {{\NN\minuszero}}
\newcommand   \RR                 {{\mathbb R}}
\newcommand   \RRm                {\RR^m}
\newcommand   \RRn                {\RR^n}
\newcommand   \QQ                 {{\mathbb Q}}
\newcommand   \ZZ                 {{\mathbb Z}}
\newcommand   \CC                 {{\mathbb C}}
\newcommand   \CCm                {\CC^m}
\newcommand   \CCn                {\CC^n}
\newcommand   \KK                 {{\mathbb K}}
\newcommand   \KKm                {\KK^m}
\newcommand   \KKn                {\KK^n}

\newcommand   \OO     {O}
\newcommand   \OOsoft {\tilde O}
\newcommand   \M      {\mathsf{M}}
\newcommand   \MM     {\mathsf{MM}}
\newcommand   \MZ     {\mathsf{M}_{\mathbb{Z}}}

\usepackage{color}

\definecolor{dblackcolor}{rgb}{0.0,0.0,0.0}
\definecolor{dbluecolor}{rgb}{0.01,0.02,0.7}
\definecolor{dgreencolor}{rgb}{0.2,0.4,0.0}
\definecolor{dgraycolor}{rgb}{0.30,0.3,0.30}
\newcommand{\dblue}{\color{dbluecolor}\bf}
\newcommand{\dred}{\color{dredcolor}\bf}
\newcommand{\dblack}{\color{dblackcolor}\bf}

\newcommand*\Let[2]{\State #1  #2}
\algrenewcommand\algorithmicrequire{\textbf{Precondition:}}
\algrenewcommand\algorithmicensure{\textbf{Postcondition:}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\clubpenalty=10000
\widowpenalty=10000

\begin{document}

\title{Evaluating parametric holonomic sequences using rectangular splitting}

\numberofauthors{1}

\author{\alignauthor Fredrik Johansson\titlenote{Supported by the Austrian Science Fund (FWF) grant Y464-N18.}\\smallskipamount]
      \email{\strut fredrik.johansson@risc.jku.at}
}

\maketitle

\begin{abstract}
We adapt the rectangular splitting technique of
Paterson and Stockmeyer to the problem of evaluating
terms in holonomic sequences that depend on a parameter.
This approach allows computing the -th term
in a recurrent sequence of suitable type using  ``expensive'' operations
at the cost of an increased number of ``cheap'' operations.

Rectangular splitting has little overhead and can perform better
than either naive evaluation or asymptotically faster algorithms
for ranges of  encountered in applications.
As an example,
fast numerical evaluation of the gamma function is investigated.
Our work generalizes two previous algorithms of Smith.
\end{abstract}

\category{I.1.2}{Computing Methodologies}{Symbolic and Algebraic Manipulation}[Algorithms]
\category{F.2.1}{Theory of Computation}{Analysis of Algorithms and Problem Complexity}[Numerical Algorithms and Problems]

\terms{Algorithms}

\keywords{Linearly recurrent sequences, Numerical evaluation, Fast arithmetic, Hypergeometric functions, Gamma function}

\overfullrule=5pt

\section{Introduction}

A sequence  is called \emph{holonomic}
(or \emph{P-finite}) of order  if it satisfies a linear recurrence equation

where  are polynomials.
The class of holonomic sequences enjoys many useful closure properties:
for example, holonomic sequences form a ring, and if  is holonomic
then so is the sequence of partial sums 
A~sequence is called \emph{hypergeometric} if it is holonomic of
order . The sequence of partial sums of a hypergeometric sequence
is holonomic of  order (at most) .

Many integer and polynomial sequences of interest in number theory and
combinatorics are holonomic, and the power series expansions of many well-known
special functions (such as the error function and Bessel functions)
are holonomic.

We are interested in efficient algorithms for evaluating an isolated term
 in a holonomic sequence when  is large.
Section \ref{sect:matrix} recalls the well-known
techniques of rewriting \eqref{eq:holeq} in matrix form and applying
\emph{binary splitting}, which gives a near-optimal asymptotic speedup
for certain types of coefficients,
or \emph{fast multipoint evaluation} which
in the general case is the asymptotically fastest known algorithm.

In section \ref{sect:rectangular}, we give an algorithm
(Algorithm~\ref{alg:rsshift}) which becomes
efficient when the recurrence equation involves an ``expensive''
parameter (in a sense which is made precise),
based on the baby-step giant-step technique
of Paterson and Stockmeyer \cite{PatersonStockmeyer1973}
(called \emph{rectangular splitting} in \cite{mca}).

Our algorithm can be viewed as a generalization of the method given by Smith in
\cite{Smith2001} for computing rising factorials. Conceptually, it also
generalizes an algorithm given by Smith in \cite{Smith1989} for evaluation
of hypergeometric series. Our contribution is to
recognize that rectangular splitting can be applied
systematically to a very general class of sequences,
and in an efficient way (we provide a detailed cost analysis,
noting that some care is required in the construction
of the algorithm to get optimal performance).

The main intended application of rectangular splitting is
high-precision numerical evaluation
of special functions, where the parameter is a real or complex number
(represented by a floating-point approximation),
as discussed further in section \ref{sect:numerical}.
Although rectangular splitting is asymptotically slower than
fast multipoint evaluation, it is competitive in practice.
In section \ref{sect:casestudy}, we present implementation
results comparing several different algorithms for
numerical evaluation of the gamma function
to very high precision.

\section{Matrix algorithms} \label{sect:matrix}

Let  be a commutative ring with unity and of sufficiently
large characteristic where necessary. 
Consider a sequence of rank- vectors 
satisfying a recurrence equation of the form

where  (or )
and where  denotes entrywise
evaluation. Given an initial vector , we wish to evaluate
the single vector  for some , where we assume that no
denominator in  vanishes for .
A scalar recurrence of the form \eqref{eq:holeq}
can be rewritten as \eqref{eq:matrec} by taking the vector
to be  and setting 
to the companion matrix

In either case, we call the sequence holonomic (of order ).

Through repeated application of the recurrence equation,
 can be evaluated using  arithmetic operations
(or  if  is companion) and temporary storage of  values.
We call this strategy the \emph{naive algorithm}.

The naive algorithm is not generally
optimal for large . The idea
behind faster algorithms is to first compute the matrix product

and then multiply it by the vector of initial values
(matrix multiplication is of course noncommutative,
and throughout this paper the notation in \eqref{eq:matrixprod} is understood to mean
).
This increases the cost to 
arithmetic operations where  is the exponent of matrix
multiplication, but we can save time for large  by
exploiting the structure of the matrix product.
The improvement is most dramatic when all matrix entries are constant,
allowing binary exponentiation (with  complexity)
or diagonalization to be used,
although this is a rather special case.
We assume in the
remainder of this work that  is fixed,
and omit  factors from
any complexity estimates.

From this point, we may view the problem
as that of evaluating
\eqref{eq:matrixprod} for some .
It is not a restriction to demand
that the entries of  are polynomials: if ,
we can write  where
 and
. This reduces the
problem to evaluating two denominator-free products,
where the second product has order 1.

\subsection{Binary splitting}

In the \emph{binary splitting} algorithm, we recursively compute a
product of square matrices
 (where the
entries of  need not necessarily be polynomials of ), 
as
 where .
If the entries of partial products grow in size,
this scheme balances the sizes of the subproducts in a way
that allows us to take advantage of fast multiplication.

For example, take  where all 
have bounded degree.
Then  has entries in  of
degree , and binary splitting can be shown to compute
 using  operations in  where
 is the complexity of polynomial multiplication,
using  extra storage.
Over a general ring , we have
 by the result of \cite{CantorKaltofen1991},
making the binary splitting softly optimal.
This is a significant improvement over the naive algorithm, which in general
uses  coefficient operations to generate the -th entry in a
holonomic sequence of polynomials.

Analogously, binary splitting reduces
the bit complexity for evaluating holonomic sequences over  or
 (or more generally the algebraic numbers) from
 to . For further references and
several applications of the binary splitting technique,
we refer to Bernstein \cite{Bernstein2008}.

\subsection{Fast multipoint evaluation}

The \emph{fast multipoint evaluation} method is useful when all
arithmetic operations are assumed to have uniform cost. Fast
multipoint evaluation allows evaluating a polynomial of
degree  simultaneously at  points using 
operations and  space.
Applied to a polynomial matrix product, we obtain
Algorithm \ref{alg:multipoint}, which is due to
Chudnovsky and Chudnovsky \cite{ChudnovskyChudnovsky1988}.

\begin{algorithm}
  \caption{Polynomial matrix product using fast multipoint evaluation}
  \label{alg:multipoint}
  \begin{algorithmic}[1]
    \Require , 
    \Ensure 
    \State 
    \Statex \Comment{Compute entrywise Taylor shifts of the matrix}
    \State  \Comment{Binary splitting in }
    \State 
    \Statex \Comment{Fast multipoint evaluation}
    \State \Return{} \Comment{Repeated multiplication in }
  \end{algorithmic}
\end{algorithm}

We assume for simplicity of presentation
that  is a multiple of the parameter
 (in general, we can take 
and multiply by the remaining factors naively).
Taking ,
Algorithm \ref{alg:multipoint} requires 
arithmetic operations in the ring ,
using  temporary storage
during the fast multipoint evaluation step.
Bostan, Gaudry and Schost \cite{BostanGaudrySchost2007} improve
the algorithm to obtain an 
operation bound, which is the best available result for evaluating the
-th term of a holonomic sequence over a general ring.
Algorithm \ref{alg:multipoint} and some of its applications
are studied further by Ziegler \cite{Ziegler2005}.

\section{Rectangular splitting for \\ parametric sequences} \label{sect:rectangular}

We now consider holonomic sequences whose recurrence equation
involves coefficients from a commutative ring  with unity
as well as an additional, distinguished parameter .
The setting is as in the previous section, but with
. In other words, we are considering
holonomic sequences of polynomials (or, by clearing denominators, rational functions)
of the parameter. We make the following definition.

\begin{definition}
A holonomic sequence

is parametric over  (with parameter )
if it satisfies a linear recurrence equation of
the form \eqref{eq:matrec} with  where .
\end{definition}

Let  be a commutative -algebra, and let  be a
parametric holonomic sequence defined by a recurrence matrix
 and an initial vector
.
Given some  and , we wish to compute
the single vector  efficiently subject to
the assumption that operations in  are expensive
compared to operations in . Accordingly, we distinguish between:
\begin{itemize}
\item \emph{Coefficient operations} in 
\item \emph{Scalar operations} in  (additions in  and multiplications )
\item \emph{Nonscalar multiplications} 
\end{itemize}

For example, the sequence of rising factorials

is first-order holonomic (hypergeometric) with
the defining recurrence equation

and parametric over . In some applications,
we wish to evaluate  for  where
 or .

The Paterson-Stockmeyer algorithm \cite{PatersonStockmeyer1973}
solves the problem of evaluating a polynomial
 with 
at  using a reduced
number of nonscalar multiplications. The idea is to write
the polynomial as a rectangular array

After computing a table containing , the
inner (rowwise) evaluations can be done using only scalar multiplications,
and the outer (columnwise) evaluation with respect to 
can be done using about  nonscalar multiplications.
With , this algorithm requires  nonscalar
multiplications and  scalar operations.

A straightforward application of the Paterson-Stockmeyer
algorithm to evaluate each entry of 
yields Algorithm \ref{alg:rsps}
and the corresponding complexity estimate of Theorem \ref{propsqrt}.

\begin{algorithm}
  \caption{Polynomial matrix product and evaluation using rectangular splitting}
  \label{alg:rsps}
  \begin{algorithmic}[1]
    \Require , , 
    \Ensure 
    \State 
    \Statex \Comment{Evaluate matrix w.r.t. , giving }
    \State  \Comment{Binary splitting in }
    \State 
    \Statex \Comment{Evaluate  entrywise using Paterson-Stockmeyer with step length }
    \State \Return{}
  \end{algorithmic}
\end{algorithm}

\begin{theorem}
The -th entry in a parametric holonomic sequence can be evaluated
using  nonscalar multiplications,  scalar
operations, and  coefficient operations.
\label{propsqrt}
\end{theorem}

\begin{proof}
We call Algorithm \ref{alg:rsps} with .
Letting  and ,
computing 
takes  coefficient operations.
Since , generating 
using binary splitting costs
 coefficient operations.
Each entry in  has degree at most ,
and can thus be evaluated using 
nonscalar multiplications and  scalar operations
with the Paterson-Stockmeyer algorithm.
\end{proof}

If we only count nonscalar multiplications, Theorem \ref{propsqrt}
is an asymptotic improvement over fast multipoint evaluation which
uses  nonscalar multiplications
( with the improvement of
Bostan, Gaudry and Schost).

Algorithm \ref{alg:rsps} is not ideal in practice
since the polynomials in  grow to degree .
Their coefficients also grow
to  bits when  (for example,
in the case of rising factorials,
the coefficients are the Stirling numbers of the first
kind  which grow to a magnitude between  and ).
This problem can be mitigated by repeatedly applying Algorithm \ref{alg:rsps}
to successive subproducts  where ,
but the nonscalar complexity is then no longer the best possible.
A better strategy is to apply
rectangular splitting to the matrix product itself, leading to
Algorithm~\ref{alg:rsshift}.
We can then reach the same operation complexity
while only working with polynomials of degree ,
and over , having coefficients of bit size .

\begin{theorem}
For any choice of , Algorithm~\ref{alg:rsshift} requires
 nonscalar multiplications,
 scalar operations, and  coefficient operations.
In particular, the complexity bounds stated in Theorem \ref{propsqrt}
also hold for Algorithm \ref{alg:rsshift} with .
Moreover, Algorithm \ref{alg:rsshift} only
requires storage of  elements of  and~,
and if , the coefficients have bit size .
\end{theorem}

\begin{proof}
This follows by applying a similar argument as used in the proof of
Theorem~\ref{propsqrt}
to the operations in the inner loop of Algorithm~\ref{alg:rsshift},
noting that  has entries of degree 
and that the matrix multiplication 
requires  nonscalar multiplications and scalar
operations (recalling that we consider  fixed).
\end{proof}

\begin{algorithm}
  \caption{Improved polynomial matrix product and evaluation using rectangular splitting}
  \label{alg:rsshift}
  \begin{algorithmic}[1]
    \Require , , 
    \Ensure 
    \State Compute power table , 
    \State  \Comment{Start with the identity matrix}
    \For{}
        \State 
        \Statex \Comment{Evaluate matrix w.r.t. , giving }
        \State  \Comment{Binary splitting in }
        \State  \Comment{Evaluate w.r.t.  using power table}
        \State  \Comment{Multiplication in }
    \EndFor
    \State \Return{}
  \end{algorithmic}
\end{algorithm}

\subsection{Variations}

Many variations of Algorithm~\ref{alg:rsshift}
are possible. Instead of using binary
splitting directly to compute , we can generate the bivariate matrix

at the start of the algorithm, and then
obtain  by evaluating  at .
We may also work with differences of two successive 
(for small , this can introduce cancellation
resulting in slightly smaller polynomials or coefficients).
Combining both variations, we end up with Algorithm~\ref{alg:rectdelta}
in which we expand and evaluate the bivariate polynomial matrices

This version of the rectangular splitting algorithm can be viewed as a generalization of an
algorithm used by Smith \cite{Smith2001} for computing rising factorials (we consider
the case of rising factorials further in Section \ref{sect:rising}).
In fact, the author of the present paper first
found Algorithm~\ref{alg:rectdelta} by generalizing
Smith's algorithm, and only later discovered
Algorithm \ref{alg:rsshift} by ``interpolation'' between
Algorithm~\ref{alg:rsps} and Algorithm~\ref{alg:rectdelta}.

\begin{algorithm}
  \caption{Polynomial matrix product and evaluation using rectangular splitting (variation)}
  \label{alg:rectdelta}
  \begin{algorithmic}[1]
    \Require , , 
    \Ensure 
    \State Compute power table , 
    \State 
    \Statex \Comment{Binary splitting in }
    \State 
    \Statex \Comment{Evaluate w.r.t. , and w.r.t.  using power table}
    \For{}
      \State 
      \Statex \Comment{Evaluate w.r.t. , and w.r.t.  using power table}
      \State 
    \EndFor
    \State \Return{}
  \end{algorithmic}
\end{algorithm}

The efficiency of Algorithm~\ref{alg:rectdelta} is theoretically
somewhat worse than that of Algorithm~\ref{alg:rsshift}.
Since  and ,
 has  terms (likewise for ),
making the space complexity higher and increasing
the number of coefficient operations to 
for the evaluations with respect to .
However, this added cost may be negligible in practice.
Crucially, when , the coefficients
have similar bit sizes as in Algorithm \ref{alg:rsshift}.

Initially generating  or  also adds some cost,
but this is cheap compared to the evaluations when  is
large enough: binary splitting over 
costs  coefficient operations by
Lemma 8.2 and Corollary 8.28 in \cite{vonzurGathenGerhard2003}.
This is essentially the same as the total cost of binary
splitting in Algorithm~\ref{alg:rsshift} when .

We also note that a small improvement to Algorithm~\ref{alg:rsshift}
is possible if :
instead of computing  from scratch using binary splitting
in each loop iteration, we can update it using a Taylor shift.
At least in sufficiently large characteristic,
the Taylor shift can be computed using  coefficient
operations with the
convolution algorithm of Aho, Steiglitz and Ullman \cite{Aho1975evaluating},
saving a factor  in the total number of coefficient operations.
In practice,
basecase Taylor shift algorithms may also be beneficial (see \cite{von1997fast}).

In lucky cases, the polynomial coefficients (in either
Algorithm \ref{alg:rsshift} or \ref{alg:rectdelta}) might
satisfy a recurrence relation, allowing them
to be generated using  coefficient operations
(and avoiding the dependency on polynomial arithmetic).



\subsection{Several parameters}

The rectangular splitting technique can be generalized
to sequences  depending on several parameters.
In Algorithm~\ref{alg:rsshift}, we simply replace the power
table by a -dimensional array of the possible
monomial combinations. Then we 
have the following result (ignoring coefficient operations).

\begin{theorem}
The -th entry in a holonomic sequence depending on  parameters
can be evaluated with rectangular splitting
using  nonscalar multiplications and
 scalar multiplications.
In particular, taking , 
nonscalar multiplications and  scalar multiplications
suffice.
\label{thm:several}
\end{theorem}

\begin{proof}
If , the entries of a
product of  successive shifts of  are -linear
combinations of ,
, so there is a total of  powers.
\end{proof}

Unfortunately, this gives rapidly diminishing returns for large .
When , the number of nonscalar multiplications
according to Theorem~\ref{thm:several} is
asymptotically worse than with fast multipoint evaluation,
and reducing the number of nonscalar multiplication
requires us to perform more than  scalar multiplications,
as shown in Table \ref{tab:dimension}.
Nevertheless, rectangular splitting could perhaps still be useful
in some settings where the cost of nonscalar
multiplications is sufficiently large.

\begin{table}[ht!]
\centering
\begin{tabular}{ l | l l l }
 &  & Nonscalar & Scalar\\ \hline
 &  &  &  \\
 &  &  &  \\
 &  &  &  \\
 &  &  &  \\
\end{tabular}
\caption{Step size  minimizing the number of
nonscalar multiplications for rectangular splitting involving  parameters.}
\label{tab:dimension}
\end{table}

\section{Numerical evaluation} \label{sect:numerical}

Assume that we want to evaluate 
where the underlying coefficient ring is
 (or )
and the parameter  is a real or complex number
represented by a floating-point approximation with
a precision of  bits.

Let  denote the bit complexity of some algorithm
for multiplying two -bit integers or floating-point numbers. Commonly
used algorithms include classical multiplication with
, Karatsuba multiplication with
, and
Fast Fourier Transform (FFT) based multiplication,
such as the Sch\"{o}nhage-Strassen algorithm, with
.
An unbalanced multiplication where the smaller operand has
 bits can be done using
 bit operations \cite{mca}.

The naive algorithm clearly uses  bit operations
to evaluate , or  with FFT multiplication.
In Algorithm~\ref{alg:rsshift}, the nonscalar
multiplications cost  bit operations.
The coefficient operations cost 
bit operations (assuming the use of fast polynomial arithmetic),
which becomes negligible if  grows faster than .
Finally, the scalar multiplications (which are unbalanced) cost

bit operations.
Taking  for , we get
an asymptotic speedup with classical or Karatsuba multiplication
(see Table~\ref{tab:multalg})
provided that  grows sufficiently rapidly along with .
With FFT multiplication, the scalar multiplications
become as expensive as the nonscalar multiplications,
and rectangular therefore does not give an asymptotic
improvement.

\begin{table}[ht!]
\centering
\begin{tabular}{ l | l l }
Mult. algorithm & Scalar multiplications & Naive \\ \hline
Classical &    &  \\
Karatsuba &  &  \\
FFT       &            &  \\
\end{tabular}
\caption{Bit complexity of scalar multiplications in Algorithm~\ref{alg:rsshift}
and total bit complexity of the naive algorithm}
\label{tab:multalg}
\end{table}

However, due to the overhead of FFT multiplication,
rectangular splitting is still likely to save a constant
factor over the naive algorithm. In practice,
one does not necessarily get the best performance by choosing
 to minimize the number of nonscalar multiplications alone;
the best  has to be determined empirically.

Algorithm~\ref{alg:multipoint} is asymptotically
faster than the naive algorithm as well
as rectangular splitting, with a
bit complexity of .
It should be noted that this estimate does not reflect the complexity
required to obtain a given \emph{accuracy}.
As observed by K\"{o}hler and Ziegler \cite{KohlerZiegler2008},
fast multipoint evaluation can exhibit
poor numerical stability,
suggesting that~ might have to
grow at least as fast as  to get accuracy proportional to .

When  and all coefficients in  are
positive, rectangular splitting introduces no subtractions that
can cause catastrophic cancellation, and the reduction
of nonscalar multiplications
even improves stability compared to the naive algorithm,
making  guard bits sufficient to reach -bit accuracy.
When sign changes are present, evaluating degree-
polynomials in expanded form can reduce accuracy, typically
requiring use of  guard bits. In this case
Algorithm~\ref{alg:rsshift} is a
marked improvement over Algorithm~\ref{alg:rsps}.

\subsection{Summation of power series}

A common situation is that we wish to evaluate
a truncated power series

where  is a holonomic sequence taking rational (or algebraic)
values and  is a real or complex number.
In this case the Paterson-Stockmeyer algorithm is applicable,
but might not give a speedup when applied directly
as in Algorithm~\ref{alg:rsps} due to the growth of the coefficients.
Since  and  are
holonomic sequences with  as parameter,
Algorithm~\ref{alg:rsshift} is applicable.

Smith noted in \cite{Smith1989} that when  is hypergeometric
(Smith considered the Taylor expansions of elementary functions
in particular),
the Paterson-Stockmeyer technique can be combined with scalar divisions
to remove accumulated factors from the coefficients.
This keeps all scalars at a size of  bits,
giving a speedup over naive evaluation when
non-FFT multiplication is used
(and when scalar divisions are assumed to be roughly
as cheap as scalar multiplications).
This algorithm is studied in more detail by Brent and Zimmermann \cite{mca}.

At least conceptually, Algorithm~\ref{alg:rsshift} can be viewed as a
generalization of Smith's hypergeometric summation algorithm
to arbitrary holonomic sequences depending on a parameter
(both algorithms can be viewed as means to
eliminate repeated content from the associated matrix product).
The speedup is not quite as good since we only reduce
the coefficients to  bits versus Smith's .
However, even for hypergeometric series, Algorithm~\ref{alg:rsshift}
can be slightly faster than Smith's algorithm for small 
(e.g. ) since divisions tend to be more expensive than
scalar multiplications in implementations.

Algorithm \ref{alg:rsshift} is also more general:
for example, we can use it to evaluate the
generalized hypergeometric function

where  (as opposed to  alone) are rational functions of
the real or complex parameter .

An interesting question, which we do not attempt to answer here, is
whether there is a larger class of parametric sequences other than
hypergeometric sequences and their sums for which we can reduce
the number of nonscalar multiplications to 
while working with coefficients that are strictly smaller than
 bits.

\subsection{Comparison with asymptotically faster \\ algorithms}

If all coefficients in \eqref{eq:fseries} including the parameter 
are rational or algebraic numbers and the series converges,  can be evaluated
to -bit precision using  bit operations
using binary splitting. 
An  bit complexity can also
be achieved for arbitrary real or complex  by combining
binary splitting with translation of the
differential equation for .
The general version of this algorithm, sometimes called the \emph{bit-burst algorithm},
was developed by Chudnovsky and Chudnovsky and independently
with improvements by van der Hoeven \cite{vdH:hol}. It is used
in some form
for evaluating elementary functions in several libraries,
and a general version has been implemented by Mezzarobba \cite{Mezzarobba2010}.

For high-precision evaluation of elementary functions, binary splitting typically only
becomes worthwhile at a precision of several thousand digits,
while implementations typically use Smith's algorithm for summation of hypergeometric series
at lower precision.
We expect that Algorithm~\ref{alg:rsshift} can be used
in a similar fashion for a wider class of special functions.

When  in  involves real or complex numbers,
binary splitting no longer gives a speedup. In this case,
we can use fast multipoint to evaluate \eqref{eq:fseries} using
 bit operations
(Borwein \cite{Borwein1987} discusses the application to numerical evaluation
of hypergeometric functions). This method does not appear to be
widely used in practice, presumably owing to its high overhead and relative
implementation difficulty. Although rectangular splitting is
not as fast asymptotically, its ease of implementation and low overhead
makes it an attractive alternative.

\section{High-precision computation of the gamma function} \label{sect:casestudy}

In this section, we consider two holonomic sequences depending
on a numerical parameter: rising factorials, and the partial
sums of a certain hypergeometric series defining the incomplete
gamma function. In both cases, our goal is to accelerate numerical
evaluation of the gamma function at very high precision.

We have implemented the algorithms in the present section using
floating-point ball arithmetic (with rigorous error bounding)
as part of the Arb library\footnote{\url{http://fredrikj.net/arb}}.
All arithmetic in  is done via FLINT \cite{Hart2010},
using a Sch\"{o}nhage-Strassen FFT implemented by W. Hart.

Fast numerically stable multiplication in  is
done by breaking polynomials into segments with
similarly-sized coefficients and computing the subproducts
exactly in  (a simplified version of
van der Hoeven's block multiplication algorithm
\cite{vdH:stablemult}), and asymptotically fast
polynomial division is implemented using Newton iteration.

All benchmark results were obtained on a 2.0 GHz Intel Xeon E5-2650 CPU.

\subsection{Rising factorials} \label{sect:rising}

Rising factorials of a real or complex argument appear when
evaluating the gamma function via the asymptotic Stirling series

To compute  with -bit accuracy, we choose a
positive integer  such that there is an  for which
, and then evaluate
.
It is sufficient to choose  such that the
real part of  is of order  where
.

The efficiency of the Stirling series can be improved
by choosing  slightly larger than the absolute minimum
in order to reduce . For example,
 is a good choice.
A faster rising factorial is doubly advantageous: it
speeds up the argument reduction, and making larger  cheap
allows us to get away with fewer Bernoulli numbers.

Smith \cite{Smith2001} uses the difference of four consecutive terms

to reduce the number of nonscalar multiplications to compute  from 
to about . This is precisely Algorithm~\ref{alg:rectdelta}
specialized to the sequence of rising factorials and with a fixed step length .

Consider Smith's algorithm with a variable step length~.
Using the binomial theorem and some rearrangements,
the polynomials can be written down explicitly as

where

and where  denotes an unsigned Stirling number of the first kind.
This formula can be used to generate  efficiently in practice without
requiring bivariate polynomial arithmetic. In fact, the coefficients can
be generated even cheaper by taking advantage of the recurrence (found by
M. Kauers)


We have implemented several algorithm for evaluating the rising
factorial of a real or complex number.
For tuning parameters, we empirically determined simple
formulas that give nearly optimal
performance for different combinations of 
(typically within 20\% of the speed with the best tuning
value found by a brute force search):
\begin{itemize}
\item In Algorithm~\ref{alg:multipoint}, .
\item Algorithm~\ref{alg:rsps} is applied to subproducts of length , with .
\item In Algorithms~\ref{alg:rsshift} and \ref{alg:rectdelta}, .
\end{itemize}

Our implementation of Algorithm~\ref{alg:rectdelta} uses \eqref{eq:ccoeff}
instead of binary splitting, and Algorithm~\ref{alg:rsshift}
exploits the symmetry of  and  to update the
matrix  using Taylor shifts instead of repeated binary splitting.

Figure \ref{fig:rising} compares the running times where 
is a real number with a precision of  bits. This input
corresponds to that used in our Stirling series implementation
of the gamma function.

\begin{figure}[width=8cm] \label{fig:rising}
\begin{center}
\includegraphics[width=8cm]{rising}
\caption{Timings of rising factorial algorithms, normalized
against the naive algorithm.}
\end{center}
\end{figure}

On this benchmark, Algorithms \ref{alg:rsshift} and \ref{alg:rectdelta} are the
best by far, gaining a 20-fold speedup over the naive algorithm for large 
(the speedup levels off
around , which is expected since this is
the approximate point where FFT integer multiplication kicks in).
Algorithm~\ref{alg:rectdelta} is slightly faster than Algorithm \ref{alg:rsshift}
for , even narrowly beating the naive algorithm for
 as small as .

Algorithm~\ref{alg:multipoint} (fast multipoint evaluation)
has the most overhead of all algorithms and only overtakes
the naive algorithm around  (at a precision of  bits).
Despite its theoretical advantage, it is slower than rectangular splitting
up to  exceeding .

Table \ref{tab:gammatime} shows absolute timings for evaluating 
where  is a small real number in Pari/GP 2.5.4, and our implementation
in Arb (we omit results for MPFR 3.1.1 and Mathematica 9.0, which
both were slower than Pari).
Both implementations use the Stirling series, caching
the Bernoulli numbers to speed up multiple evaluations.
The better speed of Arb for a repeated evaluation (where the Bernoulli numbers
are already cached) is mainly due to the use of rectangular splitting
to evaluate the rising factorial.
The total speedup is smaller than it would be for computing the
rising factorial alone since we still have to evaluate the Bernoulli
number sum in the Stirling series.
The gamma function implementations
over  have similar characteristics.

\begin{table}[ht!] \label{tab:gammatime}
\centering
\begin{tabular}{r | l l l l}
Decimals & Pari/GP & (first) & Arb & (first) \\ \hline
100 & 0.000088 & & 0.00010 & \\
300 & 0.00048 & & 0.00036 & \\
1000 & 0.0057 & & 0.0025 & \\
3000 & 0.072 & (9.2) & 0.021 & (0.090) \\
10000 & 1.2 & (324) & 0.25 & (1.4) \\
30000 & 15 & (8697) & 2.7 & (22) \\
100000 &   & &  39 & (433) \\
300000 &   & &  431 & (7131) \\
\end{tabular}
\caption{Timings in seconds for evaluating 
where  is a small real number (timings for the first evaluation,
including Bernoulli number generation, is shown in parentheses).}
\end{table}

\subsection{A one-parameter hypergeometric series}

The gamma function can be approximated via the (lower)
incomplete gamma function as

Borwein \cite{Borwein1987} noted that applying fast multipoint evaluation
to a suitable truncation of the hypergeometric series in \eqref{eq:gammahyper}
allows evaluating the gamma function
of a fixed real or complex argument to -bit precision
using  bit operations,
which is the best known result for general  (if  is algebraic,
binary splitting evaluation of the same series
achieves a complexity of ).

Let  and ,
giving

For , choosing
 and 
gives an error of order  (it
is easy to compute strict bounds). The partial sums
satisfy the order-2 recurrence

where

The matrix product \eqref{eq:gammahypproduct} may be computed
using fast multipoint evaluation
or rectangular splitting. We note that the denominators
are identical to the top left entries of the numerator matrices,
and therefore do not need to be computed separately.

Figure \ref{fig:gammahyp} compares
the performance of the Stirling series (with fast argument reduction using rectangular splitting)
and three different implementations of the  series (naive summation,
fast multipoint evaluation, and rectangular splitting
using Algorithm~\ref{alg:rsshift} with )
for evaluating  where  is a real argument
close to unity.

\begin{figure}[width=8cm] \label{fig:gammahyp}
\begin{center}
\includegraphics[width=8cm]{gammahyp}
\caption{Timings of gamma function algorithms,
normalized against the Stirling series with Bernoulli numbers cached.}
\end{center}
\end{figure}

Both fast multipoint evaluation and rectangular splitting
speed up the hypergeometric series
compared to naive summation.
Using either algorithm, the
hypergeometric series is competitive with the Stirling
series for a single evaluation
at precisions above roughly 10,000 decimal digits.

Algorithm~\ref{alg:multipoint} performs better than on
the rising factorial benchmark, and is
faster than Algorithm~\ref{alg:rsshift} above  bits.
A possible explanation for this difference is that
the number of terms used in the hypergeometric series roughly is 
where  is the precision in bits, compared to 
for the rising factorial, and rectangular splitting
favors higher precision and fewer terms.

The speed of Algorithm~\ref{alg:rsshift} is remarkably close to
that of Algorithm~\ref{alg:multipoint} even for 
as large as .
Despite being asymptotically slower, the
simplicity of rectangular splitting
combined with its lower memory consumption and better
numerical stability (in our implementation, Algorithm~\ref{alg:rsshift}
only loses a few significant digits,
while Algorithm~\ref{alg:multipoint} loses a few percent of the number of significant digits)
makes it an attractive option for extremely high-precision
evaluation of the gamma function.

Once the Bernoulli numbers have been cached after the first evaluation, the Stirling series
still has a clear advantage up to precisions exceeding  bits.
We may remark that our implementation of the Stirling series
has been optimized for multiple evaluations: by choosing
larger rising factorials and generating the Bernoulli numbers dynamically
without storing them, both the speed and memory consumption
for a single evaluation could be improved.

\section{Discussion}

We have shown that rectangular splitting can be profitably
applied to evaluation of a general class of linearly recurrent sequences.
When used for numerical evaluation of special functions, our benchmark results
indicate that rectangular splitting can be faster than either naive evaluation
or fast multipoint evaluation over a wide precision range
(between approximately  and  bits).

Two natural questions are whether this approach can be generalized
further (to more general classes of sequences), and whether
it can be optimized further (perhaps for more specific classes
of sequences).

\section{Acknowledgements}

The author thanks Manuel Kauers for providing
useful feedback on draft versions of this paper.

\bibliographystyle{plain}
\bibliography{references.bib}

\end{document}
