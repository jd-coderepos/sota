\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{enumitem}
\usepackage[table]{xcolor}
\usepackage{verbatim}
\usepackage{titling}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 \iccvfinalcopy 

\def\iccvPaperID{1355} 
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\setlength{\droptitle}{-3.4em}  


\ificcvfinal\pagestyle{empty}\fi

\begin{document}
\title{Monocular, One-stage, Regression of Multiple 3D People}
\predate{}
\author{Yu Sun\\
\normalsize Harbin Institute of Technology\\
{\tt\small yusun@stu.hit.edu.cn}
\and
Qian Bao\\
\normalsize JD AI Research\\
{\tt\small baoqian@jd.com}
\and
Wu Liu\\
\normalsize JD AI Research\\
{\tt\small liuwu@live.cn}
\and
Yili Fu\\
\normalsize Harbin Institute of Technology\\
{\tt\small meylfu@hit.edu.cn}
\and
Michael J. Black\\
\normalsize MPI for Intelligent Systems\\
{\tt\small black@tuebingen.mpg.de}
\and
Tao Mei\\
\normalsize JD AI Research\\
{\tt\small tmei@live.com}
}
\posttitle{\par\end{center}}
\postdate{}
\date{}
\maketitle 
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
This paper focuses on the regression of multiple 3D people from a single RGB image.
Existing approaches predominantly follow a multi-stage pipeline, which first detects people with the bounding boxes and then regresses their 3D body meshes. 
In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP), which is conceptually simple, bounding box-free, and able to learn per-pixel representation in an end-to-end manner.
Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level.
Through a body-center-guided sampling process, the body mesh parameters of all people in the image can be easily extracted from the Mesh Parameter map.
Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion.
Compared with the state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person/occlusion benchmarks, including 3DPW, CMU Panoptic, and 3DOH50K. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. 
It is also worth noting that our released demo code\footnote{\href{https://github.com/Arthur151/ROMP}{https://github.com/Arthur151/ROMP}} 
is the first real-time (over 30 FPS) implementation of monocular multi-person 3D mesh regression to date.


\end{abstract}
\vspace{-3mm}
\section{Introduction}~\label{sec:intro}
\vspace{-5mm}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{Figure1.png}
	\caption{
    Given the challenging multi-person images in (a), the recent state-of-the-art human pose and shape estimation approaches, e.g., VIBE~\cite{kocabas2020vibe} (left), fail to deal with truncation, scene occlusion, and person-person occlusion. 
 The reason lies in the multi-stage design (b), where the bounding-box-level features are often implicit, ambiguous, and inseparable in multi-person cases.
 We propose to regress all meshes in one single stage for multiple 3D people.
Specifically, our proposed ROMP develops an explicit pixel-level representation (c) for fine-grained estimation, which increases robustness to the truncation and occlusion and also significantly reduces computational complexity.
}\vspace{-5mm}
	\label{fig:motivation}
\end{figure}

Recently, great progress has been made in monocular 3D human pose and shape estimation, particularly in the single person scene~\cite{keep,Guler_2019_CVPR,hmr,kocabas2020vibe,kolotouros2019spin,surreal,Xu_2019_ICCV}.
However, as we progress toward more general scene, it is crucial to deal with the truncation, person-person occlusion, and environmental occlusion in the multi-person cases. 
Robustness to such occlusions is critical for real-world applications.

Existing approaches~\cite{jiang2020coherent,kocabas2020vibe,zanfir2018monocular,zanfir2018deep} follow a multi-stage design that equips the single-person method with a 2D person detector to handle  multi-person scenes.
Generally, they first detect the person areas and then extract the bounding-box-level features from them, which are used to regress each single 3D human mesh \cite{Guler_2019_CVPR,hmr,kanazawa2019learning,kocabas2020vibe,kolotouros2019spin,gcmr,pavlakos2019texturepose,sun2019dsd-satn,Xu_2019_ICCV,Zhu_2019_CVPR}. 
However, as shown in Figure~\ref{fig:motivation}, this strategy is prone to fail in cases of multi-person occlusion and truncation. 
Specifically, as shown in Figure~\ref{fig:motivation}(b), when two people overlap, it is hard for the multi-stage method to estimate diverse body meshes from similar image patches.
It is the ambiguity of the implicit bounding-box-level representation that leads to the failure in such inseparable multi-person cases. 

For multi-person 2D pose estimation, this problem is tackled via a subtle and effective bottom-up framework. The paradigm is to first detect all body joints and then assign them to different people by joint grouping. It is the pixel-level body joint representation that guarantees their impressive performance in crowded scenes~\cite{openpose,cheng2020bottom,pishchulin2016deepcut}. 
However, it is non-trivial to extend the bottom-up one-stage process beyond joints~\cite{jiang2020coherent}.
Unlike 2D pose estimation that predicts dozens of body joints, we need to regress a human body mesh with thousands of vertices, making it hard to follow the paradigm of body joint detection and grouping.

In this paper, we introduce ROMP, a one-stage network for regressing multiple 3D people in a per-pixel prediction fashion.
It directly estimates multiple differentiable maps from the whole image, from which we can easily parse out the 3D meshes of all people.
Specifically, as shown in Figure~\ref{fig:motivation}(c), ROMP predicts a Body Center heatmap and a Mesh Parameter map, representing the 2D position of the body center and the parameter vectors of the corresponding 3D body mesh, respectively.
Via a simple parameter sampling process, we can easily extract 3D body mesh parameter vectors of all people from the Mesh Parameter map at the body center locations described by the heatmap. 
Then we put the sampled mesh parameter vectors into the SMPL body model~\cite{SMPL} to derive multi-person 3D meshes.
Such a body-center-guided pixel-level representation explicitly points out the target from the background/occlusion, which promotes effective learning from multi-person overlapping cases. 
Additionally, different from the bounding-box-level features learned in a local view, end-to-end learning from the whole image make the model get used to predicting in a holistic view.
This pure holistic view exactly fits the real-world domain, which guarantees the generalization and robustness of our model in unseen/in-the-wild scenes.

Moreover, considering that the body center of severe overlapping people may collide at the same 2D position, we further develop the body-center-guided representation into an advanced version, collision-aware representation (CAR). The key idea is to construct a repulsion field of body centers, where close body centers are analogous to positive charges that are pushed apart by the mutual repulsion. In this way, the body centers of the overlapping people would be more distinguishable. Especially in the face of severe overlap, most part of the human body is invisible. Mutual repulsion will push the center to the visible body area, making the model tend to sample 3D mesh parameters estimated from the position centered on the visible body parts. It improves the robustness under heavy occlusion between people. 

Compared with previous state-of-the-art (SOTA) methods for multi-person~\cite{jiang2020coherent,zanfir2018monocular,zanfir2018deep} and single-person~\cite{kocabas2020vibe,kolotouros2019spin,zhang2020object} 3D mesh regression, ROMP 
achieves superior performance on challenging benchmarks, including 3DPW~\cite{3dpw}, CMU Panoptic~\cite{cmu_panoptic}, and 3DOH50K~\cite{zhang2020object}. 
Experiments on person-person occlusion datasets (Crowdpose~\cite{crowdpose} and 3DPW-PC, a person-occluded subset of 3DPW~\cite{3dpw}) demonstrate the effectiveness of the proposed CAR under person-person occlusion.  
To further evaluate it in general cases, we test ROMP on images from the Internet and web camera videos. 
With the same backbone as those of the multi-stage counterparts, ROMP achieves real-time performance with over 30 FPS on one 1070Ti GPU.

In summary, the contributions are:
\begin{itemize}[itemsep=-2pt,topsep=2pt]\item A simple yet effective one-stage regression network, ROMP, is proposed for monocular multi-person 3D mesh regression.
\item The proposed explicit body-center-guided representation facilitates the pixel-level human mesh regression in an end-to-end manner.
\item We develop an collision-aware representation to deal with the severe overlapping cases.
\item ROMP outperforms the previous SOTA methods in both accuracy and efficiency. It is the first open-source real-time method for multi-person 3D mesh regression from monocular images.
\end{itemize}

\begin{figure*}[t]
	\centering
	\includegraphics[width=1.00\textwidth]{framework_new.png}
	\caption{An overview of ROMP. Given an input image, ROMP predicts multiple maps: 1) the Body Center heatmap  predicts the probability of each position being a body center, 2) the Camera map and 3) SMPL map  contain the camera and SMPL parameters~\cite{SMPL} of the person at each center, respectively. As the concatenation of the Camera map and SMPL map, the Mesh Parameter map contains the information of the predicted 3D body mesh and its location. Via the designed parameter sampling process, we can obtain the final 3D mesh results by parsing the Body Center heatmap and sampling the Mesh Parameter map.
}\vspace{-3mm}
	\label{fig:framework}
\end{figure*}
\newcommand{\squeezeup}{\vspace{-3mm}}

\vspace{-2mm}
\section{Related Work}
\vspace{-1mm}
\textbf{Single-person 3D mesh regression.} 
Parametric human body models, e.g., SMPL~\cite{SMPL}, have been widely adopted to encode the complex 3D human mesh into a low-dimensional parameter vector.
Therefore, many methods tend to estimate the SMPL parameters instead of the 3D mesh vertices, to reduce the complexity.
Recently, impressive performance has been achieved for single-person scenes using various weak supervision signals, such as 2D pose~\cite{hmr,sun2019dsd-satn}, semantic segmentation~\cite{Xu_2019_ICCV}, geometric prior~\cite{hmr}, motion dynamics~\cite{kanazawa2019learning}, temporal coherence~\cite{kocabas2020vibe,sun2019dsd-satn}, texture consistency~\cite{pavlakos2019texturepose}, SMPLify~\cite{keep} in the loop~\cite{kolotouros2019spin}, etc.
In this way, the available 2D/3D data is well explored to alleviate the lack of 3D data.
However, all these methods adopt a bounding-box-level representation, which is implicit and ambiguous for multi-person/occlusion cases.
For object occlusion, Zhang et al.~\cite{zhang2020object} use a 2D UV map to represent a 3D human mesh. 
Considering the object-occluded body parts are blank areas in the partial UV map, they propose to in-paint the partial UV map to make up the occluded information.
However, in the case of person-person occlusion where one person's body parts are occluded by those of another, it is hard to generate the partial UV map.

\textbf{Multi-person 3D pose estimation}. 
The mainstream methods can be roughly divided into two categories, the multi-stage paradigm and the one-stage paradigm.
Many multi-stage methods follow the top-down design of the well-known Faster R-CNN~\cite{ren2015fasterrcnn}, such as LCR-Net++~\cite{rogez2019lcr} and 3DMPPE~\cite{moon2019camera}. 
Using the detected bounding boxes or anchor-based feature proposals, they then estimate the target via regression.
Other works explore the one-stage solution that reasons about all people in a single forward pass. They estimate all the body joint positions and then group the joints into each person.
Mehta et al.~\cite{mehta2018single} propose the occlusion-robust pose-maps and exploit the body part association to avoid the bounding box prediction.
Benzine et al.~\cite{PandaNet_Benzine_2020_CVPR} propose an anchor-based one-stage model, which directly estimates the 2D/3D pose results for each anchor position. 
For person-person occlusion, Zhen et al.~\cite{zhen2020smap} adopt the PAFs of OpenPose~\cite{openpose} to make part association.
Benefiting from the explicit bottom-up joint-based representation, such as the volumetric heatmap, 3D pose estimation methods show impressive performance.
Our proposed ROMP extends the end-to-end one-stage process beyond the body joints via a body-center-guided pixel-level representation.

\textbf{Multi-person 3D mesh regression.} 
There are only a few approaches for multi-person 3D mesh regression.  Zanfir et al.~\cite{zanfir2018deep} estimate the 3D mesh of each person from its intermediate 3D pose estimate.
Zanfir et al.~\cite{zanfir2018monocular} further employ multiple scene constraints to optimize the multi-person 3D mesh results. 
Jiang et al.~\cite{jiang2020coherent} propose a network for Coherent Reconstruction of Multiple Human (CRMH), which is built on Faster-RCNN~\cite{ren2015fasterrcnn}. 
They use the RoI-aligned feature of each person to predict the SMPL parameters. 
Additionally, they develop the interpolation and depth ordering loss to supervise the relative position between multiple people.
All these existing methods follow the multi-stage design.
The complex multi-step process requires repeated feature extraction, which slows down the computational efficiency. 
Moreover, since relying on the detected bounding boxes, the ambiguity and the limited local view of the bounding-box-level features make them hard to effectively learn from person-person occlusion and truncation.
Instead, our proposed simple yet effective one-stage method learns an explicit pixel-level representation with a holistic view, which significantly improves both accuracy and efficiency in multi-person in-the-wild scenes. 
    
 \textbf{Pixel-level representation} has been proven useful in anchor-free detection, such as CornerNet~\cite{law2018cornernet}, CenterNet~\cite{duan2019centernet,zhou2019objects}, and ExtremeNet~\cite{ExtremeNet}.
They attempt to directly estimate the corner or center point of the bounding box in a heatmap manner.
In this way, they avoid the dense anchor-based proposal.
Our method draws inspiration from them to develop a pixel-level fine-grained representation for multi-person 3D meshes.
Different from the bounding box center used in~\cite{zhou2019objects}, our body center is determined by the body joints, which will be introduced in Sec.~\ref{sec:CAR}. 
Furthermore, we develop a collision-aware version of the body center to deal with the inherent center collision problem. 

\vspace{-2mm}
\section{Our Approach}
\vspace{-1mm}
\subsection{Overview}
\vspace{-1mm}
The overall framework of the one-stage ROMP is illustrated in Figure~\ref{fig:framework}.
It adopts a simple multi-head design with a backbone and three head networks.
Given a single RGB image as input, it outputs a Body Center heatmap, Camera map, and SMPL map, describing the detailed information of the estimated 3D human mesh. 
In the Body Center heatmap, we predict the probability of each position being a human body center. 
At each position of the Camera/SMPL map, we predict the camera/SMPL parameters of the person that takes the position as the center. 
For simplicity, we combine the Camera map and SMPL map into the Mesh Parameter map. 
During inference, we sample the 3D body mesh parameter results from the Mesh Parameter map at the 2D body center locations parsed from the Body Center heatmap. 
Finally, we put the sampled parameters into the SMPL model to generate the 3D body meshes. 

\vspace{-1mm}
\subsection{Basic Representations~\label{sec:representations}}
\vspace{-1mm}

In this section, we introduce the detailed representation of each map. Each output map is of size , where  is the number of channels and . 

\textbf{Body Center heatmap:}  is a heatmap representing the 2D human body center in the image. 
Each body center is represented as a Gaussian distribution in the Body Center heatmap. 
For better representation learning, the Body Center heatmap also integrates the scale information of the body in the 2D image.
Specifically, we calculate the Gaussian kernel size  of each person center in terms of its 2D body scale in the image. 
Given the diagonal length   of the person bounding box and the width  of the Body Center heatmap, the Gaussian kernel size is derived from

where  is the minimum kernel size and  is the variation range of . We set  and   by default.

\textbf{Mesh Parameter map:}  consists of two parts, the Camera map and SMPL map.
Assuming that each location of these maps is the center of a human body, we estimate its corresponding 3D body mesh parameters.
Specifically, we estimate the parameters of SMPL model that encode the 3D body mesh into a set of low-dimensional parameters. 
Estimating these SMPL parameters instead of the complex 3D mesh greatly reduces the complexity of our task. 
Additionally, following the previous works~\cite{hmr,sun2019dsd-satn}, we employ a weak-perspective camera model to project  3D body joints  of the estimated 3D mesh back to the 2D joints   on the image plane. 
This facilitates training the model with in-the-wild 2D pose datasets in varied imagery (e.g. COCO~\cite{coco}), which helps with robustness and generalization.
 
\textbf{Camera map:}   contains the 3-dim camera parameters  that describe the 2D scale  and translation  of the person in the image. 
The scale  reflects the body size and the depth to some extent.
 and , ranging in , reflect the normalized translation of the human body relative to the image center on the  and  axis, respectively.
The 2D projection  of  3D body joints  can be derived as  .
The translation parameters allow more accurate position estimates than the Body Center heatmap. 

\textbf{SMPL map:}   contains the 142-dim SMPL parameters, which describe the 3D pose and shape of the 3D human body mesh. 
SMPL establishes an efficient mapping from the pose    and shape  parameters to the human 3D body mesh .
The shape parameter  is the top-10 PCA weights of the SMPL statistical shape space. 
The pose parameters  contain the 3D rotation of the 22 body joints in a 6D representation~\cite{Zhou_2019_CVPR}.  Instead of using the full 24 joints of the original SMPL model, we drop the last two hand joints. 
The 3D rotation of the first joint denotes the body 3D orientation in the camera coordinate, while the remainders are the relative 3D orientations of each body part with respect to its parent in a kinematic chain. 
Then the 3D joints  are derived via , where  is a sparse weight matrix that describes the linear mapping from 6890 vertices of the body mesh to the  body joints.

\vspace{-1mm}
\subsection{CAR: Collision-Aware Representation~\label{sec:CAR}}
\vspace{-1mm}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{CAR.png} \caption{{\bf Collision-Aware Repulsion (CAR).} The body centers of overlapping people are treated as positive charges, which are pushed apart if they are too close in the repulsion field. }\vspace{-4mm}
	\label{fig:CAR}
\end{figure}

The entire framework is based on a concise body-center-guided representation. 
It is crucial to define an explicit and robust body center so that the model can easily estimate the center location in various cases.  Here we introduce the basic definition of the body center for the general case and its advanced version for severe occlusion.

\textbf{Basic definition of the body center.} Existing center-based methods~\cite{duan2019centernet,zhou2019objects} define the center of the bounding box as the target center. 
This works well for general objects (e.g., a ball, bottle) that lack semantically meaningful keypoints.
However, bounding box center is not a meaningful point on the human body and may even fall outside the body area.
For a stable parameter sampling, we need an explicit body center.
Therefore, we calculate each body center from the ground truth 2D pose. 
Considering that any body joint may be occluded in general cases, we define the body center as the center of visible torso joints (neck, left/right shoulders, pelvis, and left/right hips). 
When all torso joints are invisible, the center is simply determined by the average of the visible joints.
In this way, the model is encouraged to predict the body location from the visible parts. 

However, in cases of severely overlapping people, the body center of the people might be very close or even at the same location on .
This center collision problem makes the center ambiguous and hard to identify in crowded cases.
To tackle this, we develop a more robust representation to deal with person-person occlusion.
To alleviate the ambiguity, the center points of overlapping people should be kept at a minimum distance to ensure that they can be well distinguished. 
Additionally, to avoid sampling multiple parameters for the same person, the network should assign a unique and explicit center for each person.

Based on these principles, we develop a novel \textbf{Collision-Aware Representation (CAR)}. 
To ensure that the body centers are far enough from each other, we construct a repulsion field. 
In this field, each body center is treated as a positive charge, whose radius of repulsion is equal to its Gaussian kernel size derived by Eq.~(\ref{eq:center kernel size}).
In this way, the closer the body centers are, the greater the mutual repulsion and the further they will be pushed apart. 
Figure \ref{fig:CAR} illustrates the principle of CAR.  
Suppose that  are the  body centers of two overlapping people. 
If  their Euclidean distance  and Gaussian kernel sizes  satisfy  , the repulsion is triggered to push the close centers apart  via

where  is the repulsion vector from  to  and  is an intensity coefficient to adjust the strength. 

During training, we use CAR to push apart close body centers to supervise the Body Center heatmap. 
In this way, the model is encouraged to estimate the  centers that maintain a distinguishable distance.
For the Body Center heatmap, it helps the model to effectively locate the occluded person.
For the Mesh Parameter map, sampling the parameters from these shifted locations enables the model to extract diverse and individual features for each person.
The model trained with CAR is more suitable for crowded scenes with significant person-person occlusion such as train stations, canteens, etc.

\vspace{-1mm}
\subsection{Parameter Sampling~\label{sec:parameter sampling}}
\vspace{-1mm}

To estimate 3D human body meshes from the image, we need to first parse the 2D body center coordinates  from  and then use them to sample  for the SMPL parameters. 
In this section, we introduce the process of the center parsing, matching and sampling. 

 is a probability map whose local maxima are regarded as the body centers. 
The local maxima are derived via  where  is the max pooling operation and  is the logical conjunction operation. 
Let  be the 2D coordinates of a local maximum with confidence score larger than a threshold . 
We rank the confidence score at each  and take the top  as the final centers.
During inference, we directly sample the parameters from  at . 
During training, the estimated  are matched with the nearest ground truth body center according to the  distance. 

Additionally, we approximate the depth order between multiple people by using the center confidence from  and the 2D body scale  of the camera parameters from .
For people of different , we regard the one with a larger  located in the front.
For people of similar , the person with a higher center confidence is considered to be in the front.

\vspace{-1mm}
\subsection{Loss functions}
\vspace{-1mm}

To supervise ROMP, we develop individual loss functions for different maps.
In total, ROMP is supervised by the weighted sum of the body center loss  and mesh parameter loss . 

\textbf{Body Center Loss.}  encourages a high confidence value at the body center  of the Body Center heatmap  and low confidence elsewhere.
To deal with the imbalance between the center location and the non-center locations in , we train the Body Center heatmap based on the focal loss~\cite{lin2017focal}. 
Given the predicted Body Center heatmap  and the ground truth ,   is defined as 

where   is a binary matrix with a positive value at the body center location, and  is the loss weight.

\textbf{Mesh Parameter Loss.} 
As we introduced in Sec.~\ref{sec:parameter sampling}, the parameter sampling process matches each ground truth body with a predicted parameter result for supervision.
The mesh parameter loss is derived as 

 is the  loss of the pose parameters in the  rotation matrix format. 
 is the  loss of the shape parameters. 
 is the  loss of the 3D joints  regressed from the body mesh .
 is the  loss of the 3D joints  after Procrustes alignment.
 is the  loss of the projected 2D joints . 
 is the Mixture Gaussian prior loss of the SMPL parameters adopted in~\cite{keep,SMPL} for supervising the plausibility of 3D joint rotation and body shape. Lastly,  denotes the corresponding loss weights.


\begin{table*}[t]
\setlength\tabcolsep{2pt}
  \centering
  \caption{{Comparisons to the SOTA methods  on 3DPW following \textit{Protocol 1} (without using any ground truth during inference).  means using extra datasets for training.}}  \label{tab:3DPW}{
  \rowcolors{2}{gray!10}{white}
    \begin{tabular}{l|cccccc}
    \hline
    \textbf{Method} & \textbf{MPJPE} &  \textbf{PMPJPE} &\textbf{PCK} &	\textbf{AUC} &\textbf{MPJAE} & \textbf{PMPJAE} \\
    \hline
        OpenPose + SPIN~\cite{kolotouros2019spin}&  95.8 &  66.4 &33.3 & 55.0  & 23.9 & 24.4\\
        YOLO + VIBE~\cite{kocabas2020vibe}& 94.7 & 66.1 & 33.9 & 56.6 &25.2 &20.46 \\
        CRMH~\cite{jiang2020coherent}& 105.9 &71.8 &   28.5 & 51.4 & 26.4 & 22.0\\
        \hline
        ROMP (ResNet-50) & 87.0 & 62.0 & 34.4 & 57.6 & 21.9 & 20.1\\
        ROMP (ResNet-50) & \textbf{80.1} &  \textbf{56.8} &36.4 & \textbf{60.1} & 20.8 &19.1\\
        ROMP (HRNet-32) &82.7 & 60.5 &  \textbf{36.5} & 59.7 & \textbf{20.5} & \textbf{18.9}\\
    \hline
    \end{tabular} } \vspace{-0.3cm}
\end{table*}

\begin{table}
 \setlength{\tabcolsep}{2pt}{
	\begin{center}
		\caption{{Comparisons to the SOTA methods on 3DPW following VIBE~\cite{kocabas2020vibe}, using \textit{Protocol 2} (on the test set only).  means using extra datasets (compared with SPIN) for training.}}\label{tab:3DPW_VIBE2}
        \rowcolors{2}{gray!10}{white}
			\begin{tabular}{l|ccc}
				\hline
			    \textbf{Method}  &  \textbf{MPJPE} & \textbf{PMPJPE}  & \textbf{PVE}\\
				\hline 
                 HMR~\cite{hmr} & 130.0 & 76.7 & - \\
                 Kanazawa et al.~\cite{kanazawa2019learning}  & 116.5 & 72.6 & 139.3 \\
                 Arnab et al.~\cite{arnab2019exploiting}  & - & 72.2 & - \\
                 GCMR~\cite{gcmr} & - & 70.2 & - \\
                 DSD-SATN~\cite{sun2019dsd-satn}  & - & 69.5 & - \\
                 SPIN~\cite{kolotouros2019spin}  & 96.9 & 59.2 & 116.4 \\
                 ROMP (ResNet-50) & \textbf{91.3} & \textbf{54.9} & \textbf{108.3}\\
                 \hline
                  I2L-MeshNet~\cite{moon2020i2l} & 93.2 & 58.6 & - \\
                 EFT~\cite{joo2020eft}  & - & 54.2 & - \\
                 VIBE~\cite{kocabas2020vibe} & 93.5 & 56.5 & 113.4\\
                 ROMP (ResNet-50)& 89.3 & 53.5 & 105.6\\
                 ROMP (HRNet-32) &\textbf{85.5} & \textbf{53.3} & \textbf{103.1}\\
				\hline
		\end{tabular}
	\end{center}}
    \vspace{-6mm}
\end{table}



\vspace{-2mm}
\section{Experiments}
\vspace{-2mm}

In this section, we first introduce the implementation details and experimental settings. 
Then compare ROMP with the SOTA approaches on multiple benchmarks.
We further conduct ablation study of the proposed CAR and finally discuss the source of our performance gains.

\vspace{-1mm}
\subsection{Implementation Details}
\vspace{-1mm}

\textbf{Network Architecture.} 
For a fair comparison with other approaches, we use ResNet-50~\cite{resnet} as the default backbone.
Since our method is not limited to a specific backbone, we also test HRNet-32~\cite{cheng2020bottom} in the experiments.
Through the backbone, a feature vector  is extracted from the input single RGB image. 
Also, we adopt the CoordConv~\cite{liu2018coordconv} to enhance the spatial information at each body center.
Therefore, the backbone feature  is the combination of a coordinate index map  and .
Next, from , three head networks are developed to estimate the Body Center, Camera, and SMPL maps. Each head network is composed of two ResNet blocks~\cite{resnet} with batch normalization. More details of the architecture are in the supplementary material.

\textbf{Setting Details.} 
The input images are resized to , keeping the same aspect ratio and padding with zeros. 
The size of the backbone feature is . 
The maximum person number  could be determined by the certain situation.
The loss weights are set to , and  to ensure that the weighted loss items are in the same magnitude.
The threshold of Body Center heatmap is  and the intensity coefficient of CAR is . 

\textbf{Training Datasets.}
For a fair comparison with previous methods~\cite{jiang2020coherent,hmr,kolotouros2019spin,sun2019dsd-satn}, the basic training datasets we used in experiments includes two 3D pose datasets (Human3.6M~\cite{h36m} and MPI-INF-3DHP~\cite{mono-3dhp2017}), one pseudo-label 3D dataset (UP~\cite{unite}) and four in-the-wild 2D pose datasets (MS COCO~\cite{coco}, MPII~\cite{mpii}, LSP~\cite{lsp,lsp_extended} and AICH~\cite{aich}) with their pseudo 3D annotations~\cite{kolotouros2019spin} of part images.

Besides, to further explore the upper limit of performance, we also use additional training datasets, including two 3D pose datasets (MuCo-3DHP~\cite{mono-3dhp2017} and OH~\cite{zhang2020object}), the pseudo-3D-label~\cite{joo2020eft} of part 2D pose datasets, and two 2D pose datasets (PoseTrack~\cite{PoseTrack} and Crowdpose~\cite{crowdpose}), to train an advanced model.

\textbf{Evaluation Benchmarks.} 
3DPW~\cite{3dpw} is employed as the main benchmark for evaluating 3D mesh/joint error since it contains in-the-wild multi-person videos with abundant 2D/3D annotations, such as 2D pose, 3D pose, SMPL parameters, human 3D mesh, etc.
Specially, we divide 3DPW into 3 subsets, including 3DPW-PC for person-person occlusion, 3DPW-OC for object occlusion, and 3DPW-NC for the non-occluded/truncated cases, to evaluate the performance in different scenarios.
Additionally, to compare with previous multi-person SOTA approaches, we also evaluate on CMU Panoptic~\cite{cmu_panoptic}, a standard multi-person 3D pose benchmark captured in a lab environment. 
Furthermore, we evaluate the stability under object/person-person occlusion on 3DOH50K~\cite{zhang2020object}, a object-occluded single-person indoor 3D benchmark, and Crowdpose~\cite{crowdpose}, a crowded-person in-the-wild 2D pose benchmark.

\textbf{Evaluation Metrics.}
We adopt per-vertex error (PVE) to evaluate the 3D surface error.
To evaluate the 3D pose accuracy, we employ mean per joint position error (MPJPE), Procrustes-aligned MPJPE (PMPJPE), percentage of correct keypoints (PCK), area under the PCK-threshold curve (AUC).
We also adopt mean per joint angle error (MPJAE), and Procrustes-aligned MPJAE (PA-MPJAE) to evaluate the 3D joint rotation accuracy. 
Besides, to evaluate the pose accuracy in crowded scenes, we calculate the average precision () between the 2D-projection  and the ground truth 2D poses on Crowdpose.

\begin{table}
 \setlength{\tabcolsep}{2pt}{
	\begin{center}
		\caption{ {Comparisons to the SOTA methods on 3DPW following \textit{Protocol 3} (fine-tuned on the training set).  means using extra datasets (compared with EFT) for training.}}\label{tab:3DPW_VIBE3}
        \rowcolors{2}{gray!10}{white}
			\begin{tabular}{l|ccc}
				\hline
			    \textbf{Method} &  \textbf{MPJPE} & \textbf{PMPJPE}  &\textbf{PVE}\\
				\hline 
                 EFT~\cite{joo2020eft} &   - & 52.2 & - \\
                 VIBE~\cite{kocabas2020vibe} & 82.9 & 51.9 & 99.1\\
                 \hline 
                 ROMP (ResNet-50) &  84.2 & 51.9 & 100.4 \\
ROMP (HRNet-32)   & 78.8 & 48.3 & 94.3 \\
                 ROMP (HRNet-32) & \textbf{76.7} & \textbf{47.3} & \textbf{93.4}\\
				\hline
		\end{tabular}
	\end{center}}
    \vspace{-6mm}
\end{table}


\begin{table}[t]
\setlength\tabcolsep{0.5mm}
  \centering
  \caption{ {Comparisons to the SOTA methods on CMU Panoptic~\cite{cmu_panoptic} benchmark. The evaluation metric is MPJPE after centering the root joint. All methods are directly evaluated without any fine-tuning.   means using extra datasets for training.}}  \label{tab:CMU Panoptic}
  {
    \rowcolors{2}{gray!10}{white}
    \begin{tabular}{l|cccc|c}
    \hline
    \textbf{Method} &  \textbf{Haggling} & \textbf{Mafia} &\textbf{Ultim.} & \textbf{Pizza} & \textbf{Mean}\\
    \hline
        Zanfir et. al.~\cite{zanfir2018deep} & 141.4 & 152.3 & 145.0 & 162.5 & 150.3 \\
        MSC~\cite{zanfir2018monocular} & 140.0 & 165.9 & 150.7 & 156.0 & 153.4 \\
        CRMH~\cite{jiang2020coherent} & 129.6 & 133.5 & 153.0 & 156.7 & 143.2 \\
        ROMP (ResNet-50) & \textbf{111.8} & \textbf{129.0} & \textbf{148.5} & \textbf{149.1} & \textbf{134.6} \\
        \hline
        ROMP (ResNet-50) & \textbf{107.8} & 125.3 & \textbf{135.4} & \textbf{141.8} & \textbf{127.6} \\
        ROMP (HRNet-32) & 110.8 & \textbf{122.8} & 141.6 & \textbf{137.6} & 128.2 \\
    \hline
    \end{tabular} }
    \vspace{-0.2cm}
\end{table}


\begin{table}
	\begin{center}
		\caption{ {Comparisons to the SOTA methods on the person-occluded (3DPW-PC),  object-occluded (3DPW-OC) and  non-occluded/truncated (3DPW-NC) subsets of 3DPW, meanwhile, ablation study on using CAR with diverse repulsion coeff. .  The evaluation metric is PMPJPE. }}\label{tab:3DPW-SPLIT}
        \setlength{\tabcolsep}{0.8mm}{
        {
          \rowcolors{2}{gray!10}{white}
			\begin{tabular}{l|ccc}
				\hline
				\textbf{Method} & \textbf{3DPW-PC} &  \textbf{3DPW-NC} & \textbf{3DPW-OC} \\
				\hline 
                CRMH~\cite{jiang2020coherent} & 103.5  & 68.7 & 78.9\\
                VIBE~\cite{kocabas2020vibe}  &103.9  & 57.3 &\textbf{ 65.9}\\
                \hline 
                ROMP w/o CAR & 79.7 & 56.7 &  67.0\\
                - w/ CAR () & 77.6 & \textbf{55.6} & 66.6 \\
                - w/ CAR () & \textbf{75.8}  & 57.1 & 67.1\\
                - w/ CAR () & 77.0 & 56.4 & 66.5 \\
				\hline
		\end{tabular}}}
	\end{center}
    \vspace{-0.5cm}
\end{table}


\vspace{-1mm}
\subsection{Comparisons to the State-of-the-Art~\label{sec:comparisons}}
\vspace{-1mm}
\textbf{3DPW.} 
We adopt three evaluation protocols to compare from different aspects.
To validate the performance in actual scenes, we follow \textit{Protocol 1} from the 3DPW Challenge to evaluate the entire 3DPW dataset without using any ground truth during inference, especially the ground truth bounding box. 
With the whole image as input, we equip each single-person method~\cite{kocabas2020vibe,kolotouros2019spin} with a human detector (OpenPose~\cite{openpose} or YOLO~\cite{redmon2018yolov3}).
For a fair comparison, ROMP uses the same backbone (ResNet-50) and training datasets as the competing SOTA method~\cite{kolotouros2019spin}.
We obtain the results of  OpenPose + SPIN from~\cite{imry2020challengespin}. 
The results of  YOLO + VIBE are obtained using their officially released code, which already contains the YOLO part for human detection.
In Tab.~\ref{tab:3DPW}, ROMP outperforms previous multi-stage methods in all evaluation metrics by a significant amount.
We observe significant improvement in MPJPE, PMPJPE, and MPJAE metrics.
These results validate that learning pixel-level representation with a holistic view is vital for improving the robustness and generalization in actual scenes.
Results of training with extra datasets show that ROMP could be further improved via adding the training data. 

As a sanity check, we compare ROMP with the single-person approaches using their evaluation protocols, which allows the use of the cropped single-person image as input.
While as a multi-person method, ROMP still takes the whole image as input.
Following VIBE~\cite{kocabas2020vibe}, \textit{Protocol 2} directly uses the 3DPW test set for evaluation without fine-tuning on the training set, while \textit{Protocol 3} fine-tunes the model on the 3DPW training set and utilizes the test set for evaluation.
In Tab.~\ref{tab:3DPW_VIBE2}, ROMP outperforms previous SOTA methods on \textit{Protocol 2}, which further demonstrates the superior of our one-stage design.
In  Tab.~\ref{tab:3DPW_VIBE3}, ROMP achieves comparable results with the SOTA methods. 
Note that, if we replace the ResNet-50 with HRNet-32 which extracts multi-resolution features, superior performance could be achieved after fine-tuning. While in Tab.~\ref{tab:3DPW} and \ref{tab:CMU Panoptic}, the model using HRNet-32 as backbone only achieves comparable results with the one using ResNet-50. 
These results show that using a heavier backbone helps to fit a specific domain.

\begin{table}
	\begin{center}
		\caption{{Comparisons to the SOTA methods on Crowdpose~\cite{crowdpose} benchmark.  The evaluation metric is .}}\label{tab:Crowdpose}
        \setlength{\tabcolsep}{0.6mm}{
			\begin{tabular}{l|c>{\columncolor{gray!10}}ccc}
                 \hline
				\textbf{Split}  & CRMH~\cite{jiang2020coherent}& ROMP &  ROMP+CAR \\
				\hline 
               Test & 33.9 & 54.1 & \textbf{59.7}\\
               Validation  & 32.9& 55.6 & \textbf{58.6}\\
				\hline
		\end{tabular}}
	\end{center}
     \vspace{-0.5cm}
\end{table}

\begin{table}
	\begin{center}
		\caption{{Comparisons to the SOTA methods on object occlusion benchmark,  3DOH50K. }}\label{tab:3DOH50K}
        \setlength{\tabcolsep}{0.6mm}{
   			\begin{tabular}{l|c>{\columncolor{gray!10}}cc>{\columncolor{gray!10}}c}
				\hline
				\textbf{Method} &  SPIN~\cite{gcmr} &OOH~\cite{zhang2020object} & CRMH~\cite{jiang2020coherent} & \textbf{ROMP} \\
				\hline 
				 \textbf{PMPJPE}& 67.5 & 58.5 & 56.9 &\textbf{43.9} \\
				\hline
		\end{tabular}}
	\end{center}
    \vspace{-0.5cm}
\end{table}


\begin{table}
	\begin{center}
		\caption{{Run-time comparisons on a 1070Ti GPU. }}\label{tab:runtime comparison}
        \setlength{\tabcolsep}{0.6mm}{
			\begin{tabular}{c|c>{\columncolor{gray!10}}cc>{\columncolor{gray!10}}c}
				\hline
				\textbf{Method} & VIBE~\cite{kocabas2020vibe} & CRMH~\cite{jiang2020coherent}&ROMP& ROMP \\
                \hline
                FPS & 10.9 & 14.1 & 20.8 & \textbf{30.9}\\
                \hline
                Backbone & ResNet-50 & ResNet-50 & HRNet-32 & ResNet-50\\
				\hline
		\end{tabular}}
	\end{center}
     \vspace{-5mm}
\end{table}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{FPS_nonum.png} 
	\caption{ The FPS variations of ROMP and YOLO+VIBE~\cite{kocabas2020vibe} when processing images with different number of people.
}\vspace{-1mm}
	\label{fig:fps}
\end{figure}

\textbf{CMU Panoptic.} 
Following the evaluation protocol of CRMH~\cite{jiang2020coherent}, we evaluate ROMP on the multi-person benchmarks, CMU Panoptic, without any fine-tuning.
For a fair comparison, we use the same backbone and similar training set as CRMH.
As shown in Tab.~\ref{tab:CMU Panoptic} , ROMP outperforms the existing multi-stage  methods~\cite{jiang2020coherent,zanfir2018monocular,zanfir2018deep} in all activities by a large margin.
These results further demonstrate that learning pixel-level representation with a holistic view improves the performance in the multi-person scenes.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\columnwidth]{demo_comparison.png}
	\caption{Qualitative comparisons to the multi-person SOTA method, CRMH~\cite{jiang2020coherent}, on the Crowdpose and the internet images.}\vspace{-5mm} 	\label{fig:demo_comparison}
\end{figure}

\textbf{Occlusion benchmarks.}
To validate the stability under occlusion, we evaluate ROMP on multiple occlusion benchmarks.
Firstly, on the person-occluded \textbf{3DPW-PC} and \textbf{Crowdpose}~\cite{crowdpose}, results in Tab.~\ref{tab:3DPW-SPLIT} (second column) and \ref{tab:Crowdpose} show that ROMP significantly outperforms previous SOTA methods~\cite{jiang2020coherent,kocabas2020vibe}.
Additionally, in Figure~\ref{fig:demo_comparison}, some qualitative comparisons to 
the multi-person SOTA methods in the person-person occlusion scenes also demonstrate our robustness.
These results suggest that the pixel-level representation is important for improving the performance under person-person occlusion.
Finally,  on the object-occluded \textbf{3DOH50K}~\cite{zhang2020object} and \textbf{3DPW-OC}, ROMP also achieves superior performance in Tab.~\ref{tab:3DOH50K}.
These results demonstrate that the fine-grained pixel-level representation is beneficial for dealing with various occlusion cases.

\textbf{Runtime.} 
To validate the computational efficiency, we compare ROMP with the SOTA methods in processing videos captured by a web camera.
As shown in Tab.~\ref{tab:runtime comparison}, ROMP achieves real-time performance (30.9 FPS using ResNet-50 and 20.8 FPS using HRNet-32), significantly faster than the competing methods.
Additionally, as shown in Fig.~\ref{fig:fps}, compared with the multi-stage methods~\cite{jiang2020coherent,kocabas2020vibe}, ROMP's processing time is roughly constant regardless of the number of people. 
Please refer to the supplementary material for more experiments.


\vspace{-1mm}
\subsection{Ablation Study of the CAR}
\vspace{-1mm}

We perform the ablation study of the proposed CAR on person-occluded 3DPW-PC and Crowdpose.
As the results shown in Tab.~\ref{tab:3DPW-SPLIT} and \ref{tab:Crowdpose}, the proposed CAR improve the PMPJPE metric on the 3DPW-PC and the Crowdpose datasets by 4.8\% and 10.3\%, respectively.
Except for the quantitative results of the proposed CAR, we also provide the qualitative ablation study in Figure~\ref{fig:CAR_qab}.
Both results show that adding the proposed CAR further improves the performance, which demonstrates that CAR effectively tackles the center collision problem in the crowded scenes.

\textbf{Intensity coefficient  of the CAR. }
To figure out the proper setting of the intensity coefficient , we conduct a further ablation study on 3DPW-PC.
Results in Tab.~\ref{tab:3DPW-SPLIT} show that if  is too large, the 3D pose accuracy on 3DPW-PC  decreases. 
Therefore, we adopt  to balance between the distinguishable center and consistent representation learning.
Further, in Tab.~\ref{tab:3DPW-SPLIT}, we observe varying degrees of performance degradation by adding the CAR in these cases without strong person-person occlusion.
It indicates the reason of performance degradation is probably that pushing apart the body centers affects the consistency of the body-center-guided representation.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\columnwidth]{CAR_qab.png}
	\caption{Qualitative ablation study of the CAR on the Crowdpose.}\vspace{-5mm}
	\label{fig:CAR_qab}
\end{figure}

\vspace{-1mm}
\subsection{Discussion}
\vspace{-2mm}
To determine the source of our performance gains, we conduct an ablation study on 3DPW subsets containing different scenes. 
Compared with the SOTA methods~\cite{jiang2020coherent,kocabas2020vibe} in Tab.~\ref{tab:3DPW-SPLIT}, our main gains come from the person-occluded and the non-occluded/truncated cases. 
These results show that the proposed pixel-level representation can promote more effective learning of the person-person occlusion and truncation cases.
Additionally, as introduced in Sec.~\ref{sec:comparisons}, ROMP achieves superior results on multiple benchmarks without fine-tuning, which demonstrates our robustness and generalization in various actual scenes.
Through controlling the variables (e.g., backbone, training settings, etc.) in the experiments, we narrow the difference between ROMP and the SOTA methods~\cite{jiang2020coherent,kocabas2020vibe,kolotouros2019spin} to the way of representation learning.
ROMP learns the pixel-level representation in a holistic view, while the multi-stage SOTA methods learn the bounding-box-level representation in a local view.
Except for the guidance of the robust body centers, our fully convolutional design promotes ROMP to learn more discriminative features against rich disturbances outside the bounding box, which is important for better generalization. 

\vspace{-2mm}
\section{Conclusion}
\vspace{-2mm}
We introduce a novel one-stage network, ROMP, for monocular multi-person 3D mesh regression. 
For pixel-level estimation, we propose an explicit body-center-guided representation and further develop it as a collision-aware version, CAR, which enables robust prediction under the person-person occlusion. 
ROMP is the first open-source one-stage method that achieves SOTA performance on multiple benchmarks as well as real-time inference speed.
For the community, ROMP could be a simple yet effective baseline for the related multi-person 3D tasks, such as depth estimation, tracking, and interaction modeling. 


{\small
\bibliographystyle{ieee_fullname}
\bibliography{arxiv}
}

\end{document}