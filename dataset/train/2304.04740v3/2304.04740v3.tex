

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} \usepackage{lipsum}
\usepackage{subcaption}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2023}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{bbm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\se}[1]{\textcolor{magenta}{[SE: #1]}}

\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\syl}{Syl}
\DeclareMathOperator{\aut}{Aut}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\Char}{char}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\arccosh}{arccosh}
\DeclareMathOperator{\arcsinh}{arcsinh}
\DeclareMathOperator{\ddiv}{div}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\refl}{refl}

\newcommand{\incl}{\hookrightarrow}
\newcommand{\onto}{\twoheadrightarrow}
\newcommand{\gen}[1]{\left\langle#1\right\rangle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\RP}{\mathbb{RP}}
\newcommand{\M}{\mathcal{M}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\paren}[1]{\left(#1\right)}
\newcommand{\brac}[1]{\left\{#1\right\}}
\newcommand{\sqbrac}[1]{\left[#1\right]}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\grad}{\nabla}
\newcommand{\parderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\inn}[1]{\left\langle#1\right\rangle}
\renewcommand{\vec}{\mathbf}
\newcommand{\dvb}{\parallel}
\newcommand{\dd}{\mathrm{d}}


\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}
 
\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Reflected Diffusion Models}

\begin{document}

\twocolumn[
\icmltitle{Reflected Diffusion Models}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aaron Lou}{stanford}
\icmlauthor{Stefano Ermon}{stanford}
\end{icmlauthorlist}

\icmlaffiliation{stanford}{Department of Computer Science, Stanford University}

\icmlcorrespondingauthor{Aaron Lou}{aaronlou@stanford.edu}

\icmlkeywords{Diffusion Models}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
    Score-based diffusion models learn to reverse a stochastic differential equation that maps data to noise. However, for complex tasks, numerical error can compound and result in highly unnatural samples. Previous work mitigates this drift with thresholding, which projects to the natural data domain (such as pixel space for images) after each diffusion step, but this leads to a mismatch between the training and generative processes. To incorporate data constraints in a principled manner, we present Reflected Diffusion Models, which instead reverse a reflected stochastic differential equation evolving on the support of the data. Our approach learns the perturbed score function through a generalized score matching loss and extends key components of standard diffusion models including diffusion guidance, likelihood-based training, and ODE sampling. We also bridge the theoretical gap with thresholding: such schemes are just discretizations of reflected SDEs. On standard image benchmarks, our method is competitive with or surpasses the state of the art without architectural modifications and, for classifier-free guidance, our approach enables fast exact sampling with ODEs and produces more faithful samples under high guidance weight. 

\vspace{-0.6cm} \end{abstract}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.95\textwidth]{fig/main.pdf}
    \caption{\textbf{Overview of Reflected Diffusion Models.} We map a data distribution  supported on  to the prior distribution  through a reflected stochastic differential equation (Section \ref{sec:method:rsde}). Whenever a Brownian trajectory hits , it is reflected back in instead of escaping (circled in red), so  is supported on  for all . We can recover  from  with a reversed reflected stochastic differential equation (Section \ref{sec:method:reverse}) by learning the Stein score  (Section \ref{sec:scorematching}). Our generative model is guaranteed to be constrained in .
    }\label{fig:main}
\end{figure*}

\section{Introduction}

Originally introduced in \citet{SohlDickstein2015DeepUL} and later augmented in \citet{song2019generative,Ho2020DenoisingDP, Song2020ScoreBasedGM}, diffusion models have quickly become one of the most ubiquitous deep generative models, with applications in many domains including images \citep{Dhariwal2021DiffusionMB}, natural language \citep{Li2022DiffusionLMIC}, and molecule generation \citep{Xu2022GeoDiffAG}. Additionally, their stability and scabality have enabled the deployment of large text-to-image systems \citep{Ramesh2022HierarchicalTI}.\blfootnote{Code Link: \href{https://github.com/louaaron/Reflected-Diffusion/}{https://github.com/louaaron/Reflected-Diffusion/}}


Diffusion models learn to reverse a stochastic process that maps data to noise, but, as a result of inherent approximation error of both the SDE discretization and score matching, they often follow incorrect trajectories. This behavior compounds error, so models can diverge and generate highly unnatural samples on more complex tasks (see for instance Figure \ref{fig:noclip}). To mitigate this degeneration, many diffusion models modify the sampling process by projecting to the support of the data after each diffusion step \citep{Ho2020DenoisingDP, Li2022DiffusionLMIC}, a technique known as thresholding. This incorporates the known constraints of the data distribution, stabilizing sampling and avoiding divergent behavior. Notably, this oft-overlooked detail underlies many pixel-based image diffusion models \citep{Ho2020DenoisingDP, Dhariwal2021DiffusionMB} (appearing as a  clipping function at each diffusion step) and is essential for text-to-image generation \citep{Saharia2022PhotorealisticTD}. Although thresholding avoids failure, it is theoretically unprincipled because it leads to a mismatch between the training and generative processes. Furthermore, this mismatch can introduce artifacts such as oversaturation \citep{Ho2022ClassifierFreeDG} that necessitate further modifications \citep{Saharia2022PhotorealisticTD}.

In this work, we present Reflected Diffusion Models, a class of diffusion models that, by design, respects the known support of the data distribution. Unlike standard diffusion models, which perturbs the data density with Brownian motion, our method evolves the distribution with reflected Brownian motion that always stays within the boundary. We then parameterize the reversed diffusion process with the scores of the perturbed density, which we learn using a new score matching method on bounded domains. The resulting generative model is a reflected SDE that automatically incorporates the data constraints without altering the generative process. We provide an overview of our method in Figure \ref{fig:main}.

Our proposed methodology has several merits:

\textbf{Scales to high dimensions.} To learn the score function on a general bounded domain, we introduce constrained denoising score matching (CDSM). Unlike previous methods \cite{Hyvrinen2007SomeEO}, CDSM scales to high dimensions, and we develop an algorithm for fast computation. Since reflection operations are negligible compared to neural network computation, our training and inference times are effectively equivalent to those of standard diffusion models.

\textbf{Key features transfer over.} We show that ODE sampling \citep{Song2020ScoreBasedGM}, diffusion guidance \citep{Ho2022ClassifierFreeDG}, and maximum likelihood bounds \citep{Song2021MaximumLT} extend to the reflected setting. As such, our method can be modularly applied to preexisting diffusion model systems.

\textbf{Justifying and correcting previous methods.} We draw connections with the thresholding methods used in pixel-space diffusion models \citep{Saharia2022PhotorealisticTD}. These methods all sample from a reflected stochastic differential equation despite being trained on a standard diffusion process. Correctly training with our CDSM loss avoids pathological behavior and allows for equivalent ODE sampling.

\textbf{Broad Applicability.} We apply our method to high-dimensional simplices (e.g. class probabilities) and hypercubes (e.g. images). Using a synthetic example, we show that our method is the a simplex diffusion model that scales to high dimensions. On common image generation benchmarks, our results are competitive with or surpass the current state of the art. In particular, on unconditional CIFAR-10 generation \citep{Krizhevsky2009LearningML}, we achieve a state of the art Inception Score of 10.46 and a comparable FID score of 2.72. For likelihood estimation, our method achieves a second best score of 2.68 and 3.74 bits per dimension on CIFAR-10 and ImageNet32 \citep{Oord2016PixelRN} without relying on either importance sampling or learned noise schedules. \section{Background}

To introduce diffusion models (in the continuous time formalism of \cite{Song2020ScoreBasedGM}), we first transform a data density  on  by applying a ``forward" diffusion process. This takes the form of perturbing points  with an SDE with a fixed drift coefficient , diffusion coefficient , and Brownian motion :

The resulting family of time varied distributions  approaches a known prior distribution . This density evolution process can be reversed by perturbing samples  with a reversed SDE \citep{Anderson1982ReversetimeDE}:

where  is time reversed Brownian motion. Diffusion models approximate this reverse process by learning , known as the score function, through a -weighted score matching loss:

which most commonly takes the form of the more tractable denoising score matching loss \citep{Vincent2011ACB}:

Here,  is the transition kernel induced by the SDE in Equation \ref{eqn:forwardsde}. With a learned score , one can define a generative model by first sampling  and then solving the reverse SDE

from time  to , giving an approximate sample from .

Diffusion models enjoy many special properties. For example, for certain , Equation \ref{eqn:weighteddenoisescorematching} can be reformulated as an ELBO using Girsanov's theorem \citep{Song2021MaximumLT,Kingma2021VariationalDM, Huang2021AVP}, allowing for maximum likelihood training. Furthermore, one can derive an equivalent Neural ODE that can be used for sampling and exact likelihood evaluation \citep{Chen2018NeuralOD}.

\textbf{Guidance.} One can also control the diffusion model to sample from a synthetic distribution . Here,  is a desired condition such as a class or text description, and interpolating the guidance weight  controls the fidelity and diversity of the samples. This requires the score

which can be learned  without requiring explicit training on . For example, classifier guided methods \citep{Song2020ScoreBasedGM, Dhariwal2021DiffusionMB} combine a pretrained score function  and classifier :

and classifier-free guidance methods \citep{Ho2022ClassifierFreeDG} uses a -conditioned score function and an implicit Bayes classifier 

\textbf{Thresholding.} However, since  is not a perfect score function, there is a mismatch between the modeled backward process and the true forward process. Thus, diffusion models can push a sample to areas where  is small, which creates a negative feedback loop since score matching struggles in low probability areas \citep{Song2019GenerativeMB, Koehler2022StatisticalEO}. This causes the sampling process to diverge and commonly occurs for more complex tasks, especially those involving diffusion guidance.

To combat this, many previous works alter the diffusion sampling procedure using thresholding \citep{Saharia2022PhotorealisticTD, Li2022DiffusionLMIC}, which stabilizes the sampling process with inductive biases from the data. In particular, thresholding applies an operator  that projects back to the data domain  during each discretized SDE step:

For the case of images,  can be static thresholding, which clips each dimension to the pixel range , and dynamic thresholding, which first normalizes all pixels by the -th percentile pixel before clipping \citep{Saharia2022PhotorealisticTD}.

Thresholding alleviates divergent sampling but comes with considerable downsides. For example, it breaks the theoretical setup since the generative model no longer approximates the reverse diffusion process. This mismatch induces artifacts during sampling and precludes the use of ODE sampling \citep{Song2020ScoreBasedGM}.
 \section{Reflected Diffusion Models}\label{sec:method}

In this section, we present Reflected Diffusion Models. These define a generative model on a data domain  (assumed to be connected and compact with nonempty interior and uniform Hausdorff dimension) which outer-bounds the support of the data distribution . Our method retains the theoretical underpinnings of diffusion models while incorporating inductive biases from thresholding. We highlight the core mechanisms in Figure \ref{fig:main}.

\subsection{Reflected Stochastic Differential Equations}\label{sec:method:rsde}
To model diffusion processes on a compact domain , we use reflected SDEs. For ease of presentation, we only give an intuitive definition of reflected SDEs and simplify so that  is scalar and  reflects in the normal direction. In Appendix \ref{sec:app:theory:rsde}, we provide a more rigorous mathematical definition and generalize to matrix diffusion coefficients and oblique reflections. For a full introduction, we recommend the readers consult a monograph such as \citet{Pilipenko2014AnIT}.

Our reflected SDEs perturb an initial datum  and are parameterized by a drift coefficient  and diffusion coefficient :

The first two terms on the right hand side of Equation \ref{eqn:reflsde} are exactly those of Equation \ref{eqn:forwardsde}, showing that our reflected SDE behaves like a regular SDE in the interior of .  is the additional boundary constraint that, intuitively, forces the particle to stay inside . When  hits ,  neutralizes the outward normal-pointing component.

This reflected SDE has a unique strong solution as long as  and  are Lipschitz in state and time and  satisfies the uniform exterior sphere condition \citep[Theorem 2.5.4]{Pilipenko2014AnIT}, which ensures that  is sufficiently regular. In particular, the uniform exterior sphere condition holds true when  is smooth and even when  is a convex polytope.

\subsection{Density Evolution and Time Reversal}\label{sec:method:reverse}

When we perturb  with the reflected SDE in Equation \ref{eqn:reflsde}, our density evolves according to the Fokker-Planck equation with Neumann boundary condition \citep{Schuss2013BrownianDA}:
\vspace{-1.5mm}

In addition to allowing us to characterize the limiting density , this induces a reversed reflected stochastic differential equation \citep{Cattiaux1988TimeRO, Williams1988OnTO}:

where  is the reversed boundary condition. For our case,  also reflects in the normal direction.
\begin{remark}
    The reversed reflected SDE closely resembles the reversed standard SDE given in Equation \ref{eqn:backwardsde}. On one hand, this is natural because local dynamics match: when ,  disappears since  can never hit . On the other hand, it is surprising that we can reverse a reflected diffusion process with another reflected diffusion process, something that does not hold in the discrete time case.
\end{remark}

\subsection{Reflected SDEs in Practice}\label{sec:method:examples}

In our experiments,  will be either the unit cube  or the unit simplex, which is given by . We often find it more convenient to work with the projected simplex  as it is bounded in  instead of in a hyperplane.

We will diffuse with the  Reflected Variance Exploding SDE (RVE SDE), a generalization of the Variance-Exploding SDE introduced in \citet{Song2020ScoreBasedGM}. A RVE SDE is parameterized by  and is defined for  by

where . The reverse is

Note that the RVE SDE corresponds to a time dilated version of reflected Brownian motion: time  of a RVE SDE corresponds to time  of reflected Brownian motion, where . As a result of Equation \ref{eqn:neumfp},  evolves under a heat equation with Neumann boundary conditions:

Note that  becomes a uniform density over  for large enough . To see this, we can draw intuition from physics: heat homogenizes in a closed container.

 \section{Score Matching on Bounded Domains}\label{sec:scorematching}

While the reflected SDE framework provides a nice theoretical pathway to construct a reflected diffusion model, it requires one to learn the score function  on . We minimize the constrained score matching loss:

where we omit time-dependence for presentation purposes. Furthermore,  indicates the domain of the expectation (as opposed to  which is an integral over ). This is because  can be discontinuous at  (since it is  outside of  and can be nonzero on ), so constraining the integral ensure regularity properties used for theorems (such as Stokes').

In this section, we review previous methods for score matching on bounded domains, discuss their fundamental limitations, and propose constrained denoising score matching to overcome these difficulties. Additionally, for the RVE SDE introduced in Section \ref{sec:method:examples}, we show how to quickly compute the score matching training objective.

\subsection{Pitfalls of Implicit Score Matching}

One may hope to draw inspiration from the standard paradigm, which transforms the score matching integral

into the implicit score matching loss \citep{Hyvrinen2005EstimationON}:

This removes the intractable , allowing for estimation using Monte Carlo sampling. However, the derivation requires the use of Stokes' theorem; applying Stokes' theorem to Equation \ref{eqn:bsm} would instead result in

where  is the interior pointing normal vector. Unlike the case of , where the second term disappears since , this result is computationally intractable. Thus, previous work instead proposes to re-weight the loss function with a nonnegative function  that vanishes on the boundary \citep{Hyvrinen2007SomeEO, Yu2020GeneralizedSM}, minimizing

Since  vanishes on , we can cleanly apply Stokes' theorem and derive a result without a boundary term, giving an implicit score matching loss:

However, this formulation is not suitable for high dimensions, even with fast numerical algorithms for the divergence operator \citep{Hutchinson1989ASE, Song2019SlicedSM}. This is because the loss is downweighted near the boundaries, so, for a fixed budget, the error can become unbounded as . For high dimensions, the space near the boundary becomes an increasingly larger proportion of the total volume\footnote{Consider the case when . For large , almost all the mass is close to the boundary.}, which greatly hampers the sample efficiency of the loss.

\subsection{Constrained Denoising Score Matching}

Inspired by the empirical success of denoising score matching \citep{Vincent2011ACB, Song2019GenerativeMB}, we present constrained denoising score matching (CDSM). Crucially, denoising score matching, unlike implicit score matching, can directly generalize to bounded domains due to how it handles discontinuities. This means that, unlike previous methods for constrained score matching, the derivation transfers smoothly. The core mechanism is presented in the following proposition, which we prove in Appendix \ref{sec:app:theory:cdsm}.
\begin{proposition}\label{prop:cdsm}
    Suppose that we perturb an -supported density  with noise  (also supported on ) to get a new density . Then, under suitable regularity conditions for the smoothness of  and , the score matching loss for :
    
    is equal (up to a constant factor that does not depend on ) to the CSDM loss:
    
\end{proposition}

With the constrained denoising score matching loss, we are then able to define a training objective for reflected diffusion models. In particular, since  is a by definition perturbed density of  with transition kernel , the weighted score score matching loss directly becomes:

For our reflected SDE, we will set , mirroring previous work and minimizing variance during optimization. Interestingly, as we prove in Section \ref{sec:likelihood}, this corresponds to an ELBO loss when we reverse a RVE SDE.

\subsection{Scaling Score Matching Computation}\label{sec:sm:scaling}

\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{fig/method1.pdf}
        \caption*{(i)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{fig/method2.pdf}
        \caption*{(ii)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{fig/method3.pdf}
        \caption*{(iii)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{fig/method4.pdf}
        \caption*{(iv)}
    \end{subfigure}
    \caption{\textbf{An overview of our computational method for constrained denoising score matching with Brownian transition probabilities.} (i) We can draw samples by sampling  and then applying reflections on the boundary. (ii) When  is small, we compute the transition density by summing up a mixture of Gaussians (shown for ). (iii) When  is large, we compute using the frequencies of  (shown for ). (iv) We diffeomorphically transform , where the transition score is tractable.}\label{fig:rhk}
\end{figure*}

We finalize by showing how to sample from and compute the score of the transition density  for the RVESDE. Note that this is the transition density of a reflected Brownian Motion \citep{Harrison1981ReflectedBM}. We highlight the key features of our method in Figure \ref{fig:rhk}.

\textbf{Sampling.} To sample from , we can repeatedly reflect a sample  from . In particular, we follow the line segment , reflecting in the normal direction when it crosses  and repeating until we reach . This works because, intuitively, the boundary redirects the the Brownian motion but does not change the magnitude. In practice, this process can be quickly computed with classic computational geometric techniques.

\textbf{Score Computation.} There are two approaches for computing the score of  on general geometric domains:

\textit{Approximation with Sum of Gaussians} \citep{Jing2022TorsionalDF}. This method decomposes  into an infinite sum of Gaussian densities that depend on , , and the geometry of the domain. For our bounded  with the reflection condition, this gives us the equation

where  is the pdf of the Gaussian centered at  with variance  and  is the set of all  s.t. the repeated reflection of the path  ends in . Note that this reflection scheme is the same one we use for sampling. Furthermore, through elementary derivations, this gives us a formula for the score .

Generally, this method works quite well for small , as we only need to take a small number of local reflections to approximate . However, for larger , we need to take many more reflections since the underlying Gaussian is too dispersed, greatly increasing the computational cost.

\textit{Approximation with Laplacian Eigenfunctions} \citep{Bortoli2022RiemannianSG}. This method instead computes using Laplacian Eigenfunctions, a standard technique for solving the heat equation \citep{evans10}. For our problem, these are a (known for each ) set of functions  that satisfy  and  on . In particular, these form an orthonormal basis for , allowing us to solve Equation \ref{eqn:neumhe} directly for an initial density of :

This method works well for large  because this means that , removing the need to evaluate many of the terms. However, for small , this method becomes costly because it requires many more terms. Similar to the above method, we can derive a formula for  through this sum.

\textit{Our Method.} We instead propose to combine the above two approaches. In particular, we note that they complement each other: Gaussian sum is accurate for small  and eigenfunction sum is accurate for large . We can therefore set a  and compute with Gaussian sum when  and with eigenfunction sum when . In practice, this allows us to upper-bound the number of reflections/eigenfunctions used to , much fewer than the exponential amount required for each method individually. 

\textit{Scaling to High Dimensions.} By itself, this branching method is unfortunately not enough to scale to very high dimensions. In particular, our computation is, in the worst case,  where  is the dimension of  and  is the number of reflection steps or the highest eigenfunction frequency. We have only bounded  to something more manageable. 

To overcome this scaling issue, we consider the simple case of the hypercube . Since our Brownian motion does not have inter-dimensional interactions, reflections do not interact between non-parallel hyperplanes, and Laplacian eigenfunctions factorize by dimension, we can decompose the probability along each component interval:

where  and  are the -components of  and  respectively and  is the marginal probability on the -th coordinate. Note that the RVE SDE on  marginalizes to a RVE SDE on  for each dimension:

where  and  are the Brownian motion and boundary condition (respectively) for dimension . We can therefore compute on each  and combine the results, reducing the cost from  to . Regular score matching is , and since  is small, we can train Reflected Diffusion Models just as quickly as regular diffusion models.

For more general domains, under certain conditions, we can smoothly and bijectively map from . Thus, we can instead learn a diffusion model on  and then project back to . More details are given in Appendix \ref{sec:app:prac:diffmap}. In particular, this mapping procedure allows us to learn a diffusion model on high-dimensional simplices .
%
 \section{Simulating Reflected SDEs}

Combining a score  learned through CSDM and the reverse reflected SDE, we have a Reflected Diffusion Model: sample  and solve the reflected SDE:

In this section, we examine numerical methods for simulating examples from this reflected SDE.

\subsection{Euler-Maruyama Discretizations and Thresholding}

The typical Euler-Maruyama discretization of a standard SDE (Equation \ref{eqn:forwardsde}) is given by

where . For reflected SDEs, one can adapt this discretization by approximating the effect of  with some suitable operators .

Common examples of  include the projection operator  \citep{Liu1993NumericalAT} or the reflection operator  used in Section \ref{sec:sm:scaling} \citep{Schuss2013BrownianDA}. One can see that, as , both the projection and reflection schemes converge in distribution. Empirically, we find that reflection generates better samples.

Interestingly, this closely mirrors the thresholding step given in Equation \ref{eqn:thresholding}, with the only difference being the choice of operator  and whether  is applied before or after the noise step. This difference disappears when :
\begin{proposition}[Thresholding solves a reflected SDE]
    Both types of thresholding solve the reflected SDE (Equation \ref{eqn:reflsde}) as  under suitable conditions.
\end{proposition}
The full proposition and proof are given in Appendix \ref{sec:app:theory:thresh}.

\subsection{Predictor Corrector}

We extend the predictor-corrector (PC) framework of \citet{Song2020ScoreBasedGM}, which has been shown to improve results. In particular, our learned scores can be used to augment the sampling procedure using Langevin Dynamics \citep{Song2019GenerativeMB}. However, this requires Langevin dynamics for a constrained domain \citep{Bubeck2015FiniteTimeAO}, which, for the probability , are given by the reflected SDE:

During our reversed diffusion iterations, we can discretize the langevin dynamics using Reflected Euler-Maruyama and apply our learned score :

In practice, we find that PC sampling with a small signal-to-noise ratio noticeably improves image generation results.

\textbf{CIFAR-10 Quality Results.} With these components, we test our method for image generation on the CIFAR-10 dataset and report Inception Score (IS) \citep{Salimans2016ImprovedTF} and Frechet Inception Distance (FID) \citep{Heusel2017GANsTB} in table \ref{tab:cifar10results}. Our models remain competitive, achieving a SOTA Inception score of 10.42. However, Tweedies' formula does generalize to reflected diffusion \citep{Efron2011TweediesFA} (more details are in Appendix \ref{sec:app:prac:denoise}), so our model generates images with imperceptible noise (on the scale of  pixels), which degrades the FID score to 2.72 \citep{JolicoeurMartineau2020AdversarialSM}. Despite this, our samples are diverse and visually indistinguishable (Appendix \ref{sec:app:fig}).

\begin{table}[t]
    \centering
    \begin{tabular}{l c c}
        Model & IS  & FID \\
        \hline
        NCSN++ \citep{Song2020ScoreBasedGM} & 9.89 & 2.20\\
        DDPM++ \citep{Song2020ScoreBasedGM} & 9.68 & 2.41\\
        Styleformer \citep{Park2021StyleformerTB} & 9.94 & 2.82\\
        UNCSN++ \citep{Kim2021SoftTA} & 10.11 & --\\
        VitGAN \citep{Lee2021ViTGANTG} & 9.89 & 4.87\\
        Subspace NCSN++ \citep{Jing2022SubspaceDG} & 9.99 & 2.17\\
        EDM \citep{Karras2022ElucidatingTD} & -- & \textbf{1.97}\\
        \hline
        Reflected Diffusion (ours) & \textbf{10.46} & 2.72\\
    \end{tabular}
    \caption{\textbf{CIFAR10-Sample Quality Results.} We test Reflected Diffusion Models on CIFAR-10 Image Generation and report IS and FID scores. Our model is highly competitive, achieving a state of the art-inception score for unconditional generation. However, FID lags behind due to noise (as discussed in Appendix \ref{sec:app:prac:denoise})}.
    \label{tab:cifar10results}
    \vspace{-5mm}
\end{table}

\subsection{Probability Flow ODE}

Similarly to the probability flow ODE derived in \citet{Song2020ScoreBasedGM}, one can construct an equivalent deterministic process for a reflected SDE. Interestingly, doing this removes the boundary reflection term, so our deterministic process is exactly the original probability flow ODE derived in \citet{Song2020ScoreBasedGM}:

Crucially, the thresholding effect is maintained due to the Neumann condition for  (Equation \ref{eqn:neumfp} line 2) and can't be replicated for standard diffusion models. We elaborate on this construction, as well as connections with DDIM \citep{Song2020DenoisingDI} in Appendix \ref{sec:app:theory:ode}.
  \section{Diffusion Guidance}

Both classifier and classifier-free guidance (Equations \ref{eqn:classguid} and \ref{eqn:classfreeguid}) extend to Reflected Diffusion Models by logarithm and gradient rules. Since thresholding is primarily useful for diffusion guidance, we investigate the relationship between thresholding, diffusion guidance, and Reflected Diffusion Models on the relatively simple downsampled 64x64 ImageNet dataset \citep{Russakovsky2014ImageNetLS}.

\textbf{Thresholding is critical.} We corroborate \citet{Saharia2022PhotorealisticTD}, showing that pixel-spaced diffusion guidance requires thresholding. We show this for classifier-free guidance in Figure \ref{fig:noclip}, where even a low weight  causes about half of the samples to diverge. For classifier guidance, around \% of samples diverge (Figure \ref{fig:cl_guidance_noclip}).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{fig/baseline_noclip_w1.png}
    \caption{\textbf{Without thresholding, standard diffusion models easily diverge}. We sample using classifier-free guidance ( from a standard diffusion model without using thresholding. Around half of the samples diverge (generating blank images).}\label{fig:noclip}
    \vspace{-5mm}
\end{figure}

\begin{figure*}[!h]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{fig/guide_ours.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{fig/guide_baseline.png}
    \end{subfigure}
    \caption{\textbf{Non cherry-picked guided samples from a reflected and standard diffusion model with high guidance weight.} We compare Reflected Diffusion Models with standard diffusion models for generating class-conditioned 64x64 ImageNet samples for a guidance weight . Our generated images are shown on the left, and the baseline is shown on the right (same positions have same classes). Our method retains fidelity while the baseline suffers from oversaturation.}\label{fig:diffguide}
\end{figure*}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.4\textwidth]{fig/ode_classifier-free_1_5.png}
    \caption{\textbf{Guided ODE samples}. We sample using our ODE with a guidance weight , retaining image fidelity with fewer forward evaluations (around 100 compared with 1000).} \label{fig:odesample}
    \vspace{-5mm}
\end{figure}

\textbf{Our method retain fidelity under high guidance weight.} Thresholding produces oversaturated images under high guidance weight  \citep{Ho2022ClassifierFreeDG, Saharia2022PhotorealisticTD}, hampering applications which require high fidelity generation. We hypothesize that this is caused by the training and sampling mismatch, and we show in Figure \ref{fig:diffguide} that our method retains fidelity under high guidance weight. We did not find dynamic thresholding method to perform better.

\textbf{ODE sampling works for classifier-free guidance.} The composed score function in classifier-free guidance (Equation \ref{eqn:classfreeguid}) maintains the Neumann boundary condition (Equation \ref{eqn:neumfp}), allowing for ODE sampling. Using this, we demonstrate the first case of high-fidelity classifier-free guided generation using ODEs in Figure \ref{fig:odesample}. Interestingly, ODE equivalent DDIM sampling fails for classifier-free guidance but works for classifier guidance, despite classifier guidance being worse without thresholding (Appendix \ref{sec:app:fig}).




 \section{Likelihood Bound}\label{sec:likelihood}

Incidentally, our weighted score matching loss corresponds to an ELBO for our generative model. To show this, we extend Girsanov's Theorem \citep{ksendal1987StochasticDE}, which is used to th derive the ELBO for standard diffusion models \citep{Song2021MaximumLT,Kingma2021VariationalDM,Huang2021AVP}:
\begin{theorem}[Reflected Girsanov for KL divergence]\label{thm:reflgirsanov}
    Suppose we have two reflected SDEs on the same domain 
    
    from  to  with . 
    
    Let  be the path measures for (resp.)  and . Then,
    
\end{theorem}
The full theorem and proof are given in Appendix \ref{sec:app:theory:girs}.

\begin{table}[!h]
    \centering
    \begin{tabular}{l c c}
        Model & C-10 & IN32\\
        \hline
        \textbf{Non-diffusion} & &\\
        \hline
        Flow++ \citep{Ho2019FlowIF} & 3.08 & --\\
        Pixel-CNN++ \citep{Salimans2017PixelCNNIT} & 2.92 &--\\
        Sparse Transformer \citep{Child2019GeneratingLS} & 2.80 & --\\
        \hline
        \textbf{Diffusion: Modified Noise Schedule} & &\\
        \hline
        ScoreFlow \citep{Song2021MaximumLT}& 2.83 & 3.76\\
        VDM \citep{Kingma2021VariationalDM} & \textbf{2.65} & \textbf{3.72}\\
        \hline
        \textbf{Diffusion: No Noise Modifications} & &\\
        \hline
        ScoreSDE \citep{Song2020ScoreBasedGM} & 2.99 & --\\
        ARDM \citep{Hoogeboom2021AutoregressiveDM} & 2.71 & --\\
        ScoreFlow \citep{Song2021MaximumLT}& 2.86 & 3.83\\
        VDM \citep{Kingma2021VariationalDM} & 2.70 & --\\
        \hline
        Reflected Diffusion (ours) & \textbf{2.68} & \textbf{3.74}\\
    \end{tabular}
    \caption{\textbf{CIFAR-10 and ImageNet32 Bits-per-Dimension (BPD).} No data augmentaiton; lower is Better. We test the likelihood of Reflected Diffusion Models for CIFAR-10 and downsampled ImageNet32 without data augmentation. Our method is second best, nearly matching the state of the art (VDM), without requiring importance sampling or a learned noise schedule.}
    \label{tab:likelihoods}
    \vspace{-4mm}
\end{table}

Note that, by also incorporating the prior and reconstruction loss, Equation \ref{eqn:smgirs} gives us an upper bound on the negative log-likelihood (Appendix \ref{sec:app:theory:girs}). Furthermore, for our reversed RVE SDE, Equation \ref{eqn:smgirs} becomes

which is a scaled version of our proposed weighted score matching loss in Equation \ref{eqn:wcdsm}. Therefore, we already implicitly train with maximum likelihood. Furthermore, when applied to an individual data point , we recover the constrained denoising score matching loss:

which allows us to derive an upper bound on .

\textbf{Image Likelihood Results.} We test Reflected Diffusion Models on CIFAR-10 \citep{Krizhevsky2009LearningML} and ImageNet32 \citep{Oord2016PixelRN} for likelihoods, both without data augmentation. Our method performs comparatively to the SOTA while reducing the number of hyperparameters (in the form of importance sampling and learned noise schedules)\footnote{We omit several results which report a better BPD than VDMs \citep{Kingma2021VariationalDM} on Imagenet32 but a much worse CIFAR-10 result as they test on the the ImageNet32 dataset used for classification \citep{Chrabaszcz2017ADV}, which is significantly easier and incomparable due to the use of anti-aliasing.}. Note that we can compute exact likelihoods through the probability flow ODE, which typically improves results \citep{Song2021MaximumLT}, but, for a fair comparison with VDM, we report the likelihood bound.
 \section{Simplex Diffusion}

We also demonstrate that our reflected diffusion model can scale to high dimensional simplices. We train on softmaxed Inception classifier logits for ImageNet \citep{Szegedy2014GoingDW}, which take values in a -dimensional simplex. Our training dynamics are reported in Figure \ref{fig:simplexresults} (with a  EMA), showing that our method is able to optimize the loss (and thus maximize the ELBO) even in high dimensions. Our diffusion process is fundamentally different from the simplex diffusion method from \citet{Richemond2022CategoricalSW}, as we evolve our dynamics directly on the simplex while the previous method diffuses on a higher dimensional space (the positive orthant) and projects to the simplex.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{fig/simplex.png}
    \caption{\textbf{Simplex Diffusion Training and Validation Curves.} Our method trains stably in high dimensions.}
    \label{fig:simplexresults}
    \vspace{-5mm}
\end{figure}








%
 \section{Conclusion}

We introduced Reflected Diffusion Models, a diffusion model which respects natural data constraints through reflected SDEs. Our method scales score matching on general bounded geometries and retains theoretical constructs from standard diffusion models. Our analysis also sheds light on the commonly used thresholding sampling method and provides improvements through correct training.

We did not explore architecture or noise scheduling, which are critical for state of the art results; we leave this (and scaling to text to-image-generation) for future work.

Latent Diffusion (LD) \citep{Rombach2021HighResolutionIS} is a diffusion model method that also incidentally does not require thresholding. We hypothesize that this is because both our method and LD directly incorporate data space constraints. Notably, we work over an outer bound of the support of the data distribution, while LD works over a submanifold learned by a VAE. Future work could try to find a middle ground between these two data support approaches. \section{Acknowledgements}

This project was supported by NSF (\#1651565), ARO (W911NF-21-1-0125), ONR (N00014-23-1-2159), CZ Biohub, and Stanford HAI GCP Grants. AL is supported by a NSF Graduate Research Fellowship. We would also like to thank Chenlin Meng for helpful discussions.
 
\bibliography{refs}
\bibliographystyle{icml2023}


\newpage
\appendix
\onecolumn
\section{Theoretical Constructs}

\subsection{Reflected Stochastic Differential Equations}\label{sec:app:theory:rsde}

We follow \citet{Pilipenko2014AnIT} in our presentation. Given a domain  and an oblique reflection vector field  that satisfies , where  is the inward pointing unit normal vector field, the reflected SDE is defined as

where  is defined recursively as . Here, we see that  is a process that determines whether  hits the boundary and then applies a reflection. For our purposes in the main paper,  and we surppress the notation for compactness.
Define . When

then Equation \ref{eqn:neumfp} generalizes (under suitable regularity conditions) \citep{Schuss2013BrownianDA}:

As such, this induces a reverse process \citep{Williams1988OnTO, Cattiaux1988TimeRO} that one can easily check has the same marginal probability distributions:

Here  is a vector field that satisfies the condition  and  is a positive multiple of .

\subsection{Constrained Denoising Score Matching}\label{sec:app:theory:cdsm}

\begin{proposition}\label{prop:app:cdsm}
    Suppose that we perturb an -supported density  with noise  (also supported on ) to get a new density . Then, under suitable regularity conditions for the smoothness of  and , the score matching loss for :
    
    is equal (up to a constant factor that does not depend on ) to the CSDM loss:
    
\end{proposition}

\begin{proof}
    This proof comes down to showing that

    

    which can be done directly

    
\end{proof}

This proof is exactly the same as the one presented in \citep{Vincent2011ACB}. The only difference is that we replace the domain of integration with . Note that the key property that allows us to complete the proof is the convolution identity, which generalizes unlike Stokes' theorem for implicit score matching.

\subsection{Probability Flow ODE and Connections to DDIM}\label{sec:app:theory:ode}

We now derive the probability flow ODE, show how to use it to sample, and discuss connections with DDIM. For convenience, we will work with the assumptions given in the paper (that the diffusion coefficient is a scalar depending on only time and that reflection is in the normal direction), but our results directly generalize (given sufficient regularity conditions) to general noise schedules and oblique reflections.

\begin{proposition}[Probability Flow ODE]
    For the reflected SDE
    
    The ODE given by
    
    follows the same probability evolution .
\end{proposition}
\begin{proof}
    By the forward Kolmogorov Equation, we ca see that the ODE follows
    
    However, we must confirm that the ODE doesn't exit . By the Neumann boundary conditions for the SDE, we see that 
    
    on the boundary, so the flow induced by the ODE is indeed a valid diffeomorphism from .
\end{proof}
Similar to DDIM, we can derive equivalent processes by annealing the noise.
\begin{proposition}[Annealing Noise Level]
    For the reflected SDE
    
    The reflected SDE
    
    follows the same probability evoluation  for all noise levels .
\end{proposition}
\begin{proof}
    This follows directly from our Fokker-Planck Equation.
\end{proof}
\begin{remark}
    In the above proposition,  as there is no concept of a reflected ordinary differential equation. However, when the noise is , our limiting process yields an ODE.
\end{remark}
To sample with our score function , we simply solve the reversed process, which is

for our ODE and

for our annealed reflected SDE.

When training with our CSDM objective , the ODE sampler (Equation \ref{eqn:app:odesampler}) to mimic standard reflected diffusion sampling, which includes thresholding. Conversely, when the score is trained with standard score matching, the sampler just removes thresholding, causing the process to simulate the diffusion path without thresholding.

To mimic the thresholding effect, one must instead turn to the the annealed reflected SDE sampler of Equation \ref{eqn:app:ddimsampler}. If we discretize the equation and (with an abuse of notation) set , then we recover the thresholded DDIM sampler. Unfortunately, changing  necessarily causes the sampled distribution to shift since  is not trained to mimic the correct , so the reverse process necessarily results in divergent behavior.

\subsection{Girsanov Theorem for Reflected SDEs and Likelihood Evaluation}\label{sec:app:theory:girs}

We derive our likelihood bounds. We first recall Girsanov's Theorem for SDEs \citep{ksendal1987StochasticDE}

\begin{theorem}[Girsanov Theorem]\label{thm:app:girs}
    Let  be a bounded functional on the space of continuous functions . For the SDE evolving on  with

    

    we have

    

    where the expectation is taken is the path measure of the SDE.
\end{theorem}

We then prove the analogue of this for reflected SDEs:

\begin{theorem}[Girsanov Theorem for Reflected SDEs]\label{thm:app:girsrefl}
    Let  be a bounded functional on the space of continuous functions . For the reflected SDE evolving on  space and  time with

    

    where  is assumed to have normal reflection. We have

    

    where the expectation is taken over the path measure of the reflected SDE.
\end{theorem}

\begin{proof}
   We first smoothly extend  and  to all of  s.t. the value goes to  very quickly on . We then consider the processes  defined  by

    

    where  where  is the distance function and  is the unit normal vector pointing from  to  where . It is well known that  in measure as  \citep{Liu1993NumericalAT}. Since  is a bounded (and thus continuous) functional, we thus have  as . We finalize by noting that
    
    As ,  will remain in  w.p.  and  on . Therefore, we have the desired convergence
    
    as desired.
\end{proof}
\begin{corollary}
    As a corollary, when  is , this gives us Theorem \ref{thm:reflgirsanov}.
\end{corollary}

\begin{remark}
    It is possible that we can generalize our theorem to obliquely reflected SDEs, although we did not pursue this line of inquiry.
\end{remark}

\begin{remark}
    Theorem \ref{thm:reflgirsanov} recovers the denoising score matching loss if we slice an initial  distribution. In particular, this is the continuous time ``diffusion loss"  that is used to the form the ELBO for standard diffusion models  \citep{Kingma2021VariationalDM, Ho2020DenoisingDP}.
\end{remark}

\subsection{Thresholding}\label{sec:app:theory:thresh}

On , the dynamic thresholding operator is defined by

which of course can be scaled to  (for our setup).
\begin{proposition}[Static Thresholding Solves the Reflected SDE]
    On domains  between times , the discretization
    
    solves the reflected SDE
    
    as  when  and  are uniformly Lipschitz in time and space and satisfy the linear growth condition for any Lipschitz extension of  to the general space .
\end{proposition}

\begin{proof}
    This closely mirrors the proof showing that the standard projection scheme
    
    converges to the solution of the reflected SDE as  \citep{Skorokhod1961StochasticEF, Schuss2013BrownianDA, Liu1993NumericalAT}. The key difference is that the process is not supported on  since the projection happens after each. However, since our extension on  is Lipschitz, this error is well behaved and disappears as .
\end{proof}

\begin{corollary}[Dynamic Thresholding Solves the Reflected SDE]
    With the same conditions as given above, on , the discretization
    
    converges to the solution of the reflected SDE when  does not point outside of  on  dimensions.
\end{corollary}
\begin{proof}
    Under our conditions,  becomes the projection operator since the -th percentile of  will always be . This replicates the above proposition.
\end{proof}
\begin{remark}
    In practice, we found that learned score networks  satisfy the ``pointing inside" condition above. In particular, as ,  tends to behave exactly like  for all .
\end{remark} \section{Practical Implementation}

\subsection{Exact Equations for Reflected Brownian Transition Probabilities}

For , the reflected transition probability for a source  with diffusion value  (which correspond to the mean and standard deviation for the standard normal distribution) is given by

Note that this means that the eigenfunctions of  under our Neumann boundary condition are  and , with eigenvalues of  and 

\subsection{Mapping  to }\label{sec:app:prac:diffmap}

Our domain  has an interior which maps bijectively to  iff  is simply connected. Note that this encompasses a wide variety of domains, notably convex sets.

To construct a map , we use a variant of the common stick breaking procedure:



which admits an inverse


\subsection{Denoising The Final Proability Distribution} \label{sec:app:prac:denoise}

We note that Tweedies' formula \citep{Efron2011TweediesFA} does not hold for general bounded domains . We show this for : given an initial distribution , a perturbed distribution  constructed by , where  has a Tweedie denoiser:

The reason why this works in the standard case is because the score of the Gaussian distribution is , which allows us to extract out the desired value. For reflected Gaussians, this does not hold.

Instead, for our experimental results on CIFAR-10, we denoise by training a denoising autoencoder \citep{Vincent2008ExtractingAC} trained on our reflected Gaussian noise. This follows the same architecture as our score network, and is trained to predict the noise (so that subtracting it recovers the initial sample). In general, this is required to get a decent FID score, but makes little difference in terms of perceptual quality as our images are accurate to within a  standard deviation noise to begin with. \section{Experimental Setup}

\subsection{Image Generation}\label{sec:app:experiment:imgqual}

We exactly follow \citet{Song2020ScoreBasedGM} for both models and training hyperparameters. The only differences are that we set  instead of  (for the VE SDE) since we mix well with  while VE SDE needs a much larger  to mix. Furthermore, we use the deep DDPM++ architecture, but we rescale the output  as is done for the NCSN++ architecture (for VE SDE).

For sampling, we sample with  predictor (Reflected Euler-Maruyama) steps with  corrector (Reflected Langevin) steps \citep{Song2020ScoreBasedGM}. We use a signal-to-noise ratio of .

\subsection{Image Likelihood}\label{sec:app:experiment:imglike}

We almost exactly follow \citet{Kingma2021VariationalDM} for both models and training hyperparameters, replacing the standard diffusion with our reflected diffusion. We do not train with the noise schedule, instead setting  and , which causes the reconstruction and prior losses to be (numerically) .

\subsection{Guided Diffusion}\label{sec:app:experiment:guided}

We exactly follow \citet{Ho2022ClassifierFreeDG} and train using the ADM architecture \citep{Dhariwal2021DiffusionMB} with the same parameters (for standard diffusion). For reflected diffusion, we train with  and , following our CIFAR-10 experiments. Furthermore, we scale the output by  as the neural network outputs the noise vector and not the score.

For the classifier-free guidance baseline, we retrain a ImageNet64 model following \citet{Ho2022ClassifierFreeDG}. For the classifier guided basleine, we use the pretrained models from \citet{Dhariwal2021DiffusionMB}.

We sample using  steps each. For our diffusion model, we use reflected Euler Maruyama. For the standard model, we use a standard Euler-Maruyama with thresholding after each step. For ODE sampling, we sample using a RK45 solver \citep{Dormand1980AFO}.

\subsection{Simplex Diffusion}\label{sec:app:experiment:simplex}

We consider class probabilities outputted from the Inceptionv3 ImageNet classifier \citep{Szegedy2015RethinkingTI}. In particular, if  is a set of images and  is a classifier that outputs a -dimensional vector of class probabilities, then we learn a distribution over . For learning purposes, we clip this to a value in  and apply the transformation given in Appendix \ref{sec:app:prac:diffmap}.

Our model is a simple MLP autoencoder with  intermediate layers of width . We use the Swish activation \citep{Ramachandran2017SwishAS} and apply LayerNorm \citep{Ba2016LayerN}. We train with Adam \citep{Kingma2014AdamAM} at a  learning rate. We apply an exponential moving average with a rate of  before evaluating/generating our data. We visualize our full training and eval graphs below, as well as some samples taken from our model. Overall, we seem to be able to match the distribution reasonably well.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{fig/simplex_train.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{fig/simplex_eval.png}
    \end{subfigure}
    \caption{Training Dynamics for Simplex Diffusion}
\end{figure}
To ensure that we are able to generate data, we generated 10000 and compare the generated histograms of the (most likely) classes. Results are shown below:
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{fig/simplex_gt_hist.png}
        \caption{Ground Truth}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{fig/simplex_gen_hist.png}
        \caption{Generated}
    \end{subfigure}
    \caption{Generated Simplex Probabilities for Simplex Diffusion}
\end{figure}

 \newpage

\section{Additional Generated Images}\label{sec:app:fig}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{fig/cifar10.png}
    \caption{CIFAR-10 Generated Images.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{fig/imagenet64_w1.png}
    \caption{ImageNet64  classifier-free guided samples.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{fig/imagenet64_w2_5.png}
    \caption{ImageNet64  classifier-free guided samples.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{fig/cifar10_nll.png}
    \caption{CIFAR-10 Generated Images (trained for BPD).}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{fig/imagenet.png}
    \caption{ImageNet32 Generated Images (trained for BPD).}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{fig/classifier-guidance-noclip.png}
    \caption{ baseline classifier guided images without thresholding. We sample from the pretrained model from \citet{Dhariwal2021DiffusionMB}. Around \% of samples diverge, while most of the rest have noticeable artifacts such as a glaring white background.}\label{fig:cl_guidance_noclip}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{fig/cg_ddim.png}
    \caption{ baseline classifier guided images without thresholding and DDIM sampling. Interestingly, these samples don't diverge.}\label{fig:cg_ddim}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{fig/ode_cf_baseline.png}
    \caption{ baseline classifier-free guided images sampled with DDIM without thresholding. This corresponds to ODE sampling.}
    \label{fig:app:ode_cf_baseline}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{fig/dyn_thresh.png}
    \caption{Dynamically thresholded images, matches Figure \ref{fig:diffguide}.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{fig/ode_classifier-free_0_5.png}
    \caption{ ODE samples, Reflected Diffusion.}
    \label{fig:app:0.5ode}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/ode_cf_nfe.png}
    \caption{ODE number of forward evaluations (NFE) vs guidance weight for Reflected Diffusion. Increasing the guidance weight tends to increase the number of forward evaluations, but this is still relatively low.}
    \label{fig:app:ode_nfe}
\end{figure} 


\end{document}
