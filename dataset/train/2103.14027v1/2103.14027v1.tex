\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{etoolbox}
\usepackage{times}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{tabularx,booktabs}
\usepackage[font=small,labelsep=period,skip=2pt]{caption}
\usepackage{pifont}
\usepackage{xurl}
\usepackage[percent]{overpic}
\usepackage[subrefformat=parens]{subcaption}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{4820} \def\confYear{CVPR 2021}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


\newcommand{\argmaxunder}[1]{\underset{#1}{\operatorname{arg\,max}}}
\newcommand{\argminunder}[1]{\underset{#1}{\operatorname{arg\,min}}}
\newcommand{\argmax}{\mathop{\mathrm{arg\,max}}}
\newcommand{\argmin}{\mathop{\mathrm{arg\,min}}}

\newcommand{\Univ}{UniverseNet\xspace}
\newcommand{\Univs}{UniverseNets\xspace}
\newcommand{\OurOrig}{UniverseNet\xspace}
\newcommand{\OurOrigAbbr}{Univ\xspace}
\newcommand{\OurGFL}{UniverseNetGFL\xspace}
\newcommand{\OurGFLlite}{UniverseNetGFL lite\xspace}
\newcommand{\OurAugustD}{UniverseNet-20.08d\xspace}
\newcommand{\OurAugust}{UniverseNet-20.08\xspace}
\newcommand{\OurAugustAbbr}{Univ20.08\xspace}
\newcommand{\OurAugustS}{UniverseNet-20.08s\xspace}

\ificcvfinal\newcommand{\UnivRepo}{\url{https://github.com/shinya7y/UniverseNet}\xspace}
\else\newcommand{\UnivRepo}{our GitHub repository and in the Supplementary Material\xspace}
\fi
\ificcvfinal\newcommand{\WaymoCOCO}{\url{https://github.com/shinya7y/WaymoCOCO}\xspace}
\else\newcommand{\WaymoCOCO}{Our GitHub repository (anonymized for review).\xspace}
\fi
\ificcvfinal\newcommand{\mangaapi}{\url{https://github.com/shinya7y/manga109api}\xspace}
\else\newcommand{\mangaapi}{Our GitHub repository (anonymized for review).\xspace}
\fi
\ificcvfinal\newcommand{\nightowlstalks}{\cite{NightOwls_talks_CVPRW2020}\xspace}
\else\newcommand{\nightowlstalks}{\cite{NightOwls_talks_CVPRW2020_anonymize}\xspace}
\fi

\newcommand{\SEPCa}{SEPCa\xspace}
\newcommand{\ATSEPC}{ATSEPC\xspace}

\def\AppendixSection{Supplementary Material\xspace}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\mell}{m_l}
\newcommand{\mellhat}{m_l^{\#}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Eqref}[1]{Eq. \eqref{#1}}
\newcommand{\Tr}{\mathrm{Tr}}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\cm}{\ding{51}}\newcommand{\xm}{\ding{55}}

\newcommand{\APS}{AP\xspace}
\newcommand{\APM}{AP\xspace}
\newcommand{\APL}{AP\xspace}

\newcommand{\TB}{\textbf}
\newcommand{\K}{\,k\xspace}
\newcommand{\M}{\,M\xspace}
\newcommand{\Hz}{\,Hz\xspace}

\newcommand{\Pascal}{\textsc{Pascal}\xspace}
\newcommand{\Mangas}{Manga109-s\xspace}
\newcommand{\MangasAbbr}{M109s\xspace}
 
\AtBeginDocument{
	\abovedisplayskip     =0.3\abovedisplayskip
	\abovedisplayshortskip=0.3\abovedisplayshortskip
	\belowdisplayskip     =0.3\belowdisplayskip
	\belowdisplayshortskip=0.3\belowdisplayshortskip
}

\begin{document}





\title{USB: Universal-Scale Object Detection Benchmark}

\author{Yosuke Shinya\thanks{This work was done independently of the author's employer.}
}

\makeatletter
\let\@oldmaketitle\@maketitle \renewcommand{\@maketitle}{\@oldmaketitle \vspace{-5mm}
	\begin{center}
		\begin{minipage}[b]{0.32\hsize}
			\centering
			\includegraphics[width=\textwidth,height=0.707376\textwidth]{images/coco_keybowl.jpg}
			\label{fig:teaser_a}
		\end{minipage}
		\hfill
		\begin{minipage}[b]{0.339540508\hsize}
			\centering
			\includegraphics[width=\textwidth]{images/00079_00120_camera1_.jpg}
			\label{fig:teaser_b}
		\end{minipage}
		\hfill
		\begin{minipage}[b]{0.32\hsize}
			\centering
			\begin{overpic}[width=\textwidth]{images/HinagikuKenzan_026.jpg}
				\put( 30, -1.5){\scriptsize{Hinagiku Kenzan\textit{!} \copyright Minene Sakurano}}
			\end{overpic}
			\label{fig:teaser_c}
		\end{minipage}
		\vspace{-3mm}
		\captionof{figure}{
			Universal-scale object detection.
			For realizing human-level perception,
			object detection systems must detect both tiny and large objects,
			even if they are out of natural image domains.
			To this end,
			we introduce the \textit{Universal-Scale object detection Benchmark (USB)} that
			consists of the COCO dataset (left), Waymo Open Dataset (middle), and \Mangas dataset (right).
		}
		\label{fig:teaser}
	\end{center}
	\bigskip}
\makeatother

\maketitle




\begin{abstract}
Benchmarks, such as COCO, play a crucial role in object detection.
However, existing benchmarks are insufficient in scale variation, and their protocols are inadequate for fair comparison.
In this paper, we introduce the \TB{Universal-Scale object detection Benchmark (USB)}.
USB has variations in object scales and image domains
by incorporating COCO with the recently proposed Waymo Open Dataset and Manga109-s dataset.
To enable fair comparison, we propose \TB{USB protocols}
by defining multiple thresholds for training epochs and evaluation image resolutions.
By analyzing methods on the proposed benchmark,
we designed fast and accurate object detectors called \TB{UniverseNets},
which surpassed all baselines on USB and achieved state-of-the-art results on existing benchmarks.
Specifically, UniverseNets achieved 54.1\% AP on COCO \texttt{test-dev} with 20 epochs training,
the top result among single-stage detectors on the Waymo Open Dataset Challenge 2020 2D detection,
and the first place in the NightOwls Detection Challenge 2020 all objects track.
The code is available at \UnivRepo.
\end{abstract}







\section{Introduction}
\label{sec:introduction}



\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{images/coco_ap_time_epoch_thinline.pdf}
	\caption{
		Speed-accuracy trade-offs in the current standard COCO benchmark.
		Most works train models with standard settings (\eg, within 24 epochs),
		while some works train with abnormal settings (\eg, 300 epochs).
		To enable fair comparison,
		we propose \textit{USB protocols} that urge the latter works to report results with standard settings.
		Additionally, we design \textit{\Univs}
		that achieve state-of-the-art results with standard settings.
		We show some detection examples of \OurAugust in Figure~\ref{fig:teaser}.
	}
	\label{fig:coco_speed_accuracy}
	\vspace{-2mm}
\end{figure}



Humans can detect various objects. See Figure~\ref{fig:teaser}.
One can detect
close equipment in everyday scenes,
far vehicles in traffic scenes,
and texts and persons in manga (Japanese comics).
If computers can automatically detect various objects,
they will yield significant benefits to humans.
For example, they will
help impaired people and the elderly,
save lives by autonomous driving,
and provide safe entertainment during pandemics by automatic translation.


Researchers have pushed the limits of object detection systems
by establishing datasets and benchmarks~\cite{object_detection_survey_Liu_IJCV2020}.
One of the most important milestones is \Pascal VOC~\cite{PASCALVOC_IJCV2015}.
It has enabled considerable research on object detection,
leading to the success of deep learning-based methods and successor datasets such as ImageNet~\cite{ImageNet_IJCV2015} and COCO~\cite{COCO_ECCV2014}.
Currently, COCO serves as \textit{the} standard dataset and benchmark for object detection
because it has several advantages over \Pascal VOC~\cite{PASCALVOC_IJCV2015}.
COCO contains more images, categories, and objects (especially small objects) in their natural context~\cite{COCO_ECCV2014}.
Using COCO, researchers can develop and evaluate methods for multi-scale object detection.
However, the current object detection benchmarks, especially COCO, have the following three problems.

\textbf{Problem 1: Variations in object scales and image domains remain limited.}
To realize human-level perception,
computers must handle various object scales and image domains as humans can.
Among various domains~\cite{UniversalObjectDetection_CVPR2019},
the traffic and artificial domains have extensive scale variations (see Sec.~\ref{sec:usb}).
COCO is far from covering them.
Nevertheless, the current computer vision community is overconfident in COCO results.
For example, most studies on state-of-the-art methods in 2020
only report COCO results~\cite{ATSS_CVPR2020, SEPC_CVPR2020, PAA_ECCV2020, GFL_NeurIPS2020, RepPointsv2_NeurIPS2020, RelationNet2_NeurIPS2020} or
those for bounding box object detection~\cite{EfficientDet_CVPR2020, SpineNet_CVPR2020, YOLOv4_2020, DetectoRS_2020}.
Readers cannot assess whether these methods are specialized for COCO or generalizable to other datasets and domains.





\textbf{Problem 2: Protocols for training and evaluation are not well established.}
There are standard experimental settings for the COCO benchmark~\cite{Detectron2018, MMDetection, FPN_CVPR2017, RetinaNet_ICCV2017, FCOS_ICCV2019, ATSS_CVPR2020, GFL_NeurIPS2020}.
Many works train detectors within 24 epochs
using a learning rate of 0.01 or 0.02
and evaluate them on images within 1333800.
These settings are not obligations but non-binding agreements for fair comparison.
Some works do not follow the standard settings for accurate and fast detectors\footnote{YOLOv4 was trained for 273 epochs~\cite{YOLOv4_2020},
DETR for 500 epochs~\cite{DETR_ECCV2020},
EfficientDet-D6 for 300 epochs~\cite{EfficientDet_CVPR2020},
and EfficientDet-D7x for 600 epochs~\cite{EfficientDet_arXiv}.
SpineNet uses a learning rate of 0.28~\cite{SpineNet_CVPR2020},
and YOLOv4 uses a searched learning rate of 0.00261~\cite{YOLOv4_2020}.
EfficientDet finely changes the image resolution from 512512 to 15361536~\cite{EfficientDet_CVPR2020}.}.
Their abnormal and scattered settings hinder the assessment of the most suitable method
(see Figure~\ref{fig:coco_speed_accuracy}).
Furthermore,
by ``buying stronger results''~\cite{GreenAI_CACM2020},
they build a barrier for those without considerable funds to develop and train detectors.






\textbf{Problem 3: The analysis of methods for multi-scale object detection is insufficient.}
Numerous studies have proposed methods for multi-scale object detection~\cite{object_detection_survey_Liu_IJCV2020, Faster_R-CNN_NIPS2015, SSD_ECCV2016, FPN_CVPR2017, DCN_ICCV2017}.
In recent years, improvements for network components have made significant progress in COCO
(\eg, Res2Net~\cite{Res2Net_TPAMI2020} for the backbone, SEPC~\cite{SEPC_CVPR2020} for the neck, and ATSS~\cite{ATSS_CVPR2020} for the head).
These works have an insufficient analysis of
combinability, effectiveness, and characteristics, especially on datasets other than COCO.

This study makes the following three contributions to resolve the problems.

\textbf{Contribution 1:}
We introduce the \textit{Universal-Scale object detection Benchmark (USB)} that consists of three datasets.
In addition to COCO, we selected the Waymo Open Dataset~\cite{WaymoOpenDataset_CVPR2020} and \Mangas~\cite{Manga109_Matsui_MTAP2017, Manga109_Aizawa_IEEEMM2020} to cover various object scales and image domains.
They are the largest public datasets in their domains and enable reliable comparisons.
We conducted experiments using eight methods and found weaknesses of existing COCO-biased methods.

\textbf{Contribution 2:}
We established the \textit{USB protocols} for fair training and evaluation
for more inclusive object detection research.
USB protocols enable fair, easy, and scalable comparisons
by defining multiple thresholds for training epochs and evaluation image resolutions.

\textbf{Contribution 3:}
We designed fast and accurate object detectors called \textit{\Univs}
by analyzing methods developed for multi-scale object detection.
\Univs outperformed all baselines on USB
and achieved state-of-the-art results on existing benchmarks.
In particular,
our finding on USB enables a 9.3 points higher score than YOLOv4~\cite{YOLOv4_2020}
on the Waymo Open Dataset Challenge 2020 2D detection.




\section{Related Work}
\label{sec:related_work}



\subsection{Object Detection Methods}

Deep learning-based detectors dominate the recent progress in object detection~\cite{object_detection_survey_Liu_IJCV2020}.
They can be divided as~\cite{object_detection_survey_Liu_IJCV2020, MMDetection, Faster_R-CNN_NIPS2015}
single-stage detectors without region proposal~\cite{YOLO_CVPR2016, SSD_ECCV2016, RetinaNet_ICCV2017} and
multi-stage (including two-stage) detectors with region proposal~\cite{Faster_R-CNN_NIPS2015, FPN_CVPR2017, Cascade_R-CNN_CVPR2018}.
Our \Univs are single-stage detectors for efficiency~\cite{GFL_NeurIPS2020, EfficientDet_CVPR2020, YOLOv4_2020, SpeedAccuracyTradeOffs_CVPR2017}.

Detecting multi-scale objects is a fundamental challenge in object detection~\cite{object_detection_survey_Liu_IJCV2020, UniversalObjectDetection_ZhaoweiCai_2019}.
Various components have been improved, including
backbones and modules~\cite{Inception_CVPR2015, ResNet_CVPR2016, BagOfTricks_Classification_CVPR2019, Res2Net_TPAMI2020, DCN_ICCV2017},
necks~\cite{FPN_CVPR2017, SEPC_CVPR2020, EfficientDet_CVPR2020},
heads and training sample selection~\cite{Faster_R-CNN_NIPS2015, SSD_ECCV2016, ATSS_CVPR2020}, and
multi-scale training and testing~\cite{Rowley_PAMI1998, SNIP_Singh_CVPR2018, ATSS_CVPR2020}
(see \AppendixSection for details).


Some recent or concurrent works~\cite{SM-NAS_AAAI2020, YOLOv4_2020, Waymo2d_1st_2020} have combined multiple methods.
Unlike these works, we analyzed various components developed for scale variations,
without computation for neural architecture search~\cite{SM-NAS_AAAI2020}, long training (273 epochs)~\cite{YOLOv4_2020}, and multi-stage detectors~\cite{Waymo2d_1st_2020}.


Most prior studies have evaluated detectors on limited image domains.
We demonstrate the superior performance of our \Univs
across various object scales and image domains through the proposed benchmark.




\subsection{Object Detection Benchmarks}





There are numerous object detection benchmarks.
For specific (category) object detection,
recent benchmarks such as WIDER FACE~\cite{WIDERFACE_CVPR2016} and TinyPerson~\cite{TinyPerson_WACV2020} contain tiny objects.
Although they are useful for evaluation for a specific category,
many applications should detect multiple categories.
For autonomous driving,
KITTI~\cite{KITTI_CVPR2012} and Waymo Open Dataset~\cite{WaymoOpenDataset_CVPR2020}
mainly evaluate three categories (car, pedestrian, and cyclist) in their leaderboards.
For generic object detection, \Pascal VOC~\cite{PASCALVOC_IJCV2015} and COCO~\cite{COCO_ECCV2014} include 20 and 80 categories, respectively.
The number of categories has been further expanded by recent benchmarks, such as Open Images~\cite{OpenImagesDataset_IJCV2020}, Objects365~\cite{Objects365_ICCV2019}, and LVIS~\cite{LVIS_CVPR2019}.
All the above datasets comprise photographs,
whereas Clipart1k, Watercolor2k, Comic2k~\cite{CrossDomainDetection_Inoue_CVPR2018}, and \Mangas~\cite{Manga109_Matsui_MTAP2017, Manga109_Aizawa_IEEEMM2020}
comprise artificial images.

Detectors evaluated on a specific dataset may perform worse on other datasets or domains.
To address this issue, some benchmarks consist of multiple datasets.
In the Robust Vision Challenge 2020~\cite{RVC2020}, detectors were evaluated on three datasets in the natural and traffic image domains.
For \textit{universal-domain} object detection, the Universal Object Detection Benchmark (UODB)~\cite{UniversalObjectDetection_CVPR2019} comprises 11 datasets in the natural, traffic, aerial, medical, and artificial image domains.
Although it is suitable for evaluating detectors in various domains, variations in object scales are limited.
Unlike UODB, our USB focuses on \textit{universal-scale} object detection.
Datasets in USB contain more instances, including tiny objects, than the datasets used in UODB.

As discussed in Sec.~\ref{sec:introduction},
the current benchmarks allow extremely unfair settings (\eg, 25 training epochs).
We resolved this problem by establishing USB protocols for fair training and evaluation.










\section{Benchmark Protocols of USB}
\label{sec:usb}

Here, we present the principle, datasets, protocols, and metrics of USB.
See \AppendixSection for additional information.


\subsection{Principle}

We focus on the \textit{Universal-Scale Object Detection (USOD)} task
that aims to detect various objects in terms of object scales and image domains.
Unlike separate discussions for multi-scale object detection (Sec.~\ref{sec:related_work}) and universal (-domain) object detection~\cite{UniversalObjectDetection_CVPR2019},
USOD does not ignore the relation between scales and domains (Sec.~\ref{sec:usb_datasets}).


For various applications and users, benchmark protocols should cover 
from short to long training and from small to large test scales.
On the other hand, they should not be scattered for meaningful benchmarks.
To satisfy the conflicting requirements, we define multiple thresholds for training epochs and evaluation image resolutions.
Furthermore, we urge rich participants to report results with standard training settings.
This request enables fair comparison and allows many people to develop and compare object detectors.


\subsection{Datasets}
\label{sec:usb_datasets}

\begin{figure}[t]
	\centering
	\begin{minipage}[c]{0.41\linewidth}
		\includegraphics[width=\linewidth]{images/instance_scale_distribution_kde_cumulative_thinline.pdf}
	\end{minipage}\hfill
	\begin{minipage}[c]{0.575\linewidth}
		\caption{
			Object scale distributions.
			USB covers extensive scale variations quantitatively.
			The relative scale is the square root of the ratio of bounding box area to image area~\cite{TinyPerson_WACV2020, SNIP_Singh_CVPR2018, Waymo2d_1st_2020}.
		}
		\label{fig:instance_scale_distribution}
	\end{minipage}
\end{figure}

\begin{table}[t]
	\setlength{\tabcolsep}{1.4mm}
	\renewcommand\arraystretch{0.85}
	\begin{center}
		\scalebox{0.75}{\begin{tabular}{llll}
			\toprule
			Dataset                                                             & Domain     & Color         & Main sources of scale variation \\ \midrule
			COCO~\cite{COCO_ECCV2014}                                           & Natural    & RGB           & Categories, distance            \\
			WOD~\cite{WaymoOpenDataset_CVPR2020}                                & Traffic    & RGB           & Distance                        \\
			\Mangas~\cite{Manga109_Matsui_MTAP2017, Manga109_Aizawa_IEEEMM2020} & Artificial & Grayscale & Viewpoints, page layouts        \\ \bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-3mm}
	\caption{
		Characteristics of datasets.
		USB covers many qualitative variations related to scales and domains.
		: Few RGB images.
	}
	\label{table:usb_characteristics}
\end{table}

\begin{table}[t]
	\setlength{\tabcolsep}{0.97mm}
	\renewcommand\arraystretch{0.85}
	\begin{center}
		\scalebox{0.75}{\begin{tabular}{lllcccc}
			\toprule
			Benchmark                                                      & Dataset                                                             & Domain     & Classes &   Boxes    &   Images   &    B/I    \\ \midrule
			\multirow{3}{*}{USB (Ours)}                                    & COCO~\cite{COCO_ECCV2014}                                           & Natural    &   80    & \TB{897\K} & \TB{123\K} & \TB{7.3}  \\
			                                                               & WOD~\cite{WaymoOpenDataset_CVPR2020} v1.2 \texttt{f0}               & Traffic    &    3    & \TB{1.0\M} & \TB{100\K} & \TB{10.0} \\
			                                                               & \Mangas~\cite{Manga109_Matsui_MTAP2017, Manga109_Aizawa_IEEEMM2020} & Artificial &    4    & \TB{401\K} & \TB{8.5\K} & \TB{47.0} \\ \midrule
			\multirow{3}{*}{UODB~\cite{UniversalObjectDetection_CVPR2019}} & COCO~\cite{COCO_ECCV2014} \texttt{val2014}                          & Natural    &   80    &   292\K    &    41\K    &    7.2    \\
			                                                               & KITTI~\cite{KITTI_CVPR2012}                                         & Traffic    &    3    &    35\K    &   7.5\K    &    4.7    \\
			                                                               & Comic2k~\cite{CrossDomainDetection_Inoue_CVPR2018}                  & Artificial &    6    &   6.4\K    &   2.0\K    &    3.2    \\ \bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-3mm}
	\caption{
		Statistics of datasets in USB
		and counterpart datasets in UODB~\cite{UniversalObjectDetection_CVPR2019}.
		Values are based on publicly available annotations.
		B/I: Average number of boxes per image.
	}
	\label{table:dataset_stats}
\end{table}




To establish USB, we selected the COCO~\cite{COCO_ECCV2014}, Waymo Open Dataset (WOD)~\cite{WaymoOpenDataset_CVPR2020}, and \Mangas (\MangasAbbr)~\cite{Manga109_Matsui_MTAP2017, Manga109_Aizawa_IEEEMM2020}.
WOD and \MangasAbbr are the largest public datasets with many small objects in the traffic and artificial domains, respectively.
Object scales in these domains vary significantly with distance and viewpoints, unlike those in the medical and aerial domains.
The USB covers extensive scale variations quantitatively (Figure~\ref{fig:instance_scale_distribution}) and qualitatively (Table~\ref{table:usb_characteristics}).
As shown in Table~\ref{table:dataset_stats},
these three datasets in USB contain more instances
than their counterpart datasets in UODB~\cite{UniversalObjectDetection_CVPR2019}
(COCO~\cite{COCO_ECCV2014} \texttt{val2014} subset, KITTI~\cite{KITTI_CVPR2012}, and Comic2k~\cite{CrossDomainDetection_Inoue_CVPR2018}).
USOD needs to evaluate detectors on datasets with many instances
because more instances enable more reliable comparisons of scale-wise metrics.


For the first dataset, we adopted the COCO dataset~\cite{COCO_ECCV2014}.
COCO contains natural images of everyday scenes collected from the Internet.
Annotations for 80 categories are used in the benchmark.
As shown in Figure~\ref{fig:teaser} (left),
object scales mainly depend on the categories and distance.
Although COCO contains objects smaller than those of \Pascal VOC~\cite{PASCALVOC_IJCV2015},
objects in everyday scenes (especially indoor scenes) are relatively large.
Since COCO is the current standard dataset for multi-scale object detection,
we adopted the same training split \texttt{train2017} (also known as \texttt{trainval35k}) as the COCO benchmark
to eliminate the need for retraining across benchmarks.
We adopted the \texttt{val2017} split (also known as \texttt{minival}) as the test set.


For the second dataset, we adopted the WOD,
which is a large-scale, diverse dataset for autonomous driving~\cite{WaymoOpenDataset_CVPR2020}
with many annotations for tiny objects (Figure~\ref{fig:instance_scale_distribution}).
The images were recorded using five high-resolution cameras mounted on vehicles.
As shown in Figure~\ref{fig:teaser} (middle), object scales vary mainly with distance.
The full data splits of the WOD are too large for benchmarking methods.
Thus, we extracted 10\% size subsets from the predefined training split (798 sequences) and validation split (202 sequences)~\cite{WaymoOpenDataset_CVPR2020}.
Specifically,
we extracted splits based on the ones place of the frame index (frames 0, 10, ..., 190) in each sequence.
We call the subsets \texttt{f0train} and \texttt{f0val} splits.
Each sequence in the splits contains 20 frames (20\,s, 1\Hz),
and each frame contains five images for five cameras.
We used three categories (vehicle, pedestrian, and cyclist) following the official \textit{ALL\_NS} setting~\cite{WaymoOpenDataset_2D_detection_leaderboard} used in WOD competitions.


For the third dataset, we adopt the \MangasAbbr~\cite{Manga109_Matsui_MTAP2017, Manga109_Aizawa_IEEEMM2020}.
\MangasAbbr contains artificial images of manga (Japanese comics) and annotations for four categories (body, face, frame, and text).
Many characteristics differ from those of natural images.
Most images are grayscale.
The objects are highly overlapped~\cite{Manga109_detection_Ogawa_2018}.
As shown in Figure~\ref{fig:teaser} (right), object scales vary unrestrictedly with viewpoints and page layouts.
Small objects differ greatly from downsampled versions of large objects
because small objects are drawn with simple lines and points.
For example, small faces look like a sign ().
This characteristic may ruin techniques developed mainly for natural images.
Another challenge is ambiguity in annotations.
Sometimes, a small-scale object is annotated, and sometimes, a similar scale object on another page is not annotated.
Since annotating small objects is difficult and labor-intensive, this is an important and practical challenge.
We carefully selected 68, 4, and 15 volumes for training, validation, and testing splits,
and we call them the \texttt{68train}, \texttt{4val}, and \texttt{15test}, respectively.

We selected the test splits from images with publicly available annotations
to reduce the labor required for submissions.
Participants should not fine-tune hyperparameters based on the test splits
to prevent overfitting.



\subsection{Training Protocols}
\label{sec:usb_training}


\begin{table}[t]
	\setlength{\tabcolsep}{1.05mm}
	\renewcommand\arraystretch{0.85}
	\begin{center}
		\scalebox{0.8}{\begin{tabular}{lrcll}
			\toprule
			Protocol  & Max epoch & AHPO & Compatibility     & Example                                                              \\ \midrule
			USB 1.0   &        24 & \xm  & ---               & 2 schedule~\cite{Detectron2018, RethinkingImageNet_ICCV2019} \\
			USB 2.0   &        73 & \xm  & USB 1.0           & 6 schedule~\cite{RethinkingImageNet_ICCV2019}                \\
			USB 3.0   &       300 & \xm  & USB 1.0, 2.0      & EfficientDet-D6~\cite{EfficientDet_CVPR2020}                         \\
			USB 3.1   &       300 & \cm  & USB 1.0, 2.0, 3.0 & YOLOv4~\cite{YOLOv4_2020}                                            \\
			Freestyle &   & \cm  & ---               & EfficientDet-D7x~\cite{EfficientDet_arXiv}                           \\ \bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-3mm}
	\caption{
		USB training protocols.
		For models trained with masks, 0.5 is added.
		AHPO: Aggressive hyperparameter optimization.
	}
	\label{table:USB_training}
\end{table}


For fair training, we propose the \textit{USB training protocols} shown in Table~\ref{table:USB_training}.
By analogy with the backward compatibility of the Universal Serial Bus,
USB training protocols emphasize compatibility between protocols.
Importantly,
\textit{participants should report results with not only higher protocols but also lower protocols.}
For example, when a participant trains a model for 150 epochs with standard hyperparameters,
it corresponds to USB 3.0.
The participant should also report the results of models trained for 24 and 73 epochs in a paper.
The readers of the paper can judge whether the method is useful for standard training epochs.

The number of maximum epochs for USB 1.0 is 24, which is the most popular setting in COCO (see Table~\ref{table:coco_sota}).
We adopted 73 epochs for USB 2.0,
where models trained from scratch can catch up with those trained for 24 epochs from ImageNet pre-trained models~\cite{RethinkingImageNet_ICCV2019}.
We adopted 300 epochs for USB 3.x such that 
YOLOv4~\cite{YOLOv4_2020} and most EfficientDet models~\cite{EfficientDet_arXiv} correspond to this protocol.
Models trained for more than 300 epochs are regarded as Freestyle.
They are not suitable for benchmarking methods,
although they may push the empirical limits of detectors~\cite{EfficientDet_arXiv, DETR_ECCV2020}.

For models trained with mask annotations, 0.5 is added to their number of protocols.
Results without mask annotations should be reported if possible for their algorithms.

For ease of comparison, we limit the pre-training datasets to the three and ImageNet (ILSVRC 1,000-class classification).
Other datasets are welcome only when the results with and without additional datasets are reported.
Participants should describe how to use the datasets.
A possible way is to fine-tune the models on WOD and \MangasAbbr from COCO pre-trained models.
Another way is to train a single model jointly~\cite{UniversalObjectDetection_CVPR2019} on the three datasets.

In addition to long training schedules, hyperparameter optimization is resource-intensive.
If authors of a paper fine-tune hyperparameters for their architecture,
other people without sufficient computational resources cannot compare methods fairly.
We recommend roughly tuning the minimum hyperparameters, such as batch sizes and learning rates
(\eg, from choices , , and ).
When participants optimize hyperparameters aggressively by manual fine-tuning or automatic algorithms,
they should report both results with and without aggressive optimization.




\subsection{Evaluation Protocols}
\label{sec:usb_evaluation}


\begin{table}[t]
	\setlength{\tabcolsep}{0.95mm}
	\renewcommand\arraystretch{0.85}
	\begin{center}
		\scalebox{0.8}{\begin{tabular}{lrrl}
			\toprule
			Protocol     & Max reso. &       Typical scale & Reference                                                        \\ \midrule
			Standard USB & 1,066,667 & 1333\;\:800 & Popular in COCO~\cite{COCO_ECCV2014, MMDetection, Detectron2018} \\
			Mini USB     &   262,144 &  512\;\:512 & Popular in VOC~\cite{PASCALVOC_IJCV2015, SSD_ECCV2016}           \\
			Micro USB    &    50,176 &  224\;\:224 & Popular in ImageNet~\cite{ImageNet_IJCV2015, ResNet_CVPR2016}    \\
			Large USB    & 2,457,600 &    19201280 & WOD front cameras~\cite{WaymoOpenDataset_CVPR2020}               \\
			Huge USB     & 7,526,400 &    33602240 & WOD top methods (\cite{Waymo2d_1st_2020}, ours)                  \\
			Freestyle    &   &   ---\:\;\;\;\;\;\; & ---                                                              \\ \bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-3mm}
	\caption{
		USB evaluation protocols.
	}
	\label{table:USB_resolutions}
\end{table}


For fair evaluation, we propose the \textit{USB evaluation protocols} shown in Table~\ref{table:USB_resolutions}.
By analogy with the size variations of the Universal Serial Bus connectors for various devices,
USB evaluation protocols have variations in test image scales for various devices and applications.

The maximum resolution for Standard USB follows the popular test scale of 1333800 in the COCO benchmark (see Table~\ref{table:coco_sota} and \cite{MMDetection, Detectron2018}).
For Mini USB, we limit the resolution based on 512512.
This resolution is popular in the \Pascal VOC benchmark~\cite{PASCALVOC_IJCV2015, SSD_ECCV2016}, which contains small images and large objects.
It is also popular in real-time detectors~\cite{EfficientDet_CVPR2020, YOLOv4_2020}.
We adopted a further small-scale 224224 for Micro USB.
This resolution is popular in ImageNet classification~\cite{ImageNet_IJCV2015, ResNet_CVPR2016}.
Although small object detection is extremely difficult,
it is suitable for low-power devices.
Additionally, this protocol enables people to manage object detection tasks using one or few GPUs.
To cover larger test scales than Standard USB, we define Large USB and Huge USB based on WOD resolutions.
The maximum resolution of the Huge USB is determined by
the top methods on WOD (see Sec.~\ref{sec:sota_comparison}).
Although larger inputs (regarded as Freestyle) may be preferable for accuracy,
excessively large inputs significantly reduce the practicality of detectors.

In addition to test image scales,
the presence and degree of Test-Time Augmentation (TTA) make large differences in accuracy and inference time.
When using TTA,
participants should report its details (including the number of scales of multi-scale testing)
and results without TTA.





\subsection{Evaluation Metrics}

We mainly use the COCO metrics~\cite{COCO_ECCV2014, cocoapi} to evaluate the performance of detectors on each dataset.
We provide data format converters for WOD\footnote{\WaymoCOCO} and \MangasAbbr\footnote{\mangaapi}.


We first describe the calculation of COCO metrics according to the official evaluation code~\cite{cocoapi}.
True or false positives are judged by measuring the Intersection over Union (IoU)
between predicted bounding boxes and ground truth bounding boxes~\cite{PASCALVOC_IJCV2015}.
For each category,
the Average Precision (AP) is calculated as
precision averaged over 101 recall thresholds .
The COCO-style AP (CAP) for a dataset  is calculated as

where
 denotes the predefined 10 IoU thresholds,
 denotes categories in the dataset ,
 denotes the cardinality of a set (\eg,  for COCO),
and  denotes AP for an IoU threshold  and a category .
For detailed analysis, five additional AP metrics (averaged over categories) are evaluated.
AP and AP denote AP at single IoU thresholds of  and , respectively.
\APS, \APM, and \APL are variants of CAP, where target objects are limited to
small (area ),
medium ( area ),
and large ( area) objects, respectively.
The area is measured using mask annotations for COCO and bounding box annotations for WOD and \MangasAbbr.

As the primary metric for USB,
we use the mean COCO-style AP (mCAP)
averaged over all datasets  as

Since USB adopts the three datasets described in Sec.~\ref{sec:usb_datasets},

Similarly,
we define five metrics from
AP, AP, \APS, \APM, and \APL
by averaging them over the datasets.
We plan to define finer scale-wise metrics for USOD in future work.

For ease of quantitative evaluation,
we limit the number of detections per image to 100 across all categories, following the COCO benchmark~\cite{cocoapi}.
For qualitative evaluation, participants may raise the limit to 300
(1\% of images in the \MangasAbbr \texttt{15test} set contain more than 100 annotations).



\section{\Univs}

For fast and accurate detectors for USOD, we designed \Univs.
Single-stage detectors were adopted for efficiency.
See \AppendixSection for details of the methods and architectures used in \Univs.

As a baseline model,
we used RetinaNet~\cite{RetinaNet_ICCV2017} implemented in MMDetection~\cite{MMDetection}.
Specifically,
the backbone is ResNet-50-B~\cite{BagOfTricks_Classification_CVPR2019} (a variant of ResNet-50~\cite{ResNet_CVPR2016}, also known as the PyTorch style).
The neck is FPN~\cite{FPN_CVPR2017}.
We used focal loss~\cite{RetinaNet_ICCV2017}, single-scale training, and single-scale testing.

Built on the RetinaNet baseline,
we designed \textit{\OurOrig} by collecting human wisdom about multi-scale object detection as of May 2020.
We used ATSS~\cite{ATSS_CVPR2020} and SEPC without iBN~\cite{SEPC_CVPR2020}
(hereafter referred to as \textit{ATSEPC}).
The backbone is Res2Net-50-v1b~\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels}.
Deformable Convolutional Networks (DCN)~\cite{DCN_ICCV2017} were adopted in the backbone and neck.
We used multi-scale training.
Unless otherwise stated, we used single-scale testing for efficiency.

By adding GFL~\cite{GFL_NeurIPS2020}, SyncBN~\cite{MegDet_CVPR2018}, and iBN~\cite{SEPC_CVPR2020},
we designed three variants of \OurOrig
around August 2020.
\textit{\OurAugustD} heavily uses DCN~\cite{DCN_ICCV2017}.
\textit{\OurAugust} speeds up inference (and training) by the light use of DCN~\cite{DCN_ICCV2017, SEPC_CVPR2020}.
\textit{\OurAugustS} further speeds up inference using the ResNet-50-C~\cite{BagOfTricks_Classification_CVPR2019} backbone.







\section{Experiments}

Here, we present benchmark results on USB
and comparison results with state-of-the-art methods on the three datasets.
Thereafter, we analyze the characteristics of detectors by additional experiments.
See \AppendixSection for details of the experimental settings and results,
including the KITTI-style AP~\cite{KITTI_CVPR2012, WaymoOpenDataset_CVPR2020} on WOD
and the effects of COCO pre-training on \MangasAbbr.


\subsection{Experimental Settings} \label{sec:experimental_settings}

\vspace{-4mm}
\begin{table}[ht]
	\setlength{\tabcolsep}{1.0mm}
	\renewcommand\arraystretch{0.85}
	\begin{minipage}{0.71\hsize}
		\scalebox{0.6}{\begin{tabular}{lccc}
			\toprule
			Hyperparameters                    &      COCO       &       WOD       &   \MangasAbbr   \\ \midrule
			LR for multi-stage detectors       &      0.02       &      0.02       &      0.16       \\
			LR for single-stage detectors      &      0.01       &      0.01       &      0.08       \\
			Test scale                         & 1333800 & 1248832 & 1216864 \\
			Range for multi-scale training &    480--960     &    640--1280    &    480--960     \\ \bottomrule
		\end{tabular}}
	\end{minipage}
	\hfill
	\begin{minipage}{0.26\hsize}
		\scalebox{0.6}{\begin{tabular}{lc}
			\toprule
			Hyperparam.  &  Common   \\ \midrule
			Epoch        &    12     \\
			Batch size   &    16     \\
			Momentum     &    0.9    \\
			Weight decay &  \\ \bottomrule
		\end{tabular}}
	\end{minipage}
	\caption{
		Default hyperparameters.
		: Shorter side pixels.
	}
	\label{table:hyperparameters}
\end{table}
\vspace{-3mm}

Our code is built on MMDetection~\cite{MMDetection} v2.
We used the COCO pre-trained models of the repository for existing methods
(Faster R-CNN~\cite{Faster_R-CNN_NIPS2015} with FPN~\cite{FPN_CVPR2017},
Cascade R-CNN~\cite{Cascade_R-CNN_CVPR2018},
RetinaNet~\cite{RetinaNet_ICCV2017},
ATSS~\cite{ATSS_CVPR2020},
and GFL~\cite{GFL_NeurIPS2020}).
We trained all models with Stochastic Gradient Descent (SGD).

The default hyperparameters are listed in Table~\ref{table:hyperparameters}.
Most values follow standard settings~\cite{MMDetection, RetinaNet_ICCV2017, ATSS_CVPR2020, FPN_CVPR2017}.
We used some dataset-dependent values.
For \MangasAbbr, we roughly tuned the learning rates (LR)
based on a preliminary experiment with the RetinaNet~\cite{RetinaNet_ICCV2017} baseline model.
Test scales were determined within the standard USB protocol,
considering the typical aspect ratio of the images in each dataset.
The ranges for multi-scale training for COCO and \MangasAbbr follow prior work~\cite{SEPC_CVPR2020}.
We used larger scales for WOD because the objects in WOD are especially small.



COCO models were fine-tuned from ImageNet pre-trained backbones.
We trained the models for WOD and \MangasAbbr from the corresponding COCO pre-trained models.
We follow the learning rate schedules of MMDetection~\cite{MMDetection}.
We mainly used the 1 schedule (12 epochs).
For comparison with state-of-the-art methods on COCO,
we used the 2 schedule (24 epochs) for most models
and the 20e schedule (20 epochs) for \OurAugustD due to overfitting with the 2 schedule.
For comparison with state-of-the-art methods on WOD,
we trained \OurOrig on the WOD full training set for 7 epochs.
We used a learning rate of  for 6 epochs and  for the last epoch.




\subsection{Benchmark Results on USB}
\label{sec:usb_results}

\begin{table}[t]
	\setlength{\tabcolsep}{0.8mm}
	\renewcommand\arraystretch{0.85}
	\begin{center}
		\scalebox{0.72}{\begin{tabular}{lccccccccc}
			\toprule
			Method                                      &   mCAP    & AP & AP &   \APS    &   \APM    &   \APL    &   COCO    &    WOD    & \MangasAbbr \\ \midrule
			Faster R-CNN~\cite{Faster_R-CNN_NIPS2015}   &   45.9    &   68.2    &   49.1    &   15.2    &   38.9    &   62.5    &   37.4    &   34.5    &    65.8     \\
			Cascade R-CNN~\cite{Cascade_R-CNN_CVPR2018} &   48.1    &   68.5    &   51.5    &   15.6    &   41.3    &   65.9    &   40.3    &   36.4    &    67.6     \\
			RetinaNet~\cite{RetinaNet_ICCV2017}         &   44.8    &   66.0    &   47.4    &   12.9    &   37.3    &   62.6    &   36.5    &   32.5    &    65.3     \\
			ATSS~\cite{ATSS_CVPR2020}                   &   47.1    &   68.0    &   50.2    &   15.5    &   39.5    &   64.7    &   39.4    &   35.4    &    66.5     \\
			\ATSEPC~\cite{ATSS_CVPR2020, SEPC_CVPR2020} &   48.1    &   68.5    &   51.2    &   15.5    &   40.5    &   66.8    &   42.1    &   35.0    &    67.1     \\
			GFL~\cite{GFL_NeurIPS2020}                  &   47.7    &   68.3    &   50.6    &   15.8    &   39.9    &   65.8    &   40.2    &   35.7    &    67.3     \\
			\OurOrig                                    &   51.4    &   72.1    &   55.1    &   18.4    &   45.0    &   70.7    &   46.7    &   38.6    &    68.9     \\
			\OurAugust                                  & \TB{52.1} & \TB{72.9} & \TB{55.5} & \TB{19.2} & \TB{45.8} & \TB{70.8} & \TB{47.5} & \TB{39.0} &  \TB{69.9}  \\ \bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-3mm}
	\caption{
		Benchmark results on USB.
	}
	\label{table:usb}
\end{table}



\begin{figure}[t]
	\includegraphics[width=1.0\linewidth]{images/usb_corr_thinline.pdf}\caption{
		Correlation between mCAP and CAP on each dataset.
	}
	\label{fig:usb_correlation}
\end{figure}


\begin{table*}[t]
	\setlength{\tabcolsep}{0.15em}
	\renewcommand\arraystretch{0.85}
	\begin{minipage}[c]{0.289\hsize}
	\begin{center}
		\scalebox{0.66}{\begin{tabularx}{1.5\textwidth}{l*{6}{>{\centering\arraybackslash}X}}
			\toprule
			Method                                      &    AP     & AP & AP &   \APS    &   \APM    &   \APL    \\ \midrule
			Faster R-CNN~\cite{Faster_R-CNN_NIPS2015}   &   37.4    &   58.1    &   40.4    &   21.2    &   41.0    &   48.1    \\
			Cascade R-CNN~\cite{Cascade_R-CNN_CVPR2018} &   40.3    &   58.6    &   44.0    &   22.5    &   43.8    &   52.9    \\
			RetinaNet~\cite{RetinaNet_ICCV2017}         &   36.5    &   55.4    &   39.1    &   20.4    &   40.3    &   48.1    \\
			ATSS~\cite{ATSS_CVPR2020}                   &   39.4    &   57.6    &   42.8    &   23.6    &   42.9    &   50.3    \\
			\ATSEPC~\cite{ATSS_CVPR2020, SEPC_CVPR2020} &   42.1    &   59.9    &   45.5    &   24.6    &   46.1    &   55.0    \\
			GFL~\cite{GFL_NeurIPS2020}                  &   40.2    &   58.4    &   43.3    &   23.3    &   44.0    &   52.2    \\
			\OurOrig                                    &   46.7    &   65.0    &   50.7    & \TB{29.2} &   50.6    &   61.4    \\
			\OurAugust                                  & \TB{47.5} & \TB{66.0} & \TB{51.9} &   28.9    & \TB{52.1} & \TB{61.9} \\ \bottomrule
		\end{tabularx}}
	\end{center}
	\vspace{-3mm}
	\caption{
		Results on COCO \texttt{minival}.
	}
	\label{table:coco_minival}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.330\hsize}
	\begin{center}
		\scalebox{0.66}{\begin{tabularx}{1.5\textwidth}{l*{9}{>{\centering\arraybackslash}X}}
			\toprule
			Method         & AP        & AP & AP & \APS     & \APM      & \APL      & veh.      & ped.      & cyc.      \\ \midrule
			Faster         & 34.5      & 55.3      & 36.3      & 6.0      & 35.8      & 67.4      & 42.7      & 34.6      & 26.1      \\
			Cascade        & 36.4      & 56.3      & 38.6      & 6.5      & 38.1      & 70.6      & 44.5      & 36.3      & 28.5      \\
			RetinaNet      & 32.5      & 52.2      & 33.7      & 2.6      & 32.8      & 67.9      & 40.0      & 32.5      & 25.0      \\
			ATSS           & 35.4      & 56.2      & 37.0      & 6.1      & 36.6      & 69.8      & 43.6      & 35.6      & 27.0      \\
			\ATSEPC        & 35.0      & 55.3      & 36.5      & 5.8      & 35.5      & 70.5      & 43.5      & 35.3      & 26.3      \\
			GFL            & 35.7      & 56.0      & 37.1      & 6.2      & 36.7      & 70.7      & 44.0      & 36.0      & 27.1      \\
			\OurOrigAbbr   & 38.6      & 59.8      & \TB{40.9} & 7.4      & 41.0      & \TB{74.0} & 46.0      & 37.6      & \TB{32.3} \\
			\OurAugustAbbr & \TB{39.0} & \TB{60.2} & 40.4      & \TB{8.3} & \TB{41.7} & 73.3      & \TB{47.1} & \TB{38.7} & 31.0      \\ \bottomrule
		\end{tabularx}}
	\end{center}
	\vspace{-3mm}
	\caption{
		Results on WOD \texttt{f0val}.
	}
	\label{table:waymo_f0_train_f0val_832}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.370\hsize}
	\begin{center}
		\scalebox{0.66}{\begin{tabularx}{1.5\textwidth}{l*{10}{>{\centering\arraybackslash}X}}
			\toprule
			Method         & AP        & AP & AP & \APS      & \APM      & \APL      & body      & face      & frame     & text      \\ \midrule
			Faster         & 65.8      & 91.1      & 70.6      & 18.4      & 39.9      & 72.1      & 58.3      & 47.5      & 90.1      & 67.1      \\
			Cascade        & 67.6      & 90.6      & 72.0      & 17.9      & 41.9      & 74.3      & 60.8      & \TB{48.2} & 92.5      & 69.0      \\
			RetinaNet      & 65.3      & 90.5      & 69.5      & 15.7      & 38.9      & 71.9      & 58.3      & 46.3      & 88.8      & 67.7      \\
			ATSS           & 66.5      & 90.1      & 70.8      & 16.8      & 38.9      & 74.0      & 60.9      & 44.6      & 91.3      & 69.0      \\
			\ATSEPC        & 67.1      & 90.2      & 71.5      & 16.2      & 39.8      & 74.9      & 62.3      & 44.6      & 92.1      & 69.4      \\
			GFL            & 67.3      & 90.6      & 71.5      & 17.9      & 38.9      & 74.4      & 61.7      & 45.7      & 92.2      & 69.4      \\
			\OurOrigAbbr   & 68.9      & 91.4      & 73.7      & 18.7      & 43.4      & 76.6      & 65.8      & 46.6      & 93.0      & 70.3      \\
			\OurAugustAbbr & \TB{69.9} & \TB{92.5} & \TB{74.3} & \TB{20.5} & \TB{43.6} & \TB{77.1} & \TB{66.6} & 48.0      & \TB{93.7} & \TB{71.2} \\ \bottomrule
		\end{tabularx}}
	\end{center}
	\vspace{-3mm}
	\caption{
		Results on \Mangas \texttt{15test}.
	}
	\label{table:Manga109s_15test}
    \end{minipage}
\end{table*}



We trained and evaluated methods on the USB.
All methods follow the Standard USB 1.0 protocol
using the default hyperparameters in Sec.~\ref{sec:experimental_settings}.
The results are shown in Table~\ref{table:usb}.
\OurAugust
achieves the highest results on all datasets,
resulting in 52.1\% mCAP.
In most cases, methods that work on COCO also work on the other datasets.
Cascade R-CNN~\cite{Cascade_R-CNN_CVPR2018} and ATSS~\cite{ATSS_CVPR2020}
achieve over 2\% more mCAP than Faster R-CNN~\cite{Faster_R-CNN_NIPS2015} and RetinaNet~\cite{RetinaNet_ICCV2017}, respectively.
In some cases, methods that work on COCO show small or negative effects on WOD and \MangasAbbr.
Thus, USB can impose a penalty on COCO-biased methods.

To compare the effectiveness of each method on each dataset,
we show the correlation between mCAP and CAP on each dataset in Figure~\ref{fig:usb_correlation}.
SEPC~\cite{SEPC_CVPR2020} improves COCO CAP and deteriorates WOD CAP.
Multi-stage detectors~\cite{Faster_R-CNN_NIPS2015, Cascade_R-CNN_CVPR2018}
show relatively high CAP on WOD and relatively low CAP on COCO.
Adding GFL~\cite{GFL_NeurIPS2020} is especially effective on \MangasAbbr
(see improvements from ATSS to GFL and from \OurOrig to \OurAugust).


We also show detailed results on each dataset.
Table~\ref{table:coco_minival} shows the COCO results.
Since the effectiveness of existing methods has been verified on COCO,
their improvements are steady.
Table~\ref{table:waymo_f0_train_f0val_832} shows the WOD results.
Adding SEPC~\cite{SEPC_CVPR2020} to ATSS~\cite{ATSS_CVPR2020} decreases all metrics except for \APL.
We found that this reduction does not occur at large test scales in higher USB evaluation protocols (see Sec.~\ref{sec:analyses}).
\OurAugust shows worse results than \OurOrig in some metrics,
probably due to
the light use of DCN for fast inference (see Table~\ref{table:coco_OurAugust}).
Table~\ref{table:Manga109s_15test} shows the \MangasAbbr results.
Interestingly, improvements by ATSS~\cite{ATSS_CVPR2020} are smaller than those on COCO and WOD
due to the drop of face AP.
We conjecture that
this phenomenon comes from the domain differences discussed in Sec.~\ref{sec:usb_datasets} and prior work~\cite{Manga109_detection_Ogawa_2018},
although we should explore it in future work.



\begin{table*}[t]
\setlength{\tabcolsep}{1.3mm}
\renewcommand\arraystretch{0.83}
\definecolor{mygray}{gray}{0.5}
\newcommand{\DE}[1]{\small{(#1)}}\newcommand{\ES}[1]{\textcolor{mygray}{#1}}\begin{center}
\scalebox{0.75}{
\begin{tabular}{lllcclclccccccc}
	\toprule
	Protocol         & Method                                                  & Backbone             & DCN &  Epoch  & Max test scale           &   TTA   & FPS          &    AP     & AP & AP &   \APS    &   \APM    &   \APL    & Reference \\ \midrule
	Standard USB 1.0 & Faster R-CNN~\cite{Faster_R-CNN_NIPS2015, FPN_CVPR2017} & ResNet-101           &     &   22    & \ES{1333}\;\:800 &         & \DE{14.2}    &   36.2    &   59.1    &   39.0    &   18.2    &   39.0    &   48.2    &  CVPR17   \\
	Standard USB 1.0 & Cascade R-CNN~\cite{Cascade_R-CNN_CVPR2018}             & ResNet-101           &     &   19    & 1312\;\:800      &         & \DE{11.9}    &   42.8    &   62.1    &   46.3    &   23.7    &   45.5    &   55.2    &  CVPR18   \\
	Standard USB 1.0 & RetinaNet~\cite{RetinaNet_ICCV2017}                     & ResNet-101           &     &   18    & 1333\;\:800      &         & \DE{13.6}    &   39.1    &   59.1    &   42.3    &   21.8    &   42.7    &   50.2    &  ICCV17   \\
	Standard USB 1.0 & FCOS~\cite{FCOS_ICCV2019}                               & X-101 (644d) &     &   24    & 1333\;\:800      &         & \DE{\;\:8.9} &   44.7    &   64.1    &   48.4    &   27.6    &   47.5    &   55.6    &  ICCV19   \\
	Standard USB 1.0 & ATSS~\cite{ATSS_CVPR2020}                               & X-101 (644d) & \cm &   24    & 1333\;\:800      &         & 10.6         &   47.7    &   66.5    &   51.9    &   29.7    &   50.8    &   59.4    &  CVPR20   \\
	Standard USB 1.0 & FreeAnchor+SEPC~\cite{SEPC_CVPR2020}                    & X-101 (644d) & \cm &   24    & 1333\;\:800      &         & ---          &   50.1    &   69.8    &   54.3    &   31.3    &   53.3    &   63.7    &  CVPR20   \\
	Standard USB 1.0 & PAA~\cite{PAA_ECCV2020}                                 & X-101 (644d) & \cm &   24    & 1333\;\:800      &         & ---          &   49.0    &   67.8    &   53.3    &   30.2    &   52.8    &   62.2    &  ECCV20   \\
	Standard USB 1.0 & PAA~\cite{PAA_ECCV2020}                                 & X-152 (328d) & \cm &   24    & 1333\;\:800      &         & ---          &   50.8    &   69.7    &   55.1    &   31.4    &   54.7    &   65.2    &  ECCV20   \\
	Standard USB 1.0 & RepPoints v2~\cite{RepPointsv2_NeurIPS2020}             & X-101 (644d) & \cm &   24    & 1333\;\:800      &         & \DE{\;\:3.8} &   49.4    &   68.9    &   53.4    &   30.3    &   52.1    &   62.3    & NeurIPS20 \\
	Standard USB 1.0 & RelationNet++~\cite{RelationNet2_NeurIPS2020}           & X-101 (644d) & \cm &   20    & 1333\;\:800      &         & 10.3         &   50.3    &   69.0    &   55.0    & \TB{32.8} &   55.0    & \TB{65.8} & NeurIPS20 \\
	Standard USB 1.0 & GFL~\cite{GFL_NeurIPS2020}                              & ResNet-50            &     &   24    & 1333\;\:800      &         & 37.2         &   43.1    &   62.0    &   46.8    &   26.0    &   46.7    &   52.3    & NeurIPS20 \\
	Standard USB 1.0 & GFL~\cite{GFL_NeurIPS2020}                              & ResNet-101           &     &   24    & 1333\;\:800      &         & 29.5         &   45.0    &   63.7    &   48.9    &   27.2    &   48.8    &   54.5    & NeurIPS20 \\
	Standard USB 1.0 & GFL~\cite{GFL_NeurIPS2020}                              & ResNet-101           & \cm &   24    & 1333\;\:800      &         & 22.8         &   47.3    &   66.3    &   51.4    &   28.0    &   51.1    &   59.2    & NeurIPS20 \\
	Standard USB 1.0 & GFL~\cite{GFL_NeurIPS2020}                              & X-101 (324d) & \cm &   24    & 1333\;\:800      &         & 15.4         &   48.2    &   67.4    &   52.6    &   29.2    &   51.7    &   60.2    & NeurIPS20 \\
	Standard USB 1.0 & \TB{\OurAugustS}                                        & ResNet-50-C          & \cm &   24    & 1333\;\:800      &         & 31.6         &   47.4    &   66.0    &   51.4    &   28.3    &   50.8    &   59.5    &  (Ours)   \\
	Standard USB 1.0 & \TB{\OurAugust}                                         & Res2Net-50-v1b       & \cm &   24    & 1333\;\:800      &         & 24.9         &   48.8    &   67.5    &   53.0    &   30.1    &   52.3    &   61.1    &  (Ours)   \\
	Standard USB 1.0 & \TB{\OurAugustD}                                        & Res2Net-101-v1b      & \cm &   20    & 1333\;\:800      &         & 11.7         & \TB{51.3} & \TB{70.0} & \TB{55.8} &   31.7    & \TB{55.3} &   64.9    &  (Ours)   \\ \midrule
	Large USB 1.0    & \TB{\OurAugustD}                                        & Res2Net-101-v1b      & \cm &   20    & 1493\;\:896      &         & 11.6         &   51.5    &   70.2    &   56.0    &   32.8    &   55.5    &   63.7    &  (Ours)   \\
	Large USB 1.0    & \TB{\OurAugustD}                                        & Res2Net-101-v1b      & \cm &   20    & 20001200         &    5    & ---          & \TB{53.8} & \TB{71.5} & \TB{59.4} & \TB{35.3} & \TB{57.3} & \TB{67.3} &  (Ours)   \\ \midrule
	Huge USB 1.0     & ATSS~\cite{ATSS_CVPR2020}                               & X-101 (644d) & \cm &   24    & 30001800         &   13    & ---          &   50.7    &   68.9    &   56.3    &   33.2    &   52.9    &   62.4    &  CVPR20   \\
	Huge USB 1.0     & PAA~\cite{PAA_ECCV2020}                                 & X-101 (644d) & \cm &   24    & 30001800         &   13    & ---          &   51.4    &   69.7    &   57.0    &   34.0    &   53.8    &   64.0    &  ECCV20   \\
	Huge USB 1.0     & PAA~\cite{PAA_ECCV2020}                                 & X-152 (328d) & \cm &   24    & 30001800         &   13    & ---          &   53.5    & \TB{71.6} &   59.1    & \TB{36.0} &   56.3    &   66.9    &  ECCV20   \\
	Huge USB 1.0     & RepPoints v2~\cite{RepPointsv2_NeurIPS2020}             & X-101 (644d) & \cm &   24    & \ES{30001800}    & \ES{13} & ---          &   52.1    &   70.1    &   57.5    &   34.5    &   54.6    &   63.6    & NeurIPS20 \\
	Huge USB 1.0     & RelationNet++~\cite{RelationNet2_NeurIPS2020}           & X-101 (644d) & \cm &   20    & \ES{30001800}    & \ES{13} & ---          &   52.7    &   70.4    &   58.3    &   35.8    &   55.3    &   64.7    & NeurIPS20 \\
	Huge USB 1.0     & \TB{\OurAugustD}                                        & Res2Net-101-v1b      & \cm &   20    & 30001800         &   13    & ---          & \TB{54.1} & \TB{71.6} & \TB{59.9} &   35.8    &   57.2    & \TB{67.4} &  (Ours)   \\ \midrule
	Huge USB 2.0     & TSD~\cite{TSD_CVPR2020}                                 & SENet-154            & \cm & \ES{34} & \ES{20001400}    & \ES{4}  & ---          &   51.2    & \TB{71.9} &   56.0    &   33.8    &   54.8    &   64.2    &  CVPR20   \\ \midrule
	Huge USB 2.5     & DetectoRS~\cite{DetectoRS_2020}                         & X-101 (324d) & \cm &   40    & 24001600         &    5    & ---          & \TB{54.7} & \TB{73.5} & \TB{60.1} & \TB{37.4} & \TB{57.3} &   66.4    &  arXiv20  \\ \midrule
	Mini USB 3.0     & EfficientDet-D0~\cite{EfficientDet_CVPR2020}            & EfficientNet-B0      &     &   300   & \;\:512\;\:512   &         & 98.0         &   33.8    &   52.2    &   35.8    &   12.0    &   38.3    &   51.2    &  CVPR20   \\
	Mini USB 3.1     & YOLOv4~\cite{YOLOv4_2020}                               & CSPDarknet-53        &     &   273   & \;\:512\;\:512   &         & 83           & \TB{43.0} & \TB{64.9} & \TB{46.5} & \TB{24.3} & \TB{46.1} & \TB{55.2} &  arXiv20  \\ \midrule
	Standard USB 3.0 & EfficientDet-D2~\cite{EfficientDet_CVPR2020}            & EfficientNet-B2      &     &   300   & \;\:768\;\:768   &         & 56.5         &   43.0    &   62.3    &   46.2    &   22.5    &   47.0    &   58.4    &  CVPR20   \\
	Standard USB 3.0 & EfficientDet-D4~\cite{EfficientDet_CVPR2020}            & EfficientNet-B4      &     &   300   & 10241024         &         & 23.4         &   49.4    &   69.0    &   53.4    &   30.3    &   53.2    &   63.2    &  CVPR20   \\
	Standard USB 3.1 & YOLOv4~\cite{YOLOv4_2020}                               & CSPDarknet-53        &     &   273   & \;\:608\;\:608   &         & 62           &   43.5    &   65.7    &   47.3    &   26.7    &   46.7    &   53.3    &  arXiv20  \\ \midrule
	Large USB 3.0    & EfficientDet-D5~\cite{EfficientDet_CVPR2020}            & EfficientNet-B5      &     &   300   & 12801280         &         & 13.8         &   50.7    &   70.2    &   54.7    &   33.2    &   53.9    &   63.2    &  CVPR20   \\
	Large USB 3.0    & EfficientDet-D6~\cite{EfficientDet_CVPR2020}            & EfficientNet-B6      &     &   300   & 12801280         &         & 10.8         &   51.7    &   71.2    &   56.0    &   34.1    &   55.2    &   64.1    &  CVPR20   \\
	Large USB 3.0    & EfficientDet-D7~\cite{EfficientDet_CVPR2020}            & EfficientNet-B6      &     &   300   & 15361536         &         & \;\:8.2      &   52.2    &   71.4    &   56.3    &   34.8    &   55.5    &   64.6    &  CVPR20   \\ \midrule
	Freestyle        & RetinaNet+SpineNet~\cite{SpineNet_CVPR2020}             & SpineNet-190         &     &   400   & 12801280         &         & ---          &   52.1    &   71.8    &   56.5    &   35.4    &   55.0    &   63.6    &  CVPR20   \\
	Freestyle        & EfficientDet-D7x~\cite{EfficientDet_arXiv}              & EfficientNet-B7      &     &   600   & 15361536         &         & \;\:6.5      & \TB{55.1} & \TB{74.3} &   59.9    &   37.2    & \TB{57.9} & \TB{68.0} &  arXiv20  \\ \bottomrule
\end{tabular}
}
\end{center}
\vspace{-3mm}
\caption{
	State-of-the-art methods on COCO \texttt{test-dev}.
	We classify methods by proposed protocols.
	X in the Backbone column denotes ResNeXt~\cite{ResNeXt_CVPR2017}. See method papers for other backbones.
	TTA: Test-time augmentation including horizontal flip and multi-scale testing (numbers denote scales).
	FPS values without and with parentheses were measured on V100 with mixed precision and other environments, respectively.
	We measured the FPS of GFL~\cite{GFL_NeurIPS2020} models in our environment
	and estimated those of ATSS~\cite{ATSS_CVPR2020} and RelationNet++~\cite{RelationNet2_NeurIPS2020} based on the measured values and \cite{GFL_NeurIPS2020, RelationNet2_NeurIPS2020}.
	The settings of other methods are based on conference papers, their arXiv versions, and authors' codes.
	The values shown in gray were estimated from descriptions in papers and codes.
	Some FPS values are from~\cite{GFL_NeurIPS2020}.
}
\label{table:coco_sota}
\end{table*}










 



\begin{table}[t]
	\setlength{\tabcolsep}{2.3mm}
	\renewcommand\arraystretch{0.85}
	\newcommand{\RANK}{\multirow{2}{*}[-0.5\dimexpr \aboverulesep + \belowrulesep + \cmidrulewidth]{Rank}}
	\newcommand{\METHOD}{\multirow{2}{*}[-0.5\dimexpr \aboverulesep + \belowrulesep + \cmidrulewidth]{Method}}
	\newcommand{\APLL}{\multirow{2}{*}[-0.5\dimexpr \aboverulesep + \belowrulesep + \cmidrulewidth]{AP/L2}}
	\newcommand{\CMRs}{\cmidrule(l{.2em}r{.2em}){3-4}}
	\begin{center}
		\scalebox{0.75}{\begin{tabular}{llccc}
			\toprule
			\RANK & \METHOD                                                             & \multicolumn{2}{c}{\# Models} &     \APLL      \\
			\CMRs &                                                                     & Multi-stage & Single-stage    &                \\ \midrule
			\multicolumn{5}{l}{\textit{Methods including multi-stage detector:}}                                                         \\
			1     & RW-TSDet~\cite{Waymo2d_1st_2020}                                    & 6+          &                 & \textbf{74.43} \\
			2     & HorizonDet~\cite{Waymo2d_2nd_2020}                                  & 4           & 8               &     70.28      \\
			3     & SPNAS-Noah~\cite{Waymo2d_3rd_2020}                                  & 2           &                 &     69.43      \\ \midrule
			\multicolumn{5}{l}{\textit{Single-stage detectors:}}                                                                         \\
			7     & \textbf{UniverseNet (Ours)}                                         &             & \textbf{1}      & \textbf{67.42} \\
			13    & YOLO V4~\cite{YOLOv4_2020}                                          &             & 1+              &     58.08      \\
			14    & ATSS-Efficientnet~\cite{ATSS_CVPR2020, EfficientNet_ICML2019}       &             & 1+              &     56.99      \\ \bottomrule
		\end{tabular}}
\end{center}
\vspace{-3mm}
\caption{
	Waymo Open Dataset Challenge 2020 2D detection~\cite{WaymoOpenDataset_2D_detection_leaderboard}.
}
\label{table:waymo_leaderboard}
\end{table}





\begin{table*}[t]
	\begin{minipage}[c]{0.62\hsize}
		\captionsetup[sub]{font=scriptsize,width=.965\linewidth}
		\setlength{\tabcolsep}{0.8mm}
		\renewcommand\arraystretch{0.85}
		\begin{minipage}[c]{0.46\hsize}
			\begin{center}
				\scalebox{0.6}{\begin{tabular}{p{29mm}cccccc}
						\toprule
						Method                                      &    AP     & AP & AP &   \APS    &   \APM    &   \APL    \\ \midrule
						ATSS~\cite{ATSS_CVPR2020}                   &   39.4    &   57.6    &   42.8    &   23.6    &   42.9    &   50.3    \\
						\ATSEPC~\cite{ATSS_CVPR2020, SEPC_CVPR2020} & \TB{42.1} & \TB{59.9} & \TB{45.5} & \TB{24.6} & \TB{46.1} & \TB{55.0} \\ \bottomrule
				\end{tabular}}
			\end{center}
			\vspace{-5mm}
			\subcaption{
				AP improvements by SEPC without iBN~\cite{SEPC_CVPR2020}.
			}
			\label{table:coco_ATSEPC}
			\vspace{-2mm}
			\begin{center}
				\scalebox{0.6}{\begin{tabular}{p{29mm}cccccc}
						\toprule
						Method                                      &    AP     & AP & AP &   \APS    &   \APM    &   \APL    \\ \midrule
						\ATSEPC~\cite{ATSS_CVPR2020, SEPC_CVPR2020} &   42.1    &   59.9    &   45.5    &   24.6    &   46.1    &   55.0    \\
						\OurOrig                                    & \TB{46.7} & \TB{65.0} & \TB{50.7} & \TB{29.2} & \TB{50.6} & \TB{61.4} \\ \bottomrule
				\end{tabular}}
			\end{center}
			\vspace{-5mm}
			\subcaption{
				AP improvements by Res2Net-v1b~\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels}, DCN~\cite{DCN_ICCV2017}, and multi-scale training.
			}
			\label{table:coco_OurOrig}
			\vspace{-2mm}
			\begin{center}
				\scalebox{0.6}{\begin{tabular}{p{29mm}cccccc}
						\toprule
						Method                     &    AP     & AP & AP &   \APS    &   \APM    &   \APL    \\ \midrule
						\OurOrig                   &   46.7    &   65.0    &   50.7    & \TB{29.2} &   50.6    &   61.4    \\
						\OurGFL                    & \TB{47.5} & \TB{65.8} & \TB{51.8} & \TB{29.2} & \TB{51.6} & \TB{62.5} \\ \bottomrule
				\end{tabular}}
			\end{center}
			\vspace{-5mm}
			\subcaption{
				AP improvements by GFL~\cite{GFL_NeurIPS2020}.
			}
			\label{table:coco_OurGFL}
			\vspace{-2mm}
			\begin{center}
				\scalebox{0.6}{\begin{tabular}{p{29mm}cccccc}
						\toprule
						Method      &    AP     & AP & AP &   \APS    &   \APM    &   \APL    \\ \midrule
						\OurGFL     &   47.5    &   65.8    &   51.8    &   29.2    &   51.6    &   62.5    \\
						\OurAugustD & \TB{48.6} & \TB{67.1} & \TB{52.7} & \TB{30.1} & \TB{53.0} & \TB{63.8} \\ \bottomrule
				\end{tabular}}
			\end{center}
			\vspace{-5mm}
			\subcaption{
				AP improvements by SyncBN~\cite{MegDet_CVPR2018}, iBN~\cite{SEPC_CVPR2020}.
			}
			\label{table:coco_OurAugustD}
		\end{minipage}
		\hfill
		\begin{minipage}[c]{0.55\hsize}
			\begin{center}
				\scalebox{0.6}{\begin{tabular}{lcccccccc}
						\toprule
						Method      &  DCN  &    FPS    &    AP     & AP & AP &   \APS    &   \APM    &   \APL    \\ \midrule
						\OurAugustD & heavy &   17.3    & \TB{48.6} & \TB{67.1} & \TB{52.7} & \TB{30.1} & \TB{53.0} & \TB{63.8} \\
						\OurAugust  & light & \TB{24.9} &   47.5    &   66.0    &   51.9    &   28.9    &   52.1    &   61.9    \\ \bottomrule
				\end{tabular}}
			\end{center}
			\vspace{-5mm}
			\subcaption{
				Speeding up by the light use of DCN~\cite{DCN_ICCV2017, SEPC_CVPR2020}.
			}
			\label{table:coco_OurAugust}
			\vspace{-1.5mm}
			\begin{center}
				\scalebox{0.6}{\begin{tabular}{p{38mm}ccccccc}
						\toprule
						Method                                                             &    FPS    &    AP     & AP & AP &   \APS    &   \APM    &   \APL    \\ \midrule
						\OurAugust                                                         &   24.9    & \TB{47.5} & \TB{66.0} & \TB{51.9} & \TB{28.9} & \TB{52.1} & \TB{61.9} \\
						w/o SEPC~\cite{SEPC_CVPR2020}                                      &   26.7    &   45.8    &   64.6    &   50.0    &   27.6    &   50.4    &   59.7    \\
						w/o Res2Net-v1b~\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels} & \TB{32.8} &   44.7    &   62.8    &   48.4    &   27.1    &   48.8    &   59.5    \\
						w/o DCN~\cite{DCN_ICCV2017}                                        &   27.8    &   45.9    &   64.5    &   49.8    & \TB{28.9} &   49.9    &   59.0    \\
						w/o multi-scale training                                           &   24.8    &   45.9    &   64.5    &   49.6    &   27.4    &   50.5    &   60.1    \\
						w/o SyncBN, iBN~\cite{MegDet_CVPR2018, SEPC_CVPR2020}              &   25.7    &   45.8    &   64.0    &   50.2    &   27.9    &   50.0    &   59.8    \\ \bottomrule
				\end{tabular}}
			\end{center}
			\vspace{-5mm}
			\subcaption{
				Ablation from \OurAugust.
				Replacing Res2Net-v1b backbone with ResNet-B~\cite{BagOfTricks_Classification_CVPR2019} has the largest effects.
			}
			\label{table:coco_OurAugust_ablation}
			\vspace{-1.5mm}
			\begin{center}
				\scalebox{0.6}{\begin{tabular}{p{38mm}ccccccc}
						\toprule
						Backbone                                                          &    FPS    &    AP     & AP & AP &   \APS    &   \APM    &   \APL    \\ \midrule
						ResNet-50-B~\cite{BagOfTricks_Classification_CVPR2019}            & \TB{32.8} &   44.7    &   62.8    &   48.4    &   27.1    &   48.8    &   59.5    \\
						ResNet-50-C~\cite{BagOfTricks_Classification_CVPR2019}            &   32.4    &   45.8    &   64.2    &   50.0    &   28.8    &   50.1    &   60.0    \\
						Res2Net-50~\cite{Res2Net_TPAMI2020}                               &   25.0    &   46.3    &   64.7    &   50.3    &   28.2    &   50.6    &   60.8    \\
						Res2Net-50-v1b~\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels} &   24.9    & \TB{47.5} & \TB{66.0} & \TB{51.9} & \TB{28.9} & \TB{52.1} & \TB{61.9} \\ \bottomrule
				\end{tabular}}
			\end{center}
			\vspace{-5mm}
			\subcaption{
				\OurAugust with different backbones.
			}
			\label{table:coco_backbone}
		\end{minipage}
		\caption{
			Ablation studies on COCO \texttt{minival}.
		}\label{table:coco_ablation}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.36\hsize}
	\captionsetup{width=.98\linewidth}
	\setlength{\tabcolsep}{1.7mm}
	\renewcommand\arraystretch{0.85}
	\begin{center}
		\scalebox{0.6}{\begin{tabular}{lllcc}
			\toprule
			Rank & Method                     & Test scale                         & Test flip &        MR         \\ \midrule
			1    & \OurOrig                   & 1280800, 1536960   &    \cm    & \;\:\textbf{5.67} \\
			--   & \OurOrig                   & 1280800                    &           &     \;\:7.49      \\
			2    & DeepBlueAI~\nightowlstalks & 19201280, 20481280 &    \cm    &     \;\:8.06      \\
			3    & dereyly~\nightowlstalks    & ---                                &    ---    &       10.29       \\ \bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-3mm}
	\caption{
		NightOwls Detection Challenge 2020 all objects track.
		MR: Average Miss Rate (\%) on \texttt{test} set
		under \textit{reasonable} setting.
	}
	\label{table:nightowls_test}
	\vspace{1.6mm}
	\includegraphics[width=\textwidth]{images/wod_ap_test_scale_cap_thinline.pdf}
	\vspace{-5mm}
	\captionof{figure}{
		Test scales \vs CAP on WOD \texttt{f0val}.
	}
	\label{fig:waymo_scales_cap}
	\end{minipage}
\end{table*}


\subsection{Comparison with State-of-the-Art}
\label{sec:sota_comparison}


\textbf{COCO.}
We show state-of-the-art methods on COCO \texttt{test-dev}
(as of November 14, 2020)
in Table~\ref{table:coco_sota}.
Our \OurAugustD achieves the highest AP (51.3\%) in the Standard USB 1.0 protocol.
Despite 12.5 fewer epochs, the speed-accuracy trade-offs of our models are comparable to those of EfficientDet~\cite{EfficientDet_CVPR2020} (see also Figure~\ref{fig:coco_speed_accuracy}).
With 13-scale TTA, \OurAugustD achieves the highest AP (54.1\%) in the Huge USB 1.0 protocol.
Even with 5-scale TTA in the Large USB 1.0,
it achieves 53.8\% AP,
which is higher than other methods in the USB 1.0 protocols.


\textbf{WOD.}
For comparison with state-of-the-art methods on WOD,
we submitted the detection results of \OurOrig to the Waymo Open Dataset Challenge 2020 2D detection,
a competition held at a CVPR 2020 workshop.
The primary metric is AP/L2,
a KITTI-style AP evaluated with LEVEL\_2 objects~\cite{WaymoOpenDataset_CVPR2020, WaymoOpenDataset_2D_detection_leaderboard}.
We used multi-scale testing with soft-NMS~\cite{SoftNMS_ICCV2017}.
The shorter side pixels of test scales are , including 8 pixels padding.
These scales enable utilizing SEPC~\cite{SEPC_CVPR2020} (see Sec.~\ref{sec:analyses}) and detecting small objects.
Table~\ref{table:waymo_leaderboard} shows the top teams' results.
\OurOrig achieves 67.42\% AP/L2
without multi-stage detectors, ensembles, expert models, or heavy backbones, unlike other top methods.
RW-TSDet~\cite{Waymo2d_1st_2020} overwhelms other multi-stage detectors,
whereas UniverseNet overwhelms other single-stage detectors.
These two methods used light backbones and large test scales~\cite{ashraf2016shallow}.
Interestingly, the maximum test scales are the same (33602240).
We conjecture that this is not a coincidence but a convergence caused by searching the accuracy saturation point.




\textbf{\Mangas.}
To the best of our knowledge, no prior work has reported detection results on the \textit{\Mangas} dataset (87 volumes).
Although many settings differ,
the state-of-the-art method on the full \textit{Manga109} dataset (109 volumes, non-public to commercial organizations)
achieves 77.1--92.0\% (mean: 84.2\%) AP on ten test volumes~\cite{Manga109_detection_Ogawa_2018}.
The mean AP of \OurAugust on the \texttt{15test} set (92.5\%) is higher than those results.



\subsection{Analyses and Discussions}
\label{sec:analyses}


\textbf{Chaotic state of the art.}
As shown in Table~\ref{table:coco_sota},
state-of-the-art detectors on the COCO benchmark were trained with various settings.
Comparisons across different training epochs are especially difficult
because long training does not decrease FPS, unlike large test scales.
Nevertheless, the EfficientDet~\cite{EfficientDet_CVPR2020}, YOLOv4~\cite{YOLOv4_2020}, and SpineNet~\cite{SpineNet_CVPR2020} papers
compare methods in their tables without specifying the difference in training epochs.
The compatibility of the USB training protocols (Sec.~\ref{sec:usb_training}) resolves this disorder.
We hope that many papers report results with the protocols for inclusive, healthy, and sustainable development of detectors.


\textbf{Ablation studies.}
We show the results of ablation studies for \Univs on COCO in Table~\ref{table:coco_ablation}.
SEPC~\cite{SEPC_CVPR2020}, Res2Net-v1b~\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels}, DCN~\cite{DCN_ICCV2017}, multi-scale training, GFL~\cite{GFL_NeurIPS2020}, SyncBN~\cite{MegDet_CVPR2018}, and iBN~\cite{SEPC_CVPR2020}
improve AP.
\OurAugustD is much more accurate (48.6\% AP) than other models trained for 12 epochs using ResNet-50-level backbones (\eg, ATSS: 39.4\%~\cite{ATSS_CVPR2020, MMDetection}, GFL: 40.2\%~\cite{GFL_NeurIPS2020, MMDetection}).
As shown in Table~\ref{table:coco_OurAugust},
\OurAugust is 1.4 faster than \OurAugustD at the cost of a 1\% AP drop.
\OurAugustS, the variant with ResNet-50-C backbone in Table~\ref{table:coco_backbone}, shows a good speed-accuracy trade-off by achieving 45.8\% AP and over 30 FPS.

\textbf{Generalization.}
To evaluate the generalization ability, we show the results on another dataset out of the USB.
We trained \OurOrig on the NightOwls~\cite{NightowlsDataset_ACCV2018},
a dataset for person detection at night,
from the WOD pre-trained model in Sec.~\ref{sec:sota_comparison}.
The top teams' results of the NightOwls Detection Challenge 2020 are shown in Table~\ref{table:nightowls_test}.
\OurOrig is more accurate than other methods, even without TTA,
and should be faster than the runner-up method
that uses larger test scales and a heavy model (Cascade R-CNN, ResNeXt-101, CBNet, Double-Head, DCN, and soft-NMS)~\nightowlstalks.







\textbf{Test scales.}
We show the results on WOD at different test scales in Figure~\ref{fig:waymo_scales_cap}.
Single-stage detectors require larger test scales than multi-stage detectors to achieve peak performance,
probably because they cannot extract features from precisely localized region proposals.
Although \ATSEPC shows lower AP than ATSS at the default test scale (1248832 in Standard USB),
it outperforms ATSS at larger test scales (\eg, 19201280 in Large USB).
We conjecture that we should enlarge object scales in images to utilize SEPC~\cite{SEPC_CVPR2020}
because its DCN~\cite{DCN_ICCV2017} enlarges effective receptive fields.
SEPC and DCN prefer large objects empirically (see Tables~\ref{table:coco_ATSEPC}, \ref{table:coco_OurAugust_ablation}, \cite{SEPC_CVPR2020, DCN_ICCV2017}),
and DCN~\cite{DCN_ICCV2017} cannot increase the sampling points for objects smaller than the kernel size in principle.
By utilizing the characteristics of SEPC and multi-scale training,
UniverseNets achieve the highest AP in a wide range of test scales.







 


\section{Conclusions}

We introduced USB, a benchmark for universal-scale object detection.
To resolve unfair comparisons in existing benchmarks,
we established USB training/evaluation protocols.
Our \Univs achieved state-of-the-art results on USB and existing benchmarks.
We found some weaknesses in the existing methods to be addressed in future research.

There are three limitations in this work.
(1) USB depends on datasets with many instances.
Reliable scale-wise metrics for small datasets should be considered.
(2) We adopted single-stage detectors for \Univs
and trained detectors in the USB 1.0 protocol.
Although these settings are practical,
it is worth exploring multi-stage detectors in higher protocols.
(3) The architectures and results of \Univs are still biased toward COCO
due to ablation studies and pre-training on COCO.
Less biased and more universal detectors should be developed in future research.

The proposed USB protocols can be applied to other tasks with modifications.
We believe that our work is an important step toward recognizing universal-scale objects
by connecting various experimental settings.
 
{\small
\paragraph*{Acknowledgments}
We are grateful to Dr. Hirokatsu Kataoka for helpful comments.
We thank all contributors for the datasets and software libraries.
The original image of Figure~\ref{fig:teaser} (left) is
\href{https://www.flickr.com/photos/84326824@N00/428244913}{\textcolor{black}{\textit{satellite office}}} by Taiyo FUJII (\href{https://creativecommons.org/licenses/by/2.0/}{\textcolor{black}{CC BY 2.0}}).
}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{universenet}
}

\clearpage
\appendix
\section*{\AppendixSection}
\section{Details of Related Work}

\subsection{Components for Multi-Scale Object Detection}

\textbf{Backbones and modules.}
Inception module~\cite{Inception_CVPR2015} arranges , , and  convolutions to cover multi-scale regions.
Residual block~\cite{ResNet_CVPR2016} adds multi-scale features from shortcut connections and  convolutions.
ResNet-C and ResNet-D~\cite{BagOfTricks_Classification_CVPR2019}
replace the first layer of ResNet with the deep stem (three  convolutions)~\cite{Inceptionv3_CVPR2016}.
Res2Net module~\cite{Res2Net_TPAMI2020} stacks  convolutions hierarchically to represent multi-scale features.
Res2Net-v1b~\cite{Res2Net_PretrainedModels} adopts deep stem with Res2Net module.
Deformable convolution module in Deformable Convolutional Networks (DCN)~\cite{DCN_ICCV2017}
adjusts receptive field adaptively
by deforming the sampling locations of standard convolutions.
These modules are mainly used in backbones.

\textbf{Necks.}
To combine and enhance backbones' representation, necks follow backbones.
Feature Pyramid Networks (FPN)~\cite{FPN_CVPR2017}
adopt top-down path and lateral connections like architectures for semantic segmentation.
Scale-Equalizing Pyramid Convolution (SEPC)~\cite{SEPC_CVPR2020}
introduces pyramid convolution across feature maps with different resolutions
and utilizes DCN to align the features.

\textbf{Heads and training sample selection.}
Faster R-CNN~\cite{Faster_R-CNN_NIPS2015} spreads multi-scale anchors over a feature map.
SSD~\cite{SSD_ECCV2016} spreads multi-scale anchors over multiple feature maps with different resolutions.
ATSS~\cite{ATSS_CVPR2020} eliminates the need for multi-scale anchors
by dividing positive and negative samples according to object statistics across pyramid levels.

\textbf{Multi-scale training and testing.}
Traditionally, the image pyramid is an essential technique to handle multi-scale objects~\cite{Rowley_PAMI1998}.
Although recent detectors can output multi-scale objects from a single-scale input,
many works use multi-scale inputs to improve performance~\cite{Faster_R-CNN_NIPS2015, RetinaNet_ICCV2017, ATSS_CVPR2020, SEPC_CVPR2020}.
In a popular implementation~\cite{MMDetection},
multi-scale training randomly chooses a scale at each iteration
for (training-time) data augmentation.
Multi-scale testing infers multi-scale inputs and merges their outputs
for Test-Time Augmentation (TTA).
SNIP~\cite{SNIP_Singh_CVPR2018} limits the range of object scales at each image scale
during training and testing.




\section{Details of Protocols}


\subsection{Dataset Splits for \Mangas}

\begin{table}[t]
	\renewcommand\arraystretch{0.85}
	\begin{center}
		\scalebox{0.8}{\begin{tabular}{ll}
				\toprule
				Volume                    & Genre                                        \\ \midrule
				\multicolumn{2}{l}{\texttt{15test} \textit{set:}}                        \\
				Aku-Ham                   & Four-frame cartoons                          \\
				Bakuretsu! Kung Fu Girl   & Romantic comedy                              \\
				Doll Gun                  & Battle                                       \\
				Eva Lady                  & Science fiction                              \\
				Hinagiku Kenzan\textit{!} & Love romance                                 \\
				Kyokugen Cyclone          & Sports                                       \\
				Love Hina vol. 1          & Romantic comedy                              \\
				Momoyama Haikagura        & Historical drama                             \\
				Tennen Senshi G           & Humor                                        \\
				Uchi no Nyan's Diary      & Animal                                       \\
				Unbalance Tokyo           & Science fiction                              \\
				Yamato no Hane            & Sports                                       \\
				Youma Kourin              & Fantasy                                      \\
				Yume no Kayoiji           & Fantasy                                      \\
				Yumeiro Cooking           & Love romance                                 \\ \midrule
				\multicolumn{2}{l}{\texttt{4val} \textit{set:}}                          \\
				Healing Planet            & Science fiction                              \\
				Love Hina vol. 14         & Romantic comedy                              \\
				Seijinki Vulnus           & Battle                                       \\
				That's! Izumiko           & Fantasy                                      \\ \midrule
				\multicolumn{2}{l}{\texttt{68train} \textit{set: All the other volumes}} \\ \bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-3mm}
	\caption{
		Manga109-s dataset splits (87 volumes in total).
	}
	\label{table:manga109_split}
\end{table}

The \textit{Manga109-s} dataset (87 volumes) is a subset of the full \textit{Manga109} dataset (109 volumes)~\cite{Manga109_Aizawa_IEEEMM2020}.
Unlike the full Manga109 dataset, the Manga109-s dataset can be used by commercial organizations.
The dataset splits for the full Manga109 dataset used in prior work~\cite{Manga109_detection_Ogawa_2018} cannot be used for the Manga109-s dataset.

We defined the Manga109-s dataset splits shown in Table~\ref{table:manga109_split}.
Unlike alphabetical order splits used in the prior work~\cite{Manga109_detection_Ogawa_2018}, we selected the volumes carefully.
The \texttt{15test} set was selected to be well-balanced for reliable evaluation.
Five volumes in the \texttt{15test} set were selected from the 10 test volumes used in~\cite{Manga109_detection_Ogawa_2018} to enable partially direct comparison.
All the authors of the \texttt{15test} and \texttt{4val} set are different from those of the \texttt{68train} set to evaluate generalizability.



\subsection{Number of Images}

There are
118,287 images in COCO \texttt{train2017},
5,000 in COCO \texttt{val2017},
79,735 in WOD \texttt{f0train},
20,190 in WOD \texttt{f0val},
6,760 in M109s \texttt{68train},
419 in M109s \texttt{4val}, and
1,354 in M109s \texttt{15test}.




\begin{table*}[t]
\setlength{\tabcolsep}{1.3mm}
\renewcommand\arraystretch{0.85}
\newcommand{\METHOD}{\multirow{2}{*}[-0.5\dimexpr \aboverulesep + \belowrulesep + \cmidrulewidth]{Method}}
\newcommand{\FPS}{\multirow{2}{*}[-0.5\dimexpr \aboverulesep + \belowrulesep + \cmidrulewidth]{FPS}}
\newcommand{\Bb}{Backbone}
\newcommand{\CMRs}{\cmidrule(l{.2em}r{.2em}){2-3}\cmidrule(l{.2em}r{.2em}){4-6}\cmidrule(l{.2em}r{.2em}){7-9}\cmidrule(l{.2em}r{.2em}){10-10}\cmidrule{12-17}}
\newcommand{\CITERRB}{\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels}}
\newcommand{\CITEiBNSBN}{\cite{SEPC_CVPR2020, MegDet_CVPR2018}}
\begin{center}
\scalebox{0.75}{\begin{tabular}{lcccccccccc@{\hspace{.9em}}cccccc}
	\toprule
	\METHOD                                      & \multicolumn{2}{c}{Head} & \multicolumn{3}{c}{Neck} & \multicolumn{3}{c}{\Bb} &  Input  & \FPS &             \multicolumn{6}{c}{COCO (1 schedule)}             \\
	\CMRs                                        & ATSS &        GFL         & PConv  &  DCN  &    iBN     & Res2 &  DCN  &   SyncBN    & MStrain &      &    AP     & AP & AP &   \APS    &   \APM    &   \APL    \\ \midrule
	RetinaNet~\cite{RetinaNet_ICCV2017} &      &                    &     &       &            &      &       &          &         & 33.9 &   36.5    &   55.4    &   39.1    &   20.4    &   40.3    &   48.1    \\
	ATSS~\cite{ATSS_CVPR2020}           & \cm  &                    &     &       &            &      &       &          &         & 35.2 &   39.4    &   57.6    &   42.8    &   23.6    &   42.9    &   50.3    \\
	GFL~\cite{GFL_NeurIPS2020}      & \cm  &        \cm         &     &       &            &      &       &          &         & 37.2 &   40.2    &   58.4    &   43.3    &   23.3    &   44.0    &   52.2    \\
	\ATSEPC~\cite{ATSS_CVPR2020, SEPC_CVPR2020}  & \cm  &                    & \cm & P, LC &            &      &       &          &         & 25.0 &   42.1    &   59.9    &   45.5    &   24.6    &   46.1    &   55.0    \\
	\OurOrig                                     & \cm  &                    & \cm & P, LC &            & \cm  & c3-c5 &          &   \cm   & 17.3 &   46.7    &   65.0    &   50.7    &   29.2    &   50.6    &   61.4    \\
	\OurGFL                                      & \cm  &        \cm         & \cm & P, LC &            & \cm  & c3-c5 &          &   \cm   & 17.5 &   47.5    &   65.8    &   51.8    &   29.2    &   51.6    &   62.5    \\
	\OurAugustD                                  & \cm  &        \cm         & \cm & P, LC &    \cm     & \cm  & c3-c5 &   \cm    &   \cm   & 17.3 & \TB{48.6} & \TB{67.1} & \TB{52.7} & \TB{30.1} & \TB{53.0} & \TB{63.8} \\
	\OurAugust                                   & \cm  &        \cm         & \cm &  LC   &    \cm     & \cm  &  c5   &   \cm    &   \cm   & 24.9 &   47.5    &   66.0    &   51.9    &   28.9    &   52.1    &   61.9    \\ \midrule
	\OurAugust w/o SEPC~\cite{SEPC_CVPR2020}                 & \cm  &        \cm         &     &       &            & \cm  &  c5   &   \cm    &   \cm   & 26.7 &   45.8    &   64.6    &   50.0    &   27.6    &   50.4    &   59.7    \\
	\OurAugust w/o Res2Net-v1b~\CITERRB                      & \cm  &        \cm         & \cm &  LC   &    \cm     &      &  c5   &   \cm    &   \cm   & 32.8 &   44.7    &   62.8    &   48.4    &   27.1    &   48.8    &   59.5    \\
	\OurAugust w/o DCN~\cite{DCN_ICCV2017}                   & \cm  &        \cm         & \cm &       &    \cm     & \cm  &       &   \cm    &   \cm   & 27.8 &   45.9    &   64.5    &   49.8    &   28.9    &   49.9    &   59.0    \\
	\OurAugust w/o iBN, SyncBN~\CITEiBNSBN                   & \cm  &        \cm         & \cm &  LC   &            & \cm  &  c5   &          &   \cm   & 25.7 &   45.8    &   64.0    &   50.2    &   27.9    &   50.0    &   59.8    \\
	\OurAugust w/o MStrain                                   & \cm  &        \cm         & \cm &  LC   &    \cm     & \cm  &  c5   &   \cm    &         & 24.8 &   45.9    &   64.5    &   49.6    &   27.4    &   50.5    &   60.1    \\ \bottomrule
\end{tabular}}
\end{center}
\vspace{-3.6mm}
\caption{
	Architectures of \Univs with a summary of ablation studies on COCO \texttt{minival}.
	See Sec.~\ref{sec:coco_ablation} for step-by-step improvements.
    All results are based on MMDetection~\cite{MMDetection} v2.
	The ``Head'' methods (ATSS and GFL) affect losses and training sample selection.
    Res2: Res2Net-v1b~\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels}.
    PConv (Pyramid Convolution) and iBN (integrated Batch Normalization) are the components of SEPC~\cite{SEPC_CVPR2020}.
    The DCN columns indicate where to apply DCN.
    ``P'': The PConv modules in the combined head of SEPC~\cite{SEPC_CVPR2020}.
    ``LC'': The extra head of SEPC for localization and classification~\cite{SEPC_CVPR2020}.
    ``c3-c5'': conv3\_x, conv4\_x, and conv5\_x layers in ResNet-style backbones~\cite{ResNet_CVPR2016}.
    ``c5'': conv5\_x layers in ResNet-style backbones~\cite{ResNet_CVPR2016}.
	\ATSEPC: ATSS with SEPC (without iBN).
	MStrain: Multi-scale training.
	FPS: Frames per second on one V100 with mixed precision.
}
\label{table:coco_ablation_details}
\end{table*}






\subsection{Exceptions}

The rounding error of epochs between epoch- and iteration-based training can be ignored when calculating the maximum epochs.
Small differences of eight pixels or less can be ignored when calculating the maximum resolution.
For example, DSSD513~\cite{DSSD_2017} will be compared in Mini USB.



\section{Details of \Univs}

We show the detailed architectures of \Univs
in Table~\ref{table:coco_ablation_details}.






\section{Details of Experiments}

Here, we show the details of experimental settings and results.
See also the code to reproduce our settings including minor hyperparameters.

\subsection{Common Settings}

We follow the learning rate schedules of MMDetection~\cite{MMDetection}, which are similar to those of Detectron~\cite{Detectron2018}.
Specifically, the learning rates are reduced by 10 in two predefined epochs.
Epochs for the first learning rate decay, the second decay, and ending training are
 for the 1 schedule,
 for the 2 schedule, and
 for the 20e schedule.
To avoid overfitting by small learning rates~\cite{Shinya_ICCVW2019},
the 20e schedule is reasonable.


We mainly used ImageNet pre-trained backbones that are standard in MMDetection~\cite{MMDetection}.
Some pre-trained Res2Net backbones not supported in MMDetection were downloaded from the Res2Net repository~\cite{Res2Net_PretrainedModels}.
We trained most models with mixed precision and 4 GPUs ( 4 images per GPU).
All results on USB and all results of \Univs are single model results without ensemble.






\subsection{Settings on COCO}


For comparison with state-of-the-art methods with TTA on COCO,
we used soft voting with 13-scale testing and horizontal flipping following the original implementation of ATSS~\cite{ATSS_CVPR2020}.
Specifically,
shorter side pixels are (400, 500, 600, 640, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1800),
while longer side pixels are their 1.667.
For the 13 test scales, target objects are limited to corresponding 13 predefined ranges
((96, ), (96, ), (64, ), (64, ), (64, ), (0, ), (0, ), (0, ), (0, 256), (0, 256), (0, 192), (0, 192), (0, 96)),
where each tuple denotes the minimum and maximum object scales.
Each object scale is measured by ,
where  and  denote the object's width and height, respectively.
We also evaluated 5-scale TTA
because the above-mentioned ATSS-style TTA is slow.
We picked (400, 600, 800, 1000, 1200) for shorter side pixels,
and ((96, ), (64, ), (0, ), (0, ), (0, 256)) for object scale ranges.




\subsection{Settings on NightOwls}

NightOwls~\cite{NightowlsDataset_ACCV2018} is a dataset for person detection at night.
It contains three categories (pedestrian, bicycle driver, and motorbike driver).
In contrast to WOD, it is important to detect medium or large objects
because the evaluation of NightOwls follows the \textit{reasonable} setting~\cite{Caltech_PAMI2012}
where small objects (less than 50 pixels tall) are ignored.
We prevented the overfitting of the driver categories (bicycle driver and motorbike driver) in two ways.
The first is to map the classifier layer of the WOD pre-trained model.
We transferred the weights for cyclists learned on the richer WOD to those for the NightOwls driver categories.
The second is early stopping.
We trained the model for 2 epochs (4,554 iterations) without background images.









\subsection{Ablation Studies for \Univs}
\label{sec:coco_ablation}

\ificcvfinal
We describe the results of ablation studies for \Univs on COCO in more detail.
As shown in Table~\ref{table:coco_ATSEPC}, \ATSEPC (ATSS~\cite{ATSS_CVPR2020} with SEPC without iBN~\cite{SEPC_CVPR2020}) outperforms ATSS by a large margin.
The effectiveness of SEPC for ATSS is consistent with those for other detectors reported in the SEPC paper~\cite{SEPC_CVPR2020}.
As shown in Table~\ref{table:coco_OurOrig}, \OurOrig further improves AP metrics by 5\%
by adopting Res2Net-v1b~\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels}, DCN~\cite{DCN_ICCV2017}, and multi-scale training.
As shown in Table~\ref{table:coco_OurGFL}, adopting GFL~\cite{GFL_NeurIPS2020} improves AP by 0.8\%.
There is room for improvement of \APS in the Quality Focal Loss of GFL~\cite{GFL_NeurIPS2020}.
As shown in Table~\ref{table:coco_OurAugustD},
\OurAugustD achieves 48.6\% AP by making more use of BatchNorm (SyncBN~\cite{MegDet_CVPR2018} and iBN~\cite{SEPC_CVPR2020}).
It is much more accurate than other models trained for 12 epochs using ResNet-50-level backbones (\eg, ATSS: 39.4\%~\cite{ATSS_CVPR2020, MMDetection}, GFL: 40.2\%~\cite{GFL_NeurIPS2020, MMDetection}).
On the other hand, the inference is not so fast (less than 20 FPS) due to the heavy use of DCN~\cite{DCN_ICCV2017}.
\OurAugust speeds up inference by the light use of DCN~\cite{DCN_ICCV2017, SEPC_CVPR2020}.
As shown in Table~\ref{table:coco_OurAugust},
\OurAugust is 1.4 faster than \OurAugustD at the cost of a 1\% AP drop.
To further verify the effectiveness of each technique, we conducted ablation from \OurAugust shown in Table~\ref{table:coco_OurAugust_ablation}.
All techniques contribute to the high AP of \OurAugust.
Ablating the Res2Net-v1b backbone (replacing Res2Net-50-v1b~\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels} with ResNet-50-B~\cite{BagOfTricks_Classification_CVPR2019}) has the largest effects.
Res2Net-v1b improves AP by 2.8\% and increases the inference time by 1.3.
To further investigate the effectiveness of backbones, we trained variants of \OurAugust as shown in Table~\ref{table:coco_backbone}.
Although the Res2Net module~\cite{Res2Net_TPAMI2020} makes inference slower,
the deep stem used in ResNet-50-C~\cite{BagOfTricks_Classification_CVPR2019} and Res2Net-50-v1b~\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels}
improves AP metrics with similar speeds.
\OurAugustS (the variant using ResNet-50-C backbone) shows a good speed-accuracy trade-off by achieving 45.8\% AP and over 30 FPS.
\else
We describe the results of ablation studies for \Univs on COCO in more detail.
As shown in Table~12a, \ATSEPC (ATSS~\cite{ATSS_CVPR2020} with SEPC without iBN~\cite{SEPC_CVPR2020}) outperforms ATSS by a large margin.
The effectiveness of SEPC for ATSS is consistent with those for other detectors reported in the SEPC paper~\cite{SEPC_CVPR2020}.
As shown in Table~12b, \OurOrig further improves AP metrics by 5\%
by adopting Res2Net-v1b~\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels}, DCN~\cite{DCN_ICCV2017}, and multi-scale training.
As shown in Table~12c, adopting GFL~\cite{GFL_NeurIPS2020} improves AP by 0.8\%.
There is room for improvement of \APS in the Quality Focal Loss of GFL~\cite{GFL_NeurIPS2020}.
As shown in Table~12d,
\OurAugustD achieves 48.6\% AP by making more use of BatchNorm (SyncBN~\cite{MegDet_CVPR2018} and iBN~\cite{SEPC_CVPR2020}).
It is much more accurate than other models trained for 12 epochs using ResNet-50-level backbones (\eg, ATSS: 39.4\%~\cite{ATSS_CVPR2020, MMDetection}, GFL: 40.2\%~\cite{GFL_NeurIPS2020, MMDetection}).
On the other hand, the inference is not so fast (less than 20 FPS) due to the heavy use of DCN~\cite{DCN_ICCV2017}.
\OurAugust speeds up inference by the light use of DCN~\cite{DCN_ICCV2017, SEPC_CVPR2020}.
As shown in Table~12e,
\OurAugust is 1.4 faster than \OurAugustD at the cost of a 1\% AP drop.
To further verify the effectiveness of each technique, we conducted ablation from \OurAugust shown in Table~12f.
All techniques contribute to the high AP of \OurAugust.
Ablating the Res2Net-v1b backbone (replacing Res2Net-50-v1b~\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels} with ResNet-50-B~\cite{BagOfTricks_Classification_CVPR2019}) has the largest effects.
Res2Net-v1b improves AP by 2.8\% and increases the inference time by 1.3.
To further investigate the effectiveness of backbones, we trained variants of \OurAugust as shown in Table~12g.
Although the Res2Net module~\cite{Res2Net_TPAMI2020} makes inference slower,
the deep stem used in ResNet-50-C~\cite{BagOfTricks_Classification_CVPR2019} and Res2Net-50-v1b~\cite{Res2Net_TPAMI2020, Res2Net_PretrainedModels}
improves AP metrics with similar speeds.
\OurAugustS (the variant using ResNet-50-C backbone) shows a good speed-accuracy trade-off by achieving 45.8\% AP and over 30 FPS.
\fi


















\begin{figure}[t]
	\centering
	\begin{minipage}[b]{0.98\linewidth}
		\centering
		\includegraphics[width=\textwidth]{images/wod_ap_test_scale_cap_thinline.pdf}
		\vspace{-6mm}
		\subcaption{COCO-style AP}
		\label{fig:waymo_scales_cap_supmat}
		\vspace{2mm}
	\end{minipage}
	\begin{minipage}[b]{0.98\linewidth}
		\centering
		\includegraphics[width=\textwidth]{images/wod_ap_test_scale_kap_thinline.pdf}
		\vspace{-6mm}
		\subcaption{KITTI-style AP}
		\label{fig:waymo_scales_kap_supmat}
	\end{minipage}
	\caption{
		Test scales \vs different AP metrics on WOD \texttt{f0val}.
	}
	\label{fig:waymo_scales_supmat}
\end{figure}

\subsection{Differences by Metrics}
To analyze differences by metrics,
we evaluated the KITTI-style AP (KAP) on WOD.
KAP is a metric used in benchmarks for autonomous driving~\cite{KITTI_CVPR2012, WaymoOpenDataset_CVPR2020}.
Using different IoU thresholds (0.7 for vehicles, and 0.5 for pedestrians and cyclists),
KAP is calculated as

The results of KAP are shown in Figure~\ref{fig:waymo_scales_kap_supmat}.
For ease of comparison, we show again the results of CAP in Figure~\ref{fig:waymo_scales_cap_supmat}.
GFL~\cite{GFL_NeurIPS2020} and Cascade R-CNN~\cite{Cascade_R-CNN_CVPR2018}, which focus on localization quality, are less effective for KAP. 





\subsection{Effects of COCO Pre-Training}

To verify the effects of COCO pre-training,
we trained \OurAugust on \MangasAbbr from different pre-trained models.
Table~\ref{table:Manga109s_15test_pretrain} shows the results.
COCO pre-training improves all the metrics, especially body AP.

We also trained models with the eight methods on \MangasAbbr from ImageNet pre-trained backbones.
We halved the learning rates in Table~\ref{table:hyperparameters} and doubled warmup iterations~\cite{Goyal2017AccurateLM} (from 500 to 1,000)
because the training of single-stage detectors without COCO pre-training or SyncBN~\cite{MegDet_CVPR2018} is unstable.
The CAP without COCO pre-training is 1.9\% lower than that with COCO pre-training (Table~\ref{table:usb}) on average.






\begin{table}[t]
	\setlength{\tabcolsep}{0.9mm}
	\renewcommand\arraystretch{0.72}
	\begin{center}
		\scalebox{0.8}{\begin{tabular}{lcccccccccc}
			\toprule
			Pre-training   &    AP     & AP & AP &   \APS    &   \APM    &   \APL    &   body    &   face    &   frame   &   text    \\ \midrule
			ImageNet       &   68.9    &   92.2    &   73.3    &   19.9    &   42.6    &   75.8    &   64.3    &   47.6    &   93.0    &   70.7    \\
			COCO 1 & \TB{69.9} & \TB{92.5} & \TB{74.3} & \TB{20.5} & \TB{43.6} & \TB{77.1} & \TB{66.6} & \TB{48.0} &   93.7    & \TB{71.2} \\
			COCO 2 &   69.8    &   92.3    &   74.0    & \TB{20.5} &   43.4    &   77.0    &   66.5    &   47.8    & \TB{93.8} & \TB{71.2} \\ \bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-3mm}
	\caption{
		\OurAugust fine-tuned on Manga109-s \texttt{15test} from different pre-trained models.
	}
	\label{table:Manga109s_15test_pretrain}
\end{table}










 
\end{document}
