\documentclass[twoside]{article}

\usepackage[preprint]{aistats2024}




\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}








\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{notation}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{stfloats}  \usepackage{algpseudocode}
\usepackage{subfig}
\usepackage{xr}
\usepackage{algorithm}
\usepackage{MnSymbol}
\usepackage{tabularx}
\usepackage{comment}



\makeatletter
\newcommand*{\addFileDependency}[1]{\typeout{(#1)}
\@addtofilelist{#1}
\IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother
\newcommand*{\myexternaldocument}[1]{\externaldocument{#1}\addFileDependency{#1.tex}\addFileDependency{#1.aux}}
\myexternaldocument{supplement}


\begin{document}
\twocolumn[

\aistatstitle{Optimal Budgeted Rejection Sampling for Generative Models}

\aistatsauthor{Alexandre Verine \And Muni Sreenivas Pydi   \And  Benjamin Negrevergne \And Yann Chevaleyre }

\aistatsaddress{ LAMSADE, CNRS,\\
Université Paris-Dauphine,\\
Université PSL,\\
Paris, France. \And LAMSADE, CNRS,\\
Université Paris-Dauphine,\\
Université PSL,\\
Paris, France. \And LAMSADE, CNRS,\\
Université Paris-Dauphine,\\
Université PSL,\\
Paris, France. \And LAMSADE, CNRS,\\
Université Paris-Dauphine,\\
Université PSL,\\
Paris, France. } ]


\begin{abstract}

Rejection sampling methods have recently been proposed to improve the performance of discriminator-based generative models. 
However, these methods are only optimal under an unlimited sampling budget, and are usually applied to a generator trained independently of the rejection procedure. 
We first propose an Optimal Budgeted Rejection Sampling (OBRS) scheme that is provably optimal with respect to \textit{any} -divergence between the true distribution and the post-rejection distribution, for a given sampling budget. Second, 
we propose an end-to-end method that incorporates the sampling scheme into the training procedure to further enhance the model's overall performance. 
Through experiments and supporting theory, we show that the proposed methods are effective in significantly improving the quality and diversity of the samples.  

\end{abstract}


\section{Introduction}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/Surfacesclean.pdf}
    \caption{The loss landscape in the parameter domain of a GAN trained on MNIST. The x-axis and y-axis are random directions in the parameter space. The loss is between the target distribution  and the post-rejection distribution. There are three cases: no rejection (),   acceptance rate () and   acceptance rate (). OBRS not only reduces loss, but also flattens out the loss landscape and helps avoid  local minima.}\label{fig:MNISTSmooth}
\end{figure*}
Generative Adversarial Networks (GANs)  have significantly improved generation of complex, high dimensional data. In the original paper by \citet{goodfellow_generative_2014}, GANs are trained to  minimize the Jensen-Shannon divergence between true distribution  and a distribution  induced by a generator .
Since  is generally unknown, the divergence between  and  is estimated using a discriminator , i.e. a function that discriminates available samples from  and samples generated from . In practice  and  are  represented using neural networks and trained simultaneously to estimate the divergence and to minimize it. In this paper, we consider the more general framework of -GAN introduced by \citet{nowozin_f-gan_2016}, which  can be used to minimize {\em any} \fdiv between  and  , including the Jensen-Shannon divergence, the Kullback-Leibler divergence or other divergences (See Table~\ref{tab:fdiv}).

In most settings, the discriminator is not involved in the generation of new samples beyond the training phase (i.e. it is discarded after training). Building on this observation, several methods  such as  Discriminator Rejection Sampling (DRS) \citep{azadi_discriminator_2019} or Metropolis-Hastings GAN \citep{turner_metropolis-hastings_2019} have demonstrated how to combine  \emph{and}  using rejection sampling, in order to generate better samples than the ones generated using  alone. 
In the rest of this paper, we call  the distribution resulting from  enhanced with rejection sampling.



Unfortunately, these methods suffer from several limitations. First they are only provably optimal when the sampling budget is unlimited. In practice, users have to limit the rejection rate to obtain samples in reasonable time through various empirical means (e.g. by capping the number of iterations of the sampling algorithm). This strategy may not  yield the best possible sample for the given budget, an observation that leads to the first question that motivated our contribtion. 

\begin{question}\label{question:q1}
    How to devise a method that generates the best quality sample under a fixed rejection budget?  
\end{question}

Another important limitation is that, since examples are sampled from  rather than , the objective should be to minimize the divergence between  and  rather than the divergence between  and . This raises a  second research question that we address in this paper:



\begin{question}\label{question:q2}
    Can we train a generator  that directly minimizes  instead of  ?
\end{question}

In this paper, we address Question~\ref{question:q1}\&\ref{question:q2}, by making following contributions:
\begin{itemize}
    \item    
    We introduce ORBS, a method that can be used to find an  {\em acceptance function} required to reject/accept samples from  and show in Theorem~\ref{thm:optrej} that this function induces the optimal distribution  under a budget, for \emph{any} \fdiv. 
    \item We characterize the improvement of  over  in terms of Precision and Recall~\citep{sajjadi_assessing_2018} in Theorem~\ref{thm:improvalpha}.
    \item We propose a method to train a generator  to directly minimize an \fdiv between  and , and we discuss the potential benefits of our method. For example, in Figure~\ref{fig:MNISTSmooth}, we illustrate how OBRS can flatten the loss landscape.    
\end{itemize}

\textbf{Notation: } For the rest of the paper, we use  to refer to the data space. We use  to denote the set of probability measures on  defined on a measure space with the Borel -algebra. We use capital letters to denote probability measures (for e.g., ) and small letters to denote their densities (for e.g.,  for ).

\section{Background}
\subsection{\fdivs}

The framework of \fdivs can be used to specify a variety of divergences between two probability distributions. 
An \fdiv is fully characterized by a convex and lower semi-continuous function  that satisfies  . Given  and two probability distributions   and , the \fdiv  between ,  (denoted ) is defined as follows:


(We assume that  is absolutely continuous with w.r.t. .) Several notable statistical divergences, such as the Kullback-Leibler (KL) divergence (), the reverse KL divergence (), or the Total Variation (), belong to the class of \fdivs. 
A  overview is provided in Table~\ref{tab:fdiv}.

A key property of \fdivs is that every \fdiv  admits a dual variational form \citep{nguyen_surrogate_2009}:

where  be the set of all measurable functions  and  is the convex conjugate of . Specifically, the function  that yields the supremum in \eqref{eq:dual} can be used to determine the likelihood ratio  as follows.





\begin{table*}[t]
\caption{List of common \fdivs. The generator  is given with its Fenchel conjugate . The optimal discriminator  is given to compute the likelihood ratio . }
\label{tab:fdiv}
\begin{sc}
\begin{center}

\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} ccccc}
\toprule
Divergence & Notation &  &  &  \\
\midrule \addlinespace[0.5em]
KL &  & &  &   \\ \addlinespace[0.4em]
GAN &  & &  &   \\ \addlinespace[0.4em]
PR &  &  &  &     \\ \addlinespace[0.4em]
\bottomrule
\end{tabular*}
\end{center}
\end{sc}
\end{table*}

\subsection{-GAN, a generalization of GAN}\label{subsec:fgan}
Let  be the set of all measurable functions , where  is the latent space and  is the data space. 
In the -GAN framework, the generator  is used to transform samples from the latent distribution  (typically a multivariate Gaussian) into data samples following the data distribution  .  is chosen to minimize the -divergence  

Since  is usually not available, a discriminator  is used to estimate  through the dual variational form in \eqref{eq:dual}, resulting in the following minimax objective \citep{nowozin_f-gan_2016}.

The optimization procedure is detailed in Alogrithm~\ref{alg:naiveapproach}. 
An important special case is that of the original paper of \cite{goodfellow_generative_2014}, where , and the minimax objective is as follows:




\subsection{Rejection Sampling}\label{subsec: rejection sampling}
\label{subsec:rejsam}
Rejection Sampling is a classical method to generate samples from a distribution using samples drawn from a different distribution. In the context of this paper,  samples drawn from  are accepted or rejected using an {\em acceptance function} , where  is the probability of accepting a sample  from . The distribution induced by the rejection procedure based on  is a new distribution in  denoted . 
The density  of  has the following form:
 
where  is a normalizing constant that ensures that . The overall acceptance rate is . Note that .
If  are known, and if there are no constraints on the sampling budget (i.e., no lower limit on ), then  can be set to  with  so that  matches perfectly the target distribution  because  and we have . 
However in practice for high-dimensional ,  can take high values and set a very low acceptance rate \citep{mackay_information_2005}.




\paragraph{Rejection Sampling for GANs:}
\cite{azadi_discriminator_2019} propose \emph{Discriminator Rejection Sampling (DRS)} scheme wherein a trained discriminator  is used to approximate the likelihood ratio via the formula,

which is an approximation of \eqref{eq:densityestimation}.
Thus, the acceptance function of DRS is given by ,
where  is estimated using samples . To account for low acceptance rate, DRS uses a hyper parameter  to adjust the acceptance rate as,

In practice, the discriminator  is calibrated such that  which results in an overall acceptance rate of . A low value of  (typically ) boosts the acceptance rate. 

\paragraph{Related sampling methods:} The introduction of DRS has lead to the development of numerous sampling methods that are also applicable to GANs, such as MH-GAN \citep{turner_metropolis-hastings_2019}, DDLS \citep{che_your_2021}, DOT \citep{tanaka_discriminator_2019}, and DGlow \citep{ansari_refining_2021}, LatentRS \citep{issenhuth_latent_2022} and even for Normalizing Flows \citep{stimper_resampling_2022}. These methods, relying on gradient ascent or the training of a latent model, have showcased their potential through various applications. However, the sampling is computationally expensive and are not as efficient under a limited time constraint.    
\paragraph{Accounting for rejection during training:} While the majority of methods employ the rejection sampling scheme post-training, incorporating an \textit{a priori} perspective on the sampling procedure also yields good results empirically. For example, \citet{grover_variational_2018} and  \citet{stimper_resampling_2022} have embedded latent rejection sampling within their training processes, applying it within a variational inference context and a Normalizing Flow framework, respectively. 





\section{Optimal Budgeted Rejection Sampling (OBRS)}
\label{sec:optirej}
Rejection sampling exhibits a well-established efficiency on low-dimensional samples; but the acceptance rate drops when it is applied to higher dimensional samples \citep{mackay_information_2005}. In this section, we study the problem of rejection sampling under a limited sampling budget , where  represents the expected  number of samples drawn from  required to generate a sample from . We start by introducing a method to find the optimal acceptance function under a given budget  (thus addressing research Question~\ref{question:q1}), then  we characterize the improvement provided by this new method using  Precision and Recall for generative models \citep{sajjadi_assessing_2018}. 

\subsection{Optimal acceptance function}

We recall that  is the true data distribution,  (or  for short) is the distribution induced by the generator, and  is the distribution obtained by applying the acceptance function  on samples from . Given a fixed , our goal is to find the acceptance function  that minimizes the  divergence between  and  under a budget , as follows: 

Here the constraint  is used to bound the expected acceptance rate. For , the only  satisfying the constraints in \eqref{eq:divproblem} is the unit function . This case corresponds to no rejection (or accept w.p. ) and we have  almost everywhere.

Note that the objective  is continuous with respect to . Since the constraint set for  is closed and bounded, there exists an optimal  for problem~\eqref{eq:divproblem}. In the following theorem, we give an explicit form for the optimal solution  for finite  using Lagrangian~duality.



\begin{theorem}[Optimal Acceptance Function]\label{thm:optrej}
For a sampling budget  and finite , the solution to problem (\ref{eq:divproblem}) is,

where  is such that .
\footnote{This acceptance function was previously introduced by \cite{grover_variational_2018}), with the sole argument that it is a "natural" approximation of the optimal acceptance function. No theoretical argument was provided.} 
\end{theorem}
Few observations should be made on Theorem~\ref{thm:optrej}:
\begin{itemize}
    \item The constant  is solely determined by . In practice, we can compute it using a dichotomy algorithm (detailed in Appendix~\ref{app:subsec:algoc}).
    \item A budget greater than  (unbudgeted sampling) implies that , and thus . 
    \item The optimal function  does not depend on  meaning that OBRS is optimal for various  \fdivs including ones that are more sensitive to covering the probability mass or ones that are more sensitive capturing modes. This observation is the base of our analysis on how the OBRS improves Precision and Recall in Section~\ref{subsec:OBRSPR}
\end{itemize}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/1D_Gaussians2tmp.pdf}
    \caption{Comparing Unbudgeted, DRS \citep{azadi_discriminator_2019} and OBRS (ours) for a one-dimensional example. DRS and OBRS are tuned to reach an acceptance ratio of . TL: The target and learned distributions  and , along the refined distributions. TR: The acceptance functions for the unbudgeted rejection sampling (dotted black), OBRS (blue), and the DRS (green). BL: The PR-Curves of the different models. BR: Visualisation of the improvements by the OBRS. The straight dotted line corresponds to .
    }
    \label{fig:1D}
\end{figure}


Figure~\ref{fig:1D} illustrates Theorem~\ref{thm:optrej} on a one-dimensional example. On the top-left of Figure~\ref{fig:1D}, we draw  and , where the target distribution . On the top-right, DRS (green) and OBRS (blue) are compared.  We can observe how  and  lead to different refined distributions .

Finally, we present a theorem showing how much OBRS reduces the -divergence in general. We show that for any \fdiv, the improvement is linear to . A tighter version of this result for  is in Appendix~\ref{app:sec:bounds}.
\begin{theorem}\label{thm:bound}
For any -divergence,
we have 

\end{theorem}

\subsection{Improvement on the Precision/Recall}\label{subsec:OBRSPR}

A number of recent publications 
have stressed the importance of measuring the quality of generative models using precision and recall  \citep{ kynkaanniemi_improved_2019,djolonga_precision-recall_2020, naeem_reliable_2020, cheema_precision_2023, kim_toppr_2023}. In the context of generative modeling, \emph{precision} measures the quality of the generated samples,  while \emph{recall} which measures the diversity of the samples. In this section, we introduce Theorem~\ref{thm:improvalpha}, that provide a clear characterization of the improvement provided by OBRS in terms of precision and recall. 








To model the set of all precision-recall tradeoffs, \cite{simon_revisiting_2019} introduced the notion of \emph{Precision-Recall Curve} between to distributions  and . This curve, named ,  is composed of all coordinate points  defined as follows.
3pt]
        \beta_\lambda =\E_{P}\left[\min\left\{1,\frac{\whp(\vx)}{p(\vx)}\frac{1}{\lambda}\right\}\right]
\end{cases}
r(\vx)= \nabla f\s (\T(\vx))
    B_{K_1}(\whP) \subseteq B_{K_2}(\whP).

    \text{First solve }\whP\opt\in \argmin_{\whP\in \whcP} \Df (P\Vert \whP);\label{eq: obj train}\\
    \text{Next solve }\wtP\opt\in \argmin_{\wtP\in B_K(\whP\opt)} \Df(P\Vert \wtP).\label{eq: obj RS}
\label{eq:trainobrs_prequel}
 \min_{\whP\in\wh{\cal P}}\min_{\wtP\in B_{K}(\whP)}\Df(P\Vert\wtP).
\label{eq:trainobrs}
    \min_{\wtP\in \bigcup_{\whP \in \wh{\cal P}}B_K(\whP)}  \Df(P\Vert \wtP).

  \E_{\vx\sim P}\left[T(\vx) \right] - \E_{\vx\sim \wh P_G}\left[ f^*(T(\vx))\right].
   - \E_{\vx\sim \wh P_G}\left[ f^*(T(\vx))\right].\E_{\vx\sim P}\left[T(\vx) \right] - \E_{\vx\sim \wh P_G}\left[ f^*(T(\vx))\right].
   \E_{\vx\sim \wh P_G}\left[K\aobs(\vx) f\left(\frac{r\left(\vx\right)}{K\aobs(\vx)}\right)\right].
    -\E_{\wtP}\left[f\s(\widetilde T (\vx))\right]=-\E_{\whP}\left[Ka(\vx)f\s(\widetilde T (\vx))\right].

        \Df(P\Vert \wtP) 
        &= \E_{\whP}\left [ f\left( \frac{p(\vx)}{\widetilde{p}(\vx)}\right)  \right]\\
        &= \E_{\wtP}\left [ \frac{\widetilde{p}(\vx)}{\whp(\vx)} f\left( \frac{p(\vx)}{\widehat{p}(\vx)}\frac{\whp(\vx)}{\widetilde{p}(\vx)}\right)  \right]\\
        &= \E_{\whP}\left [ Ka(\vx) f\left(\frac{\nabla f\s\left(T(\vx)\right)}{Ka(\vx)}\right)\right],

P = \beta \mu +(1-\beta)\nu_P \quad \mbox{and}\quad \whP = \alpha \mu + (1-\alpha )\nu_{\whP}

    \alpha(\lambda) = \sum_{\vx_i\in\Xset} \min\left(\lambda p(\vx_i), \whp(\vx_i) \right) \quad \mbox{and} \quad  \beta(\lambda) =  \sum_{\vx_i\in\Xset} \min\left( p(\vx_i), \whp(\vx_i)/\lambda \right) 

P \geq \beta \mu  \quad \mbox{and}\quad \whP  \geq \alpha \mu. 
    \alpha(\lambda) = \int_\Xset \min\left(\lambda p(\vx), \whp(\vx)\right)\d \vx \et \beta(\lambda) = \int_\Xset \min\left( p(\vx), \whp(\vx)/\lambda\right)\d \vx.

\begin{cases}
        \alpha_\lambda = \E_{\whP}\left[\min\left\{\lambda \frac{p(\vx)}{\whp(\vx)}, 1\right\}\right] \
We can interpret this expression similar to the AUC curve in classification tasks. Consider that the maximum precision and recall are one. Therefore, whenever a point is sampled from  such that , the precision decreases further away than . In other terms, all the  for which the  overestimate  decrease the precision. On the side,  whenever a point is sampled from  such that , the recall decreases further away than , corresponding to the points where  underestimates . Let us consider two examples in Figure~\ref{app:fig:goodrecall}~and~\ref{app:fig:goodprecision}.

\begin{figure}[H]
\begin{minipage}[c]{0.3\textwidth}
    \begin{figure}[H]
        \centering
\includegraphics[width=\textwidth]{figures/PRexample_model_PR_recall.pdf}
        \caption{A target distribution  and the approximated distribution . In this setup, the model is expected to have a decent recall since it covers  but a poor precision since almost half the weight of  does not cover .  In Figures~\ref{app:fig:goodrecalllowl}~and~\ref{app:fig:goodrecallhighl}, we show the PR-Cruve and how it is computed.}
        \label{app:fig:goodrecall}
    \end{figure}
\end{minipage}
\hfill
\begin{minipage}[c]{0.65\textwidth}
    \begin{figure}[H]
        \subfloat[PR-Curve for the model in Figure~\ref{app:fig:goodrecall}, explained for a low . The area in red is  and the area in blue is .]{\includegraphics[width=\textwidth]{figures/PR_recall_5.pdf}        \label{app:fig:goodrecalllowl}}       
        
        \subfloat[PR-Curve for the model in Figure~\ref{app:fig:goodrecall}, explained for a high . The area in red in  and the area in blue is .]{\includegraphics[width=\textwidth]{figures/PR_recall_14.pdf}        \label{app:fig:goodrecallhighl}}
        \caption{PR-Curves for the model in Figure~\ref{app:fig:goodrecall}}
    \end{figure}
\end{minipage}
\end{figure}

\begin{figure}[H]
\begin{minipage}[c]{0.3\textwidth}
    \begin{figure}[H]
        \centering
\includegraphics[width=\textwidth]{figures/PRexample_model_PR_precision.pdf}
        \caption{A target distribution  and the approximated distribution . In this setup, the model is expected to have a poor recall since it covers almost only half the weight of  but a decent precision since the weight covers the contours of  well. In Figures~\ref{app:fig:goodprecisionlowl}~and~\ref{app:fig:goodprecisionhighl}, we represent the PR-Cruve and how it is computed.}
        \label{app:fig:goodprecision}
    \end{figure}
\end{minipage}
\hfill
\begin{minipage}[c]{0.65\textwidth}
    \begin{figure}[H]
        \subfloat[PR-Curve for the model in Figure~\ref{app:fig:goodprecision}, explained for a low . The area in red is  and the area in blue is .]{\includegraphics[width=\textwidth]{figures/PR_precision_5.pdf}\label{app:fig:goodprecisionlowl}}
        
        \subfloat[PR-Curve for the model in Figure~\ref{app:fig:goodprecision}, explained for a high . The area in red in  and the area in blue is .]{\includegraphics[width=\textwidth]{figures/PR_precision_14.pdf}        \label{app:fig:goodprecisionhighl}}
        \caption{PR-Curves for the model in Figure~\ref{app:fig:goodprecision}}
    \end{figure}
\end{minipage}
\end{figure}

\subsection{Precision and Recall in practice}
To perfectly compute the set , one needs the ratio  for all . In practice, a variety of heuristics are employed.
\cite{sajjadi_assessing_2018} use -NN based algorithm in the Inception latent space to estimated the densities. \cite{simon_revisiting_2019} use an ensemble of classifiers in Inception's latent space to estimate the likelihood ratio. \cite{verine_precision-recall_2023}  use a neural network based discriminator, simlarly to -GANs, to estimate the likelihood ratio. With these methods, we can compute the PR-Curve for high dimensionnal dataset such as MNIST: see Figure~\ref{app:fig:PRMNIST}.

 \begin{figure}[H]
 \centering
    \subfloat[Model 1: High Recall\\FID: , IS: ]{\hspace{0.03\textwidth}\includegraphics[width=0.15\textwidth]{figures/random_samples_1.jpg}\label{fig:model1}\hspace{0.07\textwidth}}
    \subfloat[Model 2: High Precision\\FID: , IS:]{\hspace{0.03\textwidth}\includegraphics[width=0.15\textwidth]{figures/random_samples_2.jpg}\hspace{0.07\textwidth}}
    \subfloat[PR-Curves for Model 1 and 2.]{\hspace{0.07\textwidth}\includegraphics[width=0.25\textwidth]{figures/PRcurves12.pdf}\label{fig:prcruves12} \hspace{0.07\textwidth}}
    \caption{Two different models are displayed with very different performances. Model 1 have a great diversity and display all different digits, but contours, backgrounds and shapes are sometimes incoherent. Model 2 is generating coherent samples from only half the classes. Traditional metrics - FID () and IS () - are given for comparison. }
    \label{app:fig:PRMNIST}
\end{figure}
\section{Proof and Supplementary for Section~\ref{sec:optirej}}
\subsection{Proof for Theorem~\ref{thm:optrej}}\label{app:sec:proofoptirej}
The goal is to find an acceptance function  that first minimizes the \fdiv between the target distribution  and the distribution after the rejection process . A budget is added to the problem in order to avoid low acceptance rate. We set the budget to be , the average number of samples to draw before accepting one. With a budget of , the average acceptance rate is  . In analogy with the unlimited budget rejection process, the average number of samples to draw in order to keep one is . The function  is the solution of the problem:

First, we can consider  instead of  without loss of generality: This is because   for . Further, the solution to the optimal  turns out to be independent of .

Moreover, we can assume that the budget is always lower that the unlimited budget. In other terms, instead of forcing the acceptance rate to be greater to  we can force is to be exactly equal to . Then, the probability of acceptance being ,  we can write an equivalent problem as: 



Using the definition of the densities in the rejection sampling context, , the problem is equivalent to:


Switching to the discrete case, the problem becomes :


The Lagrangian function associated with the problem  \ref{eq:app:discreteprob} is :

All constraints are affine and the objective function is a convex function, therefore the optimal vector  satisfies the KKT conditions: 

Using the 1st condition: 


Since :

If the Pearson  is put aside, all the usual  are strictly increasing functions. Therefore, according to Eq~\ref{eq:aifs}, all . Thus all .
The KKT conditions~\ref{eq:KKTl1l2} become :


And thus :

To get the full formula for , we need to compute the s. For this purpose, let us use strong duality to reformulate our problem:

Then, we can use the Fenchel Conjugate:





Define , assuming 
everywhere. Note that the constraints  and 
are equivalent. The above equation becomes



Let us make another change of variable to make a conjugate form appear.
Define . So 
and the constraint  becomes .
Also, define . Then .
Above equation becomes 



Recall that 
and .
Thus, given  we can compute the optimal values of 
one by one as follows:



So . This gives us the optimal values of .
Note that . Replacing  by 
in the formula of  gives us:



Note that  is strictly increasing, thus:



Note that  is a constant.
So the optimal acceptance function under budget looks like 
for some constant  defined by K only as:

To facilitate the understanding of , we can set this constant to be equal to  instead. Thus,

With that notation,  and if the optimal unlimited acceptance function is obtained with :  




\subsection{Algorithm to compute } \label{app:subsec:algoc}
In Section~\ref{sec:optirej},  we show that the optimal acceptance function is

The constant  is determined exclusively by the budget . In practice, we can draw a set of samples from  and adjust  to obtain the correct budget. We use a dichotomy algorithm detailed in Algorithm~\ref{alg:searchc}.
\begin{algorithm}[H]
\caption{Dichotomy to compute .}
\label{alg:searchc}
\textbf{Input}: N generated samples  \\
\textbf{Parameter}: Budget , Threshold \\
\textbf{Output}: Constant 
\begin{algorithmic}[1] \State Let  and .
\State 
\State Define the loss 
\While{ }
\If{}
\State 
\ElsIf{}
\State 
\EndIf
\State Update: 
\State Update: 

\EndWhile

\end{algorithmic}
\end{algorithm}


\subsection{Proof for Theorem~\ref{thm:improvalpha}}\label{app:sec:improvalpha}
First, with , let us recall that 

Thus:

Naturally, the precision can be evaluated for  lower or greater than . 
For :

Thus, under a given threshold , the precision is constant and equal to . Moreover, we can give a lower bound on this constant value in terms of . As a matter of fact,  is an increasing function of , therefore:

Finally, by the definition of , for every , . Consequently, 

For :

And, since :

Finally, with , 

And, since , we have:

Therefore we have two regimes:
\begin{itemize}
    \item For :
    
    \item For :
    
    This can be seen as a vertical scaling of the PR-Curve. For a given point  in , then the point with the same  in   has a precision , up to a certain saturating level (). 
\end{itemize}\newpage


\subsection{Information Divergence Frontier Improvement}\label{app:sec:djolonga}

In \cite{djolonga_precision-recall_2020}, the authors  define another precision-recall curve, named the Information Divergence Frontiers:



Where 
and where  is the Renyi divergence parametrized
by .

As an immediate corrolary of the previous theorem and of proposition
6 of \cite{djolonga_precision-recall_2020}, we can write the following:

\begin{corollary}

Under the same setting as theorem \ref{thm:improvalpha}, for any 
we have 
with . 

\end{corollary}



\section{Bounds}\label{app:sec:bounds}
\begin{theorem}
Let . For any -divergence,
we have 


and for Kullback-Leibler we have for 


where  is the Renyi divergence with parameter  
\end{theorem}
\begin{proof}
For both bounding the -divergence and the KL divergence, the strategy
will be the same. We want to show that 


Note that for any density  such that such that ,
we have  so



So once we have a suitable , we need to show the lower
bound holds:



For bounding general -divergences, we will choose 
with 

Let us first show that . 



Note that for any , 


So 


Next, let us show the lower bound. Recall that 
is convex in its second argument. Thus, convexity implies:



Now to apply the same type of idea to bound the KL, let us define
, where 
where 
is the Renyi divergence of parameter  and  is the reference
measure.

First, let us choose 
and let us show as before that . More precisely,
let us show that 

For any , we have



More generally it is easy to see that for all , we have . For convenience, we will choose . Clearly,  so .
Finally, let us compute 



Thus the result holds: 
\end{proof}




\section{Additional Experiments}
In this section, we provide more details on the different experiments. First, in Section~\ref{app:sec:MNIST}, we explain how the loss landscape is produced. Then, in Section~\ref{app:subsec:xp2D}, we provide more details on how the budget affects the results on the 25 Gaussians experiments. Finally, in Section~\ref{app:subsec:complexity}, we compare the traditional GAN training procedure and our approach in terms of time complexity. 









\subsection{Smoothing the lanscape parameters for MNIST}\label{app:sec:MNIST}
Similarly to \cite{li_visualizing_2018}, the goal is to observe a two dimensional projection of the parameters domain of a neural network, and compute the loss on this domain.

To do so, we train a simple GAN on MNIST. Both the generator and the discriminator are based on 3 linear layers with Leaky Relu. The models are trained using the tradition approach described in Algorithm~\ref{alg:naiveapproach}. Let us define , the parameter vector of the generator . We randomly draw two directions  and  in the parameter domain: defining an hyperspace of generators defined as  with .  Then, given any parameters , we train a new discriminator  based on samples of  and  to determine the baseline loss landscape (). For the OBRS loss landscape, we fine-tune the initial model  in order to perform optimal budgeted rejection sampling. Finally, similar to the baseline, a new discriminator  is trained to estimate the loss, but based on samples  and . 

In Figure~\ref{app:fig:MNIST}, we plot the loss surface. In addition to Figure~\ref{fig:MNISTSmooth}, we represent a batch of samples drawn from  (lower left) and from the  given the worst loss (upper right). When OBRS is applied, we show in red the rejected samples and in green the accepted samples.  
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Surfacesfull.pdf}
    \caption{The Loss surface in the parameters domain of a DCGAN trained on MNIST randomly projected in 2D, observed for different rejection sampling budgets. }\label{app:fig:MNIST}
\end{figure}
 \subsection{Additional Results on the 2D 25-Gaussians Dataset}  \label{app:subsec:xp2D}
In this section, we provide more details on the GAN trained on the 25 Gaussians. The goal of this experiment is to compare OBRS with other rejection sampling methods such as DRS \citep{azadi_discriminator_2019} or MH-GAN \citep{turner_metropolis-hastings_2019}, but also with other sampling techniques that involve gradient descent, such as DOT \citep{tanaka_discriminator_2019} and DGlow \citep{ansari_refining_2021}. We train a simple GAN on 25 two-dimensional Gaussians and apply each method. We tune (when possible) the method to obtain around  inferences from the generator to generate  samples. To be more precise, both ORBS and DRS are easily tunable; however, the rejection rate of MH-GAN highly depends on the number of iterations of the algorithm. Therefore, we set the number of iterations to  to obtain  and then tune  and  to achieve a similar acceptance rate.  We obtain the results plotted in Figure~\ref{app:fig:2Dviz}.
\begin{figure}[H]
     \centering
     \includegraphics[width=0.95\linewidth]{figures/25Gaussians.png}
     \caption{Visual Representation of the different sampling methods.}
     \label{app:fig:2Dviz}
 \end{figure}

In the previous experiment; we arbitrarily set up the acceptance rate (or the sampling time for the \emph{non} rejection sampling methods). We also compare the methods for different sampling time. Since for most methods, the recall was equivalent, we compare the precision denoted as \emph{high quality samples} in \cite{dumoulin_adversarially_2017}. We observe that for any given precision under , the fastest method is the OBRS. However, OBRS,  like MH-GAN and DRS, appear to be capped.   
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/25Gaussians_time.pdf}
    \caption{How the different methods behave with regard to time: to achieve similar results, DOT and DGflow need 100 more times. MH only 10 times more. And for similar time (similar budget), Budgeted Reject is better than DRS. (blue above red). MH, DRS and OBRS (Budget) are caped. They only use the discriminator to refine samples, while the DOT and DGflow sample data point from the latent space and refines the samples directly using Gradient ascent.}
    \label{fig:enter-label}
\end{figure}


\subsection{Complexity of Algorithm \ref{alg:TOBRS}}\label{app:subsec:complexity}
In Algoritm~\ref{alg:TOBRS}, between every update of  and , the parameter  is updated. In practice, the parameter  is not update every iterations. In this section, we investigate our the frequency of update affect the training procedure both in convergence speed and in terms of time.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/freqc.pdf}
    \caption{Tw/OBRS: Training BigGAN models on CIFAR-10 with the hinge loss, and  without OBRS (Tw/oOBRS) and with OBRS (Tw/OBRS). For the models trained with OBRS, the parameter  is updated every  iterations. }
    \label{app:fig:freqc}
\end{figure}

We train different BigGAN models on CIFAR-10 with different frequency of updates: every  iterations, every  iterations and every  iterations. We plot in Figure~\ref{app:fig:freqc}, the FID during training and the time during training, both as a function of the number of iterations. We observe that the frequency of updates does not affect the speed of convergence. Furthermore, we observe that update  every  operation takes on average  longer to train than  without OBRS, while updating every  and  iterations are only  and  longer.    


\section{Experimental Procedure}
In this paper, OBRS have been investigated in two different contexts.
\begin{itemize}
    \item Using OBRS to improve a pre-trained model, with different budget.
    \item Training and fine-tuning a model accounting for OBRS with a budget of .
\end{itemize}

In every experiment, we have used BigGAN models \cite{brock_large_2019}. For every dataset we have used hyperparameters as close as possible to the original ones. In the original paper, the hinge loss is used and, according to \cite{azadi_discriminator_2019}, the fact that the loss is saturating decreases the performance of the estimation of the density ratio. In the first context, we take a pretrained model, typically trained with hinge loss, and fine-tune the discriminator only, based on . And thus we can perform density estimation for the rejection sampling procedure. In the DRS method, they retrain the discriminator on 10k samples. We opt for training the discriminator on the entire data set with a learning rate of  with the same hyperparameters as the one proposed by \cite{brock_large_2019}.




Then for the second context: Tw/OBRS, we need to compare the speed of convergence for three different losses: hinge loss (since it is the original one),  (Tw/oOBRS) and  (Tw/OBRS). However, we evaluate the models based their rejected distribution in terms of FID to analyze the speed of convergence.  Therefore,  two tails for the discriminator were built: one was trained with any loss and the other systematically with . Therefore, we can train the model  based on the given loss and still evaluate the model with OBRS. For the training with OBRS we used this set of hyperparameters. Every model has been trained on a 4xV100 clusters. 


\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|ccccc|}
    \hline
       Dataset  & Task  & Tch & Gch & lr T & lr G & Batch Size  \\\hline
        CIFAR-10 & Training &  &  &  &  &    \\ 
        CelebA64 & Training &  &  &  &  &    \\ 
        CelebA64 & Fine Tuning &  &  &  &  &    \\ 
       ImageNet128 & Fine Tuning &  &  &  &  &    \\ 
\hline
    \end{tabular}
    \caption{Hyper-parameters used for the different BigGAN configurations. Tch and Gch stands for the number of channels in each model. T lr and G lr stands for the learning rate of each models.}
    \label{tab:my_label}
\end{table}

















\clearpage{}


\end{document}
