\begin{table*}[]
\renewcommand\arraystretch{1.3}
\centering
\caption{
Performance comparisons with DeiT and PoWER on ImageNet. ``Embedding Dim'' refers to the dimension of each token in the sequence. ``\#Heads'' and ``\#Blocks'' are the number of self-attention heads and blocks in Transformer, respectively. ``FLOPs'' is measured with a  image. 
``Ti'' and ``S'' are short for the tiny and small settings, respectively. “Architecture-” denotes the model with  pooling stages. ``Scale'' denotes that we scale up the embedding dimension and/or the number of self-attention heads. ``DeiT-Ti/S + PoWER'' refers to the model that applies the techniques in PoWER-BERT~\cite{powerbert} to DeiT-Ti/S.
}
\resizebox{0.9\textwidth}{!} {
\fontsize{11}{11}\selectfont
\begin{tabular}{@{}lccccc|ll@{}}
Model & Embedding Dim & \#Heads & \#Blocks & FLOPs (G) & Params (M) & Top-1 Acc. (\%) & Top-5 Acc. (\%) \\ 
\shline
DeiT-Ti~\cite{deit} & 192 & 3 & 12 & 1.25 & 5.72 & 72.20 & 91.10 \\
DeiT-Ti + PoWER~\cite{powerbert}  & 192 & 3 & 12 & 0.80 & 5.72 & 69.40 {\color[HTML]{CB0000} \fontsize{9pt}{9pt}\selectfont{(-2.80)}} & 89.20 {\color[HTML]{CB0000}\fontsize{9pt}{9pt}\selectfont{(-1.90)}} \\
HVT-Ti-1 & 192 & 3 & 12 & 0.64 & 5.74 & 69.64 {\color[HTML]{CB0000}\fontsize{9pt}{9pt}\selectfont{(-2.56)}} & 89.40 {\color[HTML]{CB0000}\fontsize{9pt}{9pt}\selectfont{(-1.70)}} \\ 
Scale HVT-Ti-4 & 384 & 6 & 12 & 1.39 & 22.12 & 75.23 {\color[HTML]{009901} \fontsize{9pt}{9pt}\selectfont{(+3.03)}} & 92.30 {\color[HTML]{009901}  \fontsize{9pt}{9pt}\selectfont{(+1.20)}} \\ \hline
DeiT-S~\cite{deit} & 384 & 6 & 12 & 4.60 & 22.05 & 79.80 & 95.00 \\
DeiT-S + PoWER~\cite{powerbert}  & 384 & 6 & 12 & 2.70 & 22.05 & 78.30  {\color[HTML]{CB0000}\fontsize{9pt}{9pt}\selectfont{(-1.50)}}  & 94.00  {\color[HTML]{CB0000}\fontsize{9pt}{9pt}\selectfont{(-1.00)}} \\
HVT-S-1 & 384 & 6 & 12 & 2.40 & 22.09 & 78.00 {\color[HTML]{CB0000}\fontsize{9pt}{9pt}\selectfont{(-1.80)}} & 93.83  {\color[HTML]{CB0000}\fontsize{9pt}{9pt}\selectfont{(-1.17)}}
\end{tabular}
}
\label{tab:compare_imagenet}
\vspace{-5pt}
\end{table*}

\begin{table*}[]
\renewcommand\arraystretch{1.2}
\centering
\caption{Effect of the prediction without the class token. ``CLS'' denotes the class token. 
}
\vspace{-5pt}
\resizebox{0.9\textwidth}{!} {
\fontsize{13}{13}\selectfont
\begin{tabular}{lcc|ll|ll}
\multirow{2}{*}{Model} & \multirow{2}{*}{FLOPs (G)} & \multirow{2}{*}{Params (M)} & \multicolumn{2}{c}{ImageNet} & \multicolumn{2}{c}{CIFAR-100} \\ \cline{4-7} 
 &  &  & Top-1 Acc. (\%)  & Top-5 Acc. (\%) & Top-1 Acc. (\%)  & Top-5 Acc. (\%) \\ \shline
DeiT-Ti with CLS & 1.25 & 5.72 & 72.20 & 91.10 & 64.49 & 89.27 \\
DeiT-Ti without CLS & 1.25 & 5.72 & 72.42{\color[HTML]{009901} \fontsize{9pt}{9pt}\selectfont{ (+0.22)}} & 91.55{\color[HTML]{009901} \fontsize{9pt}{9pt}\selectfont{ (+0.45)}} & 65.93{\color[HTML]{009901} \fontsize{9pt}{9pt}\selectfont{ (+1.44)}} & 90.33{\color[HTML]{009901} \fontsize{9pt}{9pt}\selectfont{ (+1.06)}} \\
\end{tabular}
}
\label{tab:cls_token}
\vspace{-5pt}
\end{table*}

\section{Experiments} \label{experiment}

\paragraph{Compared methods.} To investigate the effectiveness of HVT, we 
compare our method with DeiT~\cite{deit} and a BERT-based pruning method PoWER-BERT~\cite{powerbert}. DeiT is a representative Vision Transformer and PoWER progressively prunes unimportant tokens in pretrained BERT models for inference acceleration. Moreover, we 
consider two architectures in DeiT for comparisons: \textbf{HVT-Ti}: HVT with the tiny setting. \textbf{HVT-S}: HVT with the small setting. For convenience, we use “Architecture-” to represent our model with  pooling stages, \eg, HVT-S-1.


\paragraph{Datasets and Evaluation metrics.} We evaluate our proposed HVT on two image classification benchmark datasets: CIFAR-100 \cite{krizhevsky2009learning} and ImageNet \cite{russakovsky2015imagenet}. We measure the performance of different methods in terms of the Top-1 and Top-5 accuracy. Following DeiT~\cite{deit}, we measure the computational cost by FLOPs. Moreover, we also measure the model size by the number of parameters (Params).

\paragraph{Implementation details.} 
For experiments on ImageNet, we train our models for 300 epochs with a total batch size of 1024. The initial learning rate is 0.0005. We use AdamW optimizer \cite{loshchilov2019decoupled} with a momentum of 0.9 for optimization. We set the weight decay to 0.025. For fair comparisons, we keep the same data augmentation strategy as DeiT~\cite{deit}. For the downsampling operation, we use max pooling by default. The kernel size  and stride  are set to 3 and 2, respectively, chosen by a simple grid search on CIFAR100. Besides, all learnable positional embeddings are initialized in the same way as DeiT. 
More detailed settings on the other hyper-parameters can be found in DeiT. For experiments on CIFAR-100, we train our models with a total batch size of 128. The initial learning rate is set to 0.000125. Other hyper-parameters are kept the same as those on ImageNet.
\begin{figure}[!tp]
	\centering
	\includegraphics[width=0.80\linewidth]{figures/same_flops_compare.pdf}
	\vspace{-5pt}
	\caption{Performance comparisons of DeiT-Ti (1.25G FLOPs) and the proposed Scale HVT-Ti-4 (1.39G FLOPs). All the models are evaluated on ImageNet.
	Solid lines denote the Top-1 accuracy (y-axis on the right). Dash lines denote the training loss (y-axis on the left). 
	}
	\label{fig:same_flops_compare}
	\vspace{-18pt}
\end{figure}
\subsection{Main Results} 
We compare the proposed HVT with DeiT and PoWER, and report the results in Table~\ref{tab:compare_imagenet}. First, compared to DeiT, our HVT achieves nearly 2 FLOPs reduction with a hierarchical pooling.
However, the significant FLOPs reduction also leads to performance degradation in both the tiny and small settings. Additionally, the performance drop of HVT-S-1 is smaller than that of HVT-Ti-1. For example, for HVT-S-1, it only leads to  drop in the Top-1 accuracy. In contrast, it results in  drop in the Top-1 accuracy for HVT-Ti-1. It can be attributed to that, compared with HVT-Ti-1, HVT-S-1 is more redundant with more parameters. Therefore, applying hierarchical pooling to HVT-S-1 can significantly reduce redundancy while maintaining performance. Second, compared to PoWER, HVT-Ti-1 uses less FLOPs while achieving better performance. Besides, HVT-S-1 reduces more FLOPs than PoWER, while achieving slightly lower performance than PoWER. Also note that PoWER involves three training steps, while ours is a simpler one-stage training scheme.

Moreover, we also compare the scaled HVT with DeiT under similar FLOPs. Specifically, we enlarge the embedding dimensions and add extra heads in HVT-Ti. From Table~\ref{tab:compare_imagenet} and Figure~\ref{fig:same_flops_compare}, by re-allocating the saved FLOPs to scale up the model, HVT can converge to a better solution and yield improved performance. For example, the Top-1 accuracy on ImageNet can be improved considerably by 3.03\% in the tiny setting. More empirical studies on the effect of model scaling can be found in Section~\ref{ablation}.

\subsection{Ablation Study} 
\label{ablation}

\paragraph{Effect of the prediction without the class token.}
To investigate the effect of the prediction without the class token, we train DeiT-Ti with and without the class token and show the results in Table~\ref{tab:cls_token}.  
From the results, the models without the class token outperform the ones with the class token. The performance gains mainly come from the extra  discriminative information stored in the entire sequence without the class token.
Note that the performance improvement on CIFAR-100 is much larger than that on ImageNet. It may be attributed that CIFAR-100 is a small dataset, which lacks varieties compared with ImageNet. Therefore, the model trained on CIFAR-100 benefits more from the increase of model's discriminative power.



\begin{table}[]
\centering
\caption{Performance comparisons on HVT-S-4 with three downsampling operations: convolution, max pooling and average pooling. We report the Top-1 and Top-5 accuracy on CIFAR-100.
}
\vspace{-5pt}
\renewcommand\arraystretch{1.2}
\resizebox{\columnwidth}{!}{
\fontsize{15}{15}\selectfont
\begin{tabular}{@{}cccc|cc@{}}
Model & Operation & FLOPs (G) & Params (M) & Top-1 Acc. (\%)  & Top-5 Acc. (\%) \\ \shline
HVT-S & Conv       & 1.47     & 23.54     & 69.75          & 92.12 \\
HVT-S & Avg       & 1.39     & 21.77     & 70.38          & 91.39          \\
HVT-S & Max       & 1.39     & 21.77     & 75.43          & 93.56        \\ 
\end{tabular}
}
\label{tab:pool_type}
\vspace{-10pt}
\end{table}


\begin{table}[]
\caption{Performance comparisons on HVT-S with different pooling stages . We report the Top-1 and Top-5 accuracy on CIFAR-100.
}
\vspace{-10pt}
\renewcommand\arraystretch{1.1}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccc|cccc}
\multirow{2}{*}{} & \multirow{2}{*}{FLOPs} & \multirow{2}{*}{Params} & \multicolumn{2}{c}{ImageNet} & \multicolumn{2}{c}{CIFAR100} \\ \cline{4-7} 
  &      &       & Top-1 (\%) & \multicolumn{1}{c|}{Top-5 (\%)} & Top-1 (\%) & Top-5 (\%) \\ \hline
0 & 4.57 & 21.70 & 80.39           & \multicolumn{1}{c|}{95.13}            & 71.99       & 92.44       \\
1 & 2.40 & 21.74 & 78.00   & \multicolumn{1}{c|}{93.83}       & 74.27       & 93.07       \\
2 & 1.94 & 21.76 & 77.36       & \multicolumn{1}{c|}{93.55}       & 75.37       & 93.69       \\
3 & 1.62 & 21.77 & 76.32       & \multicolumn{1}{c|}{92.90}        & 75.22       & 93.90       \\
4 & 1.39 & 21.77 & 75.23       & \multicolumn{1}{c|}{92.30}        & 75.43       & 93.56      
\end{tabular}
}
\label{tab:pooling_times}
\vspace{-10pt}
\end{table}

\paragraph{Effect of different pooling stages.}
We train HVT-S with different pooling stages  and show the results in Table~\ref{tab:pooling_times}. Note that HVT-S-0 is equivalent to the DeiT-S without the class token.
With the increase of , HVT-S achieves better performance with decreasing FLOPs on CIFAR-100, while on ImageNet we observe the accuracy degrades.
One possible reason is that HVT-S is very redundant on CIFAR-100, such that pooling acts as a regularizer to avoid the overfitting problem and improves the generalization of HVT on CIFAR-100. On ImageNet, we assume HVT is less redundant and a better scaling strategy is required to improve the performance.



\paragraph{Effect of different downsampling operations.}
To investigate the effect of different downsampling operations, we train HVT-S-4 with three downsampling strategies: convolution, average pooling and max pooling. 
As Table~\ref{tab:pool_type} shows, downsampling with convolution performs the worst even it introduces additional FLOPs and parameters. Besides, average pooling performs slightly better than convolution in terms of the Top-1 accuracy. Compared with the two settings, HVT-S-4 with max pooling performs much better as it significantly surpasses average pooling by 5.05\% in the Top-1 accuracy and 2.17\% in the Top-5 accuracy. The result is consistent with the common sense~\cite{pool_anly} that max pooling performs well in a large variety of settings. To this end, we use max pooling in all other experiments by default.




\begin{table}[]
\caption{
Performance comparisons on HVT-S-4 with different number of Transformer blocks. We report the Top-1 and Top-5 accuracy on CIFAR-100.
}
\vspace{-3pt}
\renewcommand\arraystretch{1.1}
\resizebox{\columnwidth}{!} {
\begin{tabular}{@{}ccc|cc@{}}
\#Blocks & FLOPs (G) & Params (M) & Top-1 Acc. (\%)  & Top-5 Acc. (\%) \\ \shline
12 & 1.39 & 21.77 & 75.43 & 93.56 \\ 
16 & 1.72 & 28.87 & 75.32 & 93.30 \\
20 & 2.05 & 35.97 & 75.35 & 93.35 \\
24 & 2.37 & 43.07 & 75.04 & 93.39 \\
\end{tabular}
}
\label{tab:num_blocks}
\vspace{-8pt}
\end{table}

\begin{table}[]
\centering
\caption{
Performance comparisons on HVT-Ti-4 with different number of self-attention heads. We report the Top-1 and Top-5 accuracy on CIFAR-100.
}
\vspace{-5pt}
\renewcommand\arraystretch{1.1}
\resizebox{\columnwidth}{!} {
\begin{tabular}{@{}ccc|cc@{}}
\#Heads & FLOPs (G) & Params (M) & Top-1 Acc. (\%)  & Top-5 Acc. (\%) \\ \shline
3 & 0.38 & 5.58 & 69.51 & 91.78 \\
6 & 1.39 & 21.77 & 75.43 & 93.56 \\
12 & 5.34 & 86.01 & 76.26 & 93.39 \\ 
16 & 9.39 & 152.43 & 76.30 & 93.16 \\ 
\end{tabular}
}
\label{tab:num_heads}
\vspace{-15pt}
\end{table}

\paragraph{Effect of model scaling.}
\label{sec:model_scaling}
One of the important advantages of the proposed hierarchical pooling is that we can re-allocate the saved computational cost for better model capacity by constructing a model with a wider, deeper, larger resolution or smaller patch size configuration.
Similar to the CNNs literature \cite{resnet,wider_deeper,wrn}, we study the effect of model scaling in the following.

Based on HVT-S-4, we first construct deeper models by increasing the number of blocks in Transformers. Specifically, we train 4 models with different number of blocks . As a result, each pooling stage for different models would have 3, 4, 5, and 6 blocks, respectively. We train 4 models on CIFAR-100 and report the results in Table~\ref{tab:num_blocks}. From the results, we observe no more gains by stacking more blocks in HVT. 

Based on HVT-Ti-4, we then construct wider models by increasing the number of self-attention heads. To be specific, we train 4 models with different numbers of self-attention heads, \ie, 3, 6, 12, and 16, on CIFAR-100 and report the results in Table~\ref{tab:num_heads}. From the results, our models achieve better performance with the increase of width. For example, the model with 16 self-attention heads outperforms those with 3 self-attention heads by 6.79\% in the Top-1 accuracy and 1.38\% in the Top-5 accuracy.

Based on HVT-S-4, we further construct models with larger input image resolutions. Specifically, we train 4 models with different input image resolutions, \ie, 160, 224, 320, and 384, on CIFAR-100 and report the results in Table~\ref{tab:resolution}. From the results, with the increase of image resolution, our models achieve better performance. For example, the model with the resolution of 384 outperforms those with the resolution of 160 by 2.47\% in the Top-1 accuracy and 1.12\% in the Top-5 accuracy. Nevertheless, increasing image resolutions also leads to high computational cost. To make a trade-off between computational cost and accuracy, we set the image resolution to 224 by default.

We finally train HVT-S-4 with different patch sizes  and show the results in Table~\ref{tab:patch_size}. From the results, HVT-S-4 performs better with the decrease of patch size. 
For example, when the patch size decreases from 32 to 8, our HVT-S achieves 9.14\% and 4.03\% gain in terms of the Top-1 and Top-5 accuracy. Intuitively, a smaller patch size leads to fine-grained image patches and helps to learn high-resolution representations, which is able to improve the classification performance. However, with a smaller patch size, the patch sequence will be longer, which significantly increases the computational cost. To make a balance between the computational cost and accuracy, we set the patch size to 16 by default.

\paragraph{Exploration on 2D pooling.}

Compared to 1D pooling, 2D pooling brings more requirements. For example, it requires a smaller patch size to ensure a sufficient sequence length. Correspondingly, it is essential to reduce the heads at the early stages to save FLOPs and memory consumption from high-resolution feature maps. Besides, it also requires to vary the blocks at each stage to control the overall model complexity. In Table~\ref{tab:2d_pooling}, we apply 2D pooling to HVT-S-2 and compare it with DeiT-S. The results show that HVT-S-2 with 2D pooling outperforms DeiT-S on CIFAR100 by a large margin with similar FLOPs. In this case, we assume that HVT can achieve promising performance with a dedicated scaling scheme for 2D pooling. We will leave this exploration for future work.


\begin{table}[]
\caption{Performance comparisons on HVT-S-4 with different image resolutions. We report the Top-1 and Top-5 accuracy on CIFAR-100.
}
\vspace{-5pt}
\renewcommand\arraystretch{1.1}
\resizebox{\columnwidth}{!} {
\begin{tabular}{ccc|cc}
Resolution & FLOPs (G) & Params (M) & Top-1 Acc. (\%)  & Top-5 Acc. (\%) \\ \shline
160 & 0.69 & 21.70 & 73.84 & 92.90 \\
224 & 1.39 & 21.77 & 75.43 & 93.56 \\
320 & 3.00 & 21.92 & 75.54 & 94.18 \\
384 & 4.48 & 22.06 & 76.31 & 94.02 \\ 
\end{tabular}
}
\label{tab:resolution}
\vspace{-12pt}
\end{table}



\begin{table}[]
\centering
\caption{Performance comparisons on HVT-S-4 with different patch sizes . We report the Top-1 and Top-5 accuracy on CIFAR-100.
}
\vspace{-5pt}
\renewcommand\arraystretch{1.1}
\resizebox{\columnwidth}{!} {
\begin{tabular}{@{}ccc|cc@{}}
 & FLOPs (G) & Params (M) & Top-1 Acc. (\%)  & Top-5 Acc. (\%) \\ \shline
8 & 6.18  & 21.99 & 77.29  & 94.22  \\ 
16 & 1.39 & 21.77 & 75.43  & 93.56 \\ 
32 & 0.37 & 22.55 & 68.15 & 90.19 \\
\end{tabular}
}
\label{tab:patch_size}
\vspace{-10pt}
\end{table}

\begin{table}[]
\centering
\caption{Effect of 2D pooling on HVT-S-2.  We report the Top-1 and Top-5 accuracy on CIFAR-100. For HVT-S-2, we apply 2D max pooling and use a patch size of 8.}
\vspace{-5pt}
\renewcommand\arraystretch{1.1}
\resizebox{\columnwidth}{!} {
\begin{tabular}{l|cccc}
Model        & FLOPs (G) & Params (M) & Top-1 Acc. (\%) & Top-5 Acc. (\%) \\ \shline
DeiT-S       & 4.60       & 21.70      & 71.99           & 92.44           \\
HVT-S-2 (2D) & 4.62      & 21.80      & 77.58           & 94.40           
\end{tabular}
}
\label{tab:2d_pooling}
\vspace{-15pt}
\end{table}