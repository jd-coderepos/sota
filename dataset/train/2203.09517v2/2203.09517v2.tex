

\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{xcolor}
\usepackage{ifthen}
\usepackage{textcomp}
\usepackage{color}
\usepackage{subfigure}

\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} 


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{enumitem}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{hyperref}

\usepackage{caption}




\usepackage[accsupp]{axessibility}  


\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}




\newcommand{\boldstart}[1]{\noindent\textbf{#1}}
\newcommand{\boldstartspace}[1]{\vspace{0.1in}\noindent\textbf{#1}}

\makeatletter
\newcommand{\printfnsymbol}[1]{\textsuperscript{\@fnsymbol{#1}}}
\makeatother

\begin{document}
\pagestyle{headings}
\mainmatter

\title{TensoRF: Tensorial Radiance Fields} 



\titlerunning{TensoRF: Tensorial Radiance Fields}


\author{Anpei Chen$^1$\thanks{Equal contribution.\\ Research done when Anpei Chen was in a remote internship with UCSD.}
\,\,
Zexiang Xu$^{2}$\printfnsymbol{1}
\,\,
Andreas Geiger$^3$ 
\,\,
Jingyi Yu$^1$
\,\,
Hao Su$^4$ 
}
\institute{$^1$ShanghaiTech University \quad $^2$Adobe Research \\ $^3$University of Tübingen and MPI-IS, Tübingen \quad $^4$UC San Diego
\href{https://apchenstu.github.io/TensoRF/}{https://apchenstu.github.io/TensoRF/}
}

\authorrunning{A. Chen, Z. Xu et al.}



\maketitle

\begin{abstract}
We present TensoRF, a novel approach to model and reconstruct radiance fields.
Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features.
Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. 
We demonstrate that applying traditional \fv{CANDECOMP/PARAFAC (CP)} decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. 
To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. 
Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features.
Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction ($<30$ min) with better rendering quality and even a smaller model size ($<4$ MB) compared to NeRF.
Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time ($<10$ min) and retaining a compact model size ($<75$ MB).



%
 \end{abstract}


\newcommand{\Tensor}{\mathcal{T}}
\newcommand{\Vector}{\mathbf{v}}
\newcommand{\Matrix}{\mathbf{M}}
\newcommand{\AppVec}{\mathbf{b}}
\newcommand{\AppMat}{\mathbf{B}}
\newcommand{\VectorPack}{\mathbf{V}}
\newcommand{\RRR}{\mathbb{R}}
\newcommand{\DimIJK}{I\times J\times K}
\newcommand{\DimCh}{P}


\newcommand{\OuterP}{\circ}
\newcommand{\ScalarP}{\ast}
\newcommand{\Pos}{\mathbf{x}}
\newcommand{\Dens}{\sigma}
\newcommand{\Rad}{c}
\newcommand{\Color}{C}
\newcommand{\Trans}{\tau}
\newcommand{\Step}{\Delta}
\newcommand{\Comp}{\mathcal{A}}
\newcommand{\Dir}{d}
\newcommand{\Grid}{\mathcal{G}}
\newcommand{\ShadFunc}{S}
\newcommand{\loss}{\mathcal{L}}

\begin{figure}[t]
    \includegraphics[width=\linewidth]{figs/teaser_v6.pdf}
    \vspace{-5mm}
    \caption{
    Left: We model a scene as a tensorial radiance field using a set of vectors ($\Vector$) and matrices ($\Matrix$) that describe scene appearance and geometry along their corresponding axes. These vector/matrix factors are used to compute volume density $\Dens$ and view-dependent RGB color via vector-matrix outer products for realistic volume rendering.
    Right: In comparison with previous and concurrent methods, our TensoRF models can achieve the best rendering quality and are the only methods that can simultaneously achieve fast reconstruction and high compactness. (Our models are denoted with their decomposition techniques, number of components, and training steps.)
} \vspace{-5mm}
    \label{fig:teaser}
\end{figure}

\section{Introduction}

Modeling and reconstructing 3D scenes as representations that support high-quality image synthesis is crucial for computer vision and graphics with various applications in visual effects, e-commerce, virtual and augmented reality, and robotics.
Recently, NeRF \cite{mildenhall2020nerf} and its many follow-up works \cite{zhang2020nerf++,liu2020neural} have shown success on modeling a scene as a radiance field 
and enabled photo-realistic rendering of scenes with highly complex geometry and view-dependent appearance effects. Despite the fact that (purely MLP-based) NeRF models require small memory, they take a long time (hours or days) to train. In this work, we pursue a novel approach that is both \emph{efficient in training time} and \emph{compact in memory footprint}, and at the same time achieves \emph{state-of-the-art} rendering quality.




To do so, we propose TensoRF, a novel radiance field representation that is highly compact and also fast to reconstruct, enabling efficient scene reconstruction and modeling.
Unlike coordinate-based MLPs used in NeRF, we represent radiance fields as an explicit voxel grid of features.
Note that it is \emph{unclear} whether voxel grid representation can benefit the efficiency of reconstruction: While previous work has used feature grids \cite{liu2020neural,yu2021plenoctrees,hedman2021baking}, they require large GPU memory to store the voxels whose size grows cubically with resolution, and some even require pre-computing a NeRF for distillation, leading to very long reconstruction time.

Our work addresses the inefficiency of voxel grid representations in a principled framework, leading to a family of simple yet effective methods.
We leverage the fact that a feature grid can naturally be seen as a 4D tensor, where three of its modes correspond to the XYZ axes of the grid and the fourth mode represents the feature channel dimension.
This opens the possibility of exploiting classical tensor decomposition techniques -- which have been widely applied to high-dimensional data analysis and compression in various fields \cite{kolda2009tensor} --  for radiance field modeling.  We, therefore, propose to factorize the tensor of radiance fields into multiple \emph{low-rank} tensor components, leading to an accurate and compact scene representation.
Note that our central idea of tensorizing radiance fields is general and can be potentially adopted to any tensor decomposition technique. 

In this work, we first attempt the classic CANDECOMP/PARAFAC (CP) decomposition \cite{carroll1970analysis}. We show that TensoRF with CP decomposition can already achieve photo-realistic rendering and lead to a more compact model than NeRF that is purely MLP based (see Fig.~\ref{fig:teaser} and Tab.~\ref{table:results}).
However, experimentally, to further push reconstruction quality for complex scenes, we have to use more component factors, which undesirably increases training time. 

Therefore, we present a novel vector-matrix (VM) decomposition technique that effectively reduces the number of components required for the same expression capacity, leading to faster reconstruction and better rendering.
In particular, inspired by the CP and block term decomposition \cite{de2008decompositions}, we propose to factorize the full tensor of a radiance field into multiple vector and matrix factors per tensor component. 
Unlike the sum of outer products of pure vectors in CP decomposition, we consider the sum of vector-matrix outer products (see Fig.~\ref{fig:tensor-decomp}).
In essence, we relax the ranks of two modes of each component by jointly modeling two modes in a matrix factor.
While this increases the model size compared to pure vector-based factorization in CP, we enable each component to express more complex tensor data of higher ranks, thus significantly reducing the required number of components in radiance field modeling.








With CP/VM decomposition, our approach compactly encodes spatially varying features in the voxel grid. Volume density and view-dependent color can be decoded from the features, supporting volumetric radiance field rendering. Because a tensor expresses discrete data, we also enable efficient trilinear interpolation for our representation to model a continuous field.
Our representation supports various types of per-voxel features with different decoding functions, including neural features -- depending on an MLP to regress view-dependent colors from the features -- and spherical harmonics (SH) features (coefficients) -- allowing for simple color computation from the fixed SH functions and leading to a representation without neural networks.

Our tensorial radiance fields can be effectively reconstructed from multi-view images and enable realistic novel view synthesis.
In contrast to previous works that directly reconstruct voxels, our tensor factorization reduces space complexity from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$ (with CP) or $\mathcal{O}(n^2)$ (with VM), significantly lowering memory footprint. 
\fv{Note that, although we leverage tensor decomposition, we are not addressing a decomposition/compression problem, but a reconstruction problem based on gradient decent, since the feature grid/tensor is unknown.
In essence, our CP/VM decomposition offers low-rank regularization in the optimization, leading to high rendering quality.}
We present extensive evaluation of our approach with various settings, covering both CP and VM models, different numbers of components and grid resolutions. 
We demonstrate that all models are able to achieve realistic novel view synthesis results that are on par or better than previous state-of-the-art methods (see Fig.~\ref{fig:teaser} and Tab.~\ref{table:results}).
More importantly, our approach is of high computation and memory efficiency.
All TensoRF models can reconstruct high-quality radiance fields in 30 min; our fastest model with VM decomposition takes less than 10 min, which is significantly faster (about 100x) than NeRF and many other methods, while requiring substantially less memory than previous and concurrent voxel-based methods.
Note that, unlike concurrent works \cite{yu2021plenoxels,muller2022instant} that require unique data structures and customized CUDA kernels, our model's efficiency gains are obtained using a standard PyTorch implementation. 
As far as we know, our work is the first that views radiance field modeling from a tensorial perspective and pose the problem of radiance field reconstruction as one of low-rank tensor reconstructions. 















%
 


\section{Related Work}
\boldstart{Tensor decomposition.} 
Tensor decomposition \cite{kolda2009tensor} has been studied for decades \fv{with diverse applications in vision, graphics, machine learning, and other fields \cite{panagakis2021tensor,kamal2016tensor,vasilescu2004tensortextures,deng2022constant,ballester2016tensor,ji2019survey}.}
In general, the most widely used decompositions are Tucker decomposition \cite{tucker1966some} and CP decomposition \cite{carroll1970analysis,harshman1970foundations}, both of which can be seen as generalizations of the matrix singular value decomposition (SVD). CP decomposition can also be seen as a special Tucker decomposition whose core tensor is diagonal.
By combining CP and Tucker decomposition, block term decomposition (BTD) has been proposed with its many variants \cite{de2008decompositions} and 
used in many vision and learning tasks \cite{ben2019block,ye2018learning,ye2020block}.
In this work, we leverage tensor decomposition for radiance field modeling. 
We directly apply CP decomposition and also introduce a new vector-matrix decomposition, which can be seen as a special BTD. 

\boldstartspace{Scene representations and radiance fields.}
Various scene representations, including meshes \cite{groueix2018papier,wang2018pixel2mesh}, point clouds \cite{qi2017pointnet}, volumes \cite{ji2017surfacenet,qi2016volumetric}, implicit functions \cite{mescheder2018occupancy,peng2020convolutional}, have been extensively studied in recent years.
Many neural representations \fv{\cite{chen2018deep,zhou2018stereo,sitzmann2019deepvoxels,lombardi2019neural,bi2020deep}} are proposed for high-quality rendering or natural signal representation \cite{sitzmann2019siren,tancik2020fourfeat,liang2022coordx}.
NeRF \cite{mildenhall2020nerf} introduces radiance fields to address novel view synthesis and achieves photo-realistic quality.
This representation has been quickly extended and applied in diverse graphics and vision applications, including generative models \cite{chan2021pi,niemeyer2021giraffe}, appearance acquisition \cite{bi2020neural,boss2021nerd}, surface reconstruction \cite{wang2021neus,oechsle2021unisurf}, fast rendering\fv{ \cite{reiser2021kilonerf,yu2021plenoctrees,hedman2021baking,garbin2021fastnerf}}, appearance editing \cite{xiang2021neutex,liu2021editing}, dynamic capture \cite{li2021neural,park2021hypernerf} \fv{and generative model} \cite{Niemeyer2021CVPR,Chan2022EG3D}.  
While leading to realistic rendering and a compact model, NeRF with its pure MLP-based representation has known limitations in slow reconstruction and rendering. Recent methods \cite{yu2021plenoctrees,liu2020neural,hedman2021baking} have leveraged a voxel grid of features in radiance field modeling, achieving fast rendering. However, these grid-based methods still require long reconstruction time and even lead to high memory costs, sacrificing the compactness of NeRF. 
Based on feature grids, we present a novel tensorial scene representation, leveraging tensor factorization techniques, leading to fast reconstruction and compact modeling. 


Other methods design generalizable network modules trained across scenes to achieve image-dependent radiance field rendering \fv{\cite{trevithick2020grf,yu2020pixelnerf,ibrnet,Chibane2021SRF}} and fast reconstruction \cite{chen2021mvsnerf,xu2022point}. Our approach focuses on radiance field representation and only considers per-scene optimization (like NeRF). We show that our representation can already lead to highly efficient radiance field reconstruction without any across-scene generalization. We leave the extensions to generalizable settings as future work.

\boldstartspace{Concurrent work.}
The field of radiance field modeling is moving very fast and many concurrent works have appeared on arXiv as preprints over the last few months. DVGO \cite{sun2021direct} and Plenoxels \cite{yu2021plenoxels} also optimize voxel grids of (neural or SH) features for fast radiance field reconstruction. 
However, they still optimize per-voxel features directly like previous voxel-based methods, thus requiring large memory. Our approach instead factorizes the feature grid into compact components and leads to significantly higher memory efficiency.
Instant-NGP \cite{muller2022instant} uses multi-resolution hashing for efficient encoding and also leads to high compactness. 
This technique is orthogonal to our factorization-based technique; potentially, each of our vector/matrix factor can be encoded with this hashing technique and we leave such combination as future work. 
EG3D \cite{Chan2022EG3D} uses a tri-plane representation for 3D GANs; their representation is similar to our VM factorization and can be seen as a special VM version that has constant vectors. 


%
 




\section{CP and VM Decomposition}
\label{sec:vm-factor}
We factorize radiance fields into compact components for scene modeling.
To do so, we apply both the classic CP decomposition and a new vector-matrix (VM) decomposition; both are illustrated in Fig.~\ref{fig:tensor-decomp}.
We now discuss both decompsitions with an example of a 3D (3rd-order) tensor. We will introduce how to apply tensor factorization in radiance field modeling (with a 4D tensor) in Sec.~\ref{sec:tensorf}.


\boldstartspace{CP decomposition.} 
Given a 3D tensor $\Tensor \in \RRR^{I\times J\times K}$, CP decomposition factorizes it into a sum of outer products of vectors (shown in Fig.~\ref{fig:tensor-decomp}):
\begin{equation}
    \Tensor = \sum_{r=1}^R \Vector_r^{1} \OuterP \Vector_r^{2} \OuterP \Vector_r^{3}
    \label{eqn:cpvector}
\end{equation}
where $\Vector_r^{1} \OuterP \Vector_r^{2} \OuterP \Vector_r^{3}$ corresponds to a rank-one tensor component, and $\Vector_r^{1} \in \RRR^{I}$, $\Vector_r^{2} \in \RRR^{J}$, and $\Vector_r^{3} \in \RRR^{K}$ are factorized vectors of the three modes for the $r$th component. Superscripts denote the modes of each factor; $\OuterP$ represents the outer product. Hence, each tensor element $\Tensor_{ijk}$ is a sum of scalar products:
\begin{equation}
    \Tensor_{ijk} = \sum_{r=1}^R \Vector_{r,i}^{1} \Vector_{r,j}^{2} \Vector_{r,k}^{3}
    \label{eqn:cpelement}
\end{equation}
where $i$, $j$, $k$ denote the indices of the three modes. 


CP decomposition factorizes a tensor into multiple vectors, expressing multiple compact rank-one components. 
CP can be directly applied in our tensorial radiance field modeling and generate high-quality results (see Tab.~\ref{table:results}). However, because of too high compactness, CP decomposition can require many components to model complex scenes, leading to high computational costs in radiance field reconstruction.
Inspired by block term decomposition (BTD), we present a new VM decomposition, leading to more efficient radiance field reconstruction.

\begin{figure*}[t]
    \includegraphics[width=\textwidth]{figs/tensor_factorization.pdf}
\caption{Tensor factorization. Left: CP decomposition (Eqn.~\ref{eqn:cpvector}), which factorizes a tensor as a sum of vector outer products.  Right: our vector-matrix decomposition (Eqn.~\ref{eqn:vmrrr}), which factorizes a tensor as a sum of vector-matrix outer products.}
    \label{fig:tensor-decomp}\vspace{-5mm}
\end{figure*}


\boldstartspace{Vector-Matrix (VM) decomposition.} Unlike CP decomposition that utilizes pure vector factors, VM decomposition factorizes a tensor into multiple vectors and matrices as shown in Fig.~\ref{fig:tensor-decomp} right. This is expressed by
\begin{equation}
    \Tensor = \sum_{r=1}^{R_1} \Vector_r^{1} \OuterP \Matrix_r^{2,3} + \sum_{r=1}^{R_2}\Vector_r^{2} \OuterP \Matrix_r^{1,3} + \sum_{r=1}^{R_3}\Vector_r^{3} \OuterP \Matrix_r^{1,2} 
    \label{eqn:vmrrr}
\end{equation}
where $\Matrix_r^{2,3} \in \RRR^{J\times K}$, $\Matrix_r^{1,3} \in \RRR^{I\times K}$, $\Matrix_r^{1,2} \in \RRR^{I\times J}$ are matrix factors for two (denoted by superscripts) of the three modes.
For each component, we relax its two mode ranks to be arbitrarily large, while restricting the third mode to be rank-one; e.g., for component tensor $\Vector_r^{1} \OuterP \Matrix_r^{2,3}$, its mode-1 rank is 1, and its mode-2 and mode-3 ranks can be arbitrary, depending on the rank of the matrix $\Matrix_r^{2,3}$.
In general, instead of using separate vectors in CP, we combine every two modes and represent them by matrices, allowing each mode to be adequately parametrized with a smaller number of components. $R_1$, $R_2$, $R_3$ can be set differently and should be chosen depending on the complexity of each mode.
Our VM decomposition can be seen as a special case of general BTD. 

Note that, each of our component tensors has more parameters than a component in CP decomposition. 
While this leads to lower compactness, a VM component tensor can express more complex high-dimensional data than a CP component, thus reducing the required number of components when modeling the same complex function. 
On the other hand, VM decomposition is still of very high compactness, reducing memory complexity from $\mathcal{O}(N^3)$ to $\mathcal{O}(N^2)$, compared to dense grid representations.


\boldstartspace{Tensor for scene modeling.} 
In this work, we focus on the task of modeling and reconstructing radiance fields.
In this case, the three tensor modes correspond to XYZ axes, and we thus directly denote the modes with XYZ to make it intuitive.
Meanwhile, in the context of 3D scene representation, we consider $R_1=R_2=R_3=R$ for most of scenes, reflecting the fact that a scene can distribute and appear equally complex along its three axes.
Therefore, Eqn.~\ref{eqn:vmrrr} can be re-written as
\begin{equation}
    \Tensor = \sum_{r=1}^{R} \Vector_r^{X} \OuterP \Matrix_r^{Y,Z} + \Vector_r^{Y} \OuterP \Matrix_r^{X,Z} + \Vector_r^{Z} \OuterP \Matrix_r^{X,Y}
    \label{eqn:vmr}
\end{equation}
In addition, to simplify notation and the following discussion in later sections, we also denote the three types of component tensors as $\Comp^X_r=\Vector_r^X \OuterP \Matrix_r^{YZ}$, $\Comp^Y_r=\Vector_r^Y \OuterP \Matrix_r^{XZ}$, and $\Comp^Z_r=\Vector_r^Z \OuterP \Matrix_r^{XY}$; here the superscripts XYZ of $\Comp$ indicate different types of components.
With this, a tensor element $\Tensor_{ijk}$ is expressed as
\begin{align}
    \Tensor_{ijk} = & \sum_{r=1}^{R} \sum_{m} \Comp_{r,ijk}^m
    \label{eqn:vmrelement}
\end{align}
where $m\in XYZ$, $\Comp^X_{r,ijk}=\Vector_{r,i}^X \Matrix_{r,jk}^{YZ}$, $\Comp^Y_{r,ijk}=\Vector_{r,j}^Y \Matrix_{r,ik}^{XZ}$, and $\Comp^Z_{r,ijk}=\Vector_{r,k}^Z \Matrix_{r,ij}^{XY}$. Similarly, we can also denote a CP component as $\Comp^{\gamma}=\Vector_r^X \OuterP \Vector_r^Y \OuterP \Vector_r^Z$, and Eqn.~\ref{eqn:vmrelement} can also express CP decomposition by considering $m=\gamma$, where the summation over $m$ can be removed.
 









\section{Tensorial Radiance Field Representation}
\label{sec:tensorf}
We now present our Tensorial Radiance Field Representation (TensoRF). For simplicity, we focus on presenting TensoRF with our VM decomposition. CP decomposition is simpler and its decomposition equations can be directly applied with minimal modification (like Eqn.~\ref{eqn:vmrelement}).

\subsection{Feature grids and radiance field.}
\label{sec:grids}
Our goal is to model a radiance field, which is essentially a function that maps any 3D location $\Pos$ and viewing direction $\Dir$ to its volume density $\Dens$ and view-dependent color $\Rad$, supporting differentiable ray marching for volume rendering.
We leverage a regular 3D grid $\Grid$ with per-voxel multi-channel features to model such a function. We split it (by feature channels) into a geometry grid $\Grid_\Dens$ and an appearance grid $\Grid_\Rad$, separately modelling the volume density $\Dens$ and view-dependent color $\Rad$.

Our approach supports various types of appearance features in $\Grid_\Rad$, depending on a pre-selected function $\ShadFunc$ that coverts an appearance feature vector and a viewing direction $\Dir$ to color $\Rad$. For example, $\ShadFunc$ can be a small MLP or spherical harmonics (SH) functions, where $\Grid_\Rad$ contains neural features and SH coefficients respectively.
We show that both MLP and SH functions work well with our model (see Tab.\ref{table:results}).
On the other hand, we consider a single-channel grid $\Grid_\Dens$, whose values represent volume density directly, without requiring an extra converting function. The continuous grid-based radiance field can be written as
\begin{equation}
    \Dens, \Rad = \Grid_\Dens(\Pos), \ShadFunc(\Grid_\Rad(\Pos),\Dir)
    \label{eqn:rf}
\end{equation}
where $\Grid_\Dens(\Pos)$, $\Grid_\Rad(\Pos)$ represent the trilinearly interpolated features from the two grids at location $\Pos$. 
We model $\Grid_\Dens$ and $\Grid_\Rad$ as factorized tensors.






\subsection{Factorizing radiance fields}
While $\Grid_\Dens \in \RRR^{\DimIJK}$ is a 3D tensor, $\Grid_\Rad \in \RRR^{\DimIJK\times \DimCh}$ is a 4D tensor. Here $I$, $J$, $K$ correspond to the resolutions of the feature grid along the X, Y, Z axes, and $\DimCh$ is the number of appearance feature channels. 

We factorize these radiance field tensors to compact components. In particular, with the VM decomposition. the 3D geometry tensor $\Grid_\Dens$ is factorized as 
\begin{align}
    \Grid_{\Dens} = \sum_{r=1}^{R_\Dens} \Vector_{\Dens, r}^{X} \OuterP \Matrix_{\Dens,r}^{YZ} + \Vector_{\Dens,r}^{Y} \OuterP \Matrix_{\Dens,r}^{XZ} + \Vector_{\Dens,r}^{Z} \OuterP \Matrix_{\Dens,r}^{XY}
                 = \sum_{r=1}^{R_\Dens} \sum_{m\in XYZ} \Comp^m_{\Dens, r}
    \label{eqn:dense-vmr}
\end{align}



The appearance tensor $\Grid_\Rad$ has an additional mode corresponding to the feature channel dimension. Note that, compared to the XYZ modes, this mode is often of lower dimension, leading to a lower rank. Therefore, we do not combine this mode with other modes in matrix factors and instead only use vectors, denoted by $\AppVec_r$, for this mode in the factorization. 
Specifically, $\Grid_\Rad$ is factorized as
\begin{align}
    \Grid_{\Rad} = \sum_{r=1}^{R_\Rad}& \Vector_{\Rad, r}^{X} \OuterP \Matrix_{\Rad,r}^{YZ} \OuterP \AppVec_{3r-2} + \Vector_{\Rad,r}^{Y} \OuterP \Matrix_{\Rad,r}^{XZ} \OuterP \AppVec_{3r-1}
    + \Vector_{\Rad,r}^{Z} \OuterP \Matrix_{\Rad,r}^{XY} \OuterP \AppVec_{3r} \notag \\
    =\sum_{r=1}^{R_\Rad} &\Comp_{\Rad,r}^X \OuterP \AppVec_{3r-2} +\Comp_{\Rad,r}^Y \OuterP \AppVec_{3r-1} +\Comp_{\Rad,r}^Z \OuterP \AppVec_{3r}
    \label{eqn:rad-vmr}
\end{align}
Note that, we have $3R_\Rad$ vectors $\AppVec_r$ to match the total number of components.


Overall, we factorize the entire tensorial radiance field into $3R_\Dens+3R_\Rad$ matrices ($\Matrix_{\Dens,r}^{YZ}$,$...$,$\Matrix_{\Rad,r}^{YZ}$,...) and $3R_\Dens+6R_\Rad$ vectors ($\Vector_{\Dens, r}^{X}$,...,$\Vector_{\Rad, r}^{X}$,...,$\AppVec_r$). 
In general, we adopt $R_\Dens \ll I, J, K$ and $R_\Rad \ll I, J, K$, leading to a highly compact representation that can encode a high-resolution dense grid.
In essence, the XYZ-mode vector and matrix factors, $\Vector_{\Dens, r}^{X}$, $\Matrix_{\Dens,r}^{YZ}$, $\Vector_{\Rad, r}^{X}$, $\Matrix_{\Rad,r}^{YZ}$, ..., describe the spatial distributions of the scene geometry and appearance along their corresponding axes. 
On the other hand, the appearance feature-mode vectors $\AppVec_r$ express the global appearance correlations.
By stacking all $\AppVec_r$ as columns together, we have a $P\times 3R_\Rad$ matrix $\AppMat$; this matrix $\AppMat$ can also be seen as a global appearance dictionary that abstracts the appearance commonalities across the entire scene. 



\begin{figure*}[t]
    \includegraphics[width=\textwidth]{figs/newpipe-figure.pdf}
\caption{TensoRF (VM) reconstruction and rendering. We model radiance fields as tensors using a set of vectors ($\Vector$) and matrices ($\Matrix$), which describe the scene along their corresponding (XYZ) axes and are used for computing volume density $\Dens$ and view-dependent color $\Rad$ in differentiable ray marching.
For each shading location $\Pos=(x,y,z)$, we use linearly/bilinearly sampled values from the vector/matrix factors to efficiently compute the corresponding trilinearly interpolated values ($\Comp(\Pos)$) of the tensor components. The density component values ( $\Comp_\Dens(\Pos)$) are summed 
    to get the volume density ($\Dens$) directly. 
The appearance values ($\Comp_\Rad(\Pos)$) are concatenated into a vector ($\oplus[\Comp_\Rad^m(x)]_m$) that is then multiplied by an appearance matrix $\AppMat$ and sent to the decoding function $S$ for RGB color ($\Rad$) regression. 
}
    \label{fig:pipeline}\vspace{-5mm}
\end{figure*}


\subsection{Efficient feature evaluation.}


Our factorization-based model can compute each voxel's feature vector at low costs, only requiring one value per XYZ-mode vector/matrix factor.
We also enable efficient trilinear interpolation for our model, leading to a continuous field.  

\boldstartspace{Direct evaluation.}
With VM factorization, a density value $\Grid_{\Dens,ijk}$ of a single voxel at indices $ijk$ can be directly and efficiently evaluated by following Eqn.~\ref{eqn:vmrelement}:
\begin{align}
    \Grid_{\Dens,ijk} =  \sum_{r=1}^{R_\Dens} \sum_{m\in XYZ} \Comp^m_{\Dens,r,ijk}
    \label{eqn:dens-vmrelement}
\end{align}
Here, computing each $\Comp^m_{\Dens,r,ijk}$ only requires indexing and multiplying two values from its corresponding vector and matrix factors.

As for the appearance grid $\Grid_{\Rad}$, we always need to compute a full 
$\DimCh$-channel feature vector, which the shading function $\ShadFunc$ requires as input, corresponding to a 1D slice of $\Grid_{\Rad}$ at fixed XYZ indices $ijk$:
\begin{align}
    \Grid_{\Rad,ijk} =  \sum_{r=1}^{R_\Rad} \Comp^X_{\Rad,r,ijk} \AppVec_{3r-2} + \Comp^Y_{\Rad,r,ijk} \AppVec_{3r-1}
     + \Comp^Z_{\Rad,r,ijk} \AppVec_{3r}
    \label{eqn:rad-vmrslice}
\end{align}
Here, there's no additional indexing for the feature mode, since we compute a full vector.
We further simplify Eqn.~\ref{eqn:rad-vmrslice} by re-ordering the computation.
For this, we denote $\oplus [\Comp^m_{\Rad,ijk}]_{m,r}$ as the vector that stacks all $\Comp^m_{\Rad,r,ijk}$ values for $m=X,Y,Z$ and $r=1,...,R_\Rad$, which is a vector of $3R_\Rad$ dimensions; $\oplus$ can also be considered as the concatenation operator that concatenates all scalar values (1-channel vectors) into a $3R_\Rad$-channel vector in practice.
Using matrix $\AppMat$ (introduced in Sec.~\ref{sec:grids}) that stacks all $\AppVec_r$, Eqn.~\ref{eqn:rad-vmrslice} is equivalent to a matrix vector product:
\begin{align}
    \Grid_{\Rad,ijk} =  \AppMat (\oplus[\Comp^m_{\Rad,ijk}]_{m,r})
    \label{eqn:rad-vmrfeature}
\end{align}
Note that, Eqn.~\ref{eqn:rad-vmrfeature} is not only formally simpler but also leads to a simpler implementation in practice.
Specifically, when computing a large number of voxels in parallel, we first compute and concatenate $\Comp^m_{\Rad,r,ijk}$ for all voxels as column vectors in a matrix and then multiply the shared matrix $\AppMat$ once. 







\boldstartspace{Trilinear interpolation.}
We apply trilinear interpolation to model a continuous field.
Na\"{i}vely achieving trilinear interpolation is costly, as it requires evaluation of 8 tensor values and interpolating them, increasing computation by a factor of 8 compared to computing a single tensor element.
However, we find that trilinearly interpolating a component tensor is naturally equivalent to interpolating its vector/matrix factors linearly/bilinearly for the corresponding modes, thanks to the beauty of linearity of the trilinear interpolation and the outer product.


For example, given a component tensor $\Comp^X_r=\Vector_r^X \OuterP \Matrix_r^{YZ}$ with its each tensor element $\Comp_{r,ijk}=\Vector_{r,i}^X \Matrix_{r,jk}^{YZ}$, we can compute its interpolated values as:
\begin{equation}
    \Comp^X_r(\Pos) = \Vector_r^X(x)\Matrix_{r}^{YZ}(y,z)
    \label{eqn:xinter}
\end{equation}
where $\Comp^X_r(\Pos)$ is $\Comp_r$'s trilinearly interpolated value at location $\Pos=(x,y,z)$ in the 3D space, $\Vector_r^X(x)$ is $\Vector_r^X$'s linearly interpolated value at $x$ along X axis, and $\Matrix_{r}^{YZ}(y,z)$ is $\Matrix_{r}^{YZ}$'s bilinearly interpolated value at $(y,z)$ in the YZ plane. Similarly, we have $\Comp^Y_r(\Pos) = \Vector_r^Y(y)\Matrix_{r}^{XZ}(x,z)$ and $\Comp^Z_r(\Pos) = \Vector_r^Z(z)\Matrix_{r}^{XY}(x,y)$ (for CP decomposition $\Comp^\gamma_r(\Pos) = \Vector_r^X(x)\Vector_r^Y(y)\Vector_r^Z(z)$ is also valid).
Thus, trilinearly interpolating the two grids is expressed as
\begin{align}
    \Grid_{\Dens}(\Pos) &= \sum_r \sum_m \Comp^m_{\Dens,r}(\Pos) 
    \label{eqn:dens-vmrinter}
    \\
    \Grid_{\Rad}(\Pos) &=  \AppMat(\oplus[\Comp^m_{\Rad,r}(\Pos)]_{m,r})
    \label{eqn:rad-vmrinter}
\end{align}
These equations are very similar to Eqn.~\ref{eqn:dens-vmrelement} and \ref{eqn:rad-vmrfeature}, simply replacing the tensor elements with interpolated values.
We avoid recovering 8 individual tensor elements for trilinear interpolation and instead directly recover the interpolated value, leading to low computation and memory costs at run time.


\subsection{Rendering and reconstruction.}
\label{sec:rendering_reconstruction}
Equations~\ref{eqn:rf}, \ref{eqn:xinter}--\ref{eqn:rad-vmrinter} describe the core components of our model. By combining Eqn.~\ref{eqn:rf},\ref{eqn:dens-vmrinter},\ref{eqn:rad-vmrinter}, our factorized tensorial radiance field can be expressed as
\begin{equation}
    \Dens, \Rad=\sum_{r}\sum_m \Comp^m_{\Dens,r}(\Pos)\:,\:
    \ShadFunc(\AppMat(\oplus[\Comp^m_{\Rad,r}(\Pos)]_{m,r}), \Dir)
\end{equation}
i.e., we obtain continuous volume density and view-dependent color given any 3D location and viewing direction. 
This allows for high-quality radiance field reconstruction and rendering.
Note that, this equation is general and describes TensoRF with both CP and VM decomposition.
Our full pipeline of radiance field reconstruction and rendering with VM decomposition is illustrated in Fig.~\ref{fig:pipeline}.

\boldstartspace{Volume rendering.} 
To render images, we use differentiable volume rendering, following NeRF \cite{mildenhall2020nerf}. Specifically, for each pixel, we march along a ray, sampling $Q$ shading points along the ray and computing the pixel color by
\begin{equation}
        \Color =  \sum_{q=1}^Q \Trans_q (1-\exp (-\Dens_q \Step_q)) \Rad_q, \ 
        \Trans_q = \exp (-\sum_{p=1}^{q-1} \Dens_p \Step_p )
    \label{eq:raymarching}
\end{equation}
Here, $\Dens_q$, $\Rad_q$ are the corresponding density and color computed by our model at their sampled locations $\Pos_q$; $\Delta_q$ is the ray step size and $\Trans_q$ represents transmittance. 

\boldstartspace{Reconstruction.}
Given a set of multi-view input images with known camera poses, our tensorial radiance field is optimized per scene via gradient descent, minimizing an L2 rendering loss, using only the ground truth pixel colors as supervision.
Our radiance field is explained by tensor factorization and modeled by a set of global vectors and matrices as basis factors that correlate and regularize the entire field in the optimization.
However, this can sometimes lead to overfitting and local minima issues in gradient descent,
leading to outliers or noises in regions with fewer observations.
We utilize standard regularization terms that are commonly used in compressive sensing, including an L1 norm loss and a TV (total variation) loss on our vector and matrix factors, which effectively address these issues.
We find that only applying the L1 sparsity loss is adequate for most datasets. However, for real datasets that have very few input images (like LLFF\cite{llff}) or imperfect capture conditions (like Tanks and Temples \cite{Knapitsch2017,liu2020neural} that has varying exposure and inconsistent masks), a TV loss is more efficient than the L1 norm loss.




To further improve quality and avoid local minima, we apply coarse-to-fine reconstruction. Unlike previous coarse-to-fine techniques that require unique subdivisions on their sparse chosen sets of voxels, our coarse-to-fine reconstruction is simply achieved by linearly and bilinearly upsampling our XYZ-mode vector and matrix factors.
 
\section{Implementation details}





\label{sec:impl}
We briefly discuss our implementation; please refer to the appendix for more details.
We implement our TensoRF using PyTorch \cite{pytorch}, without customized CUDA kernels.
We implement the feature decoding function $\ShadFunc$ as either an MLP or SH function and use $P=27$ features for both. For SH, this corresponds to 3rd-order SH coefficients with RGB channels. For neural features, we use a small MLP with two FC layers (with 128-channel hidden layers) and ReLU activation.

We use the Adam optimizer \cite{adam} with initial learning rates of 0.02 for tensor factors and (when using neural features) 0.001 for the MLP decoder.
We optimize our model for $T$ steps with a batch size of 4096 pixel rays on a single Tesla V100 GPU (16GB).
We apply a feature grid with a total number of $N^3$ voxels; the actual resolution of each dimension is computed based on the shape of the bounding box. 
To achieve coarse-to-fine reconstruction, we start from an initial low-resolution grid with $N_0^3$ voxels with $N_0=128$; we then
 upsample the vectors and matrices linearly and bilinearly at steps 2000, 3000, 4000, 5500, 7000 with the numbers of voxels interpolated between $N_0^3$ and $N^3$ linearly in logarithmic space. 
 Please refer to Sec.~\ref{sec:exp} for the analysis on different total steps ($T$), different resolutions ($N$), and different number of total components ($3R_\Dens+3R_\Rad$).
 





































\section{Experiments}
\label{sec:exp}
We now present an extensive evaluation of our tensorial radiance fields. We first analyze our decomposition techniques, the number of components, grid resolutions, and optimization steps. We then compare our approach with previous and concurrent works on both 360$^\circ$ objects and forward-facing datasets.

\boldstartspace{Analysis of different TensoRF models.}
We evaluate our TensoRF on the Synthetic NeRF dataset \cite{mildenhall2020nerf} using both CP and VM decompositions with different numbers of components and different numbers of grid voxels.
Table~\ref{tab:decomp} shows the averaged rendering PSNRs, reconstruction time, and model size for each model. We use the same MLP decoding function (as described in Sec.~\ref{sec:impl}) for all variants and optimize each model for 30k steps with a batch size of 4096.

Note that both TensoRF-CP and TensoRF-VM achieve consistently better rendering quality with more components or higher grid resolutions. 
TensoRF-CP achieves super compact modeling; even the largest model with 384 components and $500^3$ voxels requires less than 4MB.
This CP model also achieves the best rendering quality in all of our CP variants, leading to a high PSNR of 31.56, which even outperforms vanilla NeRF (see Tab.~\ref{table:results}).

On the other hand, because it compresses more parameters in each component, TensoRF-VM achieves significantly better rendering quality than TensoRF-CP; even the smallest TensoRF-VM model with only 48 components and $200^3$ voxels is able to outperform the best CP model that uses many more components and voxels.
Remarkably, the PSNR of 31.81 from this smallest VM model (which only takes $8.6$ MB) is already higher than the PSNRs of NeRF and many other previous and concurrent techniques (see Tab.~\ref{table:results}). 
In addition, 192 components are generally adequate for TensoRF-VM; doubling the number to 384 only leads to marginal improvement. 
TensoRF-VM with $300^3$ voxels can already lead to high PSNRs close to or greater than 33, while retaining compact model sizes ($<$72MB).
Increasing the resolution further leads to improved quality, but also larger model size.

Also note that all of our TensoRF models can achieve very fast reconstruction. Except for the largest VM model, all models finish reconstruction in less than 30 min, significantly faster than NeRF and many previous methods (see Tab.~\ref{table:results}). 


\boldstartspace{Optimization steps.}
We further evaluate our approach with different optimization steps for our best CP model and the VM models with $300^3$ voxels. PSNRs and reconstruction time are shown in Tab.~\ref{tab:steps}.
All of our models consistently achieve better rendering quality with more steps.
Our compact CP-384 model (3.9MB) can even achieve a PSNR greater than 32 after 60k steps, higher than the PSNRs of all previous methods in Tab.~\ref{table:results}.
On the other hand, our VM models can quickly achieve high rendering quality in very few steps. With only 15k steps, many models achieve high PSNRs that are already state-of-the-art.

\definecolor{bronze}{rgb}{1,1,0.6}
\definecolor{silve}{rgb}{0.969,0.796,0.600}
\definecolor{gold}{rgb}{0.941,0.592,0.600}


\newcommand{\gold}[1]{\colorbox{gold}{{#1}}}
\newcommand{\silve}[1]{\colorbox{silve}{{#1}}}
\newcommand{\bronze}[1]{\colorbox{bronze}{{#1}}}


\setlength{\tabcolsep}{4pt}
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{


\begin{tabular}{l|cc|cc|cc|cc|cc}
& \multicolumn{6}{c|}{Synthetic-NeRF} & \multicolumn{2}{c|}{NSVF} & \multicolumn{2}{c}{TanksTemples} \\
Method & BatchSize & Steps & Time $\downarrow$  & Size(MB)$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ \\
\hline

SRN~\cite{sitzmann2019scene}          &  - & -       & $>$10h     & - &22.26 & 0.846 &   24.33 & 0.882  &  24.10 & 0.847 \\NSVF~\cite{liu2020neural}             & 8192 & 150k  & $>$48$^*$h &  -    & 31.75 & 0.953 &   35.18 & \silve{0.979}   &  \silve{28.48} & 0.901 \\NeRF~\cite{mildenhall2020nerf}        & 4096 & 300k  & $\sim$35h  &  \silve{5.00}    & 31.01 & 0.947 &   30.81 & 0.952   &  25.78 & 0.864 \\SNeRG~\cite{hedman2021baking}         & 8192 & 250k  & $\sim$15h      & 1771.5& 30.38 & 0.950 &  -      & -       & -      & -     \\PlenOctrees~\cite{yu2021plenoctrees}  & 1024 & 200k  & $\sim$15h      & 1976.3& 31.71 & \bronze{0.958} &   -     & -       &  27.99 & \silve{0.917} \\Plenoxels~\cite{yu2021plenoxels}      & 5000 & 128k  & \silve{11.4m}  & 778.1 & 31.71 & \bronze{0.958} &   -     & -       &  27.43 & 0.906 \\DVGO~\cite{sun2021direct}             & {5000} & \silve{30k}   & 15.0m          & 612.1 & 31.95 & 0.957 &   35.08 & 0.975.  &  \bronze{28.41} & 0.911 \\\hline
    Ours-CP-384                            & {4096} & \silve{30k}  & 25.2m           &\gold{3.9}    & 31.56 & 0.949 &   34.48 & 0.971   &  27.59 & 0.897 \\
    Our-VM-192-SH                          & {4096} & \silve{30k}  & 16.8m           &71.9   & 32.00 & 0.955 &   35.30 & 0.977   &  27.81 & 0.907 \\
    Ours-VM-48                             & {4096} & \silve{30k}  & \bronze{13.8m}  &\bronze{18.9}   & \bronze{32.39} & 0.957 &   \silve{35.34} & 0.976   &  28.06 & 0.909 \\
    Ours-VM-192                            & {4096} & \gold{15k}  & \gold{8.1m}     &71.8   & \silve{32.52} & \silve{0.959} &   \bronze{35.59} & \bronze{0.978}   &  28.07 & \bronze{0.913} \\
    Ours-VM-192                            & {4096} & \silve{30k}  & 17.4m     &71.8   & \gold{33.14} & \gold{0.963} &   \gold{36.52} & \gold{0.982}   &  \gold{28.56} & \gold{0.920} \\
    
\end{tabular}
} 
\vspace{1mm}
\caption{We compare our method with previous and concurrent novel view synthesis methods on three datasets. All scores of the baseline methods are directly taken from their papers whenever available. We also report the averaged reconstruction time and model size for the Synthetic-NeRF dataset. NVSF requires 8 GPUs for optimization (marked by a star), while others run on a single GPU. DVGO's 30k steps correspond to 10k for coarse and 20k for fine reconstruction.} \vspace{-6mm}

\label{table:results}  
\end{table}
\setlength{\tabcolsep}{1.4pt}


\begin{table*}[tpb]
    \centering
\centering
        \renewcommand\tabcolsep{5.0pt}
        \begin{tabular}{c|c|c|c|c}
        \hline
        &$\#$Comp                        &         $200^3$       &    $300^3$            &      $500^3$\\
        \hline\hline
    \multirow{4}{*}{TensoRF-CP} &48   & 27.98/09:29/0.74      &  28.24/11:45/1.09     &  28.38/14:20/1.85 \\
                                &96   & 28.50/09:57/0.88      &  28.83/12:12/1.29     &  29.06/15:27/2.18 \\
                                &192  & 29.50/11:09/1.08      &  29.99/13:41/1.59     &  30.33/18.03/2.66 \\
                                &384  & 30.47/14:41/1.59      &  31.08/18:09/2.33     &  31.56/25:11/3.93 \\

        \hline
    \multirow{4}{*}{TensoRF-VM} &48   & 31.81/11:29/08.6 &     32.39/13:51/23.5 &   32.63/18:17/55.8  \\
                                &96   & 32.33/11:54/16.5 &     32.86/14:08/37.3 &   33.06/20:11/105. \\
                                &192  & 32.63/13:26/32.3 &     33.14/17:36/76.7 &   33.31/27.18/204. \\
                                &384  & 32.69/17:24/63.4 &     33.21/25:14/143. &   33.39/43:19/397.\\
        \hline
        \end{tabular}
    \caption{
    We compare the averaged PSNRs / optimization time (mm:ss) / model sizes (MB) of CP and VM TensoRF models on Synthetic NeRF dataset \cite{mildenhall2020nerf} with different numbers of components and grid resolutions, optimized for 30k steps.
}\vspace{-6mm}
    \label{tab:decomp}
\end{table*}


\begin{table*}[!t]

  \begin{minipage}{0.51\columnwidth}
    \centering
\resizebox{\textwidth}{!}{\begin{tabular}{c|cccc|cccc}
& 5k & 15k & 30k & 60k        & 5k & 15k & 30k & 60k\\
        \hline CP-384 & 28.37 & 30.80 & 31.56 & 32.03  & 03:03 & 11:30 & 25:11 & 51:47\\
            VM-48  & 29.28 & 31.80 & 32.39 & 32.68  & 01:57 & 06:21 & 13:51 & 27:20\\
            VM-96  & 29.65 & 32.26 & 32.86 & 33.17  & 02:01 & 06:41 & 14:08 & 28:57\\
            VM-192 & 29.86 & 32.52 & 33.14 & 33.44  & 02:16 & 08:08 & 17:37 & 35:50\\
            VM-384 & 29.95 & 32.62 & 33.21 & 33.52  & 02:51 & 11:30 & 25:14 & 52:50\\
        \hline
        \end{tabular}
    }
    \caption{PSNRs and time of CP and VM models with different training steps on the Synthetic-NeRF dataset \cite{mildenhall2020nerf}.}
    \label{tab:steps}
  \end{minipage}\hfill \begin{minipage}{0.46\columnwidth}
    \centering
\resizebox{\textwidth}{!}{\begin{tabular}{l|cccc}
Method  & Time $\downarrow$ & Size & PSNR$\uparrow$ & SSIM$\uparrow$\\
        \hline
        
NeRF~\cite{mildenhall2020nerf}        & 36h          & \textbf{5.00M}    & 26.50 & 0.811\\Plenoxels~\cite{yu2021plenoxels}      & 24:00m       & 2.59G & 26.29 & 0.829 \\\hline
Ours-VM-48                             & \textbf{19:44m}        &90.4M   & 26.51 & 0.832\\
            Ours-VM-96                            & 25.43m        &179.7M   & \textbf{26.73} & \textbf{0.839} \\
        \hline
    \end{tabular}
    }
    \caption{Quantitative comparisons of our method with NeRF and Plenoxels on forward-facing scenes \cite{llff}.}
    \label{table:score_llff} 
  \end{minipage}\vspace{-10mm}
\end{table*}

\boldstartspace{Comparisons on 360$^\circ$ scenes.}
We compare our approach with state-of-the-art novel view synthesis methods, including previous works (SRN\cite{sitzmann2019scene}, NeRF\cite{mildenhall2020nerf}, NSVF\cite{liu2020neural}), SNeRG\cite{hedman2021baking}, PlenOctrees\cite{yu2021plenoctrees}) and concurrent works (Plenoxels \cite{yu2021plenoxels}, DVGO\cite{sun2021direct}).
In particular, we compare with them using our best CP model and our VM models (300$^3$ voxels) with 48 and 192 components.
We also show a 192-component VM model with spherical harmonics shading function.
Table ~\ref{table:results} shows the quantitative results (PSNRs and SSIMs) of ours and comparison methods on three challenging datasets, where we also show the corresponding batch size, optimization steps, time, and final output model size for each model, to compare all methods from multiple perspectives. 
Note that all of our CP and VM models can outperform NeRF on all three datasets while taking substantially less optimization time and fewer steps.
Our best VM-192 model can even achieve the best PSNRs and SSIMs on all datasets, significantly outperforming the comparison methods.
Our approach can also achieve qualitatively better renderings with more appearance and geometry details and less outliers, as shown in 
Fig.~\ref{fig:restuls}.








Our models are highly efficient, which all require less than 75MB space and can be reconstructed in less than 30 min.
This corresponds to more than 70x speed up compared to NeRF that requires about 1.5 days for optimization.
Our CP model is even more compact than NeRF.
Moreover, SNeRG and PlenOctrees require pre-training a NeRF-like MLP, requiring long reconstruction time too.
DVGO and Plenoxels are concurrent works, which can also achieve fast reconstruction in less than 15 min. 
However, as both are voxel-based methods and directly optimize voxel values, they lead to huge model sizes 
similar to previous voxel-based methods like SNeRG and PlenOctrees.
In contrast, we factorize feature grids and model them with compact vectors and matrices, leading to substantially smaller model sizes.
Meanwhile, our VM-192 can even reconstruct faster than DVGO and Plenoxels, taking only 15k steps, and achieving better quality in most cases. 
In fact, Plenoxels' fast reconstruction relies on quickly optimizing significantly more steps ($>4$ times our steps) with their CUDA implementation. 
Our models are implemented with standard PyTorch modules and already achieve much better rendering quality with fewer steps taking comparable and even less reconstruction time than Plenoxels. Note that our SH model essentially represents the same underlying feature grid as Plenoxels but can still lead to more compact modeling and better quality with fewer steps, showing the advantages of our factorization based modeling.
In general, our approach enables fast reconstruction, compact modeling, and photo-realistic rendering simultaneously.



\boldstartspace{Forward-facing scenes.}
Our approach can also achieve efficient and high-quality radiance field reconstruction for forward-facing scenes.
We show quantitative results of our two VM models on the LLFF dataset \cite{llff} and compare with NeRF and Plenoxels in Tab.~\ref{table:score_llff}.
Our models outperform the previous state-of-the-art NeRF and take significantly less reconstruction time.
Compared with Plenoxels \cite{yu2021plenoxels}, our approach leads to comparable or faster reconstruction speed, better quality, and substantially smaller model sizes.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/comp_syn_nerf.pdf}
    \vspace{-2mm}
    \caption{Qualitative results of our VM-192-30k model and comparison methods (NeRF\cite{mildenhall2020nerf}, plenoxels \cite{yu2021plenoxels}, DVGO \cite{sun2021direct}, NSVF \cite{liu2020neural}) on two Synthetic NeRF scenes. }
    \label{fig:restuls}\vspace{-5mm}
\end{figure}















\section{Conclusion.}
We have presented a novel approach for high-quality scene reconstruction and rendering. We propose a novel scene representation -- TensoRF which leverages tensor decomposition techniques to model \fv{and reconstruct} radiance fields compactly as factorized low-rank tensor components.
While our framework accommodates classical tensor factorization techniques (like CP), we introduce a novel vector-matrix decomposition that leads to better reconstruction quality and faster optimization speed.
Our approach enables highly efficient radiance field reconstruction in less than 30 min per scene, leading to better rendering quality compared to NeRF that requires substantially longer training time (20+ hours).
Moreover, our tensor factorization based method achieves high compactness, leading to a memory footprint of less than 75MB, substantially smaller than many other previous and concurrent voxel-grid based methods. 
\fv{We hope our findings in tensorized low-rank feature modeling can inspire other modeling and reconstruction tasks.}



















 


%
 

\bibliographystyle{splncs04}
\bibliography{tex/reference}

\appendix
\clearpage{}





\renewcommand{\gold}[1]{\colorbox{bronze}{{#1}}}
\renewcommand{\gold}[1]{\textbf{#1}}

\section{TensoRF Representation Details.}
We illustrate the feature grid of our tensorial radiance field and the tensor factors in TensoRF with both CP and VM decompositions in Fig.~\ref{fig:grids}.

\boldstartspace{Number of components.}
The total number of tensor components ($\#$Comp) is $(R_\Dens+R_\Rad)$ for TensoRF-CP and $3(R_\Dens+R_\Rad)$ for TensoRF-VM (because VM has three types of components). Therefore, the $R$ we use for TensoRF-CP is three times as large as the $R$ used for TensoRF-VM to achieve the same number of components shown in Tab.~2.
We also find that using $R_\Dens<R_\Rad$ is usually better than $R_\Dens=R_\Rad$ when $R_\Dens$ is large enough ($>8$).
In particular, for TensoRF-VM, we use $R_\Dens=R_\Rad=8$ for $\#\text{Comp}=48$; $R_\Dens=8, R_\Rad=24$ for $\#\text{Comp}=96$; $R_\Dens=16,R_\Rad=48$ for $\#\text{Comp}=192$; $R_\Dens=32,R_\Rad=96$ for $\#\text{Comp}=384$. Note that, as discussed in Eqn.~\ref{eqn:vmrrr},\ref{eqn:vmr}, here we apply the same number of components for $\Comp^X$, $\Comp^Y$, $\Comp^Z$ with $R_1=R_2=R_2=R$ for both density and appearance (where $R$ is $R_\Dens$ and $R_\Rad$ respectively), assuming the three spatial dimensions are equally complex.

\boldstartspace{Forward-facing settings.}
We use the above settings with $R_1=R_2=R_2$, for all 360$^\circ$ object datasets in Tab.~\ref{table:results}.  On the other hand, Forward-facing scenes apparently appear differently in the three dimensions; especially, in NDC space, the $X$ and $Y$ spatial modes (corresponding to the image plane) contains more appearance information that is visible to rendering viewpoints. We therefore use more components for the $X-Y$ plane, corresponding to $\Comp^Z=\Vector^Z \circ \Matrix^{X,Y}$. In this case, these $\Comp^Z$ components can also be seen as special compressed versions of neural MPIs. In particular, the detailed numbers of components we use for generating the results in Tab.~\ref{table:score_llff} are: for $\#$Comp=48 $R_{\Dens,1}=R_{\Dens,2}=4$,$R_{\Dens,2}=16$,$R_{\Rad,1}=R_{\Rad,2}=4$,$R_{\Rad,2}=16$; for $\#$Comp=96, $R_{\Dens,1}=R_{\Dens,2}=4$,$R_{\Dens,2}=16$,$R_{\Rad,1}=R_{\Rad,2}=16$,$R_{\Rad,48}=16$.

\boldstartspace{Number of parameters.}
We briefly discuss the number of parameters in our model.
With the same $\#$Comp, when $I=J=K$ and $R_\Dens+R_\Rad=R$, the total number of parameters used for TensoRF-CP is $3KR+PR_\Rad$; for Tensor-VM, the number is $K^2R+KR+PR_\Rad$ (here considering $R_\Dens/3$, $R_\Rad/3$ are used to make the $\#$Comp the same as TensoRF-CP ). For example, for a $300\times300\times300$ feature grid with $P=27$ channels (plus one density channel), the total number of parameters in a dense grid is 756 M; the number of parameters used for TensoRF-CP (when $R=192$) is 360 K; the number of parameters used for TensoRF-VM (when $R=192$) is 17 M.
Our CP and VM model can achieve $0.048\%$ and $2.25\%$ compression rates respectively.










\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/feature-grid.pdf}
\caption{Feature grids and factorized tensors in TensoRF. We leverage a regular voxel grid $\Grid$, covering a 3D scene, to model a radiance field of the scene. Each voxel of $\Grid$ contains multi-channel features, where one channel represents the volume density ($\Dens$) and the remaining $P$ channels lead to  an appearance feature vector ($f_\Rad$) for computing view-dependent colors. We split the density and appearance features into two feature grids $\Grid_\Dens$ and $\Grid_\Rad$, consider them as 3D and 4D tensors, and factorize them into compact factors with outer products. TensoRF with CP decomposition factorize the tensors into only vectors and TensoRF with VM decomposition factorize the tensor into vector and matrix factors (Eqn.~\ref{eqn:dense-vmr}, \ref{eqn:rad-vmr}). Note that, each voxel of the original grid is only related to one value from each XYZ-mode vector/matrix factor in both decompositions, which are marked in the figure.}
    \label{fig:grids}
\end{figure}

\section{More Implementation Details.}

\boldstartspace{Loss functions.}
As described in sec. \ref{sec:rendering_reconstruction}, we apply a L2 rendering loss and additional regularization terms to optimize our tensor factors for radiance field reconstruction.
In general, this loss function is expressed by


\begin{equation}
\begin{aligned}
    \loss = \|\Color - \tilde{\Color} \|^2_2 + \omega  \cdot \loss_{reg}
\label{eq:loss}
\end{aligned}
\end{equation}
Where $\tilde{\Color}$ is the groundtruth color, $\omega$ is weight of the regularization term.

To encourage the sparsity in the parameters of our tensor factors, we apply the standard L1 regularization, which we find is effective in improving the quality in extrapolating views and removing floaters/outerliers in final renderings. Note that, unlike previous methods \cite{hedman2021baking,sun2021direct} that penalize predicted per-point density with a Cauchy loss or entropy loss, our L1 regularizer is much simpler and directly applied on the parameters of tensor factors. 
We find that it is sufficient to  apply the L1 sparsity loss only on the density parameters, expressed by
\begin{equation}
\begin{aligned}
    \loss_{L1} = \frac{1}{N}\sum_{r=1}^{R_\Dens} (\|\Matrix_{\Dens,r}\| + \|\Vector_{\Dens, r}\|),
\label{eq:loss_reg}
\end{aligned}
\end{equation}
where $\|\Matrix_{\Dens,r}\|$ and $\|\Vector_{\Dens, r}\|)$ are simply the sum of absolute values of all elements, and $N= R_\Dens \cdot (I \cdot J + I \cdot K+J \cdot K + I + J + K)$ is the total number of parameters.
We use this L1 sparsity loss with a $\omega=0.0004$ for the Synthetic NeRF and Synthetic NSVF datasets. An ablation study on this L1 loss on the Synthetic NeRF dataset is shown in Tab.~\label{tab:ab_regularization}.

For real datasets that have very few input images (like LLFF\cite{llff}) or imperfect capture conditions (like Tanks and Temples \cite{Knapitsch2017} that has varying exposure and inconsistent masks), we find a TV loss is more efficient
than the L1 sparsity loss, expressed by

\begin{equation}
\begin{aligned}
    \loss_{TV} =  \frac{1}{N} \sum (\sqrt{\triangle^2 \Comp^m_{\Dens,r}} + 0.1 \cdot \sqrt{\triangle^2 \Comp^m_{\Color,r}}),
\label{eq:loss_reg}
\end{aligned}
\end{equation}

Here $\triangle^2$ is the squared difference between the neighboring values in the matrix/vector factors; we apply a smaller weight (weighted by 0.1 additionally) on appearance parameters in the TV loss.
We use $\omega=1$ when using this TV loss.

\boldstart{Binary occupancy volume.}
To facilitate reconstruction, we compute a binary occupancy mask grid at steps 2000 and 4000 using the volume density prediction from the intermediate TensoRF model to avoid computation in empty space.
For datasets that do not provide bounding boxes, we start from a conservatively large box and leverage the occupancy mask computed at step 2000 to re-compute a more compact bounding box, with which we shrink and resample our tensor factors, leading to more precise modeling. For forward-facing scenes in the LLFF dataset \cite{llff}, we apply NDC transformation that bounds the scene in a perspective frustum.
 

\boldstart{More details.}
As described in Sec.~\ref{sec:impl}, we use a small two-layer MLP with 128 channels in hidden layers as our neural decoding function.
In particular, the input to this MLP contains the viewing direction and the features recovered by our tensor factors (no xyz positions are used). Similar to NeRF and NSVF \cite{mildenhall2020nerf,liu2020neural}, we also apply frequency encodings (with Sin and Cos functions) on both the viewing direction and features. Unlike NeRF that uses ten different frequencies, we use only two.


During optimization, we also apply an exponential learning rate decay to make the optimization more stable when the reconstruction is being finished. Specifically, we decay our initial learning rates at every training step, until decayed by a factor of 0.1 in the end of the optimization. 




\section{More Evaluation.}
We perform an ablation study to evaluate our L1 regularization.
Tab. \ref{tab:ab_regularization} shows how our framework performs by removing the L1 regularization on the Synthetic-NeRF dataset, our models exceed NeRF fidelity ($31.01$ in average) even without regularization. 
We observe the performance gap between w/ and w/o L1 regularization is mostly caused by the floaters in the empty space.
We also provide more results on our model with different numbers of training steps in Tab.~\ref{tab:steps_more}, which is basically a detailed version of Tab.~\ref{tab:steps} with more settings.
These results showcase that our models consistently improve when training with more iterations.



\begin{table*}[htpb]
    \centering
\centering
        \renewcommand\tabcolsep{5.0pt}
        \begin{tabular}{c|cccc}
& PSNR & SSIM & LPIPS$_{VGG}$&LPIPS$_{Alex}$ \\
        \hline CP-384      & 31.56/31.23 & 0.949/0.947 & 0.076/0.078 & 0.041/0.043 \\
            VM-48       & 32.39/31.71 & 0.957/0.953 & 0.057/0.062 & 0.032/0.036  \\
            VM-192-SH   & 32.00/31.14 & 0.955/0.949 & 0.058/0.068 & 0.058/0.044  \\
            VM-192      & 33.14/32.43 & 0.963/0.960 & 0.047/0.052 & 0.027/0.030  \\
        \hline
        \end{tabular}
    \caption{We compare the averaged scores against w/o L1 regularization on the Synthetic-NeRF dataset.}
    \label{tab:ab_regularization} \vspace{-2mm}
\end{table*}



\begin{table*}[htpb]
    \centering
\centering
        \renewcommand\tabcolsep{4.0pt}
        \begin{tabular}{c|ccccccccccc}
        \hline
                        & 5k & 8k & 10k & 12k & 15k & 20k & 30k & 40k & 60k & 100k \\
        \hline\hline
            CP-192 & 28.38 & 29.13 & 29.93 & 30.38 & 30.80 & 31.18 & 31.56 & 31.75 & 32.03 & 32.18 \\
            VM-48  & 29.28 & 30.39 & 31.11 & 31.47 & 31.80 & 32.08 & 32.39 & 32.55 & 32.68 & 32.84 \\
            VM-96  & 29.65 & 30.72 & 31.52 & 31.93 & 32.26 & 32.56 & 32.86 & 33.00 & 33.17 & 33.29 \\
            VM-192 & 29.86 & 30.93 & 31.74 & 32.17 & 32.52 & 32.85 & 33.14 & 33.27 & 33.44 & 33.54 \\
            VM-384 & 29.95 & 30.88 & 31.75 & 32.20 & 32.62 & 32.94 & 33.21 & 33.35 & 33.52 & 33.64 \\
        \hline
        \end{tabular}
    \caption{PSNRs on the Synthetic NeRF datasets with different numbers of training steps. This is more detailed version than Tab.~\ref{tab:steps}.}
    \label{tab:steps_more} \vspace{-4mm}
\end{table*} 




\section{Discussion}

In fact, the reconstruction problem with dense feature grid representation is relatively over-parameterized/under-determined; e.g., a $300^3$ grid with $27$ channels has $>$700M parameters, while one hundred $800\times800$ images provide only 64M pixels for supervision.
Therefore, many design choices -- including pruning empty voxels, coarse-to-fine reconstruction, and adding additional losses, which have been similarly used in TensoRF and concurrent works (DVGO, Plenoxels) -- are all essentially trying to reduce/constrain the parameter space and avoid over-fitting. 
In general, low-rank regularization is crucial in addressing many reconstruction problems, like matrix completion \cite{candes2010matrix}, compressive sensing \cite{dong2014compressive}, denoising \cite{ji2010robust};
tensor decomposition has also been widely used in tensor completion \cite{liu2012tensor,gandy2011tensor}, which is similar to our task. 
Tensor decomposition naturally provides low-rank constraints and reduces parameters; this similarly benefits the radiance field reconstruction as demonstrated by our work.

Moreover, TensoRF represents a 5D radiance field function that expresses both scene geometry and appearance;
hence, we believe our 4D tensor is generally low-rank, because a 3D scene typically contains a lot of similar geometry structures and material properties across different locations. 
Note that, in various appearance acquisition tasks, similar low-rank constraints have been successfully applied for reconstructing other functions, including the 4D light transport function in relighting \cite{wang2009kernel} and the 6D SVBRDF function in material reconstruction \cite{zhou2016sparse,nam2018practical} (where a common idea is to model a sparse set of basis BRDFs; this is similar to our modeling of vector components in the feature dimension in the matrix $\textbf{B}$).
We combine low-rank constraints and neural networks from a novel perspective, in tensor-based radiance field reconstruction.
TensoRF essentially models the scene with global basis components, discovering the scene geometry and appearance commonalities across the spatial and feature dimensions.




\section{Limitations and Future Work.}
Our approach achieves high-quality radiance field reconstruction for 360$^\circ$ objects and forward-facing scenes; however, our method currently only supports bounded scenes with a single bounding box and cannot handle unbounded scenes with both foreground and background content. 
Combining our method with techniques like NeRF++ \cite{zhang2020nerf++} to separately model a foreground field inside a regular box and a background field inside another box defined in a spherical coordinate space can potentially extend our method to address unbounded scenes.
Despite the success in per-scene optimization shown in this paper, 
an interesting future direction is to discover and learn general basis factors across scenes on a large-scale dataset, leveraging data priors to further improve the quality or enable other applications like GANs (as done in GRAF \cite{schwarz2020graf}, GIRRAF \cite{niemeyer2021giraffe} and EG3D \cite{Chan2022EG3D}).



\section{Acknowledgements}
We would like to thank Yannick Hold-Geoffroy for his useful tips in video animation, Qiangeng Xu for providing some baseline results, and, Katja Schwarz and Michael Niemeyer for providing helpful video materials. This project was supported by NSF grant IIS-1764078 and gift money from Kingstar. \fv{We would also like to thank all anonymous reviewers for their encouraging  comments.}

\section{Per-scene Breakdown.}
\label{sec:perscene}

Tab. \ref{tab:supp_breakdown_nerf}-\ref{tab:supp_breakdown_llff} provide a per-scene break down for quantity metrics in Synthesis-nerf \cite{martin2021nerf}, Synthe-nsvf \cite{liu2020neural}, Tanks$\&$Templates \cite{Knapitsch2017} and forward-facing \cite{llff} dataset.

\begin{table*}[htpb]
    \vspace{2em}
    \centering
    \begin{tabular}{l|c|cccccccc}
    \hline
    Methods & Avg. & {\it Chair} & {\it Drums} & {\it Ficus} & {\it Hotdog} & {\it Lego} & {\it Materials} & {\it Mic} & {\it Ship} \\
    \hline\hline
    \multicolumn{9}{@{}l}{\rule{0pt}{3ex}\bf PSNR$\uparrow$} \\
    \hline
    SRN~\cite{sitzmann2019scene} & 22.26 & 26.96 & 17.18 & 20.73 & 26.81 & 20.85 & 18.09 & 26.85 & 20.60 \\
    NeRF~\cite{mildenhall2020nerf} & 31.01 & 33.00 & 25.01 & 30.13 & 36.18 & 32.54 & 29.62 & 32.91 & 28.65 \\
    NSVF~\cite{liu2020neural} & 31.75 & 33.19 & 25.18 & 31.23 & 37.14 & 32.29 & 32.68 & 34.27 & 27.93 \\
    SNeRG~\cite{hedman2021baking} & 30.38 & 33.24 & 24.57 & 29.32 & 34.33 & 33.82 & 27.21 & 32.60 & 27.97 \\
    PlenOctrees~\cite{yu2021plenoctrees} & 31.71 & 34.66 & 25.31 & 30.79 & 36.79 & 32.95 & 29.76 & 33.97 & 29.42 \\
    Plenoxels~\cite{yu2021plenoxels} & 31.71 & 33.98 & 25.35 & 31.83 & 36.43 & 34.10 & 29.14 & 33.26 & 29.62 \\
    DVGO~\cite{sun2021direct} & 31.95 & 34.09 & 25.44 & 32.78 & 36.74 & 34.64 & 29.57 & 33.20 & 29.13 \\

    \hline
    Ours-CP-384  & 31.56 & 33.60 & 25.17 & 30.72 & 36.24 & 34.05 & 30.10 & 33.77 & 28.84 \\
    Ours-VM-192-SH  & 32.00 & 34.68 & 25.37 & 32.30 & 36.30 & 35.42 & 29.30 & 33.21 & 29.46 \\
    Ours-VM-48  & 32.39 & 34.68 & 25.58 & 33.37 & 36.81 & 35.51 & 29.45 & 33.59 & 30.12 \\
    Ours-VM-192-15k & 32.52 & 34.95 & 25.63 & 33.46 & 36.85 & 35.78 & 29.78 & 33.69 & 30.04\\
    Ours-VM-192-30k & \gold{33.14} & \gold{35.76} & \gold{26.01} & \gold{33.99} & \gold{37.41} & \gold{36.46} & \gold{30.12} & \gold{34.61} & \gold{30.77} \\

    \hline
    \end{tabular}
    \caption{PSNR results on each scene from the {\bf Synthetic-NeRF}~\cite{mildenhall2020nerf} dataset. }
    \label{tab:supp_breakdown_nerf}
    \vspace{2em}
\end{table*}

\begin{table*}[htpb]
    \vspace{2em}
    \centering
    \begin{tabular}{l|c|cccccccc}
    \hline
    Methods & Avg. & {\it Chair} & {\it Drums} & {\it Ficus} & {\it Hotdog} & {\it Lego} & {\it Materials} & {\it Mic} & {\it Ship} \\
    \hline\hline








    \multicolumn{9}{@{}l}{\rule{0pt}{3ex}\bf SSIM$\uparrow$} \\
    \hline
    SRN~\cite{sitzmann2019scene} & 0.846 & 0.910 & 0.766 & 0.849 & 0.923 & 0.809 & 0.808 & 0.947 & 0.757 \\
    NeRF~\cite{mildenhall2020nerf} & 0.947 & 0.967 & 0.925 & 0.964 & 0.974 & 0.961 & 0.949 & 0.980 & 0.856 \\
    NSVF~\cite{liu2020neural} & 0.953 & 0.968 & 0.931 & 0.973 & 0.980 & 0.960 & 0.973 & 0.987 & 0.854 \\
    SNeRG~\cite{hedman2021baking} & 0.950 & 0.975 & 0.929 & 0.967 & 0.971 & 0.973 & 0.938 & 0.982 & 0.865 \\
    PlenOctrees~\cite{yu2021plenoctrees} & 0.958 & 0.981 & 0.933 & 0.970 & \gold{0.982} & 0.971 & \gold{0.955} & 0.987 & 0.884 \\
    Plenoxels~\cite{yu2021plenoxels} & 0.958 & 0.977 & 0.933 & 0.976 & 0.980 & 0.976 & 0.949 & 0.985 & 0.890 \\
    DVGO~\cite{sun2021direct} & 0.957 & 0.977 & 0.930 & 0.978 & 0.980 & 0.976 & 0.951 & 0.983 & 0.879 \\
\hline
    Ours-CP-384  & 0.949 & 0.973 & 0.921 & 0.965 & 0.975 & 0.971 & 0.950 & 0.983 & 0.857 \\
    Ours-VM-192-SH    & 0.955 & 0.979 & 0.928 & 0.976 & 0.977 & 0.978 & 0.941 & 0.983 & 0.875  \\
    Ours-VM-48  & 0.957 & 0.980 & 0.929 & 0.979 & 0.979 & 0.979 & 0.942 & 0.984 & 0.883 \\
    Ours-VM-192-15k & 0.959 & 0.982 & 0.933 & 0.981 & 0.980 & 0.981 & 0.949 & 0.985 & 0.886\\
    Ours-VM-192-30k &\gold{ 0.963} & \gold{0.985} & \gold{0.937} & \gold{0.982} & \gold{0.982} & \gold{0.983} & 0.952 & \gold{0.988} & \gold{0.895} \\
    \hline
    
    \hline
    
    \hline    


    \multicolumn{9}{@{}l}{\rule{0pt}{3ex}\bf LPIPS$_{VGG}\downarrow$ } \\
    \hline
    SRN~\cite{sitzmann2019scene} & 0.170 & 0.106 & 0.267 & 0.149 & 0.100 & 0.200 & 0.174 & 0.063 & 0.299 \\
    NeRF~\cite{mildenhall2020nerf} & 0.081 & 0.046 & 0.091 & 0.044 & 0.121 & 0.050 & 0.063 & 0.028 & 0.206 \\

    PlenOctrees~\cite{yu2021plenoctrees}  & 0.053 & 0.022 & 0.076 & 0.038 & \gold{0.032} & 0.034 & 0.059 & 0.017 & 0.144 \\
    Plenoxels~\cite{yu2021plenoxels}  & 0.049 & 0.031 & \gold{0.067} & 0.026 & 0.037 & 0.028 & \gold{0.057} &\gold{0.015} & \gold{0.134} \\
    DVGO~\cite{sun2021direct} & 0.053 & 0.027 & 0.077 & 0.024 & 0.034 & 0.028 & 0.058 & 0.017 & 0.161 \\
\hline
    Ours-CP-384       & 0.076 & 0.044 & 0.114 & 0.058 & 0.052 & 0.038 & 0.068 & 0.035 & 0.196 \\
    Ours-VM-192-SH    & 0.058 & 0.031 & 0.082 & 0.028 & 0.048 & 0.024 & 0.069 & 0.022 & 0.160 \\
    Ours-VM-48        & 0.057 & 0.030 & 0.087 & 0.028 & 0.039 & 0.024 & 0.072 & 0.021 & 0.155 \\
    Ours-VM-192-15k       & 0.053 & 0.026 & 0.078 & 0.025 & 0.038 & 0.021 & 0.063 & 0.020 & 0.153 \\
    Ours-VM-192-30k       & \gold{0.047} & \gold{0.022} & 0.073 & \gold{0.022} & \gold{0.032} & \gold{0.018} & 0.058 & \gold{0.015} & 0.138 \\

\hline
    \multicolumn{9}{@{}l}{\rule{0pt}{3ex}\bf LPIPS$_{Alex}\downarrow$} \\
    NSVF~\cite{liu2020neural}  & 0.047 & 0.043 & 0.069 & 0.017 & 0.025 & 0.029 & 0.021 & 0.010 & 0.162 \\
    DVGO~\cite{sun2021direct}  & 0.035 & 0.016 & 0.061 & 0.015 & 0.017 & 0.014 & \gold{0.026} & 0.014 & 0.118 \\
    
    
     \hline
    Ours-CP-384        & 0.041 & 0.022 & 0.069 & 0.024 & 0.024 & 0.014 & 0.031 & 0.018 & 0.130 \\
    Ours-VM-192-SH     & 0.058 & 0.031 & 0.082 & 0.028 & 0.048 & 0.024 & 0.069 & 0.022 & 0.160  \\
    Ours-VM-48         & 0.032 & 0.014 & 0.059 & 0.015 & 0.017 & 0.009 & 0.036 & 0.012 & 0.098 \\
    Ours-VM-192-15k        & 0.032 & 0.013 & 0.056 & 0.014 & 0.017 & 0.009 & 0.029 & 0.013 & 0.101 \\
    Ours-VM-192-30k        & \gold{0.027} & \gold{0.010} & \gold{0.051} & \gold{0.012} & \gold{0.013} & \gold{0.007} & \gold{0.026} & \gold{0.009} & \gold{0.085} \\
    \hline

    \end{tabular}
    \caption{Quantitative results on each scene from the {\bf Synthetic-NeRF}~\cite{mildenhall2020nerf} dataset. }
    \label{tab:supp_breakdown_nerf}
    \vspace{2em}
\end{table*}

\newpage
\begin{figure*}[htbp]
    \includegraphics[width=\linewidth]{figs/res_nerf.pdf}
    \caption{Our rendering results on {\bf Synthetic-NeRF} dataset. From top to bottom: Ship, Hotdog, Lego, Mic, Chair, Drums, Materials, Ficus.}
    \label{fig:synthetic_nerf}
\end{figure*}


\begin{table*}[htpb]
    \centering
    \begin{tabular}{l|c|cccccccc}
    \hline
    Methods & Avg. & {\it \tiny{Wineholder}} & {\it \tiny{Steamtrain}} & {\it \tiny{Toad}} & {\it \tiny{Robot}} & {\it \tiny{Bike}} & {\it \tiny{Palace}} & {\it \tiny{Spaceship}} & {\it \tiny{Lifestyle}} \\
    \hline\hline
    \multicolumn{9}{@{}l}{\rule{0pt}{3ex}\bf PSNR$\uparrow$} \\
    \hline
    SRN~\cite{sitzmann2019scene} & 24.33 & 20.74 & 25.49 & 25.36 & 22.27 & 23.76 & 24.45 & 27.99 & 24.58 \\
    NeRF~\cite{mildenhall2020nerf} & 30.81 & 28.23 & 30.84 & 29.42 & 28.69 & 31.77 & 31.76 & 34.66 & 31.08 \\
    NSVF~\cite{liu2020neural} & 35.13 & 32.04 & 35.13 & 33.25 & 35.24 & 37.75 & 34.05 & \gold{39.00} & \gold{34.60} \\
    DVGO~\cite{sun2021direct} &35.08 &30.26 &36.56 &33.10 &36.36 &38.33 &34.49 &37.71& 33.79\\
    \hline
    Ours-CP-384     & 34.48 & 29.92 & 36.07 & 31.37 & 35.92 & 36.74 & 36.26 & 37.01 & 32.54 \\
    Ours-VM-192-SH  & 35.30 & 29.72 & 37.33 & 34.03 & 37.59 & 38.61 & 36.09 & 35.82 & 33.21  \\
    Ours-VM-48      & 35.34 & 30.46 & 37.06 & 33.13 & 36.92 & 37.98 & 36.32 & 37.19 & 33.68 \\
    Ours-VM-192-15k & 35.59 & 30.31 & 37.20 & 33.63 & 37.29 & 38.33 & 36.57 & 37.77 & 33.62\\
    Ours-VM-192-30k & \gold{36.52} & \gold{31.32} & \gold{37.87} & \gold{34.85} & \gold{38.26} & \gold{39.23} & \gold{37.56} & 38.60 & 34.51 \\
    \hline
    
    \hline
    
    \hline    
    
    \multicolumn{9}{@{}l}{\rule{0pt}{3ex}\bf SSIM$\uparrow$} \\
    \hline
    SRN~\cite{sitzmann2019scene} & 0.882 & 0.850 & 0.923 & 0.822 & 0.904 & 0.926 & 0.792 & 0.945 & 0.892 \\
    NeRF~\cite{mildenhall2020nerf} & 0.952 & 0.920 & 0.966 & 0.920 & 0.960 & 0.970 & 0.950 & 0.980 & 0.946 \\
    NSVF~\cite{liu2020neural} & 0.979 & \gold{0.965} & 0.986 & 0.968 & 0.988 & 0.991 & 0.969 & \gold{0.991} & \gold{0.971} \\
    DVGO~\cite{sun2021direct} &0.975& 0.949& 0.989& 0.966 &0.992& 0.991& 0.962& 0.988& 0.965 \\
    \hline
    Ours-CP-384     & 0.971 & 0.947 & 0.986 & 0.950 & 0.990 & 0.987 & 0.971 & 0.984 & 0.951  \\
    Ours-VM-192-SH  & 0.977 & 0.953 & 0.988 & 0.974 & 0.993 & 0.991 & 0.972 & 0.982 & 0.964  \\
    Ours-VM-48      & 0.976 & 0.952 & 0.988 & 0.968 & 0.992 & 0.990 & 0.973 & 0.985 & 0.962 \\
    Ours-VM-192-15k & 0.978 & 0.953 & 0.989 & 0.972 & 0.993 & 0.991 & 0.975 & 0.987 & 0.964 \\
    Ours-VM-192-30k & \gold{0.982} & 0.961 & \gold{0.991} & \gold{0.978} & \gold{0.994} & \gold{0.993} & \gold{0.979} & 0.989 & 0.968 \\
    \hline

\hline
    
    \hline

    \multicolumn{9}{@{}l}{\rule{0pt}{3ex}\bf LPIPS$_{VGG}\downarrow$} \\
    \hline
    DVGO~\cite{sun2021direct} & 0.033 & 0.055 & 0.019 & 0.047 & 0.013 & 0.011 & 0.043 & \gold{0.019} & 0.054\\
    Ours-CP-384             & 0.045 & 0.082 & 0.031 & 0.067 & 0.016 & 0.023 & 0.031 & 0.028 & 0.084 \\
    Ours-VM-192-SH          & 0.031 & 0.057 & 0.024 & 0.035 & 0.011 & 0.013 & 0.030 & 0.026 & 0.051  \\
    Ours-VM-48              & 0.034 & 0.061 & 0.023 & 0.047 & 0.013 & 0.014 & 0.029 & 0.025 & 0.059 \\
    Ours-VM-192-15k         & 0.031 & 0.060 & 0.020 & 0.040 & 0.011 & 0.012 & 0.028 & 0.022 & 0.055\\
    Ours-VM-192-30k         & \gold{0.026} & \gold{0.051} & \gold{0.017} & \gold{0.031} & \gold{0.010} & \gold{0.010} & \gold{0.022} & 0.020 & \gold{0.048} \\
    
    \hline

    \multicolumn{9}{@{}l}{\rule{0pt}{3ex}\bf LPIPS$_{Alex}\downarrow$} \\
    \hline
    SRN~\cite{sitzmann2019scene} & 0.141 & 0.224 & 0.082 & 0.204 & 0.120 & 0.075 & 0.240 & 0.061 & 0.120 \\
    NeRF~\cite{mildenhall2020nerf} & 0.043 & 0.096 & 0.031 & 0.069 & 0.038 & 0.019 & 0.031 & 0.016 & 0.047 \\
    NSVF~\cite{liu2020neural} & 0.015 & \gold{0.020} & 0.010 & 0.032 & 0.007 & 0.004 & 0.018 & \gold{0.006} & \gold{0.020} \\
    DVGO~\cite{sun2021direct} & 0.019 &0.038 &0.010 &0.030& 0.005 &0.004 &0.027 &0.009 &0.027\\
    \hline
    Ours-CP-384  & 0.021 & 0.040 & 0.010 & 0.039 & 0.006 & 0.007 & 0.014 & 0.015 & 0.042  \\
    Ours-VM-192-SH            & 0.015 & 0.030 & 0.008 & 0.021 & \gold{0.003} & \gold{0.003} & 0.016 & 0.016 & 0.025  \\
    Ours-VM-48         & 0.016 & 0.031 & 0.008 & 0.025 & 0.004 & 0.004 & 0.015 & 0.013 & 0.026 \\
    Ours-VM-192-15k        & 0.015 & 0.033 & 0.008 & 0.022 & 0.004 & 0.004 & 0.015 & 0.011 & 0.026 \\
    Ours-VM-192-30k        & \gold{0.012} & 0.024 & \gold{0.006} & 0.016 & \gold{0.003} & \gold{0.003} & \gold{0.011} & 0.009 & 0.021 \\
    

    \hline

    \end{tabular}
    \caption{Quantitative results on each scene from the {\bf Synthetic-NSVF}~\cite{liu2020neural} dataset. }
    \label{tab:supp_breakdown_nsvf}
\end{table*}


\begin{figure}[t]
    \includegraphics[width=\linewidth]{figs/res_nsvf.pdf}
    \caption{Our rendering results on {\bf NSVF} \cite{liu2020neural} dataset. From top to bottom: Spaceship, Robot, Toad, Lifestyle, Palace, Wineholder, Steamtrain.}
    \label{fig:synthetic_nsvf}
\end{figure}


\begin{table*}[t]
    \centering
    \renewcommand\tabcolsep{5.0pt}
    \begin{tabular}{l|c|ccccc}
    \hline
    Methods & Avg. & {\it Ignatius} & {\it Truck} & {\it Barn} & {\it Caterpillar} & {\it Family} \\
    \hline\hline
    \multicolumn{7}{@{}l}{\rule{0pt}{3ex}\bf PSNR$\uparrow$} \\
    \hline
    SRN~\cite{sitzmann2019scene} & 24.10 & 26.70 & 22.62 & 22.44 & 21.14 & 27.57 \\
    NeRF~\cite{mildenhall2020nerf} & 25.78 & 25.43 & 25.36 & 24.05 & 23.75 & 30.29 \\
    NSVF~\cite{liu2020neural} & 28.48 & 27.91 & 26.92 & 27.16 & \gold{26.44} & 33.58 \\
    PlenOctrees~\cite{yu2021plenoctrees} & 27.99 & 28.19 & 26.83 & 26.80 & 25.29 & 32.85 \\
    Plenoxels~\cite{yu2021plenoxels} & 27.43 & 27.51 & 26.59 & 26.07 & 24.64 & 32.33  \\
    DVG ~\cite{sun2021direct} & 28.41 & 28.16 & \gold{27.15} & 27.01 & 26.00 & 33.75 \\
    \hline
    Ours-CP-384 & 27.59 & 27.86 & 26.25 & 26.74 & 24.73 & 32.39  \\
    Ours-VM-192-SH     & 27.81 & 27.78 & 26.73 & 26.03 & 25.37 & 33.12  \\
    Ours-VM-48  & 28.06 & 28.22 & 26.81 & 26.70 & 25.43 & 33.12 \\
    Ours-VM-192-15k & 28.07 & 28.27 & 26.57 & 26.93 & 25.35 & 33.22 \\
    Ours-VM-192-30k & \gold{28.56} & \gold{28.34} & 27.14 & \gold{27.22} & 26.19 & \gold{33.92} \\
    \hline
    
    \hline
    
    \hline    

    \multicolumn{7}{@{}l}{\rule{0pt}{3ex}\bf SSIM$\uparrow$} \\
    \hline
    SRN~\cite{sitzmann2019scene} & 0.847 & 0.920 & 0.832 & 0.741 & 0.834 & 0.908 \\
    NeRF~\cite{mildenhall2020nerf} & 0.864 & 0.920 & 0.860 & 0.750 & 0.860 & 0.932 \\
    NSVF~\cite{liu2020neural} & 0.901 & 0.930 & 0.895 & 0.823 & 0.900 & 0.954 \\
    PlenOctrees~\cite{yu2021plenoctrees} & 0.917 & \gold{0.948} & \gold{0.914} & 0.856 & 0.907 & 0.962 \\
    Plenoxels~\cite{yu2021plenoxels} & 0.906 & 0.943 & 0.901 & 0.829 & 0.902 & 0.956 \\
    DVGO~\cite{sun2021direct} &0.911 & 0.944 & 0.906 & 0.838 & 0.906 & 0.962\\
    \hline
    Ours-CP-384 & 0.897 & 0.934 & 0.885 & 0.839 & 0.879 & 0.948  \\
    Ours-VM-192-SH     & 0.907 & 0.942 & 0.900 & 0.834 & 0.897 & 0.960  \\
    Ours-VM-48  & 0.909 & 0.943 & 0.902 & 0.845 & 0.899 & 0.957 \\
    Ours-VM-192-15k & 0.913 & 0.944 & 0.905 & 0.855 & 0.902 & 0.960 \\
    Ours-VM-192-30k & \gold{0.920} & \gold{0.948} & \gold{0.914} & \gold{0.864} & \gold{0.912} & \gold{0.965} \\
    \hline
    
    \hline
    
    \hline    

    \multicolumn{7}{@{}l}{\rule{0pt}{3ex}\bf LPIP$_{VGG} \downarrow$} \\
    \hline
    PlenOctrees~\cite{yu2021plenoctrees} & 0.131 & 0.080 & 0.130 & 0.226 & \gold{0.148} & 0.069 \\
    Plenoxels~\cite{yu2021plenoxels} & 0.162 & 0.102 & 0.163 & 0.303 & 0.166 & 0.078 \\
    DVGO~\cite{sun2021direct} & 0.155 & 0.083 & 0.160 & 0.294 & 0.167 & 0.070 \\
    \hline
    Ours-CP-384     & 0.181 & 0.106 & 0.202 & 0.283 & 0.227 & 0.088 \\
    Ours-VM-192-SH  & 0.156 & 0.089 & 0.161 & 0.286 & 0.175 & 0.069  \\
    Ours-VM-48      & 0.155 & 0.085 & 0.161 & 0.278 & 0.177 & 0.074 \\
    Ours-VM-192-15k & 0.152 & 0.084 & 0.162 & 0.269 & 0.173 & 0.071 \\
    Ours-VM-192-30k & \gold{0.140} & \gold{0.078} & \gold{0.145} &\gold{0.252} & 0.159 & \gold{0.064} \\
    \hline

    \multicolumn{7}{@{}l}{\rule{0pt}{3ex}\bf LPIPS$_{Alex}\downarrow$} \\
    \hline
    SRN~\cite{sitzmann2019scene} & 0.251 & 0.128 & 0.266 & 0.448 & 0.278 & 0.134 \\ 
    NeRF~\cite{mildenhall2020nerf} & 0.198 & 0.111 & 0.192 & 0.395 & 0.196 & 0.098 \\
    NSVF~\cite{liu2020neural} & 0.155 & 0.106 & 0.148 & 0.307 & 0.141 & 0.063 \\
    DVGO~\cite{sun2021direct} & 0.148 & 0.090 & 0.145 & 0.290 & 0.152 & 0.064 \\
    \hline
    Ours-CP-384     & 0.144 & 0.089 & 0.154 & 0.237 & 0.176 & 0.063  \\
    Ours-VM-192-SH  & 0.164 & 0.098 & 0.168 & 0.309 & 0.175 & 0.072  \\
    Ours-VM-48      & 0.145 & 0.089 & 0.145 & 0.266 & 0.161 & 0.066 \\
    Ours-VM-192-15k & 0.140 & 0.087 & 0.150 & 0.240 & 0.157 & 0.066 \\
    Ours-VM-192-30k & \gold{0.125} & \gold{0.081} & \gold{0.129} & \gold{0.217} & \gold{0.139} & \gold{0.057} \\
    \hline

    \end{tabular}
    \caption{Quantitative results on each scene from the {\bf Tanks\&Temples} \cite{Knapitsch2017} dataset.}
    \label{tab:supp_breakdown_tanksandtemples}
\end{table*}

\begin{figure}[t]
    \includegraphics[width=\linewidth]{figs/res_tt.pdf}
    \caption{Our rendering results on {\bf Tanks\&Temples}\cite{Knapitsch2017} dataset. From top to bottom: Family, Ignatius, Truck, Caterpillar, Barn.}
    \label{fig:res_tt}
\end{figure}


\begin{table*}[htpb]
    \centering
    \renewcommand\tabcolsep{2.0pt}
    \begin{tabular}{l|c|cccccccc}
    \hline
    Methods & Avg. & {\it Room} & {\it Fern} & {\it Leaves} & {\it Fortress} & {\it Orchids} & {\it Flower} & {\it T-Rex} & {\it Horns} \\
    \hline\hline
    \multicolumn{9}{@{}l}{\rule{0pt}{3ex}\bf PSNR$\uparrow$} \\
    \hline
NeRF~\cite{mildenhall2020nerf}   & 26.50 & \gold{32.70} &25.17 &20.92 &31.16 &\gold{20.36} & 27.40 &26.80 &27.45 \\
    Plenoxels~\cite{yu2021plenoxels} & 26.29 & 30.22 & 25.46 & \gold{21.41} & 31.09 & 20.24 & 27.83 & 26.48 & 27.58 \\
\hline
Ours-VM-48 & 26.51 & 31.80 & 25.31 & 21.34 & 31.14 & 20.02 & 28.22 & 26.61 & 27.64\\
    Our-VM-96 & \gold{26.73} & 32.35 & \gold{25.27} & 21.30 & \gold{31.36} & 19.87 & \gold{28.60} & \gold{26.97} & \gold{28.14} \\
    \hline
    
    \hline
    
    \hline    
    
    \multicolumn{9}{@{}l}{\rule{0pt}{3ex}\bf SSIM$\uparrow$} \\
    \hline
NeRF~\cite{mildenhall2020nerf}   & 0.811 & 0.948 & 0.792 & 0.690 & 0.881 & 0.641 & 0.827 & 0.880 & 0.828\\
    Plenoxels~\cite{yu2021plenoxels} & \gold{0.839} & 0.937 & \gold{0.832} & \gold{0.760} & 0.885 & \gold{0.687} & 0.862 & 0.890 & 0.857\\

    
    \hline
Ours-VM-48 & 0.832 & 0.946 & 0.816 & 0.746 & 0.889 & 0.655 & 0.859 & 0.890 & 0.859 \\
    Ours-VM-96 & \gold{0.839} & \gold{0.952} & 0.814 & 0.752 & \gold{0.897} & 0.649 & \gold{0.871} & \gold{0.900} & \gold{0.877} \\
    \hline

\hline


    \hline

    \multicolumn{9}{@{}l}{\rule{0pt}{3ex}\bf LPIPS$_{VGG}\downarrow$ } \\
    \hline
NeRF~\cite{mildenhall2020nerf}   & 0.250 & 0.178 &0.280 &0.316 &0.171   &0.321 & 0.219 & 0.249 & 0.268 \\
    Plenoxels~\cite{yu2021plenoxels} & 0.210 & 0.192 & \gold{0.224} & \gold{0.198} & 0.180 & \gold{0.242} & 0.179 & 0.238 & 0.231 \\

    \hline
Ours-VM-48 & 0.217 & 0.181 & 0.237 & 0.230 & 0.159 & 0.283 & 0.187 & 0.236 & 0.221 \\
    Ours-VM-96 & \gold{0.204} & \gold{0.167} & 0.237 & 0.217 & \gold{0.148} & 0.278 & \gold{0.169} & \gold{0.221} & \gold{0.196} \\
    \hline
  
    
    
    \multicolumn{9}{@{}l}{\rule{0pt}{3ex}\bf LPIPS$_{Alex} \downarrow$ } \\
    \hline
Ours-VM-48 & 0.135 & 0.093 & 0.161 & 0.167 & 0.084 & 0.204 & 0.121 & 0.108 & 0.146 \\
    Ours-VM-96 & 0.124 & 0.082 & 0.155 & 0.153 & 0.075 & 0.201 & 0.106 & 0.099 & 0.123 \\
    \hline
    
    \end{tabular}
    \caption{Quantitative results on each scene from the {\bf forward-facing}~\cite{liu2020neural} dataset. }
    \label{tab:supp_breakdown_llff}
\end{table*}


\begin{figure}[t]
    \includegraphics[width=\linewidth]{figs/res_llff.pdf}
    \caption{Our rendering results on {\bf forward-facing}~\cite{liu2020neural} dataset. From top to bottom: Flower, Fern, Fortress, Horn, Leaves, Orchids, T-Rex, Room. }
    \label{fig:res_llff}
\end{figure}\clearpage{}
\end{document}
