\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{thm-restate}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{enumitem}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage[table]{xcolor}
\usepackage{tabularx}
\usepackage{multirow}



\usepackage{url}

\usepackage[pdfencoding=unicode,pdfusetitle,pdflang={en-US}]{hyperref}
\usepackage{pdfcomment}
\usepackage[capitalize,noabbrev]{cleveref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2020} 




\newtheorem{theorem}{Theorem} \crefname{theorem}{Theorem}{Theorems}
\newtheorem{lemma}[theorem]{Lemma} \crefname{lemma}{Lemma}{Lemmas}
\newtheorem{corollary}[theorem]{Corollary} \crefname{corollary}{Corollary}{Corollaries}
\newtheorem{definition}[theorem]{Definition} \crefname{definition}{Definition}{Definitions}
\newtheorem{prop}[theorem]{Proposition}  \crefname{prop}{Proposition}{Propositions}
\newtheorem{remark}[theorem]{Remark}  \crefname{remark}{Remark}{Remarks}

\newlist{assumplist}{enumerate}{1}
\setlist[assumplist]{label=(\textbf{\Alph*})}
\Crefname{assumplisti}{Assumption}{Assumptions}

\newlist{netassumplist}{enumerate}{1}
\setlist[netassumplist]{label=(\textbf{\Roman*})}
\Crefname{netassumplisti}{Assumption}{Assumptions}

\newlist{compactitem}{itemize}{3}
\setlist[compactitem]{topsep=0pt,partopsep=0pt,itemsep=4pt,parsep=0pt}
\setlist[compactitem,1]{label=\textbullet}
\setlist[compactitem,2]{label=---}
\setlist[compactitem,3]{label=*}

\newlist{compactdesc}{description}{3}
\setlist[compactdesc]{topsep=0pt,partopsep=0pt,itemsep=4pt,parsep=0pt}

\newcommand{\R}{\mathbb{R}} \renewcommand{\H}{\mathcal{H}} \newcommand{\N}{\mathcal{N}} \newcommand{\X}{\mathcal{X}} \renewcommand{\P}{\mathbb{P}} \newcommand{\Q}{\mathbb{Q}} \DeclareMathOperator{\E}{\mathbb{E}} \DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\bigO}{\mathcal{O}}
\DeclareMathOperator{\bigTheta}{\Theta}
\newcommand{\one}{\mathbf{1}}
\newcommand{\tp}{^\mathsf{T}}

\newcommand{\nullhyp}{\mathfrak{H}_0}
\newcommand{\althyp}{\mathfrak{H}_1}

\DeclareMathOperator{\MMD}{MMD}
\DeclareMathOperator{\acc}{acc}
\DeclareMathOperator{\mnlogit}{mn}

\newcommand{\mnstd}[2]{#1{\scriptsize#2}}

\newcommand{\wk}[1]{\textcolor{green!10!orange!90!}{(WX: #1)}}

\makeatletter
\DeclareRobustCommand{\abs}{\@ifstar\@abs\@@abs}
\newcommand{\@abs}[1]{\lvert #1 \rvert}
\newcommand{\@@abs}[1]{\lvert #1 \rvert}

\DeclareRobustCommand{\norm}{\@ifstar\@norm\@@norm}
\newcommand{\@norm}[1]{\lVert #1 \rVert}
\newcommand{\@@norm}[1]{\lVert #1 \rVert}
\makeatother

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand*\tagthis{\refstepcounter{equation}\tag{\theequation}}

\newcommand{\httpsurl}[1]{\href{https://#1}{\nolinkurl{#1}}}


\begin{document}

\twocolumn[
\icmltitle{Learning Deep Kernels for Non-Parametric Two-Sample Tests}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Feng Liu}{equal,aaii,gatsby}
\icmlauthor{Wenkai Xu}{equal,gatsby}
\icmlauthor{Jie Lu}{aaii}
\icmlauthor{Guangquan Zhang}{aaii}
\icmlauthor{Arthur Gretton}{gatsby}
\icmlauthor{Danica J.\ Sutherland}{ttic}
\end{icmlauthorlist}

\icmlaffiliation{gatsby}{Gatsby Computational Neuroscience Unit, University College London, London, UK}
\icmlaffiliation{aaii}{Australian Artificial Intelligence Institute, University of Technology Sydney, Sydney, NSW, Australia}
\icmlaffiliation{ttic}{Toyota Technological Institute at Chicago, Chicago, IL, USA}

\icmlcorrespondingauthor{Feng Liu}{Feng.Liu@uts.edu.au}
\icmlcorrespondingauthor{Wenkai Xu}{wenkaix@gatsby.ucl.ac.uk}
\icmlcorrespondingauthor{Danica J.\ Sutherland}{djs@djsutherland.ml}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]




\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
We propose a class of kernel-based two-sample tests, which aim to determine whether two sets of samples are drawn from the same distribution. Our tests are constructed from kernels parameterized by deep neural nets, trained to maximize test power. These tests adapt to variations in distribution smoothness and shape over space, and are especially suited to high dimensions and complex data. By contrast, the simpler kernels used in prior kernel testing work are spatially homogeneous, and adaptive only in lengthscale. We explain how this scheme includes popular classifier-based two-sample tests as a special case, but improves on them in general. We provide the first proof of consistency for the proposed adaptation method, which applies both to kernels on deep features and to simpler radial basis kernels or multiple kernel learning. In experiments, we establish the superior performance of our deep kernels in hypothesis testing on benchmark and real-world data. The code of our deep-kernel-based two sample tests is available at \httpsurl{github.com/fengliu90/DK-for-TST}.
\end{abstract}

\section{Introduction}

Two sample tests are hypothesis tests aiming to determine whether two sets of samples are drawn from the same distribution.
Traditional methods such as -tests and Kolmogorov-Smirnov tests are mainstays of statistical applications,
but require strong parametric assumptions about the distributions being studied
and/or are only effective on data in extremely low-dimensional spaces.
A broad set of
recent work in statistics and machine learning
has focused on relaxing these assumptions,
with methods either generally applicable
or specific to various more complex domains
\citep{Gretton2012,Szekely2013,Heller2016,Jitkrittum2016,RamdasGarciaCuturi,Lopez:C2ST,Chen2017,Gao18neurips,Ghoshdastidar2017,Graph_two_sample,LiW18TIT,Matthias:deep-test}.
These tests have also allowed application in various machine learning problems such as domain adaptation, generative modeling, and causal discovery \citep{MMD_GAN,Gong2016,DA_app_Stojanov,Lopez:C2ST}.

A popular class of
non-parametric two-sample tests is based on kernel methods \citep{smola1998learning}: such tests construct a
\emph{kernel mean embedding} \citep{BerTho04,Muandet2017} for each distribution, and measure the difference in these embeddings.
For any \emph{characteristic} kernel, two distributions are the same if and only if their mean embeddings are the same;
the distance between mean embeddings is the \emph{maximum mean discrepancy} (MMD) \citep{Gretton2012}.
There are also several closely related methods,
including tests based on checking for differences in mean embeddings evaluated at specific locations \citep{Chwialkowski2015,Jitkrittum2016}
and kernel Fisher discriminant analysis \citep{Harchaoui2007}.
These tests all work well for samples from simple distributions when using appropriate kernels.

Problems that we care about, however,
often involve distributions with complex structure,
where simple kernels will often map distinct distributions to nearby (and hence hard to distinguish) mean embeddings.
\Cref{fig:moti}a shows an example of a multimodal dataset,
where the overall modes align but the sub-mode structure varies differently at each mode.
A translation-invariant Gaussian kernel only ``looks at'' the data uniformly within each mode (see \cref{fig:moti}b),
requiring many samples to correctly distinguish the two distributions.
The distributions can be distinguished more effectively if we understand the structure of each mode,
as with the more complex kernel illustrated in \cref{fig:moti}c.

\begin{figure*}[tp]
    \begin{center}
        \subfigure[Samples drawn from  (left) and  (right).]
        {\includegraphics[width=0.48\textwidth]{fig/Motivition_TwoSamples.pdf}}
        \vspace{-0.3cm}
        \subfigure[Contour of Gaussian]
        {\includegraphics[width=0.24\textwidth]{fig/Motivition_MMD.pdf}}
        \subfigure[Contour of deep kernel]
        {\includegraphics[width=0.24\textwidth]{fig/Motivition_DK2ST.pdf}}
        \caption{In the Blob dataset,
         and  are each equal mixtures of nine Gaussians with the same modes (a),
        but each component of  is an isotropic Gaussian
        whereas the covariance of  differs in each component.
        Panels (b) and (c) show the contours of a kernel,
         for each of the nine modes ;
        contour values are ,  and .
A Gaussian kernel (b) treats points isotropically throughout the space, based only on .
        A deep kernel (c) learned by our methods behaves differently in different parts of the space, adapting to the local structure of the data distributions and hence allowing better identification of differences between  and .}
        \label{fig:moti}
    \end{center}
    \vspace{-1em}
\end{figure*}

To model these complex functions,
we adopt a \emph{deep kernel} approach \citep{wilson:deep-kernel-learning,sutherland:mmd-opt,Li2017,Jean2018,Kevin_ICML2019},
building a kernel with a deep network.
In this paper, we use

where the deep neural network  extracts features of samples,
and  is a simple kernel (e.g., a Gaussian) on those features,
while  is a simple characteristic kernel (e.g. Gaussian) on the input space.
With an appropriate choice of ,
this allows for extremely flexible kernels
which can learn complex behavior very different in different parts of space.
This choice is discussed further in \cref{sec:DKforTST}.

These complex kernels, though, cannot feasibly be specified by hand or simple heuristics, as is typical practice in kernel methods.
We select the parameters 
by maximizing the ratio of the MMD to its variance,
which maximizes test power at large sample sizes.
This procedure was proposed by \citet{sutherland:mmd-opt},
but we establish for the first time that it gives consistent selection of the best kernel in the class,
whether optimizing our deep kernels with hundreds of thousands of parameters
or simply choosing lengthscales of a Gaussian as did \citeauthor{sutherland:mmd-opt}
Previously, there were no guarantees this procedure would yield a kernel
which generalized at all from the training set to a test set.

Another way to compare distributions is to train a classifier between them, and evaluate its accuracy \citep{Lopez:C2ST}.
We show, perhaps surprisingly, that our framework encompasses this approach,
but deep kernels allow for more general model classes which can use the data more efficiently.
We also train representations
directly to maximize test power,
rather than a cross-entropy surrogate.

We test our method on several simulated and real-world datasets,
including complex synthetic distributions,
high-energy physics data,
and challenging image problems.
We find convincingly that learned deep kernels outperform simple shallow methods,
and learning by maximizing test power
outperforms learning through a cross-entropy surrogate loss.







\section{MMD Two-Sample Tests}\label{sec:background}
\paragraph{Two-sample testing.}
Let  be a separable metric space
-- in this paper, typically a subset of  --
and ,  be Borel probability measures on .
We observe independent identically distributed (\emph{i.i.d.}) samples  and .
We wish to know whether  and  come from the same distribution:
does ?

We use the null hypothesis testing framework,
where  the null hypothesis  is tested against the alternative hypothesis .
We perform a two-sample test in four steps:
select a significance level ;
compute a test statistic ;
compute the -value , the probability of the two-sample test returning a statistic as large as  when  is true;
finally, reject  if .

\paragraph{Maximum mean discrepancy (MMD).}
We will base our two-sample test statistic
on an estimate of a distance between distributions.
Our metric, the MMD,
is defined in terms of a kernel 
giving point-level ``similarities'' on .


\begin{definition}[\citealp{Gretton2012}]
Let 
be the kernel
of a reproducing kernel Hilbert space ,
with feature maps .
Let  and ,
and define the \emph{kernel mean embeddings}
 and .
Under mild integrability conditions,

For \emph{characteristic} kernels,
 implies ,
hence  if and only if .
\end{definition}

The first form shows that the MMD is an integral probability metric \citep{muller1997integral},
along with such popular distances as the Wasserstein and total variation.

There are several natural estimators of the MMD from samples.
We will assume 
and use the -statistic estimator,
which is unbiased for  and has nearly minimal variance among unbiased estimators \citep{Gretton2012}:

The similar  is
the squared MMD between the empirical distributions of  and .\footnote{Including  terms in  gives the minimal variance unbiased estimator, and allows . The -statistic is more convenient for analysis and for efficient permutations; in our settings it behaves similarly to the MVUE and .}{}

\paragraph{Testing with the MMD.}
It can be shown that under ,
 converges to a distribution depending on  and ;
we thus use this as our test statistic.

\begin{prop}[Asymptotics of ] \label{prop:asymptotics}
Under the null hypothesis, ,
we have
if ,

here  are
the eigenvalues of the -covariance operator of the centered kernel
\citep[Theorem 12]{Gretton2012},
and  denotes convergence in distribution.


Under the alternative, ,
a standard central limit theorem holds
\citep[Section 5.5.1]{serfling}:

where ,  refer to  above.
\end{prop}

Although it is possible to construct a test based on directly estimating this null distribution \citep{eig-mmd-null},
it is both simpler and, if implemented carefully, faster \citep{sutherland:mmd-opt} to instead use a permutation test.
This general method \citep{dwass1957,AlbaFernandez2008} observes that under ,
the samples from  and  are interchangeable;
we can therefore estimate the null distribution of our test statistic
by repeatedly re-computing it with the samples randomly re-assigned to  or .

\paragraph{Test power.}
The main measure of efficacy of a null hypothesis test is its \emph{power}:
the probability that, for a particular  and , we correctly reject .
\Cref{prop:asymptotics} implies,
where  is the standard normal CDF,
that

we can find the approximate test power by using the rejection threshold, found via (e.g.) permutation testing, as .
We also know via \cref{prop:asymptotics} that this  will converge to a constant,
and ,  are also constants.
For reasonably large ,
the power is dominated by the first term,
and the kernel yielding the most powerful test will approximately maximize \citep{sutherland:mmd-opt}


\paragraph{Selecting a kernel.}
The criterion  depends on the particular  and  at hand,
and thus we typically will neither be able to choose a kernel \emph{a priori},
nor exactly evaluate  given samples.
We can, however, estimate it with

where  is a regularized estimator of 
given by\footnote{This estimator, as a -statistic, is biased even when 
(although this bias is only ; see \cref{thm:var-est-bias}).
Although \citet{sutherland:mmd-opt,unbiased-var-ests} give a quadratic-time estimator unbiased for ,
it is much more complicated to implement and analyze,
likely has higher variance,
and (being unbiased) can be negative,
especially e.g.\ when the kernel is poor.}


Given  and ,
we could construct a test by choosing  to maximize ,
then using a test statistic based on .
This sample re-use, however,
violates the conditions of \cref{prop:asymptotics},
and permutation testing would require repeatedly re-training  with permuted labels.

Thus we split the data,
get ,
then compute the test statistic and permutation threshold on ,  using .
This procedure was proposed for  by \citet{sutherland:mmd-opt},
but the same technique works for a variety of tests
\citep{Gretton2012NeurIPS,Jitkrittum2016,Jitkrittum2017,Lopez:C2ST}.
Our paper adopts this framework (\cref{sec:DKforTST})
and studies it further.

\paragraph{Relationship to other approaches.}
One common scheme is to pick a kernel  based on some proxy task,
such as a related classification problem
(e.g.\ \citealt{Matthias:deep-test} or the KID score of \citealt{MMD_GAN}).
Although this approach can work quite well,
it depends entirely on features from the proxy task applying well to the differences between  and ,
which can be hard to know in general.

An alternative is to maximize simply 
(\citealt{sriperumbudur2009choice}; proposed but not evaluated by \citeauthor{Matthias:deep-test}).
Ignoring  means that,
for instance, this approach would choose to simply scale ,
even though this does not change the test at all.
Even when this is not possible,
\citet{sutherland:mmd-opt} found this approach notably worse than maximizing \eqref{eq:tpp-hat};
we confirm this in our experiments.


MMD-GANs \citep{Li2017,MMD_GAN}
also simply maximize 
to identify the differences between their model  and target .
If  is quite far from , however,
an MMD-GAN requires a ``weak'' kernel to identify a path for improving  \citep{MMD_GAN2},
while our ideal kernel is one which perfectly distinguishes  and  and would likely give no signal for improvement.
Our algorithm, theoretical guarantees, and empirical evaluations thus all differ significantly from those for MMD-GANs.


\section{Limits of Simple Kernels}
\label{sec:MMD_limited}

We can use the criterion  of \eqref{eq:tpp-hat}
even to select parameters among a simple family,
such as the lengthscale of a Gaussian kernel.
Doing so on the \emph{Blob} problem of \cref{fig:moti}
illustrates the limitations of using MMD with these kernels.

In \cref{fig:Blob_RES}c,
we show how the maximal value of 
changes as we see more samples from  and ,
for both a family of Gaussian kernels (green dashed line)
and a family \eqref{eq:deepkernel_simpleForm} of deep kernels (red line).
The optimal  is always higher for the deep kernels;
as expected, the empirical test power (\cref{fig:Blob_RES}a) is also higher for deep kernels.

Most simple kernels used for MMD tests,
whether the Gaussian we use here or Laplace, inverse multiquadric,
even automatic relevance determination kernels,
are all translation invariant:
 for any .
(All kernels used by \citet{sutherland:mmd-opt}, for instance, were of this type.)
Hence the kernel behaves the same way across space,
as in \cref{fig:moti}b.
This means that for distributions whose behavior varies through space,
whether because principal directions change (as in \cref{fig:moti}) so the shape should be different,
or because some regions are much denser than others and so need a smaller lengthscale \citep[e.g.][Figures 1 and 2]{Kevin_ICML2019},
any single global choice is suboptimal.

Kernels which are not translation invariant,
such as the deep kernels \eqref{eq:deepkernel_simpleForm}
shown in \cref{fig:moti}c,
can adapt to the different shapes necessary in different areas.

\begin{figure*}[!t]
    \begin{center}
    \small
        \subfigure
        {\pdftooltip{\includegraphics[width=0.77\textwidth]{fig/legend_crop.pdf}}{These are baselines considered in this paper. See more details in Section~\ref{sec:exp}.}}
        \subfigure[Average test power]
        {\includegraphics[width=0.25\textwidth]{fig/Test_power_blob.pdf}}
        \vspace{-0.3cm}
        \subfigure[STD of test power]
        {\includegraphics[width=0.25\textwidth]{fig/Test_power_std_blob.pdf}}
        \subfigure[ value \eqref{eq:tpp-hat}]
        {\includegraphics[width=0.24\textwidth]{fig/Jvsn.pdf}}
        \subfigure[Type-I error]
        {\includegraphics[width=0.24\textwidth]{fig/Type_I_blob.pdf}}
        \caption{Results on \emph{Blob-S} and \emph{Blob-D} given ; see \cref{sec:exp} for details.  is the number of samples at each mode, so  means drawing  samples from each of  and . We report, when increasing , (a) average test power, (b) standard deviation of test power, (c) the value of , and (d) average type-I error. (a), (b) and (c) are on \emph{Blob-D}, and (d) is on \emph{Blob-S}. Shaded regions show standard errors for the mean, and the black line shows .}
        \label{fig:Blob_RES}
    \end{center}
    \vspace{-1em}
\end{figure*}

\section{Relationship to Classifier-Based Tests} \label{sec:c2st-relation}
Another popular method for conducting two-sample tests is to train a classifier between  and ,
then assess its performance on , .
If , the classification problem is impossible and performance will be at chance.

The most common performance metric is the accuracy \citep{Lopez:C2ST};
this scheme is fairly common among practitioners, and
\citet{Ramdas:clf} showed it to be optimal in rate, but suboptimal in constant, in one limited setting
(linear discriminant analysis between high-dimensional elliptical distributions, e.g.\ Gaussians, with identical covariances).
We will call this approach a Classifier Two-Sample Test based on Sign, C2ST-S.
Letting  output classification scores,
the C2ST-S statistic
is 
given by

Let ;
 is unbiased for  and has a simple asymptotically normal null distribution.

Although it is perhaps not immediately obvious this is the case,
C2ST-S is almost a special case of the MMD.
Let
    
A C2ST-S with  is equivalent to an MMD test with :
\begin{prop} \label{thm:c2st-equiv}
    It holds that
    
\end{prop}
\begin{proof}
    The mean embedding  under 
    is simply ,
    so the MMD is
    
    Moreover,  is  on empirical distributions.
\end{proof}
The C2ST-S, however, selects  to maximize cross-entropy (approximately maximizing ),
while we maximize  \eqref{eq:tpp-hat}.
Although  is not differentiable,
maximizing \eqref{eq:tpp} would exactly maximize 
and hence maximize test power \citep[Theorem 1]{Lopez:C2ST}.



Accessing  only through its sign allows for a simple null distribution,
but it ignores 's measure of confidence:
a highly confident output extremely far from the decision boundary
is treated the same as a very uncertain one lying in an area of high overlap between  and ,
dramatically increasing the variance of the statistic.
A scheme we call C2ST-L instead
tests difference in means of  on  and  \citep{cheng:net-logits}.
Let
    
A C2ST-L is equivalent to an MMD test with :
\begin{prop} \label{thm:c2st-l-equiv}
    It holds that
    
\end{prop}
\begin{proof}
    This kernel's feature map is .
\end{proof}
Now maximizing accuracy (or a cross-entropy proxy) no longer directly maximizes power.
This kernel is differentiable,
so we can directly compare the merits of maximizing \eqref{eq:tpp-hat} to maximizing cross-entropy;
we will see in \cref{sec:tpp-vs-ce} that our more direct approach
is empirically superior.

Compared to using , however,
\cref{sec:tpp-vs-ce} shows that
learned MMD tests also obtain better performance using kernels like \eqref{eq:deepkernel_simpleForm}.
This is analogous to a similar phenomenon observed in other problems by \citet{MMD_GAN} and \citet{Kevin_ICML2019}:
C2STs learn a full discriminator function on the training set,
and then apply only that function to the test set.
Learning a deep kernel like \eqref{eq:deepkernel_simpleForm} corresponds to learning only a powerful \emph{representation} on the training set,
and then \emph{still learning}  itself from the test set
-- in a closed form that makes permutation testing simple.





\section{Learning Deep Kernels}\label{sec:DKforTST}


\paragraph{Choice of kernel architecture.} \label{sec:kernel-arch}
Most previous work on deep kernels has used a kernel  directly on the output of a featurization network ,
.
This is certainly also an option for us.
Any such , however, is characteristic if and only if  is injective.
If we select our kernel well, this is not really a concern.\footnote{A characteristic kernel on top of even

with a \emph{random} 
will be almost surely consistent \citep{Heller2016},
and in general the existence of even one good  for a particular ,  pair is enough that a perfect optimizer would be able to distinguish the distributions
\citep[Proposition 1]{MMD_GAN2}.}{}
Even so, it would be reassuring to know that,
even if the optimization goes awry,
the resulting test will still be at least consistent.
More importantly,
it can be helpful in optimization to add a ``safeguard'' preventing the learned kernel from considering extremely far-away inputs as too similar.
We can achieve these goals with the form \eqref{eq:deepkernel_simpleForm},
repeated here:

Here  is a deep network (with parameters ) that extracts features,
and  is a kernel on those features;
we use a Gaussian with lengthscale ,
.
We choose  and
 a Gaussian with lengthscale .
\begin{prop} Let  be of the form \eqref{eq:deepkernel_simpleForm}
with  and  characteristic.
Then  is characteristic.
\end{prop}









\paragraph{Learning the deep kernel.}
The kernel optimization and testing procedure is summarized in \cref{alg:learn_deep_kernel}.
For larger datasets, or when ,
we use minibatches in the training procedure;
for smaller datasets, we use full batches.
We use the Adam optimizer \citep{Adam:optimizer}.
Note that the parameters , , and 
are included in ,
all parameterized in log-space
(i.e. we optimize  where ).






\begin{algorithm}[tb]
\footnotesize
\caption{Testing with a learned deep kernel}
\label{alg:learn_deep_kernel}
\begin{algorithmic}
\STATE \textbf{Input:} , , various hyperparameters used below;

\vspace{1mm}
\STATE ; ; 

\STATE Split the data as  and ;

\vspace{1mm}
\STATE \textit{\# Phase 1: train the kernel parameters  on  and \hfill}

\FOR{}

\STATE  minibatch from ;  minibatch from ;

\STATE  kernel function with parameters ;
       \hfill \textit{\# as in \eqref{eq:deepkernel_simpleForm}}

\STATE ;
       \hfill\textit{\# using \eqref{eq:MMD_U_compute}}

\STATE ;
       \hfill\textit{\# using \eqref{eq:estimate_sigma_H1}}

\STATE ;
       \hfill\textit{\# as in \eqref{eq:tpp-hat}}

\STATE ;
       \hfill \textit{\# maximize }

\ENDFOR

\vspace{1mm}
\STATE \textit{\# Phase 2: permutation test with  on  and }
\STATE 

\FOR{}
\STATE Shuffle  into  and 
\STATE 
\ENDFOR

\STATE \textbf{Output:} , , -value 
\end{algorithmic}
\end{algorithm}









\paragraph{Time complexity.}
Let  denote the cost of computing an embedding ,
and  the cost of computing \eqref{eq:deepkernel_simpleForm} given , .
Then each iteration of training in \cref{alg:learn_deep_kernel}
costs , where  is the minibatch size;
for the moderate  that fit in a GPU-sized minibatch anyway,
the  term typically dominates,
matching the complexity of a C2ST.
Testing takes time ,
compared to  for permutation-based C2STs.
In either case, the quadratic factors could if necessary be reduced with the block estimator approach of \citet{Blaschko2013}, at the cost of some test power.
In our experiments in \cref{sec:exp}, the overall runtime of our methods was scarcely different from the overall runtime of C2STs.


\section{Theoretical Analysis}
We now show that optimizing the regularized test power criterion based on a finite number of samples works:
as  increases, our estimates converge uniformly over a ball in parameter space,
and therefore if there is a unique best kernel,
we converge to it.
\citet{sutherland:mmd-opt} gave no such guarantees;
this result allows us to trust that,
at least for reasonably large  and if our optimization process succeeds,
we will find a kernel that generalizes nearly optimally
rather than just overfitting to .

We first state a generic result,
then show some choices of kernels, particularly deep kernels \eqref{eq:deepkernel_simpleForm}, satisfy the conditions.
\begin{theorem} \label{thm:test-power-conv}
   Let  parameterize uniformly bounded kernel functions  in a Banach space of dimension ,
   with
   .
   Let  be a set of  for which
   
   and .
   Take .
   Then,
   with probability at least ,

    If there is a unique best kernel ,
    the maximizer of  converges in probability to  as .
\end{theorem}
A version with explicit constants and more details is given in \cref{Asec:proof} (as \cref{thm:ratio-conv,thm:param-conv});
the proof is based on
uniform convergence of the MMD and variance estimators
using an -net argument.

The following results are shown in \cref{sec:proof:kernel-props}.
We first show a result on simple Gaussian bandwidth selection.

\begin{prop} \label{thm:main-rbf-lip}
    Suppose each  has ,
    and we choose the bandwidth of a Gaussian kernel
    among a set whose minimum is at least .
    Then the conditions of \cref{thm:test-power-conv}
    are met with  and
    .
\end{prop}

Our results also apply to multiple kernel learning,
where in fact the exact maximizer of  is efficiently available (\cref{prop:mkl-soln}).
\begin{prop} \label{thm:main-mkl-lip}
    Let  be a fixed set of kernels,
    with  for all .
    Then
    picking 
    among some set of  with 
    satisfies the conditions of \cref{thm:test-power-conv}
    with .
\end{prop}

We finally establish our results for fully-connected deep kernels;
it also applies to convolutional networks with a slightly different  (\cref{remark:convnets}).
The constants in  are given in \cref{thm:kern-lip}.
\begin{prop} \label{thm:main-kern-lip}
    Take  as in \cref{sec:DKforTST},
    with  a fully-connected network with depth  and  total parameters,
    whose activations are 1-Lipschitz with  (e.g.\ ReLU).
    Suppose the operator norm of each weight matrix and  norm of each bias vector are is at most ,
    and each  has .
    Then  meets the conditions of \cref{thm:test-power-conv}
    with dimension  and 
    .
\end{prop}








The dependence on  in \cref{thm:test-power-conv} is somewhat unfortunate,
but the ratio structure of  means that otherwise,
errors in very small variances can hurt us arbitrarily.
Even so,
``near-perfect'' kernels (with reasonably large MMD and very small variance)
will likely still be chosen as the maximizer of the regularized criterion,
even if we do not estimate the (extremely large) ratio accurately.
Likewise, near-constant kernels (with very small variance but still small )
will generally have their  \emph{under}estimated,
and so are unlikely to be selected
when a better kernel is available. The  component in \eqref{eq:deepkernel_simpleForm}
may also help avoid extremely small variances.

Given  data points,
this result also gives insight into how many we should use to train the kernel
and how many to test.
With perfect optimization,
\cref{thm:opt-power-rate}
shows a bound on the asymptotic power of the test is maximized by
training on  points,
and testing on the remainder.


\begin{figure*}[!t]
    \begin{center}
        \subfigure
        {\pdftooltip{\includegraphics[width=0.77\textwidth]{fig/legend_crop.pdf}}{These are baselines considered in this paper. See more details in Section~\ref{sec:exp}.}}
        \subfigure[Power vs. ; ]
        {\includegraphics[width=0.26\textwidth]{fig/Test_power_HDGM_n.pdf}}
        \subfigure[Level vs. ; ]
        {\includegraphics[width=0.24\textwidth]{fig/Type_I_HDGM_n.pdf}}
        \subfigure[Power vs. ; ]
        {\includegraphics[width=0.24\textwidth]{fig/Test_power_HDGM_d.pdf}}
        \vspace{-0.3cm}
        \subfigure[Level vs. ; ]
        {\includegraphics[width=0.24\textwidth]{fig/Type_I_HDGM_d.pdf}}
        \caption{Results on \emph{HDGM-S} and \emph{HDGM-D} for  (black line).
        Left: average test power (a) and Type I error (b) when increasing the number of samples , keeping .
        Right: average test power (c) and Type I error (d) when increasing the dimension , keeping . Shaded regions show standard errors for the mean.}  \label{fig:HDGM_RES}
    \end{center}
    \vspace{-1em}
\end{figure*}

\section{Experimental Results}
\label{sec:exp}



\subsection{Comparison on Benchmark Datasets} \label{sec:benchmark-exp}



We compare the following tests on several datasets:
\begin{compactitem}
    \item MMD-D: \underline{MMD} with a \underline{d}eep kernel; our method described in \cref{sec:DKforTST}.
    \item MMD-O: \underline{MMD} with a Gaussian kernel whose lengthscale is \underline{o}ptimized as in \cref{sec:DKforTST}. This gives better results than standard heuristics.
    \item \underline{M}ean \underline{e}mbedding (ME): a state-of-the-art test \citep{Chwialkowski2015,Jitkrittum2016}
    based on differences in Gaussian kernel mean embeddings
    at a set of optimized points.
    \item \underline{S}mooth \underline{c}haracteristic \underline{f}unctions (SCF): a state-of-the-art test \citep{Chwialkowski2015,Jitkrittum2016}
    based on differences in Gaussian mean embeddings at a set of optimized frequencies.
    \item \underline{C}lassifier \underline{two}-\underline{s}ample \underline{t}ests, including C2STS-S \citep{Lopez:C2ST} and C2ST-L \citep{cheng:net-logits} as described in \cref{sec:c2st-relation}.
    We set the test thresholds via permutation for both.
\end{compactitem}

For synthetic datasets,
we take a single sample set for  and 
and learn a kernel/test locations/etc once for each method on that training set.
We then evaluate its rejection rate
on 100 new sample sets , 
from the same distribution.
For real datasets,
we select a subset of the available data for  and 
and train on that;
we then evaluate on 100 random subsets, disjoint from the training set, of the remaining data.
We repeat this full process 10 times,
and report the mean rejection rate of each test.
\Cref{tab:t_test_RES} shows significance tests. Further details
are in \cref{Asec:exp_set}.

\vspace{-1.5ex}\paragraph{\emph{Blob} dataset.}
\emph{Blob-D} is the dataset shown in \cref{fig:moti};
\emph{Blob-S} has  also equal to the distribution shown in \cref{fig:moti}a, so that the null hypothesis holds.
Details are given in \cref{tab:synthetic datasets} (\cref{Asec:syn_intro}).

Results are shown in \cref{fig:Blob_RES}.
MMD-D and C2ST-L are the clear winners in power,
with MMD-D better in the higher-sample regime, and MMD-D is more reliable than C2STs.
\Cref{fig:Blob_RES}c shows that  is higher for MMD-D than MMD-O,
in addition to the actual test power being better,
as discussed in \cref{sec:MMD_limited}.
All methods have expected Type I error rates.




\begin{table*}[ht]
\centering
  \footnotesize
  \caption{\emph{Higgs} (): average test powerstandard error for  samples. Bold represents the highest mean per row.} \label{tab:Higgs_RES1}
\vspace{1mm}
\begin{tabular}{c|cccccc}
\toprule
 & ME & SCF & C2ST-S & C2ST-L &MMD-O & MMD-D \\
\midrule
\phantom{1}1000 & \mnstd{0.120}{0.007} & \mnstd{0.095}{0.022} & \mnstd{0.082}{0.015} & \mnstd{0.097}{0.014} & \mnstd{\bf 0.132}{0.005} & \mnstd{0.113}{0.013} \\
\phantom{1}2000 & \mnstd{0.165}{0.019} & \mnstd{0.130}{0.026} & \mnstd{0.183}{0.032} & \mnstd{0.232}{0.017} & \mnstd{0.291}{0.012} & \mnstd{\bf 0.304}{0.035} \\
\phantom{1}3000 & \mnstd{0.197}{0.012} & \mnstd{0.142}{0.025} & \mnstd{0.257}{0.049} & \mnstd{0.399}{0.058} & \mnstd{0.376}{0.022} & \mnstd{\bf 0.403}{0.050} \\
\phantom{1}5000 & \mnstd{0.410}{0.041} & \mnstd{0.261}{0.044} & \mnstd{0.592}{0.037} & \mnstd{0.447}{0.045} & \mnstd{0.659}{0.018} & \mnstd{\bf 0.699}{0.047} \\
\phantom{1}8000 & \mnstd{0.691}{0.067} & \mnstd{0.467}{0.038} & \mnstd{0.892}{0.029} & \mnstd{0.878}{0.020} & \mnstd{0.923}{0.013} & \mnstd{\bf 0.952}{0.024} \\
          10000 & \mnstd{0.786}{0.041} & \mnstd{0.603}{0.066} & \mnstd{0.974}{0.007} & \mnstd{0.985}{0.005} & \mnstd{\bf 1.000}{0.000} & \mnstd{\bf 1.000}{0.000} \\
\midrule
Avg. & 0.395 & 0.283 & 0.497 & 0.506 & 0.564 & {\bf 0.579} \\
\bottomrule
\end{tabular}
\vspace{-1em}
\end{table*}

\begin{table*}[ht]
\centering
  \footnotesize
  \caption{\emph{MNIST} (): average test powerstandard error for comparing  real images to  DCGAN samples. } \label{tab:MNIST_RES1}
\vspace{1mm}
\begin{tabular}{c|cccccc}
\toprule
 & ME & SCF & C2ST-S& C2ST-L & MMD-O & MMD-D \\
\midrule
\phantom{1}200 & \mnstd{0.414}{0.050} & \mnstd{0.107}{0.018} & \mnstd{0.193}{0.037} & \mnstd{0.234}{0.031} & \mnstd{0.188}{0.010} & \mnstd{\bf 0.555}{0.044}  \\
\phantom{1}400 & \mnstd{0.921}{0.032} & \mnstd{0.152}{0.021} & \mnstd{0.646}{0.039} & \mnstd{0.706}{0.047} & \mnstd{0.363}{0.017} & \mnstd{\bf 0.996}{0.004} \\
\phantom{1}600 & \mnstd{\bf 1.000}{0.000} & \mnstd{0.294}{0.008} & \mnstd{\bf 1.000}{0.000} & \mnstd{0.977}{0.012} & \mnstd{0.619}{0.021} & \mnstd{\bf 1.000}{0.000} \\
\phantom{1}800 & \mnstd{\bf 1.000}{0.000} & \mnstd{0.317}{0.017} & \mnstd{\bf 1.000}{0.000} & \mnstd{\bf 1.000}{0.000} & \mnstd{0.797}{0.015} & \mnstd{\bf 1.000}{0.000} \\
          1000 & \mnstd{\bf 1.000}{0.000} & \mnstd{0.346}{0.019} & \mnstd{\bf 1.000}{0.000} & \mnstd{\bf 1.000}{0.000} & \mnstd{0.894}{0.016} & \mnstd{\bf 1.000}{0.000} \\
\midrule
Avg. & 0.867 & 0.243 & 0.768 & 0.783 & 0.572 & {\bf 0.910} \\
\bottomrule
\end{tabular}
\vspace{-1em}
\end{table*}

\vspace{-1.5ex}\paragraph{High-dimensional Gaussian mixtures.}
Here we study bimodal Gaussian mixtures in increasing dimension.
Each distribution has two Gaussian components;
in \emph{HDGM-S},  and  are the same,
while in \emph{HDGM-D},  and  differ in the covariance of a single dimension pair but are otherwise the same.
Details are in \cref{tab:synthetic datasets} (\cref{Asec:syn_intro}).
We consider both increasing  while keeping 
and increasing  while keeping ,
with results shown in \cref{fig:HDGM_RES}.
Again, MMD-D has generally the best test power across a range of problem settings,
with reasonable type I error.





\vspace{-1.5ex}\paragraph{\emph{Higgs} dataset \citep{Baldi_Higgs_datasets}.}
We compare the jet -momenta distribution () of the background process, , which lacks Higgs bosons,
to the corresponding distribution  for the process that produces Higgs bosons,
following \citet{Chwialkowski2015}.
As discussed in these previous works, -momenta carry very little discriminating information for recognizing whether Higgs bosons were produced.
We consider a series of tests with increased number of samples .

We report average test power (comparing  to ) in \cref{tab:Higgs_RES1},
and average type-I error (comparing  to  or  to ) in \cref{tab:Higgs_RES2} (\cref{sec:typeI}).
As before, MMD-D generally performs the best;
although the improvement over MMD-O here is not dramatic,
MMD-D does notably outperform C2ST.
All methods maintain reasonable Type I errors.


\vspace{-1.5ex}\paragraph{\emph{MNIST} generative model.}
The \emph{MNIST} dataset contains  handwritten digit images  \citep{lecun1998gradient}.
We compare true \emph{MNIST} data samples 
to samples  from a pretrained {deep convolutional generative adversarial network} (DCGAN) \citep{DCGAN_Radford}.
Samples from both distributions are shown in \cref{fig:MNIST} (in \cref{Asec:data_visual}).

We consider tests for increasing numbers of samples ,
and report average test power (for  to ) in \cref{tab:MNIST_RES1}
and average Type I error ( to ) in \cref{tab:MNIST_RES2} (in \cref{sec:typeI}).
MMD-D substantially outperforms its competitors in test power,
with the desired Type I error.
ME also does well in this case:
it is perhaps particularly suited to this problem,
since it is capable of identifying either modes dropped by the generative model or spurious modes it inserts.


\vspace{-1.5ex}\paragraph{\emph{CIFAR-10} vs \emph{CIFAR-10.1}.}
\emph{CIFAR-10.1} \citep{recht:imagenet} is an attempt to collect a new test set for the very popular \emph{CIFAR-10} image classification dataset \citep{cifar10}.
Normally, when evaluating a supervised model,
we consider the test set an independent sample from the training distribution, ideally never-before-seen by the training algorithm.
But modern computer vision model architectures and training procedures have been developed based on repeatedly evaluating on the \emph{CIFAR-10} test set (),
so it is possible that current models themselves are dependent on .
\emph{CIFAR-10.1} () is an attempt at an independent sample from this distribution, collected after the models were trained, so that they are truly independent of .
These models do obtain substantially lower accuracies on  than on  -- but this drop is surprisingly consistent across models, which seems unlikely to be due to the expected overfitting.
The main potential explanation proposed by \citeauthor{recht:imagenet}\ is dataset shift,
but their attempt (in their Appendix C.2.8) at what amounts to a C2ST-S did not reject .\footnote{Assuming pretrained classifiers are independent of , Figure 1 of \citet{recht:imagenet} indicates that the joint (images, labels) distribution certainly differs between \emph{CIFAR-10} and \emph{CIFAR-10.1}. We test here whether the marginal image distribution differs.}
Samples from each distribution are shown in \cref{fig:CIFAR10} (\cref{Asec:data_visual}).

We train on  images from each dataset and test on , so that we use the entirety of \emph{CIFAR-10.1} each time, and average over ten repetitions.
These tests provide strong evidence (\cref{tab:cifar10_RES}) that images in the \emph{CIFAR-10.1} test set \emph{are} statistically different from the \emph{CIFAR-10} test set,
with MMD-D again strongest
and ME still performing well.

Our learned kernel also helps provide some ability to interpret the difference between  and ,
particularly if we use it for an ME test.
\Cref{sec:cifar-interp} explores this.


\begin{table}[t!]
  \centering
  \footnotesize
  \caption{\emph{CIFAR-10.1} (): mean rejection rates.}
  \vspace{1mm}
    \begin{tabular}{llllll}
    \toprule
ME & SCF & C2ST-S & C2ST-L & MMD-O & MMD-D \\
    \midrule
0.588 & 0.171 & 0.452 & 0.529 & 0.316 & {\bf 0.744} \\
    \bottomrule
    \end{tabular}\label{tab:cifar10_RES}\vspace{-1em}
\end{table}


\citet{recht:imagenet} also provide a new ImageNetV2 test set for the ImageNet dataset, with similar properties;
we defer this more challenging problem to future work.



















\begin{table*}[!t]
  \centering
  \footnotesize
  \caption{Mean test power on \emph{Blob} (), \emph{HDGM} (), \emph{Higgs} () and \emph{MNIST} () for .  See \cref{sec:tpp-vs-ce} for the naming scheme; S+C corresponds to C2ST-S, L+C to C2ST-L, and D+J to MMD-D.
  L+M is the method proposed by \citet{Matthias:deep-test}.
}\label{tab:CE_for_TST}
  \vspace{1mm}
    \begin{tabular}{lllllllllll}
    \toprule
& S+C & L+C & G+C & D+C & L+M & G+M & D+M & L+J & G+J & D+J \\
    \midrule
    \emph{Blob} 
& 0.835 & 0.942 & 0.901
    & 0.900
    & 0.851
    & 0.960
    & 0.906
    & 0.952 & 0.966 & {\bf 0.985} \\
    \emph{HDGM} 
& 0.472 & 0.585 & 0.287
    & 0.302
    & 0.494
    & 0.223
    & 0.539
    & 0.635 & 0.604 & {\bf 0.659} \\
\emph{Higgs}
    & 0.257 & 0.399 & 0.353
    & 0.384
    & 0.321
    & 0.254
    & 0.379
    & 0.295 & 0.364 & {\bf 0.403} \\
    \emph{MNIST}
& 0.646 & 0.706 & 0.784
    & 0.803
    & 0.845
    & 0.680
    & 0.760
    & 0.935 & 0.976 & {\bf 0.996} \\
    \midrule
    Avg.         & 0.553 & 0.658 & 0.581
    & 0.597
    & 0.628
    & 0.529
    & 0.646
    & 0.704 & 0.727 & {\bf 0.761} \\
    \bottomrule
    \end{tabular}\vspace{-1em}
\end{table*}



\begin{table}[!t]
  \centering
  \footnotesize
  \caption{Paired t-test results () for the results of \cref{sec:benchmark-exp}. For \textit{HDGM}, we fix  (corresponding to \cref{fig:HDGM_RES}a).  indicates MMD-D achieved statistically significantly higher mean test power than the other method,  that it did not.}
  \vspace{1mm}
    \begin{tabular}{l|ccccc}
    \toprule
    Dataset & \multicolumn{1}{l}{ME} & \multicolumn{1}{l}{SCF} & \multicolumn{1}{l}{C2ST-S} & \multicolumn{1}{l}{C2ST-L} & \multicolumn{1}{l}{MMD-O} \\
    \midrule
    \emph{Blob} &  \checkmark  &  \checkmark  &  \checkmark  &  &  \\
    \emph{HDGM} &  \checkmark  &  \checkmark  &  \checkmark & \checkmark & \checkmark \\
    \emph{Higgs} &  \checkmark  & \checkmark   &  \checkmark  &  &  \\
    \emph{MNIST} &  \checkmark  &  \checkmark  &  \checkmark  & \checkmark &  \checkmark\\
    \bottomrule
    \end{tabular}\label{tab:t_test_RES}\vspace{-2em}
\end{table}

\subsection{Ablation Study} \label{sec:tpp-vs-ce}

We now study in more detail the difference between MMD-D and closely related methods.
Recall from \cref{sec:c2st-relation} that there are two main differences between MMD-D and C2STs:
first,
using a ``full'' kernel \eqref{eq:deepkernel_simpleForm}
rather than the sign-based kernel \eqref{eq:sign-kernel}
or the intermediate linear kernel \eqref{eq:lin-kernel}.
Second, training to maximize  \eqref{eq:tpp-hat}
rather than a cross-entry surrogate.
MMD-D uses a full kernel \eqref{eq:deepkernel_simpleForm} trained for test power;
C2ST-S effectively uses the sign kernel \eqref{eq:sign-kernel} trained for cross entropy.

In this section, we consider the performance of several intermediate models empirically,
demonstrating that both factors help in testing.
All are based on the same feature extraction architecture ;
some models add a classification layer with new parameters  and ,

which is treated as outputting classification logits.
The model variants we consider are
\begin{compactdesc}
\item[S] A kernel ; corresponds to a test statistic of the accuracy of  (\cref{thm:c2st-equiv}).
\item[L] A kernel ; corresponds to a test statistic comparing the mean value of  (\cref{thm:c2st-l-equiv}).
\item[G] A Gaussian kernel .
\item[D] The deep kernel \eqref{eq:deepkernel_simpleForm} based on .
\end{compactdesc}
We combine these model variants with a suffix describing the optimization objective:
\begin{compactdesc}
    \item[J] Choose , including possibly  and , to optimize the approximate test power \eqref{eq:tpp-hat}.
    \item[M] Choose , including possibly  and , to maximize the value of the empirical MMD between two samples.\footnote{If a deep kernel is unbounded, directly maximizing MMD will make optimized parameters of  be infinite. Thus, for L+M, we consider a normalized linear deep kernel: , where  and  is the Frobenius norm.}
    \item[C] Choose , including  and , to optimize cross-entropy using the classifier that specifies the probability of  belonging to  as .\footnote{G+C and D+C take the fixed  embeddings, then find the optimal lengthscale/etc by optimizing .}
\end{compactdesc}


\Cref{tab:CE_for_TST} presents results for all of these methods
(except for S+J, which is non-differentiable and hence difficult to optimize).
Performance generally improves
as we move from S to L to G to D,
and from C to J.

\subsection{Architecture design of deep kernels}
For \emph{Blob}, \emph{HDGM} and \emph{Higgs},  is a five-layer fully-connected neural network, with softplus activations. the number of neurons in hidden and output layers of  are set to  for \emph{Blob},  for \emph{HDGM} and  for \emph{Higgs}, where  is the dimension of samples.
in general, we expect similar fully-connected networks,
to be reasonable choices for datasets where strong structural assumptions are not known,
perhaps with  as a baseline width for datasets of at least moderate dimension.

For \emph{MNIST} and \emph{CIFAR},  is a \emph{convolutional neural network} (CNN) that contains four convolutional layers and one fully-connected layer.
The structure of the CNN follows the structure of the feature extractor in the DCGAN's discriminator \citep{DCGAN_Radford} (see \cref{fig:MMDDK_phi,fig:MMD_CIFAR_F} for the structure of  in MMD-D, and \cref{fig:C2ST_F,fig:C2ST_CIFAR_F} for the structure of classifier  in C2ST-S and C2ST-L).
In general, we expect GAN discriminator architectures to work well for image datasets,
as the problem is closely related.







\section{Conclusions}
The test power of MMD is limited by simple kernels (e.g., Gaussian kernel or other translation-invariant kernels) when facing complex-structured distributions,
but we can avoid this problem with richer \emph{deep kernels}, which is no longer translation-invariant.
We show that optimizing the parameters of these kernels to maximize the test power,
as proposed by \citet{sutherland:mmd-opt},
outperforms state-of-the-art alternatives
even when considering large, deep kernels with hundreds of thousands of parameters,
rather than the simple shallow kernels they considered.
We provide theoretical guarantees that this process is reasonable to conduct on finite samples, and asymptotically selects the most powerful kernel.
We also give deeper insight into the relationship between this approach and classifier two-sample tests \citep{Lopez:C2ST},
explaining why this approach outperforms that one.

We thus recommend practitioners to use optimized deep kernel methods when they wish to check if two distributions are the same, rather than indirectly training a classifier.

\ifdefined\isaccepted
\section*{Acknowledgements}
This work was supported by the Australian Research Council under FL190100149 and DP170101632, and
by the Gatsby Charitable Foundation.
FL, JL and GZ gratefully acknowledge the support of the NVIDIA Corporation with the donation of two NVIDIA TITAN V GPUs for this work. FL also acknowledges the support from UTS-FEIT and UTS-AAII.
DJS would like to thank Aram Ebtekar, Ameya Velingker, and Siddhartha Jain for productive discussions.
\fi

\bibliography{mybib}
\bibliographystyle{icml2020}


\clearpage
\onecolumn
\appendix

\section{Theoretical analysis}\label{Asec:proof}
\Cref{sec:proof:main} proves the main results under some assumptions about the kernel parameterization,
using intermediate results about uniform convergence of our estimators in \cref{sec:proof:unif-conv}.
\Cref{sec:proof:kernel-props} then shows that these assumptions hold for different settings of kernel learning.

\subsection{Preliminaries}
Given a kernel  and sample sets , , define the  matrix

we will often omit  when it is clear from context.
The -statistic estimator of the squared MMD \eqref{eq:MMD_U_compute} is

The squared MMD is .
The variance of  is given by \cref{thm:var-decomp}.

\begin{lemma} \label{thm:var-decomp}
    For a fixed kernel  and random sample sets , ,
    we have
    
    where
    
    Thus as ,
    
\end{lemma}
\begin{proof}
Let  denote the pair ,
and ,
so that .
Via Lemma A in Section 5.2.1 of \citet{serfling},
we know that \eqref{eq:var-decomp} holds with

and

\end{proof}

We use a -statistic estimator \eqref{eq:estimate_sigma_H1} for :

As a -statistic,
 is biased.
In fact, \citet{sutherland:mmd-opt} and \citet{unbiased-var-ests} provide an unbiased estimator of  -- including the terms of order .
Although this estimator takes the same quadratic time to compute as \eqref{eq:estimate_sigma_H1},
it contains many more terms, which are cumbersome both for implementation and for analysis.
\eqref{eq:estimate_sigma_H1} is also marginally more convenient in that it is always at least nonnegative.
As we show in \cref{thm:var-est-bias},
the amount of bias is negligible as  increases.
In practice, we expect the difference to be unimportant
-- or the -statistic may in fact be beneficial, since underestimating  harms the estimate of  more than overestimating it does.

Similarly, although we use the -statistic estimator \eqref{eq:MMD_U_compute},
it would be very similar to use
the biased estimator ,
or the minimum variance unbiased estimator .
Showing comparable concentration behavior to \cref{thm:mmd-conv} is trivially different,
and in fact it is also not difficult to show  is the same for all three estimators (up to lower-order terms).

\subsection{Main results} \label{sec:proof:main}
We will require the following assumptions.
These are fairly agnostic as to the kernel form;
\cref{sec:proof:deep-kernels} shows that these assumptions hold
(and gives the constants)
for the kernels \eqref{eq:deepkernel_simpleForm} we use in the paper.
\begin{assumplist}
  \item \label{assump:k-bounded}
    The kernels  are uniformly bounded:
    
    For the kernels we use in practice, .

  \item \label{assump:omega-bounded}
    The possible kernel parameters 
    lie in a Banach space of dimension .
    Furthermore, the set of possible kernel parameters 
    is bounded by ,
    .

    \Cref{sec:proof:deep-kernels} builds this space and its norm for the kernels we use in the paper.

  \item \label{assump:k-lipschitz}
    The kernel parameterization is Lipschitz:
    for all 
    and ,
    
    \cref{thm:kern-lip} in \cref{sec:proof:deep-kernels} gives an expression for  for the kernels we use in the paper.
\end{assumplist}

We will first show the main results under these general assumptions,
using uniform convergence results shown in \cref{sec:proof:unif-conv},
then show \cref{assump:omega-bounded,assump:k-lipschitz} for particular kernels in \cref{sec:proof:deep-kernels}.

\begin{theorem} \label{thm:ratio-conv}
Under \cref{assump:omega-bounded,assump:k-lipschitz,assump:k-bounded},
let  be the set of kernel parameters for which ,
and assume .
Take .
Then, with probability at least ,

and thus, treating  as a constant,


\end{theorem}
\begin{proof}
Let .
Using ,
we begin by decomposing

\Cref{thm:mmd-conv,thm:var-conv} show uniform convergence of  and , respectively.
Thus, with probability at least ,
the error is at most

Taking  gives

Using , ,
we can get the slightly simpler upper bound

\end{proof}
It is worth noting that, if we are particularly concerned about the  dependence,
we can make some slightly different choices in the decomposition to improve the dependence on  while worsening the rate with .

\begin{corollary} \label{thm:param-conv}
In the setup of \cref{thm:ratio-conv},
additionally assume that there is a unique population maximizer  of  from \eqref{eq:tpp},
i.e.\ for each  we have

For each ,
let  and  be sequences of sample sets of size ,
let  denote ,
and take  to be a maximizer of .\footnote{In fact, it suffices for the  to only approximately maximize , as long as their suboptimality is .}{}
Then  converges in probability to .
\end{corollary}
\begin{proof}
By \cref{thm:ratio-conv},
.
Then the result follows by Theorem 5.7 of \citet{van2000asymptotic}.
\end{proof}


\begin{corollary} \label{thm:opt-power}
    In the setup of \cref{thm:ratio-conv},
    suppose we use  sample points to select a kernel 
    and  sample points to run a test of level .
    Let  denote the rejection threshold for a test with that kernel of size .
Define
    ,
    and constants , , ,  depending on , , ,  and .
    For any ,
    with probability at least ,
    this test procedure has power
    
\end{corollary}
\begin{proof}
    Let .
    By \cref{thm:ratio-conv},
    there are some  depending on , , , , and 
    such that as long as ,
    with probability at least  it holds that
    
    Assume for the remainder of this proof that this event holds.
    Letting ,
    we know because  maximizes  that .
Using uniform convergence twice,
    

    Now, although
    \cref{prop:asymptotics} establishes that  
    and it is even known \citep[Theorem 5]{Korolyuk1988} that
     is ,
the constant in that convergence will depend on the choice of  in an unknown way.
    It's thus simpler to use the very loose but uniform (McDiarmid-based) bound
    given by Corollary 11 of \citet{Gretton2012},
    which implies 
    no matter the choice of .

    We will now need a more precise characterization of the power than that provided by the
    central limit theorem of \cref{prop:asymptotics}.
    \Citet{Callaert:berry-esseen-ustat} provide such a result, a Berry-Esseen bound on -statistic convergence:
    there is some absolute constant  such that
    
    Letting  be the appropriate rejection threshold for  with  samples,
the power of a test with kernel  is
    
    using a new constant .
    Combining the previous results on  and 
    yields the claim.
\end{proof}

\begin{corollary} \label{thm:opt-power-rate}
    In the setup of \cref{thm:opt-power},
    suppose we are given  data points to divide between
     training points
    and  testing points,
    and  is fixed.
    Ignoring the Berry-Esseen convergence term outside of ,
    the asymptotic power upper bound
    
    is maximized only when,
    as other quantities remain constant,
    we pick  to satisfy
    
\end{corollary}
\begin{proof}
    Because the  term is constant,
    we wish to choose
    
    Clearly neither endpoint is optimal.
    Relaxing  to be real-valued,
    the optimum must be achieved at a stationary point, where
    
    Multiplying by  and rearranging,
    we get that a stationary point is achieved exactly when
    

    Now write, without loss of generality, ,
    and so
    
    We will show that  requires ,
    implying the result.

    We first suppose ,
    further breaking into cases which result in different terms inside  and  becoming dominant:
    
    In each case,  and so ,
    contradicting that .
    Thus a stationary point requires  for a stationary point.

    We now do the same for .
    First, clearly ; suppose that in fact ,
    i.e.\ .
    In this case, we would have
    
    and ,
    so that  requires ,
    i.e.\ .
    For ,
    this contradicts .
    So we know that .
    Now, the remaining options for  all yield :
    

    Thus we have established that .
    Thus, we obtain that
    
    Asymptotic equality hence requires
    .
\end{proof}























\subsection{Uniform convergence results} \label{sec:proof:unif-conv}
These results, on the uniform convergence of  and ,
were used in the proof of \cref{thm:ratio-conv}.

\begin{prop}
\label{thm:mmd-conv}
Under \cref{assump:omega-bounded,assump:k-bounded,assump:k-lipschitz},
we have that with probability at least ,

\end{prop}
\begin{proof}
Theorem 7 of \citet{sriperumbudur2009choice} gives a similar bound in terms of Rademacher chaos complexity, but for ease of combination with our bound on convergence of the variance estimator, we use a simple -net argument instead.

We study the random error function


First, we place  points 
such that for any point ,
;
\cref{assump:omega-bounded} ensures this is possible with at most  points
\citep[Proposition 5]{cucker:foundations}.

Now, , because  is unbiased.
Recall that ,
and via \cref{assump:k-bounded} we know .
This , and hence , satisfies bounded differences:
if we replace  with ,
obtaining 
where  agrees with  except when  or  is ,
then

Using McDiarmid's inequality for each  and a union bound,
we then obtain that with probability at least ,


We also have via \cref{assump:k-lipschitz}, for any two ,

so that .
Combining these two results,
we know that with probability at least 

setting  yields the desired result.
\end{proof}







\begin{prop} \label{thm:var-conv}
Under \cref{assump:k-bounded,assump:k-lipschitz,assump:omega-bounded},
with probability at least ,

\end{prop}
\begin{proof}
We again use an -net argument
on the (random) error function

First, choose  points 
such that for any point ,
;
again,
via \cref{assump:omega-bounded} and Proposition 5 of \citet{cucker:foundations}
we have .
By \cref{thm:var-est-bias,thm:var-est-mcd} and a union bound,
with probability at least ,

\Cref{thm:sigma-hat-lip} shows that ,
which means that with probability at least ,

Taking  gives the desired result.
\end{proof}

\begin{lemma} \label{thm:var-est-mcd}
For any kernel  bounded by  (\cref{assump:k-bounded}),
with probability at least ,

\end{lemma}
\begin{proof}
    We simply apply McDiarmid's inequality to .
    Suppose we change  to ,
    giving a new  matrix  which agrees with  on all but the first row and column.
    Note that ,
    and recall
    

    The first term in the parentheses of  changes by
    
    In this sum, if none of , , or  are one, the term is zero.
    The  terms for which  are each upper-bounded by ,
    simply bounding each  or  by .
    Of the remainder, there are  terms where ,
    each .
    We are left with  terms which have exactly one of  or  equal to ;
    the  terms are ,
    so each of these terms is at most .
    The total sum is thus at most
    

    The remainder of the change in  can be determined by bounding
    
    which then gives us
    
    Thus
    
    Because the same holds for changing any of the  pairs,
    the result follows by McDiarmid's inequality.
\end{proof}

\begin{lemma} \label{thm:var-est-bias}
For any kernel  bounded by  (\cref{assump:k-bounded}),
the estimator  satisfies

\end{lemma}
\begin{proof}
We have that


Most terms in these sums have their indices distinct;
these are the ones that we care about.
(We could evaluate the expectations of the other terms exactly, but it would be tedious.)
We can thus break down the first term as

where  is the appropriately-weighted mean of the various  terms for which  are not mutually distinct.
Since ,

and so  as well.
Noting that

we obtain


The second term can be handled similarly:

where  is the appropriately-weighted mean of the non-distinct terms,
.
For  all distinct,
.
Here

and so


Recalling ,

and since , we have , yielding the result.
\end{proof}







\begin{lemma} \label{thm:sigma-hat-lip}
  Under \cref{assump:k-bounded,assump:k-lipschitz},
  we have
  
\end{lemma}
\begin{proof}
    We first handle the change in :
    
    We can handle both terms by bounding
    
    Using \cref{assump:k-lipschitz} and the definition of ,
    
    so
    
    and hence
    

Again using \eqref{eq:h-step-omega},
    we also have
    
\end{proof}


\subsection{Constructing appropriate kernels} \label{sec:proof:kernel-props}

We now show \cref{thm:main-rbf-lip,thm:main-mkl-lip,thm:main-kern-lip},
which each state that 
\cref{assump:k-lipschitz} is satisfied by various choices of kernel.
The following assumption will be useful for different kernel schemes.

\begin{netassumplist}
  \item \label{assump:x-bounded}
    The domain  is Euclidean and bounded,
     for some constant .
\end{netassumplist}

We begin by recalling a well-known property of the Gaussian kernel, useful for both Gaussian bandwidth selection and deep kernels.
A proof is in \cref{sec:misc-proofs}.
\begin{restatable}{lemma}{gausslip} \label{thm:gauss-lip}
    The Gaussian kernel
    
    satisfies
    
\end{restatable}

\subsubsection{Gaussian bandwidth selection (Proposition \ref{thm:main-rbf-lip})} \label{sec:proof:bw-sel}

\Cref{thm:gauss-lip} immediately gives us \cref{assump:k-lipschitz} when we chose among Gaussian kernels:
\begin{prop} \label{thm:rbf-lip}
    Define a one-dimensional Banach space for inverse lengthscales of Gaussian kernels ,
    so that ,
    with standard addition and multiplication
    and norms defined by the absolute value,
    and  taken to be the constant  function.
    Let  be any subset of this space.
    Under \cref{assump:x-bounded},
    \cref{assump:k-lipschitz} holds:
    for any  and ,
    
\end{prop}
\begin{proof}
    
\end{proof}


\subsubsection{Deep kernels (Proposition \ref{thm:main-kern-lip})} \label{sec:proof:deep-kernels}

To handle the deep kernel case, we will need some more assumptions on the form of the kernel.

\begin{netassumplist}[resume]
  \item \label{assump:phi-form}
    
    is a feedforward neural network with  layers given by
    
    where the network parameter 
    consists of all the weight matrices 
    and biases ,
    and the activation functions  are each 1-Lipschitz,
    ,
    with  so that .
    Define a Banach space on ,
    with addition and scalar multiplication componentwise,
    and
    
    where the matrix norm denotes operator norm
    .
    (For convolutional networks, see \cref{remark:convnets}.)

  \item \label{assump:k-form}
     is a kernel of the form \eqref{eq:deepkernel_simpleForm},
    
    with
    ,
     a kernel function,
    and  a kernel with .

    Note that this includes kernels of the form : take  and .

  \item \label{assump:kappa-lip}
     in \cref{assump:k-form} is a kernel function satisfying
    
    This holds for a Gaussian  via \cref{thm:gauss-lip}.
\end{netassumplist}

We now turn to proving \cref{assump:k-lipschitz} for deep kernels.
First, we will need some smoothness properties of the network .
\begin{restatable}{lemma}{dnnlip} \label{thm:dnn-lip}
    Under \cref{assump:phi-form},
    suppose  have
    , ,
    with .
    Then, for any x,
    
    If , we furthermore have
    
\end{restatable}
The proof, by recursion, is given in \cref{sec:misc-proofs}.
We are now ready to prove \cref{assump:k-lipschitz} for deep kernels.
\begin{prop} \label{thm:kern-lip}
    Make \cref{assump:x-bounded,assump:omega-bounded,assump:k-form,assump:kappa-lip,assump:phi-form},
    with .\footnote{Of course, if we know a bound of , the result will still hold using . It is also possible to show a tighter result, via \eqref{eq:net-growth-gen} and \eqref{eq:net-lip-gen} or their analogue for ; the expression is simply less compact.}
    Then \cref{assump:k-lipschitz} holds:
    for any 
    and ,
    
\end{prop}
\begin{proof}
    
\end{proof}

\begin{remark}
For the deep kernels we use in the paper (\cref{assump:k-form,assump:kappa-lip,assump:phi-form}) on bounded domains (\cref{assump:x-bounded}),
we know  via \cref{thm:kern-lip};
\cref{thm:test-power-conv} combines \cref{thm:ratio-conv,thm:param-conv,thm:kern-lip}.
If we further use a Gaussian kernel  of bandwidth ,
the last bracketed term in the error bound of \cref{thm:ratio-conv} becomes

The component , from \eqref{eq:net-growth},
is approximately the largest that  could make its outputs' norms;
 will generally be on a comparable scale to the norm of the actual outputs of the network,
so their ratio is something like the ``unused capacity'' of the network to blow up its inputs.
This term is weighted about equally in the convergence bound with the square root of the total number of parameters in the network.
\end{remark}

\begin{remark} \label{remark:convnets}
We can handle convolutional networks as follows.
We define  in essentially the same way,
letting  denote the convolutional kernel
(the set of parameters being optimized),
but define  in terms of the operator norm of the linear transform corresponding to the convolution operator.
This is given in terms of the operator norm
of various discrete Fourier transforms of the kernel matrix
by Lemma 2 of \citet{Bibi2019};
see also Theorem 6 of \citet{sedghi:conv-svs}.
The number of parameters  is then the actual number of parameters optimized in gradient descent,
but the radius  is computed differently.\end{remark}


\subsubsection{Multiple kernel learning (Proposition \ref{thm:main-mkl-lip})} \label{sec:proof:mkl}

Multiple kernel learning \citep{Gonen:mkl} also falls into our setting.
A special case of this family of kernels was studied for the (easier to analyze) ``streaming'' MMD estimator by \citet{Gretton2012NeurIPS}.

\begin{netassumplist}[resume]
    \item \label{assump:mkl}
      Let  be a set of base kernels,
      each satisfying  for some finite .
      Define  as
      
      Define the norm of a kernel parameter by the norm of the corresponding vector .
      Let  be a set of possible parameters such that for each ,  is positive semi-definite, and  for some .
\end{netassumplist}

Not only does learning in this setting work (\cref{thm:mkl-lip}),
it is also -- unlike the deep setting -- efficient to find an exact maximizer of  (\cref{prop:mkl-soln}).

\begin{prop} \label{thm:mkl-lip}
    \Cref{assump:mkl} implies \cref{assump:k-lipschitz,assump:omega-bounded,assump:k-bounded}.
    In particular,
    
\end{prop}
\begin{proof}
\Cref{assump:omega-bounded} is immediate from \cref{assump:mkl}, since .
Let  denote the vector whose th entry is ,
so that .
As , we know .
\Cref{assump:k-bounded,assump:k-lipschitz} follow by Cauchy-Schwartz.
\end{proof}

\begin{prop} \label{prop:mkl-soln}
    Take \cref{assump:mkl},
    and additionally assume that  for some .
    A maximizer of  can then be found
    by scaling the solution to a convex quadratic program,
    
    where
    
    as long as  has at least one positive entry.
\end{prop}
\begin{proof}
The  matrix used by  and  takes a simple form:

Thus

Note that because  for any ,
we have .
We have now obtained a problem equivalent to the one in Section 4 of \citet{Gretton2012NeurIPS};
the argument proceeds as there.
\end{proof}

\subsection{Miscellaneous Proofs} \label{sec:misc-proofs}

The following lemma was used for \cref{thm:rbf-lip,thm:kern-lip}.

\gausslip*
\begin{proof}
    We have that
    
    We can bound the Lipschitz constant as its maximal derivative norm,
    
    Noting that
    
    vanishes only at ,
    the supremum is achieved by
    using that value, giving
    
    The result follows from
    
\end{proof}

This next lemma was used in \cref{thm:kern-lip}.

\dnnlip*
\begin{proof}
    First, ,
    showing \eqref{eq:net-growth-gen} when .
    In general,
    
    and expanding this recursion gives
    


    Now, we have \eqref{eq:net-lip-gen} for 
    because .
    For , we have
    
    Expanding the recursion yields
    

    When , we have that  and ,
    giving \eqref{eq:net-growth} and \eqref{eq:net-lip}.
\end{proof}







\section{Experimental Details}\label{Asec:exp_set}

\subsection{Details of synthetic datasets}\label{Asec:syn_intro}

Table~\ref{tab:synthetic datasets} shows details of four synthetic datasets. \emph{Blob} datasets are often used to validate two-sample test methods \citep{Gretton2012NeurIPS,Jitkrittum2016,sutherland:mmd-opt}, although we rotate each blob to show the benefits of non-homogeneous kernels. \emph{HDGM} datasets are first proposed in this paper. \emph{HDGM-D} can be regarded as \emph{high-dimension Blob-D} which contains two modes with the same variance and different covariance.

\begin{table}[ht]
  \centering
\caption{Specifications of  and  of synthetic datasets.  (same with Figure~\ref{fig:moti}a). , ,  is an identity matrix with size .  if  and  if . if ,  (same with Figure~\ref{fig:moti}a).  and  are set to  and , respectively.}\label{tab:synthetic datasets}

    \begin{tabular}{lll}
    \toprule
    \rule{0em}{1em}Datasets &   &  \rule{0em}{1em} \\
    \midrule
    \rule{0em}{1.5em}\emph{Blob-S} &  &  \\
    \rule{0em}{2em}\emph{Blob-D} &  &  \\
    \rule{0em}{1.5em}\emph{HDGM-S} &  &  \\
    \rule{0em}{2.2em}\emph{HDGM-D} &  &  \\
    \bottomrule
    \end{tabular}\vspace{-0.5em}
\end{table}

\subsection{Dataset visualization}\label{Asec:data_visual}

\Cref{fig:MNIST} shows images from real-\emph{MNIST} and ``fake''-\emph{MNIST},
while \cref{fig:CIFAR10} shows samples from \emph{CIFAR-10} and \emph{CIFAR-10.1}.

\begin{figure}[ht]
    \begin{center}
        \subfigure[Real-\emph{MNIST}]
        {\includegraphics[width=0.4\textwidth]{fig/Real_MNIST_100.png}}
        \subfigure[``Fake''-\emph{MNIST}]
        {\includegraphics[width=0.4\textwidth]{fig/Fake_MNIST_100.png}}
        \caption{Images from real-\emph{MNIST} and ``fake''-\emph{MNIST}. ``Fake''-\emph{MNIST} is generated by DCGAN \citep{DCGAN_Radford}.}    \label{fig:MNIST}
    \end{center}
    \vspace{-0.5cm}
\end{figure}

\begin{figure}[!t]
    \begin{center}
        \subfigure[\emph{CIFAR-10} test set]
        {\includegraphics[width=0.4\textwidth]{fig/Cifar10_real_100.png}}
        \subfigure[\emph{CIFAR-10.1} test set]
        {\includegraphics[width=0.4\textwidth]{fig/Cifar10_v4_100.png}}
        \caption{Images from \emph{CIFAR-10} test set and the new \emph{CIFAR-10.1} test set \citep{recht:imagenet}.}  \label{fig:CIFAR10}
    \end{center}
    \vspace{-0.5cm}
\end{figure}


\subsection{Configurations}\label{Asec:configuration}

We implement all methods on Python 3.7 (Pytorch 1.1) with a NIVIDIA Titan V GPU. We run ME and SCF using the official code \citep{Jitkrittum2016}, and implement C2ST-S, C2ST-L, MMD-D and MMD-O by ourselves. We use permutation test to compute -values of C2ST-S and C2ST-L, MMD-D, MMD-O and tests in Table~\ref{tab:CE_for_TST}. We set  for all experiments.  Following \citet{Lopez:C2ST}, we use a deep neural network  as the classifier in C2ST-S and C2ST-L, and train the  by minimizing cross entropy.
To fairly compare MMD-D with C2ST-S and C2ST-L, the network  in MMD-D has the same architecture with feature extractor in . Namely, , where  is a two-layer fully-connected network. The network  is a simple binary classifier that takes extracted features (through ) as input. For test methods shown in Table~\ref{tab:CE_for_TST}, the network  in them also has the same architecture with that in MMD-D.

For \emph{Blob}, \emph{HDGM} and \emph{Higgs},  is a five-layer fully-connected neural network. The number of neurons in hidden and output layers of  are set to  for \emph{Blob},  for \emph{HDGM} and  for \emph{Higgs}, where  is the dimension of samples. These neurons are with softplus activation function, i.e., . For \emph{MNIST} and \emph{CIFAR},  is a \emph{convolutional neural network} (CNN) that contains four convolutional layers and one fully-connected layer. The structure of the CNN follows the structure of the feature extractor in the discriminator of DCGAN \citep{DCGAN_Radford} (see \cref{fig:MMDDK_phi,fig:MMD_CIFAR_F} for the structure of  in MMD-D, and \cref{fig:C2ST_F,fig:C2ST_CIFAR_F} for the structure of classifier  in C2ST-S and C2ST-L). The link of DCGAN code is \url{https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/dcgan/dcgan.py}.

We use Adam optimizer \citep{Adam:optimizer} to optimize 1) parameters of  in C2ST-S and C2ST-L, 2) parameters of  in MMD-D and 3) kernel lengthscale in MMD-O. We set drop-out rate to zero when training C2ST-S, C2ST-L and MMD-D on all datasets.

\begin{figure}[!t]
    \begin{center}
        \includegraphics[width=0.95\textwidth]{fig/MNIST_DK.pdf}
        \caption{The structure of  in MMD-D on \emph{MNIST}. The kernel size of each convolutional layer is ; stride (S) is set to ; padding (P) is set to 1. We do not use dropout. Best viewed zoomed in.}\label{fig:MMDDK_phi}
    \end{center}
\end{figure}


\begin{figure}[!t]
    \begin{center}
        \includegraphics[width=0.95\textwidth]{fig/MNIST_C2ST.pdf}
        \caption{The structure of classifier  in C2ST-S and C2ST-L on \emph{MNIST}. The kernel size of each convolutional layer is ; stride (S) is set to ; padding (P) is set to 1. We do not use dropout. In the first layer, we will convert the \emph{CIFAR} images from  to . Best viewed zoomed in.}    \label{fig:C2ST_F}
    \end{center}
\end{figure}

\begin{figure}[!t]
    \begin{center}
        \includegraphics[width=0.95\textwidth]{fig/Cifar_DK.pdf}
        \caption{The structure of  in MMD-D on \emph{CIFAR}. The kernel size of each convolutional layer is ; stride (S) is set to ; padding (P) is set to 1. We do not use dropout in all layers. In the first layer, we will convert the \emph{CIFAR} images from  to . Best viewed zoomed in.}  \label{fig:MMD_CIFAR_F}
    \end{center}
\end{figure}

\begin{figure}[!t]
    \begin{center}
        \includegraphics[width=0.95\textwidth]{fig/Cifar_C2ST.pdf}
        \caption{The structure of classifier  in C2ST-S and C2ST-L on \emph{CIFAR}. The kernel size of each convolutional layer is ; stride (S) is set to ; padding (P) is set to 1. We do not use dropout. Best viewed zoomed in.} \label{fig:C2ST_CIFAR_F}
    \end{center}
\end{figure}

\subsection{Detailed parameters of all test methods}\label{Asec:para_set}

In this subsection, we demonstrate detailed parameters of all test methods. Except for learning rate of Adam optimizer, we use default parameters of Adam optimizer provided by Pytorch. We use one validation set (with the same size of training set) to roughly search these parameters. Using these parameters, we compute test power of each test method on  test sets (with the same size of training set).

For ME and SCF, we follow \citet{Chwialkowski2015} and set  for \emph{Higgs}. For other datasets, we set .

For C2ST-S and C2ST-L, we set batchsize to  for \emph{Blob},  for \emph{HDGM} and \emph{Higgs}, and  for \emph{MNIST} and \emph{CIFAR}. We set the number of epochs to  for \emph{Blob},  for \emph{HDGM}, \emph{Higgs} and \emph{CIFAR}, and  for \emph{MNIST}. We set learning rate to  for \emph{Blob}, \emph{HDGM} and \emph{Higgs}, and  for \emph{MNIST} and \emph{CIFAR} (following \citet{DCGAN_Radford}).

For MMD-O, we use full batch (i.e., all samples) to train MMD-O. we set the number of epochs to  for \emph{Blob}, \emph{HDGM}, \emph{Higgs} and \emph{CIFAR}, and  for \emph{MNIST}. We set learning rate to  for \emph{Blob}, \emph{MNIST} and \emph{CIFAR}, and  for \emph{HDGM}.

For MMD-D, we use full batch (i.e., all samples) to train MMD-D with samples from \emph{Blob}, \emph{HDGM} and \emph{Higgs}. We use mini-batch (batchsize is ) to train MMD-D with samples from \emph{MNIST} and \emph{CIFAR}. We set the number of epochs to  for \emph{Blob}, \emph{HDGM}, \emph{Higgs} and \emph{CIFAR}, and  for \emph{MNIST}. We set learning rate to  for \emph{Blob} and \emph{Higgs},  for \emph{HDGM},  for \emph{MNIST} and  for and \emph{CIFAR} (following \citet{DCGAN_Radford}). 


\subsection{Links to datasets}
\emph{Higgs} dataset can be downloaded from UCI Machine Learning Repository. The link is \url{https://archive.ics.uci.edu/ml/datasets/HIGGS}.

\emph{MNIST} dataset can be downloaded via Pytorch. See the code in \url{https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/dcgan/dcgan.py}.

\emph{CIFAR-10.1} is available from \url{https://github.com/modestyachts/CIFAR-10.1/tree/master/datasets} (we use \texttt{cifar10.1\_v4\_data.npy}). This new test set contains  images from TinyImages \citep{torralba2008tinyimages}.






































\subsection{Type I errors on \emph{Higgs} and \emph{MNIST}} \label{sec:typeI}

\begin{table}[t]
\centering
  \small
  \caption{Results on \emph{Higgs} (). We report average Type I error on \emph{Higgs} dataset when increasing number of samples (). Note that, in \emph{Higgs}, we have two types of Type I errors: 1) Type I error when two samples drawn from  (no Higgs bosons) and 2) Type I error when two samples drawn from  (having Higgs bosons). Type I reported here is the average value of 1) and 2). Since Type I error reported here is the average value of two average Type I errors, we do not report standard errors of the average Type I error in this table.} \label{tab:Higgs_RES2}
\begin{tabular}{c|cccccc}
\toprule
 & ME & SCF & C2ST-S & C2ST-L & MMD-O & MMD-D \\
\midrule
1000 & 0.048 & 0.040 & 0.043 & 0.048 & 0.059 & 0.037\\
2000 &  0.043 & 0.032 & 0.060 & 0.056 & 0.055 & 0.053 \\
3000 & 0.049 & 0.043 & 0.046 & 0.053 & 0.051 & 0.069 \\
5000 & 0.056 & 0.035 & 0.052 & 0.065 & 0.049 & 0.062 \\
8000 & 0.050 & 0.034 & 0.065 & 0.067 & 0.056 & 0.037  \\
10000 & 0.059 & 0.032 & 0.057 & 0.058 & 0.045 & 0.048 \\
\midrule
Avg. &  0.051 & 0.036 & 0.054 & 0.058 & 0.050 & 0.051  \\
\bottomrule
\end{tabular}

\end{table}

Table~\ref{tab:Higgs_RES2} shows average Type I error on \emph{Higgs} dataset when increasing number of samples (). Table~\ref{tab:MNIST_RES2} shows  average Type I error on real-\emph{MNIST} vs. real-\emph{MNIST} when increasing number of samples ().

\begin{table}[!t]
\centering
  \small
  \caption{Results on \emph{MNIST} given . We report  average Type I errorstandard errors on real-\emph{MNIST} vs. real-\emph{MNIST} when increasing number of samples ().} \label{tab:MNIST_RES2}
\begin{tabular}{c|cccccc}
\toprule
 & ME & SCF & C2ST-S & C2ST-L & MMD-O & MMD-D \\
\midrule
200 &\mnstd{0.076}{0.011} & \mnstd{0.075}{0.010} & \mnstd{0.035}{0.006} & \mnstd{0.045}{0.005} & \mnstd{0.068}{0.004} & \mnstd{0.056}{0.003}  \\
400 & \mnstd{0.062}{0.010} & \mnstd{0.056}{0.007} & \mnstd{0.044}{0.006} & \mnstd{0.040}{0.004} & \mnstd{0.053}{0.005} & \mnstd{0.056}{0.005} \\
600 & \mnstd{0.051}{0.003} & \mnstd{0.049}{0.009} & \mnstd{0.039}{0.005} & \mnstd{0.054}{0.007} & \mnstd{0.066}{0.008} & \mnstd{0.056}{0.008} \\
800 & \mnstd{0.054}{0.006} & \mnstd{0.046}{0.006} & \mnstd{0.043}{0.005} & \mnstd{0.042}{0.007} & \mnstd{0.051}{0.005} & \mnstd{0.054}{0.007} \\
1000 & \mnstd{0.047}{0.006} & \mnstd{0.045}{0.010} & \mnstd{0.038}{0.006} & \mnstd{0.046}{0.005} & \mnstd{0.041}{0.007} & \mnstd{0.062}{0.006} \\
\midrule
Avg. &0.058 & 0.054 & 0.040 & 0.045 & 0.056 & 0.057 \\
\bottomrule
\end{tabular}

\end{table}

\section{Interpretability on \emph{CIFAR-10} vs \emph{CIFAR-10.1}} \label{sec:cifar-interp}

In Section~\ref{sec:benchmark-exp}, we have shown that images in \emph{CIFAR-10} and \emph{CIFAR-10.1} are not from the same distribution. Thus, it is interesting to try to understand the major difference between the datasets. Mean Embedding tests \citep{Chwialkowski2015} compare the mean embeddings  and  at test locations , rather than through their overall norm.
The test statistic is

the asymptotic null distribution of  is ,
and the estimator is computable in linear time rather than 's quadratic time.

\Citet{Jitkrittum2017} jointly learn the parameters  and kernel parameters to optimize test power.
The best such test locations () for a Gaussian kernel (with learned bandwidth) are shown in \cref{fig:CIFAR10_interp_learnt_ME}.
We could also try optimizing a deep kernel \eqref{eq:deepkernel_simpleForm} and the test locations together;
this procedure, however, failed to find a useful test.
We can find a better test, though, with a two-stage scheme:
first, learn a deep kernel to maximize ,
then choose  to maximize  with that kernel fixed.
Results are shown in \cref{fig:CIFAR10_interp_learnt}.

Although these approaches give nontrivial test power,
it is hard to interpret either set of images,
as the test locations have moved far outside the set of natural images.
We can instead constrain ,
simply picking the single point from the dataset which maximizes 
(shown in \cref{fig:CIFAR10_interp_learnt_SL}).
This achieves similar test power,
but lets us see that the difference might lie in images with smaller objects of interest than the mean for \emph{CIFAR-10}.

\begin{figure}[!p]
    \begin{center}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_ME_0207_0.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_ME_0207_1.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_ME_0207_2.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_ME_0207_3.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_ME_0207_4.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_ME_0207_5.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_ME_0207_6.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_ME_0207_7.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_ME_0207_8.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_ME_0207_9.png}}
        \caption{The best test locations (learned by an ME test with ) from  experiments on \emph{CIFAR-10} vs \emph{CIFAR-10.1}. Average rejection rate is .}  \label{fig:CIFAR10_interp_learnt_ME}
    \end{center}
    \vspace{-0.5cm}
\end{figure}

\begin{figure}[!p]
    \begin{center}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_0.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_1.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_2.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_3.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_4.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_5.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_6.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_7.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_8.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_9.png}}
        \caption{The best test locations (learned by an ME test, , with a deep kernel optimized for an MMD test) from  experiments on \emph{CIFAR-10} vs \emph{CIFAR-10.1}. Average rejection rate is .}  \label{fig:CIFAR10_interp_learnt}
    \end{center}
    \vspace{-0.5cm}
\end{figure}

\begin{figure}[!p]
    \begin{center}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_SL_0.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_SL_1.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_SL_2.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_SL_3.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_SL_4.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_SL_5.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_SL_6.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_SL_7.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_SL_8.png}}
        \subfigure
        {\includegraphics[width=0.17\textwidth]{fig/T_locs_CIFAR10_0207_SL_9.png}}
        \caption{The best test locations (selected among existing images with our learned deep kernel, ) from  experiments on \emph{CIFAR-10} vs \emph{CIFAR-10.1}. Average rejection rate is .}  \label{fig:CIFAR10_interp_learnt_SL}
    \end{center}
    \vspace{-0.5cm}
\end{figure}




\end{document}
