\section{Experiments}
\subsection{Datasets}

We use ChFinAnn \cite{zheng-etal-2019-doc2edag} and DuEE-fin \cite{duee-fin} datasets to make fair comparisons across all methods.

\noindent (1) \textbf{ChFinAnn} is by far the largest DEE dataset constructed by distantly supervised alignment \textbf{without trigger annotations}, which is widely used in the previous studies.
This dataset contains 32k financial announcements and 48k event records, in which 29\% documents have more than one record and 98\% of records have arguments scattered across different sentences.
On average a document contains 20 sentences and the longest document contains 6.2k Chinese characters.
(2) \textbf{DuEE-fin} is another dataset for DEE \textbf{with trigger annotations}.
It contains 13 event types and 11.7k documents, where the test set is evaluated online.
Each record in DuEE-fin has a labeled trigger word without specific positions, and 36\% of records share the same trigger in a document.

\subsection{Experiment Settings}
We choose the hyper-parameters of our system according to the performance on the development set in ChFinAnn.
In PTPCG, we use 2 layers of shared BiLSTM for event detection and entity extraction, and another 2 layers of BiLSTM for entity encoding.
We use the same vocabulary as \cite{zheng-etal-2019-doc2edag} and randomly initialize all the embeddings where $d_h$=768 and $d_l$=32.
Adam \cite{adam} optimizer is used with a learning rate of 5e-4 and the mini-batch size is 64.
The weights in Equation~\ref{eqn:total_loss} are 0.05, 1.0, 1.0, and 1.0, and $\gamma$ in Equation~\ref{eqn:adjacent} is 0.5.
Following the setting in \citet{zheng-etal-2019-doc2edag}, we train our models for 100 epochs and select the checkpoint with the best F1 score on the development set to evaluate on the test set.

\begin{table*}[tb]
    \centering
    \small
    \begin{tabular}{lcrcccccccccccc}
        \toprule
        \multirow{2}{*}{Model} & \#Params & GPU & \multicolumn{3}{c}{ChFinAnn-Single} & \multicolumn{3}{c}{ChFinAnn-All} & \multicolumn{3}{c}{DuEE-fin w/o Tgg} & \multicolumn{3}{c}{DuEE-fin w/ Tgg}\\
        \cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}\cmidrule(lr){13-15}
        &(w/o Emb) & Hours & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 \\
        \midrule
        DCFEE-O$^{*}$ & 32M (16M) & 192.0 & 73.2 & 71.6 & 72.4 & 69.7 & 57.8 & 63.2 & 56.2 & 48.2 & 51.9 & 51.9 & 49.6 & 50.7 \\
        DCFEE-M$^{*}$ & 32M (16M) & 192.0 & 64.9 & 71.7 & 68.1 & 60.1 & 61.3 & 60.7 & 38.7 & 52.3 & 44.5 & 37.3 & 48.6 & 42.2 \\
        GreedyDec$^{*}$ & 64M (48M) & 604.8 & 83.9 & 77.3 & 80.4 & 81.9 & 51.2 & 63.0 & 59.6 & 41.8 & 49.1 & 59.0 & 42.1 & 49.2 \\
        Doc2EDAG$^{*}$ & 64M (48M) & 604.8 & 83.2 & 89.3 & 86.2 & 81.1 & 77.0 & 79.0 & 66.7 & 50.0 & 57.2 & 67.1 & 51.3 & \textbf{58.1} \\
        GIT$^{*}$ & 97M (81M) & 633.6 & 85.0 & 88.7 & 86.8 & 82.4 & \textbf{77.6} & \textbf{79.9} & \textbf{68.2} & 43.4 & 53.1 & \textbf{70.3} & 46.0 & 55.6 \\
        \midrule
        PTPCG$_{|\mathcal{R}|=1}$ & 32M (16M) & \textbf{24.0} & \textbf{86.3} & \textbf{90.1} & \textbf{88.2} & \textbf{83.7} & 75.4 & 79.4 & 64.5 & \textbf{56.6} & \textbf{60.3} & 63.6 & \textbf{53.4} & \textbf{58.1} \\
        \bottomrule
    \end{tabular}
    \caption{
        Main results.
        \#Params is estimated on the ChFinAnn dataset.
        w/o Emb means the number of parameters without vocabulary embeddings.
        All models are trained with 100 epochs.
        -Single denotes the evaluation results on documents with only one event record.
        Tgg denotes the manually annotated triggers.
        $^{*}$We reproduce the results using their open-source codes.
    }
    \label{tab:exp_results_comparison}
\end{table*}

\subsection{Baselines and Metrics}

\paragraph{Baselines.}
(1) \textbf{DCFEE} \cite{yang-etal-2018-dcfee} has two variants here: DCFEE-O extracts only one record from one document while DCFEE-M extracts records as much as possible.
(2) \textbf{Doc2EDAG} \cite{zheng-etal-2019-doc2edag} constructs records as argument chains (DAG) and uses an auto-regressive way to extract final results.
(3) \textbf{GreedyDec} is a baseline from \citet{zheng-etal-2019-doc2edag} which fills one event table greedily.
(4) \textbf{GIT} \cite{xu-etal-2021-git} is another variant of Doc2EDAG and utilizes a graph neural network to further encode entities and add more features in DAG generation.

\paragraph{Metrics.}
We use the same evaluation setting as in the previous studies \cite{zheng-etal-2019-doc2edag,xu-etal-2021-git}.
For each predicted record, a golden record is selected without replacement by matching the record that has the same event type and the most shared arguments, and F1 scores are calculated by comparing arguments.
DuEE-fin uses an online evaluation fashion and the platform only provides micro-averaged scores.


\subsection{Main Results}

As shown in Table~\ref{tab:exp_results_comparison}, PTPCG achieves better or competitive results than the previous systems.
On the ChFinAnn dataset, our PTPCG achieves the best scores evluated on ChFinAnn-Single.
As for ChFinAnn-All, our model outperforms Doc2EDAG and is very close to GIT.
On the DuEE-fin dataset, PTPCG achieves the best F1 scores whether or not trigger words are provided, demonstrating good compatibility and universality.
Our PTPCG outperforms GIT and improves the overall F1 with 7.2\% w/o manually annotated triggers.
For DuEE-fin w/ Tgg, we find our approach declines from 60.3\% to 58.1\%.
After anaylzing the dataset, we find that the manually annotated triggers have less importance scores (62.9) than our pseudo triggers (83.8, $|\mathcal{R}|$=1).
Although adding annotated triggers when $|\mathcal{R}|$=1 can boost the importance score to 93.7, it is a trade-off between the number of triggers (annotated \& pseudo) and the adjacent matrix prediction.
We will discuss more in Section~\ref{sec:err_analyze}.

\subsection{Comparison on Model Parameters and Speed}
\paragraph{Comparison on the Number of Parameters.}
As shown in Table~\ref{tab:exp_results_comparison}, among all the models, PTPCG is the most lightweight one and is in the same scale with DCFEE,
while PTPCG outperforms DCFEE-O with 16.2\% absolute gain in F1 scores on ChFinAnn-All.
Without considering 16M vocabulary embedding parameters, PTPCG takes only 19.8\% parameters of GIT and reaches competitive results.

\paragraph{Comparison on Speed.}
Benefiting from the lightweight architecture design and non-autoregressive decoding style, our PTPCG is fast in both training and inference.

Comparing the training time as shown in Table~\ref{tab:exp_results_comparison}, it takes 633.6 GPU hours to train GIT, while PTPCG is 26.4 times faster and takes only 24.0 GPU hours.
These facts indicate that PTPCG is more efficient in training and requires much lower computation resource cost than other methods, and the whole training process has been reduced from almost a week with 4 NVIDIA V100 GPUs to just one day with 1 GPU.

According to the inference speed test in Figure~\ref{fig:inference_speed}, PTPCG is more scalable than other models.
With the growth of batch size, PTPCG becomes faster and finally stabilized near 125 documents per second, while Doc2EDAG and GIT are barely growing with batch size, peaking at 19 and 15 docs/s respectively.
PTPCG is up to 7.0 and 8.5 times faster compared to Doc2EDAG and GIT.
As an enhanced version of Doc2EDAG, GIT is 21.2\% slower than Doc2EDAG on average, and raises OOM error on a 32GB memory GPU when batch size is 128.
To further check the effect of different encoders, we substitute all BiLSTM layers into transformer encoders and create the TransPTPCG model with the same scale to Doc2EDAG parameters.
The results in Figure~\ref{fig:inference_speed} show that TransPTPCG is up to 2.5 times faster than Doc2EDAG, validating the advantage of non-autoregressive combination decoding.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/inference_speed_merged_w_deppn.pdf}
    \caption{Inference speed comparison with baselines (left) and with different $|\mathcal{R}|$ (right) with 1 NVIDIA V100 GPU for all models.}
    \label{fig:inference_speed}
\end{figure}


\subsection{Pseudo Triggers as Supplements}

PTPCG is capable of handling with or without manually annotated triggers, and the automatically selected pseudo triggers can be supplements to enhance the performance.

As shown in Table~\ref{tab:dueefin_comparison}, we find the pseudo triggers could assist the trigger words to boost the importance score from 62.9 to 93.7, and the results show that this helps identify combinations and bring 0.7\% improvement in offline dev evaluation and 0.8\% improvement in the online test set.


\begin{table}[b]
    \centering
    \small
    \begin{tabular}{cccccccc}
        \toprule
        Pseudo & \multirow{2}{*}{Impt.} & \multicolumn{3}{c}{Dev} & \multicolumn{3}{c}{Online Test} \\
        \cmidrule(lr){3-5}\cmidrule(lr){6-8}
        Trigger&& P & R & F1 & P & R & F1 \\
        \midrule
$\times$ & 62.9 & \textbf{76.5} & 56.8 & 65.2 & \textbf{69.8} & 48.5 & 57.3 \\
        \checkmark & \textbf{93.7} & 70.5 & \textbf{62.3} & \textbf{66.2} & 63.6 & \textbf{53.4} & \textbf{58.1} \\
        \bottomrule
    \end{tabular}
    \caption{PTPCG results on DuEE-fin w/ annotated triggers. Impt denotes the importance score.}
    \label{tab:dueefin_comparison}
\end{table}


To further validate the effectiveness of importance scores for pseudo trigger selection, we select groups of pseudo triggers with the middle and the lowest importance scores instead of the highest ones and analyze the results in Table~\ref{tab:importance_validation}.
The results show that there is a positive correlation between the importance and overall scores.
Nevertheless, the highest importance score (88.3\%) is not equal to 100.0\%, which may limit the upper bound of decoding.
We will explain more about error analysis in Section~\ref{sec:err_analyze}.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{cccccccc}
        \toprule
        \multirow{2}{*}{Impt.} & \multicolumn{3}{c}{ChFinAnn} & \multirow{2}{*}{Impt.} & \multicolumn{3}{c}{DuEE-fin w/o Tgg}\\
        \cmidrule(lr){2-4}\cmidrule(lr){6-8}
        & P & R & F1 & & P & R & F1 \\
        \cmidrule(lr){1-4}\cmidrule(lr){5-8}
        88.3 & 83.7 & \textbf{75.4} & \textbf{79.4} & 83.8 & 66.7 & \textbf{54.6} & \textbf{60.0} \\
        62.5 & 88.7 & 60.9 & 72.2 & 37.0 & 65.5 & 49.6 & 56.4 \\
        22.4 & \textbf{90.5} & 59.5 & 71.8 & 15.0 & \textbf{74.2} & 45.3 & 56.3 \\
        \bottomrule
    \end{tabular}
    \caption{PTPCG results with different importance scores.}
    \label{tab:importance_validation}
\end{table}


\subsection{Error Analysis}\label{sec:err_analyze}

Although pruned complete graph structure is efficient for training and inference, it is not perfect in similarity calculation (adjacent matrix prediction) and combination decoding.
Here we analyze the upper bounds of the combination decoding algorithm and discuss the future directions.

Results in Table~\ref{tab:upper_bound} show that models with the lower number of pseudo triggers are better.
However, models with more pseudo triggers have higher upper bounds for decoding and have higher importance scores.
Why models with more pseudo triggers have greater importance but still result in lower performance?
Results in Table~\ref{tab:importance_validation} and \ref{tab:upper_bound} may answer this question.
Models with the same $|\mathcal{R}|$ has a strong correlation that higher importance brings higher metric scores which validates the effectiveness of the pseudo trigger selection strategy based on importance scores.
However, more pseudo triggers bring more links to connect, and the model may be not robust enough for predicting each connection correctly, leading to an adjacent accuracy decline.

Overall, it is a trade-off that more pseudo triggers improves the upper bounds and reduces combination error rates, but also brings new challenges to recover connections between entities.
We believe it is the future direction to improve the similarity calculation and adjacent matrix prediction.

\begin{table}[bth]
    \centering
    \small
    \begin{tabular}{crrrrrrr}
        \toprule
        $|\mathcal{R}|$ & Impt. & SE & ME & TotE & \#links & \makecell{Adj\\Acc.} & F1 \\
        \midrule
        1 & 88.3 & 5.0 & 37.5 & 14.6 & 10,502 & 65.8 & 79.4 \\
        2 & 95.7 & 1.0 & 20.4 & 6.7 & 23,847 & 59.1 & 77.7 \\
        3 & 97.2 & 0.9 & 18.0 & 5.9 & 55,961 & 56.7 & 74.9 \\
        4 & 97.6 & 0.5 & 16.9 & 5.3 & 75,334 & 58.2 & 74.0 \\
        5 & 97.8 & 0.4 & 13.9 & 4.4 & 88,752 & 59.5 & 73.1 \\
        all & 97.8 & 0.2 & 13.4 & 4.1 & 140,989 & 60.1 & 69.5 \\
        \bottomrule
    \end{tabular}
    \caption{
        Analysis of combination decoding errors on ChFinAnn.
        SE, ME, TotE are error upper bounds of single-record documents, multiple-record documents, and overall error respectively.
        \#links is the number of connected links among all graphs in the test set.
        Adj Acc is the accuracy of adjacent matrix prediction given golden entities.
        F1 is micro-averaged.
    }
    \label{tab:upper_bound}
\end{table}
