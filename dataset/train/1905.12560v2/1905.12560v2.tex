\pdfoutput=1

\documentclass{article}

\usepackage[final, nonatbib]{neurips_2019}
\usepackage[square, sort, comma, numbers]{natbib}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{ upgreek }
\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{neurips_2019}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{xcolor}

\usepackage{tikz-cd}
\usetikzlibrary{cd}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{notation}{Notation}

\def\Gfun{\mathcal{G}}
\def\Qfun{\mathcal{Q}}

\definecolor{mydarkblue}{rgb}{0,0.08,0.55}
\definecolor{mypurple}{rgb}{0.5,0,0.15}
\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    linkcolor = blue,
    urlcolor  = black,
    citecolor = teal,
    anchorcolor = blue
}


\title{On the Equivalence between Graph Isomorphism Testing and Function Approximation with GNNs}

\author{Zhengdao Chen \\
  Courant Institute of Mathematical Sciences\\
  New York University\\
  \texttt{zc1216@nyu.edu} \\
\And
     Soledad Villar \\
  Courant Institute of Mathematical Sciences\\
  Center for Data Science \\
  New York University\\
  \texttt{soledad.villar@nyu.edu} \\ 
   \AND
     Lei Chen \\
  Courant Institute of Mathematical Sciences\\
  New York University\\
  \texttt{lc3909@nyu.edu} \\
   \And
    Joan Bruna \\
  Courant Institute of Mathematical Sciences\\
  Center for Data Science \\
  New York University\\
  \texttt{bruna@cims.nyu.edu} \\ 
}

\begin{document}

\maketitle


\begin{abstract}
Graph Neural Networks (GNNs) have achieved much success on graph-structured data. In light of this, there have been increasing interests in studying their expressive power. One line of work studies the capability of GNNs to approximate permutation-invariant functions on graphs, and another focuses on the their power as tests for graph isomorphism. Our work connects these two perspectives and proves their equivalence. We further develop a framework of the expressive power of GNNs that incorporates both of these viewpoints using the language of sigma-algebra, through which we compare the expressive power of different types of GNNs together with other graph isomorphism tests. In particular, we prove that the second-order Invariant Graph Network fails to distinguish non-isomorphic regular graphs with the same degree. Then, we extend it to a new architecture, Ring-GNN, which succeeds in distinguishing these graphs and achieves good performances on real-world datasets.
\end{abstract}


\section{Introduction}
Graph structured data naturally occur in many areas of knowledge, including computational biology, chemistry and social sciences. Graph Neural Networks, in all their forms, yield useful representations of graph data partly because they take into consideration the intrinsic symmetries of graphs, such as invariance and equivariance with respect to a reordering of the nodes \cite{scarselli2008graph, duvenaud2015convolutional, kipf2016semi, gilmer2017neural, hamilton2017inductive, velickovic2017graph, bronstein2017geometric, you2019pgnn}.
All these different architectures are proposed with different purposes (see \cite{wu2019comprehensive} for a survey and the references therein), and a priori it is not obvious how to compare their power. Recent works \cite{xu2018powerful, morris2019higher} proposed to study the representation power of GNNs via their performance on graph isomorphism tests. They proposed GNN models that are as powerful as the one-dimensional Weisfeiler-Lehman (-WL) test for graph isomorphism \cite{weisfeiler1968reduction} and showed that no GNN based on neighborhood aggregation can be more powerful than the 1-WL test. 

Meanwhile, for feed-forward neural networks, many positive results have been obtained regarding their ability to approximate continuous functions, including the seminal results of the the universal approximation theorems \cite{cybenko1989approximation, hornik1991hornik}. Following this line of work, it is natural to study the expressive power of GNNs also in terms of function approximation, especially whether certain families of GNN can achieve universal approximation of continuous functions on graphs that are invariant to node permutations. 
Recent work \cite{maron2019universality} showed the universal approximation power of Invariant Graph Networks (IGNs), constructed based on the invariant and equivariant linear layers studied in \cite{maron2018invariant}, if the order of the tensor involved in the models are allowed to grow as the graph gets larger. 
However, these models are are not quite feasible in practice when the tensor order is high.

The first part of this work aims at building a bridge between graph isomorphism testing and invariant function approximation, the two main perspectives for studying the expressive power of GNNs. We demonstrate an equivalence between the the ability of a class of GNNs to distinguish between any pairs of non-isomorphic graph and its ability to approximate any (continuous) invariant functions on graphs. Furthermore, we show that it is natural to characterize the expressive power of function families on graphs via the sigma-algebras they generate on the graph space, allowing us to build a taxonomy of GNNs based on the inclusion relationships among the respective sigma-algebras.

Building on this theoretical framework, we identify an opportunity to increase the expressive power of the second-order Invariant Graph Network (-IGN) in a tractable way by considering a ring of invariant matrices under addition and multiplication. We show that the resulting model, which we refer to as \emph{Ring-GNN}, is able to distinguish between non-isomorphic regular graphs where -IGN provably fails. We illustrate these gains numerically in prediction tasks on synthetic and real graphs. 

Summary of main contributions:
\begin{itemize}
    \item We show the equivalence between testing for graph isomorphism and approximating of permutation-invariant functions on graphs as perspectives for characterizing the expressive power of GNNs.
    \item We further show that the expressive power of GNNs can be described by the sigma-algebra that they induce on the graph space, which unifies the two perspectives above and enables us to compare the expressive power of different GNNs variants.
    \item We propose Ring-GNN, a tractable extension of -IGN that explores the ring of matrix addition and multiplication, which is more expressive than -IGN and achieves good performances on practical tasks.
\end{itemize}

\section{Related Work}
\paragraph{Graph Neural Networks (GNNs) and graph isomorphism.} Graph isomorphism is a fundamental problem in theoretical computer science. 
It can be solved in quasi-polynomial-time \cite{babai2016graph}, but currently there is no known polynomial-time algorithm for solving it. For each positive integer , the -dimensional Weisfeiler-Lehman test (-WL) is an iterative algorithm for determining if two graphs are isomorphic \cite{weisfeiler1968reduction}. -WL is known to succeed on almost all pairs of non-isomorphic graphs \cite{babai1980random}, and the power of -WL further increases as  grows. The -WL test has inspired the design of several GNN models \cite{hamilton2017inductive, zhang2017wlnm}.
Recently,  
\cite{xu2018powerful, morris2019higher} introduced graph isomorphism tests as a characterization of the power of GNNs and showed that if a GNN follows a neighborhood aggregation scheme, then it cannot distinguish pairs of non-isomorphic graphs that the -WL test fails to distinguish. They also proposed particular GNN architectures that exactly achieves the power of the -WL test by using multi-layer perceptrons (MLPs) to approximate injective set functions. 
A concurrent work \cite{maron2019provably} proves that the th-order Invariant Graph Networks (IGNs) are at least as powerful as the -WL tests, and similarly to our proposal, they augment the -IGN model with matrix multiplication and show that the new model is at least as expressive as the -WL test. \cite{murphy2019relational} proposed relational pooling (RP), an approach that combines \textit{permutation-sensitive} functions under all permutations to obtain a permutation-invariant function. If RP is combined with permutation-sensitive functions that are sufficiently expressive, then it can be shown to be a universal approximator. 
A drawback of RP is that its exact version is intractable computationally, and therefore it needs to be approximated by averaging over randomly sampled permutations, in which case the resulting functions is not guaranteed to be permutation-invariant. 

\paragraph{Universal approximation of functions with symmetry.} Many works have discussed the function approximation capabilities of neural networks that satisfy certain symmetries.
\cite{bloemreddy2019probabilistic} studied the probabilistic and functional symmetry in neural networks, and we discuss its relationship to our work in more detail in Appendix \ref{app.probabilistic}. \cite{ravanbakhsh2017sharing} showed that the equivariance of a neural network corresponds to symmetries in its parameter-sharing scheme. \cite{yarotsky2018universal} proposed a neural network architecture with polynomial layers that is able to achieve universal approximation of invariant or equivariant functions.  
\cite{maron2018invariant} studied the spaces of all invariant and equivariant linear functions, and obtained bases for such spaces. Building upon this work, \cite{maron2019universality} proposed the -invariant networks for parameterizing functions invariant to a symmetry group  on general domains. When we focus the graph domain with the natural symmetry group of node permutations, the model is also known as the -IGNs, where  represents the maximal order of the tensors involved in the model. It was shown in \cite{maron2019universality} that -IGN achieves universal approximation of permutation-invariant functions on graphs if  grows quadratically in the graph size, but such high-order tensors are prohibitive in practice. Upper bounds on the approximation power of the -invariant networks when the tensor order is limited remains an open problem except for when the symmetry group is  \cite{maron2019universality}. The concurrent work of \cite{keriven2019universal} provides an alternative proof and extends the result to the equivariant case, although it suffers from the same issue of possibly requiring high-order tensors. Specifically for learning in graphs, \cite{kondor2018covariant} proposed the compositional networks, which achieve equivariance and are inspired by the WL test. In the context of machine perception of visual scenes, \cite{herzig2018mapping} proposed an architecture that can potentially express all equivariant functions.

To the best our knowledge, this is the first work that shows an explicit connection between the two aforementioned perspectives of studying the representation power of GNNs - testing for graph isomorphism and approximating permutation-invariant functions on graphs. Our main theoretical contribution lies in showing an equivalence between them for under both finite and continuous feature spaces, with a natural generalization of the notion of graph isomorphism testing to the latter scenario. Then, we focus on the -IGN model and prove that it cannot distinguish between non-isomorphic regular graphs with equal degrees. Hence, as a corollary, this model is not universal. Note that our result shows an \emph{upper} bound on the expressive power of -IGN, whereas concurrently to us, \cite{maron2019provably} provides a \emph{lower} bound by relating -IGNs to -WL tests. In addition, similar to \cite{maron2019provably}, we also propose a modified version of -IGN to capture higher-order interactions among the nodes without computing tensors of higher-order.



\section{Graph Isomorphism Testing and Universal Approximation}
\label{sec:giso_ua}
\paragraph{Notations} Let  be a graph with  nodes, . The graph structure is characterized by the \emph{adjacency matrix}, , where  if and only if nodes  and  are connected by an edge. We allow additional information to be stored in the form of node and edge features, which we assume to belong to a compact set , and we define . With an abuse of notation, we also regard  as an  matrix that belongs to , where ,  is the feature of the th node, and ,  is the feature of the edge from node  node  (and equal to some predefined null value if  and  are not connected). For example, an undirected graph is represented by a symmetric matrix . If there are no node or edge features (in other words, all nodes / edges are intrinsically indistinguishable),  can be viewed as identical to . Thus, modulo the permutation of the node orders,  is the space of graphs with  nodes. For two graphs , we say they are \emph{isomorphic} (and write ) if  such that , where  denotes the set of all  permutation matrices. A function  on  is called \textit{permutation-invariant} if , , . 

A GNN with a graph-level scalar output represents a parameterized \emph{collection} of functions from  to , which are typically permutation-invariant by design.
Given such a collection, we will show a close connection between its ability to approximate 
 permutation-invariant functions on  and its power as graph isomorphism tests.  
First, we define these two properties precisely for any collection of permutation-invariant functions on , denoted by :
\begin{definition}
\label{pd}
We say  is \textbf{GIso-discriminating} if  such that ,   such that . This definition is illustrated by Figure~\ref{fig:giso}. 
\end{definition}
\begin{figure}
\label{fig:giso}
    \centering
    \includegraphics[width=0.3\textwidth,trim={6cm 3cm 14cm 4cm},clip]{separating_functions.pdf}
\caption{Illustrating the definition of GIso-discriminating.  and  are mutually non-isomorphic, and each of the big circles with dashed boundary represents an equivalence class under graph isomorphism.  is a permutation-invariant function that obtains different values on equivalence class of  and on that of , and similar . If the graph space has only these three equivalence classes of graphs, then  is GIso-discriminating.}
\end{figure}
\begin{definition}
\label{def:ua}
We say  is \textbf{universally approximating} if for all continuous permutation-invariant function , and ,  such that .
\end{definition}

\subsection{Finite feature space}
We first consider the simpler case where  is finite and show the equivalence between the two properties defined above.
\begin{theorem}
\label{UA2PD}
Suppose  is finite. If  is universally-approximating, then it is also GIso-discriminating.
\end{theorem}
\begin{proof}
As  is finite, we assume without loss of generality that it is a subset of . Consider any pair of non-isomorphic graphs . We define the \emph{-indicator} function, , by  if   and 0 otherwise, which is evidently permutation-invariant. If  is universally-approximating, then  can be approximated with precision  by some function . Then,  is a function that distinguishes  from .
\end{proof}

To obtain our result on the reverse direction, we introduce a concept of \emph{(NN-)augmented} collection of functions, which is especially natural when  itself is parameterized as neural networks.
It is defined to include any function that maps an input graph  to , where  is a feed-forward neural network (i.e. multi-layer perceptron) with  as the input dimension and ReLU as the activation function, and . When  is restricted to neural networks with at most  layers, we denote the augmented collection by . 

\begin{remark}
If  is the collection of feed-forward NNs with  layers, then  represents the collection of feed-forward NNs with  layers.
\end{remark}

\begin{remark}
If  is a collection of permutation-invariant functions, then so is .
\end{remark}

Then, we are able to state the following result, which is proved in Appendix~\ref{app:pf_thm2}.\footnote{A later work \cite{chen2020can} contains a simpler proof.} 
\begin{theorem}
\label{PD2UAfin}
Suppose  is finite. If  is GIso-discriminating, then  is universal approximating.
\end{theorem}

\subsection{Continuous feature space}
While graph isomorphism is inherently a discrete problem, 
the question of universal approximation is also interesting in cases where the input space is continuous.
We can naturally generalize the above results naturally to the scenarios of continuous input space under Definitions~\ref{pd} and \ref{def:ua}. 

\begin{theorem}\label{ua2pdinf}
Suppose  is a compact subset of . If  is universally approximating, then it is also GIso-discriminating.
\end{theorem}

\begin{theorem}
\label{thm:4}
Suppose  is a compact subset of . If  is a collection of continuous permutation-invariant functions on  that is is GIso-discriminating, then  is universally approximating.
\end{theorem}
These results are proved in Appendices~\ref{app:pf_thm3} and \ref{app:pf_thm4}.

\section{Characterizing Expressive Power through Sigma-Algebras}
\label{sec.sigma}
In this section, we assume for simplicity that  is a finite set, and hence so is . Given a graph , we use  to denote its \emph{isomorphism class}, defined as . Then, we let  denote the set of isomorphism classes in , that is, 
. The proofs of the theorems in this section are given in Appendix~\ref{sec.proofs.reformulating}.
\subsection{Defining sigma-algebras}

A maximally-expressive collection of permutation-invariant functions, , is one that allows us to identify exactly the isomorphism class  that any given graph  belongs to through the outputs of the functions  applied to . 
In other words, each function on  can be viewed as a``measurement'' that partitions the space  into different subsets, where each subset contains all graphs in  on which the function outputs the same value. Then, heuristically speaking,  being maximally expressive means that, collectively, the functions in  partition  into exactly .

This intuition can be formalized via the language of sigma-algebra. Recall that an \emph{algebra} on the set  is a collection of subsets of  that includes  itself, is closed under complement, and is closed under finite union. In our case, as  is assumed to be finite, it holds that an algebra on  is also a \emph{sigma-algebra} on , where a sigma-algebra further satisfies the condition of being closed under countable unions. If  is a collection of subsets of , we let  denote the sigma-algebra \emph{generated} by , defined as the smallest algebra that contains . Then, given a function  on , we also let  denote the sigma-algebra generated by , defined as the smallest sigma-algebra that contains all the pre-images under  of the (Borel) sigma-algebra on . For more background on sigma-algebras and measurability, we refer the interested readers to standard texts on measure theory, such as \cite{bartle2014elements}.
Then, the following observation is straightforward:
\begin{proposition}
\label{prop:measurable}
A function  on  is permutation-invariant if and only if  is measurable with respect to .
\end{proposition}

\subsection{Graph isomorphism testing, universal approximation, and sigma-algebra inclusions}
\label{sec.reformulating}
Let  be a class of permutation-invariant functions on . We further define the sigma-algebra generated by , denoted by , as the smallest sigma-algebra that includes  for all . Then, Proposition~\ref{prop:measurable} implies that . Furthermore, as we shall demonstrate below, the expressiveness of  is reflected by the fineness of , with a maximally-expressive  attaining :
\begin{theorem}\label{teo5}
Suppose  is finite. If  is GIso-discriminating, then .
\end{theorem}
Together with Theorem \ref{UA2PD}, the following is then an immediate consequence:
\begin{corollary}
Suppose  is finite. If  achieves universal approximation, then .
\end{corollary}
Conversely, we can also show that:
\begin{theorem} \label{teo6}
Suppose  is finite. If , then  is GIso-discriminating.
\end{theorem}


The framework of sigma-algebra not only allows us to reformulate the notions of being GIso-discriminating or universally-approximating, as shown above, but also allows us to compare the expressive power of different function families on graphs formally.
Given two classes of functions  and , we can formally define the statement `` has less expressive power than '' as equivalent to .
In Appendix \ref{app.comparison}, we use this notion to compare the expressive power of several different types of GNNs as well as other graph isomorphism tests like 1-WL and the linear programming relaxation, and the results are illustrated in Figure~\ref{fig.diagram}.


\begin{figure}[ht]
\label{diagram_main_text}
\small
\centering
\begin{tikzcd}
\text{sGNN}_1 \arrow[r,hook] & 1\text{-WL} \equiv \text{GIN} \equiv \text{LP} \arrow[d, hook] \arrow[dr, hook] &  \\
\text{adjacency spectrum} \arrow[dr,hook] & 2\text{-WL} \equiv 2\text{-IGN} \arrow[d,hook] & \text{SDP} \arrow[d,hook]\\
\text{sGNN}_J~ (J>1) \arrow[r, hook] & 3\text{-WL} \equiv \text{Ring-GNN / PPGN} \arrow[d, hook] &\text{SoS hierarchy} \\
& k\text{-WL}~ (k > 3) \equiv k\text{-GNN} \equiv k\text{-IGN} &
\end{tikzcd}
\caption{\small Comparison of function classes on graphs in terms of their expressive power under the sigma-algebra framework proposed in Section \ref{sec.sigma}, with details given in Appendix \ref{app.comparison}. For completeness, we have included relevant results that appeared later than the original publication of this work.}
\label{fig.diagram}
\end{figure}

\section{Ring-GNN: Exploring the Ring of Graph Operators with a GNN}
\subsection{Limitation of the -Invariant Graph Network (-IGN)}
We start by considering the \emph{Invariant Graph Networks} (IGNs, a.k.a. -invariant networks) proposed in \cite{maron2019universality}, which are designed by interleaving permutation-equivariant linear layers (between tensors of potentially different orders) and point-wise nonlinear activation functions.
We present its definition in Appendix~\ref{app.Ginvariant} for completeness. In particular, for a positive integer , an IGN model that involves tensors of order at most  is called a -IGN.
It is proved in \cite{maron2019universality} that -IGN can achieve universality on graphs of size  if  grows quadratically in , but less is known about its expressive power when  is restricted. Meanwhile, in practice, it is difficult to implement an -IGN when . Hence, our first goal is to examine its expressive power when .
The following result shows that -IGN is not universal\footnote{In fact, it has later been proved that -IGN is exactly as powerful as the -WL test \cite{chen2020can, geerts2020expressive, geerts2022expressiveness}.}. The proof is given in Appendix \ref{app.Ginvariant}. 
\begin{theorem} \label{prop.Ginvariant}
-IGN cannot distinguish among non-isomorphic regular graphs with the same degree.
\end{theorem}
For example, it cannot distinguish the pair of Circular Skip Link (CSL) graphs shown in Figure \ref{cslfig}.
\subsection{Ring-GNN as an extension of -IGN}
\label{sec:ringgnn}
Given this limitation, we propose a novel GNN architecture that extends the -IGN model without resorting to higher order tensors, which, specifically, are able to distinguish certain pairs of non-isomorphic regular graphs with the same degree. 

To gain intuition, we take 
the pair  and  illustrated in Figure \ref{cslfig}
as an example.
Since all nodes in both graphs have the same degree, we are unable to break the symmetry among the nodes by simply updating the node hidden states through either neighborhood aggregation (as in -WL and GIN) or through second-order permutation-equivariant linear layers (as in -IGN). Meanwhile, however, nodes in the \textit{power graphs}\footnote{If  is the adjacency matrix of a graph, its power graph has adjacency matrix . See Appendix~\ref{app:diagram}.} of  and  have different degrees.
This observation motivates us to consider an augmentation of the -IGN model that explores the polynomial \emph{ring} generated by the input matrix.
Then, together with point-wise nonlinear activation functions such as ReLU, power graph adjacency matrices like  can be expressed with suitable choices of model parameters. 

To define our model, we revisit the theory of linear equivariant functions developed in \cite{maron2018invariant}.
It is shown that any linear permutation-equivariant layer from  to  can be represented as , where  is the set of 15 basis functions
for the space of linear permutation-equivariant functions from  to ,  and  are the basis for the permutation-equivariant bias terms, and  are trainable parameters. Generalizing to maps from  to , the full permutation-equivariant linear layer can be defined as , with .

We now define a new architecture as follows. We let  denote the generic input. For example, we can set  and . We fix some integer  that denotes the number of layers, and for , we iteratively define

where  and  are trainable parameters, and  is the (entry-wise) ReLU function. 
If a graph-level scalar output is desired, then at the final layer, we compute the output as , where  are additional trainable parameters. Note that this final layer is invariant to node permutations, and hence the overall model is also permutation-invariant. We call the resulting architecture the \textit{Ring-GNN}.

Note that each layer is equivariant, and the map from  to the final scalar output is invariant. A Ring-GNN can reduce to a -IGN if  for each . With  layers and suitable choices of the parameters, it is possible to expressed  in the  layer. Therefore, we expect it to succeed in distinguishing certain pairs of regular graphs that the -IGN fail on, such as the CSL graphs. Indeed, this is verified in the synthetic experiment presented in the next section. In addition, the normalized Laplacian matrix  can also be approximated, since the degree matrix  can be obtained from  through a permutation-equivariant linear function, and then entry-wise inversion and square-root on the diagonal can be approximated by an MLP.

Computationally, the complexity of running the forward model grows as  as  increases, which is dominated by the matrix multiplication steps \cite{coppersmith1987arithmetic}.
In comparison, a -IGN model will have complexity . Therefore, the matrix multiplication steps enable the Ring-GNN to compute some higher-order interactions in the graph (which is neglected by -IGN) while remaining relatively tractable computationally.
We also note that Ring-GNN can be augmented with matrix inverses or, more generally, functions on the spectrum of any of the intermediate representations. \footnote{When  is the adjacency matrix of an undirected graph, one easily verifies that  contains only symmetric matrices for all .} 

\begin{figure}
    \label{cslfig}
    \centering
    \includegraphics[width=0.15\textwidth,trim={6cm 4.8cm 20cm 7.8cm},clip]{skl2black}
    \includegraphics[width=0.15\textwidth,trim={6cm 4.8cm 20cm 7.8cm},clip]{skl3black}
    \caption{The Circular Skip Link (CSL) graphs  are undirected graphs with  nodes such that nodes  and  are connected if and only if . In this figure, we depict (left)  and (right) . It is easy to check that  and  are not isomorphic unless  and . Both -WL and -IGN fail to distinguish them.}
    \label{fig.skiplength}
\end{figure}

\section{Experiments}
\label{experiments}
The different models and the detailed setup of the experiments are discussed in Appendix \ref{archi}. All experiments are conducted on GeForce GTX 1080 Ti and RTX 2080 Ti.\footnote{The code is available at  \url{https://github.com/leichen2018/Ring-GNN}.}

\subsection{Classifying Circular Skip Links (CSL) graphs}
\label{cslexp}
The following experiment on synthetic data demonstrates the connection between function fitting and graph isomorphism testing. The CSL graphs\footnote{Link: \url{https://github.com/PurdueMINDS/RelationalPooling/tree/master/}.} are undirected regular graphs with node degree 4 \cite{murphy2019relational}, as illustrated in Figure \ref{cslfig}. Note that two CSL graphs  and  are not isomorphic unless  and . In the experiment, which has the same setup as in \cite{murphy2019relational}, we fix  and set , and each  corresponds to a distinct isomorphism class. The task is then to classify a graph  by its skip length .

Note that since the 10 classes have the same size, a naive uniform classifier should obtain  accuracy. As we see from Table \ref{table.synthetic}, neither GIN and -IGN outperform the naive classifier. Their failure in this task is unsurprising: -WL is proved to fall short of distinguishing such pairs of non-isomorphic regular graphs \cite{cai1992optimal}, and hence neither can GIN \cite{xu2018powerful}; by Theorem~\ref{prop.Ginvariant}, -IGN is unable to distinguish them either. Therefore, their empirical failure in this classification task is consistent with their theoretical limitations as graph isomorphism tests (and can also be understood as approximating the function that maps the graphs to their class labels).

It should be noted that, since graph isomorphism tests are not entirely well-posed as classfication tasks, the performance of GNN models can vary due to randomness. But the fact that Ring-GNNs achieve a relatively high \emph{maximum} accuracy indicates that, as a class of functions on graphs, it is rich enough to contain a good proportion of functions that distinguish the CSL graphs. 

\begin{table}[ht]
\centering
\begin{tabular}{l|lll||ll|ll}
\hline
& \multicolumn{3}{|c||}{CSL} & \multicolumn{2}{c|}{IMDB-B} & \multicolumn{2}{c}{IMDB-M} \\
GNN architecture              & max  & min & std  & mean & std & mean & std \\
\hline \hline
RP-GIN               & 53.3 & 10  & 12.9 & -     & -     & -     & -     \\
GIN         & 10   & 10  & 0    & 75.1  & 5.1   & 52.3  & 2.8   \\
-IGN  & 10   & 10  & 0    & 71.3 & 4.5   & 48.6 & 3.9   \\
                        & 80   & 80  & 0    & 72.8  & 3.8   & 49.4  & 3.2   \\
                        & 30   & 30  & 0    & 73.1  & 5.2   & 49.0    & 2.1   \\
                        & 10   & 10  & 0    & 72.7  & 4.9   & 49.0    & 2.1   \\
LGNN \cite{chen2019cdsbm}                         & 30   & 30  & 0    & 74.1  & 4.6   & 50.9  & 3.0     \\
Ring-GNN                          & 80   & 10  & 15.7    & 73.0  & 5.4   & 48.2  & 2.7  \\
Ring-GNN (w/ degree)   &   -    &   -    &   -    &  73.3   & 4.9  & 51.3  & 4.2 \\
\hline
\end{tabular}
\vspace{5pt}
\caption{\textbf{(left)} Accuracy of different GNNs at classifying CSL graphs(see Section \ref{cslexp}). We report the best and worst performances among 10 experiments.
\textbf{(right)} Accuracy of different GNNs in classication tasks on the two IMDB datasets (see Section \ref{sec.imbdb}). We report the best performance among all 350 epochs on 10-fold cross-validation, following in \cite{xu2018powerful}. 
: Reported in \cite{murphy2019relational}, \cite{xu2018powerful} and \cite{maron2018invariant}. 
: On the IMDB datasets, both GIN and the Ring-GNN (w/ degree) take the node degrees as input node features (see Section \ref{sec.imbdb}). 
}
\label{table.synthetic}
\end{table}

\subsection{IMDB datasets} \label{sec.imbdb}
We use the two IMDB datasets (IMDB-B and IMDB-M)\footnote{Link: \url{https://github.com/weihua916/powerful-gnns/blob/master/dataset.zip}.} to test different models in real-world social networks. Since our focus is on distinguishing graph structures, these datasets are convenient as they do not contain node or edge features \cite{yanardag2015deep}.
IMDB-B has 1000 graphs, with 19.8 nodes per graph on average and 2 class labels. IMDB-M has 1500 graphs, with 13.0 nodes per graph on average and 3 class labels. 
Both datasets are randomly partitioned into training and validation sets with ratio . 
As these two datasets have no informative node features, GIN uses one-hot encoding of the node degrees as input node features, while the other baseline models treat all nodes as having identical features. For a fairer comparison, we apply two versions of Ring-GNN: the first one treats all nodes as having identical input features, denoted as \emph{Ring-GNN}; the second one uses the node degree as input features (though not via one-hot encoding, due to computational constraints, but simply as one integer per node), denoted as \emph{Ring-GNN w/ degree}. All models are evaluated via 10-fold cross validation and the best accuracy is calculated through averaging across folds and then maximizing along epochs, following \cite{xu2018powerful}. Table \ref{table.synthetic} shows that the Ring-GNN models achieve higher or similar performance compared to -IGN on both datasets, and slightly worse performance compared to GIN. 

\subsection{Other real-world datasets}
We perform further experiments on four other real-world datasets for classification tasks, including a social network dataset, COLLAB, and three bioinformatics datasets, MUTAG, PTC, PROTEINS\footnote{Same link as above.}~\cite{yanardag2015deep}. The experiment setup (10-fold cross validation, training/validation split) is identical to that of the IMDB datasets, except that all the bioinformatics datasets contain node features, and more details of hyperparameters are included in Appendix~\ref{archi}. As shown in Table \ref{tab.other}, Ring-GNN outperforms -IGN in all four datasets, and outperforms GIN in one out of the four datasets. 

\begin{table}[ht]
\centering
\begin{tabular}{l|l|l|l|l}
\hline
            & COLLAB & MUTAG & PTC   & PROTEINS \\ \hline
Ring-GNN     & 80.11.4   & 86.86.4  & 65.77.1  & 75.72.9     \\ \hline
GIN           & 80.21.9   & 89.45.6  & 64.67.0  & 76.22.8     \\ \hline
-IGN  & 77.9  1.7  & 84.610.0 & 59.57.3 & 75.24.3   \\ \hline
\end{tabular}
\vspace{5pt}
\caption{Accuracy of different GNNs evaluated on several other real-world datasets.  We report the best performance among all epochs on 10-fold cross-validation. : Reported in \cite{xu2018powerful} and \cite{maron2018invariant}. }
\label{tab.other}
\end{table}

\section{Conclusions}
In this work, we address the important question of organizing the fast-growing zoo 
of GNN architectures in terms of what functions they can and cannot represent. 
We follow the approach via graph isomorphism tests and show that is equivalent 
to the classical perspective of function approximation. 
We leverage the theoretical insights to augment the -IGN model
with the ring of operators associated with matrix multiplication, which gives provable gains in expressive power with tractable computational complexity and is amenable to further efficiency improvements by leveraging sparsity in the graphs. 

Our general framework leaves many interesting questions unresolved. First, our current GNN taxonomy is still incomplete. Second, we need a deeper analysis on which elements of the ring of operators created in Ring-GNN are really relevant for different types of applications.
Finally, beyond strict graph isomorphism, a natural next step is to consider weaker metrics in the space of graphs, such as the Gromov-Hausdorff distance,
which could better reflect the requirements on the stability of powerful graph representations to small graph perturbations in real-world applications \cite{gama2019stability}. 

\paragraph{Acknowledgements} We thank Haggai Maron and Thomas Kipf for fruitful discussions that pointed us towards Invariant Graph Networks as powerful models to study representational power in graphs. We thank Sean Disar\`{o} for pointing out errors in an earlier version of this paper.
We also thank Michael M. Bronstein for supporting this research with computing resources.
This work was partially supported by NSF grant RI-IIS 1816753, NSF CAREER CIF 1845360, the Alfred P. Sloan Fellowship, Samsung GRP and Samsung Electronics.
SV was partially funded by EOARD FA9550-18-1-7007 and the Simons Collaboration Algorithms and Geometry.

\newpage
\bibliographystyle{plain}
\bibliography{ref}





\newpage
\appendix
\section{Proofs for Section~\ref{sec:giso_ua}}
\label{app.universal}
\subsection{Proof of Theorem~\ref{PD2UAfin}}
\label{app:pf_thm2}
Theorem~\ref{PD2UAfin} is a consequence of the two following lemmas.
\begin{lemma}
\label{lemma1}
    If  is GIso-discriminating, then for all , there exists a function  such that for all  if and only if .
\end{lemma}

\begin{lemma}
\label{lemma2}
Let  be a class of permutation-invariant functions from  to  so that for all , there exists  satisfying  if and only if . 
Then   is universally approximating.
\end{lemma}

\subsubsection{Proof of Lemma \ref{lemma1}}
Given  with , we let  be a function that distinguishes this pair, i.e. . Then, we define a function  by .
We see that , if , then , and so ; if , then ; otherwise, .

Next, we define a function  by . We see that if , we have ; if , then  .

Thus, it is left to show that . Following the definition of the augmented collection, we choose  to be the cardinality of the set  and select the subset of  functions, , from . Since


we see that each  can be obtained from  by passing through one ReLU layer. Therefore, .
\qed

\subsubsection{Proof of Lemma~\ref{lemma2}}
In the setting of a finite feature space, we can in fact obtain a stronger result: for all  that is permutation-invariant, , which means no approximation is needed.

First, for every , we can use each  to construct the indicator function . To achieve this, as  is finite, we let . We then introduce a ``bump'' function,  with parameters  and , defined by , where . We see that  is nonnegative, , and  only on . Hence,  equals the indicator function  on .

Given any function  that is permutation-invariant, as the input space  is finite, we can decompose it as
.
Thus,  can be realized in , as each ``'' on the right-hand side is treated as a constant in the neural network.
\qed

\subsection{Proof of Theorem \ref{ua2pdinf}}
\label{app:pf_thm3}
, if , define a function . It is a continuous and permutation-invariant function on , and therefore can be approximated by a function  to within  accuracy. Then  is a function that can discriminate between  and . \qed

\subsection{Proof of Theorem~\ref{thm:4}}
\label{app:pf_thm4}
\begin{definition}
\label{locate}
Let  be a class of functions . We say it is able to \textbf{locate every isomorphism class} if for all  and for all  there exists  such that:
\begin{itemize}
    \item for all ;
    \item for all , if , then ; and
    \item there exists  such that for all , if , then  such that .
\end{itemize}
\end{definition}

Then, Theorem~\ref{thm:4} is a consequence of the two following lemmas.
\begin{lemma} \label{lemma.C+1}
Let  be a collection of continuous permutation-invariant functions from  to . If  is GIso-discriminating, then  is able to locate every isomorphism class.
\end{lemma}

\begin{lemma} \label{lemma.locate.approx}
Let  be a class of continuous permutation-invariant functions . 
If  is able to locate every isomorphism class, then  is universally approximating.
\end{lemma}


\subsubsection{Proof of Lemma \ref{lemma.C+1}}
Given  and , we will construct a function  as desired in Definition~\ref{locate}. Since  is pairwise distinguishing, we know that  such that ,  such that . For each such , we define a set  by

Clearly, for each ,  while  does not. In addition, since  is assumed to be continuous,  is an open set. Meanwhile, for any , we define , the open -ball centered at  under the Euclidean distance. Thus,  is an open cover of . Since  is compact,  a finite subset  of  such that  also covers . 
Hence,  such that . 

Next, we define a function  by setting , where 

Since each  in continuous,  is also continuous. Moreover, we can show that  satisfies the desired properties described in Definition \ref{locate}:
\begin{itemize}
    \item Each  is non-negative, and hence  is non-negative on .
    \item If , then since each  is permutation-invariant, there is , and hence , . Therefore, .
    \item Define .
    Suppose that . This means that . Since  is a cover for , we know that 
     such that , and thus . Therefore, 
    
    and hence .
    Hence, in other words, if  satisfies , then it must hold that , implying that  such that .
\end{itemize}
Finally, it is clear that that  can be represented in . \qed

\subsubsection{Proof of Lemma \ref{lemma.locate.approx}}
Consider any  that is continuous and permutation-invariant. Since  is compact,  is uniformly continuous on . This implies that  such that , if , then . Moreover, the permutation-invariance of  implies that  such that , if  such that , then .

Given any , since  is able to locate every isomorphism class, we can choose a function  as in Definition \ref{locate}. We use  to denote . Then  such that , where  is the ball in  centered at  with radius  in Euclidean distance. Since  is continuous,  is open. Therefore,  is an open cover of . Because  is compact,  a finite subset  such that  also covers .

For each , we construct another function  on  by defining, ,

It is clear that . Next, for each , we construct another function  on  by defining, ,

Since  covers , we know that ,   such that , which implies that , and therefore the denominator in \eqref{eq:psi_G0} is positive. Thus, for each ,  is a well-defined function on , and . Moreover, . In other words, the set of functions  is a \emph{partition of unity} on  with respect to the open cover .

This implies that , there is

Meanwhile, if , then  such that , which implies that . Hence, if we construct a function  on  by defining, ,

we see that 

because ,


Finally, we show how to approximate  with functions from  augmented with a feed-forward neural network. Given any input graph , applying each of  to  yields a vector of size . Moreover, we see from \eqref{eq:hatf} that  can be viewed as a continuous function of this vector. Hence, by the universal approximation theorem \citep{cybenko1989approximation, hornik1991hornik},  can be approximated to arbitrary accuracy by a feed-forward neural network applied to this vector that is sufficiently wide. \qed

\section{Proofs for Section \ref{sec.sigma}} \label{sec.proofs.reformulating}
\subsection{Proof of Theorem \ref{teo5}}
If  is GIso-discriminating, then given a ,  and  such that , which is a finite intersection  of sets in . Hence, . Therefore, , and hence . Moreover, since  for all , there is 
\qed

\subsection{Proof of Theorem \ref{teo6}}
Suppose not. This implies that , and hence  such that . Note that  is an equivalence class of graphs that are isomorphic to each other. Then consider the smallest subset in  that contains , defined as 

Since  is a finite space,  is also finite, and hence this is a finite intersection. Since a sigma-algebra is closed under finite intersection, there is . As , we know that . Then,  such that . Then there does not exist any function  in  such that , since otherwise the pre-image of some interval in  under  will intersect with only  but not . Contradiction.
\qed



\section{Comparison of the expressive power of function families on graphs via the sigma-algebras} \label{app.comparison}
\subsection{Comparing sigma-algebras}
Given two classes of functions , such as two classes of GNNs, there are four possibilities regarding their relative representation power, using the language of sigma-algebra developed in the main text:

\begin{itemize}
    \item Equivalent expressive power: ; 
    \item  is strictly more powerful: ;
    \item  is strictly more powerful: ;
    \item Not comparable:  and .
\end{itemize}

\subsection{Details of Figure~\ref{fig.diagram}}
\label{app:diagram}
In this section, we summarize some results from the literature (including relevant results known after the original publication of this work) that allow us to establish partial relationships among the expressive power of different GNNs architectures under the sigma-algebra framework introduced in Section \ref{sec.sigma} and above. For simplicity, here we assume that graphs are determined by the adjacency matrix  and no node or edge features are included. The results below are illustrated in Figure \ref{fig.diagram}.

\paragraph{Spectral GNNs (sGNNs)} Let  denote a set of  matrices that represent linear operators on graph signals. 
In a spectral GNN model with  layers, we set  and , and recursively compute

where  is a trainable parameter matrix at each layer . The model finally outputs the entry-wise sum of the vector .
This type of models was proposed to solve community detection tasks in graphs in \cite{chen2019cdsbm}, and has also been applied to solve the quadratic assignment problem \cite{nowak2017note}. 

In this context, for any positive integer , we define , where  is the identity matrix,  is the degree matrix of the graph,  is the adjacency matrix of the graph, and we define , which is the adjacency matrix of the th power graph of .
We then write  for the spectral GNN model equipped with .

The power graph adjacency matrices leverage multi-scale information in the graph, but note that they differ from simply taking the powers of the adjacency matrix, which is exploited in \cite{liao2018lanczosnet} for example. As mentioned in Section~\ref{sec:ringgnn}, the power graph adjacency matrices allow the model to distinguish regular graphs that cannot distinguished by the -WL test, such as the Circular Skip Link graphs. 

See a later work \cite{wang2022powerful} for further discussions on the expressive power of spectrally-designed GNNs.

\paragraph{k-Weisfeiler-Lehman (-WL)} For positive integers , -WL is an iterative algorithm for determining whether a pair of graphs is isomorphic \cite{weisfeiler1968reduction}. The higher  is, the more powerful -WL is in distinguishing non-isomorphic graphs. In particular, -WL is as powerful as -WL on graphs with no edge features, but -WL, unlike -WL, is able to take into account edge features. When , -WL has also been called the color refinement algorithm. We note that there is an alternative ``folklore'' definition of the WL algorithm, for which we refer the readers to \cite{cai1992optimal, maron2019provably}, for example.

\paragraph{Graph Isomorphism Network (GIN) and -dimensional GNN (-GNN)} GIN is a GNN model proposed in \cite{xu2018powerful}, where it is proved that GIN is as powerful as -WL in distinguishing non-isomorphic graphs. A similar model and its generalizations to -WL with  are proposed in \cite{morris2019higher}, called -GNN.

\paragraph{-Invariant Graph Networks (-IGNs)} Proposed in \cite{maron2018invariant, maron2019universality}. In later works, it is proved that -IGN is exactly as powerful as -WL for both  \cite{chen2020can} and  \cite{geerts2020expressive, geerts2022expressiveness}.

\paragraph{Provably Powerful Graph Network (PPGN)} The PPGN model is proposed in the concurrent work of \cite{maron2019provably}. It extends the -IGN model through matrix multiplication steps in a way that highly resembles Ring-GNN. In addition, it is proved in \cite{maron2019provably} that PPGN (and hence Ring-GNN) are as powerful as -WL.

\paragraph{Linear Programming (LP)} The graph isomorphism problem can be formulated through an optimization problem. Namely, if  are the adjacency matrices of two graphs of size , we consider the optimization problem

By the definition of graph isomorphism, we see that  and  are isomorphic if and only if the minimum value is zero.
Then, one may consider an \emph{LP relaxation} of the optimization problem above:

We say the two graphs are \emph{fractionally isomorphic} if .
Thus, the LP relaxation leads to a natural sigma algebra  
It has been shown that two graphs are fractionally isomorphic if and only if they cannot be distinguished by 1-WL \cite{tinhofer1986graph, tinhofer1991note, ramana1994fractional}. 

\paragraph{Semidefinite Programming (SDP)} 
The SDP relaxation of the quadratic assignment problem \cite{zhao1998semidefinite} is always in the feasible set of the LP, and therefore LP is less expressive than SDP.
\paragraph{Sum-of-Squares (SoS) hierarchy} One can consider the hierarchy of relaxations coming from sum-of-squares (SoS). In the context of graph isomorphism, it is known that graph isomorphism is a hard problem for this hierarchy \cite{o2014hardness}. In particular the Lasserre/SoS hierarchy requires  to solve graph isomorphism (in the same sense that -WL fails to solve graph isomorphism \cite{cai1992optimal}).

\paragraph{Spectrum()} If we consider the function that takes a graph and outputs the set of eigenvalues of its adjacency matrix, such a function is permutation invariant. 
On one hand, certain regular graphs can be distinguished by their adjacency spectra but not by -WL or -WL. On the other hands, there are non-isomorphic graphs that can be distinguished by -WL / -WL but share the same adjacency spectrum (e.g., Figure 2 of \cite{ramana1994fractional}). Meanwhile, -WL is known to be strictly more powerful than the adjacency spectrum \cite{furer2010combinatorial, rattan2023spectra}.

\section{Additional Discussions of Literature}
\label{app.probabilistic}
The article~\cite{bloemreddy2019probabilistic} provides a nice and general theoretical framework that establishes an equivalence between the functional and probabilistic perspectives to symmetry via noise outsourcing in both general and particular settings. Our framework belongs to the functional perspective to symmetry (in particular, -invariance), and an extension to the probabilistic perspective with ideas from \cite{bloemreddy2019probabilistic} would be quite interesting. The concept of orbits also applies in our setting, and the concept of maximal invariants is related to our definition of GIso-discriminating. However, a key distinction is that being a maximal invariant is a property of \emph{functions}, whereas we define GIso-discriminating to be a property of \textit{classes} of functions. Our definition is arguably more suitable for studying the representation power of different GNN architectures, and moreover makes it possible to relate graph isomorphism tests to function approximation. Furthermore, the theory developed in Section 4 allows us to rigorously compare the expressive power of classes of GNN functions when they are \emph{not} necessarily GIso-discriminating, which is another novel contribution.




\section{Theoretical Limitation of the -IGN model} \label{app.Ginvariant}
\label{order_2_G_inv}

In this section, we prove Theorem \ref{prop.Ginvariant}, which states that -IGNs cannot distinguish between non-isomorphic regular graphs with the same degree.

\subsection{Defining the -IGN model}
\label{app:2ign}
Here, we state our definition of -IGN based on the -invariant networks defined in \cite{maron2019universality}.

Suppose  is a tensor containing information of a graph , where each entry is associated with a -tuple of nodes. Then , we use  to denote the  tensor obtained from  by applying the permutation represented by  to the node set. For example, if  and  is a matrix containing edge features (a simple example being the adjacency matrix), then .

\begin{definition}
A function  is \textbf{invariant} if . A function  is \textbf{equivariant} if . Note that invariance is a special case of equivariance when .
\end{definition}

\begin{definition}
For a positive integer , a -IGN parameterizes a function  in the following way:

where each  is a linear equivariant layer from  to ,  is a pointwise activation function,  is an invariant layer from  to , and  is an MLP.
\end{definition}

We will use  to denote the output of the th layer, for , i.e., they are defined recursively by

where  is the input to the model.

\subsection{Proof of Theorem \ref{prop.Ginvariant}}

In the definition of -IGN in Appendix~\ref{app:2ign}, each  can be interpreted as the dimension of the hidden state at layer  attached to each pair of nodes. For simplicity of notations,  we assume in the following proof that  (in which case each  is essentially a matrix), but note that
the proof can be extended to the cases where  by adding more subscripts in the argument.

Let  and  be two unweighted regular graphs with the same degree , and let  and  denote their adjacency matrices (which coincide with the matrix representation of  and , respectively).
To prove Theorem \ref{prop.Ginvariant}, we show that a -IGN model is bound to return the same output when applied to  and . For the graph , we let  denote its edge set, and further define  and . Similarly, we define  and  for the graph .

\begin{lemma}
\label{lem.A_ind}
,  such that ,

\end{lemma}
This Lemma is proved in Appendix~\ref{lem.A_ind}.

Since  is an linear permutation-invariant function, we can write  for some  and . Thus, 

where we use the observation that  and . As a consequence, .  \qed

\subsection{Proof of Lemma~\ref{lem.A_ind}}
\label{app:pf_lem_A_ind}
If , we say  and  are equivalent as -tuples of nodes (or node pairs) if  a permutation  (that is,  is a bijective map from  to itself) such that  and . For any , we let  denote the equivalence class of -tuples containing . Similarly, we can define a notion of equivalence between -tuples of nodes and the corresponding equivalence classes denoted by .

We prove this lemma by induction. For ,  and . By the definition of the adjacency matrix,  if  and , and  otherwise. Similar is true for . Therefore, setting  and , it is straightforward to verify that \eqref{eq:At_ind} holds.

Next, we consider the inductive steps. Assume that \eqref{eq:hatf} is satisfied at layer . To simplify the notation, we will let  and  stand for  below. Thus, we want to show that if  is any linear permutation-equivariant layer, then  also satisfies the inductive hypothesis. 
Using equation 9(b) in \cite{maron2018invariant} and adopting the notations of  defined therein, we know that  can be written as follows: for any ,

where  is summed over all equivalence classes of -tuples of nodes,  is summed over all equivalence classes of -tuples of nodes, and we define

Thus, by the inductive hypothesis and the fact that  equals the disjoint of ,  and , there is 

where  is defined as the total number of distinct  that satisfies , and similarly for  and . 

Note that ,  and  depend \emph{not} on the graph structure but only on  and . In particular, for , there exist  such that we may write

for each . The functions  depend on  and  and can be computed in a combinatorial fashion, whose values are completely enumerated in 
Tables~\ref{table:mE}, \ref{table:mN} and \ref{table:mS}. An illustration of their meanings is given in Figure \ref{coloredreg}.
\begin{figure}
\label{coloredreg}
    \centering
    \includegraphics[width=0.25\textwidth,trim={6cm 4cm 20cm 7cm},clip]{skl2color2}
    \includegraphics[width=0.25\textwidth,trim={6cm 4cm 20cm 7cm},clip]{skl3color2}
    \caption{
    Illustrations of 

    on graphs  and . In either graph,  represents the total number of node pairs  such that  and  is equivalent to  as -tuples, and equals twice the total number of black edges (twice because if  satisfies the condition, then so does , even though they represent the same undirected edge); similarly, the total number of of red edges, , equals both  and ; the total number of green edges, also , equals both , .}
    \label{fig:my_label}
\end{figure}


Combining \eqref{eq:S} and \eqref{eq:m}, we derive that

Moreover, note that  depends only on the equivalence class to which  belongs, and hence we may write

for some  and . Hence, we can now write

With a similar argument, we can derive that 

Hence, we have shown that \eqref{eq:At_ind} holds at layer  with ,  and . \qed

\begin{table}[h]
\centering
\begin{tabular}{llll}
\multicolumn{1}{l|}{}        &  &  &  \\ \hline
\multicolumn{1}{l|}{(1, 2, 3, 4)} &       &         & 0             \\
\multicolumn{1}{l|}{(1, 1, 2, 3)} & 0             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 2, 3)} &            &              & 0             \\
\multicolumn{1}{l|}{(1, 2, 1, 3)} &            &              & 0             \\
\multicolumn{1}{l|}{(1, 2, 3, 2)} &            &              & 0             \\
\multicolumn{1}{l|}{(1, 2, 3, 1)} &            &              & 0             \\
\multicolumn{1}{l|}{(1, 1, 1, 2)} & 0             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 1, 2, 1)} & 0             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 1, 2)} & 1             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 2, 1)} & 1             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 3, 3)} & 0             & 0             &         \\
\multicolumn{1}{l|}{(1, 1, 2, 2)} & 0             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 2, 2)} & 0             & 0             &              \\
\multicolumn{1}{l|}{(1, 2, 1, 1)} & 0             & 0             &              \\
\multicolumn{1}{l|}{(1, 1, 1, 1)} & 0             & 0             & 0             \\ \hline
\multicolumn{1}{l|}{Total}                             &             &             &            
\end{tabular}
\caption{}
\label{table:mE}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{llll}
\multicolumn{1}{l|}{}        &  &  &  \\ \hline
\multicolumn{1}{l|}{(1, 2, 3, 4)} &       &         & 0             \\
\multicolumn{1}{l|}{(1, 1, 2, 3)} & 0             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 2, 3)} &            &             & 0             \\
\multicolumn{1}{l|}{(1, 2, 1, 3)} &            &              & 0             \\
\multicolumn{1}{l|}{(1, 2, 3, 2)} &            &              & 0             \\
\multicolumn{1}{l|}{(1, 2, 3, 1)} &            &              & 0             \\
\multicolumn{1}{l|}{(1, 1, 1, 2)} & 0             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 1, 2, 1)} & 0             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 1, 2)} & 0             & 1             & 0             \\
\multicolumn{1}{l|}{(1, 2, 2, 1)} & 0             & 1             & 0             \\
\multicolumn{1}{l|}{(1, 2, 3, 3)} & 0             & 0             &         \\
\multicolumn{1}{l|}{(1, 1, 2, 2)} & 0             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 2, 2)} & 0             & 0             &              \\
\multicolumn{1}{l|}{(1, 2, 1, 1)} & 0             & 0             &              \\
\multicolumn{1}{l|}{(1, 1, 1, 1)} & 0             & 0             & 0             \\ \hline
\multicolumn{1}{l|}{Total}                             &             &             &            
\end{tabular}
\caption{}
\label{table:mN}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{llll}
\multicolumn{1}{l|}{}        &  &  &  \\ \hline
\multicolumn{1}{l|}{(1, 2, 3, 4)} & 0           & 0        & 0             \\
\multicolumn{1}{l|}{(1, 1, 2, 3)} &              &              & 0             \\
\multicolumn{1}{l|}{(1, 2, 2, 3)} & 0           & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 1, 3)} & 0           & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 3, 2)} & 0           & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 3, 1)} & 0           & 0             & 0             \\
\multicolumn{1}{l|}{(1, 1, 1, 2)} & 1             & 1             & 0             \\
\multicolumn{1}{l|}{(1, 1, 2, 1)} & 1             & 1             & 0             \\
\multicolumn{1}{l|}{(1, 2, 1, 2)} & 0             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 2, 1)} & 0             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 3, 3)} & 0             & 0             & 0        \\
\multicolumn{1}{l|}{(1, 1, 2, 2)} & 0             & 0             &              \\
\multicolumn{1}{l|}{(1, 2, 2, 2)} & 0             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 2, 1, 1)} & 0             & 0             & 0             \\
\multicolumn{1}{l|}{(1, 1, 1, 1)} & 0             & 0             & 1             \\ \hline
\multicolumn{1}{l|}{Total}                             &             &             &           
\end{tabular}
\caption{}
\label{table:mS}
\end{table}

\newpage
\section{GNN Models in the Experiments}
\label{archi}
In section \ref{experiments}, we show experiments on synthetic and real datasets with several different GNN architectures. Here are some additional details of these models.
\begin{itemize}
\item : The spectral GNN models defined in Appendix~\ref{app:diagram}, where we choose  and .
The models have 5 layers and hidden layer dimension (i.e. ) 64. They are trained using the Adam~\cite{kingma2014adam} optimizer with learning rate 0.01.
\item \textbf{LGNN}: Line Graph Neural Networks proposed by \cite{chen2019cdsbm}. The model has 5 layers and hidden layer dimension 64. They are trained using the Adam optimizer with learning rate 0.01.
\item \textbf{GIN}: Graph Isomorphism Network by \cite{xu2018powerful}. We take their performance on the IMDB datasets reported in \cite{xu2018powerful}, and their performance on classifying the CSL graphs reported in \cite{murphy2019relational} .
\item \textbf{RP-GIN}: Graph Isomorphism Network combined with Relational Pooling by \cite{murphy2019relational}. We took the results reported in \cite{murphy2019relational} for the CSL graphs experiment.
\item \textbf{-IGN}: The model defined in Appendix based on \cite{maron2018invariant} and \cite{maron2019universality} and implemented in \url{https://github.com/Haggaim/InvariantGraphNetworks}.
\item \textbf{Ring-GNN}: The definition is given in the main text. For the experiments on the IMDB datasets, the \emph{Ring-GNN} model has the same depth and widths of hidden layers as the -IGN model adopted in \cite{maron2018invariant}. The \emph{Ring-GNN w/ degree} model has 2 layers with 64 hidden units in each, followed by a jump knowledge network~\cite{xu2018representation}, which is then followed by a fully-connected layer with 32 hidden units. Each  is initialized independently under , and each  is initialized independently under . They are trained using the Adam~\cite{kingma2014adam} optimizer with learning rate 0.00001 for 350 epochs. The initialization of  and the learning rate were manually tuned, following the observation that Ring-GNN reduces to -IGN when .
For the other real-world datasets, models are trained via Adam with learning rate of 0.001 for 350 epochs. The model has 1 layer for MUTAG, 2 layers for PROTEINS and PTC, and 3 layers for COLLAB. Each of these layers has 64 hidden units and is followed by a jump knowledge network~\cite{xu2018representation}, which is then followed by a fully-connected layer with 32 hidden units.  is initialized to be 1, and  is initialized with , where  is the average number of nodes per graph in each dataset.
\end{itemize}

For the experiments with CSL graphs, each model is trained and evaluated using 5-fold cross-validation. For Ring-GNN, we perform training plus cross-validation 20 times with different random seeds.


\end{document}