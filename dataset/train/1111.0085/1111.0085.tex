\nonstopmode
\documentclass[submission,copyright,creativecommons]{eptcs}
\providecommand{\event}{LFMTP'11} \usepackage{hyperref}
\usepackage{proof}
\usepackage[all]{xy}

\usepackage{xspace}
\usepackage{ifthen}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{stmaryrd}


\usepackage{proof}



\newcommand{\abbrev}[1]{#1} \newcommand{\cf}{\abbrev{cf.}\ }
\newcommand{\eg}{\abbrev{e.\,g.}}
\newcommand{\Eg}{\abbrev{E.\,g.}}
\newcommand{\ie}{\abbrev{i.\,e.}}
\newcommand{\Ie}{\abbrev{I.\,e.}}
\newcommand{\etal}{\abbrev{et.\,al.}}
\newcommand{\wwlog}{w.\,l.\,o.\,g.} \newcommand{\Wlog}{W.\,l.\,o.\,g.}
\newcommand{\wrt}{w.\,r.\,t.}

\newcommand{\para}[1]{\paragraph*{\it#1}}
\newcommand{\paradot}[1]{\para{#1.}}



\newtheorem{prop}{Proposition}
\newtheorem{defin}[prop]{Definition}
\newtheorem*{cor}{Corollary}


\newcommand{\oann}[1]{{}^{#1}\kern-0.15ex}
\newcommand{\ovar}{\mathord{\bullet}}


\newcommand{\comment}[1]{} \newcommand{\sspace}{\,}
\newcommand{\lspace}{\ \,}
\newcommand{\shspace}{\vspace{1ex}}

\newcommand{\type}{\mathsf{Type}}
\newcommand{\kind}{\mathsf{Kind}}
\newcommand{\sort}{\mathsf{Sort}}
\newcommand{\la}{\lambda}
\newcommand{\emptyVec}{[]}
\newcommand{\emptySet}{\emptyset}
\newcommand{\ve}[1]{[#1]}
\newcommand{\emphSec}[1]{#1}
\newcommand{\dom}[1]{\mathsf{dom}\left(#1\right)}
\newcommand{\length}[1]{\mbox{length}(#1)}

\newcommand{\ApA}[2]{#1 \sspace #2}
\newcommand{\LaA}[2]{\la {#1}. \sspace #2}
\newcommand{\LaAT}[3]{\la {#1}:{#2}. \sspace #3}
\newcommand{\PiA}[3]{\Pi \sspace #1:#2 . \sspace #3}

\newcommand{\ApO}[3]{#1 \sspace ^{#2} \sspace #3}
\newcommand{\oapp}[1]{\sspace ^{#1} \sspace}
\newcommand{\LaO}[2]{\la ^{#1} . \sspace #2}
\newcommand{\PiO}[3]{\Pi \sspace #1 \sspace ^{#2} \sspace #3}
\newcommand{\freevars}[1]{\mathsf{fV}(#1)}

\newcommand{\ClosO}[4]{\left(\la^{#1} . \sspace {#2} \right)^{#3}_{#4}}
\newcommand{\AbsO}[3]{\left( \la {#1}. \sspace {#2} \right)^{#3}}
\newcommand{\FunO}[2]{\Pi \sspace #1 \sspace #2}
\newcommand{\multiinsert}[3]{#1 ^ {\sspace #3 / #2}}

\newcommand{\Val}[3]{ (\la^{#1} . \sspace {#2} )^{#3}}

\newcommand{\transO}[3]{ {#1}^{\mathcal{T}\left({#2}, \sspace {#3}\right)}}
\newcommand{\upch}[1]{\Uparrow^{#1}}
\newcommand{\result}[3]{\left( \, {#1}, \sspace {#2}, \sspace {#3} \, \right)}

\newcommand{\ClosS}[3]{\left(\la {#1}. \sspace #2 \right)^{#3}}
\newcommand{\KS}[2]{\left(\la . \sspace #1 \right)^{#2}}
\newcommand{\FunS}[2]{\Pi \sspace #1 \sspace #2}
\newcommand{\AbsS}[3]{\left( \la^{#1}. \sspace {#2} \right)^{#3}}

\newcommand{\LaH}[1]{\la . \sspace #1}
\newcommand{\KH}[1]{\la^c . \sspace #1}
\newcommand{\ApH}[2]{#1 \sspace #2}
\newcommand{\PiH}[2]{\Pi \sspace #1 \sspace #2}

\newcommand{\LaVH}[1]{\LaH #1} \newcommand{\KVH}[1]{\KH #1} \newcommand{\FunH}[2]{\PiH #1 #2}

\newcommand{\hersub}[3]{{#1}\,[{#3}/{#2}]}
\newcommand{\lift}[2]{ #2 \uparrow^{#1}}

\newcommand{\be}{\beta}
\newcommand{\pa}[1]{\left( #1 \right)}
\newcommand{\sub}[2]{\left[ #1 / #2 \right]}
\newcommand{\subs}[4]{\left[ #1 / #2 , #3 / #4 \right]}
\newcommand{\multisubs}[4]{\left[ #1 / #2 , \ldots, #3 / #4 \right]}
\newcommand{\bere}{\longrightarrow_\be}
\newcommand{\re}{\longrightarrow}

\newcommand{\head}{\mathsf{Head}}

\newcommand{\ev}[2]{\llbracket{#1}\rrbracket_{#2}}
\newcommand{\ap}{\,@\,} 
\newcommand{\valsub}[2]{\llparenthesis {\,#1\,} \rrparenthesis^{#2}} 

\newcommand{\parseb}[4]{{#1 \lhd #2 \rhd  #3[#4]}}
\newcommand{\parse}[3]{{#1 \, {  \mathord{\lhd} {#2} \mathord{\rhd}  } \, #3}}
\newcommand{\parseFun}[1]{{\left(\mathord{\lhd} {#1} \mathord{\rhd}\right)}}
\newcommand{\parseFunE}{\mathop{\rhd}}
\newcommand{\print}[3]{{#1 \, {  \mathord{\rhd} {#2} \mathord{\lhd}  } \, #3 }}
\newcommand{\infru}[2]{{\infer{#2}{#1}}}
\newcommand{\infnamed}[3]{{\infer[#1]{#3}{#2}}}

\newcommand{\x}{X} \newcommand{\te}{T} \newcommand{\ot}{oT} \newcommand{\otw}{oT_\Gamma} \newcommand{\otc}{\overline{oT}} \newcommand{\va}{V} \newcommand{\xl}{\vec \x} \newcommand{\vl}{\vec \va} \newcommand{\gl}{\vec \Gamma} 

\newcommand{\pr}[2]{{ \prE   \left( #1 \ , \  #2 \right)}}
\newcommand{\prE}{{\lhd \hspace{-6pt} \lhd}}
\newcommand{\prVal}[1]{{\lhd \hspace{-2pt} \left( #1 \right)}}
\newcommand{\prValE}{{\mathord{\lhd}}}
\newcommand{\prTree}[1]{{\lhd \hspace{-8pt} \lhd \hspace{-8pt} \lhd \left( #1 \right)}}
\newcommand{\prTreeE}{{\lhd \hspace{-8pt} \lhd \hspace{-8pt} \lhd }}

\newcommand{\osr}{\rightarrow}
\newcommand{\tree}[2]{\langle \hspace{-2pt}\langle #1 \ , \  #2 \rangle \hspace{-2pt}\rangle}


\newcommand{\evTr}{\mathsf{evalTrees}}

\newcommand{\lab}[1]{\mbox{(#1)}}

\newcommand{\vstart}{\vec v_{start}}
\newcommand{\vrest}{\vec v_{rest}}



 \newcommand{\define}[1]{\mbox{\textbf{\textit{#1}}}}

\title{A Lambda Term Representation \\ Inspired by Linear Ordered Logic}
\author{Andreas Abel
\institute{
Theoretical Computer Science\\
Institut f\"ur Informatik\\
Ludwig-Maximilians-Universit\"at\\
M\"unchen, Germany}
\email{andreas.abel@ifi.lmu.de}
\and
Nicolai Kraus
\institute{
Functional Programming Laboratory\\
School of Computer Science\\
University of Nottingham\\
Nottingham, United Kingdom}
\email{ngk@cs.nott.ac.uk}
}
\def\titlerunning{A Lambda Term Representation Inspired by Linear Ordered Logic}
\def\authorrunning{Andreas Abel and Nicolai Kraus}
\begin{document}
\maketitle

\begin{abstract}
We introduce a new nameless representation of lambda terms inspired by
ordered logic.  At a lambda abstraction, number and relative
position of all occurrences of the bound variable are stored, and
application carries the additional information where to cut the
variable context into function and argument part.  This way, 
complete information about free variable occurrence is available at each
subterm without requiring a traversal, and environments can
be kept exact such that they only assign values to variables that
actually occur in the associated term.
Our approach avoids space leaks in interpreters that build 
function closures.  

In this article, we prove correctness of the new representation and
present an experimental evaluation of its performance in a proof
checker for the Edinburgh Logical Framework.
  
Keywords:
representation of binders,
explicit substitutions,
ordered contexts,
space leaks,
Logical Framework.

\end{abstract}

\section{Introduction}
\label{sec:intro}

Type checking dependent types in languages like Agda~\cite{norell:PhD}
and Coq~\cite{inria:coq83} or logical frameworks like Twelf~\cite{carsten:twelf}
requires a large amount of evaluation, since types may depend on
values.  Such type checkers incorporate
an interpreter for purely functional programs with free variables---at least, the
-calculus---which is used to compute weak head normal forms
of types.  Efficiency of type checking is mostly identical with
efficiency of evaluation\footnote{Evaluation is necessary to reduce
  types to weak head normal form and compare types for equality.
  Subtracting these operations, type checking has linear complexity.
} 
(and, in case of type reconstruction,
efficiency of unification), and remains a challenge as of today.  
In seminal work, Gregoire and Leroy \cite{gregoireLeroy:icfp02} have
sped up Coq type checking by compiling to byte-code instead of
using an interpreter.  Boespflug \cite{boespflug:padl10} has obtained
further speed-ups by producing native code using stock-compilers.  

While compilation approaches are successful on batch type \emph{checking}
fully explicit programs, they have not been attempted on type
\emph{reconstruction} using higher-order unification or on interactive
program construction such as in Agda and Epigram \cite{mcBrideMcKinna:view}.
These languages are involved and constantly evolving, 
and their implementations are
prototypes and frequently modified and extended.  Implementing a
full compiler just to get type reconstruction going is deterring;
furthermore, compilation has not (yet) proven its feasibility in minor
evaluation tasks (like weak head evaluation)
that dominate higher-order unification.  
At least for language prototyping, smart
interpreters are, and may remain, competitive with compilation.  

For instance, Twelf's interpreter is sufficiently fast; it is 
inspired by a term representation with de Bruijn indices 
\cite{deBruijn:nameless} and explicit substitutions 
\cite{abadiCardelliCurienLevy:jfp91}.  In the context of functional
programming, explicit substitutions are known as \emph{closures},
consisting of the code of a function plus an environment, assigning
values to the free variables appearing in the code.  
In typical implementations of interpreters \cite{coquand:type}, 
these environments are not
precise; they assign values to all variables that are statically in
scope rather than only to those that are actually referred to in the
code.  This bears potential for space-leaks: the environment of a
closure might refer to a large value that is never used, but cannot be
garbage collected.  An obvious remedy to this threat is, when forming
a closure, to restrict the environment to the actual free variables;
however, this requires a traversal of the code.  We explore a
different direction: we are looking for a code representation that
maintains information about the free variables at each node of the
abstract syntax tree.

A principal candidate is \emph{linear typing} in Curry-Howard
correspondence with Girard's linear logic \cite{girard:linear}; there,
each variable in scope is actually referenced (more precisely,
referenced exactly once).  In other words, the
free variables are exactly the variables in the typing context.
Dropping types, we may talk of \emph{linear scoping}.
Yet we do not want to represent linear terms, but arbitrary
-terms.   Kesner and Lengrand \cite{kesnerLengrand:infcomp07}
achieve this by introducing explicit term constructs for weakening and
contraction.  We pursue a different path: we incorporate information
about variable use and multiplicity directly into abstraction and
application. 

In the context of linear -calculus, the free
variables of a function application are the disjoint union of the free
variables of the function and the free variables of the argument.  If
we want to maintain the set of free variables during a term traversal,
at an application node we need to decide which variables go into the
function part and which into the argument part.  Thus, we would store
at each application a set of variables that go into the, say, function
part, all others would go to the argument part.  Less information is
needed if we switch to an \emph{ordered} representation.

\emph{Ordered logic}, also called \emph{non-commutative linear logic}
\cite{polakowPfenning:tlca99}, refines linear logic by removing the
structural rule \emph{exchange} which restricts hypotheses to be used
\emph{in the order they have been declared}.  Transferring this
principle to ordered scoping this means that the scoping context lists
the free variables in the order they occur in the term, from left to
right.  This allows pushing the context into an application with very
little information: we just need to know \emph{how many} variables
appear in the function part so we can cut the context in two at the
right position, splitting it into function context and argument
context.  This constitutes the central idea of our representation: at
each application node of the syntax tree, we store a number denoting
the number of free variable occurrences in the function part.  During
evaluation of an application in an environment, we can cut the
environment into two, the environment needed for the evaluation of the
function and the environment needed for the evaluation of the
argument.  Thus, our environments are precise and space leaks are
avoided.  In particular, a variable is always evaluated in a singleton
environment assigning only a value to this variable.  Following this
observation, variables do not need a name, they are identified by
their position; and environments are simply sequences of values.

Since we are not interested in proof terms of ordered logic per se,
but only borrow the \emph{ordered context} idea for our
representation of untyped -calculus, 
we need to allow multiple occurrences of the same
variable.  In fact, the context shall list the variable
\emph{occurrences} in order.  At a lambda abstraction, we bind all
occurrences of the same variable.  Thus, at an abstraction node we
specify at which positions the bound variable should be inserted in
the scoping context.  This concludes the presentation of our idea.  

In the rest of the paper, after an introductory example
(Section~\ref{sec:example}) we formally define our term representation
in Section~\ref{sec:syntax}.   Interpreter and handling of
environments are described in Section~\ref{sec:values}, followed by
the translation between ordinary lambda terms and ordered terms
(Section~\ref{sec:parsing}).  Soundness of the interpreter is formally
proven in Section~\ref{sec:sound}, before we conclude with an
experimental evaluation in Section~\ref{sec:experiments}.

This article summarizes the B.Sc.\ thesis of the second author
\cite{kraus:bachelor}.

\section{An Example}
\label{sec:example}






To demonstrate the discussed risk of space-leaks during evaluation, we apply the term  in basic syntax consecutively to the free variables  and . A possible (and, if the mentioned closures are used, typical) sequence of reduction steps is given below.  
By writing , we want to express that in the term , each occurrence of the variable  () has to be replaced by the term  (resp. ) simultaneously. Such a substitution list always applies only to the directly preceding term:

Here, the substitution  could be dropped instantly and there is no need to apply the other substitution  to the term . 
However, the term representation used above comes along with the problem that such an evaluation algorithm does not have the required information in time.
This is due to the fact that the binding information is always split between the  and the actual variable occurrence, as they both carry the variable name.  
In contrast, using de Bruijn indices would make it possible to remove the piece of information from the .
Our goal is to do it the other way round: We want the whole information to be available at the , thus making it possible to know the number and places of the bound variable occurrences without looking at the whole term.







\section{Syntax}
\label{sec:syntax}

In this article, we only cover the core constructs of the lambda calculus as they are enough to make the approach clear. However, we do not see any limitations for common extensions. We first define \define{ordered preterms}:

\emph{Free variables} are denoted by their name like in the standard syntax. \emph{Bound variables}, however, are just denoted by a dot , which does not carry any information beside the fact that it is a bound variable. 
In the case of an \emph{application}, there is a first term (the function part) and a second term (the argument) as usual. 
Furthermore, the application carries an integer  as an additional piece of information that will be important for the evaluation process and is explained 
in a moment.
The most interesting part is the \emph{abstraction} . The vector  is nothing else but a list of non-negative  integers of length . It determines which dots  are bound by the  in the following way: Consider all  in the term  which are not bound in  itself. Now, the first  of these are not bound by the , the next one is, the following  are again not bound and so on (see examples below). 

We denote the number of unbound  in an ordered preterm  by . Consequently,  is simply defined by: 

We call an ordered preterm  an \define{ordered term} iff each sub-preterm  of  fulfils the following condition: If  is an application , the equation  holds and if it is an abstraction , then  is satisfied. The latter condition states that if a  in  binds a variable, this variable must actually exist while the first one just gives a meaning to the integer carried by an application. Clearly, any sub-preterm of an ordered term is again an ordered term. 
 is called \define{closed} if .


Here are some examples of closed terms.
The  combinator 

would be written as 

Moreover, the term 

from Section~\ref{sec:example} would be represented as

(note that applications are still left-associative). We can see that the first  does not bind anything as it is annotated with the empty vector , while this is less obvious when it is written as . 

At this point, we hope to have clarified the intended meaning of our syntax. A formal definition will be given in Section~\ref{sec:parsing}.


\section{Values and Evaluation} 
\label{sec:values}

Before specifying values and evaluation formally, we want to give an example to demonstrate how the information carried by a lambda should be used and why we always have exactly the needed information. Suppose we have the term

that is, the  combinator applied to three free variables
(we suppress the application indices  for better readability). We want to get rid of the beta redexes, so we start by eliminating the first one. The outermost  is decorated with the vector  of length one. Now, the single variable bound by this  should be replaced by , so we start a substitution list and insert a single :

The first remaining  is , so it does not bind the first variable (thus  remains first in the substitution list), but the second one. Consequently, we add an  after the :

Now the situation becomes more interesting. The only remaining  is now decorated with the vector , so one  has to be inserted after the first entry () and another one must be placed after the subsequent entry ():

All  have now been eliminated. The applications' indices tell us how the substitution list should be divided between the terms:

We do the same step once more:

The only thing left to be done is to apply the substitutions in the obvious way:

Here, evaluation naturally leads to a term in beta normal form. This is not always the case: as an example, if we had tried to evaluate the above term without the  (i.e. ), we would have got stuck at . However, this would have been satisfactory as it would have shown that the term's normal form is an abstraction. In other words, our evaluation results in weak head normal forms.

Consequently, we define \define{values} in the following way:

The \emph{large application}\footnote{
The argument vector  of a large application is sometimes
called a \emph{spine} \cite{cervesatoPfenning:spineCalculus}.  Large
applications  also appear in the formulation of B\"ohm
trees \cite{barendregt:lambdacalculus}.}
 consists of a variable  which is applied to a vector  of values. It is to be read as a left-associative application, i.e. as . 
Note that it is not necessarily ``large''. Quite the contrary, it
often only consists of the head (and the vector of values is empty).

A \emph{closure}  is the result of the evaluation process if the corresponding beta normal form of the term does not start with a free variable. The main part, , is nothing other than a lambda abstraction in the syntax of ordered terms.
In addition, we need the substitution list  (which is simply a list of values) that satisfies . The idea is that the  unbound  is to be replaced by .  These substitution lists have already been used in the example above.


At this point, we want to introduce a notation for inserting a single item multiple times into a list. More precisely, if  is a list, 
 is a vector (i.e. also a list) of  non-negative integers satisfying  and  is a single item, we write   for the list that is constructed by inserting  at each of the positions  into , i.e. for the list  (of course, it is possible that  starts or ends with ).


We are now able to define the evaluation function  which takes an ordered term  as well as an ordered substitution list  and returns a value. The tuple must always satisfy the condition . In other words, the list carries neither too little nor redundant information.
At the start of the evaluation, the ordered substitution list is empty. 
Additionally, we specify the application  of two values, which also returns a value and does not need anything else.
Our evaluation procedure uses a call-by-value strategy:


First, if we want to evaluate a free variable \lab 1, the substitution list must be empty because of the invariant mentioned above. 
Second, in the case of a  \lab 2, the ordered list must have exactly one entry. This entry is the result of the evaluation. 
If we evaluate an application \lab 3, we evaluate the left and the right term. The application's index enables us to split the substitution list at the right position. Then, we have to apply the first result to the second. 
Evaluating an abstraction \lab 4 is easy. We just need to keep the substitutions to build a closure. 

If we want to apply a large application to a value  \lab 5 , we just append  to the vector of values (we write  for ). 
The case of a closure  \lab 6 is less simple, but it is still quite clear what to do:  determines at which positions  should be inserted in the ordered substitution list, so we just construct the list . Then,  is evaluated.


Concerning substitution lists, we talk about ``lists of values'' for
simplicity. More specifically, we want them to be lists of pointers to
avoid the duplication of ``real'' values during constructing lists
like . 
Instead of simple linked lists,
we have also implemented these lists as dynamic functional arrays
represented as binary trees.  This reduces the asymptotic costs of
list splitting--- to
---
and multi-insertion  
from linear to logarithmic time (in terms of the length of the list
).  For an
experimental comparison of the two implementations
see Section~\ref{sec:experiments}.


\section{Parsing and Printing}
\label{sec:parsing}

In this section, we define how terms in normal syntax are translated into our ordered syntax (Parsing) and vice versa (Printing). 
To specify this, we need some notation. 
First of all, we write  for the set of variable names we want to use and  for the set of lambda terms in basic standard syntax (i.e. , furthermore,  together with  implies  and ). Additionally,  is the set of terms in our ordered syntax defined above,  the subset of closed ordered terms () and  the set of values (defined in the previous section).
Moreover, we write  for the set of lists of variable names and  for the set of lists of values. 
For each set  of variable names (), we denote the set of lists of elements of  by  and the ordered terms that do not contain any variable of  (as a free variable) by .


By writing  (resp. , , ), we mean the subset  of  (and analogous for the other cases).

For a finite set  of variable names, we define the \define{correspondence relation}

(pronounce: ``corresponds in context  to'').
The intuition is that  means: 
 is a term that corresponds to the ordered term , where unbound
 are replaced by the (\emph{not} necessarily pairwise distinct)
variables in the list . 
The set  can be seen as a filter that tells us which free variables do not occur in  but in  instead.

It is important to note that  implies that each variable occurring in  is contained in  and each free variable occurring in  is not contained in . This can be shown by induction on  (simultaneously for all sets ). 

By the same argument, one can see that (for each  and ) there exists a unique tuple  satisfying , so we can consider  a function . Furthermore, we note that  always implies . 
 
This also works the other way round. For each  and each tuple
, there is (by induction on ) a
term  satisfying . Moreover,
this term  is unique up to  equivalence. So,  is actually a bijection between  (the set of
 equivalence classes of terms) and . The
inference rules above show how to apply this bijection or its inverse
to a term or a tuple (in the last rule, any variable satisfying the
condition can be chosen for ), so we have 
a computable bijection 
determined by the rules for .

Choosing , we get a bijection .  As  is
only inhabited by the empty vector , we naturally get the
\define{parse} function  which maps 
bijectively on .




The above construction also gives us a function , but this is not enough. We want to transform closures
(elements of ) and values (elements of ) 
into basic terms .
Therefore, we
define the two \define{print} functions  and  simultaneously by recursion on the structure:

First, note that the printing functions are well-defined (i.e., they always terminate). This is because during evaluation of , we may safely assume that  is well-defined as long as  is a strict subterm of  and each value  in  is either only a variable (so termination of  is clear) or also in . Similar, during evaluation of , we may assume that  is defined for each .

For all , we have  if and only if  (which just means ) as both judgements are defined identically in the case of closed ordered terms. This essentially (with implicit use of the bijection ) means , i.e. the composition of parsing and printing is the identity.






\section{Correctness and Termination properties}
\label{sec:sound}



We still have not shown that our evaluation algorithm given in Section~\ref{sec:values} does not change the meaning of terms. The combination of parsing, evaluating and printing should never result in a term that is not beta equivalent to the original term. We also want to show a limited termination property.
To keep our argument simple, we just sketch the proofs and hope that the ideas become clear.

First, we attend to the correctness question. 
We need to convince ourselves that \emph{rewriting} according to the rules of the functions  and  does not cause an error. 
By \emph{rewriting}, we mean one step of the \emph{normal} or \emph{leftmost outermost} evaluation. We have demonstrated this in the example at the beginning of Section~\ref{sec:values}.
Printing should result in a term that is  equivalent to the term we get if we rewrite before printing. 
This basically means that, for each evaluation rule on the left hand
side of the following table, we have to check that the equality on the
right hand side holds:

\begin{center}
\begin{tabular}{|   rcl  |  rcl    | c  }
\cline{1-6}
&&&&&& \\
&& & && & \qquad \lab 1 \\
&&&&&& \\
&& & &&  & \qquad \lab 2 \\
&&&&&& \\
&& & &&  & \qquad \lab 3 \\
&&&&&& \\
&& & &&  & \qquad \lab 4 \\  
&&&&&& \\
&& & &&  & \qquad \lab 5\\
&&&&&& \\
&& & &&  & \qquad \lab 6\\
&&&&&& \\
\cline{1-6}
\end{tabular}
\end{center}

Note that ``rewriting using the evaluation rules'' results in expressions which are, more or less, a mixture of elements of  and . To be precise, such an expression is either in  or in  or a tuple (to read as simple application) of two expressions. The ,  and  rule turn an element of  into a value, the  turns it in a tuple of two such elements, and the last two rules turn a tuple of two values into one value or element. 
Although we do not define it formally, it should be clear how those expressions can be printed by using the printing functions for ordered terms and values (if an expression is a tuple, just print the function part and the argument part separately). In fact, while the function  is formulated as a big step evaluation, the rewriting process can be understood as the corresponding small step (or one step) evaluation.

In the first five rows of the table, we only have to look at the definitions of the printing functions to see that the printed terms are not only  equivalent but also equal. The very last rule  requires closer examination: 
By rule , the term  is equal to  for a (sufficiently) fresh variable . Now, the definitions of the printing functions are ``context free'' in a way that guarantees  that  occurs exactly  times (free) in . Furthermore, replacing those occurrences by  results in the term . This means that, starting with 
,
we have to use exactly one  reduction step to get the term .

As we have already seen that the composition of parsing a term and printing it afterwards does not change anything (up to  equivalence), we can now conclude that parsing, evaluating (a finite number of rewriting steps) and printing is equivalent to a number of  reduction steps.

Now we discuss termination. Obviously, our evaluation function  does not always terminate as some terms do not have a weak head normal form. However,  terminates whenever it is applied to () if the usual  reduction is strongly normalizing on . 
The main consequence of this is that evaluation terminates for all well-typed terms. To prove this statement, assume that there is such a term  so that the evaluation of  does not terminate. 
Then, we get an infinite sequence  where  is the result of rewriting (a subexpression of)  using one of the evaluation rules. If we print , we get a sequence  of terms in , where  is either () equal to  or arises from  in exactly one  reduction step. If  reduction is strongly normalizing on , the sequence has to become constant at some point, i.e.  for some . This implies that, after the first  rewriting steps, rule \lab 6 is not used anymore. Define the \emph{weight}  of an expression  to be , if the expression is just an element of , to be , if it is a value of the \emph{closure} type, to be , if it is a value of the form  (i.e. a \emph{large application}) and, if it is a tuple of two expressions, as the sum of both weights. Then, each of the rewritings that are induced by the first five lines in the table increase the weight of the expression, so we get ;  however, as the total number of values (and tuples in ) is bound by the length of the term we get after printing  (or any ), the sequence is bounded, resulting in the required contradiction.















\section{Experiments and Results}
\label{sec:experiments}

The specified term representation and evaluation have been implemented
in Haskell. They have been used by a type checker to check large files
of dependently
typed terms of the Edinburgh Logical Framework which were kindly
provided by Andrew W. Appel (Princeton University). To make this
possible, an extended syntax has been used that includes -types, constants and definitions. It is straightforward to expand our
evaluation algorithm to the extended syntax---for details consult the
Bachelor's thesis of the second author \cite{kraus:bachelor}. 
The substitution lists
have been implemented as simple Haskell lists, and also as balanced
binary trees (following Adams \cite{adams:jfp93})
for better asymptotic complexity.  Both variants were
evaluated for performance 
[referred to as \emph{Ordered (trees)} and \emph{Ordered (lists)}].

For comparison, the completely analogous algorithm for terms in
extended basic syntax (i.e. ) has been used [\emph{Simple
  Closures}]. Furthermore, we have tested a strategy that always
evaluates completely (i.e. produces  normal forms) using
Hereditary Substitution [\emph{Beta Normal Values}]
\cite{watkins:concurrentlftr}.

Our main test file \textsf{w32\_sig\_semant.elf} with a size of
approximately 21 megabytes contains a proof described in
\cite{appel:toplas10}. We also tested smaller parts of this file, more
precisely, the first ,  and  lines without the
rest (named \textsf{6000.elf} and so on). Later terms tend to be
larger, so the tests with fewer lines needed much less time.

All tests were executed on the same server 
\mbox{\texttt{baerentatze.cip.ifi.lmu.de}}
working with a CPU of type \emph{AMD Phenom II X4 B95} (only
one core used, 3 GHz) and 8 GB system memory. The measurements
of space and time consumption are given in the following tables (rounded
average values). More specifically, \emph{time} refers to the total time, including parsing the input file and transforming (if necessary) the representation to our ordered representation or to one that uses De Bruijn terms. However, in our tests, the transformation process only took a negligible amount of time. The time value does, however, not include any printing of the terms or the values (with printing, the total time increased significantly). \emph{Space} refers to the peak space usage of the whole process.

\begin{center}
\begin{tabular}{| l || c | c |}
\multicolumn{3}{c}{\textsf{6000.elf} (file size: 3.8 MB)}\\
\hline
& time (sec) & space (MB) \\
\hline
\hline
Ordered (trees) & 18.9 & 1111 \\
\hline
Ordered (lists) & 18.6 & 1114\\
\hline
Simple Closures & 18.5 & 1152\\
\hline
Beta Normal Values & 27.6 & 2034\\
\hline
\end{tabular}

\vspace{5pt}

\begin{tabular}{| l || c | c |}
\multicolumn{3}{c}{\textsf{10000.elf} (file size: 12.9 MB)}\\
\hline
& time (sec) & space (MB) \\
\hline
\hline
Ordered (trees) & 61.0 & 3230\\
\hline
Ordered (lists) & 60.6 & 3237\\
\hline
Simple Closures & 60.0 & 3302\\
\hline
Beta Normal Values & 98.7 & 5878\\
\hline
\end{tabular}

\vspace{5pt}

\begin{tabular}{| l || c | c |}
\multicolumn{3}{c}{\textsf{12000.elf} (file size: 17.8 MB)}\\
\hline
& time (sec) & space (MB) \\
\hline
\hline
Ordered (trees) & 84.3 & 5096 \\
\hline
Ordered (lists) & 83.8 & 5103 \\
\hline
Simple Closures & 83.6 & 5226 \\
\hline
Beta Normal Values & 137.7 & 8513 \\
\hline
\end{tabular}
\end{center}

Unsurprisingly, beta normal values perform significantly worse than each of the other possibilities. However, the difference is smaller than it could have been expected. This might be due to the fact that during type checking, total evaluation of a term is often necessary anyway, thereby reducing the hereditary substitution's disadvantage. 

Although none of the other strategies exhibited any shortcomings in the comparisons above, the following results for the complete file are remarkable. Here, implementing ordered substitutions as normal Haskell lists seems to be much more efficient than using tree structures: 

\begin{center}
 \begin{tabular}{| l || c | c |}
\multicolumn{3}{c}{\textsf{w32\_sig\_semant.elf} (file size: 20.9 MB)}\\
\hline
& time (sec) & space (MB) \\
\hline
\hline
Ordered (trees) & 108.4 & 8877\\
\hline
Ordered (lists) & 94.8 & 4948\\ \hline
Simple Closures & 94.3 & 5068 \\ \hline
Beta Normal Values & 169.8 & 9044  \\
\hline
\end{tabular}
\end{center}

Our Simple Closures are still on the same level as Ordered Representation with lists, but the trees are far behind. 
In comparison, the type checker of the Twelf project, \emph{Twelf
  r1697} (written in \emph{Standard ML} and compiled with
\emph{MLton}'s whole program optimizations \cite{fluetWeeks:icfp01}) 
does the job nearly five
times faster while using only 2720 megabytes of
memory. 






\section{Related Work and Conclusions}
\label{sec:concl}

Our term representation is inspired by intuitionistic implicational
linear logic in natural deduction style which has explicit operations
for weakening and contraction
\cite{bentonBiermanDePaivaHyland:tlca93}.  With explicit weakening and
contraction, one easily maintains complete information about the free
variables of a term at each node \cite{kesnerLengrand:infcomp07}.  Our
term representation incorporates weakening and contraction into lambda
abstraction.  By using inspiration from ordered logic, we reduce the
stored information at application nodes to a minimum, namely an
integer; further, our variable nodes need to carry no information at all.

Another means to maintain information about free variables are
\emph{director strings} by Sinot \cite{sinot:jlc05}.  Application
nodes come with a map that tell for each variable whether it appears
in the left or the right subterm or in both.  Our term representation
can be seen as an optimized version of director strings, however, we
have no experimental comparison. 
Sinot \etal~\cite{fernandezMackieSinot:aaecc05} present some
performance results of director strings; however, it is restricted
to evaluation of some specific big lambda-terms.  There is no study on
their relative performance in a realistic application---yet that is
our concern.

An alternative to explicit substitutions is Nadathur's suspension
calculus \cite{nadathur:jflp99}, which, in a refinement, also
maintains information about closedness of subterms.  In this
refinement, the suspension calculus maintains at least partial
information about the free variables of a subterm.  
As the basis of an implementation of
-Prolog \cite{nadathur:flops01}, 
Nadathur has proven the efficiency of his term
representation not only for normalization and equality checking, but
also for higher-order unification.

Building on the suspension calculus,
Liang, Nadathur, and Qi \cite{liangNadathurQi:jar05} have evaluated
different term representations in the context of Prolog, a
study that compares to our study of term representations for the
Edinburgh Logical Framework.  They have tested different combinations
of features, confirming our result that lazy substitution is 
preferable to eager substitution {[Beta Normal Forms]},
even more so when several substitutions are gathered into one
traversal {[Closures, Ordered]}.  They also test a variant where
each term is equipped with an \emph{annotation}, a flag telling whether
this term is open, \ie, has free variables, or closed, \ie, has no
free variables.  In their experimental evaluation, these annotations
pay off greatly for the poorly behaving eager substitution, yet give
negligible advantage for explicit substitutions.  It is
hypothesized that in a combined substitution, each subterm will
mention at least one variable with a high probability, so the
traversal has to run over most of the whole term---this is certainly
different in the substitution for a single variable.

To summarize, we have presented a new term representation for the
lambda-calculus inspired by ordered linear logic, and experimentally
compared it with well-known representations (closures, normal forms)
in a prototypical implementation of a type checker for the Edinburgh
Logical Framework.  The experiments were carried out on large realistic
proof terms, constructed manually and mechanically.  

The results were not significantly in favor of our new representation.
This might be due to the application domain, LF signature checking.
For one, LF-definitions are closed, which means that substitutions
never need to traverse a definition body when the definition is
expanded, and this optimization is shared by all the term
representations we compared.  Secondly, we only tested type checking,
not type reconstruction via unification.  During type checking, where
equality tests are expected to succeed, full normal forms are always
computed, and closures are very short-lived in memory.  More space
leaks are to be expected in applications such as logic programming or
type reconstruction, where unification is needed, which is not
expected to always succeed.   In constraint-based unification,
unsolvable constraints might be postponed, keeping closures alive for
longer.  In such situations, the benefits of our representation might
be more noticeable, more experiments are required.

In the future, we plan to investigate further term representations
such as term graphs, and perform more experiments.  The literature on
experimentally successful term representations is sparse, our work
contributes to close this gap.  Our long term goal is to find a
term representation which speeds up Agda's type reconstruction.


\paragraph*{Acknowledgements}
The idea for the here presented ordered term representation was
planted in a discussion with Christophe Raffalli who invited the first
author to Chambery in February 2010.  He mentioned to me that in his
library \texttt{bindlib} formation of closures restricts the
environment to the variables actually occurring free in the code.  I
also benefited from discussions with Brigitte Pientka and Stefan Monnier.

Thanks to Gabriel Scherer for comments on a draft version of this
paper and pointers to related work. 
The final version of this paper was stylistically improved with the highly appreciated help of Neil Sculthorpe.



\bibliographystyle{eptcs}
\bibliography{nonauto-lfmtp11}


\end{document}
