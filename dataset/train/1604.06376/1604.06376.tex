\documentclass{article}

\usepackage{paralist}


\usepackage{amsthm}     \newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{problem}{Problem}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{algorithm}{Algorithm}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc} 
\usepackage{amsfonts}
\usepackage{amssymb,amsmath} 
\usepackage{graphics,graphicx} 
\usepackage{color}
\usepackage{colordvi}
\usepackage{gastex}
\usepackage{comment} 

\usepackage{enumerate}
\sloppy





\renewcommand{\sb}[1]{\scalebox{0.75}[1]{#1}}
\newcommand{\sbcustom}[2]{\scalebox{1}[1]{#2}}    

\makeatletter
\DeclareRobustCommand\sfrac[1]{\@ifnextchar/{\@sfrac{#1}}{\@sfrac{#1}/}}
\def\@sfrac#1/#2{\leavevmode\scalebox{.9}{\kern.1em\raise.5ex
         \hbox{}\kern-.1em
         /\kern-.15em\lower.25ex
          \hbox{}}}
\DeclareRobustCommand\numfrac[1]{\@ifnextchar/{\@numfrac{#1}}{\@numfrac{#1}}}
\def\@numfrac#1{\leavevmode \hbox{}}
\makeatother


\newcommand{\real}{\mathbb R}
\newcommand{\rat}{\mathbb Q}
\newcommand{\nat}{\mathbb N}
\newcommand{\tuple}[1]{\langle #1 \rangle}
\def\abs#1{\ensuremath{\lvert #1\rvert}} 
\def\norm#1{\ensuremath{\lVert #1\rVert}}
\let\epsilon\varepsilon
\let\emptyset\varnothing


\newcommand{\Inf}{{\sf Inf}}
\newcommand{\cale}{\mathcal E}
\newcommand{\Supp}{{\sf Supp}}
\newcommand{\Last}{\mathsf{Last}}
\newcommand{\Cone}{\mathsf{Cone}}
\newcommand{\half}{}
\newcommand{\DD}{\Delta}
\newcommand{\GG}{\mathcal{G}}
\newcommand{\PP}{\delta}
\newcommand{\winsure}[1]{\langle \! \langle #1 \rangle \! \rangle_{\mathit{sure}}  }
\newcommand{\winval}[1]{\langle \! \langle #1 \rangle\! \rangle_{\mathit{val}} }
\newcommand{\winvalf}[1]{\winval{#1}^{{FM}}}
\newcommand{\va}{\winval{\ma}}
\newcommand{\vb}{\winval{\mi}}
\newcommand{\vaf}{\winval{\ma}^{{FM}}}
\newcommand{\vbf}{\winval{\mi}^{{FM}}}

\newcommand{\straava}{\winval{\straa}}
\newcommand{\straavaf}{\winval{\straa}^{{FM}}}
\newcommand{\strabvb}{\winval{\strab}}
\newcommand{\strabvbf}{\winval{\strab}^{{FM}}}
\newcommand{\Win}{\mathsf{Win}}
\newcommand{\mystravaf}[1]{\winval{#1}^{{FM}}}


\newcommand{\cpremi}{\mathsf{Cpre}_{\mi}}

\newcommand{\atma}{\mathsf{Attr}_{\ma}}
\newcommand{\atmi}{\mathsf{Attr}_{\mi}}
\newcommand{\straa}{\sigma}
\newcommand{\Straa}{\Sigma}
\newcommand{\strab}{\pi}
\newcommand{\Strab}{\Pi}


\newcommand{\LP}{{\sf LP}}


\DeclareMathOperator{\ma}{\mathsf{Max}}
\DeclareMathOperator{\mi}{\mathsf{Min}}
\newcommand{\prob}[1]{\mathbb{P}_{#1}}
\newcommand{\rwd}{\mathsf{rwd}}
\newcommand{\parity}{\operatorname{\mathsf{Par}}}
\newcommand{\MeanSup}{\operatorname{\mathsf{MeanSup}}}
\newcommand{\MeanInf}{\operatorname{\mathsf{MeanInf}}}
\newcommand{\Buchi}{\operatorname{\mathsf{B\ddot{u}chi}}}
\newcommand{\coBuchi}{\operatorname{\mathsf{coB\ddot{u}chi}}}
\def\mynote#1{{\bf  #1 }}



\begin{document}


\sloppy



\title{{\bf Perfect-Information Stochastic Games \mbox{with Generalized Mean-Payoff Objectives}}\thanks{This research was partially supported by Austrian Science Fund (FWF) 
NFN Grant No S11407-N23 (RiSE/SHiNE), ERC Start grant (279307: Graph Games), 
Vienna Science and Technology Fund (WWTF) through project ICT15-003,
and European project Cassting (FP7-601148).
}}

\author{
Krishnendu Chatterjee \quad  Laurent Doyen \\ 
\normalsize
  IST Austria \quad  CNRS \& LSV, ENS Cachan 
}

\date{}
\maketitle


\begin{comment}
\title{Perfect-Information Stochastic Games with Generalized Mean-Payoff 
Objectives\thanks{This research was supported by Austrian Science Fund (FWF) 
NFN Grant No S11407-N23 (RiSE/SHiNE), ERC Start grant (279307: Graph Games), 
Vienna Science and Technology Fund (WWTF) through project ICT15-003,
and European project Cassting (FP7-601148).
}}

\authorinfo{Krishnendu Chatterjee}
{IST Austria} 
{krish.chat@ist.ac.at} 
\authorinfo{Laurent Doyen}
{LSV, ENS Cachan \& CNRS, France} 
{doyen@lsv.fr}
\end{comment}









\begin{abstract}
Graph games provide the foundation for modeling and synthesizing 
reactive processes. In the synthesis of stochastic reactive processes, 
the traditional model is perfect-information stochastic games, 
where some transitions of the game graph are controlled by two adversarial 
players, and the other transitions are executed probabilistically. 
We consider such games where the objective is the conjunction of 
several quantitative objectives (specified as mean-payoff conditions),
which we refer to as generalized mean-payoff objectives.
The basic decision problem asks for the existence of a 
finite-memory strategy for a player that ensures the 
generalized mean-payoff objective be satisfied with a desired
probability against all strategies of the opponent.
A special case of the decision problem is the almost-sure
problem where the desired probability is~1. 
Previous results presented a semi-decision procedure for -approximations 
of the almost-sure problem. 
In this work, we show that both the almost-sure problem as well
as the general basic decision problem are coNP-complete,
significantly improving the previous results.
Moreover, we show that in the case of 1-player stochastic
games, randomized memoryless strategies are sufficient and the problem 
can be solved in polynomial time.
In contrast, in two-player stochastic games, we show that 
even with randomized strategies exponential memory is required in general,
and present a matching exponential upper bound.
We also study the basic decision problem with infinite-memory strategies
and present computational complexity results for the problem. 
Our results are relevant in the synthesis of stochastic reactive 
systems with multiple quantitative requirements.
\end{abstract}


\section{Introduction}
\label{sec-intro}

Reactive systems are non-terminating processes that interact continually with a changing environment. 
Since such systems are non-terminating, their behavior is described by infinite sequences of events.
The classical framework to model reactive systems with controllable and uncontrollable events are games on graphs. 
In the presence of uncertainties, we have stochastic reactive systems with probability distributions over state changes. 
The performance requirement on such systems, such as power consumption or latency,
can be represented by rewards (or costs) associated to the events of the system,
and a quantitative objective that aggregates the rewards of an execution to a single value.
In several modeling domains, however, there is not a single objective 
to be optimized, but multiple, potentially dependent and conflicting goals.
For example, in the design of an embedded system, the goal may be to maximize 
average performance while minimizing average power consumption.
Similarly, in an inventory management system, the goal would be to optimize
the costs associated to maintaining each kind of product~\cite{FV97,AltmanBook}. 
Thus it is relevant to study stochastic games with multiple 
quantitative objectives.

\smallskip\noindent{\em Perfect-information stochastic games.}
A perfect-information stochastic graph game~\cite{Condon92}, also known as turn-based stochastic 
game or \emph{2\half-player graph game}, consists of a finite directed graph with three kinds of states (or vertices): 
player-, player-, and probabilistic states.
The game starts at an initial state, and is played as follows: 
at player- states, player~ chooses a successor state;
at player- states, player~ (the adversary of player~)  does likewise;
and at probabilistic states, a successor state 
is chosen according to a fixed probability distribution.
Thus the result of playing the game forever is an infinite path through 
the graph.
If there are no probabilistic states, we refer to the game as a 
\emph{2-player graph game}; 
if there are no player- states, we refer to the (1\half-player) game
as a Markov decision process (MDP);
if there are no probabilistic states and no player- states,
then the (1-player) game is a standard graph.

The class of 2-player graph games has been used for a long time 
to synthesize non-stochastic reactive systems~\cite{BL69,PnueliRosner,RamadgeWonham}:
a reactive system and its environment represent the two players, whose states 
and transitions are specified by the vertices and edges of a game graph.
Similarly, MDPs have been used to model stochastic processes without 
adversary~\cite{FV97,Puterman}.
Consequently, 2\half-player graph games, which subsume both 2-player 
graph games and MDPs, provide the theoretical foundation
to model stochastic reactive systems~\cite{FV97,RF91}. 


\smallskip\noindent{\em Mean-payoff objectives.}
One of the most classical example of quantitative objectives is the mean-payoff 
objective~\cite{FV97,Puterman,Gi57,EM79}, where a reward is associated to each
state and the payoff of a path is the long-run average of the rewards of the 
path (computed as either  or  of the averages of the finite 
prefixes to ensure the payoff value always exists).
While traditionally the verification and the synthesis problems were considered
with Boolean objectives~\cite{PnueliRosner,RamadgeWonham,KV05}, recently quantitative objectives have 
received a lot of attention~\cite{BCHJ09,CCHRS11,BBFR13}, as they specify requirements 
on resource consumption (such as for embedded systems or power-limited systems) as well 
as performance-related properties.

\smallskip\noindent{\em Various semantics for multiple quantitative objectives.}
The two classical semantics for quantitative objectives are as follows~\cite{BBCFK14}:
the first is the expectation semantics, which is a probabilistic average of 
the quantitative objective over the executions of the system;
and the second is the satisfaction semantics, which consider the probability of
the set of executions where the quantitative objective is at least a required
threshold value~.
The expectation objective is relevant in situations where we are interested 
in the ``average'' behaviour of many instances of a given system, while 
the satisfaction objective is useful for analyzing and optimizing the desired
executions, and is more relevant for the design of critical stochastic reactive 
systems (see~\cite{BBCFK14} for a more detailed discussion).
For example, consider one mean-payoff objective that specifies the set of 
executions where the average power consumption is at most 5 units, and 
another mean-payoff objective that specifies the set of executions where 
the average latency is at most 10 units. 
A multiple objective asks to \emph{satisfy} both, i.e., their conjunction. 
We refer to such objectives (i.e., conjunction of multiple mean-payoff 
objectives) as {\em generalized mean-payoff objectives}\footnote{In the verification
literature, conjunction of reachability, B\"uchi, and parity objectives, 
are referred to as generalized reachability, generalized B\"uchi, and generalized
parity objectives, respectively, and generalized mean-payoff objectives naming is 
for consistency.}. 
The goal of player~ is to maximize the probability of satisfaction 
of the generalized mean-payoff objective while player~ tries to 
minimize this probability, i.e., the game is zero-sum.
Concrete applications of 2\half-player graph games with generalized mean-payoff objectives have been considered, such as 
best-effort synthesis where the goal is to minimize the violation 
of several incompatible specifications~\cite{CGHRT12}, real-time
scheduling algorithms with requirements on the utility and energy consumption~\cite{CPKS14},
and electric power distribution in an avionics application~\cite{BKTW15}.
In particular, for the real-world avionics application in~\cite{BKTW15},
both two adversarial players, stochastic transitions, as well as multiple mean-payoff
objectives are required, i.e., the application can be modeled as 2\half-player
graph games with generalized mean-payoff objectives, but not in a strict subclass.




\smallskip\noindent{\em Computational questions.} 
In this work, we consider 2\half-player graph games with generalized 
mean-payoff objectives in the satisfaction semantics.
A strategy for a player is a recipe that given the history of interaction so far 
(i.e., the sequence of states) prescribes the next move.
The basic decision problem asks, given a 2\half-player graph 
game, a generalized mean-payoff objective, and a probability threshold 
, whether there exists a strategy for player~ to ensure the objective 
be satisfied with probability at least  against all strategies of 
player~. 
Since strategies in games correspond to implementations of controllers for 
reactive systems, a particularly relevant question is to ask for the existence 
of a finite-memory strategy in the basic decision problem, instead of an 
arbitrary strategy.
Moreover, an important special case of the basic decision problem is the 
almost-sure problem, where the probability threshold  is equal to~.


\smallskip\noindent{\em Previous results.}
We summarize the main previous results for MDPs, 2-player graph games, and 
2\half-player graph games, with generalized mean-payoff objectives.

\begin{compactenum}
\item {\em MDPs.} The basic decision problem for generalized mean-payoff objectives in MDPs 
with infinite-memory strategies can be solved in polynomial time~\cite{BBCFK14}. 
The problem under finite-memory strategies has not been addressed yet.

\item {\em 2-player games.} 
The following results are known~\cite{VCDHRR15}:
the basic decision problem for generalized mean-payoff objectives in 
2-player graph games, both under finite-memory and infinite-memory 
strategies, is coNP-complete;
moreover, for infinite-memory strategies if the mean-payoff objective
is defined as the limit supremum of the averages (rather than limit infimum of 
the average), then the problem is in NP  coNP.

\item {\em 2\half-player games.} 
The almost-sure problem for generalized mean-payoff objectives in 2\half-player 
graph games under finite-memory strategies was considered in~\cite{BKTW15}, and 
a semi-algorithm (or semi-decision procedure) was presented for approximations
of the problem.

\item {\em Memory of strategies}. Infinite-memory strategies are strictly more powerful than finite-memory 
strategies, even in 1-player graph games thus also in MDPs and 2-player graph games:
there are games where an infinite-memory strategy can ensure the objective with probability~1 
while all finite-memory strategies fail to do so\footnote{However, in some variants of the decision problem 
(such as requiring the mean-payoff value, computed as the  of the averages of the finite prefixes, 
be strictly greater than a threshold~) finite-memory strategies are as powerful as
infinite-memory strategies in MDPs~\cite{CR15}. }~\cite{VCDHRR15}.
\end{compactenum}



\smallskip\noindent{\em Our contributions.}
The previous results suggest that 2\half-player graph games with generalized 
mean-payoff objectives are considerably more complicated than 2-player graph 
games as well as MDPs, as even the decidability of the almost-sure problem was 
open for 2\half-player graph games for finite-memory strategies (the previous 
result neither gives an exact algorithm, nor establishes decidability for 
approximation).
In this work we present a complete picture of decidability as well as computational 
complexity. 
Our results are as follows:
\begin{compactenum}

\item {\em MDPs.} First we study the generalized mean-payoff problem under 
finite-memory strategies in MDPs. We present a polynomial-time algorithm,
and show that with randomization, memoryless strategies (which do not depend on
histories but only on the current state) are sufficient,
i.e., for finite-memory optimal strategies no memory is required.

\item {\em 2\half-player games.} 
For 2\half-player graph games with generalized mean-payoff objectives we show 
that: 
(1)~the basic decision problem is coNP-complete under finite-memory strategies (significantly improving 
the known semi-decidability result for approximation of the almost-sure 
problem~\cite{BKTW15}), and moreover, the same complexity holds for the almost-sure problem; 
and (2)~under infinite-memory strategies, the computational complexity results 
coincide with the special case of 2-player graph games.

\item {\em Memory of strategies.} Under finite-memory strategies, 
in contrast to MDPs where we show with randomization no memory is required,
we establish an exponential lower bound (even with randomization) for memory 
required in 2\half-player graph games with generalized mean-payoff objectives.
We also present a matching upper bound showing that exponential memory
is sufficient.
\end{compactenum}





\smallskip\noindent{\em Key technical insights.}
We show that for generalized mean-payoff objectives, for the adversary, 
pure and memoryless strategies are sufficient. 
Under finite-memory strategies for player~, this result is established
using the following ideas: 
\begin{compactitem}

\item In general for prefix-independent objectives (objectives that do not change
if finite prefixes are added or removed from a path), we show that sub-game perfect 
strategies exist, where a strategy is sub-game perfect if it is optimal after every
finite history.
Such a result is known for infinite-memory strategies using results
from martingale theory~\cite{GK14}. 
Our proof for finite-memory strategies is conceptually simpler, and 
uses combinatorial arguments and well-known discrete properties of MDPs 
(see Lemma~\ref{lem:subgame-perfect}, Section~\ref{sec:half}).

\item Then using the above result we show that for a sub-class of 
prefix-independent objectives (that subsume generalized mean-payoff objectives) 
for the adversary pure memoryless strategies suffice 
(see Theorem~\ref{theo:half-memoryless}, Section~\ref{sec:half}).
Moreover, for this class of objectives we establish determinacy when each
player is restricted to finite-memory strategies, which is of independent 
interest (see also Theorem~\ref{theo:half-memoryless}); and also show that such 
determinacy result does not hold for all prefix-independent objectives 
(see Remark~\ref{rem:nodet}).


\item For MDPs, we generalize a result of~\cite{KS88} from graphs to MDPs, 
to obtain a linear-programming solution for the generalized mean-payoff objectives 
under finite-memory strategies (see Theorem~\ref{theo:mdp}, Section~\ref{sec:finmem}). 
\end{compactitem}
Combining these results we obtain the coNP upper bound for the basic 
decision problem for 2\half-player graph games and the coNP 
lower bound follows from existing results on 2-player graph games 
(see Theorem~\ref{theo:finite-coNP-complete}, Section~\ref{sec:finmem}).



\smallskip\noindent{\em Related works.} 
We have described the most relevant related works in the paragraph {\em Previous
results.} 
We discuss other relevant related works.
Markov decision processes with multiple objectives have been studied in numerous
works, for various quantitative objectives, such as mean-payoff~\cite{Cha07,BBCFK14}, 
discounted sum~\cite{CMH06,CFW13}, total reward~\cite{FKN11} as well as qualitative 
objectives~\cite{EKVY08}, and 
their combinations~\cite{CKK15,Baier-CSL-LICS-1,Baier-CSL-LICS-2,CR15}.
The problem of 2-player graph games with multiple quantitative objectives has 
also been widely studied both for finite-memory strategies~\cite{VCDHRR15,CRR14,JLS15,BR15,Vel14}
as well as infinite-memory strategies~\cite{VCDHRR15,CV13}.
In contrast, for 2\half-player games with multiple quantitative objectives only
few results are known~\cite{BKTW15,CFKSW13}, because of the inherent difficulty to handle two-players, probabilistic
transitions, as well as multiple objectives all at the same time.
A semi-decision procedure for approximation of the almost-sure problem for 2\half-player
games with generalized mean-payoff objectives was presented in~\cite{BKTW15}, 
which we significantly improve.
The class of 2\half-player graph games with positive Boolean combinations of total-reward
objectives was considered in~\cite{CFKSW13}, and the problem was established to be
PSPACE-hard and undecidable for pure strategies.










\section{Definitions}\label{sec:def}


\noindent{\bf Probability distributions.}
For a finite set , we denote by  the set of all probability 
distributions over , i.e., the set of functions  
such that .
The \emph{support} of  is the set .
For a set  let .

\smallskip\noindent{\bf Perfect-information stochastic games.}
A \emph{perfect-information stochastic game} (for brevity, stochastic games
in the sequel) 
is a tuple , 
consisting of a finite set  of states
partitioned into the set  of states controlled by player~ (depicted
as round states in figures) and the set  of states controlled by player~ (depicted
as square states in figures), a finite set  of actions, 
and a probabilistic transition function .
If , we say that  is an \emph{-successor} of .
A transition  is \emph{deterministic} if  for some 
state . 
The underlying graph of  is  where .

For complexity results, we consider that the probabilities in stochastic games
are rational numbers with numerator and denominator 
encoded in binary.

\smallskip\noindent{\bf Markov decision processes and end-components.}
A \emph{Markov decision process} (MDP) is the special case of a stochastic
game where either , or . 
Given a state  and a set ,
let  be the set of all actions  such 
that . 
A \emph{closed} set in an MDP is a set  such 
that  for all . 
\begin{comment}
The transition function  of the sub-MDP induced by a closed set  is 
defined, for all  and , by  if , 
and  for arbitrary  otherwise.
The closed set  is an \emph{end-component} if the underlying 
graph of the sub-MDP induced by  is strongly connected~\cite{CY95}.
We denote by  the set of all end-components of an MDP~.  
\end{comment}
A set  is an {\em end-component}~\cite{CY95} if (i)~ is closed, and 
(ii)~the graph  is strongly connected
where 
denote the set of edges given the actions.
We denote by  the set of all end-components of an MDP~.  

\smallskip\noindent{\bf Markov chains and recurrent sets.}
A \emph{Markov chain} is the special case of an MDP 
where the action set  is a singleton. In Markov chains,
end-components are called \emph{closed recurrent sets}.







\smallskip\noindent{\bf Plays and strategies.}
A \emph{play} is an infinite sequence  of states.
A \emph{randomized strategy} for  is a recipe to describe what is the next action to play
after a prefix of a play ending in a state controlled by player~;
formally, it is a function  that provides
probability distributions over the action set. 
A \emph{pure strategy} is a function  that provides
a single action, which can be seen as a special case of randomized strategy
where for every play prefix  there exists an action  
such that .


We consider the following memory restrictions on strategies.
A strategy  is \emph{memoryless} if it is independent of the past and depends only on 
the current state, that is  for all play prefixes 
, where .
In the sequel, we call memoryless strategies the pure memoryless strategies,
and we emphasize that strategies  are not necessarily 
pure by calling them randomized memoryless.


A strategy  uses \emph{finite memory} if it can be described 
by a transducer  consisting of a finite
set  (the memory set), an initial memory value , 
an update function  for the memory, 
and a next-action function ; the transducer  
defines the strategy  such that  
for all play prefixes 
where  extends  to sequences of states as usual (i.e.,
).
Given a finite-memory strategy  for player~, let  
be the MDP obtained by playing  in , where 
    and the transition function  is defined
for all  and action  of player~ as follows,
for all , where :
\begin{itemize}
\item if , then 
; 
\item if , then 
.
\end{itemize}


Strategies  for player~ are defined analogously, as well as the memory restrictions.
A strategy that is not finite-memory is referred to as an infinite-memory 
strategy. We denote by  the set of all strategies for player~, 
and by , 
and  respectively the set of all pure memoryless,
and all finite-memory strategies 
for player~. We use analogous notation , , and  for player~.


\smallskip\noindent{\bf Objectives.}
An {\em objective} is a Borel-measurable set of plays~\cite{Billingsley}.
In this work we consider conjunctions of mean-payoff objectives. 
Some of our results are related to more general classes of prefix-independent
and shuffle-closed objectives.
We define the relevant objectives below:
\smallskip
\begin{compactenum}
\item {\em Prefix-independent objectives.}
An objective  is \emph{prefix-independent} if for all plays ,
and all states , we have  if and only if ,
that is the objective is independent of the finite prefixes (of arbitrary length) 
of the plays.

\item {\em Shuffle-closed objectives.} 
A \emph{shuffle} of two plays ,  is a play 
such that   for all , and 
 and .
An objective  is closed under shuffling, 
if all shuffles of all plays  belong to .

\item {\em Multi-mean-payoff objectives.}
Let  be a \emph{reward function}\footnote{We use rational
rewards to be able to state complexity results. All other results in this paper
hold if the rewards are real numbers.} 
that assigns a -dimensional vector of weights to each state. 
For , we denote by  the projection of 
the function  on the -th dimension.
The conjunction of \emph{mean-payoff-inf} objectives (which we refer as generalized 
mean-payoff objectives) is the set  

that contains all plays for which the long-run average of weights (computed as )
is non-negative\footnote{Note that it is not restrictive to define mean-payoff objectives with a threshold 
since we can obtain mean-payoff objectives defined as the long-run average of weights 
above any threshold  by subtracting the constant  to the reward function.} 
in all dimensions. 
The objectives inside the above conjunction (indexed by ) are called 
one-dimensional mean-payoff-inf objectives (in dimension ), and denoted .
The conjunction of \emph{mean-payoff-sup} objectives is the set  
defined analogously, replacing  by  in the definition of .
\end{compactenum}


\begin{remark}\label{rmk:mean-payoff-inf-not-closed-under-shuffling}
It is easy to show that mean-payoff-inf objectives are closed under shuffling,
and that the conjunction of objectives that are closed under shuffling
is closed under shuffling~\cite{Kop06}. However, the conjunctions of mean-payoff-sup
objectives are in general not closed under shuffling~\cite[Example~1]{VCDHRR15}.
\end{remark}


\smallskip\noindent{\bf Probability measures.}
Given an initial state , and a pair of strategies  for  and ,
a finite prefix  of a play is \emph{compatible}
with  and  if  and for all , there exists an action
 such that , 
and either  and ,
or  and .
A probability can be assigned in a standard way to every finite play prefix , 
and by Caratheodary's extension theorem a probability measure  
of objectives can be uniquely defined.  
For MDPs, we omit the strategy of the player with empty set of states, and 
for instance if  we denote by  
the probability measure under strategy  of player~.


\smallskip\noindent{\bf Value and almost-sure winning.}
The optimal \emph{value} from an initial state  of a game with objective  is defined by

By Martin's determinacy result~\cite{Mar98}, the optimal value is also 
,
the infimum probability of satisfying  that player~ can ensure
against all strategies of player~.
In other words the determinacy shows that , and the order
of sup and inf in the quantification of the strategies can be exchanged.

A strategy  for player~ is \emph{optimal} from a state 
if for all strategies  for player~ it ensures that 
. The value (or winning probability) of
a strategy  in state  is . 
We omit analogous definitions for player~.

We say that player~ wins almost-surely from an initial state~ if there exists a strategy  for  such that for every strategy
 of player~ we have .
The state  and the strategy  are called \emph{almost-sure} winning for player~.

\smallskip\noindent{\bf Finite-memory values and almost-sure winning.}
The optimal \emph{finite-memory value} (for player~) is defined analogously, 
when the players are restricted to finite-memory strategies:



A strategy  is \emph{optimal for finite memory} from a state 
if it uses finite memory and for all finite-memory strategies  for player~ it ensures that 
.
We define analogously almost-sure winning with finite-memory strategies,
and the finite-memory value  of  in state~
(against finite-memory strategies of player~). 
We define the finite-memory value for player~ by 

and the finite-memory value of strategy  for player~ by 
.
We show in Theorem~\ref{theo:half-memoryless} for a large class of objectives 
(namely, prefix-independent shuffle-closed objectives) that 
the finite-memory value for player~ and for player~ coincide,
and allowing arbitrary strategies for player~ (against finite-memory 
strategies for player~) does not change the finite-memory value.







\smallskip\noindent{\bf Subgame-perfect strategies.}
Given a strategy  for , and a finite prefix  
of a play, we denote by  the strategy that plays from the initial 
state  what  would play after the prefix , i.e. such that
 for all play prefixes , 
and  is arbitrarily defined for all .

A strategy  for  is \emph{subgame-perfect} if for all nonempty play 
prefixes , the strategy  is optimal from the initial 
state .
Analogously, the strategy  is \emph{subgame-perfect-for-finite-memory}
if all strategies  are optimal-for-finite-memory strategies 
from .




\smallskip\noindent{\bf Value problems.}
Given an objective , a threshold , and an initial state , 
the \emph{value-strategy problem} 
asks whether there exists a strategy  for player~
such that  (or whether there exists a finite-memory 
strategy  for player~ such that ).
The \emph{value problem} asks whether  (resp., whether
).



\smallskip\noindent{\bf End-component lemma.}
An important property of the end-components in MDPs is that for all 
strategies (with finite memory or not) with probability~1 the set
of states that are visited infinitely often along a play is
an end-component~\cite{CY95,deAlfaro97}. 
Given a play , let  be the set of states 
that occur infinitely often in .

\begin{lemma}\label{lem:end-component}
  \cite{CY95,deAlfaro97} Given an MDP , for all states 
  and all strategies , we have
  .
\end{lemma}


\begin{remark}[Key properties for MDPs]\label{rem:key}
The end-component lemma is useful in the analysis of MDPs with 
prefix-independent objectives, which can be decomposed into the analysis of the 
end-components (which have useful connectedness properties), 
and a reachability analysis to the end-components.
Moreover, suppose we consider prefix-independent objectives, and the 
MDP restricted to an end-component . 
Then it follows from the results of~\cite{Cha07b} that either all states of  
have value~1 or all states of  have value~0. 
Hence for prefix-independent objectives in MDPs, the optimal value is the optimal 
reachability probability to the {\em winning} end-components, where a
winning end-component is an end-component with value~1.
\end{remark}






\section{Half-Memoryless Result under Finite-Memory Strategies}\label{sec:half}
We show a general result that gives a sufficient condition for existence of memoryless
strategies (for one of the players) in games played with finite-memory strategies.

\smallskip\noindent{\em Comment on finite- vs. infinite-memory proof.}
The statement and proof structure of the result are similar to~\cite[Theorem~5.2]{GK14}
that established a sufficient condition for existence of memoryless optimal strategies 
in games played with arbitrary (infinite-memory) strategies. 
However, the proof uses different techniques.  
The key to establish the existence of memoryless strategies for one of the 
players is to first establish the existence of subgame-perfect strategies
for the other player.
We establish such a result in Lemma~\ref{lem:subgame-perfect} for finite-memory
strategies. 
Without the restriction of finite memory, only the existence of -subgame-perfect 
strategies is known, and the proof requires intricate arguments and involved mathematical
machinery such as Doob's convergence theorem for martingales~\cite[Theorem~4.1]{GK14}.
Our proof is combinatorial and uses basic results on MDPs (e.g., discrete properties of 
end-components).

\smallskip\noindent{\em Key ideas of the proof.}
The proof of Lemma~\ref{lem:subgame-perfect} consists in constructing from a finite-memory
strategy~ a strategy that is subgame-perfect-for-finite-memory by successively ``improving'' 
the value of the strategy  for each finite prefix . 
Improvements are obtained by modifying some transitions in the transducer defining ,
from the state reached after following the finite prefix . The modification
of transitions does not change the memory space of the strategy, and since we consider 
finite-memory strategies, although there may be infinitely many finite prefixes 
where the strategy needs to be ``improved'', there is only a finite number of memory states
to consider for improvement, which guarantees the improvement process to terminate
and yields a subgame-perfect-for-finite-memory strategy.


\begin{figure}[!tb]
  \begin{center}
    \hrule  height .33pt
    



\begin{picture}(80,43)(0,2)



\gasset{Nw=6,Nh=6,Nmr=3,rdist=1, loopdiam=5}



\drawline[AHnb=0,arcradius=0, linegray=0](20,40)(5,10)
\drawline[AHnb=0,arcradius=0, linegray=0](20,40)(35,10)



\node[Nframe=n](n1)(13,36){}
\node[Nframe=n](n1)(20,42){}
\drawline[AHnb=0,arcradius=0, linegray=0.5](20,40)(19,35)(21,32)

\drawline[AHnb=0,arcradius=0, linegray=0.5](21,32)(18,28)(20,24)(18,20)
\node[Nframe=n](n1)(18,17){}
\drawline[AHnb=0,arcradius=0, linegray=0.5](21,32)(23,28)(24,24)(27,20)(27,16)
\node[Nframe=n](n1)(27,13){}

\node[Nframe=n](n1)(40,5){}

\node[Nframe=n](n1)(53,36){}
\node[Nframe=n](n1)(60,42){}
\drawline[AHnb=0,arcradius=0, linegray=0](60,40)(45,10)
\drawline[AHnb=0,arcradius=0, linegray=0](60,40)(75,10)

\drawline[AHnb=0,arcradius=0, linegray=0.5](60,40)(59,35)(61,32)

\drawline[AHnb=0,arcradius=0, linegray=0.5](61,32)(58,28)(60,24)(58,20)
\node[Nframe=n](n1)(57,16){}
\drawline[AHnb=0,arcradius=0, linegray=0.5](61,32)(63,28)(64,24)(67,20)(67,16)
\node[Nframe=n](n1)(67,13){}

\drawline[AHnb=1,arcradius=0, linegray=0.5, dash={1}0](67,16)(58,20)




















\end{picture}
     \hrule  height .33pt
    \caption{Lemma~\ref{lem:subgame-perfect}: construction of a strategy  
with higher value in subgames than the optimal-for-finite-memory strategy . \label{fig:proof1}}
  \end{center}
\end{figure}



\begin{lemma} \label{lem:subgame-perfect}
In every stochastic game with a prefix-independent objective, there exists a 
subgame-perfect-for-finite-memory strategy for player~.
\end{lemma}
\begin{proof}
Our proof is established using the following key steps:
\begin{compactenum}
\item Existence of an optimal-for-finite-memory strategy for player .

\item Modification of the strategy for improvement of values after
finite prefixes.

\item The proof that the modification provides an 
improvement in two parts: 
once the strategy for player~ is fixed, we have an MDP. 
In the MDP, we first show properties of the end-components, and 
second we provide bounds on the optimal reachability probability to 
the end-components to establish the improvement.

\end{compactenum}

\smallskip\noindent{\em Optimal-for-finite-memory strategy.}
We show the existence of a finite-memory strategy  for player~ 
in the game  such that  is optimal-for-finite-memory from every 
state for the prefix-independent objective~.
The fact that such a strategy always exists is as follows: 
it follows from~\cite[Theorem~4.3]{GH10} that it suffices to prove the result 
for almost-sure winning strategies.
Consider the set  of states with value~1 for finite-memory strategies. 
We need to show that there exists a finite-memory almost-sure winning strategy
in .
Let , and consider a finite-memory strategy that ensures 
value at least  from all states in . 
If a strategy can ensure positive winning from every state of a game, 
then it is almost-sure winning by the result of~\cite{Cha07b}.
The existence of an optimal-for-finite-memory strategy follows.

\smallskip\noindent{\em Notation.}
Consider an optimal-for-finite-memory strategy .
Thus for all states  of the game  there
exists a memory value  in the transducer of  such that the value
of the objective~ in the MDP  is the optimal finite-memory value, 
that is 
where the subscript in  indicates that 
the value is computed in the MDP  (which is a MDP for player~)
while  gives the optimal value for player~ in the game~. 


\smallskip\noindent{\em Modification of the strategy.}
If the strategy  is subgame-perfect-for-finite-memory, then the proof is done.
Otherwise, there exists a state  in 
with value below the optimal finite-memory value of , 
namely such that .
We construct an \emph{improved} strategy  as follows:
the strategy  plays like  except
that when the state  is reached, the strategy  plays
like  is playing from state  (equivalently, we remove the outgoing transitions from state 
in , and replace them by a deterministic transition to state  on all actions
to obtain , as illustrated in \figurename~\ref{fig:proof1}). 
Note that the new strategy  has the same memory set as . We show below that
the value of every state in  is at least as large as the value of the 
same state in  . It follows that the value of state  in 
is the optimal finite-memory value from , and by repeating the same construction
in every state where the value is below the optimal finite-memory value, we obtain
(in finitely many steps) a subgame-perfect-for-finite-memory strategy for player~.

\smallskip\noindent{\em Proof of .}
We proceed with the proof of , which has two steps as mentioned above.
We first define the notion of value class. 
\smallskip\noindent{\em Value class and properties.}
In the MDP , a \emph{value class} is a maximal subset of states that have 
the same value (defined as the infimum over the strategies of player~).
The following property holds in , for every state , and action :
consider the value class of , if there is an -successor of  
in a lower value class, then there is also an -successor of  in a 
higher value class (\figurename~\ref{fig:proof2}). 
If we consider the partition defined by the value classes
in , this property also holds in the modified MDP  
corresponding to strategy , because the new deterministic transition 
(dashed edge of  \figurename~\ref{fig:proof1}) goes to a higher value class.

\smallskip\noindent{\em Properties of end-components.}
Now, we claim that in the modified MDP  every end-component
is included in some value class (of the original MDP ).
We show this by contradiction (see also \figurename~\ref{fig:proof2}). 
Assume that there is an end-component  in 
with non-empty intersection with different value classes (of the original MDP ). 
Let  be a state of  with largest value. Since  is strongly connected,
there is a path from  to a lower value class, and on this path there is a state  
with largest value that has an -successor  with lower value (for some ). 
It follows that  has also an -successor with higher value, according to the above property. This
successor is outside  since there is no larger value class in  than the value class of . 
This is in contradiction with the fact that end-components are closed sets (and that ). 
We conclude that in  every end-component
is included in some value class (of the original MDP ).
Therefore, the value of each end-component in  is at least as large
as the value of the value class containing it (in ). 
It also follows that the new deterministic transitions from  to 
 do not belong to any end-component in .

\begin{figure}[!tb]
  \begin{center}
    \hrule  height .33pt
    

\renewcommand{\sb}[1]{\scalebox{0.75}[1]{#1}}

\begin{picture}(80,46)(0,2)



\gasset{Nw=6,Nh=6,Nmr=3,rdist=1, loopdiam=5}



\node[Nmarks=n,Nw=15,Nh=30,Nmr=0](n1)(10,26){}
\node[Nframe=n,Nw=20,Nh=10,Nmr=0](nl)(10,6){\begin{tabular}{c}lowest\\value class\end{tabular} }
\node[Nframe=n,Nw=0,Nh=0,Nmr=0](nC)(-1,40){}
\node[Nmarks=n,Nw=15,Nh=30,Nmr=0](n1)(30,26){}
\node[Nmarks=n,Nw=15,Nh=30,Nmr=0](n1)(50,26){}
\node[Nmarks=n,Nw=15,Nh=30,Nmr=0](n1)(70,26){}
\node[Nframe=n,Nw=20,Nh=10,Nmr=0](nh)(70,6){\begin{tabular}{c}highest\\value class\end{tabular} }
\drawedge[ELpos=50, ELside=l, ELdist=.5, curvedepth=0, linegray=0.5, dash={1}0](nl,nh){increasing}
\drawedge[ELpos=50, ELside=r, ELdist=.5, curvedepth=0, linegray=0.5, dash={1}0](nl,nh){value class}


\node[Nframe=n,Nmarks=n,Nw=0,Nh=0,Nmr=0, ExtNL=y, NLangle=60, NLdist=.5](n1)(54,20){}
\rpnode[Nmarks=n](np)(50,14)(4,1){}
\node[Nframe=n,Nw=3,Nh=3,Nmr=1.5](n2)(70,15){}
\node[Nframe=n,Nw=3,Nh=3,Nmr=1.5](n3)(30,15){}

\drawedge[ELpos=50, ELside=l, curvedepth=0](n1,np){}
\drawedge[ELpos=50, ELside=l, curvedepth=-5](np,n2){}
\drawedge[ELpos=50, ELside=l, curvedepth=5](np,n3){}



\node[Nmarks=n,Nw=43,Nh=14,Nmr=0, linegray=0.5, ExtNL=y, NLangle=162, NLdist=1](nC)(35,38){}

\rpnode[Nmarks=n, ExtNL=y, NLangle=270, NLdist=1](ny)(46,39)(4,1){}
\node[Nframe=n,Nw=0,Nh=0,Nmr=0](nL)(31,37){}
\node[Nframe=n,Nw=3,Nh=3,Nmr=1.5](nR)(60.5,43.5){}

\node[Nframe=n,Nw=0,Nh=0,Nmr=1.5, ExtNL=y, NLangle=270, NLdist=1](ndummy)(48,35.5){}
\drawedge[AHnb=0, ELpos=50, ELside=l, curvedepth=0, linegray=0.5](ny,ndummy){}
\drawline[AHnb=0, arcradius=0, linegray=0.5](48,35.5)(50,38)(52,36)(55,38)
\node[Nframe=n,Nw=0,Nh=0,Nmr=1.5](nx)(55,39.5){}
\drawline[AHnb=0, arcradius=0, linegray=0.5](31,37)(28,38)(26,35)(24,36)
\node[Nframe=n,Nw=0,Nh=0,Nmr=1.5](nx)(31,35.5){}

\drawedge[ELpos=50, ELside=l, curvedepth=-4](ny,nL){}
\drawedge[ELpos=50, ELside=l, curvedepth=2.5](ny,nR){}













\end{picture}
     \hrule  height .33pt
    \caption{Lemma~\ref{lem:subgame-perfect}: value-class analysis. No end-component  can lie across several value classes.\label{fig:proof2}}
  \end{center}
\end{figure}


\smallskip\noindent{\em Optimal reachability probability.} 
The key steps to obtain the bound on optimal reachability probability is as follows:
we observe that the optimal reachability probability in MDPs is characterized by a minimizing 
linear-programming solution, and we show that the solution before the modification 
is a feasible solution after the modification.
We now present the details.

\smallskip\noindent{\em Optimal value via optimal reachability.}
We show that the value of the state  in  is 
strictly greater than the value of  in  (for player~).
Let  be the union of all end-components in  with value 
for the prefix-independent objective~ (thus losing for player~, and winning for player~).
By Remark~\ref{rem:key}, the optimal value for player~ in the MDP
is the optimal reachability probability to . 


\smallskip\noindent{\em Optimal reachability probability to .}
Consider the following linear program in  
that computes the value (for player~) of each state  of  in variable ,
by solving a reachability problem to the states in : 
\begin{itemize}
\item[] minimize 
\item[]  for all 
\item[]  for all 
\end{itemize}
The correctness of the linear program to compute optimal reachability probability 
is standard~\cite{FV97}.
Let  be an optimal solution of this linear program. Note that the values
are computed for player~, and thus .
It follows that .

\smallskip\noindent{\em Feasible solution.}
Consider the modified MDP  (with same state space as ), 
in which the union of end-components with value  is contained in . 
Therefore, considering the same linear program for  provides 
an upper bound on the new value (for player~).
For each , define 

\noindent Then  is a feasible solution to the linear program for ,
and for the optimal solution , we have  (and
for  we have ). Since 
 is only an upper bound of the new value of  for player~ in ,
it shows that the value improved for player~ in every state.  
Since the value of  in  was the optimal finite-memory value,
it follows that in  the value of  is also the optimal finite-memory value.
Since all transitions of  lead to , the value
of  in  is the optimal finite-memory value from ,
which concludes the proof of~.
\end{proof}


The result of~\cite[Theorem~5.2]{GK14} shows that in games where the players 
are allowed to use arbitrary strategies (thus not restricted to finite-memory 
strategies), memoryless optimal strategies exist for player~ if the objective 
of player~ is prefix-independent and closed under shuffling. 
The proof of this result uses an analogue of Lemma~\ref{lem:subgame-perfect}
for arbitrary strategies, and relies on edge induction, a technique that
became standard~\cite{CD12,GK14,GZ05,Kop06}. 
The shape of the argument is not specific to games with arbitrary strategies:
in games where the players are restricted to finite-memory strategies, 
we can follow the same line of proof (using Lemma~\ref{lem:subgame-perfect}) 
to show that if the objective of a player is prefix-independent and 
closed under shuffling, then memoryless optimal strategies exist for the other player.


\begin{theorem}\label{theo:half-memoryless}
In stochastic games, if the objective  of player~ is prefix-independent and
closed under shuffling, and player~ is restricted to finite-memory strategies,
then player~ has a memoryless optimal-for-finite-memory strategy (as well as a 
memoryless optimal strategy), and determinacy holds under finite-memory strategies.
More precisely, for all states  we have:


\end{theorem}

\smallskip\noindent{\em Significance of Theorem~\ref{theo:half-memoryless}.}
We first remark on the significance of the result, and then present the main steps of the 
proof.
First, the result establishes determinacy for finite-memory strategies 
i.e., , which implies that even for finite-memory strategies
the order of sup and inf can be exchanged. 
However, note that the finite-memory value is different from the value under infinite-memory 
strategies, and the determinacy for finite-memory does not follow from the determinacy
for infinite-memory strategies.
Second, 
implies that as long as player~ is restricted to finite-memory strategies, whether 
player~ uses finite-memory or infinite-memory strategies does not matter.
Finally, 
implies that against finite-memory strategies of player~ there exists a pure memoryless
strategy for player~ that is optimal (even considering all infinite-memory strategies 
for player~). 



\smallskip\noindent{\em Main steps of the proof.}
We present the key steps of the proof of Theorem~\ref{theo:half-memoryless}, 
and we show that the argument in the proof of~\cite[Theorem~5.2]{GK14} (which we refer to 
for the precise technical steps) can be adapted for finite-memory strategies.
The key steps are: 
(i)~induction on the number of player- states;
(ii)~creating different games for different choices at a player- state, in which
player~ has memoryless optimal strategies by induction hypothesis;
and (iii)~showing the value of the original game is at least the minimum of the 
value of the different games, thus memoryless strategies suffice.

\smallskip\noindent{\em Induction on player- states.}
The proof is by induction on the number of states of player~. The base case
 corresponds to games with only states of player~. 
The result holds trivially in that case (the empty strategy of player~ 
is memoryless). For the induction step, assume that the result holds for all games
with , and consider a game  with . 

\smallskip\noindent{\em Different games for different choices.}
We explain the rest of the proof assuming the action set contains only two actions, 
that is . The proof is the same for an arbitrary finite set of actions, 
with more complication in the notation. 
In , consider a state  of player~ and construct
two games  and  obtained from  by removing  and by
replacing the incoming transitions to  by transitions to its -successors
and -successors respectively. The transition function of  (for )
is defined by 
for all , and all actions .

\smallskip\noindent{\em Value of original game at least the minimum of the value 
of the two games.}
In  and  the number of states of player~ is . 
Hence by the induction hypothesis there exist memoryless strategies  and 
for player~ that are optimal-for-finite-memory (as well as optimal among the infinite-memory strategies) 
in  and  respectively.
The proof proceeds by showing that in the game , player~ cannot obtain
a lower (i.e., better) value than in one of the games  or ,
that is for all strategies  of player~, for all states 
we have\footnote{We assume that the value  of 
a strategy  is computed in the game  in superscript.}:

To show this, we consider subgame-perfect-for-finite-memory strategies  and  
for player~ in games  and  respectively (which exist by Lemma~\ref{lem:subgame-perfect}), 
and we construct
a finite-memory strategy  in  that achieves, against all strategies 
, a value at least as large as either  in  or  
in . Intuitively,  switches between  and , 
playing according to  when in the last visit to  player~ 
played action  (thus as in ),
and playing according to  when in the last visit to  player~ 
played action  (thus as in ). 
To formally define , given a play prefix in  we use projections 
onto plays in  (resp., ) that erase all sub-plays between
successive visits to  where action~ (resp., action~) was played in .
Note that  uses finite memory.
The plays compatible with  and  are shuffles of plays compatible 
with  in  and plays compatible with  in ,
and since the objective  is closed under shuffling, the probability
measure of the plays satisfying the objective in  is no lower than
the value of either games  or :

It follows that \eqref{eq:value} holds, and thus the optimal-for-finite-memory (as well as optimal among 
infinite-memory strategies) strategies 
in the games  and  (extended to play  and  respectively in ) are sufficient
for player~ in . Therefore by the induction hypothesis, memoryless strategies
are sufficient for player~ to achieve the optimal finite-memory value, let 
 be such a strategy.
By the same argument and using the induction hypothesis, for the
finite-memory strategy  for player~ in  we have 
, 
which gives .
Note that our proof handled that the strategies for player~ are allowed to be 
infinite-memory, and the result still holds.


\begin{figure}[!tb]
  \begin{center}
    \hrule  height .33pt
    



\begin{picture}(30,12)(0,1)



\gasset{Nw=6,Nh=6,Nmr=3,rdist=1, loopdiam=5}




\node[Nmarks=n](n1)(5,8){}
\node[Nmarks=n, ExtNL=y, NLangle=270, NLdist=1](n1)(5,8){}
\node[Nmarks=r, Nmr=0](n2)(25,8){}
\node[Nmarks=r, Nmr=0, ExtNL=y, NLangle=270, NLdist=1](n2)(25,8){}


\drawloop[ELside=l, loopCW=y, loopangle=180](n1){}
\drawedge[ELpos=50, ELside=l, curvedepth=4](n1,n2){}

\drawedge[ELpos=50, ELside=l, curvedepth=4](n2,n1){}
\drawloop[ELside=l, loopCW=y, loopangle=0](n2){}















\end{picture}
     \hrule  height .33pt
    \caption{A game with prefix-independent objective  that is not determined under finite-memory strategies.\label{fig:not-determined}}
  \end{center}
\end{figure}


\begin{remark}\label{rem:nodet}
The determinacy result of Theorem~\ref{theo:half-memoryless},
which allows to switch the  and  operators ranging over finite-memory
strategies, is true for prefix-independent shuffle-closed objectives.
We present an example to show that such a result does not hold  
for general prefix-independent objectives that are not closed under shuffling. 
Consider the game of \figurename~\ref{fig:not-determined}, 
with the objective  
where  is the set of plays that visit  infinitely often, 
and  is the set of plays that eventually stay in  forever. 
Note that the game is even non-stochastic.
We show that  and . Intuitively, 
after either player fixed a finite-memory strategy, the other player can win
using slightly more memory than the first player (but still finite memory).
For all finite-memory strategies  of player~, either  there exists a compatible
play that eventually stays forever in , and then the objective 
is violated, or   is visited infinitely often in all compatible plays 
and player~ can ensure with a finite-memory strategy that both objectives  and 
 are violated by staying in  one more time than player~ 
stayed in , and then going back to . It follows that .
Analogously, against all finite-memory strategies  of player~,
player~ can ensure that the objective  is satisfied (by staying
in  one more time than player~ stayed in , and then going to ), thus 
. Hence  and the
game of \figurename~\ref{fig:not-determined} is not determined under finite-memory strategies.
\end{remark}




\smallskip\noindent{\em Upper bound on memory.} 
We now show that for prefix-independent shuffle-closed objectives, 
the memory required for player~ is exponential as compared
to the memory required for the same objective in MDPs.
If there are  states for player~, then the optimal-for-finite-memory 
strategy  constructed for player~ in the proof of Theorem~\ref{theo:half-memoryless}
is as follows: it considers strategies in the choice-fixed games 
( and ) with  states for player~, and the strategy in 
the original game considers projections of plays and then copies the strategies 
of the choice-fixed games. 
Thus the memory required for player~ in games with  states for player~ is the union
of the memory required for the choice-fixed games with  states, and there 
are at most  such choice-fixed games.
If we denote by  the memory required for player~ in games with  
player- states, then the following recurrence is satisfied:

Note that  represents the memory bound for MDPs, and thus we get a bound on 
 in games that is greater than the 
memory bound for MDPs by an exponential factor.



\begin{theorem}\label{theo:membou}
In stochastic games with a prefix-independent shuffle-closed objective , 
an upper bound on the memory required for optimal-for-finite-memory strategies
is , where  is an upper bound on memory
required for objective  in MDPs.
\end{theorem}



\section{Generalized Mean-Payoff Objectives under Finite-Memory Strategies}\label{sec:finmem}

In generalized-mean-payoff games, infinite-memory strategies are more powerful 
than finite-memory strategies, even in 1-player games with only deterministic 
transitions, i.e., graphs~\cite[Lemma~7]{VCDHRR15}.\footnote{In the example of~\cite[Lemma~7]{VCDHRR15}
all finite-memory strategies have winning probability~0 while there exists an almost-sure 
winning strategy (with infinite memory).}
It follows that in general  in generalized-mean-payoff games
(for both  and ).
In this section, we consider the value problem for finite-memory
strategies, and present complexity results showing that the problem is in PTIME for
MDPs, and is coNP-complete for games.
Finally we present optimal bounds for memory required in 2\half-player games.



\subsection{Generalized mean-payoff objectives under finite-memory in MDPs}

We consider the value problem for finite-memory strategies
in MDPs with generalized mean-payoff objectives. 
First we show that randomized memoryless strategies are as powerful as 
finite-memory strategies, and then using this result we show that the 
value problem can be solved in polynomial time.

Note that in finite-state Markov chains with a fixed reward function, from all states , 
the probability that the conjunction  of mean-payoff-sup objectives holds from  is 
the same as the probability that the conjunction  of mean-payoff-inf objectives holds 
from ~\cite{FV97}. It follows that in MDPs with finite-memory strategies, the 
value for mean-payoff-sup and mean-payoff-inf objectives coincides, thus
 for all states .


\smallskip\noindent{\em Key ideas.}
Let  be an MDP and 
be a reward function. The key ideas to show that randomized memoryless strategies
are sufficient for generalized mean-payoff objectives are: (i)~first observe that the mean-payoff value of a play depends only on the frequency of 
occurrence of each state, (ii)~under finite-memory strategies the frequencies are well defined 
(with probability~1) for each state and action, and (iii)~given the frequencies of a
finite-memory strategy, a randomized memoryless strategy that plays at every state an 
action with probability proportional to the given frequencies achieves the same frequencies
as the finite-memory strategy.


\smallskip\noindent{\em Definition of frequencies.}
In this paragraph, we assume that plays are sequences of alternating
states and actions, and the probability measure 
is (uniquely) defined over Borel sets of such sequences.
Given a play , let  be the 
number of occurrences of state  in  up to position , and
let  be the number of occurrences of the pair  of state 
followed by action  in  up to position~.
The frequency  of a state  in  and 
the frequency  of a pair  are:
 


\smallskip\noindent{\em Mean-payoff values from frequencies.}
It is easy to see that if the frequency  is defined, then 
the mean-payoff values 
of  (for ) can be computed from the values of 
as follows: .

\smallskip\noindent{\em Almost-sure existence of frequencies.}
We note that in finite-state Markov chains (and in finite-state MDPs with finite-memory
strategies) the frequencies are defined with probability~1~\cite{FV97}:

Given a finite-memory strategy  in an MDP , consider a closed recurrent set 
of the Markov chain  obtained by playing  in 
\footnote{The state space of  is the Cartesian product of the set
of states of  and of the memory set of . However, 
we say that  if  is a state of  for some memory value  of .}. 
Then, given  the frequencies are fixed in , that is there exist (unique) frequency 
vectors  and  such that:
+3pt]
\prob{s_0}^{\straa}(\{\rho \mid f_s^\rho = \lambda(s) \text{ for all } s \in B \}) = 1.
\end{array}

\prob{s_0}^{\straa'}(\{\rho \mid \frac{f_{s,a}^\rho}{f_s^\rho} = \mu(s,a) \text{ for all } s \in B \text{ and } a \in A \}) = 1

\prob{s_0}^{\straa'}(\{\rho \mid f_s^\rho = \lambda(s) \text{ for all } s \in B \}) = 1.

\prob{s_0}^{\bar{\straa}}(\{\rho \mid \frac{f_{s,a}^\rho}{f_s^\rho} = \mu(s,a) \text{ for all } s \in B \text{ and } a \in A \}) = 1,

\prob{s_0}^{\bar{\straa}}(\{\rho \mid f_s^\rho = \lambda(s) \text{ for all } s \in B \}) = 1.
-6pt]

\item  (component-wise) \label{n2} \-6pt]

\item for each  and :  \label{n4} 

\end{enumerate}
\renewcommand{\theenumi}{\arabic{enumi}}
The equations~(E\ref{n1}) above express that in every state, the incoming frequency is
equal to the outgoing frequency. Equation~(E\ref{n2}) ensures that the mean-payoff
value is nonnegative (in all dimensions). Equations~(E\ref{n3}) and~(E\ref{n4}) 
require that the frequencies are nonnegative and sum up to~.

\smallskip\noindent{\em Illustration.}
In the example of \figurename~\ref{fig:frequency}, a solution to the linear program 
gives for instance  and , which corresponds
to a randomized memoryless strategy that chooses from  to go to  with 
probability  and to go to  with probability
. This strategy satisfies the conjunction of mean-payoff objectives 
with probability~1 (it ensures that the long-run average of the rewards is 
in both dimensions).

\smallskip\noindent{\em Issues regarding connectedness.}
Arguments similar to the proof of~\cite[Theorem 2.2]{KS88} show that the linear
program  has a solution if and only if there exists a union of end-components in 
and associated frequencies with nonnegative sum of rewards. However, this union
of end-components need not to be connected and thus may not be an end-component
(see \figurename~\ref{fig:frequency-not-connected} where the union of 
the end-components  and  corresponds to a solution of ).
Note that connectedness is not an issue for infinite-memory strategies: 
in the example of \figurename~\ref{fig:frequency-not-connected} there exists 
an infinite-memory strategy to ensure the mean-payoff objectives with probability~1 
(see~\cite[Lemma~7]{VCDHRR15}).

\smallskip\noindent{\em Ensuring connectedness and frequencies.}
To find single end-components with nonnegative sum of rewards, we adapt a 
technique presented in~\cite[Section~3]{KS88}. Construct a graph  with set  of vertices,
and for each pair , if the linear program  
has a solution, add edges  in  for all -successors  of .   If the graph  is strongly connected, then it defines an end-component with nonnegative
sum of rewards in . Otherwise, consider the maximum-scc decomposition of , 
and iterate the algorithm in each scc, until the state space reduces to one element. 
The algorithm identifies in this way all (maximal) winning end-components and
arguments similar to~\cite[Theorem~3.3]{KS88} show that this algorithm runs in polynomial
time, as the recursion depth is bounded by the number of states, and the scc 
decomposition ensures that the graphs in each recursive call of a given depth 
are disjoint.




\begin{figure*}[!t]
  \centering
  \hrule  height .33pt




{\scriptsize 
\begin{picture}(115,28)(0,2)



\gasset{Nw=6,Nh=6,Nmr=3,rdist=1, loopdiam=6}
\gasset{Nw=5,Nh=5,Nmr=2.5,rdist=1, loopdiam=6, linewidth=0.12}


\node[Nmarks=n, Nmr=0](n1)(5,14){}
\node[Nmarks=n, Nmr=0](n1L)(13,19){}
\nodelabel[ExtNL=y, NLangle=90, NLdist=1](n1L){\sb{}}
\node[Nmarks=n, Nmr=0](n1R)(13,9){}
\nodelabel[ExtNL=y, NLangle=270, NLdist=1](n1R){\sb{}}
\node[Nmarks=n, Nmr=0](n2)(21,14){}

\node[Nframe=n, Nmr=0](n2L)(30,19){}
\node[Nframe=n, Nmr=0](n2R)(30,9){}

\drawedge[ELpos=50, ELside=l, curvedepth=0](n1,n1L){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](n1,n1R){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](n1L,n2){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](n1R,n2){}
\drawedge[ELpos=50, ELside=l, curvedepth=0, dash={.3 .5}0](n2,n2L){}
\drawedge[ELpos=50, ELside=l, curvedepth=0, dash={.3 .5}0](n2,n2R){}

\node[Nframe=n](dots)(29,14){}

\node[Nframe=n, Nmr=0](n2L)(28,19){}
\node[Nframe=n, Nmr=0](n2R)(28,9){}

\node[Nmarks=n, Nmr=0](n2)(37,14){}
\drawedge[ELpos=50, ELside=l, curvedepth=0, dash={.3 .5}0](n2L,n2){}
\drawedge[ELpos=50, ELside=l, curvedepth=0, dash={.3 .5}0](n2R,n2){}

\node[Nmarks=n, Nmr=0](n2L)(45,19){}
\nodelabel[ExtNL=y, NLangle=90, NLdist=1](n2L){\sb{}}
\node[Nmarks=n, Nmr=0](n2R)(45,9){}
\nodelabel[ExtNL=y, NLangle=270, NLdist=1](n2R){\sb{}}
\node[Nmarks=n, Nmr=0](n3)(53,14){}

\drawedge[ELpos=50, ELside=l, curvedepth=0](n2,n2L){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](n2,n2R){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](n2L,n3){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](n2R,n3){}






\node[Nmarks=n](t1)(65,14){}
\node[Nmarks=n](t1L)(73,19){}
\nodelabel[ExtNL=y, NLangle=90, NLdist=1](t1L){\sb{}}
\node[Nmarks=n](t1R)(73,9){}
\nodelabel[ExtNL=y, NLangle=270, NLdist=1](t1R){\sb{}}
\node[Nmarks=n](t2)(81,14){}

\node[Nframe=n](t2L)(89,19){}
\node[Nframe=n](t2R)(89,9){}


\drawedge[ELpos=50, ELside=l, curvedepth=0](n3,t1){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](t1,t1L){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](t1,t1R){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](t1L,t2){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](t1R,t2){}
\drawedge[ELpos=50, ELside=l, curvedepth=0, dash={.3 .5}0](t2,t2L){}
\drawedge[ELpos=50, ELside=l, curvedepth=0, dash={.3 .5}0](t2,t2R){}



\node[Nframe=n](dots)(89,14){}

\node[Nframe=n](t2L)(88,19){}
\node[Nframe=n](t2R)(88,9){}

\node[Nmarks=n](t2)(97,14){}
\drawedge[ELpos=50, ELside=l, curvedepth=0, dash={.3 .5}0](t2L,t2){}
\drawedge[ELpos=50, ELside=l, curvedepth=0, dash={.3 .5}0](t2R,t2){}

\node[Nmarks=n](t2L)(105,19){}
\nodelabel[ExtNL=y, NLangle=90, NLdist=1](t2L){\sb{}}
\node[Nmarks=n](t2R)(105,9){}
\nodelabel[ExtNL=y, NLangle=270, NLdist=1](t2R){\sb{}}
\node[Nmarks=n](t3)(113,14){}

\drawedge[ELpos=50, ELside=l, curvedepth=0](t2,t2L){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](t2,t2R){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](t2L,t3){}
\drawedge[ELpos=50, ELside=l, curvedepth=0](t2R,t3){}

\drawline[AHnb=1,arcradius=1](113,16.5)(113,28)(5,28)(5,16.5)










\end{picture}
}
   \hrule  height .33pt
    \caption{A family of generalized mean-payoff games where player~ (round states) needs 
	exponential memory to win almost-surely (and finite memory is sufficient).\newline \label{fig:exponential}} 
\end{figure*}



\begin{theorem}\label{theo:mdp} 
The following assertions hold for MDPs with generalized mean-payoff objectives
:
\begin{enumerate}

\item There exists a randomized memoryless strategy  such that 
 for all states  
(i.e., randomized memoryless optimal strategies wrt. to finite-memory strategies).

\item The value and value-strategy problems for generalized mean-payoff MDPs 
under finite-memory strategies (i.e., whether )
can be solved in polynomial time.
\end{enumerate}
\end{theorem}

\noindent{\em Insufficiency of pure memoryless strategies.}
While we show that randomized memoryless strategies are sufficient, 
the example of \figurename~\ref{fig:frequency} shows that pure memoryless
strategies are not sufficient to achieve the optimal finite-memory value:
from , a pure memoryless strategy can either choose  and then the
mean-payoff value in the first dimension is , or choose 
and then the mean-payoff value in the second dimension is . 
Thus for all pure memoryless strategies, the generalized mean-payoff objective is
violated with probability~1 although there exists an almost-sure
winning \emph{randomized} memoryless strategy (see the paragraph {\em Illustration} 
after Lemma~\ref{lem:randomized-memoryless}).



\subsection{Generalized mean-payoff objectives under finite-memory in 2\half-player games}

We present a result analogous to Theorem~\ref{theo:half-memoryless} 
for generalized mean-payoff stochastic games showing that memoryless strategies are sufficient
for player~ against finite-memory strategies. Note that the result extends Theorem~\ref{theo:half-memoryless}
as mean-payoff-sup objectives are not closed under shuffling 
(Remark~\ref{rmk:mean-payoff-inf-not-closed-under-shuffling}).

\begin{theorem}\label{theo:half-memoryless-mean-payoff}
In stochastic games with objective , 
there exists an optimal-for-finite-memory strategy for player~, there exists a memoryless optimal-for-finite-memory strategy
for player~, and determinacy holds under finite-memory strategies, that is for all states :


\end{theorem}

\begin{proof}
For mean-payoff-inf objectives () the result follows
from Theorem~\ref{theo:half-memoryless}.
We consider mean-payoff-sup objectives ().
Once a finite-memory strategy for player~ is fixed we have an MDP
for player~, with a disjunction of mean-payoff objectives.
We now analyze the MDP problem.
Since the objective is prefix-independent, by Remark~\ref{rem:key}, 
every end-component has value either~1 or~0. 
It follows that in every end-component with value~1, one of the mean-payoff 
objectives is satisfied with value~1, for which positional strategies are 
sufficient~\cite{FV97,Puterman}.
The optimal reachability to winning end-components is also achieved by
positional strategies. 
Given the existence of positional strategies for player~ in MDPs, we
consider the game problem.
It follows that once a finite-memory strategy for player~ is fixed, the 
counter-strategy for player~ is also finite-memory, and for finite-memory
strategies mean-payoff-sup and mean-payoff-inf objectives coincide.
The desired result follows.
\end{proof}


It follows that the value problem for generalized mean-payoff games
with finite-memory strategies can be solved in coNP by guessing a memoryless
strategy for player~ and checking whether the value of the resulting
MDP under finite-memory strategies for player~ is above the given threshold, 
which can be done in polynomial time (Theorem~\ref{theo:mdp}).
By the result of~\cite[Lemma~5, Lemma~6]{VCDHRR15}, the problem of deciding
the existence of a finite-memory almost-sure winning strategy for player~ in a game 
(even with only deterministic transitions) with a conjunction of mean-payoff-sup 
or mean-payoff-inf objectives is coNP-hard. 
Theorem~\ref{theo:finite-coNP-complete} summarizes the results of this section.



\begin{theorem}\label{theo:finite-coNP-complete}
The value and value-strategy problems for stochastic games
with generalized mean-payoff-(inf or sup) objectives 
played with finite-memory strategies for player~ 
(and finite- or infinite-memory strategies for player~) 
are coNP-complete.
\end{theorem}




\subsection{Memory bounds for strategies in 2\half-player games}\label{sec:membou}
We present both exponential lower bound and upper bound on memory of strategies.

\smallskip\noindent{\bf Lower bound.}
We show that in games where finite memory is sufficient to win almost-surely 
a conjunction of mean-payoff objectives, exponential memory is necessary in general,
even with randomized strategies. There exists a 
family of generalized mean-payoff games in which player~ has a finite-memory almost-sure winning 
strategy, and every almost-sure winning strategy needs exponential memory.
The family of games is illustrated in \figurename~\ref{fig:exponential},
and is essentially the same family used in the proof of~\cite[Lemma 8]{CRR14}
(for 2-player games with pure finite-memory strategies). 

\smallskip\noindent{\em Lower bound family.}
For , the -th game in the family consists of  gadgets where
the first  gadgets belong to player~, and the last  gadgets belong to 
player~, for a total of  states (\figurename~\ref{fig:exponential}). 
The action set is  and the unique -successor of  is ,
the unique -successor of  is , and similarly for -successors.
Thus all transitions in the games are deterministic.
The reward function has dimension . All states have reward  in all dimensions 
except the following states and dimensions:
\begin{compactitem}
\item[] 
\item[] 
\end{compactitem}
An almost-sure winning strategy for player~ in this game is to copy in 
every state  the choice of player~ in state , namely choosing 
 if player~ has chosen , and choosing  if player~ 
has chosen . This strategy ensures that the sum of rewards in all 
dimensions is bounded (in the interval ) and thus the mean-payoff value
is  in all dimensions. The memory needed to describe this strategy requires
 bits to remember the last  choices of player~, thus a memory set of 
size .

\begin{lemma}\label{lem:exponential}
There exists a family of games  with  states and generalized mean-payoff
objective of dimension  such that a finite-memory almost-sure winning strategy exists,
and all almost-sure winning randomized strategies require memory of size at least .
\end{lemma}

\begin{proof}
We show that no randomized strategy with memory of size less than  
is almost-sure winning in the family of games  presented above (see \figurename~\ref{fig:exponential}). 
The proof is by contradiction. Consider a strategy 
with memory of size less than , and show that  is not 
almost-sure winning. Since  has memory of size less than ,
after every visit to , it is possible for player~ to make two
different sequences of choices until reaching , such that from 
the strategy  of player~ behaves identically because it is in the same memory state. 
Formally, for all play prefixes  with , 
there exist two play prefixes  with 
such that ,
where  is the initial memory and  is the update function
of the strategy .
We can view  and  as sequences of  bits taking value 
or . Since , there is an index  where
the -th bit differs in  and ; we say that there is a star at
position  after prefix . Intuitively, the star represents the possibility
for player~ to pick either  or  at position  (by playing according to
 or ) without affecting the future choices of player~.
Note that the choice of  or  at position  changes the reward in 
dimensions  and  (respective rewards  and  or  and ) and
possibly in other dimensions.
Consider the (finite-memory) strategy for player~ that for each such 
prefix  plays as prescribed by , 
and consider the Markov chain obtained by fixing the strategies 
and  in the game . Consider a closed recurrent set in this Markov chain,
and compute for each index  the frequency of occurrence of a star
(the frequencies are well-defined because the Markov chain has finitely many states).
It is easy to see that for some index  this frequency is greater than  (in fact,
at least ). Let  be the frequency of occurrence of a star at 
position ,
relative to the frequency of occurrence of state .
Given the index , we consider the dimensions  and . 
In the closed recurrent set, we can analogously compute the relative frequency of the left and right
choices at  and  (without counting the choices when there is a star at position ), 
thus computing the frequency  of rewards 
in dimensions  and , the frequency  of rewards , 
the frequency  of , and  of . 
Thus we have  and . Since ,
it follows that either  or , that is
either in dimension  or , the expected reward can be made negative 
by choosing either always the choice  or always the choice  at all states that have 
a star at position  (remember that this modifies the strategy  of player~, 
but does not affect the choices made by the strategy  of player~ along
the play). It follows that almost-surely the mean-payoff value 
is negative in that dimension, showing that  is not almost-surely winning.

It follows that exponential memory is necessary in general to win almost-surely 
a generalized mean-payoff game (even when finite memory is sufficient).
\end{proof}

\smallskip\noindent{\bf Upper bound.} 
Theorem~\ref{theo:membou} and Theorem~\ref{theo:mdp} establish an  
upper bound on memory required for optimal-for-finite-memory strategies. 
Thus we obtain the following result.

\begin{theorem}\label{theo:memgenmean}
The optimal bound for memory required for optimal-for-finite-memory strategies for 
player~ in generalized mean-payoff stochastic games is exponential.
\end{theorem}



\begin{comment}
\smallskip\noindent{\em Lower bound dependent on weights.}
The family of examples in Lemma~\ref{lem:exponential} has unbounded number of states,
and unbounded dimension of the reward function. However, the rewards are bounded 
(in ). There exist games where the memory needed to win almost-surely
is exponential, but the number of states and the dimension are fixed. Thus the
memory depends exponentially only on the numbers in the reward function: in the
example of \figurename~\ref{fig:exponential-numbers}, player~ needs to stay
 times in  (resp., ) if previously player~ visited  (resp., ),
which requires exponential memory (as the number  is encoded in binary).


\smallskip\noindent{\bf Open question.}
The question whether exponential memory is sufficient for optimal-for-finite-memory 
strategies remains open.
Currently we do not have any explicit upper bound. 
\end{comment}






\section{Generalized Mean-Payoff Objectives under Infinite-Memory Strategies}\label{sec:infmem}

In this section, we consider games with a conjunction of mean-payoff objectives
and infinite-memory strategies for player~ (which are more powerful than 
finite-memory strategies~\cite[Lemma~7]{VCDHRR15}).








\subsection{ objectives}

Since  objectives are prefix-independent and closed under shuffling, 
it follows from the results of~\cite[Theorem~5.2]{GK14}
that for player~ memoryless optimal strategies exist. Therefore the
value and value-strategy problems can be solved in coNP by guessing a (optimal)
memoryless strategy for player~, and then solving an MDP with conjunction
of mean-payoff objectives under infinite-memory strategies, which can be done in 
polynomial time by the result of~\cite[Section~3.2]{BBCFK14}.
A matching coNP-hardness bound is known for 2-player games~\cite[Theorem~7]{VCDHRR15}.

\begin{theorem}\label{theo:mean-payoff-inf-infinite}
The value and the value-strategy problems for stochastic games 
with generalized mean-payoff-inf objectives under infinite-memory 
strategies 
are coNP-complete.
\end{theorem}




\subsection{ objectives}

We focus on the \emph{almost-sure winning problem} for generalized mean-payoff objectives, 
which is to decide whether there exists an almost-sure winning strategy 
for player~ from a given state.
We show that the almost-sure winning problem is in NP~~coNP for a conjunction
of mean-payoff-sup objectives.

\begin{remark}\label{rem:red-almost-sure}
As mentioned in~\cite[Remark~1]{CDGO14}, it follows from the results of~\cite[Lemma~7]{CHH09} 
and~\cite[Theorem~4.1]{GH10} that since mean-payoff objectives are prefix-independent objectives,
the memory requirement for optimal strategies of both players is the same as 
for almost-sure winning strategies, and if the almost-sure winning problem is in 
NP~~coNP, then the value-strategy problem is also in 
NP~~coNP. Thus it will follow from our results
for the almost-sure problem 
that the value and value-strategy problems are 
in NP~~coNP for  objectives.
\end{remark}




For mean-payoff-sup objectives, we show that the almost-sure winning problem
is in NP~~coNP. For player~ to be almost-sure winning for a conjunction
of mean-payoff-sup objectives, it is necessary to be almost-sure winning for
each one-dimensional mean-payoff-sup objective, and we show that it is sufficient.
An almost-sure winning strategy is to play in rounds according to the almost-sure winning
strategy of each one-dimensional objective successively, for a duration that
is always finite but long enough to ensure the corresponding one-dimensional 
average of rewards (thus over finite plays) tends to the objective mean-payoff value
with high probability (that tends to  as the number of rounds increases).

\begin{lemma}\label{lem:all-one}
If in a game, for every one-dimensional mean-payoff-sup objective  ()
all states are almost-sure winning for player~,
then for the objective  
all states are almost-sure winning for player~.
\end{lemma}

\begin{proof}
To show this result, consider for each objective  an almost-sure winning 
strategy . Then for all , there exists a number of steps 
 such that for all ,  for all states , 
all dimensions , and all strategies 
of player~, we have~\cite[Lemma~1]{CDGO14}:

Thus by playing according to strategy  for a large enough number of steps, 
the average reward in dimension  can be made arbitrarily close to ,
with probability arbitrarily close to .


Let  be the 
largest reward in absolute value. We construct an almost-sure winning strategy  for the objective  
as follows. The strategy  plays in rounds numbered , where
at round  the strategy  plays for each dimension  successively as follows:
let  be the length of the current play prefix, and let ;
play according to strategy  from the current play prefix 
for  steps.

Now we show that the strategy  is almost-sure winning for the conjunction
of mean-payoff-sup objectives. Consider an arbitrary dimension . For all
play prefixes of length , the total reward is at least . 
It follows that after playing according to  for  steps (at round ), 
since , against all strategies  of player~, we have:


Note that .
Therefore,

By Fatou's lemma~\cite{Billingsley}, for a sequence  of measurable sets we have that 
.
Hence we have 



We show that . Consider a play .
Since for , we have  and , 
for \emph{all}  there exist infinitely many integers  such that 
.
Therefore 
and thus . It follows that 
 and since this holds 
for all dimensions , we have .
Hence  is almost-sure winning for the conjunction of mean-payoff-sup 
objectives, from all states~.
\end{proof}



\begin{theorem}\label{theo:mean-payoff-sup-infinite}
The value and the value-strategy problems for stochastic games 
with generalized mean-payoff-sup objectives under infinite-memory 
strategies are in NP~~coNP.
\end{theorem}



\smallskip\noindent{\bf The NP algorithm.}
By Lemma~\ref{lem:all-one}, an NP algorithm for the almost-sure winning
problem is to guess the set  of almost-sure winning states, and check
that   \emph{induces a subgame} for player~, that is for every
state  there exists an action  such that
, and for every state  
for all actions , we have ;
and  in the subgame induced by  every state is almost-sure winning
for every one-dimensional mean-payoff-sup objective, which can be checked in NP~\cite{LigLip69}.
This establishes the first part of Theorem~\ref{theo:mean-payoff-sup-infinite}.



\smallskip\noindent{\bf The coNP algorithm.}
We now show that the almost-sure winning problem is also in coNP. 
Given a set  of states, let 
 

be the set of \emph{controllable predecessors} of set  for player~.
The \emph{positive attractor}  of  for player~ is the least 
fixed point of the operator ,
that is the
set of all states from which player~ has a memoryless strategy to ensure that  is reached
with positive probability, against all strategies of player~.
Note that the set  induces a subgame for player~.



Let  be the set of all states 
that are not almost-sure winning for player~ (for the conjunction
 of mean-payoff-sup objectives). 
By Lemma~\ref{lem:all-one} it follows that  there exist a dimension  and 
a state  that is not almost-sure winning for player~ 
(for the one-dimensional objective ), and  in the subgame induced by the set 
, the set of states that are not almost-sure winning 
for player~ (for ) is 
(because if player~ has an almost-sure winning strategy in the subgame, 
then this strategy is also almost-sure winning in the original game).

It follows that the set  can be partitioned into sets 
such that for all  there exists  such that 
 and for some dimension  
the states of  are not almost-sure winning for player~ for the one-dimensional objective ,
in the subgame induced by .
In each set  we can fix a memoryless strategy  for player~ as follows:
in  where the game objective is , fix a memoryless optimal 
strategy for player~ to violate the objective  with positive probability 
(which exists for one-dimensional mean-payoff games~\cite{LigLip69}), 
and in  fix a memoryless strategy to ensure  is reached with 
positive probability (which exists by the definition of positive attractor).
The strategy  ensures that against any strategy of player~, the
objective  is violated with positive probability. 

Hence a coNP algorithm for the almost-sure winning problem is to guess
the set  of states that are not almost-sure winning for player~,
and a memoryless strategy  for player~. The verification can be done 
by checking that the MDP obtained by playing  in the game
is not almost-sure winning for the  objective, which can be done 
in polynomial time~\cite[Section~3.2]{BBCFK14}.
This establishes the second part of Theorem~\ref{theo:mean-payoff-sup-infinite}.



Note that improving the  NP~~coNP bound to PTIME for even single dimensional objectives
would be a major breakthrough, as it would imply a polynomial solution for
simple stochastic games~\cite{Condon92}. 

\section{Conclusion}\label{sec:con}
In this work we consider 2\half-player games with generalized mean-payoff objectives.
We establish an optimal complexity result of coNP-completeness under finite-memory 
strategies, which significantly improves the previously known semi-decision procedure, 
even for the special case of the almost-sure problem.
We also establish optimal bounds for the memory required for finite-memory strategies.
Given several quantitative objectives, a more general problem is to consider
a different probability threshold for each objective (in contrast we consider
the probability of the conjunction of the objectives).
For the almost-sure problem the more general problem coincides with the problem
we consider. 
The more general problem is open, even for the special case of  
multiple reachability objectives in 2\half-player games.



\bibliographystyle{abbrv}
\bibliography{biblio}















\end{document}
