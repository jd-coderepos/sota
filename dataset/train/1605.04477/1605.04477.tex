\subsection{Distributions and Polynomials}

A \emph{probability distribution} over a finite or countably infinite set $\distDom$ is a function $\distFunc\colon\distDom\rightarrow\Ireal$ with $\sum_{\distDomElem\in\distDom}\distFunc(\distDomElem)=1$. 
The set of all distributions on $\distDom$ is denoted by $\Distr(\distDom)$.
Let $\Paramvar$ be a finite set of \emph{parameters} over $\R$.
A \emph{valuation} for $\Paramvar$ is a function $u \colon \Paramvar \to \R$.
Let $\poly[\Paramvar]$ denote the set of multivariate \emph{polynomials} with rational coefficients and $\functions[\Paramvar]$ the set of \emph{rational functions} (fractions of polynomials) over $\Paramvar$. For $\pol\in \poly[\Paramvar]$ or $\pol\in\functions[\Paramvar]$, let $\pol[u]$ denote the evaluation of $\pol$ at $u$. We write $\pol=0$ if $\pol$ can be reduced to $0$, and $\pol\not=0$ otherwise.






\subsection{Probabilistic Models}\label{sec:probmodels}

First, we introduce parametric probabilistic models which can be seen as transition systems where the transitions are labelled with polynomials in $\poly[\Paramvar]$.
\begin{definition}[pMDP and pMC]\label{def:pmdp}
A \emph{parametric Markov decision process (pMDP)} is a tuple $\MdpInit$ with a countable set $S$ of states, an initial state $\sinit \in S$, a finite set $\Act$ of actions, and a transition function $\probmdp \colon S \times \Act \times S \rightarrow \poly[\Paramvar]$ satisfying for all $s\in S\colon
\Act(s) \neq \emptyset$, where $\Paramvar$ is a finite set  of parameters over $\R$ and $\Act(s) = \{\act \in \Act \mid \exists s'\in S.\,\probmdp(s,\,\act,\,s') \neq 0\}$.
If for all $s\in S$ it holds that $|\Act(s)| = 1$, $\mdp$ is called a \emph{parametric discrete-time Markov chain (pMC)}, denoted by $\dtmc$.
\end{definition}
At each state, an action is chosen \emph{nondeterministically}, then the successor states are determined \emph{probabilistically} as defined by the transition function.
$\Act(s)$ is the set of \emph{enabled} actions at state $s$.
As $\Act(s)$ is non-empty for all $s \in S$, there are no deadlock states. For pMCs there is only one single action per state and we write the transition probability function as $\probmdp\colon S\times S\rightarrow\poly[\Paramvar]$, omitting that action.
\emph{Rewards} are defined using a \emph{reward function} $\rewFunction \colon S \rightarrow \R$ which assigns rewards to states of the model.
Intuitively, the reward $\rewFunction(s)$ is earned upon \emph{leaving} the state $s$. 

\paragraph{Schedulers.}

The nondeterministic choices of actions in pMDPs can be resolved using \emph{schedulers}\footnote{Also referred to as adversaries, strategies, or policies.}.
In our setting it suffices to consider memoryless deterministic schedulers~\cite{Var85}. 
For more general definitions we refer to~\cite{BK08}.
\begin{definition}{\bf (Scheduler)}\label{def:scheduler}
	A \emph{scheduler} for pMDP $\MdpInit$ is a function $\sched\colon S\rightarrow\Act$ with $\sched(s)\in \Act(s)$ for all $s\in S$.  
\end{definition}
Let $\Sched^\mdp$ denote the set of all schedulers for $\mdp$.
Applying a scheduler to a pMDP yields an \emph{induced parametric Markov chain}, as all nondeterminism is resolved, \ie, the transition probabilities are obtained \wrt the choice of actions.
\begin{definition}{\bf (Induced pMC)}\label{def:induced_dtmc} 
	Given a pMDP $\MdpInit$, the \emph{pMC induced by $\sched\in\Sched^\mdp$} is given by $\mdp^\sched=(S,\, \sinit,\, \Act,\, \probmdp^\sched)$, where
	\begin{align*}
		\probmdp^\sched(s,\,s')= \probmdp(s,\, \sched(s),\, s'), \quad \mbox{for all } s,s'\in S~.
	\end{align*} 
\end{definition}



\paragraph{Valuations.}

Applying a \emph{valuation} $u$ to a pMDP $\mdp$, denoted $\mdp[u]$, replaces each polynomial $\pol$ in $\mdp$ by $\pol[u]$.
We call $\mdp[u]$ the \emph{instantiation} of $\mdp$ at $u$.
A valuation $u$ is \emph{well-defined} for $\mdp$ if the replacement yields \emph{probability distributions} at all states; the resulting model $\mdp[u]$ is a Markov decision process (MDP) or, in absence of nondeterminism, a Markov chain (MC).


\paragraph{Properties.}

For our purpose we consider \emph{conditional reachability properties} and \emph{conditional expected reward properties} in MCs.
For more detailed definitions we refer to~\cite[Ch.\ 10]{BK08}.
Given an MC $\dtmc$ with state space $S$ and initial state $\sinit$, let $\pr^{\dtmc}(\neg \finally U)$ denote the probability \emph{not} to reach a set of undesired states $U$ from the initial state $\sinit$ within $\dtmc$.
Furthermore, let $\creachPr{\dtmc}{T}{U}$ denote the conditional probability to reach a set of target states $T \subseteq S$ from the initial state $\sinit$ within $\dtmc$, given that no state in the set $U$ is reached.
We use the standard probability measure on infinite paths through an MC.
For threshold $\lambda\in \Ireal$, the reachability property, asserting that a target state is to be reached with conditional probability at most $\lambda$, is denoted $\reachPropSymbol = \creachProplT$.
The property is satisfied by $\dtmc$, written $\dtmc \models \reachPropSymbol$, iff $\creachPrT[\dtmc]\leq\lambda$.
This is analogous for comparisons like $<$, $>$, and $\geq$.

The reward of a path through an MC $\dtmc$ until $T$ is the sum of the rewards of the states visited along on the path before reaching $T$.
The expected reward of a finite path is given by its probability times its reward.
Given $\reachPrT[\dtmc] = 1$, the conditional expected reward of reaching $T \subseteq S$, given that no state in set $U \subseteq S$ is reached, denoted $\cexpRewT[\dtmc]$, is the expected reward of all paths accumulated until hitting $T$ while not visiting a state in $U$ in between divided by the probability of not reaching a state in $U$ (\ie, divided by $\pr^{\dtmc}(\neg \finally U)$).
An expected reward property is given by $\ereachPropSymbol = \cexpRewPropkT$ with threshold $\kappa \in \R_{\geq 0}$.
The property is satisfied by $\dtmc$, written $\dtmc \models \ereachPropSymbol$, iff $\cexpRewT[\dtmc]\leq\kappa$.
Again, this is analogous for comparisons like $<$, $>$, and $\geq$.
For details about conditional probabilities and expected rewards see~\cite{DBLP:conf/tacas/BaierKKM14}. 

Reachability probabilities and expected rewards for MDPs are defined on induced MCs for specific schedulers. 
We take here the conservative view that a property for an MDP has to hold for \emph{all possible schedulers}. 

\paragraph{Parameter Synthesis.}
For pMCs, one is interested in \emph{synthesizing} well-defined valuations that induce satisfaction or violation of the given specifications~\cite{dehnert-et-al-cav-2015}. In detail, for a pMC $\dtmc$, a rational function $\pol\in\functions[\Paramvar]$ is computed which---when instantiated by a well-defined valuation $u$ for $\dtmc$---evaluates to the actual reachability probability or expected reward for $\dtmc$, \ie, $\pol[u]=\reachPr{\dtmc[u]}{T}$ or $\pol[u]=\expRewT[{\dtmc[u]}]$. For pMDPs, schedulers inducing \emph{maximal} or \emph{minimal} probability or expected reward have to be considered~\cite{quatmann-et-al-techreport-2016}.


\subsection{Conditional Probabilistic Guarded Command Language}

We first present a programming language which is an extension of Dijkstra's guarded command language~\cite{Dijkstra} with a binary probabilistic choice operator, yielding the 
\emph{probabilistic guarded command language} (\pGCL)~\cite{McIver:2004}. In~\cite{jansen-et-al-mfps-2015}, \pGCL was endowed with \emph{observe statements}, giving rise to conditioning. The syntax of this \emph{conditional probabilistic guarded command language} (\cpGCL) is given by
\begin{align*}
\Cmd ~\Coloneqq~  &\Skip \mid \Abort \mid \Ass x E \mid \Cmd;\Cmd \mid \If\ G\ \Then\ {\Cmd }\ \Else\ {\Cmd } \\ 
   &{}~\mid  \, \PChoice {\Cmd} {\pol} {\Cmd} \mid
\NDChoice {\Cmd}{\Cmd} \mid \WhileDo G \Cmd \mid \Observe~(G)
\end{align*}
Here, $x$ belongs to the set of \emph{program variables} $\Var$; $E$ is an arithmetical expression over $\Var$; $G$ is a \emph{Boolean expression} over arithmetical expressions over $\Var$. The \emph{probability} is given by a polynomial $\pol \in \poly[\Paramvar]$.
 Most of the \cpGCL instructions are self--explanatory; we elaborate only on the following: For \cpGCL-programs $P$ and $Q$, $\PChoice{P}{\pol}{Q}$ is a \emph{probabilistic choice} where $P$ is executed with probability $\pol$ and $Q$ with probability $1{-}\pol$; analogously, $\NDChoice{P}{Q}$ is a \emph{nondeterministic choice} between $P$ and $Q$;  \Abort is syntactic sugar for the diverging program $\While\ (\true)\ \{\Skip\}$. The statement $\Observe~(G)$ for the Boolean expression $G$ \emph{blocks} all program executions violating $G$ and induces a \emph{rescaling} of probability of the
remaining execution traces so that they sum up to one. 
For a \cpGCL-program $P$, the set of \emph{program states} is given by $\State = \{\sigma ~|~ \sigma \colon \Var \to \Q\}$, 
\ie, the set of all  variable valuations. We assume all variables to be assigned zero prior to execution or at the start of the program. This initial variable valuation $\sigma_I\in\State$ with $\forall x \in \Var.\, \sigma_I(x) = 0$ is called the \emph{initial state} of the program. 
\begin{example}\label{ex:simple}
	Consider the following \cpGCL-program with variables $x$ and $c$:
	
\begin{CenteredBox}
				\lstinputlisting[language=C,linewidth=9cm]{resources/example_benni.pp}
			\end{CenteredBox}

\noindent While $c$ is $0$, the loop body is iterated: With probability $\nicefrac 1 2$ either $x$ is incremented by one or $c$ is set to one. After leaving the loop, the event that the valuation of $x$ is odd is \emph{observed}, which means that all program executions where $x$ is even are blocked. 
	Properties of interest for this program would, \eg, concern the termination probability, or the expected value of $x$ after termination.
\hfill$\triangle$
\end{example}





\subsection{Operational Semantics for Probabilistic Programs}\label{sec:operational}
We now introduce an operational semantics for \cpGCL-programs which is given by an MDP as in Definition~\ref{def:pmdp}.
The structure of such an operational MDP is schematically depicted below.
\begin{center}
		\scalebox{0.9}{\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.5cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.2,-1.8) rectangle (6.5,1.5);
   \node [state, initial, initial text=] (init) {$\langle P,\, \sigma_I \rangle$};  
   \node [cloud, draw=black,cloud puffs=15, cloud puff arc= 150,
        minimum width=1.5cm, minimum height=.75cm, aspect=1] (exit) [right of=init] {$\exit$};
   \node [state] (bad) [above=0.3cm of exit] {$\undesired$};
   \node [state] (sink) [right of=exit] {$\sink$};
   \node [cloud, draw=black,cloud puffs=15, cloud puff arc= 150,
        minimum width=1.5cm, minimum height=.75cm, aspect=1] (diverge) [below=0.5 cm of exit] {$\phantom{\exit}$};

    \node [] (divergetext) [below=-0.825 cm of diverge] {\small$\mathpzc{diverge}$};

   \node [state] (haken1) at (2.2, .1) {\scriptsize $\exit$};
   \node [state] (haken2) at (2.25, -.1) {\tiny $\exit$};
    \node [state] (haken3) at (2.7, -.1) {\scriptsize $\exit$};
   \node [state] (haken4) at (2.75, .1) {\tiny $\exit$};
   \node [state] (haken5) at (2.95, .0) {\tiny $\exit$};

  \path [] 
      (init) edge [decorate,decoration={snake, post length=2mm}] (exit)
      (init) edge [decorate,decoration={snake, post length=2mm}] (bad)
      (init) edge [decorate,decoration={snake, post length=2mm}] (diverge)
      (exit) edge [] (sink)
      (bad) edge [] (sink)
      (sink) edge [loop right] (sink)
      (diverge) edge [loop right,decorate,decoration={snake, post length=2mm}] (diverge)
  ;
\end{tikzpicture}
 }
	\end{center}
Squiggly arrows indicate reaching certain states via possibly multiple paths and states; the clouds indicate that there might be several states of the particular kind.
$\langle P,\, \sigma_I \rangle$ marks the initial state of the program $P$. 
In general the states of the operational MDP are of the form $\langle P',\, \sigma' \rangle$ where $P'$ is the program that is left to be executed and $\sigma'$ is the current variable valuation.

All runs of the program (paths through the MDP) are either \emph{terminating} and eventually end up in the \sink state, or are \emph{diverging} (thus they never reach \sink).
Diverging runs occur due to non--terminating computations.
A terminating run has either terminated successfully, \ie, it passes a \exit--state, or it has terminated due to a \emph{violation of an observation}, \ie, it passes the $\undesired$--state.
Sets of runs that eventually reach \undesired, or \sink, or diverge are pairwise disjoint. 

\noindent The \exit--labelled states are the \emph{only ones} with positive reward, which is due to the fact that we want to capture probabilities of events (respectively expected values of random variables) occurring at \emph{successful termination} of the program.

The random variables of interest are $\Ex = \{f ~|~ f\colon \State \to \R_{\geq 0}\}$.
Such random variables are referred to as post--expectations~\cite{McIver:2004}. 
Formally, we have:
\begin{definition}[Operational Semantics of Programs]
The \emph{operational semantics} of a \cpGCL program $P$ with respect to a post--expectation $f \in \Ex$ is the MDP $\Rmdp{}{f}{P} = (S$, $\langle P,\,\sigma_I\rangle,\, \Act,\, \pdtmc)$ together with a reward function $\rewFunction$, where
\begin{itemize}
\item$S = \big\{\langle Q,\, \sigma \rangle,\langle
\term,\, \sigma \rangle ~\big|~ Q \textnormal{ is a \cpGCL program},\, \sigma \in \State\big\} \cup \{\undesired,\, \sink\}$ is the countable set of states,
\item $\langle P,\,\sigma_I\rangle\in S$ is the initial state,
\item $\Act=\{\mathit{left},\,\mathit{right},\, \mathit{none}\}$ is the set of actions, and
\item $\probmdp$ is the smallest relation defined by the SOS rules given in Figure \ref{fig:sos-rules}.
\end{itemize}
The reward function is $\rewFunction(s) = f(\sigma)$ if $s = \langle\term,\, \sigma\rangle$, and $\rewFunction(s) = 0$, otherwise.
\end{definition}
\begin{figure}[t]
\scriptsize
\begin{align*}
&(\textbf{terminal})\,\frac{\vphantom{\langle}}{\langle {\downarrow},\, \sigma \rangle ~\longrightarrow~ \sink} \quad
(\textbf{skip})\,\frac{\vphantom{\langle}}{\langle \Skip,\, \sigma \rangle ~\longrightarrow~ \langle {\downarrow},\, \sigma \rangle} \quad
(\textbf{abort})\,\frac{\vphantom{\langle}}{\langle \Abort,\, \sigma \rangle ~\longrightarrow~ \langle \Abort,\, \sigma \rangle}\\
&(\textbf{undesired})\,\frac{\vphantom{\langle}}{\undesired ~\longrightarrow~ \sink}\qquad\qquad\qquad\qquad\qquad~~~~
(\textbf{assign})\,\frac{\vphantom{\langle}}{\langle \Ass x E,\, \sigma \rangle ~\longrightarrow~ \langle {\downarrow},\, \sigma[x \leftarrow \llbracket E \rrbracket_\sigma] \rangle}\\
&(\textbf{observe1})\,\frac{\sigma \models G}{\langle \Observe\, G,\, \sigma \rangle ~\longrightarrow~ \langle {\downarrow} ,\, \sigma \rangle}\qquad\qquad\qquad\qquad\qquad\,
(\textbf{observe2})\,\frac{\sigma \not\models G}{\langle \Observe\, G,\, \sigma \rangle ~\longrightarrow~ \undesired}\\
&(\textbf{concatenate1})\,\frac{}{\langle\downarrow;{Q},\, \sigma \rangle ~\longrightarrow~ \langle Q,\, \sigma \rangle}\qquad\qquad\qquad\qquad\qquad\;
(\textbf{concatenate2})\,\frac{\langle P,\, \sigma \rangle ~\longrightarrow~ \undesired}{\langle {P};{Q},\, \sigma \rangle ~\longrightarrow~ \undesired}\\
&(\textbf{concatenate3})\,\frac{\langle P,\, \sigma \rangle ~\longrightarrow~ \mu}{\langle {P};{Q},\, \sigma \rangle ~\longrightarrow~ \nu}, 
\textnormal{where } \forall P'.\,\nu(\langle {P'};{Q}, \sigma'\rangle) := \mu(\langle P',\, \sigma'\rangle)\\
&(\textbf{if1})\,\frac{\sigma \models G}{\langle \Cond G P Q,\, \sigma \rangle ~\longrightarrow~ \langle P,\, \sigma \rangle}\qquad\qquad\qquad\qquad~~\;
(\textbf{if2})\,\frac{\sigma \not\models G}{\langle \Cond G P Q,\, \sigma \rangle ~\longrightarrow~ \langle Q,\, \sigma \rangle}\\
&(\textbf{while1})\,\frac{\sigma \models G}{\langle \WhileDo G P,\, \sigma \rangle ~\longrightarrow~ \langle {P};{\WhileDo G P},\, \sigma \rangle}~~\,
(\textbf{while2})\,\frac{\sigma \not\models G}{\langle \WhileDo G P,\, \sigma \rangle ~\longrightarrow~ \langle {\downarrow},\, \sigma \rangle}\\
&(\textbf{prob})\,\frac{\vphantom{\langle}}{\langle \PChoice P p Q,\, \sigma \rangle ~\longrightarrow~ \nu}, \textnormal{where } \nu(\langle P,\, \sigma\rangle) := p,\, \nu(\langle Q,\, \sigma\rangle) := 1 - p\\
&(\textbf{nondet1})\,\frac{\vphantom{\langle}}{\langle \NDChoice P Q ,\, \sigma \rangle ~ \xrightarrow{~\mathit{left}~} ~ \langle P,\, \sigma \rangle}\qquad\qquad\qquad
(\textbf{nondet2})\frac{\vphantom{\langle}}{\langle \NDChoice P Q ,\, \sigma \rangle ~\xrightarrow{~\mathit{right}~}~ \langle Q,\, \sigma \rangle}
\end{align*}\normalsize
\caption{
SOS rules for constructing the operational MDP of a \cpGCL program.
We use $s \longrightarrow t$ to indicate $\probmdp(s,\, \mathit{none},\, t) = 1$, $s \longrightarrow \mu$ for $\mu \in \Distr(S)$ to indicate $\forall t \in S\colon \probmdp(s,\, \mathit{none},\, t) = \mu(t)$, $s \xrightarrow{~\mathit{left}~} t$ to indicate $\probmdp(s,\, \mathit{left},\, t) = 1$, and $s \xrightarrow{~\mathit{right}~} t$ to indicate $\probmdp(s,\, \mathit{right},\, t) = 1$.
}
\label{fig:sos-rules}
\end{figure}
A state of the form $\langle {\downarrow},\, \sigma \rangle$ indicates successful termination, \ie, no commands are left to be executed.
These terminal states and the \undesired--state go to the \sink state. 
\Skip without context terminates successfully.
\Abort self--loops, \ie, diverges. 
$\Ass x E$ alters the variable valuation according to the assignment then terminates successfully. 
For the concatenation, $\langle\term;{Q},\, \sigma \rangle$ indicates successful termination of the first program, so the execution continues with $\langle{Q},\, \sigma \rangle$.
If for $P;\,Q$ the execution of $P$ leads to $\undesired$, $P;\,Q$ does so, too.
Otherwise, for $\langle P,\sigma \rangle{\longrightarrow}\mu$, $\mu$ is lifted such that $Q$ is concatenated to the support of $\mu$. For more details on the operational semantics we refer to~\cite{DBLP:journals/pe/GretzKM14}.

If for the conditional choice $\sigma\models G$ holds, $P$ is executed, otherwise $Q$.
The case for $\While$ is similar.
For the probabilistic choice, a distribution $\nu$ is created according to probability $p$. 
For $\NDChoice P Q$, we call $P$ the $\mathit{left}$ choice and $Q$ the $\mathit{right}$ choice for actions $\mathit{left}, \mathit{right}\in\Act$.
For the \Observe statement, if $\sigma\models G$ then \Observe acts like $\Skip$. 
Otherwise, the execution leads directly to \undesired indicating a violation of the \Observe statement.
\begin{figure}[t]
\begin{center}
\scalebox{0.8}{\begin{tikzpicture}[->,>=stealth',shorten >=1pt,semithick,minimum size=.5cm]
\tikzstyle{every state}=[draw=none]
\node [] (init) at (0,0) {$\langle P,\,\sigma_I \rangle$};  
    \node [] (s1) [on grid, below=1.1 cm of init] {$\langle P_1;P,\,\sigma_I\rangle$};   
    \node [] (s11) [on grid, left=4 cm of s1] {$\langle P_3;\,P,\,\sigma_I\rangle$};   
    \node [] (s12) [on grid, right=4 cm of s1] {$\langle P_4;\,P,\,\sigma_I\rangle$};   
	\node [] (s111) [on grid, below=1.1 cm of s11] {$\langle \term;\,P,\,\sigma_I[x/1]\rangle$};   
    \node [] (s121) [on grid, below=1.1 cm of s12] {$\langle \term;\,P,\,\sigma_I[c/1]\rangle$};   
    \node [] (s1111) [on grid, below=1.1 cm of s111] {$\langle P,\,\sigma_I[x/1]\rangle$};   
    \node [] (s1211) [on grid, below=1.1 cm of s121] {$\langle P,\,\sigma_I[c/1]\rangle$};   
    \node [] (s1211b) [on grid, below=1.1 cm of s1211] {$\langle \term;\,P_2,\,\sigma_I[c/1]\rangle$};   
    \node [] (s1211bb) [on grid, below=1.1 cm of s1211b] {$\langle P_2,\,\sigma_I[c/1]\rangle$}; 
\node [] (s11111) [on grid, left=4 cm of s1111] {$\langle P_3;P,\,\sigma_I[x/1]\rangle$};   
    \node [] (s11112) [on grid, right=4 cm of s1111] {$\langle P_4;P,\,\sigma_I[x/1]\rangle$};   
    \node [] (s111111) [on grid, below=1.1 cm of s11111] {$\langle \term;P,\,\sigma_I[x/2]\rangle$};   
    \node [] (s111121) [on grid, below=1.1 cm of s11112] {$\langle \term;P,\,\sigma_I[x/1,c/1]\rangle$};   
    \node [] (s1111111) [on grid, below=1.1 cm of s111111] {$\langle P,\,\sigma_I[x/2]\rangle$};   
    \node [] (s1111211) [on grid, below=1.1 cm of s111121] {$\langle P,\,\sigma_I[x/1,c/1]\rangle$};
    \node [] (s1111211b) [on grid, below=1.1 cm of s1111211] {$\langle \term;\,P_2,\,\sigma_I[x/1,c/1]\rangle$};
    \node [] (s1111211bb) [on grid, below=1.1 cm of s1111211b] {$\langle P_2,\,\sigma_I[x/1,c/1]\rangle$};
\node [label={[yshift=-0.5cm] 170:\large\boxed{$1$}}] (s11112111) [on grid, below=1.1 cm of s1111211bb] {$\langle \term,\,\sigma_I[x/1,c/1]\rangle$};


    

  \node [] (undesired) [on grid, below=1.1 cm of s1211bb] {$\undesired$};  
  \node [] (sink) [on grid, below=2.2 cm of undesired] {$\sink$};  
  \node [] (dots) [on grid, below=.5 cm of s1111111] {$\bf\vdots$};  
	
\path
      (init) edge [] node [left, near start] {} (s1)
      (s1) edge [] node [above, near start] {\scriptsize{$\frac{1}{2}$}} (s11)
      (s1) edge [] node [above, near start] {\scriptsize{$\frac{1}{2}$}} (s12)
      (s11) edge [] (s111)
      (s12) edge [] (s121)
      (s111) edge [] (s1111)
      (s121) edge [] (s1211)
(s1211) edge [] (s1211b)
      (s1211b) edge [] (s1211bb)
      (s1211bb) edge [] (undesired)
      (s1111) edge [] node [above, near start] {\scriptsize{$\frac{1}{2}$}} (s11111)
      (s1111) edge [] node [above, near start] {\scriptsize{$\frac{1}{2}$}} (s11112)
      (s11111) edge [] node [right, near start] {} (s111111)	
      (s11112) edge [] node [right, near start] {} (s111121)	
      (s111111) edge [] node [right, near start] {} (s1111111)	
      (s111121) edge [] node [right, near start] {} (s1111211)	
      (s1111211) edge [] node [right, near start] {} (s1111211b)
      (s1111211b) edge [] node [right, near start] {} (s1111211bb)	
      (s1111211bb) edge [] node [right, near start] {} (s11112111)
      (s11112111) edge [] node [right, near start] {} (sink)	
(undesired) edge [] (sink)
      (sink) edge [loop right] (sink)
;
\end{tikzpicture}
 }	
\end{center}
\caption{Partially unrolled operational semantics for program $P$}	
\label{fig:better_operational_example}
\end{figure}
\begin{example}


Reconsider Example~\ref{ex:simple}, where we set for readability $P_1= \PChoice{\Ass{x}{x + 1}}{0.5}{\Ass{c}{1}}$, $P_2= \Observe (\textit{``$x$ is odd"})$, $P_3= \{\Ass{x}{x+1}\}$, and $P_4=\{\Ass{c}{1}\}$. 
A part of the operational MDP $\Rmdp{}{f}{P}$ for an arbitrary initial variable valuation $\sigma_I$ and post--expectation $x$ is depicted in Figure~\ref{fig:better_operational_example}.\footnote{We have tacitly overloaded the variable name $x$ to an expectation here for readability. More formally, by the ``expectation $x$" we actually mean the expectation $\lambda \sigma.~ \sigma(x)$.}
Note that this MDP is an MC, as $P$ contains no nondeterministic choices. The MDP has been unrolled until the second loop iteration, \ie, at state $\langle P,\,\sigma_I[x/2]\rangle$, the unrolling could be continued. The only terminating state is $\langle \term,\,\sigma_I[x/1,c/1]\rangle$. 
As our post-expectation is the value of variable $x$, we assign this value to terminating states, \ie, reward $\boxed{1}$ at state $\langle \term,\,\sigma_I[x/1,c/1]\rangle$, where $x$ has been assigned $1$. At state $\langle P,\,\sigma_I[c/1]\rangle$, the loop condition is violated as is the subsequent observation because of $x$ being assigned an even number.
\hfill$\triangle$
\end{example}

\iffalse
\begin{example}
Consider the following \cpGCL program $P$ with variables $x$ and $c$:
\begin{CenteredBox}
				\lstinputlisting[language=C,linewidth=9cm]{resources/example_operational.pp}
			\end{CenteredBox}

\noindent With parametrized probability $q$ a nondeterministic choice between $x$ being assigned $2$ or $5$ is executed, and with probability $1{-}q$, $x$ is directly assigned $2$.
Let for readability $P_1= \NDChoice {\Ass{x}{5}} {\Ass{x}{2}}$, $P_2= \Ass{x}{2}$, $P_3= \Observe~(x{>}3)$, and $P_4=\Ass x 5$. 
The operational MDP $\Rmdp {} x {P}$ with respect to post--expectation $x$ is depicted below:
\begin{center}
\scalebox{0.8}{\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-6.6,-6.3) rectangle (3.4,0.3);
   \node [state, initial where=above, initial text=] (init) at (0,0) {$\langle P,\,\sigma_I \rangle$};  
    \node [] (s1) [on grid, below=1.5 cm of init, xshift=-2.2cm] {$\langle P_1;\,P_3,\,\sigma_I\rangle$};   
\node [] (s11) [on grid, below=1.5 cm of init, xshift=2.2cm] {$\langle P_2;\,P_3,\,\sigma_I\rangle$};   
        \node [] (s12) [on grid, left=2 cm of s1, xshift=-1.2cm] {$\langle P_4;\,P_3,\,\sigma_I\rangle$};   
\node [] (s111) [on grid, below=1.5 cm of s11] {$\langle\term;\,P_3,\,\sigma_I\subst x 2\rangle$};   
        \node [] (s121) [on grid, below=1.5 cm of s12] {$\langle\term;\,P_3,\,\sigma_I\subst x 5\rangle$};   
\node [] (s1111) [on grid, below=1.5 cm of s111] {$\langle\,P_3,\,\sigma_I\subst x 2\rangle$};   
         \node [] (s1211) [on grid, right=3.3 cm of s121] {$\langle P_3,\,\sigma_I\subst x 5\rangle$};   
         \node [label={[yshift=0.0cm, gray] 180:$5$}] (s12111) [on grid, below=1.5 cm of s1211] {$\langle\term,\,\sigma_I\subst x 5\rangle$};   
 \node [] (undesired) [on grid, below=1.5 cm of s1111, xshift=-.5cm] {$\undesired$};  
  \node [] (sink) [on grid, below=1.5 cm of s12111, xshift=1.5cm] {$\sink$};  

\path
      (init) edge [] node [left, near start] {\scriptsize{$q$}} (s1)
      (init) edge [] node [right, near start] {\scriptsize{$1-q$}} (s11)
      (s1) edge [] node [below=-0.3cm] {\scriptsize{$\mathit{left}$}} (s12)
      (s1) edge [] node [below=-0.3cm] {\scriptsize{$\mathit{right}$}} (s11)
(s12) edge [] (s121)
      (s11) edge [] (s111)
      (s111) edge [] (s1111)
      (s121) edge [] (s1211)
(s1111) edge [] (undesired)
      (s1211) edge [] (s12111)
(undesired) edge [] (sink)
      (s12111) edge [] (sink)
      (sink) edge [loop above] (sink)
;
\end{tikzpicture}
 }\\
\end{center}
The only state with positive reward is $s' \coloneqq\langle\term,\,\sigma_I\subst x 5\rangle$ and its reward is indicated by the number $5$ next to it.
This is the only state since it is the only state that constitutes a \emph{successful} termination, i.e.\ the program has terminated and no observation was violated.
Assume first a scheduler choosing action $\mathit{left}$ in state $\langle P_1;\,P_3,\,\sigma_I\rangle$.
In the induced MC the only path accumulating positive reward is the path $\pi$ going from $\langle P,\, \sigma_I\rangle$ via
$s'$ to $\sink$ with $r(\pi)=5$ and $\Pr(\pi)=q$. 
This gives an expected reward of $5\cdot q$.
The overall probability of not reaching $\undesired$ is also $q$. 
The conditional expected reward of eventually reaching $\sink$ given that $\undesired$ is not reached is hence $\nicefrac{5\cdot q}{q}=5$. 
Assume now the \emph{demonic} scheduler choosing $\mathit{right}$ at state $\langle P_1;\,P_3,\,\sigma_I\rangle$. 
In this case there is no path having positive accumulated reward in the induced MC, yielding an expected reward of $0$. 
The probability of not reaching $\undesired$ is also $0$. 
The conditional expected reward in this case is undefined ($\nicefrac{0}{0}$) and thus the $\mathit{right}$ branch is preferred over the $\mathit{left}$ branch by the demon. 
In general, the operational MDP need not be finite, even if the program terminates almost--surely (\ie, with probability 1).
\nj{show example properties in the style we had in the preliminaries.}
\hfill$\triangle$
\end{example}
\fi

