\subsection{Distributions and Polynomials}

A \emph{probability distribution} over a finite or countably infinite set  is a function  with . 
The set of all distributions on  is denoted by .
Let  be a finite set of \emph{parameters} over .
A \emph{valuation} for  is a function .
Let  denote the set of multivariate \emph{polynomials} with rational coefficients and  the set of \emph{rational functions} (fractions of polynomials) over . For  or , let  denote the evaluation of  at . We write  if  can be reduced to , and  otherwise.






\subsection{Probabilistic Models}\label{sec:probmodels}

First, we introduce parametric probabilistic models which can be seen as transition systems where the transitions are labelled with polynomials in .
\begin{definition}[pMDP and pMC]\label{def:pmdp}
A \emph{parametric Markov decision process (pMDP)} is a tuple  with a countable set  of states, an initial state , a finite set  of actions, and a transition function  satisfying for all , where  is a finite set  of parameters over  and .
If for all  it holds that ,  is called a \emph{parametric discrete-time Markov chain (pMC)}, denoted by .
\end{definition}
At each state, an action is chosen \emph{nondeterministically}, then the successor states are determined \emph{probabilistically} as defined by the transition function.
 is the set of \emph{enabled} actions at state .
As  is non-empty for all , there are no deadlock states. For pMCs there is only one single action per state and we write the transition probability function as , omitting that action.
\emph{Rewards} are defined using a \emph{reward function}  which assigns rewards to states of the model.
Intuitively, the reward  is earned upon \emph{leaving} the state . 

\paragraph{Schedulers.}

The nondeterministic choices of actions in pMDPs can be resolved using \emph{schedulers}\footnote{Also referred to as adversaries, strategies, or policies.}.
In our setting it suffices to consider memoryless deterministic schedulers~\cite{Var85}. 
For more general definitions we refer to~\cite{BK08}.
\begin{definition}{\bf (Scheduler)}\label{def:scheduler}
	A \emph{scheduler} for pMDP  is a function  with  for all .  
\end{definition}
Let  denote the set of all schedulers for .
Applying a scheduler to a pMDP yields an \emph{induced parametric Markov chain}, as all nondeterminism is resolved, \ie, the transition probabilities are obtained \wrt the choice of actions.
\begin{definition}{\bf (Induced pMC)}\label{def:induced_dtmc} 
	Given a pMDP , the \emph{pMC induced by } is given by , where
	 
\end{definition}



\paragraph{Valuations.}

Applying a \emph{valuation}  to a pMDP , denoted , replaces each polynomial  in  by .
We call  the \emph{instantiation} of  at .
A valuation  is \emph{well-defined} for  if the replacement yields \emph{probability distributions} at all states; the resulting model  is a Markov decision process (MDP) or, in absence of nondeterminism, a Markov chain (MC).


\paragraph{Properties.}

For our purpose we consider \emph{conditional reachability properties} and \emph{conditional expected reward properties} in MCs.
For more detailed definitions we refer to~\cite[Ch.\ 10]{BK08}.
Given an MC  with state space  and initial state , let  denote the probability \emph{not} to reach a set of undesired states  from the initial state  within .
Furthermore, let  denote the conditional probability to reach a set of target states  from the initial state  within , given that no state in the set  is reached.
We use the standard probability measure on infinite paths through an MC.
For threshold , the reachability property, asserting that a target state is to be reached with conditional probability at most , is denoted .
The property is satisfied by , written , iff .
This is analogous for comparisons like , , and .

The reward of a path through an MC  until  is the sum of the rewards of the states visited along on the path before reaching .
The expected reward of a finite path is given by its probability times its reward.
Given , the conditional expected reward of reaching , given that no state in set  is reached, denoted , is the expected reward of all paths accumulated until hitting  while not visiting a state in  in between divided by the probability of not reaching a state in  (\ie, divided by ).
An expected reward property is given by  with threshold .
The property is satisfied by , written , iff .
Again, this is analogous for comparisons like , , and .
For details about conditional probabilities and expected rewards see~\cite{DBLP:conf/tacas/BaierKKM14}. 

Reachability probabilities and expected rewards for MDPs are defined on induced MCs for specific schedulers. 
We take here the conservative view that a property for an MDP has to hold for \emph{all possible schedulers}. 

\paragraph{Parameter Synthesis.}
For pMCs, one is interested in \emph{synthesizing} well-defined valuations that induce satisfaction or violation of the given specifications~\cite{dehnert-et-al-cav-2015}. In detail, for a pMC , a rational function  is computed which---when instantiated by a well-defined valuation  for ---evaluates to the actual reachability probability or expected reward for , \ie,  or . For pMDPs, schedulers inducing \emph{maximal} or \emph{minimal} probability or expected reward have to be considered~\cite{quatmann-et-al-techreport-2016}.


\subsection{Conditional Probabilistic Guarded Command Language}

We first present a programming language which is an extension of Dijkstra's guarded command language~\cite{Dijkstra} with a binary probabilistic choice operator, yielding the 
\emph{probabilistic guarded command language} (\pGCL)~\cite{McIver:2004}. In~\cite{jansen-et-al-mfps-2015}, \pGCL was endowed with \emph{observe statements}, giving rise to conditioning. The syntax of this \emph{conditional probabilistic guarded command language} (\cpGCL) is given by

Here,  belongs to the set of \emph{program variables} ;  is an arithmetical expression over ;  is a \emph{Boolean expression} over arithmetical expressions over . The \emph{probability} is given by a polynomial .
 Most of the \cpGCL instructions are self--explanatory; we elaborate only on the following: For \cpGCL-programs  and ,  is a \emph{probabilistic choice} where  is executed with probability  and  with probability ; analogously,  is a \emph{nondeterministic choice} between  and ;  \Abort is syntactic sugar for the diverging program . The statement  for the Boolean expression  \emph{blocks} all program executions violating  and induces a \emph{rescaling} of probability of the
remaining execution traces so that they sum up to one. 
For a \cpGCL-program , the set of \emph{program states} is given by , 
\ie, the set of all  variable valuations. We assume all variables to be assigned zero prior to execution or at the start of the program. This initial variable valuation  with  is called the \emph{initial state} of the program. 
\begin{example}\label{ex:simple}
	Consider the following \cpGCL-program with variables  and :
	
\begin{CenteredBox}
				\lstinputlisting[language=C,linewidth=9cm]{resources/example_benni.pp}
			\end{CenteredBox}

\noindent While  is , the loop body is iterated: With probability  either  is incremented by one or  is set to one. After leaving the loop, the event that the valuation of  is odd is \emph{observed}, which means that all program executions where  is even are blocked. 
	Properties of interest for this program would, \eg, concern the termination probability, or the expected value of  after termination.
\hfill
\end{example}





\subsection{Operational Semantics for Probabilistic Programs}\label{sec:operational}
We now introduce an operational semantics for \cpGCL-programs which is given by an MDP as in Definition~\ref{def:pmdp}.
The structure of such an operational MDP is schematically depicted below.
\begin{center}
		\scalebox{0.9}{\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.5cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.2,-1.8) rectangle (6.5,1.5);
   \node [state, initial, initial text=] (init) {};  
   \node [cloud, draw=black,cloud puffs=15, cloud puff arc= 150,
        minimum width=1.5cm, minimum height=.75cm, aspect=1] (exit) [right of=init] {};
   \node [state] (bad) [above=0.3cm of exit] {};
   \node [state] (sink) [right of=exit] {};
   \node [cloud, draw=black,cloud puffs=15, cloud puff arc= 150,
        minimum width=1.5cm, minimum height=.75cm, aspect=1] (diverge) [below=0.5 cm of exit] {};

    \node [] (divergetext) [below=-0.825 cm of diverge] {\small};

   \node [state] (haken1) at (2.2, .1) {\scriptsize };
   \node [state] (haken2) at (2.25, -.1) {\tiny };
    \node [state] (haken3) at (2.7, -.1) {\scriptsize };
   \node [state] (haken4) at (2.75, .1) {\tiny };
   \node [state] (haken5) at (2.95, .0) {\tiny };

  \path [] 
      (init) edge [decorate,decoration={snake, post length=2mm}] (exit)
      (init) edge [decorate,decoration={snake, post length=2mm}] (bad)
      (init) edge [decorate,decoration={snake, post length=2mm}] (diverge)
      (exit) edge [] (sink)
      (bad) edge [] (sink)
      (sink) edge [loop right] (sink)
      (diverge) edge [loop right,decorate,decoration={snake, post length=2mm}] (diverge)
  ;
\end{tikzpicture}
 }
	\end{center}
Squiggly arrows indicate reaching certain states via possibly multiple paths and states; the clouds indicate that there might be several states of the particular kind.
 marks the initial state of the program . 
In general the states of the operational MDP are of the form  where  is the program that is left to be executed and  is the current variable valuation.

All runs of the program (paths through the MDP) are either \emph{terminating} and eventually end up in the \sink state, or are \emph{diverging} (thus they never reach \sink).
Diverging runs occur due to non--terminating computations.
A terminating run has either terminated successfully, \ie, it passes a \exit--state, or it has terminated due to a \emph{violation of an observation}, \ie, it passes the --state.
Sets of runs that eventually reach \undesired, or \sink, or diverge are pairwise disjoint. 

\noindent The \exit--labelled states are the \emph{only ones} with positive reward, which is due to the fact that we want to capture probabilities of events (respectively expected values of random variables) occurring at \emph{successful termination} of the program.

The random variables of interest are .
Such random variables are referred to as post--expectations~\cite{McIver:2004}. 
Formally, we have:
\begin{definition}[Operational Semantics of Programs]
The \emph{operational semantics} of a \cpGCL program  with respect to a post--expectation  is the MDP ,  together with a reward function , where
\begin{itemize}
\item is the countable set of states,
\item  is the initial state,
\item  is the set of actions, and
\item  is the smallest relation defined by the SOS rules given in Figure \ref{fig:sos-rules}.
\end{itemize}
The reward function is  if , and , otherwise.
\end{definition}
\begin{figure}[t]
\scriptsize
\normalsize
\caption{
SOS rules for constructing the operational MDP of a \cpGCL program.
We use  to indicate ,  for  to indicate ,  to indicate , and  to indicate .
}
\label{fig:sos-rules}
\end{figure}
A state of the form  indicates successful termination, \ie, no commands are left to be executed.
These terminal states and the \undesired--state go to the \sink state. 
\Skip without context terminates successfully.
\Abort self--loops, \ie, diverges. 
 alters the variable valuation according to the assignment then terminates successfully. 
For the concatenation,  indicates successful termination of the first program, so the execution continues with .
If for  the execution of  leads to ,  does so, too.
Otherwise, for ,  is lifted such that  is concatenated to the support of . For more details on the operational semantics we refer to~\cite{DBLP:journals/pe/GretzKM14}.

If for the conditional choice  holds,  is executed, otherwise .
The case for  is similar.
For the probabilistic choice, a distribution  is created according to probability . 
For , we call  the  choice and  the  choice for actions .
For the \Observe statement, if  then \Observe acts like . 
Otherwise, the execution leads directly to \undesired indicating a violation of the \Observe statement.
\begin{figure}[t]
\begin{center}
\scalebox{0.8}{\begin{tikzpicture}[->,>=stealth',shorten >=1pt,semithick,minimum size=.5cm]
\tikzstyle{every state}=[draw=none]
\node [] (init) at (0,0) {};  
    \node [] (s1) [on grid, below=1.1 cm of init] {};   
    \node [] (s11) [on grid, left=4 cm of s1] {};   
    \node [] (s12) [on grid, right=4 cm of s1] {};   
	\node [] (s111) [on grid, below=1.1 cm of s11] {};   
    \node [] (s121) [on grid, below=1.1 cm of s12] {};   
    \node [] (s1111) [on grid, below=1.1 cm of s111] {};   
    \node [] (s1211) [on grid, below=1.1 cm of s121] {};   
    \node [] (s1211b) [on grid, below=1.1 cm of s1211] {};   
    \node [] (s1211bb) [on grid, below=1.1 cm of s1211b] {}; 
\node [] (s11111) [on grid, left=4 cm of s1111] {};   
    \node [] (s11112) [on grid, right=4 cm of s1111] {};   
    \node [] (s111111) [on grid, below=1.1 cm of s11111] {};   
    \node [] (s111121) [on grid, below=1.1 cm of s11112] {};   
    \node [] (s1111111) [on grid, below=1.1 cm of s111111] {};   
    \node [] (s1111211) [on grid, below=1.1 cm of s111121] {};
    \node [] (s1111211b) [on grid, below=1.1 cm of s1111211] {};
    \node [] (s1111211bb) [on grid, below=1.1 cm of s1111211b] {};
\node [label={[yshift=-0.5cm] 170:\large\boxed{}}] (s11112111) [on grid, below=1.1 cm of s1111211bb] {};


    

  \node [] (undesired) [on grid, below=1.1 cm of s1211bb] {};  
  \node [] (sink) [on grid, below=2.2 cm of undesired] {};  
  \node [] (dots) [on grid, below=.5 cm of s1111111] {};  
	
\path
      (init) edge [] node [left, near start] {} (s1)
      (s1) edge [] node [above, near start] {\scriptsize{}} (s11)
      (s1) edge [] node [above, near start] {\scriptsize{}} (s12)
      (s11) edge [] (s111)
      (s12) edge [] (s121)
      (s111) edge [] (s1111)
      (s121) edge [] (s1211)
(s1211) edge [] (s1211b)
      (s1211b) edge [] (s1211bb)
      (s1211bb) edge [] (undesired)
      (s1111) edge [] node [above, near start] {\scriptsize{}} (s11111)
      (s1111) edge [] node [above, near start] {\scriptsize{}} (s11112)
      (s11111) edge [] node [right, near start] {} (s111111)	
      (s11112) edge [] node [right, near start] {} (s111121)	
      (s111111) edge [] node [right, near start] {} (s1111111)	
      (s111121) edge [] node [right, near start] {} (s1111211)	
      (s1111211) edge [] node [right, near start] {} (s1111211b)
      (s1111211b) edge [] node [right, near start] {} (s1111211bb)	
      (s1111211bb) edge [] node [right, near start] {} (s11112111)
      (s11112111) edge [] node [right, near start] {} (sink)	
(undesired) edge [] (sink)
      (sink) edge [loop right] (sink)
;
\end{tikzpicture}
 }	
\end{center}
\caption{Partially unrolled operational semantics for program }	
\label{fig:better_operational_example}
\end{figure}
\begin{example}


Reconsider Example~\ref{ex:simple}, where we set for readability , x, , and . 
A part of the operational MDP  for an arbitrary initial variable valuation  and post--expectation  is depicted in Figure~\ref{fig:better_operational_example}.\footnote{We have tacitly overloaded the variable name  to an expectation here for readability. More formally, by the ``expectation " we actually mean the expectation .}
Note that this MDP is an MC, as  contains no nondeterministic choices. The MDP has been unrolled until the second loop iteration, \ie, at state , the unrolling could be continued. The only terminating state is . 
As our post-expectation is the value of variable , we assign this value to terminating states, \ie, reward  at state , where  has been assigned . At state , the loop condition is violated as is the subsequent observation because of  being assigned an even number.
\hfill
\end{example}

\iffalse
\begin{example}
Consider the following \cpGCL program  with variables  and :
\begin{CenteredBox}
				\lstinputlisting[language=C,linewidth=9cm]{resources/example_operational.pp}
			\end{CenteredBox}

\noindent With parametrized probability  a nondeterministic choice between  being assigned  or  is executed, and with probability ,  is directly assigned .
Let for readability , , , and . 
The operational MDP  with respect to post--expectation  is depicted below:
\begin{center}
\scalebox{0.8}{\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-6.6,-6.3) rectangle (3.4,0.3);
   \node [state, initial where=above, initial text=] (init) at (0,0) {};  
    \node [] (s1) [on grid, below=1.5 cm of init, xshift=-2.2cm] {};   
\node [] (s11) [on grid, below=1.5 cm of init, xshift=2.2cm] {};   
        \node [] (s12) [on grid, left=2 cm of s1, xshift=-1.2cm] {};   
\node [] (s111) [on grid, below=1.5 cm of s11] {};   
        \node [] (s121) [on grid, below=1.5 cm of s12] {};   
\node [] (s1111) [on grid, below=1.5 cm of s111] {};   
         \node [] (s1211) [on grid, right=3.3 cm of s121] {};   
         \node [label={[yshift=0.0cm, gray] 180:}] (s12111) [on grid, below=1.5 cm of s1211] {};   
 \node [] (undesired) [on grid, below=1.5 cm of s1111, xshift=-.5cm] {};  
  \node [] (sink) [on grid, below=1.5 cm of s12111, xshift=1.5cm] {};  

\path
      (init) edge [] node [left, near start] {\scriptsize{}} (s1)
      (init) edge [] node [right, near start] {\scriptsize{}} (s11)
      (s1) edge [] node [below=-0.3cm] {\scriptsize{}} (s12)
      (s1) edge [] node [below=-0.3cm] {\scriptsize{}} (s11)
(s12) edge [] (s121)
      (s11) edge [] (s111)
      (s111) edge [] (s1111)
      (s121) edge [] (s1211)
(s1111) edge [] (undesired)
      (s1211) edge [] (s12111)
(undesired) edge [] (sink)
      (s12111) edge [] (sink)
      (sink) edge [loop above] (sink)
;
\end{tikzpicture}
 }\\
\end{center}
The only state with positive reward is  and its reward is indicated by the number  next to it.
This is the only state since it is the only state that constitutes a \emph{successful} termination, i.e.\ the program has terminated and no observation was violated.
Assume first a scheduler choosing action  in state .
In the induced MC the only path accumulating positive reward is the path  going from  via
 to  with  and . 
This gives an expected reward of .
The overall probability of not reaching  is also . 
The conditional expected reward of eventually reaching  given that  is not reached is hence . 
Assume now the \emph{demonic} scheduler choosing  at state . 
In this case there is no path having positive accumulated reward in the induced MC, yielding an expected reward of . 
The probability of not reaching  is also . 
The conditional expected reward in this case is undefined () and thus the  branch is preferred over the  branch by the demon. 
In general, the operational MDP need not be finite, even if the program terminates almost--surely (\ie, with probability 1).
\nj{show example properties in the style we had in the preliminaries.}
\hfill
\end{example}
\fi

