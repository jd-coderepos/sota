\documentclass[letterpaper]{article} \usepackage{aaai24}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in} \setlength{\pdfpageheight}{11in} \usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{threeparttable}
\usepackage{nicefrac}
\usepackage{adjustbox}
\usepackage{subcaption}

\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\pdfinfo{
/TemplateVersion (2024.1)
}



\setcounter{secnumdepth}{0} 





\title{Spatio-Temporal-Decoupled Masked Pre-training for Traffic Forecasting }




\author{
    Haotian Gao\textsuperscript{\rm 1},
    Renhe Jiang\textsuperscript{\rm 2}\thanks{Corresponding author.},    
    Zheng Dong\textsuperscript{\rm 1},
    Jinliang Deng\textsuperscript{\rm 3},
    Xuan Song\textsuperscript{\rm 1,2}
}
\affiliations{
    \textsuperscript{\rm 1}Southern University of Science and Technology, 
    \textsuperscript{\rm 2}The University of Tokyo, 
    \textsuperscript{\rm 3}University of Technology Sydney\\
    gaoht6@outlook.com, jiangrh@csis.u-tokyo.ac.jp, zhengdong00@outlook.com, jinliang.deng@student.uts.edu.au, songx@sustech.edu.cn

}







\usepackage{bibentry}


\begin{document}

\maketitle

\begin{abstract}
Accurate forecasting of multivariate traffic flow time series remains challenging due to substantial spatio-temporal heterogeneity and complex long-range correlative patterns. To address this, we propose Spatio-Temporal-Decoupled Masked Pre-training (STD-MAE), a novel framework that employs masked autoencoders to learn and encode complex spatio-temporal dependencies via pre-training. Specifically, we use two decoupled masked autoencoders to reconstruct the traffic data along spatial and temporal axes using a self-supervised pre-training approach. These mask reconstruction mechanisms capture the long-range correlations in space and time separately. The learned hidden representations are then used to augment the downstream spatio-temporal traffic predictor. A series of quantitative and qualitative evaluations on four widely-used traffic benchmarks (PEMS03, PEMS04, PEMS07, and PEMS08) are conducted to verify the state-of-the-art performance, with STD-MAE explicitly enhancing the downstream spatio-temporal models' ability to capture long-range intricate spatial and temporal patterns. Codes are available at \url{https://github.com/Jimmy-7664/STD_MAE}.
\end{abstract}

\section{Introduction}
Spatio-temporal data collected by sensor networks has become a vital area of research with many real-world applications. One key application is traffic forecasting, which has garnered significant interest in recent years. While traffic forecasting resembles multivariate time series forecasting, it benefits from extra spatial context like sensor locations and road networks that reveal dependencies between sensor signals. As a result, a key distinction from ordinary multivariate time series is that traffic data exhibits spatio-temporal heterogeneity. While traffic conditions vary across different road types (local, highway, interchange) and times (weekday versus weekend), they demonstrate consistent, predictable patterns within similar contexts. Effectively modeling the heterogeneity while identifying key patterns is crucial for accurate traffic forecasting.
\footnote{Our codes are available at \url{https://github.com/Jimmy-7664/STD_MAE}}
\begin{figure}[t]
  \centering
  \begin{subfigure}{1\linewidth}
    \includegraphics[width=\linewidth]{figure/Fig1a.png}
    \caption{Temporal Heterogeneity}
    \label{fig1-sub1}
  \end{subfigure}
  \begin{subfigure}{1\linewidth}
    \includegraphics[width=\linewidth]{figure/Fig1b.png}
    \caption{Spatial Heterogeneity}
    \label{fig1-sub2}
  \end{subfigure}
  \caption{Spatio-Temporal Heterogeneity in Traffic Data}
  \label{fig1}
\end{figure}



Figure~\ref{fig1} provides an example of spatio-temporal heterogeneity in traffic data. The traffic flow of sensor 7 in the PEMS04 dataset is plotted to illustrate the temporal dimension. As shown in Figure~\ref{fig1-sub1}, the traffic flow trends on Tuesday and Wednesday and on Saturday and Sunday are similar, but distinct patterns emerge on weekdays versus weekends, especially during peak hours. On weekdays, there is a morning rush hour peak, while the traffic flow on weekends is more evenly distributed throughout the day without dramatic peaks. This demonstrates temporal heterogeneity in traffic patterns over a week. On the other hand, the patterns within the same context exhibit temporal redundancy~\cite{he2022masked}. To demonstrate spatial heterogeneity, the traffic flow of sensor 5, 86, 155, and 177 is plotted. Sensors 5 and 177 are located on main roads, while sensors 86 and 155 are on minor roads. As illustrated in Figure~\ref{fig1-sub2}, the flow trends on the main roads are similar, which reach higher volumes than the minor roads during daytime, indicating spatial redundancy between similar road types. The minor road sensors follow comparable stationary patterns without dramatic peaks and valleys. This highlights the spatial heterogeneity between main arterial roads that experience relatively high traffic volumes and secondary roads with more consistent moderate flows. Commonly used GCN-based models~\cite{li2018diffusion, yu2018spatio, bai2020adaptive} have difficulty capturing spatio-temporal heterogeneity due to their information propagation mechanisms relying only on adjacency relationships. Moreover, these models often struggle to process very long historical time series, limiting their ability to learn from long-term spatial-temporal contexts. Although some recent non-GCN methods~\cite{shao2022pre, zhou2021informer, wu2021autoformer} can model long-range temporal correlations, they still fail to adequately capture the spatial relationships between different locations.

To address these challenges, we focus our approach on learning key patterns through masked pre-training. Masked pre-training is an effective technique to learn heterogeneity and eliminate redundancy. This approach has shown tremendous effectiveness for natural language processing (NLP)~\cite{kenton2019bert} and computer vision (CV)~\cite{bao2021beit} models. The core idea is to mask parts of the input sequence during pre-training, requiring the model to predict the missing content. This allows the model to learn robust features that capture common spatio-temporal patterns, generating useful representations for various downstream tasks. Motivated by these benefits, we propose a novel spatio-temporal-decoupled masked pre-training framework called STD-MAE. STD-MAE offers a simple yet effective solution for learning inherent spatio-temporal heterogeneity through pre-training. Specifically, STD-MAE contains two decoupled masked autoencoders (MAEs)~\cite{he2022masked}. The spatial encoder and temporal encoder separately encode long-term historical data by randomly masking traffic time series respectively on spatial and temporal axes. Then the corresponding decoders reconstruct the masked inputs. Finally, the hidden representations generated by the encoders can be applied to boost the performance of downstream spatio-temporal forecasting models. In summary, our key contributions are as follows:
\begin{itemize}
    \item We devise a pre-training technique on spatio-temporal data that can largely augment the downstream spatio-temporal predictors with learned hidden representations. \item We propose a novel spatio-temporal masking strategy to learn spatio-temporal heterogeneity by capturing long-range context across spatial and temporal axes.
    \item We validate STD-MAE on four traffic benchmark datasets (PEMS03, PEMS04, PEMS07, and PEMS08). Quantitative evaluations show significant performance gains over state-of-the-art models. Qualitative analyses demonstrate STD-MAE can capture meaningful long-range spatio-temporal patterns.
\end{itemize}



\section{Related Work}
\begin{figure*}[t]
	\centering
\includegraphics[width=1.0\textwidth]{./figure/Figure2_1.png}
	\caption{Spatio-Temporal-Decoupled Masked Pre-training Framework (STD-MAE)}
	\label{fig:architecture}
\end{figure*}
\subsection{Traffic Forecasting}
Traffic forecasting~\cite{jiang2021dl} aims to predict future traffic time series in road networks by analyzing historical data. Early work mainly relied on traditional time series models~\cite{pan2012utilizing, stock2001vector}. To capture the complex temporal dependencies, RNNs~\cite{hochreiter1997long, chung2014empirical, ma2015long, lv2018lc} and CNNs~\cite{oord2016wavenet} have gained popularity for better modeling traffic time series and achieving improved predictions.

Nevertheless, these models overlook crucial spatial correlations, limiting predictive performance on networked road systems. To further capture spatio-temporal features jointly, some studies integrate Graph Convolutional Networks (GCNs) with temporal models~\cite{yu2018spatio, li2018diffusion, zhao2019t}. Following this line of research, several novel spatio-temporal models have been proposed in recent years~\cite{wu2019graph, xu2020spatial, bai2020adaptive, cao2020spectral, wu2020connecting, guo2021hierarchical, han2021dynamic, lee2021learning, jiang2023spatio}. 

Attention mechanisms~\cite{vaswani2017attention} have also profoundly influenced traffic forecasting. A series of traffic transformers~\cite{zheng2020gman, xu2020spatial, jiang2023pdformer} and time series transformers~\cite{zhou2021informer, xu2021autoformer, liu2021pyraformer, zhou2022fedformer} were proposed to do the long sequence time series modeling. However, they often overlook crucial inter-series dependencies.

\subsection{Masked Pre-training} 
Masked pre-training has emerged as a highly effective technique for self-supervised representation learning in both natural language processing (NLP) and computer vision (CV). The key idea is to train models to predict missing or masked-out parts of the input based on surrounding context. In NLP, approaches like BERT~\cite{kenton2019bert} use masked language modeling to predict randomly masked tokens using bidirectional context. Subsequent models~\cite{liu2019roberta, lan2019albert, clark2020electra} introduced more efficient masking techniques and demonstrated performance gains from longer pre-training. In CV, similar masked pre-training strategies have been adopted. Methods like BEiT~\cite{bao2021beit} and MAE~\cite{he2022masked} mask out random patches of input images and train models to infill the missing regions based on surrounding pixels. Other techniques include predicting permutations of masked patches~\cite{xie2022simmim} and vector quantization with discrete masks~\cite{van2017neural}. In both domains, masked modeling produces substantial improvements over pre-training on various downstream tasks.




Recently, STEP~\cite{shao2022pre} becomes the first attempt to employ masked pre-training technique on traffic time series data to obtain superior hidden representations. However, STEP only did masked pre-training along temporal axis, neglecting pre-training in spatial relationships. Our proposed STD-MAE introduces a novel spatio-temporal masking strategy during pre-training. By masking separately on spatial and temporal axes, the learned representations by STD-MAE can well capture the intricate spatio-temporal heterogeneity in traffic multivariate time series data.


\section{Problem Definition}
Traffic forecasting is a specialized multivariate time series forecasting problem. Given multivariate time series  in the past  time steps, our goal is to predict the future  time steps as:    , where    for the -th time step,  is the number of spatial nodes, and  is the number of the information channel. In our study,  is equal to 1 as we only forecast the traffic flow.

\section{Methodology}
In this section, we present a spatio-temporal-decoupled masked pre-training method (\textbf{STD-MAE}) as delineated in Figure~\ref{fig:architecture}. 
STD-MAE contains two parts, i.e., Spatio-Temporal-Decoupled Masked AutoEncoder and Downstream Spatio-Temporal Predictor, which first learns long-range spatial and temporal representations via masked pre-training and then leverages them to enhance the downstream spatio-temporal predictor. We elaborate the details of them in the following two subsections. 
\subsection{Spatio-Temporal-Decoupled Masked AutoEncoder}
\noindent\textbf{Patch Embedding.} As shown in Figure~\ref{fig1-sub1}, traffic time series have long-range temporal heterogeneity between weekdays and weekends, which is crucial for traffic forecasting. However, in the standard traffic forecasting task, the input length  for  is usually equal to 12~\cite{li2018diffusion,yu2018spatio,wu2019graph,song2020spatial}. To effectively capture the long-term dependency, we introduce a pre-training phase that can involve a long input  with  time steps, containing data up to several weeks. However, long input data may cause efficiency degradation and information redundancy issue. Thus, in our study, we apply patch embedding to the long input. Specifically, the long time series is first divided into non-overlapping  =  patches by using a patch window . The patched input, donated as , is then projected via a linear layer to obtain the patch embedding  as  = , where  is the embedding dimension.





\noindent\textbf{Spatio-Temporal Positional Encoding.}
Vanilla transformer~\cite{vaswani2017attention} encodes the positional information of language sequence with one-dimensional sine-cosine positional encoding. However, traffic multivariate time series data is two-dimensional, so we implement two-dimensional positional encoding~\cite{wang2021translating} to simultaneously encode the spatio-temporal positional information. Given the patch embedding , we let the spatio-temporal positional encoding have the same embedding dimension  as . Specifically,  can be calculated as follows:

where  and  are the temporal and spatial indices for  and , and . From Equation~\ref{eq:pos}, we can see that the first and second halves of encoding dimension  are responsible for different information. Specifically, the first half of the encoding dimension  is designed for the sole purpose of encoding temporal position. Conversely, the latter half of the dimension  is attributed to the encoding of spatial position. We choose sinusoidal positional encoding instead of learned positional encoding because the former allows the application to longer input time series than those used during pre-training. Then, we sum these two embeddings together to obtain the final input embedding  by letting  =  + .







\noindent\textbf{Spatio-Temporal Masking.} To clearly and efficiently capture the long-range spatio-temporal correlations, we utilize two decoupled MAEs with different masking strategies, i.e., temporal masking (T-Mask) and spatial masking (S-Mask), as shown in Figure~\ref{fig:architecture}. Given input embedding  and masking ratio , S-Mask randomly masks the whole time series data of  =  sensors. Visible spatial patch embedding  is denoted as . Similarly, T-Mask randomly masks  =  patches from all sensors, yielding visible temporal patch embedding . Both random masking strategies can be viewed as random sampling using a Bernoulli distribution  with expectation  in different dimensions as:








\noindent\textbf{Spatio-Temporal AutoEncoding.} For the spatial and temporal encoder, we use a similar structure. They both consist of a patch embedding layer, a spatio-temporal positional encoding layer, and a transformer encoder with four standard transformer layers. Following the vanilla transformer encoder~\cite{vaswani2017attention}, we apply layer normalization, residual connection, and multi-head attention mechanism.  applies attention mechanism along the spatial axis, while the temporal transformer encoder  does the same along the temporal axis. Given visible spatial patch embedding  and temporal patch embedding , spatial representation  and temporal representation  can be generated as follows:

By focusing only on the visible part, such design reduces time and memory complexity. 






The spatial and temporal decoder consists of a padding layer, a standard transformer layer, and a regression layer. Since the decoder is only used in the pre-training part for reconstructing the inputs, we use a lightweight decoder architecture. Such asymmetrical design could dramatically reduce the pre-training time~\cite{he2022masked}. In the padding layer, we use mask tokens~\cite{devlin2018bert} to indicate missing patches, each of which is a shared, learnable vector . Given spatial representation  and temporal representation , spatial padding expands  to spatial mask tokens  while temporal padding expands  to temporal mask tokens . The same spatio-temporal positional encoding as the Encoder is added to  and . Then we do concatenation operations respectively as  and  to get the full set of patches  and . Subsequently, the full set of patches is passed to the transformer layer. Like the encoding phase, the attention mechanisms in the transformer layer of spatial and temporal decoder are respectively applied to different axes. Finally, the spatial and temporal decoder use a regression layer to reconstruct the time series at the patch level. Formally, the reconstruction  and  for the spatial/temporal masked input can be derived by:

where  is the spatial masked index and  is the temporal masked index.


Following other masked pre-training architectures~\cite{he2022masked,tong2022videomae,feichtenhofer2022masked}, we compute the loss on the masked part only. Our loss function is computed by calculating the mean absolute error (MAE) between the ground truth and the reconstruction result. 
Given spatial reconstruction , the ground truth corresponding to the masked part is , and given temporal reconstruction , the ground truth corresponding to the masked part is  .
The two loss functions in spatial and temporal can be represented as:


In summary, the spatial autoencoder is forced to reconstruct the data of masked sensors solely from the other visible sensors, through which long-range spatial dependencies could be captured. Similarly, the temporal autoencoder can learn the temporal correlations by only utilizing the intrinsic visible time series data to reconstruct the entire time series.





\subsection{Downstream Spatio-Temporal Predictor}
STD-MAE can be integrated into existing predictor structures without any architectural changes. This operation is done by adding the spatial and temporal representations generated by STD-MAE to the hidden representation of the predictor. Concretely, we first feed long-range input with  time steps into pre-trained spatial and temporal encoders to generate the corresponding spatial  representation and temporal representation . Then, we apply a downstream spatio-temporal predictor  with parameter  to obtain the hidden representation   of the widely-used short input  through the following:

where  is the hidden representation dimension of the predictor. To align with , we truncate the representation  and  of the last  patches, and reshape these two representations to  and . Next, we project these two representations into  dimension through a two-layer MLP. Finally, the augmented representation  could be derived by adding these representations together:

By far,  includes representations generated by the predictor itself as well as the long-range spatial and temporal representations from STD-MAE, which can largely enhance the performance of the downstream S-T predictor. 

Specifically, in our work, we choose GWNet~\cite{wu2019graph} as our predictor due to its superior performance. We sum up the hidden states obtained by skip connections through multiple spatio-temporal layers of GWNet and the corresponding spatial/temporal representations generated by STD-MAE. The augmented representation is then fed into GWNet's regression layers for prediction. Furthermore, we also test other classical S-T predictors with a variety of structures, i.e., DCRNN~\cite{li2018diffusion}, MTGNN~\cite{wu2020connecting}, STID~\cite{shao2022spatial} and STAEformer~\cite{liu2023spatio}. The experiments show STD-MAE could also significantly improve their performances, the details of which could be found in our ablation study. 








\section{Experiment}\label{sec:experiment-setup}
\subsection{Experimental Setup}
\noindent\textbf{Datasets.} To thoroughly evaluate the proposed STD-MAE model, we conduct extensive experiments on four real-world traffic flow benchmark datasets: PEMS03, PEMS04, PEMS07, and PEMS08~\cite{song2020spatial}. These datasets are collected from the California Transportation Performance Measurement System (PeMS)~\cite{chen2001freeway} and contain traffic data from different transportation districts, providing diversity in spatial and temporal patterns. The raw data has a fine-grained time resolution of 5 minutes between consecutive time steps. For data preprocessing, we perform Z-score normalization on the raw inputs to rescale the data to have zero mean and unit variance. Key statistics of the four benchmarks are summarized in Table~\ref{tab:dataset}.
\begin{table}[h] \renewcommand\arraystretch{1.2}
    \footnotesize
    \begin{tabular}{ccccc}
    \hline
    Datasets & PeMS03 &  PeMS04&  PeMS07& PeMS08\\ \hline
\#Sensors    & 358  &  307   & 883  & 170  \\
\#Time Steps    & 26208  & 16992  & 28224  & 17856\\
    Time Interval    &  5min&  5min&  5min& 5min\\ 
\hline
    \end{tabular}\caption{Summary of Four Traffic Benchmarks}
  \label{tab:dataset}\end{table}
\begin{table*}[h]
	\renewcommand{\arraystretch}{1.1}
	\centering
	\resizebox{1.0\linewidth}{!}{
	\begin{tabular}{l|c c c|c c c |c c c|c c c}
		\toprule[1.2pt]
		\multirow{2}{*}{Model} & \multicolumn{3}{c|}{PEMS03} & \multicolumn{3}{c|}{PEMS04} & \multicolumn{3}{c|}{PEMS07} & \multicolumn{3}{c}{PEMS08} \\
{}& MAE & RMSE & MAPE & MAE & RMSE & MAPE & MAE & RMSE & MAPE & MAE & RMSE & MAPE  \\
		\midrule
ARIMA~\cite{fang2021spatial}                & 35.31                & 47.59          & 33.78                      & 33.73                & 48.80             & 24.18                    & 38.17                &  59.27               & 19.46                & 31.09                &  44.32              & 22.73                 \\
VAR~\cite{song2020spatial}                  & 23.65                & 38.26          & 24.51                      & 23.75                & 36.66            & 18.09                    & 75.63                &  115.24              & 32.22                & 23.46                &  36.33              & 15.42                 \\
SVR~\cite{song2020spatial}                  & 21.97                & 35.29          & 21.51                      & 28.70                 & 44.56            & 19.20                     & 32.49                &  50.22               & 14.26                & 23.25                &  36.16              & 14.64                 \\
LSTM~\cite{song2020spatial}                 & 21.33                & 35.11          & 23.33                      & 27.14                & 41.59            & 18.20                     & 29.98                &  45.84               & 13.20                 & 22.20                 &  34.06              & 14.20                  \\
TCN~\cite{lan2022dstagnn}                   & 19.31                & 33.24          & 19.86                      & 31.11                & 37.25            & 15.48                    & 32.68                &  42.23               & 14.22                & 22.69                &  35.79              & 14.04                 \\
Transformer~\cite{vaswani2017attention}     & 17.50                 & 30.24          & 16.80                       & 23.83                & 37.19            & 15.57                    & 26.80                 &  42.95               & 12.11                & 18.52                &  28.68              & 13.66                 \\
DCRNN~\cite{li2018diffusion}                & 18.18                & 30.31          & 18.91                      & 24.70                 & 38.12            & 17.12                    & 25.30                 &  38.58               & 11.66                & 17.86                &  27.83              & 11.45                 \\
STGCN~\cite{yu2018spatio}                   & 17.49                & 30.12          & 17.15                      & 22.70                 & 35.55            & 14.59                    & 25.38                &  38.78               & 11.08                & 18.02                &  27.83              & 11.40                  \\
ASTGCN~\cite{guo2019attention}           & 17.69                & 29.66          & 19.40                       & 22.93                & 35.22            & 16.56                    & 28.05                &  42.57               & 13.92                & 18.61                &  28.16              & 13.08                 \\
STG2Seq~\cite{bai2019stg2seq}               & 19.03                & 29.73          & 21.55                      & 25.20                 & 38.48            & 18.77                    & 32.77                &  47.16               & 20.16                & 20.17                &  30.71              & 17.32                 \\
GWNet~\cite{wu2019graph}                    & 19.85                & 32.94          & 19.31                      & 25.45                & 39.70             & 17.29                    & 26.85                &  42.78               & 12.12                & 19.13                &  31.05              & 12.68                 \\
STSGCN~\cite{song2020spatial}               & 17.48                & 29.21          & 16.78                      & 21.19                & 33.65            & 13.90                     & 24.26                &  39.03               & 10.21                & 17.13                &  26.80               & 10.96                 \\
STFGNN~\cite{li2021spatial}                 & 16.77                & 28.34          & 16.30                       & 19.83                & 31.88            & 13.02                    & 22.07                &  35.80                & 9.21                 & 16.64                &  26.22              & 10.60                  \\
STGODE~\cite{fang2021spatial}               & 16.50                 & 27.84          & 16.69                      & 20.84                & 32.82            & 13.77                    & 22.99                &  37.54               & 10.14                & 16.81                &  25.97              & 10.62                 \\
DSTAGNN~\cite{lan2022dstagnn}               & 15.57                & 27.21          & 14.68                      & 19.30                 & 31.46            & 12.70                     & 21.42                &  34.51               & 9.01                 & 15.67                &  24.77              & 9.94                  \\
ST-WA~\cite{cirstea2022towards}             & 15.17                & 26.63          & 15.83                      & 19.06                & 31.02            & 12.52                    & 20.74                &  34.05               & 8.77                 & 15.41                &  24.62              & 9.94                  \\
ASTGNN~\cite{guo2021learning}               & 15.07                & 26.88          & 15.80                       & 19.26                & 31.16            & 12.65                    & 22.23                &  35.95               & 9.25                 & 15.98                &  25.67              & 9.97                  \\
EnhanceNet~\cite{cirstea2021enhancenet}     & 16.05                & 28.33          & 15.83                      & 20.44                & 32.37            & 13.58                    & 21.87                &  35.57               & 9.13                 & 16.33                &  25.46              & 10.39                 \\
AGCRN~\cite{bai2020adaptive}                & 16.06                & 28.49          & 15.85                      & 19.83                & 32.26            & 12.97                    & 21.29                &  35.12               & 8.97                 & 15.95                &  25.22              & 10.09                 \\
Z-GCNETs~\cite{chen2021z}                   & 16.64                & 28.15          & 16.39                      & 19.50                 & 31.61            & 12.78                    & 21.77                &  35.17               & 9.25                 & 15.76                &  25.11              & 10.01                 \\
STEP~\cite{shao2022pre}                     & 14.22                & 24.55          & 14.42                      & 18.20                 & 29.71            & 12.48                    & 19.32                &  32.19               & 8.12                 & 14.00                &  23.41              & 9.50                  \\
PDFormer~\cite{jiang2023pdformer}           & 14.94                    & 25.39              & 15.82                          & 18.32                & 29.97            & 12.10               & 19.83                &  32.87               & 8.53                 & 13.58                &  23.51              & 9.05                  \\
STAEformer~\cite{liu2023spatio}           & 15.35                    & 27.55             & 15.18                         & 18.22                & 30.18            & \bf{11.98}               & 19.14                &  32.60               & 8.01                & 13.46                &  23.25             & 8.88                  \\\hline
\textbf{STD-MAE (Ours)}                                    & \bf{13.80}           & \bf{24.43}     & \bf{13.96}                 & \bf{17.80}           & \bf{29.21}       & 12.07                    & \bf{18.65}           &  \bf{31.44}          & \bf{7.84} &       
\bf{13.44}           &  \bf{22.47}         & \bf{8.76}             \\
     \bottomrule[1.2pt]
     \end{tabular}
	}
	\caption{Performance Comparison with Baseline Models on PEMS03,04,07,08 Benchmarks}
	\label{tab:overall}
\end{table*}

\noindent\textbf{Baselines.} We compare STD-MAE with the following baseline methods. ARIMA~\cite{fang2021spatial}, VAR~\cite{song2020spatial}, SVR~\cite{song2020spatial}, LSTM~\cite{song2020spatial}, TCN~\cite{lan2022dstagnn}, and Transformer~\cite{vaswani2017attention} are time series models. For spatio-temporal models, we select several typical methods including DCRNN~\cite{li2018diffusion}, STGCN~\cite{yu2018spatio}, ASTGCN~\cite{guo2019attention}, STG2Seq~\cite{bai2019stg2seq}, GWNet~\cite{wu2019graph}, STSGCN~\cite{song2020spatial}, STFGNN~\cite{li2021spatial}, STGODE~\cite{fang2021spatial}, DSTAGNN~\cite{lan2022dstagnn}, ST-WA~\cite{cirstea2022towards}, ASTGNN~\cite{guo2021learning}, EnhanceNet \cite{cirstea2021enhancenet}, AGCRN~\cite{bai2020adaptive}, Z-GCNETs~\cite{chen2021z}, STEP~\cite{shao2022pre}, and PDFormer~\cite{jiang2023pdformer}. Detailed descriptions on these baselines could be found in Appendix.




\noindent\textbf{Settings.} Following previous work~\cite{song2020spatial,li2021spatial,fang2021spatial,jiang2023pdformer,guo2021learning}, we divide the four datasets into training, validation, and test sets according to a 6:2:2 ratio. During pre-training, the long input  of the four datasets are set to 864, 864, 864, and 2016 time steps, respectively, allowing the model to learn from long historical patterns. For prediction, we set the length of both input  and output  to 12 steps. The embedding dimension  is 96. The encoder has 4 transformer layers while the decoder has 1 transformer layer. The number of multi-attention heads in transformer layer is set to 4. We use a patch size  of 12 to align with the forecasting input.  is equal to 1, which means we truncate and keep the last one patch of  and . The masking ratio  is set to 0.25. Optimization is performed with Adam~\cite{kingma2014adam} using an initial learning rate of 0.001 and mean absolute error (MAE) loss. For evaluation, we use MAE, root mean squared error (RMSE), and mean absolute percentage error (MAPE). Performance is assessed by averaging over all 12 prediction steps. All experiments are conducted on a Linux server with four NVIDIA GeForce RTX 3090 GPUs.

\subsection{Overall Performance}\label{sec:experiment-overall}


The performance of the models is listed in Table~\ref{tab:overall}. For a fair comparison, the reported results of the baseline models are taken from the original literature, which have been widely cited and validated in the traffic forecasting task. Across all datasets, our STD-MAE model achieves superior performance over the baselines by a significant margin on almost all evaluation metrics. We believe that the long-range mask reconstruction approach enables capturing intricate spatial and temporal correlations by separately masking the spatial and temporal axes. This allows the model to learn holistic traffic flow characteristics from a broad spatio-temporal context. For other methods, the spatio-temporal models clearly outperform the time series models due to their ability to capture spatio-temporal dependencies. Our model is built upon BasicTS~\cite{shao2023exploring} to make a fair and consistent comparison. In summary, the proposed STD-MAE framework significantly advances the state-of-the-art in traffic forecasting, demonstrating its ability to augment downstream prediction models.


\subsection{Ablation Study}










\noindent\textbf{Masking Ablation.}
We design four variants to validate the effectiveness of our spatio-temporal masking mechanism:
\begin{itemize}
  \item \textbf{S-MAE}: Only masking on the spatial dimension.
  \item \textbf{T-MAE}: Only masking on the temporal dimension.
  \item \textbf{STM-MAE}: Applying spaio-temporal mixed masking.
  \item \textbf{w/o Mask}: Without applying any masked pre-training.
\end{itemize}  
We report the experimental results on the PEMS03 and PEMS07 datasets. As illustrated in Figure~\ref{fig:ablation1}, STD-MAE with both spatial and temporal masking significantly outperforms the ablated versions, demonstrating the importance of capturing spatio-temporal heterogeneity through masked pre-training. Although temporal-only and spatial-only masking still improve over the original model, they are less effective than STD-MAE. For spatio-temporal mixed masking, it even has a negative impact at times. Due to the task of mixed masking is trivial, leading to learn a representation with less rich semantics. Overall, the results highlight the value of our proposed spatio-temporal-decoupled masked pre-training strategy for traffic forecasting.
\captionsetup{justification=raggedright,singlelinecheck=false}
\begin{figure}[h]
  \centering
    \begin{subfigure}{0.95\linewidth}
    \includegraphics[width=\linewidth]{figure/ab1_title.png}
  \end{subfigure}
     \begin{subfigure}{0.95\linewidth}
    \includegraphics[width=\linewidth]{figure/Ablation11.pdf}
  \end{subfigure}
\captionsetup{justification=centering}
  \caption{Masking Ablation on PEMS03 and PEMS07}
  \label{fig:ablation1}
\end{figure}

\noindent\textbf{Predictor Ablation.} To evaluate the generality of STD-MAE, we tested five downstream predictor with different backbones including RNN, GNN, Linear and Transformer:
\begin{itemize}
  \item \textbf{STD-MAE-DCRNN}: Using DCRNN as the predictor.
  \item \textbf{STD-MAE-MTGNN}: Using MTGNN as the predictor.
  \item \textbf{STD-MAE-STID}: Using STID as the predictor.
  \item \textbf{STD-MAE-STAE}: Using STAEformer as the predictor.
  \item \textbf{STD-MAE}: Using GWNet as the predictor.
 \end{itemize}
The experiments are conducted on the PEMS04 and PEMS08 datasets. Table~\ref{tab:ablationstudy2} illustrates consistent and substantial performance gains across all five downstream spatio-temporal predictors when augmented with the STD-MAE framework. This demonstrates the robustness of the representations generated by STD-MAE, which can benefit all kinds of downstream predictors regardless of architectures.

In summary, these ablation studies validate that STD-MAE is crucial for learning intricate spatio-temporal traffic flow patterns, and can significantly enhance downstream spatio-temporal forecasting models of arbitrary architecture.
\begin{table}[t]
    \renewcommand\arraystretch{1.2}
    \footnotesize
	\setlength{\tabcolsep}{0.8mm}{
	\begin{tabular*}{8.4cm}{@{\extracolsep{\fill}}lcc}
		\hline
		\multirow{2}{*}{Model} & PEMS04 & PEMS08 \\
\multicolumn{1}{l}{} & 
		\multicolumn{1}{c}{MAE/RMSE/MAPE} & 
		\multicolumn{1}{c}{MAE/RMSE/MAPE}  \\
		\hline
DCRNN&19.63/31.24/13.52&15.21/24.11/10.04\\
STD-MAE-DCRNN&\bf{18.65/30.09/13.07}&\bf{14.50/23.38/9.36}\\
\hline
MTGNN&19.17/31.70/13.37&15.18/24.14/10.20\\
STD-MAE-MTGNN&\bf{18.93/30.94/13.09}&\bf{14.84/23.58/9.58}\\
\hline
STID&18.35/29.86/12.50  &14.21/23.35/9.32
\\
STD-MAE-STID&\bf{17.93/29.43/12.11}&\bf{13.53/22.60/8.97}
\\
\hline
STAEformer&18.22/30.18/\textbf{11.98} &13.46/23.25/8.88
\\
STD-MAE-STAE&\textbf{17.92/29.37/}12.11&\bf{13.30/22.51/8.82
}
\\\hline
GWNet&18.74/30.32/13.10&14.55/23.53/9.31\\
\textbf{STD-MAE (Ours)} &\bf{17.80/29.21/12.07}&\bf{13.44/22.47/8.76}\\
		\hline
	\end{tabular*}}
\captionsetup{justification=centering}
		\caption{Predictor Ablation on PEMS04 and PEMS08}
		\label{tab:ablationstudy2}
\end{table}
\subsection{Hyper-parameter Study}
\begin{table}[h] \renewcommand\arraystretch{1.2}
    \footnotesize
    \centering
	\setlength{\tabcolsep}{0.8mm}{
	\begin{tabular*}{8.2cm}{@{\extracolsep{\fill}}lccc}
		\hline
		\multirow{2}{*}{Masking Ratio } & PEMS03 &PEMS07 & PEMS08 \\
		\cline{2-4}
		\multicolumn{1}{l}{} & 
		\multicolumn{1}{c}{MAE / RMSE} & 
		\multicolumn{1}{c}{MAE / RMSE} & 
		\multicolumn{1}{c}{MAE / RMSE} \\
		\hline
=0.75&14.25/24.71&19.07/31.88&14.26/23/34\\
\hline
=0.50&14.38/25.32&19.10/31.91 &13.88/23.13\\
\hline
=0.25&\bf{13.80/24.43}&\bf{18.65/31.44}&\bf{13.44/22.47}\\
		\hline
	\end{tabular*}}
     \captionsetup{justification=centering}
	\caption{Hyper-parameter Study on Masking Ratio }
	\label{tab:hyper}
\end{table}
Hyperparameter studies are conducted on the masking ratio  using the PEMS03, PEMS07, and PEMS08 datasets. Specifically,  represents the fraction of input elements that are randomly masked during pre-training. This is applied equally to both S-MAE and T-MAE. Values of 0.25, 0.5, and 0.75 for  are evaluated, where the results are summarized in Table~\ref{tab:hyper}.
Across all datasets, a masking ratio of 0.25 yielded the lowest error, indicating it is the optimal value. Prior work on masked language modeling with BERT~\cite{devlin2018bert} utilizes a relatively low masking ratio of only 15\% during pre-training. In contrast, masked autoencoders for image reconstruction~\cite{he2022masked} and video modeling~\cite{tong2022videomae} have found much higher optimal ratio of 75\% and 90\%, respectively. However, we find a markedly lower optimal ratio of 25\% for traffic time series modeling due to the long input time series required to provide extensive temporal context. This presents a challenge for reconstructing masked inputs, especially with spatial masking patterns that obstruct large contiguous blocks. Furthermore, optimal ratios vary across datasets, demonstrating the importance of tuning masking specifically for traffic data. While an exact optimal is dataset-dependent, our results nonetheless show lower masking ratio is preferable for traffic time series compared to image and video.
\subsection{Efficiency Test}
Since STD-MAE introduce an extra encoder to encode spatial representation, efficiency might be a concern. However, due to the simplicity design of STD-MAE, it still outperforms in efficiency  compare to former pretrained model especially for datasets with a large amount of sensors. We have tested the efficiency on four datasets. We report training time for one epoch in the unit of second as Table \ref{tab:eff}. Note all efficiency experiments were run on a single 3090 GPU Server. 
\begin{table}[h]
    \renewcommand\arraystretch{1.2}
    \footnotesize
	\setlength{\tabcolsep}{0.8mm}{
	\begin{tabular*}{8.4cm}{@{\extracolsep{\fill}}lcccc}
		\hline
		\multirow{1}{*}{} &\# Sensors&STEP&\textbf{STD-MAE}&Imp. \\
\hline

PEMS03&358&1242.43&727.51&70.7\%\\
		\hline
PEMS04&307&445.59&416.59&6.9\%\\
		\hline
PEMS07&883&8200.88&858.13&855.6\%\\
		\hline
PEMS08&170&431.66&589.65&-26.7\%\\
		\hline
	\end{tabular*}}
\captionsetup{justification=centering}
		\caption{Efficiency Test with Pre-training Model}
		\label{tab:eff}
\end{table}

\subsection{Case Study}
We further conduct case studies to verify the decoupled design of STD-MAE pre-training.

\noindent\textbf{Reconstruction Accuracy in Pre-training.} STD-MAE exhibits a robust capacity for reconstructing long time series of temporally masked data by relying solely on the partially observed sensor recordings, as shown in Figures~\ref{case1-subfig1} and \ref{case1-subfig2}. The shaded portion indicates the masked region. Visualization of the results reveals that the model recovers temporally masked regions with high fidelity, closely matching the ground truth data in terms of periodicity and trends. This suggests STD-MAE successfully acquires generalized knowledge of temporal patterns during pre-training. Similarly, STD-MAE capably restores entirely masked sensors based on contextual data from spatially related sensors, indicating it also learns meaningful spatial patterns. Overall, the high reconstruction accuracy demonstrates STD-MAE's effectiveness in modeling spatio-temporal heterogeneity.
\begin{figure}[h]
  \captionsetup{justification=centering}
  \centering
  
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{figure/Temporal_Rec.png}
\caption{Reconstruction from T-MAE Pre-training}
    \label{case1-subfig1}
  \end{subfigure}
  
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{figure/Spatial_Rec.png}
\caption{Reconstruction from S-MAE Pre-training}

    \label{case1-subfig2}
  \end{subfigure}
  
  \captionsetup{justification=centering}
  \caption{Reconstruction Accuracy from Pre-training}
\label{fig:case1}
\end{figure}



\noindent\textbf{Attention Mechanism.} Figure~\ref{fig:case2} illustrates the spatial and temporal attention maps learned by STD-MAE on the PEMS03 dataset. Figure~\ref{case2-subfig1} shows the first 50 sensors out of 358 total sensors on the PEMS03 dataset, while Figure~\ref{case2-subfig2} presents the temporal attention over 96 timesteps. These visualizations offer insight into the distinct spatio-temporal correlations captured. For temporal modeling, STD-MAE pays attention to both nearby timesteps and more distant context, integrating local and longer-range dependencies. In contrast, spatial attention focuses on the most important few sensors within the same time period. The divergence of temporal versus spatial attention mechanisms provides justification for STD-MAE's decoupled design, which processes these two dimensions separately. \begin{figure}[h]
  \captionsetup{justification=centering}
  \centering
  \begin{subfigure}{0.22\textwidth}
    \includegraphics[width=\linewidth]{figure/Spatial_attn_Enco.png}
    \caption{Spatial Attention Map}
    \label{case2-subfig1}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\linewidth]{figure/Temporal_attn.png}
    \caption{Temporal Attention Map}
    \label{case2-subfig2}
  \end{subfigure}
  \captionsetup{justification=centering}
  \caption{Attention Map Visualization on PEMS03}
  \label{fig:case2}
\end{figure}

\noindent\textbf{Positional Encoding.} 
Figure~\ref{fig:pos} shows the positional encoding of sensor 0 in the PEMS03 dataset with a time length of 168 steps and a hidden layer dimension of 96. The first 48 dimensions of the hidden layer encode the temporal position according to Equation~\ref{eq:pos}. The lower half of Figure~\ref{fig:pos} exhibits different delta function values with time steps. The last 48 dimensions of the hidden layer encode the spatial position. For sensor 0, the even spatial dimensions have a value of 0 and the odd ones hold a value of 1, which is the same as the upper half of Figure~\ref{fig:pos}. This structured encoding explicitly distinguishes between spatial and temporal dimensions of location information, thus allowing for generalized spatio-temporal modeling. In summary, the visualization shows how positional encoding enables STD-MAE to incorporate positional information.
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{./figure/pos.png}
	\captionsetup{justification=centering}
	\caption{Visualization of S-T Positional Encoding}
	\label{fig:pos}
\end{figure}

\section{Conclusion}\label{sec:conclusion}
In this study, we propose STD-MAE, a novel spatio-temporal-decoupled masked pre-training framework for traffic forecasting. In the pre-training phase, two decoupled masked autoencoders are applied on spatial and temporal axes, enabling effective modeling of spatio-temporal heterogeneity in traffic data. For prediction, the hidden representations generated by the encoders are leveraged to boost the performance of downstream spatio-temporal predictors. Extensive experiments and analyses on four traffic benchmark datasets show the superiority of the STD-MAE framework.




\bibliography{aaai24}
\clearpage
\end{document}
