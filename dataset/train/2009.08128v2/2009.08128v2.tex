

\documentclass[11pt,a4paper]{article}
\usepackage{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{boldline}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{kotex}
\usepackage{footmisc}
\DefineFNsymbols{mySymbols}{{\ensuremath\dagger}{\ensuremath\ddagger}\S\P
   *{**}{\ensuremath{\dagger\dagger}}{\ensuremath{\ddagger\ddagger}}}
\setfnsymbol{mySymbols}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{Z}{>{\arraybackslash}X}
\graphicspath{ {./figures/} }
\hbadness=99999
\vbadness=99999

\renewcommand{\UrlFont}{\ttfamily\small}
\newcommand{\ins}[1]{\textcolor{blue}{#1}}
\newcommand{\dis}[1]{\textcolor{red}{#1}}


\usepackage{microtype}

\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{MultiOIE: Multilingual Open Information Extraction Based on Multi-Head Attention with BERT}
\author
{
  Youngbin Ro \quad Yukyung Lee \quad Pilsung Kang\thanks{\; Corresponding author} \\
  Korea University, Seoul, Republic of Korea \\
  \texttt{\{youngbin\_ro, yukyung\_lee, pilsung\_kang\}@korea.ac.kr}
}

\begin{document}
\maketitle
\begin{abstract}
In this paper, we propose MultiOIE, which performs open information extraction (open IE) by combining BERT \citep{devlin-etal-2019-bert} with multi-head attention blocks \citep{10.5555/3295222.3295349}.
Our model is a sequence-labeling system with an efficient and effective argument extraction method.
We use a query, key, and value setting inspired by the Multimodal Transformer \citep{tsai-etal-2019-multimodal} to replace the previously used bidirectional long short-term memory architecture with multi-head attention.
MultiOIE outperforms existing sequence-labeling systems with high computational efficiency on two benchmark evaluation datasets, Re-OIE2016 and CaRB.
Additionally, we apply the proposed method to multilingual open IE using multilingual BERT.
Experimental results on new benchmark datasets introduced for two languages (Spanish and Portuguese) demonstrate that our model outperforms other multilingual systems without training data for the target languages.
\end{abstract}

\section{Introduction}
Open information extraction (Open IE) \citep{10.5555/1625275.1625705} aims to extract a set of arguments and their corresponding relationship phrases from natural language text.
For example, an open IE system could derive the relational tuple (\emph{was elected}; \emph{The Republican candidate}; \emph{President}) from the given sentence ``\emph{The Republican candidate was elected President.}''
Because the extractions generated by open IE are considered as useful intermediate representations of the source text \citep{10.5555/3061053.3061220}, this method has been applied to various downstream tasks \citep{christensen-etal-2013-towards,ding-etal-2016-knowledge,khot-etal-2017-answering,10.1145/3269206.3271707}.

\begin{figure}[t]
\includegraphics[width=\columnwidth]{figure1.pdf}
\caption
{
Comparison between existing extractors and the proposed method.
We use BERT for feature embedding layers and as a predicate extractor.
Predicate information is reflected through multi-head attention instead of simple concatenation.
}
\label{fig:1}
\centering
\end{figure}

Although early open IE systems were largely based on handcrafted features or fine-grained rules \citep{fader-etal-2011-identifying,mausam-etal-2012-open,10.1145/2488388.2488420}, most recent open IE research has focused on deep-neural-network-based supervised learning models.
Such systems are typically based on bidirectional long short-term memory (BiLSTM) and are formulated for two categories: sequence labeling \citep{stanovsky-etal-2018-supervised,Sarhan2019ContextualizedWE,jia2019hybrid} and sequence generation \citep{cui-etal-2018-neural,10.1145/3159652.3159712,bhutani-etal-2019-open}.
The latter enables flexible extraction; however, it is more computationally expensive than the former.
Additionally, generation methods are not suitable for non-English text owing to a lack of training data because they are heavily dependent on in-language supervision \citep{ponti-etal-2019-towards}.
Therefore, we adopted the sequence labeling method to maximize scalability by using (multilingual) BERT \citep{devlin-etal-2019-bert} and multi-head attention \citep{10.5555/3295222.3295349}.
The main advantages of our approach can be summarized as follows:
\begin{itemize}[leftmargin=0.4cm]
\setlength{\itemindent}{0em}
\item
Our model \textbf{\textit{can consider rich semantic and contextual relationships between a predicate and other individual tokens in the same text during sequence labeling by adopting a \underline{multi}-head attention structure}}. Specifically, we apply multi-head attention with the final hidden states from BERT as a query and the hidden states of predicate positions as key-value pairs. This method repeatedly reinforces sentence features by learning attention weights across the predicate and each token \citep{tsai-etal-2019-multimodal}.
Figure \ref{fig:1} presents the difference between the existing sequence labeling methods and the proposed method.
\item
MultiOIE \textbf{\textit{can operate on \underline{multi}lingual text without non-English training datasets}} by using BERT's multilingual version.
By contrast, for sequence generation systems, performing zero-shot multilingual extraction is much more difficult \citep{ronnqvist-etal-2019-multilingual}.
\item
Our model is more \textbf{\textit{computationally efficient}} than sequence generation systems.
This is because the autoregressive properties of sequence generation create a bottleneck for real-world systems.
This is an important issue for downstream tasks that require processing of large corpora.
\end{itemize}

Experimental results on two English benchmark datasets called Re-OIE2016 \citep{Zhan2019SpanMF} and CaRB \citep{bhardwaj-etal-2019-carb} show that our model yields the best performance among the available sequence-labeling systems.
Additionally, it is demonstrated that the computational efficiency of MultiOIE is far greater than that of sequence generation systems.
For a multilingual experiment, we introduce multilingual open IE benchmarks (Spanish and Portuguese) constructed by translating and re-annotating the Re-OIE2016 dataset.
Experimental results demonstrate that the proposed MultiOIE outperforms other multilingual systems without additional training data for non-English languages.
To the best of our knowledge, ours is the first approach using BERT for multilingual open IE\footnote{Although CrossOIE \citep{Cabral2020CrossOIE} considered multilingual BERT in the system, it was not used when extracting the tuples but used only when validating the extracted results.}.
The code and related resources can be found in \url{https://github.com/youngbin-ro/Multi2OIE}.

\section{Background}
\subsection{Multi-Head Attention for Open IE}
In sequence labeling open IE systems, when extracting arguments for a specific predicate, predicate-related features are used as input variables \citep{stanovsky-etal-2018-supervised,Zhan2019SpanMF,jia2019hybrid}.
We analyzed this extraction process from the perspective of multimodal learning \citep{Mangai2010ASO,10.5555/3104482.3104569,10.1109/TPAMI.2018.2798607}, which defines an entire sequence and the corresponding predicate information as a modality.
The most frequently used method for open IE is simple concatenation (Figure \ref{fig:1}, left), which can be interpreted as an early fusion approach.
Simple concatenation has low computational complexity, but requires intensive feature engineering.
It is also highly reliant on the choice of a classifier \citep{doi:10.1142/S1793351X16400158,Liu2018LearnTC}.

Instead, we propose the use of a multi-modality mechanism \citep{tsai-etal-2019-multimodal} to capture the complicated relationships between predicates and other tokens. In our method, multi-head attention is computed by using target modality as a query with source modalities as key-value pairs to adapt the latent information from sources to targets.
This allows our model to assign greater weights to meaningful interactions between modalities.
Accordingly, MultiOIE uses multi-head attention to reflect predicate information (source modality) throughout a sequence (target modality).
We expect this module to transform a general sentence embedding into a suitable feature for extracting the arguments associated with a specific predicate.

\subsection{Multilingual Open IE}
\label{sec:2.2}
Despite the increasing amount of available web text in languages other than English, most open IE approaches have focused on the English language.
For non-English languages, most systems are heavily reliant on handcrafted features and rules, resulting in limited performance \citep{zhila-gelbukh-2014-open,Oliveira2019DptOIEAP,8903488,GUARASCI2020112954}.
Although some studies have demonstrated the potential of multilingual open IE \citep{faruqui-kumar-2015-multilingual,10.1007/978-3-319-23485-4_72,white-EtAl:2016:EMNLP2016}, most approaches are based on shallow patterns, resulting in low precision \citep{Claro_2019}.

Therefore, we introduce a multilingual-BERT-based open IE system.
BERT provides language-agnostic embedding through its multilingual version and provides excellent zero-shot performance on many classification and labeling tasks \citep{pires-etal-2019-multilingual,wu-dredze-2019-beto,Karthikeyan2020CrossLingualAO}.
In Section \ref{sec:5}, we demonstrate that our multilingual system yields acceptable performance when it is trained using only an English dataset.

\begin{figure*}[ht]
\includegraphics[width=\textwidth]{figure2.pdf}
\caption
{
Architecture of MultiOIE.
After predicates are extracted using the hidden states of BERT, the hidden sequence, average vector of predicates, and position embedding are concatenated and used as inputs for multi-head attention blocks for argument extraction.
}
\label{fig:2}
\centering
\end{figure*}

\section{Proposed Method}
MultiOIE extracts relational tuples from a given sentence in two steps. 
The first step is to find all predicates in the sentence.
The second step is to extract the arguments associated with each identified predicate.
The architecture of the proposed model is presented in Figure \ref{fig:2}.

\subsection{Task Formulation}
\label{sec:task_formulation}
Let  be an input sentence, where  is the -th token and  is the sequence length.
The objective of the proposed model  is to find a set of tags , where each element of  indicates one of the “beginning, inside, outside” (BIO) tags \citep{ramshaw-marcus-1995-text}.
However, unlike the method proposed in \citet{stanovsky-etal-2018-supervised}, which uses a predicate head as an input and predicts all tags simultaneously, we first predict a predicate tagset  using a predicate model .
An argument tagset  is predicted using  based on  and .
Therefore, our model maximizes the following log-likelihood formulation:

where  and  are the trainable parameters of  and , respectively.
In this formulation,  contributes to extracting not only the predicates, but also the arguments.
The loss and gradients derived from argument extraction are also propagated to  and .

Additionally, we treat open IE as an -ary extraction task and consider BIO tags for arguments up to ARG3. We refer readers to \citet{stanovsky-etal-2018-supervised} for a more detailed explanation of the BIO sequence labeling policy.

\subsection{Predicate Extraction}
\label{sec:predicate_extraction}
We assume that a given sentence  is tokenized by SentencePiece \citep{kudo-richardson-2018-sentencepiece}.
BERT embeds and encodes  through multiple layers. The final hidden states are defined as , where  is the hidden state size of BERT.
 is then fed into a feed-forward network and a softmax layer to calculate the probability that each token is classified into each predicate tag.
The predicted tagset  is obtained by applying the argmax operation to the softmax outputs.
Finally, the loss for predicate extraction, denoted , is calculated as per-token cross-entropy loss.

\subsection{Argument Extraction}
\label{sec:argument_extraction}
A sentence contains one or more predicates.
The argument extraction method described in this section targets only one predicate. The process is simply repeated for multiple predicates.

\paragraph{Input representation}
The inputs for argument extraction are concatenations of the following three features: , , and .
The first feature is the same as the last hidden state of BERT, as discussed in Section \ref{sec:predicate_extraction}.
The second feature is the arithmetic mean vector of hidden states at predicate positions.
We duplicate this vector to match the sequence length  and define it as .
We refer to the true tagset  to find the indices of predicates instead of using the predicted tagset  to achieve more stable training \citep{6795228}.
The final feature  is a position embedding of binary values that indicates whether each token is included in the predicate span.
We then concatenate these three features to obtain the input , where  is the dimension of multi-head attention and  is the dimension of the position embedding .

Following concatenation,  is divided into a query and key-value pairs.
We use  itself as a query, denoted as  (target sequence).
Key-value pairs, denoted as  and  (source sequence), are subsets of  derived from predicate positions.

\begin{figure}[t]
\centering
\includegraphics[scale=0.45]{figure3.pdf}
\caption
{
Multi-head attention blocks for argument extraction.
The architecture consists of  blocks and the output of final block  is used as the input for the argument classifier.
}
\label{fig:3}
\end{figure}

\paragraph{Multi-head attention block}
The argument extractor consists of  multi-head attention blocks, each of which has a multi-head attention layer followed by a position-wise feed-forward layer, as shown in Figure \ref{fig:3}.

The attention layer is the same as the encoder-decoder attention layer in the original transformer \citep{10.5555/3295222.3295349}.
It first transforms , , and  into , , and , respectively, where , , and  are weight matrices with dimensions of ().
Following transformation, the computation of attention is performed for each head as follows:

Each head is indexed by  and has dimensions of , where  denotes the number of heads.
The attention outputs for each head are then concatenated and linearly transformed.
In addition, we apply residual connections \citep{He2016DeepRL} and layer normalization \citep{Ba2016LayerN} based on the results of prior works on transformers.

The position-wise feed-forward layer consists of two linear transformations surrounding a ReLU activation function.
Residual connections and layer normalization are also applied in this layer.
Finally, the output of the final multi-head attention block is fed into the argument classifier.
The process for obtaining a predicted argument tagset  and corresponding argument loss  is the same as that described in Section \ref{sec:predicate_extraction}.
The final loss for parameter updating is the summation of  and .

\subsection{Confidence Score}
\label{sec:confidence_score}
In open IE, confidence scores can help control the precision-recall tradeoff of a system.
MultiOIE provides a confidence score for every extraction by adding the predicate score and all argument scores, as suggested in \citet{Zhan2019SpanMF}.
The score of the predicate and each argument is obtained from the probability value of the \emph{Beginning} tag.

where the probability values are given by the softmax layer in each extraction step.

\section{Experiments}
\subsection{Experimental Setup}
\label{experimental_setup}
\paragraph{Datasets}
For fair comparisons with other systems, we trained our model using the same dataset used by \citet{Zhan2019SpanMF} \footnote{\url{https://github.com/zhanjunlang/Span_OIE}}.
This dataset was bootstrapped from extractions of the OpenIE4 \citep{10.5555/3061053.3061220}.
For testing data, we used the Re-OIE2016 \citep{Zhan2019SpanMF} and CaRB \citep{bhardwaj-etal-2019-carb}, which were generated via human annotation based on the sentences in the OIE2016 \citep{stanovsky-dagan-2016-creating} dataset.
Table \ref{tab:1} lists the details of the datasets used in this study.

\begin{table}[t]
\centering
\begin{tabular*}{\columnwidth}{cccc} 
\hlineB{3}
\textbf{Split}         & \textbf{Dataset} & \textbf{\# Sents.} & \textbf{\# Tuples} \\ \hlineB{2}
Train                  & OpenIE4          & 1,109,411          & 2,175,294          \\ \hline
\multirow{2}{*}{Dev}   & OIE2016-dev      & 582                & 1,671              \\
                       & CaRB-dev         & 641                & 2,548              \\ \hline
\multirow{2}{*}{Test}  & Re-OIE2016       & 595                & 1,508              \\
                       & CaRB-test        & 641                & 2,715              \\
\hlineB{3}
\end{tabular*}
\caption
{
Numbers of sentences and tuples in each dataset used in this study.
}
\label{tab:1}
\end{table}

\paragraph{Evaluation metrics}
We evaluated each system using the \emph{area under the curve} (AUC) and \emph{F1-score} (F1).
AUC is calculated from a plot of the precision and recall values for all potential cutoffs.
The F1-score is the maximum value among the precision-recall pairs.
We used the evaluation code provided with each test data, which contains the following matching functions: \emph{lexical match}\footnote{\url{https://github.com/gabrielStanovsky/oie-benchmark}} for Re-OIE2016, and \emph{tuple match}\footnote{\url{https://github.com/dair-iitd/CaRB}} for CaRB.
Although the former only considers the existence of words within extractions, the latter is stricter in that it penalizes long extractions \citep{bhardwaj-etal-2019-carb}.

\paragraph{Hyperparameters}
Model hyperparameters were tuned by performing a grid search.
We first trained the model for one epoch with an initial learning rate of 3e-5.
The model contains four multi-head attention blocks with eight attention heads and a 64-dimensional position-embedding layer.
The batch size was set to 128.
The dropout rates for the argument classifier and attention blocks were set to 0.2, respectively.
AdamW \citep{Loshchilov2019DecoupledWD} was used as an optimizer in combination with training heuristics, such as learning rate warmup \citep{Goyal2017AccurateLM} and gradient clipping \citep{10.5555/3042817.3043083}.

\subsection{Baselines}
As baseline models, we selected RnnOIE \citep{stanovsky-etal-2018-supervised}, SpanOIE \citep{Zhan2019SpanMF}, and a few custom systems to evaluate the validity of the multi-head attention blocks (MH).
Although these are all sequence-labeling systems, note that SpanOIE uses the span selection method rather than BIO tagging.
Table \ref{tab:2} presents a summary of the main baselines used in this study.
We also report the results of the following systems developed prior to the use of neural networks: Stanford \citep{angeli-etal-2015-leveraging}, O{\sc llie} \citep{mausam-etal-2012-open}, P{\sc rop}S \citep{Stanovsky2016GettingMO}, ClausIE \citep{10.1145/2488388.2488420}, and OpenIE4.
For these systems, the results were from previous studies \citep{Zhan2019SpanMF,bhardwaj-etal-2019-carb}.

\begin{table}[t]
\centering
\begin{tabular*}{\columnwidth}{lccc} 
\hlineB{3}
                              & \small Method         &     &           \\ \hlineB{2}
\small BIO                    & \small BIO tagging    & \small BiLSTM & \small BiLSTM      \\
\small BIO+MH                 & \small BIO tagging    & \small BiLSTM & \small MH          \\ \hline
\small SpanOIE                & \small Span selection & \small BiLSTM & \small BiLSTM      \\ 
\small SpanOIE+MH             & \small Span selection & \small BiLSTM & \small MH          \\ \hline
\small BERT+BiLSTM            & \small BIO tagging    & \small BERT   & \small BiLSTM      \\ 
\textbf{\small MultiOIE} & \small BIO tagging    & \small BERT   & \small MH          \\ \hlineB{3}
\end{tabular*}
\caption
{
Baseline models with difference settings.
}
\label{tab:2}
\end{table}

\begin{figure*}[ht]
\centering
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/figure4a.png}
\caption{Re-OIE2016}
\label{fig:4(a)}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/figure4b.png}
\caption{CaRB}
\label{fig:4(b)}
\end{subfigure}
\caption
{
Precision-recall curves for each open IE system on two testing datasets. 
}
\label{fig:4}
\end{figure*}

\begin{table*}[ht]
\centering
\begin{tabularx}{0.95\textwidth}{l|YYYY|YYYY}
\hlineB{3}
\multicolumn{1}{c|}{} & \multicolumn{4}{c|}{\textbf{Re-OIE2016}}
                      & \multicolumn{4}{c}{\textbf{CaRB}}                \\ \hline
                      & \multicolumn{1}{c}{\textbf{AUC}}
                      & \multicolumn{1}{c}{\textbf{F1}}
                      & \multicolumn{1}{c}{\small PREC.}
                      & \multicolumn{1}{c|}{\small REC.}
                      & \multicolumn{1}{c}{\textbf{AUC}}
                      & \multicolumn{1}{c}{\textbf{F1}}
                      & \multicolumn{1}{c}{\small PREC.}
                      & \multicolumn{1}{c}{\small REC.}                       \\ \hlineB{2}
Stanford              & 11.5 & 16.7 & - & -    
                      & 13.4 & 23.0 & - & -                              \\
OLLIE                 & 31.3 & 49.5 & - & -     
                      & 22.4 & 41.1 & - & -                              \\
PropS                 & 43.3 & 64.2 & - & -     
                      & 12.6 & 31.9 & - & -                              \\
ClausIE               & 46.4 & 64.2 & - & -   
                      & 22.4 & 44.9 & - & -                              \\
OpenIE4               & 50.9 & 68.3 & - & - 
                      & 27.2 & 48.8 & - & -                              \\ 
RnnOIE                & 68.3 & 78.7 & 84.2 & 73.9
                      & 26.8 & 46.7 & 55.6 & 40.2          \\ \hline
BIO                   & 71.9 & 80.3 & 84.1 & 76.8 
                      & 27.7 & 46.6 & 55.1 & 40.4          \\
BIO+MH                & 71.3 & 81.5 & \textbf{87.0} & 76.6 
                      & 27.3 & 47.5 & 57.2 & 40.7          \\ \hline
SpanOIE               & 65.8 & 77.0 & 79.7 & 74.5 
                      & 30.0 & 49.4 & 60.9 & 41.6          \\
SpanOIE+MH            & 68.0 & 78.8 & 83.1 & 74.9
                      & 30.2 & 50.0 & \textbf{62.2} & 41.8 \\ \hline
BERT+BiLSTM           & 72.1 & 81.3 & 86.0 & 77.0 
                      & 30.6 & 50.6 & 61.3 & 43.1          \\ 
\textbf{MultiOIE} (ours)    & \textbf{74.6} & \textbf{83.9}
                                & 86.9   & \textbf{81.0}
                                & \textbf{32.6} & \textbf{52.3}
                                & 60.9   & \textbf{45.8}         \\
\hlineB{3}
\end{tabularx}
\caption
{
Performance of MultiOIE and baseline systems on the Re-OIE2016 and CaRB datasets.
}
\label{tab:3}
\end{table*}

\begin{table*}[ht]
\centering
\begin{tabularx}{0.95\textwidth}{c|Z}
\hlineB{3}
Sentence &  \begin{tabular}{@{}l@{}}
            \emph{At a presentation in the Toronto Pearson International Airport hangar,} \\
            \emph{Celine Dion helped the newly solvent airline debut its new image.}
            \end{tabular} \\ \hline \hline
SpanOIE  &  \begin{tabular}{@{}l@{}}
            \emph{(helped; Celine Dion; the newly solvent airline debut its new image)} \\
            \end{tabular} \\ \hline
MultiOIE  &   \begin{tabular}{@{}l@{}}
            \emph{(helped; Celine Dion; the newly solvent airline debut its new image;} \\
            \emph{\textbf{At a presentation in the Toronto Pearson International Airport hangar})} \\
            \textbf{\emph{(debut; the newly solvent airline; its new image)}} \\
            \end{tabular} \\
\hlineB{3}
\end{tabularx}
\caption
{
Extraction examples from MultiOIE and SpanOIE.
The sentences are from the CaRB testing set.
}
\label{tab:4}
\end{table*}

\subsection{Results}
\label{subsection:results}
The performance results for each system on the Re-OIE2016 and CaRB test data are presented in Table \ref{tab:3}.
The precision-recall curves are presented in Figure \ref{fig:4}.
We also present extraction examples from MultiOIE and SpanOIE in Table \ref{tab:4}.

\paragraph{Overall performance}
Our model outperforms the other systems on all datasets and metrics.
Our model yields average improvements of approximately 6.9\%p and 2.9\%p in terms of F1 for the Re-OIE2016 and CaRB datasets, respectively, compared to the state-of-the-art system (SpanOIE).

Similar to previous studies \citep{stanovsky-etal-2018-supervised,Zhan2019SpanMF}, the excellent performance of MultiOIE is attributed to improved recall.
As shown in Table \ref{tab:3}, our method achieves the highest recall rate on both datasets.
The examples in Table \ref{tab:4} also demonstrate that our model can extract more tuples from the same sentence.
An additional tuple \emph{(debut; the newly solvent airline; its new image)} is found by MultiOIE, but not by SpanOIE.
Additionally, MultiOIE extracts the place information \emph{``At a ... hangar''} for the first tuple, which is omitted by SpanOIE.

\paragraph{Effects of multi-head attention}
We compared three pairs of methods to determine the validity of multi-head attention blocks: (BIO and BIO+MH), (SpanOIE and SpanOIE+MH), and (BERT+BiLSTM and MultiOIE).
As a result, except for BIO+MH yielding a lower AUC than BIO,
the models with multi-head attention achieve higher performance than the BiLSTM-based models.
This performance improvement is consistent, regardless of the choice of classification method (BIO tagging and span selection).
These results suggest that the use of multi-head attention is superior to simple concatenation in terms of utilizing predicate information.

Additionally, the performance improvement from using MH is greater with BERT than with BiLSTM.
The average performance improvements from BIO to BIO+MH are -0.5\%p (AUC) and 1.1\%p (F1), whereas the improvements from BERT+BiLSTM to MultiOIE are 2.3\%p (AUC) and 2.2\%p (F1).
This indicates that MultiOIE has a model architecture that can create synergies between the predicate and argument extractors.

\paragraph{Computational cost}
We measured the training and inference times of each system to evaluate computational efficiency.
As an additional baseline model, we considered a recently published sequence generation system called IMoJIE \citep{kolluru2020imojie}.
It achieved state-of-the-art performance on the CaRB dataset using sequential decoding of tuples conditioned on previous extractions.
For calculating inference times, we selected 641 sentences from the CaRB testing dataset and executed the models on a single TITAN RTX GPU.

Table \ref{tab:5} reveals that MultiOIE has much greater efficiency than IMoJIE.
Our model only requires 15.5 s to process the 641 sentences, whereas IMoJIE requires more than 3 min, which is a difference of approximately 14 times.
This bottleneck of IMoJIE could be a drawback for downstream tasks, such as knowledge base construction, which must work with large amounts of text.
Considering that the performance difference between the two models is only approximately 1\%p\footnote{IMoJIE achieved (AUC, F1) of (33.3, 53.5) on the CaRB dataset.}, it may be reasonable to use MultiOIE to process large-scale corpora.
MultiOIE also exhibits competitive computational costs compared to the other sequence-labeling systems.
Our model has similar training times compared to BERT+BiLSTM, but is faster for inference.
This demonstrates that MH has a positive effect on both efficiency and performance.
In the case of SpanOIE, its span selection method creates bottlenecks for both training and inference.

\begin{table}
\centering
\begin{tabularx}{\columnwidth}{l|YYY}
\hlineB{3}
                       & Training
                       & Inference
                       & Sec./Sent.              \\ \hlineB{2}
\small BERT+BiLSTM     & \textbf{4.5h}
                       & 21.5s
                       & 0.03s                   \\
\small SpanOIE         & 10.2h & 33.8s  & 0.05s  \\
\small IMoJIE          & 7.7h  & 212.2s & 0.33s  \\ \hline
\small \textbf{MultiOIE} & 4.6h         
                       & \textbf{15.5s}
                       & \textbf{0.02s}          \\ \hlineB{3}
\end{tabularx}
\caption
{
Training and inference times of each system.
}
\label{tab:5}
\end{table}

\section{Multilingual Performance}
\label{sec:5}
As mentioned in Section \ref{sec:2.2}, we trained a multilingual version of MultiOIE using multilingual BERT and the same training dataset as the English version.
We assumed that data for non-English languages were not available and tested the model's zero-shot performance.
Evaluations were conducted using a dataset generated based on the Re-OIE2016 dataset.

\begin{table}[t]
\centering
\begin{tabularx}{\columnwidth}{l|YYYY}
\hlineB{3}
                  & AUC  & F1   & PREC. & REC. \\ \hlineB{2}
\small EN version & 32.6 & 52.3 & 60.9  & 45.8 \\
\small MT version & 31.5 & 51.9 & 59.5  & 45.9 \\
\hlineB{3}
\end{tabularx}
\caption
{
Comparison between English (EN) and Multilingual (MT) versions of our model on CaRB dataset.
}
\label{tab:6}
\end{table}

\subsection{Experimental setup}
\paragraph{Datasets}
Considering the availability of baseline systems, we selected Spanish and Portuguese as the evaluation dataset languages.
First, all sentences, predicates, and arguments from the Re-OIE2016\footnote{We chose the Re-OIE2016 because the CaRB dataset was originally created not to label sequences but to generate sequences.} dataset were translated into the target languages using Google\footnote{\url{https://cloud.google.com/translate/}}.
To prevent adverse effects from translation errors, we modified the translated sentences to make sure that the back-translated sentences have the same meaning with the original sentence.
After the translation and modification, we manually re-annotated all tuples of the target languages based on the English annotation of Re-OIE2016.

\paragraph{Evaluation metrics}
Because the baseline systems are binary extractors and do not provide confidence scores, we report binary extraction performance without AUC values.
Additionally, although the introduced dataset was generated based on the Re-OIE2016, each system was tested using CaRB's evaluation code for more rigorous evaluation.

\begin{table*}[ht]
\centering
\begin{tabularx}{0.95\textwidth}{c|Z}
\hlineB{3}
Sentence    &  \begin{tabular}{@{}l@{}}
               \small \emph{When the explosion tore through the hut,} \\
               \small \emph{Stauffenberg was convinced that no one in the room could have survived.}
               \end{tabular} \\ \hline \hline
English     &  \begin{tabular}{@{}l@{}}
               \small \emph{(tore; the explosion; through the hut)} \\
               \small \emph{(was convinced; Stauffenberg; that no one in the room could have survived)} \\
               \small \emph{(could have survived; no one in the room)}
               \end{tabular} \\ \hline
Spanish     &  \begin{tabular}{@{}l@{}}
               \small \emph{(desgarró; la explosión; a través de la cabaña)} \\
               \small \emph{(estaba convencido; Stauffenberg; de que nadie en la habitación podría haber sobrevivido)} \\
               \small \emph{(podría haber sobrevivido; nadie en la habitación)}
               \end{tabular} \\ \hline
Portuguese  &  \begin{tabular}{@{}l@{}}
               \small \emph{(rasgou; a explosão; através da cabana)} \\
               \small \emph{(estava convencido; Stauffenberg; de que ninguém na sala poderia ter sobrevivido)} \\
               \small \emph{(poderia ter sobrevivido; ninguém na sala)}
               \end{tabular} \\
\hlineB{3}
\end{tabularx}
\caption
{
Extraction examples from MultiOIE for each language.
}
\label{tab:7}
\end{table*}

\begin{table}[ht]
\centering
\begin{tabularx}{\columnwidth}{c|l|YYY}
\hlineB{3}
Lang.               & \multicolumn{1}{c|}{System}
                    & F1 & PREC. & REC. \\ \hlineB{2}
\multirow{3}{*}{EN} & \small ArgOE          
                    & 43.4 & 56.6 & 35.2                            \\
                    & \small PredPatt  
                    & 53.1 & 53.9 & 52.3                            \\
                    & \small \textbf{MultiOIE} 
                    & \textbf{69.3} & \textbf{66.9} & \textbf{71.7} \\ \hline
\multirow{3}{*}{ES} & \small ArgOE  
                    & 39.4 & 48.0 & 33.4                            \\
                    & \small PredPatt  
                    & 44.3 & 44.8 & 43.8                            \\
                    & \small \textbf{MultiOIE} 
                    & \textbf{60.2} & \textbf{59.1} & \textbf{61.2} \\ \hline
\multirow{3}{*}{PT} & \small ArgOE    
                    & 38.3 & 46.3 & 32.7                   \\
                    & \small PredPatt  
                    & 42.9 & 43.6 & 42.3                            \\
                    & \small \textbf{MultiOIE} 
                    & \textbf{59.1} & \textbf{56.1} & \textbf{62.5}          \\
\hlineB{3}
\end{tabularx}
\caption
{
Binary extraction performance without confidence scores on the multilingual Re-OIE2016 dataset.
}
\label{tab:8}
\end{table}


\paragraph{Baselines}
Our baseline models were two rule-based multilingual systems: ArgOE \citep{10.1007/978-3-319-23485-4_72} and PredPatt \citep{white-EtAl:2016:EMNLP2016}.
The former takes dependency parses in the CoNLL-X format as inputs.
Similarly, the latter uses language-agnostic patterns of UD structures\footnote{\url{https://universaldependencies.org/}}.

\subsection{Results}
\paragraph{Comparison to the English model}
Prior to comparing the multilingual systems, we evaluated whether MultiOIE's multilingual version exhibited a satisfactory performance for English compared to the English-only version.
Table \ref{tab:6} lists the performance metrics for the English and multilingual versions of our model on the CaRB dataset.
The performance of the English version was copied from Table \ref{tab:3}.
Although the multilingual version yields lower performance for both metrics compared to the English version, the F1 score is comparable and the recall is higher.
Furthermore, the multilingual version still outperforms the other sequence-labeling systems, indicating that multilingual BERT can successfully construct a MultiOIE model with favorable performance.

\paragraph{Multilingual performance}
Table \ref{tab:8} lists the performance metrics for each system for the multilingual dataset.
Table \ref{tab:7} contains an example of MultiOIE's extraction results for each language.
One can see that MultiOIE outperforms the other systems on all languages.
Similar to the results in Section \ref{subsection:results}, the superiority of our multilingual model is attributed to its high recall.
MultiOIE yields the highest recall for all languages by approximately 20\%p.
In contrast, ArgOE has relatively high precision, but low recall negatively impacts its F1 score.
PredPatt provides the best balance of precision and recall, but the overall performance is lower than that of our model.

The performance differences between languages are similar for all models.
All models exhibit the best performance for English, followed by Spanish and Portuguese.
MultiOIE also exhibits performance degradation for non-English languages.
However, considering that our model was never trained to perform open IE tasks on Spanish or Portuguese, its performance is remarkable.
For some non-English sentences, our model extracts the same results as those extracted in the English extraction result, as shown in Table \ref{tab:7}.
This result agrees with the results of previous studies \citep{pires-etal-2019-multilingual,wu-dredze-2019-beto,Karthikeyan2020CrossLingualAO}, which have demonstrated the excellent cross-lingual abilities of multilingual BERT.
Based on these results, we expect that MultiOIE will also work well on languages other than those considered in this study.

\section{Conclusion}
In this paper, we propose MultiOIE, which exploits BERT and multi-head attention for the open IE task.
Multi-head attention has the advantage of fusing sentence and predicate features, which adequately reflect predicate information throughout a sentence.
Our model achieved the best performance among sequence labeling models.
MultiOIE also exhibited superior computational efficiency with competitive performance compared to the state-of-the-art sequence generation systems.
Additionally, a MultiOIE model trained using multilingual BERT, outperformed the baseline models without training on any non-English languages.

However, some types of extractions, such as nominal relations, conjunctions in arguments, and contextual information, are not considered in MultiOIE.
Future work could investigate how to apply MultiOIE to these cases.
For multilingual open IE, performance evaluations and further study on non-alphabetic languages that were not considered in this study can be conducted.

\bibliography{emnlp2020}
\bibliographystyle{acl_natbib}

\end{document}