\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{threeparttable}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{float}
\usepackage{enumitem}
\definecolor{r1}{RGB}{48,182,86}
\definecolor{r2}{RGB}{100,209,138}
\definecolor{r3}{RGB}{168,233,191}
\definecolor{r4}{RGB}{204,245,208}
\definecolor{r5}{RGB}{235,254,236}
\definecolor{sota}{RGB}{215,215,215}
\definecolor{sota_text}{RGB}{185,185,185}
\newcommand{\method}{{BioT5}}
\newcommand{\TODO}[1]{\textcolor{orange}{#1}}
\newcommand{\bom}{bom}
\newcommand{\eom}{eom}
\newcommand{\bop}{bom}
\newcommand{\eop}{eom}
\newcommand{\dataset}{\texttt{Dataset}}
\newcommand{\selfies}{\texttt{SELFIES}}
\newcommand{\fasta}{\texttt{FASTA}}
\newcommand{\text}{\texttt{Text Description}}


\title{BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations}



\setlength\titlebox{7cm}
\author{
    Qizhi Pei\textsuperscript{1,5}, 
    Wei Zhang\textsuperscript{2}, 
    Jinhua Zhu\textsuperscript{2}, 
    Kehan Wu\textsuperscript{2},
    Kaiyuan Gao\textsuperscript{3}, \\
    {\bf Lijun Wu\textsuperscript{4}},
    {\bf Yingce Xia\textsuperscript{4}},
    {\bf Rui Yan\textsuperscript{1,6}\thanks{\ \ Corresponding authors: Lijun Wu (\url{lijuwu@microsoft.com}), Yingce Xia (\url{yinxia@microsoft.com}), and Rui Yan (\url{ruiyan@ruc.edu.cn})}} \\
    \textsuperscript{1}Gaoling School of Artificial Intelligence, Renmin University of China \\
    \textsuperscript{2}University of Science and Technology of China \\
    \textsuperscript{3}Huazhong University of Science and Technology \quad
    \textsuperscript{4}Microsoft Research\\
    \textsuperscript{5}Engineering Research Center of Next-Generation Intelligent Search\\ and Recommendation, Ministry of Education \\
    \textsuperscript{6}Beijing Key Laboratory of Big Data Management and Analysis Methods \\
    \texttt{\{qizhipei,ruiyan\}@ruc.edu.cn} \\
    \texttt{\{weizhang\_cs,teslazhu,wu\_2018\}@mail.ustc.edu.cn} \\
    \texttt{im\_kai@hust.edu.cn} \quad
    \texttt{\{lijuwu,yinxia\}@microsoft.com} 
}


\begin{document}
\maketitle
\begin{abstract}
Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose BioT5, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. BioT5 utilizes SELFIES for 100\% robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, BioT5 distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability of capturing underlying relations and properties of bio-entities. Our code is available at \url{https://github.com/QizhiPei/BioT5}.
\end{abstract}


\section{Introduction}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{case_basic.pdf}
    \caption{Representations of molecule and protein. Molecule can be represented by its name, bio-sequence (SMILES and SELFIES), and 2D graph structure. Protein can be represented by its name, corresponding gene name, bio-sequence (FASTA), and 3D structure.}
    \label{fig:molecule_protein_rep}
\end{figure}
Molecules and proteins are two essential bio-entities in drug discovery~\citep{dara2022machine}. 
Small molecule drugs have been the cornerstone of the pharmaceutical industry for nearly a century, owing to their unique advantages such as oral availability, diverse modes of action, etc \citep{future_smallM}. Proteins serve as the foundation of life science, functioning as drug targets or crucial elements in disease pathways.
As illustrated in Figure~\ref{fig:molecule_protein_rep}, both molecules and proteins can be represented using sequences. 
A molecule can be depicted by a SMILES sequence~\citep{weininger1988smiles,weininger1989smiles}, which is derived by traversing the molecular graph through depth-first search and applying specific branching rules. 
A protein can be represented by a FASTA sequence~\citep{lipman1985rapid,pearson1988improved}, which outlines the amino acids in a protein.
The sequential formats of molecules and proteins facilitate the application of Transformer models~\citep{vaswani2017attention} and pre-training techniques~\citep{liu2019roberta,radford2019language} from natural language processing (NLP) to the biomedical field. 
Chemberta~\citep{chithrananda2020chemberta} and ESM~\citep{rives2021biological,lin2022language} apply masked language modeling to molecular SMILES and protein FASTA respectively, while MolGPT~\citep{DBLP:journals/jcisd/BagalAVP22} and ProtGPT2~\citep{ferruz2022protgpt2} leverage GPT-style models for molecular and protein generation.

Scientific literature~\citep{DBLP:conf/emnlp/BeltagyLC19,canese2013pubmed} and biological databases~\citep{kim2023pubchem,boutet2007uniprotkb} serve as knowledge repositories of molecules and proteins.
These resources detail properties, experimental results, and interactions between various bio-entities, which cannot be explicitly inferred from molecular or protein sequences alone.
Consequently, a recent trend involves jointly modeling text along with molecules and proteins, allowing the textual descriptions to enhance molecular and protein representations. 
MolT5~\citep{DBLP:conf/emnlp/EdwardsLRHCJ22} adopts the T5~\citep{raffel2020exploring} framework to molecular SMILES and biomedical literature. 
MolXPT~\citep{liu2023molxpt} and Galactica~\citep{taylor2022galactica} are GPT models trained on text and bio-entities, such as SMILES and FASTA sequences.
DeepEIK~\citep{luo2023empowering} fuses the encoded features from multi-modal inputs using attention~\citep{vaswani2017attention} mechanism.
Despite their success, there is still significant room for improvement: (i) Prior work often relies on SMILES to represent molecules. However, addressing the issue of generating invalid SMILES remains a challenge to overcome~\citep{DBLP:conf/emnlp/EdwardsLRHCJ22,li2023empowering}.
(ii) The contextual information surrounding molecular or protein names could offer valuable insights for understanding the interactions and properties of bio-entities. Developing effective methods to leverage this information merits further attention. 
(iii) Existing research tends to treat structured data (e.g., molecule-text pairs from databases) and unstructured data (e.g., text sequences in literature) equally. However, structured data could be utilized more effectively to further enhance overall performance.

To address the above challenges, in this paper, we introduce \textbf{\method}, a comprehensive pre-training framework encompassing text, molecules, and proteins. \method{} leverages SELFIES \cite{krenn2020self}  to represent small molecules since its advantage over SMILES is that SELFIES offers a more robust and error-tolerant molecular representation, eliminating issues of illegitimate structures often encountered with SMILES. There are mainly two steps for \method{} pre-training:

(1) {\em Data collection \& processing}: We gather text, molecule, and protein data, as well as existing databases containing molecule-text parallel data and protein-text parallel data. For the text data (PubMed) from the biological domain, we employ named entity recognition and entity linking to extract molecular and protein mentions, replacing them with the corresponding SELFIES or FASTA sequences. Following~\citet{liu2023molxpt}, we refer to such data as ``wrapped'' text. Text tokens, FASTA sequences, and SELFIES are tokenized independently (see Section~\ref{sec:sep_token_emb} for more details).

(2) {\em Model training}: \method{} utilizes a shared encoder and a shared decoder to process various modalities. The standard T5 employs the ``recover masked spans'' objective, wherein each masked span and its corresponding part share the same sentinel token. We refer to the aforementioned training objective function as the ``T5 objective'' for simplicity. There are three types of pre-training tasks: (i) Applying the standard T5 objective to molecule SELFIES, protein FASTA, and general text independently, ensuring that the model possesses capabilities in each modality. (ii) Applying the T5 objective to wrapped text from the biological domain, where all text, FASTA, and SELFIES tokens can be masked and recovered. (iii) For the structured molecule-text data, we introduce a translation objective. Specifically, \method{} is trained to translate molecule SELFIES to the corresponding description and vice versa. Likewise, the translation objective is applied to protein-text data. 

After pre-training, we fine-tune the obtained \method{} on  tasks covering molecule and protein property prediction, drug-target interaction prediction, protein-protein interaction prediction, molecule captioning, and text-based molecule generation. \method{} achieves state-of-the-art performances on  tasks and exhibits results comparable to domain-specific large models on  tasks, demonstrating the superior ability of our proposed method. 
\method{} model establishes a promising avenue for the integration of chemical knowledge and natural language associations to augment the current understanding of biological systems.


\section{Related Work}
In this section, we briefly review related work about cross-modal models in biology and representations of molecule and protein.

\subsection{Cross-modal Models in Biology}
\label{sec:cross_modal}
Language models in the biology field have gained considerable attention.
Among these, BioBERT~\citep{lee2020biobert} and BioGPT~\citep{luo2022biogpt}, which are pre-trained on scientific corpora, have been particularly successful in effectively understanding scientific texts.
More recently, cross-modal models focusing on jointly modeling text with bio-sequences have emerged.
They can be categorized into the following three groups.

\noindent{\textbf{Cross Text-molecule Modalities}}
MolT5~\citep{DBLP:conf/emnlp/EdwardsLRHCJ22} is a T5~\citep{raffel2020exploring}-based model, which is jointly trained on molecule SMILES and general text corpus.
MoSu~\citep{su2022molecular} is trained on molecular graphs and related textual data using contrastive learning.
MolXPT~\cite{liu2023molxpt} is a GPT~\citep{radford2018improving}-based model pre-trained on molecule SMILES, biomedical text, and wrapped text.
Different from \method{}, these models all use SMILES to represent molecules, which leads to validity issues when generating molecules.

\noindent{\textbf{Cross Text-protein Modalities}}
ProteinDT~\citep{liu2023text} is a multi-modal framework that uses semantically-related text for protein design.
BioTranslator~\citep{xu2023multilingual} is a cross-modal translation system specifically designed for annotating biological instances, such as gene expression vectors, protein networks, and protein sequences, based on user-written text. 

\noindent{\textbf{Cross Three or More Biology Modalities}}
Galactica~\citep{taylor2022galactica} is a general GPT-based large language model trained on various scientific domains, including scientific paper corpus, knowledge bases (e.g., PubChem~\citep{kim2023pubchem} molecules, UniProt~\citep{uniprot2023uniprot} protein), codes, and other sources.
DeepEIK~\cite{luo2023empowering} fuses the feature from multi-modal inputs (drugs, proteins, and text). Then attention~\citep{vaswani2017attention} mechanism is adopted to do textual information denoising and heterogeneous features integration. 

Our work differs from previous studies in several ways:
(1) we primarily focus on two biological modalities—molecule, protein-with text serving as a knowledge base and bridge to enrich the underlying relations and properties in the molecule and protein domains;
(2) we use multi-task pre-training to model the connections between these three modalities in a more comprehensive manner.
(3) we use SELFIES instead of SMILES to represent molecules, which is more robust and resolves the validity issue in molecule generation tasks.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{pipeline.pdf}
    \caption{Overview of \method{} pre-training. The solid line refers to the ``T5 objective'', which aims to reconstruct the original unmasked input.
    Each consecutive span of masked tokens is replaced with a sentinel token, depicted as \texttt{<M1>}, \texttt{<M2>}, and \texttt{<M3>}.
    We apply this objective to molecule SELFIES (task \#1), protein FASTA (task \#2), general text (task \#3), and wrapped text (task \#4). 
    The dashed line represents the bidirectional translation between bio-sequences and structured text description (task \#5 and \#6).}
    \label{fig:pipeline}
\end{figure*}
\subsection{Representations of Molecule and Protein}
\label{sec:mol_pro_rep}
\noindent{\textbf{Molecule Representation}}
The representation and modeling of molecules have long been a challenge in bioinformatics.
There are many methods to represent a molecule: name, fingerprint~\citep{rogers2010extended}, SMILES~\citep{weininger1988smiles,weininger1989smiles}, InChl~\citep{heller2013inchi}, DeepSMILES~\citep{o2018deepsmiles}, SELFIES~\citep{krenn2020self}, 2D molecular graph, etc.
SMILES (Simplified Molecular-Input Line-Entry System), a compact and textual representation of the molecular structure, is the most common method. It employs a sequence of characters to encode atoms, bonds, and other molecular features.
However, SMILES has several drawbacks~\citep{krenn2022selfies}, such as the lack of syntactic and semantic robustness, which significantly affects the validity of molecules generated by deep learning models~\citep{DBLP:conf/emnlp/EdwardsLRHCJ22}.
To address this issue, SELFIES (Self-referencing Embedded Strings) is introduced as a 100\% robust molecular string representation~\citep{krenn2020self}.
Every permutation of symbols within the SELFIES alphabet invariably generates a chemically valid molecular structure, ensuring that each SELFIES corresponds to a valid molecule. 
Unlike existing works introduced in Section~\ref{sec:cross_modal} that use SMILES for molecule representation, we employ SELFIES with separate encoding in \method{} to achieve 100\% validity in downstream molecule generation tasks.

\noindent{\textbf{Protein Representation}}
Protein can also be represented in various ways, such as by its name, corresponding gene name, FASTA format, or 3D geometric structure.
The FASTA format is a common choice for encoding protein sequences, which uses single-letter codes to represent the  different amino acids. 
In \method, we also employ FASTA format for protein representation.

Unlike~\citet{DBLP:conf/emnlp/EdwardsLRHCJ22} and~\citet{taylor2022galactica} that share the dictionary between bio-sequence tokens and nature language tokens, \method{} uses a separate dictionary and biology-specific tokenization to explicitly distinguish biological modalities. 
We give further analysis of this in Section~\ref{sec:sep_token_emb}.

\section{BioT5}
The overview of the \method{} pre-training is illustrated in Figure~\ref{fig:pipeline}.
We combine data from different modalities to perform multi-task pre-training.

\subsection{Pre-training Corpus}
As shown in Figure~\ref{fig:pipeline}, the pre-training corpus of \method{} is categorized into three classes:
(1) {\em Single-modal data}, including molecule SELFIES, protein FASTA, and general text.
For small molecules, we use the ZINC20~\citep{irwin2020zinc20} dataset and convert SMILES to SELFIES.
For protein FASTA, we randomly sample proteins from the Uniref50~\citep{suzek2007uniref} dataset, filtering out proteins exceeding a specified length, resulting in a collection of M proteins
For general text, we use the ``Colossal Clean Crawled Corpus'' (C4) dataset~\citep{raffel2020exploring}.
(2) {\em Wrapped text}, where molecule names are replaced with their corresponding SELFIES and gene names are appended with related protein FASTA. 
We use M PubMed articles~\citep{canese2013pubmed} and apply BERN2~\citep{sung2022bern2} for named entity recognition.
The scientific sentences which are not replaced or appended by bio-sequences are remained as a supplement to general text.
The detailed process is depicted in Figure~\ref{fig:ner} and discussed in Appendix~\ref{sec:ner_el}.
(3) {\em Molecule-description pairs} and {\em protein-description pairs}.
For molecule-text data, we collect K molecule SELFIES along with their corresponding names and descriptions from PubChem~\citep{kim2019pug}, excluding all molecules present in the downstream ChEBI-20 dataset~\citep{DBLP:conf/emnlp/EdwardsLRHCJ22} to avoid potential data leakage.
For protein-text data, we obtain K protein FASTA-description pairs from Swiss-Prot~\citep{boutet2007uniprotkb}, which contains high-quality annotations of various protein properties. 
Details are left in Appendix~\ref{sec:special_tokens}.

\subsection{Separate Tokenization and Embedding}
\label{sec:sep_token_emb}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{case_tokenization.pdf}
    \caption{Case for tokenization. MolT5 processes ``Br''(bromine atom) as ``B'' (boron atom) and ``r'', resulting in incorrect descriptions including tetraborate (related to ``B''). \method{} retains the chemically meaningful group ``[Br-1]'' as a complete token, thereby producing the correct output.}
    \label{fig:case_tokenization}
\end{figure}
In most previous works, the representation of molecules and proteins has not been modeled with sufficient attention to detail.
MolT5~\citep{DBLP:conf/emnlp/EdwardsLRHCJ22} employs the same dictionary as the original T5, as it starts pre-training from the original T5 checkpoint.
The original T5 dictionary is derived from nature language using SentencePiece~\citep{DBLP:conf/emnlp/KudoR18}.
However, directly utilizing this dictionary for molecule SMILES is suboptimal, as some chemically meaningful tokens, such as functional groups or complete atoms, will be tokenized inaccurately.
For example, in the molecule depicted in Figure~\ref{fig:case_tokenization}, the bromine atom, symbolized as ``Br'' in SMILES, is tokenized as ``B'' (a boron atom) and ``r'' by MolT5.
Consequently, MolT5 incorrectly characterizes this molecule as both dibromolit (related to ``Br'') and tetraborate (related to ``B'').
The character-based tokenization of Galactica~\citep{taylor2022galactica} suffers the same issue.

In addition to the tokenization method, sharing token embeddings for different modalities~\cite{DBLP:conf/emnlp/EdwardsLRHCJ22,taylor2022galactica} is also questionable.
In multilingual tasks, shared embeddings allow models to accurately represent the meanings of borrowed words and cognates, which retain their original meanings across languages.
However, molecules, proteins, and text represent entirely distinct languages.
The same token within these three different modalities carries different semantic meanings.
For example, the token ``C'' signifies character C in nature language, the carbon atom in molecules, and cysteine (one of the  amino acids) in proteins. 
Studies by~\citet{DBLP:conf/emnlp/BeltagyLC19} and~\citet{gu2021domain} further emphasize the significance of domain-specific vocabulary.

To address the issues mentioned above, we employ separate vocabularies for molecule, protein, and text.
In \method{}, molecule is represented by SELFIES string, where each chemical meaningful atom group is enclosed within brackets and tokenized as a SELFIES token.
For example, \texttt{[C][=C][Br]}\texttt{[C],[=C],[Br]}.
For protein, to differentiate amino acids with capital letters in text, we introduce a special prefix \texttt{<p>} for each amino acid. For example, \texttt{<p>M<p>K<p>R}\texttt{<p>M,<p>K,<p>R}.
For text, we use the same dictionary as the original T5.
Through this, we explicitly distinguish the semantic space of different modalities, which maintains the inherent integrity of each unique modality and prevents the model from conflating meanings across modalities.

\subsection{Model and Training}
\noindent{\textbf{Model architecture}}
\method{} employs the same architecture as T5 models~\citep{raffel2020exploring}.
We follow the configuration used in T5-v1.1-base\footnote{\url{https://huggingface.co/docs/transformers/model_doc/t5v1.1}}. 
The vocabulary size of \method{} is ,  differing from the default configuration as we incorporate separate vocabulary for molecule SELFIES and protein amino acids.
In total, the \method{} model comprises M parameters.

\begin{table*}[t!]
\centering
\resizebox{0.9\textwidth}{!}{
\tiny
\begin{tabular}{lcccccccccccccc}
\toprule
Dataset & BBBP & Tox21 & ClinTox & HIV & BACE & SIDER & Avg \\
\textbf{\#Molecules} & 2039 & 7831 & 1478 & 41127 & 1513 & 1427 & -\\
\textbf{\#Tasks} & 1 & 12 & 2 & 1 & 1 & 27 & -\\
\midrule
G-Contextual& 70.31.6 & 75.20.3 & 59.98.2 & 75.90.9 & 79.20.3 & 58.40.6 & 69.8 \\
G-Motif & 66.43.4 & 73.20.8 & 77.82.0 & 73.81.4 & 73.44.0 & 60.61.1 & 70.9\\
GROVER & 70.00.1 & 74.30.1 & 81.23.0 & 62.50.9 & 82.60.7 & 64.80.6 & 72.6 \\
GROVER & 69.50.1 & 73.50.1 & 76.23.7 & 68.21.1 & 81.01.4 & 65.40.1 & 72.3\\
GraphMVP & 72.41.6 & 75.90.5 & 79.12.8 & 77.01.2 & 81.20.9 & 63.91.2 & 74.9\\
MGSSL& 70.51.1 & 76.50.3 & 80.72.1 & 79.51.1 & 79.70.8 & 61.80.8 & 74.8\\
MolCLR & 72.22.1 & 75.00.2 & 91.23.5 & 78.10.5 & 82.40.9 & 58.91.4 & 76.3\\
GEM & 72.40.4 & \textbf{78.10.1} & 90.11.3 & \underline{80.6  0.9} & 85.61.1 & 67.20.4 & 79.0 \\
\midrule
KV-PLM &  74.60.9 & 72.70.6 & -- & 74.01.2 & -- & 61.51.5 & --\\
Galactica& 66.1 & 68.9 & 82.6 & 74.5 & 61.7 & 63.2 & 69.5 \\
MoMu & 70.52.0 & 75.60.3 & 79.94.1 & 76.20.9  & 77.11.4 & 60.50.9 & 73.3 \\
MolXPT & \textbf{80.0  0.5} &  77.10.2  & \underline{95.3  0.2} & 78.10.4 & \underline{88.4  1.0} & \underline{71.7  0.2} & \underline{81.9} \\
\midrule
\method & \underline{77.70.6} & \underline{77.90.2} & \textbf{95.40.5} & \textbf{81.00.1} & \textbf{89.40.3} & \textbf{73.20.2} & \textbf{82.4}\\
\bottomrule
\end{tabular}}
\caption{Performance comparison on MoleculeNet (\textbf{Best}, \underline{Second Best}). The evaluation metric is AUROC. The baseline results are mainly sourced from MolXPT~\citep{liu2023molxpt}.}
\label{tab:moleculenet}
\end{table*}

\begin{table}[h!]
\centering
\resizebox{\linewidth}{!}{
\small
\begin{tabular}{cccc}
\toprule
Model & \#Params. & Solubility & Localization \\
\midrule
DDE & 205.3K & 59.77  1.21 & 77.43  0.42 \\
Moran & 123.4K & 57.73  1.33 & 55.63  0.85 \\
\midrule
LSTM & 26.7M & 70.18  0.63 & 88.11  0.14 \\
Transformer & 21.3M & 70.12  0.31 & 75.74  0.74 \\
CNN & 5.4M & 64.43  0.25 & 82.67  0.32 \\
ResNet & 11.0M & 67.33  1.46 & 78.99  4.41 \\
\midrule
ProtBert & 419.9M & 68.15  0.92 & 91.32  0.89 \\
ProtBert* & 419.9M & 59.17  0.21 & 81.54  0.09 \\
ESM-1b & 652.4M & \underline{70.23  0.75} & \textbf{92.40  0.35} \\
ESM-1b* & 652.4M & 67.02  0.40 & 91.61  0.10\\
\midrule
\method & 252.1M & \textbf{74.65  0.49} & \underline{91.69  0.05} \\
\bottomrule
\end{tabular}
}
\caption{Performance comparison of different methods on solubility and localization prediction tasks (\textbf{Best}, \underline{Second Best}). The evaluation metric is accuracy. * represents only tuning the prediction head. The baseline results are sourced from PEER~\citep{xu2022peer}.}
\label{tab:protein_property}
\end{table}

\begin{table*}[t!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c  ccc  cc  ccc}
\toprule
& \multicolumn{3}{c}{BioSNAP} & \multicolumn{2}{c}{Human} & \multicolumn{3}{c}{BindingDB} \\
\cmidrule(r){2-4}  \cmidrule(r){5-6}  \cmidrule(r){7-9} Method & 
AUROC & AUPRC & Accuracy & AUROC & AUPRC & AUROC & AUPRC & Accuracy\\ \midrule

SVM             & 0.8620.007       & 0.8640.004  & 0.7770.011  & 0.9400.006 & 0.9200.009 & 0.9390.001 & 0.9280.002 & 0.8250.004\\
RF        & 0.8600.005       & 0.8860.005  & 0.8040.005 & 0.9520.011 & 0.9530.010  & 0.9420.011        & 0.9210.016   & 0.8800.012\\
DeepConv-DTI           & 0.8860.006 & 0.8900.006  & 0.8050.009  & 0.9800.002 & 0.9810.002  & 0.9450.002       & 0.9250.005 &0.8820.007\\
GraphDTA      & 0.8870.008       & 0.8900.007  & 0.8000.007 & 0.9810.001 & \underline{0.9820.002}  & 0.9510.002        & 0.9340.002 & 0.8880.005\\
MolTrans & 0.8950.004       & 0.8970.005  & 0.8250.010 & 0.9800.002 & 0.9780.003  & 0.9520.002        & 0.9360.001 & 0.8870.006 \\
DrugBAN & \underline{0.9030.005}       & \underline{0.9020.004}  & \underline{0.8340.008} & \underline{0.9820.002} & 0.9800.003 & \underline{0.9600.001}        & \underline{0.9480.002} & \underline{0.9040.004}\\ \midrule
\method & \textbf{0.9370.001} & \textbf{0.9370.004} & \textbf{0.8740.001} & \textbf{0.9890.001} & \textbf{0.9850.002} & \textbf{0.9630.001} & \textbf{0.9520.001} & \textbf{0.9070.003}\\
\bottomrule
\end{tabular}
}
\caption{Performance comparison on the BindingDB, Human and BioSNAP datasets. (\textbf{Best}, \underline{Second Best}). The baseline results derive from DrugBAN~\citep{bai2023interpretable}.}
\label{tab:dti}
\end{table*}

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\small
\begin{tabular}{cccc}
\toprule
Model & \#Params. & Yeast & Human \\
\midrule
DDE & 205.3K & 55.83  3.13 & 62.77  2.30 \\
Moran & 123.4K & 53.00  0.50 & 54.67  4.43 \\
\midrule
LSTM & 26.7M & 53.62  2.72 & 63.75  5.12 \\
Transformer & 21.3M & 54.12  1.27 & 59.58  2.09 \\
CNN & 5.4M & 55.07  0.02 & 62.60  1.67 \\
ResNet & 11.0M & 48.91  1.78 & 68.61  3.78 \\
\midrule
ProtBert & 419.9M & 63.72  2.80 & 77.32  1.10 \\
ProtBert* & 419.9M & 53.87  0.38 & 83.61  1.34 \\
ESM-1b & 652.4M & 57.00  6.38 & 78.17  2.91 \\
ESM-1b* & 652.4M & \textbf{66.07  0.58} & \textbf{88.06  0.24}\\
\midrule
\method & 252.1M & \underline{64.89  0.43} & \underline{86.22  0.53} \\
\bottomrule
\end{tabular}
}
\caption{Performance comparison on Yeast and Human datasets (\textbf{Best}, \underline{Second Best}). The evaluation metric is accuracy. * represents only tuning the prediction head. The baseline results derive from PEER~\citep{xu2022peer}.}
\label{tab:ppi}
\end{table}

\noindent{\textbf{Pre-training}}
During the pre-training phase, the model is trained in a multi-task way on six tasks that can be classified into three types:
(1) Applying T5 objective to each single modality including molecule SELFIES (task \#1), protein FASTA (task \#2), and general text (task \#3) independently.
(2) Applying T5 objective to wrapped text from scientific corpus (task \#4).
(3) Bidirectional translation for the molecule SELFIES-text pairs (task \#5) and protein FASTA-text pairs (task \#6).
By effectively learning the underlying connections and properties of bio-entities from textual information through these pre-training tasks, \method{} allows for a holistic understanding of the biological domain, thereby facilitating enhanced prediction and generation abilities in various biological tasks.

\noindent{\textbf{Fine-tuning}}
\label{sec:finetune}
\method{} can be fine-tuned on various downstream tasks involving molecules, proteins, and text.
To unify different downstream tasks and reduce the gap between pre-training and fine-tuning~\citep{brown2020language} stage, we adopt the prompt-based fine-tuning~\citep{DBLP:conf/acl/GaoFC20} approach, which facilitates various task formats into a sequence generation format.

\section{Experiments and Results}
We evaluate \method{} on  well-established downstream tasks, which can be categorized into three types: single-instance prediction, multi-instance prediction, and cross-modal generation.
We include details regarding fine-tuning datasets, baselines, and prompts in Appendix~\ref{sec:finetune_detail}.

For the downstream binary classification tasks presented in Section~\ref{sec:single_instance} and ~\ref{sec:multi_instance}, the calculation of evaluation metrics such as AUROC and AUPRC necessitates the soft probability of the predicted label.
As we use the prompt-based fine-tuning method, the output is either \textit{Yes} for the positive label or \textit{No} for the negative label.
To obtain an appropriate label distribution, following~\citet{liu2023molxpt}, we first extract the probabilities of \textit{Yes} and \textit{No} tokens (denoted as  and  respectively) and normalize them.
The resulting probability for positive label is  and negative label is .

\subsection{Single-instance Prediction}
\label{sec:single_instance}
\subsubsection{Molecule Property Prediction}
\label{sec:molecule_property}
Molecule property prediction aims to determine whether a given molecule exhibits specific properties.
MoleculeNet~\citep{wu2018moleculenet} is a widely used benchmark for molecule property prediction, encompassing diverse datasets that cover numerous molecular aspects, such as quantum mechanics, physical chemistry, biophysics, etc.
In line with~\citet{liu2023molxpt}, we conduct experiments on six binary classification tasks, including BBBP, Tox21, ClinTox, HIV, BACE, and SIDER.
Following~\citep{fang2022geometry}, we adopt the scaffold splitting, which is more challenging compared to random splitting.

\noindent{\textbf{Baselines}}
We compare \method{} with two types of baselines:
(1) pre-trained Graph Neural Network (GNN) using molecular graph as input, which are G-Contextual~\citep{rong2020self}, G-Motif~\citep{rong2020self}, GROVER~\citep{rong2020self}, GROVER~\citep{rong2020self}, GraphMVP~\citep{DBLP:conf/iclr/LiuWLLGT22}, MGSSL~\citep{zhang2021motif} MolCLR~\citep{wang2022molecular} and GEM~\citep{fang2022geometry};
(2) pre-trained language model baselines, which are KV-PLM~\citep{zeng2022deep}, Galactica~\citep{taylor2022galactica}, MoMu~\citep{su2022molecular} and MolXPT~\cite{liu2023molxpt}.

\noindent{\textbf{Results}}
The results are presented in Table~\ref{tab:moleculenet} with all statistics derived from three random runs.
From these results, we can see that \method{} surpasses baselines on most downstream tasks in MoleculeNet.
\method{} exhibits superior performance compared to GNN baselines that are pre-trained on 2D/3D molecular data, underscoring the effectiveness of knowledge in text.
Furthermore, \method{} outperforms other language model baselines, which may be attributed to the presence of molecule property descriptions in scientific contextual text or existing biological database entries.

\subsubsection{Protein Property Prediction}
\label{sec:protein_property_prediction}
Protein property prediction is crucial as it provides critical insights into the behavior and functions of proteins.
We concentrate on two protein property prediction tasks on PEER benchmark~\citep{xu2022peer}: protein solubility prediction, which aims to predict whether the given protein is soluble, and protein localization prediction, which is to classify proteins as either ``membrane-bound'' or ``soluble''.


\noindent{\textbf{Baselines}}
We compare \method{} with three types of baselines provided in PEER benchmark:
(1) feature engineers, including two protein sequence feature descriptors: Dipeptide Deviation from Expected Mean (DDE)~\citep{saravanan2015harnessing} and Moran correlation (Moran)~\citep{feng2000prediction};
(2) protein sequence encoders, including LSTM~\citep{hochreiter1997long}, Transformers~\citep{vaswani2017attention}, CNN~\citep{o2015introduction} and ResNet~\citep{he2016deep};
(3) pre-trained protein language models, which are pre-trained using extensive collections of protein FASTA sequences, including ProtBert~\citep{elnaggar2021prottrans} and ESM-1b~\citep{rives2021biological}.
Both ProtBert and ESM-1b are studied with two settings 
(i) freezing the protein language model parameters and only training the prediction head;
(ii) fine-tuning all model parameters.

\noindent{\textbf{Results}}
The results are displayed in Table~\ref{tab:protein_property}, with all statistics derived from three random runs.
In the protein solubility prediction task, \method{} outperforms all baselines in PEER~\citep{xu2022peer} benchmark.
In the protein localization prediction task, \method{} is the second best among all methods.
Notably, ProtBert and ESM-1b are both pre-trained on a large corpus of protein sequences, which is comparable to or even larger than ours.
Moreover, these models are two to three times larger than \method{}.
These demonstrate the potential of \method{} for enhanced predictive capabilities in protein property prediction by integrating textual information.


\subsection{Multi-instance Prediction}
\label{sec:multi_instance}
\subsubsection{Drug-target Interaction Prediction}
\label{sec:dti}
Drug-target interaction (DTI) prediction plays a crucial role in drug discovery, as it aims to predict whether a given drug (molecule) and target (protein) can interact with each other.
We select three widely-used DTI datasets with a binary classification setting, which are BioSNAP~\citep{zitnik2018biosnap}, BindingDB~\citep{liu2007bindingdb} and Human~\citep{liu2015improving,chen2020transformercpi}.

\noindent{\textbf{Baselines}}
We compare \method{} with two types of baselines:
(1) traditional machine learning methods including SVM~\citep{cortes1995support} and Random Forest (RF)~\citep{ho1995random};
(2) deep learning methods including DeepConv-DTI~\citep{Lee2019DeepConvDTIPO}, GraphDTA~\citep{Nguyen2020GraphDTAPD}, MolTrans~\citep{Huang2021MolTransMI} and DrugBAN~\citep{bai2023interpretable}, in which drug and target feature are firstly extracted by well-design drug encoder and protein encoder then fused for prediction.

\noindent{\textbf{Results}}
The results on BioSNAP, Human, and BindingDB datasets are presented in Table~\ref{tab:dti}. All statistics are obtained from five random runs.
On BioSNAP and BindingDB datasets, \method{} consistently outperforms other methods in various performance metrics, including AUROC, AUPRC, and accuracy.
For the Human dataset, although deep learning-based models generally exhibit strong performance, the \method{} model demonstrates a slight advantage over the baseline models.
It is worth noting that, in contrast to most deep learning-based baselines, our \method{} does not rely on a specific design tailored for molecules or proteins. A possible explanation for the superior performance of \method{} is that the SELFIES and FASTA representations effectively capture the intricate structure and function of molecules and proteins, and the interaction information between them may be well-described in the contextual scientific literature or corresponding text entries in databases.

\begin{table*}[t]
\resizebox{\textwidth}{!}{
\centering
\small
\begin{tabular}{ccccccccc}
\toprule
Model & \#Params. & BLEU-2 & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & METEOR & Text2Mol \\
\midrule
RNN & 56M & 0.251 & 0.176 & 0.450 & 0.278 & 0.394 & 0.363 & 0.426 \\
Transformer & 76M & 0.061 & 0.027 & 0.204 & 0.087 & 0.186 & 0.114 & 0.057 \\\midrule
T5-small & 77M & 0.501 & 0.415 & 0.602 & 0.446 & 0.545 & 0.532 & 0.526 \\
T5-base & 248M & 0.511 & 0.423 & 0.607 & 0.451 & 0.550 & 0.539 & 0.523 \\
T5-large & 783M & 0.558 & 0.467 & 0.630 & 0.478 & 0.569 & 0.586 & 0.563 \\
\midrule
T5-small & 77M & 0.501 & 0.415 & 0.602 & 0.446 & 0.545 & 0.532 & 0.526 \\
MolT5-small & 77M & 0.519 & 0.436 & 0.620 & 0.469 & 0.563 & 0.551 & 0.540 \\
T5-base & 248M & 0.511 & 0.423 & 0.607 & 0.451 & 0.550 & 0.539 & 0.523 \\
MolT5-base & 248M & 0.540 & 0.457 & 0.634 & 0.485 & 0.578 & 0.569 & 0.547 \\
T5-large & 783M & 0.558 & 0.467 & 0.630 & 0.478 & 0.569 & 0.586 & 0.563 \\
MolT5-large & 783M & \underline{0.594} & \underline{0.508} & 0.654 & 0.510 & 0.594 & 0.614 & 0.582 \\\midrule
GPT-3.5-turbo (zero-shot) & >175B & 0.103 & 0.050 & 0.261 & 0.088 & 0.204 & 0.161 & 0.352\\
GPT-3.5-turbo (10-shot MolReGPT) & >175B & 0.565 & 0.482 & 0.623 & 0.450 & 0.543 & 0.585 & 0.560\\\midrule
MolXPT & 350M & \underline{0.594} & 0.505 & \underline{0.660} & \underline{0.511} & \underline{0.597} & \underline{0.626} & \underline{0.594} \\\midrule
\method & 252M & \textbf{0.635} & \textbf{0.556} & \textbf{0.692} & \textbf{0.559} & \textbf{0.633} & \textbf{0.656} & \textbf{0.603} \\
\bottomrule
\end{tabular}
}
\caption{Performance comparison on molecule captioning task (\textbf{Best}, \underline{Second Best}). Rouge scores are F1 values. 
The Text2Mol score between ground truth molecule and corresponding text description is .
The baseline results derive from MolT5~\citep{DBLP:conf/emnlp/EdwardsLRHCJ22}, MolXPT~\citep{liu2023molxpt}, and MolReGPT~\citep{li2023empowering}.
}
\label{tab:mol2text}
\end{table*}

\begin{table*}[t]
\resizebox{\textwidth}{!}{
\centering
\begin{tabular}{ccccccccccc}
\toprule
Model & \#Params. & BLEU & Exact & Levenshtein & MACCS FTS & RDK FTS & Morgan FTS & FCD & Text2Mol & Validity \\
\midrule
RNN & 56M & 0.652 & 0.005 & 38.09 & 0.591 & 0.400 & 0.362 & 4.55 & 0.409 & 0.542 \\
Transformer & 76M & 0.499 & 0.000 & 57.66 & 0.480 & 0.320 & 0.217 & 11.32 & 0.277 & 0.906 \\\midrule
T5-small & 77M & 0.741 & 0.064 & 27.703 & 0.704 & 0.578  & 0.525  & 2.89  & 0.479 & 0.608 \\
T5-base & 248M & 0.762 & 0.069 & 24.950 & 0.731 &  0.605 & 0.545 & 2.48 & 0.499 & 0.660  \\
T5-large & 783M & 0.854 & 0.279 & 16.721 & 0.823 &  0.731 & 0.670 & 1.22 & 0.552 & 0.902 \\
\midrule
T5-small & 77M & 0.741 & 0.064 & 27.703 & 0.704 & 0.578  & 0.525  & 2.89  & 0.479 & 0.608 \\
MolT5-small & 77M & 0.755 & 0.079 & 25.988 & 0.703 & 0.568 & 0.517 & 2.49 & 0.482 & 0.721 \\
T5-base & 248M & 0.762 & 0.069 & 24.950 & 0.731 &  0.605 & 0.545 & 2.48 & 0.499 & 0.660  \\
MolT5-base & 248M & 0.769 & 0.081 & 24.458 & 0.721 &  0.588 & 0.529 & 2.18 & 0.496 & 0.772 \\
T5-large & 783M & \underline{0.854} & 0.279 & 16.721 & 0.823 &  0.731 & 0.670 & 1.22 & 0.552 & 0.902 \\
MolT5-large & 783M & \underline{0.854} & \underline{0.311} & \underline{16.071} & 0.834 & 0.746 & \underline{0.684} & 1.20 & 0.554 & 0.905 \\\midrule
GPT-3.5-turbo (zero-shot) & >175B & 0.489 & 0.019 & 52.13 & 0.705 & 0.462 & 0.367 & 2.05 & 0.479 & 0.802 \\
GPT-3.5-turbo (10-shot MolReGPT) & >175B & 0.790 & 0.139 & 24.91 & 0.847 & 0.708 & 0.624 & 0.57 & 0.571 & 0.887\\\midrule
MolXPT & 350M & - & 0.215 & - & \underline{0.859} & \underline{0.757} & 0.667 & \underline{0.45} & \textbf{0.578} & \underline{0.983} \\\midrule
\method & 252M & \textbf{0.867} & \textbf{0.413} & \textbf{15.097} & \textbf{0.886} & \textbf{0.801} & \textbf{0.734} & \textbf{0.43} & \underline{0.576} & \textbf{1.000} \\
\bottomrule
\end{tabular}
}
\caption{Performance comparison on text-based molecule generation task (\textbf{Best}, \underline{Second Best}). Following~\citet{DBLP:conf/emnlp/EdwardsLRHCJ22}, BLEU, Exact, Levenshtein, and Validity are computed on all generated molecues while other metrics are computed only on syntactically valid molecules. 
The Text2Mol score for ground truth is .
The baseline results derive from MolT5~\citep{DBLP:conf/emnlp/EdwardsLRHCJ22}, MolXPT~\citep{liu2023molxpt}, and MolReGPT~\citep{li2023empowering}.
}
\label{tab:text2mol}
\end{table*}

\subsubsection{Protein-protein Interaction Prediction}
\label{sec:ppi}
Protein-protein interaction (PPI) prediction plays a vital role in understanding protein functions and structures, as it aims to determine the potential interactions between pairs of proteins.
Following PEER~\citep{xu2022peer} benchmark, we perform fine-tuning on two PPI datasets: Yeast~\citep{guo2008using} and Human~\citep{pan2010large}.

\noindent{\textbf{Baselines}}
The baselines for comparison are the same as that in Section~\ref{sec:protein_property_prediction}.

\noindent{\textbf{Results}}
The results are shown in Table~\ref{tab:ppi}. All statistics are over three random runs.
On two PPI datasets, \method{} shows superior performance compared to almost all baseline models.
Remarkably, \method{} outperforms both ProtBert and ESM-1b (with full parameters fine-tuned).
This result strongly highlights the crucial role of incorporating textual information during the pre-training of BioT5, which effectively establishes profound connections between proteins.
Our model, despite being smaller, is able to harness the unstructured information embedded in scientific text and structured information from biological databases, encapsulating the comprehensive knowledge of proteins in their varying contexts. 

\subsection{Cross-modal Generation}
In this section, we evaluate the performance of \method{} on the cross-modal generation task.
Specifically, we fine-tune \method{} on molecule captioning and text-based molecule generation tasks.
These two tasks are proposed by MolT5~\citep{DBLP:conf/emnlp/EdwardsLRHCJ22} and both use the ChEBI-20 dataset~\citep{DBLP:conf/emnlp/EdwardsZJ21}.
The evaluation metrics and some interesting cases are introduced in Appendix~\ref{sec:mol_text_metric} and~\ref{sec:case_study}.
\subsubsection{Molecule Captioning}
\label{sec:mol2text}
For the given molecule, the goal of molecule captioning task is to provide a description of the given molecule.
As we use SELFIES sequences to represent molecules, this task can be formulated as an exotic sequence-to-sequence translation task.

\noindent{\textbf{Baselines}}
The baselines include:
RNN~\citep{medsker2001recurrent}, Transformer~\citep{vaswani2017attention}, T5~\citep{raffel2020exploring}, MolT5~\citep{DBLP:conf/emnlp/EdwardsLRHCJ22}, GPT-3.5-turbo\footnote{\url{https://openai.com/blog/openai-api}} with zero-shot and 10-shot MolReGPT~\citep{li2023empowering} settings, and MolXPT~\cite{liu2023molxpt}.

\noindent{\textbf{Results}}
The results are shown in Table~\ref{tab:mol2text}.
\method{} only has nearly the same number of parameters as MolT5-base, but outperforms all baseline models in all metrics, including those that have more parameters.
The Text2Mol score is , which is very close to the Text2Mol score of  between the ground truth molecule and the corresponding description.
We can attribute this superior performance to the unstructured contextual knowledge and structured database knowledge induced in \method{} pre-training, which helps the model learn the intricate relationship between text and molecules. 

\subsubsection{Text-Based Molecule Generation}
\label{sec:text2mol}
This is a reverse task of molecule captioning.
Given the nature language description of the intended molecule, the goal is to generate the molecule that fits the description.

\noindent{\textbf{Baselines}}
The compared baselines are the same as baselines in Section~\ref{sec:mol2text}.

\noindent{\textbf{Results}}
The results are presented in Table~\ref{tab:text2mol}.
\method{} only uses parameters similar to MolT5-base yet delivers superior performance across nearly all metrics.
Notably, the exact match score of \method{} surpasses the MolT5-Large by 32.8\% while maintaining a validity of .
This indicates that \method{} not only generates more relevant molecules corresponding to the given text descriptions, but also ensures a 100\% validity for the generated molecules.
The overall enhanced performance of \method{} can be attributed to the incorporation of both contextual and database knowledge, as well as the utilization of SELFIES for molecular representation.

\section{Conclusions and Future Work}
In this paper, we propose \method{}, a comprehensive pre-training framework capable of capturing the underlying relations and properties of bio-entities by leveraging both structured and unstructured data sources with 100\% robust molecular representation. 
Our method effectively enriches cross-modal integration in biology with chemical knowledge and natural language associations, demonstrating notable improvements in various tasks.

For future work, we aim to further enrich our model by incorporating additional biological data types, such as genomics or transcriptomics data, to create a more holistic biological pre-training framework. 
Additionally, we plan to evaluate the interpretability of \method{} predictions, aiming to provide more insights into the biological systems under study. 
Thus, we foresee our work sparking further innovation in the use of AI models in the field of computational biology, ultimately leading to a deeper understanding of biological systems and facilitating more efficient drug discovery.

\section{Limitations}
One limitation of \method{} is conducting full-parameter fine-tuning on each downstream task. 
This is done because we do not observe generalization ability among different downstream tasks using instruction-tuning~\citep{DBLP:conf/iclr/WeiBZGYLDDL22} method.
Another reason is that combining data from different tasks using instructions results in data leakage. 
For example, have noticed overlaps between the training set of BindingDB and the test sets of BioSNAP and Human.
Additionally, we only demonstrate the ability of \method{} in text, molecule, and protein modalities. 
Numerous other biological modalities, such as DNA/RNA sequences and cells, exist, and there are many other tasks within a single modality or across multiple modalities.
Moreover, \method{} primarily focuses on the sequence format of bio-entities, yet other formats, such as 2D or 3D structures, also hold significant importance.
We leave further exploration of these to future work.

\section{Acknowledgements}
This work was supported by the National Key Research and Development Program of China (No. 2020YFB1406702), National Natural Science Foundation of China (NSFC Grant No. 62122089), Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098, and Intelligent Social Governance Platform, Major Innovation \& Planning Interdisciplinary Platform for the ``Double-First Class'' Initiative, Renmin University of China, the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China. 

\bibliography{custom}
\bibliographystyle{acl_natbib}

\clearpage
\appendix

\section{Reproducibility}
The codes for our \method{} are available at \url{https://github.com/QizhiPei/BioT5}. 

\section{NER and Entity Linking Process}
\label{sec:ner_el}
We follow KV-PLM~\citep{zeng2022deep} and MolXPT~\citep{liu2023molxpt} to conduct Named Entity Recognition (NER) and Entity Linking for the bio-entity names appearing in the scientific text.
More specifically, we firstly utilize BERN2~\citep{sung2022bern2}, an advanced neural Named Entity Recognition (NER) tool in biomedical fields, to identify all instances of molecule or protein mentions. 
Subsequently, we map them to corresponding entities within publicly accessible knowledge bases.
For molecule, we use ChEBI~\citep{hastings2016chebi} and MeSH~\citep{lipscomb2000medical} database, and for protein we use NCBI Gene~\citep{brister2015ncbi} database.
Then we can get the corresponding molecule SELFIES and protein FASTA for the matched entities.
\begin{figure}[h]
    \centering
\includegraphics[width=\linewidth]{ner.pdf}
    \caption{Wrapped text matching and mapping process.}
    \label{fig:ner}
\end{figure}
As shown in Figure~\ref{fig:ner}, for molecule, we directly replace all the detected names with its SELFIES string;
for protein, due to the length limitation, if a sentence consists of more than one protein entity, we only randomly choose one to append the protein FASTA to the name.
The motivation for appending protein FASTA instead of replacing is that the genes are transcribed and translated to generate proteins. 
Therefore, unlike the molecule names directly representing the molecule, the relation between gene names and protein FASTA is indirect.
Note that the replacement or appendage will not happen in every sentence.
Only those with detected bio-entities will be done the above process.

\section{Dictionary and SELFIES Conversion}
\label{sec:dict_and_conversion}
For molecule-related datasets, when only SMILES is provided, we utilize \textit{selfies\footnote{\url{https://github.com/aspuru-guzik-group/selfies}}} package to convert SMILES into SELFIES.

\section{Molecule-Text Generation Metrics}
\label{sec:mol_text_metric}
We follow~\citet{DBLP:conf/emnlp/EdwardsLRHCJ22} to use the same evaluation metrics for molecule captioning and text-based molecule generation tasks.
To ensure a fair comparison, we convert the molecule SEIFLES to SMILES before calculating these metrics.
\subsection{Molecule Captioning Metrics}
In the molecule caption task, NLP metrics like BLEU~\citep{DBLP:conf/acl/PapineniRWZ02}, ROUGE~\citep{lin2004rouge}, and METEOR~\citep{DBLP:conf/acl/BanerjeeL05} are utilized to evaluate the closeness of the generated description to the ground truth description.
We also adopt \textit{Text2Mol} metric, which is proposed by~\citet{DBLP:conf/emnlp/EdwardsZJ21} and employ pre-trained models to measure the similarity between the description and ground truth molecule.
Higher similarity means that the given text description is more relevant to the molecule, and the Text2Mol score between the ground truth description and molecule is also computed for comparison.

\subsection{Text-based Molecule Generation Metrics}
Since molecules can be represented in bio-sequence structure, NLP metrics like BLEU~\citep{DBLP:conf/acl/PapineniRWZ02} and Exact Match scores between generated and ground truth SMILES are directly applied for evaluation.
Additionally, we also report performance on molecule-specific metrics: three molecule fingerprints (FTS) similarity scores-MACCS~\citep{durant2002reoptimization}, RDK~\citep{DBLP:journals/jcisd/SchneiderSL15}, and Morgan~\citep{rogers2010extended};
Levenshtein distance~\citep{miller2009levenshtein}; 
FCD score~\citep{DBLP:journals/jcisd/PreuerRUHK18}, which measures molecule similarities according to biological information based on pre-trained ``ChemNet'';
validity, which is the percentage of the valid SMILES that can be processed by RDKit~\citep{Landrum2021RDKit2021_03_2}.
The \textit{Text2Mol} metric is also used to measure the similarity between the molecule SMILES and ground truth description.


\section{Pre-training Details}
\label{sec:pretrain_dataset}
\subsection{Special Tokens}
\label{sec:special_tokens}
In the pre-training of \method{}, we conduct translation tasks on molecule-text pairs and protein-text pairs extracted from PubChem~\citep{kim2023pubchem} and Swiss-Prot~\citep{boutet2007uniprotkb} separately.
We format the text description from these database entries using special tokens, which serve as anchors for embedding scientific context and structure.
For molecule, we use \textit{MOLECULE NAME} and \textit{DESCRIPTION} to represent its name and description including properties, functions, etc.
For protein, similar to~\citet{xu2023protst}, we use \textit{PROTEIN NAME}, \textit{FUNCTION}, \textit{SUBCELLULAR LOCATION}, and \textit{PROTEIN FAMILIES} to represent its name, functions, location and topology in the cell, and families it belongs to.
A complete text description is created by concatenating these fields sequentially, omitting any missing fields.
Through special tokens, we can effectively encode the intricate information associated with each bio-entity.
\subsection{Hyper-parameters}
We use the codebase {\em nanoT5}~\citep{Nawrot_nanoT5_2023} for \method{} pre-training.
We pre-train \method{} for K steps on eight NVIDIA 80GB A100 GPUs.
The batch size is  per GPU, in which a batch includes six types of data.
The ``translation'' directions for molecule-text and protein-text pair are randomly selected for each sample with a probability of .
We use AdamW~\citep{DBLP:conf/iclr/LoshchilovH19} with Root Mean Square (RMS) scaling Optimizer for optimization.
The learning rate scheduler is cosine annealing with the base learning rate set to  and the minimum learning rate set to . 
The number of warm-up steps is 10,000 and the dropout rate is .
The maximum input length for pre-training is . 
Unlike absolute position encodings, T5~\citep{raffel2020exploring} use relative position encodings.
This makes the model flexible to inputs of different lengths, which is helpful for downstream fine-tuning.

\section{Fine-tuning Details}
\label{sec:finetune_detail}
\begin{table*}[t]
    \centering
    \resizebox{0.9\textwidth}{!}{
    \small
    \begin{tabular}{ccccc}
        \toprule
        \bf{Task/Dataset} & \bf{Task Type} & \bf{\#Train} & \bf{\#Validation} & \bf{\#Test} \\
        \midrule
        \multicolumn{5}{c}{\bf{Molecule Property Prediction}} \\
        \midrule
        \bf{BBBP} & Molecule-wise Classification & 1,631 & 204 & 204 \\
        \bf{Tox21} & Molecule-wise Classification & 6,264 & 783 & 784 \\
        \bf{ClinTox} & Molecule-wise Classification & 1,181 & 148 & 148 \\
        \bf{HIV} & Molecule-wise Classification & 32,901 & 4,113 & 4,113 \\
        \bf{BACE} & Molecule-wise Classification & 1,210 & 151 & 152 \\
        \bf{SIDER} & Molecule-wise Classification & 1,141 & 143 & 143 \\
        \midrule
        \multicolumn{5}{c}{\bf{Protein Property Prediction}} \\
        \midrule
        \bf{Solubility prediction} & Protein-wise Classification & 62,478 & 1,999 & 1,999\\
        \bf{Localization prediction} & Protein-wise Classification & 5,184 & 1,749 & 1,749 \\
        \midrule
        \multicolumn{5}{c}{\bf{Drug-target Interaction Prediction}} \\
        \midrule
        \bf{BioSNAP} & Molecule-protein Classification & 19,224 & 2,747 & 5,493 \\
        \bf{Human} & Molecule-protein Classification & 4,197 & 600 & 1,200 \\
        \bf{BindingDB} & Molecule-protein Classification & 50,149 & 5,604 & 5,505 \\
        \midrule
        \multicolumn{5}{c}{\bf{Protein-protein Interaction Prediction}} \\
        \midrule
        \bf{Yeast} & Protein-pair Classification & 4,945 & 394 & 394 \\
        \bf{Human} & Molecule-pair Classification & 35,669 & 237 & 237 \\
        \midrule
        \multicolumn{5}{c}{\bf{Molecule Captioning and Text-based Molecule Generation}} \\
        \midrule
        \bf{ChEBI-20} & Molecule-text Translation & 26,407 & 3,301 & 3,300 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Downstream task descriptions, including task or dataset name, type, and the size of each split.} 
    \label{tab:downstream_task}
\end{table*}
In this section, we provide details about downstream tasks, including datasets, compared baselines, and prompts.
Some statistics about downstream tasks are shown in Table~\ref{tab:downstream_task}
When displaying prompts, \selfies{} refers to the molecule SELFIES, and \fasta{} refers to the protein FASTA. 

\subsection{Single-instance Prediction}
\subsubsection{Molecule Property Prediction}
All the datasets are split using an  ratio for train, validation, and test, respectively.
We use the scaffold splitting method, in which molecules are categorized according to the Bemis-Murcko scaffold representation.

\noindent{\textbf{Datasets}}

\noindent(1) The BBBP (Blood-Brain Barrier Penetration) is curated with the intention of aiding the modeling and forecasting of barrier permeability. It comprises compounds that are categorized using binary labels, indicating whether they can penetrate the blood-brain barrier.

\noindent(2) The Tox21 ("Toxicology in the 21st Century") initiative established a publicly accessible database that quantifies the toxicity levels of various compounds. The dataset encompasses qualitative toxicity assessments (binary labels) for approximately 8,000 compounds, targeting 12 distinct biological pathways such as nuclear receptors and stress response mechanisms.

\noindent(3) The ClinTox dataset contrasts FDA-approved drugs with those that have been unsuccessful in clinical trials owing to toxicity issues. This dataset incorporates two classification objectives for 1,491 drug compounds with established chemical structures:
(i) Presence or absence of toxicity in clinical trials; (ii) approved or unapproved by FDA.

\noindent(4) The HIV dataset assesses the inhibitory potential of over 40,000 compounds on HIV replication. The screening outcomes were classified into three categories: Confirmed Inactive (CI), Confirmed Active (CA), and Confirmed Moderately Active (CM). Subsequently, the latter two labels were combined, transforming the task into a binary classification between inactive (CI) and active (CA and CM) categories.

\noindent(5) The BACE dataset presents quantitative IC50 values and qualitative binary labels for a collection of inhibitors targeting human beta-secretase 1 (BACE-1).

\noindent(6) The SIDER (Side Effect Resource) is a comprehensive database that consists of marketed drugs and their corresponding adverse drug reactions (ADR). The drug side effects in SIDER are organized into 27 system organ classes, adhering to the MedDRA classifications. This dataset encompasses data for 1,427 approved drugs.

\noindent{\textbf{Baselines}}

\noindent(1) GROVER~\citep{rong2020self} incorporates Message Passing Networks within a Transformer-style architecture and is pre-trained on large-scale molecular dataset without any supervision. G-Contextual and G-Motif are two variants of GROVER, which are pre-trained on contextual property prediction task and motif prediction task, respectively.

\noindent(2) GraphMVP~\citep{DBLP:conf/iclr/LiuWLLGT22} employs self-supervised learning by capitalizing on the correspondence and consistency between molecule 2D topological structures and 3D geometric views. 

\noindent(3) MGSSL~\citep{zhang2021motif} incorporates a novel self-supervised motif generation framework for Graph Neural Networks.

\noindent(4) MolCLR~\citep{wang2022molecular} is a self-supervised learning framework that capitalizes on substantial unlabelled unique molecules (approximately 10 million)

\noindent(5) GEM~\citep{fang2022geometry} features a specially designed geometry-based graph neural network architecture and several dedicated geometry-level self-supervised learning strategies to capture molecular geometry knowledge effectively.

\noindent(6) KV-PLM~\citep{zeng2022deep} is a BERT-based model designed for molecular representation learning, in which molecule SMILES are appended after its name during the pre-training process. This combination of molecular names and SMILES sequences allows the model to capture both textual and structural information, thereby enhancing its performance in various downstream tasks.

\noindent(7) Galactica~\citep{taylor2022galactica} is a large GPT-based language model which is pre-trained on various corpus like papers, codes, SMILES, protein sequences, etc.

\noindent(8) MoMu~\citep{su2022molecular} is pre-trained using molecular graphs and their semantically related textual data through contrastive learning.

\noindent(9) MolXPT~\cite{liu2023molxpt} is a unified GPT-based language model for text and molecules pre-trained on ``wrapped'' text, where molecule names are replaced with corresponding SMILES.

\noindent{\textbf{Prompts}}

\noindent For the six MoleculeNet datasets mentioned above, the prompts only differ in the \underline{Task Definition}. Therefore, we will only provide the \underline{Instruction} and \underline{Output} for the first dataset, and the remaining datasets will follow the same format.

\noindent(1) BBBP

\noindent{\underline{Task Definition}}:
\textit{Definition: Molecule property prediction task (a binary classification task) for the BBBP dataset. The blood-brain barrier penetration (BBBP) dataset is designed for the modeling and prediction of barrier permeability. If the given molecule can penetrate the blood-brain barrier, indicate via "Yes". Otherwise, response via "No".} 

\noindent{\underline{Instruction}}:
\textit{Now complete the following example - Input: Molecule: \bom{}\selfies{}\eom{} Output:}.

\noindent{\underline{Output}}: \textit{Yes} for inhibitor and \textit{No} instead.

\noindent(2) Tox21

\noindent{\underline{Task Definition}}:
\textit{Definition: Molecule property prediction task (a binary classification task) for the Tox21 dataset. The Tox21 dataset contains qualitative toxicity measurements for 8k compounds on 12 different targets, including nuclear receptors and stress response pathways. If the given molecule can activate/change/affect \texttt{target}, indicate via "Yes". Otherwise, response via "No".} where \texttt{target} represents the corresponding receptor, domain, element, gene, potential, or pathway for each subtask.

\noindent(3) ClinTox

\noindent{\underline{Task Definition}}:
\textit{Definition: Molecule property prediction task (a binary classification task) for the ClinTox dataset. The ClinTox dataset compares drugs approved by the FDA and drugs that have failed clinical trials for toxicity reasons. If the given molecule is \texttt{Subtask}, indicate via "Yes". Otherwise, response via "No".} where the \texttt{Subtask} is either \textit{toxic} or \textit{FDA approved}.

\noindent(4) HIV

\noindent{\underline{Task Definition}}:
\textit{Definition: Molecule property prediction task (a binary classification task) for the HIV dataset. The HIV dataset was introduced by the Drug Therapeutics Program (DTP) AIDS Antiviral Screen, which tested the ability to inhibit HIV replication for over 40,000 compounds. If the given molecule can inhibit HIV replication, indicate via "Yes". Otherwise, response via "No".}

\noindent(5) BACE

\noindent{\underline{Task Definition}}:
\textit{Definition: Molecule property prediction task (a binary classification task) for the BACE dataset. The BACE dataset provides qualitative (binary label) binding results for a set of inhibitors of human beta-secretase 1 (BACE-1). If the given molecule can inhibit BACE-1, indicate via "Yes". Otherwise, response via "No".}

\noindent(6) SIDER

\noindent{\underline{Task Definition}}:
\textit{Definition: Molecule property prediction task (a binary classification task) for the SIDER dataset. The Side Effect Resource (SIDER) is a dataset of marketed drugs and adverse drug reactions (ADR). If the given molecule can cause the side effect of \texttt{side effect}, indicate via "Yes". Otherwise, response via "No".} where \texttt{side effect} refers to the corresponding side effects for each subtask.

\subsubsection{Protein Property Prediction}
\label{sec:protein_property_detail}
\noindent{\textbf{Datasets}}

\noindent(1) Solubility prediction is to predict whether a protein is soluble or not. We follow the same splitting method with DeepSol~\citep{DBLP:journals/bioinformatics/KhuranaRKCBM18}.

\noindent(2) Localization prediction aims predict whether a protein is ``membrane-bound'' or ``soluble'', which is a simple version of subcellular localization prediction task. We follow the same splitting method with DeepLoc~\citep{DBLP:journals/bioinformatics/ArmenterosSSNW17}.


\noindent{\textbf{Baselines}}

\noindent(1) Feature engineers. The DDE (Dipeptide Deviation from Expected Mean)~\citep{saravanan2015harnessing} feature descriptor, consisting of 400 dimensions, is based on the dipeptide frequency within a protein sequence.
The Moran feature descriptor (Moran correlation)~\citep{feng2000prediction}, with 240 dimensions, characterizes the distribution of amino acid properties within a protein sequence. 

\noindent(2) Protein sequence encoders, including LSTM~\citep{hochreiter1997long}, Transformers~\citep{vaswani2017attention}, CNN~\citep{o2015introduction} and ResNet~\citep{he2016deep}.
The amino acid features in the last layer are aggregated for final prediction.

\noindent(3) Pre-trained protein language models.
ProtBert~\citep{elnaggar2021prottrans} and ESM-1b~\citep{rives2021biological} are both pre-trained on a massive dataset of protein sequences using the masked language modeling (MLM) objective. 
Specifically, ProtBert is pre-trained on  billion protein sequences obtained from the BFD database~\citep{steinegger2018clustering}, while ESM-1b is pre-trained on a smaller dataset of  million protein sequences sourced from UniRef50~\citep{suzek2007uniref}.

\noindent{\textbf{Prompts}}

\noindent(1) Solubility prediction

\noindent{\underline{Task Definition}}:
\textit{Protein solubility prediction task (a binary classification task) for the solubility dataset. If the given protein is soluble, indicate via "Yes". Otherwise, response via "No".}

\noindent{\underline{Instruction}}
\textit{Now complete the following example - Input: Protein: \bop{}\fasta{}\eop{} Output:}.

\noindent{\underline{Output}}:
\textit{Yes} for soluble protein or \textit{No} instead.

\noindent(2) Localization prediction

\noindent{\underline{Task Definition}}:
\textit{Protein subcellular localization task (a binary classification task). If the given protein is membrane-bound, indicate via "Yes". Otherwise (the protein is soluble), response via "No".}

\noindent{\underline{Instruction}}
\textit{Now complete the following example - Input: Protein: \bop{}\fasta{}\eop{} Output:}.

\noindent{\underline{Output}}:
\textit{Yes} for membrane-bound protein or \textit{No} for soluble protein.

\subsection{Multi-instance Prediction}
\subsubsection{Drug-target Interaction Prediction}
\noindent{\textbf{Datasets}}

\noindent(1) BioSNAP~\citep{zitnik2018biosnap} is derived from the DrugBank database~\citep{wishart2018drugbank} and was created by~\citet{Huang2021MolTransMI} and~\citet{zitnik2018biosnap}. It consists of 4,510 drugs and 2,181 proteins. This dataset is balanced, containing both validated positive interactions and an equal number of randomly selected negative samples from unseen pairs.

\noindent(2) BindingDB~\citep{liu2007bindingdb} is an accessible online database that contains experimentally validated binding affinities. Its main focus is on the interactions between small drug-like molecules and proteins. We follow~\citet{bai2023interpretable} to use a modified version of the BindingDB dataset, which was previously constructed by~\citet{bai2021hierarchical} with reduced bias. 

\noindent(3) Human~\cite{liu2015improving,chen2020transformercpi} is constructed with the inclusion of highly credible negative samples. Following~\citet{bai2023interpretable}, we also use a balanced version of the Human dataset, which contains an equal number of positive and negative samples. 

\noindent{\textbf{Baselines}}

\noindent We compare the performance of \method{} with the following six models on DTI task.

\noindent(1) Support Vector Machine~\citep{cortes1995support} (SVM) on the concatenated fingerprint ECFP4~\citep{DBLP:journals/jcisd/RogersH10} (extended connectivity fingerprint, up to four bonds) and PSC~\citep{DBLP:journals/bioinformatics/CaoXL13} (pseudo-amino acid composition) features.

\noindent(2) Random Forest~\citep{ho1995random} (RF) on the concatenated fingerprint ECFP4 and PSC features.

\noindent(3) DeepConv-DTI~\citep{Lee2019DeepConvDTIPO} uses a fully connected neural network to encode the ECFP4 drug fingerprint and a Convolutional Neural Network (CNN) along with a global max-pooling layer to extract features from protein sequences. Then the drug and protein features are concatenated and fed into a fully connected neural network for final prediction.

\noindent(4) GraphDTA~\citep{Nguyen2020GraphDTAPD} uses graph neural networks (GNNs) for the encoding of drug molecular graphs, and a CNN is used for the encoding of protein sequences. The derived vectors of the drug and protein representation are concatenated for interaction prediction.

\noindent(5) MolTrans~\citep{Huang2021MolTransMI} uses transformer architecture to encode drug and protein. Then a CNN-based interaction module is used to capture their interactions.

\noindent(6) DrugBAN~\citep{bai2023interpretable} use Graph Convolution Network (GCN)~\citep{DBLP:conf/iclr/KipfW17} and 1D CNN to encode drug and protein sequences. Then a bilinear attention network are adopted to learn pairwise local interactions between drug and protein. The resulting joint representation is decoded by a fully connected neural network.

\noindent{\textbf{Prompts}}

\noindent{\underline{Task Definition}}:
\textit{Definition: Drug target interaction prediction task (a binary classification task) for the \dataset{} dataset. If the given molecule and protein can interact with each other, indicate via "Yes". Otherwise, response via "No".} where \dataset{} is one of the three DTI datasets mentioned above.

\noindent{\underline{Instruction}}:
\textit{Now complete the following example - Input: Molecule: \bom{}\selfies{}\eom{} Protein: \bop{}\fasta{}\eop{} Output:}.

\noindent{\underline{Output}}:
\textit{Yes} for positive label or \textit{No} instead.

\subsubsection{Protein-protein Interaction Prediction}
\noindent{\textbf{Datasets}}

\noindent(1) Yeast~\citep{guo2008using} involves determining whether two yeast proteins interact or not. The negative pairs are derived from distinct subcellular locations. Following~\citep{xu2022peer}, the dataset is split and removed redundancy according to protein sequences similarity, which allows for the evaluation of generalization across dissimilar protein sequences.

\noindent(2) Human~\citep{pan2010large} involves determining whether two human proteins interact or not. It comprises positive protein pairs sourced from the Human Protein Reference Database (HPRD)~\citep{peri2003development} and negative pairs derived from different subcellular locations. The dataset splitting scheme is similar to that of Yeast PPI prediction with an  ratio for train/validation/test. 

\noindent{\textbf{Baselines}}
The compared baselines are the same as the protein property prediction task in Section~\ref{sec:protein_property_detail}.

\noindent{\textbf{Prompts}}

\noindent{\underline{Task Definition}}:
\textit{Protein protein interaction prediction task (a binary classification task) for the \dataset{} dataset. If the given two yeast proteins (Protein\_A and Protein\_B) can interact with each other, indicate via "Yes". Otherwise, response via "No".} where \dataset{} is either \textit{yeast} or \textit{human}.

\noindent{\underline{Instruction}}:
\textit{Now complete the following example - Input: Protein\_A: \bop{}\fasta{}\eop{} Protein\_B: \bop{}\fasta{}\eop{} Output:}.

\noindent{\underline{Output}}:
\textit{Yes} for positive label or \textit{No} instead.


\subsection{Cross-modal Generation}
\subsubsection{Molecule Captioning}
\label{sec:mol2text_detail}
\noindent{\textbf{Datasets}}

\noindent We use ChEBI-20 dataset created by Text2mol~\citep{DBLP:conf/emnlp/EdwardsZJ21}, which consists of  molecule-text pairs and  means each text description has more than  words.
The dataset is split into  for train, validation, and test.

\noindent{\textbf{Baselines}}

\noindent(1) RNN~\citep{medsker2001recurrent} with -layer bidirectional encoder is trained from scratch on ChEBI-20 dataset.

\noindent(2) Transformer~\citep{vaswani2017attention} containing  encoder and decoder layers is trained from scratch on ChEBI-20 dataset.

\noindent(3) T5~\citep{raffel2020exploring} is directly fine-tuned on ChEBI-20 dataset from public checkpoints~\footnote{\url{https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md\#t511}} with three different model sizes: small, base and large. Note that no molecule domain knowledge is introduced in the original T5 pre-training.

\noindent(4) MolT5~\citep{DBLP:conf/emnlp/EdwardsLRHCJ22} is jointly trained on molecule SMILES from ZINC-15 dataset~\citep{DBLP:journals/jcisd/SterlingI15} and general text from C4 dataset~\citep{raffel2020exploring} so that MolT5 has prior knowledge about these two domains. It also contains three different sizes: small, base and large. Then they are further fine-tuned on ChEBI-20 dataset.

\noindent(5) GPT-3.5-turbo~\citep{li2023empowering} is used by directly call OpenAI API without further fine-tuning. The input includes five parts as~\citet{li2023empowering}: role identification, task description, examples, output instruction, and user input prompt. The examples are retrieved by Morgan Fingerprint~\citep{DBLP:journals/jcisd/Butina99} similarity for molecule captioning task and by BM25~\citep{DBLP:journals/ftir/RobertsonZ09} for text-based molecule generation task.

\noindent(6) MolXPT~\cite{liu2023molxpt} is jointly trained on molecule SMILES from PubChem~\citep{kim2023pubchem}, biomedical text from PubMed~\citep{canese2013pubmed}, and ``wrapped'' text in which molecule names are replaced with corresponding SMILES.

\noindent{\textbf{Prompts}}

\noindent Different from the classification task in which the ground truth output is either \textit{Yes} or \textit{No}, the output for molecule captioning task is text sequence.

\noindent{\underline{Task Definition}}:
\textit{Definition: You are given a molecule SELFIES. Your job is to generate the molecule description in English that fits the molecule SELFIES.}

\noindent{\underline{Instruction}}:
\textit{Now complete the following example - Input: <bom>\selfies{}<eom> Output:}.

\noindent{\underline{Output}}:
\textit{\text{}}

\subsubsection{Text-based molecule generation}
This is the reverse task of molecule captioning.
The input is the text description of the desired molecule and the output is the corresponding molecule SELFIES.
The datasets and compared baselines are the same with molecule captioning in Section~\ref{sec:mol2text_detail} so will only provide the prompts here.

\noindent{\textbf{Prompts}}

\noindent{\underline{Task Definition}}:
\textit{Definition: You are given a molecule description in English. Your job is to generate the molecule SELFIES that fits the description.}

\noindent{\underline{Instruction}}:
\textit{Now complete the following example - Input: \text{} Output:}.

\noindent{\underline{Output}}:
\textit{<bom>\selfies{}<eom>}

\section{Case Study}
\label{sec:case_study}
In this section, we show several example outputs from different models in molecule captioning and text-based molecule generation tasks.
Figure~\ref{fig:case_mol2text} shows the cases for the molecule captioning task.
In example (1), the description of \method{} matches the ground truth best, successfully localizing the position of the substituent group and ``member of pyridines and an aryl thiol''.
In example (2), MolT5 mistakenly describes that the molecule contained boron, while \method's description is more accurate.
In example (3), while MolT5 generates repetitive output, \method{} and T5 generate semantically coherent output, and \method{}'s output matches better with ground truth.
For a complex molecule in example (4), the output of \method{} is more holistic and accurate.
Notably, only \method{} describes this molecule as an inhibitor of SARS coronavirus main proteinase, which may come from our integration with protein knowledge.
Figure~\ref{fig:case_text2mol} shows the cases for the text-based molecule generation task.
From the cases, we have several findings:
(i) \method{} is more likely to generate molecules that exactly match the ground truth.
(ii) By using SELFIES, \method{} will not generate invalid molecules, especially for complex and longer molecules shown in examples (3) and (4).
(iii) Some molecules are actually short proteins. Example (3) shows a molecule that is a 33-membered polypeptide, which consists of  amino acid residues joined in sequence. Therefore, the boundary between proteins and molecules may not always be distinct, and leveraging information from both can provide reciprocal benefits.

\begin{figure*}[t]
    \centering
\includegraphics[width=\linewidth]{case_mol2text.pdf}
    \caption{Molecule captioning cases.}
    \label{fig:case_mol2text}
\end{figure*}

\begin{figure*}[t]
    \centering
\includegraphics[width=\linewidth]{case_text2mol.pdf}
    \caption{Text-based molecule generation cases.}
    \label{fig:case_text2mol}
\end{figure*}

\end{document}
