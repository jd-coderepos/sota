
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{mathabx}
\allowdisplaybreaks[4]

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

\title{Breaking the Expressive Bottlenecks \\ of Graph Neural Networks}



\iffalse
\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}
\fi

\author{Mingqi Yang, Yanming Shen, Heng Qi, Baocai Yin \\
Dalian University of Technology \\
\texttt{yangmq@mail.dlut.edu.cn, \{shen, hengqi, ybc\}@dlut.edu.cn}\\
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}



\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to measure the expressiveness of graph neural networks (GNNs), showing that the neighborhood aggregation GNNs were at most as powerful as 1-WL test in distinguishing graph structures. There were also improvements proposed in analogy to -WL test (). However, the aggregators in these GNNs are far from injective as required by the WL test, and suffer from weak distinguishing strength, making it become expressive bottlenecks. In this paper, we improve the expressiveness by exploring powerful aggregators. We reformulate aggregation with the corresponding aggregation coefficient matrix, and then systematically analyze the requirements of the aggregation coefficient matrix for building more powerful aggregators and even injective aggregators. It can also be viewed as the strategy for preserving the rank of hidden features, and implies that basic aggregators correspond to a special case of low-rank transformations. We also show the necessity of applying nonlinear units ahead of aggregation, which is different from most aggregation-based GNNs. Based on our theoretical analysis, we develop two GNN layers, ExpandingConv and CombConv. Experimental results show that our models significantly boost performance, especially for large and densely connected graphs.
\end{abstract}

\iffalse
Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to measure the expressiveness of graph neural networks (GNNs), showing that the neighborhood aggregation GNNs were at most as powerful as 1-WL test in distinguishing graph structures. There were also improvements proposed in analogy to -WL test (). However, the aggregators in these GNNs are far from injective as required by the WL test, and suffer from weak distinguishing strength, making it become expressive bottlenecks. In this paper, we improve the expressiveness by exploring powerful aggregators. We reformulate aggregation with the corresponding aggregation coefficient matrix, and then systematically analyze the requirements of the aggregation coefficient matrix for building more powerful aggregators and even injective aggregators. It can also be viewed as the strategy for preserving the rank of hidden features, and implies that basic aggregators correspond to a special case of low-rank transformations. We also show the necessity of applying nonlinear units ahead of aggregation, which is different from most aggregation-based GNNs. Based on our theoretical analysis, we develop two GNN layers, ExpandingConv and CombConv. Experimental results show that our models significantly boost performance, especially for large and densely connected graphs.
\fi

\section{Introduction}

Graphs are ubiquitous in the real world.
Social networks, traffic networks, knowledge graphs, and molecular structures are typical graph-structured data.
Graph Neural Networks (GNNs) \citep{scarselli2008graph,gori2005new}, leveraging the power of neural networks to graph-structured data, have a rapid development recently \citep{kipf2016semi,hamilton2017inductive,bronstein2017geometric,gilmer2017neural,duvenaud2015convolutional}.

Expressive power of GNNs measures their abilities to represent different graph structures \citep{sato2020survey}.
It decides the performance of GNNs where the awareness of graph structures is required,
especially on large graphs with complex topologies.
The neighborhood aggregation scheme (or message passing) follows the same pattern with weisfiler-lehman (WL) graph isomorphism test \citep{Weisfeiler1968ReductionOA} to encode graph structures,
where node representations are computed iteratively by aggregating transformed representations of its neighbors with structural information learned implicitly.
Therefore, the WL test is used to measure the expressiveness of GNNs.
Unfortunately, general GNNs are at most as powerful as 1-order WL test \citep{morris2019weisfeiler,xu2018how}.
There is also work trying to improve the expressiveness that are beyond 1-order WL test \citep{maron2019provably,morris2019weisfeiler,chen2019equivalence,li2020distance,vignac2020building}.
However, the weak distinguishing strength of aggregators is the fundamental limitation.
The expressiveness analysis measured by the WL test assumes that aggregators are injective, which is usually unreachable.
Therefore,
this motivates us to investigate the following questions:
What are the key factors to limit the expressiveness of GNN?
and how to break these limitations?

Aggregators are permutation invariant functions that operate on sets while preserving permutation invariance.
\citep{zaheer2017deep} first theoretically studied permutation invariant functions and provided a family of functions to which any permutation invariant function must belong.
\citep{xu2018how} extended it on multisets but only for countable space.
\citep{corso2020principal} further extended it to uncountable space.
\citep{murphy2018janossy} and \citep{murphy2019relational} expressed a permutation invariant function by approximating an average over permutation-sensitive functions with tractability strategies.
\citep{dehmamy2019understanding} showed that a single propagation rule applied in general GNNs is rather restrictive in learning graph moments \citep{lin1995algorithms}.
They and \citep{corso2020principal} improved the distinguishing strength of aggregation by leveraging multiple basic aggregators (SUM, MEAN, NORMALIZED MEAN, MAX/MIN, and STD).
This strategy showed its effectiveness on tasks taken from classical graph theory.

In contrast to existing studies towards aggregators in GNNs,
we provide a new GNN formulation,
where the aggregation is represented as the multiplication of the corresponding hidden feature matrix of neighbors and the aggregation coefficient matrix.
This new formulation enables us to answer the following questions:
(i) when a GNN will lose its expressive power;
(ii) How to build aggregators with higher distinguishing strength, even injective aggregators.
Based on our theoretical analysis,
we propose two GNN layers: ExpandingConv and CombConv,
and evaluate them on general graph classification and graph regression tasks.
Our key contributions are summarized as follows:
\begin{itemize}
\item We formalize the distinguishing strength of aggregators as a partial order,
and theoretically show that the choice of aggregators can be bottlenecks of expressiveness.
We also propose to apply nonlinear units ahead of aggregations to break the distinguishing strength limitations of aggregators as well as to achieve an implicit sampling mechanism.
\item We reformulate the neighborhood aggregation with the aggregation coefficient matrix and then provide a theoretical point of view on building powerful aggregators and even injective aggregators.
\item We propose ExpandingConv and CombConv layers
which achieve state-of-the-art performance on a variety of graph tasks.
We also show that multi-head GAT is one of the ExpandingConv implementations,
which brings a theoretical explanation for its effectiveness.
\end{itemize}

\section{Preliminaries}

\subsection{Notations}

For a graph ,
we denote the set of edges, nodes and node feature vectors respectively by ,  and .
 represents the set of neighbors of  including itself, i.e., .
We use  to denote the set .
 represents a multi-set, i.e., a set with possibly repeating elements.
 represents the set of all permutations of the integers 1 to .
, where , is a reordering of the elements of a sequence  according to .
Given a matrix ,
 represents the transpose of ,
and  represents the column stack of .

\subsection{Graph Neural Networks}

Most GNNs adopt the neighborhood aggregation scheme \citep{gilmer2017neural} to learn the node representations,
which utilizes both node features and graph structures.
In the -th layer,
the representation of node 


\textbf{Aggregators in GNNs.}
An aggregator is a permutation invariant function \citep{zaheer2017deep} with bounded size inputs.
It satisfies:
(i) size insensitive: an aggregator can take an arbitrary but finite size of inputs;
(ii) permutation invariant: an aggregator is invariant to the permutation of input.
There are a limited number of basic aggregators such as SUM, MEAN, NORMALIZED MEAN, MAX/MIN, STD, etc.
Most proposed GNNs apply one of these aggregators.
Sum-of-power mapping \citep{zaheer2017deep} and normalized moments \citep{corso2020principal} can also be used as aggregators and they allow for a variable number of aggregators.

\section{Proposed Model}
In this section, we first formalize the distinguishing strength of aggregators as a partial order,
and show why basic aggregators used in popular GNNs become bottlenecks of expressiveness.
Then, we analyze the requirements for building powerful aggregators and even injective aggregators.
Finally, we introduce two GNN layers based on our theoretical analysis.

\subsection{Distinguishing Strength of Aggregators}
\label{formal-aggregator}
To ensure generality, 
our analysis of aggregators is always considered in multisets and uncountable case,
where the inputs are continuous and with possibly repeating elements.
We first introduce distinguishing strength under the concept of partial order \citep{schmidt2011relational}.

\textbf{Distinguishing strength.}
The distinguishing strength of aggregator  is stronger than ,
denoted by ,
if and only if for any two multisets  and 
where the number of elements can be different,
.
Meanwhile, if there exist  and  such that  but ,
 is strictly stronger than , denoted by .
If ,
we say these two aggregators have the same distinguishing strength, denoted by .
If there exist multisets  and  such that , , and there also exist  and  such that , , we say  and  are incomparable.

Distinguishing strength is a partial order,
and the set of all aggregators form a poset.
In this poset, the aggregators with the greatest distinguishing strength should be injective.
With the definition of distinguishing strength, we can compare any two aggregators.
The distinguishing strength of widely used aggregators SUM, MEAN, MAX/MIN is incomparable.
One can easily give two multisets of elements that are distinguished by one aggregator but are not distinguished by the others as showed in \citep{corso2020principal}.

\textbf{Equivariant aggregator.}
 is an equivariant aggregator if and only if 
for any  and .

Widely used SUM and MEAN are equivariant aggregators but MAX/MIN is not.
We denote  a new aggregator by combing  and  with , where  denotes concatenation.
\begin{lemma}
(i) For any continuous function , we have , and when  is injective, ;
(ii)  and . If  and  are incomparable,  and ;
(iii) If  is an equivariant aggregator, then  for any  and .
\label{distinguish-gf}
\end{lemma}
We prove Lemma \ref{distinguish-gf} in Appendix \ref{proof-distinguish-gf}. 
Lemma \ref{distinguish-gf} indicates that aggregators become bottlenecks of distinguishing strength.
For the equivariant aggregator, any linear transformation before aggregation and any transformation after aggregation have no contribution to the distinguishing strength.
For SUM and MEAN,
we have 
 and
,
where , and  can be any continuous function.
Based on Lemma \ref{distinguish-gf},
we can now compare the distinguishing strength of aggregations in some popular GNNs.
GIN-0 sums all hidden features of neighbors at first, and then pass them to a 2-layer MLP.
Therefore,
when considering in a continuous input features space,
the distinguishing strength of GIN-0 is at most as powerful as the SUM aggregator.
GCN uses a NORMALIZED MEAN (denoted by MEAN) aggregator.
Given a node  and its neighbors,
.
MEAN is also an equivariant aggregator,
and the distinguishing strength of aggregation in GCN is at most as powerful as MEAN.
GAT corresponds to the weighted SUM aggregation,
where the weight coefficients are the functions of hidden features.
This makes the distinguishing strength of GAT and SUM incomparable.
Based on these observations,
a potential approach to breaking the distinguishing strength limitation is to apply a nonlinear processing on inputs before aggregation.


\subsection{Building Powerful Aggregators}
\label{nwf-analyze}

In this section,
we analyze the requirements for building more powerful aggregators and further injective aggregators.
We first introduce a new representation of GNN layers which unifies several popular GNN layers.
Given a node  and its neighbors ,
our new formulation represents the GNN operation as follows:

Here,  is the aggregation coefficients vector of node .
Note that  should be the mapping of local structures such as node degrees, node or edge features of the -hop neighbors assigned on node  to ensure the same encoding of isomorphic graphs.
 is the matrix representation of 's neighbors according to a permutation .
 is a neural network that extracts task-relevant information from the aggregated representation , and is used to update hidden feature  of node .

According to Equation \ref{three-stage-representation}, the aggregation should be with high distinguishing strength to avoid indistinguishability among neighbors.
Meanwhile, the extraction should be powerful enough to efficiently extract task-relevant structural patterns from the aggregated representation of neighbors.
Based on these observations, we reformulate GCN, GIN0 and GAT with their corresponding three-stage representations as follows:
\begin{small}

\end{small}
\begin{small}

\end{small}
Their default formulations are given in Appendix \ref{gcn-gat-gin}.
In the aggregation step,
GCN's  is the mapping of neighbors' degrees;
GIN0's  is the mapping of node 's degree which is equivalent to SUM aggregator;
GAT's  is the mapping of neighbors' features.
All of them are the mappings of local structures as given in Equation \ref{three-stage-representation}.


In this three-stage representation,
the aggregation is reformulated as the multiplication of the aggregation coefficients vector and the feature matrix of neighbors.
It provides insights on improving the distinguishing strength of aggregations.
First, we show how to characterize the permutation invariance in this formulation.
Let  denote an aggregation coefficient matrix where .
Note that in GCN, GIN and GAT,
 is restricted to be 1.
 is the matrix representation of  input elements according to .
The aggregation computation in the second step of Equation \ref{three-stage-representation} is
,
where  is the permutation matrix according to .
 ensures the same output for all , .
 is the reordering of columns of  according to .
For any ,
,
thus permutation invariance holds.
Once  is decided,
we obtain a unique aggregator denoted by .
For any sequence of input elements ,
,
where  can be any ordering of neighbors.
Next, we analyze the distinguishing strength of .
\begin{proposition}
For any two matrices  and  with ,
we have
(i)
,
where  means stacking these two matrices;
(ii)

if and only if
;
(iii)
Any multiset of size  is distinguishable with  if and only if .
\label{rank-single-aggregator}
\end{proposition}
We prove Proposition \ref{rank-single-aggregator} in Appendix \ref{proof-rank-single-aggregator}.
Proposition \ref{rank-single-aggregator} shows that the distinguishing strength of  is decided by the rank of the corresponding .
Yet, the distinguishing strength analysis in Proposition \ref{rank-single-aggregator} is only suitable for multisets aggregated with shared .
Next, we extend the analysis for the case of different aggregators.

Let  denote the set of all outputs of .
Our proposed three-stage representation also provides useful insight on the constraints among different aggregators.
That is,
in order to fully distinguish different local structures,
for any two different  and , .
This is because to fully distinguish different local structures, we should ensure their aggregated representations are different.
Since  is restricted to be the mapping of local structures (such as -hop neighbors),
different  means that the corresponding local structures are different. Therefore, the aggregation results of different  must be different.
However, it is not satisfied by existing GNNs,
and there are few studies on distinguishing multisets aggregated by different aggregators.
In Proposition \ref{rank-multi-aggregator},
we present a detailed analysis of it.
\begin{proposition}
\label{rank-multi-aggregator}
For any  and ,
(i) ;
(ii) If ,
then ;
\end{proposition}
We prove Proposition \ref{rank-multi-aggregator} in Appendix \ref{proof-rank-multi-aggregator}.
Proposition \ref{rank-multi-aggregator} shows the necessity of preserving the rank of aggregation coefficient matrix when considering the distinguishing strength among different aggregators.
Next, we provide a sufficient condition for building desired multiple injective aggregators with the outputs having no intersections.
\begin{proposition}
\label{injective-multi}
For any two aggregators  and  with  and ,
if ,
then  and  are injective and
.
\end{proposition}
We prove Proposition \ref{injective-multi} in Appendix \ref{proof-injective-multi}.
Proposition \ref{rank-single-aggregator}, \ref{rank-multi-aggregator} and \ref{injective-multi} provide a new perspective for building powerful aggregators and even injective aggregators.
Compared with the distinguishing strength studies in \citep{xu2018how} and \citep{corso2020principal},
as well as existing strategies for building injective aggregators,
e.g., sum-of-power mapping \citep{zaheer2017deep} and normalized moments \citep{corso2020principal},
we reformulate the aggregation with aggregation coefficients matrix and show the relations of the distinguishing strength of aggregators and the rank of the corresponding aggregation coefficients matrices.
Besides, the aggregation of this method is controlled by aggregation coefficients which can be learned from graph data to better leverage structural information.
In this paper,
to simplify the analysis,
we only consider the aggregations within one-hop neighbors.
The results can be easily extended to more sophisticated aggregators with the overall framework unchanged

In the perspective of preserving the rank of hidden features among neighbors,
 indicates that .
To preserve the rank of hidden features in aggregations such that ,
we need .
This builds a connection between improving the distinguishing strength of aggregators and preserving the rank of hidden features among neighbors,
both of which have the requirements on the rank of .
General aggregators such as ones in GCN, GIN-0 and GAT have .
Thus,  is always fixed to 1 no matter what the rank of the input features is.
Correspondingly, they have a weak distinguishing strength.

Equation \ref{three-stage-representation} splits the aggregation and feature/structure extraction into two independent steps,
which helps to figure out that the expressive power loss happens in the aggregation step,
and then the model extracts feature/structure information on the distorted encodings of neighbors.
From Equation \ref{three-stage-representation},
the aggregation can be considered as a representation regularization step,
which unifies different multisets of neighbors into the same representation style while holding permutation invariance.
Then, the model can extract structural information on this regulated data with a shared trainable matrix as the third step in Equation \ref{three-stage-representation}.
Based on this observation, we propose two novel GNN layers: ExpandingConv and CombConv.


\subsection{ExpandingConv}

In this section, we first present ExpandingConv framework.
Then we provide one of its implementations and analyze
how ExpandingConv achieves more powerful aggregations.
The ExpandingConv framework is

where  with  and
 is the mapping of local structures between nodes  and .
The implementation of  is very flexible with the only restriction of ensuring the same encoding of isomorphic graphs.
 is the expanded representation of hidden features .
Then a GNN layer  learns structural information on this expanded representations.
We introduce an implelentation as follows:

In Equation \ref{ExpC}, we implement  as the function of hidden features of nodes  and .
There can be other implementations,
and we leave them for future work.
 and  are trainable matrices.
\citep{luan2019break} empirically showed that different nonlinear activatoin functions have different contributions in preserving the rank of matrices.
We use the recommended Tanh as the activation function in the computation of  to better preserve the rank of aggregation coefficient matrices.
MLP denotes a 2-layer perceptron.

Next, we represent Equation \ref{ExpC} with the corresponding three-stage representation as given in Section \ref{nwf-analyze}
to obtain its aggregation coefficient matrix and analyze its distinguishing strength.
To simplify this process, we only consider 1-layer MLP with  and .
\begin{small}

\end{small}
where  are sampled subsets of neighbors in each dimension.

and

are aggregation coefficients matrix and hidden feature matrix corresponding to the subset of neighbors  according to .
We denote , then
.
According to Equation \ref{ExpC-h}, we finally obtain the three-stage representation equivalent to Equation \ref{ExpC}.
\begin{small}

\end{small}
According to the computation of , .
By configuring a larger ,
we have  with a high probability, which is different from general GNNs with a rank of 1.
As analyzed in Section \ref{nwf-analyze},
this achieves more powerful aggregators as well as preserves the rank of hidden features among neighbors.
The obtained  after aggregation is the unified representations of neighbors.
We then use the trainable matrix  to extracts feature/structure information.
Unlike the aggregation step, the dimensions reduction here (from  to ) would not cause information loss.
This can be explained by the fact that only task-relevant structural information needs to be preserved and passed to the next layer,
and it can be embedded in lower dimensions.

\textbf{Comparisons with multi-head GAT.}
\begin{proposition}
Multi-head GAT is an implementation of ExpandingConv as follows:

where  is the concatenation of the trainable matrix in all  heads.
\label{multi-head-gat}
\end{proposition}
We prove Proposition \ref{multi-head-gat} in Appendix \ref{proof-multi-head-gat}.
Although multi-head GAT is based on attention mechanism, ExpandingConv provides a new perspective to explain its effectiveness.
Applying multi-head attention mechanism helps to preserve the rank of hidden features as well as achieve more powerful aggregators.
However, the usage of LeakyReLU may be harmful to preserving the rank of the aggregation coefficient matrix \citep{luan2019break}.

GAT as well as most other GNNs (such as GCN, GIN, etc) follows the same pattern that applies nonlinear units after aggregation.
According to the analysis in Section \ref{formal-aggregator},
Equation \ref{ExpC} applies MLP on  before SUM to break the distinguishing strength limitation of SUM.
It also produces other interesting results.
By reformulating Equation \ref{ExpC} with its three-stage representation as Equation \ref{ExpC-nwf},
each dimension of hidden features aggregates on a subset of neighbors independently,
which corresponds to a kind of dimension-wise neighbor sampling mechanism.
We call the modification of applying ReLU ahead of SUM aggregator as -SUM mechanism.
\citep{mishra2020node} and \citep{rong2019dropedge} studied dropedge and node masking mechanism on node-level predictions,
both of which can be considered as neighbor sampling strategies
that have shown their effectiveness in improving the generalization ability of aggregation-based GNNs and are also used as unbiased data augmentation technique for training.
Compared with dropedge and node masking,
-SUM realizes a dimension-wise neighbor sampling,
and it does not need to manually set the sampling ratio since this mechanism takes effects implicitly.
-SUM shows that the neural network itself can perform sampling by properly combining nonlinear units and aggregators,
without explicitly modifying the network architecture.
Our experimental results verified the effectiveness of the -SUM on a variety of graph tasks.

\subsection{CombConv}
The CombConv framework is

where  and  denotes element-wise product.
An implementation of CombConv is given as follows:

where  and .
Similar to ExpandingConv, CombConv also applies -SUM aggregation.
The difference is that each dimension of hidden features is aggregated with an independent weighted aggregator. ExpandingConv with  corresponds to a special case of CombConv where all dimensions share the same aggregator.
Therefore, the distinguishing strength of CombConv is stronger than ExpandingConv with .
Meanwhile, CombConv does not expand the hidden features of nodes in aggregation.
Hence, it requires fewer parameters.


\section{Experiments}

In this section,
we evaluate ExpandingConv and CombConv on graph-level prediction tasks on OGB \citep{hu2020ogb}, TU \citep{KKMMN2016,yanardag2015deep} and QM9 \citep{ramakrishnan2014quantum,wu2018moleculenet,ruddigkeit2012enumeration}.
The code is available at \url{https://github.com/qslim/epcb-gnns}.


\textbf{Configurations.}
We use the default dataset splits for OGB.
The QM9 dataset is randomly split into 80\% train, 10\% validation and 10\% test as given in \citep{morris2019weisfeiler,maron2019provably}.
For TU dataset, we follow the standard 10-fold cross validation protocol and splits from \citep{zhang2018end} and report our results following the protocol described in \citep{xu2018how,ying2018hierarchical}.
\iffalse
The hyperparameters include:
number of layers ;
hidden units ;
batch size ;
learning rate ;
learning rate decay  every  epochs;
readout .
\fi
We use the concatenation of hidden features from all layers to compute the entire graph representations \citep{xu2018representation}.
In our tests,
all models are equipped with batch normalization \citep{ioffe2015batch} on each hidden layer when evaluating on OGB and TU,
and are not when evaluating on QM9.
All datasets' descriptions and detailed hyperparameter settings are given in Appendix \ref{experiments-details}.

We first conduct comprehensive ablation studies to evaluate the effectiveness of powerful aggregators and -SUM mechanism on OGB and QM9 as given in Table \ref{ogbg-ablation} and Table \ref{qm9-ablation}.
Then, we compare the performance of ExpandingConv and CombConv with competitive baselines on all three datasets as given in Table \ref{ogbg-baselines} and Table \ref{qm9-baselines} to show their improvements.
ExpC- denotes ExpandingConv with .
We use ExpC* and CombC* to denote the ExpandingConv and CombConv without -SUM.
\begin{table}[h]
\centering
\caption{Ablation studies on OGB and QM9. Higher is better.}\smallskip
\resizebox{0.55\textwidth}{!}{ 
\scriptsize
\begin{tabular}{lllll}
\toprule
                & {ogbg-ppa}                  & {ogbg-molhiv}              & {ogbg-molpcba}             & {ogbg-code}          \\
\midrule
ExpC*-1         & 70.65                      & 77.63                     & 22.65                     & 32.2                \\
ExpC-1          & 77.50                       & 76.79                    & 23.39                     & 32.6                \\
ExpC-3,4,5      &            &           & 23.44                     &      \\
\midrule
CombC*          & 73.61                      & 76.47                     & 23.45                     & 32.29               \\
CombC           & 77.64                      & 76.63                     &           & 32.72               \\
\bottomrule
\end{tabular}
}
\label{ogbg-ablation}
\end{table}
\begin{table}[h]
\centering
\caption{Ablation studies on QM9. Lower is better.}\smallskip
\resizebox{0.95\textwidth}{!}{ 
\scriptsize
\begin{tabular}{lllllllllllll}
\toprule
         &              &           &   &  &  &  &               &              &                &                &                &            \\
\midrule
ExpC*-1   & 0.467   & 0.283   & 0.00337   & 0.00340   & 0.00467   & 22.9   & 0.000205   & 0.0255   & 0.0263   & 0.0242   & 0.0261   & 0.1189 \\
ExpC-1    & 0.469   & 0.268   & 0.00326   & 0.00329   & 0.00466   & 20.8   & 0.000186   & 0.0202   & 0.0199   & 0.0202   & 0.0201   & 0.1039 \\
ExpC-4    & 0.413   & 0.255   & 0.00273   & 0.00300   & 0.00420   & 19.4   &    & 0.0184   & 0.0183   & 0.0178   & 0.0182   & 0.1115 \\
ExpC-8    & 0.400   & 0.257   & 0.00259   & 0.00286   & 0.00395   & 18.1   & 0.000172   & 0.0158   & 0.0170   & 0.0177   & 0.0184   & 0.1060 \\
ExpC-16   &    & 0.255   &     &    &    & 17.2   & 0.000170   & 0.0170   & 0.0174   & 0.0193   &    & 0.1043 \\
ExpC-32   &    &    &    &    &    &    & 0.000174   &    &    &    & 0.0198      &  \\
\midrule
CombC*    & 0.4062  & 0.248   & 0.00259   & 0.00273   & 0.00387   & 17.1   & 0.000170   & 0.0185   & 0.0181   & 0.0164   & 0.0174   & 0.1022 \\
CombC   & 0.399   &    & 0.00261   & 0.00278   & 0.00386   &    &    &    &    &    &    &  \\
\bottomrule
\end{tabular}
}
\label{qm9-ablation}
\end{table}

\subsection{Ablation Studies}
\textbf{Effect of powerful aggregators.}
For complex graph structures with dense connections or with abundant node/edge features,
they would benefit from a higher expressive model to maximumly distinguish different structures and extract relevant structural patterns as the model goes deeper to leverage large receptive fields.
This is validated on both QM9 and OGB.
We configure  of ExpC- for all 12 targets of QM9.
As we apply a larger ,
the model continuously achieves better performance on most targets.
We randomly select  for ogbg-ppa, ogbg-molhiv and  for ogbg-code.
The results show that applying larger  gains performance improvements,
especially on ogbg-ppa which involves large graphs with dense connections.


\textbf{Effect of -SUM mechanism.}
In Table \ref{ogbg-ablation} and Table \ref{qm9-ablation},
the performance differences between ExpC*-1 (CombC*) and ExpC-1 (CombC) show the effectiveness of -SUM.
In our tests,
the -SUM can be extremely powerful on graphs with dense connections such as ogbg-ppa,
which is validated on both ExpandingConv (with 6.85\% improvements) and CombConv (with 4\% improvements).
On most targets of QM9,
this mechanism also gains improvements.
For small graphs with sparse connections such as ogbg-hiv and ogbg-molpcba, the improvements are not very significant.
\begin{table}[h]
\centering
\caption{Comparisons with baselines on OGB and TU. Higher is better.
}\smallskip
\resizebox{0.99\textwidth}{!}{ 
\scriptsize
\begin{tabular}{lllll|lll}
\toprule
                                           & \multicolumn{4}{c}{OGB}                                                                                             & \multicolumn{3}{c}{TU} \\
                                           & {ogbg-ppa}                  & {ogbg-molhiv}              & {ogbg-molpcba}            & {ogbg-code}                & {COLLAB}             & {RDT-B}         & {RDT-M12} \\
\midrule
DGK          \citep{yanardag2015deep}      & NA                          & NA                         & NA                        & NA                         & 73.09  0.25 & 78.04  0.39 & 32.22  0.1 \\
PSCN    \citep{niepert2016learning}        & NA                          & NA                         & NA                        & NA                         & 73.76  0.50 & 86.30  1.58 & 41.32  0.42 \\
AWE     \citep{pmlr-v80-ivanov18a}         & NA                          & NA                         & NA                        & NA                         & 73.93  1.94 & 87.89  2.53 & 39.20  2.09 \\
GCN              \citep{kipf2016semi}      & 68.39  0.84         & 76.06  0.97        & 20.20  0.24       & 31.63  0.18        & NA                   & NA    & NA \\
GIN                 \citep{xu2018how}      & 68.92  1.0          & 75.58  1.40        & 22.66  0.28       & 31.63  0.20        & 80.2  1.9    & 92.4  2.5    & NA \\
GraphSAG\citep{hamilton2017inductive}      & NA                          & NA                         & NA                        & NA                         & 68.25               & NA    & 42.24  \\
DiffPool \citep{ying2018hierarchical}      & NA                          & NA                         & NA                        & NA                         & 75.48               & NA    & 47.08 \\
CapsGNN      \citep{xinyi2018capsule}      & NA                          & NA                         & NA                        & NA                         & 79.62  0.91  & NA    & 46.62  1.9 \\
PPGN        \citep{maron2019provably}      & NA                          & NA                         & NA                        & NA                         & 80.16  1.1   & NA    & NA  \\
DeeperGCN     \citep{li2020deepergcn}      & 77.12  0.71         & 78.58  1.17        & NA                        & NA                         & NA                   & NA    & NA  \\
HIMP            \citep{Fey/etal/2020}      & NA                          &         & NA        & NA                         & NA                   & NA    & NA   \\
WEGL   \citep{kolouri2020wasserstein}      & NA                          & 77.57  1.11        & NA                        & NA                         & NA                   & NA    & NA  \\
multi-head GAT\citep{velivckovic2017graph} & NA                          & 75.81                     & 20.10                    & 31.10                     & NA                   & NA    & NA \\
\midrule
ExpC-                        &     & 77.99  0.82        & 23.42  0.29       &     &  & 92.2  1.87 & \\
CombC                                     & 77.81  0.76          & 77.15  1.32        &    & 32.76  0.15        & 81.90  1.75 &  & 49.02  1.21 \\
\bottomrule
\end{tabular}
}
\label{ogbg-baselines}
\end{table}
\begin{table}[h]
\centering
\caption{Comparisons with baselines on QM9. Lower is better.
}\smallskip
\resizebox{0.99\textwidth}{!}{ 
\scriptsize
\begin{tabular}{lllllllllllll}
\toprule
         &              &           &   &  &  &  &               &              &                &                &                &            \\
\midrule
DTNN \citep{wu2018moleculenet}    &   & 0.95              & 0.00388            & 0.00512           & 0.0112            & 17         & 0.00172             & 2.43              & 2.43              & 2.43              & 2.43              & 0.27              \\
MPNN \citep{gilmer2017neural}    & 0.358             & 0.89              & 0.00541            & 0.00623           & 0.0066            & 28.5                  & 0.00216             & 2.05              & 2                 & 2.02              & 2.02              & 0.42              \\
k-GNN \citep{morris2019weisfeiler}    & 0.476             & 0.27              & 0.00337            & 0.00351           & 0.0048            & 22.9                  & 0.00019             & 0.0427 & 0.111             & 0.0419
& 0.0469            &  \\
PPGN \citep{maron2019provably}    &  & 0.318             &  &  &  &        & 0.000399            & 0.022  & 0.0504            & 0.0294            & 0.024             & 0.144             \\
GIN0* \citep{xu2018how}    & 0.471   & 0.281   & 0.00327   & 0.00340   & 0.00473   & 22.9   & 0.000202   & 0.0244   & 0.0245   & 0.0233   & 0.0255   & 0.1283 \\
GAT-\citep{velivckovic2017graph}     & 0.452   & 0.286   & 0.00322   & 0.00327   & 0.00460   & 22.7   & 0.000228  & 0.0212    & 0.0223   & 0.0223   & 0.0219   & 0.1247 \\
\midrule
ExpC-   & 0.368   &    &    &    &    & 16.3   &    &    &    &    &       & 0.0962 \\
CombC   & 0.399   &    & 0.00261   & 0.00278   & 0.00386   &    &    &    &    &    &    &  \\
\bottomrule
\end{tabular}
}
\label{qm9-baselines}
\end{table}
\subsection{Comparisons with Baselines}
Table \ref{ogbg-baselines} and Table \ref{qm9-baselines} show the performance comparisons of our models with baselines on QM9, TU and OGB respectively.
All datasets in QM9 and OGB graph-level predictions are used for evaluations.
For TU, we use 3 widely used datasets:
COLLAB includes graphs with dense connections;
REDDIT-BINARY (RDT-B) and REDDIT-MULTI-12K (RDT-M12) include large and sparse graphs with one center node having dense connections with other nodes.
All results of baselines are taken from the original papers except for the results of GraphSAGE on TU, multi-head GAT on OGB and GIN0* on QM9 which were not reported by the original papers.
We report the results of GraphSAGE provided by \citep{ying2018hierarchical} and evaluate multi-head GAT and GIN0* by ourselves.
To ensure a fair comparison,
for OGB and TU, we configure the number of heads in multi-head GAT and  in ExpC- to be the same which is selected in .
For QM9, the number of heads is 8 and .
GIN0* in QM9 denotes GIN0 without batch normalization.


Compared with baselines, our models achieve the best performance on 7 out of all 12 targets of QM9, 3 out of all 4 graph-level prediction datasets of OGB and all 3 selected TU datasets.
Our models get 1.9\% improvements on COLLAB and 2.83\% improvements on REDDIT-MULTI-12K compared with SOTA baselines.
On ogbg-ppa, our models achieve 2.6\% higher classification accuracies compared with SOTA baselines.
On ogbg-code, they achieve 1.5\% improvements.
Multi-head GAT can also be considered as an implementation of ExpandingConv.
However, its performance on graph-level predictions is not competitive.
According to its three-stage representation,
the usage of LeakyReLU in the aggregation step is harmful to preserving the rank, and the usage of softmax makes it harder to analyze the rank.
In the extraction step, the 1-layer MLP may have a limited representation power to represent the desired extraction functions.



\section{Conclusion}
We show how basic aggregators used in general GNNs become expressive bottlenecks.
To address this limitation,
we develop theoretical foundations of building powerful aggregators.
We also propose the -SUM mechanism which achieves dimension-wise sampling.
To evaluate their effectiveness,
we develop two novel GNN layers,
and conduct extensive experiments on public graph benchmarks.
The results are consistent with our analysis,
and our proposed models achieve SOTA performance on a variety of graph-level prediction benchmarks.



\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}


\clearpage
\appendix

\section{GCN, GAT and GIN}
\label{gcn-gat-gin}

Here, we present implementations of GCN, GAT and GIN for the usage of our analysis.

\textbf{Graph Convolution Networks (GCN) \citep{kipf2016semi}.}


\textbf{Graph Attention Networks (GAT) \citep{velivckovic2017graph}.}



\textbf{Graph Isomorphism Networks (GIN-0) \citep{xu2018how}.}



\section{Proof of Lemma \ref{distinguish-gf}}
\label{proof-distinguish-gf}

\begin{proof}
(i)
For any two multisets  and ,
if ,
then . 
Therefore,
we have .
If  is injective, then .
We have ,
therefore .

(ii)
For any two multisets  and , .
Therefore, .
If  and  are incomparable, there exist  and  such that  but .
Therefore, there exist  and  such that  but .
.

(iii)
Since  is an equivariant aggregator,
then .
\end{proof}

\section{Proof of Proposition \ref{rank-single-aggregator}}
\label{proof-rank-single-aggregator}

\begin{proof}
(i)

then for any  and , we have 

and therefore we conclude that .

(ii)
``''

We prove the claim by contradiction.
Assume that  and .

means that for any  and ,
,
where  is the ordering of input elements.
Let .
For any ,
.
Then for any , .
The system of linear equations  and  share the same solution space.
Let  denote the rank of this solution space,
then .
Therefore, ,
then we have .
Since we assumed that ,
we reach a contradiction.

``''

We prove an equivalent proposition ``''.
Note that  and  as given in Proposition \ref{rank-single-aggregator}(i).
We only need to prove ``''.
 means that any row in  is linearly dependent to rows in .
Therefore, there exists  so that .
For any  and  with ,
,
and therefore
,
where  is the ordering of input elements.
That is,
for any  and ,
,
thus
.
Finally, we have .

(iii)
``Any multiset of size  is distinguishable with ''

Since ,
we prove an equivalent proposition `` there exists at least two multisets which are indistinguishable''.
Considering the system of linear equations  where ,
if ,
then there exists  such that .
According to the RouchCapelli theorem, there are infinite solutions  such that ,
Each  comes from a multiset with a particular order.
Next, we need to prove that all these  come from more than one multiset.
As a multiset with bounded size  constitutes at most  different orders, the infinite number of  corresponds to  must come from more than one multisets, making these multisets indistinguishable.

``Any multiset of size  is distinguishable with ''

Since  and , for any ,
 is unique.
Correspondingly,
for any ,
 is unique.

\end{proof}

\section{Proof of Proposition \ref{rank-multi-aggregator}}
\label{proof-rank-multi-aggregator}

\begin{proof}
(i)
According to the proof of Proposition \ref{rank-single-aggregator}(i),
.
For any  and  with ,
 holds.
Meanwhile,
,
and .
Therefore, for any ,
we have .
That is .

(ii)
We prove an equivalent proposition
``''.

means that any row in

is linearly dependent to
.
Therefore, there exists

so that
.
Correspondingly,
 and .
For any  and  with ,
,
and therefore
,
where  is the ordering of input elements.
Thus for any  and ,
.
Hence,
.
According to Proposition \ref{rank-multi-aggregator}(i), .
\end{proof}

\section{Proof of Proposition \ref{injective-multi}}
\label{proof-injective-multi}
\begin{proof}
Since ,  and ,
we have  and .
According to Proposition \ref{rank-single-aggregator},
 and  are injective.

We build the system of linear equations ,
where ,
and
.
Then, ,
which means  has no non-zero solutions.
Let  and 
such that .
For any ,

Therefore,
for any
,

and
,
,
hence  for any  and .
As a result,
.
\end{proof}


\section{Proof of Proposition \ref{multi-head-gat}}
\label{proof-multi-head-gat}

\begin{proof}
For Multi-head GAT, there are two types of implementations on aggregating each head,  and .
Here, we only consider the average aggregation implementation.

where
 is the trainable matrix for the -th head,
and  is the concatenation of the trainable matrix in all  heads;

Let 
and .
Then,


Therefore, multi-head GAT is an implementation of ExpandingConv as follows:

\end{proof}


\section{Comparisons with Multi-aggregator Implementations}
\label{mp-multi-aggregator}

ExpandingConv can also be considered as a kind of multi-aggregator scheme.
In Equation \ref{ExpC-nwf},
each row of  can be viewed as a weighted aggregator where the weight coefficients are learned from data.
Proposition \ref{rank-single-aggregator} shows that to obtain higher distinguishing strength by utilizing more aggregators,
the weight coefficients of newly added aggregators should be linearly independent to all existing aggregators.
The distinguishing strength of weighted aggregators is incomparable with basic aggregators.
However, since each row of  is equivalent to an independent aggregator,
one can simply modify the implementation of  to obtain the variant whose distinguishing strength is strict stronger than basic aggregators as follows:

Compared with lerveraging multiple basic aggregators in \citep{corso2020principal} and \citep{dehmamy2019understanding}, lerveraging weighted aggregator allows for variable numbers of aggregators.
Meanwhile, the weighted coefficients are learned from data, which can better capture relevant structural patterns.


\section{Details of Experimental Setup}
\label{experiments-details}

\textbf{Datasets.}
Benchmark datasets for graph kernels provided by TU \citep{KKMMN2016} suffer from their small scales of data,
making them not sufficient to evaluate the performance of models \citep{dwivedi2020benchmarking}.
Our evaluations are conducted on graph property predictions datasets ogbg-ppa, ogbg-code, ogbg-molhiv in OGB \citep{hu2020ogb} and QM9 \citep{ramakrishnan2014quantum,wu2018moleculenet,ruddigkeit2012enumeration} which are large-scale graph datasets including graph classification and graph regression tasks.
ogbg-ppa is extracted from the protein-protein association networks with large and densely connected graphs.
ogbg-code is a collection of Abstract Syntax Trees (ASTs) obtained from Python method definitions with large and sparse graphs.
ogbg-molhiv is molecular property prediction datasets with relative small graphs.
QM9 consists 134K small organic molecules with the task to predict 12 targets for each molecule.
All data is obtained from pytorch-geometric library \citep{Fey/Lenssen/2019}.

\begin{table}[h]
\centering
\caption{Hyperparameter settings for OGB.}\smallskip
\resizebox{0.99\textwidth}{!}{ 
\scriptsize
\begin{tabular}{lll|ll|ll|ll}
\toprule
              & \multicolumn{2}{c}{ogbg-ppa}                      & \multicolumn{2}{c}{ogbg-molhiv}              & \multicolumn{2}{c}{ogbg-molpcba}                  & \multicolumn{2}{c}{ogbg-code} \\
\midrule
              & ExpC*-1, ExpC-      & CombC*, CombC            & ExpC*-1, ExpC-     & CombC*, CombC        & ExpC*-1, ExpC-          & CombC*, CombC        & ExpC*-1, ExpC-        & CombC*, CombC	     \\
\midrule
batch size    & 32                     & 32                       & 64                    & 64                   & 128                        & 128                  & 64                       & 64                \\
layers        & 4                      & 4                        & 3                     & 3                    & 5                          & 5                    & 4                        & 4               \\
hidden        & 256                    & 256                      & 64                    & 64                   & 512                        & 512                  & 512                      & 512        \\
lr            & 0.0005                 & 0.0002                   & 0.0001                & 0.0001               & 0.0001                     & 0.0001               & 0.0001                   & 0.0001     \\
step size     & 20                     & 20                       & 5                     & 5                    & 10                         & 10                   & 5                        & 5          \\
lr decay      & 0.8                    & 0.7                      & 0.7                   & 0.7                  & 0.6                        & 0.6                  & 0.6                      & 0.6         \\
dropout       & 0.5                    & 0.5                      & 0.5                   & 0.5                  & 0.5                        & 0.5                  & 0.5                      & 0.5            \\
readout       & SUM                    & SUM                      & MEAN                  & MEAN                 & MEAN                       & MEAN                 & MEAN                     & MEAN         \\
\bottomrule
\end{tabular}
}
\label{ogbg-hyperparaeters}
\end{table}

\begin{table}[h]
\centering
\caption{Hyperparameter settings for TU.}\smallskip
\resizebox{0.6\textwidth}{!}{ 
\scriptsize
\begin{tabular}{lll|ll|ll}
\toprule
              & \multicolumn{2}{c}{COLLAB}                        & \multicolumn{2}{c}{REDDIT-BINARY}            & \multicolumn{2}{c}{REDDIT-MULTI-12K}              \\
\midrule
              & ExpC-               & CombC                    & ExpC-              & CombC                & ExpC-                   & CombC                \\
\midrule
batch size    & 32                     & 32                       & 64                    & 64                   & 64                         & 64                  \\
layers        & 3                      & 3                        & 3                     & 3                    & 3                          & 3                    \\
hidden        & 180                    & 180                      & 256                   & 256                  & 256                        & 256                  \\
lr            & 0.001                  & 0.001                    & 0.001                 & 0.001                & 0.001                      & 0.001               \\
step size     & 10                     & 10                       & 10                    & 10                   & 10                         & 10                   \\
lr decay      & 0.8                    & 0.8                      & 0.8                   & 0.8                  & 0.8                        & 0.8                  \\
dropout       & 0.5                    & 0.5                      & 0.5                   & 0.5                  & 0.5                        & 0.5                  \\
readout       & SUM                    & SUM                      & SUM                   & SUM                  & SUM                        & SUM                 \\
\bottomrule
\end{tabular}
}
\label{ogbg-hyperparaeters}
\end{table}

The shared hyperparameter settings of ExpC*-1, ExpC-, CombC* and CombC on all 12 targets of QM9:
batch sizes = 64;
lr = 0.0001;
step size = 30;
lr decay = 0.85;
readout = SUM.
hidden = 256 for ExpC*-1 and ExpC-;
hidden = 512 for CombC* and CombC.
Table \ref{qm9-hyper-individual} gives the individual hyperparameter settings of each model on each target,
including the number of layers.


\begin{table}[h]
\centering
\caption{Number of layers for QM9.}\smallskip
\resizebox{0.9\textwidth}{!}{ 
\scriptsize
\begin{tabular}{l|llllllllllll}
\toprule
             &              &           &   &  &  &  &               &              &                &                &                &  	     \\
\midrule
ExpC*-1,ExpC-
       & 5       & 4       & 5       & 4       & 4       & 4       & 4       & 5       & 4       & 4       & 4       & 4        \\
\midrule
CombC*,CombC
       & 5       & 4       & 5       & 4       & 4       & 5       & 4       & 5       & 4       & 4       & 4       & 4        \\
\bottomrule
\end{tabular}
}
\label{qm9-hyper-individual}
\end{table}


\section{More Experimental Results}
\label{results-details}

We present more results of ablation studies on OGB and QM9,
which demonstrate the effectiveness of ExpandingConv, CombConv and -SUM.

\begin{figure}[h]
\centering
\includegraphics[width=0.32\textwidth]{results/ogbg-ppa-traP.pdf}
\includegraphics[width=0.32\textwidth]{results/ogbg-ppa-valP.pdf}
\includegraphics[width=0.32\textwidth]{results/ogbg-ppa-tstP.pdf}

\includegraphics[width=0.32\textwidth]{results/ogbg-molhiv-traP.pdf}
\includegraphics[width=0.32\textwidth]{results/ogbg-molhiv-valP.pdf}
\includegraphics[width=0.32\textwidth]{results/ogbg-molhiv-tstP.pdf}

\includegraphics[width=0.32\textwidth]{results/ogbg-molpcba-traP.pdf}
\includegraphics[width=0.32\textwidth]{results/ogbg-molpcba-valP.pdf}
\includegraphics[width=0.32\textwidth]{results/ogbg-molpcba-tstP.pdf}

\includegraphics[width=0.32\textwidth]{results/ogbg-code-traP.pdf}
\includegraphics[width=0.32\textwidth]{results/ogbg-code-valP.pdf}
\includegraphics[width=0.32\textwidth]{results/ogbg-code-tstP.pdf}
\caption{Learning curves on ogbg-ppa, ogbg-molhiv, ogbg-molpcba and ogbg-code.}
\label{curve-ogbg-code}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.32\textwidth]{results/dropedge_mae_qm9_0.pdf}
\includegraphics[width=0.32\textwidth]{results/dropedge_mae_qm9_1.pdf}
\includegraphics[width=0.32\textwidth]{results/dropedge_mae_qm9_2.pdf}
\includegraphics[width=0.32\textwidth]{results/dropedge_mae_qm9_3.pdf}
\includegraphics[width=0.32\textwidth]{results/dropedge_mae_qm9_4.pdf}
\includegraphics[width=0.32\textwidth]{results/dropedge_mae_qm9_5.pdf}
\includegraphics[width=0.32\textwidth]{results/dropedge_mae_qm9_6.pdf}
\includegraphics[width=0.32\textwidth]{results/dropedge_mae_qm9_7.pdf}
\includegraphics[width=0.32\textwidth]{results/dropedge_mae_qm9_8.pdf}
\includegraphics[width=0.32\textwidth]{results/dropedge_mae_qm9_9.pdf}
\includegraphics[width=0.32\textwidth]{results/dropedge_mae_qm9_10.pdf}
\includegraphics[width=0.32\textwidth]{results/dropedge_mae_qm9_11.pdf}
\caption{Effectiveness of -SUM on QM9.}
\label{qm9-plot-1}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.32\textwidth]{results/mae_qm9_0.pdf}
\includegraphics[width=0.32\textwidth]{results/mae_qm9_1.pdf}
\includegraphics[width=0.32\textwidth]{results/mae_qm9_2.pdf}
\includegraphics[width=0.32\textwidth]{results/mae_qm9_3.pdf}
\includegraphics[width=0.32\textwidth]{results/mae_qm9_4.pdf}
\includegraphics[width=0.32\textwidth]{results/mae_qm9_5.pdf}
\includegraphics[width=0.32\textwidth]{results/mae_qm9_6.pdf}
\includegraphics[width=0.32\textwidth]{results/mae_qm9_7.pdf}
\includegraphics[width=0.32\textwidth]{results/mae_qm9_8.pdf}
\includegraphics[width=0.32\textwidth]{results/mae_qm9_9.pdf}
\includegraphics[width=0.32\textwidth]{results/mae_qm9_10.pdf}
\includegraphics[width=0.32\textwidth]{results/mae_qm9_11.pdf}
\caption{Effectiveness of powerful aggregators on QM9.}
\label{qm9-plot-2}
\end{figure}


\end{document}
