\documentclass[journal]{IEEEtran}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{url} 
\usepackage[ruled]{algorithm2e}
\usepackage[pagebackref=False,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{fact}{Fact}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}
\usepackage[dvipsnames]{xcolor}
\newcommand{\sx}[1]{\textcolor{RedOrange}{[#1]}}
\newcommand{\cyy}[1]{\textcolor{NavyBlue}{#1}}
\usepackage{colortbl}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{arydshln}
\usepackage[position=below]{subfig}
\usepackage{graphicx}
\usepackage[T1]{fontenc} 

\begin{document}

\title{Compressing Features for Learning \\ with Noisy Labels}

\author{Yingyi Chen,
        Shell Xu Hu,
        Xi Shen,
        Chunrong Ai,
        and~Johan A.K.~Suykens\thanks{This work is jointly supported by ERC Advanced Grant E-DUALITY (787960), KU Leuven Grant C14/18/068, Grant FWO GOA4917N, Grant from the Flemish Government (AI Research Program), EU H2020 ICT-48 Network TAILOR, Leuven.AI Institute.
\textit{(Corresponding authors: Yingyi Chen; Xi Shen.)}}
\thanks{Yingyi Chen is with STADIUS, ESAT, KU Leuven, 3001 Leuven, Belgium
(e-mail: yingyi.chen@esat.kuleuven.be).}
\thanks{Shell Xu Hu is with Samsung AI Center, Cambridge, England 
(e-mail: shell.hu@samsung.com).}
\thanks{Xi Shen is with Tencent AI lab, Shenzhen, China 
(e-mail: xi.shen@enpc.fr).}
\thanks{Chunrong Ai is with School of Management and Economics, Chinese University of Hong Kong, Shenzhen, China 
(e-mail: chunrongai@cuhk.edu.cn).}
\thanks{Johan A.K.~Suykens is with STADIUS, ESAT, KU Leuven, 3001 Leuven, Belgium 
(e-mail: johan.suykens@esat.kuleuven.be)}}

\markboth{}{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}

\maketitle

\begin{abstract}
    Supervised learning can be viewed as distilling relevant information from input data into feature representations.
    This process becomes difficult when supervision is noisy as the distilled information might not be relevant.
    In fact, recent research~\cite{zhang2017understanding} shows that networks can easily overfit all labels including those that are corrupted, and hence can hardly generalize to clean datasets.
    In this paper, we focus on the problem of learning with noisy labels and introduce compression inductive bias to network architectures to alleviate this over-fitting problem.
    More precisely, we revisit one classical  regularization named Dropout~\cite{srivastava2014dropout} 
    and its variant Nested Dropout~\cite{rippel2014learning}.
    Dropout can serve as a compression constraint for its feature dropping mechanism,
    while Nested Dropout further learns ordered feature representations w.r.t.~feature importance.
    Moreover, the trained models with compression regularization are further combined with Co-teaching~\cite{han2018co} for performance boost.

    Theoretically, we conduct bias-variance decomposition of the objective function under compression regularization. 
    We analyze it for both single model and Co-teaching.
    This decomposition provides three insights:
    \textit{(i)} it shows that over-fitting is indeed an issue in learning with noisy labels; 
    \textit{(ii)} through an information bottleneck formulation, it explains why the proposed feature compression helps in combating label noise;
    \textit{(iii)} it gives explanations on the performance boost brought by incorporating compression regularization into Co-teaching.
    Experiments show that our simple approach can have comparable or even better performance than the state-of-the-art methods on benchmarks with real-world label noise including Clothing1M \cite{xiao2015learning} and ANIMAL-10N \cite{song2019selfie}.
    Our implementation is available at \href{https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/}{https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/}.
\end{abstract}

\begin{IEEEkeywords}
Label noise, compression, bias-variance decomposition, information sorting, deep learning.
\end{IEEEkeywords}



\section{Introduction} \label{sec::introduction}
\IEEEPARstart{T}{he} success of deep learning depends on the availability of massive and carefully labeled data. 
However, there is often no guarantee that all annotations are perfect, especially when the amount of data is huge and annotations are required to be fine such as optical flow and segmentation.
In contrast, with the rapid development of the Internet, there are multiple ways to have inexpensive and convenient access to large but defective data, including querying commercial search engines \cite{li2017webvision}, downloading images from social media \cite{mahajan2018exploring}, and various web crawling strategies \cite{olston2010web}.
Correspondingly, persistent efforts have been paid in literature to learn with imperfect data, among which learning with noisy labels has always been attached great significance.

The problem of learning with noisy labels dates back to \cite{angluin1988learning, quinlan1986induction}.
The mainstream methods include 
\textit{(i)}
training on reweighted samples \cite{han2018co, jiang2018mentornet, malach2017decoupling, yu2019does, wei2020combating} where samples possibly clean are assigned larger weights than those possibly corrupted;
\textit{(ii)} 
employing robust loss functions to resist noise \cite{patrini2017making, natarajan2013learning, reed2014training, goldberger2017training};
\textit{(iii)}
conducting label correction \cite{tanaka2018joint, yi2019probabilistic, zhang2021learning} where original labels are often substituted by the possible clean predictions;
\textit{(iv)}
semi-supervised learning methods
\cite{ding2018semi,kong2019recycling,li2020dividemix}
where samples are first identified as clean or corrupted, and then networks are trained in a semi-supervised manner with only the clean labels used.
Moreover, label noise itself also plays an important role in understanding the \textit{generalization puzzle} of deep learning.
Empirical experiments in \cite{zhang2017understanding} show that deep neural networks (DNNs), such as AlexNet \cite{krizhevsky2012imagenet}, can achieve almost zero training errors on randomly labelled datasets.
This analysis demonstrates that the capacities of DNNs are often high enough to memorize the entire noisy training information.
Since over-fitting is mainly due to the model capacities, an alternative way to address the problem of training with noisy labels is to introduce explicit compression inductive bias to the model architecture, which is the main focus of this work.

In this paper, we propose to combat this over-fitting problem by introducing compression inductive bias to networks.
More precisely, rather than relying on the prediction of deterministic DNNs, we introduce feature compression
to the hidden features in networks via \textit{Dropout} \cite{srivastava2014dropout} and its variant \textit{Nested Dropout} \cite{rippel2014learning}.
Dropout can be served as a compression constraint for its feature dropping mechanism, while Nested Dropout further learns ordered feature representations w.r.t. feature importance.
Leveraging Nested Dropout, we can not only constrain the model capacity, but also filter out the irrelevance while preserves the relevance w.r.t.~the learning task.
Moreover, compared to Dropout, the information sorting property of Nested Dropout is particularly useful for conducting signal-to-noise separation in the feature level.
Note that we may also consider other compression strategies such as principal component analysis (PCA) and kernel principal component analysis (kernel PCA) \cite{suykens2002least}, but Dropout/Nested Dropout is a plug-and-play component to networks, thus bringing much convenience to the implementation.

In addition to Dropout/Nested Dropout's bringing feature-level compression to networks, we find that they are suitable for incorporating into Co-teaching \cite{han2018co} which is a strong method for learning with noisy labels, for performance boost.
Specifically, Co-teaching trains two networks simultaneously where networks update themselves based on the small-loss mini-batch samples selected by their peer.
Intuitively, this sample selection mechanism discards samples with possibly wrong labels, and preserves those that are possibly clean.
We will show in this paper that the sample selection during the cross-update process together with compression techniques
will further prevent networks from over-fitting the noisy labels.
On account that good performance of Co-teaching requires the two base networks to be reliable enough, we propose our two-stage method:

\begin{itemize}
    \item Train two \textit{Dropout} / \textit{Nested Dropout} networks separately until convergence;
    \item Fine-tune these two networks with \textit{Co-teaching}.
\end{itemize}
Note that Dropout/Nested Dropout is maintained in the second stage for fine-tuning.
The efficacy of our two-stage compression approach is validated on benchmark real-world datasets by achieving comparable or even better performance than the state-of-the-art approaches.
For example, on Clothing1M~\cite{xiao2015learning}, our method obtains 75.0\% in accuracy, which achieves comparable performance to DivideMix~\cite{li2020dividemix} and ELR+~\cite{liu2020early}. 
On ANIMAL-10N~\cite{song2019selfie}, 
we achieve 84.5\% in accuracy while the state-of-the-art method PLC~\cite{zhang2021learning} is 83.4\%.

Beyond the empirical contributions, we provide theoretical explanations on why compression can combat label noise.
In particular, we conduct bias-variance decomposition of the objective function where Dropout/Nested Dropout is formulated into latent variable model.
This decomposition provides three insights:
\textit{(i)} it shows that over-fitting is indeed an issue in learning with noisy labels.
The bias term determines how close the model fits noisy labels, while the variance term promotes a consensus among individual models in latent variable model.
Deterministic DNNs have zero variance term and thus focus on minimize the bias term during training, leading to over-fitting on noisy labels.
\textit{(ii)} Through an information bottleneck formulation, it explains why the proposed feature compression helps in combating label noise.
Dropout/Nested Dropout can serve as compression constraints since they can be formulated as optimizing an information bottleneck.
These compression constraints bring non-zero variance term and thus reduce the impact of the bias term.
\textit{(iii)} It explains the performance boost brought by incorporating compression regularization into Co-teaching.
The cross-update strategy of Co-teaching together with the compression constraints bring larger variance term to further diminish the influence of the bias term, leading to even less over-fitting on the noisy labels.

This paper is based on our previous work \cite{chen2021boosting} which mainly focuses on the empirical results.
We enrich it with theoretical understanding of our method, the learning with noisy labels problem itself, and more detailed numerical assessments.
This paper is structured as follows: 
Section \ref{sec::relatedWork} summarizes the related works in learning with noisy labels.
Section \ref{sec::method} presents our algorithm.
Section \ref{sec::theory} provides a theoretical understanding of our method.
Section \ref{sec::experiments} shows illustrative toy example and experiments on benchmark real-world datasets. 
Finally, we conclude this paper in Section \ref{sec::conclusion}. 
Implementation is available at \href{https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/}{https://yingyichen-cyy.github.io/CompressFeatNoisyLabels/}.

\section{Related Works} \label{sec::relatedWork}
In this section, we briefly review the existing works related to learning with label noise.
Extensive literature reviews can be found in \cite{song2020learning,han2020survey,cordeiro2020survey}.

\paragraph{Over-fitting prevention}
The idea of preventing networks from over-fitting for better generalization has been considered in \cite{arpit2017closer,ma2018dimensionality}.
In particular, \cite{arpit2017closer} proposes that appropriately tuned explicit regularization prevents DNNs from over-fitting noisy datasets while maintains generalization on
clean data, and
\cite{ma2018dimensionality} proposes to understand the generalization of DNNs by investigating the dimensionality of the deep representation subspace of training samples.
C2D~\cite{zheltonozhskii2022contrast} uses self-supervised pre-training to learn more meaningful information before over-fitting to noise.
ELR/ELR+~\cite{liu2020early} proposes an early-learning regularization to resist over-fitting, while AugDesc~\cite{nishi2021augmentation} achieves this by employing different augmentation strategies.
Although starting from the point of preventing networks from over-fitting, our method is different from their works mainly in that 
\textit{(i)} we theoretically verify that over-fitting is indeed an issue by conducting bias-variance decomposition while \cite{arpit2017closer, ma2018dimensionality, zheltonozhskii2022contrast,nishi2021augmentation} are more from an experimental perspective.
\textit{(ii)} we inject extrinsic compression to filter out noisy information, while \cite{ma2018dimensionality} identifies network's intrinsic compression point and adapts the corresponding loss.

\paragraph{Samples reweighting}
Samples reweighting scheme learns to assign small weights to those samples supposed to be corrupted.
ActiveBias~\cite{chang2017active} reweights samples based on the variance of prediction probabilities and the closeness between the prediction probabilities and the decision threshold.
MentorNet \cite{jiang2018mentornet} trains its student network based on the clean samples selected by its teacher network.
Co-teaching \cite{han2018co} cross-updates its two base models 
based on the samll-loss samples selected by their peers.
Decoulping \cite{malach2017decoupling} updates the networks based on samples where the predictions of the two predictors are different, that is, the ``disagreement" strategy.
As for Co-teaching+ \cite{yu2019does}, it combines Co-teaching with the ``disagreement" strategy to further improve the performance.
Different from above where networks are based on ``disagreement", JoCoR \cite{wei2020combating} trains two networks as a whole by a joint loss following the ``agreement" strategy, and select the small-loss examples to update themselves.
This ``agreement" strategy shows improvement over the previous methods.
Note that we still base our method on Co-teaching since it is easier and also effective for both implementation and analysis.
\cite{pleiss2020identifying} proposes a statistic, namely AUM, which differentiates clean samples from mislabeled samples by exploiting their training dynamics.
The mislabeled ones are discarded during training. 
\cite{pleiss2020identifying} is categorized here since discarding samples is equivalent to assigning zero weights to them.

\begin{figure}[t]
	\begin{minipage}[t]{0.45\textwidth}  
		\centering  
		\includegraphics[width=\textwidth]{./images/ensemble.jpg}
	\end{minipage}  
	\centering  
	\caption{Both Dropout and Nested Dropout are ways to induct ensemble of models, not merely regularizations. 
	To be specific, network with them applied can be regarded as a latent variable model , where  is the encoder, and  is the decoder as in \eqref{eq::lvm}.
	``Models 0-2" are models corresponding to different trials of .
	For instance, ``Model 1" of Dropout is the model corresponding to trial  where the second neuron is masked out. 
	In this case, the entire Dropout model can be viewed as an ensemble of different trial models since the integral is over all possible . 
	Similar explanations also apply to Nested Dropout.}
	\label{fig::compress}
	\vspace{-5mm}
\end{figure}

\paragraph{Robust loss function}
Robust losses have been applied to achieve noise-tolerant classifications including ramp loss \cite{brooks2011support}, unhinged loss \cite{van2015learning}, mean absolute error \cite{suykens2002least, ghosh2017robust, suykens1999least}. 
However, the fact that DNNs can learn arbitrary labels may dampen the effectiveness of these losses in the context of deep learning.
In deep learning, losses are corrected to be robust to noisy samples, or more exactly, to eliminate the influence of noisy samples.
Based on the estimated noise transition matrix, Forward and Backward \cite{patrini2017making} modify the loss function and build an end-to-end framework.
HOC~\cite{zhu2021clusterability} recently proposes to work on clusterable feature representations so as to efficiently estimate noise transition matrix, and further conduct better loss correction.
Other loss correction strategies include \cite{reed2014training,goldberger2017training}.
Different from these methods, we use the cross-entropy loss albeit adapt it for latent variable models.

\paragraph{Label correction}
JO \cite{tanaka2018joint} is a joint optimization framework where network parameters and class labels are optimized alternatively in training.
Inspired but quite unlike \cite{tanaka2018joint}, rather than correcting labels by using the running average of network predictions, PENCIL \cite{yi2019probabilistic} corrects labels via an updating label distribution in an end-to-end manner. 
Moreover, those noisy labels are only utilized for initializing the label distributions, and 
the network loss function is computed using the label distributions.
SELFIE~\cite{song2019selfie} selects refurbishable samples which are of low uncertainty and can be corrected with a high precision, then replaces their labels based on past model outputs. These corrected samples together with other low-loss instances are later used to update the network.
Another state-of-the-art method named PLC \cite{zhang2021learning} focuses more on feature-dependent label noise where labels are progressively corrected based on the confidence of the noisy classifier.
Notably, we keep using all the labels including those noisy ones instead of conducting label correction which is more complicated.

\paragraph{Semi-supervised methods}
In \cite{ding2018semi}, a two-stage method is proposed where samples are identified as clean or corrupted in the first stage, and then networks are trained in a semi-supervised manner with only the clean labels utilized in stage two.
\cite{kong2019recycling} also conducts a similar two-stage method with Renyi entropy regularization used in stage two.
DivideMix \cite{li2020dividemix} is one of the state-of-the-art methods achieving high accuracy on real noisy datasets.
Specifically, it dynamically divides training data into a labeled clean set and an unlabelled corrupted set, and then trains models on both sets in a semi-supervised manner with improved MixMatch \cite{berthelot2019mixmatch} strategy.
It can be seen that these methods mainly differ in adopting different criteria for semi-supervised learning step after dividing the training set into clean and corrupted subsets.


\section{Method} \label{sec::method}
In this section, we present our approach for learning with noisy labels. 
We start with recalling compression techniques including \textit{Dropout}~\cite{srivastava2014dropout} and its structured variant named \textit{Nested Dropout}~\cite{rippel2014learning} in Section \ref{subsec::nested}.
Next, we combine them with one commonly accepted approach named \textit{Co-teaching}~\cite{han2018co} 
(Section \ref{subsec::co_teaching}) in Section \ref{subsec::combination}.
The reason for this combination will be discussed in detail in Section \ref{subsec::comco}.


\subsection{Compression regularizations} \label{subsec::nested}
Here, we consider two compression regularizations that are plug-and-play modules, which can be inserted into common network architectures.
For the sake of clarity, we summarize some necessary notations here.
Let  be the hidden feature representation obtained by the feature network , i.e.,~.
Note that we set the number of channels to be  and leave out the rest for simplicity, that is, . 
In this paper, we treat compression methods as applying masks to the obtained feature . 
In this manner, let  be the feature mask where the space  can vary for different compression methods.
The feature with mask applied is denoted by  where  is the element-wise product.
Then,  will be fed into the subsequent network structures.
The two compression methods are given w.r.t.~their specific mask distributions as follows:
 
\begin{figure}[t]
	\begin{minipage}[t]{0.38\textwidth}  
		\centering  
		\includegraphics[width=\textwidth]{./images/coteaching.pdf}
	\end{minipage}  
	\centering  
	\caption{Co-teaching trains two networks simultaneously where base network updates itself based on the small-loss mini-batch samples selected by its peer.}
	\label{fig::coteaching}
	\vspace{-3mm}
\end{figure}

\subsubsection{Dropout}
Dropout~\cite{srivastava2014dropout} is one classical method for feature compression where each feature in the network layer it applies is dropped according to a Bernoulli distribution.
The space of its feature mask  is defined by
 
where  is the Bernoulli distribution with  being either  or , and  is the drop rate. 

\subsubsection{Nested Dropout}
Nested Dropout \cite{rippel2014learning} learns ordered representations with different dimensions having different degrees of importance.
Although it is originally proposed to perform fast information retrieval and adaptive data compression, we find that it can properly regularize a network to combat label noise.
In particular, while Nested Dropout is applied, the meaningless representations can be dropped, which leads to a compressed network~\cite{gomez2019learning}.
Considering above, these ordered representations can be adapted to learning with noisy labels since representations learned from noisy data are supposed to be meaningless. 
Consequently, Nested Dropout may serve as a strong substitute of Dropout.

In order to obtain an ordered feature representation, in each training iteration, we only keep the first  dimensional feature of  and mask the rest to zeros, that is,  where

with  and  being all-ones and all-zeros tensors, respectively.
Moreover,  is sampled from a categorical distribution denoted by  with corresponding parameters as follows:

where  is the major hyper-parameter in Nested Dropout.
In this case, smaller  is preferred if  is small.
Moreover, though we could compute  with  being \eqref{eq::CatGaussian} exactly during inference, we find it more efficient to verify which  yields the best performance on the validation set, and then keep the model induced by  for testing. 

Note that rather than treating Dropout and Nested Dropout merely as regularizations, we focus on their ability of inducting ensemble of models, i.e.,~\eqref{eq::lvm}, which will be carefully discussed in Section \ref{subsec::overMemo}.
We underline this ensemble property in Fig.~\ref{fig::compress}.

\begin{figure}[t]
	\begin{minipage}[t]{0.45\textwidth}
		\centering  
		\includegraphics[width=\textwidth]{./images/main.jpg}
	\end{minipage}  
	\centering  
    \caption{Overview of our method. In stage one, the hidden activation  is computed by a feature extractor . 
    Dropout/Nested Dropout is applied to  by masking some of the features to zeros, i.e.,~. 
    The compressed feature  is then fed into the network structure , which can simply be a fully connected layer (FC), to perform the final prediction. 
    In stage two, the two base networks are fine-tuned with Co-teaching.}
	\label{fig::workflow}
	\vspace{-3mm}
\end{figure}

\subsection{Co-teaching} \label{subsec::co_teaching}
Co-teaching~\cite{han2018co} is a baseline method for learning under label noise.
It trains two deep networks with identical architecture, i.e.,~ and , simultaneously where each network selects its ) percent \textit{small-loss} instances, leading to  and  respectively, where
 is the forget rate.
Note that  is a crucial hyper-parameter in the Co-teaching architecture.
Networks update themselves basing on the data subset selected by their peers.
We provide a illustration for clarity in Fig.~\ref{fig::coteaching}.

Co-teaching bases on the concept that small-loss instances are more likely to be clean \cite{han2018co, jiang2018mentornet, yu2019does, tanaka2018joint, kumar2010self}.
Therefore, classifiers trained on them are supposed to be more resistant to noisy labels.
However, one non-negligible premise is that base models should be reliable enough to select samples which are indeed clean.
To prevent constantly bad selections, a scheduling has been proposed in \cite{han2018co}.
That is, Co-teaching first keeps all the samples in the mini-batch, then gradually decreases the sample size in  and  till the predefined -th epoch, after which the sample size used for training kept fixed.
Nevertheless, we experimentally find that the tuning of  is not stable since  varies with different levels of label noise.
Therefore, rather than training Co-teaching with random initialized base models and tuning on , we employ well-trained models as initialization for better and stable performance.


\begin{algorithm}[t]
\caption{Two-stage compression training}
	\label{alg::twostage}
	\KwIn{
		training data  with size , 
		compression hyper-parameters ,
		two initialized networks , ,
		loss  \eqref{eq::newLoss},
		forget rate .
	}   
	{\bf Ensure:} Either train with Dropout () by \eqref{eq::mask_drop}, \\
	\quad \quad \quad \, or Nested Dropout () by \eqref{eq::mask_nested}, \eqref{eq::CatGaussian}.

	\While{,  not converge}{ 
        Train ,  independently on  with loss  under
        (Nested) Dropout;
	}
	
	\While{Fine-tune with Co-teaching}{
	Randomly separate mini-batch  into two subsets: ,  with ;\\
	 selects  small-loss data ;\\
	 selects  small-loss data ;\\
	Train  on ,  on  independently with loss  under (Nested) Dropout;\\
	}
	\KwOut{}
\end{algorithm} 


\subsection{Combination} \label{subsec::combination}
Now we combine the compression regularizations with Co-teaching in a two-stage manner:
\begin{itemize}
    \item Train two \textit{Dropout} / \textit{Nested Dropout} networks separately until convergence;
    \item Fine-tune these two networks with \textit{Co-teaching}.
\end{itemize}

Co-teaching is chosen for fine-tuning since its cross-update mechanism would help in alleviating the over-fitting issue over a single model.
As mentioned in \cite{han2018co}, different classifiers are able to generate different decision boundaries and then have different abilities to learn.
During training on noisy labels, we expect the two neural networks to adaptively compress out the noisy information left by their peer networks where samples with obviously corrupted labels have been already excluded.
Hence, the base networks are less likely to overfit the corrupted labels. 
The above idea is validated in Section \ref{subsec::comco} with the help of a \textit{bias-variance decomposition} for Co-teaching.

We now specify our method.
In the first stage, two networks are trained independently until convergence so as to provide better base models for Co-teaching.
Moreover, a learning rate warm-up is set to cope with the difficulty of training with Dropout/Nested Dropout in the early epochs, which results from the high probability of dropping most of the channels in the feature layer when  is large or  is small.
In the second stage, since we only fine-tune the networks with Co-teaching, Dropout/Nested Dropout is maintained during the training of each model except for the selection procedure of small-loss data subsets , . 
Note that the performance of Co-teaching also depends on the diversity of the base models.
In this case, we modify the original Co-teaching \cite{han2018co} with batch separation strategy where each batch is divided into two data subsets with equivalent size, and small-loss data selections are then conducted on these two subsets separately.
The final result is the accuracy of the ensembled model. 
The workflow of our two-stage method is given in
Fig.~\ref{fig::workflow} and Algorithm \ref{alg::twostage} for clarity.



\section{Theoretical analysis} \label{sec::theory}
This section provides the motivation and validity of our method.
Notations and basic concepts in need are given in Section \ref{subsec::preliminaries}.
We state that the key issue in combating label noise is to prevent networks from over-fitting based on a bias-variance decomposition in Section \ref{subsec::overMemo}.
To this end, we recall two compression regularizations which can be treated as implicit information bottleneck in Section \ref{subsec::compression}.
More on Nested Dropout is in Section \ref{subsec::nested_ranking}.
Finally, we verify that the Co-teaching combination leads to even less over-fitting based on the bias-variance decomposition in \ref{subsec::comco}.
For the sake of clarity, all the proofs in this section are given in the Appendix.

\subsection{Preliminaries} \label{subsec::preliminaries}
First of all, we formulate the problem of learning with noisy labels.
Let  be the input variable where  is the input feature space.
We consider the data generation process for the training set:

where  is the noise occurred during labelling. 
In this manner, we denote by  the contaminated label where  is the corresponding signal space.
The goal is to learn a model on the corrupted dataset 
for testing on clean data drawn from the same generative process expect that .

Next, we cover some basic concepts in information theory \cite{shannon1948mathematical}.
The \textit{entropy} gives the amount of information coded in a distribution or equivalently the uncertainty about a random variable, and it is defined as the average code length:

where  is a fixed probability measure on .
Similarly, \textit{conditional entropy} gives the amount of information about one random variable given another random variable:

where  is the joint probability measure on  and  is the conditional p.d.f.
The \textit{cross entropy} with respect to a model distribution  is defined by

which upper bounds the conditional entropy as in \eqref{eq::CE2}. 
Note that the inequality holds for that the Kullback–Leibler divergence, i.e.,~the second term, is always non-negative.
A related quantity is the \textit{cross-entropy} loss .
The \textit{mutual information} measures the statistical dependency between random variables  and  by comparing their joint density with the product of each marginal density:

Note that since the mutual information is a function of , we modify the notation to  for better emphasizing on the actual variable.
Moreover, given a factorization , we have

which leads to another interpretation, that is, the reduction in uncertainty of  by knowing .
In addition, the \textit{conditional mutual information} is defined as follows:


Here and subsequently, we let  stand for the expected value of  and  for the Kullback–Leibler divergence.
Besides, we denote the capital Roman alphabet for random variables or matrices and their lowercase for the values.
Moreover, we denote by  the -th row if  is a matrix or the -th channel if  is a -dimensional tensor. 
We also write  for the slice from the -th channel to the -th channel, and  for the element-wise multiplication.



\subsection{Bias-variance decomposition for noisy labels} \label{subsec::overMemo}
Considering that deterministic networks are likely to overfit the noisy training set, we introduce \textit{an ensemble of models} and rely on the intersection of these models to extract consistent information.
The idea is that the information learned from the noise are less likely to be consistent across different models.
This motivates us to consider the \textit{latent variable model} since it can be treated as an ensemble of models:

where  is the encoder, and  is the decoder or can even be an individual model induced by a particular instance of .
For practical reasons, we would like to use existing network architectures, such as ResNet \cite{he2016deep}, to construct the encoder  and the decoder .
Our strategy is to split an entire network architecture, e.g.~a ResNet-18, into two parts as shown in Fig.~\ref{fig::workflow}.
In this case, it is natural to take the second part plus a softmax layer to implement .
The first part is however insufficient to implement  as we will discuss later.

Since the cross-entropy loss  is intractable due to the integral in \eqref{eq::lvm}, we consider a surrogate quantity to  using Jensen's inequality:

and therefore we define the new loss function as the negative log-likelihood with respect to :


Based on \eqref{eq::generation}, \eqref{eq::surrogate} and \eqref{eq::newLoss}, we now derive the bias-variance decomposition of the proposed latent variable with the new loss function under label noise as follows:
\begin{theorem} \label{thm::decomp}
Let  where , for loss  defined in \eqref{eq::newLoss}, the risk has a bias-variance decomposition:

where  is the average or an ensemble of models.
\end{theorem}



Intuitively, the bias term in \eqref{eq::bvDecompo} determines how close the average model  is to  and  is the conditional probability for the noisy , while the variance term promotes a consensus among individual models.
The variance term also serves as a regularization to combat label noise in the sense that the consensus downweights the influence of the incorrect labels.
Unlike learning with clean data, we do not expect low bias as it indicates model's over-fitting to label noise.
Instead, we rely on the variance term and early stopping to provide good training signals.
In regard of above, the problem of learning with noisy labels can be simplified to \textit{how can we prevent models from over-fitting the noisy training labels?}

\begin{figure}[t]
	\begin{minipage}[t]{0.45\textwidth}  
		\centering  
		\includegraphics[width=\textwidth]{images/infodiagram.png}
	\end{minipage}  
	\centering  
	\caption{Information diagrams for the relationship among three random variables.
	Each circle represents the entropy of the corresponding random variable. The intersection between two circles is the mutual information.
	\textbf{Left}: if  holds. \textbf{Middle}: the general case. \textbf{Right}: an over-fitting case where  is large but  is small.}
	\label{fig::infodiagram}
	\vspace{-3mm}
\end{figure}
It is worth mentioning that a careful design of the encoder  is necessary in order to make better use of the variance term.
For instance, if we choose  to be a Dirac delta function, i.e.,~a deterministic mapping from  to , the variance term is zero.
As a result, the training will focus on minimizing the bias term, leading to an easy over-fitting to the distribution of noisy labels.
On the other hand, if the variance is too large, there will be little consensus among individual models, and therefore, no consistent information could be learned by the latent variable model.
Thus in Section \ref{subsec::compression}, we propose to design  by incorporating a compression inductive bias for better combating label noise.




\subsection{Compression regularizations} \label{subsec::compression}
We would like to create an information bottleneck for the latent variable model. 
In this manner, the noisy information can be filtered out systematically.
To be specific, we propose to create an information bottleneck by masking and damping the output of the feature extractor :

where the distributions of , i.e.,~, is an extrinsic source of randomness, which is tuned on a held-out clean dataset.
Here  is also called mask as defined in \eqref{eq::mask_drop} and \eqref{eq::mask_nested}.

However, here comes the question that \textit{why don't we use Tishby's information bottleneck \cite{tishby2000information} directly for compression?}
As mentioned in \cite{tishby2000information}, the relationship among the input , the label  and the feature representation  in the network is given by a Markov chain:
.
In this consideration, \cite{tishby2000information} proposes to learn a good feature representation by minimizing the weighted sum of the data fitting term  and the complexity term  with respect to the distribution , that is

However, when learning with noisy labels, there is no clear causal relationship between  and .
Therefore, Tishby's information bottleneck principle cannot be applied in this case.
To be more specific, we may argue in terms of the information diagrams shown in Fig.~\ref{fig::infodiagram}.
The left image in Fig.~\ref{fig::infodiagram} visualizes the Markov chain , which is called the \textit{Mickey Mouse I-diagram} in \cite{kirsch2020unpacking} since , which implies that .
In this case,  is always smaller than , hence we can prevent 's over-fitting to  by reducing .
However, in general,  as shown in the middle in Fig.~\ref{fig::infodiagram}, and we can find cases where  is small but  overfits label noise.
To conclude, the traditional information bottleneck \eqref{eq::IB} may not be effective when dealing with label noise.



Comparing to Tishby's IB \eqref{eq::IB}, we discard the term  in \eqref{eq::newIB} completely and rely only on the held-out clean dataset to remove noisy information.
Since  is intractable, we further adjust it to be computationally available.
Instead of estimating the joint distribution , we consider a surrogate joint distribution , and access to  only through its samples.
Note that the idea is similar to the variational information bottleneck by \cite{alemi2016deep}.
Specifically, we first identify that  where  is a constant for the representation learning, and hence we approximate  by


Now, we rewrite our learning with noisy labels as follows:

where  as in \eqref{eq::newLoss}.
As such, we only need to learn the proposed  and .
In particular, we propose an implicit parameterization for  as specified in \eqref{eq::mask}, which involves a feature extractor  and an external random variable .
We also proposed to learn  and  jointly by minimizing , which will be optimized by stochastic gradient descent (SGD) as we only have access to the samples of ,  and .

\subsubsection{Dropout}
If we set  to a Bernoulli distribution as in \eqref{eq::mask_drop}, then \eqref{eq::mask} exactly covers Dropout.
However, different from the original formulation in \cite{srivastava2014dropout}, we formulate Dropout into the framework of latent variable models with our loss \eqref{eq::newLoss}.
In this manner, Dropout is the baseline of incorporating compression inductive bias for combating label noise.



\subsubsection{Nested Dropout}
If we specify  where  is \eqref{eq::mask_nested}, and 
 as \eqref{eq::CatGaussian}, then \eqref{eq::mask} exactly covers the Nested Dropout.
More properties are provided in the next subsection.


\begin{figure}[t]
	\begin{minipage}[t]{0.4\textwidth}  
		\centering  
		\includegraphics[width=\textwidth]{./images/channel_MI2.png}
	\end{minipage}  
	\centering  
	\caption{The mutual information of different channels of models trained on the classification task on CIFAR-10 \cite{krizhevsky2009learning}. 
	The result is the average after  runs with  confidence interval.
	We compare the model trained with and without Nested Dropout. Baseline refers to the one without Nested Dropout.}
	\label{fig::MI}
	\vspace{-3mm}
\end{figure}



\subsection{Nested Dropout} \label{subsec::nested_ranking}
Nested Dropout is a variant of Dropout where the importance of each feature channel is sorted from high to low, while channels in Dropout model are of equal importance.
For better understanding of Nested Dropout's sorting property, we theoretically work on it through mutual information.

Before presenting the theorem on the information sorting property, we need the following Assumption \ref{ass::permu} where hidden features are supposed to be exchangeable \cite{aldous1985exchangeability}.

\begin{assumption} \label{ass::permu}
    Let  be the input,  be the feature extractor,  be the subsequent network structure including the classification head, and  be the hidden feature representation.
    The hidden feature representation  is exchangeable, and  is also exchangeable.
    That is, for any permutation , the model satisfies that
    
    where  denotes equivalence in distribution.
\end{assumption}

We find this can serve as a valid assumption considering the network architectures.
According to de Finetti's theorem \cite{heath1976finetti}, a sequence of random variables  is infinitely exchangeable iff,
, for all  and some measure  on .
If we consider a simple MLP where ,  and denote .
In this way, for any permutation of the hidden feature , it can be obtained by .
As it requires to integrate all the possible  to obtain  and ,
we then have .
Since , then  is exchangeable.
Moreover, similar argument can be derived by considering the features after global average pooling in CNNs.


\begin{theorem}  \label{thm::ranking}
    Let  be the input,  be the feature extractor, and  be the hidden feature representation.
    Suppose that the model satisfies Assumption \ref{ass::permu}.
    Then, we have for ,
    
\end{theorem}

This sorting property is also discussed in \cite{rippel2014learning}, although from the perspective of  with .
In addition to the theoretical analysis, we conduct an experiment on CIFAR-10 \cite{krizhevsky2009learning} using ResNet-18 to verify Theorem \ref{thm::ranking} empirically. 
We plot the empirical estimate of the variational lower bound of , i.e.,~, for  computed by \eqref{eq::mask} and . 
The comparison is in Fig.~\ref{fig::MI} where the one without Nested Dropout is tagged as baseline. 
A clear information sorting has been achieved comparing to the baseline training. 


\begin{figure*}[t]
	\begin{minipage}[t]{0.24\textwidth}  
		\centering 
		\addtocounter{figure}{-1}
		\includegraphics[width=\textwidth]{images/baseline.pdf}
        \captionsetup{labelformat=empty}
		\caption{(a) MLP}
	\end{minipage}  
	\begin{minipage}[t]{0.24\textwidth}  
		\centering  
		\addtocounter{figure}{-1}
		\includegraphics[width=\textwidth]{images/nested200_K1.pdf} 
		\captionsetup{labelformat=empty}
        \caption{(b) MLPNested =1}
	\end{minipage} 
	\begin{minipage}[t]{0.24\textwidth}  
		\centering  
		\addtocounter{figure}{-1}
		\includegraphics[width=\textwidth]{images/nested200_K10.pdf} 
		\captionsetup{labelformat=empty}
        \caption{(c) MLPNested =10}
	\end{minipage} 
	\begin{minipage}[t]{0.24\textwidth}  
		\centering  
		\addtocounter{figure}{-1}
		\includegraphics[width=\textwidth]{images/nested200_K100.pdf}
        \captionsetup{labelformat=empty}
		\caption{(d) MLPNested =100}
	\end{minipage} 
	\begin{minipage}[t]{0.24\textwidth}  
		\centering  
		\addtocounter{figure}{-1}
		\includegraphics[width=\textwidth]{images/dropout09.pdf}
        \captionsetup{labelformat=empty}
		\caption{(e) MLPDropout 0.9}
	\end{minipage}
	\begin{minipage}[t]{0.24\textwidth}  
		\centering  
		\addtocounter{figure}{-1}
		\includegraphics[width=\textwidth]{images/dropout07.pdf}
        \captionsetup{labelformat=empty}
		\caption{(f) MLPDropout 0.7}
	\end{minipage}
	\begin{minipage}[t]{0.24\textwidth}  
		\centering  
		\addtocounter{figure}{-1}
		\includegraphics[width=\textwidth]{images/dropout05.pdf}
        \captionsetup{labelformat=empty}
		\caption{(g) MLPDropout 0.5}
	\end{minipage}
	\begin{minipage}[t]{0.24\textwidth}  
		\centering  
		\addtocounter{figure}{-1}
		\includegraphics[width=\textwidth]{images/dropout03.pdf}
        \captionsetup{labelformat=empty}
		\caption{(h) MLPDropout 0.3}
	\end{minipage}
	\centering  
	\caption{Comparisons of regression between standard MLP and MLP trained with Nested Dropout~\cite{rippel2014learning} and Dropout \cite{srivastava2014dropout} on a synthetic noisy label dataset. 
	(a) MLP with standard training; 
	(b-d) predictions of MLPNested using only the first  channels;
	(e-h) predictions of MLPDropout with drop ratio .}
	\label{fig::ResistNoise}
	\vspace{-3mm}
\end{figure*} 

\subsection{Combination with Co-teaching} \label{subsec::comco}
In this subsection, we consider the stage two in our method where 
two networks are further fine-tuned with Co-teaching.
Recall that during the cross-update state, one network selects its small-loss instances  and send them to its peer.
Intuitively, the above process resembles the teacher and student mechanism where the teacher selects possibly clean instances for the student to learn.
In this regard, let  be the teacher network,  in \eqref{eq::lvm} be the student network.
The sample selection mechanism only preserves those with small loss , i.e.,~large .
If we consider this selection w.r.t.~probability together with the following student network training, we reformulate the student's loss as:

where  represents 's probability to be selected.
Moreover, by regarding sample selection and student network training as a whole, we redefine student network's decoder by
.
In order to distinguish it from the original student network's decoder , we call  the taught student decoder.
The following Theorem \ref{thm::co_decomp} gives the bias-variance decomposition when networks are further fine-tuned with Co-teaching.

\begin{theorem} \label{thm::co_decomp}
    Let  be the Co-teaching teacher network, and  where  be the taught student decoder. 
    For the Co-teaching student loss  defined in \eqref{eq::co_loss}, the risk has a bias-variance decomposition:
    
    where  is the average or an ensemble of models. 
    Moreover, by defining , we then have:
    \begin{itemize}
        \item[\textit{(i)}] If , then
        
        \item[\textit{(ii)}] If , then
        
    \end{itemize}
\end{theorem}

\begin{remark} \label{rk::co_bv}
    The condition  equals to  where the right-hand side measures the difference between a single training network  and the teacher network  of Co-teaching. 
    The larger the difference, the smaller the value, and vise versa.
    Hence, to obtain smaller bias term than in \eqref{eq::bvDecompo}, the sample selection of Co-teaching only chooses those  with large  so as to meet the condition.
    As  by definition, if further , i.e.,~ with larger right-hand side value, then we will have larger variance term than that in \eqref{eq::bvDecompo}.
\end{remark}

Theorem \ref{thm::co_decomp} and Remark \ref{rk::co_bv} together demonstrate that choosing samples with large  during selection of Co-teaching leads to smaller bias term and larger variance term than those in \eqref{eq::bvDecompo}.
That is, the impact of the bias term can be even diminished.
Consequently, the sample selection mechanism during Co-teaching's cross-update process helps in further preventing networks from over-fitting on noisy labels, thus achieving better performance on clean datasets.

\section{Experiments} \label{sec::experiments}
In this section, we present our experimental results. 
First of all, we focus on how Dropout and Nested Dropout cope with the regression noise by a toy example in Section~\ref{sec:toy}. 
For better understanding of our methods, we assess them on real datasets albeit with synthetic label noise in Section \ref{subsec::synthetic}.
In Section~\ref{sec:compare}, we compare our method with the state-of-the-art methods on two real-world datasets: Clothing1M~\cite{xiao2015learning} and ANIMAL-10N~\cite{song2019selfie}. 
Finally, we conduct ablation study on ANIMAL-10N and Clothing1M in Section~\ref{sec::ablation}. 



\begin{table*}[t]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \caption{
    Test accuracy (\%) of state-of-the-art methods under (a) symmetric noise on CIFAR-10 \cite{krizhevsky2009learning} and CIFAR-100 \cite{krizhevsky2009learning}, (b) 40\% asymmetric noise on CIFAR-10. 
    All approaches are implemented with PreAct ResNet-18~\cite{he2016identity} architecture.}
    \begin{minipage}{0.7\textwidth}
        \centering
        \scalebox{0.95}{\subfloat[Symmetric noise on CIFAR-10 and CIFAR-100.]{
        \begin{tabular}{ccccccccc}
            \hline\hline
            \multicolumn{1}{c|}{\multirow{2}{*}{Methods / Noise Ratio (\%)}}
            & \multicolumn{4}{c|}{CIFAR-10}              
            & \multicolumn{4}{c}{CIFAR-100}                          
            \\
            \multicolumn{1}{c|}{}   
            & \multicolumn{1}{c|}{\, 20\%\,} 
            & \multicolumn{1}{c|}{\, 50\%\,} 
            & \multicolumn{1}{c|}{\, 80\%\,} 
            & \multicolumn{1}{c|}{\, 90\%\,}         
            & \multicolumn{1}{c|}{\, 20\%\,} 
            & \multicolumn{1}{c|}{\, 50\%\,} 
            & \multicolumn{1}{c|}{\, 80\%\,} 
            & {\, 90\%\,}
            \\ \hline
            \multicolumn{1}{c|}{Cross-Entropy~\cite{li2020dividemix}} 
            & 86.8 & 79.4 & 62.9 & \multicolumn{1}{c|}{42.7} & 62.0 & 46.7 & 19.9 & 10.1
            \\
            \multicolumn{1}{c|}{Bootstrap~\cite{reed2014training}}
            & 86.8 & 79.8 & 63.3 & \multicolumn{1}{c|}{42.9} & 62.1 & 46.6 & 19.9 & 10.2
            \\
            \multicolumn{1}{c|}{F-correction~\cite{patrini2017making}}  & 86.8 & 79.8 & 63.3 & \multicolumn{1}{c|}{42.9} & 61.5 & 46.6 & 19.9 & 10.2          
            \\
            \multicolumn{1}{c|}{Co-teaching+~\cite{yu2019does,li2020dividemix}}
            & 89.5 & 85.7 & 67.4 & \multicolumn{1}{c|}{47.9} & 65.6 & 51.8 & 27.9 & 13.7         
            \\
            \multicolumn{1}{c|}{Mixup~\cite{zhang2018mixup}}  
            & 95.6 & 87.1 & 71.6 & \multicolumn{1}{c|}{52.2} & 67.8 & 57.3 & 30.8 & 14.6               
            \\
            \multicolumn{1}{c|}{PENCIL~\cite{yi2019probabilistic,li2020dividemix}}
            & 92.4 & 89.1 & 77.5 & \multicolumn{1}{c|}{58.9} & 69.4 & 57.5 & 31.1 & 15.3          
            \\
            \multicolumn{1}{c|}{MLNT~\cite{li2019learning,li2020dividemix}}  
            & 92.9 & 89.3 & 77.4 & \multicolumn{1}{c|}{58.7} & 68.5 & 59.2 & 42.4 & 19.5     
            \\
            \multicolumn{1}{c|}{M-correction~\cite{arazo2019unsupervised}}        
            & 94.0 & 92.0 & 86.8 & \multicolumn{1}{c|}{69.1} & 73.9 & 66.1 & 48.2 & 24.3       
            \\
            \multicolumn{1}{c|}{DivideMix~\cite{li2020dividemix}}       & 96.1 & 94.6 & 93.2 & \multicolumn{1}{c|}{76.0} & 77.3 & 74.6 & 60.2 & 31.5      
            \\ \hline
            \multicolumn{9}{c}{\bf Ours}                                
            \\
            \multicolumn{1}{c|}{NestedCo-teaching}              
            & 95.3 & 91.9 & 78.8 & \multicolumn{1}{c|}{55.0} & 77.5 & 66.7 & 43.0 & 14.2
            \\
            \multicolumn{1}{c|}{DropoutCo-teaching}              
            & 95.0 & 90.2 & 78.8 & \multicolumn{1}{c|}{56.0} & 76.7 & 67.0 & 44.1 & 18.4
            \\ \hdashline
            \multicolumn{9}{c}{\bf Pre-Cleaning with DivideMix}      
            \\
            \multicolumn{1}{c|}{M-correction} 
            & 94.5 & 93.5 & 92.6 & \multicolumn{1}{c|}{73.5} & 66.4 & 63.8 & 54.0 & 29.1
            \\
            \multicolumn{1}{c|}{NestedCo-teaching}  & 96.0 & 94.9 & \bf 93.5 & \multicolumn{1}{c|}{\bf 78.3} & 78.3 & 76.0 & \bf 63.6 & 36.1
            \\
            \multicolumn{1}{c|}{DropoutCo-teaching }
            & \bf 96.1 & \bf 95.0 & 93.4 & \multicolumn{1}{c|}{78.0} & \bf 79.5 & \bf 76.8 & 63.2 & \bf 37.4
            \\ 
            \hline\hline
        \end{tabular}}}
    \end{minipage}\hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \scalebox{0.95}{\subfloat[40\% asym.~noise on CIFAR-10.]{
        \begin{tabular}{c|c}
            \hline\hline
            Methods  & Acc. (\%)   
            \\ \hline
            Cross-Entropy~\cite{li2020dividemix} & 85.0 
            \\ 
            F-correction~\cite{patrini2017making,li2020dividemix} & 87.2 
            \\
            M-correction~\cite{arazo2019unsupervised,li2020dividemix} & 87.4 \\
            Iterative-CV~\cite{chen2019understanding,li2020dividemix} & 88.6 \\
            PENCIL~\cite{yi2019probabilistic,li2020dividemix} & 88.5 
            \\
            JO~\cite{tanaka2018joint,li2020dividemix} & 88.9 
            \\
            MLNT~\cite{li2019learning,li2020dividemix} & 89.2 
            \\ 
            DivideMix~\cite{li2020dividemix} & 93.4 
            \\ \hline
            \multicolumn{2}{c}{\bf Ours} 
            \\
            NestedCo-teaching & 93.0 
            \\
            DropoutCo-teaching & 92.9 
            \\ \hdashline
            \multicolumn{2}{c}{\bf Pre-Cleaning with DivideMix} 
            \\
            MLNT & 92.5 
            \\
            NestedCo-teaching & \bf 94.2 
            \\
            DropoutCo-teaching & 93.8 
            \\ \hline\hline
        \end{tabular}} 
        \label{tab::cifar}}
    \end{minipage}
    \hfill
    \vspace{-5mm}
\end{table*}


\subsection{Toy example: a simple regression with noise}
\label{sec:toy}
This subsection provides an intuitive better understanding on the reason why Nested Dropout~\cite{rippel2014learning} and Dropout~\cite{srivastava2014dropout} are able to resist label noise.
To this end, we give a simulated regression experiment. 
Specifically, we generate a dataset of noisy observations from  for  where  is evenly spaced between  and  are i.i.d~sampled. 
We employ a multilayer perceptron (MLP) consisting of three linear layers with input and output dimensions being .
Moreover, we add ReLU activations to all layers except the last one.
When training model with Nested Dropout/Dropout, we only apply it to the last layer of the MLP, and the corresponding model is denoted by MLPNested, MLPDropout, respectively. 
Note that we follow \eqref{eq::CatGaussian} where .
Fig.~\ref{fig::ResistNoise} gives the results after k epochs.
The drop ratio  of Dropout varies in  where the compression ratio decreases. 
As in Fig.~\ref{fig::ResistNoise}, MLP overfits the label noise while MLPNested with the first ,  channels recover the ground-truth  better. 
Nevertheless, MLPNested gradually overfits the label noise due to over parameterization as the number of channels increases.
As for MLPDropout, with  decreasing, the models become over-fitting the noisy labels.
However, MLPNested with  still gives the best performance.
To conclude, both compression methods prevent networks from over-fitting the noisy patterns. 
Notably, for MLPNested, the main data structure information is contained in the first few channels, while noisy information is likely to be encoded in channels towards the end.


\subsection{Model analysis on synthetic noise} 
\label{subsec::synthetic}

\subsubsection{Datasets}
We evaluate our methods on CIFAR-10 \cite{krizhevsky2009learning} and CIFAR-100 \cite{krizhevsky2009learning} with synthetic label noise following ~\cite{tanaka2018joint,li2020dividemix,li2019learning}.
For the training data, we manually corrupt the label according to a transition matrix  with
,  denoting the probability of flipping clean  to noisy .
One representative structure of the matrix  is the symmetric flipping \cite{van2015learning}, that is, ,  where  is the number of classes and  is called the noise ratio.
The other representative structure is the asymmetric (or pair) flipping \cite{patrini2017making} where label mistakes only happen within very similar classes, therefore should be tailored for different datasets. For example, in CIFAR-10, the asymmetric flippings follow: truck  auto-mobile, bird  airplane, deer  horse and cat  dog. The probability is  for flipping from ground-truth to inaccurate class, while  for remaining uncorrupted. 

\begin{table}[t]
    \caption{Test accuracy (\%) of our NestedCo-teaching and DropoutCo-teaching on CIFAR-10 and CIFAR-100~\cite{krizhevsky2009learning} under symmetric noise.
    Methods based on ImageNet~\cite{deng2009imagenet} pre-trained models are marked with ``\cmark".
    All approaches are implemented with ResNet-18~\cite{he2016deep} architecture.}
    \label{tab::cifar_imgnet}
    \centering
    \resizebox{0.99\columnwidth}{!}{
    \begin{tabular}{c|c|ccc|ccc}
        \hline\hline
        \multirow{2}{*}{
        \begin{tabular}[c]{@{}c@{}}{Methods /} \\ Noise Ratio\end{tabular}} 
        & \multirow{2}{*}{
        \begin{tabular}[c]{@{}c@{}}{Pre-} \\ trained\end{tabular}} 
        & \multicolumn{3}{c|}{CIFAR-10} & \multicolumn{3}{c}{CIFAR-100} 
        \\
        & & \multicolumn{1}{c|}{20\%} 
        & \multicolumn{1}{c|}{50\%} 
        & 80\% 
        & \multicolumn{1}{c|}{20\%} 
        & \multicolumn{1}{c|}{50\%} & 80\%
        \\ \hline
        \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}{Nested}\\ {Co-teaching}\end{tabular}}   
        & \xmark & 91.7 & 86.9 & 53.4 & 69.0 & 60.6 & 28.0
        \\
        & \cmark & \bf 92.9 & 87.8 & 72.2 & 71.4 & 63.3 & 36.4 
        \\ \hline
        \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}{Dropout}\\ {Co-teaching}\end{tabular}}  
        & \xmark & 91.9 & 86.6 & 59.0 & 72.4 & 62.4 & 33.2 
        \\
        & \cmark & 91.9 & \bf 89.7 & \bf 77.9 & \bf 74.2 & \bf 65.0 & \bf 39.8
        \\ \hline\hline
    \end{tabular}}
    \vspace{-5mm}
\end{table}

\subsubsection{Implementation details}
Our methods are implemented with PyTorch.
Following the previous works~\cite{li2020dividemix,arazo2019unsupervised}, experiments on CIFAR-10, CIFAR-100 are with PreAct ResNet-18 \cite{he2016identity} trained from scratch.
In the first stage, we use SGD optimizer with a momentum of 0.9, a weight decay of 1e-4, an initial learning rate of 0.1, and batch size of 128.
We apply learning rate warm-up with  iterations and the number of epochs is 200 with learning rate decayed by 0.1 at 100 and 150 epochs.
Mixup data augmentation~\cite{zhang2018mixup} is adopted in the first stage for better performance as in~\cite{li2020dividemix,arazo2019unsupervised}.
We apply Dropout/Nested Dropout on the average pooled \textit{Conv5} features.
In the second stage, two well-trained models are set as base models for Co-teaching. 
The initial learning rate is 1e-3 and we still employ SGD as optimizer.
Moreover,  is tuned under different noise ratio, batch norm is frozen and no warm-up is applied.
Models are trained for 100 epochs with the learning rate decayed by 0.1 after 50 epochs.
We set ,  for cases on CIFAR-10, and ,  on CIFAR-100.
Note that when training with Nested Dropout, we record the optimal number of channels  of the model and use only these first  channels when testing.

We further show that our methods can also serve as good complementary strategies to other state-of-the-art methods to achieve even better performance.
In particular, we propose an additional data preprocessing step before training our methods, which is named ``Pre-Cleaning".
During this pre-cleaning, for example, we substitute the original labels of the dataset with the predictions of a well-trained DivideMix~\cite{li2020dividemix} model, resulting in a pre-cleaned dataset.
We later train our methods on this pre-cleaned dataset so as to further exceed the performance of DivideMix.
Note that we can employ any state-of-the-art methods to conduct the ``Pre-Cleaning".


\subsubsection{Results on CIFAR-10 and CIFAR-100~\cite{krizhevsky2009learning}}
We compare our two-stage methods with multiple state-of-the-art methods on CIFAR-10 and CIFAR-100 under different types and levels of synthetic label noise in Table \ref{tab::cifar}.
We consider the performance of our methods with and without the ``Pre-Cleaning" step separately.
Without the pre-cleaning step, our simple NestedCo-teaching and DropoutCo-teaching achieve the top-3 performance for all except the extreme 90\% label noise ratio cases.
Moreover, by pre-cleaning with DivideMix, our methods achieve the best performance for all cases.
Note that we also compare with M-correction~\cite{arazo2019unsupervised} and MLNT~\cite{li2019learning} with the pre-cleaning step in Table~\ref{tab::cifar}.
It can be seen that although both M-correction and MLNT improve upon their own results with the help of pre-cleaning, they fail to surpass the performance of DivideMix.
Therefore, they cannot serve as complementary strategies to DivideMix to enhance performance.
In contrast, our DropoutCo-teaching improves upon DivideMix by a maximum of 5.9\% in accuracy under 90\% symmetric noise on CIFAR-100.
We also consider the training and inference time of our methods. 
It takes 2.7 hours for training the complete two-stage model on a single NVIDIA V100 GPU, and both NestedCo-teaching and DropoutCo-teaching take 0.24 milliseconds per image for inference.
In regard of above, our methods not only perform well on their own, but can also serve as effective and efficient complementary strategies to other state-of-the-art methods with only a little extra time.
Moreover, we provide additional insight that by using ImageNet~\cite{deng2009imagenet} pre-trained models as in Table~\ref{tab::cifar_imgnet}, we can achieve even better performance.


\subsection{Comparison with state-of-the-art methods on real datasets}
\label{sec:compare}

\subsubsection{Datasets}
The following experiments are conducted on two real-world datasets with real label noise: Clothing1M~\cite{xiao2015learning} and ANIMAL-10N~\cite{song2019selfie}. 
Clothing1M is a benchmark dataset containing  million clothing images with  categories from online shopping websites, and its overall estimated noise ratio is 38.5\% according to~\cite{xiao2015learning}.
Moreover, this dataset provides k, k and k manually verified clean data for training, validation and testing. 
Note that we do not use the clean training set during training.
In our experiment, we randomly sample a balanced subset that includes k images with k images per category, from the noisy training set as in \cite{yi2019probabilistic,zhang2021learning}.
This balanced subset is used as our training set and classification accuracies are reported on the k clean test data. 
We follow the data augmentations in ~\cite{li2020dividemix,liu2020early,arazo2019unsupervised}, which includes Mixup data augmentation~\cite{zhang2018mixup}.
ANIMAL-10N is another benchmark dataset recently proposed by~\cite{song2019selfie}. 
It contains  animal classes with confusing appearance.
There are k training, k testing images, and an estimated label noise rate . 
No data augmentation is applied so as to follow the settings in \cite{song2019selfie}.

\begin{table}[t]
    \caption{Test accuracy (\%) of state-of-the-art methods on Clothing1M~\cite{xiao2015learning}. 
    All approaches are implemented with ResNet-50~\cite{he2016deep} architecture.
    Results with ``*" use a balanced subset or a balanced loss.}
    \vspace{-2mm}
    \label{tab::clothing1m}
    \begin{center}
    \begin{tabular}{c|c}
        \hline \hline
        Methods  & {\,Acc. (\%)\,}   
        \\ \hline
        Cross-Entropy~\cite{li2020dividemix} & 69.2 
        \\ 
        F-correction~\cite{patrini2017making} & 69.8 
        \\
        M-correction~\cite{arazo2019unsupervised} & 71.0 
        \\
        JO~\cite{tanaka2018joint} & 72.2 
        \\
        ELR*~\cite{liu2020early} & 72.9 
        \\
        HOC*~\cite{zhu2021clusterability} & 73.4
        \\
        PENCIL*~\cite{yi2019probabilistic} & 73.5
        \\
        MLNT~\cite{li2019learning} & 73.5 
        \\ 
        PLC*~\cite{zhang2021learning} & 74.0
        \\
        C2D*~\cite{zheltonozhskii2022contrast} & 74.6
        \\
        ELR+*~\cite{liu2020early} & 74.8 
        \\
        DivideMix*~\cite{li2020dividemix} & 74.8 
        \\ \hline
        \multicolumn{2}{c}{\bf Ours} 
        \\
        Nested* & 73.2 
        \\
        NestedCo-teaching* & \bf 75.0 
        \\
        Dropout* & 72.9
        \\
        DropoutCo-teaching* & 74.0
        \\ \hline\hline
    \end{tabular}
    \end{center}
    \vspace{-5mm}
\end{table}


\subsubsection{Implementation details}
The following experiments are implemented on PyTorch. 
Experiments on Clothing1M~\cite{xiao2015learning} are with ResNet-50~\cite{he2016deep} pre-trained on ImageNet~\cite{deng2009imagenet} following \cite{yi2019probabilistic,li2020dividemix,li2019learning}. 
Dropout/Nested Dropout is applied right before the linear classifier in the network with , .
First, SGD optimizer is used for stage one model training with momentum , weight decay 5e-4, initial learning rate 2e-2, and batch size . 
Learning rate warm-up is utilized for  iterations in stage one, and model is later trained for  epochs with the learning rate decayed by  after the -th epoch.
Second, the two models trained in stage are fine-tuned through Co-teaching in stage two.
SGD optimizer is utilized with the same settings and follows a cosine learning rate decay~\cite{loshchilov2016sgdr} with a maximum learning rate
of 5e-5 and a minimum of 1e-5 and without learning rate warm-up. 
Forget rate  is set to be 0.5, batch norms are frozen, and we train for 10 epochs.
Note that when training with Nested Dropout, 
the optimal number of channels  of the model is recorded, and only these first  channels are used for testing.

For ANIMAL-10N, VGG-19~\cite{simonyan2014very} is used with batch normalization \cite{ioffe2015batch} as in \cite{song2019selfie}. 
The two Dropout layers in the original VGG-19 architecture are substituted with Nested Dropout when ``Nested" is applied.
SGD optimizer is applied.
For more stable training, we use alternative training strategy for these two layers of Dropout/Nested Dropout.
That is, for each feed-forward, Nested Dropout is either applied to the first or the second layers.
In stage one, following \cite{song2019selfie}, the network is trained for  epochs with initial learning rate .
The learning rate is later decayed by  at -th and -th epochs.
Moreover,  models are trained with learning rate warm-up for  iterations. 
In stage two, forget rate  is set to be , batch norms are frozen and no warm-up is applied.
The initial learning rate is  and is decayed by  after the -th epoch with  epochs in total. 


\subsubsection{Results on the Clothing1M~\cite{xiao2015learning}} 
We compare our methods to state-of-the-art methods in Table~\ref{tab::clothing1m}. 
Notably, a single model trained with Nested Dropout or Dropout can not only surpass M-correction~\cite{arazo2019unsupervised}, JO~\cite{tanaka2018joint}, ELR~\cite{liu2020early}, but also
achieve comparable performance to HOC~\cite{zhu2021clusterability} and PENCIL~\cite{yi2019probabilistic}. 
The combination of DropoutCo-teaching boosts the performance of a single Dropout model by .
Moreover, the combination of NestedCo-teaching boosts from  of a single model to , achieving the best among all methods.


\subsubsection{Results on the ANIMAL-10N~\cite{song2019selfie}} 
Table~\ref{tab::animal} gives the results on ANIMAL-10N. 
It can be seen that our single Dropout model can achieve comparable performance to SELFIE~\cite{song2019selfie}.
Moreover, the combination with Co-teaching provides a consistent performance boost, which is in line with the results on Clothing1M~\cite{xiao2015learning}. 
Notably, our best performance by using DropoutCo-teaching achieves  accuracy outperforms recent approach PLC~\cite{zhang2021learning} by . 


\subsection{Ablation study with real label noise}
\label{sec::ablation}
This section provides ablation study of , , and  on ANIMAL-10N~\cite{song2019selfie}. 
Note that same as many state-of-the-art methods~\cite{song2019selfie,li2020dividemix,liu2020early,li2019learning}, our hyper-parameters need to be tuned on a clean validation set.
Moreover, we also evaluate our methods using different backbones on Clothing1M~\cite{xiao2015learning}.

\subsubsection{Ablation on }
As in Table~\ref{tab::ablation} (a), Nested Dropout provides consistent improvement compared to training with standard cross-entropy loss and the performance gain is also robust to the choices of the hyper-parameter . 
Moreover, fine-tuning through Co-teaching provides clear performance boost for all the models. 
We also present the optimal number of channels of each model (entry ``"). 
Although there are two layers of Nested Dropout applied to the classifier of VGG-19, the optimal number of channels  is recorded w.r.t.~the last Nested Dropout layer for simplicity.
Interestingly, models trained with Nested Dropout achieve better performance with only less than \% of channels comparing to their counterparts with cross-entropy loss.


\begin{table}[t]
    \caption{Average test accuracy (\%) with standard deviation (3 runs) of state-of-the-art methods on ANIMAL-10N~\cite{song2019selfie}. All approaches are implemented with VGG-19~\cite{simonyan2014very} architecture.
    Results with ``*" use two networks for training.}
    \vspace{-2mm}
    \label{tab::animal}
    \begin{center}
    \begin{tabular}{c|c}
        \hline\hline
        Methods  & Acc. (\%)   
        \\ \hline
        Cross-Entropy~\cite{song2019selfie} & 79.4 {} 0.1
        \\
        Co-teaching*~\cite{han2018co,song2019selfie} & 80.2 {} 0.1
        \\
        ActiveBias~\cite{chang2017active,song2019selfie} & 80.5  0.3
        \\
        SELFIE~\cite{song2019selfie} & 81.8 {} 0.1
        \\
        PLC~\cite{zhang2021learning} & 83.4 {} 0.4
        \\ \hline
        \multicolumn{2}{c}{\bf Ours} 
        \\
        Nested & 81.3 {} 0.6
        \\
        NestedCo-teaching* & 84.1 {} 0.1
        \\
        Dropout & 81.6 {} 0.2
        \\
        DropoutCo-teaching* & \bf 84.5 {} 0.1
        \\ \hline\hline
    \end{tabular}
    \end{center}
    \vspace{-5mm}
\end{table}


\subsubsection{Ablation on }
We experiment on different  with results given in Table \ref{tab::ablation} (a).
Note that results with  are not given in the table since the training of single VGG-19 fails on ANIMAL-10N \cite{song2019selfie}.
The performance under different choices of  are less robust compared to those of Nested Dropout.
However, what is in common is that the combination with Co-teaching again brings significant performance boost for all the models.



\begin{table*}[t]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \caption{Average test accuracy (\%) with standard deviation (3 runs) of 
    (a) different  for Nested Dropout,  for Dropout. The corresponding optimal number of channels  for each Nested Dropout model is given (entry ``"). We report test accuracy of single model (entry ``Acc.") as well as the accuracy with the combination of Co-teaching (entry ``Co-teaching Acc.")
    (b) different  for Co-teaching on ANIMAL-10N~\cite{song2019selfie}.
    }
    \begin{minipage}{0.55\textwidth}
        \centering
        \scalebox{0.9}{\subfloat[Ablation study on  and .]{
        \begin{tabular}{cccccc}
            \hline\hline
            \multicolumn{2}{c|}{Methods} &   
            & \multicolumn{1}{c|}{Acc. (\%)}  &   & Co-teaching Acc. (\%)  \\ \hline
            \multicolumn{2}{c|}{Cross-Entropy} & 4096  
            & \multicolumn{1}{c|}{79.4  0.1} & 4096 & 82.2  1.1 
            \\ \hline
            \multicolumn{1}{c|}{\multirow{5}{*}{}} 
            & \multicolumn{1}{c|}{25} & 17.7  9.7 
            & \multicolumn{1}{c|}{81.0  0.6} & 16.3  6.9 & 83.7  0.1 
            \\ 
            \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{50}  & 18.8  6.9 & \multicolumn{1}{c|}{\bf 81.3  0.6} & 13.4  4.1   & 84.1  0.2 
            \\ 
            \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{100} & 13.6  5.6 & \multicolumn{1}{c|}{81.0  0.5} & 16.8  7.1   & \bf 84.1  0.1 
            \\ 
            \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{150} & 16.0  3.6 & \multicolumn{1}{c|}{81.1  0.5} & 18.8  7.4   & 83.8  0.2 
            \\ 
            \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{250} & 13.2  3.1 & \multicolumn{1}{c|}{81.1  0.2} & 21.0  10.4  & 83.8  0.1 
            \\ \hline
            &   & \multicolumn{2}{c}{Acc. (\%)}                        
            & \multicolumn{2}{c}{Co-teaching Acc. (\%)} 
            \\ 
            \multicolumn{1}{c|}{\multirow{3}{*}{}} 
            & \multicolumn{1}{c|}{0.1} 
            & \multicolumn{2}{c|}{\bf 81.6  0.2} 
            & \multicolumn{2}{c}{\bf 84.5  0.1} 
            \\ 
            \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{0.3} & \multicolumn{2}{c|}{80.8  0.4}
            & \multicolumn{2}{c}{84.0  0.2} 
            \\ 
            \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{0.5} & \multicolumn{2}{c|}{81.1  0.8} & \multicolumn{2}{c}{84.4  0.2} 
            \\ \hline\hline
        \end{tabular}} 
        \label{tab::animal10}}
    \end{minipage}
    \hfill
    \begin{minipage}{0.44\textwidth}
        \centering
        \scalebox{0.9}{\subfloat[Ablation study on .]{
        \begin{tabular}{c|cccc}
            \hline\hline
              
            & 0.1 & 0.2 & 0.3 & 0.5 
            \\ \hline
            \multirow{4}{*}{Acc (\%)} 
            & \multicolumn{4}{c}{NestedCo-teaching} 
            \\  
            & 84.1 {} 0.1 & \bf 84.1 {} 0.1 & 83.3 {} 0.2 & 83.3 {} 0.2 
            \\ \cline{2-5} 
            & \multicolumn{4}{c}{DropoutCo-teaching} 
            \\  
            & 84.4 {} 0.1 & \bf 84.5 {} 0.1 & 84.0 {} 0.1 & 83.4 {} 0.3  
            \\ \hline\hline
        \end{tabular}} 
        \label{tab::ablation}}
    \end{minipage}\hfill
    \vspace{-5mm}
\end{table*}

\begin{table}[t]
    \caption{Test accuracy (\%) on Clothing1M~\cite{xiao2015learning} with different backbones: ResNet-18, ResNet-50, EfficientNet-B2~\cite{tan2019efficientnet}.
    Results with ``*" use a balanced subset or a balanced loss.}
    \label{tab::clothing1m_backbone}  
    \centering
    \resizebox{0.95\columnwidth}{!}{
    \begin{tabular}{cccc}
        \hline\hline
        \multicolumn{1}{c|}{\multirow{2}{*}{Method / Acc. (\%)}} 
        & \multicolumn{3}{c}{Backbones}           
        \\
        \multicolumn{1}{c|}{} 
        & ResNet-18 & ResNet-50 & EfficientNet-B2 
        \\ \hline
        \multicolumn{1}{c|}{Cross-Entropy}                       
        & 67.2~\cite{wei2020combating} & 69.2~\cite{li2020dividemix} & 69.8 
        \\
        \multicolumn{1}{c|}{DivideMix*~\cite{li2020dividemix}}  
        & --        & 74.8      & --              
        \\ \hline
        \multicolumn{4}{c}{\bf Ours} 
        \\
        \multicolumn{1}{c|}{Nested*}                             
        & 73.1      & 73.2      & 72.5            
        \\
        \multicolumn{1}{c|}{Nested+Co-teaching*}                 
        & \bf 74.9  & \bf 75.0  & \bf 73.5            
        \\
        \multicolumn{1}{c|}{Dropout*}                             
        & 72.5      & 72.9      & 72.5            
        \\
        \multicolumn{1}{c|}{Dropout+Co-teaching*}                
        & 74.0      & 74.0      & 73.4            
        \\ \hline\hline
    \end{tabular}}
\end{table}

\subsubsection{Ablation on }
We focus on how forget rate  of Co-teaching influences the performance in Table~\ref{tab::ablation} (b).
We present with the settings where Nested Dropout and Dropout have the best performance.
To be specific, we set  and  for the training, respectively.
For NestedCo-teaching, the performance drops with  increasing, and the accuracy remains  for .
The performance with  are not given in the Table~\ref{tab::ablation} (b) for simplicity.
Moreover, this performance boost of  compared to a single Nested Dropout network actually results from the ensemble estimation, not the cross-update mechanism of Co-teaching.
Therefore, considering that the estimated label noise ratio of ANIMAL-10N is , Co-teaching's core mechanism is not suitable for cases where the difference between  and the ground-truth noise ratio is large.
Furthermore, performance of DropoutCo-teaching again verify the above analysis where the best performance is achieved by .

\subsubsection{Ablation on different backbones}
As given in Table~\ref{tab::clothing1m_backbone}, the best performance is achieved by using ResNet-50 as backbone, which is 75.0\% in accuracy.
Even though EfficientNet-B2~\cite{tan2019efficientnet} cannot achieve the best performance possibly due to its capacity issue, NestedCo-teaching indeed improves upon a single Nested Dropout model.
The same applied to DropoutCo-teaching.
These results suggest the effectiveness of our approach with different backbones.


\section{Conclusion} 
\label{sec::conclusion}
In this paper, we investigate the problem of image classification in the presence of label noise.
In particular, we find that preventing networks from over-fitting the corrupted labels is one key problem in learning with noisy labels based on a bias-variance decomposition.
To this end, we introduce compression inductive bias to networks to increase the variance term so as to weaken the influence of the bias term which is associated with over-fitting.
This inductive bias is realized by applying simple compression regularizations such as Dropout \cite{srivastava2014dropout} and its variant named Nested Dropout \cite{rippel2014learning} to networks.
Notably, Nested Dropout is proved to learn ordered feature representations in this paper.
Therefore, this information sorting property can bring interpretability w.r.t.~channel importance to networks while filter out the noisy patterns.
Moreover, we combine these compression regularizations with Co-teaching \cite{han2018co}, leading to a two-stage method.
We then theoretically verify that this combination is in line with our bias-variance trade-off since Co-teaching further increases the variance term, hence further prevent networks from over-fitting.
Our method is validated on benchmark real datasets under synthetic label noise and real-world label noise including Clothing1M \cite{xiao2015learning} and ANIMAL-10N \cite{song2019selfie}.
Our method achieves comparable or even better performance than state-of-the-art approaches.
Our approach is simple compared to many existing methods.  
Therefore, we hope that our approach can serve as a strong baseline for future research on learning with noisy labels.


\section*{Acknowledgements}
We appreciate Qinghua Tao for helpful discussions, and also the anonymous reviewers for their insightful comments.


\bibliographystyle{IEEEtran}
\bibliography{egbib}

\section*{Appendix} \label{sec::app}

This appendix provides the proofs of this paper.
\begin{proof} [Proof of Theorem \ref{thm::decomp}]
Since  is a constant with respect to the model distribution , for a given input , we have

The following result from Theorem 3.1 in \cite{Brofos2019ABD} is in need.
\begin{lemma} \label{lm::decomp}
Let  and  and let  be a distribution with the same supports as a random probability distribution  for all  and . 
Then if ,

where .
\end{lemma}
Now by applying Lemma \ref{lm::decomp}, we find that the first term in \eqref{eq::decomp} can be decomposed as

where we have , and also . 
Note the proof follows if we plug in the above decomposition when computing .
Note that both  and  are constant with respect to the model .
This completes the proof.
\end{proof}


\begin{proof} [Proof of Theorem \ref{thm::ranking}]
We split the proof into two parts.

    \textbf{\textit{(i)}} [Proof of \eqref{eq::IYH}]:
    For the hidden representation , since we have \eqref{eq::permu} held, , and  without loss of generality, we exchange the -th and -th arguments:
    
    Therefore,  and  and  are identically distributed.
    Since ,  are arbitrarily chosen, we have  identically distributed.
    
    Similarly, for the joint distributions of  and  with ,  arbitrarily chosen, we have
    \vspace{-1mm}
    
    where the third equivalence holds for the permutation invariant \eqref{eq::permu},\eqref{eq::dec_permu}.
    Hence,  for arbitrary , , that is,  are identically distributed.

    Considering the formulation of mutual information, we have :
    
    This completes the proof of \eqref{eq::IYH}.



\textbf{\textit{(ii)}} [Proof of \eqref{eq::IYZ}]:
By the definition of \textit{Nested Dropout} in \eqref{eq::mask_nested} and \eqref{eq::CatGaussian}, we have
\vspace{-1mm}

\vspace{-1mm}
We first rewrite the above calculation equivalently as 

The above recursion can be solved and specified by 
\vspace{-1mm}

which suggests that

and for , we have

and also have

Hence, the equivalence holds if

More precisely, the equivalence holds if

Given the construction of the network, we obtain a Markov chain , which simply means that  is conditionally independent of  given  or  for .
Now, we need the following data processing inequality:
\begin{lemma}[Data processing inequality]
Let three random variables form the Markov chain , meaning that  is conditionally independent of  given . Then,

The equality holds if and only if .
\end{lemma}

Combining the above data processing inequality with both \eqref{eq::IYH} and \eqref{eq::net_struct}, we have

Note that  is due to the fact that

and .
This completes the proof.
\end{proof}


\begin{proof} [Proof of Theorem \ref{thm::co_decomp}]
    Recall that the taught student decoder is defined by
    
    where .
    For a given input ,
    
    where , and the last equation is according to \eqref{eq::decomp}.
    With Lemma \ref{lm::decomp} applied, the first term in \eqref{eq::decomp_co} can be decomposed as
    
    where we have , and also 
    
    Since , then we have  follows
     with .
    The proof follows if we plug in the above decomposition when computing .
    Note that both  and  are constant with respect to the model . 
    
    \textit{(i)} For the bias term, we have
    
    where .
    For , that is, , we have Co-teaching bias term satisfying 
    
    
    \textit{(ii)} For the variance term, we have
    
    where the last inequality is due to  as the output of the teacher network is derived by soft-max layer with . 
    Similarly, .
    In these regards, if we further have , then
    
    To conclude, if , then 
    
    This completes the proof.
\end{proof} 




\end{document}
