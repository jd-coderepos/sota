\documentclass[12pt]{article}
\usepackage[margin=25mm]{geometry}                \geometry{a4paper}                   \usepackage{graphicx}
\usepackage{amssymb,amsmath,amsthm,comment}

\DeclareMathOperator{\vbl}{\mathrm{vbl}}
\DeclareMathOperator{\Tr}{\mathcal{T}}
\DeclareMathOperator{\E}{\mathsf{E}}
\DeclareMathOperator{\size}{\mathrm{size}}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\let\eps=\varepsilon

\title{Probabilistic Constructions of Computable Objects and a Computable Version of Lov\'asz Local Lemma.}
\author{Andrei Rumyantsev, Alexander Shen\thanks{LIRMM CNRS \& UM~2, Montpellier; on leave from IITP RAS, Moscow. Supported by ANR NAFIT-008-01/2 grant. E-mail: \texttt{azrumuyan@gmail.com}, \texttt{alexander.shen@lirmm.fr}}}


\begin{document}
\maketitle

\begin{abstract}
A nonconstructive proof can be used to prove the existence of an object with some properties without providing an explicit example of such an object. A special case is a probabilistic proof  where we show that an object with required properties appears with some positive probability in some random process. Can we use such arguments to prove  the existence of a \emph{computable} infinite object? Sometimes yes: following~\cite{rumyantsev-positive}, we show how the notion of a layerwise computable mapping can be used to prove a computable version of Lov\'asz local lemma.
\end{abstract}

\section{Probabilistic generation of infinite sequences}

We want to show that one can use probabilistic arguments to prove the existence of inifinite objects (say, infinite sequences of zeros and ones) with some properties. So we should discuss first in which sense a probabilistic algorithm can generate such a sequence. The most natural approach: consider a Turing machine without input that has an (initially empty) work tape and a write-only output tape where machine prints bits sequentially. Using fair coin tosses, the machine generates a (finite or infinite) sequence of output bits. The output distribution of such a machine  is some probability distribution  on the set of all finite and infinite sequences. Distributions on this set are determined by their values on cones  (of all finite and infinite extensions of a binary string ). The function  that corresponds to the distribution  satisfies the following conditions:
    
(Here  denotes the empty string.) Any non-negative real function that satisfies these conditions corresponds to some probability distribution on the set of finite and infinite binary sequences. The output distributions of probabilistic machines correspond to functions  that are \emph{lower semicomputable}; this means that some algorithm, given a binary string , computes a non-decreasing sequence of rational numbers that converges to .

Now we are ready to look at the classical result of de Leeuw -- Moore -- Shannon -- Shapiro: \emph{if some individual sequence  appears with positive probability as an output of a probabilistic Turing machine, this sequence is computable}. Indeed, let this probability be some ; take a rational threshold  such that , and consider some prefix  of  such that . (Such a prefix exists, since  for prefixes of  converges to .)
Starting from , we can compute the next prefix of  by finding a son where  exceeds . The correct son satisfies this condition, and no branching is possible: if for two sons the value exceeds , then it would exceed  for the father.

This result can be interpreted as follows: \emph{if our task is to produce some specific infinite sequence of zeros and ones, randomization does not help} (at least if we ignore the computational resources). However, if our goal is to produce \emph{some} sequence with given properties, randomization can help. A trivial example: to produce a noncomputable sequence with probability  it is enough to output the random bits.

All these observations are well known, see, e.g., the classical paper of Zvonkin and Levin~\cite{zvonkin-levin}. For a less trivial example, let us consider another result (proved by N.V.~Pet\-ri) mentioned in this paper: \emph{there exists a probabilistic machine that with positive probability generates a sequence  such that \textup{(1)}~ contains infinitely many ones; \textup{(2)}~the function i\omega has no computable upper bound}. (In the language of recursion theory, this machine generates with positive probability a characteristic sequence of a hyperimmune set.) A nice proof of this result was given by Peter Gacs; it is reproduced in the next section (we thank M.~Bondarko for some improvements in the presentation).

\section{Fireworks and hyperimmune sequences}

Consider the following puzzle. We come to a shop where fireworks are sold. After we buy one, we can test it in place (then we know whether it was good or not, but it is not usable anymore, so we have to buy a new one after that), or go home, taking the untested firework with us. We look for a probabilistic strategy that with 99\% probability wins in the following sense: it \emph{either finds a bad firework during the testing} (so we can sue the shop and forget about the fireworks) \emph{or takes home a good one}.

Here is the solution: \emph{take a random number  in  range, make  tests \textup(less if the failure happens earlier\textup{);} if all  tested fireworks were good, take home the next one}. To prove that it wins with  probability, note that the seller does not get any information from our behavior: he sees only that we are buying and testing the fireworks; when we take the next one home instead of testing, it is too late for him to do anything. So his strategy is reduced to choosing some number  of good fireworks sold before the bad one. He wins only if , i.e., with probability at most~.

Another description of the same strategy: we take the first firework home with probability  and test it with probability ; in the second case, if the firework was good, we take the second one,  bringing it home with probability  and testing it with probability , etc.

Why this game is relevant? Assume we have a program of some computable function  and want to construct probabilistically a total function  not bounded by  if  is total; if  is not total, any total function  is OK for us. (It is convenient to consider a machine that constructs a total integer-valued function  and then convert it into a bit sequence by putting  zeros after th occurence of  in the sequence.) We consider  as ``fireworks'';  is considered as a good one if the computation of  terminates. First we buy ; with probability  we ``take'' it and with probability  we ``test'' it. \emph{Taking}  means that we run this computation until it terminates and then let . If this happens, we may relax and let all the other values of  be zeros. (If the computation does not terminate, i.e., if we have taken a bad firework, we are out of luck.) \emph{Testing}  means that we run this computation and at the same time let , , etc. until the computation terminates. If  is undefined,  will be zero function, and this is OK since we do not care about non-total functions~. But if  is defined, at some point testing stops, we have some initial fragment of zeros , and then consider  as the next firework bought and test [take] it with probability  [resp. ]. For example, if we decide to test it, we run the computation  until it terminates, and then let . And so~on.

In this way we can beat a given computable function  with probability arbitrarily close to~. We need more: to construct with positive probability a function not bounded by \emph{any} total computable function. How can we do this? Consider all the functions as functions of two natural arguments, using a computable bijection between  and .  Use th row  in the table of such a function to beat th potential upper bound with probability . To beat the upper bound, it is enough to beat it in some row, so we can deal with all the rows in parallel, and get error probability at most .

\section{Computable elements of closed sets}

Let us return now to the original question: can we use probabilistic arguments to construct a computable sequence with some property? As we have seen, if we are able to construct a probabilistic machine that generates some \emph{specific} sequence with positive probability, we can then conclude that this specific sequence is computable. However, we do not know arguments that follow this scheme, and it is difficult to imagine how one can describe a specific sequence that it is actually computable, and prove that it has positive probability --- without actually constructing an algorithm that computes it.

Here is another statement that may be easier to apply. In this statement we use the standard topology on the Cantor space of infinite --sequences (=~the product topology on ).

\begin{proposition}\label{closed-almost-everywhere}
Let  be a closed set of infinite sequences. Let  be a probabilistic machine \textup(without input\textup) whose output sequence belongs to  with probability . Then  contains a computable element.
\end{proposition}

\begin{proof}
Indeed, consider the output distribution  of the machine  and take a computable branch in the binary tree along which the probabilities  remain positive (this is possible since the function  is lower semicomputable). We get some computable sequence~. If , then some prefix of  has no extensions in  (recall that  is closed). This prefix has positive probability by construction, so our machine cannot generate elements in  with probability~. This contradiction shows that .
\end{proof}

In the following sections we give a specific example when this approach (in a significantly modified form) can be used.

\section{Lov\'asz local lemma}

Let  be a sequence of mutually independent random variables; each variable  has a finite range . (In the simplest case  are independent random bits.) Consider some family  of events; each  depends on a finite set of variables, denoted .   Informally speaking, Lov\'asz local lemma (LLL) guarantees that these events do not cover the entire probability space if each of them has small probability and the dependence between the events (caused by common variables) is limited. Intuitively, the events are undesirable for us and we want to avoid all of them; LLL says that this is possible (with positive probability).

To make the statement exact, we need to introduce some terminology and notation. Two events  are \emph{disjoint} if they do not share variables, i.e., if ; otherwise they are called \emph{neighbors}. For every  let  be the neighborhood of , i.e., the set of all events  that have common variables with . Each event is its own neighbor.

\begin{proposition}[Finite version of LLL, \cite{original-lovasz}]
     \label{finite}
Consider a finite family  of events. Assume that for each event  a rational number  is fixed such that
    
for all . Then

\end{proposition}

Note that we skip the event  in the right hand side of the condition.

This bound was originally proved~\cite{original-lovasz} by a simple (though a bit misterious) induction argument.

Note that the product in the right hand side of  is positive (though it can be very small), and therefore there exists an assignment that avoids all the forbidden events. This existence result can be easily extended to infinite families:

\begin{proposition}[Infinite version of LLL]
     \label{infinite}
Consider a sequence  of events. Assume that each event  has only finitely many neighbors in~, and that for each event  a rational number  is fixed such that
    
for all . Then there exists an assignment that avoids all the events .
\end{proposition}

\begin{proof}
This is just a combination of finite LLL and compactness argument (K\"onig's lemma). Indeed, each event from  is open the the product topology; if the claim
is false, these events cover the entire (compact) product space, so there exists
a finite subset of events that covers the entire space, which contradicts the
finite LLL.
\end{proof}

Our goal is to make this infinite version effective and get a \emph{computable version} of LLL.  First, we need to add some computability conditions. Let us assume that the range of  is , where  is a computable function of . We assume that  has a rational-valued probability distribution that is computable given . We assume also that events   are effectively presented, i.e., for a given  one can compute the list of all the variables in  and the event itself (i.e., the list of tuples that belong to it).  Finally, we assume that for each variable  only finitely many events involve this variable, and the list of those events can be computed given .

\begin{theorem}[Computable infinite version of LLL]
\label{computablelll}
Suppose there is a rational constant  and a
computable sequence  of rational numbers in  such that
     
for all . Then there exists a computable assignment that
avoids all .
\end{theorem}

Note that the computability restrictions look quite naturally and that we only need
to make the upper bounds for probability just a bit stronger (adding some constant factor ). It should not be a problem for typical applications of LLL; usually this stronger bound on  can be easily established.

This theorem is the main result of the paper. Lance Fortnow formulated this statement and conjectured that it should follow somehow from the Moser--Tardos effective proof of finite LLL~\cite{moser-tardos}.  It turned out that it is indeed the case (the proof, found by the first author, was published as \texttt{arxiv} preprint~\cite{rumyantsev-positive}). The  proof goes as follows: according to Proposition~\ref{closed-almost-everywhere}, it is enough to construct a computable probability distribution on infinite sequences such that every  has zero probability. To construct such a distribution, we need first to extend the class of probabilistic machines, and then construct a machine of this extended type, based on Moser--Tardos algorithm.

\section{Rewriting machines and layerwise computability}

Now we change the computational model and make the output tape of a probabilistic machine (without input) \emph{rewritable}: the machine can change several times the contents of a cell, and only the final value matters. (We may assume that th cell may contain integers in  range, and that initially all cells contain zeros.) The final value is defined if a cell changes its value finitely many times during the computation. Of course, for some values of random bits it may happen that some cell gets infinitely many changes. In this case we say that the output sequence of the machine is undefined.

If the output sequence is defined with probability , we get an almost everywhere defined mapping from Cantor space (sequences of bits produced by coin tossing) into the space of all assignments: the sequence of randomly generated bits is mapped to the output sequence. This mapping defines the output distribution on the assignments (the image of the uniform distribution on random bits). This distribution may be non-com\-put\-able (e.g., for every lower semicomputable real  it is easy to generate  with probability  and  with probability ). However, we get a computable output distribution if we impose some restriction on the machine.

Here it is: \emph{for every output cell  and for every rational  one can effectively compute integer  such that the probability of the event ``the contents of -th cell changes after step '', taken over all possible random bit sequences, does not exceed~}.

\begin{proposition}\label{output-layerwise}
 In this case the limit content of every output cell is well defined with probability , and the output distribution on the sequences of zeros and ones is computable.
\end{proposition}

\begin{proof}
Indeed, to approximate the probability of the event ``output starts with '' for a sequence  of length  with error at most , we find  for  first cells (i.e., for ). Then we take  greater than all these values of , and simulate first  steps of the machine for all possible combinations of random bits.
\end{proof}

An almost everywhere defined mappings of Cantor space defined by machines with described properties, are called \emph{layerwise computable mappings}. Initially they appeared in the context of algorithmic randomness~\cite{hoyrup-rojas}. One can show that such a mapping is defined on all Martin-L\"of random sequences. Moreover, it can be computed by a machine with a write-only output tape if the machine additionally gets the value of randomness deficiency for the input sequence. This property can be used (and was originally used) as an equivalent definition of layerwise computable mapping. (The word ``layerwise'' reflects this idea: the mapping is computable on all ``randomness levels''.)

This aspect, however, is not important for us now; all we need is to construct a layerwise computable mapping whose output distribution avoids all the undesirable events  with probability~, and then apply Proposition~\ref{closed-almost-everywhere}.

\section{Moser--Tardos algorithm and its properties}

\subsection{The algorithm}

First, we recall the Moser--Tardos probabilistic algorithm that finds a satisfying assignment for the finite case. To make this paper self-contained, we reproduce the arguments from~\cite{moser-tardos} (we need to use them in a slightly modified form).

\medskip\noindent
Moser--Tardos algorithm is very simple and natural:

\begin{itemize}
\item start by assigning random values to all  independently (according to their distributions);
\item while some of the events  are true, select one of these events and resample all the variables in  using fresh independent random values.
\end{itemize}

\noindent
Several remarks:

\begin{itemize}

\item The resampling caused by some  does not necessarily makes this event false; it may remain true and can be resampled again later.

\item Resampling for  can affect its neighbors; in particular, some of them could be false before and become true after the resampling (then further resampling is needed).

\item The algorithm may terminate (when the current assignment makes all  false) or continue indefinitely. The latter can happen even for finite case (though with zero probability, under the conditions of LLL, as we will see). In the infinite case the algorithm does not terminate: even if all  are false at some moment, we need an infinite amount of time to check them all.

\item To describe the algorithm fully, one needs to specify some (deterministic or probabilistic) rule that says which of the (true)  should be resampled. We assume that some rule is fixed. In the finite case the choice of the rule does not affect the analysis at all; for the infinite case we assume that the \emph{first true  is selected} (the one with minimal ).

\end{itemize}

\subsection{The analysis: outline}
Let us now reproduce Moser--Tardos analysis for the finite case. They show that the expected number of steps (resampling operations) does not exceed
   
This implies that the algorithm terminates with probability . How do they get this bound?

\begin{itemize}

\item For each resampling performed during the algorithm, some tree is constructed. Its vertices are labeled by events ; informally, the tree reflects the dependence between events that were resampled on this and previous steps.

\item Running Moser--Tardos algorithm step by step, we get a sequence of trees (one per each resampling); all the trees in this sequence will be different. (This sequence is a random variable depending on the random choices made when choosing the random values and the events for resampling.)

\item The number of resampling steps is the number of trees appearing in this sequence, therefore the expected number of steps is

where  includes all trees that may appear. (It is important here that a tree cannot appear twice in the same run of the algorithm, see above.)

\item This sum is now estimated by splitting it accordingly to root labels of the trees. Let  be a set of trees in  that have root label  (such a tree appears when  is resampled). Moser and Tardos show that

this is done by comparing the probabilities in question to some other probabilities (to appear in a process of Galton--Watson type, see below). Then we sum these inequalities for all~.
\end{itemize}

\subsection{The set  and the construction of the trees}

Let us now go into the details.

\begin{itemize}
\item The set  contains finite rooted trees whose vertices are labeled by events , with the following restrictions: the sons of a vertex with label  should be labeled by different neighbors of  (recall that  is a neighbor of itself), and these neighbors should be disjoint events.

\item Now we explain how trees are constructed. Assume that the events  are resampled (in this order) by the algorithm, and we now are at step  and construct a tree  that corresponds to  (=appears at step  of the algorithm). We consider the resampling operations in the reverse order, starting from . Initially the tree consists of a root vertex only; the root is labeled by  (or just by ; we may identify events with their indices). Then we consider events . If the next event  in this sequence is not a neighbor of any current tree vertex (its label), we skip it. If it is, we look at all these vertices (labeled by neighbors), take one that is most distant from the root (breaking the ties arbitrarily), and attach to it a new son labeled by  (by , if we use indices as labels).
\bigskip

\begin{figure}[h]
\begin{center}
\includegraphics{journal-1.pdf}\hspace*{20mm}
\includegraphics{journal-2.pdf}
\end{center}
\end{figure}
An example: consider four events  with neighbor pairs  and , and assume that the sequence of resamplings is . Then we start with root labeled , then skip  (since it is not a neighbor of ), then add  as a son of , then add  as another son of  (note that  and  are not neighbors, so  cannot be attached to ), and finally add first  as a son of  or  (the first possibility is shown; note that  is also a neighbor of the root, but we select a vertex on a maximal distance from the root).

\item The resulting tree belongs to . Indeed,

\begin{itemize}
\item each son is a neighbor of its father by construction;
\item different sons of the same father are not neighbors, otherwise the younger brother will become a son of the older one instead of becoming his brother.
\end{itemize}

\item The last argument shows in fact more: any two vertices of the same height (at the same distance from the root) are not neighbors. (This property will be used later.)

\item Two trees that appears at different steps of the algorithm, are different. It is obvious if they have different root labels. If they both have some  as the root label, then each tree includes all previous resamplings of , so the number of -labels is different for both trees.

\end{itemize}

\subsection{The probability estimate}

Our next goal: for a given  we upperbound the probability of the event `` appears at some stage of Moser--Tardos algorithm''. This bound is provided by the following lemma.

\begin{lemma}\label{treeprob}
For every tree, the probability of the event ``T appears at some stage of Moser--Tardos algorithm'' does not exceed of the product of  for all labels in .
\end{lemma}

\noindent
(If the label  appears several times as a label in , its probability appears several times in the product.)

To prove this lemma, it is convenient to specify more explicitly how the random values (used as original values and for resampling) are chosen. Long ago people used printed tables of random numbers. These tables were prepared in advance by a random process; when a fresh random number was needed, it was taken from the table and then crossed out (to prevent its future use). In the same way we assume that for each variable  a sequence of independent random values  is prepared in advance. When a new random value is needed, we use the first unused value from this sequence. (So  is used for the initialization, and  are then used for resampling of events that involve~.)

Now the crucial observation: \emph{the tree  that appears at some step, determines uniquely which random values were used during the resampling operations
corresponding to 's vertices}. Indeed, a given variable  appears at most once at every height level of the tree (as we have mentioned above), and the height ordering agrees with the temporal ordering of the corresponding resampling operations. No other resampling step (in between the resampling operations included in the tree) could involve  due to tree construction (such a resampling operation cannot be skipped).

Therefore, the event `` appears'' is included in the intersection of the independent events of probability  for all labels  in . The lemma is proven.

\subsection{Comparison with the Galton--Watson process}

Now we use the assumption of the finite LLL, the upper bound for . Using this bound, we need to show that

for every  (We have already noted that this gives the required bound for the expected number of steps.) This is done by the following nice (and mysterious) trick.

Consider the following probabilistic process  (of Galton--Watson type) that produces some tree in . The process starts with a tree that has only one vertex, the root, labeled by . Then for each neighbor  of  (including  itself) we decide whether the root will have a son labeled by . (Only one son with a given label can appear.) Namely, son with label  appears with probability . Then the same is done for each new vertex : each its neighbor  is attached as a son of  with probability , and so on. This process may never terminate (then we get an infinite tree), but may also terminate and produce a finite tree.

For example, if event  has neighbors  (and no others), the probability to obtain a tree with only one vertex (the root) in this process is
     
(this tree appears when none of three possible sons are attached). And the probability to get a tree with root  and its son  (and no other vertices) is
     
assuming that  has no other neighbors except for  and  itself.

Now the Galton--Watson (GW-) process (for a given ) is described, and we claim that for every tree  (with root label ) we have
    
Since only one tree can appear in GW-process, the sum of probabilities in the right hand side (over all ) does not exceed~, and we get the promised bound.

So it remains to prove our claim. According to the Lemma~\ref{treeprob}, the upper bound for the left-hand side can be obtained by multiplying the bounds for  for all labels  that appear in the tree.  In our example, if the tree consists of root  alone, we get (using the assumption of LLL)
   
Attaching the son  to the root , we multiply the left hand side by
   
in the right hand side we replace  (probability of not having the son  by  (probability of having it) and add new factors  (since  has no sons in the tree), which gives exactly the same factor. The same happens when we add more vertices.

This finishes the Moser--Tardos analysis. Let us put all the steps together again: knowing that

we now sum these inequalities for all  and get

as required.
\smallskip

\subsection{Infinite case}

Now we need to analyze the infinite case. We have already described the algorithm; now we need to show that it indeed provides a layerwise computable mapping with required properties. This means that

\begin{itemize}

\item
almost surely every variable  is changed only finitely many times and, moreover, the probability of the event `` changes after first  steps of the algorithm'' effectively converges to zero as ;

\item
the final values of the variables avoid all  (make them false).

\end{itemize}

To prove both statements, it is enough to show that

as , and the convergence is effective (for every rational  one can compute some  such that this probability is less than  when ). Indeed, every variable is involved in finitely many events (and we can compute the list of these events); note also that if some  is false for the final values of the variables, first of these  will be resampled infinitely many times (and this happens with zero probability).

We will show this effective convergence in two steps. First, consider some  and the first  events . While some of these events are true, the algorithm will never resample events  with , so it behaves exactly as the finite Moser--Tardos algorithm applied to . So we can use the result proved earlier: if  is the first moment when all  are false,
	
(we do not need to use the additional factor  yet). This means that  is finite almost surely, and the probability of  converges to  effectively as  (due to Tschebyshev inequality).

However, it is not enough for our purposes. At the moment  all the events  are false, but some of the events  with  may be true. Resampling such , we may change the variables that appear in , and this change may make some of these events true again, thus triggering further resampling operations. So we need a more detailed analysis.

Let us look how some  among  may become true again at some moment . This can happen only if some neighbor of , say, , was resampled between  and . At that moment  was true, so the are two possibilities:
\begin{itemize}
\item  (then  can be true at the moment );
\item , in this case  was false at the moment  and became true later.
\end{itemize}
In the second case ( was false and then became true) some neighbor of , say, , was resampled before  became true. Again, we have two cases  and ; in the latter case  became true because some neighbor  of  was resampled, etc. We come to the following crucial observation: \emph{if  is resampled after the moment , there is a chain of neighbors that starts with  and ends with some  with , and all elements of this chain appear in the tree for the -resampling}. For fixed  and very large  the length of this chain (and therefore the size of the tree) is big, since all events that are close to  are included in . And for big trees the additional factor  decreases significantly the probability of their appearance.

Indeed, the inequality

 (for every ) now gets an additional factor  in the right hand side (since we add factor  for every vertex). We can now proceed as follows. Fix some . For an arbitrary natural  take  large enough, so that  contain all events that are at distance  or less from  in the neigborhood graph. Then the event `` is resampled after time '' is covered by two events:

\begin{itemize}
\item ;
\item  was resampled after .
\end{itemize}

The probability of the first event () effectively converges to  as  increases, so it remains to bound the probability of the second event. As we have seen, this event happens only if a large tree ( or more vertices) with root  appears during the algorithm, so this probability is bounded by
 
so we get what we wanted.

\medskip
This finishes the proof of Theorem~\ref{computablelll}.

\section{Corollaries}

We conclude the paper by showing some special cases where computable LLL can be applied.

A standard illustration for LLL is the following result:
\emph{a CNF where each clause contains  different variables and
has at most  neighbors, is always satisfiable}.
Here neighbors are clauses that have common variables.

Indeed, we let  for all the events and note that
  
since the expression in square brackets is approximately .

This was about finite CNFs; now we may consider \emph{effectively presented} infinite CNFs. This means that we consider CNFs with countably many variables and clauses (numbered by natural numbers); we assume that for given  we
can compute the list of clauses where th variable appears, and for given
 we can compute th clause.

\begin{corollary}
For every effectively presented infinite CNF where each clause contains  different variables and has at most  neighbors, one can find a computable
    assignment that satisfies it.
\end{corollary}

\begin{proof}
Indeed, the same choice of  works, if we choose  close to  (say, ).
\end{proof}

Similar argument can be applied in the case where there are clauses
of different sizes. The condition now is as follows: for every variable there
are at most  clauses of size  that involve this variable,
where  is some constant. Note that now we do not assume
that every variable appears in finitely many clauses, so the notion of
effectively presented infinite CNF should be extended: we assume that for each
 and for each  one can compute the list of clauses of size  that
include .

\begin{corollary}\label{variable-cnf}
    For every  there exists some  such that every effectively presented infinite CNF where each variable appears in at most  clauses of size  \textup(for every \textup) and all clauses have size at least , has a computable satisfying assignment.
\end{corollary}

\begin{proof} Let us consider first a special case when each variable appears
only in finitely many clauses. Then we are in the situation covered by
Theorem~\ref{computablelll}, and we need only to choose the values of .
These values will depend on the size of the clause: let us choose
  
for clauses of size , where  is some constant. In fact, any constant
between  and  will work, so we can use, e.g., .
So we need to check (for clause  of some size~ and for some constant ) that
  
(in fact we can omit the clause  in the product, but this does not matter).
Note that for each of  variables in  there are at most 
clauses of size  that involve it. So together  has at most 
neighbors of size . So it is enough to show that
  
Taking th roots (we replace  by , but this only makes the requirement stronger) and using that , we see that it is enough
to show that
  
Since the series  is converging, this is
guaranteed for large  and for  sufficiently close to .

So we have proven Corollary~\ref{variable-cnf} for the special case when
each variable appear only in finitely many clauses (and we can compute
the list of those clauses).

The general case is easily reducible to this special one. Indeed, fix some  and delete from each clause -fraction of its variables with minimal indices. The CNF becomes only harder to satisfy. But if  is small enough, the condition of the theorem (the number of clauses with  variables containing a given variable is bounded by ) is still true for some , because the deletion makes the size of clauses only slightly smaller and decreases the set of clauses containing a given variable. And in this modified CNF each variable appears only in clauses of limited size (it is deleted from all large enough clauses).
\end{proof}

Let us note an important special case. Assume that  is a set of binary strings that contains (for some fixed  and for every ) at most  strings of length . Then one can use LLL to prove the existence of an infinite (or bi-infinite) sequence  and a number  such that  does not have substrings in  of length greater than . There are several proofs of this statement;  the most direct one is given in~\cite{miller-two-notes}; one may also use LLL or Kolmogorov complexity, see~\cite{rumyantsev-1,rumyantsev-2}.

Joseph Miller noted that his proof (given in~\cite{miller-two-notes}) can be used to show that for a decidable (computable) set  with this property one can find a computable  that avoids long substrings in . Konstantin Makarychev extended this argument to bi-infinite strings (personal communication). Now this result becomes a special case of Corollary~\ref{variable-cnf}: places in the sequence correspond to variables, each forbidden string gives a family of clauses (one per each starting position), and there is at most  clauses of size  that involve a given position (and this number is bounded by  for slightly bigger  and large enough ).

Moreover, we can do the same for 2-dimensional case: having a decidable
set  of rectangular patterns that contains at most  different
patterns of size (=~area) , one can find a number  and a computable 2D configuration (a mapping ) that does not contain patterns from  of size  or more. (It is not clear how to get this result directly, without using Moser--Tardos technique.)

Authors are grateful to Lance Fortnow who suggested to apply Moser--Tardos
technique to get the infinite computable version of LLL, and to the anonymous referees for their comments.

\begin{thebibliography}{9}

\bibitem{original-lovasz}
Erd\"os P., Lov\'asz L., Problems and results of 3-chromatic hypergraphs and some related questions. In: A.~Hajnal, R.~Rado, V.T.~S\'os, Eds. Infinite and Finite Sets. North-Holland, \emph{II}, 609--627.

\bibitem{hoyrup-rojas}
Hoyrup M., Rojas C., Applications of Effective Probability Theory to Martin-L\"of Randomness. \emph{Lectures Notes in Computer Science}, 2009, v.~5555, p.~549--561.

 \bibitem{miller-two-notes}
 Miller J.,
 \emph{Two notes on subshifts}. Available from\\
 \texttt{http://www.math.wisc.edu/~jmiller/downloads.html}

\bibitem{moser-tardos}
Moser R.A., Tardos G., A constructive proof of the general Lov\'asz Local Lemma, \emph{Journal of the ACM}, 2010,  57(2), 11.1--11.15. See also:\\ \texttt{http://arxiv.org/abs/0903.0544}.


\bibitem{rumyantsev-1}
 Rumyantsev A.,
 Forbidden Substrings, Kolmogorov Complexity and Almost Periodic Sequences,
 \emph{STACS 2006, 23rd Annual Symposium on Theoretical Aspects of Computer Science},
 Marseille, France, February 23--25, 2006. Lecture Notes in Computer Science,
 3884, Springer, 2006, p.~396--407.

 \bibitem{rumyantsev-2}
 Rumyantsev A.,
 Kolmogorov Complexity, Lov{\'a}sz Local Lemma and Critical Exponents.
 \emph{Computer Science in Russia}, 2007, Lecture Notes in Computer Science, 4649,
 Springer, 2007, p.~349-355.

\bibitem{rumyantsev-negative}
Rumyantsev A., \emph{Everywhere complex sequences and probabilistic method},\\ \texttt{arxiv.org/abs/1009.3649}

\bibitem{rumyantsev-positive}
Rumyantsev A., \emph{Infinite computable version of Lovasz Local Lemma},\\ \texttt{arxiv.org/abs/1012.0557
}

\bibitem{zvonkin-levin}
Zvonkin A.~K., Levin L.~A., The complexity of finite objects and the development of the concepts of information and randomness by means of the theory of algorithms, Uspekhi Matematicheskikh Nauk, 1970, v.~25, no.~6~(156), p.~85--127.


\end{thebibliography}

\end{document}
