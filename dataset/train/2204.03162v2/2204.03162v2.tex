

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{5457} \def\confName{CVPR}
\def\confYear{2022}

\graphicspath{{images/}{images/vilt_ot_heatmap}}



\begin{document}

\title{Winoground: Probing Vision and Language Models\\ for Visio-Linguistic Compositionality} 

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\author{Tristan Thrush\thanks{Equal contribution. TT, AS, and DK conducted most of the work for this paper when they were at Facebook AI Research.}, Ryan Jiang, Max Bartolo,\\ Amanpreet Singh, Adina Williams, Douwe Kiela, Candace Ross\samethanks\\
 Hugging Face;  Facebook AI Research;  University of Waterloo;  University College London\\
{\tt\small tristan@huggingface.co, ccross@fb.com}
}
\maketitle



\begin{abstract}
We present a novel task and dataset for evaluating the ability of vision and language models to 
conduct visio-linguistic compositional reasoning, which we call Winoground. Given two images and two captions, the goal is to match them correctly---but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set of fine-grained tags to assist in analyzing model performance. We probe a diverse range of state-of-the-art vision and language models and find that, surprisingly, none of them do much better than chance. Evidently, these models are not as skilled at visio-linguistic compositional reasoning as we might have hoped. We perform an extensive analysis to obtain insights into how future work might try to mitigate these models' shortcomings. We aim for Winoground to serve as a useful evaluation set for advancing the state of the art and driving further progress in the field. The dataset is available at \scriptsize{\url{https://huggingface.co/datasets/facebook/winoground}}.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Despite the impressive performance of pretrained vision and language transformers on a wide variety of multimodal tasks~\cite{lu2019vilbert, li2019visualbert, radford2021clip}, they remain poorly understood~\cite{dou2021empirical,cao2020behind,li2020closer,singh2020we}. One important question is to what extent such models are able to conduct unimodal and multimodal compositional reasoning. For humans, the visual differences between images depicting ``the tree is in the shopping cart’’ and ``the shopping cart is in the tree’’ will be blatantly obvious, even when the words in the captions are identical---but is the same true for machines?

While matching simple images and captions may seem almost too trivial a task, recent work in NLP has shown that transformers are often remarkably insensitive to word order \cite{sinha2021matterslittle}. Understanding the relationship between text in captions and corresponding visual content is a fundamental goal of computer vision, and the fact that different word orders correspond to wildly different visual depictions should be reflected in the capabilities of our models.

Motivated by this, we propose a novel task, called Winoground, for measuring visio-linguistic compositional reasoning, 
whereby two images and two captions have to be matched correctly; both captions contain exactly the same set of words, ordered in such a way that each describes primarily one of the images. To perform well on Winoground, models must not only encode text and images well (i.e., be sensitive to the compositional structure present in each modality), but they also must be able to synthesize information across the two modalities.

\begin{figure}[!t]\centering
    \subfloat[\centering some plants surrounding a lightbulb]{{\includegraphics[height=3cm]{ex_155_img_0.png} }}\qquad
    \subfloat[\centering a lightbulb surrounding some plants]{{\includegraphics[height=3cm]{ex_155_img_1.png} }}\caption{An example from Winoground. The two sentences contain the same words but in a different order. The task of understanding which image and caption match is trivial for humans but much harder for vision and language models. Every model that we tested (UNITER, ViLLA, VinVL, VisualBERT, ViLT, LXMERT, ViLBERT, UniT, FLAVA, CLIP, VSE++, and VSRN) fails to correctly pair the images and captions, except the large checkpoint of ViLLA by a very thin margin (0.00013 confidence).}\label{fig:teaser-ex}\end{figure}

We draw inspiration from the Winograd Schema Challenge \cite{levesque2012winograd}, which tests the commonsense capabilities of models. In the challenge, a model is given two sentences that minimally differ and is tasked with performing coreference resolution. The Winograd twin sentence format has been used for a variety of language-related tasks \cite{rudinger2018gender, zhao2018gender, sakaguchi2020winogrande}.
In this work, we study the image-grounding of twin sentences with identical but differently ordered words.

Winoground was hand-crafted by expert annotators and is labeled with a rich set of fine-grained tags to assist in analyzing model performance. In efforts to shed better light on what exactly models learn, the NLP community has designed a wide variety of ``probing tasks’’: specialized, targeted tasks meant specifically for evaluation. The primary purpose of Winoground is to serve as a probing task for vision and language models. See \cref{fig:teaser-ex} for an example.

We evaluate a variety of state-of-the-art vision and language (V\&L) transformers \cite{gan2020villa,li2019visualbert,lu2019vilbert,hu2021unit,chen2020uniter,tan2020lxmert,radford2021clip,zhang2021vinvl,kim2021vilt,singh2022flava} and RNN-based models \cite{faghri2018vse,li2019vsrn}. Surprisingly,
all of the models rarely---and if so only barely---outperform chance. Our findings indicate that the visio-linguistic compositional reasoning capabilities of these models fall dramatically short of what we might have hoped.

In what follows, we introduce the Winoground task and dataset. We then describe the models we tested and discuss our findings. Next, we conduct an analysis of the performance of different models. We hope that insights from this work will lead to more robust vision and language models.

\section{Related Work}







\textbf{Visio-linguistic stress testing.} There are a number of existing multimodal stress tests about correctly understanding implausible scenes \cite{choi2012context}, exploitation of language and vision priors \cite{chao2017being, goyal2017making}, single word mismatches \cite{shekhar2017foil}, hate speech detection \cite{hosseinmardi2015detection,zhong2016content,gomez2020exploring, kirk2021hatemoji}, memes \cite{kiela2020hateful,suryawanshi2021trollmeme}, ablation of one modality to probe the other \cite{frank-etal-2021-vision}, distracting models with visual similarity between images \cite{hu2019evaluating, bogin2021covr}, distracting models with textual similarity between many suitable captions \cite{ding2016understanding, akula2020words}, collecting more diverse image-caption pairs beyond the predominately English and North American/Western European datasets \cite{liu-etal-2021-visually}, probing for an understanding of verb-argument relationships \cite{hendricks2021probing}, counting \cite{parcalabescu2021seeing}, or specific model failure modes \cite{singh2019towards,sidorov2020textcaps}. Many of these stress tests rely only on synthetically generated images, often with minimal visual differences, but no correspondingly minimal textual changes \cite{vedantam2021curi}. Other datasets test models with a single caption \cite{suhr2017corpus} or a single image \cite{johnson2017clevr, bitton-etal-2021-automatic}. There are also purely visual stress tests with naturalistic images: ImageNet-C/ImageNet-P \cite{hendrycks2019robustness} tests models on perturbations for a variety of image features.  Unlike Winoground, these stress tests tend to come from existing datasets that have images and text from typical training domains, such as Conceptual Captions \cite{sharma2018conceptual}, COCO \cite{lin2014microsoft}, Visual7W \cite{zhu2016visual7w} and VQA \cite{antol2015vqa,goyal2017making}. None of them hold the set of words constant in the captions, which is what allows us to carefully test for compositional reasoning without any biases stemming from the presence of altogether different words. While it is theoretically possible for unstructured bag of words models to do well on these previous datasets, that is not possible on Winoground.

\textbf{Probing.} Measuring what exactly a model knows about word order and linguistic structure has been explored in natural language processing. Sinha et al. \cite{sinha2021matterslittle} found that word order information does not have a large impact on performance when pretraining large transformer language models, across a variety of metrics. This suggests that transformers use high-level word co-occurence statistics, which gives the illusion of an understanding of word order. Other work in this space has tried to understand what models know about syntax \cite{linzen2016assessing, gulordava2018, williams-etal-2018-latent, hu-etal-2020-systematic, gauthier-etal-2020-syntaxgym,  sinha-etal-2021-unnatural, parthasarathi-etal-2021-sometimes-want} or the complex interaction between syntactic and semantic categories \cite{kann-etal-2019-verb,warstadt-etal-2019-investigating,thrush2020investigating, warstadt-etal-2020-blimp-benchmark}.

\textbf{Winograd schemas.} The Winograd Schema Challenge~\cite{levesque2012winograd} was named after a coreference resolution problem presented by Terry Winograd \cite{winograd1972understanding}. The goal is to correctly resolve (an) ambiguous referent(s) in two English sentences. The sentences have a minor difference that changes how a human resolves the referent. Winograd schema examples are easily handled by humans, and commonsense reasoning is said to be required \cite{bender2015establishing}.
For example, in the sentence \textit{``The city councilmen refused the demonstrators a permit because they [feared/advocated] violence"}, the pronoun \textit{they} can either refer to the councilmen or to the demonstrators depending on which word is chosen.
The format has been used in a variety of other tasks and datasets. For instance, Sakaguchi et al.\cite{sakaguchi2020winogrande} introduce WinoGrande: a large-scale approach to building a Winograd Schema dataset that uses Amazon Mechanical Turk to generate sentences instead of expert annotators like the original work of Levesque et al. \cite{levesque2012winograd}.
Other approaches use ambiguous pronouns in sentences to probe for gender biases in models \cite{rudinger2018gender,zhao2018gender}.
See Kotcijan et al. \cite{kocijan2020review} for an in-depth review. Winoground is the first work to apply these ideas to the vision and language domain, by using twin captions with identical word content and two images that are each associated with one caption over the other. 










\section{Winoground}

In this section, we describe how the dataset was constructed and how performance on the task is to be measured.

\subsection{Dataset}
\begin{figure}
    \centering
        \begin{minipage}[t]{.15\textwidth}
            \begin{subfigure}[t]{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_14_img_0.png}
            \caption{there is [a mug] in [some grass]}
            \end{subfigure}\\
            \begin{subfigure}[t]{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_14_img_1.png}
            \caption{there is [some grass] in [a mug]}
            \end{subfigure}\caption*{\textit{Object}}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{.15\textwidth}
            \begin{subfigure}[t]{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_21_img_0.png}
            \caption{a person [sits] and a dog [stands]}
            \end{subfigure}\\
            \begin{subfigure}[t]{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_21_img_1.png}
            \caption{a person [stands] and a dog [sits]}
            \end{subfigure}\caption*{\textit{Relation}}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{.15\textwidth}
            \begin{subfigure}[t]{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_72_img_1.png}
            \caption{it's a [truck] [fire]}
            \end{subfigure}\\
            \vspace{9pt}
            \begin{subfigure}[t]{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_72_img_0.png}
            \caption{it's a [fire] [truck]}
            \end{subfigure}\vspace{9pt}
            \caption*{\textit{Both}}
        \end{minipage}\end{figure}
    
\begin{figure}
    \centering
        \begin{minipage}{.15\textwidth}
            \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_75_img_0.png}
            \caption{the kid [with the magnifying glass] looks at them []}
            \end{subfigure}\\
            \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_75_img_1.png}
            \caption{the kid [] looks at them [with the magnifying glass]}
            \end{subfigure}\caption*{\textit{Pragmatics}}
        \end{minipage}
        \hfill
        \begin{minipage}{.15\textwidth}
            \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_27_img_0.png}
            \caption{the person with the ponytail [packs] stuff and other [buys] it}
            \end{subfigure}\\
            \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_27_img_1.png}
            \caption{the person with the ponytail [buys] stuff and other [packs] it}
            \end{subfigure}\caption*{\textit{Series}}
        \end{minipage}
        \hfill
        \begin{minipage}{.15\textwidth}
            \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_61_img_0.png}
            \caption{there are [three] people and [two] windows}
            \end{subfigure}\\
            \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_61_img_1.png}
            \caption{there are [two] people and [three] windows}
            \end{subfigure}\caption*{\textit{Symbolic}}
        \end{minipage}
        \if0
        \hfill
        \begin{minipage}{.15\textwidth}
            \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_74_img_0.png}
            \caption{[walk]1ing out []2 on a [rock]1 [somewhere]2}
            \end{subfigure}\\
            \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[height=1.85cm]{ex_74_img_1.png}
            \caption{[rock]1ing out [somewhere]2 on a [walk]1 []2}
            \end{subfigure}\caption*{\textit{Morpheme}}
        \end{minipage}\fi
        \caption[]{Examples from our dataset for the swap-dependent linguistic tags (top) and visual tags (bottom). The visual examples are additionally tagged with the \textit{Relation} tag, and 1, 2, and 1 main predicates from left to right. The linguistic examples are additionally tagged with 2, 1, and 1 main predicates from left to right.}
        \label{fig:dataset-examples}
    \end{figure}

\iffalse
\begin{table*}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{lrrrrrrrr}
\toprule
 Feature                 & All             & Object          & Relation         & Both            & Symbolic        &  Series           & Pragmatics      \\
\midrule
 Caption Length          & 9.11  4.30 & 8.47  4.49 & 10.25  3.86 & 5.79  3.41 & 8.66  4.19  & 13.39  5.30 & 8.08  3.96 \\
 
 Dependency Parse Depth & 3.56  1.46 & 3.61  1.51 & 3.72  1.36  & 2.57  1.34 & 3.63  1.25  & 4.85  1.86  & 3.35  1.66 \\
 
 Swap Length             & 1.62  1.14 & 1.93  1.36 & 1.49  0.99  & 1.03  0.17 & 1.46  1.23 &  2.48  1.97  & 1.46  0.85 \\
\bottomrule
\end{tabular}
}
\caption{Winoground dataset characteristics, averaged with standard deviation shown.}
\label{tab:feature-stats-tag-subset}
\end{table*}
\fi

\begin{table}
\centering
\begin{tabular}{lrr}
\toprule
 Category & Tag    &   Count \\
\midrule
& Object   &     141 \\
 Linguistic & Relation &     233 \\
 & Both &      26 \\\midrule
 Linguistic & 1 Main Pred & 293 \\
 & 2 Main Preds & 108 \\\midrule
 & Symbolic &  41 \\
 Visual & Series &  31 \\
 & Pragmatics &  24\\
\bottomrule
\end{tabular}
\caption{Linguistic and visual tag counts in the Winoground dataset. Every example has a linguistic tag; only examples that contain the visual phenomena have visual tags.}
\label{tab:stats-tag-subset}
\end{table}


The Winoground dataset was hand-curated by four expert annotators with extensive experience in vision and language research as well as computational linguistics. Let~ and  be two image-caption pairs. An example satisfies the Winoground schema if and only if:
\begin{itemize}
    \item  and  are preferred by the annotator over  and ; and
    \item  and  have the same words and/or morphemes but the order differs.
\end{itemize}

We have secured a license from Getty Images to distribute images for research purposes. Thus, the expert annotators were given access to the Getty Images API \cite{getty}, and tasked with jointly creating captions and finding images to compose examples. We encouraged them to be as creative as possible, and to mark each of their examples with fine-grained linguistic tags. If applicable, annotators also marked examples with one or more visual reasoning tags.

The annotators created a total of 70 linguistic tags for the swaps that make caption pairs different. This set of tags can be split into three broad groups: objects, relations, and swaps involving both relations and objects. Object swaps reorder elements such as noun phrases that tend to refer to objects in the real world. Relation swaps reorder elements such as verbs, adjectives, prepositions, and/or adverbs, which tend to take nouns referring to objects as semantic arguments \cite{altshuler2019course}. Swaps of both relations and objects can involve two separate swaps, or can involve a single swap that changes parts of speech (e.g., ``it's a [fire] [truck]" vs. ``it's a [truck] [fire]"). Examples of each broad tag group can be seen in \cref{fig:dataset-examples}. For examples for each fine-grained linguistic tag, see Appendix~C.

Separately, the annotators tagged examples for how many main predicates were in the captions, which is not dependent on the specific swap happening between the two captions. For example, ``left is blue and right is red'' has two main predicates and ``water is in a bottle'' has one main predicate. It turned out that all examples in Winoground have either one main predicate or two.

Finally, examples were tagged from a set of three non-mutually exclusive visual reasoning tags, which are tied in some way to the images in an example, and not necessarily the captions. The ``Pragmatics'' tag comprises examples where the images need to be interpreted non-literally due to idiomatic uses of language in a caption (e.g. ``it starts with Z and ends with A'' describing an image of a Zebra) or due to attachment preferences of prepositional phrases in the captions (e.g. ``the kid looks at them with the magnifying glass'' describing an image of a child looking at someone through a magnifying glass with greater confidence than an image of a child looking at someone while holding a magnifying glass at their side). The ``Symbolic" tag represents whether a symbolic depiction of something must be understood to make a correct prediction (e.g., objects in a child's drawing). Lastly, the ``Series'' tag is given to examples where both images come from the same photo series on Getty, which typically means that the same people occur in both images, with a similar background and in similar lighting.

See \cref{fig:dataset-examples} for representative examples of the tags, and \cref{tab:stats-tag-subset} for tag counts. As noted, Winoground is a probing dataset and so we prioritize clean, expert annotations over mere size. Our dataset has 1600 image-text pairs in total, with 800 correct and 800 incorrect pairings. These comprise 400 examples, with 800 unique captions and images.







\subsection{Metrics}\label{sec:metrics}
Performance on Winoground is computed according to three different metrics that evaluate different aspects of the models' visio-linguistic reasoning abilities.
The first metric is the \textbf{text score}, which measures whether a model can select the correct caption, given an image.
Given images  and  and captions  and , the text score for an example  is computed according to:

where  is the model's score for the image/caption pair.
This metric tests whether the ground truth caption for a given image in our dataset is scored higher than the alternative caption \textit{and} whether this holds for the other image/caption pair in the example too.



The second metric is the \textbf{image score}, which measures whether a model can select the correct image, given a caption.
Given images  and  and captions  and , the image score for an example is computed according to:

This metric tests whether the ground truth image for a given caption is scored higher than the image corresponding to the alternative caption \textit{and} whether this holds vice versa.

Our final metric combines the previous two. In their analysis of the Winograd Schema Challenge, Elazar et al.~\cite{elazar2021back} find that evaluation metrics tend to overestimate model performance by computing scores for the twin sentences individually instead of as a set. So, we also evaluate using the \textbf{group score}, where every combination for a given example \{\} must be correctly scored by the model in order for the example to be considered correct.
The group score in our framework is computed according to:




\section{Experimental Setup}

We evaluate various configurations of the following multimodal transformers: CLIP \cite{radford2021clip}, FLAVA \cite{singh2022flava}, LXMERT \cite{tan2020lxmert}, UniT \cite{hu2021unit}, UNITER \cite{chen2020uniter}, VILLA \cite{gan2020villa}, VinVL \cite{zhang2021vinvl}, ViLT \cite{kim2021vilt}, VisualBERT \cite{li2019visualbert} and ViLBERT \cite{lu2019vilbert}. We also evaluate several configurations of two types of RNN-based models: VSE++ \cite{faghri2018vse} and VSRN \cite{li2019vsrn}.
We detail differences between these models and provide a high-level overview in \cref{tab:model-types}.
We also establish a human baseline using crowdworkers, as described in \cref{sec:human-perf}.

\subsection{Vision \& Language Transformers}\label{sec:vl-transformers}
\begin{table*}[!ht]
    \centering
    \small
    \resizebox{\textwidth}{!}{\begin{tabular}{l|lr|l|l}
    \toprule
    \textit{Model} & \textit{Datasets} & \textit{\# Images, Captions (Millions)} & \textit{Architecture} & \textit{Attention} \\\midrule
    VinVL \cite{zhang2021vinvl}  & VQA, GQA, VG-QA, COCO, Flickr30k, CC, SBU & 1.89, 4.87 & single-stream & merged \\
    UNITER \cite{chen2020uniter}  & COCO, VG, CC, SBU & 4.20, 9.58 & single-stream  &  merged \\
    ViLLA \cite{gan2020villa} & COCO, VG, CC, SBU  & 4.20, 9.58 & single-stream  &  merged \\
    VisualBERT \cite{li2019visualbert}& COCO, NVLR2 & 0.30, 0.52  & single-stream  & merged \\
    ViLT \cite{kim2021vilt}  & COCO, VG, SBU, CC & 4.10, 9.85 & single-stream  & merged \\
    LXMERT \cite{tan2020lxmert}  & COCO, VG & 0.18, 9.18 & dual-stream & modality-specific, co-attn, merged \\
    ViLBERT \cite{lu2019vilbert}  & CC & 3.30, 3.30 & dual-stream  & modality-specific, co-attn, merged \\
    UniT \cite{hu2021unit} & COCO detect., VG detect., VQAv2, SNLI-VE QNLI, MNLI-mm, QQP, SST-2 & 0.69, 1.91 & dual-stream & modality-specific, merged\\
    FLAVA  \cite{singh2022flava}  & COCO, SBU, LN, CC, VG, WIT, CC 12M, RC, YFCC100M & 70.00, 70.00 & dual-stream & modality-specific, merged \\
    FLAVA  \cite{singh2022flava}  & COCO, SBU, LN, CC, VG, WIT, CC 12M, RC, YFCC100M & 70.00, 70.00 & dual-stream & modality-specific \\
    CLIP \cite{radford2021clip}  &  & 400.00, 400.00 & dual-stream & modality-specific \\
    VSE++ and VSRN  & COCO & 0.11, 0.57 & dual-stream & \\
    VSE++ and VSRN  & Flickr30k & 0.03, 0.16 & dual-stream & \\
    \bottomrule
    \end{tabular}
    }
    \caption{A high-level overview of the differences between the models we evaluate by the pretraining datasets, architecture, and attention mechanisms between the modalities. We omit datasets that were only used to train backbones. We exclude the language embedding from this table as every model uses a pretrained BERT tokenizer, except CLIP, VSE++, and VSRN. The pretraining datasets include COCO \cite{lin2014microsoft}, Visual Genome (VG) \cite{krishna2016visual}, Conceptual Captions (CC) \cite{sharma2018conceptual}, SBU Captions \cite{ordonez2011im2text}, Flickr30k \cite{young2014image}, VQA 2.0 \cite{goyal2017making}, VCR \cite{zellers2019recognition}, NLVR2 \cite{suhr2017corpus}, SNLI-VE \cite{xie2018visual}, QNLI \cite{rajpurkar2016squad}, MLNI-mm \cite{williams2017broad}, QQP \cite{QQPDataset}, Localized Narratives (LN) \cite{pont-tuset2020localized-narratives}, Wikipedia Image Text (WIT) \cite{srinivasan2021wit}, Conceptual Captions 12M (CC 12M) \cite{changpinyo2021conceptual12m}, Red Caps (RC) \cite{desai2021redcaps}, YFCC100M \cite{thomee2016yfcc100m}, and SST-2 \cite{Socher2013RecursiveDM}. CLIP uses their own dataset for pretraining.}
    \label{tab:model-types}
\end{table*}

\textbf{Image and language embedding.} All transformer models we evaluate use a pretrained BERT tokenizer \cite{devlin2019bert}, except CLIP, which uses a Byte-Pair Encoding tokenizer \cite{sennrich2015neural} trained from scratch. For the image embedding, five transformers (VisualBERT, ViLBERT, LXMERT, UNITER, ViLLA)\cite{li2019visualbert,lu2019vilbert,tan2020lxmert,chen2020uniter,gan2020villa} use region features extracted from the \texttt{fc6} layer of a Faster R-CNN~\cite{ren2015faster} trained on Visual Genome \cite{krishna2016visual}.
VinVL trains its own feature extractor on a large combined dataset from public sources with a unified object vocabulary \cite{zhang2021vinvl}.
The CLIP, FLAVA, and ViLT that we test all use Vision Transformer (ViT) \cite{dosovitskiy2020imageworth}.
In ViT, images are flattened into patches that are linearly projected and combined with a position encoding. UniT \cite{hu2021unit} alternatively uses a transformer network \cite{vaswani2017attention} on top of a convolutional network following Carion et al.\cite{carion2020end}.

\textbf{Single-stream vs. dual-stream encoders.} Vision and language transformers are mainly single- or dual-stream models: the embeddings for the image and text modalities are either concatenated and then jointly encoded (single-stream), or encoded by two separate modality-specific encoders with optional cross-modality fusion (dual-stream).
Five of our transformers are single-stream \cite{chen2020uniter, gan2020villa, zhang2021vinvl, kim2021vilt, li2019visualbert}. VinVL additionally concatenates object tags, which are the set of objects detected by the X152-C4 model during feature extraction, to the language tokens before encoding.
All single-stream models use merged attention, where the language and visual input attend to both themselves and the other modality.
The dual-stream transformers we evaluate are CLIP, FLAVA, UniT, LXMERT and ViLBERT \cite{radford2021clip, singh2022flava, hu2021unit, tan2020lxmert, lu2019vilbert}.
CLIP and the contrastive configuration of FLAVA lack cross-modal attention. ViLBERT has language-only transformer layers that are then fused by cross-modal transformer layers.
LXMERT, the ITM configuration of FLAVA, and UniT each use language-only and vision-only layers that are also fused by cross-modal transformer layers, which perform a combo of modality-specific attention and co-attention across modalities.


\textbf{Pretraining objectives.} V\&L transformers use a number of pretraining objectives including but not limited to masked language modeling, masked region modeling (classification of object classes and regression over image features) and image-text matching.
As we are evaluating a model's ability to determine if an image and a corresponding caption match, we select V\&L transformers that are pretrained with an image-text matching classification head or that produce a similarity score between the two modalities\footnote{UniT is the only model we selected that was not pretrained on image-text matching. To get image-text alignment scores, we finetuned UniT on image-text matching loss using MS-COCO\cite{lin2014microsoft}}.

\if0
\subsubsection{Cross-Modal Attention}\label{sec:cross-modal-attn}

Next, we note the differences in the way cross-modal attention between the language and vision modalities is performed by the models we evaluate.
Recall that for query, key and value matrices Q, K and V as described in \cite{vaswani2017attention}, scaled dot-product attention is computed according to .
In vision and language transformers, these matrices can further be delineated by the whether they refer to the language and vision modality, i.e.,  and  refer to the query matrix for the language and visual modality respectively,  and  to the keys and  and  to the values.
Following the models descriptions of attention in \cite{vaswani2017attention} and the definitions of different types of multimodal attention as presented \cite{hendricks2021decoupling}, our models span three types of attention. In \textit{modality-specific attention}, the language input only attends to other language input and visual input only attends to other visual input. This corresponds to computing  for the language input and separately computing  for the visual input. In \textit{merged attention}, the language and visual input attend to both themselves and the other modality. This corresponds to computing , where Q, K and V are formed by concatenating  and ,  and  and  and  respectively. With \textit{co-attention}, the language input only attends to the visual input and the visual input only attends to the language input. This corresponds to computing  and  for the language and vision outputs respectively. Merged attention and co-attention are both instances of cross-modal attention as there is explicit information shared between the modalities through the attention mechanism.


Every single-stream model we evaluate concatenates the language and visual input and uses merged attention.
For dual-stream models, ViLBERT has  layers of language-only modality-specific attention followed by  layers of co-attention feeding into modality-specific attention layers.
LXMERT is similar to ViLBERT except that it includes modality-specific attention for the visual input as well; there are  and  layers of modality-specific attention followed by  layers of co-attention feedings into modality-specific attention layers.
The ITM version of FLAVA and UniT similarly use  and  layers of modality-specific attention then has  layers of merged attention.
CLIP and the contrastive version of FLAVA are the only models we evaluate that do not use any cross-modal attention; both the language and vision encoders only use modality-specific attention.
\fi

\subsection{Multimodal RNNs}

To determine whether low performance on Winoground is unique to transformer-based models, we include results for two sequence-based models, which are VSRN\cite{li2019vsrn} and VSE++\cite{faghri2018vse}. Both VSE++ and VSRN have a loss function that prioritizes minimizing the hardest negative's score. The hardest negative is the highest-scoring image-caption pair that is not correct. Intuitively, this type of loss function could enable models to get higher scores on Winoground in particular and may be useful in future work. Although we show later in the paper that VSRN and VSE++ do not do well, perhaps due to issues besides the loss function. Both models use a GRU\cite{chung2014gru} to get language embeddings and a separate pipeline to get image embeddings. Scores for image-caption pairs are found by taking an inner-product of the embeddings. VSE's image encoder is a linear projection of the embedding from a backbone (either ResNet152\cite{he2016deep} or VGG19\cite{simonyan2015very}). In VSRN, a ResNet101-based Faster R-CNN with graph convolutions on top is used to get a sequence of features which are fed into a GRU. The GRU's last hidden state is then used as the image embedding.

\subsection{Human Performance}\label{sec:human-perf}

We employed crowd workers on the Amazon Mechanical Turk platform to establish a more conservative human baseline than the expert annotator upper bound of a perfect score.
Like the models, annotators are shown one image and one caption at a time. Annotators are asked the binary choice question ``Does the caption match the image?".
All 1600 combinations of images and captions are labeled by at least ten annotators.
We compute the human image-caption score as the ratio of annotators who said the image/caption pair match over the total number of annotators for the pair. More details about the human labelling interface, onboarding criteria, and quality control are provided in Appendix~E.

\if0
\begin{figure}[!t]\centering
    \includegraphics[width=\columnwidth]{latex/images/winoground_mturk_image.png}\caption{The human validation interface.}\label{fig:teaser-ex}\end{figure}
\fi

\section{Results}\label{sec:results}

\subsection{Compared to humans}
As shown in \cref{tab:results-aggr}, the models struggle across the board on Winoground, often performing close to or below random chance. Comparatively, as expected, the human performance is high across the full range of linguistic and visual phenomena.
For the \textbf{text score}, we observe 50\% absolute difference between humans and the best performing models---UNITER, VILLA VinVL, ViLT, FLAVA, and CLIP---with the remaining models below chance.


\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{lrrr}
\toprule
 Model      & Text          & Image         & Group         \\
\midrule
 MTurk Human                  & \textbf{89.50} & \textbf{88.50} & \textbf{85.50}\\
 Random Chance                  & 25.00 & 25.00 & 16.67 \\\midrule
 VinVL                        & \textbf{37.75} & 17.75          & 14.50          \\
 UNITER             & \textbf{38.00} & 14.00          & 10.50          \\
 UNITER              & \textbf{32.25} & 13.25          & 10.00          \\
 ViLLA              & \textbf{37.00} & 13.25          & 11.00          \\
 ViLLA               & \textbf{30.00} & 12.00          & 8.00           \\
 VisualBERT          & 15.50          & 2.50           & 1.50           \\
 ViLT (ViT-B/32)              & \textbf{34.75} & 14.00          & 9.25           \\
 LXMERT                       & 19.25          & 7.00           & 4.00           \\
 ViLBERT             & 23.75          & 7.25           & 4.75           \\
 UniT       & 19.50          & 6.25           & 4.00           \\
 FLAVA                & \textbf{32.25} & 20.50 & 14.25 \\
 FLAVA        & \textbf{25.25} & 13.50          & 9.00           \\
 CLIP (ViT-B/32)              & \textbf{30.75} & 10.50          & 8.00           \\
 VSE++ (ResNet)      & 22.75          & 8.00           & 4.00           \\
 VSE++ (VGG)         & 18.75          & 5.50           & 3.50           \\
 VSE++ (ResNet) & 20.00          & 5.00           & 2.75           \\
 VSE++ (VGG)    & 19.75          & 6.25           & 4.50           \\
 VSRN                & 17.50          & 7.00           & 3.75           \\
 VSRN           & 20.00          & 5.00           & 3.50           \\
\bottomrule
\end{tabular}
}
\caption{Results on the Winoground dataset across the text, image and group score metrics. Results above random chance in \textbf{bold}.}
\label{tab:results-aggr}
\end{table}

The human performance is only slightly lower for the \textbf{image score}, whereas all models perform much worse. Even the highest performing model, FLAVA, has a 70\% performance gap compared to humans. This gap is not unique to our dataset: in prior work \cite{faghri2018vse} \cite{radford2021clip}, models also tend to perform significantly better on caption retrieval compared to image retrieval. More investigation is required to pinpoint the reasons: perhaps textual encoders are stronger, or the text modality has different biases.

Lastly, we consider the \textbf{group score}. For humans, it is not appreciably lower than their text and image scores. All of the models are below random chance here as well.
We report confidence intervals for these results in Appendix~A.

\begin{table*}[!ht]
    \centering
    \resizebox{!}{1.32in}{\begin{tabular}{lrrr|rrr|rrr|rrr|rrr}
    \toprule
     &
      \multicolumn{3}{c}{\textit{Object}} &
      \multicolumn{3}{c}{\textit{Relation}} &
      \multicolumn{3}{c}{\textit{Both}} &
      \multicolumn{3}{c}{\textit{1 Main Pred}} &
      \multicolumn{3}{c}{\textit{2 Main Preds}}\\
    \textit{Model} & Text & Image & Group & Text & Image & Group & Text & Image & Group & Text & Image & Group & Text & Image & Group \\\midrule
 MTurk Human                  & \textbf{92.20} & \textbf{90.78} & \textbf{88.65} & \textbf{89.27} & \textbf{90.56} & \textbf{86.70} & \textbf{76.92} & \textbf{57.69} & \textbf{57.69} & \textbf{87.33} & \textbf{85.62} & \textbf{82.53} & \textbf{95.37} & \textbf{96.30} & \textbf{93.52} \\
 VinVL                        & \textbf{36.88} & 17.73          & 14.18          & \textbf{37.77} & 17.60          & 14.16          & \textbf{42.31} & 19.23          & \textbf{19.23} & \textbf{39.38} & 21.23          & \textbf{17.47} & \textbf{33.33} & 8.33           & 6.48           \\
 UNITER             & \textbf{39.01} & 12.77          & 9.93           & \textbf{36.05} & 14.16          & 9.87           & \textbf{50.00} & 19.23          & \textbf{19.23} & \textbf{40.07} & 16.44          & 13.36          & \textbf{32.41} & 7.41           & 2.78           \\
 UNITER              & \textbf{34.04} & 11.35          & 9.22           & \textbf{30.04} & 14.16          & 10.30          & \textbf{42.31} & 15.38          & 11.54          & \textbf{35.27} & 14.73          & 11.99          & 24.07          & 9.26           & 4.63           \\
 ViLLA              & \textbf{36.88} & 14.89          & 11.35          & \textbf{37.34} & 12.88          & 11.16          & \textbf{34.62} & 7.69           & 7.69           & \textbf{39.73} & 17.12          & 14.38          & \textbf{29.63} & 2.78           & 1.85           \\
 ViLLA               & \textbf{33.33} & 15.60          & 9.93           & \textbf{27.04} & 9.01           & 6.01           & \textbf{38.46} & 19.23          & 15.38          & \textbf{33.22} & 14.04          & 10.27          & 21.30          & 6.48           & 1.85           \\
 VisualBERT          & 19.15          & 2.13           & 0.71           & 12.88          & 2.15           & 1.72           & 19.23          & 7.69           & 3.85           & 16.44          & 2.74           & 1.71           & 12.96          & 1.85           & 0.93           \\
 ViLT (ViT-B/32)              & \textbf{31.91} & 15.60          & 9.22           & \textbf{36.91} & 11.59          & 8.15           & \textbf{30.77} & \textbf{26.92} & \textbf{19.23} & \textbf{35.27} & 17.12          & 11.64          & \textbf{33.33} & 5.56           & 2.78           \\
 LXMERT                       & 22.70          & 9.22           & 6.38           & 17.60          & 5.58           & 2.58           & 15.38          & 7.69           & 3.85           & 19.18          & 8.56           & 5.14           & 19.44          & 2.78           & 0.93           \\
 ViLBERT             & \textbf{29.08} & 10.64          & 7.09           & 19.31          & 3.00           & 1.72           & \textbf{34.62} & \textbf{26.92} & \textbf{19.23} & 23.97          & 8.90           & 5.82           & 23.15          & 2.78           & 1.85           \\
 UniT       & 17.73          & 5.67           & 2.13           & 18.03          & 4.72           & 3.43           & \textbf{42.31} & 23.08          & \textbf{19.23} & 21.58          & 6.85           & 4.11           & 13.89          & 4.63           & 3.70           \\
  FLAVA                & \textbf{31.91} & 23.40 & 14.89 & \textbf{30.04} & 16.31 & 12.02 & \textbf{53.85} & \textbf{42.31} & \textbf{30.77} & \textbf{36.30} & 24.66 & \textbf{17.81} & 21.30          & 9.26 & 4.63 \\
 FLAVA        & 23.40          & 19.15 & 11.35 & 23.61          &  8.58 &  5.58 & \textbf{50.00} & \textbf{26.92} & \textbf{26.92} & \textbf{26.37} & 16.44 & 10.62          & 22.22          &  5.56 & 4.63 \\
 CLIP (ViT-B/32)              & \textbf{34.75} & 7.80           & 6.38           & 22.75          & 8.58           & 5.58           & \textbf{80.77} & \textbf{42.31} & \textbf{38.46} & \textbf{35.27} & 13.01          & 10.27          & 18.52          & 3.70           & 1.85           \\
 VSE++ (ResNet)      & 21.99          & 6.38           & 1.42           & 23.61          & 9.01           & 5.58           & 19.23          & 7.69           & 3.85           & 25.00          & 9.59           & 4.79           & 16.67          & 3.70           & 1.85           \\
 VSE++ (VGG)         & 17.73          & 2.13           & 2.13           & 18.45          & 7.30           & 3.86           & \textbf{26.92} & 7.69           & 7.69           & 18.49          & 4.79           & 2.74           & 19.44          & 7.41           & 5.56           \\
 VSE++ (ResNet) & 20.57          & 6.38           & 3.55           & 18.88          & 4.29           & 2.15           & \textbf{26.92} & 3.85           & 3.85           & 21.58          & 6.51           & 3.42           & 15.74          & 0.93           & 0.93           \\
 VSE++ (VGG)    & 17.73          & 4.96           & 2.84           & 19.74          & 6.87           & 5.15           & \textbf{30.77} & 7.69           & 7.69           & 20.55          & 6.16           & 4.79           & 17.59          & 6.48           & 3.70           \\
 VSRN                & 15.60          & 4.96           & 2.13           & 18.88          & 7.73           & 4.72           & 15.38          & 11.54          & 3.85           & 17.12          & 7.19           & 3.77           & 18.52          & 6.48           & 3.70           \\
 VSRN           & 16.31          & 4.96           & 2.13           & 21.03          & 4.29           & 3.86           & \textbf{30.77} & 11.54          & 7.69           & 20.89          & 5.82           & 3.77           & 17.59          & 2.78           & 2.78           \\
          \bottomrule
  \end{tabular}
  }
  \caption{The results by linguistic tag. Results above chance are in \textbf{bold}.}
    \label{tab:results-by-ling-tag}
\end{table*}

\begin{table*}
    \centering
    \resizebox{!}{1.32in}{\begin{tabular}{lrrr|rrr|rrr}
    \toprule
     &
      \multicolumn{3}{c}{\textit{Symbolic}} &
      \multicolumn{3}{c}{\textit{Pragmatics}} &
      \multicolumn{3}{c}{\textit{Same Image Series}} \\
    \textit{Model} & Text & Image & Group & Text & Image & Group & Text & Image & Group \\\midrule
 MTurk Human                  & \textbf{96.43} & \textbf{92.86} & \textbf{92.86} & \textbf{58.82} & \textbf{41.18} & \textbf{41.18} & \textbf{95.65} & \textbf{91.30} & \textbf{91.30} \\
 VinVL                        & 25.00          & 17.86          & 14.29          & \textbf{29.41} & 5.88           & 5.88           & \textbf{34.78} & 17.39          & 13.04          \\
 UNITER             & \textbf{39.29} & \textbf{28.57} & \textbf{17.86} & \textbf{35.29} & 0.00           & 0.00           & 4.35           & 8.70           & 0.00           \\
 UNITER              & \textbf{46.43} & 14.29          & 14.29          & \textbf{29.41} & 17.65          & 11.76          & 8.70           & 8.70           & 0.00           \\
 ViLLA              & \textbf{39.29} & 14.29          & 10.71          & 17.65          & 0.00           & 0.00           & 17.39          & 4.35           & 0.00           \\
 ViLLA               & \textbf{42.86} & 17.86          & 14.29          & \textbf{29.41} & 5.88           & 5.88           & 13.04          & 8.70           & 4.35           \\
 VisualBERT          & \textbf{28.57} & 0.00           & 0.00           & 5.88           & 0.00           & 0.00           & 13.04          & 0.00           & 0.00           \\
 ViLT (ViT-B/32)              & \textbf{28.57} & 17.86          & 10.71          & \textbf{35.29} & 0.00           & 0.00           & \textbf{26.09} & 0.00           & 0.00           \\
 LXMERT                       & \textbf{28.57} & 3.57           & 3.57           & 17.65          & 5.88           & 0.00           & 8.70           & 4.35           & 0.00           \\
 ViLBERT             & \textbf{28.57} & 10.71          & 7.14           & \textbf{29.41} & 5.88           & 5.88           & 13.04          & 0.00           & 0.00           \\
 UniT       & 14.29          & 10.71          & 7.14           & 17.65          & 5.88           & 5.88           & 21.74          & 4.35           & 4.35           \\
 FLAVA                & 25.00          & \textbf{28.57} & \textbf{17.86} & 17.65          & \textbf{29.41} & 11.76 & 17.39          &  8.70 &  0.00 \\
 FLAVA        & 17.86          & 10.71          & 10.71          & 11.76          & 23.53          & 5.88           & 17.39          & 4.35           &  4.35 \\
 CLIP (ViT-B/32)              & \textbf{39.29} & 3.57           & 3.57           & \textbf{35.29} & 5.88           & 5.88           & 8.70           & 0.00           & 0.00           \\
 VSE++ (ResNet)      & \textbf{32.14} & 10.71          & 10.71          & 23.53          & 11.76          & 0.00           & 13.04          & 4.35           & 4.35           \\
 VSE++ (VGG)         & 17.86          & 14.29          & 7.14           & 17.65          & 0.00           & 0.00           & 13.04          & 4.35           & 4.35           \\
 VSE++ (ResNet) & 21.43          & 3.57           & 0.00           & 23.53          & 0.00           & 0.00           & 17.39          & 4.35           & 0.00           \\
 VSE++ (VGG)    & \textbf{28.57} & 10.71          & 10.71          & 11.76          & 0.00           & 0.00           & 13.04          & 4.35           & 0.00           \\
 VSRN                & 7.14           & 3.57           & 0.00           & 11.76          & 0.00           & 0.00           & 13.04          & 0.00           & 0.00           \\
 VSRN           & 21.43          & 3.57           & 3.57           & \textbf{35.29} & 11.76          & 5.88           & 8.70           & 4.35           & 4.35           \\
    \bottomrule
  \end{tabular}
  }
  \caption{The results by visual tag. Results above chance are in \textbf{bold}.}
    \label{tab:results-by-visual-tag}
\end{table*}

\subsection{Results by Tags}

For the swap-dependent linguistic tags, human performance is highest on \textbf{object}, followed by the \textbf{relation} and then \textbf{both}. For the swap-independent linguistic tags, humans do better on examples with two main predicates, which tend to contain longer and more complicated sentences. The models perform poorly on every category, but they largely show the opposite pattern. They perform better on examples with simpler and shorter sentences which more often have swaps at the morpheme level (see \cref{tab:results-by-ling-tag}). One exception to the low model performance is that CLIP performs comparably to the humans on the \textbf{both} tag text score---the 26 examples with the \textbf{both} tag have some of the shortest and least compositional captions in our dataset (e.g. ``presenting the watch'' vs ``watching the present''). 

We also evaluate performance for the visual reasoning tags as shown in \cref{tab:results-by-visual-tag}. Models and humans are particularly good at the \textbf{symbolic} examples, but the models are poor comparatively. On the \textbf{pragmatics} tag, humans have the lowest performance. Ten crowdworkers probably didn't capture slight pragmatics preferences that our expert linguist annotators agreed on. One example that the crowdworkers failed is \cref{fig:dataset-examples}(a): “the kid [with the magnifying glass] looks at them []”. All ten annotators said that ``the kid with the magnifying glass looks at them" was acceptable for both images, but captured the correct preference for the second caption. This reveals a limitation in how the task was presented to humans: our hypothesis is that if we gave humans both images and both captions at the same time, or if significantly more human annotators gave their judgements, then the human scores would be substantially higher. Finally, models do worst on the \textbf{series} tag where most get a 0\% group score, which indicates that they are always choosing one image over the other regardless of the caption (or vice versa).

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \scalebox{0.9}{
            \begin{tabular}{cccc}
\multicolumn{3}{c}{a \textbf{brown dog} is on a \textbf{white couch}}\\
\includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e29_i0_base.png}& \includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e29_c0i0_brown_dog.png} & \includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e29_c0i0_white_couch.png} \\
\includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e29_i1_base.png}& \includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e29_c0i1_brown_dog.png} & \includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e29_c0i1_white_couch.png} \\
             & & & \\
\multicolumn{3}{c}{a \textbf{white dog}\quad is on a \quad\textbf{brown couch}}\\
\includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e29_i1_base.png}& \includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e29_c1i1_white_dog.png} & \includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e29_c1i1_brown_couch.png} \\
\includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e29_i0_base.png}& \includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e29_c1i0_white_dog.png} & \includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e29_c1i0_brown_couch.png} \\
            \end{tabular}
        }
    \end{minipage}
    ~
    \begin{minipage}{0.48\textwidth}
        \centering
        \scalebox{0.9}{
            \begin{tabular}{cccc}
\multicolumn{3}{c}{\textbf{circular food} on \textbf{heart-shaped wood}}\\
\includegraphics[height=1.5cm, width=0.20\textwidth]{vilt_ot_heatmap/e118_img_0.png}& \includegraphics[height=1.5cm, width=0.20\textwidth]{vilt_ot_heatmap/e118_c0i0_circular_food.png} & \includegraphics[height=1.5cm, width=0.20\textwidth]{vilt_ot_heatmap/e118_c0i0_heart_wood.png} \\
\includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e118_img_1.png}& \includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e118_c0i1_circular_food.png} & \includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e118_c0i1_heart_wood.png} \\
             & & & \\
\multicolumn{3}{c}{ \textbf{heart-shaped food} on \textbf{circular wood}}\\
\includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e118_img_1.png}& \includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e118_c1i1_heart_food.png} & \includegraphics[height=1.5cm, width=0.25\textwidth]{vilt_ot_heatmap/e118_c1i1_circular_wood}.png \\
\includegraphics[height=1.5cm, width=0.20\textwidth]{vilt_ot_heatmap/e118_img_0.png}& \includegraphics[height=1.5cm, width=0.20\textwidth]{vilt_ot_heatmap/e118_c1i0_heart_food.png} & \includegraphics[height=1.5cm, width=0.20\textwidth]{vilt_ot_heatmap/e118_c1i0_circular_wood.png} \\
            \end{tabular}
        }
    \end{minipage}
    
    \caption{Word-region alignment scores between the image and text features for ViLT \cite{kim2021vilt} on examples from Winoground. In this case study, ViLT appears to disregard the information from adjectives. E.g., the heatmaps highlight the brown dog just as strongly regardless of whether the text was ``brown dog" or ``white dog".}
    \label{fig:vilt-heatmap}
\end{figure*}

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{lrrrr}
\toprule
 & \multicolumn{2}{c}{\textit{Perplexity}} &  \multicolumn{2}{c}{\textit{Caption Length}}\\
 \textit{Model}      &   Corr. &   p-value & Corr. &   p-value\\\midrule 
 MTurk Human                  &  0.05 & 0.07 &  \textbf{0.20} & \textbf{0.00} \\
 VinVL                        & \textbf{-0.05} & \textbf{0.04} & \textbf{-0.20} & \textbf{0.00} \\
 UNITER             & -0.01 & 0.57 & \textbf{-0.16} & \textbf{0.00} \\
 UNITER              & -0.03 & 0.22 & \textbf{-0.14} & \textbf{0.00} \\
 ViLLA              & -0.02 & 0.39 & \textbf{-0.12} & \textbf{0.01} \\
 ViLLA               & -0.04 & 0.13 & \textbf{-0.11} & \textbf{0.03} \\
 VisualBERT          & -0.04 & 0.15 & -0.06 & 0.22 \\
 ViLT (ViT-B/32)              & -0.04 & 0.16 & \textbf{-0.16} & \textbf{0.00} \\
 LXMERT                       & -0.04 & 0.12 & \textbf{-0.11} & \textbf{0.02} \\
 ViLBERT             & -0.04 & 0.11 & \textbf{-0.14} & \textbf{0.00} \\
 UniT       & -0.01 & 0.73 & -0.02 & 0.73 \\
 FLAVA                & -0.03 & 0.22 & \textbf{-0.23} & \textbf{0.00} \\
 FLAVA        & \textbf{-0.06} & \textbf{0.01} & \textbf{-0.19} & \textbf{0.00} \\
 CLIP (ViT-B/32)              & -0.04 & 0.09 & \textbf{-0.22} & \textbf{0.00} \\
 VSE++ (ResNet)      & \textbf{-0.05} & \textbf{0.04} &  0.01 & 0.90 \\
 VSE++ (VGG)         & -0.04 & 0.08 &  0.03 & 0.56 \\
 VSE++ (ResNet) & -0.02 & 0.43 &  0.02 & 0.67 \\
 VSE++ (VGG)    &  0.01 & 0.74 & \textbf{-0.10} & \textbf{0.04} \\
 VSRN                & \textbf{-0.07} & \textbf{0.01} & -0.05 & 0.36 \\
 VSRN           & -0.02 & 0.32 & -0.05 & 0.29 \\
\bottomrule
\end{tabular}
}
\caption{(left) The correlation between model image-caption scores and the caption perplexity from GPT2. (right) The correlation between the model group scores and the caption length.}
\label{tab:perplexity-and-length-correlations}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{llrr}
\toprule
Pretraining Modality & Score & Corr. & p-value\\\midrule
 & Text & \textbf{0.84} & \textbf{0.00} \\
 Image & Image & \textbf{0.76} & \textbf{0.00} \\
 & Group & \textbf{0.75} & \textbf{0.00} \\\midrule
 & Text  & \textbf{0.77} & \textbf{0.00} \\
 Caption & Image & \textbf{0.75} & \textbf{0.00} \\
 & Group & \textbf{0.71} & \textbf{0.00} \\
\bottomrule
\end{tabular}
\caption{Correlations between the number of pretraining images and captions and the model text, image, and group scores. CLIP and FLAVA are excluded as outliers.}
\label{tab:data-size-correlations}
\end{table}

\section{Discussion}

Despite the fact that every model struggled on Winoground compared to humans, we hope to gain further insights by analyzing which aspects of these models could contribute to their performance differences.

\subsection{Capabilities of Encoders}
\if0
UNITER, VILLA, VinVL, ViLT, FLAVA, and CLIP are the only models that get above random chance performance on Winoground in \cref{tab:results-aggr}, and only for the text score.
As shown in \cref{tab:model-types}, the main difference between these and the other models is in their visual features.
Of these models, UNITER and VILLA extract features from a pretrained Faster R-CNN like many other approaches, and then additionally add an image embedder that consists of fully connected layers for the regions and locations followed by a single layer norm.
VinVL first trains an object detector based on ResNeXt-152 with significantly more training data and spanning more visual concepts than the widely used Faster-RCNN implementation of \cite{ren2015faster}.
ViLT, FLAVA, and CLIP all use the approach of ViT, where images are unfolded into a series of patches and then linearly embedded.
The remaining models that did not even exceed chance level performance uses either region features from a pretrained Faster R-CNN (VisualBERT, LXMERT, ViLBERT) or use a convolutional layer on top of the transformer (UniT).
This may suggest that better visual features-either in terms of their semantic coverage or their ability to be co-trained with the model- may contribute to improved visio-linguistic understanding on tasks like Winoground that require fine-grained reasoning.
\fi 


\textbf{Richer features.} UNITER, VILLA, VinVL, ViLT, FLAVA, and CLIP are the only models that get above random chance performance in \cref{tab:results-aggr}, and only for the text score.
We hypothesize that these models perform better than others due to their richer features (unimodal features for CLIP and FLAVA, multimodal features for the others).
A potential explanation could be the large-scale pretraining used by CLIP and FLAVA, the large training dataset used to train the object detector for VinVL, or the ViT approach for image features used by ViLT, FLAVA, and CLIP that encodes every portion of the image.


\textbf{Common failure modes.} We highlight again that most of the models fail with 0\% group score on the \textit{same image series} tag.
One explanation is that the models' visual encoders might be too weak to correctly discriminate between substantially similar images. This could cause the models to fall back on their unimodal priors, picking one caption or image over the other in the majority of the four potential caption-image pairings.


\textbf{Heat maps.} We show a heatmap in \cref{fig:vilt-heatmap} of the word-region alignment between ViLT's vision and language features as a visualization for a model with some of the better performance on our dataset. ViLLA and UNITER are also trained with word-region alignment and we provide their heatmaps in Appendix~D.





\textbf{Complicated captions.} The above-chance models do worse on examples with longer captions, possibly due to weak language encoding abilities. As shown in \cref{tab:perplexity-and-length-correlations}, caption length and lower model performance significantly correlate for the best models, even though the correlation is reversed for humans. The examples with the shortest captions are also the least compositional; they are primarily the examples where the parts of speech change between swapped words, or where there is a morpheme-level swap. Finally, we show in \cref{tab:perplexity-and-length-correlations} correlations between caption perplexity\footnote{We used the standard size GPT2 checkpoint from Hugging Face transformers to get perplexity\cite{wolf-etal-2020-transformers}.} and model scores. We found that there is typically a weak correlation between models assigning an image-caption pair a higher score and a caption having low perplexity.



\subsection{By Architecture \& Type of Attention}

As shown in \cref{tab:results-aggr,tab:results-by-ling-tag,tab:results-by-visual-tag}, both single-stream and dual-stream models perform significantly worse than humans on the text, image and group scores.
We find at least one single-stream model and at least one dual-stream model are above chance for most of our experiments, suggesting there is not a distinct performance difference by architecture. Although, six single-stream model checkpoints do above chance overall, compared to only the very large dual-stream models (CLIP and FLAVA). CLIP and FLAVA were trained on an order of magnitude more data than the other models.
Across all types of attention, models struggled compared to humans.
But neither of the two models using co-attention, in conjunction with single-modality and/or merged attention, performed above chance.

\subsection{By Multimodal Pretraining Dataset Size}

We find highly significant correlations between the size of the multimodal pretraining dataset and the scores, if we remove CLIP and FLAVA as outliers. \cref{tab:data-size-correlations} shows these correlations, and Appendix~B has graphs showing each model's score versus the pretraining data size. The unimodal training data (for image backbones or pre-initialized text encoders) is not included in these calculations.





\section{Conclusion}

We introduced a novel task and dataset, Winoground, aimed at measuring visio-linguistic compositional reasoning in state of the art vision and language models. We demonstrate that models fall short, in most cases performing no better than chance. Our findings highlight that there is more work to be done. Particularly, the field could investigate possible strengths of single-stream models, the compilation of more pretraining data, improving image-encoding capabilities, and pretraining objectives that emphasize similar but wrong images. We hope that our task and dataset will help guide research in this important direction.

\textbf{Broader Impact \& Limitations.} Winoground is English-only and translation to other languages may be nontrivial \cite{liu-etal-2021-visually}.
Expert curation is time-consuming and our dataset is limited in size.
Multimodal datasets containing images of people require thoughtful consideration of how people are represented (see \cite{birhane2021multimodal} for a detailed analysis of the stereotypes present in many multimodal datasets).
We used gender underspecified human denoting terms (e.g., person, child) to avoid issues with inferring gender identity from images \cite{Scheuerman2019gender}.
Our annotators disproportionately come from the USA and the same could be true for our crowdworkers.


\textbf{Getty Acknowledgement.} Images in the paper are a compilation of assets, including \copyright Getty Images/Natasha Breen, Maki Nakamura, Jessica Peterson, Kundanlall Sharma, lacaosa, Alberto Bogo, Vu Le, Toson Rueangsuksut, Nisian Hughes, Tanja Walter, Douglas Sacha, PBNJ Productions, Glow Images, 10'000 Hours, zoranm, Marlene Ford, Westend61.








{\small
\bibliographystyle{ieee_fullname}
\bibliography{wino}
}
\end{document}