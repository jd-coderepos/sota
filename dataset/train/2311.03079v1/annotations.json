[{'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Overall score', 'Score': '37.16'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Deductive', 'Score': '36.75'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Abductive', 'Score': '47.88'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Analogical', 'Score': '28.75'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Params', 'Score': '17B'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'GPT-4 score', 'Score': '56.8'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'Params', 'Score': '30B'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'GPT-4 score', 'Score': '52.8'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'Params', 'Score': '17B'}}]
