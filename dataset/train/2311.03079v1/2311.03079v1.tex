\section{Experiments}


To rigorously validate the superior performance and robust generalization of our base model, we conduct quantitative evaluations on an array of multi-modal benchmarks. These benchmarks can be categorized into three broad areas covering a comprehensive range of measurement\footnote{Detailed summary of all benchmarks and corresponding metrics are available at Appendix~\ref{subsec:details_of_eval_benchmarks}.}:

\begin{itemize}
    \item \textbf{Image Captioning}. The main purpose of these tasks is to generate textual captions summarizing the major content of a given image. We utilize prominent datasets including NoCaps~\citep{agrawal2019nocaps}, COCO~\citep{lin2014microsoft}, Flickr30K~\citep{plummer2015flickr30k}, and TextCaps~\citep{sidorov2020textcaps} for evaluation.
    \item \textbf{Visual Question Answering}. The VQA tasks require models to answer questions that may focus on distinct visual contents based on the given image. Our assessment covers diverse datasets, including VQAv2~\citep{antol2015vqa}, OKVQA~\citep{marino2019ok}, TextVQA~\citep{singh2019towards}, VizWiz-VQA~\citep{gurari2018vizwiz}, OCRVQA~\citep{mishra2019ocr}, ScienceQA~\citep{lu2022learn}, and TDIUC~\citep{shrestha2019answer}.
    \item \textbf{Visual Grounding}. Visual grounding involves a set of tasks that establish referential links between textual mentions in a sentence and specific regions in an image. We evaluate our model on the typical datasets, including Visual7w~\citep{zhu2016visual7w}, RefCOCO~\citep{liu2017referring}, RefCOCO+, and RefCOCOg to ensure completeness.
\end{itemize}

\subsection{Image Captioning}




We evaluate the image captioning capability of our pretrained base model on the aforementioned four benchmarks. In a zero-shot evaluation on the Nocaps and Flickr datasets, we assess the precision of our model in describing long-tail visual concepts. Additionally, we present results from finetuning on the COCO and TextCaps datasets.


The detailed performance is shown in Table~\ref{caption}.  Overall, our model achieves the SOTA or compatible performance across the board. Specifically, on the NoCaps benchmark, our base model outperforms the previous best method, GIT2, across four splits with a maximum of $5.7$ points in the out-domain set while only consuming 10\% of the pretraining data (1.5B vs 12.9B). On the Flickr benchmark, our model achieves a SOTA score of $94.9$ surpassing the concurrently released Qwen-VL model by $9.1$ points. These results demonstrate a remarkable capability and robustness of our pretrained model on the image captioning task.
We also evaluate on the COCO~\citep{lin2014microsoft} and TextCaps, where the latter is specifically designed to integrate the textual information of the given image into captions.
Though training without the dedicated OCR data, encouragingly, our base model reveals a significant text-reading ability and obtains a competitive performance with PaLI-X-55B, and outperforms the previous best model of the same scale, PaLI-17B, by $9.1$ points score.

\begin{table*}[htbp]
\centering
\caption{Performance on Image Captioning benchmarks, where all tasks use CIDEr as the evaluation metric. OOD refers to out-of-domain test set. Karp. refers to the Karpathy test split.
}
\label{caption}
  \resizebox{\textwidth}{!}{\centering
  \renewcommand{\arraystretch}{1.15}
  \setlength{\tabcolsep}{4pt}
  \small
  {
    \begin{tabular}{lp{0.8cm}p{0.8cm}p{0.8cm}p{0.8cm}p{0.8cm}p{0.8cm}p{0.8cm}p{1.1cm}}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\begin{minipage}{1cm}\textbf{Train Data}\end{minipage}} & \multicolumn{2}{c}{\textbf{NoCaps val}} & \multicolumn{2}{c}{\textbf{NoCaps test}} & \multirow{1}{*}{\textbf{Flickr}} & \multirow{1}{*}{\textbf{COCO}} & \multirow{1}{*}{\textbf{TextCaps}} \\
    \cmidrule(r){3-4} \cmidrule(r){5-6} \cmidrule(r){7-7} \cmidrule(r){8-8} \cmidrule(r){9-9}
    & & OOD & overall & OOD & overall & Karp. & Karp. & test\\
        \midrule
        Human & - & 95.7 & 87.1 & 91.6 & 85.3 & - & - & 125.1 \\
        VinVL~\citep{zhang2021vinvl} & 8.9M & 83.8 & 94.3 & 78.0 & 92.5 & - & 130.8 & - \\
        SimVLM~\citep{wang2021simvlm} & 1.8B & 115.2 & 112.2 & 109.5 & 110.3 & - & 143.3 & - \\
        CoCa~\citep{yu2022coca} & 4.8B & - & 122.4 & - & 120.6 & - & 143.6 & - \\
        LEMON~\citep{hu2022scaling} & 2B & 120.2 & 117.3 & 110.1 & 114.3 & - & 139.1 & - \\
        Flamingo~\citep{alayrac2022flamingo} & 2.3B & - & - & - & - & 67.2 & 138.1 & - \\
        Prismer~\citep{liu2023prismer} & 12.7M & 113.5 & 112.9 & - & 110.8 & - & 136.5 & - \\
        BLIP-2~\citep{li2023blip} & 129M & 124.8 & 121.6 & - & - & - & 144.5 & - \\
        InstructBLIP~\citep{dai2023instructblip} & 129M & - & 123.1 & - & - & 82.4 & - & - \\
        UniversalCap~\citep{cornia2021universal} & 35M & 123.4 & 122.1 & 114.3 & 119.3 & - & 143.4 & - \\
        GIT~\citep{wang2022git} & 0.8B & 127.1 & 125.5 & 122.0 & 123.4 & 49.6 & 144.8 & 138.2 \\
        GIT2~\citep{wang2022git} & 12.9B & \underline{130.6} & \underline{126.9} & \underline{122.3} & \underline{124.8} & 50.7 & 145.0 & \underline{145.0} \\
        Qwen-VL~\citep{bai2023qwen} & 1.4B & - & 121.4 & - & - & 85.8 & - & - \\
        PaLI-17B~\citep{chen2022pali} & 1.6B & - & 127.0 & - & 124.4 & - & \underline{149.1} & 135.4 \\
        PaLI-X-55B~\citep{chen2023pali} & - & - & 126.3 & - & 124.3 & - & \textbf{149.2} & \textbf{147.0} \\
        \midrule
        CogVLM (ours) & 1.5B & \textbf{132.6} & \textbf{128.3} & \textbf{128.0} & \textbf{126.4} & \textbf{94.9} & 148.7 & 144.9 \\
\bottomrule
    \end{tabular}
  } }
\end{table*}
%
 

\subsection{Visual Question Answering}


Visual Question Answering is a task of validating general multi-modal capabilities of models, which requires a mastery of skills including vision-language understanding and commonsense reasoning. We evaluate our model on 7 VQA benchmarks: VQAv2, OKVQA, GQA, VizWiz-QA, OCRVQA, TextVQA, ScienceQA, covering a wide range of visual scenes.
We train our base model on the training sets and evaluate it on the publicly available val/test sets for all benchmarks, where both procedures adopt the open-vocabulary generation settings without OCR pipeline input.


\begin{table*}[htbp]
  \vspace*{-0.1cm}
\caption{Performance on Visual Question Answering benchmarks, where the results labeled with * refers to the few-shot or zero-shot setting. }
  \resizebox{\textwidth}{!}{\centering
  \renewcommand{\arraystretch}{1.15}
  \setlength{\tabcolsep}{1pt}
  \small
  {
    \begin{tabular}{lp{0.8cm}p{0.8cm}cp{1.2cm}p{0.8cm}p{0.8cm}>{\centering}p{1.2cm}>{\centering}p{1.2cm}c}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{p{0.4cm}}{\textbf{VQAv2}} & \multicolumn{1}{r}{\textbf{OKVQA}} & \multicolumn{1}{c}{\textbf{GQA}} & \multicolumn{2}{l}{\textbf{VizWizQA}} & \multicolumn{1}{c}{\textbf{OCRVQA}} & \multicolumn{1}{c}{\textbf{TextVQA}} & \textbf{SciQA}\\
    \cmidrule(r{10pt}){2-3} \cmidrule(r){4-4} \cmidrule(r){5-5} \cmidrule(r){6-7} \cmidrule(r){8-8} \cmidrule(r){9-9} \cmidrule(r){10-10}
    & test-dev & test-std & val & test-balanced & test-dev & test-std & test & test & IMG \\
        \midrule
        \multicolumn{3}{l}{\it{Closed-ended classification models}} \\
        \hline
        SimVLM~\citep{wang2021simvlm} & 80.0 & 80.3 & - & - & - & - & -  & - & -\\
        CoCa~\citep{yu2022coca} & 82.3 & 82.3 & - & - & - & - & -  & - & -\\
        OFA~\citep{wang2022ofa} & 82.0 & 82.0 & - & - & - & - & -  & - & -\\
        BEiT-3~\cite{wang2022image} & 84.2 & 84.0 & - & - & - & -  & - & - & -\\
        \hline
        \multicolumn{3}{l}{\it{Open-ended generation models}} \\
        \hline
        GIT~\citep{wang2022git} &  78.6 & 78.8 & - & - & 68.0 & 67.5 & 68.1  & 59.8 & -\\
        GIT2~\citep{wang2022git} &  81.7 & 81.9 & - & - & 71.0 & 70.1 & 70.3  & 67.3 & -\\
        Flamingo-80B~\citep{alayrac2022flamingo} & 82.0 & 82.1 & 57.8* & - & 65.7 & 65.4 & -  & 54.1 & -\\
        BLIP-2~\citep{li2023blip} & 82.2 & 82.3 & 59.3 & 44.7* & - & - & 72.7  & - & 89.5\\
        InstructBLIP~\citep{dai2023instructblip} & - & - & 62.1 & \underline{49.5}* & 34.5* & - & 73.3  & 50.7* & 90.7\\
        PaLI-17B~\cite{chen2022pali} & 84.3 & 84.3 & 64.5 & - & 71.6 & 70.7 & -  & 58.8 & -\\
        PaLI-X-55B~\citep{chen2023pali} & \textbf{86.0} & \textbf{86.1} & \textbf{66.1} & - & \underline{72.6} & \underline{70.9} & \textbf{75.0}  & \textbf{71.4} & -\\
        PaLM-E-84B~\citep{driess2023palm} & 80.5 & - & 63.3 & - & - & - & - & - & -\\
\hline
        CogVLM (ours) & \underline{84.7} & \underline{84.7} & \underline{64.7} & \textbf{65.2} & \textbf{76.4} & \textbf{75.8} & \underline{74.5}  & \underline{69.7} & \textbf{92.7}\\
\bottomrule
    \end{tabular}
  }
 }
\label{vqa}
\end{table*}



 As shown in Table~\ref{vqa}, our model achieves state-of-the-art performance on 6 of 7 benchmarks compared with models of similar scales, such as PALI-17B and Qwen-VL. Our model even surpasses models of much larger scale on multiple benchmarks, such as PaLI-X-55B on VizWiz-QA (test-std +5.1, test-dev +3.8), PALM-E-84B on VQAv2 (test-dev +4.2) and OKVQA(+1.4), Flamingo-80B on VQAv2 (test-dev +2.7, test-std +2.6), VizWiz-QA (test-dev +10.7, test-std +10.4) and TextVQA (+15.6).
Our model also achieves the optimal scores of $92.71$ on the multi-modal split (\emph{i.e.,} IMG) of ScienceQA~\citep{lu2022learn}, achieving a new SOTA.
These results suggest that our base model can serve as a strong multi-modal backbone capable of solving various visual question answering tasks.

\textbf{Generalist performance. }
In order to fairly compare with Unified-IO~\citep{lu2022unified}, Qwen-VL~\citep{bai2023qwen}, mPLUG-DocOwl~\citep{ye2023mplug} and other models trained in a generalist paradigm across multi-modal tasks, we further trained a unified model using data composed of dozens of multi-modal datasets and utilized a consistent checkpoint for evaluation. The datasets encompass 14 QA datasets such as VQAv2, OKVQA, and extending to TextVQA, as well as caption datasets including COCO caption, TextCaps, and those used during the pre-training phase. Experimental results show that multitask learning does not significantly reduce the model's performance on individual tasks, and CogVLM remains leading in performance across all tasks.
\begin{table*}[htbp]
\caption{Generalist performance on Image Captioning and VQA benchmarks.}
  \resizebox{\textwidth}{!}{\centering
  \renewcommand{\arraystretch}{1.15}
  \setlength{\tabcolsep}{1pt}
  \small
  {
    \begin{tabular}{lcccccccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{1}{c}{\textbf{COCO}} & \multicolumn{1}{c}{\textbf{TextCaps}} & \multicolumn{1}{c}{\textbf{NoCaps}} & \multicolumn{1}{c}{\textbf{Flickr}} & \multicolumn{1}{c}{\textbf{VQAv2}} & \multicolumn{1}{c}{\textbf{OKVQA}} & \multicolumn{1}{c}{\textbf{TextVQA}}  & \multicolumn{1}{c}{\textbf{OCRVQA}}  \\
    & Karp.-test & val & val & Karp.-test & test-dev & val & val & test\\
    \midrule
    Qwen-VL~\citep{bai2023qwen}  & - & - & 121.4 & 85.8 & 79.5 & 58.6 & 63.8 & \textbf{75.7}  \\
    mPLUG-DocOwl~\citep{ye2023mplug} & - & 111.9 & - & - & - & - & 52.6 & - \\
    Unified-IO~\citep{lu2022unified} & 122.3 & - & 100.0 & - & 77.9 & 54.0 & - & -\\
    \midrule
    CogVLM (single task) & \color{dt}148.7 & \color{dt}149.8 & \color{dt}128.3 & \color{dt}94.9 & \color{dt}84.7 & \color{dt}64.7 & \color{dt}69.3 & \color{dt}74.5 \\
    CogVLM (generalist) & \textbf{147.0}\red{(-1.7)} & \textbf{151.3}\textcolor{green}{(+1.5)} & \textbf{126.2}\red{(-2.1)} & 
    \textbf{92.7}\red{(-2.2)} & \textbf{83.4}\red{(-1.3)} & \textbf{58.9}\red{(-5.8)} & \textbf{68.1}\red{(-1.2)} & 74.1\red{(-0.4)} \\

    \bottomrule
    \end{tabular}
  }
 }
\end{table*} 
\subsection{Visual Grounding}








In order to endow our model with consistent, interactive visual grounding capabilities, we collect a high-quality dataset covering 4 types of grounding data: (1) \textbf{Grounded Captioning (GC)} - image captioning datasets where each noun phrase within the caption is followed by the corresponding referential bounding boxes; (2) \textbf{Referring Expression Generation (REG)} - image-oriented datasets that each bounding box in the image is annotated with a descriptive textual expression that accurately characterizes and refers to the content within the specific region; (3) \textbf{Referring Expression Comprehension (REC)} - text-oriented datasets that each textual description is annotated with multiple referential links associating the phrases with corresponding boxes; (4) \textbf{Grounded Visual Question Answering (GroundedVQA)} - VQA-style datasets where the questions may contain region references in a given image.
The sources of grounding data are all publicly available, including Flickr30K Entities~\citep{plummer2015flickr30k}, RefCOCO~\citep{kazemzadeh2014referitgame,mao2016generation,yu2016modeling}, Visual7W~\citep{zhu2016visual7w}, VisualGenome~\citep{krishna2017visual} and Grounded CoT-VQA~\citep{chen2023shikra}.
$[box]$ in this section is in the format of $[[x_0, y_0, x_1, y_1]]$. 



After the second pretraining stage using our 40M visual grounding dataset, we continue to train our model on this high-quality dataset, resulting in a generalist grounding-enhanced model, CogVLM-Grounding.
It is noteworthy that the curated datasets exhibit a versatility of visual grounding capabilities, and many datasets can be adapted and repurposed across different tasks.
For instance, grounded captioning datasets can be reformulated to suit REG and REC tasks. Taking the example of \textit{``A man $[box_1]$ and a woman $[box_2]$ are walking together.''}, this can be reframed into question answering pairs like \textit{(``Describe this region $[box_2]$.'', ``A woman.'')} and \textit{(``Where is the man?'', ``$[box_1]$'')}. Similarly, REC datasets can be translated into REG tasks by switching the input and output, and vice versa. However, certain conversions might lead to ambiguities. For example, when presented with the isolated query ``Where is another man?'' from the caption ``A man $[box_1]$ is running, while another man $[box_2]$ is looking.'', the distinction between $[box_1]$ and $[box_2]$ becomes unclear, potentially leading to errors.

Table~\ref{tab:grounding} shows the result on the standard visual grounding benchmarks.
We find that our generalist model achieves state-of-the-art performance across the board, with a significant advantage over the previous or concurrent models. Moreover, we also evaluate the specialist performance of our model finetuned on each individual training set of benchmarks for fair comparison with the best models dedicated on each task. As shown in the bottom part of Table~\ref{tab:grounding}, our model achieves the SOTA performance over 5 of 9 splits, and the compatible result on the other subsets. These results suggest a remarkable visual grounding capability of our model incorporating our training paradigm.

























\begin{table*}[htbp]
\centering
\caption{Results on Referring Expression Comprehension and Grounded Visual Question Answering.}
\setlength{\tabcolsep}{2.5pt}
  \resizebox{\textwidth}{!}{\begin{tabular}{p{1.5cm}|l|ccccccccc}
\toprule
\multirow{2}{*}{\quad \textbf{Type}} & \multirow{2}{*}{\quad\quad\quad\quad\quad\, \textbf{Model}} & \multicolumn{3}{c}{\textbf{RefCOCO}} & \multicolumn{3}{c}{\textbf{RefCOCO+}} & \multicolumn{2}{c}{\textbf{RefCOCOg}} & \textbf{Visual7W} \\
\cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-10} \cmidrule(r){11-11}
 &  & val & test-A & test-B & val & test-A & test-B & val & test & test \\ \midrule
\multirow{7}{*}{\textit{Generalist}} & OFA-L*~\citep{wang2022ofa} & 79.96 & 83.67 & 76.39 & 68.29 & 76.00 & 61.75 & 67.57 & 67.58 & -\\
 & VisionLLM-H~\citep{wang2023visionllm} &  - & 86.70 & - & - & - & - & - & - & -\\
 & Shikra-7B~\citep{chen2023shikra} & 87.01 & 90.61 & 80.24 & 81.60 & 87.36 & 72.12 & 82.27 & 82.19 & - \\
 & Shikra-13B & 87.83 & 91.11 & 81.81 & 82.89 & 87.79 & 74.41 & 82.64 & 83.16 & 85.33\\
 & Qwen-VL~\citep{bai2023qwen} & 89.36 & 92.26 & 85.34 & 83.12 & 88.25 & 77.21 & 85.58 & 85.48 & -\\
& \textbf{CogVLM} & \textbf{92.51} & \textbf{93.95} & \textbf{88.73} & \textbf{87.52} & \textbf{91.81} & \textbf{81.43} & \textbf{89.46} & \textbf{90.09} & \textbf{90.96} \\ \midrule
\multirow{4}{*}{\textit{Specialist}} & G-DINO-L~\cite{liu2023grounding} & 90.56 & 93.19 & 88.24 & 82.75 & 88.95 & 75.92 & 86.13 & 87.02 & -\\
 & UNINEXT-H~\citep{lin2023uninext} & 92.64 & \textbf{94.33} & \textbf{91.46} & 85.24 & 89.63 & 79.79 & 88.73 & 89.37 & -\\
 & ONE-PEACE~\citep{wang2023one} & 92.58 & 94.18 & 89.26 & \textbf{88.77} & 92.21 & \textbf{83.23} & 89.22 & 89.27 & -\\
 & \textbf{CogVLM (single task)} & \textbf{93.40} & 94.06 & 90.28 & 87.76 & \textbf{93.02} & 81.81 & \textbf{90.07} & \textbf{90.53} & \textbf{91.17} \\
 \bottomrule
\end{tabular}}
\label{tab:grounding}

\end{table*} 





\subsection{Instruction Following in Real-world User Behavior}
To evaluate the CogVLM-Chat model’s capacity under real-world user behavior, we further employ TouchStone~\citep{bai2023touchstone}, an extensive benchmark for multimodal language models. 
Table ~\ref{tab:chat} shows the GPT-4~\citep{openai2023} similarity scores of the generated and standard answer, suggesting CogVLM-Chat significantly outperforms all the other publicly available VLMs.







    \begin{table}[ht]
  \caption{Evaluation results on TouchStone in English.}
  \renewcommand{\arraystretch}{1.15}
  \resizebox{\textwidth}{!}{\begin{tabular}{l|ccccccc}
\toprule
\textbf{Models} & MiniGPT4 & InstructBLIP & LLaMA-AdapterV2 & LLaVA & mPLUG-Owl & Qwen-VL-Chat & \textbf{CogVLM-Chat} \\ \midrule
\textbf{Score}  & 531.7    & 552.4        & 590.1           & 602.7 & 605.4     & 645.4        & \textbf{662.6}     \\ \bottomrule
\end{tabular}
\label{tab:chat}
}
\end{table}
 






\begin{table}[h]
\vspace*{-0.1cm}

\caption{{Ablation studies for various components and training settings.} 
}
\vspace*{-0.1cm}
\setlength{\tabcolsep}{1pt}
\resizebox{\textwidth}{!}{\begin{tabular}{cc|cc|c|ccccc}
\toprule
& \multirow{2}{*}{Ablated Aspects}   & \multirow{2}{*}{Original (CogVLM)}  & \multirow{2}{*}{Ablated Setting} & \small{Trainable}& COCO & NoCaps & OKVQA  & TextVQA & VQAv2  \\
& & & & \small{params} & CIDEr$\uparrow$  &  CIDEr$\uparrow$ & top1$\uparrow$     & top1$\uparrow$   & top1$\uparrow$  \\ \midrule

& \multirow{4}{*}{Tuned Parameters} & \multirow{4}{*}{\begin{minipage}{2.8cm}
    \begin{center}
        \textit{VE-full} every layer \\ + \\ MLP Adapter
    \end{center}
\end{minipage}} & \multicolumn{1}{c|}{MLP Adapter} & 140M & 131.2	& 111.5	& 55.1	& 40.7	& 73.8\\
& &  & \multicolumn{1}{c|}{LLM+MLP Adapter} & 6.9B & 140.3	& 118.5	& 56.8	& 44.7	& 78.9\\
& & & \multicolumn{1}{c|}{\textit{VE-full} every 4th layer} & 1.7B & 138.7	& 117.4	& 58.9	& 44.1 & 77.6\\
& & & \multicolumn{1}{c|}{\textit{VE-FFN} every layer} & 4.4B & 140.0	& 118.7	& 58.2	& 45.1 & 78.6\\
\midrule
& Init method & From LLM & Random init & 6.6B & 138.0	& 117.9	& 55.9	& 44.0 & 79.1\\
\midrule
& Visual attention mask & Causal mask & Full mask & 6.6B & 141.0 & 117.2 &	57.4 &	45.1 &	79.6 \\
\midrule
& Image SSL loss & \XSolidBrush & \Checkmark(clip feature) & 6.6B & 142.9 & 119.8 & 58.7 & 45.9 & 79.7\\
\midrule
&EMA & \Checkmark & \XSolidBrush & 6.6B & 143.1 & 119.2 & 57.1 & 43.8 & 79.4\\
\midrule
& \textit{CogVLM (ours)} & --- & --- & 6.6B & 142.8	& 120.1	& 59.3	& 45.3	& 80.0 \\ 
\midrule
\end{tabular}}
\vspace*{-0.4cm}
\label{tab:ablation-table-no-classif}
\end{table} 
\subsection{Ablation Study}
To understand the impact of various components and settings on our model's performance, we conduct an extensive ablation study for 6,000 iterations and a batch size of 8,192. Table~\ref{tab:ablation-table-no-classif} summarizes the results about the following aspects:



\textbf{Model structure and tuned parameters}. We investigate the effectiveness of tuning only the MLP Adapter layer or tuning all LLM parameters and the Adapter without adding VE, as well as modifying the VE architecture to add full VE at every 4th LLM layer or only the FFN-equipped VE at all layers.
From the results we can see that only tuning the adapter layer (\emph{e.g.,} BLIP2) may result in a shallow alignment with significantly inferior performance, and decreasing either the number of VE layers or the VE parameters at each LLM layer suffers a prominent degradation.




\textbf{Initialization Method}. We investigate the effectiveness of initializing VE weights from LLM, and the slight decrease in performance suggests a positive impact of this method.

\textbf{Visual Attention Mask}. We empirically find that using a causal mask on visual tokens will yield a better result in comparison with a full mask. We hypothesize the possible explanation for this phenomenon is that the causal mask better fits the inherent structure of LLM.

\textbf{Image SSL Loss}. We also investigated the self-supervised learning loss on image features, where each visual feature predicts the CLIP feature of the next position for visual self-supervision. Align with the observation from PaLI-X~\citep{chen2023pali}, we find it brings no improvement on downstream tasks, although we indeed observed improvements in small models in our early experiments.

\textbf{EMA}.
We utilize EMA (Exponential Moving Average) during pretraining, which often brings improvements across various tasks.
