\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath}\usepackage{pifont}
\usepackage{graphicx,verbatimbox}
\usepackage{hyperref} 
\usepackage{tcolorbox}


\usepackage{xspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{color}
\usepackage{palatino}


\newcommand{\stdminus}[1]{ \scalebox{0.65}{}}


\newcommand{\cmark}{\ding{51}\xspace}\newcommand{\cmarkg}{\textcolor{lightgray}{\ding{51}}\xspace}\newcommand{\xmark}{\ding{55}\xspace}\newcommand{\xmarkg}{\textcolor{lightgray}{\ding{55}}\xspace}\newcommand{\nomark}{~}\newcommand{\adamw}{adamw}
\newcommand{\adamwg}{\textcolor{lightgray}{adamw}}
\newcommand{\sgd}{SGD}
\newcommand{\sgdg}{\textcolor{lightgray}{SGD}}

\def \ours          {DeiT\xspace}
\def \ourstiny      {DeiT-Ti\xspace}
\def \ourssmall     {DeiT-S\xspace}
\def \oursbase      {DeiT-B\xspace}
\def \ourslarge     {DeiT-L\xspace}
\def \ourstinydis      {DeiT-Ti\alambic\xspace}
\def \ourssmalldis     {DeiT-S\alambic\xspace}
\def \oursbasedis   {DeiT-B\alambic\xspace}
\def \oursbaseup    {DeiT-B}
\def \ourstinyup      {DeiT-Ti}
\def \ourssmallup     {DeiT-S}
\def \oursbasedisup {DeiT-B\alambic}
\def \ourstinydisup     {DeiT-Ti\alambic}
\def \ourssmalldisup     {DeiT-S\alambic}

\def \distil  {\vspace{-2pt}\alambic}
\def \oursdis {DeiT\alambic}
\def \oursdisup {DeiT\alambic}

\def \oursfix {FixDeiT-B\xspace}
\def \oursfixdis {FixDeiT-B\alambic\xspace}

\def \oursfixdistil {FixDeiT-B\alambic\xspace}

\def \pzo {\phantom{0}} \def \alambic {\includegraphics[width=0.02\linewidth]{figs/alembic-crop.pdf}\xspace}



\title{Training data-efficient image transformers \\ \& distillation through attention}
\author{
\begin{minipage}{\linewidth}
\begin{center}
\large Hugo Touvron \hspace{0.23cm} Matthieu Cord \hspace{0.23cm} Matthijs Douze \0.5cm]
\scalebox{1.}{Facebook AI\hspace{0.6cm} Sorbonne University}\
    \mathrm{Attention}(Q, K, V) = \mathrm{Softmax}(Q K^\top/\sqrt{d}) V,

    \mathcal{L}_\mathrm{global} = (1-\lambda) \mathcal{L}_\mathrm{CE}(\psi(Z_\mathrm{s}),y) + \lambda  \tau^2 \mathrm{KL}(\psi(Z_\mathrm{s}/\tau),\psi(Z_\mathrm{t}/\tau)).

    \mathcal{L}_\mathrm{global}^\mathrm{hard Distill} =  \frac{1}{2}\mathcal{L}_\mathrm{CE}(\psi(Z_s),y) +  \frac{1}{2}\mathcal{L}_\mathrm{CE}(\psi(Z_s),y_\mathrm{t}).



For a given image, the hard label associated with the teacher may change depending on the specific data augmentation. 
We will see that this choice is better than the traditional one, while being parameter-free and conceptually simpler: The teacher prediction  plays the same role as the true label .

Note also that the hard labels can also be converted into soft labels with label smoothing~\cite{Szegedy2016RethinkingTI}, where the true label is considered to have a probability of , and the remaining  is shared across the remaining classes. We fix this parameter to  in our all experiments that use true labels.  

\paragraph{Distillation token.} 
We now focus on our proposal, which is illustrated in Figure~\ref{fig:distillation}. 
We add a new token, the distillation token, to the initial embeddings (patches and class token).
Our distillation token is used similarly as the class token: it interacts with other embeddings through self-attention, and is output by the network after the last layer.
Its target objective is given by the distillation component of the loss. 
The distillation embedding allows our model to learn from the output of the teacher, as in a regular distillation, while remaining complementary to the class embedding.


Interestingly, we observe that the learned class and distillation tokens converge towards different vectors: the average cosine similarity between these tokens equal to 0.06. As the class and distillation embeddings are computed at each layer, they gradually become more similar through the network, all the way through the last layer at which their similarity is high (cos=0.93), but still lower than 1. This is expected since as they aim at producing targets that are similar but not identical. 

We verified that our distillation token adds something to the model, compared to simply adding an additional class token associated with the same target label:  instead of a teacher pseudo-label, we experimented with a transformer with two class tokens. Even if we initialize them randomly and independently, during training they converge towards the same vector (cos=0.999), and the output embedding are also quasi-identical.  This additional class token does not bring anything to the classification performance. 
In contrast, our distillation strategy provides a significant improvement over a vanilla distillation baseline, as validated by our experiments in Section~\ref{sec:distillation_results}.  

\paragraph{Fine-tuning with distillation.} 
We use both the true label and teacher prediction during the fine-tuning stage at higher resolution. We use a teacher with the same target resolution, typically obtained from the lower-resolution teacher by the method of Touvron et al~\cite{Touvron2019FixRes}. We have also tested with true labels only but this reduces the benefit of the teacher and leads to a lower performance. 


\paragraph{Classification with our approach: joint classifiers.} 
At test time,  both the class or the distillation embeddings produced by the transformer are associated with linear classifiers and able to infer the image label. Yet our referent method is the late fusion of these two separate heads, for which we add the softmax output by the two classifiers to make the prediction. We evaluate these three options in Section~\ref{sec:experiments}. 
 \section{Experiments}
\label{sec:experiments}

This section presents a few analytical experiments and results. We first discuss our distillation strategy. Then we comparatively analyze the efficiency and accuracy of convnets and vision transformers. 

\subsection{Transformer models}
As mentioned earlier, our architecture design is identical to the one proposed by Dosovitskiy et al.~\cite{dosovitskiy2020image} with no convolutions. 
Our only differences are the training strategies, and the distillation token. Also we do not use a MLP head for the pre-training but only a linear classifier. 
To avoid any confusion, we refer to the results obtained in the prior work by ViT, and prefix ours by \ours. If not specified, \ours refers to our referent model \oursbase, which has the same architecture as ViT-B. 
When we fine-tune \ours at a larger resolution, we append the resulting operating resolution at the end, e.g, \oursbaseup384.  
Last, when using our distillation procedure, we identify it with an alembic sign as \oursdis. 

The parameters of ViT-B (and therefore of \oursbase) are fixed as  ,  and .
We introduce two smaller models, namely \ourssmall and \ourstiny, for which we change the number of heads, keeping  fixed. Table~\ref{tab:models} summarizes the models that we consider in our paper. 


\begin{table}[t]
\caption{Variants of our DeiT architecture.
The larger model, DeiT-B, has the same architecture as the ViT-B~\cite{dosovitskiy2020image}.
The only parameters that vary across models are the embedding dimension and the number of heads, and we keep the dimension per head constant (equal to 64). 
Smaller models have a lower parameter count, and a faster throughput. The throughput is measured for images at resolution 224224. 
\label{tab:models}}
\smallskip
\centering
\scalebox{0.8}
{
\begin{tabular}{l|cccccccc}
\toprule
Model       & ViT model         & embedding  & \#heads & \#layers & \#params & training & throughput  \\
            &                   & dimension  &         &           &          & resolution & (im/sec)\\
\midrule
\ourstiny   &  N/A  &  \pzo192  & \pzo3 & 12 & \pzo\pzo5M & 224 & 2536 \\
\ourssmall  &  N/A  &  \pzo384 & \pzo6 & 12 & \pzo22M& 224& \pzo940 \\
\oursbase   &  ViT-B & \pzo768   & 12 & 12 & \pzo86M & 224 & \pzo292\\
\bottomrule
\end{tabular}}\end{table}


\subsection{Distillation}
\label{sec:distillation_results}

Our distillation method produces a vision transformer that becomes on par with the best convnets in terms of the trade-off between accuracy and throughput, see Table~\ref{tab:throughput}. Interestingly, the distilled model outperforms its teacher in terms of the trade-off between accuracy and throughput. Our best model on ImageNet-1k is 85.2\% top-1 accuracy outperforms the best Vit-B model pre-trained on JFT-300M at resolution 384 (84.15\%). For reference, the current state of the art of 88.55\% achieved with extra training data was obtained by the ViT-H model (600M parameters) trained on JFT-300M at resolution 512. 
Hereafter we provide several analysis and observations. 

\paragraph{Convnets teachers.}

We have observed that using a convnet teacher gives better performance than using a transformer.
Table~\ref{tab:teacher} compares distillation results with different teacher architectures.
The fact that the convnet is a better teacher is probably due to the inductive bias inherited by  the transformers through distillation, as explained in Abnar et al.~\cite{abnar2020transferring}.
In all of our subsequent distillation experiments the default teacher is a RegNetY-16GF~\cite{Radosavovic2020RegNet} (84M parameters) that we trained with the same data and same data-augmentation as \ours.
This teacher reaches  top-1 accuracy on ImageNet.


\begin{table}[t]
\caption{We compare on ImageNet~\cite{Russakovsky2015ImageNet12} the performance (top-1 acc., \%) of the student as a function of the teacher model used for distillation. \label{tab:teacher}
\smallskip
}
\centering
\scalebox{0.9}
{
\begin{tabular}{lc|cc}
\toprule
\multicolumn{2}{c}{Teacher}   & \multicolumn{2}{c}{Student: \oursbase\distil} \\
 Models & acc. & pretrain & 384\\
\midrule
\oursbase  & 81.8 & 81.9 & 83.1\\
\midrule
RegNetY-4GF & 80.0 & 82.7 & 83.6\\
RegNetY-8GF & 81.7 & 82.7 &  83.8\\
RegNetY-12GF & 82.4 & 83.1 & 84.1\\
RegNetY-16GF & 82.9 &  83.1 & 84.2\\
\bottomrule
\end{tabular}}
\end{table}


\paragraph{Comparison of distillation methods.} 

We compare the performance of different distillation strategies in Table~\ref{tab:distillation}. 
Hard distillation significantly outperforms soft distillation for transformers, even when using only a class token: hard distillation reaches 83.0\% at resolution 224224, compared to the soft distillation accuracy of 81.8\%.
Our distillation strategy from Section~\ref{sec:distillation} further improves the performance, showing that the two tokens provide complementary information useful for classification: the classifier on the two tokens is significantly better than the independent class and distillation classifiers, which by themselves already outperform the distillation baseline. 

The distillation token gives slightly better results than the class token. 
It is also more correlated to the convnets prediction. This difference in performance is probably due to the fact that it benefits more from the inductive bias of convnets.
We give more details and an analysis in the next paragraph.
The distillation token has an undeniable advantage for the  initial training. 

\paragraph{Agreement with the teacher \& inductive bias?}
As discussed above, the architecture of the teacher has an important impact.  Does it inherit existing inductive bias that would facilitate the training?  While we believe it difficult to formally answer this question, we analyze in  
Table~\ref{tab:agreement} the decision agreement between the convnet teacher, our image transformer \ours learned from labels only, and our transformer \oursdis. 

Our distilled model is more correlated to the convnet than with a transformer learned from scratch. As to be expected, the classifier associated with the distillation embedding is closer to the convnet that the one associated with the class embedding, and conversely the one associated with the class embedding is more similar to \ours learned without distillation. Unsurprisingly, the joint class+distil classifier offers a middle ground.  


\begin{table}[t]
\caption{Distillation experiments on Imagenet with \ours, 300 epochs of pre-training. 
We report the results for our new distillation method in the last three rows. 
We separately report the performance when classifying with only one of the class or distillation embeddings, and then with a classifier taking both of them as input.  \label{tab:distillation}
In the last row (class+distillation), the result correspond to the late fusion of the class and distillation classifiers. }
\smallskip
\centering
\scalebox{0.88}
{
\begin{tabular}{l|cc|cccc}
\toprule
                              &  \multicolumn{2}{c}{Supervision}  &  \multicolumn{4}{c}{ImageNet top-1 (\%)} \\
 method           & label & teacher        & Ti 224  & S 224 & B 224 & B384 \\
\midrule
 \ours -- no distillation    & \cmark &  \xmark  & 72.2 & 79.8    & 81.8  & 83.1 \\
 \ours -- usual distillation & \xmark & soft & 72.2 & 79.8    & 81.8  &  83.2    \\ 
 \ours -- hard  distillation &  \xmark& hard & 74.3 & 80.9    & 83.0  & 84.0 \\ 
\midrule 
\oursdis: class embedding   & \cmark & hard & 73.9    & 80.9  & 83.0  &  84.2\\
\oursdis: distil. embedding & \cmark & hard & 74.6      &  81.1  & 83.1 & 84.4 \\ 
\oursdis: class+distillation& \cmark & hard & 74.5      & 81.2 & 83.4  &  84.5 \\
\bottomrule
\end{tabular}}
\end{table}





\paragraph{Number of epochs.} Increasing the number of epochs significantly improves the performance of training with distillation, see Figure~\ref{fig:distillation_epochs}. With 300 epochs, our distilled network \oursbasedis is already better than \oursbase. But while for the latter the performance saturates with longer schedules, our distilled network clearly benefits from a longer training time. 


\begin{table}[t]
\caption{Disagreement analysis between convnet, image transformers and distillated transformers: We report the fraction of sample classified differently for all classifier pairs, i.e., the rate of different decisions. We include two models without distillation (a RegNetY and DeiT-B), so that we can compare how our distilled models and classification heads are correlated to these teachers. \smallskip  
\label{tab:agreement}}
\centering
\scalebox{0.86}
{
\begin{tabular}{l|c|cc|ccc}
\toprule
             & \multirow{2}{*}{groundtruth}  &  \multicolumn{2}{c}{no distillation}  & \multicolumn{3}{c}{\oursdis student (of the convnet)} \\ 
             &              & convnet & \ours &  class  &  distillation &  \oursdis \\
\midrule 
groundtruth                 & 0.000        & 0.171   & 0.182    & 0.170  &  0.169 &  0.166      \\
convnet (RegNetY)           & 0.171        & 0.000   & 0.133    & 0.112  & 0.100 &   0.102     \\
\ours                       & 0.182        & 0.133   &  0.000   &   0.109            & 0.110            &  0.107      \\
\midrule
\oursdis -- class only   &  0.170 &  0.112 &  0.109       & 0.000         & 0.050 &   0.033     \\
\oursdis -- distil. only & 0.169 &  0.100 &   0.110      & 0.050  &  0.000     &  0.019      \\
\oursdis -- class+distil.&  0.166 &  0.102 &  0.107       &      0.033         &   0.019        &  0.000  \\
\bottomrule
\end{tabular}}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figs/epochs_dist_ImageNet.pdf}
    \caption{Distillation on ImageNet~\cite{Russakovsky2015ImageNet12} with \ours-B: performance as a function of the number of training epochs. We provide the performance without distillation (horizontal dotted line) as it saturates after 400 epochs. 
    \label{fig:distillation_epochs}}
\end{figure}



\subsection{Efficiency vs accuracy: a comparative study with convnets}


In the literature, the image classificaton methods are often compared as a compromise between accuracy and another criterion, such as FLOPs, number of parameters, size of the network, etc. 

We focus  in Figure~\ref{fig:efficiency} on the tradeoff between the throughput (images processed per second) and the top-1 classification accuracy on ImageNet. 
We focus on the popular state-of-the-art EfficientNet convnet, which has benefited from years of research on convnets and was optimized by architecture search on the ImageNet validation set. 

Our method \ours is slightly below EfficientNet, which shows that we have almost closed the gap between vision transformers and convnets when training with Imagenet only. These results are a major improvement (\textbf{+6.3\%} top-1 in a comparable setting) over previous ViT models trained on Imagenet1k only~\cite{dosovitskiy2020image}. 
Furthermore, when \ours benefits from the distillation from a relatively weaker RegNetY to produce \oursdis, it outperforms EfficientNet. It also outperforms by 1\% (top-1 acc.) the Vit-B model pre-trained on JFT300M at resolution 384 (85.2\% vs  84.15\%), while being significantly faster to train. 

Table~\ref{tab:throughput} reports the numerical results in more details and additional evaluations on ImageNet V2 and ImageNet Real, that have a test set distinct from the ImageNet validation, which reduces overfitting on the validation set. Our results show that \oursbasedis and \oursbasedisup384  outperform, by some margin, the state of the art on the trade-off between accuracy and inference time on GPU. 

\begin{table}[p]
    \centering
    \scalebox{0.85}
    {\small 
    \begin{tabular}{@{\ }l|r|c@{\ }c|c@{\ }|c@{\ }|c@{\ }}
    \toprule
            &              &     image       & throughput & \multicolumn{1}{c}{ImNet} & \multicolumn{1}{|c|}{Real}& \multicolumn{1}{c}{V2}\\ 
    Network & \#param. & size & (image/s) &  top-1 &  top-1 &  top-1\\
    \toprule
    \multicolumn{7}{c}{Convnets}\\
    \midrule
    ResNet-18~\cite{He2016ResNet} & 12M &  & 4458.4 &  69.8  & 77.3 & 57.1\\
    ResNet-50~\cite{He2016ResNet} & 25M &  & 1226.1 & 76.2 & 82.5 & 63.3\\
    ResNet-101~\cite{He2016ResNet}& 45M &  & \pzo753.6 & 77.4 & 83.7 & 65.7\\
    ResNet-152~\cite{He2016ResNet} & 60M &  & \pzo526.4 & 78.3& 84.1 & 67.0 \\
    \midrule
    RegNetY-4GF~\cite{Radosavovic2020RegNet}  &  21M &  & 1156.7 & 80.0 & 86.4 & 69.4 \\
    RegNetY-8GF~\cite{Radosavovic2020RegNet}  &  39M &  & \pzo591.6 & 81.7 & 87.4  & 70.8 \\ 
    RegNetY-16GF~\cite{Radosavovic2020RegNet} &  84M &  & \pzo334.7 & 82.9 & 88.1 & 72.4 \\
    \midrule
    EfficientNet-B0~\cite{tan2019efficientnet} & 5M &  & 2694.3 & 77.1 & 83.5 & 64.3\\
    EfficientNet-B1~\cite{tan2019efficientnet} & 8M &  & 1662.5 & 79.1  & 84.9 & 66.9 \\
    EfficientNet-B2~\cite{tan2019efficientnet} & 9M &  & 1255.7  & 80.1 & 85.9 & 68.8 \\
    EfficientNet-B3~\cite{tan2019efficientnet} &12M &  & \pzo732.1 & 81.6  & 86.8 & 70.6 \\
    EfficientNet-B4~\cite{tan2019efficientnet} & 19M &  & \pzo349.4 & 82.9  & 88.0& 72.3 \\
    EfficientNet-B5~\cite{tan2019efficientnet} & 30M &  & \pzo169.1 & 83.6  & 88.3 & 73.6 \\
    EfficientNet-B6~\cite{tan2019efficientnet} & 43M &  & \pzo\pzo96.9 & 84.0 & 88.8 & 73.9 \\
    EfficientNet-B7~\cite{tan2019efficientnet} & 66M &  & \pzo\pzo55.1 & 84.3 & \_& \_ \\
    \midrule
    EfficientNet-B5 RA~\cite{Cubuk2019RandAugmentPA} & 30M &  & \pzo\pzo96.9 & 83.7  & \_& \_ \\
    EfficientNet-B7 RA~\cite{Cubuk2019RandAugmentPA} & 66M &  & \pzo\pzo55.1 & 84.7 & \_& \_ \\
    \midrule
    KDforAA-B8 & 87M &  & \pzo\pzo25.2 & 85.8 & \_ & \_ \\
    \toprule
    \multicolumn{7}{c}{Transformers}\\
    \midrule
    ViT-B/16~\cite{dosovitskiy2020image} & 86M  &  & \pzo\pzo85.9 & 77.9 & 83.6 & \_ \\
    ViT-L/16~\cite{dosovitskiy2020image} & 307M &  & \pzo\pzo27.3 & 76.5  & 82.2 & \_ \\
    \midrule
    \ourstiny & 5M &  & 2536.5 & 72.2  & 80.1  &  60.4\\
    \ourssmall & 22M &  & \pzo940.4 & 79.8 & 85.7  & 68.5 \\
    \oursbase & 86M &  & \pzo292.3 & 81.8  & 86.7 & 71.5\\
    \midrule
    \oursbaseup384 & 86M &  & \pzo\pzo85.9 & 83.1 & 87.7 & 72.4\\
    
    \midrule
    \ourstinydis & 6M &  & 2529.5 & 74.5  &  82.1  & 62.9 \\
    \ourssmalldis & 22M &  & \pzo936.2 & 81.2 & 86.8  & 70.0  \\

    \oursbasedis & 87M &   & \pzo290.9 & 83.4 & 88.3 & 73.2 \\
    \midrule
    \ourstinydis / 1000 epochs  & 6M &   & 2529.5 & 76.6  & 83.9 & 65.4 \\
    \ourssmalldis / 1000 epochs & 22M &   & \pzo936.2 & 82.6 & 87.8 & 71.7\\
    \oursbasedis / 1000 epochs  & 87M &   & \pzo290.9 & 84.2 & 88.7 & 73.9 \\


    \midrule
    \oursbasedisup384 & 87M &  & \pzo\pzo85.8 & 84.5  & 89.0 & 74.8 \\
    \midrule
    \oursbasedisup384 / 1000 epochs & 87M &  & \pzo\pzo85.8 & 85.2  & 89.3 & 75.2 \\
    \bottomrule
    \end{tabular}}
    \caption{Throughput on and accuracy on Imagenet~\cite{Russakovsky2015ImageNet12}, Imagenet Real~\cite{Beyer2020ImageNetReal} and Imagenet V2 matched frequency~\cite{Recht2019ImageNetv2} of \ours and of several state-of-the-art convnets, for models trained with no external data.  The throughput is measured as the number of images that we can process per second  on one 16GB V100 GPU. For each model we take the largest possible batch size for the usual resolution of the model and calculate the average time over 30 runs to process that batch. With that we calculate the number of images processed per second. Throughput can vary according to the implementation: for a direct comparison and in order to be as fair as possible, we use for each model the definition in the same GitHub~\cite{pytorchmodels} repository. 
    \newline 
     \textit{Regnet optimized with a similar optimization procedure as ours, which boosts the results. These networks serve as teachers when we use our distillation strategy. }
    \label{tab:throughput}}
\end{table}





\subsection{Transfer learning: Performance on downstream tasks}

Although \ours perform very well on ImageNet it is important to evaluate them on other datasets with transfer learning in order to measure the power of generalization of \ours.
We evaluated this on transfer learning  tasks by fine-tuning on the datasets in Table~\ref{tab:dataset}.
Table~\ref{tab:sota} compares \ours transfer learning results  to those of ViT~\cite{dosovitskiy2020image} and state of the art convolutional architectures~\cite{tan2019efficientnet}.
\ours is on par with competitive convnet models, which is in line with our previous conclusion on ImageNet.

\begin{table}[t]
\caption{Datasets used for our different tasks.  \label{tab:dataset}}
\smallskip
\centering
{\small
\begin{tabular}{l|rrr}
\toprule
Dataset & Train size & Test size & \#classes   \\
\midrule
ImageNet \cite{Russakovsky2015ImageNet12}  & 1,281,167 & 50,000 & 1000  \\ 
iNaturalist 2018~\cite{Horn2018INaturalist}& 437,513   & 24,426 & 8,142 \\ 
iNaturalist 2019~\cite{Horn2019INaturalist}& 265,240   & 3,003  & 1,010  \\ 
Flowers-102~\cite{Nilsback08}& 2,040 & 6,149 & 102  \\ 
Stanford Cars~\cite{Cars2013}& 8,144 & 8,041 & 196  \\  
CIFAR-100~\cite{Krizhevsky2009LearningML}  & 50,000    & 10,000 & 100   \\ 
CIFAR-10~\cite{Krizhevsky2009LearningML}  & 50,000    & 10,000 & 10   \\ 
\bottomrule
\end{tabular}}
\vspace{-8pt}
\end{table}


\begin{table}[t]
    \caption{We compare Transformers based models on different transfer learning task with ImageNet pre-training. We also report results with convolutional architectures for reference. 
    \label{tab:sota}}
    \smallskip
    \centering
    \scalebox{0.68}{
\begin{tabular}{l|c|cccccc|r}
    \toprule
    Model                                      & ImageNet & CIFAR-10 & CIFAR-100  & Flowers & Cars & iNat-18 & iNat-19 & im/sec \\
    \midrule                                                                          
    Grafit ResNet-50~\cite{Touvron2020GrafitLF} & 79.6 & \_   & \_ & 98.2 & 92.5 & 69.8 & 75.9 & 1226.1\\
    Grafit RegNetY-8GF~\cite{Touvron2020GrafitLF} & \_ & \_   & \_ & 99.0 & 94.0 & 76.8 & 80.0 & 591.6\\
    ResNet-152~\cite{Chu2020FeatureSA} & \_ & \_ & \_ & \_ & \_ & 69.1 & \_ & 526.3\\
    EfficientNet-B7~\cite{tan2019efficientnet}  & 84.3 & 98.9 & 91.7  & 98.8 & 94.7 & \_ & \_ & 55.1\\
    \midrule                                                                          
    ViT-B/32~\cite{dosovitskiy2020image}        & 73.4 & 97.8 & 86.3 & 85.4 & \_   & \_ & \_ & 394.5\\
    ViT-B/16~\cite{dosovitskiy2020image}        & 77.9 & 98.1 & 87.1 & 89.5 & \_   & \_ & \_ & 85.9 \\
    ViT-L/32~\cite{dosovitskiy2020image}        & 71.2 & 97.9 & 87.1 & 86.4 & \_   & \_ & \_ & 124.1\\
    ViT-L/16~\cite{dosovitskiy2020image}        & 76.5 & 97.9 & 86.4 & 89.7 & \_   & \_ & \_ & 27.3 \\
\midrule
    \oursbase                                     & 81.8 & 99.1 &  90.8  & 98.4 &  92.1    & 73.2 & 77.7 &  292.3 \\
    \oursbaseup384    & 83.1 & 99.1 &  90.8  & 98.5 &  93.3    &  79.5 &  81.4    &  \pzo85.9 \\
    \oursbasedis   & 83.4 & 99.1  &  91.3  & 98.8 & 92.9 & 73.7  & 78.4 &   290.9\\
    \oursbasedisup384 & 84.4 & 99.2 & 91.4   & 98.9 & 93.9 & 80.1 & 83.0 &  \pzo85.9  \\
    \bottomrule
    \end{tabular}}
\end{table}


\paragraph{Comparison vs training from scratch.}
We investigate the performance when training from scratch on a small dataset, without Imagenet pre-training. We get the following results on the small CIFAR-10, which is small both w.r.t. the number of images and labels:
\begin{center}
    {\small 
    \begin{tabular}{c|c|c|c}
    \toprule
    Method  & RegNetY-16GF & \ours-B & \oursbasedis \\
   Top-1 & 98.0 & 97.5 & 98.5 \\ 
    \bottomrule
    \end{tabular}}
\end{center}

For this experiment, we tried we get as close as possible to the Imagenet pre-training counterpart, meaning that (1) we consider longer training schedules (up to 7200 epochs, which corresponds to 300 Imagenet epochs) so that the network has been fed a comparable number of images in total; (2) we re-scale images to  to ensure that we have the same augmentation. 
The results are not as good as with Imagenet pre-training (98.5\% vs 99.1\%), which is expected since the network has seen a much lower diversity. However they show that it is possible to learn a reasonable transformer on CIFAR-10 only.

\begin{comment}
\begin{table}[t]
    \centering
    \scalebox{1.0}
    {\small 
    \begin{tabular}{c|c|c|c}
    \toprule
    Method  & RegNetY-16GF & \ours-B & \oursbasedis \\
   Top-1 & 98.0 & 97.5 &  \\ 
    \bottomrule
    \end{tabular}}
    \caption{ Performance with training on CIFAR-10 only.\hugo{Add results in the main text}
    \label{tab:cifar_only}}
\end{table}
\end{comment} \section{Training details \& ablation}
\label{sec:training}


\begin{table*}[t]
    \centering
    \scalebox{0.85}
    {\small
    \begin{tabular}{c|cc|cccc|ccccc|cc}

\multicolumn{12}{c}{~} & \multicolumn{2}{c}{top-1 accuracy} \\  
    \cmidrule(lr){13-14} 
    
    Ablation on 
    & \rotatebox{90}{Pre-training}
    & \rotatebox{90}{Fine-tuning}
    & \rotatebox{90}{Rand-Augment}
    & \rotatebox{90}{AutoAug}
    & \rotatebox{90}{Mixup}
    & \rotatebox{90}{CutMix}
    
    & \rotatebox{90}{Erasing}
    & \rotatebox{90}{Stoch. Depth}
    & \rotatebox{90}{Repeated Aug.} 
    & \rotatebox{90}{Dropout} 
    & \rotatebox{90}{Exp. Moving Avg.} 
    
    & \rotatebox{90}{pre-trained }
    & \rotatebox{90}{fine-tuned }  \\
    
    \toprule
    none: \oursbase  & adamw & adamw & \cmark & \xmark &  \cmark & \cmark & \cmark & \cmark & \cmark & \xmark & \xmark & 81.8\stdminus{0.2} & 83.1\stdminus{0.1} \\
    \midrule

        
    \multirow{2}{*}{\rotatebox{0}{optimizer}}
                      & SGD & \adamwg & \cmarkg & \xmarkg &  \cmarkg & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \xmarkg & \xmarkg & 74.5   &  77.3\\ 
                      & \adamwg & SGD & \cmarkg & \xmarkg &  \cmarkg & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \xmarkg & \xmarkg & 81.8  & 83.1 \\ 
    \midrule
    
    \multirow{6}{*}{\rotatebox{0}{\makebox{\begin{minipage}{2cm}\centering data\\ augmentation \\ ~\end{minipage}}}}
                      & \adamwg & \adamwg & \xmark  & \xmarkg & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \xmarkg  & \xmarkg & 79.6  & 80.4 \\ 
                      & \adamwg & \adamwg & \xmark  & \cmark  & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \xmarkg  & \xmarkg & 81.2  & 81.9 \\ 
                      & \adamwg & \adamwg & \cmarkg & \xmarkg & \xmark  & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \xmarkg  & \xmarkg & 78.7  & 79.8 \\ 
                      & \adamwg & \adamwg & \cmarkg & \xmarkg & \cmarkg & \xmark  & \cmarkg & \cmarkg & \cmarkg & \xmarkg  & \xmarkg & 80.0  & 80.6 \\ 
                      & \adamwg & \adamwg & \cmarkg & \xmarkg & \xmark  & \xmark  & \cmarkg & \cmarkg & \cmarkg & \xmarkg  & \xmarkg & 75.8  & 76.7 \\ 
    \midrule
    \multirow{5}{*}{\rotatebox{0}{regularization}}
                      & \adamwg & \adamwg & \cmarkg & \xmarkg & \cmarkg & \cmarkg & \xmark & \cmarkg & \cmarkg & \xmarkg & \xmarkg & \pzo4.3*  &  \pzo0.1\\ 
                      & \adamwg & \adamwg & \cmarkg & \xmarkg & \cmarkg & \cmarkg & \cmarkg & \xmark & \cmarkg &  \xmarkg & \xmarkg & \pzo3.4*  & \pzo0.1 \\ 
                      & \adamwg & \adamwg & \cmarkg & \xmarkg & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \xmark &  \xmarkg & \xmarkg & 76.5   & 77.4 \\ 
                      & \adamwg & \adamwg & \cmarkg & \xmarkg & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \cmark & \xmarkg & 81.3  & 83.1\\ 
                      & \adamwg & \adamwg & \cmarkg & \xmarkg & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \xmarkg & \cmark & 81.9 & 83.1\\     \bottomrule
    \end{tabular}}
    \caption{Ablation study on training methods on ImageNet~\cite{Russakovsky2015ImageNet12}. The top row ("none") corresponds to our default configuration employed for \ours. The symbols \cmark and \xmark indicates that we use and do not use the corresponding method, respectively.  
    We report the accuracy scores (\%) after the initial training at resolution 224224, and after fine-tuning at resolution 384384. 
    The hyper-parameters are fixed according to Table~\ref{tab:comp_hyperparameters}, and may be suboptimal.\newline 
    {\footnotesize * indicates that the model did not train well, possibly because hyper-parameters are not adapted.} 
    }
    \label{tab:ablation}
\end{table*}

In this section we discuss the \ours training strategy to learn vision transformers in a data-efficient manner. 
We build upon PyTorch~\cite{paszke2019pytorch} and the timm library~\cite{pytorchmodels}\footnote{The timm implementation already included a training procedure that improved the accuracy of ViT-B from 77.91\% to 79.35\% top-1, and trained  on Imagenet-1k with a 8xV100 GPU machine.}.
We provide hyper-parameters as well as an ablation study in which we analyze the impact of each choice. 

\paragraph{Initialization and hyper-parameters.}

Transformers are relatively sensitive to initialization. After testing several options in preliminary experiments, some of them not converging, we follow the recommendation of Hanin and Rolnick~\cite{hanin2018start} to initialize the weights with a truncated normal distribution. 

Table~\ref{tab:comp_hyperparameters} indicates the hyper-parameters that we use by default at training time for all our experiments, unless stated otherwise. 
For distillation we follow the recommendations from Cho et al.~\cite{Cho2019OnTE} to select the parameters  and . 
We take the typical values  and  for the usual (soft) distillation.




\begin{table}[t]
\centering
\scalebox{0.9}
{
\begin{tabular}{lccc}
\toprule
Methods & ViT-B~\cite{dosovitskiy2020image}  & DeiT-B \\
\midrule
Epochs   & 300 & 300     \\
\midrule
Batch size & 4096 & 1024\\
Optimizer & AdamW & AdamW\\
     learning rate       & 0.003 &     \\
     Learning rate decay & cosine & cosine  \\
     Weight decay        & 0.3    & 0.05    \\
     Warmup epochs  & 3.4 & 5       \\
\midrule
     Label smoothing  & \xmark & 0.1     \\
     Dropout      & 0.1 & \xmark     \\
     Stoch. Depth & \xmark & 0.1 \\
     Repeated Aug & \xmark & \cmark \\
     Gradient Clip. & \cmark & \xmark \\
\midrule
     Rand Augment  & \xmark        & 9/0.5 \\
     Mixup prob.  & \xmark & 0.8     \\
     Cutmix prob.   & \xmark & 1.0    \\
     Erasing prob.    & \xmark & 0.25    \\
 \bottomrule
\end{tabular}}
\vspace{-8pt}
\caption{
 Ingredients and hyper-parameters for our method and Vit-B.
\label{tab:comp_hyperparameters}}
\end{table}



\paragraph{Data-Augmentation.}


Compared to models that integrate more priors (such as convolutions), transformers require a larger amount of data.
Thus, in order to train with datasets of the same size, we rely on extensive data augmentation.
We evaluate different types of strong data augmentation, with the objective to reach a data-efficient training regime.  

Auto-Augment~\cite{Ekin2018AutoAugment}, Rand-Augment
~\cite{Cubuk2019RandAugmentPA}, and random erasing~\cite{Zhong2020RandomED} improve the results. For the two latter we use the timm~\cite{pytorchmodels} customizations, and after ablation we choose Rand-Augment instead of AutoAugment. 
Overall our experiments confirm that transformers require a strong data augmentation: almost all the data-augmentation methods that we evaluate prove to be useful. One exception is dropout, which we exclude from our training procedure. 


\paragraph{Regularization \& Optimizers.}


We have considered different optimizers and cross-validated different learning rates and weight decays. 
Transformers are sensitive to the setting of optimization hyper-parameters. Therefore, during cross-validation, we tried 3 different learning rates () and 3 weight decay (, , ).
We scale the learning rate according to the batch size with the formula:
, similarly to Goyal et al.~\cite{Goyal2017AccurateLM} except that we use 512 instead of 256 as the base value.

The best results use the AdamW optimizer with the same learning rates as ViT~\cite{dosovitskiy2020image} but with a much smaller weight decay, as the weight decay reported in the paper hurts the convergence in our setting.

We have employed stochastic depth~\cite{Huang2016DeepNW}, which facilitates the convergence of transformers, especially deep ones~\cite{fan2019reducing,fan2020training}. For vision transformers, they were first adopted in the training procedure by Wightman~\cite{pytorchmodels}. 
Regularization like Mixup~\cite{Zhang2017Mixup} and Cutmix~\cite{Yun2019CutMix} improve performance. We also use repeated augmentation~\cite{berman2019multigrain,hoffer2020augment}, which provides a significant boost in performance and is one of the key ingredients of our proposed training procedure. 

\paragraph{Exponential Moving Average (EMA).} 
We evaluate the EMA of our network obtained after training. 
There are small gains, which vanish after fine-tuning: the EMA model has an edge of is 0.1 accuracy points, but when fine-tuned the two models reach the same (improved) performance.

\paragraph{Fine-tuning at different resolution.}  


We adopt the fine-tuning procedure from Touvron et al.~\cite{Touvron2020FixingTT}: 
our schedule, regularization and optimization procedure are identical to that of FixEfficientNet but we keep the training-time data augmentation (contrary to the dampened data augmentation of Touvron et al.~\cite{Touvron2020FixingTT}).
We also interpolate the positional embeddings:  
In principle any classical image scaling technique, like bilinear interpolation, could be used. 
However, a bilinear interpolation of a vector from its neighbors reduces its -norm compared to its neighbors.
These low-norm vectors are not adapted to the pre-trained transformers and we observe a significant drop in accuracy if we employ use directly without any form of fine-tuning.
Therefore we adopt a bicubic interpolation that approximately preserves the norm of the vectors, before fine-tuning the network with either AdamW~\cite{Loshchilov2017AdamW} or SGD. These optimizers have a similar performance for the fine-tuning stage, see Table~\ref{tab:ablation}. 



By default and similar to ViT~\cite{dosovitskiy2020image} we train \ours models with at resolution  and we fine-tune at resolution .
We detail how to do this interpolation in Section~\ref{sec:position_encoding}. 
However, in order to measure the influence of the resolution we have finetuned \ours at different resolutions. 
We report these results in Table~\ref{tab:res_finetune}.

 \begin{table}[t]
    \centering
    \scalebox{0.955}
    {\small 
    \begin{tabular}{c@{\ }c|c@{\ }|c@{\ }|c@{\ }}
    \toprule
             image       & throughput & \multicolumn{1}{c}{Imagenet~\cite{Russakovsky2015ImageNet12}} & \multicolumn{1}{|c|}{Real~\cite{Beyer2020ImageNetReal}}& \multicolumn{1}{c}{V2~\cite{Recht2019ImageNetv2}}\\ 
     size & (image/s) &  acc. top-1 &  acc. top-1 &  acc. top-1\\
    \toprule

      & 609.31   & 79.9 &   84.8   & 67.6 \\
      & 291.05    & 81.8 & 86.7 & 71.5 \\
      & 134.13 & 82.7 &  87.2 & 71.9 \\
      & \pzo85.87 & 83.1 & 87.7 & 72.4 \\

    \bottomrule
    \end{tabular}}
    \caption{Performance of \ours trained at size  for varying finetuning sizes on ImageNet-1k, ImageNet-Real and ImageNet-v2 matched frequency. 
    \label{tab:res_finetune}}
\end{table}



\paragraph{Training time. }

A typical training of 300 epochs takes 37 hours with 2 nodes or 53 hours on a single node for the \ours-B.As a comparison point, a similar training with a RegNetY-16GF~\cite{Radosavovic2020RegNet} (84M parameters) is 20\% slower.
\ours-S and \ourstiny are trained in less than 3 days on 4 GPU.
Then, optionally we fine-tune the model at a larger resolution. This takes 20 hours on a single node (8 GPU) to produce a \oursfix model at resolution 384384, which corresponds to 25 epochs.
Not having to rely on batch-norm allows one to reduce the batch size without impacting performance, which makes it easier to train larger models.
Note that, since we use repeated augmentation~\cite{berman2019multigrain,hoffer2020augment} with 3 repetitions, we only see one third of the images during a single epoch\footnote{Formally it means that we have 100 epochs, but each is 3x longer because of the repeated augmentations. We prefer to refer to this as 300 epochs in order to have a direct comparison on the effective training time with and without repeated augmentation.}.




 \section{Conclusion}
\label{sec:conclusion}

In this paper, we have introduced \ours, which are image transformers that do not require very large amount of data to be trained, thanks to improved training and in particular a novel distillation procedure. 
Convolutional neural networks have optimized, both in terms of architecture  and  optimization during almost a decade, including through extensive architecture search that is prone to overfiting, as it is the case for instance for EfficientNets~\cite{Touvron2020FixingTT}. For \ours we have started the existing data augmentation and regularization strategies pre-existing for convnets, not introducing any significant architectural beyond our novel distillation token. Therefore it is likely that research on data-augmentation more adapted or learned for transformers will bring further gains.  
 
Therefore, considering our results, where image transformers are on par with convnets already, we believe that they will rapidly become a method of choice considering their lower memory footprint for a given accuracy. 



We provide an open-source implementation of our method. It is available at
\url{https://github.com/facebookresearch/deit}.
\subsection*{Acknowledgements}

Many thanks to Ross Wightman for sharing his ViT code and bootstrapping training method with the community, as well as for valuable feedback that helped us to fix different aspects of this paper. Thanks to Vinicius Reis, Mannat Singh, Ari Morcos, Mark Tygert, Gabriel Synnaeve, and  other colleagues at Facebook for brainstorming and some exploration on this axis. Thanks to Ross Girshick and Piotr Dollar for constructive comments. 
 
\clearpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\end{document}
