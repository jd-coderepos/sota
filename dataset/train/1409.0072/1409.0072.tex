\include{Macros}
\documentclass[twocolumn,12pt]{autart}    

\theoremstyle{plain}
\usepackage[font=small, labelfont=bf]{caption}













\usepackage[normalem]{ulem}
\usepackage{mathdesign}
\usepackage{draftcopy}
\usepackage{color}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{ulem}
\usepackage{cite}
\usepackage{subfigure}
\usepackage{graphics}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{psfrag}
 \newtheorem{theorem}{Theorem}
 \newtheorem{lemma}{Lemma}
 \newtheorem{proposition}{Proposition}
  \newtheorem{corollary}{Corollary}
   \newtheorem{definition}{Definition}
 \newtheorem{remark}{Remark}
 \newtheorem{assumption}{Assumption}
 \newtheorem{example}{Example}
 \newtheorem{step}{Step}
  \newtheorem{steps}{Step}
  \newtheorem{Steps}{Step}
  \newtheorem{stp}{Step}
 \newtheorem{scenario}{Scenario}
 \newtheorem{problem}{Problem}
 \newcommand{\eref}[1]{Eq.~(\ref{#1})}
\newcommand{\fref}[1]{Fig.~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\VECT}[1]{\boldsymbol{#1}}
\newcommand{\at}{\char64}
\renewcommand{\emph}{\textit}
\newcommand{\alert}[1]{\textcolor{magenta}{#1}}
\newcommand{\yy}[1]{\alert{\bf{#1}}}
\newcommand{\todo}[1]{\vspace{5 mm}\par \noindent
\framebox{\begin{minipage}[c]{8cm} \tt #1
\end{minipage}}\vspace{5 mm}\par}
\newenvironment{proof}[1][Proof]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\begin{document}
\begin{frontmatter}
\title{On minimal realisations of dynamical structure functions}\thanks{This paper has not been published in any conference, some preliminary results in Section~\ref{sec:special} have been published in \cite{yetac}. Ye Yuan and Jorge Gon\c{c}alves acknowledge the support from EPSRC through EP/I03210X/1, EP/G066477/1.
}
\author[1]{Ye Yuan}, \author[1]{Keith Glover} and \author[1,2]{Jorge Gon\c{c}alves}
\maketitle
\address[1]{Control Group, Department of Engineering, University of Cambridge, UK.} 
\address[2]{Luxembourg Centre for Systems Biomedicine, Luxembourg.}      

\begin{abstract}
Motivated by the fact that transfer functions do not contain structural information about networks, dynamical structure functions were introduced to capture causal relationships between measured nodes in networks. From the dynamical structure functions, a) we show that the actual number of hidden states can be larger than the number of hidden states estimated from the corresponding transfer function; b) we can obtain partial information about the true state-space equation, which cannot in general be obtained from the transfer function. Based on these properties, this paper proposes algorithms to find minimal realisations for a given dynamical structure function. This helps to estimate the minimal number of hidden states, to better understand the complexity of the network, and to identify potential targets for new measurements.
\end{abstract}
\begin{keyword}
 Network reconstruction, Linear system theory.
 \end{keyword}
\end{frontmatter}

\section{INTRODUCTION}
Networks have received increasing attention in the last
decade. In our ``information-rich'' world, questions pertaining to network
reconstruction and network analysis have become crucial for the
understanding of complex systems. In particular, the analysis of molecular
networks has gained significant interest due to the recent explosion
of publicly available high-throughput biological data. Another example are social networks, which are social structures made up of individuals, the nodes, tied by one or more specific types of interdependencies, the edges (e.g. friendship).  
In this context, identifying and analysing network structures from measured data become key questions. 

To mathematically represent networks, we use the standard graph-theoretical notation , where  is the set of nodes,  is the set of edges, and  is the corresponding  by  weighted adjacency matrix, with  when there is a link from  to , and  when there is no link from  to . 
In the classic state-space form, we usually write

 is the state vector containing the state (normally physical quantity) of the system.  is the weighted adjacency matrix reflecting the direct causal relations between the state variables, , and  is a vector of  inputs.  

This work assumes that  states are measured. Without loss of generality, the output equation can be written as , where ,  is the  identity matrix, and  is the  matrix of zeros.  Hence, the first  elements of the state vector  are exactly the measured variables in the system, and the remaining  state variables are unmeasured ``hidden" states. The zero structure of the  and  matrices exactly describe the structure of the network, and the values of these matrices encode the dynamics of the system.


Finding the matrices  and , however, can be a difficult problem in the presence of hidden states (). Even with just one hidden state, the realisation problem becomes ill-posed; a transfer function will have many state space realisations, which may suggest entirely different network structures for the system.  This is true even if it is known that the true system is, in fact, a minimal realisation of the identified transfer function.  As a result, failure to explicitly acknowledge the presence and the ambiguity in network structure caused hidden states can lead to a deceptive and erroneous process for network discovery.



Motived by this, we developed a new theory for network inference that reflected the effects of hidden states in a network~\cite{08net_rec}. It introduced a new representation for LTI systems called dynamical structure functions (DSF). DSF capture information at an intermediate level between transfer function and state space
representation (see Figure~\ref{Fig:math}). Specifically, dynamical structure functions not only encode structural information at the measurement level, but also contain some
information about hidden states. In \cite{08net_rec}, we proposed some guidelines for the design of an experimental data-acquisition protocol which allows the collection of data containing sufficient information for the network structure reconstruction problem
to become solvable. Using dynamical structure functions as a means to solve the network reconstruction problem, the following aspects need to be considered:
\begin{figure}
\centering
\includegraphics[scale=0.3]{Problem_structure.pdf}
\caption{Mathematical structure of the network reconstruction problem using dynamical structure functions. Red arrows mean ``uniquely determine'', blue arrows indicate our work.}\label{Fig:math}
\end{figure}

First (see (A) in Figure~\ref{Fig:math}), the properties of a dynamical
structure function and its relationship with the transfer
function associated with the same system were precisely established~\cite{08net_rec}.


Second (see (B) in Figure~\ref{Fig:math}), an efficient method to reconstruct networks in the presence of noise and nonlinearities was developed~\cite{robust}.
In this method, steady-state (resp. time-series data) can be used to reconstruct the Boolean (resp. dynamical network) structure of the system (see~\cite{robust} for more details). 

Third (see (C) in Figure~\ref{Fig:math}), once the dynamical structure function is obtained, an algorithm for constructing a minimal order state-space realisation of such function needs to be developed. This third point is the main contribution of this paper. In an application, this provides an estimate of the complexity of the system by determining the minimal number of hidden states in the system. For example, in the context of biology it helps understand the number of unmeasured molecules in a particular pathway: a low number of hidden states means that most molecules in that pathway have been identified and measured, showing a good understanding of the system; while a large number shows that there are still many unmeasured variables, suggesting that new experiments should be carried out to better characterise that pathway.

For a given dynamical structure function,the major contributions of this paper are: 
\begin{itemize} 
\item[a)]  it explicitly characterises the {\em direct} causal information between measured states and between measured states and inputs;
\item[b)] it introduces a number of new concepts such as hidden observability and controllability; 
\item[c)] it extends the results in \cite{yetac} by considering the minimal realisation problem of more general classes of dynamical structure functions. 
\end{itemize} 

The notation in this paper is standard. For a matrix ,  denotes the element in the  row and 
column,  denotes its 
row,  denotes its 
column, and  
denotes the submatrix of  defined by the rows  to  and the columns  to .
For a column vector , 
denotes its  element. 
We denote . Furthermore, 
 denotes the identity matrix of size .





\section{DYNAMICAL STRUCTURE FUNCTIONS AND ITS PROPERTIES}\label{sec:systemmodel}
Consider the following linear system (we put a superscript  indicating the original system)

where  is the full
state vector,  is a partial measurement of the
state,  are the  ``hidden'' states, and
 is the control input. In this work we restrict our attention to situations where output measurements constitute partial state information, i.e., . It is well known that the transfer function of this system can be defined by . 

\subsection{Definitions of transfer functions and dynamical structure functions}
Dynamical structure functions can be uniquely determined by state-space realisations. It is more involved comparing with the definition of a transfer function \cite{08net_rec}.

Taking the Laplace transforms of the signals in~(\ref{eq:LTI}) yields

where , , and  are the Laplace
transforms of , , and , respectively.
Solving for  gives

Substituting this last expression of  into~(\ref{eq:LTIlaplace}) then yields

where  and
.

Now, let  be a
diagonal matrix formed of the diagonal terms of  on its
diagonal, i.e., . Subtracting  from both sides of \eqref{eq:WV}, we obtain:

Note that  is a matrix with zeros on its
diagonal. We thus have:

where

and

Note that  has zero on the diagonal. Given the system in~(\ref{eq:LTI}),  denotes the {\it dynamical structure functions} of the system.



\subsection{Final value properties}
Next we shall explore some properties of . One of the most important properties is that the dynamical structure functions capture the {\em direct} causal relations between measured states .
\begin{proposition}\label{th:qp}
Given a dynamical system~(\ref{eq:LTI}) and its associated dynamical structure
  functions  with  constructed as
  explained above (see \eqref{eq:LTI}-\eqref{eq:P}), the following conditions must hold

\end{proposition}
\begin{proof}
See Appendix~\ref{sec:appendixA}.
\end{proof}

\begin{remark}
This Proposition reveals an important property of dynamical structure functions: they encode the direct causal relations between observed variables, i.e., . These relations cannot be revealed by transfer functions. 
\end{remark}


\begin{example} \label{ex1}
Consider a network with the structure depicted in Fig. \ref{fig:ex1}. The linear state-space representation of this network is given by

Following the definitions in \eqref{eq:Q} and \eqref{eq:P}, the corresponding dynamical structure functions  are

\begin{figure}[!] \centering
\includegraphics[width=1\linewidth]{Ex1}
\caption{\textbf{(a)} An example system with two inputs, three measured states (red states , , and ) and two hidden states (blue states  and ). \textbf{(b)} The corresponding dynamical structure functions.}
\label{fig:ex1}
\end{figure}
From Proposition~\ref{th:qp}, we can check that:

\end{example}




\subsection{Realisation problem}

In general,  and  carry more information than . This can be seen from the equality .  However,  and  carry less information than the state-space model \eqref{eq:LTI} (see~\cite{08net_rec,robust} and Figure~\ref{fig:problems}). This leads to the problem of realisation of , similar to the problem of realisation of . Basically, just like the fact that there are infinite state-space realisations that give the same transfer function (realisation problem (1) and set red in Figure~\ref{fig:problems}), there are an infinite state-space realisations that give the same  (realisation problem (2) and set magenta in Figure~\ref{fig:problems}).

\begin{figure}[h]
\centering
  \includegraphics*[width=.48\textwidth]{ideaonrealisation.pdf}
\caption{Relations among transfer functions, dynamical structure functions and state-space realisations.} \label{fig:problems}     
\end{figure}


\begin{definition}A system  is a realisation of  if that  gives  from eq.~\eqref{eq:Q} and eq.~\eqref{eq:P}.
\end{definition}






\begin{definition}
We say that a realisation  of  is -minimal if this realisation corresponds to a minimal realisation of . We say that a realisation  of  is -minimal if this realisation of  has the smallest order. 
\end{definition}



\subsection{Observability and controllability properties}
Let a system  have the following form

be a realisation of . In this subsection, we shall introduce properties of minimal realisations of  and all the proofs in this subsection can be found in Appendix~\ref{sec:appendixA}.
.




\begin{proposition}\label{lemma:tra}
Let  be a realisation of  (eq.~\eqref{eq:sigma}) and consider a linear transformation mapping  to ,   is also a realisation of  for any  with the following form 
for any invertible matrix .
\end{proposition} 

\begin{remark}
According to the above proposition, one can apply linear transformations to the hidden states without changing the dynamical structure function. \end{remark}
Similar to minimal realisation of transfer functions, based on Proposition~\ref{lemma:tra} we can define the following hidden observability and controllability concepts. 

\begin{definition}[Hidden Observability]
Given a realisation   of , we say it is hidden observable if and only if  is observable.
\end{definition}

\begin{definition}[Hidden Controllability]
Given a realisation  of , we say it is hidden controllable if and only if  is controllable.  
\end{definition}

From these two definitions, we can show that if a realisation  is -minimal then it is both hidden observable and controllable.

\begin{remark}
Linear transformations of the form  in eq.~\eqref{eq:t} do not change the hidden observability and hidden controllability of a system.
\end{remark}

\begin{proposition}\label{th:hidden}
If a realisation  of  is minimal, then it is hidden observable and hidden controllable.
\end{proposition}
\begin{proof}
It is easy to show by contradiction. 
\end{proof}

\begin{remark}
Note that a realisation  of  can be hidden observable and hidden controllable and not necessarily  minimal. 
\end{remark}

\begin{proposition}\label{th:obserable} 
If a system  is hidden observable, then it is observable.
\end{proposition}

Based on the above proposition, we can show the following Corollary.
\begin{corollary}
Given a minimal realisation  of , then the order of this realisation is equal to the order of  if and only if  is controllable.
\end{corollary}








\section{PROBLEM FORMULATION}\label{se:pf}
From here on, the paper assumes that the dynamical structure functions  and the transfer function  are known, and  the original state-space realisation~\eqref{eq:LTI} is unknown. We then proceed to search for a minimal realisation of . Just like the minimal realisation of a transfer function, the underlying principle to find a -minimal realisation is to search for a realisation with the minimal number of hidden states.  The rest of the paper aims to solve the following problem.

\begin{problem}\label{prob:main}
{\em [}{\bf Minimal  realisation}{\em ]} Given a dynamical structure function , find a minimal realisation  of . 
\end{problem}

\begin{remark}
From the above definitions, the order of a minimal structural realisation of  is always higher or equal to that of a minimal realisation of a transfer function . 
\end{remark}

Note that the original transfer matrices  cannot be reconstructed from the  dynamical structure function  since there is no information regarding the diagonal proper transfer function matrix . Hence, choosing an arbitrary diagonal proper transfer function matrix  leads to an arbitrary  from the following equation 
 
which is obtained from reversing the steps in equations~(\ref{eq:Q}) and~(\ref{eq:P}). Note that, in general,  will be different from .
A realisation of  is given by

where  and  are state-space matrices, structured similarly to equation~(\ref{eq:LTI}).
Again, this realisation is, in general, different from~(\ref{eq:LTI}), since it is not possible to recover~(\ref{eq:LTI}) from  alone.



\begin{remark}
Any realisation of  can be obtained from eq.~\eqref{eq:Rrealization} and eq.~\eqref{eq::WVrealization}.
\end{remark}

The idea for solving Problem~\ref{prob:main} is to use a state-space realisation approach to find an  that  minimises the order of . Such realisation is also a  minimal realisation. Mathematically, the problem can be reformulated according to finding such  

where deg is the McMillan degree \cite{zdg} and  is the set of all proper diagonal transfer matrices with dimension  (the number of measured states) that admits a diagonal realisation. This is equivalent to finding  from the following equation

This non-convex optimisation is, in general, hard to solve directly. 
Note that a random choice of a proper diagonal transfer function matrix  is likely to result in additional zeros in .
\begin{proposition}\label{prop:control}
If  has a zero, then for any realisation  obtained from eq.~\eqref{eq::WVrealization},  is not controllable.
\end{proposition}
\begin{proof}
See Appendix~\ref{sec:appendixA}.  
\end{proof}
\begin{remark}
Proposition~\ref{prop:control} shows that these additional zeros in  lead to unnecessary uncontrollable modes in the realisation  which means that 
 is not a minimal realisation of .
\end{remark}

\begin{remark}
There might be many choices for  that minimise
the order of minimal realisations of .
\end{remark}

\begin{remark}
After solving the problem in eq.~(\ref{eq:D}) and obtaining a minimal realisation of , we can use Proposition~\ref{lemma:tra} to find other minimal realisations of . 
\end{remark}
 



Next, we shall convert the optimisation in eq.~\eqref{eq:D} into a simpler form that explores the structure of the optimisation. To start,  
let 

and note that there is an one-to-one map between  and .

\begin{proposition}\label{prop:convert}
For any  with full normal row rank, the following equality holds

\end{proposition}
\begin{proof}
See Appendix~\ref{se:AppB}. 
\end{proof}

From the above proposition, the next section shall focus on solving







\section{MAIN ALGORITHM FOR OBTAINING A MINIMAL REALISATION OF }
\subsection{Analysis}\label{se:4.1}
This section proposes an algorithm to solve the optimisation in eq.~\eqref{eq:Np}. It follows that

Next, we shall derive conditions on  for cancelling zeros and poles of . 
\begin{assumption}\label{ass:1}
Assume that  only has simple poles and does not have the same poles and zeros. 
\end{assumption}


Since  are strictly proper, a minimal realisation of  has the following form: . When  has  simple poles, Gilbert's realisation \cite{gilbert} gives

where  and has
rank , since we are assuming that  has simple poles. Consider the following matrix decomposition for :

where  and
. Then , ,  and
.

Similarly,  is a diagonal transfer matrix with its minimal realisation . Without loss of generality, assume the matrix  is diagonal  (otherwise, a linear transform can diagonalise  without changing ). A minimal realisation of a diagonal transfer matrix can be obtained from a composition of Gilbert realisations of all transfer functions on the diagonal. Let  where  is the minimal realisation of the the  transfer function on the diagonal. Then a minimal realisation of  has the form

 (with , where  is the McMillan degree of ). In the following results, let  be the Boolean operator which maps a matrix/vector to a Boolean one.

\begin{theorem}\label{thm:cancelz}
Under Assumption~\ref{ass:1}, if a zero   of  (with direction ) is cancelled by cascading a system , then  has a pole at  for any  such that .
\end{theorem}
\begin{proof}
If a zero of , say , is cancelled by cascading a system , then the realisation of the cascaded system  loses controllability. 

In this case, it follows that there exists a nonzero vector  such that

This leads to 
\begin{itemize}
\item[1.] 
which indicates that  is an eigenvector of  corresponding to . 
\item[2.] 

\end{itemize}
Notice that, 

and, since  is not a pole of , it follows from eq.~\eqref{eq:temp} that

By definition,  is a zero of  if
there exists a  such that 

By comparing eqs.~(\ref{eq:reqs}) and~(\ref{eq:zero}) we conclude that  is the vector associated with the zero direction of . 
Then, it also follows that  

Since  in eq.~\eqref{eq:realisationofN} are diagonal matrices for all , then without loss of generality 

if  has an eigenvalue as . 
Since  also has a block diagonal structure,  we have 

This implies that the  nonzero elements in  corresponds to a nonzero element in  which further implies that  is a pole of , the  transfer function on the diagonal of . 
\end{proof}

\begin{theorem}\label{thm:cancelp}
Under Assumption~\ref{ass:1}, if a pole  of  is cancelled by cascading a system , then
where  is defined in eq.~\eqref{eq:E}.
\end{theorem}
\begin{proof}
If a pole of , say , is cancelled by , then the realisation of the
cascade  loses
observability. In this case, it follows that there exists a nonzero vector
 such that

The first equation in eq.~\eqref{eq:chobv} shows that  is an eigenvector of
 corresponding to . Since
 is diagonal, we can directly compute . Therefore we have

Noticing that  from eq.~\eqref{eq:E}, that

and that  is not a pole of , we obtain

\end{proof}

Based on Theorem~\ref{thm:cancelp}, we have the following Corollary.
\begin{corollary}\label{coro:cancelp}
If a pole  of  is cancelled by cascading a system , then  has a zero at  for any  such that .
\end{corollary}

\begin{remark}
In summary, designing  to cancel any pole
 of  is equivalent to imposing that
eq.~\eqref{eq:reqne} holds. 
\end{remark}

\begin{remark}
The Boolean structure of ,  imposes constraints on the diagonal terms in  for cancelling the poles of .
\end{remark}


\subsection{Algorithm to find }\label{se:4.2}
Following the derivations and analysis of the previous section, we shall propose an algorithm to directly answer the question in Problem~\ref{prob:main}: given , what is the maximal number of poles that can be cancelled by left multiplication of , bearing in mind that ? 

Recall from eq.~\eqref{eq:degree} that 

Since the  is known and fixed, let  then the optimisation problem in above equation becomes

Note that any  can be written as 

where  and  are coprime factors for all . Then 

where  for all . Next, we shall propose how to design  and  based on Theorems~\ref{thm:cancelz} and~\ref{thm:cancelp}.

To minimise the above cost function, we are aiming to have maximal number of zeros of  as poles of some diagonal elements in  from Theorem~\ref{thm:cancelz}, since this a) will not increase the McMillan degree of  and b) gives more degrees of freedom in zeros of  to cancel the poles of  (and therefore minimise the McMillan degree of the cascaded system), since the number of zeros in  equals the number of its poles. Moreover, to cancel one pole  of , Corollary~\ref{coro:cancelp} requires that  has a zero at  if . Assuming the sparsity of  is  (), this would then lead to an increase of the degree of  by  since  is a diagonal transfer matrix with every element in .

Based on the analysis above and Theorems~\ref{thm:cancelz} and~\ref{thm:cancelp}, to maximise the the number of poles in  that can be cancelled, one should a) design poles of  to cancel all the zeros of  and b) use the degrees of freedom in designing zeros of  to cancel as many poles of .

The next question is then how to solve the optimisation problem in b) once a) is done. Technically, we can use table~\ref{table:example}, which is generated as follows. Column  corresponds to the  place on the diagonal of the to-be-designed  and the rows are the poles of ,  , in any order.  The intersection of the  row and the  column is a Boolean value corresponding to , the Boolean map of the corresponding direction of the  pole. It is  if we require the  element of  to have a zero at  to cancel the  pole  of . Hence, table~\ref{table:example} shows the requirements to cancel each of the poles as expressed in eq.~\eqref{eq:reqne}. 
 
\begin{table}[htbp]
 \centering
 \begin{tabular}{|c|c|c|c|c|c|}
   \hline
    Poles & Place & Place &\ldots& Place &  Place  \\ \hline
    &  &  & \ldots &  &  \\ \hline
    &  &  & \ldots&  & 1 \\ \hline
    &  &   & \ldots &  &  1 \\ \hline
    &  &  && & \\ \hline
    & 0& 0&\ldots &0 &1 \\ \hline
    &0   & 0  &\ldots&1 &1  \\ \hline
 \end{tabular}
 \caption{Table for computing the maximum number of pole-zero cancellations.}
  \label{table:example}
\end{table}



We then maximise the largest number of rows such that, for any column, the summation of the elements on the selected rows is less or equal to a constant obtained from eq.~\eqref{eq:vector}. Choosing a row is equivalent to cancelling the corresponding pole in . So, the question is how to cancel the largest number of poles without introducing more poles in the cascaded systems of  and ?

Mathematically, let  be the maximum number of zeros allowed for the th diagonal element of  and let  be the binary element in the
th row and th column of Table~\ref{table:example}. Then, the original problem can be written as the following optimisation

where card is the cardinality of a set. 





Let  be binary numbers. We can reformulate eq.~\eqref{eq:k} to the following optimisation problem

where the inequality in the constraint is element-wise. This is in a standard form of binary integer programming. When the number of poles is small, the problem is easy to solve, as we can use the exhaustive attack method to go through all the possible cases and find the largest . In general, however, it is an integer optimisation problem and can be viewed as a -dimensional Knapsack problem and therefore NP-hard. We can use, for example, the standard Balas algorithm \cite{balas} to solve it. Once we have determined  , we can compute the corresponding zeros and poles of  and then solve for  .

The above analysis can be summarised with the following algorithm to find a minimal realisation of .
\begin{algorithm}[!]
\caption{Minimal  realisation}
\begin{step}
Compute the zeros  of  and the corresponding directions . Take the Boolean structure , and define the vector  ;
\end{step}
\begin{step}
Find a Gilbert realisation of  and find the conditions in eq.~\eqref{eq:reqne}
for cancelling the poles ;
\end{step}
\begin{step}
Build a table for the cancelling conditions from Step  and compute the maximum 
number of poles that can be cancelled from eq.~\eqref{eq:k};
\end{step}
\begin{step}
Determine  based on the table and obtain ;
\end{step}
\begin{step}
Compute .
\end{step}
\begin{step}
Find a minimal realisation of  and obtain corresponding  

\end{step}
\label{alg:iqpzero}
\end{algorithm}

\subsection{Special case:   does not have zeros}\label{sec:special}
A special case of Algorithm~\ref{alg:iqpzero} is that when  does not have any zeros and simple poles. In this case, we have the following proposition. 
\begin{proposition}\label{prop:d}\cite{yetac}
Assume  only has simple poles as in Assumption~\ref{ass:1} and does not have any zeros. A minimal realisation of 
can be obtained using a constant diagonal matrix  in eq.~\eqref{eq:Rrealization} and in eq.~\eqref{eq::WVrealization}.\end{proposition}

Basically, when  does not have zeros and simple poles,  is a constant matrix. Hence, there is a much simpler algorithm to obtain the maximum number of cancelling poles, rather than solving the linear integer programming. The problem reduces to the following

This problem still takes exponential-time to solve. There exist, however, a number of graph theoretical tools to solve it efficiently. As explained in \cite{GA}, an undirected graph is
denoted by 
where  is the set
of nodes and  is the set of edges. For our purposes, we construct an undirected graph  using the following rules:
\begin{itemize}
\item A node is associated with each vector in the set . There are thus  nodes in the considered graph. \item An undirected edge  is drawn between node  and node  if the equality  is satisfied.
\end{itemize}
Then, the maximum cardinality of  in eq.~\eqref{eq:knonzero} corresponds to the maximum number of nodes in a complete
subgraph  of the graph . Although the problem of finding the largest complete subgraphs in an undirected graph is an NP-hard problem, solutions have been proposed in \cite{link}. For an arbitrary graph, the fastest algorithm has a complexity of ~\cite{rob}. Hence, we can use these methods to obtain one of the largest complete subgraphs and consequently compute the corresponding set  with cardinality . 





\section{SIMULATION}
In this section, we will illustrate the above algorithms with examples.
\begin{example}
Consider  with the following form

Here is an illustration of Algorithm \ref{alg:iqpzero}.
\begin{Steps}
Compute the zeros and corresponding zero directions of . In this case, it only has one zero at  with a corresponding zero direction of . From eq.~\eqref{eq:vector} and definition of  we can see that .
\end{Steps}
\begin{Steps}
Obtain a Gilbert realisation of  

Based on the above analysis, we can draw Table~\ref{table:example1}. There, we see that to cancel pole , we need have 
a zero on the first diagonal element of , similarly for other poles.

\begin{table}[!]
 \centering
 \begin{tabular}{|c|c|c|c|c|c|}
   \hline
    Poles & Place & Place  & Place \\ \hline
    &  &  & \\ \hline
    &  &  &   \\ \hline
    &   &   &  \\ \hline
    & & & \\\hline
    &  &    &  \\ \hline
    &  &  &  \\\hline
  \end{tabular}
\caption{Table for computing maximum cancelled poles.}\label{table:example1}
\end{table}
\end{Steps}



\begin{Steps}
Solve the following optimisation problem

where ,  is the binary element in the  row and  column of Table~\ref{table:example1}. By solving the above optimisation, the optimal solution is .  Hence, the dimension of  is . 
\end{Steps}

\begin{Steps}
There are several optimal solutions. Choose, for example, the solution . Then 
 
where  are nonzero parameters.
\end{Steps}
\begin{Steps}
If , then 

which gives
 
with

\end{Steps}

\begin{Steps}
Find a minimal realisation of  and obtain the corresponding  matrices:

Hence, a minimal realisation has the following form:

\end{Steps}


Note that, as mentioned in Step 3, there are several solutions to . For example, chosing of the solution  would have lead to a different .
However, ultimately all optimal solutions have  matrices of the same dimension.

\end{example}
\section{CONCLUSION}
This paper presented an algorithm for obtaining a minimal order realisation of a given dynamical structure function. This provided a way to estimate the complexity of systems by determining the minimal number of hidden states in networks. This can help understand the minimal number of unknown states interacting in a particular network.
 
\begin{thebibliography}{1}








\bibitem{zdg}
K. Zhou, J. Doyle and K. Glover, \newblock {\it Robust and Optimal
Control.} \newblock  Prentice Hall, 1996.

\bibitem{08net_rec}
 J. Gon\c{c}alves and S. Warnick, ``Necessary and sufficient conditions for dynamical structure reconstruction of LTI networks'', IEEE Transactions on Automatic Control, 53(7): 1670-1674, 2008.

\bibitem{yetac}
Y. Yuan, G. Stan, S. Warnick and J. Gon\c{c}avles, ``Minimal dynamical structure realisations with application to network reconstruction from data,'' IEEE Conference on Decision and Control, 2009.

\bibitem{robust}
Y. Yuan, G. Stan, S. Warnick and J. Gon\c{c}avles, ``Robust dynamical network structure reconstruction,'' Special Issue on System Biology, Automatica, 47(6): 1230-1235, 2011.

\bibitem{MIT} S. Skogestad and I. Postlethwaite,\newblock{\it Multivariable Feedback Control-Analysis and Design.}\newblock Wiley,1996.

\bibitem{gilbert}
E. G. Gilbert, ``Controllability and observability in multivariable control systems,'' J.S.I.A.M. Control Series A, 1: 128-151, 1963.





\bibitem{balas}E. Balas and E. Zemel, ``An algorithm for large zero-one Knapsack problems,'' Operations Research, Vol. 28, No. 5, pp. 1130-1154, 1980. 


\bibitem{GA} C. Godsil and G. Royal, \newblock{ \it Algebraic Graph Theory.}\newblock New York: Springer-Verlag, 2001.
\bibitem{link}
I. Bomze, M. Budinich, P. Pardalos and M. Pelillo, ``The maximum clique problem'', Handbook of Combinatorial Optimization, 1999.

\bibitem{rob}
J. Robson, ``Finding a maximum independent set,'' Journal of Algorithms, 7:425-440, 1986.

\end{thebibliography}


\appendix
\section{Appendix: proof of Proposition~\ref{th:qp}, \ref{lemma:tra}, \ref{th:obserable} and~\ref{prop:control}}\label{sec:appendixA}
\begin{proof}\textbf{[Proposition~\ref{th:qp}]}
Eq.~\eqref{eq:Ds} is directly obtained from the definition of :

Since the proofs of eq.~\eqref{eq:Qs} and~\eqref{eq:Ps} are very
similar, we focus on eq.~\eqref{eq:Qs} only. In the following, we use the fact that
for any square matrix , if  when , then  
. From the definition of  in \eqref{eq:Q},
 and
, when .
Hence,
, in
which  is a matrix polynomial of , whose largest degree is
. Finally, multiplying by  on both sides and taking the limit
as  goes to  results in eq.~\eqref{eq:Qs}.  A similar
argument can be used to prove eq.~\eqref{eq:Ps}.
\end{proof}


\begin{proof}\textbf{[Proposition~\ref{lemma:tra}]}
Partition  and  according to the following form

From this partition, we have that 

We can then directly compute  for the transformed system and verify that such transformation will preserve  (the details of the rest of the proof are omitted).
\end{proof}



\begin{proof}
\textbf{[Proposition~\ref{th:obserable}]}
From the Popov-Belevitch-Hautus (PBH) rank test~\cite{zdg}, a matrix pair
   is observable iff
  
  for all . 
  
   If a realisation is hidden observable, then it implies that the
  pair  is observable from its definition, i.e.,
  
  Hence
  
  which concludes the proof. 
  \end{proof}

\begin{proof}
\textbf{[Proposition~\ref{prop:control}]}
If  is a zero of  with direction , by definition of zeros of a transfer function \cite{zdg},

Recall the definition of 

Let , then we have 

where eq. ~\eqref{eq:unob} is obtained from the definition.
We can rewrite

which means that  is not controllable.
\end{proof}


\section{Appendix: proof of Proposition~\ref{prop:convert}}\label{se:AppB}
The proof of Proposition~\ref{prop:convert} will be divided into several steps. Start by rewriting eq.~(\ref{eq:Rrealization}) as

For any , and corresponding , and any , define

Let  be the normal row rank of  and consider the Smith-McMillan of ,
where  are unimodular matrices in , and  

where  divides  and  divides  for any . Let  be the smallest integer that , which means that polynomial  exactly divides polynomial  (otherwise ).

\begin{lemma}\label{lemma1}
If follows that

\end{lemma}

\begin{proof}
Rewrite eq.~(\ref{eq:Rrealization}) as

Then, for any , and corresponding , and any ,

It follows that 

Rewrite the expression of  as

where  
Since 

and  for all 
then

Based on the above analysis, we can reformulate the optimisation on the left-hand side of eq.~\eqref{eq:B3} into the following form


\end{proof}

The above optimisation is hard to solve since both ,  and  depend on the choice of . However, we will show next that an  that minimises  is also a solution to eq.~\eqref{eq:N}. Such  results in  and . 

The remaining part of the proof will use notation and content from sections~\ref{se:4.1} and ~\ref{se:4.2}. Hence, the reader is expected to have read these sections before continuing.  First, we shall discuss why an optimal  guarantees  and then that it also guarantees . From eqs.~(\ref{eq:Nnew1}) and~(\ref{eq:Nnew2})





\begin{lemma}
With  defined in~(\ref{eq:Np}),  in eq.~(\ref{eq:jj}).
\end{lemma}
\begin{proof}
Let
From the design process, if  has a zero at , then it would be cancelled by designing .  is not designed have a zero at  unless it was used to cancel poles in  of . Then  in which  does not have a zero at .

Next, we show that  does not have a zero at . Otherwise, there would exist a  such that 

which would lead to  having a zero at  and a contradiction. Then  does not have any zero at , and therefore  from a similar analysis using Smith-McMillan form.

\end{proof}


\begin{lemma}
With  defined in~(\ref{eq:Np}),  in eq.~(\ref{eq:jj}).
\end{lemma}
\begin{proof}
From eq.~\eqref{eq:xystar}, to show that  has full normal rank, it is equivalent to show that 
 has a full normal rank. 

Since  does not have a zero at , then it has full rank. At , we have that

Hence, the normal rank of . Therefore, with  defined in~(\ref{eq:Np}),  in eq.~(\ref{eq:jj}).
\end{proof}


In summary,  obtained in Algorithm~\ref{alg:iqpzero} minimises not only eq.~\eqref{eq:Np} but also eq.~\eqref{eq:N}. This completes the proof of Proposition~\ref{prop:convert} since  ,  and  minimise eq.~\eqref{eq:N}.
\end{document}