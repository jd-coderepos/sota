\documentclass{article}

\PassOptionsToPackage{numbers}{natbib}


\usepackage{amsmath,amssymb,graphicx,color,algorithm,subfig, algpseudocode,booktabs,bm,relsize,enumitem,multirow,amsthm,epsfig,caption}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[final]{neurips_2021}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[dvipsnames]{xcolor}







\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{wrapfig}
\usepackage{bbm}
\usepackage{subfloat}

\usepackage{graphicx}
\newsavebox{\measurebox}

\hypersetup{colorlinks=true}
\hypersetup{citecolor=blue}
\hypersetup{linkcolor=blue}

\title{Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings}

\author{\qquad
  \textbf{Lili Chen}$^{1}$
  \qquad
  \textbf{Kimin Lee}$^{1}$
  \qquad
  \textbf{Aravind Srinivas}$^{2}$
  \qquad
  \textbf{Pieter Abbeel}$^{1}$
  \qquad
  \\
  $^{1}$UC Berkeley \vspace{.1em} \hspace{4pt}
  $^{2}$OpenAI \vspace{.1em} \hspace{4pt}
}

\begin{document}

\maketitle

\begin{abstract}
Recent advances in off-policy deep reinforcement learning (RL) have led to impressive success in complex tasks from visual observations. Experience replay improves sample-efficiency by reusing experiences from the past, and convolutional neural networks (CNNs) process high-dimensional inputs effectively. However, such techniques demand high memory and computational bandwidth. In this paper, we present Stored Embeddings for Efficient Reinforcement Learning (SEER), a simple modification of existing off-policy RL methods, to address these computational and memory requirements.
To reduce the computational overhead of gradient updates in CNNs, we freeze the lower layers of CNN encoders early in training due to early convergence of their parameters. Additionally, we reduce memory requirements by storing the low-dimensional latent vectors for experience replay instead of high-dimensional images, enabling an adaptive increase in the replay buffer capacity, a useful technique in constrained-memory settings. In our experiments, we show that SEER does not degrade the performance of RL agents while significantly saving computation and memory across a diverse set of DeepMind Control environments and Atari games. 

\end{abstract}

\section{Introduction}
Success stories of deep reinforcement learning (RL) from high dimensional inputs such as pixels or large spatial layouts include achieving superhuman performance on Atari games~\citep{mnih2015human, schrittwieser2019mastering,  badia2020agent57}, grandmaster level in Starcraft II~\citep{vinyals2019grandmaster} and grasping a diverse set of objects with impressive success rates and generalization with robots in the real world~\citep{kalashnikov2018qt}.
Modern off-policy RL algorithms~\citep{mnih2015human, hessel2018rainbow, hafner2018learning, hafner2019dream, srinivas2020curl, kostrikov2020image, laskin2020reinforcement} have improved the sample-efficiency of agents that process high-dimensional pixel inputs with convolutional neural networks (CNNs; \citealt{lecun1998gradient}) using past experiential data that is typically stored as raw observations in a replay buffer~\citep{lin1992self}.
However, these methods demand high memory and computational bandwidth, which makes deep RL inaccessible in several scenarios, such as learning with much lighter on-device computation (e.g. mobile phones or other light-weight edge devices).

For compute- and memory-efficient deep learning, 
several strategies, such as network pruning~\citep{han2015deep, frankle2018lottery}, quantization~\citep{han2015deep,iandola2016squeezenet} and freezing~\citep{yosinski2014transferable,46337} 
have been proposed in supervised learning and unsupervised learning for various purposes (see Section~\ref{sec:ref} for more details).
In computer vision, 
\citet{46337} and \citet{brock2017freezeout} showed that the computational cost of updating CNNs can be reduced by freezing lower layers earlier in training, and \citet{han2015deep} introduced a deep compression, which reduces the memory requirement of neural networks by producing a sparse network.
In natural language processing,
several approaches~\citep{tay2019lightweight,sun2020mobilebert} have studied improving the computational efficiency of Transformers~\citep{vaswani2017attention}.
In deep RL, however, developing compute- and memory-efficient techniques has received relatively little attention despite their serious impact on the practicality of RL algorithms.

In this paper, we propose {\bf S}tored {\bf E}mbeddings for {\bf E}fficient {\bf R}einforcement Learning (SEER),
a simple technique to reduce computational overhead and memory requirements that is compatible with various off-policy RL algorithms~\citep{haarnoja2018soft,hessel2018rainbow,srinivas2020curl}.
Our main idea is to freeze the lower layers of CNN encoders of RL agents early in training, which enables two key capabilities: 
(a) compute-efficiency: reducing the computational overhead of gradient updates in CNNs; 
(b) memory-efficiency: saving memory by storing the low-dimensional latent vectors to experience replay instead of high-dimensional images. Additionally, we leverage the memory-efficiency of SEER to adaptively increase replay capacity, resulting in improved sample-efficiency of off-policy RL algorithms in constrained-memory settings. SEER achieves these improvements without sacrificing performance due to early convergence of CNN encoders. 

The main contributions of this paper are as follows:
\begin{itemize} [leftmargin=5.5mm]
    \item We present SEER, a compute- and memory-efficient technique that can be used in conjunction with most modern off-policy RL algorithms~\citep{haarnoja2018soft,hessel2018rainbow}.
    \item We show that SEER significantly reduces computation while matching the original performance of existing RL algorithms on both continuous control tasks from DeepMind Control Suite~\citep{tassa2018deepmind} and discrete control tasks from Atari games~\citep{bellemare2013arcade}.
    \item We show that SEER improves the sample-efficiency of RL agents in constrained-memory settings by enabling an increased replay buffer capacity.
\end{itemize}

\section{Related work} \label{sec:ref}

{\bf Off-policy deep reinforcement learning.} The most sample-efficient RL agents often use off-policy RL algorithms, a recipe for improving the agent's policy from experiences that may have been recorded with a different policy~\citep{sutton2018reinforcement}. Off-policy RL algorithms are typically based on Q-Learning~\citep{watkins1992q} which estimates the optimal value functions for the task at hand, while actor-critic based off-policy methods~\citep{lillicrap2015continuous, schulman2017equivalence, haarnoja2018soft} are also commonly used. 
In this paper we will consider Deep Q-Networks (DQN;~\citealt{mnih2015human}),which combine the function approximation capability of deep convolutional neural networks (CNNs; \citealt{lecun1998gradient}) with Q-Learning along with the usage of the experience replay buffer~\citep{lin1992self} as well as off-policy actor-critic methods~\citep{lillicrap2015continuous,haarnoja2018soft}, which have been proposed for continuous control tasks.

Taking into account the learning ability of humans and practical limitations of wall clock time for deploying RL algorithms in the real world, particularly those that learn from raw high dimensional inputs such as pixels~\citep{kalashnikov2018qt}, the sample-inefficiency of off-policy RL algorithms has been a research topic of wide interest and importance~\citep{lake2017building, kaiser2019model}. To address this, several improvements in pixel-based off-policy RL have been proposed recently: algorithmic improvements such as Rainbow~\citep{hessel2018rainbow} and its data-efficient versions~\citep{van2019use}; using ensemble approaches based on bootstrapping~\citep{osband2016deep, lee2020sunrise}; combining RL algorithms with auxiliary predictive, reconstruction and contrastive losses~\citep{jaderberg2016reinforcement, higgins2017darla, oord2018representation, yarats2019improving, srinivas2020curl, stooke2020decoupling}; using world models for auxiliary losses and/or synthetic rollouts~\citep{sutton1991dyna, ha2018world, kaiser2019model, hafner2019dream}; using data-augmentations on images~\citep{laskin2020reinforcement, kostrikov2020image}.

{\bf Compute-efficient techniques in machine learning.} 
Most recent progress in deep learning and RL has relied heavily on the increased access to more powerful computational resources. To address this, \citet{mattson2019mlperf} presented MLPerf, a fair and precise ML benchmark to evaluate model training time on standard datasets, driving scalability alongside performance, following a recent focus on mitigating the computational cost of training ML models.
Several techniques, such as pruning and quantization \citep{han2015deep,frankle2018lottery,blalock2020state,iandola2016squeezenet,tay2019lightweight} have been developed to address compute and memory requirements. \citet{46337} and \citet{brock2017freezeout} proposed freezing earlier layers to remove computationally expensive backward passes in supervised learning tasks, motivated by the bottom-up convergence of neural networks. This intuition was further extended to recurrent neural networks \citep{morcos2018insights} and continual learning \citep{pellegrini2019latent}, and \citet{yosinski2014transferable} study the transferability of frozen and fine-tuned CNN parameters. \citet{fang2019scene} store low-dimensional embeddings of input observations in scene memory for long-horizon tasks. We focus on the feasibility of freezing neural network layers in deep RL and show that this idea can improve the compute- and memory-efficiency of many off-policy algorithms using standard RL benchmarks.

\begin{figure*} [t] \centering
\subfloat[SEER before freezing.]
{
\includegraphics[width=0.45\textwidth]{draft_figures/before.pdf} 
\label{fig:main_transfer_stand_to_walk}} 
\subfloat[SEER after freezing.]
{
\includegraphics[width=0.45\textwidth]{draft_figures/after.pdf} 
\label{fig:main_transfer_stand_to_cheetah}} 
\caption{Illustration of our framework.
(a) Before the encoder is frozen,
all forward and backward passes are active through the network, and we store images in the replay buffer.
(b) After freezing,
we store latent vectors in the replay buffer, 
and remove all forward and backward passes through the encoder. We remark that more samples can be stored in the replay buffer due to the relatively low dimensionality of the latent vector.} \label{fig:main_figure}
\end{figure*}

\section{Background} \label{sec:background}
We formulate visual control task as a partially observable Markov decision process (POMDP; \citealt{sutton2018reinforcement, kaelbling1998planning}).
Formally, at each timestep $t$, the agent receives a high-dimensional observation $o_t$, which is an indirect representation of the state $s_t$, and chooses an action $a_t$ based on its policy $\pi$.
The environment returns a reward $r_t$ and the agent transitions to the next observation $o_{t+1}$.
The return $R_t = \sum_{k=0}^\infty \gamma^k r_{t+k}$ is the total accumulated rewards from timestep $t$ with a discount factor $\gamma \in [0,1)$.
The goal of RL is to learn a policy $\pi$ that maximizes the expected return over trajectories.
By following the common practice in DQN~\citep{mnih2015human}, we handle the partial observability of environment using stacked input observations, which are processed through the convolutional layers of an encoder $f_{\psi}$.

{\bf Soft Actor-Critic}. SAC~\citep{haarnoja2018soft} is an off-policy actor-critic method based on the maximum entropy RL framework \citep{ziebart2010modeling}, which encourages robustness to noise and exploration by maximizing a weighted objective of the reward and the policy entropy.
To update the parameters, SAC alternates between a soft policy evaluation and a soft policy improvement.
At the soft policy evaluation step,
a soft Q-function, which is modeled as a neural network with parameters $\theta$, is updated by minimizing the following soft Bellman residual:
\begin{align*}
  \mathcal{L}^{\tt SAC}_Q (\theta,\psi) = &\mathbb{E}_{\tau_t \sim \mathcal{B}}\bigg[ \Big(Q_\theta(f_\psi(o_t),a_t) - r_t \\
  &- \gamma \mathbb{E}_{a_{t+1}\sim \pi_\phi} \big[ Q_{\bar \theta} (f_{\bar\psi}(o_{t+1}),a_{t+1})
  - \alpha \log \pi_{\phi} (a_{t+1}|f_\psi(o_{t+1})) \big]\Big)^2\bigg], 
\end{align*} \label{eq:sac_critic_tot} 
where $\tau_t = (o_t,a_t,r_t,o_{t+1})$ is a transition,
$\mathcal{B}$ is a replay buffer,
$\bar \theta, \bar \psi$ are the delayed parameters,
and $\alpha$ is a temperature parameter.
At the soft policy improvement step,
the policy $\pi$ with its parameter $\phi$ is updated by minimizing the following objective:
\begin{align*} 
  \mathcal{L}^{\tt SAC}_\pi(\phi) = \mathbb{E}_{o_t\sim \mathcal{B}, a_{t}\sim \pi_\phi} \big[  \alpha \log \pi_\phi (a_t|f_\psi(o_t))
  - Q_{ \theta} (f_\psi(o_{t}),a_{t}) \big].
\end{align*}
Here, the policy is modeled as a Gaussian with mean and covariance given by neural networks.

{\bf Deep Q-learning.} DQN algorithm~\citep{mnih2015human} learns a Q-function, which is modeled as a neural network with parameters $\theta$,
by minimizing the following Bellman residual:
\begin{align*}
  \mathcal{L}^{\tt DQN} (\theta, \psi) = \mathbb{E}_{\tau_t \sim \mathcal{B}} \Bigg[\Big(Q_\theta(f_\psi(o_t),a_t) - r_t 
  - \gamma \max_a Q_{\bar \theta} (f_{\bar \psi}(o_{t+1}),a) \Big)^2\Bigg], \label{eq:dqn}
\end{align*}
where $\tau_t = (o_t,a_t,r_t,o_{t+1})$ is a transition,
$\mathcal{B}$ is a replay buffer, 
and $\bar \theta, \bar \psi$ are the delayed parameters.
Rainbow DQN integrates several techniques,
such as double Q-learning~\citep{van2016deep} and distributional DQN~\citep{bellemare2017distributional}.
For exposition, we refer the reader to \citet{hessel2018rainbow} for more detailed explanations of Rainbow DQN.

\section{SEER: Stored Embeddings for Efficient Reinforcement Learning}

In this section, we present SEER: {\bf S}tored {\bf E}mbeddings for {\bf E}fficient {\bf R}einforcement Learning, which can be used in conjunction with most modern off-policy RL algorithms, such as SAC~\citep{haarnoja2018soft} and Rainbow DQN~\citep{hessel2018rainbow}.
Our main idea is to freeze lower layers during training and only update higher layers, which eliminates the computational overhead of computing gradients and updating in lower layers.
We additionally improve the memory-efficiency of off-policy RL algorithms by storing low-dimensional latent vectors in the replay buffer instead of high-dimensional pixel observations. See Figure \ref{fig:main_figure} and 
Appendix \ref{appendix:pseudocode} for more details of our method.

\subsection{Freezing encoder for saving computation and memory} \label{sec:freezing_encoder}

We process high-dimensional image input with an encoder $f_{\psi}$ to obtain $z_t = f_{\psi}(o_t)$, which is used as input for policy $\pi_{\phi}$ and Q-function $Q_\theta$ as described in Section \ref{sec:background}. In off-policy RL, 
we store transitions $(o_t,a_t,o_{t+1},r_t)$ in the replay buffer $\mathcal{B}$ to improve sample-efficiency by reusing experience from the past.
However, processing high-dimensional image input $o_t$ is computationally expensive. 
To handle this issue, after $T_f$ updates,
we freeze the parameters of encoder $\psi$, and only update the policy and Q-function.
We remark that this simple technique can save computation without performance degradation because the encoder is modeled as deep convolutional neural networks, while a shallow MLP is used for policy and Q-function. Freezing lower layers of neural networks also has been investigated in supervised learning based on the observation that neural networks converge to their final representations {\em from the bottom-up}, i.e., lower layers converge very early in training~\citep{46337}. For the first time, we show the feasibility and effectiveness of this idea in pixel-based reinforcement learning (see Figure~\ref{fig:transfer_dmc_walk} for supporting experimental results) and present solutions to its RL-specific implementation challenges. 


Moreover, in order to save memory, 
we consider storing (compressed) latent vectors instead of high-dimensional image inputs.
Specifically, 
each experience in $\mathcal{B}$ is replaced by the latent transition $(z_t,a_t,z_{t+1},r_t)$, and the replay capacity is increased to $\widehat{C}$ (see Section~\ref{sec:detail} for more details). 
Thereafter, for each subsequent environment interaction, 
the latent vectors $z_t=f_\psi(o_t)$ and $z_{t+1}=f_\psi(o_{t+1})$ are computed prior to storing $(z_t,a_t,z_{t+1},r_t)$ in $\mathcal{B}$. During agent updates, the sampled latent vectors are directly passed into the policy $\pi_{\phi}$ and Q-function $Q_\theta$, bypassing the encoder convolutional layers. Since the agent samples and trains with latent vectors after freezing, we only store the latent vectors and avoid the need to maintain large image observations in $\mathcal{B}$.

\subsection{Additional techniques and details for SEER} \label{sec:detail}

\textbf{Data augmentations.} Recently, various data augmentations~\citep{srinivas2020curl, laskin2020reinforcement,kostrikov2020image} have provided large gains in the sample-efficiency of RL from pixel observations.
However, SEER precludes data augmentations because we store the latent vector instead of the raw pixel observation.
We find that the absence of data augmentations could decrease sample-efficiency in some cases, e.g., when the capacity of $\mathcal{B}$ is small.
To mitigate this issue,
we perform $K$ number of different data augmentations for each input observation $o_t$ and store $K$ distinct latent vectors $\{ z^k_{t} = f_\psi(\text{AUG}_k (o_t)) | k=1\cdots K\}$.
We find empirically that $K=4$ achieves competitive performance to standard RL algorithms in most cases.

\textbf{Increasing replay capacity.} 
By storing the latent vector in the replay buffer,
we can adaptively increase the capacity (i.e., total number of transitions), which is determined by the size difference between the input pixel observations and the latent vectors output by the encoder, with a few additional considerations.
The new capacity of the replay buffer is
\begin{center}
    $\widehat{C}=  \Bigl\lfloor C*\left(\frac{P}{4NKL}\right)\Bigl\rfloor $,
\end{center}
where $C$ is the capacity of the original replay buffer, $P$ is the size of the raw observation, $L$ is the size of the latent vector, and $K$ is the number of data augmentations. 
The number of encoders $N$ is algorithm-specific and determines the number of distinct latent vectors encountered for each observation during training.
For Q-learning algorithms $N=1$, whereas for actor-critic algorithms $N=2$ if the actor and critic each compute their own latent vectors. Some algorithms employ a target network for updating the  Q-function~\citep{mnih2015human,haarnoja2018soft}, 
but we use the same latent vectors for the online and target networks after freezing to avoid storing target latent vectors separately and find that tying their parameters does not degrade performance.\footnote{We remark that the higher layers of the target network are not tied to the online network after freezing.}
The factor of 4 arises from the cost of saving floats for latent vectors, while raw pixel observations are saved as integer pixel values. We assume the memory required for actions and rewards is small and only consider only the memory used for observations. 

\begin{figure*} [t] \centering
\includegraphics[width=1\textwidth]{rebuttal/dm_flop_total.pdf}
\caption{Learning curves for CURL with and without SEER, where the x-axis shows estimated cumulative FLOPs. The dotted gray line denotes the encoder freezing time $t=T_f$. 
The solid line and shaded regions represent the mean and standard deviation, respectively, across five runs.}
\label{fig:main_dmc}
\vspace{-0.1in}
\end{figure*}

\begin{figure*} [t] \centering
\includegraphics[width=1\textwidth]{rebuttal/atari_flop_total.pdf}
\caption{Learning curves for Rainbow with and without SEER, where the x-axis shows estimated cumulative FLOPs. The dotted gray line denotes the encoder freezing time $t=T_f$. The solid line and shaded regions represent the mean and standard deviation, respectively, across five runs.}
\label{fig:main_atari}
\vspace{-0.1in}
\end{figure*}



\section{Experimental results}


We designed our experiments to answer the following questions: 
\begin{itemize} [leftmargin=8mm] \setlength\itemsep{0.1em}
  \item Can SEER reduce the computational overhead of various off-policy RL algorithms for both continuous (see Figure~\ref{fig:main_dmc}) and discrete (see Figure~\ref{fig:main_atari}) control tasks?
  \item Can SEER reduce the memory consumption and improve the sample-efficiency of off-policy RL algorithms by adaptively increasing the buffer size (see Figure~\ref{fig:memory_atari} and Figure~\ref{fig:memory_dmc})?
  \item Can SEER be useful for compute-efficient transfer learning (see Figure~\ref{fig:transfer_dmc_walk})?
  \item Do CNN encoders of RL agents converge early in training (see Figure~\ref{fig:attention_viz} and Figure~\ref{fig:svcca_viz})? 
\end{itemize}

\begin{table*}[]
\centering
\small
\begin{tabular}{l|ll|ll}
\toprule
 & \multicolumn{2}{c|}{Scores at 45T FLOPs} & \multicolumn{2}{c}{Scores  at 500K environment steps (0.07GB)} \\ \cline{2-5} 
 & Rainbow         & Rainbow+SEER         
 & Rainbow                 & Rainbow+SEER                  \\
 \midrule
Alien & $992.0 \pm 152.7$ & $\bf{1172.6}$ $\pm 239.0$ & $1038.4 \pm 101.1$ & $\bf{1134.6}$ $\pm 452.9$ \\
Amidar & $144.0 \pm 27.4$ & $\bf{250.5}$ $\pm 47.4$ &           $121.0 \pm 31.2$ & $\bf{165.3}$ $\pm 47.6$ \\
BankHeist & $145.8 \pm 61.2$ & $\bf{276.6}$ $\pm 98.1$ &            $\bf{161.6}$ $\pm 57.7$ & $151.8 \pm 65.8$ \\
CrazyClimber & $21580.0 \pm 3514.6$ & $\bf{28066.0}$ $\pm 4108.5$  & $10498.0 \pm 1387.8$ & $\bf{17620.0}$ $\pm 4418.4$  \\
Krull & $2799.5 \pm 468.1$ & $\bf{3277.5}$ $\pm 440.5$  &            $2215.7 \pm 336.9$ & $\bf{3069.2}$ $\pm 377.6$ \\
Qbert & $2325.5 \pm 1152.7$ & $\bf{4123.5}$ $\pm 1385.5$ &            $2430.5 \pm 658.8$ & $\bf{3231.0}$ $\pm 1567.6$ \\
RoadRunner & $10376.0 \pm 2886.0$ & $\bf{11794.0}$ $\pm 1745.3$ &       $10612.0 \pm 2059.3$ & $\bf{13064.0}$ $\pm 2489.2$ \\
Seaquest & $402.8 \pm 48.4$ & $\bf{561.2}$ $\pm 100.5$ &             $262.8 \pm 19.1$ & $\bf{336.8}$ $\pm 45.9$ \\
 \bottomrule
\end{tabular}
\caption{Scores on Atari games at 45T FLOPs corresponding to Figure \ref{fig:main_atari} and at 500K environment interactions in the constrained-memory setup (0.07GB) corresponding to Figure \ref{fig:memory_atari}. The results show the mean and standard deviation averaged five runs, and the best results are indicated in bold.} \label{tbl:main_atari}
\vspace{-0.1in}
\end{table*}

\begin{figure*} [ht] \centering
\includegraphics[width=1\textwidth]{rebuttal/atari_mem_total.pdf}
\caption{Comparison of the sample-efficiency of Rainbow with and without SEER in constrained-memory (0.07 GB) settings. The dotted gray line denotes the encoder freezing time $t=T_f$. The solid line and shaded regions represent the mean and standard deviation, respectively, across five runs.} \label{fig:memory_atari}
\vspace{-0.1in}
\end{figure*}

\begin{figure*} [t] \centering
\subfloat[Cartpole-swingup]
{
\includegraphics[width=0.24\textwidth]{rebuttal/mem_cart.pdf} 
\label{fig:memory_dmc_cart}} 
\subfloat[Finger-spin]
{
\includegraphics[width=0.24\textwidth]{rebuttal/mem_finger.pdf}
\label{fig:memory_dmc_finger}} 
\subfloat[Reacher-easy]
{
\includegraphics[width=0.24\textwidth]{rebuttal/mem_reac.pdf}
\label{fig:memory_dmc_reacher}} 
\\
\subfloat[Walker-walk]
{
\includegraphics[width=0.24\textwidth]{rebuttal/mem_walk.pdf}
\label{fig:memory_dmc_walk}} 
\subfloat[Cheetah-run]
{
\includegraphics[width=0.24\textwidth]{rebuttal/mem_cheetah.pdf}
\label{fig:memory_dmc_cheetah}}
\caption{Comparison of the sample-efficiency of CURL with and without SEER in constrained-memory settings. The dotted gray line denotes the encoder freezing time $t=T_f$. The solid line and shaded regions represent the mean and standard deviation, respectively, across five runs.} \label{fig:memory_dmc}
\vspace{-0.1in}
\end{figure*}

\subsection{Setups} \label{setups}

\textbf{Compute-efficiency.} 
We first demonstrate the compute-efficiency of SEER on the DeepMind Control Suite (DMControl; \citealt{tassa2018deepmind}) and Atari games~\citep{bellemare2013arcade} benchmarks.
DMControl is commonly used for benchmarking sample-efficiency for image-based continuous control methods.
For DMControl experiments,
we consider a state-of-the-art model-free RL method, which applies contrastive learning (CURL; ~\citealt{srinivas2020curl}) to SAC~\citep{haarnoja2018soft}, using the image encoder architecture from
SAC-AE~\citep{yarats2019improving}.
For evaluation,
we compare the computational efficiency of CURL with and without SEER by measuring floating point operations (FLOPs).\footnote{
We explain our procedure for counting the number of FLOPs in Appendix \ref{appendix:flop_counting}. 
The gain on wall-clock time is discussed in Appendix \ref{appendix:wall_clock_time}.}.
For discrete control tasks from Atari games,
we perform similar experiments comparing the FLOPs required by Rainbow~\citep{hessel2018rainbow} with and without SEER. For all experiments, we use the hyperparameters and architecture of data-efficient Rainbow \citep{van2019use}. 


\textbf{Memory efficiency.} We showcase the memory efficiency of SEER with a set of constrained-memory experiments in DMControl. For Cartpole and Finger, the memory allocated for storing observations is constrained to 0.03 GB, corresponding to an initial replay buffer capacity $C=1000$. For Reacher and Walker, the memory is constrained to 0.06 GB for an initial capacity of $C=2000$. In this constrained-memory setting, we compare the sample-efficiency of CURL with and without SEER.
As an upper bound, we also report the performance of CURL without memory constraints, i.e., the replay capacity is set to the number of training steps.
For Atari experiments,
the baseline agent is data-efficient Rainbow and the memory allocation is 0.07 GB, corresponding to initial replay capacity $C=10000$. 
The other hyperparameters are the same as those in the compute-efficiency experiments. 
Before the encoder is freeze, the replay buffer still needs to store the images and if the replay buffer slots number is equal with the baseline settings, the performance is equal to the baseline in theory. After the freeze time, the replay buffer slots number grows more larger. So the benefit is seems like on the condition of the assumption that a larger replay buffer would brings performance improvement? Such assumption needs to be claimed and discussed more clearly in the paper. Further discussions and experiments on the different limitations of the memory cost would be helpful.

The encoder architecture used for our experiments with CURL is used in \citet{yarats2019improving}. It consists of four convolutional layers
with 3 x 3 kernels and 32 channels, with the ReLU activation applied after each conv layer. The architecture used for our Rainbow experiments is from \citet{van2019use}, consisting of a convolutional layer with 32 channels followed by a convolutional layer with 64 channels, both with 5 x 5 kernels and followed by a ReLU activation. For SEER, we freeze the first fully-connected layer in CURL experiments and the last convolutional layer of the encoder in Rainbow experiments. We present the best results across various values of the encoder freezing time $T_f$. See Appendices \ref{appendix:dmc_implementation_details} and  \ref{appendix:atari_implementation_details} for more hyperparameters and Appendix \ref{appendix:source_code} for source code.

\subsection{Improving compute- and memory-efficiency} \label{main_exps}

Experimental results in DMControl and Atari showcasing the computational efficiency of SEER are provided in Figures \ref{fig:main_dmc} and Figure \ref{fig:main_atari}. 
CURL and Rainbow both achieve higher performance within significantly fewer FLOPs when combined with SEER in DMControl and Atari, respectively.
Additionally, Table \ref{tbl:main_atari} compares the performance of Rainbow with and without SEER at 45T (4.5e13) FLOPs. In particular, the average returns are improved from 145.8 to 276.6 compared to baseline Rainbow in BankHeist and from 2325.5 to 4123.5 in Qbert. We remark that SEER achieves better computational efficiency while maintaining the agent's final performance and comparable sample-efficiency (see Appendix \ref{appendix:additional_figures2} for corresponding figures).

Experimental results in Atari and DMControl showcasing the sample-efficiency of SEER in the constrained-memory setup are provided in Figure~\ref{fig:memory_atari} and Figure~\ref{fig:memory_dmc}. CURL and Rainbow achieve higher final performance and better sample-efficiency when combined with SEER in DMControl and Atari, respectively. 
Additionally, Table \ref{tbl:main_atari} compares the performance of unbounded memory Rainbow and constrained-memory (0.07 GB) Rainbow with and without SEER at 500K interactions. In particular, the average returns are improved from 10498.0 to 17620.0 compared to baseline Rainbow in CrazyClimber and from 2430.5 to 3231.0 in Qbert.
Although we disentangle the computational and memory benefits of SEER in these experiments, 
we also highlight the computational gain of SEER in constrained-memory settings (effectively combining the benefits) in Appendix \ref{appendix:additional_figures1}. For an ablation on the freezing time, see Appendix \ref{appendix:freezing_time_ablation}.
These experimental results show the real-world applicability of SEER (see Appendix \ref{appendix:memory_assumptions} for more details).


\begin{figure*} [t] \centering
\subfloat[Cartpole-swingup]
{
\includegraphics[width=0.24\textwidth]{rebuttal/impala_cartpole.png} 
\label{fig:impala_cartpole}} 
\subfloat[Walker-walk]
{
\includegraphics[width=0.24\textwidth]{rebuttal/impala_walker.png} 
\label{fig:impala_walker}} 
\caption{Learning curves using IMPALA architecture, where the x-axis shows estimated cumulative FLOPs. The dotted gray line denotes the encoder freezing time $t=T_f$. The solid line and shaded regions represent the mean and standard deviation, respectively, across three runs.}
\end{figure*} \label{fig:impala_plots}

\subsection{Freezing larger convolutional encoders} \label{impala}

We also verify the benefits of SEER using deeper convolutional encoders, which are widely used in a range of applications such as visual navigation tasks and favored for their superior generalization ability.
Specifically, 
we follow the setup described in Section \ref{setups} and replace the SAC-AE architecture (4 convolutional layers) with the IMPALA architecture \citep{espeholt2018impala} (15 convolutional layers containing residual blocks~\citep{he2016deep}).
Figure \ref{fig:impala_plots} shows the computational efficiency of SEER in Cartpole-swingup and Walker-walk with the IMPALA architecture. CURL achieves higher performance within significantly fewer FLOPs when combined with SEER.
We remark that the gains due to SEER are more significant because computing and updating gradients for large convolutional encoders is very computationally expensive.


\subsection{Improving compute-efficiency in transfer settings}

\begin{wrapfigure}{r}{0.48\textwidth} \centering
\vspace{-10mm}
\subfloat[To Walker-walk]
{
\includegraphics[width=0.22\textwidth]{rebuttal/rebuttal_walkerstand_to_walkerwalk.png} 
\label{fig:transfer_dmc_walk}} 
\subfloat[To Hopper-hop]
{
\includegraphics[width=0.22\textwidth]{rebuttal/rebuttal_walkerstand_to_hopperhop.png} 
\label{fig:transfer_dmc_hopper}} 
\caption{Comparison of the computational efficiency of agents trained from scratch with CURL and agents trained with CURL+SEER from Walker-stand pretraining. The solid line and shaded regions represent the mean and standard deviation, respectively, across three runs.}
\vspace{-0.2in}
\end{wrapfigure}

We demonstrate, as another application of our method, that SEER increases compute-efficiency in the transfer setting: utilizing the parameters from Task A on unseen Tasks B.
Specifically,
we train a CURL agent for 60K environment interactions on Walker-stand; 
then, we only fine-tune the policy and Q-functions on unseen tasks using network parameters from Walker-stand.
To save computation, during fine-tuning, we freeze the encoder parameters.
Figure \ref{fig:transfer_dmc_walk} shows the computational gain of SEER in task transfer (i.e., Walker-stand to Walker-walk similar to \citet{yarats2019improving}), and domain transfer (i.e., Walker-stand to Hopper-hop) is shown in Figure \ref{fig:transfer_dmc_hopper}. Due to the generality of CNN features,
we can achieve this computational gain using a pretrained encoder.
For the task transfer setup, we provide more analysis on the number of frozen layers and freezing time hyperparameter $T_f$ in Appendix \ref{appendix:additional_transfer_dmc}. 
While these transfer learning experiments are relatively independent to the compute-efficiency experiments in Section \ref{main_exps}, we believe this is an exciting additional application of SEER and that more comprehensive investigations in this direction would be interesting future work.

\subsection{Encoder analysis}

In this subsection we present visualizations to verify that the neural networks employed in deep reinforcement learning indeed converge {\em from the bottom up}, similar to those used in supervised learning \citep{46337}. 
Figure \ref{fig:attention_viz} shows the spatial attention map for two Atari games and one DMControl environment at various points during training. Similar to \citet{laskin2020reinforcement} and \citet{zagoruyko2016paying}, we compute the spatial attention map by mean-pooling the absolute values of
the activations along the channel dimension and follow with a 2-dimensional spatial softmax.
The attention map shows significant change in the first $20\%$ of training, and remains relatively unchanged thereafter, suggesting that the encoder converges to its final representations early in training. Figure \ref{fig:svcca_viz} shows the SVCCA \citep{46337} score, a measure of neural network layer similarity, between a layer and itself at time $t$ and $t + 10K$. The convolutional layers of the encoder achieve high similarity scores with themselves between time $t$ and $t + 10K$, while the higher layers of the policy and Q-network continue to change throughout training. In our DMControl environments we freeze the convolutional layers and the first fully-connected layer of the policy and Q-network (denoted fc1). Although the policy fc1 continues to change, the convergence of the Q-network fc1 and the encoder layers allow us to achieve our computational and memory savings with minimal performance degradation.

We remark that while the encoder can be frozen early in RL training, using a randomly initialized encoder is ineffective \citep{stooke2021decoupling}. It is important to train encoders on the task in order to learn useful features (as is done by widely used methods such as \citet{srinivas2020curl} and \citet{laskin2020reinforcement}), but our finding is that these encoders converge early in task-specific training.

\begin{figure} [ht] \centering
\subfloat[Spatial attention map]
{
\includegraphics[width=0.5\textwidth]{draft_figures/attention_figure_final.pdf} 
\label{fig:attention_viz}} 
\subfloat[SVCCA similarity scores]
{
\includegraphics[width=0.46\textwidth]{draft_figures/svcca_figure_final.pdf} 
\label{fig:svcca_viz}} 
\caption{Visualizations of encoder features throughout training. (a) Spatial attention map from CNN encoders. (b) SVCCA \citep{46337} similarity scores between each layer and itself at time $t$ and $t + 10K$ throughout training for Walker-walk task.} \label{fig:viz}
\end{figure}

\section{Discussion and Limitations} \label{discussion}
In this paper, we proposed a technique that reduces computation requirements for visual reinforcement learning, which we hope serves to facilitate a shift toward more compute-efficient RL. Here, we highlight other techniques for reducing training time. For experimentation in computationally intensive environments, \citet{obando2020revisiting} propose to use small- and medium-scale experiments, which could reproduce the conclusions of the Rainbow DQN paper in Atari games. For faster training time in a particular experiment, one can also lower the resolution of the input images. In Figures \ref{fig:reduce_res_cartpole} and \ref{fig:reduce_res_walker} we show that reducing the resolution by a factor of 2, from $100 \times 100$ to $50 \times 50$ (and scaling crops appropriately) produces significant compute-efficiency gain in DeepMind Control Suite without sacrificing performance, and emphasize that this technique can be combined with SEER for further improved efficiency. We remark that the additional gain from SEER is larger in more complex environments (e.g., Walker) where learning requires more steps. However, we find that naive resolution reduction may not generally be applicable across environments and may require domain knowledge in order to prevent excessive information loss. In Figures \ref{fig:reduce_res_alien} and \ref{fig:reduce_res_amidar} we show that resolution reduction by a factor of 2, from $84 \times 84$ to $42 \times 42$, results in noticeably worse performance in several Atari games. In contrast, SEER successfully improves compute-efficiency without sacrificing performance in these games (see Figure~\ref{fig:main_atari}). Overall, SEER is highly generalizable across visual domains, and can be easily combined with other modifications.

A limitation of our work is the introduction of a hyperparameter for the freezing time $t$. While domain knowledge can be used to decide a reasonable range for $t$ and reduce the search space, an interesting future direction would be to adaptively determine the freezing time using a metric of convergence. We also do not show the application of SEER to tasks which are more computationally expensive or even infeasible. We evaluate our method in DM Control and Atari because they are common RL benchmarks used in many recent works on RL from pixels, but the full impact of SEER may be more easily seen in very visually complex and challenging tasks such as 3D navigation. We do not foresee any negative societal impacts of our work, as it simply reduces training time of already existing algorithms.

\begin{figure*} [t] \centering
\subfloat[Cartpole-swingup]
{
\includegraphics[width=0.23\textwidth]{rebuttal/reduce_res_cartpole.png} 
\label{fig:reduce_res_cartpole}} 
\subfloat[Walker-walk]
{
\includegraphics[width=0.23\textwidth]{rebuttal/reduce_res_walker.png} 
\label{fig:reduce_res_walker}}
\subfloat[Alien]
{
\includegraphics[width=0.23\textwidth]{rebuttal/reduce_res_alien.png} 
\label{fig:reduce_res_alien}} 
\subfloat[Amidar]
{
\includegraphics[width=0.23\textwidth]{rebuttal/reduce_res_amidar.png} 
\label{fig:reduce_res_amidar}}
\caption{Evaluation of the compute-efficiency of CURL ((a) and (b)) and Rainbow ((c) and (d)) with original and reduced (by factor of 2) resolutions. The solid line and shaded regions represent the mean and standard deviation, respectively, across five runs.} \label{fig:reduce_res_results}
\end{figure*}

\section{Conclusion}
We presented SEER, a simple but powerful modification of off-policy RL algorithms that significantly reduces computation and memory requirements while maintaining state-of-the-art performance. We leveraged the intuition that CNN encoders in deep RL converge to their final representations early in training to freeze the encoder and subsequently store latent vectors to save computation and memory. In our experimental results, we demonstrated the compute- and memory-efficiency of SEER in various DMControl environments and Atari games, and proposed a technique for compute-efficient transfer learning. With SEER, we highlight the potential for improvements in compute- and memory-efficiency in deep RL that can be made without sacrificing performance, in hopes of making deep RL more practical and accessible in the real world. 

\section{Acknowledgements}
This research is supported in part by Open Philanthropy,
ONR PECASE N000141612723, 
NSF NRI \#2024675,
and Berkeley Deep Drive.
We would like to thank Kourosh Hakhamaneshi, Fangchen Liu, and anonymous reviewers for providing helpful feedback and suggestions.
We would also like to thank Denis Yarats for the IMPALA encoder architecture implementation and Kai Arulkumaran for help with modifying the Rainbow DQN codebase.






\bibliography{main}
\bibliographystyle{icml2021}

\section*{Checklist}



\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work?
    \answerYes{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerYes{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}

\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{}
	\item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}

\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerYes{}
	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerYes{}
	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerNo{We did not track this information.}
\end{enumerate}

\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{}
  \item Did you mention the license of the assets?
    \answerNo{This information can be found in the publicly available repositories.}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerYes{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerNA{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerNA{}
\end{enumerate}

\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}

\end{enumerate}



\newpage
\appendix
\onecolumn

\begin{center}{\bf {\LARGE Appendix}}
\end{center}

\section{Source Code} \label{appendix:source_code}
We provide source code in the supplementary materials.

\section{Algorithm} \label{appendix:pseudocode}
We detail the specifics of modifying off-policy RL methods with SEER in Algorithm \ref{lever_pseudocode}. For concreteness, we describe SEER combined with deep Q-learning methods.

\begin{algorithm*}
\caption{Stored Embeddings for Efficient Reinforcement Learning (DQN Base Agent)}\label{euclid}
\begin{algorithmic}[1]
\State Initialize replay buffer $\mathcal{B}$ with capacity $C$
\State Initialize action-value network $Q$ with parameters $\theta$ and encoder $f$ with parameters $\psi$
\For{each timestep $t$}
    \State Select action: $a_t \leftarrow \argmax_a Q_\theta(f_\psi(o_t), a)$
    \State Collect observation $o_{t+1}$ and reward $r_t$ from the environment by taking action $a_t$
    \If{$t \leq T_f$}
        \State Store transition $(o_t,a_t,o_{t+1},r_t)$ in replay buffer $\mathcal{B}$
    \Else
        \State Compute latent states $z_t, z_{t+1} \leftarrow f_\psi(o_t), f_\psi(o_{t+1})$
        \State Store transition $(z_t,a_t,z_{t+1},r_t)$ in replay buffer $\mathcal{B}$
    \EndIf
    \State \textsc{// Replace pixel-based transitions with latent trajectories}
    \If{$t = T_f$}
        \State Compute latent states $\{(z_t, z_{t+1})\}_{t=1}^{\min(T_f, c)} \leftarrow \{(f_\psi(o_t), f_\psi(o_{t+1}))\}_{t=1}^{\min(T_f, c)}$
        \State Replace $\{(o_t,a_t, o_{t+1},r_t)\}_{t=1}^{\min(T_f, c)}$ with latent transitions $\{(z_t,a_t,z_{t+1},r_t)\}_{t=1}^{\min(T_f, c)}$
        \State Increase the capacity of $\mathcal{B}$ to $\widehat{C}$
    \EndIf
    \State \textsc{// Update parameters of Q-network with sampled images or latents}
    \For{each gradient step}
        \If{$t < T_f$}
            \State Sample random minibatch $\{(o_j,a_j,o_{j+1},r_j)\}_{j=1}^b\sim\mathcal{B}$
            \State Calculate target $y_j = r_j + \gamma \max_{a'} Q_{\bar{\theta}}(f_{\bar \psi}(o_{j+1}),a')$
            \State Perform a gradient step on $\mathcal{L}^{\tt DQN} (\theta, \psi)$
        \Else
            \State Sample random minibatch $\{(z_j,a_j,z_{j+1},r_j)\}_{j=1}^b\sim\mathcal{B}$
            \State Calculate target $y_j = r_j + \gamma \max_{a'} Q_{\bar{\theta}}(z_{j+1},a')$
            \State Perform a gradient step on $\mathcal{L}^{\tt DQN} (\theta)$
        \EndIf
    \EndFor
    
\EndFor
\item[]
\end{algorithmic} \label{lever_pseudocode}
\end{algorithm*} 

\section{Calculation of Floating Point Operations} \label{appendix:flop_counting}
We consider each backward pass to require twice as many FLOPs as a forward pass. \footnote{This method for FLOP calculation is used in \url{https://openai.com/blog/ai-and-compute/}.} Each weight requires one multiply-add operation in the forward pass. In the backward pass, it requires two multiply-add operations: at layer $i$, the gradient of the loss with respect to the weight at layer $i$ and with respect to the output of layer ($i-1$) need to be computed. The latter computation is necessary for subsequent gradient calculations for weights at layer ($i-1$). 

We use functions from \citet{huang2018condensenet} and \citet{jeong2019training} to obtain the number of operations per forward pass for all layers in the encoder (denoted $E$) and number of operations per forward pass for all MLP layers (denoted $M$). 

We denote the number of forward passes per training update $F$, the number of backward passes per training update $B$, and the batch size $b$. We assume the number of updates per timestep is 1. Then, the number of FLOPs per iteration before freezing at time $t=T_f$ is:
\begin{center}
    $bF(E + M) + 2bB(E + M) + (E + M),$
\end{center}
where the last term is for the single forward pass required to compute the policy action. For the baseline, FLOPs are computed using this formula throughout training.

SEER reduces computational overhead by eliminating most of the encoder forward and backward passes. The number of FLOPs per iteration after freezing is:
\begin{center}
    $bFM + 2bBM + (E + M) + EKN,$
\end{center}
where $K$ is the number of data augmentations and $N$ is the number of networks as described in Section ~\ref{sec:detail}. The forward and backward passes of the encoder for training updates are removed, with the exception of the forward pass for computing the policy action and the $EKN$ term at the end that arises from calculating latent vectors for the current observation.

At freezing time $t=T_f$, we need to compute latent vectors for each transition in the replay buffer. This introduces a one-time cost of ($EKN\min(T_f,C)$) FLOPs, since the number of transitions in the replay buffer is $\min(T_f,C)$, where $C$ is the initial replay capacity. 

\section{Discussions on Constrained-Memory Experiments} \label{appendix:memory_assumptions}

We acknowledge that the memory efficiency advantage of SEER is conditioned on the assumption that a larger replay buffer capacity would improve performance. While the replay buffer capacity used in DM Contorl and Atari benchmarks is typically large enough to achieve strong performance, there are many real-world scenarios where memory may be limited, such as training on small devices (e.g., on the scale of mobile phones, drones, Raspberry Pi's). Our constrained-memory experiments aim to show the potential of SEER to improve performance in scenarios such as these. As a side note, another potential benefit of reduced memory requirements is the ability to store the replay buffer in GPU and reduce expensive CPU to GPU transfers, allowing for fast data reads, which would be interesting future work.

\section{Wall-Clock Time} \label{appendix:wall_clock_time}

Given our computational constraints, it is difficult to accurately measure wall-clock time and we did not run all agents on the same machine without other jobs running. To give a rough idea of wall-clock time, Figure \ref{fig:wall_clock} shows learning curves for Amidar where the x-axis shows wall-clock time. Since wall-clock time takes into account computational costs besides neural network training (e.g., interacting with the environment in the simulator), the gains are less noticeable, but Rainbow + SEER is still more compute-efficient than Rainbow. We remark that this is a very imperfect estimate of wall-clock time, due to our computational constraints.

\begin{figure} [t] \centering
\includegraphics[width=0.4\textwidth]{rebuttal/amidar_wall_clock.png}
\caption{Learning curves for Rainbow with and without SEER in Amidar, where the x-axis shows wall-clock time. The dotted gray line denotes the encoder freezing time $t=T_f$. The solid line and shaded regions represent the mean and standard deviation, respectively, across five runs.} \label{fig:wall_clock}
\end{figure}


\section{Transfer Setting Analysis} \label{appendix:additional_transfer_dmc}
In Figure \ref{fig:transfer_dmc_walk} we show the computational efficiency of SEER on Walker-walk with Walker-stand pretrained for 60K steps, with four convolutional layers frozen. We provide analysis for the number of layers frozen and number of environment interactions before freezing $T_f$ in Figure \ref{fig:transfer_dmc_ablation}. We find that freezing more layers allows for more computational gain, since we can avoid computing gradients for the frozen layers without sacrificing performance. Longer pretraining in the source task improves compute-efficiency in the target task; however, early convergence of encoder parameters enables the agent to learn a good policy even with only 20K interactions before transfer.

We remark that \citet{yosinski2014transferable} examine the generality of features learned by neural networks and the feasibility of transferring parameters between similar image classification tasks. \citet{yarats2019improving} show that transferring encoder parameters pretrained from Walker-walk to Walker-stand and Walker-run can improve the performance and sample-efficiency of a SAC agent. For the first time, we show that encoder parameters trained on simple tasks can be useful for compute-efficient training in complex tasks and new domains. 


\begin{figure} [t] \centering
\subfloat[Number of frozen layers.]
{
\includegraphics[width=0.4\textwidth]{rebuttal/walker_layer_ablation.png}} 
\subfloat[Freezing time hyperparameter $T_f$.]
{
\includegraphics[width=0.4\textwidth]{rebuttal/walker_time_ablation.png}} 
\caption{(a) Analysis on the number of frozen convolutional layers in Walker-walk training from Walker-stand pretrained for 60K steps. (b) Analysis on the number of environment steps Walker-stand agent is pretrained prior to Walker-walk transfer, where the first four convolutional layers are frozen.} \label{fig:transfer_dmc_ablation}
\end{figure}














\section{Compute-Efficiency in Constrained-Memory Settings} \label{appendix:additional_figures1}
In our main experiments, we isolate the two major contributions of our method, reduced computational overhead and improved sample-efficiency in constrained-memory settings. In Figure \ref{fig:memory_flops_atari} we show that these benefits can also be combined for significant computational gain in constrained-memory settings. 

\begin{figure*} [t] \centering
\subfloat[Alien]
{
\includegraphics[width=0.22\textwidth]{rebuttal/alien_sb_flops.png} 
\label{fig:app_atari_alien}} 
\subfloat[Amidar]
{
\includegraphics[width=0.22\textwidth]{rebuttal/amidar_sb_flops.png} 
\label{fig:app_atari_amidar}} 
\subfloat[BankHeist]
{
\includegraphics[width=0.22\textwidth]{rebuttal/bank_heist_sb_flops.png} 
\label{fig:app_atari_bank}} 
\subfloat[CrazyClimber]
{
\includegraphics[width=0.22\textwidth]{rebuttal/crazy_climber_sb_flops.png} 
\label{fig:app_atari_crazy}} 
\\
\subfloat[Krull]
{
\includegraphics[width=0.22\textwidth]{rebuttal/krull_sb_flops.png} 
\label{fig:app_atari_krull}} 
\subfloat[Qbert]
{
\includegraphics[width=0.22\textwidth]{rebuttal/qbert_sb_flops.png} 
\label{fig:app_atari_qbert}} 
\subfloat[RoadRunner]
{
\includegraphics[width=0.22\textwidth]{rebuttal/road_runner_sb_flops.png} 
\label{fig:app_atari_road}} 
\subfloat[Seaquest]
{
\includegraphics[width=0.22\textwidth]{rebuttal/seaquest_sb_flops.png} 
\label{fig:app_atari_sea}} 
\caption{Comparison of Rainbow in constrained-memory settings with and without SEER, where the x-axis shows estimated cumulative FLOPs, corresponding to Figure \ref{fig:memory_atari}. The dotted gray line denotes the encoder freezing time $t=T_f$. The solid line and shaded regions represent the mean and standard deviation, respectively, across five runs.} \label{fig:memory_flops_atari}
\end{figure*}

\section{Sample-Efficiency Plots} \label{appendix:additional_figures2}
In section \ref{main_exps} we show the compute-efficiency of our method in DMControl and Atari environments. We show in Figure \ref{fig:main_samples_dmc} that our sample-efficiency is very close to that of baseline CURL \citep{srinivas2020curl}, with only slight degradation in Cartpole-swingup and Walker-walk. In Atari games (Figure \ref{fig:main_samples_atari}), we match the sample-efficiency of baseline Rainbow \citep{hessel2018rainbow} very closely, with no degradation.

\section{General Implementation Details} \label{appendix:freezing_details}
SEER can be applied to any convolutional encoder which compresses the input observation into a latent vector with smaller dimension than the observation. We generally freeze all the convolutional layers and possibly the first fully-connected layer. In our main experiments, we chose to freeze the first fully-connected layer for DM Control experiments and the last convolutional layer for Atari experiments. We made this choice in order to simultaneously save computation and memory; for those architectures, if we freeze an earlier layer, we save less computation, and the latent vectors (convolutional features) are too large for our method to save memory. In DM Control experiments, the latent dimension of the first fully-connected layer is 50, which allows a roughly 12X memory gain. In Atari experiments, the latent dimension of the last convolutional layer is 576, which allows a roughly 3X memory gain.

\begin{figure*} [t] \centering
\subfloat[Cartpole-swingup]
{
\includegraphics[width=0.24\textwidth]{rebuttal/cartpole_lb_samples.png} 
\label{fig:app2_dmc_cart}} 
\subfloat[Finger-spin]
{
\includegraphics[width=0.24\textwidth]{rebuttal/finger_lb_samples.png} 
\label{fig:app2_dmc_finger}}
\subfloat[Reacher-easy]
{
\includegraphics[width=0.24\textwidth]{rebuttal/reacher_lb_samples.png} 
\label{fig:app2_dmc_reacher}} 
\\
\subfloat[Cheetah-run]
{
\includegraphics[width=0.24\textwidth]{rebuttal/cheetah_lb_samples.png} 
\label{fig:app2_dmc_cheetah}} 
\subfloat[Walker-walk]
{
\includegraphics[width=0.24\textwidth]{rebuttal/walker_lb_samples.png} 
\label{fig:app2_dmc_walk}} 
\caption{Comparison of the sample-efficiency of CURL with and without SEER, corresponding to Figure \ref{fig:main_dmc}. The dotted gray line denotes the encoder freezing time $t=T_f$. The solid line and shaded regions represent the mean and standard deviation, respectively, across five runs.} \label{fig:main_samples_dmc}
\end{figure*}

\begin{figure*} [t] \centering
\subfloat[Alien]
{
\includegraphics[width=0.22\textwidth]{rebuttal/alien_lb_samples.png} 
\label{fig:app2_atari_alien}} 
\subfloat[Amidar]
{
\includegraphics[width=0.22\textwidth]{rebuttal/amidar_lb_samples.png} 
\label{fig:app2_atari_amidar}} 
\subfloat[BankHeist]
{
\includegraphics[width=0.22\textwidth]{rebuttal/bank_heist_lb_samples.png} 
\label{fig:ap2_bank}} 
\subfloat[CrazyClimber]
{
\includegraphics[width=0.22\textwidth]{rebuttal/crazy_climber_lb_samples.png} 
\label{fig:app2_crazy}} 
\\
\subfloat[Krull]
{
\includegraphics[width=0.22\textwidth]{rebuttal/krull_lb_samples.png} 
\label{fig:app2_krull}} 
\subfloat[Qbert]
{
\includegraphics[width=0.22\textwidth]{rebuttal/qbert_lb_samples.png} 
\label{fig:app2_qbert}} 
\subfloat[RoadRunner]
{
\includegraphics[width=0.22\textwidth]{rebuttal/road_runner_lb_samples.png} 
\label{fig:app2_road}} 
\subfloat[Seaquest]
{
\includegraphics[width=0.22\textwidth]{rebuttal/seaquest_lb_samples.png} 
\label{fig:app2_sea}} 
\caption{Comparison of the sample-efficiency of Rainbow with and without SEER, corresponding to Figure \ref{fig:main_atari}. The dotted gray line denotes the encoder freezing time $t=T_f$. The solid line and shaded regions represent the mean and standard deviation, respectively, across five runs.} \label{fig:main_samples_atari}
\end{figure*}

\section{Freezing Time Ablation} \label{appendix:freezing_time_ablation}
The general trend for the freezing time hyperparameter $T_f$ is that freezing time around $T_f=100000$ usually works well in Atari, and in our experiments, $T_f \in \{50000, 100000, 150000\}$ produce similar results so it is not particularly sensitive to freezing time (see Figure \ref{fig:amidar_ablation} for learning curves for Amidar with $T_f \in \{50000, 100000, 150000\}$). In DM Control you need to do per-environment hyperparameter tuning since the tasks are more varied.

\begin{figure*}
\subfloat[Sample-efficiency in Amidar]
{
\includegraphics[width=0.44\textwidth]{rebuttal/amidar_sample_eff.png}} 
\subfloat[Compute-efficiency in Amidar]
{
\includegraphics[width=0.44\textwidth]{rebuttal/amidar_compute_eff.png}} 
\caption{(a) Rainbow + SEER with freezing at $T_f \in \{50000, 100000, 150000\}$, and no freezing (Rainbow) all result in similar sample-efficiency. This demonstrates that SEER is not extremely sensitive to freezing time. (b) Looking at compute-efficiency (with x-axis showing FLOPs), freezing earlier generally produces more compute-efficiency gains, but freezing at $T_f=150000$ still results in better compute-efficiency than baseline Rainbow.} \label{fig:amidar_ablation}
\end{figure*}


\section{DMControl Implementation details} \label{appendix:dmc_implementation_details}
We use the network architecture in \url{https://github.com/MishaLaskin/curl} for our CURL \citep{srinivas2020curl} implementation. We show a full list of hyperparameters in Table \ref{tbl:dmc_hyperparameters}.

\begin{table*}[ht]
\caption{Hyperparameters used for DMControl experiments. Most hyperparameter values are unchanged across environments with the exception of initial replay buffer size, action repeat, and learning rate.}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value}  \\
\midrule
Augmentation    & Crop  \\ 
Observation rendering    & $(100,100)$  \\ 
Observation down/upsampling    & $(84,84)$ \\ 
Replay buffer size in Figure \ref{fig:main_dmc}   & Number of training steps \\ 
Initial replay buffer size in Figure \ref{fig:memory_dmc}  & $1000$ cartpole, swingup; cheetah, run; finger, spin \\
 & $2000$ reacher, easy; walker, walk \\
Number of updates per training step  & $1$ \\
Initial steps    & $1000$  \\ 
Stacked frames    & $3$  \\ 
Action repeat    & $2$ finger, spin; walker, walk\\
 & $4$ cheetah, run; reacher, easy  \\
 & $8$ cartpole, swingup \\
Hidden units (MLP)    & $1024$  \\ 
Evaluation episodes    & $10$  \\ 
Evaluation frequency    & $2500$ cartpole, swingup \\ 
 & $10000$ cheetah, run; finger, spin; reacher, easy; walker, walk \\
Optimizer    & Adam  \\ 
$(\beta_1,\beta_2) \rightarrow (f_\psi, \pi_\phi, Q_\theta)$   & $(.9,.999)$  \\
$(\beta_1,\beta_2) \rightarrow (\alpha)$   & $(.5,.999)$  \\
Learning rate $(f_\psi, \pi_\phi, Q_\theta)$ & $2e-4$ cheetah, run \\ 
& $1e-3$ cartpole, swingup; finger, spin; reacher, easy; walker, walk \\
Learning rate ($\alpha$) & $1e-4$ \\
Batch Size    & $512$ cheetah, run \\
& $128$ cartpole, swingup; finger, spin; reacher, easy; walker, walk \\
$Q$ function EMA $\tau$ & $0.01$ \\
Critic target update freq & $2$ \\
Convolutional layers & $4$ \\
Number of filters & $32$ \\
Non-linearity & ReLU \\
Encoder EMA $\tau$ & $0.05$ \\
Latent dimension & $50$ \\
Discount $\gamma$ & $.99$ \\
Initial temperature & $0.1$ \\
Freezing time $T_f$ in Figure \ref{fig:main_dmc} 
 & $10000$ cartpole, swingup \\
 & $50000$ finger, spin; reacher, easy \\
 & $60000$ walker, walk  \\
 & $80000$ cheetah, run \\
Freezing time $T_f$ in Figure \ref{fig:memory_dmc} 
 & $10000$ cartpole, swingup \\
 & $50000$ finger, spin \\
 & $30000$ reacher, easy  \\
 & $80000$ cheetah, run; walker, walk  \\
\bottomrule
\end{tabular}
\end{small}
\label{tbl:dmc_hyperparameters}
\end{center}
\vskip -0.1in
\end{table*} 

\section{Atari Implementation details} \label{appendix:atari_implementation_details}
We use the network architecture in \url{https://github.com/Kaixhin/Rainbow} for our Rainbow \citep{hessel2018rainbow} implementation and the data-efficient Rainbow \citep{van2019use} encoder architecture and hyperparameters. We show a full list of hyperparameters in Table \ref{tbl:atari_hyperparameters}. 

\begin{table*}[ht]
\caption{Hyperparameters used for Atari experiments. All hyperparameter values are unchanged across environments with the exception of encoder freezing time.}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value}  \\
\midrule
Augmentation    & None  \\ 
Observation rendering    & $(84,84)$  \\ 
Replay buffer size in Figure \ref{fig:main_atari}   & Number of training steps \\ 
Initial replay buffer size in Figure \ref{fig:memory_atari}  & $10000$ \\
Number of updates per training step  & $1$ \\
Initial steps    & $1600$  \\ 
Stacked frames    & $4$  \\ 
Action repeat    & $1$ \\
Hidden units (MLP)    & $256$  \\ 
Evaluation episodes    & $10$  \\ 
Evaluation frequency    & $10000$  \\ 
Optimizer    & Adam  \\ 
$(\beta_1,\beta_2) \rightarrow (f_\psi, Q_\theta)$   & $(.9,.999)$  \\
Learning rate $(f_\psi, Q_\theta)$ 
& $1e-3$ \\ 
Learning rate ($\alpha$) & $0.0001$ \\
Batch Size    & $32$   \\ 
Multi-step returns length & $20$ \\
Critic target update freq & $2000$ \\
Convolutional layers & $2$ \\
Number of filters & $32, 64$ \\
Non-linearity & ReLU \\
Discount $\gamma$ & $.99$ \\
Freezing time $T_f$ in Figure \ref{fig:main_atari} 
 & $50000$ Alien; Amidar; BankHeist; Krull; RoadRunner; Seaquest \\
 & $100000$ CrazyClimber; Qbert \\
Freezing time $T_f$ in Figure \ref{fig:memory_atari} 
 & $50000$ Amidar; BankHeist; Krull; RoadRunner \\
 & $100000$ Alien; CrazyClimber; Qbert \\
 & $150000$ Seaquest  \\
\bottomrule
\end{tabular}
\end{small}
\label{tbl:atari_hyperparameters}
\end{center}
\vskip -0.1in
\end{table*} 

\end{document}
