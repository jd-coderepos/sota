

\documentclass{article}


\usepackage{iclr2022_conference,times}
\iclrfinalcopy


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{graphicx}
\usepackage{subfig}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{amsthm}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage[mathscr]{euscript}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{color}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            anchorcolor = blue]{hyperref}
\definecolor{COLOR}{rgb}{0, 0, 0}




\newcommand{\theHalgorithm}{\arabic{algorithm}}





\title{Spherical Message Passing for 3D Molecular Graphs}


\author{Yi Liu\thanks{These authors contribute equally to this work.}\ , \ Limei Wang\footnotemark[1]\ , \ Meng Liu, \ Yuchao Lin, \ Xuan Zhang, \ Bora Oztekin \& Shuiwang Ji \\
Department of Computer Science \& Engineering\\
Texas A\&M University\\
College Station, TX 77843, USA\\
\texttt{\{yiliu,limei,mengliu,kruskallin,xuan.zhang,bora,sji\}@tamu.edu} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\begin{document}

\maketitle



\begin{abstract}



We consider representation learning of 3D molecular graphs in which each atom is associated with a spatial position in 3D.
This is an under-explored area of research, and a principled message passing framework is currently lacking.
In this work, we conduct analyses in the
spherical coordinate system (SCS) for the complete identification of 3D graph structures.
Based on such observations, we
propose the spherical message passing (SMP) as a novel
and powerful scheme for 3D molecular learning.
SMP dramatically reduces training complexity, enabling it to perform efficiently on large-scale molecules.
In addition, SMP is capable of distinguishing almost all molecular structures,
and the uncovered cases may not exist in practice.
Based on meaningful physically-based representations of 3D information, we further propose the SphereNet for 3D molecular learning.
Experimental results demonstrate that the use of meaningful 3D information in SphereNet leads to significant performance improvements in prediction tasks.
Our results also demonstrate the advantages of SphereNet in terms of capability, efficiency, and scalability.
Our code is publicly available
as part of the DIG library (\url{https://github.com/divelab/DIG}).
\end{abstract}


\section{Introduction}
In many real-world studies, structured objects
such as molecules are naturally modeled as 
graphs~\citep{gori2005new,wu2018moleculenet,shervashidze2011weisfeiler,fout2017protein,liu2020deep,wang2020advanced}.
With the advances of deep learning, graph neural networks (GNNs)
have been developed for learning from graph 
data~\citep{kipf2016semi,defferrard2016convolutional,velivckovic2017graph,zhang2018end,xu2018powerful,gao2019graph,Gao:KDD18,gao2020topology}.
Currently, the message passing scheme~\citep{gilmer2017neural,sanchez2020learning,vignac2020building,battaglia2018relational}
is one of the commonly used architectures for realizing GNNs.
In this work, we aim at developing a novel message passing method
for 3D graphs.
Generally, a 3D molecular graph contains 3D coordinates for each atom given in the 
Cartesian system
along with the graph structure~\citep{liu2018n,townshend2019end,axelrod2020geom}.
Different types of relative 3D information can be derived from 3D molecular graphs,
and they can be important in molecular learning, such as
bond lengths, angles between bonds~\citep{schutt2017schnet,klicpera_dimenet_2020}.

We first investigate complete representations of 3D molecules.
This requires the graph structure to be uniquely defined by relative 3D information.
To this end, we conduct formal analyses in the spherical coordinate system (SCS)~\citep{chen2019clusternet}, and show that
relative location of each atom in a 3D graph is uniquely determined by three geometries, including distance, angle, and torsion.
However, such completeness needs to involve edge-based 2-hop information, leading to excessively high computational complexity.
To circumvent the computational cost, we propose a novel message passing scheme, known
as the spherical message passing (SMP),
for fast and accurate 3D molecular learning. 
Our SMP is efficient and approximately complete in representing 3D molecules.
First, we design a novel strategy to compute torsion, which only
considers edge-based 1-hop information, thus substantially reducing training complexity.
This enables the generalization of SMP to large-scale molecules.
In addition, we show that our SMP can distinguish almost all 3D graph structures.
The uncovered cases seem rarely appear in practice.
By naturally using relative 3D information and a novel torsion, SMP yields predictions that are invariant to
translation and rotation of input graphs.

We apply the SMP to real-world molecular learning, where
meaningful physical representations are needed.
\textcolor{COLOR}{Geometries $(d, \theta, \varphi)$ specified by SMP are then physically represented 
by $\Psi(d, \theta, \varphi)$, which can be a solution 
to the Schrödinger equation,
as described in Sec.~\ref{sec:spherenet}.
}
Based on this, we develop the spherical message passing neural networks,
known as the SphereNet, for 3D molecular learning.
We conduct experiments on various types of datasets including
OC20, QM9, and MD17.
Results show that, compared with baseline methods, SphereNet achieves the best performance without increasing the computing budget.
Ablation study reveals contributions and necessity of different types of 3D information, including 
distance, angle, and torsion.
Particularly, we compare with a complete message passing scheme that can distinguish all 3D graph structures but involves edge-based 2-hop information.
Experimental results
show that SphereNet achieves comparable performance but reduces running time by 4 times.
This suggests the use of SphereNet in practice rather than the complete message passing scheme, whose computational complexity prevents its use
on large molecules.



\section{Complete Representations of Molecules} \label{sec:comp}
Equivariant graph neural networks (EGNNs) represent one research area for 3D molecular graphs,
as introduced in Sec.~\ref{sec:related_3dgn}.
\textcolor{COLOR}{These methods usually take coordinates in the Cartesian coordinate system (CCS) for
all atoms as the raw input.
Hence, all the network layers need to be carefully designed to be equivariant.
The computing of some equivariant components is expensive,
like spherical harmonics and Clebsh-Gordan coefficients~\citep{thomas2018tensor,fuchs2020se}.
In addition, the complicated SE(3) group representations may not be necessary for molecular learning where final representations are
generally required to be invariant.}
In this work, we focus on the other category of methods that take relative position information purely as input to graph learning models.
Relative 3D information could be distance or angle, which is inherently invariant to translation and rotation
of input molecules.
It is natural to consider such information in the spherical coordinate system (SCS).
We start by investigating the structure identification of 3D molecules
in the SCS.
For any point in the SCS, its location is specified 
by a 3-tuple $(d, \theta, \varphi)$,
where $d$, $\theta$, and $\varphi$
denote the radial distance, polar angle,
and the azimuthal angle, respectively.
When modeling 3D molecular graphs in the SCS,
any atom $i$ can be the origin of a local SCS,
and $d$, $\theta$, and $\varphi$ naturally become
the bond length,
the angle between bonds,
and the torsion angle, respectively.
Thus, the relative location of each neighboring atom 
of atom $i$ can be specified
by the corresponding tuple $(d, \theta, \varphi)$.
Similarly, the relative location of each atom
in the 3D molecular graph can be determined,
leading to the identified structure,
which is naturally invariant to translation and rotation of the input graph.
The SCS can be easily converted from the Cartesian coordinate system,
thus, the tuple $(d, \theta, \varphi)$ can be easily obtained.

\begin{wrapfigure}[9]{r}{0.45\textwidth}\vspace{-0.4 cm}
    \includegraphics[width=0.45\textwidth]{FIG/h2o2.pdf}
    \vspace{-25 pt}
    \caption{The chemical structure of the $\mbox{H}_2\mbox{O}_2$.
    }\label{fig:h2o2}
    \vspace{-10 pt}
\end{wrapfigure}

As in Fig.~\ref{fig:h2o2}, we use the chemical structure of the hydrogen peroxide ($\mbox{H}_2\mbox{O}_2$) to show how $d$, $\theta$, and $\varphi$ are vital
for the molecular structure identification.
It is obvious that the structure is uniquely defined by the three bond lengths 
$d_1$, $d_2$, $d_3$,
the two bond angles $\theta_1$, $\theta_2$,
and the torsion angle $\varphi$.
Note that the input may not contain all pairwise distances (all possible bond lengths).
This is because the atomic connectivity is usually based on
real chemical bonds and cut-off distances.
\textcolor{COLOR}{The cut-off distance is usually set as a hyperparameter.
It is hard to guarantee that the cut-off is larger than any pairwise distance in a molecule.
Hence, in this example, H-H bond length may not be considered as input
if the cut-off is small.
Setting a proper cut-off is even harder for other complicated and large molecules
where a distance between two atoms could be large.
In addition, considering all pairwise distances will cause severe redundancies,
dramatically increasing the computational complexity.
The model also easily gets confused by excessive noise,
leading to unsatisfactory performance.
From the perspective of completeness,
using all pairwise distance is not capable of recognizing
the chirality property.
To overcome the above challenges, we use a combination of distance, angle, and torsion
for rigorous design and accurate learning.}
Apparently, the two O-H bonds can rotate around the O-O bond
without changing any of the bond lengths and bond angles.
In this situation, however,
the torsion angle $\varphi$ changes and the structure of the $\mbox{H}_2\mbox{O}_2$
varies accordingly.
The importance of
torsion angle has also been demonstrated in related research domains.
\cite{garg2020generalization} formally shows that
the torsion along with the port numbering can improve
the expressive power of GNNs in distinguishing geometric graph properties, such as
girth and circumference, \emph{etc}.
Other studies~\citep{ingraham2019generative,simm2020reinforcement}
reveal that protein sequences and molecules can be accurately generated by
considering the torsion in the given 3D structures.
In this work, we propose SMP to systematically consider distance, angle,
and torsion
for approximately complete representation learning of 3D molecular graphs.
\textcolor{COLOR}{Note that by using angle and torsion, SMP can easily 
recognize the chirality property.
}

\section{Spherical Message Passing} \label{sec:smp}
\subsection{Message Passing Scheme}
Currently, the class of message passing neural networks (MPNNs)~\citep{gilmer2017neural}
are one of the most widely used architectures for GNNs.
Based upon the completeness analyses in Sec.~\ref{sec:comp},
we propose to perform message passing in the spherical coordinate system (SCS), resulting in a novel and efficient scheme known as spherical message passing (SMP).
We show that message passing schemes used in existing methods, 
such as SchNet and DimeNet,
are special cases of SMP.


\begin{figure*}[t]
     \centering
     \subfloat[]
     {\includegraphics[width=0.41\textwidth]{FIG/final_sphere_a.pdf}\label{fig:sphere_a}}
     \qquad\quad
     \subfloat[]
     {\includegraphics[width=0.41\textwidth]{FIG/final_sphere_b.pdf}\label{fig:sphere_b}}
     \vspace{-10 pt}
    \caption{(a). The message aggregation scheme for the spherical message passing.
    (b). An illustration for computing torsion angles in the spherical message passing architecture.}
    \label{fig:sphere}
    \vspace{-12 pt}
\end{figure*}

We first formally define a 3D molecular graph, which is usually represented as a 4-tuple $G=(\mathbf{u},V,E,P)$.
The $\mathbf{u}\in\mathbb{R}^{d_\mathbf{u}}$ is a global feature vector for the molecular graph $G$. $V=\{\mathbf{v}_i\}_{i=1:n}$ is the set of atom features, where each $\mathbf{v}_i\in\mathbb{R}^{d_v}$ is the feature vector for the atom $i$.
$E=\{(\mathbf{e}_k, r_k, s_k)\}_{k=1:m}$ is the set of edges, where each $\mathbf{e}_k\in\mathbb{R}^{d_e}$ is the feature vector, 
$r_k$ is the index of the receiver atom, and $s_k$ is the index of the sender atom for the edge $k$. 
$P=\{\mathbf{r}_h\}_{h=1:n}$ is the set of 3D Cartesian coordinates that
contains 3D spatial information for each atom.
In addition, we let
$E_i = \{(\mathbf{e}_k, r_k, s_k)\}_{r_k =i, k=1:m}$ denote
the set of edges that point to the atom $i$, 
and $\mathcal{N}_i$ denote the indices of incoming nodes of atom $i$.
The outputs after a message passing process include the updated global feature vector $\mathbf{u^\prime}\in\mathbb{R}^{d_\mathbf{u}}$,
the updated atom features $V^\prime=\{\mathbf{v}^\prime_i\}_{i=1:n}$,
and the updated edges $E^\prime=\{(\mathbf{e}^\prime_k, r_k, s_k)\}_{k=1:m}$.

An illustration of the message aggregation scheme 
for SMP is provided in Fig.~\ref{fig:sphere} (a).
Apparently, the embedding of the atom $r_k$ is
obtained by aggregating each incoming message $\mathbf{e}_k$.
The message $\mathbf{e}_k$ is updated based on $E_{s_k}$,
the set of incoming messages pointing to the atom $s_k$.
Let $q$ denote the sender atom of any message in $E_{s_k}$.
Hence, we can define a local SCS,
where $s_k$ serves as the origin,
and the direction of the message  $\mathbf{e}_k$ naturally serves as the $z$-axis.
We define a neighboring atom $o$ of $s_k$ as the reference atom. Thus, 
the reference plane is formed by three atoms $s_k$, $r_k$, and $o$.
For atom $q$, its location is uniquely defined by 
the tuple $(d, \theta, \varphi)$, as shown 
in Fig.~\ref{fig:sphere} (a).
Specifically, 
$d$ determines its distance to the atom $s_k$, $\theta$
specifies its direction to update the message $\mathbf{e}_k$.
The torsion angle $\varphi$ is formed by the defined reference plane and the plane spanned by $s_k$, $r_k$, and $q$. Intuitively, as an advanced message passing architecture in spherical coordinates for 3D graphs, SMP specifies relative location for any neighboring atom $q$ by considering all the distance, angle, and torsion information, leading to more comprehensive representations for 3D molecular graphs.

Generally, the atom $s_k$ may have several neighboring atoms,
which we denote as $q_1, ..., q_t$.
It is easy to compute the corresponding bond lengths and bond angles for these $t$ atoms.
The SMP computes torsion angles by projecting
all the $t$ atoms to the plane that is perpendicular to $\mathbf{e}_k$
and intersect with $s_k$.
Then on this plane, the torsion angles are formed in a predefined direction, such as the anticlockwise direction.
By doing this, any atom naturally becomes the reference atom for its next atom
in the anticlockwise direction.
Notably, the sum of these $t$ torsion angles is $2\pi$.
A simplified case is illustrated in Fig.~\ref{fig:sphere} (b).
The atom $s_k$ has three neighboring atoms $q_1$, $q_2$, and $q_3$;
$q_3$ is the reference atom for $q_1$, and they form $\varphi_1$;
$q_1$ is the reference atom for $q_2$, and they form $\varphi_2$;
similarly, $q_2$ is the reference atom for $q_3$, and they form $\varphi_3$.
It is obvious that the sum of $\varphi_1$, $\varphi_2$, and $\varphi_3$ is $2\pi$.
\textcolor{COLOR}{As the torsion is defined relatively, $q_1$ can be picked arbitrarily, which will not
affect the output of the message passing scheme, as we perform summation when aggregating information to $s_k$ from its neighbors
$q_1$, $q_2$, and $q_3$.}
Notably, by designing each atom to be the reference atom of the next one in the predefined direction,
invariance is effectively achieved because reference atom is naturally relative.
In addition, our method computes torsion within edge-based 1-hop neighborhood. 
Even though a torsion angle involves four atoms, 
our design avoids the number
of torsion angles to be exponential,
but makes it the same as the number of neighboring atoms.
Hence, it is efficient and will not cause time or memory issues.
Formally, the proposed SMP can be defined in the SCS as
\begin{equation}\label{eq:sp_fw}
\begin{aligned}
&\mathbf{e}^\prime_k = \phi^e\left(\mathbf{e}_k, \mathbf{v}_{r_k}, \mathbf{v}_{s_k}, E_{s_k}, \rho^{p\rightarrow e}\left(\{\mathbf{r}_h\}_{h=r_k \cup s_k \cup \mathcal{N}_{s_k}}\right)\right), \\
&\mathbf{v}^\prime_i = \phi^v\left(\mathbf{v}_i, \rho^{e\rightarrow v}\left(E_i^{\prime}\right)\right),
\mathbf{u}^\prime =  \phi^u\left(\mathbf{u}, \rho^{v\rightarrow u}\left(V^\prime\right)\right),
\end{aligned}
\end{equation}

where $\phi^e$, $\phi^v$, and $\phi^u$ are three information update functions on edges, atoms, and the whole graph, respectively.
$\rho^{e\rightarrow v}$ and $\rho^{v\rightarrow u}$ aggregate information between different types of geometries.
Especially, in SMP, the 3D information in $P$ is converted and incorporated to
update each message $\mathbf{e}^k$. Hence,
SMP employs another position 
aggregation function $\rho^{p\rightarrow e}$ for message update.
\textcolor{COLOR}{Notably, the main difference between our SMP scheme
defined in Eq.~\ref{eq:sp_fw} and the GN framework in~\cite{battaglia2018relational} is the inclusion of 3D information $P$. In line with the research area described in Sec.~\ref{sec:rw_relative},
we focus on such 3D information and develop a systematic solution to incorporate it
completely and efficiently.}
Detailed description of these functions is given in Appendix~\ref{sec:supp_A}.

\begin{wrapfigure}[21]{r}{0.7\textwidth}\vspace{-0.4 cm}
    \includegraphics[width=0.7\textwidth]{FIG/complete.pdf}
    \caption{An illustration of cases that SMP can and cannot distinguish.
    All the neighboring nodes of $s_k$ are projected to the plate perpendicular to
    the message of interest. We assume all the distances and angles are fixed (the molecules can be more 
    easily distinguished otherwise).
    Hence, all the angle shown are torsion angles and
    they are formed in the anticlockwise direction.
    (a) and (b) are chiral and SMP can distinguish them. This is because in (a),
    $q^\prime_1(90^\circ)$, $q^\prime_2(60^\circ)$, $q^\prime_3(120^\circ)$,
    $q^\prime_4(90^\circ)$; in (b),
    $q^\prime_1(60^\circ)$, $q^\prime_2(120^\circ)$, $q^\prime_3(90^\circ)$,
    $q^\prime_4(90^\circ)$.
    SMP cannot distinguish (b) and (c) but this scenario may not exist in nature.
    $\angle q^\prime_1s_kq^\prime_2$ in (b) and $\angle q^\prime_1s_kq^\prime_3$ in (c)
    usually are different as $q^\prime_2$ and $q^\prime_3$ are different atoms and the corresponding
    distances and angles are the same.
    }\label{fig:complete}
    \vspace{-10 pt}
\end{wrapfigure}

\subsection{Completeness Versus Efficiency} \label{sec:comp_effi}
The identification criteria described in Sec.~\ref{sec:comp}
can fully determine the structure of a 3D molecule, but
involves edge-based 2-hop information.
Hence, the computational complexity is as sizeable as $O(nk^3)$,
where $n$ is the number of atoms, and $k$ denotes the average number
of neighboring atoms for each center atom.
Unfortunately, such design can hardly generalize to 
large molecular graphs.
To this end, we propose SMP as an efficient and scalable scheme to
realize message passing in SCS.
Our SMP only involves edge-based 1-hop information,
thus the time complexity is reduced to $O(nk^2)$.
This enables the application of SMP to large molecules,
like the newly released OC20 data~\citep{chanussot2020open}.
We rigorously investigate the completeness of SMP and 
show that it can distinguish even complex geometric 
properties such as chirality, as indicated by Fig.~\ref{fig:complete} (a) and Fig.~\ref{fig:complete} (b).
As SMP uses the last atom as the reference atom (like $q_2$ is the reference atom for $q_3$ in Fig.~\ref{fig:sphere} (b)) in a predefined direction, the relative order between adjacent atoms is considered while the absolute order is neglected.
Hence, SMP can not distinguish the two molecules illustrated by Fig.~\ref{fig:complete} (b) and Fig.~\ref{fig:complete} (c).
However, this scenario may not exist in nature. This is also demonstrated in experiments
that our SMP achieves comparable performance with the complete representations, while the latter
induces huge time complexity and severe memory issues.

\subsection{Relations with Prior Message Passing Methods}
When developing message passing methods for 3D graphs,
sphere message passing is an advanced scheme
where the relative location of each atom is more specified.
The development for 3D graphs with relative information is still in early stage.
To our best knowledge, there exist several notable methods
in literature, and they can be viewed
as special cases of the SMP, as they capture partial 3D position information.
For example, the SchNet and PhysNet consider distance, and 
the DimeNet encodes directional information.
Formally, these methods can be perfectly fit 
the Spherical scheme defined in Eq.~\ref{eq:sp_fw}.
We describe the details of these methods in Appendix~\ref{sec:supp_C}.
Notably, compared with prior models, SMP
provides a rigorous justification on 
its completeness
with failure cases clearly described.
Importantly, SMP is developed based on the
identification analyses of 3D molecular graphs.
Hence, it aims at learning complete data representations for 3D molecular graphs, rather than simply including extra 3D information (like angle or torsion). 


\section{SphereNet} \label{sec:spherenet}
The obtained 3-tuple $(d, \theta, \varphi)$ indicates the relative location of 
any atom in a 3D molecular graph. However, 
it cannot serve as the direct input to neural networks as
it lacks meaningful representations.
Essentially, molecules are quantum systems thus the representation design needs to
follow physics laws.
An important aspect is to choose appropriate basis functions that 
transform the 3-tuple $(d, \theta, \varphi)$ into physically-based representations.
Several basis functions have been explored in~\citet{hu2021forcenet,klicpera_dimenet_2020}, including
MLP, Gaussian and sine functions, spherical Bessel basis, and spherical harmonics.
Especially, spherical Bessel is shown to be the best basis for encoding distance, and
spherical harmonics is the most appropriate
one for encoding angle~\citep{hu2021forcenet,klicpera_dimenet_2020}.
We denote the final representation as $\Psi(d, \theta, \varphi)$.
Referring to theories in~\citet{griffiths2018introduction,cohen2019gauge,klicpera_dimenet_2020},
one form of the representation
can be expressed as $\Psi(d, \theta, \varphi) = j_{\ell}\left(\frac{\beta_{\ell n}}{c}d\right)Y_{\ell}^{m}(\theta, \varphi)$,
where $j_{\ell}(\cdot)$ is a spherical Bessel function of order $\ell$,
$Y_{\ell}^{m}$ is a spherical harmonic function of degree $m$ and order $\ell$,
$c$ denotes the cutoff,
$\beta_{\ell n}$ is the $n$-th root of the
Bessel function of order $\ell$.
We also have
$\ell \in [0,\cdots,L-1]$, $m \in [-\ell,\cdots,\ell]$ and $n \in [1,\cdots,N]$.
$L$ and $N$ denote
the highest orders for the spherical harmonics and spherical Bessel functions,
respectively.
They are hyperparameters in experimental settings.
In addition, we can derive two simplified representations $\Psi(d)$ and $\Psi(d, \theta)$
from $\Psi(d, \theta, \varphi)$.


Based upon the spherical message passing scheme described in Sec.~\ref{sec:smp} and physical representations, we build the SphereNet
for molecular learning.
Apparently, SphereNet can produce data representations
that are both accurate and physically meaningful.
By incorporating the position information in
spherical coordinates,
SphereNet also generates predictions invariant to 
translation and rotation of input molecules.
\textcolor{COLOR}{Following the architecture design in the research line
stated in Sec.~\ref{sec:rw_relative},}
our network is composed of an input block,
several interaction blocks, and an output block.
For clear description, we assume the message 
$\mathbf{e}^k$ for the edge $k$ in Fig.~\ref{fig:sphere} and Eq.~(\ref{eq:sp_fw}) is the message for update.
The update process and detailed architecture for the SphereNet are 
provided
in Appendix~\ref{sec:supp_B}.



\section{Related Work}



\subsection{Methods for 3D Molecular Graphs}  \label{sec:related_3dgn}
\subsubsection{Equivariant Graph Neural Networks}
One research line for 3D molecular graphs is equivariant graph neural networks (EGNNs) including
tensor field networks (TFNs)~\citep{thomas2018tensor},
SE(3)-transformers~\citep{fuchs2020se},
PaiNN~\citep{schutt2021equivariant},
NequIP~\citep{batzner2021se}, Noisy Nodes~\citep{godwin2021very}, \emph{etc.}
\textcolor{COLOR}{The raw input of 
these methods usually contains the absolute information, such as coordinates in the Cartesian coordinate system.
In intermediate layers, absolute information could be decomposed into partial absolute information and partial relative information as needed.
A simple example is that a vector can be decomposed into its orientation (absolute) and length (relative)~\citep{thomas2018tensor,schutt2021equivariant}.
Apparently, network components
of these methods should be carefully designed to be equivariant.}
The preliminary work like TFNs were developed for 3D point clouds.
However, it is demonstrated that for molecules whose downstream tasks usually require
the systems to be invariant, the complicated SE(3) group representations are not necessary but $S^2$ representations are enough~\citep{klicpera2021gemnet}.
Moreover, their performance on molecular tasks is not satisfactory.

\subsubsection{Invariant Graph Neural Networks} \label{sec:rw_relative}
Another category of methods purely take relative 3D information as input,
such as
distances between atoms, angles between bonds, angles between planes, \emph{etc.}
\textcolor{COLOR}{Hence, the network is naturally invariant.}
The development of these methods is in early stage, and
existing studies focus on leveraging different geometries.
The SchNet~\citep{schutt2017schnet} 
incorporates the distance information during the information aggregation stage by using continuous-filter convolutional layers.
The PhysNet~\citep{unke2019physnet} integrates both
the atom features and distance information in the proposed interaction block.
The DimeNet~\citep{klicpera_dimenet_2020}
is developed based on the PhysNet and moves a step forward by
considering directional information in the interaction block.
The GemNet~\citep{klicpera2021gemnet} is proposed recently for universal 
molecular representations.
The OrbNet~\citep{qiao2020orbnet}
combines distance information with the 
atomic orbital theory to design important SAAO features as inputs to
GNNs.
Generally, the use of 3D position information usually results in
improved performance. However,
existing methods simply include additional geometries such as distance and angle, and there lacks a rigorous justification on how different geometries contribute to the information aggregation process.
We conduct formal analysis and show that all the distance, angle, and torsion are necessary for 3D molecular identification, based on which we propose SphereNet to generate more powerful molecular representations.


\subsection{Methods for Other Objects Modeled as Graphs}
\textcolor{COLOR}{Besides molecules,
many other data objects are also represented as graphs, such as 3D point clouds~\citep{guo2020deep,simonovsky2017dynamic,shen2018mining,landrieu2018large} and meshes~\citep{bronstein2021geometric,de2020gauge,perraudin2019deepsphere}.}
When modeling 3D point clouds as 3D graphs,
points are represented as nodes and connections between points are directed edges. Existing methods mainly capture distance information from local neighborhood in 3D space.
In DGCNN~\citep{wang2019dynamic}, a novel layer namely EdgeConv is proposed to aggregate
distance-based edges features for node learning.
In~\cite{landrieu2019point}, neighborhood radius along with spatial orientation
is incorporated in local point embedding.
The work~\cite{wang2019graph} proposes a graph attention convolution for 3D point clouds.
Generally, these methods can be fit into our message passing scheme defined in Eq.~\ref{eq:sp_fw}.
\textcolor{COLOR}{
The work~\cite{de2020gauge} is an exemplary study that
formulates meshes as graphs with considering geometrical information. 
The used convolutional kernel depends on the angle 
between a predefined reference edge and any other edge projected to the tangent plane for each vertex. It focuses on the design of gauge equivariance rather than the learning of complete geometry information.
In this work, we study the complete learning of 3D molecules
and leave extensive studies
on other data types as future work.}


\section{Experimental Studies}
\subsection{Experimental Setup} \label{sec: setup}
We apply our SphereNet to three benchmark datasets, including 
Open Catalyst 2020 (OC20)~\citep{chanussot2020open}, QM9~\citep{ramakrishnan2014quantum}, and MD17~\citep{chmiela2017machine}.
Baseline methods include 
PPGN~\citep{maron2019provably},
SchNet~\citep{schutt2017schnet},
PhysNet~\citep{unke2019physnet},
Cormorant~\citep{anderson2019cormorant},
PaiNN~\citep{schutt2021equivariant},
NequIP~\citep{batzner2021se},
MGCN~\citep{lu2019molecular},
DimeNet~\citep{klicpera_dimenet_2020}, DimeNet++~\citep{klicpera_dimenetpp_2020},
GemNet~\citep{klicpera2021gemnet}, CGCNN~\citep{xie2018crystal}, and sGDML~\citep{chmiela2018towards}.
Detailed configurations of all the models used in the following sections are provided in the supplementary material.
Unless otherwise specified, for all the baseline methods, we report the results taken from the referred papers
or provided by the original authors.
For the SphereNet,
all models are trained using the Adam
optimizer~\citep{kingma2014adam}. 
The optimal hyperparameters are obtained by grid search.
Network configurations and 
search space for all models are provided in
Appendix~\ref{sec:supp_D}.


\begin{table*}[t]
    \begin{center}
        \caption{Comparisons between SphereNet and other models on IS2RE in terms of energy MAE and the percentage of EwT of the ground truth energy. Results reported for models trained on the All training dataset. The best results are shown in bold.}
    \label{tb:oc20}
    \resizebox{\textwidth}{!}
            {\begin{tabular}{l ccccc | ccccc  }
            \bottomrule
            &\multicolumn{5}{c|}{Energy MAE [eV] $\downarrow$} & \multicolumn{5}{c}{EwT $\uparrow$}  \\
            \cmidrule(l{4pt}r{4pt}){2-6}
            \cmidrule(l{4pt}r{4pt}){7-11}
         Model & ID &  OOD Ads & OOD Cat & OOD Both &Average& ID &  OOD Ads & OOD Cat & OOD Both &Average\\
                \midrule
               CGCNN
                    & 0.6203 & 0.7426 & 0.6001& 0.6708& 0.6585
                    & 3.36\% & 2.11\% & 3.53\% & 2.29\% & 2.82\% \\
                 SchNet
                    & 0.6465 & 0.7074& 0.6475 & 0.6626& 0.6660
                    & 2.96\% & 2.22\% &3.03\%& 2.38\% & 2.65\% \\
                 DimeNet++
                    & 0.5636 & 0.7127 & 0.5612& 0.6492& 0.6217
                    & 4.25\% & 2.48\%& 4.40\%& 2.56\% & 3.42\%\\
                GemNet-T
                    & \textbf{0.5561} & 0.7342 & 0.5659& 0.6964& 0.6382
                    & 4.51\% & 2.24\%& 4.37\%& 2.38\% & 3.38\%\\
                 \textbf{SphereNet}
                    & 0.5632 & \textbf{0.6682} & \textbf{0.5590} & \textbf{0.6190}& \textbf{0.6024}
                    & \textbf{4.56\%} & \textbf{2.70\%} & \textbf{4.59\%} & \textbf{2.70\%}& \textbf{3.64\%} \\
            \bottomrule
            \end{tabular}}
            \vspace{-10pt}
    \end{center}
\end{table*}
\setlength{\tabcolsep}{1.4pt}



\subsection{OC20}
The Open Catalyst 2020 (OC20) dataset is a newly released large-scale dataset for catalyst discovery and optimization~\citep{chanussot2020open}.
It comprises millions of DFT relaxations across huge
chemical structure space such that machine learning models can
be fully trained.
We focus on the IS2RE task in this work, and description
of the data is provided in Appendix~\ref{sec:oc20_intro}.
Results for CGCNN, SchNet, and DimeNet++ are provided in~\citet{chanussot2020open}.
The original GemNet paper does not contain results on the OC20 dataset,
We use the publicly available code from the OC Project website\footnote{\url{https://github.com/Open-Catalyst-Project/ocp}}
to produce results for GemNet-T.
We report evaluation results of fixed epochs for SphereNet.
Following a setting in~\citet{chanussot2020open}, we use the direct approach and all the training data for training models.
The used metrics are the energy MAE and 
the percentage of Energies within a Threshold (EwT) of the ground truth energy.
Table~\ref{tb:oc20} shows that the SphereNet achieves the best performance
on 3 out of 4 splits and the average in terms of energy MAE.
For EwT, SphereNet is the best on all the 4 splits.
Specifically, it reduces the average energy MAE by 0.019,
which is 3.10\% of the second best model.
In addition, it improves the average EwT from 3.42\% to 3.64\%,
which is a large margin considering the inherently low EwT values.

Notably, ForceNet~\citep{hu2021forcenet} and GemNet~\citep{klicpera2021gemnet} are recently proposed
for quantum system learning.
A prominent advantage for ForceNet is its high efficiency and scalability to large molecules.
ForceNet focuses on S2EF thus there are no original results for the IS2RE task.
However, DimeNet++ is slightly better than ForceNet in terms of performance,
and our SphereNet outperforms DimeNet++ significantly.
GemNet has two variants GemNet-T and GemNet-Q.
GemNet-T considers distance and angle information as input,
and contains an effective architecture
with novel network components, such as bilinear layers and scaling factors.
We can see GemNet-T is similar as DimeNet++ in terms of performance.
GemNet-Q is claimed to be able to capture universal
representations of molecules. However, it considers edge-base 2-hop information 
and the time complexity is 
extremely high. It may not be configured properly on the large
catalyst molecules.

\begin{table*}[t]
\centering
\caption{Comparisons between SphereNet and other models
    in terms
    of MAE and the overall mean std. MAE on QM9.
    ‘-’ denotes no results are reported in the referred papers for the corresponding properties.
    The best results are shown in bold and the
    second best results are shown with underlines.
}\label{tb:qm9}
\resizebox{\textwidth}{!}
{\begin{tabular}{llccccccccc}
\bottomrule 
Property &                                             Unit &         PPGN &                SchNet &       PhysNet &    Cormorant &   MGCN &   DimeNet  &DimeNet++ & PaiNN & \textbf{SphereNet}\\
\hline
$\mu$                        &                                      D &  0.047 &           0.033 &  0.0529 &      0.13 & 0.0560&  0.0286
&  0.0297 & \textbf{0.012} &  \underline{0.0245}\\
$\alpha$                     &                                     ${a_0}^3$ &  0.131 &           0.235 &  0.0615 &    0.092 & \textbf{0.0300} & 0.0469 &  \underline{0.0435} & 0.045& 0.0449\\
$\epsilon_\text{HOMO}$       &                         meV &   40.3 &              41 &    32.9 &         36 &  42.1  & 27.8 
&  \underline{24.6} & 27.6 & \textbf{22.8}\\
$\epsilon_\text{LUMO}$       &                          meV &   32.7 &              34 &    24.7 &        36 &  57.4 &  19.7 
&  \underline{19.5} & 20.4 & \textbf{18.9}\\
$\Delta\epsilon$             &                          meV &     60.0 &              63 &    42.5 &        60&  64.2 &  34.8 
&\underline{32.6} & 45.7 &   \textbf{31.1}\\
$\left< R^2 \right>$         &                                     ${a_0}^2$ &  0.592 &  \underline{0.073} &   0.765 &    0.673 &   0.110    &      0.331 &  0.331 & \textbf{0.066} & 0.268\\
ZPVE                         &                          meV &   3.12 &             1.7 &    1.39 &      1.98 & \textbf{1.12}    & 1.29
&  \underline{1.21}&1.28 &  \textbf{1.12}\\
$U_0$                        &                          meV &   36.8 &              14 &    8.15 &        28&  12.9 &  8.02 
&  6.32 & \textbf{5.85}& \underline{6.26}\\
$U$                          &                          meV &   36.8 &              19 &    8.34 &                - &  14.4 &  7.89
&  \underline{6.28} & \textbf{5.83}& 6.36\\
$H$                          &                         meV &   36.3 &              14 &    8.42 &               - & 14.6  &  8.11
&  6.53 & \textbf{5.98}& \underline{6.33}\\
$G$                          &                          meV &   36.4 &              14 &    9.40 &                 - &  16.2  & 8.98 
&  \underline{7.56} & \textbf{7.35}&  7.78\\
$c_\text{v}$ \vspace{1pt}    &  $\frac{\mbox{cal}}{\mbox{mol K}}$ &  0.055 &           0.033 &  0.0280 &    0.031 & 0.0380 & 0.0249 
&  \underline{0.0230} & 0.024&  \textbf{0.0215}\\
\hline 
std. MAE &                                    \% &   1.84 &            1.76 &    1.37 &     2.14 & 1.86   & 1.05 &  \underline{0.98} & 1.01& \textbf{0.91}\\
\bottomrule
\end{tabular}}
\end{table*}









\subsection{QM9}
We apply the SphereNet to the QM9 dataset,
which is widely used
for predicting various properties of molecules. 
It consists organic molecules composed of up to 9 heavy atoms. Thus, this test can examine
the power of the SphereNet for similar quantum chemistry systems.
The dataset is original split into three sets, where the training set
contains 110,000, the validation set contains 10,000, and the test set contains 10,831 molecules.
For energy-related properties, the training processes
use the unit eV.
All hyperparameters are tuned on the validation set
and applied to the test set.
We compare our SphereNet with baselines 
using mean absolute error (MAE) for each property and 
the overall mean standarized MAE (std. MAE) for all the 12 properties.
The comparison results are 
summarized in Table~\ref{tb:qm9}.
SphereNets achieves best
performance on 5 properties and the second best performance
on 3 properties.
It also improves the overall mean std. MAE
of the QM9 dataset from 0.98 to 0.91 and sets the new state of the art.
\textcolor{COLOR}{Notably, the most recent method PaiNN uses the same data splits as SphereNet in terms of sample numbers.
Its final performance is the average of three different runs
on three random splits.
We follow such settings and run SphereNet on four properties including
$\epsilon_\text{HOMO}$, $\epsilon_\text{LUMO}$, $U_0$, and $\mu$.
The corresponding results are $22.9\pm0.2$, $18.8\pm0.2$, $6.28\pm0.05$, and $0.0243\pm0.00$, respectively.
It is obvious that these results are highly close to those in Table~\ref{tb:qm9}, thus, we can draw consistent conclusions.}




\subsection{MD17}

\begin{table}[t]
\centering 
\caption{Comparisons between SphereNets and other models
    in terms
    MAE of forces on MD17. WoFE indicates weight of force over energy in loss functions.
    Results of all baseline models are directed taken or adapted (if the unit varies) from the original papers,
    and SphereNet uses two WoFEs in line with the original papers of different baselines for fair comparisons.
    The best results are shown in bold and the
    second best results are shown with underlines.
}\label{tb:md17}
{\begin{tabular}{l cccc | cccc  }
            \bottomrule
            &\multicolumn{4}{c|}{WoFE = 100} & \multicolumn{4}{c}{WoFE = 1000}  \\
            \cmidrule(l{4pt}r{4pt}){2-5}
            \cmidrule(l{4pt}r{4pt}){6-9}
Molecule &        sGDML &                SchNet  & DimeNet & \textbf{SphereNet} &NequIP& GemNet-T & GemNet-Q & SphereNet \\
\midrule
Aspirin & 0.68& 1.35& \underline{0.499} &\textbf{0.430} & 0.353  &0.220 & 0.217 & 0.209\\
Benzene  & 0.20& 0.31& \underline{0.187} &\textbf{0.178} & 0.186   &0.145 & 0.145 & 0.147\\
Ethanol & 0.33& 0.39& \underline{0.230} &\textbf{0.208} &  0.204  &0.086 & 0.088 & 0.091\\
Malonaldehyde  & 0.41& 0.66& \underline{0.383} &\textbf{0.340} & 0.328  &0.155 & 0.160 & 0.172\\
Naphthalene & \textbf{0.11}& 0.58& 0.215 &\underline{0.178}&  0.105 &0.055 & 0.051 & 0.048\\
Salicylic acid & \textbf{0.28}& 0.85& 0.374 &\underline{0.360} & 0.242  &0.127 & 0.125 & 0.113\\
Toluene  & \textbf{0.14}& 0.57& 0.216 &\underline{0.155} &  0.102  &0.060 & 0.060 &0.054\\
Uracil & \textbf{0.24}& 0.56& 0.301 &\underline{0.267} &  0.173  &0.097 & 0.104& 0.106\\
\hline
std. MAE & 1.11& 2.38& \underline{1.10} &\textbf{0.97} &  0.79 &0.45& 0.45 & 0.44\\
\bottomrule
\end{tabular}}
\end{table}
\setlength{\tabcolsep}{1.4pt}

The MD17 dataset is used to examine the expressive
power of SphereNet for molecular dynamics simulations.
Following the settings in~\citet{schutt2017schnet,klicpera_dimenet_2020},
we train a separate model for each molecule to predict atomic forces.
We use 1000 samples for training, and each of the eight molecules
has both the validation and test sets.
Note that all the baseline models employ a joint loss of forces and conserved energy
during training.
In the original SchNet~\citep{schutt2017schnet} and DimeNet~\citep{klicpera_dimenet_2020} papers, the authors set the weight of force over energy (WoFE) to 100,
while the NequIP~\citep{batzner2021se} and GemNet~\citep{klicpera2021gemnet} papers use a weight of 1000. As the WoFE tends to affect the force prediction significantly,
we perform SphereNet with both WoFE values for fair comparisons.
PaiNN~\citep{schutt2021equivariant} uses neither 100 nor 1000 as WoFE, so we do not compare with it on MD17.
The results for forces are reported in Table~\ref{tb:md17}.
Note that for Benzene, all models are evaluated on Benzene17,
thus, the result for sGDML is 0.20 rather than 0.06 (Benzene18).
We can observe from the table that when WoFE is 100 for all models, SphereNet consistently outperforms
SchNet and DimeNet by largin margins.
Notably, sGDML is one of the original work that created the MD17 dataset
with carefully-designed features.
Compared with sGDML,
SphereNet performs better on four and worse on the other four molecules, which is similar to DimeNet. One reason is sGDML incorporates molecular symmetries to boost precision, and different molecules have different symmetries. However, sGDML
has poorer generalization power to larger datasets without hand-engineered features.
In addition, SphereNet achieves much better overall std. MAE than sGDML.
When using the same WoFE that is 1000,
SphereNet achieves similar results with GemNet
in spite of that GemNet-T is of high complexity and contains carefully designed network components for performance
boost. 







\subsection{Completeness versus Efficiency}

\begin{wraptable}[15]{r}{0.6\textwidth}\vspace{-22 pt}
\centering 
\caption{Comparisons bewtween SMP and Q-MP on MD17 using two backbone networks.
}\label{tb:complete}
{\begin{tabular}{l cc | cc  }
            \bottomrule
            &\multicolumn{2}{c|}{SphereNet Backbone} & \multicolumn{2}{c}{GemNet Backbone}  \\
            \cmidrule(l{4pt}r{4pt}){2-3}
            \cmidrule(l{4pt}r{4pt}){4-5}
Molecule &             SMP & Q-MP &  SMP & Q-MP \\
\midrule
Aspirin &  0.209 &0.247 & 0.225 & 0.231 \\
Benzene  & 0.147 &0.153 & 0.144 & 0.149 \\
Ethanol &  0.091 &0.102 & 0.089 & 0.083 \\
Malonaldehyde & 0.172&0.168& 0.169 & 0.176\\
Naphthalene &  0.048&0.057& 0.063 & 0.062 \\
Salicylic acid & 0.113&0.125& 0.111 & 0.114 \\
Toluene  &  0.054&0.043 & 0.052 &0.063\\
Uracil &  0.106 &0.106 & 0.098 & 0.113\\
\hline
Time/Epoch (s) & 324 & 1270 &295 & 1185\\
\bottomrule
\end{tabular}}
\end{wraptable}
The message passing scheme Q-MP in GemNet represents the edge-based 2-hop
geometric message passing and can generate complete representations of
3D molecular graphs.
We study the capability and efficiency of the proposed SMP by comparing with Q-MP.
Specifically, We use the same backbone network for these two MP methods
for fair comparisons.
We extensively use two backbones, which are the SphereNet backbone introduced in 
Sec.~\ref{sec:spherenet} and the GemNet backbone proposed in~\cite{klicpera2021gemnet}.
We conduct experiments on MD17, reporting performance and average running time for all the 8 molecules per epoch
using the same computing infrastructure (Nvidia GeForce RTX 2080 TI 11GB).
Results are shown in Table~\ref{tb:complete}, from which we can observe
that on either backbone network, SMP achieves very similar results with Q-MP.
However, the time cost is much less than SMP, which indicates it is much more efficient
than Q-MP. Based on analyses in Sec.~\ref{sec:comp_effi}, SMP can distinguish
almost all molecular structures, and the failure cases may not exist in nature.
Hence, SMP performs similarly with Q-MP even though the latter is complete theoretically 
but not scalable in practice.
We further compare the efficiency between SphereNet and
other models in terms of parameters and time cost in Appendix~\ref{sec:efficiency}.
SphereNet uses similar computing budget with others but achieves the best performance.

\subsection{Ablation Study}

\begin{wraptable}[9]{r}{0.6\textwidth}
\vspace{-20pt} \centering 
\caption{Comparisons among three message passing strategies
on the same SphereNet architecture
on the partial MD17 dataset.
}\label{tb:abl}
\setlength{\tabcolsep}{1.6mm}
\begin{tabular}{lc c c}
\bottomrule Molecule &  SMP w/o ($\theta$, $\varphi$)&   SMP w/o $\varphi$ 
& SMP\\
\hline
Ethanol &  0.249& 0.22& 0.208\\
Malonaldehyde  &  0.550& 0.360& 0.340\\
Naphthalene &  0.372& 0.205& 0.178\\
Toluene  &  0.446& 0.182& 0.155\\
\bottomrule
\end{tabular}
\end{wraptable}
The proposed SMP considers all the distance, angle, and torsion, leading to more powerful data representations.
We investigate contributions of different geometries to demonstrate the advances of our SMP.
We remove torsion information from SMP which we denote as
``SMP w/o $\varphi$'';
we further remove angle information which we denote as
``SMP w/o ($\theta$, $\varphi$)''.
The three message passing strategies are integrated to the same architecture with other network parts remaining the same.
We evaluate these models on four molecules 
of MD17.
Table~\ref{tb:abl} shows
that SMP outperforms SMP w/o $\varphi$, and SMP w/o $\varphi$
outperforms ``SMP w/o ($\theta$, $\varphi$)''.
These results demonstrate the effectiveness of angle and torsion information used in the SMP.
The best performance of SMP further reveals
that SMP represents an accurate scheme for 3D graphs.
In addition, we provide visualization results
for SphereNet filters in Appendix~\ref{sec:supp_E}
to further show that
all the distance, angle, and torsion information
determine the structural semantics of filters.





\section{Conclusions} \label{sec:conc}
3D information is important for molecules
but there lacks a principled message passing framework to consider it.
We first propose the
spherical message passing as a unifying and efficient
scheme that can achieve approximately complete representations of molecules
without increasing computing budget.
Based on SMP and meaningful physical representations,
SphereNet is presented,
and experiments on various types of datasets demonstrates its
capability, efficiency, and scalibility.


\newpage
\subsection*{REPRODUCIBILITY STATEMENT}
Detailed experimental setup is provided in Appendix~\ref{sec:supp_D}. 
Implementation hyper-parameters of SphereNet
on all the three datasets OC20, QM9, and MD17 are given in Table~\ref{tb:hyper_oc20}, Table~\ref{tb:hyper_qm9},
and Table~\ref{tb:hyper_md17}, respectively.
Code is integrated in the DIG 
library~\citep{liu2021dig}
and available 
at \url{https://github.com/divelab/DIG}.

\subsection*{ACKNOWLEDGMENTS}
This work was supported in part by National Science Foundation grant IIS-1908198 and National Institutes of Health
grant 1R21NS102828. 
We thank Hannes Stärk for his valuable suggestions and discussions when developing the methods.

\bibliography{deep}
\bibliographystyle{iclr2022_conference}





\clearpage
\appendix

\setcounter{page}{1}


\begin{center}

    \Large{Spherical Message Passing for 3D Molecular Graphs: Appendix}

\end{center}


\begin{figure}[th!]
    \centering
    \includegraphics[width=\textwidth]{FIG/arch.pdf}
    \vspace{-6pt}
    \caption{Architecture of SphereNet. LB2 denotes a linear block with two linear layers, $\sigma$(LB) denotes a linear layer followed by an activation function, $\|$ denotes concatenation, and $\odot$ denotes element-wise multiplication.
    Each LB2 aims at canceling bottlenecks by performing downprojection,
    followed by upprojection.
    Hence, it is related to three hyperparameters; these are, 
    input embedding size, intermediate size, and output embedding size.
    Each linear block LB is related to hyperparameters including input embedding size and output embedding size.
    Description of each block is in Sec.~\ref{sec:supp_B}.
    }\label{fig:arch}
    \vspace{-10 pt}
\end{figure}



\section{Update Functions in SMP}  \label{sec:supp_A}
\begin{wrapfigure}[14]{r}{0.5\textwidth}\vspace{-0.2cm}
    \includegraphics[width=0.5\textwidth]{FIG/frame1.pdf}
    \vspace{-10 pt}
    \caption{Illustrations of the functions $\phi^e$ (a) and $\phi^v$ (b).
}\label{fig:frame1}
    \vspace{-10 pt}
\end{wrapfigure}
The function $\phi^e$ is applied to each edge $k$ and outputs the updated edge vector $\mathbf{e}^\prime_k$. The indices of the
input geometries to $\phi^e$ are illustrated in Fig.~\ref{fig:frame1} (a).
Correspondingly, the inputs 
include the old edge vector $\mathbf{e}_k$, the receiver node vector $\mathbf{v}_{r_k}$, the sender node vector $\mathbf{v}_{s_k}$, the set of edges $E_{s_k}$ that point to the node $s_k$ , and the 3D position information for all the nodes connected by the edge $k$ and edges in $E_{s_k}$ with the index set as $r_k \cup s_k\cup \mathcal{N}_{s_k}$. 
The function $\rho^{p\rightarrow e}$ aggregates 3D information from these nodes to update the edge $k$.
The function $\phi^v$ is used for per-node update and generates the new node vector 
$\mathbf{v}^\prime_i$ for each node $i$. An illustration of the indices of the inputs to $\phi^v$ is provided in Fig.~\ref{fig:frame1} (b). The inputs include the old node vector $\mathbf{v}_i$, the set of edges $E_i^\prime$ that point to the node $i$, and 3D information for all the related nodes (the index set is $i\cup \mathcal{N}_i$). 
The functions $\rho^{e\rightarrow v}$ is applied to aggregate the input edge features for updating the node $i$.
The function $\phi^u$ is used to update the global graph feature, while the function $\rho^{v\rightarrow u}$ aggregates information from all the edge features.

The three information update functions $\phi^e$, $\phi^v$, and $\phi^u$
can be implemented in different ways, such as using neural networks and mathematical operations.
In SMP, the 3D information in $P$ is converted and incorporated to
update each message $\mathbf{e}^k $. Hence,
SMP uses $\rho^{p\rightarrow e}$ compute position representations for edges. 
Note that as absolute Cartesian coordinates stored in $P$
are not invariant to translation and rotation,
they are not used as immediate inputs to machine learning models.
The position aggregation function can be flexibly adapted to generate invariant representations. For example, $\rho^{p\rightarrow e}$ in Eq.~(\ref{eq:sp_fw}) can be adapted to a spherical Bessel basis function.

\section{Information Update and Architecture of SphereNet} \label{sec:arch} \label{sec:supp_B}
we assume the message 
$\mathbf{e}^k$ for the edge $k$ is the center message for update.
The input block generates the initial message for the edge $k$, 
and
takes only the distance representation $\Psi(d)$ as the input. 
Each interaction block updates the message for the edge $k$.
The inputs include messages for all the neighboring edges,
and all three representations, including
$\Psi(d, \theta, \varphi)$,
$\Psi(d, \theta)$,
and $\Psi(d)$
based on the edge $k$ and its neighboring edges.
The output block first takes both the distance representation and the
current message for $k$ as inputs.
Then the feature vector of the receiver atom for the edge $k$ (atom $r_k$ in Fig.~\ref{fig:sphere} and Eq.~(\ref{eq:sp_fw})) is obtained by 
aggregating all the messages pointing to it,
where other messages have a similar update process as $\mathbf{e}^k$.

Detailed architecture of SphereNet is provided in Fig.~\ref{fig:arch}.
Specifically, SphereNet is composed of an input block, followed by multiple interaction blocks and output blocks.
For the purpose of simplicity, the architecture is explained by
updating the receiver note $r_k$ of the message $\mathbf{e}_k$,
as described in Eq.~(\ref{eq:sp_fw}) and Sec.~\ref{sec:spherenet}
in main paper.\\
\textbf{Input Block} aims at constructing initial message $\mathbf{e}_k$
for the edge $k$.
Inputs include the distance representation $\Psi(d)$ for edge $k$, initial node embeddings
$\mathbf{v}_{s_k}$, $\mathbf{v}_{r_k}$
for the sender node $s_k$,
and the receiver node $r_k$.
The distance information is encoded by using a LB2 block.\\
\textbf{Interaction Block} updates the message $\mathbf{e}_k$
with incorporating all the three physical representations.
Input 3D information includes the distance embedding $\Psi(d)$,
the angle $\Psi(d, \theta)$,
and the torsion $\Psi(d, \theta, \varphi)$.
The initial embedding sizes for them are $L$,
$N\times L$,
and $N^2\times L$, respectively.
Other inputs are old message $\mathbf{e}_k$ and the set of messages $E_{s_k}$ that
point to the sender node $s_k$.
Similar to the input block,
each type of 3D information is encoded by using a block LB2.
Note that each $\odot$ indicates
the element-wise multiplication between the corresponding 
3D information represented as a vector and each message in the set $E_{s_k}$.
Thus, each neighboring message of $\mathbf{e}_k$ is gated by the encoded 3D information.
The $\sum$ aggregates all the gated messages in $E_{s_k}$ to a vector,
which is added to the transformation of the old message $\mathbf{e}_k$
as the updated message $\mathbf{e}^\prime_k$.
The transformation branch for old message $\mathbf{e}_k$
is composed of several nonlinear layers and residual blocks,
as shown in Fig.~\ref{fig:arch}.\\
\textbf{Output Block} aggregates all the incoming messages to
update the feature for node $r_k$.
Each incoming message has the same update process as $\mathbf{e}_k$
by interaction blocks.
For the purpose of clear illustration, we use $\mathbf{e}^\prime_k$ to represent each updated incoming message, which is further gated by the distance representation vector $\Psi(d)$.



\section{Relations with Prior Message Passing Methods} \label{sec:supp_C}
Our SMP is a specific architecture for 3D graphs
and is formally defined in Eq.~(\ref{eq:sp_fw}).
Especially, the message passing schemes used in some existing models can be viewed as special cases of SMP as they only encode
partial 3D information.
In this section, we clearly give the used functions as well as their inputs for each existing method.
Basically,
we describe how each method realizes Eq.~(\ref{eq:sp_fw}).

\subsection[SchNet,]{SchNet, \cite{schutt2017schnet}}
In SchNet, the used aggregation function to encode 3D position information is  $\rho^{p\rightarrow e}\left(\{\mathbf{r}_h\}_{h=r_k \cup s_k}\right)=\Psi\left(\|\mathbf{r}_{r_k}-\mathbf{r}_{s_k} \|\right)$, which converts the position information to an embedding of distance with radial basis functions. In addition to the $\rho^{p\rightarrow e}$ function, the $\phi^e$ function used is $\text{NN}\left(\text{NN}\left(\mathbf{v}_{r_k}\right)\odot \text{NN}\left(\Psi\left(\|\mathbf{r}_{r_k}-\mathbf{r}_{s_k} \|\right)\right)\right)$, where $\text{NN}$ denotes a neural network and $\odot$ denotes the element-wise multiplication. The $\rho^{e\rightarrow v}$ function is $\sum_{\left(\mathbf{e}^\prime_k, r_k, s_k\right)\in E_{i}^\prime}\mathbf{e}^\prime_k$. The $\phi^v$ function is $\mathbf{v}_i+\sum_{\left(\mathbf{e}^\prime_k, r_k, s_k\right)\in E_{i}^\prime}\mathbf{e}^\prime_k$. The global feature $\mathbf{u}$ is updated based on the final node features $V^T$ and the function is $\phi^u=\sum_{i=1:n}\text{NN}\left(\mathbf{v}^T_i\right)$.
Formally, the update process is expressed as
\begin{equation}\label{eq:schnet}
\begin{aligned}
\mathbf{e}^\prime_k 
=& \phi^e\left( \mathbf{v}_{r_k}, \rho^{p\rightarrow e}\left(\{\mathbf{r}_h\}_{h=r_k \cup s_k}\right)\right)\\
=& \phi^e\left( \mathbf{v}_{r_k}, \Psi\left(\|\mathbf{r}_{s_k}-\mathbf{r}_{r_k} \|\right)\right)\\
=&\text{NN}\left(\text{NN}\left(\mathbf{v}_{r_k}\right)\odot \text{NN}\left(\Psi\left(\|\mathbf{r}_{r_k}-\mathbf{r}_{s_k} \|\right)\right)\right),\\
\mathbf{v}^\prime_i 
=&\phi^v\left(\mathbf{v}_i, \rho^{e\rightarrow v}\left(E_{i}^\prime\right)\right)\\
=&\phi^v\left(\mathbf{v}_i, \sum_{\left(\mathbf{e}^\prime_k, r_k, s_k\right)\in E_{i}^\prime}\mathbf{e}^\prime_k\right)\\
=&\mathbf{v}_i+\sum_{\left(\mathbf{e}^\prime_k, r_k, s_k\right)\in E_{i}^\prime}\mathbf{e}^\prime_k,\\
\mathbf{u} 
=&  \phi^u\left(\rho^{v\rightarrow u}\left(V^T\right)\right)\\
=& \sum_{i=1:n}\text{NN}\left(\mathbf{v}^T_i\right).
\end{aligned}
\end{equation}


\subsection[PhysNet,]{PhysNet, ~\cite{unke2019physnet}}
PhysNet uses distance between atoms as an important feature and proposes more powerful neural networks for chemical applications. The position aggregation function is $\rho^{p\rightarrow e}\left(\{\mathbf{r}_h\}_{h=r_k \cup s_k}\right)=\Psi\left(\|\mathbf{r}_{r_k}-\mathbf{r}_{s_k} \|\right)$, where $\Psi$ is any radial basis function with a smooth cutoff. For the information update functions, the $\phi^e$ function is $\sigma \left(\mathbf{W}_1\right) \sigma \left(\mathbf{v}_{s_k}\right)\odot \mathbf{W}_2 \Psi\left(\|\mathbf{r}_{r_k}-\mathbf{r}_{s_k} \|\right)$, the $\phi^v$ function is $\text{NN}\left(\mathbf{W}_3 \odot \mathbf{v}_{i}+\text{NN}\left( \sigma \left(\mathbf{W}_4\right) \sigma \left(\mathbf{v}_i\right)+\sum_{\left(\mathbf{e}^\prime_k, r_k, s_k\right)\in E_{i}^\prime}\mathbf{e}^\prime_k\right)\right)$ and the $\phi^u$ function is $\mathbf{u} + \sum_{i=1:n}\text{NN}\left(\mathbf{v}^\prime_i\right)$. Here $\text{NN}$ denotes a neural network, $\mathbf{W}_1, \mathbf{W}_2, \mathbf{W}_3, \mathbf{W}_4$ are learnable weight matrices, $\sigma$ is an activate function, and $\odot$ denotes the element-wise multiplication. PhysNet is expressed as
\begin{equation}\label{eq:phynet}
\begin{aligned}
\mathbf{e}^\prime_k 
=& \phi^e\left( \mathbf{v}_{s_k}, \rho^{p\rightarrow e}\left(\{\mathbf{r}_h\}_{h=r_k \cup s_k}\right)\right)\\
=& \phi^e\left( \mathbf{v}_{s_k}, \Psi\left(\|\mathbf{r}_{r_k}-\mathbf{r}_{s_k} \|\right)\right)\\
=&\sigma\left( \mathbf{W}_1\right)\sigma\left(\mathbf{v}_{s_k}\right)\odot \mathbf{W}_2 \Psi\left(\|\mathbf{r}_{r_k}-\mathbf{r}_{s_k} \|\right),\\
\mathbf{v}^\prime_i 
=&\phi^v\left(\mathbf{v}_i, \rho^{e\rightarrow v}\left(E_{i}^\prime\right)\right)\\
=&\phi^v\left(\mathbf{v}_i, \sum_{\left(\mathbf{e}^\prime_k, r_k, s_k\right)\in E_{i}^\prime}\mathbf{e}^\prime_k\right)\\
=&\text{NN}\left(\mathbf{W}_3 \odot \mathbf{v}_{i}+\text{NN}\left( \sigma \left(\mathbf{W}_4\right) \sigma \left(\mathbf{v}_i\right)+\sum_{\left(\mathbf{e}^\prime_k, r_k, s_k\right)\in E_{i}^\prime}\mathbf{e}^\prime_k\right)\right),\\
\mathbf{u}^\prime 
=&  \phi^u\left(\rho^{v\rightarrow u}\left(V^\prime\right), \mathbf{u}\right)\\
=& \mathbf{u} + \sum_{i=1:n}\text{NN}\left(\mathbf{v}^\prime_i\right).
\end{aligned}
\end{equation}


\subsection[DimeNet,]{DimeNet, \cite{klicpera_dimenet_2020}}
DimeNet explicitly considers distances between atoms and directions of directed edges.
The aggregation functions on the position information is $\rho^{p\rightarrow e}=\left(\Psi\left(d\right)\|\Psi\left(d, \theta\right)\right)$, where $\|$ denotes concatenation,
$\Psi\left(d\right)$ and $\Psi\left(d, \theta\right)$ are the same basis functions used in SphereNet
as introduced in Sec.~\ref{sec:spherenet}. Specifically,
$\Psi\left(d\right)$ denotes the representation of the distance based on spherical Bessel function, and $\Psi\left(d, \theta\right)$ denotes the representation of distance and angle based on spherical Bessel function and spherical harmonics. For other functions, the $\phi^e$ function used is $\mathbf{e}^\prime_k=\left(\mathbf{e}^\prime_{k,1}\|\mathbf{e}^\prime_{k,2}\right)$ with $\mathbf{e}^\prime_{k,1}=\text{NN}\left(\mathbf{e}_{k,1}+\text{NN}\left(\sigma\mathbf{W}_1\mathbf{e}_{k,1}+\sum_{\left(\mathbf{e}_j, r_j, s_j\right)\in E_{s_k}}\mathbf{W}_2\Psi\left(d_j, \theta_{jk}\right)\left(\mathbf{W}_3 \Psi\left(d_j\right) \odot \sigma\mathbf{W}_4\mathbf{e}_{j,1}\right)\right)\right)$ and $\mathbf{e}^\prime_{k,2}=\mathbf{W}_5\Psi\left(d_j\right)\odot \mathbf{e}^\prime_{k,1}$, where $\text{NN}$ denotes a neural network, $\mathbf{W}_1, \mathbf{W}_2, \mathbf{W}_3,\mathbf{W}_4, \mathbf{W}_5$ are different weight matrices, and $\sigma$ is an activation function. The $\rho^{e\rightarrow v}$ function is $\sum_{\left(\mathbf{e}^\prime_k, r_k, s_k\right)\in E_{i}^\prime}\mathbf{e}^\prime_{k,2}$ and the $\phi^v$ is $\text{NN}\left(\sum_{\left(\mathbf{e}^\prime_k, r_k, s_k\right)\in E_{i}^\prime}\mathbf{e}^\prime_{k,2}\right)$. The $\rho^{v\rightarrow u}$ is $\sum_{i=1:n}\mathbf{v}^\prime_i$ and the $\phi^u$ is $\mathbf{u} + \sum_{i=1:n}\mathbf{v}^\prime_i$. 
Note that $\rho^{p\rightarrow v}$, $\rho^{p\rightarrow u}$, $\rho^{e\rightarrow u}$ functions are not required in DimeNet. The whole model is expressed as
\begin{equation}\label{eq:dimenet}
\begin{aligned}
\mathbf{e}_k
=&\left(\mathbf{e}_{k,1}\|\mathbf{e}_{k,2}\right),\\
\rho^{p\rightarrow e}
=&\left(\Psi\left(d\right)\|\Psi\left(d, \theta\right)\right),\\
\mathbf{e}^\prime_{k,1} 
=& \phi^e\left( \mathbf{e}_k, E_{s_k}, \rho^{p\rightarrow e}\left(\{\mathbf{r}_h\}_{h=r_k \cup s_k \cup \mathcal{N}_{s_k}}\right)\right)\\
=&\text{NN}\left(\mathbf{e}_{k,1}+\text{NN}\left(\sigma\mathbf{W}_1\mathbf{e}_{k,1}+\sum_{\left(\mathbf{e}_j, r_j, s_j\right)\in E_{s_k}}\mathbf{W}_2\Psi\left(d_j, \theta_{jk}\right)\left(\mathbf{W}_3 \Psi\left(d_j\right) \odot \sigma\mathbf{W}_4\mathbf{e}_{j,1}\right)\right)\right),\\
\mathbf{e}^\prime_{k,2} 
=&\mathbf{W}_5\Psi\left(d_j\right)\odot \mathbf{e}^\prime_{k,1},\\
\mathbf{v}^\prime_i 
=&\phi^v\left(\rho^{e\rightarrow v}\left(E_{i}^\prime\right)\right)\\
=&\text{NN}\left(\sum_{\left(\mathbf{e}^\prime_k, r_k, s_k\right)\in E_{i}^\prime}\mathbf{e}^\prime_{k,2}\right),\\
\mathbf{u}^\prime 
=&  \phi^u\left(\mathbf{u},\rho^{v\rightarrow u}\left(V^\prime\right)\right)\\
=& \mathbf{u} + \sum_{i=1:n}\mathbf{v}^\prime_i.
\end{aligned}
\end{equation}

\begin{figure}[b]
    \centering
    \includegraphics[width=0.9\textwidth]{FIG/filter2.pdf}
    \vspace{-6pt}
    \caption{Visualization of six SphereNet filters.
    Each row corresponds to a filter with torsion angles
    0, $\pi/4$, $\pi/2$, $3\pi/4$, $\pi$, $5\pi/4$, $3\pi/2$,
    and $7\pi/4$ from left to right.
    }\label{fig:filter2}
    \vspace{-10 pt}
\end{figure}

\section{Experimental Setup} \label{sec:supp_D}
For all the models used in three datasets,
we set input embedding size = 256
and output embedding size = 64 for both LB2 and LB blocks.
For each separate model, we first perform warmup on initial learning rate.
Then two learning rate strategies, including ReduceLROnPlateau and StepLR, are used for training. For StepLR, the learning rate is decayed by the decay ratio every fixed epochs represented as step size.
We do not use weight decay or dropout for all models.
Some hyperparameters are fixed values, and 
some are tuned by grid search.
Values/search space of hyperparameters for OC20, QM9, and MD17 are provided in
Table~\ref{tb:hyper_oc20}, Table~\ref{tb:hyper_qm9},
and Table~\ref{tb:hyper_md17}, respectively.
Optimized hyperparameters are tuned on validation sets
and applied to test sets for QM9 and MD17.
For OC20, optimized hyperparameters are obtained on the ID split within max epochs, and then applied to the other three splits.
Pytorch is used to implement all methods.
For QM9 and MD17 datasets, all models are trained using
one NVIDIA GeForce RTX 2080 Ti 11GB GPU.
For the OC20 dataset, all models are trained using
four NVIDIA RTX A6000 48GB GPUs.

\section{OC20 Data Description} \label{sec:oc20_intro}
There exist three tasks including
S2EF, IS2RS,
and IS2RE.
In this work, we focus on IS2RE that
predicts structure's energy in the relaxed state.
It is the most common task in catalysis as relaxed
energies usually influence the catalyst activity.
The dataset for IS2RE is originally split into 
training/validation/test sets.
There are 460,318 structures in the training dataset in total.
The test label is not publicly available.
Performance is evaluated on the validation set,
which has four splits
including In Domain (ID), Out of Domain Adsorbates (OOD Ads),
Out of Domain catalysts (OOD cat), and Out of Domain Adsorbates and catalysts (OOD Both), where numbers of structures are 24,943,
24,961, 24,963, 24,987, respectively.
The average number of atoms per structure is 77.75.


\begin{table}[t]
\renewcommand\arraystretch{1.2}
\begin{center}
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{10pt} \caption{
Values/search space for hyperparameters on OC20.
} \label{tb:hyper_oc20} \scalebox{1}{
\begin{tabular}{l|cccccccccccc}
\bottomrule Hyperparameters &Values/search space\\
\hline 
Interaction block - distance LB2 intermediate size & 8 \\
Interaction block - angle LB2 intermediate size & 8 \\
Interaction block - torsion LB2 intermediate size & 8 \\
\# of interaction blocks & 3, 4 \\
\# of RBFs $N$ & 6\\ 
\# of spherical harmonics $L$  & 3, 5, 7\\
Cutoff distance & 5, 6 \\
Batch size & 16, 32 \\
Initial learning rate & 1e-4, 5e-4, 1e-3\\
Learning rate strategy & ReduceLROnPlateau, StepLR\\
Learning rate decay ratio (for StepLR) & 0.4, 0.5, 0.6\\
Learning rate milestones (for StepLR) & 4, 7, 10, 12, 14\\
Learning rate warmup epochs & 2 \\
Learning rate warmup factor & 0.2 \\
Max \# of Epochs & 20 \\
\bottomrule
\end{tabular}}
\end{center}
\end{table}

\begin{table}[t]
\renewcommand\arraystretch{1.2}
\begin{center}
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{10pt} \caption{
Values/search space for hyperparameters on QM9.
} \label{tb:hyper_qm9} \scalebox{1}{
\begin{tabular}{l|cccccccccccc}
\bottomrule Hyperparameters &Values/search space\\
\hline 
Interaction block - distance LB2 intermediate size & 4, 8, 16 \\
Interaction block - angle LB2 intermediate size & 4, 8, 16 \\
Interaction block - torsion LB2 intermediate size & 4, 8, 16 \\
\# of interaction blocks & 3, 4, 5 \\
\# of RBFs $N$ & 6\\ 
\# of spherical harmonics $L$  & 3, 5, 7\\
Cutoff distance & 4, 5, 6 \\
Batch size & 32, 64 \\
Initial learning rate & 1e-4, 5e-4, 1e-3\\
Learning rate strategy & StepLR\\
Learning rate decay ratio & 0.4, 0.5, 0.6\\
Learning rate step size & 50, 100, 150\\
Max \# of Epochs & 500, 1000 \\
\bottomrule
\end{tabular}}
\end{center}
\end{table}


\begin{table}[t]
\renewcommand\arraystretch{1.2}
\begin{center}
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{10pt} \caption{
Values/search space for hyperparameters on MD17.
} \label{tb:hyper_md17} \scalebox{1}{
\begin{tabular}{l|cccccccccccc}
\bottomrule Hyperparameters &Values/search space\\
\hline 
Interaction block - distance LB2 intermediate size & 4, 8, 16 \\
Interaction block - angle LB2 intermediate size & 4, 8, 16 \\
Interaction block - torsion LB2 intermediate size & 4, 8, 16 \\
\# of interaction blocks & 2, 3, 4, 5 \\
\# of RBFs $N$ & 6\\ 
\# of spherical harmonics $L$  & 3, 5, 7\\
Cutoff distance & 4, 5, 6 \\
Batch size & 1, 2, 4, 16, 32 \\
Initial learning rate & 1e-4, 5e-4, 1e-3\\
Learning rate strategy & StepLR\\
Learning rate decay ratio & 0.4, 0.5, 0.6\\
Learning rate step size & 50, 100, 200\\
Max \# of Epochs & 500, 1000, 2000 \\
\bottomrule
\end{tabular}}
\end{center}
\end{table}

\section{Efficiency Study of SphereNet} \label{sec:efficiency}

\begin{table}[t]
\centering 
\caption{Efficiency comparisons between SphereNet and other models
    in terms of number of parameters and time cost per epoch using the same
    infrastructure.
}\label{tb:effi}
\setlength{\tabcolsep}{0.9mm}
\setlength{\tabcolsep}{1.4mm}
\begin{tabular}{lccccccccc}
\bottomrule  &               SchNet  &   DimeNet & DimeNet++ & GemNet-T
& SphereNet\\
\hline
\#Param. & 185,153&2100,070&1887,110 & 2040,194 & 1898,566 \\
Time (s)  &100 &840&240 &290 &340 \\
\bottomrule
\end{tabular}
\end{table}

We study the efficiency of SphereNet by comparing with other models regarding number of parameters
and time cost per epoch using the same computing 
infrastructure (Nvidia GeForce RTX 2080 TI 11GB).
Experiments are conducted on the property $U_0$ of QM9 and results are shown in Table~\ref{tb:effi}. It is obvious that 
SphereNet uses similar computational resources
as DimeNet++ and GemNet-T, and is much more efficient than DimeNet.
The main reason could be we develop an efficient way
to compute torsion, as introduced
in Sec.~\ref{sec:smp} and Fig.~\ref{fig:sphere} (b).
Moreover, GemNet-Q cannot run on QM9 using the infrastructure as mentioned above.

\newpage

\section{SphereNet Filter Visualization}  \label{sec:supp_E}
We visualize SphereNet filters from a learned SphereNet model.
Specifically, we port learned weights for the block LB2 after the torsion embedding
$\Psi(d, \theta, \varphi)$ in Fig.~\ref{fig:arch}.
For each location represented by a tuple $(d, \theta, \varphi)$, the initial embedding size is
$N^2\times L$.
The computation for the above LB2 is $\mathbf{W}_1\left(\mathbf{W}_2\Psi(d, \theta, \varphi)\right)$,
which results in the new embedding size of 64 for each location $(d, \theta, \varphi)$.
We then perform sampling on locations in 3D space for
visualizing weights as SphereNet filters.
The visualization results are provided in
Fig.~\ref{fig:filter2}. 
We set sampling rate in the torsion direction to be $\pi/4$,
thus, there are eight samples in the torsion direction.
There are totally 64 elements for each location, and we randomly pick 6
elements. Apparently, among the distance, angle and torsion, considering any one when fixing the other two, the structural value of filters will be different when the one of interest changes. It essentially shows that all the distance, angle, and torsion information determine the structural semantics of filters. This further demonstrates that SMP enables the learning of different 3D information for improved representations.
\end{document}
