\section{Results} \label{sec:results}

This section starts by introducing the different metrics that have been
measured during the experiments and the datasets that have been used. Then, the
evaluation is split in two parts: first, using a simulation engine, we evaluate
in detail the behavior of \dfep; then, using the Amazon EC2 cluster, we
evaluate whether \etsch actually improves the computing time of the shortest
path algorithm introduced in Section~\ref{sec:framework}.

\subsection{Metrics}

In the Amazon EC2 cluster, the most important metric is the actual running time
of our algorithm and how does it scale with the number of machines. The simulation
engine, on the other hand, allows us to obtain a better understanding of the
behavior of \dfep; the metrics considered in such case are the following:

\metric{Number of rounds} the number of rounds executed by \dfep to complete
the partitioning. This is a good measure of the amount of synchronization
needed and can be a good indicator of the eventual running time in a real world
scenario.

\metric{Balance} Each partition should be as close as possible to the same 
size. To obtain a measure the balance between the partitions we first normalize 
the sizes, so that a partition of size 1 represent a partition with exactly  edges. We then measure both the size of the largest partition and the standard 
deviation of the sizes, computed as in the following formula.  is the number of 
vertices,  is the number of partitions and  is the size of the -th 
partition):


\metric{Communication cost} As illustrated in Section~\ref{sec:framework}, at
the end of each round all vertices that appear in multiple partitions must
collapse their state to a common value. The amount of messages needed by \etsch
when executed on a specific edge partitioning is computed using the following
formula ( is the set of frontier vertices of partition ).


\metric{Path compression} A good edge-partitioning will also reduce the number
of rounds needed by \etsch to finish its computation. How much will it improve
\etsch performances depends on the specific problem, therefore we chose the
shortest path algorithm presented in Section~\ref{sec:framework} as a
representative. We thus call the \emph{gain} of an edge-partitioning of a graph
the fraction of total iterations avoided by the shortest path algorithm implemented
in \etsch.

\subsection{Datasets}

Since the simulation engine is not able to cope with larger datasets, we used
different datasets for the experiments in the simulation engine and  the
real world experiments. For both types of datasets we list the size of the
graphs, the diameter , the clustering coefficient  and the
clustering coefficient  of a random graph with the same size.

Table~\ref{tab:datasets_sim} contains the characteristics of the four different
datasets used in the simulation engine. \dataset{astroph} is a collaboration
network in the astrophysics field, while \dataset{email-enron} is an email
communication network from Enron. Both datasets are small-world, as shown by
the small diameter. The \dataset{usroads} dataset is a road networking the US, and
thus is a good example of a large diameter network. Finally, \dataset{wordnet} is
a synonym network, with small diameter and very high clustering coefficient.

The three larger graphs that are used to run the Hadoop implementation of both
\dfep and \etsch are presented in Table~\ref{tab:datasets_exp}. \dataset{dblp} is
the co-authorship network from the DBLP archive, \dataset{youtube} is the
friendship graph between the users of the service while \dataset{amazon} is a
co-purchasing network of the products sold by the website.

All the networks have been taken from the SNAP graph library~\cite{Leskovec11}
and cleaned for our use, making directed edges undirected and removing
disconnected components.

\begin{table}
\centering
\caption{Datasets used in the simulation engine \label{tab:datasets_sim}}
\begin{tabular}{|c|ccccc|}
\hline
Name &  &  &  &  & \\
\hline
\dataset{astroph} & 17903 & 196972 & 14 &  & \\
\dataset{email-enron} & 33696 & 180811 & 13 &  &  \\
\dataset{usroads} & 126146 & 161950 & 617 &  & \\ 
\dataset{wordnet} & 75606 & 231622 & 14 &  & \\
\hline
\end{tabular}
\vspace{0.5cm}
\caption{Datasets used on the Amazon EC2 cloud \label{tab:datasets_exp}}
\centering
\begin{tabular}{|c|ccccc|}
\hline
Name &  &  &  &  & \\
\hline
\dataset{dblp} & 317080 & 1049866 & 21 & & 2.09\\
\dataset{youtube} & 1134890 & 2987624 & 20 & &  \\
\dataset{amazon} & 400727 & 2349869 & 18 & & \\ 
\hline
\end{tabular}
\end{table}


\subsection{Simulations}

Figure~\ref{fig:growingK} shows the performance of the two versions of
\dfep against the parameter , in the \dataset{astroph} and
\dataset{usroads} datasets. As expected, the larger the number of partitions, the larger
is the variance between the sizes of those partitions and the amount of
messages that will have to be sent across the network. The rounds needed to
converge to a solution go down with the number of partitions, since it will
take less time for the partitions to cover the entire graph. Finally, the gain
obtained by using \etsch is larger when there are only few partitions, since
the paths are more compressed. This property will emerge also from the
experimental results on the EC2 cloud.

\begin{figure*}[!ht]
\setlength{\belowcaptionskip}{-5pt}
\captionsetup[subfigure]{skip=-12pt}

\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/growingK/maxsize.pdf}
\caption{Size of the largest partition}
\label{fig:growingK-maxsize}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/growingK/stdev.pdf}
\caption{Standard deviation of partition sizes}
\label{fig:growingK-stdev}
\end{subfigure}

\vspace{-5pt}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/growingK/mess.pdf}
\caption{Communication cost}
\label{fig:growingK-mess}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/growingK/adv.pdf}
\caption{Gain on iterations}
\label{fig:growingK-adv}
\end{subfigure}

\vspace{-5pt}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/growingK/iter.pdf}
\caption{Rounds needed by \dfep to converge}
\label{fig:growingK-iter}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/growingK/conn.pdf}
\caption{Percentage of disconnected partitions}
\label{fig:growingK-conn}
\end{subfigure}

\caption{Behavior of \dfep and \dfepc with different values of  ( samples)}
\label{fig:growingK}
\end{figure*}

The diameter of a graph is a strong indicator of how our proposed approach will 
behave. To test \dfep on graphs with similar characteristics but different diameter 
we followed a specific protocol: starting from the \dataset{usroads} dataset (a graph with 
a very large diameter) we remapped random edges, thus decreasing the diameter. The
remapping has been performed in such a way to keep the number of triangles as close
as possible to the original graph. 

Figure~\ref{fig:diameter} shows that changing the diameter leads to completely
different behaviors. The size of the largest partitions and the standard
deviation of partitions size rise steeply with the growth of the diameter,
since in a graph with higher diameter the starting vertices chosen by our
algorithm affect more deeply the quality of the partitioning. As expected, the
number of rounds needed also rise linearly with the diameter, as does the gain
of \etsch. While the partitions may be less balanced, they will be more
interconnected and thus the amount of messages sent across the network will
decrease steeply.

\begin{figure*}
\setlength{\belowcaptionskip}{-5pt}

\begin{center}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/diameter/maxsize.pdf}
\caption{Relative size of largest partition}
\label{}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/diameter/adv.pdf}
\caption{Gain on iterations by \etsch}
\label{}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/diameter/mess.pdf}
\caption{Communication cost}
\label{}
\end{subfigure}
\end{center}


\vspace{-10pt}
\begin{center}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/diameter/iter.pdf}
\caption{Rounds needed by \dfep to converge}
\label{}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/diameter/conn.pdf}
\caption{Percentage of disconnected partitions}
\label{}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/diameter/stdev.pdf}
\caption{Standard deviation of partition sizes}
\label{}
\end{subfigure}
\end{center}

\caption{Behavior of \dfep with graphs of same size but different diameter (,  samples)}
\label{fig:diameter}
\end{figure*}

Finally, we compare the 
two version of \dfep against the JaBeJa\cite{Rahimian2013} algorithm. Since JaBeJa is a vertex-
partitioning algorithm, its output must be converted into an edge-partitioning. 
Two approaches have been considered: running the algorithm directly on the line 
graph of the input graph, creating a vertex for each edge in the original graph, or 
assigning each edge to a partition by following the vertex-partitioning and 
assigning each cut edge randomly to one of the two neighboring partitions. Since 
the line graph can be orders of magnitude bigger than the original graph we followed the second approach.

Figure \ref{fig:comparison}
shows the experimental results over 100 samples, on the four different datasets.
A pattern can be discerned: the algorithms have wildly different behaviors in the 
small world dataset than in the road network. In the small world datasets our 
approaches results in more balanced partitions, while keeping the gain similar to 
JaBeJa. In the \dataset{usroads} dataset JaBeJa creates more balanced
partitions, but the gain is much lower and, more importantly, the amount of
messages that have to been sent is roughly ten times higher. This result shows
the importance of creating partitions that are as much connected as possible.

Since JaBeJa uses simulated annealing to improve the candidate solution, the
number of round needed is mostly independent from the structure of the graph.
As shown in Figure \ref{fig:diameter} the number of rounds \dfep needs depend mostly
from the diameter of the graph.



\subsection{Amazon EC2}

Both \dfep and \etsch have been also implemented in Apache Hadoop, in the
MapReduce model and tested over the Amazon EC2 cloud. All the experiments have
been repeated  times on \emph{m1.medium} machines initiated by the Apache
Whirr tool, using the version 1.2.1 of Hadoop.

In the \dfep implementation the  edges from which the partitions should start are 
chosen using a simple selection algorithm: each edge computes a random number in 
the Map phase and through first the usage of Combiners and finally of a single 
Reducer the  edges that chose the smallest  numbers are selected and assigned to 
a single partition each. 
It was not possible to implement \dfep using a single Map-Reduce round for 
each iteration while keeping exactly the same structure. Each instance of the Map 
function is executed on a single vertex, which will output messages to its 
neighbor and a copy of itself. Each instance of the Reduce function will receive 
a vertex and all the funding sent by the neighbors on common edges. The part of 
the algorithm that should be executed on each edge is instead executed by both its 
neighboring vertices, with special care to make sure that both executions will get 
the same results to avoid inconsistencies in the graph. This choice, which sounds 
counterintuitive, allows us to use a single Map-Reduce round for each iteration of the algorithm, 
thus decreasing the communication and sorting costs inherent in the MapReduce model.

Figure~\ref{fig:dfep_amazon} presents the scalability results for the \dfep
algorithm, when run with the three different datasets that are listed in
Table~\ref{tab:datasets_exp}, with . The algorithm scales with the number
of computing nodes, with a speedup larger than  with  nodes instead of .

\begin{figure*}
\setlength{\belowcaptionskip}{-5pt}

\begin{center}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/comparison/maxsize.pdf}
\caption{Relative size of largest partition}
\label{}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/comparison/adv.pdf}
\caption{Gain on iterations by \etsch}
\label{}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/comparison/mess.pdf}
\caption{Communication cost}
\label{}
\end{subfigure}
\end{center}


\vspace{-8pt}
\begin{center}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/comparison/iter.pdf}
\caption{Rounds needed to converge}
\label{}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/comparison/conn.pdf}
\caption{Percentage of disconnected partitions}
\label{}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=\textwidth]{figures/comparison/stdev.pdf}
\caption{Standard deviation of partition sizes}
\label{}
\end{subfigure}
\end{center}


\caption{Comparison between the two versions of \dfep and JaBeJa (,  samples)}
\label{fig:comparison}
\end{figure*}

To test the practical advantages of \etsch we prepared a Hadoop implementation
of the framework in which the user can define the three functions as defined in
Section~\ref{sec:framework}. We used the edge-partitioning obtained by running
\dfep, setting the number of desired partitions equal to the number of
available nodes, and run the \etsch implementation of the shortest path
algorithm. We compare this approach against running our baseline vertex-based
implementation of the shortest path algorithm on the unpartitioned graph.
Figure~\ref{fig:comp_amazon} shows that our approach is much more efficient
when the number of processing nodes is small, since the partitions are larger
and paths are more easily compressed. When the number of partitions grows, the
baseline approach gets closer to \etsch, but still seems less efficient.

While the baseline implementation could easily be optimized, the same can be
said about our implementation of the \etsch framework.
The experimental results thus show that \etsch and edge-partitioning is a 
promising approach.



