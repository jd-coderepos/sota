Large language models (LLMs) demonstrate strong reasoning capabilities across various domains, including dialogue \cite{glaese2022improving,thoppilan2022lamda}, step-by-step reasoning \cite{wei2022chain,kojima2022large}, math problem solving \cite{lewkowycz2022solving,polu2022formal}, and code writing \cite{chen2021evaluating}.
However, a limitation of such models for inference in the real world is the issue of
grounding: while training LLMs on massive textual
data may lead to representations that relate to our
physical world, \emph{connecting} those representations \emph{to} real-world visual
and physical sensor modalities is essential to solving a wider range of \emph{grounded} real-world
problems in computer vision and robotics \cite{tellex2020robots}.
Previous work \citep{ahn2022can} interfaces the output of LLMs with learned robotic
policies and affordance functions to make decisions,
but is limited in that the LLM itself is only provided with textual
input, which is insufficient for many tasks where the geometric configuration of the scene is important.
Further, in our experiments we show that current state-of-the-art \emph{visual}-language models trained on typical vision-language tasks such as visual-question-answering (VQA) cannot directly solve robotic reasoning tasks.

In this paper we propose embodied language models, which
directly incorporate continuous inputs from sensor modalities of an
embodied agent and thereby enable the language model \emph{itself} to
make more grounded inferences for sequential decision making in the real world.
Inputs such as images and state estimates are embedded into the same latent embedding
as language tokens and processed by the self-attention layers of a
Transformer-based LLM in the same way as text.
We start from a pre-trained LLM in which we inject the continuous inputs through an encoder.
These encoders are trained end-to-end to output sequential decisions in terms of natural text that can be interpreted by the embodied agent by conditioning low-level policies or give an answer to an embodied question.
We evaluate the approach in a variety of settings, comparing different input representations (e.g.\ standard vs.\ object-centric ViT encodings for visual input), freezing vs.\ finetuning the language model while training the encoders, and investigating whether co-training on multiple tasks enables transfer.

To investigate the approach's breadth, we evaluate on three robotic manipulation domains (two of which are closed-loop in the real-world), standard visual-language tasks such as VQA and image captioning, as well as language tasks.
Our results indicate that multi-task training improves performance compared to training models on individual tasks.
We show that this \emph{transfer} across tasks can lead to high data-efficiency for robotics tasks, e.g.\ significantly increasing learning success from handfuls of training examples, and even demonstrating one-shot or zero-shot generalization to novel combinations of objects or unseen objects.

We scale PaLM-E up to 562B parameters, integrating the 540B PaLM \cite{chowdhery2022palm} LLM and the 22B Vision Transformer (ViT) \cite{dehghani2023scaling} into, to our knowledge, the largest vision-language model currently reported.
PaLM-E-562B achieves state-of-the-art performance on the OK-VQA \cite{okvqa} benchmark, without relying on task-specific finetuning. Although not the focus of our experimentation, we also find (Fig.~\ref{fig:figure2}) that PaLM-E-562B exhibits a wide array of capabilities including zero-shot multimodal chain-of-thought (CoT) reasoning, few-shot prompting, OCR-free math reasoning, and multi-image reasoning, despite being trained on only single-image examples. Zero-shot CoT \cite{kojima2022large}, originally a language-only concept, has been shown on {\em{multimodal}} data with task-specific programs \cite{zeng2022socratic} but to our knowledge, not via an end-to-end model.

To summarize our main contributions, we (1) propose and demonstrate that a generalist, transfer-learned, multi-embodiment decision-making agent can be trained via mixing in embodied data into the training of a multimodal large language model. We show that, (2) while current state-of-the-art general-purpose visual-language models out-of-the-box (zero-shot) do not well address embodied reasoning problems, it is possible to train a competent general-purpose visual-language model that is also an efficient embodied reasoner.  In studying how to best train such models, we (3) introduce novel architectural ideas such as neural scene representations and entity-labeling multimodal tokens.  Finally, in addition to our focus on PaLM-E as an embodied reasoner we (4) show that PaLM-E is also a quantitatively competent vision and language generalist, and (5) demonstrate that scaling the language model size enables multimodal finetuning with less catastrophic forgetting.


