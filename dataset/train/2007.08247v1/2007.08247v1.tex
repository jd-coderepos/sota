
Our goal is to learn a representation that maximizes the MI, denoted as ,
between different views of the input. These views are generated using various orderings,
capturing different aspects  of the inputs.
Formally, let  be an unlabeled data point, and
 be a deep representation to be learned as a mapping
between the inputs and the outputs.
For clustering,  is the set of possible
clusters corresponding to semantic 
classes, and for representation learning,  corresponds to a lower-dimensional
space of the output features. Let  be
two orderings  and  obtained from the set of possible and valid 
orderings  (\cref{fig:raster_orderings}).
For two outputs  and ,
the objective is to maximize the predictability of  from
 and vice-versa, where  corresponds to applying the learning function 
with a given ordering  to process the image .
This objective is equivalent to maximizing the MI
between the two encoded variables:


We start by presenting different forms of masked convolutions
to generate various raster-scan orderings,
and propose an attention augmented variant
for a larger receptive field and use it to extend
the set of possible orderings (\cref{orderings}).
We then formulate the training objective for maximizing \cref{eq:mainobj}
for both clustering and unsupervised representation learning (\cref{obj}).
We finally conclude with a flexible design architecture
for the function  (\cref{mdl}).






\subsection{Orderings} \label{orderings}

\subsubsection{Masked Convolutions}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/ranster_scan_oderings.pdf}
\vspace{-0.3in}
\caption{\textbf{Raster-scan type orderings.}}
\label{fig:raster_orderings}
\vspace{-0.2in}
\end{figure}


In neural autoregressive modeling \cite{pcnn,pcnnpp,psnail}, for an input image 
with  color channels, a raster-scan
ordering is first imposed on the image (see \cref{fig:raster_orderings}, ordering ).
Such an ordering, where the pixel 
only depends on the pixels that come before it,
is maintained using masked convolutions\footnote{
Note that for a convolution weight tensor of shape , the masking in applied over all
values of both channels,  and .}
\cite{pcnn,prnn} (\cref{fig:convolutions} (a)).

Our proposition is to use all 8 possible raster-scan type orderings as the set
of valid orderings  as illustrated in \cref{fig:raster_orderings}.
A simple way to obtain them is to use a single ordering  with the
standard masked convolution (\cref{fig:convolutions} (a)),
along with geometric transformations  (\ie, image rotations by
multiples of 90 degrees and horizontal flips), resulting in 8 versions of the input image.
We can then maximize the MI between the
two outputs, \ie,  with .
In this case, since the masked weights are never trained, 
we cannot fall-back to the normal convolution
where the function  has access to the full input during inference,
greatly limiting the performance of such approach.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/convolutions.pdf}
\vspace{-0.2in}
\caption{\textbf{Masked Convolutions.} (a) Standard masked convolution used in autoregressive
generative modeling, yielding an ordering . (b) A relaxed version of standard masked convolution
where we have access to the current pixel at each step.
(c) A simplified version of masked convolution with a reduced number  of masked weights.
(d) The 8 versions of the standard masked convolution to construct all of the possible
raster-scan type orderings.
(e) The proposed types of masked convolutions with
the corresponding shifts to obtain all of the 8 desired raster-scan types orderings.
 in this case.}
\label{fig:convolutions}
\vspace{-0.2in}
\end{figure}

This point motivates our approach.
Our objective is to learn all the weights
of the masked convolution during training, and use
an unmasked version during inference.
This can be achieved by using a normal convolution,
and for a given ordering ,
we mask the corresponding weights during the forward
pass to construct the desired view of the inputs.
Then in the backward pass, we only
update the unmasked weights and the masked weights remain unchanged.
In this case, all of the weights will be learned and we will converge to
a normal convolution given enough training iterations.
During inference, no masking is applied, giving
the function  full access to the inputs.

A straight forward way to implement this is to use 8 versions of the standard masked
convolution to create the set  (\cref{fig:convolutions} (d)).
However, for each forward pass,
the majority of the weights are masked, resulting in a reduced receptive field and a fewer number of weights will be learned at each iteration,
leading to some disparity between them. 

Given that we are interested in a discriminative task, rather than
generative image modeling where the access to the current pixel is not allowed.
We start by relaxing the conditional dependency, and allow the model to have
access to the current pixel, reducing the number of masked locations
by one (\cref{fig:convolutions} (b)).
To further reduce the number of masked weights,
for an  convolution, instead of 
masking the lower rows,
we can simply shift the input by the same amount and only
mask the weights of the last row.
We thus reduce the number of masked weight from
 (\cref{fig:convolutions} (b))
to  (\cref{fig:convolutions} (c)).
With four possible masked convolutions: 
and four possible shifts:\footnote{\eg, for  and a  convolution,
an image of spatial dimensions  is first padded on the top resulting in ,
the last row is then cropped, going back to .} ,
we can create all of 8 raster-scan orderings as illustrated in \cref{fig:convolutions} (e).
The proposed masked convolutions do not introduce any additional computational overhead, neither in training,
nor inference, making them easy to implement and integrate into existing architectures with minor changes.







\vspace{-0.1in}
\subsubsection{Attention Augmented Masked Convolutions} \label{acc}
As pointed out by \cite{pcnn}, the proposed masked convolutions
are limited in terms of expressiveness since they create
blind spots in the receptive field (\cref{fig:blind_spots}).
In our case, by applying different orderings, we will have access to all of the input  over
the course of training, and
this \textit{bug} can be seen as a \textit{feature} where the blind spots can be considered as an additional restriction.
This restricted receptive filed, however, can be overcome using the self-attention mechanism \cite{attalluneed}.
Similar to previous works \cite{nonlocal,atgan,attaugmentedconv},
we propose to add attention blocks to model long range dependencies that are hard to access
through standalone convolutions.
Given an input tensor of shape , after reshaping it into a matrix ,
we can apply a masked version of attention \cite{attalluneed} in a straight forward manner.
The output of the attention operation is:

with ,  and , where 
and 
are learned linear transformations that map the input  to queries , keys  and values , and
 corresponds to a masking operation
to maintain the correct ordering .

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/zigzag_orderings.pdf}
\vspace{-0.3in}
\caption{\textbf{Zigzag type orderings.}}
\label{fig:zigzag_orderings}
\vspace{-0.2in}
\end{figure}

The output is then projected into the output space using 
a learned linear transformation  obtaining .
The output of the attention operation  is concatenated channel wise
with the input , and then merged using a  convolution
resulting in the output of the attention block.




\vspace{-0.1in}
\subsubsection{Zigzag Orderings.} \label{zo}
Using attention gives us another benefit, we can extend the set of possible orderings to include
zigzag type orderings introduced in \cite{psnail} (\cref{fig:zigzag_orderings}).
With zigzag orderings, the outputs at each spatial location will be mostly influenced
by the values of the corresponding
neighboring input pixels, which can give rise to more semantically meaningful representations compared
to that of raster-scan orderings. This is done by simply 
using a mask  corresponding to the desired zigzag ordering .
Resulting in a set  of 16 possible and valid orderings  with  in total.
See \cref{fig:att_masks} for an example.


\begin{figure}
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/masks.pdf}
        \caption{\textbf{Attention Masks.} Examples of the different
          attention masks  of shape  applied for a given ordering . With .}
        \label{fig:att_masks}
    \end{minipage}\qquad
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/blind_spots.pdf}
        \caption{\textbf{Blind Spots.} Blind spots in the receptive field of
        pixel \crule[orange]{0.2cm}{0.2cm} as a result of
        using a masked convolution for a given ordering .}
        \label{fig:blind_spots}
    \end{minipage}
\vspace{-0.1in}
\end{figure}


















\subsection{Training Objective} \label{obj}
In information theory, the MI  between two random variables  and 
measures the \textit{amount of information} learned from the knowledge of  
about  and vice-versa. The MI can be expressed as the difference of two entropy terms:

Intuitively,  can be seen as the reduction of uncertainty in one of the 
variables, when the other one is observed. If  and  are independent,
knowing one variable exposes nothing about the other, in this case, .
Inversely, if the state of one variable is deterministic when the state of the other is revealed,
the MI is maximized. Such an interpretation explains the goal behind maximizing
\cref{eq:mainobj}.
The neural network  must be able to preserve information
and extract semantically similar representations regardless of the applied ordering ,
and learn representations that encode the underlying shared information between the different views.
The objective can also be interpreted as having a regularization effect,
forcing the function  to focus 
on the different views and subparts of the input  to produce similar outputs,
reducing the reliance on specific objects or parts of the image.

Let  be the joint distribution produced by
sampling examples  and
then sampling two outputs  and 
with two possible orderings  and .
In this case, the MI in \cref{eq:mainobj} can be defined as the Kullbackâ€“Leibler (KL) divergence between
the joint and the product of the marginals:


To maximize \cref{eq:mikl}, we can either maximize the exact MI for a clustering task over discrete predictions,
or a lower bound for an unsupervised learning of representations over the continuous outputs.
We will now formulate the loss functions  and 
of both objectives for a segmentation task.













\subsubsection{Autoregressive clustering (AC).} 
In a clustering task, the goal is to train a neural network  to predict a
cluster assignment corresponding to a given semantic class  with  possible clusters at
each spatial location. 
In this case, the encoder-decoder type 
network  is terminated with -way softmax, outputting
 of the same spatial dimensions as the input.
Concretely, for a given input image  and two valid orderings ,
we forward pass the input through the network
producing two output probability distributions  and
 over the  clusters and at
each spatial location.
After reshaping the outputs into two matrices of shape , with each element
corresponding to the probability of assigning pixel  with  to cluster ,
we can compute the joint distribution  of shape  as follows:

The marginals  and  can then be obtained by
summing over the rows and columns of .
Similar to IIC \cite{IIC}, we symmetrize  using
 to
maximize the MI in both directions.
The clustering loss  in this case can be written as follows:


In practice, instead of only maximizing the MI between two corresponding spatial locations,
we maximize it between each spatial location and its intermediate neighbors over small
displacements  (see \cref{fig:overview}). This can be efficiently implemented
using a convolution operation as demonstrated in \cite{IIC}.







\subsubsection{Autoregressive representation learning (ARL).} \label{arl}
Although the clustering objective in \cref{eq:Lac} can also
be used as a pre-training objective for , Tschannen \etal
\cite{onMImaximization} recently showed  that maximizing 
the MI does not often results in transferable and semantically meaningful features, especially when
the down-stream task is a priori unknown.
To this end, we follow recent representation learning works based on MI maximization
\cite{cpc,infomax,amdim,CMC}, where a lower bound estimate of MI
(\eg, InfoNCE \cite{cpc}, NWJ \cite{NWJ}) is maximized between different views
of the inputs. These estimates are based on the simple intuitive idea, that
if a critic  is able to differentiate between samples drawn from
the joint distribution  and samples drawn from the marginals ,
then the true MI is maximized.
We refer the reader to \cite{onMImaximization} for a detailed discussion.

In our case, with image segmentation as the target down-stream task, 
we maximize the InfoNCE estimator \cite{cpc} over the continuous outputs.
Specifically, with two outputs
 as -dimensional feature maps.
The training objective is to maximize the infoNCE
based loss :



For an input image 
and two outputs  and . Let 
 and  correspond to -dimensional
feature vectors at spatial positions  and
 in the first and second outputs respectively.
We start by creating  pairs of feature vectors ,
with one positive pair drawn from the joint distribution and  negative pairs drawn from the marginals.
A positive pair is a pair of feature vectors corresponding to the same
spatial locations in the two outputs,
\ie, a pair  with . The negatives 
are pairs  corresponding to two distinct
spatial positions .
In practice, we also consider small displacements  (\cref{fig:overview}) when constructing positives.
Additionally, the negatives are generated from two distinct images, since
two feature vectors might share similar characteristics even with different spatial positions.
By maximizing \cref{eq:Larl}, we push the model  to produce similar representations for the same
spatial location regardless of the applied ordering, so that the critic 
function  is able to give high matching scores to the positive pairs and low matching to the negatives.
We follow \cite{infomax} and use separable
critics , where the functions 
 non-linearly transform the outputs to a higher vector space, and 
 produces a scalar corresponding to a matching score between
the two representations at two spatial positions  and  of the two outputs.

Note that both losses  and  can be applied
interchangeably for both objectives, a case we investigate in our experiments (\cref{sec:ablations}).
For , we can consider the clustering objective as an
intermediate task for learning useful representations.
For , during inference, K-means \cite{JDH17} algorithm
can be applied over the outputs to obtain the cluster assignments.



































\subsection{Model} \label{mdl}
The representation  can be implemented in a general manner using three sub-parts, 
\ie, , with a feature extractor , an autoregressive encoder
 and a decoder . With such a formulation, the function  is flexible and
can take different forms. With  as an identity mapping,  becomes a fully autoregressive
network, where we apply different orderings directly over
the inputs. Inversely, if 
is an identity mapping,  becomes a generic encoder-decoder network,
where  plays the role of 
an encoder.
Additionally,  can be a simple convolutional stem that plays
an important role in learning local features such as edges, or even multiple residual blocks \cite{resnet}
to extract higher representations. In this case, the orderings are applied over 
the hidden features using .  is similar to ,
containing a series of residual blocks,
with two main differences, the proposed masked convolutions are used,
and the batch normalization \cite{batchnorm}
layers are omitted to maintain the autoregressive dependency,
with an optional attention block.
The decoder  can be a simple  to adapt the channels
to the number of cluster , followed by bilinear upsampling and
a softmax operation for a clustering objective.
For representation learning,  consists of two separable critics ,
which are implemented as a series of 
and  for projecting to a higher dimensional space. See sup. mat. for
the architectural details. In the experimental section, we will investigate different versions of .

