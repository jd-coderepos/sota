\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{rotating}

\usepackage{amssymb,amsmath}
\usepackage{graphicx, epsfig}

\def\id#1{\ensuremath{\mathit{#1}}}
\let\idit=\id
\def\idbf#1{\ensuremath{\mathbf{#1}}}
\def\idrm#1{\ensuremath{\mathrm{#1}}}
\def\idtt#1{\ensuremath{\mathtt{#1}}}
\def\idsf#1{\ensuremath{\mathsf{#1}}}
\def\idcal#1{\ensuremath{\mathcal{#1}}}  
\def\floor#1{\lfloor #1 \rfloor}
\def\ceil#1{\lceil #1 \rceil}
\def\etal{\emph{et~al.}}

\newcommand{\no}[1]{}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}
\newtheorem{problem}{Problem}
\newenvironment{proof}{\trivlist\item[]\emph{Proof}:}{\unskip\nobreak\hskip 1em plus 1fil\nobreak
\parfillskip=0pt\endtrivlist}

\newcommand{\cL}{{\cal L}}
\newcommand{\cT}{{\cal T}}
\newcommand{\cP}{{ K}}
\newcommand{\cD}{{\cal D}}
\newcommand{\cM}{{\cal M}}
\newcommand{\cC}{{\cal C}}
\newcommand{\cF}{{\cal F}}
\newcommand{\cQ}{{\cal Q}}
\newcommand{\cN}{{\cal N}}
\newcommand{\cS}{{\cal S}}
\newcommand{\oS}{\overline{S}}
\newcommand{\tS}{\widetilde{S}}
\newcommand{\tI}{\widetilde{I}}
\newcommand{\tD}{\widetilde{D}}
\newcommand{\oL}{\overline{L}}
\newcommand{\oB}{\overline{B}}
\newcommand{\oP}{\overline{{ K}}}
\newcommand{\oi}{\overline{i}}
\newcommand{\osigma}{\overline{\sigma}}


\newcommand{\todo}[1]{ }



\newcommand{\ra}{\idrm{rank}}
\newcommand{\sel}{\idrm{select}}
\newcommand{\acc}{\idrm{access}}
\newcommand{\offset}{\idtt{offset}}
\newcommand{\soffset}{\idtt{soffset}}
\newcommand{\doffset}{\idtt{doffset}}
\newcommand{\sdoffset}{\idtt{sdoffset}}
\newcommand{\lleft}{\idtt{left}}
\newcommand{\dleft}{\idtt{dleft}}

\newcommand{\eps}{\varepsilon}

\newcommand{\unbounded}[1]{#1}

\begin{document}

\title{\Large Optimal Dynamic Sequence Representations
\thanks{Partially funded by Fondecyt grant 1-110066, Chile.
An early partial version of this paper appeared in {\em Proc. SODA'13}.}}

\author{
Gonzalo Navarro\thanks{Department of Computer Science, University of Chile.
{\tt  gnavarro@dcc.uchile.cl}.}
\and
Yakov Nekrich\thanks{Department of Electrical Engineering and Computer Science,
University of Kansas.
{\tt yakov.nekrich@googlemail.com}.}
}
\date{}

\maketitle

\begin{abstract} 
We describe a data structure that supports access, rank and select queries,
as well as symbol insertions and deletions, on a string  over alphabet 
 in time , which is optimal even on binary
sequences and in the amortized sense. Our time is worst-case for the queries 
and amortized for the updates. This complexity is better than the best previous
ones by a  factor.
We also design a variant where times are worst-case, yet rank and updates
take  time.
Our structure uses 
bits, where  is the zero-order entropy of .
\no{Finally, we pursue various extensions, like handling general alphabets
(such as reals and strings) and supporting a more general algebra of string
operations including concatenations, splits, and block edits.}
Finally, we pursue various extensions and applications of the
result.
\end{abstract}

\section{Introduction}
\label{sec:intro}

String representations supporting  and  queries are fundamental
in many data structures, including 
full-text indexes \cite{GGV03,FMMN07,GMR06}, 
permutations \cite{GMR06,BGNN10}, 
inverted indexes \cite{BFLN08,BGNN10}, 
graphs \cite{CN10},
document retrieval indexes \cite{VM07}, 
labeled trees \cite{GMR06,BHMR07}, 
XML indexes \cite{GHSV07,FLMM09},
binary relations \cite{BHMR07}, 
and many more. 
The problem is to encode a string  over alphabet  
so as to support the following queries:


There exist various {\em static} representations of  (i.e.,  cannot change) that support these 
operations \cite{GGV03,GMR06,FMMN07,BGNN10,BN12}. The most recent one
\cite{BN12} shows a lower bound of  time
for operation  on a RAM machine with -bit words, using any
 space. It also provides a matching upper bound that in addition 
achieves almost constant time for  and , using compressed space. 
Thus the problem is essentially closed.

However, various applications need dynamism, that is, the ability to insert and 
delete symbols in . A lower bound for this case, in order to support
operations , insertions and deletions, even for bitmaps () and
in the amortized sense, is  \cite{FS89}. On the other
hand the best known upper bound \cite{HM10,NS10} is 
, that is, a factor 
 away from the lower bound for
alphabets larger than polylogarithmic. Their space is ,
where 
is the zero-order entropy of , where  is the number of occurrences of 
 in . 

In this paper we close this gap by providing an {\em optimal-time} dynamic
representation of sequences. Our representation takes 
time for all the operations, worst-case for the three queries and amortized
for insertions and deletions. We present a second variant achieving worst-case
bounds for all the operations,  for  and 
and  for , insertions and deletions. The space is also
. This  is still faster than previous
work for . This gets much closer to
closing this problem under the dynamic scenario as well.

We then 
\no{consider some extensions of our results. First, we }
show how to 
handle general alphabets, such as , or 
 for a symbol alphabet , in optimal time. 
For example, in the comparison model for ,
the time is , where  is the number of 
distinct symbols that appear in ; in the case  for 
general , the time is , where
 is the length of the involved symbol (a string) and  the number of distinct 
symbols of  that appear in the elements of .
Previous dynamic solutions have assumed that the alphabet
 was static.

\no{ -> needs quadratic blocks, at best, but how to handle the split-find
and partial sums?
As a second extension, we enhance the set of update operations to include
sequence splitting, concatenating, and general block updates. This makes up
a much stronger algebra on sequences. The time for these operations is
, whereas the only previous solution \cite{NS10} achieved
 for any constant .
}

At the end we describe several applications where our result offers improved
time/space tradeoffs. These include compressed indexes for dynamic text 
collections, construction of the Burrows-Wheeler transform \cite{BW94} and 
static compressed text indexes within compressed space, as well as compressed
representations of dynamic binary relations, directed graphs, and inverted
indexes.
 
We start with an overview of the state of the art, putting our solution in 
context, in Section~\ref{sec:related}. We review the wavelet tree data 
structure \cite{GGV03}, which is at the core of our solution (and most
previous ones) in Section~\ref{sec:wavel}. In Section~\ref{sec:basic} we 
describe the core of
our amortized solution, deferring to Section~\ref{sec:rank} the management of 
deletions and its relation with a split-find data structure needed for rank
and inserts. Section~\ref{sec:space} deals with the changes in  and
how we obtain times independent of , and concludes
with Theorem~\ref{thm:optimal}, our result on uncompressed sequences.
Then Section~\ref{sec:rrr} shows how to improve the data encoding to obtain
compressed space in Theorem~\ref{thm:compr}, and Section~\ref{sec:worst} 
how to obtain worst-case times, Theorem~\ref{thm:worstcase}.
\no{Finally, Section~\ref{sec:ext} describes some extensions of our results and
Section~\ref{sec:app} describes various applications. }
Finally, Section~\ref{sec:app} describes some extensions and several applications of our results.
We conclude in
Section~\ref{sec:concl}. An important technical part of the paper, describing
the structure of blocks that handle subsequences of polylogarithmic size, is
deferred to Section~\ref{app:A} to avoid distractions.

\section{Related Work}
\label{sec:related}

With one exception \cite{GHSV07}, all the previous work on dynamic sequences
build on the {\em wavelet tree} structure \cite{GGV03}.
The wavelet tree decomposes  hierarchically. In a first level, it separates
larger from smaller symbols, by marking in a bitmap which symbols of  were
larger and which were smaller. The two subsequences of  are recursively
separated. The  levels of bitmaps describe , and , 
and  operations on  are carried out via   and
 operations on the bitmaps (see Section~\ref{sec:wavel} for more details).

In the static case,  and  operations on bitmaps take constant time,
and therefore ,  and  on  takes  time
\cite{GGV03}. This can be reduced to  by using
multiary wavelet trees \cite{FMMN07}. These separate the symbols into
 ranges, and instead of a bitmap store a sequence over an 
alphabet of size . 

Insertions and deletions in  can also be carried out by inserting and 
deleting bits from  bitmaps. However, the operations on dynamic 
bitmaps are bound to be slower. Fredman and Saks \cite{FS89} show that 
 time is necessary, even in the amortized sense, to 
support , insert and delete operations on a bitmap. By using dynamic 
bitmap solutions \cite{HSS03,CHL04,BB04,CHLS07,HSS11} on the wavelet tree
levels, one immediately obtains a dynamic sequence representation, 
where the space and the time of the dynamic bitmaps
solution is multiplied by  (the sum of the zero-order entropies of
the bitmaps also adds up to  \cite{GGV03}). With this combination one
can obtain times as good as  (and  bits) \cite{CHLS07} and spaces as good as  
(and  time) \cite{BB04}.

M\"akinen and Navarro \cite{MN06,MN08} made the above combination explicit
and obtained compressed bitmaps with logarithmic-time operations, yielding
 time for all the sequence operations and the best
compressed space until then,  bits. 
They also obtained  query time, but with
an update time of , for any constant .
This was achieved by replacing binary with multiary wavelet trees, and
obtaining  query time for the operations on sequences over a small
alphabet of size .

Lee and Park \cite{LP07,LP09} pursued this path further, obtaining the
 time for queries and update
operations, yet the space was not compressed, ,
and update times were amortized.
Shortly after, Gonz\'alez and Navarro \cite{GN08,GN09} obtained the best
of both worlds, making all the times worst-case and compressing the space
again to  bits. Both solutions managed to solve all
query and update operations in  time on sequences over small
alphabets of size .

Finally, almost simultaneously, He and Munro \cite{HM10} and Navarro and 
Sadakane \cite{NS10} obtained the currently best result, 
 time, still within the same 
compressed space. They did so by improving the times of
the dynamic sequences on small alphabets to , which is
optimal even on bitmaps and in the amortized sense \cite{FS89}. 

As mentioned, the solution by Gupta et al.~\cite{GHSV07} deviates from this
path and is a general framework for using any static data structure
and periodically rebuilding it. By using it over a given representation
\cite{GMR06}, it achieves  query time and  amortized
update time. It would probably achieve compressed space if combined with more
recent static data structures \cite{BGNN10}. This shows that query times can
be significantly smaller if one allows for much higher update times. In this
paper, however, we focus in achieving similar times for all the
operations.

\begin{sidewaystable}[p]
\caption{History of results on managing dynamic sequences  over alphabet
, assuming  to simplify. 
Some results \cite{HSS03,BB04,CHL04} were presented only for binary 
sequences and the result we give is obtained 
by using them in combination with wavelet trees.
Column W/A tells whether the update times are (W)orst-case or (A)mortized.}
\label{tab:res}
\begin{center}
\footnotesize
\begin{tabular}{l|c|c|c|c}
Source & Space (bits) & Query time & Update time & W/A \\
\hline
\cite{FS89} &    & \multicolumn{2}{c|}{ for  and indels} & A \\
\hline
\cite{HSS03,HSS11} &  &  &  & A \\
\cite{CHL04} &  &  &  & W \\
\cite{BB04} &  &  &  & W \\
\cite{MN06,MN08} &  &  &  & W \\
            &  &  &  & W \\
\cite{CHLS07} &  &  &  & W \\
\cite{GHSV07} &  &  &  & A \\
\cite{LP07,LP09} &  &  &  & A \\
\cite{GN08,GN09} &  &  &  & W \\
\cite{HM10} &  &  &  & W \\
\cite{NS10} &  &  &  & W \\
\hline
Ours   &  &  &  & A \\
Ours   &  & ,  for  &  & W \\
\end{tabular}
\end{center}
\end{sidewaystable}

Table~\ref{tab:res} gives more details on previous and our new results.
Wavelet trees can also be used to model  grids of points, in 
which case . Bose et al.~\cite{BHMM09} used a wavelet-tree-like
structure to solve range counting in optimal static time ,
using operations slightly more complex than rank on the wavelet tree levels.
It is conceivable that this can be turned into an 
time algorithm using dynamic sequences on the wavelet tree levels. On the
other hand,  is a lower bound for dynamic range
counting in two dimensions \cite{Pat07}. This seems to suggest that it is
unlikely to obtain better results than those previously known for dynamic 
wavelet trees.

In this paper we show that this dead-end can be broken by abandoning the
implicit assumption that, to provide ,  and  on , we
{\em must} provide  and  on the bitmaps (or sequences over 
). We show that
all what is needed is to {\em track} positions of  downwards and upwards
along the wavelet tree. It turns out that this tracking can be done in 
{\em constant} time per level, breaking the  per-level
barrier.

As a result, we obtain the {\em optimal} time complexity
 for all the queries (worst-case) and update operations
(amortized), independently of the alphabet size.
This is  times faster 
than what was believed to be the ``ultimate'' solution. 
Our space is  bits, similar to previous solutions.
We develop, alternatively, a data structure achieving worst-case time for all 
the operations, yet this raises to  for , insertions and 
deletions. 
\no{The set of operations can be extended to more general splitting,
concatenations, and block moves, at  time cost per operation.
The only previous solution handling these operations \cite{NS10} achieved
.}

Among the many applications of this result, it is worth mentioning that 
any dynamic sequence representation supporting  and insertions in 
amortized time can be used to compute the Burrows-Wheeler transform (BWT)
\cite{BW94} of a sequence  in worst-case time . Thus our 
results allow us to build the BWT in  time and compressed
space. The best existing space-time tradeoffs are by Okanohara and Sadakane 
\cite{OS09}, who achieve optimal  time within 
 bits, and Hon et al.~\cite{HSS09}, who 
achieve  time with  bits. 
K\"arkk\"ainen \cite{Kar07} had obtained before  time and
 extra bits for a parameter .
Using less space
allows us to improve BWT-based compressors (like {\sc Bzip2}) by allowing them
to cut the sequence into larger blocks, given a fixed amount of main memory
for the compressor. 
Many other results will be mentioned in Section~\ref{sec:app}.


\section{The Wavelet Tree}
\label{sec:wavel}

Let  be a string over alphabet .  We associate each
 to a leaf  of a full balanced  binary tree . The essential idea of the wavelet tree structure \cite{GGV03} is the representation of elements from a string  by bit sequences stored in
 the nodes of tree . We  associate a subsequence  
of  with every node  of . For the root , . 
In general,  consists of all the occurrences of symbols  
in , where .
The wavelet tree does not store  explicitly, but just a bit vector
. We set  if the -th element
of  also belongs to , where  is the -th child of 
(the left child corresponds to  and the right to ).
This data structure (i.e.,  and bit vectors ) is called a 
\emph{wavelet tree}. Since  has  nodes and 
 levels, and the bitmaps at each level add up to length
, it requires  bits of space. If
the bitmaps  are compressed to  bits, the total size
adds up to  bits \cite{GGV03}. Various surveys on
wavelet trees are available, for example \cite{NM06,Nav12}.

For any  symbol  and  every internal node  such that , 
there is exactly one bit  in  that indicates in which child 
of   the leaf  is stored. We will say that such  
\emph{encodes}  in ; we will also say that bit  from  
\emph{corresponds} to a bit  from  if both  and  encode 
the same symbol  in two nodes  and . 
Identifying the positions of bits that encode the same 
symbol plays a crucial role in wavelet trees. Other, more complex, 
operations rely on the ability to navigate in the tree and keep track of 
bits that encode the same symbol.

The wavelet tree encodes , in the sense that it allows us to extract any
. To implement  we traverse a path from the root to the leaf 
. In each visited node we read the bit  that encodes  and 
proceed to the corresponding bit in the -th child of . Upon arriving 
to a leaf  we answer . 

The wavelet tree also implements operations  and .
To compute , we start at the position of  and 
identify the corresponding bit  in the parent  of . We continue
this process to the root until reaching a position . Then the answer
is .
Finally, to compute , we traverse the wavelet tree from 
to the leaf . At each element  of each node  in the path, 
we identify the last bit  that precedes  and encodes an . Then we
move to the bit  corresponding to  in the -th child of . Upon
arriving at position , the answer is .


The standard method used in wavelet trees for identifying
 the corresponding bits 
is to maintain rank/select data structures on the bit vectors . 
Let ; we can find the offset of the 
corresponding bit in the child of  by answering a query .
If  is the -th child of a node , we can find the offset of the
corresponding bit in  by answering a query . Finally, the
more complicated process of finding  needed for  is easily
solved using binary : if  is at position  in , then without 
need of finding  we know that its corresponding position in the -th 
child of  is .
This approach leads to  query times in the static case because 
rank/select queries on a bit vector  can be answered in constant time
and  bits of space \cite{Mun96,Cla96}, and even using 
 bits \cite{RRR07}. 
However, we need  time to support rank/select and
updates on a bit vector \cite{FS89}, which multiplies the operation times in
the dynamic case. 

An improvement (for both static and dynamic wavelet trees) can be achieved by increasing the fan-out of the wavelet tree to  for a constant : as before,  if the -th element
of  also belongs to  for the -th child  of . 
This enables us to reduce the height of the wavelet trees and the 
query time by a  factor, because the / times
over alphabet  is still constant in the static case \cite{FMMN07}
and  in the dynamic case \cite{HM10,NS10}. 
However, it seems that further improvements that are based on dynamic rank/select queries 
in every node are not possible.

In this paper we use a different approach to identifying the 
corresponding elements. We partition sequences  into blocks, which are stored
in compact list structures . Pointers from selected positions 
in  to the structure  in children nodes  (and vice versa) 
enable us to navigate between nodes of the wavelet tree in constant time. We
extend the idea to multiary wavelet trees.
While similar techniques have been used in some geometric data structures 
\cite{N11,B08}, applying them on compressed data structures 
where the bit budget is severely limited  is much more challenging.
We describe our new solution next.


\section{Basic Structure}
\label{sec:basic}

We start by describing the main components of our modified 
wavelet tree. Then, we show how our structure supports 
 and . In the third part of this 
section we describe additional structures that enable us to 
answer . Finally, we show how to support updates. 

\subsection{Structure}

We assume that the wavelet tree  has node degree . 
We divide sequences  into {\em blocks} and store those blocks 
 in a doubly-linked list .  
Each block  contains  consecutive 
elements from , except the last, which can be smaller.
That is,  if measured in bits.
For each   we maintain a data structure  that supports ,  and  queries on elements of . Since a block contains a poly-logarithmic 
number of elements over an alphabet of size , we can answer those queries
in  time using  additional bits
(see Section~\ref{app:A} for details).

A \emph{pointer} to an element  consists of two parts:
a unique id of the block  that contains the offset  and 
the index of  in . Such a pair (block id, local index) will be
called the {\em position} of  in .

We maintain pointers between selected corresponding elements in  and its children.  
If  an element  is stored in a block  and 
 for all   in  (i.e.,  is the first
occurrence of  in its block), then we store 
a pointer from  to the offset  of the 
corresponding element  in ,
where  is the -th child of . 
Pointers are bidirectional, that is, we also store a pointer from  to .
In addition, if  is the first offset in its block and  
corresponds to  in the parent  of , then we store a pointer from
 to  and, by bidirectionality, from  to .
All these pointers will be called 
\emph{inter-node pointers}. 
We describe how they are implemented later in this section.

It is easy to see that the number of inter-node pointers from  in  
to  in , for any fixed , is , where  is the number of blocks 
in . Hence, the total number of pointers that point down from a node 
 is bounded by . Additionally, there are  pointers 
up to the parent of . Thus, the total number of pointers 
in the wavelet tree equals . Note that the term
 is only necessary to account for nodes that have just
one block (with  bits). Since the children of those nodes
must also have just one block, we avoid storing their pointers, as we know that
all point to the same block and their index inside the block can be found with
constant-time / operations inside the block from where pointers 
leave. Their upwards pointer to their parent, if the parent has more than one
block, can be represented and charged to the space of the parent. 
This yields the cleaner expression
 for the number of pointers.

The pointers from a block  are stored in a data structure . Using  we can find, for any 
offset  in  and any , the last 
 in  such that there is a pointer from  to an 
offset  in .  
We describe in Section~\ref{app:A} how  implements the queries and 
updates in constant time.

For the root node , we store a dynamic searchable partial-sum data 
structure  that contains the number of positions in each block of 
. Using , we can find the block  that contains the 
-th element of  (query {\em search} on the partial sums), as well 
as the number of elements in all the blocks that precede a given block 
 (operation {\em sum} on the partial sums).
Both operations can be supported in  time and linear space
\cite[Lem.~1]{NS10}.
The same data structures 
 are also stored in the leaves  of . 
Since , and also , we store  elements in the partial
sums  and , for an overall size of 
bits.

We observe that we do not store a sequence  in a leaf node
, only in internal nodes. Nevertheless, we divide the (implicit) sequence 
 into blocks and store the  number of positions in each 
block in ; we maintain  only if  consists of more than one block. Moreover we store inter-node pointers from 
the parent of  to  and vice versa.
Pointers in a leaf are maintained using the same rules of any other node.

For future reference, we provide the list of secondary data structures in Table~\ref{tab:not}.

\begin{table}
\caption{Structures inside any node  of the wavelet tree , 
or only in the root node  and the leaves . The third column gives
the extra space in bits, on top of the data, for the whole structure; here
 is  in the uncompressed case and  in the
compressed case.}
\label{tab:not}
\begin{center}
\begin{tabular}{l|l|c}
Structure & Meaning & Extra space in bits \\
\hline
 & List of blocks storing  &  \\
 & -th block of list  &  \\
 & Supports \ra/\sel/\acc\ inside  &  \\
 & Pointers leaving from  & \\
 & Pointers arriving at  &  \\
 & Predecessor in  containing symbol  &  \\
 & Partial sums on block lengths for  and  &  \\
 & Deleted elements in , for  and  &  \\
    & Global list of deleted elements in  &  \\
\end{tabular}
\end{center}
\end{table}


\subsection{Access and Select Queries}

Assume the position of an element  in  is known, 
and let  be the index of offset  in its block . 
Then the position of the corresponding offset  in  is computed as follows.
Using , we find the index  of the largest  in  
such that
there is a pointer from  to some  in . Due to our
construction, such  must exist (it may be  itself). 
Let  denote the block that contains , and let
 be the index of  in .
Due to our rules to define pointers,  also 
belongs to , since if it belonged to another block  
the upward pointer from  would point between  and ,
and since pointers are bidirectional, this would contradict the definition of
. 
 Furthermore, let  and . 
Then the index of  is . 
Thus we can find the position of  in 
 time if the position of  is known. 

Analogously, assume we know a position  at  and want to 
find the position of the corresponding offset  in its parent node . Using  
we find the last  in  that has a pointer to its
parent, which exists by construction (it can be the upward pointer from the 
first index in  or the reverse of some pointer from  to ). 
Let  point to , with index  in a block . 
Then, by our construction,  is also in , since if it belonged
to a different block , then the first occurrence of  in 
would point between  and , and its bidirectional version would 
contradict the definition of .
Furthermore, let
 and  be the indexes of  and  in , respectively.
Then the index of  is 
.

To solve , we visit the nodes , where 
 is the height of ,
 is the -th child of  and  encodes 
. We do not find out the offsets , but just their
positions. The position of  is found in  
time  using the partial-sums structure . 
If the position of   is known, we can find that of  in 
 time, as explained above. 
When a leaf node  is reached, we know that  .

To solve , we set  and identify its position in the 
list  of the leaf , using structure . 
Then we traverse the path 
 where  is the parent of , until 
the root node is reached. In every node , we find the position of
 in  that corresponds to  as explained above. 
Finally, we compute 
the number of elements that precede  in  using 
structure .  

Thus  and  require 
worst-case time.


\subsection{Rank Queries}

We need some additional data structures for the efficient support of rank queries. 
In every node  such that  consists of more than one block, we store a data structure . Using 
 we can find, for any  and for any block 
, the last block   that precedes  and contains 
an element . 
 consists of  predecessor data structures  for 
. 
We describe in 
Section~\ref{sec:rank} a way to support these predecessor queries in constant 
time in our scenario.

Let the position of offset  be the -th element in a block . 
 enables us to find the position of the last  such that
. First, we use  to compute
. If  , then  belongs to the same 
block as  and its index in the block  is
. Otherwise, we use  to find the 
last block  that precedes  and contains an  element 
. We then find the last such element in 
 using .

Now we are ready to describe the procedure to answer . The symbol 
 is represented as a concatenation of symbols 
, where each  is between 1 and . 
We traverse the path from the root  to the leaf .  
We find the position of  in  using 
the data structure . In each node , , 
 we identify the position of the last element  that precedes 
, using . 
Then we find the offset  in the list  that 
corresponds to .

When our procedure reaches the leaf node , the element  encodes the last 
symbol  that precedes . We know the position of offset , say
index  in its block . Then we find the number  of 
elements in all the blocks that precede  using . 
Finally, .

Since structures  answer queries in constant time, the overall time for
 is .


\subsection{Updates} \label{sec:H}

Now we describe how inter-node pointers are implemented.
We say that an element of  is \emph{pointed} if there 
is a pointer to its offset. 
Unfortunately, we cannot store the local index of a pointed 
element in the pointer: when a new element is inserted into 
a block, the indexes of all the elements that follow it are incremented 
by . Since a block can contain  pointed 
elements, we would have to update that many
pointers after each insertion and deletion. 

Therefore we resort to the following two-level scheme. 
Each pointed element in a block is assigned a unique id. 
When a new element is inserted, we assign it the id 
, where  is the maximum 
id value used so far. 
We also maintain a data structure  for each block 
 that enables us to find the position 
of a pointed element if its id in  is known.
Implementation of  is based on standard 
word RAM techniques and a table that contains ids of the 
pointed elements; details are given in Section~\ref{app:A}.


We describe now how to insert a new symbol  into  at position . 
Let  be the offsets of the elements that will encode 
 in . 
We can find the position of  in  in  time
using , and insert  at that position, .
Now, given the position of , in , where  has
just been inserted,
we find the position of the last  such that 
, in the same way as  for rank queries.
Once we know the position of  in , we find 
the position of  in  that corresponds to . 
The element  must then be inserted into  
immediately after , at position .

The insertion of a new element  into a block  
is handled by structure  and the memory manager of the block.
We must also update structures
 and  to keep the correct alignments, and possibly
to create and destroy a constant number inter-node pointers to maintain 
our invariants.
Also, since pointers are bidirectional, a constant number of inter-node 
pointers in the parent and children of node  may be updated. All those 
changes can be done in  time; see Section~\ref{app:A} for the details. 
Insertions may also require updating structures , which require
 amortized time, see Section~\ref{sec:rank}. 
Finally, if  is the root node or a leaf, we also update .
This update is only by , so it requires just
 time \cite[Lem.~1]{NS10}.

If the number of elements in  exceeds 
, we split  evenly into two blocks,  and 
.
Then, we rebuild the data structures ,  and  for the two new blocks.
Note that there are inter-node pointers to  that now could
become dangling pointers, but all those can be known from , since
pointers are bidirectional, and updated to point to the right places in
 or .
Finally, if  is the root or a leaf, then  is updated,
meaning that we replace an existing element by two.

The total cost of splitting a block is dominated by that
of building the new data structures ,  and .
These are easily built in  time. Since
we split a block  at most once per sequence of  
insertions in , the amortized cost incurred by splitting a block 
is . Therefore the total cost of an insertion in  is .
The insertion of a new 
symbol leads to  insertions into 
lists . 

Our partial-sums structures  and  do not support updates with large 
values. Inserting a new value for  and moving part of the value 
of  to  can be done in 
 time by subtracting  units from
the value for  until it becomes , then inserting a number after
it with value zero and increasing it by  units until it becomes
. Each such increment/decrement and insertion takes
 time \cite[Lem.~1]{NS10} and we carry it out
 times. Still this total cost amortizes to  per operation.

Hence, the total cost of an insertion is .

We describe next how deletions are handled, where
we also describe the data structure . 

\section{Lazy Deletions and Data Structure }
\label{sec:rank}

We do not process deletions immediately, but in lazy form: we do not
maintain exactly  but a supersequence  of it. When a symbol  is 
deleted from , we retain it in  but take a notice that 
 is deleted. When the number of deleted symbols exceeds 
a certain threshold, we expunge from the data structure all the elements 
marked as deleted.
We define  and the list  for the sequence 
 in the same way as  and  are defined for 
.

Since elements of  are never removed, we can implement 
 as an insertion-only data structure. For any 
, , we store information about all the blocks 
of a node  in a data structure . 
 contains one element for each block  and is 
implemented as an incremental split-find data structure that supports 
insertions and splitting in  amortized time and queries in  
worst-case time 
\cite{IA84}. The splitting positions in  are the blocks  that 
contain an occurrence of , so the operation ``find'' in  allows us 
to locate, for any , the last block preceding  that contains 
an occurrence of . 

The insertion of a symbol  in  may induce a new split in . 
Furthermore, overflows in a block , which convert it into two blocks 
 and , induce insertions in .
Note that an overflow in  triggers  insertions in the 
structures, but this  time amortizes to  because overflows
occur every  operations.

Structures  do not support ``unsplitting'' nor removals.
The replacement of  by   and  is implemented as 
leaving in 
 the element corresponding to  and inserting one corresponding 
to either  or . If  contained , then at
least one of  and  contain , and the other can be
inserted as a new element (plus possibly a split, if it also contains ).

We need some additional data structures to support lazy deletions. 
A data structure  stores the number of non-deleted elements 
in each block of  and supports partial-sum queries. 
We will maintain  in the root of the wavelet tree and 
in all leaf nodes. Moreover, we maintain a data structure 
 for every block , where  is either the root 
or a leaf node.  can be used to count the number of deleted 
and non-deleted elements before the -th element in a block  for any query index , as well as to find the index in  of the 
-th non-deleted element. 
The implementation of  is described in Section \ref{app:A}. 
We can use  and  to find the index  in 
where the -th non-deleted element occurs, and to count the number 
of non-deleted elements that occur before the index  in . 

We also store a global list {\em DEL} that contains, in any order, all 
the deleted symbols that have not yet been expunged from the wavelet tree. 
For any symbol  in the list {\em DEL} we store a pointer to the 
offset  in  that encodes . Pointers in 
list {\em DEL} are implemented in the same way  as inter-node
 pointers.

\subsection{Queries}

Queries are answered very similarly to 
Section~\ref{sec:basic}. The main idea is that we can essentially ignore
deleted elements except at the root and at the leaves.
\begin{description}
\item[:] 
Exactly as in Section 3, except that  encodes the -th 
non-deleted element in , and is found using  and .

\item[:] We find the position of the offset  of the 
-th non-deleted element in , where , using  and some . 
Then we move up in the tree exactly as in Section~\ref{sec:basic}.
When the root node  is reached, we count the number of non-deleted 
elements that precede offset  using . 

\item[:] We find the position of the offset  of the -th 
non-deleted element in . Let  be defined as in Section~\ref{sec:basic}. 
In every node , we find the last offset  such that
. Note that this element may be a deleted one, but it still drives us to
the correct position in . We proceed exactly as in 
Section~\ref{sec:basic} until we arrive at a leaf .
At this point, we count the number of non-deleted elements that precede 
offset  using  and .
\end{description}

\subsection{Updates} \label{sec:rankdel}

Insertions are carried out just as in Section~\ref{sec:basic}. 
The only difference is that we also update the data structure  when 
an element  that encodes the inserted symbol  is added to a block 
. When a symbol  is deleted, we append it to the list {\em DEL} of
deleted symbols. Then we visit each block  containing the element 
 that encodes  and update the data structures . 
Finally,  and  are also updated. This takes in total
 time.

When the number of symbols in the list {\em DEL} reaches
, we perform a \emph{cleaning} procedure 
and get rid of all the deleted elements. Therefore {\em DEL} never requires
more than  bits, and the overhead due to storing deleted symbols
is  bits.

Let , , be the sequence of elements that encode 
a symbol . The method for tracking the elements 
, removing them from their blocks , 
and updating the block structures, is symmetric to the insertion procedure 
described in Section~\ref{sec:basic}.  
In this case we do not need the predecessor
queries to track the symbol to delete, as the procedure is similar to that
for accessing . When the size of a block  falls below
 and it is not the last block of , we merge it with 
, and then split the result if its size exceeds .
This retains  amortized time per deletion in any node , 
including the updates to  structures, and this adds up to
 amortized time per deleted symbol.

Once all the pointers in {\em DEL} are processed, we rebuild from scratch 
the structures  for all nodes . The total size of all the
 structures is  elements. 
Since a data structure 
for incremental split-find is constructed in linear time, 
all the s are rebuilt in  time. 
Hence the amortized time to rebuild the s is 
, which does not affect the amortized time
 to carry out the effective deletions.

\section{Changes in  and Alphabet Independence}  \label{sec:space}

Note that our structures depend on the value of , so they
should be rebuilt when  changes. We use  as a
fixed value and rebuild the structure from scratch when  reaches another 
power of two (more precisely, we use words of  bits
until  increases by 1 or decreases by 2, and only then
update  and rebuild). These reconstructions do not affect the amortized 
complexities, and the slightly larger words waste an  extra space 
factor in the redundancy.

We take advantage of using a fixed  value 
to get rid of the alphabet dependence. If
, our time complexities are the optimal 
. However, if  is larger, this means that not all
the alphabet symbols can appear in the current sequence (which contains at most
 distinct symbols). Therefore, in this case we create
the wavelet tree for an alphabet of size , not 
(this wavelet tree is created when  changes).
We also set up a {\em mapping}
array  that will tell to which value in  is a symbol 
mapped, and
a {\em reverse mapping}  that tells to which original symbol in
 does a mapped symbol correspond. Both  and  are initialized
in constant time \cite[Section III.8.1]{Meh84} and require  bits of space. Since this is used only when , the
space is .

Upon operations  and , the symbol  is mapped using
 (the answer is obvious if  does not appear in ) in constant time.
The answer of operation  is mapped using  in constant time as 
well. Upon insertion of , we also map  using . If not present in ,
we find a free slot  (we maintain a list of free slots) and assign 
 and .
When the last occurrence of a symbol  is deleted we return its slot to
the free list and unitialize its entry in . In this way, when , we can support all the operations in time .

We are ready to state a first version of our result, not yet compressing
the sequence. In Section~\ref{app:A} it is seen
that the time for the operations is . Since the
height of the wavelet tree is , then we have  time 
for all the operations.

As for the space,
we show in Section~\ref{app:A} how to manage the data in blocks  
so that all the elements stored in lists  use  bits, plus
the overhead  of the data
organization and the memory manager.
The internal structures
 add up to  extra bits.
Since there are  blocks overall,
all the pointers between blocks of 
the same lists add up to  bits. 
All the data structures  add up to  bits. 
We have shown that there are
 inter-node pointers, hence all inter-node 
pointers (i.e.,  and  structures) use 
 bits. 
Structures  use 
bits as they have  integers per block, and  takes  bits
plus the overhead of  of keeping deleted elements.
The overall space is then 
 bits.
(Note that when  we use an alphabet of size , but then still
we need the  mapping, that takes  bits.)
This gives our first result.

\begin{theorem}
\label{thm:optimal}
A dynamic string  over alphabet  
can be stored in a structure using 
 bits,
for any ,
and supporting queries
,  and  in time .
Insertions and deletions of symbols 
are supported in  amortized time. 
\end{theorem}

\section{Compressed Space}
\label{sec:rrr}

We now compress the space of the data structure to
zero-order entropy ( plus redundancy).
We show how a different encoding of the bits within the blocks reduces
the  to  in the space without affecting the time
complexities. 

Raman et al.\ \cite{RRR07} describe an encoding for a bitmap  that
occupies  bits of space. It consists of
cutting the bitmap into chunks of length  and encoding each
chunk  as a pair :  is the {\em class}, which indicates how 
many 1s are there in the chunk, and  is the {\em offset}, which is the 
index of this particular chunk within its class. The  components add up to
 bits, whereas the  components add up to .
Navarro and Sadakane \cite[Sec.\ 8]{NS10} describe a technique to maintain a 
dynamic bitmap in this format. They allow the chunk length  to vary, so 
they encode triples  maintaining the invariant that 
 for any . They show that this retains the same space, 
and that each update affects  chunks.

We extend this encoding to handle an alphabet  
\cite{FMMN07}, so that  symbols, and each chunk is
encoded as a tuple  where  counts
the occurrences of  in the block. The classes
 use  bits, and
the offsets still add up to . Blocks are encoded/decoded in
 time, as the class takes  bits and
the block encoding requires at most  bits.
In Section~\ref{app:A} we show how using compressed chunks does not affect
their handling inside blocks.

The sum of the local entropies of the chunks, across the whole 
, adds up to , and these add up to  \cite{GGV03}.
The redundancy over the entropy is  bits per
miniblock, adding up to  bits, and
we have also a fixed redundancy of , according to
Section~\ref{app:A}.
The fact that we store  instead of , with up to  spurious
symbols, can increase  up to  bits.
Thus we get the following result, for any desired .

\begin{theorem}
\label{thm:compr}
A dynamic string  over alphabet  
can be stored in a structure using 
 bits,
for any ,
and supporting queries
,  and  in time .
Insertions and deletions of symbols 
are supported in  amortized time. 
\end{theorem}

\section{Worst-Case Complexities} \label{sec:worst}

While in previous sections we have obtained optimal time and compressed space,
the time for the update operations is amortized. In this section we derive
worst-case time complexities, at the price of losing the time optimality,
which will now become logarithmic for some operations. 
Along the rest of the section we remove the various sources of amortization 
in our solution.

\subsection{Block Splits and Merges} \label{sec:indels}

Our amortized solution splits overflowing blocks and rebuilds the two new
blocks from scratch (Section~\ref{sec:H}). Similarly, it merges underflowing 
blocks (as a part of the cleaning of the global  list in 
Section~\ref{sec:rankdel}). This gives good amortized times but in the worst
case the cost is .

We use a technique \cite{GN09} that avoids global rebuildings. A block
is called {\em dense} if it contains at least  bits, and {\em
sparse} otherwise. While sparse blocks of any size (larger than zero) are 
allowed, we maintain the invariant that no two consecutive sparse blocks may 
exist. This retains the fact that there are  
blocks in the data structure. The maximum size of a block will be 
bits.
When a block overflows due to an insertion, we move its last element to the
beginning of the next block. If the next block would also overflow,
then we are entitled to create a new sparse block between both dense blocks, 
containing only that element. Analogously, when a deletion converts a dense
block into sparse (i.e., it falls below length ), we check if both
neighbors are dense. If they are, the current block can become sparse. If,
instead, there is a sparse neighbor, we move its first/last element
into the current block to avoid it becoming sparse. If this makes that sparse
neighbor block become of size zero, we remove it.

Therefore, we only create and destroy empty blocks, and move a constant 
number of elements to neighboring blocks. This can be done in constant
worst-case time. It also simplifies the operations on the partial-sum
data structures , since now only updates by  and insertions/deletions
of elements with value zero are necessary, and these are carried out in
 worst case time \cite[Lem.~1]{NS10}.
Recall that  is fixed in each instance of our data
structure, so the definition of sparse and dense is static.

\subsection{Split-Find Data Structure and Lazy Deletions}

The split-find data structure \cite{IA84} we used in Section~\ref{sec:rank}
to implement the  structures has constant amortized insertion time. We
replace it by  another one \cite[Thm 4.1]{Mor03} achieving 
worst-case time. Their structure handles a list of colored elements (list nodes), where
each element can have  colors (each color is a positive integer bounded by  for a constant ). 
We will only use list nodes with 0 or 1 color.
The operations of interest to us are: creating a new list node without colors, 
assigning or removing a color to/from a list node, and finding the last list node 
preceding a given node and having some given color. Node deletions are not
supported. The number of list nodes must be smaller than a certain upper bound ,
and the operations cost . In our case, since  is fixed,
we can use  as the upper bound.

We use  colors, one per symbol in the sequences. Each time we create
a block, we add a new uncolored node to the list, with a 
bidirectional pointer to the block. Each time we insert a symbol
 for the first time in a block, we add a new node colored
 to the list, right after the uncolored element that represents the block,
and also set a bidirectional pointer between this node and the block.

We cannot use the lazy deletions mechanism of Section~\ref{sec:rank}, as it
gives only good amortized complexity. We carry out the deletions immediately
in the blocks, as said in Section~\ref{sec:indels}.
Each time the last occurrence of a symbol  is deleted from a
block, we remove the color from the corresponding list node (if the symbol reappears
later, we reuse the same node and color it, instead of creating a new one). 

Therefore, finding
the last block where a symbol  appears, as needed by the  query and
for insertions, corresponds to finding the last list node colored  and
preceding the uncolored node that represents the current block.

Since list nodes cannot be deleted, when a block disappears its (uncolored)
list nodes are left without an associated block. This does not alter the result
of queries, but there is the risk of maintaining too many
useless nodes. We permanently run an incremental list
``copying'' process, traversing the current list of blocks and inserting the
corresponding nodes into a new list. This new list is also updated, together
with the current list, on operations concerning the blocks already copied.
When the new list is ready it becomes
the current list and the previous list is incrementally deleted. In
 steps we have copied the current list; by this time the
number of useless nodes is at most  and just poses 
 bits of space overhead.

Note that blocks must manage the sets of up to  pointers to their
colored nodes. This is easily handled in constant time with the same
techniques used for structure  in Section~\ref{app:A}.

Since the colored list data structure requires  time,
operations  and insert take worst-case time , 
whereas access, select and delete still stay in .

\subsection{Changes in }

As an alternative to reconstructing the whole structure when  doubles
or halves, M\"akinen and
Navarro \cite{MN08} describe a way to handle this problem without affecting
the space nor the time complexities, in a worst-case scenario.
The sequence is cut into a prefix, a middle part, and a suffix. The middle
part uses a fixed value , the prefix uses  and the suffix uses . Insertions and deletions
trigger slight expansions and contractions in this separation, so that when
 doubles all the sequence is in the suffix part, and when  halves all
the sequence is in the prefix part, and we smoothly move to a new value of
. This means that the value of  is fixed for any instance of our
data structure. Operations access, rank and select, as well as insertions
and deletions, are easily adapted to handle this split string.

Actually, to have sufficient time to build universal tables of size 
 for , the solution \cite{MN08} maintains the sequence
split into five, not three, parts. This gives also sufficient time to build
any universal table we need to handle block operations in constant time, as
well as to build the wavelet tree structures of the new partitions.

\subsection{Memory Management Inside Blocks}

The EAs of Lemma~\ref{lem:EA} (Section~\ref{app:A})
have amortized times to grow and shrink.
Converting those to worst-case time requires a constant space overhead
factor. While this is acceptable for the EAs of structures  in 
Section~\ref{app:A}, they
raise the overall space to  bits if used to maintain the main data.
Instead, we get rid completely of the EA mechanism to maintain the data, and 
use a single large memory area for all the miniblocks
of Section~\ref{app:A}, using Munro's technique \cite{Mun86}. 

The problem of using a single memory area is that the pointers to the
miniblocks require  bits, which is excessive because miniblocks
are also of  bits. Instead, we use slightly larger miniblocks,
of  bits. This makes the overhead due to pointers
to miniblocks , adding up to additional
 bits.

The price of using larger miniblocks is that now the operations on blocks
are not anymore constant time because they need to traverse a miniblock,
which takes time . We can still retain constant time for the
query operations, by considering {\em logical} miniblocks of 
bits, which are stored in {\em physical} areas of 
miniblocks. However, update operations like insert and delete must shift 
all the data in the miniblock area and possibly relocate it in the memory 
manager, plus updating pointers to all the logical miniblocks displaced or 
relocated. This costs  time per insertion and deletion.
This completes our result.


\begin{theorem}
\label{thm:worstcase}
A dynamic string  over alphabet  
can be stored in a structure using 
 bits, for any constant ,
and supporting queries  and  in worst-case time
, and query , insertions and deletions in  worst-case time .
\end{theorem}

\no{
\section{Extensions}
\label{sec:ext}

\subsection{Handling General Alphabets}
\label{sec:alphabet}
}

\section{Data Structures for Handling Blocks} \label{app:A}

We describe the way the data is stored in blocks , as well as 
the way the various structures inside blocks operate. All the data 
structures are based on the same idea: We maintain a tree with node degree 
 and leaves that contain  bits. Since elements 
within a block can be addressed with  bits, each internal node 
and each leaf fits into one machine word. Moreover, we can support searching 
and basic operations  in each node in constant time.  

\subsection{Data Organization}

The block data is physically stored as a sequence of {\em miniblocks} of
 symbols, or  bits. Thus there are 
 mini\-blocks in a block. These miniblocks will be the 
leaves of a -ary tree , for  and some 
constant . The height of this tree is constant, . 
Each node of  stores  counters telling the number 
of symbols stored at the leaves that descend from each child. This requires 
just  bits. To access any position of ,
we descend in , using the counters to determine the correct child.
When we arrive at a leaf, we know the local offset of the desired symbol
within the leaf, and can access it directly. Since the counters fit in 
less than a machine word, a small universal table gives the correct child in 
constant time, therefore we have  time access to any symbol (actually to any 
 consecutive symbols).

Upon insertions or deletions, we arrive at the correct leaf, insert or delete 
the symbol (in constant time because the leaf contains  bits
overall), and update the counters in the path from the root (in constant time
as they have  bits). The leaves may have  to  bits.
Splits/merges upon overflows/underflows are handled as usual, and can be
solved in a constant number of -time operations (
operates as a B-tree; internal nodes may have  to  children).

The space overhead due to the nodes of  is
 bits, where we measure
 in bits, not symbols. The factor  disappears 
because each leaf of  has  miniblocks. 

We consider now the space used by the data itself.
In order not to waste space, the miniblock leaves are stored using a memory
management structure by Munro \cite{Mun86}. For our case, it allows us to 
allocate, free, and access  miniblocks of length up to  in
constant time. Its space waste, given that our pointers are internal to blocks
and require 
bits, is  per allocated miniblock, which adds up to
, plus a global redundancy
of  bits.
If we used one allocation structure per block, handling its miniblocks,
the global redundancy of  bits per block would add
 bits 
overall. This is reduced to  by
using one allocation structure per group of  blocks. This
reduces the overhead of the structures and the address space is
still of size , so pointers can still be of length .

Each allocation structure uses a memory area of fixed-size cells (inside which 
the variable-length miniblocks are stored) that grows or shrinks at the end
as miniblocks are created or destroyed. A structure to store those memory
areas with fixed-size cells and allowing them to grow and shrink
is the {\em extendible array (EA)} \cite{RR03}. We need to handle a
set of  EAs, what is called a 
{\em collection of extendible 
arrays}. It supports accessing any cell of any EA, letting
any EA grow or shrink by one cell, and create and destroy EAs. 
The following lemma,
simplified from the original \cite[Lemma 1]{RR03}, and using words of 
bits, is useful.

\begin{lemma} \label{lem:EA}
A collection of  EAs of total size  bits can be represented using
 bits of space, so that the operations of 
creation of an empty EA and access take constant worst-case time, whereas 
grow/shrink take constant amortized time. An EA of  bits can be destroyed 
in time .
\end{lemma}

In our case  and , so 
the space overhead posed by the EAs is 
.

When we store the miniblocks in compressed form, in 
Section~\ref{sec:rrr}, they could use as little as 
 bits, and thus we could store up to 
 miniblocks in a single leaf of . 
This can still can be handled in constant time using (more complicated)
universal tables \cite{MN08}, and the counters and pointers of  
bits are still large enough.

\subsection{Structure }

To support  and  we enrich  with further information
per node. We store  counters with the number
of occurrences of each symbol in the subtree of each child. The node size 
becomes 
as long as . This adds up to 
 bits because the leaves of 
handle  miniblocks.

With this information on the nodes we can easily solve  and  in
constant time, by descending on  and determining the correct child
(and accumulating data on the leftward children) in  time using
universal tables. Nodes can also be updated in constant time even upon splits
and merges, since all the counters can be recomputed in  time.

\subsection{Structure }

This structure stores all the inter-node pointers leaving from 
block , to its parent and to any of the  children of node .

The structure is a tree  very similar in spirit to . The pointers
are stored at the leaves of , in increasing order of their source
position inside .
The pointers stored are inter-node, and
thus require  bits. Thus we store a constant number of pointers 
per leaf of . For each pointer we store the position in  holding the pointer
(relative to the starting position of the leaf node inside ) 
and the target position (as an absolute pointer to another ). 
The internal nodes, of arity , maintain 
information on the number of positions of  covered by each child,
and the number of pointers of each kind ( counters) stored in the 
subtree of each child. This requires  bits, as
before. To find the last position before  holding a pointer of a certain
kind (parent or -th wavelet tree child, for any ), we 
traverse  from the root looking for position . At each node , it 
might be that the child  where we have to enter holds pointers of that kind,
or not. If it does, then we first enter into child . If we return with an 
answer, we recursively return it. If we return with no answer, or there are
no pointers of the desired kind below , we enter into the last sibling to 
the left of  that holds a pointer of the desired kind, and switch to a 
different mode where we simply go down the tree looking for the rightmost 
child with a pointer of the desired kind. It is not hard to see that this 
procedure visits  nodes, and thus it is constant-time because all 
the computations inside nodes can be done in  time with universal tables.
When we arrive at the leaf, 
we scan for the desired pointer in constant time.

The tree  must be updated when a symbol  is inserted before any other 
occurrence of  in , when a symbol is inserted at the first position 
of  and, similarly, when symbols are deleted from . The needed
queries are easily answered with tree . Moreover, due to the 
bidirectionality, we must also update  when pointers to  are 
created from the parent or a child of , or when they are deleted. 
Those updates work just like on the tree
.  is also updated upon insertions and deletions of symbols, even 
if they do not change pointers, to maintain the positions up to date. In this
case we traverse  looking for the position of the update, change the 
offsets stored at the leaf, and update the subtree sizes stored at the nodes. 

\subsection{Structure } 

This structure manages the inter-node pointers that point inside .
As explained in Section~\ref{sec:H}, we give a handle to the outside nodes, 
that does
not change over time, and  translates handles to positions in .

We store a tree  that is just like , where the incoming pointers 
are stored.  is simpler, however, because at each node we only need to
store the number of positions covered by the subtree of each child. It must
also be possible to traverse  from a leaf to the root. 

In addition, we manage a table  so that  points to the leaf of 
 where the pointer corresponding to handle  is stored.  is also
managed as a tree similar to , with pointers sorted by id,
where a constant number of ids  are stored
at the leaves together with their pointers to the leaves of  (note that 
there are  ids at most, so we need  
bits for both ids and their pointers to ). Each internal node in  
maintains the maximum id stored at its leaves and the number of ids stored at 
its leaves. Thus one can in constant time find the pointer to  
corresponding to a given id, and also find the smallest unused id when a
fresh one is needed (by looking for the first leaf of  where the maximum
id is larger than the number of ids).

At the leaves of  we store, for each pointer, a backpointer 
to the corresponding leaf of  and the position in  (in relative 
form). Given a handle , we find using  the corresponding position in the
leaf of , and move upwards up to the root of , adding to the leaf 
offset the 
number of positions covered by the leftward children of each node. At the end 
we have obtained the position in constant time.

When pointers to  are created or destroyed, we insert or remove
pointers in . This requires traversing it top-down to find the appropriate
leaf position and returning back to the root updating offsets. Backpointers to
 are used to adjust a constant number of positions in the leaf of .
We must also update  upon symbol insertions and deletions in
, to maintain the positions up to date. When a leaf splits or merges,
we also update the pointers from a constant number of positions in
, found with the backpointers. Similarly, the insertion and deletion of
pointers from outside require updating , and the backpointers from 
are maintained up to date using the pointers from  to .

 may contain up to  pointers of  
bits, which can be significant for some blocks. However, across the whole
structure there can be only  pointers,
adding up to  bits, spread across
 tables . Using again Lemma~\ref{lem:EA},
a collection of EAs poses an overhead of  bits.

\subsection{Structure  and the Final Result}

Structure  is implemented as a tree  analogous to , storing at 
each node the number 
of positions and the number of non-deleted positions below each child. It
requires  bits. Since these are stored only for
the root  and the leaves  of , its space adds up to
 bits.

While the raw data adds up to  bits, the space overhead adds up
to  for all the pointers plus
 for the memory management overhead. 
We can use, say,  and then have  time and
 bits for any 
 (renaming  as ).
However, when the data is compressed (Section~\ref{sec:rrr}), the sum of all the
 terms in the space is .
This makes the space overhead related to the memory management and of
 structures add up to 
bits.

\section{Extensions and Applications}
\label{sec:app}

We first describe an extension of our results to handling general alphabets,
and then various applications of the original and the extended results.

\subsection{Handling General Alphabets}
\label{sec:alphabet}

Our time results do not depend on the alphabet size , yet our space 
does, in a way that ensures that  gives no problems as long as 
 (so ).

Let us now consider the case where the alphabet  is much larger than
the {\em effective} alphabet of the string, that is, the set of symbols that 
actually appear in  at a given point in time. Let us now use 
to denote the effective alphabet size. Our aim is to maintain the space within 
 
bits, even when the symbols come from a large universe , 
or even from a general ordered universe such as  or
 (i.e.,  are words over another
alphabet ).

Our mappings  and  of Section~\ref{sec:space} 
give a simple way to handle a 
sequence over an unbounded ordered alphabet. By changing 
to a custom structure to search , and storing elements of 
 in array , we obtain the following results, using respectively
Theorems~\ref{thm:compr} and \ref{thm:worstcase}.

\begin{theorem}
\label{thm:general}
A dynamic string  over a general alphabet  
can be stored in a structure using 
 
bits and supporting queries
,  and  in time 
.
Insertions and deletions of symbols 
are supported in  amortized 
time. 
Here  is the number of distinct symbols of  occurring in ,
 is the number of bits used by a dynamic data structure 
to search over  elements in  plus to refer to  
elements in ,  is the worst-case time to search 
for an element among  of them in , and 
is the amortized time to insert/delete symbols of  in the structure.
\end{theorem}

\begin{theorem}
\label{thm:generalwc}
A dynamic string  over a general alphabet  
can be stored in a structure using 
 
bits and supporting queries
 and  in time  and
 in time .
Insertions and deletions of symbols 
are supported in  time. 
Here  is the number of distinct symbols of  occurring in ,
 is the number of bits used by a dynamic data structure 
to search over  elements in  plus to refer to  
elements in ,  is the time to search 
for an element among  of them in , and 
is the time to insert/delete symbols of  in the structure.
All times are worst-case
\end{theorem}

Using general and dynamic alphabets had not been achieved in previous 
dynamic sequence data structures, because the wavelet has a static shape
(and changing it is costly). These results open the door to using
these solutions in various scenarios where alphabet dynamism is essential.
We examine a few interesting particular cases:

\begin{itemize} 
\item 
We can handle a sequence of arbitrary real numbers in the comparison model,
by using a balanced tree for the alphabet data structure.
If  we have 
times using Theorem~\ref{thm:general} and  worst-case times using
Theorem~\ref{thm:generalwc}. Those complexities are optimal in the comparison
model.
\item 
We can handle a sequence of strings, that is,
 on a general alphabet
. Here we can store the effective set of strings in a data structure 
by Franceschini and Grossi \cite{FG04}, so that operations involving a string 
 take ,
where  is the number of symbols of  actually in use.
With Theorem~\ref{thm:generalwc} we obtain worst-case times 
.
\item
If  is a large integer range, we can obtain time
, or worst-case times 
, and the space increases by  bits, 
by using y-fast tries \cite{Wil83} to handle the alphabet.
\item
Another important particular case is when we maintain a contiguous effective
alphabet , and only insert new symbols . This is the case
where the symbol identities themselves are not important. In this case
there is no time penalty for letting the alphabet grow dynamically.
\end{itemize}

\no{
\subsection{Block Updates}

\section{Applications} \label{sec:app}

Our new results impact in a number of applications that build on dynamic
sequences. We describe several here.
}

\subsection{Dynamic Sequence Collections} 

A landmark application of dynamic
sequences, stressed out in several papers along time
\cite{CHL04,MN06,CHLS07,MN06,LP07,MN08,GN08,LP09,GN09,HM10,NS10}, 
is to maintain a collection  of
texts, where one can carry out indexed pattern matching, as well as inserting
and deleting texts from the collection. Plugging in our new representation we 
can significantly improve the time and space of previous work, with an amortized and with
a worst-case update time, respectively.

\begin{theorem} \label{thm:fmindex}
There exists a data structure for handling a collection  of
texts over an alphabet  within size
 bits, 
simultaneously for all . 
Here  is the length of the concatenation
of  texts, 
  , and we assume that
the alphabet size is .
The structure supports counting of the occurrences
of a pattern  in  time.
After counting, any occurrence can be located in time . 
Any substring of length  from any  in the collection can be displayed 
in time .
Inserting or 
deleting a text  takes  amortized time.
For , for any constant , the 
space simplifies to  bits.
\end{theorem}

\begin{theorem} \label{thm:fmindexwc}
There exists a data structure for handling a collection  of
texts over an alphabet  within size
 bits, 
simultaneously for all . 
Here  is the length of the concatenation
of  texts, 
  , and we assume that
the alphabet size is .
The structure supports counting of the occurrences
of a pattern  in  time.
After counting, any occurrence can be located in time . 
Any substring of length  from any  in the collection can be displayed 
in time .
Inserting or 
deleting a text  takes  time.
For , for any constant , the 
space simplifies to  bits.
\end{theorem}

The theorems refer to , the -th order empirical 
entropy of sequence  \cite{Man01}. This is a lower bound to any
semistatic statistical compressor that encodes each symbol as a function of
the  preceding symbols in the sequence, and it holds  for any .
To offer search capabilities, the Burrows-Wheeler Transform (BWT)
\cite{BW94} of , , is represented, not 
; then  and  operations on  
are used to support pattern
searches and text extractions. K\"arkk\"ainen and Puglisi \cite{KP11} showed 
that, if 
 is split into superblocks of size ,
and a zero-order compressed representation is used for each superblock, the
total bits are .

We use their partitioning, and Theorems~\ref{thm:compr} or
\ref{thm:worstcase} to represent each superblock. 
For Theorem~\ref{thm:fmindex}, the superblock sizes are easily maintained upon 
insertions and deletions of symbols, by splitting and merging superblocks and 
rebuilding the structures involved, without affecting the amortized time per 
operation. They \cite{KP11} also need to manage a table storing the rank of 
each symbol up to the beginning of each superblock. This is arranged, in the 
dynamic scenario, with  partial sum data structures containing 
 elements each, plus another one storing the superblock 
lengths. This adds  bits and  time per 
operation \cite[Lem.~1]{NS10}. Upon blocks splits and merges, we use the
same techniques used for  structures described in Section~\ref{sec:H}.

For Theorem~\ref{thm:fmindexwc} we use the smooth block size management
algorithm described in Section~\ref{sec:indels} for the superblocks, which 
guarantees worst-case times and the same space redundancy. Then partial-sum 
data structures are used without problems.

Finally, the locating and displaying overheads are obtained by marking one
element out of , so that the space overhead of 
 is maintained. Other simpler data structures used in previous
work \cite{MN08}, such as mappings from document identifiers to their position
in  and the samplings of the suffix array, can easily be 
replaced by  time partial-sums data structures and simpler
structures to maintain dictionaries of values \cite[Lem.~1]{NS10}.

\subsection{Burrows-Wheeler Transform} 

Another application
of dynamic sequences is to build the BWT of a text , , within
compressed space, by starting from an empty sequence and inserting each new
character, , , , , at the proper positions. 
Equivalently, this corresponds to initializing an empty collection and then
inserting a single text  using Theorem~\ref{thm:fmindex}. The
result is also stated as the compressed construction of a static FM-index
\cite{FMMN07}, a compressed index that consists essentially of a (static) 
wavelet tree of . Our new representation improves upon the best 
previous result on compressed space \cite{NS10}.

\begin{theorem}
The Alphabet-Friendly FM-index \cite{FMMN07}, as well as the BWT \cite{BW94}, 
of a text  over an alphabet of size , can be built using 
 bits, simultaneously for
all  and any constant , in time
. It can also be built within the same time and 
 bits, for
any alphabet size .
\end{theorem}

We are using Theorem~\ref{thm:fmindex} for the case , and 
Theorem~\ref{thm:compr} to obtain a less alphabet-restrictive result for
 (in this case, we do not split the text into superblocks of 
 symbols, but just use a single sequence). Note that,
although insertion times are amortized in those theorems, this result is
worst-case because we compute the sum of all the insertion times.

This is the first time that  time
complexity is obtained within compressed space. Other
space-conscious results that achieve better time complexity (but more space)
are Okanohara and Sadakane \cite{OS09}, who achieved optimal 
 time within  bits, and
Hon et al.\ \cite{HSS09}, who achieved  time and 
 bits. Older results, like K\"arkk\"ainen's \cite{Kar07}, 
are superseded.

\subsection{Binary Relations}

Barbay et al.\ \cite{BGMR07} show how to represent a binary relation of  pairs
relating  ``objects'' with  ``labels'' by means of a string of
 symbols over alphabet  plus a bitmap of length . The
idea is to traverse the matrix, say, object-wise,
and write down in a string the labels of the pairs found. Meanwhile we append
a 1 to the bitmap each time we find a pair and a 0 each time we move to the
next object. Then queries like: find the objects related to a label, find the
labels related to an object, and tell whether an object and a label are
related, are answered via access, rank and select operations on the string
and the bitmap.

A limitation in the past to make this representation dynamic was that creating
or removing labels implied changing the alphabet of the string. Now we can use
Theorem~\ref{thm:general} to obtain a fully dynamic representation. We 
illustrate the case where labels and objects are contiguous values in integer 
intervals  and , respectively. We note that the structure
on the sequence of labels is so fast that the bitmap, which is longer, dominates
the times.

\begin{theorem} \label{thm:binrel}
A dynamic binary relation consisting of  pairs relating  objects 
with  labels can support the operations of 
counting and listing the objects 
related to a given label, counting and listing the labels related to a given 
object, and telling whether an object and a label are related, all in time 
 per delivered datum. Pairs, objects 
and labels can also be added and deleted in amortized time 
. The space required is 
 bits,
where , where  is the number of objects related to label .
Only labels and objects with no related pairs can be deleted.
\end{theorem}

\begin{theorem} \label{thm:binrelwc}
A dynamic binary relation consisting of  pairs relating  objects 
with  labels can support the operations of 
counting and listing the objects 
related to a given label, counting and listing the labels related to a given 
object, and telling whether an object and a label are related, all in time 
 per delivered datum. Pairs, objects 
and labels can also be added and deleted in time 
. The space required is 
 bits,
where , where  is the number of objects related to label .
Only labels and objects with no related pairs can be deleted.
\end{theorem}

The careful reader may notice that we have uniformized the times of all the
operations for simplicity, yet some can be slightly faster. For example,
listing the  labels related to a given object requires only
 time. Also, obviously, we can
exchange labels and objects if desired.

\subsection{Directed Graphs} 

A particularly interesting and general binary
relation is a directed graph with  nodes and  edges. Our binary relation
representation allows one to navigate a directed graph in forward and backward 
direction, and modify it, within about the space needed by a classical
adjacency list representation, and even less.

\begin{theorem}
A dynamic directed graph consisting of  nodes and  edges 
can support the operations of counting and listing the neighbors pointed from 
a node, counting and listing the reverse neighbors pointing to a node, and 
telling whether there is a link from one node to another, all in time 
 per delivered datum. Nodes and edges 
can be added and deleted in amortized time 
. The space used is 
 bits, 
where 
and  is the outdegree of node .
\end{theorem}

\begin{theorem}
A dynamic directed graph consisting of  nodes and  edges 
can support the operations of counting and listing the neighbors pointed from 
a node, counting and listing the reverse neighbors pointing to a node, and 
telling whether there is a link from one node to another, all in time 
 per delivered datum. Nodes and edges 
can be added and deleted in time 
. The space used is 
 bits, 
where 
and  is the outdegree of node .
\end{theorem}

Note also that we can change ``outdegree'' by ``indegree'' in the theorem by
representing the transposed graph, as operations are symmetric. Our ability
to handle dynamic alphabets is essential here to allow node insertions and
deletions in the graph.

\subsection{Inverted Indexes}

Finally, we consider an application where the symbols are strings. Take a
text  as a sequence of  {\em words}, which are strings over a set of
letters . The alphabet  is integer and fixed, of size . 
The alphabet of  is , and its effective
alphabet is called the {\em vocabulary}  of , of size .
A {\em positional inverted index} is a data structure that, given a word
, returns the positions in  where  appears \cite{BYRN11}.

A well-known way to simulate a positional inverted index within no extra space
on top of the compressed text is to use a compressed sequence representation 
for  (over alphabet ), so that operation  simulates
access to the th position of the list of word , whereas access to the
original  is provided via . Operation rank can be used to 
emulate various inverted index algorithms, particularly for intersections
\cite{BN09}. The space is the zero-order entropy of the text seen as a
sequence of words, which is very competitive in practice \cite{BYRN11}. 
Our new technique
permits modifying the underlying text, that is, it simulates a dynamic
inverted index. For this sake we use Theorem~\ref{thm:general}
and compact tries to handle a vocabulary over a fixed alphabet.

\begin{theorem}
A text of  words with a vocabulary of  words and total length 
over a fixed alphabet  of size  can be represented 
within 
bits of space, where  is
the word-wise entropy of .
The representation outputs any word 
given , finds the position of the th occurrence of any word ,
and tells the number of occurrences of any word  up to position , all
in time . A word  can be inserted or deleted at 
any position in  in amortized time .
\end{theorem}

\begin{theorem}
A text of  words with a vocabulary of  words and total length 
over a fixed alphabet  of size  can be represented 
within 
bits of space, where  is
the word-wise entropy of .
The representation outputs any word 
given , and finds the position of the th occurrence of any word ,
in time .
It tells the number of occurrences of any word  up to position , and
supports the insertion or deletion of any word  in , in time 
.
\end{theorem}

We remark that  and  are assumed to be  for some 
 in information retrieval models \cite{BYRN11}. Under this
assumption the space is just .

Another kind of inverted index, a {\em non-positional} one, relates each word
with the documents where it appears (not to the exact positions). This can be
seen as a direct application of our binary relation representation
\cite{BCN10}, and our dynamization theorems apply to it as well.

\section{Conclusions and Further Challenges} \label{sec:concl}

We have obtained  time for all the operations that handle 
a dynamic sequence on an arbitrary alphabet , matching lower 
bounds that apply to binary alphabets \cite{FS89}, and using zero-order
compressed space. Our structure is faster than the best previous work 
\cite{HM10,NS10} by a factor of  when the
alphabet is larger than polylogarithmic. 
The query times are worst-case, yet the update times are amortized. We also
show that it is possible to obtain worst-case for all the operations, although
times for  and updates raises to .
We also show how to handle general and infinite alphabets.
Our result can be applied to a number of problems and improve previous
upper bounds on those; we have described several ones.

We remark that the lower bounds \cite{FS89} are valid also for amortized times,
so our amortized solution is optimal, yet it is not known whether our
worst-case solution is optimal. Thus the main remaining challenge is whether 
it is possible to attain the optimal  worst-case time 
for all the operations. 

Another interesting challenge is to support a stronger set of update 
operations, such as block edits, concatenations and splits in the sequences.
Navarro and Sadakane \cite{NS10} support those operations within time
. While it seems feasible to achieve, in our 
structure,  time by using blocks of  bits,
the main hurdle is the difficulty of mimicking the same splits and
concatenations on the list maintenance data structures we use 
\cite{IA84,Mor03}.


\bibliographystyle{alpha}
\bibliography{paper}

\end{document}
