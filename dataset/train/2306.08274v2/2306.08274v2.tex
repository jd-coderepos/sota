\section{Experiments}
\label{sec:experiments}


We aim to answer the following questions: 
\begin{itemize}
    \item[\textbf{Q1.}] \textit{How effectively does A2DUG perform on various directed graphs?} 
    \item[\textbf{Q2.}] \textit{How does A2DUG scale well?} 
    \item[\textbf{Q3.}] \textit{To what extent do aggregated features and adjacency lists in directed/undirected graphs affect node classification?}
    \item[\textbf{Q4.}] \textit{Is the model design of A2DUG valid?}
\end{itemize}


\smallskip \noindent \textbf{Datasets.}
To comprehensively evaluate A2DUG with existing methods, we test them on 13 directed graphs with varying the graph sizes.
Table \ref{tb:04_dataset} shows the statistics of datasets.
We use seven small-scale directed graphs:
\begin{itemize}
    \item \texttt{cornell}, \texttt{texas}, and \texttt{wisconsin}\footnote{\url{http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/}} are graphs representing links between
web pages of the corresponding universities. The task is to classify the nodes into one of the five categories, student, project, course, staff, and faculty.
    \item \texttt{citeseer} and \texttt{coraML}\footnote{We use the versions
of these datasets provided in \cite{zhang2021magnet}.} are widely used citation networks. The task is to predict the research fields of nodes. 
    \item \texttt{chameleon} and \texttt{squirrel}~\cite{platonov2023a} are Wikipedia networks on specific topics. The task is to predict the page views. Since a study \cite{platonov2023a} pointed out the presence of a large number of duplicate nodes in the original datasets of \texttt{chameleon} and \texttt{squirrel}, we use the filtered versions that do not have any duplicate nodes.
\end{itemize}
We use three middle-scale directed graphs:
\begin{itemize}
    \item \texttt{genius} \cite{lim2021large} is a subset of the social network on genius.com. Nodes are users, and edges connect users that follow each other on the site. The task is to predict whether nodes are marked ``gone'', which appears to often include spam users.
    \item \texttt{arxiv-year} \cite{lim2021large} is a citation network, where the nodes are arXiv papers and directed edges connect a paper to other papers that it cites. The task is to predict the year at which the paper is posted.
    \item \texttt{ogbn-arxiv} \cite{hu2020ogb} has the same graph structure and node features to \texttt{arxiv-year} but has different classes that indicate the research fields of papers. 
\end{itemize}
Also, we use three large-scale directed graphs: 
\begin{itemize}
    \item \texttt{pokec} \cite{lim2021large} is the friendship graph of a Slovak online social network, where nodes are users and edges are directed friendship relations. Nodes are labeled with reported gender.
    \item \texttt{snap-patents} \cite{lim2021large} is a dataset of utility patents in the US. Each node is a patent, and edges connect patents that cite each other. The task is to predict the time at which a patent was granted. 
    \item \texttt{wiki} \cite{lim2021large} is a dataset of Wikipedia articles, where nodes represent pages and edges represent links between them. The task is to predict page views. 
\end{itemize}

\smallskip\noindent\textbf{Data split. }
We follow the same way to split training/validation/test sets as the papers that proposed the datasets \cite{hu2020ogb,lim2021large,platonov2023a}.
To be concrete, for cornell, texas, and wisconsin, we use the default split (48/32/20 train/val/test) provided in \textit{PyG}\footnote{\url{https://github.com/pyg-team/pytorch_geometric}}. 
For chameleon and squirrel, the study \cite{platonov2023a} that has proposed them provides $10$ different splits. Hence, we utilize five different splits from them in our experiments. 
As for citeseer and coraML, following the rules in \cite{zhang2021magnet}, we choose 20 labels per class for the training set, 500 labels for the validation set, and the rest for the test set. 
As for ogbn-arxiv, the study \cite{hu2020ogb} that has proposed them provides a single data split. 
Hence, we simply utilize it and initialize model parameters by using different random seeds for each run. 
For genius, snap-pantents, pokec, and wiki, the study \cite{lim2021large} that has proposed them  run each method on the same five random 50/25/25 train/val/test splits for each dataset. 
We follow the way to split the datasets. 

\begin{table*}[t]
\caption{Experimental results. Test accuracy is displayed for most datasets, while genius displays test ROC AUC. Standard deviations are calculated from five runs using different random seeds. The two best results per dataset are highlighted. 
In the bottom row, we show the accuracy difference between A2DUG and the best existing method for each dataset. 
(M) denotes some (or all) hyperparameter settings run out of memory and (TO) denotes that the runs do not finish in 24 hours. We show the average rank of each method across all datasets. 
}
\centering
\scalebox{.96}{
\begin{tabular}{l|ccccccc}
\toprule
 & cornell & texas & wisconsin & citeseer & coraML & chameleon & squirrel \\
\midrule
MLP & $75.68_{\pm0.00}$ & $73.51_{\pm3.52}$ & $75.29_{\pm1.75}$ & $52.89_{\pm2.76}$ & $62.23_{\pm2.76}$ & $37.65_{\pm3.10}$ & $35.13_{\pm3.87}$ \\\midrule
GCN & $42.16_{\pm3.08}$ & $51.89_{\pm3.52}$ & $51.76_{\pm2.24}$ & \colorbox{mycolor}{$65.88_{\pm1.50}$} & \colorbox{mycolor}{$79.27_{\pm2.85}$} & $37.17_{\pm3.37}$ & $32.26_{\pm2.96}$ \\
SGC & $41.62_{\pm1.48}$ & $59.46_{\pm0.00}$ & $55.29_{\pm0.88}$ & $62.61_{\pm2.19}$ & $78.68_{\pm2.87}$ & $38.64_{\pm3.79}$ & $38.50_{\pm1.93}$ \\
GPRGNN & $63.24_{\pm3.08}$ & $82.16_{\pm4.10}$ & $74.90_{\pm2.91}$ & $63.00_{\pm1.55}$ & $77.96_{\pm2.75}$ & $38.22_{\pm5.05}$ & $35.03_{\pm1.46}$ \\
FSGNN & \colorbox{mycolor}{$76.22_{\pm1.21}$} & \colorbox{mycolor}{$83.24_{\pm2.26}$} & $76.47_{\pm1.39}$ & \colorbox{mycolor}{$68.56_{\pm0.72}$} & \colorbox{mycolor}{$81.11_{\pm2.05}$} & \colorbox{mycolor}{$44.96_{\pm1.02}$} & \colorbox{mycolor}{$41.12_{\pm2.58}$} \\
ACMGCN & $60.54_{\pm8.02}$ & $71.89_{\pm4.10}$ & $65.88_{\pm5.48}$ & $64.18_{\pm2.31}$ & $77.03_{\pm3.71}$ & $39.44_{\pm4.92}$ & $38.85_{\pm0.56}$ \\\midrule
LINK undirected & $56.76_{\pm4.27}$ & $73.51_{\pm2.26}$ & $41.57_{\pm2.56}$ & $28.67_{\pm4.78}$ & $51.24_{\pm3.37}$ & $41.60_{\pm5.62}$ & $36.31_{\pm1.90}$ \\
LINKX undirected & \colorbox{mycolor}{$76.76_{\pm3.08}$} & $75.14_{\pm4.01}$ & $75.69_{\pm1.75}$ & $47.18_{\pm5.13}$ & $70.10_{\pm1.69}$ & $41.55_{\pm2.45}$ & $40.10_{\pm2.64}$ \\
GloGNN++ undirected & $75.14_{\pm2.26}$ & $78.38_{\pm0.00}$ & \colorbox{mycolor}{$79.22_{\pm1.07}$} & $53.50_{\pm2.55}$ & $63.43_{\pm6.56}$ & $38.49_{\pm9.02}$ & $34.48_{\pm7.13}$ \\\midrule
LINK directed & $49.19_{\pm2.26}$ & $71.89_{\pm3.63}$ & $50.20_{\pm5.48}$ & $20.58_{\pm1.86}$ & $51.67_{\pm5.59}$ & $38.21_{\pm2.82}$ & $33.06_{\pm2.62}$ \\
LINKX directed & $75.14_{\pm2.26}$ & $73.51_{\pm2.26}$ & $77.65_{\pm2.24}$ & $52.51_{\pm1.40}$ & $59.42_{\pm7.85}$ & $39.64_{\pm5.95}$ & $36.00_{\pm1.18}$ \\
GloGNN++ directed & $64.86_{\pm14.93}$ & $78.38_{\pm0.00}$ & \colorbox{mycolor}{$80.39_{\pm0.00}$} & $46.31_{\pm15.95}$ & $69.02_{\pm1.48}$ & $40.07_{\pm3.36}$ & $34.87_{\pm8.12}$ \\ \midrule
DGCN & $56.76_{\pm8.96}$ & $67.03_{\pm7.74}$ & $54.90_{\pm2.40}$ & $64.65_{\pm3.41}$ & $78.05_{\pm3.00}$ & $42.42_{\pm4.60}$ & $41.04_{\pm2.07}$ \\
Digraph & $47.03_{\pm3.63}$ & $42.70_{\pm4.83}$ & $50.98_{\pm0.00}$ & $61.41_{\pm1.95}$ & $73.62_{\pm4.78}$ & $34.29_{\pm0.82}$ & $35.70_{\pm1.60}$ \\
DigraphIB & $48.65_{\pm2.70}$ & $61.08_{\pm4.52}$ & $58.04_{\pm2.63}$ & $59.40_{\pm1.80}$ & $74.62_{\pm3.63}$ & $38.28_{\pm2.69}$ & $33.90_{\pm2.04}$ \\
Magnet & $71.89_{\pm3.08}$ & \colorbox{mycolor}{$83.24_{\pm7.97}$} & $72.94_{\pm3.22}$ & $55.39_{\pm9.23}$ & $76.93_{\pm4.44}$ & $35.16_{\pm3.38}$ & $31.37_{\pm1.46}$ \\ \midrule
\textbf{A2DUG} & $74.59_{\pm1.48}$ & \colorbox{mycolor}{$84.32_{\pm2.96}$} & $77.65_{\pm1.75}$ & $64.55_{\pm3.13}$ & $77.64_{\pm1.71}$ & \colorbox{mycolor}{$42.78_{\pm4.79}$} & \colorbox{mycolor}{$42.28_{\pm2.36}$} \\
Gain over SOTA & $-2.17$ & $+1.08$ & $-2.74$ & $-4.01$ & $-3.47$ & $-2.18$ & $+1.16$ \\ \bottomrule
\end{tabular}
}
\vspace{3mm}

\scalebox{1.}{
\begin{tabular}{l|ccc|ccc|c}
\toprule

{}&genius&arxiv-year&ogbn-arxiv&snap-patents&pokec&wiki&Avg. rank\\
\midrule
MLP & $85.84_{\pm0.88}$ &$36.92_{\pm0.23}$ & $53.78_{\pm0.29}$ &$31.49_{\pm0.07}$ &$62.53_{\pm0.04}$ &$39.74_{\pm0.28}$ & $10.54$\\ \midrule
GCN & $82.23_{\pm3.42}$ &$43.73_{\pm0.22}$ & $69.30_{\pm0.16}$ &$39.99_{\pm0.35}$ & $63.05_{\pm4.23}$ & (M) & $10.38$\\
SGC & $80.08_{\pm2.82}$ &$38.79_{\pm0.22}$ & $67.40_{\pm0.02}$ &$35.26_{\pm0.04}$ &$69.83_{\pm0.32}$ &$45.07_{\pm0.09}$ & $9.77$\\
GPRGNN& $83.98_{\pm0.54}$ &$40.21_{\pm0.33}$ & $67.52_{\pm0.80}$ &$32.44_{\pm0.25}$ &$65.79_{\pm8.59}$ & (M) & $8.69$\\
FSGNN & $88.95_{\pm1.51}$ &$45.99_{\pm0.35}$ & \colorbox{mycolor}{$71.26_{\pm0.33}$} &$45.44_{\pm0.05}$ &$78.21_{\pm1.09}$ &$58.40_{\pm0.26}$ & $3.38$ \\
ACMGCN& $73.16_{\pm8.27}$ &$43.30_{\pm0.90}$ & $67.51_{\pm0.69}$ &$40.07_{\pm0.40}$ &$66.91_{\pm0.66}$ & (M) & $8.69$ \\\midrule
LINK undirected & $69.16_{\pm0.11}$ &$48.43_{\pm0.10}$ & $63.33_{\pm0.04}$ &$49.93_{\pm0.07}$ &$79.17_{\pm0.05}$ &$58.42_{\pm0.04}$ & $9.62$\\
LINKX undirected& $89.27_{\pm1.11}$ &$47.90_{\pm0.20}$ & $61.78_{\pm0.40}$ &$51.40_{\pm0.11}$ &$79.44_{\pm0.13}$ &$61.02_{\pm0.36}$ & $6.08$ \\
GloGNN++ undirected & \colorbox{mycolor}{$90.00_{\pm0.38}$} &$50.55_{\pm0.12}$ & $46.30_{\pm2.74}$ & (M) &\colorbox{mycolor}{$82.66_{\pm0.07}$} & (M) & $7.85$ \\ \midrule
LINK directed & $55.46_{\pm0.11}$ &$51.71_{\pm0.22}$ & $57.17_{\pm0.04}$ &$57.54_{\pm0.07}$ &$71.53_{\pm0.09}$ &$59.69_{\pm0.03}$ & $11.31$ \\
LINKX directed& $88.35_{\pm0.45}$ &$52.61_{\pm0.26}$ & $59.81_{\pm0.43}$ &\colorbox{mycolor}{$61.09_{\pm0.07}$} &$71.88_{\pm0.09}$ &\colorbox{mycolor}{$62.08_{\pm0.14}$} & $6.77$ \\
GloGNN++ directed & $87.82_{\pm0.13}$ &\colorbox{mycolor}{$53.67_{\pm0.32}$} &$55.36_{\pm20.59}$ & (M) &$75.36_{\pm0.07}$ & (M) & $7.54$ \\ \midrule
DGCN& (TO) & (TO) &(TO) &(M) & (M) &(M) & $10.15$\\
Digraph & (M) & (M) &(M) & (M) & (M) &(M) & $13.31$\\
DigraphIB & (M) & (M) &(M) & (M) & (M) &(M) & $12.62$\\
Magnet& $86.68_{\pm2.78}$ &$52.10_{\pm0.32}$ & $68.50_{\pm0.11}$ & (M) &$75.14_{\pm1.59}$ & (M) & $8.69$ \\ \midrule
\textbf{\textsc{A2DUG}}&\colorbox{mycolor}{$89.85_{\pm3.15}$}&\colorbox{mycolor}{$59.14_{\pm0.48}$}&\colorbox{mycolor}{$69.51_{\pm0.24}$}&\colorbox{mycolor}{$72.38_{\pm0.10}$}&\colorbox{mycolor}{$82.55_{\pm0.08}$}&\colorbox{mycolor}{$65.13_{\pm0.07}$} & $2.46$\\
Gain over SOTA & $-0.15$ & $+5.47$ & $-1.75$ & $+11.29$ & $-0.11$ & $+3.05$ &  \\ \bottomrule
\end{tabular}
}
\label{tb:accuracy}
\end{table*} 
\smallskip \noindent \textbf{Baselines. }
For GNNs using feature aggregation in undirected graphs, we use GCN\footnote{\url{https://github.com/tkipf/pygcn}} \cite{kipf2017semi}, SGC\footnote{\url{https://github.com/Tiiiger/SGC}} \cite{wu2019simplifying}, 
GPRGNN\footnote{\url{https://github.com/jianhao2016/GPRGNN}} \cite{chien2021adaptive}, FSGNN\footnote{\url{https://github.com/sunilkmaurya/FSGNN}} \cite{maurya2021improving},
and ACMGCN\footnote{\url{https://github.com/SitaoLuan/ACM-GNN}} \cite{luan2022revisiting}. 
For GNNs for directed graphs, we use DGCN\cite{tong2020directed}, DiGraph \cite{tong2020digraph}, its variant DiGraphIB, and MagNet\footnote{\url{https://github.com/matthew-hirn/magnet}}~\cite{zhang2021magnet}. 
As for methods using adjacency lists as node features, we use LINK~\cite{zheleva2009join}, LINKX\footnote{\url{https://github.com/CUAI/Non-Homophily-Large-Scale}}~\cite{lim2021large}, and GloGNN++\footnote{\url{https://github.com/recklessronan/glognn}}~\cite{li2022finding}. 
We use their official implementations if they are publicly available\footnote{We used the code of LINK which was implemented in LINKX's repository and the codes of DGCN, DiGraph, and DiGraphIB which were implemented in Magnet's repository.}.
We also execute a graph-agnostic classifier, multi-layer perceptron (MLP) as a baseline, i.e., it ignores the topology structure. 
Though we drop several existing methods such as \cite{velickovic2018graph, xu2018powerful, hamilton2017inductive, sign_icml_grl2020, zhu2020beyond} due to the space limitation, we do not fail to use the state-of-the-art methods that show superior performance to them in the papers. 

\smallskip \noindent \textbf{Settings.} 
We report the performance as mean classification accuracy and standard deviation over five random runs with different random seeds. 
As following \cite{lim2021large}, we use ROC-AUC as the metric for the class-imbalanced genius dataset (about $80\%$ of nodes are in the majority class), as it is less sensitive to class-imbalance than accuracy. 
In the methods using adjacency lists (i.e., LINK, LINKX, and GloGNN++), we evaluate both directed and undirected graphs as their inputs\footnote{In their papers, the authors manually chose a directed or undirected graph for each dataset, which obtains better results than another. }.
Following \cite{lim2021large}, we use the AdamW optimizer for gradient-based optimization in all experiments of this paper. 

We use a single NVIDIA A100-PCIE-40GB for all our experiments.
In large-scale datasets, snap-patents, pokec, and wiki, we use minibatch training by setting batchsizes to $n/10, n/10$, and $n/20$, respectively, for SGC, FSGNN, LINK, LINKX, and A2DUG which support minibatch training. 
To efficiently precompute aggregated features on the largest dataset wiki, which does not fit into the GPU memory, we utilize the block-wise
precomputation scheme proposed in \cite{maekawa2022gnn}.

\smallskip\noindent\textbf{Hyperparameters. } 
According to the papers or codebases of existing methods, we select hyperparameters for the search space, e.g., weight decays, learning rates, the number of hidden units, and dropout ratio. 
For all experiments, we choose the best parameter set from these candidates by utilizing Optuna~\cite{akiba2019optuna} for $100$ trials. 
For the reproduction of all experiments in this section,
we report the best set of hyperparameters for each experiment in our codebase. 
Please see README in the codebase, to find the search space and best parameter sets. 



\begin{table*}[t]
    \caption{Comparison on training time [s] (per epoch / total). Note that total training time includes precomputation time for SGC, FSGNN, and \textsc{A2DUG}. SGC, FSGNN, LINK, LINKX, and A2DUG use minibatch training on snap-patent, pokec, and wiki. (M) denotes some (or all) hyperparameter settings run out of memory and (TO) denotes that the runs do not finish in 24 hours. 
    }
    \centering
\scalebox{0.95}{
\begin{tabular}{l|ccccccc}
\toprule
 & cornell & texas & wisconsin & citeseer & coraML & chameleon & squirrel \\
\midrule
MLP & $0.01$ / $5.86$ & $0.01$ / $6.08$ & $0.01$ / $5.76$ & $0.02$ / $6.57$ & $0.02$ / $7.28$ & $0.02$ / $7.03$ & $0.03$ / $7.63$ \\ \midrule
GCN & $0.02$ / $5.92$ & $0.02$ / $5.92$ & $0.02$ / $6.08$ & $0.02$ / $6.20$ & $0.02$ / $6.33$ & $0.02$ / $7.58$ & $0.03$ / $9.25$ \\
SGC & $0.01$ / $5.72$ & $0.01$ / $5.92$ & $0.01$ / $6.02$ & $0.02$ / $6.60$ & $0.02$ / $7.06$ & $0.02$ / $7.34$ & $0.02$ / $8.44$ \\
GPRGNN & $0.02$ / $6.71$ & $0.02$ / $7.74$ & $0.02$ / $6.35$ & $0.03$ / $6.33$ & $0.03$ / $6.67$ & $0.03$ / $7.40$ & $0.03$ / $6.98$ \\
FSGNN & $0.03$ / $7.44$ & $0.03$ / $7.15$ & $0.03$ / $7.97$ & $0.05$ / $9.90$ & $0.04$ / $10.03$ & $0.05$ / $10.95$ & $0.06$ / $12.80$ \\
ACMGCN & $0.03$ / $7.46$ & $0.03$ / $6.73$ & $0.03$ / $7.18$ & $0.03$ / $9.33$ & $0.03$ / $8.35$ & $0.03$ / $8.74$ & $0.04$ / $9.70$ \\ \midrule
LINK undirected & $0.01$ / $6.14$ & $0.01$ / $6.45$ & $0.02$ / $5.88$ & $0.02$ / $6.32$ & $0.02$ / $6.30$ & $0.03$ / $8.09$ & $0.03$ / $8.65$ \\
LINKX undirected & $0.02$ / $6.50$ & $0.02$ / $6.41$ & $0.02$ / $6.40$ & $0.02$ / $6.76$ & $0.02$ / $6.59$ & $0.04$ / $9.09$ & $0.04$ / $10.07$ \\
GloGNN++ undirected & $0.02$ / $8.87$ & $0.02$ / $7.84$ & $0.03$ / $8.01$ & $0.03$ / $6.97$ & $0.04$ / $17.43$ & $0.03$ / $11.55$ & $0.04$ / $11.33$ \\ \midrule
LINK directed & $0.02$ / $6.12$ & $0.01$ / $6.39$ & $0.02$ / $6.07$ & $0.02$ / $6.13$ & $0.02$ / $6.61$ & $0.02$ / $6.70$ & $0.02$ / $7.39$ \\
LINKX directed & $0.02$ / $6.36$ & $0.02$ / $6.78$ & $0.02$ / $6.34$ & $0.02$ / $6.60$ & $0.02$ / $6.34$ & $0.03$ / $8.27$ & $0.03$ / $7.39$ \\
GloGNN++ directed & $0.03$ / $7.70$ & $0.02$ / $7.60$ & $0.03$ / $8.49$ & $0.03$ / $10.02$ & $0.03$ / $9.32$ & $0.03$ / $10.10$ & $0.04$ / $14.88$ \\ \midrule
DGCN & $0.03$ / $7.09$ & $0.03$ / $7.82$ & $0.03$ / $6.83$ & $0.03$ / $12.89$ & $0.03$ / $13.51$ & $0.03$ / $11.23$ & $0.04$ / $27.55$ \\
Digraph & $0.01$ / $6.26$ & $0.02$ / $5.89$ & $0.02$ / $6.60$ & $0.02$ / $6.33$ & $0.02$ / $6.45$ & $0.02$ / $6.56$ & $0.02$ / $7.59$ \\
DigraphIB & $0.02$ / $6.18$ & $0.02$ / $6.29$ & $0.02$ / $6.42$ & $0.02$ / $6.08$ & $0.02$ / $6.92$ & $0.02$ / $6.58$ & $0.03$ / $7.26$ \\
Magnet & $0.04$ / $8.20$ & $0.04$ / $7.34$ & $0.05$ / $8.20$ & $0.07$ / $9.19$ & $0.09$ / $10.99$ & $0.05$ / $8.72$ & $0.08$ / $9.51$ \\ \midrule
\textbf{A2DUG} & $0.04$ / $8.43$ & $0.04$ / $7.94$ & $0.04$ / $8.08$ & $0.08$ / $17.71$ & $0.07$ / $21.33$ & $0.08$ / $14.13$ & $0.13$ / $21.23$ \\
\bottomrule
\end{tabular}
}

\vspace{2mm}

\scalebox{0.95}{
\begin{tabular}{l|ccc|ccc}
\toprule

{}&genius&arxiv-year&ogbn-arxiv&snap-patents&pokec&wiki\\
\midrule
MLP & $1.14$ / $64.94$ & $0.28$ / $85.14$ &$0.43$ / $148.84$ &$4.79$ / $439.55$ & $5.37$ / $1862.87$ &$3.42$ / $711.87$ \\\midrule
GCN &$1.47$ / $162.67$ &$0.39$ / $217.17$ &$0.55$ / $264.27$ & $4.57$ / $2461.39$ & $4.42$ / $1894.87$ &(M) \\
SGC & $1.04$ / $87.46$ &$0.26$ / $101.32$ &$0.39$ / $218.24$ & $5.69$ / $1005.80$ & $4.80$ / $2593.40$ & $3.37$ / $1248.20$ \\
GPRGNN& $1.09$ / $70.06$ &$0.31$ / $107.89$ &$0.51$ / $122.57$ &$4.41$ / $628.06$ & $3.97$ / $1580.53$ &(M) \\
FSGNN &$1.28$ / $189.42$ &$0.44$ / $215.82$ &$0.58$ / $285.87$ & $8.88$ / $4868.19$ & $5.31$ / $2862.49$ &$10.65$ / $5555.48$ \\
ACMGCN& $1.34$ / $86.94$ &$0.46$ / $225.45$ &$0.63$ / $334.94$ & $4.35$ / $2369.63$ & $4.83$ / $2643.42$ &(M) \\\midrule
LINK undirected & $1.27$ / $91.71$ & $0.41$ / $25.79$ &$0.48$ / $32.94$ & $10.59$ / $508.43$ &$5.31$ / $251.76$ &$11.84$ / $1280.93$ \\
LINKX undirected&$1.53$ / $177.23$ & $0.50$ / $32.95$ &$0.56$ / $47.10$ & $12.92$ / $592.94$ &$5.65$ / $299.37$ &$13.29$ / $2005.72$ \\
GloGNN++ undirected &$1.46$ / $136.71$ & $0.48$ / $58.40$ & $0.81$ / $258.42$ &(M) &$5.47$ / $847.04$ &(M) \\ \midrule
LINK directed & $1.12$ / $68.16$ & $0.65$ / $38.64$ &$0.71$ / $50.14$ & $10.87$ / $719.02$ &$6.38$ / $300.13$ &$11.18$ / $1306.04$ \\
LINKX directed&$1.68$ / $209.04$ & $0.64$ / $63.96$ &$0.78$ / $51.99$ & $12.96$ / $656.74$ &$7.81$ / $421.38$ &$12.31$ / $1616.59$ \\
GloGNN++ directed &$1.70$ / $212.61$ & $0.33$ / $62.49$ &$0.63$ / $268.37$ & (M) & $5.04$ / $1221.02$ &(M) \\ \midrule
DGCN& (TO) & (TO) &(TO) &(M) & (M) &(M)\\
Digraph & (M) & (M) &(M) & (M) & (M) &(M)\\
DigraphIB & (M) & (M) &(M) & (M) & (M) &(M)\\
Magnet&$1.31$ / $159.55$ &$0.60$ / $233.67$ &$0.74$ / $876.22$ &(M) &$5.92$ / $10112.13$ &(M) \\\midrule
\textbf{A2DUG} &$1.43$ / $357.05$ &$0.78$ / $113.27$ &$0.81$ / $180.28$ &$27.97$ / $2997.52$ & $9.92$ / $3293.78$ &$50.53$ / $5129.92$ \\

\bottomrule
\end{tabular}
}
\label{tb:efficiency}
\end{table*} 
\begin{table*}[t]
    \caption{Ablation study of A2DUG.
    We highlight the best score on each dataset. 
    }
    \centering

\begin{tabular}{l|ccccccc}
\toprule
 & cornell & texas & wisconsin & citeseer & coraML & chameleon & squirrel \\
\midrule
\textbf{A2DUG} & \colorbox{mycolor}{$74.59_{\pm1.48}$} & $84.32_{\pm2.96}$ & $77.65_{\pm1.75}$ & $64.55_{\pm3.13}$ & $77.64_{\pm1.71}$ & $42.78_{\pm4.79}$ & \colorbox{mycolor}{$42.28_{\pm2.36}$} \\\midrule
wo directed & $72.97_{\pm1.91}$ & $86.49_{\pm4.68}$ & $74.90_{\pm2.15}$ & \colorbox{mycolor}{$65.22_{\pm2.72}$} & \colorbox{mycolor}{$80.85_{\pm2.33}$} & $41.38_{\pm2.68}$ & $35.68_{\pm9.95}$ \\
wo undirected & $69.73_{\pm4.01}$ & \colorbox{mycolor}{$87.57_{\pm3.08}$} & \colorbox{mycolor}{$80.00_{\pm0.88}$} & $55.42_{\pm2.73}$ & $71.00_{\pm2.25}$ & $40.83_{\pm7.91}$ & $40.87_{\pm2.98}$ \\
wo aggregation & $72.43_{\pm7.74}$ & $77.84_{\pm2.96}$ & $76.47_{\pm3.67}$ & $54.70_{\pm1.93}$ & $71.85_{\pm1.45}$ & $43.45_{\pm1.68}$ & $40.82_{\pm1.80}$ \\
wo adjacency & $72.97_{\pm3.31}$ & $82.16_{\pm2.42}$ & $66.27_{\pm0.88}$ & $64.64_{\pm1.76}$ & $78.79_{\pm1.90}$ & \colorbox{mycolor}{$44.79_{\pm2.14}$} & $41.93_{\pm2.10}$ \\
wo transpose & $68.65_{\pm4.10}$ & $84.86_{\pm4.10}$ & $72.16_{\pm1.64}$ & $64.19_{\pm1.79}$ & $79.35_{\pm2.17}$ & $40.73_{\pm8.90}$ & $41.39_{\pm1.97}$ \\
\bottomrule
\end{tabular}

\vspace{3mm}
    

\begin{tabular}{l|ccc|ccc}
\toprule
{}&genius&arxiv-year&ogbn-arxiv&snap-patents&pokec&wiki\\
\midrule
\textbf{\textsc{A2DUG}}&$89.85_{\pm3.15}$&$59.14_{\pm0.48}$&$69.51_{\pm0.24}$&\colorbox{mycolor}{$ 72.38_{\pm0.10}$}&$82.55_{\pm0.08}$&\colorbox{mycolor}{$65.13_{\pm0.07}$}\\ \midrule
wo directed&\colorbox{mycolor}{$90.96_{\pm0.10}$}&$51.46_{\pm0.28}$&\colorbox{mycolor}{$70.54_{\pm0.12}$}&$55.09_{\pm0.10}$&\colorbox{mycolor}{$83.02_{\pm0.06}$}&$63.11_{\pm0.09}$\\
wo undirected&$89.32_{\pm1.17}$&\colorbox{mycolor}{$60.13_{\pm0.45}$}&$65.80_{\pm0.23}$&$71.60_{\pm0.09}$&$81.14_{\pm0.04}$&$64.33_{\pm0.19}$\\
wo aggregation&$89.24_{\pm0.32}$&$54.88_{\pm0.36}$&$66.64_{\pm0.05}$&$62.97_{\pm0.40}$&$81.18_{\pm0.07}$&$64.57_{\pm0.17}$\\
wo adjacency&$90.65_{\pm1.72}$&$56.88_{\pm0.19}$&$69.54_{\pm0.91}$&$70.24_{\pm0.16}$&$78.42_{\pm2.64}$&$57.42_{\pm1.82}$\\
wo transpose&$89.63_{\pm0.58}$&$57.31_{\pm0.29}$&$70.29_{\pm0.27}$&$69.09_{\pm0.15}$&$81.85_{\pm0.11}$&$64.81_{\pm0.19}$\\
\bottomrule
\end{tabular}
\label{tb:ablation}
\end{table*} 
\subsection{Q1. Effectiveness}
\label{ssec:effectiveness}

\smallskip \noindent \textbf{A2DUG achieves promising results across all datasets. } 
To answer the first question, we benchmark the performance of existing methods and A2DUG in various directed graphs in Table \ref{tb:accuracy}. 
The table shows the performance stability of A2DUG in various datasets. 
Despite its simple architecture, \textsc{A2DUG} outperforms other state-of-the-art methods with large margins in arxiv-year, snap-patents, and wiki, while the relative performance drop over the best method is less than $6\%$. 
The obtained findings suggest the imperative need to control the effects stemming from aggregated features and adjacency lists in both directed/undirected graphs for optimal performance across the three datasets. 
Notably, A2DUG stands out as the sole method capable of adaptively controlling these effects, rather than merely selecting pivotal features for classification. 
Consequently, A2DUG demonstrates strong performance in this regard.

\noindent\textbf{No existing method stably achieves the state-of-the-art classification quality across various datasets. }
For example, FSGNN perform well on small-scale commonly used datasets but shows significant accuracy drops in arxiv-year, snap-patents, and wiki compared with the state-of-the-art performance. This indicates that aggregated features in undirected graphs are important signal for correctly predicting node labels in small-scale commonly used datasets. 
In contrast, for genius, arxiv-year, snap-patents, pokec, and wiki proposed in a recent work \cite{lim2021large}, the results imply that aggregated features are not enough signal for predicting node labels. 
Another example is GloGNN++ undirected which achieves the best results on genius and pokec but struggles in homomorphic graphs such as citeseer, coraML, and ogbn-arxiv. 


These results validate that an appropriate selection of node representations and edge direction awareness is necessary to obtain state-of-the-art classification quality.
In summary, we observe that A2DUG stably perform well in various graphs, whereas no single existing method stably obtains state-of-the-art classification results. 



\subsection{Q2. Scalability}
\label{ssec:performance_A2DUG}
Table \ref{tb:efficiency} shows the training time of existing methods and \textsc{A2DUG}. 

\smallskip \noindent \textbf{A2DUG scales well to the largest dataset wiki, which has 0.3 billion edges. }
In small-scale and middle-scale datasets, A2DUG runs within comparable time with existing methods in terms of both time per epoch and total time. 
Since A2DUG utilizes only MLPs in the model architecture\footnote{The time per epoch correlates with the number of MLP layers, which we adjust as a hyperparameter to optimize node classification quality. However, we observe that this adjustment does not markedly impact the training time.}, it does not require a significant increase in training time compared with other methods. 
Also, it can adopt minibatch training and thus scales well to large-scale datasets. 
In large-scale datasets, \textsc{A2DUG} needs longer training time per epoch than other methods because it combines both aggregated features and adjacency lists in directed/undirected graphs\footnote{In scenarios where the graph size is substantial, there is a modest increase in the time required to transfer matrices from the CPU memory to the GPU memory for each minibatch. This aspect slightly extends the time per epoch for A2DUG, which is attributed to its utilization of a larger set of node representations, enhancing the method's comprehensiveness.}. 


The GNNs for directed graphs DGCN, DiGraph, and DiGraphIB do not scale to middle-scale graphs.
DGCN \cite{tong2020directed} calculates the second-order proximity matrices, which requires unacceptable computational cost $O(n^2)$. DiGraph and DiGraphIB \cite{tong2020digraph} calculate the eigenvalue decomposition of an adjacency matrix, which requires $O(n^3)$. These procedures are impractical even for middle-scale graphs.

In summary, A2DUG demonstrates strong performance on diverse datasets without imposing a significant time overhead. 

\begin{table*}[t]
    \caption{Comparison of the original A2DUG and its extension with learnable layer selection parameters. 
    }
    \centering
\begin{tabular}{l|ccc|ccc}
\toprule
 & genius & arxiv-year & ogbn-arxiv & snap-patents & pokec & wiki \\
\midrule
\textbf{A2DUG} & $89.85_{\pm3.15}$ & $59.14_{\pm0.48}$ & $69.51_{\pm0.24}$ & $72.38_{\pm0.10}$ & $82.48_{\pm0.10}$ & $65.13_{\pm0.07}$ \\
w layer selection & $90.22_{\pm0.32}$ & $60.18_{\pm0.16}$ & $69.52_{\pm0.22}$ & $72.20_{\pm0.06}$ & $82.67_{\pm0.11}$ & $64.80_{\pm0.08}$ \\
\bottomrule
\end{tabular}
\label{tb:layer_selection}
\end{table*}

\subsection{Q3. Ablation Study}
\label{ssec:ablation}
To investigate the performance contributions of aggregated features and adjacency lists in directed/undirected graphs, we conduct ablation studies with the variants removing directed graphs (``wo directed''), undirected graphs (``wo undirected''), aggregated features (``wo aggregation''), adjacency lists (``wo adjacency''), and transposed directed graphs (``wo transpose''). 
Table \ref{tb:ablation} shows the node classification results of the variants compared with \textsc{A2DUG}. 

\smallskip \noindent \textbf{Several datasets require all the combinations to achieve the state-of-the-art results. }
In cornell, squirrel, snap-patents, and wiki, the original \textsc{A2DUG} achieves the best classification results. 
This validates our assumption that aggregated features and adjacency lists in directed/undirected graphs complement each other. 


\smallskip \noindent \textbf{The importance of aggregated features and adjacency lists depends on the characteristics of datasets. }
The variant ``wo aggregation'' obtains better results than ``wo adjacency'' in pokec and wiki. 
This means that adjacency lists play more important roles than aggregated features in these datasets. 
In contrast, the accuracy of ``wo adjacency'' is better than that of ``wo aggregation'' in snap-patent. 
This indicates that the importance of aggregated features and adjacency lists also depends on datasets. 
These observations validate that the adaptive effect control on aggregated features and adjacency lists is required. 

\smallskip \noindent \textbf{The importance of edge directions depends on the characteristics of datasets. }
In citeseer, coraML, genius, ogbn-arxiv, and pokec, the variant ``wo directed'' achieves the best results.
This implies that the information from undirected graphs is important for predicting the labels in these datasets. 
Also, directed graphs even work as noise in these datasets because the results of ``wo directed'' are slightly better than the original.
In contrast, in arxiv-year, the variant ``wo undirected'' achieves the best result. 
Even though the graph structure and node features of ogbn-arxiv are exactly the same as arxiv-year, interestingly, the importance of directed and undirected graphs is different due to the difference in their prediction targets, i.e., research fields and publication years, respectively. 
These observations clarify that the adaptive effect control on directed and undirected graphs is effective. 


\smallskip \noindent \textbf{The transposed edges give an additional signal to classification. }
In most datasets, we observe that ``wo transpose'' obtains lower results than A2DUG.
This validates the effectiveness of using inverse edges in directed graphs.
Taking arxiv-year as an example, we conjecture that it is not only important which papers the paper cites but also which papers it is cited in, to correctly predict the publication year.

Consequently, all the combinations of node representations and edge direction are necessary to stably achieve the state-of-the-art classification performance across diverse datasets. 


\subsection{Q4. Investigation on Model Design}
\label{ssec:model_validity}
While we simply concatenate all the combinations of node representations as shown in Eq. \eqref{eq:prediction}, it is a natural extension to introduce layer selection parameters that are expected to work as a soft selection of which node representation contributes to classification performance.
To validate the model design of A2DUG, we also develop and compare the extension that is formulated below:
\begin{align}
\label{eq:attention}
    \bmY = \text{MLP}_\text{final}\Bigl(\sigma(\alpha_0\bmH_{\bmX}\|\alpha_1\bmH_{\bmA}\|\alpha_2\bmH_{\bmA^{\top}}\|\alpha_3\bmH_{\bmA^\text{und}}\nonumber\\ 
    \|\alpha_4\bmH_\text{GNN}\|\alpha_5\bmH_{\text{GNN}^{\top}}\|\alpha_6\bmH_{\text{GNN}^\text{und}})\Bigr),
\end{align}
where $\alpha_0, \dots, \alpha_6$ are learnable layer selection parameters controlling the effects from node representations, which satisfy $\sum_{i=0}^6\alpha_i=1$. 

For this experiment, we use the same datasets and settings as Section \ref{ssec:effectiveness} and \ref{ssec:performance_A2DUG} and tune the hyperparameters of the extension. 
Table \ref{tb:layer_selection} shows the results comparing A2DUG and the extension (``w layer selection'').
The table demonstrates that there is no significant difference between A2DUG and the extension. 
This is because the final MLP layers of A2DUG can adaptively control the effects from node representations without the layer selection parameters as shown in Section \ref{ssec:performance_A2DUG}. 
Since the layer selection parameters do not offer benefits but complicate the model architecture, i.e., increase the number of learnable parameters, we use the simplest formulation shown in Eq. \eqref{eq:prediction} as our proposal A2DUG in this paper.
We leave the further exploration for the optimal combinations onto our future work.
