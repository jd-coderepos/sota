
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphics} \usepackage{epsfig} \usepackage{mathptmx} \usepackage{times} \usepackage{amsmath} \usepackage{amssymb}  \usepackage{graphicx}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{bbold}
\usepackage{pifont}\usepackage{colortbl}
\usepackage{tabularray}
\usepackage{subcaption}
\usepackage[export]{adjustbox}
\usepackage{float}
\usepackage{multirow}
\usepackage{array}
\usepackage{textcomp}
\usepackage{indentfirst}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{hhline} 
\usepackage{ragged2e}
\usepackage{booktabs}
\usepackage{lipsum}

\newcommand{\muz}[1]{{\textcolor{red}{#1}}}
\renewcommand*\ttdefault{cmvtt}
\newcommand{\txt}[1]{{\texttt{#1}}}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\dec}[1]{\ensuremath{_{\text{\textcolor{darkred}{(-#1)}}}}}
\newcommand{\inc}[1]{\ensuremath{_{\text{\textcolor{blue}{(+#1)}}}}}

\title{VideoPrompter: An Ensemble of Foundational Models for Zero-Shot Video Understanding}

\author{
  \textbf{Adeel Yousaf}\textsuperscript{1},
  \textbf{Muzammal Naseer}\textsuperscript{2},
  \textbf{Salman Khan}\textsuperscript{2,3},
  \textbf{Fahad Shahbaz Khan}\textsuperscript{2,4},
  \textbf{Mubarak Shah}\textsuperscript{1}
  \\
\hspace{0.9cm}\textsuperscript{1} Center for Research in Computer Vision Lab, University of Central Florida, USA\\
\textsuperscript{2} Mohamed bin Zayed University of AI \hspace{0.15cm}
\textsuperscript{3} Australian National University \hspace{0.15cm}
\textsuperscript{4} Linkoping University
}







\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}




Vision-language models (VLMs) classify the query video by calculating a similarity score between the visual features and text-based class label representations. Recently, large language models (LLMs) have been used to enrich the text-based class labels by enhancing the \emph{descriptiveness} of the class names. However, these improvements are restricted to the text-based classifier only, and the query visual features are not considered.
In this paper, we propose a framework which combines pre-trained discriminative VLMs with pre-trained generative video-to-text and text-to-text models.  We introduce two key modifications to the standard zero-shot setting. First, we propose language-guided visual feature enhancement and employ a video-to-text model to convert the query video to its descriptive form. The resulting descriptions contain vital visual cues of the query video, such as what objects are present and their spatio-temporal interactions. These descriptive cues provide additional semantic knowledge to VLMs to enhance their zero-shot performance. Second, we propose video-specific prompts to LLMs to generate more meaningful descriptions to enrich class label representations.  Specifically, we introduce prompt techniques to create a Tree Hierarchy of Categories for class names, offering a higher-level action context for additional visual cues, We demonstrate the effectiveness of our approach in video understanding across three different zero-shot settings: 1) video action recognition, 2) video-to-text and text-to-video retrieval, and 3) time-sensitive video tasks. Consistent improvements across multiple benchmarks and with various VLMs demonstrate the effectiveness of our proposed framework. Our code will be made publicly available.


\begin{figure}[!htb]
    \makebox[\textwidth][c]{\includegraphics[width=0.95\linewidth]{figures/figure_2_evolution_try9.pdf}
    }
    \caption{\textbf{(a)} The standard pre-training for zero-shot classification (e.g., CLIP 
    \citep{radford2021learning}). \textbf{(b)} Existing variants for enhancing zero-shot classification \citep{pratt2022does,menon2022visual} using GPT descriptions and attributes that improve text-based classifier features. \textbf{(c)} Our proposed framework to enhance both classifier and visual representations. It employs a video-to-text model to generate description of the query video, and these descriptive cues are combined with the visual information. A text-to-text generative model (GPT-3.5) is prompted for class attributes, descriptions, and action context to enhance the class diversity for the text-based classifier.}\label{fig:introfig}
    \vspace{-1.5em}
\end{figure}




\end{abstract}







\section{Introduction}
Open-vocabulary models \citep{roth2023waffling,radford2021learning, jia2021scaling,yuan2021florence} have demonstrated impressive performance in downstream tasks. These models undergo contrastive training on large amounts of image-text pairs, aligning their embeddings in a shared space. However, extending these models to video tasks poses significant challenges mainly for two reasons: (1) due to large computational expenses, and (2) the requirement of gathering large-scale video-text pairs. Therefore, it is critical to effectively leverage pre-trained image-language models for video tasks without affecting their zero-shot capabilities.

To extend pre-trained image-language models to videos, there are two dominant approaches. The first approach takes inspiration from the prompt learning methods \citep{zhou2022learning,zhou2022conditional,jia2022visual} and introduces learnable prompts or adapters to text, vision, or both sides \citep{yang2023aim,wasim2023vita}. In contrast, the second approach fine-tunes the whole pre-trained image-language model for video tasks \citep{Rasheed_2023_CVPR,luo2022clip4clip,wang2021actionclip,ni2022expanding}.  The aforementioned approaches have several drawbacks, e.g., the introduction of additional learnable parameters that add to overall model complexity or require extensive fine-tuning to adapt the model for video tasks. Further, these methods require access to the true distribution of the target task, which can be prohibitive in test-time adaptation and data-scarce environments.


Recently, a new line of work has emerged \citep{menon2022visual,pratt2022does,novack2023chils,roth2023waffling} that incorporates large language models (LLMs), such as GPT-3 to provide additional semantic context to existing vision-language models (VLMs) and requires no further training. These methods query LLMs to replace class names with enriched language descriptors in order to increase class descriptiveness. However, these studies have primarily focused on modifying the text-based classifier only, we propose a twofold improvement approach, simultaneously refining class representations and enriching visual features. Moreover, despite their promising results in image classification, \emph{the applicability of these method in the context of video understanding remains an open question, and our work aims to address this gap}.

We present a framework called VideoPrompter that aims to enhance the test-time zero-shot performance of the existing VLMs for video understanding and introduce two modifications to the standard zero-shot framework. \emph{First,} we query a video-to-text generative model to convert the input video to language representation, as this generated representation contains vital visual cues (\emph{such as what objects are present and how they interact spatially and temporally}), which in turn helps the VLMs to better understand the given video. For instance, for a video shown in Figure \ref{fig:introfig}, the video-to-text model can provide detailed textual description e.g., ``\emph{in this video, a woman is seen applying mascara using a makeup brush"}. As humans, we can effortlessly leverage such descriptive information and form a visual image of the video's content in our minds. \emph{Second,} to enrich the class representations of the text classifier, we query LLM with video-specific prompts and generate two types of video-specific descriptors: 1) language attributes and, 2) language descriptions. Moreover, we found that action recognition benchmarks such as UCF-101, HMDB-51 and Kinetics \citep{kuehne2011hmdb, soomro2012ucf101,kay2017kinetics} can be divided into a Tree Hierarchy of Categories by using LLM to provide high-level action context. For instance, all sports-related actions (basketball, cricket, baseball) can be added under one high-level action context, i.e., ``playing sports". This high-level action context provides additional cues to further enrich the class representations. For example,  for the action class ``hopscotch", our framework generates following language descriptors:
{\fontfamily{qcr}\selectfont\small
\begin{itemize}\setlength{\itemsep}{0em}
        \item \textbf{parent context:} playing sports.
        \item \textbf{language attributes:} grid markings, hopping, throwing an object, jumping.
        \item \textbf{language descriptions:} a child is playing hopscotch, they hop in a specific pattern, throwing a small object into numbered squares and then taking turns to retrieve it while maintaining balance.
\end{itemize}
}

To summarize, we make the following contributions:
\begin{enumerate}
        \item We introduce a framework that is an ensemble of video-to-text and text-to-text generative models to increase the zero-shot performance of existing VLMs for video understanding.        
        \item We introduce video-specific prompts to enrich the classifier representations and also propose a novel way to generate high-level action contexts from the dataset.
        \item Our framework offers a plug-and-play module adaptable to various existing VLMs such as CLIP \citep{radford2021learning}, ViFi-CLIP \citep{Rasheed_2023_CVPR}, Action-CLIP \citep{wang2021actionclip}, and AIM \citep{yang2023aim}.
        \item We demonstrate results on three different video settings namely: action recognition, video-to-text, and text-to-video retrieval, and time-aware video tasks, and show results and ablation on 7 different datasets.
\end{enumerate}







































\section{VideoPrompter}


\subsection{Preliminaries}
Let  denote the query video and  denote the target categories. Let  and  respectively be the visual and text encoders of a VLM such as CLIP. The zero-shot video classification can be defined as nearest neighbor retrieval as follows:


with prompt  \texttt A photo of a \{c\},   represents cosine similarity between visual and textual features.

Let   represent a video-to-text conversational model that converts the input query video  to its corresponding video textual description. The generated description embeds vital visual cues about the objects and the action being performed by the objects in the video. This video textual description is then processed by the VLM text encoder, , and fused together with the video features to get the enriched visual features .


Let   denote an LLM model that converts the target categories  to corresponding language attributes and descriptions. These language attributes and descriptions provide additional semantic context to enrich class label representations. These class label descriptions are then processed by the VLM text encoder, , to enhance class label features, . Our proposed zero-shot classification can then be defined as follows:

In the following sections, we examine how the visual features  and the enriched class representations  are derived. For clarity, the descriptions generated by the video-to-text model \citep{maaz2023video} are referred to as video textual descriptions. On the other hand, attributes and descriptions generated by LLM \citep{brown2020language} are called language attributes and language descriptions.

\subsection{Video-to-text Guided Visual Feature Enhancement}
\label{subsec:Video-to-text Guided Visual Feature Enhancement}
Given a video-language model, we aim to enhance its zero-shot performance by employing a video-to-text conversational model, i.e., Video-ChatGPT (VGPT) \citep{maaz2023video}, to analyze the video content. We prompt VGPT as ``\emph{describe the activity in the video}". VGPT leverages its spatiotemporal alignment between BLIP \citep{li2022blip} and an LLM \citep{chiang2023vicuna} to capture temporal dynamics and frame-to-frame consistency relationships, allowing it to generate coherent video textual descriptions of the events unfolding in the video. These video textual descriptions embed vital spatial and temporal information about the video events. The generated video textual descriptions are passed through the VLM text encoder  to generate a video description-level embedding.

Since the VLMs share a common image-text embedding space due to their contrastive learning objective \citep{radford2021learning}, we found a simple weighted average of the video embedding and video textual description embedding to be efficient. The enhanced visual embedding  is given as:




where  denotes the weights for the query video and video textual description embeddings, respectively.  




\subsection{Text-to-Text Guided Classifier Refinement}


In our approach, we leverage an LLM, \citep{brown2020language} and generate video-specific descriptors. We found that video-specific descriptors can better adapt to the action-recognition datasets, and unlike \citep{pratt2022does}, only a small number of descriptors are required. For example, our framework only requires three descriptors, unlike 50 for \citep{pratt2022does} in the case of the UCF-101 dataset \citep{soomro2012ucf101} \ref{Comparison with CUPL}.
We arrange video action datasets in high-level action contexts.
For instance, all sports-related actions (basketball, cricket, baseball) can be added under one high-level action context, i.e., ``playing sports". We propose to employ LLM to exploit this property of action-recognition datasets and generate high-level action context to provide additional cues to the classifier. In summary, we extract video-specific language descriptions (Sec. \ref{subsubsec:Video-Specific Language Descriptors}) from an LLM with high-level action context (Sec. \ref{subsubsec: Integrating High-Level Video Action Context}). 

\begin{figure}[!t]
    \makebox[\textwidth][c]{\includegraphics[width=1.0\linewidth]{figures/ICLR_figure_3_analysis1_try3.pdf}
    }
\caption{VideoPrompter's Interpretability. We divide our proposed attributes (section \ref{subsubsec:Video-Specific Language Descriptors}) and get the cosine similarity between the individual attribute and query video to find the contribution of each attribute. For example, the top row shows the prediction of our VideoPrompter is ``golf" and the reason the model made this decision is because of \emph{golf-course} in the background, \emph{golf-ball} in the scene, and \emph{swing} of the golf-stick. Similarly, the bottom row shows the \emph{upside down position} played a vital role in the model to make the decision that it's a ``handstand" action.}
    \label{fig:VideoPromper's Interoperability}
    \vspace{-1em}
\end{figure}

\subsubsection{Video-Specific Language Descriptors}
\label{subsubsec:Video-Specific Language Descriptors}
For action recognition, we propose to leverage an LLM, which is GPT-3.5 in our case \citep{brown2020language} to take a class name as input and generate two types of video-specific language descriptors: 
\begin{enumerate}
    \item  \textbf{Language Attributes} offer object-level visual cues to encourage the classifier to employ these features instead of just the class names. For instance, for the video action of ``baby-crawling" the generated language attributes are: \emph{baby, crawling, hand, knees}. We prompt GPT-3.5 as:
     \begin{quotation}
\centering \begin{minipage}{0.9\linewidth} {\texttt{Q: What are the distinct visual characteristics to identify a {\{class-name}\} video action?}} 
    \newline
    
\end{minipage}
\end{quotation}
   
    \item  \textbf{Language Descriptions} describe how a specific action is performed. It includes step-by-step directions for that action to facilitate the model's comprehension of the temporal context. For instance, for the video action of ``baby-crawling" the generated language description is: \emph{a baby is seen on all fours, using their arms and knees to move across the floor. They alternate their arms and legs in a crawling motion, exploring their surroundings with curiosity}. We prompt GPT-3.5 as:
     \begin{quotation}
\centering \begin{minipage}{0.9\linewidth} {\texttt{Q: How {\{class-name}\} action is performed visually? }} 
\end{minipage}
\end{quotation}
    
\end{enumerate}

The text encoder  extracts the embeddings of such generated language attributes and language descriptions. The modified class label representation   is the average of the embeddings of language attributes and language descriptions.


In problem setting of video-to-text or text-to-video retrieval, each video is paired with a corresponding caption. We employ GPT-3.5 to take the input caption and generate informative and semantically similar captions. For instance, for an input caption, ``man is giving a review on a vehicle", the generated caption is ``\emph{a person provides feedback on a car}". Similarly, for an input caption ``baseball player hits ball", the generated caption is ``\emph{the ball is hit by a player in a baseball game"}. We prompt GPT-3.5 as:
     \begin{quotation}
\centering \begin{minipage}{0.9\linewidth} {\texttt{Q: Given a caption: {\{input caption}\}, generate a visually similar captions.}} 


\end{minipage}
\end{quotation}

The text encoder  extracts the embeddings of the generated captions. The modified class label representation   is the average of the embeddings of the original and generated captions.



\subsubsection{Integrating High-Level Video Action Context} 
\label{subsubsec: Integrating High-Level Video Action Context}


We proposed a novel way of querying LLMs and divide the dataset into various \emph{high-level action contexts}  such that all video-action classes semantically close to each other are grouped under one high-level action context. We prompt - GPT-3.5 as:
     \begin{quotation}
\centering \begin{minipage}{0.9\linewidth} {\texttt{Q: Divide the list of {\{class-names}\} into parent and child classes. Such that actions that are visually similar to each other are in the same group. If the action is not similar to any other action in the list, assign it to others.}} 
    \newline
\end{minipage}
\end{quotation}
As a result, this converts the standard input prompt (\emph{a photo of a {\{class-name}\}}) to high-level action context prompt: \emph{a photo of a {\{high-level context}\} i.e. {\{class-name}\}}. For instance, for the video action of ``\emph{drumming}", the prompt becomes ``\emph{a photo of a playing music i.e., drumming}". The text encoder  extracts the embeddings of the generated high-level action context prompt, which can be averaged with the embeddings of the language attributes and descriptions to create a context-based classifier. 
A summary of high-level action context for the different datasets is provided in Table \ref {Table-1}, while the comprehensive list of Tree Hierarchy of Categories is provided in the Appendix \ref{High-level Action Context for HMDB-51} \ref{High-level Action Context for UCF101}, \ref{High-level Action Context for K400}.







\begin{table}[!t]
\caption{A short summary of high-level action context for HMDB-51, UCF-101, and K400 datasets.}
\centering\small
\vspace{-0.5em}
\begin{tabular}{lp{0.7\linewidth}}
\specialrule{1pt}{0pt}{0pt}
Dataset & High-Level Action-Context \\
\midrule


HMDB-51 & self-grooming, physical activities, sports, eating, social interactions, artistic activities, other \\

\hline\-3mm]

K400 & self-grooming, playing music, playing sports, exercise and fitness, household chores, social interactions, creative activities, transportation activities, water activities, other \\
\specialrule{1pt}{0pt}{0pt}

\end{tabular}
\label{Table-1}
\vspace{-1em}
\end{table}

\section{Experimental Protocols}
\label{headings}
We evaluate the effectiveness of VideoPrompter under three different video zero-shot settings: a) action recognition, b) text-to-video and video-to-text retrieval, and c) time-sensitive video tasks \citep{bagad2023test}. Additionally, we demonstrate that our VideoPrompter provides interpretability of the model decisions (Figure. \ref{fig:VideoPromper's Interoperability}). We use the ViT-B/16 backbone and sparsely sample 32 frames as consecutive frames are highly redundant with single-view evaluation \citep{Rasheed_2023_CVPR}. In the case of CLIP, the video embedding is obtained by averaging the frame-level embeddings, \citep{portillo2021straightforward,Rasheed_2023_CVPR}, while for other methods we follow their default settings. 
We use three video textual descriptions from VGPT and only two language descriptors: language attributes and descriptions. To make sure that all three video textual descriptions generated by VGPT are diverse, we set its temperature (likelihood of selecting lower probability tokens) to 0.5., while for GPT-3.5, as we only prompt the model once for each descriptor, we set its temperature to 0.2 to generate more focused and deterministic descriptors. The video textual descriptions generated by VGPT are trimmed to be consistent within the context length of the text encoder \citep{radford2021learning}, and we also apply CLIP-based filtering as a pre-processing step,  discussed in  \ref{subsubsec:Filtering of Visual descriptions} to remove erroneous video textual descriptions. We set   equal to 1.0, and  is calculated as cosine-similarity between the embeddings of query video and its video textual description. This ensures that a video textual description that is consistent with the query video is given higher weight. As AIM \citep{yang2023aim} only comprises a visual encoder, we add a CLIP text encoder for zero-shot analysis. 

\begin{table}[!t]
\caption{\small Zero-shot action recognition (top-1 \%) using our VideoPrompter provides consistent improvements across different VLMs and video datasets.}
\vspace{-1em}
\centering\small
\setlength{\tabcolsep}{7pt}
\scalebox{0.9}[0.9]{
\begin{tabular}{lcllll}
\specialrule{0.5pt}{0pt}{0pt}
\hline\-3mm]

& Method       & VideoPrompter & Action-Context  & HMDB    & UCF & K400      \\
\hline
& CLIP \citep{radford2021learning}           & \xmark   & \xmark              & 37.5               & 61.72     & 44.53      \\
& CLIP \citep{radford2021learning}           & \cmark   & \xmark            & 50.79              & 72.77     & 49.17      \\
& CLIP \citep{radford2021learning}          & \cmark   & \cmark                & 52.51 \inc{15.01}     &73.88 \inc{12.16}  &52.03 \inc{7.50}    \\


\hline
& ViFi-CLIP \citep{Rasheed_2023_CVPR}         & \xmark   & \xmark            & 51.82              & 77.5    & -         \\
& ViFI-CLIP\citep{Rasheed_2023_CVPR} & \cmark   & \xmark        & 57.12              & 79.56      & -       \\
& ViFI-CLIP\citep{Rasheed_2023_CVPR}    & \cmark   & \cmark          & 57.94 \inc{6.12}     & 80.70 \inc{3.2}  & -    \\
\hline

& AIM \citep{yang2023aim}              & \xmark   & \xmark               & 51.27              & 72.19         & -     \\
& AIM\citep{yang2023aim}                & \cmark   & \xmark             & 54.37                & 78.50  & -   \\
& AIM\citep{yang2023aim}      & \cmark   & \cmark              & 55.54 \inc{4.27}    & 77.90 \inc{5.71}         & -     \\
\hline

& Action-CLIP \citep{wang2021actionclip}                & \xmark   & \xmark   & 49.20              & 69.52         & -     \\
& Action-CLIP \citep{wang2021actionclip} & \cmark   & \xmark   & 51.65              & 77.07         & -     \\
& Action-CLIP \citep{wang2021actionclip}      & \cmark   & \cmark    & 54.50 \inc{5.3}   & 77.47 \inc{7.95}  & -     \\


 




\hline  \specialrule{0.5pt}{0pt}{0pt}
\end{tabular}}
\label{table-6}
\vspace{-0.5em}
\end{table}










\subsubsection{Design Choices}
To study the design effectiveness of our framework, we discuss various other design choices. First, to show that a combination of query video and its video textual description embedding is the optimal choice, we remove the visual encoder and only take the similarity of the video textual description embedding with the class representations, as shown in Figure \ref{figure-3} (left). We observe that both modalities (visual information and corresponding descriptive information) complement each other and removing visual embedding leads to sub-optimal results.

Further, we also study the impact of removing either the video-to-text model (VGPT) or the text-to-text model (GPT-3.5), as shown in Figure \ref{figure-4}. While employing these modules individually results in improved performance across all four benchmarks, their combination exhibits a complementary relationship delivering the most optimal performance. 

We also examined the possibility of predicting class names directly by providing GPT-3.5 with video textual descriptions and instructing it to select the closest matching class from a predefined list, we found that this approach fell short of producing optimal results. 

\subsubsection{Filtering of video textual descriptions}
\label{subsubsec:Filtering of Visual descriptions}
We apply CLIP-based filtering as a pre-processing step to remove erroneous visual textual descriptions.  Specifically, we generate 10 visual textual descriptions for each query video and extract corresponding textual embeddings along with the visual embedding of the query video. Cosine similarity between these embeddings is taken to filter the top-3  visual textual descriptions. As shown in Figure \ref{figure-3} (middle), filtering of visual textual descriptions further increases the performance of our framework. We only apply filtering in the action-recognition setting.


\subsubsection{Impact of visual textual description Diversity}

In order to analyze how the diversity of visual textual descriptions impacts our framework, we experimented with two varying temperature settings (0.2 and 0.5). These temperature settings directly influence the probability of selecting less common tokens, thereby increasing the diversity of the generated descriptions. As shown in Figure \ref{figure-3} (right), the higher temperature setting leads to better results (as it generates more diverse descriptions). Here, \emph{VGPT only} indicate that only video textual descriptions are used, while language attributes and descriptions are not employed.



\subsubsection{Comparison with CUPL}
\label{Comparison with CUPL}
We compare our work with one of the recent works CUPL \citep{pratt2022does} and show that only a handful of carefully designed video prompts combined with the video-to-text guided visual feature enhancement module leads to superior performance. CUPL employs GPT-3 and designs multiple dataset-specific prompts to generate the language descriptions.  For instance, for UCF-101, \citep{pratt2022does} design 5 prompts and generate 10 responses for each prompt leading to 50 descriptions in total. As shown in Table \ref{table-5}, our framework obtains superior performance with only 3 language descriptors i.e. language attributes, language descriptions, and high-level action context.  Further, recent work \citep{roth2023waffling} found descriptor ensemble as the main driver for performance in the case of a large number of prompts and showed comparable performance with randomized descriptors. This further validates our work that only a few carefully designed video prompts are enough to enrich the class representations.


























































\begin{figure}[!htb]
\makebox[\textwidth][c]{\includegraphics[width=1.05\linewidth]{figures/final_barplot_try6.png}
    }
    \caption{\textbf{(left)} Combination of embeddings of query video and its video textual description leads to optimal choice. \textbf{(middle)} CLIP-based filtering is applied as a pre-processing step, it further boosts the performance by removing the erroneous video textual descriptions. \textbf{(right)} A higher temperature setting leads to better results, as it generates more diverse video textual descriptions.}
    \label{figure-3}
\end{figure}
































\begin{figure}[!htb]
    \makebox[\textwidth][c]{\begin{minipage}[c]{0.7\textwidth}
            \includegraphics[width=\linewidth]{figures/plot_try8.png}
        \end{minipage}
        \begin{minipage}[c]{0.3\textwidth}
            \vspace{10pt} \caption{Video-textual description and language descriptors complement each other and their combination (VideoPrompter) leads to optimal results. We found this behavior consistent across all benchmarks and models.}
            \label{figure-4}
        \end{minipage}
    }
\end{figure}




















































\section{Literature Review}


\textbf{Vision-Language Models.} VLMs \citep{radford2021learning,jia2021scaling} have shown impressive generalization capabilities on various downstream tasks including open-vocabulary image recognition \citep{zhang2021tip,zhou2022learning,zhou2022conditional}, object detection \citep{gu2021open,rao2022denseclip,zhou2022detecting} and image segmentation \citep{ding2022decoupling,zhou2022extract}. However, the extension of these models to video-related tasks poses significant challenges, primarily due to the substantial computational costs and the requirement to collect large-scale video-text pairs. Therefore, various methods have been proposed to effectively leverage pre-trained image-language models for video tasks \citep{Rasheed_2023_CVPR,wang2021actionclip, yang2023aim,wasim2023vita}. Nevertheless, these approaches either introduce additional learnable parameters that add to overall model complexity or require extensive fine-tuning to adapt the model for video tasks. Further, these methods require access to the true distribution of the target task, which can be prohibitive in test-time adaptation and data-scarce environments.

\textbf{Improving  Text Classifier Representations Using LLMs.}
As open-vocabulary models classify the input by calculating a similarity score between the image/video and textual prompt (a photo of a {\{class-name}\}.) for each class, this makes the models' performance directly dependent on the \emph{descriptiveness} of the class names. Recently, a new line of work has emerged \citep{menon2022visual,pratt2022does,roth2023waffling,novack2023chils} that incorporates LLMs to enrich these class names and requires no further
training or access to the true distribution of the target task. \citep{menon2022visual} highlights that language can provide additional context to the classifier, and LLM is employed to describe visual features that distinguish that object in an image. \citep{pratt2022does} further explores this idea and uses LLM to generate multiple descriptors for each class name in the dataset. For instance, for UCF-101, \citep{pratt2022does} designed 5 prompts and generated 10 responses for each prompt leading to 50 descriptions in total. Despite the promising results in image classification, \emph{the applicability of these methods \citep{menon2022visual,pratt2022does,roth2023waffling,novack2023chils} in the context of video understanding remains an open question, and our work aims to address this gap}. Moreover, as these methods make use of LLMs to increase the descriptiveness of the class names, the visual side of the VLMs remains unaltered. Our framework - VideoPrompter - simultaneously refines class representations and enriches visual features, utilizing text-to-text  \citep{brown2020language} and video-to-text models \citep{maaz2023video}  respectively. We do so in a two-step approach: first, we query a video-to-text generative model to convert the input video to language representation (description) and fuse it with the query video embedding to enhance the overall visual representation. Second, we use video-specific prompts to query LLM and generate language descriptors to enrich the class representations.

\textbf{Context-based Classifier Enhancement.}
\citep{roth2023waffling, novack2023chils} showed that furnishing contextual information to the VLM can significantly aid the model in directing its attention to relevant features and help resolve class ambiguities. \citep{roth2023waffling} employed LLM  to find the higher-level commonalities in the dataset and GPT-3 is used to extract common context from the datasets. For instance, in CUB200-2011, a bird-related dataset,\citep{roth2023waffling} generates the context ``bird." Likewise, in the eurosat dataset of satellite images, \citep{roth2023waffling} outputs the context ``land use". However,  video-action recognition datasets \citep{kay2017kinetics, kuehne2011hmdb, soomro2012ucf101} generally comprise actions with diverse contexts. For example, the UCF-101 dataset can be subdivided into the following high-level contexts: \emph{self-grooming, playing music, playing sports, exercise and fitness, water activities, household chores, and other activities}, therefore, \citep{roth2023waffling} cannot be directly applied to such diverse datasets. Recently, \citep{novack2023chils} proposed a sub-division of the classes to one level lower, i.e., fine-grained classes. However, their method is only applicable to datasets (unlike action recognition) where the classes are not fine-grained. \emph{We proposed an alternative way of querying LLMs and divide the dataset into various high-level action contexts such that all video-action classes semantically close to each other are grouped under one high-level action context}. For instance, all sports-related actions (basketball, cricket, baseball) can be added under one high-level action context, i.e., ``playing sports". This high-level action context provides additional cues to further enrich the class representations.  




















\section{Conclusion}
In this work, we introduced a framework - VideoPrompter -  to boost the zero-shot performance of existing VLMs for video understanding. We present a systematic way to prompt pre-trained generative video-to-text and text-to-text models to provide additional semantic context to enrich visual and class representations simultaneously. We demonstrate that, without further training, VideoPrompter performs on par with the various existing fully fine-tuned methods and outperforms adapter-based methods.  We also discuss various design choices and demonstrate that both video-to-text and text-to-text models complement each other and result in optimal performance. We also introduce a Tree Hierarchy of Categories for class names, offering a higher-level action context for additional visual cues. VideoPrompter achieved consistent improvement across three different zero-shot settings: video action recognition, video-to-text, text-to-video retrieval, and time-sensitive video tasks across multiple benchmarks and models.








\section*{Reproducibility}
We used GPT-3.5, CLIP, and VideoChatGPT in our work. All of these models/weights/APIs are publicly available. Additionally, the datasets utilized are also publicly available.  While details to reproduce our work are provided in Section 2, by following the provided instructions, our experiments can be replicated. We will also release the code upon publication.


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\newpage

\appendix
\section{Time-aware synthetic dataset}
\label{Time-aware synthetic dataset}
The dataset \citep{bagad2023test} contains 180 video-text pairs with different shapes and colors and two types of temporal relationships: \emph{before and after}. Each video has a corresponding correct caption referred to as an \emph{attractor} and a negative caption with flipped events referred to as \emph{distractor}. In the ideal scenario, the VLM should be able to associate query video with the attractor. 


\section{High-level Action Context for HMDB-51}
\label{High-level Action Context for HMDB-51}
\fontfamily{qcr}\selectfont
\textbf{Self-grooming:} \lowercase{brush hair.}\\
\textbf{Physical activities:} \lowercase{Cartwheel, Climb, Climb stairs, Dive, Flicflac, Hand stand, Jump, Pullup, Pushup, Ride bike, Ride horse, Run, Situp, Somersault, Stand, Swing baseball, Talk, Turn, Walk.}\\
\textbf{Sports:} \lowercase{Catch, Dribble, Golf, Hit, Kickball, Kick, Shootball.}\\
\textbf{Eating:} \lowercase{Chew, Drink, Eat.}\\
\textbf{Social interactions:} \lowercase{Clap, Hug, Kiss, Shake hands, Smile, Wave.}\\
\textbf{Artistic activities:} \lowercase{Draw sword, Sit, Smoke, Fencing, Laugh, Pour, Pick, Shoot gun, Shoot bow, Sword exercise, Throw, Sword.}\\
\textbf{Others:} \lowercase{fall floor, push, punch.}

\section{High-level Action Context for UCF101}
\label{High-level Action Context for UCF101}
\fontfamily{qcr}\selectfont

\textbf{Self-grooming:}  \lowercase{Apply Eye Makeup, Apply Lipstick, Blow Dry Hair, Brushing Teeth, Haircut, Head Massage, Shaving Beard.}\\
\textbf{Playing music:} \lowercase{Drumming, Playing Cello, Playing Daf, Playing Dhol, Playing Flute, Playing Guitar, Playing Piano, Playing Sitar, Playing Tabla, Playing Violin.}\\
\textbf{Playing sports:} \lowercase{Archery, Balance Beam, Band Marching, Baseball Pitch, Basketball, Basketball Dunk, Bench Press, Biking, Billiards, Bowling, Boxing Punching Bag, Boxing Speed Bag, Cricket Bowling, Cricket Shot, Field Hockey Penalty, Frisbee Catch, Gold Swing, Hammer Throw, Horse Race.}\\
\textbf{Exercise and fitness:} \lowercase{Body Weight Squats, Handstand Pushups, Pull Ups, Lunges, Handstand Walking, High Jump, Push Ups, Wall Pushups.}\\
\textbf{Water activities:} \lowercase{Breast Stroke, Cliff Diving, Diving, Kayaking, Rafting.}\\
\textbf{Household chores:} \lowercase{Cutting In Kitchen, Mixing, Mopping Floor.}\\ 
\textbf{Creative activities:} \lowercase{Knitting, Typing, Writing On Board, YoYo.}\\
\textbf{Other:} \lowercase{Baby Crawling, Blowing Candles, Hula Hoop, Nunchucks, Parallel Bars, Pizza Tossing, Rope Climbing, Salsa Spin, Swing, Tai Chi, Walking With Dog, Clean And Jerk, Fencing, Front Crawl, Floor Gymnastics, Hammering, Juggling Balls, Jump Rope, Jumping Jack, Pommel Horse, Punch, Sky Diving.}


\section{High-level Action Context for K400}
\label{High-level Action Context for K400}
\fontfamily{qcr}\selectfont

\textbf{Self-grooming:} applying cream, brushing hair, brushing teeth, cutting nails, dying hair, fixing hair, filling eyebrows, getting a haircut, getting a tattoo, grooming dog, grooming horse, massaging back, massaging feet, massaging legs, massaging person's head, shaving head, shaving legs, shining shoes, trimming or shaving beard, waxing back, waxing chest, waxing eyebrows, waxing legs. \\
\textbf{Playing music:} air drumming, beatboxing, playing accordion, playing bagpipes, playing bass guitar, playing cello, playing clarinet, playing controller, playing didgeridoo, playing drums, playing flute, playing guitar, playing harmonica, playing harp, playing ice hockey, playing keyboard, playing organ, playing piano, playing recorder, playing saxophone, playing trombone, playing trumpet, playing ukulele, playing violin,  playing xylophone, strumming guitar, tapping guitar.\\
\textbf{Playing sports:} archery, arm wrestling, bobsledding, bowling, capoeira, cartwheeling, cheerleading, climbing a rope, climbing tree, contact juggling, disc golfing, dodgeball, drop kicking, golf chipping, golf driving, golf putting, high jump, high kick, hitting baseball, hockey stop, hopscotch, hurdling, hurling (sport), ice climbing, javelin throw, kitesurfing, long jump, paragliding, parkour, passing American football (in game), pas sing American football (not in game), picking fruit, playing basketball, playing cricket, playing kickball, playing squash or racquetball, playing tennis, playing olleyball, pole vault, riding mechanical bull, riding mountain bike, roller skating, shooting basketball, shooting goal (soccer), shot put, skiing (not slalom or crosscountry), skiing crosscountry, skiing slalom, skydiving, slacklining, sled dog racing, snowboarding, snowkiting, snowmobiling, somersaulting, spinning poi, springboard diving, swing dancing, sword fighting,tobogganing, trapezing, triple jump, vault, wrestling.\\
\textbf{Exercise and fitness:} abseiling, bench pressing, blasting sand, blowing leaves, blowing nose, blowing out candles, bouncing on trampoline, braiding hair, bungee jumping, carrying baby, chopping wood, clapping, climbing ladder, crawling baby, crossing river, crying, digging, exercising arm, exercising with an exercise ball, extinguishing fire, feeding birds, feeding fish, feeding goats, front raises, garbage collecting, gargling, hammer throw, holding snake, jogging, jumping into pool, laughing, laying bricks, lunge, mopping floor, moving furniture, mowing lawn,  opening present, planting trees, reading book, rock climbing, running on treadmill, scrambling eggs, shaking hands, shaking head, sharpening pencil, shoveling snow, side kick, situp, skipping rope, smoking, smoking hookah, snatch weight lifting, sneezing, sniffing, squat, stomping grapes, stretching arm, stretching leg, surfing crowd, swinging legs, swinging on something, tai chi, taking a shower, tying bow tie, tying knot (not on a tie), tying tie, using remote controller (not gaming), walking the dog, washing feet, washing hair, washing hands, welding, yawning. \\
\textbf{Household chores:} assembling computer, breading or breadcrumbing, building cabinet, building shed, cleaning floor, cleaning gutters, cleaning pool, cleaning shoes, cleaning toilet, cleaning windows, counting money, cutting pineapple, cutting watermelon, doing laundry, doing nails, folding clothes, folding napkins, folding paper, frying vegetables, ironing, making bed, plastering, reading newspaper, ripping paper, sanding floor, setting table, sharpening knives, shredding paper, sweeping floor, trimming trees, unboxing, using computer, washing dishes, watering plants.\\
\textbf{Social interactions:} answering questions, auctioning, celebrating, checking tires, giving or receiving award, news anchoring, presenting weather forecast, testifying, texting, waiting in line.\\
\textbf{Creative activities:} arranging flowers, balloon blowing, bandaging, bartending, bee keeping, blowing glass, bookbinding, carving pumpkin, country line dancing, cracking neck, drawing, making jewelry, playing cards, playing monopoly, recording music, sticking tongue out, weaving basket, wrapping present, writing. \\
\textbf{Transportation activities:} biking through snow, driving car, driving tractor, flying kite, hoverboarding, motorcycling, riding a bike, riding camel, riding elephant, riding mule, riding or walking with horse, riding scooter, riding unicycle, unloading truck, using segway.\\
\textbf{Water activities:} canoeing or kayaking, diving cliff, ice fishing, ice skating, jetskiing, parasailing, sailing, scuba diving, snorkeling, surfing water, swimming breast stroke, swimming butterfly stroke, water skiing, water sliding, windsurfing.\\
\textbf{Other:} baby waking up, bending back, bending metal, brush painting, catching fish, catching or throwing baseball, catching or throwing frisbee, catching or throwing softball, changing oil, changing wheel, clay pottery making, clean and jerk, curling hair, deadlifting, doing aerobics, dribbling basketball, drinking, drinking beer, drinking shots, drumming fingers, dunking basketball, egg hunting, flipping pancake, grinding meat, gymnastics tumbling, hugging, jumpstyle dancing, kicking field goal, kicking soccer ball, kissing, marching, petting animal (not cat), petting cat, playing badminton, playing chess, playing cymbals, playing paintball, playing poker, pull ups, pumping fist, pumping gas, punching bag, punching person (boxing), push up, pushing car, pushing cart, pushing wheelchair, shearing sheep, shuffling cards,  sign language interpreting, ski jumping, slapping, spraying, swimming backstroke, tango dancing, tickling, tossing  coin, training dog.



\end{document}
