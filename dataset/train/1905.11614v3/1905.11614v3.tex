\section{Experimental Results}




\subsection{Supervised learning}
We evaluate the performance of UCL together with EWC \cite{(EWC)KirkPascRabi17}, SI \cite{(SI)ZenkePooleGang17}, VCL \cite{(VCL)NguLiBuiTurner18}, and HAT \cite{SerraSurisMironKarat2018(HAT)}. We also make a comparison with Coreset VCL proposed in \cite{(VCL)NguLiBuiTurner18}. The number of sampling weights was 10 for VCL, and  1 for UCL.
All of the results are averaged over 5 different seeds. For the experiments with MNIST datasets, we used fully-connected neural networks (FNN), and with CIFAR-10/100 and Omniglot datasets, we used convolutional neural networks (CNN). The detailed architectures are given in each experiment section. Moreover, the initial standard deviations for UCL, , 
were set to be 0.06 for FNNs and adaptively set like the \emph{He initialization} \cite{HeZhaRenSun15} for deeper CNNs, of which details are given in the Supplementary Material. The hyperparameter selections among the baselines are done fairly, and we list the selected hyperparameters in the Supplementary Materials. 










\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{figure/single_PMNIST.pdf}
    \caption{Experimental results on Permuted / Row Permuted MNIST with a single headed network.}
\label{fig:single_results}
\end{figure}

\textbf{Permuted / Row Permuted MNIST}\ \ 
We first test on the popular Permuted MNIST dataset. We used single-headed FNN that has two hidden layers with 400 nodes and ReLU activations for all methods.
We compare the average test accuracy over the learned tasks in Figure \ref{fig:single_results} (left). After training on 10 tasks sequentially,  EWC, SI, and VCL show little difference of performance among them achieving 91.8\%, 91.1\%, and 91.3\% respectively. Although VCL with the coreset size of 200 makes an improvement of 2\%, UCL outperforms all other baselines achieving 94.5\%. Interestingly, HAT keeps almost the same average accuracy as UCL until the first 5 tasks, but it starts to significantly deteriorate after task 7. This points out the limitation of applying HAT in a single-headed network. As a variation of Permuted MNIST, we shuffled only rows of MNIST images instead of shuffling all the image pixels, and we denoted it as Row Permuted MNIST. 
We empirically find that all algorithms are more prone to forgetting in Row Permuted MNIST.  Looking at the accuracy scale of Figure \ref{fig:single_results} (right), all the methods show severe degradation of performance compared to Permuted MNIST. This may be due to  permuting of the correlated row blocks causing more weight changes in the network. After 10 tasks, UCL again achieved the highest average accuracy, 86.5\%, in this experiment as well.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{figure/histogram_layer.pdf}\vspace{-.07in}
    \caption{Standard deviation histogram in the Permuted MNIST experiment. We randomly selected 100 standard deviations for layer 1 and 2. In layer 3, all 10 nodes are shown.}\label{fig:histogram}
    \vspace{-.2in}
\end{figure}

 For a better understanding of our model, Figure \ref{fig:histogram} visualize the learned standard deviations of nodes in all layers as the training proceeds. 
After the model trained on task 1, 
we find that just a few of them become smaller than 
the initialized value of , and most of them become much larger in the first hidden layer. 
Interestingly, the uncertain nodes in layer 1 show a drastic decline of their standard deviations at a specific task as the learning progresses, which means the model had to make them certain for adapting to the new task. On the other hand, all the nodes in the output layer had to reduce their uncertainty as early as possible considering even a small randomness can lead to a totally different prediction. Most of the nodes in layer 2, in addition, do not show a monotonic tendency. This can be interpreted as many of them need not belong to a particular task. As a result, this gives the plasticity and gracefully forgetting trait of our UCL.



\vspace{-.05in}
\begin{figure}[th]
    \centering
    \includegraphics[width=0.85\textwidth]{figure/Ablation_PMNIST_per_task_avg.pdf}
    \caption{Ablation study in Permuted MNIST. Each line denotes the test accuracy.}\label{fig:Ablation}\vspace{-.05in}
\end{figure}
We also carry out an ablation study on UCL's additional regularization terms. Figure \ref{fig:Ablation} shows the results of three variations that lack one of the ingredients of the proposed UCL on Permuted MNIST. 
``UCL w/o upper freeze'' stands for using  in (\ref{eq:L2_Reg}), and we observe regularizing the outgoing weights of an important node in UCL very important. 
``UCL w/o (\ref{eq:L1_reg})'' stands for the removing (\ref{eq:L1_reg}) from (\ref{eq:final}), and we clearly see the pruning heuristic based weight freezing is also very important. ``UCL w/o (\ref{eq:normal_convex})'' stands for not using (\ref{eq:normal_convex}) and it shows that while the accuracy of Task 1 \& 2 are even higher than UCL, but the accuracy drastically decreases after Task 3. 
This is because due to the rapid decrease of model capacity since ``actively'' learning weights reduce when (\ref{eq:normal_convex}) is not used. 














\textbf{Split MNIST}\ 
We test also in the splitted dataset setting that each task consists of 2 consecutive classes of MNIST dataset.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figure/split_MNIST.pdf}
    \caption{Experimental results on Split MNIST(top) and Split notMNIST(bottom)}\vspace{-.05in}\label{fig:smnist_results}
\end{figure}This benchmark was used in \cite{(SI)ZenkePooleGang17, (VCL)NguLiBuiTurner18} and has total 5 tasks. 
We used multi-headed FNN hat has two hidden layers with 256 nodes and ReLU activations for all methods.
In Figure \ref{fig:smnist_results} (top), we compare the test accuracy of each task together with the average accuracy over all observed tasks at the right end. UCL accomplishes the same 5 tasks average accuracy as HAT; 99.7\%, which is slightly better than the results of SI and VCL with coreset, 99.0\%, and 98.7\%, respectively. Note UCL significantly outperforms EWC and VCL. We also point out that HAT makes a critical assumption to know the number of tasks \emph{a priori}, while UCL need not. 



\textbf{Split notMNIST}\ 
Here, we make an assessment on another splitted dataset tasks with notMNIST dataset, which has 10 character classes.
We split the characters of notMNIST into 5 groups same as VCL\cite{(VCL)NguLiBuiTurner18}: A/F, B/G, C/H, D/I, and E/J. 
We used multi-headed FNN hat has four hidden layers with 150 nodes and ReLU activations for all methods.
Unlike the previous experiments, SI shows similar results to EWC around 84\% average accuracy, and VCL attains a better result of 90.1\% (in Figure \ref{fig:smnist_results}) (bottom). Our UCL again achieves a superior an outstanding result of 95.7\%, that is higher than HAT and VCL with coreset: 95.2\% and 93.7\%, respectively.





\noindent\textbf{Split CIFAR and Omniglot} To check the effectiveness of UCL beyond the MNIST tasks, we experimented our UCL on three additional datasets, Split CIFAR-100, Split CIFAR10/100 and Omniglot. For Split CIFAR-100, each task consists of 10 consecutive classes of CIFAR-100, for Split CIFAR-10/100, we combined CIFAR-10 and Split CIFAR-100,  and for Omniglot, each alphabet is treated as a single task, and we used all 50 alphabets. For Omniglot, as in \cite{(ProgressCompress)SchwarzLuketinaHadsell18}, we rescaled all images to 28  28 and augmented the dataset by including 20 random permutations (rotations and shifting) for each image. For these datasets, unlike in the previous experiments using FNNs, we used deeper CNN architectures, in which the notion of \emph{uncertainty} in the convolution layer is defined for each \emph{channel} (i.e., filter). We used multi-headed outputs for all experiments, and 8 different random seed runs are averages for all datasets. The details of experiments using CNNs, including the architectures and hyperparameters, 
are given in the Supplementary Materials.
In Figure \ref{fig:vision}, we compared UCL with EWC and SI and carried out extensive hyperparameter search for fair comparison. We did not compare with VCL since it did not have any results on vision datasets with CNN architectures. 
\vspace{-.1in}
\begin{figure}[ht]
    \centering
    \subfigure[Split CIFAR-100]{\label{fig:CIFAR-100}
    \includegraphics[width=0.29\columnwidth]{figure/Split_CIFAR100.pdf}}
    \subfigure[Split CIFAR-10/100]{\label{fig:CIFAR-10/100}
    \includegraphics[width=0.29\columnwidth]{figure/Split_CIFAR10_100.pdf}}
    \subfigure[Omniglot]{\label{fig:Omniglot}
    \includegraphics[width=0.29\columnwidth]{figure/Omniglot.pdf}}
    \vspace{-.1in}
    \caption{Experiments on supervised learning using convolutional neural network}\label{fig:vision}
\end{figure}
\vspace{-.1in}

In Split CIFAR-100, EWC and SI achieve 60.5\% and 60.0\% respectively. However, UCL outperforms SI and EWC achieving 63.4\%. In a slightly different task, Split CIFAR-10/100, which prevents overfitting on Split CIFAR-100 using a model pre-trained on CIFAR-10, UCL also outperforms baselines by achieving 73.2\%. In Omniglot, although UCL becomes slightly unstable for the first task, it eventually achieves 83.9\% average accuracy on all 50 tasks. However, EWC and SI only achieves 68.1\% and 74.2\% respectively, much lower than UCL. From above three results, we clearly observe UCL clearly outperforms the baselines for more diverse and sophisticated vision datasets and for deeper CNN architectures. 


\vspace{-.05in}


\begin{table}[th]
\small
\centering
\caption{The number of parameters used for each benchmark.
}\vspace{-.05in}\label{table:parameters}
\resizebox{0.6\linewidth}{!}{\begin{tabular}{|c|c|c|c|c|c||c|}
\hline
Dataset\textbackslash{}Methods & Vanilla  & EWC   & SI    & HAT  & VCL   & UCL\\ \hline
Permuted MNIST                 & 478K     & 1435K & 1435K & 486K & 1914K & 960K\\ \hline
Split MNIST                    & 270K     & 808K  & 808K  & 272K & 1077K & 538K\\ \hline
Split notMNIST                 & 187K     & 559K  & 559K  & 190K & 749K  & 375K\\ \hline
Split CIFAR10/100              & 839K     & 2467K & 2467K & -    & -     & 1655K\\ \hline
Omniglot                       & 1773K    & 1995K & 1995K & -    & -     & 1884K\\ \hline
\end{tabular}\vspace{-.1in}}
\end{table}

\textbf{Comparison of model parameters}\ 
Table \ref{table:parameters} shows the number of model parameters in each experiment. Vanilla stands for the base network architecture of all methods. It is shown that UCL has fewer parameters than other regularization-based approaches. Especially, UCL has almost half the number of VCL, which is based on the similar variational framework. 
Although HAT shows the least number of parameters, we stress it has the critical drawback of requiring to know the number of tasks \emph{a priori}. 










\subsection{Reinforcement learning}
\vspace{-.05in}
Here, we also tested UCL for the continual reinforcement learning tasks. Roboschool \cite{(PPO)SchulmanWlskiKlimov} consists of 12 tasks and each task has a different shape of the state and continuous action space, and goal. From these tasks, we randomly chose eight tasks and sequentially learned each task (with 5 million update steps) in the following order,
\textit{Walker-HumanoidFlagrun-Hooper-Ant-InvertedDoublePendulum-Cheetah-Humanoid-InvertedPendulum}. We trained a FNN model using PPO \cite{(PPO)SchulmanWlskiKlimov} as a training algorithm and selected EWC and Fine-tuning as baselines. All baselines were experimented in exactly the same condition, and we carried out an extensive hyperparameter search for fair comparison. 
\begin{wrapfigure}{r}{0.4\textwidth} 
    \vspace{-.1in}
    \centering
    \includegraphics[width=0.4\textwidth]{figure/cl_rl_sum.pdf}
    \vspace{-.2in}
    \caption{Cumulative normalized rewards.}\label{fig:normalized_performance}
    \vspace{-.1in}
\end{wrapfigure}
More experimental details, network architectures, and hyperparameters are given in the Supplementary Materials. Figure \ref{fig:normalized_performance} shows the cumulative normalized rewards up to the learned task, and Figure \ref{fig:RL_results} shows the normalized rewards for each task with vertical dotted lines showing the boundaries of the tasks. The normalization in the figures was done for each task with the maximum rewards obtained by EWC (). The high cumulative sum thus corresponds to effectively combating the catastrophic forgetting (CF), and we note Fine-tuning mostly suffers from CF (e.g., Task2 or Task4). Note we show two versions of UCL, with different  hyperparameter values. 
In Figure \ref{fig:normalized_performance}, we observe both versions of UCL significantly outperform both EWC and Fine-tuning.
We believe the reason why EWC does not excel as in Figure 4B of the original EWC paper \cite{(EWC)KirkPascRabi17} is because we consider pure continual learning setting, while \cite{(EWC)KirkPascRabi17} allows learning tasks multiple times in a recurring fashion. Moreover, a possible reason why UCL achieves such high rewards in RL setting may be due to the by-product of our weight sampling procedure; namely, the Gaussian perturbation of the weights for the variational inference enables an effective exploration of policies for RL as suggested in \cite{(RLNoise)PlappertAndry18}. Figure \ref{fig:RL_results} shows UCL overwhelmingly surpasses EWC particularly for Task1 and Task3 (by both achieving high rewards and not forgetting), and it contributes to the significant difference between EWC in Figure \ref{fig:normalized_performance}. We also experimentally checked the role of  for \emph{gracefully} forgetting; although UCL with  =  results in overall better rewards, with  =  does better in learning new tasks, \eg, Task5/7/8, by adding more plasticity to the network. 
To the best of our knowledge, this result shows for the first time that the pure continual learning is possible for reinforcement learning with continuous action space and different observation shapes. We stress that there are very few algorithms in the literature that work well on both SL and RL continual learning setting, and our UCL is very competitive in that sense. 
\vspace{-.051in}
\begin{figure}[th]
    \centering
    \includegraphics[width=1.0\textwidth]{figure/cl_rl.pdf}
    \vspace{-.2in}
    \caption{Normalized rewards for each task throughout learning of 8 RL tasks. Each task is learned with 5 million training steps. UCL excels in both not forgetting past tasks and learning new tasks.}
    \label{fig:RL_results}\vspace{-.1in}
\end{figure}



















