\documentclass{article}
\date{
April 13, 2014}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage[a4paper]{geometry}
\parindent=0pt
\parskip=\smallskipamount
\advance\textwidth by -2cm
\advance\oddsidemargin by 1cm
\advance\evensidemargin by 1cm
\renewcommand{\ttdefault}{cmtt}
\let\Bbbk\relax
\usepackage{bm,amsmath,amssymb}

\newcommand{\signum}{\operatorname{sgn}}
\newcommand{\Diag}[1]{\operatorname{Diag}(#1)}
\newcommand{\w}{{\bm w}}
\newcommand{\x}{{\bm x}}
\newcommand{\Gauss}{Gau\ss}
\newcommand{\1}{\mathbf 1}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\emptyset}{\varnothing}
\newcommand{\lst}[2]{,~, ,~}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\newcounter{noqed}
\newcommand{\qed}{ \ifmmode\mbox{ }\fi\rule[-.05em]{.3em}{.7em}\setcounter{noqed}{0}}
\newenvironment{proof}[1][{}]{\noindent{\bf Proof#1. }\setcounter{noqed}{1}}{\ifnum\value{noqed}=1\qed\fi\par\medskip}

\newcommand{\warning}[1]{\ensuremath{\quad\leftarrow}\mbox{\color{red}#1}}

\title{Supremum--Norm Convergence for Step--Asynchronous Successive
Overrelaxation on M-matrices}
\author{Sebastiano Vigna\thanks{The author was supported by the EU-FET
grant NADINE (GA 288956).}}
\begin{document}
\bibliographystyle{alpha}

\maketitle

\begin{abstract}
Step-asynchronous successive overrelaxation updates the values contained in
a single vector using the usual \Gauss--Seidel-like weighted rule, but 
arbitrarily mixing old and new values, the only constraint being temporal
coherence---you cannot use a value before it has been
computed. We show that given a nonnegative real matrix
, a  and a vector  such that , every iteration of
step-asynchronous successive overrelaxation for the problem , with , reduces geometrically the -norm of the current error
by a factor that we can compute explicitly. Then, we show that given a
 it is in principle always possible to compute such a .
This property makes it possible to estimate the supremum norm of the absolute
error at each iteration without any additional hypothesis on , even when 
is so large that computing the product  is feasible, but estimating the
supremum norm of  is not.
\end{abstract}

\noindent\textbf{Mathematical Subject Classification:} 65F10  	(Iterative
methods for linear systems)

\noindent\textbf{Keywords:} Successive overrelaxation; M-matrices; asynchronous
iterative solvers

\section{Introduction}

We are interested in providing \emph{computable absolute bounds in  norm} 
on the convergence of
a mildly asynchronous version of successive overrelaxation (SOR)
applied to problems of the form , where  is a
nonnegative real matrix and . A matrix of the form  under
these hypotheses is called a \emph{nonsingular M-matrix}~\cite{BePNMMS}.

We stress from the start that there are no other hypotheses on  such as
irreducibility, symmetry, positive definiteness or (weak) 2-cyclicity, and that
 is assumed to be very large---so large that computing  (or
performing a SOR iteration) is feasible (maybe streaming over the matrix
entries), but estimating  is not.

Our main motivation is the parallel computation with arbitrary guaranteed
precision of various kinds of \emph{spectral rankings with
damping}~\cite{VigSR}, most notably Katz's index~\cite{KatNSIDSA} and
PageRank~\cite{PBMPCR}, which are solutions of problems of the form above with
 derived from the adjacency matrix of a very large graph, the only relevant
difference being that the rows of  are -normalized in the case of PageRank.

By ``computable'' we mean that there must be a finite computational process that
provides a bound on ,
where  is the solution and  is the -th
approximation. Such a bound would make it possible to claim that we
know the solution up to some given number of significant fractional digits. For
example, without further assumptions on  convergence results based on the
spectral radius are not computable in this sense and results concerning the
residual are not applicable because of the unfeasibility of estimating
. 

We are also interested in highly parallel versions for modern multicore systems.
While SOR and other iterative methods are apparently strictly sequential
algorithms, there is a large body of literature that studies what happens when
updates are executed in arbitrary order, mixing old and new values. Essentially,
as long as old values come from a finite time horizon (e.g., there is a finite
bound on the ``oldness'' of a value) convergence has been proved for all major
standard sequential hypothesis of convergence\footnote{It is a bit surprising,
indeed, that the statement that \Gauss--Seidel is difficult to parallelize
appears so often in the literature. In a sense, an algorithm updating in
arbitrary order using possibly old values is not any longer \Gauss--Seidel. On
the other hand, this is exactly what one expects when asking the question ``is
\Gauss--Seidel parallelizable''?} (for the main results, see the sections
about \emph{partial asynchrony} in Bertsekas and Tsitsiklis's encyclopedic book~\cite{BeTPDCNM}).

Again, however, results are always stated in terms of convergence in the
limit, and the speed of convergence, which decays as the time horizon gets
larger, often cannot be stated explicitly. Moreover, the theory is modeled
around message-passing systems, where processor might actually use very old
values due to transmission delays. In the multicore, shared-memory system
application we have in mind it is reasonable to assume that after each iteration
memory is synchronized and all processors have the same view.

Our main motivation is obtaining (almost) ``noise-free'' scores to perform
accurate comparisons of the induced rankings using Kendall's ~\cite{KenTTRP}:
 
Computational noise can be quite problematic in evaluating Kendall's  because
the signum function has no way to distinguish large and small differences---they are all mapped to  or ~\cite{BPSTPTP}.

Suppose, for example, that we have a
graph with a large number  of nodes, and some centrality index that assigns score  the first
 nodes and score  the remaining nodes. Suppose we have also another
index assigning the same scores, and that this new index is defined
by an iterative process, which is stopped at some point (e.g., an iterative
solver for linear systems). If the computed values
include computational random noise and evaluate  on the two
vectors, we will obtain a  close to , even if the ranks are
perfectly correlated. On the other hand, with a sufficiently small guaranteed
absolute error we can proceed to truncate or round the second set of scores, obtaining
a result closer to the real correlation.

This scenario is not artificial: when comparing, for instance, indegree
with an index computed iteratively (e.g., Katz's index, PageRank, etc.), we have
a similar situation. Surprisingly, the noise from iterative computations can
even \emph{increase} correlation (e.g., between the dominant eigenvector of a graph
that is not strongly connected and Katz's index, as the residual score in nodes
whose actual score is zero induces a ranking similar to that induced by
Katz's index).

In this paper, we provide convergence bounds in  norm for SOR iterations for
the problem , where  is a nonnegative real matrix
and , in conditions of mild asynchrony, without any
additional hypothesis on . Our main result are Theorem~\ref{teo:conv}, which shows 
that given a  and a vector  such that 
SOR iterations reduce geometrically the -norm of the error (with a
computable contraction factor), and Theorem~\ref{teo:suitable}, which shows how
to compute such a  using only iterated products of  with a vector. The
two results can be viewed as a constructive and computable version of the 
standard convergence results on SOR iteration based on the spectral radius.

We remark that SOR is actually not useful for PageRank, as shown recently by
Greif and Kurokawa~\cite{GrKNCSPP}. The author has found experimentally that the
same phenomenon plagues the computation of Katz's index. However, since
generalizing from \Gauss--Seidel to SOR does not bring any significant increase
in complexity in the proof, we decided to prove our results in the more general
setting.

\section{Step-asynchronous SOR}
\label{sec:SOR}

We now define \emph{step-asynchronous} SOR for the problem .
In general, \emph{asynchronous} SOR computes new values using arbitrarily old values; in this case, the hypotheses for convergence
are definitely stronger. In the 
\emph{partially asynchronous} case, instead, there is a 
finite limit on the ``oldness'' of the values used to compute new values, and while there is a decrease
in convergence speed, the hypotheses for convergence are essentially the same of
the sequential case~(see \cite{BeTPDCNM} for more details).

Step-asynchronous SOR uses the strictest possible time bound: one step. We thus perform a SOR-like update in arbitrary order:

The only constraint 
is that for each iteration an \emph{update total preorder}\footnote{A \emph{total preorder} is a 
set endowed with a reflexive and transitive total relation. We remark that a choice of a sequence
of such preorders is equivalent to a \emph{scenario} in the terminology of~\cite{BeTPDCNM}.}  of
the indices is given:  iff  is updated before (or at the
same time of)  at iteration , and the set
 of the indices for which we use the \emph{previous}
values is such that for all  we have , whereas
 is the set indices for which we use the \emph{next}
values. Essentially,
we \emph{must} use previous values for all variables that are updated at the
same time of  or after , but we make no assumption on the remaining
variables. In this way we take into account cache incoherence, unpredictable
scheduling of multiple threads, and so on.\footnote{For example, if we have
exactly  parallel updates at the same time we would have, in fact, a Jacobi
iteration: in that case,  for all .}

Matrixwise, the set  induces a nonnegative
matrix  given by

and a \emph{regular splitting}

where  and  is nonnegative with zeros on the diagonal.
Then, equation~(\ref{eq:gs}) can be rewritten as

There is of course a permutation of row and columns (depending on ) such that 
 is strictly lower triangular, but the only claim that can be made
about  is that its diagonal is zero: actually, we could have
 and .

In particular, independently from the choice of , if  is a
solution we have as usual

and


\section{Suitability and convergence in -norm}
We now define suitability of a vector for a matrix, which will be the main tool in
proving our results. The idea is implicitly or explicitly at the core of several
classical proofs of convergence, and is closely related to that of \emph{generalized diagonal dominance}:
\begin{definition}
A vector  is \emph{-suitable} for  if
.
\end{definition}

The usefulness of suitable vectors is that they induce norms
norms in which the decrease of the error caused by a SOR
iteration for of the problem  can be controlled if
. If  is irreducible, for instance, the dominant eigenvector is suitable
for the spectral radius, but it is exactly this kind of hypotheses that we want to avoid.
\begin{definition}
Given a vector , the -norm is defined by

\end{definition}
The notation  is used also for the operator norm induced
in the usual way.
We note a few useful properties---many others can be found in~\cite{BeTPDCNM}:
\begin{proposition}
\label{prop:wnorm}
Given a vector  that is -suitable for a nonnegative matrix , the
following statements are true for all vectors :
\begin{enumerate}
\item\label{en:coord} ;
\item\label{en:min} ;
\item\label{en:max} ;
\item\label{en:unit} ;
\item\label{en:Aw} ;
\item\label{en:mindef} if , .
\item\label{en:suitable} ; in
particular, .
\end{enumerate}
\end{proposition}
\begin{proof}
The first claims are immediate from the definition of -norm. For the last
claim,

\end{proof}

The next theorem is based on the standard proof by induction of convergence
for SOR, but we make induction on the update time of a component rather than on its index,
and we use suitability to provide bounds to the norm of the error.

\begin{theorem}
\label{teo:conv}
Let  be a nonnegative matrix and let 
be -suitable for .
Then, given  step-asynchronous SOR for the problem 
converges for 

 and letting  we have

where 

Moreover, 

\end{theorem}
\begin{proof}
Let  be a sequence of update orders, and 
 a sequence of previous-value sets, one for each step  and
variable index , compatible with the respective update orders.
We work by induction on the order
, proving the statement

where , assuming it is true for all
.

Note that for all 

so for 

and analogously for

we have



Writing explicitly~(\ref{eq:err}) for the -th coordinate, we have

Since  implies by definition , we can apply the
induction hypothesis on  to state that .
The same bound applies to  using
the first statement of Proposition~\ref{prop:wnorm}. 

We now notice that -suitability implies 

which in coordinates tells us that

Thus,

By the very definition of -norm,~(\ref{eq:ind}) yields



For the second statement, we have

whence 

\end{proof}

We remark that the smallest contraction factor is obtained when , that is, with no relaxation. This does not mean,
however, that relaxation is not useful: convergence might be faster with ; it is just that the error bound we
provide features the best constant when .

\begin{corollary}
\label{cor:bound}
With the same hypotheses and notation of Theorem~\ref{teo:conv}, step-asynchronous \Gauss--Seidel iterations converge and

\end{corollary}

\begin{corollary}
\label{cor:bound}
Let  be an irreducible nonnegative matrix and  its dominant eigenvector. Then the statement of Theorem~\ref{teo:conv} 
is true in -norm with .
\end{corollary}

A simple consequence is that if we know a -suitable vector  for
 we can just behave as if the step-asynchronous SOR is converging in the
standard supremum norm, but we have a reduction in the strength 
of the bound given by the ratio between the maximum
and the minimum component of :
\begin{corollary}
\label{cor:bound}
With the same hypotheses and notation of Theorem~\ref{teo:conv}, step-asynchronous \Gauss--Seidel iterations converge and

\end{corollary}
\begin{proof}
An application of Proposition~\ref{prop:wnorm}.\ref{en:min}
and~\ref{prop:wnorm}.\ref{en:max}.
\end{proof}

We remark that 

so it is possible to restate all results in a simplified (but less powerful)
form.


\section{Practical issues}
In principle it is always better to compute the actual -norm,
rather than using the rather crude bound of Corollary~\ref{cor:bound}.\footnote{The bound is actually \emph{very} crude, in particular on
reducible matrices when  is close to .} On the other
hand, computing the -norm requires storing and accessing , which could be expensive. 

In practice, it is convenient to restrict oneself to vectors  satisfying , as in that case
, and for some  we actually have equality.
Then, we can store in few bits an approximate vector , which can be used 
to estimate , as we have, using the notation of Theorem~\ref{teo:conv},

A reasonable choice is that of keeping in memory . Using a byte of storage we can keep track of 's no smaller than
Moreover, during the evaluation of the norm we just have to multiply by a power
of two, which can be done very quickly in IEEE 754 format.



\section{Choosing a suitable vector}
\label{sec:choosing}

We now come to the main result: given a nonnegative matrix  
and a , it is possible (constructively) to compute a vector
 that is -suitable for . In essence, the
computation of a -suitable vector for  ``tames'' the
non-normality of the iterative process, at the price of a reduction of the
convergence range.

\begin{theorem}
\label{teo:suitable}
Let  be nonnegative and . Let

and

Then, . In particular,  is
-suitable for , and there is a  such that

so  is
-suitable for .
\end{theorem}
\begin{proof}
Consider the matrix , where . Since it is strictly
positive, the Perron--Frobenius Theorem tells us that there is a 
dominant eigenvector . Moreover, since for 
we have , and the spectral radius is continuous in the
matrix entries, there must be a  such that

We have

We now observe that the scaling factor is irrelevant:  is an
eigenvector, so it is defined up to a multiplicative constant. We can thus just write

and state that

which implies

Thus, as  when , for some
 we must have

\end{proof}

The previous theorem suggests the following procedure. Under the given
hypotheses, start with , and iterate

Note that this is just a Jacobi iteration for the problem , which is natural, as  is just its solution.
The iteration stops as soon as

and at that point  is by definition -suitable for , 
so we can apply Theorem~\ref{teo:conv}. 

In practice, it is useful to keep the current vector
 normalized: just set  at the start, and then iterate


We remark that, albeit used for clarity in the statement of Theorem~\ref{teo:suitable}, the (exact) knowledge of  is
not strictly necessary to apply the technique above: indeed, if the procedure terminates  by
Proposition~\ref{prop:wnorm}.  

There are a few useful observations about the behavior of the normalized
version of the procedure. First, if  necessarily  as
. Second, by Collatz's classical bound~\cite{ColECZM},
the maximum in~(\ref{eq:norm}) is an upper bound to . This happens
without additional hypotheses\footnote{We report the following two easy proofs
as in most of the literature Collatz's bounds are proved for irreducible
matrices using Perron--Frobenius theory.} on  because whenever  with  we have

If, moreover, we compute also the minimum ratio

this is a lower bound to , again without additional hypotheses on .
Indeed, note that whenever  with
, for every  if  is a positive eigenvector of
 we have

The last inequality implies  by
Proposition~\ref{prop:wnorm}.\ref{en:mindef}, and since the inequality is
true for every  it is true by continuity also for .

These properties suggest that in practice iteration should be stopped if
 goes below the minimum representable floating-point number: in this case, either
, or the finite precision at our disposal is not sufficient to
compute a suitable vector because we cannot represent correctly a transient
behavior of the powers of .

If instead the minimum~(\ref{eq:min}) becomes larger than , we can
safely stop: unfortunately, the latter event cannot be guaranteed to happen when
 without additional hypotheses on  (e.g., irreducibility):
for instance, if  has a null row the minimum~(\ref{eq:min}) will always be equal to zero.



Of course, there ain't no such thing as a free lunch. The termination
of the process above is guaranteed if , but we have no indication of how many step will be
required. Moreover, in principle some of the coordinates of the suitable vector could be so small 
to make Theorem~\ref{teo:conv}
unusable. For  close to 
convergence can be very slow, as it is related to the convergence of Collatz's
lower and upper bounds for the dominant eigenvalue.

Nonetheless, albeit all of the above must happen in pathological cases, we show on
a few examples that, actually, in real-world cases computing a -suitable
vector is not difficult.

We remark that in principle any dyadic product  such that  is irreducible will do the job in the proof of
Theorem~\ref{teo:suitable}. There might be choices (possibly depending on
) for which the computation above terminates more quickly.

\section{Examples}

\subsection{Bounding the error of }

If  is nonnegative matrix with , then  is invertible and the 
problem  has a unique solution, and in the limit we have
convergence geometric in . However, if we choose a
 (say, ) and a -suitable vector , the bounds of
Theorem~\ref{teo:conv} will be valid, so we will be
able to control the error in -norm.

\subsection{Katz's index}

Let  be a nonnegative matrix (in the standard formulation, the adjacency
matrix of a graph). Then, given  Katz's index is defined by

where  is a \emph{preference vector}, which is just  in Katz's
original definition~\cite{KatNSIDSA}.\footnote{We must note that actually Katz's index is 
. This additional multiplication by
 is somewhat common in the literature; it is probably a case of
\textit{horror vacui}.}.

If we want to apply Theorem~\ref{teo:conv}, we must choose a 
and a -suitable vector  for . The vector can then be used to
accurately estimate the computation of Katz's index for all
. This property is particularly useful, as it is common to
estimate the index for different values of , and to that purpose it is
sufficient to compute once for all a -suitable vector for  
a  chosen sufficiently close to .

\subsection{PageRank} The case of PageRank is similar to Katz's index. We have
 where  is the preference vector, and
 is a stochastic matrix;  is the adjacency matrix
of a graph , normalized so that each nonnull row adds to one,  is the
characteristic vector of \emph{dangling nodes} (nodes without outlinks, i.e.,
null rows), and  is the dangling-node distribution, used to redistribute
the rank lost through dangling nodes. It is common to use a uniform , but
most often , and in that case we speak of \emph{strongly
preferential} PageRank~\cite{BSVPFD}.

We remark that in the latter case it is well known that the \emph{pseudorank}

satisfies

That is, PageRank and the pseudorank are parallel vectors. This is relevant for
the computation of several strongly preferential PageRank vectors: just
compute a -suitable vector for  (rather than
one for each , depending on ), and compute
pseudoranks instead of ranks.

The case of PageRank is however less interesting because, as David Gleich made
the author note, assuming the notation of Section~\ref{sec:SOR} and 

Since , we can -bound the residual

we conclude that

It is thus possible, albeit wasteful, to bound the supremum norm of the error
using its  norm.

\section{Experiments}

In this section we discuss some computational experiments involving the computation of PageRank
and Katz's index on real-world graphs. We focus on a snapshot of the English version of Wikipedia taken in 2013
(about four million nodes and one hundred million arcs) and a snapshot of the \texttt{.uk} web domain
taken in may 2007 (about one hundred million nodes and almost four billion arcs).\footnote{Both datasets are
publicly available at the site of the Laboratory for Web Algorithmics (\texttt{http://law.di.unimi.it/}) under the identifiers
\texttt{enwiki-2013} and \texttt{uk-2007-05}.} These two graphs have some structural differences, which we highlight
in Table~\ref{tab:datasets}.

\begin{table}
\centering
\begin{tabular}{l|rr}
& \multicolumn{1}{c}{Wikipedia} & \multicolumn{1}{c}{\texttt{.uk}}\\
\hline
nodes &  &  \\
arcs &  & \\
avg. degree &  & \\
giant component &  &  \\
harmonic diameter &  & \\
dominant eigenvalue &  &  \\  
\end{tabular}
\caption{\label{tab:datasets}Basic structural data about our two datasets.}
\end{table}

We applied the procedure described in Section~\ref{sec:choosing} to the system associated with PageRank
and Katz's index, with  for PageRank and
 for Katz's index.

\begin{figure}
\centering
\includegraphics[scale=.6]{fig/iterations}
\caption{\label{fig:iter}Number of iterations that are necessary to compute a -suitable vector.}
\end{figure}

In Figure~\ref{fig:iter} we report the number of iterations that are necessary to compute the -th suitable
vector. The two datasets show the same behavior in the case of PageRank---an exponential
increase in the number of iterations as we get exponentially closer to the limit value. The case of Katz is more varied: whereas Wikipedia has a significant
growth in the number of iterations (but clearly slower than the PageRank case), \texttt{.uk} has a minimal variation 
across the range (from  to ).  

\begin{figure}[htb]
\centering
\includegraphics[scale=.45]{fig/enwiki-2013-katz-norm}\quad\includegraphics[scale=.45]{fig/enwiki-2013-pr-norm}\\
\includegraphics[scale=.45]{fig/uk-2007-05-katz-norm}\quad\includegraphics[scale=.45]{fig/uk-2007-05-pr-norm}\\
\caption{\label{fig:norm}Exponentially binned frequency plots of the values of -suitable vectors, , ,  and .}
\end{figure}


In Figure~\ref{fig:norm} we draw the (exponentially binned) distribution of values of suitable vectors for
a choice of four equispaced values of . The vectors are normalized in  norm, that is,
the largest value is one.

The shape of the distribution depends both on the graph
and on the type of centrality computed, but two features are constant: first, as we approach  the distribution
contains smaller and smaller values; second, the smallest value in the PageRank case is several orders of magnitude smaller.

Smaller values imply a larger -norm: indeed, one can think of the elements of an -normalized suitable
vector  as weights that ``slow down'' the convergence of problematic nodes by inflating their raw error. The intuition
we gather from the distribution of values is that bounding the convergence of PageRank is more difficult. 

\section{Conclusions}

We have presented results that make it possible to bound the supremum norm of
the absolute error of SOR iterations an -matrix  even when estimating
 is not feasible. Rather than
 relying on additional hypotheses such as positive definiteness, irreducibility
 and so on, our results suggest to
compute first a -\emph{suitable} positive vector  with the property
that SOR iterations converge geometrically in -norm by a computable factor.

While we cannot bound without additional
hypotheses the resources (number of iterations and precision) that are necessary
to compute , in practice the computation is not difficult, and given an
-matrix  the associated -suitable  can be used for all
.  

\hyphenation{ Vi-gna Sa-ba-di-ni Kath-ryn Ker-n-i-ghan Krom-mes Lar-ra-bee
  Pat-rick Port-able Post-Script Pren-tice Rich-ard Richt-er Ro-bert Sha-mos
  Spring-er The-o-dore Uz-ga-lis }
\begin{thebibliography}{PBMW98}

\bibitem[BP94]{BePNMMS}
Abraham Berman and Robert~J. Plemmons.
\newblock {\em Nonnegative Matrices in the Mathematical Sciences}.
\newblock Classics in Applied Mathematics. SIAM, 1994.

\bibitem[BPSV08]{BPSTPTP}
Paolo Boldi, Roberto Posenato, Massimo Santini, and Sebastiano Vigna.
\newblock Traps and pitfalls of topic-biased {P}age{R}ank.
\newblock In William Aiello, Andrei Broder, Jeannette Janssen, and Evangelos
  Milios, editors, {\em WAW 2006. Fourth Workshop on Algorithms and Models for
  the Web-Graph}, volume 4936 of {\em Lecture Notes in Computer Science}, pages
  107--116. Springer--Verlag, 2008.

\bibitem[BSV09]{BSVPFD}
Paolo Boldi, Massimo Santini, and Sebastiano Vigna.
\newblock Page{R}ank: {F}unctional dependencies.
\newblock {\em ACM Trans. Inf. Sys.}, 27(4):1--23, 2009.

\bibitem[BT89]{BeTPDCNM}
Dimitri~P. Bertsekas and John~N. Tsitsiklis.
\newblock {\em Parallel and Distributed Computation: Numerical Methods}.
\newblock Prentice Hall, Englewood Cliffs NJ, 1989.

\bibitem[Col42]{ColECZM}
Lothar Collatz.
\newblock Einschlie{\ss}ungssatz f{\"u}r die charakteristischen {Z}ahlen von
  {M}atrizen.
\newblock {\em Mathematische Zeitschrift}, 48(1):221--226, 1942.

\bibitem[GK11]{GrKNCSPP}
Chen Greif and David Kurokawa.
\newblock A note on the convergence of {SOR} for the {P}age{R}ank problem.
\newblock {\em SIAM J. Sci.~Computing}, 33(6):3201--3209, 2011.

\bibitem[Kat53]{KatNSIDSA}
Leo Katz.
\newblock A new status index derived from sociometric analysis.
\newblock {\em Psychometrika}, 18(1):39--43, 1953.

\bibitem[Ken45]{KenTTRP}
Maurice~G. Kendall.
\newblock {The treatment of ties in ranking problems}.
\newblock {\em Biometrika}, 33(3):239--251, 1945.

\bibitem[PBMW98]{PBMPCR}
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd.
\newblock The {P}age{R}ank citation ranking: Bringing order to the web.
\newblock Technical Report SIDL-WP-1999-0120, Stanford Digital Library
  Technologies Project, Stanford University, 1998.

\bibitem[Vig09]{VigSR}
Sebastiano Vigna.
\newblock Spectral ranking.
\newblock {\em CoRR}, abs/0912.0238, 2009.

\end{thebibliography}
\end{document}
