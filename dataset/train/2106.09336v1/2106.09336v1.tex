\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{10392} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{caption}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\dpar}[2]{\frac{\partial{#1}}{\partial{#2}}}

\newcommand{\thetab}{\bm{\theta}}
\newcommand{\betab}{\bm{\beta}}
\newcommand{\mub}{\bm{\mu}}
\newcommand{\mypar}[1]{\vspace{2mm}\noindent\textbf{#1}}
\newcommand{\Beta}{B}
\newcommand{\Betab}{\mathbf{\Beta}}
\newcommand{\Thetab}{\mathbf{\Theta}}

\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}


\newcommand{\sss}{\mathbf{s}}
\newcommand{\cb}{\mathbf{c}}
\newcommand{\tb}{\mathbf{t}}
\newcommand{\vb}{\mathbf{v}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\yb}{\mathbf{y}}
\newcommand{\pp}{\mathbf{p}}
\newcommand{\zz}{\mathbf{z}}
\newcommand{\nn}{\mathbf{n}}
\newcommand{\mm}{\mathbf{m}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\ff}{\mathbf{f}}
\newcommand{\rr}{\mathbf{r}}
\newcommand{\ttt}{\mathbf{t}}
\newcommand{\hh}{\mathbf{h}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\FF}{\mathbf{F}}
\newcommand{\KK}{\mathbf{K}}
\newcommand{\BB}{\mathbf{B}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Jb}{\mathbf{J}}
\newcommand{\Sb}{\mathbf{S}}
\newcommand{\TT}{\mathbf{T}}
\newcommand{\RR}{\mathbf{R}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\Yb}{\mathbf{Y}}
\newcommand{\Zb}{\mathbf{Z}}
\newcommand{\Vb}{\mathbf{V}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\WW}{\mathbf{W}}
\newcommand{\VV}{\mathbf{V}}
\newcommand{\MM}{\mathbf{M}}
\newcommand{\JJ}{\mathbf{J}}
\newcommand{\CC}{\mathbf{C}}
\newcommand{\EE}{\mathbf{E}}

\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\begin{document}

\title{THUNDR: Transformer-based 3D HUmaN  Reconstruction with Markers}

\author{Mihai Zanfir \\
  \texttt{mihaiz@google.com} \\
  \and
 Andrei Zanfir \\
  \texttt{andreiz@google.com} \\
\and
  Eduard Gabriel Bazavan \\
\texttt{egbazavan@google.com} \\
  \and
  William T. Freeman \\
  \texttt{wfreeman@google.com} \\
  \and
  Rahul Sukthankar \\
  \texttt{sukthankar@google.com} \\
  \and
  Cristian Sminchisescu \\
  \texttt{sminchisescu@google.com} \\
  \\
  {\bf \large Google Research}\\
}

\twocolumn[{\renewcommand\twocolumn[1][]{#1}\maketitle
\begin{center}
    \centering
    \includegraphics[width=.9\textwidth]{Figures/teaser.png}
    \vspace{-4mm}
    \captionof{figure} {Automatic 3d pose and shape reconstruction results with THUNDR. \textit{(Left)} Input image. \textit{(Middle)} Reconstructed 3d meshes projected on the camera plane and overlayed on the image. \textit{(Right)} Different viewpoint showing our intermediate predicted marker representation (in green) and the reconstructed surface geometry. THUNDR provides automatic 3D scene placement of the reconstructed humans under a perspective camera model.}
\end{center}}]
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
We present THUNDR, a transformer-based deep neural network methodology to reconstruct the 3d pose and shape of people, given monocular RGB images. Key to our methodology is an intermediate 3d marker representation, where we aim to combine the predictive power of model-free-output architectures and the regularizing, anthropometrically-preserving properties of a statistical human surface model like GHUM---a recently introduced, expressive full body statistical 3d human model, trained end-to-end. Our novel transformer-based prediction pipeline can focus on image regions relevant to the task, supports self-supervised regimes, and ensures that solutions are consistent with human anthropometry. 
We show state-of-the-art results on Human3.6M and 3DPW, for both the fully-supervised and the self-supervised models, for the task of inferring 3d human shape, joint positions, and global translation. Moreover, we observe very solid 3d reconstruction performance for difficult human poses collected in the wild.
\end{abstract}

\section{Introduction}

The significant recent progress in 3d human sensing is supported by the development of statistical human surface models 
and the emergence of different forms of supervised and self-supervised visual inference methods.
The use of statistical human pose and shape models offers advantages in the use of an anatomical and semantically meaningful human body representation, during both learning and inference. Human anthropometry could be used to regularize a learning and inference process, which, in the absence of such constraints, and given the ambiguity of 3d lifting from monocular images, could easily run haywire. This is especially true for unfamiliar and complex poses not previously seen in a `training set'---as they never all are. Semantic models offer not only correspondences with image detector responses (specific body keypoints or semantic segmentation maps) which can give essential alignment signals for 3d self-supervision, but can also help rule out 3d solutions that may otherwise entirely break the symmetry of the body, the relative proportions of limbs, the consistency of the surface in terms of non self-intersection, or the anatomical joint angle limits. 

The choice of evaluation metrics has an important role, too. For now, by far the most used representation---perceived as `model-independent'---are the `body joints', a popular concept, neither by virtue of its anatomical clarity (as that point idealization could be bio-mechanically argued against), nor---for computer vision, and more practically---given its lack of ground-truth observability. In practice, human `body joints' are obtained either by fitting proprietary articulated 3d body models to marker data (internal models of the Mocap system, where the assumptions and error models are not always available) or by human annotators eye-balling joint positions in images, followed by multi-view triangulation to obtain pseudo-ground truth. While the latter have proven extremely useful in bootstraping initial 3d predictors, the joint-click positioning cannot be considered an accurate anatomical reality, in any single image, and even less so, consistently, over a large corpora, especially as for many non-frontal-parallel  poses `joint locations' are difficult to correctly identify, visually. 
While some form of 3d body joint prediction error seems unavoidable under the current ground-truth and state of the art metrics, a safeguard could be to operate primarily with visually grounded structures and obtain joint estimates using statistical body models, based on their surface estimates, as just a final step. 


In this paper, we rely on the visual reality of 3d body surface markers (in some conceptualization, a `model-free' representation) and that of a 3d statistical body (a `model-based' concept) as pillars in designing a hybrid 3d visual learning and reconstruction pipeline. Markers have the additional advantages of being useful for registration between different parametric models, can be conveniently relied-upon for fitting, and can be used as a reduced representation of body shape and pose, as we will here show. Our model combines multiple novel transformer refinement stages for efficiency and localization of key predictive features, and relies on combining `model-free' and `model-based' losses for both accuracy and for results consistent with human anthropometry. Quantitative results in major benchmarks indicate state of the art performance. Extensive qualitative testing in the wild supports the overall feasibility, and the quality of 3d reconstructions produced by THUNDR, under both supervised and self-supervised regimes.

\noindent{\bf Related Work:} There is considerable prior work in 3d human sensing which we only briefly mention here without aiming at a full literature review \cite{sminchisescu_ijrr03,bogo2016,SMPL2015,ghum2020,dmhs_cvpr17,zanfir2018monocular,Rhodin_2018_ECCV,Kanazawa2018,kolotouros2019learning,ExPose:2020,choi2020pose2mesh,zanfir2020neural}. Methods sometimes referred as `model-based' \cite{zanfir2018monocular,kolotouros2019learning,guler2019holopose,moon2020MeshNet,jiang2020coherent,biggs20203d,xu2019denserac,zhang2020object,arnab2019exploiting,zeng20203d,georgakis2020hierarchical} rely on statistical human body models like SMPL or GHUM, whereas others sometimes referred to as `model-free' \cite{varol18_bodynet,sun2018integral,iqbal2020weakly,zeng20203d} rely on predicting a set of markers or mesh positions, without forms of statistical surface or kinematic regularization based on human anthropometry. While the second class of techniques tend to perform better in benchmarks (which are mostly emphasizing the prediction of 3d joint locations and occasionally joint angles), the former tend to be more semantically and anatomically intuitive, easier to deploy in the context of self-supervised learning, and more robust in environments, or for poses, not encountered during training. In this work we aim to leverage the advantages of both methods: predicting visually observable sets of markers, and yet regularize estimates using statistical kinematic pose and shape models. Moreover, additional innovations in the use of multiple layers of refining visual transformers, produce significant computational efficiency and accuracy gains in benchmarks, for self-supervised learning, and in the wild. 







\section{Methodology}

In this section we review our methodology including the 3d statistical body models, the marker based-modeling, as well as the proposed THUNDR learning and inference architecture.

\subsection{Statistical 3D Human Body Models}
\label{subsection:GHUM_CAMERA}
We use a recently introduced statistical 3d human body model called GHUM \cite{ghum2020}, to represent the pose and the shape of the human body. The model has been trained end-to-end, in a deep learning framework, using a large corpus of human shapes and motions. 
The model has generative body shape and facial expressions  represented using deep variational auto-encoders and generative pose  for the body, left and right hands respectively represented as normalizing flows \cite{zanfir2020weakly}. The pelvis translation and rotation are controlled separately, and represented by a 6d rotation representation \cite{zhou2018continuity}  and a translation vector  w.r.t the origin . The mesh consists of of  vertices and  triangles. To pose the mesh, we apply the GHUM network  to obtain the posed vertices. We omit the facial expressions and left and right hand poses, as we here focus on main body pose and shape. We also drop the  subscript for convenience.

\vspace{-5mm}
\paragraph{Camera Model} We adopt a pinhole camera model, with intrinsics  and associated perspective projection operation , where . Because we work with cropped images, we also adapt our intrinsics, such that projecting the same 3d points -- either in the cropped image or the original, full image -- would give the same alignment.
The transformation of image intrinsics  into corresponding crop intrinsics  is given by

where  is the scale and translation matrix, adapting the image intrinsics . 

By using a perspective camera model, we ensure that reconstructions are obtained in camera space. Hence, we have meaningful translation and relative positioning of one subject in the scene (or relative positioning of multiple subjects) when reconstructing from monocular images. A perspective model is a much more accurate and general representation of the imaging transformation compared to an orthographic one. It is in our view desirable in all cases, in order to go beyond showing just model projections or reconstructions in a human-centred coordinate system.


\subsection {Marker-based Modelling}
\label{subsection:MARKER_POSER}
Current model-based architectures directly predict specific shape or pose parameters from a raw image. Inspired by model-free methods where weaker constraints are applied on outputs, we adopt an intermediate representation given by 3d surface markers. These can capture human shape and pose and we can predict them directly in a 3d camera space. 

However, training purely model-free methods based on surface markers (as opposed to joint locations), faces additional challenges for both supervised and unsupervised learning. First as such markers are different from joint positions, very few datasets have labels for them. Markers may be available for motion capture datasets in both 2d and 3d, but training a reliable detector is not necessarily easy especially if one seeks generalization outside the lab, where most marker-based systems operate. For self-supervised learning, where additional forms of semantic (body part segmenation) analysis are often necessary, the lack of a statistical body model would render such potentially useful signals unavailable. Finally---and especially when learning with small supervised training sets or for exploratory self-supervised learning---, the lack of regularization given by a body model could lead to 3d predictions with inconsistent anthropometry, further derailing a convergent learning process.



Our approach is to use a 3d surface marker set as an intermediate representation proxy, controlled by both surface (mesh) properties and the parameters of a statistical 3d human pose and shape model (GHUM). For practical considerations, and without loss of generality, we adopt the Human3.6M marker set that consists of  units, see fig.~\ref{fig:marker_poser} for details. We next describe two network heads, which given any 3d markers  achieve the following: {\it (i)} reconstruct the GHUM mesh through a simple architecture , and {\it (ii)} recover the corresponding GHUM parameters  from , so we can also recover an anthropometric mesh equivalent to , .

\vspace{-4mm}

\paragraph{Training the Marker-based Poser (MP)}

The markers are essentially free 3 dof variables, but they follow the given surface placement description, in our case, the VICON protocol. To train a network that maps markers to vertices, we need a dataset of corresponding markers and vertices. 

We take a synthetic sampling approach based on our GHUM model. Given generative codes for pose and shape ,  drawn from the Haar distribution on , and  uniformly sampled from a  meters box, we produce a posed GHUM  sample mesh . The associated markers can be retrieved by a simple (fixed) linear regression matrix , such that . In our experiments, we noticed that injecting noise at this point, \ie , supports the more accurate retrieval of the full mesh given real, imprecise markers that one could find in motion capture datasets such as CMU or Human3.6M, or as produced by an image-based marker regressor.
An overview of the poser function, denoted MP is given in fig. \ref{fig:marker_poser}. We denote  the mesh that is directly predicted from markers. We denote  the mesh that is parametrically obtained by posing the GHUM model given parameters  regressed from markers. For training, we use the loss

\noindent where  and  are the mean-per-vertex errors, computed using a  metric, between the input mesh, and the parametric and direct meshes, respectively. We also experimented with supervising  directly, but learning was not successful. 
To make the training process easier, we subtract the mean marker position (computed as the 3d centroid of each ) before regressing  and  and we obtained lower reconstruction errors using this modification.



\begin{figure*}[!tbp]
\begin{center}
\includegraphics[width=.9\linewidth]{Figures/marker_network_5.png}
\end{center}
\vspace{-5mm}
\caption{\small Our marker poser is based on a constrained marker-prediction pipeline which auto-encodes an initially generated, body mesh that is consistent with the human anthropometry  into a set of markers  via a linear layer characterized by a matrix . The markers are then used to predict both the GHUM parameters, resulting in a mesh  (we center the markers before regressing  and ) and a free-form mesh . Training losses ensure the consistency between ,  and .  We also show a detail of the cannonical marker placement, as attached on the GHUM model. Notice slight left/right and more pronounced front/back placement asymmetries that help disambiguate the model side and facing direction.
}
\label{fig:marker_poser}
\end{figure*}

\subsection {THUNDR}

In fig.~\ref{fig:THUNDR} we show an overview of our proposed hybrid learning architecture for monocular 3d body pose and shape estimation. Our architecture is different from existing pose and shape estimation methods, that directly regress the parameters of a human model (\ie SMPL or GHUM) from a single feature representation of an image. We instead regress an intermediate 3d representation in the form of surface landmarks (markers) and regularize it in training using a statistical body model. Moreover, we preserve the spatial structure of high-level image features by avoiding pooling operations, and relying instead on self-attention to enrich our representation~\cite{Vaswani2017}. We draw inspiration from vision transformers~\cite{dosovitskiy2020image}, as we also use a hybrid convolutional-transformer architecture, and from \cite{zanfir2020neural}, as we explore the idea of iteratively refining estimates by relying on cascaded, input-sensitive processing blocks, with homogeneous parameters. 

Our network receives as input a cropped image  of a person, together with the pseudo ground-truth camera intrinsics  of the crop (see \S~\ref{subsection:GHUM_CAMERA}). We apply a convolutional neural network (CNN) on the input image and extract a downsampled feature map representation . We flatten the feature map along the spatial dimensions to get a sequence of  tokens. We append to each token the camera intrinsics and get our input feature sequence . This sequence is linearly embedded by means of matrix , where  is the embedding dimensionality, and concatenate it with an extra learnable [\textit{markers}] token, . Next, learnable positional embeddings  are added to the sequence. Different from standard transformer architectures, we use a single transformer encoder layer~\cite{Vaswani2017}, \textit{TL}, to iteratively refine our input representation for a number of  steps. We collect at each stage , a refinement update , with  the number of markers, from each transformed representation  using a shared MLP applied on the representation of the [\textit{markers}] token,  

The refinement updates  are added to the default marker coordinates, , as

\noindent where  is a parameter controlling the step size.  are computed based on the default GHUM parameters, , and camera intrinsics. That is, we find the optimal translation  such that the corresponding posed mesh projects in the center of the image~\cite{zanfir2020neural}. Finally,  is computed as



We apply the pre-trained marker-based poser MP (see \S~\ref{subsection:MARKER_POSER}) on  in order to recover the GHUM mesh and parameters, . We also compute the mesh geometry using the standard GHUM poser from the regressed model parameters, . During training, we use a mixed regime based on both weak 2d supervision losses and full 3d supervision losses, where data is available.

We include regularization losses for pose and shape, as


For this constraint to also affect the predicted markers  in a direct manner, we must formulate a consistency loss between the two representations. We set a novel loss that measures the \textit{mean per-marker position error} (i.e. MPMPE) between the predicted markers and the markers on the surface of , \ie , as


We use a standard 2d reprojection loss measured with respect to either annotated or predicted keypoints, , weighted by a per-keypoint confidence score , with  the number of keypoints. From our directly regressed mesh  we extract 3d joints  via the standard GHUM regressor and project them using camera intrinsics  to predict 2d keypoints


Similarly to \cite{zanfir2020neural}, we use a soft differentiable rasterizer ~\cite{liu2019soft} to compute a body part alignment loss with respect to either ground-truth or predicted body part maps , with  different body part labels


\noindent where  is the rasterized image of the 3d body parts of , projected using camera intrinsics .

Given access to 3d supervision with ground-truth vertices  and joints , we use standard vertex and 3d keypoints losses:

with  the MPVE (mean per vertex error) metric and  the MPJPE (mean per joint position error) metric. Parameters  and  control the importance of each loss.

Finally, we can write our full loss function, as follows

where  are used to weigh the different loss components. The fully supervised loss  is only used if there exists 3d ground truth information.
    

\begin{figure*}[!ht]
\begin{center}
\includegraphics[width=0.9\linewidth]{Figures/overview_thundr.png}
\end{center}
\vspace{-5mm}
\caption{\small Overview of our proposed THUNDR architecture, to estimate the parameters of a generative human model (GHUM). \textit{(Top)} Given an input image, we first use a CNN to extract a feature map , where  and  represent the spatial extent, and  the number of channels per feature. In this example . We serialize the feature map and concatenate to each feature the camera intrinsics of the image, . Next, we take our sequence, linearly embed it and add positional encoding. We also add an extra learnable [\textit{markers}] token to the input. This representation is iteratively transformed  times through \textit{the same} transformer encoder layer with learnable weights . At each transformation stage , we gather the representation of the [\textit{markers}] token, feed it through an MLP and regress the marker coordinates refinement . \textit{(Bottom)} We compute the default marker coordinates  as a function of the image camera intrinsics and default GHUM model parameters. The regressed marker coordinates displacements are added to it and the result represents the final estimated marker coordinates . We use the pre-trained marker-based poser MP to get our predicted ghum model vertices and parameters.}
\label{fig:THUNDR}
\end{figure*}

\section{Experiments}\label{sec:exps}

\paragraph{Datasets}
We use two datasets containing images in-the-wild, COCO2017 \cite{lin2014microsoft} (30,000 images) and OpenImages \cite{OpenImages} (24,000 images) for our weakly-supervised training (WS). We use the 2d keypoint annotations where available, otherwise we rely on a 2d pose detector to supplement missing annotations and use an additional confidence score per keypoint.

For the fully-supervised (FS) experiments, we use two standard datasets Human3.6M~\cite{Ionescu14pami} and 3DPW~\cite{vonMarcard2018}. Because the ground-truth of 3DPW is provided as SMPL~\cite{SMPL2015} 3d meshes, we use GHUM fits to these meshes to report the vertex-to-vertex errors. The MPJPE metrics are reported on the 3d joints regressed from the ground-truth SMPL meshes, as standard in the literature. Differently from existing methods, we use less 3d supervision, with superior results. We did not include additional datasets such as MuCo-3DHP~\cite{mehta2018single}, MPI-INF-3DHP ~\cite{mehta2017vnect} or UP3D~\cite{Lassner2017UP3D}, but we believe they could be helpful in further increasing our reconstruction performance.

\vspace{-3mm}


\paragraph{Implementation details} In all our experiments we use a ResNet50~\cite{he2016identity} backbone pretrained for the ImageNet~\cite{imagenet} image classification task. 
Our complete architecture has M parameters, M for the backbone and M for the transformer layer and the MLP regressor. We use  stages, step size , an embedding size  and  heads for the MultiHeadAttention layer. We train the network for  epochs, with batch size of , base learning rate of  and exponential decay . Our marker poser MP has M parameters and consists of MLPs with a hidden layer size of 256. The network is trained for M steps with a batch size of . All our networks were trained on a single V100 GPU with 16GB of memory. Our code is implemented in TensorFlow.

\begin{figure*}[!ht]
\begin{center}
\includegraphics[width=0.87\linewidth]{Figures/results_upper.png}
\includegraphics[width=0.87\linewidth]{Figures/results_lower.png}
\end{center}
\caption{\small Results of THUNDR on images in the wild. From top to bottom: {\it (i)} input image {\it (ii)} direct mesh reconstructions  {\it (iii)} parameteric mesh reconstructions . Notice that direct mesh reconstruction aligns better, particularly the feet and the limbs. {\it (iv)} reconstructions seen from a different viewpoint with regressed marker representations shown in green. {\it (v)} the image attention map for the [\textit{markers}] token, aggregated over all transformer layers. }
\label{fig:Results}
\vspace{-3mm}
\end{figure*}

\begin{table*}[!htbp]
    \small
    \centering
    \begin{tabular}[t]{|l||r|r|r|}
    \hline
    \textbf{Method}  & {MPJPE-PA} & {MPJPE} & {Translation Error} \\ 
    \hline
    \hline
    HMR (WS) \cite{Kanazawa2018} &  &  &NR \\
    \hline    
    HUND (SS) \cite{zanfir2020neural} &  &  & \\
    \hline
    \textbf{THUNDR (WS)} &  &  &  \\
    \hline
    \hline
    HMR \cite{Kanazawa2018} &  & & NR \\
    \hline
    HUND \cite{zanfir2020neural} &  & &  \\
    \hline
    \textbf{THUNDR} &  &  &  \\
    \hline
    \end{tabular}
    \caption{\small Performance of different pose and shape estimation methods on the Human3.6M dataset, with training/testing under protocol P1.}
\label{tbl:H36MP1}
\end{table*}

\subsection{3D Pose and Shape Reconstruction}

For this task, we report several common error metrics that are used for
evaluating the error of 3d reconstruction. Most commonly used for 3d joint errors are mean per joint position error (MPJPE) and MJPE-PA, which is MPJPE after rigid alignment of the prediction with ground truth via Procrustes Analysis. The latter metric, removes global misalignment (\ie scale and rotation) and mainly evaluates the quality of the reconstructed 3d pose. For evaluating 3d shape we use the MPVPE metric between the vertices of the predicted and ground-truth meshes, respectively.

We evaluate our networks on the two datasets that provide 3d ground-truth information, Human3.6M and 3DPW. For the Human3.6M dataset, there are three commonly used evaluation protocols in the literature. Protocols P1 and P2 consider splitting the official training set into new training and testing subsets, with subjects S1, S5-S8 for training and S9 and S11 for evaluation. P1 evaluates on all available camera views in testing, while P2 only on a single predefined camera view (we consider this to be a highly inconclusive protocol due to its small size and design but report on it in order to compare to other methods). The third and most representative protocol we consider is the official one, where we evaluate on the hold-out test dataset of 900K samples. We also submit predictions on the official website for other methods (where code and models are available) to get comparable results. To be fair in our comparison with other methods, we do not retrain on the whole official training dataset.  We show results for all protocols in tables ~\ref{tbl:H36MP1}, \ref{tbl:H36MP2} and \ref{tbl:H36MOfficial}. For P1, we report results for both the weakly supervised regime (WS) and for the mixed regime (WS+FS) in order to compare with prior work. For the official protocol, we report only the MPJPE rounded to the nearest integer, as this is the format the results are returned by the official site. On all protocols and in all training regimes, we obtain state-of-the-art results. In table~\ref{tbl:3DPW} we report errors on the testing split of the 3DPW dataset. We obtain better results than the prior state-of-the-art, in both the WS+FS regime and the WS regime.

In fig.~\ref{fig:Results} we show qualitative reconstructions from THUNDR in-the-wild where one can observe that direct mesh reconstructions  have better image alignment in general. We also show the image attention map for the [\textit{markers}] token, aggregated over all transformer layers. Notice how the network learns to focus on faces, hands and feet.
\vspace{-7mm}

\paragraph{Ablation Studies}
\begin{table}[!htbp]
    \small
    \centering
    \begin{tabular}[t]{|l||r|r|}
    \hline
    \textbf{Method}  & {MPJPE-PA} & {MPJPE}\\ 
    \hline
    THUNDR-GHUM (WS) &  &  \\
    \hline
    THUNDR-{} (WS) &  &  \\
    \hline
    \textbf{THUNDR (WS)} &  &  \\
    \hline
    \end{tabular}
    \vspace{-3mm}

    \caption{\small Ablation study on different variations of THUNDR: THUNDR-GHUM directly regresses GHUM parameters from the image and THUNDR-{} is our standard version were we instead evaluate on the predicted parameteric mesh {}. This evaluation is done in a weakly supervised regime and we report error metrics on Human3.6M protocol P2. }
\label{tbl:ablation_ghum}
\end{table}
In table~\ref{tbl:ablation_ghum}, we ablate different methodological choices in our proposed architecture in the weakly supervised regime and report results on protocol P2 of Human3.6M dataset. First, we change THUNDR to directly regress GHUM parameters (\ie ) from the input image, skipping our intermediate marker representation and removing the marker poser MP. The convolutional-transformer architecture stays mostly the same, with some minor modifications to accommodate more output variables (\ie we use  extra input tokens, one for each GHUM parameter, instead of ). This performs worse than our proposed architecture THUNDR and this shows that our intermediate marker representation is easier to learn from image features. Next, as in our full method we only use the direct mesh , we also show the errors if we instead evaluate on the parameteric mesh  (we denote this by THUNDR-). These results are also better than THUNDR-GHUM, but worse than THUNDR. This again suggests the utility of our intermediate representation and the importance of working with two separate mesh reconstructions.

\vspace{-3mm}
\paragraph{Marker Poser}
We present more details on the training of the marker poser and its additional benefits, outside of the transformer-based 3d pose reconstruction architecture. During training, we experiment with  levels of Gaussian noise added to the markers, as  mm. We ablate each one of the trained marker poser models on the Human3.6M ground-truth marker data. The best performance in reconstruction is achieved for the network with  mm. This model is used in all of our other experiments. During training, the error on the direct mesh reconstruction reaches  mm MPVPE, while the parameteric mesh reconstruction reaches  mm MPVPE.
\vspace{-4mm}
\paragraph{Mesh Fitting} 
We test our marker-based poser on the Human3.6M dataset, for which the authors shared 3D marker positions for the training data. We fit an associated GHUM mesh in two ways: {\it (i)} by minimizing an energy that takes into account 3d marker ground truth, 2d reprojection errors for all GHUM 3d body joints (including hands and face) and a semantic alignment cost, and {\it ( ii)} by simply running our trained marker poser on the ground-truth 3d marker positions to produce a mesh . For a sequence fitting example, see our Sup. Mat.
First, we compute the mean per-marker error for the models  obtained from energy optimization to ground-truth makers  (\ie those recovered from mocap data). This gives an error of  mm, with an average processing rate of  frames/second. Second, we compute the errors of markers placed on the predicted mesh  given ground-truth marker positions. This achieves a slightly higher error of  mm, but with an average processing rate of  frames/second, when ran sequentially. Note that our marker poser has never seen the marker sequences of Human3.6M during training, as the marker poser was trained with samples drawn from a normalizing flow prior based on the CMU motion capture dataset~\cite{CMUMocap}.\begin{table}[!htbp]
    \small
    \centering
    \begin{tabular}[t]{|l||r|r|}
    \hline
    \textbf{Method}  & {MPJPE-PA} & {MPJPE} \\ 
    \hline
    \hline
    HMR \cite{Kanazawa2018} &  & \\
    \hline
    GraphCMR \cite{kolotouros2019convolutional} &  &NR \\
    \hline
    Pose2Mesh \cite{choi2020pose2mesh} &  & \\
    \hline
    I2L-MeshNet \cite{moon2020MeshNet} &  &  \\
\hline
    SPIN \cite{kolotouros2019learning} &  &NR\\
\hline
    \hline
    \textbf{THUNDR} &  &   \\
    \hline
    \end{tabular}
    \vspace{-3mm}
    \caption{\small Performance of different pose and shape estimation methods on the Human3.6M dataset, protocol P2.}
\label{tbl:H36MP2}
\end{table}

\begin{table}[!htbp]
    \small
    \centering
    \begin{tabular}[t]{|l||r|r|r|}
    \hline
    \textbf{Method}  & {MPJPE-PA} & {MPJPE} & {MPVPE} \\ 
    \hline
    \hline
    HUND \cite{zanfir2020neural} (SS) &  &  & NR\\
    \textbf{THUNDR (WS)} &  &  &NR \\
    \hline
    \hline
    HMR \cite{Kanazawa2018} &  & NR &NR\\
    \hline
    GraphCMR \cite{kolotouros2019convolutional} &  &NR &NR\\
    \hline
    SPIN \cite{kolotouros2019learning} &  &NR & \\
    \hline
    Pose2Mesh \cite{choi2020pose2mesh} &  & &NR\\
    \hline
    I2L-MeshNet \cite{moon2020MeshNet} &  &  &NR\\
    \hline
    HUND \cite{zanfir2020neural} &  &  & NR\\
\hline
\textbf{THUNDR} &  &  & * \\
    \hline
    \end{tabular}
    \vspace{-3mm}
    \caption{\small Performance of different pose and shape estimation methods on the 3DPW dataset.*\textit{Shape evaluation is done on GHUM.}}
\label{tbl:3DPW}
\end{table}

\begin{table}[!htbp]
    \small
    \centering
    \begin{tabular}[t]{|l||r|}
    \hline
    \textbf{Method}  & {MPJPE}\\ 
    \hline
    \hline
    HMR \cite{Kanazawa2018} & \\
    \hline
    SPIN \cite{kolotouros2019learning} &  \\
    \hline
    HUND \cite{zanfir2020neural} &  \\
    \hline
    \textbf{THUNDR} &  \\
    \hline
    \end{tabular}
    \vspace{-3mm}
    \caption{\small Performance of different methods on the Human3.6M official, representative held-out test set, containing 900K samples. }
\label{tbl:H36MOfficial}
\end{table}

\vspace{-4mm}
\paragraph{Ethical Considerations} Our methodology aims to decrease bias by introducing flexible forms of self-supervision which would allow, in principle, for system bootstrapping and adaptation to new domains and fair, diverse subject distributions, for which labeled data may be difficult or impossible to collect upfront. Applications like visual surveillance and person identification would not be effectively supported currently, given that model's output does not provide sufficient detail for these purposes. This is equally true of the creation of potentially adversely-impacting deepfakes, as we do not include an appearance model or a joint audio-visual model.

\section{Conclusions}

We have presented THUNDR, a transformer-based deep neural network methodology to reconstruct the 3d pose and shape of people, given monocular RGB images.  Faced with the difficult issue of handling not directly observable human body joints, on which nevertheless many error metrics are based, and aiming at both reconstruction accuracy and good self-supervised learning and generalization under anthropometric human body constraints, we propose a novel model that combines a surface-marker representation with 3d statistical body regularization. The model is designed around a learnable pipeline that refines multiple transformer layers for computational efficiency and for precise, task-sensitive, image feature localization.
We demonstrate state-of-the-art results on Human3.6M and 3DPW, in both the fully-supervised and the self-supervised regimes. In our extensive qualitative assessment (see Sup. Mat.) we observe  accurate 3d reconstruction performance for difficult human poses collected under challenging imaging conditions.



\clearpage
\bibliographystyle{ieee_fullname}
\bibliography{egbib}


\end{document}
