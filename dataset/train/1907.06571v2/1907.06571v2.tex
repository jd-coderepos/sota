
\documentclass{article} \usepackage{iclr2020_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{subfig}
\usepackage[pdftex]{graphicx}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}
\usepackage{listings}
\usepackage{floatrow}
\usepackage{setspace}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{wrapfig,lipsum,booktabs}
\newfloatcommand{capbtabbox}{table}[\captop][]
\newfloatcommand{widecapbtabbox}{table}[\captop][\FBwidth]


\usepackage{color}
\usepackage{xcolor}
\usepackage{textcomp}


\lstset{
language=Python,
columns=fullflexible,
emph={MyClass,__init__},
frame=tb,                         showstringspaces=false            }




\usepackage{caption}
\usepackage{soul}

\newcommand{\gen}{\mathcal{G}}
\newcommand{\disc}{\mathcal{D}}

\title{Adversarial Video Generation \\on Complex Datasets}



\author{Aidan Clark\\
  DeepMind\\
  London, UK \\
  \texttt{aidanclark@google.com} \\
  \And
  Jeff Donahue\\
  DeepMind\\
  London, UK \\
  \texttt{jeffdonahue@google.com} \\
  \And
  Karen Simonyan\\
  DeepMind\\
  London, UK \\
  \texttt{simonyan@google.com} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Generative models of natural images have progressed towards high fidelity samples by the strong leveraging of scale. We attempt to carry this success to the field of video modeling by showing that large Generative Adversarial Networks trained on the complex Kinetics-600 dataset are able to produce video samples of substantially higher complexity and fidelity than previous work. Our proposed model, Dual Video Discriminator GAN (DVD-GAN), scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. We evaluate on the related tasks of video synthesis and video prediction, and achieve new state-of-the-art Fr\'echet Inception Distance for prediction for Kinetics-600, as well as state-of-the-art Inception Score for synthesis on the UCF-101 dataset, alongside establishing a strong baseline for synthesis on Kinetics-600.
\end{abstract}

\section{Introduction}

\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{samples/new_titles2.pdf}
\vspace{-0em}
\caption{Selected frames from videos generated by a DVD-GAN trained on Kinetics-600 at , , and  resolutions (top to bottom).}
\label{fig:court}
\end{figure}

Modern deep generative models can produce realistic natural images when trained on high-resolution and diverse datasets \citep{brock2018large, karras2018style, kingma2018glow, menick2018generating, razavi2019generating}. Generation of natural \textit{video} is an obvious further challenge for generative modeling, but one that is plagued by increased data complexity and computational requirements. For this reason, much prior work on video generation has revolved around relatively simple datasets, or tasks where strong temporal conditioning information is available.

We focus on the tasks of video synthesis and video prediction (defined in Section~\ref{ss:synth}), and aim to extend the strong results of generative image models to the video domain. Building upon the state-of-the-art BigGAN architecture \citep{brock2018large}, we introduce an efficient spatio-temporal decomposition of the discriminator which allows us to train on Kinetics-600 -- a complex dataset of natural videos an order of magnitude larger than other commonly used datasets. The resulting model, Dual Video Discriminator GAN (DVD-GAN), is able to generate temporally coherent, high-resolution videos of relatively high fidelity (Figure \ref{fig:court}).

Our contributions are as follows:
\begin{itemize}
\item We propose DVD-GAN -- a scalable generative model of natural video which produces high-quality samples at resolutions up to  and lengths up to 48 frames.

\item We achieve state of the art for video synthesis on UCF-101 and prediction on Kinetics-600.

\item We establish class-conditional video synthesis on Kinetics-600 as a new benchmark for generative video modeling, and report DVD-GAN results as a strong baseline.

\end{itemize}

\section{Background}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{samples/interesting_samples_iclr.pdf}

\caption{Generated video samples with interesting behavior. In raster-scan order:
\textbf{a)} On-screen generated text with further lines appearing..
\textbf{b)} Zooming in on an object. \textbf{c)} Colored detail from a pen being left on paper. \textbf{d)} A generated camera change and return.}
\label{fig:interesting}
\end{figure}

\subsection{Video Synthesis and Prediction}
\label{ss:synth}
The exact formulation of the video generation task can differ in the type of conditioning signal provided. 
At one extreme lies unconditional video synthesis where the task is to generate any video following the training distribution. 
Another extreme is occupied by strongly-conditioned models, including generation conditioned on another video for content transfer \citep{Bansal_2018_ECCV, zhou2019dance}, per-frame segmentation masks \citep{DBLP:journals/corr/abs-1808-06601}, or pose information \citep{pos_iccv2017, villegas2017learning, yang2018pose}.
In the middle ground there are tasks
which are more structured than unconditional generation, and yet are more challenging from a modeling perspective than strongly-conditional generation (which gets a lot of information about the generated video through its input). 
The objective of \textit{class-conditional video synthesis} is to generate a video of a given category (e.g., ``riding a bike'') while \textit{future video prediction} is concerned with generation of continuing video given initial frames. These problems differ in several aspects, but share a common requirement of needing to generate realistic temporal dynamics, and in this work we focus on these two problems.

\subsection{Generative Adversarial Networks}

Generative Adversarial Networks (GANs) \citep{goodfellow2014generative} are a class of generative models defined by a minimax game between a \textit{Discriminator}  and a \textit{Generator} . The original objective was proposed by \citet{goodfellow2014generative}, and many improvements have since been suggested, mostly targeting improved training stability \citep{arjovsky2017wasserstein,zhang2018self,brock2018large,gulrajani2017improved,miyato2018spectral}. We use the hinge formulation of the objective \citep{lim2017geometric, brock2018large} which is optimized by gradient descent ( is the elementwise ReLU function):



GANs have well-known limitations including a tendency towards limited diversity in generated samples (a phenomenon known as mode collapse) and the difficulty of quantitative evaluation due to the lack of an explicit likelihood measure over the data.
Despite these downsides, GANs have produced some of the highest fidelity samples across many visual domains \citep{karras2018style,brock2018large}.

\subsection{Kinetics-600}

Kinetics is a large dataset of 10-second high-resolution YouTube clips \citep{kay2017kinetics,Kinetics} originally created for the task of human action recognition. We use the second iteration of the dataset, Kinetics-600~\citep{carreira2018short}, which consists of 600 classes with at least 600 videos per class for a total of around 500,000 videos.\footnote{Kinetics is occasionally pruned and so we cannot give an exact size.}
Kinetics videos are diverse and unconstrained, which allows us to train large models without being concerned with the overfitting that occurs on small datasets with fixed objects interacting in specified ways~\citep{ebert2017self,blank2005actions}. 
Among prior work, the closest dataset (in terms of subject and complexity) which is consistently used is UCF-101~\citep{soomro2012ucf101}.
We focus on Kinetics-600 because of its larger size (almost 50x more videos than UCF-101) and its increased diversity (600 instead of 101 classes -- not to mention increased intra-class diversity). Nevertheless for comparison with prior art we train on UCF-101 and achieve a state-of-the-art Inception Score there. Kinetics contains many artifacts expected from YouTube, including cuts (as in Figure~\ref{fig:interesting}d), title screens and visual effects. Except when specifically described, we choose frames with stride 2 (meaning we skip every other frame). This allows us to generate videos with more complexity without incurring higher computational cost.

To the best of our knowledge we are the first to consider generative modelling of the entirety of the Kinetics video dataset\footnote{In parallel with the concurrent work of~\citet{weissenborn2019scaling}.}, although a small subset of Kinetics consisting of 4,000 selected and stabilized videos (via a SIFT + RANSAC procedure) has been used in at least two prior papers \citep{li2018video, balaji2018tfgan}. Due to the heavy pre-processing and stabilization present, as well as the sizable reduction in dataset size (two orders of magnitude) we do not consider these datasets comparable to the full Kinetics-600 dataset.


\subsection{Evaluation Metrics}
\label{ss:metrics}

Designing metrics for measuring the quality of generative models (GANs in particular) is an active area of research~\citep{sajjadi2018assessing,barratt2018note}. In this work we report the two most commonly used metrics, Inception Score (IS)~(\cite{salimans2016improved}) and Fr\'echet Inception Distance (FID)~\citep{heusel2017gans}.
The standard instantiation of these metrics is intended for generative image models, and uses an Inception model~\citep{szegedy2015rethinking} for image classification or feature extraction.
For videos, we use
the publicly available Inflated 3D Convnet (I3D) network trained on Kinetics-600~\citep{carreira2017quo}. 
Our Fr\'echet Inception Distance is therefore very similar to the Fr\'echet Video Distance (FVD)~\citep{unterthiner2018towards},
although our implementation is different and more aligned with the original FID metric.\footnote{We use `avgpool' features (rather than logits) by default, our I3D model is trained on Kinetics-600 (rather than Kinetics-400), and we pre-calculate ground-truth statistics on the entire training set.}

\section{Dual Video Discriminator GAN}
\label{s:dvdgan}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{graphics/arch_legend.pdf}
\vspace{-1em}
\caption{Simplified architecture diagram of  (left) and / (right). More details in \ref{ap:model}.}
\label{fig:diagram}
\end{figure}

Our primary contribution is Dual Video Discriminator GAN (DVD-GAN), a generative video model of complex human actions built upon the state-of-the-art BigGAN architecture \citep{brock2018large} while introducing scalable, video-specific generator and discriminator architectures. An overview of the DVD-GAN architecture is given in Figure~\ref{fig:diagram} and a detailed description is in Appendix~\ref{ap:model}. Unlike some of the prior work, our generator contains no explicit priors for foreground, background or motion (optical flow); instead, we rely on a high-capacity neural network to learn this in a data-driven manner. While DVD-GAN contains sequential components (RNNs), it is not autoregressive in time or in space. In other words, the pixels of each frame do not directly depend on other pixels in the video, as would be the case for auto-regressive models or models generating one frame at a time.

Generating long and high resolution videos is a heavy computational challenge: individual samples from Kinetics-600 (just 10 seconds long) contain upwards of 16 million pixels which need to be generated in a consistent fashion. This is a particular challenge to the discriminator. For example, a generated video might contain an object which leaves the field of view and incorrectly returns with a different color. Here, the ability to determine this video is generated is only possible by comparing two different spatial locations across two (potentially distant) frames. Given a video with length , height , and width , discriminators that process the entire video 
would have to process all  pixels -- limiting the size of the model and the size of the videos being generated. 

\subsection{Dual Discriminators}
\label{ss:dd}

DVD-GAN tackles this scale problem by using two discriminators: a \textit{Spatial Discriminator}  and a \textit{Temporal Discriminator} .   critiques single frame content and structure by randomly sampling  full-resolution frames and judging them individually. We use  and discuss this choice in Section~\ref{ss:kphi}. 's final score is the sum of the per-frame scores. The temporal discriminator  must provide  with the learning signal to generate movement (something not evaluated by ).
To make the model scalable, we apply a spatial downsampling function  to the whole video and feed its output to . We choose  to be  average pooling, and discuss alternatives in Section~\ref{ss:kphi}. This results in an architecture where the discriminators do not process the entire video's worth of pixels, since  processes only  pixels and  only . For a  frame video at  resolution, this reduces the number of pixels to process per video from  to : a  reduction.
Despite this decomposition, the discriminator objective is still able to penalize almost all inconsistencies which would be penalized by a discriminator judging the entire video.  judges any temporal discrepancies across the entire length of the video, and  can judge any high resolution details. The only detail the DVD-GAN discriminator objective is unable to reflect is the temporal evolution of pixels within a  window.
We have however not noticed this affecting the generated samples in practice.
DVD-GAN's  is similar to the per-frame discriminator  in MoCoGAN~\citep{tulyakov2018mocogan}. However MoCoGAN's analog of  looks at full resolution videos, whereas  is the only source of learning signal for high-resolution details in DVD-GAN. For this reason,  is essential when  is not the identity, unlike in MoCoGAN where the additional per-frame discriminator is less crucial.

\subsection{Related Work}
\label{s:rw}

Generative video modeling is a widely explored problem which includes work on VAEs~\citep{babaeizadeh2017stochastic, denton2018stochastic, lee2018stochastic, NIPS2018_7333},
auto-regressive models~\citep{ranzato14video,srivastava15unsupervised,kalchbrenner2017video, weissenborn2019scaling}, normalizing flows~\citep{kumar2019videoflow}, and GANs~\citep{mathieu2015deep, vondrick2016generating, saito2017temporal, saito2018tganv2}. Much prior work considers decompositions which model the texture and spatial consistency of objects separately from their temporal dynamics. One approach is to split  into foreground and background models ~\citep{vondrick2016generating, spampinato2018vos}, while another considers explicit or implicit optical flow in either  or ~\citep{saito2017temporal, ohnishi2018hierarchical}. 
Similar to DVD-GAN, MoCoGAN~\citep{tulyakov2018mocogan} discriminates individual frames in addition to a discriminator which operates on fixed-length -frame slices of the whole video (where ). Though this potentially reduces the number of pixels to discriminate to , \citet{tulyakov2018mocogan} describes discriminating sliding windows, which increases the total number of pixels. Other models follow this approach by discriminating groups of frames~\citep{xie2018tempogan, sun2018two, balaji2018tfgan}.

TGANv2~\citep{saito2018tganv2} proposes ``adaptive batch reduction'' for efficient training, an operation which randomly samples subsets of videos within a batch and temporal subwindows within each video. This operation is applied throughout TGANv2's , with heads projecting intermediate feature maps directly to pixel space before applying batch reduction, and corresponding discriminators evaluating these lower resolution intermediate outputs. An effect of this choice is that TGANv2 discriminators only evaluate full-length videos at very low resolution. We show in Figure~\ref{fig:sd_tf} that a similar reduction in DVD-GAN's resolution when judging full videos leads to a loss in performance. We expect further reduction (towards the resolution at which TGANv2 evaluates the entire length of video) to lead to further degradation of DVD-GAN's quality. Furthermore, this method is not easily adapted towards models with large batch sizes divided across a number of accelerators, with only a small batch size per replica.

\section{Experiments and Analysis} 

A detailed description of our training setup is in Appendix~\ref{ap:train}. Each DVD-GAN was trained on TPU pods~\citep{Google} using between 32 and 512 replicas with an Adam \citep{kingma2014adam} optimizer. Video Synthesis models are trained for around 300,000 learning steps, whilst Video Prediction models are trained for up to 1,000,000 steps.
Most models took between 12 and 96 hours to train. 

\subsection{Class-Conditional Video Synthesis}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{samples/4_classes.png}

\caption{Each row is the first frame of 15 videos from a random class, all from the same checkpoint. The classes are: \textbf{cooking scallops}, \textbf{changing wheel (not on bike)}, \textbf{calculating}, \textbf{dribbling basketball}.}
\label{fig:class_batches}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{samples/watermelon.png}

\caption{All 48 frames (in raster-scan order) from a  sample from watermelon cutting class.}
\label{fig:soccer_game}
\end{figure}


Our primary results concern the problem of \textit{Class-Conditional Video Synthesis}. 
We provide our results for the UCF-101 and Kinetics-600 datasets. With Kinetics-600 emerging as a new benchmark for generative video modelling, our results establish a strong baseline for future work.

\subsubsection{Kinetics-600 Results}

\begin{figure}[h]
\CenterFloatBoxes
\begin{floatrow}
\widecapbtabbox{\begin{tabular}{lllll}
    \toprule
    \multicolumn{1}{c}{(\# Frames / Resolution)}  & \multicolumn{2}{c}{No Truncation} & \multicolumn{2}{c}{With Truncation} \\
      & FID () & IS () & FID () & IS ()   \\
    \midrule
     & 0.85 & 53.81 & 7.13 & 187.23 \\
     & 1.16 & 77.45 & 13.04 & 246.18 \\
     & 2.05 & 62.78 & 10.17 & 162.44 \\
     & 13.75 & 104.09 & 47.86 & 264.12   \\
     & 28.44 & 81.41 & 45.79 & 188.32   \\
    \bottomrule
  \end{tabular}
}{\caption{FID/IS for DVD-GAN on Kinetics-600 Video Synthesis. We present the scores of the model taken at the point in training when the best FID was attained. The "No Truncation" columns contain the scores obtained without the truncation trick. The "With Truncation" columns contain the scores obtained at the truncation level which results in the best Inception Score.}\label{tab:kinetics_untrunc}}
\end{floatrow}
\end{figure}

In Table~\ref{tab:kinetics_untrunc} we show the main result of this paper: benchmarks for Video Synthesis on Kinetics-600. We consider a range of resolutions and video lengths, and measure Inception Score and Fr\'echet Inception Distance (FID) for each (as described in Section~\ref{ss:metrics}). We further measure each model along a truncation curve, which we carry out by calculating FID and IS statistics while varying the standard deviation of the latent vectors between 0 and 1. There is no prior work with which to quantitatively compare these results (for comparative experiments see Section~\ref{ss:ucf_sec} and Section~\ref{ss:fc_kinetics}), but we believe these samples to show a level of fidelity not yet achieved in datasets as complex as Kinetics-600 (see samples from each row in Appendix~\ref{ap:pics}). Because all videos are resized for the I3D network (to ), it is meaningful to compare metrics across equal length videos at different resolutions. Neither IS nor FID are comparable across videos of different lengths, and should be treated as separate metrics.

Generating longer and larger videos is a more challenging modeling problem, which is conveyed by the metrics (in particular, comparing 12-frame videos across ,  and  resolutions). Nevertheless, DVD-GAN is able to generate plausible videos at all resolutions and with length spanning up to 4 seconds (48 frames). As can be seen in Appendix~\ref{ap:pics}, smaller videos display high quality textures, object composition and movement. At higher resolutions, generating coherent objects becomes more difficult (movement consists of a much larger number of pixels), but high-level details of the generated scenes are still extremely coherent, and textures (even complicated ones like a forest backdrop in Figure~\ref{fig:court}a) are generated well. It is further worth noting that the 48-frame models do not see more high resolution frames than the 12-frame model (due to the fixed choice of  described in Section~\ref{ss:dd}), yet nevertheless learn to generate high resolution images.

\subsubsection{Video Synthesis on UCF-101}
\label{ss:ucf_sec}

\begin{figure}[t]
\centering
\begin{floatrow}
\widecapbtabbox{\begin{tabular}{ll}
        \toprule
        Method   & IS () \\
        \midrule
        VGAN~\citep{vondrick2016generating} & 8.31  .09     \\
        TGAN~\citep{saito2017temporal} & 11.85  .07     \\
        MoCoGAN~\citep{tulyakov2018mocogan} & 12.42  .03     \\
        ProgressiveVGAN~\citep{acharya2018towards} & 14.56  .05     \\
        TGANv2~\citep{saito2018tganv2} & 24.34  .35     \\
        \textbf{DVD-GAN (ours)} & \textbf{32.97}  1.7     \\
        \bottomrule
      \end{tabular}
}{\caption{IS on UCF-101 with C3D.}\label{tab:is}}
\widecapbtabbox{\begin{tabular}{ll}
        \toprule
        Method   & FVD () \\
        \midrule
        SVP-FP & 315.5    \\
        CDNA & 296.5    \\
        SV2P & 262.5    \\
        SAVP & 116.4    \\
        DVD-GAN-FP (ours) & 109.8     \\
        Video Transformer & \textbf{94  2}     \\
        \bottomrule
      \end{tabular}
}{\caption{FVD on BAIR.}\label{tab:bair}}
\end{floatrow}
\end{figure}

We further verify our results by testing the same model on UCF-101~\citep{soomro2012ucf101}, a smaller dataset of 13,320 videos of human actions across 101 classes that has previously been used for video synthesis and prediction~\citep{saito2017temporal, saito2018tganv2, tulyakov2018mocogan}. Our model produces samples with an IS of~, significantly outperforming the state of the art (see Table~\ref{tab:is} for quantitative comparison and Appendix~\ref{ss:ucf} for more details).

\subsection{Future Video Prediction}

\textit{Future Video Prediction} is the problem of generating a sequence of frames which directly follow from one (or a number) of initial conditioning frames. Both this and video synthesis require  to learn to produce realistic scenes and temporal dynamics, however video prediction further requires  to analyze the conditioning frames and discover elements in the scene which will evolve over time. In this section, we use the Fr\'echet Video Distance exactly as~\citet{unterthiner2018towards}: using the logits of an I3D network trained on Kinetics-400 as features. This allows for direct comparison to prior work. Our model, DVD-GAN-FP (Frame Prediction), is slightly modified to facilitate the changed problem, and details of these changes are given in Appendix~\ref{ss:dvdgan_cond}.

\begin{figure}[t]
\CenterFloatBoxes
\begin{floatrow}
\widecapbtabbox{\begin{tabular}{lll}
        \toprule
        Method   & Training Set FVD () & Test Set FVD () \\
        \midrule
        Video Transformer~\citep{weissenborn2019scaling} & - & 170  5    \\
        \textbf{DVD-GAN-\textit{FP}} & \textbf{68.66  0.78} & \textbf{69.15  1.16}     \\
        \midrule
        DVD-GAN & 32.3  0.82 & 31.1  0.56     \\
        \bottomrule
      \end{tabular}
}{\caption{DVD-GAN-FP's FVD scores on Video Prediction for 16 frames of Kinetics-600 without frame skipping. The final row represents a \textit{Video Synthesis} model generating 16 frames.}\label{tab:kinetics_fvd}}
\end{floatrow}
\end{figure}

\subsubsection{Frame-Conditional Kinetics}
\label{ss:fc_kinetics}

For direct comparison with concurrent work on autoregressive video models \citep{weissenborn2019scaling} we consider the generation of 11 frames of Kinetics-600 at  resolution conditioned on 5 frames, where the videos for training are not taken with any frame skipping.
We show results for all these cases in Table~\ref{tab:kinetics_fvd}. Our frame-conditional model \textbf{DVD-GAN-\textit{FP}} outperforms the prior work on frame-conditional prediction for Kinetics.
The final row labeled DVD-GAN corresponds to 16-frame class-conditional Video Synthesis samples, generated without frame conditioning and without frame skipping. The FVD of this video synthesis model is notably better.

On the one hand, we hypothesize that the synthesis model has an easier generative task: it can choose to generate (relatively) simple samples for each class, rather than be forced to continue frames taken from videos which are class outliers, or contain more complicated details. On the other hand, a certain portion of the FID/FVD metric undoubtedly comes from the distribution of objects and backgrounds present in the dataset, and so it seems that the prediction model should have a handicap in the metric by being given the ground truth distribution of backgrounds and objects with which to continue videos. The synthesis model's improved performance on this task seems to indicate that the advantage of being able to select videos to generate is greater than the advantage of having a ground truth distribution of starting frames.

\subsubsection{BAIR Robot Pushing}

We further test future video prediction on the single-class BAIR Robot Pushing Dataset \citep{ebert2017self}, a dataset of stationary videos of a robot arm moving around a set of changing objects. In order for direct comparison with previous results reported in \citet{unterthiner2018towards}, we consider generating 15 frames conditioned on a single starting frame. Like on prediction with Kinetics, we report FVD exactly as in \citet{unterthiner2018towards}, with ground truth statistics and conditioning frames taken from the 256-video dev set. Results are reported in Table~\ref{tab:bair}. Scores are taken from \citet{unterthiner2018towards}. DVD-GAN-FP outperforms all prior adversarial models trained on this dataset, but performs slightly worse than Video Transformer, a concurrently developed autoregressive model \citet{weissenborn2019scaling}. 


\subsection{Dual Discriminator Input}
\label{ss:kphi}


\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{experiments/fixed_kphi.png}
\vspace{-1em}
\caption{The effect of  in  (left two) and  in  (right two). FID is similar for any choice of , while IS declines as downsampling increases. Increasing  improves both with diminishing returns.}
\label{fig:sd_tf}
\end{figure}

We analyze several choices for  (the number of frames per sample in the input to ) and  (the downsampling function for ). We expect setting  to the identity or  to result in the best model, but we are interested in the maximally compressive  and  that reduce discriminator input size (and the amount of computation), while still producing a high quality generator. For , we consider:  and  average pooling, the identity (no downsampling), as well as a  which takes a random half-sized crop of the input video (as in \citet{saito2018tganv2}). Results can be seen in Figure~\ref{fig:sd_tf}.
For each ablation, we train three identical DVD-GANs with different random initializations on -frame clips of Kinetics-600 at  resolution for 100,000 steps. We report mean and standard deviation (via the error bars) across each group for the whole training period. For , we consider 1, 2, 8 and 10 frames. We see diminishing effect as  increases, so settle on . We note the substantially reduced IS of  downsampling as opposed to , and further note that taking half-sized crops (which results in the same number of pixels input to  as  pooling) is also notably worse.

\section{Conclusion}

We approached the challenging problem of modeling natural video by introducing a GAN capable of capturing the complexity of 
a large video dataset. 
We showed that on UCF-101 and frame-conditional Kinetics-600 it quantitatively achieves the new state of the art, alongside qualitatively producing sample videos with high complexity and diversity.
We further wish to emphasize the benefit of training generative models on large and complex video datasets, such as Kinetics-600, and envisage the strong baselines we established on this dataset with DVD-GAN will be used as a reference point by the generative modeling community moving forward. While much remains to be done before realistic videos can be consistently generated in an unconstrained setting, we believe DVD-GAN is a step in that direction.

\subsubsection*{Acknowledgments}

We would like to thank Eric Noland and Jo\~ao Carreira for help with the Kinetics dataset and Marcin Michalski and Karol Kurach for helping us acquire data and models for Fr\'echet Video Distance comparison. We would further like to thank Sander Dieleman, Jacob Walker and Tim Harley for useful discussions and feedback on the paper.

\medskip

\bibliography{references}
\bibliographystyle{iclr2020_conference}


\appendix

\section{Separable Attention}

A previous version of this draft contained \textit{Separable Attention}, a module which allows self-attention \citep{vaswani2017attention} to be applied to spatio-temporal features which are too large for the quadratic memory cost of vanilla self-attention. Though DVD-GAN as proposed in this draft does not contain this module, we include a formal definition to aid understanding.

Self-attention on a single batch element  of shape  (where  is the number of spatial positions and  is the number of features per location) can be given as:  where  are parameters all of shape  and the softmax is taken over the final axis. Batched  self-attention is identical, except  has a leading batch axis and matrix multiplications are batched (i.e. the  multiplies two tensors of shape  and  and results in shape ).

Separable Attention recognizes the natural decomposition of  by attending over each axis separately and in order. That is, first each feature is replaced with the result of a self-attention pass which only considers other features at the same  location (but across different frames), then the result of that layer (which contains cross-temporal information) is processed by a second self-attention layer which attends to features at different heights (but at the same width-point, and at the same frame), and then finally one which attends over width. The Python pseudocode\footnote{To be completely correct, a similar function operating on numpy arrays must properly transpose the axes before each reshape to ensure data is formatted in the proper order after the reshape operation.} below implements this module assuming that  is given with the interior axes already separated (i.e.,  is of shape ).

\begin{lstlisting}
def self_attention(x, q, k, v):
  xq, xk, xv = np.matmul(x, q), np.matmul(x, k), np.matmul(x, v)
  qv_correlations = np.matmul(xq, np.transpose(xk))
  return np.matmul(np.softmax(qv_correlations, axis=-1), xv)
 
def separable_attention(x, q1, k1, v1, q2, k2, v2, q3, k3, v3):
  b, h, w, t, c = x.shape
  # Apply attention over time.
  x = np.reshape(x, [b*h*w, t, c])
  x = self_attention(x, q1, k1, v1)
  # Apply attention over height.
  x = np.reshape(x, [b*w*t, h, c])
  x = self_attention(x, q2, k2, v2)
  # Apply attention over width.
  x = np.reshape(x, [b*h*t, w, c])
  x = self_attention(x, q3, k3, v3)
  return x
\end{lstlisting}

Separable Attention crucially reduces the asymptotic memory cost from  to  while still allowing the result of the module to contain features at each location accumulated from all other features at any spatio-temporal location.

\section{Experiment Methodology}

\subsection{Dataset Processing}

For all datasets we randomly shuffle the training set for each model replica independently. Experiments on the BAIR Robot Pushing dataset are conducted in the native resolution of , where for UCF-101 we operate at a (downsampled)  resolution. This is done by a bilinear resize such that the video's smallest dimension is mapped to 128 pixels (maintaining aspect ratio). From this we take a random 128-pixel crop along the other dimension. We use the same procedure to construct datasets of different resolutions for Kinetics-600.
All three datasets contain videos with more frames than we generate, so we take a random sequence of consecutive frames from the resized output. 

\subsection{Architecture Description}
\label{ap:model}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{graphics/arch_blocks.pdf}
\vspace{-1em}
\caption{The residual blocks for  and /. See Figure~\ref{fig:diagram} for the icons and \ref{ap:model} for more detail.}
\label{fig:blocks}
\end{figure}

Our model adopts many architectural choices from~\citet{brock2018large} including our nomenclature for describing network width, which is determined by the product of a channel multiplier  with a constant for each layer in the network. The layer-wise constants for  are  for  videos and  for . The width of the -th layer is given by the product of  and the -th constant and all layers prior to the residual network in  use the initial layer's multiplier and we refer to the product of that and  as .  in DVD-GAN is 128 for videos with  resolution and 96 otherwise. The corresponding  lists for both  and  are  for  resolution and  for .

The input to  consists of a Gaussian latent noise  and a learned linear embedding  of the desired class . Both inputs are -dimensional vectors.  starts by computing an affine transformation of  to a -shaped tensor (in Figure~\ref{fig:diagram} this is represented as a  convolution).  is used as the input to all class-conditional Batch Normalization layers throughout  (the gray line in Figure~\ref{fig:blocks}).

This is then treated as the input (at each frame we would like to generate) to a Convolutional Gated Recurrent Unit \citep{ballas2015delving, sutskever2011generating} whose update rule for input  and previous output  is given by the following:



In these equations  and  are the elementwise sigmoid and ReLU functions respectively, the  operator represents a convolution with a kernel of size , and the  operator is an elementwise multiplication. Brackets are used to represent a feature concatenation. This RNN is unrolled once per frame. The output of this RNN is processed by two residual blocks (whose architecture is given by Figure~\ref{fig:blocks}). The time dimension is combined with the batch dimension here, so each frame proceeds through the blocks independently. The output of these blocks has width and height dimensions which are doubled (we skip upsampling in the first block). This is repeated a number of times, with the output of one RNN + residual group fed as the input to the next group, until the output tensors have the desired spatial dimensions.
We do not reduce over the time dimension when calculating Batch Normalization statistics. This prevents the network from utilizing the Batch Normalization layers to pass information between timesteps.

The spatial discriminator  functions almost identically to BigGAN's discriminator, though an overview of the residual blocks is given in Figure~\ref{fig:blocks} for completeness. A score is calculated for each of the uniformly sampled  frames (we default to  ) and the  output is the sum over per-frame scores. 
The temporal discriminator  has a similar architecture, but pre-processes the real or generated video with a  average-pooling downsampling function . Furthermore, the first two residual blocks of  are 3-D, where every convolution is replaced with a 3-D convolution with a kernel size of .
The rest of the architecture follows BigGAN \citep{brock2018large}.

\subsection{Training Details}
\label{ap:train}

Sampling from DVD-GAN is very efficient, as the core of the generator architecture is a feed-forward convolutional network: two  48-frame videos can be sampled in less than 150ms on a single TPU core. 
The dual discriminator  is updated twice for every update of ~\citep{heusel2017gans} and we use Spectral Normalization~\citep{zhang2018self} for all weight layers (approximated by the first singular value) and orthogonal initialization of weights~\citep{saxe2013exact}. 
Sampling is carried out using the exponential moving average of 's weights, which is accumulated with decay  starting after 20,000 training steps.
The model is optimized using Adam~\citep{kingma2014adam} with batch size  and a learning rate of  and  for  and  respectively. Class conditioning in ~\citep{miyato2018cgans} is projection-based  whereas  relies on class-conditional Batch Normalization~\citep{ioffe2015batch, de2017modulating, dumoulin2017learned}: equivalent to standard Batch Normalization without a learned scale and offset, followed by an elementwise affine transformation where each parameter is a function of the noise vector and class conditioning. 

\subsection{Architecture Extension to Video Prediction}
\label{ss:dvdgan_cond}


\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{graphics/arch_fp_for_icrl_paper.pdf}
\vspace{-1em}
\caption{An architecture diagram describing the changes for the frame conditional model.}
\label{fig:diagram_fp}
\end{figure}

In order to provide results on future video prediction problems we describe a simple modification to DVD-GAN to facilitate the added conditioning. A diagram of the extended model is in Figure~\ref{fig:diagram_fp}.

Given  conditioning frames, our modified DVD-GAN-\textit{FP} passes each frame separately through a deep residual network identical to . The (near) symmetric design of  and 's residual blocks mean that each output from a -style residual block has a corresponding intermediate tensor in  of the same spatial resolution. After each block the resulting features for each conditioning frame are stacked in the channel dimension and passed through a  convolution and ReLU activation. The resulting tensor is used as the initial state for the Convolutional GRU in the corresponding block in . Note that the frame conditioning stack reduces spatial resolution while  increases resolution. Therefore the smallest features of the conditioning frames (which have been through the most layers) are input earliest in  and the larger features (which have been through less processing) are input to  towards the end.  operates on the concatenation of the conditioning frames and the output of , meaning that it does not receive any extra information detailing that the first  frames are special. However to reduce wasted computation we do not sample the first  frames for  on real or generated data. This technically means that  will never see the first few frames from real videos at full resolution, but this was not an issue in our experiments. Finally, our video prediction variant does not condition on any class information, allowing us to directly compare with prior art. This is achieved by settling the class id of all samples to 0.

\section{Further Experiments}

\subsection{UCF-101}
\label{ss:ucf}


UCF-101~\citep{soomro2012ucf101} is a dataset of 13,320 videos of human actions across 101 classes that has previously been used for video synthesis and prediction~\citep{saito2017temporal, saito2018tganv2, tulyakov2018mocogan}. We report Inception Score (IS) calculated with a C3D network~\citep{tran2015learning} for quantitative comparison with prior work.\footnote{We use the Chainer \citep{tokui2015chainer} implementation of Inception Score for C3D available at  \href{https://github.com/pfnet-research/tgan}{https://github.com/pfnet-research/tgan}.}
Our model produces samples with an IS of~, significantly outperforming the state of the art (see Table~\ref{tab:is}). The DVD-GAN architecture on UCF-101 is identical to the model used for Kinetics, and is trained on 16-frame  clips from UCF-101.

However, it is worth mentioning that our improved score is, at least partially, due to memorization of the training data. In Figure~\ref{fig:overfit} we show interpolation samples from our best UCF-101 model. Like interpolations in  Appendix~\ref{ss:interps}, we sample 2 latents (left and rightmost columns) and show samples from the linear interpolation in latent space along each row. Here we show 4 such interpolations (the first frame from each video). Unlike Kinetics-600 interpolations, which smoothly transition from one sample to the other, we see abrupt jumps in the latent space between highly distinct samples, and little intra-video diversity between samples in each group. It can be further seen that some generated samples highly correlate with samples from the training set.

We show this both as a failure of the Inception Score metric, the commonly reported value for class-conditional video synthesis on UCF-101, but also as strong signal that UCF-101 is not a complex or diverse enough dataset to facilitate interesting video generation. Each class is relatively small, and reuse of clips from shared underlying videos means that the intra-class diversity can be restricted to just a handful of videos per class. This suggests the need for larger, more diverse and challenging datasets for generative video modelling, and we believe that Kinetics-600 provides a better benchmark for this task.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{samples/overfit.png}

\caption{The first frames of interpolations between UCF-101 samples. Each row is a separate interpolation. Contrast with samples in Appendix~\ref{ss:interps}.
}
\label{fig:overfit}
\end{figure}

\section{Miscellaneous Experiments}
\label{ss:failed}

Here we detail a number of modifications or miscellaneous results we experimented with which did not produce a conclusive result.

\begin{itemize}
    \item We experimented with several variations of normalization which do not require calculating statistics over a batch of data. Group Normalization \citep{wu2018group} performed best, almost on a par with (but worse than) Batch Normalization. We further tried Layer Normalization \citep{lei2016layer}, Instance Normalization \citep{ulyanov2016instance}, and no normalization, but found that these significantly underperformed Batch Normalization.
    \item We found that removing the final Batch Normalization in , which occurs after the ResNet and before the final convolution, caused a catastrophic failure in learning. Interestingly, just removing the Batch Normalization layers within 's residual blocks still led to good (though slightly worse) generative models. In particular, variants without Batch Normalization in the residual blocks often achieve significantly higher IS (up to 110.05 for  12 frame samples -- twice normal). But these models had substantially worse FID scores (1.22 for the aforementioned model) – and produced qualitatively worse video samples. 
    \item Early variants of DVD-GAN contained Batch Normalization which  normalized over all frames of all batch elements. This gave  an extra channel to convey information across time. It took advantage of this, with the result being a model which required batch statistics in order to produce good samples. We found that the version which normalizes over timesteps independently worked just as well and without the dependence on statistics.
    \item Models based on the residual blocks of BigGAN-deep trained faster (in wall clock time) but slower with regards to metrics, and struggled to reach the accuracy of models based on BigGAN's residual blocks.
\end{itemize}

\section{Generated Samples}

It is difficult to accurately convey complicated generated video through still frames. Where provided, we recommend readers view the generated videos themselves via the provided links. We refer to videos within these batches by row/column number where the video in the 0th row and column is in the top left corner.

\subsection{Synthesis Samples}
\label{ap:pics}

\begin{center}
\href{https://drive.google.com/file/d/155F1lkHA5fMAd7k4W3CQvTsi1eKQDhGb/view?usp=sharing}{\includegraphics[width=1.0\linewidth]{samples/iclr_64x64_12.png}}

\captionof{figure}{The first frames from a random batch of samples from DVD-GAN trained on 12 frames of  Kinetics-600. Full samples at \url{https://drive.google.com/file/d/155F1lkHA5fMAd7k4W3CQvTsi1eKQDhGb/view?usp=sharing}.}\label{fig:64x6412}
\end{center}


\begin{center}
\href{https://drive.google.com/file/d/1FjOQYdUuxPXvS8yeOhXdPQMapUQaklLi/view?usp=sharing}{\includegraphics[width=1.0\linewidth]{samples/iclr_64x64_48.png}}

\captionof{figure}{The first frames from a random batch of samples from DVD-GAN trained on 48 frames of  Kinetics-600. Full samples at \url{https://drive.google.com/file/d/1FjOQYdUuxPXvS8yeOhXdPQMapUQaklLi/view?usp=sharing}.}
\end{center}

\begin{center}
\href{https://drive.google.com/file/d/165Yxuvvu3viOy-39LhhSDGtczbWphj_i/view?usp=sharing}{\includegraphics[width=1.0\linewidth]{samples/iclr_128x128_12.png}}

\captionof{figure}{The first frames from a random batch of samples from DVD-GAN trained on 12 frames of  Kinetics-600. Full samples at \url{https://drive.google.com/file/d/165Yxuvvu3viOy-39LhhSDGtczbWphj_i/view?usp=sharing}}
\end{center}


\begin{center}
\href{https://drive.google.com/file/d/1P8SsWEGP6tEGPPNPH-iVycOlN6vpIgE8/view?usp=sharing}{\includegraphics[width=1.0\linewidth]{samples/iclr_128x128_48.png}}

\captionof{figure}{The first frames from a random batch of samples from DVD-GAN trained on 48 frames of  Kinetics-600. Full samples at \url{https://drive.google.com/file/d/1P8SsWEGP6tEGPPNPH-iVycOlN6vpIgE8/view?usp=sharing}. The sample in row 1, column 5 is a stereotypical example of a degenerate sample occasionally produced by DVD-GAN.}
\end{center}

\begin{center}
\href{https://drive.google.com/file/d/1RGRVKCpVaG8z3p9GBCamRk4apiIR7jUc/view?usp=sharing}{\includegraphics[width=1.0\linewidth]{samples/iclr_256x256_12.png}}

\captionof{figure}{The first frames from a random batch of samples from DVD-GAN trained on 12 frames of  Kinetics-600. Full samples at \url{https://drive.google.com/file/d/1RGRVKCpVaG8z3p9GBCamRk4apiIR7jUc/view?usp=sharing}.}
\end{center}

\subsection{Interpolation Samples}
\label{ss:interps}


We expect  to produce samples of higher quality from latents near the mean of the distribution (zero). This is the idea behind the Truncation Trick~\citep{brock2018large}. Like BigGAN, we find that DVD-GAN is amenable to truncation. We also experiment with interpolations in the latent space and in the class embedding. In both cases, interpolations are evidence that  has learned a relatively smooth mapping from the latent space to real videos: this would be impossible for a network that has only memorized the training data, or which is only capable of generating a few exemplars per class. Note that while all latent vectors along an interpolation are valid (and therefore  should produce a reasonable sample), at no point during training is  asked to generate a sample halfway between two classes. Nevertheless  is able to interpolate between even very distinct classes.

\begin{center}
\includegraphics[width=1.0\linewidth]{samples/intra_interp.png}

\captionof{figure}{An example \textit{intra-class} interpolation. Each column is a separate video (the vertical axis is the time dimension). The left and rightmost columns are randomly sampled latent vectors and are generated under a shared class. Columns in between represent videos generated under the same class across the linear interpolation between the two random samples. Note the smooth transition between videos at all six timesteps displayed here.}
\end{center}

\begin{center}
\includegraphics[width=1.0\linewidth]{samples/inter_interp.png}
\captionof{figure}{An example of \textit{class} interpolation. As before, each column is a sequence of timesteps of a single video. Here, we sample a \textbf{single} latent vector, and the left and rightmost columns represent generating a video of that latent under two different classes. Columns in between represent videos of that same latent generated across an interpolation of the class embedding. Even though at no point has DVD-GAN been trained on data under an interpolated class, it nevertheless produces reasonable samples.}
\end{center}

\end{document}
