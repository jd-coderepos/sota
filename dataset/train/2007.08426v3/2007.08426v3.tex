\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{emnlp2021}

\usepackage{times}
\usepackage{latexsym}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{xspace}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{dblfloatfix}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage[inline]{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{array}
\usepackage{xcolor}
\usepackage{stmaryrd}
\usepackage{scalerel}
\usepackage{multirow}
\usepackage{capt-of,tabularx}
\newcolumntype{C}{>{\centering\arraybackslash}X}

\newcommand{\ourtag}[1]{\ensuremath{\left\langle #1 \right\rangle}}

\newcommand{\shufmodel}[3]{\ensuremath{\text{#1}^{\text{#3}}}}

\makeatletter
\newcommand*{\bigcdot}{}\DeclareRobustCommand*{\bigcdot}{\mathbin{\mathpalette\bigcdot@{}}}
\newcommand*{\bigcdot@scalefactor}{.65}
\newcommand*{\bigcdot@widthfactor}{1.15}
\newcommand*{\bigcdot@}[2]{\sbox0{}\sbox2{}\hbox to \bigcdot@widthfactor\wd2{\hfil
    \raise\ht0\hbox{\scalebox{\bigcdot@scalefactor}{\lower\ht0\hbox{}}}\hfil
  }}
\makeatother

\renewcommand{\UrlFont}{\ttfamily\small}
\newcommand{\sep}{}
\newcommand{\vertmulticell}[2]{\multirow{#1}{*}{\rotatebox[origin=c]{90}{#2}}}
\newcommand{\notfollowgraph}[1]{\textcolor{purple}{\textbf{#1}}}
\usepackage{microtype}





\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Investigating Pretrained Language Models for Graph-to-Text Generation}







\author{Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Sch{\"u}tze and Iryna Gurevych \vspace{1mm} \\
\rule{0pt}{2.5ex}
  Research Training Group AIPHES and UKP Lab, Technical University of Darmstadt\\
  Center for Information and Language Processing (CIS), LMU Munich \\
  \rule{0pt}{2.5ex}
 \texttt{\href{https://www.ukp.tu-darmstadt.de}{www.ukp.tu-darmstadt.de}}
}

\begin{document}
\maketitle
\begin{abstract}
Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recent pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that approaches based on PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. We report new state-of-the-art BLEU scores of 49.72 on AMR-LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative improvement of 31.8\%, 4.5\%, and 42.4\%, respectively, with our models generating significantly more fluent texts than human references.  In an extensive analysis, we identify possible reasons for the PLMsâ€™ success on graph-to-text tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels.\footnote{Our code is available at \href{https://github.com/UKPLab/plms-graph2text}{https://github.com/UKPLab/plms-graph2text}.}
\end{abstract}



\section{Introduction}

Graphs are important data structures in NLP as they represent complex relations within a set of objects. For example, semantic and syntactic structures of sentences can be represented using different graph representations (e.g., AMRs, \citeauthor{banarescu-etal-2013-abstract}, \citeyear{banarescu-etal-2013-abstract}; semantic-role labeling, \citeauthor{surdeanu-etal-2008-conll}, \citeyear{surdeanu-etal-2008-conll}; syntactic and semantic graphs, \citeauthor{belz-etal-2011-first}, \citeyear{belz-etal-2011-first}) and knowledge graphs (KGs) are used to describe factual knowledge in the form of relations between entities \cite{gardent-etal-2017-webnlg, VOUGIOUKLIS20181, koncel-kedziorski-etal-2019-text}.

Graph-to-text generation, a subtask of data-to-text generation \cite{10.5555/3241691.3241693}, aims to create fluent natural language text to describe an input graph (see Figure~\ref{fig:graphs}). This task is important for NLP applications such as dialogue generation \cite{moon-etal-2019-opendialkg} and question answering \cite{duan-etal-2017-question}. Recently, it has been shown that structured meaning representation, such as AMR or KG, can store the internal state of a dialog system, providing core semantic knowledge \cite{bonial-etal-2020-dialogue, bai-etal-2021-semantic} or can be the result of a database query for conversational QA \cite{yu-etal-2019-cosql}. Moreover, dialog states can be represented as KGs to encode compositionality and can be shared across different domains, slot types and dialog participators \cite{cheng-etal-2020-conversational}.

 \begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{fig-amr6.pdf}
    \caption{Examples of (a) AMR and (b) WebNLG graphs, the input for the models and the reference texts.}
    \label{fig:graphs}
\end{figure*}

Transfer learning has become ubiquitous in NLP and pretrained Transformer-based architectures \cite{NIPS2017_7181} have considerably outperformed prior state of the art in various downstream tasks \cite{devlin-etal-2019-bert, NEURIPS2019_dc6a7e65, liu2020roberta, radford2019language}. 




In this paper, we analyze the applicability of two recent text-to-text pretrained language models (PLMs), BART \cite{lewis2019bart} and T5 \cite{2019t5}, for graph-to-text generation. We choose these models because of their \emph{encoder-decoder} architecture, which makes them particularly suitable for conditional text generation. Our study comprises three graph domains (meaning representations, Wikipedia KGs, and scientific KGs). We further introduce \emph{task-adaptive} graph-to-text pretraining approaches for PLMs and demonstrate that such strategies improve the state of the art by a substantial margin.

While recent works have shown the benefit of explicitly encoding the graph structure in graph-to-text generation \cite[][to name a few]{song-etal-acl2018, ribeiro-etal-2019-enhancing, ribeiro-etal-2020-modeling,schmitt2020modeling, zhao-etal-2020-bridging}, our approaches based on PLMs consistently outperform these models, even though PLMs -- as sequence models -- do not exhibit any \emph{graph-specific structural bias}.\footnote{The model architecture does not explicitly encode the graph structure, i.e., which entities are connected to each other, but has to retrieve it from a sequence that tries to encode this information.} Simply representing the graph as a linear traversal (see Figure~\ref{fig:graphs}) leads to remarkable generation performance in the presence of a strong language model.
In our analysis
we investigate to what extent fine-tuned PLMs make use of the graph structure represented in the graph linearization.
We notably observe that PLMs achieve high performance on two popular KG-to-text benchmarks even when the KG is reduced to a mere bag of node and edge labels.


Our contributions are the following:
\begin{itemize}[noitemsep,nolistsep]
    \item We investigate and compare two PLMs, BART and T5, for graph-to-text generation, exploring \emph{language model adaptation} (\textsc{lma}) and \emph{supervised task adaptation} (\textsc{sta}) pretraining, employing additional task-specific data.\item Our approaches consistently outperform the state of the art by significant margins, ranging from 2.6 to 12.0 BLEU points, on three established graph-to-text benchmarks from different domains, exceeding specialized graph architectures (e.g., Graph Neural Networks, GNNs, \citeauthor{Kipf:2016tc}, \citeyear{Kipf:2016tc}).
    \item In a crowdsourcing experiment, we demonstrate that our methods generate texts with significantly better fluency than existing works and the human references.
    \item We discover that PLMs perform well even when trained on a shuffled linearized graph representation without any information about connectivity (bag of node and edge labels), which is surprising since prior studies showed that explicitly encoding the graph structure improves models trained from scratch (e.g., \citeauthor{zhao-etal-2020-bridging}, \citeyear{zhao-etal-2020-bridging}); and investigate the possible reasons for such a good performance.




\end{itemize}



\section{Related Work}



\paragraph{Graph-to-text Learning.} Various neural models have been proposed to generate sentences from graphs from different domains. \citet{konsas_17} propose the first neural approach for AMR-to-text generation that uses a linearized input graph. Prior approaches for KG-to-text generation train text-to-text neural models using sequences of KG triples as input \cite{trisedya-etal-2018-gtr, moryossef-etal-2019-step, castro-ferreira-etal-2019-neural, ribeiro2021smelting}. 

Recent approaches \cite{marcheggiani-icnl18, song-etal-acl2018, beck-etal-2018-acl2018, damonte-cohen-2019-structural, ribeiro-etal-2019-enhancing,zhao-etal-2020-bridging, schmitt-etal-2021-modeling, ribeiro2021structural} propose architectures based on GNNs to directly encode the graph structure, whereas other efforts \cite{ribeiro-etal-2020-modeling, schmitt2020modeling, yao-etal-2020-heterogeneous, doi:10.116200297} inject the graph structure information into Transformer-based architectures. The success of those approaches suggests that imposing a strong relational inductive bias into the graph-to-text model can assist the generation.

\paragraph{Pretrained Language Models.}  Pretrained Transformer-based models, such as BERT \cite{devlin-etal-2019-bert}, XLNet \cite{NIPS2019_8812}, or RoBERTa \cite{liu2020roberta}, have established a qualitatively new level of baseline performance for many widely used natural language understanding (NLU) benchmarks. Generative pretrained Transformer-based methods, such as GPT-2 \cite{radford2019language}, BART \cite{lewis2019bart}, and T5 \cite{2019t5}, are employed in many natural language generation (NLG) tasks.  

\citet{mager2020gpttoo} were the first to employ GPT-2, a decoder-only PLM, for AMR-to-text generation and use cycle consistency to improve the adequacy. In contrast, we are the first to investigate BART and T5 models, which have both a Transformer-based encoder and decoder, in AMR-to-text generation. Recently, \citet{harkous2020text} and \citet{kale2020texttotext} demonstrate state-of-the-art results in different data-to-text datasets, employing GPT-2 and T5 models respectively. \citet{radev2020dart} propose DART, a new data-to-text dataset, and train a BART model gradually augmenting the WebNLG training data with DART data. 

\citet{hoyle2020promoting} explore scaffolding objectives in PLMs and show gains in low-resource graph-to-text settings.
Different from the above works, we focus on a general transfer learning strategies for graph-to-text generation, investigating task-adaptive pretraining approaches, employing additional collected task-specific data for different PLMs (BART and T5) and benchmarks. In addition, we provide a detailed analysis aimed at explaining the good performance of PLMs on KG-to-text tasks.


Recently, \citet{gururangan-etal-2020-dont} explored task-adaptive pretraining strategies for text classification.
While our \textsc{lma} (see \S\ref{sec:finetuning}) is related to their \textsc{dapt} as both use a self-supervised objective on a domain-specific corpus,
they notably differ in that \textsc{dapt} operates on the model input while \textsc{lma} models the output. We are the first to show the benefits of additional task-specific pretraining in PLMs for graph-to-text tasks.




\section{PLMs for Graph-to-Text Generation}
\label{sec:finetuning}
\subsection{Models in this Study}
We investigate BART \cite{lewis2019bart} and T5 \cite{2019t5}, two PLMs based on the Transformer \emph{encoder-decoder} architecture \cite{NIPS2017_7181}, for graph-to-text generation. They mainly differ in how they are pretrained and the input corpora used for pretraining. We experiment with different T5 (\emph{small} - 60M  parameters, \emph{base} - 220M, and \emph{large} - 770M) and BART (\emph{base} - 140M and \emph{large} - 400M) capacity models.






We fine-tune both PLMs for a few epochs on the supervised downstream graph-to-text datasets. For T5, in the supervised setup, we add a prefix ``translate from Graph to Text:'' before the graph input. We add this prefix to imitate the T5 setup, when translating between different languages. 







\subsection{Task-specific Adaptation} 
\label{sec:domainpretraining}



Inspired by previous work \cite{konsas_17,gururangan-etal-2020-dont}, we investigate whether leveraging additional task-specific data can improve the PLMs' performance on graph-to-text generation. Task-specific data refers to a pretraining corpus that is more task-relevant and usually smaller than the text corpora used for task-independent pretraining. In order to leverage the task-specific data, we add an intermediate adaptive pretraining step between the original pretraining and fine-tuning phases for graph-to-text generation. 



More precisely, we first continue pretraining BART and T5 using language model adaptation (\textsc{lma}) or supervised task adaptation (\textsc{sta}) training. In the supervised approach, we use pairs of graphs and corresponding texts collected from the same or similar domain as the target task. In the \textsc{lma} approach, we follow BART and T5 pretraining strategies for language modeling, using the reference texts that describe the graphs. Note that we do not use the graphs in the \textsc{lma} pretraining, but only the target text of our task-specific data collections. The goal is to adapt the decoder to the domain of the final task  \cite{gururangan-etal-2020-dont}. In particular, we randomly mask text spans, replacing 15\% of the tokens.\footnote{Please, refer to \citet{lewis2019bart} and \citet{2019t5} for details about the self-supervised pretraining strategies.}
Before evaluation, we finally fine-tune the models using the original training set as usual.


\section{Datasets}
\label{sec:data}

We evaluate the text-to-text PLMs on three graph-to-text benchmarks: AMR (LDC2017T10), WebNLG \cite{gardent-etal-2017-webnlg}, and AGENDA \cite{koncel-kedziorski-etal-2019-text}. We chose those datasets because they comprise different domains and are widely used in prior work. Table~\ref{tab:datastatistics} in Appendix shows statistics for each dataset.



\paragraph{AMR.} Abstract meaning representation (AMR) is a semantic formalism that represents the meaning of a sentence as a rooted directed graph expressing ``who is doing what to whom'' \cite{banarescu-etal-2013-abstract}. In an AMR graph, nodes represent concepts and edges represent semantic relations. An instance in LDC2017T10 consists of a sentence annotated with its corresponding AMR graph. Following ~\citet{mager2020gpttoo}, we linearize the AMR graphs using the \textsc{penman} notation (see Figure~\ref{fig:graphs}a).\footnote{Details of the preprocessing procedure of AMRs are provided in Appendix~\ref{sec:amrinput}.}






\paragraph{WebNLG.} Each instance of WebNLG contains a KG from DBPedia \cite{10.5555/1785162.1785216} and a target text with one or multiple sentences that describe the graph. The test set is divided into two partitions: \textit{seen}, which contains only DBPedia categories present in the training set, and \textit{unseen}, which covers categories never seen during training. Their union is called \textit{all}.
Following previous work \cite{harkous2020text}, we prepend \ourtag{H}, \ourtag{R}, and \ourtag{T} tokens before the head entity, the relation and tail entity of a triple (see Figure~\ref{fig:graphs}b).

\paragraph{AGENDA.} In this dataset, KGs are paired with scientific abstracts extracted from proceedings of AI conferences. Each sample contains the paper title, a KG, and the corresponding abstract. The KG contains entities corresponding to scientific terms and the edges represent relations between these entities. This dataset has loose alignments between the graph and the corresponding text as the graphs were automatically generated. The input for the models is a text containing the title, a sequence of all KG entities, and the triples. The target text is the paper abstract. We add special tokens into the triples in the same way as for WebNLG. 





\subsection{Additional Task-specific Data}
In order to evaluate the proposed task-adaptive pretraining strategies for graph-to-text generation, we collect task-specific data for two graph domains: meaning representations (like AMR) and scientific data (like AGENDA).
We did not attempt collecting additional data like WebNLG because the texts in this benchmark do not stem from a corpus but were specifically written by annotators.



\paragraph{AMR Silver Data.} In order to generate additional data for AMR, we sample two sentence collections of size 200K and 2M from the Gigaword\footnote{\href{https://catalog.ldc.upenn.edu/LDC2003T05}{https://catalog.ldc.upenn.edu/LDC2003T05}} corpus and use a state-of-the-art AMR parser \cite{cai-lam-2020-amr} to parse them into AMR graphs.\footnote{We filter out sentences that do not yield well-formed AMR graphs.} For supervised pretraining, we condition a model on the AMR silver graphs to generate the corresponding sentences before fine-tuning it on gold AMR graphs. For self-supervised pretraining, we only use the sentences.\footnote{Gigaword and AMR datasets share similar data sources.}


\begin{table}[t]
\centering
{\renewcommand{\arraystretch}{0.6}
\begin{tabular}{@{\hspace*{1mm}}l@{\hspace*{1mm}}ccc@{\hspace*{1mm}}}  
\toprule
\textbf{Model} & \textbf{BLEU} & \textbf{M} & \textbf{BT}  \\
\midrule
\citet{ribeiro-etal-2019-enhancing}  & 27.87 & 33.21 & -\\
\citet{zhu-etal-2019-modeling} & 31.82 & 36.38 & -\\
\citet{zhao-etal-2020-line} & 32.46 & 36.78 & -\\
\citet{doi:10.116200297} & 33.90 & 37.10 &  -\\
\citet{yao-etal-2020-heterogeneous}  & 34.10 & 38.10 & -\\
\midrule
\small{\textit{based on PLMs}} & & & \.2em]
BART\textsubscript{large} + \textsc{lma} & 43.94 & 42.36 & 58.54 \\
T5\textsubscript{large} + \textsc{lma} & 46.06 & 44.05 & 62.59 \.2em]
\citet{harkous2020text} & 52.90 & - & - & 42.40 & - & - & - & - & - \\
\citet{kale2020texttotext} & 57.10 & 63.90 & 52.80 & 44.00 & 46.00 & 41.00 & - & - & -\\
\citet{radev2020dart} & 45.89 & 52.86 & 37.85 & 40.00 & 42.00 & 37.00 & - & - & -\\
\midrule
BART\textsubscript{base} & 53.11 & 62.74 & 41.53 & 40.18 & 44.45 & 35.36 & 70.02 & 76.68 & 62.76 \\
BART\textsubscript{large} & 54.72 & 63.45 & 43.97 & 42.23 & 45.49 & 38.61 & 72.29 & 77.57 & 66.53 \\
T5\textsubscript{small} & 56.34 & \textbf{65.05} & 45.37 & 42.78 & 45.94 & 39.29 & 73.31 & \textbf{78.46} & 67.69 \\
T5\textsubscript{base} & 59.17 & 64.64 & 52.55 & 43.19 & \textbf{46.02} & 41.49 & 74.82 & 78.40 & 70.92 \\
T5\textsubscript{large} & \textbf{59.70} & 64.71 & \textbf{53.67} & \textbf{44.18} & 45.85 & \textbf{42.26} & \textbf{75.40} & 78.29 & \textbf{72.25} \\


\bottomrule
\end{tabular}}
\caption{Results on WebNLG. A, S and U stand for \textit{all}, \textit{seen}, and \textit{unseen} partitions of the test set, respectively.}
\label{tab:results-webnlg}   
\end{table*}

\paragraph{Semantic Scholar AI Data.} We collect titles and abstracts of around 190K scientific papers from the Semantic Scholar \cite{ammar-etal-2018-construction} taken from the proceedings of 36 top Computer Science/AI conferences. We construct KGs from the paper abstracts employing DyGIE++ \cite{wadden-etal-2019-entity}, an information extraction system for scientific texts. Note that the AGENDA dataset was constructed using the older SciIE system \cite{luan-etal-2018-multi}, which also extracts KGs from AI scientific papers. A second difference is that in our new dataset, the domain is broader as we collected data from 36 conferences compared to 12 from AGENDA. Furthermore, to prevent data leakage, all AGENDA samples used for performance evaluation are removed from our dataset. We will call the new dataset \textsc{KGAIA} (KGs from AI Abstracts).\footnote{We will release the collected additional task-specific data.} Table~\ref{tab:augstatistics} in Appendix shows relevant dataset statistics.



\section{Experiments}



We modify the BART and T5 implementations released by Hugging Face \citep{wolf2019huggingfaces} in order to adapt them to graph-to-text generation. For the KG datasets, we add the \ourtag{H}, \ourtag{R}, and \ourtag{T} tokens to the models' vocabulary. We add all edge labels seen in the training set to the vocabulary of the models for AMR. Following \citet{wolf2019huggingfaces}, we use the Adam optimizer \cite{kingma:adam} with an initial learning rate of . We employ a linearly decreasing learning rate schedule without warm-up. The batch and beam search sizes are chosen from \{2,4,8\} and \{1,3,5\}, respectively, based on the respective development set. Dev BLEU is used for model selection. 




Following previous works, we evaluate the results with BLEU \cite{Papineni:2002:BMA:1073083.1073135}, METEOR \cite{Denkowski14meteoruniversal}, and chrF++ \cite{popovic-2015-chrf} metrics. We also use MoverScore~\cite{zhao-etal-2019-moverscore}, BERTScore~\cite{bert-score}, and BLEURT~\cite{sellam-etal-2020-bleurt} metrics, as they employ contextual and semantic knowledge and thus depend less on the surface symbols. Additionally, we perform a human evaluation (cf.\ \S\ref{sec:human_eval}) quantifying the fluency, semantic adequacy and meaning similarity of the generated texts.






\subsection{Results on AMR-to-Text}
\label{sec:amr}
Table~\ref{tab:results-amr} shows our results for the setting without additional pretraining, with additional self-supervised task-adaptive pretraining solely using the collected Gigaword sentences (\textsc{lma}), and with additional supervised task adaptation (\textsc{sta}), before fine-tuning. We also report several recent results on the AMR test set. \citet{mager2020gpttoo} and \citet{harkous2020text} employ GPT-2 in their approaches. Note that GPT-2 only consists of a Transformer-based decoder.

Only considering approaches without task adaptation, BART\textsubscript{large} already achieves a considerable improvement of 5.77 BLEU and 3.98 METEOR scores over the previous state of the art. With a BLEU score of 45.80, T5\textsubscript{large} performs best. The other metrics follow similar trends. See Table~\ref{tab:results-amr-appendix} in Appendix for evaluation with more metrics. The strong performance of both BART and T5 in the AMR dataset suggests that PLMs can infer the AMR structure by a simple linear sequence of the graph, in contrast to GNN-based models that explicitly consider the graph structure using \emph{message-passing} between adjacent nodes \cite{beck-etal-2018-acl2018}.



\paragraph{Task-specific Adaptation.}
\textsc{lma}
already brings some gains with T5 benefitting more than BART in most metrics.
It still helps less than \textsc{sta} even though we only have automatically generated annotations.
This suggests that the performance increases with \textsc{sta} do not only come from additional exposure to task-specific target texts and that the models learn how to handle graphs and the graph-text correspondence even with automatically generated AMRs. After \textsc{sta}, T5 achieves 49.72 BLEU points, the new state of the art for AMR-to-text generation. Interestingly, gains from \textsc{sta} with 2M over 200K are larger in BART than in T5, suggesting that large amounts of silver data may not be required for a good performance with T5. 

In general, models pretrained on the \textsc{sta} setup converge faster than without task-specific adaptation. For example, T5\textsubscript{large} without additional pretraining converges after 5 epochs of fine-tuning whereas T5\textsubscript{large} with \textsc{sta} already converges after 2 epochs.



\subsection{Results on WebNLG}



Table~\ref{tab:results-webnlg} shows the results for the WebNLG test set. Neural pipeline models \cite{moryossef-etal-2019-step, castro-ferreira-etal-2019-neural} achieve strong performance in the \emph{unseen} dataset. On the other hand, fully end-to-end models \cite{ribeiro-etal-2020-modeling,schmitt2020modeling} have strong performance on the \emph{seen} dataset and usually perform poorly in \textit{unseen} data. Models that \emph{explicitly encode the graph structure} \cite{ribeiro-etal-2020-modeling, zhao-etal-2020-bridging} achieve the best performance among approaches that do not employ PLMs. Note that T5 is also used in \citet{kale2020texttotext}. Differences in our T5 setup include a modified model vocabulary, the use of beam search, the learning rate schedule and the prefix before the input graph. Our T5 approach achieves 59.70, 65.05 and 54.69 BLEU points on \emph{all}, \emph{seen} and \emph{unseen} sets, the new state of the art. 

We conjecture that the performance gap between \emph{seen} and \emph{unseen} sets stems from the advantage obtained by a model seeing examples of relation-text pairs during fine-tuning. For example, the relation \emph{party} (political party) was never seen during training and the model is required to generate a text that verbalizes the tuple: \emph{Abdul Taib Mahmud, party, Parti Bumiputera Sarawak}. Interestingly, BART performs much worse than T5 on this benchmark, especially in the \emph{unseen} partition with 9.7 BLEU points lower compared to T5.

For lack of a suitable data source (cf.\ \S\ref{sec:data}), we did not explore our \textsc{lma} or \textsc{sta} approaches for WebNLG.
However, we additionally discuss cross-domain \textsc{sta} in Appendix~\ref{sec:crossdomain}.









\subsection{Results on AGENDA}


\begin{table}[t]
\centering
{\renewcommand{\arraystretch}{0.9}
\begin{tabular}{@{\hspace*{1mm}}lccc@{\hspace*{1mm}}}  
\toprule
\textbf{Model} & \textbf{BLEU} & \textbf{M} & \textbf{BT}  \\
\midrule
Koncel et al. \citeyear{koncel-kedziorski-etal-2019-text}  & 14.30 & 18.80 & - \\
\citet{An2019RepulsiveBS} & 15.10 & 19.50 & -\\
\citet{schmitt2020modeling} & 17.33 & 21.43  & -\\
\citet{ribeiro-etal-2020-modeling} & 18.01 & 22.23 & -\\
\midrule
BART\textsubscript{base} & 22.01 & 23.54 & -13.02 \\
BART\textsubscript{large} & \textbf{23.65} & \textbf{25.19} & \textbf{-10.93} \\
T5\textsubscript{small} & 20.22 & 21.62 & -24.10 \\
T5\textsubscript{base} & 20.73 & 21.88 & -21.03 \\
T5\textsubscript{large} & 22.15 & 23.73 & -13.96 \\
\midrule
\multicolumn{2}{l}{\small{\textit{with task-adaptive pretraining}}} & & \.7em]
BART\textsubscript{large} + \textsc{sta} & \textbf{\textit{25.66}} & \textbf{\textit{25.74}} & \textbf{\textit{-08.97}} \\
T5\textsubscript{large} + \textsc{sta} & 23.69 & 24.92 & -08.94 \\
\bottomrule
\end{tabular}}
\caption{Results on AGENDA test set. \textbf{Bold} (\textbf{\textit{Italic}}) indicates best scores without (with) task-adaptive pretraining.}
\label{tab:results-agenda}
\vspace{-0.4cm}
\end{table}

Table~\ref{tab:results-agenda} lists the results for the AGENDA test set. The models also show strong performance on this dataset. We believe that their capacity to generate fluent text helps when generating paper abstracts, even though they were not pretrained in the scientific domain. BART\textsubscript{large} shows an impressive performance with a BLEU score of 23.65, which is 5.6 points higher than the previous state of the art.

\paragraph{Task-specific Adaptation.} On AGENDA, BART benefits more from our task-adaptive pretraining, achieving the new state of the art of 25.66 BLEU points, a further gain of 2 BLEU points compared to its performance without task adaptation. The improvements from task-adaptive pretraining are not as large as for AMR. We hypothesize that this is due to the fact that the graphs do not completely cover the target text \cite{koncel-kedziorski-etal-2019-text}, making this dataset more challenging. See Table~\ref{tab:results-agenda-appendix} in Appendix for more automatic metrics. 



\begin{table}[t]
\centering
{\renewcommand{\arraystretch}{0.9}
\begin{tabular}{lll} 
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textbf{AMR}}  \\
\midrule
& \, \,\textbf{F} & \,\textbf{MS} \\
\midrule
\citet{mager2020gpttoo} &&  \\
\citet{harkous2020text} &&  \\
T5\textsubscript{large} &&  \\
BART\textsubscript{large} &&  \\
Reference && - \\
\midrule
\textbf{Model} & \multicolumn{2}{c}{\textbf{WebNLG}}  \\
 \midrule
& \, \,\textbf{F} & \, \,\textbf{SA} \\
\midrule
 \citet{castro-ferreira-etal-2019-neural} &&  \\
 \citet{harkous2020text} &&  \\
 T5\textsubscript{large} &&  \\
BART\textsubscript{large} &&  \\
Reference &&  \\
\bottomrule
\end{tabular}}
\caption{Fluency (F), Meaning Similarity (MS) and Semantic Adequacy (SA) obtained in the human evaluation. Differences between models which have a letter in common are not statistically significant and were determined by pairwise Mann-Whitney tests with .}
\label{tab:humanevevaluation}
\vspace{-0.3cm}
\end{table}

 \begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/corrupt.pdf}
    \caption{Example graph with 5 triples, from WebNLG dev linearized with the neutral separator tag, denoted \sep{}, (top left), its shuffled version (top right), texts generated with two fine-tuned versions of T5\textsubscript{small} and a gold reference (bottom). Note that T5 can produce a reasonable text even when the input triples are shuffled randomly.}
    \label{fig:graphs-shuffle}
\end{figure*}

 \begin{figure}[t]
    \centering
    \includegraphics[width=.48\textwidth]{graph_samples.pdf}
    \caption{Performance of BART\textsubscript{base} and T5\textsubscript{base} in the dev set when experimenting with different amounts of training data.}
    \label{fig:graphs-trainingexamples}
    \vspace{-3mm}
\end{figure}

\subsection{Human Evaluation}
\label{sec:human_eval}
To further assess the quality of the generated text, we conduct a human evaluation on AMR and WebNLG via crowd sourcing on Amazon Mechanical Turk.\footnote{We exclude AGENDA because its texts are scientific in nature and annotators are not necessarily AI experts.} Following previous works \cite{gardent-etal-2017-webnlg, castro-ferreira-etal-2019-neural}, we assess three quality criteria: (i) \emph{Fluency} (i.e., does the text flow in a natural, easy-to-read manner?), for AMR and WebNLG; (ii) \emph{Meaning Similarity} (i.e., how close in meaning is
the generated text to the reference sentence?) for AMR; (ii) \emph{Semantic Adequacy} (i.e., does the text clearly express the data?) for WebNLG. We randomly select 100 generated texts of each model, which the annotators then rate on a 1-7 Likert scale. For each text, we collect scores from 3 annotators and average them.\footnote{Inter-annotator agreement for the three criteria ranged from 0.40 to 0.79, with an average Krippendorff's  of 0.56.} 


Table~\ref{tab:humanevevaluation} shows the results.  Our approaches improve the fluency, meaning similarity, and semantic adequacy on both datasets compared to other state-of-the-art approaches with statistically significant margins (). Interestingly, the highest fluency improvement () is on AMR, where our approach also has the largest BLEU improvement () over \citet{harkous2020text}. Finally, our models score higher than the references in fluency with statistically significant margins, highlighting their strong language generation abilities.\footnote{Examples of fluent generations can be found in the Tables~\ref{tab:human_examples} and \ref{tab:human_examples_agenda} in Appendix.}



\begin{table}[t]
\centering
{\renewcommand{\arraystretch}{0.9}


\begin{tabular}{lrrr}  
\toprule
\textbf{Model} & \textbf{AMR} & \textbf{WebNLG} & \textbf{AGENDA} \\
\midrule


\shufmodel{T5}{small}{order} & 36.83 & 63.41 & 19.86 \\
\shufmodel{T5}{small}{shuf} & 15.56 &  61.54 & 19.08 \\
\bottomrule
\end{tabular}}
\caption{Impact (measured with BLEU) of using a bag of entities and relations (\emph{shuf}) as input for T5\textsubscript{small}.}
\label{tab:shuffle}
\vspace{-3mm}
\end{table}

\subsection{Limiting the Training Data}

In Figure~\ref{fig:graphs-trainingexamples}, we investigate the PLMs' performance, measured with BLEU score, while varying (from 1\% to 100\%) the amount of training data used for fine-tuning. We find that, when fine-tuned with only 40\% of the data, both BART and T5 already greatly improve the performance compared to using the entire training data in all three benchmarks. For example, BART fine-tuned on 40\% of AMR training data achieves 91\% of the BLEU score when fine-tuned on full data.

Note that in a low-resource scenario in AMR and WebNLG, T5 considerably outperforms BART. In particular, with only 1\% of training examples, the difference between T5 and BART is 7.51 and 5.64 BLEU points for AMR and WebNLG, respectively. This suggests that T5 is more data efficient when adapting to the new task, likewise our findings in \textsc{AMR-sta} (cf.\ \S\ref{sec:amr}). 









         


















\begin{table*}[t]
    \centering
\footnotesize
    \begin{tabular}{@{\hspace{.1em}}c@{\hspace{.1em}}c@{\hspace{.1em}}p{4.9cm}@{\hspace{.8em}}p{4.8cm}@{\hspace{.8em}}p{4.75cm}@{\hspace{.2em}}}
    \toprule
         &\textbf{T/F}&\multicolumn{1}{c}{\textbf{Input Fact}} & \multicolumn{1}{c}{\textbf{\shufmodel{T5}{small}{order}}} & \multicolumn{1}{c}{\textbf{\shufmodel{T5}{small}{shuf}}} \\
         
         
         \midrule
         (1) &S& \sep{} German language \sep{} Antwerp \sep{} Antwerp \sep{} Antwerp International Airport \sep{} Belgium \sep{} Belgium \sep{} Charles Michel \sep{} city Served \sep{} leader Name \sep{} Belgium \sep{} language \sep{} country & Antwerp International Airport serves the city of Antwerp. German is the language spoken in Belgium where Charles Michel is the leader. & Antwerp International Airport serves the city of Antwerp in Belgium where the German language is spoken and Charles Michel is the leader.\\
        \midrule

         
          (2) &T& \sep{} California \sep{} is Part Of \sep{} US \sep{} California \sep{} capital \sep{} Sacramento & California is part of the United States and its capital is Sacramento. & California is part of the United States and its capital is Sacramento. \\
(3) &F& \sep{} US \sep{} is Part Of \sep{} California \sep{} California \sep{} capital \sep{} Sacramento & California's capital is Sacramento and the United States is part of California. & California is part of the United States and its capital is Sacramento.\\
\midrule
            (4) &T& \sep{} Amarillo, Texas \sep{} is Part Of \sep{} United States & Amarillo, Texas is part of the United States. & Amarillo, Texas is part of the United States.\\
(5) &F& \sep{} United States \sep{} is Part Of \sep{} Amarillo, Texas & Amarillo, Texas is part of the United States. & Amarillo, Texas is part of the United States.   \\
         

\bottomrule
    \end{tabular}
    \caption{Example generations from shuffled (S), true (T), and corrupted (F) triple facts by T5\textsubscript{small}, fine-tuned on correctly ordered triples (\emph{order}) and randomly shuffled input (\emph{shuf}).} \label{tab:qualitative}
\end{table*} 
\section{Influence of the Graph Structure}


We conduct further experiments to examine how much the PLMs consider the graph structure.
To this end, we remove parentheses in AMRs and replace \ourtag{H}, \ourtag{R}, and \ourtag{T} tokens with neutral separator tokens, denoted \sep{}, for KGs, such that the graph structure is only defined by the order of node and edge labels.
If we shuffle such a sequence,
the graph structure is thus completely obscured and the input effectively becomes a bag of node and edge labels.
See Figure~\ref{fig:graphs-shuffle} for an example of both a correctly ordered and a shuffled triple sequence.


\subsection{Quantitative Analysis}

Table~\ref{tab:shuffle} shows the effect on T5's performance when its input contains correctly ordered triples (\shufmodel{T5}{small}{order}) vs.\ shuffled ones (\shufmodel{T5}{small}{shuf}) for both training and evaluation.
We first observe that \shufmodel{T5}{small}{order} only has marginally lower performance (around 2-4\%{}) with the neutral separators than with the \ourtag{H}/\ourtag{R}/\ourtag{T} tags or parentheses.\footnote{See a more fine-grained comparison in Appendix~\ref{section:inputgraphsize}.} We see that as evidence that the graph structure is similarly well captured by \shufmodel{T5}{small}{order}.
Without the graph structure (\shufmodel{T5}{small}{shuf}), AMR-to-text performance drops significantly. Possible explanations of this drop are: (i) the relative ordering of the AMR graph is known to correlate with the target sentence order \cite{konsas_17}; (ii) in contrast to WebNLG that contains common knowledge, the AMR dataset contains very specific sentences with higher surprisal;\footnote{Perplexities estimated on the dev sets of AMR and WebNLG datasets, with GPT-2 fine-tuned on the corresponding training set, are 20.9 and 7.8, respectively.} (iii) AMRs are much more complex graph structures than the KGs from WebNLG and AGENDA.\footnote{In Appendix~\ref{sec:graphstats}, we present the graph properties of the datasets and discuss the differences.}  

On the other hand, KG-to-text performance is not much lower,
indicating that most of the PLMs' success in this task stems from their language modeling rather than their graph encoding capabilities. 
We hypothesize that a PLM can match the entities in a shuffled input with sentences mentioning these entities from the pretraining or fine-tuning phase.
It has recently been argued that large PLMs can recall certain common knowledge facts from pretraining \citep{petroni-etal-2019-language,bosselut-etal-2019-comet}.







\subsection{Qualitative Analysis}
\label{sec:qualitative}

The example in Figure~\ref{fig:graphs-shuffle} confirms our impression.
\shufmodel{T5}{small}{shuf} produces a text with the same content as \shufmodel{T5}{small}{order} but does not need the correct triple structure to do so.
Example (1) in Table~\ref{tab:qualitative} shows the output of both models with shuffled input.
Interestingly, even \shufmodel{T5}{small}{order} produces a reasonable and truthful text.
This suggests that previously seen facts serve as a strong guide during text generation, even for models that were fine-tuned with a clearly marked graph structure,
suggesting that \shufmodel{T5}{small}{order} also relies more on language modeling than the graph structure.
It does have more difficulties covering the whole input graph though.
The fact that \emph{Antwerp} is located in \emph{Belgium} is missing from its output.

To further test our hypothesis that
PLMs make use of previously seen facts
during KG-to-text generation, we generate example true facts, corrupt them in a controlled setting, and feed them to both \shufmodel{T5}{small}{order} and \shufmodel{T5}{small}{shuf} to observe their output (examples (2)--(5) in Table~\ref{tab:qualitative}).
The model trained on correctly ordered input has learned a bit more to rely on the input graph structure. 
The false fact in example (3) with two triples is reliably transferred to the text by \shufmodel{T5}{small}{order} but not by \shufmodel{T5}{small}{shuf}, which silently corrects it.
Also note that, in example (5), both models refuse to generate an incorrect fact. More examples can be found in Table~\ref{tab:qualitative_appendix} in the Appendix.

Our qualitative analysis illustrates that state-of-the-art PLMs, despite their fluency capacities (cf.\ \S\ref{sec:human_eval}), bear the risk of parroting back training sentences while ignoring the input structure. This issue can limit the practical usage of those models as, in many cases, it is important for a generation model to stay true to its input \cite{wiseman-etal-2017-challenges, falke-etal-2019-ranking}.












\section{Conclusion}

We investigated two pretrained language models (PLMs) for graph-to-text generation and show that the pretraining strategies, language model adaptation (\textsc{lma}) and supervised task adaptation (\textsc{sta}), can lead to notable improvements. Our approaches outperform the state of the art by a substantial margin on three graph-to-text benchmarks. Moreover, in a human evaluation our generated texts are perceived significantly more fluent than human references. Examining the influence of the graph structure on the text generation process, we find that PLMs may not always follow the graph structure and instead use memorized facts to guide the generation. A promising direction for future work is to explore ways of injecting a stronger graph-structural bias into PLMs, thus possibly leveraging their strong language modeling capabilities and keeping the output faithful to the input graph.


\section*{Acknowledgments}
We thank our anonymous reviewers for their thoughtful feedback. Leonardo F. R. Ribeiro is supported by the German Research Foundation (DFG) as part of the Research Training Group ``Adaptive Preparation of Information form Heterogeneous Sources'' (AIPHES, GRK 1994/1) and as part of the DFG funded project UKP-SQuARE with the number GU 798/29-1. Martin Schmitt is supported by the BMBF as part of the project MLWin (01IS18050) and by the German Academic Scholarship Foundation (Studienstiftung des deutschen Volkes).


\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}



\clearpage

\appendix

\section*{Appendices}

In this supplementary material, we provide: (i) additional information about the data used in the experiments, and (ii) results that we could not fit into the main body of the paper.

\section{AMR Input Representation}
\label{sec:amrinput}
We test three variants for the representation of the input AMR graph. Following previous work~\cite{konsas_17, mager2020gpttoo}, we evaluate (i) only node representation, where the edge information is removed from the linearization; (ii) depth-first search (DFS) through the graph and the (iii) \textsc{penman} representation. An example for each representation is illustrated below: 
\begin{table}[h]
\begin{tabular}{c p{5.5cm}}
\vspace{3mm}
\small only nodes   & \small \texttt{value interrogative commodity true}\\
\small DFS     & \small \texttt{value :mode interrogative :ARG1 commodity :ARG1-of true} \\
\small \textsc{penman}     & \small \texttt{( value :mode interrogative :ARG1 ( commodity ) :ARG1-of ( true ) )}
\end{tabular}
\end{table}

In this experiment we employ T5\textsubscript{small}. Table~\ref{tab:amrinputs} shows the results on the AMR development set. The \textsc{penman} representation leads to best results. Therefore, this representation is used in the rest of the experiments.

\begin{table}[h]
\centering
{\renewcommand{\arraystretch}{0.8}

\begin{tabular}{lc}  
\toprule
 \textbf{Input} & \textbf{BLEU}  \\
 \midrule
 only nodes & 28.22 \\
 DFS & 34.94 \\
 \textsc{penman} & 38.27 \\
\bottomrule
\end{tabular}}
\caption{Results on the AMR dev set using T5\textsubscript{small} for different AMR linearizations.}
\label{tab:amrinputs}
\end{table}
\vspace{-4mm}

\section{Cross-domain Adaptation}
\label{sec:crossdomain}

For a given task, it is not always possible to collect closely related data -- as we saw, e.g., for WebNLG.
We therefore report \textsc{sta} in a cross-domain setting for the different KG-to-text benchmarks.
Table~\ref{tab:crossdomain_adddata} shows the results using BART\textsubscript{base} and T5\textsubscript{base}. While the texts in KGAIA and AGENDA share the domain of scientific abstracts, texts in WebNLG are more general. Also note that WebNLG graphs do not share any relations with the other KGs. For BART\textsubscript{base}, \textsc{sta} increases the performance in the cross-domain setting in most of the cases. For T5\textsubscript{base}, \textsc{sta} in KGAIA improves the performance on WebNLG.

In general, we find that exploring additional adaptive pretraining for graph-to-text generation can improve the performance even if the data do not come from the same domain.


\begin{table}[h]
\centering
{\renewcommand{\arraystretch}{0.8}
\begin{tabular}{lcc}  
\toprule
\textbf{STA on} & \multicolumn{2}{c}{\textbf{Fine-tuned \&{} Evaluated on}} \\
\cmidrule(lr){2-3}
 & WebNLG-\textit{Seen} & AGENDA \\
\midrule
\multicolumn{3}{c}{BART\textsubscript{base}} \\
\midrule
None & 58.71 & 22.01 \\
KGAIA & 63.20 & 23.48 \\
WebNLG & - & 21.98 \\
AGENDA & 61.25 & - \\

\midrule
\multicolumn{3}{c}{T5\textsubscript{base}} \\
\midrule
None & 62.93 & 20.73 \\
KGAIA & 63.19 & 22.44 \\
WebNLG & - & 20.27 \\
AGENDA & 62.75 & - \\

\bottomrule
\end{tabular}}
\caption{Effect (measured with BLEU score) of cross-domain \textsc{sta}.}
\label{tab:crossdomain_adddata}
\end{table}
\vspace{-5mm}

\section{Input Graph Size}
\label{section:inputgraphsize}
Figure~\ref{fig:graphs-triples} visualizes T5\textsubscript{small}'s performance with respect to the number of input graph triples in WebNLG dataset.
We observe that \shufmodel{T5}{small}{order} and \shufmodel{T5}{small}{shuf} perform similarly for inputs with only one triple but that the gap between the models increases with larger graphs. While it is obviously more difficult to reconstruct a larger graph than a smaller one, this also suggests that the graph structure is more taken into account for graphs with more than 2 triples.
For the \textit{unseen} setting, the performance gap for these graphs is even larger, suggesting that the PLM can make more use of the graph structure when it has to.

 \begin{figure}[h]
    \centering
    \includegraphics[width=.3\textwidth]{graph_triples.pdf}
    \caption{chrF++ scores with respect to the number of triples for WebNLG \textit{seen} and \textit{unseen} test sets.}
    \label{fig:graphs-triples}
\end{figure}
\vspace{-6mm}


\begin{table*}[t]
\centering
{\renewcommand{\arraystretch}{0.8}
\begin{tabular}{lp{0.2cm}rrrp{0.2cm}rrrp{0.2cm}rrrp{0.2cm}}  
\toprule
& & \multicolumn{3}{c}{\textbf{AMR}} & & \multicolumn{3}{c}{\textbf{WebNLG}} & & \multicolumn{3}{c}{\textbf{AGENDA}} \\
\midrule
min, avg and max number of nodes & & 2 & 28.6 & 335 &  & 2 & 6.8 & 15 & & 2 & 10.5 & 80 \\
min, avg and max node degrees & & 1 & 2.2 & 21 & & 1 & 1.7 & 7 & & 1 & 1.67 & 15 \\
min, avg and max number of edges & & 1 & 32.3 & 554 & & 1 & 5.9 & 14 & & 1 & 8.8 & 124 \\
min, avg and max graph diameter & & 1 & 12.2 & 40 & & 1 & 4.1 & 10 & & 1 & 3.1 & 20\\
min, avg and max shortest path length & & 0 & 7.49 & 40 & & 0 & 2.4 & 10 & & 0 & 2.3 & 20\\
\bottomrule
\end{tabular}}
\caption{Graph statistics of AMR, WebNLG and AGENDA datasets. The values are calculated using the training data. Note that AMR graphs contain a more complex structure than WebNLG and AGENDA graphs.}
\label{tab:graphstats}
\end{table*}



\section{Graph Statistics}
\label{sec:graphstats}


In Table~\ref{tab:graphstats}, we present the graph properties of the three datasets. All statistics are calculated using the Levi transformation \cite{beck-etal-2018-acl2018} of the undirected version of the graphs, where edges are also considered nodes in the graph. WebNLG and AGENDA datasets contain disconnected graphs, and we use the largest subgraph to calculate the diameter. Note that AMR graphs have a much more complex structure: (i) they have more nodes and edges than WebNLG and AGENDA graphs; (ii) the average graph diameter and the average shortest path between nodes in AMRs are at least three times larger than in WebNLG and AGENDA graphs; (iii) nodes in AMRs have larger degrees than nodes in WebNLG and AGENDA graphs.

\vspace{15mm}

\begin{table}[h]
{\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{\hspace*{1mm}}l@{\hspace*{2mm}}r@{\hspace*{3mm}}r@{\hspace*{2mm}}r@{\hspace*{2mm}}r@{\hspace*{1mm}}}  
\toprule
 & \textbf{AMR17} & \textbf{WebNLG} & \textbf{AGENDA}  \\
\midrule
\#Train & 36,521 & 18,102 & 38,720  \\
\#Dev & 1,368 & 872 & 1,000  \\
\#Test & 1,371 & 1,862 & 1,000 \\
\midrule
\#Relations & 155 & 373 & 7 \\
Avg \#Tokens & 16.1 & 31.5 & 157.9 \\


\bottomrule
\end{tabular}}
    \caption{Statistics for the graph-to-text benchmarks.}
    \label{tab:datastatistics}
\end{table}
\begin{table}[h]
{\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{\hspace*{4mm}}l@{\hspace*{6mm}}c@{\hspace*{6mm}}c@{\hspace*{6mm}}c@{\hspace*{3mm}}c@{\hspace*{14mm}}}  
\toprule
 & \textbf{Title} & \textbf{Abstract} & \textbf{KG}  \\
 \midrule
 Vocab & 48K & 173K & 113K \\
 Tokens & 2.1M & 31.7M & 9.6M \\
 Entities & - & - & 3.7M \\
 Avg Length & 11.1 & 167.1 & - \\
 Avg \#Nodes & - & - & 19.9 \\
 Avg \#Edges & - & - & 9.4 \\

\bottomrule
\end{tabular}}
\caption{Statistics for the KGAIA dataset.}
\label{tab:augstatistics}
\end{table}

\begin{table}[h]
{\renewcommand{\arraystretch}{0.6}
\begin{tabular}{@{\hspace*{1mm}}l@{\hspace*{1mm}}ccc@{\hspace*{1mm}}}  
\toprule
\textbf{Model} & \textbf{chrF++} & \textbf{BS (F1)} & \textbf{MS}  \\
\midrule
\citet{schmitt2020modeling} & 44.53 & -& -\\
\citet{ribeiro-etal-2020-modeling} & 46.37 & -& - \\
\midrule
BART\textsubscript{base} & 48.02 & 89.36 & 34.33 \\
BART\textsubscript{large} & \textbf{50.44} & 88.74 & 32.24 \\
T5\textsubscript{small}  & 44.91 & 88.56 & 30.25 \\
T5\textsubscript{base} & 48.14 &  88.81 & 31.33 \\
T5\textsubscript{large} &  48.14 & \textbf{89.60} & \textbf{35.23} \\
\midrule
\multicolumn{2}{l}{\small{\textit{with task-adaptive pretraining}}}  \.7em]
BART\textsubscript{large} + \textsc{sta} & \textbf{\textit{51.63}} & 89.27 & 34.28 \\
T5\textsubscript{large} + \textsc{sta} & 50.27 & \textbf{\textit{89.93}} & \textbf{\textit{36.86}} \\
\bottomrule
\end{tabular}}
\caption{Results of the chrF++, BertScore (BS) and MoverScore (MS) scores for AGENDA test set. \textbf{Bold} (\textbf{\textit{Italic}}) indicates best scores without (with) task-adaptive pretraining.}
\label{tab:results-agenda-appendix}
\end{table}

\begin{table}[t]
{\renewcommand{\arraystretch}{0.6}
\begin{tabular}{@{\hspace*{1mm}}l@{\hspace*{1mm}}c@{\hspace*{2mm}}c@{\hspace*{2mm}}c@{\hspace*{1mm}}}  
\toprule
\textbf{Model} & \textbf{chrF++} & \textbf{BS (F1)} & \textbf{MS}  \\
\midrule
\citet{dcgcnforgraph2seq19guo} & 57.30 & - & - \\
\citet{zhu-etal-2019-modeling} & 64.05 & - & - \\
\citet{cai-lam-2020-graph} & 59.40 & - & - \\
\citet{doi:10.116200297} &  65.80 & - & - \\
\citet{yao-etal-2020-heterogeneous}  &  65.60 & - & - \\
\midrule
\small{\textit{based on PLMs}}  &  \.2em]
BART\textsubscript{large} + \textsc{lma} & 71.14 & 95.94 & 64.75  \\
T5\textsubscript{large} + \textsc{lma} & 72.83 & 96.32 & 67.44  \.7em]
BART\textsubscript{large} + \textsc{sta} \small{\textsc{(200K)}} & 72.26 & 96.21 &66.75 \\
BART\textsubscript{large} + \textsc{sta} \small{\textsc{(2M)}} & 73.58 & 96.43 & 68.14 \\
T5\textsubscript{large} + \textsc{sta} \small{\textsc{(200K)}} & 74.09 & 96.51 & 68.86 \\
T5\textsubscript{large} + \textsc{sta} \small{\textsc{(2M)}} & \textbf{\textit{74.79}} & \textbf{\textit{96.59}} & \textbf{\textit{69.53}} \\
\bottomrule
\end{tabular}}
\caption{Results of the chrF++, BertScore (BS) and MoverScore (MS) scores for the LDC2017T10 test set. \textbf{Bold} (\textbf{\textit{Italic}}) indicates the best score without (with) task-adaptive pretraining.}
\label{tab:results-amr-appendix}
\end{table}









\begin{table*}[t]
    \centering
\footnotesize
    \begin{tabular}{@{\hspace{.1em}}c@{\hspace{.1em}}c@{\hspace{.8em}}p{5cm}@{\hspace{.8em}}p{4.5cm}@{\hspace{.8em}}p{4.5cm}@{\hspace{.8em}}}
    \toprule
         &\textbf{T/F}&\multicolumn{1}{c}{\textbf{Input Facts}} & \multicolumn{1}{c}{\textbf{\shufmodel{T5}{small}{order}}} & \multicolumn{1}{c}{\textbf{\shufmodel{T5}{small}{shuf}}}  \\
         \midrule
(1) &S& \sep{} capital \sep{} leader Name \sep{} London \sep{} Pound sterling \sep{} United Kingdom \sep{} leader Name \sep{} United Kingdom \sep{} Elizabeth II \sep{} United Kingdom \sep{} Boris Johnson \sep{} London \sep{} currency & The capital city is London, the currency is the Pound sterling and the leader is Elizabeth II. Boris Johnson is also a leader in the UK. & The capital of the United Kingdom is London, the currency is the Pound sterling and the country is lead by Elizabeth II and Boris Johnson.\\
         \midrule
                 (2) &T& \sep{} Germany \sep{} capital \sep{} Berlin & Berlin is the capital of \mbox{Germany.}  & Berlin is the capital of \mbox{Germany.}  \\
                 \midrule
         (3) &F& \sep{} Berlin \sep{} capital \sep{} Germany & Berlin's capital is \mbox{Germany.}  & Berlin is the capital of \mbox{Germany.}  \\
         
         \midrule
                 (4) &F& \sep{} Leinster \sep{} is Part Of \sep{} Dublin & Leinster is part of Dublin. & Leinster is part of Dublin. \\
         \midrule
         (5) &F& \sep{} Rome \sep{} capital \sep{} Italy & Romeâ€™s capital is Italy. & Rome is the capital of Italy. \\
         \midrule
         (6) &T& \sep{} Italy \sep{} capital \sep{} Rome & Italyâ€™s capital is Rome. & Rome is the capital of Italy.\\
         \midrule
          (7) &T& \sep{} Texas \sep{} capital \sep{} Austin \sep{} Andrews County Airport \sep{} location \sep{} Texas & Austin is the capital of Texas where Andrews County Airport is located.  &  Austin is the capital of Texas where Andrews County Airport is located. \\
          \midrule
         (8) &F& \sep{} Austin \sep{} capital \sep{} Texas \sep{} Andrews County Airport \sep{} location \sep{} Texas  & The capital of Austin is Texas and Andrews County Airport is located in Texas. & Andrews County Airport is located in Texas where Austin is the capital.
  \\
\bottomrule
    \end{tabular}
    \caption{Example generations from shuffled (S), true (T), and corrupted (F) triple facts by T5\textsubscript{small}, fine-tuned on correctly ordered triples (\emph{order}) and randomly shuffled input (\emph{shuf}).}
    \label{tab:qualitative_appendix}
\end{table*} \begin{table*}[h]
\begin{center}
\begin{tabular}{llp{11cm}}
\toprule
\textbf{D}     & \textbf{Model}       & \textbf{Examples}    \\
\midrule
\vertmulticell{10}{AMR}  & Reference  & I had to deal with verbal abuse from my dad for a long 8 years before I came to uni and honestly, the only reason why I'm here is because it was the only way out. \\
 & T5 & I had to deal with 8 years of verbal abuse from my dad before coming to university and honestly the only reason I'm here is because it's the only way out. \\
 & BART & I had to deal with my dad's verbal abuse for 8 years long before coming to uni and honestly the only reason I'm here is because it's the only way out. \\
 & \citet{mager2020gpttoo} & i've had to deal with verbal abuse from my dad for 8 years (before i came to uni i was honestly the only reason i was here) and it's only because of the way it is. \\
 
 \midrule
\vertmulticell{10}{WebNLG}  & Reference & Aaron Turner is an electric guitar player who has played with the black metal band Twilight and with Old Man Gloom. Death metal is a musical fusion of black metal.  \\
    & T5 & Aaron Turner plays the electric guitar and is associated with the band Twilight. He is also a member of the Old Man Gloom band. Black metal and death metal are both genres of music. \\
       & BART & The black metal genre is a variation of the death metal genre. It is also where the band, Twilight, and the alternative rock band, Old Man Gloom, are from. One of the members of the band is Aaron Turner, who plays the electric guitar. \\
   & \citet{harkous2020text} & Aaron Turner, a.k.a. Black Metal, is a member of the Twilight (band) and Old Man Gloom bands. He also plays electric guitar and has a strong connection with the Death Metal genre. \\
\midrule
\end{tabular} 
\caption{Examples of text generated by the different models. D refers to the dataset.}\label{tab:human_examples}
\end{center}
\end{table*} 

\begin{table*}[h]
\begin{center}
\begin{tabular}{lp{12cm}}
\toprule
 \textbf{Model}       & \textbf{Examples}    \\
\midrule
    Reference &  in this paper, a new array signal processing technique by using particle swarm optimization is proposed to identify multipath channel parameters. the proposed array signal processing technique provides estimates to the channel parameters by finding a global minimum of an optimization problem. since the optimization problem is formulated in the cross-ambiguity function (caf) domain of the transmitted signal and the received array outputs, the proposed array signal processing technique is called as particle swarm optimization. the performance of the particle swarm optimization is compared with the space alternating generalized expectation maximization technique and with another recently proposed pso based technique for various snr values. simulation results indicate the superior performance of the pso based technique over mentioned techniques for all snr values. \\
    \midrule
 T5  & this paper, we propose an array signal processing technique based on particle swarm optimization to estimate the multipath channel parameters from the received and the transmitted signal in a cross-ambiguity domain. the proposed array signal processing technique is formulated as an optimization problem and particle swarm optimization is compared with the generalized expectation maximization technique. simulation results show that the proposed particle swarm optimization outperforms the pso based technique for a wide range of snr values. in addition, the proposed particle swarm optimization is computationally more efficient than the gem based technique for small snr values. \\
 \midrule
 BART  & in this paper, a new array signal processing technique based on particle swarm optimization is proposed. the proposed array signal processing technique is used to estimate the multipath channel parameters from the transmitted signal. the proposed array signal processing technique is formulated as an optimization problem in the cross-ambiguity domain. the particle swarm optimization is compared with the generalized expectation maximization technique. simulation results show that the proposed particle swarm optimization outperforms the pso based technique for all snr values. furthermore, the proposed particle swarm optimization is able to estimate the channel parameters more accurately than the generalized expectation maximization technique. \\
 \midrule
  \citet{ribeiro-etal-2020-modeling}  & in this paper, a novel array signal processing technique based on particle swarm optimization is proposed to estimate the multipath channel parameters from the transmitted signal. the proposed array signal processing technique uses particle swarm optimization to estimate the multipath channel parameters. the proposed array signal processing technique is formulated as an optimization problem. simulation results show that the proposed array signal processing technique outperforms the conventional generalized expectation maximization technique and the pso based technique is robust to the snr values. \\

\midrule
\end{tabular} 
\caption{Examples of text generated by the different models trained on the AGENDA dataset.}\label{tab:human_examples_agenda}
\end{center}
\end{table*} 

 


\end{document}
