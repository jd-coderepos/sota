
In this section, the results obtained from the experimental studies are presented. Typical industry simulation specifications are considered; from 200,000 to 800,000 trials with each trial comprising 1,000 events, and one Layer covering 16 XELTs (industry practitioners were consulted in choosing the size of the simulation). The combined size of the YET comprising 800,000 trials, the 16 XELTs, and the metadata that defines one Layer is approximately 28 GB for each Layer. So if $N$ Layers are to be used in ARA then the input data will be $N \times 28$ GB. The size of the input data is reduced when the ARA only considers PU since the YET and ELTs are not as large as when SU is considered. We refer to `ARA with PU' when only primary uncertainty is considered and `ARA with SU' when secondary uncertainty is also taken into account. A single library (for example, Boost) or a combination of libraries (for example, MKL and Beta\_nc) is employed for the statistical functions in secondary uncertainty. 

Figure \ref{figure1} and Figure \ref{figure2} are the graphs obtained for the sequential implementation. Figure \ref{figure3} to Figure \ref{figure9} are the results obtained for parallel implementations on the CPUs, on hardware accelerators and on hybrid platforms. The summary of the results from both sequential and parallel implementations are presented in Figure \ref{figure13}.

\subsection{Sequential Implementation}

Figure \ref{fig:1a} and Figure \ref{fig:1b} show the results from the i7 and Xeon CPUs respectively, when the Boost, MKL and Beta\_nc libraries are employed to sequentially perform ARA. 
In the best case, the i7 is nearly 49\% faster than Xeon for ARA with PU and 95\% faster than Xeon for ARA with SU, resulting in the i7 being 1.32 times faster than Xeon. 

Surprisingly, the combination of MKL and the vectorised Beta\_nc libraries are outperformed by the combination using MKL and non-vectorised Beta\_nc library. 
The vectorised code performs well when the precision of the inverse Beta CDF is high. 
In all the experiments presented in this paper, the solution for the inverse Beta CDF converges when there is accuracy for up to six decimal places (or relative error is less than $10^{-6}$). 
However, if the precision of the solution is increased for greater accuracy, for example, twelve decimal places, then the vectorised library improves the performance significantly. 
When the precision is low only few iterations are required for the solution to converge; few iterations cannot reap the benefit of vectorisation, but introduces overheads which is a trade-off. 

All possible combinations of the libraries for the CDF and the Quantile of the CDF for the Normal distribution and the Quantile of the Beta distribution were employed. On both the i7 and Xeon, the combination of the MKL (for the Normal distribution functions) and Beta\_nc (for the Beta distribution function) libraries gave the best results. One advantage of using the MKL library is that it supports the Intel AVX instructions and performs well on Intel hardware.

\begin{figure} \centering
	\subfloat[i7]{\label{fig:1a}\includegraphics[width=0.5\textwidth]{Figure1a.jpg}} \\
	\subfloat[Xeon]{\label{fig:1b}\includegraphics[width=0.5\textwidth]{Figure1b.jpg}}
\caption{Sequential ARA with PU and SU for 200,000 to 800,000 trials using the Boost, MKL and Beta\_nc libraries on CPU}
\label{figure1}
\end{figure}

Figure \ref{figure2} shows the individual times taken to apply PU and SU on the i7 and Xeon CPUs when Boost library is used. 
The time for applying SU is nearly 2.5 times the time taken for ARA in each case of trials shown in the graph. 
The mathematical functions employed for applying SU are fast methods, although the inverse CDF of the beta distribution which takes majority of the time is an exception; over 95\% of the time taken for SU is required by the inverse CDF. 
The time for applying secondary uncertainty is nearly 1.5 - 1.7 times the time taken for ARA in each case of trials shown in the graph. 

\begin{figure} \centering
	\subfloat[i7]{\label{fig:2a}\includegraphics[width=0.5\textwidth]{Figure2a.jpg}} \hspace{12pt}
	\subfloat[Xeon]{\label{fig:2b}\includegraphics[width=0.5\textwidth]{Figure2b.jpg}}
\caption{Time taken for sequential ARA with PU and SU using the Boost library on CPU}
\label{figure2}
\end{figure}

The time taken for ARA with PU and SU with increasing number of trials should scale linearly. This is observed both in Figure \ref{figure1} and Figure \ref{figure2}. 



\subsection{Parallel Implementations}

The results obtained from parallel implementations on the multi-core CPUs are shown in Figure \ref{figure3} to Figure \ref{figure5}. The results when many core hardware accelerators are used in the native mode or individually are shown in Figure \ref{figure6} to Figure \ref{figure11}. Results obtained on hybrid platforms comprising multi-cores and many-cores are shown in Figure \ref{figure12} to Figure \ref{figure9}. 



\subsubsection{On CPU}
Figure \ref{figure3} shows the time taken for performing ARA with PU and SU when the number of threads are varied from 1 to 8 on the i7 (Figure \ref{fig:3a}) and the Xeon (Figure \ref{fig:3b}) platforms using the Boost, MKL and Beta\_nc libraries for 800,000 trials. 
A single thread is run on each core of the CPU and the number of cores are varied from 1 to 8. 
Each threads performs ARA with PU and SU for a single trial. 
Multiple threads are used by employing OpenMP directive \texttt{\small \#pragma omp parallel} in the C++ source. 
With respect to the overall time on the i7 and Xeon CPUs, the MKL and Beta\_nc combination performs the best requiring around 113 seconds.
On both platforms, the performance of the standalone Boost library is poorer than the combination of libraries. 
The vectorised library does not perform better than the unvectorised library. 
This is so, since the benefits of vectorisation are evident only when high precision is required from the solution of the inverse Beta CDF.
While there are only few iterations required for converging to six decimal place precision, as required in this research, the benefits of vectorisation are balanced off by the overheads in the vectorised library.

In the case of the best performer, the MKL and the Beta\_nc library, a speed up of 4.6x is obtained on i7 and 7.6x is obtained on Xeon. The performance on the Xeon is better due to the larger number of cores compared to the i7 and the bandwidth to memory on the Xeon which is 2.4 times that on the i7. The majority of the time taken to compute primary uncertainty is for performing random access reads into the data structure representing the XELT (this is further discussed in Section \ref{experimentalresults:discussion}; on the i7 and Xeon, the time taken for such memory operations is nearly five times the time taken for the remaining computations while applying financial terms in PU, also shown later in Figure \ref{figure13}). The majority of the time for applying secondary uncertainty in ARA is consumed in the inverse cumulative distribution function of the beta distribution (nearly 95\% of the time taken for SU). 

\begin{figure} \centering
	\subfloat[i7]{\label{fig:3a}\includegraphics[width=0.5\textwidth]{Figure3a.jpg}} \\
	\subfloat[Xeon]{\label{fig:3b}\includegraphics[width=0.5\textwidth]{Figure3b.jpg}}
\caption{Time taken for ARA with PU and SU using one thread per computing core on multi-core CPUs for 800,000 trials}
\label{figure3}
\end{figure}

Figure \ref{figure4} shows the graphs plotted for the time taken for performing ARA accounting both for PU and SU when the number of threads are varied from 16 to 2048 on the i7 (Figure \ref{fig:4a}) and Xeon (Figure \ref{fig:4b}) platforms using the Boost, MKL and Beta\_nc libraries for 800,000 trials. 
Multiple threads are run on each core of the CPU with the exception of the Xeon until 32 threads are used. 
For example, when 16 threads are employed on the i7 and Xeon two threads and one thread run on each core respectively, and when 2048 threads are employed on the i7 and Xeon 256 threads and 128 threads run on each core respectively. 
In practice, the cores are not generally over subscribed, but is presented in the experiments to show the effect of over subscription.
For example, super linear speed ups are observed due to hyperthreading. 
On the i7 a small drop is observed first in the absolute time followed by a slight but gradual increase when many threads are executed on each core. 
However, on the Xeon, before there is a similar observation, there is a steep drop in the time moving from 16 to 32 threads. 
This is because the Xeon has 16 cores and the timings start to stagnate only after all the 16 cores are used. 
On the i7 the best result is 139 seconds compared to 48 seconds on the Xeon; the better performance by a factor of 3 is attributed to twice the number of cores and 2.6 times the bandwidth to memory compared to the i7.

\begin{figure} \centering
	\subfloat[i7]{\label{fig:4a}\includegraphics[width=0.5\textwidth]{Figure4a.jpg}} \\
	\subfloat[Xeon]{\label{fig:4b}\includegraphics[width=0.5\textwidth]{Figure4b.jpg}}
\caption{Time taken for ARA with PU and SU using multiple thread per computing core on multi-core CPUs for 800,000 trials}
\label{figure4}
\end{figure}

Figure \ref{figure5} shows the graphs plotted for the time taken for ARA with PU and SU on the i7 and Xeon platforms for 32 threads (best performance is obtained for 32 threads on both platforms). The time taken for applying SU increases with the number of trials in each case of trials shown in the graph. The time taken for applying SU on the i7 is only $\frac{1}{6}^{th}$ the time taken in the sequential implementation on the i7. The time taken for ARA with SU on the Xeon is only $\frac{1}{21}^{th}$ the time taken in the sequential implementation. The Xeon is nearly three times faster than the i7 for applying PU and upto 2.6 times faster for applying SU. Overall a speed up of approximately 5x and 18x is obtained on the i7 and Xeon over their respective sequential implementations.

\begin{figure} \centering
	\subfloat[i7]{\label{fig:5a}\includegraphics[width=0.5\textwidth]{Figure5a.jpg}} \\
	\subfloat[Xeon]{\label{fig:5b}\includegraphics[width=0.5\textwidth]{Figure5b.jpg}}
\caption{Time taken for ARA with PU and SU using MKL and Beta\_nc libraries using 32 threads on multi-core CPUs}
\label{figure5}
\end{figure}




\subsubsection{On Hardware Accelerators only}



Figure \ref{fig:6a} and Figure \ref{fig:6b} show the graphs plotted for the time taken for performing ARA considering both PU and SU for 800,000 trials on the many-core GPU using the CUDA Math and Beta\_nc libraries. The Boost library is not available for GPUs. 

In Figure \ref{fig:6a}, the analysis is performed for 800,000 trials on the GPU by varying the number of threads per block from 16 to 512 threads. 
An improvement in the performance is seen as the number of threads per block increase from 16 to 128 since the latency for accessing the global memory drops. 
Beyond 128 threads, the performance starts to drop, since the shared and constant memory available to each thread decreases. 
The lowest time taken is 55 seconds when 128 threads per block are used, which is nearly 12 times faster than the best sequential performance on i7 and 15 times faster than the best sequential performance on Xeon. 
The overall time for the analysis on the GPU is comparable to the performance of the multi-threaded implementation of 2048 threads on the Xeon.

Figure \ref{fig:6b} shows the time taken for performing ARA incorporating PU and SU using 128 threads per block on the GPU. Both times scale linearly as expected. The time taken for applying SU is over twice the time taken for performing ARA.

\begin{figure} \centering
	\subfloat[For 800,000 trials using varying threads per block]{\label{fig:6a}\includegraphics[width=0.5\textwidth]{Figure6.jpg}} \\
	\subfloat[Using 128 threads per block for varying number of trials]{\label{fig:6b}\includegraphics[width=0.5\textwidth]{Figure7.jpg}}
\caption{Time taken for ARA on the GPU}
\label{figure6}
\end{figure}








Figure \ref{figure10} and Figure \ref{figure11} show the experimental results obtained when the Intel Phi alone is employed for performing ARA with PU and SU using Boost, MKL and Beta\_nc libraries. 
Figure \ref{fig:10a} shows the performance of ARA when one thread is executed on each core of the Phi (the number of threads is varied from 1 to 60). 
As expected there is an increase in speed when the number of threads is increased. 
The fastest time obtained is 78.32 seconds on 60 threads using MKL and Beta\_nc. 
The combination of Boost and Beta\_nc also perform comparably. 
ARA with PU consists mostly of memory related operations and does not experience any significant speedup. 
For example, 12 seconds are required on 30 threads as against 7 seconds on 60 threads. 
Over 90\% of the time is required for applying secondary uncertainty when 60 threads are used. 
Vectorisation does not yield any potential benefit over non-vectorised functions in applying secondary uncertainty due to overheads; vectorised function is nearly 38\% slower than the fastest non-vectorised function on 60 threads.  

\begin{figure} \centering
	\subfloat[Single thread per core]{\label{fig:10a}\includegraphics[width=0.5\textwidth]{Figure8a.jpg}} \\
	\subfloat[Multiple threads per core]{\label{fig:10b}\includegraphics[width=0.5\textwidth]{Figure8b.jpg}}
\caption{Time taken for ARA with PU and SU using Intel Phi for 800,000 trials}
\label{figure10}
\end{figure}

Figure \ref{fig:10b} shows the performance of ARA when PU and SU are applied using multiple threads per core on the Phi (number of threads is varied from 60 to 240). 
Speedup is observed although there is limited efficiency. 
Again best performance is noted when MKL and Beta\_nc libraries are employed. 
The analysis is completed in 52.16 seconds and 42.1 seconds when two threads per core and three threads per core are used respectively.

Figure \ref{figure11} shows that there is a linear increase in the time taken for applying PU and SU on the Phi processor for varying number of trials. The time taken for SU is over 8 times the time taken for PU; the for SU is just under 38 seconds both for the Phi and the GPU. The real difference is observed for PU in that the Phi is four times faster than the GPU for performing memory computations. 

\begin{figure}
	\centering
	\includegraphics[width = 0.5\textwidth]{Figure11.jpg}
	\caption{Times taken for applying PU and SU in ARA on the Phi for varying number of trials}
	\label{figure11}
\end{figure}



\subsubsection{On Hybrid platforms}



Figure \ref{figure12} shows the time taken for performing ARA on the i7 CPU and the Tesla C2075 GPU used as a hybrid platform. 
The workload of 800,000 trials is split between the CPU and the GPU, with the intention that the CPU does not remain idle, in contrast to when the GPU alone is employed. 
The CPU makes use of the multi-cores of the processor and 16 threads using OpenMP to execute the workload; the MKL and Beta\_nc libraries are employed for applying secondary uncertainty. 
The workload on the GPU makes use of the many cores and 128 threads per core; the combination of the CUDA Math and Beta\_nc libraries are made use of to apply secondary uncertainty. 
The x-axis of the graph shows the varying workload of trials on the CPU and the GPU and the corresponding time taken for the workload on each platform. 
The best case scenario is when the CPU and the GPU take similar times for execution, which is the point of inflection on the graph. 
The CPU tends to be idle before the plots converge and the GPU is not utilised beyond the point of convergence. 
The optimal workload is when the CPU performs the analysis for 220,000 trials and the GPU performs the analysis on the remaining 580,000 trials. The analysis on the hybrid platform is 25\% faster (41 seconds) than the GPU (55 seconds). 

\begin{figure}
	\centering
	\includegraphics[width = 0.5\textwidth]{Figure12.jpg}
	\caption{Times taken on the i7 CPU and Tesla C2075 GPU for applying PU and SU in ARA for varying trials}
	\label{figure12}
\end{figure}





Figure \ref{figure8} shows the experimental results obtained when the Intel Xeon and Phi Coprocessor are employed for performing ARA accounting for both PU and SU using the Boost, MKL and Beta\_nc libraries. 
Figure \ref{fig:8a} is the graph showing the performance of ARA when the number of threads are varied from 1 to 60 (i.e., execution of single thread per core). 
As expected an increase in speed is noted with increasing number of threads. 
The fastest time obtained for ARA with SU is less than 102 seconds on 60 threads using the MKL and Beta\_nc libraries; the combination of the Boost and Beta\_nc libraries is also comparable.
ARA accounting only for PU does not enjoy the same degree of speedup, since it consists mostly of memory operations; 18 seconds are required on 60 threads and 24 seconds on 30 threads. 
Nearly 82\% percent more time is required for SU using 60 threads. Despite the increased potential for vectorization on the Phi coprocessor, the overhead in the vectorized functions causes it to be nearly 20\% slower than the non-vectorised library. 

Figure \ref{fig:8b} is the graph showing the performance of ARA incorporating PU and SU when the number of threads are varied from 60 to 240 (i.e., execution of multiple threads per core) on the Phi coprocessor. The speedup steadily decreases when multiple threads are executed on each core. For example, when two threads are executing per core the best time is 72 seconds compared to 62.88 seconds when three threads are executing per core. 

\begin{figure} \centering
	\subfloat[Single thread per core]{\label{fig:8a}\includegraphics[width=0.5\textwidth]{Figure10a.jpg}} \\
	\subfloat[Multiple thread per core]{\label{fig:8b}\includegraphics[width=0.5\textwidth]{Figure10b.jpg}}
\caption{Time taken for ARA with PU and SU using Intel Xeon and Intel Phi Coprocessor}
\label{figure8}
\end{figure}

Figure \ref{figure9} shows the time taken for applying PU and SU on the Xeon and Phi. Both times scale linearly as expected. The time taken for applying SU is over twice the time taken for performing ARA with PU. The best library combination of MKL and Beta\_nc takes approximately 3.5 times longer for SU than for applying primary uncertainty. 

\begin{figure}
	\centering
	\includegraphics[width = 0.5\textwidth]{Figure9.jpg}
	\caption{Times taken for applying PU and SU in ARA for varying number of trials on Xeon and Phi}
	\label{figure9}
\end{figure}



\subsection{Discussion}
\label{experimentalresults:discussion}

Figure \ref{figure13} is a graph that summarises the key results from the experimental study. The set of three bars represents the time taken for (i) fetching Events from memory and for look up of Loss Sets in memory, (ii) applying Financial Terms and performing other computations in ARA, and (iii) applying secondary uncertainty on the sequential implementation on the CPU platforms and the parallel implementations on the multi-core CPU, many-core GPU and the Phi Co-processor. In each case, parameters specific to the implementation, such as the number of threads, were set to the best value identified during the experiments.

In the parallel implementations for ARA only considering PU, a speedup of 2.3x, of 20x and of 90x is achieved on the CPU, the GPU and the Phi respectively when compared to the corresponding sequential implementations (the parallel GPU results are compared against the sequential i7 results and the parallel Phi results are compared against the sequential Xeon results). 
A speedup of nearly 12x is achieved in the overall time for the implementation on the GPU and a speed up of 21x is obtained for the overall time for the implementation on the Phi in contrast to the respective sequential implementations on the CPU. 
For applying SU, multiple threading on the CPU is nearly between six to twenty two times faster than the sequential implementations. 
For the numeric computations on the GPU, an acceleration of approximately 26x is achieved over the sequential implementation.
Limited memory bandwidth is a bottleneck in the CPU that results in approximately 27\% and 53\% of the time spent for fetching Events and for look up of Loss Sets in memory for the sequential and parallel implementation on the CPU respectively. 
While the time for fetching Events and for look up of Loss Sets in memory have been significantly lowered on the GPU, 39\% of the total time is still used to this end. 
In the case of the Phi, this time is lowered to 3 seconds as against 16 seconds required by the GPU. 

On hybrid platforms, the combination of GPU and CPU outperform the Phi and CPU. 
The GPU and CPU perform ARA with PU and SU in only 41 seconds compared to 63 seconds required by Phi and CPU; the Phi and CPU is 50\% slower than the GPU and CPU platform, which is nearly 16x faster than the baseline implementation on i7. 
On individual accelerator platforms, without considering the time required for transferring data to the device from the host, the Phi only requires 42 seconds against 55 seconds required by the GPU; the Phi is 1.3 times faster than the GPU for ARA and 21x faster than the baseline implementation on Xeon. 

The Phi in the native mode seems faster than the combination of the Xeon and the Phi as a hybrid system. However, this is not always a practical solution since data needs to be copied offline to the memory of the Phi and is time consuming. In the case of ARA, copying data of 800,000 trials required more than three hours which is not included in the execution time. Although the execution takes only 42 seconds once all data resides in the Phi memory, it is less likely that most application can use the Phi in the native mode. The Phi is not intended to be used natively, but as a coprocessor. Hence, in the timing results the time taken for copying data to the Phi is not included.

\begin{figure*}
	\centering
	\includegraphics[width = \textwidth]{Figure13.jpg}
	\caption{Time taken for fetching Events from memory and for look up of Loss Sets in memory, applying Financial Terms and performing other computations in PU, and applying SU on sequential and parallel implementations of ARA}
	\label{figure13}
\end{figure*}

In sequential or parallel implementations on CPU, accelerator and hybrid platforms require nearly 50\% or more time for applying SU in ARA. The majority of this time is required by the computations of the inverse CDF of the beta distribution. This calls for not only the development of fast methods to apply secondary uncertainty in risk analytics, but also the development of fast methods for the underlying statistical functions. Fast methods have been implemented for computing the inverse CDF of the symmetrical beta distribution \cite{invsymbetadist} which considers one shape parameter, but there are minimal implementations of fast assymetrical beta distribution that takes two shape parameters as required for the secondary uncertainty methodology reported in this paper.