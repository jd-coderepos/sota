\documentclass{llncs}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{bm}
\usepackage{amsmath,amsfonts,amssymb,multirow}
\usepackage{multicol}
\usepackage{fullpage}
\usepackage{epsfig}
\usepackage{graphics}
\usepackage{float}

\usepackage[dvipsnames,usenames]{color}
\usepackage[colorlinks=true,urlcolor=Blue,citecolor=Green,linkcolor=BrickRed]{hyperref}
\usepackage[usenames,dvipsnames]{xcolor}
\urlstyle{same}

\pagestyle{plain}

\makeatletter


 
\let\markeverypar\everypar
\newtoks\everypar
\everypar\markeverypar
\markeverypar{\the\everypar\looseness=-1}

\setlength\marginparwidth{60pt}

\def\marrow{\marginpar[\hfill]{}}
\def\rem#1{{\marginpar{\raggedright\scriptsize #1}}}
\newcommand{\polylog}{\text{\,polylog}}
\newcommand{\pred}[1]{pred(#1)}

\sloppy
\begin{document}

\title{Submatrix Maximum Queries in Monge Matrices\\ are
Equivalent to Predecessor Search\thanks{This paper is based on two preliminary papers that appeared in ICALP 2014 and ICALP 2015.}}

\author{
Pawe{\l} Gawrychowski\inst{1} \and Shay Mozes\inst{2}\thanks{Mozes and Weimann supported in part by
  Israel Science Foundation grant 794/13.} \and Oren Weimann\inst{3}
}
\institute{
University of Wroc≈Çaw, \href{mailto:gawry@cs.uni.wroc.pl}{gawry@cs.uni.wroc.pl} \and
IDC Herzliya, \href{mailto:smozes@idc.ac.il}{smozes@idc.ac.il}  \and
University of Haifa, \href{mailto:oren@cs.haifa.ac.il}{oren@cs.haifa.ac.il}  
}


\date{}
\maketitle

\begin{abstract}
We present an optimal data structure for submatrix maximum
queries in  Monge matrices.
Our result is a two-way reduction showing that the problem is
equivalent to the classical predecessor problem in a universe of polynomial size. This gives a data
structure of  space that answers submatrix maximum queries in
 time, as well as a matching lower bound, showing that  query-time is optimal for any data structure
of size . 
Our result settles the problem, improving on the 
query-time in SODA'12, and on the  query-time in ICALP'14. 

In addition, we show that partial Monge matrices can be handled in the same bounds as full Monge matrices. In both previous results, partial Monge matrices incurred additional inverse-Ackermann factors.
\end{abstract}


 
\section{Introduction}
Data structures for range queries and for predecessor queries are
among the most studied data structures in computer science. Given an
 matrix ,  a {\em range maximum} (also called submatrix
maximum) data structure can report the maximum entry in any query
submatrix (a set of consecutive rows and a set of consecutive columns)
of . Given a set  of  integers from a
polynomial universe , 
a {\em predecessor} data structure can report the predecessor (and successor) in  of any query integer . 
In this paper, we prove that these two seemingly unrelated problems are in fact equivalent when the matrix  is a {\em Monge} matrix.

\paragraph{\bf Range maximum queries.}
A long line of research over the last three decades including~\cite{AFL07,CR1989,DemaineLandauWeimann,GBT84,YuanA10} achieved range maximum data structures of  space and  query time\footnote{The  notation
  hides polylogarithmic factors in .}, culminating with the -space  -query data structure of Yuan and Atallah~\cite{YuanA10}. In general matrices, this is optimal since representing the input matrix already
requires  space. In fact, reducing the additional space to  is known to incur an  query-time~\cite{BrodalDR10} and such tradeoffs can indeed be achieved for any value of ~\cite{BrodalESA,BrodalDR10}. 

However, in many applications, the matrix  is not stored explicitly
but any entry of  can be computed when needed in  time. One
such case is when the matrix  is sparse, i.e.,  has 
  nonzero entries. In this case 
the problem is known in computational geometry as the {\em orthogonal range searching} problem  on the  grid. In this case as well, various data structures with -space and -query appear in a long history of results including
~\cite{AlstrupEtAl,Patrascu,Chazelle88,Munro,GBT84}. 
For a survey on orthogonal range searching see~\cite{Nekrich}. 
Another case where the additional space can be made  (and in fact even ) is when the matrix is a Monge matrix.

\paragraph{\bf  Range maximum queries in Monge matrices.}
A matrix  is {\em Monge} if for any pair of rows  and columns
 we have that .\footnote{Monge matrices are often defined with a 
  (rather than ) in the condition. Our results apply to both
  definitions, as well as to minimum (rather than maximum) queries.}
 A matrix  is {\em Totally Monotone} (or TM) if for any pair of
 rows  and columns  we have that if 
 then .  Notice that the Monge property
 implies total monotonicity but the converse is not true. Whenever
 possible, we state our results for the more general class of TM
 matrices. Throughout the paper we use a top-down and left-to-right ordering of the
 elements of a matrix. We say that  is above  if
 , and to the left  of  if .
 
Submatrix maximum queries on Monge matrices 
have various important applications in combinatorial optimization and computational
geometry 
such as problems involving distances in the plane, and in problems on
convex -gons. 
See~\cite{BKR96} for a survey on Monge matrices and their uses in combinatorial
optimization. Submatrix maximum queries on Monge matrices are
used in algorithms that efficiently find the largest empty rectangle containing a
query point, in dynamic distance oracles for planar graphs,
and in algorithms for maximum flow in planar graphs.
See~\cite{KaplanMozesNussbaumSharir} for more details on the
history of this problem and its applications.

Given an  Monge matrix  it is possible to obtain compact data structures of only  space that can answer submatrix maximum queries in  time.  The first such data structure was given by 
Kaplan, Mozes, Nussbaum and
Sharir~\cite{KaplanMozesNussbaumSharir}. 
They presented an -space data structure with   query time.
This was improved in~\cite{ourICALP}  to  space and  query time.  

\paragraph{\bf Breakpoints and Partial Monge matrices.}
Given an  Monge matrix , let  be the row containing the maximum element in the -th
column of . It is easy to verify that the  values are monotone, i.e., .
Columns  such that  (or ) are called the {\em breakpoints} of . A Monge matrix
consisting of  rows has  breakpoints, which can be found in
 time using the SMAWK algorithm~\cite{SMAWK} (total monotonicity
suffices for SMAWK). 

Some applications involve  {\em partial} Monge matrices rather than
full Monge matrices.
A partial matrix is a matrix where some of the
entries are undefined, but the defined entries in each row
and in each column are contiguous. A partial Monge matrix is a partial matrix in which the Monge inequality is satisfied whenever all four involved entries are defined. The total number of breakpoints in a partial Monge matrix is still  (as we show in Section~\ref{sec:partial}), and they can be found in  time\footnote{Here  is the inverse-Ackermann function.} using an algorithm of Klawe and Kleitman~\cite{KK89}. This was used in~\cite{ourICALP,KaplanMozesNussbaumSharir} to extend their solutions to partial Monge matrices at the cost of an additional  factor to the query time.\footnote{In~\cite{KaplanMozesNussbaumSharir}, there was also an additional  factor to the space.}

\paragraph{\bf Our results.} 
In this paper, we fully resolve the submatrix maximum
query problem in  Monge matrices by presenting a data structure of  space and  query time. 
Consequently, we obtain an improved query time for other applications such as finding the largest empty
rectangle containing a query point. 
We compliment our upper bound with a matching lower bound, showing
that  query-time is optimal for any data structure of
size . 
Implicit in our upper and lower bound is an equivalence
between the predecessor problem in a universe of polynomial size and the range maximum query problem in
Monge matrices. The upper bound essentially reduces a submatrix
query to a constant number of predecessor problems, and vice versa, the lower bound
reduces the predecessor problem to a submatrix query problem. In fact, the lower bound holds even for the more restricted case where the submatrix query is a subcolumn.

Finally, we extend our result to partial Monge matrices with the exact
same bounds (i.e.,  space and  query time). Our
result is the first to achieve such extension with no overhead. 



\paragraph{\bf Techniques.} 
Let  be an  Monge matrix\footnote{We consider  matrices, but for simplicity we sometimes state the
results for  matrices.}. Consider a full binary tree
 whose leaves are the rows of . Let  be the
submatrix of  composed of all rows (i.e., leaves) in the subtree of
a node  in .
Both existing data structures for submatrix maximum queries~\cite{ourICALP,KaplanMozesNussbaumSharir} store, for each node  in  a data structure . The goal of  is to answer submatrix maximum queries  that include an arbitrary interval of columns and {\em exactly all rows}  of .   
This way, an arbitrary query is covered
in~\cite{ourICALP,KaplanMozesNussbaumSharir} by querying the  structures of  canonical nodes of .  An  bound is thus inherent for any solution that examines the canonical nodes. 
We overcome this obstacle by designing a stronger data
structure . Namely, one that supports  queries
that include an arbitrary interval of columns and {\em a prefix of
  rows} or {\em a suffix of rows} of . 
This way, an arbitrary query can be covered by just two s. The
idea behind the new design  is to efficiently  encode the changes in column maxima as we add rows to  one by one. Retrieving this information is done using weighted ancestor search and range maximum queries on trees. This is a novel use of these techniques.     

For our lower bound, we show that for any set of  integers
 there exists an  Monge matrix  such that the predecessor of  in 
can be found with submatrix maximum queries on . 
The predecessor lower bound of
P{\v{a}}tra{\c{s}}cu and Thorup~\cite{PT2006} then implies that  space requires   query time. 
We overcome two technical
difficulties here: First,  should be Monge. Second, there must be an  -size representation of  which can retrieve any entry  in  time. 

Finally, for handling partial Monge matrices, and unlike previous
solutions for this case, we do not directly adapt the solution for the
full Monge case to partial Monge matrices. Instead we decompose the partial Monge matrix into
many full Monge matrices, that can be preprocessed to be queried
cumulatively in an efficient way. This requires significant technical work and
careful use of the structure of the decomposition.

\paragraph{\bf Computational model.}
We assume the standard word RAM model with word size . However,
this is just an internal assumption and the elements of the matrix  are only accessed
through a comparison oracle, that is, we only assume that we are able to check in constant
time if  and no arithmetical manipulation on the elements of  is 
performed.

\paragraph{\bf Roadmap.}
In Section~\ref{sec:structure} we present an -space data structure for Monge matrices that answers submatrix maximum queries in  time. In Section~\ref{sec:linear} we reduce the space to . Our lower bound is given in Section~\ref{sec:lower bound}, and the extension to partial Monge matrices in Section~\ref{sec:partial}.


\section{Data structure for Monge matrices}
\label{sec:structure}
Our goal in this section is to construct, for a given   Monge matrix , a data structure
of size  that answers submatrix maximum queries in  time. In Section~\ref{sec:linear}
we show how to reduce the space from  to  when .
We will actually show a stronger result, namely the structure allows us to reduce in  time
a submatrix maximum query into  predecessor queries on a set consisting of  integers from
a polynomial universe.

We denote by  the complexity of a predecessor query on a set of  integers from
a universe . It is well known
that there are -space data structures achieving .

Recall
that a submatrix maximum query returns the maximum  over all  and
 for given  and . We start by answering the easier 
 \emph{subcolumn maximum queries} within these space and time bounds. 
That is,  finding the maximum  over all  for given
 and .

We construct a full binary tree  over the rows of . Every leaf of the tree corresponds to a single row
of , and every inner node corresponds to the range of rows in its subtree. To find the maximum  over all
 for  given  and , we first
locate the lowest common ancestor (lca)  of the leaves
corresponding to  and  in the tree. Then we decompose the query into two parts:
one fully within the range of rows  of the left child of , and one fully within
the range of rows  of the right child of . The former ends at the last row of 
and the latter starts at the first row of . We equip every node with two data structures
supporting  such simpler subcolumn maximum queries. Because of symmetry (if  is
Monge, so is , where ) it suffices to show how to answer
subcolumn maximum queries starting at the first row. 

\begin{lemma}
\label{lem:subcolumn}
Given an  TM matrix , a data structure of size  can be constructed
in  time to answer in  time subcolumn maximum queries starting at the first row of .
\end{lemma}

\begin{proof}
Consider queries spanning an {\em entire} column  of . To answer such a query, we only need to find
the corresponding . If we store the breakpoints of  in a predecessor structure, where
every breakpoint  links to its corresponding value of , a query can be answered
with a single predecessor search. More precisely, to determine the maximum in the -th
column of , we locate the largest breakpoint , and then set .
Hence we can construct a data structure of size  to answer {\em entire column} maximum
queries in  time.

Let  be a TM matrix consisting of the first  rows of . By applying the above
reasoning to every  separately, we immediately get a structure of size  answering
subcolumn maximum queries starting at the first row of  in  time. We
want to improve on this by utilizing the dependency of the structures constructed for different 's. Observe that the list of breakpoints of  is a prefix
of the list of breakpoints of  to which we append at most one new element. In other words,
if the breakpoints of  are stored on a stack, we need to pop zero or more elements and push
at most one new element to represent the breakpoints of . Consequently, instead of storing a separate list for every ,
we can succinctly describe the content of all stacks with a single tree  on at most 
nodes. For every , we store a pointer to a node , such that the ancestors of 
(except for the root) are exactly the breakpoints of .
Whenever we pop an element from the current stack, we move to the parent of the current
node, and whenever we push an element, we create a new node and make it a child of the
current node. Initially, the tree consists of just the root. Every node is labelled with a column
number and by construction these numbers are strictly increasing on any path starting at
the root (the root is labelled with ). 
Therefore, a predecessor search for  among the 
breakpoints of  reduces to finding the leafmost ancestor of  whose label is at most . 
This is known as the {\em weighted ancestor} problem.
Weighted ancestor queries on a tree of size 
are equivalent to predecessor searching on a number of sets of 
total size~\cite{Kopelowitz},\footnote{The reduction described in~\cite{Kopelowitz}
needs  additional time and (adaptively) queries two sets. The additional
time is required to reduce the total size of the sets to , which is done
by recursively decomposing the tree. However, this recursive decomposition
can be avoided using atomic heaps as explained in Lemma 11 of~\cite{GawrychowskiLN14}.
Then in  additional time we are able to reduce a weighted ancestor query
to a single predecessor query in one of the sets.}
achieving the claimed space and query time bounds.

To finish the proof, we need to bound the construction time. The bottleneck is constructing
the tree . Let  for some  be the breakpoints of . As long as
 we decrease  by one, i.e., remove the last breakpoint.
This process is repeated  times in total. If  we create a new breakpoint
. 
If  and  , we check if .
If so, we need to create a new breakpoint. To this end, we need to find the smallest  such
that . This can be done in
 using binary search. Consequently,  can be constructed in  time. Then augmenting
it with a weighted ancestor structure takes  time.
\qed \end{proof}

We apply Lemma~\ref{lem:subcolumn} twice to every node of the full version tree .
Once for subcolumn maximum queries starting at the first row and once for queries ending at the last row. Since the total
size of all structures at the same level of the tree is , the total size of our subcolumn
maximum data structure becomes , and it can be constructed in 
time to answer queries in  time. Hence we have proved
the following.

\begin{theorem}
\label{thm:subcolumn}
Given an  TM matrix , a data structure of size  can be constructed
in  time to answer subcolumn maximum queries in  time.
\end{theorem}

By symmetry (a transpose of a Monge matrix is Monge) we can answer subrow maximum
queries (where the query is a single row and a range of columns) in
 time. We are now ready to
tackle general submatrix maximum queries.


At a high level, the idea is identical to the one used for subcolumn maximum queries: we construct
a~full binary tree  over the rows of , where every node corresponds to a range of rows. To
find maximum  over all  and
 for given  and , we locate the lowest
common ancestor of the leaves corresponding to  and  and decompose
the query into two parts, the former ending at the last row of  and the latter
starting at the first row of . Every node is equipped with two data structures
allowing us to answer submatrix maximum queries starting at the first row or ending
at the last row. As before, it suffices to show how to answer submatrix maximum queries starting
at the first row.

\begin{lemma}
\label{lem:submatrix}
Given an  Monge matrix , and a data structure that
answers subrow maximum queries on  in  time, one can
construct in  time  a data structure consuming 
additional space, that answers submatrix maximum queries starting at
the first row of  in  time.
\end{lemma}

\begin{proof}
We extend the proof of Lemma~\ref{lem:subcolumn}.  Let 
be the breakpoints of  stored in a predecessor structure. For every 
we precompute and store the value  
These values are
augmented with a (one dimensional) range maximum query data structure. 
To begin with, consider a submatrix maximum query starting at the first row of  and ending
at the last row of , i.e., we need to calculate the maximum  over all  and . 
We find in  the successor of , denoted , and the predecessor of , denoted
. There are  three possibilities:
\begin{enumerate}
\item The maximum is reached for ,
\item The maximum is reached for ,
\item The maximum is reached for .
\end{enumerate}
The first and the third possibilities can be calculated with subrow maximum queries in  time, because
both ranges span an interval of columns and a single row.
The second possibility can be calculated with a range maximum query on the range  over
the precomputed values  associated to the breakpoints.
Consequently, we can construct a data structure of size  to answer such submatrix
maximum queries in  time.

The above solution can be generalized to queries that start at the first row of  but do not necessarily end at the last row of . This is done
by considering the Monge matrices  consisting
of the first  rows of . For every such matrix, we need a predecessor structure
storing all of its breakpoints, and additionally a range maximum structure over
their associated values . Hence now we need to construct a similar
tree  as in Lemma~\ref{lem:subcolumn} on  nodes,
but now every node has both a weight and a value. The weight of a node is the column number
of the corresponding breakpoint , and the value is its  (or undefined if ).
As in Lemma~\ref{lem:subcolumn}, the breakpoints of  are exactly the ancestors of the node . Note that
every  is defined in terms of  and , but this is not a problem because
the predecessor of a breakpoint does not change during the whole construction.
We maintain a weighted ancestor structure
using the weights (in order to find  and  in  time), and a {\em generalized range maximum structure} using the values. A generalized range maximum structure of a tree , given two query nodes  and , returns the maximum value on the unique -to-
path in . It can be implemented
in  space and  query time after  preprocessing~\cite{DemaineLandauWeimann}
once we have the values. The values can be computed with subrow maximum queries
in  total time.
\qed \end{proof}

By applying Lemma~\ref{lem:submatrix} twice to every node of the full binary tree ,
we construct in  time a data structure of size  to
answer submatrix maximum queries in  time. In
order to apply Lemma~\ref{lem:submatrix} to a node of  we
need a subrow maximum query data structure for the corresponding rows
of the matrix . Note, however, that a single subrow maximum query
data structure for  can be used for all nodes of . We
thus obtained the following theorem.

\begin{theorem}
\label{thm:submatrix}
Given an  Monge matrix , and a data structure 
answering subrow maximum queries on  in  time, one can
construct in  time  a data structure
taking 
additional space, that answers submatrix maximum queries on  in  time.
\end{theorem}

By combining Theorem~\ref{thm:subcolumn} with Theorem~\ref{thm:submatrix},
given an  Monge matrix , a data structure of size  can be
constructed in  time to answer submatrix maximum queries in
 time.



\section{Obtaining linear space}
\label{sec:linear}
In this section we show how to decrease the space of the data structure presented in Section~\ref{sec:structure} to be linear.
We extend the idea developed in our previous paper~\cite{ourICALP}.
The previous linear space solution was based on partitioning the matrix   into  matrices , where each  is a {\em slice} of  consisting of  consecutive rows. 
Then, instead of working with the matrix , we worked with the  matrix , where  is the maximum entry in the -th column of .


\paragraph{\bf Subcolumn queries.}
Consider a subcolumn query. Suppose the query is entirely contained in some . This means it spans less than  rows. In~\cite{ourICALP}, since the desired query time was , a query  simply inspected all elements of the subcolumn. In our case however, since the desired query time is only , 
we apply the above partitioning scheme twice. We explain this now.

We start with the following lemma, that provides an efficient data structure for  queries consisting of a single column and {\em all} rows in rectangular matrices.



\begin{lemma}[the micro data structure] \label{lemma:micro}
Given an  TM matrix and , one can construct in  time, a data structure of size  that given a query column can report the maximum entry in the entire column in 
 time. 
\end{lemma}
\begin{proof}
Out of all  columns of the input matrix , we will designate
 columns as {\em special} columns. For each of these special columns we will eventually compute its maximum element.  
The first  special columns of  are  columns  and are denoted . 

Let  denote the  submatrix obtained by taking all 
rows but only the  special columns . It is easy to verify that 
is TM. We can therefore run the SMAWK
algorithm~\cite{SMAWK} on  in   time and obtain the column
maxima of all special columns. 
Let  denote the row containing the maximum element in column ~\footnote{We assume that no elements of the matrix are equal. The ties are resolved lexicographically.}
Since  is TM, the  values are monotonically
non-decreasing. Consequently,  of a non-special column  must be
between  and  where  and  are
the two special  columns bracketing   
 (see Figure~\ref{fig}). 
 
\begin{figure}[h!]
   \centering
   \includegraphics[scale=0.4]{Figure}
   \caption{An  matrix inside an  matrix. The
     black columns are the first  special columns. The
     (monotonically  non-decreasing) gray cells inside these special
     columns are the column maxima (i.e., the  values of breakpoints ). The maximum element of column  in the  matrix must be between  and  (i.e., in matrix ).}
  \label{fig}
 \end{figure}

For every , let . If  then
{\em no} column between  and  will ever be a special
column. When we will query such a column  we can simply check
(at query-time) the  elements of  between rows  and
 in  time. If, however, , then we designate
more special columns between  and . This is done
recursively on the  matrix  composed of
rows  and columns . That is, we mark  evenly-spread columns of  as
special columns,  and run SMAWK in  time on the  submatrix    obtained by taking all  rows but only
these  special columns. We continue recursively until either  or the number of columns in  is at most . In the
latter case, before terminating, the recursive call runs SMAWK in  time on the  submatrix  obtained by taking the  rows and {\em
  all} columns of  (i.e., all columns of  will become
special). 

After the recursion terminates, every column  of  is either
special (in which case we computed its maximum), or its maximum  is
known to be in one of at most  rows (these rows are specified by the 
values of the two special columns bracketing ). 
Let  denote the total number of columns that are marked as special. 
We claim that . To see this, notice that the
number of columns in every recursive call decreases by a factor of at
least  and so the recursion depth is . In every recursive level, the number of added special columns is
 over all  in this level that are at least . In
every recursive level, this sum is bounded by  because each one of
the  rows of  can appear in at most two 's  (as the last
row of one and the first row of the other). Overall, we get . 

Notice that  implies that the total time
complexity of the above procedure is also . This
is because whenever we run SMAWK on a  matrix it takes
 time and  new columns are marked as special.  To complete
the construction, we go over the  special columns from left to right in
 time and throw away (mark as non-special) any column whose
 value is the same as that of the preceding special column. This
way we are left with only  special columns, and the difference in
 between consecutive special columns is at least  and at most
. In fact, it is easy to maintain  (and not ) space
{\em during} the construction by only recursing on sub matrices 
where . We note that when , the eventual special columns are
exactly the set of breakpoints of the input matrix .

The final data structure is a predecessor data structure that holds
the  special columns and their associated 
values. Upon query of some column , we search in  time
for the predecessor and successor of  and obtain the two 
values. We then  search for the maximum of column  by explicitly
checking all the (at most ) relevant rows of column . The query time is
therefore   and the space .  
\qed \end{proof}

 

In the case of , using atomic heaps~\cite{FredmanW94} (which support
predecessor searches in constant time) we obtain the following corollary:



\begin{corollary}\label{lem:micro}
Given an  TM matrix, a data structure of size  can be constructed in
 time to answer entire-column maximum queries  in  time, if .
\end{corollary}

It is possible to use Lemma~\ref{lemma:micro} to obtain a subcolumn data
structure with faster  preprocessing time,
at the cost of slower  query time (cf.~\cite[Lemma
2]{ourICALP}).  We next describe our new subcolumn data structure, which uses the above corollary and two applications of
the partitioning scheme.


















\begin{theorem}
\label{thm:subcolumn2}
Given an  Monge matrix , a data structure of size  can be constructed
in  time to answer subcolumn maximum queries in  time.
\end{theorem}

\begin{proof}
We first partition  into  matrices
, where .
Every  is a slice of  consisting of  consecutive rows. Next,  we partition
every  into  matrices , where .
Every  is a slice of  consisting of  consecutive rows (without loss of generality, assume that  divides  and  divides ).
Now we define a new  matrix , where  is the maximum
entry in the -th column of . Similarly, for every  we define
a new  matrix , where  is the maximum entry
in the -th column of .

We apply Corollary~\ref{lem:micro} on every  and  in  total time
and  total space, so that any  or  can be retrieved in  time. Furthermore,
it can be easily verified that  and all s are also Monge. 
To prove this, it is enough to argue that if  is an 
Monge matrix,
the  matrix  created by partitioning  into two
slices, each consisting
of two rows, whose elements are the maxima in every column of each slice, is also
Monge. To this end, we need to compare:

and

Let , where , and similarly
, where . Then

which is at least  because of  being Monge.
  
Therefore, because  and all  are all Monge, and
by Corollary~\ref{lem:micro} their entries can be accessed in  time, 
we can apply Theorem~\ref{thm:subcolumn} on 
and every . The total construction time is 
, 
and the total size of all structures constructed so far is 
.

Now consider a subcolumn maximum query. If the range of rows is fully within a single
, the query can be answered naively in  time.
Otherwise, if the range of rows is fully within a single , the query can be decomposed
into a prefix fully within some , an infix corresponding to a range of rows
in , and a suffix fully within some . The maximum in the prefix and the suffix can
be computed naively in  time, and the maximum in the infix
can be computed in  time using the structure constructed for .
Finally, if the range of rows starts inside some  and ends inside another ,
the query can be decomposed into two queries fully within  and , respectively,
which can be processed in  time as explained before, and an infix
corresponding to a range of rows of . The maximum in the infix can be computed
in  time using the structure constructed for .
\qed \end{proof}

\paragraph{\bf Submatrix queries.}
We are ready to present the final version of our data structure. It is based on two
applications of the partitioning scheme, and an additional trick of transposing the matrix.

\begin{theorem}
\label{thm:submatrix2}
Given an  Monge matrix , a data structure of size  can be constructed
in  time to answer submatrix maximum queries in  time.
\end{theorem}

\begin{proof}
We partition  as described in the proof of Theorem~\ref{thm:subcolumn2}, i.e.,  is
partitioned into  matrices , where , and every 
is then partitioned into  matrices , where .
Then we define smaller Monge matrices  and , and provide  time access to their
entries with Corollary~\ref{lem:micro}. We apply
Theorem~\ref{thm:subcolumn2} to the transpose of  to get a subrow
maximum query data structure for . This takes  space and
 time. With this data structure we can apply Theorem~\ref{thm:submatrix} on
, which takes an additional  space and  time. 
We also apply Theorem~\ref{thm:subcolumn2} to the transpose of the
-by-
matrix obtained by stacking the   matrices. 
This takes  space and
 time. This serves as a subrow maximum data structure for
each , so we can apply Theorem~\ref{thm:submatrix} to each
 separately, which takes a total of   additional space and  time. 




We repeat the above preprocessing on the transpose of .
Now consider a submatrix maximum query. If the range of rows starts inside some  and 
ends inside another , the query can be decomposed into two queries fully within  and
, respectively, and an infix corresponding to a range of rows of . The maximum
in the infix can be computed in  time using the structure constructed
for . Consequently, it is enough to show how to answer a query in 
time when the range of rows is fully within a single . In such case, if the range of rows
starts inside some  and ends inside another , the query can be decomposed
into a prefix fully within , an infix corresponding to a range of rows in 
and a suffix fully within some . 
The query on the infix can be answered using the data structure for . 
Consequently, we reduced the  query  in  time to four
queries such that the range of rows in each query is fully
within a single .
Since each  consists of  rows of ,
by taking the union of the rows of  corresponding to all
these 's and also including the row containing the maximum in the infixes,
we have identified, in  time, a set of  rows of
 that contain the desired submatrix maximum. 

Now we repeat the same procedure on the transpose of  to identify a set of
 columns of  that contain the desired submatrix
maximum.
Since a submatrix of a Monge matrix is also Monge, the submatrix of
 corresponding to these sets of candidate rows and columns is an
 Monge matrix. 
By running the SMAWK algorithm~\cite{SMAWK} in  time on
this small
Monge matrix, we can finally determine the answer.
\qed \end{proof}



\section{Lower Bound}
\label{sec:lower bound}
A predecessor structure stores a set of  integers , so that given  we can determine the largest 
such that . As shown by P{\v{a}}tra{\c{s}}cu and Thorup~\cite{PT2006}, for 
any predecessor structure consisting of  words needs  time to answer queries,
assuming that the word size is . We will use their result to prove that our structure is in fact optimal.

Given a set of  integers  we want to construct an  Monge matrix  such that the predecessor of
any  in  can be found using one submatrix maximum query on  and  additional time (to decide which query to ask
and then return the final answer). Then, assuming that for any  Monge matrix there exists a data structure of size 
answering submatrix maximum queries in  time, we can construct a predecessor structure
of size  answering queries in  time, which is not possible.
The technical difficulty here is twofold. First,  should be Monge. Second, we are working in the indexing model, i.e., the
data structure for submatrix maximum queries should be able to access the matrix. Therefore, for the lower bound to carry over,  should have the following
property: there is a data structure of size  which retrieves any  in  time. Guaranteeing
that both properties hold simultaneously is not trivial. 

Before we proceed, let us comment on the condition . While quadratic universe is enough to invoke
the  lower bound for structures of size
, our reduction actually implies  that even for larger
polynomially bounded universes, i.e., , for any
fixed , it is possible to construct an  Monge matrix 
such that the predecessor of  in  can be found with  submatrix maximum queries on  and 
additional time (and, as previously, any  can be retrieved in
 time with a structure of size ). This is a consequence of the following lemma. 


\begin{lemma}
\label{lem:universe reduction}
For any constant , predecessor queries on a set of  integers  can be reduced in  time to 
predecessor queries on a set of  integers  with a structure of size .
\end{lemma}

\begin{proof}
First we describe a weaker version of the reduction for , where the resulting set of integers
is .

Let . We represent every  in base  as , where
. We create a new set  storing all s and
a new set  storing all s. For any , let
 and  denote the rank of  in  and , respectively,
where rank is the number of smaller elements in the set.
We create another set  storing 
elements of the form .
We also create a perfect hash table of size  mapping  to 
and  to .
To find the predecessor of  in , we first represent it as .
We claim that it is always possible to reduce locating the predecessor
of  in  to the case where  and  in two steps.
Let  denote the predecessor of  in  and  denote the predecessor of  in .

\begin{enumerate}
\item If  is not defined, we decrease  by one (adjusting  if necessary)
and replace  by the largest element of . Otherwise, we replace  by .
\item If  is not defined,  has no predecessor in . Otherwise, if  we replace
 by  and  by the largest element of .
\end{enumerate}

Both steps maintain the predecessor of  in  and take  time.
Finally, having reduced the general case so that  and , we
locate the predecessor of  in .
Because  and , both  and  can be
retrieved in  time from the perfect hash tables.
The predecessor of  in  corresponds to the predecessor of  in ,
because comparing two elements of the same set is equivalent to comparing their ranks there.
Formally,  iff  or  and , which is equivalent
to  or 
and , which because the ranks are all from 
can be stated as .
Consequently, a predecessor query on  can be reduced into one predecessor query
into each of . These three sets can be combined into a single set ,
such that predecessor queries in either of them
can be answered with predecessor queries on , by simply shifting every element of  by 
and every element of  by .
Finally, the size of , which is up to  right now, can be reduced to  as follows.
Let the elements of  be . We store every ,
for  in the predecessor structure. Additionally, for every  we
explicitly store  and . Knowing the predecessor  of  among the
chosen elements allows us to find its predecessor among all elements in  time by additionally
inspecting  and .

Now we explain how to extend the above reduction for any constant , while also
ensuring that the resulting set of integers is .
If , we answer predecessor queries naively in  time.
If , by modifying the above reduction so that every  is represented as ,
where  and , we obtain a set of  integers from .
Hence, by iterating  times we finally obtain a set of  integers from .
Then one final iteration, where we represent every  as ,
with  with ,
allows us to reduce the size of the universe to , which is at most 
for .
To reduce the size of the universe to , we divide every  by 2.
Let the resulting set be . 
We store a perfect hash table mapping  to  (if it exists) and a list of elements  such that
 (note that there are at most two such s).
To find the predecessor of , we then find the predecessor  of  in the obtained set.
Then, we inspect all the elements stored on the lists of  and  (accessed from the entry of  in the
perfect hash table) and return the largest not exceeding  in  time.
\qed \end{proof}

\noindent The following propositions are easy to verify:

\begin{proposition}
\label{prop:adjacent condition}
An  matrix  is Monge iff  for all  and .
\end{proposition}

\begin{proposition}
\label{prop:adjustment}
If a matrix  is Monge, then for any vector  the matrix , where  for all , is also Monge.
\end{proposition}

\begin{proposition}
\label{prop:undef}
If a matrix  is partial Monge, then it remains partial Monge after replacing any element of  by a blank, so long as the defined entries in each row and in each column remain contiguous.
\end{proposition}

\begin{proposition}
\label{prop:duplicate}
If a -by- matrix  is (partial) Monge, then the -by- matrix resulting by replacing any row of  by two identical copies of that row is also (partial) Monge. An analogous statement holds for duplicating any column of . 
\end{proposition}

\begin{theorem}
\label{thm:reduction}
For any set of  integers , there exists a data structure of size  returning any 
in  time, where  is a Monge matrix such that the predecessor of  can be found using  time and one
submatrix maximum query on .

\end{theorem}

\begin{proof}


We partition the universe  into  parts . The -th part 
defines a Monge matrix  consisting of  rows and  columns. The first and the last row are artificial, and others encode the
elements of .
The idea is to encode the predecessor of  by the
maximum element in the -th column
of . 
 We first describe
how these matrices are defined, and then show how to stack them together.

Consider any . Every element in  has a unique
corresponding row in .
Let , so that 
and  for all , and also define . We
describe an incremental construction of .
For technical reasons, we start with an artificial top row containing . Then we add the rows corresponding
to . The row corresponding to  consists of three parts. The middle part starts at the
-th column, ends at the -th column, and contains only 's. The elements in the left part 
increase by  and end with  at the -th column, similarly
the elements in the right part (if any) start with  at the
-th column and decrease by .
Formally, the -th element of the -th row, denoted , is defined as follows.



Finally, we end with an artificial bottom row containing .
See Figure~\ref{fig:reduction} for an example. We need to argue that every  is Monge. By
Proposition~\ref{prop:adjacent condition}, it is enough to consider every pair of adjacent rows  there.
Define  and similarly . To prove that  is Monge, it is
enough to argue that  for all . By construction, both  and  are of the form
, and all 's in  are on the right of all 's in .
Therefore,  is Monge.

Now one can observe that the predecessor of  can be found by looking at the -th column
of . We check if , and if so return the predecessor of  in the whole .
This can be done in  time and  additional space by explicitly storing  and its predecessor for every .
Otherwise we know that the predecessor of  is  such that , and, 
by construction, we only need to find  such that the -th element of row  in  is .
This is exactly a subcolumn maximum query.

We cannot simply concatenate all 's to form a larger Monge matrix. We use Proposition~\ref{prop:adjustment}
instead. Initially, we set . Then we consider every other  one-by-one maintaining invariant
that the current  is Monge and its last row is . In every step we add the vector 
 to the current matrix , obtaining a matrix  whose last row is . By Proposition~\ref{prop:adjustment},  is Monge. 
Then we can construct the new  by appending  without its first row to .
Because the first row of  is also , the new  is also Monge. Furthermore, because we add the
same value to all elements in the same column of , answering subcolumn maximum queries on  can
be done with subcolumn maximum queries on the final .  The right side of Figure~\ref{fig:reduction} depicts
the final Monge matrix .

We need to argue that elements of  can be accessed in  time using a data structure of size . To
retrieve , first we lookup in  time the appropriate  from which it originates. This
can be preprocessed and stored for every  in  total space and allows us to reduce the question to retrieving
. Because Proposition~\ref{prop:adjustment} is applied exactly  times after appending 
to the current , then we can return . To find , we just directly use Equation~\ref{eqn:M_i},
which requires only storing  in  total space.
\qed \end{proof}



\begin{figure}[htb]
\centering
{\small
\begin{minipage}[]{0.2\linewidth}




\end{minipage}
\hspace{0.5cm}
\begin{minipage}[]{0.2\linewidth}




\end{minipage}
\hspace{2cm}
\begin{minipage}[]{0.3\linewidth}

\end{minipage}
\caption{Reduction for  and .}
\label{fig:reduction}
}
\end{figure}


\section{Data structure for partial Monge matrices}
\label{sec:partial}
Our goal in this section is to extend the solution described in Section~\ref{sec:linear} to \emph{partial}
Monge matrices. Recall that in a partial Monge matrix , for any
 and , the condition  holds only if all of 
are defined. Not all entries in  are defined, but the defined
entries in every row and every column are contiguous. Let  and  denote the first and last columns containing defined entries in the 'th row respectively. 
We assume that we know the coordinates
of at least one of the defined entries. This allows us to find all 's and 's in  time.

The following Lemma states that we can implicitly fill appropriate constants instead of the undefined (blank) entries to turn a partial Monge matrix into a full Monge matrix:
 
 
\begin{lemma}\label{lemma:filltheblanks}
The blank entries in an  partial Monge matrix  can be implicitly replaced in  time so that  becomes Monge and each  can be returned in  time.
\end{lemma}
\begin{proof}
Let  (resp. ) denote the index of the leftmost (resp. rightmost) column that is defined in row . 
Since the defined (non-blank) entries of each row and column are continuous we have that the sequence  starts with a non-increasing prefix  and ends with a non-decreasing suffix . 
Similarly, the sequence  starts with a non-decreasing prefix  and ends with a non-increasing suffix . 

We partition the blank region of  into four regions: (I) entries that are above and to the left of  for , 
(II) entries that are below and to the left of  for , 
(III) entries that are above and to the right of  for , 
(IV) entries that are below and to the right of  for .
We first describe how to replace all entries in region I to make them non-blank and
obtain a valid partial Monge matrix (whose blank entries are only in regions II, III, and IV). The remaining regions are handled in a similar manner, one after the other.

We describe our method for filling in the blank entries in region I in two steps. In the first step we show how to implicitly fill in the blanks in a lower right triangular Monge matrix so that each filled blank entry can be computed in  time. By a lower right triangular Monge matrix we mean a partial Monge square matrix with  rows and columns, such that, for all , . In the second step we explain that any -by- partial Monge matrix whose blank entries are in region I can be turned into a lower right triangular Monge matrix with at most  rows and columns. The only operations used in the transformation are duplicating rows, duplicating columns, and turning elements into blanks. We will show an  procedure for computing two tables. One specifying, for each row index , the corresponding row index in the larger  triangular matrix. The second is an analogous table for the columns indices. The lemma then follows for the blank entries in region I. The other regions are treated by reducing to the region I case, one after the other.

We now describe how to fill in the blank regions in a lower right triangular Monge matrix. Let  denote the largest absolute value of any non-blank entry in  (We can find  by applying the
algorithm of Klawe and Kleitman~\cite{KK89}). 
Intuitively, we would like to make every  in the upper left triangle very large. However,
we cannot simply assign the same large value to all of them, because then the Monge
inequality would not be guaranteed to hold if more than one of the four considered elements
belongs to the replaced part of the matrix. 
A closer look at all possible cases shows that setting all the entries of each diagonal to the same value does work. 
More precisely, we replace the blank element  with . Thus, each element in the first diagonal off the main diagonal () is set to , the elements of the second diagonal off the main diagonal are set to , etc. Note that the maximum element in the resulting matrix is . 
To prove that the resulting new matrix  is Monge, it suffices, by Proposition~\ref{prop:adjacent condition}, to show that, for all ,  .
To this end we consider the following cases:
\begin{enumerate}
\item , so all  are non-blank, and the inequality holds because 
is partial Monge.
\item , so  is blank and  are non-blank. Then\\
.
\item , so  are blank, and  is non-blank. Then\\
.
\item , so all  are blank. Then,\\ 
.
\end{enumerate}
Hence the new matrix  is indeed Monge.

Next, we describe how to turn any -by- partial Monge matrix  whose blank entries are in region I into a slightly larger lower right triangular matrix . This is done by duplicating some rows or columns of  and replacing by blanks a nonempty prefix in all but a single copy. Thus, each row  (column ) of  has at least one appearance in  in which no elements are replaced by blanks. We say that  () is mapped to such an appearance in .   Propositions~\ref{prop:undef} and~\ref{prop:duplicate} guarantee that  is partial Monge. 
For ease of presentation we describe the process as if we actually transform the  into , although in reality we only need to compute the mappings of rows and columns. 

The assumption that the blank entries are in region I implies that  and that . 
We first guarantee that the 's are strictly decreasing. We do this by iterating through the 's. If , we duplicate the column  of  (i.e., shift all columns at indices greater than  by one position, and insert a copy of column  at the vacant index ), make  blank, and mark the column currently at index  as a duplicate (the index of this column might change later if columns with smaller indices will be duplicated). This column duplication has the effect of increasing by 1 all 's for . Let  denote the matrix obtained from  at the end of this process, and let  denote the index of the first defined entry in row  of . If  does not have  columns, we insert a sufficient number of copies of first column of  to make it so. 
We construct a table  which keeps track of the mapping of columns of  to  by recording for each non-duplicate row its original index in  and its index in . Clearly, computing  and the indices 's can be done in  time without actually duplicating the columns. See Figure~\ref{fig:stretch} (middle) for an illustration.

We will further modify  and 
 use a table  to keep track of the mapping from rows of  to rows of . 
For convenience, we define , and . We iterate through the sequence . 
We add to   copies of row  of , and, for , replace the prefix of length  from the 'th copy by blanks, so only the last copy remains unchanged. We therefore set  to . Clearly, we can compute the table  in  time without actually modifying .
See Figure~\ref{fig:stretch} (right) for an illustration.

\begin{figure}[h!]
\centering
   \includegraphics[width=0.9\textwidth]{stretching}
   \caption{
   A staircase matrix  is transformed into a triangular matrix  in two steps by duplicating columns and rows. Defined entries
   are gray, undefined white, and duplicated columns/rows black.\label{fig:stretch}}
 \end{figure}

Finally, to obtain the value with which the blank entry at  should be replaced when converting  into a full Monge matrix, we return .

Regions II, III, and IV can be handled symmetrically to region I. To handle undefined entries in region II, we 
implicitly reverse the order of the rows and negate all the elements of the matrix. It is easy to verify that the resulting matrix is Monge with undefined entries in region I. We then implicitly fill in the undefined values using the method described above, negate all the elements and revert the order of rows to its original order. The transformation for region III is reversing the order of columns and negating all elements, and the transformation for region IV is reversing the order of both rows and columns.
Note that to make 
full Monge we first need to fill the blanks in region I, then calculate the new value of 
and fill the blanks in region II accordingly, and so on.
\qed \end{proof}

For subcolumn (or subrow) maximum queries, the above lemma implies that we can handle partial Monge matrices in the same bounds as full Monge matrices (i.e., the bounds of Theorem~\ref{thm:subcolumn2} and Corollary~\ref{lem:micro} also apply to partial Monge matrices). 
Upon subcolumn query (a column  and a range of rows ) we first restrict  to the defined entries in the column   and only then query the data structure. 

For submatrix queries however, this trick only works if the query range is entirely defined. In general, it does not work because the defined entries in the query range do not necessarily form a submatrix. Handling submatrix queries is therefore more complicated. Our solution is based on the following decomposition. 

\subsection{Decomposing a partial Monge matrix into staircase matrices}  

Our data structure relies on a decomposition of  into {\em staircase}
matrices. A partial matrix is staircase if the defined entries in its rows either all  begin in the first column and the s are monotone, or all end in the last column and the s are monotone. 
It is well known (cf.~\cite{AggarwalK90}) that by cutting
 along columns and rows, it can be decomposed into staircase
matrices  such that each row is covered by at most two matrices,
and each column is covered by at most three  matrices. 
For completeness, we  describe such a decomposition below.

\begin{lemma}\label{lemma:decomposition}
A partial matrix  can be decomposed into staircase
matrices  such that each row is covered by at most two matrices,
and each column is covered by at most three  matrices.
\end{lemma}
\begin{proof}

Let  and  denote the smallest and largest column index
in which an element in row  is defined, respectively. 
The fact that the defined entries of  are contiguous in both rows
and columns implies that the sequence  consists of a
non-increasing prefix and a non-decreasing suffix. Similarly, the 
sequence  consists of a
non-decreasing prefix and a non-increasing suffix. 
It follows that the rows of  can be divided into three ranges - 
a prefix where  is non-increasing and  is non-decreasing, an infix where
both  and  have the same monotonicity property, and a suffix
where  is non-decreasing and  is non-increasing.
The defined entries in the prefix of the rows can be divided into two
staircase matrices by splitting  at , the largest column where the
first row has a defined entry. 
Similarly, the defined entries in the suffix of the rows can be divided into two
staircase matrices by splitting it at , the largest column where the
last row has a defined entry. 
The defined entries in the infix of the rows form a double staircase
matrix. It can be broken into staircase matrices by dividing along
alternating rows and columns as shown in Figure~\ref{fig:partial}. 

\begin{figure}[h!]
\centering
   \includegraphics[scale=0.6]{partial2}
   \caption{
   A decomposition of a partial matrix (where the defined entries are gray and the undefined white) into staircase matrices
     (defined by solid thick black lines) and into blocks of consecutive
     columns with the same defined entries (indicated by thin vertical
     red lines).\label{fig:partial}}
 \end{figure}

It is easy to verify that, in the resulting decomposition, each row
is covered by at most two staircase matrices, and each column is covered
by at most three staircase matrices.
Also note that  every set of consecutive
columns whose defined elements are in exactly the same set of rows are covered
in this decomposition by the same three row-disjoint staircase matrices. \qed
\end{proof}

Before we use the above decomposition for our data structure, we show how it can be used  to prove that, if  is an  TM (or Monge) staircase
matrix, then the number of breakpoints of  is
. This result illustrates the use of the decomposition, it was used in the data structure of~\cite{ourICALP}, and we believe is of independent interest.


\begin{theorem}\label{lemma:partial-breakpoints}
Let  be a partial  matrix in which the defined entries in each row
and in each column are
contiguous.
If  is TM (i.e., for all  where
 are all defined, ), then the number of breakpoints of  is .
\end{theorem}
\begin{proof}
We first show that the number of breakpoints of an  TM {\em staircase} matrix is at
most .
We focus on the case where the defined entries of all rows begin in the first column and end in non-decreasing columns. In other words, for all , =1 and . The other cases are symmetric. 

A breakpoint is a situation where the maximum in column  is at row
 and the maximum in column  is at a different row .
We say that  is the departure row of the breakpoint, and  is
the  entry row of the breakpoint.
There are two types of breakpoints: decreasing (), and
increasing  ().
We show that 
(1) each row can be the entry row of at most one decreasing breakpoint, and (2) each row can be the
departure row of at most one increasing breakpoint. 
\begin{enumerate}
\item[(1)] Assume that row  is an entry row of two decreasing
  breakpoints:  One is the pair of entries  and
  the other is the pair  . We know that
  , , and wlog . 
Since the maximum in column  is in row , we have
.
However, since the maximum in column  is in row , we have
, contradicting the total monotonicity of
. Note that  is defined since  is defined.


\item[(2)]
Assume that row  is a departure row of two increasing breakpoints:
One is the pair of entries  and the other is
the pair  . We know that  and
. 
Since the maximum in column  is in row , we have
.
However, since the maximum in column  is in row , we have
, contradicting the total monotonicity of
. Note that  is defined since  is defined.
\end{enumerate} 

The above two claims prove that the number of breakpoints of a staircase matrix is at
most .
We use this fact, and the above decomposition to staircase matrices to prove an  bound for arbitrary partial matrices.


Let  denote the number of breakpoints in matrix . 
Let  denote the number of rows in .
Since each row appears in at most two s, .
The total number of breakpoints in all  s is
 since 
.

Consider now a partition of  into rectangular blocks  defined by maximal
sets of contiguous columns whose defined entries are at the same set
of rows, see Figure~\ref{fig:partial}. There are  such blocks.
Notice that the number of breakpoints of  is  (the
 term accounts for the possibility of a new breakpoint between every two
consecutive blocks). Therefore, it suffices to bound .

Consider some block . As we mentioned above, the columns of 
appear in at most three row-disjoint staircase matrices  in the decomposition of
. The column maxima of  are a subset of the column maxima of
. Assume wlog that the indices of rows covered by  are smaller than
those covered by  for every . 

The breakpoints of 
are either breakpoints of , or
breakpoints that occur when the maxima in consecutive columns of 
originate in different . However, since  is a (non-partial)  TM matrix, its column maxima are
monotone. So once a column maximum originates in , no maximum in
greater columns will ever originate in  for . It follows
that the number of breakpoints in  that are not breakpoints of
 is at most two. Since there are   blocks, 
. This completes the proof of Theorem~\ref{lemma:partial-breakpoints}.\qed \end{proof}

\subsection{The data structure}  



We begin with a weaker result (Theorem~\ref{thm:staircasesubmatrixlarge}), which is that one can answer submatrix maximum queries on  an  staircase matrix in  time
with a structure of size . We will then (Theorem~\ref{thm:staircasesubmatrix}) show how to reduce the space to ,  and finally (Theorem~\ref{thm:partialsubmatrix}) how to handle arbitrary partial Monge matrices using the decomposition into staircase matrices. 


We will need the following preliminary lemma, that follows quite easily from the persistent predecessor structure of Chan~\cite{ChanPersistent}. 

\begin{lemma}
\label{lem:dominancemaximum}
A collection  of  weighted points on an  grid can be preprocessed in  time and  space,
so that, given any , the maximum weight of a point  such that  and  can be calculated in
 time.
\end{lemma}

\begin{proof}
We use the standard geometric idea of sweeping the grid with a horizontal line while maintaining a data structure describing the current
situation. The data structure is made partially persistent so that after sweeping, given a query , we can retrieve the version
of the structure corresponding to a horizontal line passing through . Querying that version of the data structure will allow us
to answer the request. The data structure will be a predecessor structure made persistent using the result of Chan~\cite{ChanPersistent}.
See Theorem 5 of~\cite{MosheSurvey} 
for a more detailed description of a similar lemma.

Denote the points by  and their corresponding weights by . We assume that the weights are distinct.
We sweep the grid with a horizontal line starting at . The predecessor structure stores -coordinates of some of the already
seen points. Coordinate  is stored in the predecessor structure iff  and there is no 
such that ,  and . This is because otherwise the -th point is a better answer
than the -th point for any query processed using this or any future version of the data structure.
Consequently, the points whose -coordinates are stored in the predecessor structure can be arranged so that
their -coordinates are increasing and the weights decreasing. Then it follows that locating the maximum weight
of a point  such that  and  can be done by finding the successor of  in the
version of the predecessor structure corresponding to . Maintaining the structure while sweeping the grid
is also done with a predecessor search. After having seen a new point  we locate the predecessor of .
If the weight of the corresponding point is smaller than , we remove it from the structure and repeat.

A persistent predecessor search structure can be implemented in space  while keeping the query
time ~\cite{ChanPersistent}. Consequently, we can build in  time a~structure of
size  answering queries in  time. \qed
\end{proof}



\begin{theorem}
\label{thm:staircasesubmatrixlarge}
Given an  staircase Monge matrix , a data structure of size  can be constructed
in  time to answer submatrix maximum queries in  time.
\end{theorem}
\begin{proof}
Because of left-right symmetry, we can assume that the defined entries in row  
start in the first column and end in column . Notice that either  or .
Without loss of generality we will assume the latter. This is enough because we will not be explicitly using the Monge property in our
solution, except for applying Theorem~\ref{thm:submatrix2} on a copy of  (called ) where the undefined entries are
appropriately filled. 
 
We partition  into full Monge matrices using a standard method: First, create a full Monge matrix by taking the upper-left fragment  of . Then, recursively decompose the staircase  matrices created by taking the upper-right fragment  and the lower-left fragment  of . See Figure~\ref{fig:staircaseDecoposition}. It is easy to verify that
the decomposition consists of at most  full Monge matrices (called fragments). The decomposition has other useful
properties on which we elaborate further.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\textwidth]{staircaseDecoposition}
\end{center}
\caption{(a) A staircase  Monge matrix partitioned into  smaller full Monge matrices (fragments). (b) A query range  decomposed into two full Monge matrices  and  and one dominance query . (c) The dominance query as vertical and horizontal lines (the green fragment is fully inside the range and the blue and red fragment intersect the horizontal line). }
\label{fig:staircaseDecoposition}
\end{figure}

Consider a query range .
To find the maximum (defined)  over all  and  
we proceed as follows. The simple case is when the query range is fully within the defined part of . To handle this case, we apply Theorem~\ref{thm:submatrix2} on a copy of  (denoted ) where the undefined entries are appropriately (and implicitly) filled using Lemma~\ref{lemma:filltheblanks}. This allows us to do submatrix queries in  time when the query range is fully defined.
Otherwise, we decompose the query into three parts. 
The first part, which we call a \emph{dominance maximum query}, is to
find the maximum  over all  and , for
 to be defined shortly.
The other two are submatrix maximum queries fully within the defined part of  (and hence can be processed by querying the
structure built for  in  time). The decomposition is performed in  time by setting
 and choosing the smallest  such that  (which can be preprocessed for every possible
 in  space). The two submatrix maximum queries are therefore over the full Monge matrices  and  . Hence,
it is enough to focus on answering dominance maximum queries.

To answer a dominance maximum query (i.e., to find the maximum  over all  and ) we use the partition of  into full Monge matrices (fragments). Every such fragment
is either fully inside the query range, fully outside of the query range, or intersected by the query range boundary. 

\paragraph{\bf Fragments inside the query range.}
A fragment  is fully inside the query range iff
 and . This observation allows us to reduce computing the maximum
over all matrices fully inside the query to the problem defined in Lemma~\ref{lem:dominancemaximum}.
The reduction is simply that for every fragment  we create
a point  and set its weight to be the maximum inside the fragment. As a result, we
create at most  points on the  grid. 
Using Theorem~\ref{thm:submatrix} on  to create every point separately 
takes total  in the preprocessing time, so in  time
we can construct a structure of size  answering queries in  time.

\paragraph{\bf Fragments intersected by the query range.}
We are left only with finding the maximum over all fragments intersected by the boundary of our dominance
maximum query. We partition these fragments into three groups. The first consists of the single fragment
containing . The maximum there can be found with a submatrix maximum query
on  in  time.
All other fragments intersected by the boundary are either intersected by the horizontal line 
or the vertical line , but not both. We show how to find the maximum over all matrices intersected by the horizontal
line  and fully to the right of the vertical line  (the other case is symmetric). 

By the properties of our decomposition
scheme, there are at most  fragments intersected by any horizontal line, and
they can be arranged in the natural left-to-right order. For every possible horizontal line,
we store these at most  fragments in an array. For every fragment we store the coordinates
of its corresponding submatrix of  and the maximum in all of its entries below the horizontal
line. The array is additionally equipped with the maximum over all maxima in each one of its suffixes.
Such preprocessed data allows us to find the maximum over all fragments intersected by
a horizontal line  and fully on the right of a vertical line  in  time:
First, we binary search over the array stored for  to locate the leftmost fragment completely on
the right of . Then we return the stored corresponding maximum. Notice that
the binary search also allow us to locate the fragment containing . Consequently,
the whole query time is  using  space for this part
of the implementation. To guarantee  preprocessing time, we  run the 
SMAWK algorithm on every fragment in the decomposition in total  time. This gives us the maximum in every row of every fragment. This is then enough to construct all arrays in  time.
\qed \end{proof}

We now proceed to improving Theorem~\ref{thm:staircasesubmatrixlarge} so that the structure needs just linear space. The main idea
is to partition the  staircase matrix  into cells of size  and then define a new smaller 
staircase matrix  (whose entries correspond to cell-maxima in ) on which we apply Theorem~\ref{thm:staircasesubmatrixlarge}. To implement this idea
we need a number of additional auxiliary data structures, which take  space in total. 
We start with an auxiliary lemma, which will be used to provide constant-time access to entries of . 

\begin{lemma}
\label{lem:cellmaximum}
Given an  Monge matrix  partitioned into  cells, a data structure
of size  can be constructed in  time to find the maximum in a given cell in  time.
\end{lemma}

\begin{proof}
We partition  into  horizontal slices, each consisting of  rows (and all columns). Consider a single slice,
which is a  Monge matrix.
We store its breakpoints  (where ) in an atomic heap,
consequently allowing predecessor queries in  time (this is exactly how the structure from
Corollary~\ref{lem:micro} works). Additionally, similarly to Lemma~\ref{lem:submatrix}, for every 
we precompute the value of

and augment these values with a (one dimensional) range maximum data structure. Here,  denotes the row containing
the maximum element in the -th column of the slice in question. 
Using two predecessor queries
and one range maximum query, the problem of finding the maximum in a given cell
(which is fully contained in a single horizontal slice) reduces in  time to finding the maximum in at most two rows.
The total space is  and the bottleneck
in the preprocessing is computing the breakpoints for all slices.
The breakpoints of a single slice can be computed in  by
adding one row at a time, as done in the proof of
Lemma~\ref{lem:subcolumn}. In total, this  takes  total time. 

We repeat the above reasoning on the transpose of . As a result, we either already know the maximum
element, or we have isolated at most two rows and at most two columns, such that the maximum lies in one of these rows and one of these columns. This gives us at most four candidates
for the maximum, which can be retrieved and compared naively. \qed
\end{proof}

\noindent We are now ready to present our linear-space improvement to  Theorem~\ref{thm:staircasesubmatrixlarge}.

\begin{theorem}
\label{thm:staircasesubmatrix}
Given an  staircase Monge matrix , a data structure of size  can be constructed
in  time to answer submatrix maximum queries in  time.
\end{theorem}

\begin{proof}
As in the proof of Theorem~\ref{thm:staircasesubmatrixlarge}, we can assume that the defined entries in row  
start in the first column and end in column , and that  .

We partition  into cells of size  and then define a  smaller  
staircase matrix . Notice that, unlike Lemma~\ref{lem:cellmaximum},  is a staircase Monge matrix (and not a full Monge matrix). This means that there are three types of cells in : fully defined, partially defined, and fully undefined. An entry of  is defined iff its corresponding cell in  is
fully defined. In this case the entry is equal to the maximum in the corresponding
cell. The undefined entries of  are the ones corresponding to either partially defined or fully undefined cells of . 
We appropriately (and implicitly)  fill these entries using Lemma~\ref{lemma:filltheblanks} to turn ' into a full Monge
matrix , on which we apply Lemma~\ref{lem:cellmaximum}. This gives us
constant-time access to the entries of , so finally we can apply Theorem~\ref{thm:staircasesubmatrixlarge}
to preprocess it in  space and  time to answer submatrix maximum queries
in  time.

Regarding partially defined
cells, we observe that there are at most  of them. Furthermore, they can be arranged in a linear order, so that if the part of 
corresponding to the -th partially defined cell is , then
for all  either  and  or  and 
(to be more precise, we might need to declare some fully defined cells partially defined to
guarantee this property). We create a predecessor structure storing all s and a separate
predecessor structure storing all s. We also compute the maximum in every partially
defined cell and store them in an array (arranged in the aforementioned linear order) augmented
with a (one dimensional) range maximum structure. Computing the maximum in all partially defined
cells is done in  time using~\cite{KK89}.

By the same reasoning given in the proof of Theorem~\ref{thm:staircasesubmatrixlarge}, it is enough
to implement dominance maximum queries on . A dominance maximum query can be
decomposed into (i) a dominance maximum query on , which can be answered
in  time, (ii) finding the maximum inside all partially defined cells fully within
the query range, and (iii) finding the maximum inside partially defined cells intersected by the
boundaries of the query range. All partially defined cells fully within the query range create
a contiguous interval in the linear order. The range can be determined in 
using the predecessor structures storing all s and s, and then the maximum can
be found in  time with a (one dimensional) range maximum query. It remains to calculate the maximum inside partially defined cells
intersected by the boundaries of the query range. We will describe how to process all partially
defined cells intersected by the horizontal boundary. Handling the vertical boundary is symmetric.

Let the dominance maximum query be specified by . We want to compute the maximum
inside the query range and belonging to a partially defined cell intersected by the horizontal line .
All such cells create a contiguous interval in the linear order, which can be determined
with two predecessor queries in  time. In the same complexity, we can find
the leftmost such cell  which is not fully on the left of the vertical line . We decompose
the original query into a dominance maximum query inside , and the remaining
part. The remaining part starts at a left boundary of a partially defined cell and consists
of the entries at or below  in all partially defined cells to
the right of .
Consequently, the answer can be preprocessed
for every point on a left boundary of a partially defined cell using 
space and  time using~\cite{KK89}.
The bottleneck in the preprocessing is computing the maximum in every row of every
partially defined cell.

It remains to describe how to handle the dominance query in . In
other words, after constructing in  time an  size
structure, we have,  in  time, 
reduced an arbitrary dominance maximum query into a dominance maximum query inside
a single partially defined cell. This cell is a smaller  staircase
 matrix, and furthermore there are at most  such cells. By recursing on each of
 these smaller staircase matrices separately, we construct in
 additional  time an
  size structure, which reduces the
 original dominance query, in
 additional  time, into 
 a dominance maximum query inside one of  tiny 
 staircase matrix (each of them being a submatrix of the original ). By recursing again
 on every tiny staircase matrix separately,
 we construct in additional  time an  size
 structure, which reduces the original arbitrary dominance query in
 additional   time  into a dominance maximum query inside 
 an  submatrix of . Such dominance maximum
 query can be answered naively resulting in  total query time.
 \qed \end{proof}

We are now ready to prove the main theorem of this section, which is that using Theorem~\ref{thm:staircasesubmatrix} we can actually implement submatrix maximum queries on arbitrary (and not just staircase) partial Monge
matrices. The idea is to partition the partial Monge matrix into
staircase matrices, so that each row and each column belong to 
staircase matrices.  Such partitioning was used in~\cite{AggarwalK90,ourICALP} . We build the data structure of
Theorem~\ref{thm:staircasesubmatrix} on each staircase matrix in the
decomposition, and build an additional data structure for queries
spanning more than one staircase matrix. 



\begin{theorem}
\label{thm:partialsubmatrix}
Given an  partial Monge matrix , a data structure of size  can be constructed
in  time to answer submatrix maximum queries in  time.
\end{theorem}

\begin{proof}
We partition  into staircase matrices as done in the proof of Lemma~\ref{lemma:decomposition} (depicted in
Figure~\ref{fig:partial}). 
Recall that the partition divides the rows of  into three ranges. The first range
contributes two staircase matrices, the second range creates a double staircase
matrix, which is further broken into multiple staircase matrices, and the third range
contributes two staircase matrices. 
It is easy to verify that, in the resulting decomposition, each row
is covered by at most two staircase matrices, and each column is covered
by at most three staircase matrices.
Additionally, the staircase matrices contributed by the second range
can be partitioned into two \emph{collections},
such that any two matrices in the same collection are row-disjoint and
column-disjoint.




The data structure consists of the following components. We apply Theorem~\ref{thm:staircasesubmatrix} on every staircase  matrix in our partition. We also store additional
data for both collections. By left-right symmetry, we can assume that
the ranges of rows and columns of the matrices in the collection
are  and , respectively.
We create a predecessor structure storing all 's and a separate predecessor structure
storing all 's. We also compute and store the maximum inside every staircase 
matrix in the collection (this is done in total  time using the algorithm of Klawe and Kleitman~\cite{KK89}), and augment these maxima with a (one dimensional) range maximum structure.

Now consider a submatrix maximum query  . We first query the  structures built for
the staircase matrices corresponding to the first and the third range.
Next, we consider each of the two collections separately. To find the maximum  over
all  and , we use the predecessor structures
to determine in  the following values (without loss of generality,
they all exist):
\begin{enumerate}
\item  such that ,
\item  such that ,
\item  such that ,
\item  such that .
\end{enumerate}
We then query the structures built for the -th, -th,
-th, and -th staircase matrix in the collection.
Now either we have already found the maximum, or it belongs
to one of the staircase matrices fully contained in the query range. Consequently, the maximum
can be found in  time with a single (one dimensional) range maximum query.
\qed \end{proof}

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{AggarwalK90}
A.~Aggarwal and M.~Klawe.
\newblock Applications of generalized matrix searching to geometric algorithms.
\newblock {\em Discrete Appl. Math.}, 27:3--23, 1990.

\bibitem{SMAWK}
A.~Aggarwal, M.~M. Klawe, S.~Moran, P.~Shor, and R.~Wilber.
\newblock Geometric applications of a matrix-searching algorithm.
\newblock {\em Algorithmica}, 2(1):195--208, 1987.

\bibitem{AlstrupEtAl}
S.~Alstrup, G.~S. Brodal, and T.~Rauhe.
\newblock New data structures for orthogonal range searching.
\newblock In {\em 41st FOCS}, pages 198--207, 2000.

\bibitem{AFL07}
A.~Amir, J.~Fischer, and M.~Lewenstein.
\newblock Two-dimensional range minimum queries.
\newblock In {\em 18th CPM}, pages 286--294, 2007.

\bibitem{BrodalESA}
G.~S. Brodal, P.~Davoodi, M.~Lewenstein, R.~Raman, and S.~S. Rao.
\newblock Two dimensional range minimum queries and {F}ibonacci lattices.
\newblock In {\em 20th ESA}, pages 217--228, 2012.

\bibitem{BrodalDR10}
G.~S. Brodal, P.~Davoodi, and S.~S. Rao.
\newblock On space efficient two dimensional range minimum data structures.
\newblock In {\em 18th ESA}, pages 171--182, 2010.

\bibitem{BKR96}
R.~E. Burkard, B.~Klinz, and R.~Rudolf.
\newblock Perspectives of {M}onge properties in optimization.
\newblock {\em Discrete Appl. Math.}, 70:95--161, 1996.

\bibitem{ChanPersistent}
T.~M. Chan.
\newblock Persistent predecessor search and orthogonal point location on the
  word {RAM}.
\newblock {\em ACM Trans. Algorithms}, 9(3):22:1--22:22, 2013.

\bibitem{Patrascu}
T.~M. Chan, K.~G. Larsen, and M.~P{\v a}tra{\c s}cu.
\newblock Orthogonal range searching on the {RAM}, revisited.
\newblock In {\em 27th SOCG}, pages 354--363, 2011.

\bibitem{Chazelle88}
B.~Chazelle.
\newblock A functional approach to data structures and its use in
  multidimensional searching.
\newblock {\em SIAM Journal on Computing}, 17:427--462, 1988.

\bibitem{CR1989}
B.~Chazelle and B.~Rosenberg.
\newblock Computing partial sums in multidimensional arrays.
\newblock In {\em 5th SOCG}, pages 131--139, 1989.

\bibitem{DemaineLandauWeimann}
E.~D. Demaine, G.~M. Landau, and O.~Weimann.
\newblock On {C}artesian trees and range minimum queries.
\newblock {\em Algorithmica}, 68(3):610--625, 2014.

\bibitem{Munro}
A.~Farzan, J.~I. Munro, and R.~Raman.
\newblock Succinct indices for range queries with applications to orthogonal
  range maxima.
\newblock In {\em 39th ICALP}, pages 327--338, 2012.

\bibitem{FredmanW94}
M.L. Fredman and D.E. Willard.
\newblock Trans-dichotomous algorithms for minimum spanning trees and shortest
  paths.
\newblock {\em J. Comput. Syst. Sci.}, 48(3):533--551, 1994.

\bibitem{GBT84}
H.~Gabow, J.~L. Bentley, and R.E Tarjan.
\newblock Scaling and related techniques for geometry problems.
\newblock In {\em 16th STOC}, pages 135--143, 1984.

\bibitem{GawrychowskiLN14}
P.~Gawrychowski, M.~Lewenstein, and P.~K. Nicholson.
\newblock Weighted ancestors in suffix trees.
\newblock In {\em 22th ESA}, pages 455--466, 2014.

\bibitem{ourICALP}
P.~Gawrychowski, S.~Mozes, and O.~Weimann.
\newblock Improved submatrix maximum queries in {M}onge matrices.
\newblock In {\em 41st ICALP}, pages 525--537, 2014.

\bibitem{KaplanMozesNussbaumSharir}
H.~Kaplan, S.~Mozes, Y.~Nussbaum, and M.~Sharir.
\newblock Submatrix maximum queries in {M}onge matrices and {M}onge partial
  matrices, and their applications.
\newblock In {\em 23rd SODA}, pages 338--355, 2012.

\bibitem{KK89}
M.~M. Klawe and D~J. Kleitman.
\newblock An almost linear time algorithm for generalized matrix searching.
\newblock {\em SIAM Journal Discret. Math.}, 3(1):81--97, 1990.

\bibitem{Kopelowitz}
T.~Kopelowitz and M.~Lewenstein.
\newblock Dynamic weighted ancestors.
\newblock In {\em 18th SODA}, pages 565--574, 2007.

\bibitem{MosheSurvey}
M.~Lewenstein.
\newblock Orthogonal range searching for text indexing.
\newblock In {\em Space-Efficient Data Structures, Streams, and Algorithms -
  Papers in Honor of J. Ian Munro on the Occasion of His 66th Birthday}, volume
  8066, pages 267--302. Springer, 2013.

\bibitem{Nekrich}
Y.~Nekrich.
\newblock Orthogonal range searching in linear and almost-linear space.
\newblock {\em Comput. Geom.}, 42(4):342--351, 2009.

\bibitem{PT2006}
M.~P{\v{a}}tra{\c{s}}cu and M.~Thorup.
\newblock Time-space trade-offs for predecessor search.
\newblock In {\em 38th STOC}, pages 232--240, 2006.

\bibitem{YuanA10}
H.~Yuan and M.~J. Atallah.
\newblock Data structures for range minimum queries in multidimensional arrays.
\newblock In {\em 21st SODA}, pages 150--160, 2010.

\end{thebibliography}



 
\end{document}
