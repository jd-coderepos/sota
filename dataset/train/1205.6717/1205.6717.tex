\documentclass{article}
\usepackage{geometry}                \geometry{letterpaper}                   \usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{multirow}
\usepackage{color}
\usepackage[lofdepth,lotdepth]{subfig}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}


\newtheorem{theorem}{Theorem}
\newtheorem{invar}{Invariant}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{obs}{Observation}
\newtheorem{conj}{Conjecture}
\newtheorem{prop}[theorem]{Proposition}

\newcommand{\SD}[1]{{\color{red}SD: {#1}}}
\newcommand{\JM}[1]{{\color{red}JM: {#1}}}
\newcommand{\HL}[1]{{\color{blue}{#1}}}

\def\QED{\ensuremath{{\square}}}
\def\markatright#1{\leavevmode\unskip\nobreak\quad\hspace*{\fill}{#1}}
\newenvironment{proof}
  {\begin{trivlist}\item[\hskip\labelsep{\bf Proof.}]}
  {\markatright{\QED}\end{trivlist}}



\begin{document}

\title{Robust Non-Parametric Data Approximation of Pointsets via Data Reduction\thanks{Work was supported in part by the Natural Science and Engineering Research Council of Canada (NSERC)}}

\author{Stephane Durocher\thanks{University of Manitoba, Winnipeg, Canada, {\tt durocher@cs.umanitoba.ca}, {\tt alex\_{}leblanc@umanitoba.ca}, {\tt jason\_{}morrison@umanitoba.ca} and {\tt mskala@cs.umanitoba.ca}} \and Alexandre Leblanc\footnotemark[2] \and Jason Morrison\footnotemark[2] \and Matthew Skala\footnotemark[2]}



\maketitle

\begin{abstract}
In this paper we present a novel non-parametric method of simplifying
piecewise linear curves and we apply this method 
as a statistical approximation of structure within sequential data in the plane.
We consider the problem of minimizing the average length of sequences of
consecutive input points that lie on any one side of the simplified curve.
Specifically, given a sequence  of  points in the plane that determine a 
simple polygonal chain consisting of  segments, 
we describe algorithms for selecting an ordered subset 
(including the first and last points of ) that determines a second
polygonal chain to approximate ,
such that the number of crossings between the two polygonal chains is maximized,
and the cardinality of  is minimized among 
all such maximizing subsets of .
Our algorithms have respective running times 
 when  is monotonic and
 when  is an arbitrary simple polyline.
Finally, we examine the application of our algorithms iteratively in a 
bootstrapping technique to define a smooth robust non-parametric approximation of the original sequence.
\end{abstract}

\section{Introduction}

Given a simple
polygonal chain  (a \emph{polyline}) defined by a sequence of points
 in the plane, the \emph{polyline simplification problem} is
to produce a simplified polyline , where .
The polyline  represents an approximation of 
that optimizes one or more objectives evaluated as functions of  and .
For  to be \emph{simple}, the points  must be
distinct and  cannot intersect itself.

Motivation for studying polyline simplification comes from the
fields of computer graphics and cartography, 
where simplification is used to render
vector-based features such as
streets, rivers, or coastlines onto a screen or a map at appropriate
resolution with acceptable error \cite{agarwal:2002},
as well in problems involving computer animation, pattern matching and  
geometric hasing (see survey by Alt and Guibas for details \cite{alt:1999}).
Our present work removes the arbitrary parameter
previously required to describe acceptable error between  and , and
provides a simplification method that is robust to some forms of noise.

Typical polyline simplication algorithms require that ``distance'' between
two polylines be measured using a function denoted here by . 
The specific measure of interest differs depending on the focus of the
particular problem or article; however, three measures are popular:
Chebyshev error , Hausdorff distance , and Fr\'{e}chet
distance .  In informal terms, the Chebyshev error is the maximum
absolute difference between -coordinates of  and  (maximum
residual); the symmetric Hausdorff distance is the distance between the most
isolated point of  or  with respect to the other polyline; and the
Fr\'{e}chet distance is more complicated, being the shortest possible
maximum distance between two particles each moving forward along  and
.  Alt and Guibas give more formal definitions \cite{alt:1999}.  We define
a new measure of quality or similarity, to be maximized, rather than using
an ``error'' to be minimized.  Our crossing measure is a combinatorial
description of how well  approximates .  It is invariant under a
variety of geometric transformations of the polylines, and is often robust to
uncertainty in the locations of individual points.

Previous work on polyline simplification is generally divided into
four categories depending on what property
is being optimized and what restrictions
are placed on ~\cite{alt:1999}. Problems can be classified as those
that either
require an approximating polyline  having the minimum number of segments 
(minimizing ) for a
given acceptable error , or a  with minimum
error  for a given value of .  These are called min-
problems and min- problems respectively.  These two types of
problems are each further divided into ``restricted'' problems where the
points of  are required to be a subset of those in  and to include the
first and last points of  ( and ), 
and ``unrestricted'' problems, where the
points of  may be arbitrary points on the plane.  Under this classification,
the polyline simplification  we examine is a restricted min- problem
for which a subset of points of  is selected (including  and )
where the objective measure
 is the number of crossings between  and  
and an optimal simplification first
maximizes (rather than minimizing) the crossing number and then has a
minimum  given the maximum crossing number.  

While the restricted min- problems find the smallest sized approximation within a 
given error , an earlier approach was to find any approximation within the 
given error.  The cartographers Douglas and Peucker \cite{douglas:1973fk}
 developed a heuristic algorithm
where an initial (single segment) approximation was evaluated and the furthest point was then added to the simplification.  This technique remained inefficient until series of papers by Hershberger and Snoeyink  concluded that the problem could be solved in  time and linear space \cite{hershberger:1998uq}.

The most relevant previous
literature is on restricted min- problems. Imai and Iri~\cite{imai:1988fk} presented 
an early solution to the
restricted polyline simplification problem using  time and
 space.  The version they study optimizes  while maintaining
that the Hausdorff metric between  and  is less than the parameter
.  Their algorithm was subsequently improved by Melkman and
O'Rourke~\cite{melkman:1988uq} to  time and then by Chan and
Chin~\cite{chan:1992fk} to  time.  Subsequently, Agarwal and
Varadarajan~\cite{agarwal:2000kx} changed the approach from finding a
shortest path in an explicitly constructed graph to an implicit method that
runs in  time.  Agarwal and Varadarajan
used the  Manhattan and  Chebyshev metrics instead of the
previous works' Hausdorff metric.  Finally, Agarwal \emph{et al.}~study a
variety of metrics and give approximations of the min- problem in 
or  time.

Our algorithm for minimizing  while optimizing our non-parametric
quality measure requires  time when
 is monotonic in , or  time when  is a
non-monotonic simple polyline on the plane, both in  space.
The near-quadratic times are
remarkably similar to the optimal times achieved in the parametric
version of the problem using Hausdorff distance
\cite{agarwal:2002,chan:1992fk},
suggesting the possibility that the problems may have similar complexities.

In the next section, we define the crossing measure  and relate
the concepts and properties of  to previous work in both
polygonal curve simplification and robust approximation.  
In Section~\ref{sect:method}, we describe our algorithms to compute
simplifications of monotonic and non-monotonic simple polylines that
maximize .  Section~\ref{sect:monotonicresults} presents our
results in applying the method to -monotonic polylines that model 2-D
functional (e.g., measured) data and describes the use of this
simplification method to approximate ``shape'' and ``noise'' without assuming
a parametric model for either.

\section{Crossing Measure}
\label{sect:measure}

The crossing measure  is defined for a sequence of  distinct
points  and a subsequence of  distinct points  with the same first and last values:
 and .  For each  let .  To understand the crossing measure it is necessary to
introduce the idea of left and right sidedness of a point relative to a
directed line segment.  A point  is on the left side of a segment
 if the signed area of the triangle formed by the
points  is positive.  Correspondingly, 
is on the right side of the segment if the signed area is negative.  The
three points are collinear if the area is zero.

For any endpoint  of a segment in  it is possible to determine the side
of  on which  lies. Since  is a polyline using a
subset of the points defining , for every segment  there
exists a corresponding segment of  such that .  The endpoints of  are given a side
based on  and vice versa.  Two segments intersect if they share a
point.  Such a point is interior to both segments if only if both segments
change sides with respect to each other or the intersection is at an
endpoint of at least one endpoint is collinear to the other segment \cite[p. 
566]{skiena:2008uq}.  The \emph{crossing measure}  is the number
of times that  changes sides from properly left
to properly right of  due to an intersection between the polylines.  A
single crossing can be generated by any of five cases listed below (see Figure~\ref{fig:cases}):

\begin{enumerate}
  \item{A segment of  intersects  at a point distinct from any
    endpoints;}
  \item{two consecutive segments of  meet and cross at a point interior
    to a segment of ;}
  \item{one or more consecutive segments of  are collinear to the interior of a segment of  with the previous and following segments of  on opposite sides of that segment of ;}
\item{two consecutive segments of  share their common point with two consecutive segments of  and form a crossing; or }
  \item{in a generalization of the previous case, instead of being a
    single point the intersection comprises one or more sequential segments
    of  and possibly  that are collinear or identical.}
\end{enumerate}

\begin{figure}
\subfloat{
\includegraphics[scale=0.6]{case1}
}
\subfloat{
\includegraphics[scale=0.6]{case2}
}
\subfloat{
\includegraphics[scale=0.6]{case3}
}
\subfloat{
\includegraphics[scale=0.6]{case4}
}
\subfloat{
\includegraphics[scale=0.6]{case5}
}
\caption{Examples of the five cases generating a single crossing.}
\label{fig:cases}
\end{figure}

In Section~\ref{sect:xsingleseg} we discuss how to compute the crossings for the first three
cases, which are all cases where
crossings involve only one segment of .  The remaining cases involve more
than one segment of , because an endpoint of one segment of  or even
some entire segments of  are coincident with one or more segments of ;
those cases are discussed in Section~\ref{sect:xmultiplesegment}.
 	
\begin{figure}
\label{fig}
\subfloat{
\includegraphics[scale=0.6]{cross1}
}
\qquad
\subfloat{
\includegraphics[scale=0.6]{cross2}
}
\qquad
\subfloat{
\includegraphics[scale=0.6]{cross3}
}
\caption{Crossings are indicated with a square and false crossings are
  marked with a red x.  Crossing are only counted when a segment intersects
  the portion of the chain between its own endpoints.}
\end{figure}
	


In the case where the -coordinates of  are monotonic,  describes a
function  of  and  is an approximation  of that function.  
The signs of the residuals  
are computed at the -coordinates of  and are equivalent to
the sidedness described above.  The crossing number is the
number of proper sign changes in the sequence of residuals.  The resulting
simplification maximizes the likelihood that adjacent residuals would have
different signs, while minimizing the number of original data points
retained conditional on that number of sign changes.  Note that if  was
independently and identically selected at random from a distribution with
median zero, then any adjacent residuals in the sequence
 would have different signs with probability .

\subsection{Counting Crossings With a Segment}
\label{sect:xsingleseg}

To compute a simplification  with optimal crossing number for a given ,
we consider the optimal numbers of crossings for segments of  and combine
them in a dynamic programming algorithm.  Starting from a point  we
compute optimal crossing numbers for each of the  segments that start
at point  and end at some  with .  Computing all 
optimal crossing numbers for a given  simultaneously in a single pass
is more efficient than computing them for each  pair separately. 
These batched computations are performed for each  and the results used
to find .

To compute a single batch we will consider the angular order of points in
 with respect to .  Let
 be a function on the indices representing the clockwise angular
order of points  within this set, such that  for 
all  having the smallest clockwise angle measured from the vertical
line passing through , and  if and only if this
angle for  is less than or equal to the corresponding angle for .
See Figure~\ref{fig:angularchains}.  Using this
angular ordering we partition  into chains and process the
batch of crossing number problems as discussed below.

We define a \emph{chain} with respect to  to be a consecutive sequence
 with non-decreasing angular order.  That
is, either  or
, with the added
constraint that chains cannot cross the vertical ray above .  Each
segment that does cross is split into two pieces using two ``artificial''
points on the ray per crossing segment.  The points on the ``low'' segment
portions have rank  and the identically placed other points have
rank .  These points do not increase the complexities by more
than a constant factor and are not mentioned again unless specifically
required.  Processing  into its chains is done by first computing
the angle from vertical for each point and storing that information with the
points.  Then the points are sorted by angular order around  and
 is computed as the rank of  in the sorted list.  Since
this algorithm works in the real RAM model, this step can be done in
 time with linear space to store the angles and ranks.  Creating
a list of chains is then computable in  time and space by storing the indices of the
beginning and end of each chain encountered while checking points  in
increasing order from  to .  The process to identify all chains involves two steps.
First all
segments are checked to determine if they intersect with the vertical ray,
each in  time.  Such an intersection implies that the previous chain
should end and the segment that crosses the ray should be a new chain (note an artificial index of  can denote the point that crosses the vertical).  The
second check is to determine if the most recent segment has a different
angular direction from the previous segment.  If so, the previous chain ``ended''
with the previous point and the new chain ``begins'' with the current segment. 
Each chain is oriented from lowest angular order to highest angular
order.

\begin{figure}
\includegraphics[scale=0.5]{Chains}
\caption{An example of the angular order of vertices in  and the
  resulting chains}
\label{fig:angularchains}
\end{figure}
	
\begin{lemma}\label{lem:segmentcross}
	
Consider any chain  (wlog\ assume ). With respect
to  the segment  can have at most one crossing
strictly interior to .
	
\end{lemma}

\begin{proof} Three cases need to be considered.
	
{\bf Case 1:}  or . Note
that if  then no crossing can exist because at least one end (or all)
of  is collinear with  and no proper change in sidedness
can occur in this chain to generate a crossing.
		
{\bf Case 2:} , .  These
cases have no crossings with the chain because  is entirely on
one side of .  A ray exists
between either  or  that separates
 from  and thus no crossings can occur between the
segment and the chain.
		
{\bf Case 3:} .
Assume that the chain causes at least two crossings.  Pick the lowest index segment for each of the two crossings that are the fewest segments away from .
By definition there are no crossings of segments between these two segments.
Label the point with lowest index of these two segments  and
the point with greatest index .  Define a possibly degenerate cone  with a base  and rays through 
 and .  This cone, by definition, separates the segments from  to  from the remainder of the chain.  Since this sub-chain cannot circle  entirely there must exist one or more points that have a maximum (or minimum) angular index, which is a contradiction to the definition of the chain.  Hence there must be zero or one crossings only.\end{proof}
	
The algorithm for computing the crossing measure on a batch of segments
depends on the nature of .  If  is
-monotone, then the chains can be ordered by increasing -coordinates or
equivalently by the greatest index amoung the points that define them.  Then
a segment  intersects any chain  exactly once if
its -coordinates are less than  and  (i.e., Case 3 of
Lemma~\ref{lem:segmentcross}).  The algorithm maintains a modified segment
tree with one angular order interval per chain previously included, using
the modified segment tree described by van~Kreveld~\emph{et~al.}
\cite[p.~237]{berg:2008fk}.  This data structure requires  time
per insertion and  space.  Each point's crossing number is queried in
 time, with points examined in order of increasing indices. 
Once each chain's points have all been queried the chain's interval is
added.  Correctness follows from the fact that no segment considered
can have a crossing within any chain it ends, and chains that span a
point's angular order intersect once if the point is sufficiently distant
from  relative to the chain.  These facts are guaranteed by
-monotonicity and the proof of Lemma~\ref{lem:segmentcross}.
	
The problem becomes more difficult
if we assume that  is simple but not necessarily monotonic in .
While chains describe angular order quite
nicely, the non-monotone nature of  does not allow a consistent
implicit ordering of chain boundaries.  Thus queries will be of a specific
nature: for a given point , we must determine how many chains are
closer to  and have a lower maximum index than .  Note that chains do
not cross and can only intersect at their endpoints due to the
non-overlapping definition of chains and the simplicity of .  Therefore,
sweeping a ray from , initially vertical, in increasing  order
defines a partial order on chains with respect to their distance from . 
Using a topological sweep~\cite[p.~481]{skiena:2008uq} it is possible to
determine a unique order that preserves this partial ordering of chains. 
Since there are  chains and changes in ``neighbours'' defining the
partial order occur at chain endpoints, there are  edges in the partial
order and this operation requires  time to determine the events
in a sweep and  time to compute the topological ordering.  Without
loss of generality assume that the chains closest to  have a lower
topological order.

In our algorithm each chain will be labelled with two labels: the maximum
index of its defining points and the topological order.  Furthermore, each
point  will be labelled with the topological order of the chain to which 
it belongs (or the minimum of the two if it is in two chains).  A sweep in
increasing  order maintains the set of chains whose range of angular
orders properly includes the current .  Thus to query the number
of crossings of  we need to determine from the current set of
chains the number of them whose topological order is strictly less than the
chain or chains containing  and whose maximum index is less than . 
Querying the set in this way is an orthogonal range counting query in
, and such queries can be performed in  time and
 space with insertion and deletion of chains in  time per
event on an elementary pointer machine~\cite{chazelle:1988vn}.  The order of
operations is as follows: first, build the range counting structure by
inserting all chains that begin at the vertical ray from ; next, for
each unique angular order delete all segments whose maximum  order
is achieved (this maintains the proper intersection of  previously
mentioned); compute the crossing number of all points with this angular
order by querying the data structure; finally, add any new chains starting
at this .  The artificial points on the vertical are not queried. 
Correctness follows from Lemma~\ref{lem:segmentcross} and the
previous discussion.

\subsection{Crossings Due to Neighbouring Simplification Segments}
\label{sect:xmultiplesegment}

There are two cases of a crossing being generated that involve more than one segment of  and it
is these cases we address now.  Suppose that .  Then there is
an intersection between  and  at this point, and we must detect if a
change in sidedness accompanies this intersection.  Assume initially
that  does not contain any consecutively collinear segments; we will
consider the other case later.  

We begin with the non-degenerate case where  
are all distinct points. Each of the
points  and  can be in one of four locations: in the cone
left of ; in the cone right of
; on the ray defined by ; or on the ray
defined by .  These are labelled in Figure~\ref{fig:locations} as 
regions  through  respectively.  In Cases  and  it
may also be necessary to consider the location of  or
 with respect to  or .

Within the degenerate ``case'' where the points may not be unique: if   and , then any change in sidedness is handled at  and can be detected by verifying the
previous side from the polyline.  If, however, , then any change in
sidedness will be handled further along in the simplification.
	
\begin{figure}
\includegraphics{Regions}
\caption{Regions around  that determine a crossing at }
\label{fig:locations}
\end{figure}

By examining these points it is possible to assign a sidedness to
the end of  and the beginning of
.  Note that the sidedness of a point  with
respect to  can be inferred from the sidedness of  with
respect to , and that property is used in the case of
regions  and . 
The assumed lack of consecutive collinear segments requires that
 and thus Table~\ref{tab:leftright} is a
complete list of the possible cases when .
For cases involving  or  where  then the case
is labelled collinear (we discuss the consequences of this choice later).

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Entity & Categorization & Conditions \\
\hline
\multirow{8}{*}{End of } & \multirow{2}{*}{collinear (1)} &  \\
&&  \\ 
\cline{2-3}
& \multirow{3}{*}{left (2)} &  \\
& &   \\
& &   \\
\cline{2-3}
& \multirow{3}{*}{right (3)} &  \\
& &   \\
& &  \\
\hline
\multirow{8}{*}{Beginning of } & \multirow{2}{*}{collinear (1)} &  \\
&&  \\ 
\cline{2-3}
& \multirow{3}{*}{left (2)} &  \\
& &   \\
& &   \\
\cline{2-3}
& \multirow{3}{*}{right (3)} &  \\
& &   \\
& &  \\
\hline
\end{tabular}
\caption{Left, right, and collinear labels applied to beginning or end of a segment at }
\label{tab:leftright}
\end{center}
\end{table}

A single crossing occurs if and only if the end of  is on
the left or right when the beginning of  is the opposite. 
Furthermore, the end of any simplification  of  that ends
in  is labelled left or right in the same way that the end
of  is labelled.  This labelling is consistent with the
statement that the simplification last approached the polyline  from the
side indicated by the labelling.  To maintain this invariant in the
labelling of the end of polylines, if  is labelled as
collinear then the simplification  needs to have the same labelling
as .  As a basis case, the simplifications of  and
 are the result of the identity operation so they must be
collinear.  Note that a simplification labelled collinear
has no crossings.

The constant number of cases in Table~\ref{tab:leftright}, and the
constant complexity of the sidedness test, imply that we can compute the
number of crossings between a segment and a chain, and therefore the
labelling for the segment, in constant time.
Let  represent the number of extra crossings
(necessarily  or ) introduced at  by joining  and
.  We have , which highlights possibility of computing
the optimal simplification incrementally in a dynamic programming
algorithm.

It remains to consider the case of sequential collinear segments.  The
polyline  can be simplified into  by merging sequential collinear
segments, effectively removing points of  without changing its shape. 
When joining two segments where ,  and  define
the regions as before but there is no longer a guarantee regarding
non-collinearity of  or  with respect to the other
points.  The points  and  are now collinear if and only if
either of them are entirely collinear to the relevant segments of .  Our
check for equality is changed to a check for equality or collinearity.  We
examine the previous and next points of  that are not collinear to the
two segments  and .  We find such points
for every  in a preprocessing step requiring linear time and space, by
scanning the polyline for turns and keeping two queues of previous and
current collinear points.

\section{Optimal Crossing Measure Simplification}
\label{sect:method}

In this section we describe our dynamic programming approach to computing a
polyline  that is a subset of  having minimum size  conditional on
maximal crossing measure .  We compute  in
batches, as described in the previous section.  Our algorithm will maintain
the best known simplifications of  for all  and each of
the three possible labellings of the ends.  We refer to these paths as
 where  describes the labelling at :
 for collinear,  for left, or  for right.

To reduce the space complexity we do not explicitly maintain the
(potentially exponential-size) set of
all simplifications .  Instead, for each
simplification corresponding to  we maintain:
 (initially zero); the size of the
simplification found  (initially ); the
starting index of the last segment added  (initially
zero); and the end labelling of the best simplification that the last
segment was connected to  (initially zero).  The
initial values described represent the fact that no simplification is yet
known.  The algorithm begins by setting the values for the optimal identity
simplification for  to the following values (note ).

 

A total of  iterations are performed one for each  where a 
batch of segments  is each considered in a possible 
simplification ending in that segment.  Each iteration begins with the 
set of simplifications  being optimal,
with maximal values of  and
minimum size  for each of the specified
 and  combinations.  The iteration proceeds to calculate the
crossing numbers of all segments starting at  and ending at a later
index , using the
method from Section~\ref{sect:measure}.  For each of the segments 
we compute the sidedness of both the end at  () and the start at
 ().  Using  and all values of  it is possible to compute
 using just the labellings of the two
inputs (see Table~\ref{tab:etapsi}).  It is also possible to determine
the labelling of the end of the concatenated polyline
 using the labelling of the end of the previous
polyline  and the end of the additional segment  (also
shown in Table~\ref{tab:etapsi}).
 
 \begin{table}[ht]
	\begin{minipage}[b]{0.45\linewidth}\centering
	\begin{tabular}{|c|ccc|}
	\hline  \multirow{2}{*}{} & \multicolumn{3}{|c|}{} \\
	   & 1 & 2 & 3  \\  
	\hline
	 & & &  \\
	1 & 0 & 0 & 0  \\
	2  & 0 & 0 & 1  \\
	3  & 0 & 1 & 0  \\
	\hline
	\end{tabular}
	\end{minipage}
	\begin{minipage}[b]{0.45\linewidth}
	\centering
	\begin{tabular}{|c|ccc|}
	\hline  \multirow{2}{*}{} & \multicolumn{3}{|c|}{} \\
	   & 1 & 2 & 3  \\  
	\hline
	 & & & \ \\
	1  & 1 & 2 & 3  \\
	2  & 2 & 2 & 2  \\
	3  & 3 & 3 & 3  \\
	\hline
	\end{tabular}
	\end{minipage}
	\caption{Tables defining the computation of additional crossings  due to concatenation and the end labelling of the concatenated polyline }
	\label{tab:etapsi}
	\end{table}
 
With these values computed, the current value of
 is compared to
 and if the new simplification has a
greater or equal number of crossings crossings then we can compute:
 


Correctness of this algorithm follows from the fact that each possible
segment ending at  is considered before the -st iteration.  For
each segment and each labelling, at least one optimal polyline with that
labelling and leading to
the beginning of that segment must have been considered, by the inductive
assumption.  Since the number of crossings in a polyline
only depends on the crossings
within the segments and the labellings where the segments meet,
the inductive hyopthesis is maintained through the -st iteration.
It was also trivially true in the basis case .
With the exception of computing the crossing number for all of the segments,
the algorithm requires  time and space to update the remaining
information in each iteration.  The final post-processing step is to determine
,
finding the simplification that has the best crossing number.
We use the  and  information to reconstruct
 in  remaining time.

The algorithm requires  space in each iteration and 
time per iteration to compute crossings of each batch of segments dominates
the remaining time per iteration.  Thus for simple polylines
 is computable in  time and
 space and for monotonic polylines it is computable in 
time and  space.

\section{Results and Smooth Shape Approximation}
\label{sect:monotonicresults}

Our goal was to approximate shape (and noise) in a parameterless fashion.  In
this section we present results of applying the simplification to monotonic
data with and without noise.  We then describe how we perform a
parameterless \emph{smooth boostrap-like} operation, and give results for the
median and the 95\% confidence intervals (i.e.,~error approximations).  We
conclude by showing the results applied to a spectrum acquired from a
Fourier transformed infrared microscope.

Our first point set is given by  for 101 equally
spaced points .  The maximal-crossing simplification for this
point set has 5 points and 7 crossings.  We generated a second point set by
adding standard normal noise generated in \textsc{Matlab} with
\textit{randn} to the first point set.  The maximal-crossing simplification
of the data with standard normal noise has 19 points and a crossing number
of 54.  We generated a third point set from the first by adding
heavy-tailed noise consisting of standard normal noise for 91 data points
and standard normal noise multiplied by ten for the remaining ten points. 
The maximal-crossing simplification of the signal contaminated by
heavy-tailed noise has 20 points and a crossing number of 50.  These results
are shown in Figure~\ref{fig:cleanresults}.
	
\begin{figure}
\subfloat{
	\includegraphics[scale=0.32]{clean}
}
\qquad
\subfloat{
	\includegraphics[scale=0.32]{normal}
}
\qquad
\subfloat{
	\includegraphics[scale=0.32]{heavytailed}
}
\caption{The optimal crossing path for , without
noise, with standard normal noise and with heavy-tailed (mixed gaussian)
noise.}
\label{fig:cleanresults}
\end{figure}

As can be seen in Figure~\ref{fig:cleanresults}, the crossing-maximization
procedure gives a much closer approximation to the signal when there is some
nonzero amount of noise present to provide opportunities for crossings.  We
might expect that in the case of a clean signal, we could obtain a more
useful approximation by artificially adding some noise before computing our
maximal-crossing polyline.  However, to do so requires choosing an
appropriate distribution for the added noise, and we wish to keep our
procedure parameterless.

The residuals between the data and the optimal crossing approximation form a good
first approximation of the noise within the data.  If these residuals are not
zero-centered then their median should be subtracted to provide a
zero-centered distribution.  We can take the original data points and
subtract, at each point, a value selected uniformly at random with
replacement from the zero-centered residuals.  Then by finding the
maximal-crossing polyline of the resulting modified data, we obtain a
\emph{noise-based approximation}.  We repeat this procedure for different random
selections of which residuals to apply to which data points.  This
\emph{smooth bootstrap-like} approach is similar to ``smooth bootstrap''
estimation, which normally would use a parameter-based model of the error
\cite{wilcox:2010fk}.  Our procedure is parameterless.  Repeated evaluation
of noise based approximations produce multiple  values for all  values. 
Using the median of these and finding the 5th and 95th percentiles results
in an approximation of signal and noise after relatively few iterations.  Results
from 90 iterations of this calculation applied to a Fourier transformed
infrared spectrum are shown in Figure~\ref{fig:ftir}.

\begin{figure}
\subfloat{
		\includegraphics[scale=0.5]{bootstrap1}
	}
	\qquad
	\subfloat{
		\includegraphics[scale=0.5]{bootstrap2}
	}
\caption{The data is in black (with crosses) and contains 1610 data points), the median bootstrap approximation is in red and the green lines are the bootstrapped approximations of the 5 and 95 percentiles.}
\label{fig:ftir}
\end{figure} 


\section{Discussion and Conclusions}
\label{sect:discconc}

The optimal crossing measure simplification is robust to small changes
of - or -coordinates of any  when the points are in general
position. This robustness can be seen by considering that the crossing
number of every simplification depends on the arrangement of lines
induced by the line segments, and any point in general position (by
definition) can be moved some  without affecting the
combinatorial structure of the arrangement. The simplification is also
invariant under affine transformations because these too do not modify
the combinatorial structure of the arrangement. In the case of
-monotonic polylines, the simplification possesses another useful
property: the more a point is an outlier, the less likely it is to be
included in the simplification. In the limit, increasing the
-coordinate of any point  to infinity (-monotonicity
remains unchanged) will remove  from the simplification. That is,
if  is initially included in the simplification, then once 
moves sufficiently upward, the two segments of the simplification
adjacent to  cease to cross any input segments in .

We discuss an additional improvement achievable by bounding sequence lengths. 
If a parameter  is chosen in advance such that we require that the
longest segment considered can span at most  vertices, then with the
appropriate changes the algorithm can find the minimum sized simplification
conditional on maximum crossing number and having a longest segment of
length at most  in  time for simple polylines or 
time for monotonic polylines, both with linear space.  Since long line
segments tend to be rare in good simplifications, we can set  to a relatively
small value and still obtain good simplification results while significantly
improving speed.

Finally, this research presents a method to approximate the shape and noise
without an implied model for the data nor a need for parameters. 
Application of the shape and noise approximation in the monotonic (functional)
data case has shown promising results when used in conjunction with the
bootstrap method described here.
	
\bibliographystyle{plain}
\bibliography{DataSimplification}

\end{document}  
