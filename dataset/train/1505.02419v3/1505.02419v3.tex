

\documentclass[11pt,a4paper]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{multirow}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{ amssymb }
\usepackage{array}
\usepackage{todonotes}
\usepackage{arydshln}
\usepackage{footnote}
\usepackage{paralist}





\setlength\titlebox{6.5cm}    

\newcommand{\Note}[2]{} 
\newcommand{\SideNote}[2]{} 
\newcommand{\NoteMRG}[1]{\Note{blue!40}{#1 --MRG}}   
\newcommand{\SideNoteMRG}[1]{\SideNote{blue!40}{#1 --MRG}}   
\newcommand{\NoteMY}[1]{\Note{green!40}{#1 --MY}}   
\newcommand{\SideNoteMY}[1]{\SideNote{green!40}{#1 --MY}}   
\newcommand{\NoteMD}[1]{\Note{red!40}{#1 --MD}}   
\newcommand{\SideNoteMD}[1]{\SideNote{red!40}{#1 --MD}}   

\newcommand{\secref}[1]{\S~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\fct}{\textsc{fcm}}
\newcommand{\GoldSet}{{ST}}
\newcommand{\bx}{\mathbf{x}}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}

\newcommand{\feat}[1]{\texttt{{\scriptsize #1}}} 

\newcommand{\featnormal}[1]{\texttt{{#1}}} \newcommand{\vc}[1]{\boldsymbol{#1}}

\newcommand{\CommentForSpace}[1]{}

\title{Improved Relation Extraction with\\Feature-Rich Compositional Embedding Models}

\author{
Matthew R. Gormley \and Mo Yu\thanks{Gormley and Yu contributed equally.} \and  Mark Dredze\\
Human Language Technology Center of Excellence\\
 Center for Language and Speech Processing \\
 Johns Hopkins University, Baltimore, MD, 21218 \\
Machine Intelligence and Translation Lab\\
Harbin Institute of Technology, Harbin, China \\
\texttt{gflfof@gmail.com, \{mrg, mdredze\}@cs.jhu.edu} \\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Compositional embedding models build a representation (or embedding)
  for a linguistic structure based on its component word embeddings.
  We propose a Feature-rich Compositional Embedding Model (\fct{}) for relation extraction that is expressive,
  generalizes to new domains, and is easy-to-implement. The key idea is to
  combine both (unlexicalized) hand-crafted features with 
  learned word embeddings. The model is able to directly tackle the difficulties met by
  traditional compositional embeddings models, such as handling
  arbitrary types of sentence annotations and utilizing global
  information for composition.  We test the proposed model on two relation
  extraction tasks, and demonstrate that our model outperforms both 
  previous compositional models and traditional feature rich models
  on the ACE 2005 relation extraction task, and the SemEval 2010 relation
  classification task. The combination of our model and a log-linear classifier
  with hand-crafted features gives state-of-the-art results.
  We made our implementation available for general use\footnote{https://github.com/mgormley/pacaya}.
\end{abstract}









\newcommand{\colorA}[1]{{\color{BrickRed}#1}}
\newcommand{\colorB}[1]{{\color{Orange}#1}}
\newcommand{\colorC}[1]{{\color{Green}#1}}

\section{Introduction}
Two common NLP feature types are lexical properties of words and unlexicalized
linguistic/structural interactions between words. Prior work on
relation extraction has
extensively studied how to design such features by combining
\emph{discrete} lexical properties (e.g. the identity of a word, its
lemma, its morphological features) with aspects of a word's linguistic
context (e.g. whether it lies between two entities or on a dependency
path between them). While these help learning, they make generalization to unseen words
difficult. An alternative approach to capturing lexical information relies on 
\emph{continuous} word embeddings\footnote{Such embeddings
  have a long history in NLP, including term-document frequency
  matrices and their low-dimensional counterparts obtained by linear
  algebra tools (LSA, PCA, CCA, NNMF), Brown clusters, random
  projections and vector space models. Recently, neural networks / deep
learning have provided several popular methods for obtaining such
embeddings.} as representative of words but generalizable to new words.
Embedding features have improved many tasks,
including NER, chunking, dependency parsing, semantic role
labeling, and relation extraction
\cite{miller_name_2004,turian2010word,koo_simple_2008,roth_composition_2014,sun_semi-supervised_2011,plank_embedding_2013,nguyen_employing_2014}.
Embeddings can capture lexical information, but alone they are
insufficient: in state-of-the-art systems, they are used
alongside features of the broader linguistic context.

In this paper, we introduce a compositional model that combines unlexicalized
linguistic context and word embeddings for relation extraction, a task in which contextual feature
construction plays a major role in generalizing to unseen data.
Our model allows for the composition of embeddings with arbitrary linguistic structure, as
expressed by hand crafted features.
In the following sections, we begin with a
precise construction of compositional embeddings using word embeddings
in conjunction with unlexicalized features. Various feature
sets used in prior work
\cite{turian2010word,nguyen_employing_2014,hermann-EtAl:2014:P14-1,roth_composition_2014}
are captured as special cases of this construction.
Adding these
compositional embeddings directly to a standard log-linear model
yields a special case of our full model.
\begin{savenotes}
\begin{table*}[htbp]
\centering
\small
\begin{tabular}{|p{.03cm}l|c|c|c|}
\hline
& \bf Class & \bf M & \bf M & \bf Sentence Snippet\\
\hline
(1) & ART(M,M) & a man & a taxicab & \textit{A  man \colorA{driving} what
appeared  to be a taxicab} \==
e_{x} = \sum_{i=1}^n  f_{w_i} \otimes e_{w_i}

P(y | \bx; T, \mathbf{e}) = \frac{ \exp\left( \sum_{i=1}^n T_y \odot \left( f_{w_i} \otimes e_{w_i} \right) \right)  }{ Z(\bx) }
\label{eq:fct}

p_{\text{\fct{}+loglin}}(y | x) = \frac{1}{Z} p_{\fct{}}(y|x) p_{\text{loglin}}(y|x)
  \label{eq:hybrid}  

\ell(D;T,\mathbf{e})  = \sum_{(\bx, y)\in D} \log P(y |
\bx; T,\mathbf{e})

\frac{\partial \ell}{\partial \mathbf s} =\left[ \left(
    I[y'=y]-P(y' | \bx;T,\mathbf{e}) \right)_{1 \le y \le L}
\right]^T, \nonumber

\frac{\partial \ell}{\partial T_{y'}} = \left( I[y=y']-P(y' | \bx; T,\mathbf{e}) \right) \cdot 
\sum_{i=1}^n  f_{w_i} \otimes e_{w_i}. \nonumber

\frac{\partial \ell}{\partial e_w} 
&= \sum_{i=1}^n \left(\left( \sum_y \frac{\partial \ell}{\partial \mathbf s_y} T_y\right) \cdot {f_i}  \right) \cdot  I[w_i= w] . \nonumber



As is common in deep learning, we
initialize these embeddings from an neural
language model and then \emph{fine-tune} them for our supervised task.
The training process for the hybrid model (\secref{sec:hybrid}) is
also easily done by backpropagation since each sub-model has separate
parameters.



\section{Experimental Settings}
\label{sec:exp_setting}


\paragraph{Features}

Our \fct{} features (\tabref{tab:fea}) use a feature vector  over the word , the two target
entities , and their dependency path.
Here  are the indices of the two head words of
,
 refers to the Cartesian product between two sets,  and
 are entity types (named entity tags for ACE 2005 or WordNet supertags for SemEval 2010) of the head
words of two entities, and  stands for the empty feature.
 refers to the conjunction of two elements.
The \featnormal{In-between} features indicate whether a word  is in
between two target entities, and the \featnormal{On-path} features indicate
whether the word is on the dependency path, on which there is a set of words , between the two entities. 




We also use the target entity type as a feature.
Combining this with the basic features results in more powerful compound features,
which can help us better 
distinguish the functions of word embeddings for predicting certain relations. For example, if 
we have a person and a vehicle, we know it will be more likely that they have an 
\textit{ART} relation. For the \textit{ART} relation, we introduce a corresponding 
weight vector, which is closer to lexical embeddings similar to the embedding of ``drive''.

All linguistic annotations needed for features (POS, chunks\footnote{Obtained from the constituency parse
using the CONLL 2000 chunking converter (Perl script).}, parses) are from Stanford CoreNLP \cite{manning-EtAl:2014:P14-5}. 
Since SemEval does not have gold entity types we
obtained WordNet  and named entity tags using 
\newcite{ciaramita-altun:2006:EMNLP}.
For all experiments we use 200-d word embeddings trained on the NYT
portion of the Gigaword 5.0 corpus \cite{parker2011english}, with word2vec 
\cite{mikolov2013distributed}. We use the CBOW model
with negative sampling (15 negative words). We set a window size =5, and 
remove types occurring less
than 5 times.









\begin{table}[tb]
\centering
\scriptsize
\begin{tabular}{|l|c|}
\hline
\bf Set & \bf Template\\
\hline
\feat{HeadEmb} &  \\
& ( is head of ) \\
\hline
\feat{Context} &   (left/right token of ) \\
&   (left/right token of ) \\
\hline
 \feat{In-between}  &  (in between ) \\
 & \\
 \hline
 \feat{On-path} &  (on path)  \\
 &  \\
 \hline
\end{tabular}
\vspace{-0.1 in}
\caption{Feature sets used in \fct.}
\label{tab:fea}
\vspace{-0.1 in}
\end{table}



\paragraph{Models}

We consider several methods. 
(1) \fct{} in isolation without fine-tuning.  
(2) \fct{} in isolation with fine-tuning (i.e. trained as a log-bilinear model).
(3) A log-linear model 
with a rich binary feature set from
\newcite{sun_semi-supervised_2011} (Baseline)---this consists of all
the baseline features of \newcite{zhou_exploring_2005} plus several additional
carefully-chosen features that have been highly tuned for ACE-style
relation extraction over years of research. We exclude the Country
gazetteer and WordNet features from \newcite{zhou_exploring_2005}.
The two remaining methods are hybrid models that integrate \fct{} as a submodel within the
log-linear model (\secref{sec:hybrid}). We consider two 
combinations.
(4) The feature set of \newcite{nguyen_employing_2014} obtained by using the embeddings of
heads of two entity mentions (+HeadOnly). 
(5) Our full \fct\ model (+\fct).
All models use L2 regularization tuned on dev data. 


\subsection{Datasets and Evaluation}

\paragraph{ACE 2005}
We evaluate our relation extraction system on the English portion of the
ACE 2005 corpus \cite{walker_ace_2006}.\footnote{Many relation extraction systems
  evaluate on the ACE 2004 corpus \cite{mitchell_ace_2005}. Unfortunately, the
  most common convention is to use 5-fold cross validation,
  treating the entirety of the dataset as both train \emph{and}
  evaluation data. 
  Rather than continuing to overfit this data by perpetuating
  the cross-validation convention, we instead focus on ACE 2005.}
There are 6 domains: Newswire (\texttt{nw}), Broadcast
Conversation (\texttt{bc}), Broadcast News (\texttt{bn}), Telephone
Speech (\texttt{cts}), Usenet Newsgroups (\texttt{un}), and Weblogs
(\texttt{wl}). Following prior work
we focus on the domain adaptation
setting, where we train on one set
(the
union of the news domains (\texttt{bn}+\texttt{nw}),
tune hyperparameters on a dev domain (half of
\texttt{bc}) and evaluate on the remainder (\texttt{cts}, \texttt{wl}, and the remainder of
\texttt{bc}) \cite{plank_embedding_2013,nguyen_employing_2014}.
We assume that 
gold entity spans
\emph{and} types are available for train and test. We use all
pairs of entity mentions to yield 43,518 total relations in the
training set. 
We report precision, recall, and F1 for relation
extraction.
While it is not our focus, for completeness we include results with
unknown entity types following \newcite{plank_embedding_2013} (Appendix 1).


\paragraph{SemEval 2010 Task 8}
We evaluate on the SemEval 2010 Task 8
dataset\footnote{\tiny{\url{http://docs.google.com/View?docid=dfvxd49s_36c28v9pmw}}} \cite{Hendrickx:EtAl:10}
to compare with other compositional models and highlight the advantages of \fct{}.
This task is to determine
the relation type (or no relation) between two entities in a
sentence.
We adopt the setting of
\newcite{socher-EtAl:2012:EMNLP-CoNLL}. 
We use 10-fold cross validation on the training data to select hyperparameters
and do regularization by early stopping.
The learning rates for \fct\ with/without
fine-tuning are 5e-3 and 5e-2 respectively.  We report macro-F1 and
compare to previously published results.


\section{Results}
\label{sec:exp}

\paragraph{ACE 2005}
\label{ssec:res_ace}

\begin{table*}[t]
\centering
\small
\begin{tabular}{|p{.1cm}l|c|c|c|c|c|c|c|c|c|c|}
\hline
& \multirow{2}{*} & \multicolumn{3}{|c|}{\bf bc} &
 \multicolumn{3}{|c|}{\bf cts} & \multicolumn{3}{|c|}{\bf wl} & \bf Avg.\\
\cline{3-11}
& \bf Model & \bf P & \bf R & \bf F1& \bf P & \bf R & \bf F1& \bf P &
\bf R & \bf F1 & \bf  F1\\
        \hline
(1) & \fct\ only (ST) & 66.56 & \textbf{57.86} & 61.90 & 65.62 & 44.35 & 52.93 & 57.80 & 44.62 & 50.36 & 55.06 \\
      (3) & Baseline (ST) & \textbf{74.89} & 48.54 & 58.90 & 74.32 & 40.26 & 52.23 & 63.41 & 43.20 & 51.39 & 54.17 \\
      (4) & + HeadOnly (ST)& 70.87 & 50.76 & 59.16 & 71.16 & 43.21 & 53.77 & 57.71 & 42.92 & 49.23 & 54.05 \\
       (5) & + \fct\ (ST) & 74.39 & 55.35 & \textbf{63.48} & \textbf{74.53} & \textbf{45.01} & \textbf{56.12} & \textbf{65.63} & \textbf{47.59} & \textbf{55.17} & \textbf{58.26} \\
        \hline
\end{tabular}
\caption{Comparison of models on ACE 2005 out-of-domain test sets. 
Baseline + HeadOnly is our reimplementation
of the features of
\newcite{nguyen_employing_2014}.
}
\label{tab:tmp_sub_results}
\end{table*}






Despite \fct{}'s (1) simple feature set, it is competitive with the log-linear
baseline (3) on out-of-domain test sets (\tabref{tab:tmp_sub_results}).
In the typical gold entity spans and types setting, both
\newcite{plank_embedding_2013} and 
\newcite{nguyen_employing_2014} found that they were unable to obtain improvements
by adding embeddings to baseline feature sets. By contrast, we
find that on all domains the combination baseline + \fct\ (5) obtains the highest F1
and significantly outperforms the other baselines, {\em yielding the best reported
results for this task.}
We found that fine-tuning of embeddings (2) did not yield improvements on our
out-of-domain development set, in contrast to our results below for 
SemEval. We suspect this is
because fine-tuning allows the model to overfit the training domain,
which then hurts performance on the unseen ACE test domains.
Accordingly, \tabref{tab:tmp_sub_results} shows only
the log-linear model.

Finally, we highlight an important contrast between \fct{} (1) and
the log-linear model (3): the latter uses over 50
feature templates based on a POS tagger, dependency parser,
chunker, and constituency parser. \fct\ uses only a 
dependency parse
but still obtains better results (Avg. F1).

\begin{table*}[tb]
\centering
\small
\begin{tabular}{|l|l|c|}
\hline
\bf Classifier & \bf Features & \bf F1\\
\hline
SVM \cite{rink-harabagiu:2010:SemEval}  &POS, prefixes, morphological, WordNet, dependency parse,
 & \multirow{3}{*}{82.2} \\
(Best in SemEval2010)	&Levin classed, ProBank, FrameNet, NomLex-Plus, &\\
&Google n-gram, paraphrases, TextRunner&\\
\hline
RNN &  word embedding, syntactic parse & 74.8\\
RNN + linear&  word embedding, syntactic parse, POS, NER, WordNet & 77.6 \\
\hline
 MVRNN  & word embedding, syntactic parse & 79.1\\
 MVRNN + linear& word embedding, syntactic parse, POS, NER, WordNet & 82.4\\
 \hline
 CNN \cite{zeng-EtAl:2014:Coling}  & word embedding, WordNet & 82.7\\\hline
 CR-CNN (log-loss) & word embedding & 82.7\\ 
 CR-CNN (ranking-loss) & word embedding & \textbf{84.1}\\
 \hline
  RelEmb (word2vec embedding) & word embedding & 81.8\\
 RelEmb (task-spec embedding) & word embedding & 82.8\\
  RelEmb (task-spec embedding)  + linear & word embedding, dependency paths, WordNet, NE & 83.5\\
 \hline
  DepNN & word embedding, dependency paths& 82.8\\
  DepNN  + linear & word embedding, dependency paths, WordNet, NER & 83.6\\
 \hline
 \multirow{2}{*}{(1) \fct\ (log-linear)} & word embedding, dependency parse, WordNet & 82.0\\
 & word embedding, dependency parse, NER & 81.4\\
 \hline
 \multirow{2}{*}{(2) \fct\ (log-bilinear)} & word embedding, dependency parse, WordNet &  {82.5}\\
 & word embedding, dependency parse, NER & 83.0\\
 \hline
 \multirow{2}{*}{(5) \fct\ (log-linear) + linear (Hybrid)} & word embedding, dependency parse, WordNet & 83.1\\
 & word embedding, dependency parse, NER & \textbf{83.4}\\
\hline
\end{tabular}
\caption{Comparison of \fct{} with previously published results for SemEval 2010 Task 8.}
\label{tab:res}
\end{table*}



\paragraph{SemEval 2010 Task 8}
\label{ssec:res_semeval}











Table \ref{tab:res} shows \fct\ compared to the best reported results
from the SemEval-2010 Task 8 shared task and several other 
compositional models. 

For the \fct\ we considered two feature sets. We found that using NE
tags instead of WordNet tags helps with fine-tuning but hurts
without. This may be because the set of WordNet tags is larger making
the model more expressive, but also introduces more parameters.  When
the embeddings are fixed, they can help to better distinguish
different functions of embeddings. But when fine-tuning, it becomes
easier to over-fit. Alleviating over-fitting is a subject for future
work (\secref{sec:conclusion}).

With either WordNet or NER features, \fct{} achieves better
performance than the RNN and MVRNN. With NER features and fine-tuning,
it outperforms a CNN \cite{zeng-EtAl:2014:Coling} and also the
combination of an embedding model and a traditional log-linear model
(RNN/MVRNN + linear) \cite{socher-EtAl:2012:EMNLP-CoNLL}.
As with ACE, \fct{} uses less linguistic resources than many close competitors \cite{rink-harabagiu:2010:SemEval}.

We also compared to concurrent work on enhancing the compositional models with task-specific 
information for relation classification, including
\newcite{hashimoto2015task} (RelEmb), which trained task-specific word embeddings,
and \newcite{santos2015classifying} (CR-CNN), which proposed a task-specific ranking-based loss function.
Our Hybrid methods (\fct{} + linear) get comparable results to theirs. 
Note that their base compositional model results without any task-specific enhancements,
i.e. RelEmb with word2vec embeddings and CR-CNN with log-loss, are still lower than
the best \fct{} result. We believe that \fct{} can be also improved with
these task-specific enhancements, e.g. replacing the word embeddings to the task-specific
ones from \cite{hashimoto2015task} increases the result to 83.7\% (see \S\ref{ssec:emb_res} for details).
We leave the application of ranking-based loss to future work.

Finally, a concurrent work 
\cite{liu-EtAl:2015:ACL-IJCNLP} proposes DepNN, which builds representations for the 
dependency path (and its attached subtrees) between two entities by applying recursive
and convolutional neural networks successively.
Compared to their model, our \fct{} achieves comparable results.
Of note, our \fct{} and the RelEmb are also the most efficient models among all above
compositional models since they have linear time complexity with respect to the dimension of 
embeddings.



\subsection{Effects of the embedding sub-models}
We next investigate the effects of different types
of features on \fct{} using ablation tests on ACE 2005 (\tabref{tab:fct_sub_results}.) We focus on \fct{} alone with the feature templates of \tabref{tab:fea}.
Additionally, we show results of using \emph{only} the head embedding features from
\newcite{nguyen_employing_2014} (HeadOnly).
Not surprisingly, the HeadOnly model performs poorly (F1 score = 14.30\%),
showing the importance of our rich binary feature set.
Among all the features templates, removing \featnormal{HeadEmb} results in the
largest degradation.
The second most important feature template is \featnormal{In-between}, while
\featnormal{Context} features have little impact.
Removing all entity type features () does 
significantly worse than the full model,
showing the value of our entity type features.
\SideNoteMD{Someone in the above section we need to say that we now
  have the best results on ACE.}
\SideNoteMRG{For now, we're sticking with `we attain state-of-the-art'
  in the intro. Nothing stronger.}
\SideNoteMY{details(MD)}


\begin{table}[tbp]
\centering
\small
\begin{tabular}{|l|c|c|c|}
\hline
\bf Feature Set & \bf Prec & \bf Rec & \bf F1\\
\hline
\multirow{1}{*}{HeadOnly} &31.67 & 9.24 & 14.30\\
\hdashline
\fct\ & 69.17 & \bf 56.73 & \bf 62.33\\
\quad-\featnormal{HeadEmb} & 66.06 & 47.00 & 54.92 \\
 \quad-\featnormal{Context}  & 70.89 & 55.27 & 62.11\\
 \quad-\featnormal{In-between}  & 66.39 & 51.86 & 58.23 \\
 \quad-\featnormal{On-path} &  69.23 & 53.97 & 60.66 \\
 \hdashline
 \fct-\featnormal{EntityTypes} & \bf 71.33 & 34.68 & 46.67\\
\hline
\end{tabular}
\caption{Ablation test of \fct\ on development set.}
\label{tab:fct_sub_results}
\end{table}


\subsection{Effects of the word embeddings}
\label{ssec:emb_res}
Good word embeddings are critical for both \fct{} and other compositional models. 
In this section, we show the results of \fct{} with embeddings used to
initialize other recent state-of-the-art models. 
Those embeddings include the 300- baseline embeddings trained on English Wikipedia (w2v-enwiki-d300) and the 100- task-specific embeddings (task-specific-d100)\footnote{
In the task-specific setting, \fct{} will represent entity words and context words with separate sets of embeddings.}
from the RelEmb paper \cite{hashimoto2015task}, the 400- embeddings from the CR-CNN paper \cite{santos2015classifying}.
Moreover, we list the best result (DepNN) in \newcite{liu-EtAl:2015:ACL-IJCNLP}, which uses the same
embeddings as ours.
\tabref{tab:emb_res} shows the effects of word embeddings on \fct{}
and provides relative comparisons between \fct{} and the other state-of-the-art models.
We use the same hyperparameters and number of iterations in
\tabref{tab:res}.

The results show that using different embeddings to initialize \fct{}
can improve F1 beyond our previous results. We also find that
increasing the dimension of the word embeddings does not necessarily lead to better results due to the
problem of over-fitting (e.g.w2v-enwiki-d400 vs. w2v-enwiki-d300).
With the same initial embeddings, \fct{} usually gets better results
without any changes to the hyperparameters than the competing model,
further confirming the advantage of \fct{} at the model-level as discussed under \tabref{tab:res}.
The only exception is the DepNN model, which gets better result than \fct{} on the same embeddings.
The task-specific embeddings from \cite{hashimoto2015task} leads to the
best performance (an improvement of 0.7\%). This observation suggests that the other compositional models may also benefit from the work of \newcite{hashimoto2015task}.
 
\begin{table}[tb]
\centering
\small
\begin{tabular}{|l|l|c|}
\hline
\bf Embeddings & \bf Model & \bf F1\\
\hline
\multirow{2}{*}{w2v-enwiki-d300} &  RelEmb  & 81.8\\
 & {(2) \fct\ (log-bilinear)} & 83.4\\
\hline
\multirow{3}{*}{task-specific-d100}  & RelEmb & 82.8\\
& RelEmb+linear & 83.5\\
& {(2) \fct\ (log-bilinear)} & \bf 83.7\\
 \hline
 \multirow{2}{*}{w2v-enwiki-d400} & CR-CNN & 82.7\\
 &{(2) \fct\ (log-bilinear)} & 83.0\\
 \hline
 \multirow{2}{*}{w2v-nyt-d200} & DepNN   & 83.6\\
 & {(2) \fct\ (log-bilinear)}  & 83.0\\
 \hline
\end{tabular}
\caption{Evaluation of \fct{}s with different word embeddings on SemEval 2010 Task 8.}
\label{tab:emb_res}
\end{table}

\section{Related Work}
\label{sec:related}

\NoteMY{Move the discussion about disadvantages of Feature Engineering early: Lexicalized features are often hand designed to capture the linguistic
interactions between words. Yet, such features don't generalize to
new words not seen during training nor do they allow sharing of statistical strength between
lexically similar substructures: for example, learning that the bigram
`father of' intervening between two entity mentions is indicative of a
familial relation won't allow for generalization to the bigram `mother
of' which expresses a similar relation. Back-off features are often employed using the word's prefix,
suffix, lemma, or POS tag---but these may miss the semantic
connections required for the previous example.
For example, back-off based on POS tags cannot distinguish the different roles of ``mother of" and ``director of" on relation extraction.
Word embeddings can
assist with exactly this issue.}
\NoteMY{move forward
Lexical features, although powerful, are easy to suffer from data sparsity.
In this situation, word embeddings, which represents word in some low-dimensional
space, become a important type of back-off features---in
part because they provide a simple method of semi-supervised learning.}


\NoteMY{
Usually, a major benefit to hand designing features is that one can
easily incorporate arbitrary substructures of the input.  However, the
methods of incorporation for dense, real-valued vectors have been
somewhat limited: commonly only a short list of word properties are
conjoined with a small set of word embeddings from the sentence (see
\S~\ref{sec:related} for further discussion of this point). Certainly,
one challenge along this direction is how to compose the vectors for
multiple words (e.g. the bigram to the left of a mention) into a
feature.}

\paragraph{Compositional Models for Sentences}
In order to build a representation
(embedding) for a sentence based on its component word
embeddings and structural information, 
recent work on compositional models (stemming from the deep learning
community) has designed model structures that mimic the structure of
the input. For example, 
these models could take into account the order of the
words (as in Convolutional Neural Networks (CNNs))
\cite{collobert2011natural} or build off of an
input tree (as in Recursive Neural Networks (RNNs) or the Semantic
Matching Energy Function)
\cite{socher-EtAl:2013:EMNLP,bordes2012semantic}.
\NoteMY{
Several results (c.f. \newcite{collobert2011natural}) have suggested
that this method of \emph{learning} features (as opposed to hand
designing them) is a promising avenue for supplanting traditional
feature engineering approaches. } 

While these models work well on
sentence-level representations, the nature of their designs
also limits them to fixed types of substructures from the annotated
sentence, such as chains for CNNs and trees for RNNs. Such models
cannot capture arbitrary combinations of linguistic annotations
available for a given task, such as word order, dependency tree, and
named entities used for relation extraction.
Moreover, these approaches ignore the differences in
functions between words appearing in different roles. 
This does not suit more general substructure labeling tasks in NLP, e.g. these models cannot be directly applied to relation extraction since they will output the same result for any pair of entities in a same sentence.



\paragraph{Compositional Models with Annotation Features}
To tackle the problem of traditional compositional models, \newcite{socher-EtAl:2012:EMNLP-CoNLL} made
the RNN model specific to relation extraction tasks by working on the minimal 
sub-tree which spans the two target entities. 
However, these
specializations to relation extraction does not generalize easily to
other tasks in NLP. There are two ways to achieve such specialization
in a more general fashion:

\textit{1. Enhancing Compositional Models with Features.}
A recent trend enhances compositional models with annotation features.
Such an approach has been shown to significantly improve
over pure compositional models.
For example,
\newcite{hermann-EtAl:2014:P14-1} and 
\newcite{nguyen_employing_2014}
gave different weights to words with different syntactic context types or
to entity head words with different argument IDs.
\newcite{zeng-EtAl:2014:Coling} use concatenations of embeddings as features
in a CNN model, according to their positions relative to the
target entity mentions.
\newcite{belinkov2014exploring} enrich embeddings with linguistic features 
before feeding them forward to a RNN model.
\newcite{socher2013parsing} and \newcite{hermann2013role} enhanced RNN models by refining
the transformation matrices with phrase types and CCG super tags.


\textit{2. Engineering of Embedding Features.}
A different approach to combining traditional linguistic features and embeddings is hand-engineering features with word 
embeddings and adding them to log-linear models.
Such approaches have achieved state-of-the-art results in
many tasks including NER, chunking, dependency parsing, 
semantic role labeling, and relation extraction
\cite{miller_name_2004,turian2010word,koo_simple_2008,roth_composition_2014,sun_semi-supervised_2011,plank_embedding_2013}.
\newcite{roth_composition_2014} considered features similar to ours
for semantic role labeling.

However, in prior work both of above approaches are only able to utilize limited information,
usually one property for each word.
Yet there may be different useful properties of a word which can contribute 
to the performances of the task. By contrast, our
\fct\ can easily utilize these features without changing the model
structures. 


In order to better utilize the dependency annotations,
recently work built their models according to the dependency paths
\cite{ma-EtAl:2015:ACL-IJCNLP,liu-EtAl:2015:ACL-IJCNLP}, 
which share similar motivations to the usage of \texttt{On-path} features in our work.

\paragraph{Task-Specific Enhancements for Relation Classification}

An orthogonal direction of improving compositional models for relation classification
is to enhance the models with task-specific information.
For example,
\newcite{hashimoto2015task} trained task-specific word embeddings,
and \newcite{santos2015classifying} proposed a ranking-based loss function for relation classification.







\section{Conclusion}
\label{sec:conclusion}
We have presented \fct, a new compositional model for deriving
sentence-level and substructure embeddings from word embeddings.  Compared to existing
compositional models, \fct\ can easily handle arbitrary types of input
and handle global information for composition, while remaining
easy to implement. We have demonstrated that  \fct\  alone attains near state-of-the-art
performances on several relation extraction tasks, and in 
combination with
traditional feature based log-linear models it 
obtains state-of-the-art results. 


Our next steps in improving \fct{} focus on enhancements
based on task-specific embeddings or loss functions as in 
\newcite{hashimoto2015task,santos2015classifying}.
Moreover, as the model provides
a general idea for representing both sentences and sub-structures in language, 
it has the potential to contribute useful components to various tasks,
such as dependency parsing, SRL and paraphrasing. 
Also as kindly pointed out by one anonymous reviewer,
our \fct\ can be applied to the TAC-KBP \cite{ji2010overview} tasks,
by replacing the training objective to a multi-instance multi-label one (e.g. \newcite{surdeanu2012multi}).
We plan to explore the above applications of \fct\  in the future. 

\section*{Acknowledgments}
We thank the anonymous reviewers for their comments, and Nicholas Andrews, Francis Ferraro, and Benjamin Van Durme for their input.
We thank Kazuma Hashimoto, C{\'i}cero Nogueira dos Santos, Bing Xiang and Bowen Zhou for sharing their word embeddings and many helpful discussions.
Mo Yu is supported by the China Scholarship Council and by NSFC 61173073.

\bibliographystyle{acl}
\bibliography{acere-2015.bib}


\clearpage
\newpage



\section*{Appendix 1: Experiments on ACE 2005 where Gold Entity Types Are Unknown}
\label{sec:appendix}
\paragraph{Experimental Settings:}
For comparison with prior work \cite{plank_embedding_2013}, we (1) generate relation
instances from all pairs of entities within each sentence with three
or fewer intervening entity mentions---labeling those pairs with no
relation as negative instances, (2) use gold entity spans (but not types) at
train and test time, and
(3) evaluate on the 7 coarse relation types, ignoring the subtypes. 
In the training set, 35,990 total relations are annotated of which
only 3,658 are non-nil relations. 
We did not match the number of tokens they reported in the \texttt{cts}
and \texttt{wl} domains. Therefore, in this section we only report the results on the test set
of \texttt{bc} domain. We will leave experiments on additional domains in future work.


We run the same models as in \S\ref{ssec:res_ace} on this task. Here the \fct\ does not use entity type features.
\newcite{plank_embedding_2013} also use Brown clusters and word
vectors learned by latent-semantic analysis (LSA).
In order to make a fair comparison with their method, we also report the \fct\ 
result using Brown clusters (prefixes of length 5) of entity heads as entity types.
Furthermore, we report non-comparable settings using WordNet super-sense tags
of entity heads as types. The WordNet features were also used in their paper but not
as substitution of entity types. 
We use the same toolkit to get the WordNet tags as in \S\ref{sec:exp_setting}.
The Brown clusters are from \cite{koo_simple_2008}\footnote{\url{
http://people.csail.mit.edu/maestro/papers/bllip-clusters.gz}}.

\paragraph{Results:}
Table \ref{tab:pm13_results} shows the results under the low-resource setting.
When no entity types are available, the performance of our \fct\ only model greatly
decreases to 48.15\%, which is consistent with our observation in the ablation tests.
The baseline model also relies heavily on the entity types.
After we remove all the hand-engineering features that contain entity type information,
the performance of our baseline model drop to 40.62\%, even  lower than the 
reduced \fct\ only model.

The combination of baseline model and head embeddings (Baseline + HeadOnly) 
greatly improve the results.
This is consistent with the observation in \newcite{nguyen_employing_2014} that
when the gold entity types are unknown, information of the entity heads
provided by their embeddings will play a more important role.
Combination of the baseline and  \fct\ (Baseline + \fct)
also achieves improvement but not significantly
better than Baseline + HeadOnly.
A possible explanation is that \fct\ becomes less efficient on using
context word embeddings when the entity type information is unavailable.
In this situation the head embeddings provided by \fct\ become the dominating
contribution to the baseline model, making the model have similar behavior
as the Baseline + HeadOnly method.

Finally, we find Brown clusters can help \fct\ when entity types are unknown.
Although the performance is still not significantly better than Baseline + HeadOnly, 
it outperforms all the results in \newcite{plank_embedding_2013} as a \emph{single model},
and with the \emph{same source of features}.
WordNet super-sense tags further improves  \fct{}, and achieves the 
best reported results on this low-resource setting.
These results are encouraging since it shows  \fct\ may be more useful
under the end-to-end setting where predictions of both entity mentions and relation mentions
are required in place of predicting relation based on gold tags \cite{li-ji:2014:P14-1}. 

Recently \newcite{nguyen-plank-grishman:2015:ACL-IJCNLP} proposed a novel way of applying
embeddings to tree-kernels.
From the results, our best single model achieves comparable result with their best single system, while their
combination method is slightly better than ours.
This suggests that we may benefit more from combining the usages of multiple word representations; and we will investigate it in future work.



\begin{table}[htbp]
\centering
\small
\begin{tabular}{|l|c|c|c|c|}
\hline
\multirow{2}{*} & \multicolumn{3}{|c|}{\bf bc} \\
\cline{2-4}
\bf Model & \bf P & \bf R & \bf F1\\
        \hline
       PM'13 (Brown) &54.4&43.4&	48.3\\
       PM'13 (LSA) & 53.9	& 45.2	&49.2\\
       PM'13 (Combination) &55.3&	43.1&	48.5\\
        \hdashline
        (1) \fct\ only& 53.7 & {43.7} & 48.2  \\
        (3) Baseline & 59.4 & 30.9 & 40.6 \\
        (4) + HeadOnly & 64.9 & 41.3 & 50.5 \\
        (5) + \fct & \textbf{65.5} & 41.5 & {50.8}  \\
        \hdashline
        (1) \fct\ only w/ Brown & 64.6 & 40.2 & 49.6\\
        (1) \fct\ only w/WordNet & 64.0 & 43.2 &  51.6\\
       \hline
Linear+Emb &46.5 & \textbf{49.3} & 47.8\\
       Tree-kernel+Emb (Single) & 57.6 & 46.6 & 51.5\\
       Tree-kernel+Emb (Combination) &58.5 & 47.3 & \textbf{52.3}\\
       \hline
\end{tabular}
\caption{\small{Comparison of models on ACE 2005 out-of-domain test sets
for the low-resource setting, where the
gold entity spans are known but
entity types are unknown.
PM'13 is the results reported in
\newcite{plank_embedding_2013}.
``Linear+Emb" is the implementation of our method (4) in \cite{nguyen-plank-grishman:2015:ACL-IJCNLP}. The ``Tree-kernel+Emb" methods are the enrichments of tree-kernels with embeddings proposed by \newcite{nguyen-plank-grishman:2015:ACL-IJCNLP}.}
}
\label{tab:pm13_results}
\end{table}


\end{document}
