
\title{Realistic Blur Synthesis for Learning Image Deblurring \\
—– Supplementary Material —–} 

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{Realistic Blur Synthesis for Learning Image Deblurring}
\author{Jaesung Rim \and
Geonung Kim \and
Jungeon Kim \and
Junyong Lee \and \\
Seungyong Lee \and
Sunghyun Cho}
\authorrunning{J. Rim et al.}
\institute{
POSTECH, Pohang, Korea \\
\email{\{jsrim123,k2woong92,jungeonkim,junyonglee,leesy,s.cho\}@postech.ac.kr}}
\maketitle

\section{Statistics of the RSBlur}

\Tbl{statistical_reports} shows a statistical comparison with the RSBlur and other real-world blur datasets.
The proposed RSBlur dataset consists of 13,358 real blurred images, which make the dataset the second largest real-world dataset.
While the RealBlur~\cite{jsrim-ECCV2020} and BSD~\cite{Zhong_2020_ECCV,Zhong_2021_arxiv} datasets consist of real blurred images and ground-truth sharp images, we provide real blurred images and sequences of nine sharp images to enable analysis on the blur generation process between real blurred and synthetic blurred images. In terms of image resolution, the RSBlur dataset provides the largest resolution.
We also report the estimated noise levels using a single image noise estimation method~\cite{Chen-ICCV15} for comparing the amounts of noise in the real-world blurred datasets.


\begin{table}[h]
\centering
\setlength{\tabcolsep}{3.0pt}
\caption{Statistical comparison of real-world blur datasets. The average noise levels are estimated using a single image noise estimation method~\cite{Chen-ICCV15}.}
\label{tbl:statistical_reports}
\scalebox{1.0}{
\begin{tabular}{|cccccc|}
\hline
         & Frames & Real/Synth.         & Resolution       & Shutter (ms) & Noise \\ \hline
RealBlur~\cite{jsrim-ECCV2020} & 4,738    & Real &    & 500          & 0.4378 \\
BSD~\cite{Zhong_2020_ECCV,Zhong_2021_arxiv}      & 33,000   & Real       &    & 8, 16, 24    & 0.3404 \\ 
RSBlur   & 13,358   & Real \& Synth       &  & 100          & 0.7736 \\ \hline
\end{tabular}
}
\end{table}

\section{Real-world Deblurring Benchmark on the RSBlur}
While there exist a couple of real-world blur datasets such as RealBlur~\cite{jsrim-ECCV2020} and BSD~\cite{Zhong_2020_ECCV,Zhong_2021_arxiv},
their coverage is limited.
The real-world blurred images in the RSBlur dataset can also serve as an additional benchmark dataset that complements the existing benchmark datasets in terms of coverage.

In this section, we provide a benchmark on recent state-of-the-art deblurring methods using the RSBlur dataset to provide a basis for future deblurring research.
Using real-world blurred images of the RSBlur dataset, we train state-of-the-art deblurring methods~\cite{Tao-CVPR18,Cho_2021_ICCV,Zamir_2021_CVPR,Zamir_2022_CVPR,Wang_2022_CVPR}.
We use the source codes provided by the authors for training, and evaluate their performance using the real-blur test set of the RSBlur dataset.
Here, we briefly report the qualitative and quantitative results of the state-of-the-art methods in \Tbl{deblurring_benchmark} and in \Fig{our_system_diagram}, respectively.

\setlength{\tabcolsep}{4pt}
\begin{table}[t]
\caption{Benchmark of state-of-the-art deblurring methods on the real blurred test set of the RSBlur dataset. We trained all methods using real blurred training set of the RSBlur dataset.}
\label{tbl:deblurring_benchmark}
\centering
\scalebox{1.0}{
\begin{tabular}{|cc|}
\hline
\multicolumn{1}{|c|}{Methods}    & PSNR / SSIM      \\ \hline
\multicolumn{1}{|c|}{SRN-Deblur~\cite{Tao-CVPR18}} & 32.53 / 0.8398 \\
\multicolumn{1}{|c|}{MiMO-UNet~\cite{Cho_2021_ICCV}}  & 32.73 / 0.8457 \\
\multicolumn{1}{|c|}{MiMO-UNet+~\cite{Cho_2021_ICCV}} & 33.37 / 0.8560 \\
\multicolumn{1}{|c|}{MPRNet~\cite{Zamir_2021_CVPR}}     & 33.61 / 0.8614 \\
\multicolumn{1}{|c|}{Restormer~\cite{Zamir_2022_CVPR}}  & 33.69 / 0.8628 \\ 
\multicolumn{1}{|c|}{Uformer-B~\cite{Wang_2022_CVPR}}    & 33.98 / 0.8660 \\ \hline
\end{tabular}
}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{figs_supple/RSBlur_bench_8_v4_comp.pdf}
\caption{Qualitative comparison of state-of-the-art deblurring methods on real-world blurred images of the RSBlur test set.}
\label{fig:RSBlur_benchmark_results}
\end{figure}



\section{Details of Dual-Camera System}

In this section, we describe the details of our dual-camera system. \Fig{our_system_diagram} shows our dual-camera system and a diagram of the system.  The system consists of a mount for the lens, one beam splitter, and two camera modules with imaging sensors (Basler daA1920-160uc) so that the camera modules can capture the same scene while sharing one lens.
For the lens, we used a Samyang 10mm F2.8 ED AS NCS CS.
We installed a 5\% neutral density filter (OD 1.3 VIS, 12.5mm Dia. Non-Reflective ND Filter) in front of a camera module. For compensation of beam-splitter tolerance, a 63\% neutral density filter (0.2 OD, 25mm Dia., Precision Absorptive ND Filter) is installed in front of the other camera module. Two camera modules are installed on adjustable plates, so we can physically align the modules by adjusting the plates.

One camera module with a 5\% neutral density filter captures a blurred image with a long exposure time (0.1 seconds). The other module captures nine sharp images with a short exposure time (0.005 seconds) during the exposure time of a blurred image. The gains of the two modules are set to 0 for both. To increase the number of images and diversity of blur, we capture 20 pairs of a blurred image and a sequence of nine sharp images of the same scene. 


\begin{figure}[h]
\begin{center}
\includegraphics [width=0.95\linewidth] {figs_supple/diagram_system_supple_comp.pdf}
\end{center}
\vspace{-0.5cm}
\caption{The dual-camera system and detailed diagram of the system.}
\label{fig:our_system_diagram}
\vspace{-0.6cm}
\end{figure}


\section{Camera ISP}
\label{sec:details_ISP}


To collect our dataset, we captured all images in the camera RAW format, and converted them into the nonlinear sRGB space using a simple image signal processing (ISP) pipeline.
Our ISP consists of four steps: 1) white balance, 2) demosaicing, 3) color correction, and 4) conversion to the sRGB space using a camera response function.
We use the demosaicing method of Malvar \etal~\cite{Malvar_2004_ICASSP} for the second step and a gamma correction of standard RGB space for the fourth step.


For the white balance and color correction steps in our ISP, we utilize a color chart.
Specifically, when collecting our dataset, we captured reference images of a color chart for different scenes.
Using the reference images, we estimate the gain  for the color channel  for the white balance as:

where  is the mean intensity of the color channel  of neutral patches in the color chart.
Then, in the first step of our ISP, each color channel of a RAW image is multiplied by the corresponding gain.



For the color correction in the third step of our ISP,
we estimate a color correction matrix of the XYZ color space as:

where  is the reference XYZ color of the -th color chart patch,
and  is the measured XYZ color after white balancing and demosaicing.
 is a single scalar value for matching the brightness levels of the color chart patches and the captured patches.
 is a  color correction matrix.
We first estimate  by minimizing the mean-squared error between the color chart patches and captured patches.
Then, we estimate  by finding the least-squares solution of \Eq{color_correction_matrix} with fixed .

Once a color correction matrix  is obtained, we apply  in the third step of our ISP as follows:

where  is an RGB color of an image after demosaicing, and  is a resulting RGB color in the linear sRGB space.
 and  are matrices for color conversion between the linear sRGB and XYZ color spaces.
\Fig{example_isp} shows intermediate results of our camera ISP.

\begin{figure}[t]
\begin{center}
\includegraphics [width=1\linewidth] {figs_supple/example_isp_comp.pdf}
\end{center}
\vspace{-0.4cm}
\caption{Intermediate images of our camera ISP. All images are gamma corrected for visualization. }
\label{fig:example_isp}
\end{figure}


\section{Photometric Alignment between Camera Modules}

Due to the optical spectrum difference caused by the beam splitter and ND filters, captured images may have slight photometric misalignments. To mitigate this, we conduct photometric alignment using a color chart image after the color correction step of our camera ISP.
Specifically, we formulate the relationship between the colors of images from the two camera modules as:


where  and  are the XYZ color values of the -th color chart patch after the color correction of one camera module (C1) and the other camera module (C2), respectively.  is a  matrix for the photometric alignment. We estimate  by finding the least-squares solution of \Eq{photometric_alignment}. 
As shown in \Fig{photometric_alignment}(a)-(b), images captured by the two camera modules have color differences. After photometric alignment, the color difference is significantly reduced, as shown in \Fig{photometric_alignment}(c).


\begin{figure}[t]
\begin{center}
\includegraphics [width=1\linewidth] {figs_supple/photometric_alignment_comp.pdf}
\end{center}
\vspace{-0.4cm}
\caption{Result of photometric alignment. (a) \& (b) show captured images from a one camera module and the other camera module, respectively. (c) shows the photometric alignment result of (b). (d)-(f) show magnified views of (a)-(c).}
\label{fig:photometric_alignment}
\end{figure}

\section{Geometric Alignment between Camera Modules}

Although the two camera modules are physically aligned as much as possible, there may exist a small amount of geometric misalignment between images from them (\Fig{geometric_alignment}(c)).
Thus, after capturing images, we conduct geometric alignment to compensate for this.
The geometric alignment is performed for each pair of a blurred image and its corresponding sharp image sequence as the degree of misalignment can vary with respect to the distance between the camera system and scene.

Specifically, for a given sharp image sequence, we first increase the frame rate 8 using a frame interpolation method~\cite{Park_2021_ICCV}, and synthesize a blurred image by averaging them.
Then, we estimate a homography between a real blurred image and the synthesized one using the enhanced correlation coefficient method~\cite{Evangelidis-TPAMI08}.
Finally, we warp the sharp images according to the estimated homography.
We perform geometric alignment to the images processed by the ISP.
\Fig{geometric_alignment}(d) shows a result of our geometric alignment, where the red and cyan lights are better aligned after geometric alignment. Also, it shows that the real blurred image and nine sharp images are well synchronized.


\begin{figure}[t]
\begin{center}
\includegraphics [width=1\linewidth] {figs_supple/geometric_alignment_comp.pdf}
\end{center}
\vspace{-0.4cm}
\caption{Result of geometric alignment. (a) \& (b) show stereo-anaglyph images, where a real blurred and the averaging of nine sharp images are visualized in red and cyan, respectively. (c) \& (d) show magnified views of (a) and (b).}
\label{fig:geometric_alignment}
\end{figure}


\section{Camera ISP for the RealBlur Dataset} \label{sec:details_ISP_A7R3}

In the main paper, we improve the performance of SRN-DeblurNet~\cite{Tao-CVPR18} by mimicking the ISP of the Sony A7R3 camera, which is used for the RealBlur dataset~\cite{jsrim-ECCV2020}. 
Our camera ISP for the Sony A7R3 also consists of the white balance, demosaicing, color correction, and camera response function (CRF) steps. 

As the ISP affects the noise distribution and non-linearity of the blur, we match the white balance gains, color correction matrix, and CRF as much as possible to those of the RealBlur dataset. For white balance, we extract the white balance gains from the RAW images of the RealBlur training set.
Similar to~\cite{Brooks_2019_CVPR_denoising}, we randomly sample the gains from the RealBlur training set and multiply a RAW image by the gains. After white balancing, we apply the demosaicing method of Malvar \etal~\cite{Malvar_2004_ICASSP}. 

For the color correction, we extract a characterization matrix of A7R3 from the Libraw library and convert it into a color correction matrix following~\cite{Andrew_2020_Optical_Engineering}. 
The extracted color correction matrix directly maps from the RAW space to the XYZ color space.
We convert a demosaiced image into the linear sRGB space as:

where  is an RGB color of an image after demosaicing, and  is a resulting RGB color in the linear sRGB space.


\begin{figure}[t]
\begin{center}
\includegraphics [width=1.0\linewidth] {figs_supple/crfs_sRGB.pdf}
\end{center}
\vspace{-0.4cm}
\caption{The blue and red lines show the estimated CRFs of the Sony A7R3 using color chart images and a raw-RGB image from the internet, respectively. The orange dots show measured values. The green lines show gamma correction of sRGB space.}
\label{fig:A7R3_CRF}
\end{figure}

The final step applies a CRF. Motivated by~\cite{Mitsunaga_1999_CVPR}, we model the CRF as a high-order polynomial as follows:

where , , and  are a non-linear and a linear sRGB image, and polynomial coefficients, respectively.  is the polynomial order, which is set to 7. We capture 11 color chart images with different shutter speeds using a Sony A7R3 camera.
Then, we measure the RGB values of the color patches in JPEG images from the camera and in linear sRGB images converted from RAW images.
Using the pairs of the measured RGB values, we estimate the coefficients of the CRF  by solving a least-squares problem.
\Fig{A7R3_CRF}(a) shows the estimated CRF using color chart images.

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\linewidth]{figs/overview_process_horizontal.pdf}
\vspace{-0.35cm}
\caption{Overview of our realistic blur synthesis pipeline. Lin2Cam: Inverse color correction, i.e., color space conversion from the linear sRGB space to the camera RAW space. WB: White balance. Cam2Lin: Color correction.}
\vspace{-0.25cm}
\label{fig:generation_process_supple}
\end{figure}




\section{Conversion from sRGB to RAW}

\Fig{generation_process_supple} shows an overview of our realistic blur synthesis pipeline.
As described in the main paper, we convert the image from the saturation synthesis step into the mosaiced camera RAW space.
To this end, we apply inverse color correction, mosaicing, and inverse white balance sequentially.
Then, we apply the camera ISP to reflect distortions introduced by the camera ISP.
In the case of Sony A7R3, we apply the ISP described in \Sec{details_ISP_A7R3}.
In the following, we describe each step of the conversion to RAW in more detail.

\paragraph{Lin2Cam}
This step performs inverse color correction.
As color correction is a simple linear operation, we can apply inverse color correction as follows:

where  and  are RGB colors in the linear sRGB and RAW RGB spaces, respectively.
 and  are matrices for color conversion between the linear sRGB and XYZ color spaces.  is the inverse of a color correction matrix . 


In the case of the Sony A7R3, the color correction matrix  directly maps colors in the RAW color space into the XYZ color space as mentioned in \Sec{details_ISP_A7R3}.
Thus, we perform inverse color correction as: 

where  and  are RGB colors in the linear sRGB and RAW RGB spaces, respectively.


\paragraph{Mosaic} Following~\cite{Guo_2019_CVPR}, we randomly sample a Bayer pattern from RGGB, BGGR, GRBG, and GBRG to reflect distortions caused by various Bayer patterns.
Then, we perform mosaicing using the sampled pattern.

\paragraph{Inverse WB}
As we already know white balance gains for each image, in this step, we simply apply their inverse  to each color channel of a mosaiced image.


\section{Additional Analysis Results}

\afterpage{
\setlength{\tabcolsep}{3pt}
\begin{table}[t]
\centering
\caption{Additional analysis using MIMO-Unet~\cite{Cho_2021_ICCV} on the RSBlur dataset. Interp.: Frame interpolation. Sat.: Saturation synthesis. sRGB: Gamma correction of sRGB space. G: Gaussian noise. G+P: Gaussian and Poisson noise.}
\label{tbl:analysis_result_mimounet}
\vspace{-0.25cm}
\scalebox{0.9}{
\begin{tabular}{|ccccccc|c|c|c|}
\hline
\multicolumn{7}{|c|}{Blur Synthesis Methods}                                                      & \multicolumn{3}{c|}{PSNR / SSIM}                 \\ \hline
No. & Real                      & CRF    & Interp.             & Sat. & Noise & ISP     & All & Saturated      & No Saturated            \\ \hline
1 & \checkmark &        &                           &            &       &        & 32.73 / 0.8457 & 31.44 / 0.8385 & 33.93 / 0.8524  \\
2 &                           & sRGB    & \checkmark &            &       &         & 28.83 / 0.7164 & 27.42 / 0.7052 & 30.16 / 0.7270 \\
3 &                           & sRGB    & \checkmark &            & G     &         & 29.63 / 0.7552 & 28.28 / 0.7486 & 30.90 / 0.7614 \\
4 &                          & sRGB    & \checkmark & Ours    & G     &         & 29.84 / 0.7658 & 28.49 / 0.7590 & 31.12 / 0.7723 \\
5 &                          & sRGB    & \checkmark & Ours   & G+P   & \checkmark   & 32.08 / 0.8362 & 30.68 / 0.8290 & 33.39 / 0.8429 \\

\hline
\end{tabular}
}
\vspace{-0.20cm}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs_supple/qualatative_mimounet_comp.pdf}
\vspace{-0.75cm}
\caption{Qualitative comparison of deblurring results on the RSBlur test set produced by  MIMO-Unet~\cite{Cho_2021_ICCV} trained with different synthesis methods.
(b)-(e) \& (h)-(k) Methods 1, 2, 3 and 5 in \Tbl{analysis_result_mimounet}.
Best viewed in zoom in.}
\vspace{-0.3cm}
\label{fig:qualitative_results_supple}
\end{figure}
}

In this section, we also provide additional analysis results using MIMO-Unet~\cite{Cho_2021_ICCV}. For the experiments, we train MIMO-Unet model for 790K iterations, which is half the number of iterations suggested in \cite{Cho_2021_ICCV}, with variants of our pipeline.
\Tbl{analysis_result_mimounet} shows that the method 2 performs worse than method 1, which uses real blurred images for training.
Methods 3 and 4 show that adding Gaussian noise () and our saturation synthesis significantly improves the deblurring performance.
The method 5, which corresponds to our full pipeline, achieves 32.08 dB.
The analysis shows that MIMO-Unet~\cite{Cho_2021_ICCV} also has significant performance improvement with the proposed synthesis pipeline.
\Fig{qualitative_results_supple} shows results of MIMO-Unet~\cite{Cho_2021_ICCV} trained on the RSBlur dataset with different methods in \Tbl{analysis_result_mimounet}.

\section{Experiments with Rough Camera Parameters}

Estimating accurate camera-specific parameters can be easily done by taking a few shots of images.
Even if the camera is not available, rough parameters can be easily obtained using images available on the internet in the case of most consumer cameras.
To show this, assuming that an A7R3 camera is not available, we conducted additional experiments on the RealBlur\_J~\cite{jsrim-ECCV2020} dataset where we estimated camera-specific parameters from images from the internet.

In the case of most consumer cameras, the characterization matrix is easily obtained from the Libraw library\footnote{\href{https://github.com/LibRaw/LibRaw/blob/2a9a4de21ea7f5d15314da8ee5f27feebf239655/src/tables/colordata.cpp}{https://github.com/LibRaw/src/tables/colordata.cpp}}. As described in \Sec{details_ISP_A7R3}, we extract the characterization matrix of the A7R3 camera and convert it into a color correction matrix.
The SIDD dataset~\cite{Abdelhamed_2018_CVPR} provides the noise parameters of four cameras on different ISO settings. 
As the RealBlur\_J dataset is mostly captured with ISO 100, we sample the noise parameters of Google Pixel on the ISO 100 setting. 
Following~\cite{Brooks_2019_CVPR_denoising}, we randomly sample gains for the red and blue channels from  and  for the white balance.

\afterpage{
\setlength{\tabcolsep}{4pt}
\begin{table*}[t]
\centering
\caption{ Performance comparison of different blur synthesis methods on the RealBlur\_J~\cite{jsrim-ECCV2020} test sets. Interp.: Frame interpolation. Sat.: Saturation synthesis. sRGB: Gamma correction of sRGB space. G: Gaussian noise. G+P: Gaussian and Poisson noise. A7R3: Using camera ISP with accurate parameters estimated from a Sony A7R3 camera. A7R3*: Using camera ISP with rough  parameters.}
\label{tbl:rough_camera_params}
\vspace{-0.2cm}
\scalebox{0.90}{
\begin{tabular}{|ccccccc|c|}
\hline
\multicolumn{7}{|c|}{Blur Synthesis Methods}                                                         & \multicolumn{1}{c|}{PSNR / SSIM}       \\ \hline
No. & Training set & CRF    & Interp.             & Sat. & Noise         & ISP             & RealBlur\_J           \\ \hline
1 & RealBlur\_J  &        &                           &            &               &                  & 30.79 / 0.8985      \\
2 & BSD\_All     &        &                           &            &               &                 & 28.66 / 0.8589        \\
3 & GoPro      & sRGB    & \checkmark &            &               &                 & 28.92 / 0.8711        \\
4 & GoPro      & sRGB, A7R3    & \checkmark & Ours   & G+P          & A7R3 & 30.32 / 0.8899  \\ 5 & GoPro      & sRGB, A7R3*    & \checkmark & Ours   & G+P          & A7R3* & 30.23 / 0.8864  \\ \hline
6 & GoPro\_U     & sRGB    &                           &            &               &                  & 29.28 / 0.8766       \\
7 & GoPro\_U     & sRGB, A7R3    &                           &  Ours & G+P  & A7R3 & 30.75 / 0.9019         \\
8 & GoPro\_U     & sRGB, A7R3*    &                           &  Ours & G+P  & A7R3* & 30.55 / 0.8956        \\
\hline
\end{tabular}
}
\vspace{-0.30cm}
\end{table*}


\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs_supple/qualatative_rough_params_comp.pdf}
\vspace{-0.8cm}
\caption{Qualitative comparison of deblurring results on the RealBlur\_J test set produced by models trained with different synthesis methods.
(b)-(e) Methods 1, 3, 4 and 5 in \Tbl{rough_camera_params}.
(g)-(j) Methods 2, 6, 7 and 8 in \Tbl{rough_camera_params}.
Best viewed in zoom in.
}
\label{fig:qualitative_rough_params}
\end{figure}
}

In the case of the CRF, there is no dataset including the CRFs of the latest consumer cameras, and it is difficult to estimate CRFs without cameras. Instead, we utilize the camera profile of Sony A7R3 available on Adobe Lightroom. Specifically, we first download a RAW image captured by an A7R3 from the internet. Then, we convert the RAW image into an sRGB image using the camera profile of Adobe Lightroom. We also convert the RAW RGB image into a linear sRGB image using the color correction matrix from the Libraw library and white balance gains of the RAW RGB image.
Then, using the pixel values of the converted linear sRGB image and sRGB image, we estimate the coefficients of \Eq{crf} by solving a least-squares problem.
\Fig{A7R3_CRF}(b) shows the estimated CRF using the RAW image resembles the estimated CRF using a color chart very closely.


To verify the effectiveness of the rough parameters estimated as described above, we train SRN-DeblurNet~\cite{Tao-CVPR18} using the proposed synthesis pipeline with the estimated parameters, and evaluate its performance.
In \Tbl{rough_camera_params}, methods 5 and 8 that use the rough parameters perform worse than methods 4 and 7 that use accurate camera parameters.
Neverthelss, compared to the methods 3 and 6, the methods 5 and 8 still perform significantly better, validating the effectiveness of the rough parameters.
\Fig{qualitative_rough_params} shows qualitative examples of using the proposed pipeline with rough and accurate camera parameters and other na\"{i}ve methods.


\setlength{\tabcolsep}{4pt}
\begin{table*}[t]
\centering
\caption{ Performance comparison of SRN-DeblurNet~\cite{Tao-CVPR18} trained on the RealBlur\_J~\cite{jsrim-ECCV2020}, BSD\_All~\cite{Zhong_2020_ECCV,Zhong_2021_arxiv}, and RSBlur datasets. }
\label{tbl:Limited_coverage_realdataset}
\begin{tabular}{|c|ccc|}
\hline
\multirow{2}{*}{\backslashbox{Train}{Test}} & \multicolumn{3}{c|}{PSNR / SSIM}  \\ \cline{2-4} 
                                      & \multicolumn{1}{c|}{RealBlur\_J}    & \multicolumn{1}{c|}{BSD\_All}       & RSBlur         \\ \hline
RealBlur\_J                           & \multicolumn{1}{c|}{30.79 / 0.8985} & \multicolumn{1}{c|}{29.67 / 0.8922} & 29.86 / 0.7895 \\
BSD\_All                              & \multicolumn{1}{c|}{28.66 / 0.8589} & \multicolumn{1}{c|}{33.35 / 0.9348} & 30.89 / 0.8049 \\
RSBlur                                & \multicolumn{1}{c|}{29.86 / 0.8855} & \multicolumn{1}{c|}{30.85 / 0.9069} & 32.53 / 0.8398 \\ \hline
\end{tabular}
\end{table*}
\setlength{\tabcolsep}{1.4pt}

\section{Limited Coverage of Real Datasets}


In the main paper, we show the limited coverage of the existing real datasets including RealBlur~\cite{jsrim-ECCV2020} and BSD\_All~\cite{Zhong_2020_ECCV,Zhong_2021_arxiv}.
Specifically, we train SRN-DeblurNet~\cite{Tao-CVPR18} using one dataset, and evaluate its performance on the other dataset.
In this supplementary material, we report a full comparison result among the real datasets including the RSBlur real dataset in \Tbl{Limited_coverage_realdataset}.
As the table shows, the deblurring performance of one dataset significantly drops on the other datasets.
This result again verifies the limited coverage of the existing real datasets and the usefulness of our synthesis pipeline.

\section{Saturation Synthesis: Local vs.~Global}

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs_supple/sat_syn_supple_comp.pdf}
\caption{Generated images from saturation synthesis methods using global scaling and local scaling.}
\label{fig:generating_saturation_synthesis}
\end{figure}

One important component in our blur synthesis pipeline is the saturation synthesis step, which locally scales intensity values of a blurred image before clipping as done in \cite{Hu-CVPR14}.
Another option that has been used in several previous works is global scaling~\cite{PAN_2017_TPAMI, Chen_2021_CVPR, NEURIPS2018_0aa1883c}.
In this section, we discuss the global and local scaling approaches and compare their performance.

Without considering noise and camera ISP for the brevity of the discussion, we can model a blurred image  with clipped intensity values as , where  is a clipping function,  is a convolution kernel, and  is a sharp image.
To obtain clipped intensity values in ,  should contain unclipped intensity values larger than the upper limit of the dynamic range.
However, as sharp images also have the limited dynamic range, it is inevitable to synthesize sharp images with large intensity values, which can be done by either global scaling~\cite{PAN_2017_TPAMI, Chen_2021_CVPR, NEURIPS2018_0aa1883c} or local scaling~\cite{Hu-CVPR14}.

The global scaling approach generates saturated pixels by scaling all the intensity values as , where  is a scaling factor. The local scaling approach, on the other hand, scales only some intensity values as , where  and  are images that have non-zero pixels on non-saturated and saturated region of , respectively.
Due to the information loss at clipped pixels, both methods randomly sample  to generate non-clipped pixels.
By replacing the convolution operation with  by averaging operation over consecutive video frames, we can also model the blur synthesis based on averaging video frames, on which our analysis in the main paper is based.

\Fig{generating_saturation_synthesis} shows real saturated images and synthesized images using global scaling and local scaling.
Both methods cannot exactly reproduce the real saturated pixels (\Fig{generating_saturation_synthesis}(c),(e), and (f)) due to the missing information in sharp images caused by clipping. 
Nevertheless, the local scaling approach has a couple of advantages over the global scaling approach.
First, global scaling affects all the pixels, thus introduces a larger domain gap (brighter images) as shown in \Fig{generating_saturation_synthesis}(e), and severe distortion of the distribution of signal-dependent noise.
Second, as global scaling increases the intensities of larger areas, it is usually more difficult to mimic sharp light streaks often observed in real blurred images.
This difference can also affect the deblurring performance.
We conducted an experiment using the GoPro\_U training set with global scaling, and found that global scaling performs worse than our saturation synthesis by 0.35 dB on the RealBlur test set.



\section{Direct Measurement of the Quality of Blur Synthesis}

To evaluate the synthesis methods, we measure the deblurring performance trained with them in the main paper.
Another possible option would be to directly compare synthetic blurred images against real blurred images to quantify the quality of the synthetic methods using the RSBlur dataset.
In this section, we discuss about the direct measurement of the quality of blur synthesis and report additional evaluation results.

To measure the quality of different blur synthesis approaches, we measure the PSNR and SSIM values of synthetic blurred images against real-blurred images.
However, the PSNR or SSIM values of synthetic images are not 100\% reliable due to noise.
One possible option is to measure the KL-divergence of the distributions of real and synthesized images as done in noise-synthesis approaches~\cite{Abdelhamed_2019_ICCV, Chang_2020_ECCV, Jang_2021_ICCV}.
Specifically, we compute the KL-divergence between pixel-wise marginal distributions  and  where  and  is a real blurred image and an averaging of interpolated images, respectively.  is a synthesized blurred image using our pipeline.
We use  as rough noise-free counterparts of real blurred images for computing the KL-divergence.

\Tbl{additional_metrics} shows PSNR, SSIM, and the KL-divergence of synthesized images using variants of our synthesis methods. 
The table shows that frame interpolation and saturation synthesis are effective in terms of PSNR (methods 2 and 3). The method 7 shows noise and saturation synthesis methods improve the KL-divergence. We omit the KD-divergence values of methods 2 and 3 as the synthetic images have no noise thus their noise distributions are not properly defined.
Note that, measuring the KL-divergence also has its own flaws as we don't have accurate noise-free counterparts of real blurred images. So, 100\% accurate estimation of real distribution is not feasible.
Also, as our ultimate goal is to improve the deblurring quality, the deblurring performance of the trained network is a most reasonable measure. 

\setlength{\tabcolsep}{7pt}
\begin{table}[t]
\centering
\caption{Comparison among different blur synthesis methods. We compute PSNR, SSIM and KL-divergence (KLD) from synthesized and real blurred images.}
\label{tbl:additional_metrics}
\scalebox{1.0}{
\begin{tabular}{|c|cccc|c|}
\hline
\multicolumn{5}{|c|}{Blur Synthesis Methods}                                                     &                           \\ \hline
No.                     & Interp.                   & Sat.   & Noise & ISP                       & PSNR / SSIM / KLD         \\ \hline
1                       &                           &        &       &                           & 35.74 / 0.8802 / 1.0064  \\
2                       & \checkmark &        &       &                           & 35.97 / 0.8888 / \ \ \ \ - \ \ \ \\
3                       & \checkmark & Ours &       &                           & 36.03 / 0.8888 / \ \ \ \ - \ \ \ \\
4                       & \checkmark &        & G     &                           & 34.16 / 0.8075 / 0.4312   \\
5                       & \checkmark &        & G+P   & \checkmark & 33.14 / 0.7928 / 0.3319   \\
6                       & \checkmark & Ours   & G     &                           & 34.21 / 0.8077 / 0.4313   \\
7                       & \checkmark & Ours   & G+P   & \checkmark & 33.18 / 0.7928 / 0.3263   \\ \hline
\end{tabular}
}
\end{table}



\section{Training with Both Real and Synthetic Datasets}


Even if a real-world blur training set is available, an additional synthetic dataset generated by our method could further improve the deblurring performance on real blurred images.
One important question when training with two datasets is how to mix them.
To verify the effect of using both real and synthetic datasets and the mixing strategy, in this section, we compare the deblurring performance obtained using one of real and synthetic datasets and using both of them.
Specifically, we compare four different training strategies: training 1) with only RealBlur\_J, 2) with only GoPro\_U, 3) with both RealBlur\_J and GoPro\_U together half and half at each iteration, and 4) with RealBlur\_J and GoPro\_U one by one at each iteration.
We train SRN-DeblurNet using the four strategies and evaluate their performances on the RealBlur test set.
As the table shows, using both RealBlur\_J and GoPro\_U clearly improves the deblurring performance regardless of the training strategies.


\setlength{\tabcolsep}{5pt}
\begin{table}[t]
\centering
\caption{Comparison of different training strategies using additional synthetic datasets for further performance improvements.}
\label{tbl:training_process}
\scalebox{1.0}{
\begin{tabular}{|c|cc|c|}
\hline
No. & Train dataset & Strategy      & PSNR / SSIM    \\ \hline
1   & RealBlur\_J          &             & 30.79 / 0.8985 \\
2   & GoPro\_U           &             & 30.75 / 0.9019 \\
3   & RealBlur\_J + GoPro\_U    &       Half \& Half      & 31.17 / 0.9065 \\
4   & RealBlur\_J + GoPro\_U    & One  One & 31.15 / 0.9059 \\ \hline
\end{tabular}
}
\end{table}

\if 0
\begin{tabular}{|c|cc|cc|}
\hline
No. & Train dataset          & Process     & RealBlur\_J    & RSBlur         \\ \hline
1   & RealBlur\_J            &             & 30.79 / 0.8985 &                \\
2   & RealBlur\_J + GoPro\_U & Mixing      & 31.17 / 0.9065 &                \\
3   & RealBlur\_J + GoPro\_U & Alternating & 31.15 / 0.9059 &                \\ \hline
4   & RSBlur                 &             &                & 32.53 / 0.8398 \\
5   & RSBlur + RSBlur\_add   & Mixing      &                & 32.42 / 0.8376 \\
6   & RSBlur + RSBlur\_add   & Alternating &                & 32.53 / 0.8396 \\ \hline
\end{tabular}
\fi


\section{Additional Results on Other Datasets}

In the main manuscript, we evaluate the proposed pipeline on the RealBlur~\cite{jsrim-ECCV2020} and BSD\_All~\cite{Zhong_2020_ECCV,Zhong_2021_arxiv} datasets.
Additionally, in this supplementary material, we also report evaluation results on K\"{o}hler~\etal's dataset~\cite{Kohler-ECCV12}.
\Tbl{kohler_result} shows the performance of SRN-DeblurNet~\cite{Tao-CVPR18} trained with variants of the proposed convolution-based synthesis pipeline.
Note that the images in K\"{o}hler~\etal's dataset are in the linear RGB space, and do not have saturated pixels.
Thus, the method 5 performs the best, while the other methods with wrong CRFs show significant performance drops.
Again, the poor performances of the existing real datasets on K\"{o}hler~\etal's dataset prove their limited coverage, as discussed in the main manuscript.

\setlength{\tabcolsep}{4pt}
\begin{table*}[ht]
\centering
\caption{Performance comparison of different blur synthesis methods on the K\"{o}hler \etal's~\cite{Kohler-ECCV12} dataset. Sat.: Saturation synthesis. sRGB: Gamma correction of sRGB space. G: Gaussian noise. G+P: Gaussian and Poisson noise. A7R3: Using camera ISP parameters estimated from a Sony A7R3 camera, which was used for collecting the RealBlur dataset.}
\label{tbl:kohler_result}
\begin{tabular}{|cccccc|c|}
\hline
\multicolumn{6}{|c|}{Blur Synthesis Methods}     & PSNR / MSSIM   \\ \hline
No. & Training set & CRF    & Sat. & Noise & ISP & K\"{o}hler \etal             \\ \hline
1   & RealBlur\_J  &        &      &       &     & 26.79 / 0.8401 \\
2   & BSD\_All     &        &      &       &     & 25.24 / 0.7920 \\
3   & RSBlur       &        &      &       &     & 26.29 / 0.8285 \\
4   & GoPro\_U     & Linear &      &       &     & 28.28 / 0.8599 \\
5   & GoPro\_U     & Linear &      & G     &     & 28.41 / 0.8624 \\
6   & GoPro\_U     & sRGB    &      &       &     & 26.86 / 0.8430 \\
7   & GoPro\_U     & sRGB    &      & G     &     & 27.23 / 0.8529 \\
8   & GoPro\_U     & sRGB    & Ours & G     &     & 27.10 / 0.8500 \\
9   & GoPro\_U     & sRGB, A7R3 & Ours & G+P   &A7R3 & 27.41 / 0.8516 \\ \hline
\end{tabular}
\end{table*}

We also compare variants of the convolution-based synthesis pipeline in \Tbl{kohler_result} on Lai~\etal's dataset~\cite{Lai-CVPR16}, which provides 100 real blurred images without ground-truth sharp images for qualitative comparison. 
In \Figs{qualitative_lai} and \ref{fig:qualitative_lai2}, methods 1, 2, and 3 show that the models trained on real datasets do not successfully deblur real blurred images of Lai~\etal's dataset. 
On the other hand, the method 9 shows that the deblurring results produced by the model trained with our pipeline performs better.
This again shows the limited coverage of the real datasets and the practicality of the proposed pipeline.
The proposed pipeline can generate a huge number of images with various contents, and blur shapes and sizes effortlessly compared to collecting real datasets, leading to better deblurring performance.


\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs_supple/qualatative_lai.pdf}
\vspace{-0.8cm}
\caption{Qualitative comparison of deblurring results on the Lai~\etal's~dataset~\cite{Lai-CVPR16} produced by models trained with different synthesis methods in \Tbl{kohler_result}.
Best viewed in zoom in.
}
\label{fig:qualitative_lai}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs_supple/qualatative_lai2.pdf}
\vspace{-0.8cm}
\caption{Qualitative comparison of deblurring results on the Lai~\etal's~dataset~\cite{Lai-CVPR16} produced by models trained with different synthesis methods in \Tbl{kohler_result}.
Best viewed in zoom in.
}
\label{fig:qualitative_lai2}
\end{figure}


\section{Additional Qualitative Examples}

\Figs{RSBlur_results}, \ref{fig:RealBlur_results} and \ref{fig:BSD_results} show additional qualitative examples on the RSBlur, RealBlur\_J~\cite{jsrim-ECCV2020}, and BSD\_All~\cite{Zhong_2020_ECCV,Zhong_2021_arxiv} datasets, respectively.
The figures show deblurring results of SRN-DeblurNet~\cite{Tao-CVPR18} trained with training images synthesized by different methods in order to compare different blur synthesis methods.


\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\linewidth]{figs_supple/additional_RSBlur_comp.pdf}
\caption{Qualitative comparison on the RSBlur dataset.
\textcolor{green}{Green}: Trained on real blurred images.
\textcolor{red}{Red}: Trained on synthetic blurred images.
Avg.: Na\"{i}ve averaging-based blur synthesis.
Interp.: Averaging-based blur synthesis using frame interpolation.  
G: Gaussian noise. 
Oracle: Using oracle saturated images.
Sat: Our saturation synthesis. 
All of synthesis methods consider gamma decoding and encoding.}
\label{fig:RSBlur_results}
\end{figure*}


\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\linewidth]{figs_supple/additional_realblur_comp.pdf}
\caption{Qualitative comparison on the RealBlur\_J dataset~\cite{jsrim-ECCV2020}.
\textcolor{green}{Green}: Trained on real blurred images.
\textcolor{red}{Red}: Trained on synthetic blurred images.
Interp.: Averaging-based blur synthesis using frame interpolation. 
G: Gaussian noise. 
GP: Gaussian and Poisson noise with a camera ISP of Sony A7R3. 
Sat: Our saturation synthesis. 
Conv.: Convolution-based blur synthesis.
All of synthesis methods consider gamma decoding and encoding.}
\label{fig:RealBlur_results}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\linewidth]{figs_supple/additional_bsd_comp.pdf}
\caption{Qualitative comparison on the BSD\_All dataset~\cite{Zhong_2020_ECCV,Zhong_2021_arxiv}.
\textcolor{green}{Green}: Trained on real blurred images.
\textcolor{red}{Red}: Trained on synthetic blurred images.
Linear: Na\"{i}ve averaging-based blur synthesis with linear CRF.
Avg.: Na\"{i}ve averaging-based blur synthesis.
Interp.: Averaging-based blur synthesis using frame interpolation.  
G: Gaussian noise. 
Sat: Our saturation synthesis. 
All of synthesis methods except Linear consider gamma decoding and encoding.}
\label{fig:BSD_results}
\end{figure*}
