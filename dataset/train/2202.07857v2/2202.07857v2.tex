\section{Experiments}
In this section, we conduct a comprehensive set of experiments to validate the effectiveness of the proposed {\method} framework. In particular, they are designed to answer the following questions:
\vspace{-0.2em}
\begin{itemize}[leftmargin=*]
  \item \textbf{Q1:} Can {\method} accurately detect anomalies and estimate densities?
  \item \textbf{Q2:} Does the proposed graph structure learning help? Is the framework sufficiently flexible to include various normalizing flow backbones?
  \item \textbf{Q3:} What can one observe for a dataset spanning a long time? E.g., does the graph pattern change?
\end{itemize}


\subsection{Settings}
\label{sec:exp_set}

\textbf{Datasets.}
To evaluate the effectiveness of {\method} for anomaly detection and density estimation, we conduct experiments on two power grid datasets, one water system dataset, and one traffic dataset.
\begin{itemize} [leftmargin=*]
\item \textbf{PMU-B} and \textbf{PMU-C}: These two datasets correspond to two separate interconnects of the U.S. power grid, containing time series recorded by 38 and 132 phasor measurement units (PMUs), respectively. We process one-year data at the frequency of one second to form a ten-month training set, one-month validation set, and one-month test set. Each multiple time series is obtained by shifting a one-minute window. Additionally, to investigate distribution drift, we shift a one-month window to obtain multiple training/validation/test sets (12 in total, because of availability of two-year data). Sparse grid events (anomalies) labeled by domain experts exist for evaluation; but note that the labels are both noisy and incomplete. These datasets are proprietary.

\item \textbf{SWaT}: We also use a public dataset for evaluation. The Secure Water Treatment (SWaT) dataset originates from an operational water treatment test-bed coordinated with Singapore's Public Utility Board~\citep{goh2016dataset}. The data collects 51 sensor recordings lasting four days, at the frequency of one second. A total of 36 attacks were conducted, resulting in approximately 11\% time steps as anomaly ground truths. We use a sliding window of 60 seconds to construct series data and perform a 60/20/20 chronological split for training, validation, and testing, respectively.

\item \textbf{METR-LA}: This dataset is also public; it contains speed records of 207 sensors deployed on the highways of Los Angles, CA~\citep{Li2018}. No anomaly labels exist however and we use this dataset for exploratory analysis only. Results are deferred to Appendix~\ref{sec:metr.la}.
\end{itemize}

\textbf{Evaluation metrics (under noisy labels).} For SWaT, which offers reliable ground truths, we use the standard ROC and AUC metrics for evaluation. For the two PMU datasets, however, the resolution of the time series and the granularity of the events result in rather noisy ground truths. Hence, we adapt ROC for noisy labels. We smooth the time point of a ``ground truth'' event (anomaly) by introducing probabilities to the label. Specifically, the probability that a multiple time series starting at time  is a ground truth anomaly is , where  is the starting time of the th labeled anomaly. Then, when computing the confusion matrix, we sum probabilities rather than counting 0/1s. The smoothing window  is chosen to be 6 time steps.

\textbf{Baselines.} We compare with the following representative, state-of-the-art deep methods.
\begin{itemize}[leftmargin=*]
\item \textbf{EncDecAD}~\citep{malhotra2016lstm}: In this method, an autoencoder based on LSTM is trained. The reconstruction error is used as the anomaly measure.
\item \textbf{DeepSVDD}~\citep{ruff2018deep}: This method minimizes the volume of a hypersphere that encloses the representations of data. Samples distant from the hypersphere center are considered anomalies.
\item \textbf{ALOCC}~\citep{sabokrou2020deep}: In this GAN-based method, the generator learns to reconstruct normal instances, while the discriminator works as an anomaly detector.
\item \textbf{DROCC}~\citep{goyal2020drocc}: This method performs adversarial training to learn robust representations of data and identifies anomalies.
\item \textbf{DeepSAD}~\citep{ruff2019deep}: This method extends DeepSVDD with a semi-supervised loss term for training. We use noisy labels as supervision.
\end{itemize}
To apply these baselines on multiple time series, we concatenate the constituent series along the attribute dimension (resulting in high-dimensional series) and use LSTM or CNN as the backbones. On the other hand, for the proposed method, we use LSTM as the RNN model and MAF as the normalizing flow. See Appendix~\ref{sec:app_exp} for more implementation details.


\subsection{Performance of Anomaly Detection and Density Estimation}

\begin{table}[h]
    \centering
    \small
    \caption{AUC-ROC (\%) of anomaly detection.}
    \vskip -1em
    \begin{tabularx}{0.92\linewidth}{XXXXXXX}
    \toprule
    Dataset & EncDecAD & DeepSVDD & ALOCC & DROCC & DeepSAD & \method\\
    \midrule
    PMU-B&  55.6 & 55.6 & 62.9 & 58.6 & 63.7 & \textbf{67.5} \\
    \midrule 
    PMU-C & 53.7 & 56.9 & 60.9 & 61.9 & 60.1 & \textbf{70.6} \\
    \midrule
    SWaT  & 76.5 & 68.8 & 75.4 & 73.3 & 75.4 & \textbf{79.6} \\
    \bottomrule
    \end{tabularx}
    \label{tab:results}
    \vspace{-1em}
\end{table}

To answer {\textbf{Q1}}, we evaluate quantitatively and qualitatively on datasets with labels.

\textbf{Anomaly detection.} We compare {\method} with the aforementioned baselines in Table~\ref{tab:results}, where standard deviations of the AUC scores are additionally reported through five random repetitions of model training. The table suggests an overwhelmingly high AUC score achieved by {\method}. Observations follow. (\textbf{i}) {\method} outperforms generative model-based methods (EncDecAD and ALOCC). Being a generative model as well, normalizing flows augmented with a graph structure leverage the interdependencies of constituent series more effectively, leading to a substantial improvement in detection. (\textbf{ii}) {\method} significantly outperforms deep one-class models (DeepSVDD and DROCC), corroborating the appeal of using densities for detection. (\textbf{iii}) {\method} also performs better than the semi-supervised method DeepSAD, probably because such methods rely on high quality labels for supervision (especially in the case of label scarcity) and they are less effective facing noisy labels.

Besides a single score, we also plot the ROC curve in Figure~\ref{fig:roc}. One sees that the curve of {\method} dominates those of others. This behavior is generally more salient in the low false-alarm regime.

\begin{figure}[t]
\centering
\begin{subfigure}{0.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/B_roc.pdf}
\caption{PMU-B}
\end{subfigure}
\hfill
\begin{subfigure}{0.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/C_roc.pdf}
\caption{PMU-C}
\end{subfigure}
\hfill
\begin{subfigure}{0.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/water_roc.pdf}
\caption{SWaT}
\end{subfigure}
\vspace{-1em}
\caption{ROC curves of anomaly detection on various datasets.}
\label{fig:roc}
\end{figure}

\begin{figure}[t]
\centering
\begin{subfigure}{0.4 \linewidth}
    \centering
    \includegraphics[width=0.81\linewidth]{figs/hist.pdf}
    \vskip -0.5em
    \caption{}
    \label{fig:hist}
\end{subfigure}
\begin{subfigure}{0.58 \linewidth}
    \centering
    \includegraphics[width=0.91\linewidth]{figs/pmu_c.pdf}
    \vskip -0.5em
    \caption{}
    \label{fig:pmu_c}
\end{subfigure}
\vskip -1em
\caption{Qualitative evaluation of {\method} on PMU-C. (a) Distribution of log-densities on the test set (note in log scale). (b) Anomaly detection results for a week in the test set.}
\label{fig:qualitative}
\vskip -1em
\end{figure}

\textbf{Density estimation.} We investigate the densities estimated by {\method}, shown in Figure~\ref{fig:qualitative}. Distributions of the log-densities in the test set are shown in Figure~\ref{fig:hist}. We use log-density as the anomaly measure; the lower the more likely. Note that the vertical axis is in the log-scale. One sees that a log-density of 16 approximately separates the majority normal instances from the minority anomalies. To cross-verify that the instances with low densities are suspiciously anomalous, we investigate Figure~\ref{fig:pmu_c}, which is a temporal plot of log-densities for a week, overlaid with given labels. From this plot, one sees that the noisily labeled series generally have low densities or are near a low density time step. Additionally, {\method} discovers a few suspicious time steps with low densities undetected earlier. These new discoveries raise interest to power system experts for analysis and archiving.


\subsection{Ablation Study}

\begin{table}[h]
    \centering
    \small
    \caption{Performance of variants of the proposed method.}
    \vskip -1em
    \begin{tabular}{ccccccc}
    \toprule
    Dataset & Metrics & {\methodG} & {\methodD} & {\methodT} & {\method} & {\method}\\
    \midrule
    \multirow{2}{*}{PMU-B}& AUC-ROC & 0.641 & 0.643 & 0.653 & \ud{0.661} & {\bf0.678}  \\
    & Log-Density & 15.31 & 8.70 & 15.09 & \ud{15.90} & {\bf16.22} \\
    \midrule 
   \multirow{2}{*}{PMU-C}& AUC-ROC & 0.630 & 0.544 & 0.688 & \ud{0.703} & {\bf0.705} \\
    & Log-Density& 15.55 & 8.94 & 15.70 & {\bf17.06} & \ud{16.98} \\
    \bottomrule
    \end{tabular}
    \label{tab:abla}
    \vspace{-0.5em}
\end{table}

To answer \textbf{Q2}, we conduct an ablation study (including varying architecture components) to investigate impacts of DAG structure learning and the flexibility of the {\method} framework. To investigate the power of modeling pairwise relationship, we train a variant {\methodG} that factorizes ; i.e., assuming independence among constituent series. To investigate the effectiveness of graph structure learning, we train a variant {\methodD} that decomposes the joint density as ; i.e., a full decomposition without a DAG. It is equivalent to concatenating the series along the attribute dimension and running MAF on the resulting series. To verify the contribution of joint training of  and , we train a variant {\methodT} where  is separately learned by using NOTEARS~\citep{Zheng2018}. To prove the flexibility of {\method}, we replace the MAF-based normalizing flow by RealNVP, denoted as {\method}.

Results are presented in Table~\ref{tab:abla}. Apart from AUC-ROC, the log-density is also reported. Observations follow. (\textbf{i}) {\method} significantly outperforms {\methodG} and {\methodD}, corroborating the importance of interdependency modeling among constituent series. Note that {\methodD} results in particularly poor performance in general, likely because the high dimensional input (resulting from concatenating too many series) impedes the learning of normalizing flows. (\textbf{iii}) {\methodT} is slightly better than {\methodG}, because of the presence of relational modeling, but it cannot match the performance of {\method} and {\method} that jointly train the DAG and the flow. (\textbf{ii}) These latter two models are the best for both datasets and both metrics. MAF works more often better than RealNVP.


\begin{figure}[t]
\centering
\begin{subfigure}{0.23\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/month_0.pdf}
    \vskip -0.3em
    \caption{Initial graph}
\end{subfigure}
\begin{subfigure}{0.23\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/month_1.pdf}
    \vskip -0.3em
    \caption{One-month shift}
\end{subfigure}
\begin{subfigure}{0.23\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/month_2.pdf}
    \vskip -0.3em
    \caption{Two-month shift}
\end{subfigure}
\begin{subfigure}{0.23\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/month_3.pdf}
    \vskip -0.3em
    \caption{Three-month shift}
\end{subfigure}
 \vspace{-0.6em}
\caption{Evolution of the learned DAG on PMU-B over time.}
\vspace{-0.9em}
\label{fig:grpah_evolve}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/all.pdf}
    \vspace{-1em}
    \caption{Evolution of edge weights in the DAG learned by {\method} over time (PMU-B).}
    \label{fig:edge_weight}
    \vskip -1.5em
\end{figure}


\subsection{Evolution of the DAG Structure}
To answer \textbf{Q3}, we investigate how the learned DAG evolves by shifting the train/validation/test sets month by month. The graphs within the first three-month shiftings are shown in Figure~\ref{fig:grpah_evolve} and more can be found in Appendix~\ref{sec:app_result}. In addition to the graph structure, we plot in Figure~\ref{fig:edge_weight} the learned edge weights over time, one column per edge. The appearance and disappearance of edges demonstrate changes of the conditional independence structure among constituent series over time, suggesting data distribution drift (i.e., a change of internal data generation mechanism). It is interesting to observe the seasonal effect. The columns (edges) in Figure~\ref{fig:edge_weight} can be loosely grouped in three clusters: those persisting the entire year, those appearing in the first half of the year, and those existing more briefly (e.g., within a season). Such a pattern plausibly correlates with electricity consumption, which is also seasonal. Were spatial information of the PMUs known, these identified DAGs would help mapping the seasonal patterns to geography and help planning a more resilient grid.
