\section{Experiments}
\subsection{Datasets}
We conduct experiments on three popular datasets for the ASC task:
Lap14 and Res14 datasets are from SemEval 2014 task 4 \cite{semeval2014}, and Res15 dataset is from SemEval 2015 task 12 \cite{semeval2015}.
The statistics of all datasets are presented in Table \ref{table: dataset}.
\begin{table}[ht]
\centering
\caption{Dataset statistics of the three datasets.}
\fontsize{8}{10}\selectfont
\setlength{\tabcolsep}{2.5mm}{
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{Dataset} & \multicolumn{2}{c}{Positive} & \multicolumn{2}{c}{Neutral} & \multicolumn{2}{c}{Negative} \\\specialrule{0em}{0pt}{1pt} \cline{2-3} \cline{4-5} \cline{6-7}\specialrule{0em}{1pt}{1.5pt}
                         & Train         & Test         & Train         & Test        & Train         & Test         \\ \specialrule{0em}{0pt}{1pt}\hline \specialrule{0em}{1.5pt}{1.5pt}
Lap14                    & 994           & 341          &  464          &    169      &  870          &    128       \\
Res14                    & 2164          & 728          & 637           & 196         & 807           & 196          \\
Res15                    & 912           & 326          & 36            & 34          & 256           & 182         \\\bottomrule
\end{tabular}}
\label{table: dataset}
\end{table}


\subsection{Experiment Setup}
We adopt the BERT-base uncased version \cite{bert}. We train our model using Adam optimizer \cite{Adam} with default configuration.
The hyper-parameters are listed in Table \ref{table: hyper-param}.
\begin{table}[t]
\fontsize{9}{11}\selectfont
\centering
\caption{Setting of hyper-parameters.}
\begin{tabular}{c|ccc}
\toprule
\multirow{2}{*}{Hyper-params} & \multicolumn{3}{c}{Dataset} \\\cline{2-4}
                              & Lap14   & Res14   & Res15   \\ \midrule
         learning rate        & $1\times 10^{-5}$  &   $5\times 10^{-5}$      & $3 \times 10^{-5}$        \\
          batch size          & 32     &   32    &   32    \\
            dropout rate     &  0.3    &    0.3  &   0.3   \\
           $d_e$              &  768    &  768    &  768   \\ 
$d_s$                 &  256     &    256     &   128      \\
$H_n^s$                 &  3     &   3     &     6    \\
$H_n^d$                &  2     &   4    &    6     \\
$\mathbf{T}$                 &   4    &      4   &    2    \\
GCN layer number             &   2      &    2     &   2      \\
           \bottomrule
\end{tabular}
\label{table: hyper-param}
\end{table}
Accuracy (Acc) and Macro-F1 (F1) are adopted as evaluation metrics.
As there is no official validation set, following previous works \cite{asgcn,bigcn,DGEDT}, we run our model three times with random initialization and report the average results on test sets, as shown in Table \ref{table: avg results}.
And to compare with the works reporting best results, we also report the best results on test sets, as shown in Table \ref{table: best results}.
All computations are done on an NVIDIA Quadro RTX 6000 GPU.

\begin{comment}
Besides standard KaGRMN-DSG (M$_0$), we also evaluate some variants (M$_{1}$\textasciitilde M$_4$) of M$_0$ to empirically verify its effectiveness from different perspectives.
M$_1$: DSG-Net is evaluated as a single model.
M$_2$: we replace the input description with the aspect so no aspect knowledge is leveraged.
M$_3$: KaGR-MN is evaluated as a single model.
M$_4$: KI Gate is removed.
\end{comment}
\subsection{Compared Baselines}
According to what kinds of external information are utilized, we divide the baselines into several group:\\
1) No external information is used:
 \begin{itemize}
 \item IAN \cite{IAN} separately encodes the aspect and context, then model their interactions using an interactive attention mechanism.
\end{itemize}
2) External corpus is used:
 \begin{itemize}
 \item  PRET+MULT \cite{hrd} first pre-trains the model on document-level task, then trains the model on both document-level sentiment classification and ASC in the multi-task learning framework.
 \item TransCap \cite{transcap} utilizes a devised aspect-based capsule network to transfer knowledge from document-level task to aspect-level task.
\end{itemize}
3) Syntax Graph is used: 
 \begin{itemize}
 \item ASGCN \cite{asgcn} employs a GCN to encode the syntax graph for capturing local syntactic information.
 \item BiGCN \cite{bigcn} convolutes over hierarchical syntactic and lexical graphs to encode not only original syntactic information but also the corpus level word co-occurrence information.
\end{itemize}
4) BERT encoder is used: 
 \begin{itemize}
 \item BERT-SPC \cite{bert} takes the same input as our model and use $h_{cls}$ for sentiment classification.
 \item AEN-BERT \cite{aen-bert} adopts BERT encoder and uses the attentional encoder network to model the interactions between the aspect and context.
\end{itemize}
5) Both of syntax graph and BERT encoder are used: 
  \begin{itemize}
 \item R-GAT+BERT \cite{RGAT} use the relational graph attention network to aggregate the global relational information from all context word into the aspect node representation.
 \item  DGEDT-BERT \cite{DGEDT} employs a dual-transformer network to model the interactions between the flat textual knowledge and dependency graph empowered knowledge. 
 \item A-KVMN+BERT \cite{kvmn-eacl} uses a key-value memory network to leverage not only word-word relations but also their dependency types.
 \item BERT+T-GCN \cite{tgcn} leverages the dependency types in T-GCN and use an attentive layer ensemble to learn the comprehensive representation from different T-GCN layers.
 \item SAGAT \cite{sagat} utilizes graph attention network and BERT to fully obtain both syntax and semantic information.
 \item KGCapsAN-BERT \cite{kgcap} utilizes multi-prior knowledge to guide the capsule attention process and use a GCN-based syntactic layer to integrate the syntactic knowledge.
\end{itemize}

And we label all models with what kinds of external information they leverage, as shown in Table \ref{table: avg results} and Table \ref{table: best results}.



\subsection{Main Results}

\begin{table*}[ht]
\fontsize{8}{10}\selectfont
\centering
\caption{Performances comparisons of average results with random initialization. $\mathcal{K}, \mathcal{B}, \mathcal{T}$ and $\mathcal{G}$ denote the model leverages aspect $\mathcal{K}$nowledge, $\mathcal{B}$ERT, extra $\mathcal{T}$raining corpus and syntax $\mathcal{G}$raph, respectively. Best results are in \textbf{bold} and previous SOTA results are \underline{underlined}. 
$^*$ denotes that we produce the results using their original source codes.
$^\dag$ indicates KaGRMN-DSG significantly outperforms baselines under t-test ($p<0.01$).}
\setlength{\tabcolsep}{3mm}{
\begin{tabular}{ccccccccccccc}\toprule
\multirow{2}{*}{\tabincell{c}{External \\ Information}}  & \multirow{2}{*}{Model} & \multicolumn{2}{c}{Lap14} & \multicolumn{2}{c}{Res14} & \multicolumn{2}{c}{Res15}  \\\cline{3-8} 
                                         &                               &Acc       &F1        &Acc        &F1        &Acc        &F1       \\\midrule
$\mathcal{-} \quad \mathcal{-} \quad \mathcal{-}$  & IAN \cite{IAN}       &72.05     & 67.38    &79.26    &70.09  & 78.54    & 52.65         \\
$\mathcal{-} \quad \mathcal{T} \quad \mathcal{-}$  & PRET+MULT \cite{hrd} &71.15     & 67.46    &79.11   & 69.73  & 81.30    & 68.74       \\
$\mathcal{-} \quad \mathcal{T} \quad \mathcal{-}$& TransCap \cite{transcap} & 73.51  & 69.81    & 79.55    & 71.41  & -      & -      \\ 
$\mathcal{-} \quad \mathcal{-} \quad \mathcal{G}$& ASGCN \cite{asgcn}     & 75.55    &71.05     &80.77    & 72.02  & 79.89   & 61.89        \\
$\mathcal{-} \quad \mathcal{-} \quad \mathcal{G}$& BiGCN \cite{bigcn}    & 74.59    &71.84    &81.97     &73.48    & 81.16    & 64.79       \\
$\mathcal{-} \quad \mathcal{B} \quad \mathcal{-}$& BERT-SPC$^*$ \cite{bert} & 78.47    &73.67  &84.94   & 78.00   & 83.40    &  65.00   \\
$\mathcal{-} \quad \mathcal{B} \quad \mathcal{-}$& AEN-BERT \cite{aen-bert} & 79.93    &76.31  &83.12   & 73.76   & -    &  -   \\
$\mathcal{-} \quad \mathcal{B} \quad \mathcal{G}$& R-GAT+BERT$^*$ \cite{RGAT} & 79.31    &75.40  &86.10   & \underline{80.04}   & 83.95    &  69.47   \\
$\mathcal{-} \quad \mathcal{B} \quad \mathcal{G}$& DGEDT-BERT \cite{DGEDT}   & 79.8    &75.6    &\underline{86.3}      & 80.0   & 84.0   &71.0    \\


 $\mathcal{-} \quad \mathcal{B} \quad \mathcal{G}$ & A-KVMN+BERT$^*$ \cite{kvmn-eacl}   & 79.20    &75.76   &85.89      & 78.29   & 83.89  &67.88    \\
 $\mathcal{-} \quad \mathcal{B} \quad \mathcal{G}$ & BERT+T-GCN$^*$ \cite{tgcn}    & \underline{80.56}    &\underline{76.95}    &85.95      & 79.40   & \underline{84.81} & \underline{71.09} \\\hline
$\mathcal{K} \quad \mathcal{B} \quad \mathcal{G}$ & KaGRMN-DSG (Ours)    &\textbf{81.87$^\dag$}    & \textbf{78.43$^\dag$}     &\textbf{87.35$^\dag$}   & \textbf{81.21$^\dag$}      &\textbf{86.59$^\dag$}   &\textbf{74.46$^\dag$}        \\
   & Our Improvements      & \textbf{1.62\%}  &\textbf{1.92\%} & \textbf{1.22\%} &\textbf{1.46\%} &\textbf{2.10\%} &\textbf{4.74\%} \\
\bottomrule
\end{tabular}}
\label{table: avg results}
\end{table*}


\begin{table*}[ht]
\fontsize{8}{10}\selectfont
\centering
\caption{Performances comparisons of best results. $\mathcal{K}, \mathcal{B}, \mathcal{T}$ and $\mathcal{G}$ denote the model leverages aspect $\mathcal{K}$nowledge, $\mathcal{B}$ERT, extra $\mathcal{T}$raining corpus and syntax $\mathcal{G}$raph, respectively. Best results are in \textbf{bold} and previous SOTA results are \underline{underlined}. $^*$ denotes that we produce the results using their original source codes.}
\setlength{\tabcolsep}{3mm}{
\begin{tabular}{ccccccccccccc}\toprule
\multirow{2}{*}{\tabincell{c}{External \\ Information}}  & \multirow{2}{*}{Model} & \multicolumn{2}{c}{Lap14} & \multicolumn{2}{c}{Res14} & \multicolumn{2}{c}{Res15}  \\\cline{3-8} 
                                         &                               &Acc       &F1        &Acc        &F1        &Acc        &F1       \\\midrule
$\mathcal{-} \quad \mathcal{B} \quad \mathcal{-}$& BERT-SPC$^*$ \cite{bert} & 78.84    &73.95 &85.80   & 78.48   & 83.76    &  68.33   \\
$\mathcal{-} \quad \mathcal{B} \quad \mathcal{G}$& SAGAT \cite{sagat} & 80.37    &76.94  &85.08   & 77.94   & -    &  -   \\
$\mathcal{-} \quad \mathcal{B} \quad \mathcal{G}$& KGCapsAN-BERT \cite{kgcap} & 79.47    &76.61  &85.36   & 79.00   & -    &  -   \\
$\mathcal{-} \quad \mathcal{B} \quad \mathcal{G}$& R-GAT+BERT$^*$ \cite{RGAT} & 79.46   &75.75  &\underline{86.61}   & \underline{80.78}   & 84.13   &  71.12   \\
$\mathcal{-} \quad \mathcal{B} \quad \mathcal{G}$& A-KVMN+BERT \cite{kvmn-eacl}   & 79.78    &76.14    &85.98      & 77.94   & 84.14   &68.49    \\
 $\mathcal{-} \quad \mathcal{B} \quad \mathcal{G}$ & BERT+T-GCN \cite{tgcn}    & \underline{80.88}    &\underline{77.03}    &86.16      & 79.95   & \underline{85.26} & \underline{71.69} \\\hline
$\mathcal{K} \quad \mathcal{B} \quad \mathcal{G}$ & KaGRMN-DSG (Ours)    &\textbf{82.13}    & \textbf{79.42}     &\textbf{87.68}  & \textbf{81.98}      &\textbf{87.08}   &\textbf{75.34}        \\
   & Our Improvements      & \textbf{1.55\%}  &\textbf{3.10\%} & \textbf{1.24\%} &\textbf{1.49\%} &\textbf{2.13\%} &\textbf{5.09\%} \\
\bottomrule
\end{tabular}}
\label{table: best results}
\end{table*}

The performance comparison of all models on average scores is shown in Table \ref{table: avg results}, and the comparison on best scores is shown in Table \ref{table: best results}.
We can observe that:
Syntax graphs, external training corpus, and BERT can all improve ASC.
Especially, simple BERT-SPC significantly outperforms all models that do not adopt BERT, even if some of them leverage syntax graph and external training corpus.
This shows the power of pre-trained language models on ASC.
And combining BERT and syntactic information can further improve results as sufficient semantics captured by BERT and the syntactic information conveyed by syntax graphs can cooperate to assist ASC.
However, all baselines do not leverage aspect knowledge and only consider either local syntactic information or global relational information.
As a result, their derived aspect representation lack some important clues of aspect and their captured syntactic information is insufficient, leading to their inferior performance compared to our KaGRMN-DSG model.


We obtain consistent improvements over baselines in terms of Acc and F1 on all datasets, achieving new state-of-the-art results.
On average results, our KaGRMN-DSG overpasses previous best results by 1.92\%, 1.46\%, and 4.74\% in terms of Macro-F1 on Lap14, Res14, and Res15 datasets respectively.
On best results, KaGRMN-DSG overpasses previous best results by 3.10\%, 1.49\%, and 5.09\% in terms of Macro-F1 on Lap14, Res14, and Res15 datasets respectively.
The improvements are contributed by the superiorities of KaGR-MN, which effectively leverage beneficial aspect knowledge, and DSG-Net, which combines GCN and Relational MHA to capture sufficient syntactic information.





\subsection{Ablation Study}
\begin{table*}[t]
\fontsize{8}{11}\selectfont
\centering
\caption{Results of ablation study.}
\setlength{\tabcolsep}{3mm}{
\begin{tabular}{l|ccc}\toprule
\multirow{2}{*}{Variants} & Lap14 & Res14 & Res15  \\\cline{2-4}
               &Acc               &Acc                &Acc              \\\midrule
    M$_0$: KaGRMN-DSG (full model) &\textbf{81.87}     &\textbf{87.35 }     &\textbf{86.59} \\ \hline
     M$_1$: w/o Aspect Knowledge (descriptions are replaced with aspects)    &80.30 ($\downarrow1.57$)  &86.43  ($\downarrow0.92$)  &85.24  ($\downarrow1.35$)   \\
M$_2$: only KaGRMN (w/o DSG-Net + KI Gate + A2C Att)   &80.72 ($\downarrow1.15$)  &86.55  ($\downarrow0.80$)    &85.36  ($\downarrow1.23$)    \\ \hline
  M$_3$:  w/o DSG-Net   &80.56 ($\downarrow1.31$)  &86.43  ($\downarrow0.92$)  &85.56 ($\downarrow1.03$)\\
    M$_4$: w/o Relational MHA     & 80.98 ($\downarrow0.89$)   &86.67 ($\downarrow0.68$)    &85.79  ($\downarrow0.8$)    \\
   M$_5$: w/o Position-aware GCN    & 81.03 ($\downarrow0.84$)   &86.76 ($\downarrow0.59$)   &85.67   ($\downarrow0.92$) \\ \hline
 M$_6$: w/o KI Gate    &81.09 ($\downarrow0.78$)   &87.11 ($\downarrow0.24$)    &85.79    ($\downarrow0.80$)    \\ \hline
M$_7$: w/o A2C Att   &81.50 ($\downarrow0.37$)     &87.00 ($\downarrow0.35$)   &86.41  ($\downarrow0.18$)     \\ \hline
    M$_8$: w/o  A2D Att &80.93 ($\downarrow0.94$)  & 86.70 ($\downarrow0.65$)  &85.61    ($\downarrow0.98$)      \\
    M$_9$: w/o Self MHA   &  80.36 ($\downarrow1.51$)  & 86.46 ($\downarrow0.89$)   &85.24  ($\downarrow1.35$)    \\

\bottomrule
\end{tabular}}
\label{table: ablation}
\end{table*}
We empirically analyze KaGRMN-DSG and prove the necessity of every component by conducting an ablation study, whose results are shown in Table \ref{table: ablation}.
In this section we answer the following research questions (RQs):


\textit{Effect of Aspect Knowledge.}
To study the pure impact of aspect knowledge, we devise two variants: M$_1$ and M$_2$. 
In M$_1$, the original description is replaced with the aspect itself.
In this case, there is no aspect knowledge available for KaGR-MN and its function becomes modeling the interactions between the aspect and context.
Surprisingly, even without knowledge, M$_2$ can obtain promising results. 
We attribute this to the advanced architecture and effective functions of KaGR-MN, in which aspect and context are separately encoded and their interactions are effectively modeled by KaGR-MN.
On the other hand, the performance degradation of M$_0$ convincingly demonstrates the pure improvements contributed by the aspect knowledge conveyed by aspect descriptions.
In M$_2$, DSG-Net, KI Gate, and A2C Att are all removed, so M$_2$ has a BERT+KaGR-MN architecture and the final aspect representation is used for prediction.
M$_2$ consistently outperforms baselines, proving that KaGR-MN can derive a good enough aspect representation in which the clues for aspect sentiment reasoning are retained.
Along time steps, recurrently leveraging aspect knowledge, KaGR-MN can capture more and more beneficial clues, semantics and dependencies then retain them in aspect representation and context memories.
And effectively utilizing beneficial aspect knowledge is the key advantage of our method compared with previous works.

\textit{Effect of Syntactic Information.}
The results gap of M$_3$ and M$_0$ shows the improvement DSG-Net achieves by cooperating with the aspect knowledge.
These results validate the advantages of combining both kinds of syntactic information to capture sufficient syntactic information.
We then study the effects of Position-aware GCN and Relational MHA.
We can observe that both M$_4$ and M$_5$ perform worse than M$_0$, proving both the local syntactic information and global relational information should be captured for ASC.
In previous works, only either one of them is considered, leading to insufficient syntactic information.
In contrast, our model marries them and lets them compensate for each other, sufficiently capturing syntactic clues.




\textit{Effect of Knowledge Integration Gate.}
Without KI Gate, M$_6$ obtains worse results than M$_0$.
This indicates that after DSG-Net, some aspect knowledge is further needed and KI Gate is efficient to re-enhance the final aspect representation with the needed knowledge.

\textit{Effect of Aspect-to-Context Attention.}
In M$_7$, the final aspect representation is used for prediction.
We can find that M$_7$ has limited performance degradation compared to M$_0$.
This proves that although previous modules can discover and extract clues for ASC, there are still important clues contained in non-aspect hidden states rather than final aspect representation.
Hence it is necessary to employ A2C Att to aggregate the aspect-related semantics in all hidden states into the final representation.

\textit{Effect of A2D Att and Self MHA in KaGR-MN Cell.}
The significant performance decrease of M$_8$ shows that A2D Att is indispensable to dynamically summarize the specifically needed aspect knowledge from $\mathbf {M_D}$.
Without Self MHA, the integrated knowledge in aspect representation can not be contextualized and context memories cannot be updated.
As a result, M$_9$ performs much worse than M$_0$.







\subsection{Investigation on Knowledge Gates} \label{sec: gate}
\begin{table*}[t]
\fontsize{8}{11}\selectfont
\centering
\caption{Results of different knowledge gate settings.}
\setlength{\tabcolsep}{2.5mm}{
\begin{tabular}{l|c|c|ccc}\toprule
\multirow{2}{*}{Variants} & \multirow{2}{*}{Gate 1} &\multirow{2}{*}{Gate 2} & Lap14 & Res14 & Res15 \\ \cline{4-6}
      & &                                                  & Acc   & Acc   & Acc   \\ \midrule
M$_0$          & AdaKI         & KI           &\textbf{81.87}     &\textbf{87.35 }     &\textbf{86.59} \\ \hline
M$_{10}$        & AdaKI     & AdaKI        & 81.50 ($\downarrow0.37$)  &  87.05 ($\downarrow0.30$) &  85.98 ($\downarrow0.61$)     \\
M$_{11}$        & KI            & KI     &  81.09 ($\downarrow0.78$)  &  86.73 ($\downarrow0.62$)  &   85.36 ($\downarrow1.23$) \\
M$_{12}$         & KI       & AdaKI        &  81.03 ($\downarrow0.84$) &  86.58 ($\downarrow0.77$)    &  85.24 ($\downarrow1.35$)  \\
\bottomrule
\end{tabular}}
\label{table: gate}
\end{table*}
KaGRMN-DSG has two different knowledge gates (AdaKI and KI) for knowledge integrating.
Here we empirically investigate these two knowledge gates by testing their four different settings.
The results are shown in Table \ref{table: gate}.
We can find that M$_{10}$ and M$_{12}$ have slight decreases in performances when respectively compared with M$_0$ and M$_{11}$.
This is because KI Gate can preserve the knowledge in $\mathbf{r_k^\mathbf{T}}$ while AdaKI Gate may lose some knowledge when adapting to the semantic space of $\mathbf{\widetilde{R_a}}$.
M$_{11}$ and M$_{12}$ perform much worse than M$_0$ and M$_{10}$.
This is because the semantic space adaption of AdaKI Gate in KaGR-MN can maintain the semantics consistency of $\mathbf{r_a^{t*}}$ and $\mathbf{M_C^{t-1}}$, which is crucial for subsequent knowledge contextualizing.



\subsection{Impact of Time Step Number $\mathbf{T}$ }
\begin{figure*}[th]
 \centering
 \includegraphics[width =\textwidth]{kagrmn-layer.pdf}
 \caption{Impact of the time step number $\mathbf{T}$}
 \label{fig: t-effect}
\end{figure*}
We plot the performance trends of KaGRMN-DSG with increasing $\mathbf{T}$ on the three datasets, as presented in Fig. \ref{fig: t-effect}.
We can observe that the performances show a trend of increases at first and then decreases.
And the best result is obtained when $\mathbf{T}$ is 2 or 3 for Res15 and 4 for Lap14 and Res14.
This shows that appropriately increasing $\mathbf{T}$ can gradually improve the results, which is consistent with our expectation.
This can also prove the effectiveness of the recurrent manner of KaGR-MN.
However, too large $\mathbf{T}$ leads to inferior performances, which is also consistent with our expectation. One possible explanation is that too much knowledge integrated into the aspect representation and context memories will harm their original contextual information.
Another is that too many recurrent steps will lead to overfitting on training sets.
\subsection{Case Study} \label{sec: casestudy}
\begin{table*}[t]
\fontsize{8.9}{12}\selectfont
\centering
\caption{Cases demonstration. [\textbf{N}, \textbf{P}, \textbf{O}] denotes predicted sentiment distribution: [Negative, Positive, Neutral]. }
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{l|l} \toprule
\textbf{Case} &  [\textbf{N}, \textbf{P}, \textbf{O}] \\ \hline
1. \tabincell{l}{$\mathbf{C}$: The \textbf{\textcolor{blue}{[Mountain Lion OS]}}$^\mathbf{A}$ is not hard to figure out if you are familiar with Microsoft Windows.\\ 
$\mathbf{D}$: OS X Mountain Lion is ... Apple Inc.'s desktop and server operating system ...}    &  \tabincell{l}{M$_0$: [0.0, \textbf{0.999}$^\checkmark$, 0.001] \\ M$_1$: [0.01, \textcolor{red}{0.49$^\times$}, 0.5]}        \\ \hline       
2. \tabincell{l}{$\mathbf{C}$: On start up it asks endless questions just so \textbf{\textcolor{blue}{[iTune]}}$^\mathbf{A}$ can sell you more of their products.\\ 
$\mathbf{D}$: iTunes is a media player, media library, Internet radio broadcaster, mobile device management utility ...}    &  \tabincell{l}{M$_0$: [\textbf{0.57}$^\checkmark$, 0.41, 0.02] \\ M$_1$: [0.03, \textcolor{red}{$0.67^\times$}, 0.30]}        \\ \hline
3. \tabincell{l}{$\mathbf{C}$: While the \textbf{\textcolor{blue}{[smoothies]}}$^\mathbf{A}$ are a little big for me, the fresh juices are the best i have ever had!\\ 
$\mathbf{D}$: A smoothie is a drink made from pureed raw fruit and/or vegetables, typically using a blender ...}    &  \tabincell{l}{M$_0$: [\textbf{0.62}$^\checkmark$, 0.0, 0.38] \\ M$_1$: [0.02, \textcolor{red}{$0.97^\times$}, 0.01]}        \\ \hline
4. \tabincell{l}{$\mathbf{C}$: All the various Greek and Cypriot dishes are excellent, but the \textbf{\textcolor{blue}{[gyro]}}$^\mathbf{A}$ is the reason to come -- if you \\ don't eat one your trip was wasted.\qquad
$\mathbf{D}$: A gyro or gyros is a Greek dish made from meat cooked on a ...}    &  \tabincell{l}{M$_0$: [0.02, \textbf{0.98}$^\checkmark$, 0.0] \\ M$_1$: [\textcolor{red}{0.88$^\times$}, 0.11, 0.01]} \\ \bottomrule
\end{tabular}}
\label{table: case}
\end{table*}
We show some cases in Table \ref{table: case}. Note that the only difference between KaGRMN-DSG (M$_0$) and M$_1$ is that the input $\mathbf{D}$ in M$_1$ is replaced with $\mathbf{A}$.
We can observe that M$_0$ can accurately predict the correct labels in all cases, while M$_1$ fails all cases although its overall performance is promising (as shown in Table \ref{table: ablation})


Without leveraging aspect knowledge, the aspect representation and semantics derived by M$_1$ are inadequate. 
As shown in Table \ref{table: entity}, BERT cannot capture the exact meanings and properties of \textit{Mountain Lion OS} and \textit{iTune}, although it is one of the strongest language models. 
In Case 1, M$_1$ regards \textit{Mountain Lion OS} as `lion' which is `dangerous'. Then considering `not hard', M$_1$ is confused on P and O.
In contrast, leveraging aspect knowledge, M$_0$ captures the exact meaning: an operating system.
Then considering the aspect-related semantics (`not hard'), M$_0$ correctly predicts P.
In Case 2, the aspect sentiment expression is a little obscure as there are no explicit sentiment trigger words (e.g. delicious, good, expensive).
Even if M$_1$ captures aspect-related context semantics, it fails due to the lack of property information of \textit{iTune}.
Thanks to the integrated aspect knowledge, M$_0$ is aware that \textit{iTune} is primarily used for media playing rather than selling products, thus correctly predicts N.



Looking into Case 3 and Case 4, we can find that due to the lack of aspect knowledge, M$_1$ is prone to be affected by some misleading sentiment trigger words: `best' in case 3 and `but' in case 4.
The reason why M$_0$ wins M$_1$ is that M$_0$ can combine the aspect knowledge and the aspect-related semantics together to capture the correct clues for ASC.
\begin{table}[t]
\fontsize{8}{10}\selectfont
\centering
\caption{Misunderstanding from BERT presented by semantic cosine similarity ($S$). $v$ is the average of entity's hidden states.
 $a_i$ denotes the $\mathbf{A}$ in case $i$.}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{c|c||c|c}\toprule
{Entity ($e$)} & $S(v_e, v_{a_1})$ &{Entity ($e$)} & $S(v_e, v_{a_2})$          \\ \midrule
lion     & 0.8516   &media player       &0.4720    \\
mountain           &0.7997  &   radio broadcaster& 0.5887        \\ 
operating system & 0.6826       & software & 0.7051        \\
dangerous animal & 0.8272 & utility & 0.6982\\
          \bottomrule
\end{tabular}}
\label{table: entity}
\end{table}

\subsection{Computation Time Analysis}
\begin{table}[t]
\fontsize{8}{10}\selectfont
\centering
\caption{Comparison of training time and inference time (per sample) as well as the avg F1 on the three datasets.}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{c|c|c|c}\toprule
Models & Training Time$\downarrow$ & Inference Time$\downarrow$ &Avg F1$\uparrow$ \\ \midrule
BERT-SPC & \textbf{0.007309}s &\textbf{0.002219}s   &    73.59\%    \\
BERT+T-GCN & 0.033835s & 0.003350s  &   76.22\%    \\ \hline
KaGRMN-DSG  &0.015333s & 0.004208s  &   \textbf{78.91}\%   \\ \bottomrule
\end{tabular}}
\label{table: time}
\end{table}
The comparison of time costing and avg F1 of BERT-SPC, BERT+T-GCN and our KaGRMN-DSG model is shown in Table \ref{table: time}.
We can find that although our model demands more training time and inference time than BERT-SPC, it overpasses BERT-SPC on avg F1 by a large margin (6.3\%).
As for BERT+T-GCN, which is the best-performing baseline, although it costs lightly less inference time than our KaGRMN-DSG, it costs much more time for training, and more importantly, its performance is significantly inferior to us.
Additionally, since \textit{Local Syntactic Information Modeling} and \textit{Global Relational Information Modeling} both take the output of KaGRMN as input, they can be parallelized theoretically, so the training time and inference time of our KaGRMN-DSG model can be further reduced in practice.
In a word, our model may cost more time for training and inference than some baseline models, but it is worthy considering the significant performance improvement.







