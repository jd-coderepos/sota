








\documentclass[journal]{IEEEtran}



\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[ruled]{algorithm2e}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{bm}
\usepackage{url}
\usepackage{xspace}
\usepackage{amsmath,amsfonts}
\usepackage{amsthm}
\usepackage{amssymb,amsopn}
\usepackage{bm} 

\newcommand{\vct}[1]{\boldsymbol{#1}} \newcommand{\mat}[1]{\boldsymbol{#1}} 

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\R}{\field{R}} \newcommand{\F}{\field{F}} \newcommand{\T}{^{\textrm T}} \newcommand{\TN}{^{-\textrm T}} 


\newcommand{\cst}[1]{\mathsf{#1}}

\newcommand{\inner}[2]{#1\cdot #2}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\twonorm}[1]{\left\|#1\right\|^2}
\newcommand{\Map}[1]{\mathcal{#1}}

\newcommand{\ProbOpr}[1]{\mathbb{#1}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{}\mkern2mu{#1#2}}}
\newcommand{\cind}[3]{{#1} \independent{#2}\,|\,#3}
\newcommand{\cndexp}[2]{\ProbOpr{E}\,[ #1\,|\,#2\,]}

\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}



\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}



\newcommand{\KL}[2]{D_{KL}[#1\|#2]}
\newcommand{\EP}[2]{\mathbb{H}{(#1,#2)}}
\newcommand{\ETP}[1]{\mathbb{H}{(#1)}}
\newcommand{\MI}[2]{\mathbb{I}(#1; #2)}
\newcommand{\CMI}[3]{I(#1; #2 | #3)}
\newcommand{\II}[3]{I(#1; #2 ; #3)}
\newcommand{\eat}[1]{}
 \usepackage{hyperref}
\usepackage{xcolor}
\usepackage{booktabs}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\ie}{\emph{i.e.}\xspace}
\newcommand{\eg}{\emph{e.g.}\xspace}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  



















\ifCLASSINFOpdf
\else
\fi



















































\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{SPICE: Semantic Pseudo-Labeling for \\ Image Clustering}


\author{Chuang Niu,~\IEEEmembership{Member,~IEEE,}
        Hongming Shan,~\IEEEmembership{Member,~IEEE,}
        and~Ge~Wang,~\IEEEmembership{Fellow,~IEEE}

\thanks{ C. Niu and G. wang are with Department of Biomedical Engineering, Center for Biotechnology and Interdisciplinary Studies, Rensselaer Polytechnic Institute, Troy, NY USA, 12180. E-mail: niuc@rpi.edu; wangg6@rpi.edu.}
\thanks{H. Shan is with the Institute of Science and Technology for Brain-inspired Intelligence and MOE Frontiers Center
for Brain Science, Fudan University, Shanghai 200433, China, and also with the Shanghai Center for Brain Science and Brain-Inspired
Technology, Shanghai 201210, China. Email: hmshan@fudan.edu.cn.}
\thanks{Manuscript received xx xx, 2022; revised xx xx, 2022.}}






\markboth{A Manuscript}{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}














\maketitle

\begin{abstract}
    The similarity among samples and the discrepancy among clusters are two crucial aspects of image clustering. However, current deep clustering methods suffer from inaccurate estimation of either feature similarity or semantic discrepancy.
    In this paper, we present a Semantic Pseudo-labeling-based Image ClustEring (SPICE) framework, which divides the clustering network into a feature model for measuring the instance-level similarity and a clustering head for identifying the cluster-level discrepancy.
    We design two semantics-aware pseudo-labeling algorithms, prototype pseudo-labeling and reliable pseudo-labeling, which enable accurate and reliable self-supervision over clustering.
    Without using any ground-truth label, we optimize the clustering network in three stages: 1) train the feature model through contrastive learning to measure the instance similarity;
    2) train the clustering head with the prototype pseudo-labeling algorithm to identify cluster semantics;
    and 3) jointly train the feature model and clustering head with the reliable pseudo-labeling algorithm to improve the clustering performance.
    Extensive experimental results demonstrate that SPICE achieves significant improvements (10\%) over existing methods and establishes the new state-of-the-art clustering results on six image benchmark datasets in terms of three popular metrics. 
    Importantly, SPICE significantly reduces the gap between unsupervised and fully-supervised classification; \eg there is only 2\% (91.8\% vs 93.8\%) accuracy difference on CIFAR-10. 
    Our code is made publically available at~\url{https://github.com/niuchuangnn/SPICE}.
\end{abstract}

\begin{IEEEkeywords}
Deep clustering, self-supervised learning, semi-supervised learning, representation learning.
\end{IEEEkeywords}






\IEEEpeerreviewmaketitle



\section{Introduction}
\IEEEPARstart{I}{mage} clustering aims to group images into different meaningful clusters without human annotations, and is an essential task in unsupervised learning with the applications in many areas. At the core of image clustering are the measurements of the similarity among samples (images) and the discrepancy among semantic clusters. 
Recently, deep learning based clustering methods  have achieved great progress thanks to the strong representation capability of deep neural networks. 

\begin{figure}[bt!]
    \centering
    \includegraphics[width=1\linewidth]{figures/similarity.pdf}
    \caption{Semantic relevance in the feature space. (a) Neighboring samples of different semantics, where the first image is the query image and the other images are nearest images with closest features provided by SCAN~\cite{scan}. (b) Instance similarity without semantics, where each point denotes a sample in the feature space, and white lines indicate similar samples. (c) Semantics-aware similarity, where different colors denote different semantic clusters, stars denote cluster centers, the points within large circles are similar to the cluster centers, and the points with yellow circles are semantically inconsistent to neighbor samples.  }
    \label{fig_general}
\end{figure}

Initially, by combining autoencoders with clustering algorithms, some deep clustering methods were proposed to learn representation features and perform clustering simultaneously and alternatively~\cite{Xie2016, LI2018161, DCN2016, DeepCluster2017, Zhang_2019_CVPR, DEPICT2017, VaDE2017, GMVAE, DASC2018}, achieving better results than the traditional methods. Due to the overestimation of low-level features, these autoencoder-based methods hardly capture discriminitative features of complex images. Thus, a number of methods were proposed to learn discriminitative label features under various constraints~\cite{DAIC2017,IIC2019,Wu_2019_ICCV,gatcluster,gatcluster, cc, idfd}. However, these methods have limited performance when directly using the label features to measure the similarity among samples. This is because the category-level features lose too much instance-level information to accurately measure the instance similarity.
Very recently, Van Gansbeke~\etal~\cite{scan} proposed to leverage the embedding features of unsupervised representation learning model to search for similar samples across the whole dataset, and then encourage a clustering model to output the same labels for similar instances, which further improved the clustering performance.
Considering the imperfect embedding features, the local nearest samples in the embedding space do not always have the same semantics especially when the samples lie around the borderlines between different clusters as shown in Fig.~\ref{fig_general}(a), which may compromise the performance.
Essentially, SCAN only utilizes the instance similarity for training the clustering model without explicitly exploring the semantic discrepancy between clusters, as shown in Fig.~\ref{fig_general}(b), so that it cannot identify the semantically inconsistent samples.


\begin{figure*}[bt!]
    \centering
    \includegraphics[width=1\textwidth]{figures/arcs.pdf}
    \caption{Training framework of different  deep clustering methods. (a) The initial deep clustering methods that combine traditional clustering algorithms with the deep neural networks, most of them combine with the autoencoders and some combine with an encoder only. (b) The label feature based methods that directly map images to cluster labels, where self-supervision is calculated based on the label features only. (c) A two-stage unsupervised classification method that first trains the feature model and then trains the whole classification work with a label feature based self-supervision and an embedding feature based contrastive loss simultaneously and separately. (d) The SCAN learning framework that constrains the similar samples in the embedding space having the same cluster label. (e) The proposed SPICE framework that synergizes both the similarity among samples and the discrepancy between clusters for training a clustering network through semantics-aware pseudo-labeling.}
    \label{fig_arcs}
\end{figure*}

To this end,  we propose a Semantic Pseudo-labeling-based Image ClustEring (SPICE) framework that synergizes the similarity among instances and semantic discrepancy between clusters to generate accurate and reliable self-supervision over clustering.
In SPICE, the clustering network is divided into two parts: a feature model and a subsequent clustering head, which is exactly a traditional classification network.
To effectively measure the instance similarity and the cluster discrepancy, we split the training process into three stages: 1) training the feature model; 2) training the clustering head; and 3) jointly training the feature model and clustering head. We highlight that there is no any annotations throughout the training process.
More specifically, in the first stage we adopt the self-supervised contrastive learning paradigm to train the feature model, which can accurately measure the similarity among instance samples. In the second stage, we  propose a prototype pseudo-labeling algorithm to train the clustering head in the expectation-maximization (EM) framework with the feature model fixed, which can take into account both the instance similarity and the semantic discrepancy for clustering. In the final stage, we propose a reliable pseudo-labeling algorithm to jointly optimize the feature model and the clustering head, which can boost both the clustering performance and the representation ability.

Compared with the instance similarity based method~\cite{scan}, the proposed prototype pseudo-labeling algorithm can leverage the predicted cluster labels, obtained from the clustering head, to identify cluster prototypes in the feature space, and then assign each prototype label to its neighbor samples for training the clustering head alternatively. Thus, the inconsistency among borderline samples can be avoided when the prototypes are well captured, as shown in Fig.~\ref{fig_general}(b).
On the other hand, given the predicted labels, the reliable pseudo-labeling algorithm can identify unreliable samples in the embedding space, as the yellow circles in Fig.~\ref{fig_general}(c), which will be filtered out during joint training. Therefore, the proposed SPICE can generate more accurate and reliable supervision by synergizing the similarity and discrepancy.







Our main contributions are summarized as follows.
\begin{enumerate}
\item  We propose a novel SPICE framework for image clustering, which can generate accurate and reliable self-supervision over clustering by synergizing the similarity among samples and the discrepancy between clusters.
\item We use the divide-and-conquer strategy to divide the clustering network into a feature model and a clustering head, and gradually train the feature model, clustering head, and both of them jointly, without using any annotations.
\item  We design a prototype pseudo-labeling algorithm to identity prototypes for training the clustering head in an EM framework, which can reduce the semantic inconsistency of the samples around borderlines.
\item  We design a reliable pseudo-labeling algorithm to select reliable samples for jointly training the feature model and clustering head, which effectively improves the clustering performance.
\item  Extensive experimental results demonstrate that SPICE outperforms the state-of-the-art clustering methods on common image clustering benchmarks by a large margin (10\%), closing the gap between unsupervised and supervised classification (down to 2\% on CIFAR10).
\end{enumerate}


\section{Related work}

In this section, we first analyze the deep image clustering methods systematically, and then briefly review the related unsupervised representation learning and semi-supervised classification methods.

\subsection{Deep Clustering}
\label{sec_udc}
Deep clustering methods have shown significant superiority over traditional clustering algorithms, especially in computer vision.
In a data-driven fashion, deep clustering can effectively utilize the representation ability of deep neural networks.
Initially, some methods were proposed to combine deep neural networks with traditional clustering algorithms, as shown in Fig.~\ref{fig_arcs}(a).
Most of these clustering methods combine the stacked auto-encoders (SAE)~\cite{SDAE2010} with the traditional clustering algorithms, such as k-means~\cite{DEN2014, Xie2016, DCN2016, DMC2017, LI2018161}, Gaussian mixture model~\cite{DeepCluster2017, VaDE2017, GMVAE}, spectral clustering~\cite{DSCN2017}, subspace clustering~\cite{DASC2018, Zhang_2019_CVPR}, and relative entropy constraint~\cite{DEPICT2017}. However, since the pixel-wise reconstruction loss of SAE tends to over-emphasize low-level features, these methods have inferior performance in clustering images of complex contents due to the lack of object-level semantics.
Instead of using SAE, Yang~\etal~\cite{Yang2016Joint} alternately perform the agglomerative clustering~\cite{Agglomerative} and train the representation model by enforcing the samples within a selected cluster and its nearest cluster having similar features while pushing away the selected cluster from its other neighbor clusters. However, the performance of JULE can be compromised by the errors accumulated during the alternation, and their successes in online scenarios are limited as they need to perform clustering on the entire dataset.

Recently, novel methods emerged that directly learn to map images into label features, which are used as the representation features during training and as the one-hot encoded cluster indices during testing~\cite{pami-c2, DAIC2017, dsec, Wu_2019_ICCV, IIC2019, Huang_2020_CVPR, gatcluster, scan, cc, pami-c1, idfd}, as shown in Fig.~\ref{fig_arcs}(b). Actually, these methods aim to train the classification model in the unsupervised setting while using multiple indirect loss functions, such as sample relations~\cite{DAIC2017}, invariant information~\cite{IIC2019, cc}, mutual information~\cite{Wu_2019_ICCV}, partition confidence maximisation~\cite{Huang_2020_CVPR}, attention~\cite{gatcluster}, and entropy~\cite{gatcluster, Huang_2020_CVPR, scan, cc}. 
Gupta~\etal~\cite{Gupta2020Unsupervised} proposed to train an ensemble of deep networks and select the predicted labels that a large number of models agree on as the high-quality labels, which are then used to train a ladder network~\cite{ladder} in a semi-supervised learning mode.
However, the performance of these methods may be sub-optimal when using such label features to compute the similarity and discrepancy between samples, as the category-level label features can hardly reflect the relations of instance-level samples accurately.

To improve the representation learning ability, a two-stage unsupervised classification method (TSUC)~\cite{tsuc} was proposed. In the first stage, the unsupervised representation learning model was trained for initialization. In the second stage, a mutual information loss~\cite{Wu_2019_ICCV} based on the label features and a contrastive loss based on the embedding features were simultaneously optimized for training the whole model, as shown in Fig.~\ref{fig_arcs}(c).
Although the better initialization and contrastive loss can help learn representation features, the supervision based on the similarity and the discrepancy are computed independently, and the end-to-end training without accurate discriminative supervision will even harm the representation features as analyzed in Section~\ref{sec_abl}.
In contrast, Van Gansbeke~\etal~\cite{scan} proposed a method called SCAN to use embedding features of the representation learning model for computing the instance similarity, based on which the label features are learned by encouraging similar samples to have the same label, as shown in Fig.~\ref{fig_arcs}(d).
Sharing the same idea with SCAN, NNM~\cite{Dang_2021_CVPR} was proposed to enhance SCAN by searching similar samples on both the entire dataset and the sub-dataset.
However, since the embedding features are not perfect, similar instances do not always have the same semantics especially when the samples lie near the borderlines of different clusters. Therefore, only using the instance similarity and ignoring the semantic discrepancy between clusters to guide model training may limit the clustering performance.
Recently, Park~\etal~\cite{Park_2021_CVPR} proposed an add-on module for improving the off-the-shelf unsupervised clustering method based on the semi-supervised learning~\cite{mixmatch} and label smoothing techniques~\cite{Li2020DivideMix, pmlr-v119-lukasik20a}.


Based on the comprehensive analysis of existing deep cluttering methods, we present a new framework for image clustering, as shown in Fig.~\ref{fig_arcs}(e), which can accurately measure the similarity among samples and the discrepancy between clusters for the model training, and effectively reduce the semantic inconsistency for similar samples.
In addition to the image clustering methods mentioned above, there are also  promising deep learning-based clustering methods focusing on special~\cite{TIP4, TIP5} or multi-view clustering datasets~\cite{TIP2, TIP3}.

\begin{figure*}[bt!]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/spice.pdf}
    \caption{Illustration of the SPICE framework. (a) Train the feature model with the contrastive learning based unsupervised representation learning. (b) Train the clustering head via the prototype pseudo-labeling algorithm in an EM framework. (c) Jointly train the feature model and the clustering head through the reliable pseudo-labeling algorithm.}
    \label{fig_framework}
\end{figure*}



\subsection{Unsupervised Representation Learning}
Unsupervised representation learning aims to map samples/images into semantically meaningful features without human annotations, which facilitates various down-stream tasks, such as object detection and classification. Previously, various pretext tasks were heuristically designed for this purpose, such as colorization~\cite{colorization2016}, rotation~\cite{rotation2018}, jigsaw~\cite{jigsaw2016}, etc.
Recently, contrastive learning methods combined with data augmentation strategies have achieved great success, such as SimCLR~\cite{simclr}, MOCO~\cite{He_2020_CVPR}, and BYOL~\cite{byol}, just to name a few.
On the other hand, clustering based representation learning methods have also achieved great progress.
Caron \etal~\cite{vf2018} proposed to alternatively performs k-means clustering algorithm on the entire dataset and train the classification network using the cluster labels.
Without explicitly computing cluster centers, Asano~\etal~\cite{Self-labelling} proposed a self-labeling approach that directly infers the pseudo-labels from the predicted cluster labels of the full dataset based on the Sinkhorn-Knopp algorithm~\cite{Sinkhorn}, and then uses the pseudo labels to the clustering network. 
Taking the advantages of contrastive learning, SwAV~\cite{caron2020swav} was proposed to simultaneously cluster the data while enforcing different transformations of the same image having the same cluster assignment.
It is worth emphasizing that, different from the unsupervised deep clustering methods in Section~\ref{sec_udc},  these clustering based representation learning methods aim to learn the representation features by clustering a much larger number of clusters than the number of ground-truth classes. This is consistent with our observation that directly clustering the target number of classes without accurate supervision will harm the representation learning due to the over-compression of instance-level features. Usually, to evaluate the quality of learned features, a linear classifier is independently trained with ground truth labels by freezing the parameters of representation learning models.

In this work, we aim to achieve unsupervised clustering with the exact number of real classes. On the other hand, we not only train the feature model but also the clustering head without using any annotations.
Actually, any unsupervised representation learning methods can be implemented as our feature model, which can be further improved via the joint training in SPICE.

\subsection{Semi-Supervised Classification}
Our method is also related to the semi-supervised classification methods as actually we reformulate the unsupervised clustering task into a semi-supervised learning paradigm in the joint training stage.
Semi-supervised classification methods aim to reduce the requirement of labeled data for training a classification model by providing a means of leveraging unlabeled data. In this category, remarkable results were obtained with consistency regularization~\cite{NIPS2016_30ef30b6, DBLP} that constrains the model to output the same prediction for different transformations of the same image, pseudo-labeling~\cite{pseudo} that uses confident predictions of the model as the labels to guide training processes, and entropy minimization~\cite{NIPS2004_96f2b50b, pseudo} that steers the model to output high-confidence predictions. MixMatch~\cite{mixmatch} algorithm combines these principles in a unified scheme and achieves an excellent performance, which is further improved by ReMixMatch~\cite{remixmatch} along this direction. Recently, FixMatch~\cite{fixmatch} proposed a simplified framework that uses the confident prediction of a weakly transformed image as the pseudo label when the model is fed a strong transformation of the same image, delivering superior results.

In this work, we target a more challenging task of training the clustering network without using any annotations, sometimes achieving comparable or even better results than the state-of-the-art semi-supervised learning methods.

\section{Method}
We aim to cluster a set of  images  into  classes by training a clustering network without using any annotations.
The clustering network can be conceptually divided into two parts: a feature model that maps images to feature vectors, , and a clustering head that maps feature vectors to the probabilities over  classes, , where  and  represent the trainable parameters of the feature model  and the clustering head , respectively.
Different from the existing deep clustering methods, we use the outputs of  the feature model to measure the similarity among samples and use the clustering head to identify the discrepancy between clusters for pseudo-labeling, as shown in Fig.~\ref{fig_arcs}.
By effectively measuring both the similarity and discrepancy, we design two semantics-aware pseudo-labeling algorithms, prototype pseudo-labeling and reliable pseudo-labeling, to generate accurate and reliable self-supervision.


Specifically, we split the network training into three stages as shown in Fig.~\ref{fig_framework}.
First, we optimize the feature model  through the instance-level contrastive learning that enforces the features from different transformations of the same image being similar and the features from different images being discriminative from each other. 
Second, we optimize the clustering head  with the proposed prototype pseudo-labeling algorithm while freezing the feature model learned in the first stage.
Third, we optimize the feature model and the clustering head jointly with the proposed reliable pseudo-labeling algorithm.


In the following subsections, we introduce each training stage in detail.



\subsection{Feature Model Training with Contrastive Learning}\label{sec_fea}
To accurately measure the similarity of instance samples, here we adopt the instance discrimination based unsupervised representation learning method~\cite{He_2020_CVPR} for training the feature model.
As shown in Fig.~\ref{fig_framework}(a), there are two branches taking two random transformations of the same image as inputs, and each branch includes a feature model and a projection head that is a two-layer multilayer perceptron (MLP).
During training, we only optimize  the lower branch while the upper branch is updated as the moving average of the lower branch.
As contrastive learning methods benefit from a large training batch, a memory bank~\cite{wu2018unsupervised} is used to maintain a queue of encoded negative samples for reducing the requirement of GPU memory size, which is denoted as , where  is the queue size. 

Formally, given two transformations  and  of an image , the output of the upper branch is , and the output of the lower branch is , where  denotes the projection head with the parameters , and  and  are the moving averaging versions of  and .
The parameters  and   are optimized with the following loss function:

where the negative sample  may be computed from any images other than the current image , and  is the temperature. Then, the parameters of the upper branch is updated as  , and , where  is a momentum coefficient.
The queue is updated by adding  to the end and removing the first item. All hyperparameters including  and  are the same as those in~\cite{He_2020_CVPR}.
The finally optimized feature model parameters are denoted as , which will be used in the next stage.

\noindent\emph{Remark.} In practice, any unsupervised representation learning methods and network architectures can be applied in the SPICE framework. 


\subsection{Clustering Head Training with Prototype Pseudo-Labeling}
\label{sec_proto}

Based on the trained feature model, here we train the clustering head by explicitly exploring both the similarity among samples and the discrepancy between clusters. 
Formally, given the image dataset  and the feature model parameters  obtained in Section~\ref{sec_fea}, we aim to train the clustering head only for predicting the cluster labels .
The clustering head  is a two-layer MLP mapping the features to the probabilities, , where .
However, in the unsupervised setting we do not have the ground truth for training.
To address this issue, we propose a prototype pseudo-labeling algorithm that alternatively estimates the pseudo labels of batch-wise samples and optimizes the parameters of the clustering head in an EM framework.

Generally, this training stage is to solve two sets of variables, i.e., the parameters of the clustering head , , and the cluster labels  of  over  clusters.
Analogous to k-means clustering algorithm~\cite{kmeans1967}, we solve two underlying sub-problems alternatively in an EM framework: the expectation (E) step is solving  given , and the maximization (M) step is solving  given .
Taking the advantages of contrastive learning, we clone the feature model into three branches as shown in Fig.~\ref{fig_framework}(b):
\begin{itemize}
\item The top branch takes original images as inputs and outputs the embedding features ;
\item The middle branch takes the weakly transformed images as inputs and estimates the probabilities  over  clusters, which is then combined with  to generate the pseudo labels  through the proposed prototype pseudo-labeling algorithm; 
\item The bottom branch takes strongly transformed images as inputs and optimizes  with the pseudo-labels. \end{itemize}





The EM process for the clustering head training is  detailed as follows. 

\noindent\textbf{Prototype Pseudo-Labeling (E-step).} The top branch computes the embedding features, , of a mini-batch of samples , and the middle branch computes the corresponding probabilities, , for the weakly transformed samples, ; here,  is mini-batch size,  is the dimension of the feature vector, and  denotes the weak transformation over the input image.


Given  and , the top confident samples are selected to estimate the prototypes for each cluster, and then the indices of the cluster prototypes are assigned to their nearest neighbors as the pseudo labels.
Formally, the top confident samples for each cluster, taking the -th cluster as an example, are selected as:

where  denotes the -th column of matrix  and  returns the top  confident sample indices from .
Naturally, the cluster centers  in the embedding space are computed as

By computing the cosine similarity between embedding features  and the cluster center , we select  nearest samples to , denoted by , to have the same cluster label, . Thus, a mini-batch of images with semantic pseudo-labels, , is constructed as



A toy example of the prototype pseudo-labeling process is shown in Fig.~\ref{fig_sl}, where there is a batch of 10 samples and 3 clusters, 3 confident samples for each cluster are selected according to the predicted probabilities to calculate the prototypes in the feature space, and 3 nearest samples to each cluster are selected and labeled. Note that there may exist overlapped samples between different clusters, so there are two options to handle these labels: one is the overlap assignment that one sample may have more than one cluster labels as indicated by the blue and red circles in Fig.~\ref{fig_sl}, and the other is non-overlap assignment that all samples have only one cluster label as indicated by the dished red circle. We found that the overlap assignment is better as analyzed in Section~\ref{sec_abl}.


\begin{figure}[bt!]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/semantic-label.pdf}
    \caption{A toy example for prototype pseudo-labeling. First, given the predicted probabilities of 10 samples over 3 clusters, top 3 confident samples are selected for each cluster, marked as green, blue, and red colors respectively. Then, the selected samples are mapped into the corresponding features (denoted by dots) to estimating the prototypes (denoted by stars) for each cluster, where stars are estimated with connected dots. Finally, the top 3 nearest samples to each cluster prototype (the dots within the same ellipse) are selected and assigned with the index of the corresponding prototype. Other unselected samples are signed with -1 and will not be used for training. The dashed ellipse denotes non-overlap assignment.}
    \label{fig_sl}
\end{figure}





\noindent\textbf{Training Clustering Head (M-step).} 
Given the labeled samples , the clustering head parameters are optimized  in a supervised learning manner.
Specifically, 
we compute the probabilities of strong transformations   in the forward pass, where  denotes the strong augmentation operator.
Then, the clustering head  can be optimized in the backward pass by minimizing the following cross-entropy (CE) loss:

where  and  
        , 
 is the cross-entropy loss function.

In Eq.~\eqref{eq_loss}, we use double softmax functions before computing CE loss, as  is already the outputs of a softmax function.
Considering that the pseudo-labels are not as accurate as the ground truth labels, the basic idea behind the double softmax implementation is to reduce the learning speed especially when the predictions are of low probabilities, which can benefit the dynamically clustering process during training 
(please see Appendix~A for the detailed analysis of the double softmax implementation).


The above process for training the clustering head is summarized in Algorithm~\ref{alg_1}.



\begin{algorithm}
\scriptsize
\label{alg_1}
\caption{Training Clustering Head.}
\LinesNumbered
\KwIn{Dataset , , , , , , , }
\KwOut{Cluster label  of }
Set feature model parameters to , , and initialize  \;
\While{ T}{

    \For{}{
            \textbf{\emph{E-step}:} \\
            Select  samples  from  as \;
Compute embedding features  \;
            Predict probabilities  \;
            Construct labeled image set  with Eqs.~\eqref{eq_conf},~\eqref{eq_center}, and~\eqref{eq_label} \;
            \textbf{\emph{M-step}:} \\


            
Compute probabilities  \;
            Optimize  by minimizing Eq.~\eqref{eq_loss} \;
            

    }
    
}

Select the best clustering head with the minimum loss as  \;

\ForEach{}{
         \;
        \;
}
\end{algorithm}

\noindent\emph{Remark.} In this stage, we fix the parameters of feature models from the representation learning, and only optimize the light-weight clustering head. Thus, the computational burden is significantly reduced so that we can train multiple clustering heads simultaneously and independently. By doing so, the instability of clustering from the initialization can be effectively alleviated through selecting the best 
clustering head. Specifically, the best head with the parameters  can be selected for the minimum loss value of  over the whole dataset; \ie, we set  and follow E-step and M-step in Algorithm~\ref{alg_1} to compute the loss value.
During testing, the best clustering head is used to cluster the input images into different clusters.




\subsection{Joint Training with Reliable Pseudo-Labeling}
\label{sec_semi}

The feature model and the clustering head are optimized separately so far, which tends to be a sub-optimal solution.
On the one hand, the imperfect feature model may lead to some similar features corresponding to really different clusters; thus, assigning neighbor samples with the same pseudo-label is not always reliable.
On the other hand, the imperfect clustering head may assign really dissimilar samples with the same cluster label, such that only using the predicted labels for fine-tuning is also not always reliable.
To overcome these problems, we design a reliable pseudo-labeling algorithm to train the feature model and clustering head  jointly for further improving the clustering performance.

\noindent\textbf{Reliable Pseudo-Labeling.}
Given the embedding features and the predicted labels  obtained in Section~\ref{sec_proto}, we select  nearest samples for each sample  according to the cosine similarity between embedding features. The corresponding labels of these nearest samples are denoted by . Then, the semantically consistent ratio  of the sample  is defined as

Given a predefined threshold , if , the sample  is identified as the reliably labeled for joint training, and otherwise the corresponding label is ignored.
Through the reliable pseudo-labeling, a subset of reliable samples  are selected as: 



\noindent\textbf{Joint Training.}
Given the above partially labeled samples, the clustering problem can be converted into a semi-supervised learning paradigm to train the clustering network jointly.
Here we adapt a simple semi-supervised learning method~\cite{fixmatch}.
During training, the subset of reliably labeled samples keep fixed.
On the other hand, all training samples should be consistently clustered, \ie, different transformations of the same image are constrained to have the consistent prediction.
To this end, as shown in Fig.~\ref{fig_framework}(c), the confidently predicted label of weak transformations is used as the pseudo-label for strong transformations of the same image.
Formally, the consistency pseudo label  of the sample  is calculated as in Eq.~\eqref{eq_cl}:

where , and  is the confidence threshold.



Then, the whole network parameters  and  are optimized with the following loss function:



where the first term is computed with reliably labeled samples  drawn from , and the second term is computed with pseudo-labeled samples  drawn from the whole dataset , which is dynamically labeled by thresholding the confident predictions as in Eq.~\eqref{eq_cl}.
 and  denote the numbers of labeled and unlabeled images in a mini-batch.

\noindent\emph{Remark.} 
Although we adapt the FixMatch~\cite{fixmatch} semi-supervised classification method for  image clustering in this work, we highlight that other semi-supervised algorithms can also be used here with reliable samples generated by the proposed reliable pseudo-labeling algorithm.



\emph{Note that SPICE sheds light on the importance of utilizing both instance-level similarity and semantic level discrepancy for clustering.
With this key idea in mind, this study focuses on developing the semantic pseudo-labeling algorithms that can actually estimate the instance similarity and semantic discrepancy for better clustering results.
Actually, SPICE presents a general unsupervised clustering framework that gradually trains the feature model, clustering head, and whole model end-to-end.
This framework is able to organically unify advanced unsupervised representation learning and semi-supervised learning methods for clustering through the proposed semantic pseudo-labeling method.}
 

\section{Experiments and Results}

\subsection{Benchmark Datasets and Evaluation Metrics}

\begin{table}[ht]
\footnotesize
\renewcommand{\arraystretch}{1.3}
\caption{Specifications and partitions of selected datasets.}
\label{table_data}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{lcccc}
\toprule
Dataset & Image size & \# Training & \# Testing & \# Classes (K) \\
\midrule
STL10         &    & 5,000      & 8,000   &  10   \\
CIFAR-10       &    & 50,000     & 10,000  &  10   \\
CIFAR-100-20   &    & 50,000     & 10,000  &  20   \\
ImageNet-10   &   & 13,000     & N/A    &  10   \\
ImageNet-Dog  &   & 19,500     & N/A    &  15   \\
Tiny-ImageNet &    & 100,000    & 10,000  &  200  \\
\bottomrule

\end{tabular}}
\end{table}

We evaluated the performance of SPICE on six commonly used image clustering datasets, including
STL10, CIFAR-10, CIFAR-100-20, ImageNet-10, ImageNet-Dog, and Tiny-ImageNet.
The key details of each dataset are summarized in Table~\ref{table_data}, where the datasets reflect a diversity of image sizes, the number of images, and the number of clusters.
Different existing methods used different image sizes for training and testing; for example, CC~\cite{cc} resizes all images of these six datasets into , and GATCluster~\cite{gatcluster} studies the effectiveness of different image sizes on ImageNet-10 and ImageNet-Dog, showing that too large or too small may harm the clustering performance.
In this work, we naturally use the original size of images without resizing to a larger size of images. For the ImageNet, we adopt the commonly used image size of .

Three popular metrics are used to evaluate clustering results, including Adjusted Rand Index (ARI)~\cite{hubert1985comparing}, Normalized Mutual Information (NMI)~\cite{strehl2002clusterensembles}, and clustering Accuracy (ACC)~\cite{LiD06}.


\begin{table*}[t]
\footnotesize
\renewcommand{\arraystretch}{1.3}
\renewcommand\tabcolsep{3.6pt}
\caption{Comparison with competing methods that were trained and tested on the whole dataset  (train and test split datasets are merged as one). Note that SPICE was trained on the target datasets only without using extra data and without using any annotations, which are the exactly the same as data used in the existing methods. The best results are highlighted in \textbf{bold}.}
\label{table_results_cluster}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{ rcccccccccccccccccc}
\toprule

\multirow{2}{*}{Method}             & \multicolumn{3}{c}{STL10}&\multicolumn{3}{c}{ImageNet-10}&\multicolumn{3}{c}{ImageNet-Dog-15}&\multicolumn{3}{c}{CIFAR-10}&\multicolumn{3}{c}{CIFAR-100-20}&\multicolumn{3}{c}{Tiny-ImageNet-200}\\
                                                                        \cline{2-19}
                                                                        & ACC  & NMI  & ARI& ACC&NMI&ARI   & ACC&NMI&ARI  &ACC&NMI&ARI   &ACC&NMI&ARI  &ACC&NMI&ARI\\
\midrule
k-means~\cite{kmeans1967}                & 0.192  & 0.125 & 0.061   & 0.241 &0.119& 0.057   & 0.105 &0.055&0.020   & 0.229 &0.087&0.049   & 0.130 &0.084&0.028   & 0.025 &0.065&0.005\\
SC~\cite{Spectral2002}                   & 0.159  & 0.098 & 0.048   & 0.274 &0.151&0.076    & 0.111 &0.038&0.013   & 0.247 &0.103&0.085   & 0.136 &0.090&0.022   & 0.022 &0.063&0.004\\
AC~\cite{Pasi2006Fast}                   & 0.332  & 0.239 & 0.140   & 0.242 &0.138&0.067    & 0.139 &0.037&0.021   & 0.228 &0.105&0.065   & 0.138 &0.098&0.034   & 0.027 &0.069&0.005\\
NMF~\cite{NMF}                           & 0.180  & 0.096 & 0.046   & 0.230 &0.132&0.065    & 0.118 &0.044&0.016   & 0.190 &0.081&0.034   & 0.118 &0.079&0.026   & 0.029 &0.072&0.005\\
AE~\cite{Bengio2007Greedy}               & 0.303  & 0.250 & 0.161   & 0.317 &0.210&0.152    & 0.185 &0.104&0.073   & 0.314 &0.239&0.169   & 0.165 &0.100&0.048   & 0.041 &0.131&0.007\\
SDAE~\cite{SDAE2010}                     & 0.302  & 0.224 & 0.152   & 0.304 &0.206&0.138    & 0.190 &0.104&0.078   & 0.297 &0.251&0.163   & 0.151 &0.111&0.046   & 0.039 &0.127&0.007\\
DCGAN~\cite{dcgan}                       & 0.298  & 0.210 & 0.139   & 0.346 &0.225&0.157    & 0.174 &0.121&0.078   & 0.315 &0.265&0.176   & 0.151 &0.120&0.045   & 0.041 &0.135&0.007\\
DeCNN~\cite{Zeiler2010Deconvolutional}   & 0.299  & 0.227 & 0.162   & 0.313 &0.186&0.142    & 0.175 &0.098&0.073   & 0.282 &0.240&0.174   & 0.133 &0.092&0.038   & 0.035 &0.111&0.006\\
VAE~\cite{vae}                           & 0.282  & 0.200 & 0.146   & 0.334 &0.193&0.168    & 0.179 &0.107&0.079   & 0.291 &0.245&0.167   & 0.152 &0.108&0.040   & 0.036 &0.113&0.006\\
JULE~\cite{Yang2016Joint}                & 0.277  & 0.182 & 0.164   & 0.300 &0.175&0.138    & 0.138 &0.054&0.028   & 0.272 &0.192&0.138   & 0.137 &0.103&0.033   & 0.033 &0.102&0.006\\
DEC~\cite{Xie2016}                       & 0.359  & 0.276 & 0.186   & 0.381 &0.282&0.203    & 0.195 &0.122&0.079   & 0.301 &0.257&0.161   & 0.185 &0.136&0.050   & 0.037 &0.115&0.007\\
DAC~\cite{DAIC2017}                      & 0.470  & 0.366 & 0.257   & 0.527 &0.394&0.302    & 0.275 &0.219&0.111   & 0.522 &0.396&0.306   & 0.238 &0.185&0.088   & 0.066 &0.190&0.017\\
DeepCluster~\cite{vf2018}                & 0.334  & N/A   &  N/A    & N/A &N/A&N/A          & N/A &N/A&N/A         & 0.374& N/A   &  N/A  & 0.189 & N/A &  N/A   & N/A &N/A&N/A\\
DDC~\cite{DDC2019}                       & 0.489  & 0.371 & 0.267   & 0.577 &0.433&0.345    & N/A &N/A&N/A         & 0.524 &0.424&0.329   & N/A &N/A&N/A         & N/A &N/A&N/A\\
IIC ~\cite{IIC2019}                      & 0.610  & N/A   &  N/A    & N/A &N/A&N/A          & N/A &N/A&N/A         & 0.617& N/A & N/A     & 0.257& N/A & N/A     & N/A &N/A&N/A\\
DCCM~\cite{Wu_2019_ICCV}                 & 0.482  & 0.376 & 0.262   & 0.710 &0.608 &0.555   & 0.383&0.321 &0.182   & 0.623& 0.496&0.408   & 0.327 &0.285&0.173   & 0.108 &0.224&0.038\\
DSEC~\cite{dsec}                         & 0.482  & 0.403 & 0.286   & 0.674 &0.583&0.522    & 0.264 &0.236&0.124   & 0.478 &0.438&0.340   & 0.255 &0.212&0.110   & 0.066 &0.190&0.017\\
GATCluster~\cite{gatcluster}             & 0.583  & 0.446 & 0.363   & 0.762 &0.609 &0.572   & 0.333&0.322 & 0.200  & 0.610&0.475 &0.402   & 0.281 &0.215&0.116   & N/A &N/A&N/A\\
PICA~\cite{Huang_2020_CVPR}              & 0.713  & 0.611 & 0.531   & 0.870 &0.802 &0.761   & 0.352&0.352 & 0.201  & 0.696&0.591 &0.512   & 0.337 &0.310&0.171   & 0.098 &0.277&0.040\\
CC~\cite{cc}                             & 0.850  & 0.746 & 0.726   & 0.893 &0.859 &0.822   & 0.429&0.445 & 0.274  & 0.790&0.705 &0.637   & 0.429 &0.431&0.266   & 0.140 &0.340&0.071\\
IDFD~\cite{idfd}                         & 0.756  & 0.643  & 0.575  & 0.954 &0.898 &0.901   & 0.591&0.546 & 0.413  & 0.815& 0.711 &0.663  & 0.425 &0.426&0.264   & N/A &N/A&N/A\\
\hline
\textbf{SPICE}                  & 0.908  & 0.817 & 0.812   & 0.921 &0.828 &0.836 & 0.646&0.572&0.479   & 0.838&0.734  &0.705    & 0.468 &0.448&0.294      & \textbf{0.305} &\textbf{0.449}&\textbf{0.161}\\
\textbf{SPICE}                        & \textbf{0.938}&\textbf{0.872}&\textbf{0.870} & \textbf{0.959} &\textbf{0.902} &\textbf{0.912} & \textbf{0.675} & \textbf{0.627} & \textbf{0.526}    & \textbf{0.926}&\textbf{0.865}&\textbf{0.852}   &\textbf{0.538}& \textbf{0.567}&\textbf{0.387}     & N/A &N/A&N/A\\
\bottomrule










\end{tabular}
}
\end{table*}





\subsection{Implementation Details}
For fair comparison, we mainly adopted two backbone networks, \ie ResNet18 and ResNet34~\cite{ResNet2015}, for representation learning. 
The classification head in SPICE consists of two fully-connected layers; \ie, D-D-K, where  and  are the dimension of features and the number of clusters, respectively.
Specifically,  for both ResNet18 and ResNet34 backbone networks, and the cluster number  is predefined as the number of classes on the target dataset as shown in Table~\ref{table_data}.
To show how the joint training (third stage) improves the clustering performance, we refer to SPICE without joint training as SPICE, where
the subscript  indicates the separate training.


For representation learning, we use MoCo-v2~\cite{He_2020_CVPR} in all our experiments, which was also used in SCAN~\cite{scan}.
For weak augmentation, a standard flip-and-shift augmentation strategy is implemented as in FixMatch~\cite{fixmatch}.
For strong augmentation, we adopt the same strategies used in SCAN~\cite{scan}. Specifically, the images were strongly augmented by composing Cutout~\cite{cutout} and four randomly selected transformations from RandAugment~\cite{randaugent}.

In SPICE, we use 10 clustering heads, and select the best head with the minimum loss for the final head in each trial.
To select the reliably labeled images in SPICE through reliable pseudo-labeling, we empirically set  and .
The hyperparameters involved in FixMatch~\cite{fixmatch} keep the same as those in the original paper, including .
We set the batch size  to 1,000.




\subsection{Clustering Performance Comparison}
\label{sec_resutls_clustering}
The existing methods can be divided into two groups according to their training and testing settings.
One is to train and test the clustering model on the whole dataset combining the train and test splits as one.
The other is to train and test the clustering model on the separate train and test datasets.
For fair comparison, the proposed SPICE is evaluated under both two settings and compared with the existing methods accordingly.


Table~\ref{table_results_cluster} shows the comparison results of clustering on the whole dataset.
In reference to the recently developed methods~\cite{IIC2019, gatcluster, cc}, the same backbone, \ie ResNet34, was used during learning the feature model and clustering head. Duplicating the original settings in FixMatch~\cite{fixmatch}, we used WideResNet-28-2 for CIFAR-10, WideResNet-28-8 for CIFAR-100-20, and WideResNet-37-2 for STL-10. For ImageNet-10, ImageNet-Dog, and Tiny-ImageNet datasets that were not used in FixMatch, we simply used the same ResNet34 during joint learning.
The results show that SPICE improves ACC, NMI, and ARI by 8.8\%, 12.6\%, and 14.4\% respectively over the previous best results that were recently reported by CC~\cite{cc} on STL10. On average, our proposed method also improves ACC, NMI, and ARI by about 10\% on ImageNet-Dog-15, CIFAR-10, CIFAR-100-20, and Tiny-ImageNet-200.
It is worth emphasizing that, without the joint training stage, SPICE still performs better than the exiting deep clustering methods using the same network architecture on the most of the datasets.
\emph{These results convincingly show the superior performance of the proposed method using exactly the same backbone networks and datasets.}
The final joint training results are obviously better than those from the separate training on all datasets, especially on CIFAR-10 (improved by 8.8\% for ACC) and CIFAR-100-20 (improved by 7.0\% for ACC).
In clustering the images in Tiny-ImageNet-200, although our results are significantly better than the existing results, while still very low. This is mainly due to the class hierarchies; \ie, some classes share the same supper class, as analyzed in~\cite{scan}. Due to the low performance, some clusters cannot be reliably labeled based on the reliable pseudo-labeling algorithm so that end-to-end training cannot be applied for further boosting clustering performance. Thus, it is still an open problem for clustering a large number of hierarchical clusters.




\begin{table}[t]
\footnotesize
\renewcommand{\arraystretch}{1.3}
\renewcommand\tabcolsep{1.5pt}
\caption{Comparison with competing methods on split datasets (training and testing images are mutually exclusive). For fair comparison, both SCAN and SPICE used MoCo for feature learning, and ResNet18 as backbone in all training stages. Here the best results for all methods were used for comparison. SPICE used the ResNet34 as backbone. The best two unsupervised results are highlighted in \textbf{bold}.}
\label{table_results_cls}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{lccccccccc}
\toprule

\multirow{2}{*}{Method}             & \multicolumn{3}{c}{STL10}&\multicolumn{3}{c}{CIFAR-10}&\multicolumn{3}{c}{CIFAR-100-20}\\
                                                                        \cline{2-10}
                                    & ACC  & NMI  & ARI   &ACC&NMI&ARI   &ACC&NMI&ARI \\
\midrule
ADC~\cite{ADC2019}                & 0.530 & N/A &N/A            & 0.325 & N/A& N/A     &0.160  &N/A&N/A        \\
TSUC~\cite{tsuc}                  & 0.665  & N/A & N/A         & 0.810&N/A &N/A       & 0.353 &N/A&N/A  \\
NNM~\cite{Dang_2021_CVPR}                   & 0.808  & 0.694 & 0.650      & 0.843&0.748 &0.709   & 0.477 &0.484&0.316  \\
SCAN~\cite{scan}                  & 0.809  & 0.698 & 0.646      & 0.883&0.797 &0.772   & 0.507 &0.486&0.333  \\
RUC~\cite{Park_2021_CVPR}              & 0.867  & N/A & N/A          & 0.903&N/A &N/A       & 0.533 &N/A&N/A  \\

SCAN~\cite{scan}                           & 0.855  & 0.758 & 0.721      & 0.874&0.786 &0.756   & 0.455 &0.472&0.310   \\



\hline
SPICE               & 0.862  & 0.756 & 0.732          & 0.845&0.739 &0.709   & 0.468 & 0.457 & 0.321 \\
\textbf{SPICE}                    & \textbf{0.920}  & \textbf{0.852} & \textbf{0.836}         & \textbf{0.918} & \textbf{{}0.850} & \textbf{0.836}  & \textbf{0.535} & \textbf{0.565} & \textbf{0.404} \\

\textbf{SPICE}                   & \textbf{0.929}  & \textbf{0.860} & \textbf{0.853}   & \textbf{0.917} & \textbf{0.858} & \textbf{0.836}  & \textbf{0.584} & \textbf{0.583} & \textbf{0.422}      \\

\hline

Supervised                        & 0.806 & 0.659 & 0.631        & 0.938 & 0.862 & 0.870  & 0.800 & 0.680 & 0.632 \\
\bottomrule

\end{tabular}}
\end{table}
Table~\ref{table_results_cls} shows the comparison results of clustering on the split train and test datasets.
For fair comparison, we re-implement SCAN with MoCo~\cite{He_2020_CVPR} for representation learning, denoted as SCAN.
It can be seen that the results of SCAN on SLT10 are obviously better than SCAN, while the performance on CIFAR-10 and CIFAR-100-20 drops slightly.
Compared with the baseline method SCAN, SPICE improves ACC, NMI, and ARI by 6.5\%, 9.4\%, and 11.5\% on STL10, by 4.4\%, 6.4\%, and 8.0\% on CIFAR-10, and by 8.0\%, 9.3\%, and 9.4\% on CIFAR-100-20, under the exactly same setting.
Without joint learning, SPICE already performs better than SCAN that contains the pretraining, clustering, and finetuning stages on STL10 and CIFAR100-20.
Moreover, we evaluate the SPICE using larger backbone networks as used in the whole dataset setting, the results on STL10 and CIFAR-10 are very similar while the results on CIFAR-100-20 are significantly improved.
Remarkably, SPICE significantly reduces the gap between unsupervised and supervised classification. On the STL10 that includes both labeled and unlabeled images, the results of unsupervised methods are better than the supervised as the unsupervised methods can leverage the unlabeled images for representation learning while the supervised cannot.
On CIFAR-10 and CIFAR-100-20, all methods using the same images for training and the proposed SPICE further reduces the performance gap compared with the supervised counterpart, particularly, only 2\% for ACC gap on CIFAR-10.

\begin{table}[ht]
\footnotesize
\renewcommand{\arraystretch}{1.2}
\renewcommand\tabcolsep{6.8pt}
\caption{More detailed comparison results on STL10. Here all methods were trained and tested on the split train and test datasets respectively. Both the mean and standard deviation results were reported. Each method was conducted five times. Here all methods used the ResNet18 backbone, SCAN and SPICE used MoCo for feature learning with STL10 images only. SCAN means no self-labeling.}
\label{table_results_detail}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{lccc}

\toprule

Method                          & ACC&NMI&ARI\\

\midrule
Supervised                      & 0.806 & 0.659 & 0.631    \\

MoCo+k-means                  & 0.7970.046 & 0.7680.021 & 0.6240.041     \\

SCAN                        & 0.7870.036 & 0.6970.026 & 0.6390.041    \\


SCAN                        & 0.7970.034 & 0.7010.032 & 0.6490.044    \\

SPICE                      & 0.8520.011 & 0.7490.008 & 0.7190.015    \\

SPICE                           & 0.9180.002 & 0.8490.003 & 0.8360.002   \\
\bottomrule
\end{tabular}}
\end{table}
Table~\ref{table_results_detail} provides more detailed comparison results on STL10, where all these three methods use MoCo~\cite{He_2020_CVPR} for representation learning and were conducted five times for computing the mean and standard deviation of the results.
Compared with SCAN that explores the instance similarity only without fine-tuning, SPICE explicitly leverages both the instance similarity and semantic discrepancy for learning clusters. On the other hand, different from k-means that infers the cluster labels with cluster centers, SPICE uses the non-linear clustering head to predict the cluster labels.
It can be seen that SPICE is significantly better than MoCo+k-means and SCAN  in terms of both the mean and standard deviation metrics, demonstrating the superiority of the proposed prototype pseudo-labeling algorithm.
Also, the final results of SPICE is significantly better than those of SCAN in terms of mean performance and the stability.

Overall, our comparative results systematically demonstrate the superiority of the proposed SPICE method on both the whole and split dataset settings.








\subsection{Semi-Supervised Classification Comparison}

\begin{table*}[ht]
\renewcommand{\arraystretch}{1.3}
\caption{Comparison results with semi-supervised learning on STL10 and CIFAR-10. The best three results are in \textbf{bold}.}
\label{table_results_semi}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{ccccccccccc}

\toprule
\multirow{3}{*}{Method}             & \multicolumn{7}{c}{Semi-supervised}&&\multicolumn{2}{c}{Unsupervised}\\
                                                                        \cline{2-8}\cline{10-11}
                          & \tabincell{c}{-Model\\ \cite{ladder}}& \tabincell{c}{Pseudo-Labeling\\\cite{pseudo-label}}& \tabincell{c}{Mean Teacher\\\cite{mean-teacher}}& \tabincell{c}{MixMatch\\\cite{mixmatch}}&\tabincell{c}{UDA\\\cite{UDA}}&\tabincell{c}{ReMixMatch\\\cite{remixmatch}}&\tabincell{c}{FixMatch\\\cite{fixmatch}}&& \tabincell{c}{SCAN\\\cite{scan}} & \tabincell{c}{SPICE\
    p_k = \frac{\exp(a_k)}{\sum_{j=1}^K \exp(a_j)},

    p'_k = \frac{\exp(p_k)}{\sum_{j=1}^K \exp(p_j)}.

    \mathcal{L}(\vct{y}, \vct{p}) = -\sum_{k=1}^K y_{k} \log(p_k),

    \label{eq_a1}
    \frac{\partial \mathcal{L}(\vct{y}, \vct{p})}{\partial a_k} = p_k - y_k.

    \label{eq_a2}
    \frac{\partial \mathcal{L}(\vct{y}, \vct{p}')}{\partial a_k} &= \sum_{j=1}^{K} \frac{\partial \mathcal{L}(\vct{y}, \vct{p}')}{\partial p_j} \frac{\partial p_j}{\partial a_k} =\sum_{j=1}^{K}(p'_j-y_j)p_j(\delta_{jk}-p_k),

    \label{eq_a4}
    r_k = \frac{\sum_{j=1}^{K}(p'_j-y_j)p_j(\delta_{jk}-p_k)}{p_k - y_k}

    \label{eq_a5}
    r_c &= \frac{\sum_{j=1}^{K}(p'_j-1)p_j(\delta_{jc}-p_c)}{p_c - 1} \\
    \label{eq_a6}
        & = \sum_{j=1}^K (1-p'_j)p_j - \sum_{j=1, j\ne c}^K \frac{(1-p'_j)p_j}{1-p_c} \\
        \label{eq_a7}
        & = (1-p'_c)p_c + \frac{p_c}{1-p_c} \sum_{j=1, j\ne c}^K (1-p'_j)p_j.

    \label{eq_a8}
    r_k &= \frac{\sum_{j=1}^{K}(p'_j-0)p_j(\delta_{jk}-p_k)}{p_k - 0} \\
    \label{eq_a9}
        & = p'_k - \sum_{j=1}^K p'_j p_j 

According to Eq.~\eqref{eq_a9}, we have  as the second term is a constant for a specific prediction , note that here the index  is the variable. 
\end{itemize}
Thus, the overall learning speed for the double softmax is adaptively reduced by a foctor of  compared with that for the normal single softmax. Importantly, when , the learning speed for smaller  is relatively slower than that for larger probabilities.
In this way, the double softmax implementation will prevent the low probabilities from being too small after the optimization with current pseudo-labels, which will benefit the training process given imperfect pseudo-labels.
In another angle, the relatively larger learning speed for the false high probability will accelerate the sample changing between different clusters during training, which will benefit the dynamically searching process for good clusters.

The corresponding numerical simulation results are shown in Fig. \ref{fig_dsce}, where there are  clusters and the pseudo-label is .
We can see that, except , the larger predicted probability  corresponds to the relatively larger ratio . For , although  is small,  is relatively large as there is a big difference between the prediction and the pseudo-label.
For the false high-probability, i.e. ,  is the largest among other ratios. These simulation results are consistent with the above analytical results.

\begin{figure*}[hbt!]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/dsce.pdf}
    \caption{Simulation results for double softmax mechanism.}
    \label{fig_dsce}
\end{figure*}


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi







\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,./spice}



















\end{document}
