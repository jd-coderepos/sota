\section{Results}
\label{sec:results}
In this section, we present the empirical evaluation of our models on many image understanding tasks.
We evaluate both global and local image representations, on category and instance-level recognition, semantic segmentation, monocular depth prediction, and action recognition. 
We detail the list of benchmarks in Appendix \ref{app:benchmarks_list}.
The goal of this evaluation is twofold.
First, we show that our self-supervised features outperform the current state of the art by a very large margin.
Second, we show that they match, or surpass the performance of weakly-supervised ones on a substantial number of tasks.


\myparagraph{Baselines.} 
In our comparisons, we use two kinds of models as baselines.
We compare to the best performing self-supervised models that are openly available.
First, we run our evaluations for MAE~\citep{he2021masked}, DINO~\citep{caron2021emerging}, SEERv2~\citep{goyal2022vision}, MSN~\citep{assran2022masked}, EsViT~\citep{li2021efficient}, Mugs~\citep{zhou2022mugs} and iBOT~\citep{zhou2021ibot}.
When several architectural variants were proposed for a given method, we report results for the one that leads to best top-1 accuracy on ImageNet-1k.
Second, we report performance of open-source weakly-supervised models such as CLIP~\citep{radford2021learning}, OpenCLIP~\citep{ilharco_gabriel_2021_5143773}, and SWAG~\citep{singh2022revisiting}.
When evaluating models on ImageNet-1k, we report the performance for each of the aforementioned methods. 
For all other evaluations, we report the four best-performing models amongst SSL ones.
Also, for reference, we report the best performing OpenCLIP-G for weakly-supervised ones.

\subsection{ImageNet Classification}
\label{sec:imagenet}
As a first evaluation, we probe the quality of the holistic image representation produced by the model on the ImageNet-1k classification dataset.
We evaluate the quality of features by training a simple classifier over a frozen backbone, and do not perform finetuning of the backbone weights.
Following previous work, we use a linear model for simplicity, ensuring a reproducible evaluation, despite the fact that classes may not be linearly separable.
Because most SSL methods were developped using ImageNet-1k validation performance as a debugging signal, we also report the top-1 accuracy on ImageNet-ReaL and ImageNet-V2.
In order to report this additional validation performance, for all models, we run the evaluation with our code.
We compare our frozen features to the best publicly available SSL features in Table~\ref{tab:lin-inet1k}, regardless of architecture or pretraining data. 
We see the components proposed in this work lead to a very significant improvement () over the previous state of the art (iBOT ViT-L/16 trained on ImageNet-22k) on linear evaluation.
At the same time, we also see that the performance increase on the alternative test sets is larger for our method, indicating stronger generalization. We describe details of our linear evaluation in Appendix \ref{app:linearprobing}.

\begin{table}[t]
  \centering
  \begin{tabu}{@{} lll c cc cccc@{}}
    \toprule
    &&&&& kNN && \multicolumn{3}{@{}c@{}}{linear} \\
    \cmidrule{6-6}\cmidrule{8-10}
    Method & Arch. & Data & Text sup. && val && val & ReaL & V2 \\
    \midrule
\multicolumn{10}{@{}c@{}}{\textbf{Weakly supervised }}\\
\midrule
    CLIP        & ViT-L/14                & WIT-400M & \checkmark  && 79.8 && 84.3 & 88.1 & 75.3 \\
    CLIP        & ViT-L/14 & WIT-400M & \checkmark  && 80.5 && 85.3 & 88.8 & 75.8 \\
    SWAG     & ViT-H/14                & IG3.6B   & \checkmark  && 82.6 && 85.7 & 88.7   & 77.6   \\
    OpenCLIP & ViT-H/14   & LAION    & \checkmark  && 81.7 && 84.4 & 88.4 & 75.5 \\
    OpenCLIP & ViT-G/14   & LAION    & \checkmark  && 83.2 && 86.2 & 89.4 & 77.2 \\
    EVA-CLIP & ViT-g/14   & custom  & \checkmark && \bf 83.5 && 86.4 & 89.3 & 77.4 \\
    \midrule
\multicolumn{10}{@{}c@{}}{\textbf{Self-supervised }}\\
    \midrule
    MAE     & ViT-H/14                & INet-1k  & \ding{53} && 49.4 && 76.6 & 83.3 & 64.8 \\
    DINO    & ViT-S/8                 & INet-1k  & \ding{53} && 78.6 && 79.2 & 85.5   & 68.2 \\
    SEERv2  & RG10B                   & IG2B     & \ding{53} && -- && 79.8 & --   & -- \\
    MSN     & ViT-L/7                 & INet-1k  & \ding{53} && 79.2 && 80.7 & 86.0   & 69.7 \\ 
    EsViT   & Swin-B/W=14             & INet-1k  & \ding{53} && 79.4 && 81.3 & 87.0 & 70.4 \\
    Mugs    & ViT-L/16                & INet-1k  & \ding{53} && 80.2 && 82.1 & 86.9 & 70.8 \\
    iBOT    & ViT-L/16                & INet-22k & \ding{53} && 72.9 && 82.3 & 87.5   & 72.4 \\
    \midrule
    \multirow{4}{*}{\OURS}  & ViT-S/14 & \LaViDa & \ding{53} && 79.0 && 81.1 & 86.6 & 70.9 \\ 
                            & ViT-B/14 & \LaViDa & \ding{53} && 82.1 && 84.5 & 88.3 & 75.1 \\ 
                            & ViT-L/14 & \LaViDa & \ding{53} && \bf 83.5 && 86.3 & 89.5 & 78.0 \\ 
                            & ViT-g/14 & \LaViDa & \ding{53} && \bf 83.5 && \bf 86.5 & \bf 89.6   & \bf 78.4   \\ 
    \bottomrule
  \end{tabu}
  \caption{
    \textbf{Linear evaluation on ImageNet-1k of frozen pretrained features.}
    We report Top-1 accuracy on the validation set for publicly available models trained on public or private data, and with or without text supervision (text sup.). For reference, we also report the kNN performance on the validation set.
    We compare across any possible architectures (Arch.), at resolution  unless stated otherwise.
    The dataset used for training EVA-CLIP is a custom mixture, see paper for details~\citep{fang2022eva}.
  }  
  \label{tab:lin-inet1k}
\end{table}

\paragraph{How far are we from weakly-supervised models?}
We also want to validate that our features are competitive with state-of-the-art open-source weakly supervised models.
To this end, we compare on ImageNet-1k, using the linear evaluation, to three off-the-shelf methods with several architectural variants.
For all models, we run the linear evaluation using our code, after making sure that our numbers match those reported in technical reports and papers.
We show the result of this evaluation in Table~\ref{tab:lin-inet1k}.
We see that our backbone, surpases the performance of OpenCLIP with a ViT-G/14 architecture () and EVA-CLIP with a ViT-g/14 ().
At the same time, we also observe that our performance on the ImageNet-V2 test set is significantly better ( versus EVA-CLIP), indicating better generalization.
For the remainder of this section, we report OpenCLIP-G as a reference for weakly-supervised models.

\paragraph{Can we finetune the encoders?}
We question if the ability of our models to produce high quality frozen features impact their performance when finetuned with supervision on a specific dataset.
While this is not core to this paper, this experiment is indicative of whether we have involuntarily specialized our models to the setting of linear evaluations of frozen features.
To run this sanity check, we apply the finetuning pipeline from~\citet{touvron2022deit}, without tweaking hyper-parameters.
In Table~\ref{tab:ft-inet1k-alone}, we show that the Top-1 accuracy on the validation set of ImageNet-1k improves by more than  when the backbone is finetuned.
This is true both when using models at resolution  and .
Further gains can be obtained by tuning the hyper-parameters of the finetuning, but this is beyond the goal of this sanity check.
Nonetheless, our best finetuned performance () is only a couple of percent below () the absolute state of the arts (), obtained by~\citet{chen2023symbolic}.
As \OURS{} leads to features that are strong in both the linear and finetuning settings, a strong property of our approach is that \textit{finetuning is optional}.

\begin{table}[ht]
  \centering
  \begin{tabular}{@{}l cccc@{}}
    \toprule
     Arch. & Res. & Linear & Finetuned &  \\
    \midrule
     \multirow{2}{*}{ViT-g/14} & 224 & 86.5 & 88.5 & +2.0 \\
              & 448 & 86.7 & 88.9 & +2.2 \\
    \bottomrule
  \end{tabular}
  \caption{
    \textbf{Supervised finetuning on ImageNet-1k.}
    We use the pipeline of~\cite{touvron2022deit} to finetune our encoders on ImageNet-1k at resolutions  or . We compare with the accuracy obtained with linear probing and observe only modest improvements with fine-tuning: this suggests that \OURS{} features already perform well out-of-the-box.
  }
  \label{tab:ft-inet1k-alone}
\end{table}


\paragraph{Robustness analysis.}
To complement our study, and probe the generalization of our features, we evaluate our ImageNet-1k models trained with linear classification heads on domain generalization benchmarks.
We use the best performing linear classifier as described above and simply run inference on those benchmarks.
Please note that most results in the litterature are obtained with models that are finetuned end-to-end on ImageNet-1k.
We show the result of this experiment in Table~\ref{tab:robustness}.
When comparing with state-of-the-art SSL methods, our models shows drastically better robustness ( on A,  on R and  on Sketch compared to iBOT).
Our model also improves upon the best weakly-supervised model on ImageNet-A while lagging behind on R and Sketch.

\begin{table}[t]
  \centering
  \begin{tabu}{@{} lll c cccc @{}}
    \toprule
    Method & Arch & Data && Im-A & Im-R & Im-C & Sketch \\
    \midrule
    OpenCLIP    & ViT-G/14 & LAION      && 63.8 &\bf 87.8 & 45.3 & \bf 66.4 \\
    \midrule
    MAE       & ViT-H/14    & INet-1k   && 10.2  & 34.4 & 61.4   & 21.9 \\
    DINO      & ViT-B/8     & INet-1k   && 23.9 & 37.0 & 56.6   & 25.5 \\
    iBOT      & ViT-L/16    & INet-22k  && 41.5 & 51.0 & 43.9   & 38.5 \\
    \midrule
    \multirow{4}{*}{\OURS}  & ViT-S/14 & \LaViDa   && 33.5 & 53.7 & 54.4 & 41.2 \\ 
                            & ViT-B/14 & \LaViDa   && 55.1 & 63.3 & 42.7 & 50.6 \\ 
                            & ViT-L/14 & \LaViDa   && 71.3 & 74.4 & 31.5 & 59.3 \\ 
                            & ViT-g/14 & \LaViDa   && \bf 75.9 & 78.8 & \bf 28.2 & 62.5 \\ 
    \bottomrule
  \end{tabu}
  \caption{
    \textbf{Domain Generalization with a linear probe} on top of frozen features at a resolution of 224. 
Higher numbers are better for all benchmarks except Im-C.
  }
  \label{tab:robustness}
\end{table}


\subsection{Additional Image and Video classification Benchmarks} 
In this section we study the generalization of our features on downstream classification benchmarks.
We consider two sets of evaluations in that context.
On one hand, we use large and finegrained datasets such as iNaturalist and Places205.
On the other, we use the 12 image classification tasks originally proposed in SimCLR~\citep{chen2020simple}.
For iNaturalist 2018, iNaturalist 2021, and Places205, we train a linear classifier with data augmentations as in Sec.~\ref{sec:imagenet}
We report top-1 accuracy for those three datasets in Table~\ref{tab:finegrained_video}.
Interestingly, our model significantly outperforms OpenCLIP ViT-G/14 on both variants of iNaturalist ( and  for 2018 and 2021 respectively), and lags slightly behind on Places 205 ().

\begin{table}[t]
  \centering
  \begin{tabu}{ll c ccc c ccc}
    \toprule
&  && \multicolumn{3}{c}{Image classification} && \multicolumn{3}{c}{Video classification}\\
\cmidrule{4-6}\cmidrule{8-10}
    Feature & Arch && iNat2018 & iNat2021 & Places205 && K400 & UCF-101 & SSv2 \\
    \midrule
    OpenCLIP  & ViT-G/14    && 73.0 & 76.0 & \bf 69.8 && 78.3 & 90.7 & 35.8 \\
    \midrule                                                                       
    MAE       & ViT-H/14    && 31.0 & 32.3 & 52.4 && 54.2 & 70.6 & 29.2 \\
    DINO      & ViT-B/8     && 59.6 & 68.3 & 60.4 && 64.5 & 85.0 & 32.6 \\
    iBOT      & ViT-L/16    && 66.3 & 74.6 & 64.4 && 72.6 & 88.6 & \bf 38.7 \\
    \midrule
    \multirow{4}{*}{\OURS}  & ViT-S/14 && 69 & 74.2 & 62.9 && 67.8 & 87 & 33.1 \\ 
                            & ViT-B/14 && 76.4 & 81.1 & 66.2 && 73.2 & 89.1 & 34.4 \\ 
                            & ViT-L/14 && 80.4 & 85.1 & 67.3 && 76.3 & 90.5 & 35.6 \\ 
                            & ViT-g/14 && \bf 81.6 & \bf 85.7 & 67.5 && \bf 78.4 & \bf 91.2 & 38.3  \\ 
    \bottomrule
  \end{tabu}
  \caption{
    \textbf{Linear evaluation on other image and video classification.}
The image benchmarks contain a large quantity of fine-grained examples about objects or scenes.
The video benchmarks cover action classification and human-object interaction. 
All the features are frozen with a linear probe on top.
  }
  \label{tab:finegrained_video}
\end{table}

In a second set of evaluations, we measure the performance of our model on video action recognition even though our features were not trained on videos..
We evaluated features on three datasets, namely UCF-101~\citep{soomro2012ucf101}, Kinetics-400~\citep{kay2017kinetics} and Something-Something v2~\citep{goyal2017something}.
For this evaluation, we pick  evenly spaced frames in the video and train a linear classifier on the average of the features for UCF and K-400. 
For SSv2, we opt for concatenation to retain more temporal information than with feature averaging.
For each dataset, we measure average accuracy and report the results in Table~\ref{tab:finegrained_video}.
We see that amongst self-supervised approaches, our model clearly sets a new state of the art.
Moreover, our model matches the accuracy of the OpenCLIP features on UCF and Kinetics ( and  respectively) and clearly outperforms them on SSv2 ().
This is particularly interesting, as SSv2 requires a much richer understanding of the video frames.

Finally, in Table~\ref{tab:finegrained}, we compare selected frozen features on 12 transfer classification benchmarks initially proposed by~\citet{chen2020simple}.
This benchmark covers scenes, objects (food, cars, planes), and textures.
We replace the Birdsnap dataset with CUB because the former was not publicly available in its entirety.
We follow the experimental protocol as outlined by~\citet{chen2020simple}, namely training a logistic regression on precomputed features.
Our model significantly outperforms state-of-the-art SSL models, with most notable differences on Stanford Cars ( versus DINO ViT-B/8) and FGVC Aircraft ( versus iBOT ViT-L/16).
Even though these benchmarks favor text-guided pretraining, our features are still competitive with OpenCLIP on most classification benchmarks, with the exception of a few datasets, especially SUN () and Cars ().

\begin{table}[t]
  \centering
  \setlength{\tabcolsep}{4pt}
  \begin{tabu}{@{}l@{}l cccccccccccc c @{}}
    \toprule
    Feature & Arch & \scriptsize Food & \scriptsize C10 & \scriptsize C100 & \scriptsize SUN & \scriptsize Cars & \scriptsize Aircr & \scriptsize VOC & \scriptsize DTD & \scriptsize Pets & \scriptsize Cal101 & \scriptsize Flowers & \scriptsize CUB & Avg\\
    \midrule
    OpenCLIP~ & ViT-G/14 & 94.5 & 98.7 & 91.0 & \bf 84.0 & \bf 96.1 & 80.2 & \bf 89.3 & \bf 86.0 & 95.7 & \bf 98.1 & 99.5 & 89.9 & 91.9\\
    \midrule
    MAE   & ViT-H/14    & 78.4 & 96.1 & 83.9 & 63.9 & 56.1 & 63.4 & 84.3 & 75.4 & 89.4 & 95.9 & 92.3 & 57.2 &  78.0\\
    DINO  & ViT-B/8     & 85.1 & 97.2 & 86.9 & 70.3 & 76.6 & 70.6 & 86.7 & 79.6 & 93.2 & 95.4 & 97.6 & 81.7 &  85.1\\
    iBOT  & ViT-L/16    & 91.0 & 99.0 & 92.8 & 75.6 & 71.8 & 72.4 & 89.0 & 80.7 & 87.7 & 97.5 & 99.6 & 82.1 &  86.6\\
    \midrule
    \multirow{4}{*}{\OURS}  & ViT-S/14    & 89.1 & 97.7 & 87.5 & 74.4 & 81.6 & 74.0 & 87.8 & 80.6 & 95.1 & 97.0 & 99.6 & 88.1 & 87.7 \\ 
                            & ViT-B/14    & 92.8 & 98.7 & 91.3 & 77.3 & 88.2 & 79.4 & 88.2 & 83.3 & 96.2 & 96.1 & 99.6 & 89.6 & 90.1 \\ 
                            & ViT-L/14    & 94.3 & 99.3 & 93.4 & 78.7 & 90.1 & 81.5 & 88.3 & 84.0 & 96.6 & 97.5 & 99.7 & 90.5 & 91.2\\ 
                            & ViT-g/14    & \bf 94.7 & \bf 99.5 & \bf 94.4 & 78.7 & 91.4 & \bf 87.2 & 89.0 & 84.5 & \bf 96.7 & 97.6 & \bf 99.7 & \bf 91.6 & \bf 92.1 \\
    \bottomrule
  \end{tabu}
  \caption{
    \textbf{Linear evaluation of frozen features on fine-grained benchmarks.} 
    Accuracy on 12 benchmarks covering objects, scenes and textures following the evaluation protocol proposed in~\citet{chen2020simple}.
  }
  \label{tab:finegrained}
\end{table}


\subsection{Instance Recognition}
In this experiment, we probe our model on the task of instance-level recognition using a non-parametric approach.
Images from a database are ranked according to their cosine similarity with a query image.
We evaluated our model and compare to baselines on Paris and Oxford, that are landmark recognition benchmarks.
We also evaluated on Met, a dataset of artworks from the Metropolitan museum, and AmsterTime, containing street view images matched to archival images of Amsterdam.
We measure performance by computing the mean average precision and report our results in Table~\ref{tab:retrieval}.
We see that our features significantly outperform both SSL ( mAP on Oxford-Hard), and weakly-supervised ( mAP on Oxford-Hard) ones.
It is interesting to see that our features perform well across task granularities, both at the category-level and instance-level.
This is a desirable property for strong off-the-shelf computer vision features.

\begin{table}[t]
  \centering
  \begin{tabu}{ll c cc c cc c ccc c c}
    \toprule
    & && \multicolumn{2}{c}{Oxford} && \multicolumn{2}{c}{Paris} && \multicolumn{3}{c}{Met} && AmsterTime \\
    \cmidrule{4-5} \cmidrule{7-8} \cmidrule{10-12} \cmidrule{14-14}
    Feature & Arch && M & H && M & H && GAP & GAP- & ACC && mAP \\
    \midrule
    OpenCLIP  & ViT-G/14    && 50.7 & 19.7 && 79.2 & 60.2 &&  6.5 & 23.9 & 34.4 && 24.6 \\
    \midrule
    MAE       & ViT-H/14    && 11.7 & 2.2  && 19.9 & 4.7  &&  7.5 & 23.5 & 30.5 && 4.2 \\
    DINO      & ViT-B/8     && 40.1 & 13.7 && 65.3 & 35.3 && 17.1 & 37.7 & 43.9 && 24.6 \\
    iBOT      & ViT-L/16    && 39.0 & 12.7 && 70.7 & 47.0 && 25.1 & 54.8 & 58.2 && 26.7 \\
    \midrule
    \multirow{4}{*}{\OURS}  & ViT-S/14 && 68.8 & 43.2 && 84.6 & 68.5 && 29.4 & 54.3 & 57.7 && 43.5 \\ 
                            & ViT-B/14 && 72.9 & 49.5 && 90.3 & 78.6 && 36.7 & 63.5 & 66.1 && 45.6 \\ 
                            & ViT-L/14 && \bf 75.1 & \bf 54.0 && \bf 92.7 & \bf 83.5 && \bf 40.0 & 68.9 & 71.6 && \bf 50.0 \\ 
                            & ViT-g/14 && 73.6 & 52.3 && 92.1 & 82.6 && 36.8 & \bf 73.6 & \bf 76.5 && 46.7  \\ 
    \bottomrule
  \end{tabu}
  \caption{
    \textbf{Evaluation of frozen features on instance-level recognition.}
    We consider 4 different benchmarks and report their main metrics.
  }
  \label{tab:retrieval}
\end{table}


\subsection{Dense Recognition Tasks}
We probe the quality of patch-level features extracted from our network on several dense downstream tasks.
We consider semantic image segmentation and monocular depth estimation in several settings and we conduct evaluations on multiple datasets for each.

\paragraph{Semantic segmentation.}
For our semantic segmentation evaluation, we consider two different setups.
\textbf{Linear}: a linear layer is trained to predict class logits from a patch tokens. 
It is used to produce a low-resolution logit map (eg 32x32 for a model with patch size 16), which is then upsampled to full resolution (512x512) to obtain a segmentation map. 
This procedure is extremely simple but cannot easily produce high-resolution segmentations.
\textbf{+ms}: a boosted version of the linear setup. 
We concatenate the patch tokens of the 4 last layers, use a larger image  resolution of 640, and use multiscale test-time augmentations to improve the predictions. 
We report the performance of our model variants as well as the baselines on three datasets under the two setups in Table~\ref{tab:semseg}.

Our models show very good performance on all datasets and for all setups. 
Interestingly, our evaluation using \textbf{+ms} is on par with fully finetuning MAE with an Upernet decoder ( versus  mIoU).
This is surprising because we use a significantly simpler predictor.
Also, our best model, when evaluated using the boosted recipe, almost matches the state of the art on Pascal VOC ( versus  mIoU).

In a final experiment, we freeze our backbone, and plug it into a ViT-Adapter \cite{chen2022vision} with a Mask2former head~\citep{cheng2022masked}. 
We tune the weights of the adapter and head, but keep the backbone frozen: 
only a fraction of the weights are tuned, keeping the training procedure lightweight. 
We reach  mIoU on ADE20k, close to the competitive state of the art, standing at 62.9 mIoU~\citep{wang2022internimage}.

\begin{table}[t]
  \centering
  \begin{tabu}{@{}ll c cc c cc c cc@{}}
    \toprule
    & && \multicolumn{2}{c}{ADE20k} && \multicolumn{2}{c}{CityScapes} && \multicolumn{2}{c}{Pascal VOC} \\
    & && \multicolumn{2}{c}{(62.9)} && \multicolumn{2}{c}{(86.9)} && \multicolumn{2}{c}{(89.0)} \\
    \cmidrule{4-5} \cmidrule{7-8} \cmidrule{10-11}
    Method & Arch. && lin. & +ms && lin. & +ms && lin. & +ms \\
    \midrule
    OpenCLIP & ViT-G/14 && 39.3 & 46.0   && 60.3 & 70.3 && 71.4 & 79.2 \\
    \midrule
    MAE       & ViT-H/14  && 33.3 & 30.7 && 58.4 & 61.0 && 67.6 & 63.3 \\
    DINO      & ViT-B/8   && 31.8 & 35.2 && 56.9 & 66.2 && 66.4 & 75.6 \\
    iBOT      & ViT-L/16  && 44.6 & 47.5 && 64.8   & 74.5  && 82.3 & 84.3 \\
    \midrule
    \multirow{4}{*}{\OURS}  & ViT-S/14     && 44.3 & 47.2 && 66.6 & 77.1 && 81.1 & 82.6 \\ 
                            & ViT-B/14     && 47.3 & 51.3 && 69.4 & 80.0 && 82.5 & 84.9 \\ 
                            & ViT-L/14     && 47.7 & \bf 53.1 && 70.3 & 80.9 && 82.1 & 86.0 \\ 
                            & ViT-g/14     && \bf 49.0 & 53.0 && \bf 71.3 & \bf 81.0 && \bf 83.0 & \bf 86.2 \\ 
    \bottomrule
  \end{tabu}
  \caption{
    \textbf{Semantic segmentation on ADE20K, CityScapes and Pascal VOC with frozen features} and a linear classifier (lin.) and with multiscale (+ms).
    The absolute state of the art -- from \cite{wang2022internimage}, \cite{liu2021polarized} and \cite{chen2018encoder} respectively -- are mentioned at the top of the Table. 
    For reference, using the Mask2Former pipeline~\citep{steiner2021train} with a ViT-Adapter~\citep{chen2022vision} on top of our frozen ViT-g/14 backbone gives 60.2 mIoU on ADE-20k.
  }
  \label{tab:semseg}
\end{table}


\paragraph{Depth estimation.}
In this experiment, we evaluate our patch-level features on three monocular depth estimation benchmarks: NYUd, KITTI and zero-shot transfer from NYUd to SUN3d.
We follow the evaluation protocol of~\citet{li2022binsformer}.
We consider three different setups for this evaluation.
\textbf{lin. 1}: we extract the last layer of the frozen transformer and concatenate the \texttt{[CLS]} token to each patch token. 
Then we bi-linearly upsample the tokens by a factor of 4 to increase the resolution. 
Finally we train a simple linear layer using a classification loss by dividing the depth prediction range in 256 uniformly distributed bins and use a linear normalization following ~\cite{bhat2019adabins}.
\textbf{lin. 4}: we use the same protocol that we use with one layer, but concatenate the tokens from layers  for ViT-S/B,  for ViT-L, and  for ViT-g.
\textbf{DPT}: we use the DPT decoder~\citep{ranftl2021vision} on top of our frozen models and setup a regression task. 
We scale the size of the head following the dimension of the features for each architecture. 
We show results for all baselines, all datasets and all setups in Table \ref{tab:depth}.

From this table, we see that our features clearly surpass the best SSL and WSL features available.
It is interesting to see that iBOT features extracted from a ViT-L outperform the ones of OpenCLIP with a ViT-G.
This observation supports the intuition that caption-based feature learning fails to learn subtle patterns like this one.
Also, our model, with the DPT decoder and frozen backbone, matches or exceeds the performance of the recent work of~\citet{li2022binsformer}.
Finally, the out-of-domain generalization result on SUN-RGBd shows that our features allow very good transfer between domains. 
A depth prediction module trained on indoor scenes from NYUd generalizes pretty well to the outdoor examples of SUN-RGBd.

\begin{table}[t]
  \centering
  \begin{tabu}{@{}ll@{} c ccc c ccc c ccc@{}}
    \toprule
    & && \multicolumn{3}{c}{NYUd} && \multicolumn{3}{c}{KITTI} && \multicolumn{3}{c}{NYUd  SUN RGB-D} \\
    & && 
    \multicolumn{3}{c}{(0.330)} 
    && 
    \multicolumn{3}{c}{(2.10)} 
    && 
    \multicolumn{3}{c}{(0.421)} \\
    \cmidrule{4-6} \cmidrule{8-10} \cmidrule{12-14}
    Method & Arch. && lin. 1 & lin. 4 & DPT && lin. 1 & lin. 4 & DPT && lin. 1 & lin. 4 & DPT  \\
    \midrule 
    OpenCLIP  & ViT-G/14    && 0.541 & 0.510 & 0.414    && 3.57 & 3.21 & 2.56   && 0.537 & 0.476 & 0.408  \\
    \midrule
    MAE       & ViT-H/14    && 0.517 & 0.483 & 0.415    && 3.66 & 3.26  & 2.59   && 0.545 & 0.523 & 0.506  \\
    DINO      & ViT-B/8     && 0.555 & 0.539 & 0.492    && 3.81 & 3.56 & 2.74   && 0.553 & 0.541 & 0.520 \\
    iBOT      & ViT-L/16    && 0.417 & 0.387 & 0.358 && 3.31 & 3.07 & 2.55 && 0.447 & 0.435 & 0.426 \\
    \midrule
    \multirow{4}{*}{\OURS}  & ViT-S/14 && 0.449 & 0.417 & 0.356 && 3.10 & 2.86 & 2.34 && 0.477 & 0.431 & 0.409 \\ 
                            & ViT-B/14 && 0.399 & 0.362 & 0.317 && 2.90 & 2.59 & 2.23 && 0.448 & 0.400 & 0.377 \\ 
                            & ViT-L/14 && 0.384 & 0.333 & 0.293 && 2.78 & 2.50 & 2.14 && 0.429 & 0.396 & 0.360  \\ 
                            & ViT-g/14 && \bf 0.344 & \bf 0.298 & \bf 0.279 && \bf 2.62 & \bf 2.35 & \bf 2.11 && \bf 0.402 & \bf 0.362 & \bf 0.338 \\
    \bottomrule		
  \end{tabu}
  \caption{
    \textbf{Depth estimation with frozen features}. 
    We report performance when training a linear classifier on top of one (lin. 1) or four (lin. 4) transformer layers, as well, as the DPT decoder (DPT) of ~\cite{ranftl2021vision}.
    We report the RMSE metric on the 3 datasets. 
    Lower is better.
    For reference, we report state-of-the-art results taken from~\cite{li2022binsformer} on each benchmark on top of the Table.
  }
  \label{tab:depth}
\end{table}


\subsection{Qualitative Results}
In this final section of the empirical evaluation of our features, we propose a few qualitative analyses.

\paragraph{Semantic Segmentation and Depth Estimation.}
We show some qualitative results for our dense prediction evaluations: segmentation on ADE20K in Fig.~\ref{fig:segm_qual} and depth estimation on NYUd, KITTI and SUN RGB-D in Fig.~\ref{fig:depth_qual}.
We compare \OURS{} with OpenCLIP with a linear classifier on each dataset.
While not perfect, the linear segmentation model using our DINOv2 backbone produces good results and behaves much better than the OpenCLIP one under this evaluation setup.
Indeed, the segmentation mask produced by OpenCLIP-G shows many artifacts and disconnected components. 
The qualitative results on depth estimation clearly illustrate the quantitative gap between OpenCLIP and \OURS.
These results highlight that our features, as well as the features extracted from OpenCLIP, are able to linearly separate complex information such as depth, even though neither was trained with this type of information.
However, our features lead to a much smoother depth estimation, with less artifacts.
Some objects such as the chair on the SUN RGB-D image are completely ignored by OpenCLIP and correctly positioned using our features.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{new-figure-7.jpg}  
  \caption{
    \textbf{Segmentation and depth estimation with linear classifiers.} 
    Examples from ADE20K, NYUd, SUN RGB-D and KITTI with a linear probe on frozen OpenCLIP-G and \OURS-g features.
  }
  \label{fig:segm_qual}
  \label{fig:depth_qual}
\end{figure}


\paragraph{Out-of-distribution generalization.}
We show a few examples of applying the depth prediction and segmentation linear classifiers to out-of-distribution examples in Fig.~\ref{fig:depth_ood}.
The qualitative results support our claim that our features transfer between domains.
The quality of the depth and segmentation predicted for pictures of animals, or paintings is very good, even though the domains are very different.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{new-figure-8.jpg} 
  \caption{
    \textbf{Examples of out-of-distribution examples} with frozen \OURS-g features and a linear probe.
  }
  \label{fig:depth_ood}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{new-figure-9.jpg}
  \caption{
    \textbf{More visualization of the first PCA components.} 
    We compute the PCA between the patches from all of the images and show their first 3 components.
    Each component corresponds to a specific color channel. Same parts are matched between related images depsite changes of pose, style or even objects.
    Background is removed by removing patches with a negative score of the first PCA component.
  }
  \label{fig:pca}
\end{figure}


\paragraph{PCA of patch features.}
We show the results of the principal component analysis (PCA) performed on the patch features extracted by our model.
We keep only patches with a positive value after we threshold the first component.
This procedure turns out to separate the image's main object from the background.
We compute a second PCA on the remaining patches across three images depicting the same category.
We color the three first components with three different colors and present the results in Fig.~\ref{fig:qualitative} and~\ref{fig:pca}.
There are two interesting observations:
first, our unsupervised foreground / background detector, based on detecting the highest variance direction, performs very well and is capable of delineating the boundary of the main object in the picture.
Second, the other components correspond to "parts" of objects and match well for images of the same category.
This is an emerging property -- our model was not trained to parse parts of objects.

\paragraph{Patch matching.}
Finally, we explore what type of information our patch-level features contain by matching them across images. 
We start by detecting the foreground object using the procedure described above.
Then, we compute the euclidean distance between patch features extracted from two images and map them by solving an assignment problem.
In order to reduce the number of matches, we then apply a non-maximum suppression to keep only the salient ones.
In Fig.~\ref{fig:matching}, we show some examples of such matchings. 

We observe that the features seem to capture information about semantic regions that serve similar purpose in different objects or animals.
For instance, the wing of a plane matches the wing of a bird.
We also observe that the model is robust to style (image versus drawing), and to large variation of poses (see the elephant).

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{new-figure-10.jpg}
  \caption{
    \textbf{Matching across images.} We match patch-level features between images from different domains, poses and even  objects that share similar semantic information. 
    This exhibits the ability of our model to transfer across domains and understand relations between similar parts of different objects.
  }
  \label{fig:matching}
 \end{figure*}
