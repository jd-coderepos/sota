\label{results}

\label{table_fine_grained}


\begin{table}[t!]

\setlength{\tabcolsep}{4.5pt}
\begin{center}
\begin{tabular}{|l||c|c|c|c|}
\hline
\thead{\textbf{Teacher} \\ \textbf{Student}} & \thead{\textbf{ResNet32x4}\\\textbf{ResNet8x4}}  & \thead{\textbf{ResNet110}\\\textbf{ResNet20}} & \thead{\textbf{ResNet110}\\\textbf{ResNet32}} & \thead{\textbf{ResNet56}\\\textbf{ResNet20}} \\
\hline
Teacher & 79.42 & 74.31 & 74.31 & 72.34 \\
Student & 72.50  & 69.06 & 71.14 & 69.06 \\
\hline

KD \cite{hinton2015distilling} & 73.33  & 70.67 & 73.08 & 70.66 \\
FitNets \cite{romero2015fitnets} & 73.50  & 68.99 & 71.06 & 69.21  \\
AT \cite{passalis2019learning} & 73.44  & 70.22 & 72.31 & 70.55 \\
PKT \cite{passalis2019learning}  & 73.64 & 70.25 & 72.61  & 70.34 \\
AB \cite{heo2018knowledge} & 73.17 & 69.53 & 70.98  & 69.47  \\
FT \cite{kim2020paraphrasing} & 72.86 & 70.22 & 72.37 & 69.84 \\
FSP \cite{8100237} & 72.62 & 70.11 & 71.89 & 69.95 \\
NST \cite{huang2017like} & 73.30 & 69.53 & 71.96 & 69.60  \\
CRD \cite{tian2020contrastive}  & 75.51 & 71.46 & 73.48 & 71.16 \\
CRD+KD \cite{tian2020contrastive} & 75.46  & 71.56 & 73.75 & \textbf{71.63}  \\
SRRL \cite{Yang2021KnowledgeDV} & 75.92 & 71.51 & 73.80 & 71.44 \\
\hline
TH-KD & 75.24   & 71.61 & 73.32 & 71.58 \\
SH-KD & \textbf{75.94}  & \textbf{71.65} & \textbf{73.77} & 71.36  \\
\hline
TH-KD + CRD  & 75.59 & 71.92 & 74.13 & \textbf{72.22} \\
SH-KD + CRD & \textbf{76.61} & \textbf{72.12} & \textbf{74.16} & 72.05 \\
\hline
\end{tabular}
\vspace{0.2cm}
\caption{\textbf{Test accuracy (\%) on CIFAR100 dataset.} We follow the same protocol as in the CRD work \protect\cite{tian2020contrastive}. SH-KD achieves superior results. When combined with the CRD loss, further improvement is obtained in most experiments.}
\label{tbl:cifar100_same}
\end{center}
\vspace{-20pt}
\end{table}



















































%
 

\begin{figure*}[t!]
\centering
\begin{subfigure}[a]{.33\textwidth}
  \centering
  \includegraphics[width=1 \linewidth]{Figures/plot_016_stanford_cars_ofa595}
  \caption{Stanford-cars dataset}
\end{subfigure}\begin{subfigure}[h]{.33\textwidth }
  \centering
  \includegraphics[width=1 \linewidth]{Figures/plot_016_foodx}
  \caption{FoodX-251 dataset}
\end{subfigure}
\begin{subfigure}[h]{.33\textwidth }
  \centering
  \includegraphics[width=1\linewidth]{Figures/plot_016_face}
  \caption{IJB-C dataset}
\end{subfigure}
\caption{\textbf{Average angle between the teacher and student embedding vectors, and student model accuracy.} Both TH-KD, and to a greater extent SH-KD, reduce the angle between the teacher and student embedding vectors and improve test accuracy.}

\label{fig:angle}
\vspace{-3mm}
\end{figure*}



In this section, we report our main results on three domains: image classification, fine-grained classification and face-verification. 
Also, we study the impact of the proposed schemes, TH-KD and SH-KD, on the representation distillation quality.
The training details are provided in the Appendix.
\subsection{Benchmark Results}
\subsubsection{CIFAR-100}

The CIFAR-100 \cite{krizhevsky2009learning} consists of 50K training images (500 samples per class) and 10K test images. In 
Table \ref{tbl:cifar100_same} we show the results obtained by our two explored schemes, TH-KD and SH-KD, and compare them to previous approaches for KD. 
As the proposed schemes are complementary to other methods with different objectives, we tested the TH-KD and SH-KD with the original KD loss and with CRD \cite{tian2020contrastive}. When combined with CRD, both TH-KD and, to a greater extent SH-KD, outperform all the baseline approaches.

\subsubsection{Fine-grained Classification}
We evaluated our methods on two fine-grained classification datasets, Stanford-cars \cite{stanford_cars_2013} and FoodX-251 \cite{foodx_dataset_2019}.
Stanford-cars contains 196 classes and consists of 8,041 training images and 8,144 test images. FoodX-251 \cite{foodx_dataset_2019} contains 251 classes and consists of 118K training images and 28K test images.

We tested six training configurations with different architectures for the teacher and student models. As teachers, we used TResNet-M and TResNet-L \cite{ridnik2020TResNet}, and ResNet101 \cite{resnet_2015}.
As students, we used once-for-all (OFA) models \cite{ofa_2020}; OFA-595 and OFA-62, and small ResNet variants; ResNet18 and ResNet26.
The OFA architectures were designed for cost-effective mobile deployment.

We compared five training regimes: a vanilla training without any KD loss, regular KD, training with L2E for representation distillation, and the proposed approaches TH-KD and SH-KD.

We summarize the results obtained on Stanford-cars and FoodX-251 in Table \ref{tbl:stanford_cars} and Table \ref{tbl:foodx}, respectively. 
On the Stanford-cars dataset, the SH-KD method was consistently superior in all training configurations. Interestingly, for Stanford-cars, regular KD degrades the accuracy compared to a vanilla training. On the FoodX-251 dataset, the highest results were achieved by TH-KD or SH-KD. The TH-KD method was superior in four out of the six tested configurations.


\begin{table*}[t!]
\setlength{\tabcolsep}{3pt}
\begin{center}
\begin{tabular}{|l||c|c|c|c|c|c|}
\hline
\thead{Teacher \\Student} & \thead{TResNetM\\OFA-62} & \thead{TResNetM\\OFA-595} & \thead{TResNetL\\OFA-62} & \thead{TResNetL\\OFA-595} & \thead{ResNet101\\ResNet18} & \thead{ResNet101\\ResNet26} \\
\hline
Teacher & 95.53 &  95.53 & 96.19 & 96.19 & 95.63 & 95.63 \\
\hline
Vanilla & 94.66 & 95.41 & 94.66 & 95.41 & 94.66 & 95.20 \\
KD & 94.51 & 95.36 & 94.62 & 95.31 & 94.29 & 94.95 \\
L2E & 95.11 & 95.37 & 94.94 & 95.27 & 94.73 & 95.27 \\
TH-KD & 95.16 & 95.28 & 95.08 & 95.38 & 94.83 & 95.19 \\
SH-KD & \textbf{95.21}& \textbf{95.52} & \textbf{95.13} & \textbf{95.46} & \textbf{94.98} & \textbf{95.38} \\
\hline
\end{tabular}
\vspace{1pt}
\caption{\textbf{Test \emph{accuracy} (\%) on Stanford-cars.} The SH-KD method outperforms other approaches  consistently in all training configurations.}
\label{tbl:stanford_cars}
\end{center}
\vspace{-20pt}
\end{table*}



\begin{table*}[t!]
\setlength{\tabcolsep}{3pt}
\begin{center}
\begin{tabular}{|l||c|c|c|c|c|c|}
\hline
\thead{Teacher \\Student} & \thead{TResNetM\\OFA-62} & \thead{TResNetM\\OFA-595} & \thead{TResNetL\\OFA-62} & \thead{TResNetL\\OFA-595} & \thead{ResNet101\\ResNet18} & \thead{ResNet101\\ResNet26} \\
\hline
Teacher & 76.36 & 76.36 & 77.11 & 77.11 & 75.51 & 75.51 \\
\hline
Vanilla & 69.91 & 73.80 & 69.91 & 73.80 & 67.08 & 70.80 \\
KD & 70.87 & 73.98 & 70.74 & 74.04 & 67.88 & 71.75 \\
L2E & 71.62 & 74.62 & 71.67 & 74.65 & 69.19 & 72.99 \\
TH-KD & \textbf{71.99} & \textbf{75.23} & 72.09 & \textbf{75.20} & \textbf{69.59} & 72.79 \\
SH-KD & 71.94 & 74.56 & \textbf{72.29} & 74.75 & 69.17 & \textbf{73.12} \\
\hline
\end{tabular}
\vspace{1pt}
\caption{\textbf{Test \emph{accuracy} (\%) on FoodX-251.} The highest results are achieved by the TH-KD or SH-KD approaches. TH-KD is superior in most of the training configurations.}
\label{tbl:foodx}
\end{center}
\vspace{-20pt}
\end{table*}








 


\begin{table*}[t!]
\setlength{\tabcolsep}{4.5pt}
\begin{center}
\begin{tabular}{|l||l||c|c|c|}
\hline
\thead{\textbf{Method}} & \thead{\textbf{Model}} & \thead{\textbf{TAR@}\\\textbf{FAR=1e-6}} & \thead{\textbf{TAR@}\\\textbf{FAR=1e-5}} &
\thead{\textbf{TAR@}\\\textbf{FAR=1e-4}}\\
\hline
Martinez* & MobileFaceNet & -- & 92.20 & 94.70  \\
L2E+ES-sampling* & MobileFaceNet &-- & 93.20 & 95.39  \\
L2E+IS-sampling* & MobileFaceNet &  -- & 93.25 & 95.49  \\
\hline
Teacher, Vanilla& R100  & 91.49 &  95.52 & 97.00  \\
Teacher, SH-KD & R100  & 90.88 &  95.58 & 97.03  \\
\hline
Vanilla & MobileFaceNet & 88.72 & 92.75 & 95.42  \\
L2E & MobileFaceNet & 88.47  & 93.49 & 95.48  \\
TH-KD (Ours) & MobileFaceNet & 89.82 & 93.50 & 95.48  \\
SH-KD (Ours) & MobileFaceNet & \textbf{90.24}& \textbf{93.73} & \textbf{95.64} \\
\hline
\end{tabular}
\vspace{1pt}
\caption{\textbf{Results on the IJB-C dataset.} The reported results of the first three rows, denoted by *, were taken from the papers \protect\cite{martinez2021benchmarking} and \protect\cite{Liu_2021_ICCV}.
}
\label{tbl:face}
\end{center}
\vspace{-35pt}
\end{table*}

 
\subsubsection{Face Verification}
We evaluated our methods on the face verification task of the IJB-C dataset \cite{8411217}. 
For training, we used a refined version of the popular MS-Celeb-1M dataset \cite{guo2016msceleb1m} named MS1MV3 \cite{deng2019lightweight} which contains about 93K identities and 5.2M images.
We used a ResNet-like network \cite{resnet_2015}, R100 as a teacher, and MobileFaceNet as a student \cite{chen2018mobilefacenets}. 
We used the L2 loss between the embedding features of the teacher and the student (L2E) as the representation distillation loss, and the large-margin cosine loss, CosFace \cite{Wang_2018_CVPR} as the base loss. 


In Table 4 we show the results obtained by our approaches, TH-KD and SH-KD, and compare them to other baselines and previous state-of-the-art methods \cite{Liu_2021_ICCV}. 
We report three common metrics for evaluating the performance of the IJB-C dataset: TAR(@FAR=1e-6), TAR(@FAR=1e-5) and  TAR(@FAR=1e-4). 
In the vanilla training, we used the CosFace loss only. The TH-KD scheme was performed with  in equation (\ref{TH_CE_loss}). We also report the results obtained by the teacher models: the regular teacher and the teacher training with the student's classifier following the SH-KD method. As can be seen, both teachers provide similar metric results.

The student model trained using the TH-KD scheme outperforms the baseline approaches considerably for the  TAR(@FAR=1e-6) metric, improving the L2E method from 88.47\% to 89.82\%. Using the SH-KD scheme, we obtain a significant improvement compared to the other baselines and previous state-of-the-art approaches in all the tested metrics. For example, the TAR(@FAR=1e-6) metric is improved to 90.24\%.


\begin{figure}[t!]
\centering
\begin{subfigure}[a]{.48\textwidth}
  \centering
  \includegraphics[width=0.85\linewidth]{Figures/plot_015_regime_cars_512}
  \caption{Stanford-cars}
\end{subfigure}\begin{subfigure}[h]{.48\textwidth }
  \centering
  \includegraphics[width=0.85\linewidth]{Figures/plot_015_regime_foodx_2048}
  \caption{FoodX-251}
\end{subfigure}
\caption{\textbf{MSC score over training epochs for different KD methods.} TH-KD consistently increases the MSC score over the L2E baseline. Regular KD produces poor MSC score.}
\label{fig:MSC}
\vspace{-5mm}
\end{figure}


\subsection{Analysis} \label{sec: analysis}
In this section, we study notable properties of the proposed approaches, TH-KD and SH-KD, including their ability to accelerate the training process and how they impact the representation learning of the student.
\vspace{-3mm}



\subsubsection{Representation Similarity Between Teacher and Student.} \label{subsec:rep}
To demonstrate the ability to reliably transfer the representation knowledge from the teacher to the student, we measure the angle between corresponding embedding vectors extracted by the teacher and the student.
For each image the angle is given by,

In Fig.~\ref{fig:angle}, we show the averaged angle between the teacher and student embeddings for Standford-cars, FoodX-251 and the face IJB-C datasets. The reported angle is obtained by averaging the instance angles  computed for all the test samples. 

Examination of Fig.~\ref{fig:angle} yields several observations.
First, the angle obtained by the regular KD is similar to the angle obtained in the vanilla model (between two unrelated models). This may arise from the fact that regular KD operates solely on the final network predictions. Therefore, KD does not offer any ability to transfer representational knowledge from the teacher to the student.
Second, L2E significantly improves both the similarity between the embedding vectors of the teacher and the student and the accuracy of the student. 
Third, the TH-KD and SH-KD further improve the similarity between the embedding vectors of the student and the teacher, as well as the classification accuracy. Fourth, using SH-KD considerably reduces the angle between the embedding vectors of the teacher and student models, across all datasets.
    


\begin{figure*}[t!]
\begin{subfigure}[a]{0.5\textwidth}\label{subfig:converge_cars}
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/training_convergence}
  \caption{FoodX-251 dataset}
\end{subfigure}\begin{subfigure}[h]{0.5\textwidth }\label{subfig:converge_food}
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/training_convergence_cars}
  \caption{Stanford-cars dataset}
\end{subfigure}
\vspace{-3mm}
  \caption{\textbf{Training convergence.} 
  Training with TH-KD and SH-KD converges faster compared to regular training with L2E loss.}
\label{fig:accelerate}
\vspace{-3mm}
\end{figure*}

\vspace{-3mm}
\subsubsection{TH-KD Improves Representation Quality.} 
We use the mean silhouette coefficient (MSC) score \cite{silhouettes} to measure the clustering quality of the embeddings generated by the models' backbones, in terms of intra-class variation and inter-class separability. 
Let  be the set of extracted embedding vectors: , where  is the number of samples. The MSC is defined as,

where  is the averaged distance between  to the other embedding vectors residing in the same category as , and  is the minimum distance between  and the centers of the other categories. Higher  implies larger inter-class separability, and lower  implies smaller intra-class variation. Typically, stronger models with higher capacity produce an embedding space with a higher MSC.

In Fig.~\ref{fig:MSC},  we plot the MSC score computed throughout the training epochs for several training modes: vanilla model trained without KD, KD, L2E, and TH-KD.
In both datasets (Stanford-cars and FoodX-251), training with TH-KD increases the MSC score compared to training with L2E. 
This supports the claim that training the student with the teacher head enables better representational knowledge transfer from the teacher to the student.
Note that in both datasets, the regular KD produces a poor MSC score. In the case of Stanford-cars the MSC is even degraded compared to the vanilla model.

\vspace{-3mm}
\subsubsection{KD Training Convergence.}\label{sec:convergence}
Often, training with knowledge distillation requires many more epochs than regular training \cite{beyer2021knowledge}. This is particularly true for representation distillation where the optimization involves the minimization of a distance function between the teacher and student embeddings. 
A slow training process increases the computational cost and limits resource utilization.   

In addition to improving a model's accuracy, we observe that networks trained using TH-KD and SH-KD converge faster compared to a baseline training with representation distillation. Classifier sharing reduces the number of trainable parameters and eases the training. TH-KD circumvents the learning of the student's classifier weights by initializing them with the teacher's head weights and freezing them. Moreover, SH-KD further accelerates the training because the features extracted by the teacher are more applicable for the student.
In Fig.~\ref{fig:accelerate}, we show the test accuracy on FoodX-251 and Stanford-cars, over the training epochs for TH-KD and SH-KD methods and compare them to a baseline training using the L2E loss. We used the TResNet-L as a teacher and OFA-62 as a student. As can be seen, both TH-KD and SH-KD result in faster training convergence compared to a baseline training with the L2E, and SH-KD further accelerates the training process.
To some extent, this compensates for the fact that SH-KD scheme requires an additional phase of training the initial student.

\begin{figure}[t!]
\begin{subfigure}[a]{0.33\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{Figures/plot_011_shc_architecture_foodx_2048_students}
  \caption{Accuracy}
  \label{subfig:shc_acc}
\end{subfigure}\hfill \begin{subfigure}[h]{0.33\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{Figures/plot_010_shc_architecture_foodx_2048_students}
  \caption{Confidence gap}
  \label{subfig:shc_conf}
\end{subfigure}\hfill \begin{subfigure}[h]{0.33\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{Figures/plot_023_shc_architecture_foodx_2048_teachers}
  \caption{Embedding angle}
  \label{subfig:shc_angle}
\end{subfigure}
\caption{\textbf{Effect of initial student capacity on SH-KD.} 
We trained a set of teachers (TResNet-L), each with a head from an initial student of different capacity.  Then, we trained a final student (OFA-62) based on each teacher.
For each teacher-student pair, we report their accuracies, confidence gaps, and embedding angles on FoodX-251.}
\label{fig:shc_ablation}
\vspace{-5mm}
\end{figure}

\subsubsection{Effect of Initial Student Capacity on SH-KD.} 
In the SH-KD scheme, the teacher classifier head is replaced by the classifier head of the initial student.
Typically, a student model has lower capacity than the teacher. How does this capacity gap affect the trained teacher and the training of its final student?

We investigated how the capacity of the initial student affects the SH-KD training process, by examining a variety of initial student backbones. We used heads of students with different architectures to train different teacher models with similar backbones (TResNet-L). Then, we used each teacher to train a final student model (OFA-62). In Fig.~\ref{fig:shc_ablation}, we show the effects of the initial model selection on the outcome and process of SH-KD training. 

In Fig.~\ref{subfig:shc_acc} we report the accuracy of each teacher and its final student. We observe that decreasing initial student capacity reduces the teacher's accuracy, but may positively affect the final student's accuracy. Notably, using an initial student model from the OFA family yields a better final student than either using TH-KD or other architectures, even if the initial student architecture does not exactly match the final OFA-62 student architecture.


For the same settings, we present in Fig.~\ref{subfig:shc_conf} the confidence gap between the teacher and the student. Following \cite{xu2020computationefficient}, we measure the model confidence by the mean difference between first and second prediction values. A high confidence gap between the teacher and student predictions may imply a high capacity gap between the models \cite{guo2021reducing}.
As can be seen, the confidence gap between the teacher and the final student (whose architectures are fixed) is substantially reduced when the initial student model belongs to the OFA family. This is an indication that SH-KD can mitigate the capacity gap between student and teacher by adapting the properties of the teacher to match the capacity of the student.


To further understand the SH-KD process and its outcomes, we present the difference between teacher's and student's embedding vectors, as measured by their angle (see equation (\ref{eq:angles})), in Fig.~\ref{subfig:shc_angle}. On one hand, the difference between the embedding vectors of each initial student model and its matching teacher increases accordingly when their capacity gap increases. On the other hand, for the final student, its embedding vectors match the teacher's embedding vectors more closely if its teacher was trained with a smaller initial student model. Thus, using an initial student of a capacity matching the low capacity of the final student constraints the trained teacher, and therefore allows the final student to match it better, and eventually yield better performance.



