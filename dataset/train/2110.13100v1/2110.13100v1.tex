\appendix
\setlength\cftbeforesubsecskip{-1pt}
\renewcommand\cftsecafterpnum{\vskip0pt}

\part{Appendix}\label{sec:apdx}

\vspace{-10pt}
\begin{spacing}{0.2}
	\parttoc \end{spacing}


\vspace{-3pt}
\section{\dataset Details \label{apdx:dataset}}
\vspace{-5pt}

\subsection{Generating \dataset using DARTS\label{apdx:darts_bg}}
\vspace{-3pt}

In this section, we elaborate on our description in \S~\ref{sec:bg_darts} about how DARTS~\cite{liu2018darts} defines networks. We also elaborate on our discussion in \S~\ref{sec:dataset} about how we modify the DARTS framework to generate our \dataset dataset of architectures and summarize these modifications here, in Table~\ref{tab:darts_diff}.
We visualize examples of architectures defined using DARTS~\cite{liu2018darts} and corresponding computational graphs obtained using our code (Fig.~\ref{fig:darts_bg}). 

\textbf{Overall architecture structure.} At a high level, all our \iid and \ood networks are composed of a stem, repeated normal and reduction cells, global average pooling and a classification head (Fig.~\ref{fig:darts_bg}, (a)). We optionally sample fully-connected layers between the global pooling and the last classification layer and/or replace global pooling with fully connected layers, e.g. as in VGG~\cite{simonyan2014very}. The stem in DARTS, and in other NAS works, is predefined and fixed for each image dataset. We uniformly sample either a CIFAR-10 style or ImageNet style stem, so that our network space is unified for both image datasets. To prevent extreme GPU memory consumption when using a non-ImageNet stem for ImageNet images, we additionally use a larger stride in the stem that does not affect the graph structure.
\textbf{At test time}, however, we can predict \params for networks without these constraints, but the performance of the predicted \params might degrade accordingly. For example, we can successfully predict \params for ResNet-50 (Fig.~\ref{fig:darts_bg}, (e)), which has 13 normal and 3 reduction cells placed after the 3rd, 7th and 13th cells. ResNet-50's cells (Fig.~\ref{fig:darts_bg}, (d)) are similar to those of ResNet-18 (Fig.~\ref{fig:darts_bg}, (b)), but have $1\PLH 1$ bottleneck layers.

\textbf{Within and between cell connectivity.}
Within each cell and between them, there is a certain pattern to create connections in DARTS (Fig.~\ref{fig:darts_bg}, (b,d)): each cell receives features from the two previous cells, each summation node can only receive features from two nodes, the last concatenation node can receive features from an arbitrary number of nodes. But due to the presence of the Zero (`none') and Identity (`skip connection') operations, we can enable any connectivity. 
We represent operations as nodes\footnote{In DARTS, the operations are placed on the edges, except for inputs, summations and concatenation (Fig.~\ref{fig:darts_bg}).\looseness-1} and drop redundant edges and nodes. For example, if the node performs the Zero operation, we remove the edges connected to that node. This can lead to a small fraction of disconnected graphs, which we remove from the training/testing sets. If the node performs the Identity operation, we remove the node, but keep corresponding edges. 
We also omit ReLU operations and other nonlinearities in a graph representation to avoid significantly enlarging graphs, since the position of nonlinearities w.r.t. other operations is generally consistent across architectures (e.g. all convolutions are preceded by ReLUs except the first layer).
This leads to graphs visualized in Fig.~\ref{fig:darts_bg}, (c,e).

\textbf{Operations.} The initial choice of operations in DARTS is quite standard in NAS. In normal cells, each operation returns the tensor of the same shape as it receives. So any differentiable operation that can preserve the shape of the input tensor can be used, therefore extending the set of operations is relatively trivial. In reduction cells, spatial resolution is reduced by a factor of 2, so some operations can have stride 2 there. In both cells, there are summation and concatenation nodes that aggregate features from several operations into one tensor. Concatenation (across channels) is used as the final node in a cell. To preserve channel dimensions after concatenating many features, $1\PLH 1$ convolutions are used as needed.
For the Squeeze\&Exicte (SE) and Visual Transformer (ViT) operations, we use open source implementations\footnote{SE: \url{https://github.com/ai-med/squeeze_and_excitation/blob/master/squeeze_and_excitation/squeeze_and_excitation.py}, ViT: \url{https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py}} with default configurations, e.g. 8 heads in the multihead self-attention of ViT.\looseness-1


\begin{figure}[tbhp]
	\vspace{-10pt}
	\centering
	\begin{tabular}{ccc}
		\includegraphics[width=0.12\textwidth,trim={0 0 0 0},clip,align=c]{network_structure.png}
		& 
		\includegraphics[width=0.3\textwidth,trim={0 0 0 0},clip,align=c]{darts_cells.png} 
		& 
		\includegraphics[width=0.25\textwidth,trim={2cm 2cm 2cm 2cm},clip,align=c]{dag_resnet.pdf} \\
		(a) & (b) & (c) \\
		& \includegraphics[width=0.3\textwidth,trim={0 0 0 0},clip,align=c]{darts_resnet50_cells.png} 
		& \includegraphics[width=0.25\textwidth,trim={2cm 2cm 2cm 2cm},clip,align=c]{dag_resnet_50.pdf} \vspace{-15pt}\\
		& (d) & (e) \\
	\end{tabular}
	\vspace{-8pt}
	\caption{\small \textbf{(a)} Network's high-level structure introduced in \cite{zoph2018learning} and employed by many following papers on network design, including DARTS~\cite{liu2018darts} and ours, where N$\geq 1$; \textbf{(b)} A residual block~\cite{he2016deep} in terms of DARTS normal and reduction cells, where green nodes denote outputs from the two previous cells, blue nodes denote summation, a yellow node denotes concatenation$^\dagger$; edges denote operations, `none' indicates dropping the edge$^\ddagger$; the reduction cell has the same structure in ResNets, but decreases spatial resolution by 2 using a downsample operation and stride 2 in operations, at the same time, optionally increasing the channel dimensionality by 2. \textbf{(c)} The result of combining (a) and (b) for 8 cells using our code to build an analogous of the ResNet-18 architecture$^\star$.
		\textbf{(d)} A residual block of ResNet-50 with $1 \PLH 1$ bottleneck layers defined using DARTS and \textbf{(e)} the graph built using our code, where 3 reduction cells are placed as in the original ResNet-50 architecture. }
	\label{fig:darts_bg}
	\vspace{-20pt}
\end{figure}


\blfootnote{$^\dagger$Concatenation is redundant in ResNets and removed from our graphs due to only one input node in cells.\looseness-1}
\blfootnote{$^\ddagger$In ResNets~\cite{he2016deep}, there is no skip connection between the input of a given cell and the output of the cell before the previous one.}
\blfootnote{$^\star$Note that ResNets of~\cite{he2016deep} commonly employed in practice have 3 reduction cells instead of 2 and have other minor differences (e.g. the order of convolution, BN and ReLU, down sampling convolution type, bottleneck layers, etc.). We still can predict \params for them, but such architectures would be further away from the training distribution, so the predicted \params might have significantly lower performance.\vspace{-15pt}}



\begin{table}[thbp]
	\centering
	\caption{\small Summary of differences between the DARTS design space and ours. \textsuperscript{*}Means implementation-wise possibility of predicting \params given a trained GHN, and does not mean our testing \iid architectures, which follow the training design protocol. Overall, from the implementation point of view, our trained GHNs allow to predict \params for arbitrary DAGs composed of our 15 primitives with the parameters of arbitrary shapes$^{\mathparagraph}$. We place ResNet-50 in a separate column even though it is one of the evaluation architectures of \dataset, because it has different properties as can be seen in the table.}
	\label{tab:darts_diff}
	\vspace{3pt}
	\setlength{\tabcolsep}{2.5pt}
	\small
	\begin{tabular}{lcccc}
		\toprule
		
		\textbf{\textsc{Property}} & \textbf{\textsc{DARTS}} & \textbf{\textsc{\dataset}} & \textbf{\textsc{ResNet-50}} & \textbf{\textsc{Testing GHN}}\textsuperscript{*} \\
		\midrule
		
		Unified style across image datasets & \xmark & \cmark & \xmark & \cmark \\
		
		VGG style classification heads~\cite{simonyan2014very} & \xmark & \cmark & \xmark & \cmark \\
		
		Visual Transformer stem~\cite{dosovitskiy2020image} & \xmark & \cmark & \xmark & \cmark \\
		
		Channel expansion ratio & 2 & 1 or 2 & 2 & arbitrary \\
		
		Bottleneck layers (e.g. in ResNet-50~\cite{he2016deep}) & \xmark & \xmark & \cmark & \cmark \\
		
		Reduction cells position (w.r.t. total depth) & 1/3, 2/3 & 1/3, 2/3 & 3,7,17 cells & arbitrary \\
		
		Networks w/o $1\PLH 1$ preprocessing layers in cells & \xmark & \cmark & \cmark & \cmark \\
		
		Networks w/o batch norm & \xmark & \cmark & \xmark & \cmark \\
		
		\bottomrule
	\end{tabular}
\end{table}

\blfootnote{$^{\mathparagraph}$While we predefine a wide range of possible shapes in our GHNs according to \S~\ref{apdx:ghn_1}, in the rare case of using the shape that is not one of the predefined values, we use the closest values, which worked reasonably well in many cases.}

\subsection{\dataset Statistics\label{apdx:stats}}

We show the statistics of the key properties of our \dataset in Fig.~\ref{fig:vis_stats} and more examples of computational graphs for different subsets in Fig.~\ref{fig:more_examples}.

\begin{figure}[tbhp]
	\centering
	\small
	\setlength{\tabcolsep}{0pt}
	\begin{tabular}{cccc}
		\includegraphics[width=0.25\textwidth,align=c,trim={0 0 0 0},clip]{node_distr2.pdf} & \includegraphics[width=0.25\textwidth,align=c,trim={0 0 0 0},clip]{node_types.pdf} &
		\includegraphics[width=0.25\textwidth,align=c,trim={0 0 0 0},clip]{channel_distr2.pdf} &
		\includegraphics[width=0.25\textwidth,align=c,trim={0 0 0 0},clip]{figs/params_distr2.pdf}\\
		(a) & (b) & (c) & (d) \\
		\includegraphics[width=0.25\textwidth,align=c,trim={0 0 0 0},clip]{path_vs_degree.pdf} & 
		\includegraphics[width=0.25\textwidth,align=c,trim={0 0 0 0},clip]{figs/err_vs_path_c10.pdf} & 
		\includegraphics[width=0.25\textwidth,align=c,trim={0 0 0 0},clip]{figs/err_vs_path_imagenet.pdf} &
		\includegraphics[width=0.25\textwidth,align=c,trim={0 0 0 0},clip]{figs/hungarian_dist2.pdf}\\
		(e) & (f) & (g) & (h) \\
	\end{tabular}
	\vspace{-2pt}
	\caption{\small Visualized statistics of \dataset. (\textbf{a}) A violin plot of the number of nodes showing the distribution shift for the \deep and \dense subsets. (\textbf{b}) Node types (primitives) showing the distribution shift for the \bnfree subset. (\textbf{c}) Number of initial channels in networks (for \iidtrain, the number is for training GHNs on CIFAR-10). Here, the distribution shift is present for all test subsets (due to computational challenges of training GHNs on wide architectures), but the largest shift is for \wide. (\textbf{d}) Total numbers of trainable parameters in case of CIFAR-10, where the distribution shifts are similar to those for the number of channels. (\textbf{e}) Average shortest path length versus average node degree (other subsets that are not shown follow the distribution of \iidtrain), confirming that nodes of the \dense subset have generally more dense connections (larger degrees), while in \deep the networks are deeper (the shortest paths tend to be longer). (\textbf{f}) The validation error for 1000 \iidval+\iidtest architectures (trained with SGD for 50 epochs) versus their average shortest path lengths, indicating the ``sweet spot'' of architectures with strong performance (same axes as in~\cite{you2020graph}). (\textbf{g}) Same as ({f}), but with the y axis being the top-5 validation error on ImageNet of the same architectures trained for 1 epoch according to our experiments. (\textbf{h}) The distribution of the distances between the architectures of a given test subset and the ones from a subset of \iidtrain computed using the Hungarian algorithm~\cite{kuhn1955hungarian}, confirming that the evaluation architectures are different from the training ones.\looseness-1}
	\label{fig:vis_stats}
	\vspace{-2pt}
\end{figure}


\begin{figure}[thbp]
	\centering
	\newcommand{\width}{0.19\textwidth}
	\setlength{\tabcolsep}{0pt}
	\begin{tabular}{cp{0.2cm}cccc}
		\toprule
		\multicolumn{1}{c}{{ \textbf{\textsc{In-Distribution}}}} & &
		\multicolumn{4}{c}{{ \textbf{\textsc{Out-of-Distribution}}}}
		\Bstrut\\
		{\small \textbf{\iidtrain/\iidval/\iidtest}} & & {\small \textbf{\wide}} & {\small \textbf{\deep}} & {\small \textbf{\dense}} & {\small \textbf{\bnfree}} \\
		\cline{1-1}\cline{3-6} \\[-2ex]
		\multicolumn{1}{c}{{\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_train_1.pdf}}} & & {\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_1.pdf}} & 
		{\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_ood_deep36_1.pdf}} & 
		\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_ood_conn_1.pdf} & 
		{\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_ood_nobn_1.pdf}}\Bstrut\\
		
		\multicolumn{1}{c}{{\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_train_2.pdf}}} & & {\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_2.pdf}} & 
		{\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_ood_deep36_2.pdf}} & 
		\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_ood_conn_2.pdf} & 
		{\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_ood_nobn_2.pdf}}\Bstrut\\
		
		\multicolumn{1}{c}{{\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_train_3.pdf}}} & & {\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_3.pdf}} & 
		{\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_ood_deep36_3.pdf}} & 
		\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_ood_conn_3.pdf} & 
		{\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_ood_nobn_3.pdf}}\Bstrut\\
		\bottomrule
	\end{tabular}
	\caption{Examples of graphs in each subset of our \dataset visualized using NetworkX~\cite{hagberg2008exploring}.}
	\label{fig:more_examples}
	\vspace{-2pt}
\end{figure}

\section{GHN Details\label{apdx:ghn_bg}}

\subsection{Baseline \textsc{Ghn}: \ghnbase\label{apdx:ghn_1}}

GHNs were designed for NAS, which typically make strong assumptions about the choice of operations and their possible dimensions to make search and learning feasible. For example, non-separable 2D convolutions (e.g. with weights like $512 \PLH 512 \PLH 3 \PLH 3$ in ResNet-50) are not supported. Our parameter prediction task is more general than NAS, and tackling it using the vanilla GHNs of~\cite{zhang2018graph} is not feasible (mainly, in terms of GPU memory and training efficiency) as we show in \S~\ref{apdx:ablations} (Table~\ref{tab:ablations_more}). So we first make the following modifications to GHNs and denote this baseline as \ghnbase.\looseness-1


\begin{enumerate}
	
	\item \textbf{Compact decoder:} We support the prediction of full 4D weights of shape $<$\textit{out channels $\PLH$ input channels $\PLH$ height $\PLH$ width}$>$	
	that is required for non-separable 2D convolutions.
	Using an MLP decoder of vanilla GHNs~\cite{zhang2018graph} would require it to have a prohibitive number of parameters (e.g.~$\sim$4 billion parameters, see Table~\ref{tab:ablations_more}).
	To prevent that, we use an MLP decoder only to predict a small 3D tensor and then apply a $1 \PLH 1$ convolutional layer across the channels to increase their number followed by reshaping it to a 4D tensor.
	
	\item  \textbf{Diverse channel dimensions:}
	To enable prediction of \params with channel dimensions larger than observed during training we implement a simple tiling strategy similar to~\cite{ha2016hypernetworks}. In particular, instead of predicting a tensor of the maximum shape that has to be known prior training (as done in~\cite{zhang2018graph}), we predict a tensor with fewer channels, but tile across channel dimensions as needed.
	Combining this with \#1 described above, our decoder first predicts a tensor of shape $128 \PLH {\cal S} \PLH {\cal S}$, where ${\cal S}=11$ for CIFAR-10 and ${\cal S}=16$ for ImageNet. Then, the $1 \PLH 1$ convolutional decoder with 256 hidden and 4096 output units transforms this tensor to the tensor of shape $64 \PLH 64 \PLH {\cal S} \PLH {\cal S}$.
	Modifications \#1 and \#2 can be viewed as strong regularizers that can hinder expressive power of a GHN and the \params it predicts, but on the other side permit a more generic and efficient model with just around 1.6M-2M parameters.
	
	\item  \textbf{Fewer decoders:} One alternative strategy to reduce the number of parameters in the decoder of GHNs is to design multiple specialized decoders. However, this strategy does not scale well with adding new operations. Our modifications \#1 and \#2 allow us to have only three decoders:
	for convolutional and fully connected weights, for 1D weights and biases, such as affine transformations in normalization layers, and for the classification layer. 
	
	\item \textbf{Shape encoding:} The vanilla GHNs does not leverage the information about channel dimensionalities  of parameters in operations. For example, the vanilla GHNs only differentiate between $3\PLH 3$ and $5\PLH 5$ convolutions, but not between $512 \PLH 512 \PLH 3\PLH 3$ and $512 \PLH 256 \PLH 3\PLH 3$.
	To alleviate that, we add two shape embedding layers: one for the spatial and one for the channel dimensions.
	For the spatial dimensions (height and width of convolutional weights) we predefine 11 possible values from 1 to $S$. 
	For the channel dimensions (input and output numbers of channels) we predefine 392 possible values from 1 to 8192. We describe 3D, 2D and 1D tensors as special cases of 4D tensors using the value of 1 for missing dimensionalities (e.g. $10\PLH1\PLH1\PLH1$ for CIFAR-10 biases in the classification layer). The shape embedding layers transform the input shape into four (two for spatial and two for channel dimensions) 8-dimensional learnable vectors that are concatenated to obtain a 32-dimensional vector. This vector is summed with a 32-dimensional vector encoding one of the 15 operation types (primitives). In the rare case of feeding the shape that is not one of the predefined values, we look up for the closest value. This can work well in some cases, but can also hurt the quality of predicted parameters, so some continuous encoding as in~\cite{vaswani2017attention} can be used in future work.
\end{enumerate}


\subsection{Our improved \textsc{Ghn}: \ghnours\label{apdx:ghn_2}}

\subsubsection{\ghnours Architecture}
Our improved \ghnours is obtained by adding to \ghnbase the differentiable normalization of predicted \params, virtual edges (and the associated `mlp\_ve' module, see the architecture below), meta-batching and layer normalization (and the associated `ln' module). A high-level PyTorch-based overview of the \ghnours model architecture used to predict the \params for ImageNet is shown below (see our code for implementation details).


\vspace{15pt}

\begin{small}
	\begin{Verbatim}[commandchars=\\\{\}]
	(ghn): GHN(  \textcolor{green}{# our hypernetwork H_D with total 2,319,752 parameters (theta)}
	(gcn): GCNGated(  \textcolor{green}{# Message Passing for Equations 3 and 4}
	(mlp): MLP(
	(fc): Sequential(
	(0): Linear(in_features=32, out_features=16, bias=True)
	(1): ReLU()
	(2): Linear(in_features=16, out_features=32, bias=True)
	(3): ReLU()
	)
	)
	(mlp_ve): MLP(  \textcolor{green}{# only in GHN-2 (see Eq. 4)}
	(fc): Sequential(
	(0): Linear(in_features=32, out_features=16, bias=True)
	(1): ReLU()
	(2): Linear(in_features=16, out_features=32, bias=True)
	(3): ReLU()
	)
	)
	(gru): GRUCell(32, 32)  \textcolor{green}{\scriptsize # all GHNs use d=32 for input, hidden and output node feature dimensionalities}
	)
	(ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)  \textcolor{green}{# only in GHN-2}
	(shape_embed_spatial): Embedding(11, 8)  \textcolor{green}{# encodes the spatial shape of predicted params}
	(shape_embed_channel): Embedding(392, 8)  \textcolor{green}{# encodes the channel shape of predicted params}
	(op_embed): Embedding(16, 32)  \textcolor{green}{# 15 primitives plus one extra embedding for dummy nodes}
	(decoder): Decoder(
	(fc): Sequential(
	(0): Linear(in_features=32, out_features=32768, bias=True)
	(1): ReLU()
	)  \textcolor{green}{# predicts a 128x16x16 tensor}
	(conv): Sequential(
	(0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
	(1): ReLU()
	(2): Conv2d(256, 4096, kernel_size=(1, 1), stride=(1, 1))
	)  \textcolor{green}{# predicts a 64x64x16x16 tensor}
	(class_layer_predictor): Sequential(
	(0): ReLU()
	(1): Conv2d(64, 1000, kernel_size=(1, 1), stride=(1, 1))
	)  \textcolor{green}{# predicts 64x1000 classification weights given the 64x64x16x16 tensor}
	)
	(norm_layers_predictor): Sequential(
	(0): Linear(in_features=32, out_features=64, bias=True)
	(1): ReLU()
	(2): Linear(in_features=64, out_features=128, bias=True)
	)  \textcolor{green}{# predicts 1x64 weights and 1x64 biases given the 1x32 node embeddings}
	(bias_predictor): Sequential(
	(0): ReLU()
	(1): Linear(in_features=64, out_features=1000, bias=True)
	)  \textcolor{green}{# predicts 1x1000 classification biases given the 64x64x16x16 tensor}
	)
	\end{Verbatim}
\end{small}


\subsubsection{Differentiable Normalization of Predicted Parameters\label{apdx:renorm}}

We analyze in more detail the effect of normalizing predicted parameters (Fig.~\ref{fig:activations}).

\paragraph{Setup.}
As we discussed in \S~\ref{sec:renorm}, \citet{chang2019principled} proposed a method to initialize a hypernetwork to stabilize the activations in the network for which the parameters are predicted. However, this technique requires knowing upfront the shapes of the predicted \params, and therefore is not applicable out-of-the-box in our setting, where we predict the \params of diverse architectures with arbitrary shapes of weights. So, instead we apply {operation-dependent normalizations}. We analyze the effect of this normalization by taking a GHN in the beginning and end of training on CIFAR-10 and predicting parameters of ResNet-50. To compute the variance of activations in the ResNet-50, we forward pass a batch of test images through the predicted parameters. 

\paragraph{Observations.} The activations obtained using \ghnbase explode after training, which aligns with the analysis of \cite{chang2019principled} (this is more obvious on the left of Fig.~\ref{fig:activations}, where a linear scale is used on the y axis). For \ghnours, in the beginning of training the activations match the ones of ResNet-50 initialized randomly using Kaiming He's method~\cite{he2015delving}, which validates the choice of our normalization equations in \S~\ref{sec:renorm}. By the end of training, the activations of models for the random-based and the \ghnours-based cases decrease (perhaps, due to the weight decay), however, the ones of \ghnours reduce less, indicating that the predicted \params do not reach the state of those trained with SGD from scratch. In contrast, the activations corresponding to \ghnbase have small values in the beginning, but explode by the end of training. Matching the activations of the models trained with SGD can be useful to improve training of GHNs and, for example, to make fine-tuning of predicated parameters easier as we show in \S~\ref{sec:finetune}, where the parameters predicted by \ghnbase are shown difficult to be fine-tuned with SGD.\looseness-1

\begin{figure}[tbhp]
	\vspace{-1pt}
	\centering
	\setlength{\tabcolsep}{10pt}
	\begin{tabular}{cc}
		\includegraphics[width=0.46\textwidth,align=c,trim={0 0 0 0cm}, clip]{variances.pdf} & \includegraphics[width=0.46\textwidth,align=c,trim={0 0 0 0cm}, clip]{figs/variances_log10.pdf}
	\end{tabular}
	\vspace{-5pt}
	\caption{The effect of normalizing predicted parameters on the variance of activations in the first several layers of ResNet-50: a linear (\textbf{left}) and log (\textbf{right}) scale on the y axis. }
	\label{fig:activations}
	\vspace{-1pt}
\end{figure}

\subsubsection{Meta-batching\label{apdx:meta}}
We analyze how meta-batching affects the training loss when training GHNs on CIFAR-10 (Fig.~\ref{fig:meta_batch}). The loss of the \ghnours with $b_m=8$ is less noisy and is lower throughout the training compared to using $b_m=1$. In fact, the loss of $b_m=1$ is unstable to the extent that oftentimes the training fails due to the numerical overflow, in which case we ignore the current architecture and resample a new one.
For example, the standard deviation of gradient norms with $b_m=8$ is significantly lower than with $b_m=1$: 18 vs 145 with means 2.7 and 7.4 respectively.
Training the model with $b_m=1$ eight times longer (Fig.~\ref{fig:meta_batch}, right) boosts the performance of predicted parameters (Table~\ref{tab:ablations_more}), but still does not reach the level of $b_m=8$; it also still suffers from the aforementioned numerical issues and does not leverage the parallelism of $b_m=8$ (all architectures in a meta-batch can be processed in parallel). 
Further increasing the meta-batch size is an interesting avenue for future research.

\begin{figure}[tbhp]
	\vspace{-1pt}
	\centering
	\setlength{\tabcolsep}{10pt}
	\begin{tabular}{cc}
		\includegraphics[width=0.46\textwidth,align=b,trim={0 0 0 0cm}, clip]{b8_vs_b1_c10.pdf} & \includegraphics[width=0.46\textwidth,align=b,trim={0 0 0 0cm}, clip]{b8_vs_b1_c10_long2.pdf}\\
	\end{tabular}
	\vspace{-5pt}
	\caption{Effect of using more architectures per batch of images on the training loss (\textbf{left}) and comparison to training longer (\textbf{right}). In the figure on the right, we shrink the x axis of the $b_m=1$ case, so that both plots can be compared w.r.t. the total number of epochs.   }
	\label{fig:meta_batch}
	\vspace{-1pt}
\end{figure}

\section{Additional Experiments and Details\label{apdx:exper}}


\subsection{Experimental Details\label{apdx:details}}


\paragraph{Training GHNs.} 
We train all GHNs for 300 epochs using Adam with an initial learning rate 0.001, batch size of 64 images for CIFAR-10 (256 for ImageNet), weight decay of 1e-5. The learning rate is decayed after 200 and 250 epochs. On ImageNet \ghnours's validation accuracy plateaued faster (in terms of training epochs), so we stopped training after 150 epochs decaying it after 100 and 140 epochs.\looseness-1


\paragraph{Evaluation of networks with BN using GHNs.} While our GHNs predict all \textit{trainable} parameters, batch norm networks have running statistics that \textit{are not learned by gradient descent}~\cite{ioffe2015batch}, and so are not predicted by GHNs. To obtain these statistics, we evaluate the networks with BN by computing per batch statistics on the test set as in \cite{zhang2018graph} using a batch size 64. Another alternative we have considered is updating the running statistics by forward passing several batches of the training or testing images through each evaluation network (no labels, gradients or parameter updates are required for this stage). For example on CIFAR-10 if we forward pass 200 batches of the training images (takes less than 10 seconds on a GPU), we obtain a higher accuracy of 71.7\% on \iid-\iidtest compared to 66.9\% when the former strategy is used. On ImageNet, the difference between the two strategies is less noticeable.
For both strategies, the results can slightly change depending on the order of images for which the statistics is estimated. 


\subsection{Additional Results\label{apdx:results}}

\subsubsection{Additional Ablations\label{apdx:ablations}}

We present results on CIFAR-10 for different variants of GHNs (Table~\ref{tab:ablations_more}) in addition to those presented in Tables~\ref{tab:bench_c10} and~\ref{tab:ablations}.
Overall, different GHN variants show worse or comparable results on all evaluation architectures compared to our \ghnours, while in some cases having too many trainable parameters making training infeasible in terms of GPU memory or being less efficient to train (e.g. with $T=5$ propagation steps).
For ViT, \ghnours is worse compared to other GHN variants, which requires further investigation.

\begin{table}[tbhp]
	\caption{\small CIFAR-10 results of predicted parameters for the evaluation architectures of \dataset. Mean (\sem{}standard error of the mean) accuracies are reported. Different ablated \ghnours models are evaluated. \textsuperscript{*}GPU seconds per a batch of 64 images and $b_m$ architectures. \textsuperscript{**}In~\cite{zhang2018graph} the GHNs have fewer parameters due to a more constrained network design space (as discussed in~\ref{apdx:ghn_1}) and applying specialized decoders for different operations. The best result in each column is bolded, the best results with $b_m = 1$ (excluding training 8 $\PLH$ longer) are underlined.
	}
	\label{tab:ablations_more}
	\centering
	\tiny
	\vspace{3pt}
	\setlength{\tabcolsep}{2pt}
	\newcommand{\better}[1]{\underline{#1}}
	\rowcolors{4}{white}{gray!10}
	\begin{tabular}{lcccp{0.1cm}ccp{0.1cm}llp{0.1cm}lllll}
		\toprule
		\textbf{\textsc{\small Model}} & \textbf{Norm} & \textbf{Virt.} & \textbf{M. batch} & & \textbf{\#GHN} & \textbf{Train.} & & \multicolumn{2}{c}{\textbf{\textsc{\small \iid-test}}} &
		& 
		\multicolumn{5}{c}{\small \textbf{\textsc{OOD-test}}} \\
		& $\hat{\w}_p$ & \textbf{edges} & $b_m=8$ & & \textbf{params} & \textbf{speed\textsuperscript{*}} & & \multicolumn{1}{c}{avg} & max & & \wide & \deep & \dense & \bnfree & \tiny \textsc{ResNet/ViT}\Bstrut\\ 
		\cline{1-4}\cline{6-7}\cline{9-10}\cline{12-16} \\
		\ghnours & \cmark & \cmark & \cmark & & 1.6M & 3.6 & & \textbf{66.9}\sem{0.3} & {77.1} & & \textbf{64.0}\sem{1.1} & \textbf{60.5}\sem{1.2} & \textbf{65.8}\sem{0.7} & {36.8}\sem{1.5} & \textbf{58.6}/11.4\Bstrut\\
		\midrule
		
		
		1000 archs & \cmark & \cmark & \cmark & & 1.6M & 3.6 & & 65.1\sem{0.5} & \textbf{78.4} & & 61.5\sem{1.6} & 56.0\sem{1.5} & 65.0\sem{0.9} & 27.6\sem{1.1} & 58.2/10.5\Tstrut \\
		100 archs & \cmark & \cmark & \cmark & & 1.6M & 3.6 & & 47.1\sem{0.8} & 77.1 & & 38.8\sem{1.9} & 28.3\sem{1.6} & 41.9\sem{1.5} & 11.0\sem{0.2} & 38.7/10.3 \\
		No normalization & \xmark & \cmark & \cmark & & 1.6M & 3.6 & & 62.6\sem{0.6} & 75.9 & & 52.3\sem{2.1} & 59.5\sem{1.1} & 62.3\sem{1.2} & 14.4\sem{0.4} & 58.3/17.0 \\
		No virtual edges & \cmark & \xmark & \cmark & & 1.6M & 3.6 & & 61.5\sem{0.4} & 73.2 & & 58.2\sem{1.0} & 55.0\sem{0.9} & 61.5\sem{0.6} & \textbf{40.8}\sem{0.8} & 41.9/12.1\\
		No LayerNorm & \cmark & \cmark & \cmark & & 1.6M & 3.6 & & 64.5\sem{0.4} & 75.9 & & 62.4\sem{1.1} & 59.0\sem{1.1} & 64.6\sem{0.6} & 39.6\sem{1.2} & 55.1/8.9\\
		No GatedGNN (MLP) & \cmark & \cmark & \cmark & & 1.6M & 1.5 & & 42.2\sem{0.6} & 60.2 & & 22.3\sem{0.9} & 37.9\sem{1.2} & 44.8\sem{1.1} & 23.9\sem{0.7} & 17.7/10.0\\
		Train 8$\PLH$ longer & \cmark & \cmark & \xmark & & 1.6M & 0.7 & & 62.4\sem{0.5} & 75.8 & & 63.0\sem{1.3} & 58.0\sem{1.3} & 62.1\sem{0.9} & 24.5\sem{0.7} & 57.0/14.6\Bstrut\\
		\midrule
		
		No Meta-batch ($b_m=1$) & \cmark & \cmark & \xmark & & 1.6M & 0.7 & & 54.3\sem{0.3} & 63.0 & & \better{53.1}\sem{0.8} & 51.9\sem{0.6} & 53.4\sem{0.5} & \better{31.7}\sem{0.8} & \better{50.6}/\better{17.8}\Tstrut\\
		
		No Shape Encoding & \cmark & \cmark & \xmark & & 1.6M & 0.7 & & 53.1\sem{0.4} & 61.7 & & 52.4\sem{0.9} & 51.4\sem{0.7} & 53.5\sem{0.6} & 24.7\sem{0.8} & 31.5/14.0\\
		
		No virtual edges (VE) & \cmark & \xmark & \xmark & & 1.6M & 0.7 & & 51.7\sem{0.4} & 62.0 & & 49.7\sem{0.8} & 47.4\sem{0.8} & 52.0\sem{0.8} & 24.5\sem{0.5} & 34.2/14.7 \\
		
		+Stacked GHN, no VE & \cmark & \xmark & \xmark & & 1.6M & 0.7 & & 52.2\sem{0.4} & 63.6 & & 51.3\sem{0.9} & 46.9\sem{0.9} & 52.3\sem{0.7} & 20.2\sem{0.6} & 44.5/15.4\\
		
		+Stacked GHN & \cmark & \cmark & \xmark & & 1.6M & 0.9 & & 53.1\sem{0.4} & 61.3 & & 51.5\sem{1.1} & 50.9\sem{0.7} & 53.5\sem{0.7} & 23.1\sem{0.7} & 42.7/15.1 \\
		
		
		$\pi=$ only fw (Eq.~\ref{eq:ghn_prop}) & \cmark & \cmark & \xmark & & 1.6M & 0.4 & & 53.9\sem{0.3} & 62.5 & & 51.2\sem{1.0} & 51.7\sem{0.7} & \better{54.3}\sem{0.5} & 31.0\sem{0.7} & 49.9/11.5 \\
		$T=5$ (Eq.~\ref{eq:ghn_prop}) & \cmark & \cmark & \xmark & & 1.6M & 2.6 & & \better{54.4}\sem{0.4} & 63.3 & & 52.8\sem{1.0} & 50.4\sem{0.9} & 53.4\sem{0.8} & 22.6\sem{1.0} & 50.1/10.1\\ 
		Fan-out & \cmark & \cmark & \xmark & & 1.6M & 0.7 & & 53.8\sem{0.4} & 63.6 & & 52.6\sem{1.0} & 51.2\sem{0.8} & \better{54.3}\sem{0.8} & 19.8\sem{0.6} & 48.5/11.1\\
		MLP decoder & \cmark & \cmark & \xmark & & 32M & 0.7 & & 53.1\sem{0.4} & \better{64.0} & & 52.9\sem{1.0} & \better{52.5}\sem{0.7} & 54.0\sem{0.8} & 22.1\sem{0.5} & 44.1/16.3\\
		No tiling & \cmark & \cmark & \xmark & & 135M & & & \multicolumn{8}{c}{out of GPU memory} \\
		\parbox{2.3cm}{No tiling, MLP decoder \\ (as in a vanilla GHN~\cite{zhang2018graph})} & \cmark & \cmark & \xmark  & & 4.1B\textsuperscript{**} & & & \multicolumn{8}{c}{out of GPU memory} \Bstrut\\
		\midrule
		\ghnbase & \xmark & \xmark & \xmark & & 1.6M & 0.6 & & 51.4\sem{0.4} & 59.9 & & 43.1\sem{1.7} & 48.3\sem{0.8} & 51.8\sem{0.9} & 13.7\sem{0.3} & 19.2/\textbf{18.2}\Tstrut \\
		\ghnbase + LayerNorm & \xmark & \xmark & \xmark & & 1.6M & 0.6 & & 50.1\sem{0.5} & 58.9 & & 43.8\sem{1.4} & 47.5\sem{0.8} & 50.8\sem{1.0} & 11.4\sem{0.2} & 49.2/16.3\\ 
		\bottomrule
	\end{tabular}
\end{table}


\subsubsection{Generalization Properties\label{apdx:gener}}
On the \ood subsets, GHN results are lower than on \iid-\iidtest as expected, so we inspect in more detail \textit{how} performance changes with an increased distribution shift (Fig.~\ref{fig:generalize_all}).
For example, training wider nets with SGD leads to similar or better performance, perhaps, due to increased capacity. 
However, GHNs degrade with larger width, since wide architectures are underrepresented during training for computational reasons (Table~\ref{tab:graphs}, Fig.~\ref{fig:vis_stats}). As for the depth and number of nodes, there is a certain range of values (``sweet spot'') with higher performance. For architectures without batch norm (Fig.~\ref{fig:generalize_all}, the very right column), the results of \ghnours are strong starting from a certain depth ($\geq$ 8-10 cells) matching the ones of training with SGD from scratch (for 50 epochs on CIFAR-10 and 1 epoch on ImageNet). This can be explained by the difficulty of training models without BN from scratch with SGD, while parameter prediction with \ghnours is less affected by that.
Generalization appears to be worse on ImageNet, perhaps due to its more challenging nature.


\begin{figure}[tbhp]
	\centering
	\setlength{\tabcolsep}{1pt}
	\begin{tabular}{cccc}
		{\includegraphics[width=0.24\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip,align=c]{c10_acc_vs_channels.pdf}} & \includegraphics[width=0.24\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip,align=c]{c10_acc_vs_layers.pdf} &
		\includegraphics[width=0.24\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip,align=c]{c10_acc_vs_nodes.pdf} &
		\includegraphics[width=0.24\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip,align=c]{c10_acc_vs_bn.pdf} \\
		\includegraphics[width=0.24\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip,align=c]{imagenet_acc_vs_channels.pdf} & \includegraphics[width=0.24\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip,align=c]{imagenet_acc_vs_layers.pdf} &
		\includegraphics[width=0.24\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip,align=c]{imagenet_acc_vs_nodes.pdf} &
		\includegraphics[width=0.24\textwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip,align=c]{imagenet_acc_vs_bn.pdf} \\
	\end{tabular}
	\caption{Generalization performance w.r.t. (from left to right): width, depth, number of nodes and depths for architectures without batch norm. A green rectangle denotes the training regime; bars are standard errors of the mean. Top row: CIFAR-10, bottom row: ImageNet.}
	\label{fig:generalize_all}
\end{figure}



\subsubsection{Property Prediction\label{apdx:prop}}

In \S~\ref{sec:prop_pred}, we experiment with four properties of neural architectures that can be estimated given an architecture and image dataset:
\begin{enumerate}
	\item ``Clean'' classification accuracy measured on the validation sets of CIFAR-10 and ImageNet.
	\item Classification accuracy on the corrupted images, which is created by adding the Gaussian noise of medium intensity to the validation images following~\cite{hendrycks2019benchmarking}\footnote{https://github.com/hendrycks/robustness}: with zero mean and the standard deviation of 0.08 on CIFAR-10 and 0.18 on ImageNet.
	\item The inference speed is measured as GPU seconds required to process the entire validation set.\looseness-1
	\item For the convergence speed, we measure the number of SGD iterations to achieve a training top-1 accuracy of 95\% on CIFAR-10 and top-5 accuracy of 10\% on ImageNet.
\end{enumerate}

\begin{figure}[tbhp]
	\centering
	\vspace{-10pt}
	\small
	\setlength{\tabcolsep}{0pt}
	\begin{tabular}{ccc}
		{\includegraphics[width=0.33\textwidth,align=c,trim={0 0cm 0 0cm},clip]{properties_confusion_kendal_c10.pdf}} & {\includegraphics[width=0.33\textwidth,align=c,trim={0 0cm 0 0cm},clip]{properties_confusion_kendal_imagenet.pdf}} &
		{\includegraphics[width=0.33\textwidth,align=c,trim={0 0cm 0 0cm},clip]{properties_confusion_kendal_c10_imagenet.pdf}}\\
		\textbf{CIFAR-10} & \textbf{ImageNet} & \textbf{CIFAR-10 vs ImageNet} \vspace{0pt}\\
	\end{tabular}
	\vspace{-5pt}
	\caption{Cross correlation (Kendall's Tau) between ground truth values of properties.}
	\label{fig:properties_cross}
	\vspace{-5pt}
\end{figure}

\paragraph{Ground truth property values.} We first obtain ground truth values for each property for each of the 500 \iid-\iidval and 500 \iid-\iidtest architectures of \dataset trained from scratch with SGD for 50 epochs (on CIFAR-10) and 1 epoch (on ImageNet) as described in \S~\ref{sec:exper}. The ground truth values between these properties and between CIFAR-10 and ImageNet correlate poorly (Fig.~\ref{fig:properties_cross}). Interestingly, on CIFAR-10 the architectures that rank higher on the clean images generally rank lower on the corrupted set and vice verse (correlation is -0.40). On ImageNet the correlation between these two properties is positive, but low (0.15). Also, the transferability of architectures between CIFAR-10 and ImageNet is poor contrary to a common belief, e.g. with the correlation of 0.29 on the clean and -0.06 on the corrupted sets. However, this might be due to training on ImageNet only for 1 epoch. The networks that converge faster have generally worse performance on the clean set, but tend to perform better on the corrupted set. The networks that classify images faster (have higher inference speed), also tend to perform better on the corrupted set. Investigating the underlying reasons for these relationships as well as extending the set of properties would be interesting in future work.


\paragraph{Estimating properties by predicting parameters.}  A straightforward way to estimate this kind of properties using GHNs is by predicting the architecture parameters and forward passing images as was done in~\cite{zhang2018graph} for accuracy. 
However, this strategy has two issues: (a) the performance of parameters predicted by GHNs is strongly affected by the training distribution of architectures (Fig.~\ref{fig:generalize_all});
(b) estimating properties of thousands of networks for large datasets such as ImageNet can be time consuming.
Regarding (a), for example the rank correlation on CIFAR-10 between the accuracy of the parameters predicted by \ghnours and those trained with SGD is only 0.4 (down from 0.8 obtained with the regression model, Fig.~\ref{fig:properties}). 

\paragraph{Training regression models.} To report the results in Fig.~\ref{fig:properties}, we treat the 500 \iid-\iidval architectures of \dataset as the training ones in this experiment. We train a simple regression model for each property using graph embeddings (obtained using MLP, \ghnbase or \ghnours) and ground truth property values of these architectures. We use Support Vector Regression\footnote{https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html} with the RBF kernel and tune hyperparameters (C, gamma, epsilon) on these 500 architectures using 5-fold cross-validation. We then estimate the properties given a trained regression model on the 500 \iid-\iidtest architectures of \dataset and measure Kendall's Tau rank correlation with the ground truth test values. We repeat the experiment 5 times with different random seeds, i.e. different cross-validation folds. In Fig.~\ref{fig:properties}, we show the average and standard deviation of correlation results across 5 runs.
To train the graph convolutional network of Neural Predictor ~\cite{wen2020neural}, we use the open source implementation\footnote{\url{https://github.com/ultmaster/neuralpredictor.pytorch}} and train it on the 500 validation architectures from scratch for each property.

\paragraph{Downstream results.} Next, we verify if higher correlations translate to downstream gains. We consider the clean accuracy on CIFAR-10 in this experiment as an example. We retrain the regression model on the graph embeddings of the combined \iid-\iidval and \iid-\iidtest sets and generate 100k new architectures (similarly to how \iid-\iidtest are generated) to pick the most accurate one according to the trained regression model. We train the chosen architecture from scratch following~\cite{liu2018darts,zhang2018graph,chen2019progressive}, i.e. with SGD for 600 epochs using cutout augmentation (C), an auxiliary classifier (A) and the drop path (D) regularization. In addition, we train the chosen architecture for just 50 epochs using the same hyperparameters we used to train our \iid-\iidval and \iid-\iidtest architectures as in \S~\ref{sec:our_task}. We train the chosen architecture 5 times using 5 random seeds and report an average and standard deviation of the final classification accuracy on the test set of CIFAR-10 (Table~\ref{tab:nas_results}). We perform this experiment for \ghnbase and \ghnours in the same way.
Among the methods we compare in Table~\ref{tab:nas_results}, our \ghnours-based search finds the most performant network if training is done for 50 epochs (without and with C, A and D) and finds a competitive network if training is done for 600 epochs with C, A and D.

\begin{table}[tbhp]
	\centering
	\caption{CIFAR-10 best architectures and their performance on the test set. C --- cutout augmentation, A --- auxiliary classifier, D --- drop path regularization. The best result in each row is bolded.}
	\label{tab:nas_results}
	\vspace{3pt}
	\small
	\setlength{\tabcolsep}{0pt}
	\begin{tabular}{lcccc}
		\toprule
		& \textbf{\ghnbase} & \textbf{\ghnours} & \textbf{DARTS}~\cite{liu2018darts} & \textbf{PDARTS}~\cite{chen2019progressive} \Bstrut\\
		\midrule
		& {\includegraphics[width=0.18\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_search_79995.pdf}} & {\includegraphics[width=0.18\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_search_35133.pdf}} & {\includegraphics[width=0.18\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{figs/dag_darts.pdf}} & {\includegraphics[width=0.18\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{figs/dag_pdarts.pdf}}\Tstrut\\
		\midrule
		\# params (M) &  3.1 & 3.1 & 3.3 & 3.4 \Tstrut\Bstrut\\
		50 epochs & 92.61\std{0.16} & \textbf{93.94}\std{0.11} & 93.11\std{0.09} & 92.95\std{0.14}\\
		50 epochs + C,A,D~\cite{liu2018darts} & 91.80\std{0.14} & \textbf{95.24}\std{0.14} & 94.50\std{0.08} & 94.22\std{0.06} \\
		600 epochs + C,A,D~\cite{liu2018darts} & 95.90\std{0.08} & 97.26\std{0.09} & 97.17\std{0.06} & \textbf{97.48}\std{0.06} \\
		\bottomrule
	\end{tabular}
\end{table}



\subsubsection{Comparing Neural Architectures\label{apdx:graph_compare}}

In addition to our experiments in \S~\ref{sec:prop_pred}, we further evaluate representation power of GHNs by computing the distance between neural architectures in the graph embedding space.

\textbf{Experimental setup.}
We compute the pairwise distance between the computational graphs $a_1$ and $a_2$ as the $\ell_2$ norm between their graph embedding $||\h_{a_1} - \h_{a_2}||$. 
We chose ResNet-50 as a reference network and compare its graph structure to the other three predefined architectures: ResNet-34, ResNet-50 without skip connections, and ViT. 
Among these, ViT is intuitively expected to have the largest $\ell_2$ distance, because it is mainly composed of non-convolutional operations. ResNet-50-No-Skip should be closer (in the $\ell_2$ sense) to ResNet-50 than ViT, because it is based on convolutions, but further away than ResNet-34, since it does not have skip connections. 

\textbf{Additional baseline.}
As a reference graph distance, we employ the Hungarian algorithm~\cite{kuhn1955hungarian}.
This algorithm computes the total assignment ``cost'' between two sets of nodes. It is used as an efficient approximation of the exact graph distance algorithms~\cite{ma2021deep,bai2019simgnn}, which are infeasible for graphs of the size we consider here.\looseness-1


\begin{table}[tbhp]
	\caption{Comparing ResNet-50 to the three predefined architectures using the GHNs trained on CIFAR-10 in terms of the $\ell_2$ distance between graph embeddings.}
	\label{fig:qualit}
	\vspace{5pt}
	\definecolor{bad}{rgb}{0.958, 0.788, 0.878}
	\centering
	\setlength{\tabcolsep}{0pt}
	\newcommand{\width}{0.2\textwidth}
	\scriptsize
	\renewcommand{\arraystretch}{0.8}
	\begin{tabular}{l|ccc}
		\multicolumn{1}{c|}{\scriptsize \scriptsize \textsc{ResNet-50}} & \scriptsize \textsc{ResNet-34} & \tiny \textsc{ResNet-50-No-Skip} & \scriptsize \textsc{ViT} \Bstrut\\
		\toprule
		{\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{figs/dag_resnet_50.pdf}} & 
		{\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{figs/dag_resnet_34.pdf}}
		& {\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{figs/dag_resnet_50_noskip.pdf}} & {\includegraphics[width=\width,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{figs/dag_transformer.pdf}}\Tstrut \\
		\midrule		
		Hungarian & 118.0 & 134.0 & 207.5 \\
		MLP & 0.4 & \cellcolor{bad}0.2 & 1.0\\
		\ghnbase & 0.6 & \cellcolor{bad}0.5 & 1.7 \\
		\ghnours & 1.0 & 1.3 & 3.1 \\
		\bottomrule
	\end{tabular}
	
\end{table}

\textbf{Qualitative results.}
The $\ell_2$ distances computed based on \ghnours align well with our initial hypothesis of the relative distances between the architectures as well as with the Hungarian distances (Table~\ref{fig:qualit}). In contrast, the baselines inappropriately embed ResNet-50 closer to ResNet-50-No-Skip  than to ResNet-34. Thus, based on this simple evaluation, \ghnours captures graph structures better than the baselines.

We further compare the architectures in the \iid-\iidtest set and visualize the most similar and dissimilar ones using our trained models. Based on the visualizations in Fig.~\ref{fig:compare}, MLP as expected is not able to capture graph structures, since it relies only on node features (the most similar architectures shown on the left are quite different). The difference between \ghnbase and \ghnours is hard to understand qualitatively, so we compare them numerically below.


\begin{figure}[htpb]
	\centering
	\scriptsize
	\vspace{0pt}
	\setlength{\tabcolsep}{1pt}
	\begin{tabular}{cc|cc}
		{\includegraphics[width=0.22\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_583.pdf}} & {\includegraphics[width=0.22\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_962.pdf}} & 
		{\includegraphics[width=0.22\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{figs/dag_test_757.pdf}} & {\includegraphics[width=0.22\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_549.pdf}} \\
		\multicolumn{2}{c}{$\ell_2=0.02$} & \multicolumn{2}{c}{$\ell_2=2.4$} \Bstrut \\
		
		{\includegraphics[width=0.22\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_703.pdf}} & {\includegraphics[width=0.22\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_376.pdf}} & 
		{\includegraphics[width=0.22\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{figs/dag_test_283.pdf}} & {\includegraphics[width=0.22\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_899.pdf}} \\
		\multicolumn{2}{c}{$\ell_2=0.03$} & \multicolumn{2}{c}{$\ell_2=3.4$} \Bstrut \\
		
		{\includegraphics[width=0.22\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_798.pdf}} & {\includegraphics[width=0.22\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_316.pdf}} & 
		{\includegraphics[width=0.22\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_73.pdf}} & {\includegraphics[width=0.22\textwidth,align=c,trim={2.3cm 3cm 2.3cm 3cm},clip]{dag_test_894.pdf}} \\
		\multicolumn{2}{c}{$\ell_2=0.08$} & \multicolumn{2}{c}{$\ell_2=3.6$}
	\end{tabular}
	\caption{Most similar (left) and dissimilar (right) architectures (in terms of the $\ell_2$ distance) in the \iid-\iidtest set based on the graph embeddings obtained by the MLP (top row), \ghnbase (middle row) and \ghnours (bottom row) trained on CIFAR-10.}
	\label{fig:compare}
	\vspace{0pt}
\end{figure}


\textbf{Quantitative results.} To quantify the representation power of GHNs, we exploit the fact that, by design, the \ood architectures in our \dataset are more dissimilar from the \iid architectures than the architectures within the \iid distribution. So, a strong GHN should reflect that design principle and map the \ood architectures further from the \iid ones in the embedding space. One way to measure the distance between two feature distributions, such as our graph embeddings, is the Fr\'{e}chet distance (FD)~\cite{dowson1982frechet}. We compute the FD between graph embeddings of 5000 training architectures and five test subsets (in the similar style as in~\cite{liu2019auto2,zilly2019frechet,thompson2020building}): \iid-\iidtest and four \ood subsets (Table~\ref{tab:fd}).
\ghnours maps the \iid-\iidtest architectures close to the training ones, while the \ood ones are further away, reflecting the underlying design characteristics of these subsets. 
On the other hand, both baselines inappropriately map the \deep and \dense architectures as close or even closer to the training ones than \iid-\iidtest, despite the differences in their graph properties (Table~\ref{tab:graphs}, Fig.~\ref{fig:vis_stats}). This indicates \ghnours's stronger representation compared to the \ghnbase and MLP baselines.

\begin{table}[htpb]
	\vspace{0pt}
	\centering
	\definecolor{bad}{rgb}{0.958, 0.788, 0.878}
	\caption{FD between test and training graph embeddings on CIFAR-10. The average assignment cost between two sets of nodes using the Hungarian algorithm is shown for reference.}
	\label{tab:fd}
	\vspace{3pt}
	\small
	\setlength{\tabcolsep}{5pt}
	\begin{tabular}{p{1.2cm}ccccc}
		\toprule
		\textbf{\textsc{Model}} & \textbf{\iid-\iidtest} & \textbf{\wide} & \textbf{\deep} & \textbf{\dense} & \textbf{\bnfree} \\
		\midrule
		Hungarian & 208.7 & 199.9 & 365.6 & 340.7 & 193.1 \\
		MLP & 0.50 & 1.04 & \cellcolor{bad}0.37 & \cellcolor{bad}0.39 & 1.10\Tstrut\\
		\ghnbase & 0.15 & 0.45 & \cellcolor{bad}0.16 & \cellcolor{bad}0.21 & 5.45 \\
		\ghnours & 0.30 & 0.80 & 1.11 & 0.59 & 3.64 \\
		\bottomrule
	\end{tabular}
	\vspace{0pt}
\end{table}

Alternative ways to compare graphs include running expensive graph edit distance (GED) algorithms, infeasible for graphs of our size, or designing task-specific graph kernels~\cite{kriege2020survey,yanardag2015deep,jin2019auto,ma2021deep}. As a more efficient (and less accurate) variant of GED, we employ the Hungarian algorithm. It finds the optimal assignment between two sets of nodes, so its disadvantage is that it ignores the edges. It still can capture the distribution shifts between graphs that can be detected based on the number of nodes and their features, such as for the \deep and \dense subsets (Table~\ref{tab:fd}). It does not differentiate \iidtrain and \bnfree, perhaps, due to the fact that a small portion of architectures in \iidtrain does not have BN. Finally, the inability of the Hungarian algorithm to capture the difference between \iidtrain and \wide can be explained by the fact that we ignore the shape of parameters when running this algorithm.\looseness-1



\subsection{Analysis of Predicted Parameters}

\subsubsection{Diversity}

We analyze how much parameter prediction is sensitive to the input network architecture. For that purpose, we analyze the parameters of 1,000 evaluation architectures (\iid-\iidval and \iid-\iidtest) of \dataset on CIFAR-10. We compare the diversity of the parameters trained with SGD from scratch (He’s initialization+SGD) to the diversity of the parameters predicted by GHNs. To analyze the parameters, we consider all the parameters associated with an operation, which is in general a 4D tensor (i.e. out channels $\PLH$ in channels $\PLH$ height $\PLH$ width). As tensor shapes vary drastically across architectures and layers, it is challenging to find a large set of tensors of the same shape. We found that the shape of 128$\PLH$1$\PLH$3$\PLH$3 is one of the most frequent ones: appearing 760 times across the evaluation architectures. So for a given method of obtaining parameters (i.e. He’s initialization+SGD or GHN), we obtain a set of 760 tensors with 128$\PLH$1$\PLH$3$\PLH$3 = 1152 values in each tensor, i.e. a 760$\PLH$1152 matrix. For each pair of rows in this matrix, we compute the absolute cosine distance, which results in a 760$\PLH$760 matrix with values in range [0,1]. We report the mean value of this matrix in Table~\ref{tab:sensitivity_sparsity}.\looseness-1

\textbf{Results.} The parameters predicted by GHN-2 are more diverse than the ones predicted by GHN-1: average distance is 0.17 compared to 0.07 respectively (Table~\ref{tab:sensitivity_sparsity}). The parameters predicted by MLPs are extremely similar to each other, since the graph structure is not exploited by MLPs. The cosine distance is not exactly zero for MLPs, because two different primitives (group convolution and dilated group convolution) can have the same 128$\PLH$1$\PLH$3$\PLH$3 shape. The parameters predicted by GHN-2 are more similar (low cosine distance) to each other compared to He’s initialization+SGD. Low cosine distances in case of GHNs indicate that GHNs ``store'' good parameters to some degree However, our GHN-2 seems to rely on storing the parameters less than GHN-1 and MLP. Instead, GHN-2 relies more on the input graph to predict more diverse parameters depending on the layer and architecture. So, GHN-2 is more sensitive to the input graph. We believe this is achieved by our enhancements, in particular virtual edges, that allow GHN-2 to better propagate information across nodes of the graph.

\begin{table}[tbhp]
	\caption{Analysis of predicted parameters on CIFAR-10.}
	\label{tab:sensitivity_sparsity}
	\vspace{3pt}
	\small
	\centering
	\begin{tabular}{lcc}
		\toprule
		\textbf{Method of obtaining parameters} & \textbf{Average distance between parameter tensors} &	\textbf{Average sparsity} \\
		\midrule
		He’s init. + SGD &	0.98 & 33\% \\
		MLP	& 0.01	& 16\%\Tstrut\\
		GHN-1 & 0.07 & 20\% \\
		GHN-2 &	0.17 & 39\% \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection{Sparsity}

We also analyze the sparsity of predicted parameters using the same 1,000 evaluation architectures. The sparsity can change drastically across the layers, so to fairly compare sparsities we consider the first layer only. We compute the sparsity of parameters $\mathbf{w}$ as the percentage of absolute values satisfying $|\mathbf{w}| < 0.05$. We report the average sparsity of all first-layer parameters in Table~\ref{tab:sensitivity_sparsity}.


\textbf{Results.} The first-layer parameters predicted by GHN-2 are similar to the parameters trained by SGD in terms of sparsity: average sparsity is 39\% compared to 33\% respectively (Table~\ref{tab:sensitivity_sparsity}). Higher sparsity of the parameters predicted by GHN-2 (39\%) compared to the ones of GHN-1 (20\%) may have been achieved due to the proposed parameter normalization method. The parameters predicted by GHN-2 are also more sparse than the ones obtained by SGD. Qualitatively, we found that GHN-2 predicts many values close to 0 in case of convolutional kernels 5$\PLH$5 and larger, which is probably due to the bias towards more frequent 3$\PLH$3 and 1$\PLH$1 shapes during training. Mitigating this bias may improve GHN’s performance.

\subsection{Training Speed of GHNs}
Training GHN-2 with meta-batching takes 0.9 GPU hours per epoch on CIFAR-10 and around 7.4 GPU hours per epoch on ImageNet (Table~\ref{tab:cost}). The training speed is mostly affected by the meta-batch size ($b_m$) and the sequential nature of the GatedGNN. Given that GHNs learn to model 1M architectures, the training is very efficient. The speed of training GHNs with  can be further improved by better parallelization of the meta-batch. Note that GHNs need to be trained only once for a given image dataset. Afterwards, trained GHNs can be used to predict parameters for many arbitrary architectures in less than a second per architecture (see Table~\ref{tab:bench_imagenet} in the main text).

\begin{table}[tbhp]
	\caption{Times of training GHNs on NVIDIA V100-32GB using our code.}
	\label{tab:cost}
	\vspace{3pt}
	\small
	\centering
	\begin{tabular}{lccccc}
		\toprule
		\textbf{\textsc{Model}} & \textbf{\# GPUs} & \multicolumn{2}{c}{\textbf{\textsc{Cifar-10}}} &	\multicolumn{2}{c}{\textbf{\textsc{ImageNet}}} \\
		& & \multicolumn{2}{c}{64 images/batch} &	\multicolumn{2}{c}{256 images/batch}\Bstrut\\
		\midrule
		& & sec/batch & hours/epoch & sec/batch & hours/epoch \\
		Training a single ResNet-50 with SGD & 1 & 0.10  & 0.02 & 0.77 & 1.03\Bstrut\\
		MLP with meta-batch size $b_m=1$ & 1 & 0.32 & 0.06 & 0.67 & 0.90 \\
		\ghnours with meta-batch size $b_m=1$ &	1 & 1.16 & 0.23 & 1.54 & 2.06 \\
		\ghnours with meta-batch size $b_m=8$ & 1 & 7.17 & 1.40 & \multicolumn{2}{c}{out of GPU memory} \\
		\ghnours with meta-batch size $b_m=8$ & 4 &	4.62 & 0.90 & 5.53 & 7.40 \\
		\bottomrule
	\end{tabular}
\end{table}



\section{Additional Related Work\label{apdx:related_work}}

Besides the works discussed in \S~\ref{sec:related}, our work is also loosely related to other parameter prediction methods~\cite{Denil2013-la,bertinetto2016learning,ratzlaff2019hypergan}, analysis of graph structure of neural networks~\cite{you2020graph}, knowledge distillation from multiple teachers~\cite{liu2019knowledge}, compression methods~\cite{cheng2017survey} and optimization-based initialization~\cite{dauphin2019metainit,zhu2021gradinit,das2021data}. \citet{Denil2013-la} train a model that can predict a fraction of network parameters given other parameters requiring to retrain the model for each new architecture.
\citet{bertinetto2016learning} train a model that predicts parameters given a new few-shot task similarly to~\cite{ravi2016optimization,requeima2019fast}, and the model is also tied to a particular architecture.
The HyperGAN~\cite{ratzlaff2019hypergan} allows to generate an ensemble of trained parameters in a computationally efficient way, but as the aforementioned works is constrained to a particular architecture.
Finally, MetaInit~\cite{dauphin2019metainit}, GradInit~\cite{zhu2021gradinit} and Sylvester-based initialization~\cite{das2021data} can initialize arbitrary networks by carefully optimizing their initial parameters, but due to the optimization loop they are generally more computationally expensive compared to predicting parameters using GHNs.
Overall, these prior works did not formulate the task nor proposed the methods of predicting performant \params for diverse and large-scale architectures as ours.\looseness-1

Finally, the construction of our \dataset is related to the works on network design spaces. Generating arbitrary architectures using a graph generative model, e.g.~\cite{yu2019dag,guo2020systematic,you2020graph}, can be one way to create the training dataset $\nets$. Instead, we leverage and extend an existing DARTS framework~\cite{liu2018darts} specializing on neural architectures to generate $\nets$. 
More recent works~\cite{radosavovic2020designing} or other domains~\cite{you2020design} can be considered in future work.

\section{Limitations\label{sec:limit}}

Our work makes a significant step towards reducing the computational burden of iterative optimization methods. However, it is limited in several aspects.

\textbf{Fixed dataset and objective.} Compared to GHNs, one of the crucial advantages of iterative optimizers such as SGD is that they can be easily used to train on new datasets and new loss functions. Future work can thus focus on fine-tuning GHNs on new datasets and objectives or conditioning GHNs on data and hyperparameters akin to SGD.

\textbf{Training speed.} Training \ghnours on larger datasets and with a larger meta-batch ($b_m$) becomes slower. For example, on ImageNet with $b_m=8$ it takes about 7.4 hours per epoch using 4$\PLH$NVIDIA V100-32GB using our PyTorch implementation. So, training of our GHN-2 on ImageNet for 150 epochs took about 50 days. The slow training speed is mainly due to limited parallelization of the meta-batch (i.e. using $b_m$ GPUs should be faster) and the sequential nature of the GatedGNN. \looseness-1

\textbf{Fine-tuning predicted \params.}
While in \S~\ref{sec:finetune} we showed significant gains of using GHN-2 for initialization on two low-data tasks, we did not find such an initialization beneficial in the case of more data. 
In particular, we compare training curves of ResNet-50 on CIFAR-10 when starting from the predicted parameters versus from the random-based He's initialization~\cite{he2015delving}.
For each initialization case, we tune hyperparameters such as an initial learning rate and weight decay.
Despite ResNet-50 being an OOD architecture, \ghnours-based initialization helps the network to learn faster in the first few epochs compared to He's initialization
(Fig.~\ref{fig:fine_tune}). 
However, after around 5 epochs, He's initialization starts to outperform \ghnours, diminishing its initial benefit. By fine-tuning the predicted parameters with Adam we could slightly improve \ghnours's results. However, the results are still worse than He's initialization in this experiment, which requires further investigation.
Despite this limitation, \ghnours significantly improves on \ghnbase in this experiment similarly to \S~\ref{sec:finetune}.\looseness-1 



\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\textwidth,trim={0.5cm 0 0 0},clip,align=c]{figs/resnet50_ft_c10_adam.pdf}
	\vspace{-5pt}
	\caption{Training ResNet-50 on CIFAR-10 from random and GHN-based initializations.
	}
	\label{fig:fine_tune}
	\vspace{-5pt}
\end{figure}

\section{Societal Impact\label{sec:society}}
One of the negative impacts of training networks using iterative optimization methods is the environmental footprint~\cite{strubell2019energy,cai2019onceforall}. Aiming at improving on the state-of-the-art, practitioners often spend tremendous computational resources, e.g.~for manual/automatic hyperparameter and architecture search requiring rerunning the optimization.
Our work can positively impact society by making a step toward predicting performant parameters in a resource-sustainable fashion. 
On the other hand, compared to random initialization strategies, in parameter prediction a single deterministic model could be used to initialize thousands or more downstream networks. All predicted parameters can 
inherit the same bias or expose any offensive or personally identifiable content present in a source dataset.\looseness-1

\newcommand{\discardpages}[1]{\xdef\discard@pages{#1}\AtBeginShipout{\renewcommand*{\do}[1]{\ifnum\value{page}=##1\relax \AtBeginShipoutDiscard \gdef\do####1{}\fi }\expandafter\docsvlist\expandafter{\discard@pages}}}
\discardpages{31}

%
