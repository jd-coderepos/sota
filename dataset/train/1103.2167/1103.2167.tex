\documentclass{article}
\usepackage{makeidx}
\usepackage[justification=centering]{caption}

\usepackage{graphicx,float}
\usepackage{graphics}
\usepackage{t1enc}
\usepackage{pdfsync}
\usepackage[latin1]{inputenc}
\renewcommand{\rmdefault}{ptm}
\usepackage[scaled=0.92]{helvet}
\ifdefined\mtpro\usepackage{mtpro2}\fi
\usepackage{mathrsfs,xspace}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{authblk} 










\floatstyle{ruled}
\newfloat{Algorithm}{htpb}{alg}

\newcommand{\exit}[1]{\operatorname{exit}(#1)}
\newcommand{\parex}[1]{\operatorname{parex}(#1)}
\newcommand{\lcp}{\operatorname{lcp}}
\newcommand{\Pref}[1]{\operatorname{Pref}(#1)}
\newcommand{\pred}{\operatorname{pred}}

\newcommand{\fbs}{\operatorname{FBS}}
\newcommand{\LDS}{\operatorname{LDS}}

\newcommand{\N}{\mathbf N}
\newcommand{\T}{\operatorname{T}}
\newcommand{\BT}{\operatorname{BT}}
\newcommand{\HT}{\operatorname{HT}}
\newcommand{\Q}{\mathscr Q}
\newcommand{\E}{\mathscr E}
\newcommand{\gleq}{\sqsubseteq}
\newcommand{\gsup}{\sqcup}
\newcommand{\ginf}{\sqcap}
\newcommand{\gbsup}{\bigsqcup}
\newcommand{\gbinf}{\bigsqcap}
\newcommand{\sing}[1]{\left\{\?#1\?\right\}}
\newcommand{\?}{\mskip1.5mu}

\newcommand{\todo}[1]{\textbf{#1}}

\newcommand{\lst}[2]{,~, ,~}
\newcommand{\lstone}[2]{,~, ,~}
\newcommand{\lstp}[2]{,~, ,~}

\newcounter{prgline}
\newcommand{\pl}{\\\theprgline\addtocounter{prgline}{1}\>}

\newenvironment{renumerate}{\renewcommand{\theenumi}{(\roman{enumi})}\begin{enumerate}}{\end{enumerate}}


\newcommand{\IF}{\text{\textbf{if}}\xspace}
\newcommand{\FI}{\text{\textbf{fi}}\xspace}
\newcommand{\BEGIN}{\text{\textbf{begin}}\xspace}
\newcommand{\FOR}{\text{\textbf{for}}\xspace}
\newcommand{\TO}{\text{\textbf{to}}\xspace}
\newcommand{\END}{\text{\textbf{end}}\xspace}
\newcommand{\DO}{\text{\textbf{do}}\xspace}
\newcommand{\OD}{\text{\textbf{od}}\xspace}
\newcommand{\TEST}{\text{\textbf{test}}\xspace}
\newcommand{\THEN}{\text{\textbf{then}}\xspace}
\newcommand{\ELSE}{\text{\textbf{else}}\xspace}
\newcommand{\TRUE}{\text{\textbf{true}}\xspace}
\newcommand{\FALSE}{\text{\textbf{false}}\xspace}
\newcommand{\VAR}{\text{\textbf{var}}\xspace}
\newcommand{\CONST}{\text{\textbf{const}}\xspace}
\newcommand{\RETURN}{\text{\textbf{return}}\xspace}
\newcommand{\ARRAY}{\text{\textbf{array}}\xspace}
\newcommand{\ZERO}{\text{\textbf{zero}}\xspace}
\newcommand{\INC}{\text{\textbf{inc}}\xspace}
\newcommand{\DEC}{\text{\textbf{dec}}\xspace}
\newcommand{\LOR}{\text{\textbf{or}}\xspace}
\newcommand{\LAND}{\text{\textbf{and}}\xspace}
\newcommand{\OF}{\text{\textbf{of}}\xspace}
\newcommand{\WHILE}{\text{\textbf{while}}\xspace}
\newcommand{\SET}{\text{\textbf{set}}\xspace}
\newcommand{\BOOLEAN}{\text{\textbf{boolean}}\xspace}
\newcommand{\PROC}{\text{\textbf{procedure}}\xspace}
\newcommand{\GOTO}{\text{\textbf{goto}}\xspace}
\newcommand{\STOP}{\text{\textbf{stop}}\xspace}
\newcommand{\SUCC}{\text{succ}}
\newcommand{\NULL}{\text{\textbf{null}}\xspace}
\newcommand{\FUNCTION}{\text{\textbf{function}}\xspace}
\newcommand{\PROCEDURE}{\text{\textbf{procedure}}\xspace}
\newcommand{\COMMENT}[1]{\ \ \sffamily\textbraceleft\,#1\,\textbraceright}

\providecommand{\comp}{\circ}

\newcommand{\lb}{LB}
\newcommand{\rb}{RB}

\newcommand{\rank}{\operatorname{rank}}
\newcommand{\lrange}{\operatorname{left}}
\newcommand{\rrange}{\operatorname{right}}
\newcommand{\extent}{\operatorname{extent}}
\newcommand{\cnt}{\operatorname{count}}
\newcommand{\select}{\operatorname{select}}
\newcommand{\logl}{\lceil\log\ell\rceil}

\newcommand{\lsem}{\mathopen{[\![}}
\newcommand{\rsem}{\mathclose{]\!]}}
\newcommand{\sat}[2]{\lsem #2 \rsem_{#1}}

\def\..{\,\mathpunct{\ldotp\ldotp}} 

\newcommand{\ebsuff}{\trianglelefteq}
\newcommand{\rmore}{\trianglerighteq}
\newcommand{\sbprol}{\preceq}
\newcommand{\sbprolneq}{\prec}
\newcommand{\kw}[1]{\textbf{#1}}

\newcommand{\I}{\mathscr I}
\newcommand{\C}{\mathscr C}
\newcommand{\A}{\mathscr A}
\newcommand{\op}{{\operatorname{op}}}
\newcommand{\url}{\cite{myurl}}


\renewcommand{\emptyset}{\varnothing}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}

\newtheorem{lemma}{Lemma} 
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}


\newcounter{noqed}
\newcommand{\qed}{ \ifmmode\mbox{
}\fi\rule[-.05em]{.3em}{.7em}\setcounter{noqed}{0}}
\newenvironment{proof}[1][{}]{\noindent{\bf Proof#1.
}\setcounter{noqed}{1}}{\ifnum\value{noqed}=1\qed\fi\par\medskip}

\begin{document} 
\sloppy
\title{Improved space-time tradeoffs for approximate full-text indexing with one edit error\thanks{Most of this work was done when the author was a student at LIAFA, University Paris Diderot - Paris 7. The work was partially supported by the French
 ANR project MAPPI (project number ANR-2010-COSI-004).}}
\author{Djamal Belazzougui}
\affil{Helsinki Institute for Information Technology (HIIT),
Department of Computer Science, University of Helsinki, Finland.}
\bibliographystyle{abbrv}
\maketitle
\begin{abstract}
In this paper we are interested in indexing texts for substring matching queries with one edit error. That is, given a text  of 
 characters over an alphabet of size , we are asked to build a data structure that answers the following query:
find all the  substrings of the text that are at edit distance at most  from a given string  of length . In this paper we show two new results for this problem. The first result, suitable for an unbounded alphabet, uses  (where  is any constant such that ) words of space and answers to queries in time . This improves simultaneously in space and time over the result of Cole et al. The second result, suitable only for a constant alphabet, relies on compressed text indices and comes in two variants: the first variant uses  bits of space and answers to queries in time , while the second variant uses  bits of space and answers to queries in time . This second result improves on the previously best results for constant alphabets achieved in Lam et al. and Chan et al. 
\end{abstract}
\section{Introduction}
The problem of approximate string matching over texts has been intensively studied. The problem consists in, given a pattern , a text  (the characters of  and  are drawn from the same alphabet of size ), and a parameter , to find the starting points of all the substrings of  that are at distance at most  from . There exist many different distances that can be used for this problem. In this paper, we are interested in the edit distance, in which the distance between two strings  and  is defined as the minimal number of edit operations needed to transform  into , where the considered edit operations are deletion of a character, substitution of a character by another and, finally, insertion of a character at some position in the string. Generally two variants of the problem are considered, depending on which of the pattern and the text is considered as fixed. In our case we are interested in the second variant, in which the text is fixed and can thus be processed in advance so as to efficiently answer to queries that consist of a pattern and a parameter . Further, we restrict our interest to the case . 
\subsection{Related Work}
The best results we have found in the literature with reference to our problem follow. We only consider the results with worst case space and time bounds. We thus do not consider results like the one in Maa{\ss} and Nowak~\cite{MN05} in which either the query time or the space usage only hold on average on the assumption that the text and/or the patterns are drawn from some random distribution. 
For a general integer alphabet (unbounded alphabet) a result by Amir et al.~\cite{AKLLLR00} further improved by Buchsbaum et al.~\cite{BGW00} has led to ~\footnote{In this paper  stands for .} bits of space with query time . Later Cole et al.~\cite{CGL04} described an index for an arbitrary number of errors , which for the case  uses  bits of space and answers to queries in  time. For the special case of constant-sized alphabets, a series of results culminated with those of Lam et al.~\cite{LSW08}, Chan et al.~\cite{chan2011linear} and Chan et al.~\cite{CLSTW10} with various tradeoffs between the occupied space and query time. 

Belazzougui~\cite{B09} presented a solution to an easier problem: build a data structure for dictionaries so as to support approximate queries with one edit error. In that problem the indexed elements are not text substrings but are, instead, strings coming from a dictionary. Then Belazzougui used his solution to the easier dictionary problem to solve the harder full-text indexing problem. His solution used a brute-force approach: build the proposed dictionary on all sufficiently short text substrings (more precisely, factors with length less than ) and use it to answer queries for sufficiently short query patterns. Queries for longer patterns are answered using the index of Cole et al.~\cite{CGL04}. The main drawback of that solution is that it incurs a large polylogarithmic factor in the space usage (due to the large space needed to index short text substrings). 

By adapting some ideas of Belazzougui~\cite{B09} and combining with two indices described in Chan et al.~\cite{chan2011linear} we are able to remove the additive polylogarithmic term from the query times associated with some of the best previously known results while using the same space (or even less in some cases). When compared with Belazzougui~\cite{B09} our query time is identical but our space usage is much better. 
The reader can refer to Tables~\ref{table:compar_table0} and~\ref{table:compar_table1} for a full comparison between our new results and the previous ones. 


\begin{table}
\centering
\begin{tabular}{|l|l|l|}
  \hline
  Data structure &  Space usage (in bits) & Query time \\
  \hline
  Lam et al.~\cite{LSW08}  &  & \\
  Lam et al.~\cite{LSW08}  &  & \\
  Lam et al.~\cite{LSW08}  &  & \\
  \hline
   Chan et al.~\cite{chan2011linear} &   &  \\
   Chan et al.~\cite{CLSTW10} &  &  \\
  \hline
  Belazzougui~\cite{B09} &  & \\
  \hline
   This paper &  &  \\
   This paper &  &  \\
  \hline
\end{tabular}
\caption{Comparison of existing solutions for constant alphabet sizes}
\label{table:compar_table0}
\end{table}
\begin{table}
\centering
\begin{tabular}{|l|l|l|}
  \hline
  Data structure &  Space usage (in bits) & Query time \\ 
  \hline
  Buchsbaum et al.~\cite{BGW00} &  & \\
  Cole et al.~\cite{CGL04} &   & \\
  Belazzougui~\cite{B09} &  & \\
  This paper &  & \\ 
  \hline
\end{tabular}
\caption{Comparison of existing solutions for arbitrary alphabets}
\label{table:compar_table1}
\end{table}
As can be seen from the tables, both our indices improve on the state of the art. We should mention that the second result attributed to Lam et al.~\cite{LSW08} in Table 1 is not stated in that paper, but can be easily deduced from the main result in Lam et al.~\cite{LSW08} by using a different time/space tradeoff for the compressed text index implementations. 
The results in Table 1 are all unsuitable for large alphabets as all their query times have a hidden linear dependence on  (which for simplicity is not shown in the table). This means that for very large alphabets of size   for example, the query time of those algorithms will be unreasonable. By contrast the query times of the algorithms in Table 2 do not have any dependence on the alphabet size. Our result in Table 2 always dominates Cole et al.'s result in both space and time. 




\section{Preliminaries and Outline of the Results}
At the core of our paper is a result for indexing all substrings of the text of length bounded by a given parameter . In particular, we prove the following two theorems: 
\begin{theorem}
\label{constant_fix_theorem}
For any text  of length  characters over an alphabet of constant size, given a parameter , we can build an index of size  bits  (where  is any constant such that ) so that for any given string  of length  we can report all of the  substrings of the text that are at edit distance at most  from  in time . Alternatively we can build a data structure that occupies  bits of space and answers to queries in time . 
\end{theorem}
Theorem ~\ref{constant_fix_theorem} is obtained by combining ideas from Belazzougui~\cite{B09} with a recent result of Belazzougui et al.~\cite{BBPV10} (weak prefix search) and with known results on compressed text indices. 
The following theorem is an extension of Theorem~\ref{constant_fix_theorem} to unbounded alphabets:  
\begin{theorem}
\label{arb_fix_theorem}
For any text  of length  characters over an alphabet of size , given a parameter , we can build an index of size  bits (where  is any constant such that ) such that for any given string  of length  we can report all of the  substrings of the text that are at edit distance at most  from  in time . 
\end{theorem}
Theorem ~\ref{arb_fix_theorem} is obtained by using one crucial idea that was used in Cole et al.'s result~\cite{CGL04} (heavy-light decomposition of the suffix tree) in combination with the weak prefix search result of Belazzougui et al.~\cite{BBPV10} and a recent result of Sadakane~\cite{Sa07} (1D-colored rangeed reporting). 

Theorem~\ref{arb_fix_theorem} can be used for any alphabet size while Theorem~\ref{constant_fix_theorem} holds only for a constant-sized alphabet. Both theorems can only be used for matching strings of bounded length but provide an improvement when used in combination with previous results that are efficient only for long strings.


Theorem~\ref{constant_fix_theorem} gives an immediate improvement for constant-sized alphabets when combined with a result appearing in Chan et al.~\cite{chan2011linear}:
\begin{theorem}
\label{full_constant_theorem}
For any text  of length  over an alphabet of size  we can build the following indices, which are both able to return for any query string  of length , the  occurrences of substrings of  that are at edit distance at most  from :
\begin{itemize}
\item An index that occupies  bits of space and answers to queries in time  where  is any constant such that . 
\item An index that occupies  bits of space and answers to queries in time . 
\end{itemize}
\end{theorem}

Theorem~\ref{arb_fix_theorem} can also be combined with another result that has appeared in Chan et al.~\cite{chan2011linear} to get the following result suitable for arbitrary alphabet sizes:
\begin{theorem}
\label{full_arb_theorem}
For any text  of length  over any integer alphabet of size  we can build an index that occupies  bits of space and is able to return for any query string  of length , all the  occurrences of substrings of  that are at edit distance at most  from  in time . 
\end{theorem}

Our new methods for proving Theorems~\ref{constant_fix_theorem} and ~\ref{arb_fix_theorem} make use of some ideas introduced in Belazzougui~\cite{B09} combined with tools which were recently proposed in Belazzougui et al.~\cite{BBPV10} and Sadakane~\cite{Sa07} and with compressed text indices proposed in~\cite{FM05,GV05}. In Belazzougui~\cite{B09} a new dictionary for approximate queries with one error was proposed. A naive application of that dictionary to the problem of full-text indexing was also proposed in that paper. However while this leads to the same  query time achieved in this paper, the space usage was too large,  namely  bits of space for alphabet of size . Nonetheless we will borrow some ideas from that paper and use them to prove our main results. 

The paper is organized as follows: we begin with the data structure suitable for constant-sized alphabets (Theorems~\ref{constant_fix_theorem} and~\ref{full_constant_theorem}) in Section~\ref{section:const_alpha} before showing the data structure for large alphabets (Theorems~\ref{arb_fix_theorem} and ~\ref{full_arb_theorem}) in Section~\ref{section:large_alpha}. We  conclude the paper in Section~\ref{section:conclusion}.

\subsection{Model and Notation}
In the remainder, we note by  the reverse of the string . That is  is the string   written in reverse order. For a given string , we note by  or by  the substring of  spanning the characters  through . We assume that the reader is familiar with the trie concept~\cite{Fr60,Kn73} and with classical text indexing data structures like suffix trees and suffix arrays (although we provide a brief recall in the next subsection). The model assumed in this paper is the word RAM model with word length  where  is the size of the considered problem. We further assume that standard arithmetic operations including multiplications can be computed in constant time. We assume that the text  to be indexed is of length  and its alphabet is of size . At the end of the paper, we show how to handle the extreme case . 
\subsection{Basic Definitions}
We now briefly recall some basic (and standard) text indexing data structures that will be extensively used in this paper. 
\subsubsection{Suffix array}
A suffix array~\cite{MM93} (denoted ) built on a text  of length  just stores the pointers to the suffixes of  in sorted order (where by order we mean the usual lexicographic order defined for strings). Clearly a suffix array occupies  bits. A suffix array example is illustrated in Figure~\ref{pic:suffix_array}
\begin{figure}[htb] 
\centering\includegraphics[scale=1]{suffix_array.pdf} 
\caption{Suffix array for the text } \label{pic:suffix_array} \end{figure}
\subsubsection{Suffix tree}
A suffix tree~\cite{Wr73,Mc76} is a \emph{compacted} trie built on the suffixes of a text  appended with , a special character outside of the original alphabet and smaller than all the characters of the original alphabet. 
A suffix tree has the following properties:
\begin{itemize}
\item Every suffix of  is associated with a leaf in the tree.
\item A factor  of  is associated with an internal node in the tree iff there exist two characters  and  such that  and  are also factors of . Each internal has thus at least two children, one associated with a factor that starts with  and another associated with a factor that starts with . 
\item The subtree rooted at any internal node associated with a prefix  contains (in its leaves) all the suffixes of  which have  as a prefix. 
\item Suppose that an edge connects an internal node  associated with a factor  to a node  (which could be a leaf or an internal node) associated with a string  (which could either be a suffix or another factor) where  is a character and  is string. Then the character  will be called the \emph{label} of  and  will be called the compacted path of . 
\end{itemize}
 The essential property of a suffix tree is that it can be implemented in such a way
that it occupies  pointers (that is  bits) in addition to the text and that given any factor  of  it is possible to find all the suffixes of  which are prefixed by  in  time. A suffix tree can also be augmented in several ways so as to support many other operations, but in this paper we will use very few of them. A suffix tree example is illustrated in Figure~\ref{pic:suf_tree}.

For a more detailed description of the suffix array or the suffix tree, the reader can refer to any book on text indexing algorithms~\cite{Gu97,CR03}.
\begin{figure}[htb] 
\centering\includegraphics[scale=1]{suf_tree.pdf} 
\caption{Suffix tree for the text } \label{pic:suf_tree} \end{figure}


\section{Solution for Constant-Sized Alphabets}
\label{section:const_alpha}
In this section we give a proof of Theorems~\ref{constant_fix_theorem} and~\ref{full_constant_theorem}. 
Theorem~\ref{full_constant_theorem} is proved in Section~\ref{section:arb_const_query}. This theorem uses the data structure of Theorem~\ref{constant_fix_theorem}, which is described in Section~\ref{section:const_fix_data_struct}. Then  we describe how the queries are executed on the data structure in Section~\ref{section:const_fix_query}. Finally, in Section~\ref{subsec:dupl_occ} we show how we deal with duplicate occurrences. 
\subsection{Solution for Arbitrary Pattern Length}
\label{section:arb_const_query}
We use the following lemma proved by Chan et al.~\cite[Section 3.2]{chan2011linear}.
\begin{lemma}~\cite[Section 3.2]{chan2011linear}
\label{CLSTW06b0_fix_lemma}
For any text  of length  characters over an alphabet of constant-size we can build an index of size  bits so that we can report all of the  substrings of the text which are at edit distance  from any pattern  of length  in time . 
\end{lemma}

The solution for Theorem~\ref{full_constant_theorem} is easily obtained by combining Theorem~\ref{constant_fix_theorem} with Lemma~\ref{CLSTW06b0_fix_lemma} in the following way: we first build the index of Chan et al.~\cite{chan2011linear} whose query time is upper bounded by  whenever  and whose space usage is  bits. Then we build the data structure of Theorem~\ref{constant_fix_theorem} in which we set  and  where  is any constant that satisfies . In the case where we have a string of length less than , we use the index of Theorem~\ref{constant_fix_theorem} to answer the query in time  when using the first variant, or in time  when using the second variant. In the case where we have a string of length at least , we use the index of Chan et al.~\cite{chan2011linear} answering to queries in time . The space is thus dominated by our index, which uses either  or  bits of space. 


\subsection{Data Structure for Short Patterns}
\label{section:const_fix_data_struct}
Our data structure for short patterns relies on a central idea used in Belazzougui~\cite{B09}. That paper was concerned with building an approximate dictionary that had to support searching queries that tolerate one edit error. The idea for obtaining the result was that of using a hash-based dictionary combined with a trie and a reverse trie, so that finding all the strings in the dictionary which are at distance  from a pattern  of length  can be done in constant time (amortized) per pattern character and constant time per reported occurrence. This constant time derives from two facts:
\begin{enumerate}
\item If the dictionary uses some suitable perfect hash function , then after we have done a preprocessing step on  in  time, the computation of  takes constant time for any string  at distance  from . 
\item Using the trie and reverse trie, the matching of any candidate string  at distance  from  can be verified in constant time (this idea has frequently been used before, for example by Brodal and G{\c{a}}sieniec~\cite{BG96}). 
\end{enumerate}
In our case we will use different techniques from the ones used in Belazzougui~\cite{B09}. As we are searching in a text rather than a dictionary, we will be looking for suffixes prefixed by some string  instead of finding exact matching entries in a dictionary. For that purpose we will replace the hash-based dictionary with a weak prefix search data structure~\cite{BBPV10},  which will allow us to find a range of candidate suffixes prefixed by any given string  at edit distance  from . For checking the candidate suffixes, we will replace the trie and reverse trie with compressed suffix arrays built on the text and the reverse of the text. Both the weak prefix search and the compressed suffix arrays have the advantage of achieving interesting tradeoffs between space and query time. 

We now describe in greater detail the data structure we use to match patterns of bounded length over small alphabets (Theorem~\ref{constant_fix_theorem}). This data structure uses the following components:

\begin{enumerate}
\item A suffix array  built on the text . 
\item A suffix tree  built on the text . In each node of the suffix tree representing a factor  of , we store the range of suffixes which start with . That is we store a range  such that any suffix starts with  iff its rank  in lexicographic order is included in . 
\item A reverse suffix tree  built on the text , the reverse of the text  (we could call  a prefix tree as it actually stores prefixes of ). In each node of  representing a factor  we store the range of suffixes of  which start with . That is we store a range  such that any suffix of  starts with  iff its rank  in lexicographic order among all the suffixes of  is included in . 
\item A table . This table stores for each suffix  for all , the rank of the suffix  in lexicographic order among all the suffixes of . 
\item A table . This table stores for each prefix  the rank of the reverse of prefix  in lexicographic order among the reverses of all prefixes of .  
\item A polynomial hash function ~\cite{KR87} parameterized with a prime  and an integer  (a seed). For a string  we have . The details of the construction are described below. The hash function essentially uses just  bits of space to store the numbers  and . 
\item A weak prefix search data structure (which we denote by ) built on the set , the set of substrings (factors) of  of fixed length  characters (to which we add  artificial factors obtained by appending  to every suffix of  of length  ). Note that . This data structure, which is described in Belazzougui et al.~\cite{BBPV10}, comes in two variants. The first one uses  bits of space for any constant  and answers to queries in  time.  The second one uses space  and has query time . We note the query time of the weak prefix search data structure by . The details are described below. 
\item Finally a prefix-sum data structure  built on top of an array  which stores for every  sorted in lexicographic order, the number of suffixes of  prefixed by  (for each of the artificial strings this number is set to one). This prefix-sum data structure uses  bits of space and answers in constant time to the following queries: given an index  return the sum . The details are given below. 
\end{enumerate}
Note that the total space usage is dominated by the text indexing data structures (the suffix and prefix trees and the structures ,,,) which occupy  bits of space. These data structures are illustrated in Figures~\ref{pic:text_idx_DS} and ~\ref{pic:text_idx_DS2}. 

\begin{figure}[htb] 
\centering\includegraphics[scale=0.8]{SA_SAinv.pdf} 
\includegraphics[scale=0.8]{PA_PAinv.pdf} 

\caption[justification=centering]{Data structures ,, and  for the string } 

\label{pic:text_idx_DS} \end{figure}

\begin{figure}[htb] 
\centering\includegraphics[scale=1]{suf_pref_tree.pdf} 
\caption{The suffix tree (on the left) and the prefix tree (on the right) for the string } \label{pic:text_idx_DS2} \end{figure}

We now describe in detail the results from the literature that will be used to implement the data structures above. 

\subsubsection{Text indexing data structures}
The only operation we need to do on the prefix tree is, for a given pattern , to determine for each prefix  of  the range of all prefixes of  which have  as a suffix. Similarly for the suffix tree we only need to know for each suffix  of  the range of suffixes which are prefixed by . 
The classical representations for our text indexing data structures (,,, suffix and prefix trees) all occupy  bits of space. However in our case, we need to use less than the  bits needed by the classical representations. We will thus make use of compressed representations of the text indexing data structures~\cite{FM05,GV05}. In particular we use the following results: 
\begin{enumerate}
\item For every prefix  of  of length  determine the range  of prefixes of  which are suffixed by . This can be  accomplished incrementally in  time by following the suffix links~\footnote{A suffix link connects a suffix tree node associated with a factor  (with  being a character) to the suffix tree node associated with the factor .} in the prefix tree . That is, deducing the range corresponding to the prefix of  of length  from the range of the prefix of  of length  in  time (following a suffix link at each step takes  time). In the context of compressed data structures, this can be accomplished using the backward search on the compressed representation of the prefix array ~\cite{FM05} still in time  and representing  in  bits only (assuming a constant alphabet)~\footnote{ can be represented in compressed suffix array representation of~\cite{FM05}, since  is actually the suffix array of the reverse of the text.}. In this case the range corresponding to the prefix of  of length  (that is, ) is deduced from the range corresponding to the prefix of  of length  (that is, ). 

\item For every suffix  of  of length  determine the range  of suffixes of  which are prefixed by . This can be done in a similar way in total  time by either following suffix links in a standard representation of the suffix tree  or by backward search~\cite{FM05} in a compressed representation of the suffix array . The compressed representation occupies  bits only.  
\item For any  we need to have a fast access to ,,,. In case those four tables are represented  explicitly in  bits of space, the access time is trivially . However in the context of compressed representation, we need to use less than  bits of space and still be able to have fast access to the arrays. 
\end{enumerate}
The first two results can be summarized with the following lemma:
\begin{lemma}~\cite{FM05}
Given a text  of length  over a constant-sized alphabet, we can build a data structure with  bits of space such that given a pattern  of length , we can in  time determine:
\begin{itemize}
\item the range of suffixes of  (sorted in lexicographic order) prefixed by  for all suffixes  of  of length .
\item the range of prefixes of  (sorted in reverse lexicographic order) suffixed by  for all prefixes  of  of length .
\end{itemize}
If the alphabet is non constant, then we can obtain the same results using  bits of space~\cite{BN11}. 
\end{lemma}
The third needed text indexing result is summarized with the following lemma:
\begin{lemma}~\cite{GV05,Rao02,LSW05}
\label{lemma:compr_suf_array}
Assuming a constant alphabet size, we can compress the arrays ,, and  with the following tradeoffs : 
\begin{itemize}
\item space  bits with access time  time. 
\item space  bits with access time  time, where  is any constant
such that . 
\end{itemize}
\end{lemma}



\subsubsection{Weak prefix search data structure}
A weak prefix search data structure built on a set of strings  (sorted by increasing lexicographic order) permits, given a prefix  of any element in , to return the range of elements of  prefixed by . If given an element which is not prefix of any element in , it returns an arbitrary range. We will use the following result:
\begin{lemma}~\cite{BBPV10}
\label{lemma:weak_pref_search}
Given a set of  strings of fixed length  each over the alphabet , we can build a weak prefix search data structure with the following time/space tradeoffs:
\begin{itemize}
\item Query time  with a data structure which uses  bits of space\footnote{Actually the result in Belazzougui et al.~\cite{BBPV10} states a space usage  but assumes a constant alphabet size. However, it is easy to see that the same data structure just works for arbitrary  in which case it uses  bits of space.} for any integer constant . 
\item Query time  with a data structure which uses  bits of space. 
\end{itemize}
\end{lemma}
The weak prefix search data structure of the lemma above needs to use a perfect hash function  and assumes that after preprocessing a query string , the computation of  for any  takes constant time. This is essential to achieve  query time, since a query involves either  or  computations of the hash function on prefixes of . 


In our case, the weak prefix search data structure will be built on , the set of factors of  of fixed length , where  is a special character lexicographically smaller than all the characters which appear in . The only reason we use  instead of  in the weak prefix search is to ensure that the last suffixes of length less than  are all present in the weak prefix (a suffix  of length  will be stored in the weak prefix search as the string ).


\subsubsection{Hash function}
As described above we will use a polynomial hash function ~\cite{KR87} parameterized with a prime number  and a seed . The parameter  is fixed but the seed  is chosen randomly. Our goal is to build a hash function  such that all the hash values of the substrings of  used by the weak prefix search are all distinct (that is,  is a perfect hash function for the considered set if substrings). For a randomly chosen  this is the case with high probability. If it is not the case, we randomly choose a new  and repeat the construction until all the needed substrings of  are mapped to distinct hash values. 
We notice that the hash function  can be evaluated on any string  in deterministic linear time in the length of . However the time to find a suitable  is randomized only, as we may in the worst case do many trials before finding a suitable  (although the number of trials is on average only  as each trial succeeds with high probability). 

\subsubsection{Prefix-sum data structure}
A succinct prefix-sum data structure is a data structure that permits the succinct encoding of an array  of integers of total sum  in space  bits, so that the sum  for any  can be computed in constant time. This can be obtained by combining fast indexed bitvector implementations ~\cite{J89,Mu96,CM96} with Elias-Fano coding~\cite{EliESRCASF,FanNBRISM}. 
\subsection{Queries}
\label{section:const_fix_query}
\subsubsection{Preprocessing}
To make a query on our full-text index for a string  of length , we will proceed in a preprocessing step which takes  time. The preprocessing consists in the following phases: 
\begin{enumerate}
\item We fill two arrays  and  by using Lemma~\ref{lemma:compr_suf_array} on the string . More precisely  stores the range of prefixes suffixed by  and  stores the range of suffixes prefixed by  (we naturally associate the range  with the empty strings  and  which, are respectively suffix and prefix of any other string). This step takes time  as stated in Lemma~\ref{lemma:compr_suf_array}. 
\item We precompute an array which stores all the values of  for all .
\item We precompute all the values  for all . That is all the hash values for all the prefixes of . This can easily be done incrementally as we have  and then  for all . 
\item We precompute all the values  for all . That is all the hash values for all the suffixes of . This can also easily be done incrementally as we have  and then  for all . 
\end{enumerate}

\subsubsection{Hash function computation}
We now describe some useful properties of the hash function  which will be of interest for queries. 
An interesting property of the hash function  is that after the precomputation phase, computing  for any  at edit distance  from  takes constant time:
\begin{enumerate}
\item Deletion at position : computing the hash value of  (note that  is defined as the empty string when ) is done by the formula  (note that  if ). 
\item Substitution at position : computing the hash value of  is done by the formula . 
\item Insertion after position : computing the hash value of ~\footnote{Note that insertion before position  is equivalent to insertion after position  in which case  will be the empty string.} is done by the formula .
\end{enumerate}
It can easily be seen that the computation of  takes constant time in each case. The reason is that the three values involved in each computation have all been obtained in the precomputation phase. 

Moreover, computing  for any prefix  of a string  at edit distance  from  also takes constant time:
\begin{itemize}
\item The hash value for a prefix  of length  of a string  obtained by deletion at position  in  can be obtained by  (note that  if ) whenever  or  otherwise. 
\item The hash value for a prefix  of length  of a string  obtained by substitution at position  in  can be obtained by  (note that  if ) whenever  or  otherwise.
\item The hash value for a prefix  of length  of a string  obtained by insertion after position  in  can be obtained by  (note that  if ) whenever  or  otherwise. 
\end{itemize}
\subsubsection{Checking occurrences}
Suppose we have found a potential occurrence of a matching substring of the text obtained by one deletion, one insertion or one substitution. There exists a standard way to check for the validity of the matching (this has been used several times before, for example in Chan et al.~\cite{CLSTW10}) using the arrays  and . Suppose we have found a potential occurrence of a string  obtainable by deletion of the character at position  in the query string . In this case  we have . Moreover, suppose we have found for  a potentially matching location  in the text. Then checking whether this matching location is correct is a matter of just checking that  and, . That is, checking whether  is just a matter of checking that , and  which amounts to checking that the prefix of  ending at  is suffixed by , and the suffix of  starting at  is prefixed by . Those two conditions are equivalent to checking that , and  respectively. Checking a matching location for an insertion or a substitution can be done similarly. For checking a matching at position  in the text of a pattern obtainable by insertion of a character  after position , it suffices to check that ,  and finally . Similarly checking for a matching of a string obtainable by a substitution at position  can be done by checking that ,  and finally . 
We thus have the following lemma which will be used as a central component for query implementation:
\begin{lemma}
\label{check_occ_lemma}
Given any pattern  for which the arrays  and  have been precomputed, we can, for any string  at distance  from  (where  is described using  words that store the edit operation, the edit position and a character in case the operation is an insertion or a substitution) and a location , check whether  occurs at location  in the text by doing a constant number of probes to the text and the arrays ,   and thus in total time . 
\end{lemma}

\subsubsection{Query algorithm}
\label{section:const_query_algo1}
We now describe how queries for a given string  of length  are implemented. Recall that we are dealing with an alphabet of constant size . This means that the number of strings at distance  from a given  is , because an edit operation is specified by a position and a character (except for deletions which are specified only by a position) and thus, we can have at most  different combinations. 

Our algorithm will simply check exhaustively for matching in the text of every string  which can be obtained by one insertion, one deletion or one substitution in the string . 
Each time we check for a string  we also report the location of all occurrences in which it matches. The matching for a given string  proceeds in the following way:
\begin{itemize}
\item Do a weak prefix search on  for the string  which takes either constant time or  time depending on the implementation used (see Lemma~\ref{lemma:weak_pref_search})~\footnote{Note that the query time bound uses the essential fact that the function  can be computed on any prefix  of  in constant time.}. The result of this weak prefix search is a range  of elements in  which are potentially prefixed by . 

\item Using the prefix-sum data structure , compute the range  of suffixes of  potentially prefixed by . This range is given by  and  and its computation takes constant time. 
\item Do a lookup for  in the suffix array. This takes either time  or  depending on the suffix array implementation (see Lemma~\ref{lemma:compr_suf_array}). 
\item Finally check that there is a match in the location  in the text (this is done differently depending on whether we are dealing with an insertion, a substitution or a deletion). This checking, which is done with the help of Lemma~\ref{check_occ_lemma}, needs to do one access to  and one access to  and thus takes either  or  time depending on the implementation. If the match is correct we report the position  and additionally report all the remaining matching locations which are at positions  (by querying the compressed suffix array using Lemma~\ref{lemma:compr_suf_array}). Otherwise we return an empty set. 
\end{itemize}
For proving the correctness of the query, we first prove the following lemma:
\begin{lemma}
Given a string  of length at most  such that  is a prefix of at least one suffix of , with the help of  and , we can find the interval of suffixes prefixed by  in time  . Further, given any  we can check whether it prefixes some suffix of  in time  and if not return an empty set. 
\end{lemma}
\begin{proof}
We start with the first assertion. If  is prefix of some suffix  then it will also be prefix of some element in . This is trivially the case if  and this is also the case if  as we are storing in  the string  which is necessarily prefixed by  as well. Now we prove that the returned interval  is the right interval of suffixes prefixed by . First notice that the weak prefix search by definition returns the interval  of elements of  which are prefixed by . Now, we can easily prove that  is exactly the number of suffixes that are lexicographically smaller than . This is the case as we know that the sum  includes exactly the following:
\begin{enumerate}
\item All suffixes of length less than  which are lexicographically smaller than  and for which an artificial element was inserted in . 
\item All the suffixes of length at least  whose prefixes of length  are lexicographically smaller than . 
\end{enumerate}
On the other hand we can prove that  gives exactly the number of suffixes prefixed by . That is  which gives the number of suffixes of length at least  prefixed by elements of  prefixed by  in addition to the suffixes of length less than  prefixed by  and for which a corresponding artificial element has been stored in . Now that the first assertion of the lemma has been proved we turn our attention to the second assertion. This is immediate: given a string  which does not prefix any suffix, we know by Lemma~\ref{check_occ_lemma} that the checking will fail for any suffix of  and thus fail for the suffix at position  in the last step which thus returns an empty set. 
\qed
\end{proof}

The following lemma summarizes the query for a prefix  at distance one from : 
\begin{lemma}
\label{modif_pattern_query}
Given any pattern  for which the arrays  and  have been computed, we can for any string  at edit distance  from  (where  is described using  words that store the edit operation, the edit position and a character in case the operation is an insertion or a substitution) search for all the  suffixes prefixed by  in time . 
\end{lemma}
\begin{proof}
We first prove the correctness of the operations as described above and then prove the time bound. For that we examine the two possibilities: 
\begin{itemize}
\item  is not prefix of any suffix in , in which case the query should return an empty set. It is easy to prove the equivalent implication: if the data structure returns a non empty set, then there exists at least some suffix of  prefixed by . For the data structure to return a non empty set, the checking using Lemma~\ref{check_occ_lemma} must return true for the location  in the text and for this checking to return true,  must be prefix of the suffix starting at position  in the text. 
\item  is prefix of some suffix of , in which case the query must return all those suffixes. Note that by definition the weak prefix search  will return the right range of elements of  which are prefixed by . 
The justification for this is that a single match implies that there exists at least one suffix prefixed by , which implies the weak prefix search  returns the correct range of factors in  and thus the bit-vector  returns the correct range of suffixes , which must also have  as a prefix. 
\end{itemize}
We now prove the time bound. In the case that  is not prefix of any suffix in , the query time is clearly  as we are doing one query on  (which takes  time), one query of the prefix-sum data structure  in constant time and finally the checking using Lemma~\ref{check_occ_lemma} which takes  time. In case  is prefix of some suffixes, then the first step for checking that the set is non empty also takes constant time and reporting each occurrence takes additional  time per occurrence. 
\qed
\end{proof}


\subsection{Duplicated occurrences}
\label{subsec:dupl_occ}
A final, important detail is to avoid reporting the same occurrence more than once. This can happen in a few cases if insufficient care is taken in the query algorithm of Section~\ref{section:const_query_algo1}. As an example, take the case of the string ; deleting one character at any of the  first positions of  would result in the same string . The same problem occurs if we delete one of the last  characters of , resulting in the same string . 
In order to avoid needing to check the same string more than once we can use the following simple rules: 
\begin{enumerate}
\item For deletions, we avoid testing for the deletion of a character  if  as this has the same effect as deleting character . 
\item For substitutions we avoid testing for the replacement of the character  by the same , as this would result in the same string . 
\item For insertions, we avoid inserting the character  between positions  and  as it would have the same effect as inserting character  between positions  and . 
\end{enumerate}
It is easy to check that the three rules above will avoid generating the same candidate strings for each of the three edit operations~\footnote{This is evident for substitutions. It is also true for deletion, since only the last character in a run of equal characters is deleted. For insertions, the only problematic case is when inserting the same character in a run of equal characters of length at least , and this case is  avoided since we only insert such a character at the end of the run.}. Note however that a string obtained by two different operations could still generate the same locations in the text. As an example the string  can generate the string  by a delete operation and the string  by a substitution. Note that all occurrences of  are also occurrences of . 
There exist a few different ways to fix this problem. The first is to just not do anything if the user can tolerate that an occurrence is reported a constant number of times. Indeed each occurrence can only be reported at most three times (once for each edit operation) as all occurrences generated by each edit operation must be distinct. 

If we require that each occurrence is only reported once, then we can just use a bitvector of  bits to track the already reported occurrences. Before starting to answer to queries, we initially set all the positions in the bitvector to . Then when executing a query we only report an occurrence if the corresponding position in the bitvector is unmarked and then mark the position. Finally after the query has finished, we reset all the positions that were marked during the query so that the bits in the bitvector are again all zero before the next query is executed. 



\section{Solution for Large Alphabets}
\label{section:large_alpha}
The time bound of Theorem~\ref{constant_fix_theorem} has a linear dependence on the alphabet size as the query time is actually . This query time is not reasonable in the case where  is non constant. In this section we show a solution which has no dependence on alphabet size. In order to achieve this solution we combine the result of Chan et al.~\cite{chan2011linear} with an improved version of Chan et al.~\cite{CLSTW10} that works only for query strings of length bounded by a parameter . This is shown in Section~\ref{section:full_arb_solution}. We then show how we improve the  solution in Chan et al.~\cite{CLSTW10} in Sections~\ref{section:arb_high_level_desc},~\ref{section:DS_impl} and~\ref{section:gen_weight_decomp}. 
This improvement is an extension of the data structures described in the previous section, but with two major differences: it does not use compressed variants of the text indexing data structures and its query time is independent of the alphabet size. Before describing the details of the used data structures, we recall in Section~\ref{section:arb_tools} a few definitions and data structures which will be used in our construction. 

\subsection{Solution for Arbitrary Pattern Length}
\label{section:full_arb_solution}
In order to prove Theorem~\ref{full_arb_theorem} we will make use of the following lemma from Chan et al.~\cite{chan2011linear}: 
\begin{lemma}~\cite[Section 2.3,Theorem 6]{chan2011linear}
\label{CLSTW06b1_fix_theorem}
For any text  of length  characters over an alphabet of size  we can build an index of size  bits so that given any pattern  of length  we can report all of the  substrings of the text which are at edit distance  from  in time . 
\end{lemma}
In order to get Theorem~\ref{full_arb_theorem}, we combine this lemma with Theorem~\ref{arb_fix_theorem}. 
The combination is also straightforward. That is, we build both indices, where the index of Theorem~\ref{arb_fix_theorem} is built using the parameter . Then if given a pattern of short length , we use the index of Theorem~\ref{arb_fix_theorem} to answer in time , otherwise given a pattern of length  we use the index of Lemma~\ref{CLSTW06b1_fix_theorem} to answer in time . The space is dominated by our index, which uses  (by adjusting ).
\subsection{Tools}
\label{section:arb_tools}
In order to prove Theorem~\ref{arb_fix_theorem}, we will make use of the following additional tools:
\subsubsection{Heavy-light tree decomposition}
The heavy-light decomposition of a tree~\cite{HT84} decomposes the edges that connect a node to its children into two categories: heavy and light edges. Suppose that a given node  in the tree has  leaves in its subtree. We call  the weight of . We let  be the heaviest child of  ( is the child of  with the greatest number of leaves in its subtree). If there is more than one child sharing the heaviest weight, then we choose  arbitrarily among them. Then  is considered  a \emph{heavy} node and all the other children of  are considered light nodes. The edge connecting  to  is considered a \emph{heavy} edge and all the other edges that connect  to its children are considered light edges. A heavy-path is a maximal sequence of consecutive heavy edges. We will make use of the essential property summarized by  the following lemma:
\begin{lemma}
Any root to leaf path in a tree with  nodes contains at most  light edges.
\end{lemma}
\begin{comment}
\begin{proof}
Suppose that the path traverses  light edges. First note that if a light edge connects a node  of weight  to its child node  of weight , then necessarily . This can be proved by contradiction. Suppose that . Then the heaviest child of  has weight at least  which means that . So each time we traverse a light edge we divide the size of the subtree (in terms of the number of leaves) by a factor of at least . In order to reach a node of weight  (a leaf) starting from a node of weight  we can traverse at most  light edges as, at each traversal of a light edge, we get to a subtree whose number of leaves is at most half the number of nodes in the parent node subtree. 
\qed
\end{proof}
\end{comment}
Thus for a given pattern of length  the traversal of a suffix tree decomposed according to a heavy path decomposition will contain at most  light edges. An example of a heavy-light decomposition of a suffix tree is illustrated in Figure~\ref{pic:heavy_path_suf_tree} in which the light edges are represented by dashed lines and the node weights are stored inside the nodes. 
\begin{figure}[htb] 
\centering\includegraphics[scale=1]{suf_tree_heavypath.pdf} 
\caption{Heavy-light decomposition of a suffix tree for the string } \label{pic:heavy_path_suf_tree} \end{figure}

\subsubsection{1D colored range reporting data structure}
A 1D colored range reporting data structure solves the following problem: given an array  of colors each chosen from the same alphabet of size , answer to the following query: given an interval  return all the  distinct colors which occur in the array elements . A solution devised by Muthukrishnan~\cite{Mu02} uses  bits of space and allows queries to be answered in optimal  time. Later Sadakane~\cite{Sa07} improved the solution so as to use only  bits~\footnote{The term  comes from the use of a succinct index for range minimum queries (RMQ). The term can be reduced to  if the recent optimal solution of Fischer and Heun~\cite{Fi10} is used. The term  comes from the use of a bitvector of  bits that needs to be writable. The bit-vector is used to avoid reporting a single color more than once.} on top of the array , reducing the space usage to  bits. \begin{lemma}~\cite{Sa07}
~\label{lemma:1D_color_rep}
Given an array  of colors from the set , we can build a data structure of size  so that given any range  we can return all the  distinct colors which appear in  in time . Moreover the colors can be returned one by one in  time per color.
\end{lemma}

\subsection{High Level Description}
\label{section:arb_high_level_desc}

The solution of Theorem~\ref{constant_fix_theorem} has a too strong alphabet dependence in its query time. In particular, testing all insertion and substitution candidate strings takes  time as we have  candidates and spend  time for testing each candidate. By contrast, deletion candidates are at most  and thus can be tested in  time. 
In order to reduce the number of candidates for insertions and substitutions we could use substitution lists as described in the solution of Belazzougui~\cite{B09}, which deals with approximate queries with one error over a dictionary of strings. 
A substitution list associates a list of characters   to each pair of strings  such that the string  is in the dictionary. When querying for substitutions on a pattern  of length , we can get the characters to substitute at position  by querying the substitution list for the pair . This will in effect return every character  such that the string  is in the dictionary. 

Concerning the space usage, the total space used by all the substitution lists is , where  is the sum of lengths of the strings in the dictionary. This is because every string of length  in the dictionary  contributes exactly  entries to the substitution lists and the substitution lists are implemented in a succinct way that allows them to use  bits per entry. 

We can now try to apply the idea of substitution lists to our problem. In our case we are indexing suffixes of a text of total length . A naive implementation of the substitution lists would use in total  bits of space. 
However it turns out that we only need to index strings of length at most  since we can use Lemma~\ref{CLSTW06b1_fix_theorem} to answer queries of length at least  in time . 


Now we could naively implement the idea by indexing all the factors of the text of length at most . Because we have  such factors with each of them being of length , this would result in  bits of space in total.

In order to further reduce the space, we will only store substitution lists for factors of length exactly  rather than length at most . This will reduce the space to  bits. We will make use of a weak prefix search data structure that will allow us to get the contiguous range of substitution lists that correspond to a given prefix. The substitution list corresponding to that prefix will be obtained by merging all the substitution lists that are in the given range. 
However there could be duplication in the characters stored in each substitution list (the same character could be stored in multiple lists). In order to ensure that each character is only reported once we use Lemma~\ref{lemma:1D_color_rep}. 

We can further reduce the space of the substitution lists from  to  by making use of an observation that was made in Cole et al.~\cite{CGL04} and that was further used in Chan et al.~\cite{CLSTW10}. 

The main idea of Cole et al.~\cite{CGL04} was to build correction trees (which are of three types, deletion, substitution and insertion). The trees will store  elements in total. The  factor comes from the fact that each suffix may be stored up to  times in the correction trees of a given type. More precisely a suffix is duplicated  times if the path from the root to that suffix has  light edges. We will not make use of correction trees but will make essential use of the following related crucial observation: for each indexed factor we will need only to store  elements in the substitution list. More precisely we will need only to store the characters that are labels of light edges. The reason is that a label of a heavy edge is already known when traversing the suffix tree and thus, need not be stored in a substitution list. Any suffix tree node has always one outgoing heavy edge and we can just consider its labeling character as a potential candidate for substitution or insertion. 

Finally we reduce the space further from  to  by using a more fine-grained encoding of the substitution lists. For that we will use a more precise categorization of the light edges according to their weight, and store for each node a list of all edges in each category. Then encoding a substitution list character can be done by using an index into the category of the edge labeled by that character. The crucial observation that allows for the use of less space lies in the fact that there cannot be too many heavier edges and, thus, an index into a category that contains heavier edges will use less space. 

\subsection{Data Structures for short patterns}
\label{section:DS_impl}

We will now present our solution that will be used to answer queries for patterns whose length is bounded by a certain parameter . The implementation of our solution will consist of two parts. The first part reuses the same data structures used for Theorem~\ref{constant_fix_theorem} (see Section~\ref{section:const_fix_data_struct}), but with slight modifications. Our second part is an efficient implementation of substitution lists. We now describe our first part. We make only two modifications to the data structures of Section~\ref{section:const_fix_data_struct}: 
\begin{enumerate}
\item We use non compressed variants of the text indexing data structures. 
\item We mark the heavy edge of every suffix tree node. 
\end{enumerate}

The second part of our solution consists in a set of  substitution list stores, where the substitution store number  will contain all substitution lists for position . 
Our substitution lists are built on the factors of length  of the text (from now on we call them b-factors of the text). We thus have a dictionary of  strings of length  each. 
We start by building a compacted trie on those b-factors. This compacted trie is built by cutting the suffix tree below level  (removing all nodes that have depth above  and transforming internal nodes that span depth  into leaves). 

We now give more detail on how the substitution stores are implemented. Recall that a substitution list as described in~\cite{B09} associates a list of characters   to each pair of strings  such that each string  is in the dictionary. In our case, the substitution lists are built by doing a top-down traversal of the compacted trie. Then each time we encounter a node associated with a factor  and having a light edge labeled with character , we do the following: we traverse all the leaves below that light edge (leaves of the subtree rooted at the target of that edge) and for each leaf associated with the string , we append  at the end of the substitution list corresponding to the pair . 
We thus avoid storing a character associated with a b-factor when it is :
\begin{enumerate}
\item Part of a compacted path of an edge. 
\item A label of a heavy edge. 
\end{enumerate}
The order of the characters in a substitution list is arbitrary. We will have  substitution stores, one per position . 
A substitution store for position  will contain all substitution lists associated with pairs  such that . We first sort all those substitution lists by  (the concatenation of the two strings of their associated pairs). 
We let those sorted substitution lists be . We store them in a compact, consecutive and packed way into an array . Note that this array uses space at most  bits. This is because we have at most  b-factors and we are storing at most one character from each b-factor (the character at position ). However when summed up over all the arrays, the total space (summing up  will be  and not .
This is because each b-factor, i.e., root-to-leaf path in the trie, can contribute at most  light edges. 

We now state how we implement a substitution store for position . We will make use of the following data structures:
\begin{enumerate}
\item A constant-time weak prefix search on the set of strings  formed by the concatenation of pairs of strings associated with substitution lists. That is for each pair  we insert the string  in . This weak prefix search will be able to return in constant-time the range of elements  prefixed by some string  (the constant query time supposes some suitable preprocessing whose time is not taken into account). The space used by the weak prefix search data structure will be  for any desired constant . 
\item A 1D colored range reporting data structure implemented using Lemma~\ref{lemma:1D_color_rep} on top of the array . This will allow to report the  distinct characters in any given range  in time . The space used will be  bits. 
\item A prefix-sum data structure that stores the size of each of the substitution lists in sorted order. The space used will be  bits. In other words, given any , the prefix-sum data structure will be able to return . 
\end{enumerate}

By extension, given a pair  we will associate a substitution list formed by the union of the substitution lists associated with all pairs  such that  is prefixed by . We call a substitution list  a \emph{virtual} substitution list as such a list does not exist in the store, but is in reality obtained by the union of existing  substitution lists. The role of the weak prefix search and the 1D colored range reporting data structures is precisely to allow efficient computation of the virtual list  as the union of all lists . 

The  weak prefix search data structures will be implemented using the same global hash function. 
\subsubsection{Query algorithm}

We now show how queries are implemented. We will concentrate on queries for substitution candidates. As pointed out before, queries for deletion candidates are handled in exactly the same way described in Section~\ref{section:const_query_algo1}. This is possible because we have  candidates and we can exhaustively check each candidate within the same time bounds as in the constant alphabet size. Querying for insertion candidates is only slightly different from querying for substitution candidates and is omitted. 

Queries for substitution candidates are implemented in the following way: given a query string  of length , we traverse the trie top-down and find the locus of . The locus of a string  is defined by a pair . We let  denote the lowest node  in the trie whose associated factor  is prefix of  (it is easy to see that such a node is unique). Then the locus  of  is defined in the following way: 
\begin{enumerate}
\item If  has a child labeled with character , then  will be the child of  labeled with  and  will be the length of the longest common prefix between  and  where  is the factor associated with  (that is  differs with  on character number , but agrees on characters ). We call this a type  locus. 
\item Otherwise ( has no child labeled with character ). Then  and the locus will be . We call this a type  locus. 
\end{enumerate}
As we traverse the nodes from the root down to the node , we identify all the nodes with their associated factors. Assume that the nodes are (in top-down order)  and their associated factors are  (note that the factors are of increasing length). Note that every factor  is prefix of  except  which could potentially not be prefix of . 

Notice that because of the suffix tree properties, we know that candidate positions for substitutions can only be  to which we add position  in case . To see why, notice that every prefix  of  of length between  and  (or of length less than  or  between  and ) is not associated with any suffix tree node, but still appears as a factor in the text. This means that there is a unique character  such that  appears in the text (if there were two such characters then there would have been a suffix tree node associated with ). However  is also prefix of  and thus agrees on character  with . Thus we have no character that we can substitute for character  in  and yet obtain a factor appearing in the text. 

In summary, the substitution candidate characters will be :
\begin{enumerate}
\item For every node  with  (and also for the node  when the locus is of type ) we have the following candidates:
\begin{enumerate}
\item The character that labels a heavy child of  at position  (recall that  is the factor associated with ) if this character is different from . 
\item The characters obtained from the substitution list corresponding to the pair  stored in the substitution store number  (we naturally avoid querying for the character  if it is contained in the substitution list). We only test one of those characters by substituting it at position  and, if we have a match, then we can report all the remaining characters in the list (except character  if it appears in the list). Details on how the substitution store is queried are below. 
\end{enumerate}
\item In case of a type  locus and , we also try to substitute the character  at position . 
\end{enumerate}

We now give more details on how the substitution store is queried. Recall that a substitution store number  stores the substitution lists associated with the pairs  such that  and  and moreover there exists at least one character  such that  is a b-factor of the text (the substitution list contains at least the character ). 

Thus in case we have , we can directly query the relevant substitution list. 
Otherwise, we first query the weak prefix search data structure which returns a range of substitution lists, then use the prefix-sum data structure to convert this range into a range in the array .

We finally use the 1D-colored rangeed reporting data structure to report all the distinct colors in the range. We need to use the special colored range reporting data structure because the colors may be duplicated in different substitution lists. We only test for the first character (the first reported color), and if it gives a match, then we go on and substitute the following characters (the following colors obtained from the colored rangeed range reporting data structure). 



In the next subsection, we describe how to reduce the space used per substitution list element from  to just , hence removing any dependence on . 



\subsection{Space reduction using generalized weight decomposition}
\label{section:gen_weight_decomp}
In order to reduce the space we will use a more fine-grained categorization of the nodes. Instead of having only two types of nodes (heavy and light), we will instead define multiple types of nodes according to their weight. We first define the generalized weight decomposition, then prove an important lemma on the decomposition and finally show how to use it to reduce the space usage of our index. 
\subsubsection{Generalized weight decomposition}
We order the children of a node  by their weights (from the heaviest to the lightest node). We say that a child  has rank  if it is the th heaviest child of its parent. By extension we say that the edge connecting  to  is of rank . 
We have the following lemma:
\begin{lemma}
\label{lemma:heav_tree_lemma}
Any root-to-leaf path in a tree with  nodes contains at most  light edges of rank at least .
\end{lemma}
\begin{proof}
The proof is easy. We first prove that traversing an edge of rank  reduces the weight of the current node by a factor at least . This is easy to see by contradiction. Suppose  has weight  and its child  of rank  has weight . Then necessarily all the  children of rank at most  are at least as heavy as  and thus have necessarily weight at least  each. 
Thus the total weight of the first  children of  is at least  which is more than the weight of . 
We thus have proved that traversing an edge of rank  reduces the weight of the current node by a factor at least . 
From there we can easily prove our lemma. We start the traversal at the root which has weight  and each time we traverse a child of rank at least  we divide the weight by a factor at least . After traversing  nodes we reach a node with weight  which by definition can not contain any light edge in its subtree. Thus we reach a leaf after traversing at most  nodes of rank at least . 
\qed
\end{proof}
An observation similar to the lemma above was already made in a recent paper by Bille et al.~\cite{BGSV12}. 

\subsubsection{Implementation}
The generalization works in the following way: we categorize the light children of a node into at most  categories. Each category  contains the children whose rank is between  and . 
To implement our generalized method, we reuse exactly the same data structures used in Section~\ref{section:DS_impl}, except for the following two points:
\begin{enumerate}
\item In the suffix tree, we order all the labels of the children of every suffix tree node by decreasing weight. For a given node , with  children, we order the children by decreasing weight and store an array  where we put at position  the character that labels the child number  in the ordering. 
\item Then we will use  substitution stores instead of  substitution stores. More precisely we store a substitution store for each position  and category . 
\end{enumerate}
The  substitution stores for the same position will all share the same weak prefix search data structure (that will store exactly the same content that was originally stored when there was a single substitution store for the position), but use  different  colored range reporting and prefix-sum data structures. Note that the prefix-sum data structures could possibly indicate empty substitution lists, in case there exists another non-empty substitution list with the same pair . 
We now bound the space incurred by the modifications. The table  uses   bits and, when added over all the suffix tree nodes, the space becomes  bits. We will prove that all the substitution stores with the same category  will use in total  bits of space. 
In order to reduce the space used by the substitution lists (stored using the  colored range reporting and the prefix-sum data structures), we will use variable lengths to code the characters. More precisely, instead of storing the character (in the  colored range reporting data structure) using  bits, we will store the rank of the child (among the children of the parent of that child) labeled by that character. 

Then using the table , we can easily recover the character. The fact that category  contains nodes of rank at most , means that we only need  bits to encode each substitution list element in a substitution store of category . 

We note that each root-to-leaf path has at most  nodes of category . This is the case as nodes in category  have rank at least . Thus by Lemma~\ref{lemma:heav_tree_lemma} any root-to-leaf path has at most  light edges of rank at least , and we deduce that the substitution stores of category  will have at most  elements. As each element is encoded using  bits, we conclude that the total space to encode the substitution lists of category  is  bits of space.

When multiplied over all the (at most)  categories, we get total space  bits.
This bounds the space used by the  colored range reporting data structure. The space used by the prefix-sum data structures will be at most , since for every non-empty substitution list associated with a pair , we could have up to -1 empty ones. 
The space used by the weak prefix search data structure remains the same. 
Overall, the total space will be  bits. 
\subsubsection{Queries}
We only describe queries for substitutions. A query will work in the following way: we traverse the suffix tree top-down and for each node of the suffix tree at depth , we use the label of the heavy child as a candidate character for substitution (or insertion). 
To get candidate characters that label light children, we query the substitutions stores  for every  separately. We first start by querying the weak prefix search data structure (which is shared by all the 
 substitution stores). We then query successively all the  prefix-sum data structures associated with the same pair  until we find the first non-empty one, in which case we retrieve the characters in the substitution list from the  colored range reporting structure and check that the first character gives a match (exactly as in Section~\ref{section:DS_impl}). If that is the case, we continue reporting all the other characters in the substitution list and in all the other non-empty substitution lists associated with the pair . If the first substituted character did not give a match, then we know that all the other substitution lists will also  not give a match. 

Since we have in total up to  empty substitution stores, and a query on each substitution store takes constant time independent of ~\footnote{Note that a query on all but the first non-empty substitution store takes constant time per reported character, since it involves querying the prefix-sum and the  colored range reporting data structures which answer in, respectively, constant time and constant time per element.}, the new query algorithm will incur an additive  term in the query time. 

\subsection{Handling very large }
Throughout the paper, we have assumed that . In this section we show how to handle the case . To handle this case, we can use an indexable dictionary  that stores the set  of characters which appear in the text . As a byproduct, the indexable dictionary will associate a unique number  to each character . Moreover, the dictionary should also be able to do the reverse mapping, returning  when given . 
In order to implement the indexable dictionary, we can use the data structure of~\cite{RRS07} which supports the mapping and reverse mapping in constant time and uses  bits of space which is less than the space used by the index of Theorem~\ref{full_arb_theorem}. 
Then, we replace the character  in the text  with , giving a new text . Then the size of the alphabet of  will be  and we can build the index of Theorem~\ref{full_arb_theorem} on  rather than . 

To implement a query on a string , we first query the dictionary  for each character  obtaining the number . We now have three cases:
\begin{enumerate}
\item If all characters of  have been found in , then we replace each  with  obtaining a new string  and query the index of Theorem~\ref{full_arb_theorem}. 
Finally when querying the index, we should use the dictionary  to translate the characters output by the index to the original alphabet (that is given , return ). 
\item If exactly one character of  was not found in the dictionary, we conclude that the only two possible errors are the deletion or substitution of that character. We thus can just query the suffix tree for an exact matches on the two strings obtained by deleting or substituting the character. 
\item If two or more of the characters of  were not found in , then this means we have more than one error and the result of the query will be empty. 
\end{enumerate}




\section{Conclusion}
\label{section:conclusion}
In this paper we have presented new and improved solutions for indexing a text  for substring matching queries with one edit error. 
Given a query string  of length  our indices are able to find the  occurrences of substrings of  at edit distance  from  in time . Our first index is only suitable for constant alphabet sizes and uses   bits of space, where  is the length of the text  and  is any constant such that . The same index can be tuned to achieve a different tradeoff, namely to use  bits of space while answering to queries in time . 

The other index presented in this paper is suitable for an arbitrary alphabet size and uses  bits of space while answering to queries in optimal time . 

It is an interesting open problem to show whether our indices are optimal or whether they can be improved. It is interesting to compare our indices with known results for substring matching with zero errors. For constant-sized alphabets, the best known results achieve  optimal  query time using  bits of space~\cite{GV05}. The  is known to be the best possible for constant-sized alphabets as the time needed to read the query string is at least  (assuming that the characters of the strings are tightly packed). Our first index achieves the same space usage but has a  factor slowdown in the length of the query string. It is an interesting open problem to show whether this  factor is inherent in the substring matching with one error when the space is required to be small. 

For arbitrary alphabet sizes, the  query time of our second solution is optimal as it is known that  time is needed in the worst case to read the query string when the alphabet is of arbitrary size~\footnote{Each character could occupy up to  bits which means that we need at least  time to read each character in a our RAM model with .}. On the other hand the space usage  is a factor  more than the  bits used by standard indices for substring matching with zero errors (suffix arrays and suffix trees). It is an intriguing open problem to show whether this  factor is inherent in substring matching with one error or whether it can be removed. 

\section*{Acknowledgements}
The author wishes to thank the anonymous reviewers for their helpful comments and corrections
and Travis and Meg Gagie for their many helpful corrections and suggestions. 


\small 
\bibliography{full_text_edit_dist} 
\normalsize

\newpage

\end{document}  
