\documentclass[11pt, letter]{article}

\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{xspace}
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{array}


\usepackage{fullpage}

\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\SetFuncSty{textsc}
\SetDataSty{textit}
\SetCommentSty{textrm}
\SetKwFunction{Extract}{ExtractFirstDigit}
\SetKwData{Forward}{forward}
\SetKw{True}{true}






\newcommand{\XSays}[2]{{
      {\fbox{\tt
            #1:} }
      #2
      \marginpar{#1}
      {\fbox{\tt
            end}}
   }
}
\newcommand{\XSaysExt}[2]{{
      ~\\
      \fbox{\begin{minipage}{0.99\linewidth}
             {\fbox{\tt
                   #1:}} #2
         \end{minipage}}
      \marginpar{#1}}}


\newcommand{\Anne}[1]{{\XSays{Anne}{#1}}}
\newcommand{\AnneX}[1]{{\XSaysExt{Anne}{#1}}}
\newcommand{\Amer}[1]{{\XSays{Amer}{#1}}}
\newcommand{\AmerX}[1]{{\XSaysExt{Amer}{#1}}}
\newcommand{\Christian}[1]{{\XSays{Christian}{#1}}}
\newcommand{\ChristianX}[1]{{\XSaysExt{Christian}{#1}}}



\newcommand{\ie}{i.\,e.,}
\newcommand{\eg}{e.\,g.,}
\newcommand{\etal}{\textit{e{}t~a{}l.}\xspace}
\newcommand{\REAL}{\mathbbm{R}}
\DeclareMathOperator{\cost}{cost}
\DeclareMathOperator{\opt}{opt}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator{\Exp}{E}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\high}{high}
\DeclareMathOperator{\low}{low}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{case}[]{Case}
\newtheorem{assumption}[]{Assumption}
\newtheorem{property}[]{Property}

\newcommand{\seclab}[1]{\label{sec:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\subsecref}[1]{Subsection~\ref{sec:#1}}

\newcommand{\thmlab}[1]{{\label{theo:#1}}}
\newcommand{\thmref}[1]{Theorem~\ref{theo:#1}}

\newcommand{\lemlab}[1]{\label{lemma:#1}}
\newcommand{\lemref}[1]{Lemma~\ref{lemma:#1}}

\newcommand{\figlab}[1]{\label{fig:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}

\newcommand{\tablab}[1]{\label{tab:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}

\newcommand{\deflab}[1]{\label{def:#1}}
\newcommand{\defref}[1]{Definition~\ref{def:#1}}

\newcommand{\alglab}[1]{\label{alg:#1}}
\newcommand{\algref}[1]{Algorithm~\ref{alg:#1}}

\newcommand{\problab}[1]{\label{prob:#1}}
\newcommand{\probref}[1]{Problem~\ref{prob:#1}}

\newcommand{\obslab}[1]{\label{obs:#1}}
\newcommand{\obsref}[1]{Observation~\ref{obs:#1}}

\newcommand{\corlab}[1]{\label{cor:#1}}
\newcommand{\corref}[1]{Corollary~\ref{cor:#1}}

\newcommand{\caselab}[1]{\label{case:#1}}
\newcommand{\caseref}[1]{Case~\ref{case:#1}}

\newcommand{\claimlab}[1]{\label{claim:#1}}
\newcommand{\claimref}[1]{Claim~\ref{claim:#1}}

\newcommand{\asslab}[1]{\label{ass:#1}}
\newcommand{\assref}[1]{Assumption~\ref{ass:#1}}

\newcommand{\proplab}[1]{\label{prop:#1}}
\newcommand{\propref}[1]{Property~\ref{prop:#1}}

\newcommand{\NP}{\textbf{NP}} \newcommand{\spine}{\textsl{spine}} 

\renewcommand{\O}[1]{\ensuremath{{O\pth{#1}}}}
\newcommand{\Frechet}{Fr\'echet\xspace}
\newcommand{\ProblemGM}{{MAGICIAN}}
\newcommand{\ProblemSP}{{SAM-PROP}}
\newcommand{\atgen}{\symbol{'100}}
\providecommand{\eps}{{\varepsilon}}\newcommand{\Astop}{\overline{a}}

\renewcommand{\th}{t{}h\xspace}
\providecommand{\ceil}[1]{\left\lceil {#1} \right\rceil}
\providecommand{\floor}[1]{\left\lfloor {#1} \right\rfloor}
\providecommand{\pth}[2][\!]{#1\left({#2}\right)}
\providecommand{\brc}[1]{\left\{ {#1} \right\}}
\providecommand{\cbrc}[1]{\left\langle{#1}\right\rangle}
\providecommand{\sep}[1]{\,\left|\, {#1} \MakeBig\right.}
\providecommand{\cardin}[1]{\left\lvert#1\right\rvert}
\newcommand{\pbrc}[1]{\left[ {#1} \right]}
\renewcommand{\Re}{{\rm I\!\hspace{-0.025em} R}}
\newcommand{\Na}{{\rm I\!\hspace{-0.025em} N}}

\newcommand{\ambientD}{\ensuremath{d}}
\newcommand{\nrClusters}{\ensuremath{k}}
\newcommand{\lenClusters}{\ensuremath{\ell}}
\newcommand{\constA}{\ensuremath{\alpha}}
\newcommand{\constB}{\ensuremath{\beta}}
\newcommand{\costParam}{\ensuremath{\delta}}
\newcommand{\initialCost}{\ensuremath{D}}
\newcommand{\costOpt}{\ensuremath{\delta^{*}}}

\newcommand{\trajectory}[2]{\ensuremath{{#1}_{#2}}}
\newcommand{\trajectorySimp}[2]{\ensuremath{\widehat{{#1}_{#2}}}}
\newcommand{\trajectoryVtx}[3]{\ensuremath{\trajectory{#1}{#2,#3}}}

\newcommand{\inputSym}{\ensuremath{\tau}}
\newcommand{\centerSym}{\ensuremath{c}}

\newcommand{\inputTraj}[1]{\trajectory{\inputSym}{#1}}
\newcommand{\inputTrajVtx}[2]{\trajectoryVtx{\inputSym}{#1}{#2}}
\newcommand{\inputTrajSimp}[1]{\trajectorySimp{\inputSym}{#1}}
\newcommand{\inputTrajPiece}{\widetilde{\inputSym}}
\newcommand{\centerTraj}[1]{\trajectory{\centerSym}{#1}}
\newcommand{\centerTrajVtx}[2]{\trajectoryVtx{\centerSym}{#1}{#2}}

\newcommand{\VtxSet}{\ensuremath{\mathcal{V}}}
\newcommand{\EdgeSet}{\ensuremath{\mathcal{E}}}
\newcommand{\Coreset}{\ensuremath{\mathcal{M}}}

\newcommand{\distFr}[2]{\ensuremath{d_F\pth{#1,#2}}}
\newcommand{\distFrConst}{\ensuremath{d_F}}

\newcommand{\ball}[2]{\ensuremath{ball\pth{#1,#2}}}

\newcommand{\setSubC}[4]{\ensuremath{\brc{ #1(#2) ~|~ #2 \in [#3,#4]} }}

\newcommand{\range}[2]{\ensuremath{[#1]_{#2}}}
\newcommand{\minSubC}[4]{\ensuremath{\min( #1[#3,#4])}}
\newcommand{\maxSubC}[4]{\ensuremath{\max( #1[#3,#4])}}




\begin{document}



\title{Clustering time series under the \Frechet distance
\thanks{The conference version of this paper will be published at 26th ACM-SIAM Symposium on Discrete Algorithms (SODA) 2016.}
}

\author{Anne Driemel\thanks{Department of Mathematics and Computer Science, TU Eindhoven, The
   Netherlands; 
      \texttt{a.driemel}\hspace{0cm}\texttt{\atgen{}tue.nl}. 
      Work on this paper was partially funded by NWO STW project ``Context
      awareness in predictive analytics'' and NWO Veni project ``Clustering time
      series and trajectories (10019853)''. } \and Amer Krivo\v{s}ija\thanks{Department of Computer Science, TU Dortmund, Germany; 
      \texttt{amer.krivosija}\hspace{0cm}\texttt{\atgen{}tu-dortmund.de}. Work on this paper has been partly supported by DFG within the Collaborative Research Center SFB 876 ``Providing Information by Resource-Constrained Analysis'', project A2.} \and Christian Sohler\thanks{Department of Computer Science, TU Dortmund, Germany; 
      \texttt{christian.sohler}\hspace{0cm}\texttt{\atgen{}tu-dortmund.de}. Work on this paper has been partly supported by DFG within the Collaborative Research Center SFB 876 ``Providing Information by Resource-Constrained Analysis'', project A2.} }

\date{\today}

\maketitle

\begin{abstract}
The \Frechet{} distance is a popular distance measure for curves. We study the problem of clustering time series under the \Frechet{} distance.  In particular, we give -approximation algorithms for variations of the following problem with parameters  and .  Given  univariate time series , each of complexity at most , we find  time series, not necessarily from , which we call \emph{cluster centers} and which each have complexity at most , such that (a) the maximum distance of an element of  to its nearest cluster center or (b) the sum of these distances is minimized.  Our algorithms have running time near-linear in the input size for constant  and .  To the best of our knowledge, our algorithms are the first clustering algorithms for the \Frechet{} distance which achieve an approximation factor of  or better. 

Keywords: time series, longitudinal data, functional data, clustering, \Frechet{} distance, dynamic time warping, approximation algorithms.
\end{abstract}
\vfill
\thispagestyle{empty}
\pagebreak
\setcounter{page}{1}

\section{Introduction}

Time series are sequences of discrete measurements of a continuous signal. Examples of data that are often represented as time series include stock market values, electrocardiograms (ECGs), temperature, the earth's population, and the hourly requests of a webpage. In many applications, we would like to automatically analyze time series data from different sources: for example in industry 4.0 applications, where the performance of a machine is monitored by a set of sensors. An important tool to analyze (time series) data is clustering. The goal of clustering is to partition the input into groups of similar time series. Its purpose is to discover hidden structure in the input data and/or summarize the data by taking a representative of each cluster.  Clustering is fundamental for performing tasks as diverse as data aggregation, similarity retrieval, anomaly detection, and monitoring of time series data.  Clustering of time series is an active research topic
\cite{
ar-dcaa-2013,
Boecking2014129,
cmp-fcis-07, 
Hsu2014358,
jp-fdc-13,
Li2013243,
liao05survey,
tc-sbc-12,
Petitjean2011678,
rm-fcbwm-06, 
sf-mpfa-14,
hd-tsc-14,
wsh-cbc-06,
zt-MODIS-14,
qg-nadtw-12}
however, most solutions lack a rigorous algorithmic analysis. Therefore, in this paper we study the problem of clustering time series from a theoretical point of view. 


Formally, a \emph{time series} is a recording of a signal that changes over time. It consists of a series of paired values , where  is the th measurement of the signal, and  is the time at which the th measurement
was taken. A common approach is to treat time series data as point data in high-dimensional space.  That is, a time series
 of the input set is treated as the point
 in -dimensional Euclidean space. Using this simple interpretation of the data, any clustering algorithm for points can be applied. Despite it being common practice (for a survey, see~\cite{liao05survey}), it has many limitations. 
One major drawback is the requirement that all time series must have the same length and the sampling must be regular and synchronized. In particular, the latter requirement is often hard to achieve. 

In this paper, we follow a different approach that has also received much attention in the literature. In order to formulate the objective function of our clustering problem, we consider a distance measure that allows for irregular sampling and local shift and that only depends on the ``shape'' of the analyzed time series: the \Frechet{} distance.
The \Frechet{} distance is defined for continuous functions .
Two functions  have \Frechet{} distance at most , if one can move simultaneously 
but at different and possibly changing positive speed on the domains of  and  from  to  such 
that at all times , where  and  are the current positions on the respective domains. 
The \Frechet{} distance is the infimum over the values  that allow such a movement. For a formal
definition, see \secref{problem}. The \Frechet{} distance between two monotone
functions  equals the maximum of  and . 
This also implies, that the \Frechet{} distance between two functions is completely 
determined by the sequence of local extrema ordered from  to . 
In order to consider the \Frechet{} distance of time series, we view a time series as a specification of
the sequence of local extrema of a function. 


Once we have defined the distance measure we can also formulate the clustering problem we want to study.
We will look at two different variants: -center clustering and -median clustering. Both methods are
based on the idea of representing a cluster by a cluster center, which can be thought of as being a
representative for the cluster. In -center clustering, the objective is to find a set of  time series (the cluster centers)
such that the maximum \Frechet{} distance to the nearest cluster center is minimized over all input time series.
In -median clustering, the goal is to find a set of  centers that minimizes the sum of distances to 
the nearest centers. However, this is not yet the problem formulation we consider.

We would like to address another problem that often occurs with time series: noise. In many applications where
we study time series data, the measurements are noisy. For example, physical measurements typically have measurement errors
and when we want to determine trends in stock market data, the effects of short term trading is usually not of interest.
Furthermore, a cluster center that minimizes the \Frechet{} distance to many curves may have a complexity (length of the time series)
that equals the sum of the complexities of the time series in the cluster. This will typically lead to a vast overfitting
of the data. We address this problem by limiting the complexity of the cluster center. Thus,
our problem will be to find  cluster centers each of complexity at most  that minimize the -center and
-median objective under the \Frechet{} distance.
It seems that  none of the existing approaches summarizes the
input along the time-dimension in such a way.  

\paragraph{Our results} 

To the best of our knowledge, clustering under the \Frechet{} distance has not been studied before. 
We develop the first -approximation algorithm for the -center and the -median problem 
under \Frechet{} distance when the complexity of the centers is at most . For constant  and , the running time of our algorithm is , where  is the number of input 
time series and  their maximum complexity. (see \thmref{k:l:center:main} and \thmref{k:l:median:main}.)
We also prove that clustering univariate curves is \NP-hard and show that the doubling dimension of the 
\Frechet{} (pseudo)metric space is infinite for univariate curves.

\paragraph{Challenge and ideas}
High-dimensional data pose a common challenge in many clustering applications.
The challenge in clustering under the \Frechet distance is twofold:
\begin{compactenum}[(A)]
\item High dimensionality of the joint parametric
space of the set of time series:  For the seemingly simple task of computing the (\Frechet{})
\emph{median} of a fixed group of time series, state-of-the-art algorithms run
exponentially in the number of time series~\cite{ahn2015middle, hr-fdre-14}, since the standard
approach is to search the joint parametric space for a monotone 
path~\cite{ag-cfdbt-95, akw-mpcfd-10,buchin2012four, bbw-eapcm-09, feldman2012gps, GudVahr12,
hr-fdre-14, mssz-fdsl-2011}. 
\item High dimensionality of the metric space: the doubling dimension of the \Frechet metric space is
infinite, as we will show, even if we restrict the length of the time series.
Existing (-approximation algorithms \cite{abs-cm-10, kumar2010lineartime} with comparable running time are only known to work for special cases
as the Euclidean -median problem or (more generally) for metric spaces with bounded doubling dimension 
\cite{abs-cm-10}.
\end{compactenum}
These two challenges make the clustering task particularly difficult, even for short univariate time
series. Our approach exploits the low dimensionality of the ambient space of the
time series and the fact that we are looking for low-complexity cluster 
centers which best describe the input. 
We introduce the concept of \emph{signatures} (\defref{signature}) which capture critical points of
the input time series. We can show that each signature vertex of an input curve
needs to be matched to a different vertex of its nearest cluster center~(\lemref{nec:suff}).  
Furthermore, we use a technique akin to \emph{shortcutting}, which has been used
before in the context of partial curve matching~\cite{bbw-eapcm-09, dh-jydfd-13}.
We show that any vertex of an optimal solution that is \emph{not} matched to a
signature vertex, can be omitted from the solution without changing its
cost~(\thmref{remove:one}).

These ingredients enable us to generate a constant-size set of candidate solutions.
The ideas going into the individual parts can be summarized as follows.
For the -center problem we generate a candidate set based on the 
entire input and a decision parameter. If the candidate set turns out too large,
then we conclude that there exists no solution for this parameter, since more
vertices would be needed to ``describe'' the input. 
For the -median problem we use the approach of random sampling used
previously by Kumar~\etal~\cite{kumar2010lineartime} and Ackermann~\etal~\cite{abs-cm-10}.
We show that one can generate a constant-size candidate set that contains a
-approximation to the -median based on a sample of constant size.
To achieve this, we observe that a vertex of the optimal solution that is
matched to a signature vertex and which is unlikely to be induced by our
sample, can be omitted without increasing the cost by a factor of more than
~(\lemref{omit:low:prob}).



\paragraph{Related Work}
A distance measure that is closely related to the \Frechet{} distance is \emph{Dynamic time warping (DTW)}.
DTW has been popularized in the field of data mining~\cite{dtswk-08,mueller07dtw} and is known for its unchallenged
universality.
It is a discrete version of the \Frechet
distance which measures the total sum of the distances taken at certain fixed
points along the traversal. The process of traversing the curves with varying
speeds is sometimes referred to as ``time-warping'' in this context.
It has been successfully used in the automated classification of time series
describing phenomena as diverse as surgical processes \cite{Forestier2012255},
whale singing \cite{bmp-ackw-07}, chromosomes \cite{Legrand2008215},
fingerprints \cite{888711}, electrocardiogram (ECG) frames \cite{1013101} and
many others.  
While DTW was born in the 80's from the use of dynamic programming with the
purpose of aligning distorted speech signals, the \Frechet{} distance was
conceived by Maurice \Frechet{} at the beginning of the 20th century in the
context of the study of general metric spaces. 
The best known algorithms for computing either distance measure between two
input time series have a worst-case running time which is roughly quadratic in
the number of time stamps~\cite{buchin2012four, mueller07dtw}. Faster
algorithms exist for both problems under certain input
assumptions~\cite{dhw-afd-12,keogh2005exact}. Recently, Bringmann
showed~\cite{Bringmann14} that the \Frechet{} distance between two polygonal
curves lying in the plane cannot be computed in strongly subquadratic time, unless
the Strong Exponential Time Hypothesis (SETH) fails. This was extended by Bringmann and Mulzer to the 1-dimensional case \cite{bm-adfd-15}.

Both distance measures consider
only the ordering of the measurements and ignore the explicit time stamps. This
makes them robust against local deformations.  Both distance measures deal well
with irregular sampling and have been used in combination with curve
simplification techniques~\cite{ahmw-nltcs-05,dhw-afd-12,keogh1999scaling}.
However, while the \Frechet distance is inherently independent of the sampling
of the curves, DTW does not work well when one of the two curves is sampled
much less frequently (see \figref{subsampling}). Since we are interested in finding cluster centers of low
complexity, we therefore focus on the \Frechet distance.

\begin{figure}\centering
\includegraphics{subsampling.pdf}
\caption{Illustration why a continuous distance measure performs better than a
discrete distance measure if sampling rates differ substantially. Shown is the
curve  and the same curve subsampled at times  (for
better visibility, the subsampled curve  is translated by ). A discrete 
assignment between the vertices of the curves is shown in light blue.
 records a linear process with some error . A suitable distance measure should estimate
the distance between  and  in the interval . This is true
for the (normalized) DTW, if the rate of subsampling is high enough. Similarly
it is true for the (continuous) \Frechet distance at any rate of subsampling. 
However, as the rate of subsampling decreases, DTW will estimate the distance
increasingly larger.
}
\figlab{subsampling}
\end{figure}

The problem of clustering points in general metric spaces has been extensively studied 
in many different settings. The problem is known to be computationally hard in most of its
variants~\cite{aloise2009nphard,jain2002greedy,megiddo1984geo,Vaziranibook}.
A number of polynomial-time constant-factor approximation algorithms are known 
~\cite{AryaGKMMP04,CharikarG05, cgts-kmedian-02, CharikarL12, c-kmc-09,feder1988optimal,Gonzalez1985,HochbaumShmoys1985,jain2002greedy,ls-akm-13}. 
For the -center problem the best such algorithm achieves a -approximation
\cite{Gonzalez1985, HochbaumShmoys1985} and this is also the best lower bound
for the approximation ratio of the polynomial-time algorithms unless . This picture
looks much different if one makes certain assumptions on the graph that defines
the metric, as done in the work by Eisenstadt~\etal~\cite{emk-akcpg-14}, for
example. 

For the -median problem in a general metric space a
-approximation can be achieved \cite{ls-akm-13}. The
best lower bound for the approximation ratio of polynomial time algorithm is
 unless  \cite{jain2002greedy}. If the data points are from the
Euclidean space , a number of different algorithms are known that compute
a -approximation to the -median problem 
\cite{c-kmc-09,FeldmanLangberg11, HarPeledKushal07, harpeledmazumdar2004, KR99, kumar2010lineartime}. 
Ackermann \etal~\cite{abs-cm-10}
show that under certain conditions a randomized -approximation algorithm by Kumar
\etal~\cite{kumar2010lineartime} can be extended to general distance measures. 
In particular, they show that one can compute a -approximation to 
the -median problem if the distance metric is such that the -median 
problem can be -approximated using information only from a random
sample of constant size. They show that this is the case for
metrics with bounded doubling dimension. In our case, the doubling dimension is,
however, unbounded.



A different approach would be to use the technique of metric
embeddings, which proved successful for clustering in Euclidean 
high-dimensional spaces. A metric embedding is a mapping between two metric
spaces which preserves distances up to a certain distortion.
Unlike the similar Hausdorff distance, for which non-trivial embeddings are
known, it is not known if the \Frechet distance can be embedded into
an  space using finite dimension~\cite{IndMat04}.
Any finite metric space can be embedded into
, which is therefore considered as ``the
mother of all metrics''. In turn, any bounded set in  with  
can be embedded as curves with the \Frechet distance (see also
\secref{nphard}).   Recent work by Bartal~\etal~\cite{bartal2014impossible}
suggests that, due to the infinite doubling dimension, a metric embedding of the
\Frechet distance into an -space needs to have at least super-constant
distortion.  On the positive side, Indyk showed how to embed the \Frechet
distance into an inner product space for the purpose of designing approximate
nearest-neighbor data structures \cite{i-approxnn-02}. 
However, this work focuses on a different version of the \Frechet distance,
namely the \emph{discrete} \Frechet distance, which is aimed at sequences
instead of continuous curves. For any , the resulting data structure is
-approximate, uses space exponential in
, and achieves  query time, where  is the maximum length of
a sequence and  is the number of sequences stored in the data structure. 

The problem of clustering under the DTW distance has also been studied in the data mining community. In particular, substantial effort has been put into extending Lloyd's algorithm~\cite{lloyd82} to the case of DTW, an example is the work of Petitjean \etal~\cite{Petitjean2011678}. In order to use Lloyds algorithm, one first has to be able to compute the mean of a set of time series. For DTW, this turns out to be a non-trivial problem and current solutions are not satisfactory.  One of the problems is that the complexity of the mean is prone to become quite high, namely linear in the order of the total complexity of the input. This can lead to overfitting in the same way as discussed for the \Frechet distance. For a more extensive discussion we refer to~\cite{ar-dcaa-2013} and references therein.

Also in statistics, the problem of clustering longitudinal data, or functional data, is an active research topic and solutions based on wavelet decomposition and principal component analysis have been suggested \cite{cmp-fcis-07, jp-fdc-13, rm-fcbwm-06, wsh-cbc-06}.

\section{Preliminaries}

A \emph{time series} is a series  of measurements
 of a signal taken at times . We assume  and  is finite. A
time series may be viewed as a continuous function  by
linearly interpolating  in order of , . We
obtain a polygonal curve with \emph{vertices}  and segments between  and  called \emph{edges}
. We will simply refer
to  as a \emph{curve}. 
When defining such a curve, we may write ``curve  with 
vertices'', or ``curve ''.
\footnote{Notice that this notation does not specify the points of time at which the measurements
are taken. The reason is that the \Frechet distance only depends on the ordering of
the measurements (and their values), but not on the exact points of time when the measurements
are taken.}
We say that such a curve  has \emph{complexity} . 
We denote its set of vertices with  . For
any , we denote the subcurve of  starting at
 and ending at  with . 
We define 
, and
, 
to denote the minimum and maximum along a (sub)curve.

Furthermore, we will use the following non-standard notation for an interval.
For any  we define . 
We denote . For any
set , we denote its cardinality with .


Let  denote the set of continuous and increasing functions
 with the property that  and . 
For two given functions
 and ,
their \emph{\Frechet distance} is defined as 

The \Frechet{} distance between two time series is defined as the \Frechet{} distance
of their corresponding 
continuous functions.
\footnote{
In the above definition, the \Frechet{} distance is only defined for functions with domain .
This is mostly for simplicity of analysis.
We can easily extend it to other domains using an arbitrary homeomorphism that identifies this domain with . 
The \Frechet{} distance is invariant under reparametrizations. 
All our algorithms use the parametrizations only implicitely. Any input time
series is given as an ordered list of measurements, without explicit
time-stamps. Therefore our definitions are without loss of generality.
}
Note that any  induces a bijection between the two curves.
We refer to the function  that realizes the \Frechet distance as a
\emph{matching}. It may be that such a matching exists in the limit only. That
is, for any , there exists a  that matches each point
on  to a point on  within distance .



The \Frechet{} distance is a pseudo metric~\cite{ag-cfdbt-95}, i.e. it satisfies all properties of
a metric space except that there may be two different functions that have distance
. If one considers the equivalence classes that are induced by functions of pairwise
distance  we can obtain a metric space  defined by the \Frechet
distance and the set  of all (equivalence classes of) univariate time series.
We denote with  the set of all univariate time series of complexity at
most .

We notice that only the ordering of the  is relevant and that under \Frechet distance two curves can be thought of being identical, if they have the same sequence of local minima and maxima. Therefore, we can assume that a curve is induced by its sequence of local minima and maxima and we will use the term curve in the paper to describe the equivalence class of curves with pairwise \Frechet distance 0.

\begin{definition}\deflab{concatenation}
Let two curves , , and ,  be given, such that . The concatenation of  and  is a curve  defined as , such that 


\end{definition}

We are going to use the following simple observations throughout the paper. 
\begin{observation}\obslab{fd:concat}
Let two curves  and 
be the concatenations of two subcurves each,  and
, then it holds  that 
\end{observation}

\begin{observation}\obslab{segments}
Given two edges  and  with , it holds that 

\end{observation}

We make the following \emph{general position} assumption on the input. For every
input curve  we assume that no two vertices  have
the same coordinates and any two differences between coordinates of two
vertices of  are different. This assumption can easily be achieved
by symbolic perturbation. Furthermore we assume that  has no edges 
of length zero and its vertices are an alternating sequence of minima and
maxima, i.e. no vertex lies in the linear interpolation of its two neighboring vertices.

\subsection{Problem statement}
\seclab{problem}

Given a set of  time series  and parameters , we define a \emph{-clustering}  as a set of  time series  taken from   which minimize one of the following cost functions:
 

 

We refer to the clustering problem as 
\emph{-center} (Equation~\ref{def:kcenter}) 
and \emph{-median} (Equation~\ref{def:kmedian}),
respectively. 
We denote the cost of the optimal solution as 

where the restrictions on  are as described above and .
Note that this corresponds to the classical definition of the -median problem (resp. -center problem) 
if  is a distance measure on , 
defined as  for 
and  otherwise. Note that the new distance measure 
does not satisfy the triangle inequality and is therefore not a metric.

\section{On signatures of time series}
\seclab{on:signatures}

Before introducing our signatures, we first review similar notions traditionally
used for the purpose of curve compression. 
A simplification of a curve is a curve which is lower in complexity (it has
fewer vertices) than the original curve and which is similar to the original
curve. This is captured by the following standard definitions.  

\begin{definition}\deflab{min:error:simp}
We call a curve  a minimum-error -simplification of  if
for any curve  of at most  vertices, it holds that
.
\end{definition}

\begin{definition}\deflab{min:size:simp}
We call a curve  a minimum-size -simplification of  if  and
for any curve  such that  , it holds that the complexity of  is at least as much as the complexity of .
\end{definition}

The simplification problem has been studied under different names for
multidimensional curves and under various error measures, in domains, such as
cartography~\cite{dp-73,ramer1972iterative}, computational
geometry~\cite{godau1991natural}, and pattern recognition~\cite{pratt2002search}. 
  Often, the simplified curve is restricted to vertices of the input curve and
  the endpoints are kept. However, in our clustering setting, we need to use  
  the more general problem definitions stated above.
  
  Historically, the first minimal-size curve simplification algorithm was a
  heuristic algorithm independently suggested in the 1970's by Ramer and Douglas
  and Peucker~\cite{dp-73,ramer1972iterative} and it remains popular in the area
  of geographic information science until today.  It uses the Hausdorff error
  measure and has running time  (where  denotes the complexity of the
  input curve), but does not offer a bound to the size of the simplified curve.
  Recently, worst-case  and average-case lower bounds on the number of vertices
  obtained by this algorithm were proven by
  Daskalakis~\etal~\cite{daskalakis2010good}.  Imai and Iri~\cite{ii-pac-1988}
  solved both the minimum-error and minimum-size simplification problem under
  the Hausdorff distance by modeling it as a shortest path problem in directed
  acyclic graphs.

Curve simplification using the \Frechet distance was first proposed by
Godau~\cite{godau1991natural}.  The current state-of-the-art approximation algorithm for
simplification under the \Frechet distance was suggested by
Agarwal~\etal~\cite{ahmw-nltcs-05}. This algorithm computes a -approximate
minimal-size simplification in time .  The framework of Imai and Iri
is also used for the streaming algorithm of Abam~\etal~\cite{abh-sals-10} under
the \Frechet distance.  Driemel and Har-Peled~\cite{dh-jydfd-13} introduced the
concept of a \emph{vertex permutation} with the aim of preprocessing a curve for
curve simplification. The idea is that any prefix of the permutation represents
a bicriteria approximation to the minimal-error curve simplification. In
\secref{computing:signatures} we will use this concept to develop improved
algorithms in our setting where the curves are time series.  

For time series, a concept similar to simplification  called 
\emph{segmentation} has been extensively studied in the area of data
mining~\cite{bingham2006segmentation,himberg01,terzi2006efficient}.
The standard approach for computing exact segmentations is to use dynamic
programming which yields a running time of .




\begin{figure}[h]
\centering
\includegraphics{signature.pdf}
\caption{Example of a -signature of a time series}
\figlab{signature}
\end{figure}

We now proceed to introduce the concept of signatures (\defref{signature}).  Our definition
aligns with the work on computing important minima and
maxima in the context of time series compression~\cite{pratt2002search}.
Intuitively, the signatures provide us with the ``shape'' of a time series at
multiple scales.  Signatures have a unique hierarchical structure
(see~\lemref{canonical:signature}) which we can exploit in order to achieve
efficient clustering algorithms. Furthermore, the signatures of a curve
approximate the respective simplifications under the \Frechet distance (see \lemref{apx:min:error:simp}).
\figref{signature} shows an example of a signature. We show several crucial
properties of signatures in \subsecref{properties:signatures}.  Signatures
always exist and are easy to compute (see \secref{computing:signatures}).  In
particular, we show how to compute the -signature of a curve in one pass
in linear time~(\thmref{computing:delta:signature}), and how to preprocess a
curve in near-linear time for fast queries of the signature of a certain
size~(\thmref{compute:signatures}). 

\begin{definition}[-signature]\deflab{signature}
We define the -signature of any 
curve  as follows.
The signature is a curve  defined by a series of values  as the linear interpolation of  in the order of the index , and with the following properties.\\ 
For  the following conditions hold: 
\begin{compactenum}[(i)]
\item (non-degeneracy) if  then ,
\item (direction-preserving)\\if  for :
, and \\if  for : ,
\item (minimum edge length)\\if  then , and \\if
 then ,
\item (range) for : \\if 
 then , and \\if 
 and  then , and \\if 
  and  then , and \\if
 and  then 
\end{compactenum}
\end{definition}

It follows from the properties (i) and (iv) of \defref{signature} that the parameters  for  specify vertices of . 
Furthermore, it follows that the vertex  is either a minimum or maximum on  for .

For a signature  we will simply write \emph{signature  with  vertices} or \emph{signature }, instead of \emph{signature , with vertices , where }. We assume that the parametrization of  is chosen such that , for any .

We remark that while the above definition is somewhat cumbersome, the stated
properties turn out to be the exact properties needed to prove
\thmref{remove:one}, which in turn enables the basic mechanics of our clustering
algorithms.

\subsection{Useful properties of signatures}
\seclab{properties:signatures}

\begin{lemma}\lemlab{fd:signature}
It holds for any -signature  of  that .
\end{lemma}


\begin{proof}
Let  be the series of parameter values of vertices on  that describe . We construct a greedy matching between each signature edge  and the corresponding subcurve  of . Assume first for simplicity that it holds  (i.e. the traversal of the signature is directed upwards at the time) and none of its endpoints are endpoints of . We process the vertices of the subcurve  while keeping a current position  on the edge . The idea is to walk as far as possible on  while walking as little as possible on . We initialize  and match the first vertex of  to . When processing a vertex , we update  to  and match  to the current position  on . By the direction-preserving condition in \defref{signature} and by \obsref{segments} every subcurve of  is matched to a subsegment of  within \Frechet distance . If for the edge  it holds that  (traversal directed downwards) the construction can be done symmetrically by walking backwards on  and . If the first vertex of  is an endpoint of , we start the above construction with the first vertex that lies outside the range . The skipped vertices can be matched to . As for the remaining case if the last vertex of  is an endpoint of , we can again walk backwards on  and  and the case is analogous to the above.
\footnote{
Technically, the constructed matching is not strictly increasing. However, for any  it can be perturbed slightly to obtain a proper bijection. The result is then obtained in the limit.
}
\end{proof}

\begin{lemma}\lemlab{nec:suff}
Let  be a -signature of . Let ,  for , be ranges centered at the vertices of  ordered along . It holds for any curve  if , then  has a vertex in each range , and such that these vertices appear on  in the order of .
\end{lemma}

\begin{proof}
For any  the vertices  and  satisfy that  and . This implies that , . 
Let  be the point matched to  under a matching that witnesses  and , for all . It holds that .
Therefore the curve  visits the ranges ,  and  in the order of the index . Since  the curve  must change direction (from increasing to decreasing or vice versa) between visiting  and . Furthermore,  cannot go beyond  between visiting  and , i.e. there is no point  such that it holds that  and there is an ordering  or . This follows from  being a local extremum on . Therefore, the change of the direction of  takes place in a vertex in . 




For  we use a similar argument. Note that  has to be matched to  by the definition of the \Frechet distance. As before,  has to visit the ranges  and  in this order and it holds that . Either the first vertex of  already lies in , or again  has to change direction and therefore needs to have a vertex in . The case  is symmetric.  
The fact that the points  and  have to be matched to  and  closes the proof.
\end{proof}




The following is a direct implication of \lemref{nec:suff} and the minimum-edge-length condition in \defref{signature}, since  is a -signature and there has to be at least one vertex in each of the ranges centered in vertices which are not endpoints of . 
\begin{corollary}\corlab{nec:suff}
Let  be a signature of  with  vertices and . Then any curve  with  needs to have at least  vertices.
\end{corollary}


\begin{theorem}\thmlab{remove:one}
Let  be a -signature of .
Let  be ranges centered at the vertices of 
ordered along , where  and
. Let  be a curve with
 and let  be a curve obtained by removing 
some vertex  from  with . 
It holds that . 
\end{theorem}

In order to prove this theorem, we have the following lemma, which is a slight variation of the Theorem and it simplifies the case when the \Frechet distance is obtained in the limit.


\begin{lemma}\lemlab{remove:one}
Let  be a -signature of .
Let  be ranges centered at the vertices of 
ordered along , where  and
. Let  be a curve with
 and let  be a curve obtained by removing 
some vertex  from  with . 
For any , it holds that . 
\end{lemma}

We obtain the \thmref{remove:one} from \lemref{remove:one} as follows. 
\begin{proof}[Proof of \thmref{remove:one}]
By the theorem statement, we are given  and , such that 
. By the definition of the \Frechet distance
 it holds for any  that . 
Let  for some  small enough such that: 
\begin{compactenum}[(i)]
\item the -signature of  is equal to the -signature of  (see also \lemref{canonical:signature} for the existence of such a signature), and
\item any vertex  of  satisfying the conditions in \thmref{remove:one} also 
satisfies the conditions of \lemref{remove:one} for .
\end{compactenum}
Now we can apply \lemref{remove:one} using , implying that .
Since this is implied for any  small enough, we have
. 
\end{proof}

\begin{proof}[Proof of \lemref{remove:one}]
Let  denote the witness matching from  to , that maps each point on  to a point on  within distance .\footnote{The existence of such a matching  is ensured since .} Intuitively, we removed  and its incident edges from  by replacing the incident edges with a new ``edge'' connecting the two subcurves which were disconnected by the edge removal. The obtained curve is called . We want to construct a matching  from  to  based on  to show that their \Frechet distance is at most . 

Because of the continuity of the curves, 
we have to describe the ``edge'' connecting disconnected parts. Let  and  be the endpoints of the disconnected components. Let  denote the subcurve by which  and  differ.  In particular,  and  are such that  can be written as a concatenation of a prefix and a suffix curve of : 
 
and  is contained in the open interval . Note that .  Furthermore, it is clear that  consists of two edges with  being the minimum or maximum connecting them. (Otherwise, if  was neither a minimum nor a maximum on , then  is empty. In this case the claim holds trivially.) 

The new ``edge''  consists of three parts: the edge , the point  and the edge . This is illustrated by \figref{vertexremovalexample}.


\begin{figure}[h]\centering
\includegraphics[page=1]{th37image.pdf}\\
\caption{The removal of the vertex  from . The curve  is marked red}
\figlab{vertexremovalexample}
\end{figure}


In the construction of  we need to show that the subcurve , which was matched by  to the missing part, can be matched to some subcurve of , while respecting the monotonicity of the matching. The proof is a case analysis based on the structure of the two curves.  In order to focus on the essential arguments, we first make some global assumptions stated below.  The first two assumptions can be made without loss of generality. We also introduce some basic notation which is used throughout the rest of the proof.
\begin{assumption}
We assume that  is a local minimum on  (otherwise we 
first mirror the curves  and  across the horizontal time axis to obtain
this property without changing the \Frechet distance).
\end{assumption}

Let . 
Let  be the subcurve of  bounded by two
consecutive signature vertices, such that .

\begin{assumption}
We assume that  (otherwise we first reparametrize
the curves  and  with , i.e., reverse the direction of
the time axis, to obtain this property without changing the \Frechet distance;
note that this does not change the property of  being a local
minimum).
\asslab{sign:edge:asc}
\end{assumption}

\begin{assumption}
We assume that neither , nor  (These are boundary cases which
will be handled at the end of the proof).
\asslab{inner:sig:edge}
\end{assumption}

\begin{property}[Signature]
By \defref{signature} we can assume that 
\begin{compactenum}[(i)] 
\item ,\label{item:sig:length}
\item ,\label{item:sig:sj:min}
\item ,\label{item:sig:sj1:max}
\item  for .\label{item:sig:descent}
\item ,\label{item:sig:length:2}
\end{compactenum}
By the general position assumption the minimum  and the maximum
 are unique on their respective subcurves.
\proplab{signature}
\end{property}

\begin{property}[\Frechet]
Any two points matched by  have distance at most  from each other.
In particular, for any two , it holds that
\begin{compactenum}[(i)] 
\item ,\label{item:frechet:dist}
\item ,\label{item:frechet:min}
\item .\label{item:frechet:max}
\end{compactenum} 
\proplab{frechet}
\end{property}

Our proof is structured as case analysis. We consider first the case .
This is illustrated by \figref{shortcutting:case1}. 

\begin{case}[Trivial case]

\caselab{trivial}
\end{case}
\begin{figure}[h]\centering
\includegraphics[page=1]{shortcutting_case1234.pdf}\\
\caption{Example of \caseref{trivial}. The broken part of the matching  is indicated by fat lines.}
\figlab{shortcutting:case1}
\end{figure}
\begin{proof}[Proof of \caseref{trivial}]
As a warm-up exercise we quickly check that the above case is indeed trivial. In
this case, we would simply match  to the subcurve
 and the remaining subcurves  and
 can be matched as done by .\footnote{Note that the constructed
matching is not a bijection. However, for any , it can be perturbed to obtain a 
proper bijection.} Indeed, 

since by the case distinction 

and by \propref{frechet}, 

\end{proof}

\begin{assumption}
We assume in the rest of the proof that  (non-trivial case).
\asslab{nontrivialcase}
\end{assumption}

Intuitively, we want to extend the subcurves of the
trivial case in order to fix the broken matching. The difficulty lies in finding
suitable subcurves which cover the broken part  and
whose \Frechet distance is at most . Furthermore, the endpoints need to
line up suitably such that we can re-use  for the suffix and prefix curves.

The next two claims settle the question, to which extent signature vertices can be
included in the subcurve  for which we need to fix the broken matching. 

\begin{claim}
If  then
.
\claimlab{descent:after:sj1}
\end{claim}
\begin{proof}
We have to prove that


The subcurve  consists of two edges  and   and  is the minimum of the subcurve. For the lower bound we distinguish two cases:  and . 

If , then by \propref{frechet}


If , then since  it holds that

It follows that 
 
as claimed.



Furthermore, by \propref{signature}(\ref{item:sig:length:2}) it follows that

and therefore   .

Now, \propref{signature}(\ref{item:sig:sj1:max}) implies the upper bound:



\end{proof}


\begin{claim}

\claimlab{no:minimum}
\end{claim}
\begin{proof}
For the sake of contradiction, assume the claim is false, i.e. .
We have (by definition)

Furthermore, by definition 
, and
by \propref{signature}(\ref{item:sig:sj:min}),
we have
.
This would imply that 

By the theorem statement
. 
However, by \propref{frechet},



\end{proof}




We now introduce some more notation which will be used throughout the proof.


In the next few claims we argue that these variables are well-defined. In
particular, that  and  always exist in the non-trivial case
(\claimref{x:exists} and \claimref{y:exists}).  Clearly  is
well-defined and by our initial assumptions we have 
(since ). We also derive some bounds along the way,
which will be used throughout the later parts of the proof.

\begin{claim}[Existence of ]
It holds that
\begin{compactenum}[(i)]
\item 
\item 
\item 
\end{compactenum}
\claimlab{x:exists}
\claimlab{x:high}
\claimlab{sj:low}
\end{claim}
\begin{proof}
We first prove part (i) of the claim.
We show that there exist two parameters
 such that 

Since the curve is continuous, this would imply the claim.
Indeed, we can choose  and  .
If  we have 

(since we assume the non-trivial case). 
Otherwise, if   and therefore , then by \propref{signature} and
\propref{frechet},

Thus, in both cases, it holds that 
.

As for , by \claimref{no:minimum} we have  and 
by \propref{frechet} 

It follows by \propref{signature}(\ref{item:sig:sj:min}) that 

and by \propref{signature}(\ref{item:sig:length}) that 

Now, part (ii) of the claim follows directly from the above,
since  is defined as the last point along the prefix subcurve 
 with the specified value and since .
Note that part (iii) we indirectly proved above. 
\end{proof}


\begin{claim}[Existence of ] It holds that
\begin{compactenum}[(i)]
\item 
\item 
\item 
\end{compactenum}
\claimlab{y:exists}
\claimlab{sj1:high}
\end{claim}
\begin{proof}
To prove part (i) of the claim we show that there exist two parameters
,
such that 

We choose  and .
Since we have the non-trivial case, we know 

Now, for , we know that .
By \propref{frechet} and by \propref{signature}(\ref{item:sig:sj1:max})
 
Since the subcurve is continuous, there must be a parameter  which satisfies the claim.
Now, part (ii) of the claim also follows directly, since  is the first point along the suffix subcurve
 with the specified value and since .
Note that part (iii) we just proved above.
\end{proof}

The following claim follows directly from \claimref{x:exists} and \claimref{y:exists}.
\begin{claim}

\claimlab{xy:inside}
\end{claim}

The following claim will be used throughout the proof.
\begin{claim} 

\claimlab{not:so:bad}
\claimlab{max:dist}
\end{claim}

\begin{proof}
We need to show that 

\claimref{y:exists} immediately implies .
On the other hand, by \claimref{xy:inside}, 

By \propref{frechet} and by \propref{signature}(\ref{item:sig:descent}),

\end{proof}




The next two claims (\claimref{frechet:y} and
\claimref{frechet:x}) show that our choice of  and  is suitable for fixing some part
of the broken matching: the subcurve  can be matched
entirely to  and the subcurve  can be matched entirely to
. After that, it remains to match the subcurve . For
this we have the case analysis that follows.


\begin{claim}

\claimlab{frechet:y}
\claimlab{frechet:ya}
\end{claim}
\begin{proof}
By \claimref{x:high} and \claimref{not:so:bad},

On the other hand, by definition of , we have

The last equality of the claim follows directly from the definition of  and from \claimref{y:exists} ( is well-defined).
\end{proof}

\begin{claim}

\claimlab{frechet:x}
\end{claim}
\begin{proof}
We first prove the lower bound on the minimum of the subcurve .
By \claimref{x:high}, and by \propref{frechet}, we have
 
By definition,  is a minimum on , thus

for , which is ensured by \claimref{xy:inside}. 

We now prove the upper bound on the maximum of the subcurve .
Since by \claimref{sj1:high}  and since by 
\propref{signature}(\ref{item:sig:descent}),  may
not descend by more than , it follows that 

By definitions of  and  and by \claimref{max:dist}

By \propref{signature}(\ref{item:sig:sj1:max}) we also have, 

for , which is ensured by \claimref{xy:inside}. 

Together this implies the claim. The last equality of the claim follows directly from the
definition of  and from \claimref{x:exists} ( is well-defined).
\end{proof}

Now we have established the basic setup for our proof. In the following, we
describe the case analysis based on the structure of the two curves  and
.  Consider walking along the
subcurve . At the beginning of the subcurve, we have .  One of the following events may happen
during the walk: either we go above , or we go below 
, or we stay inside this interval. Let  denote
the time at which for the first time one of these events happens. Formally,
we define the intersection function ,

where  is some fixed constant for the case that the suffix curve
 does not contain the value .
We distinguish the following main cases. In each of the cases, we devise a
matching scheme to fix the broken matching. For each case, our construction
ensures that the extended subcurves cover the subcurve 
and that the subcurves line up with suitable prefix and suffix curves, such that
we can always use  for the parts of  and  not covered in the matching
scheme. We need to prove in each case that the \Frechet distance between the
specified subcurves is at most . If this is the case, we call the
matching scheme \emph{valid}.

We have to make further distinction between the case when  and the case . If  holds, the three aforementioned events are described by \caseref{level}, \caseref{upwards} and \caseref{downwards}. If it happens that , it becomes more complicated to repair the matching. This is discussed in \caseref{matroska}.


\begin{case}[ stays level]
.
\caselab{level}
\end{case}
\begin{figure}[h]\centering
\includegraphics[page=2]{shortcutting_case1234.pdf}\\
\caption{Example of \caseref{level}. The broken part of the matching  is indicated by fat lines.}
\figlab{shortcutting:case2}
\end{figure}
\caseref{level} is the simplest case. We intend to use the following matching scheme:


\begin{proof}[Proof of \caseref{level}]
\claimref{frechet:x} implies that the \Frechet distance between 
and  is at most .
\claimref{frechet:y} implies that the \Frechet distance between 
and  is at most .
Finally, by our case distinction and by \claimref{not:so:bad}

Therefore, also the \Frechet distance between  and  is at most
.
\end{proof}

\begin{case}[ tends upwards]
 and 
\caselab{upwards}
\end{case}
\begin{figure}[h]\centering
\includegraphics[page=3]{shortcutting_case1234.pdf}\\
\caption{Example of \caseref{upwards}. The broken part of the matching  is indicated by fat lines.}
\figlab{shortcutting:case3}
\end{figure}
 In \caseref{upwards}, let  and . We intend to use
 the following matching scheme:

(Note that if , then the last line of the above matching scheme is simply dropped.) 

\begin{proof}[Proof of \caseref{upwards}]
We first argue that  exists. To this end, we show that there exist
two parameters , such that

We choose  and .
Note that .
Now, by \propref{frechet},
 
Since we are assuming the non-trivial case, 
 
Thus, since  is continuous,  must exist and it holds that .
It remains to prove that the matching scheme is valid.
Since ,
\claimref{frechet:x} implies that the \Frechet distance between 
and  is at most .
\claimref{frechet:y} implies that the \Frechet distance between 
and  is at most .
For the last two lines of the matching scheme we distinguish two cases.
If , then  and we need to prove that

By our case distinction and by \claimref{not:so:bad}

Since , this implies the validity of the matching.
Otherwise, if , then  and we need to prove 
that 

This can be derived as follows. On the one hand, by \propref{frechet}, since 

On the other hand, by the definition of  and since 

\end{proof}

\begin{case}[ tends downwards]
 and .
\caselab{downwards}
\end{case}
\begin{figure}[h]\centering
\includegraphics[page=4]{shortcutting_case1234.pdf}\\
\caption{Example of \caseref{downwards}. The broken part of the matching  is indicated by fat lines.}
\figlab{shortcutting:case4}
\end{figure}
In \caseref{downwards}, let . 
We intend to use the following matching scheme:


\begin{proof}[Proof of \caseref{downwards}]
Clearly,  exists in the non-trivial case, since  

and .

We prove the validity of the matching scheme line by line.
Note that by definition .
For the first matching:
by the definition of  and by \propref{frechet},

The validity of the second matching follows from \claimref{frechet:y} since .
For the third matching:
By our case distinction and by \claimref{not:so:bad}

As for the last matching, since  and since by our case distinction
, \claimref{frechet:x} implies

\end{proof}





\begin{case}[The matroska case] .
\caselab{matroska}
\end{case}
\caseref{matroska} seems to be the most difficult case to handle. However, we have
already established a suitable set of tools in the previous cases. We devise an iterative
matching scheme and prove an invariant (\claimref{frechet:xa}) to verify that the \Frechet distance of
the subcurves is at most .
We first define , , 
, and .
Now, for  let 

We describe the intended matching scheme, beginning with the following subcurves:

where the last two matchings are repeated while incrementing  (starting with
).   After each iteration, we are left with the
unmatched subcurves  and .  
We would like to complete the matching with the following scheme


This is indeed possible if 

The above is the equivalent to the trivial case (\caseref{trivial}).  
We first prove correctness in this case (\caseref{matroska}(\ref{item:trivial})). To this end, we extend
\claimref{frechet:x} as follows. Note that this claim will also be used in the non-trivial cases that follow.

\begin{claim}

\claimlab{frechet:xa}
\end{claim}
\begin{proof}
Recall that  by
\claimref{xy:inside}.
By definition of ,

By \propref{signature}(\ref{item:sig:descent}) and the definitions of  and , we also have that

and by \propref{signature}(\ref{item:sig:sj1:max}), we also have that

This proves the first part of the claim. For the second part we use the definition of , which implies

\end{proof}



\begin{claim}[Correctness of \caseref{matroska}(\ref{item:trivial})]
If for some value of , it holds that 
 then
the above matching scheme is valid.
\claimlab{trivial:subcase}
\end{claim}

\begin{proof}
By \claimref{frechet:x}, \claimref{frechet:ya} and \claimref{frechet:xa} the
iterative part of the matching scheme is valid.
It remains to prove the validity of the last two matchings.
By \claimref{frechet:y},

Since , this implies that the \Frechet distance
between  and  is at most .
As for the other matching, we have by our case distinction 

while (by \propref{frechet}) the matching  testifies that

Since , the above implies

Note that the proof holds both if  or .
\end{proof}

From now on, we will assume the non-trivial (sub)case.  Our matching scheme is
based on a stopping parameter , which (intuitively) depends on whether
 matched some point on the missing subcurve  to a signature
vertex  of .

\begin{definition}(Stopping parameter )
If  then let  be the minimal value of 
satisfying  
Otherwise, let  be the minimal value of  such that 
\deflab{a:stop}
\end{definition}

\begin{claim}
The stopping parameter  (\defref{a:stop}) is well-defined 
and the matching scheme is valid for  
\end{claim}
\begin{proof}
\newcommand{\Astall}{\widehat{a}}
We first argue that there must be a value of  such that 
 for any .
Recall that by our initial assumptions,  and 
thus . As a consequence, \claimref{y:exists} 
testifies that in the non-trivial case, the point  exists and is well-defined.
We defined  and for  we defined

Since by \claimref{sj1:high}, ,
there must be a value of  such that 

Let this value of  be denoted .
In this case, it follows by definition that , 
which implies that  and .
This has the effect that  for any .

Now, if , then the above analysis implies that  is well-defined.
However, if , we defined  to be the minimal value of 
such that . Now it might happen that .
In this case, there exists no value of  such that , thus  does not exist.
We can reduce this case to the trivial case (\caseref{matroska}(\ref{item:trivial})) as follows. 
By \claimref{sj1:high},  and by
\propref{signature}(\ref{item:sig:sj1:max}),  must be a maximum on . Thus, by definition of , we would have 
, which is
\caseref{matroska}(i) (the trivial case).
Thus, also in the non-trivial case,  is well-defined.

The validity of the matching scheme  for  follows from \claimref{frechet:x}, \claimref{frechet:ya} and \claimref{frechet:xa}.
\end{proof}

It follows that the iterative part of the matching scheme is valid for  
Now we are left with the unmatched subcurves  and
 and we have to complete the matching scheme.
In order to set up a case analysis with a similar structure as before, we define 


\begin{table}[h]
\center
\begin{tabular}{|c|m{7.5cm}|m{5cm}|}
\hline
case & definition & intended matching\\
\hline
\ref{case:matroska}(\ref{item:trivial}) & 

&
\newline
\\
\hline
\ref{case:matroska}(\ref{item:level}) & 

&
\newline
 \\
\hline
\ref{case:matroska}(\ref{item:upwards}) & 
 and \newline
 
&
This case can be reduced\newline
to \caseref{matroska}(i) 
\\
\hline
\ref{case:matroska}(\ref{item:downwards}) & 
 and\newline
 
&
\newline
\newline

\\
\hline
\ref{case:matroska}(\ref{item:upwards:sig}) &
 and \newline
\newline
For the matching scheme, let\newline 
 \newline
 \newline
 
& 
\newline
\newline
\newline
\newline

\\
\hline
\ref{case:matroska}(\ref{item:downwards:sig}) & 
 and \newline

&
\newline
\newline
\\
\hline
\end{tabular}
\caption{Subcases for \caseref{matroska}:
(\ref{item:trivial}) trivial case (\claimref{trivial:subcase}),
(\ref{item:level})  stays level, 
(\ref{item:upwards})  tends upwards,
(\ref{item:downwards})  tends downwards,
(\ref{item:upwards:sig}) unmatched signature vertex and  tends upwards,
(\ref{item:downwards:sig}) unmatched signature vertex and  tends downwards.
Examples of these cases are shown in \figref{shortcutting:case5i} and
\figref{shortcutting:case5v}.
}
\tablab{matroska:subcases}
\end{table}


The exact case distinction is specified in \tabref{matroska:subcases}:
\begin{inparaenum}[(i)]
\item trivial case (see \claimref{trivial:subcase}),\label{item:trivial} 
\item  stays level, \label{item:level}
\item  tends upwards,\label{item:upwards}
\item  tends downwards,\label{item:downwards}
\item unmatched signature vertex and  tends upwards, \label{item:upwards:sig}
\item unmatched signature vertex and  tends downwards. \label{item:downwards:sig}
\end{inparaenum}
It remains to prove that the case analysis is complete and to prove correctness
in each of these subcases.

\begin{claim}
The case distinction of subcases of \caseref{matroska} (\tabref{matroska:subcases}) is complete. 
\claimlab{matroska:complete}
\end{claim}
\begin{proof}
We assume that we are not in the trivial case
\caseref{matroska}(\ref{item:trivial}). 
If  (also ) 
we get one of
\caseref{matroska}(\ref{item:level})-(\ref{item:downwards}).
Otherwise we have  (also ). In this case, we get one of  
\caseref{matroska}(\ref{item:upwards:sig})-(\ref{item:downwards:sig}).
In the following, we argue that, indeed, if the subcurve of  specified by the parameter interval 
 contains the signature vertex at , it must be that 

and thus,  and  is be one of 
.
Assume that , i.e., the next signature vertex after
 is not the last signature vertex. In this case, by
\propref{signature} and \propref{frechet}, we have

Since  is continuous, this implies that there must exist a point 
with  and .
Now, assume that . In this case, we have by the theorem statement
that .
It must be that either 
(in which case we can apply the above argument), or 
. The second
case is not possible since by \claimref{max:dist} and by \propref{frechet}
we have 

Here,  follows from  
in \caseref{matroska}(\ref{item:upwards:sig})-(\ref{item:downwards:sig})
and the fact that 

by our initial assumptions.
\end{proof}

\begin{proof}[Proof of \caseref{matroska}(\ref{item:level})]


By \claimref{frechet:y} the \Frechet distance between  and
 is at most .
By our case distinction,  
 
since by \claimref{max:dist},

(Note that  always exists since  by
\claimref{xy:inside}.) 
This implies that also the second matching is valid.
\end{proof}

\begin{proof}[Proof of \caseref{matroska}(\ref{item:upwards})]
We can reduce this case to \caseref{matroska}(\ref{item:trivial}) (the trivial case) as
follows.
By our case distinction,  . Let  be the
maximal value of  such that .
By \propref{frechet} 
it must be that . Thus . This holds since for any ,  goes upwards in , then intersects  downwards and goes upwards again in . By our case distinction, . Thus, 

\end{proof}

\begin{proof}[Proof of \caseref{matroska}(\ref{item:downwards})]
In this case, we rollback the last two matchings of the iterative matching
scheme and instead end with . Thus, we are left with the 
unmatched subcurves  and .  
We complete the matching scheme as defined in \tabref{matroska:subcases}.
The validity of the first matching follows directly from
\claimref{frechet:y}, since .
By the definition of  and our case distinction, 

This proves validity of the second matching.
\claimref{frechet:xa} implies the validity of the last matching.
\end{proof}

\begin{figure}\centering
\includegraphics[page=1]{shortcutting_case5.pdf}\\
\vspace{\baselineskip}
\includegraphics[page=2]{shortcutting_case5.pdf}\\
\vspace{\baselineskip}
\includegraphics[page=3]{shortcutting_case5.pdf}\\
\vspace{\baselineskip}
\includegraphics[page=4]{shortcutting_case5.pdf}\\
\caption{Examples of
\caseref{matroska}(\ref{item:trivial})-(\ref{item:downwards}).  The broken part of the matching
 is indicated by fat lines.}
\figlab{shortcutting:case5i}
\end{figure}

\begin{figure}\centering
\includegraphics[page=5]{shortcutting_case5.pdf}\\
\vspace{\baselineskip}
\includegraphics[page=6]{shortcutting_case5.pdf}\\
\vspace{\baselineskip}
\includegraphics[page=7]{shortcutting_case5.pdf}\\
\vspace{\baselineskip}
\includegraphics[page=8]{shortcutting_case5.pdf}\\
\caption{Examples of
\caseref{matroska}(\ref{item:upwards:sig})-(\ref{item:downwards:sig}).  The broken part of the matching
 is indicated by fat lines.}
\figlab{shortcutting:case5v}
\end{figure}

We have now handled
\caseref{matroska}(\ref{item:trivial})-(\ref{item:downwards}). Examples of these
cases are shown in \figref{shortcutting:case5i}.
We now move on to prove correctness of the remaining cases 
\caseref{matroska}(\ref{item:upwards:sig}) and \caseref{matroska}(\ref{item:downwards:sig}). 



\begin{proof}[Proof of \caseref{matroska}(\ref{item:upwards:sig})]
Observe that in this case , as in \caseref{upwards}.
Therefore,  and  are the same as in \caseref{upwards} and must exist.
We argue that  must also exist. Indeed, we can derive 
,
as follows. Recall that by our case distinction .
By \propref{frechet}, it follows that


Now we need to prove the validity of the matching scheme.
The first line follows from \claimref{frechet:ya}.
For the second line we need to prove that

The upper bound follows from \propref{signature}(\ref{item:sig:sj1:max}).
As for the lower bound, by the definition of the stopping parameter,

(as we just proved above).
By \claimref{descent:after:sj1}, 

By our case distinction and by \propref{frechet},

The validity of the third matching is implied by \claimref{frechet:y}.
For the last two matchings we can apply the respective part of the proof of
\caseref{upwards} verbatim.
\end{proof}

\begin{proof}[Proof of \caseref{matroska}(\ref{item:downwards:sig})]
The validity of the first matching follows from \claimref{frechet:y} and
since .
By our case distinction,

Thus, also the second matching is valid.
For the last matching we need to prove that

Again, as in \caseref{matroska}(\ref{item:upwards:sig}), it holds that 

By our case distinction and by \propref{frechet} 

This also implies that , by
\propref{signature}(\ref{item:sig:length}). 
Thus, by \propref{signature}(\ref{item:sig:sj1:max}), we conclude

Together this implies the validity of the last matching.
\end{proof}

We now proved correctness of the last two cases
\caseref{matroska}(\ref{item:upwards:sig}) and
\caseref{matroska}(\ref{item:downwards:sig}).  Examples of these cases are shown
in \figref{shortcutting:case5v}.

It remains to prove the boundary cases, which we have ruled out so far by
\assref{inner:sig:edge}. There are three boundary cases:
\begin{compactenum}[(B1)]
\item  and ,
\item  and ,
\item  and .
\end{compactenum}

To prove the claim in each of these cases, we can use the above proof verbatim
with minor modifications.  Note that in the proof, we used  in its function
as the minimum on the signature edge , resp., we used
 in its function as the maximum
on this edge.  Thus, let
 

\begin{claim}
In each of the cases (B1), (B2) and (B3), it holds that
 and .
\claimlab{pi:inside}
\end{claim}
\begin{proof}
By the theorem statement and by \defref{signature}, it holds that

i.e., the removed vertex  lies very far from the endpoints of
the curve . At the same time, by \defref{signature},
in case ,
 
and, in case ,

By the direction-preserving property of \defref{signature} and by
\propref{frechet}, this implies that .
In the cases where , this implies
 
therefore, by the above, .
Similarly in the cases, where , 
we can derive that .
In each of the cases (B1), (B2) and (B3), this implies the second part of the claim.
\end{proof}

We replace \propref{signature} with the following property. 
\begin{property}[Signature (boundary case)]
\mbox{}
\begin{compactenum}[(i)]
\item ,
\item  
      (if , then ),
\item  
      (if , then ),
\item  for ,
\item if , then .
\end{compactenum}
\proplab{signature:boundary}
\end{property}

\propref{signature:boundary}(\ref{item:sig:sj:min}), (\ref{item:sig:sj1:max}), 
(\ref{item:sig:descent}), and (\ref{item:sig:length:2}) hold by \defref{signature}.
\propref{signature:boundary}(\ref{item:sig:length}) follows from
\claimref{pi:inside}.

Instead of \claimref{descent:after:sj1} we use the claim
\begin{claim}
If  then
.
\end{claim}

Instead of \claimref{no:minimum} we use the claim
\begin{claim}
.
\end{claim}

Now, the theorem follows in the boundary cases (B1), (B2) and (B3), by replacing
 with  and replacing  with .




This closes the proof of \lemref{remove:one}
\end{proof}




\section{-center}

\begin{algorithm}[h]\alglab{candidate:generator:center}
 \KwData{curves , parameters , }
 \KwResult{candidate set }
 \caption{Generate candidates for -center from signature vertices}
For each , let  be the vertex set of its -signature computed by \algref{low:pass}\; 
Compute the union  of the intervals  for \; 
\eIf{ }
{Return the empty set\;}
{Discretize  with resolution , thereby generating a set of vertices~\;
Return all possible curves consisting of  vertices from \;}
\end{algorithm}

\begin{lemma}\lemlab{candidate:generator:center}
Given a set of curves  and parameters
, and , then \algref{candidate:generator:center}
generates a set of candidate solutions 
of size at most .
Furthermore, if , then the generated set contains 
 candidates  with 

\end{lemma}

\begin{proof} 
Let   denote an optimal solution for  and  
let  denote the vertices for each cluster
center. 
Consider the union of intervals

\lemref{nec:suff} implies that  contains all elements of , the
signature vertices computed by \algref{candidate:generator:center}.
Now consider the dual statement, namely, whether the vertices  are contained in the set  computed by the algorithm.
If there exists a  which is not contained in , then
\thmref{remove:one} implies that we can omit  from the solution 
while not increasing the cost beyond .
Therefore, let  denote the solution 
where all vertices that lie outside  have been omitted.
Clearly,  contains all remaining vertices of cluster centers in . 
Therefore,  must contain 
candidates  with 

Note that  consists of at most  intervals and has measure at most
.
Therefore, the measure of  can be at most 

In the worst case a signature vertex lies at each boundary point of .
Furthermore,  consists of at most  
intervals, since each interval has measure at least .
\end{proof}



\begin{theorem}\thmlab{k:l:center:main}
Let  and  be given constants. 
Given a set of curves 
we can compute a -approximation to 
in time . 
\end{theorem}

\begin{proof}
We use 
\algref{phase1}
described in \secref{cf:approx:uni} to compute a
constant-factor approximation.  We obtain an interval
 which contains
 and such that
 by \thmref{cf:approx:center:main}.
We can now do a binary search in this interval.  
In each step of the binary
search, we apply \algref{candidate:generator:center} to  a constant number
of times and evaluate every candidate solution. 
More specifically, if we apply \algref{candidate:generator:center} to  with parameters
 and , 
by \lemref{candidate:generator:center}
we gain the following knowledge:
\begin{compactenum}[(i)]
\item Either , or
\item  and 
we have computed a solution with cost at most .
\end{compactenum}
In both cases, the outcome is correct. Since we want to take an exact decision 
during the binary search, we simply call the procedure twice with 
parameters  and
. 
Now there are three possible outcomes:
\begin{compactenum}[(i)]
\item Either , or
\item , or
\item .
\end{compactenum} 
So either we can take an exact decision and proceed with the
binary search, or we obtain a -approximation to the solution and we
stop the search.



By \lemref{candidate:generator:center} we know that the size of the
candidate set  is 
(where the constant depends on  and .

One execution of \algref{candidate:generator:center} takes  time for
computing the  signatures (using \algref{low:pass}) and 
 time for generating the candidate set.
Evaluating one candidate solution (consisting of  centers from the candidate set) 
takes  \Frechet distance computations, where one
\Frechet distance computation takes time 
using the algorithm by Alt and Godau
\cite{ag-cfdbt-95}.  The number of binary search steps depends only on the constant
 and so is , which implies the running time.
\end{proof}

\section{-median}

In this section we will make use of a result by Ackermann \etal \cite{abs-cm-10} for 
computing an approximation to the -median problem under an arbitrary dissimilarity
measure  on a ground set of items , i.e. a 
function that satisfies , iff . The result roughly says that
we can obtain an efficient -approximation algorithm for the -median
problem on input , if there is an algorithm that given a random sample 
of constant size returns
a set of candidates for the -median that contains with constant probability (over the
choice of the sample) a -approximation to the -median.

We restate the sampling property defined by Ackermann \etal~(\cite{abs-cm-10},Property 4.1).

\begin{definition}[sampling property]
\deflab{sampling:property}
We say a dissimilarity measure D satisfies the (weak)
-sampling property iff there exist integer constants
 and  such that for each 
of size  and for each uniform sample multiset  of size  a set 
 of size at most  can be computed satisfying

Furthermore,  can be computed in time depending on  and  only.
\end{definition} 

It is likely that the sampling property
(\defref{sampling:property}) does not hold for the \Frechet distance for
arbitrary value of . We will therefore prove a modified sampling
property, which allows the size of the sample to depend on .

The following lemma intuitively says that curves that lie far away from a candidate median 
have little influence on the shape of the candidate median.

\begin{lemma}\lemlab{omit:far:away}
Given a set of  curves  and a polygonal curve , it holds that 
 
for a curve  obtained from  by omitting any subset of vertices lying outside 
the following union :
where the  are sorted in increasing order of  and where
 is the -signature of . \end{lemma}

\begin{proof}
By \lemref{nec:suff}, the curve  has a vertex in each range centered at the vertices 
of 
. These will not be omitted, therefore it is ensured that
 has at least 2 vertices, i.e. defines a curve.
By  \thmref{remove:one},  it holds that
 for the curves 
with , that is, for the curves that lie close
to~.
We now argue using the triangle inequality that the distances to the curves
that lie further away are only altered by a factor of at most .
Consider any index , such that .
By the triangle inequality, it holds that 




Therefore,


\end{proof}

The following lemma is in similar spirit as \lemref{omit:far:away}. We prove that the
basic shape of a candidate median can be approximated based on a constant-size
sample.

\begin{lemma}\lemlab{omit:low:prob}
There exist integer constant  such that given a
set of curves   and a curve  for each uniform
sample multiset  of size 
it holds that 
 
for a curve  obtained from  by omitting any subset of vertices lying outside 
the following union :

where the  are sorted in increasing order of  and where
 is the -signature of .
\end{lemma}
\begin{proof}
If all vertices of  are contained in , then  and the claim is implied.
However, this is not necessarily the case. In the following, we consider a fixed vertex  
and we prove that it is either contained in  with sufficiently high probability or
ignoring it will not increase the cost of a solution significantly. 

For this purpose, let  be the subset of curves  with 
 
If any curve of  is contained in our sample , then  is
contained in .

We distinguish two cases.  If  is large enough then  is
contained in  with high probability, or we argue that the total change in cost resulting 
from omitting  from  will be small.  
We will first argue that for all  with  and with  we obtain for our choice of  that at least one element of  is contained in . Indeed, we have

We use the union bound, to estimate the probability that this event fails for at least one of the 
sets  in question. We choose the parameter  large
enough, such that it holds for the failure probability that

For this, it suffices to choose 

to obtain that with probability at least  for all , simultaneously,
we have that at least one element of  is in , if .

Now consider the set of curves .
By our previous considerations we have that with probability at least ,
. We will assume in the following
that this event happens.

Let  denote the curve obtained from  by removing all vertices from
, which is equivalent to removing all vertices  with . 

In the following, let  be the set of input curves that are
contained in one of the sets in .
By \thmref{remove:one}, it holds for any curve , that 
, i.e., 
their distances do not increase 
beyond 
by the removal. 
Let  be the curve of this set with minimal distance to 
(i.e. with smallest index ).  
Since at least half of the input curves have to lie within a radius of  from  (two times the average distance of the input curves to )
and since the union of the sets from  has size less than  (with probability at least
), this implies that .
Therefore,

\end{proof}

\subsection{Generating Candidate Solutions}

Our next step is to define an algorithm that generates a set of candidate curves from the 
sample set. 

\begin{algorithm}[h]\alglab{candidate:generator}
\KwData{curves , parameters , }
 \KwResult{candidate set }
 \caption{Generate candidates for -median from signature vertices}
For each , let  be the vertex set of the 
signature of size at most ~(\thmref{compute:signatures})\; 
Compute the union  of the intervals  for \; 
Discretize  with resolution , thereby generating a set of vertices~\;
Return all possible curves consisting of  vertices from
\;
\end{algorithm}

We prove some properties of \algref{candidate:generator} and of the candidate
set generated by it. This proof serves as a basis for the proof of the sampling
property in \thmref{modified:sampling:prop}. 


\begin{lemma}\lemlab{candidate:generator}
Given a set of curves  and parameters
, and , with 
, where  denotes an optimal -median of . 
There exist  with
 
and 
\algref{candidate:generator} computes a set of candidates 
of size  
which contains an element , such that 

\end{lemma}

\begin{proof}
Let  denote the input curves in the increasing order of their
distance denoted by . For every ,
consider its -signature denoted by .   
By \lemref{nec:suff}, each vertex of
 lies within distance  to a vertex of some signature
 otherwise we can omit it by \thmref{remove:one}.  Hence, there must be a solution where
 has its vertices in the union of the intervals. 
 

Since  could be very large, we cannot cover this entire region with candidates. 
Instead, we consider the following union of intervals: 

with .
Now, let  be the median obtained from  by
omitting all vertices which do not lie in . 
\lemref{omit:far:away} implies 

Clearly, the generated set contains a curve 
which lies within \Frechet distance  of . 
Indeed, by \lemref{canonical:signature}, the vertices of  are contained in
the set of signature vertices computed by \algref{candidate:generator}, since 
\corref{nec:suff} implies that 
.  
If a signature of size  does not
exists, then by the general position assumption, there must be a signature of
size .
The algorithm sets . 
Therefore, the generated candidate set covers the region  with resolution~.
\end{proof}


Before we prove the modified sampling property we prove the following two easy
lemmas.


\begin{lemma}\lemlab{markov}
Let . Given a set of curves , for each uniform sample multiset
 it holds that 

where  is an optimal median of .
\end{lemma}

\begin{proof}
It holds that 

Since  is a nonnegative random variable we can apply Markov's inequality and obtain 

which implies the claim.
\end{proof}

\begin{lemma}\lemlab{diam:S}
Let . Given a set of curves , for each uniform sample multiset
 of size at least  it holds that 

where  denotes an optimal -median of .
\end{lemma}

\begin{proof}
We analyze two cases. For the first case, assume that there exists a curve , such that 
 
where .
That is, a large fraction of   lies within a small ball far away from the
optimal center. We let

and we claim that  has size at most .
Assume the opposite for the sake of contradiction. In this case, it follows by the triangle inequality that

This would imply that  is not optimal. 
We analyze the event that at least one curve of  lies within
\Frechet distance  of  and at least one curve lies further than  from
. If this event happens, then again by the triangle inequality 

Clearly, it holds for the th sample point , that

Using this, we can show that  samples suffice to ensure that this
event happens with probability of at least .

Now, assume the second case that no such  exists. Let  be the first
sample point and let  be its minimum-error -simplification
(\defref{min:error:simp}). We need to prove the claim in the case that

since  is lower-bounded by
. 
By the case analysis, it holds that 
 
for . Therefore, it holds for each of the remaining sample points , for , that

In case this event happens, it holds by the triangle inequality

The analysis of this event is almost the same as in the first case. In this
case, a total number of  samples suffices to ensure that this
event happens with probability of at least .
\end{proof}


We are now ready to prove the modified sampling property.
\begin{theorem}\thmlab{modified:sampling:prop}
There exist  integer constants  and
 such that given a set of curves
  for a uniform
sample multiset  of size 
we can generate a candidate set  of size  satisfying
 
Furthermore, we can compute  in time depending on  and  only.
\end{theorem}

\begin{proof}
Let  and .
Let  denote an optimal -median of  and let  denote an optimal -median of .  
We use the algorithm described in \secref{cf:approx:uni} to compute a
constant-factor approximation to  and obtain an interval 
 which contains
 and by \thmref{cf:approx:median:main} it holds
that .
We apply \algref{candidate:generator} to  with parameters

and obtain a set .

With the help of \lemref{omit:low:prob}, we can now adapt the proof of 
\lemref{candidate:generator} to our probabilistic setting.
Let  be an optimal 
-median of .
Let  denote the input curves in the increasing order of their
distance denoted by . For every ,
consider its -signature denoted by .   
By \lemref{nec:suff}, each vertex of
 lies within distance  to a vertex of some signature
, otherwise we can omit it by \thmref{remove:one}.  Hence, there must
be a -median whose vertex set is contained in the union of
the intervals 
 
Let this solution be denoted . 


So, consider the following union of intervals: 


Let  be the median obtained from  by
omitting all vertices which do not lie in . 
\lemref{omit:low:prob} implies 

if we choose .

So, now consider the following union of intervals: 

where .
Let  be the median obtained from  by
omitting all vertices which do not lie in .
\lemref{diam:S} implies that if , then
it holds with a probability of at least  that

\algref{candidate:generator} sets , therefore  

Thus, we can apply \lemref{omit:far:away} and obtain 

Therefore, with probability , the generated set
 contains a curve 
which lies within \Frechet distance  of . 

\lemref{markov} implies that with probability at least  it holds that 

Thus, with the same probability it holds that


We conclude that with probability  (union bound)
there exists a candidate in  such that


Furthermore, by \lemref{candidate:generator} the size of  is bounded as follows

where  is a sufficiently large constant.
\end{proof}







The following theorem follows from Ackermann \etal~\cite{abs-cm-10} (\thmref{modified:sampling:prop}).
For this purpose, recall that the analysis of Ackermann \etal does not require the distance function 
to satisfy the triangle inequality. Therefore we can adopt the -median formulation from 
\secref{problem} which uses the dissimilarity measure  on the set . 
To achieve the running time we use Alt and Godau's algorithm~\cite{ag-cfdbt-95} for distance computations.




\begin{theorem}\thmlab{k:l:median:main}
Let  be constants.
Given a set of curves , there exists an algorithm that with
constant probability returns a -approximation to the
-median problem for input instance , and that
has running time .
\end{theorem}



\section{Constant-factor approximation in various cases}
\seclab{cf:approx:uni}

It is not difficult to compute a constant-factor approximation for our problem.
We include the details for the sake of completeness. Our algorithm first
simplifies the input curves before applying a known approximate clustering algorithm
designed for general metric spaces.  Note that an approximation scheme which
first applies clustering and then simplification does not yield the same
running time, since the distance computations are expensive.

\begin{algorithm}[h]\alglab{phase1}
 \KwData{curves , parameters }
 \KwResult{cluster centers  and cost }
 \caption{Constant-factor approximation for -clustering}
For each  we compute an approximate minimum-error
-simplification  (\lemref{apx:min:error:simp})\;
We apply a known approximation algorithm for clustering in general metric spaces
on  (i.e.,
Gonzales' algorithm~\cite{Gonzalez1985} for -center and
Chen's~algorithm~\cite{c-kmc-09} for -median)\; 
We return the resulting cluster centers
 with approximate cost 

for -center and -median, respectively\;
\end{algorithm}

\begin{lemma}\lemlab{c:f:approx}
The cost  (resp., ) and solution  computed by \algref{phase1} constitute a
-approximation to the
-center problem (resp., the
-median problem), where  is the approximation
factor of the simplification step and  is the approximation factor of
the clustering step. 
\end{lemma}


\begin{proof}
We first discuss the case of -center. The
-median will follow with a simple modification.
First, we have that 


Now, let  be the optimal cost for a solution to the -center problem
 for .
It holds that 

since  is lower bounded by the distance of any input time series to its optimal
-simplification and this is the minimal \Frechet distance the time series can have to any curve
with at most  vertices. 
Now, consider an optimal solution  with cost . We can relate it to
 as follows. In the following, let  be the center of this
optimal solution which is closest to .

It follows that . 

\end{proof}

\begin{theorem}\thmlab{cf:approx:center:main}
Given a set of  curves  and parameters ,
we can compute an -approximation to 
and a witness solution in time .
\end{theorem}
\begin{proof}
The theorem follows by \lemref{c:f:approx} and by setting .  We
use \lemref{apx:min:error:simp} to obtain a -approximate simplification for
each curve.  Then, we use Gonzales' algorithm which yields a -approximation
for the simplifications. Each distance computation takes  time using Alt and Godau's algorithm~\cite{ag-cfdbt-95}.
\end{proof}


\begin{theorem}\thmlab{cf:approx:median:main}
Given a set of  curves  and parameters ,
we can compute a -approximation to 
and a witness solution in time .
\end{theorem}
\begin{proof}
The theorem follows from \lemref{c:f:approx} and by setting  and
.  We use \lemref{apx:min:error:simp} to obtain a -approximate
simplification for each curve.  Then, we use the algorithm of
Chen~\cite{c-kmc-09} to solve the discrete version of the -median problem on
the simplifications.  Each distance computation takes  time using Alt and Godau's algorithm~\cite{ag-cfdbt-95}.
Chen's algorithm yields an -approximation for the discrete problem, where
the centers are constrained to lie in .  Since the \Frechet distance
satisfies the triangle inequality, this implies a -approximation for our
problem. Therefore, setting  yields a correct bound.
\end{proof}

These results can be easily extended to a -means variant
of the problem, as well as to multivariate time series, using known
simplification algorithms, such as the algorithm by Abam \etal~\cite{abh-sals-10}. 



\section{On computing signatures}
\seclab{computing:signatures}

In this section we discuss how to compute signatures efficiently. Our signatures have a unique hierarchical structure as testified by \lemref{canonical:signature}.  Together with the concept of vertex permutations~(\defref{vtx:permutation}) this allows us to construct a data structure, which supports efficient queries for the signature of a given size~(\thmref{compute:signatures}).  If the parameter  is given, we can compute a signature in linear time using \algref{low:pass}. Furthermore, we show that our signatures are approximate simplifications in \lemref{apx:min:error:simp}. 


\begin{lemma}\lemlab{canonical:signature}
Given a polygonal curve  with vertices in general position, there exists a
series of signatures  and corresponding parameters
, such that 
\begin{compactenum}[(i)]
\item  is a -signature of  for any , 
\item the vertex set of  is a subset of the vertex set of ,
\item  is the linear interpolation of  and .
\end{compactenum}
\end{lemma}

\begin{proof}
We set  and obtain the desired series by a series of edge contractions.  Clearly,  is a minimal -signature for . We now conceptually increase the signature parameter  until a smaller signature is possible. In general, let  be the series of parameters that defines . Let 

We contract the edge where the minimum is attained to obtain . By the general position assumption, this edge is unique. If the edge is connected to an endpoint, we only remove the interior vertex, otherwise we remove both endpoints of the edge. We now argue that the obtained curve  is a -signature. 

Let  be the contracted edge and assume for now that .  We prove the conditions in \defref{signature} in reverse order.  Observe that 

since otherwise the contracted edge would not minimize the expression in (\ref{min:edge}).  By induction the \emph{range condition} was satisfied for  and by the statement in (\ref{range}) it cannot be violated by the edge contraction.

The contracted edge was the shortest interior edge of  and by construction we have that

Therefore, the \emph{minimum-edge-length} condition is
also satisfied for . 

Since , we have to prove the \emph{direction-preserving} condition only for the newly established edge  of . For any  it holds that . Indeed, by induction, the range condition held true for the contracted edge and by Equation (\ref{edge:length}) its length was . For any  the direction-preserving condition holds by induction, and the same holds for . The remaining case is  where the interval   crosses the boundary of at least one of the edges. In this case, the direction-preserving condition holds by the range property of  and by Equation~(\ref{edge:length}).

It remains to prove the \emph{non-degeneracy} condition. Assume for the sake of contradiction that it would not hold, i.e., either that , or that  . Since the two cases are symmetric, we only discuss the first one and the other case will follow by analogy. Then, (\ref{range}) would imply that , which contradicts the range property of .

So far we proved the conditions of \defref{signature} in the case that an interior edge is being contracted.  Now, assume that  and again let the contracted edge be  (the case  is analogous). Again, we prove the conditions in reverse order.  By induction, the range condition is satisfied for the first two edges of , as well as the non-degeneracy condition.  Since it holds for the length of the second edge that , it must be that  spans the range of values  on . Thus, the range condition is implied for . Similarly,  and  implies the minimum-edge-length condition, i.e. that  . The arguments for the direction-preserving condition are the same as above for . The non-degeneracy condition on the vertex at  is not affected by the edge-contraction, since  stays a minimum (resp. maximum) in  if it was a minimum (resp. maximum) in . Otherwise, the contracted edge would not minimize the expression in (\ref{min:edge}).

By construction it is clear that the vertex set of the   is a subset of the vertex set of  for each , as well as that  is the linear interpolation of  and . This completes the proof of the lemma.  
\end{proof}

\begin{definition}[Canonical vertex permutation]
\deflab{vtx:permutation}
Given a curve  with  vertices in general position, consider its canonical signatures  of \lemref{canonical:signature}. We call a permutation of the vertices of  canonical if for any two vertices  of  it holds that if  (the vertex set of ) and , for some , then  appears before  in the permutation. Furthermore, we require that the permutation contains a token separator for every , for , such that  consists of all vertices appearing after the separator.
\end{definition}

\begin{lemma}\lemlab{compute:vtx:permutation}
Given a curve   with  vertices
in general position, we can compute a canonical vertex permutation (\defref{vtx:permutation}) in  time and  space.
\end{lemma}
\begin{proof}
Let  be the vertices of the curve .
The idea is to simulate the series of edge contractions done in the proof of \lemref{canonical:signature}. We build a min-heap from the vertices  using certain weights, which will be defined shortly.\footnote{A heap of the edges can be alternatively used.} We then iteratively extract the (one or more) vertices with the current minimum weight from the heap and update the weights of their neighbors along the current signature curves. The extracted vertices are recorded in a list  in the order of their extraction and will form the canonical vertex permutation in the end. Before every iteration we append a token separator to . In this way, all vertices extracted during one iteration are placed between two token separators in . After the last iteration we again append a token separator and at last the two vertices  and .

More precisely, let  denote the vertices contained in the heap in the beginning of one particular iteration, sorted in the order in which they appear along the curve . We call the curve
 
the \emph{current signature}. For every vertex we keep a pointer to the heap element which represents its current predecessor and successor along the current signature. We also keep these pointers to the virtual elements  and , which are not included in the heap.  We define the weight  for every vertex  in the heap as follows:
\begin{compactenum}[(i)]
\item if , then ,
\item if , then , otherwise
\item .
\end{compactenum}
Initially, the current signature equals  and initializing these weights takes  time in total. Following the argument in \lemref{canonical:signature}, we need to contract the edge(s) with minimum length (where exceptions hold for the first and last edge). This is captured by the choice of the weights above. Assume for simplicity that the minimum is attained for exactly one edge\footnote{The two other possible cases are as follows.  It may be that multiple edges of the same length are contracted at once. In this case more than two vertices need to be extracted. Furthermore, it may be that only one vertex  or  is extracted at once. This corresponds to the case that an edge adjacent to  or  is being contracted. } with endpoints  and  for some . In this case,  and  are the next two elements to be extracted from the heap and their weight must be equal to . Using the pointers to  (unless ) and  (unless ), we now update the weights of these neighbors and update the pointers such that  (resp., ) becomes predecessor of  (resp., ). Computing the new weight of one of these neighboring vertices can be done in  time, updating the weights in the heap takes  time per vertex. We can charge every update to the extraction of a neighboring vertex. Since every vertex is extracted at most once, we have  weight updates in total.
\end{proof}
\begin{lemma}\lemlab{lb:vtx:permutation}
Given a curve  with  vertices
in general position, the problem of computing a canonical vertex permutation (\defref{vtx:permutation}) has time-complexity .
\end{lemma}
\begin{proof}
By \lemref{compute:vtx:permutation}, we can compute this canonical vertex permutation in time .  We show the lower bound via a reduction from the problem of sorting a list of  natural numbers. Let  be the elements of the list in the order in which they appear in the list. We can determine the maximal element  in  time. We now construct a curve  as follows:  where . The constructed curve contains an edge of length  for every  of our sorting instance. We call these edges \emph{variable edges}. The remaining edges of the  are called \emph{connector edges}. All connector edges are longer than . A canonical vertex permutation of  would provide us the with variable edges in ascending order of their length. 
\end{proof}

The following lemma testifies that we can query the canonical vertex permutation for a signature of a given size . (Note that a canonical signature of size
exactly  may not exist.)

\begin{lemma}\lemlab{extract:signature}
Given a canonical vertex permutation (\defref{vtx:permutation}) of a curve , we can in  time extract the canonical signature of  of maximal size  with .
\end{lemma}
\begin{proof}
Let  denote the suffix of the canonical vertex permutation which contains the last  vertices. If there is no token separator at the starting position of , then we remove the maximal prefix of  which contains not token separator. In this way, we obtain the vertices of the canonical signature  of maximal size  with . We now sort the vertices in the order of their appearance along  and return the resulting curve.
\end{proof}

The following theorem follows from \lemref{compute:vtx:permutation} and \lemref{extract:signature}. Furthermore, \lemref{lb:vtx:permutation} testifies that the preprocessing time is asymptotically tight.

\begin{theorem}\thmlab{compute:signatures}
Given a curve  with  vertices in general position, we can construct a data structure in time  and space , such that given a parameter  we can extract in time  a canonical signature of maximal size  with .
\end{theorem}

\begin{algorithm}[]\alglab{low:pass} \KwData{curve  with , parameter }
 \KwResult{values , such that  is the -signature of }
 \caption{Computing a -signature}
       ; ;  \tcc*[r]{assign first vertex of }
       \lRepeat(\tcc*[f]{scan beginning of time series}){  or  }{\;}
       \tcc*[r]{ is first point outside }
       \For(\tcc*[f]{scan remaining time series}){ to }{
           \eIf{}{
                    \tcc*[r]{update furthest point from  seen so far}
           }{
           \If(\tcc*[f]{gone backwards too far}){}{
                   ;
                     \tcc*[r]{append furthest point to }  
                    \tcc*[r]{update furthest point (and change direction)}  
           }}   
       }
       \If{}{
       ; ;  
       }
       ; ;  \tcc*[r]{assign last vertex of }
\end{algorithm}






\begin{lemma}\lemlab{low:pass:correct}
Given a curve  with  vertices 
in general position, and given a parameter , \algref{low:pass} computes a -signature  of  with  vertices in  time and space.
\end{lemma} 

\begin{proof}
We prove that \algref{low:pass} produces the values  that define a proper -signature
 of  according to
\defref{signature}.  The algorithm operates in three phases: (1) lines 2-4, (2)
lines 5-11, and (3) lines 12-14. In the first phase the algorithm finds the
first vertex  of  which lies outside the interval
 and assigns its index to the variable . 

In the trivial case,  is entirely contained in the interval
. In this case, the second phase is not executed and
the condition in line 12 evaluates to false. The algorithm returns the correct
signature, which has two vertices,  and .
Otherwise,  must leave the interval .
We claim that the following invariants hold at the end of each iteration of the for-loop in phase 2 (lines 5-11):
\begin{compactitem}
\item[(I1)]   is a correct prefix of the -signature,
\item[(I2)]  for any  it holds: 
\begin{compactenum}[(a)] 
\item if  then 
\item if  then  when
\\ (resp.,  when
).
\end{compactenum}
\item[(I3)]  \begin{compactenum}[(a)]
\item if , then ,
\item if  then ,
\end{compactenum}
\item[(I4)]  if , then for any  ,
\item[(I5)]  the direction-preserving property holds for the subcurve ,
\end{compactitem}
We prove the invariants by induction on . The base case happens after execution of
line 4, before the first iteration of the for-loop. For ease of notation, we
define  for this case. Invariants (I1), (I3) and (I4) hold by construction.
The other two invariants follow immediately from the observation that
 is the first point outside the interval .


Now we prove the induction step.  One may have the following intuition.
During the execution of the for-loop in lines 5-11, we implicitely maintain a
general direction in which the curve  is moving. 
This direction is \emph{upwards} if  and
\emph{downwards} otherwise.
Furthermore, we maintain that  is the furthest point from 
on the current signature edge (starting at ) in the current general
direction.
Note that a new vertex is appended to the signature prefix only when  has
already moved in the opposite direction by a distance greater than .
Only then, we say that the current general direction of the curve has changed.

Consider an arbitrary iteration  of the for-loop. There are three cases,
\begin{compactenum}[(i)]
\item line 7 is executed and  becomes the new \\
(this happens if  is moving in the current general direction beyond
),
\item lines 10 and 11 are executed and a new signature vertex is appended to the
signature prefix\\
(this happens if  has changed its general direction), 
\item no assignments are being made\\
(this happens if  locally changes direction, but the current general
direction does not change).
\end{compactenum}

For each invariant we consider each of the three cases above.
\begin{compactitem}
\item (I1): If the signature prefix was not changed in the previous iteration
(cases (i) and (iii)), then (I1) simply holds by induction. Otherwise, we argue
that the new signature prefix is correct. By induction,
 is a correct signature prefix.  
The conditions of \defref{signature} for  follow by the induction
hypotheses (I2),(I3), and (I5) in the iteration step , in which the last 
value of  was assigned. In particular, (I2) implies the range condition
and the non-degeneracy, (I3) implies the minimum-edge-length condition, and (I5)
implies the direction-preserving condition.

\item (I2): Assume  and . Since , we cannot be in
case (ii). Furthermore, once we enter the for-loop, the current general
direction is fixed until  is incremented for the first time. 
Therefore, by (I2) we have for  that ,
where  holds the value of  before we entered the for-loop in the current
iteration.  Now, in case (i) the claim follows immediately.
In case (iii) it follows from the (false) condition in line 9, that
, and by the (false)
condition in line 6, that . 
The case  and  is analogous.

It remains to prove the invariant for .
Assume case (ii) and assume that the general direction changed from
upwards to downwards (the opposite case is analogous). 
Let  and  be the values of  and  before the
new assignment in lines 10 and 11. By (I2),
we have  for any . By (I4), we
have  for any .
By the (true) condition in line 9, we have .
Therefore, for any , we have
, which implies (I2) after the assignment
in lines 10 and 11.

Now, in case (i) and case (ii), we have by (I2)  for  
that . The correctness in case (i)
follows immediately. In case (iii), assume  (the opposite
case is analogous). 
It follows from the (false) condition in line 9 and by (I3), that
, and by the (false)
condition in line 6, that . 

\item (I3) holds in case , since the for-loop was 
started after the curve  left the interval 
for the first time and by (I2)  is furthest
point from . In case , (I3) also holds, since we append the
parameter  to the signature prefix and re-initialize  with  only
after the curve has moved by a distance of at least  (line 9) from
. The distance is further maintained by (I2).

\item (I4) is clearly satisfied in case (i) and (ii), since  is assigned.
In case (iii) it holds by the (false) condition in line 9 that
 and for the remaining curve  it follows by induction.

\item (I5) holds since we assign a new signature vertex with parameter 
as soon as the curve moves by more than  in the opposite direction (case (ii)).  
\end{compactitem}

In phase 3, there are two cases. If the range condition is satisfied for the
last signature edge from  to , the algorithm only appends
the last vertex  of the curve  to the signature .
Otherwise, the algorithm appends  and  to the signature.  In
both cases, the conditions in \defref{signature} are satisfied also for the last
part. 

If we use a linked-list for the signature, the running time and space
requirements of the algorithm are linear in , since the execution of one
iteration of the for-loop takes constant time and there are at most  such
iterations.
\end{proof}

From \lemref{low:pass:correct} we obtain the following theorem.

\begin{theorem}\thmlab{computing:delta:signature}
Given a curve  with  vertices 
in general position, and given a parameter , we can compute a -signature of  in  time and space.
\end{theorem}



\begin{lemma}\lemlab{apx:min:error:simp}
Given a curve  with  vertices 
in general position, and given a parameter , we can compute in  time a curve  of at most  vertices, such that , for  being a minimum-error -simplification of  (\defref{min:error:simp}). 
\end{lemma}

\begin{proof}
Let  be the signatures of  with corresponding parameters  as defined in \lemref{canonical:signature}. \lemref{fd:signature} implies that . Consider the signature  with the maximal number of  vertices. We claim that 

The second inequality follows from the definition of  and the fact that  consists of at most  vertices. To see the first inequality, consider the signature  with . By \lemref{canonical:signature}, for any , the signature  is a -signature of  for . By \defref{signature}, it holds for

that for any  By the arguments in the proof of \lemref{nec:suff}, it follows that any curve  with  needs to consist of at least  vertices. Therefore, the first inequality follows. We can compute the signature  in  time using \thmref{compute:signatures}.
\end{proof}






\section{Hardness of clustering under the \Frechet distance}
\seclab{nphard}

A metric embedding is a function between two metric spaces which preserves the
distances between the elements of the metric space. The embedding is called
isometric if distances are preserved exactly. 
It is known that one can isometrically embed any bounded subset of a
-dimensional vector space equipped with the -norm into
~\cite{IndMat04}.
This immediately implies \NP-hardness for  knowing that 
the clustering problems we consider are \NP-hard under the
 distance for . In this section we prove that the \NP-hardness
holds from . This is achieved by preserving  in the
metric embedding. 

We begin by establishing the basic facts about clustering under the 
-norm.
The -center problem under  is \NP-hard for  as shown
by Feder and Greene \cite{feder1988optimal}.\footnote{Note that this result can
also be obtained from an earlier result by Megiddo
and~Supowit~\cite{megiddo1984geo}. They show that approximating the -center
problem under  within a factor smaller than  is \NP-hard.}
Even approximating the optimal cost within a factor smaller than  was
shown to be \NP-hard by the same authors.
The -median problem under  was proven to be \NP-hard  for  by Megiddo
and~Supowit~\cite{megiddo1984geo}. The following well-known observation implies
that the -median problem is also \NP-hard under  for  (and
therefore also for ). 

\begin{observation}
For any two points  and  in  it holds that 

where  is a rotation by  followed by a uniform scaling with
.
\end{observation}


We now describe the metric embedding used in the \textbf{NP}-hardness reduction.

\begin{lemma}\lemlab{embedding}
Any metric space , where  is a bounded set, can be embedded
isometrically into . Furthermore, if  is discrete, the embedding
and its inverse can be computed in time linear in  and~.  \end{lemma}
\begin{proof}
In the following, we view a list of reals , , 
from two different perspectives. We either (i) interpret  as an element of , 
or (ii) interpret  as a curve of .
The interpretation we take should be clear from the context. 
 
Let , that is, the maximal norm of an
element of .  Since  is bounded,  is well-defined. Note that
 also bounds the maximal coordinate value of an element of  and
likewise  bounds the minimal coordinate of an element of .  

We define the
translation vector  
Thus,  translates every even coordinate by  and every odd
coordinate by . 

Let  be two elements of the metric space. 
We argue that 
Note that by the triangle inequality 


By \obsref{segments}, the \Frechet distance between 
and  is at most , since we can associate the
th coordinate of  with the th coordinate of  to construct an
eligible matching~.  We claim that the matching  is in fact optimal.
Assume for the sake of contradiction that there exists a matching  which is
``better'' than , that is,  matches each point of  to a
point on  within a distance strictly smaller than .
It must be that  and  are structurally different from each other, in the
sense that their corresponding paths in the free space diagram do not visit the
same cells. This follows from our construction of  which ensures that the
path corresponding to  is optimal among all paths which visit the same cells.

So consider an arbitrary prefix curve  where  and 
structurally differ, that is, the image of  under  contains
either fewer or more vertices than the image under .
Assume fewer (otherwise let  be the corresponding suffix curve of
).  By our construction of , the image of  under
 has the same number of vertices as .  Let  denote
the image of  under .   
By our construction of , it holds that the difference between 
any two consecutive coordinate values of  is at least .  
Therefore, the -signature of  is equal to .
It follows from \lemref{nec:suff} that  needs to have at least as
many vertices as . However, by our choice of , the subcurve
 has fewer vertices than . A contradiction.
Thus  is optimal and .
\end{proof}

 The \textbf{NP}-hardness reduction takes an instance of the -center (resp.,
-median) problem under  in  and embeds it into 
using \lemref{embedding}.  If we could solve the -center (resp.,
-median) problem described in \secref{problem}, then by
\lemref{embedding}, we could apply the inverse embedding function to the
solution to obtain a solution for the original problem instance. The same holds
for approximate solutions. Note that the embedding given in \lemref{embedding}
works for any point in the convex hull of , therefore also for the centers
(resp., medians) that form the solution. We obtain the following theorems.

\begin{theorem}\thmlab{center:nphard}
The -center problem (where  is part of the input) is \NP-hard for
.  Furthermore, the problem is \NP-hard to approximate within a
factor of .
\end{theorem}

\begin{theorem}\thmlab{median:nphard}
The -median problem (where  is part of the input) 
is \NP-hard for . 
\end{theorem}









\section{Doubling dimension of the metric space}
\label{section:doublingdimension}


In this section we show that the unconstrained metric space of univariate time series 
under the \Frechet distance has unbounded doubling dimension~(\lemref{doub:unlimit}).
Even if we restrict the complexity of the curves 
to , the 
doubling dimension is unbounded~(\lemref{doub:limit}). 
For lower complexities, one can easily
show that the doubling dimension is bounded. Note that for  the 
metric space  equals the metric space . 
Note that the infinity of the doubling dimension is simply caused by the fact that the
metric space is incomplete. However, it remains that standard techniques for
doubling spaces cannot be applied.


\begin{definition}
In any metric space  a ball of center  and radius   is defined as the set
.
\end{definition}

\begin{definition}
The doubling dimension of a metric space is the smallest positive integer 
such that every ball of the metric space can be covered by  balls of half
the radius. 
\end{definition}


\begin{lemma}
\label{lemma:doub:unlimit}
The doubling dimension of the metric space  is unbounded.
\end{lemma}

\begin{proof}
Assume for the sake of contradiction that there exists a positive integer 
which equals the doubling dimension of the metric space .  We
show that such  cannot exist by constructing  elements of
 which lie in a ball of radius  while no two elements can be
covered by a ball of radius .  However, by the pidgeon hole principle, at
least one of the smaller balls would have to cover two different curves in the set. 
We construct a set of curves
 with . The set
 is contained in the ball of radius  centered at the curve
 See \figref{ddex}~(left) for an example.
Any two curves 
have \Frechet distance  to each other.  Now, assume that a ball of
radius  exists which contains both  and . Let 
its center be denoted . We can derive a contradiction using the triangle inequality:

\end{proof}

\begin{figure}
\centering
\includegraphics{ddex.pdf}
\caption{Examples of the constructed curves in \lemref{doub:unlimit}~(left) and \lemref{doub:limit}~(right). }
\figlab{ddex}
\end{figure}



\begin{lemma}
\label{lemma:doub:limit}
For any integer , the doubling dimension of the metric space  is unbounded.
\end{lemma}
\begin{proof}
The proof is similar to the proof of \lemref{doub:unlimit}. However, this time we argue that no two 
curves in the set can be covered by a ball of half the radius because there exists no suitable 
center in , that is, the center would need to have complexity higher than .
As in the other proof, we define a set .
For , let  
See \figref{ddex}~(right) for an example with .
Clearly, each  is an element of , since its complexity is at most .
Furthermore, the set  is contained in the ball with radius  centered at .
Note that the -signature of any  is equal to  itself.
Thus, by \lemref{nec:suff}, any center of a ball of radius at most , which contains 
 needs to have a vertex in each interval  for each vertex  of .
By construction, these intervals are pairwise disjoint for each curve and
across all curves in  (except for the intervals around the two endpoints).
Therefore, such a ball with radius at most  which would cover two
different curves , would need to have more than 
vertices and is therefore not contained in .
Indeed,  the number of pairwise disjoint signature intervals induced by any  with ,
is .
\end{proof}




\bibliographystyle{abbrv}
      \bibliography{Trajectories}

\end{document}
