



\section{Experiment}
In this section, we compare \name on benchmark and large-scale few-shot class-incremental learning datasets with state-of-the-art methods. Ablations show the effectiveness of sampling fake-incremental tasks and training meta-calibration module, and visualizations indicate \mame's ability to update with limited instances. We also analyze the influence of hyper-parameters in fake-incremental learning on the ability to learn new classes.





\subsection{Implementation Details}
\noindent {\bf Dataset}: Following the benchmark setting ~\cite{tao2020few}, we evaluate the performance on CIFAR100~\cite{krizhevsky2009learning}, CUB200-2011~\cite{WahCUB2002011} and {\it mini}ImageNet~\cite{russakovsky2015imagenet}. Additionally, we also compare the learning performance on large-scale dataset, \ie, ImageNet ILSVRC2012~\cite{deng2009imagenet}.  They are listed as:


\begin{table*}[t] 
	\caption{ Comparison with the state-of-the-art on CUB200 dataset. We report the results of compared methods from~\cite{tao2020few} and~\cite{zhang2021few}. 
		\name outperforms the runner-up method by 5.13 for the last accuracy and 5.09 for the performance dropping rate. 	\mame$^\dagger$ denotes our method with the same data augmentation (random crop and random horizontal flip) in CEC.
	}
	\centering	
\resizebox{\textwidth}{!}{
		
		\begin{tabular}{llllllllllllll}
			\toprule
			\multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{11}{c}{Accuracy in each session (\%) $\uparrow$} & \multirow{2}{*}{PD $\downarrow$} & {Our relative
			} \\ \cmidrule{2-12}
			\multicolumn{1}{c}{} & 0   & 1      & 2      & 3    & 4     & 5  & 6     & 7      & 8     &   9  & 10   & & improvement    \\ \midrule
			Finetune                & 68.68   & 43.70      & 25.05    & 17.72   & 18.08     & 16.95  & 15.10  & 10.06     & 8.93   &8.93  & 8.47   & 60.21&\bf +41.73     \\
			iCaRL~\cite{rebuffi2017icarl}        & 68.68   & 52.65     & 48.61    & 44.16  & 36.62   & 29.52  & 27.83  & 26.26    & 24.01   &23.89  & 21.16  & 47.52&\bf +29.04   \\
			EEIL~\cite{castro2018end}         & 68.68   & 53.63      &47.91    & 44.20  & 36.30     & 27.46  & 25.93  & 24.70     & 23.95   &24.13 & 22.11  & 46.57&\bf +28.09    \\
			Rebalancing~\cite{hou2019learning}         & 68.68   & 57.12     & 44.21    & 28.78  & 26.71    & 25.66  & 24.62  & 21.52    & 20.12   &20.06  & 19.87  & 48.81&\bf +30.33     \\
			TOPIC~\cite{tao2020few}            & 68.68   & 62.49      & 54.81    & 49.99   & 45.25     & 41.40 & 38.35  & 35.36    & 32.22  &28.31 & 26.26   & 42.40&\bf +23.92      \\
			Decoupled-Cosine~\cite{vinyals2016matching}    &75.52  & 70.95     & 66.46   & 61.20   & 60.86    & 56.88  & 55.40  & 53.49     & 51.94   &50.93 & 49.31   & 26.21&\bf +7.73     \\
			Decoupled-DeepEMD~\cite{zhang2020deepemd}     & 75.35   & 70.69     & 66.68   & 62.34  & 59.76     & 56.54  & 54.61  & 52.52    &50.73   &49.20 & 47.60   & 27.75&\bf +9.27    \\
			CEC~\cite{zhang2021few}                & 75.85   & 71.94    & 68.50   & 63.50   & 62.43    & 58.27 & 57.73 & 55.81    &54.83  &53.52  & 52.28   & 23.57&\bf +5.09   \\
			CEC~\cite{zhang2021few} + AutoAug                & 75.74   & 71.77    & 68.46   & 63.21   & 61.95    & 57.44 & 56.97 & 55.24    &54.23  &52.95  & 51.38   & 24.36&\bf +5.88   \\
			\midrule
			\mame$^\dagger$             &  76.32   &  74.18     & 72.68    & 69.19  & 68.79   &65.64  & 63.57   & 62.69     & 61.47   & 60.44   & 58.45& 17.87 \\
			\name             & \bf 75.89   & \bf 73.55     & \bf71.99    & \bf68.14  & \bf67.42   &\bf63.61  & \bf62.40   &\bf 61.35     & \bf59.91   & \bf58.66   & \bf57.41&\bf 18.48 \\
			
			
			\bottomrule
		\end{tabular}
	}\label{table:cub}
\end{table*}


\begin{table*}[t]
	\centering{
		\caption{Comparison with the state-of-the-art on CIFAR100 dataset. We report the results of compared methods from~\cite{tao2020few} and~\cite{zhang2021few}.  
			\name outperforms the runner-up method by 2.09 for the last accuracy and 1.35 for the performance dropping rate.  \mame$^\dagger$ denotes our method with the same data augmentation (random crop and random horizontal flip) in CEC.
		}\label{tab:cifar}
		\resizebox{\textwidth}{!}{
			\begin{tabular}{llllllllllll}
				\toprule
				\multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{9}{c}{Accuracy in each session (\%) $\uparrow$} & \multirow{2}{*}{PD $\downarrow$} & {Our relative
				} \\ \cmidrule{2-10}
				\multicolumn{1}{c}{} & 0   & 1      & 2      & 3    & 4     & 5  & 6     & 7      & 8     &     &  improvement      \\ \midrule
				Finetune                & 64.10   & 39.61      & 15.37      & 9.80   & 6.67     & 3.80  & 3.70    & 3.14      & 2.65     & 61.45   & \bf +38.87     \\
				iCaRL~\cite{rebuffi2017icarl}       & 64.10   & 53.28      & 41.69      & 34.13   & 27.93     & 25.06  & 20.41   & 15.48      &13.73    & 50.37  & \bf +27.79  \\
				EEIL~\cite{castro2018end}         & 64.10   & 53.11     & 43.71     & 35.15   & 28.96     & 24.98  & 21.01    & 17.26     & 15.85    & 48.25  & \bf +25.67    \\
				Rebalancing~\cite{hou2019learning}           & 64.10   & 53.05     & 43.96      & 36.97   & 31.61     & 26.73  & 21.23   & 16.78    & 13.54     &50.56  & \bf +27.98   \\
				TOPIC~\cite{tao2020few}                & 64.10   & 55.88     & 47.07      & 45.16   & 40.11   & 36.38 & 33.96   & 31.55      & 29.37     & 34.73   & \bf +12.15      \\
				
				Decoupled-Cosine~\cite{vinyals2016matching}  & 74.55   & 67.43      & 63.63      & 59.55  & 56.11    & 53.80  & 51.68   & 49.67     & 47.68     & 26.87  & \bf +4.29     \\
				Decoupled-DeepEMD~\cite{zhang2020deepemd}    & 69.75   & 65.06     & 61.20     & 57.21  & 53.88    & 51.40  & 48.80  & 46.84     & 44.41     & 25.34   & \bf +2.76    \\
				CEC~\cite{zhang2021few}                    & 73.07   & 68.88     & 65.26    & 61.19  & 58.09   &55.57  & 53.22   & 51.34     & 49.14   & 23.93   & \bf +1.35    \\
				CEC~\cite{zhang2021few}+ AutoAug      & 73.17   & 69.06    & 65.31    & 61.21  & 57.92   &55.82  & 53.47   & 51.67     & 49.55   & 23.62   & \bf +1.04    \\
				\midrule
				\mame$^\dagger$                 & 73.02   & 70.76     & 67.45    & 63.38  & 59.97   &56.90  & 54.84   & 52.18     & 49.92   & 23.10  &    \\
				\name        & \bf 73.81   & \bf 72.09    & \bf67.87   & \bf63.89 & \bf60.70  &\bf57.77 & \bf55.67   &\bf  53.52     & \bf51.23   & \bf22.58   &   \\
				
				
				
				\bottomrule
			\end{tabular}
	}}
\end{table*}


\begin{table*}[t]
	\centering{
		\caption{Comparison with the state-of-the-art on \textit{mini}ImageNet dataset. We report the results of compared methods from~\cite{tao2020few} and~\cite{zhang2021few}. 
			\name outperforms the runner-up method by 1.56 for the last accuracy and 1.24 for the performance dropping rate. 
		\mame$^\dagger$ denotes our method with the same data augmentation (random crop and random horizontal flip) in CEC. }\label{tab:mini}
		\resizebox{\textwidth}{!}{
			
			\begin{tabular}{llllllllllll}
				\toprule
				\multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{9}{c}{Accuracy in each session (\%) $\uparrow$} & \multirow{2}{*}{PD $\downarrow$} & {Our relative
				} \\ \cmidrule{2-10}
				\multicolumn{1}{c}{} & 0   & 1      & 2      & 3    & 4     & 5  & 6     & 7      & 8     &     &  improvement      \\ \midrule
				Finetune                & 61.31  & 27.22      & 16.37     & 6.08   & 2.54     & 1.56  & 1.93    & 2.60      & 1.40     & 59.91   & \bf +36.78    \\
				iCaRL~\cite{rebuffi2017icarl}       & 61.31   & 46.32     & 42.94      & 37.63   & 30.49     & 24.00  & 20.89   & 18.80      &17.21  & 44.10  & \bf +20.97 \\
				EEIL~\cite{castro2018end}         & 61.31   & 46.58     & 44.00    & 37.29   & 33.14    & 27.12  & 24.10    & 21.57     & 19.58    & 41.73 & \bf +18.60   \\
				Rebalancing~\cite{hou2019learning}           & 61.31   & 47.80     & 39.31      & 31.91  & 25.68     & 21.35  & 18.67   & 17.24    & 14.17     &47.14  & \bf +24.01    \\
				TOPIC~\cite{tao2020few}                & 61.31 & 50.09    & 45.17      & 41.16   & 37.48   & 35.52 & 32.19  & 29.46    & 24.42     & 36.89   & \bf +13.76      \\
				
				Decoupled-Cosine~\cite{vinyals2016matching}  & 70.37  & 65.45      & 61.41     & 58.00  & 54.81   & 51.89  & 49.10   & 47.27     & 45.63    & 24.74 & \bf +1.61   \\
				Decoupled-DeepEMD~\cite{zhang2020deepemd}    & 69.77   & 64.59     & 60.21    & 56.63  & 53.16   & 50.13  & 47.79  & 45.42     & 43.41    & 26.36   & \bf +3.23   \\
				CEC~\cite{zhang2021few}                    & 72.00   & 66.83     & 62.97   & 59.43  & 56.70   &53.73  & 51.19   & 49.24     & 47.63   & 24.37   & \bf +1.24     \\
				CEC~\cite{zhang2021few} + AutoAug                    & 72.23   & 66.96     & 62.98   & 59.62  & 56.86   &53.85  & 51.40   & 49.32     & 47.97   & 24.26   & \bf +1.13     \\
				\midrule
				\mame$^\dagger$                & 71.92   & 67.93     & 63.58   & 60.22  & 57.33   &54.30  & 51.97   & 50.01     & 48.40   & 23.52   &     \\
				\name              & \bf 72.32   & \bf 68.47     & \bf64.30    & \bf60.78  & \bf57.95   &\bf55.07  & \bf52.70  &\bf 50.72     & \bf 49.19   & \bf23.13   &   \\
				\bottomrule
				
			\end{tabular}
	}}
\end{table*}


\begin{itemize}
	\item{  \bfname{CIFAR100}}: There are 100 classes with 60,000 images. 
	Following the dataset splits of~\cite{tao2020few}, 60 classes are used as base classes, and the rest 40 classes are organized as incoming new classes. We then divide these classes into eight incremental sessions; each contains a 5-way 5-shot incremental task.
	
	
	\item \bfname{CUB200}: a fine-grained image classification task with 11,788 images from 200 classes. We follow the dataset configuration in~\cite{tao2020few} and use 100 classes as base classes. The other 100 classes are formulated into ten sessions; each contains a 10-way 5-shot incremental task.
	
	\item{\bfname{{\textit {mini}}ImageNet}}:  is a subset of ImageNet~\cite{deng2009imagenet} with 100 classes. 
	These classes are divided into 60 base classes and 40 new classes. New classes are organized into eight sessions, and each contains a 5-way 5-shot incremental task.
	
	\item{\bfname{ImageNet1000}}:  is a large scale dataset with 1,000 classes, with about
	1.28 million images for training and 50,000 for validation. Following the split of CIFAR100, we divide these classes into 600 base classes and 400 new classes. The new classes are organized into eight sessions, and each contains a 50-way 5-shot incremental task. We also follow~\cite{yu2020semantic} to sample a 100 class subset of ImageNet1000, denoted as \textbf{ImageNet100}. The dataset split of ImageNet100 is the same as CIFAR100.
	
	
	
\end{itemize}

We use the \emph{same} training splits (including instances of base and incremental sessions) for every compared method for a fair comparison. The testing set is the same as the original one to evaluate the generalization ability holistically. We use the policies in AutoAugment~\cite{cubuk2019autoaugment} for data augmentations, \ie, {\it CIFAR10 policy} for CIFAR100, and {\it ImageNet policy} for other datasets. Please refer to Section~\ref{sec:data_aug} for more details.

\begin{figure}[t]
	\begin{center}
		\subfigure[ImageNet100]
		{	\includegraphics[width=.475\columnwidth]{pics/imagenet100_5shot}}
		\subfigure[ImageNet1000]
		{		\includegraphics[width=.475\columnwidth]{pics/imagenet1000_5shot}}
		
	\end{center}
	\caption{ Incremental accuracy along incremental tasks on ImageNet. \name consistently outperforms the state-of-the-art method with a substantial margin.
	} \label{figure:imagenet}
\end{figure}


\begin{figure*}[h]
	\begin{center}
		\subfigure[Finetune]{
			\includegraphics[width=.465\columnwidth]{pics/finetune_cm}
			\label{fig:ft}}
		\subfigure[iCaRL]{
			\includegraphics[width=.465\columnwidth]{pics/icarl_cm}
			\label{fig:icarl}}
		\subfigure[Decoupoled-Cosine]{
			\includegraphics[width=.465\columnwidth]{pics/cosine_cm}
			\label{fig:cosine}}
		\subfigure[\name]{
			\includegraphics[width=.536\columnwidth]{pics/limit_cm}
			\label{fig:ours}}
	\end{center}
	
	\caption{  Confusion matrix on CUB200 after the last incremental session.  	{\name adapts to new classes with a generalizable feature and stably resists catastrophic forgetting. }
	} \label{figure:confmat}
	
\end{figure*}



\noindent {\bf Compared methods:} We first compare to classical class-incremental learning methods, \eg, iCaRL~\cite{rebuffi2017icarl}, EEIL~\cite{castro2018end} and Rebalancing~\cite{hou2019learning}. Besides, we also compare to current state-of-the-art FSCIL algorithms: TOPIC~\cite{tao2020few}, 
 Decoupled-DeepEMD/Cosine~\cite{zhang2020deepemd,vinyals2016matching} and CEC~\cite{zhang2021few}. We also report the baseline  which finetunes the limited instances, denoted as finetune.





\noindent {\bf Training details:} 
All models are deployed with PyTorch~\cite{paszke2019pytorch}.
 We use the \emph{same} network backbone~\cite{tao2020few} for \emph{all} compared methods. For CIFAR100, we use ResNet20~\cite{he2015residual}, while for CUB200, ImageNet and {\it mini}ImageNet we use ResNet18. We follow the most standard implementations for transformer~\cite{vaswani2017attention}. We use a shallow transformer with only one layer, and the number of multi-head attention is set to 1 in our implementation.  The hidden dimension is set to 64 for CIFAR100 and 512 for CUB, ImageNet and \textit{mini}ImageNet. The dropout rate in transformer is set as 0.5.
 Before the fake-FSCIL learning process, we pre-train the model using cross-entropy loss. We use SGD with an initial learning rate of 0.1 and momentum of 0.9. The pre-training epoch is set to 300 for all datasets with a batch size of 128. After that, the model is utilized as the initialization of fake-incremental learning. The learning rate is set to $0.0002$ and suffers a decay of 0.5 every 1000 iterations.
 During the fake-FSCIL learning process, we sample 2-phase fake-tasks to optimize the model. 
 The fake-shot and fake-way are discussed in Section~\ref{sec:hyperparam}. The backbone and the calibration module are fixed after the meta-training process.
  The source code of \name will be made publicly available upon acceptance. 
  
 


\noindent {\bf Evaluation protocol:}  Following~\cite{tao2020few,zhang2021few}, we denote the Top-1 accuracy after the i-th session as $\mathcal{A}_i$. Algorithms with higher $\mathcal{A}_i$ have the better prediction accuracy.
To quantitatively evaluate the forgetting phenomena of each method, we also use performance dropping rate (PD), \ie, $\text{PD}=\mathcal{A}_0-\mathcal{A}_B$, where $\mathcal{A}_0$ stands for the accuracy after the base session, and $\mathcal{A}_B$ is the accuracy after the last incremental session. The method with a lower performance dropping rate suffers less forgetting phenomena.










\experimentsection{Ablation Study}
We first analyze the importance of each component in \name on the CIFAR100 dataset. The results are shown in Table~\ref{tab:ablation}. We separately construct models with different combinations of the core elements in \mame, \eg, replace classifier with $\p_i$ (denoted as `Prototype'), utilize meta-calibration module $\mathcal{T}$ for calibration (denoted as `Calibration'), train with one-phase fake-tasks (denoted as `Meta-1', \ie, the degradation version of \mame), train with multi-phase fake-tasks (denoted as `Meta-C'. We report the results for $C=2$).



From Table~\ref{tab:ablation}, we can infer that directly optimizing the model with few-shot images, \ie, without any elements in \name will suffer severe catastrophic forgetting (Line 1). Besides, utilizing the prototypes to initialize new class classifiers can relieve the overfitting and forgetting phenomena to some extent (Line 2). However, training the meta-calibration module can further calibrate the relationship between old and new classes, which improves the performance (Line 3). It should be noted that the transformer structure is not used as the embedding in our model, and the main reason for the performance improvement is the learned calibration information during meta-learning.
 Additionally, when using our fake-incremental training scheme, the prediction performance will be further improved (Line 4 and 5). When comparing Line 5 to Line 4, we can infer that multi-phase meta-training helps the model prepare for the multi-phase incremental training process and helps obtain a more generalizable feature space.  
 These results imply that our proposed training paradigm obtains \emph{substantial improvement over the baseline method, \ie, ProtoNet} (Line 2). Besides, learning the meta-calibration module and the meta-learning protocol is helpful for the FSCIL task. 





\begin{figure*}[t]
	\begin{center}
		
		\subfigure[Fake-way/shot ]
		{	\includegraphics[width=.64\columnwidth]{pics/wayshot}
			\label{fig:cifar-wayshot}}
		\hfill
		\subfigure[Fake-incremental phase]
		{	\includegraphics[width=.64\columnwidth]{pics/Stages_performance}		\label{fig:meta-stage}}
		\hfill
		\subfigure[Multiple trials]
		{	\includegraphics[width=.64\columnwidth]{pics/multiple_runs}		\label{fig:multiple-run}}
		\\
\subfigure[Incremental shot on CIFAR100]
		{	\includegraphics[width=.64\columnwidth]{pics/cifar100_changeshot}
			\label{fig:testshot-cifar}}
		\hfill
		\subfigure[Incremental shot on \textit{mini}ImageNet]
		{	\includegraphics[width=.64\columnwidth]{pics/mini100_changeshot}
			\label{fig:testshot-mini}}
		\hfill
		\subfigure[Incremental shot on CUB]
		{	\includegraphics[width=.64\columnwidth]{pics/cub_changeshot}
			\label{fig:testshot-CUB}}
	\end{center}
\caption{Analysis of the hyper-parameters. Large way and large shot in fake-incremental learning, more than one fake-incremental phase, and more shots of new classes will improve the performance.
	} \label{figure:ablation}
\end{figure*}



\experimentsection{Benchmark Comparison}
In this section, we report the experimental results on three benchmark datasets, \ie, CIFAR100, CUB200, and \textit{mini}ImageNet in  Figure~\ref{figure:benchmark}.
The detailed values are reported in Table~\ref{table:cub}~\ref{tab:cifar}~\ref{tab:mini} 
We report the baseline performance from~\cite{tao2020few,zhang2021few}. 
The results of \name are measured by training 2-phase fake-incremental tasks.

From Figure~\ref{figure:benchmark}, we can infer that \name consistently outperforms the SOTA methods on these datasets. For CIFAR100 and \textit{mini}ImageNet, \name outperforms the runner-up method by 1.5\%-2\%. The improvement on CUB200 is much greater, which reaches 5\%. Finetune does not consider regularizing former knowledge, which suffers catastrophic forgetting and gets the worst performance. Correspondingly, class-incremental learning algorithms consider maintaining former knowledge to resist forgetting. iCaRL restricts the old class discriminability by knowledge distillation, but it easily overfits on few-shot new classes and performs poorly in FSCIL. 
EEIL considers an extra balanced-finetuning process, which alleviates the forgetting to some extent. Rebalancing enhances the incremental model with contrastive training, but such a learning process is hindered by the limited instances. However, we can observe the overfitting phenomena of these CIL methods, indicating that CIL methods are not suitable for few-shot inputs. 
To this end, some algorithms are proposed for tailoring few-shot class-incremental learning scenarios. TOPIC utilizes a neural gas structure, which can maintain the topology of features between many-shot old classes and few-shot new classes. As a result, during the incremental learning process, the overfitting phenomenon is alleviated by such topology restriction, and TOPIC gets better performance than CIL methods. Motivated by this, CEC is proposed to decouple the learning process and further resist overfitting. The decoupled training paradigm generalizes the discriminability from base classes to new classes, obtaining the runner-up performance among all compared methods. Note that Decoupled-Cosine/DeepEMD and CEC also adopt the prototypical network protocol. \emph{The improvement of \name over these methods implies that our training paradigm is more proper for few-shot class-incremental learning}.




 When comparing \name to CEC, we can infer that \name consistently outperforms it in these benchmark settings. There are three reasons for such a performance gap: (1) We provide a novel sampling framework directly derived from the expected risk in Eq.~\ref{eq:fscil_risk}. Since FSCIL has multiple incremental sessions, \name directly optimizes the expected risk in Eq.~\ref{eq:meta-risk}, while CEC only minimizes the approximation with 1-stage tasks. (2) CEC uses image rotation to synthesize new classes, which is irrelevant to the FSCIL context. By contrast, we sample new classes from the base session, which is relevant and beneficial to real FSCIL tasks. (3) The meta-calibration module implemented with transformer can encode the calibration information between old and new classes.



We report the detailed value of these benchmark datasets in Table~\ref{table:cub}~\ref{tab:cifar}~\ref{tab:mini}, and show the corresponding performance dropping rate of each compared method. The last column shows the relative improvement, indicating how much our \name outperforms the other methods in terms of PD, \ie, resisting forgetting. 
We can infer from these tables that \name  consistently resists forgetting in the few-shot class-incremental tasks.

Apart from these benchmark datasets, we also conduct experiments on the popular large-scale datasets, \ie, ImageNet100 and ImageNet 1000. We report the incremental performance of \name and the competitive methods, \ie, CEC and Decoupled-Cosine in Figure~\ref{figure:imagenet}. As we can infer from these figures, \name consistently outperforms CEC in the large-scale learning scenario. To conclude, \name outperforms the current state-of-the-art method with a substantial margin on large-scale and small-scale datasets.







\subsection{Visualization of Confusion Matrix}

In this section, we visualize the confusion matrix after the last session for different methods on CUB200. The results are shown in Figure~\ref{figure:confmat}. The former 100 classes are base classes, and the rest 100 classes are incremental classes. 
Warm colors indicate higher accuracy in these figures, and cold colors indicate lower accuracy.

The confusion matrix of finetune is shown in Figure~\ref{fig:ft}, and we can infer that the method tends to predict the labels of the last session. Finetune easily falls overfitting on these new classes and suffers severe catastrophic forgetting. Figure~\ref{fig:icarl} shows the confusion matrix of iCaRL, which resists forgetting via knowledge distillation. iCaRL works better than finetune, with the diagonal brighter. However, it also tends to predict instances as new classes and suffers catastrophic forgetting. Figure~\ref{fig:cosine} shows the confusion matrix of Decoupled-cosine. It follows the prototypical network framework and utilizes a cosine classifier for classification. The results indicate that replacing classifiers with prototypes will not change the embedding, and overfitting phenomena will be alleviated. 
Although Decoupled-cosine maintains old class performance, the accuracy of new classes is not good enough. The results of \name are shown in Figure~\ref{fig:ours}, which has more competitive performance on \emph{new classes}.  The visualization of the confusion matrix indicates that \name adapts to new classes with a generalizable feature and stably resists catastrophic forgetting.

\begin{table}[t] 
	\caption{Accuracy analysis of base and incremental classes after the last incremental session on CUB200. \name improves the accuracy on new classes with a more generalizable feature embedding. }
	\centering
	{\begin{tabular}{lccc}
			\addlinespace
			\toprule
			{Methods} &{Base}  &{Incremental} &{Harmonic Mean}  \\
			\midrule
			Decoupled-Cosine & 71.5 &  28.8 &41.1\\
			CEC & 71.1 & 33.9 &45.9 \\
			\midrule
			\name & \bf 73.6 & \bf41.8 &\bf 53.3\\
			\bottomrule
		\end{tabular}\label{table:knownandunknown}}
\end{table}

To quantitatively measure the generalization ability of the model, we also report the average accuracy of base classes (classes in $Y_0$) and incremental classes (classes in $Y_1\cup\cdots Y_{B}$) after the last incremental session in Table~\ref{table:knownandunknown}.
We also follow~\cite{Cheraghian_2021_CVPR,ye2021learning} and report the harmonic mean between old and new classes.
 For comparison, we report the most competitive compared methods, \ie, Decoupled-Cosine and CEC. Table~\ref{table:knownandunknown} indicates that the performances for base classes are almost the same.
In contrast, CEC trains a graph model to incrementally update tasks and improves the accuracy of new classes by 5\%. However, \name considers sampling the multi-phase incremental sessions instead of single-phase. It utilizes the transformer architecture to extract invariant information for model extension, which achieves a remarkable improvement of 13\% on new classes.
Experimental results validate that \name learns a more generalizable feature embedding during the fake-incremental tasks.




\experimentsection{Analysis of Hyper-Parameters} \label{sec:hyperparam}
In this section, we study the influence of hyper-parameters on the final performance. 
In detail, we change the fake-shot/way in  fake-incremental learning, the number of fake phases, training instance selection, and test shot to find out their impact on final results.

\subsubsection{Fake-Way/Shot}
We first report the final accuracy on CIFAR100 by varying the fake-way and fake-shot in Figure~\ref{fig:cifar-wayshot}.
According to the discussions in Section~\ref{sec:discussions}, \name does not require the meta-training and testing stage to follow the `same-way same-shot' protocol. As a result, we fix the other settings the same as in benchmark experiments and change the fake-incremental way from $\{1, 5, 10, 15, 20\}$. We also choose the fake-incremental shot from $\{1, 5, 10, 15, 20\}$, resulting in 25 compared results. We can infer from Figure~\ref{fig:cifar-wayshot} that \name prefers large fake-training way and fake-training shot, \ie, training with 20-way 20-shot gets the best performance. Nevertheless, we also notice the influence of fake-incremental way is stronger than fake-incremental shot.

\subsubsection{Fake-Incremental Phase}
We then report the final accuracy on three benchmark datasets by varying the number of fake-incremental phases $C$. 
We sample a 5-way 5-shot fake-incremental task for each phase and choose $C$ from $\{1, 2, 3, 4, 5\}$. 
From Figure~\ref{fig:meta-stage}, we can infer that the performance of more than one incremental phase is better than that of one phase, which verifies the effectiveness of our training paradigm for FSCIL.
The number above each cluster indicates the improvement of multi-phase training over single-phase training.
However, we also find the improvement is trivial for more than two phases, and we set the sampling phases to 2 for all datasets.



\subsubsection{Multiple Trials}
The current benchmark-setting is defined in~\cite{tao2020few}, where the methods are evaluated with the same training instances for \emph{only one time}. To empirically evaluate the robustness of algorithms, we conduct more trials and report the average and standard deviation.
In detail, following the same class split as~\cite{tao2020few}, we suggest sampling different few-shot instances as the training sets $\{\D^1,\D^2,\cdots,\D^B\}$. Specifically, we first filter out the instances of each class and then randomly sort them, picking the first $K$ instances. The sampling process can be reproduced by assigning specific random seeds.\footnote{In our implementation, we use random seeds from $\{1,2,\cdots, E\}$, where $E$ stands for the total rounds.} 
We sample different FSCIL episodes 30 times and report the average final accuracy and standard deviation. The results of five typical methods are shown in Figure~\ref{fig:multiple-run}.

We can infer from Figure~\ref{fig:multiple-run} that \name works robustly facing different episode combinations. Besides, iCaRL utilizes knowledge distillation to prevent forgetting, and its performance varies between different task combinations. The ranking order of these five methods is the same as reported in the benchmark tables, indicating \name consistently outperforms other methods.

\subsubsection{Incremental Shot}
In the FSCIL setting, each incremental training set $\mathcal{D}^b$ can be formulated as $N$-way-$K$-shot.
We also change the shot number $K$  of each new class during the incremental tasks, and report the learning trends of \name in Figure~\ref{fig:testshot-cifar}, ~\ref{fig:testshot-mini}, and~\ref{fig:testshot-CUB}. 
 We keep the testing way the same as the benchmark-setting and vary the shot number from $\{1, 5, 10,  20\}$.
Results indicate that the model receives more information for new classes with more instances from each class. 
 Hence, the estimation of prototypes will be more precise, and the prediction performance will correspondingly improve. However, the increasing trend will converge as more training instances are available, \eg, $K=20$.



\begin{figure}[t]
	\begin{center}
		\subfigure[Five old classes]
		{	\includegraphics[width=.465\columnwidth]{pics/base} \label{fig:tsne1}}
		\subfigure[Five old \& five new classes]
		{		\includegraphics[width=.465\columnwidth]{pics/increment} \label{fig:tsne2}}
		
	\end{center}
	\caption{ t-SNE visualization of the learned decision boundary on CUB200 between two sessions. Old classes are shown in dots, and new classes are shown in triangles. The shadow region represents the decision boundary of each class. 
	} \label{figure:tsne}
\end{figure}


\experimentsection{Visualization of Decision Boundaries}
In this section, we visualize the learned decision boundaries on the CUB200 dataset.  We use t-SNE~\cite{van2008visualizing} to visualize the test instances and corresponding decision boundaries of each class in 2D, as shown in Figure~\ref{figure:tsne}. 

In Figure~\ref{fig:tsne1}, we show model before incremental updating. 
Shadow regions represent the decision boundary of five base classes, and embeddings of test instances are shown with dots.
After that, we extend our model with $5$-way $5$-shot new classes and visualize the updated decision boundary in Figure~\ref{fig:tsne2}. We can infer that the meta-calibration module helps to adapt the prototype and calibrate the decision boundary between old and new classes. 
As a result,  \name works competitively with few-shot inputs, which efficiently calibrates old and new classes in the few-shot class-incremental setting. The figures also indicate that incorporating  the knowledge of new classes does not harm the classification performance of old classes, \ie, \name maintains classification performance and resists catastrophic forgetting.





\experimentsection{Visualization of Meta-Calibration Module}


In this section, we visualize the prediction results before and after meta-calibration and analyze their differences. We choose images from \textit{mini}ImageNet and use the model trained under the benchmark setting. The results are shown in Figure~\ref{figure:meta-calibration}. We show the original images in the first row, the top-5 prediction probability before meta-calibration module (\ie, $W^\top \phi(\x)$) in the second row, and the top-5 prediction probability after meta-calibration (\ie, ${\tilde{W}}^\top \tilde{\phi}(\x)$) in the last row.



\begin{figure}[t]
	\begin{center}
		
		
		{	
			\includegraphics[width=.95\columnwidth]{pics/ca5}
			\includegraphics[width=.95\columnwidth]{pics/ca3}
			\includegraphics[width=.95\columnwidth]{pics/ca10}
			\includegraphics[width=.95\columnwidth]{pics/ca1}
			
			
			
			
		}
		
	\end{center}
\caption{ Visualization of the prediction probability before and after meta-calibration on \textit{mini}ImageNet. The first row indicates original images. The second row indicates the top-5 output probability before meta-calibration. The third row indicates the top-5 output probability after meta-calibration.
		Base classes are shown with brown color, and incremental classes are shown in blue. The ground-truth class is shown with red edges.
	} \label{figure:meta-calibration}
\end{figure}


The predicted probabilities of base classes are shown in brown bars, and the predicted probabilities of incremental classes are shown in  blue bars.
 The probability of the ground-truth label is denoted with red edges. 
 As shown in these figures, the top two figures are from the base classes, and the bottom two are from the incremental classes.
  For the many-shot base classes, the model may have wrong predictions, \ie, predicting a golden retriever into a lion or predicting a goose into an arctic fox. To this end, the meta-calibration module can correct the model and increase the probability of the ground-truth class with the transformer. 
 
 When switching to the inference of new classes, since the model has only seen few-shot training images, it tends to predict them as base classes, \eg, predicting a wok into a cocktail-shaker or predicting a trifle into a barrel.
  Under such circumstances, the meta-calibration module will help to re-rank the ordering of these outputs and increase the output probability on new classes. These visualizations indicate that the meta-learned calibration module has encoded the inductive bias in the fake-incremental learning process and generalized it into the inference time. These conclusions are consistent with Table~\ref{table:knownandunknown} and Figure~\ref{figure:confmat} that \name improves the learning ability of new classes.
 
 

\experimentsection{Analysis of Incremental Task Sampling,  Model Efficiency and Data Augmentations}\label{sec:c2cec}


In this section, we analyze the task sampling strategy and model efficiency of different methods. In detail, we interchangeably use the fake task sampling strategy to find out the proper one. Besides, we report the training time and model parameters for different methods to get a holistic evaluation. We also analyze the results of different methods with different augmentations.

\begin{table}[t]
	\centering
	\caption{Ablation study of fake task sampling strategy. Fake incremental tasks (FIT) are our sampling strategy, and pseudo incremental learning (PIL) are from CEC. We interchangeably use these task sampling strategy to train CEC and \name, and report the final accuracy on CUB200 and CIFAR100. }
	\resizebox{0.8\columnwidth}{!}{
		\begin{tabular}{l|cc|cc}
			\addlinespace
			\toprule
			Dataset& \multicolumn{2}{c|}{CUB200} & \multicolumn{2}{c}{CIFAR100}\\
			Task Sampling& FIT & PIL & FIT & PIL \\
			\midrule
			{CEC} & \bf 53.01 & 52.28 &\bf 49.60 & 49.14\\
			{\name} & \bf 57.41 & 56.37 & \bf 51.23 & 51.03\\
			\bottomrule
		\end{tabular}
	}
	\label{tab:ablation_fake_task}
\end{table}

\subsubsection{Task Sampling Strategy}
According to the discussions in Section~\ref{sec:discussions}, there are other ways to sample fake-incremental tasks for meta-training. We choose the most typical method, \ie, CEC, to conduct the ablation analysis. Specifically, we interchangeably use the fake task sampling process in CEC and our \name to train the model and report the last accuracy of these methods. In CEC, the meta-learning tasks are denoted as pseudo incremental learning (PIL), which is called fake-incremental tasks (FIT) in ours. The results are reported in Table~\ref{tab:ablation_fake_task}. We can infer from Table~\ref{tab:ablation_fake_task} that our fake task sampling strategy can improve the performance of CEC, which verifies the strengths discussed in Section~\ref{sec:discussions}. Besides, when changing our fake-task sampling strategy to the way of CEC, the performance of \name degrades. These conclusions imply that our sampling strategy is a better choice for few-shot class-incremental learning. 

\subsubsection{Model Efficiency}
Since \name includes an extra transformer model to calibrate prototype and query instances, we report the model size and running time comparison of several typical methods on CUB200 in Figure~\ref{figure:time_and_param}.

The training time of different methods is reported in Figure~\ref{figure:time_and_param_a}. The training time of \name includes the pre-training process (which is the same for all methods) and the meta-training process.
We can infer that \name only requires slight extra training time to learn the meta-calibration module. The extra running time of \name is humble compared to that of CEC, and our running time is at the same scale as other methods. Besides, we can infer from Figure~\ref	{figure:time_and_param_b} that the extra model size of \name is at the same scale as CEC. The number of model parameters of the transformer is about 1 million, which is neglectable compared to the backbone network (ResNet18). There are some methods utilizing knowledge distillation to resist forgetting, \eg, iCaRL and EEIL. These methods need to save an extra backbone as the old model to provide supervision of knowledge distillation, and \name is more memory efficient during the training process.



	\begin{figure}[t]
	\begin{center}
		\subfigure[Training time]
		{\includegraphics[width=.48\columnwidth]{./pics/Running_Time}
			\label{figure:time_and_param_a}}
		\subfigure[Model parameters]
		{\includegraphics[width=.48\columnwidth]{./pics/Memory_Size}
			\label{figure:time_and_param_b}}
	\end{center}
\caption{Running time and model parameter comparison of several typical methods.  The bars with shadow denote the parameters used during training but dropped during inference.
	} \label{figure:time_and_param}
\end{figure}

\subsubsection{Data Augmentations} \label{sec:data_aug}

As a common technique in deep learning, augmentations are proven to be effective in model training. To investigate the influence of using these augmentations, we conduct experiments by combining different augmentations to ours and CEC, and report the results on benchmark datasets in Table~\ref{tab:ablation_augmentation}. We also plot the incremental performance in Figure~\ref{figure:augmentation_ablation}. Denote the augmentations in CEC as BasicAug (\ie, random crop and random horizontal flip). We train CEC and \name separately with BasicAug/AutoAug, resulting in four combinations. We can draw several conclusions from these ablations:



	\begin{table}[t]
	\centering
	\caption{Ablation study of augmentation strategies. BasicAug denotes the augmentation policy adopted in CEC. We interchangeably use these augmentation strategies to train CEC and \mame, and report the final accuracy. }
		\begin{tabular}{cccc}
		\addlinespace
		\toprule
		Dataset& {CIFAR100} &{CUB200} & {\textit{mini}ImageNet}\\
		\midrule
		{CEC + BasicAug} &49.14 & 52.28 & 47.63 \\
		{\name + BasicAug} &\bf 49.92 & \bf 58.45 & \bf 48.40 \\
		\midrule
		{CEC + AutoAug} & 49.55& 51.38 & 47.97 \\
		{\name + AutoAug} & \bf 51.23&  \bf 57.41 & \bf 49.19 \\
		\bottomrule
	\end{tabular}
	\label{tab:ablation_augmentation}
\end{table}

\begin{figure}[t]
	\begin{center}
		\subfigure[CUB200]
		{\includegraphics[width=.48\columnwidth]{./pics/cub200_augmentation}
			\label{figure:augmentation_a}}
		\subfigure[{\it mini}ImageNet]
		{\includegraphics[width=.48\columnwidth]{./pics/mini_augmentation}
			\label{figure:augmentation_b}}
	\end{center}
\caption{Ablation study of data augmentations. Our proposed method outperforms CEC no matter with or without AutoAug.
	} \label{figure:augmentation_ablation}
\end{figure}


	\begin{itemize}
	\item Stronger augmentations can boost the performance of FSCIL models. Both CEC and \name facilitate from AutoAug and obtain better performance. CUB200 is the particular case where AutoAug leads to inferior performance. The main reason is the domain gap between CUB200 and ImageNet, since the augmentation policy is optimized for ImageNet.
	\item Our proposed method consistently outperforms CEC on both conditions, \ie, with or without AutoAug. Besides, comparing \name + BasicAug to CEC + AutoAug, we find \name can obtain competitive results against CEC even with weaker augmentations.	
\item Our proposed method facilitates more from the augmentations, \ie, the improvement of \name is larger than CEC. 
The main reason is that CEC relies on image rotation to synthesize new classes, which may conflict with the augmentation policies defined in AutoAug.
It indicates that our proposed method can be orthogonally combined with other useful tricks to further improve the incremental performance.
\end{itemize}
