\documentclass{article}





\usepackage[preprint]{neurips_2020}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{multirow}
\usepackage{tabularx}
\usepackage[tbtags]{amsmath}
\usepackage{amssymb}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\PriorFlow}{PriorFlow}
\DeclareMathOperator{\DecoderFlow}{DecoderFlow}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{listings}
\usepackage{color}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ 
  backgroundcolor=\color{white},   basicstyle=\footnotesize,        breakatwhitespace=false,         breaklines=true,                 captionpos=b,                    commentstyle=\color{mygreen},    deletekeywords={...},            escapeinside={\%*}{*)},          extendedchars=true,              frame=single,	                   keepspaces=true,                 keywordstyle=\color{blue},       language=Python,                 morekeywords={*,...},            numbers=left,                    numbersep=5pt,                   numberstyle=\tiny\color{mygray}, rulecolor=\color{black},         showspaces=false,                showstringspaces=false,          showtabs=false,                  stepnumber=2,                    stringstyle=\color{mymauve},     tabsize=2,	                   title=\lstname                   }


\title{SoftFlow: Probabilistic Framework for\\Normalizing Flow on Manifolds}



\author{Hyeongju Kim, Hyeonseung Lee, Woo Hyun Kang, Joun Yeop Lee, Nam Soo Kim \\
  Department of Electrical and Computer Engineering and INMC,\\
  Seoul National University,\\
  Seoul, South Korea \\
  \texttt{\{hjkim, hslee, whkang, jylee\}@hi.snu.ac.kr}, \texttt{nkim@snu.ac.kr}
}


\begin{document}

\maketitle

\begin{abstract}
 Flow-based generative models are composed of invertible transformations between two random variables of the same dimension. Therefore, flow-based models cannot be adequately trained if the dimension of the data distribution does not match that of the underlying target distribution. In this paper, we propose SoftFlow, a probabilistic framework for training normalizing flows on manifolds. To sidestep the dimension mismatch problem, SoftFlow estimates a conditional distribution of the perturbed input data instead of learning the data distribution directly. We experimentally show that SoftFlow can capture the innate structure of the manifold data and generate high-quality samples unlike the conventional flow-based models. Furthermore, we apply the proposed framework to 3D point clouds to alleviate the difficulty of forming thin structures for flow-based models. The proposed model for 3D point clouds, namely SoftPointFlow, can estimate the distribution of various shapes more accurately and achieves state-of-the-art performance in point cloud generation.
\end{abstract}


\section{Introduction}
Ever since \citet{dinh2014nice} first introduced Non-linear Independent Component Estimation (NICE) that exploits a change of variables for density estimation, flow-based generative models have been widely studied and have shown competitive performance in many applications such as image generation~\citep{kingma2018glow}, speech synthesis~\citep{prenger2019waveglow,kim2018flowavenet}, video prediction~\citep{kumar2019videoflow} and machine translation~\citep{ma2019flowseq}. With this success, flow-based models are considered a potent technique for unsupervised learning due to their attractive merits: (i) exact log-likelihood evaluation, (ii) efficient synthetic data generation, and (iii) well-structured latent variable space. These properties enable the flow-based model to learn complex dependencies within high-dimensional data, generate a number of synthetic samples in real-time, and learn a semantically meaningful latent space which can be used for downstream tasks or interpolation between data points.

There also have been some theoretical developments as well as various application of flow-based models in recent years. For example, unlike the conventional flow-based models which typically perform dequantization by adding uniform noise to discrete data points~(e.g., image) as a pre-process for the change of variable formula~\citep{dinh2016density,papamakarios2017masked}, Flow++~\citep{ho2019flow++} proposed to leverage a variational dequantization technique to provide more natural and smoother density approximator of discrete data. Another example is a continuous normalizing flow~(CNF)~\citep{chen2018neural, grathwohl2018ffjord}. While discrete flow-based models adopt a restricted architecture for ease of computing the determinant of the Jacobian, CNFs impose no limits on the choice of model architectures since the objective function of CNFs can be efficiently calculated via Hutchinson's estimator~\citep{hutchinson1990stochastic}.

In this paper, we further aim to overcome another limitation of existing flow-based models, i.e., normalizing flows on manifolds. There have been some interesting works on the similar topic~\citep{gemici2016normalizing, rezende2020normalizing, brehmer2020flows} but they may not guarantee numerical stability, or require the information about manifolds~(e.g., structure and dimensionality) in advance or additional training steps to estimate it. Here, we propose a novel probabilistic framework for training normalizing flows on manifolds without any assumption and prescribed information. To begin with, we show that conventional normalizing flows cannot accurately estimate the data distribution if the data resides on a low dimensional manifold. To circumvent this issue, the proposed method, namely SoftFlow, perturbs the data with random noise sampled from different distributions and estimates the conditional distribution of the perturbed data. Unlike the conventional normalizing flows, SoftFlow is able to capture the distribution of the manifold data and synthesize high-quality samples. Furthermore, we also propose SoftPointFlow for 3D point cloud generation which relieves the difficulty of forming thin structures. We experimentally demonstrate that SoftPointFlow achieves cutting-edge performance among many point cloud generation models. Our framework is intuitive, simple and easily applicable to the existing flow-based models including both discrete and continuous normalizing flows. To encourage reproducibility, we
attach the code for both SoftFlow and SoftPointFlow used in the experiments\footnote{https://github.com/ANLGBOY/SoftFlow}.


\section{Flow-based generative model}
A normalizing flow~\citep{rezende2015variational} consists of invertible mappings from a simple latent distribution ~(e.g., isotropic Gaussian) to a complex data distribution . Let  be an invertible transformation from  to  ,  and  ( for ). Then, the log-likelihood  can be expressed in terms of the latent variable  following the change of variables theorem:


Eqs.~\eqref{eq:eq1} and \eqref{eq:eq2} suggest that the optimization of flow-based models requires the tractability of computing  and . After training, sampling process can be performed efficiently as follows:



Recently, \citet{chen2018neural} introduced a continuous normalizing flow~(CNF) where the latent variable is assumed to be time-varying and the change of its log-density follows the instantaneous change of variables formula. More specifically, continuous-time analogs of Eqs.~\eqref{eq:eq1} and \eqref{eq:eq2} can be given by


where ,  and . Unlike conventional normalizing flows, CNFs impose no restriction on the choice of model architecture since  the trace operation in Eq.~\eqref{eq:eq6} can be efficiently computed using the Hutchinson's estimator~\citep{grathwohl2018ffjord} and the sampling process is performed by reversing the time interval in Eq.~\eqref{eq:eq5}. However, due to the large computational cost of the ODE solver, CNFs usually require a long time for training~(e.g., \citet{grathwohl2018ffjord} reported that they trained the CNF on MNIST for 5 days using 6 GPUs).



\section{Normalizing flow on manifolds}
\begin{wrapfigure}{r}{0.5\textwidth}
\vskip -5pt
\includegraphics[width=1.0\linewidth]{fig1-cropped.pdf} 
\caption{Illustration of normalizing flow trained on 2D data distribution~(top) and 1D manifold data distribution~(bottom).}
\label{fig:fig1}
\end{wrapfigure}
Although normalizing flows have shown promising results on various tasks such as image generation~\citep{kingma2018glow}, voice conversion~\citep{serra2019blow} and machine translation~\citep{ma2019flowseq}, current flow-based models are not suitable for estimating the distribution of the data sitting on a lower-dimensional manifold. We note that Eqs.~\eqref{eq:eq2} and \eqref{eq:eq6} are valid only when the data distribution and the target distribution have the same dimensions. 
To demonstrate this limitation, we trained 2 FFJORD models~\citep{grathwohl2018ffjord} on different data distributions and present the results in Fig.~\ref{fig:fig1} where the left column represents the data distribution, the central column represents the scatter plot of the corresponding latent variables warped from the data points through the trained networks, and the right column denotes the target latent distribution that we initially set for training. When the dimension of the data distribution matches to that of the target distribution, the FFJORD model properly transforms the data points into the latent points. However, when the FFJORD model is trained on 1D manifold data scattered over 2D space, the distribution of the latent variables corresponding to the data points is quite different from the target latent distribution. This simple experiment exhibits the shortcoming of the current normalizing flows that they cannot expand the 1D manifold data points to the 2D shape of the target distribution since the transformations used in flow networks are homeomorphisms~\citep{dupont2019augmented}. If the transformed latent variables cannot represent the whole 2D space, it is unclear which part of the data space would match the latent points outside the lines. The observation suggests that training the normalizing flows on manifolds according to Eq.~\eqref{eq:eq2} or Eq.~\eqref{eq:eq6} may result in degenerated performance. 

\begin{wrapfigure}{r}{0.5\textwidth}
\vskip -7pt
\includegraphics[width=1.0\linewidth]{fig2-cropped.pdf} 
\caption{Example of the function that maps the contented subset of  to the manifold of .}
\label{fig:fig2}
\end{wrapfigure}
In fact, the change of variables theorem used in Eq.~\eqref{eq:eq2} is not useful anymore if the dimension of the domain is lower than the dimension of the image. Let  be a function from the contented subset  to a manifold  where  as shown in Fig.~\ref{fig:fig2}. If  and  satisfy , the -dimensional infinitesimal volume  around  is given by

where  is the  Jacobian matrix~(i.e., ),  is the infinitesimal volume around , and  represents the transpose operation~\citep{gemici2016normalizing, ben1999change}. Therefore, the log-likelihood  can be computed as follows:


Unfortunately, however, it is not straightforward to design a flow-based model according to Eq.~\eqref{eq:eq8} for a few reasons. First, transforming  to  is problematic as  cannot be invertible in general. This prevents the use of maximum likelihood since flow-based models cannot be optimized according Eq.~\eqref{eq:eq8} without knowing . Secondly, we are no longer able to employ the trick of setting the Jacobian to a lower triangular matrix as in the general flow models. It is because that  is always a symmetric matrix. This restriction may lead to  computational cost for the determinant. Finally, we need prior knowledge on the dimension of the manifold data in order to exactly determine . Otherwise, we may rely on a heuristic search for . These difficulties motivated us to come up with a novel probabilistic framework for training normalizing flows on manifolds which is presented in the following section.


\section{SoftFlow}
\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{fig3-cropped.pdf}
    \caption{Proposed technique for training a normalizing flow on manifold data.}
	\label{fig:fig3}
\end{figure}

Our ultimate goal is to appropriately train the normalizing flows on manifolds and generate realistic samples. The main cause of the aforementioned difficulties is the inherent nature of normalizing flows that the network output is homeomorphic to the input. To bridge the gap between the dimensions of the data and the target latent variable, we propose to estimate a conditional distribution of the perturbed data. The key idea is to add noise sampled from a randomly selected distribution and use the distribution parameters as a condition. For implementation, we perform the following steps for the -th data point : First, we sample a random value  from the uniform distribution , and set the noise distribution to  where . Next, we sample a noise vector  from  and obtain the perturbed data point  by adding  to . Note that now the distribution of  is not confined to a low dimensional manifold due to the addition of . Let  be the flow transformation from the latent variable  to ~(i.e., ), then the final objective function is given by

A summary of the proposed training procedure is illustrated in Fig.\ref{fig:fig3}. Since the support of the perturbed data distribution spans the entire dimensions of the data space, the normalizing flow on manifolds can be reliably trained according to Eq.~\eqref{eq:eq9}. In addition, during training, the flow networks observe various distributions with different volumes and learn to transform the randomly perturbed data points into the latent variables properly. This enables the flow networks to understand and generalize the relation between the shape of data distributions and the noise distribution parameters. As a result, we can synthesize a realistic sample  by setting  to a small value or even zero as follows:


Furthermore, it is obvious that the method can be extended to any CNF by adopting the following dynamics:


The proposed framework, namely , provides a new way to exploit a normalizing flow for manifold data. SoftFlow overcomes the dimension mismatch by estimating a perturbed data distribution which is conditioned on noise parameters. Both the training and sampling processes can be easily implemented within the existing flow-based frameworks.


\subsection{Experiments on artificial data}
\begin{figure}[t]
	\centering
	\hspace*{-0.57in}
	\includegraphics[width=0.75\linewidth]{fig4-cropped.pdf}
    \caption{Samples from SoftFlow, Glow, and FFJORD trained on 5 different distributions.}
	\label{fig:fig4}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.75\linewidth]{fig5-cropped.pdf}
    \caption{Examples of synthetic data points sampled from SoftFlow with different values of .}
	\label{fig:fig5}
\end{figure}


We conducted a set of experiments in order to validate the proposed framework. To implement SoftFlow within the FFJORD architecture, we augmented another dimension for a noise distribution parameter to the conditioning networks.
During training, we sampled  from  and perturbed each data point  by adding  which was drawn from . We scaled up  by multiplying 20 to get  and passed  to the CNF networks for conditioning. 
We employed the same way of conditioning time  in FFJORD for conditioning . 
SoftFlow and FFJORD were trained on the data sampled from 5 different distributions\footnote{The implementation of the data distributions can be found in our code.}\footnote{We provide the code for the 2sines and target distributions in Appendix A.} using the Adam optimizer~\citep{kingma2014adam} with learning rate  for 36K iterations. 
We also trained a 100-layer Glow model for 100K iterations. 
All the distribution shapes were set to be composed of only 1D lines to exclude volume components.

The generative performance of SoftFlow, Glow, and FFJORD is shown in Fig.~\ref{fig:fig4}. For sampling,  in SoftFlow was set to zero. We can observe that Glow showed the worst performance in most distributions, and some parts of the sample clusters generated by FFJORD failed to fit the data distribution. Especially in the case of the circles distribution, both Glow and FFJORD generated poor samples which were scattered around the circles and formed a curved line connecting the inner and outer circles.
The results demonstrate that Glow and FFJORD suffer from difficulties in estimating the distribution of the manifold data and cannot synthesize appropriate samples that agree with the data distribution. In contrast, SoftFlow is optimized according to the adequate objective function for manifold data. As a result, SoftFlow is capable of generating high-quality samples that follow the data distribution quite perfectly. 

To examine how well the proposed model understands and generalizes the relation between the shape of a distribution and a noise distribution parameter, we drew different groups of samples obtained from SoftFlow by varying  from 0 to 0.1. As shown in Fig.~\ref{fig:fig5}, SoftFlow generated various samples which faithfully follow different distributions as we intended. The experimental results imply that SoftFlow can be further exploited to estimate an unseen distribution or produce a plausible synthetic distribution.

 \section{SoftPointFlow}
 \begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{fig6-cropped.pdf}
    \caption{Schematic block diagram of SoftPointFlow.}
	\label{fig:fig6}
\end{figure}
3D point clouds are a compact representation of geometric details which leverage sparsity of the data. Point clouds are becoming popular and attractive since they are processed by simple geometric operations and can be efficiently acquired by using various range-scanning devices such as LiDARs. In light of the benefits of point clouds, some recent works have proposed generative models for point clouds~\citep{yang2019pointflow,achlioptas2017learning,groueix2018papier}. However, generative modelling of point clouds is still a challenging task due to the high complexity of the space of point clouds. PointFlow~\citep{yang2019pointflow} is the current state-of-the-art model for point cloud generation but still had difficulty forming thin structures. We assume that the difficulty stems from the inability of normalizing flows to estimate the density on lower-dimensional manifolds, and propose SoftPointFlow to mitigate the issue by applying the SoftFlow technique to PointFlow.

The overall architecture of SoftPointFlow\footnote{The CNF networks in PointFlow are replaced with discrete normalizing flows for two reasons: (1) slow convergence of the CNF networks, and (2) validation of the proposed framework on discrete normalizing flows.} is shown in Fig.~\ref{fig:fig6}. SoftPointFlow models two-level hierarchical distributions of shape and points, following the same training framework of PointFlow. Given a point set  consisting of  points, we first encode  into a latent variable  using the reparameterization trick~\citep{kingma2013auto}. The encoder employs the same architecture as in PointFlow. We provide a more expressive and learnable prior for  by employing , a Glow-like architecture for estimating the likelihood of . Each  is randomly perturbed as follows:
 
 
 The perturbed point  goes through  to compute the conditional likelihood given  and .  adopts an autoregressive function for flow transformation~(AR Layer) which offers parallel computation for training. Even though AR Layer requires serial operations for sampling, the sampling speed will not be degraded significantly since AR Layer only processes a 3-dimensional vector. Our final objective function  is given as follows:
 
where , , and  represents the entropy. 

\subsection{Experiments on point clouds}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{fig7-cropped.pdf}
    \caption{Examples of point clouds generated by PointFlow and SoftPointFlow. From left to right: reconstructed samples of seen data, reconstructed samples of unseen data, and synthetic samples.}
	\label{fig:fig7}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.85\linewidth]{fig8-cropped.pdf}
    \caption{Reconstructed samples transformed from different latent distributions.}
	\label{fig:fig8}
\end{figure}
We conducted a set of experiments using the ShapeNet Core dataset (v2)~\citep{chang2015shapenet} to evaluate the proposed framework for 3D point clouds. Three different categories were used for the experiments: airplane, chair, and car. We followed the same configuration for the  training and test sets as in ~\citet{yang2019pointflow}, and used 5K points per shape in the training set to construct the validation set. We trained SoftPointFlow for 15K epochs with a batch size of 128 using four 2080-Ti GPUs. The initial learning rate of the Adam optimizer was set to  and decayed by half after every 5K epochs. Each point set  was compoesd of 2048 points~().


SoftPointFlow was built on two discrete normalizing flow networks,  and .  consisted of 12 flow blocks of an actnorm, an invertible 1x1 convolution, and an affine coupling layer~(). For the affine coupling layer,  employed 4 convolution layers with gated \texttt{tanh} nonlinearities as well as residual connections and skip connections with 256 channels. The latent variable  was squeezed to have 8 channels before going through  and 2 of the channels were factored out after every 4 flow blocks following the multi-scale architecture~\citep{dinh2016density}. On the other hand,  consisted of 9 flow blocks of an actnorm, an invertible 1x1 convolution, and an autoregressive~(AR) layer~(). Each AR layer employed 3 linear layers with gated \texttt{tanh} units. We set the number of channels in the linear and gate layers to 256. A concatenated vector of the input point and the noise parameter was passed to the linear layers. Also, the latent variable  was used as a global condition by going through the gate layers. During training,  was sampled from  and scaled up to have a maximum value 2 for the AR layers. For sampling,  was set to zero.

We report some samples generated by PointFlow and SoftPointFlow in Fig.~\ref{fig:fig7}. The axis ratio was adjusted to highlight the difference between the samples. We can observe that PointFlow generated blurry samples that failed to form thin structures such as chair legs or wing tips. In contrast, SoftPointFlow captured fine details of an object well and produced high-quality samples. We provide various samples generated by each model in Appendices B and C. 

What would happen if we sample latent variables from  with different values of , and transform them into the data space? If the latent variable is sampled from the vicinity of high density~(i.e., a low value of ), we can expect that the corresponding points in the data space would be focused on the main part of an object~(e.g., a chair body). In the opposite case, the points may be gathered around the thin structure~(e.g., chair legs). To investigate the representations that each model learned, we generated various point sets by varying , and report the results in Fig.~\ref{fig:fig8}. The overall tendency is similar to what we expected. However, as  increases, we observe that PointFlow failed to capture an X-shaped structure beneath the chair body while SoftPointFlow produced points that form the X shape. Also, as  decreases, the samples generated by PointFlow are concentrated on the center of the chair while the point clouds of SoftPointFlow well preserve the whole structure. The results demonstrate that SoftPointFlow learns more desirable features from manifold data and is robust to the variance of latent variables.

\begin{wraptable}{r}{0.5\linewidth}
\vspace{-15pt}
\caption{Generation results on 1-NNA~(\%). Lower is better.}
\vspace{7pt}
\label{tab:table1}
\centering
\begin{tabular}{@{}clll@{}}
\toprule
~~Category                  & \multicolumn{1}{c}{Model} & \multicolumn{1}{c}{CD} & \multicolumn{1}{c}{EMD} \\ \midrule
\multirow{4}{*}{Airplane} & l-GAN (EMD)               & 87.65                  & 85.68                   \\
                          & PC-GAN                    & 94.35                  & 92.32                   \\
                          & PointFlow                 & 75.68                  & 75.06                   \\
                          & SoftPointFlow             & \textbf{70.92}         & \textbf{69.44}          \\ \midrule
\multirow{4}{*}{Chair}    & l-GAN (EMD)               & 64.73                  & 65.56                   \\
                          & PC-GAN                    & 76.03                  & 78.37                   \\
                          & PointFlow                 & 60.88                  & \textbf{59.89}          \\
                          & SoftPointFlow             & \textbf{59.95}         & 63.51                   \\ \midrule
\multirow{4}{*}{Car}      & l-GAN (EMD)               & 69.74                  & 68.32                   \\
                          & PC-GAN                    & 92.19                  & 90.87                   \\
                          & PointFlow                 & \textbf{60.65}         & \textbf{62.36}          \\
                          & SoftPointFlow             & 62.63                  & 64.71                   \\ \bottomrule
\end{tabular}
\end{wraptable}
In order to compare SoftPointFlow with other generative models, we measured 1-nearest neighbor accuracy~(1-NNA) of SoftPointFlow. The 1-NNA represents the leave-one-out accuracy of the 1-NN classifier and its ideal score is 50\%~\citep{lopez2016revisiting}. To compute the 1-NNA, two different distance metrics, Chamfer distance (CD) and earth mover’s distance (EMD), can be employed to measure the similarity between point clouds. The generation results on 1-NNA are shown in Table~\ref{tab:table1}. The results of l-GAN~\citep{achlioptas2017learning}, PC-GAN~\citep{li2018point} and PointFlow are taken from \citet{yang2019pointflow}. 
In all categories, SoftPointFlow achieved the significantly better results than the GAN-based models. Compared to PointFlow, SoftPointFlow showed the competitive performance on the airplane and chair data sets, and recorded the slightly lower scores on the car data set. Since the proportion of thin structures in the car data set is relatively low, we believe the results still support the validity of the proposed framework.

\begin{wrapfigure}{r}{0.5\textwidth}
	\vskip -10pt
	\centering
	\includegraphics[width=1.0\linewidth]{fig9-cropped.pdf}
    \caption{Results on preference test.}
	\label{fig:fig9}
\end{wrapfigure}
We also conducted a preference test to evaluate the perceptual quality of samples. We randomly selected 60 point clouds for each category~(airplane, chair, and car) and obtained the reconstructed point clouds from PointFlow and SoftPointFlow. We asked 31 participants to assess which sample is more similar to the reference and better in quality. Each question presented a reference point cloud and two reconstructed point clouds as shown in the left and center columns of Fig.~\ref{fig:fig7} but the order was random. The results are shown in Fig.~\ref{fig:fig9}. Surprisingly, SoftPointFlow received better scores than PointFlow in all cases. In particular, SoftPointFlow outperformed by a large margin in the airplane shapes and the seen chair shapes. The overall results demonstrate that the proposed framework is considerably useful and suitable for modelling generative flows on point clouds. 


\section{Conclusion}
In this paper, we have introduced a novel probabilistic framework, SoftFlow, for training a normalizing flow on manifolds. We experimentally demonstrated that SoftFlow is capable of capturing the innate structure of the manifold data and produces high-quality samples while the current flow-based models cannot. Also, we have successfully applied the proposed framework to point clouds to overcome the difficulty of modelling thin structures. Our generative model, SoftPointFlow, produced point clouds that describe more exactly the details of an object and achieved state-of-the-art performance for point cloud generation. We believe that our framework can be further improved by theoretically identifying which noise distribution is more useful for training or by searching an architecture to leverage noise parameters efficiently.


\section*{Broader Impact}
This paper introduces a new way of designing a generative model for manifold data. This paper will motivate other researchers and engineers to employ the proposed framework for various applications. Like other generative models, the proposed model could produce biased samples if the training set is not properly set up. However, we believe that this paper will not cause a bad influence to the society in general use.


\begin{ack}
This work was supported by Samsung Research Funding Center of Samsung Electronics under Project Number SRFC-IT1701-04.
\end{ack}







\bibliography{neurips_2020}
\bibliographystyle{icml2020}

\newpage

\appendix


\section{Code for the 2-sines and target distributions}
\label{appendix:a}

\begin{lstlisting}
import numpy as np

def get_data_batch(bsz, dist):
    # bsz: batch size
    
    rng = np.random.RandomState()
    
    if dist == "2-sines": 
        x = (rng.rand(bsz) - 0.5) * 2 * np.pi
        u = (rng.binomial(1, 0.5, bsz) - 0.5) * 2
        y = u * np.sin(x) * 2.5
        
    elif dist == "target":
        shapes = np.random.randint(7, size=bsz)
        mask = []
        for i in range(7):
            mask.append((shapes==i) * 1.) # boolean to float
            
        theta = np.linspace(0, 2 * np.pi, bsz, endpoint=False)

        x = (mask[0] + mask[1] + mask[2]) * (rng.rand(bsz) - 0.5) * 4 +\
         (-mask[3] + 0 * mask[4] + mask[5]) * 2 * np.ones(bsz) +\
         mask[6] * np.cos(theta)

        y = (-mask[0] + 0 * mask[1] + mask[2]) * 2 * np.ones(bsz) +\
        (mask[3] + mask[4] + mask[5]) * (rng.rand(bsz) -0.5) * 4 +\
         mask[6] * np.sin(theta)
        
    return np.stack((x, y), 1)
\end{lstlisting}

\newpage

\section{Examples of generated point clouds}
\label{appendix:b}
\subsection{Reconstruction of seen data}
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{appendix_recon_seen-cropped.pdf}
    \caption{Reconstructed point clouds of seen data.}
	\label{fig:appendix_fig_1}
\end{figure}

\newpage
\subsection{Reconstruction of unseen data}
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{appendix_recon_unseen-cropped.pdf}
    \caption{Reconstructed point clouds of unseen data.}
	\label{fig:appendix_fig_2}
\end{figure}

\newpage
\subsection{Generation}
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{appendix_sample-cropped.pdf}
    \caption{Synthetic point clouds generated by SoftPointFlow.}
	\label{fig:appendix_fig_3}
\end{figure}

\newpage

\section{Point clouds generated by PointFlow and SoftPointFlow (original ratio)}
\label{appendix:c}
\subsection{Reconstruction of seen data}
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{appendix_recon_seen_orig-cropped.pdf}
    \caption{Reconstructed point clouds of seen data.}
	\label{fig:appendix_fig_4}
\end{figure}

\newpage
\subsection{Reconstruction of unseen data}
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{appendix_recon_unseen_orig-cropped.pdf}
    \caption{Reconstructed point clouds of unseen data.}
	\label{fig:appendix_fig_5}
\end{figure}

\newpage
\subsection{Generation}
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{appendix_sample_orig-cropped.pdf}
    \caption{Synthetic point clouds generated by SoftPointFlow.}
	\label{fig:appendix_fig_6}
\end{figure}
\end{document}