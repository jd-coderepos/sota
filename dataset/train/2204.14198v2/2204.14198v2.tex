\newcommand{\expect}[2]{\mathds{E}_{{#1}} \left[ {#2} \right]}
\newcommand{\myvec}[1]{\boldsymbol{#1}}
\newcommand{\myvecsym}[1]{\boldsymbol{#1}}
\newcommand{\vx}{\myvec{x}}
\newcommand{\vy}{\myvec{y}}
\newcommand{\vz}{\myvec{z}}
\newcommand{\vtheta}{\myvecsym{\theta}}
\newcommand{\videotextpairs}{Video \& Text Pairs}
\newcommand{\shortvideotextpairs}{VTP}
\newcommand{\imagetextpairs}{Long Text \& Image Pairs}
\newcommand{\shortimagetextpairs}{LTIP}
\newcommand{\shortmmw}{M3W}

\newcommand{\dev}{\textsc{dev}}
\newcommand{\standard}{\textsc{hidden}}
\newcommand{\offbeat}{\textsc{offbeat}}
\newcommand{\devmultibenchmark}{\dev~multi-benchmark}
\newcommand{\standardmultibenchmark}{\standard~multi-benchmark}
\newcommand{\offbeatmultibenchmark}{\offbeat~multi-benchmark}

\newcommand{\metadevquery}{\emph{validation query} subset}
\newcommand{\metadevsupport}{\emph{validation support} subset}
\newcommand{\metadevsubsets}{\emph{validation} subsets}
\newcommand{\metatestquery}{\emph{test query} subset}
\newcommand{\metatestsupport}{\emph{test support} subset}


\newcommand{\metadevqueryshort}{\emph{validation query}}
\newcommand{\metadevsupportshort}{\emph{validation support}}
\newcommand{\metadevsubsetsshort}{\emph{validation} subsets}
\newcommand{\metatestqueryshort}{\emph{test query}}
\newcommand{\metatestsupportshort}{\emph{test support}}


\newcommand{\newreferencetoappendix}[1]{#1}

\newcommand{\maintoappref}[1]{\ifwithappendix\ref{#1}\else\ref*{#1}\fi}

\newcommand{\ifNewIntro}[2]{#1}  


\definecolor{greencode}{RGB}{13, 144, 79}


\newcommand{\teaserimheight}{1.5cm}
\newcommand{\teasertextwidth}{1.5cm}
\newcommand{\teasertext}[1]{
    \begin{minipage}[b][\teaserimheight][c]{\teasertextwidth}
    \tiny
    \centering
    #1
    \end{minipage}
}
\setlength{\fboxsep}{0pt}
\setlength{\fboxrule}{0.5pt}
\newcommand{\teaserhspace}{\hspace{0.15cm}}
\newcommand{\teaserarrow}{\begin{teaserarrowbox} \centering  \end{teaserarrowbox}}


\definecolor{llgray}{RGB}{243,243,243}
\definecolor{llpink}{RGB}{255,226,226}

\newtcolorbox{teaserpromptbox}{
    enhanced,
    boxsep=2pt,left=0pt,right=0pt,top=0pt,bottom=0pt,
    width=0.77\textwidth,
    colback={llgray},
    baseline=3.5mm,
    box align=center,
    boxrule=0.5pt,
    nobeforeafter
}
\newtcolorbox{teaseroutputbox}{
    enhanced,
    boxsep=2pt,left=0pt,right=0pt,top=0pt,bottom=0pt,
    width=0.17\textwidth,
    colback={llpink},
    baseline=3.5mm,
    box align=center,
    boxrule=0.5pt,
    nobeforeafter
}
\newtcolorbox{teaserarrowbox}{
    enhanced,
    boxsep=2pt,left=0pt,right=0pt,top=0pt,bottom=0pt,
    width=0.04\textwidth,
    colback={white},
    colbacklower=white,
    colframe=white,
    baseline=3.5mm,
    box align=center,
    boxrule=0pt,
    frame hidden,
    nobeforeafter
}



\newcommand{\teaserdialogueimheight}{3cm}
\newcommand{\teaserdialogueboxrule}{0pt}

\newtcolorbox{teaserdialogueenvelope}{
    enhanced,
    boxsep=6pt,left=0pt,right=0pt,top=0pt,bottom=0pt,
    width=\textwidth,
    colback=white,
    colframe=black,
    baseline=0mm,
    boxrule=0.85pt,
    box align=center,
    nobeforeafter
}
\newtcolorbox{teaserdialogueuserbox}{
    enhanced,
    boxsep=4pt,left=0pt,right=0pt,top=0pt,bottom=0pt,
    hbox,
    colback={llgray},
    baseline=0mm,
    boxrule=\teaserdialogueboxrule,
    frame hidden,
    nobeforeafter
}
\newtcolorbox{teaserdialogueuserboxwrap}{
    enhanced,
    boxsep=4pt,left=0pt,right=0pt,top=0pt,bottom=0pt,
    width=0.75\textwidth,
    colback={llgray},
    baseline=0mm,
    boxrule=\teaserdialogueboxrule,
    frame hidden,
    nobeforeafter
}
\newtcolorbox{teaserdialogueflamingobox}{
    enhanced,
    boxsep=4pt,left=0pt,right=0pt,top=0pt,bottom=0pt,
    hbox,
    colback={llpink},
    baseline=0mm,
    boxrule=\teaserdialogueboxrule,
    frame hidden,
    nobeforeafter
}
\newtcolorbox{teaserdialogueflamingoboxwrap}{
    enhanced,
    boxsep=4pt,left=0pt,right=0pt,top=0pt,bottom=0pt,
    width=0.75\textwidth,
    colback={llpink},
    baseline=0mm,
    boxrule=\teaserdialogueboxrule,
    frame hidden,
    nobeforeafter
}

\newcommand{\dialogueavatarsep}{\hspace{0.1cm}}
\newcommand{\useravatar}{\includegraphics[height=0.6cm]{figures/PersonPaperColor.pdf}}
\newcommand{\flamingoavatar}{\includegraphics[height=0.6cm]{figures/FlamingoPaperColor.pdf}}

\newcommand{\userchat}[1]{\begin{flushright}
    \begin{teaserdialogueuserbox} #1 \end{teaserdialogueuserbox}\dialogueavatarsep{}\useravatar{}
    \end{flushright}}

\newcommand{\userchatw}[1]{\begin{flushright}
    \begin{teaserdialogueuserboxwrap} #1 \end{teaserdialogueuserboxwrap}\dialogueavatarsep{}\useravatar{}
    \end{flushright}}

\newcommand{\flamingochatw}[1]{\begin{flushleft}
    \flamingoavatar{}\dialogueavatarsep{}\begin{teaserdialogueflamingoboxwrap} #1 \end{teaserdialogueflamingoboxwrap}
    \end{flushleft}
}

\newcommand{\flamingochat}[1]{\begin{flushleft}
    \flamingoavatar{}\dialogueavatarsep{}\begin{teaserdialogueflamingobox} #1 \end{teaserdialogueflamingobox}
    \end{flushleft}
}

\definecolor{shadecolor}{rgb}{0.97, 0.97, 0.97}
\newcommand{\chatsep}{\vspace{-0.35cm}}

\begin{figure}
\centering
\vspace{-0.6cm}


\begin{teaserpromptbox} \centering Input Prompt \end{teaserpromptbox}
\begin{teaserarrowbox} {} \end{teaserarrowbox}
\begin{teaseroutputbox} \centering \small \flamingoemojiavatar{} Completion \end{teaseroutputbox}
\\

\begin{teaserpromptbox} \centering
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser1a.png}}
    \teasertext{This is a chinchilla. They are mainly found in Chile.}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser1b.png}}
    \teasertext{This is a shiba. They are very popular in Japan.}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser1c.png}}
    \teasertext{This is}
\end{teaserpromptbox}
\teaserarrow{}
\begin{teaseroutputbox} \centering \bf
    \teasertext{a flamingo. They are found in the Caribbean and South America.}
\end{teaseroutputbox}
\\

\begin{teaserpromptbox} \centering
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser2a.png}}
    \teasertext{What is the title of this painting? Answer: The Hallucinogenic Toreador.}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser2b.png}}
    \teasertext{Where is this painting displayed? Answer: Louvres Museum, Paris.}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser2c.png}}
    \teasertext{What is the name of the city where this was painted? Answer:}
\end{teaserpromptbox}
\teaserarrow{}
\begin{teaseroutputbox} \centering \bf
    \teasertext{Arles.}
\end{teaseroutputbox}
\\

\begin{teaserpromptbox} \centering
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser3a.png}}
    \teasertext{Output: "Underground"}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser3b.png}}
    \teasertext{Output: "Congress"}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser3c.png}}
    \teasertext{Output:}
\end{teaserpromptbox}
\teaserarrow{}
\begin{teaseroutputbox} \centering \bf
    \teasertext{"Soulomes"}
\end{teaseroutputbox}
\\

\begin{teaserpromptbox} \centering
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser4a.png}}
    \teasertext{2+1=3}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser4b.png}}
    \teasertext{5+6=11}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser4c.png}}
    \teasertext{\phantom{?}}
\end{teaserpromptbox}
\teaserarrow{}
\begin{teaseroutputbox} \centering \bf
    \teasertext{3x6=18}
\end{teaseroutputbox}
\\

\begin{teaserpromptbox} \centering
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser5a.png}}
    \teasertext{Output: A propaganda poster depicting a cat dressed as French emperor Napoleon holding a piece of cheese.}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser5b.png}}
    \teasertext{Output: A pink room with a flamingo pool float.}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser5c.png}}
    \teasertext{Output:}
\end{teaserpromptbox}
\teaserarrow{}
\begin{teaseroutputbox} \centering \bf
    \teasertext{A portrait of Salvador Dali with a robot head.}
\end{teaseroutputbox}
\\

\begin{teaserpromptbox} \centering
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser6a.png}}
    \teasertext{Les sanglots longs des violons de l’automne blessent mon coeur d’une langueur monotone.}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser6b.png}}
    \teasertext{Pour qui sont ces serpents qui sifflent sur vos têtes?}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser6c.png}}
    \teasertext{\phantom{?}}
\end{teaserpromptbox}
\teaserarrow{}
\begin{teaseroutputbox} \centering \bf
    \teasertext{Je suis un cœur qui bat pour vous.}
\end{teaseroutputbox}
\\

\begin{teaserpromptbox} \centering
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser7a.png}}
    \teasertext{pandas: 3}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser7b.png}}
    \teasertext{dogs: 2}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/teaser7c.png}}
    \teasertext{\phantom{?}}
\end{teaserpromptbox}
\teaserarrow{}
\begin{teaseroutputbox} \centering \bf
    \teasertext{giraffes: 4}
\end{teaseroutputbox}
\\

\begin{teaserpromptbox} \centering
    \includegraphics[height=\teaserimheight]{figures/teaserpics/placeholder.png}
    \teasertext{I like reading}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/shakespeare.png}}
    \teasertext{, my favourite play is Hamlet. I also like}
    \teaserhspace{}
    \fbox{\includegraphics[height=\teaserimheight]{figures/teaserpics/obama.png}}
    \teasertext{, my favorite book is}
\end{teaserpromptbox}
\teaserarrow{}
\begin{teaseroutputbox} \centering \bf
    \teasertext{Dreams from my Father.}
\end{teaseroutputbox}
\\


\newcommand{\whitebox}{\hfill\textcolor{white}{\rule[0.46875mm]{0.84375mm}{1.3125mm}}\hfill}
\newcommand{\filmbox}[1]{\setlength{\fboxsep}{0pt}\colorbox{black}{\begin{minipage}{\teaserimheight}
            \rule{0mm}{2.25mm}\whitebox\whitebox\whitebox\whitebox\whitebox \whitebox\whitebox\whitebox\whitebox\null\\\null\hfill\includegraphics[width=\teaserimheight]{#1}\hfill\null\\\null\whitebox\whitebox\whitebox\whitebox\whitebox \whitebox\whitebox\whitebox\whitebox\null
        \end{minipage}}}


\begin{teaserpromptbox} \centering
    \fbox{\includegraphics[height=\teaserimheight]{figures/videoframes/imvid_shot_0002.png}}
    \hspace{-0.20cm}
    \fbox{\includegraphics[height=\teaserimheight]{figures/videoframes/imvid_shot_0003.png}}
    \hspace{-0.20cm}
    \fbox{\includegraphics[height=\teaserimheight]{figures/videoframes/imvid_shot_0004.png}}
    \hspace{-0.20cm}
    \fbox{\includegraphics[height=\teaserimheight]{figures/videoframes/imvid_shot_0005.png}}
    \hspace{-0.20cm}
    \fbox{\includegraphics[height=\teaserimheight]{figures/videoframes/imvid_shot_0006.png}}
    \teasertext{What happens to the man after hitting the ball? Answer:}
\end{teaserpromptbox}
\teaserarrow{}
\begin{teaseroutputbox} \centering \bf
    \teasertext{he falls down.}
\end{teaseroutputbox}
\\


\scalebox{0.545}{
\begin{minipage}[b]{0.45\textwidth}
\begin{teaserdialogueenvelope}
\userchat{\fbox{\includegraphics[height=3.2cm]{figures/moonbears.jpeg}}}
\chatsep{}
\flamingochatw{This is a picture of two teddy bears on the moon.}
\chatsep{}
\userchat{What are they doing?}
\chatsep{}
\flamingochatw{They are having a \mbox{conversation}.}
\chatsep{}
\userchat{What object are they using?}
\chatsep{}
\flamingochat{It looks like a computer.}
\chatsep{}
\userchat{Is this surprising?}
\chatsep{}
\flamingochat{Yes, it is surprising.}
\chatsep{}
\userchatw{Why is this picture surprising to you?}
\chatsep{}
\flamingochatw{I think it is surprising because teddy bears are not usually found on the moon.}
\end{teaserdialogueenvelope}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
\begin{teaserdialogueenvelope}
\userchat{\fbox{\includegraphics[height=1.8cm]{figures/teaserpics/flamingo_icon.png}}}
\chatsep{}
\userchat{\fbox{\includegraphics[height=1.8cm]{figures/teaserpics/flamingo_picture.png}}}
\chatsep{}
\userchat{\fbox{\includegraphics[height=1.8cm]{figures/teaserpics/flamingo_3d.png}}}
\chatsep{}
\userchatw{What is the common thing about \mbox{these three images?}}
\chatsep{}
\flamingochat{They are all flamingos.}
\chatsep{}
\userchatw{What is the difference between \mbox{these three images?}}
\chatsep{}
\flamingochatw{The first one is a cartoon, the second one is a real flamingo, and the third one is a 3D model of a flamingo.}
\end{teaserdialogueenvelope}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
\begin{teaserdialogueenvelope}
\userchat{\fbox{\includegraphics[height=3.8cm]{figures/appleipod.png}}}
\chatsep{}
\flamingochatw{This is an apple with a sticker on it.}
\chatsep{}
\userchat{What does the sticker say?}
\chatsep{}
\flamingochat{The sticker says "iPod".}
\chatsep{}
\userchat{Where is the photo taken?}
\chatsep{}
\flamingochatw{It looks like it's taken in a \mbox{backyard}.}
\chatsep{}
\userchatw{Do you think it is printed or \mbox{handwritten?}}
\chatsep{}
\flamingochat{It looks like it's handwritten.}
\chatsep{}
\userchat{What color is the sticker?}
\chatsep{}
\flamingochat{It's white.}
\end{teaserdialogueenvelope}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
\begin{teaserdialogueenvelope}
\userchat{\fbox{\includegraphics[height=1.8cm]{figures/boston_skyline.png}}}
\chatsep{}
\flamingochatw{This is a cityscape. It looks like Chicago.}
\chatsep{}
\userchatw{What makes you think this is Chicago?}
\chatsep{}
\flamingochatw{I think it's Chicago because of the Shedd Aquarium in the background.}
\userchat{\fbox{\includegraphics[height=1.7cm]{figures/tokyo_skyline.png}}}
\chatsep{}
\userchatw{What about this one? Which city is this and what famous landmark helped you recognise the city?}
\chatsep{}
\flamingochatw{This is Tokyo. I think it's Tokyo because of the Tokyo Tower.}
\end{teaserdialogueenvelope}
\end{minipage}

}

\centering
\caption{\textbf{Selected examples of inputs and outputs obtained from \largemfull{}.}
\largem{} can rapidly adapt to various image/video understanding tasks with few-shot prompting (top).
Out of the box, \largem{} is also capable of multi-image visual dialogue (bottom). More examples in
Appendix~\maintoappref{app:qual_res}.
}
\label{fig:teaser}
\end{figure} 
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{scaling_vlm_neurips}
    \caption{\capfontsize{} \textbf{\method{} results overview.} \textit{Left}: Our largest model, dubbed \largem{}, outperforms state-of-the-art fine-tuned models on 6 of the 16 tasks we consider
    with no fine-tuning.
    For the 9 tasks with published few-shot results,
    \largem{} sets the new few-shot state of the art.
    \emph{Note:} We omit RareAct, our 16th benchmark, as it is a zero-shot benchmark with no available fine-tuned results to compare to.
    \textit{Right}: \method{} performance improves with model size and number of shots.
    \vspace*{-0.4cm}
    }
    \label{fig:results}
\end{figure}

\section{Introduction}
One key aspect of intelligence is the ability to quickly learn to perform a new task given a short instruction~\citep{griffiths2019doing,markman1989categorization}.
While initial progress has been made towards a similar capability in computer vision,
the most widely used paradigm still consists of first pretraining on a large amount of supervised data, before fine-tuning the model on the task of interest~\citep{lu2019vilbert,wang2021ufo,zellers2022merlot}.
However, successful fine-tuning often requires many thousands of annotated data points.
In addition, it often requires careful per-task hyperparameter tuning and is also resource intensive.
Recently, multimodal vision-language models trained with a contrastive objective~\citep{align,clip} have enabled zero-shot adaptation to novel tasks, without the need for fine-tuning. 
However, because these models simply provide a similarity score between a text and an image, they can only address limited use cases such as classification, where a finite set of outcomes is provided beforehand.
They crucially lack the ability to generate language, which makes them less suitable to more open-ended tasks such as captioning or visual question-answering.
Others have explored visually-conditioned language generation~\citep{wang2021simvlm,tsimpoukelli2021multimodal,cho2021unifying,wang2022unifying,xu2021vlm} but have not yet shown good performance in low-data regimes.

We introduce~\largem, a Visual Language Model (VLM) that sets a new state of the art in few-shot learning on a wide range of open-ended vision and language tasks, simply by being prompted with a few input/output examples, as illustrated in Figure~\ref{fig:teaser}.
Of the 16 tasks we consider, \largem{} also surpasses the fine-tuned state of the art on 6 tasks, despite using orders of magnitude less task-specific training data (see Figure~\ref{fig:results}).
To achieve this, Flamingo takes inspiration from recent work on large language models (LMs) which are good few-shot learners~\citep{gpt3,gopher,chinchilla,chowdhery2022palm}.
A single large LM can
achieve strong performance on many tasks using only its text interface: a few examples of a task are provided to the model as a prompt, along with a query input, and the model generates a continuation to produce a predicted output for that query.
We show that the same can be done for image and video understanding tasks such as classification, captioning, or question-answering: these can be cast as text prediction problems with visual input conditioning.
The difference from a LM is that the model must be able to ingest a multimodal prompt containing images and/or videos interleaved with text.
\methodfamily{} have this capability---they are
visually-conditioned autoregressive text generation models able to ingest a sequence of text tokens interleaved with images and/or videos, and produce text as output.
\methodfamily{} leverage two complementary pre-trained and frozen models: a vision model which can ``perceive'' visual scenes and a large LM which performs a basic form of reasoning.
Novel architecture components are added in between these models to connect them in a way that preserves the knowledge they have accumulated during computationally intensive pre-training.
\methodfamily{} are also able to ingest high-resolution images or videos thanks to a Perceiver-based~\citep{jaegle2021perceiver} architecture that can produce a small fixed number of visual tokens per image/video, given a large and variable number of visual input features.

\parbox{\textwidth}{A crucial aspect for the performance of large LMs is that they are trained on a large amount of text data.
This training provides general-purpose generation capabilities that allows these LMs to perform well when prompted with task examples.
Similarly, we demonstrate that the way we train
the \method{} models is crucial for their final performance.
They are trained on a carefully chosen \parfillskip=0pt} \newpage mixture of complementary large-scale multimodal data coming only from the web, \emph{without using any data annotated for machine learning purposes}.
After this training,
a \method{} model can be directly adapted to vision tasks via simple few-shot learning without any
task-specific tuning.




\noindent
\textbf{Contributions.}
In summary, our contributions are the following:
\textbf{(i)} We introduce the \method{} family of VLMs which can perform various multimodal tasks (such as captioning, visual dialogue, or visual question-answering) from only a few input/output examples.
Thanks to architectural innovations, the \method{} models can efficiently accept arbitrarily interleaved visual data and text as input and generate text in an open-ended manner. \textbf{(ii)} We quantitatively evaluate how~\method{} models can be adapted to various tasks via few-shot learning. We notably reserve a large set of held-out benchmarks which have not been used for validation of any design decisions or  hyperparameters of the approach. 
We use these to estimate unbiased few-shot performance.
\textbf{(iii)} \largem{} sets a new state of the art in few-shot learning on a wide array of 16 multimodal language and image/video understanding tasks.
On 6 of these 16 tasks, \largem{} also outperforms the fine-tuned state of the art despite using only 32 task-specific examples, around 1000 times less task-specific training data than the current state of the art.
With a larger annotation budget,~\largem{} can also be effectively fine-tuned to set a new state of the art on five additional challenging benchmarks: VQAv2, VATEX, VizWiz, MSRVTTQA, and HatefulMemes.







\section{Approach}
\label{sec:approach}

\begin{figure}[t!]
\includegraphics[width= \linewidth]{figures/fig2_overview_interleaved_v2.pdf}
\centering
\caption{\capfontsize{} \textbf{\method{} architecture overview.} Flamingo is a family of visual language models (VLMs) that take as input visual data interleaved with text and  produce free-form text as output. 
}
\label{fig:overview}
\end{figure}

This section describes Flamingo: a visual language model that accepts text interleaved with images/videos as input and outputs free-form text. The key architectural components shown in Figure~\ref{fig:overview}
are chosen to leverage pretrained vision and language models and bridge them effectively. 
First, the Perceiver Resampler (Section~\ref{sec:transformer_resampler}) receives spatio-temporal features from the Vision Encoder (obtained from either an image or a video) and outputs a fixed number of visual tokens.
Second, these visual tokens are used to condition the frozen LM using freshly initialised cross-attention layers (Section~\ref{sec:xattn_dense}) that are interleaved between the pretrained LM layers.
These new layers offer an expressive way for the LM to incorporate visual information for the next-token prediction task. Flamingo models the likelihood of text  conditioned on interleaved  images and videos  as follows:

where  is the -th language token of the input text,  is the set of preceding tokens,  is the set of images/videos preceding token  in the interleaved sequence and  is parametrized by a \method{} model.
The ability to handle interleaved text and visual sequences (Section~\ref{sec:multi_im_att}) makes it natural to use \method{} models for in-context few-shot learning, analogously to GPT-3 with few-shot text prompting.
The model is trained on a diverse mixture of datasets as described in Section~\ref{sec:datasets}.





\subsection{Visual processing and the Perceiver Resampler}
\label{sec:transformer_resampler}

\noindent
\textbf{Vision Encoder: from pixels to features.}
Our vision encoder is a pretrained and frozen Normalizer-Free ResNet (NFNet) \citep{nfnets} -- we use the F6 model.
We pretrain the vision encoder using a contrastive objective on our datasets of image and text pairs, using the two-term contrastive loss from \citet{clip}.
We use the output of the final stage, a 2D spatial grid of features that is flattened to a 1D sequence.
For video inputs, frames are sampled at 1 FPS and encoded independently to obtain a 3D spatio-temporal grid of features to which learned temporal embeddings are added.
Features are then flattened to 1D before being fed to the Perceiver Resampler.
More details on the contrastive model training and performance are given in Appendix~\maintoappref{app:contrastive_details} and Appendix~\maintoappref{app:contrastive_ablation}, respectively.

\noindent
\textbf{Perceiver Resampler: from varying-size large feature maps to few visual tokens.} 
This module connects the vision encoder to the frozen language model as shown in Figure~\ref{fig:overview}.
It takes as input a variable number of image or video features from the vision encoder and produces a fixed number of visual outputs (64),
reducing the computational complexity of the vision-text cross-attention.
Similar to Perceiver~\citep{jaegle2021perceiver} and DETR~\citep{carion2020end}, we learn a predefined number of latent input queries
which are fed to a Transformer and cross-attend to the visual features.
We show in our ablation studies (Section~\ref{sec:ablations}) that using such a vision-language resampler module outperforms a plain Transformer and an MLP.
We provide an illustration, more architectural details, and pseudo-code in Appendix~\maintoappref{app:transformer_resampler}.


\subsection{Conditioning frozen language models on visual representations}
\label{sec:xattn_dense}


Text generation is performed by a Transformer decoder, conditioned on the visual representations produced by the Perceiver Resampler. 
We interleave pretrained and frozen text-only LM blocks with blocks trained from scratch that cross-attend to the visual output from the Perceiver Resampler.


\begin{figure}[t]
\includegraphics[width=\linewidth]{figures/fig4_xattn_dense.pdf}
\centering
\caption{\capfontsize{} \textbf{\textsc{gated xattn-dense} layers.} To condition the LM on visual inputs, we insert new cross-attention layers between existing pretrained and frozen LM layers. The keys and values in these layers are obtained from the vision features while the queries are derived from the language inputs.
They are followed by dense feed-forward layers.
These layers are \emph{gated} so that the LM is kept intact at initialization for improved stability and performance.}
\label{fig:xattn_dense}
\end{figure}

\textbf{Interleaving new \textsc{gated xattn-dense} layers within a frozen pretrained LM.} 
We freeze the pretrained LM blocks, and insert \textit{gated cross-attention dense} blocks (Figure~\ref{fig:xattn_dense}) between the original layers, trained from scratch.
To ensure that at initialization, the conditioned model yields the same results as the original language model, we use a -gating mechanism~\citep{hochreiter1997long}.
This multiplies the output of a newly added layer by  before adding it to the input representation from the residual connection, where  is a layer-specific learnable scalar initialized to ~\cite{bachlechner2021rezero}.
Thus, at initialization, the model output matches that of the pretrained LM, improving training stability and final performance.
In our ablation studies (Section~\ref{sec:ablations}), we compare the proposed \textsc{gated xattn-dense} layers against recent alternatives~\citep{desai2021virtex,luo2022vc} and explore the effect of how frequently these additional layers are inserted to trade off between efficiency and expressivity.
See Appendix~\maintoappref{app:xattn_dense} for more details.

\textbf{Varying model sizes.}
We perform experiments across three models sizes, building on the 1.4B, 7B, and 70B parameter Chinchilla models~\citep{chinchilla}; calling them respectively \base{}, \medium{} and \largemfull{}.
For brevity, we refer to the last as \largem{} throughout the paper.
While increasing the parameter count of the frozen LM and the trainable vision-text \textsc{gated xattn-dense} modules, we maintain a fixed-size frozen vision encoder and trainable Perceiver Resampler across the different models (small relative to the full model size).
See Appendix~\maintoappref{sec:models_details} for further details.


\subsection{Multi-visual input support: per-image/video attention masking}
\label{sec:multi_im_att}


The image-causal modelling introduced in Equation~\eqref{eq:modeling} is obtained by masking the full text-to-image cross-attention matrix,
limiting which visual tokens the model sees at each text token.
At a given text token, the model attends to the visual tokens of the image that appeared just before it in the interleaved sequence, rather than to all previous images (formalized and illustrated in Appendix~\maintoappref{app:multi-visual-details}).
Though the model only \textit{directly} attends to a single image at a time, the dependency on all previous images remains via self-attention in the LM.
This single-image cross-attention scheme importantly allows the model to seamlessly generalise to any number of visual inputs, regardless of how many are used during training.
In particular, we use only up to 5 images per sequence when training on our interleaved datasets, yet our model is able to benefit from sequences of up to 32 pairs (or ``shots'') of images/videos and corresponding texts during evaluation.
We show in Section~\ref{sec:ablations} that this scheme is more effective than allowing the model to cross-attend to all previous images directly.




\subsection{Training on a mixture of vision and language datasets}
\label{sec:datasets}\label{sec:training}


We train the \method{} models on a mixture of three kinds of datasets, all scraped from the web: an interleaved image and text dataset derived from webpages, image-text pairs, and video-text pairs. 


\textbf{M3W: Interleaved image and text dataset.}
\label{sec:interleaved_datasets}
The few-shot capabilities of Flamingo models rely on training on interleaved text and image data.
For this purpose, we collect the \emph{MultiModal MassiveWeb} (\mmmw) dataset.
We extract both text and images from the HTML of approximately 43 million webpages, determining the positions of images relative to the text based on the relative positions of the text and image elements in the Document Object Model (DOM).
An example is then constructed by inserting \texttt{<image>} tags in plain text at the locations of the images on the page, and inserting a special \texttt{<EOC>} (\textit{end of chunk}) token (added to the vocabulary and learnt) prior to any image and at the end of the document.
From each document, we sample a random subsequence of  tokens and take up to the first  images included in the sampled sequence.
Further images are discarded in order to save compute.
More details are provided in Appendix~\maintoappref{app:datasets}.

\textbf{Pairs of image/video and text.}
For our image and text pairs we first leverage the ALIGN~\citep{align} dataset, composed of 1.8 billion images paired with alt-text.
To complement this dataset,  we collect our own dataset of image and text pairs targeting better quality and longer descriptions:  \shortimagetextpairs{} (\imagetextpairs) which consists of 312 million image and text pairs.
We also collect a similar dataset but with videos instead of still images: \shortvideotextpairs{} (\videotextpairs) consists of 27 million short videos (approximately 22 seconds on average) paired with sentence descriptions.
We align the syntax of paired datasets with the syntax of M3W by prepending \texttt{<image>} and appending \texttt{<EOC>} to each training caption (see Appendix~\maintoappref{app:vtp_and_itp} for details).



\textbf{Multi-objective training and optimisation strategy.}
We train our models by minimizing a weighted sum of per-dataset expected negative log-likelihoods of text, given the visual inputs:

where  and  are the -th dataset and
its weighting, respectively.
Tuning the per-dataset weights  is key to performance.
We accumulate gradients over all datasets, which we found outperforms a ``round-robin'' approach~\citep{cho2021unifying}.
We provide further training details and ablations in Appendix~\maintoappref{app:large_scale_training}.

\subsection{Task adaptation with few-shot in-context learning}
\label{sec:adapt-vlm}

Once Flamingo is trained, we use it to tackle a visual task by conditioning it on a multimodal interleaved prompt. We evaluate the ability of our models to rapidly adapt to new tasks using \textbf{in-context learning}, analogously to GPT-3~\citep{gpt3}, by interleaving support example pairs in the form of  or , followed by the query visual input, to build a prompt (details in Appendix~\maintoappref{app:in_context_eval_details}).
We perform \textbf{open-ended} evaluations using beam search for decoding, and \textbf{close-ended} evaluations using our model's log-likelihood to score each possible answer.
We explore \textbf{zero-shot generalization} by prompting the model with two text-only examples from the task, with no corresponding images.
Evaluation hyperparameters and additional details are given in Appendix~\maintoappref{app:fewshot-eval-hyper}.



\section{Experiments}
\label{sec:experiments}

Our goal is to develop models that can rapidly adapt to diverse and challenging tasks. 
For this, we consider a wide array of 16 popular multimodal image/video and language benchmarks.
In order to validate model design decisions during the course of the project, 5 of these benchmarks were used as part of our development (\dev{}) set: COCO, OKVQA, VQAv2, MSVDQA and VATEX.
Performance estimates on the \dev{} benchmarks may be biased, as a result of model selection.
We note that this is also the case for prior work which makes use of similar benchmarks to validate and ablate design decisions.
To account for this, we report performance on an additional set of 11 benchmarks,
spanning captioning, video question-answering, as well as some less commonly explored capabilities such as visual dialogue and multi-choice question-answering tasks.
The evaluation benchmarks are described in Appendix~\maintoappref{sec:eval_benchmarks}.
We keep all evaluation hyperparameters fixed across all benchmarks.
Depending on the task, we use four few-shot prompt templates we describe in more detail in 
Appendix~\maintoappref{app:fewshot-eval-hyper}.
We emphasize that \emph{we do not validate any design decisions on these 11 benchmarks} and use them solely to estimate unbiased few-shot learning performance of our models.

Concretely, estimating few-shot learning performance of
a model involves prompting it with a set of \emph{support} samples and evaluating it on a set of \emph{query} samples.
For the \dev{} benchmarks that are used both to validate design decisions and hyperparameters, as well as to report final performance, we therefore use four subsets:
\metadevsupportshort{}, \metadevqueryshort{}, \metatestsupportshort{} and \metatestqueryshort{}. 
For other benchmarks, we need only the latter two. 
We report in Appendix~\maintoappref{sec:eval_benchmarks} how we form these subsets.

We report the results of the~\method{} models on few-shot learning in Section~\ref{sec:fewshot_openended}.
Section~\ref{sec:ft_results} gives~\largem{} fine-tuned results.
An ablation study is given in Section~\ref{sec:ablations}.
Appendix~\maintoappref{app:more_performance} provides more results including \method{}'s performance on the ImageNet and Kinetics700 classification tasks, and on our contrastive model's performance.
Appendix~\maintoappref{app:qual_res} includes additional qualitative results.






\subsection{Few-shot learning on vision-language tasks}
\label{sec:fewshot_openended}


\begin{table}[t]
\resizebox{\textwidth}{!}{\begin{tabular}{ccccccccccccccccccc}
\toprule
Method                                                                        & FT                & Shot                                                                            & \rotatebox[origin=c]{90}{OKVQA \textbf{(I)}}                                                                              & \rotatebox[origin=c]{90}{VQAv2 \textbf{(I)}}                                                                                      & \rotatebox[origin=c]{90}{COCO \textbf{(I)}}                                                                                      & \rotatebox[origin=c]{90}{MSVDQA \textbf{(V)}}                                                                                & \multicolumn{1}{c}{\rotatebox[origin=c]{90}{VATEX \textbf{(V)}}}                                                             & \rotatebox[origin=c]{90}{VizWiz \textbf{(I)}}                                                                                 & \rotatebox[origin=c]{90}{Flick30K \textbf{(I)}}                                                                                 & \rotatebox[origin=c]{90}{MSRVTTQA \textbf{(V)}}                                                                               & \rotatebox[origin=c]{90}{iVQA \textbf{(V)}}                                                                                 & \multicolumn{1}{c}{\rotatebox[origin=c]{90}{YouCook2 \textbf{(V)}}}                                                       & \rotatebox[origin=c]{90}{STAR \textbf{(V)}}                                                                                & \rotatebox[origin=c]{90}{VisDial \textbf{(I)}}                                                                                     & \rotatebox[origin=c]{90}{TextVQA \textbf{(I)}}                                                                              & \rotatebox[origin=c]{90}{NextQA \textbf{(I)}}                                                                                & \rotatebox[origin=c]{90}{\phantom{a}HatefulMemes \textbf{(I)}\phantom{a}}                                                                           & \rotatebox[origin=c]{90}{RareAct \textbf{(V)}}                                             \\ \hline
\begin{tabular}[c]{@{}c@{}}Zero/Few \\ shot SOTA\end{tabular}                 & \ding{55}         & \begin{tabular}[c]{@{}c@{}}\phantom{0}\\ \phantom{0}\\ (X)\end{tabular}         & \begin{tabular}[c]{@{}c@{}}\small{[\citenum{gui2021kat}]}\\ 43.3 \\ (16)\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}\small{[\citenum{tsimpoukelli2021multimodal}]}\\ 38.2\\ (4)\end{tabular}                  & \begin{tabular}[c]{@{}c@{}}\small{[\citenum{wang2021simvlm}]}\\ 32.2\\ (0)\end{tabular}                             & \begin{tabular}[c]{@{}c@{}}\small{[\citenum{li2022blip}]}\\ 35.2\\ (0)\end{tabular}                             & -                                                                                                                & -                                                                                                                & -                                                                                                                  & \begin{tabular}[c]{@{}c@{}}\small{[\citenum{li2022blip}]}\\ 19.2\\ (0)\end{tabular}                              & \begin{tabular}[c]{@{}c@{}}\small{[\citenum{yang2021just}]}\\ 12.2\\ (0)\end{tabular}                          & -                                                                                                             & \begin{tabular}[c]{@{}c@{}}\small{[\citenum{zellers2022merlot}]}\\ 39.4\\ (0)\end{tabular}                    & \begin{tabular}[c]{@{}c@{}}\small{[\citenum{murahari2020large}]}\\ 11.6\\ (0)\end{tabular}                            & -                                                                                                              & -                                                                                                               & \begin{tabular}[c]{@{}c@{}}\small{[\citenum{clip}]}\\ 66.1\\ (0)\end{tabular}                                    & \begin{tabular}[c]{@{}c@{}}\small{[\citenum{clip}]}\\ 40.7\\ (0)\end{tabular} \\ \hline
\multirow{3}{*}{\base{}}                                                      & \ding{55}         & 0                                                                               & 41.2                                                                                                          & 49.2                                                                                                                 & 73.0                                                                                                                & 27.5                                                                                                            & 40.1                                                                                                             & 28.9                                                                                                             & 60.6                                                                                                               & 11.0                                                                                                             & 32.7                                                                                                           & 55.8                                                                                                          & 39.6                                                                                                          & 46.1                                                                                                                  & 30.1                                                                                                           & 21.3                                                                                                            & 53.7                                                                                                             & 58.4                                                                          \\
                                                                              & \ding{55}         & 4                                                                               & 43.3                                                                                                          & 53.2                                                                                                                 & 85.0                                                                                                                & 33.0                                                                                                            & 50.0                                                                                                             & 34.0                                                                                                             & 72.0                                                                                                               & 14.9                                                                                                             & 35.7                                                                                                           & 64.6                                                                                                          & 41.3                                                                                                          & 47.3                                                                                                                  & 32.7                                                                                                           & 22.4                                                                                                            & 53.6                                                                                                             & -                                                                             \\
                                                                              & \ding{55}         & 32                                                                              & 45.9                                                                                                          & 57.1                                                                                                                 & 99.0                                                                                                                & 42.6                                                                                                            & 59.2                                                                                                             & 45.5                                                                                                             & 71.2                                                                                                               & 25.6                                                                                                             & 37.7                                                                                                           & 76.7                                                                                                          & 41.6                                                                                                          & 47.3                                                                                                                   & 30.6                                                                                                           & 26.1                                                                                                            & 56.3                                                                                                             & -                                                                             \\ \hline
\multirow{3}{*}{\medium{}}                                                    & \ding{55}         & 0                                                                               & 44.7                                                                                                          & 51.8                                                                                                                 & 79.4                                                                                                                & 30.2                                                                                                            & 39.5                                                                                                             & 28.8                                                                                                             & 61.5                                                                                                               & 13.7                                                                                                             & 35.2                                                                                                           & 55.0                                                                                                          & 41.8                                                                                                          & 48.0                                                                                                                  & 31.8                                                                                                           & 23.0                                                                                                            & 57.0                                                                                                             & 57.9                                                                          \\
                                                                              & \ding{55}         & 4                                                                               & 49.3                                                                                                          & 56.3                                                                                                                 & 93.1                                                                                                                & 36.2                                                                                                            & 51.7                                                                                                             & 34.9                                                                                                             & 72.6                                                                                                               & 18.2                                                                                                             & 37.7                                                                                                           & 70.8                                                                                                          & {\ul \textbf{42.8}}                                                                                                          & 50.4                                                                                                                  & 33.6                                                                                                           & 24.7                                                                                                            & 62.7                                                                                                             & -                                                                             \\
                                                                              & \ding{55}         & 32                                                                              & 51.0                                                                                                          & 60.4                                                                                                                 & 106.3                                                                                                               & 47.2                                                                                                            & 57.4                                                                                                             & 44.0                                                                                                             & 72.8                                                                                                               & 29.4                                                                                                             & 40.7                                                                                                           & 77.3                                                                                                          & 41.2                                                                                                          & 50.4                                                                                                                  & 32.6                                                                                                           & 28.4                                                                                                            & 63.5                                                                                                             & -                                                                             \\ \hline
\multirow{4}{*}{\largem{}}                                                    & \ding{55}         & 0                                                                               & 50.6                                                                                                          & 56.3                                                                                                                 & 84.3                                                                                                                & 35.6                                                                                                            & 46.7                                                                                                             & 31.6                                                                                                             & 67.2                                                                                                               & 17.4                                                                                                             & 40.7                                                                                                           & 60.1                                                                                                          & 39.7                                                                                                          & 52.0                                                                                                                  & 35.0                                                                                                           & 26.7                                                                                                            & 46.4                                                                                                             & {\ul \textbf{60.8}}                                                           \\
                                                                              & \ding{55}         & 4                                                                               & 57.4                                                                                                          & 63.1                                                                                                                 & 103.2                                                                                                               & 41.7                                                                                                            & 56.0                                                                                                             & 39.6                                                                                                             & 75.1                                                                                                               & 23.9                                                                                                             & 44.1                                                                                                           & 74.5                                                                                                          & 42.4                                                                                                          & \textbf{55.6}                                                                                                                  & 36.5                                                                                                           & 30.8                                                                                                            & 68.6                                                                                                             & -                                                                             \\
                                                                              & \ding{55}         & 32                                                                              & {\ul \textbf{57.8}}                                                                                           & \textbf{67.6}                                                                                                        & \textbf{113.8}                                                                                                      & {\ul \textbf{52.3}}                                                                                             & \textbf{65.1}                                                                                                    & \textbf{49.8}                                                                                                    & {\ul \textbf{75.4}}                                                                                                               & \textbf{31.0}                                                                                                    & {\ul \textbf{45.3}}                                                                                            & \textbf{86.8}                                                                                                 & 42.2                                                                                                          & \textbf{55.6}                                                                                                                   & \textbf{37.9}                                                                                                  & {\ul \textbf{33.5}}                                                                                             & \textbf{70.0}                                                                                                    & -                                                                             \\ \hline
\begin{tabular}[c]{@{}c@{}}\vlpft{Pretrained}\\  \vlpft{FT SOTA}\end{tabular} & \vlpft{\ding{52}} & \begin{tabular}[c]{@{}c@{}}\phantom{0}\\ \phantom{0}\\ \vlpft{(X)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{54.4}\\ \vlpft{\small{[\citenum{gui2021kat}]}}\\ \vlpft{(10K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{80.2}\\ \vlpft{\small{[\citenum{yuan2021florence}]}}\\ \vlpft{(444K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{143.3}\\ \vlpft{\small{[\citenum{wang2021simvlm}]}}\\ \vlpft{(500K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{47.9}\\ \vlpft{\small{[\citenum{fu2021violet}]}}\\ \vlpft{(27K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{76.3}\\ \vlpft{\small{[\citenum{zhu2019vatex}]}}\\ \vlpft{(500K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{57.2}\\ \vlpft{\small{[\citenum{liu2021vizwiz}]}}\\ \vlpft{(20K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{67.4}\\ \vlpft{\small{[\citenum{zhou2020unified}]}}\\ \vlpft{(30K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{46.8}\\ \vlpft{\small{[\citenum{wang2022allinone}]}}\\ \vlpft{(130K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{35.4}\\ \vlpft{\small{[\citenum{yang2021just}]}}\\ \vlpft{(6K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{138.7}\\ \vlpft{\small{[\citenum{xu2021vlm}]}}\\ \vlpft{(10K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{36.7}\\ \vlpft{\small{[\citenum{wu2021star}]}}\\ \vlpft{(46K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{75.2}\\ \vlpft{\small{[\citenum{murahari2020large}]}}\\ \vlpft{(123K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{54.7}\\ \small{\vlpft{[\citenum{yang2021tap}}]}\\ \vlpft{(20K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{25.2}\\ \vlpft{\small{[\citenum{xiao2021next}]}}\\ \vlpft{(38K)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\vlpft{79.1}\\ \vlpft{\small{[\citenum{lippe2020multimodal}]}}\\ \vlpft{(9K)}\end{tabular} & -                                                                             \\ \bottomrule
\end{tabular}
}

\vspace{0.5em}

\caption{
\capfontsize{}
\label{tab:fewshot_all_tasks} \textbf{Comparison to the state of the art.} A \emph{single} Flamingo model reaches the state of the art on a wide array of image \textbf{(I)} and video \textbf{(V)} understanding tasks with few-shot learning, significantly outperforming previous best zero- and few-shot methods with as few as four examples.
More importantly, using only  examples and without adapting any model weights, \method{} {\em outperforms} the current best methods -- fine-tuned on thousands of annotated examples -- on seven tasks.
Best few-shot numbers are in \textbf{bold}, best numbers overall are {\ul underlined}. 
}
\vspace{-0.5cm}
\end{table}
 



\textbf{Few-shot results.}
Results are given in Table~\ref{tab:fewshot_all_tasks}.
\largem{} outperforms by a large margin \emph{all} previous zero-shot or few-shot methods on the 16 benchmarks considered.
This is achieved with as few as four examples per task, demonstrating practical and efficient adaptation of vision models to new tasks.
More importantly, \largem{} is often competitive with state-of-the-art methods additionally fine-tuned on up to hundreds of thousands of annotated examples.
On six tasks, \largem{} even outperforms the fine-tuned SotA despite using a \emph{single} set of model weights and only 32 task-specific examples.
Finally, despite having only used the \dev{} benchmarks for design decisions, our results generalize well to the other benchmarks, confirming the generality of our approach.

\textbf{Scaling with respect to parameters and shots.}
As shown in Figure~\ref{fig:results}, the larger the model, the better the few-shot performance, similar to GPT-3~\citep{gpt3}.
The performance also improves with the number of shots.
We further find that the largest model better exploits larger numbers of shots.
Interestingly, even though our \method{} models were trained with sequences limited to only 5 images on \mmmw{}, they are still able to benefit from up to 32 images or videos during inference.
This demonstrates the flexibility of the \method{} architecture for processing a variable number of videos or images.


\subsection{Fine-tuning \largem{} as a pretrained vision-language model}
\label{sec:ft_results}

While not the main focus of our work, we verify that when given more data, \method{} models can be adapted to a task by fine-tuning their weights.
In Table~\ref{tab:ft-sota-table-compressed}, we explore fine-tuning our largest model, \largem{}, for a given task with no limit on the annotation budget.
In short, we do so by fine-tuning the model on a short schedule with a small learning rate by additionally unfreezing the vision backbone to accommodate a higher input resolution (details \newreferencetoappendix{in Appendix~\maintoappref{app:finetuning}}).
We find that we can improve results over our previously presented in-context few-shot learning results, setting a new state of the art on five additional tasks: VQAv2, VATEX, VizWiz, MSRVTTQA, and HatefulMemes.

\newcommand{\uln}[1]{\underline{#1}}
\newcommand{\bo}[1]{\bf{\underline{#1}}}
\newcommand{\ofa}{\footnotesize{[\citenum{wang2022unifying}]}}
\newcommand{\simvlm}{\footnotesize{[\citenum{wang2021simvlm}]}}
\newcommand{\florence}{\footnotesize{[\citenum{yuan2021florence}]}}
\newcommand{\alice}{\footnotesize{[\citenum{yan2021achieving}]}}
\newcommand{\vatex}{\footnotesize{[\citenum{zhu2019vatex}]}}
\newcommand{\vizwiz}{\footnotesize{[\citenum{liu2021vizwiz}]}}
\newcommand{\allinon}{\footnotesize{[\citenum{wang2022allinone}]}}
\newcommand{\visdial}{\footnotesize{[\citenum{murahari2020large}]}}
\newcommand{\vdbert}{\footnotesize{[\citenum{wang2020vdbert}]}}
\newcommand{\youcook}{\footnotesize{[\citenum{xu2021vlm}]}}
\newcommand{\tap}{\footnotesize{[\citenum{yang2021tap}]}}
\newcommand{\teammia}{\footnotesize{[\citenum{qiao2021winner}]}}
\newcommand{\flava}{\footnotesize{[\citenum{singh2021flava}]}}
\newcommand{\hateful}{\footnotesize{[\citenum{lippe2020multimodal}]}}
\newcommand{\zhu}{\footnotesize{[\citenum{zhu2020enhance}]}}
\newcommand{\sota}{SotA}

\begin{table*}[t]
\resizebox{\textwidth}{!}{\begin{tabular}{rccccccccccccc}
\hline
Method            & \multicolumn{2}{c|}{VQAV2} & \multicolumn{1}{c|}{COCO} & \multicolumn{1}{c|}{VATEX} & \multicolumn{2}{c|}{VizWiz} & \multicolumn{1}{c|}{MSRVTTQA} & \multicolumn{2}{c|}{VisDial} & \multicolumn{1}{c|}{YouCook2} & \multicolumn{2}{c|}{TextVQA} & \multicolumn1{c}{HatefulMemes} \\
                  & test-dev       & \multicolumn{1}{c|}{test-std}       & \multicolumn{1}{c|}{test}                           & \multicolumn{1}{c|}{test}                            & test-dev        & \multicolumn{1}{c|}{test-std}       & \multicolumn{1}{c|}{test}                               & \multicolumn{1}{c|}{valid}                              & \multicolumn{1}{c|}{test-std}                              & \multicolumn{1}{c|}{valid}        & \multicolumn{1}{c|}{valid}        & \multicolumn{1}{c|}{test-std}         & \multicolumn{1}{c}{test seen}                                   \\ \hline
\flamingoemoji{} 32 shots&67.6     &  -      & 113.8    & 65.1    & 49.8     & -       &   31.0  & 56.8  &  -  & 86.8    & 36.0     &  -  & 70.0   \\
\flamingoemoji{} Fine-tuned &\bo{82.0}&\bo{82.1}&  138.1   &\bo{84.2}&\bo{65.7}&\bf{65.4}&\bf{47.4}& 61.8 & 59.7 &  118.6  &\bf{57.1}&54.1      &\bo{86.6}\\
\hline
                                     &   81.3 & 81.3    &\bf{149.6}&  81.4  &  57.2    &   60.6  &    46.8   & \bf{75.2}   &   \bf{75.4}    &   \bf{138.7}   &   54.7   &   \bf{73.7}    & 84.6\\
\multirow{-2}{*}{\sota}             &\alice   &\alice   &   \ofa   & \vatex  &     \vizwiz     & \vizwiz&   \allinon  &    \visdial  & \vdbert &    \youcook &   \tap   &  \teammia     & \zhu \\
\hline
\end{tabular}}
\caption{
\capfontsize{} \label{tab:ft-sota-table-compressed} \textbf{Comparison to SotA when fine-tuning \largem{}.}
We fine-tune~\largem{} on all nine tasks where \largem{} does not achieve SotA with few-shot learning.
\largem{} sets a new SotA on five of them, outperfoming methods (marked with ) that use tricks such as model ensembling or domain-specific metric optimisation (e.g., CIDEr optimisation).}
\end{table*}
 
\subsection{Ablation studies}
\label{sec:ablations}


\begin{table}[t]
\resizebox{\textwidth}{!}{\begin{tabular}{@{}rlll|cc|ccccc|cc@{}}
\toprule
&Ablated  & \base{} & Changed & \small{Param.}  &            \small{Step}     & COCO & OKVQA & VQAv2  & MSVDQA & VATEX & Overall \\
&setting & original value & value  & \small{count }  &   \small{time }               & CIDEr  & top1 & top1     & top1   & CIDEr  &   score   \\ \midrule
&\multicolumn{3}{c|}{\textbf{\base{} model}}    & 3.2B & 1.74s &   86.5  &   42.1    &  55.8        &     36.3     &  53.4     &  \textbf{70.7}     \\ \midrule
\multirow{4}{*}{\textbf{(i)}}&\multirow{4}{*}{Training data} & \multirow{4}{*}{All data} & w/o Video-Text pairs &  3.2B & 1.42s &  84.2 &   43.0  &  53.9  &  34.5  &  46.0  & 67.3    \\
& &  & w/o Image-Text pairs    &  3.2B &  0.95s &  66.3 &   39.2  &  51.6    &  32.0  &  41.6  &  60.9   \\
& &  & Image-Text pairs LAION    &  3.2B &  1.74s &  79.5 &   41.4  &  53.5    &  33.9  &  47.6  &  66.4   \\
& &  & w/o M3W & 3.2B &  1.02s                  &  54.1  &  36.5  &  52.7&  31.4  &  23.5  &  53.4  \\  \midrule
\textbf{(ii)}& Optimisation & Accumulation  & Round Robin & 3.2B & 1.68s &   76.1 & 39.8   &  52.1   &   33.2    & 40.8  &  62.9 \\  \midrule
\textbf{(iii)} & Tanh gating & \ding{51}  &  \ding{55} & 3.2B & 1.74s &  78.4 & 40.5   &  52.9      &    35.9    &  47.5     &     66.5    \\  \midrule
\multirow{2}{*}{\textbf{(iv)}}& Cross-attention  & \multirow{2}{*}{\shortstack[c]{\textsc{gated}\\\textsc{xattn-dense}}} & \small{\textsc{Vanilla xattn}}  & 2.4B & 1.16s &   80.6 & 41.5    &   53.4     &    32.9    &   50.7    &   66.9    \\ 	
&architecture  & & \small{\textsc{Grafting}}   & 3.3B & 1.74s &   79.2  & 36.1   &  50.8     &   32.2    &    47.8   &   63.1      \\ \midrule
\multirow{3}{*}{\textbf{(v)}}&\multirow{3}{*}{\shortstack[l]{Cross-attention\\ frequency}} & \multirow{3}{*}{Every}  & Single in middle & 2.0B & 0.87s &  71.5  & 38.1 &   50.2       &    29.1    &  42.3  &   59.8    \\  					
& & & Every 4th &  2.3B & 1.02s  &   82.3  & 42.7    &    55.1       &    34.6    &   50.8    &   68.8    \\ 						
& & & Every 2nd &  2.6B & 1.24s  &   83.7  & 41.0    &   55.8      &  34.5      &   49.7    &   	    68.2   \\  \midrule 		
\multirow{2}{*}{\textbf{(vi)}}& \multirow{2}{*}{Resampler}  & \multirow{2}{*}{Perceiver}  & MLP &  3.2B & 1.85s &   78.6 &  42.2     &   54.7     &    35.2    &  44.7     &  66.6      \\				
& & &  Transformer &  3.2B & 1.81s  &  83.2 &  41.7     &   55.6       &   31.5     &  48.3     &   66.7     \\  \midrule
\multirow{2}{*}{\textbf{(vii)}}&\multirow{2}{*}{Vision encoder} & \multirow{2}{*}{NFNet-F6}  &  CLIP ViT-L/14 & 3.1B & 1.58s   &  76.5 & 41.6    &   53.4      &   33.2     &   44.5    &    64.9    \\  	
& & &  NFNet-F0 & 2.9B & 1.45s  &    73.8 & 40.5     &  52.8       &    31.1    &  42.9     &    62.7  \\ \midrule
\multirow{2}{*}{\textbf{(viii)}}& \multirow{2}{*}{Freezing LM} & \multirow{2}{*}{\ding{51}}  & \ding{55} (random init)   &  3.2B & 2.42s &  74.8 & 31.5   &  45.6    &  26.9   &  50.1 &  57.8  \\
& &  & \ding{55}  (pretrained)   &  3.2B & 2.42s &   81.2  & 33.7   &  47.4 &   31.0    &    53.9  &   62.7      \\  \midrule
\end{tabular}}
\caption{\capfontsize{} \textbf{Ablation studies.} 
Each row should be compared to the baseline~\method{} run (top row).
Step time measures the time spent to perform gradient updates on all training datasets.
}
\vspace{-0.4cm}
\label{tab:ablation-table-no-classif}
\end{table}

 
In Table~\ref{tab:ablation-table-no-classif}, we report our ablation results using \base{}~on the \metadevsubsets~of the five \dev{} benchmarks with 4 shots.
Note that we use smaller batch sizes and a shorter training schedule compared to the final models.
The \textbf{Overall score} is obtained by dividing each benchmark score by its state-of-the-art (SotA) performance from Table~\ref{tab:fewshot_all_tasks} and averaging the results.
More details and results are given in Appendix~\maintoappref{app:all_ablation_studies} and Table~\ref{tab:ablation-table-appendix}.

\noindent
\textbf{Importance of the training data mixture.}
As shown in row \textbf{(i)}, getting the right training data plays a crucial role.
In fact, removing the interleaved image-text dataset \mmmw{} leads to a \emph{decrease of more than } in performance while removing the conventional paired image-text pairs also decreases performance (by ), demonstrating the need for different types of datasets.
Moreover, removing our paired video-text dataset negatively affects performance on all video tasks.
We ablate replacing our image-text pairs (ITP) by the publicly available LAION-400M dataset~\cite{schuhmann2021laion}, which leads to a slight degradation in performance.
We show in row \textbf{(ii)} the importance of our gradient accumulation strategy compared to using round-robin updates~\citep{cho2021unifying}.

\noindent
\textbf{Visual conditioning of the frozen LM.}
We ablate the use of the 0-initialized tanh gating when merging the cross-attention output to the frozen LM output in row \textbf{(iii)}.
Without it, we see a drop of  in our overall score.
Moreover, we have noticed that disabling the 0-initialized tanh gating leads to training instabilities.
Next, we ablate different conditioning architectures in row \textbf{(iv)}.
\textsc{vanilla xattn}, refers to the vanilla cross-attention from the original Transformer decoder~\citep{vaswani2017attention}.
In the \textsc{grafting} approach from~\cite{luo2022vc}, the frozen LM is used as is with no additional layers inserted, and a stack of interleaved self-attention and cross-attention layers that take the frozen LM output are learnt from scratch.
Overall, we show that our \textsc{gated xattn-dense} conditioning approach works best.

\noindent
\textbf{Compute/Memory vs. performance trade-offs.}
In row \textbf{(v)}, we ablate the frequency at which we add new \textsc{gated xattn-dense} blocks.
Although adding them at every layer is better, it significantly increases the number of trainable parameters and time complexity of the model.
Notably, inserting them every fourth block accelerates training by  while only decreasing the overall score by .
In light of this trade-off, we maximize the number of added layers under hardware constraints and add a \textsc{gated xattn-dense} every fourth layer for \medium{} and every seventh for \largemfull{}.
We further compare in row \textbf{(vi)} the Perceiver Resampler to a MLP and a vanilla Transformer given a parameter budget.
Both underperform the Perceiver Resampler while also being slower.


\noindent
\textbf{Vision encoder.}
In row \textbf{(vii)}, we compare our NFNet-F6 vision encoder pretrained with contrastive learning \newreferencetoappendix{(details in Appendix~\maintoappref{app:contrastive_details})} to the publicly available CLIP ViT-L/14~\citep{clip} model trained at 224 resolution.
Our NFNet-F6 has a  advantage over the CLIP ViT-L/14 and  over a smaller NFNet-F0 encoder, which highlights the importance of using a strong vision backbone.

\noindent
\textbf{Freezing LM components prevents catastrophic forgetting.} We verify the importance of freezing the LM layers at training in row \textbf{(viii)}.
If trained from scratch, we observe a large performance decrease of .
Interestingly, fine-tuning our pretrained LM also leads to a drop in performance of .
This indicates an instance of ``catastrophic forgetting''~\citep{mccloskey1989catastrophic}, in which the model progressively forgets its pretraining while training on a new objective. In our setting, freezing the language model is a better alternative to training with the pre-training dataset (MassiveText) in the mixture.


\section{Related work}

\textbf{Language modelling and few-shot adaptation.}
Language modelling has recently made substantial progress following the introduction of Transformers~\citep{vaswani2017attention}.
The paradigm of first pretraining on a vast amount of data followed by an adaptation on a downstream task has become standard~\citep{mikolov2010recurrent,graves2013generating,jozefowicz2016exploring,howard2018universal,bert,t5,sutskever2011generating,gpt3}.
In this work, we build on the 70B Chinchilla language model~\citep{chinchilla} as the base LM for \largem{}.
Numerous works have explored techniques to adapt language models to novel tasks using a few examples.
These include adding small adapter modules~\citep{houlsby2019parameter}, fine-tuning a small part of the LM~\citep{zaken_bitfit_2022}, showing in-context examples in the prompt~\citep{gpt3}, or optimizing the prompt~\citep{li2021prefix,lester2021power} through gradient descent.
In this paper, we take inspiration from the in-context~\citep{gpt3} few-shot learning technique instead of more involved few-shot learning approaches based on metric learning~\citep{doersch2020crosstransformers,vinyals2016matching,snell2017prototypical,tian2020rethinking} or meta-learning~\citep{finn2017model,bertinetto2018meta,zintgraf2019fast,requeima2019fast,gordon2018meta,bertinetto2016learning}.

\textbf{When language meets vision.}
These LM breakthroughs have been influential for vision-language modelling.
In particular, BERT~\citep{bert} inspired a large body of vision-language work~\citep{lu2019vilbert,su2019vl,chen2020uniter,hendricks2021decoupling,wang2021vlmo,li2020oscar,tan2019lxmert,zhu2020actbert,wang2021ufo,li2020hero,gan2020large,fu2021violet,zellers2021merlot,zellers2022merlot,singh2021flava,sun2019videobert}.
We differ from these approaches as \methodfamily{} do not require fine-tuning on new tasks.
Another family of vision-language models is based on contrastive learning~\citep{alayrac2020self,clip,align,zhai2021lit,pham2021combined,miech2020end,bain2021frozen,yuan2021florence,li2021align,yao2021filip,jain2021mural}.
\method{} differs from contrastive models as it can generate text,
although we build and rely upon them for our vision encoder.
Similar to our work are VLMs able to generate text in an autoregressive manner~\citep{vinyals2015show,donahue2015long,luo2020univl,hu2021scaling,dai2022}.
Concurrent works~\citep{wang2021simvlm,cho2021unifying,wang2022unifying,zhu2021uni,li2022blip} also propose to formulate numerous vision tasks as text generation problems.
Building on top of powerful pretrained language models has been explored in several recent works.
One recent line of work~\citep{tsimpoukelli2021multimodal,eichenberg2021magma,mokady2021clipcap,luo2022vc,yang2021empirical,zeng2022socraticmodels} proposes to freeze the pretrained LM weights to prevent catastrophic forgetting~\citep{mccloskey1989catastrophic}.
We follow this idea by freezing the Chinchilla LM layers~\citep{chinchilla} and adding learnable layers within the frozen LM.
We differ from prior work by introducing the first LM that can ingest arbitrarily interleaved images, videos, and text.

\textbf{Web-scale vision and language training datasets.}
Manually annotated vision and language datasets are costly to obtain and thus relatively small (10k-100k) in scale~\citep{young2014image,chen2015microsoft,antol2015vqa,marino2019ok,wang2019vatex,xiao2021next}.
To alleviate this lack of data, numerous works~\citep{align,sharma2018conceptual,changpinyo2021conceptual,thomee2016yfcc100m} automatically scrape readily available paired vision-text data.
In addition to such paired data, we show the importance of also training on entire multimodal webpages containing interleaved images and text as a single sequence.
Concurrent work CM3~\citep{aghajanyan2022cm3} proposes to generate HTML markup from pages, while we simplify the text prediction task by only generating plain text.
We emphasize few-shot learning and vision tasks while CM3~\citep{aghajanyan2022cm3} primarily evaluates on language-only benchmarks in a zero-shot or fine-tuned setup.


\section{Discussion}

\label{sec:discussion}

\noindent
\textbf{Limitations.} First, our models build on pretrained LMs, and as a side effect, directly inherit their weaknesses.
For example, LM priors are generally helpful, but may play a role in occasional hallucinations and ungrounded guesses.
Furthermore, LMs generalise poorly to sequences longer than the training ones.
They also suffer from poor sample efficiency during training.
Addressing these issues can accelerate progress in the field and enhance the abilities of VLMs like Flamingo.

Second, the classification performance of \method{} lags behind that of state-of-the-art contrastive models~\citep{clip,pham2021combined}.
These models directly optimize for text-image retrieval, of which classification is a special case.
In contrast, our models handle a wider range of tasks, such as open-ended ones.
A unified approach to achieve the best of both worlds is an important research direction.

Third, in-context learning has significant advantages over gradient-based few-shot learning methods, but also suffers from drawbacks depending on the characteristics of the application at hand.
We demonstrate the effectiveness of in-context learning when access is limited to only a few dozen examples.
In-context learning also enables simple deployment, requiring only inference,
generally with no hyperparameter tuning needed.
However, in-context learning is known to be highly sensitive to various aspects of the demonstrations~\citep{zhao2021calibrate,truefewshot},
and its inference compute cost and absolute performance scale poorly with the number of shots beyond this low-data regime.
There may be opportunities to combine few-shot learning methods to leverage their complementary benefits.
We discuss the limitations of our work in more depth in Appendix~\maintoappref{sec:limitations}.


\noindent
\textbf{Societal impacts.}
In terms of societal impacts, \largem{} offers a number of benefits while carrying some risks.
Its ability to rapidly adapt to a broad range of tasks have the potential to enable non-expert users to obtain
good
performance in data-starved regimes, lowering the barriers to both beneficial and malicious applications.
\largem{} is exposed to the same risks as large language models, such as outputting offensive language, propagating social biases and stereotypes, as well as leaking private information~\citep{weidinger2021harms,chinchilla}.
Its ability to additionally handle visual inputs poses specific risks such as gender and racial biases relating to the contents of the input images, similar to a number of visual recognition systems~\citep{hendricks2018women,zhao2021understanding,buolamwini2018gender,de2019does,schwemmer2020diagnosing}.
We refer the reader to Appendix~\maintoappref{sec:broader_impact} for a more extensive discussion of the societal impacts of our work, both positive and negative;
as well as mitigation strategies and early investigations of risks relating to racial or gender bias and toxic outputs.
Finally we note that, following prior work focusing on language models~\citep{thoppilan2022lamda,perez2022red,menick2022teaching},
the few-shot capabilities of \method{} could be useful for mitigating such risks.

\noindent
\textbf{Conclusion.}
We proposed Flamingo, a general-purpose family of models that can be applied to image and video tasks with minimal task-specific training data.
We also qualitatively
explored interactive abilities of~\largem{} such as ``chatting'' with the model, demonstrating flexibility beyond traditional vision
benchmarks.
Our results suggest that connecting pre-trained large language models with powerful visual models is an important step towards general-purpose visual understanding.