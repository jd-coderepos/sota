\documentclass{article}









\usepackage[preprint]{neurips_2020}
\usepackage{microtype}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs} 



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage[inline]{enumitem}

\usepackage{wrapfig, blindtext}
\usepackage{epstopdf}
\usepackage{booktabs} \usepackage{extarrows}

\usepackage{amsmath}   
\usepackage{amssymb}
\usepackage{wasysym}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{footmisc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{setspace}
\usepackage{caption}
\usepackage{makecell}

\usepackage{natbib}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{graphicx}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{graphicx}
\usetikzlibrary{positioning}

\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{framed}
\usepackage{microtype}


\usepackage{siunitx}
\usepackage{xr-hyper}
\usepackage{xr}
\usepackage{xr}
\usepackage{wrapfig}
\usepackage{amsmath,amsthm}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{mathtools}

\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}



\definecolor{mygray}{gray}{0.5}
\renewcommand{\algorithmiccomment}[1]{\;\;\;\textcolor{mygray}{\;\textit{#1}}}
\makeatletter
\newcommand{\algorithmicbreak}{\textbf{break}}
\newcommand{\BREAK}{\STATE \algorithmicbreak}
\newcommand{\mm}{\textit{Manifold Mixup }}
\newcommand{\graphmix}{GraphMix }
\newcommand{\graphmixns}{GraphMix}
\newcommand{\graphmixgcn}{GraphMix(GCN) }
\newcommand{\graphmixgat}{GraphMix(GAT) }
\newcommand{\pmodel}{{\rm p}_{\rm{model}}}
\makeatother



\DeclareMathOperator{\augment}{Augment}
\DeclareMathOperator{\betadist}{Beta}
\DeclareMathOperator{\guesslabel}{GuessLabel}
\DeclareMathOperator{\onehot}{OneHot}
\DeclareMathOperator{\sharpen}{Sharpen}
\DeclareMathOperator{\shuffle}{Shuffle}
\DeclareMathOperator{\concat}{Concat}
\DeclareMathOperator{\uniform}{Uniform}
\DeclareMathOperator{\mixup}{MixUp}
\DeclareMathOperator{\mixmatch}{MixMatch}
\DeclareMathOperator{\xent}{H}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\subjectto}{{\text {subject to}}}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\expectation}{\mathbb{E}}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\softmin}{\mathrm{soft}\!\min}
\DeclareMathOperator*{\infsup}{\inf\!/\!\sup}
\newcommand\myeq{\stackrel{\left(\lambda\right)}{\inf\!\sup}}
\newcommand{\advgrad}{\overset{\left(adv\right)}{\nabla}}
\newcommand\Myperm[2][n]{\prescript{#1\mkern-2.5mu}{}P_{#2}}
\newcommand{\widesim}[2][1.5]{
  \mathrel{\overset{#2}{\scalebox{#1}[1]{}}}
}

\newcommand{\graphmixkk}{\normalfont{\text{GraphMix}}}
\newcommand{\gnn}{\normalfont{\text{GNN}}}
\newcommand{\fcn}{\normalfont{\text{FCN}}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\EE}{\mathbb{E}} 
\ifx\BlackBox\undefined
\newcommand{\BlackBox}{\rule{1.5ex}{1.5ex}}  \fi
\ifx\proof\undefined
\newenvironment{proof}{\par\noindent{\emph{Proof.}}}{\hfill\BlackBox\
\label{eq:gnn}
    \mathbf{h}^{(l+1)} = \sigma(AGGREGATE(\mathbf{h}^{(l)} \mathbf{W}, \mathcal{A} ))

\label{eq:mixup_form}
    \mathcal{L}_{\text{MM}}(\mathcal{D},T_{\theta}, \alpha) = &\expectation_{(\mathbf{x},\mathbf{y}) \sim \mathcal{D}}\,
    \expectation_{(\mathbf{x'}, \mathbf{y'}) \sim \mathcal{D}}\,
    \expectation_{\lambda \sim \text{Beta}(\alpha, \alpha)}\, 
     &\ell(f(\text{Mix}_\lambda(g(\mathbf{x}), g(\mathbf{x'}))), \text{Mix}_\lambda(\mathbf{y}, \mathbf{y'})).

\label{eq:fcn_sup}
    \mathcal{L}_{\text{supervised}} = \mathcal{L}_{\text{MM}}((\mathbf{X}_l,\mathbf{Y}_l),F_{\theta},\alpha)

\label{eq:predict}
\mathbf{\hat{Y}_u} = G_{\theta}(\mathbf{X}_u)

\label{eq:fcn_usup}
    \mathcal{L}_{\text{unsupervised}} = \mathcal{L}_{\text{MM}}((\mathbf{X}_u,\mathbf{\hat{Y}}_u),F_{\theta},\alpha)

\label{eq:fcn_loss}
    \mathcal{L}_{\text{FCN}}= \mathcal{L}_{\text{supervised}} + w(t)*\mathcal{L}_{\text{unsupervised}}

\label{eq:graphmix_loss}
    \mathcal{L}_{\text{\graphmix}}= \mathcal{L}_{\text{GNN}} + \lambda*\mathcal{L}_{\text{FCN}}

\varphi(S')-\varphi(S) 
 &\le \sup_{f_\gamma \in \Fcal_\gamma} L_{\graphmixkk}(S,f_\gamma)-L_{\graphmixkk}(S',f_\gamma) \\ &=  \sup_{f_\gamma \in \Fcal_\gamma} (L_{\gnn}(S,f_\gamma)-L_{\gnn}(S',f_\gamma)) \\ & +\lambda(L_{\fcn}(S,f_\gamma)-L_{\fcn}(S',f_\gamma))

L_{\gnn}(S,f_\gamma)-L_{\gnn}(S',f_\gamma)
 = \\ \frac{1}{m}\left(\ell(f_\gamma(x_{i_0};G),y_{{i_0}})-  (\ell(f_\gamma(x_{i_0}';G),y_{{i_0}}') \right) \le \frac{c}{m}, 

\scalebox{1.0}{} \\ 
\le \frac{c (2m-1)}{m^2} 
\le \frac{2c}{m},
 
L_{\fcn}(S,f_\gamma)-L_{\fcn}(S',f_\gamma) \le \frac{4c}{m}. 

\varphi(S')-\varphi(S) \le \frac{c(1+4 \lambda)}{m}.  

\varphi(S) \le  \EE_{\bar S}[\varphi(\bar S)] + c(1+4 \lambda) \sqrt{\frac{\ln( |\Gamma|/\delta)}{2m}}.

\EE_{\bar S}[\varphi(\bar S)] + \lambda\EE_{\bar S}\left[\inf_{f_\gamma \in \Fcal_\gamma}  L_{\fcn}(\bar S,f_\gamma) \right] \\
  \le \EE_{\bar S}\left[\sup_{f_\gamma \in \Fcal_\gamma} \EE_{\bar x',\bar y' \sim\Dcal}[\ell(f_\gamma(\bar x';G), \bar y')]-L_{\gnn}(\bar S,f_\gamma)\right]   
 \\  \scalebox{0.95}{}  
 \\  \scalebox{0.9}{} 
  \\  \le 2\Rcal_{n}(\Theta)    

\EE_{x,y \sim\Dcal}[\ell(f_\gamma(x;G),y)]-L_{\graphmixkk}(S,f_\gamma) \\
 \le \Rcal_{m}^{\ell}(\Fcal_\gamma) +c \sqrt{\frac{\ln(|\Gamma|/\delta)}{2m}} - \lambda V. 

    \sharpen(p_{i}, T) := p_{i}^{\frac{1}{T}}\bigg/ \sum_{j = 1}^C p_j^{\frac{1}{T}}
    \label{eqn:sharpen}



\subsection{Connection to Co-training}
\label{sec:cotraining}
The GraphMix approach can be seen as a special instance of the Co-training framework \citep{cotraining}. Co-training assumes that the description of an example can be partitioned into two \textit{distinct} views and either of these views would be sufficient for learning given sufficient labeled data. In this framework, two learning algorithms are trained separately on each view and then the prediction of each learning algorithm on the unlabeled data is used to enlarge the training set of the other. Our method has some important differences and similarities to the Co-training framework. Similar to Co-training, we train two neural networks and the predictions from the GNN are used to enlarge the training set of the FCN. An important difference is that instead of using the predictions from the FCN to enlarge the training set for the GNN, we employ parameter sharing for passing the learned information from the FCN to the GNN. In our experiments, directly using the predictions of the FCN for GNN training resulted in reduced accuracy. This is due to the fact that the number of labeled samples for training the FCN is sufficiently low and hence the FCN does not make accurate enough predictions. Another important difference is that unlike the co-training framework, the FCN and GNN do not use completely distinct views of the data: the FCN uses feature vectors  and the GNN uses the feature vector and adjacency matrix .  

\subsection{Algorithm}
\label{app:algo}
The procedure for GraphMix training is given in Algorithm \ref{alg:graphmix}.

\begin{algorithm}[t]
        \caption{\graphmix: A procedure for improved training of Graph Neural Networks (GNN)}
   \label{alg:graphmix}
   \begin{algorithmic}[1]
   \footnotesize
   \STATE {\bfseries Input:} A GCN: , a FCN:  which shares parameters with the GCN.  distribution parameter  for \mm. Number of random perturbations ,  Sharpening temperature . maximum value of weight  in the weighted averaging of supervised FCN loss and unsupervised FCN loss. Number of parameter updates . : rampup function for increasing the importance unsupervised loss in FCN training.  represents labeled samples and  represents unlabeled samples.

   
   \FOR{ to }
   \STATE i = random(0,1) \COMMENT{\textit{generate randomly 0 or 1}} \\
   \IF {i=0} \STATE  \COMMENT{supervised loss from FCN using the \mm }
   \label{line:MM_sup}\\
   
   \FOR{ to }
   \STATE  \COMMENT{\textit{Apply  round of random perturbation to }} \label{line:augment_unlabeled} \\
   \ENDFOR
   \STATE  \COMMENT{\textit{Compute average predictions across K perturbations of } using the GCN} \label{line:average_prediction} \\
   \STATE  \COMMENT{\textit{Apply temperature sharpening to the average prediction}} \label{line:sharpen} \\
   \STATE  \COMMENT{unsupervised loss from FCN using the \mm }
   \label{line:MM_unsup}\\
   \STATE  \COMMENT{\textit{Total loss is the weighted sum of supervised and unsupervised FCN loss}} \label{line:consistency_coeff}\\
   \ELSE
   \STATE   \COMMENT{\textit{Loss using the vanilla GCN }}
   
   \ENDIF
   \STATE Minimize  using gradient descent based optimizer such as SGD.
   \ENDFOR
   
\end{algorithmic}
\end{algorithm}




\subsection{Datasets}
\label{app:datasets}
We use three standard benchmark citation network datasets for semi-supervised node classification,  namely Cora, Citeseer and Pubmed. In all these datasets, nodes correspond to documents and edges correspond to citations. Node features correspond to the bag-of-words representation of the document. Each node belongs to one of  classes. During training, the algorithm has access to the feature vectors and edge connectivity of all the nodes but has access to the class labels of only a few of the nodes. 

For semi-supervised link classification, we use two datasets Bitcoin Alpha and Bitcoin OTC from \citep{kumar2016edge, kumar2018rev2}.  The nodes in these datasets correspond to the bitcoin users and the edge weights between them correspond to the degree of trust between the users. Following \citep{gmnn},  we treat edges with weights greater than 3 as positive instances, and edges with weights less than -3 are treated as negative ones. Given a few labeled edges, the task is to predict the labels of the remaining edges. 
The statistics of these datasets as well as the number of training/validation/test nodes is presented in Appendix \ref{app:datasets}. 


The statistics of standard benchmark datasets as well as the number of training/validation/test nodes is presented in Table \ref{tab::dataset}. The statistics of larger datasets in given in Table \ref{tab::dataset_big}.

\begin{table}[bht]
	\caption{Dataset statistics.}
	\label{tab::dataset}
	\begin{center}
		{
		\resizebox{1.0\textwidth}{!}{\begin{tabular}{c  c c c c c c c}\hline
		    \textbf{Dataset}  & \textbf{\# Nodes} & \textbf{\# Edges} & \textbf{\# Features} & \textbf{\# Classes} & \textbf{\# Training} & \textbf{\# Validation} & \textbf{\# Test}  \\
	        \midrule
	        Cora &  2,708 & 5,429 & 1,433 & 7 & 140 & 500 & 1,000\\
	        Citeseer  & 3,327 & 4,732 & 3,703 & 6 & 120 & 500 & 1,000 \\
	        Pubmed  & 19,717 & 44,338 & 500 & 3 & 60 & 500 & 1,000\\
	        Bitcoin Alpha  & 3,783 & 24,186 & 3,783 & 2 & 100 & 500 & 3,221 \\
	        Bitcoin OTC  & 5,881 & 35,592 & 5,881 & 2 & 100 & 500 & 5,947 \\
	        \midrule
	    \end{tabular}}
	}
	\end{center}
\end{table}

For larger datasets of Section \ref{app:larger_datasets}, we took processed versions of these dataset available  here \footnote{https://github.com/shchur/gnn-benchmark}. We did 10 random splits of the the data into train/validation/test split. For the classes which had more than 100 samples. We choose 20 samples per class for training, 30 samples per class for validation and the remaining samples as test data. For the classes which had less than 100 samples, we chose 20\% samples, per class for training, 30\% samples for validation and the remaining for testing. For each split we run experiments using 100 random seeds. The statistics of these datasets is presented in Appendix Table \ref{table:dataset_new_statistics} and 

\begin{table}[h!]
    \caption{Dataset statistics for Larger datasets
    }
    \label{tab::dataset_big}
    \begin{center}
        \begin{tabular}{l r r r r }
            \toprule
            \textbf{Datasets}    & \textbf{Classes} & \textbf{Features} & \textbf{Nodes} & \textbf{Edges}  \\
            \midrule
            
            Cora-Full     &           67 &          8710 &      18703 &      62421 \\
            Coauthor-CS       &           15 &          6805 &      18333 &      81894  \\
            Coauthor-Physics        &            5 &          8415 &      34493 &     247962  \\
            NELL                &        210    &            5414        & 65755     & 266144 \\
            \bottomrule
           
        \end{tabular}
    \end{center}
    
    \label{table:dataset_new_statistics}
\end{table}


\subsection{Ablation Study}
\label{subsection:ablation}
Since \graphmix consists of various components, some of which are common with the existing literature of semi-supervised learning, we set out to study the effect of various components by systematically removing or adding a component from \graphmix. We measure the effect of the following:

\begin{itemize}
    \item Removing both the Manifold Mixup and predicted targets from the FCN training.
    \item Having only the Manifold Mixup training for FCN ( No predicted targets for FCN training)
    \item Using the predicted targets for the FCN training (No Manifold Mixup in FCN training)
    \item Using both Manifold Mixup and predicted targets for FCN training
\end{itemize}

\begin{table*}[]
\caption{Ablation study results using 10 labeled samples per class (\% test accuracy). We report mean and standard deviation over ten trials. See Section \ref{subsection:ablation} for the meaning of methods in leftmost column.}
\centering
\begin{tabular}{lrrr}
\toprule
\textbf{Ablation}	& \textbf{Cora} & \textbf{Citeseer} & \textbf{Pubmed} \\ 
\midrule


\makecell[l]{ Without Manifold Mixup and\\ without predicted-targets} & 68.783.54  & 61.011.24 & 72.561.08  \\
With Predicted targets & 69.085.03 & 62.661.80 & 73.070.94  \\
With Manifold Mixup & 73.553.29  & 65.722.10  &  75.741.69 \\
With Manifold Mixup and Predicted targets & 79.301.36  & 70.781.41  & 77.133.60   \\





		   
\bottomrule
\end{tabular}
\vskip 0.1in
\label{tab:ablation}
\vskip -0.2in
\end{table*}


The ablation results for semi-supervised node classification are presented in Table \ref{tab:ablation}. We did not do any hyperparameter tuning for the ablation study and used the best performing hyperparameters found for the results presented in Table \ref{tab::standard_split}. We observe that all the components of \graphmix contribute to its performance, with Manifold Mixup training of FCN contributing possibly the most. 



\iffalse
\begin{minipage}{0.50\textwidth}
\begin{wraptable}{t}{6.5cm}

\caption{Ablation study results using 10 labeled samples per class (\% test accuracy). We report mean and standard deviation over ten trials.  }
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lrrr}
\toprule
\textbf{Ablation}	& \textbf{Cora} & \textbf{Citeseer} & \textbf{Pubmed} \\ 
\midrule

\graphmix & 79.301.36  & 70.781.41  & 77.133.60   \\
\enskip \makecell[l]{-w/o Manifold Mixup and\\ w/o predicted-targets} & 68.783.54  & 61.011.24 & 72.561.08  \\
\enskip -w/o predicted-targets & 72.853.79  & 64.402.20  &  74.741.69 \\
\enskip -w/o Manifold Mixup & 69.085.03 & 62.661.80 & 74.110.94  \\

\enskip -w/o Sharpening &73.253.41 & 64.652.21  & 74.971.47  \\
\enskip -w/o Prediction Averaging & 74.171.99  & 65.521.78 & 75.592.63 \\

\enskip -with EMA & 79.842.28 & 71.211.32   & 77.463.13  \\
    
		   
\bottomrule
\end{tabular}
}
\vskip 0.1in
\label{tab:ablation}
\vskip -0.2in
\end{wraptable}
\end{minipage}
\fi



\subsection{Comparison with State-of-the-art Methods}
\label{app:standard_split}
We present the comparion of \graphmix with the recent state-of-the-art methods as well as earlier methods using the standard Train/Validation/Test split in Table \ref{tab:full}. We additionally use self-training based baselines, where FCN, GCN, GAT and Graph-U-Net are trained with self-generated targets. These are named as FCN (self-training), GCN (self-training), GAT (self-training) and Graph-U-Net(self-training) respectively in Table ~\ref{tab:full}. For generating the predicted-targets in above two baselines, we followed the procedure of Appendix ~\ref{sec::predict}.

\begin{table*}[ht!]
\caption{Comparison of GraphMix with other methods (\% test accuracy ), for Cora, Citeseer and Pubmed.}
\label{tab:full}
\begin{center}
\begin{tabular}{l l l l}
\toprule
{\bf Method} & {\bf Cora} & {\bf Citeseer} & {\bf Pubmed}\\ \midrule
\midrule
Results reported from the literature & & & \\
\midrule
\midrule
MLP & 55.1\% & 46.5\% & 71.4\% \\
ManiReg \citep{belkin2006manifold} & 59.5\% & 60.1\% & 70.7\%\\
SemiEmb \citep{weston2012deep} & 59.0\% & 59.6\% & 71.7\%\\
LP \citep{zhu2003semi} & 68.0\% & 45.3\% & 63.0\%\\
DeepWalk \citep{perozzi2014deepwalk} & 67.2\% & 43.2\% & 65.3\%\\
ICA \citep{lu2003link} & 75.1\% & 69.1\% & 73.9\%\\
Planetoid \citep{yang2016revisiting} & 75.7\% & 64.7\% & 77.2\%\\
Chebyshev \citep{defferrard} & 81.2\% & 69.8\% & 74.4\%\\
GCN \citep{kipf2016semi} & 81.5\% & 70.3\% &  79.0\%\\
MoNet \citep{monti2016geometric} & 81.7  0.5\% & --- & 78.8  0.3\%\\ 
 GAT \citep{velivckovic2018graph}&  83.0  0.7\% &  72.5  0.7\% &  79.0  0.3\%\\
GraphScan \citep{graphscan} & 83.3 1.3 & 73.11.8 & --- \\
DisenGCN  \citep{ma2019disentangled} & 83.7\% & 73.4\% & 80.5\% \\
Graph U-Net  \citep{u-g-net} & \textbf{84.4\%} & 73.2\% & 79.6\% \\
BVAT \citep{BVAT} & 83.60.5 & \textbf{74.00.6} & 79.90.4 \\

\midrule
\midrule
Our Experiments & & & \\
\midrule
\midrule


 GCN & 81.300.66  & 70.610.22 & 79.860.34\\
 GAT & 82.700.21  & 70.400.35 & 79.050.64 \\
 
 Graph U-Net & 81.740.54  & 67.691.10 & 77.73 0.98 \\
 
FCN (self-training) &  80.300.75 & 71.500.80 & 77.400.37\\
 GCN (self-training) &  82.030.43 & 73.380.35 & 80.420.36\\
 GAT (self-training) & 82.950.23 & 72.870.51 & 79.670.69 \\
 Graph-U-Net (self-training) & 82.010.91   & 68.231.57 & 78.121.23 \\

 \midrule 
 
 \graphmix(GCN) & \textbf{83.940.57}  & \textbf{74.520.59} & 80.980.55 \\ 
 \graphmix(GAT) & 83.320.18  & 73.080.23 & \textbf{81.100.78} \\
 
 \graphmix(Graph U-Net) & 82.180.63  & 69.00 1.32 & 78.761.09 \\
 
 \bottomrule
\end{tabular}
\end{center}
\end{table*}


\subsection{Results with fewer labeled samples}
\label{sec:fewerlabels}
We further evaluate the effectiveness of \graphmix in the learning regimes where fewer labeled samples exist. For each class, we randomly sampled  samples for training and the same number of samples for the validation. We used all the remaining labeled samples as the test set. We repeated this process for  times. The results in Table \ref{tab::results_less_labeled} show that \graphmix achieves even better improvements when the labeled samples are fewer.  


\begin{table*}[bht]
        \caption{Results using less labeled samples (\% test accuracy).  referes to the number of labeled samples per class.}
	\label{tab::results_less_labeled}
	\begin{center}
	{
		\resizebox{\textwidth}{!}{\begin{tabular}{c c c c c c c}\hline
		    	\multirow{2}{*}{\textbf{Algorithm}}	& \multicolumn{2}{c}{\textbf{Cora}} & \multicolumn{2}{c}{\textbf{Citeseer}} & \multicolumn{2}{c}{\textbf{Pubmed}} \\ 
& &  &  & & &  \\
		    \midrule
		    
		     
		     GCN & 66.394.26  & 72.913.10 & 55.615.75 & 64.193.89 &66.06 & 75.571.58 \\
		     
		     GAT & 68.175.54 & 73.884.35 &55.541.82 & 61.630.42 &  64.244.79 & 73.601.85 \\
		     
		     Graph U-Net &64.425.44 & 71.483.03 & 49.435.81 & 61.163.47 &65.054.69  &  68.653.69 \\
		     
		     \midrule
		     \graphmix(GCN) & \textbf{71.996.46}  & \textbf{79.301.36} & \textbf{58.552.26} & \textbf{70.781.41}& \textbf{67.663.90}  & \textbf{77.133.60} \\
		     
		     \graphmix(GAT) &72.016.68  & 75.822.73 & 57.60.64 & 62.242.90  & 66.613.69 & 75.961.70 \\
		     \graphmix(Graph U-Net) &66.846 5.10 &73.143.17 & 54.395.07 & 64.363.48   & 67.405.33 &70.433.75  \\
		     
		     \midrule
	    \end{tabular}}
	}
	\end{center}
\end{table*}





\subsection{Implementation and Hyperparameter Details}
\label{app:hyper}
We use the standard benchmark architecture as used in GCN \citep{kipf2016semi}, GAT \citep{velivckovic2018graph} and GMNN \citep{gmnn}, among others. This architecture has one hidden layer and the graph convolution is applied twice : on the input layer and on the output of the hidden layer. The FCN in \graphmix shares the parameters with the GCN.

\graphmix introduces four additional hyperparameters, namely the  parameter of  distribution used in Manifold Mixup training of the FCN, the maximum weighing coefficient  which controls the trade-off between the supervised loss and the unsupervised loss (loss computed using the predicted-targets) of FCN, the temparature  in sharpening and the number of random perturbations  applied to the input data for the averaging of the predictions.

We conducted minimal hyperparameter seach over only  and   and fixed the hyperparameters  and  to  and  respectively. The other hyperparameters were set  to the best values for underlying GNN (GCN or GAT), including the learning rate, the  decay rate, number of units in the hidden layer etc. We observed that \graphmix is not very sensitive to the values of  and  and similar values of these hyperparameters work well across all the benchmark datasets. Refer to Appendix \ref{app:hyper} and \ref{app:hyper_selection}  for the details about the hyperparameter values and the procedure used for the best hyperparameters selection.


\subsubsection{For results reported in Section \ref{subsec:results}}
\label{sec:hyper_main}
 
For GCN and GraphMix(GCN), we used Adam optimizer with learning rate  and -decay 5e-4, the number of units in the hidden layer  , dropout rate in the input layer and hidden layer was set to  and , respectively. For GAT and GraphMix(GAT), we used Adam optimizer with learning rate  and -decay 5e-4, the number of units in the hidden layer  , and the dropout rate in the input layer and hidden layer was searched from the values  .

For   and  of \graphmixgcn and \graphmixgat, we searched over the values in the set  and    respectively.

For \graphmixgcn:  works best across all the datasets. 
 works best for Cora and Citeseer and  works best for Pubmed.

For \graphmixgat:  works best for Cora and Citeseer and   works best for Pubmed.  works best for Cora and Citeseer and  works best for Pubmed. Input droputrate=0.5  and hidden dropout rate=0.5 work best for Cora and Citeseer and Input dropout rate=0.2 and hidden dropout rate =0.2 work best for Pubmed.

We conducted all the experiments for 2000 epochs. The value of weighing coefficient  in Algorithm \ref{alg:graphmix}) is increased from  to its maximum value  from epoch 500 to 1000 using the sigmoid ramp-up of Mean-Teacher \citep{meanteacher}. 

\subsubsection{Hyperparameter Details for Results in Section \ref{app:larger_datasets}}
\label{app:hyper_largerdata}
For all the experiments we use the standard architecture mentioned in Section \ref{app:hyper} and used Adam optimizer with learning rate 0.001 and 64 hidden units in the hidden layer. For Coauthor-CS and Coauthor-Physics, we trained the network for 2000 epochs. For Cora-Full, we trained the network for 5000 epochs because we observed the training loss of Cora-Full dataset takes longer to converge.

For Coauthor-CS and Coauthor-Physics:  We set the input layer dropout rate to 0.5 and weight-decay to 0.0005, both for GCN and \graphmixgcn. We did not conduct any hyperparameter search over the \graphmix hyperparameters , , temparature  and number of random permutations  applied to the input data for \graphmixgcn for these two datasets, and set these values to , ,  and  respectively.  

For Cora-Full dataset: We found  input layer dropout rate  0.2 and weight-decay  0.0 to be the best for both GCN and \graphmixgcn.
For \graphmixgcn we fixed ,  temparature  and number of random permutations  to   and  respectively. For  , we did search over  and found that  works best.

For all the \graphmixgcn experiments, the value of weighing coefficient   in Algorithm \ref{alg:graphmix}) is increased from  to its maximum value  from epoch 500 to 1000 using the sigmoid ramp-up of Mean-Teacher \citep{meanteacher}. 

\subsubsection{For results reported in Section \ref{linkclass}}
For  of \graphmixgcn, we searched over the values in the set  and found that  works best for both the datasets. For , we searched over the values in the set  and found that  works best for both the datasets. We conducted all the experiments for 150 epochs. The value of weighting coefficient   in Algorithm \ref{alg:graphmix}) is increased from  to its maximum value  from epoch 75 to 125 using the sigmoid ramp-up of Mean-Teacher \citep{meanteacher}.

Both for \graphmixgcn and GCN, we use Adam optimizer with learning rate  and -decay , the number of units in the hidden layer  , dropout rate in the input layer  was set to . 


\subsubsection{For results reported in Section \ref{sec:fewerlabels}}
For  of \graphmixgcn, we searched over the values in the set  and found that  works best across all the datasets. For , we searched over the values in the set  and found that  and  works best across all the datasets. Rest of the details for \graphmixgcn and GCN are same as Section \ref{sec:hyper_main}.




\subsection{Hyperparameter Selection}
\label{app:hyper_selection}

For each configuration of hyperparameters, we run the experiments with  random seeds. We select the hyperparameter configuration which has the best validation accuracy averaged over these  trials. With this best hyperparameter configuration, for  random seeds,  we train the model again and use the validataion set for model selection ( i.e. we report the test accuracy at the epoch which has best validation accuracy.)





\begin{figure*}
    \centering
    \includegraphics[width=0.32\linewidth,trim={1.1cm 0.5cm 1.6cm 1.4cm},clip]{figures/featureviz/cora_gcn.png}
    \includegraphics[width=0.32\linewidth,trim={1.1cm 0.5cm 1.6cm 1.4cm},clip]{figures/featureviz/pubmed_gcn.png}
    \includegraphics[width=0.32\linewidth,trim={1.1cm 0.5cm 1.6cm 1.4cm},clip]{figures/featureviz/citeseer_gcn.png}
    \includegraphics[width=0.32\linewidth,trim={1.1cm 0.5cm 1.6cm 1.4cm},clip]{figures/featureviz/cora_graphmix.png}
    \includegraphics[width=0.32\linewidth,trim={1.1cm 0.5cm 1.6cm 1.4cm},clip]{figures/featureviz/pubmed_graphmix.png}
    \includegraphics[width=0.32\linewidth,trim={1.1cm 0.5cm 1.6cm 1.4cm},clip]{figures/featureviz/citeseer_graphmix.png}
    \caption{T-SNE of hidden states for Cora (left), Pubmed (middle), and Citeseer (right).  Top row is GCN baseline, bottom row is GraphMix.}
    \label{fig:fullfeatureviz}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.45\linewidth]{figures/train_loss.png}
    \includegraphics[width=0.45\linewidth]{figures/val_loss.png}
    \caption{GCN train loss and validation loss for Alternate optimization vs. weighted joint optimization. lambda = X.X represents the value of  for simultaneous optimization in Eq \ref{eq:graphmix_loss}. }
    \label{fig:alternate_optimization}
\end{figure*}

\subsection{Soft-Rank}
\label{app:softrank}
Let  be a matrix containing the hidden states of all the samples from a particular class. The Soft-Rank of  matrix  is defined by the sum of the singular values of the matrix divided by the largest singular value.  A lower Soft-Rank implies fewer dimensions with substantial variability and it provides a continuous analogue to the notion of rank from matrix algebra.  This provides evidence that the concentration of class-specific states observed when using GraphMix in Figure~\ref{fig:fullfeatureviz} can be measured directly from the hidden states and is not an artifact of the T-SNE visualization.  

    


\subsection{Feature Visualization}
\label{app:featureviz}
We present the 2D visualization of the hidden states learned using GCN and GraphMix(GCN) for Cora, Pubmed and Citeseer datasets in Figure \ref{fig:fullfeatureviz}. We observe that for Cora and Citeseer, GraphMix learns substantially better hidden states than GCN. For Pubmed, we observe that although there is no clear separation between classes, "Green" and "Red" classes overlap less using the GraphMix, resulting in better hidden states.


\subsection{Joint Optimization vs Alternate optimization}
\label{app:joint_alternate}

In this Section, we discuss the effect of hyperparameter , that is used to compute the weighted sum of GCN and FCN losses in in Eq \ref{eq:graphmix_loss}. In Figure \ref{fig:alternate_optimization}, we see that a wide range of  ( from 0.1 to 1.0) achieves better validation accuracy than the vanilla GCN. Furthermore, alternate optimization of the FCN loss and the GCN loss achieves substantially better validation accuracy than the simultaneous optimization. 





\end{document}