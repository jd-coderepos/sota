\section{Derivation for the Gaussian Posterior} \label{app: posterior}
\citet{ho2020denoising} provide a derivation for the Gaussian posterior distribution. We include it here for completeness. Consider the forward diffusion process in Eq. \ref{eq: forward process}, which we repeat here:

Due to the Markov property of the forward process, we have the marginal distribution of  given the initial clean data : 
 
where we denote  and . Applying Bayes' rule, we can obtain the forward process posterior
when conditioned on : 


where the second equation follows from the Markov property of the forward process. Since all three terms in Eq. \ref{posterior bayes} are Gaussians, the posterior  is also a Gaussian distribution, and it can be written as

with mean  and variance . Plugging the expressions in Eq. \ref{forward repeat} and Eq. \ref{marginal} into Eq. \ref{posterior ddpm}, we obtain

after minor simplifications.

\section{Parametrization of DDPM}\label{app: x0 prediction}
In Sec. \ref{sec:ddgan}, we mention that the parametrization of the denoising distribution for current diffusion models such as \citet{ho2020denoising} can be interpreted as . However, such a parametrization is not explicitly stated in \citet{ho2020denoising} but it is discussed by \citet{song2020denoising}. To avoid possible confusion, here we show that the parametrization of \citet{ho2020denoising} is equivalent to what we describe in Sec \ref{sec:ddgan}. 

\citet{ho2020denoising} train a noise prediction network  which predicts the noise that perturbs data  to , and a sample from  is obtained as (see Algorithm 2 of \citet{ho2020denoising})

where  except for the last denoising step where , and  is the standard deviation of the Gaussian posterior distribution in Eq. \ref{gaussian pos parameters}.

Firstly, notice that predicting the perturbation noise  is equivalent to predicting . We know that  is generated by adding  noise as:

Hence, after predicting the noise with  we can obtain a prediction of  using:



Next, we can plug the expression for  in Eq. \ref{eq x0 prediction} into the mean of the Gaussian posterior distribution in Eq. \ref{gaussian pos parameters}, and we have 

after simplifications. Comparing this with Eq. \ref{ddpm sampling}, we observe that Eq. \ref{ddpm sampling} simply corresponds to sampling from the Gaussian posterior distribution. Therefore, although \citet{ho2020denoising} use an alternative re-parametrization, their denoising distribution can still be equivalently interpreted as , i.e, first predicting  using the time-dependent denoising model, and then sampling  using the posterior distribution  given  and the predicted .


\section{Experimental Details} \label{app experimental detail}
In this section, we present our experimental settings in detail. 

\subsection{Network Structure}
\textbf{Generator: }Our generator structure largely follows the U-net structure \citep{ronneberger2015u} used in NCSN++ \citep{song2020score}, which consists of multiple ResNet blocks \citep{he2016deep} and Attention blocks \citep{vaswani2017attention}. Hyper-parameters for the network design, such as the number of blocks and number of channels, are reported in Table \ref{table generator structure}. We follow the default settings in \citet{song2020score} for other network configurations not mentioned in the table, including Swish activation function, upsampling and downsampling with anti-aliasing based on Finite Impulse Response (FIR) \citep{zhang2019making}, re-scaling all skip connections by , using residual block design from BigGAN \citep{brock2018large} and incorporating progressive growing architectures \citep{karras2020analyzing}. 
See Appendix H of \citet{song2020score} for more details on these configurations.

We follow \citet{ho2020denoising} and use sinusoidal positional
embeddings for conditioning on integer time steps. The dimension for the time embedding is  the number of initial channels presented in Table~\ref{table generator structure}. Contrary to previous works, we did not find the use of Dropout helpful in our case.

The fundamental difference between our generator network and the networks of previous diffusion models is that our generator takes an extra latent variable  as input. Inspired by the success of StyleGANs, we provide -conditioning to the NCSN++ architecture using \textit{mapping networks}, introduced in StyleGAN~\citep{karras2019style}. We use  for all experiments. We replace all the group normalization (GN) layers in the network with adaptive group normalization (AdaGN) layers to allow the input of latent variables. The latent variable  is first transformed by a fully-connected network (called mapping network), and then the resulting embedding vector, denoted by , is sent to every AdaGN layer. Each AdaGN layer contains one fully-connected layer that takes  as input, and outputs the per-channel shift and scale parameters for the group normalization. The network's feature maps are then subject to affine transformations using these shift and scale parameters of the AdaGN layers. The mapping network and the fully-connected layer in AdaGN are independent of time steps , as we found no extra benefit in incorporating time embeddings in these layers. Details about latent variables are also presented in Table \ref{table generator structure}. 

\begin{table}
\centering
\caption{Hyper-parameters for the generator network.}
\label{table generator structure}
\begin{tabular}{lccc}
\toprule
& CIFAR10 & CelebA-HQ & LSUN Church 
\\ \midrule
 of ResNet blocks per scale &2&2&2\\
Initial  of channels &128& 64&128\\
Channel multiplier for each scale &&& \\
Scale of attention block & 16 & 16 & 16\\
Latent Dimension &256&100&100\\
 of latent mapping layers &3&3&3\\
Latent embedding dimension &512&256&256\\
\bottomrule
\end{tabular}
\end{table}



\textbf{Discriminator: }We design our time-dependent discriminator with a convolutional network with ResNet blocks, where the design of the ResNet blocks is similar to that of the generator. The discriminator tries to discriminate real and fake , conditioned on  and . The time conditioning is enforced by the same sinusoidal positional
embedding as in the generator. The  conditioning is enforced by concatenating  and  as the input to the discriminator. We use LeakyReLU activations with a negative slope 0.2 for all layers. Similar to \citet{karras2020analyzing}, we use a minibatch standard deviation layer after all the ResNet blocks. We present the exact architecture of discriminators in Table \ref{table discriminator structure}. 

\begin{table}[ht]
  \caption{Network structures for the discriminator.  The number on the right indicates the number of channels in each residual block.}\label{table discriminator structure}
     \centering
     \begin{tabular}{c}
     \toprule
     CIFAR-10\\
     \midrule
      conv2d, 128\\
     ResBlock, 128\\
     ResBlock down, 256\\
     ResBlock down, 512\\
     ResBlock down, 512\\
     minibatch std layer\\
     Global Sum Pooling\\
     FC layer  scalar\\
     \midrule
     \end{tabular}
     \quad
     \begin{tabular}{c}
     \toprule
     CelebA-HQ and LSUN Church\\
     \midrule
      conv2d, 128\\
     ResBlock down, 256\\
     ResBlock down, 512\\
     ResBlock down, 512\\
     ResBlock down, 512\\
     ResBlock down, 512\\
     ResBlock down, 512\\
     minibatch std layer\\
     Global Sum Pooling\\
     FC layer  scalar\\
     \bottomrule
     \end{tabular}
 \end{table}
 
\subsection{Training}
\textbf{Diffusion Process: } For all datasets, we set the number of diffusion steps to be . In order to compute  per step, we use the discretization of the continuous-time extension of the process described in Eq. \ref{eq: forward process}, which is called the Variance Preserving (VP) SDE by \citet{song2020score}. We compute  based on the continuous-time diffusion model formulation, as it allows us to ensure that variance schedule stays the same independent of the number of diffusion steps. Let's define the normalized time variable by  which normalizes  to .
The variance function of VP SDE is given by: 

with the constants  and . Recall that sampling from  step in the forward diffusion process can be done with .
We compute  by solving :

for . This choice of  values corresponds to equidistant steps in time according to VP SDE. Other choices are possible, but we did not explore them.

\textbf{Objective: }We train our denoising diffusion GAN with the following adversarial objective:
\iffalse

\fi

where the outer expectation denotes ancestral sampling from  and  is our implicit GAN denoising distribution.


Similar to \citet{ho2020denoising}, during training we randomly sample an integer time step  for each datapoint in a batch. Besides the main objective, we also add an  regularization term \citep{mescheder2018training} to the objective for the discriminator. The  term is defined as

where  is the coefficient for the regularization. We use  for CIFAR-10, and  for CelebA-HQ and LSUN Church. Note that the  regularization is a gradient penalty that encourages the discriminator to stay smooth and improves the convergence of GAN training \citep{mescheder2018training}. 


\textbf{Optimization: }We train our models using the Adam optimizer \citep{kingma2015adam}. We use cosine learning rate decay \citep{loshchilov2016sgdr} for training both the generator and discriminator. Similar to \citet{ho2020denoising, song2020score, karras2020training}, we observe that applying an exponential moving average (EMA) on the generator is crucial to achieve high performance. We summarize the optimization hyper-parameters in Table \ref{table optimizer}. 

We train our models on CIFAR-10 using 4 V100 GPUs. On CelebA-HQ and LSUN Church we use 8 V100 GPUs. The training takes approximately 48 hours on CIFAR-10, and 180 hours on CelebA-HQ and LSUN Church. 


\begin{table}
\centering
\caption{Optimization hyper-parameters.}
\label{table optimizer}
\begin{tabular}{lccc}
\toprule
& CIFAR10 & CelebA-HQ & LSUN Church 
\\ \midrule
Initial learning rate for discriminator &&  & \\
Initial learning rate for generator &&  & \\
Adam optimizer  &0.5&0.5& 0.5\\
Adam optimizer  &0.9&0.9&0.9\\
EMA &0.9999&0.999&0.999\\
Batch size &128&32&64\\
 of training iterations &400k&750k&600k\\
 of GPUs &4&8&8\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Evaluation}
When evaluating IS, FID and recall score, we use 50k generated samples for CIFAR-10 and LSUN Church, and 30k samples for CelebA-HQ (since the CelebA HQ dataset contains only 30k samples). 

When evaluating sampling time, we use models trained on CIFAR-10 and generate a batch of 100 samples. We benchmark the sampling time on a machine with a single V100 GPU. We use Pytorch 1.9.0 and CUDA 11.0. 

\subsection{Ablation Studies}
Here we introduce the settings for the ablation study in Sec. \ref{sec: ablation}. We observe that training requires a larger number of training iterations when  is larger. As a result, we train the model for each  until the FID score does not increase any further. The number of training iteration is 200k for  and , 400k for  and 600k for . We use the same network structures and optimization settings as in the main experiments.

For the data augmentation baseline, we follow the differentiable data augmentation pipeline in \citet{zhao2020differentiable}. In particular, for every (real or fake) image in the batch, we perturbed it by sampling from a random timestep at the diffusion process (except the last diffusion step where the information of data is completely destroyed). We find the results insensitive to the number of possible perturbation levels (i.e, the number of steps in the diffusion process), and we report the result using a diffusion process with 4 steps. Since the perturbation by the diffusion process is differentiable due to the re-parametrization trick \citep{kingma2014vae}, we can train both the discriminator and generator with the perturbed samples. See \citet{zhao2020differentiable} for a detailed explanation for the training pipeline.

For the experiments on alternative parametrizations, we use  for the diffusion process and keep other settings the same as in the main experiments.

For the experiment on training a model without latent variables, similar to the main experiments, the generator takes the conditioning  as its input, and the time conditioning is still enforced by the time embedding. However, the AdaGN layers are replaced by plain GN layers, such that no latent variable is needed, and the mapping network for  is removed. Other settings follow the main experiments.


\subsection{Toy data and StackedMNIST}
For the 25-Gaussian toy dataset, both our generator and discriminator have 3 fully-connected layers each with 512 hidden units and LeakyReLU activations (negative slope of 0.2). We enforce both the conditioning on  and  by concatenation with the input. We use the Adam optimizer with a learning rate of  for both the generator and discriminator. The batch size is 512, and we train the model for 50k iterations.

Our experimental settings for StackedMNIST are the same as those for CIFAR-10, except that we train the model for only 150k iterations. 


\section{Training Stability} \label{sec: stability}
In Fig. \ref{fig: loss curve}, we plot the discriminator loss for different time steps in the diffusion process when . We observe that the training of our denoising diffusion GAN is stable and we do not see any explosion in loss values, as is sometimes reported for other GAN methods such as \cite{brock2018large}. The stability might be attributed to two reasons: First, the conditioning on  for both generator and discriminator provides a strong signal. The generator is required to generate a few plausible samples given  and the discriminator requires classifying them. The  conditioning keeps the discriminator and generator in a balance. Second, we are training the GAN on relatively smooth distributions, as the diffusion process is known as a smoothening process that brings the distributions of fake and real samples closer to each other~\citep{lyu2012interpretation}. As we can see from Fig. \ref{fig: loss curve}, the discriminator loss for  is higher than  (the last denoising step). Note that  corresponds to training the discriminator on noisy images, and in this case the true and generator distributions are closer to each other, making the discrimination harder and hence resulting in higher discriminator loss. We believe that such a property prevents the discriminator from overfitting, which leads to better training stability.

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.63]{iclr2022/figures/exp/loss_curve.png}
    \caption{\label{fig: loss curve}
     The discriminator loss per denoising step during training.}
\end{figure*}

\section{Additional Qualitative Results} \label{app: extra sample}
We show additional qualitative samples of CIFAR-10, CelebA-HQ and LSUN Church Outdoor in Figure \ref{fig: cifar extra}, Figure \ref{fig: celeba extra} and Figure \ref{fig: lsun extra}, respectively.

\section{Additional Visualization for } \label{app: x0 visualization}
In Figure \ref{fig: celeba x0} and Figure \ref{fig: lsun x0}, we show visualizations of samples from  for different . Note that except for , the samples from  do not need to be sharp, as they are only intermediate outputs of the sampling process. The conditioning is less preserved as the perturbation in  increases, and in particular  ( in our example) contains almost no information of clean data .

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.75]{iclr2022/figures/exp/cifar10_large.png}
    \caption{\label{fig: cifar extra}
     Additional qualitative samples on CIFAR-10.}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.26]{iclr2022/figures/exp/celeba_48.jpg}
    \caption{\label{fig: celeba extra}
     Additional qualitative samples on CelebA-HQ.}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.26]{iclr2022/figures/exp/lsun_48.jpg}
    \caption{\label{fig: lsun extra}
     Additional qualitative samples on LSUN Church Outdoor.}
\end{figure*}


\begin{figure}[h]
    \begin{subfigure}{.99\linewidth}
    \centering
    \includegraphics[scale=0.6]{iclr2022/figures/exp/celeba_last_1.png}
    \end{subfigure} \begin{subfigure}{.99\linewidth}
    \centering
    \includegraphics[scale=0.6]{iclr2022/figures/exp/celeba_last_2.png}
\end{subfigure}
\caption{Visualization of samples from  for different  on CelebA-HQ. For each example, the top row contains  from diffusion process steps, where  is a sample from the dataset. The bottom rows contain 3 samples from  for different 's.}\label{fig: celeba x0}
\end{figure}

\begin{figure}[h]
    \begin{subfigure}{.99\linewidth}
    \centering
    \includegraphics[scale=0.6]{iclr2022/figures/exp/lsun_last_1.png}
\end{subfigure} \begin{subfigure}{.99\linewidth}
    \centering
    \includegraphics[scale=0.6]{iclr2022/figures/exp/lsun_last_2.png}
\end{subfigure}
\caption{Visualization of samples from  for different  on LSUN Church. For each example, the top row contains  from diffusion process steps, where  is a sample from the dataset. The bottom rows contain 3 samples from  for different 's. }\label{fig: lsun x0}
\end{figure}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.1\linewidth}
    \includegraphics[scale=0.7]{iclr2022/figures/exp/cifar_nn_sample.png}
    \end{subfigure}
    \begin{subfigure}{.65\linewidth}
    \includegraphics[scale=0.7]{iclr2022/figures/exp/cifar_nn_data.png}
    \end{subfigure}
    \caption{\label{nn cifar}
    CIFAR-10 nearest neighbors in VGG feature space. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.09\linewidth}
    \includegraphics[scale=0.13]{iclr2022/figures/exp/celeba_nn_sample.jpg}
    \end{subfigure}
    \begin{subfigure}{.9\linewidth}
    \includegraphics[scale=0.2785]{iclr2022/figures/exp/celeba_nn_data.jpg}
    \end{subfigure}
    \caption{\label{nn celeba}
     CelebA-HQ nearest neighbors in the VGG feature space. Generated samples are in the leftmost column, and training set nearest neighbors are in the
     remaining columns.}
\end{figure*}



\section{Nearest Neighbor Results} \label{app: nearest neighbor}
In Figure \ref{nn cifar} and Figure \ref{nn celeba}, we show the nearest neighbors in the training dataset, corresponding to a few generated samples, where the nearest neighbors are computed using the feature distance of a pre-trained VGG network \citep{simonyan2014very}. We observe that the nearest neighbors are significantly different from the samples, suggesting that our models generalize well.