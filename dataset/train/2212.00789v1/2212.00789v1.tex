

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bbm}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{8031} \def\confName{CVPR}
\def\confYear{2023}

\begin{document}

\title{Attribute-based Representations for\\ Accurate and Interpretable Video Anomaly Detection}


\author{Tal Reiss, Yedid Hoshen\\
School of Computer Science and Engineering\\
The Hebrew University of Jerusalem, Israel\\
}
\maketitle

\begin{abstract}
Video anomaly detection (VAD) is a challenging computer vision task with many practical applications. As anomalies are inherently ambiguous, it is essential for users to understand the reasoning behind a system's decision in order to determine if the rationale is sound. In this paper, we propose a simple but highly effective method that pushes the boundaries of VAD accuracy and interpretability using attribute-based representations. Our method represents every object by its velocity and pose. The anomaly scores are computed using a density-based approach. Surprisingly, we find that this simple representation is sufficient to achieve state-of-the-art performance in ShanghaiTech, the largest and most complex VAD dataset. Combining our interpretable attribute-based representations with implicit, deep representation yields state-of-the-art performance with a , and  AUROC on Ped2, Avenue, and ShanghaiTech, respectively. Our method is accurate, interpretable, and easy to implement. Our code is available at \url{https://github.com/talreiss/Accurate-Interpretable-VAD}.
\end{abstract}


\section{Introduction}
\label{sec:intro}
Video anomaly detection (VAD) is a key goal of surveillance but is very challenging. One of the most common VAD settings is the one-class classification (OCC). In this setting, only normal videos are seen during the training stage without any anomalies. At deployment, the trained model is required to distinguish between normal events and those that are abnormal in \textit{a semantically meaningful way}. The key difficulty is that the difference between patterns that are semantically meaningful and those that are not is subjective. In fact, two operators may disagree on whether an event is anomalous. Furthermore, as no labeled anomalies are provided for training, it is not possible to directly learn the discriminative patterns.

VAD has been researched for decades, but the advent of deep learning has brought significant breakthroughs. Recent approaches to anomaly detection follow two main directions: (i) handcrafted priors for self-supervised learning: many methods designed auxiliary tasks such as rotation prediction, invariance to hand-selected augmentations, predicting the arrow of time and rate of temporal flow. These approaches dominate video anomaly detection. (ii) Representation extraction using pretrained encoder: a two-stage approach which first computes representations using pretrained encoders (such as ResNet pretrained on ImageNet), followed by standard density estimation such as NN or Mahalanobis distance. This approach is successful in image anomaly detection and segmentation. The issue with both approaches is the representations that they learn are opaque and non-interpretable. As anomalies are ambiguous, it is essential that the reasoning is made explicit for the researcher to understand if the criteria for the decision are justified. 

Most state-of-the-art anomaly detection methods are not interpretable, despite their use in safety-critical applications. In this paper, we follow a new direction, representing data using semantic attributes which are meaningful to humans and therefore easier to interpret. 
Our method extracts representations consisting of the velocity and pose attributes. We selected these attributes, as previous works e.g. \cite{shai_avidan,Georgescu2021AnomalyDI} highlighted their importance. We use the representations to score anomalies by density estimation. Our method classifies frames as anomalous if their velocity and/or pose take an unusual value. This allows automatic interpretation; the attribute taking an unusual value is interpreted to be the rationale behind the decision (see \cref{fig:interpretability}).

We obtain the surprising results that our simple attribute-based velocity and pose representation achieves state-of-the-art performance on the largest and most complex VAD dataset with 85.9\% AUROC in ShanghaiTech. While our attribute-based representation is very powerful, there are concepts that are not adequately represented by it. The reason is that some attributes cannot simply be quantified using semantic human attributes. Consequently, to model the residual attributes, we couple our explicit attribute-based representation with an implicit, deep representation, obtaining the best of both worlds. Our final method achieves state-of-the-art performance on the three most commonly reported datasets while being highly interpretable.

The advantages of our method are three-fold:
\begin{enumerate}
    \item Achieving state-of-the-art results in the three most commonly used public datasets: 99.1\%, 93.3\%, 85.9\% AUROC on Ped2, Avenue and ShanghaiTech.
    \item Making interpretable decisions, making our method applicable to critical environments where human understanding is key.
    \item Being easy to implement.
\end{enumerate}

\begin{figure*}[t]
  \centering
    \begin{tabular}{c}
    \includegraphics[scale=1]{figures/interpretability.png} 
    \end{tabular}
    \caption{Human-interpretable visualizations on Avenue and ShanghaiTech. We present the most normal and anomalous frames for each feature. For anomalous frames, we visualize the bounding box of the object with the highest anomaly score. Best viewed in color.}
    \label{fig:interpretability}
\end{figure*}

\section{Related Work}
Classical video anomaly detection methods were typically composed of two steps: handcrafted feature extraction and anomaly scoring. Some of the manual features that were extracted were: optical flow histograms \cite{hoof3,hoof,hoof2} and SIFT \cite{sift}. Commonly used scoring methods include: density estimation \cite{knn,gmm,kde}, reconstruction \cite{pca}, and one-class classification \cite{ocsvm}. 

In recent years, deep learning has gained in popularity as an alternative to these early works. The majority of video anomaly detection methods utilize at least one of three paradigms: reconstruction-based, prediction-based, or auxiliary classification-based methods. 

\textbf{Reconstruction \& prediction based methods.} In the reconstruction paradigm, the normal train data is typically characterized by an autoencoder, which is then used to reconstruct input video clips. The assumption is that a model trained solely on normal train clips will not be able to reconstruct anomalous frames. This assumption does not always hold true, as neural networks can often generalize to some extent out-of-distribution. Notable works are \cite{reconstruction_am_corr,reconstruction_clusterAE,reconstruction_convae,reconstruction_stackrnn,yu2020cloze,Park2020LearningMN}. 

Prediction-based methods learn to predict frames or flow maps in video clips, including inpainting intermediate frames and predicting future frames \cite{shanghaitech,prediction_1,prediction_2,lee2019bman,prediction_3,Park2020LearningMN,prediction_4,Feng2021ConvolutionalTB,yu2020cloze}. Additionally, some works take a hybrid approach combining the two paradigms \cite{vad_iccv_2021,hybrid_1,hybrid_2,hybrid_3,hybrid_4}. As these methods are trained to optimize both objectives, input frames with large reconstruction or prediction errors are considered anomalous.

\textbf{Self-supervised auxiliary tasks.} There has been a great deal of research on learning from unlabeled data. A common approach is to train neural networks on suitably designed auxiliary tasks with automatically generated labels. Tasks include: video frame prediction \cite{mathieu2015deep}, image colorization \cite{zhang2016colorful,larsson2016learning}, puzzle solving \cite{noroozi2016unsupervised}, rotation prediction \cite{gidaris2018unsupervised}, arrow of time \cite{arrow_of_time}, predicting playback velocity \cite{playback_speed} and verifying frame order \cite{frame_order}. Many video anomaly detection methods use self-supervised learning. In fact, self-supervised learning is a key component in the majority of reconstruction-based and prediction-based methods. SSMTL \cite{Georgescu2021AnomalyDI} trains a CNN jointly on three auxiliary tasks: arrow of time, motion irregularity and middle box prediction, in addition to knowledge distillation. Jigsaw-Puzzle \cite{jigsaw_puzzles_eccv2022} trains neural networks to solve spatio-temporal jigsaw puzzles. The networks are then used for VAD.

\textbf{Object-level video anomaly detection.}  Early methods, both classical and deep learning, operated on entire video frames. This proved difficult for anomaly detection as frames contain many variations, as well as a large number of objects. More recent methods \cite{Georgescu2021AnomalyDI,vad_iccv_2021,jigsaw_puzzles_eccv2022} operate at the object level by first extracting object bounding boxes using off-the-shelf object detectors. Then, they detect if each object is anomalous. This is an easier task, as objects contain much less variation than whole frames. Object-based methods yield significantly better results than frame-level methods. 

It is often believed that due to the complexity of realistic scenes and the variety of behaviors, it is difficult to craft features that will discriminate between them. As object detection was inaccurate prior to deep learning, classical methods were previously applied at the frame level rather than at the object level, and therefore underperformed on standard benchmarks. This paper breaks this misconception and demonstrates that it is possible to craft semantic features that are both accurate and interpretable.

\section{Preliminaries}
In the VAD task, we are given a training set  consisting of  video clips that are all normal (i.e., do not contain any anomalies). Each clip  is comprised of  frames, . Given an inference clip  the goal is to classify each frame  as being normal or anomalous. Each frame  is represented using a function , where  is the feature dimension. Next, an anomaly scoring function  computes the anomaly score for . The frame is classified as anomalous if  exceeds a constant threshold.

\section{Methodology}
\label{sec:method}
\subsection{Overview}
Our method contains three stages: pre-processing, feature extraction, and density estimation. In our pre-processing stage, an off-the-shelf motion estimator is applied to predict the optical flow of each frame. An off-the-shelf object detector is also used to localize and classify the bounding boxes of all objects within a frame. We then use the outputs of both models to extract velocity, pose, and deep representations (see \cref{met:features}) to create object-level feature descriptors. Finally, the anomaly score of each test frame is computed using density estimation. The first two stages are illustrated in \cref{fig:framework}.

\subsection{Pre-processing}
Anomalous objects in video clips typically exhibit unusual motions or activities. Therefore, we rely on representations that are linked to objects and motions.

\textbf{Optical flow.} Our method uses optical flow as a preliminary stage for inferring object movement. It is computed between every pair of two successive frames. We extract the optical flow for each frame  in every video clip  using an off-the-shelf optical flow model. The optical flow map is denoted by .

\textbf{Object detection.} Our method models frames by representing every object individually. This follows many recent papers, e.g., \cite{Georgescu2021AnomalyDI,vad_iccv_2021,jigsaw_puzzles_eccv2022} that found that object-based representations to be more effective than global, frame-level representations. Similarly to the recent papers, we first detect all objects in each frame using an off-the-shelf object detector. Formally, our object detection generates a set of  bounding boxes  for each frame, with corresponding class labels .

\begin{figure*}[t]
  \centering
    \begin{tabular}{c}
    \includegraphics[scale=1]{figures/framework.png} 
    \end{tabular}
    \caption{An overview of our proposed method for extracting explicit attribute-based representations, along with implicit, deep representations. As a first step, we extract optical flow maps and bounding boxes for all of the objects in the frame. We then crop each object from the original image and its corresponding optical flow map. Our representation consists of velocity, pose, and deep (CLIP) features.}
    \label{fig:framework}
\end{figure*}


\subsection{Feature extraction}
\label{met:features}
Our method represents each object by two attributes: velocity and pose:

\textbf{Velocity features.} Our working hypothesis is that unusual velocity is a relevant attribute for identifying anomalies in video. As objects can move in both  and  axes and both the magnitude (speed) and orientation of the velocity may be anomalous, we represent each object in each frame by velocity features. We begin by cropping the frame-level optical flow map by the bounding box of each object detected by the object detector. Following this step, we obtain a set of cropped object flow maps, as illustrated in \cref{fig:framework}. These flow maps are then rescaled to a fixed size of . Next, we propose to use as a feature the average motion for each orientation (a similar idea as HOOF \cite{hoof3}). The orientation for each flow vector is computed as . The orientations are quantized into  equi-spaced bins and the final representation consists of the average flow magnitudes of the flow vectors assigned to each bin. This representation is capable of describing motion in both the radial and tangential directions. Formally, let  be a flow map of an object and let . Then all flow vectors  of  with direction  in the range:

will contribute  to the sum in bin . As a final step, the flow vectors magnitudes that contributed to the b'th bin are averaged. We denote our velocity feature extractor as: .


\textbf{Pose features.} Irregular human activity is often anomalous. While a full understanding of activity requires temporal features, we find that human pose from even a single frame may provide a sufficiently discriminative signal of irregular activities. We represent human pose by its body landmark positions. Our method obtains pose feature descriptors for each human object  using an off-the-shelf deep keypoints extractor, denoted by , where  is the number of keypoints. In practice, we used AlphaPose \cite{alphapose}, which we found to work well. The output of the keypoints extractor is the pixel coordinates of each landmark position. We perform a simple normalization stage to ensure that the keypoints are invariant to the position and size of the human.  We first subtract from each landmark, the coordinates of the top-left corner of the object bounding box. We then scale the  and  axes so that the object bounding box has a final size of  (where  are constants). Formally, let  be the top-left corner of the human bounding box. The pose description becomes:

Where  is the object  bounding box height and width respectively. Finally, we flatten  to obtain the final pose feature vector.

\textbf{Deep features.} While our attribute-based representation is already very powerful, it is sometimes insufficiently expressive to detect all anomalies. Powerful deep features are very expressive, bundling together many different attributes.
Hence, we use implicit, deep representations to model the residual attributes which are not described by velocity and pose.  Following the results from the image anomaly detection literature, the implicit representations are pretrained on external, generic datasets and transferred to the anomaly detection task. Previous work \cite{panda,mean_shifted} showed that coupling such powerful representations with simple anomaly detection classifiers (e.g., NN) can achieve outstanding results. Concretely, our implicit representation is computed using a pretrained CLIP encoder \cite{clip}, denoted by , to represent each object bounding box in each frame.  

\begin{table*}[t]
  \centering
  \begin{tabular}{|l|c|c|c|c|c|c|c|}
    \hline
\multirow{2}{*}{Dataset} & \multicolumn{5}{c|}{Number of Frames}  & \multirow{2}{*}{Scenes} & \multirow{2}{*}{Anomaly Types} \\
\cline{2-6}
& Total & Train set & Test set &  Normal & Anomalous & & \\
\hline
UCSD Ped2 \cite{ped2} & 4,560 & 2,550 & 2,010 & 2,924 & 1,636 & 1 & 5 \\
\hline
CUHK Avenue \cite{avenue} & 30,652 & 15,328 & 15,324 & 26,832 & 3,820 & 1 & 5\\
\hline
ShanghaiTech  \cite{shanghaitech} & 317,398 & 274,515 & 42,883 & 300,308 & 17,090 & 13 & 11\\
    \hline
  \end{tabular}
    \caption{Statistics of the evaluation datasets.}
    \label{tab:statsistics}
\end{table*}

\subsection{Density Estimation}
\label{met:density}
We use density estimation for scoring samples as normal or anomalous, where low estimated density is indicative of anomaly. To estimate the density, we fit a separate estimator for each feature. For velocity features, which are lower dimensional, we use a Gaussian mixture model (GMM) estimator. As our pose and deep features are high-dimensional and are not assumed to obey particular parametric assumptions, we estimate their density using NN. I.e., we compute the  distance between feature  of a target object and the  exemplars in the corresponding train feature set. See Sec.~\ref{experiments} for a comparison between different exemplar selection methods. We denote our density estimators by .

\textbf{Score calibration.}  Combining the three density estimators requires calibration. To do so, we estimate the distribution of anomaly scores on the normal training set. We then scale the scores using min-max normalization. The NN used for scoring pose and deep features present a subtle point. When computing NN on the training set, the exemplars must not be taken from the same clip as the target object. The reason is that the same object appears in nearby frames with virtually no variation, distorting NN estimates. Instead, we compute the NN between each training set object and all objects in the other video clips provided in the training set.

We now can define the following parameters:


\subsection{Inference} Each inference clip  is fed frame by frame into both the optical flow estimator and the object detector. We then extract three features from each object :

where  is the number of detected objects. We train density estimators for each feature. The score for every frame is simply the maximum score across all objects. The final anomaly score is the sum of the individual feature scores normalized by our calibration parameters computed in \cref{eq:calibration}.

Let us denote the anomaly score for every frame in clip  as . As we expect anomalous events to be prolonged, we smooth the results by applying a temporal 1-D Gaussian filter over .

\begin{table*}[t]
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
	\centering
	\begin{tabular}{| c | l | c | c | c | c | c | c |}
		\hline
		\multirow{2}{*}{Year} & \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Ped2} & \multicolumn{2}{c|}{Avenue} & \multicolumn{2}{c|}{ShanghaiTech} \\ 
		\cline{3-8}
		& & Micro & Macro & Micro & Macro & Micro & Macro \\
		\hline
\multirow{5}{*}{Before 2018} & HOOF \cite{hoof3}  & 61.1 & - & - & - & - & - \\
 & HOFM \cite{hoof}  & 89.9 & - & - & - & - & - \\
 & SCL \cite{avenue}  & - & - & 80.9 & - & - & - \\
 & Conv-AE \cite{2016_CONVAE}  & 90.0 & - & 70.2 & - & - & - \\
 & StackRNN \cite{2017_STACKRNN}  & 92.2 & - & 81.7 & - & 68.0 & - \\
\hline
\multirow{3}{*}{2018} & STAN \cite{stan} & 96.5 & - & 87.2 & - & - & - \\
& MC2ST \cite{bmvc2018} & 87.5 & - & 84.4 & - & - & - \\
& Frame-Pred. \cite{shanghaitech}  & 95.4 & - & 85.1 & - & 72.8 & - \\
\hline
\multirow{4}{*}{2019} & Mem-AE. \cite{gong2019iccv}  & 94.1 & - & 83.3 & - & 71.2 & - \\
& CAE-SVM \cite{ionescu2019cvpr}  &  94.3 & 97.8 & 87.4 & 90.4 & 78.7 & 84.9 \\
& BMAN \cite{lee2019bman}  & 96.6 & - & 90.0 & - & 76.2 & - \\
& AM-Corr \cite{reconstruction_am_corr} & 96.2 & - & 86.9 & - & - & - \\ 
\hline
\multirow{5}{*}{2020} & MNAD-Recon. \cite{Park2020LearningMN}  & 97.0 & - & 88.5 & - & 70.5 & -\\
& CAC \cite{Wang2020ClusterAC}  &  - & - & 87.0 & - & 79.3 & - \\
& Scene-Aware \cite{Sun2020SceneAwareCR}  & - & - & 89.6 & - & 74.7 & - \\
& VEC \cite{yu2020cloze}  & 97.3 & - & 90.2 & - & 74.8 & -\\
& ClusterAE \cite{reconstruction_clusterAE} & 96.5 & - & 86.0 & - & 73.3 & - \\
\hline
\multirow{6}{*}{2021} & AMMCN \cite{Cai2021AppearanceMotionMC} & 96.6 & - & 86.6 & - & 73.7 & - \\
& SSMTL \cite{Georgescu2021AnomalyDI}  & 97.5 & 99.8 & 91.5 & 91.9 & 82.4 & 89.3 \\
& MPN \cite{Lv_2021_CVPR}  & 96.9 & - & 89.5 & - & 73.8 & - \\
& HF \cite{Liu_2021_ICCV} & \textbf{99.3} & - & 91.1 & \underline{93.5} & 76.2 & -\\
& CT-D2GAN \cite{Feng2021ConvolutionalTB} & 97.2 & - & 85.9 & - & 77.7 & -\\
& BA-AED \cite{georgescu2021tpami}  & 98.7 & 99.7 & 92.3 & 90.4 & 82.7 & 89.3 \\
\hline
\multirow{4}{*}{2022} & BA-AED \cite{georgescu2021tpami} + SSPCAB \cite{attentive_block_cvpr2022} & - & - & \underline{92.9} & 91.9 & 83.6 & 89.5 \\
& DLAN-AC \cite{eccv2022_dynamic} & 97.6 & - & 89.9 & - & 74.7 & - \\
& Jigsaw-Puzzle \cite{jigsaw_puzzles_eccv2022} & 99.0 & \textbf{99.9} & 92.2 & 93.0 & \underline{84.3} & \textbf{89.8} \\
\cline{2-8}
& Ours & \underline{99.1} & \textbf{99.9} & \textbf{93.3} & \textbf{96.2} & \textbf{85.9} & \underline{89.6}\\
\hline
\end{tabular}
\caption{Frame-level AUROC (\%) comparison. The best and second-best results are bolded and underlined, respectively.}
	\label{tab:AUC_compare}
\end{table*}


\section{Experiments}

\subsection{Datasets}
Our experiments were conducted using three publicly available video anomaly detection datasets. Training and test sets are defined for each dataset, and anomalous events are only included during testing. All datasets were collected outdoors. We report the statistics of the datasets in \cref{tab:statsistics}.

\textbf{UCSD Ped2.} The Ped2 dataset \cite{ped2} contains 16 normal training videos and 12 test videos at a  pixel resolution. Videos are gathered from a fixed scene with a camera above the scene and pointed downward. The training video clips contain only normal behavior of pedestrians walking, while examples of abnormal events are bikers, skateboarding, and cars. 

\textbf{CUHK Avenue.} The Avenue dataset \cite{avenue} contains 16 normal training videos and 21 test videos at  pixel resolution. Videos are gathered from a fixed scene using a ground-level camera. The training video clips contain only normal behavior. Examples of abnormal events are: strange activities (e.g. throwing objects, loitering, and running), movement in the wrong direction, and abnormal objects.

\textbf{ShanghaiTech Campus.} The ShanghaiTech dataset \cite{shanghaitech} is the largest publicly available dataset for VAD. There are 330 training videos and 107 test videos from 13 different scenes at  pixel resolution. ShanghaiTech contains video clips with complex light conditions and camera angles, making this dataset more challenging than the other two. Anomalies include robberies, jumping, fights, car invasions, and bike riding in pedestrian areas.

\subsection{Implementation Details}
We use ResNet50 Mask-RCNN \cite{mask_rcnn} pretrained on MS-COCO \cite{mscoco} to extract object bounding boxes. To filter-out low confidence objects, we follow the same configurations as in \cite{Georgescu2021AnomalyDI}. Specifically for Ped2, Avenue, and ShanghaiTech, we set confidence thresholds of 0.5, 0.8, and 0.8. The confidence thresholds are equal in both train and test sets. In order to generate optical flow maps, we use FlowNet2.0 \cite{flownet2}. For our landmark detection, we use AlphaPose \cite{alphapose} pretrained on MS-COCO with  keypoints. We use a pretrained ViT B-16 \cite{vision_transformer} CLIP \cite{clip} image encoder as our deep feature extractor. Our method is built around the extracted objects and flow maps. We use  to rescale flow maps. As for  rescaling, we calculate the average height and width from the bounding boxes of the train set and use those values. The lower resolution of Ped2 prevents objects from filling a histogram, and to extract pose representations, therefore we use  orientations and rely solely on velocity and deep representations. We use  orientations for Avenue and ShanghaiTech. Due to the high resolution of ShangahiTech we normalize the magnitude by the bounding box height. When testing, for anomaly scoring we use NN for the pose and deep representations with  nearest neighbors. For velocity, we use GMM with  Gaussians for Avenue and ShanghaiTech and  for Ped2. Finally, the anomaly score of a frame represents the maximum score among all the objects within that frame.

\subsection{Evaluation Metrics}
Our study follows the popular evaluation metric in video anomaly detection literature by varying the threshold over the anomaly scores to measure the frame-level Area Under the Receiver Operation Characteristic (AUROC) with respect to the ground-truth annotations. We report two types of AUROC: (i) Micro-averaged AUROC, which is calculated by concatenating frames from all videos and then computing the score. (ii) Macro-averaged, which is calculated by averaging the frame-level AUROCs for each video. In most existing studies, micro-averaged AUROC is reported, while only a few report macro-averaged AUROC.

\subsection{Experimental Results}
\label{experiments}
We compare our method and state-of-the-art from recent years in \cref{tab:AUC_compare}. The performance numbers of the baseline methods were directly taken from their original papers. We report both micro and macro average AUROC (when available) for the three publicly available most commonly used datasets: UCSD Ped2, CUHK Avenue, and ShanghaiTech.

\textbf{Ped2 Results.} Ped2 is a long-standing video anomaly detection dataset and has therefore been reported by many previous papers. Most methods obtained over 94\% on Ped2, indicating that of the three public datasets, it is the simplest. While our method is comparable to the current state-of-the-art method (HF \cite{vad_iccv_2021}) in terms of performance, it also provides an interpretable representation. The near-perfect results of our method on Ped2 indicate it is practically solved. 

\textbf{Avenue Results.} It is evident from previous works that Avenue is a dataset of a different complexity level than Ped2. Nevertheless, our method applied to this dataset obtained a new state-of-the-art AUROC of 93.3\% in terms of micro-averaged AUROC. Additionally, our method performance exceeds the current state-of-the-art by a considerable margin of 2.7\%, reaching 96.2\% macro-averaged AUROC.

\textbf{ShanghaiTech Results.} Our method outperforms all previous methods on the hardest dataset, ShanghaiTech, by a considerable margin. Accordingly, our method achieves 85.9\% AUROC, while the highest performance previous methods have achieved is 84.3\% (Jigsaw-Puzzle \cite{jigsaw_puzzles_eccv2022}), surpassing the current state-of-the-art by a margin of 1.6\%.   

To summarize, our method achieves state-of-the-art performance on the three most commonly used public benchmarks. It outperforms all previous approaches without any training or optimization while utilizing representations that can be interpreted by humans.

\begin{table*}[t]
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
	\centering
	\begin{tabular}{| c | c | c | c | c | c | c |}
		\hline
	    \multirow{2}{*}{Pose Features} & \multirow{2}{*}{Deep Features} & \multirow{2}{*}{Velocity Features} & \multicolumn{2}{c|}{Avenue} & \multicolumn{2}{c|}{ShanghaiTech} \\ 
		\cline{4-7}
		& & & Micro & Macro & Micro & Macro \\
		\hline
		\checkmark & &  & 73.8 & 76.2 & 74.5 & 81.0 \\
		& \checkmark &  & 85.4 & 87.7 & 72.5 & 82.5 \\
		&  & \checkmark & 86.0 & 89.6 & 84.4 & 84.8 \\
		\hline
		\checkmark & \checkmark & & 89.3 & 88.8  & 76.7 & 84.9\\
		 & \checkmark & \checkmark  & \underline{93.0} & \underline{95.5} & 84.5 & 88.7 \\
		\checkmark &  & \checkmark & 87.8 & 93.0 & \textbf{85.9} & \underline{88.8} \\
		\hline
		\checkmark & \checkmark & \checkmark & \textbf{93.3} & \textbf{96.2} & \underline{85.1} & \textbf{89.6} \\
		\hline
\end{tabular}
	\caption{Ablation study, frame-level AUROC (\%) comparison. The best and second-best results are bolded and underlined, respectively.}
	\label{tab:ablation}
\end{table*}

\begin{figure}[t]
  \centering
    \begin{tabular}{c}
    \includegraphics[scale=1]{figures/avenue.png} 
    \end{tabular}
    \caption{Frame-level scores and anomaly localization examples for test video 04 from Avenue. Best viewed in color.}
    \label{fig:qualitative_avenue}
\end{figure}


\subsection{Ablation Study}
\label{exp:ablation_study}
We conducted an ablation study on Avenue and ShanghaiTech datasets to better understand the factors contributing to the performance of our method. We report anomaly detection performance of all feature combinations in \cref{tab:ablation}. Our findings reveal that the velocity features provide the highest frame-level AUROC on both Avenue and ShanghaiTech, with 86.0\% and 84.4\% micro-averaged AUROC, respectively. In ShanghaiTech, our velocity features on their own are already state-of-the-art compared with all previous VAD methods. We expect this to be due to the large number of anomalies associated with speed and motion, such as running people and fast-moving objects, e.g. cars and bikes. The combination of velocity and pose results in an 85.9\% AUROC in ShanghaiTech. The pose features are designed to detect unusual behavior, such as fighting between people and unnatural poses, as illustrated in \cref{fig:qualitative_shanghaitech} and \cref{fig:interpretability}. However, we observe a slight degradation when we combine our attribute-based representation with the deep residual representation; this may be because deep representations bundle together many different attributes, and they are often dominated by irrelevant nuisance attributes that do not distinguish between normal and anomalous objects.
As for Avenue, our attribute-based representation performs well when combined with the deep residual representation, resulting in state-of-the-art results of 93.3\% micro-averaged AUROC and 96.2\% macro-averaged AUROC.

Overall, we have observed that using all three features was key to achieving state-of-the-art results.


\begin{figure}[t]
  \centering
    \begin{tabular}{c}
    \includegraphics[scale=1]{figures/shanghaitech.png} 
    \end{tabular}
    \caption{Frame-level scores and anomaly localization examples for test video 03\_0059 from Shanghaitech. Best viewed in color}
    \label{fig:qualitative_shanghaitech}
\end{figure}

\subsection{Qualitative Results} 
We provide visualization of the anomaly detection process for Avenue and ShanghaiTech in \cref{fig:qualitative_avenue} and \cref{fig:qualitative_shanghaitech}, where the anomaly curve shows the anomaly scores across all frames of a video. Our anomaly scores are highly correlated with the ground-truth occurrence of anomalous events. This demonstrates the effectiveness of our method. 

\subsection{Further Analysis \& Discussion}

\textbf{Interpretable decisions.} We use a semantic attribute-based representation, which allows interpretation of the rationale behind decisions. This is based on the fact that our method categorizes frames as anomalous if their velocity and/or pose take an unusual value. The user can observe which attribute had an unusual value, this would indicate that the frame is anomalous in this attribute. To demonstrate the interpretability of our method, we present in \cref{fig:interpretability} a visualization of most normal and anomalous frames in Avenue and ShangahiTech for each representation. High anomaly scores from the velocity representation are attributed to fast-moving (often non-human) objects. As can also be seen from the pose representation, the most anomalous frames contain anomalous human poses that are indicative of unusual behavior. Finally, our implicit deep representation captures concepts that cannot be adequately represented by our semantic attribute representation (for example, unusual objects). This complements the semantic attributes, obtaining the best of both worlds.

\textbf{-Means as a faster alternative to NN.} We can speed up NN by reducing the number of samples via -means. In \cref{tab:k-means}, we compare the performance of our proposed method when combined with velocity, pose, and deep features as well as its approximations based on -means. Our method still uses NN by the anomaly scores are calculated using the sum of distances to nearest neighbor means. This is much faster than the original NN as there are much fewer means than the number of objects in the training set. It is evident that a small loss in accuracy can result in a significant improvement in inference time.

\begin{table}[t]
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
	\centering
\begin{tabular}{| l | c | c | c | c |}
		\hline
	    \multirow{2}{*}{} & \multicolumn{2}{c|}{Avenue} & \multicolumn{2}{c|}{ShanghaiTech} \\ 
		\cline{2-5}
		 & Micro & Macro & Micro & Macro \\
		\hline
		  &  91.8 & 94.0 & 84.2 & 87.2 \\
		   & 92.0 & 94.2 & 84.3 & 88.1 \\
		  & 92.1 & 94.5 & 84.6 & 88.1 \\
		  & 92.9 & 95.2 & 84.8 & 88.6 \\
		All  & \textbf{93.3} & \textbf{96.2} & \textbf{85.1} & \textbf{89.6} \\
		\hline
\end{tabular}
	\caption{Our final results when NN is replaced by -means. Frame-level AUROC (\%) comparison.}
	\label{tab:k-means}
\end{table} 

\textbf{What are the benefits of pretrained features?} Previous image anomaly detection work \cite{panda,mean_shifted} demonstrated that using feature extractors pretrained on external, generic datasets (e.g. ResNet on ImageNet classification) achieves high anomaly detection performance. This was demonstrated on a large variety of datasets across sizes, domains, resolutions, and symmetries. These representations achieved state-of-the-art performance on distant domains, such as aerial, microscopy, and industrial images. As the anomalies in these datasets typically had nothing to do with velocity or human pose, it is clear the pretrained features model many attributes beyond velocity and pose. Consequently, by combining our attribute-based representations with CLIP's image encoder, we are able to emphasize both explicit attributes (velocity and pose) derived from real-world priors and attributes that cannot be described by them, allowing us to achieve the best of both worlds.

\textbf{Why do we use an image encoder instead of a video encoder?} Newer and better self-supervised learning methods e.g. TimeSformer \cite{timesformer}, VideoMAE \cite{videomae}, X-CLIP \cite{x_clip} and CoCa \cite{coca} are constantly improving the performance of pretrained video encoders on downstream supervised tasks such as Kinetics-400 \cite{kinetics400}. Hence, it is natural to expect that video encoders that utilize both temporal and spatial information will provide a higher level of performance than image encoders that do not. Unfortunately, in preliminary experiments, we found that features extracted by pretrained video encoders did not work as well a pretrained image features on the type of benchmark videos used in VAD. this result underscores the strong generalizability properties of pretrained image encoders, previously highlighted in the context of image anomaly detection. Improving the generalizability of pretrained video features in the one-class classification VAD setting is a promising avenue for future work.

\section{Conclusion} Our paper proposes a simple yet highly effective attribute-based method that pushes the boundaries of video anomaly detection accuracy and interpretability. In every frame, we represent each object using velocity and pose representations, which is followed by density-based anomaly scoring. These simple velocity and pose representations allow us to achieve state-of-the-art in ShanghaiTech, the most complex video anomaly dataset. When we combine interpretable attribute-based representations with implicit deep representations, we achieve top video anomaly detection performance with a 99.1\%, 93.3\%, and 85.9\% AUROC on Ped2, Avenue, and ShanghaiTech, respectively. We also demonstrated the advantages of our three feature representations in a comprehensive ablation study. Our method is highly accurate, interpretable, and easy to implement.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
