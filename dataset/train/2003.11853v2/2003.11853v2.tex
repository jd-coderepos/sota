\documentclass[10pt,letterpaper,twocolumn]{article}
\usepackage[latin9]{inputenc}
\pagestyle{plain}
\usepackage{color}
\usepackage{array}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
urlcolor=red,
}
\makeatletter
\newcommand{\mypar}[1]{\smallskip\noindent {\bf #1}\enskip}
\newcommand{\LZ}[1]{\textcolor{red}{{[\textbf{LZ}: #1]}}}
\newcommand{\yikai}[1]{\textcolor{red}{{[\textbf{yikai}: #1]}}}
\special{papersize=\the\paperwidth,\the\paperheight}
\providecommand{\tabularnewline}{\\}
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}
\newenvironment{lyxlist}[1]
	{\begin{list}{}
		{\settowidth{\labelwidth}{#1}
		 \setlength{\leftmargin}{\labelwidth}
		 \addtolength{\leftmargin}{\labelsep}
		 \renewcommand{\makelabel}[1]{##1\hfil}}}
	{\end{list}}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{url}
\cvprfinalcopy 

\def\cvprPaperID{6422} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\makeatother
\begin{document}
\title{Instance Credibility Inference for Few-Shot Learning}
\author{Yikai Wang\textsuperscript{1,4}\qquad 
Chengming Xu\textsuperscript{1}\qquad 
Chen Liu\textsuperscript{1}\qquad 
Li Zhang\textsuperscript{2}\qquad 
Yanwei Fu\textsuperscript{1,3,4}\thanks{Corresponding author.}\\
\textsuperscript{1}School of Data Science, Fudan University\\
\textsuperscript{2}Department of Engineering Science, University of Oxford\\
\textsuperscript{3}MOE Frontiers Center for Brain Science, Fudan University\\
\textsuperscript{4}Shanghai Key Lab of
Intelligent Information Processing, Fudan University\\
{\tt\small 
\{yikaiwang19, cmxu18, chenliu18, yanweifu\}@fudan.edu.cn,
lz@robots.ox.ac.uk
}
}
 
 
\maketitle
\begin{abstract}
Few-shot learning (FSL) aims to recognize new objects with extremely limited training data for each category. 
Previous efforts are made by either leveraging meta-learning paradigm or novel principles in data augmentation to alleviate this extremely data-scarce problem. 
In contrast, 
this paper presents a simple statistical approach, 
dubbed Instance Credibility Inference (ICI) to exploit the distribution support of unlabeled instances for few-shot learning. 
Specifically, 
we first train a linear classifier with the labeled few-shot examples and use it to infer the pseudo-labels for the unlabeled data. 
To measure the credibility of each pseudo-labeled instance, 
we then propose to solve another linear regression hypothesis by increasing the sparsity of the incidental parameters and rank the pseudo-labeled instances with their sparsity degree. 
We select the most trustworthy pseudo-labeled instances alongside the labeled examples to re-train the linear classifier. 
This process is iterated until all the unlabeled samples are included in the expanded training set, 
i.e. the pseudo-label is converged for unlabeled data pool.
Extensive experiments under two few-shot settings show that our simple approach can establish new state-of-the-arts on four widely used few-shot learning benchmark datasets including \textit{mini}ImageNet, \textit{tiered}ImageNet, CIFAR-FS, and CUB. 
Our code is available at: \url{https://github.com/Yikai-Wang/ICI-FSL}
\end{abstract}

\section{Introduction}
\begin{figure*}[hbt!]
\begin{centering}
\includegraphics[width=2\columnwidth]{fig/framework-01.pdf}
\par\end{centering}
\caption{\label{fig:framework} 
Schematic illustration of our proposed framework. 
In the inference process of -way--shot FSL task with unlabeled data, 
we embed each instance, 
inference each unlabeled data and use ICI to select the most trustworthy subset to expand the support set. 
This process is repeated until all unlabeled data are included in the support set.}
\end{figure*}

Learning from one or few examples is an important ability for humans.
For example, children have no problem forming the concept of ``giraffe'' by only taking a glance from a picture in a book, 
or hearing its description as looking like a deer with a long neck~\cite{zhang2017learning}.
In contrast, the most successful recognition systems~\cite{krizhevsky2012imagenet,simonyan2014very,he2016deep,huang2017densely} still highly rely on an avalanche of labeled training data.
This thus increases the burden in rare data collection (\eg accident data in the autonomous driving scenario) and
expensive data annotation (\eg disease data for medical diagnose), 
and more fundamentally limits their scalability to open-ended learning of the long tail categories in the real-world.

Motivated by these observations, there has been a recent resurgence of research interest in few-shot learning~\cite{finn2017model,snell2017prototypical,sung2018learning,vinyals2016matching}.
It aims to recognize new objects with extremely limited training data for each category. 
Basically, a few-shot learning model has the chance to access the source/base dataset with many labeled training instances for model training and then is able to generalize to a disjoint but relevant target/novel 
dataset
with only scarce labeled data.
A simplest baseline to transfer learned knowledge to the novel set is fine-tuning~\cite{yosinski2014transferable}.
However, it would cause severely overfitting as one or a few instances are insufficient to model the data distributions of the novel classes.
Data augmentation and regularization techniques can alleviate overfitting in such a limited-data regime, but they do not solve it.
Several recent efforts are made in leveraging learning to learn, or meta-learning paradigm by simulating the few-shot scenario in the training process~\cite{lemke2015metalearning}. 
However, Chen \etal~\cite{DBLP:journals/corr/abs-1904-04232} empirically argue that such a learning paradigm often results in inferior performance compared to a simple baseline with a linear classifier coupled with a deep feature extractor.

Given such a limited-data regime (one or few labeled examples per category), one of the fundamental problems for few-shot learning is that one can hardly estimate the data distribution without introducing the inductive bias.
To address this problem, 
two types of strategy resort to model the data distribution of novel category beyond traditional \emph{inductive} few-shot learning:
(i) semi-supervised few-shot learning (SSFSL)~\cite{liu2018learning,ren2018meta,sun2019learning} supposes that we can utilize 
unlabeled data (about ten times more than labeled data) to help to learn the 
model;
furthermore,
(ii) transductive inference~\cite{joachims1999transductive} for few-shot learning (TFSL)~\cite{liu2018learning,qiao2019transductive} assumes we can access to all the test data, rather than evaluate them one by one in the inference process. 
In other words, the few-shot learning model can utilize the data distributions of testing examples. 

Self-taught learning~\cite{self-taught-learning} is one of the most straightforward ways in leveraging the information of unlabeled data. Typically, a trained classifier infers the labels of unlabeled data, which are further taken to update the classifier. 
Nevertheless, the inferred pseudo-labels may not be always trustworthy; the wrongly labeled instances may jeopardize the performance of the classifier.
It is thus essential to investigate the labeling confidence of each unlabeled instance.

To this end, we present a simple statistical approach, dubbed Instance Credibility Inference (ICI) to exploit the distribution support of unlabeled instances for few-shot learning. 
Specifically, we first train a linear classifier (\eg, logistic regression) with the labeled few-shot examples and use it to infer the pseudo-labels for the unlabeled data.
Our model aims to iteratively select the most trustworthy pseudo-labeled instances according to their credibility measured by the proposed ICI to augment the training set. 
The classifier thus can be progressively updated and further infer the unlabeled data.
We iterate this process until all the unlabeled samples are included in the expanded training set, \ie the pseudo-label is converged for unlabeled data pool.
The schematic illustration is shown in Figure~\ref{fig:framework}. 

Basically, we re-purpose the standard self-taught learning algorithm by our ICI algorithm.
How to select the pseudo-labeled data to exclude the wrong-predicted samples,~\ie, excluding the noise introduced by the self-taught learning strategy?
Our intuition is that the algorithm of sample selection can neither rely only on the label space (\eg based on the probability of each class given by the classifier) nor the feature space (\eg select samples most similar to training data). 
Instead, 
we introduce a linear regression hypothesis by regressing each instance (labeled and pseudo-labeled) from feature to label space and increase the sparsity of the incidental parameter~\cite{fan2012partial} until it vanishes.
Thus we can rank pseudo-labeled instances with sparsity degree as their credibility.
We conduct extensive experiments on major few-shot learning datasets to validate the effectiveness of our proposed algorithm. 

The contributions of this work are as follows:
(i) 
We present a simple statistical approach, dubbed Instance Credibility Inference (ICI) to exploit the distribution support of unlabeled instances for few-shot learning. 
Specifically, our model iteratively selects the pseudo-labeled instances according to its credibility measured by the proposed ICI for classifier training.
(ii) 
We re-purpose the standard self-taught learning algorithm~\cite{self-taught-learning} by our proposed ICI. To measure the credibility of each pseudo-labeled instance, we solve another linear regression hypothesis by increasing the sparsity of the incidental parameter~\cite{fan2012partial} and rank the sparsity degree as the credibility for each pseudo-labeled instance.
(iii)
Extensive experiments under two few-shot settings show that our simple approach can establish new state-of-the-arts on four widely used few-shot learning benchmark datasets including \textit{mini}ImageNet, \textit{tiered}ImageNet, CIFAR-FS, and CUB.
\section{Related work}
\mypar{Semi-supervised learning.} 
Semi-supervised learning (SSL) aims to improve the learning performance with limited labeled data by exploiting large amount of unlabeled data.
Conventional approaches focus on finding the low-density separator within both labeled and unlabeled data~\cite{vapnik1998statistical,bennett1999semi,joachims1999transductive},
and avoid to learn the ``wrong'' knowledge from the unlabeled data~\cite{li2014towards}.
Recently, 
semi-supervised learning with deep learning models use consistency regularization~\cite{conf/iclr/LaineA17},
moving average technique~\cite{tarvainen2017mean} and
adversarial perturbation regularization~\cite{miayto2016virtual}
to train the model with large amount of unlabeled data.
The key difference between semi-supervised learning and few-shot learning with unlabeled data is that the unlabeled data is still limited in the latter.
To some extent, the low-density assumption widely utilized in SSL is hard to achieve in the few-shot scenario, making SSFSL a more difficult problem.


Self-taught learning~\cite{self-taught-learning}, also known as self-training~\cite{NoisyStudent},
is a traditional semi-supervised strategy of utilizing unlabeled data to improve the performance of classifiers~\cite{amini2002semi,grandvalet2005semi}. 
Typically, an initially trained classifier predicts class labels of unlabeled instances; the  unlabeled data with  pseudo-labels are further selected to update the classifier.~\cite{lee2013pseudo}. 
Current algorithms based on self-taught learning includes training neural networks using labeled data and pseudo-labeled data jointly~\cite{lee2013pseudo}, 
using mix-up between unlabeled data and labeled data to reduce the influence of noise~\cite{arazo2019pseudo}, 
using label propagation for pseudo-labeling based on a nearest-neighbor graph and measuring the credibility using entropy~\cite{iscen2019label},
and re-weighting the pseudo-labeled data based on the cluster assumption on the feature space~\cite{shi2018transductive}. 
Unfortunately, the predicted pseudo-labels may not be trustworthy.
Different and orthogonal to previous re-weighting or mix-up works,  we design a statistical algorithm in estimating the credibility of each instance assigned with its corresponding pseudo-label. Only the most confident instances are employed to update the classifier.

\mypar{Few-shot learning.}
Recent efforts on FSL are made towards the following aspects.
(1) Metric learning methods, putting emphasis on finding better distance metrics, 
include weighted nearest neighbor classifier (\eg Matching Network~\cite{vinyals2016matching}), 
finding prototype for each class (\eg Prototypical Network~\cite{snell2017prototypical}),
or learning specific metric for each task (\eg TADAM~\cite{oreshkin2018tadam});
(2) Meta learning methods, such as Meta-Critic~\cite{sung2017learning}, MAML~\cite{finn2017model}, Meta-SGD~\cite{li2017meta}, Reptile~\cite{nichol2018first}, and LEO~\cite{rusu2018meta},  optimize the models for the capacity of rapidly adapted to new tasks. 
(3) Data augmentation algorithms enlarge available data to alleviate the lack of data in the image level~\cite{chen2019image} or the feature level~\cite{ren2018meta}. Additional, SNAIL~\cite{mishra2017simple} utilizes the sequence modeling to create a new framework.
The proposed statistical algorithm is orthogonal but potentially useful to improve these algorithms -- it is always worth increasing the training set by utilizing the unlabeled data with confidently predicted labels.


\mypar{Few-shot learning with unlabeled data.}
Recently approaches  tackle few-shot learning problems by resorting to additional  unlabeled data. Specifically,  in semi-supervised few-shot learning settings, recent works~\cite{ren2018meta,liu2018learning} enables unlabeled data from the same categories to better handle the true distribution of each class. Furthermore, transductive settings have also been considered recently. For example,  LST~\cite{sun2019learning} utilizes self-taught learning strategy in a meta-learning manner. 
Different from these methods, this paper presents a conceptually simple statistical approach derived from self-taught learning; our approach, empirically and significantly improves the performance of FSL on several benchmark datasets, by only using very simple classifiers, \eg, logistic regression, or Support Vector Machine (SVM).
\section{Methodology}

\subsection{Problem formulation}

We introduce the formulation of few-shot learning here. Assume a base category
set , and a novel category set 
with . 
Accordingly, the base and novel datasets are ,
and ,
respectively. 
In few-shot learning, the recognition models on 
should be generalized to the novel category  with only one
or few training examples per class. 

For evaluation, we adopt the standard \emph{-way--shot} classification as \cite{vinyals2016matching} on . 
Specifically, in each episode, we randomly sample  classes ;
and  and  labeled images per class are randomly sampled in  to construct the support set  and the query set , respectively.
Thus we have  and .
The classification accuracy is averaged on query sets  of many
meta-testing episodes. In addition, we have unlabeled
data of novel categories . 

\subsection{Self-taught learning from unlabeled data}

In general, labeled data for machine learning is often very difficult
and expensive to obtain, while the unlabeled data can be utilized
for improving the performance of supervised learning. Thus we recap
the self-taught learning formalism -- one of the most classical semi-supervised
methods for few-shot learning~\cite{self-taught-learning}. Particularly,
assume  is the feature extractor trained
on the base dataset . One can train a supervised classifier
 on the support set , and pseudo-labeling unlabeled data, 
with corresponding confidence  given by the classifier. The most confident unlabeled
instances will be further taken as additional data of corresponding
classes in the support set . Thus we obtain the updated supervised
classifier . To this end, few-shot classifier
acquires additional training instances, and thus its performance can
be improved. 

However, it is problematic if directly utilizing self-taught learning
in one-shot cases. Particularly, the supervised classifier 
is only trained by few instances. The unlabeled instances with high
confidence may  not be correctly categorized, and the classifier will
be updated by some wrong instances. Even worse, one can not assume
the unlabeled instances follows the same class labels or generative
distribution as the labeled data. Noisy instances or outliers may
also be utilized to update the classifiers. To this end, we propose
a systematical algorithm: Instance Credibility Inference (ICI) to reduce the noise.

\subsection{Instance credibility inference (ICI)}
To measure the credibility of predicted labels over unlabeled data, we introduce a hypothesis of linear model by regressing each instance from feature to label spaces.  Particularly, given   instances of  classes,
, where  is the ground truth when  come from the support set, or the pseudo-label when  come from the unlabeled set, we employ a simple linear regression model to ``predict'' the class label,


\noindent where  is the
coefficient matrix for classification; 
is the feature vector of instance ;  is  dimension
one-hot vector denoting the class label of instance . 
Note that to facilitate the computations, we employ PCA~\cite{tipping1999probabilistic} to reduce the dimension of extracted features  to . 
is the Gaussian noise of zero mean and  variance. 
Inspired by incidental parameters~\cite{fan2012partial}, 
we introduce  to amend the chance of instance  belonging to class  .
Larger , the higher difficulty in attributing instance  to  class  .


Write Eq.~\ref{eq:lm} in a matrix form for all instances, we are thus solving the problem of:

where  denotes the Frobenius norm.  and

indicate label and feature input respectively. 
is the incidental matrix, with the penalty .
 is the coefficient of penalty. To solve Eq.~\ref{eq:loss_func},
we re-write the function as 

Let , we have 


\noindent where  denotes the Moore-Penrose pseudo-inverse. 
Note that 
(1) we are interested in utilizing 
to measure the credibility of each instance along its regularization path, rather than estimating
, since the linear regression model is not good enough
for classification in general. 
(2) the  also relies
on the estimation of . 
To this end, we take Eq.~\ref{eq:beta}
into  and solve the problem
as,

where 

is the hat matrix of
.
We further define  and .
Then the above equation can be simplified as

which is a multi-response regression problem. 
We seek the best subset by checking the regularization path, 
which can be easily configured by 
a blockwise descent algorithm
implemented in Glmnet~\cite{simon2013blockwise}. 
Specifically,
we have a theoretical value of ~\cite{simon2013blockwise} to guarantee the solution of Eq.~\ref{eq:penalty} all 0.
Then we can get a list of s from  to . 
We solve a specific Eq.~\ref{eq:penalty} with each ,
and get the regularization path of  along the way.
Particularly, we regard  as a function of . 
When  changes from  to , the sparsity of  is increased until all of its elements are forced to be vanished. 
Further, our penalty  encourages  vanishes row by row, \ie, instance by instance. 
Moreover, the penalty will tend to vanish the subset of  with the lowest deviations, indicating less discrepancy between the prediction and the ground truth.
Hence we could rank the pseudo-labeled data by their  value when the corresponding  vanishes. 
As shown in one toy example of Figure~\ref{fig:illu}, the  value of the instance denoted by the red line vanishes first, and thus it is the most trustworthy sample by our algorithm. 
\begin{figure}
\begin{centering}
\includegraphics[width=1\columnwidth]{fig/illu.pdf}
\par\end{centering}
\caption{\label{fig:illu}
Regularization path of  on ten samples. Red line is corresponding to the most trustworthy sample suggested by our ICI algorithm.}
\end{figure}
\begin{algorithm}
\textbf{Input}:support data, query data , unlabeled data 

\textbf{Initialization}: support set , feature matrix , classifier

\textbf{Repeat:}

Train classifier using ;

Get pseudo-label  for  by classifier;

Rank  by ICI;

Select a subset  into ;

\textbf{Until Converged.}

\textbf{Inference:}

Train classifier using ;

Get pseudo-label  for  by classifier;

\textbf{Output}: inference labels 

\caption{\label{alg:Inference-process.}Inference process of our algorithm.}
\end{algorithm}
\subsection{Self-taught learning with ICI}

The proposed ICI can thus be easily integrated to improve the self-taught learning algorithm. Particularly, the initialized classifier can predict the pseudo-labels of unlabeled instances; and we further employ the ICI algorithm to select the most confident subset of unlabeled instances, to update the classifier. The whole algorithm can be iteratively updated, as summarized in Algorithm~\ref{alg:Inference-process.}. 



\section{Experiments}


\mypar{Datasets.}
Our experiments are conducted on several widely few-shot learning benchmark datasets for general object recognition and fine-grained classification, including
\emph{mini}ImageNet~\cite{ravi2016optimization}, \emph{tiered}ImageNet~\cite{ren2018meta}, 
CIFAR-FS~\cite{bertinetto2018metalearning} and 
CUB~\cite{wah2011caltech}.
\textbf{\emph{mini}}\textbf{ImageNet} consists of  classes with  labeled instances in each category.
We follow the split proposed by~\cite{ravi2016optimization}, using  classes as the base set to train the feature extractor,  classes as the validation set and 
report performance on the novel set which consists of  classes. 
\textbf{\emph{tiered}}\textbf{ImageNet} is a larger dataset compared with \emph{mini}ImageNet, and its categories are selected with hierarchical structure to split base and novel datasets semantically. 
We follow the split introduced in~\cite{ren2018meta} with base set of  superclasses ( classes), validation set of  superclasses ( classes) and novel set of  superclasses ( classes). 
Each class contains  images on average. 
\textbf{CUB} is a fine-grained dataset of  bird categories with  images in total. 
Following the previous setting in~\cite{hilliard2018few}, we use  classes as the base set,  for validation and  as the novel set. 
To make a fair comparison, we crop all images with the bounding box provided by ~\cite{triantafillou2017few}.
\textbf{CIFAR-FS} is a dataset with lower-resolution images derived from CIFAR-100~\cite{krizhevsky2009learning} .
It contains  classes with  instances in each class. 
We follow the split given by~\cite{bertinetto2018metalearning}, using  classes to construct the base set,  for validation and  as the novel set.

\vspace{0.1in}
\mypar{Experimental setup.}
Unless otherwise specified, we use the following settings and implementation  in the experiments for our approach to make a fair comparison. 
As in~\cite{mishra2017simple,oreshkin2018tadam,lee2019meta}, we use ResNet-12~\cite{DBLP:journals/corr/HeZRS15} with  residual blocks as the feature extractor in our experiments. 
Each block consists of three  convolutional layers, each of which followed by a BatchNorm layer and a LeakyReLu(0.1) activation. In the end of each block, a  max-pooling layer is utilized to reduce the output size.
The number of filters in each block is , ,  and  respectively.
Specifically, referring to~\cite{lee2019meta}, we adopt the Dropout~\cite{JMLR:v15:srivastava14a} in the first two block to vanish  of the output,
and adopt DropBlock~\cite{ghiasi2018dropblock} in the latter two blocks to vanish  of output in channel level.
Finally, an average-pooling layer is employed to produce the input feature embedding.
We select 90\% images from each training class (\eg, 64 categories for \textit{mini}ImageNet) to construct our training set for training the feature extractor and use the remaining 10\% as the validation set to select the best model.
We use SGD with momentum as the optimizer to train the feature extractor from scratch.
Momentum factor and  weight decay is set to  and , respectively.
All inputs are resized to .
We set the initial learning rate of , decayed by  after every  epochs.
The total training epochs is  epochs. 
In all of our experiments, we normalize the feature with  norm and reduce the feature dimension to  using PCA~\cite{tipping1999probabilistic}.
Our model and all baselines are evaluated over  episodes with  test samples from each class.

\begin{table*}
\centering
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}llllllllll}
\toprule 
\multirow{2}{*}{Setting} & \multirow{2}{*}{Model} &  \multicolumn{2}{c}{\emph{mini}ImageNet} & \multicolumn{2}{c}{\emph{tiered}ImageNet} & \multicolumn{2}{c}{CIFAR-FS} & \multicolumn{2}{c}{CUB}\tabularnewline
 & &shot & shot & shot & shot & shot & shot & shot & shot\tabularnewline
\midrule
\multirow{11}{*}{In.}
&Baseline~\cite{DBLP:journals/corr/abs-1904-04232} &  &  & - & - & - & - &  & \tabularnewline
& Baseline++~\cite{DBLP:journals/corr/abs-1904-04232}& & & - & - & - & - &  & \tabularnewline
& MatchingNet~\cite{vinyals2016matching}&  &  & - & - & - & - &  & \tabularnewline
& ProtoNet~\cite{snell2017prototypical}&  &  & - & - &  &  &  & \tabularnewline
& MAML~\cite{finn2017model}& &  & - & - & - & - &  & \tabularnewline
&RelationNet~\cite{sung2018learning} &  &  & - & - & - & - &  & \tabularnewline
& adaResNet~\cite{munkhdalai2017rapid}&  &  & - & - & - & - & - & -\tabularnewline
& TapNet~\cite{yoon2019tapnet} &  &  & &   & - & - & - & -\tabularnewline
& CTM~\cite{li2019finding} &  &  &  &   & - & - & - & -\tabularnewline
&MetaOptNet~\cite{lee2019meta}&&&&&&&-&-\tabularnewline
\midrule

\multirow{2}{*}{Tran.}
&TPN~\cite{liu2018learning} &  &  &  &  &  &  & - & -\tabularnewline
&TEAM~\cite{qiao2019transductive}  &  &  & - & - &  &  &  &  \tabularnewline
\midrule
\multirow{4}{*}{Semi.}
&MSkM with MTL &  &  &  &   & - & - & - &- \tabularnewline
&TPN with MTL &  &  &  &  & - & - & - & -\tabularnewline
&MSkM~\cite{ren2018meta}& &  &  &  & - & - & - &  - \tabularnewline
&TPN~\cite{liu2018learning}& &  &  &  & - & - & - & -  \tabularnewline
&LST~\cite{sun2019learning}& &  &  &  & - & - & - & -  \tabularnewline
\midrule 
\midrule 
In. &LR &  &  &  &  &  &  &  & \tabularnewline
In. &SVM &  &  &  & &  &  &  & \tabularnewline
\midrule 
Tran.&LR + ICI &  &  &  &  &  &  &  & \tabularnewline
Tran. &SVM + ICI &  &  &  & &  &  &  &  \tabularnewline
\midrule 
Semi. &SVM + ICI (/) &  &  &  &  &  &  &  & \tabularnewline
Semi. &SVM + ICI (/) &  &  &  & &  &  &   & \tabularnewline
Semi. &LR + ICI (/) &  &  &  & &  &  &  &  \tabularnewline
Semi. &LR + ICI (/) &  &  &  & &   &  &  & \tabularnewline
Semi. &LR + ICI (/) &  &  &  & &  &  &  & \tabularnewline
\bottomrule
\end{tabular*}
\vspace{0.01mm}
\caption{\label{fig:tfsl results} Test accuracies over  episodes on several datasets.  
Results with  are reported in~\cite{DBLP:journals/corr/abs-1904-04232}, 
with  are reported in~\cite{sun2019learning}, 
with  are reported in~\cite{lee2019meta}.
 is our implementation with the official code of~\cite{liu2018learning}. 
Methods denoted by  denotes ResNet-18 with input size , while  denotes ResNet-18 with input size . 
Our method and other alternatives use ResNet-12 with input size .
\textbf{In.} and \textbf{Tran.} indicate inductive and transductive setting, respectively. 
\textbf{Semi.} denotes semi-supervised setting where  shows the number of unlabeled data available in -shot and -shot experiments.
}
\end{table*}
  
\subsection{Semi-supervised few-shot learning}
\mypar{Settings.} 
In the inference process, the unlabeled data from the corresponding category pool is utilized to help FSL. 
In our experiments, we report following settings of SSFSL: 
(1) we use  unlabeled samples for each class, the same as TFSL, to compare our algorithm in SSFSL and TFSL settings; 
(2) we use  unlabeled samples in -shot task, and  unlabeled samples in -shot task, the same as current SSFSL approaches~\cite{sun2019learning}; 
(3) we use  unlabeled samples, to show the effectiveness of ICI compared with FSL algorithms with a larger network and higher-resolution inputs.
We denote these as (/), (/) and (/) in Table~\ref{fig:tfsl results}. 
Note that CUB is a fine-grained dataset and does not have so sufficient samples in each class, so we simply choose  as support set,  as query set and other samples as unlabeled set (about  samples on average) in the -shot task in the latter two settings. 
For all settings, we select  samples for every class in each iteration. The process is finished when at most five instances for each class are excluded from the expanded support set. \ie, select (/), (/), (/) unlabeled instances in total. 
Further, we utilize Logistic Regression (denoted as \emph{LR}) and linear Support Vector Machine (denoted as \emph{SVM}) to show the robustness of ICI against different linear classifiers. 

\mypar{Competitors.} 
We compare our algorithm with current approaches in SSFSL. TPN~\cite{liu2018learning} uses labeled support set and unlabeled set to propagate label to one query sample each time. LST~\cite{sun2019learning} also uses self-taught learning strategy to pseudo-label data and select confident ones, but they do this by a neural network trained in the meta-learning manner for many iterations. 
Other approaches include Masked Soft k-Means~\cite{ren2018meta} and a combination of MTL with TPN and Masked Soft k-Means reported by LST.

\mypar{Results.} are shown in Table~\ref{fig:tfsl results} where denoted as Semi. in the first column. 
Analysis from the experimental results, we can find that:
(1) Compare SSFSL with TFSL with the same number of unlabeled data, we can see that our SSFSL results are only reduced by a little or even beat TFSL results, which indicates that the information we got from the unlabeled data are robust and we can indeed handle the true distribution with unlabeled data practically.
(2) The more unlabeled data we get, the better performance we have. Thus we can learn more knowledge with more unlabeled data almost consistently using a linear classifier (\eg logistic regression). When lots of unlabeled data are accessible, ICI achieves state-of-the-art in all experiments even compared with competitors which use bigger network and higher-resolution inputs. 
(3) Compared with other SSFSL approaches, ICI also achieves varying degrees of improvements in almost all tasks and datasets. These results further indicate the robustness of our algorithm. Compared logistic regression with SVM, the robustness of ICI still holds.

\subsection{Transductive few-shot learning}
\mypar{Settings.} 
In transductive few-shot learning setting, we have chance to access the query data in the inference stage. 
Thus the unlabeled set and the query dataset are the same. 
In our experiments, we select  instances for each class in each iteration and repeat our algorithm until all the expected query samples are included, \ie, each class will be expanded by at most  images.
We also utilize both Logistic Regression and SVM as our classifier, respectively.

\mypar{Competitors.}
We compare ICI with current TFSL approaches. 
TPN~\cite{liu2018learning} constructs a graph and uses label propagation to transfer label from support samples to query samples and learn their framework in a meta-learning way.
TEAM~\cite{qiao2019transductive} utilizes class prototypes with a data-dependent metric to inference labels of query samples.

\mypar{Results.} are shown in Table~\ref{fig:tfsl results} where denoted as Tran. in the first column. Experiments cross four benchmark datasets indicate that:
(1) Compared with basic linear classifier, ICI enjoys consistently improvements, especially in the 1-shot setting where the labeled data is extremely limited and  such improvements are robust regardless of utilizing which linear classifiers.
Further, compared results between \emph{mini}ImageNet and \emph{tiered}ImageNet, we can find that the improvement margin is in the similar scale, indicating that the improvement of ICI does not rely on the semantic relationship between base set and novel set.
Hence the effectiveness and robustness of ICI is confirmed practically. 
(2) Compared with current TFSL approaches, ICI also achieves the state-of-the-art results.
\subsection{Ablation study\label{subsec:Ablation-Study}}
\begin{figure}[h]
\begin{centering}
\includegraphics[width=1\columnwidth]{fig/visual.pdf}
\caption{\label{fig:effective}Regularization path of . Red lines are correct-predicted instances while black lines are wrong-predicted ones. ICI will choose instances in the lower-left subset.}
\end{centering}
\end{figure}
\mypar{Effectiveness of ICI.} To show the effectiveness of ICI, we visualize the regularization path of  in one episode of inference process in Figure~\ref{fig:effective} where red lines are instances that are correct-predicted while black lines are wrong-predicted ones. 
It is obvious that that most of the correct-predicted instances lie in the lower-left part. Since ICI will select samples whose norm will vanish in a lower . 
We could get more correct-predicted instances than wrong-predicted instances in a high ratio. 

\begin{table}[H]
\centering
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lcccc}
\toprule 
\multirow{2}{*}{Model}&\multicolumn{2}{c}{Tran.}&\multicolumn{2}{c}{Semi.}\tabularnewline
&1shot&5shot&1shot&5shot\tabularnewline
\midrule
LR&&&&\tabularnewline
+ ra&&&&\tabularnewline
+ nn&&&&\tabularnewline
+ co&&&&\tabularnewline
 \midrule
ICI&    & & & 
\tabularnewline
\bottomrule
\end{tabular*}
\vspace{0.01mm}
\caption{\label{tab:ablation}
Compare to baselines on \emph{ mini}ImageNet under several settings.}
\end{table}
\mypar{Compare to baselines.} 
To further show the effectiveness of ICI, 
we compare ICI with other sample selection strategies under the self-taught learning pipeline. 
One simple strategy is randomly sampling the unlabeled data into the expanded support set in each iteration, denoted as \emph{ra}. 
Another is selecting the data based on the confidence given by the classifier, denoted by \emph{co}. 
In this strategy, 
the more confident the classifier is to one sample, 
the more trustworthy that sample is. 
The last one is replacing our algorithm of computing credibility by choosing the nearest-neighbor of each class in the feature space, denoted as \emph{nn}. 
In this part, we have  unlabeled instances for each class and select  to re-train the classifier by different methods for Semi. and Tran. task on \emph{ mini}ImageNet. 
From Table~\ref{tab:ablation},
we observe that ICI outperforms all the baselines in all settings.

\begin{figure}[h]
\centering
\includegraphics[width=1\columnwidth]{fig/iter.pdf} 
\caption{\label{fig:iter-manner}
Variation of accuracy as the selected samples increases over 600 episodes on \emph{mini}ImageNet. 
``ICI (\textit{n})'': select \textit{n} samples per class in each iteration.}
\end{figure}
\mypar{Effectiveness of iterative manner.}
Our intuition is the proposed ICI learns to generate a set of trustworthy unlabelled data for classifier training. 
Select
all the unlabelled data in \textit{one go}
cannot take the distribution, or the credibility of the unlabeled data into account, and thus produce more noise labels to hurt the performance of the model.
The classifier thus be trained with its prediction, resulting in no improvements in TFSL setting.
We briefly validate this as ICI (15) in Figure~\ref{fig:iter-manner} whilst ICI obtained better accuracy with iterative selection manner. 
For example, select  images with two iterations (ICI(3)) is superior to select  images in one iteration (ICI(8)).


\begin{table}[h]
\centering
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lccccc}
\toprule 
Acc (\%)&0-10&10-20&20-30&30-40&40-50\tabularnewline
\midrule
b/t&0/0&0/0&1/3&16/23&105/125\tabularnewline
\midrule
\midrule
Acc (\%)&50-60&60-70&70-80&80-90&90-100\tabularnewline
\midrule
b/t&193/218&171/189&34/40&2/2&0/0\tabularnewline
\bottomrule
\end{tabular*}
\vspace{0.01mm}
\caption{\label{tab:stat}
We run 600 episodes, with each episode training an initial classifier.
We denote 
``Acc'' as the accuracy intervals; and 
``b/T'' as the number of classifiers experienced improvement v.s. 
total classifiers in this accuracy interval. }
\end{table}
\mypar{Robustness against initial classifier.} 
What are the requirements for the initial linear classifier? Is it necessary to satisfy that the accuracy of the initial linear classifier is higher than 50\% or even higher? 
The answer is no.
As long as the initial linear classifier can be trained, theoretically our method should work.  
It thus is a future open question of how the initial classifier affects. 
We briefly validate it in Table~\ref{tab:stat}.
We run 600 episodes, with each episode training an initial classifier with different classification accuracy.
Table~\ref{tab:stat}
shows that most classifiers can get improved by ICI regardless of the initial accuracy (even with accuracy of 30-40\%).









\mypar{Influence of reduced dimension.}  
In this part, we study the influence of reduced dimension  in our algorithm on -way -shot \textit{mini}ImageNet experiments.
The results with reduced dimension , , , , , and without dimensionality reduction \ie, , are shown in Table~\ref{tab:reduced}. 
Our algorithm achieves better performance when the reduced dimension is much smaller than the number of instances (\ie, ), which is consistent with the theoretical property~\cite{fan2012partial}. 
Moreover, we can observe that our model achieves the best accuracy  when .
Practically, we adopt  in our model.

\begin{table}[t!]
\begin{centering}
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lclc}
\toprule
 & Acc (\%)&Alg.&Acc (\%)\tabularnewline
\cmidrule{1-2} \cmidrule{3-4}
 & &Isomap~\cite{tenenbaum2000global} & \tabularnewline
 & &PCA~\cite{tipping1999probabilistic} & \tabularnewline
 & &LTSA~\cite{zhang2004principal} & \tabularnewline
 & &MDS~\cite{borg2003modern} & \tabularnewline
 & &LLE~\cite{roweis2000nonlinear} & \tabularnewline
 & &SE~\cite{belkin2003laplacian} &  \tabularnewline 
\bottomrule
\end{tabular*}
\par\end{centering}
\vspace{2mm}
\caption{\label{tab:reduced}Influence of dimensionality reduction dimensions and algorithms.}
\end{table}


\mypar{Influence of dimension reduction algorithms.} 
Furthermore, we study the robustness of ICI to different dimension reduction algorithms.
We compare
Isomap~\cite{tenenbaum2000global},
principal components analysis~\cite{tipping1999probabilistic} (PCA),
local tangent space alignment~\cite{zhang2004principal} (LTSA),
multi-dimensional scaling~\cite{borg2003modern} (MDS),
locally linear embedding~\cite{roweis2000nonlinear} (LLE) and
spectral embedding~\cite{belkin2003laplacian} (SE)
on -way -shot \textit{mini}ImageNet experiments.
From Table~\ref{tab:reduced} we can observe that
ICI is robust across most of the dimensionality reduction algorithms (from LTAS  to SE ) except MDS ().
We adopt PCA for dimension reduction in our method.




\section{Conclusion}

In this paper, we have proposed a simple method, called Instance Credibility Inference (ICI) to exploit the distribution support of unlabeled instances for few-shot learning. 
The proposed ICI effectively select the most trustworthy pseudo-labeled instances according to their credibility to augment the training set. 
In order to measure the credibility of each pseudo-labeled instance, 
we propose to solve a linear regression hypothesis by increasing the sparsity of the incidental parameters~\cite{fan2012partial} and rank the pseudo-labeled instance with their sparsity degree. 
Extensive experiments show that our simple approach can establish new state-of-the-arts on four widely used few-shot learning benchmark datasets including \textit{mini}ImageNet, \textit{tiered}ImageNet, CIFAR-FS, and CUB.

\mypar{Acknowledgement.}
This work was supported in part by NSFC Projects (U1611461,61702108), 
Science and Technology Commission of Shanghai Municipality Projects (19511120700, 19ZR1471800), Shanghai Municipal Science and Technology Major Project (2018SHZDZX01), and  Shanghai Research and Innovation Functional Program (17DZ2260900).


{\small
\bibliographystyle{ieee_fullname}
\bibliography{main.bbl}
}
\end{document}
