
\documentclass{article} \usepackage{iclr2021_conference,times}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{inconsolata}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[font=small]{caption}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{soul}
\usepackage{bm}

\newcommand{\hsnote}[1]{[{\color{purple}HS: {#1}}]}
\newcommand{\afnote}[1]{[{\color{brown}AF: {#1}}]}
\newcommand{\change}[1]{{\color{red}{#1}}}


\graphicspath{{./imgs/}}


\title{Refining Deep Generative Models via\\Discriminator Gradient Flow}

\author{Abdul Fatir Ansari, Ming Liang Ang \& Harold Soh\\
Department of Computer Science, School of Computing\\
National University of Singapore\\
\texttt{\{abdulfatir, angmingliang\}@u.nus.edu, harold@comp.nus.edu.sg}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\ourmethod}{\textsc{DG}low}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Deep generative modeling has seen impressive advances in recent years, to the point where it is now commonplace to see simulated samples (e.g., images) that closely resemble real-world data. However, generation quality is generally inconsistent for any given model and can vary dramatically between samples. We introduce Discriminator Gradient low (DGlow), a new technique that improves generated samples via the gradient flow of entropy-regularized -divergences between the real and the generated data distributions. The gradient flow takes the form of a \emph{non-linear} Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. By refining inferior samples, our technique avoids wasteful sample rejection used by previous methods (DRS \& MH-GAN). Compared to existing works that focus on specific GAN variants, we show our refinement approach can be applied to GANs with vector-valued critics and even other deep generative models such as VAEs and Normalizing Flows. Empirical results on multiple synthetic, image, and text datasets demonstrate that DGlow leads to significant improvement in the quality of generated samples for a variety of generative models, outperforming the state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator Driven Latent Sampling (DDLS) methods.
\end{abstract}


\section{Introduction}
Deep generative models (DGMs) have excelled at numerous tasks, from generating realistic images~\citep{brock2018large} to 
learning policies in reinforcement learning~\citep{Ho2016GenerativeAI}. Among the variety of proposed DGMs, Generative Adversarial Networks (GANs)~\citep{goodfellow2014generative} have received widespread popularity for their ability to generate high quality samples that resemble real data. 
Unlike Variational Autoencoders (VAEs)~\citep{Kingma2014AutoEncodingVB} and Normalizing Flows~\citep{Rezende2015VariationalIW,kingma2018glow}, GANs are likelihood-free methods; training is formulated as a minimax optimization problem involving a generator and a discriminator. The generator seeks to generate samples that are similar to the real data by minimizing a measure of discrepancy (between the generated samples and real samples) furnished by the discriminator. The discriminator is trained to distinguish the generated samples from the real samples. Once trained, the generator is used to simulate samples and the discriminator has traditionally been discarded. 

However, recent work has shown that discarding the discriminator is wasteful --- it actually contains useful information about the underlying data distribution. This insight has led to \emph{sample improvement} techniques that use this information to improve the quality of generated samples~\citep{azadi2018discriminator,turner2019metropolis,tanaka2019discriminator,che2020your}. Unfortunately, current methods either rely on wasteful rejection operations in the data space~\citep{azadi2018discriminator,turner2019metropolis}, or require a sensitive diffusion term to ensure sample diversity~\citep{che2020your}. Prior work has also focused on improving GANs with scalar-valued discriminators, which excludes a large family of GANs with vector-valued critics, e.g., MMDGAN \citep{li2017mmd,binkowski2018demystifying} and OCFGAN~\citep{ansari2020characteristic}, and likelihood-based generative models.  

\begin{wrapfigure}[25]{r}{0.40\textwidth}
	\centering
\includegraphics[width=0.40\textwidth]{IntroGraphic}
	\caption{\small An illustration of refinement using \ourmethod{}, with the gradient flow in the 2-Wasserstein space  (top) and the corresponding discretized SDE in the latent space  (bottom). The image samples from the densities along the gradient flow are shown in the middle.}  
	\label{fig:intro_fig}
\end{wrapfigure}
In this work, we propose Discriminator Gradient low (\ourmethod{}) which  formulates sample improvement as refining inferior samples using the \emph{gradient flow} of -divergences between the generator and the real data distributions (Fig. \ref{fig:intro_fig}). \ourmethod{} avoids wasteful rejection operations and can be used in a deterministic setting without a diffusion term. Existing state-of-the-art methods --- specifically, Discriminator Optimal Transport (DOT)~\citep{tanaka2019discriminator} and Discriminator Driven Latent Sampling (DDLS)~\citep{che2020your} --- can be viewed as special cases of \ourmethod{}. Similar to DDLS, \ourmethod{} recovers the real data distribution when the gradient flow is simulated exactly. 

We further present a generalized framework that employs existing pre-trained discriminators to refine samples from a \emph{variety} of deep generative models: we demonstrate our method can be applied to GANs with vector-valued critics, and even likelihood-based models such as VAEs and Normalizing Flows. Empirical results on synthetic datasets, and benchmark image (CIFAR10, STL10) and text (Billion Words) datasets demonstrate that our gradient flow-based approach outperforms DOT and DDLS on multiple quantitative evaluation metrics. 

In summary, this paper's key contributions are:

\begin{itemize}
	\item \ourmethod{}, a method to refine deep generative models using the gradient flow of -divergences;
\end{itemize}

\begin{itemize}
	\item a framework that extends \ourmethod{} to GANs with vector-valued critics, VAEs, and Normalizing Flows;
	\item experiments on a variety of generative models trained on synthetic, image (CIFAR10 \& STL10), and text (Billion Words) datasets demonstrating that \ourmethod{} is effective in improving samples from generative models. \end{itemize}



 \section{Background: Gradient Flows}
The following gives a brief introduction to gradient flows; we refer readers to the excellent overview by~\citet{santambrogio2017euclidean} for a more thorough introduction.


Let  be a Euclidean space and  be a smooth energy function. The gradient flow of  is the smooth curve  that follows the direction of steepest descent, i.e.,

 The value of the energy  is minimized along this curve. 
 This idea of steepest descent curves can be characterized in arbitrary metric spaces via the \emph{minimizing movement scheme}~\citep{jordan1998variational}. Of particular interest is the metric space of probability measures that is endowed with the Wasserstein distance (); the Wasserstein distance is a metric and the  topology satisfies weak convergence of probability measures~\citep[Theorem~6.9]{villani2008optimal}. Gradient flows in the 2-Wasserstein space () --- i.e., the space of probability measures with finite second moments and the 2-Wasserstein metric --- have been studied extensively. Let  be the gradient flow of a functional  in the 2-Wasserstein space, where  is absolutely continuous with respect to the Lebesgue measure. The curve  satisfies the continuity equation~\citep[Theorem~8.3.1]{ambrosio2008gradient}, 
 

The velocity field  in Eq. (\ref{eq:cont-eq}) is given by 

where  denotes the first variation of the functional . 

Since the seminal work of \citet{jordan1998variational} that showed that the Fokker-Plank equation is the gradient flow of a particular functional in the Wasserstein space, gradient flows in the Wasserstein metric have been a popular tool in the analysis of partial differential equations (PDEs). For example, they have been applied to the study of the porous-medium equation~\citep{otto2001geometry}, crowd modeling~\citep{maury2010macroscopic,maury2011handling}, and mean-field games~\citep{almulla2017two}. More recently, gradient flows of various distances used in deep generative modeling literature have been proposed, notably that of the sliced Wasserstein distance~\citep{liutkus2019sliced}, the 
maximum mean discrepancy~\citep{arbel2019maximum}, the Stein discrepancy~\citep{liu2017stein}, and the Sobolev discrepancy~\citep{mroueh2019sobolev}. Gradient flows have also been used for learning non-parametric and parametric implicit generative models~\citep{liutkus2019sliced,gao2019deep,gao2020learning}. As an example of the latter, Variational Gradient Flow~\citep{gao2019deep}  learns a mapping between latent vectors and samples evolved using the gradient flow of -divergences. In this work, we present a method using gradient flows of entropy-regularized -divergences for refining samples from deep generative models employing existing discriminators as density-ratio estimators.

 \vspace{-.8em}
\section{Generator Refinement via Discriminator Gradient Flow}
\vspace{-.8em}


This section describes our main contribution: Discriminator Gradient low (DGlow). As an overview, we begin with the construction of the gradient flow of entropy-regularized -divergences and describe its application to sample refinement. We then discuss how to simulate the gradient flow in the latent space of the generator --- a procedure more suitable for high-dimensional datasets. Finally, we present a simple technique that extends our method to generative models that have not yet been studied in the context of refinement. Due to space constraints, we focus on conveying the key concepts and relegate details (e.g., proofs) to the appendix.



 The entropy-regularized -divergence functional is defined as

where the -divergence term  ensures that the ``distance" between the probability density  and the target density  decreases along the gradient flow. The differential entropy term  improves diversity and expressiveness when the gradient flow is simulated for finite time-steps. We now construct the gradient flow of .
\begin{lemma}
	\label{thm:gradient-flow}
	Define the functional  as
	
	where  is a twice-differentiable convex function with . The gradient flow of the functional  in the Wasserstein space () is given by the following PDE,
	
	where  and  denote the divergence and the Laplace operators respectively.
\end{lemma}
The proof is given in Appendix \ref{subsec:proof_gradient-flow}.
The PDE in Eq. (\ref{eq:fokker-plankpde}) is a type of Fokker-Plank equation (FPE). FPEs have been studied extensively in the literature of stochastic processes and have a Stochastic Differential Equation (SDE) counterpart~\citep{risken1996fokker}. In the case of Eq. (\ref{eq:fokker-plankpde}), the equivalent SDE is given by

where  denotes the standard Wiener process. Eq. (\ref{eq:mckean-vlasov-process}) defines the evolution of a particle  under the influence of drift and diffusion. Specifically, it is a McKean-Vlasov process~\citep{braun1977vlasov} which is a type of \emph{non-linear} stochastic process as the drift term at any time  depends on the distribution  of the particle . Eqs. (\ref{eq:fokker-plankpde}) and (\ref{eq:mckean-vlasov-process}) are equivalent in the sense that the distribution of the particle  in Eq. (\ref{eq:mckean-vlasov-process}) solves the PDE in Eq. (\ref{eq:fokker-plankpde}). Consequently, samples from the density  along the gradient flow can be obtained by first drawing samples  and then simulating the SDE in Eq. (\ref{eq:mckean-vlasov-process}). The SDE can be approximately simulated via the stochastic Euler scheme (also known as the Euler-Maruyama method)~\citep{beyn2011numerical} given by

 where , the time interval  is partitioned into equal intervals of size  and  denote the discretized time-steps.

Eq. (\ref{eq:euler-maruyama}) provides a non-parametric procedure to refine samples from a generator  where we let  be the density of real samples and  the density of samples generated from  obtained by first sampling from the prior latent distribution  and then feeding  into . We first generate particles  and then update  the particles using Eq. (\ref{eq:euler-maruyama}) for  time steps.

Given a binary classifier (discriminator)  that has been trained to distinguish between samples from  and , the density-ratio  can be estimated via the well-known \emph{density-ratio trick}~\citep{sugiyama2012density},

where  denotes the conditional probability  of the sample  being from  and  denotes the logit output of the classifier . We term this procedure where samples are refined via gradient flow of -divergences as Discriminator Gradient low (\ourmethod{}).

\subsection{Refinement in the latent space}
\label{sec:latent-space-refinement}

Eq. (\ref{eq:euler-maruyama}) requires a running estimate of the density-ratio , which can be approximated using the \emph{stale estimate}  for  and small , where the density  will be close to . However, our  initial image experiments showed that refining directly in high-dimensional data-spaces with the stale estimate is problematic; error is accumulated at each time-step leading to a visible degradation in the quality of data samples (e.g., appearance of artifacts in images).

To tackle this problem, we propose refining the latent vectors before mapping them to samples in data-space using . We describe a procedure analogous to Eq. (\ref{eq:euler-maruyama}) but in the latent space for generators  that take a latent vector  as input and generate a sample . We first show in Lemma \ref{lemma:latent-density-ratio} that the density-ratio in the latent space between two distributions can be estimated via the density-ratio of corresponding distributions in the data space.
\begin{lemma}
\label{lemma:latent-density-ratio}
	Let  be a sufficiently well-behaved injective function where  and  with .
	Let ,  be probability densities on  and ,  be the densities of the pushforward measures ,  respectively. Assume that  and  have same support, and the Jacobian matrix  has full column rank. Then, the density-ratio  at the point  is given by
	
\end{lemma}
\begin{wrapfigure}[12]{r}{0.55\textwidth}
\vspace{-2em}
\begin{minipage}{0.55\textwidth}
\begin{algorithm}[H]
\caption{Refinement in the Latent Space using \ourmethod{}.}
\label{alg:dgflow}
\begin{algorithmic}[1]
\Require First derivative of  (), generator (), discriminator (), number of update steps (), step-size (), noise factor ().
\State{\Comment{Sample from the prior.}}
\For{}
\State 
\State 
\EndFor
\State \textbf{return} \Comment{The refined sample.}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{wrapfigure} 
The proof is in Appendix \ref{subsec:proof_densityratio}. We let  be the density of the ``correct" latent space distribution induced by a generator , i.e.,  is the density of a probability measure whose pushforward under  approximately equals the target data density . The density-ratio of the prior latent distribution  and  can now be computed by combining Lemma \ref{lemma:latent-density-ratio} with Eq. (\ref{eq:density-ratio-trick}),

Although a generator  parameterized by a neural network may not satisfy the conditions of injectivity and full column rank Jacobian matrix , Eq. (\ref{eq:latent-density-ratio}) provides an approximation that works well in practice as shown by our experiments. Combining Eq. (\ref{eq:latent-density-ratio}) with Eq. (\ref{eq:euler-maruyama}) provides us with an update rule for refining samples in the latent space,

where  and the density-ratio  is approximated using the stale estimate . We summarize the complete algorithm in Algorithm \ref{alg:dgflow}.
\subsection{Refinement for All}
\label{sec:refinement-for-all}


Thus far, prior work~\citep{azadi2018discriminator,turner2019metropolis,tanaka2019discriminator,che2020your} has focused on improving samples for GANs with scalar-valued discriminators, which comprises the canonical GAN as well as recent variants, e.g., WGAN~\citep{gulrajani2017improved}, and SNGAN~\citep{miyato2018spectral}. Here, we propose a technique that extends our approach to refine samples from a larger class of DGMs including GANs with vector-valued critics, VAEs, and Normalizing Flows.

Let  be the density of the samples generated by a generator  and  be the density of the real data distribution. We are interested in refining samples from ; however, a corresponding density-ratio estimator for  is unavailable, as is the case with the aforementioned generative models.

Let  be a discriminator that has been trained on the same dataset but for a different generative model  (e.g., let  and  be the generator and discriminator of SNGAN respectively).  can be used to compute the density ratio . A straightforward technique would be to use the crude approximation , which could work provided  and  are not too far from each other. Our experiments show that this simple approximation works to a limited extent (see appendix \ref{sec:additional-results}). 

To improve upon the crude approximation above, we propose to correct the density-ratio estimate. Specifically, a discriminator  is initialized with the weights from  and is fine-tuned on samples from  and .  and  are then used to approximate the density-ratio ,

where  and  are logits output from  and , respectively. We term the network  the \emph{density ratio corrector}, which experiments show produces higher quality samples than using . The estimate in Eq. (\ref{eq:density-ratio-correction}) is similar to telescoping density-ratio estimation (TRE), a technique proposed in very recent independent work~\citep{rhodes2020telescoping}. In brief, \citet{rhodes2020telescoping} show that classifier-based density ratio estimators perform poorly when distributions are ``too far apart'';   
 the classifier can easily distinguish between the distributions, even with a poor estimate of the density ratio. TRE expands the standard density ratio into a telescoping product of more difficult-to-distinguish intermediate density ratios. Likewise, in Eq. (\ref{eq:density-ratio-correction}), we treat  as an intermediate distribution and estimate the final density-ratio as a product of two density-ratios.





 \vspace{-.8em}
\section{Related Work}
\vspace{-.8em}

\citet{azadi2018discriminator} first proposed the idea of improving samples from a GAN's generator by discriminator rejection sampling (DRS), making use of the density-ratio provided by the discriminator to estimate the acceptance probability.  Metropolis-Hastings GAN (MH-GAN) \citep{turner2019metropolis} improved upon the costly rejection sampling procedure via the Metropolis-Hastings algorithm. Unlike \ourmethod{},  both of these methods reject inferior samples instead of refining them. 

Recent work has also sought to improve generative models via sample evolution in the training/generation process. In energy-based generative models~\citep{arbel2020generalized, Deng2020Residual}, the energy functions can be viewed as a component that improves some base generator. For example, the Generalized Energy-Based Model (GEBM) \citep{arbel2020generalized} jointly trains an implicit generative model with an energy function and uses Langevin dynamics to sample from the combination of the two. The Noise Conditional Score Network (NCSN)~\citep{song2019generative,song2020improved} --- a score-based generative model --- can be seen as a gradient flow that refines a sample right from noise to data. Latent Optimization GAN (LOGAN)~\citep{wu2020logan} optimizes a latent vector via natural gradient descent as part of the GAN training process. While related,  these techniques result in ``self-contained'' generative models; unlike \ourmethod{}, they are not general sample refinement techniques that can be applied across generative models\footnote{For further discussion about these techniques, please refer to appendix \ref{app:relatedwork}.}.


Our method is closely related to recent state-of-the-art techniques, specifically Discriminator-Driven Latent Sampling (DDLS)~\citep{che2020your} and Discriminator Optimal Transport (DOT)~\citep{tanaka2019discriminator}. In fact, both these methods can be seen as special cases of \ourmethod{}. 

DDLS treats a GAN as an energy-based model and uses Langevin dynamics to sample from the energy-based latent distribution  induced by performing rejection sampling in the latent space. This distribution is the same as , which can be seen by rearranging terms in Eq. (\ref{eq:latent-density-ratio}). If we use the KL-divergence by setting  , \ourmethod{} is equivalent to DDLS. However, there are practical differences that make \ourmethod{} more appealing. DDLS requires estimation of the score function  to perform the update which becomes undefined if  escapes the support of , e.g., in the case of the uniform prior distribution commonly used in GANs; handling such cases would require techniques such as projected gradient descent. This problem does not arise in the case of \ourmethod{} since it only uses the density-ratio that is implicitly defined by the discriminator. Moreover, DDLS uses Langevin dynamics which \emph{requires} the sensitive diffusion term to ensure diversity and to prevent points from collapsing to the maximum-likelihood point. In \ourmethod{}, the sample diversity is ensured by the density-ratio term and the diffusion term serves as an enhancement. Note that \ourmethod{} performs well even without the diffusion term (i.e., with , see Tables \ref{tab:gans-without-noise} \& \ref{tab:cifar-inception-no-noise} in the appendix). This deterministic variant of \ourmethod{} is a practical alternative with one less hyperparameter to tune.


DOT refines samples by constructing an Optimal Transport (OT) map induced by the WGAN discriminator. The OT map is realized by means of a deterministic optimization problem in the vicinity of the generated samples.
If we further analyze the case of \ourmethod{} with  and solve the resulting ordinary differential equation (ODE) using the backward Euler method,

DOT emerges as a special case when we consider a single update step of Eq. (\ref{eq:backward-euler-latent}) using gradient descent and set \footnote{This implies that , which is a twice-differentiable convex function with .} with . This connection of \ourmethod{} to DOT, an optimal transport technique, is perhaps unsurprising given the relationship between gradient flows and the dynamical Benamou-Brenier formulation of optimal transport~\citep{santambrogio2017euclidean}.
 \vspace{-.8em}
\section{Experiments}
\vspace{-.8em}

In this section, we present empirical results on various deep generative models trained on multiple synthetic and real world datasets. Our primary goals were to determine if (a) \ourmethod{} is effective in improving the quality of samples from generative models, (b) the proposed extension to other generative models improves their sample quality, and (c) \ourmethod{} is generalizable to different types of data and metrics. Note that we did not seek to achieve state-of-the-art results for the datasets studied but to demonstrate that \ourmethod{} is able to significantly improve samples from the bare generators for different models.

We experimented with three -divergences, namely the Kullback-Leibler (KL) divergence, the Jensen-Shannon (JS) divergence, and the  D divergence~\citep{gao2019deep}. The specific forms of the functions  and corresponding derivatives are tabulated in Table \ref{tab:f-divergences} (appendix). We compare \ourmethod{} with two state-of-the-art competing methods: DOT and DDLS. In this section we discuss the main results and relegate details to the appendix. Our code is available  online  at \url{https://github.com/clear-nus/DGflow}. 

\begin{figure}[!htb]
	\centering
	\subfloat{\includegraphics[width=0.7\textwidth]{comp-wgan-25gaussians}}\\
	\subfloat{\includegraphics[width=0.7\textwidth]{comp-wgan-swissroll}}
	\caption{\small{Qualitative comparison of \ourmethod{}\textsubscript{(KL)} with DOT and DDLS on synthetic 2D datasets.}}
	\label{fig:2ddatasets}
	\vspace{-1em}
\end{figure}

\subsection{2D Datasets}

\begin{wraptable}{R}{.5\textwidth}
	\vspace{-.8em}
	\scriptsize
	\caption{\small{Quantitative comparison on the 25Gaussians dataset. Higher scores are better.}}
	\label{tab:2ddatasets}
	\centering
	\begin{tabular}{lrr}
	\toprule
	& \% High Quality & KDE Score\\
	\midrule
	GAN & 26.5  .8  & -7037  64\\
	DOT & 69.8  .7 & -4149  39\\
	DDLS & \textbf{89.3  .6} & -2997  17\\
	\midrule
	DGlow\textsubscript{(KL)} & \textbf{89.5  .4} & \textbf{-2893  07}\\
	DGlow\textsubscript{(JS)} & 82.6  .4 & -3118  19\\
	DGlow\textsubscript{( D)} & 84.5  .3 & -3036  14\\
	\bottomrule
	\end{tabular}
\end{wraptable}

We first tested \ourmethod{} on two synthetic datasets, 25Gaussians and 2DSwissroll, to visually inspect the improvement in the quality of generated samples. We generated 5000 samples from a trained WGAN-GP generator and refined them using DOT, DDLS, and \ourmethod{}. We performed refinement in the latent space for DDLS and directly in the data-space for DOT and \ourmethod{}. Fig. \ref{fig:2ddatasets} shows the samples generated from the WGAN-GP generator (blue) and the refined samples using different techniques (red) against the real samples from the training dataset (brown). Although the WGAN-GP generator learned the overall structure of the dataset, it also learned a number of spurious modes. DOT is able to refine the spurious samples but to a limited degree. In contrast, DDLS and \ourmethod{} are able to correct almost all spurious samples and are able to recover the correct structure of the data. Visualizations for \ourmethod{} with different -divergences can be found in the appendix (Fig. \ref{fig:f-div-comparison-2d-datasets}).

We also compared the different methods quantitatively on two metrics: \% high quality samples and kernel density estimate (KDE) score. A sample is classified as a high quality sample if it lies within 4 standard deviations of its nearest Gaussian. The KDE score is computed by first estimating the KDE using generated samples and then computing the log-likelihood of the training samples under the KDE estimate. We computed both the metrics 10 times using 5000 samples and report the mean in Table \ref{tab:2ddatasets}. The quantitative metrics reinforce the qualitative analysis and show that DDLS and \ourmethod{} significantly improve the samples from the generator, with \ourmethod{} performing slightly better than DDLS in terms of the KDE score.
\begin{wrapfigure}[20]{r}{.6\textwidth}
	\vspace{-1.5em}
	\centering
	\subfloat[CIFAR10]{\includegraphics[width=0.25\textwidth]{cifar_resnet}}\quad
	\subfloat[STL10]{\includegraphics[width=0.25\textwidth]{stl}}
	\caption{\small{Improvement in the quality of samples generated from the base model (leftmost columns) over the steps of \ourmethod{} for SN-ResNet-GAN and SN-DCGAN on the CIFAR10 and STL10 datasets respectively.}}
	\label{fig:refinement-over-steps}
	\vspace{-1em}
\end{wrapfigure}
\vspace{-.8em}
\subsection{Image Experiments}
\vspace{-.5em}

We conducted experiments on the CIFAR10 and STL10 datasets to demonstrate the efficacy of \ourmethod{} in the real-world setting. We followed the setup of \citet{tanaka2019discriminator} for our image experiments. We used the Fr\'echet Inception Distance (FID)~\citep{heusel2017gans} and Inception Score (IS)~\citep{salimans2016improved} metrics to evaluate the quality of generated samples before and after refinement. 
A high value of IS and a low value of FID corresponds to high quality samples, respectively. 

We first applied \ourmethod{} to GANs with scalar-valued discriminators (e.g., WGAN-GP, SNGAN) trained on the CIFAR10 and the STL10 datasets. Table \ref{tab:scalar-valued-fid} shows that \ourmethod{} significantly improves the quality of the samples in terms of the FID score and outperforms DOT on multiple models. The corresponding values of the Inception score can be found in the Appendix (Table \ref{tab:gans-iscore}), which shows \ourmethod{} outperforms DOT on all models. In Table \ref{tab:cifar-inception}, we reproduce previously reported IS results for generative models and other sample improvement methods (DRS, MH-GAN, and DDLS) for completeness. \ourmethod{} performs the best in terms of relative improvement from the base score and even outperforms the state-of-the-art BigGAN~\citep{brock2018large}, a conditional generative model, without the need for additional labels. Qualitatively, \ourmethod{} improves the vibrance of the samples and corrects deformations in the foreground object. Fig. \ref{fig:refinement-over-steps} shows the change in the quality of samples when using \ourmethod{} where the leftmost columns show the image generated form the base models and the successive columns show the refined sample using \ourmethod{} over increments of 5 update steps. 

We then evaluated the ability of \ourmethod{} to refine samples from generative models \emph{without corresponding discriminators}, namely MMDGAN, OCFGAN-GP, VAEs, and Normalizing Flows (Glow). We used the SN-DCGAN (ns) as the surrogate discriminator  for these models and fine-tuned density ratio correctors  for each model as described in section \ref{sec:refinement-for-all}. Table \ref{tab:other-gen-models-fid} shows the FID scores achieved by these models without and with refinement using \ourmethod{}. We obtain a clear improvement in quality of samples when these generative models are combined with \ourmethod{}. 


\begin{table*}
	\footnotesize
	\caption{\small{Comparison of different variants of \ourmethod{} with DOT on the CIFAR10 and STL10 datasets. For SN-DCGAN, (hi) denotes the hinge loss and (ns) denotes the non-saturating loss. Lower scores are better. \ourmethod{}'s results have been averaged over 5 random runs with the standard deviation in parentheses.}}
	\label{tab:scalar-valued-fid}
	\centering
	\begin{tabular}{llrrrrr} 
	\toprule
	& \multirow{3}{*}{Model} & \multicolumn{5}{c}{Fr\'echet Inception Distance}\\
	\cmidrule{3-7}
	& & \multicolumn{1}{c}{Base Model} & \multicolumn{1}{c}{DOT} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{(KL)}} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{(JS)}} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{( D)}}	\\
	\midrule
	\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{CIFAR10}}} &
	WGAN-GP & 28.37 (.08) & 24.14 & 24.68 (.09) & \textbf{23.15 (.07)} & 24.53 (.11)\\ 
	& SN-DCGAN (hi) & 20.70 (.05) & 17.12 & \textbf{15.68 (.07)} & 16.45 (.06) & 17.36 (.05)\\
	& SN-DCGAN (ns) & 20.90 (.11) & 15.78 & \textbf{15.30 (.08)} & 15.90 (.11) & 16.42 (.05)\\
	& SN-ResNet-GAN & 14.10 (.06) & -- & \textbf{9.62 (.03)} & 9.79 (.02) & 9.73 (.05)\\
	\midrule
	\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize{STL10}}}
	& WGAN-GP & 51.50 (.15) & 44.45 & \textbf{39.07 (.07)} & 50.83 (.06) & 39.71 (.29)\\
	& SN-DCGAN (hi) & 40.54 (.17) & \textbf{34.85} & 34.95 (.06) & 36.37 (.12) &  36.56 (.08)\\
	& SN-DCGAN (ns) & 41.86 (.12) & 34.84 & \textbf{34.60 (.11)} & 35.37 (.12) & 37.07 (.14)\\
	\bottomrule
	\end{tabular}
\end{table*}

\begin{table*}
	\footnotesize
	\caption{\small{Inception scores of different generative models, DRS, MH-GAN, DDLS, and \ourmethod{} on the CIFAR10 dataset. Higher scores are better.}}
	\label{tab:cifar-inception}
	\centering
	\begin{tabular}{lr} 
	\toprule
	\multirow{1}{*}{Model} & \multicolumn{1}{c}{Inception Score}\\
	\midrule
	WGAN-GP~\citep{gulrajani2017improved} & 7.86 (.07)\\
	ProgressiveGAN~\citep{karras2017progressive} & 8.80 (.05)\\
	SN-ResNet-GAN~\citep{miyato2018spectral} & 8.22 (.05)\\
	NCSN~\citep{song2019generative} & 8.87 (.12)\\
	\midrule
	DCGAN & 2.88\\
	DCGAN + DRS (cal)~\citep{azadi2018discriminator} & 3.07\\
	DCGAN + MH (cal)~\citep{turner2019metropolis} & 3.38\\
	\midrule
	SN-ResNet-GAN (our evaluation) & 8.38 (.03)\\
	SN-ResNet-GAN + DDLS (cal)~\citep{che2020your} & 9.09 (.10)\\
	SN-ResNet-GAN + \ourmethod{}\textsubscript{(KL)} & \textbf{9.35 (.03)}\\
	\midrule
	BigGAN & 9.22\\
	\bottomrule
	\end{tabular}
	\vspace{-.8em}
\end{table*}


\begin{table*}
	\footnotesize
	\caption{\small{Comparison of different variants of \ourmethod{} applied to MMDGAN, OCFGAN-GP, VAE, and Glow models. Lower scores are better. Results have been averaged over 5 random runs with the standard deviation in parentheses.}}
	\label{tab:other-gen-models-fid}
	\centering
	\begin{tabular}{llrrrr} 
	\toprule
	& \multirow{2}{*}{Model} & \multicolumn{4}{c}{Fr\'echet Inception Distance}\\
	\cmidrule{3-6}
	 & & Base Model & \multicolumn{1}{c}{\ourmethod{}\textsubscript{(KL)}} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{(JS)}} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{( D)}}	\\
	\midrule
	\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{CIFAR10}}}
	& MMDGAN & 41.98 (.12) &  \textbf{36.75 (.09)} & 38.06 (.14) & 37.75 (.10)\\
	& OCFGAN-GP & 31.98 (.12) & \textbf{26.89 (.06)} & 28.20 (.06) &  27.82 (.09)\\
	& VAE & 129.5 (.13) & \textbf{116.0 (.21)} & 128.9 (.13) &  115.2 (.06)\\
	& Glow & 100.5 (.52) &  \textbf{79.02 (.23)} & 94.61 (.34) &  81.12 (.35)\\
	\midrule
	\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize{STL10}}}
	& MMDGAN & 47.20 (.07) & 43.21 (.06) & 46.74 (.05) & \textbf{43.06 (.05)}\\
	& OCFGAN-GP & 36.55 (.08)  & 31.12 (.13) & 36.05 (.11) & \textbf{30.61 (.14)}\\
	& VAE & 150.5 (.09) & \textbf{130.1 (.18)} & 149.9 (.08) & 132.5 (.28)\\ 
	\bottomrule
	\end{tabular}
\end{table*}

\begin{table}[htb]
	\centering
	\caption{\small{Results of \ourmethod{} on a character-level GAN language model.}}
	\label{tab:language-results}
	\subfloat[JS-4 and JS-6 scores. Lower scores are better.]{
	\begin{tabular}{lrr} 
	\toprule
	Model & JS-4 & JS-6\\
	\midrule
	WGAN-GP & 0.224 (.0009) & 0.574 (.0015)\\
	\ourmethod{}\textsubscript{(KL)} & 0.212 (.0008) & 0.512 (.0012)\\
	\ourmethod{}\textsubscript{(JS)} & \textbf{0.186} (.0007) & 0.508 (.0011)\\
	\ourmethod{}\textsubscript{( D)} & 0.209 (.0005) & \textbf{0.506} (.0008)\\
	\bottomrule
	\end{tabular}	
 }\\
	\subfloat[Examples of text samples refined by \ourmethod{}.]{
	\footnotesize
	\begin{tabular}{ll} 
	\toprule
	Generated by WGAN-GP & Refined by \ourmethod{}\\
	\midrule
	\texttt{In Ruoduce that fhance would pol} & \texttt{In product that chance could rol}\\
	\texttt{I said thowe toot lind talker . } & \texttt{I said this stood line talked 10}\\
	\texttt{Now their rarning injurer  hows } & \texttt{Now their warning injurer shows }\\
	\texttt{Police report in B0sbu does off } & \texttt{Police report inturner will befe}\\
	\texttt{We gine jaid 121 , one bub like } & \texttt{We gave wall said left out like }\\
	\texttt{In years in 19mbisuch said he h} & \texttt{In years in 1900b such said he h}\\
	\bottomrule
	\end{tabular}}
	\vspace{-2em}
\end{table}


\subsection{Character-Level Language Modeling}

Finally, we conducted an experiment on the character-level language modeling task proposed by~\citet{gulrajani2017improved} to show that \ourmethod{} works on different types of data. We trained a character-level GAN language model on the Billion Words Dataset~\citep{chelba2013one}, which was pre-processed into 32-character long strings. We evaluated the generated samples using the JS-4 and JS-6 scores which compute the Jensen-Shannon divergence between the 4-gram and 6-gram probabilities of the data generated by the model and the real data. Table \ref{tab:language-results} (a) shows that \ourmethod{} leads to an improvement in the JS-4 and JS-6 scores. Table \ref{tab:language-results} (b) shows example sentences where \ourmethod{} visibly improves the quality of generated text.




 \vspace{-1em}
\section{Conclusion}
\vspace{-1em}
In this paper, we proposed a technique to improve samples from deep generative models by refining them using gradient flow of -divergences between the real and the generator data distributions. We also presented a simple framework that extends the proposed technique to  commonly used deep generative models: GANs, VAEs, and Normalizing Flows. Experimental results indicate that gradient flows provide an excellent alternative methodology to refine generative models. Moving forward, we are considering several technical enhancements to improve  \ourmethod{}'s performance. At present, \ourmethod{} uses a stale estimate of the density-ratio, which could adversely affect sample evolution when the gradient flow is simulated for larger number of steps; how we can efficiently update this estimate is an open question.  Another related question is when the evolution of the samples should be stopped; running chains for too long may modify characteristics of the original sample (e.g., orientation and color) which may be undesirable. This issue does not just affect \ourmethod{}; a method for automatically stopping sample evolution could improve results across  refinement techniques.

\section*{Acknowledgements}

This research is supported by the National Research Foundation Singapore under its AI Singapore Programme (Award Number: AISG-RP-2019-011) to H. Soh. Thank you to J. Scarlett for his comments regarding the proofs. \bibliography{refs}
\bibliographystyle{iclr2021}
\clearpage
\appendix
\section{Proofs}
\subsection{Lemma \ref{thm:gradient-flow}}
\label{subsec:proof_gradient-flow}
\begin{proof}
	Gradient flows in the Wasserstein space are of the form of the continuity equation (see \citet{ambrosio2008gradient}, page 281), i.e,
	
	
The velocity field  in Eq. (\ref{eq:app-continuity-eq}) is given by
	
	where  denotes the first variation of the functional . The first variation is defined as
	
where  for some .

Let's derive an expression for the the first variation of . In the following, we drop the notation for dependence on  for clarity,

	
	
Substituting  in Eq. (\ref{eq:velocity-formula}) we get,
	
	Substituting  in Eq. (\ref{eq:app-continuity-eq}) we get  the gradient flow, 
	
	
	where  and  denote the Laplace and the divergence operators respectively.
\end{proof}

\subsection{Lemma \ref{lemma:latent-density-ratio}}
\label{subsec:proof_densityratio}
\begin{proof}
	Let  be an integrable function on . If  has full column rank and  is an injective function, then we have the following change-of-variables equation~\citep{ben1999change,gemici2016normalizing},
	
	This implies that the infinitesimal volumes  and  are related as  and the densities  and  are related as .
	 Similarly, . Finally, the density-ratio  at the point  is given by
	 
\end{proof}

\section{A Discussion on \ourmethod{} for WGAN}
\label{sec:wgan-discussion}
We apply \ourmethod{} to WGAN models by treating the output from their critics as the logit for the estimation of density-ratio. However, it is well-known that WGAN critics are not density-ratio estimators as they are trained to maximize the 1-Wasserstein distance with an unconstrained output. In this section, we provide theoretical justification for the good performance of \ourmethod{} on WGAN models. We show that \ourmethod{} is related to the gradient flow of the entropy-regularized 1-Wasserstein functional ,
	
where  denotes the target density,  denotes the Lipschitz constant of the function . 

Let  be the function that achieves the supremum in Eq. (\ref{eq:-er-1-wasserstein}). This results in the functional,


Following a similar derivation as in Appendix \ref{subsec:proof_gradient-flow}, the gradient flow of  is given by the following PDE,


If  is approximated using the critic () of WGAN, we get the following gradient flow, 

which is same as the gradient flow of entropy-regularized -divergence with  (i.e., the KL divergence) when  is treated as a density-ratio estimator. The gradient flow of entropy-regularized -divergence with  is simplified below,


The equality of Eq. (\ref{eq:gradient-flow-w1}) and Eq. (\ref{eq:gradient-flow-kl}) implies that \ourmethod{} approximates the gradient flow of the 1-Wasserstein distance when the critic of WGAN is used for density-ratio estimation.

\section{Further Discussion on Related Work}
\label{app:relatedwork}
\paragraph{Energy-based \& Score-based Generative Models} 
\ourmethod{} is related to recently proposed energy-based generative models~\citep{arbel2020generalized, Deng2020Residual} --- one can view the energy functions used in these methods as a component that improves some base model. For example, the Generalized Energy-Based Model (GEBM) \citep{arbel2020generalized} jointly trains an implicit generative model with an energy function and uses Langevin dynamics to sample from the combination of the two. Similarly, in \citet{Deng2020Residual}, a discriminator that estimates the energy function is combined with a language model to train an energy-based text-generation model. 

Score-based generative modeling (SBGM)~\citep{song2019generative,song2020improved} is another active area of research closely-related to energy-based models. Noise Conditional Score Network (NCSN)~\citep{song2019generative,song2020improved}, a SBGM, trains a neural network to estimate the score function of a probability density at various noise levels. Once trained, this score network is used to evolve samples from noise to the data distribution using Langevin dynamics. NCSN can be viewed as a gradient flow that refines a sample right from noise to data; however, unlike \ourmethod{}, NCSN is a complete generative models in itself and not a sample refinement technique that can be applied to other generative models.



\paragraph{Other Related Work} Monte Carlo techniques have been used for improving various components in generative models, e.g., \citet{grover18variational} proposed Variational Rejection Sampling which performs rejection sampling in the latent space of VAEs to improve the variational posterior and \citet{grover2019bias} used likelihood-free importance sampling for bias correction in generative models . \citet{wu2020logan} proposed Latent Optimization GAN (LOGAN) which optimizes the latent vector as part of the training process unlike \ourmethod{} that refines the latent vector post training.


\section{Implementation Details}

\subsection{2D Datasets}

\paragraph{Datasets} The 25 Gaussians dataset was constructed by generating 100000 samples from a mixture of 25 equally likely 2D isotropic Gaussians with means  and standard deviation 0.05. Once generated, the data-points were normalized by  following \citet{tanaka2019discriminator}. The 2DSwissroll dataset was constructed by first generating 100000 samples of the 3D swissroll dataset using \texttt{make\_swiss\_roll} from \texttt{scikit-learn} with \texttt{noise=0.25} and then only keeping dimensions . The generated samples were normalized by 7.5. 

\paragraph{Base Models} We trained a WGAN-GP model for both the datasets. The generator was a fully-connected network with ReLU non-linearities that mapped  to . Similarly, the  discriminator was a fully-connected network with ReLU non-linearities that mapped  to . We refer the reader to \citet{gulrajani2017improved} for the exact network structures. The gradient penalty factor was set to 10. The models were trained for 10K generator iterations with a batch size of 256 using the Adam optimizer with a learning rate of , , and . We updated the discriminator 5 times for each generator iteration.

\paragraph{Hyperparameters} We ran DOT for 100 steps and performed gradient descent using the Adam optimizer with a learning rate of 0.01 and  as suggested by \citet{tanaka2019discriminator}. DDLS was run for 50 iterations with a step-size of 0.01 and the Gaussian noise was scaled by a factor of 0.1 as suggested by \citet{che2020your}. For \ourmethod{}, we set the step-size , the number of steps , and the noise regularizer . We used the output from the WGAN-GP discriminator directly as a logit for estimating the density ratio for DDLS and \ourmethod{}.

\paragraph{Metrics} We compared the different methods quantitatively on two metrics: \% high quality samples and kernel density estimate (KDE) score. A sample is classified as a high quality sample if it lies within 4 standard deviations of its nearest Gaussian. The KDE score is computed by first estimating the KDE using generated samples and then computing the log-likelihood of the training samples under the KDE estimate. KDE was performed using \texttt{sklearn.neighbors.KernelDensity} with a Gaussian kernel and a kernel bandwidth of 0.1. The quantitative metrics were averaged over 10 runs with 5000 samples from each method.

\subsection{Image Experiments}

\begin{table*}
	\centering
	\caption{\small Network architectures used for MMDGAN and VAE models.}
	\label{tab:network-arch}
	\subfloat[Generator or Decoder]{\begin{tabular}{l}
		\toprule
		Input Shape: \texttt{(b, d, 1, 1)}\\
		\midrule
		\texttt{Upconv(256)}\\
		\texttt{BatchNorm}\\
		\texttt{ReLU}\\
		\texttt{Upconv(128)}\\
		\texttt{BatchNorm}\\
		\texttt{ReLU}\\
		\texttt{Upconv(64)}\\
		\texttt{BatchNorm}\\
		\texttt{ReLU}\\
		\texttt{Upconv(3)}\\
		\texttt{Tanh}\\
		\midrule
		Output Shape: \texttt{(b, 3, 32, 32)}\\
		\bottomrule
	\end{tabular}}\qquad
	\subfloat[Discriminator or Encoder]{\begin{tabular}{l}
		\toprule
		Input Shape: \texttt{(b, 3, 32, 32)}\\
		\midrule
		\texttt{Conv(64)}\\
		\texttt{LeakyReLU(0.2)}\\
		\texttt{Conv(128)}\\
		\texttt{BatchNorm}\\
		\texttt{LeakyReLU(0.2)}\\
		\texttt{Conv(256)}\\
		\texttt{BatchNorm}\\
		\texttt{LeakyReLU(0.2)}\\
		\texttt{Conv(m)}\\
		\midrule
		Output Shape: \texttt{(b, m, 1, 1)}\\
		\bottomrule
	\end{tabular}}
\end{table*}

\paragraph{Datasets} CIFAR10~\citep{krizhevsky2009learning} is a dataset of 60K natural RGB images of size  from 10 classes. STL10 is a dataset of 100K natural RGB images of size  from 10 classes. We resized the STL10~\citep{coates2011analysis} dataset to  for SNGAN and WGAN-GP, and to  for MMDGAN, OCFGAN-GP, and VAE since the respective base models were trained on these sizes.  

\paragraph{Base Models for CIFAR10} We used the publicly available pre-trained models for WGAN-GP, SN-DCGAN (hi), and SN-DCGAN (ns). We refer the reader to \citet{tanaka2019discriminator} for exact details about these models. For SN-ResNet-GAN and OCFGAN-GP we used the pre-trained models from \citet{miyato2018spectral} and \citet{ansari2020characteristic} respectively. We used the respective discriminators of SN-DCGAN (ns), SN-DCGAN (hi), and WGAN-GP for density-ratio estimation when refining their generators. For the SN-ResNet-GAN (hi) generator, we used SN-DCGAN (ns) discriminator as the non-saturating loss provides a better density-ratio estimation than a discriminator trained using the hinge loss.

We trained our own models for MMDGAN, VAE, and Glow. We used the generator and discriminator architectures shown in Table \ref{tab:network-arch} for MMDGAN with . VAE used the same architecture with . Our Glow model was trained using the code available at \url{https://github.com/y0ast/Glow-PyTorch} with a batch size of 56 for 150 epochs. The density ratio correctors,  (see section \ref{sec:refinement-for-all}), were initialized with the weights from the SN-DCGAN (ns) released by \citet{tanaka2019discriminator}.  was then fine-tuned on images from SN-DCGAN (ns)'s generator and the generator being improved (e.g., MMDGAN and OCFGAN-GP) using SGD with a learning rate of  and momentum of 0.9.  We fine-tuned  for 10000 iterations with a batch size of 64.

\paragraph{Base Models for STL10} We used the publicly available pre-trained models~\citep{tanaka2019discriminator,ansari2020characteristic} for WGAN-GP, SN-DCGAN (hi), SN-DCGAN (ns), and OCFGAN-GP. We trained our own models for MMDGAN and VAE with the same architecture and training details as CIFAR10. We fine-tuned the density ratio correctors for STL10 for 5000 iterations with other details being the same as CIFAR10.

\paragraph{Hyperparameters} We performed 25 updates of \ourmethod{} for CIFAR10 and STL10 with a step size of 0.1 for models that do not require density ratio corrections. For STL10 models that require a density ratio correction, we performed 15 updates with a step size of 0.05. The noise regularizer (), whenever used, was set to 0.01.

\paragraph{Metrics}  We used the Fr\'echet Inception Distance (FID)~\citep{heusel2017gans} and Inception Score (IS)~\citep{salimans2016improved} metrics to evaluate the quality of generated samples before and after refinement. The IS denotes the confidence in classification of the generated samples using a pretrained InceptionV3 network whereas the FID is the Fr\'echet distance between multivariate Gaussians fitted to the 2048 dimensional feature vectors extracted from the InceptionV3 network for real and generated data. Both the metrics were computed using 50K samples for all the models, except Glow where we used 10K samples. Following \citet{tanaka2019discriminator}, we used the entire training and test set (60K images) for CIFAR10 and the entire unlabeled set (100K images) for STL10 as the set of real images used to compute FID.

\subsection{Character Level Language Modeling}
\paragraph{Dataset} We used the Billion Words dataset~\citep{chelba2013one} which was pre-processed into 32-character long strings.

\paragraph{Base Model} Our generator was a 1D CNN which followed the architecture used by \citet{gulrajani2017improved}.
\paragraph{Hyperparameters} We performed 50 updates of \ourmethod{} with a step size of 0.1 and noise factor .
\paragraph{Metrics} The JS-4 and JS-6 scores were computed using the code provided by \citet{gulrajani2017improved} at \url{https://github.com/igul222/improved_wgan_training}. We used 10000 samples from the models to compute the JS-4 score.


\begin{table*}
	\centering
	\footnotesize
	\caption{\small -divergences and their derivatives.}
	\label{tab:f-divergences}
	\begin{tabular}{llll}
		\toprule
		-divergence & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}\\
		\midrule
		KL          &  &  &  \\
		JS          &  &  &  \\
		 D    &  &  &  \\
		\bottomrule
	\end{tabular}
\end{table*}

\section{Additional Results}
\label{sec:additional-results}
\begin{figure}[htb]
	\centering
	\subfloat{\includegraphics[width=0.7\textwidth]{comp-f-wgan-25gaussians}}\\
	\subfloat{\includegraphics[width=0.7\textwidth]{comp-f-wgan-swissroll}}
	\caption{\small Qualitative comparison of \ourmethod{} with different -divergences on the 25Gaussians and 2DSwissroll datasets.}
	\label{fig:f-div-comparison-2d-datasets}
\end{figure}

\begin{figure}[htb]
	\centering
	\subfloat{\includegraphics[width=0.7\textwidth]{velocity-wgan-25gaussians}}\\
	\subfloat{\includegraphics[width=0.7\textwidth]{velocity-wgan-swissroll}}
	\caption{\small A vector plot showing the deterministic component of the velocity, i.e., the drift , for different -divergences on the 25Gaussians and 2DSwissroll dataset.}
	\label{fig:velocity-plot}
\end{figure}
\begin{wrapfigure}{R}{.4\textwidth}
\vspace{-3em}
	\centering
	\subfloat[25 Gaussians]{\includegraphics[width=0.4\textwidth]{wgan-25gaussians-ls}}\\
	\subfloat[2D Swissroll]{\includegraphics[width=0.4\textwidth]{wgan-swissroll-ls}}
	\caption{\small Latent space recovered by \ourmethod{} (right) for the 2D datasets is same as the one derived by \citet{che2020your} (left).}
	\label{fig:induced-latent-space}
\end{wrapfigure}
Fig. \ref{fig:f-div-comparison-2d-datasets} shows the samples generated by WGAN-GP (leftmost, blue) and refined samples generated using \ourmethod{} with different -divergences (red). Fig. \ref{fig:velocity-plot} shows the deterministic component, , of the velocity for different -divergences on the 2D datasets. Fig. \ref{fig:induced-latent-space} (right) shows the latent space distribution recovered by \ourmethod{} when applied in the latent space for the 2D datasets. This latent space is same as the one derived by \citet{che2020your}, i.e.,  which is shown in Fig. \ref{fig:induced-latent-space} (left) for both datasets.

Table \ref{tab:gans-iscore} shows the comparison of \ourmethod{} with DOT in terms of the inception score for the CIFAR10 and STL10 datasets. \ourmethod{} outperforms DOT significantly for all the base GAN models on both the datasets. Table \ref{tab:others-iscore} compares different variants of \ourmethod{} applied to MMDGAN, OCFGAN-GP, VAE, and Glow generators in terms of the inception score. \ourmethod{} leads to a significant improvement in the quality of samples for all the models. Tables \ref{tab:gans-without-noise} and \ref{tab:cifar-inception-no-noise} compare the deterministic variant of \ourmethod{} () against DOT and DDLS. These results show that the diffusion term only serves as an enhancement for \ourmethod{}, not a necessity, and it outperforms competing methods even without added noise. Table \ref{tab:other-gen-models-fid-without-correction} shows the results of \ourmethod{} on MMDGAN, OCFGAN-GP, and VAE models when the SN-DCGAN (ns) discriminator is directly used as a density-ratio estimator without an additional density-ratio corrector. Figures \ref{fig:cifar10-samples1}, \ref{fig:cifar10-samples2}, \ref{fig:stl10-samples1}, and \ref{fig:stl10-samples2} show the samples generated by the base model (left) and the refined samples (right) using \ourmethod{} for the CIFAR10 and STL10 datasets.

\paragraph{Runtime}

\ourmethod{} performs a backward pass through  to compute the gradient of the density-ratio with respect to the latent vector. This results in the same runtime complexity as that of DOT and DDLS. Table \ref{tab:runtime-comparison} shows a comparison of the runtimes of DOT, DDLS, and, \ourmethod{} on the 25Gaussians dataset under same conditions. As expected, these refinement methods have similar runtimes in practice. The wall-clock time required for \ourmethod{}\textsubscript{(KL)} to refine 100 samples from different base models on the CIFAR10 and STL10 datasets is reported in tables \ref{tab:runtime-nodrc} and \ref{tab:runtime-drc}. 

 \begin{table*}
 \centering
 	\caption{\small Runtime comparison of DOT, DDLS, and \ourmethod{}\textsubscript{(KL)} on the 25Gaussians dataset. The runtime is averaged over 100 runs with standard deviation reported in parentheses.}
 	\label{tab:runtime-comparison}
	\begin{tabular}{lr} 
	\toprule
	Method & Runtime (s) per 5K samples\\
	\midrule
	DOT & 2.24 (0.18)\\
	DDLS & 2.23 (0.14) \\
	\ourmethod{} & 2.22 (0.15)\\
	\bottomrule
	\end{tabular}	
 \end{table*}
  

 
  \begin{table*}
 \centering
 	\caption{\small Runtime of \ourmethod{}\textsubscript{(KL)} for models that do not require density-ratio correction on a single GeForce RTX 2080 Ti GPU. The runtime is averaged over 100 runs with standard deviation reported in parentheses.}
 	\label{tab:runtime-nodrc}
	\begin{tabular}{llr} 
	\toprule
	& Model &  Runtime (s) per 100 samples \\
	\midrule
	\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{CIFAR10}}} & WGAN-GP & 0.897 (0.017)\\
	& SN-DCGAN (hi) & 0.952 (0.008)\\
	& SN-DCGAN (ns) & 0.952 (0.007)\\
	& SN-ResNet-GAN & 1.982 (0.013)\\
	\midrule
	\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize{STL10}}}&
	WGAN-GP & 1.376 (0.025)\\
	& SN-DCGAN (hi) & 1.413 (0.015)\\
	& SN-DCGAN (ns) & 1.415 (0.013)\\
	\bottomrule
	\end{tabular}	
 \end{table*}
 
   \begin{table*}
 \centering
 	\caption{\small Runtime of \ourmethod{}\textsubscript{(KL)} for models that require density-ratio correction on a single GeForce RTX 2080 Ti GPU. The runtime is averaged over 100 runs with standard deviation reported in parentheses.}
 	\label{tab:runtime-drc}
	\begin{tabular}{llr} 
	\toprule
	& Model &  Runtime (s) per 100 samples\\
	\midrule
	\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize{CIFAR10}}} & MMDGAN & 1.192 (0.007)\\
	& OCFGAN-GP & 1.186 (0.011)\\
	& VAE & 1.186 (0.012)\\
	\midrule
	\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize{STL10}}}& MMDGAN & 1.036 (0.004)\\
	& OCFGAN-GP & 1.029 (0.010)\\
	& VAE & 1.028 (0.011)\\
	\bottomrule
	\end{tabular}	
 \end{table*}

\begin{table*}
	\footnotesize
	\caption{\small Comparison of different variants of \ourmethod{} with DOT on the CIFAR10 and STL10 datasets. Higher scores are better.}
	\label{tab:gans-iscore}
	\centering
	\begin{tabular}{llrrrrr} 
	\toprule
	& \multirow{3}{*}{Model} & \multicolumn{5}{c}{Inception Score}\\
	\cmidrule{3-7}
	& & \multicolumn{1}{c}{Base Model} & \multicolumn{1}{c}{DOT} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{(KL)}} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{(JS)}} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{( D)}}	\\
	\midrule
	\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{CIFAR10}}}
	& WGAN-GP & 6.51 (.02) & 7.45 & \textbf{7.99 (.02)} & 7.71 (.02) & 7.11 (.03)\\ 
	& SN-DCGAN (hi) & 7.35 (.03) & 8.02 & \textbf{8.13 (.02)} & 7.98 (.01) & 7.85 (.02)\\
	& SN-DCGAN (ns) & 7.38 (.03) & 7.97 & \textbf{8.14 (.03)} & 7.98 (.04) & 7.94 (.01)\\
	& SN-ResNet-GAN & 8.38 (.03) & -- & \textbf{9.35 (.03)} & 9.13 (.04) & 9.05 (.03)\\
	\midrule
	\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize{STL10}}}
	& WGAN-GP & 8.72 (.02) & 9.31 & \textbf{10.41 (.02)} & 8.85 (.06) & 9.80 (.03)\\ 
	& SN-DCGAN (hi) & 8.77 (.03) & 9.35 & \textbf{9.74 (.04)} & 9.50 (.05) & 9.41 (.07)\\
	& SN-DCGAN (ns) & 8.61 (.04) & 9.45 & \textbf{9.66 (.01)} & 9.49 (.03) & 9.18 (.03)\\
	\bottomrule
	\end{tabular}
\end{table*}

\begin{table*}
	\footnotesize
	\caption{\small Comparison of different variants of \ourmethod{} applied to MMDGAN, OCFGAN-GP, VAE, and Glow models. Higher scores are better.}
	\label{tab:others-iscore}
	\centering
	\begin{tabular}{llrrrr} 
	\toprule
	& \multirow{2}{*}{Model} & \multicolumn{4}{c}{Inception Score}\\
	\cmidrule{3-6}
	 & & Base Model & \multicolumn{1}{c}{\ourmethod{}\textsubscript{(KL)}} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{(JS)}} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{( D)}}	\\
	\midrule
	\multirow{4}{*}{\rotatebox[origin=c]{90}{\scriptsize{CIFAR10}}}
	& MMDGAN & 5.74 (.02) & \textbf{6.27 (.05)} & 5.99 (.03) & 6.02 (.01)\\
	& OCFGAN-GP & 6.52 (.02) & \textbf{7.21 (.05)} & 6.93 (.03) & 6.92 (.03)\\
	& VAE & 3.20 (.01) & \textbf{3.85 (.01)} & 3.21 (.02) & 3.57 (.02)\\
	& Glow & 3.64 (.02) & \textbf{4.57 (.02)} & 3.91 (.03) & 4.47 (.03)\\
	\midrule
	\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize{CIFAR10}}}
	& MMDGAN & 6.07 (.02) & \textbf{6.16 (.01)} & 6.12 (.03) & 6.12 (.03)\\
	& OCFGAN-GP & 7.09 (.01) & \textbf{7.46 (.04)} & 7.10 (.03) & 7.33 (.02)\\
	& VAE & 3.25 (.01) & \textbf{3.72 (.04)} & 3.27 (.01) & 3.65 (.03)\\ 

	\bottomrule
	\end{tabular}
	
\end{table*}

\begin{table*}
	\footnotesize
	\caption{\small Comparison of different variants of \ourmethod{} without diffusion (i.e., ) on the CIFAR10 and STL10 datasets. Lower scores are better.}
	\label{tab:gans-without-noise}
	\centering
	\begin{tabular}{llrrrrr} 
	\toprule
	& \multirow{3}{*}{Model} & \multicolumn{5}{c}{Fr\'echet Inception Distance}\\
	\cmidrule{3-7}
	& & \multicolumn{1}{c}{Base Model} & \multicolumn{1}{c}{DOT} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{(KL)}} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{(JS)}} & \multicolumn{1}{c}{\ourmethod{}\textsubscript{( D)}}	\\
	\midrule
	\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize{CIFAR10}}}
	& WGAN-GP & 28.34 (.11) & 24.14  & 24.64 (.13) & \textbf{23.30 (.11)} & 24.42 (.19)\\ 
	& SN-DCGAN (hi) & 20.67 (.09) & 17.12 & \textbf{15.79 (.07)} & 16.79 (.09) & 17.79 (.05)\\
	& SN-DCGAN (ns) & 20.94 (.12) & 15.78 & \textbf{15.47 (.11)} & 16.32 (.11) & 16.97 (.08)\\
	\midrule
	\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize{STL10}}}
	& WGAN-GP & 51.34 (.21) & 44.45 & \textbf{38.96 (.08)} & 50.44 (.09) & 39.35 (.12)\\ 
	& SN-DCGAN (hi) & 40.82 (.16) & \textbf{34.85} & 35.18 (.09) & 36.53 (.13) & 36.75 (.13)\\
	& SN-DCGAN (ns) & 41.83 (.20) & \textbf{34.84} & \textbf{34.81 (.08)} & 35.75 (.10) &  37.68 (.08)\\
	\bottomrule
	\end{tabular}
\end{table*}

\begin{table*}
	\footnotesize
	\caption{\small Comparison of DDLS with \ourmethod{} (with and without diffusion) on the CIFAR10 dataset. Higher scores are better.}
	\label{tab:cifar-inception-no-noise}
	\centering
	\begin{tabular}{lr} 
	\toprule
	\multirow{1}{*}{Model} & \multicolumn{1}{c}{Inception Score}\\
	\midrule
	SN-ResNet-GAN~\citep{miyato2018spectral} & 8.22 (.05)\\
	SN-ResNet-GAN + DDLS (cal)~\citep{che2020your} & 9.09 (.10)\\
	\midrule
	SN-ResNet-GAN (our evaluation) & 8.38 (.03)\\
	SN-ResNet-GAN + \ourmethod{}\textsubscript{(KL)} () & \textbf{9.35 (.04)}\\
	SN-ResNet-GAN + \ourmethod{}\textsubscript{(KL)} () & \textbf{9.35 (.03)}\\
	\midrule
	BigGAN & 9.22\\
	\bottomrule
	\end{tabular}
\end{table*}


\begin{table*}
	\footnotesize
	\caption{\small Comparison of different variants of \ourmethod{} applied to MMDGAN, OCFGAN-GP, and VAE models without density-ratio correction. Lower scores are better.}
	\label{tab:other-gen-models-fid-without-correction}
	\centering
	\begin{tabular}{lrrrrr} 
	\toprule
	\multicolumn{2}{c}{\multirow{2}{*}{Model}} & \multicolumn{3}{c}{Fr\'echet Inception Distance}\\
	\cmidrule{3-6}
	& & \multicolumn{1}{c}{Base Model} & \multicolumn{1}{c}{KL} & \multicolumn{1}{c}{JS} & \multicolumn{1}{c}{ D}	\\
	\midrule
	\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize{CIFAR10}}} &
	MMDGAN & 42.03 (.06) & \textbf{39.06 (.08)} & 39.68 (.06) & 39.47 (.07)\\ 
	& OCFGAN-GP & 31.95 (.07) & 27.92 (.08) & 29.25 (.06) & \textbf{28.82 (.10)}\\
	& VAE & 129.49 (.19) & \textbf{127.50 (.15)} & 128.24 (.11) & 128.3 (.14)\\
	& Glow & 100.7 (.14) & \textbf{93.47 (.09)} & 97.50 (.11) & 97.78 (.14)\\
	\midrule
	\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize{STL10}}} &
	MMDGAN & 47.22 (.04) & \textbf{45.75 (.10)} & 45.96 (.07) & 46.26 (.13)\\ 
	& OCFGAN-GP & 36.60 (.15) & \textbf{34.17 (.18)} & 34.42 (.04) & 34.99 (.07)\\
	& VAE & \textbf{150.49 (.07)} & 151.76 (.01) & 152.03 (.05) & 151.88 (.11)\\
	\bottomrule
	\end{tabular}
	
\end{table*}

\begin{figure}
	\centering
	\subfloat[WGAN-GP]{\includegraphics[width=0.32\textwidth]{cifar10_wgan_before}}\qquad
	\subfloat[WGAN-GP + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{cifar10_wgan_after}}\\
	\subfloat[SN-DCGAN (hi)]{\includegraphics[width=0.32\textwidth]{cifar10_snganhi_before}}\qquad
	\subfloat[SN-DCGAN (hi) + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{cifar10_snganhi_after}}\\
	\subfloat[SN-DCGAN (ns)]{\includegraphics[width=0.32\textwidth]{cifar10_snganns_before}}\qquad
	\subfloat[SN-DCGAN (ns) + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{cifar10_snganns_after}}\\
	\subfloat[SN-ResNet-GAN]{\includegraphics[width=0.32\textwidth]{cifar10_snresnet_before}}\qquad
	\subfloat[SN-ResNet-GAN + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{cifar10_snresnet_after}}\\
	\vspace{-0.7em}
	\caption{\small Samples from different models for the CIFAR10 dataset before (left) and after (right) refinement using \ourmethod{}.}
	\label{fig:cifar10-samples1}
\end{figure}

\begin{figure}
	\centering
	\subfloat[MMDGAN]{\includegraphics[width=0.32\textwidth]{cifar10_mmdgan_before}}\qquad
	\subfloat[MMDGAN + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{cifar10_mmdgan_after}}\\
	\subfloat[OCFGAN-GP]{\includegraphics[width=0.32\textwidth]{cifar10_ocfgan_before}}\qquad
	\subfloat[OCFGAN-GP + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{cifar10_ocfgan_after}}\\
	\subfloat[VAE]{\includegraphics[width=0.32\textwidth]{cifar10_vae_before}}\qquad
	\subfloat[VAE + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{cifar10_vae_after}}\\
	\subfloat[Glow]{\includegraphics[width=0.32\textwidth]{cifar10_glow_before}}\qquad
	\subfloat[Glow + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{cifar10_glow_after}}\\
	\vspace{-0.7em}
	\caption{\small Samples from different models for the CIFAR10 dataset before (left) and after (right) refinement using \ourmethod{}.}
	\label{fig:cifar10-samples2}
\end{figure}

\begin{figure}
	\centering
	\subfloat[WGAN-GP]{\includegraphics[width=0.32\textwidth]{stl10_wgan_before}}\qquad
	\subfloat[WGAN-GP + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{stl10_wgan_after}}\\
	\subfloat[SN-DCGAN (hi)]{\includegraphics[width=0.32\textwidth]{stl10_snganhi_before}}\qquad
	\subfloat[SN-DCGAN (hi) + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{stl10_snganhi_after}}\\
	\subfloat[SN-DCGAN (ns)]{\includegraphics[width=0.32\textwidth]{stl10_snganns_before}}\qquad
	\subfloat[SN-DCGAN (ns) + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{stl10_snganns_after}}
	\vspace{-0.7em}
	\caption{\small Samples from different models for the STL10 dataset before (left) and after (right) refinement using \ourmethod{}.}
	\label{fig:stl10-samples1}
\end{figure}

\begin{figure}
	\centering
	\subfloat[MMDGAN]{\includegraphics[width=0.32\textwidth]{stl10_mmdgan_before}}\qquad
	\subfloat[MMDGAN + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{stl10_mmdgan_after}}\\
	\subfloat[OCFGAN-GP]{\includegraphics[width=0.32\textwidth]{stl10_ocfgan_before}}\qquad
	\subfloat[OCFGAN-GP + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{stl10_ocfgan_after}}\\
	\subfloat[VAE]{\includegraphics[width=0.32\textwidth]{stl10_vae_before}}\qquad
	\subfloat[VAE + \ourmethod{}]{\includegraphics[width=0.32\textwidth]{stl10_vae_after}}
	\vspace{-0.7em}
	\caption{\small Samples from different models for the STL10 dataset before (left) and after (right) refinement using \ourmethod{}.}
	\label{fig:stl10-samples2}
\end{figure}

 
\end{document}
