\onecolumn
\section{Proof of Proposition 1, 2, and 3}
\label{breg:ns:details}
We can reformulate  as follows:

Letting , , , and , we can reformulate Eq.~(\ref{app:eq:nce:int}) as:

With  and , we can reformulate Eq.~(\ref{app:eq:nce:exp}) as:

From Eq.~(\ref{app:eq:ns:breg}), when  is minimized,  is satisfied. In this condition,  becomes , and  becomes  as follows:

Based on the Eq.~(\ref{eq:softmax}) and  Eq.~(\ref{app:eq:ns:obj-true}), the objective distribution for  is as follows:


\section{Proof of Proposition 4}
\label{app:proof:pmi}
PMI is induced by multiplying  to the right-hand side of Eq.~(\ref{eq:ns:obj-true}) and then computing logarithm for both sides as follows:


\section{Proof of Proposition 5}
\label{app:proof:uninoise}
When  is a uniform distribution, , and thus, Eq.~(\ref{eq:ns:softmax}) becomes .

\section{Experimental Details}
\label{app:params}

\noindent\textbf{Dataset}: We use FB15k-237~\cite{toutanova-chen-2015-observed}\footnote{\url{https://www.microsoft.com/en-us/download/confirmation.aspx?id=52312}} and WN18RR~\cite{dettmers2018conve}\footnote{\url{https://github.com/TimDettmers/ConvE}} datasets in the experiments. We followed the standard split in the original papers for each dataset. Table \ref{tab:app:data} lists the statistics for each dataset.
\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{llllll}
    \toprule
    \multirow{2}{*}{Dataset}&\multirow{2}{*}{Entities}&\multirow{2}{*}{Relations}&\multicolumn{3}{c}{Tuples}\\
    \cmidrule(lr){4-6}
 &  &  & Train & Valid & Test\\
    \midrule
WN18RR &40,943 &11 &86,835 &3,034 &3,134\\
FB15k-237 &14,541 &237 &272,115 &17,535& 20,466  \\
\bottomrule
    \end{tabular}
    \caption{The numbers of each instance for each dataset.}
    \label{tab:app:data}
\end{table}

\noindent\textbf{Metric}: We evaluated the link prediction performance of models with MRR, Hits@1, Hits@3, and Hits@10 by ranking test triples against all other triples not appeared in the training, valid, and test datasets. We used LibKGE for calculating these metric scores.

\noindent\textbf{Model}: We compared the following models:
TuckER~\cite{balazevic-etal-2019-tucker}; RESCAL~\cite{10.5555/2900423.2900470}; ComplEx~\cite{DBLP:conf/icml/TrouillonWRGB16}; DistMult~\cite{yang2015embedding}; TransE~\cite{NIPS2013_1cecc7a7}; RotatE~\cite{DBLP:journals/corr/abs-1902-10197}.
For each model, we also trained a model for the reverse direction that shares the entity embeddings with the model for the forward direction. Thus, the dimension size of subject and object embeddings are the same in all models.

\noindent\textbf{Implementation}: We used LibKGE~\cite{libkge}\footnote{\url{https://github.com/uma-pi1/kge}} as the implementation.
We used its 1vsAll setting for SCE-based loss functions and negative sampling setting for NS-based loss functions.
We modified LibKGE to be able to use label smoothing on the 1vsAll setting.
We also incorporated NS w/ Freq and SCE w/ BC into the implementation.

\noindent\textbf{Hyper-parameter}: Table \ref{tab:hp:fb15k-237} and \ref{tab:hp:wn18rr} show the hyper-parameter settings of each method for each dataset.
In RESCAL, ComplEx, and DistMult we used the settings that achieved the highest performance for each loss function in the previous study \cite{Ruffinelli2020You}\footnote{\url{https://github.com/uma-pi1/kge-iclr20}}.
In TuckER and RotatE, we follow the settings from the original paper. When applying SANS, we set  to an initial value of 1.0 for LibKGE for all models except TransE and RotatE, and for TransE and RotatE, where we followed the settings of the original paper of SANS since SANS was used in it.
When applying SCE w/ LS, we set  to the initial value of LibKGE, 0.3, except on TransE and RotatE.
In the original setting of TransE and RotatE, because the value of SANS was tuned for comparison, for fairness, we selected  from \{0.3, 0.1, 0.01\} by using the development data through a single run for each value.
We set the maximum epoch to 800.
We calculated MRR every five epochs on the developed data, and the training was terminated when the highest value was not updated ten times.
We chose the best model by using the MRR score on the development data. 
These hyperparameters were also used in the pre-training step.

\begin{table}[h!]
\resizebox{\columnwidth}{!}{
\begin{tabular}{llllllllllllllllll}
\toprule
 &
   &
  \multicolumn{16}{c}{\textbf{FB15k-237}} \\
  \midrule
\multicolumn{2}{c}{\multirow{2}{*}{Model}} &
  \multirow{2}{*}{Batch} &
  \multirow{2}{*}{Dim} &
  \multirow{2}{*}{Initialize} &
  \multicolumn{3}{l}{Regularize} &
  \multicolumn{2}{l}{Dropout} &
  \multicolumn{4}{l}{Optimizer} &
  \multicolumn{2}{l}{Sample} &
  \multirow{2}{*}{} &
  \multirow{2}{*}{} \\
  \cmidrule(lr){6-8}\cmidrule(lr){9-10}\cmidrule(lr){11-14}\cmidrule(lr){15-16}
\multicolumn{2}{l}{} &
   &
   &
   &
  Type &
  Entity &
  Relation &
  Entity &
  Rel. &
  Type &
  LR &
  Decay &
  P. &
  sub. &
  obj. &
   &
   \\
   \midrule
\multirow{4}{*}{TuckER} &
  SCE &
  128 &
  200 &
  xn: 1.0 &
  - &
  - &
  - &
  0.3 &
  0.4 &
  Adam &
  0.0005 &
  - &
  - &
  All &
  All &
  - &
  - \\
 &
  SCE w/ LS &
  128 &
  200 &
  xn: 1.0 &
  - &
  - &
  - &
  0.3 &
  0.4 &
  Adam &
  0.0005 &
  - &
  - &
  All &
  All &
  0.3 &
  - \\
 &
  NS &
  128 &
  200 &
  xn: 1.0 &
  - &
  - &
  - &
  0.3 &
  0.4 &
  Adam &
  0.0005 &
  - &
  - &
  All &
  All &
  - &
  - \\
 &
  SANS &
  128 &
  200 &
  xn: 1.0 &
  - &
  - &
  - &
  0.3 &
  0.4 &
  Adam &
  0.0005 &
  - &
  - &
  All &
  All &
  - &
  1.0 \\
  \midrule
\multirow{4}{*}{Rescal} &
  SCE &
  512 &
  128 &
  n: 0.123 &
  - &
  - &
  - &
  0.427 &
  0.159 &
  Adam &
  7.39E-5 &
  0.95 &
  1 &
  All &
  All &
  - &
  - \\
 &
  SCE w/ LS &
  512 &
  128 &
  n: 0.123 &
  - &
  - &
  - &
  0.427 &
  0.159 &
  Adam &
  7.39E-5 &
  0.95 &
  1 &
  All &
  All &
  0.3 &
  - \\
 &
  NS &
  256 &
  128 &
  xn: 1.0 &
  lp: 3 &
  1.22E-12 &
  4.80E-14 &
  0.347 &
  - &
  Adagrad &
  0.0170 &
  0.95 &
  5 &
  22 &
  155 &
  - &
  - \\
 &
  SANS &
  256 &
  128 &
  xn: 1.0 &
  lp: 3 &
  1.22E-12 &
  4.80E-14 &
  0.347 &
  - &
  Adagrad &
  0.0170 &
  0.95 &
  5 &
  22 &
  155 &
  - &
  1.0 \\
  \midrule
\multirow{4}{*}{ComlEx} &
  SCE &
  512 &
  128 &
  u: 0.311 &
  - &
  - &
  - &
  0.0476 &
  0.443 &
  Adagrad &
  0.503 &
  0.95 &
  7 &
  All &
  All &
  - &
  - \\
 &
  SCE w/ LS &
  512 &
  128 &
  u: 0.311 &
  - &
  - &
  - &
  0.0476 &
  0.443 &
  Adagrad &
  0.503 &
  0.95 &
  7 &
  All &
  All &
  0.3 &
  - \\
 &
  NS &
  512 &
  256 &
  n: 4.81E-5 &
  lp: 2 &
  6.34E-9 &
  9.08E-18 &
  0.182 &
  0.0437 &
  Adagrad &
  0.241 &
  0.95 &
  4 &
  1 &
  48 &
  - &
  - \\
 &
  SANS &
  512 &
  256 &
  n: 4.81E-5 &
  lp: 2 &
  6.34E-9 &
  9.08E-18 &
  0.182 &
  0.0437 &
  Adagrad &
  0.241 &
  0.95 &
  4 &
  1 &
  48 &
  - &
  1.0 \\
  \midrule
\multirow{4}{*}{DistMult} &
  SCE &
  512 &
  128 &
  n: 0.806 &
  - &
  - &
  - &
  0.370 &
  0.280 &
  Adam &
  0.00063 &
  0.95 &
  1 &
  All &
  All &
  - &
  - \\
 &
  SCE &
  512 &
  128 &
  n: 0.806 &
  - &
  - &
  - &
  0.370 &
  0.280 &
  Adam &
  0.00063 &
  0.95 &
  1 &
  All &
  All &
  0.3 &
  - \\
 &
  NS &
  1024 &
  256 &
  u: 0.848 &
  lp: 3 &
  1.55E-10 &
  3.93E-15 &
  0.455 &
  0.360 &
  Adagrad &
  0.141 &
  0.95 &
  9 &
  557 &
  367 &
  - &
  - \\
 &
  SANS &
  1024 &
  256 &
  u: 0.848 &
  lp: 3 &
  1.55E-10 &
  3.93E-15 &
  0.455 &
  0.360 &
  Adagrad &
  0.141 &
  0.95 &
  9 &
  557 &
  367 &
  - &
  1.0 \\
  \midrule
\multirow{4}{*}{TransE} &
  SCE &
  128 &
  128 &
  u: 1.0E-5 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.0003 &
  0.95 &
  5 &
  All &
  All &
  - &
  - \\
 &
  SCE w/ LS &
  128 &
  128 &
  u: 1.0E-5 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.0003 &
  0.95 &
  5 &
  All &
  All &
  0.01 &
  - \\
 &
  NS &
  1024 &
  1000 &
  xu: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00005 &
  0.95 &
  5 &
  256 &
  256 &
  - &
  - \\
 &
  SANS &
  1024 &
  1000 &
  xu: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00005 &
  0.95 &
  5 &
  256 &
  256 &
  - &
  1.0\\
  \midrule
\multirow{4}{*}{Rotate} &
  SCE &
  1024 &
  1000 &
  xu: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00005 &
  0.95 &
  5 &
  All &
  All &
  - &
  - \\
 &
  SCE w/ LS &
  1024 &
  1000 &
  xu: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00005 &
  0.95 &
  5 &
  All &
  All &
  0.01 &
  - \\
 &
  NS &
  1024 &
  1000 &
  xu: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00005 &
  0.95 &
  5 &
  256 &
  256 &
  - &
  - \\
 &
  SANS &
  1024 &
  1000 &
  xu: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00005 &
  0.95 &
  5 &
  256 &
  256 &
  - &
  1.0\\
  \bottomrule
\end{tabular}}
\caption{The hyper-parameters for each model in FB15k-237. Rel. denotes relation, P. denotes patience, sub. denotes subjective, obj. denotes objective, xn denotes xavier normal, n denotes normal, xu denotes xavier uniform, and u denotes uniform.}
\label{tab:hp:fb15k-237}
\end{table}

\begin{table}[h!]
\resizebox{\columnwidth}{!}{
\begin{tabular}{llllllllllllllllll}
\toprule
 &
   &
  \multicolumn{16}{c}{\textbf{WN18RR}} \\
  \midrule
\multicolumn{2}{c}{\multirow{2}{*}{Model}} &
  \multirow{2}{*}{Batch} &
  \multirow{2}{*}{Dim} &
  \multirow{2}{*}{Initialize} &
  \multicolumn{3}{l}{Regularize} &
  \multicolumn{2}{l}{Dropout} &
  \multicolumn{4}{l}{Optimizer} &
  \multicolumn{2}{l}{Sample} &
  \multirow{2}{*}{} &
  \multirow{2}{*}{} \\
  \cmidrule(lr){6-8}\cmidrule(lr){9-10}\cmidrule(lr){11-14}\cmidrule(lr){15-16}
\multicolumn{2}{l}{} &
   &
   &
   &
  Type &
  Entity &
  Relation &
  Entity &
  Rel. &
  Type &
  LR &
  Decay &
  P. &
  sub. &
  obj. &
   &
   \\
   \midrule
\multirow{4}{*}{TuckER} &
  SCE &
  128 &
  200 &
  xn: 1.0 &
  - &
  - &
  - &
  0.2 &
  0.2 &
  Adam &
  0.0005 &
  - &
  - &
  All &
  All &
  - &
  - \\
 &
  SCE w/ LS &
  128 &
  200 &
  xn: 1.0 &
  - &
  - &
  - &
  0.2 &
  0.2 &
  Adam &
  0.0005 &
  - &
  - &
  All &
  All &
  0.3 &
  - \\
 &
  NS &
  128 &
  200 &
  xn: 1.0 &
  - &
  - &
  - &
  0.2 &
  0.2 &
  Adam &
  0.0005 &
  - &
  - &
  All &
  All &
  - &
  - \\
 &
  SANS &
  128 &
  200 &
  xn: 1.0 &
  - &
  - &
  - &
  0.2 &
  0.2 &
  Adam &
  0.0005 &
  - &
  - &
  All &
  All &
  - &
  1.0 \\
  \midrule
\multirow{4}{*}{Rescal} &
  SCE &
  512 &
  256 &
  xn: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00246 &
  0.95 &
  9 &
  All &
  All &
  - &
  - \\
 &
  SCE w/ LS &
  512 &
  256 &
  xn: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00246 &
  0.95 &
  9 &
  All &
  All &
  0.3 &
  - \\
 &
  NS &
  512 &
  128 &
  n: 1.64E-4 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00152 &
  0.95 &
  1 &
  6 &
  8 &
  - &
  - \\
 &
  SANS &
  512 &
  128 &
  n: 1.64E-4 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00152 &
  0.95 &
  1 &
  6 &
  8 &
  - &
  1.0 \\
  \midrule
\multirow{4}{*}{ComlEx} &
  SCE &
  512 &
  128 &
  u: 0.281 &
  lp: 2 &
  4.52E-6 &
  4.19E-10 &
  0.359 &
  0.311 &
  Adagrad &
  0.526 &
  0.95 &
  5 &
  All &
  All &
  - &
  - \\
 &
  SCE w/ LS &
  512 &
  128 &
  u: 0.281 &
  lp: 2 &
  4.52E-6 &
  4.19E-10 &
  0.359 &
  0.311 &
  Adagrad &
  0.526 &
  0.95 &
  5 &
  All &
  All &
  0.3 &
  - \\
 &
  NS &
  1024 &
  128 &
  xn: 1.0 &
  - &
  - &
  - &
  0.0466 &
  0.0826 &
  Adam &
  3.32E-5 &
  0.95 &
  7 &
  6 &
  6 &
  - &
  - \\
 &
  SANS &
  1024 &
  128 &
  xn: 1.0 &
  - &
  - &
  - &
  0.0466 &
  0.0826 &
  Adam &
  3.32E-5 &
  0.95 &
  7 &
  6 &
  6 &
  - &
  1.0 \\
  \midrule
\multirow{4}{*}{DistMult} &
  SCE &
  512 &
  128 &
  u: 0.311 &
  lp: 2 &
  1.44E-18 &
  1.44E-18 &
  0.0476 &
  0.443 &
  Adagrad &
  0.503 &
  0.95 &
  7 &
  All &
  All &
  - &
  - \\
 &
  SCE w/ LS &
  512 &
  128 &
  u: 0.311 &
  lp: 2 &
  1.44E-18 &
  1.44E-18 &
  0.0476 &
  0.443 &
  Adagrad &
  0.503 &
  0.95 &
  7 &
  All &
  All &
  0.3 &
  - \\
 &
  NS &
  1024 &
  128 &
  xn: 1.0 &
  - &
  - &
  - &
  0.0466 &
  0.0826 &
  Adam &
  3.32E-5 &
  0.95 &
  7 &
  6 &
  6 &
  - &
  - \\
 &
  SANS &
  1024 &
  128 &
  xn: 1.0 &
  - &
  - &
  - &
  0.0466 &
  0.0826 &
  Adam &
  3.32E-5 &
  0.95 &
  7 &
  6 &
  6 &
  - &
  1.0 \\
  \midrule
\multirow{4}{*}{TransE} &
  SCE &
  128 &
  512 &
  xn: 1.0 &
  lp: 2 &
  2.13E-7 &
  8.99E-13 &
  0.252 &
  - &
  Adagrad &
  0.253 &
  0.95 &
  5 &
  All &
  All &
  - &
  - \\
 &
  SCE w/ LS &
  128 &
  512 &
  xn: 1.0 &
  lp: 2 &
  2.13E-7 &
  8.99E-13 &
  0.252 &
  - &
  Adagrad &
  0.253 &
  0.95 &
  5 &
  All &
  All &
  0.01 &
  - \\
 &
  NS &
  512 &
  500 &
  xu: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00005 &
  0.95 &
  5 &
  1024 &
  1024 &
  - &
  - \\
 &
  SANS &
  512 &
  500 &
  xu: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00005 &
  0.95 &
  5 &
  1024 &
  1024 &
  - &
  0.5\\
  \midrule
\multirow{4}{*}{Rotate} &
  SCE &
  512 &
  500 &
  xu: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00005 &
  0.95 &
  5 &
  All &
  All &
  - &
  - \\
 &
  SCE w/ LS &
  512 &
  500 &
  xu: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00005 &
  0.95 &
  5 &
  All &
  All &
  0.01 &
  - \\
 &
  NS &
  512 &
  500 &
  xu: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00005 &
  0.95 &
  5 &
  1024 &
  1024 &
  - &
  - \\
 &
  SANS &
  512 &
  500 &
  xu: 1.0 &
  - &
  - &
  - &
  - &
  - &
  Adam &
  0.00005 &
  0.95 &
  5 &
  1024 &
  1024 &
  - &
  0.5\\
  \bottomrule
\end{tabular}}
\caption{The hyper-parameters for each model in WN18RR. The notations are the same as Table \ref{tab:hp:fb15k-237}.}
\label{tab:hp:wn18rr}
\end{table}

\noindent\textbf{Validation Score} Table \ref{tab:valid}, \ref{tab:valid:pretraining}, and \ref{tab:valid:pretrained} show the best MRR scores of each loss for each model on the validation dataset.

\begin{table*}[h!]
\centering
\small
\begin{tabular}{llcc}
\toprule
             \textbf{Model}       &  \textbf{Loss} & \textbf{FB15k-237} & \textbf{WN18RR} \\
\midrule
\multirow{4}{*}{TuckER}  & SCE       & 0.345     & 0.451  \\
                         & SCE w/ LS & 0.350     & 0.470  \\
                         & NS        & 0.261     & 0.433  \\
                         & SANS      & 0.337     & 0.441  \\
\midrule
\multirow{4}{*}{RESCAL}  & SCE       & 0.359     & 0.461  \\
                         & SCE w/ LS & 0.369     & 0.474  \\
                         & NS        & 0.344     & 0.389  \\
                         & SANS      & 0.344     & 0.390  \\
\midrule
\multirow{4}{*}{ComplEx} & SCE       & 0.304     & 0.468  \\
                         & SCE w/ LS & 0.324     & 0.478  \\
                         & NS        & 0.302     & 0.399  \\
                         & SANS      & 0.308     & 0.433  \\
\midrule
\multirow{4}{*}{DistMult} & SCE      & 0.350    & 0.441  \\
                         & SCE w/ LS & 0.351    & 0.451  \\
                         & NS        & 0.308    & 0.391  \\
                         & SANS      & 0.326    & 0.412  \\
\midrule
\multirow{4}{*}{TransE} & SCE       & 0.328 & 0.227 \\
                         & SCE w/ LS & 0.322 & 0.220 \\
                         & NS        & 0.289 & 0.216 \\
                         & SANS      & 0.333 & 0.218 \\
\midrule
\multirow{4}{*}{RotatE}  & SCE       & 0.320     & 0.452  \\
                         & SCE w/ LS & 0.320     & 0.449  \\
                         & NS        & 0.306     & 0.472  \\
                         & SANS      & 0.340     & 0.475  \\
\bottomrule
\end{tabular}
\caption{The best MRR scores on validation data.}
\label{tab:valid}
\end{table*}

\begin{table}[h]
\centering
\small
\begin{tabular}{lll}
\toprule
Dataset                    & Mehotd            & MRR   \\
\midrule
\multirow{2}{*}{FB15k-237} & RESCAL+SCE w/BC   & 0.149 \\
                           & RESCAL+NS w/ Freq & 0.171 \\
\midrule
\multirow{2}{*}{WN18RR}    & ComplEx+SCE w/ BC & 0.361 \\
                           & RotatE+NS w/ Freq & 0.469 \\
\bottomrule
\end{tabular}
\caption{The best MRR scores of pre-trained models on validation data.}
\label{tab:valid:pretraining}
\end{table}

\begin{table}[h]
\centering
\small
\begin{tabular}{lll}
\toprule
\multicolumn{3}{c}{\textbf{FB15k-237}}                           \\
\midrule
Method                             & Pretrain   & MRR   \\
\midrule
\multirow{3}{*}{RESCAL+SCE w / LS} & SCE        & 0.369 \\
                                   & SCE w/ BC  & 0.369 \\
                                   & SCE w/ LS  & 0.371 \\
\midrule
\multirow{3}{*}{RESCAL+SANS}       & NS         & 0.349 \\
                                   & NS w/ Freq & 0.348 \\
                                   & SANS       & 0.350 \\
\midrule
\multicolumn{3}{c}{\textbf{WN18RR}}                              \\
\midrule
Method                             & Pretrain   & MRR   \\
\midrule
\multirow{3}{*}{ComplEx+SCE w/ LS} & SCE        & 0.483 \\
                                   & SCE w/ BC  & 0.469 \\
                                   & SCE w/ LS  & 0.481 \\
\midrule
\multirow{3}{*}{RotatE+SANS}       & NS         & 0.472 \\
                                   & NS w/ Freq & 0.474 \\
                                   & SANS       & 0.475 \\
\bottomrule
\end{tabular}
\caption{The best MRR scores of models initialized with pre-trained embeddings on validation data.}
\label{tab:valid:pretrained}
\end{table}

\noindent\textbf{Device}: In all models, we used a single NVIDIA RTX2080Ti for training. Except for RotetE with SCE-based loss functions, all models finished the training in one day.
The RotetE with SCE-based loss function finished the training in at most one week.

\clearpage
\section{Note: the divergence between the NS and SCE loss functions}
\label{app:proof:bound}
\citet{8930624} proved that the upper bound of the Bregman divergence for binary labels when . However, to compare the SCE and NS loss functions in general, we need to consider the divergence of multi labels in SCE.
When , we can derive the following inequality by using the log sum inequality:

Eq. (\ref{eq:div:sce:multi}) shows that the divergence of multi labels is larger than that of binary labels in SCE.
As we explained,  of SCE is larger than  of NS in binary labels.
Therefore, the SCE loss imposes a larger penalty on the same predicted value than the NS loss when the value of the learning target is the same between the two losses.