

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage[ruled,vlined,noend]{algorithm2e}

\usepackage{rotating}
\usepackage{commath}
\usepackage{algorithmic}
\usepackage{adjustbox}
\usepackage{booktabs}



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{4984} \def\confYear{CVPR 2021}



\begin{document}

\title{Interpretable Visual Reasoning via Induced Symbolic Space}

\author{Zhonghao Wang, Mo Yu, Kai Wang,
Jinjun Xiong, \\Wen-mei Hwu, Mark Hasegawa-Johnson, Humphrey Shi\\
\\
{\small UIUC, IBM Research, University of Oregon}}

\maketitle

\vspace{-3mm}
\begin{abstract}


We study the problem of concept induction in visual reasoning, i.e., identifying concepts and their hierarchical relationships from question-answer pairs associated with images; and achieve an interpretable model via working on the induced symbolic concept space.
To this end, we first design a new framework named object-centric compositional attention model (OCCAM) to perform the visual reasoning task with object-level visual features. Then, we come up with a method to induce concepts of objects and relations using clues from the attention patterns between objects' visual features and question words.
Finally, we achieve a higher level of interpretability by imposing OCCAM on the objects represented in the induced symbolic concept space.
Experiments on the CLEVR dataset demonstrate: 1) our OCCAM achieves a new state of the art without human-annotated functional programs; 2) our induced concepts are both accurate and sufficient as OCCAM achieves an on-par performance on objects represented either in visual features or in the induced symbolic concept space.
Our code will be made available at \href{https://github.com/SHI-Labs/Interpretable-Visual-Reasoning}{https://github.com/SHI-Labs/Interpretable-Visual-Reasoning}.
\vspace{-3mm}
\end{abstract}

\section{Introduction}

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{./pics/teaser1.pdf}
\caption{Illustration of our framework. Our model induces the concepts and super concepts with the attention correlation between the objects and question words in image-question pairs as the paths shown in blue arrows. Then, it answers a question about an image via compositional reasoning on the induced symbolic representations of objects and object relations, shown as the orange paths.}
\label{teaser}
\vspace{-3mm}
\end{figure}

Recent advances in Visual Question Answering (VQA)~\cite{anderson2018bottom, perez2018film,hudson2018compositional, andreas2016neural, hu2017learning, johnson2017inferring, hu2018explainable, yi2018neural, mao2018neuro} usually rely on carefully designed neural attention models over images, and rely on pre-defined lists of concepts to enhance the compositional reasoning ability of the attention modules.
Human prior knowledge plays an essential role in the success of the model design.

We focus on a less-studied problem in this field -- given only question-answer pairs and images, whether it is possible to induce the visual concepts that are sufficient for completing the visual reasoning tasks. 
By sufficiency, we hope to maintain the predictive accuracy of the visual reasoning process, when using the induced concepts in place of the original visual features.
We consider concepts that are important for visual reasoning, including properties of objects (e.g., \emph{red}, \emph{cube}) and relations between objects (e.g., \emph{left}, \emph{front}).
The aforementioned scope and sufficiency criterion require accurately associating the induced symbols of concepts to both visual features and words that can express the meanings of concepts, so that each new instance of question-image pair can be transformed into the induced concept space for further computations.
Additionally, it is necessary to identify super concepts, i.e., hypernyms of concept subsets (e.g., \emph{shape}). The concepts inside a super concept are exclusive, so that the system knows each object can only possess one value in each subset.
This introduces structural information to the concept space (multiple one-hot vectors for each visual object) and further guarantees the accuracy of the aforementioned transformation.

The value of the study has two folds.
First, our proposed problem aims to identify visual concepts, their argument patterns (properties or relations) and their hierarchy (super concepts) \emph{without} using any concept-level supervision. Solving the problem frees both the efforts of human annotations and human designs of concept schema required in previous visual reasoning works. At the same time, the problem is technically more challenging compared to the related existing problem like unsupervised or weakly-supervised visual grounding~\cite{yeh2018unsupervised}.
Second, by constraining the visual reasoning models to work over the induced concepts, the ability of concept induction improves the interpretability of visual reasoning models.
Unlike previous interpretable visual reasoning models that rely on human-written rules to associate neural modules with given concept definitions~\cite{hu2018explainable,mao2018neuro,shi2019explainable}, our method resolves the concept definitions and associations interpretability automatically in the learning process, without the need of trading off for hand-crafted model designs.



We achieve the proposed challenge in three steps.
First, we propose a new model architecture, object-centric compositional attention model (OCCAM), that performs object-level visual reasoning instead of pixel-level by extracting object-level visual features with ResNet \cite{He_2016_CVPR} and pooling the features according to each object's bounding box.
The object-level reasoning not only improves over the state-of-the-art methods, but also provides a higher-level interpretability for concept association and identification.
In our second step, we benefit from this model's attention values over objects to create classifiers mapping visual objects to words; then derive the concepts and super concepts from the object-word cooccurrence matrices as shown in Figure \ref{teaser}.
Finally, our concept-based visual reasoning framework predicts the concepts of objects and object relations and performs compositional reasoning using the predicted symbolic concept embeddings instead of the original visual features as shown in Figure \ref{teaser}.





Experiments on the CLEVR dataset demonstrate that our overall approach improves the level of interpretability of neural visual reasoning models, and maintains the predictive accuracy: (1) our object-level visual reasoning model improves over the previous state-of-the-art methods; (2) our induced concepts and concept hierarchy is accurate in human study; and (3) our induced concepts are sufficient for visual reasoning -- replacing visual features with concepts leads to only 0.7\% performance drop.


\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{./pics/framework_refined.pdf}
\caption{The framework and the compositional reasoning module. The left graph shows the general framework; The phase 1 training path is drawn in purple and the phase 2 training paths are drawn in red. The black paths are shared for both training phases. The structures of our proposed object-level feature extractor, concept regression module and concept projection module are shown in Figures \ref{ext}, \ref{conc_regre} and \ref{conc_proj}.}
\label{framework}
\vspace{-3mm}
\end{figure*}

\section{Related Works}
\textbf{Visual Question Answering (VQA)} requires models to reason a question about an image to infer an answer. Recent works on this task can be partitioned into two groups: holistic models \cite{yang2016stacked, xu2016ask, anderson2018bottom, perez2018film, hudson2018compositional} and modular models \cite{andreas2016learning, andreas2016neural, hu2017learning, johnson2017inferring, hu2018explainable, yi2018neural, mao2018neuro}, according to whether the approach has explicit sub-task structures. A typical holistic model, MAC \cite{hudson2018compositional}, perform iterative reasoning steps with an attention mechanism on the image.  A modular framework, NS-CL \cite{mao2018neuro}, designs multiple principle functions over the extracted features to explain the reasoning process. 


\textbf{Model interpretability.} Plenty of previous works attempt to explain the decision rules of the learned models. For instance, Bau et al. \cite{bau2017network} proposed network dissection to quantify interpretability of CNNs. Zhang et al. \cite{zhang2019interpreting} explained a CNN prediction at the semantic level with decision trees. Shi et al. \cite{shi2019explainable} generated scene graphs from images to explicitly trace the reasoning-flow. 
Whereas some works \cite{park2016attentive, hu2018explainable} focused on visual attentions to provide enhanced interpretability. 
Our work is closely related to \emph{the self-explaining systems via rationalization}~\cite{lei2016rationalizing,chen2018learning,yu2019rethinking}. 
Different from previous works that extract subsets of inputs as explanations, our work moves one-step further by learning parts of the structural explanation definitions (i.e., the concepts hierarchy) together with explanations.

\textbf{Visual concept learning} contributes to many visual-linguistic applications, such as cross-modal retrieval \cite{kiros2014unifying}, visual captioning \cite{karpathy2015deep}, and visual-question answering \cite{malinowski2015ask, antol2015vqa}. Some recent papers \cite{yi2018neural, mao2018neuro} attempt to disentangle visual concept learning and reasoning. Based on the visual concepts learned from VQA, Han et al. \cite{han2019visual} learned metaconcepts, i.e., relational concepts about concepts, with augmented questions and answers. In contrast, our model learns super concepts without external knowledge.





\section{Object-Centric Compositional Reasoning}
In this section, we introduce a new neural architecture for neural reasoning.
Our model adopts an object-centric approach, which performs compositional reasoning over the extracted object-level visual features.
This proposed object-centric reasoning approach not only achieves state-of-the-art performance, but also plays a key role in the task of inducing object-wise or relational concepts as will be described in section \ref{sec:concept_induction}.

Figure \ref{framework} shows our general framework. It includes two training phases involving the process from feeding the input images and questions to attain the final answers.
Phase 1 (black-colored paths) corresponds to the training of our object-centric neural model, in which we train the object-level feature extractor, the compositional reasoning module and the question embedding LSTM; 
Phase 2 (\textcolor{red}{red}-colored paths) corresponds to the induction of symbolic concepts based on the aforementioned trained neural modules, as well as the training of a concept projection module so that the induced concepts can be accommodated in our reasoning pipeline. 
The figure shows the central role that the object-centric model plays in our framework.





In the following sections, we will first review the background of the compositional reasoning framework which was first proposed by Hudson and Manning \cite{hudson2018compositional} (section \ref{ssec:mac}); then, we will introduce the proposed object-level compositional attention network (section \ref{ssec:object_mac}). 


\subsection{Background on compositional reasoning}
\label{ssec:mac}
\paragraph{Notations} As shown in Figure \ref{framework}, we name the visual vectors as , the output memory vector from the compositional reasoning module as , the embedded word vectors for questions as , and the question embedding as . 
\vspace{-3mm}
\paragraph{Baseline visual reasoning framework} The original compositional reasoning framework \cite{hudson2018compositional} is similar to the phase 1 of our framework in Figure \ref{framework}, except that it works on pixel-level instead of object-level features. To generate , it feeds the image to a ResNet101 \cite{He_2016_CVPR} pretrained on ImageNet \cite{deng2009imagenet} and flatten the last feature maps across the width and height as . For the question inputs, we first convert each question word to its word embedding vector (), then input  to a bidirectional LSTM \cite{hochreiter1997long,graves2005framewise} to extract the question embedding vector . The compositional reasoning module takes ,  and  as inputs and performs multi-step reasoning to attain , the final step memory output. Finally, the classifier outputs the probability for each answer choice with a linear classifier over the concatenation of  and . The loss function can thus be written as:

 is the total number of image-question pairs,  is the one-hot ground truth vector,  is the classifier,  is the compositional module,  is the question embedding LSTM,  is the visual feature extractor and  is the image input.



\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{./pics/feature_extractor.pdf}
\caption{The structure of \textbf{object-level feature extractor}.}
\label{ext}
\vspace{-6mm}
\end{figure}
\vspace{-2mm}
\paragraph{The MAC reasoning module}
The aforementioned framework employs the MAC (i.e., \emph{Memory, Attention, and Composition}) cells~\cite{hudson2018compositional} as the compositional reasoning module.
This module processes visual and language inputs in a sequential way. Shown in Figure \ref{framework} (right), each MAC cell contains a control unit and an R/W (Read/Write) unit; the blue diagrams labeled with  stand for fully connected layers and the symbol  stands for Hadamard product.



At each step, the -th MAC cell receives the control signal  and the memory output from the previous step, , and outputs the new memory vector .
The control unit computes the single  to control reading of  in the R/W unit. Specifically, it computes the interactions among , , and each vector in  to produce the attention weights, and weighted averages  to produce . The control unit of each MAC cell has a unique question embedding projection layer, while all other layers are shared.
The R/W unit aims to read the useful  and store the read information into . It first computes the interactions among ,  and each vector in  to attain the attention weights, weighted averages  to produce a read vector , and finally computes the interaction of  and  to produce .  The weights of the R/W units are shared across all MAC cells. 
The initial control signal and memory  and  are learnable parameters. 






\subsection{Object-level compositional attention network}
\label{ssec:object_mac}
The object-level compositional attention network is shown in Figure \ref{framework} with phase 1 path, and optimizes Eqn (\ref{loss_fn}) with  generated by our object-level feature extractor. 
\vspace{-4mm}

\paragraph{Object-level feature extraction}
Fed with an image, the object-level feature extractor produces a set of vectors, each of which encodes both a single object's unary visual features and its interactions with other objects. The structure of this module is shown in Figure \ref{ext}. Following \cite{mao2018neuro}, we use Mask-RCNN \cite{he2017mask} to detect all objects in an image and output the bounding boxes for them. 
The image is fed to a ResNet34 network \cite{He_2016_CVPR} pretrained on ImageNet \cite{deng2009imagenet} to generate the feature maps in parallel.

On top of the ResNet34 feature maps, we apply a global average pooling to get a single \textbf{global feature vector} (the gray vector in the figure). We concatenate this global vector with the feature map at each location, followed by \textbf{three convolution layers}.
This global vector is crucial since it allows the visual features to encode the interaction among objects; and the three convolution layers {fuse} the local and global features into a single visual vector at each position.

Finally, to get object-level features from the above pixel-level fused features, we utilize RoI align \cite{he2017mask} to project the objects' bounding boxes onto the fused feature vectors to generate the RoI feature maps, and average pool the RoI feature maps for each object to produce the object-level .
\vspace{-3mm}


\section{Concept Induction and Reasoning}
\label{sec:concept_induction}

In this section, we describe how we achieve our goal of inducing symbolic concepts for objects and performing compositional reasoning on the induced concepts. 
We first formalize the problem of concept induction~(section \ref{ssec:concept_formulation}).
Then building on the learned object-centric reasoning model introduced in the previous section, we propose to induce concepts of both unary object properties or the binary relations between objects~(section \ref{ssec:concept_induction}). Finally we introduce how to achieve compositional reasoning over symbolic concepts by substituting the object-level features with the induced concepts (section \ref{ssec:concept_reasoning}). 

\subsection{Problem definition}
\label{ssec:concept_formulation}
We consider identifying three types of concepts: (1) the \textbf{unary concepts}  that are properties of objects (e.g., \emph{red}, \emph{cube}, etc.); (2) the \textbf{binary concepts}  that are relation descriptions between any two objects (e.g., \emph{left}, \emph{front} etc.); and (3) the \textbf{super concepts}  that are hypernyms of certain subsets of concepts (e.g., \emph{color}, \emph{shape}, etc.), subject to that each object can only possess one concept under each super concept, e.g., \emph{cube} and \emph{sphere}.

As questions refer to objects and describe object relations in images and, more importantly, include all the semantic information to reach an answer, it is natural to induce the concepts from question words. Therefore we assume that all the unary and binary concepts have their corresponding words; and these words are a subset of the nouns or adjectives from all the training questions. We denote the sets of words that describe unary concepts and binary concepts as  and  respectively. Therefore, the goal of concept induction consists of the following tasks:

\noindent \textbf{Visual mapping:} for each concept  or , learning a mapping from the visual feature  to . In other words, a prediction function  is learned to predict the existence of concept  from the visual feature  of an object.

\noindent \textbf{Word mapping:} for each concept  or , identifying a subset of words  or  that are synonyms representing the same concept, e.g., the concept of \emph{'cube'} corresponds to set of words .

\noindent \textbf{Super concept induction:} clustering of concepts to form super concepts. Each super concept  contains a set of concepts   or .





\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{./pics/conc_regre_refined.pdf}
\caption{The structure of the \textbf{concept regression module}.  and  are the object-level visual vectors representing two objects respectively, and  is the word vector.  is a fixed vector and  equals to  for the unary concept classifier.}
\label{conc_regre}
\vspace{-3mm}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{./pics/attn_vis_refined.pdf}
\caption{Attention visualization and attention logit distributions. (a) The attention visualization corresponding to the words describing the unary concepts by performing . Each of the words above the latter 4 images corresponds to a unique  and the value on each object is the attention logit (the same applies to (b)). (b) The attention visualization corresponding to the words describing the binary concepts by performing .  represents the object bounded by a red rectangle in the first image. (c) the attention logit distribution corresponding to each word describing a concept. }
\label{attn_vis}
\vspace{-3mm}
\end{figure*}

\subsection{Concept induction}
\label{ssec:concept_induction}
This section describes how we achieve the aforementioned tasks of concept induction. The key idea of our approach includes: (1) benefiting from the R/W unit from the trained MAC cells to achieve the visual mapping to textual words; (2) utilizing the inclusiveness of words' visual mapping to induce each concept's multiple word descriptions; (3) clustering super concepts from the mutual exclusiveness between concepts.
To achieve the above, we first train two binary classifiers that can determine if a word correctly describes an object's unique feature and if a word correctly describes a relation between two objects respectively. Then, with the help of these classifiers, we produce zero-one vectors for words that properly describe the unique features for each object and the relations between any pair of objects in single images across the dataset. Finally, we perform a clustering method on the word vectors to generalize unary and binary concepts, and the super concept sets.
\vspace{-3mm}

\paragraph{Visual mapping via regression from MAC cells}
The concept regression module is shown in Figure \ref{conc_regre}. It is composed of a classifier for the unary concept word regression, , and a classifier for the binary concept word regression, .  is expected to produce 1 if  can be described by the word vector . Likewise,  is expected to produce 1 if the relation of  to  can be described by the word vector  . 

We generate training data points  and  for  and  by utilizing the Read/Write unit (Figure \ref{framework}(right)) in the reasoning module after phase 1 training.
The whole generation process is described in Algorithm \ref{dp_gen}.
We denote  for the sequence of functions before the softmax operation in the Read unit and  for the function of the Write unit, where  is the set of objects in an image and  is the vector dimension.

\begin{algorithm}[t]
\SetAlgoLined
\small
 \caption{\small{Classifier data points generation. ST() splits a vector  to a set of  values. GMM() uses Gaussian Mixture Model to cluster a set of data points. FB() finds the decision boundary for the 2 Gaussian components.  is the indicator function.}}
 \label{dp_gen}
\KwResult{, }
 , \\
 \For{}{, } 
 \For{ DATASET}{
    \For{}{
         ST
    }
    \For{}{
        \For{}{
             ST
        }
    }
 }
 \For{}{ FBGMM}
 \For{ DATASET}{
     \For{}{
        \For{}{
            \\
            
        }
        \For{}{
            \For{}{
                \\
                
            }
        }
    }
 }
\end{algorithm}


Specifically, our algorithm first uses  and  to find the attention logits on the objects corresponding to words describing the unary and binary concepts in a question as shown in Figure \ref{attn_vis}(a\&b). 
We then use the values of logits to determine if the object possesses the concept of the word (positive) or not (negative).
Noticing the attention logit distribution of the sampled objects for each word is a two-peak distribution (Figure \ref{attn_vis}(c)), we use a GMM \cite{xuan2001algorithms} with two Gaussian components to model the distribution and find the decision boundary for each word's attention logit distribution. It is worth mentioning that the attention logit distribution for a unary concept word has two waves that do not interfere each other; while for a binary concept word the two waves interfere. 
This is because in some cases it is hard to tell if two objects have the relation described by a word. For example, it is hard to describe the \emph{``in front of''} relation between two objects on the same horizontal axis (e.g., the blue and yellow objects in Figure~\ref{attn_vis}(b)). Finally,  and  are generated by classifying the data points to positives and negatives with the decision boundaries. 

With the data points  and  generated, we can train  and  by minimizing the binary cross entropy loss.




\vspace{-3mm}
\paragraph{Binary coding of objects}
With trained  and , we represent an object  with a binary code vector. Each dimension corresponds a word. A dimension has value 1 if the corresponding word can describe  and 0 otherwise. The binary vectors of object properties and of the relations between two objects,  and  can be computed with the functions  and  respectively:

where  and  are the object-level visual vectors of  and ,  and  are the stacks of word embeddings in vocabulary  and .  performs elementwise on : return 1 if the element satisfies condition  or 0 otherwise. 

By applying  and  to all the objects and relations in the dataset, we can attain a matrix  and a matrix  as shown in Figure \ref{cls_vector}, where  and  are the total numbers of objects and co-occurred object pairs. The two matrices summarize each word's corresponding visual objects in the whole dataset.

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{./pics/cls_vector.pdf}
\caption{The zero-one matrices indicating word descriptions of objects and object relations across the dataset. (a) The matrix  indicates what words can describe objects. (b) The matrix  indicates what words can describe the relations object 's (bounded by green rectangles) are to object 's (bounded by red rectangles).}
\label{cls_vector}
\vspace{-3mm}
\end{figure}

\vspace{-3mm}
\paragraph{Concept/super-concept induction}
Finally, we group synonym words to unary and binary concepts and generate the super concepts. 
These two tasks are achieved via exploring the word inclusiveness and the concept exclusiveness captured by  and :
(1) words describing the same concept correspond to similar column vectors, e.g.,  and ;
(2) words describing exclusive concepts have column vectors that usually do not have 1 values on same objects simultaneously, e.g.,  and . 
Based on the aforementioned ideas, we define the correlation metric between two words  and  as below:

This guarantees that  for two synonym words,  for two words corresponding to exclusive concepts and  for words corresponding to different nonexclusive concepts. We can produce the correlation sets for the words describing the unary concepts and the binary concepts respectively with Eqn (\ref{corr_set}).


Our final step fits two GMM on  and  respectively. Each GMM has three components ,  and , with their mean values initialized with 0,1 and 2. 
We then induce the unary and binary concepts, where each concept consists of synonym words whose mutual correlation is clustered to the Gaussian component . 
Similarly, we induce the super concepts, where each super concept contains multiple concepts and any two words from different concepts have correlation clustered to the Gaussian component of .

We denote the set of words corresponding to a concept  as , the set of the super concept sets as , the set of all concepts as . Then, we can represent all the objects in an image with a unary concept matrix  and represent all the relations between any two objects in an image with a binary concept matrix  with Algorithm \ref{conc_gen}.

\begin{algorithm}[t]
\small
\SetAlgoLined
\KwResult{, }
 , \\
 \For{}{
    \For{}{
        MAX)\\
    }
    \For{}{
        HARDMAX\\
    }
    \For{}{
        \For{}{
            MAX\\
        }
        \For{}{
            HARDMAX\\
        }
    }
 }
 \caption{\small{Concept vector generalization. MAX returns the greatest value in the vector  and HARDMAX returns a zero-one vector indicating the position of the greatest value in the vector .}}
 \label{conc_gen}
\end{algorithm}

\subsection{Concept compositional reasoning}
\label{ssec:concept_reasoning}
\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{./pics/conc_proj_refined.pdf}
\caption{The structure of the \textbf{concept projection module}. We label the dimensions of matrices near them in the graph.}
\label{conc_proj}
\vspace{-3mm}
\end{figure}

Our ultimate goal is to perform compositional reasoning to answer a question with the generated concept representations  and  for an image; so as to confirm that our induced concepts are accurate and sufficient. We achieve this with the phase 2 training process in Figure \ref{framework}.
The key idea is to transplant the learned compositional reasoning module from manipulating the visual features to manipulating  and , for attaining the answer to a question.



To this end, first, we project  and  to the same vector space with  with the concept projection module shown in Figure \ref{conc_proj}, so that the compositional module can perform the reasoning steps on the projected concept vectors.
Specifically, we first reduce the dimension of  from  to , resulted in , because  can be understood as the relations to other objects for each object in an image. Then, we use two separate fully connected networks to project  and  respectively, concatenate and use a sequence of 1D convolution layers to project the results to the same dimension of 's. 

Second, to minimize the discrepancy between the distribution of our projected vectors and that of the original visual vectors , we fix the weights of other modules in the framework and only train the concept project module by optimizing the target function Eqn. (\ref{loss_fn}). Then, we train the concept projection module and the compositional reasoning module with other modules' weights fixed to better optimize Eqn. (\ref{loss_fn}). The result is a compositional reasoning model that works on the induced concepts only. 




\section{Experiment}
\subsection{Settings}
\paragraph{Dataset}
We use the CLEVR \cite{johnson2017clevr} dataset to evaluate our model. The dataset comprises images of synthetic objects of various shapes, colors, sizes and materials and question/answer pairs about these images. The questions require multi-hop reasoning, such as finding the transitive relations, counting numbers, comparing properties, to attain correct answers. 
Each question is provided a ground truth human-written programs.
Because the programs rely on pre-defined concepts thus do not fit our problem, we let our framework learn the compositional reasoning by itself \emph{without} using the program annotations.
There are 70k images and 700k questions in the training set; 15k images and 150k questions in the validation set. We follow the previous works \cite{yi2018neural,hudson2018compositional,mao2018neuro} to train our model on the whole training set and test our model on the validation set. 
\paragraph{Training details}
We set the hidden dimension  to in all modules. We follow \cite{hudson2018compositional} to design the question embedding module, the compositional module and the classifier. For the object-level feature extracter, we make the backbone ResNet34 learnable and zero-pad the output  to 12 vectors in total for any image. Notice that the maximum number of objects in an image is 11, so that the reasoning module is able to read nothing into the memory for some steps. For the concept projection module, to cover the full view of , the conv1D consists of five 1D convolution layers with kernel sizes (7,5,5,5,5),
each followed by a Batch Norm layer \cite{ioffe2015batch} and an ELU activation layer \cite{clevert2015fast}. 

We use Adam optimizer \cite{kingma2014adam} with momentum 0.9 and 0.999. Phase 1 and phase 2 share a same training schedule: the learning rate is initiated with  for the first 20 epochs and is halved every 5 epochs afterwards until stopped at the 40th epoch. We train the concept regression module separately with learning rate of  for 6 epochs. Phase 1 uses 4 Nvidia V100 GPUs with batch size 256. The other training processes use a single GPU with a batch size of 192.

\subsection{Object-level reasoning}
\begin{table}[t!]
    \centering
    \caption{The comparison of our object-level compositional reasoning framework to the state-of-the-art methods. * indicates the method uses external program annotations.}
    \small
    \begin{tabular}{l|c|p{0.45cm}p{0.45cm}p{0.53cm}p{0.53cm}p{0.53cm}}
        \toprule
         method & overall & count & exist & comp numb & query attr & comp attr  \\
         \midrule
         Human \cite{johnson2017inferring} & 92.6 & 86.7 & 96.6 & 86.5 & 95.0 & 96.0 \\
         \midrule
NMN* \cite{andreas2016neural} & 72.1 & 52.5 & 72.7 & 79.3 & 79.0 & 78.0 \\
         N2NMN* \cite{hu2017learning} &83.7 & 68.5 & 85.7 & 84.9 & 90.0 & 88.7 \\ 
         IEP* \cite{johnson2017inferring} & 96.9 & 92.7 & 97.1 & 98.7 & 98.1 & 98.9 \\
         TbD* \cite{Mascharka_2018_CVPR} & 99.1 & 97.6 & 99.4 & 99.2 & 99.5 & 99.6 \\
         NS-VQA* \cite{yi2018neural} & \textbf{99.8} & \textbf{99.7} & \textbf{99.9} & \textbf{99.9} & \textbf{99.8} & \textbf{99.8}\\ 
         \midrule
         RN \cite{santoro2017simple} & 95.5 & 90.1 & 93.6 & 97.8 & 97.1 & 97.9 \\
         FiLM \cite{perez2018film} & 97.6 & 94.5 & 93.8 & 99.2 & 99.2 & 99.0 \\
         MAC \cite{hudson2018compositional} & 98.9 & 97.2 & 99.4 & \textbf{99.5} & 99.3 & 99.5 \\
         NS-CL \cite{mao2018neuro} & 98.9 & \textbf{98.2} & 99.0 & 98.8 & 99.3 & 99.1\\
         \midrule
         OCCAM (ours) & \textbf{99.4}&98.1&\textbf{99.8}&99.0&\textbf{99.9}&\textbf{99.9}\\
         \bottomrule
    \end{tabular}
    \label{sota_comp}
    \vspace{-1mm}
\end{table} \begin{table}[t!]
\small
    \centering
    \caption{\small{The ablation study on the choice of reasoning steps for the object-level compositional reasoning.}}
    \begin{tabular}{l|cccc}
        \toprule
         steps & 4 & 8 & 12 & 16  \\
         \midrule
         accuracy & 94.3 & 98.6 & \textbf{99.4} & 99.1 \\
         \bottomrule
    \end{tabular}
    \label{ablation}
    \vspace{-3mm}
\end{table} We first perform the end-to-end phase 1 training shown in Figure \ref{framework}. The performance comparison of our model to the state-of-the-art models is shown in Table \ref{sota_comp}. Under the condition that no external human-labeled programs are used, our model achieves 99.4\% in accuracy, a new state-of-the-art performance on CLEVR validation set compared to other methods under the same condition. Our model also has an on-par performance with the best model \cite{yi2018neural} using external human-labeled programs. Compared to the original MAC \cite{hudson2018compositional} framework which uses image-level attentions and achieves 98.9\% in accuracy on CLEVR validation set, our model proves that the constraint of attentions on the objects are useful for improving the performance. We do not use the position embedding to explicitly encode the positions of objects for relational reasoning; however, we use the global features to enhance the model's understanding of inter-object relations. This implicates that the relations among objects are totally learnable concepts without external knowledge for the deep network. 

Table \ref{ablation} further gives an ablation study on the numbers of reasoning steps, i.e., the number of MAC modules, for our model. The reasoning model with 4 steps has a performance gap to the models with 8, 12 or 16 steps, while the latter three models have on-par performances. We conjecture that the model with low reasoning steps may not be able to capture multiple hops of a question and the model performance converges with an increasing number of reasoning steps.

\subsection{Concept induction and reasoning}

\paragraph{Results of concept induction}
To achieve the balance of the performance and the interpretability, we choose the object-level compositional reasoning model with 8 reasoning steps for the concept induction and reasoning. After visual mapping, binary coding and concept/super-concept induction described in section \ref{ssec:concept_induction}, the unary concepts and super concepts are induced as shown in Figure \ref{conc_cls}; the binary concepts are 'left', 'right', 'front' and 'behind', and \{'left', 'right'\} and \{'front', 'behind'\} form two super concept sets.
Appendix~\ref{app:concept_correlation} provides more detailed information on how these clusters are generated by presenting the concept correlations. 
The generated concept hierarchy perfectly recovers the definition in CLEVR data generator and matches human prior knowledge, showing the success of our approach.
\vspace{-3mm}
\paragraph{Concept-level reasoning}
For each image in the dataset, we can now represent each object and the relations between any two objects with the induced unary and binary concepts. With the method described in section \ref{ssec:concept_reasoning}, we project the concepts back to the visual feature space, so that the compositional reasoning module can perform the reasoning steps on the projected vectors(). Our concept compositional reasoning model achieves an on-par performance with the one of the object-level compositional reasoning model as shown in Table \ref{obj_conc_comp}. We also visualize the reasoning steps for the concept reasoning module as shown in Appendix \ref{app:visualization}.

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{./pics/concept_cls.pdf}
\caption{Concepts and super concept sets. Each circle represents a concept described by the words in that circle. A super concept set comprises the concepts represented by circles of the same color.}
\label{conc_cls}
\vspace{-3mm}
\end{figure}



\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{./pics/ops_mainpaper.pdf}
\caption{An multi-modal analogy example enabled by our results.}
\label{ops_main}
\vspace{-1mm}
\end{figure}

\begin{table}[t!]
    \centering
    \caption{\small{The comparison of our object-level compositional reasoning framework and our concept compositional reasoning framework. The number of reasoning steps is set to 8.}}
    \small
    \begin{tabular}{l|c|p{0.45cm}p{0.45cm}p{0.53cm}p{0.53cm}p{0.53cm}}
        \toprule
         method & overall & count & exist & comp numb & query attr & comp attr  \\
         \midrule
         object & 98.6 & 95.9 & 99.8 & 96.2 & 99.8 & 99.7 \\
         
         concept & 97.9&95.6&98.7&97.3&98.4&99.3\\
         \bottomrule
    \end{tabular}
    \label{obj_conc_comp}
    \vspace{-3mm}
\end{table} 





\vspace{-3mm}
\paragraph{Discussion}
Our concept induction results bridge the visual and symbolic spaces. The results enable to extend word analogy~\cite{mikolov2013distributed} (e.g., ``Madrid'' - ``Spain'' + ``France''  ``Paris'') into the multi-modality setting.
Figure~\ref{ops_main} gives an example, starting with the initial object  and its predicted concepts , subtracting concepts  and adding new concepts  result in a new concept set  (Figure~\ref{ops_main} (bottom)). Then if we retrieve visual object  with each concept set  along the path (Figure~\ref{ops_main} (top)), we have  in the original visual feature space.
Similarly, with the exclusiveness inside a super concept, we can quantify the distance between two visual objects with the super concept space.
More details are provided in Appendix \ref{app:concept_dis}.

For future works, our method can be extended to more sophisticated induction tasks, such as inducing concepts from phrases, with more complicated hierarchy, with degrees of features (e.g., \emph{dark blue}, \emph{light blue}) and inducing complicated relations between objects (e.g. \emph{a little bigger}).




\section{Conclusions}
Our proposed OCCAM framework performs pure object-level reasoning and achieves a new state-of-the-art without human-annotated functional programs on the CLEVR dataset.
Our framework makes the object-word cooccurrence information avaiable, which enables induction of the concepts and super concepts based on the inclusiveness and the mutual exclusiveness of words' visual mappings.
When working on concepts instead of visual features, OCCAM achieves comparable performance, proving the accuracy and sufficiency of the induced concepts. 



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\appendix
\clearpage

\section{Unary concept correlations }
\label{app:concept_correlation}

We present the unary concept correlations  in Figure \ref{conc_corr}. 
The correlation between any pair of synonyms is close to 0, the correlation between words belonging to the same super concept set is close to 2, and the correlation between words belonging to two different super concept sets is in the middle of the range. Therefore, a GMM with three components whose mean values are initialized with 0,1 and 2 can successfully generate the concepts and super concept sets. 

\begin{figure}[h]
\centering
\includegraphics[width=0.49\textwidth]{./pics/conc_corr.pdf}
\caption{The unary concept correlations .}
\label{conc_corr}
\vspace{-1mm}
\end{figure}

\section{Derivation from the concept interpretation}
\label{app:concept_dis}

\label{app:visualization}
\begin{figure}[h]
\centering
\includegraphics[width=0.49\textwidth]{./pics/conc_dis.pdf}
\caption{Illustration of the semantic distance.}
\label{conc_dis}
\end{figure}

With the induced concepts and super concept sets, each object can be represented with a zero-one vector, , where the entry is 1 if that object possesses the corresponding concept or 0 otherwise. Notice that the super concept sets split the whole concept set; we thereby name the entries of  corresponding to one super concept set as a super concept. The super concept is thus a zero-one vector with exactly one entry to be 1. We name this pattern as the super concept constraint. Therefore, we can define the semantic distance between two visual objects by the number of different super concepts or by Eqn. (\ref{dis}). 

where  and  are the concept vectors representing two objects and  is the operation XOR. Studying the concepts and super concept sets induced, we acknowledge that the super concept sets correspond to color, shape, size and material in semantics. Thereby, we give an example of the semantic distances of multiple objects to one object as shown in Figure \ref{conc_dis}. The circle radii indicate the semantic distances to the object at the centers of these circles. The inner three circles are segmented so that each segment represents what super concepts are different. The outer circle represents all the 4 super concepts are different between the object on that circle and the object at the center.

\begin{figure}
\centering
\includegraphics[width=0.49\textwidth]{./pics/semantic_vis.pdf}
\caption{The original images for extracting visual features. The object-level features corresponding to the objects bounded by red rectangles are used for the illustration of semantic operations in the visual feature space. }
\label{semantic_vis}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{./pics/visual_angle.pdf}
\caption{Illustration of the semantic analogy in the visual feature space. (a) The operations on the visual features. (b) The cosine similarities between pairs of visual feature vectors. }
\label{visual_angle}
\end{figure}

We can further interpret the semantic analogy in the visual feature space with the induced concept vectors. Shown in Figure \ref{semantic_vis}, we first generate four images of different objects; then, we use our trained OCCAM structure to extract the object-level features corresponding to the objects bounded by red rectangles. Shown in Figure \ref{visual_angle}(a), we can move the visual feature vector of the leftmost object closer to that of the rightmost object by subtracting and adding visual feature vectors of two other objects. The proximity between pairs of visual feature vectors is measured with cosine similarity as shown in Figure \ref{visual_angle}(b). In the concept vector space, we can define a 'minus' operation, , as eliminate the shared super concepts between  and  from . We can also define a 'plus' operation, , between a concept vector template  and a concept vector  as add the super concepts of  that  misses to . Therefore, The operations in the visual feature space can be explained with the operations we defined in the concept vector space shown in Figure \ref{vis_op}.




\begin{figure}
\centering
\includegraphics[width=0.49\textwidth]{./pics/vis_op.pdf}
\caption{Operations on the concept vectors.}
\label{vis_op}
\end{figure}

\section{Visualization of reasoning steps}

\begin{figure*}
\centering
\includegraphics[width=0.89\textwidth]{./pics/steps.pdf}
\caption{Visualization of reasoning steps. (a) The question, image, prediction and ground truth answer. The index of each object is shown on the upper left of the object. (b) The induced concepts of objects and relations. (c) The stepwise attentions on question words. (d) The stepwise attentions on objects. (e) The concept vector read into the memory of the reasoning module in each step.}
\label{steps}
\end{figure*}

We give an example of the compositional reasoning steps on the induced concept space of OCCAM as shown in Figure \ref{steps}. While the attention is directly imposed on the projected concept vectors in the read unit of the compositional reasoning module, the attention can be equally mapped to the concept vectors and the visual objects as the projected concept vector to the concept vector or the projected concept vector to the visual object is a one-to-one mapping relationship.

\end{document}
