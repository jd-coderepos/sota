
\section{Experiments and Analysis}
\label{sec:exp}
In this section, we first introduce our implementation details and evaluation metrics.
We then evaluate our method against the state-of-the-art VSR models on benchmark datasets.
In addition, we demonstrate that our method performs better 
than a baseline method based on a denoiser and a VSR model.
We also evaluate the COMISR model on real-world compressed YouTube videos.
Finally, we show ablation on the three novel modules with analysis, and user study results.
\subsection{Implementation Details}
\label{impldetails}

\paragraph{Datasets.} 
We use the REDS~\cite{Nah_2019_CVPR_Workshops_REDS} and Vimeo~\cite{xue2019video} datasets for training. 
The REDS dataset contains more than 200 video sequences for training, each of which has 100 frames with  resolution.
The Vimeo-90K dataset contains about 65k video sequences for training, each of which has 7 frames with  resolution.
One main difference between these two datasets is the REDS dataset contains images with much larger motion 
captured from a hand-held device.
To train and evaluate the COMISR model, the frames are first smoothed by a Gaussian kernel with the width of 1.5 and downsampled by a factor of .

We evaluate the COMISR model on the Vid4~\cite{vid4} and REDS4~\cite{Nah_2019_CVPR_Workshops_REDS} datasets (clip\# 000, 011, 015, 020). 
All the testing sequences contain more than 30 frames.  In the following experiments, the COMISR model evaluated on the REDS4 dataset is trained with the REDS dataset using the same setting described in~\cite{edvr}. 
The COMISR model in all the other experiments is trained using the Viemo-90K.

\vspace{-4mm}
\paragraph{Compression methods.} We use the most common setting for the H.264 codec at different compression rates (i.e., different CRF values).  
The recommended CRF value is between 18 and 28, and the default is 23 (although the CRF value ranges between 0 and 51).
In our experiments, we use CRF of 15, 25, and 35 to evaluate video super-resolution with a wide range of compression rates. 
For fair comparisons, when evaluating other methods, we use the same degradation method to generate the LR sequences before compression.  
Finally, these compressed LR sequences are fed into the VSR models for inference.

\vspace{-4mm}
\paragraph{Training process.} For each video frame, we randomly crop  patches from a mini-batch as input.
Each mini-batch consists of 16 samples. 
The , , and  parameters described in Section~\ref{method} are set to 1, 20, 1, respectively.
The model trained with the loss functions described in the Section~\ref{loss_function}. 
We use the Adam optimizer~\cite{adam_opt} with  and . 
The learning rate is set to .
While we aim to train the COMISR model for VSR with compressed videos as input, we first feed uncompressed images to the model, and 
during the last  of the training epochs, 
we randomly add compressed images in the training process with a probability of 50\%.  
The FFmpeg codec is employed for compression with a CRF value randomly selected between 15 and 25.
All the models were trained on 8 NVidia Tesla V100 GPUs.
More details can be found on the project website. 


\vspace{-4mm}
\paragraph{Evaluation metrics.} 
We use PSNR, SSIM, and LPIPS~\cite{zhang2018perceptual} for quantitative evaluation of video super-resolution results. 
For the experiments on YouTube videos, we only present video SR results for evaluation since the ground-truth frames are not available.  


\subsection{Evaluation against the State-of-the-Arts}
\label{compsota}


\begin{figure*}[!htb]
\centering
\includegraphics[width=1.0\linewidth]{figures/comparison_sota_vid4_comisr.pdf}
\caption{Qualitative evaluation on the Vid4 dataset for  VSR.
The COMISR model can recover more structure details such as faces and boundaries, with much fewer artifacts. Zoom in for best view.
}
\label{fig:vid4_sota}
\end{figure*}

\begin{figure*}[!htb]
\centering
\includegraphics[width=1.0\linewidth]{figures/compare_sota_reds4_comisr.pdf}
\caption{Qualitative results on videos from the REDS4 dataset  VSR.
The COMISR model achieves much better quality on detailed textures, with much fewer artifacts. 
The brightness of the images is adjusted for viewing purposes. 
Zoom in for best view.}
\vspace{-0.3cm}
\label{fig:reds4_sota}
\end{figure*}


\begin{table*}[tp]
\centering
\resizebox{2.1\columnwidth}{!}{
\begin{tabular}{l||ccccc||cccc}
& FLOPs & \multicolumn{4}{c||}{CRF 25} &  No compression & \multicolumn{3}{c}{Compressed Results} \\
 Model & \#Param. & calendar  & city  & foliage & walk  & - & CRF15 & CRF25 & CRF35  \\ \toprule [0.2em]
 FRVSR~\cite{frvsr} & \makecell{0.05T\\2.53M} &\makecell{21.55 / {0.631} \\ 19.75 / {0.606}} & \makecell{25.40 / {0.575} \\ 23.79 / {0.572}}  & \makecell{24.11 / {0.625} \\ 24.49 / {0.751}}  & \makecell{26.21 / {0.764} \\ 25.22 / {0.815}}& \makecell{26.71 / {0.820} \\ 25.22 / {0.815}} & \makecell{26.01 / {0.766} \\ 24.38 / {0.753}} & \makecell{24.33 / {0.655} \\ 22.59 / {0.640} }& \makecell{22.05 / {0.482} \\20.35 / {0.469}} \\ \hline
 DUF~\cite{duf}& \makecell{0.62T\\5.82M} & \makecell{21.16 / 0.634 \\ 19.40 / 0.588} & \makecell{23.78 / 0.632 \\ 22.25 / 0.594} & \makecell{22.97 / 0.603 \\ 21.30 / 0.567} & \makecell{24.33 / 0.771 \\ 22.66 / 0.737} & \makecell{27.33 / 0.832 \\ 25.79 / 0.814} & \makecell{24.40 / 0.773 \\ 22.81 / 0.744} & \makecell{23.06 / 0.660 \\ 21.41 / 0.621} & \makecell{21.27 / 0.515 \\ 19.61 / 0.468} \\ \hline
 EDVR~\cite{edvr}& \makecell{0.93T\\20.6M} & \makecell{21.69 / 0.648 \\ 19.87 / 0.599} & \makecell{25.51 / 0.626 \\ 23.90 / 0.586} & \makecell{24.01 / 0.606 \\ 22.27 / 0.570} & \makecell{26.72 / 0.786 \\ 24.89 / 0.754} & \makecell{27.35 / 0.826\\25.85 / 0.808} & \makecell{26.34 / 0.771\\24.67 / 0.740} & \makecell{24.45 / 0.667\\22.73 / 0.627} & \makecell{22.31 / 0.534\\20.62 / 0.487}  \\ \hline
 TecoGan~\cite{tecogan2020}& \makecell{0.14T\\5.05M} & \makecell{21.34 / 0.624\\19.55 / 0.601} & \makecell{25.26 / 0.561\\23.65 / 0.559} & \makecell{23.50 / 0.592\\21.73 / 0.573} & \makecell{25.73 / 0.756\\24.40 / 0.743} & \makecell{25.88 / 0.794\\24.34 / 0.788} & \makecell{25.25 / 0.741\\23.61 / 0.728} & \makecell{23.94 / 0.639\\22.22 / 0.624} & \makecell{21.99 / 0.479\\20.28 / 0.466}     \\ \hline
 MuCAN~\cite{mucan}& - & \makecell{21.60 / 0.643\\19.81 / 0.597} & \makecell{25.38 / 0.620\\23.78 / 0.581} & \makecell{23.93 / 0.599\\22.20 / 0.564} & \makecell{26.43 / 0.782\\24.72 / 0.750} & \makecell{27.26 / 0.822\\25.56 / 0.801} & \makecell{25.85 / 0.753\\24.22 / 0.725} & \makecell{24.34 / 0.661\\22.63 / 0.623} & \makecell{22.26 / 0.531\\20.57 / 0.485}  \\ \hline
 RSDN~\cite{rsdn}& \makecell{0.13T\\6.19M} & \makecell{21.72 / 0.650\\19.89 / 0.599} & \makecell{25.28 / 0.615\\23.68 / 0.575} & \makecell{23.69 / 0.591\\21.94 / 0.554} & \makecell{25.57 / 0.747\\23.91 / 0.711} & \makecell{\colorbox{shadecolor}{\bf{27.92 / 0.851}}\\26.43 / 0.835} & \makecell{\colorbox{shadecolor}{\bf{26.58 / 0.781}}\\24.88 / 0.750} & \makecell{24.06 / 0.650\\22.36 / 0.610} & \makecell{21.29 / 0.483\\19.67 / 0.437}   \\ \toprule [0.2em]
 COMISR & \makecell{0.06T \\ 2.63M} & \makecell{\colorbox{shadecolor}{\bf{22.81 / 0.695}}\\20.39 / 0.667} & \makecell{\colorbox{shadecolor}{\bf{25.94 / 0.640}}\\24.30 / 0.633} & \makecell{\colorbox{shadecolor}{\bf{24.66 / 0.656}}\\22.88 / 0.638} & \makecell{\colorbox{shadecolor}{\bf{26.95 / 0.799}}\\25.21 / 0.788} & \makecell{27.31 / 0.840\\25.79 / 0.835} & \makecell{26.43 / 0.791\\24.76 / 0.778} & \makecell{\colorbox{shadecolor}{\bf{24.97 / 0.701}}\\23.21 / 0.686} & \makecell{\colorbox{shadecolor}{\bf{22.35 / 0.509}}\\20.66 / 0.494}   \\ \hline
\end{tabular}
}
\vspace{1mm}
\caption{Performance evaluation on compressed Vid4 videos. 
For each entry, the first row is PSNR/SSIM on Y channel, and the second row is PSNR/SSIM on RGB channels. 
The best method on the Y channel for each column is highlighted in bold and shade. 
The FLOPs are reported based on the Vid4  VSR. The FLOPs and \#Param of FRVSR is based on our implementation.}
\label{tbl:vid4_compare}
\vspace{-2mm}
\end{table*}





\begin{table*}[tp]
\centering
\resizebox{2.1\columnwidth}{!}{
\begin{tabular}{l||ccccc||cccc}
&  & \multicolumn{4}{c||}{CRF 25} &  No compression & \multicolumn{3}{c}{Compressed Results} \\ 
 Model & \#Frame & clip\_000  & clip\_011  & clip\_015 & clip\_020  & - & CRF15 & CRF25 & CRF35  \\ \toprule [0.2em]
 FRVSR~\cite{frvsr} & recur(2) & 24.25 / 0.631 & 25.65 / 0.687  & 28.17 / 0.770 & 24.79 / 0.694 & 28.55 / 0.838 & 27.61 / 0.784 & 25.72 / 0.696& 23.22 / 0.579 \\ 
 DUF~\cite{duf}& 7 & 23.46 / 0.622 & 24.02 / 0.686 & 25.76 / 0.773 & 23.54 / 0.689 & 28.63 / 0.825 & 25.61 / 0.775 & 24.19 / 0.692 & 22.17 / 0.588 \\
 EDVR~\cite{edvr}& 7 & 24.38 / 0.629 & 26.01 / 0.702 & 28.30 / 0.783 & 25.21 / 0.708 & \colorbox{shadecolor}{\bf{31.08 / 0.880}} & \colorbox{shadecolor}{\bf{28.72 / 0.805}} & 25.98 / 0.706 & 23.36 / 0.600  \\ 
 TecoGan~\cite{tecogan2020}& recur(2) & 24.01 / 0.624 & 25.39 / 0.682 & 27.95 / 0.768 & 24.48 / 0.686 & 27.63 / 0.815 & 26.93 / 0.768 & 25.46 / 0.690 & 22.95 / 0.589     \\ 
 MuCAN~\cite{mucan}& 5 & 24.39 / 0.628 & 26.02 / 0.702 & 28.25 / 0.781 & 25.17 / 0.707 & 30.88 / 0.875 & 28.67 / 0.804 & 25.96 / 0.705 & 23.55 / 0.600  \\
 RSDN~\cite{rsdn}& recur(2) & 24.04 / 0.602 & 25.40 / 0.673 & 27.93 / 0.766 & 24.54 / 0.676 & 29.11 / 0.837 & 27.66 / 0.768 & 25.48 / 0.679 & 23.03 / 0.579   \\ \toprule [0.2em]
 
 COMISR & recur(2) & \colorbox{shadecolor}{\bf{24.76 / 0.660}} & \colorbox{shadecolor}{\bf{26.54 / 0.722}} & \colorbox{shadecolor}{\bf{29.14 / 0.805}} & \colorbox{shadecolor}{\bf{25.44 / 0.724}} & 29.68 / 0.868 & 28.40 / 0.809 & \colorbox{shadecolor}{\bf{26.47 / 0.728}} & \colorbox{shadecolor}{\bf{23.56 / 0.599}}  \\ \hline
\end{tabular}
}
\vspace{1mm}
\caption{Performance evaluation on compressed the REDS4 dataset. 
Each entry shows the PSNR/SSIM on RGB channels.
The best method for each column is highlighted in bold and shade, and recur(2) indicates a recurrent network using 2 frames.}
\label{tbl:reds4_compare}
\vspace{-2mm}
\end{table*}

We evaluate the COMISR model against state-of-the-art VSR methods, including FRVSR~\cite{frvsr}, DUF~\cite{duf}, EDVR~\cite{edvr}, TecoGan~\cite{tecogan2020}, MuCAN~\cite{mucan}, and RSDN~\cite{rsdn}.
Three of the evaluated methods are based on recurrent models, whereas the other three use temporal sliding windows (between 5 and 7 frames).
When available, we use the original code and trained models, and otherwise implement these methods.
For fair comparisons, the LR frames have been generated the same as described in the published work. 
These LR frames are then compressed and fed into the super-resolution networks for performance evaluation.

For the Vid4 dataset~\cite{vid4}, the PSNR and SSIM metrics are measured on both the Y-channel and RGB-channels, as shown in Table~\ref{tbl:vid4_compare}. 
We present the averaged performance on uncompressed videos (original sequences), and videos compressed at different levels (CRF15, 25, 35).
We also report the individual sequence performance under CRF25.
More results on other CRF factors are presented in the supplementary material. 
Overall, the COMISR method outperforms all the other methods on videos with medium to high compression rates by 0.5-1.0db in terms of PSNR. 
Meanwhile, our method performs well (2nd or 3rd place) in less compressed videos.
Figure~\ref{fig:vid4_sota} shows some results by the evaluated methods from two sequences. 
The COMISR model can recover more details from the LR frames with fewer compression artifacts.
Both quantitative and visual results show that the COMISR method achieves the state-of-the-art results on compressed videos. 

We also evaluate the COMISR model against the state-of-the-art methods on the REDS4 dataset~\cite{Nah_2019_CVPR_Workshops_REDS}.
Unlike the Vid4 dataset, the sequences in this set are longer (100 frames) and more challenging with larger movements between frames. 
Table~\ref{tbl:reds4_compare} shows the COMISR model achieves the best performance on the compressed videos from the REDS4 dataset.
Figure~\ref{fig:reds4_sota} shows that 
our method is able to recover more details such as textures from the bricks on the sidewalk and windows on the buildings.

It is known that low-level structure accuracy (e.g., PSNR or SSIM) does not necessarily correlate well with high-level 
perceptual quality.
In other words, perceptual distortion cannot be well characterized by such low-level structure accuracy~\cite{blau2018}.
We also use the LPIPS~\cite{zhang2018perceptual} for performance evaluation. 
Table~\ref{tbl:lpips} shows the evaluation results using the LPIPS metric on both Vid4 and REDS4 datasets. 
Overall, the COMISR model performs well against the state-of-the-art methods on both datasets using the LPIPS metric.



\begin{table}[tp]
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|ccccccc}
     & FRVSR  & TecoGan  & DUF & EDVR & MuCAN & RSDN & COMISR  \\ \toprule [0.2em]
 Vid4& 4.105&\colorbox{shadecolor}{\bf{3.245}}&4.010&4.396&3.985&4.292&3.689  \\
 REDS4 & 4.188&3.643&4.223&4.075&4.085&4.423&\colorbox{shadecolor}{\bf{3.384}} \\
\end{tabular}
}
\vspace{1mm}
\caption{Performance evaluation using the LPIPS~\cite{zhang2018perceptual} metric (lower is better). Our method performs well, especially on the more challenging REDS4 dataset. 
}
\label{tbl:lpips}
\vspace{-2mm}
\end{table}

We show video super-resolution results on the project website. 
Although the compression artifacts are not easily observable in the LR frames, 
such artifacts are amplified and easily observed after super-resolution.
For the compressed videos, the COMISR model
effectively recovers more details from the input videos with fewer artifacts.
The COMISR model does not perform well on highly compressed (e.g., CRF35) videos. 
Some failure cases are due to heavy compression so that necessary details are missing for super-resolving frames.
Other failure cases are caused by extremely large movements 
in the videos.

\begin{figure*}[!htb]
\centering
\includegraphics[width=1.0\linewidth]{figures/REDS_360P_comisr.pdf}
\caption{ VSR results on REDS4 videos downloaded from YouTube with resolution of 360 pixels. Zoom in for best view.}
\label{fig:reds4_360p}
\vspace{-0.3cm}
\end{figure*}

\subsection{VSR on Denoised Videos}
\label{denosiervsr}
As shown in Figure~\ref{fig:vid4_sota} and Figure~\ref{fig:reds4_sota}, the COMISR model generates high-quality frames with fewer artifacts from compressed videos. 
An interesting question is whether the state-of-the-art methods can achieve better results if the compressed videos are first denoised. 
As such, we use the state-of-the-art compressed video quality method, STDF~\cite{stdf}, for evaluation.

Using the settings described in Section~\ref{compsota}, we compress video frames with CRF25.
The STDF method is then used to remove the compression artifacts and generate enhanced LR frames as inputs for the state-of-the-art VSR methods.  
Table~\ref{tbl:denoiser_vsr} shows
the quantitative results by the COMISR model and the state-of-the-art VSR methods on videos denosied by the STDF scheme.
We note that the performance of all of the evaluated method drops on the denoised LR frames.
This can be attributed to that a separate denoising step is not compatible with the learned degradation kernel from the VSR methods.
In addition, as discussed in Section~\ref{modelablation}, simply using compressed images for model training does not lead to good VSR performance. 
These results show that the COMISR model is able to efficiently recover more details from compressed videos, and outperforms state-of-the-art models on denoised videos.




\begin{table}[tp]
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|cccc}
            & \multicolumn{2}{c}{VSR only} & \multicolumn{2}{c}{Video Denoiser + VSR} \\
 Model &     Y-Channel      &   RGB-Channels       &   Y-Channel        &   RGB-Channels       \\ \toprule [0.2em]
 EDVR  &    24.45 / 0.667   &  22.73 / 0.627        &  22.56 / 0.581         &   20.94 / 0.541       \\
 TecoGan &  23.94 / 0.639 & 22.22 / 0.624 & 22.25 / 0.541 & 20.63 / 0.530         \\
 MuCan &   24.34 / 0.661 & 22.63 / 0.623 & 22.47 / 0.577 & 20.87 / 0.538        \\
 RSDN &  24.06 / 0.650 & 22.36 / 0.610 & 22.19 / 0.560 & 20.59 / 0.520  \\ \hline  
 COMISR &  \colorbox{shadecolor}{\bf{24.97 / 0.701}} & \colorbox{shadecolor}{\bf{23.21 / 0.686}} &  - & -  \\ \hline  
\end{tabular}
}
\vspace{1mm}

\caption{Ablation study on applying a video denoiser to the compressed frames before the VSR models using the Vid4 dataset.
Each entry shows the PSNR/SSIM results on the Y or RGB channel.
The COMISR model outperforms the state-of-the-art VSR methods with the STDF~\cite{stdf} denoiser.
}
\vspace{-2mm}
\label{tbl:denoiser_vsr}
\end{table}



\subsection{Evaluation on Real-World Compressed Videos}
\label{youtuberesults}
Most videos on the web are compressed where
frames can be preprocessed by a combination of proprietary   methods.
We use the videos from the REDS4 testing dataset for experiments as the image resolution is higher.

We first generate uncompressed videos out of the raw frames, and then upload them to YouTube.
These videos are encoded and compressed at different resolutions for downloading.
In our setting, the uploaded videos are of  pixels.
The resolutions that are available for downloading on YouTube are 480p, 360p, 240p, and 144p.
In the following experiments, we download the videos at 360p using the YouTube-dl~\cite{youtube-dl}.
We evaluate three state-of-the-art methods, including MuCAN~\cite{mucan}, RSDN~\cite{rsdn}, and TecoGan~\cite{tecogan2020} on these videos that are compressed by proprietary methods by YouTube. 
Figure~\ref{fig:reds4_360p} shows the VSR results by the evaluated methods, where the COMISR model produces better visual results with less artifacts.



\subsection{Ablation Study}
\label{modelablation}
We analyze the contribution of each module in the COMISR model.
We start with the recurrent module described in Section~\ref{method} as the baseline model. 
Similar to FRVSR~\cite{frvsr}, the recurrent model computes the flow between consecutive frames, warps the previous frame to the current, and upscales the frames. 
We carry out two sets of ablation studies, with or without using compressed images, to show the effectiveness of each module (see Section~\ref{impldetails}).

Table~\ref{tbl:model_ablation} shows the ablation studies where we incrementally add each module to the basic recurrent model.
For each setting, the model is trained with and without compressed images, and then evaluated on original and compressed frames.
The results show that each module helps achieve additional performance gain, in both training process with only compressed images or a combination of compressed and uncompressed images. 
We note it is important to add some uncompressed images in the training process to achieve best results on compressed videos. 
The full COMISR model performs best among all settings. 
For example, the fourth row in Table~\ref{tbl:model_ablation}, the uncompressed PSNR on Vid4 drops 0.17 dB.



\begin{table}[tp]
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|cccc}
            & \multicolumn{2}{c}{No compression Aug} & \multicolumn{2}{c}{Aug CRF15-25} \\
 Components &   Uncompressed      &   CRF25       &   Uncompressed    &   CRF25       \\ \toprule [0.2em]
 Recur  &    26.61 / 0.808   &  23.97 / 0.634         &  26.53 / 0.815         &   24.23 / 0.648       \\
 Recur + a &   27.16 / 0.837 &  24.24 / 0.650       &    26.64 / 0.818   &  24.74 / 0.686          \\
 Recur + ab&   27.45 / 0.844  &  24.27 / 0.649    &   27.27 / 0.838        &  24.92 / 0.696        \\
 Recur + abc&  27.48 / 0.845  & 24.31 / 0.650     &  27.31 / 0.840 & 24.97 / 0.701  \\ \hline      
\end{tabular}
}
\vspace{1mm}
\caption{Ablations on three modules of the COMISR model on Vid4: (a) bi-directional recurrent module, (b) detail-aware flow estimation, and (c) Laplacian enhancement module.
Each entry shows the PSNR/SSIM values on the Y-channel.
}
\label{tbl:model_ablation}
\vspace{-0.1cm}
\end{table}

\subsection{User Study}
\label{userstudy}
To better evaluate the visual quality of the generated HR videos, we conduct a user study using Amazon MTurk~\cite{mturk} on the Vid4~\cite{vid4} and REDS4~\cite{Nah_2019_CVPR_Workshops_REDS} datasets.
We evaluate the COMISR model against all other methods using videos compressed with CRF25.
In each experiment, two videos generated by the COMISR model and other methods are presented side by side and 
each user is asked ``which video looks better?''
For the Vid4 and REDS4 datasets, all the test videos are used for the user study.
For each of the video pairs, we assign to 20 different raters.
The aggregated results are shown in Figure~\ref{fig:userstudy}.
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\linewidth]{figures/vid4_reds4_userstudy_comisr_tall.pdf}
\caption{Aggregated user study results on Vid4 and Reds4.
Results show that users favored COMISR against all other compared methods.
}
\vspace{-0.2cm}
\label{fig:userstudy}
\end{figure}