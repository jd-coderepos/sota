

\documentclass[nohyperref]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{multirow}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2022} 


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{tabularx}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{Class-Continuous Conditional Generative Neural Radiance Field}

\begin{document}

\twocolumn[
\icmltitle{Class-Continuous Conditional Generative Neural Radiance Field}



\begin{icmlauthorlist}
\icmlauthor{Jiwook Kim}{sch}
\icmlauthor{Minhyeok Lee}{sch}
\end{icmlauthorlist}


\icmlaffiliation{sch}{School of Electrical \& Electronics Engineering, Chung-Ang University, Seoul 06974, Korea}

\icmlcorrespondingauthor{Minhyeok Lee}{mlee@cau.ac.kr}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{}  





\begin{abstract}
The 3D-aware image synthesis focuses on conserving spatial consistency besides generating high-resolution images with fine details. Recently, Neural Radiance Field (NeRF) has been introduced for synthesizing novel views with low computational cost and superior performance. While several works investigate a generative NeRF and show remarkable achievement, they cannot handle conditional and continuous feature manipulation in the generation procedure. In this work, we introduce a novel model, called Class-Continuous Conditional Generative NeRF (G-NeRF), which can synthesize conditionally manipulated photorealistic 3D-consistent images by projecting conditional features to the generator and the discriminator. The proposed G-NeRF is evaluated with three image datasets, AFHQ, CelebA, and Cars. As a result, our model shows strong 3D-consistency with fine details and smooth interpolation in conditional feature manipulation. For instance, G-NeRF exhibits a Fréchet Inception Distance (FID) of 7.64 in 3D-aware face image synthesis with a  resolution. Additionally, we provide FIDs of generated 3D-aware images of each class of the datasets as it is possible to synthesize class-conditional images with G-NeRF.
\end{abstract}



\section{Introduction}
\label{introduction}
There have been many approaches \cite{VAE, pixel_rnn} for synthesizing novel images. Generative Adversarial Network (GAN) \cite{gan} has shown outstanding results in generating images by learning the distributions of datasets, resulting in the synthesis of photo-realistic instances. Furthermore, many studies have improved the ability to generate high-resolution images \cite{chen2016infogan, karras2017progressive, karras2020analyzing, choi2018stargan}. Notwithstanding the advances made by existing studies, GANs still have limitations when synthesizing multiple views of a single object due to most data collections being based on two-dimensional information, which can lead to instability in 3D-consistency. 


\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig/fig1}}
\caption{\textbf{Synthesized images of each class of AFHQ by our model (with a  resolution).} A row displays a single object with different rotation input vectors. Note that the images of different classes are generated by a single model with different conditional input vectors. Our model can generate various views of an object that conserves strong 3D-consistency.}
\label{figure_1}
\end{center}
\vskip -0.2in
\end{figure}



In order to address this problem, 3D-based GANs \cite{wu2016learning, nguyen2019hologan, nguyen2020blockgan} have been studied, which make use of volume rendering methods. However, those methods require high computational power and memory in order to train effectively. In recent years, Mildenhall et al. \yrcite{mildenhall2021nerf} proposed the Neural Radiance Field (NeRF) as an alternative to conventional voxel-based volume rendering methods \cite{kajiya1984ray, drebin1988volume, henzler2019escaping}. NeRF greatly reduces the complexity of computations and memories required compared to other approaches. Accordingly, NeRF has been extended for use in 3D-based GAN studies \cite{schwarz2020graf, niemeyer2021giraffe, chan2021pi, deng2022gram} with excellent results and low complexities. 


Nevertheless, those NeRF-based GANs cannot control image generations with conditional labels because they do not provide the required conditional information into the generator of the GAN. GIRAFFE \cite{niemeyer2021giraffe}, which is a NeRF-based generative model, was capable of generating 3D-aware images without any extra information, such as camera location and direction of certain images for training, which conventional NeRF required. Even though GIRAFFE demonstrated surpassing results, it is not able to disentangle the various features \cite{bengio2013representation, locatello2019challenging, nguyen2020blockgan} in images that are needed in order to generate an image containing desired features. Jo et al. \yrcite{jo2021cg} tackled this problem by trying to provide condition information, such as image types and texts; however, they cannot represent the intensity of conditions. 

In this paper, we propose a novel method to address a new task of 3D-aware conditional image generation. The given task requires that conditional label values \cite{cgan, odena2017conditional, cgan_proj} be able to control features of generated images, as shown in \cref{figure_1}, and that the condition label values be continuous in order to continuously change the intensity of the corresponding conditions. To the best of our knowledge, this paper is the ﬁrst to tackle this task. 

We have named our proposed method Continuous Conditional Generative Neural Radiance Field (G-NeRF). Our model is based on the GIRAFFE, which showed superior performances in 3D-aware image generation. For the conditional generation, our model takes numerical and continuous values as input, which enables continuous feature representations. Most datasets containing conditions are composed of numerical conditions, so it is expected that our model can be applied to numerous datasets. 

Conventional NeRF-based generative models often require a lot of time for training. Hence, we employed residual modules \cite{he2016deep, he2016identity} in our model architecture to accelerate training speed and synthesize superior images.

Generating multi-views of an instance is a significant problem in generating 3D-aware images. To address this problem, G-NeRF aims at providing multi-view with fine 3D-consistency. We provide results of 3D controllable image synthesis in three datasets which are AFHQ \cite{choi2020stargan}, CelebA \cite{liu2015deep} and Cars \cite{ashrafi_2022}. We experiment to show controlling image synthesis by translating and rotating an object as well as adding numbers of the objects in a single image. 

The contributions of this paper are as follows: 


\begin{itemize}
\item We propose a model called G-NeRF to tackle a novel task: conditional and continuous feature manipulation in 3D-aware image generation. 
\item We reduced training time and increased performance by using residual modules in the NeRF architecture. 
\item We demonstrated the conditional and continuous feature manipulation in 3D-aware image generation with multiple datasets: AFHQ, CelebA and Cars. 
\item Since it is possible to generate class-conditional 3D-aware images, we provide Fréchet Inception Distance (FID) of each label in AFHQ and Cars datasets. 
\end{itemize}

\section{Related Work}
\label{related_work}
\textbf{Implicit neural representation and rendering:} The use of deep learning techniques \cite{lecun2015deep} to represent three-dimensional space has received considerable attention in recent years. Among the various approaches that have been proposed, implicit neural representations \cite{sitzmann2019scene, tulsiani2020implicit, rajeswar2020pix2shape} have shown particular promise. NeRFs have been proposed by combining an implicit neural representation with volume rendering \cite{drebin1988volume} to enable the synthesis of novel views that are not explicitly represented in the training data. As a result, NeRF is capable of synthesizing 3D-consistent images with fine details. This is accomplished by mapping a 3D point and direction onto an RGB color and corresponding volume density using Multi-Layer Perceptrons (MLPs) \cite{pinkus1999approximation} which learn an implicit representation function \cite{mescheder2019occupancy, chabra2020deep, yariv2020multiview}. However, one downside to using NeRFs is that they require highly constrained images for supervision during training. Another concern is that each instance of a NeRF can only represent a single object rather than multiple objects simultaneously. It has been proposed that generative NeRFs may be able to alleviate these problems.

\textbf{Generative NeRF:} There has been some recent progress in NeRF-based methods for generating 3D-aware images from 2D unconstrained image datasets. In these methods, generative models are trained to ensure continuous 3D geometric consistency. For instance, the GRAF \cite{schwarz2020graf} and pi-GAN \cite{chan2021pi} both proposed a generative NeRF, and showed promising results. GIRAFFE is another method that is more closely related to our work and improves on GRAF by separating an object from its background scene. However, none of these methods can control the conditional generation of images, which enables various feature manipulation in generated images. To address this issue, Jo et al.\yrcite{jo2021cg} proposed CG-NeRF; however, since CG-NeRF utilizes pi-GAN structures instead of GIRAFFE, it removes background scene in most of their datasets while training. Specifically, CG-NeRF takes two types of conditional information. First, conditional data forms are used to translate one image into another; while our study aims to generate novel images directly from noise vectors instead. Second, CG-NeRF takes texts as conditions with CLIP (a natural language processing technique) \cite{radford2021learning}; whereas our model uses numerical conditions, which can be interpolated by varying values, allowing for continuous feature manipulation in image synthesis that represents intensity changes in relation to the given values.

\begin{figure*}[t]
\begin{center}
\centering {\includegraphics[width=\textwidth]{fig/fig2}}
\caption{\textbf{Overview of the proposed G-NeRF.} Since our model is inspired by the architecture of GIRAFFE, our model generates  objects and the background with  decoders and a composition operator. D\_ indicates th decoder and  represents the composition operator. The decoders take a 3D coordinate vectors of positional encoding  and viewing direction , where  indicates positional encoding functions. In addition, the decoders take conditional vectors , which are encoded by linear layers, shape codes , and appearance codes . By compositing the outputs of each decoders with the composition operator  and then volume-renders the result. Consequently, a composited feature vector  is produced. The feature vector  passes the neural rendering module . In this process, the generator  synthesizes a fake image . The discriminator  takes a real image  or the fake image  projected by the conditional labels .}
\label{figure_2}
\end{center}
\vskip -0.2in
\end{figure*}


\section{Methods}
\label{methods}
Our goal is to make a framework for conditionally controllable 3D-aware image synthesis that guarantees representations of the intensity of conditions with continuous values. Given a labeled real-world 2D image dataset, the 3D-aware image generator \textit{G} takes conditions and camera pose, which are denoted by  and , respectively, and latent vectors for representing shape and appearance, i.e.,  and . Then, \textbf{G} produces an image \textbf{I}, corresponding to the input condition. At training time, real images from the dataset and \textbf{I} are directed to the discriminator \textbf{D}, which is comprised of residual modules \cite{he2016deep}, while conventional convolutional layers \cite{o2015introduction} and fully-connected layers \cite{pinkus1999approximation} have been used in other generative NeRF models in recent years. \cref{figure_2} shows an overview of our model.


\subsection{Conditional Neural Radiance Fields} A conventional neural radiance field maps a 3D coordinate  and viewing direction  to a volume density  and a view-dependent RGB color value \text{R}, \text{G}, and \text{B} with fully-connected layers. However, several studies \cite{rahaman2019spectral} observed that deep learning techniques are difficult to represent high-frequency details, especially with low-dimensional inputs. To diversify the inputs, NeRFs introduced positional encoding \cite{tancik2020fourier} to the inputs, \textbf{x} and \textbf{d}, before the fully-connected layers:

where  indicates positional encoding,  is the dimensionality of positional encoding, and  is a scalar value as a component of \text{x} and \text{d}. To extend to generative neural radiance fields, shape and appearance codes,  and , are fed into the MLP as follows:



In GIRAFFE, the generative neural radiance field was substituted to generative neural feature fields by extending the dimensionality of color \cite{niemeyer2021giraffe}, which was originally three-dimensional, to a feature space having a dimension of  as:





where  represents a generative neural feature field,  and  indicate dimensionalities of positional encoding output, and  and  are dimensionalities of latent encodings of the shape and appearance, respectively.

In this work, we project conditions to the latent vectors \cite{cgan_proj},  and , by element-wise production. Before the projection, conditional vectors  having a dimension of  are encoded by a fully-connected layer to make the dimension the same as the dimensionalities of  and . The shape and appearance conditional encodings,  and , are constructed as

where  and  are the encoding layers for the shape and appearance, respectively, and  denotes element-wise multiplication. We employ these conditional projections as a replacement for conventional latent vectors in generative feature fields:







\subsection{Scene compositions}
As our model is motivated by GIRAFFE, it can separate scenes and individual objects with multiple feature vectors \cite{niemeyer2021giraffe}. Each entity requires a generative feature field and is encoded by a respective feature vector. Consequently, in the model, there are  generative feature fields when  objects and a background scene exist in an image. To composite  entities, the composition operator  composites all feature fields from the  entities. As previously described, each single entity  outputs a volume density  and a feature vector . Following the method in GIRAFFE, we composite each component of entities with density-weighted mean-based composition. The mathematical expression of the composition operator can be represented as

where .


\subsection{Volume rendering and neural rendering}
For scene rendering, previous studies \cite{schwarz2020graf} have volume-rendered a camera ray  directly to RGB colors, while GIRAFFE volume-renders it to feature vectors \cite{niemeyer2021giraffe}, and subsequently, neural-renders the feature vectors to generate synthetic images. Our approach basically follows a discretized form of volume rendering methods used in NeRF \cite{mildenhall2021nerf}; nonetheless, detailed methods follow those of GIRAFFE to render features to images, which can be represented as

where  represents a final feature vector,  denotes an accumulated transmittance along the cast ray, and  is the number of sample points along a cast ray for an arbitrary camera pose . By sampling points along the camera ray, we utilize a feature vector  and a density  corresponding to each point. Furthermore, the feature images have a  resolution, i.e.,  for cost-effectiveness.
Existing studies, including GRAF and NeRF \cite{schwarz2020graf, mildenhall2021nerf}, commonly adopted the volume rendering approach for 3D-to-2D projections. However, in our model, an additional 2D neural rendering network is required since the volume rendering composes feature images, not colored high-resolution images. The additional neural rendering network,

maps outputs of the volume rendering to a synthetic image, , by upsampling the feature images. The architecture of neural rendering network is similar to a neural rendering operator in existing studies \cite{niemeyer2021giraffe}; however, we hire residual modules instead of conventional convolutional networks to enhance training speed and performance.

\subsection{Training details}
At training time, we randomly sample latent vectors  and , as well as camera pose  from prior distributions , , and . Prior distributions  and  are defined as Gaussian distributions, and camera pose distribution  are set to a uniform distribution. In the generator \textit{G}, we project conditional labels to latent vectors \cite{cgan_proj}  and . Similarly, conditional labels are projected to real images  or synthesized images , in the discriminator . Real images  are randomly sampled from the training dataset, which follows distribution . We use a GAN loss with R1 gradient penalty \cite{mescheder2018training} as follows:

We train the generator and discriminator of the proposed model by competing for a zero-sum game with the above loss function. We use the RMSprop optimizer \cite{rmsprop} with a learning rate of  and  for the generator and the discriminator, respectively. The model is trained on one A6000 GPU with 48GB memory with a batch size of 32. We set  for a  resolution and a  resolution, while we set  for a  resolution. We utilize ReLU activations \cite{relu} as activation functions used in our model, except for the final layer in the neural renderer, which uses a sigmoid function \cite{ramachandran2017searching}.



\subsection{Accelerating training with residual modules}
We observe elaborate conditional generation with GIRAFFE is not feasible without residual modules \cite{he2016deep, mescheder2018training}. In the experiments of this study, we demonstrate that conditional generation with plain networks shows lagging performance. We argue that this result is because conditionally projected latent vectors are too far from the discriminator, which causes gradient vanishing. Therefore, residual modules support conveying the information to conditionally projected latent vectors from the discriminator. Moreover, the range of varying latent vectors expands since we projected conditional labels to latent vectors, which is challenging to learn with plain networks. We apply residual modules to our model in the decoder, neural rendering network, and discriminator. We validate the efficiency of this contribution in the Section \ref{experiments}.


\section{Experiments}
\label{experiments}
In this section, we evaluate our G-NeRF on three real-world datasets: CelebA \cite{liu2015deep}, AFHQ \cite{choi2020stargan}, and Cars \cite{ashrafi_2022}. CelebA contains approximately 200,000 face images with 40 conditional binary labels. AFHQ contains 16,000 high-resolution animal faces of 7 species; Cars contains 9,737 car images of 13 car models with various camera views. We train and evaluate  resolution for all datasets,  for CelebA and AFHQ, and  for AFHQ and Cars.

We first evaluate the conditionally controlling 3D-consistent image generations. We then evaluate the quality of generation by FIDs \cite{ttur}. Furthermore, we show training speed by comparing with synthesized images at each iteration in the training process. Finally, we include an evaluation of residual modules to validate the efficiency of residual modules adopted in our model.

\begin{figure}[t]
\begin{center}
\centering {\includegraphics[width=\columnwidth]{fig/fig3}}
\vskip -0.15in
\caption{\textbf{Class conditional synthetic object rotation generated by G-NeRF trained with CelebA.} Each row represents a single object with the same latent vectors. Each column indicates rotation angles. In (a), we fixed the input conditions as a bald man, whereas we fixed the conditions as a blonde smiling woman in (b). All images have a resolution of . Using G-NeRF, 3D-consistent image generation is successful under the given conditions.}
\label{figure_3}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig/fig4}}
\vskip -0.1in
\caption{\textbf{Visualization of various 3D-aware manipulation in Cars.} By controlling the horizontal and depth translation, the disentanglement of the objects and background are shown in (a) and (b). After training with unstructured 2D images with a single object, we can generate  objects in one scene by replicating  decoders as in (c). Furthermore, our model enables  object rotation for each object in the scene as in (d).}
\label{figure_4}
\end{center}
\vskip -0.3in
\end{figure}





\subsection{Controllable features in 3D object generation}
We evaluate our model by rotating, translating depth and horizontal, and adding objects. \cref{figure_1,figure_3,figure_4} demonstrates that G-NeRF successfully learns 3D-consistency with fine details for AFHQ, CelebA, and Cars, respectively, since the performed rotation and translations of objects are conserving the spatial consistency. Also, we confirm that features of the generated images successfully follow conditional inputs.

Furthermore, in \cref{figure_4}, we assess G-NeRF using out-of-distribution images with translating depth and horizontal at test time. This means G-NeRF can generate beyond the distribution of training images by using extended rotation and transition values over the training sets. Moreover, we can finely control each object and scene in the generated images by G-NeRF; for instance, while adding the objects in a scene, each object can be controlled by translating and rotating in 3D space.



\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig/fig5}}
\vskip -0.1in
\caption{\textbf{Interpolation and extrapolation on conditional input values.} Each row represents a single object. We present the diverse conditional results according to conditional input values with the range of zero to three. Note that, in training time, the features are trained only with the two values of zero and one, which indicate the existence of the corresponding feature. The features in the face images with interpolated and extrapolated input values smoothly change, which means the continuous conditional learning is adequately progressed.}
\label{figure_5}
\end{center}
\vskip -0.4in
\end{figure}


\subsection{Continuously controllable features in 3D object generation}
To show the ability to manipulate each condition, we analyze the model by interpolating 40 conditional binary values of the CelebA dataset, such as chubby, smiling, blonde, and pale skin. While the range of conditional values is zero and one in the training procedure since the labels are binarily encoded in the CelebA dataset, in the test procedure, we further provide extrapolated values by expanding the range to zero to three, as shown in \cref{figure_5}. This experiment can demonstrate whether each face feature is mapped into each input label of the generator. With various conditional label values, G-NeRF shows superior performances in interpolation \cite{davis1975interpolation} and extrapolation \cite{brezinski2013extrapolation} of each condition. For example, chubby and smiling conditions \cite{liu2015deep} smoothly change according to conditional values regardless of the extrapolation range. This result also indicates the ability of our model to generate out-of-distribution images; for instance, exaggerated features can be generated with a high input label value exceeding one. 


\begin{figure*}[h!]
\begin{center}
\centering {\includegraphics[width=\textwidth]{fig/fig6}}
\vskip -0.15in
\caption{\textbf{Class interpolation on conditional input values with the AFHQ dataset.} Each row and column represent the same latent vectors (identical object) and the same class-conditional values, respectively. By interpolating the values of each class, features of each category coexist at the intermediate state of class-conditional values.}
\label{figure_6}
\end{center}
\vskip -0.2in
\end{figure*}


We experiment with non-characteristic (implicit) labels, corresponding to categories (classes) of AFHQ. This class label is more challenging to learn since the image features of each class are not obvious. We generate inter-class images with interpolated class-conditional input values as shown in \cref{figure_6}. We observe that features of each class coexist at the intermediate state of class-conditional values. This result indicates that the ability of feature manipulation can be applied to implicit class labels, which is generally more challenging to be trained.


\begin{figure}[h!]
\begin{center}
\centering {\includegraphics[width=\columnwidth]{fig/fig7}}
\vskip -0.15in
\caption{\textbf{Interpolating latent vectors.} In (a) and (b), the leftmost images and rightmost images are generated with the same conditional inputs but different latent vectors. Intermediate images are generated with the interpolated values of the two latent vectors. By interpolating values of the latent vectors, our model offers diverse images with a certain condition (i.e. a smiling blonde woman or a glassed man). The conditional input labels of (a) and (b) are fixed as a glassed man and a smiling blonde woman, respectively. We observe that various images can be synthesized with the same condition, which demonstrates the properties of the generative model.}
\label{figure_7}
\end{center}
\vskip -0.2in
\end{figure}



\subsection{Diversity of generation}
We verify that our model can synthesize diverse images under the same conditions. To verify diverse generations, we fix the conditional input values and interpolate the values of the latent vectors \cite{shmelkov2018good}. With varying the latent vectors, our model generates various intermediate images with conserving the fixed conditional features, as shown in \cref{figure_7}. This is due to our generator can disentangle the condition to the latent vectors and magnificently learn the distribution of each conditional case. 



\begin{table}[h!]
\caption{\textbf{Quantitative comparison with FIDs () with three datasets.} The baseline is set to the conditional GIRAFFE with plain networks. The , , and  are the image resolutions of the generated images and real images.}
\label{table_1}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}
{
\begin{tabular}{c|ccc|cc|cc}
\hline\hline
\multirow{2}{*}{Model} & \multicolumn{3}{c|}{AFHQ} & \multicolumn{2}{c|}{CelebA} & \multicolumn{2}{c}{Cars}\\
\cline{2-8}
      &  &  &  &  &  &  &  \\
\hline
 Baseline & 212.74 & 226.51 & 239.01 & 55.90 & 83.89 & 244.55 & 266.51 \\
 Ours & 26.72 & 25.79 & 28.58 & 5.60 & 7.64 &43.29 & 31.63 \\
 
\hline
\hline
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{table}[t]
\caption{\textbf{Quantitative comparison with FIDs () with each class of AFHQ and Cars.} The , , and  are the image resolutions of the generated images and real images. The FIDs of  resolution of the Cars dataset are not evaluated due to training challenges with full-degree images under a low resolution.}
\label{table_2}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{  >{\centering\arraybackslash}m{2.8cm} | >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.2cm}  } 
\hline\hline
\multirow{2}{*}{Category} & \multicolumn{3}{c}{AFHQ}\\
\cline{2-4}\
      &  &  & \\
\hline
 cat & 10.38 & 13.65 & 15.48\\
 dog & 31.64 & 43.37 & 51.73\\
 leopard & 17.20 & 14.98 & 13.42\\
 fox & 25.97 & 28.01 & 22.50\\
 lion & 8.76 & 12.20 & 7.61\\
 tiger & 12.78 & 8.96 & 5.93\\
 wolf & 28.39 & 30.82 & 14.85\\
\hline\hline

\multirow{2}{*}{Category} & \multicolumn{3}{c}{Cars}\\
\cline{2-4}\
      &  &  & \\
 \hline
 Peykan & - & 67.47 & 68.57\\
 Quik & - & 62.71 & 49.64\\
 Samand & - & 50.53 & 61.01\\
 Peugeot-Pars & - & 66.27 & 46.45\\
 Peugeot-207i& - &71.06 & 52.99\\
 Pride-111 & - & 62.28 & 68.84\\
 Pride-131 & - & 57.70 & 65.43\\
 Tiba2 & - & 61.79 & 55.57\\
 Renault-L90 & - & 65.51 & 76.84\\
 Nissan-Zamiad & - & 88.83 & 133.77\\
 Peugeot-206 & - & 61.55 & 128.10\\
 Peugeot-405 & - & 66.15 & 71.45\\
 Mazda-2000 & - & 77.23 & 104.53\\

\hline
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}


\subsection{Quantitative evaluation}
We evaluate the image quality by measuring FIDs with 20,000 randomly sampled real images and 20,000 generated images, which is one of the conventional methods to assess generative NeRFs. We furnish two evaluations to demonstrate the quality of generation on each dataset as well as each label in the dataset. We evaluate images generated with random conditional inputs by FID using different image resolutions and datasets.


As can be seen in \cref{table_1}, G-NeRF achieves prominent FIDs in the conditional 3D-consistent generation. We observe that FIDs of G-NeRF outperform the conditional GIRAFFE irrespective of resolution; this signifies that G-NeRF is robust to training in high resolutions. The conditional GIRAFFE exhibits considerably high FIDs, indicating training problems. For example, in the CelebA face image generation with a  resolution, G-NeRF demonstrates an FID value of 7.64, reducing the value by  compared to the baseline. However, in the Car dataset, we notice that FIDs are relatively inferior to the other datasets; we attribute this to a difference in view degree (360° in Car dataset vs limited degrees in other datasets). Consequently, we believe that the FID evaluation may not be appropriate for different views when comparing results across datasets.


The FIDs for each label in \cref{table_2} show the level of conditional disentanglement that has been achieved by G-NeRF. We can see that the FIDs for each label are similar to, or better than, the FIDs for all labels combined. This suggests that G-NeRF is able to satisfactorily learn the features associated with each condition across all datasets. In addition, even in cases where some labels have very few examples (e.g., foxes, lions, and tigers in AFHQ), we still observe comparable FIDs suggesting that G-NeRF is adept at learning from data regardless of its unposed distribution. In the DOG class of the AFHQ dataset, the FIDs are inferior to other classes. This result is due to the variety of dog images while the other classes consist of similar images.

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig/fig8}}
\caption{\textbf{Training results in terms of iteration.} By employing residual modules, our model produces diverse and high-quality images with fine details within a small number of iterations.}
\label{figure_8}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centering {\includegraphics[width=\columnwidth]{fig/fig9}}
\vskip -0.15in
\caption{\textbf{Generated objects by the conditional GIRAFFE with plain networks trained with 500,000 iterations.} (a) and (b) show the synthesized images with plain networks-based conditional GIRAFFE trained in AFHQ and Cars, respectively. Despite enough iterations (five times larger compared to our model), the generator synthesizes low-quality images.}
\label{figure_9}
\end{center}
\vskip -0.2in
\end{figure}



\subsection{Evaluation of residual modules}
We compare G-NeRFs with residual modules and conditional GIRAFFE with a plain network in terms of generated image quality. This experiment demonstrates the effect of adopting residual modules \cite{he2016deep, mescheder2018training} in G-NeRF architecture. As shown in \cref{figure_8}, G-NeRF generates high-quality images with fine details after a small number of iterations. In contrast, the conditional GIRAFFE using plain networks shows poor generation quality as seen in \cref{figure_9} with more (five times larger) iterations. Therefore, we can confirm that it is necessary to use the proposed residual modules in the NeRF structures in order to generate high-quality conditional images. 

We further compare the FIDs of each model in three datasets. As can be seen in \cref{table_1}, G-NeRF demonstrates significantly better FIDs than conditional GIRAFFE with plain networks in the qualitative comparison. This confirms our hypothesis that residual modules improve the generation quality and is extremely helpful for the conditional 3D-aware generation. As previously described, the residual modules assist the learning of conditional information by helping the gradient flow from the output of the discriminator to the conditional inputs of the generator.


\section{Conclusions}
We presented a novel model, G-NeRF, for conditional and continuous feature manipulation in 3D-aware image generation. Our core idea is projecting conditional labels with encoding layers to the latent vectors of the generator and an intermediate layer of the discriminator in order to disentangle features in a dataset. G-NeRF incorporates residual modules for facilitating 3D-aware conditional training. By interpolating and extrapolating the conditional input values, we demonstrated 3D-consistent image generation with elaborate manipulations of the intensity of features. We believe that our method pioneers a new conditional generation with NeRF. 




\bibliography{example_paper}
\bibliographystyle{icml2022}


\newpage
\appendix
\onecolumn
\section{Model details}
In this section, we discuss our detailed network architectures with residual modules. We adopt the residual modules to the discriminator, decoder, and neural renderer as \cref{figure_10,figure_11,figure_12}.


\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{appendix/resblocks.png}}
\caption{\textbf{Architectures of ResBlocks used in our model.} We employ two types of residual modules, the ResBlock and ResBlockFC. While the ResBlock is comprised of convolution layers, the ResBlockFC is based on linear layers. Therefore, we utilize the ResBlock for the discriminator and the neural renderer, which require spatial information, whereas the ResBlockFC is utilized for the decoder, which maps a 3D coordinate, a viewing direction, and conditional encodings to a feature space and a volume density with linear layers.}
\label{figure_10}
\end{center}
\vskip -0.2in
\end{figure}




\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=0.6\columnwidth]{appendix/resnet1.png}}
\caption{\textbf{Architecture of the discriminator.} The \textbf{x} and \textbf{y} indicate an input image of the discriminator and a conditional label, respectively. Each \textbf{x} and \textbf{y} are embedded with the convolutional layer and the linear layer. The embedded image is encoded by passing several average pooling layers and ResBlocks. The encoded image is projected to the embedded conditional label. By the projection, we can fuse information of the image and conditional label.}
\label{figure_11}
\end{center}
\vskip -0.2in
\end{figure}




\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{appendix/resnet2.png}}
\caption{\textbf{Architectures of the decoder and neural renderer.} The (a) and (b) show the architecture of the decoder and neural renderer, respectively. In (a), the shape conditional encoding  and the positional encoded 3D point  are embedded and multiplied. We use 8 blocks of the ResBlockFC and one skip-connnection. After passing the blocks, the volume density  is estimated as the output of the final ResBlockFC. By adding the appearance conditional encoding  and the positional encoded viewing direction  to the volume density and passing the linear layer, the decoder outputs the feature . In (b),  and  represent a bilinear upsampling and a nearest neighbor upsampling, respectively. The neural renderer takes the feature  as an input and outputs a synthesized image .}
\label{figure_12}
\end{center}
\vskip -0.2in
\end{figure}
\clearpage
\newpage

\section{Controllable features in 3D object generation}
In this section, we report additional 3D-aware generations with controlling the features as well as a negative result as \cref{figure_13,figure_14}.


\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=0.95\columnwidth]{appendix/scale.png}}
\caption{\textbf{Visualization of manipulating the scaling.} Each row and column indicate a single object with the same conditional class and the same scale value, respectively. Our model generate a object with various scales, including out-of-distribution images.}
\label{figure_13}
\end{center}
\vskip -0.2in
\end{figure}




\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{appendix/negative.jpg}}
\caption{\textbf{Negative result.} Each column indicates different rotation angles. Near to the edge of columns represents more rotation. While we trained with  rotation angle, we produce the results with  rotation angle, corresponding to out of distribution. The excessive out of distribution occurs negative results as the edge of column images.}
\label{figure_14}
\end{center}
\vskip -0.2in
\end{figure}
\clearpage
\newpage

\section{Continuously controllable features in 3D object generation}
We report additional interpolation and extrapolation results on CelebA with manipulations of different labels as \cref{figure_15,figure_16,figure_17,figure_18,figure_19,figure_20}.

\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{appendix/glass.jpg}}
\caption{\textbf{Interpolation and extrapolation results with the glass condition.} Each column indicates generated images with the corresponding label values between zero and three.}
\label{figure_15}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{appendix/younger.jpg}}
\caption{\textbf{Interpolation and extrapolation results with the aging condition.} Each column indicates generated images with the corresponding label values between zero and three.}
\label{figure_16}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{appendix/makeup.png}}
\caption{\textbf{Interpolation and extrapolation results with the make-up condition.} Each column indicates generated images with the corresponding label values between zero and three.}
\label{figure_17}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{appendix/rosy_cheeks.png}}
\caption{\textbf{Interpolation and extrapolation results with the rosy cheek condition.} Each column indicates generated images with the corresponding label values between zero and three.}
\label{figure_18}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{appendix/bald.png}}
\caption{\textbf{Interpolation and extrapolation results with the bald condition.} Each column indicates generated images with the corresponding label values between zero and three.}
\label{figure_19}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{appendix/mustache.png}}
\caption{\textbf{Interpolation and extrapolation results with the mustache condition.} Each column indicates generated images with the corresponding label values between zero and three.}
\label{figure_20}
\end{center}
\vskip -0.2in
\end{figure}




\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{appendix/afhq_images.jpg}}
\caption{\textbf{Random Samples on AFHQ with a  resolution.}}
\label{figure_21}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{appendix/celeba_images.jpg}}
\caption{\textbf{Random Samples on CelebA with a  resolution.}}
\label{figure_22}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{appendix/cars_images.jpg}}
\caption{\textbf{Random Samples on Cars with a  resolution.}}
\label{figure_23}
\end{center}
\vskip -0.2in
\end{figure}














\end{document}
