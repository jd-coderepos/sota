\label{introduction} 

\begin{figure}\label{fig:intro_figure}
\begin{subfigure}[a]{.2\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{graphs/Intro_a.eps}
  \caption{}
  \label{fig:sfig1}
\end{subfigure}\begin{subfigure}[h]{.21\textwidth }
  \centering
  \includegraphics[width=1.3\linewidth]{graphs/intro_enlarg.eps}
  \caption{}
  \label{fig:sfig2}
\end{subfigure}
\caption{(a) \textbf{Real world challenges in multi-label classification}. A typical image contains few positive samples, and many negative ones, leading to high negative-positive imbalance. Also, missing labels in ground-truth are common in multi-label datasets. (b) \textbf{Proposed solution with ASL}. 
The loss properties will be detailed in Section \ref{sec:ASL_gradients}}

\label{fig:fig}
\vspace{-3mm}
\end{figure}

Typical natural images contain multiple objects and concepts~\cite{yang2016exploit, yun2021relabel}, highlighting the importance of multi-label classification for real-world tasks. Recently, remarkable advances have been made in multi-label benchmarks such as MS-COCO~\cite{lin2014microsoft}, NUS-WIDE~\cite{chua2009nus}, Pascal-VOC~\cite{everingham2007pascal} and Open Images~\cite{kuznetsova2018open}. 
Notable success was reported by exploiting label correlation via graph neural networks which represent the label relationships~\cite{chen2019multi_MLGCN, chen2019multi, durand2019learning} or word embeddings based on knowledge priors~\cite{chen2019multi_MLGCN, Wang2019MultiLabelCW}. Other approaches are based on modeling image parts and attentional regions~\cite{you2020cross,gao2020multi,wang2017multi,yeattention}, as well as using recurrent neural networks~\cite{nam2017maximizing, wang2016cnn}.



Despite their effectiveness, recent approaches are characterized by extensive architecture modifications and relying on additional external information, such as word embeddings and NLP models. In this work, we question whether such intricate solutions are truly necessary for achieving high performance in multi-label classification tasks.
In particular, we demonstrate that a careful design of the loss function can greatly benefit classification accuracy, while still maintaining a simple and efficient solution, based on standard architectures and training schemes.

A key characteristic of multi-label classification is the inherent positive-negative imbalance created when the overall number of labels is large. Most images contain only a small fraction of the possible labels, implying that the number of positive samples per category will be, on average, much lower than the number of negative samples. 
To address this, \cite{tong2020distribution} suggested a loss function for statically handling the imbalance in multi-label problems. However, it was aimed specifically at long-tail distribution scenarios. 
High imbalance is also encountered in dense object detection, where it stems from the ratio of foreground vs. background regions. Some solutions based on resampling methods were proposed, by selecting only a subset of the possible background examples \cite{oksuz2020imbalance}. However, resampling methods are not suitable for handling multi-label classification imbalancing, since each image contains many labels, and resampling cannot change the distribution of only a specific label.

Another common solution in object detection is to adopt the focal loss~\cite{tsung2017focal}, which decays the loss as the label’s confidence increases. This puts focus on hard samples, while down-weighting easy samples, which are mostly related to easy background locations. Surprisingly, focal loss is seldom used for multi-label classification, and cross-entropy is often the default choice (see \cite{chen2019multi_MLGCN,bartlett2008classification,chen2019learning,liu2018multi, gao2020multi}, for example). Since high negative-positive imbalance is also encountered in multi-label classification, focal loss might provide better results, as it encourages focusing on relevant hard-negative samples, which are mostly related to images that do not contain the positive class, but do contain some other confusing categories.
Nevertheless, for the case of multi-label classification, treating the positive and negative samples equally, as proposed by focal loss, is sub-optimal, as it results in the accumulation of more loss gradients from negative samples, and down-weighting of important contributions from the rare positive samples. In other words, the network might focus on learning features from negative samples while under-emphasizing learning features from positive samples.


In this paper, we introduce an asymmetric loss (ASL) for multi-label classification, which explicitly addresses the negative-positive imbalance.
ASL is based on two key properties:
first, to focus on hard negatives while maintaining the contribution of positive samples, we decouple the modulations of the positive and negative samples and assign them different exponential decay factors.
Second, we propose to shift the probabilities of negative samples to completely discard very easy negatives (hard thresholding). By formulating the loss derivatives, we demonstrate that probability shifting also enables to discard very hard negative samples, suspected as mislabeled, which are common in multi-label problems \cite{durand2019learning}.

We compare ASL to the common symmetrical loss functions, cross-entropy and focal loss, and show significant mAP improvement using our asymmetrical formulation. By analyzing the model's probabilities, we demonstrate the effectiveness of ASL in balancing between negative and positive samples. We also introduce a method that dynamically adjusts the asymmetry level throughout the training process, by demanding a fixed gap between positive and negative average probabilities, allowing simplification of the hyper-parameter selection process. 


The paper’s contributions can be summarized as follow:
\begin{itemize}[leftmargin=0.4cm]
  \setlength{\itemsep}{0.2pt}
  \setlength{\parskip}{0.2pt}
  \setlength{\parsep}{0.2pt}
\item We design a novel loss function, ASL, which explicitly copes with two main challenges in multi-label classification: high negative-positive imbalance, and ground-truth mislabeling.
  \item We thoroughly study the loss properties via detailed gradient analysis. An adaptive procedure for controlling the asymmetry level of the loss 
    is introduced, to simplify the process of hyper-parameter selection. 
  \item Using ASL, we obtain state-of-the-art results on four popular multi-label benchmarks. For example, we reach  mAP on MS-COCO dataset, surpassing the previous top result by .
  \item Our solution is effective and easy to use. It is based on standard architectures, does not increase training and inference time, and does not need any external information, in contrast to recent approaches. To make ASL accessible, we share our trained models and a fully reproducible training code.
\end{itemize}


%
