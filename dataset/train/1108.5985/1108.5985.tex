\documentclass{article}

\usepackage{enumerate,subfigure,latexsym} 

\usepackage{amsfonts,amssymb,amsmath,amsthm}

\usepackage[numbers]{natbib}

\usepackage{url}

\makeatletter
\def\blfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother
\usepackage{graphicx}
\newcommand\idagger{\rotatebox[origin=c]{180}{\ensuremath{\dagger}}}

\makeatletter
\newif\if@restonecol
\makeatother
\let\algorithm\relax
\let\endalgorithm\relax
\usepackage[algoruled,vlined]{algorithm2e}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output} 

\usepackage{tabularx, multirow, subfigure, rotating}
\usepackage{ifdraft}

\usepackage[usenames,dvipsnames, table]{xcolor}
\ifdraft{\usepackage[normalem]{ulem}}

\newtheorem{open}{\noindent Open problem}
\newtheorem{rem}{\noindent Remark}
\newtheorem{algo}{Algorithm}{\bfseries}{\rmfamily}
\newenvironment{spf}{\noindent{\it Sketch of the Proof:}}{\qed}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\newcommand\refcite[1]{\citealp{#1}} \newcommand\citess[1]{\textsuperscript{\textup{\citealp{#1}}}} 

\newcommand\coltwo[2]{{\left[\begin{array}{c} #1\\ #2\end{array}\right] }} 
\def\CC{{\mathbb C}} \def\PP{{\mathbb P}}
\def\NN{{\mathbb N}} \def\QQ{{\mathbb Q}}
\def\RR{{\mathbb R}} \def\ZZ{{\mathbb Z}} 
\def\A{{\mathcal A}} \def\R{{\mathcal R}} 
\newcommand{\Rd}{{\rm Res}_{\mathcal A}}
\newcommand{\Aff}{\mbox{Aff}}
\newcommand{\link}{\mbox{Link}}
\newcommand{\CH}{\mbox{CH}}
\newcommand{\pNR}{{\pi(N(\R))}}
\usepackage{color}
\newcommand{\fix}[1]{\textcolor{red}{#1}}

\hyphenation{speed-up}

\begin{document}

\markboth{I.\,Z.~Emiris, V. Fisikopoulos, C. Konaxis \& L. Pe{\~n}aranda}
{An Algorithm for Projections of Resultant Polytopes}

\title{An Oracle-based, Output-sensitive Algorithm\\
for Projections of Resultant Polytopes}
\author{Ioannis Z.~Emiris\thanks{Department of Informatics \&
        Telecommunications, University of Athens, Athens, 15784, Greece.
        {\tt emiris@di.uoa.gr}, {\tt vfisikop@di.uoa.gr}}
        \and
        Vissarion Fisikopoulos\footnotemark[1]
        \and
        Christos Konaxis\thanks{Archimedes Center for Modeling, Analysis \&
        Computation (ACMAC), University of Crete, Heraklio, 71409, Greece.
        {\tt ckonaxis@acmac.uoc.gr}}
        \and
        Luis Pe{\~n}aranda\textsuperscript{\idagger}
}
\date{}

\maketitle
\blfootnote{\hspace{-3.8pt}\textsuperscript{\idagger}IMPA -- Instituto
Nacional de Matem{\'a}tica Pura e Aplicada, Rio de Janeiro, 22460-320,
Brazil.
{\tt luisp@impa.br}}

\begin{abstract}
We design an algorithm to compute the Newton polytope
of the resultant, known as resultant polytope, or its 
orthogonal projection along a given direction.
The resultant is fundamental in algebraic elimination, optimization,
and geometric modeling.
Our algorithm exactly computes vertex- and halfspace-representations
of the polytope using an oracle producing resultant vertices in a
given direction,
thus avoiding walking on the polytope whose dimension is ,
where the input consists of  points in .
Our approach is output-sensitive as it makes one
oracle call per vertex and facet.
It extends to any
polytope whose oracle-based definition is advantageous, such as
the secondary and discriminant polytopes.
Our publicly available implementation uses the experimental
CGAL package {\tt triangulation}.
Our method computes -, - and -dimensional polytopes
with K, K and  vertices, respectively, within hrs,
and the Newton polytopes of many important surface equations encountered in
geometric modeling in sec,
whereas the corresponding secondary polytopes are intractable.
It is faster than tropical geometry software up to dimension  or .
Hashing determinantal predicates accelerates execution up to  times.  
One variant computes inner and outer approximations with, respectively,
90\% and 105\% of the true volume, up to  times faster.

\paragraph{Keywords: } General Dimension, Convex Hull, Regular
Triangulation, Secondary Polytope, Resultant, CGAL Implementation,
Experimental Complexity.
\end{abstract}

\section{Introduction}

Given pointsets , we define the pointset

where  form an affine basis of :
 is the zero vector,\linebreak
.
Clearly, , where  denotes cardinality.
By Cayley's trick (Proposition~\ref{P:Cayley_trick}) the
regular tight mixed subdivisions of the Mink\-owski sum   
are in bijection with the regular triangulations of , which
are in bijection with the vertices of the {\em secondary polytope} 
(see Section~\ref{Scombinatorics}).

The {\em Newton polytope} of a polynomial is 
the convex hull of its {\em support}, i.e.\
the exponent vectors of monomials with nonzero coefficient.
It subsumes the notion of degree for sparse multivariate polynomials by
providing more precise information (see Figures~\ref{fig:NewPol}
and~\ref{Fbuchberger}).
Given  polynomials in  variables, with fixed supports 
and symbolic coefficients, their {\em sparse (or toric) resultant} 
is a polynomial in these coefficients which vanishes exactly when the
polynomials have a common root (Definition~\ref{Dresultant}).
The resultant is the most fundamental tool in elimination theory,
it is instrumental in system solving and optimization, and is crucial
in geometric modeling, most notably for
changing the representation of parametric hypersurfaces to implicit.

The Newton polytope of the resultant , or \textit{resultant polytope},
is the object of our study; it is of dimension 
(Proposition~\ref{Psummand_dimRes}).
We further consider the case when some of the input coefficients are not
symbolic, hence we seek an orthogonal projection of the resultant polytope.
The lattice points in  yield a superset of the support of ;
this reduces implicitization~\citess{EmKaKoLB,StuYu08} and computation of
 to sparse interpolation (Section~\ref{Scombinatorics}).  
The number of coefficients of the  polynomials ranges from 
for sparse systems,
to , where  bounds their total degree.
In system solving and implicitization, one computes  when all but
 of the coefficients are specialized to constants, hence the
need for resultant polytope projections.  

The resultant polytope is a Minkowski summand of ,
which is also of dimension .
We consider an equivalence relation defined on the 
vertices, where the classes are in bijection with the vertices of 
the resultant polytope.
This yields an oracle
producing a resultant vertex in a given direction, thus avoiding to compute
, which typically has much more vertices than .
This is known in the literature as an {\em optimization} oracle since
it optimizes inner product with a given vector over the (unknown) polytope.

\begin{figure*}[t] \centering
 \includegraphics[width=.8\textwidth]{NewtonPol_example}
 \caption{The Newton polytope of a polynomial of degree  in two variables.
Every monomial corresponds to an integral point on the plane. 
The dashed triangle is the corresponding polytope of the dense polynomial of
degree . 
\label{fig:NewPol}} 
\end{figure*}

\begin{example}\label{ExamBicubic}{\rm [The bicubic surface]}
A standard benchmark in geometric modeling is the implicitization of the
bicubic surface, with ,
defined by  polynomials in two parameters.
The input polynomials have supports
, with cardinalities , respectively;
the total degrees are , respectively. 
The Cayley set , constructed as in Equation~\ref{EQ:Cayley}, has
 points. It is depicted in the following matrix, with coordinates as
columns, where the supports from different polynomials and the Cayley
coordinates are distinguished.
By Proposition~\ref{Psummand_dimRes} it follows that  
has dimension ; it lies in .

\setlength{\tabcolsep}{0pt} \newcommand{\w}{3.8mm}  

Implicitization requires eliminating the two parameters to obtain a
constraint equation over the symbolic coefficients of the polynomials.
Most of the coefficients are specialized except for 
variables, hence the sought for implicit equation of the surface 
is trivariate and the projection of  lies in .

TOPCOM~\citess{RambTOPCOM} 
needs more than a day and GB of RAM to compute 
regular triangulations of , corresponding to  of the vertices of
, and crashes before computing the entire .
Our algorithm yields the projected vertices
 
of the -di\-men\-sional projection of ,
which is the Newton polytope of the implicit equation, in msec.
Given this polytope, the implicit equation of the 
bicubic surface is interpolated in 42 seconds~\citess{EKKL12spm}. 
It is a polynomial of degree~ containing  terms which corresponds
exactly to the lattice points contained in the predicted polytope.
\end{example} 
 
Our main contribution is twofold.
First, we design an oracle-based algorithm for computing the
Newton polytope of , or of specializations of .  
The algorithm utilizes the Beneath-and-Beyond method to compute 
both vertex (V) and halfspace (H)
representations, which are required by the algorithm and may
also be relevant for the targeted applications.
Its incremental nature implies that we also obtain a triangulation of the
polytope, which may be useful for enumerating its lattice points.
The complexity is proportional to the number of output vertices and facets;
in this sense, the algorithms is output sensitive.
The overall cost is asymptotically dominated by computing as many
regular triangulations of  (Theorem~\ref{Ttotalcomplexity}).
We work in the space of the projected  and revert to
the high-dimensional space of  only if needed. 
Our algorithm readily extends to computing , the Newton
polytope of the discriminant
and, more generally, any polytope that can
be efficiently described by a vertex oracle or its orthogonal projection.
In particular, it suffices to replace our oracle by the oracle in
Ref.~\refcite{Rincon12} to obtain a method for computing the discriminant
polytope.

Second, we describe an efficient, publicly available implementation
based on CGAL~\citess{CGAL}
and its experimental package {\tt triangulation}.
Our method computes instances of -, - or -dimensional polytopes
with K, K or  vertices, respectively, in hr.
Our code is faster up to dimensions  or , compared to a method computing 
 via tropical geometry, implemented in the {\tt Gfan} 
library~\citess{JensenYu11}. In higher dimensions {\tt Gfan} seems to perform 
better although neither implementation can compute enough instances for a fair comparison.
Our code, in the critical step of computing the convex hull of the resultant
polytope, uses {\tt triangulation}.
On our instances, {\tt triangulation}, compared to state-of-the-art software
{\tt lrs}, {\tt cdd}, and {\tt polymake}, is the fastest together with {\tt
polymake}.
We factor out repeated computation by reducing the bulk of our
work to a sequence of determinants: this is often the case 
in high-dimensional geometric computing.
Here, we exploit the nature of our problem and matrix structure to capture the
similarities of the predicates, and hash the computed minors which are needed later,
to speedup subsequent determinants.
A variant of our algorithm computes successively tighter inner and outer
approximations: when these polytopes have, respectively,
90\% and 105\% of the true volume, runtime is reduced up to  times.
This may lead to an approximation algorithm.  


\paragraph{Previous work.}
Sparse (or toric) elimination theory was introduced in Ref.~\refcite{GKZ}.
They show that ,
for two univariate polynomials with 
monomials, has  vertices and, when both ,
it has  facets.
In Section~6~of~Ref.~\refcite{St94} is proven that  is -dimensional
if and only if , for all , the only planar 
is the triangle, whereas the only -dimensional ones are the tetrahedron,
the square-based pyramid, and the resultant polytope of two univariate trinomials;
we compute an affinely isomorphic instance of the latter (Figure~\ref{fig:sec_res}(b))
as the resultant polytope of three bivariate polynomials.
Following Theorem~6.2~of~Ref.~\refcite{St94}, the -dimensional polytopes
include
the 4-simplex, some  obtained by pairs of univariate polynomials,
and those of~3 trinomials, which have been investigated with our
code in~Ref.~\refcite{DEF12}. 
The maximal (in terms of number of vertices) such polytope we have computed has
f-vector  (Figure~\ref{fig:sec_res}(c)). 
Furthermore, Table~\ref{tbl:triang_size} presents some typical f-vectors of
-dimensional projections of resultant polytopes. 

A lower bound on the volume of the Newton polytope of the discriminant polynomial 
that refutes a conjecture in algebraic geometry  
is presented in~Ref.~\refcite{discrim_vol}.

A direct approach for computing the vertices of  might
consider all vertices of  since the vertices of
the former are equivalence classes over the vertices of the latter.
Its complexity grows with the number of vertices
of , hence is impractical (Example~\ref{ExamBicubic}).

The computation of secondary polytopes has been efficiently implemented in
TOPCOM~\citess{RambTOPCOM}, which has been the reference software for computing
regular or all triangulations.  
The software builds a search tree with flips as edges over the vertices of .  
This approach is limited by space usage.  
To address this, reverse search was proposed~\citess{IMTI02}, but the
implementation cannot compete with TOPCOM.
The approach based on computing  is not efficient
for computing .
For instance, in implicitizing parametric surfaces with up to 
terms, which includes all common instances in geometric modeling,
we compute the Newton polytope of the equations in less than sec,
whereas  is intractable (see e.g.\ Example~\ref{ExamBicubic}). 

In Ref.~\refcite{MicCoo00} they describe all Minkowski summands of .
In Ref.~\refcite{MicVer99} is defined an equivalence class over
 vertices having the same mixed cells.
The classes map in a many-to-one fashion to resultant vertices;
our algorithm exploits a stronger equivalence relationship.

Tropical geometry is a polyhedral analogue of algebraic geometry 
and can be viewed as generalizing sparse elimination theory.
It gives alternative ways of recovering resultant
polytopes~\citess{JensenYu11} and Newton polytopes of implicit
equations~\citess{StuYu08}. 
See Section~\ref{Simplement} for comparisons of 
the software in Ref.~\refcite{JensenYu11}, called {\tt Gfan},  
with our software.
In Ref.~\refcite{Rincon12}, tropical geometry
is used to define vertex oracles for the Newton polytope of the
discriminant polynomial.

In Ref.~\refcite{Hug06} there is a general implementation of a
Beneath-and-Beyond based procedure which reconstructs a polytope given by a
vertex  oracle. This implementation, as reported in~Ref.~\refcite{JensenYu11}, 
is outperformed by {\tt Gfan}, especially in dimensions higher than . 

As is typical in computational geometry, the practical bottleneck
is in computing determinantal predicates.
For determinants, the record bit complexity is
~\citess{KaVi05},
while more specialized methods exist for the sign of general determinants,
e.g. Ref.~\refcite{BEPP99}.
These results are relevant for higher dimensions and do not exploit the
structure of our determinantal predicates, nor the fact that we deal
with sequences of determinants whose matrices are not very different
(this is formalized and addressed in Section \ref{Shasheddets}).
We compared linear algebra libraries LinBox~\citess{DGGGHKSTV} and
Eigen~\citess{eigenweb}, which seem most suitable in dimension greater than  and
medium to high dimensions, respectively, whereas CGAL provides the most
efficient determinant computation for the dimensions to which we focus.

The roadmap of the paper follows:
Section~\ref{Scombinatorics} describes the combinatorics of resultants, and
the following section presents our algorithm.
Section~\ref{Shasheddets} overcomes the bottleneck of Orientation predicates.
Section~\ref{Simplement} discusses the implementation, experiments, and 
comparison with other software.
We conclude with future work.  

A preliminary version containing most of the presented results 
appeared in Ref.~\refcite{EFKP12}. This extended version contains a more
detailed presentation of the background theory of resultants, applications
and examples, a more complete account of previous work, omitted proofs, 
an improved description of the approximation
algorithm, an extended version of the
hashing determinants method, and more experimental results.  

\section{Resultant polytopes and their projections}\label{Scombinatorics}

We introduce tools from combinatorial geometry~\citess{DeLRamSan,Ziegler}
to describe resultants~\citess{GKZ,CLO2}. 
We shall denote by vol  the normalized Euclidean volume,
 the linear -dimensional functionals,  the affine hull, and 
 the convex hull. 

Let  be a pointset whose convex hull is of dimension . 
For any triangulation  of , define vector 
with coordinate

summing over all simplices  of  having  as a vertex; 
is the convex hull of  for all triangulations .
Let   denote pointset  lifted to 
via a generic lifting function  in .
{\em Regular triangulations} of  are obtained
by projecting the upper (or lower) hull of  
back to .
\begin{proposition}{\rm [Ref.~\refcite{GKZ}]}
The vertices of  correspond to the regular triangulations of ,
while its face lattice corresponds to the poset 
of regular polyhedral subdivisions of , ordered by refinement. 
A lifting vector produces a regular triangulation 
(resp.\ a regular polyhedral subdivision of )
if and only if it lies in the normal cone of vertex 
(resp.\ of the corresponding face) of .  
The dimension of  is . 
\end{proposition}
Let  be subsets of ,
 their convex hulls, and 
 their Minkowski sum.
A \emph{Minkowski (maximal) cell} of  is any full-dimensional convex
polytope , where each  is a convex polytope
with vertices in .
Minkowski cells 
intersect properly when  is a face of both and their
Minkowski sum descriptions are compatible, i.e.\ coincide on the common face.
A \textit{mixed subdivision} of  is any family of
Minkowski cells which partition  and intersect properly.
A Minkowski cell is \textit{-mixed} or \textit{-mixed},
if it is the Minkowski sum of  one-dimensional segments from
, and some vertex . In the sequel we shall call
a Minkowski cell, simply cell.

Mixed subdivisions contain {\em faces} of all dimensions between~0 and ,
the maximum dimension corresponding to cells.
Every face of a mixed subdivision of  has a unique description as
Minkowski sum of .
A mixed subdivision is {\em regular} if it is obtained as the projection
of the upper (or lower) hull of the Minkowski sum of lifted polytopes
, for lifting
.
If the lifting function  is sufficiently generic,
then the mixed subdivision is \emph{tight}, and
, for every cell.
Given  and the affine basis  of ,
we define the Cayley pointset  as in
equation~(\ref{EQ:Cayley}).

\begin{proposition}{\rm [Cayley
trick,~Ref.~\refcite{GKZ}]}\label{P:Cayley_trick}
There exist bijections between:
the regular tight mixed subdivisions of  and
the regular triangulations of ;
the tight mixed subdivisions of  and the triangulations of ;
the mixed subdivisions of  and the polyhedral subdivisions of . 
\end{proposition} 

The family  is \textit{essential}
if they jointly affinely span  and every subset
of cardinality , spans a space of dimension greater than or equal to .
It is straightforward to check this property algorithmically and, if it
does not hold, to find an essential subset \citess{St94}. 
In the sequel, the input  is supposed
to be essential. 
Given a finite , we denote by  the space of all Laurent polynomials
of the form .
Similarly, given  we denote by 
the space of all systems of polynomials

where .
The vector of all coefficients  of \eqref{Esystem} defines 
a point in .
Let  be the set of points corresponding to 
systems \eqref{Esystem} which have a solution in ,
and let  be its closure.  is an irreducible variety defined over .  

\begin{definition}\label{Dresultant}
If {\rm codim}, then the
\textit{sparse (or toric) resultant} of the system of polynomials \eqref{Esystem} 
is the unique (up to sign)
polynomial  in   ,
which vanishes on .
If {\rm codim}, then .
\end{definition}

The resultant offers a solvability condition from which 
has been eliminated, hence is also known as the eliminant. 
For , it is named after Sylvester.
For linear systems, it equals the determinant of the
 coefficient matrix.
The discriminant of a polynomial  is given by the resultant
of  .

The Newton polytope  of the resultant is a lattice polytope called the 
\textit{resultant polytope}. The resultant
has  variables, hence  lies in ,
though it is of smaller dimension (Proposition~\ref{Psummand_dimRes}).
The monomials corresponding to vertices of  are the
extreme resultant monomials.
\begin{proposition}{\rm [Refs.~\refcite{GKZ,St94}]} \label{PSturmf_extreme}
For a sufficiently generic lifting function , 
the -extreme monomial of , whose exponent vector 
maximizes the inner product with , equals

where  ranges over all -mixed cells of the regular tight mixed
subdivision  of  induced by , and  is the
coefficient of the monomial  in .
\end{proposition}
Let  be the regular triangulation corresponding, via the Cayley trick,
to , and  the exponent of the -extreme monomial.
For simplicity we shall denote by , both a cell of   and its 
corresponding simplex in . Then,

where simplex  is -mixed if and only if the corresponding cell 
is -mixed in . 
Note that, , since it is
a sum of volumes of mixed simplices , and each of these volumes is
equal to the \emph{mixed volume}\citess{CLO2} of a set of \emph{lattice}
polytopes, the Minkowksi summands of the corresponding . In
particular, assuming that  is -mixed, it can be written as
, and

where  denotes the mixed volume function which is integer valued for 
lattice polytopes~\citess{CLO2}.
Now,  is the convex hull of all  vectors~\citess{GKZ,St94}.

\begin{figure*}[t] \centering
 \includegraphics[width=\textwidth]{full_generic_grapf_res.pdf}
 \caption{(a) 
The secondary polytope  of two triangles (dark, light grey) and one
segment
,
where  is defined as in Equation~\ref{EQ:Cayley};
vertices correspond to mixed subdivisions of the
Minkowski sum  and edges to flips between them
(b)
, whose vertices correspond to the dashed classes of .
Bold edges of , called cubical flips, map to edges of 
(c)
-dimensional  of 3 generic trinomials with f-vector ;
figure made with {\tt polymake}.
}
\label{fig:sec_res} \end{figure*}

Proposition~\ref{PSturmf_extreme} establishes
a many-to-one surjection from regular triangulations of  to
regular tight mixed subdivisions of , or, equivalently,
from vertices of  to those of .
One defines an {\em equivalence relationship} on all regular tight mixed
subdivisions, where equivalent subdivisions yield the same vertex in . 
Thus, equivalent vertices of  correspond to the same
resultant vertex.
Consider  lying in the union of
outer-normal cones of equivalent vertices of .
They correspond to a resultant vertex whose outer-normal cone
contains ; this defines a -extremal resultant monomial.
If  is non-generic, it specifies a sum of extremal monomials in ,
i.e. a face of .
The above discussion is illustrated in Figure~\ref{fig:sec_res}(a),(b).

\begin{proposition}\label{Psummand_dimRes}{\rm [Ref.~\refcite{GKZ}]}
 is a Minkowski summand of 
, and both  and  have 
dimension 
\end{proposition} 

Let us describe the  hyperplanes in whose intersection lies .
For this, let  be the  matrix whose columns are the
points in the , where each  is followed by
the -th unit vector in .
Then, the inner product of any coordinate vector of  with row 
of  is: constant, for , and known, and depends on ,
for , see Prop.~7.1.11 of Ref.~\refcite{GKZ}.
This implies that one obtains an isomorphic polytope when projecting
 along  points in  which affinely span ;
this is possible because of the assumption of essential family.
Having computed the projection, we obtain  by computing
the missing coordinates as the solution of a linear system:
we write the aforementioned inner products as , where 
is a known matrix and  is a transposed  matrix,
expressing the partition of the coordinates to unknown and known values,
where  is the number of  vertices. 
If the first  columns of  correspond to specialized coefficients, 
, where submatrix  is of dimension 
and invertible, hence .

We compute some orthogonal projection of , denoted
, in :

By reindexing, this is the subspace of the first  coordinates,
so\linebreak
.
It is possible that none of the coefficients  is specialized,
hence ,  is trivial, and .
Assuming the specialized coefficients take sufficiently
generic values,  is the Newton polytope of the corresponding
specialization of .
The following is used for preprocessing.

\begin{lemma}{\rm [Ref.~\refcite{JensenYu11}~Lemma~3.20]}\label{Linsidepoints}
If  corresponds to a specialized coefficient of ,
and lies in the convex hull of the other points in  corresponding
to specialized coefficients, then removing  from  does not
change the Newton polytope of the specialized resultant.
\end{lemma} 

We focus on three applications. First,
we interpolate the resultant in all coefficients, thus illustrating
an alternative method for computing resultants.
\begin{example}\label{ExamGenRes}
Let , , with supports
.
Their (Sylvester) resultant is a polynomial in  .
Our algorithm computes its Newton polytope with vertices
,  , ;
it contains 4 lattice points, corresponding to 4 potential resultant monomials
.
Knowing these potential monomials, to interpolate the resultant, we need 4 
points 
for which the system  has a solution.
For computing these points we use the
parameterization 
of resultants in Ref.~\refcite{Kap91}, which yields:
,  ,
, , 
where the 's are parameters.
We substitute these expressions to the monomials,
evaluate at~4 sufficiently random 's, and obtain a matrix
whose kernel vector  yields
.
\end{example}

Second, consider system solving by the
rational univariate representation of roots~\citess{BaPoRo}. 
Given ,
define an overconstrained system by adding
 with symbolic 's.
Let coefficients , take specific values,
and suppose that the roots of  are isolated, denoted
.
Then the -resultant is
, 
, where  is the multiplicity of .
Computing  is the bottleneck; our method computes 
(a superset of) .

\begin{example}\label{ExamUres}
Let , , and .
Our algorithm computes a polygon with vertices 
, which contains . 
The coefficient specialization is not generic, hence  is
strictly contained in the computed polygon.
Proceeding as in Example~\ref{ExamGenRes},
, which factors as
.
\end{example} 

The last application comes from geometric modeling, where
, , ,
defines a parametric hypersurface. 
Many applications 
require the equivalent implicit representation
.
This amounts to eliminating , so 
it is crucial to compute the resultant 
when coefficients are specialized except the 's.
Our approach computes a polytope
that contains the Newton polytope of , thus reducing implicitization
to interpolation~\citess{EKKL12spm,EmKaKoLB}.
In particular, we compute the polytope of surface equations within sec,
assuming  terms in parametric polynomials,
which includes all common instances in geometric modeling.

\begin{figure*}[t]\centering
\raisebox{6.5mm}{
\includegraphics[width=0.6\textwidth]{polytopesBuch.pdf}}
\qquad \includegraphics[width=0.6\textwidth]{sdivBuch.pdf} 
\caption[]{The supports  of Example~\ref{ExamBuchberger}, their
Newton polytopes (segments) and the two mixed subdivisions of their Minkowski
sum.
\label{Fbuchberger}} 
\end{figure*}

\begin{example} \label{ExamBuchberger} 
Let us see how the above computation can serve in implicitization.
Consider the surface given by the polynomial parameterization 

For polynomials

with supports
 and .
The resultant polytope is a segment in  with endpoints
,   and, actually,
.
The supports and the two mixed subdivisions corresponding to the vertices of
 are illustrated in Figure~\ref{Fbuchberger}.
Specializing the symbolic coefficients of the polynomials as: 

yields the vertices of the implicit polytope: ,
which our algorithm can compute directly.
The implicit equation of the surface turns out to be .
\end{example}


\section{Algorithms and complexity}\label{Sproject}
\newcommand{\Li}{{\cal L}}
 \newcommand{\Sub}{{\cal S}}
 \newcommand{\T}{{T}}
 \newcommand{\J}{{\cal J}}
 \newcommand{\illH}{{\cal H}_{illegal}}
 \newcommand{\W}{{\cal W}}
 \newcommand{\Q}{Q^{H}}
 \newcommand{\Qo}{Q_o^{H}}
 \newcommand{\V}{Q}

This section analyzes our exact and approximate algorithms for computing
orthogonal projections of polytopes whose vertices are defined by an
\emph{oracle}.
This oracle computes a 
vertex of the polytope which is extremal in a given
direction . If there are more than one such vertices 
the oracle returns exactly one of these. Moreover, we define such an oracle for the
vertices of orthogonal projections  of  which results in algorithms for
computing  while avoiding computing . 
Finally, we analyze the
asymptotic complexity of these algorithms.

Given a pointset , {reg\_subdivision()} 
computes the regular subdivision of its convex hull by projecting 
the upper hull of  lifted by , and
{conv()} computes the H-representation of the convex hull of .
The oracle {VTX}() 
computes a point in , extremal
in the direction . 
First it adds to  an infinitesimal symbolic
perturbation vector, thus obtaining . 
Then calls reg\_subdivision(),  that yields 
a regular triangulation  of , since  is generic,
and finally returns . 
It is clear that the 
triangulation  constructed by {VTX} is regular and
corresponds to some secondary vertex  which maximizes the
inner product with . 
Since the perturbation is arbitrarily small, both  also maximize the
inner product with .

We use perturbation to avoid computing non-vertex points on the boundary of .
The perturbation can be implemented in VTX, without
affecting any other parts of the algorithm, either by case analysis or by
a method of symbolic perturbation.
In practice, our implementation does avoid computing non-vertex points on 
the boundary of  by 
computing a refinement of the subdivision obtained by calling 
reg\_subdivision(). 
This refinement is implemented in {\tt triangulation} 
by computing a placing triangulation 
with a random insertion order~\citess{BoiDevHor09} (Section~\ref{Simplement}).

\begin{lemma}\label{Lpointonboundary}
All points computed by {\em VTX} are vertices of .
\end{lemma} 
\begin{proof}
Let .
We first prove that  lies on .
The point  of  is a Minkowski summand of the vertex  of
 extremal with respect to , hence  is extremal
with respect to . 
Since  is perpendicular to projection , 
projects to a point in .
The same argument implies that every vertex , where  is a
triangulation refining 
the subdivision produced by , corresponds to a  resultant vertex
 such that  lies on a face
of . This is actually the same face on which  lies.
Hence  also lies on .

Now we prove that  is a vertex of  
by showing that it does not lie in the 
relative interior of a face of .
Let  be such that 
the face  of  extremal with respect to
 contains a vertex  which projects to
, where  denotes relative interior.
However,  will not be extremal with respect 
to  and since VTX uses the perturbed vector , 
it will never compute a vertex of   whose projection lies inside a face 
of . 
\end{proof}

The \textit{initialization algorithm}
computes an inner approximation of 
in both V- and H-representations (denoted , respectively),
and triangulated.
First, it calls {VTX} for
; the set  is either random or contains,
say, vectors in the  coordinate directions.
Then, it updates  by adding {VTX} and {VTX},
where  is normal to hyperplane  containing ,
as long as either of these points lies outside .
Since every new vertex lies outside the affine hull of the current 
polytope , all polytopes produced are simplices.
We stop when these points do no longer increase . 

\begin{lemma}\label{Linit}
The initialization algorithm computes 
such that .
\end{lemma}
\begin{proof}
Suppose that the initialization algorithm computes a polytope  
such that . Then there exists vertex
,  and vector
 perpendicular to ,
such that  belongs to the normal cone of 
in  and .
This is a contradiction, since such a  would have been computed
as VTX() or VTX(),
where  is normal to the hyperplane  containing .
\end{proof} 

Incremental Algorithm~\ref{AlgComputeP} computes both V- and
H-representa\-tions of  and a triangulation
of , given an inner approximation  of  computed at
the initialization.
A hyperplane  is called \emph{legal}
if it is a supporting hyperplane to a facet of ,
otherwise it is called \emph{illegal}.
At every step of Algorithm~\ref{AlgComputeP}, we compute
 for a supporting hyperplane  of a facet of

with normal .
If , it is a new vertex thus yielding a tighter \textit{inner
approximation} of  by inserting it to , i.e.\ . 
This happens when the preimage   of the facet  of
 
defined by , is not a Minkowski summand of a face of  having 
normal .
Otherwise, there are two cases: either  and , 
thus the algorithm simply decides hyperplane  is legal, or
 and , in which case the algorithm again
decides  is legal but also inserts  to .

The algorithm computes  from , then iterates over the
new hyperplanes to either compute new vertices or decide they are legal,
until no increment is possible, which happens when all hyperplanes
are legal. 
Algorithm~\ref{AlgComputeP} ensures that each normal  to a hyperplane
supporting a facet of  is used only \emph{once}, by storing all used 's in
a set .
When a new normal  is created, the algorithm checks if ,
then calls VTX and updates .
If  then the same or a parallel hyperplane has been
checked in a previous step of the algorithm.
It is straightforward that  can be safely ignored;
Lemma~\ref{Noparallel} formalizes the latter case.


\begin{lemma}\label{Noparallel}
Let  be a hyperplane supporting a facet constructed
by Algorithm~\ref{AlgComputeP}, and  an illegal hyperplane
at a previous step.
If  are parallel then  is legal.
\end{lemma}
\begin{proof}
Let  be the outer normal vectors of the 
facets supported by  respectively.
If  are parallel then  maximizes the 
inner product with  in 
which implies that hyperplane  is legal.
\end{proof}
\begin{algorithm}[ht]
\BlankLine
  \Input{essential  processed by
	Lemma~\ref{Linsidepoints},\\
         projection ,\\
H-, V-repres.~; triang.~ of .}
  \Output{H-, V-repres.~; triang.~ of .}
  \BlankLine\BlankLine
  
  	\hspace{1em}\tcp{Cayley trick}
  \; 
  \lForEach{}{\hspace{2em}}
  \BlankLine
  \While{}{
    select  and \;
     is the outer normal vector of \;
       VTX()\;
      \If {}{
	 \tcp{convex hull computation}
        \ForEach{-face  visible from }{
          
        } 
        \ForEach{}{
	   \tcp{ separates }
	}
        \ForEach{}{
	   \tcp{new hyperplane}
	}
        \;
        \;
      }
  }
  \Return \;
  \BlankLine
  \caption{\label{AlgComputeP} Compute }
\end{algorithm} 

The next lemma formulates the termination criterion of our algorithm.

\begin{lemma}\label{Lwcriterion} 
Let , where  is
normal to a supporting hyperplane  of ,
then  if and only if  is not a supporting hyperplane of .
\end{lemma}

\begin{proof}
Let , where  is a
triangulation refining subdivision  in {VTX}.
It is clear that, since  is extremal with respect to ,
if  then  cannot be a supporting hyperplane of .
Conversely, let .
By the proof of Lemma~\ref{Lpointonboundary}, every other vertex
 
on the face of  is extremal with respect to , hence lies on ,
thus  is a supporting hyperplane of .
\end{proof}

We now bound the {\it complexity} of our algorithm.
Beneath-and-Beyond, given a -dimensional polytope with
 vertices, computes its H-representation and a triangulation
in , where  is the number of full-dimensional faces (cells)
Ref.~\refcite{Josw03bb}.
Let  be the number of vertices and facets of .

\begin{lemma}\label{Loneperhplane}
Algorithm~\ref{AlgComputeP} 
executes {VTX}
at most  
times.
\end{lemma}
\begin{proof}
The steps of Algorithm~\ref{AlgComputeP} increment .
At every such step, and for each supporting hyperplane  of  with normal
, 
the algorithm calls {VTX}
and computes one vertex of , 
by Lemma~\ref{Lpointonboundary}.
If  is illegal, this vertex is unique 
because  separates the set of (already computed) vertices of  from the set
of vertices of  which are extremal with respect to ,
hence, an appropriate translate of  also separates the corresponding sets of
vertices of  (Figure~\ref{fig:oneperhplane}).
This vertex is never computed again because it now belongs to .
The number of {VTX} calls
yielding vertices is thus bounded by .

For a legal hyperplane of , we compute one vertex of 
that confirms its legality; the 
{VTX} call
yielding this vertex is accounted for by the legal hyperplane.
The statement follows by observing that 
every normal to a hyperplane of  is used only once
in Algorithm~\ref{AlgComputeP}
(by the earlier discussion concerning the set  of all used normals).
\end{proof}

\begin{figure}[t]
\centering
\includegraphics[scale=0.9]{intuition3.pdf}
\caption{Lemma~\ref{Loneperhplane}: each illegal hyperplane of  with normal
, separates
the already computed vertices of  (here equal to ) from new ones,
extremal with respect to .  is a polytope
such that .}
\label{fig:oneperhplane}
\end{figure}

Let the size of a triangulation be the number of its cells.
Let  denote the size of the largest triangulation of 
computed by {VTX}, and  that
of  computed by Algorithm~\ref{AlgComputeP}.
In {VTX}, the computation of a regular triangulation reduces to a
convex hull, computed in ;
for  we compute Volume for all cells of  in .
The overall complexity of {VTX} becomes .
Algorithm~\ref{AlgComputeP} calls, in every step, {VTX} 
to find a point on  and insert it to ,
or to conclude that a hyperplane is legal.
By Lemma~\ref{Loneperhplane} it executes {VTX} as many as 
 
times, in 
, and
computes the H-representation of  in .
Now we have,
 and as the input  grows large
we can assume that  and thus  dominates
. Moreover, .
Now, let
 imply that polylogarithmic factors are ignored.

\begin{theorem}\label{Ttotalcomplexity}
The time complexity of Algorithm~\ref{AlgComputeP} to compute 
is ,
which becomes 
when .
\end{theorem}

This implies our algorithm is output sensitive.
Its experimental performance confirms this property, see Section~\ref{Simplement}.

We have proven that oracle {VTX} (within our algorithm)
has two important properties:\-14pt]
\end{enumerate}
The algorithm can easily be generalized to incrementally compute any
polytope 
if the oracle associated with the problem satisfies property~\eqref{oracle1}; if
it satisfies 
also property~\eqref{oracle2}, then the computation can be done in
 oracle calls, where 
,  denotes the number of vertices and number of facets of ,
respectively.
For example, if the described oracle returns  instead of
, it can be used to compute orthogonal projections of secondary
polytopes.

The algorithm readily yields an approximate variant:
for each supporting hyperplane ,
we use its normal  to compute {VTX}.
Instead of computing a convex hull, now simply
take the hyperplane parallel to  through . The set of
these hyperplanes defines a polytope , i.e.\ an
\textit{outer approximation} of .
In particular, at every step of the algorithm,  and  are 
an inner and an outer approximation of , respectively.
Thus, we have an approximation algorithm by stopping Algorithm~\ref{AlgComputeP}
when  achieves a user-defined threshold.
Then,  is bounded by the same threshold.
Implementing this algorithm yields a speedup of up to 25 times 
(Section~\ref{Simplement}).
It is clear that vol is available by our incremental convex hull algorithm.
However, vol is the critical step; we plan to examine algorithms
that update (exactly or approximately) this volume.  

When all hyperplanes of  are checked,
knowledge of legal hyperplanes accelerates subsequent computations of ,
although it does not affect its worst-case complexity.
Specifically, it allows us to avoid checking legal facets against
new vertices.  
 
\section{Hashing of Determinants} \label{Shasheddets}

This section discusses methods to avoid duplication of computations by
exploiting the nature of the determinants appearing in the inner loop of
our algorithm.  
Our algorithm computes many regular triangulations,
which are typically dominated by the computation of determinants.
A similar technique, using dynamic determinant computations, is used to
improve determinantal predicates in incremental convex hull
computations~\citess{FP_ESA12}.

Consider the  matrix with the points of  as columns.
Define  as the extension of this matrix by adding lifting values
 as the last row.
We use the Laplace (or cofactor) expansion along the last row for computing
the determinant of the square submatrix formed by any  columns of
; without loss of generality, we assume these are the first 
columns .
Let   be the vector
resulting from removing the -th element from the vector 
 and let 
be the  matrix obtained from the
 elements of the columns whose indices are in 
.

The Orientation predicate is the sign of the determinant of
,
constructed by columns
 and adding  as the last row.
Computing a regular subdivision is a long sequence of such predicates,
varying 's on each step.
We expand along the next-to-last row, which contains the lifting values,
and compute the determinants

for .
Another predicate is Volume, used by {VTX}.
It equals the determinant of ,
constructed by columns 
and replacing the last row of the matrix by .

\begin{example}\label{ExamHash}
Consider the polynomials
,
 and

and the lifting vector  yielding the matrix .
\setlength{\tabcolsep}{0pt} \newcommand{\w}{6mm} 
We reduce the computations of predicates to computations of minors of the
matrix obtained from deleting the last row of .
Computing an Orientation predicate using Laplace expansion consists of
computing  minors. On the other hand, if we compute
, the computation of
 requires
the computation of only
 new minors.
More interestingly, when given a new lifting , we
compute 
without computing any new minors.
\end{example}

Our contribution consists in maintaining a hash table with the computed
minors, which will be reused at subsequent steps of the algorithm.  
We store all minors of sizes between  and .
For Orientation, they are independent of  and once computed they
are stored in the hash table.
The main advantage of our scheme is that, for a new , the only change
in  are  (nonzero) coordinates in the last row, hence 
computing the new determinants
can be done by reusing hashed minors.
This also saves time from matrix constructions.

Laplace expansion computation of a matrix of size  has
complexity\linebreak
, where  is the cost of computing the
-th minor.  equals  when the -th minor was
precomputed; otherwise, it is bounded by .
This allows us to formulate the following Lemma.
\begin{lemma}
Using hashing of determinants, the complexity of the Orientation and Volume
predicates is  and , respectively, if all minors have
already been computed.
\end{lemma}

Many determinant algorithms modify the input matrix;
this makes necessary to create a new matrix and introduces a constant
overhead on each minor computation.
Computing with Laplace expansion, while
hashing the minors of smaller size, performs better than
state-of-the-art algorithms, in practice.
Experiments in Section~\ref{Simplement} show that our algorithm with hashed
determinants outperforms the version without hash.
For  and , we experimentally observed that the speedup factor
is between 18 and 100; Figure~\ref{fig:gfan_hash} illustrates the second case.

The drawback of hashing determinants is the amount of storage, which 
is in .
The hash table can be cleared at any moment to limit memory consumption, at
the cost of dropping all previously computed minors. Finding a policy
to clear the hash table according to the number of times each minor was
used would decrease the memory consumption, while keeping running times
low.
Exploring different heuristics, such as using a LRU (least recently used)
cache, to choose which minors to drop when freeing memory will be an
interesting research subject.

It is possible to exploit the structure of the above  minor
matrices. Let  be such a matrix, with
columns corresponding to points of .
After column permutations, we split  into four 
submatrices , where  is the identity matrix 
and  has at most one  in each column.
This follows from the fact that the bottom half of every column in 
has at most one  and the last  rows of  contain at least
one  each, unless , which is easily checked.
Now, , with  constructed in .
Hence, the computation of  minors
is asymptotically equal to computing 
an  determinant. 
This only decreases the constant within the asymptotic bound.
A simple implementation of this idea is not faster than Laplace expansion in
the dimensions that we currently focus.  However, this idea should be valuable
in higher dimensions.


\section{Implementation and Experiments} \label{Simplement}

We implemented Algorithm~\ref{AlgComputeP} in C++ to compute ; our code
can be obtained from 
\begin{center}
 \url{http://respol.sourceforge.net}. 
\end{center}
All timings shown in this section were obtained on an Intel Core i5-2400
GHz, with MB L2 cache and GB RAM, running 64-bit Debian
GNU/Linux.

Our implementation, {\tt respol}, relies on CGAL, using mainly a
preliminary version of package {\tt triangulation}~\citess{BoiDevHor09}, 
for both regular
triangulations, as well as for the V- and H-representation of .
As for hashing determinants, we looked for a hashing function, that takes
as input a vector of integers and returns an integer, which
minimizes collisions.
We considered many different hash functions, including some variations of
the well-known FNV hash~\citess{fnv}.
We obtained the best results with the implementation of Boost
Hash~\citess{boosthash}, which shows fewer collisions than the other tested
functions.
We clear the hash table when it contains  minors. This gives a good
tradeoff between efficiency and memory consumption. Last column of
Table~\ref{tab:CHcompare} shows that the memory consumption of our
algorithm is related to  and .

We start our experiments by comparing four state-\-of-\-the-\-art exact
convex hull packages: {\tt triangulation} implementing Ref.~\refcite{CMS93}
and 
{\tt beneath-\-and-\-beyond (bb)} in {\tt polymake}~\citess{GaJo02};
double description implemented in {\tt cdd}~\citess{cddFuku};
and {\tt lrs} implementing reverse search~\citess{Avis98lrs}.
We compute , actually extending the work in Ref.~\refcite{AvBrSe97}
for the new class of polytopes .
The {\tt triangulation} package was shown to be faster in computing Delaunay
triangulations in
 dimensions~\citess{BoiDevHor09}.  
The other three packages are run through {\tt polymake}, where we have ignored
the time to load the data.
We test all packages in an {offline version}. We first compute the
V-representation of  using our implementation and then we give this as
an input to the convex hull packages that compute the H-representation of
. Moreover, we test {\tt triangulation}
by inserting points in the order that Algorithm~\ref{AlgComputeP} computes them,
while improving the point location of these points since we know by the
execution of Algorithm~\ref{AlgComputeP} one facet to be  removed (online version).
The experiments show that {\tt triangulation} and {\tt bb}
are faster than {\tt lrs}, which outperforms {\tt cdd}. 
Furthermore, the online version of {\tt triangulation} is  times
faster than its offline counterpart due to faster point location
(Table~\ref{tab:CHcompare}, Figure~\ref{fig:CHcompare}).

\begin{table*}[t]\footnotesize \centering
\begin{tabular}{@{}crr|rr|rrrr|r}
\multirow{2}{*}{} & 
\multicolumn{1}{c}{\multirow{2}{*}{}} &
\multicolumn{1}{c|}{\# of } &
\multicolumn{6}{c|}{time (seconds)} &
\multicolumn{1}{c}{{\tt respol}} \\
\cline{4-9} & &
\multicolumn{1}{c|}{vertices} &
\multicolumn{1}{c}{{\tt respol}} &
\multicolumn{1}{c|}{{\tt tr/on}} &
\multicolumn{1}{c}{{\tt tr/off}} &
\multicolumn{1}{c}{{\tt bb}} &
\multicolumn{1}{c}{{\tt cdd}} &
\multicolumn{1}{c|}{{\tt lrs}} &
\multicolumn{1}{c}{Mb}\\
\hline
3 & 2490 & 318 & 85.03 & 0.07 & 0.10 & 0.07 & 1.20 & 0.10 & 37 \\
4 & 27 & 830 & 15.92 & 0.71 & 1.08  & 0.50 & 26.85 & 3.12 & 46 \\ 
4 & 37 & 2852 & 97.82 & 2.85 & 3.91  & 2.29 & 335.23 & 39.41 & 64 \\ 
5 & 15 & 510 & 11.25 & 2.31 & 5.57  & 1.22 & 47.87 & 6.65 & 44 \\ 
5 & 18 & 2584 & 102.46 & 13.31 & 34.25 & 9.58 & 2332.63 & 215.22 & 88 \\
5 & 24 & 35768 & 4610.31 & 238.76 & 577.47 & 339.05 & hr & hr & 360 \\
6 & 15 & 985 & 102.62 & 20.51 & 61.56 & 28.22 & 610.39 & 146.83 & 2868 \\
6 & 19 & 23066 & 6556.42 & 1191.80 & 2754.30 & hr & hr & hr &
        6693 \\
7 & 12 & 249 & 18.12 & 7.55 & 23.95 & 4.99 & 6.09 & 11.95 & 114 \\
7 & 17 & 500 & 302.61 & 267.01 & 614.34 & 603.12 & 10495.14 & 358.79 &
        5258 \\
\end{tabular}
\caption{Total time and memory consumption of our code ({\tt respol}) and
time comparison of online version 
of {\tt triangulation (tr/on)} and offline versions of all convex hull
packages for
computing the H-representation of .}
\label{tab:CHcompare} \end{table*}

\begin{figure*}[t] \centering
  \subfigure[]{\label{fig:ch}\includegraphics[width=0.48\textwidth]
{chQ4.pdf}}
\subfigure[]{\label{fig:ch5}\includegraphics[width=0.48\textwidth]
{chQ5.pdf}}
\caption{Comparison of convex hull packages for  -dimensional (a) and
-dimensional (b) . 
{\tt triang\_on}/{\tt triang\_off} are the online/offline versions of {\tt
triangulation} package (y-axis is in logarithmic scale).
\label{fig:CHcompare}}
\end{figure*}


A {\it placing triangulation} of a set of points is a triangulation produced by
the Beneath-and-Beyond convex hull algorithm for some ordering of the points.
That is, the algorithm places the points in the triangulation with respect to
the ordering. Each point which is going to be placed, 
is connected to all
visible faces of the current triangulation resulting to the construction of new
cells. 
An advantage of {\tt triangulation} is that it maintains a placing
triangulation of a polytope in  by storing the -dimensional
cells of the triangulation. This is useful when the oracle VTX needs
to refine the regular subdivision of  which is obtained by projecting the
upper hull of the lifted pointset 
(Section~\ref{Sproject}).
In fact this refinement is attained by a placing triangulation, i.e., by
computing the projection of the upper hull of the placing triangulation of
.
This is implemented in two steps:\n=3,\ m=4|\A| m   n  |\A|>13.

\renewcommand{\tabcolsep}{0.3cm} \begin{table*}[t]\footnotesize
\centering
\begin{tabular}{@{} c@{}c @{\hspace{0.2cm}}
|@{\hspace{0.5cm}}  c  @{\hspace{0.2cm}}  |  
@{\hspace{0.5cm}} rrrrrr  @{}}
\multirow{2}{*}{{ input}}
& & m & 3 & 3 & 4 & 4 & 5 & 5\\ 
& & & 200 & 490& 20 & 30& 17& 20\\\hline
\multirow{2}{*}{{ approximation}} &
& \# of  vertices&15 & 11& 63 & 121 & hr &hr\\
& & & 0.96& 0.95& 0.93& 0.94 & hr
&hr\\
\multirow{2}{*}{{ algorithm}}
& & &1.02& 1.03 & 1.04& 1.03& hr
&hr\\
& & time (sec)& 0.15& 0.22& 0.37& 1.42& hr &hr\\\hline
\multirow{2}{*}{{ uniformly }}
&
& & 34& 45& 123 & 207&  228& 257\\ 
& & random vectors& 606 & 576& 613& 646& 977& 924\\
 \multirow{2}{*}{{ random}}
& & & 0.93& 0.99& 0.94 & 0.90& 0.90& 0.90\\
& & time (sec)& 5.61& 12.78& 1.10&4.73& 8.41& 16.90\\\hline
\multirow{1}{*}{{ exact}}
&
 & \# of 
vertices& 98 & 133& 416& 1296& 1674& 5093\\
\multirow{1}{*}{{ algorithm}}
& & time (sec)& 2.03& 5.87& 3.72& 25.97 &51.54& 239.96\\
\end{tabular}
\caption{Results on experiments computing  using the
approximation algorithm and the random vectors procedure; we stop the
approximation algorithm when
; the results with random vectors
are the average
values over  independent experiments; ``hr'' indicates 
computation of  was interrupted after hr. 
\label{randQ}}
\end{table*}

We analyze the computation of inner and
outer \textit{approximations}  and .
We test the variant of Section~\ref{Sproject} by
stopping it when . In the
experiments, the number of  vertices is  of the 
 vertices, thus there is a speedup of up to  times over
the exact algorithm at the largest instances.
The approximation of the volume is very satisfactory:
 and
 
for the tested instances (Table~\ref{randQ}).
The bottleneck here is the computation of
vol, where
 is given in H-representation: the runtime explodes for .
We use 
{\tt polymake} in every step to
compute vol because we are lacking of an implementation that, given a
polytope  in H-representation, its volume and a halfspace , computes the
volume of the intersection of  and .
Note that we do not include this computation time in the reported time.
Our current work considers ways to extend these observations to a polynomial
time approximation algorithm for the volume and the polytope itself when the
latter is given by an optimization oracle, as is the case here.

Next, we study procedures that compute only the V-rep\-re\-sen\-ta\-tion of
.
For this, we count 
how many \textit{random vectors} uniformly distributed on the
-dimensional sphere are needed to obtain
. 
This procedure runs up to  times faster than the exact algorithm
(Table~\ref{randQ}). 
Figure~\ref{fig:rand} illustrates the convergence of
 to the threshold value  in
typical -dimensional examples. 
The basic drawback of this method is that it does not provide guarantees for
 because we do not have sufficient
{\em a priori} information on .
These experiments also illustrate the extent in which
the normal vectors required to deterministically construct 
are uniformly distributed over the sphere.


\section{Future work}

One algorithm that should be experimentally evaluated is the following. 
We perform a search over the vertices of , that is, we build a search
tree with flips as edges. We keep a set with the extreme vertices with respect
to a given projection. Each computed vertex that is not extreme in the above set is
discarded and no flips are executed on it, i.e. the search tree is pruned in
this vertex. The search procedure could be the algorithm of TOPCOM or the one
presented in Ref.~\refcite{MicVer99} which builds a search
tree in some equivalence classes of . The main advantage of this
algorithm is that it does not involve a convex hull computation. On the other
hand, it is not output-sensitive with respect to 
the number of vertices of the resultant polytope;
its complexity depends on the number of vertices on the \emph{silhouette}
of , with respect to a given projection and those that 
are connected by an edge with them.

As shown, {\tt polymake}'s convex hull algorithm is competitive,
thus one may use it for implementing our algorithm. 
On the other hand, {\tt triangulation} is expected to include fast
enumeration of all regular triangulations for a given (non generic)
lifting, in which case  may be extended by more
than one (coplanar) vertices.

Our proposed algorithm uses an incremental convex hull algorithm and it is
known that any such algorithm has a worst-case super-polynomial \emph{total time
complexity}~\citess{Bremner} in
the number of input points and output facets.
The basic open question that this paper raises is whether there is a polynomial
total time algorithm for  or even for the set of its vertices.

\section{Acknowledgments}
All authors were partially supported from project
``Computational Geometric Learning'', which acknowledges the
financial support of the Future and Emerging Technologies (FET)
programme within the 7th Framework Programme for research of
the European Commission, under FET-Open grant number: 255827.
Most of the work was done while C.~Konaxis and L.~Pe{\~n}aranda
were at the University of Athens.
C.~Konaxis' research leading to these results has also received
funding from the European Union's 
Seventh Framework Programme (FP7-REGPOT-2009-1) under grant agreement n\textsuperscript{o} 245749.
We thank O.~Devillers and S.~Hornus for discussions on
\texttt{triangulation}, and A.\ Jensen and J.~Yu for discussions 
and for sending us a beta version of their code.

\bibliographystyle{unsrt}
\bibliography{algebra,emiris,geometry,bibliography}

\end{document}
