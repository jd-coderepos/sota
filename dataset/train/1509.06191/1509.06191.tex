

\documentclass{daj}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amsfonts, amssymb}
\usepackage{tikz}
\usepackage{thm-restate}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage{enumerate}

\usepackage{bbm}
\newcommand{\1}{\mathbbm{1}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator*{\EE}{E}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\Cov}{Cov}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Inf}{Inf}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbR}{\mathbb{R}}

\definecolor{DSgray}{cmyk}{0,0,0,0.7}
\definecolor{DSred}{cmyk}{0,0.7,0,0.7}
\newcommand{\Authornote}[2]{\noindent{\small\textcolor{DSgray}{\sf{
\textcolor{red}{[#1: #2]\marginpar{\textcolor{red}{\fbox{\Large !}}}}}}}}
\newcommand{\Authormarginnote}[2]{\marginpar{\parbox{2.2cm}{\raggedright\tiny \textcolor{red}{#1: #2}}}}
\newcommand{\Tnote}{\Authornote{Thomas}}
\newcommand{\Tmarginnote}{\Authormarginnote{Thomas}}
\newcommand{\Jnote}{\Authornote{Jan}}
\newcommand{\Jmarginnote}{\Authormarginnote{Jan}}
\newcommand{\Enote}{\Authornote{Elchanan}}
\newcommand{\Emarginnote}{\Authormarginnote{Elchanan}}

\dajAUTHORdetails{title = {Product Space Models of Correlation:
    Between Noise Stability and Additive Combinatorics},
author = {Jan H\k{a}z\l{}a, Thomas Holenstein, and Elchanan Mossel},
plaintextauthor = {Jan Hazla, Thomas Holenstein, and Elchanan Mossel},
keywords = {correlated product spaces, invariance principle, noise stability},
}   

\dajEDITORdetails{year={2018},
number={20},
   received={20 June 2017},   published={27 December 2018},  doi={10.19086/da.6513},       }   

\begin{document}

\begin{frontmatter}[classification=text]




\author[jh]{Jan H\k{a}z\l{}a\thanks{
      During his studies at ETH Zurich
      J.\,H.~was supported by the Swiss National Science Foundation (SNF),
      project no.~200021-132508.
    }}
\author[th]{Thomas Holenstein}
\author[em]{Elchanan Mossel\thanks{
    E.\,M. was supported by NSF grant DMS-1106999, NSF grant CCF 1320105 and DOD ONR grant N000141110140 and grant 328025 from the Simons foundation.
    Part of this work was done while T.\,H.~and E.\,M.~were at the 
    Simons Institute.
  }}

\begin{abstract}
  There is a common theme to some research questions in
  additive combinatorics and noise stability.
  Both study the following basic question: 
  Let  be a probability distribution
  over a space  with all  marginals equal. Let
   be random vectors such
  that for every coordinate  the tuples  
  are i.i.d.~according to .
  
  A central question that is addressed in both areas is: 
  \begin{itemize}
  \item 
    Does there exist a function  independent
    of  such that
    for every  with 
:

Instances of this question include the finite field model versions of Roth's and 
 Szemerédi's theorems as well as Borell's result about the optimality of noise stability 
of half-spaces.
\end{itemize}

Our goal in this paper is to interpolate between the noise stability theory and
the finite field additive combinatorics theory and address the question above
in greater generality than considered before. 
In particular, we settle the question for  and when  and 
has bounded correlation . Under the same conditions we also
characterize the {\em obstructions} for similar lower bounds in the case of
 different functions. 
Part of the novelty in our proof is the combination of analytic arguments from
the theories of influences and hyper-contraction with arguments from
additive combinatorics.
\end{abstract}
\end{frontmatter}

\section{Introduction}

\subsection{Setup and same-set hitting} 
In this paper we analyze a general framework which includes many fundamental
questions in both
the theory of noise stability and in finite field models of additive combinatorics. 
We begin with formally defining this general setting. 
Let  be a finite set  
and assume we are given a probability distribution 
over 
for some  -- we will call it an
\emph{-step probability distribution over }.

Furthermore, assume we are given .
We consider  vectors ,
 such that 
for every , the -tuple  
is sampled according to , independently of the other
coordinates 
(see Figure~\ref{fig:naming} for an overview of the notation). 

\begin{definition}
\label{def:same-hitting}
Let . We say that a distribution
\emph{ is -same-set hitting},
if, for all , whenever a function  satisfies

for every ,
we have


We call  \emph{same-set hitting} if for every 
 there exists  such that
 is -same-set hitting.
\end{definition}

It is not difficult to see that the definition
of same-set hitting is equivalent to the one where functions 
are restricted to be set indicators .
The value 
then can be interpreted as 

for the respective set  of density
at least . This special case motivated the name ``same-set hitting'',
and all our theorems and proofs can be read with that case
in mind.

In this paper we address the question: which distributions 
are same-set hitting? We achieve full characterization for 
and answer the question affirmatively for a large class of distributions
with .

The question of set hitting was studied extensively in additive combinatorics and in the theory of influences and noise stability. Perhaps the most well-studied case is that of random arithmetic progressions. Let  be a finite additive group
and .
Then, we can define a distribution  
of random -step arithmetic progressions in . 
Specifically, for every  we set:


Some of the distributions  can be shown to be
same-set hitting using, e.g., the hypergraph regularity lemma:
\begin{theorem}[\cite{RS04}, \cite{RS06}, \cite{Gow07}, cf.~Theorem 11.27,
Proposition 11.28 and Exercise 11.6.3 in \cite{TV06}]
\label{thm:progressions}
If  is coprime to , then  is same-set hitting.
\end{theorem}

Taking  and  we obtain the classical
formulation of Szemerédi's theorem for progressions of length 
in the finite field model. The special case  is also known
as the \emph{capset problem}.
As is well known, the case  follows from the arguments of
Roth~\cite{Rot53} applied to the finite field setup \cite{Mes95},
while the general case
follows a long line of work, starting by Szemerédi's regularity lemma
\cite{Sze75}, its proof by Furstenberg using the ergodic theorem \cite{Fur77}
as well as the finite group and multi-dimensional versions, see, e.g.,
\cite{Rot53, FK91, Gow01, Green05}.

It is natural to consider a generalization of the question where different
functions are applied to different . This question was studied in the
theories of Gaussian noise stability and hyper-contraction as we explain next.

\subsection{Set hitting} 
The generalization to multiple sets is defined as follows. 
\begin{definition}
\label{def:hitting}
Let . We say that a distribution
\emph{ is -set hitting},
if, whenever functions  satisfy

for every ,
we have


We call  \emph{set hitting} if for every 
 there exists  such that
 is -set hitting.
\end{definition}

Borell~\cite{Bor85} established the set hitting property in the Gaussian case where 
 are i.i.d and 
. 
In fact~\cite{Bor85} does much more: it finds the optimal  in terms of  and  in this case. 
(Note that in this case  is infinite). 

In earlier work,~\cite{Bor82} Borell also proved some of the first
reverse hypercontractive inequalities. 
 These give a different proof that the Gaussian example above is set hitting but also imply 
  the same for the binary analog where 
 satisfy  and .
See~\cite{MOR06} for a discussion of this result and some of its implications.   

The full classification of set hitting distributions can be deduced from
a paper on reverse hypercontractivity\footnote{
That  is set hitting if (\ref{eq:84a}) holds is a consequence
of Lemma 8.3 in \cite{MOS13}. If (\ref{eq:84a}) does not hold,
an appropriate combination of dictators establishes a counterexample.  
}
by Mossel, Oleszkiewicz and Sen
\cite{MOS13}:
\begin{theorem}[\cite{MOS13}]
\label{thm:different-sets-classification}
A finite probability space  is set hitting if and only if:

\end{theorem}

In many interesting settings, including the finite field models in
additive combinatorics, the distribution  does not have full
support. In these settings, as we discuss next,
the goal is to understand sufficient conditions
on the functions which imply that (\ref{eq:85a}) does hold. 

\subsection{Obstructions in additive combinatorics} 
In general much of the interest in additive combinatorics is in understanding
what conditions on functions  imply (\ref{eq:85a}).
For example, the starting point of the proof of Roth's theorem~\cite{Rot53}
on arithmetic progressions of length three is that
if the functions  all satisfy that 
 is small
then (\ref{eq:85a}) holds. That is, the distribution of arithmetic progressions
of length three is set hitting for all functions  with
all (positive degree) Fourier coefficients small in absolute value.
As a matter of fact,
in that case  are known to be pseudorandom in the sense
that .

The proof of Roth's theorem then proceeds roughly as follows: If a function 
is pseudorandom, we are done. Otherwise, we are guaranteed a large Fourier
coefficient. This is then exploited in a density increment argument:
It turns out that a large Fourier coefficient implies that  must
have increased relative density on an affine subspace of 
of codimension one.
One iterates the density increment until  becomes pseudorandom.

A similar situation arises in a more recent proof
for longer arithmetic progression by Gowers:
If the functions  have low Gowers uniformity norm, then 
 (\ref{eq:85a}) holds, see e.g.~\cite{Green05}. 
 
 In one of our main results (see Section~\ref{sec:intro-fourier} below)
 we show that in a pretty general setup (which does not include the additive
 combinatorics setup), the only obstruction for  to hold is for
  to have a large low-degree Fourier coefficient. 

\subsection{Basic example}
\label{sec:basic}

At this point we would like to introduce the simplest example that is not covered by either the theory of influences or techniques from additive combinatorics. 
Let  be a non-empty set of density
.  We pick a random vector
 uniformly from , and then
sample another vector  such that for each
 independently, coordinate  is picked uniformly in
.  Our goal is to show that:

In other words, we want to bound away the probability from 
by an expression which only depends on  and not on .
Similarly, given sets  and  of density at least ,
we want to find under what conditions does it hold that the
probability  can be lower
bounded effectively. We note that the support of the distribution on
 is not full
(hence, Theorem~\ref{thm:different-sets-classification} does not apply)
and that the distribution is not of arithmetic nature. 
\subsection{Our results}


\subsubsection{Same-set hitting for two steps}

In case of  we establish the following theorem:
\begin{theorem}[cf.~Theorem~\ref{thm:main-two-variables}]
\label{thm:two-steps-classification}
A two-step probability distribution with equal marginals 
is same-set hitting if and only if
.
\end{theorem}

Of course, if , then Theorem \ref{thm:two-steps-classification}
follows from Theorem \ref{thm:different-sets-classification}. 
Our work is novel in case ,
i.e., when the distribution is same-set hitting but not set hitting.
In particular we establish same-set hitting
for the probability space from Section \ref{sec:basic}.

\subsubsection{Same-set hitting for more than two steps}

In a general case of an -step distribution with equal marginals, it
is still clear that, letting
, the condition
 is necessary. However, it remains
open if it is sufficient.

We provide the following partial results. Firstly, by a simple inductive argument
based on Theorem \ref{thm:main-two-variables}, we show that multi-step 
probability  spaces induced by Markov chains are same-set hitting
(cf.~Section \ref{sec:markov}).

Secondly, we show that  is same-set hitting 
if  and its 
\emph{correlation}  is smaller than . 
The opposite condition  is equivalent
to the following:
There exist , , 
 such that
 and:

For the full definition of , see~Definition \ref{def:correlation}.

\begin{theorem}[cf.~Theorem \ref{thm:main-multiple}]
\label{thm:rho-hitting}
Let  be a probability distribution with equal marginals.
If  and , then 
is same-set hitting.
\end{theorem}

We are not aware of any general results in case .
In particular, let  be a three-step distribution over
 such that  are uniform
over . To the best of our knowledge,
it is an open question whether this distribution  is same-set 
hitting.
One might conjecture that  is the sole sufficient
condition for same-set hitting.
Unfortunately, the techniques used to prove Theorem \ref{thm:progressions}
do not seem to extend easily to spaces with less algebraic structure.

\subsubsection{Set hitting for functions  with no large Fourier coefficients}
\label{sec:intro-fourier}

The methods developed here also allow to obtain lower bounds on the 
probability of hitting multiple sets. In fact, we show that if , 
then such lower bounds exist in terms of , 
the measures of the sets and the largest non-empty Fourier coefficient. 

\begin{theorem}[Informal, cf.~Theorem \ref{thm:local-variance}]
\label{thm:local-variance-basic}
Let  be a probability distribution with . Then, 
is set-hitting for functions  that have both:
\begin{itemize}
  \item Noticeable expectations, i.e., 
.
  \item No large Fourier coefficients, i.e.,
.
\end{itemize}
\end{theorem}


\subsection{Other related work}

In the case of symmetric two-step spaces
(which can be thought of as product graphs) 
works by Dinur, Friedgut and Regev \cite{DFR08, FR18}
establish a removal lemma: They show
that if  is small,
then it must be possible to remove a small number of elements from 
to obtain  with .
They go on to use this result to characterize all sets with
: It turns out
that every such set must be almost contained in a junta. 
Interestingly, \cite{FR18} obtain
a tower-type dependence between  and  in the removal lemma,
in contrast to ours which is ``merely'' triply exponential.

The case of  has also been studied in the context of extremal 
combinatorics and hardness of approximation.
In particular, Mossel \cite{Mos10} uses the 
invariance principle to prove that if , then  is
set hitting for low-influence functions. We use this result to establish
Theorem \ref{thm:rho-hitting}. Additionally, Theorem 
\ref{thm:local-variance-basic} can be seen as a strengthening of
\cite{Mos10}.

Furthermore, Austrin and Mossel \cite{AM13} establish the result equivalent to 
Theorem \ref{thm:local-variance-basic} assuming in addition to 
also that  is pairwise independent (they also prove results for the case
 with pairwise independence but these involve only bounded degree 
functions).

Our work is related to problems and results in inapproximability in theoretical
computer science. 
For example, our theorem is related to the proof of hardness
for rainbow colorings of hypergraphs by Guruswami and Lee
\cite{GL15}. In particular, it is connected to their Theorem 4.3
and partially answers their Questions C.4 and C.6.

There are works in additive combinatorics that treat 
specific classes of distributions with . For example, one
can take  to be uniform over solutions to a fixed full-rank
system of  linear equations with  variables over .
There is extensive work on removal lemmas (which imply same-set hitting)
for different cases in this setting, see, e.g.,
\cite{Gre05a, KSV09, Sha10, FLS18}.

\paragraph{Follow-up work}
There are two subsequent preprints by some of the authors:
\cite{Mos17} strengthens Theorem~\ref{thm:local-variance} to obtain
precise Gaussian bounds for functions with small low-degree
Fourier coefficients in case  (one can also use the technique
from \cite{Mos17} to deduce an
alternative proof of Theorem~\ref{thm:main-multiple} with roughly
the same dependence).
Another author \cite{SymProg} shows
same-set hitting for symmetric sets for the distribution of
arithmetic progressions with restricted differences mentioned
in Section~\ref{sec:open}.


\subsection{Proof ideas: additive combinatorics and theory of influences}
Interestingly, the proof of our results interpolates between additive
combinatorics and the theory of influences. Results of \cite{Mos10} imply that
if a collection of functions have low influences then they are same-set
hitting. In the proof of Theorem~\ref{thm:main-multiple} we apply a variant of
a density increment argument to reduce to this case. First, we apply the standard
density increment argument to assume without loss of generality that
conditioning on a small number of coordinates does not change the measure of
the set by much. Then we show, under this assumption, by applying another variant
of density increment that we can additionally assume
w.l.o.g.~that all influences are small. 

\subsection{Outline of the paper}

The rest of the paper is organised as follows: the notation is introduced
in Section~\ref{sec:notation}, Section~\ref{sec:results} contains full
statements of our theorems, and Section~\ref{sec:proof-sketch} sketches
the proof of our main theorem.

The full proof of the multi-step theorem follows in 
Section~\ref{sec:main-proof}. The proof of the two-step theorem is in 
Section~\ref{sec:two-steps} and the proof for functions with
small Fourier coefficients in Section~\ref{sec:local-variance}.
A theorem for Markov chains is introduced in Section~\ref{sec:markov}
and better bounds for symmetric spaces in Section~\ref{sec:polynomial-hitting}.
Finally, the modified proof of the low-influence theorem from
\cite{Mos10} is presented in the appendix.
We note that an extended abstract of our results appeared 
in~\cite{HaHoMo:16}. 

\section{Notation and Preliminaries}
\label{sec:notation}

\subsection{Notation}

We will now introduce our setting and notation.
We refer the reader to Figure~\ref{fig:naming} for an overview.

\begin{figure}
\begin{tikzpicture}[yscale=-0.7]
\node(X__) at (0,0) {};
\node(X1_) at (1,0) {};
\node(X2_) at (2,0) {};
\node(X3_) at (3,0) {};
\node(X4_) at (4,0) {};
\node(X5_) at (5,0) {};
\node(X6_) at (6,0) {};

\node(X_1) at (0,1) {};
\node(X11) at (1,1) {};
\node(X21) at (2,1) {};
\node(X31) at (3,1) {};
\node(X41) at (4,1) {};
\node(X51) at (5,1) {};
\node(X51) at (6,1) {};

\node(X_2) at (0,2) {};
\node(X12) at (1,2) {};
\node(X22) at (2,2) {};
\node(X32) at (3,2) {};
\node(X42) at (4,2) {};
\node(X52) at (5,2) {};
\node(X52) at (6,2) {};

\node(X_3) at (0,3) {};
\node(X13) at (1,3) {};
\node(X23) at (2,3) {};
\node(X43) at (4,3) {};
\node(X63) at (6,3) {};

\node(X_4) at (0,4) {};
\node(X14) at (1,4) {};
\node(X24) at (2,4) {};
\node(X34) at (3,4) {};
\node(X44) at (4,4) {};
\node(X54) at (5,4) {};
\node(X64) at (6,4) {};

\node(X_5) at (0,5) {};
\node(X15) at (1,5) {};
\node(X25) at (2,5) {};
\node(X45) at (4,5) {};
\node(X65) at (6,5) {};


\node(X_6) at (0,6) {};
\node(X16) at (1,6) {};
\node(X26) at (2,6) {};
\node(X36) at (3,6) {};
\node(X46) at (4,6) {};
\node(X56) at (5,6) {};
\node(X66) at (6,6) {};

\draw[-] (0.5,-.5) to (0.5,6.5);
\draw[-] (-.5,0.5) to (6.5,0.5);

\node(iid) at (4,-3) {\begin{minipage}{4.2cm}\raggedright
Tuples  are 
i.i.d.~according to .
Each of the  marginals of  is~.\end{minipage}};

\draw[->] (iid) to (X1_.north);
\draw[->] (iid) to (X2_.north);
\draw[->] (iid) to (X4_.north);
\draw[->] (iid) to (X6_.north);

\node[rotate=0](rowdistr) at (-2.5, 4) 
{\begin{minipage}{3cm}\raggedright Vectors 
are distributed (dependently) according to .
\end{minipage}};
\draw[->] (rowdistr) to (X_1.west);
\draw[->] (rowdistr) to (X_2.west);
\draw[->] (rowdistr) to (X_4.west);
\draw[->] (rowdistr) to (X_6.west);

\node(commonDist) at (-1, -3) 
{\begin{minipage}{3.2cm}\raggedright Distributed according to 
.\end{minipage}};
\draw[->] (commonDist) to (X__.north west);

\node(propertiesofP) at (0,9) {\begin{minipage}{5cm}\end{minipage}};

\node(domainsofxij) at (6,10) {\begin{minipage}{4cm}\end{minipage}};

\end{tikzpicture}
\caption{Naming of the random variables in the general case.
The columns  are distributed i.i.d~according to
. 
Each  is distributed according to .
The overall distribution of  is
.}
\label{fig:naming}
\end{figure}

We always assume that we have  independent coordinates.
In each coordinate~
we pick~ values  for  
at random using some distribution.
Each value  is chosen from the same fixed set , and 
the distribution
of the tuple  of
values from  is given by a distribution~.

This gives us values  for  and 
.
Thus, we have  vectors
,
where 
represents the -th step of the random process. In case ,
we might call our two vectors  and  instead.

For reasons outlined in Section \ref{sec:differentMarginals} we assume that
all of  have the same marginal distribution,
which we call . We assume that  is the support of .

Even though it is not necessary,
for clarity of the presentation
we assume that each coordinate  has the 
same distribution .

\medskip

We consistently use index  to index over the coordinates 
(from )
and  to index over the steps (from ).

As visible in Figure \ref{fig:naming},
we denote the aggregation across the coordinates by the underline
and the aggregation across the steps by the overline.
For example, we write ,
, 

and .

We sometimes call  a tensorized, 
multi-step probability distribution
as opposed to a tensorized, single-step distribution 
 and single-coordinate, multi-step distribution
.

Furthermore, we extend the index notation to subsets of indices or steps.
For example, for  we define  to be the
collection of random variables .

We also use the set difference symbol to mark vectors with one element missing,
e.g., .

\medskip

One should think of  and  as constants and of  as large.
We aim to get bounds which are independent of .

\subsection{Correlation}
\label{sec:correlation}
In case , the bound we obtain will depend on the
\emph{correlation} of the distribution .
This concept was used before in \cite{Mos10}.

\begin{definition}\label{def:correlation}
Let  be a single-coordinate distribution
and let . We define the \emph{correlation}:

The correlation of  is
.
\end{definition}

\subsection{Influence}

A crucial notion in the proof of Theorem~\ref{thm:rho-hitting} is
the \emph{influence} of a function. It expresses the average variance of a 
function, 
given that all but one of its  inputs have been fixed to random values:
\begin{definition}
Let  be a random vector over alphabet 
and  be a function
and . The \emph{influence of  on the -th coordinate} is:

The \emph{(total) influence of } is 
.
\end{definition}
Note that the influence depends both on the function  and
the distribution of the vector .

\section{Our Results}
\label{sec:results}

Here we give precise statements of our results presented in the introduction.

\subsection{The case of 
\texorpdfstring{}{l = 2}}

\begin{restatable}{theorem}{maintwovariables}
\label{thm:main-two-variables}
Let  be a finite set and
 a probability distribution over  
with equal marginals .
Let pairs  
be i.i.d.~according to  for 
.

Then, for every  with 
:

where the function  is positive whenever
.
\end{restatable}
We remark that Theorem~\ref{thm:main-two-variables} does not depend
on  in any way. 
This is in contrast to the case .
It is possible to obtain an inverse polynomial bound

for symmetric two-step spaces (see Section~\ref{sec:polynomial-hitting}).

To prove Theorem \ref{thm:main-two-variables} we make
a convex decomposition argument and then apply
the multi-step Theorem \ref{thm:main-multiple}
(see Section~\ref{sec:two-steps}).
For completeness, we provide a proof of 
Theorem \ref{thm:two-steps-classification} assuming Theorem
\ref{thm:main-two-variables}.
\begin{proof}[Proof of Theorem \ref{thm:two-steps-classification}]
The ``if'' part follows from Theorem \ref{thm:main-two-variables}.
The ``only if'' can be seen by taking  to be an appropriate dictator. 
\end{proof}

\subsection{The general case}

\begin{restatable}{theorem}{mainmultiple}
\label{thm:main-multiple}
Let  be a finite set and  a distribution over  in
which all marginals are equal.  Let tuples
 be i.i.d.~according to
 for .

Then, for every function 
with :

where the function  is positive whenever 
and .

Furthermore, there exists some 
(more precisely,  depends on 
,  and ) such that
if , one can take:

\end{restatable}

Note that this bound \emph{does} depend on .
We also obtain a bound that does not depend on 
for multi-step probability spaces generated by Markov chains
(see Section~\ref{sec:markov}).

\subsection{Hitting of different sets by uniform functions}

Finally, we state the generalization of low-influence theorem from
\cite{Mos10}. We assume that the reader is familiar with
Fourier coefficients  and the basics of discrete function 
analysis, for details see, e.g., Chapter 8 of \cite{Dol14}.
Note that this theorem requires neither equal marginals
nor .
For the proof see Section~\ref{sec:local-variance}.

\begin{restatable}{theorem}{localvariance}
\label{thm:local-variance}
Let  be a random vector distributed according
to an -step distribution  with  and
let . 

There exist
 and  (both depending only
on  and ) such that for all functions
,
if  and
, then

\end{restatable}

\subsection{Assumptions of the theorems}

\subsubsection{Equal distributions: unnecessary}
\label{sec:equal-distributions}
In Theorems~\ref{thm:main-two-variables},~\ref{thm:main-multiple}
and~\ref{thm:local-variance}  we assumed that the tuples 
 are distributed 
identically for each .
It is natural to ask if it is indeed necessary.

This is not the case.
Instead, we made this assumption for simplicity of notation and presentation.
If one is interested in statements which are valid where coordinate
 is distributed according to , one simply needs
to assume that there are   and  such 
that
 and .


\subsubsection{Equal marginals: necessary}
\label{sec:differentMarginals}

We quickly discuss the case when  does not have equal marginals.
Recall that . If , then,
by Theorem~\ref{thm:different-sets-classification},  is set hitting,
and therefore also same-set hitting.

In case , we demonstrate an example which shows that 
 
can be exponentially small in . 
For concreteness, we set  and  
and consider  which picks uniformly
among .
We then set \newcommand{\wt}{\mathrm{wt}}

where  is the Hamming-weight of , i.e., the number of ones
in~.

For large enough , a concentration bound 
implies that  
and .
Hence, if we set  to be the indicator function of , 
the assumption of Theorem~\ref{thm:main-multiple} holds.
However, because of the first coordinate we have
,
and the right hand side is easily seen to be exponentially
small.

It is not difficult to extend this example to any distribution 
with  that does not
have equal marginals.

\section{Proof Sketch}
\label{sec:proof-sketch}

In this section we briefly outline the proof of Theorem \ref{thm:main-multiple}. 
For simplicity, we assume that the probability space is the one from 
Section \ref{sec:basic}, i.e.,
 are distributed uniformly in .
Additionally, we assume that we are given a set 
with , so that we want a bound of the form


The proof consists of three steps.
Intuitively, in the first step we deal with dictator sets, e.g.,
,
in the second step with linear sets, e.g.,

and in the third step with threshold sets, e.g.,
.

\subsection{Step 1 --- making a set resilient}

We call a set resilient if  does not change by more
than a (small) multiplicative constant factor whenever conditioned on 
 on a constant number  of 
coordinates.

In particular,  is not resilient (because
conditioning on  increases the measure of the set to ), 
while  and  are.

If a set is not resilient, using  for every 
,
one can find an event 
such that for some constant  we have
 and, at the same time,
 .

Since each such conditioning increases the measure of the set  by a constant 
factor,  must become resilient after a constant number of iterations.
Furthermore,
each conditioning induces only a constant factor loss in
.

It is worth noting that this is the only stage of the proof where
we assume the same-set property (and utilize the assumption
).

\subsection{Step 2 --- eliminating high influences}

In this step, assuming that  is resilient, we condition on a constant
number of coordinates to transform it into two
sets  and  such that:
\begin{itemize}
\item Both of them have low influences on all coordinates.
\item Both of them are supersets of  (after conditioning).
\end{itemize}

The first property allows us to apply low-influence set hitting 
from  to
 and . The second one, together with the resilience of , 
ensures that .

In fact, it is more convenient to assume that we are initially
given two resilient sets  and .

Assume w.l.o.g.~that  for some .
Given , let 
.
Furthermore, let .

Since , we can show that
there exists  such that, after conditioning on
,
the sum  is strictly greater than
the sum :


We choose to disregard the first coordinate and replace
 with  and  with .
Equation (\ref{eq:83a}) implies that after a constant number of such 
operations, neither  nor  has any remaining high-influence coordinates. 

Crucially, with respect to same-set hitting our set replacement
is essentially equivalent to conditioning on
 and .
Therefore, each operation induces only a constant factor loss in 
.

\subsection{Step 3 --- applying low-influence theorem from 
\texorpdfstring{\cite{Mos10}}{[Mos10]}}

Once we are left with two low-influence, somewhat-large sets 
and , we 
obtain

by a straightforward application of a slightly
modified version of Theorem 1.14 from \cite{Mos10}.
The theorem gives that  implies that
the distribution  is set hitting for low-influence functions:

\begin{restatable}{theorem}{lowinfluence}
\label{thm:low-influence}
Let  be a random vector distributed
according to  
such that  has equal marginals, 
and .

Then, for all , there exists 

such that if functions 
 satisfy

then, for :


Furthermore, there exists an absolute constant  such that
for  one can take

\end{restatable}

The proof of Theorem \ref{thm:low-influence} can be found in
Appendix~\ref{sec:low-influence-proof}.
The first part of the appendix contains a short
explanation of differences
between \cite{Mos10} and our version.

\subsection{The case
\texorpdfstring{}{rho = 1}
: open question}
\label{sec:open}

Theorem~\ref{thm:main-multiple} requires that  in order to give a
meaningful bound.  It is unclear whether this is an artifact of our proof or if
it is necessary.  In particular, consider the three step distribution  
which
picks a uniform triple from
.
In other words, sampling from  picks a random
arithmetic progression  with 
and .
One easily
checks that  and that all marginals are uniform.
We do not know if this distribution is same-set hitting.

However, the method of our proof breaks down. 
We illustrate the reason in the following lemma.
\begin{lemma}\label{lem:threeSetFail}
For every  there exist three sets , , and  such 
that for
the distribution  as described above we have
\begin{itemize}
\item .
\item .
\item The characteristic functions  of the three sets all
satisfy

\end{itemize}
\end{lemma}
While the lemma does not give information about whether  is
same-set hitting, it shows that
our proof fails (since the analogue of Theorem~\ref{thm:low-influence}
fails).
\begin{proof}
We let 

Whenever we pick
, the number of
twos in  plus the number of ones in 
plus the number of zeros in  always equals  (there is a
contribution of one from each coordinate).  All three properties are now easy to
check.
\end{proof}

\section{Proof for General
\texorpdfstring{}{l}
and
\texorpdfstring{}{rho(P) < 1}}
\label{sec:main-proof}

The goal of this section is to prove our second main result, which we
restate here for convenience.

\mainmultiple*

\subsection{Properties of the correlation}
\label{sec:correlation-properties}

Recall Definition \ref{def:correlation}.
We now give an alternative characterization of  which will be useful later.
For this, we first define certain random process and an associated Markov chain.
\begin{definition}
\label{def:double-sample}
Let  be a single-coordinate distribution
and let .
We call a collection of random variables 

a \emph{double sample on step  from } if:
\begin{itemize}
\item
   is first sampled according to , ignoring step .
\item Assuming that ,
the random variables  and  are then sampled independently of each other 
according to the
-th step of  conditioned on 
.
\end{itemize}
Sometimes we will omit  from the notation 
and refer as double sample to  alone.
\end{definition}

An equivalent interpretation of a double sample
is that after sampling  according to
 we ``forget'' about  and sample  again from the same distribution
(keeping the same value of ).
Therefore, both
 and 
are distributed
according to .

If we let

we see that

which means that  is the kernel of a Markov chain that is reversible with
respect to  (see e.g., \cite[Section 1.6]{LevinPW08}).  Thus,  has an
orthonormal eigenbasis with eigenvalues
, 
(e.g., \cite[Lemma 12.2]{LevinPW08}).
We will say that  is the \emph{Markov kernel induced by the double sample
  }.
  
A standard fact from the Markov chain theory
expresses  in terms of covariance of functions
:
\begin{lemma}[Lemma 13.12 in \cite{LevinPW08}]
  Let  be two consecutive steps of a reversible Markov chain with kernel
   such that both  and  are distributed according to a stationary
  distribution of . Then,
\label{lem:lambda-lpw}

\end{lemma}

\begin{lemma}
\label{lem:lambda-rho}
Let  be a single-coordinate distribution and let 
 be a double sample from 
that induces a Markov kernel .
Then,

\end{lemma}
\begin{proof}
  For readability, let us write  instead of
  .

  Consider first two functions  and  as in
  Definition~\ref{def:correlation} and assume without loss of generality that
  .
  Of course, we also assume that
   as specified by
  Definition~\ref{def:correlation}. We will show that

and that there exists a choice of  and  that achieves equality
in (\ref{eq:73a}).

Let  and observe
that
\begin{IEEEeqnarray}{rCl}
\EE[f(Y)f(Z)] &=& \sum_{\overline{x},y,z} \Pr[\overline{X}=\overline{x}]
\Pr[Y=y\mid \overline{X}=\overline{x}]\Pr[Z=z\mid \overline{X}=\overline{x}]
f(y)f(z) \nonumber
\\ &=& 
\EE[h(\overline{X})^2] \; .
\label{eq:71a}
\end{IEEEeqnarray}

Now, by Cauchy-Schwarz, (\ref{eq:71a}) and Lemma \ref{lem:lambda-lpw} 
we see that
\begin{IEEEeqnarray*}{rCl}
\Cov[f(Y),g(\overline{X})]^2 &=& \EE[f(Y)g(\overline{X})]^2 = 
\EE[h(\overline{X})g(\overline{X})]^2
\le \EE[h(\overline{X})^2] \EE[g(\overline{X})^2] \\
&=& \EE[h(\overline{X})^2]
= \EE[f(Y)f(Z)] \le \lambda_2(K) \; . 
\end{IEEEeqnarray*}
The equality is obtained for  that maximizes the right-hand side of 
(\ref{eq:72a}) and  for some .
\end{proof}

For later use, we make the following implication of Lemma \ref{lem:lambda-rho}.
\begin{corollary}\label{cor:varianceToEdgeVar}
Let  be a double sample on step  from a single-coordinate
distribution  with .
Then, for every function ,

\end{corollary}
\begin{proof}
Assume w.l.o.g.~that .
By Lemmas \ref{lem:lambda-lpw} and \ref{lem:lambda-rho},

\end{proof}

\subsection{Reduction to the resilient case}
In this section, we will prove that we can assume that 
the function  is \emph{resilient} in the following sense:
whenever we fix a constant number of inputs to some 
value, the expected value of  remains roughly the same.

The intuitive reason for this is simple: if there is some way to fix
the coordinates which changes the expected value of , we
can fix these coordinates such that the expected value \emph{increases}, 
which only makes our task easier
(and can be done only a constant number of times).

\medskip

We first make the concept of ``fixing'' a subset of the coordinates formal.

\begin{definition}
Let  be a function.
A \emph{restriction}  is a sequence 
where each  is either an element ,
or the special symbol .

The coordinates with  are \emph{unrestricted}, the coordinates
where  are \emph{restricted}.
The \emph{size} of a restriction is the number of restricted coordinates.

A restriction  operates on a function  as

where  if  and  otherwise.
\end{definition}

Next, we define what it means for a function to be resilient:
restrictions do not change the expectation too much.
\begin{definition}
Let  be a random vector distributed according
to a (single-step) distribution .
A function  is \emph{-resilient up to
size } if for every restriction  of size at most  we have that
.
\end{definition}

The function is upper resilient if the expectation cannot increase too much.
\begin{definition}
Let  be a random vector distributed according
to a distribution .
A function  is \emph{-upper resilient
up to size } if for every 
restriction  of size at most  we have that
.
\end{definition}

Resilience and upper resilience are equivalent up to a multiplicative
factor which depends only on  and the smallest probability in the
marginal distribution  .
Intuitively the reason is that if there is some restriction which
decreases the -norm, then some other restriction on the same
coordinates must increase the -norm somewhat.

\begin{lemma}
\label{lem:stability-equivalence}
Suppose that a function  is -upper resilient up to size .
Then,  is -resilient up to size , where 
.
\end{lemma} 
\begin{proof}
Fix a subset  of the coordinates of size .
We consider a random variable  whose values are
restrictions with restricted coordinates being exactly .
The elements  for  are picked according to the 
distribution .
We let  be the probability a certain restriction  is picked, and
get

where we sum over all restrictions  that restrict exactly the coordinates
in .

Let now  be one of the possible choices for .
Then,

and hence:

Since  we get the bound
for the restriction , which was chosen arbitrarily.
\end{proof}

\begin{lemma}\label{lem:restrictToMakeStable}
Let  be a random vector distributed according to
a distribution with equal marginals 

and  be a function
with .

Let . Then, there exists
a restriction  such that  is -resilient
up to size  and

where  
with .
\end{lemma}

In particular,  depends only on  and 
(requiring ).

\begin{proof}
Let  and
choose a restriction  such that 
.
We repeat this, replacing  with , until there is no such 
restriction.

Since the expectation of  only increases, we get (\ref{eq:241a}).
Finally, once the process stops, the resulting function is -resilient
due to Lemma~\ref{lem:stability-equivalence} 
(note that ).

It remains to argue that (\ref{eq:3a}) holds for the resulting function.
Note first that the expectation cannot exceed~, and
hence the process will be repeated at 
most  times.
Therefore, the final restriction  obtained after at most  iterations
of the process above is of size at most .

Define  and
let  be the event that all 
strings 
agree with the restriction  in its restricted coordinates.
We will use  to denote the function which is 
 if event  happens and  otherwise.
We see that

Finally,

\end{proof}

\subsection{Reduction to the low-influence case}
We next show that if  is resilient, we can also assume that it has
only low influences.
However, this part of the proof actually produces a collection of functions 
 such that each of them
has small influences: it operates differently on each function.
In turn, it is more convenient to do this part of the proof also starting 
from a collection , as long as all of
them are sufficiently resilient.

\medskip

As in the previous section, we use restrictions.
Here, however, we are only interested in restrictions of size one.
Consequently, we write  to denote 
the restriction 
with  and  for .

Furthermore, we require a new operator.
\begin{definition}
Let , ,
and fix values .

We define the operator  as

\end{definition}

The operator  is useful for two reasons.
First, if  is ``large'', then

for some  and .  This implies that we can use this
operator to increase the expectation of a function unless all of its
influences are small. We will prove this property later.

Second, fix a step  and assume that for some values

both conditional probabilities

and
 are ``somewhat large'' (larger than some constant).
We imagine now that 

and that we have also picked all values
.
We then hope that  is picked among  and  such that it
maximizes~.  Since this happens with constant probability, we
conclude the following: Suppose we replace  with
 and then prove that afterwards
 is large.  Then,
 was large before.

This second point is formalized in the following lemma:
\begin{lemma}\label{lem:maximumKeepsProbability}
  Let  be a random vector distributed according to
  .  Fix ,
  
  and
  .  Suppose that:

Let , and
for  define:

Then:

\end{lemma}
\begin{proof}
We first define a random variable , 
which is the value among  and  which  needs to take 
in order to maximize . 
Formally,

Consider now the event  which occurs if 
.
We get


The equality from the first to the second line follows because
if the event  happens, then the functions 
 
and  are equal.
From the third to the fourth line we use that conditioned on 
 the 
functions  are constant.
Finally, the last inequality follows because by (\ref{eq:845a})
and~(\ref{eq:846a}), for every choice of
 event 
has probability at least .
\end{proof}

The obvious idea for the next step would be to 
find values 
such that 

and fix them.

Unfortunately, there is a problem with this strategy.  To replace 
the function  with
, Lemma~\ref{lem:maximumKeepsProbability} also
replaces  with  for  (and this is
required for the proof to work).  Unfortunately, it is possible that
.
We remark that we \emph{cannot} use that  is resilient here: while
 is resilient the first time we condition, the functions
 obtained in the subsequent steps are not resilient in general,
so later steps will not have the guarantee.

\medskip

Our solution is to pick the values  at
random, as a double sample on coordinate  
(cf.~Definition \ref{def:double-sample}).
Let:

Note that the random variable  is part of the double sample
 and sampled separately (and independently)
from the random vector . In particular, it should
not be confused with the ``input'' random variable .
We prove that (in expectation over 
) the sum of expectations
 is greater by a constant than
the sum .
To argue that the sum of expectations increases, the key part is to show that
 increases by a constant.

\begin{lemma}\label{lem:fjNormIncrease}
Let  be a double sample from a
single-coordinate distribution .

Let  be a random vector, independent of this double sample
and distributed according to a single-step
distribution  such that
 is the -th marginal distribution of .

Then, for every  and every function 
 we have

where .
\end{lemma}
Recall that the distribution of  depends on .
We do not need to
consider the full multi-step process in this lemma, but when applying it later
we will set  and .

\begin{proof}
Fix a vector  for 
,
and define the function  as
.
By Corollary~\ref{cor:varianceToEdgeVar},

and hence, averaging over ,


Since  and  are symmetric (i.e., they define a reversible Markov chain,
cf.~remarks after Definition \ref{def:double-sample}) and by (\ref{eq:19a}),

as claimed.
\end{proof}

\begin{lemma}\label{lem:averageNormIncrease}
Let a random vector  be distributed according to
 and functions
.
Let ,  and  be such that
 and let .

Pick a double sample  from  and let:

Then:

\end{lemma}
Note that (\ref{eq:9a}) defines the functions  as random variables
which is why we use capital letters.
\begin{proof}
If  we have

since the marginal distribution of  is exactly as in the 
marginal  of .
Hence, it suffices to show that

but this is exactly Lemma~\ref{lem:fjNormIncrease}.
\end{proof}

\begin{lemma}
\label{lem:second-reduction-single-step}
Let  be a random vector distributed according to
 and also let
,
, , ,
.

Then, there exist values 

such that the functions

satisfy

\end{lemma}
While (\ref{eq:15a}) is immediate from
Lemma~\ref{lem:averageNormIncrease}, we have to do a little bit of work 
to guarantee that it holds simultaneously with (\ref{eq:16a}).
\begin{proof}
Choose  as a double sample
from  and let  be defined as in
(\ref{eq:9a}).

Define 
,
,
an event 
and a random variable

By Lemma \ref{lem:averageNormIncrease}, we have .

Since there are  possible tuples
, by union bound we have
.
Bearing in mind the above and that
,


As a consequence, we can choose 
such that  and
 does not happen. (\ref{eq:15a}) is now immediate, while for
(\ref{eq:16a}) observe that  implies

and 
and apply Lemma
\ref{lem:maximumKeepsProbability}.
\end{proof}

We can now repeat the process from Lemma~\ref{lem:second-reduction-single-step}
multiple times to get the result of this section.

\begin{corollary}
\label{cor:low-influence-reduction}
Let  be a random vector distributed according
to  with 
.
Then, for every  there exist  and 
 such that:

For every  and functions
 such that each
 is -resilient up to size , there exist

with the following properties:
\begin{enumerate}
\item
  .
\item .
\item For all :
  .
\end{enumerate}
Furthermore, one can take 
and .
\end{corollary}
In particular, both  and  depend only on  and 
(requiring  and ).
\begin{proof}
We repeat the process from Lemma~\ref{lem:second-reduction-single-step},
always replacing the collection of functions 
with  until condition  is satisfied.
Since  cannot exceed 
and every time it increases by , we have to do this at most
 times.

The first point is then obvious, and the second point follows
from Lemma~\ref{lem:second-reduction-single-step}.

Finally, the third point follows 
because the functions  are all -resilient up to size
, and each of the functions  can be written as 
a maximum of restrictions
of size at most  of . Since the maximum only increases expectations, the proof follows. 
\end{proof}

\subsection{Finishing the proof}
\begin{proof}[Proof of Theorem~\ref{thm:main-multiple}]
  Let us assume that , the computations being only easier if
  this is not the case. To establish (\ref{eq:76a}), whenever we say
  ``constant'', in the  notation or otherwise, we mean ``depending only on
   (in particular, on , ,  and ),
  but not on ''.

The proof consecutively applies Lemma \ref{lem:restrictToMakeStable},
Corollary \ref{cor:low-influence-reduction} and
Theorem \ref{thm:low-influence}.

Given  with
, first apply Lemma
\ref{lem:restrictToMakeStable} to  with  and 
 for a constant  large enough
(where ``large enough'' will depend on another constant  to be defined
later).
This gives us a function  such that:
\begin{itemize}
\item  is -resilient up to size .
\item .
\item 

where:
\begin{IEEEeqnarray*}{rCl}
  c &=& 1/\exp\left( \left(1/\alpha\right)^{2k} \cdot 4\ln 1/\mu \right) \ge
  1/\exp\left( \exp\left(O\left(k\right)\right) \cdot 4\ln 1/\mu \right) 
  \\ &\ge&
  1/\exp\left(\exp\left(\exp\left(\left(1/\mu\right)^{O(1)}\right)\right) \cdot
    4\ln 1/\mu\right) 
  \\ &\ge& 
  1/\exp\left(\exp\left(\exp\left(\left(1/\mu\right)^{O(1)}\right)\right)\right) \; .
\end{IEEEeqnarray*}
\end{itemize}

Next, apply Corollary \ref{cor:low-influence-reduction}.
Set  and 
 for a constant  large 
enough.
We need to check if  we have chosen satisfies the assumption
of Corollary \ref{cor:low-influence-reduction}:

Therefore, Corollary \ref{cor:low-influence-reduction} is applicable
and yields 
such that:
\begin{itemize}
\item
  .
\item
  .
\item

where:
\begin{IEEEeqnarray*}{rCl}
  \beta &=& 
  \left(\frac{\tau(1-\rho^2)}{2\ell|\Omega|^{\ell+1}}\right)^k
  \ge
  1/O\left(\exp\left(\left(1/\mu\right)^{D'}\right)\right)^k
  \\ &\ge&
  1/\exp\left(\left(1/\mu\right)^{O(1)} \cdot k\right)
  \ge 1/\exp\left(\exp\left(\left(1/\mu\right)^{O(1)}\right)\right) \; .
\end{IEEEeqnarray*}
\end{itemize}

Finally, we need to apply Theorem \ref{thm:low-influence}. To this end, set:

and verify (\ref{eq:38a}):
\begin{IEEEeqnarray*}{rCl}
  \left(\frac{(1-\rho^2)\epsilon}{\ell^{5/2}}
  \right)^{O\left(\frac{\ln(\ell/\epsilon) \ln(1/\alpha)}{(1-\rho)\epsilon}\right)}
  &\ge&
  \Omega\left(\epsilon\right)^{\left(O(1) + \ln 1/\epsilon\right)
    \cdot O\left(1/\epsilon\right)}
  \\ &\ge&
  1/\exp\left( \left(O(1) + \ln 1/\epsilon \right)^2 \cdot 
    O\left(1/\epsilon\right)  \right)
  \\ &\ge&
  1/\exp\left( \left(1/\epsilon\right)^{O(1)} \right)
    \ge 1/\exp\left(\left(1/\mu\right)^{O(1)}\right) \; .
\end{IEEEeqnarray*}
Hence, from Theorem \ref{thm:low-influence}:

(\ref{eq:77a}), (\ref{eq:78a}) and (\ref{eq:79a}) put together give:

as claimed.
\end{proof}

\section{Proof for Two Steps}
\label{sec:two-steps}

Our goal in this section is to prove Theorem \ref{thm:main-two-variables}
assuming Theorem \ref{thm:main-multiple}.

In the following  we will sometimes drop the assumption that  is 
necessarily the support of a probability distribution . 
One can check that this will not cause problems.

\subsection{Correlation of a cycle}

Assume we are given a support set  of size
. Let  and let 
be a sequence of distinct .

\begin{definition}
We call a probability distribution  over 
an \emph{-cycle} if

\end{definition}

\begin{lemma}
\label{lem:cycle-rho}
Let  be an -cycle. Then

\end{lemma}

\begin{proof}
Let  be the Markov kernel induced by a double sample on 
( is the same whether a sample is on the first or the second step,
cf.~Section \ref{sec:correlation-properties}). Observe that

Let .
One can check that the eigenvalues
of  are 
with . 
This is easiest if one knows the respective (complex) eigenvectors 
 
(where  is the imaginary unit).

Using  for 
and  for  we obtain that if ,
then


The bound on  now follows from Lemma \ref{lem:lambda-rho}.
\end{proof}

\subsection{Convex decomposition of 
\texorpdfstring{}{P}}

In this section we show that if a distribution  
can be decomposed into a convex combination
of distributions  and each 
distribution  is same-set hitting, 
then also  is same-set hitting.

\begin{definition}
We say that a probability distribution with equal marginals  has an
\emph{}-convex decomposition if there exist 
 with 
and distributions with equal marginals
 such that

and  and  for every .
\end{definition}

\begin{lemma}
\label{lem:convex-decomposition}
Let an -step distribution  with equal marginals have an
-convex decomposition
for some  and . 

Then, for every function  with 
:

\end{lemma}

\begin{proof}
Let us write the relevant decomposition as .
The existence of this decomposition implies that there exists a random vector
 such that:
\begin{itemize}
  \item The variables  are i.i.d.~with .
  \item For every  and , conditioned on ,
    the tuple  is distributed according to .
\end{itemize}

Let  be an arbitrary assignment to  and let
.
If , by Theorem \ref{thm:main-multiple}\footnote{
  Technically, Theorem \ref{thm:main-multiple} requires the distributions to
be the same for each coordinate, which is not the case in our setting. However,
this is not a problem, cf.~Section \ref{sec:equal-distributions}.
}


Since , by Markov

(\ref{eq:86a}) and (\ref{eq:87a}) together give
 
\end{proof}

\subsection{Decomposition of 
\texorpdfstring{}{P} 
into cycles}

\begin{definition}
Let us consider weighted directed graphs with non-negative weights 
over a vertex set . We will identify such a digraph 
with its weight matrix.

We say that such a weighted digraph is \emph{regular}, if for every vertex
the total weight of the incoming edges is equal to the total weight of
the outgoing edges.

We call a weighted digraph a \emph{weighted cycle}, if it is a directed cycle 
over a subset of  with all edges of the same weight .
We call  the \emph{weight} of the cycle and number of its edges 
the \emph{size} of the cycle.

We say that a weighted digraph  can be 
\emph{decomposed into  weighted cycles}
if there exist weighted cycles   such that 
.
\end{definition}

\begin{lemma}
\label{lem:digraph-decomposition}
Every regular weighted digraph  over a set  of size  
can be decomposed into at most  weighted cycles.
\end{lemma}
\begin{proof}
Since the digraph is regular, it must have a cycle. Remove it from the graph
(taking as weight  the minimum weight of the edge on this cycle).

Since the resulting graph is still regular, 
proceed by induction until the graph is empty.

At each step at least one edge is completely removed from the graph,
therefore there will be at most  steps.
\end{proof}

To see that a two-step distribution  can be decomposed into cycles, 
it will be useful to take  and
look at it as a weighted directed graph ,
where  is interpreted as a weight function 
.

\begin{lemma}
\label{lem:cycle-decomposition}
Let  be a two-step distribution 
with equal marginals 
over an alphabet  with size .

Then,  has a convex decomposition 
 such that each  either
has support of size  or is
an -cycle with  and . 

Consequently,  has an -convex decomposition with
 and .
\end{lemma}

\begin{proof}
Throughout this proof we will treat  as a weight matrix of
a digraph. Since  has equal marginals, this weighted digraph is regular.
Use Lemma \ref{lem:digraph-decomposition} to decompose
 into weighted cycles, which allows us to write

where  is a weighted cycle with weight  and size 
and .
Take  and let
 be the identity matrix restricted to the support of . Now we
can write  as


Firstly, 
can be decomposed into distributions with support size .

As for the other term, note that 
 is a probability
distribution that
either has support of size  (iff  has
support of size ) or is an -cycle with 
 and .

If , then . If , then
.
Therefore, , as stated.

Consequently, 
and, by Lemma \ref{lem:cycle-rho}, 
 and, since
every -cycle has equal marginals,
we obtained
an -convex decomposition of .
\end{proof}

\subsection{Putting things together}

\begin{proof}[Proof of Theorem \ref{thm:main-two-variables}]
From Lemmas \ref{lem:cycle-decomposition} and
\ref{lem:convex-decomposition}.
\end{proof}

\begin{remark}
One can see that see that, as in Theorem \ref{thm:main-multiple}, we obtain
a triply exponential explicit bound, i.e, there exists 
 such that if , then

\end{remark}

\section{Local Variance}
\label{sec:local-variance}

In this section we state and prove a generalization of the low-influence theorem 
from \cite{Mos10}. We assume that the reader is familiar with
Fourier coefficients  and the basics of discrete function 
analysis, for details see, e.g., Chapter 8 of \cite{Dol14}. 

\cite{Mos10} shows that  implies that  is set hitting
for low-influence functions. We extend this result to
a weaker notion of influence. In particular, we show that  is set hitting
for functions with  measure and  largest Fourier coefficient.
The main result of this section is Theorem~\ref{thm:local-variance}.

We remark that Theorem~\ref{thm:local-variance} 
does \emph{not} require equal marginals.
The rest of this section contains the proof of Theorem~\ref{thm:local-variance}.
First, from Corollary \ref{cor:low-influence-reduction} and
Theorem \ref{thm:low-influence} it is easy to establish\footnote{
One needs to check that the assumption about equal marginals is not necessary,
but that turns out to be the case (the bound in Theorem \ref{thm:low-influence}
then depends on ).
} the following:
\begin{theorem}\label{thm:stabilityExpectation}
Let  be a random vector distributed according to
an -step distribution  with  and
let .

Then, for
all  there exists
 
such that for all functions
,
if  and
if  are all -resilient up to size
, then

\end{theorem}

\begin{definition}
Let  be a single-step distribution and let 
 be a 
function. Let  with .
We define  as

where , 
is the vector  restricted to coordinates in ,
and  is a random vector of
 elements with each coordinate distributed i.i.d.~in .
\end{definition}

A proof of the following claim can be found, e.g., in~\cite{Dol14}:
\begin{claim}
\label{cl:fourier-vs-variance}
Let  be a single-step distribution and let 
, .
If a random vector  
is distributed according to 
and  form a Fourier basis for  and
, then
.
In particular,

\end{claim}

\begin{lemma}\label{lem:localInfluenceToStability}
Let a random vector  be distributed according to a single-step
distribution  with 
 and
let , .

Then, for every 
with , if for every
 with  it holds that

then  is -resilient up to size .
\end{lemma}
\begin{proof}
We prove the contraposition. 

If  is not -resilient up to size , by definition
of  it implies that there exist  with
 and  such that

But this gives

as required.
\end{proof}

Using Lemma~\ref{lem:localInfluenceToStability} we can weaken
the assumption in Theorem~\ref{thm:stabilityExpectation}
such that it only requires that all Fourier coefficients of degree at most 
are small:

\begin{proof}[Proof of Theorem~\ref{thm:local-variance}]
From Theorem \ref{thm:stabilityExpectation}, there exists
 such that if
 are all -resilient up to size ,
then (\ref{eq:89a}) holds. Therefore, it is sufficient to show that
the functions  are indeed -resilient up to size 
if the parameter  is chosen small enough.

By Claim~\ref{cl:fourier-vs-variance}, if , then for any
 with  we have
. With that in mind it is easy to choose 
such that Lemma 
\ref{lem:localInfluenceToStability} 
can be applied to each .
\end{proof}

\section{Multiple Steps of a Markov Chain}
\label{sec:markov}

Next, we consider the case where the distribution 
is such that the random variables
 form a Markov chain.

\begin{definition}
Let  be a an -step distribution with equal marginals and let
 be a random variable distributed
according to . We say that
\emph{ is generated by Markov chains}\footnote{
Note that our definition allows for different Markov chains
in different steps.
} if for every  and
 we have
\begin{IEEEeqnarray*}{rCl}
\IEEEeqnarraymulticol{3}{l}{
  \Pr[{X^{(j)} = x^{(j)}}  | { X^{(1)} = x^{(1)}} \land 
  \dots \land {X^{(j-1)} = x^{(j-1)}}]
}
\\ \qquad &=& 
\Pr[X^{(j)} = x^{(j)} | X^{(j-1)} = x^{(j-1)}] \; .
\end{IEEEeqnarray*}
\end{definition}

Observe that since we still require  to have equal marginals,
the mar\-ginal  is then simply a stationary distribution of the chain.

In this case, we give a reduction to Theorem~\ref{thm:main-two-variables}
to prove a bound that does not depend on :

\begin{restatable}{theorem}{mainmarkov}
\label{thm:main-markov}
Let  be a finite set and  a probability distribution over 
 with equal marginals
generated by Markov chains.
Let tuples  be 
i.i.d.~according to  for .

Then, for every  with 
:

where the function  is positive whenever 
.
\end{restatable}

\begin{proof}
Let  be a distribution generated by Markov chains
with  and let 
with .

The proof is by induction on . For , apply 
Theorem \ref{thm:main-two-variables} directly. For , define
the function  as

Applying Theorem \ref{thm:main-two-variables} for the distribution
of the last two steps,


Now we have
\begin{IEEEeqnarray}{rCl}
\EE\left[ \prod_{j=1}^{\ell} f(\underline{X}^{(j)}) \right]
	& = &
	\EE \left[ \left( \prod_{j=1}^{\ell-2} f(\underline{X}^{(j)}) \right) 
          g(\underline{X}^{(\ell-1)}) \right] 
		\label{eq:07a} \\
	& \ge &
	\EE \left[ \prod_{j=1}^{\ell-1} g(\underline{X}^{(j)}) \right] 
		\label{eq:08a} \\
	& \ge &
	c\left(\alpha, \ell-1, c(\alpha, \mu) \right) = c(\alpha, \ell, \mu) > 0,
		\label{eq:09a}
\end{IEEEeqnarray}
where (\ref{eq:07a}) holds since  is generated by Markov chains,
(\ref{eq:08a}) is due to  pointwise and 
(\ref{eq:09a}) is an application of the induction and (\ref{eq:88a}).
\end{proof}

\begin{remark}
Unfortunately, this proof worsens the explicit bound. One can check that
for a Markov-generated distribution with  steps the dependence on 
is a tower of exponentials of height .
\end{remark}

\section{Polynomial Same-Set Hitting}
\label{sec:polynomial-hitting}

The property of set hitting establishes a lower bound on
 that is
independent of . However, it might be the case that this bound is very
small, perhaps far from the best possible one. In particular, our bound
from Theorem \ref{thm:main-multiple} is triply exponentially small, and
the bound from Theorem \ref{thm:progressions} is not even primitive recursive.

\begin{definition}
A distribution  is \emph{polynomially set hitting}
(resp.~polynomially same-set hitting) if there exists 
such that  is -set hitting (resp.~same-set hitting)
for every .
\end{definition}

As a matter of fact, \cite{MOS13} 
(cf.~Theorem \ref{thm:different-sets-classification}) establishes that
all distributions that are set hitting are also polynomially set hitting.
We suspect that this is also the case for two-step same-set hitting,
but this remains an open problem.

However, it is possible to harness reverse hypercontractivity to show
that all \emph{symmetric} two-step distributions are polynomially same-set
hitting:
\begin{theorem}
\label{thm:symmetric}
Let a two-step probability distribution with equal marginals 
 be symmetric, i.e.,
 for all .
If , then  is polynomially same-set
hitting.
\end{theorem}

We omit the proof of Theorem \ref{thm:symmetric}, 
noting that the idea is similar as in Section \ref{sec:two-steps}:
one performs an obvious convex decomposition of  into cycles
of length two and applies
the result of \cite{MOS13} to each term of this decomposition.

\appendix \section{Appendix: Proof of Theorem \ref{thm:low-influence}}
\label{sec:low-influence-proof}

Our proof of Theorem \ref{thm:low-influence} follows in this appendix. 
It is only a slight adaptation of the argument from \cite{Mos10}, but
we include it in full for the sake of completeness.

We first restate the theorem and discuss the differences between
our proof and the one in \cite{Mos10}:
\lowinfluence*

Theorem \ref{thm:low-influence} is very similar to a subcase of Theorem 1.14
from \cite{Mos10}.  We make a stronger claim with one respect: in
\cite{Mos10} the influence threshold  depends among others on:

while our bound depends only on the smallest marginal probability:


The main differences to the proof in \cite{Mos10} are:
\begin{itemize}
\item \cite{Mos10} proves the base case  and then obtains the result
for general  by an inductive argument 
(cf., Theorem 6.3 and Proposition 6.4 in \cite{Mos10}). Since the induction
is applied to functions  and ,
where  is viewed as a function on a single-step space, the information 
on the smallest marginal is lost in the case of . To avoid this, our proof
proceeds directly for general . However, the structure 
and the main ideas are really the same as in \cite{Mos10}.
\item In Section~\ref{sec:hyper}, in hypercontractivity bounds for Gaussian
  and discrete spaces
(Theorem~\ref{thm:hypercontractivity-degree} and 
Lemma~\ref{lem:hypercontractivity-technical}) we are slightly more careful
to obtain bounds which depend on  rather than 
(as defined in (\ref{eq:75a}) and (\ref{eq:74a})). This better
bound is then propagated in the proof of the invariance principle.
\item Another change is not related to the dependency on the smallest 
  marginal. In Section~\ref{sec:gaussian-hyper},
  in the Gaussian reverse hypercontractivity bound
(Theorem \ref{thm:gaussian-hypercontractivity-main}) instead of
using the result of Borell (\cite{Bor85}, Theorem 5.1 in \cite{Mos10}) 
for a bound expressed in terms of the cdf
of bivariate Gaussians, we utilize the results of \cite{CDP13} and \cite{Led14}
 for a more convenient bound
of the form .
\end{itemize}

The proof can be generalized in several directions, but for the sake
of clarity we present the simplest version sufficient for our purposes.

\subsection{Preliminaries --- the general framework}

We start with explaining the notation of random variables and  spaces 
that we will use throughout the proof.

\begin{definition}
\label{def:l2}
Let  be a probability space.
We define the real inner product space 
as the set of all square-integrable functions 
, i.e., the functions that satisfy

with inner product defined as

\end{definition}

\begin{remark}
As we will see shortly,
if  is a random variable sampled from  according to
,
the equations (\ref{eq:44a}) and (\ref{eq:45a}) can be written as
\begin{IEEEeqnarray*}{rCl}
\EE[f^2(X)] & < & +\infty \; , \\
\langle f, g \rangle & = & \EE[f(X) g(X)] \; .
\end{IEEEeqnarray*}
\end{remark}

\begin{remark}
We omitted the event space  in the definition
of . This is because  is always
implicit in the choice of the measure~. 

In particular,  when  is discrete,
of course we choose  to be the powerset
of .
When  is continuous over ,
we use the ``standard'' real event space, i.e.,~the 
completion of the Borel algebra.  
\end{remark}

While this will not be our usual way of thinking,
at this point it makes sense to introduce the formal
definition of a random variable: a function from a probability
space to some set.
\begin{definition}
Let  be a probability space. 
We say that  is a random variable over
a set  
if it is a measurable function .
\end{definition}
As usual, we will assume throughout the
proof that all random variables are induced by some underlying probability 
space .


Using this,
a random variable induces some distribution, which we can study.

\begin{definition}
We say that a random variable  over a set  is \emph{distributed
according to a probability space } 
if for every event :

\end{definition}

\begin{definition}
\label{def:l2-rv}
Let  be a random variable distributed over .
By  we denote the inner product space of random variables that
correspond to square-integrable functions :

with the inner product given as

\end{definition}

\begin{remark}
We consider the formal setting again, i.e., 
suppose  is the underlying probability 
space, and  a random variable. 
Then,  is a subspace of .
Intuitively, it contains all real valued functions 
which ``depend only on ''.
\end{remark}

\begin{example}
Fix  to be the uniform distribution on 
 and let  be distributed according
to . Then  has dimension three
and one of its orthonormal bases is
\begin{IEEEeqnarray*}{rCl}
Z_0 & :\equiv & 1 \\
Z_1 &:=& \begin{cases}
  \sqrt{6}/2 & \text{if ,}\\
  -\sqrt{6}/2 & \text{if ,}\\
  0 & \text{if .}
\end{cases}\\
Z_2 &:=& \begin{cases}
  \sqrt{2}/2 & \text{if ,}\\
  -\sqrt{2} & \text{if .}\\
\end{cases}
\end{IEEEeqnarray*}  
\end{example}

After this point, we will have no need to refer explicitly 
to the underlying probability space  anymore.
Nevertheless, it will be useful to remember that random variables are 
functions of this underlying space.

It immediately follows from the definitions that:

\begin{lemma}
Let  be a random variable distributed according to .
Then  is isomorphic to .
\end{lemma}

\subsection{Preliminaries --- orthonormal ensembles and multilinear polynomials}

In this section we introduce orthonormal ensembles
and multilinear polynomials over them.

\begin{definition}
We call a finite family  
of random variables \emph{orthonormal}
if they satisfy  for every 
and  for every .
\end{definition}

\begin{definition}
We call a finite family of orthonormal random variables

an . 
We call  the \emph{size} of the ensemble.

An  is a sequence of independent families of 
random variables

such that each  is an orthonormal ensemble
 of the same size .
We call  the \emph{size} of the sequence. 
\end{definition}
The notation  is a little awkward, but we do not
need to use it often.
The reason for it is that we want to to make sure that one cannot confuse 
one of the random variables  within an orthonormal
ensemble with the orthonormal ensemble  itself.
Whenever a random variable  is part of an ensemble
, 
there is no reason to use the -symbol.
Instead we use the index of the ensemble.

Note that in an orthonormal ensemble for  we have .

\begin{definition}
We call two ensemble sequences 

and 
\emph{compatible} if  and the sizes of the individual ensembles
 and  are the same.
\end{definition}

\begin{definition}
Let 
be an ensemble sequence such that each ensemble  is of size .

A \emph{monomial compatible with } is a term

where  with 
.

A (formal) \emph{multilinear polynomial
compatible with } is
a sum of compatible monomials, i.e., a polynomial  of the form

where the sum goes over all tuples  
as above, and  .

For a tuple  we define its \emph{support}
as 
and its
 \emph{degree} as the size of its support:
.
Also, we will write the tuple  as .
\end{definition}

Let a multilinear polynomial  compatible with 
be given. Then,  is what one expects:
the random variable obtained by evaluating the polynomial on the given input.
Analogously, if  is a tuple as above we write  for 
the random variable corresponding to the evaluation of the monomial .

\begin{lemma}\label{lem:orthonormality}
Let  be an ensemble sequence and ,  
two tuples whose monomials ,  are compatible with 
.
Then, 

and

\end{lemma}
\begin{proof}
By independence of the coordinates we have

and now we can use the orthonomality of each ensemble .
For the second part, we apply the first on .
\end{proof}

\begin{definition}
Given a multilinear polynomial 

we define its following properties:
\begin{IEEEeqnarray}{rCl}
\deg(P) & := & \begin{cases}
\max_{\sigma: \alpha_\sigma \ne 0} |\sigma| \label{eq:27a} & \text{if  is non-zero}\\
-\infty & \text{if  is the zero polynomial} \\
\end{cases}\\
\EE[P] & := & \alpha(0^n) \label{eq:28a} \\
\EE[P^2] & := & \sum_{\sigma} \alpha(\sigma)^2 \label{eq:29a}  \\
\Var[P] & := & \EE[P^2] - {\EE}^2[P] \label{eq:30a} \\
\Inf_i(P) & := & \sum_{\sigma: \sigma_i \ne 0} \alpha(\sigma)^2 \label{eq:31a} \\
\Inf(P) & := & \sum_{i=1}^n \Inf_i(P) \label{eq:32a} 
\end{IEEEeqnarray}
\end{definition}

The next lemma states that the formal expressions defined above
are consistent with the corresponding probabilistic interpretations
for every ensemble sequence.

\begin{lemma}
\label{lem:poly-expectation-equiv}
For an ensemble sequence 
and a multilinear polynomial  compatible with it we have
\begin{IEEEeqnarray}{rCl}
\EE[P] & = & \EE[P(\underline{\mathcal{X}})] \label{eq:14a} \\
\EE[P^2] & = & \EE[(P(\underline{\mathcal{X}}))^2] \label{eq:33a} \\ 
\Var[P] & = & \Var[P(\underline{\mathcal{X}})] \label{eq:34a} \; .
\end{IEEEeqnarray}
Furthermore, if all random variables in 
are discrete, then

\end{lemma}

\begin{proof}
Linearity of expectation
and (\ref{eq:69a}) yield
, which
is (\ref{eq:14a}).
Next, (\ref{eq:68a}) gives
, 
i.e.~(\ref{eq:33a}), and hence
(\ref{eq:34a}) by the definition of the variance.


As for (\ref{eq:35a}), fix an assignment

to the ensemble sequence 
.\footnote{
Note that each entry in this tuple is itself a tuple:
,
where  is the size of the ensemble. 
}
We suppose that this tuple has a non-zero probability of occurence.
Since  is an orthornormal ensemble,
\begin{IEEEeqnarray*}{rCl}
  \Var[P(\underline{\mathcal{X}}) \mid 
  \underline{\mathcal{X}}_{\setminus i} = \underline{x}_{\setminus i} ]
& = & \sum_{k = 1}^p \left( \sum_{\sigma: \sigma_i = k}
    \alpha(\sigma) \cdot \prod_{j \ne i} x_{j, \sigma_j}
    \right)^2
\end{IEEEeqnarray*}
From Lemma~\ref{lem:orthonormality}, for a fixed ,

Together this gives

as claimed.
\end{proof}

\begin{definition}
For a multilinear polynomial  and
 we let  be 
restricted to tuples  with ,
i.e., .

Then, let  be 
restricted to tuples with the degree greater than . 
We also define ,  etc.~in the analogous way.
\end{definition}

\begin{lemma}
\label{lem:orthogonal-decomposition}
Let  and  be multilinear polynomials compatible with an ensemble
sequence . Then,

\end{lemma}

\begin{proof}
It is enough to show that for 

Let 
and .
Assume w.l.o.g.~that there exists .
Then,
\begin{IEEEeqnarray*}{l}
\EE\left[ P_S(\underline{\mathcal{X}}) Q_T(\underline{\mathcal{X}}) \right] = 
  \\
\qquad = \sum_{\substack{\sigma: \supp(\sigma) = S\\ \sigma': \supp(\sigma') = T}}
\alpha(\sigma) \beta(\sigma') \EE \left[ \mathcal{X}_{i^*, \sigma_{i^*}} \right]
\EE \left[ \prod_{i \ne i^*} \mathcal{X}_{i,\sigma_i} \mathcal{X}_{i,\sigma'_i}  
\right] = 0 \; .
\end{IEEEeqnarray*}
\end{proof}

\begin{corollary}
\label{cor:orthogonal-esquared}
Let  be a multilinear polynomial.
Then, .
\end{corollary}
\begin{proof}
Taking any ensemble sequence  compatible with ,

\end{proof}

\begin{claim}
\label{cl:orthogonal-variance}
Let  be a multilinear polynomial. Then, 
.
\end{claim}
\begin{proof}
Observing that , 
and  for , by Corollary
\ref{cor:orthogonal-esquared}

\end{proof}

\begin{lemma}
\label{lem:influence-vs-variance}
Let  be a multilinear polynomial with . Then,

\end{lemma}
\begin{proof}

\end{proof}

\begin{definition}
\label{def:t-rho}
Let . We define the operator  as follows:
let  be a multilinear 
polynomial. Then,

\end{definition}

We will mostly use the operator  with .

\begin{definition}
\label{def:gaussian-ensemble}
We call an orthonormal ensemble  of size 
\emph{Gaussian} if random variables 

are independent  Gaussians.

We say that an ensemble sequence
 
is Gaussian if for each  the ensemble 
is Gaussian.
\end{definition}

We remark than as in all ensemble sequences, in a Gaussian ensemble sequence
we have  for all .

\begin{definition}
For tuples of multilinear polynomials 
such that each polynomial  is 
compatible with an ensemble sequence  we write
 for the tuple 
.

Similarly, given multilinear polynomials 
 and a collection of ensemble
sequences
 such that
 is compatible with  we write
 for
.
\end{definition}

\subsection{Preliminaries --- ensemble collections}
\label{sec:app-pre-2}

In this section we recall the setting of Theorem \ref{thm:low-influence}
and introduce some other concepts we will need throughout the proof.

From now on we will always implicitly assume that all multi-step distributions 
 have equal marginals (denoted as ). 
This assumption is not necessary, but sufficient for our main purpose,
while making the notation easier.

\begin{definition}
Let  be a random variable distributed according to a 
single-step, single-coordinate distribution .
We say that an orthonormal ensemble  is
\emph{constructed from } if the elements
of  form an orthonormal basis of .

Similarly, let  be a random vector distributed
according to .
We say that an ensemble sequence  is constructed 
from~ if for each  
the ensemble  is constructed from .
\end{definition}

The definition of ensemble sequences requires that  for every ; of course we can find a basis 
of  which satisfies this requirement, so that 
ensemble sequences constructed from  indeed exist.

\begin{lemma}
\label{lem:product-base}
Let  be an ensemble sequence constructed
from a random vector 
 distributed according to
.
Assume that the size of each ensemble  is .
Then the set of monomials

is an orthonormal basis of .
\end{lemma}

\begin{proof}
Observe that the dimension of  is , (note that
it is the support size of the single-coordinate distribution 
).
Hence, the dimension of  is ,
which equals the size of .
Therefore, it is enough to check that  is
orthonormal, which is done in Lemma~\ref{lem:orthonormality}. 
\end{proof}

\begin{definition}
Let  be an ensemble sequence constructed from a 
random vector  distributed according to 
.

For a function 
and a multilinear polynomial  compatible with

we say that 
is \emph{equivalent} to  if it always holds that

\end{definition}

\medskip

Recall the operator  from Definition \ref{def:t-rho}.
We show that it has a natural counterpart in 
.
\begin{definition}
\label{def:t-rho-function}
Let  and let

be a single-step probability space (with
 a corresponding 
single-coordinate probability space). 

We define a linear operator 
 as

where  
is a random vector with independent coordinates
distributed such that  
with probability  
and  is 
(independently)
distributed according to  with probability 
.
\end{definition}

The next lemma states that taking operator  preserves
the equivalence of functions and polynomials:
\begin{lemma}
Let  be an ensemble sequence constructed from
a random vector  distributed according
to .

Let , 
and  be a multilinear polynomial equivalent to .
Then,  and  are equivalent, i.e.,

\end{lemma}

\begin{proof}
Fix an input 
in the support of .
Let 
 
be the random sequence where for each coordinate , independently

Note that  is not an 
ensemble sequence, but this will not cause problems. 

Writing  we can calculate
\begin{IEEEeqnarray*}{rCl}
  T_\rho f ({\underline{x}}) & = & \EE [ 
  f(\underline{{Y}}^{\rho, {\underline{x}}}) ]
  = \EE [ P(\underline{\mathcal{Y}}^{\rho, {\underline{x}}})]
  = \sum_\sigma \alpha(\sigma)
    \EE[ \mathcal{Y}_\sigma^{\rho, {\underline{x}}} ] \\
  & = & \sum_\sigma \rho^{|\sigma|} \alpha(\sigma) \cdot 
    \mathcal{X}_\sigma({\underline{x}})
  = T_\rho P ({\underline{x}}) \; .
\end{IEEEeqnarray*}
Since  was arbitrary, the claim is proved.
\end{proof}

Recall Definition \ref{def:gaussian-ensemble}.
In the proof we will construct a tuple of ensemble sequences

from a random vector 
and consider relations between those sequences
and compatible Gaussian ensemble sequences. 
To this end, we need to introduce the Gaussian equivalent of
marginal ensemble sequences .

\begin{definition}
\label{def:vg}
Let  be a Gaussian 
orthonormal ensemble of size . We define
an inner product space  as

with the inner product of  given by
.

Similarly, given a Gaussian ensemble sequence 
 such that each of its ensembles is of size 
we let

with the inner product .
\end{definition}

\begin{lemma}\label{lem:cov-xg}
Let a random tuple  be distributed according
to a single-coordinate distribution 
.
Let  be such
that  is an orthonormal ensemble constructed 
from .

Then, there exist Gaussian orthonormal ensembles
 
compatible with  such
that for all , and all  we have

\end{lemma}
\begin{proof}
Consider
 
as a single-step probability space, and let  
be the corresponding random variable.
Let now  be an orthonormal ensemble constructed
from . Recall that this means that
the elements of  form an orthonormal basis
of .

Let  be a Gaussian ensemble sequence compatible
with .
Define the map
 by linearly extending
.
In this way  becomes an isomorphism between 
and  (and as such it preserves inner products).

Since  is a subspace of , we can define
 as .
Since  preserves inner products we get (\ref{eq:70a}).

We still need to argue that for each  the
orthonormal ensemble  is Gaussian.
The fact that  is an ensemble sequence
follows from (\ref{eq:70a}) for  (note that ).

The variables  are clearly jointly Gaussian, 
since they can be written as sums of independent Gaussians.
By (\ref{eq:70a}), their covariance matrix is identity.
This finishes the proof, since joint Gaussians with 
the identity covariance matrix must be independent.
\end{proof}

Since the proof of Lemma \ref{lem:cov-xg} is somewhat abstract, 
we illustrate the construction of  
with an example.

\begin{example}
Consider  distributed according to  
over  with
 and . We can take
the following for the ensemble :

\begin{tabular}{|l||c|c|c|c|}
\hline
 & (0,0) & (0, 1) & (1, 0) & (1, 1) 
\\ \hline \hline
 & 1 & 1 & 1 & 1
\\ \hline
 & 2 & 0 & 0 & -2
\\ \hline
 & 0 &  &  & 0
\\ \hline
 &  &  &  
                                                  & 
\\ \hline
\end{tabular}

For the marginal ensemble  we can take

\begin{tabular}{|l||c|c|}
\hline
 & 0 & 1 
\\ \hline \hline
 & 1 & 1
\\ \hline
 &  & 
\\ \hline
\end{tabular}

Now one can check that 
and 
. 
Defining the ensemble  in the same way we get
 and
.

Let  be a Gaussian ensemble sequence compatible with
. One easily checks that our construction gives

\end{example}

Since the covariances between independent coordinates are always zero,
Lemma \ref{lem:cov-xg} applied to each coordinate separately gives:
\begin{corollary}
\label{cor:cov-xg}
Let a random vector  be distributed according
to a distribution 
.
Let  be such
that  is an ensemble sequence constructed 
from .

Then, there exist Gaussian ensemble sequences
 
compatible with  such
that for all , , and all  we have

\end{corollary}

\begin{definition}
An \emph{ensemble collection for 
}
is a tuple 
where 
\begin{itemize}
\item  is a random vector
distributed according to ,
\item 

are ensemble sequences constructed from 
, respectively, 
\item and 

are obtained from Corollary~\ref{cor:cov-xg}.
\end{itemize}
\end{definition}

\subsection{Hypercontractivity}
\label{sec:hyper}

In this section we develop a version of hypercontractivity for products
of multilinear polynomials. Our goal is to prove Lemma
\ref{lem:hypercontractivity-technical}.

Recall the operator  from Definition \ref{def:t-rho}.

\begin{definition}
Let  be an ensemble sequence
and let  and .
We say that the sequence  is
\emph{-hypercontractive} if for
every multilinear polynomial  compatible with 
we have

\end{definition}

\begin{definition}
Let  be an orthonormal ensemble
and let  and .
We say that the ensemble  is
\emph{-hypercontractive} if
the one-element ensemble sequence
 is
-hypercontractive.
\end{definition}

We start with stating without proofs the hypercontractivity of
orthonormal ensembles that we use in
the invariance principle:

\begin{theorem}[\cite{Bon70, Nel73, Gro75, Bec75}]
\label{thm:hypercontractivity-single-gauss}
Let  be a Gaussian orthonormal
ensemble and . Then,
 is -hypercontractive.
\end{theorem}

\begin{theorem}[Special case of Theorem 3.1 in \cite{Wol07}]
\label{thm:hypercontractivity-single-discrete}
Let  be an orthonormal ensemble constructed from a random variable
 distributed according to a 
(single-coordinate, single-step) probability space 
 with 
.

Then,  is -hypercontractive.
\end{theorem}

Subsequently, we observe that
an ensemble sequence constructed from
hypercontractive ensembles is itself hypercontractive:

\begin{theorem}
\label{thm:tensorization}
Let ,  and let

be an ensemble sequence such that
for every , the ensemble 
is -hypercontractive. Then,
the sequence  is also
-hypercontractive.
\end{theorem}

Yet again, we omit the proof of Theorem \ref{thm:tensorization}.
We remark that it is well-known as the \emph{tensorization argument}.
The argument can be found, e.g., in the proof of Proposition 3.11
in \cite{MOO10}.

\begin{definition}
Let  be a random vector distributed
according to a (single-step, tensorized) probability space 
.
We say that an ensemble sequence

is \emph{-Gaussian-mixed} if
for each :
\begin{itemize}
\item Either  is constructed
from the random variable ,
\item or  is a Gaussian ensemble.
\end{itemize}
\end{definition}

Theorems \ref{thm:hypercontractivity-single-gauss},
\ref{thm:hypercontractivity-single-discrete} and
\ref{thm:tensorization} immediately imply:

\begin{corollary}
\label{cor:hypercontractivity}
Let  be a random vector distributed according to
a probability space
 with

and let  be an 
-Gaussian-mixed
ensemble sequence.

Then,  is -hypercontractive.
\end{corollary}

\begin{theorem}
\label{thm:hypercontractivity-degree}
Let  be a random vector distributed according to a
probability space 
with 
and let  be an -Gaussian-mixed
ensemble sequence.
Let  be a multilinear
polynomial compatible with  of degree at most .\
Then,

\end{theorem}

\begin{proof}
Let  and write
. 
By Corollary \ref{cor:hypercontractivity}, definitions of  and
, and the degree bound on ,
\begin{IEEEeqnarray*}{l}
\EE \left[ \left| P(\underline{\mathcal{X}}) \right|^3 \right]^{1/3} 
= \EE \left[ \left| T_\rho T_{1/\rho} P(\underline{\mathcal{X}}) 
\right|^3 \right]^{1/3}
\le \sqrt{ \EE \left[ (T_{1/\rho} P)^2 \right] }
\\ \qquad = \sqrt{ \sum_{\sigma} \rho^{-2|\sigma|} \beta(\sigma)^2}
\le \sqrt{ \sum_{\sigma} \rho^{-2d} \beta(\sigma)^2}
= \rho^{-d} \sqrt{ \EE[P^2] } \; .
\end{IEEEeqnarray*}
\end{proof}

\begin{lemma}
\label{lem:hypercontractivity-technical}
Let  be a random vector distributed according to
a (multi-step)  probability space with equal marginals

with .

Let

be ensemble sequences such that  is
-Gaussian-mixed.
Let  be multilinear polynomials
such that  is
compatible with  
and also .

Then, for every triple :

\end{lemma} 

\begin{proof}
Let . By Hölder's inequality
and  Theorem 
\ref{thm:hypercontractivity-degree},
\begin{IEEEeqnarray*}{rCl}
\EE \left[ \left| \prod_{k=1}^3 P^{(j_k)}(\underline{\mathcal{S}}^{(j_k)})
\right| \right]
& \le &
\prod_{k=1}^3 \EE \left[ \left| P^{(j_k)}(\underline{\mathcal{S}}^{(j_k)}) 
\right|^3 \right]^{1/3}
\\ & \le & \rho^{-3d} \cdot 
\sqrt{ \prod_{k=1}^3 \EE \left[ (P^{(j_k)})^2\right] } \; .
\end{IEEEeqnarray*}
\end{proof}
\subsection{Invariance principle}

In this section we prove a basic version of invariance principle for multiple
polynomials.

We say that a function is -smooth if all of its third-order partial
derivatives are uniformly bounded by :
\begin{definition}
For  we say that a function  is
\emph{-smooth} if  and
for every  and every 
 we have

\end{definition}

\begin{theorem}[Invariance Principle]
\label{thm:invariance-main}
Let  be an ensemble collection
for a probability space 

with .

Let  be
such that  is a multilinear polynomial compatible with 
the ensemble sequence .

Let  and  and assume that
 and  for each , 
and that  for each .

Finally, let 
be a -smooth function. Then,

\end{theorem}

\begin{remark}
A typical setting of parameters for which Theorem \ref{thm:invariance-main}
might be successfully applied is constant , , , and ,
while  (as ).
\end{remark}

The rest of this section is concerned with proving Theorem 
\ref{thm:invariance-main}.

For  and  let the ensemble sequence 
 be
defined as .

\begin{claim}
\label{cl:invariance-triangle-inequality}

\end{claim}

\begin{proof}
By the triangle inequality.
\end{proof}

Due to Claim 
\ref{cl:invariance-triangle-inequality}, we will estimate

for every . Fix  and write
 and
 for 
readability.
For  we can write

where  and  do not depend on the coordinate 
and, if ,
then .
At the same time, since  and  do not depend on the 
-th coordinate,


We note for later use that the construction gives us


\medskip

The rest of the proof proceeds as follows:
we calculate the multivariate second order Taylor expansion (i.e., with 
the third-degree rest) of the expression, getting
\begin{IEEEeqnarray*}{l}
\Psi(\overline{P}(\overline{\underline{\mathcal{T}}}))
- \Psi(\overline{P}(\overline{\underline{\mathcal{U}}})) =
\\ \qquad =
\Psi\left(A^{(1)} + \sum_{k>0} \mathcal{X}_{i,k}^{(1)} B_{k}^{(1)}, \ldots,
A^{(\ell)} + \sum_{k>0} \mathcal{X}_{i,k}^{(\ell)} B_{k}^{(\ell)}\right) \\
\qquad \quad - \: \Psi\left(A^{(1)} + \sum_{k>0} 
\mathcal{G}_{i,k}^{(1)} B_{k}^{(1)}, \ldots,
A^{(\ell)} + \sum_{k>0} \mathcal{G}_{i,k}^{(\ell)} B_{k}^{(\ell)}\right)
\end{IEEEeqnarray*}
around the point .
We will see that:
\begin{itemize}
  \item All the terms up to the second degree cancel in expectation 
    due to the properties of ensemble sequences.
  \item The remainder, which is of the third degree,
    can be bounded using that  is -smooth,
    properties of , and
    hypercontractivity, in particular
    Lemma \ref{lem:hypercontractivity-technical}.
\end{itemize}

We proceed with a detailed description.
The first result we will need is multivariate Taylor's theorem
for -smooth functions:
\begin{theorem}
\label{thm:taylor}
Let  be a -smooth function
and let 
.
Then,
\begin{IEEEeqnarray*}{l}
\Bigg| 
\Psi \left( x^{(1)}+\epsilon^{(1)}, \ldots, x^{(\ell)}+\epsilon^{(\ell)} \right)
- \\
\quad \left( \Psi(\overline{x}) + 
\sum_{j \in [\ell]} \epsilon^{(j)} 
\frac{\partial}{\partial x^{(j)}}\Psi(\overline{x})
+ \frac12 \sum_{j_1,j_2 \in [\ell]} \epsilon^{(j_1)} \epsilon^{(j_2)}
\frac{\partial^2}{\partial x^{(j_1)} \partial x^{(j_2)}} \Psi(\overline{x})
\right) \Bigg|
\\ \qquad \le \frac{B}{6} \sum_{j_1, j_2, j_3 \in [\ell]} \left|
\epsilon^{(j_1)} \epsilon^{(j_2)} \epsilon^{(j_3)} \right| \; .
\end{IEEEeqnarray*}
\end{theorem}

We omit the proof of Theorem \ref{thm:taylor}.

\begin{lemma}
Fix  and write 

and .
Then,
\begin{IEEEeqnarray}{l}
\EE\left[ \Psi(\overline{P}(\overline{\underline{\mathcal{T}}})) \right]
= \nonumber  \\
= \EE \left[
\Psi(\overline{A}) + \frac12 \sum_{j_1, j_2 \in [\ell]} \left(
\sum_{k_1, k_2 > 0} \mathcal{X}_{i,k_1}^{(j_1)} \mathcal{X}_{i,k_2}^{(j_2)}
B_{k_1}^{(j_1)} B_{k_2}^{(j_2)}
\frac{\partial^2}{\partial A^{(j_1)} \partial A^{(j_2)}} \Psi(\overline{A}) 
\right)
+ R_{\underline{\mathcal{T}}} \right] 
\; , \label{eq:60a} \IEEEeqnarraynumspace
\end{IEEEeqnarray}
and
\begin{IEEEeqnarray}{l}
\EE\left[ \Psi(\overline{P}(\overline{\underline{\mathcal{U}}})) \right]
= \nonumber  \\
= \EE \left[
\Psi(\overline{A}) + \frac12 \sum_{j_1, j_2 \in [\ell]} \left(
\sum_{k_1, k_2 > 0} \mathcal{G}_{i,k_1}^{(j_1)} \mathcal{G}_{i,k_2}^{(j_2)}
B_{k_1}^{(j_1)} B_{k_2}^{(j_2)}
\frac{\partial^2}{\partial A^{(j_1)} \partial A^{(j_2)}} \Psi(\overline{A}) 
\right)
+ R_{\underline{\mathcal{U}}} \right] 
\; , \label{eq:61a} \IEEEeqnarraynumspace
\end{IEEEeqnarray}
where random variables  and  
are such that
\begin{IEEEeqnarray}{l}
\EE\left[
\left| R_{\underline{\mathcal{T}}} \right|
\right],
\EE\left[
\left| R_{\underline{\mathcal{U}}} \right|
\right] 
\le \frac{\ell^{3/2} B}{6} \left(\frac{8}{\sqrt{\alpha}}\right)^d
\left( \sum_{j=1}^\ell \Inf_i(P^{(j)}) \right)^{3/2} \; .
\label{eq:65a}
\end{IEEEeqnarray}
\end{lemma}

\begin{proof}
We show only (\ref{eq:60a}) and the bound on ,
the proofs for the ensemble sequence  being
analogous.

As a preliminary remark,
note that since all the random ensembles we are dealing with are hypercontractive,
and since  is -smooth,
all the terms in the expressions above have finite expectations.

Keeping in mind both decompositions from (\ref{eq:62a}), 
by Theorem \ref{thm:taylor}
\begin{IEEEeqnarray}{l}
\Psi(\overline{P}(\overline{\underline{\mathcal{T}}}))
=
\Psi(\overline{A}) + 
\sum_{j \in [\ell]} \left(
\sum_{k>0} \mathcal{X}_{i,k}^{(j)} B_k^{(j)}
\frac{\partial}{\partial A^{(j)}} \Psi(\overline{A})
\right) + \nonumber \\
\qquad + \: \frac12 \sum_{j_1, j_2 \in [\ell]} \left(
\sum_{k_1,k_2>0} \mathcal{X}_{i,k_1}^{(j_1)} \mathcal{X}_{i,k_2}^{(j_2)} 
B_{k_1}^{(j_1)} B_{k_2}^{(j_2)} 
\frac{\partial^2}{\partial A^{(j_1)} \partial A^{(j_2)}} \Psi(\overline{A})
\right)
+ R_{\underline{\mathcal{T}}} \; ,
\IEEEeqnarraynumspace \label{eq:63a}
\end{IEEEeqnarray}
where
\begin{IEEEeqnarray}{l}
\label{eq:64a}
\EE[ | R_{\underline{\mathcal{T}}} | ] \le \frac{B}{6}
\sum_{j_1, j_2, j_3 \in [\ell]} \EE\left[ \left|
\prod_{k=1}^3 P_i^{(j_k)}(\underline{\mathcal{T}}^{(j_k)})
\right| \right] \; .
\end{IEEEeqnarray}
Since , and all other terms
are independent of coordinate , we have

which together with (\ref{eq:63a}) yields (\ref{eq:60a}).

As for the bound on , since
 is -Gaussian-mixed ensemble sequence,
due to (\ref{eq:64a}), Lemma \ref{lem:hypercontractivity-technical}
(note that the degree is bounded due to (\ref{eq:36a})), and~(\ref{eq:37a}),
\begin{IEEEeqnarray*}{rCl}
\EE[|R_{\underline{\mathcal{T}}}|]
&\le&
\frac{B}{6} \left(\frac{8}{\sqrt{\alpha}}\right)^d
\sum_{j_1,j_2,j_3 \in [\ell]} \sqrt{
\prod_{k=1}^3 \EE\left[\left(P_i^{(j_k)}\right)^2\right]} \\
&=&
\frac{B}{6} \left(\frac{8}{\sqrt{\alpha}}\right)^d
\sum_{j_1,j_2,j_3 \in [\ell]} \sqrt{
\prod_{k=1}^3 \Inf_i(P^{(j_k)})} \\
&\le&
\frac{\ell^{3/2} B}{6} \left(\frac{8}{\sqrt{\alpha}}\right)^d
\left( \sum_{j=1}^\ell \Inf_i(P^{(j)}) \right)^{3/2} \; ,
\end{IEEEeqnarray*}
where the last inequality uses

for the vector  with entries 
.
\end{proof}

\begin{lemma}
\label{lem:invariance-single-diff}
Fix  and write 

and .
Then,

\end{lemma}

\begin{proof}
First, we need to show that the second-order terms in 
(\ref{eq:60a}) and (\ref{eq:61a}) cancel out.
Since
by Lemma \ref{lem:cov-xg}
for every  and :

and since all the other terms are independent of coordinate , we have
\begin{IEEEeqnarray*}{l}
\EE \left[ \sum_{j_1, j_2 \in [\ell]}
\sum_{k_1,k_2>0} \mathcal{X}_{i,k_1}^{(j_1)} \mathcal{X}_{i,k_2}^{(j_2)} 
B_{k_1}^{(j_1)} B_{k_2}^{(j_2)} 
\frac{\partial^2}{\partial A^{(j_1)} \partial A^{(j_2)}} \Psi(\overline{A})
\right] \\
\qquad =
\EE \left[ \sum_{j_1, j_2 \in [\ell]}
\sum_{k_1,k_2>0} \mathcal{G}_{i,k_1}^{(j_1)} \mathcal{G}_{i,k_2}^{(j_2)} 
B_{k_1}^{(j_1)} B_{k_2}^{(j_2)} 
\frac{\partial^2}{\partial A^{(j_1)} \partial A^{(j_2)}} \Psi(\overline{A})
\right] \; .
\end{IEEEeqnarray*}
Therefore, by (\ref{eq:60a}), (\ref{eq:61a}) and (\ref{eq:65a}),
\begin{IEEEeqnarray*}{rCl}
\left| \EE\left[
\Psi(\overline{P}(\overline{\underline{\mathcal{T}}}))
- \Psi(\overline{P}(\overline{\underline{\mathcal{U}}}))
\right] \right| &\le&
\EE[|R_{\underline{\mathcal{T}}}|] + \EE[|R_{\underline{\mathcal{U}}}|]\\
&\le&
\frac{\ell^{3/2} B}{3} \left(\frac{8}{\sqrt{\alpha}}\right)^d
\left( \sum_{j=1}^\ell \Inf_i(P^{(j)}) \right)^{3/2} \; ,
\end{IEEEeqnarray*}
as claimed.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:invariance-main}]
Recall that 
and that .
By Claim \ref{cl:invariance-triangle-inequality},
Lemma \ref{lem:invariance-single-diff}
and Claim \ref{lem:influence-vs-variance},
\begin{IEEEeqnarray*}{l}
\left| \EE \left[
	\Psi(\overline{P}(\overline{\underline{\mathcal{X}}}))
	- \Psi(\overline{P}(\overline{\underline{\mathcal{G}}}))
	\right] \right|
  \le
\sum_{i=1}^n \left| \EE \left[
	\Psi(\overline{P}(\overline{\underline{\mathcal{U}}}_{(i-1)}))
	- \Psi(\overline{P}(\overline{\underline{\mathcal{U}}}_{(i)}))
	\right] \right|
\\ \qquad \le \frac{\ell^{3/2}B}{3} \left(\frac{8}{\sqrt{\alpha}}\right)^d
\sum_{i=1}^n \left( \sum_{j=1}^\ell \Inf_i(P^{(j)})  \right)^{3/2}
\\ \qquad \le \frac{\ell^{3/2}B}{3} \left(\frac{8}{\sqrt{\alpha}}\right)^d
\sqrt{\tau} \sum_{i=1}^n \sum_{j=1}^\ell \Inf_i(P^{(j)})
\\ \qquad = \frac{\ell^{3/2}B}{3} \left(\frac{8}{\sqrt{\alpha}}\right)^d
\sqrt{\tau} \sum_{j=1}^\ell \Inf(P^{(j)})
\le \frac{\ell^{5/2} d B}{3} \left(\frac{8}{\sqrt{\alpha}}\right)^d \sqrt{\tau} \; .
\end{IEEEeqnarray*}
\end{proof}

\subsection{A tailored application of invariance principle}
\begin{definition}
\label{def:xi}
Define  as

and 
as .
\end{definition}

\begin{definition}
\label{def:gamma-smooth}
Let  be a multilinear polynomial and .
We say that  is \emph{-decaying} if
for each  we have


We also say that a tuple of multilinear polynomials
 is -decaying
if  is -decaying for each .
\end{definition}

Note that if a multilinear polynomial 
 is -decaying, then, in particular,
.

Our goal in this section is to prove a version of invariance principle
for -decaying multilinear polynomials and the function :
\begin{theorem}
\label{thm:invariance-smoothed}
Let  be an ensemble collection
for a probability space 

with , .

Let  be such that 
is a multilinear polynomial compatible with the ensemble sequence
.

Let ,  and assume that  is
-decaying and that  for each
. There exists an absolute constant  such that

\end{theorem}

Two obstacles to proving Theorem \ref{thm:invariance-smoothed}
by direct application of Theorem \ref{thm:invariance-main} are:
\begin{enumerate}
  \item The function  is not .
  \item A -decaying multilinear polynomial does not have bounded degree.
\end{enumerate}
We will deal with those problems in turn.

\subsubsection{Approximating
\texorpdfstring{}{chi}
with a 
\texorpdfstring{}{C\^{}3}
function}

To apply Theorem \ref{thm:invariance-main}, we are going
to approximate  and 
with 
(in fact, ) functions.

For that we need to introduce the notion of convolution and a basic calculus 
theorem, whose proof we omit (see, e.g., Chapter 9 in \cite{Rud87}):
\begin{definition}
Let  and . We say
that  is a  of  if  implies .

We say that  has  if there exists a bounded
interval  that is a support of .
\end{definition}

\begin{definition}
The convolution  of two continuous functions 
, at least one of which has
compact support,
is .
\end{definition}

\begin{theorem}
\label{thm:convolution-differentiation}
Let functions  be such that  is continuous 
on ,  and  has compact support. 
Then, .
Furthermore, for every  and :

\end{theorem}

We also need a special density function with support :

\begin{theorem}
\label{thm:smooth-distribution}
There exists a function 
such that all of the following hold:
\begin{itemize}
\item .
\item  has support .
\item .
\item .
\end{itemize}
\end{theorem}
\begin{proof}
Consider

and set  where .
\end{proof}

For any  we can rescale
 to an analogous distribution with support :
\begin{definition}
Let  and define 
as .
\end{definition}

It is easy to see that  has properties analogous to :
\begin{claim}
Let .  has the following properties:
\begin{itemize}
\item .
\item  has support .
\item .
\item .
\end{itemize}
\end{claim}

We see that convoluting  with  for a small  
results in a smooth function that is still very close to :
\begin{definition}
Let  and define  
as .
\end{definition}

To start with, we state some easy to verify properties of :
\begin{claim}
\label{cl:phi-lambda}
Let . The function  has the following 
properties:
\begin{itemize}
  \item .
  \item .
  \item .
  \item .
  \item .
\end{itemize}
\end{claim}

\begin{lemma}
\label{lem:phi-lambda}
Let :
\begin{enumerate}[1)]
\item .
\item . Furthermore, for each
   there exists a constant  such that
  .
\end{enumerate}
\end{lemma}

\begin{proof}
\begin{enumerate}[1)]
\item From Claim \ref{cl:phi-lambda}.
\item Since , due to Theorem
  \ref{thm:convolution-differentiation} we have
  .

  For  the function 
  is constant with
  .

  For , first note that for every
  , since  has support , also all of 
  its derivatives have support  and therefore
  .
  Together with Theorem \ref{thm:convolution-differentiation}
  this gives (substituting )
\begin{IEEEeqnarray*}{rCl}
  \left| \frac{\partial^k}{\partial x^k} \phi_\lambda(x) \right| & = & \left|
    \frac{\partial^k}{\partial x^k} \left( \phi \ast \psi_\lambda \right)(x)
  \right| = \left| \int_{-\infty}^{+\infty} \phi(x-y)
    \frac{\partial^k}{\partial y^k} \psi_\lambda(y) \, \mathrm{d}y \right| \\
  & = & 
  \left| \int_{-\lambda}^{\lambda} \phi(x-y)
    \frac{\partial^k}{\partial y^k} \psi_\lambda(y) \, \mathrm{d}y \right|
  \\ & = & \frac{1}{\lambda^{k+1}} \left| \int_{-\lambda}^{\lambda} \phi(x-y)
    \frac{\partial^k}{\partial z^k} \psi(z) \, \mathrm{d}y \right|
    \le \frac{2B_k}{\lambda^{k}} \; ,
\end{IEEEeqnarray*}
as claimed.
\end{enumerate}
\end{proof}

Now we are ready for the approximation of :
\begin{definition}
  Let . Define function
   as

\end{definition}

From Lemma \ref{lem:phi-lambda} we easily get:
\begin{corollary}
\label{cor:xi}
Let . The function  has the following
properties:
\begin{enumerate}[1)]
\item
  .
\item There exists a universal constant  such that  is
-smooth.
\end{enumerate}
\end{corollary}

After developing the approximation we are ready to prove the invariance
principle for the function :

\begin{theorem}
\label{thm:invariance-xi}
Let  be an ensemble collection
for a probability space 

with .

Let  be
such that  is a multilinear polynomial compatible with 
the ensemble sequence .

Let  and  and assume that
 and  for each , 
and that  for each .

There exists a universal constant  such that

\end{theorem}

\begin{proof}
Let . By the triangle inequality we get
\begin{IEEEeqnarray*}{rCl}
\left| \EE \left[ 
	\chi(\overline{P}(\underline{\overline{\mathcal{X}}}))
	- \chi(\overline{P}(\underline{\overline{\mathcal{G}}}))
	\right] \right|
& \le & 
\left| \EE \left[ 
	\chi(\overline{P}(\underline{\overline{\mathcal{X}}}))
	- \chi_\lambda(\overline{P}(\underline{\overline{\mathcal{X}}}))
	\right] \right|
\\ & & +
\left| \EE \left[ 
	\chi_\lambda(\overline{P}(\underline{\overline{\mathcal{X}}}))
	- \chi_\lambda(\overline{P}(\underline{\overline{\mathcal{G}}}))
	\right] \right|
\\ & & +
\left| \EE \left[ 
	\chi_\lambda(\overline{P}(\underline{\overline{\mathcal{G}}}))
	- \chi(\overline{P}(\underline{\overline{\mathcal{G}}}))
	\right] \right| \; .
      \IEEEyesnumber \label{eq:66a}
\end{IEEEeqnarray*}
From Corollary \ref{cor:xi}.1 and the definition of  
we get both


By Theorem \ref{thm:invariance-main} and Corollary \ref{cor:xi}.2 we get


We can assume w.l.o.g.~that  (otherwise the theorem
is trivial).
Using the definition of , 
and  we see that

Inserting (\ref{eq:20a}), (\ref{eq:21a}), and the combination
of (\ref{eq:22a}) and~(\ref{eq:23a}) into (\ref{eq:66a}) gives the result.
\end{proof}

\subsubsection{Invariance principle for 
\texorpdfstring{}{gamma}-decaying polynomials}

Let  be a tuple of mutlilinear
polynomials and let .
We will deal with a -decaying 
by estimating 
 for appropriately
chosen .

First, we need a bound on the change of :

\begin{lemma}
\label{lem:xi-change}
For all :

\end{lemma}

\begin{proof}
Letting ,
\begin{IEEEeqnarray*}{l}
  \left| \chi(x^{(1)}+\epsilon^{(1)}, \ldots, x^{(\ell)}+\epsilon^{(\ell)}) -
    \chi(x^{(1)}, \ldots, x^{(\ell)}) \right| \\
  \qquad \le \sum_{j=1}^\ell \left| \chi(\overline{y}_{(j-1)}) -
    \chi(\overline{y}_{(j)}) \right| \le \sum_{j=1}^\ell \left| \epsilon^{(j)}
  \right| \; .
\end{IEEEeqnarray*}
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:invariance-smoothed}]
Let . 
By the triangle inequality,
\begin{IEEEeqnarray*}{rCl}
  \left| \EE \left[ \chi(\overline{P}(\underline{\overline{\mathcal{X}}})) -
      \chi(\overline{P}(\underline{\overline{\mathcal{G}}})) \right] \right|
  & \le & \left| \EE \left[
      \chi(\overline{P}(\underline{\overline{\mathcal{X}}})) -
      \chi(\overline{P}^{<d}(\underline{\overline{\mathcal{X}}})) \right] \right|
  \\ & & + \left| \EE \left[
      \chi(\overline{P}^{<d}(\underline{\overline{\mathcal{X}}})) -
      \chi(\overline{P}^{<d}(\underline{\overline{\mathcal{G}}})) \right] \right|
  \\ & & + 
  \left| \EE \left[ \chi(\overline{P}^{<d}(\underline{\overline{\mathcal{G}}})) -
      \chi(\overline{P}(\underline{\overline{\mathcal{G}}})) \right] \right|
    \; .  \IEEEyesnumber \label{eq:67a}
\end{IEEEeqnarray*}

We proceed to demonstrate that all three terms on the right hand side of
(\ref{eq:67a}) are
,
which will finish the proof.

\begin{lemma}

and, similarly,

\end{lemma}

\begin{proof}

We prove only (\ref{eq:24a}), the argument for (\ref{eq:25a}) being the same.
Using Lemma \ref{lem:xi-change}, Cauchy-Schwarz, the fact that 
is -decaying
and the definition of ,
\begin{IEEEeqnarray*}{l}
\left|  \EE\left[
  \chi(\overline{P}(\underline{\overline{\mathcal{X}}})) -
  \chi(\overline{P}^{<d}(\underline{\overline{\mathcal{X}}}))
\right] \right|
\le
\sum_{j=1}^\ell \EE \left[ \left|
	\left(P^{(j)}\right)^{\ge d}(\underline{\mathcal{X}}^{(j)}) \right| \right]
    \\ \qquad \le \sum_{j=1}^\ell
    \sqrt{\EE \left[ \left(\left( P^{(j)}\right)^{\ge d} \right)^2 \right]}
\le \ell (1-\gamma)^{d/2} \le 2 \ell \tau^{\frac{\gamma}{128 \ln 1/\alpha}} \; .
\end{IEEEeqnarray*}
\end{proof}

\begin{lemma}

\end{lemma}

\begin{proof}
From Theorem \ref{thm:invariance-xi},

From the definition of  (recall that ),

as claimed.
\end{proof}
This finishes the proof of Theorem~\ref{thm:invariance-smoothed}.
\renewcommand{\qedsymbol}{}
\end{proof}

\subsection{Reduction to the 
\texorpdfstring{}{gamma}-decaying case}

To apply Theorem \ref{thm:invariance-smoothed} we
need to show that ``smoothing out''  of multilinear polynomials 

does not change the expectation of their product too much.

Recall Definitions \ref{def:t-rho} and \ref{def:t-rho-function} 
for the operator . Our goal in this section is to prove:
\begin{theorem}
\label{thm:smoothing}
Let  be a random vector distributed according
to 
with .
Let  be an ensemble sequence constructed from
 and 

be ensemble sequences constructed from 
, respectively.

Let  and 
.

Then, for all multilinear polynomials  such that
:

\end{theorem}

Let us start with an intuition:
Due to Lemma \ref{lem:orthogonal-decomposition},
it is enough to bound

for every . If  is small, we use the fact that
 shrinks by a factor of 
for every .
If  is large, we exploit that both

are small (roughly  times smaller compared to their variances).

To give a formal argument, we use yet another ensemble sequence:
let . We define   to be an
ensemble sequence constructed from .
Furthermore, let

Note that since ,
there exists a multilinear polynomial  compatible with
 such that


\begin{lemma}
\label{lem:smooth-step-decomposition}

\end{lemma}

\begin{proof}
By definition of .
\end{proof}

\begin{lemma}
\label{lem:decomposition-rho-bound}
For every  and , :

\end{lemma}

\begin{proof}
For ease of notation let us write ,
, 
and .

Let 
and .

We know that

and 
for every , . Furthermore, if ,
then 
and .
By definition of , this implies


Expanding the expectation and using (\ref{eq:52a}) and Cauchy-Schwarz,
\begin{IEEEeqnarray*}{rCl}
	\left| \EE \left[ P_S(\underline{\mathcal{X}}) 
            Q_S(\underline{\mathcal{Y}}) \right] \right|
	& = &
	\left| \EE \left[ \left( \sum_{\sigma: \supp(\sigma)=S}  
              \alpha(\sigma) \mathcal{X}_\sigma \right)
		\left( \sum_{\sigma': \supp(\sigma')=S} 
                  \beta(\sigma') \mathcal{Y}_{\sigma'} \right)
		\right] \right|
	\\ & \le &
	\sum_{\substack{\sigma, \sigma': \\ \supp(\sigma)=\supp(\sigma')=S}}
		\left| \alpha(\sigma) \beta(\sigma') \prod_{i \in S}
		\EE \left[ \mathcal{X}_{i, \sigma_i}
                  \mathcal{Y}_{i, \sigma'_i}  \right] \right|
	\\ & \le &
	\rho^{|S|} \sum_{\substack{\sigma, \sigma': 
            \\ \supp(\sigma)=\supp(\sigma')=S}}
	| \alpha(\sigma) \beta(\sigma') |
	\\ & \le &
	\rho^{|S|} \sqrt{\Var[P_S] \Var[Q_S]} \; ,
\end{IEEEeqnarray*}
\end{proof}

\begin{lemma}
\label{lem:k-epsilon-bound}
Let . Then, . 
\end{lemma}

\begin{proof}
If  we are done, therefore assume that .
If , then .

If , then by Bernoulli's inequality,

\end{proof}

\begin{lemma}
\label{lem:smooth-subset}
For every  and , :

\end{lemma}

\begin{proof}
As in the proof of Lemma \ref{lem:decomposition-rho-bound},
we will write ,
, 
and .

By definition of ,


From (\ref{eq:51a}), Lemma \ref{lem:decomposition-rho-bound}
and Lemma \ref{lem:k-epsilon-bound},
\begin{IEEEeqnarray*}{rCl}
\left| \EE\left[
  (\Id-T_{1-\gamma})P_S(\underline{\mathcal{X}}) \cdot
  Q_S(\underline{\mathcal{Y}})
\right]\right| & \le & 
\min\left( 1-(1-\gamma)^{|S|}, \rho^{|S|} \right) 
\sqrt{\Var[P_S] \Var[Q_S]} \\
& \le & \frac{\epsilon}{\ell} \sqrt{\Var[P_S] \Var[Q_S]} \; .
\end{IEEEeqnarray*}
\end{proof}

\begin{lemma}
\label{lem:smooth-coordinate}
Fix . Then,

\end{lemma}

\begin{proof}
For ease of notation write ,
, 
and .

Observe that since 
,
also .

From Lemma \ref{lem:orthogonal-decomposition},
Lemma \ref{lem:smooth-subset} and Cauchy-Schwarz,
\begin{IEEEeqnarray*}{rCl}
\label{eq:50a}
\left| \EE \left[ (\Id-T_{1-\gamma})P(\underline{\mathcal{X}}) \cdot
Q(\underline{\mathcal{Y}}) \right| \right]
& \le & \sum_{S \subseteq [n]} \left| \EE \left[
(\Id-T_{1-\gamma})P_S(\underline{\mathcal{X}}) \cdot
Q_S(\underline{\mathcal{Y}})
\right] \right| \\
& \le &
\frac{\epsilon}{\ell} \sum_{S \ne \emptyset} \sqrt{\Var[P_S] \Var[Q_S]} \\
& \le &
\frac{\epsilon}{\ell} \sqrt{\Var[P] \Var[Q]} \le \epsilon/\ell \; . 
\end{IEEEeqnarray*}
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:smoothing}]
By Lemma \ref{lem:smooth-step-decomposition} and Lemma
\ref{lem:smooth-coordinate},
\begin{IEEEeqnarray*}{rCl}
\left| \EE \left[
\prod_{j=1}^\ell P^{(j)}(\underline{\mathcal{X}}^{(j)})
- \prod_{j=1}^\ell T_{1-\gamma}P^{(j)}(\underline{\mathcal{X}}^{(j)})
\right] \right|
& \le &
\sum_{j=1}^\ell \left| \EE \left[
(\Id-T_{1-\gamma})P^{(j)}(\underline{\mathcal{X}}^{(j)}) \cdot
Q^{(j)}(\underline{\mathcal{Y}}^{(j)})
\right] \right| \\
& \le & \epsilon \; .
\end{IEEEeqnarray*}
\end{proof}

\subsection{Gaussian reverse hypercontractivity}
\label{sec:gaussian-hyper}

\begin{definition}
Let  be the inner product space
of functions with standard  Gaussian measure.
\end{definition}

Our goal in this section is to prove the following bound:
\begin{theorem}
\label{thm:gaussian-hypercontractivity-main}
Let  
be an ensemble collection
for a probability space 

with 
and such that each orthonormal ensemble in 
has size .

Then, for all 
such that  and
:

\end{theorem}

\begin{remark}
Since the random variables  are constant,
it suffices to consider consider  as functions of  rather 
than  inputs. 
\end{remark}

In order to prove Theorem \ref{thm:gaussian-hypercontractivity-main},
we will use a multidimensional version of Gaussian reverse hypercontractivity
stated as Theorem 1 in \cite{CDP13} (cf.~also Corollary 4 in \cite{Led14}).

\begin{theorem}[\cite{CDP13}]\label{thm:gaussian-cdp} Let  and let

be a jointly Gaussian collection of  random vectors such that:
\begin{itemize}
  \item
    For each , 
    
    is a random vector distributed as  independent 
    Gaussians.
  \item
    For every collection of real numbers :
    
\end{itemize}
Then, for all functions 
such that 
and :

\end{theorem}

\begin{remark}
An equivalent formulation of the condition in (\ref{eq:46a}) 
is that the matrix  is positive semidefinite,
where  is the covariance matrix of .  
\end{remark}

To reduce Theorem \ref{thm:gaussian-hypercontractivity-main}
to Theorem \ref{thm:gaussian-cdp} we 
first look at a single-coordinate variance bound for 
ensembles from . 
Next, we will extend this bound to multiple coordinates
and ensembles from  .

\begin{lemma}
\label{lem:gaussian-single-coordinate}
Let 
be an ensemble collection
for a probability space 

with 
and such that each orthonormal ensemble in 
has size .

Fix  and for ease of notation
let us write 

for the random ensemble 
.

Then,
for every collection of real numbers :

\end{lemma}

\begin{proof}
For any  we define

and .

We compute

where in the last inequality we used that the definition of  implies

since  and 
.

Therefore, 

\end{proof}

\begin{lemma}
\label{lem:gaussian-x}
Let  be an ensemble collection
for a probability space 

with .

Then,
for every collection of real numbers :

\end{lemma}

\begin{proof}
Since ensembles  are independent, by
Lemma \ref{lem:gaussian-single-coordinate},
\begin{IEEEeqnarray*}{rCl}
 \Var\left[ \sum_{i,j\ge1, k>0} \alpha_{i,k}^{(j)} \cdot \mathcal{X}_{i,k}^{(j)} 
  \right] & = & \sum_{i=1}^n \Var\left[
  \sum_{j\ge 1,k>0} \alpha_{i,k}^{(j)} \cdot \mathcal{X}_{i,k}^{(j)} \right]
  \\ & \ge & \frac{1-\rho^2}{\ell} \cdot \sum_{i,j\ge1,k>0} 
  \left(\alpha_{i,k}^{(j)}\right)^2 \; .
\end{IEEEeqnarray*}
\end{proof}

\begin{lemma}
\label{lem:gaussian-g}
Let  be an ensemble collection
for a probability space 

with .

Then,
for every collection of real numbers :

\end{lemma}

\begin{proof}
By Corollary \ref{lem:cov-xg}  and Lemma \ref{lem:gaussian-x}.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:gaussian-hypercontractivity-main}]
By application of Theorem \ref{thm:gaussian-cdp} to
,
where .

Since  is a Gaussian ensemble sequence,
 is distributed as  independent
 Gaussians. Condition (\ref{eq:46a}) 
for  is fulfilled
due to Lemma \ref{lem:gaussian-g}.
\end{proof}


\subsection{The main theorem}

We recall the low-influence theorem that we want to prove:
\lowinfluence*

We need to define some new objects in order to proceed with the proof.
Let  be an ensemble collection for
.

For , let  be a multilinear polynomial compatible
with  and equivalent to 
. For
some small  to be fixed later let 
. Finally, letting  be the size
of each of the ensembles  and ,
define a function  as

Note that it might be impossible to write  as a multilinear 
polynomial, but it will not cause problems in the proof.
Finally, let 
.

The proof proceeds by decomposing the expression we are bounding into
several parts:
\begin{IEEEeqnarray}{l}
  \EE \left[ \prod_{j=1}^\ell f^{(j)}(\underline{X}^{(j)}) \right]
  = \EE \left[ \prod_{j=1}^\ell P^{(j)}(\underline{\mathcal{X}}^{(j)}) \right] 
  = \nonumber \\
  \quad = \EE \left[ \prod_{j=1}^\ell P^{(j)}(\underline{\mathcal{X}}^{(j)})   
  - \prod_{j=1}^\ell Q^{(j)}(\underline{\mathcal{X}}^{(j)}) \right] +  
  \label{eq:53a} \\
  \qquad + \EE \left[ \prod_{j=1}^\ell Q^{(j)}(\underline{\mathcal{X}}^{(j)})
  - \prod_{j=1}^\ell R^{(j)}(\underline{\mathcal{G}}^{(j)}) \right] + 
  \label{eq:54a} \\
  \qquad + \EE \left[ \prod_{j=1}^\ell R^{(j)}(\mathcal{G}^{(j)}) \right] \; .
  \label{eq:55a}
\end{IEEEeqnarray}
We use the theorems proved so far to bound each of the terms 
(\ref{eq:53a}), (\ref{eq:54a}) and (\ref{eq:55a})
in turn. First, we apply Theorem \ref{thm:smoothing} to show that (\ref{eq:53a})
has small absolute value. Then, we use the invariance principle
(Theorem \ref{thm:invariance-smoothed}) to argue that (\ref{eq:54a}) has
small absolute value. Finally, using Gaussian reverse hypercontractivity
(Theorem \ref{thm:gaussian-hypercontractivity-main}) we show that
(\ref{eq:55a}) is bounded from below by (roughly) 
.

We proceed with a detailed argument in the following lemmas.
In the following assume w.l.o.g~that 
and .

\begin{lemma}
\label{lem:main-smoothing}
Set . Then,

\end{lemma}

\begin{proof}
By Theorem \ref{thm:smoothing}.
\end{proof}

\begin{lemma}
\label{lem:main-invariance}
There exists an absolute constant  such that

\end{lemma}

\begin{proof}
Note that for every  the polynomial  is 
-decaying and that it has bounded influence for every :


By definition of  (Definition \ref{def:xi}) and Theorem
\ref{thm:invariance-smoothed},
\begin{IEEEeqnarray*}{rCl}
\left| \EE \left[ \prod_{j=1}^\ell Q^{(j)}(\underline{\mathcal{X}}^{(j)}) - 
\prod_{j=1}^\ell R^{(j)}(\underline{\mathcal{G}}^{(j)}) \right] \right|
& = & \left| \EE \left[ 
  \chi\left(\overline{Q}(\overline{\underline{\mathcal{X}}})\right) 
  - 
  \chi\left(\overline{Q}(\overline{\underline{\mathcal{G}}})\right)
  \right] \right|
  \\
  & \le & C \ell^{5/2} \cdot \tau^{\frac{\gamma}{C \ln 1/\alpha}} \; .
\end{IEEEeqnarray*}
\end{proof}

\begin{lemma}
\label{lem:main-gaussian}

\end{lemma}

\begin{proof}
By Theorem \ref{thm:gaussian-hypercontractivity-main}.
\end{proof}

Lastly, we need to show that the difference between
 and
 is small.

\begin{claim}
\label{cl:bound-mu}
Let .
Then, .
\end{claim}
\begin{proof}
The function 
is non-decreasing (since 
).
Hence, 

where in the last step we applied Bernoulli's inequality.
\end{proof}

\begin{lemma}
\label{lem:main-mu}
There exists an absolute constant  such that

\end{lemma}

\begin{proof}
By Claim \ref{cl:bound-mu},


Since ,

For a fixed , from the definition of  and Theorem
\ref{thm:invariance-smoothed} applied with ,

Inequalities (\ref{eq:58a}), (\ref{eq:56a}) and (\ref{eq:57a}) together give the claim.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:low-influence}]
Following the decomposition of 
into subexpressions (\ref{eq:53a}), (\ref{eq:54a}) and (\ref{eq:55a}), from
Lemma \ref{lem:main-smoothing}, Lemma \ref{lem:main-invariance},
Lemma \ref{lem:main-gaussian} and Lemma \ref{lem:main-mu},
\begin{IEEEeqnarray*}{rCl}
\EE \left[ \prod_{j=1}^\ell f^{(j)}(\underline{X}^{(j)}) \right]
& \ge & 
\left(\prod_{j=1}^\ell \mu^{(j)} \right)^{\ell / (1-\rho^2)}  
\!\!\!\!\!\!\! -\epsilon/2
- C\ell^{5/2} \cdot \tau^{\frac{\gamma}{C \ln 1/\alpha}}
- \frac{C\ell^2}{1-\rho^2} \cdot \tau^{\frac{\gamma}{C\ln 1/\alpha}} \\
& \ge & 
\left(\prod_{j=1}^\ell \mu^{(j)} \right)^{\ell / (1-\rho^2)} 
\!\!\!\!\!\!\! - \epsilon/2
- \frac{2C\ell^{5/2}}{1-\rho^2} \cdot \tau^{\frac{\gamma}{C\ln 1/\alpha}} \; .
\end{IEEEeqnarray*}

By choosing 
small enough we get 

which is the main part of the theorem (recall that 
).

To see that we can choose  as in (\ref{eq:38a}), note that
for  big enough we have
\begin{IEEEeqnarray*}{rCl}
\tau &:=& \left( \frac{(1-\rho^2)\epsilon}{\ell^{5/2}}
  \right)^{D \frac{ \ell \ln(\ell/\epsilon)\ln(1/\alpha)}{(1-\rho)\epsilon}}
  \le
  \left( \frac{(1-\rho^2)\epsilon}{\ell^{5/2}}  
  \right)^{D' \frac{2 C \ell \ln(2\ell/\epsilon)\ln(1/\alpha)}
  {(1-\rho)\epsilon}}
\\
&=& 
\left( \frac{(1-\rho^2)\epsilon}{\ell^{5/2}} 
\right)^{D' \frac{C \ln(1/\alpha)}{\gamma}}
\end{IEEEeqnarray*}
for  as needed.
Hence, we obtain

which establishes (\ref{eq:59a}) for this choice of .
\end{proof}



\bibliographystyle{alpha}
\newcommand{\etalchar}[1]{}
\begin{thebibliography}{MOR{\etalchar{+}}06}

\bibitem[AM13]{AM13}
Per Austrin and Elchanan Mossel.
\newblock Noise correlation bounds for uniform low degree functions.
\newblock {\em Arkiv för Matematik}, 51(1):29--52, 2013.

\bibitem[Bec75]{Bec75}
William Beckner.
\newblock Inequalities in {F}ourier analysis.
\newblock {\em Annals of Mathematics}, 102(1):159--182, 1975.

\bibitem[Bon70]{Bon70}
Aline Bonami.
\newblock Étude des coefficients de {F}ourier des fonctions de {}.
\newblock {\em Annales de l'institut Fourier}, 20(2):335--402, 1970.

\bibitem[Bor82]{Bor82}
Christer Borell.
\newblock Positivity improving operators and hypercontractivity.
\newblock {\em Mathematische Zeitschrift}, 180(3):225--234, 1982.

\bibitem[Bor85]{Bor85}
Christer Borell.
\newblock Geometric bounds on the {Ornstein--Uhlenbeck} velocity process.
\newblock {\em Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte
  Gebiete}, 70(1):1--13, 1985.

\bibitem[CDP15]{CDP13}
{Wei-Kuo} Chen, Nikos Dafnis, and Grigoris Paouris.
\newblock Improved {H}ölder and reverse {H}ölder inequalities for {G}aussian
  random vectors.
\newblock {\em Advances in Mathematics}, 280:643--689, 2015.

\bibitem[DFR08]{DFR08}
Irit Dinur, Ehud Friedgut, and Oded Regev.
\newblock Independent sets in graph powers are almost contained in juntas.
\newblock {\em Geometric and Functional Analysis}, 18(1):77--97, 2008.

\bibitem[FK91]{FK91}
Harry Furstenberg and Yitzhak Katznelson.
\newblock A density version of the {H}ales-{J}ewett theorem.
\newblock {\em Journal d’Analyse Mathématique}, 57(1):64--119, 1991.

\bibitem[FLS18]{FLS18}
Jacob Fox, László~Miklós Lovász, and Lisa Sauermann.
\newblock A polynomial bound for the arithmetic {}-cycle removal lemma in
  vector spaces.
\newblock {\em Journal of Combinatorial Theory, Series A}, 160:186--201, 2018.

\bibitem[FR18]{FR18}
Ehud Friedgut and Oded Regev.
\newblock Kneser graphs are like {S}wiss cheese.
\newblock {\em Discrete Analysis}, 2, 2018.

\bibitem[Fur77]{Fur77}
Harry Furstenberg.
\newblock Ergodic behavior of diagonal measures and a theorem of {S}zemerédi
  on arithmetic progressions.
\newblock {\em Journal d'Analyse Mathématique}, 31(1):204--256, 1977.

\bibitem[GL15]{GL15}
Venkatesan Guruswami and Euiwoong Lee.
\newblock Strong inapproximability results on balanced rainbow-colorable
  hypergraphs.
\newblock In {\em SODA}, pages 822--836, 2015.

\bibitem[Gow01]{Gow01}
W.~T. Gowers.
\newblock A new proof of {S}zemerédi's theorem.
\newblock {\em Geometric \& Functional Analysis GAFA}, 11(3):465--588, 2001.

\bibitem[Gow07]{Gow07}
W.~T. Gowers.
\newblock Hypergraph regularity and the multidimensional {S}zemerédi theorem.
\newblock {\em Annals of Mathematics}, 166(3):897--946, 2007.

\bibitem[Gre05a]{Green05}
Ben Green.
\newblock Finite field models in additive combinatorics.
\newblock In Bridget~S. Webb, editor, {\em Surveys in Combinatorics}, pages
  1--27. 2005.

\bibitem[Gre05b]{Gre05a}
Ben Green.
\newblock A {S}zemer{\'e}di-type regularity lemma in abelian groups, with
  applications.
\newblock {\em Geometric {\&} Functional Analysis GAFA}, 15(2):340--376, 2005.

\bibitem[Gro75]{Gro75}
Leonard Gross.
\newblock Logarithmic {S}obolev inequalities.
\newblock {\em American Journal of Mathematics}, 97(4):1061--1083, 1975.

\bibitem[HHM16]{HaHoMo:16}
Jan Hązła, Thomas Holenstein, and Elchanan Mossel.
\newblock Lower bounds on same-set inner product in correlated spaces.
\newblock In {\em APPROX-RANDOM}, volume~60 of {\em LIPIcs}, pages 34:1--34:11,
  2016.

\bibitem[H{ą}z18]{SymProg}
Jan H{ą}zła.
\newblock On very restricted arithmetic progressions in symmetric sets in
  finite field model.
\newblock arXiv:1811.09947, 2018.

\bibitem[KSV09]{KSV09}
Daniel Krá\v{l}, Oriol Serra, and Lluís Vena.
\newblock A combinatorial proof of the removal lemma for groups.
\newblock {\em Journal of Combinatorial Theory, Series A}, 116(4):971--978,
  2009.

\bibitem[Led14]{Led14}
Michel Ledoux.
\newblock Remarks on {G}aussian noise stability, {B}rascamp-{L}ieb and
  {S}lepian inequalities.
\newblock In {\em Geometric Aspects of Functional Analysis: Israel Seminar
  (GAFA) 2011--2013}, pages 309--333, 2014.

\bibitem[LPW08]{LevinPW08}
David~A. Levin, Yuval Peres, and Elizabeth~L. Wilmer.
\newblock {\em Markov Chains and Mixing Times}.
\newblock American Mathematical Society, 2008.

\bibitem[Mes95]{Mes95}
Roy Meshulam.
\newblock On subsets of finite abelian groups with no 3-term arithmetic
  progressions.
\newblock {\em Journal of Combinatorial Theory, Series A}, 71(1):168--172,
  1995.

\bibitem[MOO10]{MOO10}
Elchanan Mossel, Ryan O'Donnell, and Krzysztof Oleszkiewicz.
\newblock Noise stability of functions with low influences: Invariance and
  optimality.
\newblock {\em Annals of Mathematics}, 171(1):295--341, 2010.

\bibitem[MOR{\etalchar{+}}06]{MOR06}
Elchanan Mossel, Ryan O'Donnell, Oded Regev, Jeffrey~E. Steif, and Benny
  Sudakov.
\newblock Non-interactive correlation distillation, inhomogeneous {M}arkov
  chains, and the reverse {B}onami-{B}eckner inequality.
\newblock {\em Israel Journal of Mathematics}, 154(1):299--336, 2006.

\bibitem[Mos10]{Mos10}
Elchanan Mossel.
\newblock Gaussian bounds for noise correlation of functions.
\newblock {\em Geometric and Functional Analysis}, 19(6):1713--1756, 2010.

\bibitem[MOS13]{MOS13}
Elchanan Mossel, Krzysztof Oleszkiewicz, and Arnab Sen.
\newblock On reverse hypercontractivity.
\newblock {\em Geometric and Functional Analysis}, 23(3):1062--1097, 2013.

\bibitem[Mos17]{Mos17}
Elchanan Mossel.
\newblock Gaussian bounds for noise correlation of resilient functions.
\newblock arXiv:1704.04745, 2017.

\bibitem[Nel73]{Nel73}
Edward Nelson.
\newblock The free {M}arkoff field.
\newblock {\em Journal of Functional Analysis}, 12(2):211--227, 1973.

\bibitem[{O'D}14]{Dol14}
Ryan {O'Donnell}.
\newblock {\em Analysis of Boolean Functions}.
\newblock Cambridge University Press, 2014.

\bibitem[Rot53]{Rot53}
Klaus~F. Roth.
\newblock On certain sets of integers.
\newblock {\em Journal of the London Mathematical Society}, s1-28(1):104--109,
  1953.

\bibitem[RS04]{RS04}
Vojtěch Rödl and Jozef Skokan.
\newblock Regularity lemma for k-uniform hypergraphs.
\newblock {\em Random Structures \& Algorithms}, 25(1):1--42, 2004.

\bibitem[RS06]{RS06}
Vojtěch Rödl and Jozef Skokan.
\newblock Applications of the regularity lemma for uniform hypergraphs.
\newblock {\em Random Structures \& Algorithms}, 28(2):180--194, 2006.

\bibitem[Rud87]{Rud87}
Walter Rudin.
\newblock {\em Real and Complex Analysis}.
\newblock McGraw-Hill, Inc., 3rd edition, 1987.

\bibitem[Sha10]{Sha10}
Asaf Shapira.
\newblock A proof of {G}reen's conjecture regarding the removal properties of
  sets of linear equations.
\newblock {\em Journal of the London Mathematical Society}, 81(2):355--373,
  2010.

\bibitem[Sze75]{Sze75}
Endre Szemerédi.
\newblock On sets of integers containing no k elements in arithmetic
  progression.
\newblock {\em Acta Arithmetica}, 27(1):199--245, 1975.

\bibitem[TV06]{TV06}
Terence Tao and Van~H. Vu.
\newblock {\em Additive Combinatorics}.
\newblock Cambridge University Press, 2006.

\bibitem[Wol07]{Wol07}
Paweł Wolff.
\newblock Hypercontractivity of simple random variables.
\newblock {\em Studia Mathematica}, 180:219--236, 2007.
\end{thebibliography}


\begin{dajauthors}
\begin{authorinfo}[jh]
  Jan Hązła\\
  Massachusetts Institute of Technology\\
  Cambridge, Massachusetts, USA\\
  jhazla\imageat{}mit\imagedot{}edu \\
  \url{https://idss.mit.edu/staff/jan-hazla/}
\end{authorinfo}
\begin{authorinfo}[th]
  Thomas Holenstein\\
  Google\\
  Zurich, Switzerland\\
  thomas\imagedot{}holenstein\imageat{}google\imagedot{}com \\
\end{authorinfo}
\begin{authorinfo}[em]
  Elchanan Mossel\\
  Massachusetts Institute of Technology\\
  Cambridge, Massachusetts, USA\\
  elmos\imageat{}mit\imagedot{}edu\\
  \url{https://math.mit.edu/~elmos/}
\end{authorinfo}
\end{dajauthors}

\end{document}
