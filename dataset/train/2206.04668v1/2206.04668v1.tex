

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[accsupp]{axessibility}
\usepackage[dvipsnames]{xcolor}
\newcommand{\com}[1]{\textcolor{red}{[#1]}}
\newcommand{\mei}[1]{\textcolor{blue}{[#1]}}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\methodname}{GateHUB}

\def\cvprPaperID{3084} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{GateHUB: Gated History Unit with Background Suppression for Online Action Detection}










\author{Junwen Chen ~~~~ Gaurav Mittal ~~~~ Ye Yu ~~~~ Yu Kong ~~~~  Mei Chen\\
Microsoft ~~~~~~~~~~~~~~~~~~~~~~~~ Rochester Institute of Technology\\
{\tt\small \{gaurav.mittal, yu.ye, mei.chen\}@microsoft.com} ~~~
{\tt\small \{jc1088,yu.kong\}@rit.edu}
}
\maketitle

\begin{abstract}
\vspace{-0.5em}
Online action detection is the task of predicting the action as soon as it happens in a streaming video. A major challenge is that the model does not have access to the future and has to solely rely on the history, i.e., the frames observed so far, to make predictions.
It is therefore important to accentuate parts of the history that are more informative to the prediction of the current frame.
We present GateHUB, \textbf{Gate}d \textbf{H}istory \textbf{U}nit with \textbf{B}ackground Suppression, that comprises a novel position-guided gated cross-attention mechanism to enhance or suppress parts of the history as per how informative they are for current frame prediction. 
GateHUB further proposes Future-augmented History~(FaH) to make history features more informative by using subsequently observed frames when available.
In a single unified framework, GateHUB integrates the transformer's ability of long-range temporal modeling and the recurrent model's capacity to selectively encode relevant information. 
GateHUB also introduces a background suppression objective to further mitigate false positive background frames that closely resemble the action frames.
Extensive validation on three benchmark datasets, THUMOS, TVSeries, and HDD, demonstrates that 
GateHUB significantly outperforms all existing methods and is also more efficient than the existing best work.
Furthermore, a flow-free version of GateHUB is able to achieve higher or close accuracy at 2.8 higher frame rate compared to all existing methods that require both RGB and optical flow information for prediction.






\end{abstract}

%
 \vspace{-1.5em}
\section{Introduction}
\let\thefootnote\relax\footnote{Authors with equal contribution.}
\let\thefootnote\relax\footnote{This work was done as Junwen Chen’s internship project at Microsoft.}
\label{sec:intro}
\begin{figure}[t]
\begin{center}
    \includegraphics[width=\columnwidth]{figures/gatehub_teaser_v11.pdf}
\end{center}
\vspace{-1.5em}
\caption{We show an example video stream (middle row) where the current frame~({\color{magenta}magenta}) contains \textit{Cliff Diving} action. Weights from vanilla cross-attention~(top row) do not correlate with how informative each history frame is to current frame prediction, leading to incorrect prediction of \textit{Background}. Our novel Gated History Unit~(GHU) (bottom row) calibrates cross-attention weights using gating scores to enhance history frames that are informative to current frame prediction ({\color{green}green}) and suppress uninformative ones ({\color{red}red}), leading to accurate prediction of \textit{Cliff Diving}.
}
\label{fig:teaser}
\vspace{-1.5em} 
\end{figure}
Online action detection is the task to predict actions in a streaming video as they unfold~\cite{de2016online}. It is critical to applications including autonomous driving, public safety, virtual and augmented reality. Unlike action detection in the offline setting, where the entire untrimmed video is observable at any given moment, a major challenge for online action detection is that the predictions are solely based on observations of history without access to video frames in the future. The model needs to build a causal reasoning of the present in correlation to what happened hitherto, and as efficiently as possible for the online setting.


Prior work for online action detection~\cite{xu2019temporal, eun2020learning, eun2021temporal, gao2020woad,qu2020lap,zhao2020privileged} include recurrent-based LSTMs~\cite{hochreiter1997long} and GRUs~\cite{cho2014learning} that are prone to forgetting informative history as sequential frame processing is ineffective in preserving long-range interactions. Emerging methods~\cite{wang2021oadtr,xu2021long} employ transformers~\cite{vaswani2017attention} to mitigate this by encoding sequential frames in parallel via self-attention. Some improve model efficiency by using cross-attention~\cite{xu2021long, jaegle2021perceiverio} to compress the video sequence into a fixed-sized latent encoding for prediction. 

Fig.~\ref{fig:teaser} shows an example video stream (middle row) where the latest~(current) frame contains \textit{Cliff Diving} action. It is worth noting that, as commonly observed in video sequences, not every history frame is informative for current frame prediction (\eg frames showing people cheering or camera panning in Fig.~\ref{fig:teaser}).
Existing transformer-based approaches~\cite{xu2021long} use vanilla cross-attention to learn attention weights for history frames that determine their contribution to the current frame prediction. Such attention weights do not correlate with how informative each history frame is to current frame prediction.
As shown in Fig.~\ref{fig:teaser}~(top row), when history frames are ordered from lower to higher cross-attention weights for vanilla cross-attention, frames that are informative for current frame prediction may have lower weights while uninformative frames may have higher weights, leading to incorrect current frame prediction. Another common challenge for existing methods is false positive prediction for background frames that closely resemble action frames~(\eg pre-shot routine before golf swing). 
Existing methods also do not leverage that although future frames are not available for the current frame prediction, subsequently observed frames that are future to the history can be leveraged to enhance history encoding, which in return improves current frame prediction.








To address the above limitations, we propose \methodname, \textbf{Gate}d \textbf{H}istory \textbf{U}nit with \textbf{B}ackground suppression. \methodname~comprises a novel Gated History Unit~(GHU), a position-guided gated cross-attention module that enhances informative history while suppressing uninformative frames via gated cross-attention (as shown in Fig.~\ref{fig:teaser}, bottom row). GHU enables \methodname~to encode more informative history into the latent encoding to better predict for current frame. GHU combines the benefit of an LSTM-inspired gating mechanism to filter uninformative history with the transformer’s ability to effectively learn from long sequences.

\methodname~leverages \textit{future frames for history} by introducing Future-augmented History~(FaH). FaH extracts features for a history frame using its future, \ie the subsequently \textit{observed} frames. This makes a history frame aware of its future and helps it to be more informative for current frame prediction. To tackle the common false positives in prior art, \methodname~proposes a novel background suppression objective that has different treatments for low-confident action and background predictions. These novel approaches enable \methodname~to outperform all existing methods on common benchmark datasets THUMOS~\cite{THUMOS14}, TVseries~\cite{de2016online}, and HDD~\cite{ramanishka2018toward}.  Keeping model efficiency in mind for the online setting, we also validate that \methodname~is more efficient than the existing best method~\cite{xu2021long} while being more accurate. Moreover, our proposed optical flow-free variant is  faster than all existing methods that require both RGB and optical flow data with higher or close accuracy.
To summarize, our main contributions are:
\begin{enumerate}
	\item Gated History Unit~(GHU), a novel position-guided gated cross-attention that explicitly enhances or suppresses parts of video history as per how informative they are to predicting action for the current frame.
	\item Future-augmented History~(FaH) to extract features for a history frame using its subsequently observed frames to enhance history encoding. 
\item A background suppression objective to mitigate the false positive prediction of background frames that closely resemble the action frames.
	\item \methodname~is more accurate than all existing methods and is also more efficient than the existing best work. Moreover, our proposed optical flow-free model is  faster compared to all existing methods that require both RGB and optical flow information while achieving higher or close accuracy.
\end{enumerate}



















%
 \section{Related Work}
\begin{figure*}[t]
\begin{center}
\includegraphics[width=\textwidth]{figures/model_overview_v13.pdf}
\end{center}
\vspace{-1.5em}
\caption{{\bf Model Overview.} \methodname~comprises a novel Gated History Unit~(GHU)~(a) as part of History Encoder~(b) to explicitly enhance or suppress history frames, \ie streaming video frames observed so far, as per how informative they are to current frame prediction. GHU encodes them by cross-attending with a latent encoding~(Q). \methodname~uses Future-augmented History features~(FaH)~(d) to encode each history frame using  subsequently observed future frames. The Present Decoder~(c) correlates with history by cross-attending the encoded history with the present, \ie, a small set of most recent frames, to make current frame prediction. We subject the prediction to a background suppression loss~(e) to reduce false positives by effectively separating action frames from closely resembling background frames.} 
\label{fig:gatehub_model}
\vspace{-1em}
\end{figure*}



{\noindent \bf Online Action Detection.} 
Previous methods for online action detection include use 3D ConvNet~\cite{de2016online}, reinforcement learning~\cite{gao2017red}, recurrent networks~\cite{xu2019temporal, eun2020learning, qu2020lap, gao2020woad,zhao2020privileged,qu2020lap} and more recently, transformers~\cite{wang2021oadtr, xu2021long}. The primary challenge in leveraging history is that for long untrimmed videos, its length becomes intractably long over time. To make it computationally feasible, some ~\cite{eun2020learning, wang2021oadtr, gao2020woad,qu2020lap} make the online prediction conditioned only on the most recent frames spanning less than a minute. This way the history beyond 
this duration 
that might be informative to current frame predictions is left unused. 
TRN~\cite{xu2019temporal} mitigates this by the hidden state in LSTMs~\cite{hochreiter1997long} to memorize the entire history during inference.
But LSTM limits its ability to model long-range temporal interactions. More recently, \cite{xu2021long} proposes to scale transformers to the history spanning longer duration.
However, not every history frame is informative and useful. \cite{xu2021long} lacks the forgetting mechanism of LSTM to filter uninformative history which causes it to encode uninformative history into the encoding leading to incorrect predictions. Our Gated History Unit~(GHU) and Future-augmented History~(FaH) combine the benefits of LSTM's selective encoding and transformer's long range modeling to leverage long-duration history more informatively to outperform all previous methods. 




{\noindent \bf Transformers for Video Understanding.} Transformers can achieve superior performance on video understanding tasks by effectively modeling the spatio-temporal context via attention. Most of the previous transformer-based methods~\cite{bertasius2021space,arnab2021vivit,fan2021multiscale, neimark2021video} focus on action recognition in trimmed videos~\cite{carreira2017quo}~(videos spanning few seconds) due to the quadratic complexity \wrt video length. Untrimmed videos have a longer duration from a few minutes to hours and contain frames with irrelevant actions~(labeled as background). Temporal action localization (TAL)~\cite{shou2016temporal,xu2017r,gao2017turn,shou2017cdc,Zhao_2017_ICCV,buch2017sst,liu2019multi,lin2019bmn,zhu2021enriching,zhang2021temporal} and temporal action proposal generation (TAP)~\cite{lin2018bsn,lin2019bmn,tan2021relaxed} are two fundamental tasks in untrimmed video understanding. AGT\cite{nawhal2021activity} proposes activity graph transformer for TAL based on DETR~\cite{carion2020end}. TAPG\cite{wang2021temporal} applies transformer to predict the activity boundary for TAP. However, unlike TAL and TAP which are both offline tasks having access to the entire video, online action detection does not have access to the future and requires causal understanding from history to present. We follow the existing transformer-based streaming tasks\cite{girdhar2021anticipative, chen2021developing, xu2021long} and apply a causal mask to address online action detection.




{\noindent \bf Long Sequence Modeling.} To model long input sequences, recent work~\cite{dosovitskiy2020image} proposes to reduce complexity by factorizing~\cite{touvron2021training} or subsampling the inputs~\cite{chen2020generative}. Another group of work focuses on modifying the internal dense self-attention module to boost the efficiency~\cite{beltagy2020longformer,wang2020linformer}. More recently, Perceiver~\cite{jaegle2021perceiver} and PerceiverIO~\cite{jaegle2021perceiverio} propose to cross-attend long-range inputs to a small fixed-sized latent encoding, adding further flexibility in terms of input and reducing the computational complexity. However, unlike our GHU, PerceiverIO lacks an explicit mechanism to enhance/suppress history frames making it sub-optimal for online action detection. Our method uses LSTM-inspired gating to calibrate cross-attention to enhance/suppress history frames per their informative-ness while employing transformers to learn from long history sequences effectively.











%
 \section{Methodology}

Given a streaming video sequence
,
our task is to identify \textit{if} and \textit{what} action  occurs at the current frame . We have a total of  action classes and label 0 for background frames with no action. Since future frames , are NOT accessible, the model makes the -way prediction for the current frame based on the recent  frames, , 
observed up until the current frame. While  may be large in an untrimmed video stream, as shown in the top row of Fig.~\ref{fig:teaser}, all frames observed in past history  may not be  equally informative to the prediction for the current frame.







\subsection{Gated History Unit based History Encoder}
To make the -way prediction accurately for current frame  based on  history frames, , we employ transformers to first encode the video sequence history and then associate the current frame with the encoding for prediction. Inspired by the recently introduced PerceiverIO~\cite{jaegle2021perceiverio}, our method consists of a History Encoder (Fig.~\ref{fig:gatehub_model}b) that uses cross-attention to project the variable length history to a fixed-length learned latent encoding. Using cross-attention is more efficient than using self-attention because its computational complexity is quadratic \wrt latent encoding size instead of the video sequence length which is typically orders of magnitude larger. This is crucial to developing a model for the online setting. However, as shown in Fig.~\ref{fig:teaser}, vanilla cross-attention, as used in PerceiverIO and LSTR~\cite{xu2021long}, fails to learn attention weights for history frames that correlate with how informative each history frame is for  prediction. 
We therefore introduce a novel Gated History Unit~(GHU) (Fig.~\ref{fig:gatehub_model}a) that has a position-guided gated cross-attention mechanism which learns a set of gating scores  to calibrate the attention weights to effectively enhance or suppress history frames based on how informative they are to current frame prediction. 









Specifically, given  
as the streaming sequence of  history frames ending at current frame , we encode  with a feature extraction backbone, , followed by a linear encoding layer . We then subject the output to a learnable position encoding, , relative to the current frame, , to give  
where , ,  and .  and  denote the dimensions of extracted features and post-linear encoding features, respectively. We also define a learnable latent query encoding, , that we cross-attend with . Following the standard multi-headed cross-attention setup~\cite{jaegle2021perceiver, jaegle2021perceiverio}, let  be the number of heads in GHU such that , ,  be the queries, keys and values, respectively, for each head ~(Fig.~\ref{fig:gatehub_model}a) where projection matrices  and . We assign  in our set up~\cite{vaswani2017attention}. Next, we obtain the position-guided gating scores, , for  as,

where  is the matrix projecting each history frame to a scalar.  is a sequence of scalars for the history frames  after applying sigmoid .  is the gating score sequence for history frames in GHU. By using  which already contains the position encoding, the gates are guided by the relative position of the history frame to the current frame . As also shown in Fig.~\ref{fig:gatehub_model}a, we now compute the gated cross-attention for each head, , as,

and multi-headed gated cross-attention defined as,

where  re-projects the attention output to  dimension.  It is possible to define  separately for each head but in our method, we find sharing  across all heads to perform better~(Sec.~\ref{sec:ablation}). From Eqn.~\ref{eqn:gate_sigmoid} and \ref{eqn:gate_val}, we can observe that each scalar in  lies in  due to sigmoid which implies that each gating score in  lies in . This enables the softmax function in Eqn.~\ref{eqn:ghu} to calibrate the attention weight for each history frame by a factor in  such that a factor in  suppresses a given history frame and a factor in  enhances a given history frame. This provides an explicit ability to GHU to learn to calibrate the attention weight of a history frame based on how informative the history frame is for prediction of . 
Unlike previous methods with relative position bias~\cite{liu2021swin,dai2019transformer},  is input-dependent and learns based on the history frame and its position \wrt current frame. This enables GHU to assess how informative each history frame is based on its feature representation and relative position from the current frame . We feed the output of GHU to a series of  self-attention layers to obtain the final history encoding~(Fig.~\ref{fig:gatehub_model}b).























\subsection{Hindsight is 2020: Future-augmented History}
Existing methods~\cite{wang2021oadtr,xu2019temporal,xu2021long, gao2020woad, eun2020learning} extract features for each frame by feed-forwarding the frame and optionally, a small set of past consecutive frames through pretrained networks like TSN~\cite{wang2016temporal} and I3D~\cite{carreira2017quo}. It is worth noting that although for current frame prediction its future is not available, for the history frames their \textit{future} is accessible and this \textit{hindsight} can potentially improve the encoding of history for current frame prediction. Existing methods do not have a mechanism to leverage this.
This inspires us to propose a novel feature extraction scheme, Future-augmented History~(FaH), where we aggregate observed future information into the features of a history frame to make it aware of its so far observable future. 
Fig.~\ref{fig:gatehub_model}d illustrates the FaH feature extraction process. For a history frame  and a feature extraction backbone , when  \textit{future history frames} for  can be observed, FaH extracts features for  using a set of frames  (\ie history frame itself and its subsequently observed  future frames). Otherwise, FaH extracts features for  using a set of frames  (\ie history frame itself and its past  frames),

At each new time step with one more new frame getting observed, FaH will feed-forward through  twice to extract features for (1) the new frame using  frames and (2)  that is now eligible to aggregate future information using  frames~(as shown in Fig.~\ref{fig:gatehub_model}d purple and green cuboid respectively). Thus, FaH has the same time complexity as existing feature extraction methods. FaH does not trivially incorporate all available subsequently observed frames. Instead, it encodes only from a set of future frames that are the most relevant to a history frame (as we empirically explain later in Section~\ref{sec:ablation}).




\subsection{Present Decoder}
In order to correlate the present with history to make current frame prediction, we sample a subset of  most recent history frames  to model the present~(\ie the most immediate context) for  using the Present Decoder~(Fig.~\ref{fig:gatehub_model}c). After extracting the features via FaH, we apply a learnable position encoding, , to each of the  frame features and subject them to a multi-headed self-attention with a causal mask. The causal mask limits the influence of only the preceding frames on a given frame. We then cross-attend the output from self-attention with the history encoding from the History Encoder. Inspired by Perceiver~\cite{jaegle2021perceiver}, we repeat this process twice and the self-attention does not need a causal mask the second time. Finally, we feed the output corresponding to each of  frames to the classifier layer for prediction.





























\subsection{Background Suppression Objective}
\label{sec:bg_suppression}
Existing online action detection methods~\cite{wang2021oadtr,xu2019temporal,xu2021long, gao2020woad, eun2020learning} apply standard cross entropy loss for -way multi-label per-frame prediction. Standard cross entropy loss does not consider that the ``no action'' background class does not belong to any specific action distribution and is semantically different from the  action classes. This is because background frames can be anything from completely blank at the beginning of a video to closely resemble action frames without actually being action frames (\eg, aiming before making a billiards shot). The latter is a common cause for false positives in online action detection. In addition to the complex distribution of background frames, untrimmed videos suffer from a sharp data imbalance where background frames significantly outnumber action frames. 

To tackle these challenges, we design a novel background suppression objective that applies separate emphasis on low-confident action and background predictions during training to increase the margin between action and background frames~(Fig.~\ref{fig:gatehub_model}e). Inspired by focal loss~\cite{lin2017focal}, our objective function,  for frame  is defined as,

where  enables low-confident samples to contribute more to the overall loss forcing the model to put more emphasis on correctly predicting these samples. Unlike original focal loss~\cite{lin2017focal}, our background suppression objective specializes for online action detection by applying separate  to action classes and background. This separation is necessary to distinguish the action classes that have a more constrained distribution from the background class whose distribution is more complex and unconstrained. Our objective is the first attempt in online action detection to put separate emphasis on low-confident hard action and background predictions. 








  


\subsection{Flow-free Online Action Detection}
Existing methods~\cite{xu2019temporal, wang2021oadtr, eun2020learning} for online action detection use optical flow in addition to RGB to capture fine-grained motion among frames. Computing optical flow takes much more time than feature extraction or model inference, and can be unrealistic for time-critical applications such as autonomous driving. This motivates us to develop an optical flow-free version of \methodname~that is able to achieve higher or close accuracy compared to existing methods without time-consuming optical flow estimation.
To capture motion without optical flow using only RGB frames, we leverage multiple temporal resolutions 
using a spatio-temporal backbone such as TimeSformer~\cite{bertasius2021space}. We extract two feature vectors for a frame  by encoding a frame sequence sampled at a higher frame rate spanning a smaller time duration and another frame sequence sampled at a lower frame rate spanning a larger time duration. Similar to the setup using RGB and optical flow features, we concatenate the two feature vectors before feeding them to \methodname.




%
 \section{Experiments}


\subsection{Datasets}
Following existing online action detection work~\cite{wang2021oadtr, xu2019temporal, eun2020learning, gao2017red, xu2021long}, we evaluate \methodname~on three common benchmark datasets -- THUMOS'14, TVSeries, and HDD.

{\bf THUMOS’14}~\cite{THUMOS14} consists of over 20 hours of sports video and is annotated with 20 actions.
We follow prior work~\cite{wang2021oadtr, xu2019temporal} and train on the validation set~(200 untrimmed videos) and evaluate on the test set~(213 untrimmed videos). 

{\bf TVSeries}~\cite{de2016online} includes 27 episodes of 6 popular TV shows with a total duration of 16 hours. It is annotated with 30 real-world everyday actions, \eg open door, run, drink.


{\bf HDD}~(Honda Research Institute Driving Dataset)~\cite{RamanishkaCVPR2018} includes 137 driving videos with a total duration of 104 hours. Following prior work~\cite{wang2021oadtr}, we use the vehicle sensor as input signal and divide data into 100 sessions for training and 37 sessions for testing. 

\subsection{Implementation Details}
For TVSeries and THUMOS'14, following~\cite{wang2021oadtr, xu2019temporal, eun2020learning, gao2017red, xu2021long}, we resample the videos at 24 FPS~(frames per second) and then extract frames at 4 FPS for training and evaluation. The sizes of \textit{history} and \textit{present} are set to 1024 and 8 most recently observed frames, respectively, spanning durations of 256s and 2s correspondingly at 4 FPS.  
For HDD, following OadTR~\cite{wang2021oadtr}, we extract the sensor data at 3 FPS for training and evaluation. The sizes of \textit{history} and \textit{present} are 48 and 6 most recently observed frames respectively, spanning durations of 16s and 2s correspondingly at 3 FPS.


{\bf Feature Extraction.}
Following~\cite{xu2021long, wang2021oadtr}, we use mmaction2~\cite{2020mmaction2}-based two-stream TSN~\cite{wang2016temporal} pretrained on Kinetics-400~\cite{carreira2017quo} to extract frame-level RGB and optical flow features for THUMOS'14 and TVSeries. 
We concatenate the RGB and optical flow features along channel dimension before feeding to the linear encoding layer in \methodname. For HDD, we directly feed the sensor data as input to \methodname.
To fully leverage the proposed FaH, the feature extraction backbone needs to support multi-frame input. Since TSN only supports single-frame input, we explore spatio-temporal TimeSformer~\cite{bertasius2021space}~(pretrained on Kinetics-600 using  frame sampling) that supports multiple-frame input. 
We set the time duration for past  and future  frames under FaH to be 1s and 2s respectively. 
We use TimeSformer to extract RGB features and use TSN-based optical flow features as TimeSformer only supports RGB. We also demonstrate FaH using RGB features from I3D~\cite{carreira2017quo} with results in the supplementary. For our flow-free version, we replace optical flow features with features obtained from an additional multi-frame input of RGB frames uniformly sampled from a duration of 2s. Please refer to supplementary for additional details.




{\bf Training.}
We train \methodname~for 10 epochs using Adam optimizer~\cite{kingma2014adam}, weight decay of ,  batch size of 50, OneCycleLR learning rate schedule of PyTorch~\cite{paszke2017automatic} with pct\_start of 0.25, , latent encoding size , number of self-attention layers in History Decoder ,  for each attention layer and  for background suppression.





{\bf Evaluation Metrics}
We follow the protocol of per-frame mean average precision~(mAP) for THUMOS and HDD and calibrated average precision (mcAP)~\cite{de2016online} for TVSeries.

\subsection{Comparison with State-of-the-Art}


\begin{table}[h]
\vspace{-1em}
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{llccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Feature Backbone} & THUMOS14  \\  & \multicolumn{1}{l}{RGB} & \multicolumn{1}{c}{Optical Flow} & mAP~(\%)  \\ \hline
FATS~\cite{kim2021temporally}  &  \multirow{8}{*}{TSN} & \multirow{8}{*}{TSN}    & 59.0        \\
      IDN~\cite{eun2020learning}   &   &   & 60.3        \\
TRN~\cite{xu2019temporal}   &    &     & 62.1         \\

PKD~\cite{zhao2020privileged} &  &  & 64.5 \\
OadTR~\cite{wang2021oadtr} &    &    & 65.2     \\
WOAD~\cite{gao2020woad}  &     &   & 67.1        \\
LSTR~\cite{xu2021long}  &     &    & 69.5      \\
\methodname~(Ours)  &      &     & \textbf{70.7}        \\ \hline
TRN~\cite{xu2019temporal}   &  \multirow{4}{*}{TimeSformer} & \multirow{4}{*}{TSN}  &   68.5 &   \\
OadTR~\cite{wang2021oadtr} &   &    &  65.5     \\
LSTR~\cite{xu2021long}  &     &      &   69.6   \\
\methodname~(Ours)  &      &      & \textbf{72.5}       \\ \hline
\end{tabular}}
\vspace{-0.5em}
\caption{Online action detection results on THUMOS'14 comparing \methodname~with SoTA methods on mAP~(\%) when the RGB-based features are extracted from either TSN or TimeSformer. Optical flow-based features are extracted from TSN in all settings.}
\label{tab:thumos}
\vspace{-0.75em}
\end{table}



%
 Table~\ref{tab:thumos} compares \methodname~with existing state-of-the-art~(SoTA) online action detection methods on THUMOS'14 for two different setups, one using RGB features from TSN~\cite{wang2016temporal} and the other using RGB features from TimeSformer~\cite{bertasius2021space}. Both setups use optical flow features from TSN. WOAD~\cite{gao2020woad} uses RGB features from I3D~(equivalent to TSN). For TSN RGB features, all mAP in Table~\ref{tab:thumos} are as reported in the references. 
For TimeSformer RGB features, we use the official code for TRN, OadTR and LSTR for fair comparison.
From the table, we can observe that \methodname~outperforms all existing methods by at least  when using RGB features from TSN. Moreover, \methodname~ outperforms existing methods by a larger margin of at least  using RGB features from TimeSformer.
\methodname~is also the first approach to surpass  on THUMOS'14 benchmark. 
This validates that \methodname, comprising GHU, Background Suppression and FaH to holistically leverage the long history more informatively, outperforms all SoTA on THUMOS’14.



We further compare \methodname~with SoTA on TVSeries and HDD in Table~\ref{tab:tvseries_hdd}a and~\ref{tab:tvseries_hdd}b, respectively. Following protocol, we use RGB and optical flow features from TSN for TVSeries and sensor data for HDD. All results from SoTA are as reported in the references. We can observe that \methodname~outperforms all SoTA on both TVSeries and HDD. The large improvement on HDD using sensor data validates that \methodname~is also effective on data modalities other than RGB or optical flow. 
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{cc}  
\begin{tabular}{lc}
\hline
Method  & mcAP~(\%) \\\hline
FATS~\cite{kim2021temporally}     & 84.6       \\
      IDN~\cite{eun2020learning}        & 86.1       \\
TRN~\cite{xu2019temporal}      & 86.2        \\

PKD~\cite{zhao2020privileged}    & 86.4  \\
OadTR~\cite{wang2021oadtr}    & 87.2         \\
LSTR~\cite{xu2021long}       & 89.1      \\
\methodname~(Ours)         & \textbf{89.6}        \\ \hline
\end{tabular}&  
     \begin{tabular}{lc}
\hline
Method &  mAP~(\%) \\ \hline CNN~\cite{de2016online}   &   22.7 \\
LSTM~\cite{ramanishka2018toward}  &  23.8 \\
RED~\cite{gao2017red}   &   27.4 \\
TRN~\cite{xu2019temporal}  &   29.2 \\
OadTR~\cite{wang2021oadtr} &   29.8 \\
\methodname~(Ours)   & \textbf{32.1} \\ \hline
\end{tabular}\\
     (a) & (b)  \\
\end{tabular}
}
\vspace{-1em}
\caption{Online action detection results comparing \methodname~with state-of-the-art methods on (a) TVSeries using RGB + Optical Flow data as input on mcAP metric and (b) HDD using sensor data as input on mAP metric.}
\label{tab:tvseries_hdd}
\vspace{-1em}
\end{table} 





















\begin{table*}[h]
\centering
\setlength{\tabcolsep}{4pt}
\resizebox{0.9\textwidth}{!}{\begin{tabular}{ccc} 
\begin{tabular}{l|c}
\hline
Method & mAP~(\%)    \\ \hline
w/ GHU (Ours)           & \textbf{70.7} \\ 
w/o GHU             & 69.6 \\
w/ GHU suppress only     & 70.5    \\ 
w/ GHU enhance only      & 70.5 \\ 
w/ GHU w/o position-guidance    & 70.3 \\ 
w/ GHU per head            &  68.0\\\hline
\end{tabular} 
&
\begin{tabular}{ll}
\hline
Method &  mAP~(\%)    \\ \hline
Ours        & \textbf{70.7} \\
Ours             &  70.2 \\  
w/ cross-entropy       & 69.9 \\
w/ standard focal loss      & 70.2 \\ \hline
\end{tabular}&
     \begin{tabular}{lcc}
\hline
Method & Future Duration & mAP~(\%)    \\ \hline
w/o FaH    & -       & 71.5 \\ \hline
\multirow{4}{*}{w/ FaH}    & 0.5       & 71.1 \\
& 1s       & 72.0 \\
& 2s        &  \textbf{72.5} \\
    & 4s        & 71.4 \\ \hline
\end{tabular}\\
     (a) & (b) & (c) \\
\end{tabular}
}
\vspace{-1em}
\caption{Ablation study comparing different variants of (a) Gated History Unit~(GHU), (b) background suppression objective and (c) Future-augmented History~(FAH). Ablation in (a) and (b) is conducted with RGB features from TSN and in (c) are conducted with RGB features from TimeSformer. Optical flow features from TSN are used in all settings.
}
\label{tab:ghu_bg_fah}
\vspace{-1.25em}
\end{table*} \subsection{\methodname: Ablation Study}
\label{sec:ablation}
In this section, we conduct an ablation study to highlight the impacts of the novel components of \methodname. Unless stated otherwise, all experiments are on THUMOS'14 using RGB and optical flow features from TSN. 

{\noindent \bf Impact of Gated History Unit~(GHU).} We conduct an experiment where we test different variants of our Gated History Unit~(GHU) by removing one or more of its design elements. Table~\ref{tab:ghu_bg_fah}a summarizes the results of this experiment. In the table, `w/o GHU' refers to replacing GHU with vanilla cross-attention from Perceiver IO~\cite{jaegle2021perceiverio} and LSTR~\cite{xu2021long}, \ie, . In `w/ GHU enhance only',  we remove  from Eqn.~\ref{eqn:gate_val} that suppresses history frames, \ie . Conversely, in `w/ GHU suppress only', we remove  from Eqn.~\ref{eqn:gate_val} that enhances history frames, \ie . In `w/ GHU w/o position guidance', we operate on frame features before subjecting them to learned position encoding, \ie  where . We also compare with `w/ GHU per head' where G is learned separately for each cross-attention head.


Table~\ref{tab:ghu_bg_fah}a shows that our implementation of GHU significantly outperforms all other variants of GHU and cross-attention. 
We can observe that `w/o GHU' performs  worse than `w/ GHU'. This is because, without explicit gating, vanilla cross-attention fails to learn attention weights for history frames that correlate with how informative history frames are to current frame prediction~(also depicted in Figure~\ref{fig:teaser}).
Moreover, the lower performances of `w/ GHU suppress only' and `w/ GHU enhance only' validate that we need to both enhance the informative history frames and suppress the uninformative ones to achieve the best performance. Without the ability to both enhance and suppress, the model may encode uninformative history frames into the latent encoding or inadequately emphasize the informative ones, leading to worse performance. 
The performance is also lower when using history frame features without position encoding (`w/ GHU w/o position guidance'). This is because without position guidance, the model cannot assess the relative position of a particular history frame \wrt the current frame which is an important factor in deciding how informative a history frame is to current frame prediction. We also find having separate G per head (`w/ GHU per head) performs much worse than sharing G across heads due to overfitting from  times more parameters.






{\noindent \bf Impact of Background Suppression.} We compare our background suppression objective with standard cross-entropy loss~(\ie, ) and standard focal loss(\ie, )~\cite{lin2017focal} as shown in Table~\ref{tab:ghu_bg_fah}b. First, compared to our background suppression objective, both standard cross-entropy and focal loss achieve lower accuracy. This validates that it is important to put separate emphasis on the low-confident action and background predictions to effectively differentiate action frames and closely resembling background frames. Furthermore, we find that across different combinations of  and , choosing a pair where  leads to higher accuracy. Specifically, we find  and  to give the highest accuracy. This can be attributed to the high data imbalance. Action frames are much lower in number than background frames and therefore require a stronger emphasis than background. 

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{4pt}
\resizebox{\textwidth}{!}{\begin{tabular}{ccc}  
\begin{tabular}{@{}l|lc@{}}
\hline
Method & mAP~(\%)    \\ \hline
Ours & \textbf{70.7} \\
w/o self-attention             & 67.7 \\
w/ cross-attention only at layer 1            & 68.6 \\
w/ disjoint history and present & 69.4\\\hline
\end{tabular}
&
\multicolumn{2}{c}{
\begin{tabular}{@{}l|c|c|cccc|c|@{}c@{}}  \hline
\multirow{3}{*}{Method}    & \multicolumn{2}{c|}{Model} & \multicolumn{5}{c|}{Inference Speed (FPS)} & \multirow{2}{*}{{ mAP(\%)}} \\ \cline{2-8}
& \multirow{2}{*}{\makecell[c]{Parameter\\Count}} & \multirow{2}{*}{GFLOPs} & \multirow{2}{*}{\makecell[c]{Optical Flow\\Computation}} & \multirow{2}{*}{\makecell[c]{RGB Feature\\Extraction}}  & \multirow{2}{*}{\makecell[c]{Flow Feature\\Extraction}}  & Model & Overall & \\
&  & &  &  &  &  & & \\ \hline
TRN~\cite{xu2021co}   & 402.9M &    1.46    & 8.1          & 70.5                 &      14.6                   & 123.3    &  8.1  & 62.1       \\
OadTR~\cite{wang2021oadtr}   & 75.8M & 2.54  & 8.1          & 70.5                 & 14.6                    &     110.0     & 8.1 & 65.2 \\
LSTR~\cite{xu2021long}(Flow-free)   & 54.2M & 6.33  & -       & 22.7                     & -                  &   99.2   & 22.7 & 63.5 \\ 
LSTR~\cite{xu2021long}   & 58.0M  &   7.53   & 8.1        & 70.5                     & 14.6                    & 91.6     & 8.1 &69.5 \\ \hline
Ours~(Flow-free) & 41.8M & 3.47 & -           &  22.7                      & -                     &  83.3        & \textbf{22.7} & 66.5 \\ 
Ours    & 45.2M  &  6.98   & 8.1         &  70.5                       & 14.6                       &    71.2    & 8.1 & \textbf{70.7} \\ \hline
\end{tabular}
}
     \\
     (a) & \multicolumn{2}{c}{(b)} \\
\end{tabular}
}
\vspace{-1em}
\caption{ (a) Ablation study for Present Decoder. (b) Efficiency comparison of \methodname~using RGB and optical flow features and our optical flow-free version with existing methods. \methodname~using RGB and optical flow has the least parameter count compared to existing methods, and higher accuracy and lower GFLOPs than the existing best method. Moreover, our flow-free version attains higher or close accuracy compared to existing methods that require RGB and optical flow features at  faster inference speed.}
\label{tab:present_efficiency}
\vspace{-1.25em}
\end{table*} {\noindent \bf Impact of Future-augmented History~(FaH).}
Table~\ref{tab:ghu_bg_fah}c shows the ablation on FaH. Since the TSN backbone is not compatible with multi-frame input, we conduct this study using RGB features from TimeSformer. The table shows that with 2s of future information incorporated into history features, we achieve the best accuracy which is  higher than without future-augmented history~(`w/o FaH'). The accuracy is also improved with 1s of future information incorporated into history features. We further observe that the accuracy drops when future duration is much longer \eg 4s or much shorter \eg 0.5s. This shows that making a history frame aware of its future enables it to be more informative for current frame prediction. At the same time, future duration up to a certain extent~(in our case, 2s) can encode meaningful future into history frames. Much beyond that, the future changes enough to be of little use for a given history frame, while much shorter future duration may also add noise rather than information. We wish to emphasize that all future duration are bound by the frames observed so far and do not extend into inaccessible future frames.


{\noindent \bf \methodname~Present Decoder.} Table~\ref{tab:present_efficiency}a shows the ablation study on our Present Decoder by altering different aspects of the design. Unlike the original PerceiverIO~\cite{jaegle2021perceiverio}, where the output queries are independent, we model the present~(equivalent of output queries in our method) via a causal self-attention and cross-attend it with history encoding multiple times (inspired by Perceiver~\cite{jaegle2021perceiver}). We can observe in Table~\ref{tab:present_efficiency}a that treating present frames independently~(`\ie w/o self-attention') and having only one cross-attention~(`\ie w/ cross-attention only at first layer') both reduce the accuracy considerably. Unlike LSTR~\cite{xu2021long} that uses a FIFO queue with disjoint long-term and short-term memory, in our design, the sequences of history and present frames fully overlap. Table~\ref{tab:present_efficiency}a shows that having disjoint history and present frames (\ie, `w/ disjoint history and present') leads to a  lower performance, further validating our design of Present Decoder and \methodname~overall.

\subsection{\methodname~Efficiency}
For online action detection setting, model efficiency is an important metric.
We compare \methodname~with existing methods \wrt parameter count, GFLOPs, and inference speed in terms of FPS as shown in Table~\ref{tab:present_efficiency}b. We first observe that \methodname~achieves the highest accuracy with the least number of model parameters compared to all existing methods. We also note that while methods like OadTR~\cite{wang2021oadtr} and TRN~\cite{xu2019temporal} are more efficient in terms of GFLOPs, their accuracy is much lower. \methodname~achieve a more favorable accuracy-efficiency trade-off with fewer GFLOPs than the existing best method LSTR~\cite{xu2021long} while obtaining a higher accuracy. All aforementioned methods require optical flow computation which is time-consuming, therefore the inference speed of these methods is governed by the optical flow computation speed of 8.1 FPS.
Meanwhile, our flow-free model obviates optical flow computation by using RGB features from TimeSformer at two different frame rates, and attains higher or close accuracy compared to existing work at  faster inference speed. 
When compared with flow-free LSTR,~\methodname~achieves 3\% higher mAP, thus providing a significantly better speed-accuracy tradeoff than the existing best method.
\begin{figure}[h]
\begin{center}
    \includegraphics[width=\columnwidth]{figures/ghu_vis_v2.pdf}
\end{center}
\vspace{-2em}
  \caption{Examples of the most suppressed and most enhanced history frames as per the gating score learned by GHU. Frames in the same row belong to the same video. 
}
\label{fig:ghu_vis}
\vspace{-1em}
\end{figure}
\subsection{Qualitative Evaluation}
{\noindent \bf Gated History Unit~(GHU).} We qualitatively assess the effect of GHU by visualizing examples of the most suppressed and most enhanced history frames in a streaming video when ordered as per the gating scores  learned by GHU in Eqn.~\ref{eqn:gate_val}. Fig.~\ref{fig:ghu_vis} shows examples from three videos where frames in the same row belong to the same video. From the figure, we can observe that GHU learns to suppress frames that exhibit no discernible action from the  action classes. The suppressed frames either have people arbitrarily moving or are uninformative background frames (\eg crowd cheering) that convey no useful information to predict action for the current frame. On the other hand, GHU learns to maximize emphasis on history frames with action from the  classes and on background frames that provide meaningful context to determine the current frame action (\eg long jump athlete running toward the pit).
\begin{figure}[t]
\begin{center}
    \includegraphics[width=\linewidth]{figures/visual_v3.pdf}
\end{center}
\vspace{-2em}
  \caption{Visualization of \methodname's online prediction. The curves indicate the predicted confidence of the ground-truth class (\textit{High Jump}) using TSN backbone with and without GHU.
}
  \vspace{-1.5em}
\label{fig:oad_vis}
\end{figure}

{\noindent \bf Current Frame Prediction.} We visualize \methodname's current frame prediction in Fig.~\ref{fig:oad_vis}. The confidence in the range  on y-axis denotes the probability of predicting the correct action~(\ie \textit{High Jump} in Fig.~\ref{fig:oad_vis}).
We can observe that \methodname~with GHU~(red) is effective in 
reducing false positives for background frames that closely resemble action frames compared to without GHU~(orange). 
Please refer to supplementary material with visualizations highlighting more online action detection scenarios.
























 \section{Conclusion and Future Work}
We present \methodname~for online action detection in untrimmed streaming videos. It consists of novel designs including Gated History Unit~(GHU), Future-augmented History~(FaH), and a background suppression loss to more informatively leverage history and reduce false positives for current frame prediction. \methodname~achieves higher accuracy than all existing methods for online action detection, and is more efficient than the existing best method. Moreover, its optical flow-free variant is  faster than previous methods that require both RGB and optical flow while obtaining higher or close accuracy.  

While \methodname~outperforms all existing methods, there is ample room for improvement. Although \methodname~can leverage long history, the length is still finite and may not be adequate when actions occur infrequently over long duration. It would be worthwhile to investigate ways to leverage history sequences of any length. Another challenge is slow motion action which is uncommon and can have considerably different temporal distribution, making it difficult to predict as accurately as common actions.



%
 ~\0pt]
\textbf{Acknowledgements}:
At Rochester Institute of Technology, Junwen Chen and Yu Kong are supported by NSF SaTC award 1949694, and the Army Research Office under grant number W911NF-21-1-0236. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government.


















































































































































{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\end{document}
