









\section{Ablation Study}
\label{sec:ablation}
\vspace{-0.1cm}



In this section, we validate the contribution of our main components, thoroughly analyzing the effectiveness of our design choices by conducting an ablation study. We focus on the fidelity to the input image which is an essential evaluation for image editing. In \cref{sec:results} we demonstrate that our method performs high-quality and meaningful manipulations.




\vspace{-0.4cm}
\paragraph{Experimental setting.} Evaluation is provided in \cref{fig:exp_graph}. We have used a subset of $100$ images and captions pairs, randomly selected from the \textit{COCO} \cite{chen2015microsoft} validation set.
We then applied our approach on each image-caption pair using the default Stable Diffusion hyper-parameters for an increasing number of iterations per diffusion step, $N=1,\ldots,20$ (see algorithm~\ref{alg:null}). The reconstruction quality was measured in terms of mean PSNR. We now turn to analyze different variations of our algorithm. 

\vspace{-0.4cm}
\paragraph{DDIM inversion.} We mark the DDIM inversion as a lower bound for our algorithm, as it is the starting point of our optimization, producing unsatisfying reconstruction when classifier-free guidance is applied (see \cref{sec:pivotal}).

\vspace{-0.4cm}
\paragraph{VQAE.} For an upper bound, we consider the reconstruction using the VQ auto-encoder \cite{esser2020taming}, denoted \textit{VQAE}, which is used by the Stable Diffusion model. Although the latent code or the VQ-decoder can be further optimized according to the input image, this is out of our scope, since it would be only applicable to this specific model \cite{rombach2021highresolution} while we aim for a general algorithm. Therefore, our optimization treats its encoding $z_0$ as ground truth, as the obtained reconstruction is quite accurate in most cases.


\begin{figure}
\setlength{\tabcolsep}{0.1pt}
{ \scriptsize 

\begin{tabular}{p{0.2\columnwidth}  p{0.2\columnwidth} p{0.2\columnwidth} p{0.2\columnwidth} p{0.2\columnwidth}}


\multicolumn{1}{c}{{ \bf Input}}&
\multicolumn{1}{c}{{ \bf Text2LIVE}}&
\multicolumn{1}{c}{{ \bf VQGAN+CLIP}}&
\multicolumn{1}{c}{{ \bf SDEdit}}&
\multicolumn{1}{c}{{ \bf Ours}} \\





































{\includegraphics[width=0.19\columnwidth]{images/gt/135.jpg}}&
{\includegraphics[width=0.19\columnwidth]{images/text2live/135.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/vqclip/135.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/sde/135.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/ours/135.jpg}} \\



\multicolumn{5}{c}{"A baby holding her \st{monkey} {\color{RoyalPurple} \bf zebra} doll."} \\

{\includegraphics[width=0.19\columnwidth]{images/gt/bicycle_039.jpg}}&
{\includegraphics[width=0.19\columnwidth]{images/text2live/bicycle_039.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/vqclip/039.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/sde/bicycle_039.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/ours/bicycle_039.jpg}} \\



\multicolumn{5}{c}{"A {\color{RoyalPurple} \bf blue } bicycle is parking on the side of the street"} \\



{\includegraphics[width=0.19\columnwidth]{images/gt/girl_nature_001.jpg}}&
{\includegraphics[width=0.19\columnwidth]{images/text2live/japan_007.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/vqclip/007.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/sde/japan_007.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/ours/japan_007.jpg}} \\



\multicolumn{5}{c}{"A girl sitting in a \st{field} {\color{RoyalPurple} \bf boat}"} \\










{\includegraphics[width=0.19\columnwidth]{images/gt/kid_tree.jpg}}&
{\includegraphics[width=0.19\columnwidth]{images/text2live/kid_tree_097.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/vqclip/097.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/sde/kid_tree_097.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/ours/kid_tree_097.jpg}} \\



\multicolumn{5}{c}{"A \st{child} {\color{RoyalPurple} \bf tiger} is climbing on a tree""} \\



\end{tabular}
}


\vspace{-0.2cm}
\caption{{\bf Comparison.} {\it Text2LIVE \cite{bar2022text2live} excels at replacing textures locally but struggles to perform more structured editing, such as replacing a kid with a tiger. VQGAN+CLIP \cite{katherine2021vqganclip} obtains inferior realism. SDEdit \cite{meng2021sdedit} fails to faithfully reconstruct the original image, resulting in identity drift when humans are involved. Our method achieves realistic editing of both textures and structured objects while retaining high fidelity to the original image. Additional examples provided in \cref{sec:supp_results} (\cref{fig:supp_comparison}). }}
\vspace{-0.25cm}
\label{fig:comparison} \end{figure} 
\vspace{-0.4cm}
\paragraph{Our method.} As can be seen in \cref{fig:exp_graph}, our method converges to a near-optimal reconstruction with respect to the VQAE upper bound after a total number of $500$ iterations ($N=10$) and even after $250$ iterations (\(\sim\) 1 minute on an A100 GPU) we achieve high-quality inversion. 

\vspace{-0.4cm}
\paragraph{Random Pivot.}
We validate the importance of the DDIM initialization by replacing the DDIM-based trajectory with a \textit{single random trajectory} of latent codes, sharing the same starting point $z_0$ --- the input image encoding. In other words, we randomly sample a single Gaussian noise $\sim N(0, I)$ for each image and use it to noise the corresponding encoding $z_0$ from $t=1$ to $t=T$ using the diffusion scheduler. As presented in \cref{fig:exp_graph}, the DDIM initialization is crucial for fast convergence, since the initial error becomes significantly larger when the pivot is random. 




\vspace{-0.4cm}
\paragraph{Robustness to different input captions.}
Since we require an input caption, it is only natural to ask whether our method is highly sensitive to the chosen caption. We take this to the extreme by sampling a \textit{random caption} from the dataset for each image. Even with unaligned captions, the optimization converges to an optimal reconstitution with respect to the VQAE. Therefore, we conclude that our inversion is robust to the input caption. Clearly, choosing a random caption is undesired for text-based editing. But, providing any reasonable and editable prompt would work, including using an off-the-shelve captioning model \cite{mokady2021clipcap, stefanini2022show}. This is illustrated in \cref{sec:ablation_supp} (\cref{fig:exp_multi_cap}). We invert an image using multiple captions, demonstrating that the edited parts should be included in the source caption in order to produce semantic attention maps for editing. For example, to edit the print on the shirt, the source caption should include a "shirt with a drawing" or a similar phrase.


\vspace{-0.4cm}
\paragraph{Global null-text embedding.}
We refer to optimizing a single embedding $\varnothing$ for all timestamps as a \textit{Global} embedding. As can be seen, such optimization struggles to converge, since it is less expressive than our final approach, which uses embedding per-timestamp $\{\varnothing_t\}_{t=1}^{T}$. See additional implementation details in \cref{sup_implementation}.



\vspace{-0.4cm}
\paragraph{Textual inversion.}
We compare our method to \textit{textual inversion}, similar to the proposed method by Gal et al.~\cite{gal2022image}. We optimize the textual embedding $\textemb = \psi(\mathcal{P})$ using random noise samples instead of pivotal inversion. 
That is, we randomly sample a different Gaussian noise for each optimization step and obtain $z_t$ by adding it to $z_0$ according to the diffusion scheduler. Intuitively, this objective aims to map all noise vectors to a single image, in contrast to our pivotal tuning inversion which focuses on a single trajectory of noisy vectors. The optimization objective is then defined: \\[-8pt]
\begin{equation}
\min_\textemb E_{z_0,\eps\sim N(0,I),t} \norm{\eps-\eps_\theta(z_t,t,\textemb)}^2.
\end{equation}
\\[-10pt]
Note that Gal et al.~\cite{gal2022image} have attempted to regenerate a specific object rather than achieve an accurate inversion. As presented in \cref{fig:exp_graph}, the convergence is much slower than ours and results in poor reconstruction quality.




\vspace{-0.4cm}
\paragraph{Textual inversion with a pivot.}
We observe that employing our pivotal inversion with the mentioned textual inversion improves the reconstruction quality significantly, results in a comparable reconstruction to ours. This further demonstrates the power of performing the optimization using a pivot. However, we do observe that editability is reduced compared to the null-text optimization. In particular, as demonstrated in \cref{sec:ablation_supp} (\cref{fig:albation_text_pivot}), the attention maps are less accurate which decreases the performance of Prompt-to-prompt editing.








\vspace{-0.4cm}
\paragraph{Null-text optimization without pivotal inversion.}
We observe that optimizing the unconditional null-text embedding using random noise vectors, instead of pivotal inversion as described in previous paragraphs, completely breaks the null-text optimization. The results are inferior even to the DDIM inversion baseline as presented in \cref{sec:ablation_supp} (\cref{fig:exp_ablation_qual_sup,fig:exp_ablation_qual_sup2}). We hypothesize that null-text optimization is less expressive than model-tuning and thus depends on the efficient pivotal inversion, as it struggles to map all noise vectors to a single image. 




\begin{figure}
\centering 
\vspace{-0.5cm}
\includegraphics[width=\columnwidth]{figures/inpainting_v3.pdf} 

\vspace{-0.2cm}
\caption{{\bf Comparison to mask-based methods .} {\it As can be seen, mask-based methods do not require inversion as the region outside the mask is kept. However, unlike our approach, such methods often struggle to preserve details that are found inside the masked region. For example, basket size is not preserved. } } 
\vspace{-0.4cm}
\label{fig:exp_inaint} 

\end{figure} 

\section{Results}
\label{sec:results}



Real image editing is presented in \cref{fig:exp_edit_people,fig:exp_reweight,fig:teaser}, showing our method not only reaches remarkable reconstruction quality but also retains high editability. In particular, we use the intuitive approach of Prompt-to-Prompt \cite{hertz2022prompt} and demonstrate that the editing capabilities which previously were constrained to synthesized images are now applied to real images using our inversion technique.



As can be seen in \cref{fig:exp_edit_people}, our method effectively modifies both textures ("floral shirt") and structured objects ("baby" to "robot"). Since we support the local editing of Prompt-to-Prompt and achieve high-fidelity reconstruction, the original identity is well preserved, even in the challenging case of a baby face.
\cref{fig:exp_edit_people} also illustrates that our method requires only a single inversion process to perform multiple editing operations. Using a single inversion procedure, we can modify hair color, glasses, expression, background, and lighting and even replace objects or put on a joker make-up (bottom rows). Using Prompt-to-Prompt, we can also attenuate or amplify the effect of a specific word over real images, as appeared in \cref{fig:exp_reweight}.
For additional examples, please refer to \cref{sec:supp_results}.


Visual results for our high-fidelity reconstruction are presented in \cref{fig:teaser,fig:sdedit_null}, and \cref{sec:supp_results} (\cref{fig:supp_comparison}), supporting our quantitative measures in \cref{sec:ablation}.






 \begin{table}
\caption{{\bf User study results.} {\it The participants were asked to select the best editing result in terms of fidelity to both the input image and the textual edit instruction.}}
 \footnotesize
 \centering
\vspace{-0.25cm}
\begin{tabular}{cccc}

\toprule

VQGAN+CLIP & Text2Live & SDEDIT & Ours \\
3.8\% & 16.6\% & 14.5\%  & {\bf 65.1\%} \\


\bottomrule
\end{tabular}
\vspace{-0.2cm}
\label{tab:user-study}
 \end{table}
 
  
\subsection{Comparisons}
\label{sec:comparisons}




Our method aims attention at intuitive editing using only text, and so we compare our results to other text-only editing methods: (1) \textit{VQGAN+CLIP} \cite{katherine2021vqganclip}, (2) \textit{Text2Live} \cite{bar2022text2live}, and (3) \textit{SDEedit} \cite{meng2021sdedit}. We evaluated these on the images used by Bar-Tal et al.~\cite{bar2022text2live} and photos that include more structured objects, such as humans and animals, which we gathered from the internet. In total, we use $100$ samples of images, input captions, and edited captions. 


We also compare our method to the mask-based methods of (4) Glide \cite{nichol2021glide}, (5) Blended-Diffusion \cite{avrahami2022blended}, and (6) Stable Diffusion Inpaint \cite{rombach2021highresolution}. The latter fine-tunes the diffusion model using an inpainting objective, allowing simple editing by inpainting a masked region using a target prompt.



Lastly, we consider the concurrent work of (7) Imagic \cite{Kawar2022ImagicTR}, which employs model-tuning per editing operation and has been designed for the Imagen model \cite{saharia2022photorealistic}. 
We refrain from comparing to the concurrent works of Unitune \cite{valevski2022unitune} and DiffEdit \cite{couairon2022diffedit} as there are no available implementations.







\vspace{-0.3cm}
\paragraph{Qualitative Comparison.}
As presented in \cref{fig:comparison}, VQGAN+CLIP \cite{katherine2021vqganclip} mostly produces unrealistic results. Text2LIVE \cite{bar2022text2live} handles texture modification well but fails to manipulate more structured objects, e.g., placing a boat (3rd row). 
Both struggle due to the use of CLIP \cite{radford2021learning} which lacks a generative ability.
In SDEdit \cite{meng2021sdedit}, the noisy image is fed to an intermediate step in the diffusion process, and therefore, it struggles to faithfully reconstruct the original details. This results in severe artifacts when fine details are involved, such as human faces. For instance, identity drifts in the top row, and the background is not well preserved in the 2nd row.
Contrarily, our method successfully preserves the original details, while allowing a wide range of realistic and meaningful editing, from simple textures to replacing well-structured objects.



\cref{fig:exp_inaint} presents a comparison to mask-based methods, showing these struggles to preserve details that are found inside the masked region. This is due to the masking procedure that removes important structural information, and therefore, some capabilities are out of the inpainting reach.



A comparison to Imagic \cite{Kawar2022ImagicTR}, which operates in a different setting -- requiring model-tuning for each editing operation, is provided in \cref{sec:ablation_supp} (\cref{fig:imagic}). We first employ the unofficial Imagic implementation for Stable Diffusion and present the results for different values of the interpolation parameter $\alpha = 0.6,0.7,0.8,0.9$. This parameter is used to interpolate between the target text embedding and the optimized one \cite{Kawar2022ImagicTR}. In addition, the Imagic authors applied their method using the Imagen model over the same images, using the following parameters $\alpha=0.93, 0.86, 1.08$.
As can be seen, Imagic produces highly meaningful editing, especially when the Imagen model is involved.
However, Imagic struggles to preserve the original details, such as the identity of the baby ($1$st row) or cups in the background ($2$nd row). Furthermore, we observe that Imagic is quite sensitive to the interpolation parameter $\alpha$, as a high value reduces the fidelity to the image and a low value reduces the fidelity to the text guidance, while a single value cannot be applied to all examples. Lastly, Imagic takes a longer inference time, as shown in \cref{sec:supp_results} (\cref{tab:times}).





\vspace{-0.3cm}
\paragraph{Quantitative Comparison.}

Since ground truth is not available for text-based editing of real images, quantitative evaluation remains an open challenge. Similar to \cite{bar2022text2live, hertz2022prompt}, we present a user study in \cref{tab:user-study}. $50$ participants have rated a total of $48$ images for each baseline. The participants were recruited using \emph{Prolific} (prolific.co). We presented side-by-side images produced by: VQGAN+CLIP, Text2LIVE, SDEdit, and our method (in random order). We focus on methods that share a similar setting to ours -- no model tuning and mask requirement. The participants were asked to choose the method that better applies the requested edit while preserving most of the original details. A print screen is provided in \cref{sup:user_study} (\cref{fig:user_study}).
As shown in \cref{tab:user-study}, most participants favored our method.


Quantitative comparison to Imagic is presented in \cref{sec:ablation_supp} (\cref{fig:exp_graph_imagic}), using the unofficial Stable Diffusion implementation. According to these measures, our method achieves better scores for LPIPS perceptual distance, indicating a better fidelity to the input image.

\label{tab:times}


\subsection{Evaluating Additional Editing Technique}


Most of the presented results consist of applying our method with the editing technique of Prompt-to-Prompt \cite{hertz2022prompt}. However, we demonstrate that our method is not confined to a specific editing approach, by showing it improves the results of the SDEdit \cite{meng2021sdedit} editing technique.

In \cref{fig:sdedit_null} (top), we measure the fidelity to the original image using LPIPS perceptual distance \cite{Zhang2018TheUE} (lower is better), and the fidelity to the target text using CLIP similarity \cite{radford2021learning} (higher is better) over $100$ examples. We use different values of the SDEdit parameter $t_0$ (marked on the curve), i.e., we start the diffusion process from different $t = t_0 \cdot T$ using a correspondingly noised input image. This parameter controls the trade-off between fidelity to the input image (low $t_0$) and alignment to the text (high $t_0$). We compare the standard SDEdit to first applying our inversion and then performing SDEdit while replacing the null-text embedding with our optimized embeddings. As shown, our inversion significantly improves the fidelity to the input image.

This is visually demonstrated in \cref{fig:sdedit_null} (bottom). Since the parameter $t_0$ controls a reconstruction-editability trade-off, we have used a different parameter for each method (SDEdit with and without our inversion) such that both achieve the same CLIP score. As can be seen, when using our method, the true identity of the baby is well preserved. 






\begin{figure}
\setlength{\tabcolsep}{0.2pt}
{ \scriptsize \vspace{-0.4cm}
\includegraphics[width=\columnwidth]{figures/experiments_graph_sdedit_pdf.pdf} 
\\[4pt]
\begin{tabular}{>{\centering\arraybackslash}p{0.2\columnwidth}  >{\centering\arraybackslash}p{0.2\columnwidth} >{\centering\arraybackslash}p{0.2\columnwidth} >{\centering\arraybackslash}p{0.2\columnwidth} >{\centering\arraybackslash}p{0.2\columnwidth}}



\multicolumn{1}{c}{{ \bf Input Image}}&
\multicolumn{1}{c}{{ \bf Our Inversion}}&
\multicolumn{1}{c}{{ \bf SDEdit}}&
\multicolumn{1}{c}{{ \bf Ours + SDEdit}}&
\multicolumn{1}{c}{{ \bf Ours + P2P}} \\





{\includegraphics[width=0.19\columnwidth]{images/gt/153.jpg}}&
{\includegraphics[width=0.19\columnwidth]{images/inversion/6336601581_f53b8b1e86_z.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/sde/153.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/sdedit_null/153.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/ours/153.jpg}} \\



\multicolumn{5}{c}{"{\color{RoyalPurple} \bf Macaroni} cake on a table."} \\

{\includegraphics[width=0.19\columnwidth]{images/gt/069.jpg}}&
{\includegraphics[width=0.19\columnwidth]{images/inversion/aviv.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/sde/068.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/sdedit_null/068.jpg}} &
{\includegraphics[width=0.19\columnwidth]{images/ours/068.jpg}} \\



\multicolumn{5}{c}{"A baby wearing a blue shirt lying on the \st{sofa} {\color{RoyalPurple} \bf beach}"} \\











\end{tabular}
}


\vspace{-0.25cm}
\caption{{\bf Our method improves SDEdit results.} {\it 
Top: we evaluate SDEdit with and without applying null-text inversion. 
In each measure, a different SDEdit parameter is used, i.e., different percent of diffusion steps are applied over the noisy image (marked on the curve). 
We measure both fidelity to the original image (via LPIPS, low is better) and fidelity to the target text (via CLIP, high is better).
Bottom, from left to right: input image, null-text inversion, SDEdit, applying SDEdit after null-text inversion, and applying Prompt-to-Prompt after null-text inversion. 
As can be seen, our inversion significantly improves the fidelity to the original image when applied before SDEdit. }}
\vspace{-0.4cm}
\label{fig:sdedit_null} \end{figure}





 

\section{Limitations}


While our method works well in most scenarios, it still faces some limitations. The most notable one is inference time. Our approach requires approximately one minute on GPU for inverting a single image. Then, infinite editing operations can be made, each takes only ten seconds. This is not enough for real-time applications.
Other limitations come from using Stable Diffusion \cite{rombach2021highresolution} and Prompt-to-Prompt editing \cite{hertz2022prompt}. First, the VQ auto-encoder produces artifacts in some cases, especially when human faces are involved. We consider the optimization of the VQ decoder as out of scope here, since this is specific to Stable Diffusion and we aim for a general framework. Second, we observe that the generated attentions maps of Stable Diffusion are less accurate compared to the attention maps of Imagen \cite{saharia2022photorealistic}, i.e., words might not relate to the correct region, indicating inferior text-based editing capabilities. 
Lastly, complicated structure modifications are out of reach for Prompt-to-Prompt, such as changing a seating dog to a standing one as in \cite{Kawar2022ImagicTR}.
Our inversion approach is orthogonal to the specific model and editing techniques, and we believe that these will be improved in the near future.











