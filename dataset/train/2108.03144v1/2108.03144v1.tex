

\documentclass[preprint,12pt]{elsarticle}







\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{url}
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algpseudocode} \usepackage{multirow}



\journal{Pattern Recognition}


\def\mat#1{\mathchoice{\mbox{\boldmath }}
{\mbox{\boldmath}}
{\mbox{\boldmath}}
{\mbox{\boldmath}}}

\def\vect#1{\mathchoice{\mbox{\boldmath }}
{\mbox{\boldmath  }}
{\mbox{\boldmath  }}
{\mbox{\boldmath  }}}

\def\textphd#1{\texttt{\textsc{#1}}}



\def\inv#1{{#1}^{-1}}

\def\dt{{\delta t}}
\def\dtau{{\delta \tau}}

\def\dif{{\mbox{d}}}

\def\v0{{\vect 0}}
\def\vone{{\vect 1}}

\def\va{{\vect a}}
\def\vb{{\vect b}}
\def\vc{{\vect c}}
\def\vd{{\vect d}}
\def\ve{{\vect e}}
\def\vf{{\vect f}}
\def\vg{{\vect g}}
\def\vh{{\vect h}}
\def\vi{{\vect i}}
\def\vj{{\vect j}}
\def\vl{{\vect l}}
\def\vm{{\vect m}}
\def\vn{{\vect n}}
\def\vp{{\vect p}}
\def\vq{{\vect q}}
\def\vo{{\vect o}}
\def\vr{{\vect r}}
\def\vs{{\vect s}}
\def\vt{{\vect t}}
\def\vu{{\vect u}}
\def\vv{{\vect v}}
\def\vw{{\vect w}}
\def\vx{{\vect x}}
\def\vy{{\vect y}}
\def\vz{{\vect z}}


\def\vA{{\vect A}}
\def\vB{{\vect B}}
\def\vE{{\vect E}}
\def\vC{{\vect C}}
\def\vD{{\vect D}}
\def\vF{{\vect F}}
\def\vH{{\vect H}}
\def\vI{{\vect I}}
\def\vJ{{\vect J}}
\def\vL{{\vect L}}
\def\vS{{\vect S}}
\def\vO{{\vect O}}
\def\vP{{\vect P}}
\def\vM{{\vect M}}
\def\vN{{\vect N}}
\def\vT{{\vect T}}
\def\vW{{\vect W}} \def\vX{{\vect X}}
\def\vY{{\vect Y}}
\def\vZ{{\vect Z}}

\def\vepsilon{{\vect\epsilon}}

\def\dvt{{\delta\vt}}

\def\vmu{{\vect\mu}}
\def\dvmu{{\delta\vmu}}
\def\dvc{{\delta\vc}}
\def\vSigma{{\vec\Sigma}}
\def\vomega{{\boldsymbol\omega}}
\def\vOmega{{\boldsymbol\Omega}}
\def\vtheta{{\vect\theta}}
\def\vphi{{\vect\phi}}
\def\vpsi{{\vect\psi}}
\def\vPi{{\vect\pi}}
\def\vpsi{{\vect\psi}}
\def\vPsi{{\vect\Psi}}
\def\vepsilon{{\vect\epsilon}}

\def\v0{{\vect 0}}
\def\vzero{{\vect 0}}
\def\vone{{\vect 1}}

\def\bvb{{\bar{\vb}}}
\def\bvu{{\bar{\vu}}}
\def\bvx{{\bar{\vx}}}
\def\bvI{{\bar{\vI}}}

\def\mDelta{{\mat\mDelta}}
\def\mSigma{{\mat\Sigma}}
\def\mGamma{{\mat\Gamma}}
\def\mLambda{{\mat\Lambda}}
\def\mOmega{{\mat\Omega}}
\def\mTau{{\mat\tau}}


\def\mZero{{\mat 0}}

\def\mm{{\mat m}}
\def\ms{{\mat s}}
\def\mA{{\mat A}}
\def\mB{{\mat B}}
\def\mC{{\mat C}}
\def\mD{{\mat D}}
\def\mE{{\mat E}}
\def\mF{{\mat F}}
\def\mh{{\mat h}}
\def\mH{{\mat H}}
\def\mI{{\mat I}}
\def\mJ{{\mat J}}
\def\mK{{\mat K}}
\def\mL{{\mat L}}
\def\mM{{\mat M}}
\def\mN{{\mat N}}
\def\mP{{\mat P}}
\def\mQ{{\mat Q}}
\def\mR{{\mat R}}
\def\mS{{\mat S}}
\def\mT{{\mat T}}
\def\mU{{\mat U}}
\def\mV{{\mat V}}
\def\mW{{\mat W}}
\def\mX{{\mat X}}
\def\mY{{\mat Y}}
\def\mZ{{\mat Z}}

\def\dmH{\delta{\mat H}}
\def\dmR{\delta{\mat R}}
\def\ddmR{\dot{\mR}}

\def\mZero{{\mat 0}}
\def\m1{{\mat 1}}


\def\cA{\mathcal{A}}
\def\cB{\mathcal{B}}
\def\cC{\mathcal{C}}
\def\cl{\ell}
\def\cD{\mathcal{D}}
\def\cF{\mathcal{F}}
\def\cE{\mathcal{E}}
\def\cF{\mathcal{F}}
\def\cG{\mathcal{G}}
\def\cH{\mathcal{H}}
\def\cI{\mathcal{I}}
\def\cJ{\mathcal{J}}
\def\cK{\mathcal{K}}
\def\cL{\mathcal{L}}
\def\cN{\mathcal{N}}
\def\cM{\mathcal{M}}
\def\cP{\mathcal{P}}
\def\cQ{\mathcal{Q}}
\def\cR{\mathcal{R}}
\def\cS{\mathcal{S}}
\def\cU{\mathcal{U}}
\def\cV{\mathcal{V}}
\def\cW{\mathcal{W}}
\def\cX{\mathcal{X}}
\def\cY{\mathcal{Y}}
\def\cZ{\mathcal{Z}}
\def\cT{\mathcal{T}}
\def\cUT{\mathcal{UT}}

\def\vcl{\vect \ell}
\def\vcL{\vect \cL}

\def\sfE{\mbox{\textsf{E}}}



\def\argmin{\mbox{arg}\min}


\def\Matrices{\mathbb{M}}
\def\Reales{\mathbb{R}}
\def\Pspace{\mathbb{P}}
\def\RigidBody{\mathbb{SE}}
\def\Rotations{\mathbb{SO}}


\def\hF{\hat{F}}
\def\hY{\hat{Y}}
\def\hvc{\hat{\vc}}
\def\hvf{\hat{\vf}}
\def\hvp{\hat{\vp}}
\def\hvr{\hat{\vr}}
\def\hvt{\hat{\vt}}
\def\hvu{\hat{\vu}}
\def\hvx{\hat{\vx}}
\def\hvy{\hat{\vy}}
\def\hvz{\hat{\vz}}
\def\hvX{\hat{\vX}}
\def\hvY{\hat{\vY}}
\def\hmR{\hat{\mR}}
\def\halpha{\hat{\alpha}}
\def\hbeta{\hat{\beta}}
\def\hgamma{\hat{\gamma}}
\def\hpsi{\hat{\psi}}
\def\hvpsi{\hat{\vpsi}}
\def\homega{\hat{\omega}}
\def\hvmu{\hat{\vmu}}

















\def\tve{\mathbf{\tilde{e}}}
\def\tvx{\mathbf{\tilde{x}}}
\def\tvu{\mathbf{\tilde{u}}}
\def\tmH{\tilde{\mH}}

\newcommand{\todo}[1]{\addcontentsline{tdo}{todo}{\protect{#1}}\begin{tikzpicture}[remember picture, baseline=-0.75ex]\node [coordinate] (inText) {};
  \end{tikzpicture}\marginpar{\begin{tikzpicture}[remember picture]\definecolor{orange}{rgb}{1,0.5,0}
      \draw node[draw=black, fill=orange, text width = 2.1cm] (inNote)
      {#1};\end{tikzpicture}}\begin{tikzpicture}[remember picture, overlay]\draw[draw = orange, thick]
    ([yshift=-0.2cm] inText)
    -| ([xshift=-0.2cm] inNote.west)
    -| (inNote.west);
  \end{tikzpicture}}

 
\begin{document}

\begin{frontmatter}





\title{ELSED: Enhanced Line SEgment Drawing}

\author[lbl:graffter,lbl:upm]{Iago Su\'arez}
\author[lbl:urjc]{Jos\'e M. Buenaposada}
\author[lbl:upm]{Luis Baumela}
\affiliation[lbl:graffter]{organization={The Graffter},
            addressline={Campus Montegancedo s/n. Centro de Empresas},
            city={Pozuelo de Alarc{\'o}n},
            postcode={28223},
            country={Spain}}

\affiliation[lbl:upm]{organization={Departamento de Inteligencia Artificial. Universidad Polit{\'e}cnica  de Madrid},
            addressline={Campus Montegancedo s/n},
            city={Boadilla del Monte},
            postcode={28660},
            country={Spain}}
\affiliation[lbl:urjc]{organization={ETSII. Universidad Rey Juan Carlos},
            addressline={C/ Tulip{\'a}n, s/n},
            city={M{\'o}stoles},
            postcode={28933},
            country={Spain}}

\begin{abstract}
Detecting local features, such as corners, segments or blobs, is the first step in the pipeline of many Computer Vision applications. Its speed is crucial for real time applications. 
In this paper we present ELSED, the fastest line segment detector in the literature. The key for its efficiency is a local segment growing algorithm that connects gradient aligned pixels in presence of small discontinuities. 
The proposed algorithm not only runs in devices with very low end hardware, but may also be parametrized to foster the detection of short or longer segments, depending on the task at hand. 
We also introduce new metrics to evaluate the accuracy and repeatability of segment detectors. In our experiments with different public benchmarks we prove that our method is the most efficient in the literature and quantify the accuracy traded for such gain\footnote{The source code of the method and experiments will be made public upon acceptance. \url{https://github.com/iago-suarez/ELSED}}.
\end{abstract}

\begin{graphicalabstract}
\end{graphicalabstract}

\begin{highlights}
\item Segment detection for low computational power devices.
\item Enhanced Edge Drawing algorithm.
\item Integrated capability of jumping over small discontinuities.
\item New methodology to evaluate accuracy and repeatability.
\item Best AP for efficient segment detectors and best overall repeatability. 
\end{highlights}

\begin{keyword}
Image edge detection, Efficient Line Segment Detection, Line Segment Detection Evaluation.






\end{keyword}

\end{frontmatter}



\section{Introduction}
\label{sec:introduction}

Detecting segments and full lines in digital images is a recurrent problem in Computer Vision (CV). 
Line segments play a important role towards understanding the geometric content of a scene as they are a compressed and meaningful representation of the image content. Moreover, segments are still present in low textured settings where the classical methods based on corners or blobs usually fail.
Segment detection has been employed in a large number of CV tasks such as
3D reconstruction~\cite{zhou2019learning, miraldo2018minimal, li2017line}, 
SLAM~\cite{li2020structure, gomez2019pl, pumarola2017pl},
Visual Odometry~\cite{gomez2016pl}, 
3D camera orientation via Vanishing Points Detection~\cite{lezama2014finding, suarez2018fsg}, 
cable detection in aircrafts~\cite{tian2021noncontact}, 
biomedical images~\cite{santosh2017line} 
or road detection in Synthetic Aperture Radar images~\cite{Liu2020LSDSAR}.

Nowadays CV algorithms are ubiquitous and they are expected to run on resource limited devices. To this end, low level algorithms such as the local feature detectors must be very efficient. 
Traditional global line detection approaches based on the Hough transform lack efficiency. Thus, various local methods emerged addressing the issue of efficiency. LSD~\cite{grompone2010lsd} was one of the first approaches to achieve excellent results with a local approach. 
Edge drawing methods further improve the efficiency~\cite{topal2012edge, akinlar2011edlines, zhang2021ag3line}. In a first step, they work by connecting edge pixels following the direction perpendicular to the gradient. In a second step, they fit the desired curve, a line in the simplest case, to these edges.

The method presented in this paper improves on the drawing methods by fitting a line segment to the connected edge pixels and using its direction to guide the drawing process. Fusing the drawing and line segment fitting in a single step saves time and improves the overall quality of the detected segments.
In addition, our proposal allows to jump over gradient discontinuities and detect full lines or just detect the individual linear segments without jumping.
This is important because line segments are features that, at the gradient level, can be easily broken by occlusions, shadows, glitches, etc.
In this way, the user can define the type of segments that best suits the application. For example, we may choose to detect large segments if the goal is to do Vanishing Points estimation or short ones for reconstruction and matching. 

In this paper we present an efficient method for line segment detection termed Enhanced Line SEgment Drawing (ELSED). In our experiments we compare the accuracy and efficiency of ELSED with that of the most relevant detectors in the literature. As shown in Fig.~\ref{fig:sample_of_line_segments}, ELSED is not only the most efficient (note the logarithmic scale in the speed dimension) but also the most accurate among the fastest in the literature.
It improves the efficiency of present methods in resource limited devices, opening the door to new CV applications running on any type of hardware. The main contributions of this paper are:
\begin{itemize}
 \item A new Edge Drawing (ED) algorithm that combines in one step the edge detection and line fitting processes.
 \item An approach to deal with the small discontinuities that usually appear in segments.
 \item A methodology to evaluate the accuracy and repeatability of segment detection algorithms.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{speed_vs_accuracy_i7.pdf}
    \caption{Average Precision (AP) vs execution time (ms) curve in the line segment detection problem. Local features based methods are displayed with circular markers and global ones with square markers.} 
    \label{fig:sample_of_line_segments}
\end{figure}


\section{Previous work}
\label{sec:state_of_art}

In this section we review the segment detection literature. To this end we organize it in three broad groups: full line detectors with global methods~\cite{matas2000robust, tal2012accurate,fernandes2008real, almazan2017mcmlsd}, those that use local properties to greedily detect line segments~\cite{grompone2010lsd, akinlar2011edlines, Ding2016OTLines, cho2017novel, zhang2021ag3line} and the deep line segment detectors~\cite{huang2018learning, xue2019learning, zhou2019end, newell2016stacked, xue2020hawp, xue2020hawp, dai2021fully, xu2021line, pautrat2021sold2}.

\subsection{Global information based approaches} \label{sec:full_line_soa}

Global methods are able to detect all full lines in the image with enough edge pixels support in spite of discontinuities. These methods start with an edge detector and then they apply a Hough Transform-like algorithm~\cite{ballard1987generalizing}. 

The Canny algorithm~\cite{canny1987computational} is one of the most used and simple approaches for edge detection. It consists of five basic steps: Gaussian filtering, gradient estimation, non-maximal suppression (NMS), double thresholding and edge tracking.

After edge detection, global approaches use a voting scheme, similar to the Hough transform, in which each edge pixel adds one vote to the bin representing each line across it in the image. Lines are detected as local maxima in this voting space.
There are well known issues with these methods: the omission of some weak edges, the generation of false positives in regions with high edge density (e.g. tree leaves) or the large amount of memory required to store the accumulators.

The Progressive Probabilistic Hough Transforms (PPHT)~\cite{KIRYATI1991303, matas2000robust} solve the efficiency problem changing the entire image voting scheme by a random sampling scheme. 
Some more recent works~\cite{tal2012accurate,fernandes2008real} address the quantization problem and also allows an efficient execution of the method. 
In \cite{XU20154012} the direction, length, and width of a line segment is extracted in a closed form that uses a fitted quadratic polynomial curve. To generate better segments, MCMLSD~\cite{almazan2017mcmlsd} 
uses the Elder \& Zucker edge detector~\cite{elder1998edge} 
that propagates edge uncertainty to the Hough histogram accumulator. 
In the final step they split the full lines detected in line segments using a Markov Chain Model and a standard dynamic programming algorithm. They also validate the resulting segments with a distribution learnt in the York Urban Database (YUD)~\cite{denis2008efficient}. 
The main drawback of this method is the efficiency. It requires 3.7 seconds on a desktop computer to process a 640480 image. 
An important merit of this work is being one of the first that proposed an quantitative evaluation of the performance w.r.t. manually annotated segments in a number of images. 
This opens the door to a new Line Segment Detector that using the information available in the modern data sets and a empirical evaluation process, reach both: a high accuracy and a low computational cost.

\subsection{Local methods} \label{sec:local_properties_soa}

Alternative methods have recently emerged to overcome some of the drawbacks of global approaches~\cite{grompone2010lsd,akinlar2011edlines,cho2017novel,zhang2021ag3line}. They all depart from strong gradient pixels and greedily add neighboring pixels using the gradient information. 

LSD~\cite{grompone2010lsd} detects segments in images by grouping and validating image regions with a significant gradient magnitude and a similar gradient orientation in , being  the number of image pixels. Unlike the NMS used in the Canny edge detector, LSD uses a region growing process to select interesting pixels. Then, they carry out a segment validation step based on the expected Number of False Alarms (NFA), computed with an A-Contrario statistical model~\cite{desolneux2000meaningful}. 
This validation method finds segments as outlier alignments of pixels to a background statistical model of gradient orientations built over a Gaussian white noise image. 
LSD is is more efficient than the full line approaches and it is able to deal with areas with high density of edge pixels (e.g. trees). However, as pointed out by~\cite{akinlar2011edlines}, the validation step mainly rejects short segments, since long segments are always outliers to the background statistical model. We reach the same conclusion in the experiments in Section~\ref{sec:experiment-validation}.

EDLines~\cite{akinlar2011edlines} is also an efficient  algorithm () based on local gradient orientations. It performs segment detection in two steps: 1) edge detection and 2) line segment detection using a local approach. The edge detection step is performed with the Edge Drawing (ED) algorithm~\cite{topal2012edge}. ED 
applies the first three Canny steps and tries to connect the anchor pixels (local maxima in gradient magnitude) with a greedy procedure that takes into account the thresholded gradient. ED is faster than Canny and returns a list of connected edge pixel chains instead of an edge map. In a second step EDLines performs line fitting on each of these chains of pixels. Finally, it applies the same validation as LSD, although the authors state that this last step is optional. 
OTLines~\cite{Ding2016OTLines} uses an orientation transformation to improve EDLines and avoid segment detections on circular structures but has a slower implementation. 
AG3line~\cite{zhang2021ag3line} instead of drawing over all pixels only finds aligned anchors. 
They also implement a continuous validation strategy to decide whether the segment has reached its endpoint and a jumping scheme to overcome gradient discontinuities. A key difference between AG3line and ELSED is that we use every pixel in the image as part of the drawing process, which generates a chain of continuous pixels, while AG3line only uses the detected anchors. This makes the method fast but unstable, thus it needs to validate each step, which at the end make it slower than ELSED. 

The approach of Cho \emph{et al.}~\cite{cho2017novel} is based on \emph{linelets} detection, i.e. chunks of horizontally or vertically connected pixels that result from line digitization. The linelet detection, with  time complexity, is conducted by first computing the gradient magnitude and orientation on every pixel, then performing NMS in the vertical and horizontal directions
and finally grouping the pixels in horizontal or vertical chains of connected pixels. Adjacent linelets are further grouped into line segments using a,  probabilistic model, where  is the number of detected linelets. Line segments are validated using another probabilistic model that consists of a mixture of experts model learnt from the gradient magnitudes, orientation and from the line length of the segments in a labeled data set. 
They also propose a quantitative methodology for comparing different segment detection algorithms. We improve their evaluation framework to compare the most significant line segment detectors in our experiments (see section~\ref{sec:experiments}).
Contrary to other local approaches, Linelet takes 16.7 seconds per image (see Table~\ref{table:segment_detection_times}) which is far away from real-time. 

\subsection{Deep line segment detectors}\label{sec:soa_endpoint_prediction}

A closely related problem to line segments detection is wireframe parsing. This problem consists of predicting the scene salient straight lines and their junctions. The ShanghaiTech Wireframe data set~\cite{huang2018learning} contains over 5,000 hand-labelled images that allowed different methods to train obtaining competitive results. 
AFM~\cite{xue2019learning} uses an attraction field map that is next squeezed to obtain line segments. 
L-CNN~\cite{zhou2019end} proposes an End-to-end model that uses a stacked hourglass~\cite{newell2016stacked} backbone to obtain a junction proposal heatmap that is extensively sampled to obtain the output segments. 
HAWP~\cite{xue2020hawp} improves the L-CNN sampling step by reparametrizing the line segments in a holistic 4-D attraction field map from which segments can be obtained faster. 
HT-HAWP~\cite{xue2020hawp} Adds some Deep Hough layers to the HAWP model improving its capabilities to capture lines and slightly improving the performance in some benchmarks.
F-Clip~\cite{dai2021fully} proposes a simple yet effective approach to cast line segment detection as an object detection problem that can be solved with a fully convolutional one-stage method. It also shows how modifying the backbone and exploiting multiple resolutions of the input through a parallel structure can vary the trade off between speed and accuracy. 
~\cite{pautrat2021sold2} proposes a self-supervised way towards line detection and description that optimizes the repeatability of the detected segments.

Despite the good results of these deep methods, their computational requirements are still far away from the classical methods based on gradient. This fact makes them non-viable for limited devices where there is no GPU or rather its battery consumption is prohibitive.
For this reason we introduce our drawing method that has been carefully designed to avoid floating point operation and minimize its complexity, being CPU friendly and achieving the fastest execution times on the state of the art.


\section{Enhanced Line SEgment Drawing (ELSED)}
\label{sec:smart_edlines_implementation}

In this section we introduce our line segment detection method 
and explain the different steps in our approach. 

\subsection{Enhanced Edge Drawing algorithm (EED)}
\label{sec:enhanded-edge-drawing-algorithm}

The EED entails the following high level steps:
\begin{enumerate}
 \item[1.] Gaussian smoothing to suppress noise.
 \item[2.] Gradient magnitude and orientation computation.
 \item[3.] Extraction of anchor pixels, local maxima in the gradient magnitude.
 \item[4.] Connect the anchors using the enhanced routing algorithm. 
\end{enumerate}

For the noise reduction step we use a convolution with a  Gaussian kernel with  filter. The rest of the steps are explained in the next subsections. 

\subsubsection{Gradient magnitude and orientation computation}
\label{sec:gradient}

We compute the horizontal, , and vertical, , gradients by applying the Sobel operator on the smoothed image. 
For a faster gradient magnitude computation we use the  norm, . 
Then, to make an even more efficient algorithm, we define a gradient threshold and set  for those pixels below it. 
We quantize the gradient orientation, , into two possible values: vertical edge, , or horizontal edge, . With only two possible orientation values so the routing process that connects anchor points can be performed efficiently. 

\subsubsection{Extraction of anchor pixels}
\label{sec:anchors}

The anchors are pixels where the drawing process begins. They may not be included in the set of final line segments if they belong to a very small. 
Like in ED, we scan image pixels with  and test if it is a local maxima in the gradient magnitude, , along the quantized direction of the gradient, .
If the pixel orientation  corresponds to a vertical edge, it is an anchor if  and . On the other hand, if the pixel orientation corresponds to an horizontal edge, it is an anchor if  and . To increase the processing speed, the number of anchors can be limited by increasing the value of . 
We also limit the number of anchors by scanning pixels every  columns and rows. 

\subsubsection{Connecting the anchors by an enhanced routing algorithm}
\label{sec:drawing-algorithm}

\begin{figure}
    \begin{subfigure}[t]{1.0\linewidth}
    \centering
    \includegraphics[width=0.7\textwidth]{drawing-algorithm_ED}
    \caption{ED algorithm next pixel selection. It only takes into account the current edge pixel.}
    \label{fig:edge-drawing}
    \end{subfigure}
    \begin{subfigure}[t]{1.0\linewidth}
\centering
    \includegraphics[width=0.76\columnwidth]{ED_problems_corners}
    \caption{Examples of corner shaped edges that can arise from ED. In this case the walk is coming from the blue pixel and finds an horizontal edge pixel (pink one). Thus, it will start a walk to the left and another to the right than could give edge chains with the displayed configurations (blue-pink-yellow pixel sequence).}
    \label{fig:edge-drawing-problems-corners}
    \end{subfigure}
    \caption{ED greedy segment growing}
\end{figure}

ED is faster than LSD's region growing because from an initial anchor, it only walks along a chain of edge pixels, evaluation  3 neighbours at each step (see Fig.~\ref{fig:edge-drawing}). These evaluations are critical for the speed of the algorithm since they are done for each reachable edge pixel walking from an anchor point. 

In our EED procedure, we perform the edge drawing and line fitting at the same time. This will enable us to save computations by reducing the number of checke pixels. In each step of the walk process we consider the previous and the current pixel.This allows us to avoid situations where the original ED algorithm would draw a corner breaking the line assumption (Fig.~\ref{fig:edge-drawing-problems-corners}). 
Thus, we explore the same pixels in the walking processes (see ``Go right'', ``Go left'', ``Go up'' and  ``Go down'' in Fig.~\ref{fig:enhanced-edge-drawing}) as long as the edge orientation does not change from the previous pixel to the current one. When the previous pixel is in a vertical edge, and the current pixel is in an horizontal one, ED has 6 candidate pixels to be added to the current line segment (see Fig.~\ref{fig:edge-drawing-problems-previous-pixel}). 
The same happens when previous pixel is in an horizontal edge and current is on a vertical one.
\begin{figure}
    \centering
    \begin{subfigure}{1.00\textwidth}
        \centering
        \includegraphics[width=0.8\columnwidth]{ED_problems_previous_pixel}
        \caption{Edge Drawing (ED).
        }
        \label{fig:edge-drawing-problems-previous-pixel}
    \end{subfigure}
    \begin{subfigure}{1.00\textwidth}
        \centering
        \includegraphics[width=0.8\columnwidth]{drawing-algorithm_EED}
        \caption{Enhanced Edge Drawing (EED). 
        }
        \label{fig:enhanced-edge-drawing}
    \end{subfigure}
    \caption{Original ED vs EED candidate pixels when the previous pixel was in a different orientation (vertical vs horizontal) than the current one. EED reduces the number of explored pixels from 6 to 3 or 2 by taking into account the line segment direction.
    }
    \end{figure}
However, if we now add the assumption that the edge chain should form a line, then the number of initial candidates changes from 6 to only 2 (see the four ``diagonal'' cases in Fig.~\ref{fig:enhanced-edge-drawing}). This is a feature of EED algorithm.
It has two advantages: 1) it is faster than the original ED routing algorithm as it explore less pixels and, 2) it avoids non meaningful cases for finding line segments. 

The second important change from ED is a also consequence of trying to find aligned edge pixels. Whereas ED changes the walking process direction when a change of edge orientation is detected (See Fig. \ref{fig:drawing-result-1}), EED tries to continue in the same direction following a line. However, any change of edge orientation is not forgotten and it is pushed into an stack for later processing (see Algorithm~ \ref{alg:drawing-stack-algorithm}).
EED tries to fit a line to the current chain of pixels, if more than  pixels have been chained and the squared error of alignment of the pixels is lower than . The last parameter for the segment search is the   that is the maximum distance in pixels from which we consider whether a pixel fits or not in the current segment. This is done internally in the function \emph{addpxToSegment} in line \ref{alg:addpxToSegment} of Algorithm~\ref{alg:drawing-stack-algorithm}).
The process stops when no more pixels can be chained (i.e. at the limits of the image, with only already visited pixels or weak edge pixels as candidates) or a line edge discontinuity is detected (e.g. the gap between two aligned windows, a tree branch occluding part of a building, etc.). In case a discontinuity was detected we execute, in order, the following actions:
\begin{enumerate}
    \item[1.] If we were walking along a line segment (i.e. we have fitted a line), try to extend it going on the line direction (the walking process stacked using \emph{canContinueForward} and \emph{forwardPxAndDir} functions in algorithm~\ref{alg:drawing-stack-algorithm}, lines \ref{alg:canContinueFordward} to \ref{alg:endCanContinueFordward}) 
    \item[2.] If were walking along a line segment and we cannot go continue forward, try to extend the line segment backwards (the walking process stacked using \emph{canContinueBackward} and \emph{backwardPxAndDir} functions in algorithm~\ref{alg:drawing-stack-algorithm}), lines \ref{alg:canContinueBackward} to \ref{alg:endCanContinueBackward})
    \item[3.] Continue in the gradient direction, that is changing in the discontinuity (the walking process stacked in Alg.~\ref{alg:drawing-stack-algorithm}, line~\ref{alg:drawOtherDirection}).
\end{enumerate}

This sequence of ordered actions guarantees that if a line segment is detected, all its pixels will be detected together. 

\begin{figure}
    \centering
    \begin{subfigure}{0.48\textwidth}
     \centering
     \includegraphics[width=\textwidth]{ed_edges}
     \caption{Edge Drawing~\cite{topal2012edge}}
     \label{fig:drawing-result-1}
    \end{subfigure}
    \begin{subfigure}{.48\textwidth}
     \includegraphics[width=\textwidth]{sd_edges}
     \caption{ELSED}
     \label{fig:drawing-result-2}
    \end{subfigure}
     \caption{Results of the  drawing process for ED and EED (purple pixels) from one single anchor (blue point). The green arrows and the numbers establish which segments are visited first. Since our method is aware that it is following a line it is able to continue drawing when the edge orientation changes. 
     }
     \label{fig:drawing-result}
\end{figure}

We show an illustrative example in Fig.~\ref{fig:drawing-result-2} where EED starts two walking processes, one up and another down. Differently to ED (see Fig.~\ref{fig:drawing-result-1}), EED detects the discontinuities in the edge orientation in the chessboard corners and continues walking in the current line direction. After pixels in segments (1) to (5) are chained together, the last edge orientation change detected is extracted from the stack and processed in the same way as the first long segment pixels (i.e. chaining the pixels of segments (6) to (9). The routing algorithm ends when there are no more edge orientation changes to process in the stack. Then, the next anchor point is processed by the routing algorithm.

\begin{algorithm}
	\caption{Enhanced Edge Drawing Algorithm}
    \label{alg:drawing-stack-algorithm}
\begin{algorithmic}[1]
\Procedure{EED}{}\\ 
	\textbf{Input:} Anchor pixel: . Anchor gradient direction: \\ 
	\textbf{Output:} S: List of Segments, P: List of edge pixels
	\State  ; 
	\State stack  ; stack.push([, ])
	\While{stack } \State segmentFound  \textbf{false}
		\State nOutliers  0
\State  stack.pop() \Comment Current pixel and direction
		\State  previousPixel(, ) \Comment Find previous pixel
		\While{  nOutliers  } \State   drawNextPx(, )
			\State 
			\If{segmentFound} 
				\State{ s, nOuliers  addPxToSegment(s, , nOuliers)} \label{alg:addpxToSegment}
			\Else 
				\State s, segmentFound  fitNewSegment() \Comment Can return 
				\State S  S  \{s\}
			\EndIf
		\EndWhile
		\If{} 
			\State stack.push([, lastDir]) \label{alg:drawOtherDirection} \Comment Edge orientation change
			\If{canContinueForward(s)} \label{alg:canContinueFordward}
                \State  s.forwardPxAndDir(, )
				\State stack.push([, ]) 
			\EndIf  \label{alg:endCanContinueFordward}
			\If{canContinueBackward(s)} \label{alg:canContinueBackward}
				\State  s.backwardPxAndDir(, )
				\State stack.push([, ]) 
			\EndIf \label{alg:endCanContinueBackward}
		\EndIf
	\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{The line segment discontinuities}
\label{sec:discontinuity_management}

A line segment can be interrupted by several edge discontinuities. These are regions of the image where the gradient orientation changes or the gradient magnitude goes to zero. Whether we aim to detect full lines or just line segments, the discontinuities of different lengths (i.e. the number of pixels where there is no edge or the edge direction is not aligned with the line segment) should be correctly skipped in a drawing process in order to detect the line segments correctly. 

Algorithm \ref{alg:drawing-stack-algorithm} naturally deals with this phenomena. 
Once a discontinuity is detected, our aim is to jump over it and continue drawing in the line segment direction if possible. \emph{A priori}, the discontinuity length  is unknown and our algorithm test different length candidates. As we are using a  Gaussian smoothing kernel, any  discontinuity will have effect in at least a neighbourhood of size 5 pixels, therefore we set the minimum discontinuity length to 5 pixels. In the functions \textit{canContinueFordward} (Alg. \ref{alg:drawing-stack-algorithm}, line~\ref{alg:canContinueFordward}) and \textit{canContinueBackward} (Alg. \ref{alg:drawing-stack-algorithm}, line~\ref{alg:canContinueBackward}), different jump lengths  are checked (in the default parameters of the algorithm we use ). The drawing process will continue after the discontinuity if the ordered conjunction of the following conditions is true:
\begin{enumerate}
    \item The segment is longer than the number of pixels, , we want to jump.
    \item The pixel, , that is aligned with the segment and  pixels away from current pixel, , is inside the image and has  (i.e. is not a weak edge pixel).
    \item Starting from , EED is able to draw at least  pixels following the edge direction. We call this set of  pixels the extension pixels.
    \item The extension pixels are well aligned with the line segment. To check this we calculate the auto-correlation matrix of the image gradients, , in a small neighbourhood (we take one pixel on each side of the extension pixels) and then, we assert that:
    
    where  and , , are the eigenvalues of the , and 
    
     is the angular distance,  is the first eigenvector of  and  is a vector normal to the segment.
\end{enumerate}

\begin{figure*}
    \centering
    \footnotesize
    \begin{subfigure}{.33\textwidth}
     \centering
     \includegraphics[height=\columnwidth]{original_crop}
     \caption{Original Image}
     \label{fig:discontinuity-management-original}
    \end{subfigure}\vspace{0.5ex}\begin{subfigure}{.33\textwidth}
     \centering
     \includegraphics[height=\columnwidth]{gradient_crop}
     \caption{Blurred Image gradient}
     \label{fig:discontinuity-management-gradient}
    \end{subfigure}\begin{subfigure}{.33\textwidth}
     \centering
     \includegraphics[height=\columnwidth]{grad_orientation_crop}
     \caption{Gradient orientation}     
     \label{fig:discontinuity-management-gradient-orientation}
    \end{subfigure}
\begin{subfigure}{.33\textwidth}
     \centering
     \includegraphics[height=\columnwidth]{drawing_process_outliers_crop}
     \caption{Discontinuity detection}
     \label{fig:discontinuity-management-outliers}
    \end{subfigure}\begin{subfigure}{.33\textwidth}
     \centering
     \includegraphics[height=\columnwidth]{extension_drawing_process_1_crop}
     \caption{1st discontinuity check}
     \label{fig:discontinuity-management-extension1}
    \end{subfigure}\begin{subfigure}{.33\textwidth}
     \centering
     \includegraphics[height=\columnwidth]{drawing_process_up_crop}
     \caption{Drawing 2nd segment}
     \label{fig:discontinuity-management-going-up}
    \end{subfigure}
\begin{subfigure}{.33\textwidth}
     \centering
     \includegraphics[height=\columnwidth]{extension_drawing_process_2_crop}
     \caption{2nd discontinuity check}
     \label{fig:discontinuity-management-extension2}
    \end{subfigure}\begin{subfigure}{.33\textwidth}
     \centering
     \includegraphics[height=\columnwidth]{detected_edges_crop}
     \caption{Detected edges}
     \label{fig:discontinuity-management-edges}
    \end{subfigure}\begin{subfigure}{.33\textwidth}
     \centering
     \includegraphics[height=\columnwidth]{detected_segments_crop.png}
     \caption{Detected Segments}
     \label{fig:discontinuity-management-segments}
    \end{subfigure}
\caption{Example of ELSED discontinuity management algorithm. Gradient orientations in (c) coded with red (right direction), purple (up), cyan (left) and light green (bottom)}
    \label{fig:discontinuity-management}
\end{figure*}

In Fig.\ref{fig:discontinuity-management} we show different steps of the discontinuity management algorithm in a synthetic image. 
In Fig.~\ref{fig:discontinuity-management-outliers} we show the detection process starting from the left side of the image, we fit the edge pixels (blue) to a horizontal line (green). When the edge orientation changes from horizontal to vertical, we detect the last edge pixels as outliers (orange) and thus our method detects that we are in a discontinuity. In this case, the red pixel is the last one detected when .

Next sub-figure \ref{fig:discontinuity-management-extension1} shows the check done to decide whether we should continue drawing straight or the segment has finished. Purple pixels are the pixels in the discontinuity and are skipped using the Bresenham's algorithm drawing in the current line segment direction. Red pixels are drawn following the edge direction (in green) starting from the first pixel after the discontinuity, . We also mark in red the neighbor pixels used to validate the region using the eigenvalues of  if the extension pixels in function \texttt{canContinueForward}. Pixels in this synthetic example do not have a uniform gradient direction, thus, the process discard all extensions tested with lengths . In the figure we show the last one, . Consequently, the algorithm closes the first segment and continues drawing in the dominant gradient direction upwards. 
When enough pixels are gathered, we fit another segment (Fig.~\ref{fig:discontinuity-management-going-up}). When we reach the top of the image, the segment is extended in the backward direction downwards. When this happens, the mechanism to manage discontinuities is activated again as Fig.~\ref{fig:discontinuity-management-extension2} shows, but this time the region meets all the criteria defined and thus the jump is executed. Fig.~\ref{fig:discontinuity-management-edges} and \ref{fig:discontinuity-management-segments} show respectively the fitted edges and the segments.

One of the limitations of local segment detection approaches is the generation of many small segments produced by gradient discontinuities. The process described above help us alleviate this.

\subsection{Validation of the generated segment candidates}
\label{sec:method-validation}

After EED, we have several line segments detections, many of them potentially wrong (see red segments in Fig.~\ref{fig:positive-and-negative-labels-for-validation}). They occur mainly in regions with high density of edge pixels.
To validate a line 
we use the segment pixels' gradient orientation, comparing its angular error with the direction normal to the segment. This validation can be performed efficiently, without damaging the overall performance.

\begin{figure}
  \centering
  \includegraphics[width=0.48\columnwidth]{validationDataset-GTvsDetected.jpg}
  \includegraphics[width=0.48\columnwidth]{validationDataset-TPvsFP.jpg}
  \caption{Positive and negative segments used to train the validator. The data set is composed of positive (green) and negative (red) segments. We obtain these label comparing the ground truth segments of the YorkUrban-LineSegment data set (blue) with unvalidated ELSED detections (magenta).}
  \label{fig:positive-and-negative-labels-for-validation}
\end{figure}

For a good validation it is fundamental to discard the pixels lying in a discontinuity and those near the endpoints. They usually have a different gradient orientation even in correct detections. Fig.~\ref{fig:errors-in-gradient-orientation} shows the angular difference in radians (vertical axis) of every pixel gradient to the normal of the segment shown on the right. In this case the pixels on the central discontinuity and near to the endpoints have gradient orientations different from the inlier segment pixels. Consequently, we do not use these pixels to validate our segments.

\begin{figure}
    \centering
    \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\columnwidth]{SegmentGradientErrorExample.png}
    \caption{Segment (green)}
    \label{fig:errors-in-gradient-orientation:example}
    \end{subfigure}
    \begin{subfigure}{0.40\textwidth}
    \centering
    \includegraphics[width=\columnwidth]{SegmentGradientErrorPlot.pdf}
    \caption{Angular difference}\label{fig:errors-in-gradient-orientation:plot}
    \end{subfigure}
    \caption{Angular differences between the gradient and the direction perpendicular to a segment.  
    }
    \label{fig:errors-in-gradient-orientation}
\end{figure}

Even discarding pixels on discontinuities, the gradient orientation error in a true segment is a noisy signal. 
In Fig.~\ref{fig:gradient-orientation-validation} we can see the distribution of angular errors for pixels not at discontinuities on true positive segment detections (TP) (purple), false positive segments (FP) (green) and false positives segments detected in a random noise intensity image (blue). It is clear that the pixels on true positive segments have less angular error than those  on false positive ones. However, there is a significant overlap between FP and TP distributions. 

\begin{figure}
    \centering
    \includegraphics[width=0.75\columnwidth]{gradient_orientation_error.pdf}
    \caption{Probability density functions of the gradient angular error in correct segments (blue), false positive segments (orange) and false positive segments detected in a random intensity image (green). }
    \label{fig:gradient-orientation-validation}
\end{figure}

We use a validation criteria robust to noise. We validate a segment if at least 50\% of its pixels have an angular error lower than a threshold, . To separate positive detections (i.e. valid ones) from negative ones, we learn a threshold  radians which keeps a high recall discarding few true detections.
This is similar to the NFA used in LSD~\cite{grompone2010lsd} and EDLines~\cite{akinlar2011edlines} in the sense that we set a single threshold that controls the trade-off between FP and FN detections. The main difference is that our method judges in the same way both, short and large segments while the NFA has proven to discard mainly short segments~\cite{akinlar2011edlines}. Furthermore our metric allows to learn the threshold from data which is a better approach (as shown in the experiments of section~\ref{sec:experiment-validation}).


\subsection{Parameter selection}

Most ELSED have been set empirically and do not need to be changed by the user:
\begin{itemize}
    \item \textbf{Gaussian smoothing filter} ( = 1, kernel size = 55).
    \item \textbf{Gradient Threshold} ().
    \item \textbf{Anchor Threshold} (). If the pixel's gradient value is bigger than its neighbors by a the Anchor Threshold, the pixel is marked to be an anchor.
    \item \textbf{Scan Interval} ( = 2). Anchor testing can be performed at different scan intervals, i.e., every row/column.
    \item . This is the maximum number of outlier pixels to the segment allowed before stop drawing.
    \item . The minimum length of a segment.
    \item . The maximum squared error in the line fitting allowed to consider that a certain set of pixels forms a line segment.
    \item . The maximum distance in pixels from which we consider whether a pixel is part of a line or not.
    \item : This threshold is used to validate the extensions pixels after a discontinuity.
    \item  degrees. This threshold is used to validate the extensions pixels after a discontinuity.
    \item  radians. Maximum gradient orientation difference to the line segment normal to consider a pixel as valid.
\end{itemize}

However, other parameters that may be tuned by the user to define the type of segments to be detected.

The algorithm should be provided with a list of jump lengths that will be tested in the discontinuity management. Since this is directly related with the size of the Gaussian smoothing kernel in the first step ( in our implementation) and the size of the gradient convolution kernel ( in our case), we define a set of default values, , that in the experiments of section~\ref{sec:ablation-study} provide good results. 

\section{Experiments} \label{sec:experiments}

In this section we introduce a methodology to evaluate the accuracy and repeatability of segment detectors and  follow it to compare our detector with the best in the literature. We also 
present an ablation study to analyze how each component of our algorithm contributes to final result.

We perform our evaluation in two dimensions, accuracy and efficiency. To this end we have grouped the algorithms in two sets, following the two clusters in Fig.~\ref{fig:sample_of_line_segments}. In the first set we find algorithms that run efficiently in CPU (LSD, EDLine, AG3line and ELSED). In the second set those that require more than one second to process an image (MCMLSD, Linelet, HAWP, SOLD, F-Clip). 
Although HAWP, SOLD and F-Clip are DL methods and should be run in GPU, we run them also on CPU to show the different computational requirements of each approach. It is also important to note, that most low-power devices like smartphones, drones or IoT devices are usually not prepared to run the GPU for long periods of time.
In our experiments we compare the accuracy and efficiency of ELSED with the approaches in each group.





\subsection{Segment detection evaluation}
\label{sec:exp-seg-detection}


\begin{figure}
  \centering \medskip
  \begin{subfigure}[t]{1.0\linewidth}
    \centering\includegraphics[width=0.4\linewidth]{endpoints_distance.pdf}
  \caption{Structural distance used by ELSED to fill the affinity matrix , where each element .}
    \label{fig:Structural-distance}    
  \end{subfigure}
\begin{subfigure}[t]{1.0\linewidth}
    \centering\includegraphics[width=0.49\linewidth]{MCMLSD_countersample.pdf}
  \caption{Example where MCMLSD~\cite{almazan2017mcmlsd} asignation fails. The detected segment  is matched to the GT segment  while the best possible match is . This is because  whereas .}
  \label{fig:MCMLSD-countersampple}
  \end{subfigure}
  \label{fig:segments-distance-comparison}
  \caption{Comparison of segment distance metrics for evaluation.}
\end{figure}

We evaluate the segment detection in the YUD data set~\cite{denis2008efficient} that contains indoor and outdoor man-made scenes where some salient segments have been manually labeled. The data set was extensively re-labeled later in Linelet~\cite{cho2017novel}, which contains all segments in the scene, but also some inconsistencies such as the labels in tiles and cables, segments smaller than 2 pixels or segments on curves and objects of doubtful value. 

Linelet~\cite{cho2017novel} and MCMLSD~\cite{almazan2017mcmlsd} were the first two methods that proposed an empirical evaluation. Whereas Linelet evaluation allows 1-to-N detections between the ground truth labels and the detected ones, MCMLSD methodology computes the 1-to-1 assignation based on a cost matrix  that is computed with points sampled along each segment, which is slow to compute and can lead to erroneous matches in segments really close (see Figure \ref{fig:MCMLSD-countersampple}). MCMLSD also evaluates the entire precision-recall curves obtained from the assignation, while Linelet uses only one operation point on it.
On the other hand, the wireframe line segment detection metric~\cite{huang2018wireframe}, proposes the structural Average Precision (sAP)~\cite{zhou2019end}. This metric can generate non meaningful matches because there is no check in the overlap, angle nor perpendicular distance, necessary to match small segments correctly. Furthermore, the 1-to-1 assignation is solved non-optimally because nearest neighbor assignation is used instead of Hungarian algorithm. Good matches can be also omitted if they have a large structural distance, which happens in case of fragmentation.

We propose a new evaluation framework that joins the benefits of previous evaluation protocols, namely, it is fast to compute and is a fair and stable metric for line segment detection.
We ensure a good 1-to-1 match by using the Hungarian algorithm to find the optimal bipartite match, the assignation problem is defined with a cost matrix , that is filled using the structural distance (see Fig.~\ref{fig:Structural-distance}). This metric is a good trade off between perpendicular distance, misalignment and overlap. It is also faster to compute than the matched number of sampled points.
To speed up the Hungarian algorithm and ensure a meaningful matching, we require matched segments to 
have an intersection over union bigger than , to have an angular distance smaller than  and a perpendicular distance smaller than . The pairs of segments that don't meet this criteria, have infinite cost in the corresponding entry of  avoiding their matching. Let  be the set of detected segments and  the set of ground truth segments. With the 1-to-1 assignations  between them we define:
\begin{itemize}
    \item \underline{Precision}: Length of the matched intersection measured over the detected segment , divided by the length of the detected segments, 
    \item \underline{Recall}: Length of the matched intersection measured over the ground truth segment , divided by the length of the ground truth segments,
    
    \item \underline{Intersection over Union}: Length of the matched intersection measured over the ground truth segments, divided by Length of the matched union measured over the ground truth segments.
    
\end{itemize}

In Fig.~\ref{fig:ours-eval-precision-recall} we show the Precision-Recall of our algorithm and the state-of-the-art detectors. This curve is computed w.r.t. the original YUD data set annotations and we sort the segments produced by each method using the score provided by their original code. For ELSED, the score is the percentage of pixels that have an angular error lower than .
The curve of ELSED is better than those of LSD, EDLines and AG3line, the methods able to run on CPU efficiently (see Table~\ref{table:segment_detection_times}). We obtain a better precision and reaching similar or higher recall. MCMLSD gets better recall but at the cost of very bad precision. This was expected since it is the only method based on the HT and detects all full lines with enough support, even the hallucinated ones over highly textured regions. 
ELSED is able to improve AG3line because it has a better drawing scheme tailored to the line segment detection problem.
ELSED is on par with the most efficient DL approaches, F-Clip (HG1), in the range of recall where ELSED works. Deeper networks such as HAWP that needs a GPU and are far away from real time in CPU obtain, as expected, results with better recall and precision. \begin{figure}
    \centering
        \centering
        \includegraphics[width=0.492\textwidth]{precision-recall-efficient.pdf}
        \includegraphics[width=0.47\textwidth]{precision-recall-deep2.pdf}
        \caption{Precision-Recall of the detected segments evaluated using the proposed evaluation framework. Evaluation in YUD with original labels.}
        \label{fig:ours-eval-precision-recall}
\end{figure}

Interestingly, in Fig.~\ref{fig:ours-eval-precision-recall} ELSED has a limited range of recall compared with other methods. Mostly because of two decisions taken to speed up the algorithm: 1) we have a limited number of anchor points; 2) we have limited the maximum jump across discontinuities. A longer jump length will allow to detect more segments bearing bigger discontinuities (usually longer segments). 

In Table~\ref{table:ours_segment_detection_yud} columns 2 to 5 show the highest recall values in the P-R curve for each method.
They corresponds to the case where we require the detector to detect as many segments as possible. The last two columns show metrics over all the P-R points of the curve. 
If we look at the results with the original YUD labels, ELSED has the best precision (0.3198), F-score (0.4148) and IoU (0.7111) among the efficient detectors on CPU. It also obtains the best results for the overall metrics along all the Precision-Recall points (Average Precision, AP). AP is the classic classification metric that is biased towards the detectors with a wider recall range. Thus, we also show the AP bounded to the recall interval where the curve of each method is defined (bAP). ELSED has a bAP comparable with F-Clip (HR) and better than F-Clip (HG1). This means that, although ELSED detects less segments (lower recall range than F-Clip) it has a precision similar to the top DL-based methods in the detected segments.

We also present the results for the ''YorkUrban-LineSegment annotation''~\cite{cho2017novel}. 
In the new annotation the definition of segments changes to short and broken lines from the global lines in the original YUD labels. In this case, we also show the results of ELSED without jumps (\emph{ELSED No-Jumps}), to avoid ELSED to merge segments labeled as separate ones in the data set. ELSED No-Jumps obtains competitive results, being the best in terms of precision and IoU in this data set. This shows the nice to have property of ELSED, allowing to adapt the definition of segment to the application by changing that jump length over discontinuities.

\begin{table}
\centering


\begin{tabular}{|l||l|l|l|l||l|l|} \hline
Method & P & R & IoU & F\_sc & AP & bAP \\ \hline
\multicolumn{7}{|c|}{Original YUD annotations} \\ \hline
LSD & 0.2627 & 0.5347 & 0.5891 & 0.3370     & 17.33 & 34.78\\
EDLines & 0.2682 & 0.6017 & 0.6394 & 0.3559 & 18.48 & 43.42\\
AG3line & 0.2679 & \textbf{0.6707} & 0.6855 & 0.3679 & 21.02 & 42.32\\
ELSED & \textbf{0.3198} & 0.6639 & \textbf{0.7111} & \textbf{0.4148} & \textbf{21.17} & \textbf{44.94} \\ \hline
MCMLSD & 0.2580 & 0.7750 & 0.7368 & 0.3730 & 21.98 & 37.50\\
Linelet & 0.2424 & 0.5966 & 0.6338 & 0.3309 & 21.37 & 43.62\\
HAWP & \textbf{0.4853} & \textbf{0.6032} & \textbf{0.8347} & \textbf{0.5120} & \textbf{38.53} & \textbf{51.41}\\
F-Clip (HG1) & 0.4666 & 0.4205 & 0.7980 & 0.4196 & 31.40 & 40.35 \\
F-Clip (HR) & 0.5279 & 0.4659 & 0.8159 & 0.4662 & 34.74 & 44.77 \\
\hline \hline 
\multicolumn{7}{|c|}{YorkUrban-LineSegment annotations} \\ \hline
LSD     & 0.6798 & 0.5223 & 0.6705 & 0.5823 & \textbf{38.23} & 76.35 \\
EDLines & 0.6681 & 0.5647 & 0.6770 & \textbf{0.6049} & 36.61 & 76.88 \\
AG3line & 0.5963 & \textbf{0.6243} & 0.6564 & 0.6045 & 34.69 & 69.06 \\
ELSED   & 0.6768 & 0.5258 & 0.6829 & 0.5846 & 31.01 & 71.69 \\
ELSED No-Jumps & \textbf{0.7085} & 0.5115 & \textbf{0.6924} & 0.5863 & 32.43 & \textbf{76.93} \\ \hline
MCMLSD  & 0.5456 & \textbf{0.6183} & 0.6214 & 0.5748 & 30.24 & 57.71 \\
Linelet & 0.6242 & 0.5793 & 0.6593 & \textbf{0.5950} & \textbf{39.35} & \textbf{75.52} \\
HAWP & 0.6476 & 0.3023 & 0.7023 & 0.4008 & 33.48 & 56.84 \\ 
F-Clip (HG1) & 0.6663 & 0.2189 & \textbf{0.7222} & 0.3208 & 32.13 & 50.06\\ 
F-Clip (HR) & \textbf{0.6852} & 0.2234 & 0.7220 & 0.3258 & 32.78 & 51.15 \\ \hline
\end{tabular}


\caption{Results with our evaluation protocol using the original York Urban Data set annotations and YourkUrban-LineSegment ones (top half: efficient methods, bottom half: slow medhods). We use 1-to-1 matching and . We use \textbf{bold} for best results in each experiment. The columns show for each method the Precision (P), Recall (R), Intersection over Union (IoU), F-Score (F\_sc), Average Precision (AP) and bounded Average Precision (bAP).}
\label{table:ours_segment_detection_yud}
\end{table}

With these experiments we can conclude that, despite ELSED has being designed with the aim of reducing the execution time, it is also a competitive algorithm in terms of segment detection accuracy. The reason is that the EED process suits the segment detection problem and the jumps over discontinuities produces outputs that agree with the definition of the segments (i.e. long segments) in the annotations.



\subsection{Repeatability}
\label{sec:repeatability}

Regardless the type of segments aimed for the detection, a desirable property is the robustness against changes in viewpoint, scale, rotation or lighting. In this subsection we evaluate the detectors repeatability. Given two images of the same scene under different conditions, the capacity to detect the same segments in both situations. Specifically, given two images, we define line segments repeatability as the ratio between length of 1-to-1 segment matches and the total length of segments detected in both images. We take into account only the segments located in the part of the scene present in both images, adjusting their endpoints accordingly. 

We use the HPatches\cite{hpatches_2017_cvpr} data set that mainly contains human-made environments with viewpoint and illumination changes from 116 sequences, 6 images per sequence. We match the first image of each sequence with the other 5 obtaining a repeatability value for each. The repeatability of segment detections in images  and  is defined as:



Where  are the segments detected in image  and  the segments detected in image , projected to  using the homography between them. 
The matching matrix  contains the 1-to-1 segment assignations between segments    and  obtained with the matching process described in subsection \ref{sec:exp-seg-detection} and  the one between  and .


\begin{table}
    \centering
    \begin{tabular}{|c|c|c|} \hline
    Method  & Length Repeatability & Num Segs. Repeatability \\ \hline
    LSD             & 0.53934 & 0.48707 \\ 
    EDLines         & 0.54352 & 0.47242 \\
    AG3line         & 0.43582 & 0.40002 \\
    ELSED           & \textbf{0.55928} & \textbf{0.50285} \\ \hline
MCMLSD          & 0.50322 &  0.39440 \\
    Linelet         & 0.52102 &  0.43427 \\
    HAWP            & 0.40056 & 0.41248 \\ 
     & 0.46161 & 0.47957 \\ 
    F-Clip (HG1)    & 0.39784 & 0.43560 \\ 
    F-Clip (HR)     & 0.40969 & 0.43413 \\ \hline
    \end{tabular}
    \caption{Mean repeatability of each segment detector in the HPatches data set. The higher the repeatability, the more robust the detector.}
    \label{tab:repeatability}
\end{table}
 
In Table~\ref{tab:repeatability} the second column also shows the repeatability in terms of the number of matched segments. We employ here  and  according to \cite{pautrat2021sold2}. It both cases ELSED obtains the most repeatable results. This is because EED provides stability to the edge detection and also because the jump strategy is able to overcome small discontinuities that cause other local methods to fail.
We have also observed that deep models like  or HAWP get highly repeatable results in some scenes and very bad results in others, this is possibly because they have been trained in a quite specific problem (the wireframe parsing for indoor scenes) and not for the diverse images that can be found in Hpatches. 




\subsection{Comparison of different segment validation criteria}
\label{sec:experiment-validation}

In this section we compare different validation criteria to filter the false positive segments detected by EED. The goal is to remove segments coming from high gradient density regions like trees, shrubbery or shapes that resemble a line.

The NFA~\cite{grompone2010lsd} is a widely used measure based on the \textit{a contrario} probabilistic model~\cite{desolneux2000meaningful}. 
This validator is denoted \emph{ELSED-NFA} in Fig.~\ref{fig:validation-criteria-precision-recall}. One important note about the NFA validator is that it tends to delete only short segments~\cite{akinlar2011edlines}.

Another validation approach that we have tested is based on the auto-correlation matrix of the image gradients . In a region with a dominant gradient direction (i.e. a line segment), one eigenvalue () of  must be far bigger than the other (). 
We use here the ratio of biggest to smallest eigenvalues of , , as salience measure. This is why we term this method \emph{ELSED-IHarris} (Inverse Harris) in Fig.~\ref{fig:validation-criteria-precision-recall}.

The two described methods do not use any training labeled data. 
The first step to learn from data, is having a data set with annotations of segments as ground truth. To take into account the detector bias we obtain the valid vs invalid annotations from the segments detected by the method we want to validate (ELSED in this case). We create the positive labels (valid segments) as detected segments that match with ground truth ones and the negative labels as segments that do not match. Segments were detected with ELSED without validation. 
In Fig.~\ref{fig:positive-and-negative-labels-for-validation} we show an example of training image with TP and TN used for training different validators. 
From all the training images in YUD, 9472 positive and 7597 negative segments were generated. The positive labels were detected by applying the angular, distance and area thresholds described in subsection~\ref{sec:exp-seg-detection} from the YUD-Linelet annotations~\cite{cho2017novel}. 
We test a simple Bayes classifier as validator in addition to the first two:

Where  is the gradient orientation error for the segment that we want to validate. Instead of taking their mean value along all the pixels in the segment, we sort the values and take the one that left behind the 60\% of the samples as robust estimator (60th percentile).
In this way, we can deal with the noisy gradient orientation measurements. We consider classes priors equal: .
The probabilities  and  are obtained from histograms built from the training images with the gradient orientation error distributions for TP segments and FP ones. We denote this validator as ELSED-Bayes in Fig.~\ref{fig:validation-criteria-precision-recall}. This approach is similar to the one proposed in \cite{almazan2017mcmlsd}.

The last validator we test is the one introduced in Section~\ref{sec:method-validation}.
We declare a candidate segment as a true one if at least 50\% of its pixels have an angular error lower than a threshold, . In our final implementation, we use  radians based on Fig. \ref{fig:gradient-orientation-validation} which keeps a high recall discarding few true detections. 
We term this criteria as \emph{ELSED-InRatio}.
\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{yud-gt-validators-pr.pdf}
        \label{fig:label:validation-criteria-original-yud}
        \caption{Original YUD labels}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{linelet-gt-validators-pr.pdf}
        \label{fig:validation-criteria-original-linelet}
        \caption{YorkUrban-LineSegment labels}
    \end{subfigure}
    \caption{Validators comparison precision-recall curves.}
    \label{fig:validation-criteria-precision-recall}
\end{figure}

Fig.~\ref{fig:validation-criteria-precision-recall} shows the comparison between the different segment validators. We detect all the segments in the test set of the YUD images with a low gradient magnitude threshold () to generate enough FP segments. Next, for each image, we rank the segments according the validator score. In the case of \emph{ELSED-NoValidation} we use the length in pixels of the segment as score. Finally, we plot the Precision-Recall curve varying the threshold of each validator. 
All validators are given the same noisy set of segments. These detections share a common point (bottom, right) where all the segments are accepted. As the threshold becomes more demanding, the experiment shows the ability of the validator to discard bad segments keeping the good ones (i.e. increasing the precision without damaging the recall).
Interestingly, the \emph{ELSED-NFA} criteria obtains a curve near to the \emph{ELSED-NoValidation} using detected segment length as score. This seems to confirm the findings of other authors about the NFA validator greatly preferring long segments over short ones.
The \emph{ELSED-IHarris} score is always below the trained validators and thus, we do not use it with ELSED. 
The validators that uses information from training data, \emph{ELSED-Bayes} and \emph{ELSED-InRatio}, obtain the best results. We have chosen the \emph{ELSED-InRatio} for its simplicity and its good results.





\subsection{Ablation study}
\label{sec:ablation-study}

In this section we present an ablation analysis of the components involved in our algorithm (see Table~\ref{table:ablation_study}). 
\begin{table*}
\centering
\footnotesize
\resizebox{\linewidth}{!}{\begin{tabular}{|c|c|c||c|c|c|c||c|c|} \hline
\multicolumn{3}{|c|}{\textbf{Configuration}} & \multicolumn{4}{c|}{\textbf{Last P-R point metrics}} & \multicolumn{2}{c|}{\textbf{Global metrics}} \\ \hline
Jumps & Jump Val. & Seg. Val. & P & R & IoU & F\_sc & AP & Time (ms)\\ \hline \hline
\multicolumn{9}{|c|}{Original YUD Labels} \\ \hline
 None     &            &            & 0.271 & 0.609 & 0.646 & 0.360 & 43.06 & \textbf{2.95} \\ \hline
 None     &            & \checkmark & 0.307 & 0.590 & 0.651 & 0.388 & 44.27 & 4.16 \\ \hline
 Fixed (5 px)    &            &            & 0.278 & \textbf{0.702} & \textbf{0.712} & 0.383 & 42.46 & 3.25 \\ \hline
 Fixed (5 px)   & \checkmark &            & 0.282 & 0.667 & 0.694 & 0.381 & 43.62 & 3.48 \\ \hline
 Multiple (5,7,9 px)& \checkmark &            & 0.285 & 0.686 & 0.706 & 0.387 & 43.28 & 4.16 \\ \hline
 Multiple (5,7,9 px) & \checkmark & \checkmark & \textbf{0.320} & 0.664 & 0.711 & \textbf{0.415} & \textbf{44.97} & 5.38 \\ \hline
\end{tabular}
}
\caption{Results of the ablation study using our 1-to-1 evaluation protocol 
over the original Yourk Urban Data set 
annotations. Best results are marked in 
\textbf{bold}. Execution times are measured in (Intel Core i7). 
}
\label{table:ablation_study}
\end{table*}
We start with the simplest version of ELSED: no discontinuity jumps and no validation step. This version of the algorithm obtains the worst results (IoU and F\_sc in Table~\ref{table:ablation_study}) for long segment detections in the original YUD annotations. 
When the validation is activated, we can see how the precision increases from 0.271 to 0.307 whereas the recall remains high (0.59). 

If we now add the discontinuity jump component with a fixed length of 5 pixels it can remove some small detection errors. For example, in the second row and column of Fig.~\ref{fig:images_ablation_study} the broken segments of the wall in the left, with the added fixed length jump capability are detected as a unique segment. On the other hand, now the algorithm performs some incorrect jumps going beyond the endpoint of the segment. This effect can be observed in the results of Table~\ref{table:ablation_study} where the Recall take a big leap (from 0.609 to 0.702) and the Precision also increases moderately (from 0.271 to 0.278). To fix the problem with incorrect jumps, we add the validation of the jump destination region of section~\ref{sec:discontinuity_management} (see the well-fitted endpoints of the third column in Fig.~\ref{fig:images_ablation_study}). 

\begin{figure*}
    \centering
    \includegraphics[width=0.19\textwidth]{P1020829_NV_NJ.jpg}
    \includegraphics[width=0.19\textwidth]{P1020829_NV_FJ_NJV.jpg}
    \includegraphics[width=0.19\textwidth]{P1020829_NV_FJ_JV.jpg}
    \includegraphics[width=0.19\textwidth]{P1020829_NV_MJS_JV.jpg}
    \includegraphics[width=0.19\textwidth]{P1020829_V_MJS_JV.jpg}
\includegraphics[width=0.19\textwidth]{P1040798_NV_NJ.jpg}
    \includegraphics[width=0.19\textwidth]{P1040798_NV_FJ_NJV.jpg}
    \includegraphics[width=0.19\textwidth]{P1040798_NV_FJ_WJV.jpg}
    \includegraphics[width=0.19\textwidth]{P1040798_NV_MJS_WJV.jpg}
    \includegraphics[width=0.19\textwidth]{P1040798_V_MJS_WJV.jpg}
    \caption{Form left to right columns: No Validation no jumps, No validation with fixed size jumps (5 px) and no jump validation, No validation with fixed size jumps (5 px) and jump validation, No validation with multi-size jumps (5, 7, 9 px) and jump validation, Validation with multi-size jumps (5, 7, 9 px) and jump validation}
    \label{fig:images_ablation_study}
\end{figure*}

On the other hand, a jump of 5 pixels is not big enough if the discontinuity is large. In this case, the jump validation is performed over pixels on the discontinuity. Thus, since discontinuities contains gradients in different directions to the normal of the segment, the jump validation fails. This is the reason why we add the multi-length jumps in the fourth column in Fig.~\ref{fig:images_ablation_study}. With it, we can deal with longer segment discontinuities. The last step is the validation of the whole detected segment (see section~\ref{sec:method-validation}) shown in the last column of Fig.~\ref{fig:images_ablation_study}. 
Segment validation increases the precision (from 0.285 to 0.320) with a small penalty in the recall (from 0.686 to 0.664).





\subsection{Efficiency evaluation}
\label{sec:experiment-times}

Nowadays computer vision applications not only require good accuracy, but also fast execution times and low energy consumption. This is why some methods like LSD~\cite{grompone2010lsd} and EDLines~\cite{akinlar2011edlines} are so popular. This experiment compares the execution times of the most important segment detectors when running on hardware with different computational and energy performance.

The experiments measures the average execution time in the images of YUD data set for different detectors, which is a realistic way to evaluate the computational complexity of an algorithm.
YUD contains 101  images. The times are measured on four different platforms: a laptop with an Intel Core i7 8750H CPU, 12 cores and 16GB of RAM; a smartphone \textit{Samsung J5 2017} with an Exynox Octa S CPU, 8 cores and 2GB of RAM; a smartphone \textit{One Plus 7 Pro} with Snapdragon 855 CPU, 8 cores and 6GB of RAM and a GPU GeForce GTX 1050 with 4GB of RAM. 

\begin{table}
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Method}} & \textbf{Intel Core i7} & \textbf{Snapdragon} & \textbf{Exynox} \\
 & \textbf{8750H} & \textbf{855}& \textbf{Octa S} \\ 
\hline
LSD         & 36.51	(1.60)   & 58.68  (0.81) &  390.91 (0.92) \\
EDLines     & 7.64 (0.33)    & 13.79  (0.15) &  65.79 (0.16)  \\ 
AG3line     & 13.04 (0.76)   & 18.57  (0.20) &  100.54 (0.19) \\ 
ELSED No jumps & \textbf{4.18 (0.23)} & \textbf{8.28 (0.03)}  &  \textbf{45.84 (0.02)} \\ 
ELSED       & 5.38 (0.30)    & 10.20  (0.07) &  59.99 (0.16)  \\ \hline MCMLSD     & 4.68K (1.78K) & \multicolumn{2}{c|}{\textbf{GeForce GTX 1050}} \\ 
Linelet     & 20.9K (10.1K) & \multicolumn{2}{c|}{} \\ \cline{3-4}
HAWP        & 12.4K (0.8K) & \multicolumn{2}{c|}{212.25 (8.35) } \\
 & 3.17K (0.52K) & \multicolumn{2}{c|}{417.72 (6.78) } \\
F-Clip HR &  7.94K (0.15K) & \multicolumn{2}{c|}{47.39 (1.46) } \\
F-Clip HG1 & 6.78K (0.22K) & \multicolumn{2}{c|}{11.00 (0.47) } \\ \hline 
\hline
\end{tabular}
\caption{Executions times for different state of art line segment detectors. The times are the average processing times per image, in the YUD~\cite{denis2008efficient} images of size  for different devices. }
\label{table:segment_detection_times}
\end{table}
We use the implementation provided by the authors of each method: LSD, EDLines AG3line and ELSED in C++, Linelet and MCMLSD in Matlab and HAWP,  and F-Clip in Python. 
In all platforms, ELSED is around 2 faster than AG3line,  and EDLines,  faster than LSD and much faster than MCMLSD and Linelet. DL methods are designed to run in the GPU, where some extremely optimized networks like F-Clip HG1 achieve real-time processing, however GPU may not always be available in some platforms like drones, IoT or mobile phones and when it is, it usually involves an unaffordable energy consumption. As a proxy of energy comsumption we run all methods in the same hardware, an Intel Core i7 8750H CPU. 
Looking at the CPU times, the DL methods need between 2300 (HAWP) and 1200 (F-Clip HG1) more computation  than ELSED. Moreover, even when we run the DL methods on a laptop GPU (Geforce GTX 1050) still ELSED is faster than any of the methods. Thus, for limited platforms ELSED represents a clear alternative because not only has it the faster execution times but also it detects better than the other efficient methods (see Table \ref{table:ours_segment_detection_yud}) getting the most repeatable segments (Table \ref{tab:repeatability}).



\subsection{Implementation Details}
\label{sec:implementation}
One of the reasons why ELSED is so fast is because it was carefully implemented in C++ for real-time processing. There are several considerations that have a great impact in the final performance of the detector:
\begin{itemize}
    \item \textbf{Gradient Computation.} Instead of extracting the gradient magnitude and orientation, we only compute the Sobel operators and with these outputs we compute the gradient L1 norm and the predominant direction (vertical or horizontal). The L1 norm is faster than L2 because no square root nor multiplication is required.
    \item \textbf{Least square fitting.} We fit the segments with a least squares approach oriented vertically or horizontally. This has an accuracy similar to the total least squares but it is much faster its terms can be computed incrementally. Specifically, we estimate the line parameters as:
    
    for horizontal lines and switch the roles of 's and 's for the vertical case. The terms  and   are computed incrementally each time a new pixel is added to the segment.
    
    \item \textbf{Stack element reuse.} We reuse the top element of the stack if possible in the EED Algorithm. This improves the detection times avoiding the resize of the stack and also, because we are continuously using the same region of memory, we benefit from the usage of the cache memory. 
\end{itemize}


\section{Conclusions}
\label{sec:conclusion}

In the experimental section we have proved that ELSED is a general purpose line segment detector able to detect accurate segments in a few milliseconds.
The execution times achieved by ELSED arise as a result of joining the processes of edge drawing and segment detection in one single shot, with and Enhance Edge Drawing (EED) strategy conceived for the problem of line segment detection.

Another important problem that ELSED solves for the efficient methods, is the robustness against small image noise and occlusions that cause most of the detectors to break the segments. This is especially important in problems such as Vanishing Points estimation or segment based reconstruction where the duplicity of geometric information can make the next steps of the algorithm fail.

Overall, ELSED is the fastest segment detector in the literature, getting also quite competitive results in segment detection and repeatability benchmarks, reaching a state of art accuracy at the same level as other algorithms orders of magnitude slower. These properties make it ideal for real time applications like Visual Odometry, SLAM, or self-Localization in low-power devices. 

\section*{Acknowledgment}

This research was funded by Doctorado Industrial grant DI-16-08966 and MINECO project TIN2016-75982-C2-2-R.

\bibliographystyle{elsarticle-num} 
\bibliography{mybibliography.bib}

\end{document}
