
\section{Experiments}\label{sec:exp}

To train our models, we used the AdamW optimizer~\cite{loshchilov2017decoupled}, a maximum learning rate of , and a 1-cycle learning rate policy~\cite{smith2019super}, increasing the learning rate linearly for the first 5\% of the training steps.
We ran all experiments on NVIDIA Titan RTX, Titan XP, GeForce, or A5000 GPUs. All experiments used at most 1 GPU and 30GB of GPU memory.
We selected hyperparameters by evaluating on the validation set for each dataset.
Each experiment used at most 1 GPU and 30GB of GPU memory.



\begin{table*}
  \centering
    \begin{tabular}{lcccccc}
    \toprule
    Method & Enc & Test (FF) & Test (BA) & Test (CM) & Test (NV) & Avg \\
    \midrule
    SNAIL~\cite{mishra2018simple} & RN12 & 56.3  3.5& 60.2  3.6 & 60.1  3.1 & 61.3  0.8 & 59.5 \\
    ProtoNet~\cite{snell2017prototypical} & RN12 & 64.8  0.9 &72.4  0.8  & 62.4  1.3 & 65.4  1.2  &  66.3 \\
    MetaOptNet~\cite{lee2019meta} & RN12 &60.3  0.6 & 71.7  2.5 & 61.7  1.1 & 63.3  1.9  &  64.3 \\
    ANIL~\cite{raghu2020rapid} & RN12 & 56.6  1.0 & 59.0  2.0 & 59.6  1.3 & 61.0  1.5  &  59.1 \\
    Meta-Baseline-SC~\cite{chen2020new} & RN12 &66.3  0.6 & 73.3  1.3 & 63.5  0.3 & 63.9  0.8  &  66.8 \\
    Meta-Baseline-MoCo~\cite{nie2021bongardlogo} & RN12 &65.9  1.4 & 72.2  0.8 & 63.9  0.8 & 64.7  0.3  & 66.7  \\
    WReN-Bongard~\cite{barrett2018measuring} & RN12 &50.1  0.1 & 50.9  0.5 & 53.8  1.0 & 54.3  0.6  &  52.3 \\
    CNN-Baseline~\cite{nie2021bongardlogo} & RN12 &51.9  0.5 & 56.6  2.9 & 53.6  2.0 & 57.6  0.7  &  54.9 \\
    Meta-Baseline-PS~\cite{nie2021bongardlogo} & RN12 & 68.2  0.3 & 75.7  1.5 & 67.4  0.3 & 71.5  0.5 & 70.7 \\ 
    TPT~\cite{shu2022testtime} & CLIP &52.5 & 65.9 & 58.6 & 56.3  &  58.3\\
    PredRNet~\cite{yang2023neural} & Other & \textbf{74.6}  0.3 & 75.2  0.6 & \textbf{71.1}  1.5 & 68.4  0.7 &  72.3 \\
    \midrule
    Prototype & RN12 & 68.9  0.5 & 68.8  1.6 & 64.7  0.8 & 66.2  0.8 & 67.1  0.7 \\
    Prototype + standardize & RN12 & 72.9  0.5 & 80.8  0.5 & 66.8  1.1 & 71.3  1.4 & 73.0  0.8 \\
    SVM & RN12 & 65.7  0.4 & 75.3  1.3 & 65.4  2.0 & 70.4  1.6 & 69.2  1.1 \\
    SVM + standardize & RN12 & 72.2  0.3 & 83.8  0.9 & 68.3  1.6 & 71.8  1.2 & 74.0  0.5 \\
    -NN & RN12 & 62.1  0.7 & 69.3  0.9 & 62.8  0.6 & 63.3  0.9 & 64.4  0.4 \\
    -NN + standardize & RN12 & 73.0  0.6 & \textbf{84.9}  0.8 & 67.8  1.8 & 71.0  1.3 & 74.2  0.5 \\
    Prototype-Mimic & RN12 & 73.1  0.4 & 80.9  0.6 & 68.4  1.1 & 72.0  0.9 & 73.6  0.3\\
    SVM-Mimic & RN12 & 73.3  0.3  &  84.3  0.8 & 69.4  0.8  & \textbf{74.2}  0.3 & \textbf{75.3}  0.1 \\
    \midrule
    Human Expert~\cite{nie2021bongardlogo} & & 92.1  7.0 & 99.3  1.9 & \multicolumn{2}{c}{90.7  6.1}  &  94.0 \\
    Human Amateur~\cite{nie2021bongardlogo}&  & 88.0  7.6 & 90.0  11.7 & \multicolumn{2}{c}{71.0  9.6} &  83.0 \\
    \bottomrule
  \end{tabular}
    \caption{\textbf{Results on Bongard-LOGO.}
    Error bars are obtained by evaluating three trained models over the entire test set (thousands of problems), for all methods except deterministic ones.}
    \label{tab:logo}

\end{table*}

\begin{table*}
  \centering
    \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Method} & Unseen Act / & Seen Act / & Unseen Act / & Seen Act /  & \multirow{2}{*}{Avg} \\
    & Unseen Obj & Unseen Obj & Seen Obj & Seen Obj & \\
    \midrule
    HOITrans~\cite{zou2021endtoend} &  62.87  &  64.38  &  63.10  &  59.50  &  62.46  \\
    TPT~\cite{shu2022testtime} &  65.66  &  65.32  &  68.70  &  66.03  &  66.43  \\
    \midrule
    Prototype &  65.45  &  65.73  &  76.31  &  68.02  &  68.88  \\
    Prototype + standardize &  68.11  &  68.92  &  77.44  &  69.97  &  71.11  \\
    SVM & 68.05 & 67.36 & 74.89 & 67.91 & 69.55 \\
    SVM + standardize & 69.48 & 69.50 & 75.37 & 69.68 & 71.01 \\
    -NN & 61.04 & 60.88 & 68.54 & 63.28 & 63.44 \\
    -NN + standardize & 66.72 & 67.08 & 75.65 & 68.04 & 69.37 \\
    Prototype-Mimic&   68.88  0.30 & 70.74  0.44 & 76.66  0.26 & \textbf{71.25}  0.29 & 71.88  0.20 \\
    SVM-Mimic & \textbf{69.59}  0.13 & \textbf{70.83}  0.27 & \textbf{78.13}  0.27 & 71.23  0.07 & \textbf{72.45}  0.16 \\
    \midrule
    Human~\cite{jiang2023bongardhoi} &  87.21  &  90.01  &  93.61  &  94.85  &  91.42  \\
    \bottomrule
  \end{tabular}
    \caption{\textbf{Results on Bongard-HOI with a frozen CLIP encoder.}
    Error bars are obtained by evaluating three trained models over the entire test set (thousands of problems), for all methods except deterministic ones.}
    \label{tab:hoi-table}
\end{table*}

\subsection{Bongard-LOGO}
\paragraph{Dataset}
We evaluate on the Bongard-LOGO dataset \cite{nie2021bongardlogo}, which consists of synthetic images containing geometric shapes similar to those in the original Bongard problems.
We follow previous works~\cite{nie2021bongardlogo} and use a ResNet-12~\cite{he2016deep} trained from scratch. Each Bongard-LOGO problem consists of seven positive and seven negative examples. The first six positives and first six negatives are chosen to serve as supports, and the remaining two are used as queries.
We train models for 500,000 iterations with a batch size of 2. We resize images to  pixels and apply random cropping and horizontal flipping to augment. We train all models using support dropout. Label noise did not improve results on the validation set (likely because the ground truth is error-free), so we omit it.

\paragraph{Results} Table~\ref{tab:logo} shows results on Bongard-LOGO's four test splits, each assessing various forms of out-of-domain generalization. We observe that our simple baselines (Prototype, SVM, and -NN) coupled with our contrastive encoder match or exceed most previous approaches. Incorporating standardization leads to a 5-10 point boost for each of them. The support-set Transformer models, Prototype-Mimic and SVM-Mimic, perform better than the non-Transformer variants.
SVM-Mimic performs the best, with an accuracy of 75.3\%. This is 3 points better than the the concurrent PredRNet~\cite{yang2023neural}.
We note that while PredRNet performs well on Bongard-LOGO, it is not straightforward to extend it to natural image tasks like Bongard-HOI, since it cannot leverage arbitrary vision backbones, and relies on training end-to-end from scratch.

\subsection{Bongard-HOI}
\paragraph{Dataset}
Bongard-HOI~\cite{jiang2023bongardhoi} structures Bongard problems around natural images of human-object
interaction. The positive support set of each problem involves a human interaction with an object, and the negative support set contains different interactions with the same object. Following a previous work~\cite{shu2022testtime}, we use CLIP with a ResNet-50 backbone as our image encoder. The Bongard-HOI dataset is structured similarly to the Bongard-LOGO dataset, with seven positives and seven negatives per problem. Since the dataset does not specify the support/query split for each problem, we arbitrarily select the final positive and negative images as queries. 
We manually created a clean subset of the original training dataset after observing that the publicly-available Bongard-HOI dataset is imbalanced and contains incorrect labels (details in Appendix). We publicly release this cleaned training dataset to aid future works. We use augmentations at training time, including horizontal flips, random grayscale, color jitter, and random rescaling and cropping (to ), and apply support dropout and label noise.
We train for 10,000 iterations with a batch size of 16 and weight decay of 0.01.

\paragraph{Results}

Table~\ref{tab:hoi-table} shows quantitative results on the four test splits of Bongard-HOI, each of which tests different forms of out-of-domain generalization. The ``seen act/seen obj'' split contains only human-object interactions where the actions and objects were both seen during training; ``seen act/unseen obj'' evaluates on problems with seen actions and unseen objects, and so on. We first note that our simple baselines (Prototype, SVM, and -NN) approach or beat the state-of-the-art method, Test-time Prompt Tuning (TPT; \citet{shu2022testtime}). This is surprising, as none of these baselines uses training at any point, while TPT performs gradient descent at test time to find a classifier for each Bongard problem. We find that for every baseline, standardization leads to further performance improvements. For example, Prototype baseline combined with standardization exceeds TPT by almost 5 points.
Using a support-set Transformer leads to further improvements. SVM-Mimic exceeds its counterpart baseline, SVM + standardize, by almost 1.5 points, setting a new state-of-the-art on Bongard-HOI.

\subsection{Bongard-Classic}
\paragraph{Dataset} Mikhail Bongard's original dataset~\cite{foundalis, yun2020deeper} is a challenging dataset of roughly 200 problems with no training or validation set. Since there is no training set, we directly evaluate backbones and support-set Transformers pre-trained on Bongard-LOGO. We perform the same data pre-processing steps as done for Bongard-LOGO. Since the dataset has six positive and six negative examples for each problem, we select one positive and one negative example as queries. For each Bongard problem, we average results over every possible choice of positive and negative query. Prior works use evaluation metrics other than accuracy, such as number of Bongard problems solved~\cite{youssef2022solution} and accuracy on manually-created query images~\cite{yun2020deeper}. We use our own accuracy metric (percent correct total) as it is easier to calculate and is more consistent with Bongard-HOI and Bongard-LOGO.

\paragraph{Results} Table~\ref{tab:simple} shows the results of our approaches on Bongard-Classic. Due to the  evaluation differences mentioned, we are unable to compare with other works, but we hope our results can serve as baselines for future work. Standardization leads to improvements in every case. Support-set Transformers do not lead to consistent improvements, likely because 
the Transformer has not been fine-tuned on Bongard-Classic problems. SVM-Mimic is our second-best performing method, attaining 60.84\% accuracy.


\begin{table}
  \centering
    \begin{tabular}{lc}
    \toprule
    Method & Accuracy \\
    \midrule
    Prototype & 57.52  0.36 \\
    Prototype + standardize & 57.82  0.72 \\
    SVM & 60.10  0.55 \\
    SVM + standardize & \textbf{61.27}  0.12 \\
    -NN & 56.80  0.44 \\
    -NN + standardize & 58.10  0.33 \\
    Prototype-Mimic & 57.61  0.89 \\
    SVM-Mimic & 60.84  0.43 \\
    \bottomrule
  \end{tabular}
    \caption{\textbf{Results on Bongard-Classic.} Error bars are obtained by evaluating three models over all 200 problems.}
  \label{tab:simple}

\end{table}


\begin{table}
    \centering
      \begin{tabular}{lc}
      \toprule
      Method & Accuracy \\
      \midrule
      PMF~\cite{hu2022pushing} &  74.44  0.22  \\
      Prototype + standardize + PMF &  75.83  0.20  \\
      SVM + standardize + PMF & 75.42  0.17 \\
      Prototype-Mimic + PMF &  75.60  0.14 \\
      SVM-Mimic + PMF & \textbf{76.41}  0.14 \\
      \bottomrule
    \end{tabular}
        \caption{\textbf{Results on Bongard-HOI with CLIP fine-tuned via PMF}. For brevity, we average results over all test splits.}
    \label{tab:pmf}

\end{table}

\subsection{Cross-Image Context with Fine-Tuned CLIP}

The state-of-the-art method for category-level few-shot learning tasks is PMF~\cite{hu2022pushing}, and this technique involves fine-tuning the backbone encoder to produce better prototypes. This is complementary to our approach. Table~\ref{tab:pmf} shows results from combining cross-image context with PMF, evaluated on Bongard-HOI.
PMF has not previously been applied to Bongard problems, and it cannot be fairly compared with the methods in Table~\ref{tab:hoi-table} due to the fine-tuned rather than frozen encoder. We combine support-set standardization, support-set Transformers, and our baselines with PMF by first fine-tuning the CLIP encoder with the PMF objective, freezing the weights, and then plugging the fine-tuned encoder directly into each method. 
PMF performs very well, exceeding the 
 results obtained with frozen CLIP. 
Incorporating cross-image context through support-set standardization and support-set Transformers leads to further improvements.
One failure case is Prototype-Mimic; note that
the PMF training objective already optimizes the prototypes, so it is possible that the forward pass by the Transformer is not necessary. 
SVM-Mimic combined with PMF performs best. 
PMF training details are in the Appendix. 

\subsection{Robustness to Number of Supports}

\begin{figure}
  \centering
  
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{robustness_Unseen_Act,_Unseen_Object.pdf}
    \subcaption{Unseen Act, Unseen Obj}
    \label{fig:sub1}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{robustness_Seen_Act,_Unseen_Object.pdf}
    \subcaption{Seen Act, Unseen Obj}
    \label{fig:sub2}
  \end{subfigure}
  
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{robustness_Unseen_Act,_Seen_Object.pdf}
    \subcaption{Unseen Act, Seen Obj}
    \label{fig:sub3}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{robustness_Seen_Act,_Seen_Object.pdf}
    \subcaption{Seen Act, Seen Obj}
    \label{fig:sub4}
  \end{subfigure}
  
  \caption{\textbf{Robustness to number of supports in Bongard-HOI.} The x-axis measures the number of supports of each class seen, with 6 being the default setting, and the y-axis measures accuracy. Means and standard deviations are reported across three runs, where the randomness is with respect to the subset of supports chosen.}
  \label{fig:robustness}

\end{figure}

We are interested to measure SVM-Mimic's robustness to the number of support samples provided as input. 
Figure~\ref{fig:robustness} shows the results of SVM-Mimic, a version of SVM-Mimic trained without support dropout or label noise (SVM-Mimic w/o Reg), and SVM baseline + standardization (SVM + Std) on the test splits of Bongard-HOI. The plots demonstrate that SVM-Mimic attains consistently higher accuracy than the other approaches for different numbers of supports.
This demonstrates the benefits of learning a Transformer to produce rules rather than non-parametrically fitting an SVM to each Bongard problem. In the Appendix, we also measure robustness to label noise.

\subsection{Ablation Study: Forms of Normalization}
\begin{table}
  \centering
    \begin{tabular}{lc}
    \toprule
    Method & Accuracy \\
    \midrule
    Prototype & 68.88  \\
    Prototype + train-set standardize &   70.06  \\
    Prototype + support-set standardize &  \textbf{71.11}  \\
    \midrule
    SVM & 69.55 \\
    SVM +  normalization & 67.94 \\
    SVM + train-set standardize & \textbf{71.19} \\
    SVM + support-set standardize & 71.01 \\
    \midrule
    -NN & 63.44 \\
    -NN + train-set standardize & 65.86 \\
    -NN + support-set standardize & \textbf{69.37} \\
    \bottomrule
  \end{tabular}
    \caption{\textbf{Effect of different forms of normalization in Bongard-HOI.} Results are averaged over the four test splits. Note that Prototype and -NN use  normalization by default. 
    }
  \label{tab:norm}


\end{table}

Can support-set standardization be replaced with other forms of centering and scaling?  
Table~\ref{tab:norm} compares the performance of different forms of normalization on Bongard-HOI. ``Support-set standardize" is our main approach; ``train-set standardize" computes mean and variance statistics using the entire training set of CLIP embeddings;  normalization normalizes each embedding independently.
Importantly, neither train-set standardization nor  normalization captures cross-image context at the \textit{task}-level (instead focusing on the dataset-level and image-level).
Train-set standardization leads to a 0.1 point gain over support-set standardization when using SVMs, but it performs 1 point worse with Prototypes and over 3 points worse with -NN, suggesting that support-set standardization is a safe choice. We also find that  normalization harms performance on the SVM, despite the fact that this is a design step included by default in Prototype and -NN methods for computing cosine similarity.
