\documentclass[nonacm, sigconf]{acmart}
\usepackage{listings}
\usepackage{diagbox}
\usepackage{fancybox}











\usepackage{graphicx} \usepackage{subcaption} \graphicspath{ {fig/} }
\PassOptionsToPackage{hyphens}{url}\usepackage{url}
\usepackage{todonotes}
\usepackage{enumitem} \usepackage[normalem]{ulem} 

\newcommand{\curvysymbol}[1]{\mathcal{#1}}
\newcommand{\sanssymbol}[1]{\mathbf{\mathsf{#1}}}

\usepackage{amsmath,amssymb,mathtools}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\average}{average}
\DeclareMathOperator*{\N}{\mathbb{N}}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\Var}{Var}
\def\1{\mathbbm{1}}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}\DeclarePairedDelimiter\norm{\lVert}{\rVert}\newcommand{\para}[1]{\left({#1}\right)}
\newcommand{\squarepara}[1]{\left[{#1}\right]}
\def\spara{\squarepara}
\newcommand{\curlypara}[1]{\left\{{#1}\right\}}
\def\cpara{\curlypara}
\newcommand{\trianglepara}[1]{\langle{#1}\rangle}
\def\tpara{\trianglepara}
\newcommand{\ceil}[1]{\lceil{#1}\rceil}
\newcommand{\inv}[1]{{#1}^{-1}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\family}[1]{\curvysymbol{#1}}
\newcommand{\distr}[1]{\curvysymbol{#1}}
\newcommand{\series}[1]{\pmb{#1}}

\newcommand{\Term}{Term} \newcommand{\term}{term} \newcommand{\NTerm}{NonTerm} \newcommand{\nterm}{nonterm} \newcommand{\prog}[1]{\curvysymbol{#1}}
\newcommand{\graph}[1]{{#1}}
\newcommand{\tree}[1]{\sanssymbol{#1}}
\newcommand{\order}[1]{\sanssymbol{#1}}
\newcommand{\seq}[1]{\sanssymbol{#1}}
\newcommand{\lang}[1]{\curvysymbol{#1}}
\newcommand{\Implies}{\Rightarrow}

\newcommand{\code}[1]{{\texttt{#1}}}
\usepackage{seqsplit}
\newcommand{\longcode}[1]{{\texttt\seqsplit{#1}}}


\newcommand{\softmax}{\text{softmax}}
\newcommand{\Attn}{\text{Attn}}
\newcommand{\AttnTreeRel}{\text{Attn}_\text{TreeRel}}
\newcommand{\AttnLayer}{\text{AttnLayer}}
\newcommand{\AttnBlock}{\text{AttnBlock}}
\newcommand{\LN}{\text{LN}}
\newcommand{\model}{\text{model}}
\newcommand{\UD}{\text{UD}}
\newcommand{\UDi}{\text{UDi}}


\newcommand{\abbr}[1]{\textsc{#1}~}
\newcommand{\abbrs}[1]{\textsc{#1}{\footnotesize{s}}~}

\newcommand{\SrcSeq}{\abbr{SrcSeq}} \newcommand{\SrcRNN}{\abbr{SrcRNN}} \newcommand{\LeafSeq}{\abbr{LeafSeq}} \newcommand{\RootPath}{\abbr{RootPath}} \newcommand{\LeafTokens}{\abbr{LeafTokens}} \newcommand{\DFS}{\abbr{DFS}} \newcommand{\TreeRel}{\abbr{DFS{ud}}} \newcommand{\TreeReli}{\abbr{DFS{ud+}}} 

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newenvironment{example}{\renewcommand{\proofname}{Example}\begin{proof}}{\end{proof}}

\newcommand{\algref}[1]{Algorithm~\ref{#1}}
\newcommand{\eqnref}[1]{Eq.~\eqref{#1}}
\newcommand{\figref}[1]{Fig~\ref{#1}}
\newcommand{\secref}[1]{Sec~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}

\newcommand{\inlinecomment}[3]{{\color{#1}{\noindent\textit{\textbf{#2}: {#3}}}}}
\newcommand{\jinman}[1]{\inlinecomment{ACMPurple}{Jinman}{#1}}
\newcommand{\satish}[1]{\inlinecomment{ACMRed}{Satish}{#1}}
\newcommand{\sonia}[1]{\inlinecomment{ACMRed}{Sonia}{#1}}
\newcommand{\Yuchi}[1]{\inlinecomment{ACMRed}{Yuchi comments}{#1}}

\newcommand{\newtext}[1]{{\color{blue}{#1}}}
 \begin{document}

\title{Code Prediction by Feeding Trees to Transformers}

\author{Seohyun Kim}
\authornote{Both authors contributed equally to this research.}
\affiliation{\institution{Facebook Inc.}
  \country{U.S.A.}
}
\email{skim131@fb.com}

\author{Jinman Zhao}
\authornotemark[1]
\affiliation{\institution{University of Wisconsin-Madison}
  \country{U.S.A.}
}
\email{jz@cs.wisc.edu}

\author{Yuchi Tian}
\affiliation{\institution{Columbia University}
  \country{U.S.A.}
}
\email{yuchi.tian@columbia.edu}

\author{Satish Chandra}
\affiliation{\institution{Facebook Inc.}
  \country{U.S.A.}
}
\email{schandra@acm.org}





\begin{abstract}
  In this paper, we describe how to leverage \emph{Transformer}, a recent neural  architecture for learning from sequential data (such as text), for code completion.  As in the realm of natural language processing, Transformers surpass the prediction accuracy achievable by RNNs; we provide an experimental confirmation of this over a Python dataset.
  
  Furthermore, we show that the way to obtain even better accuracy from Transformers is to expose the syntactic structure of code, which is easily recovered by parsing, to the neural network. This works significantly better than presenting the code as a linear token sequence, which is how Transformers were originally intended to be used.
  
  To accomplish this, we propose a novel enhancement to the self-attention mechanism of the Transformer.  We enable the mechanism to learn weights---that is, how much to focus on each preceding token in the input---not only on the basis of a token's value, but also on the basis of the spatial relationships, as in their positions in the abstract syntax tree, between each pair of tokens.
  
  We provide comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on a Facebook internal Python corpus.
  
\end{abstract}









\maketitle

\section{Introduction}
\label{sec:introduction}




Last several years have witnessed exciting progress in the application of machine learning techniques to developer productivity tools~\citep{allamanis2018survey}, and in particular, to code prediction~\citep{hindle2016naturalness,raychev2016probabilistic-deep3-eth-dt,li2018code-rnn-attn,brockschmidt2018generative-graph}.  
The idea of code prediction in general is to predict the next code element given previously written code.  Code prediction is commonly used in an IDE for
\textit{auto-complete}, where based on the developer's cursor position and the code already written up to the cursor position, the IDE offers the most likely next tokens (perhaps as a drop down list to choose from.)
Auto-complete, not only saves the developer from having to type in the next token(s), but is also an effective code learning mechanism: for instance, a developer might not know the name of an API call he needs off the top of his head, but is able to choose among the choices shown by an auto-complete tool.  

Recent work has shown the promise of code prediction based on machine learning.  A common idea here is to use language models trained over large code corpora---treating code as text in a natural language~\cite{hindle2016naturalness,allamanis2018survey}---to enable highly accurate code prediction.  These models have leveraged natural language processing techniques: n-grams~\citep{hindle2016naturalness,hellendoorn2017deep}, and more recently, deep neural networks such as RNNs~\citep{parvez-etal-2018-building,li2018code-rnn-attn,liu2020modeling-stack-lstm}.

A different line of work has proposed code prediction based on statistics on \textit{syntactic} structure of code, as opposed to seeing code as text. These include probabilistic context-free grammars and probabilistic higher-order grammars ~\cite{bielik2016phog,raychev2016learning-noisy,raychev2016probabilistic-deep3-eth-dt,raychev2016probabilistic-deep3-eth-dt}.  This class of models considers code artifacts as abstract syntax trees, and make their predictions based on information gleaned selectively across paths in the code's AST.
Specifically, Raychev et al.~\cite{raychev2016probabilistic-deep3-eth-dt} learn a decision tree model that uses this information essentially as features.

Researchers in the NLP community have recently developed \textit{Transformers}, a new neural architecture for even more effective natural language processing~\cite{vaswani2017attention}.  As we discuss later, Transformers promise to overcome some of the limitations of RNNs.
We investigated the use of Transformers for code prediction, treating code as textual data, and validated experimentally that Transformers indeed outperform RNNs on the next code token prediction task.

Given this already strong baseline, we consider the question of whether informing the Transformer of code's syntactic structure can further improve prediction accuracy.  Our main result is that \textit{a better way to use transformers for code prediction is to expose the syntactic structure of code to the network.} The details of how to do this are interesting, as encoding the structure of a program's abstract syntax tree is not natural for sequence models.  We show a range of design choices for communicating the AST structure to the Transformer. We find that the more faithfully we communicate the tree structure to the Transformer, the better the accuracy we obtain! 

\subsection{Key Results}

We report results based on training and evaluating various models code prediction on the py150~\cite{py150}  dataset. 

\begin{itemize}
    \item We show that a neural model based on the Transformer architecture is able to outperform state-of-the-art neural (e.g. RNN-based (e.g. in~\cite{hellendoorn2017are-deep-best, karampatsis2020big-bpe}) as well as non-neural models (e.g. Deep3~\cite{raychev2016probabilistic-deep3-eth-dt}).  Measured on the leaf tokens of the ASTs, our best Transformer model improves the mean reciprocal rank (reported as a percentage, see Sec~\ref{sec:implementation}) significantly over the prior work: upon the RNN model (40.0\% v 55.5\%) as well as upon the corresponding Deep3 model (43.9\% v 73.6\%).
    
    \item We show that a key to obtaining superior performance from the Transformer model is to feed not just the source token sequence as is common in NLP tasks, but in making the Transformer aware of the \textit{syntactic structure} of the code. We show that with more detailed syntactic structure, we get better accuracy (from 65.7\% to 74.1\% on leaf tokens). 
    
    We provide a preliminary investigation into why the Transformer model that is aware of tree structure works better than one without, by using saliency maps~\cite{simonyan2013deep-saliency}.
    
    \item Our key technical novelty is a novel enhancement to the Transformer's self-attention mechanism. We enable the mechanism to learn weights---how much to focus on each preceding token in the input---by factoring in the spatial relationship in the abstract syntax tree between each pair of tokens.
    
    \item We also evaluated our trained model on a dataset selected from a Python code repository \textit{internal} to Facebook, and found the relative benefits to be similar to those on py150.  The accuracy on this other corpus indicates that the Transformer model is generalizable to other corpora. 
    
 
\end{itemize}



\paragraph{Outline} 
\secref{sec:problem} articulates the code prediction problem in a couple of different forms, and introduces a running example. 
\secref{sec:models} gives an introduction to Transformers, including how they would apply to source code. This section also describes how to communicate tree structure to the transformer. 
\secref{sec:previouswork} provides a quick recap of the previous work, focusing on the ones against which we compare our models.  
\secref{sec:implementation} describes our datasets and implementation. \secref{sec:evaluation} presents our quantitative results. \secref{sec:inspection} takes a closer look into why our models worked well (or did not.) \secref{sec:related-works} discusses related work in the area of code prediction and in using Transformers. We conclude the paper with our future work.  
 
\section{Code Prediction}
\label{sec:problem}

\begin{figure}
\lstset{basicstyle=\footnotesize\ttfamily}
\lstset{morekeywords={atoi}}
\begin{lstlisting}
...
ip = socket.gethostbyname (host)
[port, request_size, num_requests, num_conns] = map (
                string.atoi, sys.argv[2:]
)
chain = build_request_chain(num_requests, host, request_size)
...
\end{lstlisting}
    \caption{Running example of Python code. 
    The code snippet~\protect\footnotemark is from the py150 dataset~\cite{py150}.
    }
    \label{fig:examplecode}
\end{figure}
\footnotetext{\url{data/JeremyGrosser/supervisor/src/supervisor/medusa/test/test_11.py}}

Consider the Python code fragment in Fig~\ref{fig:examplecode}.  Suppose a developer has written code up to \texttt{string} following by a dot.  At this point, it will be helpful for the IDE to prompt the developer with attribute names that are \textit{likely} to follow, preferably, with \texttt{atoi} ranked at the top because in this case that is the correct next token.

Our goal is to devise a model such that it takes some code fragment as input and predicts the next token. In this section, we describe two main methods of representing code as inputs to be fed into various models.

\subsection{Sequence-based Representation}
In NLP, a common way of feeding in information for \textbf{next token prediction} is with a linearized token sequence. The same technique can be applied with source code, where we parse the source code into tokens. To predict "atoi", we would look at the tokens: [..., "map", "(", "string", "."]. This is a natural approach for next token prediction since each input and prediction in the sequence equates to a token in source code, so we can easily evaluate on all tokens.

\subsection{AST-based Representation}

An alternate to a source token sequence is the Abstract Syntax Tree (AST), as shown in Fig~\ref{fig:exampleast} for the code fragment in Fig~\ref{fig:examplecode}. An AST can better represent spatial relationship between nodes. For example, in source code, the tokens \texttt{ip} (node 3) and \texttt{chain} (node 41) are separated by 30 tokens, but they are related in the AST via a specific (short) path.

ASTs represent some source tokens explicitly and others implicitly. Tokens corresponding to identifiers, field names, and constants appear explicitly as leaf (terminal) nodes: for instance, \texttt{ip} and \texttt{host} appear as the leaf (terminal) nodes 3 and 11, respectively. Keywords and other syntactic tokens (e.g. \texttt{=}) are implied by the \textit{type} of internal nodes (e.g. \texttt{Assign}).  Accordingly, the prediction task can be separated into:
\begin{itemize}
    \item \textbf{Value prediction}: Predicting the values at leaf nodes. For example, given nodes 0-10 of the tree, we want to predict \texttt{host}, which is the value of the leaf node at node 11.
    \item \textbf{Type prediction}: Predicting the types at internal nodes. For example, given nodes 0-33 of the tree, we want to predict \texttt{Attr}, which is the type of the internal node at node 34. 
\end{itemize}

Knowing that the type of a node is \texttt{Attr} implies that after the source tokens corresponding to its left child, there will be a token "." (dot) before the (single) token from its right child. Thus, value prediction and type prediction together can \emph{simulate} next token prediction problem, though there will need to be a stack-based controller that would call the right predictor,  maintain some state, and emit the predicted source tokens appropriately.

\begin{figure*}
\includegraphics[scale=0.3]{fig/exampleast}
\caption{AST for the example in Fig~\ref{fig:examplecode}. The leaf (terminal) nodes have values and the interior (non-terminal) nodes have types. }
\label{fig:exampleast}
\end{figure*}

\begin{table*}[]
\begin{small}
    \centering
    \begin{tabular}{l|l|c|c|c|c|c|c|c|c|c|c|c|c}
    \multicolumn{1}{l|}{} & \textbf{Token value} & \textbf{input} & ip   & socket & getHostByName & host & map  & string & atoi & sys  & argv & 2    & chain \\
    \multicolumn{1}{l|}{} & \textbf{(node id)}   & \textbf{kind} & 3    & 7      & 9             & 11   & 24   & 27     & 29   & 33   & 35   & 38   & 41    \\ \hline
    \textbf{Previous}    & SrcRNN   & seq & >10 & >10 & 3   & 2   & 7   & >10 & 2   & \textbf{1}   & \textbf{1}   & 3   & >10   \\
    \textbf{work}        & Deep3    & AST & 5   & 5   & 3   & \textbf{1}   & 5   & 5   & 5   & 5   & \textbf{1}   & 6   & 5  \\ \hline
    \textbf{Our}        & \SrcSeq  & seq & >10	& \textbf{1}	  & \textbf{1}	& 6   & >10	& >10 & \textbf{1}	& 10  & \textbf{1}	& \textbf{1}	  & >10   \\
\textbf{work}        & \DFS     & AST & >10	& \textbf{1}	  & 5	& \textbf{1}	  & 4	& \textbf{1}	  & \textbf{1}	& \textbf{1}	  & \textbf{1}	& \textbf{1}	  & >10   \\
    \multicolumn{1}{l|}{} & \textbf{\TreeRel} 
                                    & AST & 3	& \textbf{1}	  & \textbf{1}	& \textbf{1}	  & \textbf{1}	& \textbf{1}	  & 4	& \textbf{1}	  & \textbf{1}	& \textbf{1}	  & \textbf{1}    
    \end{tabular}
\end{small}
    \caption{Ranks for the predictions for the leaf nodes listed in Fig~\ref{fig:exampleast}.  >10 means the model did not get the right answer in the top 10 results. \TreeRel is our most powerful model.}
    \label{tab:results_example}
\end{table*}


\subsection{A preview of results}
In this paper, we explore both sequence-based and AST-based representation of code for code prediction, using various models (RNN, Decision Tree, Transformers).   
Table~\ref{tab:results_example} shows the \textit{ranks} (lower is better) of predicting the correct leaf node for all the leaf nodes in the AST in Fig~\ref{fig:exampleast}. It compares two models of previous work and four Transformer-based models (our work). Transformer models generally achieve lower ranks, and in some cases they are the only models that produce the right token in their top-10 predictions. This table also shows (via one example here, but the results carry over) that feeding ASTs to Transformer models brings better results than feeding them source token sequences.  The core of our paper is about how to feed ASTs to Transformers.
 
\section{Transformers for Code Prediction}
\label{sec:models}

In this section, we explain the four models of our own creation: \SrcSeq, \RootPath, \DFS, \TreeRel. All four models use Transformers~\citep{vaswani2017attention}, a class of deep learning models that have achieved the state-of-the-art results~\citep{devlin2018bert,dong2019unified-unilm,radford2019language-gpt2} for a variety of NLP tasks such as language modeling, question answering, and sentence entailment. In this section, we discuss how we can apply Transformers for next code token prediction, feeding in both sequence-based (\SrcSeq) and AST-based (\RootPath, \DFS, \TreeRel) inputs.


\begin{figure}[t]
    \centering
    \includegraphics[scale=0.35]{fig/gpt2}
    \caption{Schematic of a GPT2 Transformer. The self-attention layer is able to consider all tokens in the input up to the point of prediction. Here the self-attention box depicts the information flow when predicting next token after the "."; see Table~\ref{tab:QK_softmax} for where the numbers come from.}
    \label{fig:gpt2}
\end{figure}


\subsection{A Primer on Transformer}
Transformers belong to a class of deep neural networks that are designed for sequence processing.  Transformers eschew the hidden states of earlier generation sequence networks (such as RNNs, see Sec~\ref{sec:previouswork}) in favor of exposing the entire input sequence simultaneously, solely relying on \textit{attention} mechanisms.  In Transformers, information from \textbf{any} previous location of the sequence can directly affect the encoding of the next token, through a mechanism called \textit{self-attention}, which helps greatly improve the connectivity in long sequences.  Transformers also uses multiple heads of these self-attention blocks, called \textit{multi-headed attention}, which enables the model to simultaneously consider different ways of attending to previous information within one block and also across other blocks. 

This section explains self-attention in detail (Figure~\ref{fig:gpt2}), as it is the crux of the model. 
The purpose of self-attention is to give higher attention to more relevant tokens in the input sequence. 
To illustrate this, let's take an example input sequence: ["map", "(", "string", "."], and the target token being "atoi." This input sequence is first fed through the initial Embedding layer to give: $\vec{E} = \spara{e_\text{map}, e_\text{(}, e_\text{string}, e_\text{.}}$. Then, this embedding is used as input to three fully-connected networks ($\mat{W_q}, \mat{W_k}, \mat{W_v}$) to create a query, key, and value embedding for the sequence: 
\begin{gather*}
  \vec{Q} = \vec{E} \mat{W_q},   \vec{K} = \vec{E} \mat{W_k},  \vec{V} = \vec{E} \mat{W_v}
\end{gather*}
In our example, $\vec{Q} = \spara{q_\text{map}, q_\text{(}, q_\text{string}, q_\text{.}}, \vec{K} = \spara{k_\text{map}, k_\text{(}, k_\text{string}, k_\text{.}}$, and $\vec{V} = \spara{v_\text{map}, v_\text{(}, v_\text{string}, v_\text{.}}$. We use the query vector $\vec{Q}$ to "query" the "keys" $\vec{K}$ to see which token relationships are the most important by calculating $\vec{Q}\vec{K}^\intercal$. This results in a matrix of size n x n, as seen in ~\tabref{tab:QK}, where n is the length of the input sequence. Each row is then normalized (by square root of $d_{k}$) and passed through a softmax layer so all the scores are positive and add up to 1. ~\tabref{tab:QK_softmax}  shows an example of the self-attention weights~\footnote{The rows do not sum to 1 since there are previous tokens in the sequence that is not shown in this table}; looking at the last row, we can see that most of the self-attention is given to ".", meaning it has a greater factor in predicting the next token "atoi". Also note how the matrix is a lower triangular matrix - this is because self-attention cannot be applied to tokens that have not been seen before. Finally, this matrix is multiplied with the value vector to weight the token embeddings:
\begin{gather*}
    \mat{A} = \Attn(\mat{Q}, \mat{K}, \mat{V} ) = \softmax(\frac{\mat{Q} \mat{K}^\top}{\sqrt{d_k}}) \mat{V},
\end{gather*}
In our example, $\vec{A} = \spara{0.2 * v_\text{map}, 0.1 * v_\text{(}, 0.2 * v_\text{string}, 0.4 * v_\text{.}}$.
$\mat{A}$ is then fed through a fully-connected network, coupled with skip connections and layer normalizations. This process is repeated num\_layer number of times. Finally, the output of the last layer goes through a classification layer at the end to generate predictions for the next token.


\begin{table}
\centering
\begin{tabular}{|l|l|l|l|l|}  \hline
... & map  & ( & string & . \\ \hline
map    
    & $q_{map}k_{map}$ &&& \\ 
(      
    & $q_{(}k_{map}$ 
    & $q_{(}k_{(}$  && \\
string 
    & $q_{string}k_{map}$ 
    & $q_{string}k_{(}$  
    & $q_{string}k_{string}$ &  \\
.   
    & $q_{.}k_{map}$
    & $q_{.}k_{(}$  
    & $q_{.}k_{string}$      
    & $q_{.}k_{.}$ \\ 
\hline
\end{tabular}
\caption{Matrix for calculating the self-attention "scores" for each token combination in the input sequence for Transformers. We use the query vector $\vec{Q}$ to "query" the "keys" $\vec{K}$ to see which tokens are the most relevant for the next token prediction. The matrix multiplication is calculated with $\vec{Q}\vec{K}^\intercal$.}
\label{tab:QK}
\end{table}

\begin{table}
\centering
\begin{tabular}{|l|l|l|l|l|}  
\hline
... & map  & ( & string & . \\ \hline
map    & 0.9 &&& \\ 
(      & 0.6 & 0.1 && \\
string & 0.1   & 0.1  &  0.7  & \\
.      &  0.2     & 0.1  &  0.2  &  0.4 \\ \hline
\end{tabular}
\caption{Example matrix for the numerical self-attention "scores" after taking the softmax over the normalized values in \tabref{tab:QK}. Note that the rows listed here do not sum up to exactly 1 since there are previous tokens in the input sequence (not shown in this matrix) that self-attention gives scores to as well.}
\label{tab:QK_softmax}
\end{table}


For other details, please refer to \citet{vaswani2017attention} (especially the multi-head attention part) and in particular, GPT-2~\citep{radford2019language-gpt2}, for a more thorough description.

The next sections discuss various ways of feeding code fragments into this Transformer architecture.






\subsection{\SrcSeq}
Our first attempt is to apply a Transformer over source token sequences.
As a baseline for later models that takes more tree information, as well as a straightforward application of Transformer models, we apply a Transformer (GPT-2) over source token sequences:
\begin{align*}
    \vec{o} = Trans(\vec{e_{t}}), t\in \mathit{source\_tokens}
\end{align*}

where $\vec{o}$ is the output of the Transformer to be used for prediction, and $\vec{e}$ represents the embedding of the source tokens.
It does next token prediction by taking \textit{all} preceding source tokens, up to the point of prediction, as input. As the inputs and outputs are the same as the \SrcRNN model (introduced in the next section), we can do a direct comparison between RNNs and Transformers.
As we show in the experiments, this turns out to be an already strong baseline.

The next two subsections discuss how to present the AST to the Transformer.

\subsection{\DFS}
One way to present all AST nodes to a Transformer is to linearize them in the using a pre-order traversal, or a depth-first-search (DFS). For Fig~\ref{fig:exampleast}, for node 29, the previous nodes in DFS order would be:
[..., ``Call'', ``NameLoad'', ``map'', ``AttributeLoad'', ``NameLoad'', ``string'', ``Attr'']

The \DFS model simply feeds this sequence to the Transformer:
\begin{align*}
    \vec{o} = Trans(\vec{e_{t}}), t\in \mathit{AST\_nodes}
\end{align*}
where $\vec{o}$ is the output of the Transformer to be used for prediction, and $\vec{e}$ represents the embedding of the AST nodes.
\DFS predicts the next node in the AST; thus, it does both value (leaf) prediction and type (internal) prediction. 

\DFS presents the tree nodes in a pre-determined order, but still does not retain detailed structural relationship between nodes.
For example, consider the sequence of nodes 26 - 28 in \figref{fig:exampleast}. This would be represented as ["NameLoad", "string", "attr"], the three nodes appearing consecutively in DFS order. Looking at the AST, we can see that the relations between ("NameLoad" \& "string", and "string" \& "attr") are actually quite different: "NameLoad" is one node up from "string", while "string" is two nodes up and one node down from "attr". This path-based relation between the nodes provides richer information about the actual structure of the tree.

While \DFS itself shows only a small improvement on \SrcSeq (Table~\ref{tab:resultspy150}), it allows us to augment it with the richer information indicated above, leading to the \TreeRel model.



\subsection{\TreeRel}
\label{sec:dfsud}

\TreeRel is an extension to the \DFS model that incorporates more tree structure.  Specifically, given any two nodes $a$ and $b$ in the AST, we want to capture the shortest path needed to reach from $a$ to $b$, and communicate this to the Transformer.  The
path from $a$ to $b$ is represented abstractly only in terms of up and down moves:
\begin{align*}
   \mathit{UDpath}(a, b) = U^i D^j
\end{align*}
where $i$, and $j$ are the number of up and down nodes, respectively, node $a$ has to travel to reach node $b$.\footnote{Code2vec~\citep{alon2019code2vec} used (embeddings of) leaf-to-leaf AST paths to capture information for the purpose of code summarization; by contrast, UD paths specifically retain information on how a pair of tree nodes are situated with respect to each other.
}  
We create a matrix $\mat{R}$ to contain $\mathit{UDpath}(a, b)$ for each pair of nodes $(a,b)$, where $a$ comes after $b$ in DFS order. \tabref{tab:QKR} (ignoring the qk parts inside the parenthesis) shows an example of the $\mat{R}$ in the context of our running example (nodes 25-29 in the AST).~\footnote{Node 24 was omitted due to space constraints for the table.} 

Notice that this matrix has the same shape (lower triangular matrix) as the $\mat{Q}\mat{K}$ matrix in \tabref{tab:QK}.  We add in the $\mat{R}$ matrix in the Attn block (after passing by an embedding layer): 
\begin{equation}
  \AttnTreeRel(\mat{Q},\mat{K},\mat{V},\mat{R}) = \softmax(\frac{\mat{R} \odot (\mat{Q} \mat{K}^\top)}{\sqrt{d_k}}) \mat{V}
\end{equation}
where $\odot$ is element-wise product. 

~\tabref{tab:QKR} shows an example of the new self-attention, $\mat{R} \odot (\mat{Q} \mat{K}^\top).$  One detail to note here that $\mat{R}(a, b) = \mathit{UDpath}(a + 1, b)$ since we want the path relations to be relative to the next token we are predicting.

The rest of the Transformer model is the same as \DFS's, with the updated $\AttnTreeRel$ calculation:
\begin{align*}
    \vec{o} = \mathit{Trans}_{ud}(\vec{e_{t}}, \mat{R}), t\in AST\_nodes
\end{align*}
where $\vec{o}$ is the output of the Transformer to be used for prediction, $\vec{e}$ represents the embedding of the AST nodes, and $\mat{R}$ represents the embedding of the $\mathit{UDpath}$ relations.

\paragraph{Why might adding $\mat{R}$ help the model do better?}
Note that $\mat{Q}\mat{K}^{\top}$ provides a way for the model to learn the strength of attention it needs to pay to previous tokens, organized in the order of inputs to the network (this order is implicit in the indices used in the matrix in \tabref{tab:QK_softmax}.)   $\mat{R}$ provides a way for the model to learn the strength of the attention to pay to previous tokens, \textit{considering the AST relationship between pairs of nodes} as well.

\bigskip

\textbf{To recap, our key insight is to fortify the self-attention mechanism of the Transformer to enable it to learn weights on the basis of AST relationships between tokens as well.}

\begin{table}
\begin{tabular}{|l|l|l|l|l|}  
\hline
& 25 & 26  & 27 & 28  \\ 
\hline
25
    & $U^1(q_{25}k_{25})$ &&& \\
26   
    & $U^2(q_{26}k_{25})$ 
    & $U^1(q_{26}k_{26})$ && \\ 
27   
    & $U^1(q_{27}k_{25})$ 
    & $U^1D^1(q_{27}k_{26})$ 
    & $U^1D^2(q_{27}k_{27})$ & \\
28
    & $U^2(q_{28}k_{25})$ 
    & $U^2D^1(q_{28}k_{26})$
    & $U^2D^2(q_{28}k_{27})$  
    & $U^1(q_{28}k_{28})$  \\
\hline
\end{tabular}
\caption{Matrix for calculating the self-attention "scores" for \TreeRel. Matrix $\vec{R}$, which contains the up down path information, is multiplied with $\vec{Q}\vec{K}^\intercal$, from the traditional Transformer. In this example, node 25 represents "AttributeLoad", 26 is "NameLoad", 27 is "string", and 28 is "Attr".}
\label{tab:QKR}
\end{table}




\begin{table*}[]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Models}   & \textbf{Model Type} & \textbf{Problem} & \textbf{Input}                   & \textbf{Prediction} \\  \hline
\textbf{Deep3}      & Decision Tree       & Value pred \& type pred & AST                              & AST nodes           \\ 
\textbf{\SrcRNN}   & RNN                 & Next token pred & Source code                      & Source code tokens  \\ 
\hline 
\textbf{\SrcSeq}   & Transformer        & Next token pred & Source code                      & Source code tokens  \\ 

\textbf{\DFS}      & Transformer        & Value pred \& type pred & AST                              & AST nodes           \\ 
\textbf{\TreeRel}  & Transformer        & Value pred \& type pred & AST + path relations             & AST nodes          \\ 
\hline 

\textbf{\RootPath} & Transformer         & Value pred & Leaf nodes + leaf to root paths & Leaf nodes         \\ 
\textbf{\LeafTokens}      & Transformer        & Value pred  & Leaf nodes & Leaf nodes           \\ 
\textbf{\TreeReli}  & Transformer        & Value pred \& type pred & AST + path relations             & AST nodes          \\ 


\hline
\end{tabular}
\caption{Overview of the models presented in this paper. The first two are models from previous work using RNN and Decision Tree, and remainder are models of our own creation that uses a Transformer (the last three are exploratory and variations). The models differ in the type of prediction task, and in what the model inputs and predicts.}
\label{tab:model_overview}
\end{table*}



\subsection{Variations of Models}
In this section, we discuss some alternate models and variations of models we have explored. 

\paragraph{\textbf{\RootPath}}
\RootPath is an AST-based model that feeds tree structure information to the model in an alternate way than \DFS does. \RootPath first creates a sequence based on the leaf nodes of the AST. To expose tree structure to the Transformer, it fortifies each leaf node with the path from the leaf node to the root of the AST by traversing up its ancestors; we call such a path to be \textit{root-path}.  For Fig~\ref{fig:exampleast}, for node 29, the root-path would be:

\noindent
[..., \\
([``NameLoad'', ``Call'', ... ``Module''], “map”), \\
([``NameLoad'', ``AttributeLoad'', ``Call'', ..., ``Module''], “string”), \\
([``Attr'', ``AttributeLoad'', ``Call'', ..., ''Module''], ?)\\
]

The root-paths are first fed into a sequence encoder (such as an LSTM), coupled with the leaf node, and is fed through the Transformer:
\begin{align*}
    \vec{o} = Trans(\vec{e_{t}} + \text{LSTM}(\mat{P}_{t})), t\in \mathit{leaf\_nodes}
\end{align*}
where $\vec{o}$ is the output of the Transformer to be used for prediction, and $\vec{e}$ represents the embedding of the leaf nodes, and $\mat{P}$ is the embedding for all the root-paths.  
Since \RootPath predicts only leaf nodes, it does only value prediction.




\paragraph{\textbf{\LeafTokens}}
\LeafTokens is a lightweight variation of \RootPath, where only the leaf nodes are used. For ~\figref{fig:exampleast}, for node 29, the input sequence would be: [..., "map", "string"], and would predict "atoi". \LeafTokens feeds in the leaf nodes of the AST into a Transformer:  
\begin{align*}
    \vec{o} = Trans(\vec{e_{t}}), t\in \mathit{leaf\_nodes}
\end{align*}
where $\vec{o}$ is the output of the Transformer to be used for prediction, and $\vec{e}$ represents the embedding of the leaf nodes. We compare this model with \RootPath to determine the importance of root-path information in next token prediction.

\paragraph{\textbf{\TreeReli}}
\TreeReli is a variation to \TreeRel that uses a richer vocabulary to the up-down paths to include some child index information, as it provides extra information about the tree structure. While \TreeRel uses only $U$ and $D$ to describe the relation between two nodes in the AST, \TreeReli expands $D$ into three sub words: $D_{\mathit{first}}$, $D_{\mathit{last}}$, $D_{\mathit{middle}}$; this describes whether the node is either the first child, the last child, or somewhere in in between, respectively. For example, in ~\tabref{tab:QKR}, the new relation for node 27 and 27 would expand from $U^1D^2$ into $U^1D_{\mathit{first}}D_{\mathit{first}}$. We chose this minimal extension to limit the possible exponential growth in path vocabulary size; even with this minor extension, our path vocabulary increases from 250 to 100k to cover more than 90~\% of the vocab (with a long right tail). The rest of the model is same as \TreeRel, as described in ~\secref{sec:dfsud}. We compare this model with \TreeRel to examine whether adding in more information (at the expense of enlarging the model) improves MRR.  

A high-level overview of the models is presented in \tabref{tab:model_overview}. The next section will cover two previous models from literature.
 
\section{Background on Previous Work}
\label{sec:previouswork}

In this section, we recap two different methods for code prediction, representative of recent previous work, against which we compare
our work.  These are (1) a method based on language models that uses a sequence of source code tokens, and (2) a  method based on decision trees~\cite{raychev2016probabilistic-deep3-eth-dt} that works on ASTs.

\subsection{Language Model based prediction}

A language model computes the probability of the next word $w_{t+1}$, given some window of preceding words:
\(
P(w_{t+1} | w_t w_{t-1} w_{t-2} \ldots)
\).  Here we use an RNN to compute a language model; n-grams would be another choice.\footnote{The jury seems to be out on which one is necessarily better for the task~\cite{hellendoorn2017are-deep-best, karampatsis2020big-bpe}.}

\begin{figure}
    \centering
    \includegraphics[scale=0.35]{fig/rnn}
    \caption{Schematic of an RNN. Here $\vec{h}$, $\vec{x}$ and $\vec{y}$ are vectors and $\mat{W_{\mathbf{hh}}}$, $\mat{W_{\mathbf{xh}}}$ and $\mat{W_{\mathbf{hy}}}$ are matrices.}
    \label{fig:rnn}
\end{figure}

Fig~\ref{fig:rnn} shows an Recurrent Neural Network (RNN) operating on some of the tokens from the example in Fig~\ref{fig:examplecode}.  As the name suggests, RNNs consume input tokens recurrently, one per time step, and produce output tokens one per time step as well.  The bottom layer of the RNN \emph{embeds} input tokens into a vector: $\vec{x_t} = \mathit{emb}(\mathit{{w}_t})$, where $w_t$ is the source token seen at the $t$'th time step.  The hidden state $\vec{h_t}$ is
computed as 
\( \vec{h_t} = \mat{W_{\mathbf{xh}}} \vec{x_t} + \mat{W_{\mathbf{hh}}} \vec{h_{t-1}} \), 
using both $\vec{x_t}$ and the hidden state from the previous time step. The output is a vector of probabilities of various tokens computed by using softmax over \( \vec{y_{t}} = \mat{W_{\mathbf{hy}}}\vec{h_t} \); the diagram shows the top-ranked predictions or the ground truth.  $\mat{W_{\mathbf{hh}}}$, $\mat{W_{\mathbf{xh}}}$ and $\mat{W_{\mathbf{hy}}}$ are the parameters of the network, to be learned during training.

The pertinent point to note is that the hidden state $\vec{h_t}$ encodes the knowledge of not just the current token, but of last several previous tokens via the propagation of information in previous hidden states.  Thus, RNNs implicitly compute a language model over tokens.

A limitation of RNNs is the difficulty they have in tracking long-range dependence,  
even with various proposals to mitigate the problem (e.g. long-short-term-memory (LSTM) cells~\citep{hochreiter1997long-lstm}, which we do use in our implementation, attention on top of RNNs~\citep{iyer2016summarizing-lstm-attn}, and skip-connections between sequence locations~\citep{vinyals2015pointer}). 

In our experiments, we feed the source code tokens into an RNN and call this model \SrcRNN.


\subsection{Decision Tree based prediction}

Raychev et al.~\cite{raychev2016probabilistic-deep3-eth-dt}
presented a system, Deep3, based on a learned decision tree combined with count-based probabilities at the leaves of the decision tree.  We provide only a sketch here, highlighting how they use \emph{paths} on an AST.






\begin{figure}
\lstset{basicstyle=\footnotesize\ttfamily}
\lstset{morekeywords={Up, DownFirst, PrevDFS}}
\begin{lstlisting}
switch (Up WriteValue) {
  case Attr: switch (Up Up WriteValue) {
     case AttributeLoad: 
        switch (Up Up DownFirst WriteValue) {
           case NameLoad:
              Up PrevDFS WriteValue
           default: ...
        }
...
\end{lstlisting}
    \caption{Fragment of a TGEN program encoding a decision tree. The bold words are steps that comprise a path in a given AST.}
    \label{fig:deep3}
\end{figure}

Fig~\ref{fig:deep3} shows part of a learned decision tree, written in the form of program in a specialized language they call TGEN.  Given an AST $t$ and a starting node $n$, a TGEN program walks certain paths in $t$ starting from $n$. For example, \texttt{Up WriteValue} (line 1) goes to the parent of $n$ and records the label. If the label is \texttt{Attr}, it walks a different path (line 2) in the vicinity of $n$.  The branch outcomes and observations collected by running this TGEN program on $(t,n)$ form a \emph{context}, which is then used to look up a probability distribution conditioned on that context. For the AST in Fig~\ref{fig:exampleast}, starting with node 29, the TGEN program will produce a context for which the probabilities of different tokens for node 29 might be: [atoi: 40\%, length: 20\%, ...].  The flexibility of focusing on arbitrary paths in the AST allows the model to condition selectively on nodes farther back in the AST.

A TGEN program is learned---on a specific corpus---by a genetic search procedure that simultaneously selects paths and grows the decision tree from the training data, with an entropy minimization objective. The details are not important for this paper;
in this paper, we use their pretrained model~\cite{phog} as well as their Python dataset~\cite{py150} for our experiments.


The reader will notice that the notion of $\mathit{UDpath}$ in Section~\ref{sec:dfsud} is  akin to the AST paths expressed in TGEN programs. The paths in TGEN are more general, but at a high-level, the idea that certain "spatial" relation between nodes is important is common to both approaches. This, along with the competitive quality of results of the Deep3 model in~\tabref{tab:results_example}, makes it an interesting comparison. We explore this similarity further in Appendix \ref{sec:inspect-attn-heads}.






 
\section{Implementation and Datasets}
\label{sec:implementation}

\subsection{Dataset}
We train our models using the py150 dataset 
\cite{py150} used in \citet{raychev2016probabilistic-deep3-eth-dt}.
The dataset consists of 150k Python 2 source code files from GitHub repositories, along with their parsed ASTs, split into 100k for training and 50k for evaluation.
From the ASTs extracted from the py150 dataset, we modify the AST to ensure that the internal nodes only has types and the leaf nodes only have values. For implementation details, please refer to Appendix\ref{sec:modifyast}. To incorporate large trees (greater than 1000 nodes), we deploy a technique adopted by \cite{alrfou2018characterlevel}, which slices a large tree into shorter segments with a sliding window to maintain part of the previous context. For implementation details, please refer to Appendix\ref{sec:splittrees}. 

We evaluate our models on two evaluation datasets:
\begin{itemize}
    \item \textbf{py150}: We use the evaluation dataset used in \citet{raychev2016probabilistic-deep3-eth-dt}, which consists of 50k Python ASTs. We perform the two modifications as listed above before feeding them into our models, there are 16,003,628 leaf nodes and 30,417,894 internal nodes.
    \item \textbf{internal}: We also created an evaluation dataset consisting of 5000 Python files from a code repository internal to Facebook. With this dataset, we can evaluate how our trained model can generalize to a different dataset, even if the code comes from disjoint projects. After the modifications, there are 1,669,085 leaf nodes and 3,067,147 internal nodes. 
\end{itemize}
Recent works~\citep{karampatsis2020big-bpe,hellendoorn2017are-deep-best} have divided evaluations into \textit{static} and \textit{dynamic}, where in the dynamic evaluations, the model continues to update its parameters during evaluation. This may increase accuracy by having the model adapt to the characteristics of the evaluation dataset. In our experiments, we choose to evaluate statically, and realize that evaluating dynamically may improve accuracy.


\subsection{Implementation}

\paragraph{Transformers}. For the model that use Transformers (\RootPath, \DFS, \SrcSeq, \TreeRel), we adapt the Pytorch implementation~\footnote{\url{https://github.com/graykode/gpt-2-Pytorch}. We do not use positional encoding. Refer to Appendix~\ref{sec:posenc} for the explanation.}  of GPT-2 small~\citep{radford2019language-gpt2}.
We use six Transformer blocks, six heads in each block, $n_ctx = 1000$, and set embedding dimension $d_\model = d_k = d_q = 300$. We borrow other hyperparameters from \citet{radford2019language-gpt2}.
We limit the token vocabulary size to 100k, which covers over 90\%~ of the tokens used in the training dataset.
For \TreeRel, we limit the vocabulary to 250, which covers over 95\%~ of the path relations. For \RootPath, we limit the maximum length of the path from leaf node to root to be 13, which covers over 90\%~ of the nodes. For any path longer than 13, we keep the nodes closest to the leaf, and truncate the nodes near the root.

\paragraph{RNN} For the \SrcRNN model, we adapt the PyTorch example implementation~\footnote{\url{https://github.com/pytorch/examples/tree/master/word_language_model}} of a word-level language model LSTM. We use embedding dimension $d_\model = 300$, with $dropout = 0.5$ and $n\_layers = 1$. We limit the token vocabulary size to 100K, which covers over 90\%~ of the tokens.

\paragraph{Deep3} For the Deep3 model, since the authors have shared only the model and not the training algorithm, we used the model pretrained on py150.

We trained all models (except Deep3) on Nvidia Tesla V100 (using 4 GPUs at a time) until the loss converged, with all of the parameters randomly initialized. We used the Adam optimizer with the learning rate set to 1e-3. For convergence, \DFS took 11 epochs, \TreeRel took 21 epochs, \SrcSeq took 9 epochs, and \SrcRNN took 9 epochs (each epoch took around 45 minutes - 1 hour). 

\subsection{Evaluation Task}
We evaluate the models on the code prediction tasks that we defined in Sec~\ref{sec:problem}: next token prediction, which pertains to source code tokens taken as a linear sequence; value prediction, which pertains to predicting leaf nodes of the AST; and type prediction, which pertains to predicting internal nodes of the AST.

To measure performance on these tasks, we use mean reciprocal rank (MRR). 
The rank is defined as 
\begin{equation}
    MRR = \frac{1}{n} \sum_{i = 1}^{n} \frac{1}{rank_i}
\end{equation}
where $n$ is the number of predicting locations and $rank_i$ is the rank of the correct label given by the model for the $i^{th}$ data point. We present MRR as a percentage, in keeping with prior work~\citep{karampatsis2020big-bpe,hellendoorn2017are-deep-best}.

While Acc@1 only gives score when the correct label is ranked at the top,
MRR also give scores when the true label is not ranked as the top, but among top few prediction. 
Comparing to the hit-or-miss style metric (Acc@1), this is closer to the realistic scenario when completion suggestions are presented to developers.
With this practical perspective and for ease of computation, we only consider $rank_i \le 10$ for each  location $i$ (all $rank_i > 10$ will have a score of 0).  

We share our data processing scripts and model implementations at \url{https://github.com/facebookresearch/code-prediction-transformer}. 
 
\section{Evaluation}
\label{sec:evaluation}

\subsection{Research Questions}

At a high level, we want to answer the following research questions.

\begin{enumerate}[label=RQ\arabic*]
    \item \textbf{Overall, do Transformer-based models provide better accuracy compared to prior state-of-the-art methods of code prediction?}

    \item \textbf{Does syntactic structure of code help get better accuracy out of Transformers, and if so, by how much?}

    \item  \textbf{What did the Transformer model variants learn from the code? Did they learn the right things? What can we learn from the learned models?}

\end{enumerate}

We describe the experiments to answer the research questions RQ1 and RQ2. We discuss the evaluation of RQ3 in Section~\ref{sec:inspection}.

For RQ1, recall that prior work (Section~\ref{sec:problem}) works on two different kinds of inputs: all source tokens as in program text, and ASTs of each program unit.
To carry out a direct comparison against prior work, we split RQ1 into two specific questions:

\begin{enumerate}[label=RQ1.\arabic*]
\item \textbf{Is the Transformer-based model more accurate than the RNN-based model on the \textit{next token prediction} problem? (~\secref{sec:problem})?}  

To answer this question, we compare \SrcRNN model against the \SrcSeq model on the source tokens.

\item \textbf{Are the Transformer-based models more accurate than Deep3, on the \textit{value prediction} and on the \textit{type prediction} problems (Sec~\ref{sec:problem})?}

To answer this question, we compare Deep3 model against the \DFS variant of the Transformer on the ASTs variants: \DFS, \TreeReli, \TreeRel, and \RootPath, 

\end{enumerate}

For RQ2, we ask two sub-questions:

\begin{enumerate}[label=RQ2.\arabic*]
    
    \item \textbf{Does a Transformer model based on an AST outperform a Transformer model that takes the corresponding source token sequences?}

    This question can be answered directly only on tokens that appear both in ASTs and source token sequences: these are precisely the values at the leaf nodes of the AST.  We compare \SrcSeq and \DFS models on the \textit{terminal value prediction} problem.

    \item \textbf{Does providing more detailed structural information help with accuracy?}

    To answer this question, we compare among the tree-based Transformer models (\DFS, \TreeReli, \TreeRel, and \RootPath) on the \textit{terminal value prediction} and on the \textit{internal/type prediction} problems.

\end{enumerate}
\subsection{Results}

\begin{table*}[]
    \centering
    \begin{tabular}{l|cc|ccc}
    \hline
                     & \multicolumn{2}{c}{\textbf{Prior work}} & \multicolumn{3}{c}{\textbf{Our work}} \\
    \hline
      \textbf{Applications}               & SrcRNN & Deep3 & \SrcSeq  & \DFS & \TreeRel  \\
    \hline
  Next token prediction    & 65.7 (58.0)  & n/a   & \textbf{74.1 (68.1)}  & n/a   & n/a             \\
  Value prediction   & 36.4. (29.1)  & 43.9 (40.5)  & 50.1 (43.4)   & 58.0 (52.4) & \textbf{73.6 (71.0)}  \\
  Type prediction     & n/a   & 81.9 (75.8)  & n/a           & 89.3 (82.7)  & \textbf{98.7 (97.6)}  \\
    \hline
    \end{tabular}
    \caption{MRR and Acc@1 (in parenthesis) of various prediction tasks for py150.}
    \label{tab:resultspy150}
\end{table*}


\begin{table*}[]
    \centering
    \begin{tabular}{l|cc|ccc}
    \hline
    & \multicolumn{2}{c}{\textbf{Prior work}} & \multicolumn{3}{c}{\textbf{Our work}} \\
    \hline
    \textbf{Applications} & SrcRNN & Deep3 & \SrcSeq & \DFS        & \TreeRel         \\
\hline
  Next token prediction    & 57.4 (48.3)  & n/a    & \textbf{66.8 (60.2)} & n/a    & n/a     \\
  Value prediction   & 23.8 (17.7)  & 36.1 (33.3)   & 36.5 (30.7)  & 43.9 (38.8)  & \textbf{58.4 (55.3)}   \\
  Type prediction     & n/a   & 79.9 (73.1)   & n/a      & 87.7 (80.2)  & \textbf{98.0 (96.3)}   \\
    \hline
    \end{tabular}
    \caption{MRR and Acc@1 (in parenthesis) of various prediction tasks for the internal dataset.}
    \label{tab:resultsint}
\end{table*}


\begin{table*}[]
    \centering
    \begin{tabular}{l|c|ccc}
    \hline
    \textbf{Applications} & \TreeRel & RootPath & LeafTokens & \TreeReli \\
    \hline
    Value prediction      & \textbf{73.6 (71.0)} & 55.1 (48.4) & 41.9 (34.1) & 73.3 (70.8) \\
    Type prediction       & \textbf{98.7 (97.6)} & n/a         & n/a         & 97.8 (96.1) \\
    \hline
    \end{tabular}
    \caption{MRR and Acc@1 (in parenthesis) of the alternate models and variations of models for py150, compared against the best performing model, \TreeRel.}
    \label{tab:dfs_alternate}
\end{table*}

Our main evaluation results are reported in Table~\ref{tab:resultspy150} and Table~\ref{tab:resultsint}.




\paragraph{Q1}

For RQ1.1, see the \SrcRNN and \SrcSeq columns in Table~\ref{tab:resultspy150} and Table~\ref{tab:resultsint}. For the py150 dataset, we can see a significant improvement in MRR, from 65.7\% to 74.1\% for the \SrcRNN and \SrcSeq models, respectively. The same holds for comparing on the internal dataset: 57.4\% vs 66.8\%. (Table~\ref{tab:resultspy150_values} and ~\ref{tab:resultsint_values} in the Appendix~\ref{sec:extra-eval-results} break down the data for different kinds of next token predictions.) Not surprisingly, \tabref{tab:resultspy150} also shows that predicting the identifier and constant tokens (as in value prediction) is more challenging than predicting the keywords and punctuation tokens, which form almost 2/3 of all the source tokens.

For RQ1.2, we compare the Deep3 model against \DFS and \TreeRel models. 
Overall, we found that all the Transformer models (\SrcSeq, \DFS \TreeRel) achieve higher scores compared to Deep3. Table~\ref{tab:resultspy150} shows that \TreeRel achieves the best MRR of 73.6 for leaf node prediction compared with Deep3's MRR of 43.9.  
Similar results can be seen for the internal dataset, as shown in Table~\ref{tab:resultsint}.


\paragraph{Q2}
To answer RQ2.1, we compare the value prediction results for \SrcSeq against the AST-based models (\DFS, \TreeRel). Table~\ref{tab:resultspy150} shows that \DFS outperforms \SrcSeq by 7.9\%, and \TreeRel significantly outperforms \SrcSeq by 23.5\% (73.6\% vs 50.1\%). These results demonstrate that representing the source code as AST vs linearized source code provides better results for next value prediction.

For RQ2.2, we compare the results amongst the AST-based models. 
First, comparing \DFS and \TreeRel, \TreeRel provides more detailed structural information. \tabref{tab:resultspy150} shows significant improvements to the accuracy, achieving 15.6\% higher MRR for value prediction and 9.4\% higher MRR for type prediction than \DFS. Similar trends can be seen for the internal data set in \tabref{tab:resultsint}. 

~\tabref{tab:dfs_alternate} shows a significant drop in accuracy between \RootPath and \LeafTokens (55.1\% vs 41.9\% for all leaf nodes). This shows that the information captured by the leaf to root paths (both in terms of its values and tree structural information) gives a solid boost to accuracy. These results demonstrate that feeding the model with more structural information does improve results.

Next, we compare \RootPath and \DFS. These models are similar because both models take all of the AST nodes as the context, but are different in how they digest the context. \RootPath first aggregates the context information for each leaf node before predicting the next leaf node, while \DFS captures both leaf and internal nodes in one context. Results show that performance between the two models are pretty comparable (58.0\% vs 55.1\% for value prediction in Tables \ref{tab:resultspy150} and \ref{tab:dfs_alternate}). One drawback of \RootPath is that it can only predict leaf nodes, while \DFS  can predict all nodes in the AST, including internal nodes for type prediction.

~\tabref{tab:dfs_alternate} shows that \TreeReli did not outperform \TreeRel, which shows that simply expanding the up-down vocab may not be the right approach in exposing child index information to the model. Areas of explorations may include whether a vocabulary size of 100k is too sparse for the models to learn effectively, or whether child indices are inherently not as crucial for code prediction.
 
\subsection{Threats to Validity}
\label{sec:threats}

\paragraph{SrcRNN Implementation}
Our SrcRNN implementation is based on a PyTorch implementation\footnote{ \url{https://github.com/pytorch/examples/blob/master/word\_language\_model/model.py}} whereas related papers have generally built off of a Tensorflow implementation.\footnote{\url{https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb\_word\_lm.py}}  As the hyperparameters were similar ($dropout = 0.5$, $num\_layers = 1$, $hidden\_size = 512 vs 300$) to recent publications, we do expect our implementation to be comparable.  

\paragraph{BPE}
We have not integrated byte-pair encoding (BPE)~\cite{karampatsis2020big-bpe} into our RNN model. We expect BPE to benefit both RNN and transformer models, and plan to explore this in future work.

\paragraph{Training Corpus}
While larger Python corpora have appeared,  py150 is still sizable at \~ 500MB; we do not expect the larger corpora to reverse our findings.

\paragraph{Python specificity}
We have only carried out evaluations on Python, and have not demonstrated that our results would carry over (in trends) to other languages.  The Deep3 paper did find their results (in trends) to roughly carry over from Python to Javascript.



 
\subsection{Model Inspection}
\label{sec:inspection}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{fig/dfs_inspection}
        \caption{\DFS}
        \label{fig:dfs-pred-influence}
    \end{subfigure}
    \\
\begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\textwidth]{fig/dfs_ud_inspection}
        \caption{\TreeRel}
        \label{fig:dfs-ud-pred-influence}
    \end{subfigure}
    \caption{Influence of previous nodes in value prediction of the example in ~\figref{fig:exampleast} by \DFS and \TreeRel.
    $x$-axis is labeled with the input values. 
    $y$-axis is labeled with the values to be predicted. 
    Color indicates the model's prediction is \textcolor{green}{correct} or \textcolor{red}{wrong}.}
    \label{fig:pred-influence}
\end{figure}

In this part, we study the influence of each input features to shed light on the black box of how our models make their predictions. 
Particularly, we study how each input token attributes to the models' predictions (attribute analysis, this section) and which $\mathit{UDpath}$s are learned to be important by \TreeRel (Appendix~\ref{sec:inspect-attn-heads}).
For the latter, we found that local syntactical context is generally important and similarities exist compared to the heavily utilized Deep3 TGEN paths.


We use saliency maps~\cite{simonyan2013deep-saliency} for attribute analysis,
which are constructed by taking the partial derivative of the loss function with respect to the inputs.
\figref{fig:pred-influence} visualizes the magnitudes of the gradients falls at each input token when the model predicts a particular output.
Intuitively, the larger the value for a particular token, the more sensitive the output is to the variations at that token. 

Examining the saliency maps for \DFS and \TreeRel, we first observe that that parent node of the AST (the internal node right above the leaf) is generally important for both models.
From \figref{fig:dfs-ud-pred-influence}, we can see \TreeRel is influenced 
by \code{string} when predicting \code{atoi} 
and by \code{request\_size} when predicting \code{num\_requests}.
It is not shown in the figure but when predicting \code{2}, \TreeRel is influenced by the previous occurrence of \code{sys.argv} indexed by \code{0} and \code{1}.
Looking at the differences between \figref{fig:dfs-pred-influence} and \figref{fig:dfs-ud-pred-influence}, we found that \TreeRel is influenced by \code{ip} while predicting \code{gethostbyname} correctly but \DFS is not while predicting it wrong.
Generally, we found that \TreeRel attributes more towards terminal values relevant to the values to be predicted, while \DFS attributes little to values other than non-terminals.
This provides an evidence that \TreeRel is more likely to have learned the right features for next value prediction.

On an orthogonal note, we also observe that for many predicting locations, the magnitude of gradients are very small,
suggesting the robustness of the model in the sense that it is less sensitive to minor perturbations of the input sequence.

 
\section{Related Work}
\label{sec:related-works}

Due to the vastness of the topic, we focus on two themes of related work.
\subsection{Statistical Code Completion}
Simply put, the task of code completion is to predict the rest of the code a user is typing. 
Code completion is widely used by commercial or free integrated development environments (IDEs)
~\footnote{\url{https://code.visualstudio.com/docs/editor/intellisense}}
~\footnote{\url{https://www.jetbrains.com/help/idea/auto-completing-code.html}} ~\footnote{\url{https://flight-manual.atom.io/using-atom/sections/autocomplete/}}
to accelerate or ease the process of developing software.

Since \citet{hindle2016naturalness}, there have been the rise of statistical learning for the task of code completion, exploiting \emph{naturalness} of code~\cite{allamanis2018survey}.
Learning methods used starting from 
n-gram~\cite{nguyen2013statistical-ngram,hindle2016naturalness} 
to probabilistic grammar~\cite{allamanis2014mining,bielik2016phog} 
and decision trees~\cite{raychev2016probabilistic-deep3-eth-dt}.
Recently there have been increasing application of deep learning to code completion, especially 
\emph{recurrent neural networks}~\citep{liu2016neural-code-completion,li2018code-rnn-attn,liu2020modeling-stack-lstm} 
and \emph{graph neural networks}~\citep{allamanis2018learning-graph,brockschmidt2018generative-graph,yang2019improve}.

Among other flavors of code completion, such as
where program after the predicting location is available~\citep{raychev2014code-api,allamanis2018learning-graph,brockschmidt2018generative-graph,alon2020structural-anygen}
or where the granularity of prediction is smaller (e.g. characters~\citep{bielik2016program-character} or subtokens~\citep{karampatsis2020big-bpe})
or larger (e.g. sub-ASTs~\citep{alon2020structural-anygen}),
we focus on predicting next token given only partial program up to the predicting location.

PHOG~\citep{bielik2016phog}, DeepSyn~\citep{raychev2016learning-noisy} and Deep3~\citep{raychev2016probabilistic-deep3-eth-dt} are particularly related as all of them utilize AST information for code completion.
PHOG and DeepSyn uses a conditional probabilistic context-aware grammar based on AST walks.
Deep3 further enriched the probabilistic model with a decision tree to allow more fine-grained modeling of context-dependent code occurrences.  

However, these probabilistic models have been surpassed by deep neural networks, namely LSTMs over serialized ASTs~\citep{liu2016neural-code-completion}.
Accuracy can be further improved by stacking attention and pointer-network over an LSTM~\citep{li2018code-rnn-attn}
or by augmenting LSTMs with stacks for which the operations are guided by the AST structure~\citep{liu2020modeling-stack-lstm}.


\subsection{Transformers}
Transformers, popularized by \citet{vaswani2017attention}, are sequence-to-sequence (seq2seq) neural networks based-on layers of multi-head self-attentions.
Surpassing RNNs, Transformer models~\citep{devlin2018bert,dong2019unified-unilm,radford2019language-gpt2} have become the state-of-the-art natural language models, breaking records for a range of NLP tasks, including sentence entailment, question answering and language modeling.
See \secref{sec:models} for a more thorough introduction to Transformers.

There have been reported applications of Transformer models for code completion.
Galois~\footnote{\url{https://github.com/iedmrc/galois-autocompleter}} is an open source project that uses GPT-2~\citep{radford2019language-gpt2} for code completion.
The approach is similar to our \SrcSeq model, despite their use of non-standard tokenizer and a subtoken segmenter.
TabNine\texttrademark~published a blog post~\citep{tabnine2019autocompletion} in July 2019 mentioning the use of GPT-2 in their code completion but revealed no technical detail.
To this point, we found no formal investigation up to this date on using transformers for the task of code completion. 

There has been a surge of interest from 2019 in extending Transformer models to handle beyond sequential structures, for NLP~\citep{wang2019tree-mask,ahmed2019you-traverse,nguyen2020treestructured-hierarchical} and for learning source code~\citep{harer2019tree-correction,shiv2019novel}.
\citet{wang2019tree-mask} put constraints on self-attentions to induce tree structures. 
\citet{ahmed2019you-traverse}, \citet{harer2019tree-correction} and \citet{nguyen2020treestructured-hierarchical} modify the attention block to mix node representations according to tree structures.
\citet{shiv2019novel} proposed a tree-induced positional encoding. 
As for learning source code, it has been showed that taking tree structured helped code correction~\citep{harer2019tree-correction} and code translation~\citep{shiv2019novel}.



 
\section{Future Work}
\label{sec:future-work}

\paragraph{Handling Out-of-Vocabulary Words}
Source code presents a difficulty shared with natural language processing in handling large vocabularies and rare words. The token/word to be predicted in test data may not appear in the training data. This is even more challenging when predicting identifiers, such as method names, variable names, and so on, as developers can come up with arbitrary identifier names. Possible mitigation includes copying mechanism~\cite{allamanis2016convolutional,brockschmidt2018generative-graph,fernandes2018structured} and open-vocabulary models~\cite{cvitkovic2019open,karampatsis2020big-bpe}.

\paragraph{Exposing Tree Structure even more completely}
We saw significant improvement in performance by providing more tree structure (\DFS vs \TreeRel). Our attempt at \TreeReli, a variation to \TreeRel that enhances the path relation vocabulary, did not improve performance.  This leaves open the possibility that our way of representing AST paths needs to be improved.



\paragraph{Using Semantic Information} Recent work has also shown the promise of using easy-to-compute static analysis information, such as def-use information.  While it is harder to get such info for dynamic languages, it is still an interesting question as to how to communicate those to transformers, and compare it to graph neural networks~\citep{li2016gated,allamanis2018learning-graph} that do use it.
 



\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\appendix
\newpage
\section{Implementation Details}
\label{sec:implementation-details}

\subsection{Modifying the AST}
\label{sec:modifyast}
For the AST, we want the internal AST nodes to only have type information, and the leaf nodes to have value information. This way, our model can predict one information given a node (instead of both type and value). However, in the py150 dataset, there are internal and leaf nodes with both type and value information. To accomodate for this, we slightly modify the trees to fit our definition of ASTs. For nodes with both type and value information, we take the value information, and create a new node (now a leaf node) as the node's first child. Fig~\ref{fig:new_ast} illustrates an example of the modification. This increases the average number of nodes in a tree from 623.4 to 951.9.

\subsection{Splitting Large Trees}
\label{sec:splittrees}
For neural network models, we need to set a maximum number of nodes in the tree that the model can take as input. Ideally, we would want to set the maximum to be high enough to take in any tree of any length; however, in practice, this is infeasible due to memory constraints (and the number of nodes could be infinitely large hypothetically.) We choose the maximum number of context (number of nodes) to be 1000, inspired by the maximum number of context set by GPT2 models and as this covers > 70\% of the training data. For trees with number of nodes greater than 1000, we deploy a technique adopted by \cite{alrfou2018characterlevel}. Given a large tree, we slice it into shorter segments with a sliding window (in our implementation, we used 500, which is half the context). For example, if a tree has 1700 nodes, we would have 3 new shorter trees: from nodes 0-999, nodes 500-1499, and 699-1699. For the last two trees, we would take loss and evaluate only on the nodes that the model has not seen before (1000-1499 and 1500-1699, respectively). In this way, we provide each subsequent shorter segment with some previous context, while increasing the number of training and testing datapoints at a reasonable amount (in our datasets, it doubled the number). An improvement to this sliding window technique would be to maintain the hidden states at each segment to pass along more context information, as explained in \cite{dai2019transformerxl}. 

\subsection{Why not Positional Encoding?}
\label{sec:posenc}
Some Transformers uses positional encoding~\citep{vaswani2017attention} or positional embedding~\citep{radford2019language-gpt2} to provide model extra positional information over elements. However, our early trials with \LeafSeq suggested positional embedding is rather hurting than helping. Thus, we do not use positional encoding or embedding for all our models.
Recently, \citet{shiv2019novel} tried to introduce tree structures to Transformer models via positional encoding. However, their relative improvement is small compared to what we see with tree-relational prior in Section~\ref{sec:evaluation}.


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{fig/xform.pdf}
    \caption{Example AST and our modification to allow nodes to have either only value or type information.}
    \label{fig:new_ast}
\end{figure}


\section{Extra Results}
\subsection{Extra Evaluation Results}
\label{sec:extra-eval-results}
\tabref{tab:resultspy150_values} and \tabref{tab:resultspy150_types} show respectively the breakdown results for terminal and non-terminal value prediction at various type of locations over py150.

\tabref{tab:resultsint_values} and \tabref{tab:resultsint_types} show respectively the breakdown results for terminal and non-terminal value prediction at various type of locations over internal dataset.


\begin{table*}[h]
    \centering
    \begin{tabular}{l|cc|ccc}
    \hline
                     & \multicolumn{2}{c}{\textbf{Prior work}} & \multicolumn{3}{c}{\textbf{Our work}} \\
    \hline
      \textbf{Applications} & SrcRNN  & Deep3  & \SrcSeq  & \DFS   & \TreeRel         \\
    \hline
      Attribute access        & 39.3 (31.6) & 45.3 (41.7) & 55.9 (49.0) & 60.5 (54.4) & \textbf{75.6 (73.3)}  \\
      Numeric constant        & 40.6 (29.3) & 53.2 (46.4) & 55.9 (45.7)	& 63.5 (53.7) &	\textbf{83.1 (79.0)}  \\
      Name (variable, module) & 38.2 (29.6) & 48.9 (45.4) & 54.1 (46.5)	& 66.6 (61.0) &	\textbf{79.8 (77.4)}  \\
      Function parameter name & 57.7 (54.0)  & 58.1 (56.6) & 66.2 (62.8)	& 67.2 (63.6) & \textbf{87.1 (84.7)}  \\
      All values               & 36.6 (29.1)  & 43.9 (40.5) & 50.1 (43.4)	& 58.0 (52.4) &	\textbf{98.7 (97.6)} \\
    \hline
    \end{tabular}
    \caption{MRR and Acc@1 (in parenthesis) of various types of value predictions for py150.}
    \label{tab:resultspy150_values}
\end{table*}


\begin{table*}[h]
    \centering
    \begin{tabular}{l|c|cc}
    \hline
                     & \textbf{Prior work} & \multicolumn{2}{c}{\textbf{Our work}} \\
      \textbf{Applications}   & Deep3 & \DFS & \TreeRel  \\
    \hline
      Function call    & 81.6 (74.2) & 88.5 (81.0)	& \textbf{98.7 (97.5)}   \\
      Assignment       & 76.5 (66.7) & 78.9 (64.3)	& \textbf{98.7 (97.5)}   \\
      Return           & 52.8 (40.8) & 67.8 (51.8)	& \textbf{97.8 (95.9)}   \\
      List             & 59.4 (54.2) & 76.0 (65.8)	& \textbf{97.1 (94.7)}   \\
      Dictionary       & 66.3 (61.0) & 15.0 (9.0)	& \textbf{83.8 (74.3)}   \\
      Raise            & 35.0 (27.1) & 63.3 (47.6)	& \textbf{97.0 (94.6)}   \\
      All types        & 81.9 (75.8) & 87.3 (79.6)	& \textbf{98.7 (97.6)}   \\
    \hline
    \end{tabular}
    \caption{MRR and Acc@1 (in parenthesis) of various type predictions for py150.}
    \label{tab:resultspy150_types}
\end{table*}


\begin{table*}[h]
    \centering
    \begin{tabular}{l|cc|ccc}
    \hline
                     & \multicolumn{2}{c}{\textbf{Prior work}} & \multicolumn{3}{c}{\textbf{Our work}} \\
      \textbf{Applications}           & SrcRNN & Deep3 & \SrcSeq & \DFS & \TreeRel    \\
    \hline
      Attribute access        & 26.4 (20.9) & 38.5 (36.0) & 41.0 (35.5)	& 44.7 (39.9) & \textbf{59.3 (56.7)}    \\
      Numeric constant        & 32.2 (20.3) & 46.5 (38.2) & 51.7 (40.5)	& 61.5 (50.4) &	\textbf{84.0 (78.6)}    \\
      Name (variable, module) & 25.0 (17.8) & 41.0 (38.2) & 39.3 (32.7)	& 50.7 (45.6) & \textbf{62.8 (60.1)}   \\
      Function parameter name & 45.5 (42.8) & 50.6 (49.0) & 54.3 (51.7)	& 53.3 (49.6) & \textbf{73.7 (70.7)}    \\
      All values              & 23.8 (17.7) & 36.1 (33.3) & 36.5 (30.7) & 43.9 (38.8) & \textbf{58.4 (55.3)}   \\
    \hline
    \end{tabular}
    \caption{MRR and Acc@1 (in parenthesis) of various types of next token value prediction for internal dataset.}
    \label{tab:resultsint_values}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{l|c|cc}
    \hline
                     & \textbf{Prior work} & \multicolumn{2}{c}{\textbf{Our work}} \\
      \textbf{Applications}   & Deep3 & \DFS & \TreeRel  \\
    \hline
      Function call    & 78.2 (70.3) & 86.0 (77.1)	& \textbf{97.8 (95.9)}   \\
      Assignment       & 78.5 (69.1) & 79.7 (65.8)	& \textbf{98.7 (97.4)}   \\
      Return           & 59.9 (47.8) & 72.2 (58.3)	& \textbf{97.6 (95.5)}   \\
      List             & 40.8 (33.9) & 63.1 (48.7)	& \textbf{94.3 (89.6)}   \\
      Dictionary       & 39.8 (31.2) & 23.5 (16.7)	& \textbf{81.0 (70.4)}   \\
      Raise            & 33.5 (25.8) & 59.3 (41.7)	& \textbf{96.4 (93.5)}   \\
      All types        & 79.9 (73.1) & 87.7 (80.2)	& \textbf{98.0 (96.3)}  \\
    \hline
    \end{tabular}
    \caption{MRR and Acc@1 (in parenthesis) of various types of next token type prediction for internal dataset.}
    \label{tab:resultsint_types}
\end{table*}

\subsection{Inspecting Attention Heads}
\label{sec:inspect-attn-heads}
\TreeRel learns weights for various $\mathit{UDpath}$s between a node and other nodes in its context as a component of self-attention.
In this part, we inspect the learned weights for $\mathit{UDpath}$s in the \TreeRel model in order to understand which $\mathit{UDpath}$s are the most important for the model's prediction.

There are six attention layers and six attention heads within each layer in \TreeRel.
All of them collectively determine the importance of each previous node in the prediction of the next token. We look into the maximally and minimally weighted $\mathit{UDpath}$s at each attention head. The results are shown in \figref{fig:ud-headwise}.
Presumably, the extreme-weighted $\mathit{UDpath}$s are the most salient features for the model's prediction.
The more extreme the weight is, the more conspicuous the path is among other paths for the particular head.

For example, we found that $U^1$, $U^1 D^1$, $U^1 D^2$, $U^2$ and $U^2 D^2$ are important across multiple heads.
$U^1$, $U^1 D^1$ and $U^{12} D^1$ are particularly up-weighted by some heads;
while $U^1 D^1$, $U^1 D^2$, $U^6 D^{17}$ and $U^1$ are particularly down-weighted by some heads.
The frequent presences of $U^1$, $U^1 D^1$ and $U^1 D^2$ suggest the importance of syntactical local context in the next value prediction. 
The extreme weights of very long paths, e.g. $U^{12} D^1$, is at first baffling.
However, we found cases where they can be useful in, for example, referring 
to class names ($U^{12} D^1$ in \figref{fig:long-ud-example-U12D1}) or to related variable names under similar scopes ($U^6 D^{17}$ in \figref{fig:long-ud-example-U6D17}).

\paragraph{Comparing to Deep3}
As mentioned in Sec~\ref{sec:previouswork}, Deep3 also relies on the values collected by their tree-walk programs (in TGEN, see Fig~\ref{fig:deep3}) executed over ASTs.



Deep3's TGEN programs are strictly more expressive than our $\mathit{UDpath}$s, which are based on only up and down counts.
However, for many of the tree walks, we can find corresponding $\mathit{UDpath}$s that represent the same movement in an AST. 
For example, TGEN expression \code{[Up][Up][WRITE\_TYPE]} is similar to our $U^2$.
\code{WRITE} is disregarded as our models naturally have access to the values associated at the destination.
We collected the most frequently used TGEN's tree-walk expressions when evaluating their model (E13) over the py150 testing set.
\tabref{tab:top-eth-paths} lists the top equivalent $\mathit{UDpath}$s and their counts, assuming the node to be predicted is a leaf with a left sibling leaf.

\begin{table}[]
    \centering
    \begin{tabular}{l | r}
    \hline
    Equivalent $\mathit{UDpath}$ & Count \\
    \hline
    $U^1$ & $1.8 \times 10^{7}$ \\
$U^2D^1$ & $4.7 \times 10^{6}$ \\
$U^3$ & $4.2 \times 10^{6}$ \\
$U^2$ & $3.4 \times 10^{6}$ \\
$U^4$ & $3.0 \times 10^{6}$ \\
$U^2D^2$ & $2.9 \times 10^{6}$ \\
    \hline
    \end{tabular}
    \caption{Top $\mathit{UDpath}$-convertible tree-walks used by E13 when predicting values over py150.}
    \label{tab:top-eth-paths}
\end{table}

 
We found that $U^1$, $U^2$ and $U^2 D^2$ are at the both 
extremely weighted by many heads in our \TreeRel 
and heavily utilized in Deep3.
However, some of the potentially useful $\mathit{UDpath}$s heavily used by Deep3 are not often extremely weighted by \TreeRel.
For example $U^3$, potentially useful for knowing the scope of the value to be predicted, only appears once as the maximally value in layer 5, head 5 of \TreeRel (\figref{fig:ud-headwise-max}).

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.35\textwidth}
        \includegraphics[width=\textwidth]{fig/ud-headwise-max}
        \caption{$\max$}
        \label{fig:ud-headwise-max}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{0.35\textwidth}
        \includegraphics[width=\textwidth]{fig/ud-headwise-min}
        \caption{$\min$}
        \label{fig:ud-headwise-min}
    \end{subfigure}
    \caption{Maximally (a) or minimally (b) weighted tree-relations and their weights at each attention head in \TreeRel.
    \textcolor{ACMRed}{Red} means more extremal values.}
    \label{fig:ud-headwise}
\end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{fig/tree-119_ud-12U1D}
        \caption{\code{legofy\_gui.py}~\protect\footnotemark, highlighting $U^{12} D^{1}$}
        \label{fig:long-ud-example-U12D1}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{fig/tree-238_ud-6U17D}
        \caption{\code{views.py}~\protect\footnotemark, highlighting $U^{6} D^{17}$}
        \label{fig:long-ud-example-U6D17}
    \end{subfigure}
    \caption{Two code excerpts from py150 evaluation set. Highlighted tokens are picked by some long $\mathit{UDpath}$ in prediction of the underlined tokens.
    }
    \label{fig:long-ud-example}
\end{figure*}
\addtocounter{footnote}{-2}
\stepcounter{footnote}\footnotetext{\url{data/JuanPotato/Legofy/legofy/legofy_gui.py}}
\stepcounter{footnote}\footnotetext{\url{data/Miserlou/OpenWatch/openwatch/map/views.py}} 
\end{document}
