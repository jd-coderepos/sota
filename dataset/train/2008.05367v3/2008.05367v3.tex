
\subsection{Background}
The continuous-time replica exchange Langevin diffusion (reLD)  is a Markov process compounded with a Poisson jump process. In particular, the Markov process follows the stochastic differential equations

where  are the particles (parameters) at time  in ,  are two independent Brownian motions,  is the energy function,  are the temperatures. The jumps originate from the swaps of particles  and  and follow a Poisson process where the jump rate is specified as the Metropolis form . Here  is a constant, and  follows

Under such a swapping rate, the probability  associated with reLD at time  is known to converge to the invariant measure (Gibbs distribution) with density


In practice, obtaining the exact energy and gradient for reLD (\ref{sde_2}) in a large dataset is quite expensive. We consider the replica exchange stochastic gradient Langevin dynamics (reSGLD), which generates iterates  as follows

where  is considered to be a fixed learning rate for ease of analysis, and  and  are independent Gaussian random vectors in . Moreover, the positions of the particles swap based on the stochastic swapping rate. In particular, , and the stochastic gradient  can be written as , where both  and  are random variables with mean not necessarily zero. We also denote  as the probability measure associated with  in reSGLD (\ref{reSGLD}) at step , which is close to  in a suitable sense.


\subsection{Overview of the analysis}


We aim to study the convergence analysis of the probability measure  to the invariant measure  in terms of 2-Wasserstein distance,

where  is the Euclidean norm, and the infimum is taken over all joint distributions  with  and  being the marginals distributions.

By the triangle inequality, we easily obtain that for any  and , we have 

We start with the discretization error first by analyzing how reSGLD (\ref{reSGLD}) tracks the reLD (\ref{sde_2}) in 2-Wasserstein distance. The critical part is to study the discretization of the Poisson jump process in mini-batch settings. To handle this issue, we follow \citet{Paul12} and view the swaps of positions as swaps of temperatures. Then we apply standard techniques in stochastic calculus \citep{chen2018accelerating, yin_zhu_10, Issei14, Maxim17} to discretize the Langevin diffusion and derive the corresponding discretization error.


Next, we quantify the evolution of the 2-Wasserstein distance between  and . The key tool is the exponential decay of entropy (Kullback-Leibler divergence) when  satisfies the log-Sobolev inequality (LSI) \citep{Bakry2014}. To justify LSI, we first verify LSI for reSGLD without swaps, which is a direct result given a proper Lyapunov function criterion \citep{Cattiaux2010} and the Poincar\'{e} inequality \citep{chen2018accelerating}. Then we follow \citet{chen2018accelerating} and verify LSI for reLD with swaps by analyzing the Dirichlet form. Finally, the exponential decay of the 2-Wasserstein distance follows from the Otto-Villani theorem by connecting the 2-Wasserstein distance with the entropy \citep{Bakry2014}. 


Before we move forward, we first lay out the following assumptions:
\begin{assumption}[Smoothness]\label{assump: lip and alpha beta}
The energy function  is -smoothness, which implies that there exists a Lipschitz constant , such that for every , we have  \footnote{ denotes the Euclidean  norm.}
\end{assumption}{}


\begin{assumption}[Dissipativity]\label{assump: dissipitive}
The energy function  is -dissipative, i.e. there exist constants  and  such that ,  
\end{assumption}{}

Here the smoothness assumption is quite standard in studying the convergence of SGLD, and the dissipativity condition is widely used in proving the geometric ergodicity of dynamic systems \citep{Maxim17, Xu18}. Moreover, the convexity assumption is not required in our theory.



\subsection{Analysis of discretization error}

The key to deriving the discretization error is to view the swaps of positions as swaps of the temperatures, which has been proven equivalent in distribution \citep{Paul12}. Therefore, we model reLD using the following SDE, 
{}
where  ,  is a Brownian motion,  is a random matrix in continuous-time that swaps between the diagonal matrices  and  with probability , and  is denoted as the identity matrix.

Moreover, the corresponding discretization of replica exchange SGLD (reSGLD) follows:

where  is a standard Gaussian distribution in , and  is a random matrix in discrete-time that swaps between  and  with probability . We denote  as the continuous-time interpolation of , which satisfies the following SDE, 

Here the random matrix  follows a similar trajectory as . For  with , the relation  follows.


\begin{lemma}[Discretization error]\label{discretization}
Given the smoothness and dissipativity assumptions \eqref{assump: lip and alpha beta} and \eqref{assump: dissipitive}, and the learning rate  satisfying , there exists constants  and  such that

where  depends on ;  depends on  and ;  depends on  and .
\end{lemma}{}

\begin{proof}
Based on the replica exchange Langevin diffusion  and the continuous-time interpolation of the stochastic gradient Langevin diffusion , we have the following SDE for the difference . For any , we have

Indeed, note that

We first square both sides and take expectation, then apply the Burkholder-Davis-Gundy inequality and  Cauchy-Schwarz inequality, we have 


\noindent

For the first term , by using the inequality
 
we get  

By using the smoothness assumption \ref{assump: lip and alpha beta}, we first estimate 
{}
By applying the smoothness assumption \ref{assump: lip and alpha beta} and discretization scheme, we can further estimate 
{}
For  and , we have
{}
which indeed implies
{}
Similar to the estimate \eqref{sup norm estimate 1}, square both sides and take expectation, then apply the Burkholder-Davis-Gundy inequality, we have
{}
where the last inequality follows from the fact that  is a diagonal matrix with diagonal elements  or 
For the first term in the above inequality, we further have

where the first inequality follows from the separation of the noise from the stochastic gradient and the choice of stationary point  of  with , and  is the stochastic noise in the gradient at step . Thus, combining the above two parts and integrate  on the time interval , we obtain the following bound 
{}
By plugging the estimate \eqref{est cI2 part two} into estimate \eqref{est cI2}, we obtain the following estimates when ,
{}
where  is a constant depending on  and . Note that the above inequality requires a result on the bounded second moment of   , and this is majorly\footnote{The slight difference is that the constant in the RHS of (C.38) \citet{chen2018accelerating} is changed to account for the stochastic noise.} proved in Lemma  in \citet{chen2018accelerating} when we choose the stepzise . We are now left to estimate the term  and we have
{}
Combing all the estimates of  and , we obtain
{}
\noindent

For the second term , we have 

where  is the temperature matrix for the continuous-time interpolation of , which is similar to \eqref{SGD continuous time interpolation} without noise generated from mini-batch settings and is defined as below


We estimate  first, considering that  and  are both diagonal matrices, we have
{}
where , and the equality follows from the fact that the conditional probability . Here  denotes the higher remainder with respect to . The estimate of  without stochastic gradient for the Langevin diffusion is first obtained in \citet{chen2018accelerating}, we however present here again for reader's convenience.  
As for the second term , it follows that

where  is the noise in the swapping rate. Thus, one concludes the following estimates combing  and .

Apply Gronwall's inequality to the function
{}
and deduce that 

where  is a constant depending on ;  depends on  and ;  depends on  and . \qed 


\end{proof}


\subsection{Exponential decay of Wasserstein distance in continuous-time}


We proceed to quantify the evolution of the 2-Wasserstein distance between  and . We first consider the ordinary Langevin diffusion without swaps and derive the log-Sobolev inequality (LSI). Then we extend LSI to reLD and obtain the exponential decay of the relative entropy. Finally, we derive the exponential decay of the 2-Wasserstein distance.


In order to distinguish from the replica exchange Langevin diffusion  defined in \eqref{replica exchange}, we call it  which follows, 
{}
where  is a diagonal matrix with the form . 
The process  is a Markov diffusion process with infinitesimal generator  in the following form, for  and ,
\beaa
\cL=&-\la\nabla_{x_1}f(x_1,x_2),\nabla U(x_1)\ra+\tau_1\Delta_{x_1}f(x_1,x_2)\\
&-\la \nabla_{x_2}f(x_1,x_2),\nabla U(x_2)\ra+\tau_2\Delta_{x_2}f(x_1,x_2)
\eeaa
Note that since matrix  is a non-degenerate diagonal matrix, operator  is an elliptic diffusion operator. According to the smoothness assumption \eqref{assump: lip and alpha beta}, we have that , where , the unique invariant measure  associate with the underlying diffusion process satisfies the Poincare inequality and LSI with the Dirichlet form given as follows,
\bea\label{dirichlet form}
\cE(f)=\int \Big(\tau_1\|\nabla_{x_1}f\|^2+\tau_2\|\nabla_{x_2}f\|^2 \Big)d\pi(x_1,x_2),\qq f\in\cC_0^2(\hR^{2d}).
\eea
In this elliptic case with  being convex, the proof for LSI follows from standard Bakry-Emery calculus \cite{Bakry85}. Since, we are dealing with the non-convex function , we are particularly interested in the case of .
To obtain a Poincar\'{e} inequality for invariant measure , \citet{chen2018accelerating} adapted an argument from \citet{Bakry08} and \citet{Maxim17} by constructing an appropriate Lyapunov function for the replica exchange diffusion without swapping . Denote  as the distribution associated with the diffusion process , which is absolutely continuous with respect to . It is a direct consequence of the aforementioned results that the following log-Sobolev inequality holds.



\begin{lemma}[LSI for Langevin Diffusion]\label{LSI no swaping}
Under assumptions \eqref{assump: lip and alpha beta} and \eqref{assump: dissipitive}, we have the following log-Sobolev inequality for invariant measure , for some constant , 
\beaa
D(\n_t||\pi)\le 2c_{\text{LS}}\cE(\sqrt{\frac{d\n_t}{d\pi}}).
\eeaa
where  denotes the relative entropy and the Dirichlet form  is defined in \eqref{dirichlet form}.
\end{lemma}{}
\begin{proof}

According to \citet{Cattiaux2010}, the sufficient conditions to establish LSI are:
\begin{enumerate}
\item There exists some constant , such that .
\item  satisfies a Poincar\'{e} inequality with constant , namely, for all probability measures , , where  is the  divergence between  and .
\item There exists a  Lyapunov function  such that 
for all  and some .
\end{enumerate}
Note that the first condition on the Hessian is obtained from the smoothness assumption \eqref{assump: lip and alpha beta}. Moreover, the Poincar\'{e} inequality in the second condition is derived from Lemma C.1 in \citet{chen2018accelerating} given assumptions \eqref{assump: lip and alpha beta} and \eqref{assump: dissipitive}. Finally, to verify the third condition, we follow \citet{Maxim17} and construct the Lyapunov function 
. From the dissipitive assumption \ref{assump: dissipitive},  satisfies the third condition because

where , and . Therefore, the invariant measure  satisfies a LSI with the constant

where  and . \qed




\end{proof}{}



We are now ready to prove the log-Sobolev inequality for invariant measure associated with the replica exchange Langevin diffusion \eqref{replica exchange}. We use a similar idea from \citet{chen2018accelerating} where they prove the Poincar\'{e} inequality for the invariant measure associated with the replica exchange Langevin diffusion \eqref{replica exchange} by analyzing the corresponding Dirichlet form. In particular, a larger Dirichlet form ensures a smaller log-Sobolev constant and hence results in a faster convergence in the relative entropy and Wasserstein distance.

\begin{lemma}[Accelerated exponential decay of ]\label{exponential decay}
Under assumptions \eqref{assump: lip and alpha beta} and \eqref{assump: dissipitive}, we have that the replica exchange Langevin diffusion converges exponentially fast to the invariant distribution :

where ,  is a non-negative constant depending on the swapping rate  and obtains  only if .
\end{lemma}{}

\begin{proof}
Consider the infinitesimal generator associated with the diffusion process \eqref{replica exchange}, denoted as , contains an extra term arising from the temperature swapping. The operator  in this particular case, indeed, has the following form
\bea
\cL_{S}=\cL+ rS(x_1,x_2)\cd (f(x_2,x_1)-f(x_1,x_2)).
\eea
According to Theorem 3.3 \citep{chen2018accelerating}, the Dirichlet form associated with operator  under the invariant measure  has the form
\bea\label{dirichlet swap}
\cE_{S}(f)=\cE(f)+\underbrace{\frac{r}{2}\int S(x_1,x_2)\cd (f(x_2,x_1)-f(x_1,x_2))^2d\pi(x_1,x_2)}_{\text{acceleration}},~ f\in\cC_0^2(\hR^{2d}),
\eea
where  corresponds to , and the asymmetry of  is critical in the acceleration effect \citep{chen2018accelerating}. Given two different temperatures  and , a non-trivial distribution  and function , the swapping rate  is positive for almost any . As a result, the Dirichlet form associated with  is strictly larger than . Therefore, there exists a constant  depending on , such that . From Lemma \ref{LSI no swaping}, we have

Thus, we obtain the following log-Sobolev inequality for the unique invariant measure   associated with replica exchange Langevin diffusion  and its corresponding Dirichlet form . In particular, the LSI constant  in replica exchange Langevin diffusion with swapping rate  is strictly smaller than the LSI constant  in the replica exchange Langevin diffusion with swapping rate . By the exponential decay in entropy \citep{Bakry2014}[Theorem 5.2.1] and the tight log-Sobolev inequality in Lemma \ref{LSI no swaping}, we get that, for any , 

Finally, we can estimate the term  by the
Otto-Villani theorem \citep{Bakry2014}[Theorem 9.6.1],

\end{proof}




\subsection{Summary: Convergence of reSGLD}

Now that we have all the necessary ingredients in place, we are ready to derive the convergence of the distribution  to the invariant measure  in terms of 2-Wasserstein distance, 




\begin{theorem}[Convergence of reSGLD]Let the assumptions \eqref{assump: lip and alpha beta} and \eqref{assump: dissipitive} hold. For the unique invariant measure  associated with the Markov diffusion process \eqref{replica exchange} and the distribution  associated with the discrete dynamics , we have the following estimates, for  and the learning rate  satisfying , 

where ,  is a non-negative constant depending on the swapping rate  and obtains the minimum zero only if .
\end{theorem}{}

\begin{proof}
We reduce the estimates into the following two terms by using the triangle inequality,

The first term  follows from the analysis of discretization error in Lemma.\ref{discretization}.
Recall the very definition of the  distance defined in (\ref{w2}). Thus, in order to control the distance , , we need to consider the diffusion process whose law give  and , respectively. Indeed, it is obvious that  for . For the other measure , it follows that  for , where  is the probability measure associated with the continuous interpolation of reSGLD (\ref{resgld_2}). By Lemma.\ref{discretization}, we have that for  and , 


Recall from the accelerated exponential decay of replica exchange Langevin diffusion in Lemma.\ref{exponential decay}, we have


Combing the above two estimates completes the proof.
\qed
\end{proof}{}
