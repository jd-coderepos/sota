\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}



\usepackage{graphicx}




\usepackage[final]{neurips_2022}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{ulem}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{bbding}
\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem*{proposition}{Proposition}
\title{Periodic Graph Transformers for Crystal Material Property Prediction}





\author{Keqiang Yan \\
 Computer Science \& Engineering\\
  Texas A\&M University\\
  College Station, TX 77843 \\
  \texttt{keqiangyan@tamu.edu} \\
  \And
  Yi Liu\thanks{Equal senior contributions} \\
  ~~~~~~~~~~~~~Computer Science~~~~~~~~~~~~~\\
  Florida State University\\
  Tallahassee, FL 32306 \\
  \texttt{liuy@cs.fsu.edu} \\
  \And
  Yuchao Lin \\
  Computer Science \& Engineering\\
  Texas A\&M University\\
  College Station, TX 77843 \\
  \texttt{kruskallin@tamu.edu} \\
  \And
  Shuiwang Ji\footnotemark[1] \\
  Computer Science \& Engineering\\
  Texas A\&M University\\
  College Station, TX 77843 \\
  \texttt{sji@tamu.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
We consider representation learning on periodic graphs encoding crystal materials. Different from regular graphs, periodic graphs consist of a minimum unit cell repeating itself on a regular lattice in 3D space. How to effectively encode these periodic structures poses unique challenges not present in regular graph representation learning. In addition to being E(3) invariant, periodic graph representations need to be periodic invariant. That is, the learned representations should be invariant to shifts of cell boundaries as they are artificially imposed. Furthermore, the periodic repeating patterns need to be captured explicitly as lattices of different sizes and orientations may correspond to different materials. In this work, we propose a transformer architecture, known as Matformer, for periodic graph representation learning. Our Matformer is designed to be invariant to periodicity and can capture repeating patterns explicitly. In particular, Matformer encodes periodic patterns by efficient use of geometric distances between the same atoms in neighboring cells. Experimental results on multiple common benchmark datasets show that our Matformer outperforms baseline methods consistently. In addition, our results demonstrate the importance of periodic invariance and explicit repeating pattern encoding for crystal representation learning.


\end{abstract}


\section{Introduction}

Crystal material property prediction is important for the discovery of new materials with desirable properties~\citep{crystal1,crystal2,crystal3,crystal4,crystal5,cgcnn,schnet,megnet,gatgnn,alignn}.
Different from molecules and proteins~\citep{gori2005new, wu2018moleculenet,shervashidze2011weisfeiler,fout2017protein,wang2022advanced,liu2021dig,stokes2020deep}, which are commonly represented as graphs~\citep{gat_velivckovic2018graph,liu2020deep,gao2019graph,gao2021topology,liu2020towards,liu2021graphebm,luo2021graphdf}, crystals consist of a minimum unit cell repeating itself on a regular lattice in 3D space. Thus, crystals are naturally represented as periodic graphs.
A key challenge of crystal material property prediction lies in how to effectively encode periodic structures that are not present in regular molecular graph representations~\citep{3dtransformer,keriven2019universal,schnet,Dimenet,klicpera2020fast,liu2021spherical,klicpera2021gemnet,satorras2021n,jaini2021learning,e3nngeiger2022,nequip}. 
E(3) invariance for molecular graphs requires the representation for a given molecule to be invariant to translation, rotation and reflection transformations in 3D space.
Beyond that,
periodic graphs of crystals require unique 
periodic invariance. 
Periodic invariance has two facets; those are, the learned representations should be invariant to both the scaling up of the minimum repeatable unit cells
and shifts of periodic boundaries. Although the former has been considered in
Xie et al.~\citep{cgcnn, cdvae}, the later is rarely identified explicitly and sometimes not considered by previous studies~\citep{cgcnn,megnet,crabnet,schnet,alignn,cdvae,gatgnn,graphormer, nequip, chen2021direct}. 
Furthermore, the repeating patterns of periodic graphs should be captured explicitly. 
Given a fixed minimum unit cell structure, 
lattices of different sizes and orientations may correspond to different materials. Without such periodic patterns, the infinite structures of crystals may not be represented accurately. However, 
the explicit encoding of periodic patterns is not explored in previous studies~\citep{cgcnn,megnet,schnet,alignn,gatgnn,nequip}. 

In this work, we propose to tackle periodic graph representation learning by incorporating both periodic invariance and periodic pattern encoding. We propose a periodic graph transformer, known as Matformer, that is periodic invariant and can capture periodic repeating patterns explicitly for crystal representation learning. Matformer achieves periodic invariance through two uniquely designed graph construction methods. It further encodes periodic patterns by efficient use of geometric distances between the same atoms in neighboring cells, thereby capturing the lattice size and orientation of a given crystal.
We conduct experiments on two commonly used material benchmark datasets, including the Materials Project~\citep{jain2013commentary} and Jarvis~\citep{choudhary2020joint}. Results show that Matformer outperforms baseline methods consistently on various tasks. In addition, our results demonstrate the importance of both periodic invariance and periodic pattern encoding for crystal representation learning.



\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{sec2_v3.pdf}
    \vspace{-1mm}
    \caption{
    Illustrations of periodic patterns and the multi-edge graph construction method.
    Green lines are artificial boundaries to form one possible unit cell that repeats in infinite space for the given crystal.
    (a). An illustration of periodic patterns in 2D space. 
    We use blue and red arrows to show how the blue atom repeats itself along  and . We show a general case in nature that  and  are not orthogonal.
    In this specific example,  is less than .
    (b) and (c). Illustrations of the multi-edge graph construction method and the constructed graph. The green circle shows atom 's neighborhood ,
    and black arrows are edges from atom 's neighbors to itself.
    As atom  repeats twice within 's neighborhood in this example, 
    the multi-edge graph construction method builds two edges from atom  to atom .
    The crystals are 3D structures in practice, and we use illustrations in 2D for simplicity. 
    }
    \vspace{-4mm}
    \label{fig:1}
\end{figure}


\section{Background} \label{sec: backg}
\label{sec:pre}




\textbf{Crystal property prediction and crystal structures.}
Given a crystal represented as , , ,
crystal property prediction aims to predict a target property value , which is either real as  or categorical as 
,
for regression or classification task with  classes, respectively. 
Specifically, a crystal is represented as a unit cell with periodic patterns.
A unit cell is a minimum repeatable structure for the given crystal,
and it can be described by matrices  and .
 is the atom feature matrix, where  is the -dimensional feature vector for atom  in the unit cell.
 is the position matrix, where  contains the Cartesian coordinates for atom  in 3D space.
To further encode periodic patterns, an additional lattice matrix
 is used to describe how a unit cell repeats itself in three directions, including  and . We show a 2D case of periodic patterns for easy illustration in Fig.~\ref{fig:1} (a). Note that crystals usually possess irregular shapes in practice. Hence, 
, , and  are not always orthogonal in 3D space.
Formally, given a crystal representation ,  the infinite crystal structure can be represented as

Here,  contains all possible positions for each atom , associated with the same  in .



\textbf{Multi-edge graph construction for crystals.} The multi-edge graph construction proposed by~\citet{cgcnn} aims to capture atom interactions across cell boundaries, which are imposed artificially. In a regular molecular graph, a node corresponds to a single atom. In contrast,
in a multi-edge graph, node  represents 
atom  and all its duplicates in the infinite 3D space.
Apparently, node  contains
the atom features vector  and all positions in the set
    . 
Formally, the multi-edge graph construction method builds edges between nodes as follows.
Given a prefixed radius ,
if there exists any 3-tuple , where , such that the Euclidean distance  satisfies
,
an edge is built from  to  with the initial edge feature . An example of the edge construction is shown in Fig.~\ref{fig:1} (b). Intuitively, if there exist  positions of node  within the radius of the center node , this method builds  edges from node  to node .
By considering all possible positions of every node within a predefined radius in 3D space,
the multi-edge graph construction method can in essence capture atom interactions across cell boundaries~\citep{cyatt, gatgnn, megnet, schnet,alignn}.




\section{Periodic invariance and periodic pattern encoding for crystals}
\label{sec:3}

Different from molecular graphs, crystal graphs consist of a minimum unit cell repeating itself on a regular lattice in 3D space. When encoding such periodic structures, unique challenges lie in periodic invariance and periodic pattern encoding. In this section, we propose to formally define and analyze the importance of these two components.

\subsection{Periodic invariance for crystals}
\label{sec:p_invariance}



Periodic invariance is proposed based on E(3) invariance,  which is defined as below.



\begin{definition}[Unit Cell E(3) Invariance]
A function  is unit cell E(3) invariant such that for all  and  , we have , where  is rotation and reflection transformations, and  is translation transformations in 3D space.
\end{definition}
Intuitively, 
the structure of a cell remains the same when either applying rotations and reflections to position matrix  and lattice matrix  together, or applying translations to  only. Correspondingly, the output of the unit cell E(3) invariant function should remain the same. 

\begin{wrapfigure}[18]{r}{0.65\linewidth}
    \vspace{-18 pt}
    \centering
    \includegraphics[width=0.99\linewidth]{fig2.pdf}
    \vspace{-1mm}
    \caption{
    Illustration of periodic invariance.
Purple lines are the edges between nodes inside a unit cell.
    Red points are the corner points of the unit cells. For example,   and  are for unit cells in (a) and (b), respectively. (a) and (b) show different unit cells describing the same crystal, caused by shifting the period boundaries along  axis from  to . By comparing (a) and (b), we show a graph construction method that breaks periodic invariance.
    }
    \vspace{-4mm}
    \label{fig:invariance}
\end{wrapfigure}
In addition to unit cell E(3) invariance, periodic invariance is also shown necessary for generating valid crystal representations. Specifically, when the lattice matrix  is fixed for a crystal, we can still obtain different position matrices  and different unit cell structures by shifting the period boundaries.
As shown in Fig.~\ref{fig:invariance} (a) and (b), the formed unit cell structures are different for the same crystal by shifting period boundaries.
To this end, we further introduce periodic invariance, which shows that when the periodic boundaries are shifted or scaled up, the periodic invariant representation should remain the same. Formally, based on Sec.~\ref{sec: backg}, 
we further define a function 
simulating how to form different unit cells from a given infinite crystal structure.
For an infinite crystal structure represented as ,
 uses a corner point  and shape matrix  to form a unit cell represented as . In addition, we use  to indicate the scaling up of a repeating unit cell formed
by periodic boundaries. Then the formal definition of periodic invariance is below.

\begin{definition}[Periodic Invariance]
A unit cell E(3) invariant function  is periodic invariant if  holds for all  and .
\end{definition}


\textbf{Significance of periodic invariance.} Breaking periodic invariance will result in different crystal graphs for the same crystal.
In Fig.~\ref{fig:invariance} (a) and (b), we show a graph construction method that breaks periodic invariance. This method
is employed by a transformer-based model, known as Graphormer~\citep{graphormer}, which first uses radius to include all the atoms of interest in nearby cells and then builds a fully connected graph. A detailed illustration is shown in Fig.~\ref{fig:graph_graphormer} in Appendix.~\ref{app:breakingpi}.
Based on the formal definition of periodic invariance,
in this study, we aim to integrate such an important component in our Matformer.
By doing this, our model is able to construct a distinct crystal graph for a given crystal structure,
resulting in a more informative and discriminative crystal learning scheme.





















\subsection{Periodic pattern encoding} \label{sec:ppe}
\label{sec:periodic_patterns}
\begin{wrapfigure}[17]{r}{0.5\linewidth}
    \vspace{-40 pt}
    \begin{center}
        \includegraphics[width=0.9\linewidth]{periodic_patterns.pdf}
    \end{center}
    \vspace{-10 pt}
    \caption{
    Illustration that periodic patterns are not encoded in 
    a multi-edge graph. Grey lines show the captured topology information by the multi-edge graph construction. We use three radius circles because there are three atoms in a unit cell. (a). Illustration of the multi-edge graph construction. (b). Illustration that the multi-edge graph method only captures local geometric information but ignores periodic patterns for the infinite structure.
    }
    \label{fig:periodic}
\end{wrapfigure}
As introduced in Sec.~\ref{sec: backg},

containing periodic patterns is another key component to  
describe crystal structures. 
Essentially, periodic patterns show how the minimum repeatable structure  expands itself in infinite 3D space. Without such periodic pattern encoding in , crystal structures are treated as finite structures similar to molecules. 
As shown in Fig.~\ref{fig:periodic}, the widely used multi-edge graph construction method~\citep{cgcnn, megnet, schnet, cyatt, alignn} only captures local interactions among atoms but ignores the important periodic patterns. However, such periodic repeating patterns need to be captured explicitly, as lattices of different sizes and orientations may correspond to different materials. Hence,
To better represent the infinite structures of crystals, we argue that the periodic patterns  should be explicitly taken into consideration in crystal learning.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{matformer.pdf}
    \vspace{-15 pt}
    \caption{
    Illustrations of the used radius-based graph construction method and the proposed periodic pattern encoding for Matformer. 
Black arrows are the formed edges by radius-based graph construction and brown arrows are self-connecting edges.
    (a). Illustration that the used radius-based graph construction satisfies periodic invariance. 
    (b). Illustration of periodic pattern encoding using self-connecting edges. 
    On the left, we show the designed self-connecting edges to encode the lattice matrix
     in 3D space. 
    We design six self-connecting edges , , , , , . The geometric shape of  can be determined by these six edges. 
    On the right, we show the added self-connecting edges in 2D space for easy illustration. 
(c). Illustration of the constructed graph with periodic pattern encoding for the above 2D case.
    }
    \label{fig:matformer}
    \vspace{-20 pt}
\end{figure}

\section{The proposed Matformer}
\label{sec:4}











\subsection{The proposed graph construction methods} \label{sec:graph}
In this section, we introduce the proposed graph construction methods for Matformer. Our methods effectively integrate periodic invariance and periodic pattern encoding.

\textbf{Invariant crystal graph construction}. We consider two crystal graph construction methods, including radius-based graph construction and fully connected graph construction. Both of them satisfy periodic invariance, and mathematical proofs can be found in Appendix.~\ref{app:proof}. 
In Fig.~\ref{fig:invariance},
we show by example that treating atoms as single nodes breaks periodic invariance.
Instead, our methods follow the fashion introduced in
Sec.~\ref{sec: backg} to treat node  as atom  and all its repeated duplicates.


We use the multi-edge graph construction~\citep{cgcnn} introduced in Sec.~\ref{sec: backg} as an alternative crystal graph construction method.
Note that although the multi-edge graph construction satisfies periodic invariance as in Fig.~\ref{fig:matformer} (a), several existing works~\citep{cgcnn,gatgnn} do not follow the settings exactly
thus breaking periodic invariance. 
Within the given radius shown in Fig.~\ref{fig:matformer} (a),
these studies form the neighborhood of node  by selecting  nearest neighbors ranked by geometric distances. If there exist several different atoms with the same distance to node , there is no deterministic way to select from them, as shown in Fig.~\ref{fig:nn_wrong} in Appendix.~\ref{app:breakingpi}. 
As a result, periodic invariance cannot be guaranteed.



Given the fully-connected fashion employed in Graphormer has achieved impressive performance on molecular learning,
we further propose another graph construction method for Matformer, known as 
the fully connected graph construction. This method uses a different strategy to
determine neighbors for each center node.
Specifically, for node  and , it builds edges for the entries corresponding to the  smallest distances in . It can be seen every pair  and  is connected in the constructed graph and there are  edges between them.


Overall, the used multi-edge graph construction and proposed fully-connected graph construction
both satisfy the important periodic invariance.
Particularly, they possess great flexibility to 
be used in future studies for crystal learning.
In this study, we employ both methods as part of our proposed
Matformer, and in main experiments, the radius-based method is used due to better empirical performance.

\textbf{Periodic pattern encoding with self-connecting edges}. In this study, we propose to encode the important  into crystal graphs by adding self-connecting edges. As mentioned in Sec.~\ref{sec:ppe}, periodic patterns 
describe sizes and orientations of lattices of a crystal structure,
eventually determining the properties of this crystal.
A natural step for encoding such repeating periodic patterns is to consider the relative positions between an atom and its nearby repeated duplicates.
Formally, given an atom  with position 
and ,
we need to encode the atom's three nearby duplicates with positions , , and .
It is widely known that a direction vector  is determined
by both its length  and orientation.
Essentially,  indicates the geometric
distance between atom  and its corresponding duplicate.
However, the computing of orientation information, such as angles, usually induces high
complexity. Hence, it is not practical to encode such orientation information into transformer architectures.
To this end, we propose to use geometric distances solely to
implicitly consider the orientation information. 

Specifically, we
use additional distances to determine angles between any two direction vectors in . 
For example, 
the angle between  and  can be easily computed by 
, , 
and an additional distance .
Hence, based on these three distances,
we can determine lengths of  and ,
and the relative orientation between them.
Extensively, 
we use six geometric distances, including , , ,
, , and 
in our study, as shown in Fig.~\ref{fig:matformer} (b).
By doing this, the length of each direction vector 
and the angle between any two direction vectors can all be determined.
As a result, the shape formed by lattice matrix 
is then fixed.
Overall, we build the aforementioned six geometric
distances as six
self-connecting edges for node .
By doing this, our model is capable of
encoding periodic patterns in  completely,
resulting in a more accurate crystal representation learning scheme.
Importantly, 
our approach also guarantees periodic invariance.






Overall, the graph construction for Matformer consists of two necessary stages, including invariant graph construction and periodic pattern encoding. For the first stage, we rigorously prove that the multi-edge graph construction satisfies periodic invariance in Appendix.~\ref{app:proof} and show that several previous works~\citep{cgcnn, gatgnn} break periodic invariance using a different neighbor selection strategy. Additionally, we propose a fully-connected crystal graph construction method satisfying periodic invariance, and the method could be used in future studies for crystal representation learning.
For the second stage, we naturally encode periodic patterns into constructed graphs by adding self-connecting edges without breaking periodic invariance. A constructed graph in 2D case is shown in Fig.~\ref{fig:matformer} (c).


\subsection{Message passing scheme}
\label{sec:msg}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{net_arch.pdf}
    \caption{
    Illustration of detailed Architecture of Matformer. The overlapping graphics are used to denote two different attention heads. (a). Illustration of Matformer pipeline. (b). Illustration of the detailed Matformer layer in (a). We show the case with two attention heads for simplicity.
    }
    \label{fig:net_arch}
\end{figure}



Building on constructed graphs introduced in Sec.~\ref{sec:graph},
we propose our message passing scheme for Matformer.
Formally, we denote a constructed crystal graph as .
Here, each  is the -dimensional feature vector for atom ,
as introduced in Sec.~\ref{sec: backg}.
Particularly,
 is -dimensional feature vector for the -th edge between nodes  and . 
We follow the regular attention mechanism that computes query, key, and value~\cite{liu2020global,liu2021cleftnet}.
Our proposed message passing scheme is composed of three 
steps; those are, edge-wise attention coefficients computing, edge-wise value message computing, and node updating. 
Formally, we let  denote the input feature vector of node  for the -th layer of Matformer. The message passing scheme of the -th layer is described as below.

In the first step, ,  and  for the -th edge between  and  are computed as

where , , and  denote the linear transformations to compute query, key, and edge embedding in -th layer, respectively. 
 is the intermediate output for .
We use  and  to denote Hadamard Product and concatenation. 
Note that  is the concatenation of three  vectors to match the dimension of .
By doing this, when computing ,  attends each of , , and  for integrating more information in attention.

Particularly, we omit the softmax to enhance the model's capability to distinguish nodes with different degrees, and to make the whole network more efficient.

After obtaining coefficients , in the second step, we compute  that is the message of  as

where  and  are the linear transformations to compute value and the updated message, and  denotes the layer normalization operation. 


Finally, in the third step, we compute
node 's feature vector  .
Specifically, we first obtain message 
 by  
aggregating information from node 's neighborhood over multiple edges,
then achieve  as

where  is the used activation function, and  indicates batch normalization. In addition,  and  are linear transformations to update the messages on edges and the old atom features.

Graphormer represents an effective transformer variant for molecular graph learning. The differences between Graphormer and the proposed Matformer lie in both graph construction and message passing scheme. Firstly, Graphormer treats every atom as a single node and breaks periodic invariance, as mentioned in Sec.~\ref{sec:p_invariance}. For the message passing, Graphormer uses the node-wise attention and encodes pairwise distances as attention bias. It cannot work properly on multi-edge graphs for crystals. While Matformer is specifically designed for multi-edge crystal graphs by performing edge-wise attention and encoding geometric information into edge-wise messages, as described above. The detailed architecture of our Matformer is shown in Fig.~\ref{fig:net_arch}.


\section{Related work}
\textbf{Crystal property prediction}. Several existing methods~\citep{crabnet, jha2018elemnet, jha2019irnet, goodall2020predicting} model crystals as chemical formulas and employ sequence models to process them. Other studies~\citep{cgcnn, cyatt, gatgnn, megnet, schnet,alignn} consider 3D structures and formulate crystals as 3D graphs, then apply GNNs to learn from crystal graphs.
As crystals are essentially periodically repeated structures, the graph construction needs to consider periodic invariance and periodic pattern encoding. 
There are limited efforts to identify these two unique components. As an early work, CGCNN~\citep{cgcnn} proposes to capture atom interactions across artificial cell boundaries by using multi-edge graphs described in Sec.~\ref{sec: backg}. The multi-edge graph satisfies periodic invariance as described in Sec.~\ref{sec:p_invariance}, but fails to consider the important periodic patterns, as described in Sec.~\ref{sec:periodic_patterns}. The multi-edge graph construction method is widely used in the following studies~\citep{cyatt, gatgnn, megnet, schnet,alignn, nequip}. Based on the constructed crystal graphs, many GNN variants have been proposed for effective crystal representation learning~\citep{cgcnn, cyatt, gatgnn, megnet, schnet,alignn, nequip}. 
Specifically, Nequip~\citep{nequip} considers E(3) equivariance for materials, and satisfies periodic invariance using multi-edge graphs, but fails to capture periodic repeating patterns.
We also notice a recent work~\citep{wang2022deep_gen_perio} for periodic graph generation, which considers periodic graphs as finite graphs and breaks periodic invariance.
Recently, ALIGNN~\citep{alignn} achieves the best performance on two major material datasets.
It uses angle information in the message passing to generate more informative and discriminative representations.
However, the use of angles introduces excessive time complexity.

\textbf{Geometric GNNs and graph transformer}. Many efforts have been made to incorporate 3D geometric information in molecular learning.
Exemplary studies include SchNet~\citep{schnet}, DimeNet~\citep{Dimenet,klicpera2020fast}, SphereNet~\citep{liu2021spherical}, GemNet~\citep{klicpera2021gemnet}, etc. 
However, these methods are designed for molecules without periodic patterns. Recently, graph transformers~\citep{graphormer, fuchs2020se} using geometric information, e.g., Graphormer~\citep{graphormer}, have shown great potential on real-world graph data. 
However, Graphormer considers neither periodic invariance nor periodic pattern encoding.

\textbf{Differences with our method.} To the best of our knowledge, periodic invariance and periodic pattern encoding described in Sec.~\ref{sec:3} are rarely identified and explored in existing works for crystal property prediction. CGCNN~\citep{cgcnn} breaks periodic invariance on some corner cases because it uses twelve nearest neighbors determined only by distances as described in Sec.~\ref{sec:graph}. In addition, previous methods including CGCNN~\citep{cgcnn}, SchNet~\citep{schnet}, MEGNET~\citep{megnet}, CYATT~\citep{cyatt}, GATGNN~\citep{gatgnn}, NEQUIP~\citep{nequip} and ALIGNN~\citep{alignn}, all fail to consider the important periodic pattern encoding as introduced in Sec.~\ref{sec:periodic_patterns}. Especially, following GAT~\citep{gat_velivckovic2018graph}, GATGNN~\citep{gatgnn} employs a very limited kind of attention mechanism that is not conditioned on query, as explained in GATv2~\citep{gatv2}. As a result, the model capacity is reduced compared with the self attention mechanism employed in Matformer. In addition, the usage of softmax limits the capability of GATGNN of distinguishing nodes with different degrees, as mentioned in Sec.~\ref{sec:msg}.
For Graphormer~\citep{graphormer}, although it achieved remarkable success on the Open Catalyst Challenge~\citep{ocp_dataset}, the employed graph construction method breaks periodic invariance when applied to crystals. Compared with Graphormer, Matformer is specifically designed for crystals considering both periodic invariance and periodic patterns.





















\section{Experimental studies}
\subsection{Experimental setup}
We conduct experiments on two material benchmark datasets, including The Materials Project~\citep{jain2013commentary} and JARVIS~\citep{choudhary2020joint}. The detailed descriptions for The Materials Project and JARVIS datasets are shown in Appendix.~\ref{app:dataset_des} Baseline methods include CFID~\citep{cfid}, CGCNN~\citep{cgcnn}, SchNet~\citep{schnet}, MEGNET~\citep{megnet}, GATGNN~\citep{gatgnn}, and ALIGNN~\citep{alignn}. Unless otherwise specified, for all the baseline methods, we report the results taken from the referred papers or provided by original authors. 
All Matformer models are trained using the Adam optimizer~\citep{kingma2015adam} with weight decay~\citep{weightdecay} and one cycle learning rate scheduler~\citep{onecycle}. We only slightly adjust learning rates from 0.001 and training epochs from 500 for different tasks. Detailed Matformer configurations for different tasks are provided in Appendix.~\ref{app:matconfig}. 


\begin{table}[t]
  \vspace{-4mm}
  \caption{Comparison 
in terms of test MAE on The Materials Project dataset. To make the comparison clear and fair, We show results from retrained models using exactly the same training, validation, and test sets. 
Results from original papers are shown in Appendix~\ref{app:retrain}. The best results are shown in \textbf{bold} and the second best results are shown with \uline{underlines}.}
  \label{mp-table}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    & Formation Energy & Band Gap & Bulk Moduli & Shear Moduli \\
    \cmidrule(r){2-5}
    Method & eV/atom  &  eV &   log(GPa) & log(GPa)  \\
    \midrule
    CGCNN~\citep{cgcnn} & 0.031 & 0.292  & 0.047 &0.077 \\
    SchNet~\citep{schnet} & 0.033 & 0.345 & 0.066 & 0.099 \\
    MEGNET~\citep{megnet} & 0.030 & 0.307 & 0.060 & 0.099 \\
    GATGNN~\citep{gatgnn} & 0.033 & 0.280 & \uline{0.045} & \uline{0.075} \\
    ALIGNN~\citep{alignn} & \uline{0.022} & \uline{0.218} & 0.051 & 0.078 \\
    Matformer & \textbf{0.021} & \textbf{0.211} & \textbf{0.043} & \textbf{0.073} \\
    \bottomrule
  \end{tabular}
  \vspace{-4mm}
\end{table}






\subsection{Experimental results}
\label{sec:materialsproj}
\textbf{The Materials Project.} We first use The Materials Project-2018.6.1 dataset~\citep{megnet}, which contains 69239 crystals, to evaluate Matformer. 
We notice that previous works~\citep{cgcnn, schnet,megnet,gatgnn,alignn} compare with each other
either using datasets of different sizes,
or using datasets with the same size but splitting the datasets with different random seeds.
To make the comparison clear and fair, we retrain all corresponding models using exactly the same training, validation and test sets across all methods
and report the results in Table.~\ref{mp-table}. To avoid confusion, we still put original results from referred papers in Table.~\ref{mp-table} inside parentheses. For retrained baseline models, we provide the detailed configurations in Appendix.~\ref{app:retrain}. The used metric is test MAE following previous studies~\citep{cgcnn, schnet,megnet,gatgnn,alignn}. 

It can be seen from Table.~\ref{mp-table} that Matformer achieves the best performances on all tasks consistently by significant margins. Specifically, it reduces the formation energy by 4.5\% of the second best model, which is a significant margin. Furthermore, for Bulk Moduli and Shear Moduli tasks with only 4664 training samples, Matformer achieves the best performances, indicating Matformer's adaptive ability to tasks of small training scales.




\begin{table}[t]
  \caption{Comparison between Matformer and other baselines in terms of test MAE on JARVIS dataset. The best results are shown in \textbf{bold} and the second best results are shown with \uline{underlines}.}
  \label{jarvis-table}
  \centering
  \begin{tabular}{lccccc}
    \toprule
& Formation Energy & Bandgap(OPT) & Total Energy & Ehull & Bandgap(MBJ) \\
    \cmidrule(r){2-6}
    Method & eV/atom  &  eV & eV/atom & eV & eV    \\
    \midrule
    CFID~\citep{cfid} & 0.14 &    0.30 & 0.24 & 0.22 &  0.53  \\
    CGCNN~\citep{cgcnn}  & 0.063 &   0.20 & 0.078 & 0.17 & 0.41  \\
    SchNet~\citep{schnet} & 0.045 &   0.19 & 0.047 & 0.14 & 0.43  \\
    MEGNET~\citep{megnet}  & 0.047 &   0.145 & 0.058 & 0.084 & 0.34 \\
    GATGNN~\citep{gatgnn}  & 0.047 &   0.17 & 0.056 & 0.12 & 0.51             \\
    ALIGNN~\citep{alignn} & \uline{0.0331} &  \uline{0.142} & \uline{0.037} & \uline{0.076} & \uline{0.31}  \\
    Matformer & \textbf{0.0325} & \textbf{0.137} & \textbf{0.035} & \textbf{0.064} & \textbf{0.30}   \\
    \bottomrule
  \end{tabular}
  \vspace{-3mm}
\end{table}



\begin{table}
  \caption{Efficiency comparison with ALIGNN on Jarvis Formation Energy task. We show the training time per epoch, total training time, inference time for the whole test set, and total number of parameters.}
  \label{tab:efficient}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    Models & Time/epoch & Total & Inference & Model Para. \\
    \midrule
    ALIGNN  &  327 s &  27.3 h &  156 s &  15.4 MB\\
    Matformer  &  64 s &  8.9 h &  59 s &  11.0 MB\\
    \bottomrule
  \end{tabular}
  \vspace{-3mm}
\end{table}
\textbf{JARVIS dataset}. The quantitative results for Jarvis are shown in Table.~\ref{jarvis-table}. Matformer outperforms the baseline methods significantly on all of these five tasks.
Compared with ALIGNN, Matformer has stronger discriminative ability due to explicit encoding of periodic patterns. Specifically, Matformer reduces Jarvis Ehull by 0.012, which is 15.8\% of ALIGNN. 
Furthermore, Matformer achieves the best performances for Bulk Moduli and Shear Moduli in the Mateirals Project with  4664 training samples, and Bandgap(MBJ) in JARVIS with 14537 training samples, indicating its adaptive ability to tasks of various data scales.
Overall, the superior performances show the effectiveness of periodic pattern encoding in our Matformer message passing.
In addition, compared with ALIGNN, our Matformer is more efficient. We evaluate the efficiency of Matformer by comparing with ALIGNN using JARVIS formation energy dataset. The mean time of ten runs for training and inference using best model configurations of ALIGNN and Matformer are reported. We also report the total number of parameters of each model.
In Table.~\ref{tab:efficient}, we show that Matformer is three times faster than ALIGNN in total training time and near three times faster in inference time, for the whole test set. Matformer is also much lighter than ALIGNN in terms of model size.



\textbf{Energy within Threshold}. Following OC20~\cite{ocp_dataset}, we use energy within threshold (EwT), which measures the percentage of estimated energies that are likely to be practically useful when the absolute error is within a certain threshold, to evaluate Matformer's capability for periodic graph learning. This metric is new, but is well recognized by the community as it is useful in practice.
Due to significant performance gaps between ALIGNN and other baseline methods on formation energy and total energy for these two datasets in terms of mean absolute error (MAE), we only compare Matformer with ALIGNN. Table.~\ref{ewt-table} shows that Matformer outperforms ALIGNN consistently for all three energy prediction tasks. Interestingly, the performance gains of our Matformer beyond ALIGNN in terms of EwT mainly come from more accurate energy predictions within absolute error of 0.01. Compared with JARVIS, the Materials Project has 15422 more traning samples. As a result, the percentage of predicted energies obtained by Matformer within 0.01 increases by 14.69\%, which is much better than ALIGNN, revealing the huge potential of Matformer when larger crystal dataset is available. 

\begin{table}
  \caption{Comparison between Matformer and ALIGNN in terms of EwT on JARVIS Formation Energy, JARVIS Total Energy and The Materials Project Formation Energy. We use EwT (0.02) to mark the threshold of 0.02 and EwT (0.01) to mark the threshold of 0.01. The best results are in \textbf{bold}.}
  \label{ewt-table}
  \centering
  \begin{tabular}{lcccccc}
    \toprule
& \multicolumn{2}{c}{Formation MP} & \multicolumn{2}{c}{Formation JARVIS} & \multicolumn{2}{c}{Total JARVIS} \\
    \cmidrule(r){2-7}
    Method & EwT (0.01) &  EwT (0.02) & EwT (0.01)  &  EwT (0.02)  &EwT (0.01) &  EwT (0.02)     \\
    \midrule
    ALIGNN & 49.94\% & 71.10\% & 39.59\% & 59.64\% & 35.09\% & 55.20\% \\
    Matformer & \textbf{55.86}\% & \textbf{75.02}\% & \textbf{41.17}\% & \textbf{60.25}\% & \textbf{36.84}\% & \textbf{57.36}\%  \\
    \bottomrule
  \end{tabular}
  \vspace{-6mm}
\end{table}


\subsection{Ablation studies}
\label{sec:ablation}

In this section, we demonstrate the importance of periodic invariance and explicit repeating pattern encoding for crystal representation learning by ablation studies. We also evaluate the building blocks particularly designed for our Matformer. Specifically, we conduct experiments on JARVIS formation energy, and the test MAE is used as the quantitative evaluation metric. We also provide ablation studies on the use of sigmoid and layernorm instead of softmax in Matformer layer in Appendix~\ref{app:building}.

\begin{table}
  \caption{Ablation studies on periodic invariance and periodic pattern encoding. We use OCgraph to denote graph construction method proposed by Graphormer. PI denotes periodic invariance and PE denotes periodic encoding.}
  \label{tab:ab}
  \centering
  \begin{tabular}{lcccccc}
        \toprule
        Graph  & PI & PE & layer & head & batch &Test MAE  \\
        \midrule
        OCgraph  &  & & 3& 1 & 32 & 0.0530  \\
        Radius w/o PE  & \checkmark & & 3& 1 &32 & 0.0348  \\
        Radius w/o PE & \checkmark & & 5& 4 &64 & 0.0337  \\
        T-fully w PE  & \checkmark & \checkmark & 5 & 4 & 64 & 0.0402  \\
        Radius w PE & \checkmark & \checkmark & 5 & 4 & 64& \textbf{0.0325}  \\
        \bottomrule
  \end{tabular}
  \vspace{-3mm}
\end{table}
\textbf{Periodic invariant graph construction}. We demonstrate the importance of periodic invariant graph construction by comparing radius multi-edge graph, denoted as Radius, with the graph construction method proposed by Graphormer, denoted as OCgraph, on the exactly same Matformer architecture. Note that the constructed crystal graphs by OCgraph are super large, containing more than  edges, where  is the atom number in a cell. We adjust the Matformer configurations to train these large graphs on a single RTX A6000 GPU. 
It can be seen from Table.~\ref{tab:ab} that when using OCgraph to our Matformer, the test MAE drops dramatically of 53\% because of breaking periodic invariance, compared with radius-based multi-edge graphs in Matformer. We also compare two periodic invariant graph construction methods described in Sec.~\ref{sec:graph}. We denote fully connected graph with  smallest pairwise distances as T-fully, and use . The result shows that Radius is better than T-fully.

\textbf{Encoding of repeating patterns}. We denote periodic pattern encoding as PE. In Table.~\ref{tab:ab}, we show that omitting the periodic pattern encoding results in a significant drop of test MAE from 0.0325 to 0.0337, revealing the importance of periodic patterns for crystal representation learning.

\begin{wraptable}[8]{r}{0.5\textwidth}
\vspace{-15 pt}
\begin{center}
\caption{Ablation studies on angular information. We show the training time per epoch, total training time, and test MAE.}
\vspace{-5pt}
\label{tab:angle}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
Models & MAE & Time/epoch & Total  \\\midrule
Matformer  &  .0325 &  64 s &  8.9 h \\
Matformer + Angle SBF  &  .0332 &  173 s &  23.8 h \\
Matformer + Angle RBF  &  .0325 &  165 s &  22.9 h \\
\bottomrule
\end{tabular}}
\end{center}
\end{wraptable}
\textbf{Complexity of introducing angular information.} Dropping angular information largely improves running efficiency of Matformer compared with ALIGNN. We show that for the original crystal graph with  nodes and  edges, the corresponding line graph with angles will have  nodes and  edges, leading to high computational cost.
The detailed complexity analysis of adding angular information are provided in Appendix.~\ref{app:time_angle}. 
Additionally, we provide the running time and performance analysis of Matformer with angle information in Table.~\ref{tab:angle}. We use two Matformer layers to process extra angular information, and use Radial Basis Function kernels~\citep{alignn} and Spherical Bessel Functions with Spherical Harmonics ~\citep{Dimenet, liu2021spherical,klicpera2021gemnet} to encode angles, denoted as Matformer + Angle RBF and Matformer + Angle SBF. Table.~\ref{tab:angle} shows that introducing angular information will increase both the training time per epoch and in total by around 3 times, without much performance gain. This may due to the periodic invariant graph construction and periodic patterns encoding in Matformer already capture sufficient information to identify crystal structures.


\section{Conclusions and discussions}
In this work, we first propose to formally define periodic invariance and periodic pattern encoding for periodic graph learning. We then propose Matformer for periodic graph representation learning, which is invariant to periodicity and can capture repeating patterns explicitly. Experimental results on common benchmark datasets show that our Matformer outperforms baseline methods consistently. In addition, our results demonstrate the importance of periodic invariance and explicit periodic pattern encoding for crystal representation learning. One potential direction beyond this work is to include angular information properly to satisfy both periodic invariance and to encode periodic patterns with relatively low time complexity, and this is one limitation of our work. Besides, negative societal impacts of material discovery may apply to our work.

\begin{ack}
We thank Tian Xie for answering our questions on CGCNN. This work was supported in part by National Science Foundation grant IIS-2006861.

\end{ack}


\bibliographystyle{plainnat}
\bibliography{sample}




































\newpage

\appendix


\section{Appendix}

\subsection{Cases breaking periodic invariance}
\label{app:breakingpi}

We show two graph construction methods that break periodic invariance in Fig.~\ref{fig:graph_graphormer} and Fig.~\ref{fig:nn_wrong}. 

One is the graph construction method employed by Graphormer in OC20~\citep{ocp_dataset}, shown in Fig.~\ref{fig:graph_graphormer}. It breaks periodic invariance because it treats every atom as a single node in the constructed graph. When the periodic boundaries are shifted, the inner structure can change a lot in a unit cell. Hence, the constructed fully connected graphs can be totally different when periodic boundaries are shifted.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{graphormer_graph.pdf}
    \caption{
    Illustration of graph construction method used by Graphormer~\citep{graphormer} in OC20~\citep{ocp_dataset}. 
    We use orange, blue, and light blue to mark different atoms. We use circles of the same colors as atoms to denote the corresponding radius for atoms. It can be seen from the comparison of the left and the right that the constructed graphs are totally different when the periodic boundaries are shifted. In particular, the constructed graph on the left has three blue atoms, but the constructed graph on the right has two blue atoms. Thus, the graph construction method employed by Graphormer in OC20 breaks periodic invariance for crystals.
    }
    \label{fig:graph_graphormer}
\end{figure}

Another is the graph construction method using nearest neighbors based only on geometric pairwise distances, employed by CGCNN~\citep{cgcnn} and GATGNN~\citep{gatgnn}, shown in Fig.~\ref{fig:nn_wrong}. If several different atoms have the same geometric distances to the center atom, there is no deterministic way to determine which one to choose. Thus, for these cases, this method breaks periodic invariance.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\linewidth]{nn_wrong.pdf}
    \caption{
    An illustration that crystal graph construction using nearest neighbors based only on pairwise distances breaks periodic invariance. We illustrate the one nearest neighbor case for simplicity. Black arrows denote the same pairwise distances from nearby atoms to the center blue atom. If sorting is based only on distances, there will be two different neighborhood formation approaches for the center blue atom, either using the light-blue atom or using the orange atom as the first nearest neighbor.
    }
    \label{fig:nn_wrong}
\end{figure}


\subsection{Proofs of periodic invariance}
\label{app:proof}

\textbf{Notations}. Recall that we use matrices , , and  to describe a given crystal structure.  is the atom feature matrix, where  is the -dimensional feature vector for atom  in the unit cell.
 is the position matrix, where  contains the Cartesian coordinates for atom  in 3D space.
And the infinite crystal structure can be represented as
    , 
    and 
    .
Here, set  contains all possible positions for each atom , and set  contains the corresponding atom feature vector for each atom . 

For crystal property prediction tasks,  representing the minimum repeating patterns is given. In the following proofs, we don't consider the case that the provided  for , which means the provided periodic patterns are not the minimum repeating patterns for a given crystal. When  representing the minimum repeating patterns is given, the periodic invariance lies in the invariance to  shifts of periodic boundaries. We prove that the multi-edge graph construction and fully connected graph construction for crystals employed by our Matformer satisfy periodic invariance.

\textbf{Multi-edge graph}. The Multi-edge graph satisfies the periodic invariance by forming edges between node  and  using all items in the set , where  is a prefixed threshold. It can be seen that all pairwise Euclidean distances between node  with positions , and node  with positions  are considered, and the shifts of periodic boundaries  will not influence the pairwise Euclidean distances. Thus, this method satisfies periodic invariance. Additionally, by using pairwise Euclidean distances, unit cell E(3) invariance is naturally satisfied. 

We note that for a given node , the radius can be computed by a deterministic function that takes all pairwise distances between node  and all other nodes as input, and produces a real value  as output. This will not break periodic invariance due to the fact that the deterministic function will always produce the same  for the same input . One example is that we can use the 12-th smallest distance   in  as the radius for node .

\textbf{Fully connected graph for crystals}. To construct fully connected graphs for crystals, in our design, node  represents atom with atom feature  and all its repeats in the infinite 3D space, with positions . In the -fully-connected graph for crystals, for node  and ,  smallest pairwise distances in  are considered, where  is a hyperparameter to control the edge number. Similar to the proof of multi-edge graph, all pairwise Euclidean distances between node  with positions , and node  with positions  are considered, and the shifts of periodic boundaries will not influence the pairwise Euclidean distances. Hence, this method satisfies periodic invariance. In addition, the usage of pairwise Euclidean distances makes the unit cell E(3) invariance naturally satisfied.


\subsection{Dataset descriptions}
\label{app:dataset_des}
We show the detailed dataset Descriptions for two crystal datasets used in experimental studies, including The Materials Project and JARVIS.

\textbf{The Materials Project dataset}. In particular,
for tasks of formation energy and band gap, we directly follow ALIGNN~\citep{alignn} and use the same training, validation, and test set, including 60000, 5000, and 4239 crystals, respectively. For tasks of Bulk Moduli and Shear Moduli, we follow GATGNN~\citep{gatgnn}, the recent state-of-the-art method for these two tasks, and use the same training, validation, and test sets, including 4664, 393, and 393 crystals. In Shear Moduli, one validation sample is removed because of the negative GPa value. 
We either directly use the publicly available codes from the authors, or re-implement models based on their official codes and configurations to produce the results. Detailed configurations of these retrained models are provided in Appendix.~\ref{app:retrain}. 

\textbf{The JARVIS dataset}. JARVIS is a newly released database proposed by \citet{choudhary2020joint}. For JARVIS dataset, we follow ALIGNN~\citep{alignn} and use the same training, validation, and test set. We evaluate our Matformer on five important crystal property tasks, including formation energy, bandgap(OPT), bandgap(MBJ), total energy, and Ehull. The training, validation, and test set contains 44578, 5572, and 5572 crystals for tasks of formation energy, total energy, and bandgap(OPT). The numbers are 44296, 5537, 5537 for Ehull, and 14537, 1817, 1817 for bandgap(MBJ). The used metric is test MAE. The results for CGCNN and CFID are taken from ALIGNN~\citep{alignn}, other baseline results are obtained by retrained models. Detailed configurations of these retrained models are provided in Appendix.~\ref{app:retrain}. 

\subsection{Matformer configurations}
\label{app:matconfig}
We show the detailed configurations of our Matformer models in this section.

\textbf{Notations}.  is the -dimensional feature vector for node ,  is -dimensional feature vector for the -th edge between nodes  and , and  is the input feature vector of node  for a given layer of Matformer. , , and  denote the linear transformations to compute query, key, and edge embedding in a given Matformer layer, respectively. ,  and  are the computed query, key, and value vectors after these linear transformations for node .
 is the intermediate output in a given Matformer layer for . 

\textbf{Crystal graph construction}. For all Matformer models used in two material datasets,  including The Materials Project and JARVIS, we use the radius-based multi-edge graph construction method. We use the 12-th smallest distance between the given atom and all nearby atoms as radius, and include all nearby atoms within the radius as the neighborhood for the given atom. It satisfies periodic invariance as described in Appendix.~\ref{app:proof}. For the encoding of periodic patterns, we use six geometric distances, including , , ,
, , and 
in our study. For any of these distances, if it is already in the radius of node  computed by the previous graph construction method, it will not be added as a self-connecting edge of node  in the final graph.

\textbf{Node and edge embeddings}. For each node, we map the atomic number to a 92-dimensional embedding using CGCNN~\citep{cgcnn} atomic embedding. We then use a linear transformation to map it to a 128-dimensional vector as the input  to the first Matformer message passing layer. For each edge, we map the Euclidean distance to a 128-dimensional embedding using 128 RBF kernels with centers from 0.0 to 8.0. 
It is then mapped to a 128-dimensional vector as the edge input , by a nonlinear layer followed by a linear layer.

\textbf{Matformer layer}. For message passing,
we use five layers of Matformer Message Passing layer, with four attention heads. The embedding sizes for , ,   for a single head are 128, mapped from  using corresponding linear transformations , , and .
For a given layer, different heads use different , , , and , but share other operations. To obtain the final message , the features from four heads are concatenated and mapped to a 128-dimensional vector by a linear transformation.

\textbf{Readout layer}. We use the mean pooling to aggregate features from all nodes in a graph and then use a nonlinear layer with hidden dimension 128 followed by a linear layer to obtain the scalar output for a crystal graph.

\textbf{Training hyperparameters}. For all tasks in The Materials Project and JARVIS, we use the Adam optimizer~\citep{kingma2015adam} with weight decay~\citep{weightdecay} of 1e-5 and one cycle learning rate scheduler~\citep{onecycle}. We use the batch size of 64. We use mean square error as the objective function to train and mean absolute error as the evaluation metric to validate and test. We only slightly adjust the learning rates and training epochs for different tasks in The Materials Project and JARVIS, as shown in Table.~\ref{tab:hyper-mp} and Table.~\ref{tab:hyper-jarvis}, respectively. We use Pytorch to implement our models. For all tasks on two benchmark datasets, we use one NVIDIA RTX A6000 48GB GPU to train our Matformer models. 


\begin{table}[h]
    \centering
    \caption{Training hyperparameters for Matformer models on The Materials Project.}
    \begin{tabular}{lcccc}
        \toprule
        Parameter  & Formation Energy & Bandgap & Bulk Moduli & Shear Moduli  \\
        \midrule
        Learning rate  & 1e-3 & 5e-4 & 1e-3 & 1e-3   \\
        Epoch number  & 500 & 500 & 500 & 300  \\
        \bottomrule
    \end{tabular}
    \label{tab:hyper-mp}
\end{table}

\begin{table}[h]
    \centering
    \caption{Training hyperparameters for Matformer models on JARVIS.}
    \begin{tabular}{lccccc}
        \toprule
        Parameter  & Formation Energy & Bandgap(OPT) & Total Energy & Ehull & Bandgap(MBJ)  \\
        \midrule
        Learning rate  & 1e-3 & 8e-4 & 1e-3 & 1e-3 & 1e-3   \\
        Epoch number  & 500 & 300 & 500 & 500 & 300  \\
        \bottomrule
    \end{tabular}
    \label{tab:hyper-jarvis}
\end{table}

We also show the detailed Matformer architecture in Fig.~\ref{fig:net_arch}.


\subsection{Configurations of retrained models}
\label{app:retrain}
We show configurations of retrained models for The Materials Project and JARVIS in this section. The results from their original papers are shown in Table.~\ref{mp-table-app}.

\textbf{SchNet}~\citep{schnet}. We use six layers of SchNet message passing layer following the original paper, with feature dimension of 64. We train SchNet on these four tasks with learning rate of 5e-4 and batch size of 64 for 500 epochs. The Adam optimizer is used with 1e-5 weight decay. One cycle learning rate scheduler is also used. We use the 12-th smallest distance between the given atom and all nearby atoms to serve as the radius for this given atom for the tasks of formation energy and bandgap in The Materials Project and all tasks in JARVIS. For the tasks of shear moduli and bulk moduli, We use the 32-th smallest distance between the given atom and all nearby atoms to serve as the radius for every atom to better the performance. 

\textbf{MEGNET}~\citep{megnet}. Following the original paper, we use three layers of MEGNET message passing layer with the same feature dimensions as mentioned in the paper, and use Set2Set readout function. We train MEGNET on these four tasks with learning rate of 1e-3 and batch size of 128 for 1000 epochs following the configuration settings mentioned in the original paper. The Adam optimizer is used with 1e-5 weight decay. One cycle learning rate scheduler is also used. We try different hyperparameters of crystal graph construction and batch size to better the performances of MEGNET. In particular, for formation energy and bandgap, we use a radius of 4.0 for all atoms with batch size of 128. And for bulk moduli and shear moduli and all tasks in JARVIS, we use the 12-th smallest distance between the given atom and all nearby atoms to serve as the radius for this given atom to better the performance, and the batch size of 64 is chosen because of better empirical results.

\textbf{CGCNN}~\citep{cgcnn}. For CGCNN, we directly use the publicly available code from \citet{cgcnn}, with 128 hidden dimensions, batch size of 256, and three layers of CGCNN message passing layer. We train CGCNN with Adam optimizer because of better empirical results. We train CGCNN for these four tasks using learning rate of 1e-2 for 100 epochs and 1e-3 for the next 900 epochs following the official code. The radius cutoff of 8.0 is used for all atoms, and the nearest 12 neighbors are selected.

\textbf{GATGNN}~\citep{gatgnn}. For GATGNN, we directly use the publicly available code from \citet{gatgnn}, with 128 hidden dimensions, batch size of 256 and keep other default settings. We train GATGNN with learning rate of 5e-3 with learning rate decay milestone of 300 epochs and decay parameter of 0.5. We train GATGNN for 500 epochs for the task of formation energy and bandgap in The Materials Project and all tasks in JARVIS, with early stop. The radius cutoff of 4.0 is used for all atoms, and the nearest 16 neighbors are selected according to the original paper.

\textbf{ALIGNN}~\citep{alignn}. For ALIGNN, we directly use the publicly available code from \citet{alignn}. We use the official best model configurations of ALIGNN to train ALIGNN models on the tasks of bulk moduli and shear moduli, with learning rate of 1e-3 and batch size of 64. In particular, we use ALIGNN with four gcn layers and four alignn layers.

Overall, we either directly use the publicly available codes from corresponding authors~\citep{cgcnn,gatgnn,alignn}, or re-implement models based on their official codes and configurations~\citep{schnet,megnet} to produce the results in our experiments.

\begin{table}[t]
  \vspace{-4mm}
  \caption{Comparison 
in terms of test MAE on The Materials Project dataset. We show results both from retrained models and referred papers to make the comparison clear and fair. 
Results from original papers are shown in parentheses () on the right. - denotes no results are reported in referred papers. The best results are shown in \textbf{bold} and the second best results are shown with \uline{underlines}.}
  \label{mp-table-app}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    & Formation Energy & Band Gap & Bulk Moduli & Shear Moduli \\
    \cmidrule(r){2-5}
    Method & eV/atom  &  eV &   log(GPa) & log(GPa)  \\
    \midrule
    CGCNN~\citep{cgcnn} & 0.031~(0.039) & 0.292~(0.388)  & 0.047~(0.054) &0.077~(0.087) \\
    SchNet~\citep{schnet} & 0.033~(0.035) & 0.345~(-) & 0.066~(-) & 0.099~(-) \\
    MEGNET~\citep{megnet} & 0.030~(0.028) & 0.307~(0.33) & 0.060~(0.050) & 0.099~(0.079) \\
    GATGNN~\citep{gatgnn} & 0.033~(0.039) & 0.280~(0.31) & \uline{0.045} & \uline{0.075} \\
    ALIGNN~\citep{alignn} & \uline{0.022} & \uline{0.218} & 0.051~(-) & 0.078~(-) \\
    Matformer & \textbf{0.021} & \textbf{0.211} & \textbf{0.043} & \textbf{0.073} \\
    \bottomrule
  \end{tabular}
  \vspace{-4mm}
\end{table}


\subsection{Building block of Matformer}
\label{app:building}
We provide ablation studies about sigmoid and layernorm instead of softmax in our Matformer message passing layer in this section.


\begin{wraptable}[7]{r}{0.35\textwidth}
\vspace{-20 pt}
\begin{center}
\caption{Operation ablation study.}
\vspace{-5pt}
\label{tab:buildingblock}
\resizebox{0.35\textwidth}{!}{
\begin{tabular}{lcc}
\toprule
Operation & Test MAE \\\midrule
Softmax-scalar  &  0.0376 \\
Softmax-vector  &  0.0347 \\
Sigmoid and Norm  &  \textbf{0.0325}  \\
\bottomrule
\end{tabular}}
\end{center}
\end{wraptable}
We evaluate the operation of sigmoid and normalization instead of softmax in our message passing, which is designed to capture the node degree information in multi-edge crystal graphs, described in Sec.~\ref{sec:msg}. Compared with using softmax with scalar attention coefficient, denoted as Softmax-scalar, or softmax with vector attention coefficient, denoted as Softmax-vector, using sigmoid and normalization leads to significant performance improvement in terms of test MAE, justifying the effectiveness of this operation in Matformer message passing design, as shown in Table.~\ref{tab:buildingblock}.



\subsection{Complexity analysis of introducing angular information}
\label{app:time_angle}
We provide the complexity analysis of adding angular information to Matformer in this section.

Assume that we have  atoms in a single cell and thus  nodes in the original multi-edge graph. Also assume that every node has at least  neighbors following the graph construction method of Matformer and assume there are no self-connecting edges. This will result in a graph  where  and . When converting graph  into line graph , every edge is treated as a node in the line graph. So we have  nodes in the line graph. Every edge in the original graph is connecting  nodes and every node has  other edges, resulting in  neighboring edges for each edge in the original graph. So we have  edges in the converted line graph. Compared with original graph with  and , the converted line graph is super large with node number of  and edge number of .




























\end{document}
