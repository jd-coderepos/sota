\documentclass{article}

\pdfoutput=1

\usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amssymb,amsmath,amsthm}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{fullpage}
\usepackage{tikz} 
\usepackage{pgfplots}
\usetikzlibrary{arrows.meta}

\renewcommand{\Pr}{\field{P}}

\DeclareMathOperator{\Tr}{Tr}
\newcommand{\bA}{\boldsymbol{A}}
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bc}{\boldsymbol{c}}
\newcommand{\bC}{\boldsymbol{C}}
\newcommand{\bd}{\boldsymbol{d}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bM}{\boldsymbol{M}}
\newcommand{\bone}{\boldsymbol{1}}
\newcommand{\bI}{\boldsymbol{I}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\bhatY}{\boldsymbol{\hat{Y}}}
\newcommand{\bbary}{\boldsymbol{\bar{y}}}
\newcommand{\bg}{\boldsymbol{g}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bZ}{\boldsymbol{Z}}
\newcommand{\bbarZ}{\boldsymbol{\bar{Z}}}
\newcommand{\bbarz}{\boldsymbol{\bar{z}}}
\newcommand{\bhatZ}{\boldsymbol{\hat{Z}}}
\newcommand{\bhatz}{\boldsymbol{\hat{z}}}
\newcommand{\bhatx}{\boldsymbol{\hat{x}}}
\newcommand{\haty}{\hat{y}}
\newcommand{\barz}{\bar{z}}
\newcommand{\bS}{\boldsymbol{S}}
\newcommand{\bbarS}{\boldsymbol{\bar{S}}}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\bhatw}{\hat{\boldsymbol{w}}}
\newcommand{\bW}{\boldsymbol{W}}
\newcommand{\bU}{\boldsymbol{U}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bzero}{\boldsymbol{0}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\sC}{\mathcal{C}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sS}{\mathcal{S}}
\newcommand{\sZ}{\mathcal{Z}}
\newcommand{\sbarZ}{\bar{\mathcal{Z}}}
\newcommand{\fbag}{\bold{F}}


\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Wealth}{Wealth}
\DeclareMathOperator{\Reward}{Reward}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\fixme}[1]{\textcolor{red}{FIXME: #1}}

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\fY}{\field{Y}}
\newcommand{\fX}{\field{X}}
\newcommand{\fH}{\field{H}}

\newcommand{\R}{\field{R}}
\newcommand{\Nat}{\field{N}}
\newcommand{\E}{\field{E}}
\newcommand{\Var}{\mathrm{Var}}

\renewcommand{\H}{\mathcal{H}}  \newcommand{\norm}[1]{\left\|{#1}\right\|}
\newcommand{\indicator}{\mathbf{1}}
\DeclareMathOperator*{\dom}{dom}


\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbartheta}{\boldsymbol{\bar{\theta}}}
\newcommand\theset[2]{ \left\{ {#1} \,:\, {#2} \right\} }
\newcommand\inn[2]{ \left\langle {#1} \,,\, {#2} \right\rangle }
\newcommand\RE[2]{ D\left({#1} \| {#2}\right) }
\newcommand\Ind[1]{ \left\{{#1}\right\} }
\newcommand{\diag}[1]{\mbox{\rm diag}\!\left\{{#1}\right\}}

\newcommand{\defeq}{\stackrel{\rm def}{=}}
\newcommand{\sgn}{\mbox{\sc sgn}}
\newcommand{\scI}{\mathcal{I}}
\newcommand{\scO}{\mathcal{O}}

\newcommand{\dt}{\displaystyle}
\renewcommand{\ss}{\subseteq}
\newcommand{\wh}{\widehat}
\newcommand{\ve}{\varepsilon}
\newcommand{\hlambda}{\wh{\lambda}}
\newcommand{\yhat}{\wh{y}}

\newcommand{\hDelta}{\wh{\Delta}}
\newcommand{\hdelta}{\wh{\delta}}
\newcommand{\spin}{\{-1,+1\}}



\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{cor}{Corollary}


\newcommand{\reals}{\mathbb{R}}


\newcommand{\bbeta}{{\boldsymbol{\beta}}}
\newcommand{\bgamma}{{\boldsymbol{\gamma}}}
\newcommand{\tp}{^{\top}}
\newcommand{\ip}[1]{\left\langle #1 \right\rangle}
\newcommand{\ME}[1]{\mathbb{E}_{#1}}
\newcommand{\Sloo}{S^{\backslash i}}
\newcommand{\loo}{_{\Sloo}}
\newcommand{\fhtlold}{f^{htl}_{S}}
\newcommand{\fhtl}{f^{htl'}_{S}}
\newcommand{\fhtlloo}{f^{htl'}\loo}
\newcommand{\fh}{\hat{f}}
\newcommand{\dm}[1]{\text{dim }\left(#1\right)}

\newtheorem{alg}{Algorithm}


\newcommand{\alert}[1]{\footnote{\color{red}! NOTE: #1 \textexclamdown\par}}


\newcommand{\KL}[2]{\operatorname{D}\left({#1}\middle\|{#2}\right)}  

\DeclareMathOperator{\Risk}{Risk} 
\title{Training Deep Networks without Learning Rates\\Through Coin Betting}



\author{
  Francesco Orabona\thanks{The authors contributed equally.} \\
  Department of Computer Science\\
  Stony Brook University, NY, USA\\
  \texttt{francesco@orabona.com}
  \and
  Tatiana Tommasi\footnotemark[1] \\
  Department of Computer, Control, and Management Engineering Antonio Ruberti\\
  Sapienza University of Rome, Italy \\
  \texttt{tommasi@dis.uniroma1.it}
}

\begin{document}

\maketitle

\begin{abstract}
Deep learning methods achieve state-of-the-art performance in many application scenarios. 
Yet, these methods require a significant amount of hyperparameters tuning in order to achieve 
the best results. In particular, tuning the learning rates in the stochastic optimization 
process is still one of the main bottlenecks.
In this paper, we propose a new stochastic gradient descent procedure for deep networks that 
does not require any learning rate setting. Contrary to previous methods, we do not adapt the 
learning rates nor we make use of the assumed curvature of the objective function. Instead, 
we reduce the optimization process to a game of betting on a coin and propose a learning-rate-free
optimal algorithm for this scenario.
Theoretical convergence is proven for convex and quasi-convex functions and empirical evidence shows 
the advantage of our algorithm over popular stochastic gradient algorithms.
 \end{abstract}

\section{Introduction}

In the last years deep learning has demonstrated a great success in a large number of fields
and has attracted the attention of various research communities with the consequent development
of multiple coding frameworks (e.g., Caffe~\citep{jia2014caffe}, TensorFlow~\citep{tensorflow2015-whitepaper}), 
the diffusion of blogs, online tutorials, books, and dedicated courses. Besides reaching out scientists
with different backgrounds, the need of all these supportive tools originates also from the nature 
of deep learning:~it is a methodology that involves many structural details as well as several hyperparameters
whose importance has been growing with the recent trend of designing deeper and multi-branches networks. Some of the hyperparameters define the model itself (e.g., number of hidden layers, regularization coefficients, kernel size for convolutional layers), while others are related 
to the model training procedure.
In both cases, hyperparameter tuning is a critical step to realize deep learning 
full potential and most of the knowledge in this area comes from living practice, years 
of experimentation, and, to some extent, mathematical justification~\citep{BookBengio2012}. 

With respect to the optimization process, stochastic gradient descent (SGD) has proved itself to be 
a key component of the deep learning success, but its effectiveness strictly depends on the choice of
the initial learning rate and learning rate schedule. This has primed a line of research on algorithms
to reduce the hyperparameter dependence in SGD---see Section~\ref{sec:rel} for an overview on the related literature. However,
all previous algorithms resort on adapting the learning rates, rather than removing them, or rely on assumptions on the shape of the objective function.

In this paper we aim at removing at least one of the hyperparameter of deep learning models. We leverage over recent advancements in the stochastic optimization literature to design 
a backpropagation procedure that does not have a learning rate at all, yet it is as simple as the vanilla SGD.
Specifically, we reduce the SGD problem to the game of betting on a coin (Section~\ref{sec:coin}). 
In Section~\ref{sec:cocob}, we present a novel strategy to bet on a coin that extends previous ones in a data-dependent way, 
proving optimal convergence rate in the convex and quasi-convex setting (defined in Section~\ref{sec:def}). 
Furthermore, we propose a variant of our algorithm for deep networks (Section~\ref{sec:backprop}). 
Finally, we show how our algorithm outperforms popular optimization methods in the deep learning 
literature on a variety of architectures and benchmarks (Section~\ref{sec:exp}).

 \section{Related Work}
\label{sec:rel}

Stochastic gradient descent offers several challenges in terms of convergence speed.
Hence, the topic of learning rate setting has been largely investigated.

Some of the existing solutions are based on the use of carefully tuned momentum terms~\citep{LeCunBOM98,SutskeverMDH13,KingmaB15}.
It has been demonstrated that these terms can speed-up the convergence for convex smooth functions~\citep{Nesterov83}.
Other strategies propose scale-invariant learning rate updates to deal with gradients whose magnitude
changes in each layer of the network~\citep{DuchiHS11,TielemanH12,Zeiler12,KingmaB15}. Indeed, scale-invariance is a
well-known important feature that has also received attention outside of the deep learning community~\citep{RossML13,OrabonaCCB15,OrabonaP15}.
Yet, both these approaches do not avoid the use of a learning rate.

A large family of algorithms exploit a second order approximation of the cost function to better
capture its local geometry and avoid the manual choice of a learning rate. The step size is automatically
adapted to the cost function with larger/shorter steps in case of shallow/steep curvature.
Quasi-Newton methods~\citep{WrightN99} as well as the natural gradient method~\citep{Amari98} belong
to this family. Although effective in general, they have a spatial and computational complexity that is square
in the number of parameters with respect to the first order methods, which makes the application
of these approaches unfeasible in modern deep learning architectures. Hence, typically the required matrices are approximated with diagonal ones~\citep{LeCunBOM98,SchaulZLC13}. Nevertheless, even assuming
the use of the full information, it is currently unclear if the objective functions in deep learning have
enough curvature to guarantee any gain.

There exists a line of work on unconstrained stochastic gradient descent without learning rates~\citep{Streeter-McMahan-2012,Orabona-2013,McMahan-Orabona-2014,Orabona-2014,CutkoskyB16,CutkoskyB17}.
The latest advancement in this direction is the strategy of reducing stochastic subgradient descent to coin-betting, proposed by~\citet{OrabonaP16b}.
However, their proposed betting strategy is worst-case with respect to the gradients received and cannot
take advantage, for example, of sparse gradients.











 \section{Definitions}
\label{sec:def}

We now introduce the basic notions of convex analysis that are used in the paper---see, e.g., \citet{Bauschke-Combettes-2011}.
We denote by  the -norm in . Let , the \emph{Fenchel conjugate} of  is  with .


A vector  is a \emph{subgradient} of a convex function  at  if  for any  in the domain of . The \emph{differential set} of  at , denoted by , is the set of all the subgradients of  at . If  is also differentiable at , then  contains a single vector, denoted by , which is the \emph{gradient} of  at .

We go beyond convexity using the definition of weak quasi-convexity in \citet{HardtMR16}. 
This definition is relevant for us because \citet{HardtMR16} proved that -weakly-quasi-convex objective functions arise in the training of linear recurrent networks.
A function  is \emph{-weakly-quasi-convex} over a domain  with respect to the global minimum  if there is a positive constant 
such that for all , . From the definition, it follows that differentiable convex function are also -weakly-quasi-convex.

\paragraph{Betting on a coin.}
We will reduce the stochastic subgradient descent procedure to betting on a number of coins. Hence, here we introduce the betting scenario and its notation. We consider a gambler making repeated bets on the
outcomes of adversarial coin flips. The gambler starts with initial
money . In each round , he bets on the outcome of a coin
flip , where  denotes heads and  denotes tails.  We
do not make any assumption on how  is generated.

The gambler can bet any amount on either heads or tails. However, he is not
allowed to borrow any additional money. If he loses, he loses the betted
amount; if he wins, he gets the betted amount back and, in addition to that, he
gets the same amount as a reward.  We encode the gambler's bet in round  by
a single number . The sign of  encodes whether he is betting on heads
or tails. The absolute value encodes the betted amount.  We define 
as the gambler's wealth at the end of round  and  as the
gambler's net reward (the difference of wealth and the initial money), that is

In the following, we will also refer to a bet with , where 
is such that

The absolute value of  is the \emph{fraction} of the current wealth to
bet and its sign encodes whether he is betting on heads or tails. The
constraint that the gambler cannot borrow money implies that .
We also slighlty generalize the problem by allowing the outcome of the coin flip
 to be any real number in , that is a \emph{continuous coin}; wealth and reward in~\eqref{equation:def_wealth_reward} remain the same.
 \section{Subgradient Descent through Coin Betting}
\label{sec:coin}

In this section, following \citet{OrabonaP16b}, we briefly explain how to reduce subgradient 
descent to the gambling scenario of betting on a coin.


Consider as an example the function  and the optimization problem  .
This function does not have any curvature, in fact it is not even differentiable, thus
no second order optimization algorithm could reliably be used on it.
We set the outcome of the coin flip 
to be equal to the negative subgradient of  in , that is , where we remind that  is the amount of money we bet. 
Given our choice of , its negative subgradients are in .
In the first iteration we do not bet, hence  and our initial money is \H(\cdot)TH(\sum_{t=1}^T g_t)g_1, \cdots, g_T\tfrac{1}{T}\sum_{t=1}^T w_tx^*F(x)HH\beta_t=\tfrac{\sum_{i=1}^{t-1} g_i}{t}L_i>0, i=1,\cdots,d\bw_1\in \R^dTFG_{0,i}\leftarrow L_i\Reward_{0,i}\leftarrow0\theta_{0,i}\leftarrow0i=1,\cdots, dt=1,2,\dots, T\bg_{t}\E[\bg_t] \in \partial[- F(\bw_t)]i=1,2,\dots, dG_{t,i} \leftarrow G_{t-1,i} + |g_{t,i}|\Reward_{t,i} \leftarrow \Reward_{t-1,i} + (w_{t,i}-w_{1,i}) g_{t,i}\theta_{t,i} \leftarrow \theta_{t-1,i} + g_{t,i}\beta_{t,i}=\frac{1}{L_i}\left(2\sigma\left(\tfrac{2\theta_{t,i}}{G_{t,i}+ L_{i}}\right)-1\right)\sigma(x)=\tfrac{1}{1+\exp(-x)}w_{t+1,i} \leftarrow w_{1,i}+ \beta_{t,i} \left(L_i + \Reward_{t,i}\right)\bar{\bw}_T=\frac{1}{T} \sum_{t=1}^T \bw_{t}\bw_II1TF: \R^d \rightarrow \Rdg_{t,i}\in [-L_{i}, L_{i}]i=1,\cdots,di\bg_tF\bw_t\tfrac{1}{L_i}\left(2\sigma\left(\tfrac{2\theta_{t,i}}{G_{t,i}+ L_{i}}\right)-1\right)\sigma(x)=\tfrac{1}{1+\exp(-x)}\tfrac{\theta_{t,i}}{G_{t,i}+ L_{i}}w_{t,i}|w_{t,i} g_{t,i}| < \Wealth_{t-1,i}L_iw_{t,i}w_{t,i}\bw^*FF:\R^d \rightarrow \R\tau\bg_t|g_{t,i}|\leq L_iTIF\tau=1\bw_T\beta_{t,i}g_{i,t}L_iTL_i \exp\left(\tfrac{\theta_{T,i}^2}{2 L_i (G_{T,i}+L_i)}-\tfrac{1}{2}\ln\tfrac{G_{T,i}}{L_i}\right)|g_{t,i}|=1\beta_{t,i} \approx \tfrac{\sum_{i=1}^{t-1} g_i}{t}G_{t,i}G_{T,i}L_i T\tilde{O}(\tfrac{\|\bw^*\|_1}{\sqrt{T}})|w^*_i|O(\tfrac{1}{\sqrt{T}} \sum_{i=1}^d (\tfrac{(\bw^*)^2}{\eta_i}+\eta_i))\eta_i\eta_i\eta_i\eta_iO(\tfrac{\|\bw^*\|_1}{\sqrt{T}})\bw^*\bw^*xyxyy=|x-10|F(x)=|x-10|w_t\tfrac{\eta}{\sqrt{t}}\eta\tilde{\eta}_t := w_t \sqrt{\sum_{i=1}^t g_i^2}\alpha>0\bw_1\in \R^dTFL_{0,i}\leftarrow0G_{0,i}\leftarrow0\Reward_{0,i}\leftarrow0\theta_{0,i}\leftarrow0i=1,\cdots, \text{number of parameters}t=1,2,\dots, T\bg_{t}\E[\bg_t] \in \partial [-F(\bw_t)]iL_{t,i} \leftarrow \max(L_{t-1,i}, |g_{t,i}|)G_{t,i} \leftarrow G_{t-1,i} + |g_{t,i}|\Reward_{t,i} \leftarrow \max(\Reward_{t-1,i} + (w_{t,i}-w_{1,i}) g_{t,i},0)\theta_{t,i} \leftarrow \theta_{t-1,i} + g_{t,i}w_{t,i} \leftarrow w_{1,i} + \tfrac{\theta_{t,i}}{L_{t,i} \max(G_{t,i}+L_{t,i},\alpha L_{t,i})} \left(L_{t,i} +\Reward_{t,i}\right)\bw_{T}L_{t,i}L_{i,t-1}g_{t,i}\Reward_{t,i}2\sigma(2x)-1\approx xx\in[-1,1]\alpha L_{t,i}\etaw_{2,i}=w_{1,i}-\eta \sgn(g_{1,i})\etaw_{1,i}w_{2,i}=w_{1,i}-\frac{1}{\alpha}\sgn(g_{1,i})\frac{1}{\alpha}w_{1,i}28\times 285 \times 52 \times 232\times325 \times 5 \times 33 \times 3 \times 3\sim 0.008[-0.05, 0.05]\tau1.4M National Science Foundation grant (\#1531492). 
The authors also thank Akshay Verma for the help with the TensorFlow implementation and Matej Kristan 
for reporting a bug in the pseudocode in the previous version of the paper. T.T. was supported by the 
ERC grant 637076 - RoboExNovo. F.O. is partly supported by a Google Research Award.


\bibliography{learning}
\bibliographystyle{abbrvnat}

\appendix

\section{Proof of Theorem~\ref{theo:main}}
\label{sec:proof_main}

First we state some technical lemmas that will be used in the proof of the convergence rate of COCOB.

\begin{lemma}{\cite[extended version, Lemma~18]{OrabonaP16b}}
\label{lemma:dual}
Define , for . Then

\end{lemma}

\begin{lemma}
\label{lemma:recurrence}
Let . Then, with the notation of Algorithm~\ref{algorithm:COCOB}, for any  we have

\end{lemma}
\begin{proof}
The statement to prove is equivalent to

where for clarity we dropped the index .

Consider the function 

We have that  is piece-wise convex on  and . Hence, we have that

Also,  is such that .
Hence, we have

that is


Using this relation we have that

We now use the Taylor expansion, to obtain

and, using the expression of , have

Hence the expression 

is greater than zero if , that is true by definition of .
\end{proof}

We can now prove Theorem~\ref{theo:main}.

\begin{proof}[Proof of Theorem~\ref{theo:main}]
First, assume that , then we will show how to remove this assumption.

Define .
By induction, we first prove that . For , it is obvious because . We now assume that . Note that .
Hence, using Lemma~\ref{lemma:recurrence}, we have

that proves the induction.

Now, in the convex case, using the fact that the stochastic subgradient are unbiased, the definition of the subgradients, and Jensen's inequality, we have


While, in the the -weakly-quasi-convex case, we have

by fact that the gradient are unbiased, the definition of , and the definition of -weakly-quasi-convexity.
Hence, the two cases are the same up to the factor . We can then proceed in both cases with


Using the definition of Fenchel's conjugate, \eqref{eq:conv_wealth} and the lower bound on the wealth in \eqref{eq:bound_wealth}, we have


Also, the concavity of the logarithm implies that  for all . Hence


Using Lemma~\ref{lemma:dual}, the inequality in \eqref{eq:bound_log}, and overapproximating, we have


Putting all together, using Jensen's inequality to bring the expectation under the square root, and dividing by  give us the stated bound, with .

Now, running the algorithm on the function , for an arbitrary , would result in the update in Algorithm~\ref{algorithm:COCOB} and would guarantee the same upper bound on  that implies the stated bound.
\end{proof}

 
\end{document}
