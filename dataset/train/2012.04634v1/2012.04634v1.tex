

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  



\IEEEoverridecommandlockouts                              

\overrideIEEEmargins                                      
















\usepackage{xcolor, soul}
\sethlcolor{pink}
\newcommand{\MD}[1]{{\color{purple}\textbf{[MD:} \textit{#1}\textbf{]}}}

\usepackage{xparse, mathtools, array}
\DeclarePairedDelimiterX{\rvect}[1]{[}{]}{\,\makervect{#1}\,}
\ExplSyntaxOn
\NewDocumentCommand{\makervect}{m}
 {
  \seq_set_split:Nnn \l_tmpa_seq { , } { #1 }
  \begin{matrix}
  \seq_use:Nn \l_tmpa_seq { & }
  \end{matrix}
 }
\ExplSyntaxOff
\newcommand{\Transp}{\mathsf{T}}

\usepackage{amssymb}

\newcommand{\parsection}[1]{\vspace{2mm}\noindent\textbf{#1}~ }



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{breakcites} 

\usepackage{booktabs}
\usepackage{multirow}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
\algnewcommand\Or{\textbf{or}}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage[caption=false]{subfig}

\usepackage{pgfplots}
    \usepgfplotslibrary{colorbrewer}
    \pgfplotsset{
cycle list/Dark2,
cycle multiindex* list={
            mark list*\nextlist
            Dark2\nextlist
        },
    }
\pgfplotsset{compat=1.14}















\title{\LARGE \bf
Accurate 3D Object Detection using Energy-Based Models
}




\author{Fredrik K. Gustafsson, Martin Danelljan, and Thomas B. Sch\"on\thanks{Department of Information Technology, Uppsala University, Sweden.}\thanks{Computer Vision Lab, ETH Z\"urich, Switzerland.}\thanks{{\tt\small \{fredrik.gustafsson,thomas.schon\}@it.uu.se}}\thanks{{\tt\small martin.danelljan@vision.ee.ethz.ch}}\thanks{This research was financially supported by the Swedish Foundation for Strategic Research via the project \emph{ASSEMBLE}, the Knut and Alice Wallenberg Foundation via the \emph{Wallenberg AI, Autonomous Systems and Software Program (WASP)}, and the \emph{Kjell \& M\"arta Beijer Foundation}.}}



\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}
Accurate 3D object detection (3DOD) is crucial for safe navigation of complex environments by autonomous robots. Regressing accurate 3D bounding boxes in cluttered environments based on sparse LiDAR data is however a highly challenging problem. We address this task by exploring recent advances in conditional energy-based models (EBMs) for probabilistic regression. While methods employing EBMs for regression have demonstrated impressive performance on 2D object detection in images, these techniques are not directly applicable to 3D bounding boxes. In this work, we therefore design a differentiable pooling operator for 3D bounding boxes, serving as the core module of our EBM network. We further integrate this general approach into the state-of-the-art 3D object detector SA-SSD. On the KITTI dataset, our proposed approach consistently outperforms the SA-SSD baseline across all 3DOD metrics, demonstrating the potential of EBM-based regression for highly accurate 3DOD. Code is available at \url{https://github.com/fregu856/ebms_3dod}. 



\end{abstract}





\section{INTRODUCTION}
\label{section:introduction}

3D object detection (3DOD) is a key perception task for self-driving vehicles and other autonomous robots. 3DOD entails detecting various objects from sensor data, and estimating their size and position in the 3D world. Specifically, the goal of 3DOD is to place oriented 3D bounding boxes which tightly contain all surrounding objects of interest. See Fig.~\ref{fig:3dod} for an example. These 3D bounding boxes then serve as input to important high-level tasks such as planning and collision avoidance. Accurate 3DOD is thus crucial for safe autonomous navigation of different complex environments.

In the automotive domain, 3DOD is usually performed from LiDAR point clouds \cite{yang2019std, shi2020points, shi2020pv}, images captured by vehicle-mounted cameras \cite{simonelli2019disentangling, chen2020monopair, shi2020distance}, or from a combination of both data modalities \cite{liang2019multi, liang2018deep, ku2018joint}. Radar sensors are sometimes also utilized \cite{meyer2019automotive, meyer2019deep, yang2020radarnet}. State-of-the-art 3D object detectors employ deep neural networks (DNNs) to learn powerful feature representations directly from this data \cite{shi2020pv, pang2020CLOCs, yoo20203d}. The 3DOD task is then commonly divided into two sub-tasks, in which anchor or proposal 3D bounding boxes are classified as either background or a specific class of object, and then regressed toward ground truth boxes \cite{zhou2018voxelnet, lang2019pointpillars, shi2019pointrcnn}.

In general, regression entails predicting a continuous target  from an input . This is a fundamental machine learning problem that can be addressed using a variety of different techniques \cite{lathuiliere2019comprehensive, gast2018lightweight, makansi2019overcoming, pan2018mean, Diaz_2019_CVPR}. Specifically in 3DOD, the 3D bounding box regression problem is usually addressed by letting a DNN directly predict a target bounding box  for a given input , and training the DNN by minimizing the  or Huber loss~\cite{huber1964robust, zhou2018voxelnet, yang2019std, shi2020pv, he2020structure}. Alternatively, a probabilistic regression approach has also been employed. The conditional target density , i.e.\ the distribution for the target 3D bounding box  given the input , is then explicitly modelled using a DNN, which is trained by minimizing the associated negative log-likelihood. Previous work on 3DOD has mainly explored Gaussian models of ~\cite{feng2018towards, feng2019leveraging, feng2019can, meyer2019lasernet}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/3dod.png}\vspace{-2.0mm}
    \caption{We study how EBMs can be applied to accurately regress 3D bounding boxes in 3DOD from LiDAR point clouds. Here, we visualize the output of our detector on a validation example from the KITTI~\cite{geiger2012we} dataset.}\vspace{-3mm}
    \label{fig:3dod}
\end{figure}

A Gaussian model is however fairly restrictive, limiting  to unimodal and symmetric distributions. Instead, recent work \cite{gustafsson2019learning, danelljan2020probabilistic, gustafsson2020train} has demonstrated that improved regression accuracy can be obtained on various tasks by employing energy-based models (EBMs)~\cite{lecun2006tutorial} to represent the conditional target density . Specifically, this approach entails modeling  with the conditional EBM , and then using gradient ascent to maximize  w.r.t.\  at test-time. Since the EBM  is directly specified via the scalar function , which is defined using a DNN, it is a highly expressive model that puts minimal restricting assumptions on . Even potential multi-modality in the distribution  can therefore be learned directly from data. This EBM-based regression approach is thus an attractive alternative also for 3D bounding box regression, especially considering the impressive performance demonstrated on conventional 2D bounding box regression in images~\cite{gustafsson2019learning, danelljan2020probabilistic, gustafsson2020train}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/overview.pdf}\vspace{-2.0mm}
    \caption{An overview of our proposed approach, applying EBM-based regression to the task of 3D object detection. We integrate a conditional EBM  into the state-of-the-art 3D object detector SA-SSD~\cite{he2020structure}. We design a differentiable pooling operator that, given a 3D bounding box , extracts a feature vector from the SA-SSD output. This feature vector is processed by fully-connected layers, outputting .}\vspace{-3mm}
    \label{fig:overview}
\end{figure*}

Extending the approach from 2D to 3D is however challenging. In particular, using gradient ascent to maximize the EBM  at test-time requires the scalar DNN output  to be differentiable w.r.t.\ the bounding box . For 2D bounding boxes in images, this was achieved by applying a differentiable pooling operator \cite{jiang2018acquisition} on image features \cite{gustafsson2019learning, danelljan2020probabilistic, gustafsson2020train}, but this technique is not directly applicable to 3D bounding boxes. How EBM-based regression should be applied to 3DOD is thus currently an open question, which we set out to investigate in this work.

\parsection{Contributions}
We apply conditional EBMs  to the task of 3D bounding box regression, extending the recent EBM-based regression approach \cite{gustafsson2019learning, danelljan2020probabilistic, gustafsson2020train} from 2D to 3D object detection. This is achieved by adding an extra network branch to the state-of-the-art 3D object detector SA-SSD~\cite{he2020structure}, and designing a differentiable pooling operator for 3D bounding boxes . We evaluate our proposed detector on the KITTI~\cite{geiger2012we} dataset and consistently outperform the SA-SSD baseline detector across all 3DOD metrics. Our work thus demonstrates the potential of EBM-based regression for highly accurate 3DOD.














 \section{ENERGY-BASED MODELS FOR REGRESSION}
\label{section:ebms_regression}

EBMs were extensively studied by the machine learning community in the past \cite{lecun2006tutorial, teh2003energy, bengio2003neural, mnih2005learning, hinton2006unsupervised, osadchy2005synergistic}. In recent years they have also had a resurgence within the field of computer vision, frequently being employed for generative image modeling \cite{xie2016theory, gao2018learning, nijkamp2019learning, du2019implicit, Grathwohl2020Your, gao2020flow, pang2020learning, bao2020bi}. In comparison, the application of EBMs to regression problems has not been a particularly well-studied topic.  Very recent work~\cite{gustafsson2019learning, danelljan2020probabilistic, gustafsson2020train} has however demonstrated their efficacy on diverse computer vision regression tasks such as visual object tracking, head-pose estimation and age estimation.

In regression, the task is to learn to predict targets  from inputs , given a training set  of i.i.d.\ input-target pairs, , . The input space  depends on the specific problem, but can e.g. correspond to the space of images or point clouds. The target space  is continuous,  for some .

In EBM-based regression \cite{gustafsson2019learning, danelljan2020probabilistic, gustafsson2020train}, this task is addressed by modelling the distribution  of  given  with a conditional EBM , defined according to,

Here,  is a DNN that maps any input-target pair  directly to a scalar , and  is the input-dependent normalizing partition function. The DNN output  is interpreted as the (negative) energy of the distribution .

\subsection{Prediction}
\label{section:ebms_regression_pred}
At test-time, EBM-based regression entails predicting the most likely target under the model given an input , i.e. . In practice,  is approximated by refining an initial estimate  via  steps of gradient ascent,
 
thus finding a local maximum of . Evaluation of the partition function  is therefore not required.

\subsection{Training}
\label{section:ebms_regression_train}
The DNN  that specifies the conditional EBM~(\ref{eq:ebm_def}) can be trained using various methods for fitting a density  to observed data . Generally, the most straightforward such method is probably to minimize the negative log-likelihood , which for the EBM  is given by,  

The integral in (\ref{eq:ebm_nll}) is however intractable, preventing exact evaluation of . One possible solution to this problem is to approximate the intractable integral using importance sampling, as employed in \cite{gustafsson2019learning}. However, numerous alternative approaches also exist, including noise contrastive estimation (NCE) \cite{gutmann2010noise} and score matching \cite{hyvarinen2005estimation}. The problem of how EBMs should be trained for regression was studied in detail in \cite{gustafsson2020train}, comparing six  methods on the task of 2D bounding box regression in images. From this comparison, \cite{gustafsson2020train} concluded that a simple extension of NCE should be considered the go-to training method.



NCE entails learning to discriminate between observed data examples and samples drawn from a noise distribution. NCE was adopted for EBM-based regression only recently in \cite{gustafsson2020train}, but has often been used to train EBMs for classification tasks in the past \cite{mnih2012fast, mikolov2013distributed, jozefowicz2016exploring, ma2018noise}. Recently, it has also become highly utilized within self-supervised representation learning \cite{hjelm2018learning, bachman2019learning, chen2020simple, han2020self}. Applying NCE to regression means training the DNN  by minimizing the loss,

where , and  are  samples drawn from a noise distribution  that depends on the true target~. Effectively,  in (\ref{eq:nce_loss}) is the softmax cross-entropy loss for a classification problem with  classes. A simple choice for  that was shown effective in \cite{gustafsson2020train} is setting  to a mixture of  Gaussians centered at , 

where  and the variances  are hyperparameters.

A simple extension to NCE, termed NCE+, was proposed and demonstrated to further improve the regression accuracy on certain tasks in \cite{gustafsson2020train}. The DNN  is still trained by minimizing  in (\ref{eq:nce_loss}), but  is now defined as . The true target  is thus perturbed with , where  is a zero-centered and scaled version of  in (\ref{eq:nce_noise}), i.e. . NCE+ accounts for possible inaccuracies in the annotation process producing , and can be understood as a direct generalization of NCE. In fact, NCE is recovered as a special case when .









 \section{METHOD}
\label{section:method}





We apply EBM-based regression to 3DOD by extending the state-of-the-art 3D object detector SA-SSD \cite{he2020structure} with a conditional EBM  (\ref{eq:ebm_def}). In Sec.~\ref{section:method_sassd}, we first provide necessary background on SA-SSD, including a description of its input and output data format. We then detail how the EBM  is defined, employing differentiable pooling of 3D bounding boxes  and an added network branch, in Sec.~\ref{section:method_architecture}. Our approach for training  is based on NCE and further described in Sec.~\ref{section:method_training}. Lastly, our prediction strategy using gradient ascent is detailed in Sec.~\ref{section:method_prediction}.





















\subsection{The SA-SSD 3D Object Detector}
\label{section:method_sassd}
SA-SSD \cite{he2020structure} takes a LiDAR point cloud of the scene as input  and produces a set  of  detections. Each detection  consists of a predicted 3D bounding box , 

and an associated classification confidence score . In (\ref{eq:y_def}),  is the 3D coordinate of the bounding box center,  is the 3D bounding box size, and  is the heading angle of the bounding box.  

The input LiDAR point cloud  of  points is encoded into a sparse 3D tensor by means of voxelization. This tensor is then processed by a backbone network utilizing submanifold sparse 3D convolutional layers \cite{yan2018second, graham20183d}, producing a 3D feature tensor  of shape . A bird's eye view (BEV) feature representation of the scene is then created by flattening  into the 2D feature map  of shape . Then,  is further processed by six standard 2D convolutional layers, outputting the feature map  of shape . Finally,  is fed to a detection network, in which two  convolutions are applied. The first outputs classification confidence scores and the second outputs offsets for a  grid of anchor 3D bounding boxes.

The SA-SSD backbone and detection networks are trained by minimizing a weighted sum of multiple losses. The focal loss \cite{lin2017focal} is employed for the classification sub-task, and the Huber loss~\cite{huber1964robust} is used for the regression of anchor bounding box offsets. Additionally, SA-SSD employs two losses stemming from auxiliary tasks. By inverting the voxelization via interpolation, 3D feature tensors in the backbone network are represented as point-wise feature vectors. These are then utilized for point-wise foreground segmentation, i.e. predicting whether or not a point lies within any ground truth 3D bounding box, and point-wise center offset regression, i.e. predicting the offset from a foreground point to the center of its 3D bounding box. 























\begin{figure}[t]
    \centering
    \includegraphics[width=0.425\linewidth]{figures/roialign.pdf}\vspace{-2.0mm}
    \caption{Illustration of our modified variant of RoIAlign~\cite{He2017MaskR} for oriented 2D bounding boxes. In this example, the regular  grid is .}\vspace{-3mm}
    \label{fig:modified_roialign}
\end{figure}

\subsection{Conditional EBM Definition}
\label{section:method_architecture}
In this work, we extend the SA-SSD 3D object detector with a conditional EBM , which is fully specified by the DNN . To enable the use of gradient ascent at test time (Sec.~\ref{section:ebms_regression_pred}), the DNN must be designed such that its scalar output  is differentiable w.r.t. the 3D bounding box  (\ref{eq:y_def}). To achieve this, we take inspiration from the recent work \cite{gustafsson2019learning, danelljan2020probabilistic, gustafsson2020train} applying EBM-based regression to 2D bounding box regression in images. Thus, we design a differentiable pooling operator that, for a given 3D bounding box , extracts a feature vector from the SA-SSD backbone network output. This feature vector is then processed by an added network branch of fully-connected layers, outputting the energy value .


\parsection{Differentiable Pooling of 3D Bounding Boxes}
Various pooling operators for 3D bounding boxes  (\ref{eq:y_def}) have been utilized for refining proposal bounding boxes in previous work \cite{shi2019pointrcnn, shi2020points, yang2019std, shi2020pv}, none of which are however differentiable w.r.t. the bounding box . \cite{shi2019pointrcnn} extracts all points in the point cloud  which lie within a given box , and then processes the associated point-wise features to extract a feature vector for . This operator is however not differentiable w.r.t.\ , due to the required discrete assessment of whether a point  lies within the 3D bounding box  or not. \cite{shi2020points} instead divides the box  into a 3D grid and extracts all points which lie within each grid cell. By also encoding which grid cells are empty, this pooling operator better captures geometric information. Because of the discrete extraction of points for each grid cell, it is however still not differentiable w.r.t.\ the 3D bounding box . For similar reasons, the pooling operators utilized in \cite{yang2019std, shi2020pv}, which capture even richer contextual information, are not differentiable w.r.t.\  either.

Instead, we utilize the 2D feature map  of shape  that is produced by the SA-SSD backbone network. This is a compact yet powerful BEV feature representation of the scene. Specifically, we extract a feature vector  by pooling  with ,

which is the BEV version of the 3D bounding box ~(\ref{eq:y_def}). Since  is an oriented 2D bounding box and not necessarily axis-aligned, we can not directly apply standard 2D bounding box pooling operators \cite{Girshick2015FastR, He2017MaskR, jiang2018acquisition}. Instead we employ a modified variant of RoIAlign~\cite{He2017MaskR}, which entails dividing  into a regular  grid, and extracting a feature vector  in each grid point via bilinear interpolation of . See Fig.~\ref{fig:modified_roialign} for an illustration. This operation results in a 2D feature map of shape , which we then flatten to obtain the feature vector . By flattening the feature map instead of e.g. averaging over it, more information is preserved in . It can thus be used to discriminate between a given box and the same box rotated  rad.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/pool_detail.pdf}\vspace{-6.0mm}
    \caption{Detailed illustration of the differentiable pooling operation from 3D bounding box  (\ref{eq:y_def}) to feature vector  (\ref{eq:featurevec_def}). The BEV version of  is pooled with the BEV feature map produced by SA-SSD. The  coordinate  and height  of the box  are processed by two fully-connected layers. }\vspace{-3mm}
    \label{fig:3dbbox_pooling}
\end{figure}

This pooling operation is differentiable w.r.t.\ , but the extracted feature vector  is of course only a function of ~(\ref{eq:ybev_def}), not of the full 3D bounding box ~(\ref{eq:y_def}). Using gradient ascent at test-time would thus not update the  coordinate  or height  of the bounding box . To resolve this, we take inspiration from the architecture used for EBM-based age estimation \cite{gustafsson2019learning}. We thus process  and  by two small fully-connected layers, generating feature vectors  and . Finally, we concatenate the three vectors to obtain , 

where  indicates vector concatenation. The complete pooling operation from 3D bounding box  to feature vector  is illustrated in Fig.~\ref{fig:3dbbox_pooling}.


\parsection{Energy Prediction Branch}
Following \cite{gustafsson2019learning, danelljan2020probabilistic, gustafsson2020train}, we add an extra network branch onto SA-SSD for processing the extracted feature vector. The network branch consists of three fully-connected layers. It takes the feature vector  as input and outputs the energy , thus fully specifying the conditional EBM  (\ref{eq:ebm_def}). The complete architecture of  is illustrated in Fig~\ref{fig:overview}.

























\subsection{Detector Training}
\label{section:method_training}
Following the work on EBM-based 2D object detection \cite{gustafsson2019learning, gustafsson2020train}, the extra fully-connected layers described in Sec~\ref{section:method_architecture} are added onto a pre-trained and fixed SA-SSD detector. The parameters  in  thus only stem from these added fully-connected layers, and the SA-SSD backbone and detection networks are kept fixed during training of the DNN~. To train , we use NCE as described in Sec~\ref{section:ebms_regression_train}. We employ the same training parameters (batch size, data augmentation etc.) as for SA-SSD \cite{he2020structure}, only replacing the original detector loss with the NCE loss (\ref{eq:nce_loss}).




























\begin{algorithm}
\caption{Gradient-based refinement.}
\label{algo:prediction}
\textbf{Input:} , , , , .
\begin{algorithmic}[1]
    \For{\texttt{}}
        \State .
        \For{\texttt{}}
            \State \texttt{PrevValue}  .
            \State .
            \State \texttt{NewValue}  .
            \If { }
                \State .
            \Else
                \State .
            \EndIf
        \EndFor
        \State .
    \EndFor
    \State \textbf{Return} .
\end{algorithmic}
\end{algorithm}\vspace{-3mm}

\subsection{Detector Inference}
\label{section:method_prediction}
At test-time, the input LiDAR point cloud  is first processed by the SA-SSD detector. SA-SSD outputs the 2D feature map  and a set  of  detections, where  is a 3D bounding box (\ref{eq:y_def}) and  is the associated classification confidence score. We then take all bounding boxes  as initial estimates and refine these via  steps of gradient ascent (Sec~\ref{section:ebms_regression_pred}), producing . The initial 3D bounding boxes  are thus refined by being moved toward different local maxima of . The refined boxes  are finally combined with the original confidence scores, returning the detections .

This gradient-based refinement of the detections produced by SA-SSD of course lowers the detector inference speed somewhat. The point cloud  is however still processed by SA-SSD only once, and the scalar  is extracted from  using an efficient pooling operator and a just few fully-connected layers. Moreover, the gradient  can be efficiently evaluated using auto-differentiation. The complete refinement procedure is detailed in Algorithm~\ref{algo:prediction}, where  denotes the gradient ascent step-length,  is a decay of the step-length, and the  check ensures that  is never decreased.










 \section{EXPERIMENTS}
\label{section:experiments}

We evaluate our EBM-based 3DOD approach on the KITTI 3DOD dataset \cite{geiger2012we} and compare it with the SA-SSD~\cite{he2020structure} baseline and other state-of-the-art methods. Our detector is implemented in PyTorch~\cite{paszke2019pytorch}. Training and inference code is publicly available.
















\begin{table}[t]
\centering
\caption{Results on KITTI test in terms of 3D and BEV AP.}\vspace{-2.0mm}
\label{tab:kitti_test}
\resizebox{1.0\linewidth}{!}{\begin{tabular}{@{~}l@{~}|@{~}c@{~}c@{~}c@{~}|@{~}c@{~}c@{~}c@{~}}
\toprule
 & &3D @ 0.7 & & &BEV @ 0.7 &\\
 &Easy &Moderate &Hard &Easy &Moderate &Hard\\
\midrule
Part-A~\cite{shi2020points} &87.81 &78.49 &73.51    &91.70 &87.79 &84.61\\
SERCNN~\cite{zhou2020joint} &87.74 &78.96 &74.30    &94.11 &88.10 &83.43\\
EPNet~\cite{huang2020epnet} &89.81 &79.28 &74.59    &94.22 &88.47 &83.69\\
Point-GNN~\cite{shi2020point} &88.33 &79.47 &72.29    &93.11 &89.17 &83.90\\
3DSSD~\cite{yang20203dssd} &88.36 &79.57 &74.55    &92.66 &89.02 &85.86\\
STD~\cite{yang2019std} &87.95 &79.71 &75.09    &94.74 &89.19 &86.42\\
SA-SSD~\cite{he2020structure} &88.75 &79.79 &74.16 &95.03 &\textbf{91.03} &85.96\\
3D-CVF~\cite{yoo20203d} &89.20 &80.05 &73.11  &93.52 &89.56 &82.45\\
CLOCs-PVCas~\cite{pang2020CLOCs} &88.94 &80.67 &\textbf{77.15} &93.05 &89.80 &\textbf{86.57}\\
PV-RCNN~\cite{shi2020pv}     &90.25 &\textbf{81.43} &76.82 &94.98 &90.65 &86.14\\
\midrule
\midrule
SA-SSD            &88.80 &79.52 &72.30 &95.44 &89.63 &84.34\\
\textbf{SA-SSD+EBM} &\textbf{91.05} &80.12 &72.78 &\textbf{95.64} &89.86 &84.56\\
\textit{Rel. Improvement}            &+2.53\% &+0.75\% &+0.66\% &+0.21\% &+0.26\% &+0.26\%\\
\bottomrule
\end{tabular}


%
 }\vspace{-3.0mm}
\end{table}

\subsection{Dataset} 
KITTI~\cite{geiger2012we} is the most commonly used dataset for automotive 3DOD. It contains 7\thinspace481 examples for training, and 7\thinspace518 \textit{test} examples without publicly available ground truth annotations. Following common practice \cite{he2020structure, shi2020pv}, the training examples are further divided into \textit{train} (3\thinspace712 examples) and \textit{val} (3\thinspace769 examples) splits. We train models exclusively on the \textit{train} split and set hyperparameters using the \textit{val} split. We report results both on \textit{val}, and on the \textit{test} split by submitting detections to the KITTI benchmark server. Following SA-SSD, we conduct experiments only on the car object class.

\parsection{Evaluation Metrics}
On the KITTI benchmark server, models are evaluated in terms of average precision (AP) in both 3D and BEV. It considers three different difficulty levels (easy, moderate and hard), based on how far away and occluded objects are. AP is the area under the precision-recall curve, where a predicted bounding box is considered a true positive if its 3D/BEV IoU with a ground truth box exceeds a certain threshold. For cars, the threshold is set to  on the KITTI benchmark. Two predicted boxes with IoU of, e.g.,  and  thus have identical effect on this metric. Since our main goal is to improve the accuracy of the predicted bounding boxes, we also report the AP for higher thresholds  on the \textit{val} split. All reported AP values are computed using 40 recall positions. 




















\begin{table}[t]
\centering
\caption{Results on KITTI val in terms of 3D and BEV AP.}\vspace{-2.0mm}
\label{tab:kitti_val}
\resizebox{1.0\linewidth}{!}{\begin{tabular}{@{~}l@{~}|@{~}c@{~}c@{~}c@{~}|@{~}c@{~}c@{~}c@{~}}
\toprule
 & &3D @ 0.7 & & &BEV @ 0.7 &\\
 &Easy &Moderate &Hard &Easy &Moderate &Hard\\
\midrule
SA-SSD~\cite{he2020structure} &93.23 &84.30 &81.36 &- &- &-\\
CLOCs-PVCas~\cite{pang2020CLOCs} &92.78 &85.94 &\textbf{83.25} &93.48 &91.98 &89.48\\
PV-RCNN~\cite{shi2020pv}     &92.57 &84.83 &82.69 &95.76 &91.11 &88.93\\
\midrule
\midrule
SA-SSD            &93.14 &84.65 &81.86 &96.56 &92.84 &90.36\\
\textbf{SA-SSD+EBM} &\textbf{95.45} &\textbf{86.83} &82.23 &\textbf{96.60} &\textbf{92.92} &\textbf{90.43}\\
\textit{Rel. Improvement}            &+2.48\% &+2.58\% &+0.45\% &+0.04\% &+0.09\% &+0.08\%\\
\bottomrule
\end{tabular} }\vspace{-3.0mm}
\end{table}

\begin{table*}[t]
\centering
\caption{Further comparison of our proposed detector and the SA-SSD baseline on KITTI val.}\vspace{-2.0mm}
\label{tab:kitti_val_3d}
\resizebox{0.975\linewidth}{!}{





\begin{tabular}{@{~}l@{~}|@{~}c@{~}c@{~}c@{~}|@{~}c@{~}c@{~}c@{~}|@{~}c@{~}c@{~}c@{~}|@{~}c@{~}c@{~}c@{~}}
\toprule
 & &3D @ 0.75 & & &3D @ 0.8 & & &3D @ 0.85 & & &3D @ 0.9 &\\
 &Easy &Moderate &Hard &Easy &Moderate &Hard &Easy &Moderate &Hard &Easy &Moderate &Hard\\
\midrule
SA-SSD            &84.48 &73.91 &70.99 &60.89 &50.08 &47.37 &24.29 &19.58 &18.05 &2.06 &1.58 &1.33\\
\textbf{SA-SSD+EBM} &87.85 &74.96 &71.95 &66.70 &54.32 &51.36 &31.02 &23.91 &21.95 &3.45 &2.74 &2.26\\
\textit{Rel. Improvement}            &+3.99\% &+1.42\% &+1.35\% &+9.54\% &+8.47\% &+8.42\% &+27.7\% &+22.1\% &+21.6\% &+67.5\% &+73.4\% &+69.9\%\\

\midrule
\midrule

 & &BEV @ 0.75 & & &BEV @ 0.8 & & &BEV @ 0.85 & & &BEV @ 0.9 &\\
 &Easy &Moderate &Hard &Easy &Moderate &Hard &Easy &Moderate &Hard &Easy &Moderate &Hard\\
\midrule
SA-SSD            &95.41 &87.47 &84.79  &87.12 &79.07 &74.65  &61.53 &54.15 &50.39  &17.48 &15.71 &14.58\\
\textbf{SA-SSD+EBM} &95.47 &87.54 &84.88  &88.31 &80.06 &77.25  &68.40 &58.62 &54.48  &26.60 &22.03 &19.48\\
\textit{Rel. Improvement}            &+0.06\% &+0.08\% &+0.11\%  &+1.37\% &+1.25\% &+3.48\%  &+11.2\% &+8.25\% &+8.12\%  &+52.2\% &+40.2\% &+33.6\%\\
\bottomrule
\end{tabular} }\vspace{-3.0mm}
\end{table*}

\subsection{Implementation Details}
We utilize the open-source implementation and pre-trained model provided\footnote{\url{https://github.com/skyhehe123/SA-SSD}} by the SA-SSD authors. The feature map  that is produced by the backbone network is of shape . We divide each  (\ref{eq:ybev_def}) into a regular  grid, meaning that the feature vector . We process  and  with separate fully-connected layers (dimensions: , ), generating  and . After concatenation, we thus obtain . Finally,  is processed by three fully-connected layers of dimensions , , . To train the DNN , i.e. the added fully-connected layers, we just replace the original detector loss with the NCE loss (Sec.~\ref{section:method_training}). We also considered NCE+ with , but saw no clear improvements over NCE. We hypothesize this is because there is less inherent ambiguity in the annotation process of 3D bounding boxes than of 2D bounding boxes in images. As in \cite{gustafsson2020train}, we set  with ,  for the noise distribution  (\ref{eq:nce_noise}). After ablation, optimizing 3D AP (moderate difficulty) on the \textit{val} split, we set  differently for different components of the 3D box  (\ref{eq:y_def}). Specifically,  for ,  for  and  for . Following \cite{gustafsson2019learning, gustafsson2020train}, we also set  and  for gradient-based refinement (Algorithm~\ref{algo:prediction}). The step-length  was selected based on ablation.






























\subsection{3DOD Results on KITTI}
Results on KITTI \textit{test} in terms of 3D and BEV AP ( threshold) are found in Table~\ref{tab:kitti_test}. We mainly compare the performance of our EBM-based 3D object detector (SA-SSD+EBM) to the pre-trained SA-SSD it extends, and include other state-of-the-art detectors for reference. We also include the results for SA-SSD reported in the original paper~\cite{he2020structure}, as these differ somewhat from those obtained with the provided pre-trained model. In Table~\ref{tab:kitti_test}, we observe that the added EBM and gradient-based refinement consistently improves the SA-SSD baseline across all metrics. We also observe that our SA-SSD+EBM detector achieves very competitive performance compared to previous methods. 

Results on KITTI \textit{val} in terms of 3D and BEV AP ( threshold) are found in Table~\ref{tab:kitti_val}. Again, we observe that our EBM-based detector consistently outperforms the SA-SSD baseline. On KITTI \textit{val}, our SA-SSD+EBM also sets a new state-of-the-art in terms of a majority of the metrics. A further comparison of SA-SSD+EBM and the SA-SSD baseline is provided in Table~\ref{tab:kitti_val_3d}. There, we report AP for higher thresholds  on KITTI \textit{val}. We observe that the gradient-based refinement consistently improves detector performance across all metrics, and that the relative gain is larger for higher thresholds.
















\subsection{Analysis of Inference Speed}
The improved detection performance compared to SA-SSD comes with a decreased inference speed. On a single NVIDIA TITAN Xp GPU, SA-SSD runs at  FPS, while SA-SSD+EBM runs at  FPS for  gradient ascent iterations. We further analyze how the choice of  affects detector inference speed and performance in Fig.~\ref{fig:impact_of_grad_iters}. The performance is here given in terms of 3D AP (0.7 threshold) averaged over the three difficulty levels (easy, moderate, hard), on KITTI \textit{val}. We observe that the choice  provides approximately equal performance compared to , while only decreasing the inference speed to  FPS.

\begin{figure}[t]
    \centering
            \begin{tikzpicture}[scale=0.785]
                \pgfplotsset{
                    y axis style/.style={
                        yticklabel style=#1,
                        ylabel style=#1,
                        y axis line style=#1,
                        ytick style=#1
                  }
                }
            
                \begin{axis}[
                        axis y line*=left,
                        xlabel={Number of gradient ascent iterations },
                        ylabel={Average AP},
                        xtick={0, 10, 20, 30, 40, 50, 60},
                        legend pos=north east,
                        grid style=dashed,
                        y tick label style={
                            /pgf/number format/.cd,
                                fixed,
                                fixed zerofill,
                                precision=1,
                            /tikz/.cd
                        },
                        every axis plot/.append style={thick},
                        y axis style=blue!65!black,
                    ]
                \addplot[smooth,mark=*,blue] 
                     plot [error bars/.cd, y dir = both, y explicit]
                     table[row sep=crcr, x index=0, y index=1]{
                    0 86.5666666667\\
                    1 86.71\\
                    2 87.3233333333\\
                    4 88.16\\
                    8 88.1966666667\\
                    10 88.17\\
                    16 88.1454\\
                    32 88.1193333333\\
                    64 88.1233333333\\
                    };
                \end{axis}
                
                \begin{axis}[
                    axis y line*=right,
                    axis x line=none,
                    xlabel={Number of gradient ascent iterations },
                    ylabel={FPS},
                    legend pos=north east,
                    grid style=dashed,
                    y tick label style={
                        /pgf/number format/.cd,
                            fixed,
                            fixed zerofill,
                            precision=0,
                        /tikz/.cd
                    },
                    every axis plot/.append style={thick},
                    y axis style=red!65!black,
                ]
                ]
                \addplot[smooth,mark=*,red] 
                     plot [error bars/.cd, y dir = both, y explicit]
                     table[row sep=crcr, x index=0, y index=1]{
                    0 19.1668459797\\
                    1 16.8603231361\\
                    2 15.3684413143\\
                    4 12.8144810124\\
                    8 9.67558682366\\
                    10 8.41298310584\\
                    16 6.43987684987\\
                    32 3.76447974932\\
                    64 2.14341369469\\
                    };
                \end{axis}
            \end{tikzpicture}\vspace{-2mm}
      \caption{Impact of the number of gradient ascent iterations  on detector performance (3D AP with 0.7 threshold, averaged over easy, moderate and hard) and detector inference speed (FPS), on KITTI \textit{val}.}\vspace{-3mm}
      \label{fig:impact_of_grad_iters}
\end{figure}
















\subsection{Analysis of Learned Distribution}


For 3DOD from LiDAR point clouds, it can be inherently difficult to correctly predict the heading angle  of a 3D bounding box  (\ref{eq:y_def}). This is because it often is difficult, when only given a point cloud, to distinguish between two otherwise identical cars which are heading in opposite directions. The true distribution  will thus often have two distinct modes, one at the true heading angle  and one at . In Fig.~\ref{fig:viz_angle}, we visualize  as a function of  when a predicted 3D bounding box  is rotated  rad, demonstrating that our trained EBM  does indeed capture this inherent multi-modality in .

\begin{figure}[t]
    \centering
            \begin{tikzpicture}[scale=0.785]
                \pgfplotsset{
                    y axis style/.style={
                        yticklabel style=#1,
                        ylabel style=#1,
                        y axis line style=#1,
                        ytick style=#1
                  }
                }
            
                \begin{axis}[
xlabel={},
                        ylabel={},
                        xtick={0, 3.14, 6.28},
                        legend pos=north east,
                        grid style=dashed,
                        y tick label style={
                            /pgf/number format/.cd,
                                fixed,
                                fixed zerofill,
                                precision=1,
                            /tikz/.cd
                        },
                        every axis plot/.append style={thick},
]
                \addplot[smooth,mark=] 
                     plot [error bars/.cd, y dir = both, y explicit]
                     table[row sep=crcr, x index=0, y index=1]{
                    0.000000 48.068439\\
                    0.062832 42.294743\\
                    0.125664 33.688911\\
                    0.188496 24.110680\\
                    0.251327 14.706524\\
                    0.314159 7.491511\\
                    0.376991 3.683142\\
                    0.439823 1.996514\\
                    0.502655 1.739263\\
                    0.565487 1.576064\\
                    0.628319 1.712095\\
                    0.691150 0.691976\\
                    0.753982 -0.036267\\
                    0.816814 -0.438010\\
                    0.879646 -0.613474\\
                    0.942478 -0.532532\\
                    1.005310 -0.657090\\
                    1.068142 -0.622545\\
                    1.130973 -0.466947\\
                    1.193805 -0.006368\\
                    1.256637 0.723749\\
                    1.319469 1.080728\\
                    1.382301 1.373230\\
                    1.445133 1.640777\\
                    1.507964 1.647005\\
                    1.570796 1.592185\\
                    1.633628 2.196492\\
                    1.696460 3.070720\\
                    1.759292 3.360998\\
                    1.822124 3.093468\\
                    1.884956 2.246062\\
                    1.947787 1.050786\\
                    2.010619 0.219700\\
                    2.073451 -0.389902\\
                    2.136283 -1.443546\\
                    2.199115 -2.172216\\
                    2.261947 -2.527925\\
                    2.324779 -3.022400\\
                    2.387610 -3.212375\\
                    2.450442 -2.909080\\
                    2.513274 -2.989322\\
                    2.576106 -2.807961\\
                    2.638938 -1.916174\\
                    2.701770 -0.589522\\
                    2.764602 1.828000\\
                    2.827433 5.977865\\
                    2.890265 11.626287\\
                    2.953097 19.174915\\
                    3.015929 26.542351\\
                    3.078761 32.508335\\
                    3.141593 34.989399\\
                    3.204425 33.015808\\
                    3.267256 27.996273\\
                    3.330088 21.507042\\
                    3.392920 14.869912\\
                    3.455752 9.214403\\
                    3.518584 5.244743\\
                    3.581416 2.503688\\
                    3.644247 0.403629\\
                    3.707079 -1.110806\\
                    3.769911 -1.656872\\
                    3.832743 -2.559255\\
                    3.895575 -2.844301\\
                    3.958407 -2.721656\\
                    4.021239 -2.178404\\
                    4.084070 -1.723201\\
                    4.146902 -1.103538\\
                    4.209734 -0.333984\\
                    4.272566 0.213631\\
                    4.335398 0.674516\\
                    4.398230 1.395605\\
                    4.461062 2.014120\\
                    4.523893 2.675209\\
                    4.586725 4.084327\\
                    4.649557 5.330992\\
                    4.712389 6.058853\\
                    4.775221 6.102948\\
                    4.838053 6.126039\\
                    4.900885 5.968259\\
                    4.963716 5.296932\\
                    5.026548 3.772353\\
                    5.089380 2.374893\\
                    5.152212 1.712681\\
                    5.215044 1.714821\\
                    5.277876 1.431682\\
                    5.340708 1.025762\\
                    5.403539 1.021235\\
                    5.466371 -0.101840\\
                    5.529203 -1.292487\\
                    5.592035 -2.034903\\
                    5.654867 -3.118789\\
                    5.717699 -4.276930\\
                    5.780530 -4.497327\\
                    5.843362 -4.232416\\
                    5.906194 -2.231362\\
                    5.969026 1.999620\\
                    6.031858 6.988460\\
                    6.094690 16.449743\\
                    6.157522 28.313625\\
                    6.220353 40.106926\\
                    6.283185 48.068401\\
                    };
                \end{axis}
            \end{tikzpicture}\vspace{-2mm}
    \caption{Visualization of the DNN scalar output  when a predicted 3D bounding box  (\ref{eq:y_def}) is rotated  rad, demonstrating that the trained EBM  captures the inherent multi-modality in .}\vspace{-3mm}
    \label{fig:viz_angle}
\end{figure}






 \section{CONCLUSION}
\label{section:conclusion}
We applied conditional EBMs  to the task of 3D bounding box regression, thus extending the recent EBM-based regression approach from 2D to 3D object detection. By designing a differentiable pooling operator for 3D bounding boxes, we could integrate a conditional EBM  into the state-of-the-art 3D object detector SA-SSD. On the KITTI dataset, our approach consistently outperformed the SA-SSD baseline across all 3DOD metrics. By demonstrating the potential of EBM-based regression for highly accurate 3DOD, we hope that our work will encourage the research community to further explore the application of EBMs to 3DOD and other important regression tasks.

 







\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}




\end{document}