\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage{eso-pic}
\usepackage{xspace}
\usepackage{footnote}
\usepackage{mathtools}
\usepackage[super]{nth}
\usepackage{amssymb}\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage[dvipsnames]{xcolor}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{710} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

\title{AdaCoF: Adaptive Collaboration of Flows for Video Frame Interpolation}

\author{Hyeongmin Lee \quad
	Taeoh Kim \quad
	Tae-young Chung \quad
	Daehyun Pak \quad
	Yuseok Ban \quad
	Sangyoun Lee \\ \vspace{0.01cm}
	\and 
	Yonsei University \\
	{\tt\small \{minimonia,kto,tato0220,koasing,syleee\}@yonsei.ac.kr}
	\and
	Agency for Defense Development \\
	{\tt\small ban@add.re.kr}
}

\maketitle

\begin{abstract}
	Video frame interpolation is one of the most challenging tasks in video processing research. Recently, many studies based on deep learning have been suggested. Most of these methods focus on finding locations with useful information to estimate each output pixel using their own frame warping operations. However, many of them have Degrees of Freedom~(DoF) limitations and fail to deal with the complex motions found in real world videos. To solve this problem, we propose a new warping module named Adaptive Collaboration of Flows~(AdaCoF). Our method estimates both kernel weights and offset vectors for each target pixel to synthesize the output frame. AdaCoF is one of the most generalized warping modules compared to other approaches, and covers most of them as special cases of it. Therefore, it can deal with a significantly wide domain of complex motions. To further improve our framework and synthesize more realistic outputs, we introduce dual-frame adversarial loss which is applicable only to video frame interpolation tasks. The experimental results show that our method outperforms the state-of-the-art methods for both fixed training set environments and the Middlebury benchmark. Our source code is available at \url{https://github.com/HyeongminLEE/AdaCoF-pytorch}.
\end{abstract}

\section{Introduction}

Synthesizing the intermediate frame when consecutive frames have been provided is one of the main research topics in the video processing area. Using a frame interpolation algorithm, we can obtain slow-motion videos from ordinary videos without using professional high-speed cameras. In addition, we can freely convert the frame rates of the videos so it can be applied to the video coding system. To interpolate the intermediate frame of a video requires an understanding of motion, unlike image pixel interpolation. Unfortunately, real world videos contain not only simple motions, but also large and complex ones, making the task significantly more difficult.

\begin{figure}
	\setlength{\belowcaptionskip}{-24pt}
	\begin{center}
		\subfloat[The Kernel-Based Approach]
		{\includegraphics[width=0.46\linewidth]{./images/figure1/KernelEstimation.pdf}}\,
		\hfill
		\subfloat[The Flow-Based Approach]
		{\includegraphics[width=0.46\linewidth]{./images/figure1/FlowmapEstimation.pdf}}\,
		\hfill
		\
I_{out} = \mathcal{T}_{f}(I_n) + \mathcal{T}_{b}(I_{n+1})
\label{eq:base}

I_{out} = V \odot \mathcal{T}_{f}(I_n) + (J-V) \odot \mathcal{T}_{b}(I_{n+1}),

\hat{I}(i,j) = \sum_{k=0}^{F-1}{\sum_{l=0}^{F-1}{W_{k,l}I(i+k,j+l)}},

\hat{I}(i,j) = \sum_{k=0}^{F-1}{\sum_{l=0}^{F-1}{W_{k,l}I(i+k+\alpha_{k,l},j+l+\beta_{k,l})}}

\hat{I}(i,j) = \sum_{k=0}^{F-1}{\sum_{l=0}^{F-1}{W_{k,l}(i,j)I(i+k+\alpha_{k,l},j+l+\beta_{k,l})}}

\begin{multlined}
\hat{I}(i,j) =\\
\sum_{k=0}^{F-1}{\sum_{l=0}^{F-1}{W_{k,l}(i,j)I(i+dk+\alpha_{k,l},j+dl+\beta_{k,l})}}
\end{multlined}

\mathcal{L}_1 = \|I_{out} - I_{gt}\|_1

\mathcal{L}_{vgg} = \|\mathcal{F}(I_{out}) - \mathcal{F}(I_{gt})\|_2

-\mathcal{L}_C = \log(C(\left[I_n, I_{out}\right]))+\log(1-C(\left[I_{out}, I_{n+1}\right])),

\begin{aligned}
\mathcal{L}_{adv} =& C(\left[I_n, I_{out}\right])\log(C(\left[I_n, I_{out}\right]))\\
&+C(\left[I_{out}, I_{n+1}\right])\log(C(\left[I_{out}, I_{n+1}\right]))
\end{aligned}

\mathcal{L}_d = \mathcal{L}_1,

\mathcal{L}_p = \lambda_1 \mathcal{L}_1 + \lambda_{vgg} \mathcal{L}_{vgg} + \lambda_{adv} \mathcal{L}_{adv},
-1ex]
		\subfloat[WGAN-GP]
		{\includegraphics[width=0.485\linewidth]{./images/gan_comparison2/gan_wgan.pdf}}\,
		\subfloat[TGAN]
		{\includegraphics[width=0.485\linewidth]{./images/gan_comparison2/gan_tgan.pdf}}\,
		\caption{The result of adding adversarial losses.}
		\label{fig:gan}
	\end{center}
\end{figure}

\noindent\textbf{Dilation.} In Section~\ref{network}, we add dilation to the AdaCoF operation to enforce the offset vectors to start from a wider area. We check the effect of dilation by training the network with  and .  means that the offset vectors start from the same location. Table~\ref{tbl:dilation} shows that the larger dilation generally leads to better results. As we can see from the \nth{4} - \nth{7} columns of Figure~\ref{fig:offset}, the offset vectors tend to spread more in the case of large motion. Therefore, dilation provides the effect of \emph{better initialization} for them. Figure~\ref{fig:offset} will be covered in more detail in Section~\ref{offsetvis}.

\begin{figure*}
	\setlength{\belowcaptionskip}{-5pt}
	
	\captionsetup[subfigure]{labelformat=empty}
	\subfloat
	{\includegraphics[width=0.195\linewidth]{./images/viscomp/beanbag_gt.pdf}}
	\hspace{-1ex}
	\subfloat{
		\begin{tabular}[b]{c}
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/beanbag_1_overlap.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/beanbag_1_phase.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/beanbag_1_mind.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/beanbag_1_sepconv.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/beanbag_1_dvf.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/beanbag_1_superslomo.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/beanbag_1_adc.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/beanbag_1_figan.pdf}}\
			\-4.5ex]
	\subfloat[Ground Truth]
	{\includegraphics[width=0.195\linewidth]{./images/viscomp/car_gt.pdf}}
	\hspace{-1ex}
	\subfloat{
		\begin{tabular}[b]{c}
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/car_1_overlap.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/car_1_phase.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/car_1_mind.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/car_1_sepconv.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/car_1_dvf.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/car_1_superslomo.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/car_1_adc.pdf}}\
			\subfloat
			{\includegraphics[width=0.093\linewidth]{./images/viscomp/car_1_figan.pdf}}\
			\-2ex]
		\subfloat
		{\includegraphics[width=0.138\linewidth]{./images/offset/02_skating_f1.pdf}}\
		\hfill
		\subfloat
		{\includegraphics[width=0.138\linewidth]{./images/offset/02_skating_f2.pdf}}\
		\hfill
		\subfloat
		{\includegraphics[width=0.138\linewidth]{./images/offset/02_skating_occ.pdf}}\
		\hfill
		\subfloat
		{\includegraphics[width=0.138\linewidth]{./images/offset/02_skating_flo1.pdf}}\
		\hfill
		\subfloat
		{\includegraphics[width=0.138\linewidth]{./images/offset/02_skating_flo2.pdf}}\
		\hfill
		\subfloat
		{\includegraphics[width=0.138\linewidth]{./images/offset/02_skating_v1.pdf}}\
		\hfill
		\subfloat
		{\includegraphics[width=0.138\linewidth]{./images/offset/02_skating_v2.pdf}}\
		\
\Delta p_{k,l} = (\alpha_{k,l}, \beta_{k,l})

F_m(i,j) = \sum_{k=0}^{F-1}{\sum_{l=0}^{F-1}{W_{k,l}(i,j)\Delta p_{k,l}}}

F_v(i,j) = \sum_{k=0}^{F-1}{\sum_{l=0}^{F-1}{W_{k,l}(i,j)(F_m(i,j)-\Delta p_{k,l})^2}}


\noindent The large value for this map means that the offset vectors for the pixel are more spread out so that it can refer to more pixels. According to the figure, more challenging locations such as large motions and occluded areas have larger variance values. Therefore, it can be used as a kind of uncertainty map for some motion estimation tasks. Unlike Mean Flow map, it can only be obtained through our method.

\section{Conclusion}
In this paper, we point out that the DoF of the warping operation to deal with various complex motions is one of the most critical factors in video frame interpolation. Then we propose a new operation called Adaptive Collaboration of Flows~(AdaCoF). This method is the most generalized because all of the previous approaches are special versions of AdaCoF. The parameters needed for the AdaCoF operation are obtained from a fully convolutional network which is end-to-end trainable. Our experiments show that our method outperforms most of the competing algorithms even in several challenging cases such as those with large motion and occlusion. We visualize the network outputs to check whether they behave as intended and that the visualized maps are meaningful, so they can be used for other motion estimation tasks.

\vspace{5mm}

\noindent\textbf{Acknowledgement} This research was supported by Multi-Ministry Collaborative R\&D Program(R\&D program for complex cognitive technology) through the National Research Foundation of Korea(NRF) funded by MSIT, MOTIE, KNPA(NRF-2018M3E3A1057289)

\newpage
\newpage

{\small
	\bibliographystyle{ieee_fullname}
	\bibliography{egbib}
}

\end{document}