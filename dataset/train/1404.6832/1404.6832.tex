\documentclass[envcountsame]{llncs}

\usepackage{amsmath,amssymb,stmaryrd,xspace,pdfsync,mathrsfs,graphicx,refcount,verbatim}
\usepackage{microtype}
\usepackage{lmodern}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage[mathscr]{euscript}
\usetikzlibrary{decorations.text,arrows,snakes,shapes,fit,shadows,calc,patterns,intersections}

\usepackage[textwidth=4cm]{todonotes}
\setlength{\marginparwidth}{4cm}



\newcounter{sauvegarde}
\setcounter{sauvegarde}{42}

\newcommand\adjustc[1]{\protect{\setcounter{sauvegarde}{\thetheorem}
  \setcounterref{theorem}{#1}
  \addtocounter{theorem}{-1}
}}


\newcommand\restorec{
\setcounter{theorem}{\thesauvegarde}
}

\newcommand{\mynote}[3][]{\todo[caption={\sf #3}, color={\ifnum#2=0 green!20
    \else\ifnum#2=1 orange!30
    \else\ifnum#2=2 yellow!20
    \else\ifnum#2=3 cyan!20
    \else magenta!20\fi\fi\fi\fi}, size=\tiny, #1]{\renewcommand{\baselinestretch}{1}\selectfont\sf#3}\xspace}
\newcommand\mznote[1]{\mynote0{#1}}




\definecolor{my1}{cmyk}{0,.6,0,0}
\definecolor{my2}{cmyk}{.3,.0,.0,.0}
\newcommand*{\swap}[2]{#2#1}
\newcommand{\efgame}{Ehrenfeucht-Fra\"iss\'e\xspace}
\newcommand\nat{\ensuremath{\mathbb{N}}\xspace}
\newcommand\As{\ensuremath{\mathcal{A}}\xspace}
\newcommand\Cs{\ensuremath{\mathcal{C}}\xspace}
\newcommand\Csgen[3]{\ensuremath{\Cs_{#1,#3}^{#2}}\xspace}
\newcommand\Cslev[1]{\ensuremath{\Cs_{#1}}\xspace}
\newcommand\Cslevk[2]{\ensuremath{\Cs_{#1}^{#2}}\xspace}
\newcommand\Csik{\ensuremath{\Cs_i^k}\xspace}
\newcommand\Cstwo{\ensuremath{\Cs_2}\xspace}
\newcommand\Cstwotwo{\ensuremath{\Cstwolen2}\xspace}
\newcommand\Csi{\ensuremath{\Cs_i}\xspace}
\newcommand\Csikn{\ensuremath{\Cs_{i,n}^k}\xspace}
\newcommand\Csitwo{\ensuremath{\Cs_{i,2}}\xspace}
\newcommand\Cstwolen[1]{\ensuremath{\Cs_{2,#1}}\xspace}
\newcommand\Cstwon{\ensuremath{\Cstwolen{n}}\xspace}
\newcommand\Csin{\ensuremath{\Cs_{i,n}}\xspace}
\newcommand\fCgen[3]{\ensuremath{\fC_{#1,#3}^{#2}}\xspace}
\newcommand\fCik{\ensuremath{\fC_i^k}\xspace}
\newcommand\fCi{\ensuremath{\fC_i}\xspace}
\newcommand\fCtwo{\ensuremath{\fC_2}\xspace}
\newcommand\fCtwotwo{\ensuremath{\fCtwolen2}\xspace}
\newcommand\fCikn{\ensuremath{\fC_{i,n}^k}\xspace}
\newcommand\fCin{\ensuremath{\fC_{i,n}}\xspace}
\newcommand\fCtwolen[1]{\ensuremath{\fC_{2,#1}}\xspace}
\newcommand\fCtwon{\ensuremath{\fC_{2,n}}\xspace}
\newcommand\Ds{\ensuremath{\mathcal{D}}\xspace}
\newcommand\Gs{\ensuremath{\mathcal{G}}\xspace}
\newcommand\Is{\ensuremath{\mathcal{I}}\xspace}
\newcommand\Ps{\ensuremath{\mathcal{P}}\xspace}
\newcommand\Es{\ensuremath{\mathcal{E}}\xspace}
\newcommand\Ss{\ensuremath{\mathcal{S}}\xspace}
\newcommand\Ts{\ensuremath{\mathcal{T}}\xspace}
\newcommand\Rs{\ensuremath{\mathcal{R}}\xspace}
\newcommand\Ms{\ensuremath{\mathcal{M}}\xspace}

\newcommand\ct{\ensuremath{\mathbb{T}}\xspace}
\newcommand\cs{\ensuremath{\mathbb{S}}\xspace}
\newcommand\crr{\ensuremath{\mathbb{U}}\xspace}

\newcommand\mat{\ensuremath{\mathscr{M}}\xspace}
\newcommand\mnat{\ensuremath{\mathscr{N}}\xspace}
\newcommand\rat{\ensuremath{\mathscr{R}}\xspace}
\newcommand\pat{\ensuremath{\mathscr{P}}\xspace}
\newcommand\lat{\ensuremath{\mathscr{L}}\xspace}

\newcommand{\dec}[1]{\ensuremath{\Delta_{#1}}\xspace}
\newcommand{\dew}[1]{\ensuremath{\Delta_{#1}(<)}\xspace}

\newcommand{\sic}[1]{\ensuremath{\Sigma_{#1}}\xspace}
\newcommand{\siw}[1]{\ensuremath{\Sigma_{#1}(<)}\xspace}
\newcommand{\pic}[1]{\ensuremath{\Pi_{#1}}\xspace}
\newcommand{\piw}[1]{\ensuremath{\Pi_{#1}(<)}\xspace}
\newcommand{\bsc}[1]{\ensuremath{\mathcal{B}\Sigma_{#1}}\xspace}
\newcommand{\bsw}[1]{\ensuremath{\mathcal{B}\Sigma_{#1}(<)}\xspace}

\newcommand{\decu}{\ensuremath{\Delta_{1}}\xspace}
\newcommand{\dewu}{\ensuremath{\Delta_{1}(<)}\xspace}

\newcommand{\sicu}{\ensuremath{\Sigma_{1}}\xspace}
\newcommand{\siwu}{\ensuremath{\Sigma_{1}(<)}\xspace}

\newcommand{\picu}{\ensuremath{\Pi_{1}}\xspace}
\newcommand{\piwu}{\ensuremath{\Pi_{1}(<)}\xspace}

\newcommand{\bscu}{\ensuremath{\mathcal{B}\Sigma_{1}}\xspace}
\newcommand{\bswu}{\ensuremath{\mathcal{B}\Sigma_{1}(<)}\xspace}

\newcommand{\decd}{\ensuremath{\Delta_{2}}\xspace}
\newcommand{\dewd}{\ensuremath{\Delta_{2}(<)}\xspace}

\newcommand{\sicd}{\ensuremath{\Sigma_{2}}\xspace}
\newcommand{\siwd}{\ensuremath{\Sigma_{2}(<)}\xspace}

\newcommand{\picd}{\ensuremath{\Pi_{2}}\xspace}
\newcommand{\piwd}{\ensuremath{\Pi_{2}(<)}\xspace}

\newcommand{\bscd}{\ensuremath{\mathcal{B}\Sigma_{2}}\xspace}
\newcommand{\bswd}{\ensuremath{\mathcal{B}\Sigma_{2}(<)}\xspace}


\newcommand{\dect}{\ensuremath{\Delta_{3}}\xspace}
\newcommand{\dewt}{\ensuremath{\Delta_{3}(<)}\xspace}

\newcommand{\sict}{\ensuremath{\Sigma_{3}}\xspace}
\newcommand{\siwt}{\ensuremath{\Sigma_{3}(<)}\xspace}

\newcommand{\pict}{\ensuremath{\Pi_{3}}\xspace}
\newcommand{\piwt}{\ensuremath{\Pi_{3}(<)}\xspace}

\newcommand{\bsct}{\ensuremath{\mathcal{B}\Sigma_{3}}\xspace}
\newcommand{\bswt}{\ensuremath{\mathcal{B}\Sigma_{3}(<)}\xspace}


\newcommand{\bspd}{\ensuremath{\mathcal{B}\Sigma_{2}(<,+1)}\xspace}
\newcommand{\dept}{\ensuremath{\Delta_{3}(<,+1)}\xspace}
\newcommand{\sipt}{\ensuremath{\Sigma_{3}(<,+1)}\xspace}
\newcommand{\pipt}{\ensuremath{\Pi_{3}(<,+1)}\xspace}


\newcommand{\deci}{\ensuremath{\Delta_{i}}\xspace}
\newcommand{\dewi}{\ensuremath{\Delta_{i}(<)}\xspace}

\newcommand{\sici}{\ensuremath{\Sigma_{i}}\xspace}
\newcommand{\siwi}{\ensuremath{\Sigma_{i}(<)}\xspace}

\newcommand{\pici}{\ensuremath{\Pi_{i}}\xspace}
\newcommand{\piwi}{\ensuremath{\Pi_{i}(<)}\xspace}

\newcommand{\bsci}{\ensuremath{\mathcal{B}\Sigma_{i}}\xspace}
\newcommand{\bswi}{\ensuremath{\mathcal{B}\Sigma_{i}(<)}\xspace}



\newcommand{\utl}{\ensuremath{UTL}\xspace}
\newcommand{\ltl}{\ensuremath{\textup{LTL}}\xspace}
\newcommand{\efw}{\ensuremath{F + F^{-1}}\xspace}
\newcommand{\efwn}{\ensuremath{F + F^{-1} + X + X^{-1}}\xspace}



\newcommand\fod{\ensuremath{\textup{FO}^2}\xspace}
\newcommand\fodw{\ensuremath{\textup{FO}^2(<)}\xspace}
\newcommand\fodwn{\ensuremath{\textup{FO}^2(<,Succ)}\xspace}

\newcommand{\mso}{\ensuremath{\textup{MSO}}\xspace}
\newcommand{\fo}{\ensuremath{\textup{FO}}\xspace}
\newcommand{\fow}{\ensuremath{\textup{FO}(<)}\xspace}

\newcommand{\savenotation}{
\let\dews\dew
\let\siws\siw
\let\piws\piw
\let\bsws\bsw
\let\dewus\dewu
\let\siwus\siwu
\let\piwus\piwu
\let\bswus\bswu
\let\dewds\dewd
\let\siwds\siwd
\let\piwds\piwd
\let\bswds\bswd
\let\dewts\dewt
\let\siwts\siwt
\let\piwts\piwt
\let\bswts\bswt
\let\dewis\dewi
\let\siwis\siwi
\let\piwis\piwi
\let\bswis\bswi
\let\fows\fow
}
\savenotation

\newcommand{\lightennotation}{
\let\dew\dec
\let\siw\sic
\let\piw\pic
\let\bsw\bsc
\let\dewu\decu
\let\siwu\sicu
\let\piwu\picu
\let\bswu\bscu
\let\dewd\decd
\let\siwd\sicd
\let\piwd\picd
\let\bswd\bscd
\let\dewt\dect
\let\siwt\sict
\let\piwt\pict
\let\bswt\bsct
\let\dewi\deci
\let\siwi\sici
\let\piwi\pici
\let\bswi\bsci
\let\fow\fo  
}
\lightennotation

\newcommand{\restorenotation}{
\let\dew\dews
\let\siw\siws
\let\piw\piws
\let\bsw\bsws
\let\dewu\dewus
\let\siwu\siwus
\let\piwu\piwus
\let\bswu\bswus
\let\dewd\dewds
\let\siwd\siwds
\let\piwd\piwds
\let\bswd\bswds
\let\dewt\dewts
\let\siwt\siwts
\let\piwt\piwts
\let\bswt\bswts
\let\dewi\dewis
\let\siwi\siwis
\let\piwi\piwis
\let\bswi\bswis
\let\fow\fows
}

\newcommand\Cc{\ensuremath{\mathcal{C}}\xspace}
\newcommand\Dc{\ensuremath{\mathcal{D}}\xspace}


\newcommand\foeq[1]{\ensuremath{\cong_{#1}}\xspace}
\newcommand\kfoeq{\foeq{k}}
\newcommand{\foclos}[2]{\ensuremath{[#1]_{\foeq{k}}}\xspace}
\newcommand\sieq[2]{\ensuremath{\lesssim^{#1}_{#2}}\xspace}
\newcommand\ksieq[1]{\sieq{k}{#1}}
\newcommand{\siclos}[2]{\ensuremath{[#1]_{\ksieq{n}}}\xspace}

\newcommand\bceq[2]{\ensuremath{\cong^{#1}_{#2}}\xspace}
\newcommand\kbceq[1]{\bceq{k}{#1}}

\newcommand\gmo{\ensuremath{\geqslant}\xspace}
\newcommand\lmo{\ensuremath{\leqslant}\xspace}

\let\leq\leqslant
\let\le\leqslant
\let\geq\geqslant
\let\ge\geqslant

\newcommand\Sep{\ensuremath{\mathsf{Sep}}\xspace}
\newcommand\um{\ensuremath{\mathbbold{1}}}
\newcommand\content[1]{\ensuremath{\contentmorphism(#1)}}
\newcommand\contentscc[2]{\ensuremath{\contentmorphism\_\mathsf{scc}(#1,#2)}}
\newcommand\arity[1]{\ensuremath{\textsf{arity}(#1)}}
\newcommand\contentmorphism{\ensuremath{\textsf{alph}}}
\newcommand\scc[2]{\ensuremath{\textsf{scc}(#1,#2)}}
\newcommand\lab[1]{\ensuremath{\textsf{lab}(#1)\xspace}}
\newcommand\val[1]{\ensuremath{\textsf{val}(#1)\xspace}}
\newcommand\valc[1]{\ensuremath{\textsf{val}_c(#1)\xspace}}
\newcommand\valr[1]{\ensuremath{\textsf{val}_r(#1)\xspace}}
\newcommand\vall[1]{\ensuremath{\textsf{val}_{\ell}(#1)\xspace}}
\newcommand\cval[1]{\ensuremath{\textsf{cval}(#1)\xspace}}
\newcommand\crec[1]{\ensuremath{\textsf{crec}(#1)\xspace}}

\newcommand\conscon[2]{\ensuremath{[#1]_{#2}}\xspace}



\newcommand\iword{\ensuremath{\omega}-word\xspace}
\newcommand\ilang{\ensuremath{\omega}-language\xspace}
\newcommand\iwords{\ensuremath{\omega}-words\xspace}
\newcommand\ilangs{\ensuremath{\omega}-languages\xspace}
\newcommand\sisemi{sub-\ensuremath{\omega}-semigroup\xspace}
\newcommand\isemi{\ensuremath{\omega}-semigroup\xspace}
\newcommand\isemis{\ensuremath{\omega}-semigroups\xspace}
\newcommand\iclass{\ensuremath{\omega}-class\xspace}
\newcommand\iclasses{\ensuremath{\omega}-classes\xspace}

\newcommand\nice{nice\xspace}
\newcommand\Nice{Nice\xspace}
\newcommand\chain{chain\xspace}
\newcommand\qchain[1]{\ensuremath{\sic{#1}}-chain\xspace}
\newcommand\chains{chains\xspace}
\newcommand\qchains[1]{\ensuremath{\sic{#1}}-chains\xspace}
\newcommand\Chain{Chain\xspace}
\newcommand\qChain[1]{\ensuremath{\sic{#1}}-Chain\xspace}
\newcommand\Chains{Chains\xspace}
\newcommand\qChains[1]{\ensuremath{\sic{#1}}-Chains\xspace}
\newcommand\qpchain[2]{\ensuremath{\sic{#1}[#2]}-chain\xspace}
\newcommand\qpchains[2]{\ensuremath{\sic{#1}[#2]}-chains\xspace}
\newcommand\qpChain[2]{\ensuremath{\sic{#1}[#2]}-Chain\xspace}
\newcommand\qpChains[2]{\ensuremath{\sic{#1}[#2]}-Chains\xspace}

\newcommand\ichain{\qchain{i}}
\newcommand\uchain{\qchain{1}}
\newcommand\dchain{\qchain{2}}
\newcommand\ichains{\qchains{i}}
\newcommand\uchains{\qchains{1}}
\newcommand\dchains{\qchains{2}}
\newcommand\iChain{\qChain{i}}
\newcommand\uChain{\qChain{1}}
\newcommand\dChain{\qChain{2}}
\newcommand\iChains{\qChains{i}}
\newcommand\uChains{\qChains{1}}
\newcommand\dChains{\qChains{2}}

\newcommand\ikchain{\qpchain{i}{k}}
\newcommand\ukchain{\qpchain{1}{k}}
\newcommand\dkchain{\qpchain{2}{k}}
\newcommand\ikchains{\qpchains{i}{k}}
\newcommand\ukchains{\qpchains{1}{k}}
\newcommand\dkchains{\qpchains{2}{k}}
\newcommand\ikChain{\qpChain{i}{k}}
\newcommand\ukChain{\qpChain{1}{k}}
\newcommand\dkChain{\qpChain{2}{k}}
\newcommand\ikChains{\qpChains{i}{k}}
\newcommand\ukChains{\qpChains{1}{k}}
\newcommand\dkChains{\qpChains{2}{k}}


\newcommand\pair[1]{\ensuremath{#1}-triplet\xspace}
\newcommand\pairs[1]{\ensuremath{#1}-triplets\xspace}
\newcommand\npair{\pair{n}}
\newcommand\npairs{\pairs{n}}

\newcommand\genab{generable\xspace}
\newcommand\gend[1]{\ensuremath{#1}-generated\xspace}
\newcommand\genb[1]{\ensuremath{#1}-generable\xspace}
\newcommand\kgend{\gend{k}}
\newcommand\kgenb{\genb{k}}

\newcommand\decomp[1]{\ensuremath{(i,#1)}-decomposition\xspace}
\newcommand\ldecomp{\decomp{l}}


\newcommand\upA{{\bf A}\xspace}
\newcommand\upB{{\bf B}\xspace}
\newcommand\upC{{\bf C}\xspace}
\newcommand\upS{{\bf S}\xspace}
\newcommand\upR{{\bf R}\xspace}
\newcommand\upT{{\bf T}\xspace}

\newcommand\fCon{\ensuremath{\mathfrak{Concat}}\xspace}


\newcommand\fI{\ensuremath{\mathfrak I}\xspace}
\newcommand\fM{\ensuremath{\mathfrak M}\xspace}
\newcommand\fN{\ensuremath{\mathfrak N}\xspace}
\newcommand\fC{\ensuremath{\mathfrak C}\xspace}
\newcommand\fS{\ensuremath{\mathfrak S}\xspace}
\newcommand\fR{\ensuremath{\mathfrak R}\xspace}
\newcommand\fO{\ensuremath{\mathfrak O}\xspace}
\newcommand\fT{\ensuremath{\mathfrak T}\xspace}
\newcommand\fG{\ensuremath{\mathfrak G}\xspace}
\newcommand\fn{\ensuremath{\mathfrak n}\xspace}
\newcommand\fm{\ensuremath{\mathfrak m}\xspace}
\newcommand\fs{\ensuremath{\mathfrak s}\xspace}
\newcommand\fr{\ensuremath{\mathfrak r}\xspace}
\newcommand\ft{\ensuremath{\mathfrak t}\xspace}
\newcommand\fe{\ensuremath{\mathfrak e}\xspace}
\newcommand\ff{\ensuremath{\mathfrak f}\xspace}

\newcommand\gen{\ensuremath{\mathfrak{g}\mathfrak{e}\mathfrak{n}}\xspace}
\newcommand\Gen{\ensuremath{\mathfrak{G}\mathfrak{e}\mathfrak{n}}\xspace}

\newcommand\tame[2]{\ensuremath{(#1,#2)}-tame\xspace}
\newcommand\xytame{\tame{x}{y}}


\newcommand\nest[1]{\ensuremath{[#1]^\fn}\xspace}
\newcommand\ninc{\ensuremath{\sqsubseteq}\xspace}
\newcommand\ncon{\ensuremath{\sqsupseteq}\xspace}

\newcommand\myhill[1]{\ensuremath{\cong_{#1}}\xspace}
\newcommand\lmyhill{\myhill{L}}

\newcommand\idem[1]{\ensuremath{id(#1)}\xspace}

\DeclareMathOperator{\downclos}{\downarrow}

\tikzstyle{nor}=[minimum size=0.35cm,draw,rectangle,inner sep=2pt]
\tikzstyle{nod}=[minimum size=0.35cm,draw,circle,inner sep=2pt]
\tikzstyle{nof}=[minimum size=0.35cm,draw,circle,double,double
distance=1pt]

\tikzstyle{nol}=[minimum size=0.35cm,draw,rectangle,inner sep=1pt,rotate=90]

\tikzstyle{ar}=[line width=0.5pt,->,double]
\tikzstyle{siar}=[line width=1.5pt,->]

\newcommand{\upclos}[1]{\ensuremath{\llceil{#1}\rrceil}\xspace}
\newcommand{\unclos}[1]{\ensuremath{\llfloor{#1}\rrfloor}\xspace}
\newcommand\setsem[1]{\ensuremath{#1}\xspace}
\newcommand\sets{\setsem{S}}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{clm}{Claim}
\newenvironment{mproof}{\begin{proof}}{\qed\end{proof}}

\usepackage{enumitem}
\usepackage{hyperref}

\title{Going higher in the First-order\\ Quantifier Alternation Hierarchy on Words\thanks{Supported by ANR 2010 BLAN 0202 01 FREC}}
\author{Thomas~Place and Marc~Zeitoun}
\institute{LaBRI, Universit\'e de Bordeaux, France}

\begin{document}
\maketitle

\begin{abstract}
  We investigate the quantifier alternation hierarchy in first-order
  logic on finite words. Levels in this hierarchy are defined by counting the
  number of quantifier alternations in formulas. We prove that one can
  decide membership of a regular language to the levels \bscd (boolean
  combination of formulas having only  alternation) and \sict
  (formulas having only  alternations beginning with an existential
  block). Our proof works by considering a deeper problem, called
  separation, which, once solved for lower levels, allows us to solve
  membership for higher~levels.
\end{abstract}

\label{sec:intro}
The connection between logic and automata theory is well known and has a
fruitful history in computer science. It was first observed when B\"uchi,
Elgot and Trakhtenbrot proved independently that the regular languages are
exactly those that can be defined using a monadic second-order logic (\mso)
formula. Since then, many efforts have been made to investigate and understand
the expressive power of relevant fragments of \mso. In this field, the
yardstick result is often to prove \emph{decidable
  characterizations}, \emph{i.e.}, to design an algorithm which, given as input a
regular language, decides whether it can be defined in the fragment under
investigation. More than the algorithm itself, the main motivation is
the insight given by its proof. Indeed, in order to prove a decidable
characterization, one has to consider and understand \emph{all}
properties that can be expressed in the~fragment.


The most prominent fragment of \mso is first-order logic (\fo)
equipped with a predicate "" for the linear-order. The expressive
power of \fo is now well-understood over words and a decidable
characterization has been obtained. The result, Sch\"utzenberger's
Theorem~\cite{sfo,mnpfo}, states that a regular language is definable
in \fo if and only if its syntactic monoid is aperiodic. The syntactic
monoid is a finite algebraic structure that can effectively be
computed from any representation of the language. Moreover,
aperiodicity can be rephrased as an equation that needs to be
satisfied by all elements of the monoid. Therefore, Sch\"utzenberger's
Theorem can indeed be used to decide definability in \fo.

In this paper, we investigate an important hierarchy inside \fo, obtained by
classifying formulas according to the number of quantifier alternations in
their prenex normal form. More precisely, an \fo formula is \sici if its
prenex normal form has at most  quantifier alternations and starts with
a block of existential quantifiers. The hierarchy also involves the classes
\bsci of boolean combinations of \sici formulas, and the classes \deci of
languages that can be defined by both a \sici and the negation of a \sici
formula. The quantifier alternation hierarchy was proved to be
strict~\cite{BroKnaStrict,ThomStrict}: . In the
literature, many efforts have been made to find decidable characterizations of levels of
this well-known~hierarchy.

Despite these efforts, only the lower levels are known to be
decidable. The~class \bscu consists exactly of all piecewise testable
languages, \emph{i.e.}, such that membership of a word only depends on its
subwords up to a fixed~size. These languages were characterized by
Simon~\cite{simon75} as those whose syntactic monoid is
-trivial. A decidable characterization of \sicd (and hence of
\dewd as well) was proven in~\cite{arfi87}.  For~\dewd, the literature is very
rich~\cite{Tesson02diamondsare}. For example, these are exactly the languages
definable by the two variable restriction of \fo~\cite{twfodeux}. These are also those whose syntactic monoid
is in the class~~\cite{pwdelta}. For higher levels in the hierarchy, getting
decidable characterizations remained an important open problem. In particular,
the case of \bscd has a very rich history and a series of combinatorial,
logical, and algebraic conjectures have been proposed over the years.  We refer 
to~\cite{Pin-ThemeVar2011,AK2010,pinbridges,pin-straubing:upper} for an
exhaustive bibliography. So far, the only known effective result was partial,
working only when the alphabet is of size ~\cite{StrauDD2}. One of the main
motivations for investigating this class in formal language theory is its ties
with two other famous hierarchies defined in terms of regular expressions. In
the first one, the \emph{Straubing-Th\'erien
  hierarchy}~\cite{StrauConcat,TheConcat}, level  corresponds exactly to
the class \bsci~\cite{Thom82}. In the second one, the \emph{dot-depth
  hierarchy}~\cite{BrzoDot}, level  corresponds to adding a predicate for
the successor relation in \bsci~\cite{Thom82}. Proving decidability for \bscd
immediately proves decidability of level  in the Straubing-Th\'erien
hierarchy, but also in the dot-depth hierarchy using a reduction by
Straubing~\cite{StrauVD}.

In this paper, we prove decidability for \bscd, \dect and \sict. These new
results are based on a deeper decision problem than decidable
characterizations: the separation problem. Fix a class \Sep of languages. The
\Sep-separation problem amounts to decide whether, given two input regular
languages, there exists a third language in \Sep containing the first language
while being disjoint from the second one. This problem generalizes
decidable characterizations. Indeed, since regular languages are closed under
complement, testing membership in \Sep can be achieved by testing whether the
input is \Sep-separable from its complement. Historically, the separation problem
was first investigated as a special case of a deep problem in semigroup
theory, see~\cite{MR1709911}. This line of research gave solutions to the
problem for several classes. However, the motivations are disconnected from our own, and the proofs rely on
deep, purely algebraic arguments. Recently, a research effort has been made to
investigate this problem from a different perspective, with the aim of finding
new and self-contained proofs relying on elementary ideas and notions from
language theory only~\cite{martens,pvzmfcs13,pzfo,pvzltt}. This paper is a
continuation of this effort: we solve the separation problem for \sicd, and
use our solution as a basis to obtain decidable characterizations for \bscd,
\dect and~\sict.

Our solution works as follows: given two regular languages, one can
easily construct a monoid morphism  that
recognizes both of them. We then design an algorithm that computes, inside the
monoid ,
enough \sicd-related information to answer the \sicd-separation
question for \emph{any} pair of languages that are recognized by
. It turns out that it is also possible (though much more
difficult) to use this information to obtain decidability of \bscd,
\dect and \sict. This information amounts to the notion of \dchain,
our main tool in the paper. A \dchain is an \emph{ordered
  sequence}  that witnesses a property of
 wrt.\ \sicd. Let us give some intuition in the case  -- which is enough to make the
link with \sicd-separation. A sequence  is a \dchain if
any \sicd language containing all words in  
intersects . In terms of separation, this means that
 is \emph{not} separable from  by a
 definable language.

This paper contains three main separate and difficult new results:
(1) an algorithm to compute \dchains\ -- hence \sicd-separability is decidable (2)
decidability of \sict (decidability of \dect is an immediate
consequence), and (3) decidability of~\bscd. Computing \dchains is
achieved using a fixpoint algorithm that starts with trivial \dchains
such as , and iteratively computes more \dchains until a
fixpoint is reached. Note that its completeness proof relies on
the Factorization Forest Theorem of Simon~\cite{simonfacto}. This is
not surprising, as the link between this theorem and the quantifier
alternation hierarchy was already observed in~\cite{pwdelta,bfacto}.



For \sict, we prove a decidable characterization via an
equation on the syntactic monoid of the language. This equation is  
parametrized by the set of \dchains of length . In other words,
we use \dchains to abstract an infinite set of equations into a
single one. The proof relies again on the Factorization Forest Theorem
of Simon~\cite{simonfacto} and is actually generic to all levels in
the hierarchy. This means that for any , we define a notion of
\ichain and characterize \sic{i+1} using an equation parametrized by
\ichains of length . However, decidability of \sic{i+1}
depends on our ability to compute the \ichains of length , which we
can only do for .

Our decidable characterization of \bscd is the most difficult result of the
paper. As for \sict, it is presented by two equations parametrized by \dchains
(of length  and ). However, the characterization is this time specific
to the case . This is because most of our proof relies on a deep
analysis of our algorithm that computes \dchains, which only works for . The equations share surprising similarities with the ones used
in~\cite{bpopen} to characterize a totally different formalism: boolean
combination of open sets of infinite trees. In~\cite{bpopen} also, the authors
present their characterization as a set of equations parametrized by a notion
of ``\chain'' for open sets of infinite trees (although their ``\chains'' are
not explicitly identified as a separation relation). Since the formalisms are
of different nature, the way these \chains and our \dchains are constructed
are completely independent, which means that the proofs are also mostly
independent. However, once the construction analysis of \chains has been done,
several combinatorial arguments used to make the link with equations are
analogous. In particular, we reuse and adapt definitions from~\cite{bpopen} to
present these combinatorial arguments in our proof. One could say that the
proofs are both (very different) setups to apply similar combinatorial
arguments in the end.


\noindent
{\bf Organization.} We present definitions on languages
and logic in Sections~\ref{sec:words} and~\ref{sec:logic}
respectively. Section~\ref{sec:chains} is devoted to the presentation
of our main tool: \ichains. In Section~\ref{sec:comput}, we give our
algorithm computing \dchains. The two remaining sections present
our decidable characterizations, for \sict and \dect in
Section~\ref{sec:caracsi} and for \bscd in
Section~\ref{sec:caracbc}. Due to lack of space, proofs
can be found in~\cite{pz:qalt:2014}.



\section{Words and Algebra}
\label{sec:words}
\newcommand\one{\textup{1}}

\medskip
\noindent
{\textbf{Words and Languages.}} We fix a finite alphabet  and we
denote by  the set of all words over . If  are words, we 
denote by  or  the word obtained by concatenation of  and
. If  we denote by \content{u} its alphabet, \emph{i.e.}, the
smallest subset  of  such that . A \emph{language} is a subset of . In
this paper we consider regular languages: these are languages
definable by \emph{nondeterministic finite automata}, or
equivalently by \emph{finite monoids}. In the paper, we only work with
the monoid representation of regular languages.


\medskip
\noindent
{\textbf{Monoids.}} A \emph{semigroup} is a set  equipped with an
associative multiplication denoted by ''. A \emph{monoid} 
is a semigroup in which there exists a neutral element denoted
. In the paper, we investigate classes of languages, such as
\siwi, that are not closed under complement. For such classes, it is
known that one needs to use \emph{ordered monoids}.  An ordered
monoid is a monoid endowed with a partial order '' which is
compatible with multiplication:  and 
imply~.
Given any finite semigroup , it is well known
that there is a number  (denoted by  when  is
understood from the context) such that for each element  of ,
 is an idempotent: . 

Let  be a language and  be a monoid. We say that \emph{ is
  recognized by } if there exists a monoid morphism  and an \emph{accepting set}  such that
. It is well known that a language is regular if and only if it
can be recognized by a \emph{finite monoid}.



\medskip
\noindent {\textbf{Syntactic Ordered Monoid of a Language.}} The \emph{syntactic preorder}  of a language  is defined as follows
on pairs of words in :  if for all , . Similarly, we define , the \emph{syntactic
  equivalence} of  as follows:  if  and . One can verify that  and  are compatible with
multiplication. Therefore, the quotient  of  by  is an
ordered monoid for the partial order induced by the preorder~.  It is
well known that  can be effectively computed from . Moreover, 
recognizes . We call  the \emph{syntactic ordered monoid of } and
the associated morphism the \emph{syntactic morphism}.


\medskip\noindent
{\bf Separation.} Given three languages , we say that 
\emph{separates}  from  if . Set  as a class of languages, we say that
 is \emph{-separable} from  if some language in 
separates  from . Observe that when  is not closed under
complement, the definition is not symmetrical:  could be
-separable from  while  is not -separable from .

When working on separation, we consider as input two regular languages
. It will be convenient to have a \emph{single} monoid recognizing
both of them, rather than having to deal with two objects. Let  be monoids recognizing  together with the morphisms
, respectively. Then,  equipped
with the componentwise multiplication  is a monoid that recognizes both  and  with
the morphism . From now
on, we work with such a single monoid recognizing both languages.



\medskip
\noindent
{\bf \Chains and Sets of \Chains.} Set  as a finite monoid. A
\emph{\chain } for  is a word over the alphabet , \emph{i.e.}, an
element of . A remark about notation is in order here. A word is
usually denoted as the concatenation of its letters. Since  is a
monoid, this would be ambiguous here since  could either mean a
word with 2 letters  and , or the product of  and  in
. To avoid confusion, we will write  a \chain of
length  on the alphabet .

In the paper, we will consider both sets of \chains (denoted by
) and sets of sets of \chains (denoted by ). In particular, if  is a set of sets of \chains, we define
, the \emph{downset} of , as the set:

We will often restrict ourselves to considering only \chains of a
given fixed length. For , observe that , the set of
\chains of length , is a monoid when equipped with the
componentwise multiplication. Similarly the set  of sets of \chains of
length  is a monoid for the operation: .


\section{First-Order Logic and Quantifier Alternation Hierarchy}
\label{sec:logic}
We view words as logical structures made of a sequence of positions labeled
over~. We denote by  the linear order over the positions. We work with
first-order logic \fow using unary predicates  for all  that
select positions labeled with an , as well as a binary predicate for the
linear order . The \emph{quantifier rank} of an \fow formula is the length
of its longest sequence of nested quantifiers.

One can classify first-order formulas by counting the number of
alternations between  and  quantifiers in the prenex
normal form of the formula. Set , a formula is said to be
\siw{i} (resp.\ \piw{i}) if its prenex normal form has 
quantifier alternations (\emph{i.e.},  blocks of quantifiers) and starts
with an   (resp.\ ) quantification. For example, a
formula whose prenex normal form is

\noindent
is \piwt. Observe that a \piw{i} formula is by definition the negation of a
\siw{i} formula. Finally, a \bsw{i} formula is a boolean combination of
\siw{i} formulas. For  or , we say that a
language  is -definable if it can be defined by an -formula. Finally,
we say that a language is \dew{i}-definable if it can be defined by
\emph{both} a \siw{i} and a \piw{i} formula. It is known that this gives a
strict infinite hierarchy of classes of languages as represented in
Figure~\ref{fig:hiera}.

\tikzstyle{non}=[inner sep=1pt]
\tikzstyle{tag}=[draw,fill=white,sloped,circle,inner sep=1pt]
\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}

      \node[non] (d1) at (0.0,0.0) {\decu};
      \node[non] (s1) at () {\sicu};
      \node[non] (p1) at () {\picu};
      \node[non] (b1) at () {\bscu};

      \node[non] (d2) at (3.5,0.0) {\decd};
      \node[non] (s2) at () {\sicd};
      \node[non] (p2) at () {\picd};
      \node[non] (b2) at () {\bscd};

      \node[non] (d3) at (7.0,0.0) {\dect};
      \node[non] (s3) at () {\sict};
      \node[non] (p3) at () {\pict};
      \node[non] (b3) at () {\bsct};

      \node[non] (d4) at (10.5,0.0) {\dec{4}};

      \draw[thick] (d1.south) to [out=-90,in=180] node[tag] {\scriptsize
        } (s1.west);
      \draw[thick] (d1.north) to [out=90,in=-180] node[tag] {\scriptsize
        } (p1.west);
      \draw[thick] (s1.east) to [out=0,in=-90] node[tag] {\scriptsize
        } (b1.south);
      \draw[thick] (p1.east) to [out=0,in=90] node[tag] {\scriptsize
        } (b1.north);
      \draw[thick] (b1.east) to [out=0,in=180] node[tag] {\scriptsize
        } (d2.west);

      \draw[thick] (d2.south) to [out=-90,in=180] node[tag] {\scriptsize
        } (s2.west);
      \draw[thick] (d2.north) to [out=90,in=-180] node[tag] {\scriptsize
        } (p2.west);
      \draw[thick] (s2.east) to [out=0,in=-90] node[tag] {\scriptsize
        } (b2.south);
      \draw[thick] (p2.east) to [out=0,in=90] node[tag] {\scriptsize
        } (b2.north);
      \draw[thick] (b2.east) to [out=0,in=180] node[tag] {\scriptsize
        } (d3.west);

      \draw[thick] (d3.south) to [out=-90,in=180] node[tag] {\scriptsize
        } (s3.west);
      \draw[thick] (d3.north) to [out=90,in=-180] node[tag] {\scriptsize
        } (p3.west);
      \draw[thick] (s3.east) to [out=0,in=-90] node[tag] {\scriptsize
        } (b3.south);
      \draw[thick] (p3.east) to [out=0,in=90] node[tag] {\scriptsize
        } (b3.north);
      \draw[thick] (b3.east) to [out=0,in=180] node[tag] {\scriptsize
        } (d4.west);

      \draw[thick,dotted] () to
      ();

    \end{tikzpicture}
  \end{center}
  \caption{Quantifier Alternation Hierarchy}
  \label{fig:hiera}
\end{figure}




\medskip
\noindent {\bf Preorder for \siw{i}.} Let  and . We write  if any \siw{i} formula of quantifier rank 
satisfied by  is also satisfied by~. Observe that since a \piw{i}
formula is the negation of a \siw{i} formula, we have  iff any \piw{i} formula of quantifier rank  satisfied by 
is also satisfied by~. One can verify that  is a preorder for
all . Moreover, by definition, a language  can be defined by a \siw{i}
formula of rank  iff  is saturated by , \emph{i.e.}, for all  and all  such that , we have .  


\section{\iChains}
\label{sec:chains}
We now introduce the main tool of this paper:
\emph{\ichains.} Fix a level~ in the quantifier alternation
hierarchy and  a monoid morphism. A
\emph{\ichain } for  is a \chain  such
that for arbitrarily large , there exist words  mapped respectively to  by~. Intuitively, this contains information about the
limits of the expressive power of the logic  with respect to
. For example, if  is a \ichain, then any 
language that contains all words of image  must also contain at
least one word of image~.

\smallskip
In this section, we first give all definitions related to \ichains. We then
present an immediate application of this notion: solving the separation problem
for \siw{i} can be reduced to computing the \ichains of length .

\subsection{Definitions}

\noindent
{\bf \iChains.} Fix  a level in the hierarchy,  and . We define  (resp.\ ) as
the \emph{set of \ikchains for } (resp.\ for ) and
 (resp.\ ) as
the \emph{set of \ichains for } (resp.\ for ). For , we set . Otherwise,
let . We let

\begin{itemize}
\item  if there exist 
  verifying  and for all
  , we have . Moreover,  if
  the words  can be chosen so that they satisfy additionally
   for all .
\item  if for all , we have . That is, . In the same
  way, .
\end{itemize}

One can check that if , then
, since  the fragment  can
detect the alphabet (\emph{i.e.}, for , 
implies ). Similarly for , the set
of \ichains for  is . Observe that all these sets are closed under subwords. Therefore, by Higman's lemma, we get the
following fact.

\begin{fact} \label{fct:high}
  For all  and ,  and
   are regular languages.
\end{fact}

Fact~\ref{fct:high} is interesting but essentially useless in our
argument, as Higman's lemma provides no way for actually computing a
recognizing device for .

For any fixed , we let   be the
set of \ikchains of length~ for ,
\emph{i.e.}, . We define
 and  similarly. The
following fact is immediate.

\begin{fact} \label{fct:chaincomp} If , then
  .  In
  particular,  and  (resp. 
  and ) are submonoids (resp. subsemigroups) of~.
\end{fact}





This ends the definition of \ichains. However, in order to define our
algorithm for computing \dchains and state our decidable
characterization of \bswd, we will need a slightly refined notion:
\emph{compatible sets of \chains}.

\medskip
\noindent
{\bf Compatible Sets of \iChains.} In some cases, it will be useful to
know that several \ichains with the same first element can be
`synchronized'. For example take two \ichains  and 
of length . By definition, for all  there exist words
 whose images under  are  respectively, and such that  and . In some cases (but not all),
it will be possible to choose  for all . The goal of the
notion of compatible sets of \chains is to record the cases in which
this is true.

Fix  a level in the hierarchy,  and .
We define two sets of sets of~\chains:
, the \emph{set of compatible sets of \ikchains for}
(), and
, the \emph{set of compatible sets of
  \ichains for} ().
Let \Ts be a set of \chains, all having the same
length  and the same
first element~.
\begin{itemize}
  \itemsep1ex
\item  if there exists  such
  that , , and for all \chains
  , there exist 
  verifying , and for all ,
  , and .
\item  if   for all . 
\end{itemize}

As before we set  and  as the union
of these sets for all . Moreover, we denote by
 and
 the restriction of these sets to sets of \chains of
length  (\emph{i.e.}, subsets of ).

\begin{fact} \label{fct:setcomp} If , then
  .  In
  particular,  and  (resp. 
  and ) are submonoids (resp. subsemigroups) of .
\end{fact}


\subsection{\iChains and Separation}
We now state a reduction from the separation problem by  and by
-definable languages to the computation of \ichains of length 2.


\begin{theorem} \label{thm:sep}
  Let  be regular languages and  be
  a morphism into a finite monoid recognizing both languages with
  accepting sets . Set . Then the
  following properties hold:
  \begin{enumerate}
  \item  is -separable from  iff for all , .
  \item  is -separable from  iff for all , .
  \end{enumerate}
\end{theorem}





The proof of Theorem~\ref{thm:sep}, which is parametrized by \ichains, is
standard and identical to the corresponding theorems in previous separation
papers, see \emph{e.g.,}~\cite{pzfo}. In Section~\ref{sec:comput}, we present
an algorithm computing \ichains of length~2 at level  of the
alternation hierarchy (in fact, our algorithm needs to compute the more
general notion of sets of compatible \dchains). This makes
Theorem~\ref{thm:sep} effective for \siwd and \piwd.



\section{Computing \dChains}
\label{sec:comput}
In this section, we give an algorithm for computing all \dchains and
sets of compatible \dchains of a given fixed length. We already know
by Theorem~\ref{thm:sep} that achieving this for length  suffices
to solve the separation problem for \siwd and \piwd. Moreover, we will
see in Sections~\ref{sec:caracsi} and~\ref{sec:caracbc} that this
algorithm can be used to obtain decidable characterizations for 
\siwt, \piwt, \dewt and~\bswd. Note that in this section, we only provide the
algorithm and intuition on its correctness.

For the remainder of this section, we fix a morphism  into a finite monoid . For any fixed 
and , we need to compute the following:

\begin{enumerate}
  \itemsep1ex
\item the sets  of \dchains of length  for
  . 
\item the sets  of compatible subsets of
  .
\end{enumerate}

Our algorithm directly computes the second item, \emph{i.e.},
. More precisely, we compute the map . Observe that this is enough to obtain
the first item since by definition, 
iff . Note that going through
compatible subsets is necessary for the technique to work, even if we
are only interested in computing the map .

\medskip
\noindent {\bf Outline.} We begin by explaining what our algorithm does. For
this outline, assume . Observe that for all  such that
, we have . The algorithm starts from these trivially compatible sets,
and then saturates them with two operations that preserve membership
in . Let us describe these two operations. The first one is
multiplication: if  and 
then  by
Fact~\ref{fct:setcomp}. The main idea behind the second operation is to
exploit the following property of~\siwd:

This is why compatible sets are needed: in order to use this property,
we need to have a single word  such that  and , which is information that is not provided by \dchains. This
yields an operation that states that whenever  belongs to ,
then so does , where  is the set of
\chains  with . Let us now formalize
this procedure and generalize it to arbitrary length.

\medskip
\noindent
{\bf Algorithm.} As we explained, our algorithm works by fixpoint,
starting from trivial compatible sets. For all  and , we let  be the set . Our algorithm will start from the function 
that maps any  to .

Our algorithm is defined for any fixed length .  We use a procedure
 taking as input a mapping  and producing another such
mapping.  The algorithm starts from  and iterates  
until a fixpoint is reached.

When , the procedure  is parametrized by
, the sets of \dchains of length , for . This means that in order to use , one needs to have
previously computed the \dchains of length  with~.

\medskip
We now define the procedure . If \Ss is a set of \chains of
length  and , we write  for the set
, which consists of
\chains of length . Let  be a mapping, written . For all , we define a set
 in . That is,  is again a mapping from
 to .
Observe that when ,
there is no computation to do since for all ,  by definition. Therefore, we simply set . When , we define  as the set  with

This ends the description of the procedure .
We now formalize how to iterate~it. For any mapping  and any  ,
we set . For all , we set
. By
definition of , for all  and , we have
. Therefore, there exists  such that . We denote by  this set. This
finishes the definition of the algorithm. Its correctness 
and completeness are stated in the following proposition. 

\begin{proposition} \label{prop:compu}
  Let ,  and . Then 
\end{proposition} 
\noindent
Proposition~\ref{prop:compu} states correctness of the algorithm (the set
 \emph{only} consists of
compatible sets of \dchains) and completeness (this set contains \emph{all}
such sets). It also establishes a bound . This bound is a byproduct of
the proof of the algorithm. It is of particular interest for separation and
Theorem~\ref{thm:sep}. Indeed, one can prove that for any two languages that are
\siwd-separable and recognized by , the separator can be chosen with
quantifier rank  (for ).

We will see in Sections~\ref{sec:caracsi} and~\ref{sec:caracbc} how to use Proposition~\ref{prop:compu}
to get decidable characterizations of \siwt, \piwt, \dewt and  \bswd. We
already state the following corollary as a consequence of
Theorem~\ref{thm:sep}.

\begin{corollary} \label{cor:decidsep}
  Given as input two regular languages  it is decidable to test
  whether  can be -separated
  (resp. -separated) from .
\end{corollary}

\section{Decidable Characterizations of \siwt, \piwt, \dewt}
\label{sec:caracsi}
In this section we present our decidable characterizations for \dewt,
\siwt and \piwt. We actually give characterizations for all classes
\dew{i}, \siw{i} and \piw{i} in the quantifier alternation hierarchy.
The characterizations are all stated in terms of equations on
the syntactic monoid of the language. However, these equations are
parametrized by the \qchains{i-1} of length . Therefore, getting
\emph{decidable} characterizations depends on our ability to compute the set
of \qchains{i-1} of length , which we are only able to do for
.  We begin by stating
our characterization for \siw{i}, and the characterizations for \piw{i}
and \dew{i} will then be simple corollaries.

\begin{theorem} \label{thm:caracsig}
  Let  be a regular language and  be its
  syntactic morphism. For all ,  is definable in \siw{i}
  iff  satisfies the following property: 
  
\end{theorem}

It follows from Theorem~\ref{thm:caracsig} that it suffices to compute
the \qchains{i-1} of length  in order to decide whether a
language is definable in \siw{i}. Also observe that when ,
by definition we have  for all . Therefore, \eqref{eq:sig} can be rephrased as  for all
, which is the already known equation for \siwu, see~\cite{pwdelta}. Similarly, when , \eqref{eq:sig} can be
rephrased as  whenever  is a
`subword' of , which is the previously known equation for \siwd
(see~\cite{pwdelta,bfacto}).

The proof of Theorem~\ref{thm:caracsig} is done using Simon's
Factorization Forest Theorem and is actually a generalization of a
proof of~\cite{bfacto} for the special case of \siwd. Here, we state characterizations of \piw{i}
and \dew{i} as immediate corollaries. Recall that a language is
\piw{i}-definable if its complement is \siw{i}-definable, and that
it is \dew{i}-definable if it is both \siw{i}-definable and
\piw{i}-definable. 

\begin{corollary} \label{cor:caracpi}
  Let  be a regular language and let  be its
  syntactic morphism. For all , the following properties hold:
  \begin{itemize}
  \item  is definable in \piw{i} iff  satisfies  for all .
  \item  is definable in \dew{i} iff  satisfies  for all .
  \end{itemize}
\end{corollary}

We finish the section by stating decidability for the case .
Indeed by Proposition~\ref{prop:compu}, one can compute the \dchains
of length  for any morphism. Therefore, we get the following
corollary.

\begin{corollary} \label{cor:decid}  
  Definability of a regular language in \dewt, \siwt or \piwt is decidable.
\end{corollary}


\section{Decidable Characterization of \bswd}
\label{sec:caracbc}
In this section we present our decidable characterization for
\bswd. In this case, unlike Theorem~\ref{thm:caracsig}, the
characterization is specific to the case  and does not generalize
as a non-effective characterization for all levels. The main reason is
that both the intuition and the proof of the characterization rests on
a deep analysis of our algorithm for computing \dchains, which is
specific to level . The characterization is stated as two
equations that must be satisfied by the syntactic morphism of the
language. The first one is parametrized by \dchains of length , and
the second one by sets of compatible \dchains of length  through a
more involved relation that we define below.

\medskip
\noindent
{\bf Alternation Schema.} Let  be a monoid
morphism and let . A \emph{-schema} for  is
a triple  such that there exist  and  verifying ,
 and . Intuitively, the purpose of
-schemas is to abstract a well-known property of \siwd on elements of
: one can prove that if  is a -schema, then for
all , there exist , mapped respectively to  under , and such that for all , . 


\begin{theorem} \label{thm:caracbc}
  Let  be a regular language and  be its
  syntactic morphism. Then  is definable in \bswd iff  satisfies the
  following properties:
  

  
\end{theorem}


The proof of Theorem~\ref{thm:caracbc} is far more involved than that
of Theorem~\ref{thm:caracsig}. However, a simple consequence is decidability of
definability in \bswd. Indeed, it suffices to compute \dchains of
length  and the -schemas for all  to check validity of both
equations. Computing this information is possible by
Proposition~\ref{prop:compu}, and therefore, we get the following
corollary.

\begin{corollary} \label{cor:decid2} Definability of a regular language in
  \bswd is decidable.
\end{corollary}





\section{Conclusion}
\label{sec:conc}
We solved the separation problem for \siwd using the new notion of \dchains,
and we used our solution to prove decidable characterizations for \bswd,
\dewt, \siwt and \piwt. The main open problem in this field remains to lift up
these results to higher levels in the hierarchy. In particular, we proved that
for any natural~, generalizing our separation solution to \siwi (\emph{i.e.}, being able
to compute the \ichains of length~) would yield a decidable
characterization for \siw{i+1}, \piw{i+1} and \dew{i+1}.

Our algorithm for computing \dchains cannot be directly generalized
for higher levels. An obvious reason for this is the fact that it
considers \dchains parametrized by sub-alphabets. This parameter is
designed to take care of the alternation between levels  and ,
but is not adequate for higher levels. However, this is unlikely
to be the only problem. In particular, we do have an algorithm that
avoids using the alphabet, but it remains difficult to generalize. We 
leave the presentation of this alternate algorithm for further work.

\restorenotation
Another open question is to generalize our results to logical
formulas that can use a binary predicate  for the successor
relation. In formal languages, this corresponds to the well-known
\emph{dot-depth hierarchy}~\cite{BrzoDot}. It was proved
in~\cite{StrauVD} and~\cite{pinweilVD} that decidability of \bspd and
\sipt is a consequence of our results for \bswd and \siwt.
However, while the reduction itself is simple, its proof rely on deep
algebraic arguments. We believe that our techniques can be generalized
to obtain direct proofs of the decidability of \bspd and~\sipt.


\bibliographystyle{abbrv}
\begin{thebibliography}{10}

\bibitem{MR1709911}
J.~Almeida.
\newblock Some algorithmic problems for pseudovarieties.
\newblock {\em Publ. Math. Debrecen}, 54:531--552, 1999.
\newblock {P}roc. of {Automata and Formal Languages, VIII}.

\bibitem{AK2010}
J.~Almeida and O.~Kl\'{\i}ma.
\newblock New decidable upper bound of the 2nd level in the
  {S}traubing-{T}h{\'e}rien concatenation hierarchy of star-free languages.
\newblock {\em {DMTCS}}, 2010.

\bibitem{arfi87}
M.~Arfi.
\newblock Polynomial operations on rational languages.
\newblock In {\em STACS'87}, 1987.

\bibitem{bfacto}
M.~Bojanczyk.
\newblock Factorization forests.
\newblock In {\em DLT'09}, pages 1--17, 2009.

\bibitem{bpopen}
M.~Bojanczyk and T.~Place.
\newblock Regular languages of infinite trees that are boolean combinations of
  open sets.
\newblock In {\em ICALP'12}, pages 104--115, 2012.

\bibitem{BroKnaStrict}
J.~Brzozowski and R.~Knast.
\newblock The dot-depth hierarchy of star-free languages is infinite.
\newblock {\em J. Comp. Syst. Sci.}, 16(1):37--55, 1978.

\bibitem{BrzoDot}
R.~S. Cohen and J.~Brzozowski.
\newblock Dot-depth of star-free events.
\newblock {\em J. Comp. Syst. Sci.}, 5:1--16, 1971.

\bibitem{martens}
W.~Czerwinski, W.~Martens, and T.~Masopust.
\newblock Efficient separability of regular languages by subsequences and
  suffixes.
\newblock In {\em ICALP'13}, pages 150--161, 2013.

\bibitem{kfacto}
M.~Kufleitner.
\newblock The height of factorization forests.
\newblock In {\em MFCS'08}, 2008.

\bibitem{mnpfo}
R.~McNaughton and S.~Papert.
\newblock {\em Counter-Free Automata}.
\newblock {MIT} Press, 1971.

\bibitem{pinbridges}
J.-E. Pin.
\newblock {Bridges for concatenation hierarchies}.
\newblock In {\em {ICALP'98}}, 1998.

\bibitem{Pin-ThemeVar2011}
J.-E. Pin.
\newblock Theme and variations on the concatenation product.
\newblock In {\em 4th Int. Conf. on Algebraic Informatics}, pages 44--64.
  Springer, 2011.

\bibitem{pin-straubing:upper}
J.-E. Pin and H.~Straubing.
\newblock {Monoids of upper triangular boolean matrices}.
\newblock In {\em {Semigroups. Structure and Universal Algebraic Problems}},
  volume~39 of {\em Colloquia Mathematica Societatis Janos Bolyal}, pages
  259--272. North-Holland, 1985.

\bibitem{pwdelta}
J.-E. Pin and P.~Weil.
\newblock Polynomial closure and unambiguous product.
\newblock {\em Theory of Computing Systems}, 30(4):383--422, 1997.

\bibitem{pinweilVD}
J.-E. Pin and P.~Weil.
\newblock {The wreath product principle for ordered semigroups}.
\newblock {\em Communications in Algebra}, 30:5677--5713, 2002.

\bibitem{pvzmfcs13}
T.~Place, L.~{\swap{Rooijen}{van }}, and M.~Zeitoun.
\newblock Separating regular languages by piecewise testable and unambiguous
  languages.
\newblock In {\em MFCS'13}, pages 729--740, 2013.

\bibitem{pvzltt}
T.~Place, L.~van Rooijen, and M.~Zeitoun.
\newblock Separating regular languages by locally testable and locally
  threshold testable languages.
\newblock In {\em FSTTCS'13}, LIPIcs, 2013.

\bibitem{pz:qalt:2014}
T.~Place and M.~Zeitoun.
\newblock Going higher in the first-order quantifier alternation hierarchy on
  words.
\newblock {\em Arxiv}, 2014.

\bibitem{pzfo}
T.~Place and M.~Zeitoun.
\newblock Separating regular languages with first-order logic.
\newblock In {\em CSL-LICS'14}, 2014.

\bibitem{sfo}
M.~P. Sch{\"u}tzenberger.
\newblock On finite monoids having only trivial subgroups.
\newblock {\em Information and Control}, 8:190--194, 1965.

\bibitem{simon75}
I.~Simon.
\newblock Piecewise testable events.
\newblock In {\em 2nd GI Conference on Automata Theory and Formal Languages},
  pages 214--222, 1975.

\bibitem{simonfacto}
I.~Simon.
\newblock Factorization forests of finite height.
\newblock {\em {TCS}}, 72(1):65--94, 1990.

\bibitem{StrauConcat}
H.~Straubing.
\newblock A generalization of the {S}ch{\"u}tzenberger product of finite
  monoids.
\newblock {\em {{TCS}}}, 1981.

\bibitem{StrauVD}
H.~Straubing.
\newblock Finite semigroup varieties of the form {V {\textasteriskcentered} D}.
\newblock {\em J. Pure App. Algebra}, 36:53--94, 1985.

\bibitem{StrauDD2}
H.~Straubing.
\newblock Semigroups and languages of dot-depth two.
\newblock {\em {TCS}}, 1988.

\bibitem{bookstraub}
H.~Straubing.
\newblock {\em Finite Automata, Formal Logic and Circuit Complexity}.
\newblock 1994.

\bibitem{Tesson02diamondsare}
P.~Tesson and D.~Therien.
\newblock Diamonds are forever: The variety {DA}.
\newblock In {\em Semigroups, Algorithms, Automata and Languages}, pages
  475--500. World Scientific, 2002.

\bibitem{TheConcat}
D.~Th{\'e}rien.
\newblock Classification of finite monoids: the language approach.
\newblock {\em {{TCS}}}, 1981.

\bibitem{twfodeux}
D.~Th\'{e}rien and T.~Wilke.
\newblock Over words, two variables are as powerful as one quantifier
  alternation.
\newblock In {\em STOC'98}, pages 234--240. ACM, 1998.

\bibitem{Thom82}
W.~Thomas.
\newblock Classifying regular events in symbolic logic.
\newblock {\em J. Comp. Syst. Sci.}, 1982.

\bibitem{ThomStrict}
W.~Thomas.
\newblock A concatenation game and the dot-depth hierarchy.
\newblock In {\em Computation Theory and Logic}, pages 415--426. 1987.

\end{thebibliography}

\appendix
\newpage

\section*{Appendix}
We divide this appendix into several sections. In Appendix~\ref{app:facto}, we
define the main tools we will use for our proofs: \efgame games and
factorization forests.  In Appendix~\ref{app:algo}, we complete
Section~\ref{sec:comput} by proving the correctness and completeness of our
algorithm for computing \dchains. In Appendix~\ref{app:sig}, we prove
Theorem~\ref{thm:caracsig}, i.e. our characterization of \siw{i} (which is
decidable for ). The remaining appendices are then devoted to the
proof of Theorem~\ref{thm:caracbc}, i.e. our decidable characterization of
\bswd. In Appendix~\ref{app:ctrees} we define \emph{\Chains Trees} which are
our main tool for proving the difficult direction of the characterization. In
Appendix~\ref{app:bc} we give an outline of the proof. Finally,
Appendix~\ref{app:depth} and Appendix~\ref{app:width} are devoted to proving
the two most difficult propositions in the proof.

\section{Tools}
\label{app:facto}
In this appendix we define \efgame games and factorization
forests. Both notions are well-known and we will use them
several times in our proofs.

\subsection{\efgame Games}

It is well known that the expressive power of logics can be
expressed in terms of games. These games are called \efgame games. We
define here the game tailored to the quantifier alternation hierarchy.

Before we give the definition, a remark is in order. There are actually two
ways to define the class of \siw{i}-definable languages.  First, one can
consider all first-order formulas and say that a formula is \siw{i} if it has
at most  blocks of quantifiers once rewritten in prenex normal form. This
is what we do. However, one can also restrict the set of allowed formulas to
those that are already in prenex form and have at most  blocks of
quantifiers. While this does not change the class of \siw{i}-definable
languages as a whole, this changes the set of formulas of quantifier rank 
for a fixed . Therefore, this changes the preorder . This means
that there is a version of the \efgame game for each definition. In this
paper, we use the version that corresponds to the definition given in the
main part of the paper (\emph{i.e.}, the one considering all first-order formulas).

\medskip
\noindent
{\bf \efgame games.} Set  a level in the quantifier alternation
hierarchy. We define the game for \siw{i}. The board of the game
consists of two words  and there are two players called
\emph{Spoiler and Duplicator}. Moreover, there exists a
distinguished word among  that we call the \emph{active
  word}. The game is set to last a predefined number  of
rounds. When the game starts, both players have  pebbles. Moreover,
there are two parameters that get updated during the game, the active
word and a counter  called the \emph{alternation counter}. Initially,  is
set to .

At the start of each round , Spoiler chooses a word, either  or
. Spoiler can always choose the active word, in which case both 
and the active word remain unchanged. However, Spoiler can only choose the
word that is not active when , in which case the active
word is switched and  is incremented by  (in particular this
means that the active word can be switched at most  times). If
Spoiler chooses  (resp. ), he puts a pebble on a position  in
 (resp.  in ).

Duplicator must answer by putting a pebble at a position  in
 (resp.  in ). Moreover, Duplicator must ensure that all
pebbles that have been placed up to this point verify the following
condition: for all  , the labels at positions
 are the same, and  iff .  

Duplicator wins if she manages to play for all  rounds, and Spoiler
wins as soon as Duplicator is unable to play.

\begin{lemma}[Folklore] \label{lem:efgame}
  For all  and ,  iff
  Duplicator has a winning strategy for playing  rounds in the
  \siw{i} game played on  with  as the initial active word.
\end{lemma}

Note that we will often use Lemma~\ref{lem:efgame} implicitly and
alternate between the original and the game definition of
. We now give a few classical lemmas on \efgame games that
we reuse several times in our proofs. We begin with a 
lemma stating that  is a pre-congruence, \emph{i.e.} that it is
compatible with the concatenation product.

\begin{lemma} \label{lem:efconcat}
  Let  and let  such that  and . Then .
\end{lemma}

\begin{proof}
  By Lemma~\ref{lem:efgame}, Duplicator has winning strategies in the level
   games between  and , with  as initial
  active words respectively. These strategies can be easily combined into a
  strategy for the level  game between  and  with
   as intial active word. We conclude that . \qed
\end{proof}

The second property concerns full first-order logic.

\begin{lemma} \label{lem:aperiodic} Let  be such that
  . Let . Then
  
\end{lemma}

\begin{proof}
  This is well known for full first-order logic
  (see~\cite{bookstraub} for details).\qed
\end{proof}

We finish with another classical property, which is this time specific
to \siw{i}.

\begin{lemma} \label{lem:siprop}
  Let , let  be such
  that  and let  such that . Then we have:
  
\end{lemma}

\begin{proof}
  Set  and . We prove
  that  using an \efgame argument: we prove that Duplicator
  has a winning strategy for the game in  rounds for \siw{i+1} played on  with 
  as initial active~word. The proof goes by induction on . We
  distinguish two cases depending on the value, 0 or 1, of the alternation counter 
  after Spoiler has played the first round.

  \medskip
  \noindent
  {\bf Case 1: .} In this case, by definition of the game, it
  suffices to prove that . From our hypothesis we
  already know that . Moreover, it follows from
  Lemma~\ref{lem:aperiodic} that 
  and . It then follows from
  Lemma~\ref{lem:efconcat} that .

  \medskip
  \noindent
  {\bf Case 2: .} By definition, this means that Spoiler played on some
  position  in . Therefore  is inside a copy of the word . Since
   contains more than  copies of , by symmetry we can assume
  that there are at least  copies of  to the right of . We now
  define a position  inside  that will serve as Duplicator's
  answer. We choose  so that it belongs to a copy of  inside  and
  is at the same relative position inside this copy as  is in its own copy
  of~. Therefore, to fully define , it only remains to define the copy
  of  in which we choose~. Let  be the number of copies of  to
  the \emph{left} of  in , that is,  belongs to the
  -th copy of    starting from the left of . If , then  is chosen inside the -th copy of 
  starting from the left of . Otherwise,  is chosen inside the
  -th copy of  starting from the left of . Observe
  that these copies always exist, since .

  Set  and , with the two distinguished  factors 
  being the copies containing the positions . By definition of
  the game, it suffices to prove that  and  to conclude that Duplicator
  can play for the remaining  rounds. If , then by 
  definition, , therefore it is immediate that
  . Otherwise, both  and
   are concatenations of at least  copies of
  . Therefore  follows
  Lemma~\ref{lem:aperiodic}. Finally observe that by definition  and  with  and . Therefore, it is immediate
  by induction on  that . \qed
\end{proof}



\subsection{Simon's Facorization Forests Theorem}

In this appendix, we briefly recall the definition of factorization
forests and state the associated theorem. Proofs and 
more detailed presentations can be found in~\cite{kfacto,bfacto}

Let  be a finite monoid and  a
morphism. An \emph{-factorization forest} is an ordered
unranked tree with nodes labeled by words in  and such that for
any inner node  with label , if  are its children
listed from left to right with labels , then
. Moreover, all nodes  in the forest must be of
the three following kinds:

\begin{itemize}
\item \emph{leaf nodes} which are labeled by either a single letter or
  the empty word.
\item \emph{binary nodes} which have exactly two children.
\item \emph{idempotent nodes} which have an arbitrary number of
  children whose labels  verify  for some idempotent .
\end{itemize}

If , an \emph{-factorization forest for } is an
-factorization forest whose root is labeled by .

\begin{theorem}[Factorization Forest Theorem of
  Simon~\cite{simonfacto,kfacto}] \label{thm:facto}
  For all , there exists an -factorization forest for
   of height smaller than .
\end{theorem}












\section{Appendix to Section~\ref{sec:comput}: Proving the Algorithm}
\label{app:algo}
In this appendix, we prove Proposition~\ref{prop:compu}, that it is
the correctness and completeness of our algorithm which computes sets
of compatible \dchains. Recall that our algorithm works by
fixpoint. Given as input a morphism  into a
finite monoid  and a natural , it applies iteratively
the procedure , starting from the application , where  is the set of trivial sets of compatible
\dchains of length  for . The fixpoint is a collection of
sets indexed by subalphabets , denoted by . 

We have to show that when the algorithm reaches its fixpoint, the
computed set  consists
exactly of all compatible sets of \dchains of length~. This is
formulated in Proposition~\ref{prop:compu}, which we restate. In
addition, it states that for every length , one can compute a rank
 that suffices to capture all sets of compatible sets of
\dchains of length . In the following, we~let


\adjustc{prop:compu}
\begin{proposition}
  Let ,  and . Then 
\end{proposition}
\restorec

We proceed by induction on . Observe that when  all three
sets are by definition equal to , therefore the result is
immediate. Assume now that . Using our induction hypothesis
we have the following fact.

\begin{fact} \label{fct:inducomp}
  Let , then . Moreover, it follows that .
\end{fact}



For all , we prove the following inclusions:
. Observe that  is immediate by definition. Therefore, we
have two inclusions to prove:
\begin{itemize}
\item , this corresponds to correctness of the
  algorithm: all computed sets are indeed sets of compatible
  \dchains. 
\item , this corresponds to completeness of the
  algorithm: all sets of compatible \dchains are computed.
\end{itemize}
\noindent
We give each proof its own subsection. Note that
Fact~\ref{fct:inducomp} (i.e., induction on ) is
only used in the completeness proof.

\subsection{Correctness of the Algorithm}
In this subsection, we prove that for all , . This is
a consequence of the following proposition.

\begin{proposition} \label{prop:correc}
  Set , for all , .
\end{proposition}

Before proving Proposition~\ref{prop:correc}, we explain how it is
used to prove correctness. By definition, for all ,
. Therefore, it is immediate from the
proposition that . Moreover, by definition, . We conclude that
 which 
terminates the correctness proof. It now remains to prove
Proposition~\ref{prop:correc}.

Let ,  and . We need to prove that . By
definition,  for some
. We proceed by induction on . If , this is immediate
since .

Assume now that . For all , we set . By induction hypothesis, for every
, every element of
 belongs to .
Since , by definition we have  with

If , it is immediate by induction that  and we are finished. Assume now that . This means that there exist  such that ,
 and  such that . By induction hypothesis, we have  and . It is
then immediate by Fact~\ref{fct:setcomp} that .

It remains to treat the case when . In that case, we
get  such that . In the following, we
write  (with  as
). Note that by definition of the number , we have
, and in particular, . Observe first that by
induction hypothesis, we know that . In
particular, this means that all \chains in  have the same first
element. We denote by  this element. By definition of
, we get  such that ,  and for all \chains 
there exist  satisfying  and for all ,  and
. 

\smallskip We now prove that . Set  and , by definition . Observe that since , every \chain in \Rs has  as first element. We now
prove that for any \chain , there exist
 satisfying  and for all ,  and . By
definition, this will mean that . Set
. By hypothesis,  with 
 and . In particular, . Since , we have
, so we get
 such that for all ,
,  and
 and we have:

On the other hand, using the fact that , we get words , mapped to
 by  and all having alphabet , such that . For all , set . Observe that for any ,  
and . Therefore it remains to prove that  to terminate the
proof. That  is immediate by
Lemma~\ref{lem:efconcat}. Recall that , therefore the
last inequality is a consequence of the following lemma.

\begin{lemma} \label{lem:efcorrec}
  
\end{lemma}


\begin{proof}
  By Lemma~\ref{lem:efconcat}, we have . Therefore, it suffices to prove that  to conclude. Recall that by definition
  , therefore, it is straightforward to
  see that 

  

  Moreover, we chose . Therefore, it is immediate from Lemma~\ref{lem:siprop}
  and~\eqref{eq:inegcorrec} that . \qed 
\end{proof}

\subsection{Completeness of the Algorithm}
We need to prove that for all , we have
 for , where . We do this by proving a slightly more general
proposition by induction. To state this proposition, we need more
terminology.

\medskip
\noindent
{\bf Generated Compatible Sets.} Set ,  and . We set  as the following set
of \chains of length :  iff
 and there exists  satisfying
\begin{itemize}
\item for all , .
\item .
\end{itemize}
\noindent
Observe that the last item implies that all  have the same
alphabet . Therefore, by definition, any  is a compatible set of
\dchains of length : . Moreover, any compatible set of \dchains of
length ,  is a subset of  for
some  of alphabet . We finish the definition with a decomposition lemma
that will be useful in the proof.


\begin{lemma} \label{lem:gendecomp}
  Let  and  with , then: 
  
\end{lemma}


\begin{proof}
  Let . By definition,
  there exists  such that , for
  all ,  and . Using a simple \efgame argument, we obtain that all words 
  can be decomposed as  with  and for all : . For all , set . By definition, for all , . Moreover, we have 
  
  Therefore, we have  which terminates the proof. \qed
\end{proof}

We can now state our inductive proposition and prove that
. Set  defined as 
.

\begin{proposition} \label{prop:comp}
  Let ,  and  that admits a
  -factorization forest of height  and such that . Set , then .
\end{proposition}

Before proving Proposition~\ref{prop:comp}, we explain how to use it
to terminate our completeness proof. Set , by definition, this means that there exists
 such that  and . By Theorem~\ref{thm:facto}, we know that  admits a
-factorization forest of height at most .
Therefore, by choice of , we can apply
Proposition~\ref{prop:comp} and we obtain . By definition of
 it is then immediate that  which terminates the proof.

It remains to prove Proposition~\ref{prop:comp}. Note that this is
where we use Fact~\ref{fct:inducomp} (i.e. induction on ).
Set  that admits a -factorization forest of height
,  and . We need to prove that , i.e., to construct  such that . The proof is by induction on the height  of the factorization
forest of . It works by applying the proposition inductively to
the factors given by this factorization forest. In particular, we
will use Lemma~\ref{lem:gendecomp} to decompose 
according to this factorization forest. Then, once the factors have
been treated by induction, we will use the definition of the
procedure  (i.e. Operations~\eqref{eq:mul} and~\eqref{eq:oper})
to conclude. In particular, we will use the following fact several
times.

\begin{fact} \label{fct:semig}
   is subsemigroup of .
\end{fact}

\begin{proof}
  We prove that  is subsemigroup of
  , the result is then immediate by definition of .
  Set . By definition
  of  (see Operation~\eqref{eq:mul}), we have .\qed
\end{proof}

We now start the induction. We distinguish three cases depending on
the nature of the topmost node in the -factorization forest of
.

\medskip
\noindent
{\bf Case 1: the topmost node is a leaf.} In that case,  and
 is a single letter word . In particular . Observe that , therefore, one can verify that
. It follows that
 which terminates the proof for this  
case.

\medskip
\noindent
{\bf Case 2: the topmost node is a binary node.} We use induction on
 and Operation~\eqref{eq:mul} in the definition of . By
hypothesis  with  words admitting 
-factorization forests of heights . Set  and , by definition,
we have . Moreover, observe that

Therefore, we can apply our induction hypothesis to  and we
obtain  and  such that  and . By
Operation~\eqref{eq:mul} in the definition of , it is immediate
that . Moreover, by Lemma~\ref{lem:gendecomp}, . It follows that  which terminates this case.


\medskip
\noindent
{\bf Case 3: the topmost node is an idempotent node.} This is the 
most difficult case. We use induction on ,
Operation~\eqref{eq:oper} in the definition of  and
Fact~\ref{fct:semig}. Note that this is also where
Fact~\ref{fct:inducomp} (i.e. induction on  in the general proof of
Proposition~\ref{prop:compu}) is used. We begin by summarizing our
hypothesis:  admits what we call an -decomposition.

\medskip
\noindent
{\bf -Decompositions.} Set ,  an idempotent and . We
say that  admits an \emph{-decomposition} 
if
\begin{enumerate}[label=,ref=\alph*]
\item\label{item:1} ,
\item\label{item:2} for all ,  and
   and
\item\label{item:3} for all , .
\end{enumerate}
Note that  means that  is a constant idempotent, where we recall that  is the morphism defined by .

\begin{fact} \label{fct:ebdecomp}
   admits an -decomposition for some idempotent .
\end{fact}

\begin{proof}
  By hypothesis of Case~3, there exists a decomposition 
  of  that satisfies points  and . Moreover, for all ,
   admits a -factorization forest of height . Therefore point  is obtained by induction hypothesis on the height
  . \qed
\end{proof}

For the remainder of this case, we assume that the idempotent  and the -decomposition  of  are fixed. We
finish the definition, with the following useful fact, which follows
from Fact~\ref{fct:inducomp}.

\begin{fact} \label{fct:correc}
  Assume that  admits an \emph{-decomposition}
   and let . Then, .
\end{fact}

\begin{proof}
  Let . Since , we have . Moreover, it is immediate from
  Fact~\ref{fct:inducomp} that . We conclude that . \qed
\end{proof}

Recall that we want to prove that . In general, the number of factors
 in the -decomposition of  can be arbitrarily large. In
particular, it is possible that . This
means that we cannot simply use Lemma~\ref{lem:gendecomp} as we did in
the previous case to conclude that . However, we will partition  
as a bounded number of subdecompositions that we can treat using
Operation~\eqref{eq:oper} in the definition of . The partition
is given by induction on a parameter of the -decomposition
 that we define now.

\medskip
\noindent
{\bf Index of an -decomposition.} Set  (the
size of the monoid ). Let  that admits an
-decomposition  and let  such that  (i.e.  is the index of one of the first  factors in the decomposition). The \emph{-sequence} occurring
at  is the sequence . The \emph{index} of  is the number of
-sequences that occur in . Observe that by
definition, there are at most 
-sequences. Therefore the index of the decomposition is bounded
by . We proceed by induction on the index of the
decomposition and state this induction in the following lemma.


\begin{lemma} \label{lem:inducase3}
  Let  admitting an -decomposition  of
  index  and set . Then .
\end{lemma}

Before proving this lemma, we use it to conclude Case~3. We know that our
-decomposition  has an index . Therefore, it suffices to prove that  to conclude that
 using
Lemma~\ref{lem:inducase3}. One can verify that  as soon as . It is then immediate that


\begin{proof}[of Lemma~\ref{lem:inducase3}]
  The proof goes by induction on the index . We distinguish two cases
  depending on whether there exists a -sequence that occurs at two
  different positions in the -decomposition.

  Assume first that this is not the case, i.e., all -sequences occurring at
  positions  are different. Since there are exactly 
  -sequences occurring in the decomposition, a simple pigeon-hole
  principle argument yields that . We use our choice of
   to conclude with a similar argument to the one we used in
  Case~2. By Lemma~\ref{lem:gendecomp}, we have: 
  
  Observe that by hypothesis of this case, . Therefore, by definition of -decompositions,
  for all , . It is then immediate from
  Fact~\ref{fct:semig} that . We conclude that
   which terminates this case.



  \medskip
  Assume now that there exist  such that , and the
  -sequences occurring at  and  are the same. For the
  remainder of the proof, we set  as this
  common -sequence. Moreover, we assume that  and  are
  chosen minimal and maximal respectively, i.e. there exists no
   or  such that  occur
  at . By definition of a -sequence, recall that we have
  . Set
  
  By Lemma~\ref{lem:gendecomp}, we know that
  
  We prove that for , . By Fact~\ref{fct:semig}, it will
  then be immediate that  which terminates the proof. Observe
  that by choice of ,  and  are -decompositions of index smaller than  (the
  -sequence  does not occur in these
  decompositions). Therefore, it is immediate by induction hypothesis on
   that .

  \medskip
  It remains to prove that . If , then 
  admits an -decomposition of length smaller than  and
  we can conclude using Lemma~\ref{lem:gendecomp} as in the previous
  case. Therefore, assume that  and set  and observe that by definition . Moreover,
  , using
  Lemma~\ref{lem:gendecomp} we get that  
  
  By definition  is the -sequence occurring
  at both  and . Therefore, it follows that 
  
  Intuitively, we want to find an idempotent in the sequence  in order to apply
  Operation~\eqref{eq:oper}. Observe that since the  are elements
  of the monoid  and , the sequence  must contain a ''loop.'' By this we mean that there
  exists  such that . Set ,  and . By definition of , we have . Note that by Fact~\ref{fct:semig}, we have . By replacing this
  in~\eqref{eq:inc2}, we get
  
  Moreover, observe that , therefore, using
  Fact~\ref{fct:correc}, we get that . Moreover, since all \chains in  have 
  as first element (see Fact~\ref{fct:correc}), it is immediate that
  . This
  yields
  
  Since , it is
  immediate by Operation~\eqref{eq:oper} in the definition of 
  that . It then follows from
  Fact~\ref{fct:semig} that  and therefore that  which terminates the proof.\qed
\end{proof}


\section{Proof of Theorem~\ref{thm:caracsig}: Characterization of \siw{i}}
\label{app:sig}
In this appendix, we prove Theorem~\ref{thm:caracsig}, i.e., our
characterization for \siw{i}. For this whole appendix, we assume that
the level  in the quantifier alternation hierarchy is fixed.

\adjustc{thm:caracsig}
\begin{theorem}
  Let  be a regular language and  be its
  syntactic morphism. For all ,  is definable in \siw{i}
  iff  satisfies: 
  
\end{theorem}
\restorec
There are two directions. We give each one its own subsection.

\subsection{Equation~\eqref{eq:sig} is necessary}

We prove that the syntactic morphism of any \siw{i}-definable language
satisfies~\eqref{eq:sig}. We state this in the following proposition.

\begin{proposition} \label{prop:signec}
  Let  be a \siw{i}-definable language and let  be its syntactic morphism. Then 
  satisfies~\eqref{eq:sig}.
\end{proposition}

\begin{proof}
  By hypothesis,  is defined by some \siw{i}-formula . Let  be
  its quantifier rank. Set , we need to prove
  that . Since , by definition, there exist  such that ,  and .  By Lemma~\ref{lem:siprop}, we
  immediately obtain
  
  It then follows from Lemma~\ref{lem:efconcat} that for any 
  we have:
  
  By definition, this means that 
  implies that . Which, by definition of the syntactic preorder, means that .\qed
\end{proof}


\subsection{Equation~\eqref{eq:sig} is sufficient}

It remains to prove that whenever  satisfies~\eqref{eq:sig},
 is definable in \siw{i}. This is a consequence of the following
proposition.

\begin{proposition} \label{prop:signec}
  Let  be a regular language such that its syntactic morphism  satisfies~\eqref{eq:sig}. Then there exists  such that for all :
  
\end{proposition}
Assume for now that Proposition~\ref{prop:signec} holds and let
 satisfy~\eqref{eq:sig}. Let then  with 
and . By Proposition~\ref{prop:signec}, we deduce that
 which, by definition of the preorder ,
implies that . Therefore,  saturates , so  is
definable in \siw{i}.

It remains to prove Proposition~\ref{prop:signec}. We begin by
choosing . The choice depends on the following lemma. Recall that
 is the set of chains of length 2 belonging
to .

\begin{fact} \label{fct:boundk}
  For any morphism  into a finite monoid ,
  there exists  such that for all ,
  .
\end{fact}

\begin{proof}
  This is because for all , . Since  is a finite set, there
  exists an index  such that for all ,
  . It is then
  immediate by definition that .\qed
\end{proof}

Observe that while proving proving the existence  is easy, the
proof is non-constructive and computing  from  is a
difficult problem. In particular, having  allows us to compute all
\ichains of length  via a brute-force algorithm. When , we
proved in Proposition~\ref{prop:compu} that it suffices to take .

We can now prove Proposition~\ref{prop:signec}. Set  as defined
in Fact~\ref{fct:boundk} for . This means that  is a
\qchain{i-1} for  iff there exists  such that
,  and . We prove
that Proposition~\ref{prop:signec} holds for . This
follows from the next lemma.


\begin{lemma} \label{lem:factosig}
  Let  and , such that  admits an
  -factorization forest of height smaller than . Then  
  
\end{lemma}

Observe that by Theorem~\ref{thm:facto} all words admit an
-factorization forest of height less than . Therefore,
Proposition~\ref{prop:signec} is an immediate consequence of
Lemma~\ref{lem:factosig}. It remains to prove the lemma.

\begin{proof}[of Lemma~\ref{lem:factosig}]
  We distinguish three cases depending on the nature of the topmost node
  in the -factorization forest of . If the topmost node is a
  leaf then  is a single letter word. Moreover, since , we have , therefore,  and
  .

  If the topmost node is a binary node then  with
   admitting -factorization forests of height . Using a simple \efgame argument, we get that  with  and . Since ,
  we can use our induction hypothesis which yields that  and . By combining the
  two we obtain that .


  If the topmost node is an idempotent node for some idempotent ,
  then  such that  and  admit -factorization forests of
  height . By using a simple \efgame argument we get
  that  such that ,  and . Applying the induction hypothesis as in
  the previous case, we get that  and
  . However, we cannot apply induction
  on  since the height of its -factorization forest has not
  decreased. We use Equation~\eqref{eq:sig} instead. We know that , therefore, by choice of , we have
  . Recall that by
  hypothesis of this case, . Therefore, by
  Equation~\eqref{eq:sig}, we get that:
  
  \noindent
  which terminates the proof.\qed
\end{proof}

\section{Analyzing \dChains: \Chain Trees}
\label{app:ctrees}
In this appendix, we define \chain trees. \Chain trees are our main
tool in the proof of the difficult 'if' direction of
Theorem~\ref{thm:caracbc}. The main goal of the notion is to analyze
how \dchains are constructed. In particular we are interested in a
specific property of the set of \dchains that we define now.

\medskip
\noindent
{\bf Alternation.} Let  be a finite monoid. We say that a \chain  has \emph{alternation}  if there are exactly 
indices  such that . We say that a set of
\chains  has \emph{bounded alternation} if there exists a bound
 such that all \chains in  have alternation at
most .

We will see in Appendix~\ref{app:bc} that  having
bounded alternation is another characterization of . The
difficult direction of Theorem~\ref{thm:caracbc} will then be reduced
to proving that if  has \emph{unbounded alternation}
then one of the two equations in the characterization is contradicted.
Therefore, we will need a way to analyze how \dchains with high
alternation are built. In particular, we will need to extract a
property from the set of \dchains that decides which equation is
contradicted. This is what \chain trees are for. Intuitively, a \chain
tree is associated to a single \dchain and represents a computation of
our algorithm (see Section~\ref{sec:comput}) that yields this \dchain.

As we explained in the main paper, one can find connections between
our proof and that of the characterization of boolean combination of
open sets of trees~\cite{bpopen}. In~\cite{bpopen} as well, the
authors consider a notion of  ``\chains'' which corresponds to open
sets of trees and need to analyze how they are built. This is achieved
with an object called ``Strategy Tree''. Though strategy trees and
\chain trees share the same purpose, i.e., analyzing how \chains are
built, there is no connection between the notions themselves since
they deal with completely different objects.

We organize the appendix in three subsections. We first define the 
general notion of \chain trees. In the second subsection, we define
the main tool we use to analyze \chain trees: context values. In
particular, we prove that we can use context values to generate
-schemas. Finally, in the last subsection, we define a strict
subset of \chain trees: the \emph{locally optimal \chain trees} and
prove that it suffices to consider only such  \chain trees (i.e., that
for any \dchain there exists a locally optimal \chain tree that
``computes'' it). 

\subsection{Definition}

Set  a morphism into a finite monoid . We
associate to  a set  of \emph{\chain trees}. As we
explained, a \chain tree is associated to a single \dchain for  and
represents a way to compute this \dchain using our algorithm. Note that our
algorithm works with sets of compatible sets of \dchains, while \chain trees
are for single \dchains. This difference will be reflected in the definition.
For all  we define .

\medskip
\noindent
{\bf \Chain Trees.} Set . A \emph{\chain tree}  \emph{of
  level}  for  is an ordered unranked tree that may have
two types of (unlabeled) inner nodes: product nodes and operation
nodes, and two types of leaves, labeled with a \dchain of length :
initial leaves and operation leaves. Moreover, to each node  in
the tree, we associate an alphabet  and a
value  by induction on the structure of the tree.

Intuitively, each type of node corresponds to a part of the algorithm that
computes \dchains. Initial leaves correspond to the initial trivial compatible
sets from which the algorithm starts, product nodes correspond to the product~\eqref{eq:mul},
finally operation nodes and leaves can only be used together and correspond to
the application of~\eqref{eq:oper}. We now give a precise definition of each
type of node.

\medskip
\noindent
{\it Initial Leaves.} An initial leaf  is labeled  with a constant
\dchain  for some .
We set  and .

\medskip
\noindent
{\it Operation Leaves.} An operation leaf  is labeled with an arbitrary
\dchain  for some .
We set  and . Note that we
will set constraints on the parents of operation leaves. In
particular, these parents are always operation nodes. We will see
this in details when defining operation nodes.


\medskip
\noindent
{\it Product Nodes.} A product node  is unlabeled. It can have an
arbitrary number of children  which are all initial
leaves, product nodes or operation nodes. In particular, we set
 and
.

\medskip
\noindent
{\it Operation Nodes.} An operation node  has exactly  children sharing the same alphabet . The -th
child, called the \emph{central child} of , has to be an operation 
leaf. The other children, called the \emph{context children} of ,
are either operation nodes, product nodes or initial leaves and the
set of their values must be \emph{compatible for } (i.e. it
must belong to ). Finally, we set a restriction on
the value of the central child. Since the values of the context
children of  form a compatible set of \dchains, they all share the
same first component, that we call . We require the first component
of the value of the central child to be . This means that
the central child is an operation node labeled with
. Finally, we
set  and .







\medskip
This terminates the definition of \chain trees. The alphabet and value
of a \chain tree , \content{T} and \val{T}, are the alphabet and
value of its root. We give an example of a \chain tree in
Figure~\ref{fig:ctree}. Moreover, the following fact is immediate by
definition.





\begin{fact} \label{fct:value}
  Let  be a \chain tree and let  be its leaves listed
  from left to right. Then .
\end{fact}


\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \node[nod] (x) at (0.0,0.0) {o};

      \node[nod] (x1) at (-4.8,-1.5) {\scriptsize p};
      \node[nod] (x2) at (-1.6,-1.5) {\scriptsize p};

      \node[nod] (x3) at (1.6,-1.5) {\scriptsize p};
      \node[nod] (x4) at (4.8,-1.5) {\scriptsize p};



      \draw (x) to (x1);
      \draw (x) to (x2);
      \draw (x) to (x3);
      \draw (x) to (x4);
      \draw[thick,dotted] (x1) to (x2);
      \draw[thick,dotted] (x3) to (x4);


      \node[nor,align=center,very thick,fill=white] (la) at (0.0,-1.5)
      {\small \\};
      \draw[siar,-] (la) to (x);

      \draw[thick,decoration={brace,mirror},decorate] () to coordinate (l1)
      ();

      \node[anchor=south,inner sep=0pt,fill=white] at () { Left children};


      \draw[thick,decoration={brace,mirror},decorate] () to coordinate (l2)
      ();

      \node[anchor=south,inner sep=0pt,fill=white] at () { Right children};



      \begin{scope}[xshift=-4.8cm]

        \node[nol,anchor=east] (y1) at (-0.3,-2.0) {\small };
        \node[nod] (y2) at (+0.3,-3.0) {o};

        \node[nol,anchor=east] (z1) at (-0.8,-4.2) {\small };
        \node[nol,anchor=east] (z2) at (-0.1,-4.2) {\small };
        \node[nol,anchor=east] (z3) at (+0.7,-4.2) {\small };
        \node[nol,anchor=east] (z4) at (+1.4,-4.2) {\small };



        \draw (x1) to (y1);
        \draw (x1) to (y2);

        \draw (y2) to (z1.east);
        \draw (y2) to (z2.east);
        \draw (y2) to (z3.east);
        \draw (y2) to (z4.east);

        \draw[thick,dotted] (z1) to (z2);
        \draw[thick,dotted] (z3) to (z4);

        \draw[thick,decoration={brace,mirror},decorate] () to coordinate (l1) ();

        \node[anchor=south,inner sep=0pt,fill=white] at () {};


        \draw[thick,decoration={brace,mirror},decorate] () to coordinate (l2)
        ();

        \node[anchor=south,inner sep=0pt,fill=white] at () {};

        \node[nor,align=center,very thick] (lb) at (0.3,-6.2) {\small };

        \draw[siar,-] (lb) to (y2);

      \end{scope}

      \begin{scope}[xshift=-1.6cm]

        \node[nol,anchor=east] (y1) at (-0.3,-2.0) {\small };
        \node[nod] (y2) at (+0.3,-3.0) {o};

        \node[nol,anchor=east] (z1) at (-0.8,-4.2) {\small };
        \node[nol,anchor=east] (z2) at (-0.1,-4.2) {\small };
        \node[nol,anchor=east] (z3) at (+0.7,-4.2) {\small };
        \node[nol,anchor=east] (z4) at (+1.4,-4.2) {\small };



        \draw (x2) to (y1);
        \draw (x2) to (y2);

        \draw (y2) to (z1.east);
        \draw (y2) to (z2.east);
        \draw (y2) to (z3.east);
        \draw (y2) to (z4.east);

        \draw[thick,dotted] (z1) to (z2);
        \draw[thick,dotted] (z3) to (z4);

        \draw[thick,decoration={brace,mirror},decorate] () to coordinate (l1) ();

        \node[anchor=south,inner sep=0pt,fill=white] at () {};


        \draw[thick,decoration={brace,mirror},decorate] () to coordinate (l2)
        ();

        \node[anchor=south,inner sep=0pt,fill=white] at ()
        {};


        \node[nor,align=center,very thick] (lb) at (0.3,-6.2) {\small };

        \draw[siar,-] (lb) to (y2);

      \end{scope}

      \begin{scope}[xshift=1.6cm]

        \node[nol,anchor=east] (y1) at (-0.3,-2.0) {\small };
        \node[nod] (y2) at (+0.3,-3.0) {o};

        \node[nol,anchor=east] (z1) at (-0.8,-4.2) {\small };
        \node[nol,anchor=east] (z2) at (-0.1,-4.2) {\small };
        \node[nol,anchor=east] (z3) at (+0.7,-4.2) {\small };
        \node[nol,anchor=east] (z4) at (+1.4,-4.2) {\small };



        \draw (x3) to (y1);
        \draw (x3) to (y2);

        \draw (y2) to (z1.east);
        \draw (y2) to (z2.east);
        \draw (y2) to (z3.east);
        \draw (y2) to (z4.east);

        \draw[thick,dotted] (z1) to (z2);
        \draw[thick,dotted] (z3) to (z4);

        \draw[thick,decoration={brace,mirror},decorate] () to coordinate (l1) ();

        \node[anchor=south,inner sep=0pt,fill=white] at () {};


        \draw[thick,decoration={brace,mirror},decorate] () to coordinate (l2)
        ();

        \node[anchor=south,inner sep=0pt,fill=white] at () {};

        \node[nor,align=center,very thick] (lb) at (0.3,-6.2) {\small };

        \draw[siar,-] (lb) to (y2);

      \end{scope}

      \begin{scope}[xshift=4.8cm]

        \node[nol,anchor=east] (y1) at (-0.3,-2.0) {\small };
        \node[nod] (y2) at (+0.3,-3.0) {o};

        \node[nol,anchor=east] (z1) at (-0.8,-4.2) {\small };
        \node[nol,anchor=east] (z2) at (-0.1,-4.2) {\small };
        \node[nol,anchor=east] (z3) at (+0.7,-4.2) {\small };
        \node[nol,anchor=east] (z4) at (+1.4,-4.2) {\small };

        \draw (x4) to (y1);
        \draw (x4) to (y2);

        \draw (y2) to (z1.east);
        \draw (y2) to (z2.east);
        \draw (y2) to (z3.east);
        \draw (y2) to (z4.east);

        \draw[thick,dotted] (z1) to (z2);
        \draw[thick,dotted] (z3) to (z4);

        \draw[thick,decoration={brace,mirror},decorate] () to coordinate (l1) ();

        \node[anchor=south,inner sep=0pt,fill=white] at () {};


        \draw[thick,decoration={brace,mirror},decorate] () to coordinate (l2)
        ();

        \node[anchor=south,inner sep=0pt,fill=white] at ()
        {};

        \node[nor,align=center,very thick] (lb) at (0.3,-6.2) {\small };

        \draw[siar,-] (lb) to (y2);

      \end{scope}

      \node[anchor=west] at (-5.25,-7.3) { Operation Node (no label)};
      \node[anchor=west] at (-5.25,-7.8) { Product Node (no label)};
      \node[anchor=west] at (-5.25,-8.3) { Initial Leaf (label written inside)};
      \node[anchor=west] at (-5.25,-8.8) { Operation Leaf (label written inside)};

      \node[nod] at (-5.5,-7.3) {o};
      \node[nod] at (-5.5,-7.8) {\scriptsize p};
      \node[nol] at (-5.5,-8.3) {};
      \node[nol,very thick] at (-5.5,-8.8) {};
    \end{tikzpicture}
  \end{center}
  \caption{An example of \chain tree of level }
  \label{fig:ctree}
\end{figure}

We denote by  the set of all \chain trees of level  and
alphabet  associated to  and by  the set of all \chain
trees associated to . If  is a set of \chain trees, we define
. We now state ``correctness'' and
``completeness'' of \chain trees, i.e., a \chain is a \dchain iff it is the
value of some \chain tree. We prove this as a consequence of the validity of
our algorithm for computing \dchains, stated in
Proposition~\ref{prop:compu}. 



\begin{proposition} \label{prop:ctree}
  .
\end{proposition}

\begin{proof}
  That  is immediate
  by definition and Fact~\ref{fct:chaincomp}. We concentrate on the other
  inclusion. Since Proposition~\ref{prop:compu} deals with sets of
  compatible \dchains rather than just \dchains, we prove a slightly
  stronger result. Two \chain trees are said \emph{compatible} if they
  have the same structure, the same alphabet and differ only by the
  labels of their operation leaves. For all , we set
    as the set of all \chain trees that are
  compatible with .

  \begin{lemma} \label{lem:ctrees}
    Let . Then 
  \end{lemma}

  By Proposition~\ref{prop:compu}, if  is a \dchain of length
   for , there exists  such that . Therefore the inclusion
   is an immediate
  consequence of Lemma~\ref{lem:ctrees}. It remains to prove
  Lemma~\ref{lem:ctrees}.

  Let . We
  need to construct  such that . By definition, 
  for some . We proceed by induction on . Assume first
  that . Then . By definition
  this means that  where  is the \chain tree
  composed of a single initial leaf with label  and
  alphabet . Assume now that . For all , we
  set . By definition, we
  have  with
  
  If , the result is immediate by induction
  hypothesis. Assume now that . By definition, this means
  that there exist  such that  and  in
   such that . Using our induction
  hypothesis, we get  such that 
  and . Consider  the \chain tree
  whose topmost node is a product with  as children. It is
  immediate by definition that .

  It remains to treat the case when . By definition, we
  get  such that . Note that since , by Proposition~\ref{prop:compu},
  . We denote by  the first element
  common to all \chains in . Note that since , the first element common to all \chains in \Ts is
  . Set  as the set of all \dchains of
  length  for  that have  as first element. By
  definition . Moreover,
  
  By induction hypothesis there exists a \chain tree  of
  alphabet  such that . Let  be
  the \chain tree whose topmost node is an operation node whose context
  children are all copies of  and whose central child is the
  operation leaf labeled with some arbitrary chosen \dchain in
  . Observe that by definition, 
  . Therefore, since , we have  which
  terminates the proof. Note that the tree we obtained is particular: all
  subtrees rooted at context children of an operation node are identical. \qed
\end{proof}

\medskip
\noindent
{\bf Alternation and Recursive Alternation of a \Chain Tree.} The
\emph{alternation} of a \chain tree is the alternation of its
value. We say that  has \emph{unbounded alternation} if  
the set  has unbounded alternation. Note that by
Proposition~\ref{prop:ctree},  has unbounded
alternation iff  has unbounded alternation.

In the proof we will be interested in another property of \chain 
trees: \emph{recursive alternation}. Recursive alternation corresponds
to the maximal alternation of labels of operation leaves in the tree.
More precisely, if  is a \chain tree, its \emph{recursive
  alternation} is the largest natural  such that there exists an
\emph{operation leaf} in  whose label has alternation . An
important idea in the proof will be to separate the case when we can
find a set of \chain trees with unbounded alternation but bounded
recursive alternation from the converse one. However, in order to make
this work, we will need to add one more condition to our
trees. Intuitively, we need to know that if a tree has high recursive
alternation, this is necessary, i.e, the tree cannot be modified into 
a tree that has low alternation while keeping the same value. This is
what we do with local optimality.

\subsection{Locally Optimal \Chain Trees}

For all , we define a strict subset of  called 
the set of \emph{Locally Optimal \Chain Trees}. We then prove that
we can assume without loss of generality that all \chain trees we
consider are locally optimal.

We first define local optimality as a property of a single node  
in a \chain tree . We will generalize the notion to a whole tree
by saying that it is locally optimal if and only if all its
\emph{operation leaves} are locally optimal. Given a node , local
optimality of  depends on two parameters of : its value
 and a new parameter called its \emph{context value},
, that we define now. 

\medskip
\noindent
{\bf Context Value of a Node.} Let  be a \chain tree of level 
and let  be the leaves of  sorted in prefix
order. Recall that by Fact~\ref{fct:value}, . To every node~ of , we associate a
pair  called the \emph{context 
  value} of . Set  as the leaves of the subtree
rooted at  (in prefix order). We set . Note that since
for all , ,  is indeed a
pair in . By definition and Fact~\ref{fct:value}
one can verify the two following facts:



\begin{fact} \label{fct:value2}
  Let  be a node in a \chain tree  and set . Then .
\end{fact}
\begin{fact} \label{fct:value3}
  Let  be an inner node in a \chain tree  and set .  Set  as the children of  with
  context values . Then, for all
  ,  and
  .
\end{fact}

In many cases, we will work with context values that are constant,
i.e. . In these cases, if
 is a \chain, it will be convenient to simply write
 for .

\medskip
\noindent
{\bf Local Optimality.} Set  and  a \chain
tree. Let  be any node in , 
and . We say
that  is \emph{locally optimal for } if for all 
such that  the following condition holds:

Intuitively this means that for all , changing  to 
in the value of  is necessary to get alternation at position 
in the value of the tree (see Fact~\ref{fct:value2}). We say that a
\chain tree  is \emph{locally optimal for}  if all its
{\bf operation leaves} are locally optimal for . We say that
 is locally optimal iff it is locally optimal for .
This means that locally optimality of a \chain tree only depends on
the context values and labels of operation leaves in the tree. The
following fact is immediate from the definitions:

\begin{fact} \label{fct:optimal}
  Let . Assume that  is locally optimal for
  . Then  is locally optimal (i.e. locally optimal for
  ). 
\end{fact}

We finish with our main proposition, which states that for any \chain tree,
there exists a locally optimal one with the same value. In particular,
this means that we will always be able to assume that our \chain trees
are locally optimal.

\begin{proposition} \label{prop:optimal}
  Let  and . There exists  which is locally optimal for  and such that
  .
\end{proposition}

\begin{proof}
  Set , we explain how to construct . For all
  , we define the -alternation of  as the number of
  operation leaves  in  such that 
  with . Finally, we define the \emph{index of } as
  the sequence of its -alternations ordered with increasing .

  We can now describe the construction. Assume that  is not locally
  optimal for . We explain how to construct a second \chain tree
   such that
  \begin{enumerate}
  \item .
  \item  has strictly smaller index than .
  \end{enumerate}
  It then suffices to apply this operation recursively to  until we
  get the desired tree. We now explain the construction. Since  is
  not locally optimal for , there exists an operation leaf 
  of  that is not locally optimal for . Let
   and
  . By choice of
  , there exists  such that  and
  . We set  as 
  the \chain tree obtained from  by replacing the label of  with
  . By choice of  and
  Fact~\ref{fct:value2}, it is immediate that . Moreover, for any ,  have the
  same -alternation and  has by definition strictly smaller
  -alternation than . It follows that  has strictly smaller
  index than  which terminates the proof. \qed
\end{proof} 

\section{Proof of Theorem~\ref{thm:caracbc}: Characterization of \bswd}
\label{app:bc}
This appendix is devoted to the proof of Theorem~\ref{thm:caracbc},
i.e., the decidable characterization of \bswd. We actually prove a
more general theorem that includes a second characterization in terms
of alternation of , which will be needed as an
intermediary step when proving the difficult 'if' direction of
Theorem~\ref{thm:caracbc}.

\begin{theorem} \label{thm:caracbc2}
  Let  be a regular language and let  be its
  syntactic morphism. The three following properties are equivalent:

  \begin{enumerate}
  \item  is definable in \bswd.
  \item  has bounded alternation.
  \item  satisfies the following equations:

    

    
  \end{enumerate}
\end{theorem}

Observe that Theorem~\ref{thm:caracbc} is exactly the equivalence
between Items~1 and~3 in Theorem~\ref{thm:caracbc2}. Therefore it
suffices to prove Theorem~\ref{thm:caracbc2}. Intuitively, Item~2
seems harder to decide than Item~3, since it requires computing a
description of the whole set  rather than just the
\dchains and sets of compatible \dchains of length  and
. However, it will serve as a convenient intermediary for proving
Item~3.

We now turn to the proof of Theorem~\ref{thm:caracbc2}. We prove that
1~~3~~2~~1. In this appendix,
we give full proofs for the two ''easy'' directions: 1~~3 
and 2~~1. For the direction 3~~2, we use
\chain trees to reduce the proof to two propositions. We then give
each proposition its own Appendix: Appendix~\ref{app:depth} and
Appendix~\ref{app:width}.

\subsection{1~~3}
\label{sec:1-rightarrow-3-1}

We prove the direction 1~~3 in Theorem~\ref{thm:caracbc2}
which is stated in the following lemma.

\begin{lemma} \label{lem:bcnec}
  Let  be a regular language and  be its syntactic
  morphism. Assume that  is definable in \bswd, then 
  satisfies~\eqref{eq:bcs1} and~\eqref{eq:bcs2} .
\end{lemma}

The remainder of this subsection is devoted to proving
Lemma~\ref{lem:bcnec}. The proof is an \efgame argument. We begin by
defining the equivalence associated to \bswd. For any , we
write  iff  and  satisfy the same \bswd
formulas of quantifier rank~. Therefore, a language if definable by
a  formula of rank~ iff it is saturated by . One
can verify that  is an equivalence and that  iff  and .

We can now prove the lemma. By hypothesis there exists some 
formula  that defines , we set  as the quantifier rank  
of this formula. 

\medskip
\noindent
{\bf Proving Equation~\eqref{eq:bcs1}.} Set , we prove that  (the dual case is proved in the same
way). We prove that there exist  such that
, ,  and for all
pair of words : 

Set . By definition of \kbceq{2}, \eqref{eq:bcnec} means that  and
 cannot be distinguished by a \bswd
formula of quantifier rank . Hence, by definition of , we get

Therefore, by definition of , of , and of the syntactic monoid
this will prove that .

Since  by assumption, there
exist  such that  and
, , . Set . We need to prove that

By definition of , we have . By Lemma~\ref{lem:aperiodic}, we obtain
. Therefore, using 
Lemma~\ref{lem:efconcat} we first get , and
then that~\eqref{eq:1} holds.

The proof of  is similar: by definition, we have , and by
Lemma~\ref{lem:aperiodic} we get . Using
Lemma~\ref{lem:efconcat} again, we conclude that , and then
that~ holds.

\medskip
\noindent
{\bf Proving Equation~\eqref{eq:bcs2}.} It remains to prove that
 satisfies Equation~\eqref{eq:bcs2}. We begin with a lemma
on -schemas.

\begin{lemma} \label{lem:schemprop}
  Assume that  is a -schema. Then for all  there exist  such that:
  \begin{itemize}
  \item .
  \item  and .
  \item for all , .
  \end{itemize}
\end{lemma}

\begin{proof}
  This is proved using Lemma~\ref{lem:siprop}. Fix a -schema
   and . By definition, there exist  and  satisfying ,
   and  with  and
  . By definition
  of \dchains, we obtain words  satisfying
  the following properties:
  \begin{enumerate}[label=]
  \item 
  \item ,
    , ,  and .
  \item  and .
  \end{enumerate}

  Set  and observe that
  by item , . Moreover, by item , ,
   and . Finally, it is immediate using \efgame games that for any
  word , . Therefore it follows
  from Lemma~\ref{lem:siprop} that . Using item , we then
  conclude that . \qed  
\end{proof}

We can now use Lemma~\ref{lem:schemprop} to prove that 
satisfies Equation~\eqref{eq:bcs2}. Let  and
 be -schemas. Let  of images
 and  of images 
satisfying the conditions of Lemma~\ref{lem:schemprop}. We prove that
for any :

where again . By definition of the syntactic monoid and since  is defined by a 
\bswd formula of rank , Equation~\eqref{eq:bcs2} will
follow. Observe that the words  and  given by
Lemma~\ref{lem:schemprop} satisfy

Using Lemma~\ref{lem:efconcat}, we may multiply \eqref{eq:3} by
 on the left and by  on the right:

For the converse direction, from Lemma~\ref{lem:aperiodic}, we have 

and . Using   \eqref{eq:4} and Lemma~\ref{lem:efconcat} again, we
conclude that:

i.e.,


\subsection{2~~1}

We prove the direction 2~~1 in Theorem~\ref{thm:caracbc2}
which is stated in the following lemma. 

\begin{lemma} \label{lem:bcinter}
  Let  be a regular language and  its syntactic
  morphism. Assume that  has bounded alternation, then
   is definable in \bswd.
\end{lemma}

\begin{proof}
  Assume that  has bounded alternation. We prove that
  there exists  such that for all , . This proves that 
  is saturated with  and hence definable by a \bsw{2} formula
  of quantifier rank .

  We proceed by contradiction. Assume that for all  there
  exists  such that  and
  . Notice that since there are only
  finitely many pairs in , there must exist a pair 
  such that  and there exists arbitrarily large naturals 
  such that  and . We prove that
   which contradicts that
   has unbounded alternation (recall that ). By definition for all  there exists 
  such that  and , since  and by definition of  this means that :
  
  Hence for all ,  and therefore,
  for all    which terminates the proof.
  \qed
\end{proof}

\subsection{3~~2}

This is the most difficult direction of Theorem~\ref{thm:caracbc2}. We
state it in the following proposition.

\begin{proposition} \label{prop:bcsuff}
  Let  be a regular language,  be its
  syntactic morphism. Assume that  satisfies~\eqref{eq:bcs1}
  and~\eqref{eq:bcs2}, then  has bounded alternation.
\end{proposition}

For the remaining of the section, we assume that  and 
are fixed as in the statement of the proposition. We prove the
contrapositive of Proposition~\ref{prop:bcsuff}: if 
has unbounded alternation, then either Equation~\eqref{eq:bcs1} or
Equation~\eqref{eq:bcs2} must be contradicted. We use \chain trees to
separate this property into two properties that we will prove in
Appendix~\ref{app:depth} and Appendix~\ref{app:width}. Consider the
two following propositions 

\begin{proposition} \label{prop:width}
  Assume that there exists a set of locally optimal \chain trees  with unbounded alternation but bounded
  recursive alternation. Then  does not satisfy
  Equation~\eqref{eq:bcs1}.
\end{proposition}

\begin{proposition} \label{prop:depth}
  Assume that there exists a set of locally optimal \chain
  trees  with unbounded alternation and that
  all such sets have unbounded recursive alternation. Then  does
  not satisfy Equation~\eqref{eq:bcs2}.
\end{proposition}

Proposition~\ref{prop:width} and Proposition~\ref{prop:depth} are
proven in Appendix~\ref{app:width} and Appendix~\ref{app:depth}. We
finish this appendix by using them to conclude the proof of
Proposition~\ref{prop:bcsuff}.

If  has unbounded alternation. By
Proposition~\ref{prop:optimal}, we know that there exists a set of
locally optimal \chain trees  with
unbounded alternation. If  can be chosen with bounded recursive
alternation, there is a contradiction to Equation~\eqref{eq:bcs1} by
Proposition~\ref{prop:width}. Otherwise there is a contradiction to
Equation~\eqref{eq:bcs2} by Proposition~\ref{prop:depth} which
terminates the proof of Proposition~\ref{prop:bcsuff}.


\section{Proof of Proposition~\ref{prop:depth}}
\label{app:depth}
Recall that we fixed a morphism  into a
finite monoid . We prove Proposition~\ref{prop:depth}.

\adjustc{prop:depth}
\begin{proposition}
  Assume that there exists a set of locally optimal \chain
  trees  with unbounded alternation and that
  all such sets have unbounded recursive alternation. Then  does
  not satisfy Equation~\eqref{eq:bcs2}.
\end{proposition}
\restorec

We define a new object that is specific to this case: the \emph{\Chain
  Graph}. The \chain graph describes a construction process for a subset
of the set of \dchains for . While this subset is potentially
strict, we will prove that under the hypothesis of
Proposition~\ref{prop:depth}, it is sufficient to derive a
contradiction to Equation~\eqref{eq:bcs2}.

\medskip
\noindent
{\bf \Chain Graph}. We define a graph  whose edges
are labeled by subsets of the alphabet . We call  the
\emph{\chain graph} of . The set  of nodes of 
is the set . Let  and  be
nodes of  and , then  contains an edge
labeled by  from  to  iff there exists a
-schema  such that:

\begin{itemize}
\item .
\item  and .
\end{itemize} 

Observe that the definition does not depend on . We say that
 is \emph{recursive} if it contains a cycle such that
\begin{itemize}
\item [] all edges in the cycle are labeled by the same alphabet ,
\item [] the cycle contains two nodes , 
  such that .
\end{itemize}
We now prove
Proposition~\ref{prop:depth} as a consequence of the two following 
propositions.

\begin{proposition} \label{prop:graphcont1}
  Assume that  is recursive. Then  does not
  satisfy~\eqref{eq:bcs2}.
\end{proposition}

\begin{proposition} \label{prop:graphcont2}
  Assume that there exists a set of locally optimal \chain
  trees  with unbounded alternation and that
  all such sets have unbounded recursive alternation. Then 
  is recursive.
\end{proposition}

Observe that Proposition~\ref{prop:depth} is an immediate consequence of
Propositions~\ref{prop:graphcont1} and~\ref{prop:graphcont2}. Before proving
them, note that the notion of \chain graph is inspired from the notion of
strategy graph in~\cite{bpopen}. This is because both notions are designed to
derive contradiction to similar equations. However, our proof remains fairly
different from the one of~\cite{bpopen}. The reason for this is that the main
difficulty here is proving Proposition~\ref{prop:graphcont2}, i.e., going from
\chain trees (which are unique to our setting) to a recursive \chain graph. On
the contrary, the much simpler proof of Proposition~\ref{prop:graphcont1} is
similar to the corresponding one in~\cite{bpopen}.

\subsection{Proof of Proposition~\ref{prop:graphcont1}}

\adjustc{prop:graphcont1}
\begin{proposition}
  Assume that  is recursive then  does not satisfy~\eqref{eq:bcs2}.
\end{proposition}
\restorec

Assume that  is recursive. By definition, we get , a cycle whose edges are all labeled with  and two
consecutive nodes  and  in this cycle such
that . Since there exists an edge , we obtain a -schema 
such that

Moreover, one can verify that since  and
 are in the same cycle with all edges labeled by , there
exists another -schema  and  such
that

By combining all these
definitions we get: 

Set ,  and . One can verify that since  and
 is a -schema,  is a -schema as
well. Moreover, by reformulating the equalities above we get:

Therefore, Equation~\eqref{eq:bcs2} would require . Since  by hypothesis,  does not satisfy~\eqref{eq:bcs2} and we 
are finished.

\subsection{Proof of Proposition~\ref{prop:graphcont2}}

\adjustc{prop:graphcont2}
\begin{proposition}
  Assume that there exists a set of locally optimal \chain
  trees  with unbounded alternation and that
  all such sets have unbounded recursive alternation. Then 
  is recursive.
\end{proposition}
\restorec

In the remainder of the section, we assume that  satisfies the
hypothesis of Proposition~\ref{prop:graphcont2}. Set 
and let  be a node of , we say that
 is \emph{-alternating} if for all , there exists 
 such that the \chain
 has alternation  and . 

\begin{lemma} \label{lem:alt1}
   contains at least one -alternating node for some .
\end{lemma}

\begin{proof}
  This is because  has unbounded alternation. It follows that there
  exists a least one  such that there are \dchains with
  arbitrary high alternation and  as first element. By definition,
  the node  is then -alternating for some . \qed
\end{proof}

For the remainder of the proof we define  as a minimal alphabet
such that there exists a -alternating node in . By this
we mean that for any , there exists no
-alternating node in .

\begin{lemma} \label{lem:alt2}
  Let  be any -alternating node of . Then
  there exists a node  such that

  \begin{enumerate}
  \item  is -alternating.
  \item .
  \item .
  \end{enumerate}
\end{lemma}

By definition  has finitely many nodes. Therefore, since by
definition, there exists at least one -alternating node, it is
immediate from Lemma~\ref{lem:alt2} that  must contain a
cycle whose edges are all labeled by . Moreover, by Item~3 in
Lemma~\ref{lem:alt2}, this cycle contains two nodes  and
 such that . We conclude that  is
recursive which terminates the proof of
Proposition~\ref{prop:graphcont2}. It remains to prove Lemma~\ref{lem:alt2}.  

\begin{proof} We proceed in three steps. We first use our hypothesis
  to construct a special set of \chain trees \crr of alphabet .
  Then, we choose a \chain tree  in \crr with large enough recursive
  alternation. Finally, we use  to construct the desired node
  . We begin with the construction of \crr.

  \medskip
  \noindent
  {\bf Construction of \crr.} We construct a set \crr of \chain trees
  that satisfies the following properties: 
  \begin{enumerate}
  \item For all , .
  \item All \chains in  have  as first
    element.
  \item All trees in  are locally optimal for .
  \item  has unbounded recursive alternation.
  \end{enumerate}
  We use the fact that  is -alternating and the
  hypothesis in Proposition~\ref{prop:graphcont2}. Since  is
  -alternating, we know that for any , there exists
   such that the \chain
   has alternation  and . We 
  denote by  the set of all these
  \dchains. Observe that by definition,  has unbounded
  alternation. It follows from Proposition~\ref{prop:ctree} that one 
  can construct a set of \chain trees  whose set of values
  is exactly . By definition,  satisfies Items~1 and~2 and
   has unbounded alternation.

  We now use Proposition~\ref{prop:optimal} to construct \crr from
   which is locally optimal for  and satisfies . We now know that
  \crr satisfies properties~1 to~3. Observe that by definition \crr has
  unbounded alternation. By hypothesis of
  Proposition~\ref{prop:graphcont2}, it follows that \crr has also 
  unbounded recursive alternation and all items are satisfied.

  \medskip
  \noindent
  {\bf Choosing a \chain tree .} We now select a special
  \chain tree  in \crr. We want  to have large enough recursive
  alternation in order to use it to construct the node 
  . We define the needed recursive alternation in the
  following lemma.

  \begin{lemma} \label{lem:chooseK}
    There exists  such that for all  and all , .
  \end{lemma}

  \begin{proof}
    It suffices to take  as the largest  such that there exists
     and  with  but . \qed
  \end{proof}

  Set  with  as defined in
  Lemma~\ref{lem:chooseK}. By hypothesis on \crr (see property~4) there
  exists a tree   with recursive alternation . We set 
  as the level of .

  \medskip
  \noindent
  {\bf Construction of the node .} Set  as the first
  element in \val{T}. Recall that by choice of  in \crr, . 
  By definition of recursive alternation,  must contain an operation
  leaf  whose label  has alternation
  . Set  and . Note that since , . Recall that by Fact~\ref{fct:value2}, we have 
  
  Note that ,  and . We know that
   has alternation . It follows
  from a pigeon-hole principle argument that there exists  and a set  of size at least 
  such that for all ,  and . Observe
  that by definition, the \chain  is a subword of
   and therefore a \dchain for . By choice
  of  it follows that . Note
  that this means that the node  is
  -alternating. Therefore, by minimality of , we have .
  Choose some arbitrary , say the first element in . Recall
  that  and therefore locally optimal for
  . The following fact is immediate by definition of local
  optimality: 

  \begin{fact} \label{fct:ccont}
    .
  \end{fact}

  We now define the node . It is immediate from
  Fact~\ref{fct:ccont} that either  or
  , we set  as this element.
  Finally, we set  and . Observe that by
  Fact~\ref{fct:ccont} , therefore since
   and by choice of , we 
  know that  is -alternating. It remains to prove that
  . We already know that ,  and . We need to prove that
   is a -schema.

  \medskip

  Using the definition of operation nodes, we prove that 
  and define  such that  and  which terminates the
  proof. Set  as the parent of . By definition,  is an
  operation node, set  as the children of 
  (). By definition, 
  
  Set  has the common first value of all \chains in  and
  . By Fact~\ref{fct:value3}, we have   
  
  By Fact~\ref{fct:value2}, and definition of operation nodes, . It follows that .

  Since  has alphabet , we have 
  for some . Using~\eqref{eq:fin} and the definition of
   as , we get that . Moreover, since , .
  Using a symetrical argument, we get that .

  Finally, set \Ts as the set of \chains of length  obtained from
  \chains in \Rs by keeping only the values at component  and
  . Since \dchains are closed under subwords, it is immediate from
   that .
  Moreover, by definition, we have  and . We conclude that
   is a -schema  which terminates the proof. \qed
\end{proof}

\section{Proof of Proposition~\ref{prop:width}}
\label{app:width}
\newcommand\alt[2]{\ensuremath{\textsf{alt}(#1,#2)}\xspace}
Recall that we fixed the morphism . We prove Proposition~\ref{prop:width}.

\adjustc{prop:width}
\begin{proposition}
  Assume that there exists a set of locally optimal \chain trees  with unbounded alternation but bounded
  recursive alternation. Then  does not satisfy
  Equation~\eqref{eq:bcs1}.
\end{proposition}
\restorec

As for the previous section, we will use a new object that is specific
to this case: \emph{\chain matrices}.


\medskip
\noindent
{\bf \Chain Matrices.} Let . A \emph{\chain matrix of
  length } is a rectangular matrix with  columns and such that
rows belong to . If \mat is a \chain matrix, we
will denote by  the entry at row  (starting from the
top) and column  (starting from the left) in \mat. If \mat is a
\chain matrix of length  and with  rows, we call the \chain
, the \emph{value} of \mat. By Fact~\ref{fct:chaincomp},
the value of a \chain matrix is a \dchain. We give an example with
 rows in Figure~\ref{fig:valmat}.

\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \node[anchor=mid] (s1) at (0.0,0) {};
      \node[anchor=mid] (s2) at (1.0,0) {};
      \node[anchor=mid] (s3) at (2.0,0) {};
      \node[anchor=mid] (s4) at (3.0,0) {};
      \node[anchor=mid] (s5) at (4.0,0) {};
      \node[anchor=mid] (sn) at (5.0,0) {};

      \node[anchor=mid] (t1) at (0.0,-0.5) {};
      \node[anchor=mid] (t2) at (1.0,-0.5) {};
      \node[anchor=mid] (t3) at (2.0,-0.5) {};
      \node[anchor=mid] (t4) at (3.0,-0.5) {};
      \node[anchor=mid] (t5) at (4.0,-0.5) {};
      \node[anchor=mid] (tn) at (5.0,-0.5) {};

      \node[anchor=mid] (r1) at (0.0,-1.0) {};
      \node[anchor=mid] (r2) at (1.0,-1.0) {};
      \node[anchor=mid] (r3) at (2.0,-1.0) {};
      \node[anchor=mid] (r4) at (3.0,-1.0) {};
      \node[anchor=mid] (r5) at (4.0,-1.0) {};
      \node[anchor=mid] (rn) at (5.0,-1.0) {};

      \draw (-0.5,0.25) to (-0.5,-1.25);
      \draw (0.5,0.25) to (0.5,-1.25);
      \draw (1.5,0.25) to (1.5,-1.25);
      \draw (2.5,0.25) to (2.5,-1.25);
      \draw (3.5,0.25) to (3.5,-1.25);
      \draw (4.5,0.25) to (4.5,-1.25);
      \draw (5.5,0.25) to (5.5,-1.25);

      \draw (-0.5,0.25) to (5.5,0.25);
      \draw (-0.5,-0.25) to (5.5,-0.25);
      \draw (-0.5,-0.75) to (5.5,-0.75);
      \draw (-0.5,-1.25) to (5.5,-1.25);






      \node[anchor=mid] (p1) at (-0.55,-2.2) {};
      \node[anchor=mid] (v1) at (0.0,-2.2) {};
      \node[anchor=mid] (v2) at (1.0,-2.2) {};
      \node[anchor=mid] (v3) at (2.0,-2.2) {};
      \node[anchor=mid] (v4) at (3.0,-2.2) {};
      \node[anchor=mid] (v5) at (4.0,-2.2) {};
      \node[anchor=mid] (vn) at (5.0,-2.2) {};
      \node[anchor=mid] (p2) at (5.8,-2.2) {};
      \node[anchor=mid] (text) at (-2.0,-2.2) {Value};
      \draw[ar] (text) to (p1);

      \draw[->] () to (v1);
      \draw[->] () to (v2);
      \draw[->] () to (v3);
      \draw[->] () to (v4);
      \draw[->] () to (vn);
    \end{tikzpicture}
  \end{center}
  \caption{Value of \chain matrix with  rows}
  \label{fig:valmat}
\end{figure}

Given a \chain matrix, \mat, the \emph{alternation} of \mat is the
alternation of its value. Finally, the \emph{local alternation} of a
\chain matrix, \mat, is the largest natural  such that \mat has a
row with alternation . We now prove the two following propositions.

\begin{proposition} \label{prop:matinit}
  Assume that there exists a set of locally optimal \chain trees  with unbounded alternation and recursive
  alternation bounded by . Then there exist \chain
  matrices with arbitrarily large alternation and local alternation
  bounded by .   
\end{proposition}

\begin{proposition} \label{prop:contradend}
  Assume that there exist \chain matrices with arbitrarily large alternation
  and local alternation bounded by . Then  does not
  satisfy~\eqref{eq:bcs2}. 
\end{proposition}

Proposition~\ref{prop:width} is an immediate consequence of
Proposition~\ref{prop:matinit} and~\ref{prop:contradend}. Note that
\chain matrices are reused from~\cite{bpopen} (where they are called
''strategy matrices''). Moreover, in this case going from \chain trees
to \chains matrices (i.e. proving Proposition~\ref{prop:matinit}) is
simple and the main difficulty is proving
Proposition~\ref{prop:contradend}. This means that while our presentation
is slightly different from that of~\cite{bpopen}, the arguments
themselves are essentially the same. We give a full proof for
the sake of completeness. We begin by proving Proposition~\ref{prop:matinit}.

\begin{proof}[of Proposition~\ref{prop:matinit}]
  We prove that for all , there exists a \chain matrix \mat
  of alternation  and local alternation bounded by . By definition
  of  there exists a tree  whose value has alternation
   and has recursive alternation bounded by . Set 
  as leaves of  listed from left to right. 
  By Fact~\ref{fct:value}, . Observe that by definition, for all ,  has
  alternation bounded by . Therefore it suffices to set \mat as the
   rows matrix where row  is filled with .\qed
\end{proof}


It now remains to prove Proposition~\ref{prop:contradend}. We proceed
as follows. Assuming there exists a \chain matrix \mat with local
alternation bounded by  and very large alternation, we refine \mat
in several steps to ultimately obtain what we call a
\emph{contradiction matrix}. There are two types of contradiction
matrices, \emph{increasing} and \emph{decreasing}, both are
\chain matrices of length  and with the following entries:

\begin{center}
  \begin{tikzpicture}

    \node (m1) at (0,0) {};
    \node (l1) at(0,-1.2) {Increasing Contradiction Matrix};


    \node (m2) at (6,0) {};
    \node (l2) at(6,-1.2) {Decreasing Contradiction Matrix};
  \end{tikzpicture}
\end{center}

\noindent
such that  are idempotents and . As the
name suggests, the existence of a contradiction matrix contradicts
Equation~\eqref{eq:bcs1}. This is what we state in the following
lemma.

\begin{lemma} \label{lem:contradmat}
  If there exists a contradiction matrix,  does not
  satisfy~\eqref{eq:bcs1}.
\end{lemma}

\begin{proof}
  Assume that we have an increasing contradiction matrix (the other case
  is treated in a symmetrical way). Since , either
   or . By symmetry assume it is the
  former. Since  are idempotents, this means that . However by definition of \chain
  matrices  and therefore   which contradicts Equation~\eqref{eq:bcs1}. Note 
  that we only used one half of Equation~\eqref{eq:bcs1}, the other half 
  is used in the decreasing case.\qed
\end{proof}

By Lemma~\ref{lem:contradmat}, it suffices to prove the existence of a
contradiction matrix to conclude the proof of
Proposition~\ref{prop:contradend}. This is what we do in the remainder
of this Appendix. By hypothesis, we know that there exist \chain
matrices with arbitrarily large alternation and local alternation
bounded by . For the remainder of the section, we assume
that this hypothesis holds. We use several steps to prove that we can
choose our \chain matrices with increasingly strong properties until
we get a contradiction matrix. We use two intermediaries that we 
call \emph{Tame \Chain Matrices} and \emph{Monotonous \Chain
  Matrices}. We divide the proof in three subsections, one for each
step.

\subsection{Tame \Chain Matrices}

Let \mat be a \chain matrix of \emph{even length}  and let . The \emph{set of alternating rows for }, denoted by
\alt{\mat}{j}, is the set . Let 
 be the value of \mat. We say that \mat is \emph{tame} if
\begin{itemize}
\item [] for all , ,
\item [] for all ,
  \alt{\mat}{j} is a singleton and
\item [] if  then .
\end{itemize}
We represent a tame \chain matrix
of length  in Figure~\ref{fig:tamemat}. Observe that the definition
only considers the relationship between odd columns and the next even
column. Moreover, observe that a tame \chain matrix of length 
has by definition alternation at least .


\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \draw[pattern=north west lines,draw] (0.0,1.5) to (0.5,1.5) to
      (0.5,2.0) to (0.0,2.0) to (0.0,1.5); 
      \draw[pattern=crosshatch dots,draw] (0.0,1.0) to (0.5,1.0) to
      (0.5,1.5) to (0.0,1.5) to (0.0,1.0); 
      \draw[pattern=north west lines,draw] (0.0,0.5) to (0.5,0.5) to
      (0.5,1.0) to (0.0,1.0) to (0.0,0.5); 
      \draw[pattern=crosshatch dots,draw] (0.0,0.0) to (0.5,0.0) to
      (0.5,0.5) to (0.0,0.5) to (0.0,0.0); 

      \draw[pattern=north west lines,draw] (0.5,1.5) to (1.0,1.5) to
      (1.0,2.0) to (0.5,2.0) to (0.5,1.5); 
      \draw[pattern=crosshatch dots,draw] (0.5,1.0) to (1.0,1.0) to
      (1.0,1.5) to (0.5,1.5) to (0.5,1.0); 
      \draw[pattern=north west lines,draw] (0.5,0.5) to (1.0,0.5) to
      (1.0,1.0) to (0.5,1.0) to (0.5,0.5); 
      \draw[pattern=vertical lines,draw] (0.5,0.0) to (1.0,0.0) to (1.0,0.5)
      to (0.5,0.5) to (0.5,0.0);

      \draw[pattern=horizontal lines,draw] (1.0,1.5) to (1.5,1.5) to
      (1.5,2.0) to (1.0,2.0) to (1.0,1.5); 
      \draw[pattern=bricks,draw] (1.0,1.0) to (1.5,1.0) to
      (1.5,1.5) to (1.0,1.5) to (1.0,1.0); 
      \draw[pattern=north east lines,draw] (1.0,0.5) to (1.5,0.5) to
      (1.5,1.0) to (1.0,1.0) to (1.0,0.5); 
      \draw[pattern=north east lines,draw] (1.0,0.0) to (1.5,0.0) to
      (1.5,0.5) to (1.0,0.5) to (1.0,0.0); 

      \draw[pattern=crosshatch dots,draw] (1.5,1.5) to (2.0,1.5) to
      (2.0,2.0) to (1.5,2.0) to (1.5,1.5); 
      \draw[pattern=bricks,draw] (1.5,1.0) to (2.0,1.0) to
      (2.0,1.5) to (1.5,1.5) to (1.5,1.0); 
      \draw[pattern=north east lines,draw] (1.5,0.5) to (2.0,0.5) to
      (2.0,1.0) to (1.5,1.0) to (1.5,0.5); 
      \draw[pattern=north east lines,draw] (1.5,0.0) to (2.0,0.0) to
      (2.0,0.5) to (1.5,0.5) to (1.5,0.0);

      \draw[pattern=bricks,draw] (2.0,1.5) to (2.5,1.5) to
      (2.5,2.0) to (2.0,2.0) to (2.0,1.5); 
      \draw[pattern=bricks,draw] (2.0,1.0) to (2.5,1.0) to
      (2.5,1.5) to (2.0,1.5) to (2.0,1.0); 
      \draw[pattern=crosshatch dots,draw] (2.0,0.5) to (2.5,0.5) to
      (2.5,1.0) to (2.0,1.0) to (2.0,0.5); 
      \draw[pattern=crosshatch dots,draw] (2.0,0.0) to (2.5,0.0) to
      (2.5,0.5) to (2.0,0.5) to (2.0,0.0); 

      \draw[pattern=bricks,draw] (2.5,1.5) to (3.0,1.5) to
      (3.0,2.0) to (2.5,2.0) to (2.5,1.5); 
      \draw[pattern=north east lines,draw] (2.5,1.0) to (3.0,1.0) to
      (3.0,1.5) to (2.5,1.5) to (2.5,1.0); 
      \draw[pattern=crosshatch dots,draw] (2.5,0.5) to (3.0,0.5) to
      (3.0,1.0) to (2.5,1.0) to (2.5,0.5); 
      \draw[pattern=crosshatch dots,draw] (2.5,0.0) to (3.0,0.0) to
      (3.0,0.5) to (2.5,0.5) to (2.5,0.0); 

      \node[anchor=mid,inner sep=1pt] (s1) at (0.25,-0.7) {};
      \node[anchor=mid,inner sep=1pt] (s2) at (0.75,-0.7) {};
      \node[anchor=mid,inner sep=1pt] (s3) at (1.25,-0.7) {};
      \node[anchor=mid,inner sep=1pt] (s4) at (1.75,-0.7) {};
      \node[anchor=mid,inner sep=1pt] (s5) at (2.25,-0.7) {};
      \node[anchor=mid,inner sep=1pt] (s6) at (2.75,-0.7) {};


      \node[anchor=mid east] (text) at (-1.0,-0.7) {Value};
      \draw[ar] (text) to (s1);

      \draw (s1.south) to [in=-90,out=-90]
      node[sloped,draw,fill=white,circle,inner sep=0.5pt] {\tiny }
      (s2.south);
      \draw (s3.south) to [in=-90,out=-90]
      node[sloped,draw,fill=white,circle,inner sep=0.5pt] {\tiny }
      (s4.south);
      \draw (s5.south) to [in=-90,out=-90]
      node[sloped,draw,fill=white,circle,inner sep=0.5pt] {\tiny }
      (s6.south);

    \end{tikzpicture}
  \end{center}
  \caption{A tame \chain matrix of length }
  \label{fig:tamemat}
\end{figure}

\begin{lemma} \label{lem:tame}
  There exists tame \chain matrices of arbitrarily large length.
\end{lemma}

\begin{proof}
  Set , we explain how to construct a tame \chain matrix of
  length . By hypothesis, there exists a \chain matrix \mat with
  local alternation at most~ and alternation greater than . Set
   as the number of rows of \mat. We explain how to modify \mat to
  obtain a matrix satisfying ,  and . Recall that \dchains
  are closed under subwords, therefore removing columns from \mat yields
  a \chain matrix. Since \mat has alternation , it is simple to see
  that by removing columns one can obtain a \chain matrix of length
   that satisfies . We denote by \mnat this matrix. We now
  proceed in two steps: first, we modify the entries in \mnat to get a
  matrix \pat of length  satisfying both  and . Then we use
  our bound on local alternation to remove columns and enforce  in
  the resulting matrix.

  \medskip
  \noindent
  {\bf Construction of \pat.} Let  such that \alt{\mnat}{j} 
  is of size at least . We modify the matrix to reduce
  the size of \alt{\mnat}{j} while preserving . One can then repeat
  the operation to get the desired matrix. Let .
  Set  and . We distinguish two~cases.

  First, if , then for all ,
  we replace entry  with entry . One can verify
  that this yields a \chain matrix of length , local alternation bounded by
  . Moreover, it still satisfies , since . Finally, \alt{\mnat}{j} is now a singleton, namely .

  In the second case, we have . In that
  case, we replace  with . One can
  verify that this yields a \chain matrix of length , local
  alternation bounded by . Moreover, it still satisfies  since we did not
  change the value on the whole. Finally,
  the size of \alt{\mnat}{j} has decreased by~.

  \medskip
  \noindent
  {\bf Construction of the tame matrix.} We now have a \chain matrix
  \pat of length , with local alternation bounded by  and
  satisfying both  and . Since  and  are satisfied, for
  all  there exists exactly one row  such that
  . Moreover, since each row has
  alternation at most , a single row  has this property for at
  most  indices . Therefore, it suffices to remove at most
   pairs of odd-even columns to get a matrix that satisfies
  . Since the original matrix had length , this leaves a matrix
  of length at least  and we are finished. \qed 
\end{proof}



\subsection{Monotonous \Chain Matrices}

Let \mat be a tame \chain matrix of length  and let 
be naturals such that for all , . We say
that \mat is a \emph{monotonous \chain matrix} if it has exactly 
rows and  (in which case the
matrix is said \emph{increasing}) or 
(in which case we say the matrix is \emph{decreasing}). We give a 
representation of the increasing case in Figure~\ref{fig:inctame}.

\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \draw[pattern=north west lines,draw] (0.0,1.5) to (0.5,1.5) to
      (0.5,2.0) to (0.0,2.0) to (0.0,1.5); 
      \draw[pattern=bricks,draw] (0.0,1.0) to (0.5,1.0) to
      (0.5,1.5) to (0.0,1.5) to (0.0,1.0); 
      \draw[pattern=north west lines,draw] (0.0,0.5) to (0.5,0.5) to
      (0.5,1.0) to (0.0,1.0) to (0.0,0.5); 
      \draw[pattern=crosshatch dots,draw] (0.0,0.0) to (0.5,0.0) to
      (0.5,0.5) to (0.0,0.5) to (0.0,0.0); 

      \draw[pattern=bricks,draw] (0.5,1.5) to (1.0,1.5) to
      (1.0,2.0) to (0.5,2.0) to (0.5,1.5); 
      \draw[pattern=bricks,draw] (0.5,1.0) to (1.0,1.0) to
      (1.0,1.5) to (0.5,1.5) to (0.5,1.0); 
      \draw[pattern=north west lines,draw] (0.5,0.5) to (1.0,0.5) to
      (1.0,1.0) to (0.5,1.0) to (0.5,0.5); 
      \draw[pattern=crosshatch dots,draw] (0.5,0.0) to (1.0,0.0) to (1.0,0.5)
      to (0.5,0.5) to (0.5,0.0);

      \draw[pattern=north east lines,draw] (1.0,1.5) to (1.5,1.5) to
      (1.5,2.0) to (1.0,2.0) to (1.0,1.5); 
      \draw[pattern=crosshatch dots,draw] (1.0,1.0) to (1.5,1.0) to
      (1.5,1.5) to (1.0,1.5) to (1.0,1.0); 
      \draw[pattern=north east lines,draw] (1.0,0.5) to (1.5,0.5) to
      (1.5,1.0) to (1.0,1.0) to (1.0,0.5); 
      \draw[pattern=north east lines,draw] (1.0,0.0) to (1.5,0.0) to
      (1.5,0.5) to (1.0,0.5) to (1.0,0.0); 

      \draw[pattern=north east lines,draw] (1.5,1.5) to (2.0,1.5) to
      (2.0,2.0) to (1.5,2.0) to (1.5,1.5); 
      \draw[pattern=north west lines,draw] (1.5,1.0) to (2.0,1.0) to
      (2.0,1.5) to (1.5,1.5) to (1.5,1.0); 
      \draw[pattern=north east lines,draw] (1.5,0.5) to (2.0,0.5) to
      (2.0,1.0) to (1.5,1.0) to (1.5,0.5); 
      \draw[pattern=north east lines,draw] (1.5,0.0) to (2.0,0.0) to
      (2.0,0.5) to (1.5,0.5) to (1.5,0.0);

      \draw[pattern=bricks,draw] (2.0,1.5) to (2.5,1.5) to
      (2.5,2.0) to (2.0,2.0) to (2.0,1.5); 
      \draw[pattern=horizontal lines,draw] (2.0,1.0) to (2.5,1.0) to
      (2.5,1.5) to (2.0,1.5) to (2.0,1.0); 
      \draw[pattern=crosshatch dots,draw] (2.0,0.5) to (2.5,0.5) to
      (2.5,1.0) to (2.0,1.0) to (2.0,0.5); 
      \draw[pattern=crosshatch dots,draw] (2.0,0.0) to (2.5,0.0) to
      (2.5,0.5) to (2.0,0.5) to (2.0,0.0); 

      \draw[pattern=bricks,draw] (2.5,1.5) to (3.0,1.5) to
      (3.0,2.0) to (2.5,2.0) to (2.5,1.5); 
      \draw[pattern=horizontal lines,draw] (2.5,1.0) to (3.0,1.0) to
      (3.0,1.5) to (2.5,1.5) to (2.5,1.0); 
      \draw[pattern=bricks,draw] (2.5,0.5) to (3.0,0.5) to
      (3.0,1.0) to (2.5,1.0) to (2.5,0.5); 
      \draw[pattern=crosshatch dots,draw] (2.5,0.0) to (3.0,0.0) to
      (3.0,0.5) to (2.5,0.5) to (2.5,0.0); 

      \draw[pattern=crosshatch dots,draw] (3.0,1.5) to (3.5,1.5) to
      (3.5,2.0) to (3.0,2.0) to (3.0,1.5); 
      \draw[pattern=north east lines,draw] (3.0,1.0) to (3.5,1.0) to
      (3.5,1.5) to (3.0,1.5) to (3.0,1.0); 
      \draw[pattern=grid,draw] (3.0,0.5) to (3.5,0.5) to
      (3.5,1.0) to (3.0,1.0) to (3.0,0.5); 
      \draw[pattern=north east lines,draw] (3.0,0.0) to (3.5,0.0) to
      (3.5,0.5) to (3.0,0.5) to (3.0,0.0); 

      \draw[pattern=crosshatch dots,draw] (3.5,1.5) to (4.0,1.5) to
      (4.0,2.0) to (3.5,2.0) to (3.5,1.5); 
      \draw[pattern=north east lines,draw] (3.5,1.0) to (4.0,1.0) to
      (4.0,1.5) to (3.5,1.5) to (3.5,1.0); 
      \draw[pattern=grid,draw] (3.5,0.5) to (4.0,0.5) to
      (4.0,1.0) to (3.5,1.0) to (3.5,0.5); 
      \draw[pattern=grid,draw] (3.5,0.0) to (4.0,0.0) to
      (4.0,0.5) to (3.5,0.5) to (3.5,0.0); 

      \draw[very thick,red] (0.0,2.0) to (1.0,2.0) to
      (1.0,1.5) to (0.0,1.5) to (0.0,2.0);

      \draw[very thick,red] (1.0,1.5) to (2.0,1.5) to
      (2.0,1.0) to (1.0,1.0) to (1.0,1.5);

      \draw[very thick,red] (2.0,1.0) to (3.0,1.0) to
      (3.0,0.5) to (2.0,0.5) to (2.0,1.0);

      \draw[very thick,red] (3.0,0.5) to (4.0,0.5) to
      (4.0,0.0) to (3.0,0.0) to (3.0,0.5);


      \node[anchor=mid,inner sep=1pt] (s1) at (0.25,-0.7) {};
      \node[anchor=mid,inner sep=1pt] (s2) at (0.75,-0.7) {};
      \node[anchor=mid,inner sep=1pt] (s3) at (1.25,-0.7) {};
      \node[anchor=mid,inner sep=1pt] (s4) at (1.75,-0.7) {};
      \node[anchor=mid,inner sep=1pt] (s5) at (2.25,-0.7) {};
      \node[anchor=mid,inner sep=1pt] (s6) at (2.75,-0.7) {};
      \node[anchor=mid,inner sep=1pt] (s7) at (3.25,-0.7) {};
      \node[anchor=mid,inner sep=1pt] (s8) at (3.75,-0.7) {};

      \node[anchor=mid east] (text) at (-1.0,-0.7) {Value};
      \draw[ar] (text) to (s1);

      \draw (s1.south) to [in=-90,out=-90]
      node[sloped,draw,fill=white,circle,inner sep=0.5pt] {\tiny }
      (s2.south);
      \draw (s3.south) to [in=-90,out=-90]
      node[sloped,draw,fill=white,circle,inner sep=0.5pt] {\tiny }
      (s4.south);
      \draw (s5.south) to [in=-90,out=-90]
      node[sloped,draw,fill=white,circle,inner sep=0.5pt] {\tiny }
      (s6.south);
      \draw (s7.south) to [in=-90,out=-90]
      node[sloped,draw,fill=white,circle,inner sep=0.5pt] {\tiny }
      (s8.south);
    \end{tikzpicture}
  \end{center}
  \caption{A monotonous \chain matrix (increasing)}
  \label{fig:inctame}
\end{figure}

\begin{lemma} \label{lem:mono}
  There exists monotonous \chain matrices of arbitrarily large length.
\end{lemma}

\begin{proof}
  Set , we explain how to construct a tame \chain matrix of
  length . By Lemma~\ref{lem:tame}, there exists a tame \chain matrix
  \mat of length . Set  the indices such that
  for all , . Note that by tameness,  for . Since the sequence 
  is of length , we can extract, using Erd\"os-Szekeres theorem, a
  monotonous sequence of length ,  or
   with .  By symmetry
  we assume it is the former and construct an increasing \chain matrix
  of length .

  Let \pat be the matrix of length  obtained from \mat, by keeping
  only the pairs of columns  for . Set
   the indices such that for all , . By definition, . We now want
   to have exactly  rows. Note that the rows  that do not belong to
   are constant chains. We simply merge these rows
  with others. For example, if row  is labeled with the 
  constant \chain , let  be the label of
  row . We remove row  and replace row  by the \dchain
  . Repeating the operation yields the desired increasing
  monotonous \chain~matrix. \qed
\end{proof}


\subsection{Construction of the Contradiction Matrix}  

We can now use Lemma~\ref{lem:mono} to construct a contradiction
matrix and end the proof of Proposition~\ref{prop:width}. We state
this in the following proposition.

\begin{proposition} \label{prop:contmat}
  There exists a contradiction matrix.
\end{proposition}

The remainder of this appendix is devoted to the proof of
Proposition~\ref{prop:contmat}. The result follows from a Ramsey
argument. We use Lemma~\ref{lem:mono} to choose a monotonous matrix of
sufficiently large length. Then, we use Ramsey's Theorem (for
hypergraphs with edges of size ) to extract the desired
contradiction matrix.

We first define the length of the monotonous \chain matrix that we need to
pick. By Ramsey's Theorem, for every  there exists a number
 such that for any complete 3-hypergraph with hyperedges colored
over the monoid~, there exists a complete sub-hypergraph of size  in
which all edges share the same color. We choose . By Lemma~\ref{lem:mono}, there exists a monotonous
\chain matrix \mat of length . Since it is monotonous, \mat has  rows.

By symmetry, we assume that \mat is increasing and use it to construct an
increasing contradiction matrix.  We use our choice of  to extract a
contradiction matrix from \mat. We proceed in two steps using Ramsey's Theorem
each time. In the first step we treat all entries above the diagonal in \mat
and in the second step all entries below the diagonal. We state the first step
in the next lemma.

\begin{lemma} \label{lem:matlemma}
  There exists an increasing monotonous matrix \mnat of length  such that all cells above the diagonal contain the same
  idempotent .
\end{lemma}

\begin{proof}
  This is proved by applying Ramsey's Theorem to \mat. Consider the
  complete 3-hypergraph whose nodes are . We label the
  hyperedge  where   by the value obtained by
  multiplying in the monoid , the cells that appear in rows
   in column . Observe that since , by monotonicity, these entries are the
  same as in column . More formally, the label of the hyperedge  is therefore
  
  By choice of , we can apply Ramsey's Theorem to this coloring. We get a
  subset of  vertices, say , such that all hyperedges connecting nodes in  have the same
  color, say . For  in~, note that the color of the hyperedge
   is by definition the product of the colors of the hyperedges 
   and . Therefore, the common color  needs to be an idempotent
  (i.e. ). We now extract the desired matrix \mnat from \mat according
  to the subset . The main idea is that the new row  in \mnat will be the
  merging of rows  to  in \mat and the new pair of columns
   will correspond to the pair  in \mat.

  We first merge rows. For all , we ''merge'' all rows from
   to  into a single row. More precisely, this means
  that we replace the rows  to  by a single row
  containing the \dchain 
  

  Moreover, we remove the top and bottom rows, i.e. row  to  and
  rows  to . Then we remove all columns from  to
  , all columns from  to , and for all , all columns from  to . One can verify
  that these two operations applied together preserve
  monotonicity. Observe that the resulting matrix \mnat has exactly
   columns. Moreover, the cell  in the new
  matrix contains entry . In particular if , by definition of
  the set , this entry is , which means \mnat satisfies the
  conditions of the lemma.\qed
\end{proof}

It remains to apply Ramsey's Theorem a second time to the matrix \mnat
obtained from Lemma~\ref{lem:matlemma} to treat the cells below the
diagonal and get the contradiction matrix. We state this in the
following last lemma.


\begin{lemma}
  There exists an increasing monotonous matrix \pat of length  such
  that all cells above the diagonal contain the same idempotent  and all cells below the diagonal contain the same idempotent  (i.e. \pat is an increasing contradiction matrix). 
\end{lemma}

\begin{proof}
  The argument is identical to the one of Lemma~\ref{lem:matlemma}. This
  time we apply it to the matrix \mnat of length  for the
  cells below the diagonal.\qed
\end{proof}


\end{document}
