\begin{abstract}
    Polynomial filters, a kind of Graph Neural Networks, typically use a predetermined polynomial basis and learn the coefficients from the training data. It has been observed that the effectiveness of the model is highly dependent on the property of the polynomial basis. Consequently, two natural and fundamental questions arise: Can we learn a suitable polynomial basis from the training data? Can we determine the optimal polynomial basis for a given graph and node features?

    In this paper, we propose two spectral GNN models that provide positive answers to the questions posed above. First, inspired by Favard's Theorem, we propose the FavardGNN model, which learns a polynomial basis from the space of all possible orthonormal bases. Second, we examine the supposedly unsolvable definition of optimal polynomial basis from \citet{Wang2022jacobi} and propose a simple model, OptBasisGNN, which computes the optimal basis for a given graph structure and graph signal. Extensive experiments are conducted to demonstrate the effectiveness of our proposed models.
    Our code is available at  
    \href{https://github.com/yuziGuo/FarOptBasis}{https://github.com/yuziGuo/FarOptBasis}.
\end{abstract} \section{Introduction}


\torevise{Spectral Graph Neural Networks are a type of 
Graph Neural Networks that apply filtering operations on graph Laplacian spectrums. }
\revised{
Spectral Graph Neural Networks are a type of Graph Neural Networks 
that comprise the majority of filter-based GNNs~\cite{Shuman2013,isufi2021edgenets,isufi2022graph}.}
They are designed to create graph signal filters in the spectral domain.
To avoid eigendecomposition, spectral GNNs approximate the desired filtering operations by polynomials of laplacian eigenvalues. 


As categorized in \citet{he2022chebii}, 
there are mainly two kinds of spectral GNNs. 
In some works, the desired polynomial filters 
are \textbf{predefined}.  
For example, 
GCN~\cite{kipf2016semi} fixes 
the filter 
to be , 
and APPNP~\cite{klicpera2019appnp} restricts
the filtering function 
within the Personalized Pagerank.



Another line of research approximates \textbf{arbitrary} filters with learnable polynomials. These models typically fix a predetermined polynomial basis and learn the coefficients from the training data. 
ChebNet~\cite{Defferrard2016cheb} uses Chebyshev basis following the tradition of Graph Signal Processing~\cite{Hammond2009}. 
GPR-GNN~\cite{chien2021gprgnn} uses Monomial basis, which is straightforward.
BernNet~\cite{He2021bern} uses the non-negative Bernstein basis for regularization and interpretation.
JacobiConv~\cite{Wang2022jacobi} chooses among the family of Jacobi polynomial bases, 
with the exact basis determined by two extra hyperparameters. 
ChebNetII~\cite{he2022chebii} revisits the Chebyshev basis, 
and incorporates the power of Chebyshev interpolation
 by reparameterizing learnable coefficients by chebynodes.   
Please refer to Section \ref{sec:background_spectral} for more concrete backgrounds about polynomial filtering.   

However, there are still two fundamental challenges on the choice of basis. 

\textbf{Challenge 1: }
\revised{
It is well known and checked by ablation studies~\cite{Wang2022jacobi} that the choice of basis has a significant impact on practical performance.
}
However, the proportion of known polynomial bases is small and may not include the best-fitting basis for a given graph and signal. Therefore, we pose the following question: \textbf{Can we learn a polynomial basis from the training data out of all possible orthonormal polynomials?}
\footnote{For the concrete definition of orthonormal polynomial bases, 
please check the preliminaries in Section \ref{para:inner_product}.}






\begin{figure*}
    \centering
    \includegraphics[width=2\columnwidth]{images/example.pdf}
    \caption{Representation of  by 
       different bases.}
    \label{fig:example}
 \end{figure*}

\textbf{Challenge 2: }
On the other hand, although these bases differ in empirical performances, their expressiveness should be the same:
any target polynomial of order  can be represented by 
any complete polynomial basis with truncated order  (See Figure \ref{fig:example} for an example). 
Therefore, \citet{Wang2022jacobi} raised a definition of \textit{optimal basis} from an optimization perspective,   
which promises an optimal convergence rate. 
However, this basis is believed to be unsolvable using existing techniques. Consequently, a natural question is: \textbf{can we compute this optimal basis for a given graph and signal using innovative techniques?}





 


In this paper, we provide positive answers to the questions
posed above. We summarize our contributions in three folds. 
Firstly, we propose FavardGNN with  \textbf{learnable orthonormal basis} to tackle the first challenge. The theoretical basis of 
FavardGNN is two Theorems in orthogonal polynomials: the Three-term recurrences and its converse, Favard's Theorem. FavardGNN learns from the \textit{whole space} of possible orthonormal basis with  extra parameters.
Secondly, we propose OptBasisGNN with \textbf{solvable optimal basis}. 
We solve the optimal basis raised by \citet{Wang2022jacobi} by avoiding the explicit solving of the weight function 
\revised{, which invites the need for eigendecomposition}. 
Note that although we write out the implicitly defined/solved polynomial series in the methodology section, 
we never need to solve it explicitly.   
Last but not least, we conduct \textbf{extensive experiments}
to 
demonstrate the effectiveness of our proposed models.

 \section{Background and Preliminaries}
\subsection{Background of Spectral GNNs}
\label{sec:background_spectral}
In this section, 
we provide some necessary backgrounds of spectral graph neural networks, 
and show how the choice of polynomial bases emerges as a problem. 
Notations used are summarized in 
Table \ref{tbl:notations} in 
Appendix \ref{sec:notations}.

\textbf{Graph Fourier Transform.\quad}
Consider an undirected and connected graph  with  nodes, 
its symmetric normalized adjacency matrix 
and laplacian matrix are denoted as
 and , respectively, .
\textit{Graph Fourier Transform}, as defined in the spatial/spectral domain of graph signal processing, is analogous to the time/frequency domain Fourier Transform
\cite{Hammond2009, Shuman2013}
. 
One column in the representations of  nodes, 
, 
is considered a \textit{graph signal}, 
denoted as .
The complete set of  eigenvectors of , denoted as ,
who show varying structural frequency characteristics~\cite{Shuman2013},
are used as \textit{frequency components}.
\textit{Graph Fourier Transform} 
is defined as ,
where signal  is projected to the frequency responses of all components.
It is then followed by 
\textit{modulation},
which suppresses or strengthens certain frequency components, 
denoted as  
.
After modulation, \textit{inverse Fourier Transform}:
 transforms 
back to the spatial domain.
The three operations form the process of \textit{spectral filtering}:
.




  


 








\textbf{Polynomial Approximated Filtering.\quad}
In order to avoid time-consuming eigendecomposition,
a line of work approximate  
 by 
some polynomial function of , 
which is the -th eigenvalue of , 
i.e. 
. 
Equation~\eqref{eq-IFT} then becomes a form that is easy for fast
{\textit{localized}} calculation:

As listed in Introduction, 
various \textit{polynomial bases} have been utilized, 
denoted as . 
For further simplicity, 
we equivalently use  instead of  in this paper, where 
. 
Note that  is defined on the spectrum of , 
and the -th eigenvalue of , denoted as , equals .


The filtering process on the input signal  is then expressed as 
.
When consider 
independent filtering on each of the  channels in  simultaneously, the \textbf{multichannel filtering} can be denoted as:   
.






%
 \subsection{Orthogonal and Orthonormal  Polynomials}

In this section, we give a formal definition of orthogonal and orthonormal polynomials, which plays a central role in the choosing of polynomial bases~\cite{simon2014spectral}.



\textbf{Inner Products.}
\label{para:inner_product}
The inner product of polynomials 
is defined as 
,
where ,  and  are functions of  on interval , 
and the \textit{weight function}  should be non-negative 
to 
guarantee the 
positive-definiteness of inner-product space.



The definition of the inner products induces 
the definitions of \textit{norm} and \textit{orthogonality}.
The norm of polynomial  is defined as: 
,
and  and  are orthogonal to each other when 
. Notice that the concept of inner product, norm, and orthogonality
are all defined with respect to some weight function. 

\textbf{Orthogonal Polynomials.}
A sequence of polynomials  
where  is of exact degree , is called \textit{orthogonal} 
w.r.t. the positive weight function  if, 
for , there exists  
,
where the inner product  is defined w.r.t.
. When  for ,  
is known as \textbf{orthonormal} polynomial series.

When a weight function is given, the orthogonal or orthonormal series 
with respect to the weight function 
can be solved by \textit{Gram-Schmidt process}.

\begin{remark}
\label{remark:increasing_order}
In this paper, the orthogonal/orthonormal polynomial bases we consider 
are truncated polynomial series, i.e. the polynomials that form a basis are of 
increasing order. 
\end{remark}





%
 























 \section{Learnable Basis via Favard's Theorem}
\label{sec:methodI}






Empirically,
spectral GNNs with different polynomial bases vary in performance on different datasets, which leads to two observations: (1) the choice of bases matters; (2) whether a basis is preferred might be related to the input, i.e. 
different signals on their accompanying underlying graphs.

For the first observation, we notice that up to now, polynomial filters \textit{pick} polynomial bases 
from well-studied polynomials, e.g. Chebyshev polynomials, Bernstein polynomials, \textit{etc}, 
which narrows down the range of choice.  
For the second observation, we question the reasonableness of fixing a basis during training. 
A related effort is made by JacobiConv~\cite{wang2019improving}, 
who adapt to a Jacobi polynomial series from the family of Jacobi polynomials via \textit{hyperparameter tuning}. However, 
the range they choose from is discrete. 
Therefore, we aim at dynamically \textbf{learn} polynomial basis from the input from a \textbf{vast range}. 














\subsection{Recurrence Formula for Orthonormal Bases}
\begin{algorithm}[tb]
    \SetAlgoNoLine
    \KwIn{Input signals  with  channels; 
            Normalized graph adjacency ; 
            Truncated polynomial order 
            }
    \Parameter{, , 
          }
    \KwOut{Filtered Signals } 
    \BlankLine
\\
    \For{ \KwTo }{ 
        
        ,\ 
        ,\  
        \\ \For { \KwTo }{
            
            \\ 
        }
        
    }
    \KwRet{Z}
    \caption{\textsc{FavardFiltering}}
    \label{alg:favard}
\end{algorithm} \begin{algorithm}[t]
\KwIn{Raw features ; 
            Normalized graph adjacency ; 
            Truncated polynomial order 
            }
    \Parameter{, , , ,
          , , 
          }
    \KwOut{Label predictions } 
    \BlankLine\Indp
    
    \\
    {\textsc{FavardFiltering}}(,
      , , , , )
    \\
    Softmax()
    \caption{\textsc{FavardGNN} (For Classification)}
    \label{alg:favardgnn_cls}
\end{algorithm} Luckily, the Three-term recurrences and Favard's theorem of orthonormal polynomials provide a 
\textit{continuous} parameter space to learn 
 basis.   Generally speaking, three-term recurrences states that every orthonormal polynomial series satisfies a very characteristic form of recurrence relation, and   
Favard's theorem states the converse.  
 










\begin{theorem}[Three Term Recurrences for Orthonormal Polynomials] 
\citep[p.~12]{gautschi2004orthogonal}
\label{thm:3term_orthonormal}
    For orthonormal polynomials  w.r.t. weight function , 
    suppose that the leading coefficients of all polynomials are positive, 
    there exists the three-term recurrence relation:
    
with .

 \end{theorem}
 




\begin{theorem}[Favard's Theorem; Orthonormal Case]
\cite{favard1935polynomes}, \citep[p.~14]{simon54orthogonal}
    \label{thm:far-orthonormal}
    A polynomial series  who satisfies the recurrence relation in Equation~\eqref{eq:formula_orthonormal}
    is orthonormal w.r.t. a weight function  that  . 
\end{theorem}

 





By Theorem \ref{thm:far-orthonormal}, 
any possible recurrences with the form~\eqref{eq:formula_orthonormal} 
defines an orthonormal basis. 
By Theorem~\ref{thm:3term_orthonormal}, 
such a formula covers the whole space of orthonormal polynomials.
If we set  and  to be learnable parameters with , any orthonormal basis can be obtained.

We put the more general \textit{orthogonal} form of Theorem~\ref{thm:3term_orthonormal} and Theorem~\ref{thm:far-orthonormal} 
in Appendix~\ref{sec:proof-of-3term} to \ref{sec:proof-of-favard-orthonormal}.  
In fact, the property of three-term recurrences for orthogonal polynomials has been used multiple times 
in the context of current spectral GNNs
to reuse  and  
for the calculation of .  
\citet{Defferrard2016cheb} 
owe the fast filtering of ChebNet to employing the three-term recurrences of \textit{Chebyshev polynomials} (the first kind, 
which is orthogonal w.r.t.  ): 
. 
Similarly, 
JacobiConv~\cite{Wang2022jacobi} employs the 
three-term recurrences for \textit{Jacobi polynomials} (orthogonal w.r.t. to ). In this paper, however, we focus on orthonormal bases because they minimize  the mutual influence of basis polynomials and the influence of the unequal norms of different basis polynomials. 


\subsection{FavardGNN}




\textbf{Formulation of FavardGNN.\quad}We formally write the architecture of
\textsc{FavardGNN} (Algorithm \ref{alg:favardgnn_cls}), with the filtering process illustrated in \textsc{FavardFiltering} (Algorithm \ref{alg:favard}). Note that the iterative process of Algorithm \ref{alg:favard} (lines 3-5) follows exactly from Equation~\eqref{eq:formula_orthonormal} in Favard's Theorem. The key insight is to treat the coefficients  in Equation~\eqref{eq:formula_orthonormal} as learnable parameters. Since Theorem~\ref{thm:3term_orthonormal} and Theorem~\ref{thm:far-orthonormal}  state that the orthonormal basis must satisfy the employed iteration and vice versa, it follows that the model can learn a suitable orthonormal polynomial basis from among all possible orthonormal bases.

Following convention, before \textsc{FavardFiltering}, an MLP is used to map the raw features onto the signal
channels (often much less than the dimension of raw features). In regression
problems, the filtered signals are directly used as predictions; for
classification problems, they are combined by another MLP followed by a
softmax layer.


\textbf{Parallel Execution.\quad}Note that for convenience of presentation, we
write the \textsc{FavardFiltering} Algorithm in a form of nested loops. In fact, the
computation on different channels (the inner loop ) is conducted
simultaneously. We put more concrete implementation in PyTorch-styled
the pseudocode in Appendix \ref{sec:pseudo_torch_Favard}.







































\subsection{Weaknesses of FavardGNN}
\label{sec:weakness}




However, there are still two main weaknesses of FavardGNN. Firstly, the orthogonality lacks interpretability. 
The weight function  can only be solved analytically in a number of cases \cite{Geronimo1991weightfunction}.  
Even if the weight function is solved, the form of  might be too complicated to understand. 

Secondly, 
\textsc{FavardFiltering} is not good in  convergence properties:
consider a simplified optimization problem  which has been examined in the context of GNN 
\cite{keyulu2021Optm, Wang2022jacobi}, 
even this problem is non-convex w.r.t the learnable parameters in . 
We will re-examine this problem in the experiment section.  \section{Achieving Optimal Basis}
\label{sec:optbasis}

Although FavardGNN potentially reaches the whole space of orthonormal polynomial series, 
on the other hand, 
we still want to know: 
\textbf{whether there is an optimal and accessible basis} 
in this vast space. 


Recently, \citet{Wang2022jacobi} raises a criterion for 
optimal basis. Since different bases are the same in expressiveness,
this criterion is induced from an angle of optimization.
However, \citet{Wang2022jacobi} believe that this optimal basis 
is unreachable.
In this section, we follow this definition of optimal basis, 
and show how we can \textit{exactly} apply this optimal basis to our polynomial filter  
with  time complexity.



\citet{Wang2022jacobi} make an essential step towards this question: 
they derive and define an optimal basis from the angle of optimization. 
However, they do not exhaust their own finding in their model, 
since based on a habitual process, they believe that the optimal basis they find is inaccessible. 
In this section, we show how we can \textit{exactly} apply this optimal basis to our polynomial filter 
in  time complexity. 




\subsection{A Review: A Definition for Optimal Basis}
\label{sec:optdefinition}
We start this section with a quick review of the related part from \citet{Wang2022jacobi}, with a more complete review put in Appendix \ref{sec:SumWang}.

\textbf{Definition of Optimal Basis.\quad}
\label{sec:SumWang_short}
\citet{Wang2022jacobi} considers the squared loss 
, 
where  is the target signal.
Since each signal channel 
contributes independently to the loss, 
the authors then consider the loss function channelwisely and 
ignore the index , that is,
, 
where .

The task at hand is to seek a polynomial series  which is \textit{optimal} for the convergence of coefficients .
Since  is convex w.r.t. ,
the gradient descent's convergence rate reaches optimal 
when the {\textbf{Hessian matrix}} is identity. 
The  element 

of the Hessian matrix is:


\begin{tcolorbox}[boxrule=0.pt,height=18mm,valign=center,colback=blue!3!white]
    \begin{definition}[Optimal basis for signal ]
        For a given graph signal , polynomial basis  
        is optimal in convergence rate when 
        given in \eqref{eqHessian} is an \textbf{identity matrix}.
    \label{def:opt_basis}
    \end{definition}
\end{tcolorbox}

\citet{Wang2022jacobi} further reveal the orthonormality inherent in the optimal basis by rephrasing Equation~\eqref{eqHessian} into  
,
where the form of  is given in Proposition~\ref{prop:exact_weight} and 
and derivation is delayed in Appendix~\ref{sec:SumWang}.
Combining Definition \ref{def:opt_basis},
we soonly get:

\begin{tcolorbox}[boxrule=0.pt,height=19mm,valign=center,colback=blue!3!white]
\begin{proposition}[Exact weight function of optimal basis]
    The optimal polynomial basis in Definition \ref{def:opt_basis} is orthonormal w.r.t. weight function , 
        where 
        , 
        with
        .
        \label{prop:exact_weight}
\end{proposition}
\end{tcolorbox}


\paragraph{Unachievable Algorithm Towards Optimal Basis.}
Now we illustrate why \citet{Wang2022jacobi} believe that
though properly defined, this optimal basis is unachievable,     
and how they took a step back to get their final model. 
We summarize the process they thought of in Algorithm \ref{alg:unreacheable}. 
This process is quite habitual:  
with the weight function in Proposition~\ref{prop:exact_weight} solved,  
it is natural to use it to determine the first  polynomials by the Gram-Schmidt process 
and then use the solved polynomials in filtering as other bases, e.g. Chebyshev polynomials. 
This process is unreachable due to the eigendecomposition step, 
which is essential for the calculation of  (see Proposition~\ref{prop:exact_weight}), 
but prohibitively expensive for larger graphs. 


\begin{algorithm}[tb]
    \SetAlgoLined
\KwIn{Graph signal ; 
            Normalized graph adjacency ; 
            Truncated polynomial order 
            } 
    \KwOut{Optimal basis }
    \BlankLine
    \Indp 
    {} Eigendecomposition of {}
    \\
    Calculate  as descripted in
Proposition~\ref{prop:exact_weight}
    \\
    Use Gram-Schmidt process and weight function  to contruct an orthonormal basis 
    \\
    Apply  in polynomial filtering
    \caption{(An Unreachable Algorithm for Utilizing Optimal Basis)}
    \label{alg:unreacheable}
\end{algorithm} 
As a result, 
\citet{Wang2022jacobi} came up with a compromise. They allow their
model, namely JacoviConv, to choose from the family of orthogonal Jacobi bases, 
who have "{\tmem{flexible enough weight functions}}'', 
i.e., . 
In their implementation,   and  are discretized and chosen via hyperparameter tuning. 
Obviously, the fraction of possible weight functions JacoviConv can cover is still small, 
very possibly missing the optimal weight function in Proposition~\ref{prop:exact_weight}.  



\subsection{OptBasisGNN}

In this section, we show how the polynomial filter 
can employ the optimal basis in Definition~\ref{def:opt_basis} 
efficiently via an innovative  methodology. 
Our method does not follow the convention in  
Algorithm \ref{alg:unreacheable} 
where four progressive steps  
are included to solve the optimal polynomial bases out and utilize them.   
Instead, our solution to the optimal bases is implicit, 
accompanying the process of solving a related vector series. 
Thus, our method bypasses the untractable eigendecomposition step.



\textbf{Optimal Vector Basis with Accompanying Polynomials.\quad} 
Still, we consider graph signal filtering on one channel, 
that is,
.
Instead of taking the matrix polynomial 

as a whole, 
we now regard 

as a \textit{vector basis}. 
Then the filtered signal  is a linear combination of 
the vector basis, namely 
.
When  meets Definition \ref{def:opt_basis}, 
for all , 
the vector basis satisfies:



Given , 
we term  the \textit{\textbf{accompanying polynomial}} of a vector 
if . 
Note that an accompanying polynomial does not always exists for any vector. 
Following Equation~\eqref{eq:optvecbasis}, finding the optimal \textit{polynomial} basis for filtering  
is equivalent to finding a \textit{vector} basis  that 
satisfies two conditions: 
\textbf{Condition 1}: Orthonormality; 
\textbf{Condition 2}:  Accompanied by the optimal polynomial basis, that is, 
 establishes for each , 
where  follows Definition~\ref{def:opt_basis}.
We term such  the optimal vector basis.


\begin{algorithm}[t]
    \SetAlgoNoLine
    \caption{
        \textsc{OptBasisFiltering} \\ 
        {
          \footnotesize 
          \linespread{0.8} 
          \textbf{1.} In the comment, we write the implicitly undergoing process of obtaining the accompanying optimal polynomial basis.
          \\
          \textbf{2.} Steps 1-3 will be further substituted by Algorithm~\ref{alg:nextbasis} after the derivative of Proposition~\ref{prop:onlytwo}. 
}
    }
    \KwIn{
        Input signals  with  channels; 
        Normalized graph adjacency ;
        Order 
        }
    \Parameter{}
    \KwOut{Filtered signals } 
    \BlankLine
    \For{ \KwTo }{ 
         
        \\  \tcp*[r]{} 
  
        \\ \For{ \KwTo }{
            Step : 
             
            \tcp*[r]{}
            
            Step : 
             \tcp*[r]{}

            Step : 
             \tcp*{}
              
            
        }
         
    }
    \KwRet{Z} 
    \label{alg:OptBasisFilteringRaw}
\end{algorithm} 

When focusing solely on Condition 1, one can readily think of the fundamental Gram-Schmidt process,   
which generates a sequence of orthonormal vectors through a series of iterative steps: each subsequent basis vector is derived by 1) orthogonalization with respect to \textit{all} the previously obtained vectors, and 2) normalization. 

Moreover, with a slight generalization, Condition 2 can also be met. 
As illustrated in our \textsc{OptBasisFiltering} algorithm (Algorithm~\ref{alg:OptBasisFilteringRaw}), 
besides Steps 2-3 taken directly from the Gram-Schmidt process to ensure orthonormality, 
there is an additional Step 1 that guarantees the existence of the \textit{subsequent accompanying polynomial}.
To show this, we can write out the accompanying polynomial in each step. 
Inductively, assuming that the accompanying polynomials 
for the formerly obtained basis vectors are , 
we can observe immediately from the algorithmic flow that the -th 
accompanying polynomial is

with  as the initial step.
Since for each , 
 establishes, 
the sequence 
is exactly the optimal basis in Definition~\ref{def:opt_basis}.
Thus, by solving the vectors in the optimal vector basis in order   
and at the same time apply them in filtering by Equation~\eqref{eq:vecbasis}, 
\textbf{we can make implicit yet exact use of the optimal polynomial basis}. 
Thus, 
we can make implicit and exact use of the optimal polynomial basis 
via solving the optimal vector basis and applying them by Equation~\eqref{eq:vecbasis}. 
The cost, due to the recursive conducting over Step 2 until  is obtained, 
is in total . 

\begin{remark}
  It is revealed by Equation~\eqref{eq:implicit-poly-raw} that 
  we have in fact provided an \textit{alternative solution} to the optimal basis.   
  However, notice that we never need to explicitly compute the polynomial series.
\end{remark}


\textbf{Achieving  Time Complexity.\quad} 
We can further reduce the cost to  
by Proposition~\ref{prop:onlytwo}, 
which shows that in Step 2, instead of subtracting all the former vectors, 
we just need to subtract  and  from .

\begin{tcolorbox}[boxrule=0.pt,height=12mm,valign=center,colback=blue!3!white]
  \begin{proposition}
      In Algorithm \ref{alg:OptBasisFilteringRaw},  is only denpendent with  and .
    \label{prop:onlytwo}
    \end{proposition}
\end{tcolorbox}

\begin{proof}
Please check Appendix~\ref{sec:proof-of-vec3term}.
\end{proof}

\begin{remark}
The proof is hugely inspired by the core proof part of the Theorem \ref{thm:3term}
(Appendix \ref{sec:proof-of-3term}, the three-term recurrences theorem for orthogonal
polynomials), which shows that 
{ is only relevant to ,  and }. 
The difference is just a shift of consideration of 
the inner-product space from polynomials to vectors.
\end{remark}

\begin{algorithm}[t]
    \SetNoFillComment
    \SetAlgoNoLine
    \DontPrintSemicolon
    \KwIn{
        Normalized graph adjacency ; 
        \textbf{Two} solved basis vectors  ()
    }
\KwOut{} 
    \BlankLine
    \Indp
Step :  


    Step :  


    Step :  


    \KwRet{}
    \caption{
        \textsc{ObtainNextBasisVector} 
}
    \label{alg:nextbasis}
\end{algorithm} 
By Proposition~\ref{prop:onlytwo}, 
we substitute Steps 1-3 in Algorithm~\ref{alg:OptBasisFilteringRaw}
by Algorithm~\ref{alg:nextbasis}.
Note that 
we define 
for consistency and simplicity of presentation.
The improved \textsc{OptBasisFiltering} algorithm 
serves as the core part of the complete OptBasisGNN.
The processes on all channels are conducted in parallel.
Please check the Pytorch-style pseudo-code in Appendix~\ref{sec:pseudo_torch_OptBasis}.


\subsection{More on the Implicitly Solved Polynomial Basis}

This section is a more in-depth discussion about the nature of our method, that is, we implicitly determine the optimal polynomials by \textit{three-term recurrence relations} rather than the \textit{weight function}.  

We begin with a lemma. The proof can be found in Appendix~\ref{sec:proof-of-consistent-equation}.

\begin{lemma}
  In Algorithm~\ref{alg:nextbasis}, 
  
  \label{lemma:consistent}.
\end{lemma}


This lemma soonly leads to the following theorem.\footnote{Here,   is the input signal.}

\begin{tcolorbox}[boxrule=0.pt,height=52mm,valign=top,colback=blue!3!white]
\begin{theorem}
[Three-term Recurrences of Accompanying Polynomials (Informal)]The process for deriving the vector basis  
correspondingly defines the optimal polynomial basis 
through the following \textit{three-term relation}: 

\label{thm:3term-implicit}
\end{theorem}
\end{tcolorbox}


\begin{proof}
Combining Proposition~\ref{prop:onlytwo}, 
the accompanyingly derived basis polynomial in Equation~\eqref{eq:implicit-poly-raw} comes to 

By employing Lemma~\ref{lemma:consistent} on the right-hand side 
and staking the steps, 
the proof is completed.
\end{proof}

This implicit recurring relation revealed in Theorem~\ref{thm:3term-implicit} perfectly matches the three-term formula in Equation~\eqref{eq:formula_orthonormal} 
if we substitute  by , 
and  by . 
This is guaranteed by the orthonormality of the optimal basis 
(Proposition~\ref{prop:exact_weight}) 
and the three-term recurrence formula that constrains any orthonormal polynomial series (Theorem~\ref{thm:far-orthonormal}). 
From this perspective,
OptBasisGNN is a \textbf{particular case} of FavardGNN. 
FavardGNN is possible to reach the whole space of orthonormal bases, 
among which OptBasisGNN employs the ones that promise optimal convergence property.

Let us recall the Favard's theorem (Theorem~\ref{thm:3term_orthonormal}) and three-term recurrence theorem (Theorem~\ref{thm:far-orthonormal})
from a different perspective: 
An orthonormal polynomial series can be defined either through a \textit{weight function} or a \textit{recurrence relation} of a specific formula. 
We adopt the latter definition,    
bypassing the need for eigendecomposition as a prerequisite for the weight function.
Moreover, our adoption of such a way of definition is hidden behind the calculation of vector basis.
 




\begin{algorithm}[htp]
    \SetAlgoNoLine
    \KwIn{
        Input signals  with  channels; 
        Normalized graph adjacency ;
        Order 
}
    \Parameter{}
    \KwOut{Filtered signals } 
    \BlankLine
\\
    \For{ \KwTo }{ 
        
        ,\  
        ,\  
        \\ \For { \KwTo }{
            {\textsc{ObtainNextBasisVector}}(,, 
        \\ 
        }
        
    }
    \KwRet{Z}
    \caption{\textsc{OptBasisFiltering}}
    \label{alg:OptBasisFiltering}
\end{algorithm} \subsection{Scale Up OptBasisGNN}
\label{sec:scale-up}
\revised{
By slightly generalizing OptBasisGNN, it becomes feasible to scale it up for significantly larger graphs, such as ogbn-papers100M\cite{ogb}. 
This follows the approach of previous works that achieve scalability in GNNs by decoupling feature propagation from transformation~\cite{Chen2020GBP,Felix2019SGC,he2022chebii}.
}
We make several modifications to OptBasisGNN. First, we remove the MLP layer before \textsc{OptBasisFiltering}, resulting in the optimal basis vectors for all channels being computed in just one pass. 
Second, we preprocess the entire set of basis vectors () on CPU. 
Third, we adopt batch training, where for each batch of nodes , the corresponding segment of basis vectors  is transferred to the GPU.
 \section{Experiments}

\begin{table*}[htb]
  \tiny
  \caption{
\textbf{Experimental results.}     
    {\tmem{Accuracies  
    confidence intervals}} are displayed for each model on each dataset. 
    The best-performing two results are highlighted. 
    The results
    of GPRGNN are taken from \citet{He2021bern}. The results of BernNet, ChebNetII and
    JacobiConv are taken from original papers. The results of FavardGNN and OptBasisGNN are the  average of repeating experiments over 20 cross-validation splits.
  }
  \centering
    \begin{tabular}{lllllll}
      \toprule
      Dataset & Chameleon & Squirrel & Actor  & Citeseer & Pubmed\\ 
       & 2,277 & 5,201 & 7,600 & 3,327 & 19,717\\
       & .23 & .22 & .22  & .74 & .80\\
      \midrule
      MLP &  &  &  &
       9 & \\
GCN~\cite{kipf2016semi} &  &  &  &  
        & \\
ChebNet~\cite{Defferrard2016cheb} &  &  &  &
        & \\
ARMA~\cite{arma2021bianchi} &  &  &  & 
       & \\
APPNP~\cite{klicpera2019appnp} &  &  &  & 
        & 
      \\  
      GPR-GNN~\cite{chien2021gprgnn} &  &  &  & 
       4 & 
      \\
      BernNet~\cite{He2021bern} &  &  &  & 
        & 
      \\
ChebNetII~\cite{he2022chebii} &  &  &   &   & 
      \\
JacobiConv~\cite{Wang2022jacobi} & \bestcell {} &  &  &  \bestcell  & 
      \\
      \midrule
FavardGNN &  & \bestcell  &  &  \bestcell  & \bestcell \\
OptBasisGNN & \bestcell  & \bestcell  &
      {\bestcell } &  {} & {\bestcell }\\
      \bottomrule
    \end{tabular}
    \label{tbl:node_cls}
  \end{table*} \begin{table*}[htb]
    \centering
    \tiny
    \caption{{\textbf{Experimental results} of large-scale datasets (non-homophilous).} {\tmem{Accuracies  standard errors}} are displayed for each model on each dataset. 
    The best-performing two results are highlighted. Results of BernNet and ChebNet are taken from \citet{he2022chebii}. Other results are from \citet{Lim2021large}. \textbf{Note that} for the large Pokec and Wiki datasets, we use the \textit{scaled-up} version of OptBasisGNN, which is introduced in Section ~\ref{sec:scale-up}. 
    } 
\begin{tabular}{llllll}
      \toprule
      Dataset & Penn94 & Genius & Twitch-Gamers & Pokec & Wiki\\
       & 41,554 & 421,961 & 168,114 & 1,632,803 & {1,925,342}\\
       & 1,362,229 & 984,979 & 6,797,557 & 30,622,564 & {303,434,860}\\
       & .470 & .618 & .545 & .445 & .389\\
      \midrule
      MLP &  &  &  & 
       &  \\
GCN~\cite{kipf2016semi} &  &  &  & 
       & OOM \\
GCNII~\cite{chen2020gcnii} &  &  &  &
        & OOM \\
      MixHop~\cite{abu2019mixhop} &  &  &  &
        &  \\
LINK~\cite{Lim2021large} &  &  &  & 
        &   \\
LINKX~\cite{Lim2021large} &  &  & \bestcell 
      &    &  \\
GPR-GNN~\cite{chien2021gprgnn} &  &  &  &
        &  \\
      BernNet~\cite{He2021bern} &  &  &  &
        &  \\
ChebNetII~\cite{he2022chebii} & \bestcell {} & \bestcell {{}} &  &  \bestcell {} & \bestcell {} \\
\midrule
      FavardGNN & \bestcell  &   &  & - & - 
      \\
      OptBasisGNN &   & \bestcell  &
      \bestcell {} & \bestcell  &  \bestcell  
      \\
      \bottomrule
    \end{tabular}
    \label{tbl:nonHomo}
  \end{table*} 





In this section, we conduct a series of comprehensive experiments to demonstrate the effectiveness of the proposed methods.
Experiments consist of node classification tasks on small and large graphs, the learning of multi-channel filters, and a comparison of FavardGNN and OptBasisGNN.








\subsection{Node Classification}
\paragraph*{Experimental Setup.} 
We include medium-sized graph datasets conventionally used in preceding 
graph filtering works, 
including  
three heterophilic datasets (Chameleon, Squirrel, Actor) 
provided by \citet{Pei2020GeomGCN}
and
two citation datasets (PubMed, Citeseer)
provided by \citet{yang2016revisiting} and \citet{sen2008collective}
. 
For all these graphs, we take a  train/validation/test 
split proportion following former works, e.g. \citet{chien2021gprgnn}. 
We report our results
of twenty runs over random splits with random initialization seeds. 
For baselines, we choose sota spectral GNNs.  
For other experimental settings, please refer to Appendix~\ref{expappendix:nodecls}.
Besides, for evaluation of OptBasisGNN, 
please also check the results in the scalability 
experimental section (Section \ref{sec:exp_scaleup}). 


\textbf{Results.\quad} 
As shown in Table \ref{tbl:node_cls}, 
FavardGNN and OptBasisGNN outperform most strong baselines. Especially, in Chameleon, Squirrel and Actor, we see a big lift.
The vast selection range and learnable nature of FavardGNN and the optimality of convergence provided by OptBasisGNN both enhance the performance of polynomial filters, and their performances hold flat. 

\subsection{Node Classification on Large Datasets}
\label{sec:exp_scaleup}
\paragraph*{Experimental Setup.}
We perform node classification tasks on 
two large citation networks: ogbn-arxiv and ogbn-papers100M \cite{ogb}, 
and five large non-homophilic networks from the LINKX datasets \cite{Lim2021large}
.
Except for Penn94, Genius and Twitch-Gamers, 
all other mentioned datasets use the scaled version of OptBasisGNN.

For ogbn datasets, 
we run repeating experiments on the given split with ten random model seeds, 
and choose baselines following the scalability experiments in ChebNetII \cite{he2022chebii}.
{For LINKX datasets}, 
we use the five given splits 
to align with other reported experiment results
for Penn94, Genius, Twitch-Gamer and Pokec.
For Wiki dataset, 
since the splits are not provided, 
we use five random splits. 
For baselines, we choose spectral GNNs 
as well as top-performing spatial  models reported by \citet{Lim2021large}, including LINK, LINKX, 
GCNII~\cite{chen2020gcnii} 
and MixHop~\cite{abu2019mixhop}.  
For more detailed experimental settings, please refer to Appendix \ref{expappendix:nodecls}.

\begin{table}[h]
    \centering
    \tiny
    \caption{\textbf{Experimental results} of large-scale datasets (ogbn-citation datasets).
    {\tmem{Accuracies 
     {standard errors}}} are displayed. Besides OptBasisGNN,
    all the reported results are taken from ChebNetII. The dash line in BernNet
    means failing in preprocessing basis vectors in 24 hrs. Fixed splits of
    train/validation/test sets are used. 10 random model seeds are used for
    repeating experiments.}
    \begin{tabular}{lll}
    \toprule
    Dataset & ogbn-arxiv & ogbn-papers100M \\
     & 169,343 & 111,059,956 \\
     & 1,166,243 & 1,615,685,872 \\
     & 0.66 & - \\
    \midrule
    GCN~\cite{kipf2016semi} &  & OOM  \\
    ChebNet~\cite{Defferrard2016cheb} &  & OOM \\
    ARMA~\cite{arma2021bianchi} &  & OOM \\
    GPR-GNN~\cite{chien2021gprgnn} &  &  \\
    BernNet~\cite{He2021bern} &  &  \\
    SIGN~\cite{sign2020Frasca} &  &  \\
    GBP~\cite{Chen2020GBP} &  &  \\
    NDLS*~\cite{zhang2021ndls} &  &  \\
    ChebNetII~\cite{he2022chebii} & \bestcell  & \bestcell  \\
    \midrule
    OptBasisGNN & \bestcell  & \bestcell  \\
    \bottomrule
  \end{tabular}
\label{tbl:ogbn}
\end{table} 
\textbf{Results.\quad} As shown in Table \ref{tbl:nonHomo} and Table \ref{tbl:ogbn}, 
On Penn94, Genius and Twitch-gamer, 
our two models achieve 
comparable results to those of the state-of-the-art spectral methods. 
On ogbn datasets as well as Pokec and Wiki with tens or hundreds of millions of edges,   
we use the scaled version of OptBasisGNN with batch training. 
We do not conduct FavardGNN on these datasets, 
since the basis vectors of FavardGNN cannot be precomputed.  
Notably, on Wiki dataset, the largest non-homophilous dataset, our method surpasses the second top method by nearly one percent, this demonstrates the effectiveness of our scaled-up version of OptBasisGNN.

\subsection{Learning Multi-Channel Filters from Signals}





\textbf{Experimental Setup.\quad}
We extend the experiment of 
learning filters conducted by \citet{He2021bern} and \citet{Balcilar2021Analyzing}. 
The differences are twofold: 
First, we consider the case of \textit{multi-channel} input signals 
and learn filters \textit{channelwisely}.
Second, the \textit{only} learnable parameters are the coefficients 
.
Note that the optimization target of this experiment is identical to how the optimal basis was derived by \citet{Wang2022jacobi}
(See Section \ref{sec:SumWang_short}). 
\begin{table}[htb]
    \tiny
    \centering
    \caption{Illustration of our multichannel filter learning experiment.}
    \vskip 0.01in
    \begin{tabular}
      {p{1.6cm}p{1.6cm}p{1.6cm}}
      \toprule
      Original Image 
      & 
      Y: Band Reject
      
      Cb: :Low pass
      
      Cr: High Pass & Y: Low Pass
      
      Cb: Band Reject
      
      Cr: Band Reject\\
      \midrule
      \raisebox{0.0\height}{\includegraphics[width=1.5cm,height=1.5cm]{figures/icml-1.pdf}}
      &
      \raisebox{0.0\height}{\includegraphics[width=1.5cm,height=1.5cm]{figures/icml-2.pdf}}
      &
      \raisebox{0.0\height}{\includegraphics[width=1.5cm,height=1.5cm]{figures/icml-5.pdf}}\\
      \bottomrule
    \end{tabular}
\label{tbl:filtering_less}
  \end{table} 

We put the practical background of our multichannel experiment in 
YCbCr color space. 
Each  image is considered as a grid graph
with input node signals on three channels: Y, Cb and Cr.
Each signal might be 
filtered by complex filtering operations defined in 
\cite{He2021bern}.  
As shown in Table \ref{tbl:filtering_less}, 
using different filters on each channel 
results in different combination effects.
We create a synthetic dataset 
with 60 samples from 15 original images. 
More about the synthetic dataset are in Appendix \ref{expappendix:regression}. 

Following \citet{He2021bern}, 
we use input signals  and the true filtered signals  
to supervise the learning process of . 
The optimization goal is to minimize , 
where  is the output multi-channel signal  defined in Equation~\eqref{eq:MultiChannelFilter}.
During training, 
we use an Adam optimizer with a learning rate of  and a weight decay of 
 .
We allow a maximum of  epochs, 
and stop iteration when the difference of losses between two epochs 
is less than .

For baselines, we choose the Monomial basis, Bernstein basis, 
Chebyshev basis (with Chebyshev interpolation) 
corresponding to GPR-GNN, BernNet and ChebNetII, respectively. 
We also include {arbitrary} orthonormal basis learned by Favard for comparison.
Note that, 
we learn \textit{different filters on each channel
for all baseline basis} for fairness.  


\textbf{Results.} We exhibit 
the mean MSE losses with standard errors of the 60 samples achieved by different bases in Table~\ref{tbl:filter_all}. Optbasis, which promises the best convergence property, demonstrates an overwhelming advantage. 
A special note is needed that, 
the Monomial basis has \textit{not finished converging} at the maximum allowed th epoch. In Section~\ref{sec:exp_compare}, we extend the maximum allowed epochs to 10,000, and use the slowly-converging Monomial basis curve as a counterpoint to the non-converging Favard curve. 

\begin{table}[b]
\centering
\caption{
Experimental results of the multichannel filtering learning task. 
\textit{MSE loss}  \textit{standard errors}
of the 60 samples achieved by different bases are exhibited. 
}
\resizebox{\columnwidth}{!}{\begin{tabular}{llllll}
\toprule
{BASIS} & OptBasis                                                & ChebII & Bernstein & Favard & Monomial 
\\ \midrule
\begin{tabular}[c]{@{}l@{}}
{MSE}\\  \footnotesize{STDV}
\end{tabular} 
&
\begin{tabular}[c]{@{}l@{}}
\textbf{0.0058}\\  \footnotesize{\textbf{0.0157}}
\end{tabular} 
&    
\begin{tabular}[c]{@{}l@{}}
 \\  \footnotesize{0.2433}
\end{tabular} 
&    
\begin{tabular}[c]{@{}l@{}}
\\  \footnotesize{0.4918}
\end{tabular} 
& 
\begin{tabular}[c]{@{}l@{}}
\\  \footnotesize{0.2840}
\end{tabular} 
&   
\begin{tabular}[c]{@{}l@{}}
\\  \footnotesize{2.9263}
\end{tabular} 
\\ \bottomrule
\end{tabular}}
\label{tbl:filter_all}
\end{table} 
Particularly, in Figure \ref{fig:regression}, we visualize 
the converging process on 
\textbf{one sample}. 
Obviously,  
OptBasis show \textbf{best convergence property} 
in terms of both the fastest speed and smallest MSE error. 
Check Appendix~\ref{expappendix:regression} for more samples.




\begin{figure}[h]
    \centering
    \tiny
    \includegraphics[width=0.7\linewidth]{figures/convergence_grid_sample_3_withF.pdf}
\caption{Convergence rate of minimizing  on one sample. 
    \textit{Sample message}: 
    The true filters for this sample are low-pass(Y) / band-reject(Cb) / band-reject(Cr). 
    \textit{Legends}: 
    ChebII means using Chebyshev polynomials combined with 
    interpolation on chebynodes as in ChebNetII \cite{he2022chebii}. Favard means the bases are learned as FavardGNN.  
    In 500 epochs, the experimental groups 
    of the Monomial basis and Bernstein basis did not converge. 
    OptBasis achieves 
    the smallest MSE error in the shortest time.
    }
    \label{fig:regression}
  \end{figure}




%
 
\subsection{Non-Convergence of FavardGNN}
\label{sec:exp_compare}



Notably, in Figure \ref{fig:regression},  
an obvious \textit{bump} appeared near the th epoch. We now re-examine the non-convergence problem of FavardGNN (Section \ref{sec:weakness}). 
We rerun the multi-channel filter learning task by canceling early stopping and stretching the epoch number to 10,000. As shown in Figure~\ref{fig:regression_10000} (left), the curve of Favard bump several times. In contrast with Favard is the Monomial basis, though showing an inferior performance in Table~\ref{tbl:filter_all}, it converges slowly but stably. 
We observe a similar phenomenon with a node classification setup in Figure \ref{fig:regression_10000} (right) (See Appendix \ref{expappendix:bump} for details).
Still, very large bumps appear. Such a phenomenon might seem contradictory to the outstanding performance of FavardGNN in node classification tasks. We owe the good performances in Table~\ref{tbl:node_cls} and ~\ref{tbl:nonHomo} to the early stop mechanism. 

\begin{figure}[htp]
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.48\textwidth]{figures/analysis_10000_convergence_Favard_0.pdf}  
    \includegraphics[width=.48\textwidth]{figures/analysis_classification_10000_convergence_Favard_0.pdf}  
  \end{subfigure}
  \caption{Drop of loss in 10,000 epochs. \textit{Left}: MSE loss of regression task on one sample. 
  \textit{Right}: Cross entropy loss of classification problem on the Chameleon dataset. 
  Models based on \underline{Monomial basis} converge slowly, but stably. 
  while \underline{FavardGNNs} don't converge. 
  For the convergence curve for \underline{OptBasis}, please check Figure~\ref{fig:regression}. It converges much faster than Monomial Basis. 
  }
  \label{fig:regression_10000}
  \end{figure}




%
 




































 \section{Conclusion}
In this paper, 
we tackle the fundamental challenges of basis learning and computation in polynomial filters. 
We propose two models: FavardGNN and OptBasisGNN. 
FavardGNN learns arbitrary basis from the whole space of orthonormal polynomials, 
which is rooted in classical theorems in orthonormal polynomials.
OptBasisGNN leverages the optimal basis defined by \citet{Wang2022jacobi} efficiently, 
which was thought unsolvable.   
Extensive experiments are conducted to demonstrate the effectiveness of our proposed models. An interesting future direction is to derive a convex and easier-to-optimize algorithm for FavardGNN. 

\section*{Acknowledgements}
This research was supported in part by the major key project of PCL (PCL2021A12), by National Natural Science Foundation of China (No. U2241212, No. 61972401, No. 61932001, No. 61832017), 
by Beijing Natural Science Foundation (No. 4222028), by Beijing Outstanding Young Scientist Program No.BJJWZYJH012019100020098, by Alibaba Group through Alibaba Innovative Research Program, and by Huawei-Renmin University joint program on Information Retrieval. We also wish to acknowledge the support provided by Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education. Additionally, we acknowledge the support from Intelligent Social Governance Interdisciplinary Platform, Major Innovation \& Planning Interdisciplinary Platform for the “Double-First Class” Initiative, Public Policy and Decision-making Research Lab, Public Computing Cloud, Renmin University of China. 

\Urlmuskip=0mu plus 1mu \relax
\bibliography{./references.bib}
\bibliographystyle{icml2023}

\newpage
\appendix
\onecolumn
\section{Notations}



\label{sec:notations}
\begin{table}[H]
\caption{Summation of notations in this paper. }
\label{tbl:notations}
\begin{tabularx}{1.0\columnwidth}
{lX}
    \toprule
    {\tmstrong{Notation}} & {\tmstrong{Description}}\\
    \hline
 & Undirected, connected graph\\
 & Number of nodes in \\
 & Symmetric-normalized adjacency matrix of . \\
 & Normalized Laplacian matrix of .  = .\\
 & The -th eigenvalue of .\\
 & The -th eigenvalue of . \\
 & Eigen vectors of  and . \\
    \midrule
 & Input signal on  channel.\\
 & Input features / Input signals on 
    channels.\\
 & Filtered signals. \\
  & Filtering function defined on  and
    , respectively. \\
,  & Filtering function on the th signal
    channel. .\\
  & Filtering operation on signal . .\\
    \midrule
 & A polynomial basis of truncated order
    .\\
 & Coefficients above a basis. i.e. \\
    \bottomrule
  \end{tabularx}
\end{table} 
\section{Proofs}
\label{sec:appendix-proofs}
Most subsections here are for the convenience of interested readers. 
We provided our proofs about the theorems (except for the original form of Favard's Theorem) and their relations used across our paper, although the theorems can be found in early chapters of monographs about orthogonal polynomials~\cite{gautschi2004orthogonal, simon2014spectral}. 
We assume a relatively minimal prior background in orthogonal polynomials.   



\subsection{Three-term Recurrences for Orthogonal Polynomials (With Proof)}
\label{sec:proof-of-3term}


\begin{theorem}[\textbf{Three-term Recurrences for Orthogonal Polynomials}]\citep[p.~12]{simon54orthogonal}
  For any orthogonal polynomial series , 
  suppose that the leading coefficients of all polynomial are positive, 
  the series satisfies the recurrence relation: 
 
\label{thm:3term}
\end{theorem}






\begin{proof}
  The core part of this proof is that  is orthogonal to  for , i.e.
  
  Since  is of order , we can rewrite  into the
  combination of first  polynomials of the basis:
  
   
  or in short,
  
  Project each term onto ,
  

  Using the orthogonality among , 
  we have

   

  Next, we show  when .
  Since , it is equivalent to show .
  
  When , applying  and the orthogonality between  and  when , we get
  
  Therefore, { is only relevant to ,  and }. By shifting items, we soonly get that:
  { is only relevant to , 
  and }. 
  


  At last, we show that, 
  by regularizing the leading coefficients  to be positive, . 
  Firstly, since the leading coefficients are positive, 
   defined in Equation~\eqref{eq:proof_of_3term_expansion} 
  are positive.
  Then, notice from Equation~\eqref{eq:proof_of_three_term_coef}, 
  we get 
  
  
  We have finished our proof.
\end{proof}
 \subsection{Favard's Theorem (Monomial Case)}
\label{sec:favard-monomial}
\begin{theorem}[\textbf{Favard's Theorem}]\cite{favard1935polynomes}
  If a sequence of monic polynomials  satisfies a
  three-term recurrence relation

with , 
then  is orthogonal with respect to some 
positive weight function.
\label{thm:favard-monic}
\end{theorem} \subsection{Favard's Theorem (General Case) (With Proof)}
\label{sec:proof-of-far}
\begin{corollary}[\textbf{Favard's Theorem; general case}]
   If a sequence of polynomials  statisfies a three-term recurrence
   relation
   
   with , then there exists a positive weight function 
    such that  is orthogonal with
   respect to the inner product .   
\end{corollary}


\begin{proof}
  Set . Then we can construct a sequence of
  polynomials .
  
  \textbf{Case 1}: For  and , set  .
  
  \textbf{Case 2}:  For , define  by the three-term recurrences:
  
  According to Theorem \ref{thm:favard-monic},  is an
  orthogonal basis. Since  is scaled  by some constant, so
   \ is also orthogonal.
\end{proof}
 \subsection{Proof of Theorem \ref{thm:3term_orthonormal}}
\label{sec:proof-of-3term-orthonormal}
We restate the Theorem of three-term recurrences for orthonormal polynomials
(Theorem~\ref{thm:3term_orthonormal}) as below, and give a proof.

\textbf{(Three Term Recurrences for Orthonormal Polynomials)}
    For orthonormal polynomials  w.r.t. weight function , 
    suppose that the leading coefficients of all polynomial are positive, 
    there exists the three-term recurrence relation:
    
with .
    
\begin{proof}
      \textbf{Case 1}: .  is a constant. Suppose it to be , then
      
      \textbf{Case 2}: . By Theorem~\ref{thm:3term}, since  is
      orthogonal, there exist three term recurrences as such:
      
      By setting , ,
      , it can be rewritten into
      
      
Apply dot products with  to Equation~\eqref{eq:proof_for_3term_ortho_rewrite}, 
      we get
      
      Similarly, apply dot products with , we get:
      
      Notice that in Equation~\eqref{eq:proof_for_3term_ortho_medium_2}
      
      We get:
      
      
      So we can write Equation~\eqref{eq:proof_for_3term_ortho_rewrite} into the form below: \
      
      At last, we show .
      
      Firstly, recall that . Since , which
      is of order , can be written into the combination of  which the leading coefficients to be non-zero, i.e.
      
      Secondly, since , we can restrict all the leading coefficients to be positive.
      
      Thus we have proved  holds.
      
      Furthermore, we can rewrite  into . 
      The proof is finished.
    \end{proof}

 \subsection{Proof of Theorem \ref{thm:far-orthonormal}}
\label{sec:proof-of-favard-orthonormal}

We restate Favard's Theorem for orthonormal polynomials
(Theorem~\ref{thm:far-orthonormal}) as below, and give a proof
based on the general case~\ref{sec:proof-of-far}. 



\textbf{(Favard Theorem; Orthonormal case)}
  A polynomial series  who satisfies the recurrence relation
  
  is orthonormal w.r.t. a weight function  that  . 


\begin{proof}
  First of all, according to Theorem \ref{thm:far-orthonormal}, 
  the series  is orthogonal.
  
  Apply dot products with , we get
  
  
  Similarily, apply dot products with , we get:
  
  which can be rewritten as:
  
  Notice that
  
  We get:
  
  which indicates that the polynomials  are same in their
  norm.
  
  Since  and ,
  
  =1.
Thus the norm of every polynomial in   equals . 
  
  Combining that  is orthogonal and  for all , we arrive that  is an orthonormal basis.
\end{proof}
 \newpage
\subsection{Proof of Proposition~\ref{prop:onlytwo}}
\label{sec:proof-of-vec3term}
\begin{proof}
  First, from the construction of each  (Algorithm~\ref{alg:OptBasisFilteringRaw}, ), 
  we know that  is composed of  and
  . Therefore,  can be expressed as a weighted sum of
  , denoted as 
  .
Second, notice that 
  .
Thus, for Step 2 in Algorithm~\ref{alg:OptBasisFilteringRaw}, 
  for each  
  we can rephrase  by: 
  
which equals  when .
\end{proof} \subsection{Proof of Lemma~\ref{lemma:consistent}}
\label{sec:proof-of-consistent-equation}


\begin{proof}
    First, notice that
    
    On the other hand, 
    
    So, we get 
    
\end{proof}

Thus, we have finished our proof.
\vspace{100mm}
 

\section{Pseudo-codes}


\subsection{Pseudo-code for FavardGNN.}
\label{sec:pseudo_torch_Favard}
\begin{algorithm}[H]
\caption{FavardGNN.\textit{Pytorch style}.}
\label{alg:cotextde}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt},
}
\begin{lstlisting}[language=python]
# f: raw feature dimension
# d: hidden dimension, or number of channels
# N: number of nodes
# K: order of polynomial basis
# X(Nxd): Input features 
# P(NxN): Sym-normalized adjacency matrix 
# Coef(dx(K+1)): coefficient matrix
# SqrtBeta(dx(K+1)): Coefficients for three-term recurrences 
# Gamma(dx(K+1)): Coefficients for three-term recurrences 


# Transfer raw input in signals 
X = ReLU(MLP(X.dropout())).dropout()  # (Nxd)

SqrtBeta = torch.clamp(norm, 1e-2)

# Process H_0
H_0 = X / SqrtBeta[:,0]    # (Nxd)

Z = torch.zeros_like(X)
# Add to the final representation
Z = Z + torch.einsum('Nd,d->Nd', H_0, Coef[:,0])  

last_H = H_0
second_last_H = torch.zeros_like(H_0)

for k in range(1, K):
    # Three-term Recurrence Formula for Orthonormal Polynomials
    H_k = P @ last_H   # (Nxd)
    H_k = H_k - Gamma[k,:].unsqueeze(0)*last_H - SqrtBeta[k,:].unsqueeze(0)*second_last_H
    H_k = H_k / SqrtBeta[k+1,:].unsqueeze(0)

    # Add to the final representation
    Z = Z + torch.einsum('Nd,d->Nd', H_k, Coef[:,k])

    # Update variables
    second_last_H = last_H
    last_H = H_k

# Transform the final representation into predictions
Y = MLP(ReLU(Z).dropout())
Pred = Softmax(Y)
return Pred
\end{lstlisting}
\end{algorithm} 
\subsection{Pseudo-code for OptBasisGNN.}
\label{sec:pseudo_torch_OptBasis}


\begin{algorithm}[H]
\caption{OptBasisGNN.\textit{Pytorch style}.}
\label{alg:cotextde}

\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt},
}
\begin{lstlisting}[language=python]
# f: raw feature dimension
# d: hidden dimension, or number of channels
# N: number of nodes
# K: order of polynomial basis
# X(Nxd): Input features 
# P(NxN): Sym-normalized adjacency matrix 
# Coef(dxK): coefficient matrix


# Transfer raw input in signals 
X = ReLU(MLP(X.dropout())).dropout()  # (Nxd)

# Normalize H_0
norm = torch.norm(X, dim=0).view(1, d)
norm = torch.clamp(norm, 1e-8)
H_0 = X / norm    # (Nxd)

Z = torch.zeros_like(X)
# Add to the final representation
Z = Z + torch.einsum('Nd,d->Nd', H_0, Coef[:,0])  

last_H = H_0
second_last_H = torch.zeros_like(H_0)

for k in range(1, K):
    H_k = P @ last_H   # (Nxd)

    # Orthogonalize H_k to all the former vectors
    # To achieve this, only 2 substractions are required
    project_1 = torch.einsum('Nd,Nd->1d', H_k, last_H)              # (1xd)
    project_2 = torch.einsum('Nd,Nd->1d', H_k, second_last_H)       # (1xd)
    H_k = H_k - project_1 *  last_H - project_2 * second_last_H     # (Nxd)

    # Normalize H_k
    norm = torch.norm(H_k, dim=0).view(1, d)
    norm = torch.clamp(norm, 1e-8)
    H_k = H_k / norm   # (Nxd)

    # Add to the final representation
    Z = Z + torch.einsum('Nd,d->Nd', H_k, Coef[:,k])

    # Update variables
    second_last_H = last_H
    last_H = H_k

# Transform the final representation to predictions
Y = MLP(ReLU(Z).dropout())
Pred = Softmax(Y)
return Pred
\end{lstlisting}
\end{algorithm} 
\section{Experimental Settings.}


\subsection{Node Classification Tasks on Large and Small Datasets.}
\label{expappendix:nodecls}

\paragraph*{Model setup.} The structure of FavardGNN and OptBasisGNN follow Algorithm~\ref{alg:favardgnn_cls}. 
The hidden size of the first MLP layers  is set to be , 
which is also the number of filter channels. 
For the scaled-up OptBasisGNN, 
we drop the first MLP layer to fix the basis vectors needed for precomputing, and following the scaled-up version of ChebNetII~\cite{he2022chebii}, 
we add a three-layer  MLP with weight matrices of shape 
,  and  
after the filtering process.

For both models, the initialization of  is set as follows: for each channel , 
the coefficients of the  are set to be , 
while the other coefficients are set as zeros, 
which corresponds to initializing the polynomial filter 
on each channel to be . 
For the initialization of three-term parameters that determine the initial polynomial bases on each channel, 
we simply set  to be ones, 
and  to be zeros. 

\paragraph*{Hyperparameter tunning.} 
For the optimization process on the training sets, we tune all the parameters with Adam ~\cite{kingma2014adam} optimizer. 
We use early stopping with a patience of 300 epochs.

We choose hyperparameters on the validation sets.
To accelerate hyperparameter choosing, 
we use Optuna\cite{akiba2019optuna} to select hyperparameters from the range below with a maximum of 100 complete trials\footnote{We use Optuna's Pruner to drop some hyperparameter choice in an early stay of training. This is called an incomplete/pruned trial.}:
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item Truncated Order polynomial series: ; 
    \item Learning rates: ;
    \item Weight decays: ;
    \item Dropout rates: ; 
\end{enumerate}

There are two extra hyperparameters for scaled-up OptBasisGNN:
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item Batch size: ; 
\item Hidden size (for the post-filtering MLP): .  
\end{enumerate}


\subsection{Multi-Channel Filter Learning Task.}
\label{expappendix:regression}

\paragraph*{YCbCr Channels.}
We put the practical background of our multichannel experiment in 
the YCbCr color space, a useful color space in computer vision and multi-media
~\cite{shaik2015YCbCr}. 

\paragraph*{Our Synthetic Dataset.}
When creating our datasets with 60 samples, 
we use 4 filter combinations on 15 images in \citet{He2021bern}'s single filter learning datasets. The 4 combinations on the three channels are: 
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item Band-reject(Y) / low-pass(Cb) / high-pass(Cr); 
    \item High-pass(Y) / High-pass(Cb) / low-pass(Cr);
    \item High-pass(Y) / low-pass(Cb) / High-pass(Cr);
    \item Low-pass(Y) / band-reject(Cb) / band-reject(Cr).
\end{enumerate}

The concrete definitions of the signals, i.e. band-reject are aligned with those given in~\citet{he2022chebii}.

\paragraph*{Visualization on more samples.} We visualize more samples as Figure~\ref{fig:regression} in Figure \ref{fig:more-samples}. In all the samples, the tendencies of different curves are alike. 







 \begin{figure}[htp]
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=1.\textwidth]{figures/convergence_grid_sample_0_withF.pdf}  
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/convergence_grid_sample_1_withF.pdf}  
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/convergence_grid_sample_2_withF.pdf}  
  \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/convergence_grid_sample_3_withF.pdf}  
  \end{subfigure}
  \caption{Visualization with more samples in the multi-channel filter learning task.
  }
  \label{fig:more-samples}
  \end{figure}


\subsection{Examining of FavardGNN's bump.}
\label{expappendix:bump}
Figure \ref{fig:regression_10000} (right), 
we observe bump with a node classification setup. 
To show this clearer, we let FavardGNN and GPR-GNN (which uses the Monomial basis for classification) to fit \textit{the whole set} of nodes, and move \textrm{dropout} and \textrm{Relu} layers. 
As in the regression re-examine task, 
we cancel the earlystop mechanism,
stretch the epoch number to 10,000, 
and record cross entropy loss on each epoch.  



\section{Summary of Wang's work}
\label{sec:SumWang}

This section is a restate for a part of \citet{Wang2022jacobi}. 
For the convenience of the reader's reference, 
we write this section here. 
More interested readers are encouraged to refer to the original paper.

\citet{Wang2022jacobi} raise a criterion for best basis, but states that it
{\textbf{cannot be reached}}.



\subsection{The Criterion for Optimal Basis}
\label{sec-Wang}

Following \citet{keyulu2021Optm}, 
\citet{Wang2022jacobi} considers the squared loss 
, 
where  is the target
, and  . \footnote{Here,  is not necessarily the raw feature () but often some thing like .  is irrelevant to the choice of polynomial basis, 
and merges  into .}

Since each signal channel is independent and contributes independently to the
loss, i.e. , we can
then consider the loss function channelwisely and ignore . Loss on one
signal channel  is:

where .

This loss is a convex function w.r.t. . Therefore, the gradient
descents's convergence rate depends on the {\textbf{Hessian matrix}}'s
condition number, denoted as . When  is an identity matrix,
 reaches a minimum and leads to the best convergence rate (Boyd \&
Vandenberghe, 2009).

The Hessian matrix  looks like\footnote{Note that, \citet{Wang2022jacobi}
{\small{}}define  on  (or ) while we define
it on  (or ). They are equivalent.}:






\citet{Wang2022jacobi} further write  in the following form:

which can be equivalently expressed as a Riemann sum:

where 

.
Define 
, 
 comes to

This suggests that, 
 is an optimal basis when it is
{\textbf{orthonormal}} w.r.t. {\textbf{weight function}} . (For
more about orthonormal basis, see Section~\ref{para:inner_product}.)

\subsection{Wang's Method}

Having write out the weight function , 
the optimal basis is determined. 
\citet{Wang2022jacobi} think of a regular process for getting this optimal
basis, which is unreachable since eigendecomposition is unaffordable for
large graphs. 
We summarize this process in Algorithm \ref{alg:unreacheable}.



According to \citet{Wang2022jacobi}, 
the optimal basis would be an orthonormal basis,
but unfortunately, this basis and the exact form of its weight function is
unattainable. 
As a result, 
they come up with a compromise by allowing the
model to choose from the orthogonal Jacobi bases, which have ``{\tmem{flexible
enough weight functions}}'', i.e. . The
Jacobi bases are a family of polynomial bases. A specific form Jacobi basis is
determined by two parameters . Similar to the well-known
Chebyshev basis, the Jacobi bases have a recursive formulation, making them
efficient for calculation.


  