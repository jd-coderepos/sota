\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{multirow}
\usepackage{bbm}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{blindtext}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\iccvfinalcopy 

\def\iccvPaperID{7184} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Dense Interaction Learning for Video-based Person Re-identification}

\author{
	Tianyu He\textsuperscript{1}, Xin Jin\textsuperscript{2}, Xu Shen\textsuperscript{1}, Jianqiang Huang\textsuperscript{1}, Zhibo Chen\textsuperscript{2}, and Xian-Sheng Hua\textsuperscript{1}\\
	\textsuperscript{1}DAMO Academy, Alibaba Group \\
	\textsuperscript{2}University of Science and Technology of China\\
{\tt\small timhe.hty@alibaba-inc.com}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
   Video-based person re-identification (re-ID) aims at matching the same person across video clips. Efficiently exploiting multi-scale fine-grained features while building the structural interaction among them is pivotal for its success. In this paper, we propose a hybrid framework, Dense Interaction Learning (DenseIL), that takes the principal advantages of both CNN-based and Attention-based architectures to tackle video-based person re-ID difficulties. DenseIL contains a CNN encoder and a Dense Interaction (DI) decoder. The CNN encoder is responsible for efficiently extracting discriminative spatial features while the DI decoder is designed to densely model spatial-temporal inherent interaction across frames. Different from previous works, we additionally let the DI decoder densely attends to intermediate fine-grained CNN features and that naturally yields multi-grained spatial-temporal representation for each video clip. Moreover, we introduce Spatio-TEmporal Positional Embedding (STEP-Emb) into the DI decoder to investigate the positional relation among the spatial-temporal inputs. Our experiments consistently and significantly outperform all the state-of-the-art methods on multiple standard video-based person re-ID datasets.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Person re-identification (re-ID) tells whether a person-of-interest has been noticed in a different location by another camera. It is essential to many important surveillance applications such as tracking~\cite{wang2013intelligent} and retrieval~\cite{zheng2017sift}. In recent years, significant progresses have been achieved in image-based person re-ID~\cite{li2014deepreid,su2017pose,sun2018beyond,he2021partial}, as well as the video-based one~\cite{zheng2016mars,li2018diversity,yang2020spatial}, due to the rapid development of Convolutional Neural Networks (CNN)~\cite{krizhevsky2012imagenet}.

\begin{figure}[ht]
	\centering
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figs/intro_challenge.pdf}
		\caption{Left: different identities with similar appearance. Right: the same identity with misalignment or occlusion.}
		\label{fig:intro_challenge}
	\end{subfigure}\vspace{2mm}
	\begin{subfigure}{.475\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figs/intro_denseil.pdf}
		\caption{Our proposed Dense Interaction Learning (DenseIL).}
		\label{fig:intro_denseil}
	\end{subfigure}
	\vspace{-3mm}
	\caption{(a): Two key challenges existed in video-based person re-ID. (b): To tackle these two challenges, we introduce a DenseIL framework to densely capture multi-grained spatial-temporal interaction with the guidance of global relationship, where the preferences () are automatically learned by our proposed Dense Attention.}
	\vspace{-3mm}
\end{figure}

The goal of image-based person re-ID is to match person images captured in different times and locations. To achieve this goal, recent methods are proposed to better dig appearance features while concurrently to maintain the robustness with respect to body part misalignment~\cite{li2015multi,qian2017multi,chang2018multi,zhou2019omni,wei2017glad,sun2018beyond,wang2018learning,jin2020semantics,jin2020style,he2021partial}. Nevertheless, it is still insufficient to model the discrete relationships among various misaligned body parts from a single image. This motivates the exploration on video-based re-ID, which further takes adjacent frames of the captured person into consideration, and therefore the occluded or missed parts can be inferred. In this sense, the key point to video-based re-ID lies in designing an architecture that is suitable for temporal dynamics, such as leveraging optical flow~\cite{chen2020temporal}, RNN~\cite{zhou2017see} and 3D CNN~\cite{li2019multi}.

However, the aforementioned methods neglect the importance of spatial-temporal interaction between body parts within intra- and inter-frame, which limits their effectivity. As a result, the state-of-the-arts~\cite{yang2020spatial,yan2020learning} suggest that graph convolutional network~\cite{kipf2016semi} has the merit of modeling spatial-temporal dependencies and shows promising performance on video-based re-ID task. However, their models are built upon the \emph{coarse-grained} representation while leaving the \emph{fine-grained} information implied in each frame not fully exploited.
As demonstrated in Figure~\ref{fig:intro_challenge}, when different identities share similar appearance, depending on coarse-grained knowledge is not enough to distinguish the difference. Instead, fine-grained information (such as a shoe, a shoulder bag,~\etc.) plays an enormous role in re-identification.

Inspired by this observation, in this paper, we present Dense Interaction Learning (DenseIL) that not only builds \emph{spatial-temporal interaction} between body parts but also densely exploits \emph{fine-grained cues} (see Figure~\ref{fig:intro_denseil}).
Basically, DenseIL is composed of a CNN encoder and a Dense Interaction (DI) decoder. The CNN encoder exerts its advantage on efficiently encoding spatial context into discriminative features~\cite{lecun1998gradient}. Our CNN encoder consists of several CNN Blocks (\eg, Res-Block~\cite{he2016deep}, Dense-Block~\cite{huang2017densely}, SE-Block~\cite{hu2018squeeze},~\etc.) and therefore is capable of generating a set of hidden features from low-level/high-resolution to high-level/low-resolution.

Our DI decoder comprises stacked self-attention~\cite{vaswani2017attention}, feed-forward layer, layer normalization~\cite{ba2016layer} and a newly proposed Dense Attention.
Specifically, our decoder reuses the self-attention module explored in vanilla Transformer to deliberately model spatial-temporal inherent interaction across frames. After that, the subsequent Dense Attention module simultaneously attends to \emph{both} the outputs of the self-attention module and the intermediate fine-grained CNN features of the CNN encoder with the guidance of global relationship (\ie, the last self-attention outputs). Thus, the decoder can naturally generate a multi-grained spatial-temporal representation for each video clip.

In contrast to ResNet~\cite{he2016deep} and DenseNet~\cite{huang2017densely} that associate features of preceding layers through \emph{summation} or \emph{concatenation}, we create dense information flow between CNN and Attention mechanism with the proposed Dense Attention scheme. Intuitively, our Dense Attention is designed to better facilitate the coordination of CNN and Attention mechanism by extracting hybrid information from both the convolution and preceding self-attention features in a single attention function. Acting in this way, the model automatically and flexibly learns the preference on the fine-grained context (frame level, extracted by CNN) and global spatial-temporal interaction across frames (sequence level, modeled by self-attention) when re-identifying person-of-interest, which will be beneficial to scale-variations and misalignment problem, as a by-product of our model design.

Moreover, since the DI decoder is intrinsically permutation-invariant, we further propose Spatio-TEmporal Positional Embedding (STEP-Emb) to explicitly enhance the chronological relation in intra- and inter-frame. By incorporating STEP-Emb, our DI decoder is able to investigate the absolute or relative position of the spatial-temporal inputs.

Experiments show that our method significantly outperforms the state-of-the-arts on three video-based person re-ID datasets. In particular, we achieve the best performance of 87.0\% and 97.1\% mAP on large-scale MARS and DukeMTMC-VideoReID datasets respectively.


\section{Background}
\label{sec:background}

\subsection{Related Work}

\paragraph{Video-based Person Re-identification.}
Video-based person re-identification (re-ID)~\cite{zheng2016mars} targets on re-identifying a video clip of the person-of-interest from massive gallery candidates.
The majority of the video-based re-ID algorithms pay close attention to efficiently modeling temporal context and exploiting complementary information from various frames, by leveraging optical flow~\cite{mclaughlin2016recurrent,chung2017two,chen2020temporal}, RNN~\cite{yan2016person,zhou2017see,xu2017jointly,liu2019spatial}, pooling~\cite{zheng2016mars,wu2018exploit}, 3D CNN~\cite{qiu2017learning,li2019multi,gu2020appearance}, attention mechanism~\cite{li2018diversity,fu2019sta,liu2019spatially,li2019global,zhang2020multi} and graph convolutional network~\cite{yang2020spatial,yan2020learning}. For example, Mclaughlin \etal~\cite{mclaughlin2016recurrent} utilize optical flow, RNN and temporal pooling simultaneously to take long and short term temporal cues into account. Qiu \etal~\cite{qiu2017learning} and Li \etal~\cite{li2019multi} both employ 3D CNN to attain spatial-temporal representation for each video clip. However, both optical flow and 3D CNN are sensitive to spatial misalignment between adjacent video frames.

More recently, the rise of attention mechanism~\cite{bahdanau2015neural} convincingly demonstrates high capability of selectively focusing on specific parts of the input signal. Inspired by this, lots of studies learn the weight of spatial and temporal features separately from a static perspective~\cite{liu2017quality,zhou2017see,xu2017jointly,li2018diversity,fu2019sta,hou2019vrstc,li2019global,subramaniam2019co,hou2020temporal}. However, they do not take fully advantage of the relationships between spatial and temporal body parts across different frames, thus yielding limited capability of representation.

\vspace{-3.0mm}
\paragraph{Attention Mechanism in Computer Vision Community.}
The attention mechanism is first proven to be helpful in Neural Machine Translation~\cite{bahdanau2015neural,vaswani2017attention}, of which Transformer~\cite{vaswani2017attention} is the most famous one. Existing works in computer vision community leverage the success of Transformer mainly by replacing the convolutions with self-attention operation~\cite{parmar2019stand,wang2020axial}, or by deploying attention as an add-on to existing convolutional models in tasks like image classification~\cite{hu2018squeeze,hu2018gather,bello2019attention,hu2019local}, object detection~\cite{wang2018non,cao2019gcnet}, segmentation~\cite{fu2019dual,zhang2020feature}, generative networks~\cite{zhang2019self}, inpainting~\cite{zeng2020learning}, \etc.

The tale of attention mechanism still continues. People start to consider borrowing the entire Transformer architecture to jointly modeling vision-language representations~\cite{sun2019videobert,lu2019vilbert,su2019vl,rahman2020integrating,cornia2020meshed} or exploiting relations of the objects in image object detection~\cite{carion2020end,zhu2020deformable}.
While different from aforementioned studies that build the entire Transformer \emph{on the highest-level of} CNN spatial features, we only engage the decoder of Transformer, and replace the vanilla encoder-decoder attention with the proposed Dense Attention to pay attention to multi-grained CNN representations.

\vspace{-3.0mm}
\paragraph{Encoder-Decoder Framework with Skip Connections.}
The encoder-decoder framework is widely applied in the areas of language~\cite{cho2014learning,bahdanau2015neural}, speech~\cite{chan2016listen} and vision~\cite{long2015fully,zhu2017unpaired}. For image and video processing, Ronneberger \etal~\cite{ronneberger2015u} first introduce concatenation-style skip connections~\cite{he2016deep,huang2017densely} to the encoder-decoder-based fully convolutional networks~\cite{long2015fully}, and such framework is further utilized and refined in various computer vision tasks~\cite{mao2016image,fu2017dssd,tong2017image}.

In this work, instead of using the conventional summation-style or concatenation-style skip connections like above methods, we introduce Dense Attention to \emph{densely attend} to multi-grained features generated by the CNN encoder or the preceding decoder blocks.

\begin{figure*}
	\centering
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{figs/selfattn}
		\caption{CNN-TransEnc}
		\label{fig:transenc}
	\end{subfigure}
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{figs/encdecattn}
		\caption{CNN-TransDec}
		\label{fig:transdec}
	\end{subfigure}
	\begin{subfigure}{.357\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{figs/denseattn}
		\caption{DenseIL}
		\label{fig:densetrans}
	\end{subfigure}
	\caption{The proposed three model variants for the video-based person re-ID task. (a) The decoder only consists of self-attention (is equivalent to the encoder of vanilla Transformer). (b) The decoder contains both self-attention and encoder-decoder attention (is equivalent to the decoder of vanilla Transformer). (c) Our DI decoder involves self-attention and the proposed Dense Attention (The  denotes the concatenation operation). All schemes are equipped with our proposed STEP-Emb. We omit the layer normalization for simplicity.}
	\vspace{-3mm}
	\label{fig:attndiff}
\end{figure*}

\subsection{Brief Introduction to Transformer}
\label{sec:transformer}

Transformer~\cite{vaswani2017attention} is a general encoder-decoder framework built upon self-attention and encoder-decoder attention that achieves state-of-the-art results on many language generation~\cite{he2018layer,wang2019multi,xia2019tied} and understanding tasks~\cite{devlin2019bert,yang2019xlnet}. It is a stacked architecture with several blocks. Each block composes two or three basic modules:

\vspace{-2mm}
\paragraph{A Self-Attention Module.}
Self-attention is commonly used to relate different positions of a single sequence and generate a weighted representation of its inputs. Formally, given an -element set , the self-attention is defined as:

where  (\textit{query}, \textit{key}, \textit{value}) are all from  by a linear projection, and  is the dimension of hidden states. Note that, the self-attention module can be employed in either an encoder or a decoder and is typically associated with a residual connection and layer normalization ~\cite{ba2016layer}, resulting in the final output .

\vspace{-2mm}
\paragraph{A Feed-Forward Layer.}
Feed-forward layer is analogous to activated fully-connected layer, that provides non-linearity to the model. For any , , where the 's and 's are the parameters to be learned. Similarly, this layer is followed with a residual connection and layer normalization.

\vspace{-2mm}
\paragraph{An Optional Encoder-Decoder Attention Module.}
To realize cross-lingual (\ie, source-to-target) learning, Vaswani \etal~\cite{vaswani2017attention} introduce an encoder-decoder attention that appears in the decoder only. Formally, let  be the output of the last layer in the encoder, and  is the preceding hidden state in the decoder. The encoder-decoder attention is implemented similar to Equation~\eqref{eq:selfattn}, with the difference lies in that the \textit{query} () comes from , the \textit{key} () and \textit{value} () come from . Eventually, this module outputs . More details can be found in the original paper~\cite{vaswani2017attention}.

\section{Dense Interaction Learning}
\label{sec:method}
In this section, we will give a detailed introduction to our proposed DenseIL. Our framework has a hybrid architecture and is composed of a CNN encoder and a Dense Interaction (DI) decoder. In general, with the assistance of the proposed Dense Attention, the DenseIL can enable the decoder densely attend to intermediate CNN features, naturally forming a multi-grained and interacted representation for each video clip. In the following, we first introduce each component we used in details and then give three variants for the overall architecture as demonstrated in Figure~\ref{fig:attndiff}.

\subsection{CNN Encoder}
In contrast to the vanilla Transformer that employs self-attention to construct hidden features for the source inputs, we use convolution-based transformation to generate hidden features for the inputs due to both its efficiency and high performance (\eg, translation equivariance and locality).

\vspace{-2mm}
\paragraph{Feature Extraction.}
Given a set of sampled video frames  with length , the CNN encoder extracts the hidden spatial features block by block, where each block can be an arbitrary CNN structure (\eg, Res-Block~\cite{he2016deep}, Dense-Block~\cite{huang2017densely}, \etc.). We denote the spatial features in the -th block as .
In order to fully utilize the previously accumulated experience of pre-training~\cite{zheng2016mars}, we make minimal changes on the CNN architecture. Therefore, our CNN encoder can be initialized with ImageNet-pretrained parameters~\cite{deng2009imagenet}, endowing it with more power of robust representation.

\vspace{-2mm}
\paragraph{Horizontal Partition.}
Part-based re-ID model has enjoyed rich success in person re-ID~\cite{varior2016siamese,sun2018beyond,li2017learning,fu2019horizontal}, where each input sample is partitioned into patches by predefined priori knowledge or external supervision. The spirit of the part-based schemes lay on utilizing part-level features to provide more discriminative representation to distinguish the person-of-interest from others. Inspired by this, we horizontally divide the spatial features  produced by CNN encoder into  feature patches, and then perform average pooling on the divided feature to build a part-level feature vector  for each patch, where , and  is the number of channels of the spatial features generated by CNN encoder. Eventually, we stack all feature vectors and obtain , where  and  denotes the concatenation operation.

\subsection{Dense Interaction Decoder}  As the CNN encoder generates the spatial features for each video clip, we further adopt the Dense Interaction (DI) decoder to model the long-range context, especially for excavating intra-frame and inter-frame relationships. In general, our DI decoder follows the design philosophy of the self-attention module and feed-forward layer in Transformer, but differs in the encoder-decoder attention module, which is substituted for the proposed Dense Attention module. The  main difference between the vanilla encoder-decoder attention and our Dense Attention is that ours allows the decoder to additionally attend to intermediate fine-grained CNN features, forming a hybrid dense connection between the multi-grained CNN and the attention features.

\vspace{-2mm}
\paragraph{Self-Attention.}
Our self-attention layer is developed with a focus on modeling the interaction between feature patches by reusing the innovations explored in prior works~\cite{parikh2016decomposable,vaswani2017attention,lin2017structured}. For the stacked feature vectors , we conduct self-attention in Equation~\eqref{eq:selfattn}, where

, and  are three learnable matrices to project  into different spaces. In particular, we implement  with the multi-head attention as that proposed in Vaswani \etal~\cite{vaswani2017attention}. Our self-attention module is also associated with a residual connection and layer normalization  as described in Section~\ref{sec:transformer}.

Recent study~\cite{xiong2020layer} also shows that Pre-LN (put the  inside the residual connection) demonstrates stronger stability than the original Post-LN (place the  between the residual blocks). In this paper, we thereby adopt Pre-LN structure in our DI decoder.

\vspace{-2mm}
\paragraph{Feed-Forward Layer.}
We follow the spirit of the feed-forward layer  of vanilla Transformer to endow the decoder with non-linearity.

\vspace{-2mm}
\paragraph{The Proposed Dense Attention.}
Residual networks~\cite{he2016deep,huang2017densely} build skip connections that can easily accumulate the features from the previous layer to the next layer, and can achieve great performance in a wide range of tasks. Inspired by this, we propose the Dense Attention, an operation that shares similar insights to the residual networks but realizes skip connections with attention mechanism, instead of summation~\cite{he2016deep} or concatenation~\cite{huang2017densely}.

Concretely, the Dense Attention simultaneously attends to the intermediate features  extracted by CNN, where , and the high-level features  generated from the preceding self-attention module (and the feed-forward layer) in the -th block, where . For the intermediate CNN features , which retain the fine-grained spatial information of inputs, we successively perform horizontal partition and an average pooling operation, denoted as , to compress each partition into a feature vector before fed into Dense Attention. On the other hand, the features  are directly utilized, which represents the relationship between partitioned patches across frames. More precisely, our Dense Attention  is executed in a similar way as Equation~\eqref{eq:selfattn}, with modifications that the \textit{query} () comes from , the \textit{key} () and \textit{value} () comes from the concatenation of the pooled intermediate CNN features  and the output of self-attention module :

where  represents the concatenation operation. The detailed distinction among self-attention, encoder-decoder attention and Dense Attention will be elaborated in Section~\ref{sec:overall}.

\subsection{Spatio-Temporal Positional Embedding}
Since attention mechanism has no recurrent operation like RNN~\cite{hochreiter1997long} or convolution like CNN (\ie, permutation-variant), we need explicitly introduce the absolute or relative position of inputs to the model. Thus, we equip our DI decoder with positional embedding as demonstrated in Figure~\ref{fig:spembed}. The positional embedding is twofold: a spatial embedding and a temporal one. Both of them contribute to the final Spatial-Temporal Positional Embedding (STEP-Emb).

\begin{figure}
	\centering
	\begin{subfigure}{.23\textwidth}
		\centering
		\includegraphics[width=0.3\textwidth]{figs/spatialembed}
		\caption{Spatial Pos. Emb.}
		\label{fig:spatialembed}
	\end{subfigure}
	\begin{subfigure}{.23\textwidth}
		\centering
		\includegraphics[width=0.479\textwidth]{figs/temporalembed}
		\caption{Temporal Pos. Emb.}
		\label{fig:temporalembed}
	\end{subfigure}
	\vspace{-1.5mm}
	\caption{The spatial and temporal positional embedding contribute to the proposed STEP-Emb together.}
	\vspace{-2.5mm}
	\label{fig:spembed}
\end{figure}

\vspace{-2mm}
\paragraph{Spatial Positional Embedding.}
As illustrated in Figure~\ref{fig:spatialembed}, the spatial positional embedding is constructed based on the partitioned spatial features, denoted as  for -th frame. We use sinusoidal embedding function to format embedding vectors~\cite{gu2018non,vaswani2017attention}:  equals to  if  is even, and equals to  otherwise. Here  is the index of the partition, and  is the index of the hidden dimension. For simplicity, we denote .

\vspace{-2mm}
\paragraph{Temporal Positional Embedding.}
Compared with image-based recognition, video sequences additionally leak motion context for identifying pedestrians. However, Attention-based architectures (\ie, self-attention) are naturally permutation-invariant~\cite{vaswani2017attention}. Thus, they are incapable of explicitly modeling chronological relation between video frames, leading to a disruption of the intrinsic temporal structure. To tackle this problem, apart from the spatial positional embedding, we also build a temporal positional embedding to imply chronological order of inputs, as shown Figure~\ref{fig:temporalembed}. The temporal positional embedding can be formulated as :  equals to  if  is even, and equals to  otherwise. Here  is the index of input frame. For simplicity, we denote .

\vspace{-2mm}
\paragraph{Spatial-Temporal Positional Embedding.}
Based on the above two strategies, we derive the final Spatio-TEmporal Positional Embedding (STEP-Emb), which is constructed by the summation of the spatial and the temporal embedding. More precisely, the STEP-Emb for each feature patch  can be represented as: . Intuitively, the STEP-Emb improves the power of capturing long range interaction by enhancing the structural information between partitioned patches. We will demonstrate its ability in Section~\ref{exp:ablation}.

\subsection{Overall Architecture}
\label{sec:overall}
To dive deeply into the CNN-Attention hybrid structure, we introduce three model variants for the overall architecture. Figure~\ref{fig:attndiff} demonstrates the detailed configurations. For all the three variants, the components in the dashed boxes can be regarded as the basic building blocks to stack up. The final outputs of the decoder are processed by a batch normalization layer and a non-bias classifier layer.

\vspace{-2mm}
\paragraph{CNN encoder \& Vanilla Transformer Encoder (CNN-TransEnc).}
We directly cascade the CNN encoder, and the vanilla Transformer encoder, which mainly comprises self-attention module (\texttt{SelfAttn()}), to model interactions between feature partitions across frames. Therefore, the \textit{query}, \textit{key} and \textit{value} are all from the preceding self-attention module. The resulting architecture is illustrated in Figure~\ref{fig:transenc}.

\vspace{-2mm}
\paragraph{CNN encoder \& Vanilla Transformer Decoder (CNN-TransDec).}
We employ vanilla Transformer decoder combined with our CNN encoder as another model variant. As shown in Figure~\ref{fig:transdec}, compared with CNN-TransEnc, CNN-TransDec additionally includes the encoder-decoder attention (\texttt{EncDecAttn()}) to pay closer attention to the highest-level spatial features generated by the CNN encoder.

\vspace{-2mm}
\paragraph{CNN encoder \& Our DI Decoder with Dense Attention (DenseIL).}
The aforementioned CNN-TransEnc and CNN-TransDec only take the last-layer (the highest level) representations from the CNN encoder as inputs. As a comparison, our DenseIL (see Figure~\ref{fig:densetrans}) is equipped with the proposed Dense Attention, who is able to simultaneously attend to multi-scale intermediate spatial features in stacked CNN blocks and the corresponding interaction modeled by self-attention module.

\vspace{-2mm}
\paragraph{Loss Function.}
We adopt the simplest cross-entropy loss and batch triplet loss~\cite{hermans2017defense} for a fair comparison with the previous works~\cite{subramaniam2019co,hou2020temporal,gu2020appearance}. The two loss functions are calculated on the global average-pooled sequence-level feature vectors generated by the DI decoder.

\begin{table*}[t]
    \begin{subtable}[t]{0.32\textwidth}
		\footnotesize
		\caption{We compare Dense Attention with its counterparts.}
		\label{tab:ablationarch}
		{\def\arraystretch{1}\tabcolsep=0.85em
		\begin{tabular}[t]{@{}l|ccc@{}}
			\toprule[1.5pt]
			Methods & mAP & R-1 & R-5  \\
			\midrule
			Baseline                     & 82.1    & 87.3    & 95.6     \\
			\midrule
			CNN-TransEnc               & 85.7    &  89.4   &  96.6      \\
			CNN-TransDec           & 85.8    & 90.2    & 96.5      \\
			\midrule
			w/  Dense Concat.             & 86.4    & 89.8    & 96.8     \\
			w/  Dense Sum.             & 86.2    & 89.9    & 96.7   \\
			w/  Dense Attn.               & 87.0    & 90.8    & 97.1     \\
			\bottomrule[1.5pt]
		\end{tabular}
		}
	\end{subtable}\hspace{6pt}
	\begin{subtable}[t]{0.32\textwidth}
		\footnotesize
		\caption{DenseIL with / without positional embedding.}
		\label{tab:ablationemb}
		{\def\arraystretch{1}\tabcolsep=0.8em
		\begin{tabular}[t]{@{}l|ccc@{}}
			\toprule[1.5pt]
			Methods  & mAP & R-1 & R-5  \\
			\midrule
			Baseline                     & 82.1    & 87.3    & 95.6     \\
			\midrule
			DenseIL  & \multirow{2}{*}{86.3} & \multirow{2}{*}{89.9} & \multirow{2}{*}{97.0} \\
			w/o Pos. Emb. &  &  &   \\
			\midrule
			w/ Spatial-Emb.           & 86.6    & 90.5    & 97.1    \\
			w/ Temporal-Emb.             & 86.6    & 90.2    & 97.1    \\
			w/ STEP-Emb.               & 87.0    & 90.8    & 97.1    \\
			\bottomrule[1.5pt]
		\end{tabular}
		}
	\end{subtable}\hspace{6pt}
	\begin{subtable}[t]{0.32\textwidth}
		\footnotesize
		\caption{ Different number of blocks () with fixed .}
		\label{tab:numofblocks}
		{\def\arraystretch{1.08}\tabcolsep=0.85em
		\begin{tabular}[t]{@{}l|cccc@{}}
			\toprule[1.5pt]
			\# Blocks   & \multicolumn{1}{c|}{GFs} & mAP & R-1 & R-5  \\
			\midrule
			Baseline                &  \multicolumn{1}{c|}{\multirow{2}{*}{16.39}}    & \multirow{2}{*}{82.1}    & \multirow{2}{*}{87.3}    & \multirow{2}{*}{95.6}     \\
			() & \multicolumn{1}{c|}{}  &  &  &  \\
			\midrule
			                  & \multicolumn{1}{c|}{17.43}     & 84.2    & 88.2    & 96.3    \\
			                  & \multicolumn{1}{c|}{18.31}     & 86.3    & 89.6    & 96.9   \\
			                  & \multicolumn{1}{c|}{19.18}     & 86.6    & 90.1    & 97.1    \\
			                  & \multicolumn{1}{c|}{20.06}     & 87.0    & 90.8    & 97.1    \\
			\bottomrule[1.5pt]
		\end{tabular}
		}
	\end{subtable}
	\hfill
	
	\begin{subtable}[t]{0.32\textwidth}
		\vspace{6pt}
		\footnotesize
		\caption{Dense Attention with multi-grained CNN features. ( is omitted for simplicity).}
		\label{tab:howdense}
		{\def\arraystretch{1}\tabcolsep=0.65em
		\begin{tabular}[t]{@{}l|ccc@{}}
			\toprule[1.5pt]
			\textit{key} / \textit{value}  & mAP & R-1 & R-5  \\
			\midrule
			Baseline                     & 82.1    & 87.3    & 95.6     \\
			\midrule
			 (CNN-TransDec)                    & 85.8    & 90.2    & 96.5     \\
			 +            & 86.7    & 90.6    & 97.1   \\
			 +  +              & 87.0    & 90.8    & 97.1      \\
			 +  +  +                & 86.9    & 90.5    & 97.1   \\
			\bottomrule[1.5pt]
		\end{tabular}
		}
	\end{subtable}\hspace{6pt}
	\begin{subtable}[t]{0.32\textwidth}
		\vspace{6pt}
		\footnotesize
		\caption{Different number of dimensions of hidden states with fixed blocks .}
		\label{tab:numofhidden}
		{\def\arraystretch{1.01}\tabcolsep=0.9em
		\begin{tabular}[t]{@{}l|cccc@{}}
			\toprule[1.5pt]
			\# Dims  & \multicolumn{1}{c|}{GFs} & mAP & R-1 & R-5 \\
			\midrule
			Baseline                  & \multicolumn{1}{c|}{16.39}   & 82.1    & 87.3    & 95.6      \\
			\midrule
			                  & \multicolumn{1}{c|}{16.52}     & 86.6    & 90.1    & 97.1     \\
			                  & \multicolumn{1}{c|}{16.75}     & 86.5    & 90.3    & 97.2   \\
			                  & \multicolumn{1}{c|}{17.48}     & 86.8    & 90.6    & 97.0    \\
			                  & \multicolumn{1}{c|}{20.06}     & 87.0    & 90.8    & 97.1     \\
			\bottomrule[1.5pt]
		\end{tabular}
		}
	\end{subtable}\hspace{6pt}
	\begin{subtable}[t]{0.32\textwidth}
		\vspace{6pt}
		\footnotesize
		\caption{We vary the number of partitions for each frame.}
		\label{tab:numofpart}
		{\def\arraystretch{1}\tabcolsep=1.3em
		\begin{tabular}[t]{@{}l|ccc@{}}
			\toprule[1.5pt]
			\# Partitions  & mAP & R-1 & R-5  \\
			\midrule
			Baseline                     & 82.1    & 87.3    & 95.6     \\
			\midrule
			             & 86.1    & 89.7    & 96.9     \\
			             & 86.8    & 90.5    & 97.1   \\
			             & 87.0    & 90.8    & 97.1      \\
			             & 86.6    & 90.5    & 97.0   \\
			\bottomrule[1.5pt]
		\end{tabular}
		}		
	\end{subtable}
	\hfill
	\vspace{-1mm}
	\caption{Ablation study on MARS dataset. GFs means GFLOPs. More details are explained in the text.}
	\vspace{-3mm}
    \label{tab:temps}
\end{table*}

\section{Experimental Results}
\label{sec:exps}

\subsection{Datasets and Evaluation Protocol}
\paragraph{Datasets.}
We evaluate our DenseIL on several commonly adopted video-based re-ID benchmarks, including MARS~\cite{zheng2016mars}, DukeMTMC-VideoReID (DukeV)~\cite{ristani2016performance,wu2018exploit} and iLIDS-VID~\cite{wang2014person}.


\paragraph{Evaluation Protocol.}
Following the common practices~\cite{zheng2015scalable,zheng2016mars,li2018diversity,yang2020spatial}, we resort to both Cumulative Matching Characteristic (CMC) curves at Rank-1 (R-1) to Rank-20 (R-20), and mean Average Precision (mAP) as evaluation metrics. We do not use re-ranking for all settings.

\subsection{Implementation Details}
\label{exp:expdetail}
We employ ImageNet-pretrained standard ResNet-50 as the initialization of our CNN encoder. To be comparable with the previous works~\cite{sun2018beyond,liu2019spatially,zhang2020multi,yang2020spatial}, we also remove the last spatial down-sampling operation in the  block for both the baseline and our schemes. Each input frame is resized to  with frame-level random horizontal flips~\cite{liu2019spatially} and sequence-level random erasing~\cite{zhong2020random,zhang2020multi,chen2020temporal} for data augmentation. We adopt a restricted random sampling strategy~\cite{li2018diversity,liu2019spatially,yang2020spatial} to randomly sample frames from equally divided  chunks for each video clip. For each training batch, we randomly sample  identities, each with  tracklets~\cite{hermans2017defense,yang2020spatial}. We train our network for  epoch with Adam optimizer for both the cross-entropy loss and the triplet loss~\cite{hermans2017defense}. The initial learning rate is set to  and is decayed by  every  epochs. The algorithm is implemented with PyTorch\footnote{Based on: \url{https://github.com/yuange250/not_so_strong_baseline_for_video_based_person_reID}.} and trained on a -GPU machine.


\subsection{Study on Design Choices}
\label{exp:instantiations}



\begin{figure*}[ht]
	\centering
	\includegraphics[width=.9\linewidth]{figs/vis2.pdf}
	\caption{Visualization of the attention weights learned in Dense Attention. Darker color means higher attention weight.}
	\vspace{-3mm}
	\label{fig:vis}
\end{figure*}

\begin{figure}[ht]
	\centering
	\includegraphics[width=.95\linewidth]{figs/vis_result2.pdf}
	\caption{Visualization of re-identification results of the baseline (top) and DenseIL (bottom). The left column of each case is sampled frames of query sequence and the right five columns are top-5 retrieved sequences in the gallery set, where the item annotated with green box is correctly re-identified, and the red box denotes the wrong results.}
	\vspace{-3mm}
	\label{fig:vis_result}
\end{figure}

\paragraph{Spatial-Temporal Interaction Matters.}
We present the performance (\%) of CNN-TransEnc, CNN-TransDec and DenseIL (w/ Dense Attn.) on MARS dataset in Table~\ref{tab:ablationarch}, from which we can observe that solely allocating self-attention module to the decoder (CNN-TransEnc) already brings effective performance gain (+3.6\% mAP), while the one additionally inserted with encoder-decoder attention (CNN-TransDec) achieves comparable results (+3.7\% mAP). Such observation demonstrates the strong interaction-modeling capability of self-attention and exactly verifies one of our starting point: spatial-temporal interaction matters for video-based re-ID.

\vspace{-2.5mm}
\paragraph{Dense Attention \vs Dense Summation/Concatenation.}
Unlike ResNet~\cite{he2016deep} and DenseNet~\cite{huang2017densely} that combine features through summation or concatenation, we densely attend to preceding CNN and self-attention features by our proposed Dense Attention. Accordingly, we conduct experiments on densely summing (w/ Dense Sum.) or concatenating (w/ Dense Concat.) preceding CNN features to the decoder. To be more specific, we perform partition and pooling operation on the spatial features of each CNN block, and sum / concatenate them into the output of each self-attention module. We follow the same training strategy as described in Section~\ref{exp:expdetail} for both schemes and report mAP and CMC scores on MARS dataset in Table~\ref{tab:ablationarch}. As a result, the Dense Attention (w/ Dense Attn.) reaches the highest performance over the two counterparts (+0.6\% mAP). On the one hand, compared with summation or concatenation, our Dense Attention is able to adaptively learns the preference on low-level CNN features or high-level spatial-temporal interaction. On the other hand, the features learned by the CNN and the attention module may not lie in the same manifold, and concatenating them together can result in distribution gap. Instead, our Dense Attention enable better coordination between CNN and Attention by a relatively soft connection scheme.

\vspace{-2.5mm}
\paragraph{How Dense Attention Works?}
In order to investigate the functional principle of our Dense Attention, we adjust the composition of its \textit{key} and \textit{memory} in Equation~\eqref{eq:denseattn2} and conduct corresponding experiments. More precisely, let , ,  and  be the intermediate features generated by four CNN blocks (ResNet-50), and  be the features generated from the preceding self-attention module. We allow Dense Attention to attend to  and part of . The results on MARS dataset are demonstrated in Table~\ref{tab:howdense}. We can observe that when attending to  and any of , DenseIL outperforms the CNN-TransDec counterpart by a large margin. It is also interesting to see that when further attending to the lowest-level CNN feature , the performance slightly drops (-0.1\% mAP) compared with . This might be due to the fact that the lowest-level CNN feature lacks discriminative information and is not conducive to the coordination with the attention scheme in terms of video re-ID task. For a better understanding of Dense Attention, we randomly feed image samples into DenseIL and map the attention weights of Dense Attention to colors. The results are visualized in Figure~\ref{fig:vis}, which suggests that the Dense Attention indeed cares about multi-grained CNN features simultaneously, while gives relatively more concern on the highest-level one. We also demonstrate the retrieval results in Figure~\ref{fig:vis_result}. We can observe that, in the left case, although there exists misalignment, movement and occlusion in the query respectively, our scheme is still able to match the person-of-interest accurately. In contrast, the baseline model misses the sequences of the same identity. In the right case, the baseline model re-identities the query incorrectly due to ignoring the fine-grained information between visually similar identities. In contrast, DenseIL captures the contour and the fine-grained characters on her back, yielding a satisfactory re-ID result.


\vspace{-2.5mm}
\paragraph{STEP-Emb Tackles Permutation-Invariant Issue.}
The philosophy behind STEP-Emb is to empower the DI decoder to capture long-range interaction by enhancing structural information between partitioned patches with embeddings. To verify its functionality, we remove the STEP-Emb (DenseIL w/o Pos. Emb.) and demonstrate its performance (\%) on MARS dataset in Table~\ref{tab:ablationemb}. It can be easily observed that employing either the Spatial Positional Embedding (w/ Spatial-Emb.) or the Temporal Positional Embedding (w/ Temporal-Emb) undeniably boosts the person re-ID performance (+0.3\% mAP). As the joint one, STEP-Emb, achieves the best performance consistently on all metrics. This suggests that STEP-Emb is indispensable for spatial-temporal positional modeling in our DI decoder.

\begin{table*}[t]
	\centering
	\renewcommand*{\arraystretch}{1.05}
	\small
	\begin{tabular}{@{}lcccccccccccc@{}}
	\toprule[1.5pt]
	\multicolumn{1}{c}{\multirow{2}{*}{Methods}} & \multirow{2}{*}{Proc.} & \multirow{2}{*}{Backbone} & \multicolumn{4}{c}{MARS} & \multicolumn{4}{c}{DukeV} & \multicolumn{2}{c}{iLIDS-VID}  \\ \cmidrule(l){4-13} 
	\multicolumn{1}{c}{} &  &  & mAP & R-1 & R-5 & \multicolumn{1}{c|}{R-20} & mAP & R-1 & R-5 & \multicolumn{1}{c|}{R-10} & R-1 & \multicolumn{1}{c}{R-5}  \\ \midrule
\multicolumn{1}{l|}{STAN~\cite{li2018diversity}} & \multicolumn{1}{c|}{CVPR18} & \multicolumn{1}{c|}{Res50} & 65.8 & 82.3 & - & \multicolumn{1}{c|}{-} & - & - & - & \multicolumn{1}{c|}{-} & 80.2 & \multicolumn{1}{c}{-}  \\
	\multicolumn{1}{l|}{Snippet~\cite{chen2018video}} & \multicolumn{1}{c|}{CVPR18} & \multicolumn{1}{c|}{Res50} & 76.1 & 86.3 & 94.7 & \multicolumn{1}{c|}{98.2} & - & - & - & \multicolumn{1}{c|}{-} & 85.4 & \multicolumn{1}{c}{96.7} \\
	\multicolumn{1}{l|}{STA~\cite{fu2019sta}} & \multicolumn{1}{c|}{AAAI19} & \multicolumn{1}{c|}{Res50} & 80.8 & 86.3 & 95.7 & \multicolumn{1}{c|}{-} & 94.9 & 96.2  & 99.3  & \multicolumn{1}{c|}{99.6} & - & \multicolumn{1}{c}{-}  \\
	\multicolumn{1}{l|}{ADFD~\cite{zhao2019attribute}} & \multicolumn{1}{c|}{CVPR19} & \multicolumn{1}{c|}{Res50} & 78.2 & 87.0 & 95.4 & \multicolumn{1}{c|}{98.7} & - & - & - & \multicolumn{1}{c|}{-} & 86.3 & \multicolumn{1}{c}{97.4}  \\
	\multicolumn{1}{l|}{VRSTC~\cite{hou2019vrstc}} & \multicolumn{1}{c|}{CVPR19} & \multicolumn{1}{c|}{Res50} & 82.3 & 88.5 & 96.5 & \multicolumn{1}{c|}{-} & 93.5 & 95.0  & 99.1  & \multicolumn{1}{c|}{99.4} & 83.4 & \multicolumn{1}{c}{95.5}  \\
	\multicolumn{1}{l|}{GLTR~\cite{li2019global}} & \multicolumn{1}{c|}{ICCV19} & \multicolumn{1}{c|}{Res50} & 78.5 & 87.0 & 95.8 & \multicolumn{1}{c|}{98.2} & 93.7 & 96.3 & 99.3  & \multicolumn{1}{c|}{-} & 86.0 & \multicolumn{1}{c}{\textbf{98.0}}  \\
	\multicolumn{1}{l|}{COSAM~\cite{subramaniam2019co}} & \multicolumn{1}{c|}{ICCV19} & \multicolumn{1}{c|}{SE-Res50} & 79.9 & 84.9 & 95.5 & \multicolumn{1}{c|}{97.9} & 94.1 & 95.4  &  99.3 & \multicolumn{1}{c|}{-} & 79.6 & \multicolumn{1}{c}{95.3}  \\
	\multicolumn{1}{l|}{STE-NVAN~\cite{liu2019spatially}} & \multicolumn{1}{c|}{BMVC19} & \multicolumn{1}{c|}{Res50-NL} & 81.2 & 88.9 & - & \multicolumn{1}{c|}{-} & 93.5 & 95.2 & - & \multicolumn{1}{c|}{-} & - & \multicolumn{1}{c}{-}  \\
	\multicolumn{1}{l|}{MG-RAFA~\cite{zhang2020multi}} & \multicolumn{1}{c|}{CVPR20} & \multicolumn{1}{c|}{Res50} & 85.9 & 88.8 & 97.0 & \multicolumn{1}{c|}{98.5} & - & - & - & \multicolumn{1}{c|}{-} & 88.6 & \multicolumn{1}{c}{\textbf{98.0}}  \\
	\multicolumn{1}{l|}{MGH~\cite{yan2020learning}} & \multicolumn{1}{c|}{CVPR20} & \multicolumn{1}{c|}{Res50-NL} & 85.8 & 90.0 & 96.7 & \multicolumn{1}{c|}{98.5} & - & - & - & \multicolumn{1}{c|}{-} & 85.6 & \multicolumn{1}{c}{97.1}  \\
	\multicolumn{1}{l|}{STGCN~\cite{yang2020spatial}} & \multicolumn{1}{c|}{CVPR20} & \multicolumn{1}{c|}{Res50} & 83.7 & 90.0 & 96.4 & \multicolumn{1}{c|}{98.3} & 95.7 & 97.3 & 99.3 & \multicolumn{1}{c|}{-} & - & \multicolumn{1}{c}{-}  \\
	\multicolumn{1}{l|}{TCLNet~\cite{hou2020temporal}} & \multicolumn{1}{c|}{ECCV20} & \multicolumn{1}{c|}{Res50-TCL} & 85.1 & 89.8 & - & \multicolumn{1}{c|}{-} & 96.2 & 96.9 & - & \multicolumn{1}{c|}{-} & 86.6 & \multicolumn{1}{c}{-}  \\
	\multicolumn{1}{l|}{AP3D~\cite{gu2020appearance}} & \multicolumn{1}{c|}{ECCV20} & \multicolumn{1}{c|}{AP3D} & 85.1 & 90.1 & - & \multicolumn{1}{c|}{-} & 95.6 & 96.3 & - & \multicolumn{1}{c|}{-} & 86.7 & \multicolumn{1}{c}{-}  \\
	\multicolumn{1}{l|}{AFA~\cite{chen2020temporal}} & \multicolumn{1}{c|}{ECCV20} & \multicolumn{1}{c|}{Res50} & 82.9 & 90.2 & 96.6 & \multicolumn{1}{c|}{-} & 95.4 & 97.2 & 99.4 & \multicolumn{1}{c|}{99.7} & 88.5 & \multicolumn{1}{c}{96.8}  \\ \midrule
\multicolumn{1}{l|}{Ours} & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{Res50} & \textbf{87.0} & \textbf{90.8} & \textbf{97.1} & \multicolumn{1}{c|}{\textbf{98.8}} & \textbf{97.1} & \textbf{97.6} & \textbf{99.7} & \multicolumn{1}{c|}{\textbf{99.9}}  & \textbf{92.0} & \multicolumn{1}{c}{\textbf{98.0}}   \\ 
	\bottomrule[1.5pt]
	\end{tabular}
\caption{We compare the DenseIL with state-of-the-art results.}
	\vspace{-3mm}
	\label{tab:sota}
\end{table*}

\subsection{Study on Model Variations}
\label{exp:ablation}


\paragraph{Deeper DI Decoder Improves the Performance.}
As described in Section~\ref{sec:method}, our DI decoder is stacked with  basic building blocks. Correspondingly, we vary different number of blocks to investigate how our model performs. Table~\ref{tab:numofblocks} illustrates the performance (\%) on MARS dataset with  blocks. We can observe a consistent performance improvement as we increase the number of blocks in DI decoder.

\vspace{-2.5mm}
\paragraph{Wider DI Decoder Improves the Performance.}
We vary dimension of hidden states  to discover how the width of DI decoder affects the re-ID performance. Table~\ref{tab:numofhidden} demonstrates the corresponding results, which tell that wider DI decoder indeed shows larger capability, resulting consistent performance gain on MARS dataset.

\vspace{-2.5mm}
\paragraph{Horizontal Partition Helps Our DI Decoder to Learn Discriminative Features.}
Part-based techniques for person re-ID has remained attractive in the last few years~\cite{sun2018beyond,fu2019sta,yan2020learning,yang2020spatial,zhang2020feature}, we here investigate how horizontal partition influence the learning of DenseIL. The results are illustrated in Table~\ref{tab:numofpart}, from which we can conclude that the best performance is reached when we partition each spatial feature into four patches. Interestingly but not surprisingly, training with larger number of partitions does not achieve higher result (-0.4\% mAP). This might be due to more partitions amplify the misalignment issue existing in video-based person re-ID.

\vspace{-2mm}
\paragraph{Our DI Decoder is not Limited to Specific CNN Backbone.}
For example, DenseIL is able to boost the mAP of DenseNet-121 baseline from 82.5\% to 86.7\% on MARS dataset, demonstrating great generalization ability on different CNN basic structure.

\subsection{Computational Complexity}

For all settings, we compute the theoretical GFLOPS with the tool \texttt{compute\_flops.py} \footnote{\url{https://gist.github.com/fmassa/c0fbb9fe7bf53b533b5cc241f5c8234c}} from DETR~\cite{carion2020end}. From Table~\ref{tab:numofblocks} and~\ref{tab:numofhidden} we can observe that, the deeper and the wider the DI decoder is, the higher computational complexity we have. However, such issue can be alleviated with the recently developed high efficiency Transformer variants~\cite{kitaev2019reformer,zhu2020deformable}. In addition, for the lightweight version in Table~\ref{tab:numofhidden}, \eg, , which brings negligible computational overhead, still outperforms the baseline by 4.5\% mAP.

\subsection{Comparison with State-of-the-Art Results}

We compare our proposed DenseIL with the best results reported in recent literatures in Table~\ref{tab:sota}. We can see that our method achieves significant improvement over all the competitive state-of-the-arts, including the optical flow-based~\cite{chen2020temporal}, graph-based~\cite{yang2020spatial,yan2020learning}, 3D CNN-based~\cite{li2019multi,gu2020appearance} and attention-based~\cite{fu2019sta,hou2019vrstc,li2019global,subramaniam2019co,hou2020temporal} approaches, in terms of both the mAP and CMC metrics. For example, our method surpasses the latest optical flow-based scheme~\cite{chen2020temporal} by 4.1\% mAP, outperforms the latest graph-based scheme~\cite{yang2020spatial} by 3.3\% mAP on MARS dataset. For attention-based approaches~\cite{li2019global,hou2020temporal}, our method is also superior to all of them by a large margin. We argue that the performance gain comes from the usage of both the multi-grained cues and the spatial-temporal interaction, which are not fully exploited in previous works.

\section{Conclusion}
In this paper, we successfully leverage Dense Interaction Learning (DenseIL) to alleviate the difficulties of multi-grained spatial-temporal interaction modeling for video-based person re-ID. Specifically, by incorporating the proposed Dense Attention and STEP-Emb, we let the DI decoder densely attends to intermediate CNN features and generates an intrinsically multi-grained representation for each video clip. Experimental results demonstrate that our approach surpasses all previous methods.

For future works, it is interesting to apply Dense Interaction Learning to more video understanding tasks, such as action recognition, video captioning, \etc.

\vspace{-1mm}
\paragraph{Acknowledgements.} This work was supported in part by the National Key Research and Development Program of China 2018AAA0101400 and NSFC Grant U1908209, 61632001, 62021001. We thank all the anonymous reviewers for their valuable comments on our paper.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
