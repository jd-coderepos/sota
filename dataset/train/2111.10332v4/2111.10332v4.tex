



\documentclass[sigconf]{acmart}
\let\Bbbk\relax
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{indentfirst}
\renewcommand\footnotetextcopyrightpermission[1]{}

\settopmatter{printacmref=false}
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

\acmConference[Under Review]{Make sure to enter the correct
  conference title from your rights confirmation emai}{April 04, 2022}{}
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
 June 03--05, 2018, Woodstock, NY} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


\acmSubmissionID{563}



\begin{document}

\title{DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion}















\author{Renrui Zhang, Ziyao Zeng, Ziyu Guo, Xinben Gao 
Kexue Fu, Jianbo Shi\\
  Shanghai AI Laboratory \quad 
  Uisee Research \quad
  ShanghaiTech University \quad\\
  Peking University \quad
  University of Pennsylvania \quad
\\
\texttt{zhangrenrui@pjlab.org.cn,
zengzy@shanghaitech.edu.cn,
jshi@seas.upenn.edu} \\
}


\renewcommand{\shortauthors}{Renrui, Ziyao and Ziyu, et al.}

\begin{abstract}
Point cloud processing is a challenging task due to its sparsity and irregularity. Prior works introduce delicate designs on either local feature aggregator or global geometric architecture, but few combine both advantages.   We propose \textbf{D}ual-\textbf{S}cale Point Cloud Recognition with High-frequency Fusion (\textbf{DSPoint}) to extract local-global features by concurrently operating on voxels and points.  We reverse the conventional design of applying convolution on voxels and attention to points.  Specifically, we disentangle point features through channel dimension for dual-scale processing: one by point-wise convolution for fine-grained geometry parsing, the other by voxel-wise global attention for long-range structural exploration. We design a co-attention fusion module for feature alignment to blend local-global modalities, which conducts inter-scale cross-modality interaction by communicating high-frequency coordinates information. Experiments, ablations and error mode analysis on widely-adopted ModelNet40, ShapeNet, and S3DIS demonstrate the state-of-the-art performance of our DSPoint. Our code is also available at \url{https://github.com/Adonis-galaxy/DSPoint}.\footnote{ indicates equal contributions.  indicates the corresponding author.}
\end{abstract}


\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010371.10010396.10010400</concept_id>
       <concept_desc>Computing methodologies~Point-based models</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}
\ccsdesc[500]{Computing methodologies~Point-based models}

\keywords{point cloud recognition, neural networks, dual-scale processing}





\maketitle


\section{Introduction}
\label{sec:intro}

3D vision has drawn increasing attention recently with the rapid development of 3D sensing technologies. It brings out many challenging 3D tasks, such as point cloud recognition(~\cite{xie2018attentional,rao2019spherical,mahmoudi2009three}), shape~\cite{yi2017syncspeccnn} and scene~\cite{rusu2009close,zhang2020point,yang2015new} segmentation, object detection based on point cloud~\cite{zhou2018voxelnet,shi2020point,wang2015voting,shi2020points,he2020structure} and monocular image~\cite{weng2019monocular,peng2021lidar,xu2018multi}, point cloud registration~\cite{aoki2019pointnetlk,yang2020teaser,wang2019deep}.   Unlike 2D images that consist of pixels in uniform grids, a 3D point cloud is permutation invariant, spatially irregular, and density varying, which leads to non-trivial difficulty for algorithm designs. 

Point cloud methods can be divided into two groups: projection-based~\cite{goyal2021revisiting,roveri2018network,sarkar2018learning,qi2016volumetric,wu20153d,maturana2015voxnet} and point-wise~\cite{qi2017pointnet,qi2017pointnet++,li2018pointcnn,liu2019relation,wu2019pointconv} methods. 
Projection-based models convert points into a regular grid representation, such as multi-view images~\cite{goyal2021revisiting,roveri2018network,sarkar2018learning} or voxels~\cite{qi2016volumetric,wu20153d,maturana2015voxnet}, so that convolution models~\cite{maturana2015voxnet} can be used directly for recognition. However, voxelizations lose local shape details and suffer from heavy memory and computation costs. In contrast, point-wise methods require no modal transformation and thus maintain all original information, especially the fine-grained structure. 
PointNet~\cite{qi2017pointnet} encodes each point with Multi-layer Perceptron (MLP) and eliminates the unordered set problem via max-pooling operation. PointNet++~\cite{qi2017pointnet++} further introduces the hierarchical architecture for point cloud's local feature aggregation. Point-wise convolution proposed by PointCNN~\cite{li2018pointcnn}, PAConv~\cite{xu2021paconv} and others~\cite{howard2017mobilenets} construct permutation-invariant convolution.



\begin{figure}[tb]
  \centering
\includegraphics[width=0.45\textwidth]{pic/vis_example.png}
  \caption{Visualization of our method on ShapeNet\cite{wu20153d} Part Segmentation, compared with PAConv\cite{xu2021paconv}. It shows our advantage in segmenting parts into spatial-consistent regions.}
    \label{fig:example}
    \vspace{-0.5cm}
\end{figure}

Local and global features capture different aspects of the shape.  The key question is: how to integrate local and global information while maintaining separate processing to prevent over smoothing between them. 

From a representation perspective, projection-based representations are better suited for global part-whole structure relationships, while point representations have advantages on local parsing of shape details.  
Motivated by this, PVCNN~\cite{liu2019point} designs a point-voxel module to parallelly encode point clouds from dual modalities (voxels and points), by leveraging 3D convolution for the voxel branch and point-wise MLP for the point branch. 

The conventional wisdom of applying local convolution on voxel and global attention on points makes sense from a computational viewpoint. Still, it achieves the opposite goal of extracting global structure from voxel representation and local shape information from the point representation. Furthermore, the simple combination (by addition) of two modalities (local and global)  could blur out local details.  

We propose a reverse design, where we apply a global process on voxels to extract long-range structure relationships and a local one on the points to compute detail shape features.  We call our \textbf{D}ual-\textbf{S}cale network for Point cloud understanding with high-frequency encoding, as \textbf{DSPoint}. Specifically, we disentangle the point representation along the channel dimension: one 
part encoding local feature and the other part for the global feature. In each processing block, local channels are processed by point-wise dynamic convolution~\cite{xu2021paconv}, and the global ones are firstly converted to voxelized representation and then parsed via global attention mechanism~\cite{bahdanau2014neural}. 

From the representation perspective, illustrated in Figure \ref{fig:point_voxel}, one can compute fine-grained geometry features from 3D points locally. At the same time, the voxelization process naturally aggregates neighboring points' features and is suitable for global part-whole structural relationship reasoning. From the computation view, convolution is natural for local feature aggregation, but attention is designed for long-range dependency modeling.  Consequently, processing such two modalities with convolution on the local point level and attention on the voxel level could be a good choice for point cloud understanding. 

After the concurrent pathways, the voxel modality is back-projected to points by assigning each voxel's feature to every point within it.  Here, we obtain two part-channel representations of each point: voxel-wise global feature and point-wise local feature.   We observe that the naive addition of different modalities often results in feature misalignment and blurring.  To effectively exchange local-global information, we build on a recently introduced Dual-stream Net(DS-Net)~\cite{mao2021dual} co-attention design.  In this design, the global part configuration features serve as `query' for the local shape feature 'keys', and vice versa.  However, the direct application of DS-Net is still insufficient for removing across modality mis-alignment.   This is because shape features of different points from one voxel usually are homogeneous, and directly fusing them with heterogeneous point-wise shape features would bring about ambiguities.   Borrowing the high-frequency point encoding concept in NeRF~\cite{mildenhall2020nerf}, we encode each voxel's coordinate into high-frequency representation and integrate it with point-wise features.  Aided by this inter-path coordinates communication, local-global shape features from dual modalities (voxels and points) can be highly aligned.  
The visual example in Figure~\ref{fig:example} illustrates the effectiveness of our `reverse' design of using a 3D grid for global attention and local convolution for 3D raw points. 



















We summarize the contributions as below:

\begin{itemize}
    \item We propose DSPoint, which concurrently processes point cloud with dual scales and modalities for robust local-global features extraction.
    
    \item A high-frequency fusion module is introduced by communicating high-dimensional coordinates information between voxel-wise and point-wise features.
    
    \item To illustrate our model's superiority, we experiment DSPoint on shape classification, shape and scene segmentation, respectively on ModelNet40, ShapeNet, and S3DIS datasets.
\end{itemize}

\begin{figure}[t]
  \centering
\includegraphics[width=0.2\textwidth]{pic/point_voxel.png}
  \caption{Dual modalities (voxels and points) processing: grey voxels aggregate points to represent the plane's global structure coarsely, while green 3D points describe subtle local shape details.}
    \label{fig:point_voxel}
    \vspace{-0.5cm}
\end{figure}


\section{Related Work}
\label{sec:related work}

\paragraph{Deep learning for Point Cloud.}
Projection-based models and point-wise models are two main branches of deep learning in 3D, distinguished by their data processing modality. Some of the projection-based models project raw points onto a set of image planes with pre-defined~\cite{su2015multi} or learnable~\cite{kanezaki2018rotationnet} viewpoints and then utilize 2D convolutions for robust feature extraction.  One can combine images from different views so to minimize the information loss on the original point cloud. Still, the complex and time-consuming projection process makes this approach unpractical for real-time applications. Alternatively, some approaches transform points into spatial voxels, such as VoxelNet~\cite{zhou2018voxelnet} and ~\cite{su2018splatnet,riegler2017octnet}, which are uniform grid-based representations and thus can be applied 3D convolutions~\cite{maturana2015voxnet} or attention mechanism~\cite{bahdanau2014neural}. However, voxel-based networks confront information loss due to low-resolution quantization.  It has an inpractical cubically growing running time. Point-wise networks directly process raw points with irregular distribution over 3D space. PointNet~\cite{qi2017pointnet} leverage Multi-layer Perceptron(MLP) to extract point-wise features and integrate them with a global pooling. PointNet++~\cite{qi2017pointnet++} proposes a hierarchical PointNet~\cite{qi2017pointnet} architecture to capture local contexts with sampling and grouping blocks. DGCNN~\cite{wang2019dynamic}, KPConv~\cite{thomas2019kpconv} and PAConv~\cite{xu2021paconv} further design convolutions on spatial points for better local geometry encoding. To aggregate both advantages, our DSPoint adopts dual-path architecture to concurrently encode point features with voxel branch and point branch, respectively, for understanding global and local features.



\begin{figure*}[t]
  \centering
    \includegraphics[width=0.99\textwidth]{pic/model.pdf}
  \caption{Input coordinates will pass through three Dual-scale blocks with residual connection, then feed through a classification head to obtain shape classification prediction. We split features along channels and pass in through the voxel-based global attention and point-based local convolution branches, respectively. Then, fuse two features with high-frequency module. 
  }
    \label{diagram}
    \vspace{-0.5cm}
\end{figure*}
\paragraph{Dual-path Networks.} Constructing multiple pathways for 2D deep learning has been explored by GoogLeNet~\cite{szegedy2015going}, EfficientNet~\cite{tan2019efficientnet}, and MaX-DeepLab~\cite{wang2021max}, in which different paths with varying feature resolutions and convolutional kernels are expected to extract distinct aspects of features. Recently, DS-Net~\cite{mao2021dual} and LaMa~\cite{suvorov2021resolution} have designed more delicate dual-path architectures for local and global features encoding, which successively separate and fuse the two representations for sufficient cross-scale interactions. For point cloud processing, DTNet~\cite{han2021dual} proposes to apply a multi-head attention mechanism in transformer~\cite{vaswani2017attention} for extracting both inter-channel and inter-point features. PVCNN~\cite{liu2019point} utilizes two modalities to capture point features: raw points and voxels concurrently. Therein, PVCNN encodes each raw point with MLP and each voxel with 3D convolution~\cite{liu2019point} for inter-point relation modeling. Contrary to PVCNN's design, we apply point-wise convolution on neighboring 3D points for local features and global attention on all voxels for global features extraction.

\paragraph{High-frequency Spatial Embedding.} 
Deep neural networks tend to focus more on low-frequency features but neglect the higher frequency counterparts according to ~\cite{mildenhall2020nerf}, and mapping the low dimensional data into higher ones can facilitate networks for better learning abilities.  NeRF~\cite{mildenhall2020nerf} introduces a high-frequency embedding function in a non-parametric manner.  Combining this transformation with learnable MLP could improve the performance since the network can capture slight color and geometry variation in the 3D space. The positional encoding module also utilizes the function in transformer~\cite{vaswani2017attention}, which maps two or three channels' coordinates into higher dimensions.  In DSPoint, we adopt it for encoding 3D coordinates of dual paths and implement cross-modality coordinates interaction.












\section{Method}
\label{sec:method}
In this section, we present our dual-scale network with high-frequency fusion (DSPoint) (Figure\ref{diagram},\ref{fig:hf_fusion}). In Section~\ref{dsnet}, we first briefly review Dual-stream Net~\cite{mao2021dual} for 2D recognition. Then introduce our Dual-scale Blocks in Section~\ref{dsblock}. 
In Section~\ref{fusion}, we show the high-frequency fusion module for better features alignment.

\subsection{Review of Dual-stream Net}
\label{dsnet}
Conventional deep neural networks for 2D recognition utilize single-stream architectures to encode the image, in which shallow layers focus on extracting fine-grained features by local convolutional kernels, and deep layers aim at capturing global representations with a large receptive field. However, local and global features describe the image from two different perspectives: one texture details and the other for long-range shape structure.  Dual-stream Net~\cite{mao2021dual} (DS-Net) proposes to maintain separate local and global visual representations, treat them equally while concurrently exchanging information between them. 

Computationally, DS-Net~\cite{mao2021dual} splits the image feature along the channel dimension into two parts:  and  for dual-pathway processing. In this way, features could be disentangled and computational cost could be optimized. Local features of  remain in the high resolution to preserve the visual details and are encoded by convolution layers,  Global features of   are downsampled to a smaller grid to filter out the low-level noise and extracted by global attention mechanism~\cite{bahdanau2014neural}. We formulate the parallel propagation as 

where  and  are the specific-encoded features of  and . Then,  are upsampled to the original feature resolution and conduct local-global fusion with . For better blending between the two representations, DS-Net further adopts co-attention mechanism for inter-scale features alignment. During implementations of attention, supposing there are  local and  global features,  and  respectively serve as queries and extract informative features from each other by the affinity matrixes,  and , denoted as

where  and  denote the hybrid local and global features after alignment. Finally, the two representations are concatenated together and fused by a linear layer.  In this dual-stream design, DS-Net obtains robust visual representation and achieves high image classification performance.

\subsection{Dual-scale Processing}
\paragraph{Local-global Disentangling.}
In a point cloud, global information mainly contains overall shape properties and inter-component relationships, but local information focuses on subtle spatial geometry and density variations.  Following DS-Net~\cite{mao2021dual}, in every processing stage, we disentangle global and local features along the channel dimension. Specifically, given a -channel point feature, we split it into  with  channels and  with  channels, where  and  weighing the importance between two representations and  . To reserve local details, we maintain the points' spatial density for . As for downsampling , we convert the irregular points into grid-form low-resolution voxels~\cite{liu2019point}. The voxelization averages all point features whose coordinates fall into the voxel grid. Compared to other downsampling methods in point clouds, such as farthest point sampling (FPS)~\cite{qi2017pointnet++}, voxelization is more stable without any randomness and has the reversibility for devoxelization back to points. Also, representations from another modality could capture features from diverse aspects and thus leads to better feature extraction. Therefore, we select voxels for points' global representation. After the disentangling, we concurrently conduct the dual-scale propagation of global-local features encoding.


\subsection{Dual-scale Block}
\label{dsblock}
Demonstrated in Figure \ref{diagram}, our pipeline consists of three consistent Dual-scale Blocks with residuals. Input coordinates will pass through three blocks to obtain a feature representation, then a classification head will make prediction. In each block we split features channel-wise and feed one part through the point-based convolution, and the other part through voxel-based attention branches. In point-based convolution branch, we directly apply existing 3D convolution like PAConv\cite{xu2021paconv}. In voxel based attention branch, we voxelized points according to PVCNN\cite{liu2019point}, encode voxel coordinates using a linear layer and add to voxel features, employ a layer normalization, apply self-attention between all voxels, then devoxlied voxels to restore features.
Two branches has residual connection inside shown in Figure~\ref{diagram}(b). Details of two branches will be presented below. Last, we fuse two processed features with high-frequency module (Figure~\ref{diagram}(c)), introduced in section~\ref{fusion}. Implementation details are listed in section~\ref{sec:experiments}.
\paragraph{Point-scale Local Encoding.} Convolution is natural for local feature extraction because it encodes translation-invariant properties and its limited receptive field makes it easier to compute and learn.  We use the point-specific convolution operations from ~\cite{howard2017mobilenets}. For a point,  with the local receptive field containing  points, the convolutional kernel is dynamically generated by their relative coordinates via a Multi-layer Perceptron (MLP).
We formulate the convolution operation, which transforms  into encoded local feature  as

where  denotes the predicted kernel weight of neighboring point , and  denotes element-wise product. After the point-scale convolution,  at each point location contains local features capturing fine-grained information.

\paragraph{Voxel-scale Global Encoding.}
Attention mechanism~\cite{bahdanau2014neural} operates on the entire visual domain and conducts information interaction over long distances, which is good at summarizing overall structural shape properties. Therefore, we apply a multi-head attention mechanism over the voxel-scale branch for global features exploration. Supposing there are  voxels, We denote the encoded global feature for voxel  as

where  denotes affinity matrix between the voxel  with all other voxels.  Because of the coarser resolution of transformed voxel grids, attention's computation and memory costs are much manageable. In addition, without low-level spatial detail distractions,  can concentrate on long-range part-whole object structure relationships. Afterward, the devoxelization is conducted to project low-density voxels back to the original points, during which the voxel feature is assigned to each point within.


\begin{figure*}[t]
  \centering
\includegraphics[width=0.7\textwidth]{pic/hf_fusion.pdf}
  \caption{High-frequency fusion module. We encode coordinates of one modality with a high-frequency embedding function and inject into features of another branch to help feature alignment.}
    \label{fig:hf_fusion}
    \vspace{-0.5cm}
\end{figure*}

\subsection{Fusion with High-frequency Function}
\label{fusion}
Shown as Figure\ref{fig:hf_fusion}, we have obtained the separately encoded  and  for the point cloud and require effective fusion of the two representations. However, different from 2D images, the misalignment problem in 3D lies mainly in the mismatch of spatial locations, since  is regional homogeneous due to devoxelization process, but  is relatively point-wise heterogeneous. Besides, these modality transformations are implemented through approximation and will cause features to lose their high-frequency information. To alleviate these problems, we proposed High-frequency Fusion Module (Figure \ref{diagram}(c)), injecting coordinates information of another modality through high-frequency fusion to help cross-modality alignment.

We refer to high-frequency functions proposed in NeRF~\cite{mildenhall2020nerf}, which maps a low-frequency point coordinates into higher-dimensional vectors via a set of trigonometric functions:


Here  is a function mapping point coordinates  from  into a higher dimensional space , dimension changes from 3 to 3+3.
By combining this non-parametric transformation with the learnable MLP, the issue of neglecting low-frequency information by the deep network would be largely relieved, such as subtle variations on local geometric and density. 


Specifically, we denote all the coordinates of voxels from the voxel-wise branch as , and all those of points from the point-wise branch as . Respectively, we encode them via high-frequency function  as

where  and  represent the high-frequency encoded voxels'(globle) and points'(local) coordinates, whose channels are the same as  and . Then, we aggregate the voxel-related  with point-wise features , and the point-related  with voxel-wise features , both with simple addition. On top of that, a linear layer is applied for respectively blending and transforming dimension of the above paired voxel/point-coordinate high-frequency features with point/voxel-wise features, formulated as

where  and  denote the hybrid features of the local and global features. This cross-scale communication of high-frequency coordinate information can mitigate the misalignment issue because  brings about homogeneous alignment for local feature  and, while  carries heterogeneous discrimination for . Finally, we concatenate  and  through the channel dimension and apply a linear layer for the final fusion, which restore point features into the original  channels. After this, the dual-scale branches are combined into one, and the local-global features of point clouds can be well extracted for better 3D understanding.

\begin{table}[tb]
\setlength{\tabcolsep}{2.5mm}{
\centering
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{lcccccc}
	\toprule
Method & & Input  &  & Accuracy \\ \midrule
		\multicolumn{5}{c}{Local Feature} \\  \cmidrule(lr){1-5}
	    PointNet\cite{qi2017pointnet} & &xyz & &89.2\\
		PointNet++\cite{qi2017pointnet++} & &xyz & &90.7\\
		DGCNN\cite{wang2019dynamic} & &xyz & &92.9 \\
	    KPConv\cite{thomas2019kpconv} & &xyz & &92.9 \\ 
	    FPConv\cite{lin2020fpconv} & &xyz & &92.5\\
	    PAConv (*PN)\cite{xu2021paconv} & &xyz & &93.2 (92.5)\\
	    PAConv (*DGCNN)\cite{xu2021paconv} & &xyz & &93.6 (93.4)\\
	    \midrule
	    \multicolumn{5}{c}{Global Feature} \\  \cmidrule(lr){1-5}
	    PCT\cite{guo2021pct} & &xyz & &93.2 (92.8)\\
 		PT\cite{zhao2021point} & &xyz+nor & &\textbf{93.7} \\
		\midrule
	    \multicolumn{5}{c}{Global-Local Feature} \\  \cmidrule(lr){1-5}
	    PointASNL~\cite{yan2020pointasnl} & &xyz+nor & & 93.2 \\
\textbf{Ours} & &xyz & &93.5 \\
	\bottomrule
	\end{tabular}
	
\end{adjustbox}
\caption{Results of Object Classification on ModelNet40\cite{wu20153d}. We only train one model instead of using multiple models ensemble. ("nor" indicates using extra normal vector information as input, *PN denotes using PointNet as the backbone, Results in brackets are our re-implementation results)}
\vspace*{-3pt}
\label{table:cls}}
\end{table}

\section{Experiments}
\label{sec:experiments}
\subsection{Shape Classification}
\label{sec:shape cls}
We evaluate on ModelNet40~\cite{wu20153d} dataset for object classification. This dataset contains 40 categories of 12,311 meshed CAD models, 9,843 of them are used for training and the rest 2,468 for testing. We follow the same data preprocessing in PointNet~\cite{qi2017pointnet}: for each model, we sample the first 1,024 points  and apply dropout points, random translation and shuffling all points.  We only 
employ coordinates information as input, without extra use of normal vectors.



\begin{table}[t]
\setlength{\tabcolsep}{2.2mm}{
\centering
\begin{adjustbox}{width=0.9\linewidth}
\begin{tabular}{lcc}
\toprule
    & Point + Global   & Voxel + Gobal   \\ \midrule
Point + Local & 93.2  & \bf{93.5} \\
Voxel + Local & 93.0 & 93.1 \\ \bottomrule
\end{tabular}
\end{adjustbox}
\caption{Different modalities for dual-scale processing. (Point / Voxel: point / voxel-based representation. Local / Global: local / global feature processing.)}
\label{ablation_modality}}
\vspace{-8pt}
\end{table}


\begin{table}[t]
\setlength{\tabcolsep}{1mm}{
\centering
\begin{adjustbox}{width=0.65\linewidth}
\begin{tabular}{lc}
\toprule
Local Operator  & Accuracy \\ \midrule
DSPoint w. MLP          & 91.2       \\
DSPoint w. MLP + SG       & 92.4       \\
DSPoint w. KPConv\cite{thomas2019kpconv}       & 93.2       \\
DSPoint w. PointConv\cite{wu2019pointconv}    & 92.8       \\
DSPoint w. PAConv\cite{xu2021paconv} & \bf{93.5}       \\ \bottomrule
\end{tabular}
\end{adjustbox}
\caption{Ablation of local operator. We use different local feature operator in our local branch and evaluate its performance. MLP stands for shared MLP from PointNet\cite{qi2017pointnet}. SG stands for sample and grouping from PCT\cite{guo2021pct}.}
\label{ablation_operator}}
\vspace*{-15pt}
\end{table}
\vspace*{-6pt}
\subsubsection{Experiment Setting}
\paragraph{Model Architecture.}
Shown in Figure \ref{diagram}(a), our DSPoint consists of 3 consecutive Dual-scale Blocks with skip-connection, and feeds features into a classfication head to obtain prediction. The channel dimension for each block is 64, 64, 128, respectively. Voxelization resolution for each block is 8, 6, 4, respectively. We incorporate PAConv\cite{xu2021paconv} as our local feature extraction operator, with 8 nearest neighbors and 8 weight matrices. The channel ratio between the local and global branches is 3:1, and  of high-frequency encoding is 10. The classification head has a Linear layer to project embedding dimension from 128 to 1024, followed by a max pooling layer to aggregate all points, then two Linear layers will project features' embedding from 1024 to 512, and 512 to 40, which is the number of classes. Then we apply a softmax to obtain classification score. Linear layers are all connected with batch normalization and ReLU activation function.
To be environmental-friendly, we do not train massive amounts of models then use their ensembled score, but  train only one model. 

\paragraph{Training Setting.}
We use Adam optimizer, and train our model for 250 epochs and preserve the model with the best evaluation accuracy during training. During training, we set the batch size to 32, learning rate to 0.001, weight decay to 1e-4, and reduce learning rate when a metric has stopped improving at the factor of 0.5, the patience of 10, and a minimum learning rate of 0.00001.

\subsubsection{Performance}
The result of classification experiments on ModelNet40 is shown in Table \ref{table:cls}. We list previous works based on the feature representation they are using. The local feature means they only process local features around each point during inference, such as PointNet\cite{qi2017pointnet} or KPConv\cite{thomas2019kpconv}. Global feature process points globally and builds long-range dependency using attention mechanisms, such as PCT\cite{guo2021pct}. There also exist other methods that process local and global features simultaneously, such as PointASNL\cite{yan2020pointasnl}. Results show that our method outperforms or is comparable 
with all previous methods.


\begin{table}[t!]
\setlength{\tabcolsep}{1mm}{
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{lccc}
\toprule
            & Global + Front & Global + Back & Global + None \\ \midrule
Local + Front &93.2              &93.4             &93.0             \\
Local + Back  &93.3              &\bf{93.5}             &93.1             \\
Local + None  &92.9               &93.2             &92.7             \\ \bottomrule
\end{tabular}
\end{adjustbox}
\caption{Ablation of High-Frequency Fusion. (Local / Global: local or global branch. Front / Back: put High-Frequency Fusion module before/after the feature processing. None: don't use High-Frequency Fusion module.) }
\vspace*{-10pt}
\label{ablation_hffusion}}
\end{table}



\begin{table}[t]
\setlength{\tabcolsep}{6mm}{
\centering
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{lcc}
	\toprule
Method  &   Param. & Latency   \\ \midrule
	    PointNet\cite{qi2017pointnet} &3.47M &\textbf{13.6} \\
		PointNet++\cite{qi2017pointnet++} &1.74M &35.3  \\
		DGCNN\cite{wang2019dynamic} &1.81M &85.8   \\
	    KPConv\cite{thomas2019kpconv} &- &120.5   \\ 
	    FPConv\cite{lin2020fpconv} &- &-  \\
	    PAConv (*PN)\cite{xu2021paconv} &- &-  \\
	    PCT\cite{guo2021pct} &2.88M &92.4  \\
 		PT 2021\cite{zhao2021point} &- &530.2   \\
	    PointASNL~\cite{yan2020pointasnl} &- &923.6    \\
\midrule
	    \textbf{Ours}  &\textbf{1.16M} &214.5   \\
	\bottomrule
	\end{tabular}
\end{adjustbox}
\caption{Efficiency evaluation measured on ModelNet40}
\vspace*{-15pt}
\label{efficiency}}
\end{table}



\begin{figure*}[t]
  \centering
    \includegraphics[width=0.99\textwidth]{pic/seg_vis.png}
  \caption{Visualization Results of ShapeNet~\cite{wu20153d}. It demonstrates our compared baseline PAConv\cite{xu2021paconv} (first row), our method DSPoint (second row), and ground truth, which indicates our excellent performance of spatial continuity on part segmentation.}
    \label{vis}
    \vspace{-0.3cm}
\end{figure*}

\begin{table*}[t]
\vspace{0.3cm}
\centering
\begin{adjustbox}{width=\textwidth}
	\begin{tabular}{l|cc|cccccccccccccccc}
	\toprule
		Method   &Cls. mIoU &Ins. mIoU & airplane &bag &cap &car &chair &earphone &guitar &knife &lamp &laptop &motorbike &mug &pistol &rocket &stakeboard &table \\ \midrule
		\multicolumn{19}{c}{Local Feature} \\ \midrule PointNet\cite{qi2017pointnet}  &80.4	&83.7 &83.4 &78.7 &82.5 &74.9 &89.6 &73.0 &91.5 &85.9 &80.8 &95.3 &65.2 &93.0 &81.2 &57.9 &72.8 &80.6\\
	    SO-Net\cite{li2018so} &- &84.6 &81.9 &83.5 &84.8 &78.1 &90.8 &72.2 &90.1 &83.6 &82.3 &95.2 &69.3 &94.2 &80.0 &51.6 &72.1 &82.6\\
		PointNet++\cite{qi2017pointnet++}  &81.9	&85.1 &82.4 &79.0 &87.7 &77.3 &90.8 &71.8 &91.0 &85.9 &83.7 &95.3 &71.6 &94.1 &81.3 &58.7 &76.4 &82.6\\
		DGCNN\cite{wang2019dynamic} &82.3 &85.2 &84.0 &83.4 &86.7 &77.8 &90.6 &74.7 &91.2 &87.5 &82.8 &95.7 &66.3 &94.9 &81.1 &63.5 &74.5 &82.6 \\
		P2Sequence\cite{liu2019point2sequence} &- &85.2 &82.6 &81.8 &87.5 &77.3 &90.8 &77.1 &91.1 &86.9 &83.9 &95.7 &70.8 &94.6 &79.3 &58.1 &75.2 &82.8\\
	    PAConv\cite{xu2021paconv} &\textbf{84.2} (83.8) &86.0 (85.8) &(83.9)&(\textbf{87.4}) &(88.5) &(79.0) &(90.4) &(77.1) &(\textbf{91.9}) &(87.8) &(81.6) &(95.9) &(73.0) &(94.7) &(84.1) &(59.9) &(\textbf{81.8}) &(83.8) \\
	    
	    \midrule
	    \multicolumn{19}{c}{Global Feature} \\  \midrule PCT\cite{guo2021pct} &- &86.4 &\textbf{85.0} &82.4 &\textbf{89.0} &\textbf{81.2} &\textbf{91.9} &71.5 &91.3 &88.1 &\textbf{86.3} &95.8 &64.6 &95.8 &83.6 &62.2 &77.6 &73.7\\
 		PT\cite{zhao2021point} &83.7 &\textbf{86.6} &- &- &- &- &- &- &- &- &- &- &- &- &- &- &- &- \\
		\midrule
	    \multicolumn{19}{c}{Global-Local Feature} \\ \midrule RS-CNN\cite{liu2019relation} &84 &86.2 &83.5 &84.8 &88.8 &79.6 &91.2 &\textbf{81.1} &91.6 &\textbf{88.4} &86.0 &\textbf{96.0} &\textbf{73.7} &94.1 &83.4 &60.5 &77.7 &83.6\\
\textbf{Ours}  &83.9   &85.8 &84.1 &84.6 &88.2 &79.2 &90.3 &\textbf{77.9} &91.7 &88.1 &81.6 &95.9 &72.6 &\textbf{94.9} &\textbf{84.4} &\textbf{64.4} &80.8 &\textbf{83.9}\\
	\bottomrule
	\end{tabular}
\end{adjustbox}
\caption{Results of Shape Part Segmentation on ShapeNet Parts\cite{wu20143d}, evaluating mean class and instance IoU, and IoU within each class. We only train one model instead of using multiple models ensemble. (Result in brackets: the re-implementation result by us.) }
\vspace*{0pt}
\label{shapenet}
\vspace{-0.2cm}
\end{table*}


\subsection{Ablation Study}

To quantify the effectiveness of DSPoint, we conduct ablation studies on ModelNet40\cite{wu20153d}, following the same experiment setting of Shape Classification mentioned above.

\paragraph{Dual-scale Modality}

Our DSPoint incorporates point-based representation to process local information and utilize voxel-based representation to handle long-range global dependency.  We claim that local voxel-based representation pools neighborhood information together in low resolution, losing subtle features important for local information processing.  Processing local features with point-based representation requires no pooling or grouping process, and could preserve nuanced differences among all points as much as possible, benefiting local learning.  

Furthermore, global point-based representation needs to process a continuous infinite coordinate space, whose position embedding is complex for a network to learn during attention mechanism.  Our global processing using voxel-based representation aligns all points with mesh grids and has a discrete finite coordinate space which is easy for position embedding.

To verify our claims, we run ablation studies demonstrated in Table \ref{ablation_modality}. In point-based global processing, we use sample and grouping\cite{guo2021pct} to sample 256 points and do self-attention, and use a single Linear layer to restore point number from 256 to 1024. In voxel-based local processing, we use the same 3D voxel convolution in PVCNN\cite{liu2019point}. Results show that our modality choice with point-based local processing and voxel-based global processing has the best performance among all four combinations.












\paragraph{Local Operator}
While efficient global feature extraction has the only option of using an attention mechanism, local feature extraction has many comparable operators.  In Table~\ref{ablation_operator}, we substitute our local branch point-based convolution with a different local feature operator and evaluate their performance. It shows that with PAConv\cite{xu2021paconv} consisting the local branch operator, our method has the best performance among all evaluated local operators.





\paragraph{High-Frequency Fusion}
We dive into the utility of High-Frequency Fusion module. We examine the influence of usage (whether use it or not) and location (before or after feature processing) of the High-Frequency Fusion module. The result are shown in Table~\ref{ablation_hffusion}, and we find that putting High-Frequency Fusion module after feature processing for both local and global branch will achieve the best performance.  It shows that our high-frequency fusion module incorporates coordinates and narrows the gap between two modalities after dual processing to benefit learning.









\begin{figure*}[t]
  \centering
    \includegraphics[width=\textwidth]{pic/scene_vis.png}
  \caption{Visualization of Indoor Scene Segmentation on S3DIS\cite{armeni20163d} Dataset. We project scenes onto a plane and visualize them in low-resolution to benefit comparison.  Global attention on the 3D grid incorporates information from non-adjacent parts and helps detect spatially isolated points while maintaining local label consistency within the object parts. 
}
    \label{fig:scene_vis}
    \vspace{-0.0cm}
\end{figure*}
\begin{table*}[t]
\centering
\begin{adjustbox}{width=\textwidth}
	\begin{tabular}{l|cc|ccccccccccccc}
	\toprule
	   Method &mAcc &mIoU &ceiling &floor &wall &beam &column &window &door &chair &table &bookcase &sofa &board &clutter\\ \midrule
		\multicolumn{16}{c}{Local Feature} \\  \midrule PointNet\cite{qi2017pointnet} &49.0 &41.1 &88.8 &97.3 &69.8 &\textbf{0.1} &3.9 &46.2 &10.8 &58.9 &52.6 &5.9 &40.3 &26.4 &33.2\\
        PointNet++\cite{qi2017pointnet++} &- &50.0 &90.8 &96.5 &74.1 &0.0 &5.8 &43.6 &25.4 &69.2 &76.9 &21.5 &55.6 &49.3 &41.9\\
		DGCNN\cite{wang2019dynamic} &\textbf{84.1} &56.1 &- &- &- &- &- &- &- &- &- &- &- &- &-\\
		KPConv\cite{thomas2019kpconv} &72.8 &67.1 &92.8 &97.3 &82.4 &0.0 &23.9 &58.0 &69.0 &\textbf{91.0} &81.5 &75.3 &\textbf{75.4} &66.7 &58.9\\
		FPConv\cite{lin2020fpconv} &68.9 &62.8 &\textbf{94.6} &98.5 &80.9 &0.0 &19.1 &60.1 &48.9 &88.0 &80.6 &68.4 &53.2  &68.2 &54.9\\
	    PointWeb\cite{zhao2019pointweb} &66.6 &60.3 &92.0 &\textbf{98.5} &79.4 &0.0 &21.1 &59.7 &34.8 &88.3 &76.3  &69.3 &46.9  &64.9 &52.5\\
	    PAConv\cite{xu2021paconv}&(69.6) &66.0 (62.2) &(94.3)&(97.7)&(79.8)&(0.0)&(16.5)&(51.1)&(63.6)&(76.3)&(85.2)&(58.3)&(66.5)&(59.0)&(\textbf{60.5}) \\
	    \midrule
	    \multicolumn{16}{c}{Global Feature} \\  \midrule PCT\cite{guo2021pct} &67.7 &61.3 &92.5 &98.4 &80.6 &0.0 &19.4 &61.6 &48.0 &76.6 &85.2 &46.2 &67.7 &67.9 &52.3\\
	    PT\cite{zhao2021point} &76.5 &\textbf{70.4} &94.0 &98.5 &\textbf{86.3} &0.0 &\textbf{38.0} &\textbf{63.4} &\textbf{74.3} &82.4 &\textbf{89.1} &\textbf{80.2} &74.3 &\textbf{76.0} &59.3\\
		\midrule
	    \multicolumn{16}{c}{Global-Local Feature} \\ \midrule \textbf{Ours}  &70.9 &63.3 &94.2 &98.1 &82.4 &0.0 &19.1 &49.9 &66.2 &78.2 &85.6 &59.0 &67.9 &62.3 &59.9 \\
	\bottomrule
	\end{tabular}
\end{adjustbox}
\caption{Results of Indoor Scene Segmentation on S3DIS\cite{armeni20163d} tested on Area 5. Evaluate mean accuracy, mean IoU, and IoU within each class. We only train one model instead of using multiple models ensemble.(Result in brackets: the re-implementation result by us.
:CUDA implementation)}
\vspace*{-0.5cm}
\label{s3dis}
\end{table*}


\subsection{Efficiency}
We claim that our model is light-weighted and computation-efficient. It utilizes point-wise convolution as the local feature extractor, which requires less parameters compared with transformer-based methods such as PCT\cite{guo2021pct}.  At the same time, the dual-processing makes our latency more efficient compared with other local-global methods like PointASNL\cite{yan2020pointasnl}. The comparison of parameter amount and latency are listed in Table~\ref{efficiency}, where all parameters and latency are measured on a single NVIDIA 2080Ti GPU.







\subsection{Down-stream Task}
To demonstrate the general applicability and plug-in simplicity of our method, we incorporate it into other baselines then apply to two different downstream tasks: Shape Part Segmentation and Indoor Scene Segmentation. It could be noticed that our method achieved a great trade-off by improving its efficiency by a lot margin mentioned in Table\ref{efficiency}, with a slight cost of accuracy. Such trade-off would be more worthy in the industrial field like self-driving which requires high inference speed and light model parameter amount.
\paragraph{Shape Part Segmentation.}
We evaluate our model on ShapeNet Parts\cite{wu20143d} benchmark. It comprises 16,881 shapes (14,006 for training and 2,874 for testing) with 16 categories labeled in 50 parts. For each shape, we sample 2,048 points. We incorporate our methods to the last three layers of DGCNN\cite{wang2019dynamic} with PAConv\cite{xu2021paconv} as local operator. We use channel-wise accumulation instead of channel-wise splitting for plug-in simplicity, where weight between local and global branches is 4:1. 

Results are listed in Table~\ref{shapenet}. Although our mIoU increase compared to PAConv\cite{xu2021paconv} is small, Figure ~\ref{vis} shows clear benefits from our voxel modality, which prevents points from being fragmented into many parts.  Our part segmentation is far more spatially continuous in comparison.  The mIoU measurement does not reflect the fragmentation problem in PAConv\cite{xu2021paconv}.  In many practical applications, having a spatially coherent output, as in our method, is far more important than fragmented results. It proves our strong performance by maintaining plug-in simplicity and practical utility.














\paragraph{Indoor Scene Segmentation}
We experiment on S3DIS\cite{armeni20163d} dataset, containing 272 rooms out of six areas. For a fair comparison, we use Area-5 as the test set.  Each point is labelled from 13 classes, like doors or walls. For each 1m  1m block, we sample 4096 points. We integrate our method into all four layers of encoders of PointNet++\cite{qi2017pointnet++}, with PAConv\cite{xu2021paconv} as local operator, then use channel-wise summation instead of channel-wise dividing for plug-in succinctness, where weight between local and global branches is 4:1.  The experiment results are shown in Table~\ref{s3dis}, and visualized in Figure ~\ref{fig:scene_vis}, demonstrating our excellent performance, while benefiting from long-range feature integrating in recognizing isolated parts.
















\begin{figure}[t]
  \centering
\includegraphics[width=0.45\textwidth]{pic/aleatoric.png}
  \caption{Aleatoric uncertainty exhibited in test dataset.}
    \label{fig:aleatoric}
    \vspace{-0.5cm}
\end{figure}

\begin{figure}[t]
	\centering
  \includegraphics[width=0.45\textwidth]{pic/projection.png}
	\caption{Misclassification which might be solved by processing 2D projection and 3D point cloud simultaneously.}
	  \label{fig:projection}
	  		  \vspace{-1cm}
  \end{figure}
  
\section{Error Mode Analysis}


Aleatoric uncertainty measures the uncertainty that how likely one sample would be misclassified as another class. Those data near the decision boundary would have high aleatoric uncertainty. As shown in Figure \ref{fig:aleatoric}, selected misclassified samples pair are similar to the other class hence are misclassified into each other's class. This misclassification is caused by data distribution itself which couldn't even be told by human. Thus we could not improve our performance on those data by improving our model design. By our rough estimation, it limits the upper bound of classification accuracy of this dataset to around 94\%-95\%. Under such circumstances, it would be more worthy to improve the model's efficiency by a lot margin instead of improving its accuracy slightly, which aligns with our model's superiority.

Besides, it's worth noticing that even though some test samples have significant features, they are still misclassified. As shown in Figure \ref{fig:projection}, the first sample is a cup that has a handle, like some other cups in the dataset, while all vases in the dataset have no handles. Even so, this cup is misclassified as a vase. One possible explainatino could be that its small volume of handle leads to the insignificance of the feature response, while its body resembles a vase. Similarly, the bench which has a back is mistaken as a nightstand, which has no back. It might be due to the bench having a carved back which is similar to the table-board of the nightstand. In the future, to better handle such a circumstance, we could project the point cloud into the 2D plane, which limits the z-axis variance and amplify the significance of the handle as well as the bench back. By processing 2D projection and 3D point cloud simultaneously, we might achieve better performance in this task. 



\section{Conclusion}
\label{sec:conclusion}
We propose \textbf{D}ual-\textbf{S}cale Point Cloud Recognition with High-Frequency Fusion (\textbf{DSPoint}) to conduct dual scales and representations 3D learning. Elaborate experiments have demonstrated the effectivity of our DSPoint, guiding a possible future direction.



\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\end{document}
