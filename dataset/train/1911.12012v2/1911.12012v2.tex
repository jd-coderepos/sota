\label{sec:method}
\comment{All previous works \cite{huang2018deepmvs,yao2018mvsnet,im2018dpsnet} apply deep CNNs on one single cost volume; 
in such volumes, the number of sweeping planes expresses the depth resolution and determines the precision of depth estimation, 
which is, however, strictly limited by system memory resources.
We propose a novel coarse-to-fine deep stereo approach; 
we leverage multiple small volumes that progressively sub-partitions local space around the actual depth at a finer scale, and achieve highly accurate depth reconstruction.
While each individual volume merely consists of a small number (16 or 32) of sweeping planes, the overall depth resolution increases exponentially in the process, and thus allows higher accuracy as well as less 
computation-  and memory- efficiency complexity.
}
 
Some recent works aim to improve learning-based MVS methods. 
Recurrent networks \cite{yao2019recurrent} have been utilized to achieve fine depth-wise partitioning for high accuracy; 
a PointNet-based method \cite{chen2019point} is also presented to densify the reconstruction for high completeness.
Our goal is to reconstruct high-quality 3D geometry with both high accuracy and high completeness.
To this end, we propose a novel uncertainty-aware cascaded network (UCS-Net) to reconstruct highly accurate per-view depth with high resolution. 

Given a reference image  and  source images , 
our UCS-Net progressively regresses a fine-grained depth map at the same resolution as the reference image.
We show the architecture of the UCS-Net in Fig. \ref{fig:ucnet}.
Our UCS-Net first leverages a 2D CNN to extract multi-scale deep image features at three resolutions (Sec.~\ref{sec:feature}).
Our depth prediction is achieved through three stages, 
which leverage multi-scale image features to predict multi-resolution depth maps.
In these stages, we construct multi-scale cost volumes (Sec.~\ref{sec:volume}), where each volume is a plane sweep volume or an adaptive thin volume (ATV).
We then apply 3D CNNs to process the cost volumes to predict per-pixel depth probability distributions, 
and a depth map is reconstructed from the expectations of the distributions (Sec.~\ref{sec:depth}).
To achieve efficient spatial partitioning, 
we utilize the uncertainty of the depth prediction to construct ATVs as cost volumes for the last two stages (Sec.~\ref{sec:uncertainty}).
Our multi-stage network effectively reconstructs depth in a coarse-to-fine fashion (Sec.~\ref{sec:coarse2fine}).




\subsection{Multi-scale feature extractor}
\label{sec:feature}
Previous methods use downsampling layers \cite{yao2018mvsnet,yao2019recurrent} or a UNet \cite{xu2019deep} to extract deep features 
and build a plane sweep volume at a single resolution.
To reconstruct high-resolution depth, we introduce a multi-scale feature extractor, 
which enables constructing multiple cost volumes at different scales for multi-resolution depth prediction.
As schematically shown in Fig.~\ref{fig:ucnet}, 
our feature extractor is a small 2D UNet \cite{ronneberger2015u}, which has an encoder and a decoder with skip connections.
The encoder consists of a set of convolutional layers followed by BN (batch normalization) and ReLu activation layers; 
we use stride = 2 convolutions to downsample the original image size twice.
The decoder upsamples the feature maps, convolves the upsampled features and the concatenated features from skip links,
 and also applies BN and Relu layers.
Given each input image , the feature extractor provides three scale feature maps, , , , from the decoder for the following cost volume construction.
We represent the original image size as , where  and  denote the image width and height;
correspondingly, ,  and  have resolutions of ,  and , and their numbers of channels are 32, 16 and 8 respectively.
Please refer to Tab.~\ref{fig:2d_unet} in the appendix for the details of our 2D CNN architecture.
Our multi-scale feature extractor allows for the high-resolution features to properly incorporate the information at lower resolutions through the learned upsampling process;
thus in the multi-stage depth prediction, each stage is aware of the meaningful feature knowledge used in previous stages, 
which leads to reasonable high-frequency feature extraction.


\subsection{Cost volume construction}
\label{sec:volume}
We construct multiple cost volumes at multiple scales by warping the extracted feature maps, , ,  from source views to a reference view.
Similar to previous work, this process is achieved through differentiable unprojection and projection. 
In particular, given camera intrinsic and extrinsic matrices  for each view , 
the  warping matrix at depth  at the reference view is given by:

In particular, when warping to a pixel in the reference image  at location  and depth , 
 multiplies the homogeneous vector  to finds its corresponding pixel location in each  in homogeneous coordinates.

Each cost volume consists of multiple planes; 
we use  to denote the depth hypothesis of the th plane at the th stage, 
and  represents its value at pixel .
At stage , 
once we warp per-view feature maps  at all depth planes with corresponding hypotheses , 
we calculate the variance of the warped feature maps across views at each plane to construct a cost volume.
We use  to represent the number of planes for stage .
For the first stage, we build a standard plane sweep volume, 
whose depth hypotheses are of constant values, i.e. .
We uniformly sample  from a pre-defined depth interval  to 
construct the volume, in which each plane is constructed using  to warp multi-view images.
For the second and third stages, we build novel adaptive thin volumes, 
whose depth hypotheses have spatially-varying depth values according to pixel-wise uncertainty estimates of the previous depth prediction.
In this case, we calculate per-pixel per-plane warping matrices by setting  in Eqn.~\ref{eqn:warp} to warp images and construct cost volumes.
Please refer to Sec.~\ref{sec:uncertainty} for uncertainty estimation.

\subsection{Depth prediction and probability distribution}
\label{sec:depth}
At each stage, we apply a 3D CNN to process the cost volume, infer multi-view correspondence and predict depth probability distributions.
In particular, we use a 3D UNet similar to \cite{yao2018mvsnet}, 
which has multiple downsampling and upsampling 3D convolutional layers
to reason about scene geometry at multiple scales.
We apply depth-wise softmax at the end of the 3D CNNs to predict per-pixel depth probabilities.
Our three stages use the same network architecture without sharing weights, 
so that each stage learns to process its information at a different scale.
Please refer to Tab.~\ref{fig:3d_unet} in the appendix for the details of our 3D CNN architecture.

The 3D CNN at each stage predicts a depth probability volume 
that consists of  depth probability maps  associated with the depth hypotheses .
 expresses per-pixel depth probability distributions, 
where  represents how probable the depth at pixel  is . 
A depth map  at stage  is reconstructed by weighted sum:



\subsection{Uncertainty estimation and ATV}
\label{sec:uncertainty}
The key for our framework is to progressively sub-partition the local space and refine the depth prediction with increasing resolution and accuracy.
To do so, we construct novel ATVs for the last two stages, which have curved sweeping planes with spatially-varying depth hypotheses 
(as illustrated in Fig.~\ref{fig:teaser} and Fig.~\ref{fig:ucnet}),
based on uncertainty inference of the predicted depth in its previous stage.

\comment{
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/iccv_method.pdf}
    \caption{At a cascade stage, we predict a depth map (middle) from input RGB images (left), and infer the uncertainty of the prediction, expressed by a confidence interval. 
    On the right, we show the predicted depth probabilities (connected blue dots) of a pixel (green point), depth prediction (red dash line), the ground truth depth (blue dash line) and confidence intervals of ,  and .}
    \label{fig:method}
    \vspace{-3mm}
\end{figure}
}

Given a set of depth probability maps, 
previous work only utilizes the expectation of the per-pixel distributions 
(using Eqn.~\eqref{eqn:expect}) to determine an estimated depth map.
For the first time, we leverage the variance of the distribution for uncertainty estimation, 
and construct ATVs using the uncertainty.
In particular, the variance  of the probability distribution at pixel  and stage  is calculated as:

and the corresponding standard deviation is .
Given the depth prediction  and its variance  at pixel , 
we propose to use a variance-based confidence interval to measure the uncertainty of the prediction:

where  is a scalar parameter that determines how large the confidence interval is.
For each pixel , we uniformly sample  depth values from  of the th stage, 
to get its depth values , ,..., of the depth planes for stage .
In this way, we construct  spatially-varying depth hypotheses , which 
form the ATV for stage .

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/uncertainty_new.pdf}
	\caption{ 
	We illustrate detailed depth and uncertainty estimation of two examples.
	On the top, we show the RGB image crops, predicted depth and ground truth depth.
	On the bottom, 
	we show the details of of two pixels (red points in the images) with predicted depth probabilities (connected blue dots) , 
	depth prediction (red dash line), the ground truth depth (black dash line) 
	and uncertainty intervals (purple) in the three stages.}
    \label{fig:method}
\end{figure}

The estimated  expresses the uncertainty interval of the prediction ,
which determines the physical thickness of an ATV at each pixel.
In Fig.~\ref{fig:method}, 
we show two actual examples with two pixels and their estimated uncertainty intervals  around the predictions (red dash line).
The  essentially depicts a probabilistic local space around the ground truth surface, 
and the ground truth depth is located in the uncertainty interval with a very high confidence. Note that, our variance-based uncertainty estimation is differentiable, 
which enables our UCS-Net to learn to adjust the probability prediction at each stage to 
achieve optimized intervals and corresponding ATVs for following stages in an end-to-end training process.
As a result, the spatially varying depth hypotheses in ATVs naturally adapt to the uncertainty of depth predictions, 
which leads to highly efficient spatial partitioning.



\subsection{Coarse-to-fine prediction}
\label{sec:coarse2fine}
Our UCS-Net leverages three stages to reconstruct depth at multiple scales from coarse to fine, which generally supports different numbers () of planes in each stage.
In practice, we use ,  and  to construct a plane sweep volume and two ATVs 
with sizes of ,  and  to estimate depth at corresponding resolutions.
While our two ATVs have small numbers ( and ) of depth planes,
they in fact partition local depth ranges at finer scales than the first stage volume;
this is achieved by our novel uncertainty-aware volume construction process which adaptively controls local depth intervals.
This efficient usage of a small number of depth planes enables the last two stages to deal with higher pixel-wise resolutions given the limited memory,
which makes fine-grained depth reconstruction possible.
Our novel ATV effectively expresses the locality and uncertainty in the depth prediction, 
which enables state-of-the-art depth reconstruction results with high accuracy and high completeness through a coarse-to-fine framework.


\subsection{Training details}
\label{sec:details}
\noindent\textbf{Training set.} 
We train our network on the DTU dataset~\cite{aanaes2016large}. 
We split the dataset into training, validate and testing set, and create ground truth depth similar to \cite{yao2018mvsnet}.
In particular, we apply Poisson reconstruction \cite{kazhdan2013screened} on the point clouds in DTU, and render the surface at the captured views with three
resolutions, ,  and the original .
In particular, we use  for training.

\noindent\textbf{Loss function.}
Our UCS-Net predicts depth at three resolutions; 
we apply  loss on depth prediction at each resolution with the rendered ground truth at the same resolution.
Our final loss is the combination of the three  losses.

\noindent\textbf{Training policy.}
We train our full three-stage network from end to end for 60 epochs.
We use Adam optimizer with an initial learning rate of .
We use 8 NVIDIA GTX 1080Ti GPUs to train the network with a batch size of 16 (mini-batch size of 2 per GPU).





\comment{



\subsection{Depth Prediction from Dynamic Disparities}
\label{sec:atvc}
Our plane sweep volume is novel, and is the key to achieve efficient coarse-to-fine prediction.
At each stage (except the first one), we construct an ATV with uncertainty awareness. 
Each plane of an ATV has dynamic disparities varying across pixels.
While our volume is novel and very different from a standard plane sweep volume, a depth map can be estimated from our volume similar to the standard way.
In this subsection, We discuss the disparity and depth prediction from a general volume (that can be either a standard plane seep volume or a ATV). 
We leave the discussion of the uncertainty-aware volume construction in the next subsection (Sec.~\ref{sec:uacstrct}), which relies on the disparity prediction from the previous stage.

\paragraph{Dynamic Disparities.} 
Similar to all cost-volume-based methods, our UnitNet involves constructing a plane sweep volume with  planes from  disparities.
This is achieved in a differentiable homography-based warping process using spatial transformer.
Traditionally, each plane of a swept volume is constructed from the same disparity (as shown in Fig.~\ref{fig:ucnet} a). 
In fact, this warping process can also be achieved using a volume with dynamic disparities that vary across pixels, like our ATV (as illustrated in Fig~\ref{fig:ucnet} c and e).
For the first time, we leverage the uncertainty in disparity prediction at one stage to guide the construction of a novel adaptive thin volume using spatially-varying disparities for the next stage.

In our uncertainty-aware cascaded network, individual stages can have different number of sweeping planes with different spatially-varying disparities.
In general, for the th cascade stage, we use  disparity maps  (,..., ) to construct a plane sweep volume.

\paragraph{Depth Prediction.} The network of the th stage, UnitNet, warps extracted features onto  sweeping planes and predicts  disparity probability maps  correspondingly.
Similar to \cite{yao2018mvsnet,im2018dpsnet}, we regress a disparity map  at the th stage by weighted sum:

where  represents an arbitrary pixel position in the reference view.
Correspondingly, the depth estimation  from stage  is given by:

where  is the minimum depth value we consider, which scales our maximum disparity value to .
We set  and , which follow the settings in \cite{im2018dpsnet}.

We directly supervise  with ground truth depth for each individual stage, to ensure that every stage effectively leverages the supervision and reasons about scene geometry at its own scale. 
Similar to , we also predict
a raw depth map  from the raw disparity probability maps , and provide direct supervision for  with the ground truth to let the UnitNet better regularize the cost volume.


\subsection{Uncertainty-aware Construction of Thin Volumes}
\label{sec:uacstrct}
\comment{\subsection{Adaptive thin volume construction}}

We use a standard plane sweep volume (shown in Fig.~\ref{fig:ucnet} a) for the first stage with a constant disparity  for each plane, i.e. .
In general, we uniformly sample  disparities  from range , with .
The corresponding depths  can be calculated using Eq.~\eqref{eqn:depthDisp} (by replacing  and  with  and  respectively).

The first stage predicts initial disparities and corresponding depths that are roughly around
the ground truth surface.
We use all following stages to progressively sub-partition the local space and refine the depth prediction.
For each following stage, we construct a novel ATV, which is a curved plane sweep volume with dynamic thickness 
(as illustrated in Fig.~\ref{fig:teaser} and Fig.~\ref{fig:ucnet} c, e), 
based on uncertainty inference from the predicted disparities in its previous stage.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/uncertainty.pdf}
    \caption{At a cascade stage, we predict a depth map (middle) from input RGB images (left), and infer the uncertainty of the prediction, expressed by a confidence interval. 
    On the right, we show the predicted disparity probabilities (connected blue dots) of a pixel (green point), disparity prediction (red dash line), the ground truth disparity (blue dash line) and confidence intervals of ,  and .}
    \label{fig:method}
\end{figure}

\comment{
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/iccv_method.pdf}
    \caption{At a cascade stage, we predict a depth map (middle) from input RGB images (left), and infer the uncertainty of the prediction, expressed by a confidence interval. 
    On the right, we show the predicted disparity probabilities (connected blue dots) of a pixel (green point), disparity prediction (red dash line), the ground truth disparity (blue dash line) and confidence intervals of ,  and .}
    \label{fig:method}
    \vspace{-3mm}
\end{figure}
}
Given a set of disparity probability maps, all previous work only utilizes the expectation of disparities (using Eqn.~\eqref{eqn:expect}), which is used as an estimated disparity map.
For the first time, we leverage the variance of disparity prediction to guide the construction of a novel thin plane sweep volume.
In particular, the variance map  from stage  is calculated as follows:


and the corresponding standard deviation is .
Given the disparity prediction  and its variance  of pixel , we reasonably assume its ground truth disparity  generally falls in a Guassian distribution:

Correspondingly, we can construct a confidence interval  from the distribution given by 
,
where  is a scalar parameter that determines how large the confidence interval is.
The per-pixel  determines the thickness of an ATV at each pixel, which expresses the uncertainty boundaries of the volume.
In Fig.~\ref{fig:ucnet} b, d, and f, we schematically illustrate examples of the ground truth surface, pixel-wise disparity probability distributions and the confidence intervals. 
In Fig.~\ref{fig:method}, We show an actual example of three  of a pixel around the prediction (red dash line), with , , .
The  essentially depicts a probabilistic local space around the ground truth surface, and the ground truth disparity is located in the interval with a very high probability within the corresponding uncertainty boundaries (as shown in Fig.~\ref{fig:teaser}).

For each pixel , we uniformly sample  disparities from  of the th stage, to get its disparities ,,..., for the th stage.
In this way, we construct  spatially-varying disparity maps , which 
form the ATV for the th stage.
The central plane of the volume is a curved plane, which aligns with the predicted depth map  from the previous stage; all other planes are shifted from the central one, with dynamic displacements that vary across pixels according to the estimated confidence intervals of individual pixels from the previous stage.

\subsection{Coarse-to-fine Prediction}
In general, for a -stage UCS-Net, the UnitNet of the th stage predicts  probability maps ; it then regresses a disparity map  and a corresponding depth map  as described in Sec~\ref{sec:atvc}.
At the following th stage, an ATV is constructed in a local space around the previous prediction  within the inferred uncertainty boundaries, leveraging the variance  of the prediction to attain an uncertainty-aware construction, as discussed in Sec.~\ref{sec:uacstrct}.
The volume is passed to UnitNet and is used to predict a refined depth map .
In the end, we have the final depth  from the last stage as our final depth prediction.

The UCS-Net refines the prediction  through multiple stages, which makes the variances  and uncertainty of prediction keep decreasing; hence, the following stages partition the local space at a finer scale and make sure the ground truth is still located in the partitioning space with a high probability.
As a result, every following stage (expect the first stage) leverages the uncertainty knowledge from the previous prediction to construct an ATV bounded by the confidence intervals; the plane sweep volumes become thiner and thiner 
in latter stages.
Our novel ATV effectively expresses the locality and uncertainty in the depth prediction, which enables state-of-the-art depth reconstruction results via a coarse-to-fine framework.


\subsection{Loss Function}
\label{sec:loss}
We train our cascaded UCS-Net stage by stage. Specifically, for the th stage, we freeze the network parameters of all previous  stages and only optimize the parameters in UnitNet.
The UnitNet estimates a depth map  from its predicted probability maps .
We supervise this per-stage prediction with the ground truth depth .
Similar to \cite{im2018dpsnet}, we also supervise the predicted raw depth map  of each stage, estimated from the intermediate raw probability maps  (that are generated by UnitNet after the 3D CNN before the final 2D CNN).
Our loss function for stage  is given by

where  is the Huber loss (a smoothed L1 loss). We use  for all our experiments.

}

\comment{
\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{images/iccv_result_bar.pdf}
	\caption{Comparison results (Abs Rel, Abs Diff, Sq Rel, RMSE) on all test data (MVS, SUN3D, RGBD, Scenes11) by DPSNet, double-capacity DPSNet, and our UCS-Net (two configurations: 32-32 and 32-16). The number (colored blue) in each figure represents the percentage of gain compared with DPSNet.}
	\label{fig:bar_overall}
\end{figure*}

\begin{table*}[th!]
	\footnotesize
	\centering
	\begin{tabular}[width=\linewidth]{p{1cm}p{1.5cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.5cm}p{1.2cm}p{1.2cm}p{1.3cm}}
		\Xhline{2pt}
		\makecell{Datasets} & \makecell{Method} & \makecell{Abs Rel} & \makecell{Abs Diff} & \makecell{Sq Rel} & \makecell{RMSE} & \makecell{RMSE log} & \makecell{} & \makecell{} & \makecell{}\\ 
		\Xhline{2pt}
		\multirow{5}{4em}{Overall}
		& COLMAP\cite{schonberger2016structure} & \makecell{0.5602} & \makecell{1.4796} & \makecell{2.7068} & \makecell{2.4619} & \makecell{0.7181} & \makecell{0.3667} & \makecell{0.5671} & \makecell{0.7254} \\
		& DeMoN\cite{ummenhofer2017demon} & \makecell{0.3427} & \makecell{1.7575} & \makecell{5.3248} & \makecell{2.3767} &\makecell{0.2795} & \makecell{0.6457} & \makecell{0.8435} & \makecell{0.9140}  \\
		& DeepMVS\cite{huang2018deepmvs} & \makecell{0.2489} & \makecell{0.6161} & \makecell{0.4449} & \makecell{0.9458} & \makecell{0.3152} & \makecell{0.6256} & \makecell{0.8375} & \makecell{0.9364} \\
		& DPSNet\cite{im2018dpsnet} & \makecell{0.1005}& \makecell{0.2880}& \makecell{0.1413}&  \makecell{0.5095}& \makecell{0.1707}& \makecell{0.8709}& \makecell{0.9430}& \makecell{0.9680} \\
& DPSNet-DC &\makecell{0.1019} &\makecell{0.3026} &\makecell{0.1380}  &\makecell{0.5272} &\makecell{0.1722} &\makecell{0.8692} &\makecell{0.9441} &\makecell{\underline{0.9727}}\\
		\Xhline{2pt}
&\makecell{0.9717\\  ()} \\
		\Xhline{2pt}
		Overall & \textbf{Ours(32-32)} &\makecell{\textbf{0.0913} \\ ()} 
		&\makecell{\textbf{0.2658} \\ ()} &\makecell{\textbf{0.1197} \\ ()} &\makecell{\textbf{0.4790} \\ ()} &\makecell{\textbf{0.1606}\\ ()} &\makecell{\textbf{0.8922} \\ ()} &\makecell{\textbf{0.9545} \\ ()} &\makecell{\textbf{0.9753} \\ ()} \\
		\Xhline{2pt}
	\end{tabular}
	\vspace{+1mm}
	\caption{Comparisons on all test data (MVS, SUN3D, RGBD, Scenes11) between state-of-the-art baselines and our UCS-Net (two configurations: 32-32 and 32-16). Bold numbers and underline numbers represent the highest and second highest measures. We also show our percentage improvement over DPSNet under each of our result.}
	\label{tab: alldata}
	\vspace{-3mm}
\end{table*}
}
\comment{
	\begin{table*}[th!]
		\footnotesize
		\centering
		\begin{tabular}[width=\linewidth]{p{2cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1.5cm}p{0.8cm}p{0.8cm}p{0.8cm}}
			\Xhline{2pt}
			Method & Abs Rel & Abs Diff & Sq Rel & RMSE & RMSE log &  &  & \\ 
			\Xhline{2pt}
			COLMAP & 0.324 & 0.615 &36.71 & 2.370&  0.349& 0.865& 0.903&0.927 \\
			
			DeMoN &0.191 & 0.726 &0.365 &1.059 &0.240 & 0.733&  0.898&0.951 \\
			
			DeepMVS &0.178 &0.432 &0.973 &  1.021& 0.245& 0.858& 0.911 & 0.942\\
			
			MVSNET & 1.666 &  2.165& 13.93&3.255 & 0.824& 0.555&  0.628&0.686 \\
			
			DPSNet & 0.099 &  0.365 & 0.204 & \textbf{0.703} &  \textbf{0.184} & 0.863 &0.938  &\textbf{0.963} \\
			
			\textbf{Ours}& \textbf{0.088}&	\textbf{0.360}&	\textbf{0.180}&	0.715&	\textbf{0.184}&	\textbf{0.885}&	\textbf{0.942}&	0.962\\
			\Xhline{2pt}
		\end{tabular}
		\vspace{+1mm}
		\caption{Comparison results. Multi-view stereo methods on ETH3D.}
		\label{tab: ethdata}
	\end{table*}
}



















