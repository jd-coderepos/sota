\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}



\usepackage{url}
\usepackage{graphics}
\usepackage{color}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{xcolor}
\usepackage{floatflt}
\usepackage{caption}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false,citecolor=cyan]{hyperref}
\usepackage[ruled, lined, linesnumbered, commentsnumbered, longend]{algorithm2e}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\gvect}[1]{\boldsymbol{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}

\usepackage{caption}
\captionsetup{skip=0pt}

\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\usepackage{flushend}

\newcommand{\yang}[1]{{\color{blue}#1}}
\newcommand{\nicu}[1]{\textcolor{red}{{#1}}} 

\iccvfinalcopy 

\def\iccvPaperID{2248} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Transformers Solve the Limited Receptive Field for Monocular Depth Prediction}

\author{Guanglei Yang \ Hao Tang \ Mingli Ding \ Nicu Sebe \ Elisa Ricci\\
Haerbin Institute of Technology\quad
	University of Trento \quad
	Fondazione Bruno Kessler\\
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
While convolutional neural networks have shown a tremendous impact on various computer vision tasks, they generally demonstrate limitations in explicitly modeling long-range dependencies due to the intrinsic locality of the convolution operation. 
Transformers, initially designed for natural language processing tasks, have emerged as alternative architectures with innate global self-attention mechanisms to capture long-range dependencies.
In this paper, we propose TransDepth, an architecture which benefits from both convolutional neural networks and transformers.
To avoid the network to loose its ability to capture local-level details due to the adoption of transformers, we propose a novel decoder which employs on attention mechanisms based on gates. Notably, this is the first paper which applies transformers into pixel-wise prediction problems involving continuous labels (\textit{i.e.}, monocular depth prediction and surface normal estimation).
Extensive experiments demonstrate that the proposed TransDepth achieves state-of-the-art performance on three challenging datasets. The source code and trained models are available at~\url{https://github.com/ygjwd12345/TransDepth}.
\end{abstract} \section{Introduction}

Over the past decade, convolutional neural networks have become the privileged methodology to address fundamental and challenging computer vision tasks requiring dense pixel-wise prediction, such as semantic segmentation~\cite{chen2016attention,fu2019dual}, monocular depth prediction ~\cite{liu2015deep,eigen2014depth}, and normal surface computation \cite{qi2018geonet}.
Since the seminal work of \cite{he2016deep},
existing depth prediction models' have been dominated by encoders implemented with architectures such as ResNet and VGG-Net. The encoder progressively reduces the spatial resolution and learns more concepts with larger receptive fields. Because context modeling is critical for pixel-level prediction, deep feature representation learning is arguably the most critical model component~\cite{chen2017deeplab}. However, it is still challenging for depth prediction networks to improve their ability in modeling global contexts. Traditionally, both stacked convolution layers and consecutive down-sampling are used in the encoders to generate sufficiently large receptive fields of deep layers. This problem is typically circumvented rather than resolved to some extent. Unfortunately, existing strategies bring several drawbacks: (1) the training of very deep nets is affected by the fact that consecutive multiplications wash out low-level features; (2) the local information crucial to dense prediction tasks is discarded since the spatial resolution is reduced gradually. To overcome these limitations, several methods have been recently proposed. One solution is manipulating the convolutional operation directly by using for example large kernel sizes~\cite{peng2017large}, atrous convolutions~\cite{chen2017deeplab}, and image/feature pyramids~\cite{zhao2017pyramid}. Another solution is to integrate attention modules into the fully convolutional network architecture. Such a module aims to model global interactions of all pixels in the feature map~\cite{wang2018non}. When applied to monocular depth prediction~\cite{xu2018pad,xu2020probabilistic} a general approach is to combine the attention module with a multi-scale fusion method.
More recently, Huynh et al. \cite{huynh2020guiding} proposed a depth-attention volume to incorporate a non-local coplanarity constraint to the network.
Guizilini et al. \cite{guizilini2020semantically} rely on a fixed pre-trained semantic segmentation network to guide the global representation learning. Though these methods' performance is improved significantly, still the above mentioned issues persist. 

Transformers were initially used to model sequence-to-sequence predictions in NLP tasks to obtain a larger receptive field and have  recently attracted a tremendous interest in the computer vision community. The first purely self-attention-based vision Transformers (ViT) for image recognition were proposed in~\cite{dosovitskiy2020image} attaining excellent results on ImageNet compared with the convolutional networks. 
Moreover, SETR~\cite{zheng2020rethinking} replaces the encoders with pure Transformers, obtaining competitive results on the CityScapes dataset. Interestingly, we found that SETR-like pure Transformer-based segmentation network produces unsatisfactory performance due to the lack of spatial inductive bias in modeling the local information. Meanwhile, most previous methods based on deep feature representation learning fail to solve this problem. 
Nowadays, only few researchers~\cite{carion2020end} are considering combining the CNNs with Transformers to create a hybrid structure to combine their advantages.

In contrast to treating pixel-level prediction tasks as a sequence-to-sequence prediction problem, we firstly propose to embed Transformers into the ResNet backbone in order to model semantic pixel dependencies. Moreover, we design a new and effective unified attention gate decoder to address the drawback that the pure linear Transformer's embedding feature lacks spatial inductive-bias in capturing the local representation. We show empirically that our method offers a new perspective in model design and achieves state-of-the-art on several challenging benchmarks.

To summarize, our contribution is threefold:
\begin{itemize}[leftmargin=*]
\item We are the first to propose the use of Transformers for both monocular depth estimation and surface normal prediction tasks.  Transformers can successfully improve the ability of traditional convolutional neural networks to model long-range dependencies.
\item We propose a novel and effective unified attention gate structure designed to utilize and fuse multi-scale information in a parallel manner and pass information among different affinities maps in the attention gate decoders for better modeling the multi-scale affinities.
\item We conduct extensive experiments on two distinct pixel-wise prediction tasks with three challenging datasets (\textit{e.g.}, NYU~\cite{silberman2012indoor}, KITTI~\cite{Geiger2013IJRR}, and ScanNet~\cite{dai2017scannet}), demonstrating that our TransDepth outperforms previous methods on KITTI (0.956 on ), NYU depth (0.900 on ), and achieves new state-of-the-art results on NYU surface normal estimation.
\end{itemize} \section{Related Work}

\noindent\textbf{Transformers in Computer Vision.}
Transformer and self-attention models have revolutionized machine translation and natural language processing~\cite{vaswani2017attention,dai2019transformer}.
Recently, there were also some explorations for the usage of Transformer structures in image recognition. Non-local networks~\cite{wang2018non} append a Transformer-style attention onto the convolutional backbone. 
LRNet~\cite{hu2019local} explored local self-attention to avoid the heavy computation brought by global self-attention. 
Axial-Attention~\cite{wang2020axial} decomposed the global spatial attention into two separate axial attentions such that the computation is vastly reduced. Apart from these pure Transformer-based models, there are also CNN-Transformer hybrid ones.
For instance, DETR~\cite{carion2020end} and the following deformable version utilized a Transformer for object detection where the Transformer was appended inside the detection head. 
LSTR~\cite{liu2021end} adopted Transformers for disparity estimation and for lane shape prediction. 
Most recently, ViT~\cite{dosovitskiy2020image} was the first work to show that a pure Transformer-based image classification model can achieve the state-of-the-art. This work provides a direct inspiration to us to exploit a pure Transformer-based encoder design in a semantic segmentation model. Meanwhile, SETR  \cite{zheng2020rethinking} based on ViT, leverages attention for image segmentation. However, there is no related work in continuous pixel prediction. The main reason is that the networks, designed for the continuous label task, extremely rely on deep representation learning and fully-convolutional networks (FCN) with a decoder architecture.
In this case, the pure Transformer (without convolution and resolution reduction) regarding an image as a patch sequence is not suitable for pixel-level prediction with continual labels.


To overcome the aforementioned limitation, we propose a novel combination framework putting a linear Transformer and ResNet together enabling the previous methods based on deep representing learning, such as dilated/atrous convolutions and inserting attention modules, to be also suitable for our networks. Meanwhile, the position embedding module is removed from our linear Transformer but we take advantage of multi-scale fusion in decoder to add position information.  This is essential to successfully apply Transformers to depth prediction and surface normal estimation tasks.



\noindent\textbf{Monocular Depth Estimation.} 
Most recent works on monocular depth estimation are based on CNNs~\cite{eigen2015predicting,liu2015deep,wang2015towards,laina2016deeper, fu2018deep,lee2019big,guizilini20203d,guizilini2020semantically}, which suffer from the limited receptive field problem or from the less global representation learning. For instance,
Eigen~\etal~\cite{eigen2014depth} introduced a two-stream deep network to take into account both coarse global prediction and local information. 
Fu~\etal~\cite{fu2018deep} proposed a discretization strategy to treat monocular depth estimation as a deep ordinal regression problem. They also employed a multi-scale network to capture relevant multi-scale information.
Lee~\etal~\cite{lee2019big} introduced local planar
guidance layers in the network decoder module to learn more effective features for depth estimation.
More recently, PackNet-SfM~\cite{guizilini20203d} used 3D convolutions with self-supervision to learn detail-preserving representations. At the same time, Guizilini~\etal~\cite{guizilini2020semantically} exploit semantic features into the self-supervised depth network by using a pre-trained semantic segmentation network. The new SOTA, FAL-Net~\cite{gonzalez2020forget}, focuses instead on representation learning using stereoscopic view synthesis penalizing the synthetic right-view in all image regions. Though it explicitly increases long-range modeling dependencies, more training steps are added.

\begin{figure*}[htb] \small
\centering
\includegraphics[width=1\textwidth]{images/overview/overview1.pdf}
\caption{The overview of the proposed TransDepth. The symbols \textcircled{c} and  denote concatenation and addition operations, respectively. AG is short for attention gate.}
\label{fig:overview}
\vspace{-0.4cm}
\end{figure*}

Our method focuses on representation learning as well but with only one step training strategy. The Transformer mechanism is quite suitable to solve the limited receptive field issue, to guide the generation of depth features. Unlike the previous works~\cite{zheng2020rethinking, dosovitskiy2020image} reshaping the image into a sequence of flattened 2D patches, we propose a hybrid model combining ResNet~\cite{he2016deep} and linear Transformer~\cite{dosovitskiy2020image}. 
This is quite different from the previous Transformer mechanism, taking advantage of both sides. This composite structure also holds another advantage: many deep representation learning methods can be easily transferred in this network.

\noindent\textbf{Surface Normal Estimation.}
Surface normal prediction is regarded as a close task to monocular depth prediction.
Extracting 3D geometry from a single image has been a longstanding problem in computer vision. Surface normal estimation is a classical task in this context requiring modeling both global and local features. Typical approaches leverage networks with high capacity to achieve accurate predictions at high resolution. For instance, FrameNet \cite{huang2019framenet} employed the DORN \cite{fu2018deep} architecture, a modification of DeepLabv3 \cite{chen2017deeplab} that removes multiple spatial reductions (22 max pool layers), to generate high resolution surface normal maps. A different strategy consists of designing appropriate loss terms. For instance, UprightNet \cite{xian2019uprightnet} considered an angular loss and showed its effectiveness for the task. More recently, Do~\etal~\cite{do2020surface} proposed a novel truncated angular loss and a tilted image process, keeping the  atrous spatial pyramid pooling (ASPP) module to increase the receptive field. Although its performance is SOTA, two extra training phases are added due to the tilted image process.


\noindent\textbf{Attention Models.}
Several works have considered integrating attention models within deep architectures to improve performance in several tasks, such as image categorization~\cite{xiao2015application}, image generation \cite{tang2020xinggan}, speech recognition~\cite{chorowski2015attention}, and machine translation~\cite{vaswani2017attention}. Focusing on pixel-level prediction, Chen \etal~ \cite{chen2016attention} were the first to describe an attention model to combine multi-scale features learned by a FCN for semantic segmentation. Zhang \etal~\cite{zhang2018context} designed EncNet, a network equipped with a channel
attention mechanism to model the global context. 
Huang \etal~\cite{huang2019ccnet} described CCNet, a deep architecture that embeds a criss-cross attention module with the idea of modeling contextual dependencies using sparsely-connected graphs to achieve higher computational efficiency. Fu \etal~\cite{fu2019dual} proposed to model semantic dependencies associated with spatial and channel dimensions by using two separate attention modules. 
Xu \etal~\cite{xu2020probabilistic}~proposed the PGA-Net feature dependent conditional kernels.




Our work significantly departs from these approaches as we introduce a novel attention gate mechanism, adding spatial- and channel-level attention into the attention decoder. Notably, we also prove that our model can be successfully employed in the case of several challenging dense continual pixel-level prediction tasks, where it significantly outperforms PGA-Net~\cite{xu2020probabilistic}. \section{The Proposed TransDepth}

\begin{figure*}[t] \small
\centering
\includegraphics[width=1\textwidth]{images/overview/overview2.pdf}
\caption{The overview of the proposed attention gate module. The symbols , , \textcircled{}, \textcircled{}, and \textcircled{\tiny{S}} denote element-wise multiplication, element-wise addition, sigmoid, convolution, and softmax operation, respectively.
}
\label{fig:agdeconder}
\vspace{-0.4cm}
\end{figure*}


As previously discussed, our work aims to solve limited receptive fields by adding Transformer layers and enhancing the learned representation by an attention gate decoder. 
\subsection{Transformer for Depth Prediction}

An overview of the network is depicted in Figure~\ref{fig:overview}. Unlike the previous works~\cite{zheng2020rethinking,chen2021transunet, dosovitskiy2020image} reshaping the image  into a sequence of flattened 2D patches , we propose a hybrid model. As shown in Figure~\ref{fig:overview}, the input sequence comes from an ResNet backbone~\cite{he2016deep}. 
Then the patch embedding is applied to patches extracted from the final feature output of a CNN. This patch embedding's kernel size should be , which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. In this case, we also remove position embeddings because the original physical meaning is missing. The input of the first Transformer layer is calculated as follow:

where  is mapped into a latent N-dimensional embedding space using a trainable linear projection layer. There are  Transformer layers which consist of multi-headed self-attention (MSA) and multi-layer perceptron (MLP) blocks. At each layer , the input of the self-attention block is a triplet of  (query),  (key), and  (value), similar with~\cite{vaswani2017attention}, computed from  as:

where  are the learnable parameters of weight matrices and  is the dimension of , , . The self-attention is calculated as:

where AH is short for attention head.
MSA means the attention head will be calculated  times by independent weight matrices. The final  is defined as:

where . 
The output of  is then transformed by a MLP block with residual skip as the layer output as:

where  means the layer normalization operator and . The structure of a Transformer layer is illustrated in the left part of Figure~\ref{fig:overview}. After the Transformer layer, the output will be recovered to the original feature shape. 

\subsection{Attention Gate Decoder}
Given an input image  and a generic front-end CNN model with parameters , we consider a set of 
 multi-scale feature maps . Being a generic framework, these feature maps can 
be the output of  intermediate CNN layers or of another representation, thus  is a \textit{virtual} scale. 
Opposite to previous works adopting simple 
concatenation or weighted averaging schemes 
\cite{zheng2020rethinking}, we propose to combine the multi-scale feature maps by learning a 
set of latent kernels (, , ) with a novel structure Attention-Gated 
module sketched in Figure~\ref{fig:agdeconder}. We choose  as a receive feature only, , while  are chosen as emitting features, , in all tasks. The influence of the fusion of different scales is explained in the ablation part. 

In detail, the whole attention gate can be divided into two parts, \ie, attention and message. We propose to bring together recent advances in pixel-wise prediction by formulating a novel attention gate mechanism for the attention part. Inspired by~\cite{fu2019dual}, where two spatial- and channel-wise predictions are computed, we opt to infer different spatial and channel attention variables.
Our attention tensor is defined by:

where  means  is chosen as an emitting feature.
Different from~\cite{fu2019dual}, we adapt a local conditional kernel before generating attention. The kernels , , and  are
predicted from the input features using a linear transformation as follows:

Then, the integrated attention is defined as follow:

Compared with the attention part, the message is easy to be calculated by . Finally, the output of our attention gate decoder is:


Once the hidden variables are updated, we use them to address several different discrete prediction tasks, including monocular depth estimation and surface normal estimation. Following previous works, the network optimization loss for depth prediction, updated from~\cite{eigen2014depth}, is:

where  with the ground truth depth  and the predicted depth .
We set  and  to 0.85 and 10,
same with~\cite{lee2019big}. The angular loss is chosen as the surface normal loss.  \section{Experiments}
\subsection{Datasets}
The NYU dataset~\cite{silberman2012indoor} is used to evaluate our approach in the depth estimation task. We use 120K RGB-Depth pairs with a resolution of  pixels, acquired with a Microsoft Kinect device from 464 indoor scenes. We follow the standard train/test split as in~\cite{eigen2014depth}, using 249 scenes for training and 215 scenes (654 images) for testing. We also use this dataset to evaluate our approach in the surface normal task, including 795 training images and 654 testing images.

The KITTI dataset~\cite{Geiger2013IJRR} is a large-scale outdoor dataset created for various autonomous driving tasks. We use it to evaluate the depth estimation performance of our proposed model. Following the standard training/testing split proposed by Eigen~\etal~\cite{eigen2014depth}, we specifically use 22,600 frames from 32 scenes for training and 697 frames from the rest 29 scenes for testing.

The ScanNet dataset~\cite{dai2017scannet} is a large RGB-D dataset for 3D scene understanding. We employ it to evaluate the surface normal performance of our proposed model. ScanNet dataset is divided into 189,916 for training and 20,942 for testing with file lists provided in \cite{dai2017scannet}.

\subsection{Evaluation Metrics}
\par\noindent\textbf{Evaluation Protocol on Monocular Depth Estimation.}
We follow the standard evaluation protocol as in previous works~\cite{eigen2015predicting,eigen2014depth,wang2015towards} and adopt the following quantitative evaluation metrics in our experiments: 
\begin{itemize}[leftmargin=*]
\item Abs relative error (abs-rel): 
; 
\item Squared Relative difference (sq-rel):
; 
\item root mean squared error (rms): 
;
\item mean log10 error (log-rms): 
;
\item accuracy with threshold : percentage (\%) of , subject to ;
\end{itemize}
where  and  is the ground-truth depth and the estimated depth at pixel  respectively;  is the total number of pixels of the test images.

\par\noindent\textbf{Evaluation Protocol on Surface Normal Estimation.} We utilize five standard evaluation metrics~\cite{fouhey2013data}. For space limitation, we pick up median angle distance between prediction and ground-truth for valid pixels and the fraction of pixels with angle difference with ground-truth less than  listed in the main paper. The results of five standard evaluation metrics are put into supplementary material.

\subsection{Implementation Details} 
The proposed TransDepth is implemented in~PyTorch. The experiments are conducted on four Nvidia Tesla V100 GPUs, each with 32 GB memory. The ResNet-50 architecture pretrained on ImageNet~\cite{deng2009imagenet} is considered in the experiments for initializing the backbone network of our encoder network. For parameters of Transformer, T-layers, Hidden size, and attention multi-head are set to 12, 768, and 12, respectively.
As the attention gate decoder structure setting,  is chosen as the receiving feature, , while  are taken up as emitting features, , in all tasks.

For the monocular depth estimation and surface normal prediction tasks, the learning rate is set to  with a weight decay of 0.01. The Adam optimizer is used in all our experiments with a batch size of 16 for all tasks. 
The total training epochs are set to 50 for depth prediction and 20 for surface normal prediction. We train our network on a random crop of size 352704 for KITTI dataset, 416512 for NYU dataset for depth prediction while
the input image size is uniformly set to 320256 for surface normal prediction.

\begin{figure*}[!h] \small
\centering
\subfigure[Image]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/img2.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/img4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/img5.jpg}\\    
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/img6.jpg}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/img8.jpg}\\


    \end{minipage}}\subfigure[GT]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/gt2.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/gt4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/gt5.jpg}\\    
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/gt6.jpg}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/gt8.jpg}\\ 


    \end{minipage}}\subfigure[DORN]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/dorn2.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/dorn4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/dorn5.jpg}\\   
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/dorn6.jpg}\\  
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/dorn8.jpg}\\  


    \end{minipage}}\subfigure[Ours w/ AGD]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/wotrans2.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/wotrans4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/wotrans5.jpg}\\   
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/wotrans6.jpg}\\  
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/wotrans8.jpg}\\ 
\end{minipage}}\subfigure[Ours (Full)]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/ours2.jpg}\\
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/ours4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/ours5.jpg}\\   
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/ours6.jpg}\\  
\includegraphics[width=0.993\textwidth,height=0.6in]{images/kitti_vis/ours8.jpg}\\
\end{minipage}}\centering
\caption{Qualitative examples on the KITTI dataset.}
\label{fig:vis_kitti}
\vspace{-0.4cm}
\end{figure*}

\begin{table*}[t] \small
\centering
\caption{Depth Estimation: KITTI dataset. K: KITTI. CS: CityScapes~\cite{cordts2016cityscapes}. CSK: CS pre-training. D: Depth supervision. M, Se, V, S: Monocular, segmentation, video, stereo. Sup: supervise.}
\label{tab:depth_kitti}
\resizebox{0.8\textwidth}{!}{\begin{tabular}{rccccccccc}
\toprule[1.2pt]
\multirow{2}{*}{Method}& \multirow{2}{*}{Sup} & \multirow{2}{*}{Data} & \multicolumn{4}{c}{Error (lower is better)} & \multicolumn{3}{c}{Accuracy (higher is better)} \\ \cmidrule(lr){4-7} \cmidrule(lr){8-10} 
 & &  & abs rel & sq rel & rms & log rms &  &  &  \\
\midrule
CC~\cite{ranjan2019competitive}& M+Se & K & 0.140 & 1.070 & 5.326 & 0.217 & 0.826 & 0.941 & 0.975 \\
Bian~\etal \cite{bian2019unsupervised}& M+V & K+CS & 0.137 & 1.089 & 5.439 & 0.217 & 0.830 & 0.942 & 0.975 \\
DeFeat~\cite{spencer2020defeat}& M & K & 0.126 & 0.925 & 5.035 & 0.200 & 0.862 & 0.954 & 0.980 \\
Net~\cite{cheng2020s}& M+Se & K & 0.124 & 0.826 & 4.981 & 0.200 & 0.846 & 0.955 & 0.982 \\
Monodepth2~\cite{godard2019digging}& M & K & 0.115 & 0.903 & 4.863 & 0.193 & 0.877 & 0.959 & 0.981 \\
pRGBD~\cite{tiwari2020pseudo}& M & K & 0.113 & 0.793 & 4.655 & 0.188 & 0.874 & 0.960 & 0.983 \\
Johnston~\etal \cite{johnston2020self}& M & K & 0.106 & 0.861 & 4.699 & 0.185 & 0.889 & 0.962 & 0.982 \\
SGDepth~\cite{klingner2020self}& M+Se & K+CS & 0.107 & 0.768 & 4.468 & 0.180 & 0.891 & 0.963 & 0.982 \\
Shu~\etal \cite{shu2020feature}& M & K & 0.104 & 0.729 & 4.481 & 0.179 & 0.893 & 0.965 & 0.984 \\
DORN~\cite{fu2018deep}& D & K & 0.072 & 0.307 & 2.727 & 0.120 & 0.932 & 0.984 & 0.994 \\
Yin~\etal \cite{yin2019enforcing}& M & K & 0.072 & - & 3.258 & 0.117 & 0.938 & 0.990 & 0.998 \\
PackNet~\cite{guizilini20203d}& V & K+CS & 0.071 & 0.359 & 3.153 & 0.109 & 0.944 & 0.990 & 0.997 \\
FAL-Net~\cite{gonzalez2020forget}& S & K+CS & 0.068 & 0.276 & 2.906 & 0.106 & 0.944 & 0.991 & 0.998 \\
PGA-Net~\cite{xu2020probabilistic} & D & K & 0.063 & 0.267 & \textbf{2.634} & 0.101 & 0.952 & 0.992 & 0.998\\ 
BTS \cite{lee2019big}& M & K & 0.061 & 0.261 & 2.834 & 0.099 & 0.954 & 0.992 & 0.998 \\
 \midrule
Baseline & M & K & 0.106 & 0.753 &  3.981 & 0.104 & 0.888  & 0.967  & 0.986\\
Ours w/ AGD & M & K & 0.065 & 0.261 & 2.766 & 0.101 & 0.953 & 0.993 & 0.998 \\
Ours w/ VIT & M & K & 0.064 & 0.258 & 2.761 & 0.099 & 0.955 & 0.993 & 0.999\\
Ours w/ AGD+ViT (Full) & M & K & \textbf{0.064} & \textbf{0.252} & \textbf{2.755} & \textbf{0.098} & \textbf{0.956} & \textbf{0.994} & \textbf{0.999} \\
\bottomrule[1.2pt]
\end{tabular}}
\vspace{-0.4cm}
\end{table*}


\subsection{Results on Monocular Depth Estimation}
We compare the proposed method with the leading monocular depth estimation models, \ie, \cite{ranjan2019competitive,bian2019unsupervised,spencer2020defeat,cheng2020s,godard2019digging,tiwari2020pseudo,johnston2020self,klingner2020self,shu2020feature,guizilini20203d,fu2018deep,yin2019enforcing,gonzalez2020forget,lee2019big}. 
Comparison results on the KITTI dataset are shown in Table~\ref{tab:depth_kitti}. 
Our method performs favorably versus all previous fully- and self-supervised methods, achieving the best results on the majority of the metrics. 
Our approach employs the supervised setting using single monocular images in the training and testing phase.
Specifically, BTS needs the focal length to refine the final depth output, which is not the same setting as ours and thus not directly comparable to ours. 
Compared with recent STOA, \ie, FAL-Net, our method is better by a large margin. Meanwhile, unlike FAL-Net using stereo split, two-step training, and post-processing, our method is end to end without extra post-processing. The more important thing is that ``Ours w/ VIT'' has outperformed the SOTA.
It can support our standpoint that adding a linear Transformer makes networks improve their ability to capture long-range dependencies. In other words, our network becomes simpler but more powerful by adapting the linear Transformer.

\begin{figure}[t] \small
\centering
\subfigure[Image]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/img3.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/img4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/img5.jpg}\\    
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/img7.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/img8.jpg}\\
    \end{minipage}}\subfigure[GT]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/gt3.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/gt4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/gt5.jpg}\\    
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/gt7.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/gt8.jpg}\\ 
    \end{minipage}}\subfigure[DORN]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/dorn3.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/dorn4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/dorn5.jpg}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/dorn7.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/dorn8.jpg}\\  
    \end{minipage}}\subfigure[w/ AGD]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/wotrans3.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/wotrans4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/wotrans5.jpg}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/wotrans7.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/wotrans8.jpg}\\ 
    \end{minipage}}\subfigure[Ours (Full)]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/ours3.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/ours4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/ours5.jpg}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/ours7.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_depth_vis/ours8.jpg}\\ 
    \end{minipage}}\centering
\caption{Qualitative examples on the NYU depth dataset.}
\label{fig:vis_nyu}
\vspace{-0.4cm}
\end{figure}

\begin{table}[t] \small
\centering
\caption{Depth Estimation: NYU dataset.}
\resizebox{1\linewidth}{!}{
\label{tab:depth_nyu}
\begin{tabular}{rcccccc}
\toprule[1.2pt]
\multirow{2.5}{*}{Method} & \multicolumn{3}{c}{Error (lower is better)} & \multicolumn{3}{c}{Accuracy (higher is better)} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7}  
 & rel & log10 & rms  &  &  &  \\ 
 \midrule
    PAD-Net~\cite{xu2018pad} & 0.214 & 0.091 & 0.792 & 0.643 & 0.902 & 0.977 \\
    Li~\etal~\cite{li2017two}& 0.152 & 0.064 & 0.611 & 0.789 & 0.955 & 0.988\\
    CLIFFNet~\cite{wang2020cliffnet} & 0.128 &	0.171 &	0.493 &	0.844 &	0.964 &	0.991\\
    Laina~\etal \cite{laina2016deeper} & 0.127 & 0.055 & 0.573 & 0.811 & 0.953 & 0.988\\
    MS-CRF~\cite{xu2017multi} & 0.121 & 0.052 & 0.586 & 0.811 & 0.954 & 0.987\\
    Lee~\etal \cite{lee2020multi} & 0.119 & 0.050 &	- &	0.870 &	0.974 & 0.993\\
    Xia~\etal \cite{xia2020generating} & 0.116 & - & 0.512 & 0.861 & 0.969 & 0.991 \\
    DORN~\cite{fu2018deep} & 0.115 & 0.051 & 0.509 & 0.828 & 0.965 & 0.992 \\
    BTS \cite{lee2019big} & 0.113 & 0.049 & 0.407 & 0.871 & 0.977 & 0.995 \\
Yin~\etal \cite{yin2019enforcing} & 0.108 & 0.048 & 0.416 & 0.875 & 0.976 & 0.994 \\
    Huynh~\etal \cite{huynh2020guiding} & 0.108 & -	& 0.412 & 0.882 & 0.980 & \textbf{0.996}\\
    \midrule
    Baseline & 0.118 & 0.051 & 0.414 & 0.866 & 0.979 & 0.995 \\
    Ours w/ AGD & 0.111 & 0.048 & 0.393 & 0.881 & 0.979 & 0.996\\
    Ours w/ VIT & 0.109 & 0.047 & 0.388 & 0.887 & 0.981 & 0.996\\
    Ours w/ AGD+ ViT (Full)  & \textbf{0.106} & \textbf{0.045} & \textbf{0.365} & \textbf{0.900} & \textbf{0.983} & \textbf{0.996}\\
\bottomrule[1.2pt]
\end{tabular}}
\vspace{-0.4cm}
\end{table}

To demonstrate the competitiveness of our approach in an indoor scenario, we also evaluate the proposed method on the NYU depth dataset. The results are shown in Table~\ref{tab:depth_nyu}, compared with the the state-of-the-art methods like \cite{xu2018pad,li2017two,wang2020cliffnet,laina2016deeper,xu2017multi,lee2020multi,xia2020generating,fu2018deep,lee2019big,yin2019enforcing,huynh2020guiding}.
Similar to the experiments on KITTI, it outperforms both state-of-the-art approaches and previous methods based on attention mechanism~\cite{xu2017multi,xu2018pad,huynh2020guiding}.
Our method successfully improves~ from 0.882 (Huynh~\etal~\cite{huynh2020guiding}) to 0.900 while root mean squared error significantly drops to 0.365.
Both Table~\ref{tab:depth_kitti} and ~\ref{tab:depth_nyu} also show that our AGD can merge more low-level information and can make the network learn a more efficient deep representation.
Moreover, Figure~\ref{fig:vis_kitti} shows a qualitative comparison of our method with DORN~\cite{fu2018deep}. The red box marks the significant improvement parts. Results indicate that our method generates more precise boundaries for distant stuff like vehicles and traffic signs and near stuff like humans.
Figure~\ref{fig:vis_nyu} shows a similar comparison done on the NYU dataset. Owing to applying the Transformer, the corners of the room are more distinguishable. This can support our standpoint that adapting the linear Transformer makes the CNN backbone network enhance the ability to capture long-range dependencies. 


\begin{figure}[t] \small
\centering
\subfigure[Image]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/input_22.png}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/input_25.png}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/input_26.png}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/input_30.png}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/input_65.png}\\ 
    \end{minipage}}\subfigure[GT]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/normal_gt_22.png}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/normal_gt_25.png}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/normal_gt_26.png}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/normal_gt_30.png}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/normal_gt_65.png}\\ 
    \end{minipage}}\subfigure[Baseline]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/bl_22.png}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/bl_25.png}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/bl_26.png}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/bl_30.png}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/bl_65.png}\\ 
    \end{minipage}}\subfigure[w/ AGD]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/eigen_22.png}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/eigen_25.png}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/eigen_26.png}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/eigen_30.png}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/eigen_65.png}\\
    \end{minipage}}\subfigure[Ours (Full)]{
    \begin{minipage}{0.19\linewidth}
        \centering
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/normal_pred_22.png}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/normal_pred_25.png}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/normal_pred_26.png}\\   
\includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/normal_pred_30.png}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/nyu_sn_vis/normal_pred_65.png}\\ 
    \end{minipage}}\centering
\caption{ Qualitative examples on the NYU surface normal dataset.}
\label{fig:vis_nyu_sn}
\vspace{-0.4cm}
\end{figure}

\begin{table}[t] \small
\caption{Surface Normal Estimation: NYU dataset.}
\label{tab:sn_nyu}
\centering
\resizebox{1\linewidth}{!}{\begin{tabular}{r|c|c|cc}
\toprule[1.2pt]
Method & Training Data & Testing Data & median  &  \\
\midrule
Li~\etal \cite{li2015depth} & \multirow{7}{*}{NYU} & \multirow{11}{*}{NYU} & 27.8 & 19.6 \\
Chen~\etal \cite{chen2017surface} &  &  & 15.8 & 39.2 \\
Eigen~\etal \cite{eigen2015predicting} &  &  & 13.2 & 44.4 \\
SURGE \cite{wang2016surge} &  &  & 12.2 & 47.3 \\
Bansal~\etal \cite{bansal2016marr} &  &  & 12.0 & 47.9 \\
GeoNet~\cite{qi2018geonet} &  &  & 12.5 & 46.0 \\
TransDepth (Ours) &  &  & \textbf{11.8} & \textbf{48.2} \\\cmidrule{1-2} \cmidrule{4-5} 
FrameNet~\cite{huang2019framenet} & \multirow{4}{*}{ScanNet} &  & 11.0 & 50.7 \\
VPLNet~\cite{wang2020vplnet} &  &  & 9.8 & 54.3 \\
Do~\etal~\cite{do2020surface} &  &  & 8.1 & 59.8 \\
TransDepth (Ours) &  &  & \textbf{7.8} & \textbf{61.7}\\
\bottomrule[1.2pt]
\end{tabular}}
\vspace{-0.4cm}
\end{table}

\subsection{Results on Surface Normal Estimation}
To prove our method universality, we also conduct experiments on surface normal prediction, which is regarded as a related task to depth prediction.
We compare the proposed TransDepth with several state-of-the-art methods on surface normal, including GeoNet~\cite{qi2018geonet}, VPLNet~\cite{wang2020vplnet}, FrameNet~\cite{huang2019framenet}, and Do~\etal~\cite{do2020surface}.
For a fair comparison, we report our result in two different training conditions. Because of limited space, only median angle and  are compared in Table~\ref{tab:sn_nyu} while a detail comparison is shown in the supplementary. Our method outperforms the state-of-the-art on the median angle and . Though Do~\etal reduces the median angle error much, their method needs to get extra gravity labels with two step pre-training. Our method covers these drawback. The qualitative results are shown in Figure~\ref{fig:vis_nyu_sn}. Unsurprisingly, the boundaries of stuff become more precise when AGD and ViT are jointly use.

\begin{table}[] \small
\caption{Ablation study on the NYU depth dataset: performance of TransDepth for different scales fusion.}
\label{tab:ab_scale}
\resizebox{\linewidth}{!}{\begin{tabular}{cccccccc}
\toprule[1.2pt]
 &  & \multicolumn{3}{c}{Error (lower is better)} & \multicolumn{3}{c}{Accuracy (higher is better)} \\
 \cmidrule(lr){3-5} \cmidrule(lr){6-8}
\multirow{-2}{*}{} & \multirow{-2}{*}{} & rel & log10 & rms &  &  &   \\
\midrule
- & - & 0.118 & 0.051 & 0.414 & 0.866 & 0.979 & 0.995 \\
 &  & 0.120 & 0.071 & 0.407 & 0.878 & 0.982 & 0.996 \\
 &  & 0.108 & \textbf{0.045} & 0.366 & 0.897 & 0.982 & 0.996 \\
 &  & \textbf{0.106} & \textbf{0.045} & \textbf{0.365} & \textbf{0.900} & \textbf{0.983} & \textbf{0.996} \\
 &  & 0.107 & \textbf{0.045} & 0.366 & 0.899 & \textbf{0.983} & \textbf{0.996}\\
\bottomrule[1.2pt]
\end{tabular}}
\vspace{-0.4cm}
\end{table}

\begin{table}[] \small
\centering
\caption{Ablation study about different backbone on the NYU depth dataset. R50 is short for ResNet50. B is short for base.}
\resizebox{1\linewidth}{!}{
\label{tab:ab_backbone}
\begin{tabular}{rcccccc}
\toprule[1.2pt]
\multirow{2.5}{*}{Backbone} & \multicolumn{3}{c}{Error (lower is better)} & \multicolumn{3}{c}{Accuracy (higher is better)} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7} 
 & rel & log10 & rms  &  &  &  \\ 
    \midrule
    ViT-B/32~\cite{dosovitskiy2020image} & 0.112 & 0.048 & 0.387 & 0.849 & 0.927 & 0.940 \\
    ViT-B/16~\cite{dosovitskiy2020image} & 0.108 & 0.046 & 0.371 & 0.885 & 0.967 & 0.979\\
    \midrule
    ResNet50~\cite{he2016deep} & 0.118 & 0.051 & 0.414 & 0.866 & 0.979 & 0.995\\
    ResNet101~\cite{he2016deep} & 0.112 & 0.048 & 0.387 & 0.848 & 0.927 & 0.939\\
    ResNet152~\cite{he2016deep} & 0.111 & 0.047 & 0.381 & 0.861 & 0.941 & 0.953\\
    \midrule
    R50+ViT-B/32  & 0.107 & 0.045 & 0.368 & 0.893 & 0.975 & 0.988\\
    R50+ViT-B/16 (Ours)  & \textbf{0.106} & \textbf{0.045} & \textbf{0.365} & \textbf{0.900} & \textbf{0.983} & \textbf{0.996}\\
\bottomrule[1.2pt]
\end{tabular}}
\vspace{-0.4cm}
\end{table}


\subsection{Ablation Study}
\noindent \textbf{Effect of Attention Gate Decoder.}
We perform an ablation study on the NYU depth dataset to further demonstrate the impact of the proposed AGD. 
In Table~\ref{tab:ab_scale}, in the  column we indicate the emitting features while we design , the last layer's output as the only receiving feature in all the experiments. 
We choose the ResNet-50 with the same prediction head as our baseline.
We report four different combinations with the baseline when ViT is not applied to any candidates. Interestingly, the performance does not always get better by adding more scale information. In detail, the performance increases significantly with the emitting feature increasing until the number of emitting features reaches three. Compared with the last two rows in Table~\ref{tab:ab_scale}, some metrics like rel and rms go worse when the number of emitting features further expands. This could be explained by the fact that too many scale features may lead to overfitting of the receiving feature. Undoubtedly, we choose three scales of fusion in all tasks.
According to Figure~\ref{fig:vis_att}, the attention granted by different scales fusion can capture information at different range distances. This can prove that the attention gate decoder is helpful to the receive feature to capture more position information. 

\noindent \textbf{Effect of Different Backbones.}
We also compare different backbones on the NYU depth dataset on Table~\ref{tab:ab_backbone} while the attention gate decoder is not used in this experiment. The 16/32 are no longer the input path size but the shrinkage scale of the input feature. In other words, 16 means the  is the input feature of ViT-B, while 32 represents the  is the input feature of ViT-B.
Table~\ref{tab:ab_backbone} can be split into three-part: the top part belongs to the pure Transformer backbone; the middle part belongs to the pure ResNet backbone; the bottom belongs to the mixed backbone. 
Compared with the middle part results, the mixed backbone overpasses all of the pure ResNet backbones, leaving a significant margin for each metric. Meanwhile, according to Table~\ref{tab:ab_backbone}, the mixed backbone is better than the ResNet backbone, and it outperforms the pure Transformer encoder. 
We finally pick up the ResNet-50 with ViT-B/16 as our network's encoder for every task.


\begin{figure}[!t] \small
\centering
\subfigure{
    \begin{minipage}{0.24\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att_img1.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att_img2.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att_img3.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att_img4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att_img5.jpg}\\
\end{minipage}}\subfigure{
    \begin{minipage}{0.24\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att1_img1.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att1_img2.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att1_img3.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att1_img4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att1_img5.jpg}\\
\end{minipage}}\subfigure{
    \begin{minipage}{0.24\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att2_img1.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att2_img2.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att2_img3.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att2_img4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att2_img5.jpg}\\
\end{minipage}}\subfigure{
    \begin{minipage}{0.24\linewidth}
        \centering
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att3_img1.jpg}\\  
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att3_img2.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att3_img3.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att3_img4.jpg}\\
        \includegraphics[width=0.993\textwidth,height=0.6in]{images/att/nyu_att3_img5.jpg}\\
\end{minipage}}\centering
\caption{Qualitative attention examples of monocular depth prediction on the NYU dataset. First column is th original image and next three columns are different fusion attention.}
\label{fig:vis_att}
\vspace{-0.4cm}
\end{figure} \section{Conclusions}
We propose a novel Transformer-based framework, i.e., TransDepth, for the pixel-wise prediction problems involving continuous labels. 
We are the first to propose using Transformer to solve pixel-wise prediction problems to the best of our knowledge.
The proposed TransDepth leverages the inductive bias of ResNet on modeling spatial correlation and the powerful capability of Transformers on modeling global relationships. 
Moreover, a new and effective unified attention gate structure with independent channel-wise and spatial-wise attention is applied in the decoder.  This can merge more low-level information and can make the network learn a more efficient deep representation. Extensive experiments prove that the proposed TransDepth establishes new state-of-the-art results on KITTI ( on ), NYU depth ( on ), and NYU surface normal ( on ) datasets.
We hope that this work can bring a new perspective on using Transformer-based architectures for computer vision tasks.
\yang{
} 
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\end{document}
