\documentclass{llncs}

\usepackage{geometry}
\usepackage{lipsum}



\usepackage{dblfloatfix}
\usepackage{lscape}
\usepackage{amsmath}
\usepackage{url}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color, colortbl}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{float}
\usepackage{fixltx2e}

\usepackage{multirow}

\usepackage{subfig}

\usepackage[table]{xcolor}

\usepackage[pdftex]{graphicx}


\newcommand{\EG}[1]{EG$_{\text{#1}}$}




\newcommand{\ourDelta}[2][]{\ensuremath{\Delta\textsuperscript{\text{#1}}
\textsubscript{\text{#2}}}}

\newcommand{\high}[1][ ]{\ensuremath{\text{\emph{high}}#1}}
\newcommand{\low}[1][ ]{\ensuremath{\text{\emph{low}}#1}}

\newcommand{\highDelta}[2][]{\high{\ourDelta[#1]{#2}}}
\newcommand{\lowDelta}[2][]{\low{\ourDelta[#1]{#2}}}

\newcommand{\threshold}[1][]{\ensuremath{\tau\textsubscript{#1}}}
\newcommand{\symptom}[1][]{\ensuremath{\sigma\textsubscript{#1}}}

\hyphenation{op-tical net-works semi-conduc-tor Bo-ris}


\begin{document}
\title{A Centralized Mechanism to Make Predictions \\ Based on Data From Multiple 
WSNs}

\author{Gabriel Martins Dias \and Simon Oechsner \and 
Boris Bellalta}

\authorrunning{Gabriel Martins Dias et al.} 

\tocauthor{Gabriel Martins Dias, Simon Oechsner, and 
Boris Bellalta}


\institute{Department of Information and Communication Technologies \\ Pompeu Fabra University, Barcelona, Spain}

\maketitle

\begin{abstract}




In this work, we present a method that exploits a scenario with inter-Wireless 
Sensor Networks (WSNs) information exchange by making predictions and adapting 
the workload of a WSN according to their outcomes. We show the feasibility of 
an approach that intelligently utilizes information produced by other WSNs that 
may or not belong to the same administrative domain. To illustrate how the 
predictions using data from external WSNs can be utilized, a specific use-case 
is considered, where the operation of a WSN measuring relative humidity is 
optimized using the data obtained from a WSN measuring temperature. 
Based on a dedicated performance score, the simulation results show that this 
new approach can find the optimal operating point associated to the trade-off 
between energy consumption and quality of measurements. Moreover, we outline the 
additional challenges that need to be overcome, and draw conclusions to guide 
the future work in this field.

\end{abstract}

\section{Introduction}

Nowadays, forests, cities and houses, among others, are monitored by multiple 
Wireless Sensor Networks (WSNs) that may belong to different organizations, both 
public and private, as well as to individual citizens. In addition, there is a 
high heterogeneity regarding the technologies, protocols and standards used in 
WSNs. In this situation, each WSN usually operates completely independent of 
other WSNs, even if they are covering the same physical area, and is thus not 
able to take any advantage of the presence of those other WSNs to enrich its 
collected data nor to optimize its operation.

However, WSN performance can be improved by combining data generated from 
different sensors, belonging to the same node, other nodes from the same network 
or from other WSNs. This data sharing allows each WSN to build a deeper 
knowledge about its surroundings, may reduce the probability of getting wrong 
values and taking wrong decisions, and encompasses wider areas and different 
perspectives of the same environment. 




In an era of high availability of data from the cloud, we are interested in 
using data from other WSNs to reduce the energy consumption and improve the 
quality of the measurements done by a target WSN. The external information will 
be used to make predictions and change the operation of the nodes and save 
energy when the environmental conditions do not indicate that big changes will 
happen in the near future. For example, relative humidity and temperature values 
usually have a high correlation, and the former may have a higher variation if 
the latter is changing.

This paper lists some of the existing alternatives for collaboration and 
prediction in WSNs and develops further the inter-WSNs information exchange 
concept introduced in \cite{Pal2012} and in \cite{6583430}. The main idea behind 
the inter-WSN information exchange is that the data gathered by other WSNs can 
be exchanged via their sinks and used to improve the operation of the target 
one, and vice versa. 
Our main contribution is a mechanism that uses the data from collaborating WSNs 
to make predictions.
In order to validate our idea, we show how the WSNs evolve using this kind of 
collaboration, define a way to scale the quality of the measurements and the 
WSNs' performance, and finally present some simulation results from a chosen 
scenario consisting of two WSNs, one for monitoring the relative humidity and 
another for the temperature. Based on the presented results, we show how 
energy-efficient and accurate it can be.

The paper is organized in the following sections: In 
Section~\ref{sec:related-work}, we describe related works about collaboration 
between WSNs, the use of data from external sensors and predictions in WSN 
environments; the details of our proposed mechanism are explained in 
Section~\ref{sec:simulations}; the use case considered for the tests is 
detailed in Section~\ref{sec:current-simulations-scenario}; the simulation 
results and the evaluation of the approach are explained in 
Section~\ref{sec:evaluation} and; at the end, our conclusions and ideas for 
future work are shown in Section~\ref{sec:conclusion}.


\begin{comment}
\section{Collaboration in WSNs}
\label{sec:related-work}

According to the synergy theory~\cite{Haken1980}, a system that combines the 
action of individual components may produce better results than the individual 
components acting separately. Based on that principle, several collaboration 
mechanisms in WSNs have been developed.

In those works, the information exchange in WSNs described can be categorized 
to happen on three different scales: 
(A) \textbf{Inside wireless sensor nodes} that are equipped with two or more 
sensor types, and build deeper knowledge based on the different data types; 
(B) \textbf{Inside WSNs} that enable the communication between their nodes, 
which share knowledge and resources in order to achieve common goals; and
(C) \textbf{Exporting the knowledge built in a WSN} to other systems that can 
use the data for their own purpose.

\subsection{Intra-node collaboration}

Wireless sensor nodes can be equipped with different sensors that are able to 
measure environmental parameters, such as temperature, relative humidity and 
luminosity levels. Additionally, other types of information can be extracted 
from their components, for example, the battery voltage. The combination of 
these parameters can be done by WSN applications that are able to analyze the 
data in order to take decisions based only on local information. For 
example,~\cite{Hefeeda2009} describes an application that detects fire and 
enables nodes to match information from the CO, temperature, ionization and 
photoelectric sensors in order to infer whether a fire is present or not.

\subsection{Inter-node collaboration}

As briefly described before, by making use of the radio communication, it is 
possible to exchange information between nodes and take decisions based on the 
combined information. This information, which usually represents a more complete 
and more reliable knowledge about the external world, may substitute new 
measurements, reduce the energy consumption of the nodes and, consequently, 
increase the WSNs' lifetime. Moreover, even though the use of the intra-node 
collaboration may enhance the WSNs' performance when combined with inter-node 
collaboration, the latter--done at the intra-WSN level--is enough by itself to 
improve several aspects of the WSNs:

\subsubsection{Self-organization}
 
WSNs may change their topology and find new routing paths based on node 
constraints. The Constrained Anisotropic Diffusion Routing protocol is described 
in \cite{Chu2002} as a routing protocol that determines the optimal path by 
making transmissions through the nodes that are retrieving the most relevant 
measurements for the WSN at the given time. 

Similarly, a mechanism that creates an autonomous cluster-based architecture 
with group managers that lead sets of nodes is presented in \cite{Minh2013}.
Those managers are chosen according to the amount of energy available and they  
are responsible for setting the group topology and deciding which nodes are 
going to sense data in the next time interval. This decision is based on the 
nodes' location and on their energy level.

Another example is the Biologically-Inspired Architecture for WSNs, which is 
presented in~\cite{Boonma:2007:BBM:1290550.1290828} as an option that balances 
the work load by adjusting nodes' sleeping times and the network's response 
times according to the delay tolerated by the application requirements, and by 
collecting and combining data from different types of sensors instead of making 
more measurements.

\begin{table*}
    \renewcommand{\arraystretch}{1}\begin{tabular}{|p{2.9cm}|p{2.5cm}|p{3cm}|p{7.8cm}|}
    \hline
    \textbf{Sender WSNs} 
	& \textbf{Information sent} 
	& \textbf{Receiver WSNs}    
        & \textbf{Correlation} \\ \hline
    Temperature and \newline relative humidity monitoring  			
	& Temperature and  \newline relative humidity values 			
	& Fire detection
	& Warm and dry times are more likely to have fires. \\ \hline
    Temperature monitoring
	& Temperature values                       			
	& Snowfall detection
	& Snowfall may happen only during cold times. \\ \hline
    \multirow{2}{*}{\vbox{Temperature monitoring and rainfall detection}} 	
	& \multirow{2}{*}{\vbox{Temperature values and presence of rain}}  	
	& Snowfall detection           	
	& Changes in the environment may lead to landslides and avalanches.      
          \\ \cline{3-4}							
	&  & Flood detection              	
	& Changes in the environment may change the water quality and river 
flooding.      \\ \hline
    \multirow{2}{*}{Rainfall detection}
	& \multirow{2}{*}{Presence of rain}          				
	& Water quality monitoring     	
	& During rain, the water quality is more likely to change.               
         \\ \cline{3-4}                          				
	& & Traffic monitoring
	& During rain, the traffic load and the probability of car accidents may 
increase. \\ \hline
    Traffic monitoring
	& Local traffic jams                       			
	& Pollution monitoring
	& Pollution levels may increase when traffic jams occur. \\ \hline
    Disaster monitoring
	& Phenomenon detection
	& Structural health monitoring
	& Natural disasters may change buildings' structural health. \\ \hline
    \end{tabular}
    \caption{Examples of scenarios that may take advantage from inter-WSN 
information exchange.}
    \label{table:scenarios}
\end{table*}

\subsubsection{Resource sharing}

WSNs are often composed by wireless sensor nodes with different abilities, 
which means having different sensor types, different amount of memory available 
or different types of hardware components, and therefore nodes with higher 
computational power than others. Based on the idea that the whole is more than 
the sum of its parts, resource sharing techniques are used to take advantage of 
the nodes' heterogeneity by selecting which of them are going to process certain 
tasks. As a result, WSNs' capabilities are better utilized and, because of the 
use of the best nodes for each task, the overall performance is improved as well 
as the quality of the obtained results.

For example, the mechanism proposed in \cite{Fortino2009} provides a dynamic 
task-assignment solution that enables nodes to collaboratively process sensing, 
timing and functional tasks. In short, it explores star-architectures with one 
node taking responsibility for the centralized execution of certain tasks, 
based on the information received from its neighbors. In contrast, the 
approaches presented in \cite{Guo2004} and \cite{Xu} are focused on WSNs that 
must run more than one application at the same time. However, in these networks 
the nodes are not able to perform multiple tasks or store all the applications 
in their memory at the same time, due to their resource constraints.
Thus, both approaches are meant to maximize the overall quality of the 
measurements by selecting the best combination of sensor nodes for the available 
applications according to their resources. 


\subsubsection{Cooperation}

Given the goal of the WSN application, the fusion of the data sensed by the 
nodes may represent higher data quality, reflecting their reliability and the 
network's coverage. In~\cite{Chu2002}, the mechanism called Information-Driven 
Sensor Querying is presented as an option that enables intra-WSN cooperation 
by querying data from a subset of nodes based on their localization and 
coverage, and using a \emph{belief value} in order to exclude measurements 
without interest (e.g., messages from nodes that are too far from the point of 
interest). Alternatively, in~\cite{Tan:2013:ACF:2362336.2362347}, an algorithm 
to combine data collected by a specific set of nodes is presented. Assuming that 
the network has two types of nodes, nodes equipped with high consumption sensors 
and nodes equipped with low consumption sensors, the nodes with sensors that 
consume more energy but are able to produce more detailed and less erroneous 
information about the environment remain in sleeping mode, until they 
are activated by the detection of a possible object of interest by the nodes 
with low consumption sensors. Another solution is described in \cite{Huang2003} 
as a way to select which nodes may be activated for a task, considering that an 
area must be covered by, at least, a defined number of them.

Finally, the Round Robin Cycle for Predictions in WSNs is described 
in~\cite{Yann-Ael2005}. This algorithm was developed for WSN applications that 
require a continuous delivery of sensor measurements, such as temperature or 
traffic monitoring.  In order to build sets of nodes that provide trusted 
measurements, it considers that a sensor measurement is predictable if the 
predicted value (on average) differs less than a defined threshold from the 
actual one when using other nodes' measurements. This approach can be extended 
and adapted for the use of external data to make predictions while keeping the 
quality of the measurements, which would be enough for networks that use 
continuous monitoring applications.

\subsection{Collaboration with external systems}
\end{comment}

\section{Related Work}
\label{sec:related-work}

A system that combines the action of individual components may produce better 
results than the individual components acting separately. Supported by this 
premisse, several collaboration mechanisms in WSNs have been developed.
Most of the approaches explore the collaboration between sensor nodes of the 
same WSN. In contrast to them, we extend the concept of collaboration to an 
upper layer and build the information exchange between different WSNs, without 
losing any other possible collaboration from the other levels. 

An inter-domain routing protocol is described in~\cite{Dressler2010}, where it 
is shown that the gateways may share information about their nodes and take 
advantage of being physically close to each other. This information can be used 
to transmit packets through nodes of the other WSNs and can be done either 
to share the information or for routing purposes. Even though the idea of our 
work is to create a link between nodes from different WSNs, it is neither meant 
to share resources nor information between wireless sensor nodes, but the 
knowledge that the network is able to produce based on collected data.

In \cite{Parker2008}, the authors describe a scenario where a system 
is responsible for building a richer knowledge about the environment by making 
use of the information produced by other WSNs. In their example, wireless 
sensor nodes combine sensory information with their localization and help 
other systems to localize and track objects from a distance. The goal of the 
described approach is to enable a robot to use the data retrieved by a WSN that 
detects the presence of objects inside the monitored area. After receiving the 
information from the WSN, the robot interprets the position of the object and 
moves itself to its location in order to get more details about the real 
situation. Their approach is different from ours mainly because it uses a 
non-generic solution that is highly coupled to the presented scenario without a 
WSN as the beneficiary of the collected information, besides not making any 
prediction with the information received from the others.

Besides the works that encourage the collaboration among WSNs, some authors 
applied predictions in order to reduce the energy consumption in the WSNs and 
extend their lifetime.
In~\cite{Yann-Ael2005}, the authors developed an algorithm for WSN applications 
that require a continuous delivery of sensor measurements, such as temperature 
and traffic monitoring. In order to build sets of nodes that provide trustful 
measurements, it considers that a sensor measurement is predictable if the 
predicted value (on average) differs on less than a (user) defined threshold 
when using other nodes' measurements.
After defining which sensors can be predicted by which other, the base station 
must find a set of subsets of active nodes such that a different prediction 
subset is used at each time, and such that all sensors are queried at least once 
during a cycle. 
After building this set, the base station must activate a subset of nodes at a 
time. In other words, only the sensor nodes from the active subset are activated 
during a time interval and all the others have their radios and sensors turned 
off in order to save energy and extend the WSN lifetime.
Simulations using real data show that such approach can successfully achieve 
its goals depending on the user requirements and on the quality of the data.
Similarly, our mechanism also assumes the task of selecting which sensors are 
going to be active in the next time interval. 
However, our mechanism is able to react to environmental changes, while their 
work is less dynamic. That is, once the sets of sensors are defined, they will 
be interleaved independently of changes that may happen around the WSN. 
We highlight that it may be possible to improve our mechanism by adopting their 
techniques to build the groups of sensors in a way that there is no reduction in 
the quality of the measurements and the energy savings are maximized.


The solution presented in \cite{Deshpande2004} (called BBQ) is a centralized 
mechanism used to query data based on sensor models. 
It assumes that the costs of retrieving data from many nodes can be extremely 
high and that sensors in close proximity are likely to have correlated readings, 
which may mean that most of the data provides little benefit in the quality of 
the answers given to the user.
In order to save energy, the BBQ incorporates statistical models of real-world 
processes into the query processing architecture and acquires data from the 
sensors only when the model itself is not sufficiently rich to answer the query 
with acceptable confidence.
To achieve such a goal, the BBQ approximates the probability density function 
of the measurements to multivariate Gaussian distributions and, given the 
correlation between the known measurement(s) and the unknown one(s), it 
calculates their expected value associated to a confidence interval. If the 
confidence level is greater or equal than a chosen threshold, it assumes that 
such value satisfies the system requirements. 
Otherwise, it calculates the energy consumed to retrieve new measurements 
considering the costs to activate the corresponding sensor and, finally, builds 
a query that will require the lowest energy consumption for the WSN and will 
give at least the minimum level of confidence set by the user.
Similarly to our mechanism, it exploits the correlation between different types 
of data that the sensor nodes may be able to measure, for example, their own 
voltage and the local temperature. The difference from our work is that they do 
not provide a method to measure the quality of the measurements and the 
performance of the system.





\begin{comment}
\section{Potential Scenarios}
\label{sec:scenarios}

In the real world, many WSNs with different goals may be deployed in the same 
area and sense data that is, actually, semantically linked. Temperature, 
relative humidity and wind speed are examples of measured values that may be 
correlated in places like open fields, mountains, forests, roads and buildings. 
Table~\ref{table:scenarios} lists a variety of scenarios where WSNs could take 
advantage of using the external information available. Below, we describe them, 
with emphasis on the fire detection scenario.


\subsection{Fire detection}

Fire detection may involve a complex combination of different environmental 
aspects that may change according to the surroundings monitored: in contrast to 
fires inside buildings, forest fires are usually influenced by wind and rain 
levels as well as other ambient parameters. Therefore, in order to cover these 
different environments, there are different ways of detecting fire using WSNs. 
However, most of them use a fusion of the sensed data, such as temperature, wind 
speed, humidity and CO$_2$ levels, as well as rain detection. 

Some indices, for example, the Canadian Forest Fire Weather Index (FWI) System 
\cite{Groot1998} and the American Fire Danger Rating System 
\cite{Keetch1968}, were developed in order to measure the fire risks, 
potential danger, type of fire, total amount of effort needed to extinguish the 
fires, etc. Moreover, FWI provides a mapping between environmental observations 
and fire behavior that estimates the probability of having fire according to 
weather observations, such as temperature, humidity, rain and wind speed. Also, 
it has two indices to classify the fire behavior, and three different fuel codes 
that classify fuel moisture content and relative fire behavior according to past 
and present observations of the weather on forest floor fuels. In other words, 
many relations between fire and external parameters are already known and 
documented by these indices, but they are rarely used by other systems, such as 
WSNs.

\subsection{Other scenarios}

Besides the scenario described above, there is a high number of WSNs 
combinations that are usually placed nearby. For example, the pollution levels 
in the cities may be related to the climate conditions and to the number of 
vehicles on the streets. Thus, systems that monitor pollution can predict events 
in their measurements if there is a traffic load variation or climate changes. 
Furthermore, traffic jams could be identified and pollution sensors should have 
their operation changed in order to provide more detailed measurements in these 
places during those periods of time. 

Another example in a metropolitan environment are intelligent 
transportation systems, which can make use of information exchange. 
For example, during rain, the vehicle traffic load may increase, as well as the 
probability of car accidents. Intelligent transportation systems that 
control traffic can benefit from this knowledge by increasing the number of 
active nodes, or by reducing the interval between two report transmissions 
during critical situations in order to produce more precise information.

Rural areas also have potential use cases in which inter-WSN information 
exchange can be applied. For example, information about temperature and rain 
might be useful for WSNs that are monitoring rivers and their water quality, or 
snow levels. These phenomena can represent changes in the environment as a whole 
and might increase the chances of predicting landslides, avalanches, variations 
in the water quality or river flooding. As a reaction, WSNs that monitor snow 
levels and the water quality level would change their workload according to the 
predictions. 

Finally, structural health monitoring networks, which are usually used both in 
cities and rural areas, could benefit from receiving information about climate 
changes and other phenomena (for example, earthquakes and volcanic activities), 
and adapt the interval between two measurements or the number of active nodes at 
a given time.

\end{comment}

\begin{comment}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.45\textwidth]{scenario-27}
	\caption{Chosen scenario}
	\label{fig:scenario}
\end{figure}

\section{Challenges}
\label{sec:challenges}

As this work follows a new paradigm with the collaboration between WSNs, it 
presents new challenges other than those faced by the current solutions, 
regarding the sources of the data, the type of results and 
the goals of the collaboration. 


\subsection{Joining WSNs that were not necessarily built to work together}

Working with information from external (and possibly unknown) sources requires 
some caution because: 
(1) the local WSN does not have any knowledge about how reliable the external 
information is; 
(2) the external information may refer to a different area other than the one 
monitored by the local WSN, which reduces its relevance to the local system;
(3) external WSNs have their own schedule for transmitting their data and it is 
not possible to change or predict this schedule from outside;
(4) the data may be represented in different types and parameters, for example, 
it may have different quality levels, such as a summary of the measurements done 
in the past few hours by several nodes or a fresh measurement done by one 
wireless sensor node.

Therefore, some extra effort is needed to check whether the data describes 
real, relevant and up-to-date situations, and this implies filtering wrong data 
before taking any internal action. In other words, the operation must not depend 
on the quantity of external data, but it might be enhanced when relevant 
information is available. 

\subsection{Predicting events instead of detecting them}

It is easier to measure the current status of the environment than concluding 
what is going to happen next. The proposed system will not be responsible for 
detecting events, but it should work with the probability of them occurring in 
the future. Moreover, real scenarios are under the influence of different 
factors that may happen regularly (for example, higher temperatures during the 
summer) or be sporadic, such as forest fires that, consequently, increase the 
local temperatures. Therefore, not only the correlation between different data 
types must be verified, but also the causes of the events of interest should 
be detected.

\subsection{Increasing the WSNs' lifetime}

Randomly turning off nodes to save energy is a naive solution that may result 
in missing relevant information about the environment. Instead, the system must 
measure how relevant the data provided by each node is, and use that information 
to choose the nodes that will be turned off to operate the WSN correctly when 
saving energy. This involves attending minimal requirements, such as covering 
certain areas and having a minimum number of measurements per area. Besides 
measuring and keeping the quality of the measurements, updates in the WSN 
operation must be applied at the right time in order to optimally react to the 
predicted situations.

Differently from the other challenges, this one also exists in the 
intra-network collaboration (described in Section~\ref{sec:related-work}), and 
existing solutions there may help to solve this issue.
\end{comment}



\section{Proposed Mechanism}
\label{sec:simulations}

Our system architecture is ready to use information from external WSNs, as 
described in~\cite{Pal2012} and~\cite{OechsnerVisions2014}.
To achieve the goal of optimizing the performance of the WSNs, they must be 
interconnected through their respective Enhanced Gateways (EGs). 
We explain the details of the mechanism in the following.





\subsection{Centralized Decisions}
\label{section:predictions}

Periodically, the data retrieved by the nodes are transmitted to the sink. 
After receiving all the measurements, the sink computes
the received values before reporting them to the \EG{}, which may forward them 
to external WSNs. In parallel, the \EG{} may also receive information from 
external WSNs and, up to this point, all the data are collected and stored for 
further analysis. In intervals, the \EG{} uses the collected data to predict if 
there will be changes in the near future. Figure~\ref{fig:steps} describes the 
possible states of a WSN. 

The predictions done by the \EG{} can have two different outcomes: 
\emph{positive}, when changes in the environment are expected; and 
\emph{negative}, otherwise.
If an \EG{} receives information from internal and external sources, each 
prediction may be based on a different data type and independent for each 
metric. In such cases, they can be combined in order to produce only one 
outcome.
The outcomes can be compared with the real observations in order to verify the 
performance of the predictions. 
The feedback can be incorporated by the \EG{s} in order to improve their 
future decisions.

\begin{figure}[h]
        \centering
        \begin{subfigure}[t]{0.21\textwidth}
                \centering
                \includegraphics[width=\textwidth]{step-1}
                \caption{State 1: Nodes report measured values to the sink.}
                \label{fig:step-1}
        \end{subfigure}\qquad
        \begin{subfigure}[t]{0.21\textwidth}
                \centering
                \includegraphics[width=\textwidth]{step-2}
                \caption{State 2: The sink transmits to the EG the sampled 
data, which 
                         can be forwarded to other EGs at the same time as they 
                         transmit to this WSN.}
                \label{fig:step-2}
        \end{subfigure}\qquad
        \begin{subfigure}[t]{0.21\textwidth}
                \centering
                \includegraphics[width=\textwidth]{step-3}
                \caption{State 3: EG computes a new plan for the nodes and 
transmits the 
                         new configuration to its sink.}
                \label{fig:step-3}
        \end{subfigure}\qquad
        \begin{subfigure}[t]{0.21\textwidth}
                \centering
                \includegraphics[width=\textwidth]{step-4}
                \caption{State 4: The sink updates the nodes' configuration.}
                \label{fig:step-4}
        \end{subfigure}\caption{Different states of a WSNs using inter-WSN information 
exchange}
        \label{fig:steps}
\end{figure}




\subsection{Applications}
\label{section:approaches}

Based on the outcome of its prediction, a \EG{} selects the new strategy that 
the WSN must follow and will be applied by the sink. 
At the end, the sink transmits to its nodes a new configuration that they must 
follow in the next time interval, which may be an instruction to (de)activate 
themselves or to change the sensing intervals:

\subsubsection{Adaptive sensor nodes selection}

This application reduces the energy consumption of the network by deactivating 
some nodes during a certain period of time. In other words, when a node is 
deactivated, it does not make any measurement, but it may forward messages 
exchanged by their neighbors.
We recall that the sets of active nodes can follow the guidelines described 
in~\cite{Yann-Ael2005}, so the energy savings can be maximized without 
compromising the quality of the measurements.

\subsubsection{Adaptive sampling}

Differently from the other application, this solution does not change the 
number of active nodes. However, when the \EG{} has a \emph{positive} outcome 
and changes are expected in the environment, the nodes should reduce the 
time between two consecutive measurement transmissions, consuming more energy 
and producing more information about the environment. Otherwise, the energy can 
be saved, because it is not expected big changes in the environment.

\subsection{Quality of Measurements (QoM)}
\label{sec:qom}

As explained before, one of the goals of this mechanism is to reduce the 
energy consumed in a WSN without reducing the QoM (i.e., a parameter that 
evaluates if the gathered information from the environment during a certain 
period is enough to accurately represent it). However, the level of the QoM 
depends on the type of information reported by the nodes.

We consider monitoring WSNs that make continuous transmissions to the 
sink and tolerate a small number of packet losses as well as delays between 
consecutive transmissions, but do not allow the reduction of the covered area 
because it might miss changes occurring in certain subareas. Therefore, we 
scaled the QoM as shown in Table~\ref{table:qom}. There, each interval with a 
\emph{positive} outcome should be covered by more reports, increasing the level 
of knowledge about the environment. Although a high number of measurements 
always represents a \emph{good} QoM, the intervals with a \emph{negative} 
observation can be covered by less reports without compromising the quality, 
thereby saving energy. Periods with a \emph{negative} observation that are 
wrongly predicted mean that the system expected to have a \emph{positive} 
observation in them, produced more measurements and, thus, wasted energy. 
Differently from the states that a \emph{positive} is observed and the WSN 
produced a low number of measurements, those periods still have a \emph{good} 
QoM, but the energy consumption might have been reduced and the WSN lifetime 
increased.

\begin{table}[h]
\centering
  \hspace*{0.5cm}
 \def\arraystretch{2}
\begin{tabular}{cc|c|c|c}
\cline{3-4}
& & \multicolumn{2}{ c| }{\textbf{Prediction outcome}} \\ \cline{3-4}
& & \emph{positive} & \emph{negative} \\ \cline{1-4}
\multicolumn{1}{ |c 
}{\multirow{2}{*}{\begin{minipage}{0.8in}\centering\textbf{Actual \\ 
observation}\end{minipage}} } &
\multicolumn{1}{ |c| }{\emph{positive}} & \cellcolor[gray]{0.8} GOOD & BAD &    
 \\ 
\cline{2-4}
\multicolumn{1}{ |c  }{}                        &
\multicolumn{1}{ |c| }{\emph{negative}} & GOOD & \cellcolor[gray]{0.8} GOOD &   
  \\ 
\cline{1-4}
\end{tabular}
\caption{Definition of QoM.}
\label{table:qom}
\end{table}

Based on this, the accuracy was defined as the percentage of intervals in a day 
in which the system was operating in a highlighted state. Moreover, the accuracy 
of \emph{positives} is the percentage of intervals with \emph{positives} 
covered by a high number of measurements.

Regarding the system operation, during intervals in which 
variations are predicted and the predictions have \emph{positive} outcomes, 
the \EG{} updates the operation of its WSN in order to collect more 
information. 
Each update on its operation affects either the number of active nodes or the 
time interval between two measurements done by the sensors. As a consequence of 
this, the number of measurements, the number of transmissions and the energy 
consumption have higher values during these periods of time, while the opposite 
effect occurs when no variation is predicted.

\subsection{Performance score}
\label{sec:performance}

In order to evaluate how efficient the use of external information can be, we 
developed a way to compare the approaches. For a given scenario, we calculate 
the lowest energy consumption that the WSN may have ($E_\text{min}$), which 
can be done by always setting the plan that produces less measurements during a 
day. On the other hand, we measure how much energy is consumed by the WSN when 
it produces the maximum number of measurements during the same time interval 
($E_\text{max}$). Thus, the percentage of energy saved by an approach 
($E_\text{ps}$) is derived from the energy consumed ($E_\text{consumed}$) by 
the 
relation:

\begin{equation}
E_\text{ps} = \frac{E_\text{max} - E_\text{consumed}}{E_\text{max} - 
E_\text{min}}
\end{equation}

A correct prediction about a \emph{negative} observation means that the 
system is producing less measurements and saving energy. Therefore, this 
accuracy factor is implicitly inserted in the value of $E_\text{ps}$ and should 
not be considered again in the final equation. Considering this, the trade-off 
between the QoM and the energy consumption can be calculated if we use only the 
percentage of predictions of \emph{positive} outcomes ($P_\text{\highDelta{}}$) 
that the system could successfully do:

\begin{equation}
P_\text{\highDelta{}} = \frac{\text{\# of positives correctly 
predicted}}{\text{\# of observed positives}}
\end{equation}

Finally, the \emph{Performance score} ($p$) is defined as the product 
between the percentage of saved energy and the percentage of \emph{positives} 
correctly predicted, which quantifies how much the system actually 
consumes to have such level of accuracy. If interpreted as a dot product 
between two vectors, the highest value represents the system having the highest 
possible energy savings and the highest possible accuracy \highDelta{}s:

\begin{equation}
p_{(\alpha)} =  {{E_\text{ps}}^\alpha } \cdot { 
{P_\text{\highDelta{}}}^{(1 - \alpha)} }
\label{eq:performance}
\end{equation}
where $0 \leq \alpha \leq 1$ is the exponent that represents the system's 
priority on the energy saved over its accuracy. For example, if $\alpha < 0.5$, 
the energy savings will have a bigger impact at the performance score. 
Obviously, if $\alpha = 0.5$, the system will not prioritize any of them. 

\section{Use Case}
\label{sec:current-simulations-scenario}

To create a realistic use case, we used the temperature and relative humidity 
of $16$ days measured by three different nodes in the experiments done 
in~\cite{Yann-Ael2005}. 
The simulated use case is based on a real scenario from where the data was 
fetched: an office with two WSNs deployed close to each other. 
There, nodes are positioned in a grid topology with two different WSNs: 
\emph{Network A} monitoring temperature and \emph{Network B} monitoring 
relative humidity.

\emph{Network A} has one node that retrieves data from the environment, and a 
sink node that receives the temperature values and transmits them to the 
respective EG (\EG{A}), which forwards everything to \EG{B}. On the other side, 
\emph{Network B} was composed by $26$ nodes that monitor the relative humidity 
plus a sink connected to \EG{B}, which is responsible for averaging the values 
received after each measurement. Based on the data received from \EG{A} and on 
the stored averages, \EG{B} is able to set different WSN operation plans, and to 
communicate the required changes to its sink node in order to forward them to 
the wireless sensor nodes.

\subsubsection{Adaptive sensor nodes selection}

We manually created three different sets of active nodes for the \emph{Network 
B}: One with half of the nodes plus the sink; another with the other half plus 
the sink; and the last one with all nodes together. The first two plans are 
used for saving energy and are switched on every update to extend the WSN's 
lifetime, while the goal of the all-nodes plan is to provide more information 
about the environment. The downside is that this plan consumes more energy. 
Therefore, the latter is only used when the prediction produce 
\emph{positive} outcomes and the environment is expected to change. 

\subsubsection{Adaptive sampling}

When the prediction outcome is a \emph{positive} and changes are expected in 
\emph{Network B}, nodes take measurements and transmit them every $30$ seconds, 
consuming more energy and producing more information about the environment. 
Otherwise, this is done every $180$ seconds.

\subsection{Constant Predictions}
\label{sec:system-goal-definition}

At runtime, \emph{Network B} defines how its nodes will react to environmental 
changes based on the predictions done: reporting more information when the 
environment is supposed to undergo variations and saving energy otherwise.
In order to predict these variations, we calculated the average of the 
temperature and relative humidity values, without mixing data types, in 
discrete and sequential $5$-minute window intervals. The absolute difference 
between the averages of two consecutive intervals is denoted \ourDelta{}.
In order to identify the data types, we used subscripts: \ourDelta{T} 
for temperature values and \ourDelta{RH} for relative humidity values.
We have assumed that a large difference between the averages 
represent significant changes in the environment.
Therefore, the system goal is to predict whether the next \ourDelta{} will be 
over a determined threshold, \threshold{}, or not. 
To achieve that, we used a constant na\"ive model to make the predictions, 
i.e., in case of $\ourDelta{} > \threshold{}$, we label it as \highDelta{}, 
representing a \emph{positive} outcome; 
otherwise, we call it a \lowDelta{}. 

In some cases, it may be useful to know if a \highDelta{} means that the 
average is increasing or decreasing. In order to identify it, we added an 
additional notation to \ourDelta{}. If the most recent average computed 
differs more than \threshold{} and is greater 
than the penultimate one, we mark it as \highDelta[+]{}; 
if it differs more than \threshold{} but is lower, we use 
\highDelta[-]{}, as shown in Figure~\ref{fig:delta}.
In case of having a \lowDelta{}, there is no need for highlighting if the value 
is greater or less than the penultimate one.

\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.21\textwidth}
		\centering
		
\includegraphics[width=\textwidth]{delta-1}
		\caption{The concept of \lowDelta{}}
		\label{fig:delta-low-plus}
	\end{subfigure}\qquad
	\begin{subfigure}[t]{0.21\textwidth}
		\centering
		
\includegraphics[width=\textwidth]{delta-2}
		\caption{Another case of \lowDelta{}}
		\label{fig:delta-low-minus}
	\end{subfigure}\qquad
	\begin{subfigure}[t]{0.21\textwidth}
		\centering
		
\includegraphics[width=\textwidth]{delta-3}
		\caption{The concept of \highDelta[+]{}}
		\label{fig:delta-high-plus}
	\end{subfigure}\qquad
	\begin{subfigure}[t]{0.21\textwidth}
		\centering
		
\includegraphics[width=\textwidth]{delta-4}
		\caption{The concept of \highDelta[-]{}}
		\label{fig:delta-high-minus}
	\end{subfigure}\caption{How the system labels the \ourDelta{}s.}
	\label{fig:delta}

\end{figure}

Predictions are independent for each metric.
Furthermore, any prediction is 
composed by three factors: the last two symptoms and the last prediction. The 
general idea is to try to learn the trend and avoid wrong predictions provoked 
by noise and outliers. Thus, every time that two factors agree in one 
direction, the prediction is that, in the next interval, the environment will 
follow it. 
Otherwise, if the three factors are different, the prediction is that the 
environment will not undergo variations in the near future. 
Table \ref{table:system-states} shows how we did the predictions using
\ourDelta{}s.

\begin{table}[h]
 \centering
\begin{tabular}{cccc}
\toprule
\multicolumn{2}{c}{\textbf{Last Symptoms}} & \textbf{Last Prediction} & 
\textbf{Prediction} \\ \midrule

\lowDelta{}         & \lowDelta{}            & any                  
& 
\lowDelta{}           \\ \midrule

\highDelta[+]{}     & \highDelta[+]{}    
  & any                  & \highDelta[+]{} \\ 
\midrule
\highDelta[-]{}     & \highDelta[-]{}    
  & any                  & \highDelta[-]{} \\ 
\midrule

\highDelta[+]{}     & any            & 
\highDelta[+]{} & 
\highDelta[+]{} \\ \midrule

\highDelta[-]{}     & any            & 
\highDelta[-]{} & 
\highDelta[-]{}                 \\ \midrule

\lowDelta{} & any           & \lowDelta{}        
         & \lowDelta{} \\ \midrule

\highDelta[+]{}     & \highDelta[-]{}        
& \lowDelta{}                 & \lowDelta{}                 \\ 
\midrule
         \highDelta[+]{}     & \lowDelta{}            & 
\highDelta[-]{} & \lowDelta{}                 \\ \midrule
\highDelta[-]{}     & \lowDelta{}            & 
\highDelta[+]{} & \lowDelta{}                 \\ \bottomrule
\end{tabular}
\caption{How the system reacts to the symptoms.}
\label{table:system-states}
\end{table}


Finally, if a \emph{EG} receives information from internal and external 
sources, each prediction may be based on a different data type.  In this case, 
it combines them in the simplest way: if one of the predictions is labeled as 
\highDelta{}, the final prediction is a \highDelta{}; otherwise, it is a 
\lowDelta{}.

\subsubsection{Adaptive threshold}

The value of \threshold{} is set based on the proportion of \ourDelta{}s seen 
in the historical data. For example, if the goal is to predict the highest 
quarter of \ourDelta{}s in a day, the threshold will be set at the $75$th 
percentile of \ourDelta{}s. In this case, we identify it with the number $75$ 
subscripted: \threshold[75].
\begin{comment}
As an example, Figure~\ref{fig:plot_rectangles} shows the measurements of the 
temperature and relative humidity sensors done during a day, 
and the blue background represents the intervals with \highDelta{RH}s, 
considering \threshold[70]. During these intervals, the \emph{Network B} should 
have 
activated a plan to report more data and produced more detailed 
information, since significant changes in the environment are 
generally more interesting and critical to report.


\begin{figure}[h]
	\centering
	\includegraphics[width=0.4\textwidth]{plot_rectangles2}
	\caption{Values observed during a day.}
	\label{fig:plot_rectangles}
\end{figure}
\end{comment}

\subsubsection{Symptoms}
\label{sec:symptom-definition}

To make those predictions, we must observe the measurements and find 
symptoms. A symptom, \symptom{}, is defined as a value where a 
$\ourDelta{} > \symptom{}$ represents a high probability of having 
$\ourDelta{} > \threshold{}$ in the next interval. Therefore, if we 
notice that the most recent \ourDelta{} is greater than \symptom{}, we have a 
symptom of \highDelta{}; otherwise, it is a symptom of \lowDelta{}.
Even though the concepts of \symptom{} and \threshold{} are similar, the 
numerical values may be different. For example, after observing the historical 
data, we might notice that every $\ourDelta{} > \threshold[40]$ calculated at 
time $t$ was followed by a $\ourDelta{} > \threshold[75]$ at time $t + 1$. So, 
we would set the value of \symptom{} at the $40$th percentile of \ourDelta{}s.



\section{Evaluation}
\label{sec:evaluation}


We considered each measurement done by the real nodes as the average of the 
network measurements in our simulations. 
Moreover, each set of measurements done by a node in a day was considered one 
day's worth of data. Therefore, we had enough data to simulate $48$ different 
days.
To check the feasibility of using this solution in the presented 
scenario, we evaluated the energy consumption in OMNeT++~\cite{Varga2001} and 
the calculations about the performance score in Matlab.
First, using OMNeT++ and MiXiM~\cite{Kopke}, we simulated the energy 
consumption based on TelosB nodes~\cite{Platform} using BMAC~\cite{Fakih2006} as 
MAC protocol and a flooding routing protocol. 
In these simulations, the sensor nodes received new plans from the \EG{} every 
$5$ minutes, as explained in Section~\ref{section:approaches}. We calculated 
the average energy consumption on each plan, considering also the energy spent 
to disseminate the plan changes through the network. 

In Matlab, the data from the sensors were split into a training and a validation 
datasets to avoid overfitting. Each of these datasets was defined by a set of 
$24$ days that were randomly selected on each run (repeated random sub-sampling 
validation). The model was fit to the training data, and predictive accuracy was 
assessed using the validation data. The tests were done over $10$ different 
combinations of days and the final results were averaged over the splits. In the 
end, we checked how the system behaved when the plan of \emph{Network B} was 
selected using only internal information (relative humidity values), only 
external information (temperature values) and combining both, and used the 
energy consumption levels to plot the results. 

\subsection{Training dataset}

After selecting $24$ days for the training dataset, the measured values were 
used to set three different parameters: 

\begin{itemize}
 \item \textbf{The value of \threshold{}} -- The threshold that the 
\EG{}s must set. It was calculated as explained in 
\ref{sec:system-goal-definition}, based on the measurements done during the 
training days.
 \item \textbf{The values of \symptom{}s} -- The system built a table with the 
values of $p$ based on percentiles, as shown in 
Figure~\ref{fig:performance-score}. The 
numerical value of \symptom[T] and \symptom[RH] was the same as the 
percentiles of \ourDelta{T} and \ourDelta{RH}  with the highest value 
of $p$.
\end{itemize}

We assumed that the saved energy and the accuracy of the system have similar 
importance and set the value of $\alpha = 0.5$ in Equation~\ref{eq:performance}.
Figure~\ref{fig:performance-energy-consumption} shows how much energy can be 
saved based on the thresholds that are used as symptoms of future changes. For 
example, at the point $(40, 20)$, any \ourDelta{RH} over the $40^\text{th}$ 
percentile (i.e., greater than $40\%$ of the values) is considered as a symptom 
of change, as well as any \ourDelta{T} over the $20^\text{th}$ percentile. When 
a symptom is detected, the EG may launch a plan to produce more measurements in 
the next time-interval and, consequently, consume more energy.
Figure~\ref{fig:performance-accuracy} shows the total accuracy of the 
predictions and Figure~\ref{fig:performance-high-accuracy} shows how the 
accuracy of \highDelta{}s changes depending on the threshold chosen to represent 
a symptom of changes in the future.

\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.21\textwidth}
		\centering
		
\includegraphics[width=\textwidth]{energy_savings-cv0_4}
		\caption{Energy saved ($E_\text{ps}$)}
		\label{fig:performance-energy-consumption}
	\end{subfigure}\qquad
	\begin{subfigure}[t]{0.21\textwidth}
		\centering
\includegraphics[width=\textwidth]{accuracy-cv0_4}
		\caption{Accuracy of the predictions}
		\label{fig:performance-accuracy}
	\end{subfigure}\qquad
	\begin{subfigure}[t]{0.21\textwidth}
		\centering
		
\includegraphics[width=\textwidth]{accuracy_high-cv0_4}
		\caption{Accuracy of \highDelta{}s ($P_\text{\highDelta{}}$)}
		\label{fig:performance-high-accuracy}
	\end{subfigure}\qquad
	\begin{subfigure}[t]{0.21\textwidth}
		\centering
		
\includegraphics[width=\textwidth]{evaluation-cv0_4}
		\caption{$p_{(0.5)}$}
		\label{fig:performance-score}
	\end{subfigure}\caption{Parameters obtained using the training data.}
	\label{fig:performance-process}

\end{figure}


\subsection{Validating dataset}

The other $24$ days were considered part of the validating dataset and their 
data were used to validate whether the system had chosen well and whether our 
hypothesis was valid. For this, the system used all the parameters 
calculated in the last step to calculate~$p$.






\subsection{Results}



The plots in Figure~\ref{fig:results} show the obtained results, where it is 
possible to see how much our solution was able to exploit the trade-off between 
the energy consumption and the quality of the measurements. To show better its 
benefits, we included two baseline scenarios that did not use collaboration: the 
first one saved the maximum energy possible by transmitting less measurements; 
the second did not save energy and always used the plan that transmits more 
measurements. An important remark is that both scenarios have $p_{(\alpha)} = 0$ 
for any $\alpha$, because either they did not save any energy (the highest 
consumption plan case) or their accuracy of detecting \highDelta{}s was zero 
(the lowest consumption plan case).

\begin{figure}[h]
	\centering
\begin{subfigure}[t]{0.4\textwidth}
		\centering
		
\includegraphics[width=\textwidth]{energy-2}
		\caption{Adaptive sampling.}
		\label{fig:results-energy-2}
	\end{subfigure}\qquad
	\begin{subfigure}[t]{0.4\textwidth}
		\centering
		
\includegraphics[width=\textwidth]{energy-3}
		\caption{Adaptive sensor nodes selection.}
		\label{fig:results-energy-3}
	\end{subfigure}\qquad
	\begin{subfigure}[t]{0.4\textwidth}
		\centering
		
\includegraphics[width=\textwidth]{accuracy-high}
		\caption{Accuracy of the predictions of \highDelta{}s.}
		\label{fig:results-accuracy_high}
	\end{subfigure}\qquad
	\begin{subfigure}[t]{0.4\textwidth}
		\centering
		
\includegraphics[width=\textwidth]{eval-2-half}
		\caption{Performance score of each approach.}
		\label{fig:results-eval}
	\end{subfigure}

	\centering
	\includegraphics[width=0.9\textwidth]{legend-2}
	\label{fig:results-legend}
	
	\caption{Simulation results.}
	\label{fig:results}
	
\end{figure}

\begin{comment}
To show better the benefits of the developed solution, we represent the 
collected results together with two baseline scenarios that did not use 
collaboration. In the first scenario the nodes always produced less 
measurements, and the opposite in the second one. Thus, by using them, we can 
show the lowest and the highest possible energy consumption values. An 
important remark is that these two scenarios always have $p = 0$, since either 
they did not save any energy (the highest consumption plan case) or their 
accuracy of detecting \highDelta{}s was zero (the lowest consumption plan case). 
Therefore, these plans can be considered as references and give some idea about 
how much our solution was able to exploit the trade-off between the two 
extremes.
\end{comment}

The results are split into three groups, according to the \threshold{} set for 
each case (\threshold[70], \threshold[60] and \threshold[50]). Each bar 
represents an average for the $24$ days of the validation dataset. Observing the 
data, we can see that the correlation between temperature and relative humidity 
values is closer to $-1$ when we consider only the highest \ourDelta{}s, i.e., 
\threshold[70]. Therefore, we assume that there are other factors that may 
influence the small variations in the relative humidity, such as the presence of 
persons close to the sensors. This explains why the percentage of \highDelta{}s 
correctly predicted is lower when the system tries to track a higher number of 
changes (\threshold[50]).



In Figures~\ref{fig:results-energy-2} and \ref{fig:results-energy-3}, we 
can observe that, when we used only the plan that changed the number of active 
nodes, the system spent around $54\%$ of the energy compared to the scenario in 
which the network was always producing more measurements. Also, 
Figure~\ref{fig:results-accuracy_high} shows that predictions can successfully 
improve the WSNs' operation. It is possible to see that, using only the relative 
humidity values as a reference (absence of external collaboration), $42.3\%$ of 
the $5$-minute intervals with \highDelta{RH}s were correctly predicted with 
\threshold[60]. Compared to that, we can observe that the energy consumption 
increased much less than the accuracy levels. For example, with \threshold[60], 
using the combination of internal and external information, the system was able 
to correctly predict $67.9\%$ more \highDelta{}s consuming only $33.5\%$ more 
energy. This means that the energy was used more intelligently in the second 
case.

Figure~\ref{fig:results-eval} shows that our approach for inter-WSN information 
exchange outperforms the other types of collaboration that use less information 
and spend their energy less efficiently. In summary, the trade-off between 
energy consumption and QoM was achieved and found to produce more effective 
results than the other approaches.









\section{Conclusion and Future Work}
\label{sec:conclusion}

Based on the presented results, it is possible to determine that our mechanism 
is able to use internal and external information to optimize the WSNs' 
performance, which is illustrated by the difference in the values of $p$. During 
the tests, we have also noticed that these improvements could be achieved only 
with data that is not only highly correlated, but there must also be a relation 
of causation between them. 
In this case, we noticed that changes in temperature led to changes in relative 
humidity, but the opposite was not necessarily true. Therefore, it would be 
more complex to make good predictions if we tried to predict 
temperature changes based on relative humidity values.

Although we made use of real data from existing experiments, we did generic 
calculations and assumptions that can be extended to numerous scenarios, in 
order to prove the general idea of this concept. We expect that specific 
knowledge about different scenarios may lead to better results. For example, as 
shown in \cite{Lawrence2005}, when the relative humidity is over $50\%$, it is 
possible to calculate its value based on information about the temperature 
only. Thus, in a scenario similar to ours, the system could save 
even more energy by letting the EG calculate the local data based on 
external information.




The next steps include adapt this solution to an autonomic system, as described 
in~\cite{6583430}. That is, a more generic mechanism which is able to work with 
other WSN types and is able to work with other prediction methods that may have 
better performance in different scenarios.
Additionally, the idea of an autonomic solution involves a pro-active and 
self-managing system, which improves the information fusion and the decision 
optimization, besides creating specific plans for the WSNs according to the 
predictions about the near future.










\section*{Acknowledgment}
This work has been partially supported by the Spanish Government through the 
project TEC2012-32354 (Plan Nacional I+D), by the Catalan Government 
through the project SGR2009\#00617 and by the European Union through the 
project FP7-SME-2013-605073-ENTOMATIC.







\bibliographystyle{unsrt}

\bibliography{bibliography}



\end{document}
