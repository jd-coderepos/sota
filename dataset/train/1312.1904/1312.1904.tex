
\documentclass[11pt,draftcls,onecolumn]{IEEEtran}


\usepackage{epsfig,cite}
\usepackage{amsmath,amssymb}
\usepackage{arydshln}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{assumption}[theorem]{Assumption}


\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\field}[1]{{\mathbb{#1}}}

\newcommand{\Prob}{\mathrm{Prob}}
\newcommand{\EndProof}{\hfill \mbox{}}
\newcommand{\End}{\hfill \mbox{}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\beq}{}
\newcommand{\Real}[1]{ { {\bf R}^{#1} } }
\newcommand{\Complex}[1]{ { {\bf C}^{#1} } }
\newcommand{\R}{\field{R}}
\newcommand{\Z}{\field{Z}}
\newcommand{\E}{\field{E}}

\newcommand{\fig}[3]{\resizebox{#1}{#2}{\includegraphics{#3}}}
\newcommand{\figdir}{./Figures}

\begin{document}

\title{The PageRank Problem, Multi-Agent Consensus and Web Aggregation\\
{\Large A Systems and Control Viewpoint}
}

\author{Hideaki Ishii
and
Roberto Tempo
\thanks{This work was supported in part by the Ministry of Education, Culture, Sports, 
Science and Technology, Japan, under Grant-in-Aid for Scientific Research Grant 
No.~23760385, in part by the Aihara Project, the FIRST program from JSPS, initiated
by CSTP, and in part by the European Union
Seventh Framework Programme [FP7/2007-2013] under grant agreement n.257462 HYCON2 Network
of Excellence.}
\thanks{H.~Ishii is with the Department of Computational Intelligence 
and Systems Science, Tokyo Institute of Technology,
4259 Nagatsuta-cho, Midori-ku, Yokohama 226-8502, Japan.
(e-mail: ishii@dis.titech.ac.jp).} 
\thanks{R.~Tempo is with CNR-IEIIT, Politecnico di Torino, 
Corso Duca degli Abruzzi 24, 10129 Torino, Italy.
(e-mail: roberto.tempo@polito.it).}}


\maketitle



PageRank is an algorithm introduced in 1998 and used by the Google Internet search engine.
It assigns a numerical value to each element of a set of hyperlinked documents 
(that is, web pages) within the World Wide Web with the purpose of measuring 
the relative importance of the page \cite{google:13}. 
The key idea in the algorithm is to give a higher PageRank value to 
web pages which are visited often by web surfers.
On its website, Google describes PageRank as follows:
``PageRank reflects our view of the importance of web pages by considering 
more than 500 million variables and 2 billion terms. Pages that are considered
important receive a higher PageRank and are more likely to appear at 
the top of the search results."

Today PageRank is a paradigmatic problem 
of great interest in various areas, such as
information technology, bibliometrics, biology, and e-commerce, 
where objects are often ranked in order of importance. 
This article considers a distributed randomized approach based on 
techniques from the area of Markov chains 
using a graph representation consisting of nodes and links.
We also outline connections with other 
problems of current interest to the systems and control community, 
which include ranking of control journals, consensus of multi-agent 
systems, and aggregation-based techniques.

\newpage
\section*{The PageRank Problem and Its History}

The PageRank algorithm was introduced by the cofounders of Google 
in the late 1990s \cite{BriPag:98} and has been implemented on the search engine of Google
from the time of its launching. 
It continues to be part of the search engine at Google and is said to be
one of the 200 signals used for narrowing down the search results \cite{google:13}
(with a link to the original paper \cite{BriPag:98}).
PageRank indicates the importance of a web page determined by the hyperlink structure 
of the web. Specifically, it is determined by the number of hyperlinks pointing to 
the page as well as the importance of the pages where those hyperlinks originate.
Related ideas for ranking objects had been previously used in other contexts, 
such as sociometry \cite{Hubbell:65}, and bibliometrics \cite{PinNar:76},
and they can be traced back to 1941 
to studies of quantitative data in economics \cite{Leontief:41}.
More recently, ideas from PageRank have been used to rank other objects in order of importance, including scientific papers linked by citations \cite{CXMR:09}, authors related by co-authorship 
\cite{LiBoNede:05}, professional sport players 
\cite{Radicchi:11}, and protein in systems biology \cite{ZaBeEf:12}. 
Refer to \cite{LanMey:06} for an introduction to the PageRank problem and to
\cite{Franceschet:11} for historical notes.

The PageRank problem recently attracted the attention of the systems and control community. 
In this context, the problem was first introduced and studied in \cite{IshTem:10}, where 
a randomized decentralized approach has been proposed. Such an approach is meaningful
in view of the computational difficulties due to the size of the problem and in the
web environment where computational resources are available. 
In the abovementioned paper, the mean-square ergodic convergence properties of 
the distributed algorithms have been analyzed. 
Other convergence properties have been 
subsequently studied in \cite{ZhaChFa:13}, where almost sure convergence of the same 
decentralized scheme is demonstrated using techniques of stochastic approximation algorithms.
In \cite{NazPol:11}, a randomized algorithm based on stochastic descent is proposed 
and an explicit bound on the convergence rate is computed. 
The fluctuations of the PageRank values in the presence of fragile and uncertain 
links have been studied in \cite{IshTem_sice:09}. 
In \cite{CsaJunBlo:09}, an approach based on 
Markov decision processes is developed, and optimization and robustness viewpoints are followed 
in \cite{FABG:13} and \cite{JudPol:12}.
Motivated by the PageRank problem, the recent works \cite{Nesterov:12,Necoara:13} present
randomized distributed algorithms for solving 
huge-scale convex optimization problems with underlying sparse network structures.
Other recent references on PageRank are listed in \cite{IshTemBai:12},
where a web aggregation approach is studied, and in \cite{IshTemBai_scl:12}, where
intermittent communication in the distributed randomized computation is dealt with.
Finally, as we will see later, the PageRank problem can be viewed as finding
an eigenvector of an adjacency matrix for a web graph, 
and there have been research efforts to do this decentrally in the 
area of multi-agent systems. (See, for example, \cite{KibCom:12}.)




\section*{The Hyperlink Matrix and the Random Surfer Model}

In this section, a network consisting of web pages and hyperlinks connecting them
is described based on a graph theoretic approach.
Consider a network of  web pages indexed by integers from 1 to , where
 to avoid trivial situations.
This network is represented by the graph ,
where  is the set of vertices corresponding 
to the web page indices and  is the set of edges representing
the links among the pages. 
The vertex  is connected to the vertex  by an edge, that is,
, if page  has an outgoing link to page ,
or equivalently page  has an incoming link from page .
Since the edges have directions, the graph  is said to be directed.
In particular, the index set of pages linked to page  is given by  and  is the number of outgoing links of page . 

Then, the {\it hyperlink matrix}  is defined as

The hyperlink matrix  has some important properties. First, it is a nonnegative matrix, 
that is, all of its entries   are nonnegative. This property is expressed as

Note that the link matrix is column-wise normalized by construction. 
However, in the real web, pages having no links to others are abundant and
are referred to {\it dangling nodes}. 
Such pages can be found, for example, in the form of PDF and image files
having no outgoing hyperlinks. These pages introduce zero columns into the link matrix. 
Then, it can be easily verified that the 
hyperlink matrix  is a {\it column substochastic} matrix, that is, 
it is a nonnegative matrix  having  
the property that  for .

A problem related to such nodes can be explained through 
the example web in Fig.~\ref{fig:graph} (a). Here, the dangling node 5 behaves as a ``black hole,''
that is, the entire flow of information enters into this node and cannot escape.
Thus, the PageRank value of this page does not contribute to other pages.
To resolve this problem, the graph and consequently the matrix 
need to be redefined by adding artificial links for 
all dangling nodes. As a result of this modification, the columns with only zero entries 
are removed from the link matrix , and this matrix  becomes a {\it column stochastic} matrix, 
that is, it is a nonnegative matrix 
with the property that  for .

The modification in the graph can be easily explained by introducing the concept of ``back button''
(that is, one returns to the previous page when visiting a dangling node).
Other methods to handle the issue have also been proposed such as replacing the zero columns
with any fixed stochastic vector and grouping all dangling nodes in one node \cite{LanMey:06,IpsSel:07}.
This is illustrated in the simple example in Fig.~\ref{fig:graph}
with a graph consisting of six web pages and twelve links. 
This example will be used throughout the article.



\begin{figure}[t]
  \centering
  \resizebox{6cm}{!}{\includegraphics{web_6_pages1.eps}}
  \caption{The example web with six pages. (a)~Page 5 is a dangling node. 
           (b)~This is changed by addition of a back link (red) to page 6. 
            Notice that no self-loop is present.
}
\label{fig:graph}
\end{figure}

\begin{example}\label{ex:1}\rm
Consider the web of six pages shown in Fig.~\ref{fig:graph}\;(a). 
Note that page 5 is a dangling node since it has no outgoing links.
As previously discussed, the graph is modified by adding a link from page 5 to
page 6 (Fig.~\ref{fig:graph}\;(b)). Then, the hyperlink matrix becomes a column stochastic matrix

\End
\end{example}

\vspace{.5cm}

One way to describe the PageRank problem is through 
the {\it random surfer model}: Starting from a node at random, 
the random surfer follows the hyperlink structure by picking a link at random. For example, 
if the node 3 in Fig.~\ref{fig:graph} is chosen, then the surfer goes to node 4 with probability 1/3 because 
node 3 has three outgoing links, and all links are considered equiprobable. Then,
from node 4, the surfer reaches node 6 with probability 1/3 because node 4 has three outgoing links. 
This process is then repeated. If the random surfer eventually reaches node 5, then the surfer may 
return to node 6  with probability one using the back button shown in red in 
Fig.~\ref{fig:graph}\;(b).

Mathematically, this process is described as a Markov chain 

where  is a vector representing the values of the pages. 
The initial values are normalized so that  is 
a stochastic vector, that is,
. 
Since the matrix  is stochastic,  is a real number 
in  for all  and the values are normalized so that 
 for all . 
In fact,  is the probability of being in
node  at time .

In the context of the random surfer model, the PageRank values of the web pages
represent some measure on how often the web pages are visited by the random surfer. 
More precisely, the probability of visits becomes higher if web sites have links 
from {\it important} (that is, often visited) web sites and especially those
that have smaller numbers of outlinks.
In this context, 
page  is said to be more important than page  at time  if .

An interesting question is whether or not the Markov chain asymptotically converges to its 
stationary distribution for any initial nonzero values , .
Such a distribution represents the probabilities of the random surfer visiting the
web sites, indicating the popularity of the page. 
Hence, we establish properties under which 
the vector  in equation (\ref{markov}) asymptotically converges as

where  is referred to as the {\it PageRank value}. 
Equivalently, if convergence is achieved, then it is written 


While the previous discussion is based on a Markov chain interpretation, there is a 
simple alternative linear systems interpretation. Assuming that convergence is achieved, 
the PageRank value  is a nonnegative unit eigenvector corresponding to the eigenvalue  
of the hyperlink matrix . More formally, the PageRank problem is to compute  
such that

where  and . In general, for existence and uniqueness 
of the eigenvector ,  it is sufficient that the web as a graph is strongly connected
\cite{HorJoh:85}, that is, for any two vertices , there is a sequence of
edges which connects  to . In other words, from every page, 
every other page can be reached through a connected path. 
In terms of the link matrix , strong connectivity 
of the graph is equivalent to  being an irreducible matrix; 
see ``Definitions and Properties of Stochastic Matrices.''
For stochastic matrices, there exists at least one eigenvalue equal to 1. 
However, it is easy to verify that the web is in general not strongly connected
since there are many pages in the web that cannot be visited from another page
by simply following links. Therefore, the problem should be 
modified as described in the next section.


\section*{The Teleportation Model}

The idea of the teleportation model is that the random surfer, after a while, becomes bored and 
he/she stops following the hyperlink structure as previously described. That is, at some time instant, 
the random surfer ``jumps" to another page not directly connected to the one currently being visited. 
The new page can be in fact completely unrelated 
topically or geographically to
the previous page. All  pages have the same probability  to be reached 
by the jump. For example, in Fig.~\ref{fig:graph} the random surfer may be teleported from 
node 5 to node 1, and the probability to reach node 1 is 1/6 because all nodes are equiprobable.

Mathematically, the teleportation model is represented as a convex combination of two matrices. 
Let  be a parameter such that , and let the modified link matrix  
be defined by

where  is a vector whose entries are all equal to  
and thus  is a rank one matrix with all entries being . 
Equation \eqref{eqn:M} is often referred to as the {\it PageRank equation}. The matrix  therefore 
is the convex combination of the original hyperlink matrix  and the matrix 
. The latter matrix indicates that the probability of the jump is equal for all web pages, 
that is, every page can be reached by teleportation with uniform probability equal to .
In the original algorithm \cite{BriPag:98}, 
a typical value was indicated to be . 
In this article, following classical literature \cite{LanMey:06},
the same value is used
(some comments regarding why this value is used are provided at the end of this section).
Notice that  is a stochastic matrix with all positive entries
because it is a convex combination of 
two stochastic matrices by  and  is a positive matrix.

By Perron's theorem \cite{HorJoh:85}, 
this matrix is a primitive matrix, 
which is an irreducible matrix having only one eigenvalue of maximum modulus.
In particular, the eigenvalue 1 is of multiplicity~1 and 
is the unique eigenvalue with the maximum absolute value. Furthermore,
the corresponding eigenvector is positive; 
for more details, see ``Stochastic Matrices and Perron's Theorem.''
Therefore, the PageRank vector  is redefined by using  in place of  so that

where  and . 
It now follows that the asymptotic probability  of being in a state is independent of the 
initial state . Intuitively, a positive stochastic matrix represents 
a Markov chain that is not periodic and have no sinks states.
This means that the application of the stochastic matrix to a probability 
distribution would redistribute the probability mass of the original distribution 
while preserving its total mass. If this process is applied repeatedly the distribution converges 
to a stationary distribution for the Markov chain.

Example~\ref{ex:1} is now revisited to show the computation 
of the matrix  and of PageRank .

\begin{example}\label{ex:2}\rm
The matrix  can be computed from equation (\ref{eqn:M}) as

Observe that  is not a sparse matrix and its diagonal entries are non-zero; 
see further comments later in this section.
The PageRank vector  in \eqref{eqn:prvec} is

Notice that pages~4 and 6 have the largest number of incoming links,
resulting in large PageRank values.
Page~6 is more advantageous because the pages contributing to its value
via links, that is, pages~3, 4, and 5, 
have larger values than those having links to page~4. 
Page~1 has the smallest number
of incoming links and obviously the lowest ranking in this web. 
It is interesting that pages~4 and 5 share the same value. 
\End
\end{example}

\vspace{.5cm}

\centerline{
\fbox{\parbox{16cm}{
{\Large Definitions and Properties of Stochastic Matrices}\\ \\
A matrix  in which all entries are nonnegative real numbers 
is said to be nonnegative, and it is denoted as ; a matrix whose entries are 
positive real numbers is called positive, denoted as . 
A stochastic matrix (also termed probability matrix or Markov matrix) 
is a matrix used to describe the transitions of a Markov chain. Each of its entries 
is a nonnegative real number representing a transition probability. \\ \\
A column stochastic matrix is a matrix with each column summing to one, 
and a row stochastic matrix is a matrix with each row summing to one. 
A doubly stochastic matrix has the property that each row and column sum to one. 
A stochastic vector (also called probability vector) is a vector whose elements 
are nonnegative real numbers which sum to one.\\		\\
A matrix  is said to be reducible if either (i)~ and  or (ii)~ and there exist
a permutation matrix  and an integer 
with  such that

where , , and 
. An irreducible matrix is a matrix that is not reducible.\\ \\
A nonnegative matrix is said to be primitive 
if it is irreducible and has only one eigenvalue of maximum modulus.
In the terminology of Markov chains, these conditions correspond 
to a chain being irreducible and aperiodic.
} }}

\vspace{.5cm}

\centerline{
\fbox{\parbox{16cm}{
{\Large Stochastic Matrices and Perron's Theorem}\\ \\
Let  be a nonnegative stochastic matrix. Then, 1 is an eigenvalue of  and 
there is a nonnegative eigenvector , such that .
In this case, the eigenvector is not necessarily uniquely determined.\\ \\
Let  be a positive stochastic matrix. Then, the following statements hold
based on Perron's theorem \cite{HorJoh:85}:\\
1. The eigenvalue  of  is a simple eigenvalue such that any other eigenvalue   
(possibly complex) is strictly smaller than  in absolute value, . The spectral 
radius  is equal to .\\
2. There exists an eigenvector  of  with eigenvalue  such that all components of  
are positive ,  for . \\ 
3.  is irreducible and
the corresponding graph  is 
strongly connected, that is, for any two vertices , there 
exists a sequence of edges which connects  to .\\ \\
A stationary probability vector  is the eigenvector of the positive stochastic matrix  
associated with eigenvalue ; it is a vector that does not change under application of the transition matrix. 
Perron's theorem ensures the following:\\
1. Such a vector  exists and it is unique.\\
2. The eigenvalue with the largest absolute value  is always . \\ \\
The vector  can be asymptotically computed by means of the power method

for any  which is a stochastic vector. 
Therefore, the following limit is obtained:

for . 
} }}

\vspace{.5cm}

As previously discussed, due to the large dimension of the link matrix  (currently of 
the order of ), the computation of the PageRank values is very difficult. 
The solution that has been employed in practice 
is based on the power method, which is simply the Markov chain iteration (\ref{markov}) with ,
or (\ref{markov_M}).
It appears that this computation is performed at Google once a month and it takes one week, 
even though the power method requires only 50--100 iterations
to converge to a reasonable approximation \cite{LanMey:06}.  

The value vector  is computed through the recursion 

where the initial condition  is a probability vector.  
The equality in \eqref{eqn:xM0} follows immediately from the fact 
.
For implementation, it is much more convenient to use the form 
on the right-hand side of 
(\ref{eqn:xM0}) involving the matrix  and not the matrix  because the matrix 
 is sparse, while the matrix  is not. Also notice that  has non-zero elements 
in the diagonal, and this means that self-loops are artificially introduced in 
the teleportation model, which are in fact absent in the matrix .

The convergence rate of the power method is now discussed.
Denoting by  and , respectively,  
the largest and the second largest eigenvalue of  
in magnitude, the asymptotic rate of convergence of this method is exponential and
depends on the ratio .
Since  is a positive stochastic matrix, 
it holds that  and .
Furthermore, it can be easily shown that 
the structure of the link matrix  leads us to the bound 

Therefore, after  iterations the error level is below 
, and 
after  iterations, it becomes .
Clearly, larger values of  imply faster convergence. 
However, when  is large, the emphasis on the link matrix  
and hence differences among the pages are reduced in the PageRank values. 
On the other hand, 
by performing a sensitivity analysis with respect to the parameter , 
it follows that

for . A deeper analysis \cite{LanMey:06} shows that if  
is close to , then 
the values in  become sensitive and may change even for small 
variations in .
The conclusion is that  is a reasonable compromise, 
and this is probably the reason why it is used at Google.

\section*{Distributed Randomized Algorithms for PageRank Computation}

This section studies a sequential distributed randomized approach of {\it gossip-type} which, 
at each step of the sequence, uses only the outgoing links connecting a specific web page to 
the neighboring nodes to compute the PageRank vector  \cite{IshTem:10}. That is, in contrast with  
the centralized approach (\ref{eqn:xM0}) based on the power iteration, only local information 
involving a specific web page (randomly selected) is utilized to update the PageRank value. 
Difficulties in computing PageRank have motivated various studies on efficient algorithms,
but decentralized schemes over networks \cite{BerTsi:89} 
are natural especially in the context of web data \cite{ZhaChFa:13,Nesterov:12,Necoara:13}. 

Consider the web with  pages represented by the directed graph . 
The randomized scheme is described as follows:
At time , page  is randomly selected and its PageRank value is transmitted by means of outgoing 
links to the pages that are directly linked to it, while other pages not directly connected to 
page  are not involved in the transmission.
More precisely, we introduce a random process , , and, if 
at time ,  is equal to , then page  initiates the broadcasting process 
involving only the neighboring pages connected by outgoing links. All pages involved in this 
algorithm renew their values in a random fashion based on the latest available information.  

Specifically,  is assumed to be an independent and identically distributed (i.i.d.)
random process and its probability distribution is given by

In other words, at time , the page starting the transmission process is selected with equal probability.  In principle, 
this scheme may be implemented without requiring 
a decision maker or any fixed order among the pages. Extensions of this scheme are studied in \cite{IshTem:10}
where multiple updates of web pages are considered. 
In this case, each page decides to update or not 
in an i.i.d.\ fashion under a given probability, independently of other pages. Furthermore, in \cite{IshTemBaiDab:09}
other more sophisticated schemes are presented.


Instead of the centralized scheme (\ref{eqn:xM0}), which involves the full matrix , 
consider a randomized distributed update scheme for PageRank computation of the form

where the initial condition  is a probability vector,  is 
the random process defined in (\ref{eqn:theta1}),  is a design parameter 
which replaces the parameter  used in (\ref{eqn:xM0}), and 
, , are the {\it distributed hyperlink matrices} of gossip-type subsequently 
defined in ``Distributed Link Matrices and Their Average.''




The objective of this distributed update scheme 
is to design the distributed hyperlink matrices  and the parameter 
so that the PageRank values are computed
through the time average of the state . To this end, let  be the time average of 
the sample path  defined as

For the distributed update scheme, the objective is to compute the PageRank value  using 
the {\it time average} , also called the Ces\`aro average or the Polyak average in some contexts.
For each initial state  that is a probability vector, 
 is said to converge to  in the mean-square error (MSE) sense if

where the expectation  is taken with respect to the random process
, , defined in (\ref{eqn:theta1}).
This type of convergence is called ergodicity for random processes \cite{PapPil:02}.

\subsection*{Distributed Link Matrices and Their Average}

Here, the gossip-type distributed link matrices are introduced.
Based on the definition (\ref{eqn:A}) of 
the hyperlink matrix, recall that the th column of the matrix  represents the outgoing links 
of page . Therefore, the idea of the distributed randomized algorithm is that the matrix  
uses only the column  of the matrix , and the remaining columns of  are constructed 
so that the matrix  is a stochastic matrix. 
This is a key feature of the method, distinguishing it from others as \cite{Nesterov:12,Necoara:13}. 
More precisely, the {\it distributed link matrices}
, are defined as follows: \\
(i) The th column of  coincides with the th column of . \\
(ii) The th diagonal entry of  is equal to  for 
, . \\
(iii) All of the remaining entries  are zero.\\
By construction, it follows immediately that the distributed matrices  are column stochastic. 
The next example shows the construction of the distributed link matrices.

\begin{example}\label{ex:3}\rm
In the six-page web of Example~\ref{ex:1}, 
the distributed link matrices , , can be obtained as


\End
\end{example}

\medskip
The number of nonzero entries in the matrix  is no more than , 
and  diagonal entries are equal to 1. On the other hand, the centralized matrix  
has at most  nonzero entries. The sparsity of the matrices  is useful 
from the computational and implementation viewpoint.
Furthermore, the protocol is gossip-type because 
the link matrix , using only the th column of the centralized matrix ,
is chosen randomly at each time instant. 



\subsection*{Mean-Square Error Convergence of the Distributed Update Scheme}

We now discuss 
the convergence properties of the randomized distributed scheme \eqref{eqn:xMi1}, 
where the parameter  is chosen as

For the value  used in \eqref{eqn:xMi1},
it holds that . 

It has been shown \cite{IshTem:10} that the time average of the randomized 
update scheme defined in \eqref{eqn:xMi1} and \eqref{eqn:yk} to compute PageRank 
converges to the value vector  in the mean-square error sense. 
More precisely, for any stochastic vector , it holds

The time average is necessary and, without averaging the values, 
 oscillates and does not converge to the stationary value .

Several remarks are in order. 
In the distributed computation discussed here,
it is required that pages communicate with each other and 
then make computation for the updates in the values. 
More in detail, for each page, the values of the 
pages that are directly connected to it by outgoing links need to be
sent. The link matrices  involved in the update scheme \eqref{eqn:xMi1}
are sparse. Thus, at time , communication 
is required only among the pages corresponding to the nonzero entries
in . 
Then, for each page, weighted addition of its own value, the values 
just received, and the constant term  is performed.
Consequently, the amount of computation required 
for each page is very limited at any time.

Implementation issues, such as how web pages 
make local computations, are outside the scope of this article. 
It is, however, clear that in practice, servers hosting web pages should be making the
local computations along with the communications, and not the individual pages.
Regulations may also be necessary so that page owners cooperate with the 
search engine and the PageRank values computed by them 
can be trusted. In the consensus literature, related issues 
involving cheating have been studied. 
An example is the Byzantine agreement problem,
where there are malicious agents who send
confusing information so that 
consensus cannot be achieved; see \cite{TemIsh:07} for a discussion
on a Las Vegas approach and \cite{TemCalDab_book} 
for a detailed analysis of randomized algorithms.
Another issue concerning reliability of PageRank is link spam, that is,
artificial manipulation of the rank by adding spurious links.  
There are methods \cite{LanMey:06,AndBorHop:07}
to detect link spamming.

\vspace{.5cm}

\centerline{
\fbox{\parbox{16cm}{
{\Large Distributed Link Matrices and Convergence of the Randomized Algorithm}\\ \\
Let  be an independent and identically distributed (i.i.d.) random process with probability distribution given by

Then, the average matrix is given by

where the expectation  is taken with respect to the random process 
 defined in (\ref{eqn:theta1}). 
Then, the following properties hold:
\begin{enumerate}
\item[(i)]  , where  is the identity matrix.
\item[(ii)] There exists a vector  
which is
an eigenvector corresponding to the eigenvalue 1 for both 
matrices  and .
\end{enumerate}
Therefore, even though  and  are  
completely different matrices, they share a common eigenvector for the eigenvalue 1,
which corresponds to the PageRank vector.
In showing the properties above, it is important that no self-loop is allowed in 
the web graph. \\ \\
Corresponding to the distributed update scheme in \eqref{eqn:xMi1}, 
the link matrices are given by 

where the parameter  is

Then, the average matrix of  can be expressed as

This average matrix satisfies the following properties:
\begin{enumerate}
\item[(i)] 
.
\item[(ii)]
The eigenvalue 1 is simple and is the unique eigenvalue of maximum modulus.
\item[(iii)]
The corresponding eigenvector is the PageRank value  in \eqref{eqn:prvec}.
\\
\end{enumerate}
The randomized update scheme defined in 
\eqref{eqn:xMi1} and \eqref{eqn:yk} to compute PageRank 
has the following mean-square error (MSE) property for any  which is a stochastic vector:

} }}

The next section discusses how ideas from PageRank have been
successfully used in the context of bibliometrics.

\section*{Ranking (Control) Journals}

The {\it Impact Factor} (IF) is frequently used
for ranking journals in order of importance.
The IF for the year 2012, which is the most recently available, is defined as follows:

In this criterion, there is a census period (2012) of one year and a window 
period (2010--2011) of two years.  
More precisely, this is the 2-year IF. Another criterion, the 5-year IF, 
is defined accordingly, but is not frequently used (since it was introduced more
recently).
The IF is a ``flat criterion" which does not take into account 
where the citations come from, that is, if the citations arrive 
from very prestigious journals or in fact if they are positive or negative citations.


On the other hand, different indices may be introduced using ideas from PageRank. 
The random walk of a journal reader is similar to the walk described by the 
random surfer moving continuously on the web. Therefore, 
journals and citations can be represented as a network with 
nodes (journals) and links (citations). 
Such a situation is described in \cite{WesBerBer:10}: 
\begin{quote}
``Imagine that a researcher is to spend all eternity in the library randomly 
following citations within scientific periodicals. The researcher begins by 
picking a random journal in the library. From this volume a random citation 
is selected. The researcher then walks over to the journal referenced by this 
citation. The probability of doing so corresponds to the fraction of
citations linking the referenced journal to the referring one, 
conditioned on the fact that the researcher starts from the referring
journal. From this new volume the researcher now selects another 
random citation and proceeds to that journal. 
This process is repeated ad infinitum." 
\end{quote}
An interesting question can be posed as,
What is the probability that a journal is cited? To address this question, a different criterion for ranking journals, 
called the {\it Eigenfactor Score} (EF), has been first introduced in \cite{Ber:07}. This criterion is one of the official metrics 
in the Science and Social Science Journal Citation Reports
published
by Thomson Reuters for ranking journals; see \cite{Franceschet:13} for details.

The details of EF are now described. First an adjacent matrix 
 is defined as follows: 

where  represents the total number of journals under consideration, which is currently 
over 8,400.


In this case, the window period is five years. The adjacent matrix  is then 
suitably normalized to obtain the {\it cross-citation} matrix  
as follows:

where  if . Clearly, the fact that the diagonal entries of the matrices 
 and  are set to zero means that self-citations are omitted. Furthermore, the normalization 
used to obtain the matrix  implies that this matrix is column substochastic.


In the unlikely event that there are journals that do not cite any other journal, 
some columns in the cross-citation matrix 
are identically 
equal to zero making the matrix substochastic instead of stochastic,
similarly to the situation in the web.
To resolve the issue, a trick similar to the ``back button" 
previously introduced can be useful.
To this end, let the {\it article vector} be given by
, where

That is,  represents the fraction of all published articles coming from journal  during 
the window period 2007--2011. Clearly,  is a stochastic vector. To resolve the substochasticity problem, 
the cross-citation matrix  is redefined replacing the columns having entries equal to zero with 
the article vector. More precisely, a new matrix  is introduced as 

The matrix  is a stochastic matrix, and therefore the 
eigenvector corresponding to the largest eigenvalue, which is equal 
to one, exists. However, to guarantee its uniqueness, 
a teleportation model similar to that 
previously described in (\ref{eqn:M}) needs to be introduced.
In this case, consider the {\it Eigenfactor Score equation}

where the convex combination parameter  is equal to . 
This equation has the same form 
as the PageRank equation (\ref{eqn:M}), but the matrix  is replaced with the rank-one matrix . 
The properties previously discussed for the PageRank equation hold because the matrix  is a positive 
stochastic matrix. In particular, the eigenvector  corresponding to the largest eigenvalue, 
which is equal to 1, is unique. That is,

The value  is called the {\it journal influence vector}, 
which provides suitable weights on the citation values. 

The interpretation from the point of view of Markov chains is that the value
 represents the steady state fraction of time spent vising each journal 
represented in the cross-citation matrix . The Eigenfactor Score EF is an -dimensional vector whose th 
component is defined as the percentage of the total weighted citations that journal  receives 
from all 8,400 journals. That is, we write


A related index, used less frequently, is the {\it Article Influence} AI, 
which is a measure of the citation influence 
of the journal for each article. Formally, 
the th entry of the vector AI is defined by


In order to preserve sparsity of the matrix , from the computation point of view, we notice that 
EF can be obtained without explicitly using the matrix . 
That is, the journal influence 
vector iteration is written by means of the power method 


To conclude this section, a table is shown, summarizing 
the 2012 ranking of 10 mainstream control journals according to the IF and the EF.


\begin{table}[htb]
\caption{2012 Impact Factor (IF) and 2012 Eigenfactor Score (EF)}
\label{IFandEI}
\begin{tabular}{|c|c|c|c|c|} \hline
IF & Journal & Ranking & Journal & EF\\ \hline
2.919 & Automatica & 1 & IEEE Transactions Automatic Control & 0.04492\\
2.718 &IEEE Transactions Automatic Control & 2 & Automatica & 0.04478\\
2.372 &IEEE Control Systems Magazine & 3 & SIAM J. Control \& Optimization &0.01479\\
2.000 &IEEE Transactions Contr. Sys. Tech. & 4 & IEEE Transactions Contr. Sys. Tech. & 0.01196\\
1.900 &Int. J. Robust 
Nonlinear Control & 5 & Systems \& Control Letters &0.01087\\
1.805 &Journal Process Control & 6 & Int. Journal Control &0.00859\\
1.669 & Control Engineering Practice & 7 & Int. J. Robust Nonlinear Control&0.00854\\
1.667 &Systems \& Control Letters & 8 & Control Engineering Practice & 0.00696\\
1.379&SIAM J. Control \& Optimization & 9 & Journal Process Control&0.00622\\
1.250&European J. Control & 10 & IEEE Control Systems Magazine& 0.00554\\
\hline
\end{tabular}
\end{table}



\section*{Relations with Consensus Problems}

Consensus problems for multi-agent systems have a close relationship with
the PageRank problem and has motivated the distributed randomized approach 
introduced earlier. 
This section considers 
a stochastic version 
of the consensus problem, which has been studied
in, for example, \cite{BoyGhoPra:06,HatMes:05,TahJad:08,Wu:06};
see also \cite{TemIsh:07} for a discussion from the point of view of 
randomized algorithms. 
In \cite{TsuYam:08}, a dual version of PageRank is proposed and
its usefulness in controlling consensus-based agent systems is 
explored.

Consider a  network of  agents corresponding to the vertices  
of a directed graph , where 
is the set of links connecting the agents. The agent  is said to be connected 
to the agent  by a link  if agent  transmits its 
value to agent . It is assumed that there exists a {\it globally reachable agent} 
for the  graph . This assumption implies that there exists an agent from which
every agent in the graph can be reached via a sequence of directed links, see, for example, 
\cite{BulCorMar:09,LinFraMag:05,RenBea:05}. Recall that the graph 
 has at least one globally reachable agent if and only if 1 is 
a simple eigenvalue of a row stochastic matrix representing the graph , 
where  is the Laplacian of  (for example, \cite{BulCorMar:09}).

The objective of consensus is that the values 
 of all agents reach a common value 
by communicating to each other according to a prescribed communication pattern. 
Formally, consensus is said to be achieved in the sense of mean-square error (MSE) if, 
for any initial vector , it holds

for all .
The communication pattern (see ``Update Scheme for Consensus and Convergence Properties" 
for the precise definition) 
is determined at each time  according to an i.i.d. random process
 with probability distribution given by 

where  is the number of patterns.


A classical approach used in the consensus literature is to update the value of each agent
by taking the average of the values received at that time.
The iteration can be written in the vector form as

where the matrix  is defined in
``Update Scheme for Consensus and Convergence Properties." 
In contrast to the PageRank case, only the agent values  are updated,
and time averaging is not necessary for achieving probabilistic consensus.

\vspace{.5cm}

\centerline{
\fbox{\parbox{16cm}{
{\Large Update Scheme for Consensus and Convergence Properties}\\ \\
For each , define a subset   of the edge set  as follows:
\begin{enumerate}
\item[(i)]  For all , . \\
\vspace{-1cm}

\vspace{-1cm}
\item[(ii)]
.
\end{enumerate}
Let  be an independent and identically distributed (i.i.d.) random process
and its probability distribution is given by

where  is the number of communication patterns.\\
Consider the update scheme

where the matrix  is a row stochastic matrix constructed as follows

and  is the number of agents  with , that is, the number of agents sending information to agent  under the 
communication pattern .\\
Assuming that a globally reachable agent exists, convergence of this scheme in the mean-square error (MSE) sense

for all ; see for example,
\cite{IshTem:10,HatMes:05,TahJad:08, Wu:06}.
} }}

\vspace{.5cm}

A simple example is now presented to illustrate the communication scheme. 

\begin{example}\label{ex:consensus}\rm
Consider the network of six agents illustrated in Fig.~\ref{fig:graph} (b) from Example~\ref{ex:1}. 
First, as a simple case, we look at a static communication scheme
where all agents communicate over the original edge set  at all times.
In this case, there is only one pattern  and hence . 
To be consistent with the notation used for PageRank so far, 
the underlying matrix is simply denoted by . This matrix is constructed 
using incoming (rather than outgoing) links to make it row stochastic as

This matrix is similar to the hyperlink matrix given in Example~\ref{ex:1} in the sense
that the nonzero off-diagonal entries coincide. However, this matrix is row stochastic
while the link matrix for PageRank is column stochastic.
Moreover notice that all diagonal entries of this matrix are positive,
resulting in the presence of self-loops in the graph.
(We recall that for PageRank no self-loops are considered 
because these loops may increase spamming; 
see further details in \cite{LanMey:06,AndBorHop:07}).


Next, as in the distributed case, we introduce six communication patterns 
arising from the protocol in the distributed PageRank algorithm.
The edge subset  contains all  and 
links present in the original edge set  and all self-loops  for .
Then, the first three matrices , , are

The rest of the matrices can be similarly obtained.

\End
\end{example}

\medskip



Table~\ref{consensusandpagerank}
summarizes some of the key differences and similarities between the consensus
problem addressed in this section and the distributed approach studied 
in ``Distributed Randomized Algorithms for PageRank Computation''
for the PageRank computation.




\begin{table}[htb]
\caption{Comparison between Consensus and PageRank}
\label{consensusandpagerank}
\begin{tabular}{|c|c|c|} \hline
& Consensus & PageRank \\ \hline
objective & all agent values  become equal & page values  converge to a constant \\
graph structure & a globally reachable agent exists & the web is not strongly connected \\
self-loops & presence of self-loops for agents & no self-loops are considered in the web\\
stochastic properties &row stochastic matrix  & column stochastic matrices \\
convergence & in mean-square error sense and with probability  & in mean-square error sense
and with probability 
\\
initial conditions & convergence for any initial condition   & convergence for stochastic vector \\
averaging & time averaging not necessary & time averaging  required \\
\hline
\end{tabular}
\end{table}


\section*{Aggregation-Based PageRank Computation}

In this section, we turn our attention to a distributed PageRank computation
with a focus on reduced cost in computation and communication.
The particular approach developed in this section 
is based on the web aggregation technique proposed in 
\cite{IshTemBai:12}, which
leads us to a smaller web to be used in the computation. 
A procedure is presented to compute approximated values of the
PageRank and moreover provide an analysis on error bounds.
The approach shares ties with the
aggregation methods based on singular perturbation 
developed in the control literature \cite{PhiKok:81}.

\begin{figure}
  \centering
  \resizebox{6cm}{!}{\includegraphics{aggregation1a.eps}}
  \caption{A web graph with a sparse structure (a) and 
   its aggregated graph (b). It is known in the web that many links are internal, connecting pages
   within their own domain/directories. The aggregated web is obtained by grouping such pages.}
  \label{fig:aggregation1}  
\end{figure}

\subsection*{Aggregation of the Web}

The structure of the web is known to have a sparsity property because
many links are intra-host ones. This property means that pages are often linked 
within the same domains/directories (for example, organizations, universities/companies, 
departments, etc) \cite{LanMey:06,BroLem_infret:06}. 
A simple sparse network is illustrated in Fig.~\ref{fig:aggregation1} (a):
There are four domains with many pages, but the inter-domain links are only 
among the top pages.
In such a case, it is natural to group the pages and obtain an aggregated
graph with only four nodes as shown in Fig.~\ref{fig:aggregation1} (b).

By following this line of ideas, a PageRank computation approach
based on web aggregation is developed, roughly consisting of three steps: 
\begin{enumerate}
\item[1)] \textit{Grouping step}: Find groups in the web.
\item[2)] \textit{Global step}: Compute the total PageRank for each group.
\item[3)] \textit{Local step}: Distribute the total value of the group among members.
\end{enumerate}
The grouping of pages can mainly be done at the server level for the pages that
the servers host as we describe below.
The global step is at a higher level, requiring data exchange via communication among groups. 
In the local step, most of the computation should be carried out locally
within each group. 

For the aggregation-based method, pages are grouped 
with the purpose of computing the PageRank efficiently and accurately. 
Moreover, in view of the sparsity property of the web and the distributed algorithms 
discussed earlier, 
it is reasonable to group pages under the same servers or domains. This approach
has the advantage that grouping can be done in a decentralized manner.
More generally, the problem of grouping nodes in a network can be casted
as that of community detection, which can be performed based on different
measures such as modularity \cite{Newman:06,ExpEvaBlo:11} 
and the maximum flow \cite{FlaLawGil:02}. While such methods may be useful
for our purpose, they are known to be computationally expensive.

From the viewpoint of a web page, the sparsity in the web structure can be
expressed by the limited number of links towards pages outside its own group.
That is, for page , let its \textit{node parameter}  be given by

It is easy to see that smaller  implies sparser networks. 
In the extreme case where the node parameters for \textit{all} pages are small as

where  represents the bound, 
then one may apply the aggregation techniques based on singular perturbation
for large-scale networked systems such as consensus and Markov chains
\cite{PhiKok:81,AldKha:91,BiyArc:08,ChoKok:85}.
However, in the real web, it is clear that pages with many external links
\textit{do} exist. Especially, if such pages belong to small groups, 
these results are not directly applicable (see Fig.~\ref{fig:aggregation2}).

In this aggregation approach, the main idea is to consider pages with many external links as
\textit{single groups} consisting of only one member. For such pages, the node parameters
always become 1.  Hence, such pages are excluded from the condition 
\eqref{eqn:sparse_cond} and instead we use the condition

The size of the parameter  determines the number of groups
as well as the accuracy in the computation of PageRank. This point will
be demonstrated through the analysis of the approach and a numerical example. 

Denote by  the number of groups and by  the number of single groups. 
Also, for group , let  be the number of member pages. 
For simplicity of notation, the page indices are reordered as follows:
In the PageRank vector , the first  
elements are for the pages in group , and the following  entries are 
for those in group , and so on.

\begin{figure}
  \centering
  \resizebox{8cm}{!}{\includegraphics{aggregation2.eps}}
  \caption{A web page with many external links. In the real web, some pages have many
    outgoing links to pages outside of their own domains/directories. 
    In this respect, the sparsity property of the web is limited.}
  \label{fig:aggregation2}  
\end{figure}





\subsection*{Approximated PageRank via Aggregation}

For group , its \textit{group value} denoted by  
is given by the total value of the PageRanks  of its members. 
Hence, introduce a coordinate transformation as

where
\begin{itemize}
 \item : the th entry is the group value of group ;
 \item : each entry represents the difference between 
                                   a page value and the average value of the group members.
\end{itemize}
The first vector  is called the \textit{aggregated PageRank}.
By definition, it follows that  and
.

The transformation matrix  is

where  denotes a block-diagonal matrix
whose th diagonal block is .
The matrices  and  are block diagonal,
containing  and  blocks, respectively. 
Note that in , if the th group is a single one
(that is, ), then the th block has the size
, meaning that the corresponding column is zero. 
Due to the simple structure, the inverse of this matrix  
can be obtained explicitly \cite{IshTemBai:12}.  



Now, the definition of PageRank in \eqref{eqn:prvec} can 
be rewritten for  in the new coordinate as

with  and
.
As explained in ``Approximated PageRank Computation,"
the expression \eqref{eqn:Atil0} has two 
important properties: (i)~The (1,1)-block matrix  is 
a stochastic matrix, and (ii)~the entries of  are
``small'' in magnitude due to the sparse structure of the web. 

These properties form the basis for introducing an approximate version of
PageRank by triangonalizing the matrix  in \eqref{eqn:Atil0} as follows: 
Let  and  be
the vectors satisfying 

where  is a probability vector.
The -block matrix 
is taken as a block-diagonal matrix in accordance with the grouping;
for more details, see ``Approximated PageRank Computation."

Let .
Then, the \textit{approximated} PageRank is obtained in the original coordinate as


\subsection*{Computation of Approximated PageRank}

This section outlines an algorithm, consisting of three steps, for computing the approximated PageRank.

\begin{algorithm}
\label{alg:1}\rm 
Take the initial state  as a stochastic vector, and 
then proceed according to the following three steps:

1.~Iteratively, compute the first state  by


2.~After finding the first state ,
compute the second state  by


3.~Transform the state back in 
the original coordinate by

\end{algorithm}

\medskip
The first and second steps in the algorithm are based on the
definition of  in \eqref{eqn:Atil0dash}.
It requires the recursive computation of only the first state 
, whose dimension 
equals the number  of groups. At this stage, 
information is exchanged only among the groups.
By \eqref{eqn:Atil0dash}, the second state  can
be computed recursively through

Here, we employ the steady state form, that is, , 
to obtain the equation in \eqref{eqn:updateVx_red2}.
Note that the matrix  is nonsingular
because  is a stable matrix (as seen in 
``Approximated PageRank Computation").
This approach is motivated by the time-scale separation in methods based on
singular perturbation.
The update scheme in the algorithm is guaranteed to converge to 
the approximated PageRank vector , which follows from \eqref{eqn:conv}


It is observed that this algorithm is suitable for distributed computation. 
The first step \eqref{eqn:updateVx_red1} is the global step
where the groups communicate to each other and
compute the total of the members' values, represented by 
the -dimensional state .
It is important to note that this step can be implemented via the 
distributed randomized approach discussed in 
``Distributed Randomized Algorithms for PageRank Computation.''
The rest of the algorithm can be carried out mostly  
via local interactions within each group.
This local computation can be confirmed from the block-diagonal structures in
the matrices  and .
The only part that needs communication over inter-group links
is in the second step \eqref{eqn:updateVx_red2}, when 
computing the vector .



\begin{table}[t]
\begin{center}
\caption{Comparison of operation costs with communication among groups}
\label{table:operation}
\begin{tabular}{lll}
  \hline
   Algorithm & Equation & Bound on numbers of operations\\
  \hline Original & \eqref{eqn:xM0} 
    & \\
  Aggregation based & \eqref{eqn:updateVx_red1}
    & \\
    & &  \hspace*{1.1cm}~~\\
                    & \eqref{eqn:updateVx_red2}
    & \\
  \hline
\end{tabular}
\end{center}
\hspace*{6cm} 
: The number of nonzero entries of a matrix\
   \norm{x^* - x'}_1 \leq \epsilon,

   \delta 
       \leq \frac{m\epsilon}{4(1-m)(1+\epsilon)}.
   \label{eqn:delta2}
-4mm]
    Page Index & 1 & 2 & 3 & 4 & 5 & 6\-4mm]  
   {\large }
      &  &  & 1 &  & 0 & 0\
 V = \left[
       \begin{array}{c}
         V_1\\       
         \hline
         V_2
       \end{array}
     \right]
   = \left[
     \begin{array}{ccccccc}
       \cdashline{1-2}
       \multicolumn{1}{:c}{1} & \multicolumn{1}{c:}{1} & 0 & 0 & 0 & 0\\
       \cdashline{1-3}
       0 & 0 & \multicolumn{1}{:c:}{1} & 0 & 0 & 0\\
       \cdashline{3-6}
       0 & 0 & 0 & \multicolumn{1}{:c}{1} & 1 & \multicolumn{1}{c:}{1}\\
       \cdashline{4-6}\-3mm]
       \cdashline{1-2}
       \multicolumn{1}{:c}{1/2} & \multicolumn{1}{c:}{-1/2} & 0   & 0 & 0 & 0\\
       \cdashline{1-2}\cdashline{4-6}
       0 & 0 & 0 &  \multicolumn{1}{:c}{2/3} & -1/3 & \multicolumn{1}{c:}{-1/3}\\
       0 & 0 & 0 & \multicolumn{1}{:c}{-1/3} &  2/3 & \multicolumn{1}{c:}{-1/3}\\
       \cdashline{4-6}
    \end{array}\right],

 \widetilde{x}^* 
  &= \left[
       \begin{array}{c|c}
         (\widetilde{x}_1^*)^T & (\widetilde{x}_2^*)^T  
       \end{array}
    \right]^T
  = \left[
       \begin{array}{ccc|ccc}
         0.147 & 0.122 & 0.731 & -0.0121 &  -0.0294 & -0.0294
       \end{array}
    \right]^T.    

 A &= \text{(Internal)} + \text{(External 1)} + \text{(External 2)}\\
  &= \left[
     \begin{array}{cccccc}
      \cdashline{1-2}
      \multicolumn{1}{:c}{1/2} & \multicolumn{1}{c:}{1/2}  &  0  & 0 &  0  &  0\\
      \multicolumn{1}{:c}{1/2}  & \multicolumn{1}{c:}{1/2} &  0  & 0 &  0  &  0\\
      \cdashline{1-3}
         0 &  0   &  \multicolumn{1}{:c:}{1}  & 0 &  0  &  0\\
      \cdashline{3-6}
         0 &  0   &  0  & \multicolumn{1}{:c}{1/3} & 0 & \multicolumn{1}{c:}{1/2}\\
         0 &  0   &  0  & \multicolumn{1}{:c}{1/3} & 0 & \multicolumn{1}{c:}{1/2}\\
         0 &  0   &  0  & \multicolumn{1}{:c}{1/3} &  1 & \multicolumn{1}{c:}{0} \\
      \cdashline{4-6}
     \end{array}
    \right]
   + 
    \left[
     \begin{array}{cccccc}
      \cdashline{1-2}
      \multicolumn{1}{:c}{0} &  \multicolumn{1}{c:}{0}  &  0   &  0   &  0  &  0\\
      \multicolumn{1}{:c}{0}   & \multicolumn{1}{c:}{0} &  1/3 &  0   &  0  &  0\\
      \cdashline{1-3}
       0   & 0 & \multicolumn{1}{:c:}{-1}   &  0 &  0  &  0\\   
      \cdashline{3-6}
         0 &  0   &  1/3 & \multicolumn{1}{:c}{0} &  0  & \multicolumn{1}{c:}{0}\\   
         0 &  0   &  0   & \multicolumn{1}{:c}{0}    &  0  & \multicolumn{1}{c:}{0}\\
         0 &  0   &  1/3 & \multicolumn{1}{:c}{0}    &  0  & \multicolumn{1}{c:}{0}\\
      \cdashline{4-6}
     \end{array}
    \right]
    + 
    \left[
     \begin{array}{cccccc}
      \cdashline{1-2}
      \multicolumn{1}{:c}{-1/2} &  \multicolumn{1}{c:}{0}  &  0   &  0   &  0  &  0\\
      \multicolumn{1}{:c}{0}   & \multicolumn{1}{c:}{-1/2} &  0   &  0   &  0  &  0\\
      \cdashline{1-3}
       0   & 1/2 & \multicolumn{1}{:c:}{0}   &  1/3 &  0  &  0\\   
      \cdashline{3-6}
       1/2 &  0   &  0 & \multicolumn{1}{:c}{-1/3} &  0  & \multicolumn{1}{c:}{0}\\   
         0 &  0   &  0   & \multicolumn{1}{:c}{0}    &  0  & \multicolumn{1}{c:}{0}\\
         0 &  0   &  0 & \multicolumn{1}{:c}{0}    &  0  & \multicolumn{1}{c:}{0}\\
      \cdashline{4-6}
     \end{array}
    \right].

  & \widetilde{A}
     = \left[
         \begin{array}{c|c}
          \widetilde{A}_{11} & \widetilde{A}_{12}\\
          \hline
          \widetilde{A}_{21} & \widetilde{A}_{22}
         \end{array}
       \right]
   = \left[
         \begin{array}{ccc|ccc}
           0.5  & 0.333 & 0     &  0   &  0     & 0\\
           0.25 & 0     & 0.111 & -0.5 &  0.333 & 0\\
           0.25 & 0.667 & 0.889 &  0.5 & -0.333 & 0\\
           \hline
           0   & -0.167 &      0 &  -0.5  &        0  &       0\\
           0.167 &  0.111 & -0.130 &   0.333 &   -0.389 &   -0.5\\   
          \hspace*{-1mm}-0.0833 \hspace*{-1mm}&  -0.222 \hspace*{-1mm}&  -0.0185 
                & -0.167\hspace*{-1mm} &   -0.0556\hspace*{-1mm} &   -0.5\hspace*{-1mm}
	     \end{array}
	  \right].

  \begin{bmatrix}
    I - (1-m)\widetilde{A}'_{22}
  \end{bmatrix}^{-1}
    \widetilde{A}_{21}
     = \begin{bmatrix}
         0 & -0.167 & 0\\
         0.174 & 0.161 & -0.113\\
         -0.0758 & -0.172 & -0.00177
       \end{bmatrix},

  \widetilde{A}'_{22}   
= \left[\begin{array}{ccc}
        \cdashline{1-1}
        \multicolumn{1}{:c:}{0} &   0     &   0\\
        \cdashline{1-3}
        0 &  \multicolumn{1}{:c}{-0.167} &  \multicolumn{1}{c:}{-0.5}\\
        0 &  \multicolumn{1}{:c}{-0.167} &  \multicolumn{1}{c:}{-0.5}\\
        \cdashline{2-3}
      \end{array}\right].

  x' &= V^{-1} \widetilde{x}'
      = \begin{bmatrix}
         0.0566 & 0.0920 & 0.125 & 0.212 & 0.213 & 0.302
       \end{bmatrix}^T.

 \norm{x'-x^*}_1 
    = 0.0188.

\End
\end{example}



\section*{Experimental Results}

In this section, numerical simulations are presented based on 
a web obtained from real data.

\begin{figure}[t]
  \centering
  \fig{13cm}{!}{G_orig_red_dangling1.eps}
  \caption{The graph structure of the web data used in experiments. 
   The points indicate links from page  to page  and the red points 
   are those linking to dangling nodes.
   Two clusters of pages with dense link structures can be found around 
   indices 500 and 2,500.}
  \label{fig:G_orig}
\end{figure}


\begin{figure}[t]
  \centering
  \fig{9cm}{!}{PR_orig1.eps}
  \caption{PageRank values of all pages in the example web. The values for those pages in the
   two clusters (around indices 500 and 2,500) are especially high.}
  \label{fig:PageRank_values}
\end{figure}

\subsection*{The Web Data and its PageRank}

First, we describe the web data that has been employed in this simulation. 
The data was obtained from the database \cite{webdata} collected by crawling 
web pages of various universities.
This database has previously been used as a benchmark for testing PageRank algorithms \cite{FABG:13}.
Among them, we have chosen the data from Lincoln University in New Zealand from the year 2006.
This web has 3,756 nodes
with 31,718 links and there are in total 684 domains. 
The largest is the main domain of the university (www.lincoln.ac.nz), consisting of
2,467 pages. Other larger domains in this dataset contained
221, 101, 68, 24 pages, and so on. 
In the real web, 
a fairly large portion of the nodes are dangling nodes.
In this example, there are 3,255 dangling nodes, which is over 85 percent
of the total.
Also, two nodes had no incoming links; these were removed since 
such nodes play very minor roles in the PageRank values.
The pages were indexed according to the domain/directory names in an alphabetic order.
Fig.~\ref{fig:G_orig} displays the link pattern of the web with  pages,
where the blue points represent the nonzero entries of the connectivity matrix;
the red points correspond to outgoing links from dangling nodes.

To proceed with the PageRank computation, the web needs to be modified
so that the resulting link matrix  becomes stochastic. 
This modification was done by adding back links to dangling nodes, that is, links from each dangling 
node to the pages that have links to it. 
Hence, in the link pattern of Fig.~\ref{fig:G_orig},
for each red points in the th entry, a new point in the th entry was added.
The resulting web had 40,646 links.
For this web, the PageRank values were calculated by the power method. 
About 40 iterations were sufficient for its convergence. 
The results are shown in Fig.~\ref{fig:PageRank_values}. Comparing this with the
link structure in Fig.~\ref{fig:G_orig}, we notice that the pages with higher PageRank
values are included in the two clusters where many pages are linked to each other,
especially around page indices 500 and 2,500.
The top two pages in PageRank values turned out to be the ``search'' pages of the university
while the main home page of the university came in the third place. 



\subsection*{Distributed Randomized Algorithm}

These values could also be computed via the distributed randomized algorithm.
Here, we use a modified version of the algorithm from 
``Distributed Randomized Algorithms for PageRank Computation''
based on the simultaneous updates \cite{IshTem:10}. 
In contrast with the original scheme,  
at each time step,
each page asynchronously decides to send its value to its neighbors 
in a probabilistic way under a fixed probability. 
Thus, even in the event that an agent receives data from
multiple agents at the same time, this algorithm can handle all data in
the update at that moment.
Another benefit is that the convergence is faster. 
Throughout this section, this update
probability is fixed to be 0.2, so on average each agent makes a transmission
once in every five time steps. 


In Fig.~\ref{fig:timeresp_y}, the time averages 
are displayed for the pages taking larger values of PageRank. The true PageRank values
are indicated in dashed lines, and the convergence to these lines is observed.
Moreover, to see the overall convergence rate, we plotted in Fig.~\ref{fig:timeresp_err1}
the response of the error  from the true values in 1-norm (solid line).


\begin{figure}[t]
  \centering
  \fig{9cm}{!}{timeresp_alpha02a.eps}
  \caption{Time responses of the time averages  for some pages and
       their corresponding PageRank values (in dashed lines).}
  \label{fig:timeresp_y}
\end{figure}


\begin{figure}[t]
  \centering
  \fig{9cm}{!}{timeresp_err_alpha02_full_agg.eps}
  \caption{Time responses of the error from the PageRank  in the distributed randomized algorithm 
             for the full-order case (solid) and the aggregation-based case (dash-dot).
             The response of the aggregation-based scheme is faster, but some error remains
             since the approximated PageRank  is computed.}
  \label{fig:timeresp_err1}
\end{figure}

\subsection*{Aggregation-Based Computation}

We further continued with computation based on the technique of web aggregation
from ``Aggregation-Based PageRank Computation.''
The first step is to specify the groups of pages, from which we can estimate the 
sparsity structure in the web based on the node parameters  in \eqref{eqn:node_par}; 
for page , this parameter  
indicates the fraction of internal links within its own group over all of its outlinks. 
A simple way to find the initial grouping is to divide the pages based on their domain names.
Fig.~\ref{fig:delta_i_1} shows the node parameters for all pages based on this initial grouping. 
Each mark in red indicates a page which has no other page in its domain and 
hence is identified as a single group. Such pages necessarily have node parameters of 1, and
there were 577 of them. 
Note that this grouping resulted in a limited 
number of pages in non-single groups taking large values of . 
However, some of them have , meaning that the aggregation method is not
directly applicable at this point. 

\begin{figure}[t]
  \centering
  \fig{9cm}{!}{delta_i_1.eps}
  \caption{The node parameters  with the original grouping based on domains.
           Pages taking large values are mostly single groups (marked in red), 
           but among the pages in non-single groups (marked in black), some have 
           a large portion of external links.}
  \label{fig:delta_i_1}
\end{figure}


Therefore, the next step is to remove pages taking larger values from their groups.
Such regrouping can be done by specifying a threshold  and to make each page
whose node parameter exceeds  as a separate group (that is, a single group). 
This process can be carried out at the level of domains in a distributed way. 
For example, for , the grouping and thus the node parameters 
changed as shown in Fig.~\ref{fig:delta_i_04}. Here, the number of groups has 
increased from 684 to 1,357 while the largest group has decreased
in size from 2,467 pages to 2,386 pages.
It should be noted that the removal of nodes from a group might also change 
the node parameters for pages that remain in the same group. Hence,
it is usually necessary to iterate the process several times
before all  become below the given threshold. 

Once the grouping is settled, we compute the approximate value  of PageRank
via the aggregated approach. In the case with , the error in the 
approximation seems small, where the total error was calculated to be
.
More precisely, the relation for each page between the true PageRank  and its
approximate  is shown in Fig.~\ref{fig:PR_orig_approx}. 
We made a linear model by least-square fitting, which 
resulted in the line with slope of 1.013 shown in the plot. 
Though this slope is very close to the desired 1.0, there are several points far from the line. 
The level of approximation can also be measured 
by computing the sample correlation between the two vectors  and .
The Pearson correlation \cite{SneCoc:89}, representing the similarity in the values, is very high
at 0.991. On the other hand, the Spearman correlation
\cite{SneCoc:89} 
is related to the closeness
in the rankings among the pages and turned out to be 0.906. 
It should be noted that our implementation of (re-)grouping the pages has been performed 
under very simple rules, and there certainly is room for improvement. For example,
pages can be grouped by considering not only their domains but also 
their directories, subdirectories, related sites, etc. 



\begin{figure}[t]
  \centering
  \fig{9cm}{!}{delta_i_04.eps}
  \caption{The node parameters  after regrouping with .
     Pages taking large values are all single groups (marked in red). Other pages
     are grouped so that their node parameters remain below the threshold .}
  \label{fig:delta_i_04}
\end{figure}

\begin{figure}[t]
  \centering
  \fig{9cm}{!}{PR_orig_approx.eps}
  \caption{PageRank  and its approximate  with the node parameter .
      For most pages, the error in the approximation is small, resulting in a linear model
      via least-square fitting with slope 1.013.}
  \label{fig:PR_orig_approx}
\end{figure}

Similar computations can be made for different threshold values . The results
are shown in Figs.~\ref{fig:num_grp_delta}--\ref{fig:correlation_delta},
which, respectively, display the number of groups, the error in the approximated 
PageRank, and the correlations versus the node parameter .
Overall, the curves in these plots are smooth, showing that the grouping
method is sufficiently sensitive to changes in the threshold .
In particular, it is interesting to observe that between  and , the
number of groups do not change much (Fig.~\ref{fig:num_grp_delta}),
but the reduction in approximation error (Fig.~\ref{fig:error_delta})
is very large when  is reduced in size. Such a property is also demonstrated 
in the improvement in correlations (Fig.~\ref{fig:correlation_delta}).


\begin{figure}[t]
  \centering
  \fig{9cm}{!}{num_grp_delta.eps}
  \caption{The number of groups versus the node parameter . By reducing the size of ,
  the number of groups increases since any pages with parameter  are taken out of 
   the group and then turned into single groups.}
  \label{fig:num_grp_delta}
\end{figure}

\begin{figure}[t]
  \centering
  \fig{9cm}{!}{error_delta.eps}
  \caption{The error  in approximated PageRank versus the node parameter .
           Smaller  results in better approximation.}
  \label{fig:error_delta}
\end{figure}

\begin{figure}[t]
  \centering
  \fig{9cm}{!}{correlation_delta.eps}
  \caption{Pearson correlation (solid) and Spearman correlation (dashed) versus the
           node parameter . The error in the approximated PageRank  can 
            be observed through these sample correlations between  and .}
  \label{fig:correlation_delta}
\end{figure}


Finally, computational aspects of the proposed algorithms are
briefly discussed.
In the approximated PageRank, the aggregated part can be computed through the
distributed algorithm similarly to the full-order case explained above. 
In the distributed randomized algorithm with , the aggregated state 
is of order 1,347. In Fig.~\ref{fig:timeresp_err1}, the error is shown by the dashed line 
in comparison with the original distributed algorithm of full order (in the solid line).
Note that this error is obtained from the entire vector  constructed at 
each time step and is with respect to the true PageRank
(and not the approximated version). Consequently,
the error stops decreasing after it reaches about 0.0665 as the vector  converges 
to .
It is clear that the convergence speed is faster than that of the 
non-aggregated case in the solid line. 
This speed enhancement is in fact achieved with overall less computation; see \cite{IshTemBai:12}
for further discussion on computational costs.

\section*{Conclusion}
\label{sec:concl}

PageRank is a paradigmatic problem 
of great interest when ``big data" is available, and algorithms derived 
from PageRank have been successfully used to rank different objects in 
order of importance, such as scientific papers linked by citations, 
authors related by co-authorship, proteins in system biology and professional 
athletes. Therefore, in addition to systems and control, this problem is 
attracting the attention of many researchers working in a diverse set of fields, 
such as computer engineering, communications, physics, numerical analysis, 
linear algebra, and graph theory. 

The computation of PageRank is difficult due to the size of the web and because it is hard to gather and use global information about the network structure. 
In this article, we have followed a randomized decentralized approach, which leads to distributed
and parallel implementation, 
to deal with the extremely heavy computational load involved in the PageRank computation. The efficacy of the proposed approach
has been analyzed using the database \cite{webdata} 
(which has been previously used as a benchmark for PageRank algorithms \cite{FABG:13})
collected by crawling web pages of various universities.
To deal with problems of larger scale, the aggregation-based method
may repeatedly be applied in a hierarchical manner, by partitioning the initial groups,
then further the subgroups, and so on. 
Analyzing such a method is left for future research.


\newpage
\bibliographystyle{unsrt}
\begin{thebibliography}{1}

\bibitem{google:13}
Google Web Page. 
\newblock Facts about Google and Competition.
\newblock http://www.google.com/intl/en/insidesearch/howsearchworks/algorithms.html.



\bibitem{BriPag:98}
S.~Brin and L.~Page.
\newblock The anatomy of a large-scale hypertextual {Web} search engine.
\newblock {\em Computer Networks \& ISDN Systems}, 30:107--117, 1998.


\bibitem{Hubbell:65}
C.{\;}H.\ Hubbell.
\newblock An input-output approach to clique identification.
\newblock {\em Sociometry}, 28:377--399, 1965.

\bibitem{PinNar:76}
G.~Pinski and F.~Narin.
\newblock Citation influence for journal aggregates of scientific
publications: theory, with application to the literature of physics.
\newblock {\em Information Processing \& Management}, 12:297-312, 1976.

\bibitem{Leontief:41}
W.{\;}W. Leontief.
\newblock {\em The Structure of American Economy, 1919--1929}.
\newblock Harvard Univ.\ Press, 1941.

\bibitem{CXMR:09}
P.~Chen, H.~Xie, S.~Maslov and S.~Redner. 
\newblock Finding scientific gems with Google PageRank algorithm.
\newblock {\em Journal of Informetrics}, 1:8-15, 2007.

\bibitem{LiBoNede:05}
X.~Liu, J.~Bollen, M.\;L.~Nelson and H.\;V.~de Sompel.
\newblock Co-authorship
networks in the digital library research community. 
\newblock {\em Information Processing \& Management}, 41:1462--1480, 2005.

\bibitem{Radicchi:11}
F.~Radicchi.
\newblock Who is the best player ever? A complex network analysis of the history of professional tennis.
\newblock {\em PLoS ONE}, 6:e17249, 2011.

\bibitem{ZaBeEf:12}
N.{\;}N.~Zaki, J.{\;}J.~Berengueres and D.~{\;}D.~Efimov.
\newblock Detection of protein complexes using a protein ranking algorithm.
\newblock {\em Proteins}, 80:2459--2468, 2012.

\bibitem{LanMey:06}
A.{\;}N. Langville and C.{\;}D. Meyer.
\newblock {\em Google's PageRank and Beyond: The Science of Search Engine
  Rankings}.
\newblock Princeton Univ.\ Press, 2006.


\bibitem{Franceschet:11}
M.~Franceschet.
\newblock Page{R}ank: Standing on the shoulders of giants.
\newblock {\em Communications of the ACM}, 54:92--101, 2011.

\bibitem{IshTem:10}
H.~Ishii and R.~Tempo.
\newblock Distributed randomized algorithms for the {P}age{R}ank computation.
\newblock {\em IEEE Trans.\ Auto\-m.\ Control}, 55:1987--2002, 2010.

\bibitem{ZhaChFa:13}
W.~Zhao, H.{\;}F.~Chen, H.~Fang.
\newblock Convergence of distributed randomized {P}age{R}ank algorithms.
\newblock In {\em IEEE Trans.\ Auto\-m.\ Control}, 2013, submitted for publication.

\bibitem{NazPol:11}
A.{\;}V. Nazin and B.{\;}T. Polyak.
\newblock Randomized algorithm to determine the eigenvector of a stochastic
  matrix with application to the {P}age{R}ank problem.
\newblock {\em Automation and Remote Control}, 72:342--352, 2011.

\bibitem{IshTem_sice:09}
H.~Ishii and R.~Tempo.
\newblock Computing the {P}age{R}ank variation for fragile web data.
\newblock {\em SICE J.\ Control, Measurement, and System Integration}, 2:1--9,
2009.

\bibitem{CsaJunBlo:09}
B.{\;}C. Cs\'{a}ji, R.{\;}M. Jungers, and V.{\;}D. Blondel.
\newblock {P}age{R}ank optimization by edge selection.
\newblock {\it Discrete Applied Mathematics}, to appear, 2013.

\bibitem{FABG:13}
O.~Fercoq, M.~Akian, M.~Bouhtou and S.~Gaubert.
\newblock Ergodic control and polyhedral approaches to PageRank optimization.
\newblock {\em IEEE Trans.\ Auto\-m.\ Control}, 58:134--148, 2013.

\bibitem{JudPol:12}
A.~Juditsky and B.{\;}T. Polyak.
\newblock Robust eigenvector of a stochastic matrix with application to PageRank.
In {\em Proc.\ {\rm 51}st IEEE Conf.\ on Decision and Control}, 
pages 3171--3176,  2012.

\bibitem{Nesterov:12}
Y.~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale 
optimization problems.
\newblock {\em SIAM J.\ Optim.}, 22:341-?362, 2012.

\bibitem{Necoara:13}
I.~Necoara, 
\newblock Random coordinate descent algorithms for multi-agent convex 
optimization over networks,
\newblock {\em IEEE Trans.\ Auto\-m.\ Control}, 58:2001--2012, 2013.



\bibitem{IshTemBai:12}
H.~Ishii, R.~Tempo, and E.-W.~Bai.
\newblock 
A web aggregation approach for distributed randomized PageRank algorithms.
\newblock {\em IEEE Trans.\ Auto\-m.\ Control}, 57: 2703--2717, 2012.

\bibitem{IshTemBai_scl:12}
H.~Ishii, R.~Tempo, and E.-W.~Bai.
\newblock 
PageRank computation via a distributed randomized approach with lossy communication.
\newblock {\em Systems \& Control Letters}, 61: 1221--1228, 2012. 

\bibitem{KibCom:12}
A. Kibangou and C. Commault.
\newblock Decentralized Laplacian eigenvalues estimation and collaborative 
network topology identification.
\newblock In {\em Proc.\ {\rm 3}rd Workshop on Distributed Estimation and 
Control of Networked Systems}, pages 7--12, 2012.

\bibitem{IpsSel:07}
I.{\;}C.{\;}F.~Ipsen and T.{\;}M. Selee.
\newblock PageRank computation, with special attention to dangling nodes.
\newblock {\em SIAM J.\ Matrix Anal.\ Appl.}, 29:1281--1296, 2007.


\bibitem{HorJoh:85}
R.{\;}A.\ Horn and C.{\;}R.\ Johnson.
\newblock {\em Matrix Analysis}.
\newblock Cambridge Univ.\ Press, 1985.

\bibitem{BerTsi:89}
D.{\,}P.\ Bertsekas and J.{\,}N.\ Tsitsiklis.
\newblock {\em Parallel and Distributed Computation: Numerical Methods}.
\newblock Prentice-Hall, Englewood Cliffs, NJ, 1989.


\bibitem{IshTemBaiDab:09}
H.~Ishii, R.~Tempo, E.-W.\ Bai, and F.~Dabbene.
\newblock Distributed randomized {P}age{R}ank computation based on web
  aggregation.
\newblock In {\em Proc.\ {\rm 48}th IEEE Conf.\ on Decision and Control and
  Chinese Control Conference}, pages 3026--3031, 2009.

\bibitem{PapPil:02}
A.~Papoulis and S.{\;}U. Pillai.
\newblock {\em Probability, Random Variables and Stochastic Processes, {\rm 4th
  edition}}.
\newblock McGraw Hill, New York, 2002.

\bibitem{TemIsh:07}
R.~Tempo and H.~Ishii.
\newblock Monte {C}arlo and {L}as {V}egas randomized algorithms for systems and
  control: {A}n introduction.
\newblock {\em European J.\ Control}, 13:189--203, 2007.

\bibitem{TemCalDab_book}
R.~Tempo, G.~Calafiore, and F.~Dabbene.
\newblock {\em Randomized Algorithms for Analysis and Control of Uncertain Systems, with Applications}, 
Second Edition.
\newblock Springer, London, 2013.

\bibitem{AndBorHop:07}
R.~Andersen, C.~Borgs, J.~Chayes, J.~Hopcroft, V.{\;}S. Mirrokni, and S.-H.
  Teng.
\newblock Local computation of {P}age{R}ank contributions.
\newblock In {\em Algorithms and Models for the Web-Graph}, volume 4863 of {\em
  Lect.\ Notes Comp.\ Sci.}, pages 150--165. Springer, Berlin, 2007.

\bibitem{WesBerBer:10}
J.{\;}D. West, T.{\;}C.~Bergstrom, and C.{\;}T.~Bergstrom.
\newblock The {E}igenfactor metrics:  {A} network approach to assessing scholarly journals.
\newblock {\em College of Research Libraries}, 71:236--244, 2010.

\bibitem{Ber:07}
C.{\;}T.~Bergstrom.
\newblock {E}igenfactor:  {M}easuring the value and prestige of scholarly journals.
\newblock {\em C\&RL News}, 68:314-­316, 2007.

\bibitem{Franceschet:13}
M.~Franceschet.
\newblock Ten good reasons to use the Eigenfactor metrics.
\newblock {\em Information Processing \& Management}, 46:555--558, 2010.

\bibitem{BoyGhoPra:06}
S.~Boyd, A.~Ghosh, B.~Prabhakar, and D.~Shah.
\newblock Randomized gossip algorithms.
\newblock {\em IEEE Trans.\ Inform.\ Theory}, 52:2508--2530, 2006.

\bibitem{HatMes:05}
Y.~Hatano and M.~Mesbahi.
\newblock Agreement over random networks.
\newblock {\em IEEE Trans.\ Auto\-m.\ Control}, 50:1867--72, 2005.

\bibitem{TahJad:08}
A.~Tahbaz-Salehi and A.~Jadbabaie.
\newblock A necessary and sufficient condition for consensus over random
  networks.
\newblock {\em IEEE Trans.\ Auto\-m.\ Control}, 53:791--795, 2008.

\bibitem{Wu:06}
C.{\;}W. Wu.
\newblock Synchronization and convergence of linear dynamics in random directed
  networks.
\newblock {\em IEEE Trans.\ Auto\-m.\ Control}, 51:1207--1210, 2006.

\bibitem{TsuYam:08}
K.~Tsumura and H.~Yamamoto.
\newblock Optimal multiple controlling nodes problem 
for multi-agent systems via Alt-PageRank.
\newblock In {\em Proc.\ {\rm 4}th Workshop on Distributed Estimation and 
Control of Networked Systems}, pages 433--438, 2013.



\bibitem{BulCorMar:09}
F.~Bullo, J.~Cort\'{e}s, and S.\ Mart\'{i}nez.
\newblock {\em Distributed Control of Robotic Networks}.
\newblock Applied Mathematics Series,
Princeton University Press, 2009.

\bibitem{LinFraMag:05}
Z.~Lin, B.{\;}A.~Francis, and M.~Maggiore. 
\newblock Necessary and sufficient graphical conditions for formation control 
of unicycles. 
\newblock {\em IEEE Trans.\ Auto\-m.\ Control}, 50:121--127, 2005.

\bibitem{RenBea:05}
W.~Ren and R.{\;}W.~Beard. 
\newblock Information consensus in multivehicle cooperative control. 
\newblock {\em IEEE Trans.\ Autom.\ Control}, 50:655--661, 2005.


\bibitem{PhiKok:81}
R.{\;}G. Phillips and P.{\;}V. Kokotovic.
\newblock A singular perturbation approach to modeling and control of {M}arkov
  chains.
\newblock {\em IEEE Trans.\ Auto\-m.\ Control}, 26:1087--1094, 1981.

\bibitem{BroLem_infret:06}
A.~Z. Broder, R.~Lempel, F.~Maghoul, and J.~Pedersen.
\newblock Efficient {P}age{R}ank approximation via graph aggregation.
\newblock {\em Inform.\ Retrieval}, 9:123--138, 2006.

\bibitem{Newman:06}
M.{\;}E.{\;}J.~Newman.
\newblock Modularity and community structure in networks
\newblock {\em Proc.\ Natl.\ Acad.\ Sci.\ USA.}, 
103:8577--8582, 2006.

\bibitem{ExpEvaBlo:11}
P.~Expert, T.\;S.~Evans, V.{\;}D.~Blondel, and R.\ Lambiotte.
\newblock Uncovering space-independent communities in spatial networks.
\newblock {\em Proc.\ Natl.\ Acad.\ Sci.\ USA.}, 
108:7663--7668, 2011.

\bibitem{FlaLawGil:02}
G.\;W.~Flake, S.~Lawrence, C.\;L.~Giles, and F.\;M.\ Coetzee.
\newblock Self-organization and identification of Web communities.
\newblock {\em IEEE Computer}, 35(3):66--70, 2002.



\bibitem{AldKha:91}
R.{\;}W. Aldhaheri and H.{\;}K. Khalil.
\newblock Aggregation of the policy iteration method for nearly completely
  decomposable {M}arkov chains.
\newblock {\em IEEE Trans.\ Auto\-m.\ Control}, 36:178--187, 1991.

\bibitem{BiyArc:08}
E.~B{\i}y{\i}k and M.~Arcak.
\newblock Area aggregation and time-scale modeling for sparse nonlinear
  networks.
\newblock {\em Systems \& Control Letters}, 57:142--149, 2008.

\bibitem{ChoKok:85}
J.{\;}H. Chow and P.{\;}V. Kokotovic.
\newblock Time scale modeling of sparse dynamic networks.
\newblock {\em IEEE Trans.\ Auto\-m.\ Control}, 30:714--722, 1985.


\bibitem{KamHavGol:04}
S.~Kamvar, T.~Haveliwala, and G.~Golub.
\newblock Adaptive methods for the computation of {P}age{R}ank.
\newblock {\em Linear Algebra Appl.}, 386:51--65, 2004.

\bibitem{webdata}
Academic Web Link Database Project, 
Statistical Cybermetrics Research Group,
University of Wolverhampton, UK.
New Zealand University Web Sites, 2006.
\newblock 
Available at http://cybermetrics.wlv.ac.uk/database/.


\bibitem{SneCoc:89}
G.{\;}W. Snedecor and W.{\;}G. Cochran.
\newblock {\em Statistical Methods, {\rm 8th edition}}.
\newblock Iowa State Univ.\ Press, 1989.
























































































\end{thebibliography}









\end{document}
