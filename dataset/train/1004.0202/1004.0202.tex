We present in this section our new abstract domain \texttt{FPS}. In
Section~\ref{sec:floating-point-version-of-interval-slopes}, we adapt
the computation rules of interval slopes to take into account
floating-point arithmetic. Next in
Section~\ref{sec:arithmetic-operations}, we define an abstract
semantics of arithmetic expressions over \texttt{FPS} values taking
into account the behaviors of floating-point arithmetic. And in
Section~\ref{sec:order-structure}, we define the order structure of
the \texttt{FPS} domain.

\subsection{Floating-Point Version of Interval Slopes}
\label{sec:floating-point-version-of-interval-slopes}

The definition of interval slope expansion in
Section~\ref{sec:interval-arithmetic} manipulates real numbers. In
case of floating-point numbers, we have to take into account the
round-off function and the rounding-errors.

We show in Proposition~\ref{prop:sound-slope-round-nearest} that the
range of a non-linear function $\fun{f}$ of floating-point numbers can
be soundly over-approximated by a floating-point slope. The function
$\fun{f}$ must respect the correct rounding, \ie the property of
Equation~\eqref{eq:rounding-and-error-analysis} must hold. In other
words, the result of an operation over set of floating-point numbers
is over-approximated by the result of the same operation over
floating-point slopes by adding a small quantity depending on the
relative rounding error unit $\re$ and the absolute error $\sigma$.


\begin{proposition}
  \label{prop:sound-slope-round-nearest}
  Let $\fun{f}: D \subseteq \R^n \rightarrow \R$ be an arithmetic
  operation of the form $g \diamond h$ with $\diamond \in
  \{+,-,\times,\div\}$ or $\sqrt{~}$, \ie $\fun{f}$ respects the
  correct rounding. For all $\vinterval{X} \subseteq D$ and
  $\vscalar{z} \in D$, we have: {\small
    \begin{displaymath}
      \rnd\big(\fun{f}(\vinterval{X})\big) 
      \subseteq \fun{f}(\vscalar{z})\big
      (1 + [-\re,\re]\big) + 
      \left[-\frac{\sigma}{2}, \frac{\sigma}{2}\right] +
      \slope{F}{\vinterval{X}}{\vscalar{z}}(X-z)\big(1+[-\re,\re]\big)
      \enspace .
    \end{displaymath}
  }
  \begin{proof}
    {\small  \begin{align*}
        \rnd\big(\fun{f}(\vinterval{X})\big) 
        & = \{ \fun{f}(\vscalar{x}) (1+\varepsilon_x) + 
        \bar{\varepsilon}_x : \vscalar{x} \in \vinterval{X}\}
        && \text{by Eq.~\eqref{eq:rounding-and-error-analysis}}
        \\
        & \subseteq
        \fun{f}(\vinterval{X}) + 
        \fun{f}(\vinterval{X})\{ \varepsilon_x : \vscalar{x} 
        \in \vinterval{X}\} 
        + \{ \bar{\varepsilon}_x : \vscalar{x} \in \vinterval{X}\}
        &&
        \\
        & \subseteq
        \big(\fun{f}(\vscalar{z}) + \slope{F}{\vinterval{X}}{\vscalar{z}}
        (\vinterval{X}-\vscalar{z})\big) + 
        \{ \bar{\varepsilon}_x : \vscalar{x} \in \vinterval{X}\}
        &&  \text{by Eq.~\eqref{eq:slope}} 
        \\
        & \quad + \big(\fun{f}(\vscalar{z}) + 
        \slope{F}{\vinterval{X}}{\vscalar{z}}(\vinterval{X}-\vscalar{z})\big) 
        \{ \varepsilon_x : \vscalar{x} \in \vinterval{X}\} 
        &&
        \\
        & \subseteq \fun{f}(\vscalar{z}) 
        \big(1+\{ \varepsilon_x : \vscalar{x} \in \vinterval{X}\}\big) +
        \{ \bar{\varepsilon}_x : \vscalar{x} \in \vinterval{X}\}
        &&
        \\
        &
        \quad + \slope{F}{\vinterval{X}}{\vscalar{z}}
        (\vinterval{X}-\vscalar{z}) 
        \big(1+\{ \varepsilon_x : \vscalar{x} \in \vinterval{X}\}\big)
        &&
        \\
        & \subseteq 
        \fun{f}(\vscalar{z}) \big(1+[-\re,\re]\big) + 
        \left[-\frac{\sigma}{2},\frac{\sigma}{2}\right]
        && |\varepsilon_x| \leq \re
        \text{ by Eq.~\eqref{eq:rounding-and-error-analysis}}
        \\
        & \quad + \slope{F}{\vinterval{X}}{\vscalar{z}}
        (\vinterval{X}-\vscalar{z}) 
        \big(1+[-\re,\re]\big)
        &&
        |\bar{\varepsilon}_x| \leq \frac{1}{2}\sigma
        \text{ by Eq.~\eqref{eq:rounding-and-error-analysis}}
      \end{align*}
      \qed
    }
  \end{proof}
\end{proposition}

\begin{remark}
  As the floating-point version of slopes is based on $\re$ and
  $\sigma$, we can represent the floating-point behaviors depending of
  the hardware. For example, extended precision\footnote{In some
    hardware, \eg Intel x87, floating-point numbers may be encoded
    with $80$ bits in registers, \ie the significand is $64$ bits
    long.} is represented using the values $\re=2^{-64}$ and
  $\sigma=2^{-16446}$. Furthermore following \cite{BN10}, we can
  compute the result of a double rounding\footnote{It may happen on
    hardware using extended precision. Results of computations are
    rounded in registers and they are rounded again, with a less
    precision, in memory.}  with $\re=(2^{11}+2)2^{-64}$ and
  $\sigma=(2^{11}+1)2^{-1086}$. \end{remark}

Proposition~\ref{prop:sound-slope-round-nearest} shows that we can
compute the floating-point range of a function $\fun{f}$, respecting
the correct rounding, using interval slopes expansion. That is a set
of floating-point values can is represented by a pair:{\small
  \begin{displaymath}
    \left(
      \left[\fun{f}\right](\vinterval{z})
      \big(1 + \left[-\re,\re\right]\big) + 
      \left[-\frac{\sigma}{2}, \frac{\sigma}{2}\right],
      \quad
      \slope{F}{\vscalar{X}}{\vscalar{z}}\big(1+[-\re,\re]\big)
    \right)
    \enspace.
  \end{displaymath}
}The first element is a small interval rounding to the nearest around
$\fun{f}(\vscalar{z})$ for which we have to take into account the
possible rounding errors. The second element is the interval slopes
which have to take account of relative errors. Note that this
adaptation adds a very little overhead of computations compared to the
definition of interval slopes by Krawczyk and Neumaier.

\subsection{Semantics of Arithmetic Operations}
\label{sec:arithmetic-operations}

In this section, we define the abstract semantics of arithmetic
operations over elements of floating-point slopes domain in order to
mimic the behaviors of the floating-point arithmetic. We denote by
$\I$ the set of intervals and by $\s = \I \times \I^{|\varind|}$ the
set of slopes. An element $s$ of $\s$ is represented by a pair
$(\interval{M}, \vinterval{S})$ where $\interval{M}$ is a
floating-point interval and $\vinterval{S}$ is a vector of
floating-point intervals. We denote by $\langle \I, \sqsubseteq_\I,
\bot_\I, \top_\I, \sqcup_\I, \sqcap_\I \rangle$ the lattice of
intervals. First we define some auxiliary functions before presenting
the semantics of arithmetic expressions over \textsf{FPS}.

The function $\iota$ defined in Equation~\eqref{eq:iota} computes the
interval value associated to a floating-point slopes $(\interval{M},
\vinterval{S})$. We assume that the values of independent variables
are kept in a separate interval vector $\vinterval{V}_\varind$. The
notation $\fun{mid}(\vinterval{V}_\varind)$ stands for the
component-wise application of the function $\fun{mid}$ on all the
components of the vector $\vinterval{V}_\varind$. Note that $\cdot$
represents the scalar product. {\small
\begin{equation}
  \label{eq:iota}
  \iota\big( (\interval{M}, \vinterval{S}) \big) = 
  \interval{M} + \vinterval{S} \cdot 
  \big(\vinterval{V}_\varind - \fun{mid}(\vinterval{V}_\varind)\big)
\end{equation}
}The function $\kappa$ defined in Equation~\eqref{eq:kappa} transforms
an interval value $[a,b]^\ell$ associated to the $\ell$-th independent
variable into a floating-point slope. {\small
  \begin{equation}
    \label{eq:kappa}
    \kappa\left( [a,b]^\ell\right) = \big( [m,m], \delta_\ell \big)
    \quad\text{with}\quad m=\fun{mid}([a,b])
  \end{equation}
}This function $\kappa$ is used in two cases: \textit{i)}~To
initialize all the independent variables at the beginning of an
analysis. \textit{ii)}~In the meet operation, see
Section~\ref{sec:order-structure}.

We can detect overflows and generations of zero by using the function
$\Phi$ defined in Equation~\eqref{eq:fps-zero-overflow}. We have two
kinds or rules: \textit{total} rules when we are certain that a zero
or an overflow occur and \textit{partial} rules when a part of the set
described by a floating-point slope generates a zero or an
overflow. With the function $\iota$ we can determine for an element
$(\interval{M},\vinterval{S}) \in \s$ if $(\interval{M},
\vinterval{S})$ represents an overflow or a zero. Hence we represent
the finite precision of the floating-point arithmetic. We denote by
$\text{\textbf{p}}_\infty$ and by $\text{\textbf{m}}_\infty$ the
interval vectors with all their components equal to
$[+\infty,+\infty]$ and $[-\infty,-\infty]$ respectively. We recall
that $\sigma$ is the smallest denormalized and $\Sigma$ is the largest
floating-point numbers. {\small
  \begin{equation}
    \label{eq:fps-zero-overflow}
    \Phi(\interval{M}, \vinterval{S}) = 
    \begin{cases}
      ([0,0], \mathbf{0}) 
      & \text{if } \iota(\interval{M}, \vinterval{S}) \sqsubseteq_\I
      [-\frac{\sigma}{2},\frac{\sigma}{2}]
      \\
      (\tilde{\interval{M}},
      \mathbf{0}\ \dot{\sqcup}_\I\ S) 
      & \text{if } \iota(\interval{M}, \vinterval{S})
      \ \sqcap_\I\ ]-\frac{\sigma}{2},\frac{\sigma}{2}[\ \neq \bot_\I
      \\
      & \text{and }\tilde{\interval{M}} =
      \begin{cases}
        [0,0] & \text{if } \interval{M}\ 
        \sqsubseteq_\I\ ]-\frac{\sigma}{2},\frac{\sigma}{2}[
        \\
[0,0] \sqcup_\I \interval{M} & \text{otherwise}
      \end{cases}
      \\
      ([+\infty, +\infty], \text{\textbf{p}}_\infty) 
      & \text{if } \iota(\interval{M}, \vinterval{S}) \sqsubseteq_\I\ 
      ]\Sigma,+\infty]
      \\
      (\tilde{\interval{M}}, \text{\textbf{p}}_\infty\ \dot{\sqcup}_\I\ S) & 
      \text{if }
      \iota(\interval{M}, \vinterval{S})\ 
      \sqcap_\I\ ]\Sigma,+\infty] \neq \bot_\I
      \\
      & \text{and }\tilde{\interval{M}} =
      \begin{cases}
        [+\infty, +\infty] & \text{if } 
        \interval{M}\ \sqsubseteq_\I\ ]\Sigma,+\infty]
        \\
[+\infty,+\infty] \sqcup_\I \interval{M} & \text{otherwise}
      \end{cases}
      \\
      ([-\infty, -\infty], \text{\textbf{m}}_\infty) 
      & \text{if } \iota(\interval{M}, \vinterval{S}) \sqsubseteq_\I
      [-\infty, -\Sigma[
      \\
      (\tilde{\interval{M}}, 
      \text{\textbf{m}}_\infty\ \dot{\sqcup}_\I\ S) & \text{if }
      \iota(\interval{M}, \vinterval{S})
      \sqcap_\I [-\infty, -\Sigma[\ \neq \bot_\I
      \\
      & \text{and }\tilde{\interval{M}} =
      \begin{cases}
        [-\infty, -\infty] & \text{if } 
        \interval{M}\ \sqsubseteq_\I\ [-\infty, -\Sigma[
        \\
[-\infty,-\infty] \sqcup_\I \interval{M} & \text{otherwise}
      \end{cases}
      \\
      (\interval{M}, \vinterval{S}) & \text{otherwise}
    \end{cases}
  \end{equation}
}Equation~\eqref{eq:fps-zero-overflow} is an adaptation of the rule
defined in Equation~\eqref{eq:zero-overflow} to deal with \fps
values. Furthermore, the abstract values $(+\infty,
\mathbf{p}_\infty)$ and $(-\infty, \mathbf{m}_\infty)$ represent the
special floating-point values $+\infty$ and $-\infty$ respectively. As
in floating-point arithmetic, the values $(+\infty,
\mathbf{p}_\infty)$ and $(-\infty, \mathbf{m}_\infty)$ are absorbing
elements.

An interesting feature of interval slopes is that we can mimic the
absorption phenomenon by setting to zero the interval slope of the
absorbed operand. We define the function $\rho$ for this
purpose. Indeed, an abstract value $(\interval{M}, \vinterval{S})$
already supports partial absorption as $\interval{M}$ is computed with
a rounding to the nearest but $\vinterval{S}$ have to be
\textit{reduced} to represent the absence of the influence of
particular independent variables. The reduction of an abstract value
$g=(\interval{M}_g, \vinterval{S}_g)$ compared to an abstract value
$h=(\interval{M}_h, \vinterval{S}_h)$, denoted by $\rho(g \mid h)$, is
defined in Equation~\eqref{eq:slope-reduction}.  {\small
  \begin{equation}
    \label{eq:slope-reduction}
    \rho(g \mid h) =         
    \begin{cases}
      ([0, 0], \mathbf{0}) & \text{if } 
      \iota\big(\interval{M}_g, \vinterval{S}_g\big)
      \sqsubseteq_\I [\re,\re]
      \times \iota\big(\interval{M}_h, \vinterval{S}_h\big)
      \\
      \big(\tilde{\interval{M}}_g, 
      \mathbf{0}\ \dot{\sqcup}_\I\ \vinterval{S}_g\big) 
      & \text{if } \iota
      \big(\interval{M}_g, \vinterval{S}_g\big) \sqcap_\I 
      [\re, \re] \times \iota\big(\interval{M}_h, \vinterval{S}_h\big)
      \neq \bot_\I
      \\
      & \text{and } \tilde{\interval{M}}_g =
      \begin{cases}
        [0, 0] & \text{if } \interval{M}_g\ \sqsubseteq_\I\ [\re,\re]
        \times
        \iota\big(\interval{M}_h, \vinterval{S}_h\big)
        \\
[0, 0] \sqcup_\I \interval{M}_g & \text{otherwise}
      \end{cases}
      \\
      \big(\interval{M}_g, \vinterval{S}_g\big) & \text{otherwise}
    \end{cases}
  \end{equation}
}Equation~\eqref{eq:slope-reduction} models the absorption phenomenon
by explicitly setting to zero the values of a slope. As mentioned in
Section~\ref{sec:interval-arithmetic}, a slope shows which variables
influence the computation of an arithmetic expression. But, absorption
phenomena induce that an operand does not influence the result of an
addition or a subtraction any more.

Using the functions $\Phi$, $\rho$ and $\iota$, we inductively define
on the structure of arithmetic expressions the abstract semantics
$\sem{.}$ of floating-point slopes in
Figure~\ref{fig:arithmetic-operation-floating-point-slopes}. We denote
by $\text{env}^\sharp$ an abstract environment which associates to
each program variable a floating-point slope. For each arithmetic
operation, we component-wisely combine the elements of the abstract
operands $\sem{g}(\text{env}^\sharp)=(\interval{M}_g,
\vinterval{S}_g)$ and $\sem{h}(\text{env}^\sharp)=(\interval{M}_h,
\vinterval{S}_h)$. The element $\interval{M}$ is obtained using the
interval arithmetic with rounding to the nearest. The element
$\vinterval{S}$ is computed using the definition of the slope
arithmetic defined in Table~\ref{tab:automatic-differentiation-rules}.
We take into account of the possible rounding errors in the result
$(\interval{M}, \vinterval{S})$ following
Proposition~\ref{prop:sound-slope-round-nearest}. In case of addition
and subtraction, according to the
Equation~\eqref{eq:rounding-and-error-analysis}, we do not consider
absolute error $\frac{\sigma}{2}$ which is always zero. Moreover, in
case of addition or subtraction, we handle the absorption phenomena
using the function $\rho$, defined in
Equation~\eqref{eq:slope-reduction}. Finally, we check if a zero or an
overflow is generated by applying the function $\Phi$ defined in
Equation~\eqref{eq:fps-zero-overflow}.

\begin{figure}[ht] {\small
    \begin{displaymath}
      \begin{split}
        \sem{g \pm h}(\theta^\sharp) & =\Phi\left( (\tilde{\interval{M}}_g 
          \pm \tilde{\interval{M}}_h)(1+[-\re,\re]),\quad
          \left(\tilde{\vinterval{S}}_g \pm 
            \tilde{\vinterval{S}}_h\right)(1+[-\re,\re]) \right)
        \\
        &
        \begin{flalign*}
          \text{with}
          \quad (\tilde{\interval{M}}_g,\tilde{\vinterval{S}}_g) = 
          \rho(g \mid h)
          \text{ and } (\tilde{\interval{M}}_h,\tilde{\vinterval{S}}_h)
          = \rho(h \mid g)
        \end{flalign*}
        \\
        \sem{g \times h}(\theta^\sharp) & = \Phi 
        \left(\interval{M},\quad
          \big(
          \vinterval{S}_g \times \iota(\interval{M}_h, \vinterval{S}_h) 
          + \interval{M}_g \times \vinterval{S}_h
          \big) (1+[-\re,\re])        
        \right)
        \\
        &
        \begin{flalign*}
          \text{with}
          \quad \interval{M} = 
          (\interval{M}_g \times \interval{M}_h)(1 + [-\re,\re]) + 
          \left[\frac{\sigma}{2}, \frac{\sigma}{2}\right]
        \end{flalign*}
        \\
        \sem{\frac{g}{h}}(\theta^\sharp) & = 
        \Phi \left(
          \interval{M},\quad
          \frac{\vinterval{S}_g - \vinterval{S}_h
            \frac{\interval{M}_g}{\interval{M}_h}}
          {\iota(\interval{M}_h, \vinterval{S}_h)} (1+[-\re,\re])
        \right) 
\\
        &
        \begin{flalign*}
          \begin{split}
          \text{with}\quad
          \interval{M} & =  
          \frac{\interval{M}_g}{\interval{M}_h}(1 + [-\re,\re]) + 
          \left[\frac{\sigma}{2}, \frac{\sigma}{2}\right],
          \\
          0 & \not\in \iota(\interval{M}_h, \vinterval{S}_h) 
          \text{ and } 0 \not\in \interval{M}_h
        \end{split}
        \end{flalign*}
        \\              
        \sem{\sqrt{g}}(\theta^\sharp) & = 
        \Phi \left(
        \interval{M},\quad
        \left(
          \frac{\vinterval{S}_g}{\sqrt{\interval{M}_g}+
            \sqrt{\iota(\interval{M}_g, \vinterval{S}_g)}} 
        \right) (1+[-\re,\re])
      \right)
      \\
      &
      \begin{flalign*}
        \begin{split}
        \text{with}\quad 
        \interval{M} &= 
        \left(\sqrt{\interval{M}_g}\left(1+[-\re,\re]\right)\right)+
        \left[-\frac{\sigma}{2},\frac{\sigma}{2}\right],
        \\
        \interval{M}_g & \sqcap_\I [-\infty,0] = \bot_\I        
        \text{ and } \iota(\interval{M}_g, \vinterval{S}_g) 
         \sqcap_\I [-\infty,0] = \bot_\I
      \end{split}
      \end{flalign*}
    \end{split}
  \end{displaymath}
}
\caption{Abstract semantics of arithmetic expressions on
  floating-point slopes}
\label{fig:arithmetic-operation-floating-point-slopes}
\end{figure}

\begin{remark}
  The functions $\Phi$ and $\rho$ make the arithmetic operations on
  floating-point slopes non associative and non distributive as in
  floating-point arithmetic.
\end{remark}

\subsection{Order Structure}
\label{sec:order-structure}

In this section, we define the order structure of the set $\s$ of
floating-point slopes. In particular, this structure is based on the
lattice of intervals. We recall that the set of slopes $\s = \I \times
\I^{|\varind|}$ and an element $s$ of $\s$ is a pair $(\interval{M},
\vinterval{S})$.

We define a partial order, the join and the meet operations between
elements of $\s$. All these operations are defined as a component-wise
application of the associated operations of the interval domain except
the meet operation which needs extra care. We denote by
$\dot{\sqsubseteq}_\I$ the component-wise application of the interval
order. We can define a partial order $\sqsubseteq_\s$ between elements
of $\s$ with: {\small
  \begin{multline}
    \label{eq:slopes-order}
    \forall (\interval{M}_g, \vinterval{S}_g), (\interval{M}_h, \vinterval{S}_h)
    \in \s,
\left(\interval{M}_g, \vinterval{S}_g\right) 
    \sqsubseteq_\s \left(\interval{M}_h, \vinterval{S}_h\right)
    \Leftrightarrow \interval{M}_g \sqsubseteq_\I \interval{M}_h 
    \wedge \vinterval{S}_g\
    \dot{\sqsubseteq}_\I\ \vinterval{S}_h\enspace .
  \end{multline}
}

The join operation $\sqcup_\s$ over floating-point slopes is defined
in Equation~\eqref{eq:join-operation}. We denote by $\dot{\sqcup}_\I$
the component-wise application of the operation $\sqcup_\I$.  {\small
  \begin{multline}
    \label{eq:join-operation}
    \forall (\interval{M}_g, \vinterval{S}_g), 
    (\interval{M}_h, \vinterval{S}_h) \in \s, \quad
    \big(\interval{M}_g, \vinterval{S}_g\big) \ 
    \sqcup_\s\ \big(\interval{M}_h, \vinterval{S}_h\big) = \big(\interval{M},
    S\big) 
    \\
    \text{with}\quad
    \interval{M} = \interval{M}_g 
    \sqcup_\I \interval{M}_h \quad\text{and}\quad S = \vinterval{S}_g\
    \dot{\sqcup}_\I\ \vinterval{S}_h 
  \end{multline}
}

There is no direct way to define the greatest lower bound of two
elements of $\s$. Indeed, two abstract values may represent the same
concrete value but without being comparable. Hence we only have a
join-semilattice structure. The meet operation $\sqcap_\s$ over
floating-point slopes is defined in
Equation~\eqref{eq:meet-operation}. It may require a conversion into
interval value. We consider that the result of the meet operation
introduces a new independent variable at index $\ell$. We denote by
$\sqsubset_\I$ the strict comparison of intervals and by $\bot_\s$ the
least element of $\s$. {\small
  \begin{multline}
    \label{eq:meet-operation}
    \forall (\interval{M}_g, \vinterval{S}_g), 
    (\interval{M}_h, \vinterval{S}_h) \in \s, \quad  
    \big(\interval{M}_g, \vinterval{S}_g\big) 
    \ \sqcap_\s\
    \big(\interval{M}_h, \vinterval{S}_h\big) = 
    \\
    \begin{cases}
      \bot_\s 
      & \text{if } \iota(\interval{M}_g, \vinterval{S}_g) 
      \sqcap_\I \iota(\interval{M}_h, \vinterval{S}_h) = \bot_\I
      \\
     (\interval{M}_g, \vinterval{S}_g) 
     & \text{if } \iota(\interval{M}_g, \vinterval{S}_g) 
     \sqsubset_\I \iota(\interval{M}_h, \vinterval{S}_h)
     \\
     (\interval{M}_h, \vinterval{S}_h) 
     & \text{if } \iota(\interval{M}_h, \vinterval{S}_h) 
     \sqsubset_\I \iota(\interval{M}_g, \vinterval{S}_g)
     \\
     \kappa\big(\iota(\interval{M}_h, \vinterval{S}_h) 
     \sqcap^\ell_\I \iota(\interval{M}_g, \vinterval{S}_g)\big) 
     & \text{otherwise}
    \end{cases}
  \end{multline}
}

\subsubsection*{Note on the Widening Operator.}
\label{sec:acceleration-of-convergence}

In order to enforce the convergence of the fixpoint computation, we
can define a widening operation $\nabla_\s$ over floating-point slopes
values. An advantage of our domain is that we can straightforwardly
use the widening operations defined for the interval domain denoted by
$\nabla_\I$. We define the operator $\nabla_\s$ in
Equation~\eqref{eq:widening-operation} using the widening operator
between intervals. The notation $\dot{\nabla}_\I$ represents the
component-wise application of $\nabla_\I$ between the components of
the interval slopes vector. {\small
  \begin{multline}
    \label{eq:widening-operation}
    \forall (\interval{M}_g, \vinterval{S}_g), 
    (\interval{M}_h, \vinterval{S}_h) \in \s,\quad
    \big(\interval{M}_g, \vinterval{S}_g\big) \nabla_\s 
    \big(\interval{M}_h, \vinterval{S}_h\big) = 
    \big(\interval{M}, \vinterval{S}\big)
    \\
    \text{with}\quad
    \interval{M} = \interval{M}_g\ \nabla_\I\ \interval{M}_h 
    \quad\text{and}\quad \vinterval{S} = \vinterval{S}_g\
    \dot{\nabla}_\I\ \vinterval{S}_h
  \end{multline}
}

