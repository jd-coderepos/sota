\section{Experiments}
\label{sec:exp}

\noindent\textbf{Datasets.} We evaluate \ours on three datasets: YouTube-VIS 2019/2021~\cite{yang2019video} and Occluded VIS (OVIS)~\cite{qi2021occluded}. The YouTube-VIS datasets contain 40 object classes. YouTube-VIS 2019 contains 2238/302/343 videos for training/validation/testing, while YouTube-VIS 2021 expands the dataset to 2985/421/453 videos for training/validation/testing, and includes higher quality annotations. OVIS has 25 object classes and contains 607/140/154 for training/validation/testing. While the number of videos is smaller, OVIS has more objects per frame, and the videos are also longer. This leads to more annotated instance masks compared to the YouTube-VIS datasets. In addition, OVIS also has much higher Bounding-box Occlusion Rate (0.22 v.s. 0.06/0.07) compared to the YouTube-VIS datasets, which indicates heavier occlusions between object instances.

\noindent\textbf{Metrics.} We follow previous works and use Average Precision (AP) and Average Recall (AR) as evaluation metrics~\cite{yang2019video}. AP is computed based on 10 intersection-over-union (IoU) thresholds from 50\% to 95\% with 5\% increment. The reported AP and AR are first computed for each object class and then averaged over all classes. All three datasets have public evaluation servers.

\noindent\textbf{Baselines.} We focus on results using ResNet50 and Swin-L backbones. ResNet50 is still the most widely used backbone for VIS, while Swin-L gives the best performances. Not all methods report both backbones on all three datasets. We include results that are available. For YouTube-VIS datasets, we include recent state-of-the-art results from SeqFormer~\cite{wu2021seqformer}, TeViT~\cite{yang2022tevit}, and Mask2Former-VIS~\cite{cheng2021mask2former}. These are all Transformer-based per-clip approaches as this paradigm has been recently dominating the field. On the other, out of of these methods, only TeViT is applied to OVIS. Therefore, we further compare to CMaskTrack R-CNN~\cite{qi2021occluded}, CrossVIS~\cite{yang2021crossover}, and STC~\cite{jiang2022stc}. These are all methods that allow online processing. Even TeViT uses a near online inference for OVIS~\cite{athar2020stem}. This is because OVIS has longer videos that would lead to out-of-memory for most of the per-clip approaches. 

Out of all the baselines,  Mask2Former-VIS~\cite{cheng2021mask2former} is the most related to \ours, as \ours is built on Mask2Former in this work. Mask2Former-VIS thus can be seen as the per-clip version of ours and is an important baseline for comparison. Therefore, we further apply Mask2Former-VIS on the OVIS dataset. Due to memory constraints, the videos in OVIS are first split into clips of length 30. We use the same post-processing as \ours to merge the outputs from these clips.

\noindent\textbf{Implementation Details.} 
Unless otherwise noted, our hyper-parameters follow Mask2Former-VIS~\cite{cheng2021mask2former}. All models are pre-trained with COCO instance segmentation~\cite{lin2014microsoft}. For OVIS, we use the same hyper-parameters as YouTube-VIS 2019 except training for  iterations instead of . For training losses, the weights are  for  and  for . All results of \ours are averaged over 3 random seeds. We sub-sample training to X\% by uniformly sampling frames in the video. We set a minimum of 1 frame per video. Since YouTube-VIS datasets often have videos less than a hundred frames. Our 1\% results are better seen as one frame per video results for YouTube-VIS.

\subsection{Main Results}

\begin{table}
  \caption{YouTube-VIS 2019 results. C80k indicates joint training with COCO images that have YouTube-VIS categories. \ours with X\% means sub-sampling the annotated frames in training.}
  \label{tab:ytvis2019}
  \centering
  \tabfontsize
  \begin{tabular}{lllccccc}
    \toprule
Method         & Backbone & Training  & AP   & AP & AP & AR & AR \\\midrule
TeViT~\cite{yang2022tevit}           & R50  &Full      & 42.1 & 67.8    & 44.8    & 41.3   & 49.4    \\
TeViT~\cite{yang2022tevit}           & MsgShifT  &Full      & 46.6 & \textbf{71.3}    & 51.6    & 44.9   & 54.3    \\
SeqFormer~\cite{wu2021seqformer}       & R50 &Full     & 45.1 & 66.9    & 50.5    & 45.6   & 54.6    \\
SeqFormer~\cite{wu2021seqformer}       & R50 &Full+C80k     & \textbf{47.4} & 69.8    & 51.8    & 45.5   & 54.8    \\
Mask2Former-VIS~\cite{cheng2021mask2former} & R50 &Full     & 46.4 & 68.0    & 50.0    & --     & --      \\
\ours            & R50 &Full     & \textbf{47.4} & 69.0    & \textbf{52.1}    & \textbf{45.7}   & \textbf{55.7}    \\\midrule
TeViT~\cite{yang2022tevit}           & Swin-L &Full  & 56.8 & 80.6    & 63.1    & 52.0   & 63.3    \\
SeqFormer~\cite{wu2021seqformer}       & Swin-L &Full+C80k   & 59.3 & 82.1    & 66.4    & 51.7   & 64.4    \\
Mask2Former-VIS~\cite{cheng2021mask2former} & Swin-L &Full   & 60.4 & \textbf{84.4}    & 67.0    & --     & --      \\
\ours            & Swin-L &Full  & \textbf{61.6} & 83.3    & \textbf{68.6}    & \textbf{54.8}   & \textbf{66.6}    \\\midrule
\ours      & Swin-L &1\%    & 59.0 & 81.6    & 64.7    & 54.0   & 64.0    \\
\ours      & Swin-L &5\%   & 59.3 & 81.4    & 65.8    & 53.8   & 64.1    \\
\ours    & Swin-L &10\%   & 61.0 & 83.0    & 67.7    & 54.6   & 66.1    \\
    \bottomrule
  \end{tabular}
\end{table}


\noindent\textbf{YouTube-VIS 2019.} The results for YouTube-VIS 2019 are shown in \tabref{ytvis2019}. 
\ours achieves highest AP and most other metrics for both ResNet-50 and Swin-L backbones.  SeqFormer shows that it is beneficial to jointly train with  images from COCO~\cite{lin2014microsoft} that contain YouTubeVIS categories (+C80k in table). TeViT proposes messenger shift transformer (MsgShifT) that are as efficient as ResNet backbones, while improving the VIS performances. Our ResNet-50 results match or outperform their results without further modifications. Compared to the state-of-the-art Mask2Former-VIS, which can be seen as the per-clip approach to apply Mask2Former to VIS, \ours consistently outperforms by around 1\% for both backbones.
\ours with X\% means sub-sampling the annotated frames for each video in training.  Since there are less temporal variations for videos in  YouTube-VIS 2019, \ours with 1\% of training frames only reduces AP by 2.6\%. This significantly reduces the annotation effort while not sacrificing much performance.


\begin{table}
  \caption{YouTube-VIS 2021 Results. \ours's performance improvement increases on the more challenging YouTube-VIS 2021. Our 1\% results already outperform previous state-of-the-art.}
  \label{tab:ytvis2021}
  \centering
  \tabfontsize
  \begin{tabular}{lllccccc}
    \toprule
Method          & Backbone & Training  & AP   & AP & AP & AR & AR \\\midrule
TeViT~\cite{yang2022tevit}           & MsgShifT & Full      & 37.9 & 61.2    & 42.1    & 35.1   & 44.6    \\
SeqFormer~\cite{wu2021seqformer}       & R50      & Full+C80k & 40.5 & 62.4    & 43.7    & 36.1   & 48.1    \\
Mask2Former-VIS~\cite{cheng2021mask2former} & R50      & Full      & 40.6 & 60.9    & 41.8    & --     & --      \\
\ours            & R50      & Full      & \textbf{44.2} & \textbf{66.0}    & \textbf{48.1}    & \textbf{39.2}   & \textbf{51.7}    \\\midrule
SeqFormer~\cite{wu2021seqformer}       & Swin-L   & Full+C80k & 51.8 & 74.6    & 58.2    & 42.8   & 58.1    \\
Mask2Former-VIS~\cite{cheng2021mask2former} & Swin-L   & Full      & 52.6 & 76.4    & 57.2    & --     & --      \\
\ours            & Swin-L   & Full      & \textbf{55.3} & \textbf{76.6}    & \textbf{62.0}    & \textbf{45.9}   & \textbf{60.8}    \\\midrule
\ours            & Swin-L   & 1\%       & 52.9 & 74.9    & 58.9    & 44.7   & 58.3    \\
\ours            & Swin-L   & 5\%       & 54.3 & 76.3    & 60.1    & 45.4   & 59.5    \\
\ours            & Swin-L   & 10\%      & 54.9 & 76.3    & 61.9    & 45.3   & 60.1    \\
    \bottomrule
  \end{tabular}
\end{table}

\noindent\textbf{YouTube-VIS 2021.} The results for YouTube-VIS 2021 are shown in \tabref{ytvis2021}. On this more challenging dataset, the performance improvements for \ours increase compared to YouTube-VIS 2019. Without better backbone like TeViT and additional training data like SeqFormer, our ResNet-50 results outperform by a large margin for all metrics. This is the also case for Swin-L. \ours outperforms previous state-of-the-art Mask2Former-VIS by 2.7\%. By using only 1\% of training frames, \ours's AP decrease by only 2.4\%, which means that our 1\% result still outperforms previous state-of-the art. We also see that on YouTube-VIS datasets, reducing the annotations by 10x does not significantly affect the performances (-0.6\% AP for 2019 and -0.4\% AP for 2021).




\begin{table}
  \caption{OVIS Results. \ours significantly outperform existing approaches on OVIS. Our image-based framework leads to easier and better learning on this dataset with heavy occlusions.}
  \label{tab:ovis}
  \centering
  \tabfontsize
  \begin{tabular}{lllccccc}
    \toprule
Method               & Backbone & Training & AP   & AP & AP & AR & AR \\\midrule
TeViT~\cite{yang2022tevit}                & MsgShifT & Full     & 17.4 & 34.9      & 15.0      & 11.2   & 21.8      \\
CrossVIS~\cite{yang2021crossover}             & R50      & Full     & 14.9 & 32.7      & 12.1      & 10.3   & 19.8      \\
CMaskTrack R-CNN~\cite{qi2021occluded}     & R50      & Full     & 15.4 & 33.9      & 13.1      & 9.3    & 20.0      \\
STC~\cite{jiang2022stc}                  & R50      & Full     & 15.5 & 33.5      & 13.4      & 11.0   & 20.8      \\
Mask2Former-VIS*     & R50      & Full     & 17.3 & 37.3      & 15.1      & 10.5   & 23.5      \\
\ours                 & R50      & Full     & \textbf{25.0} & \textbf{45.5}      & \textbf{24.0}      & \textbf{13.9}   & \textbf{29.7}      \\\midrule
MaskTrack R-CNN*+SWA~\cite{li2021limited} & Swin-L   & Full     & 28.9 & 56.3      & 26.8      & 13.5   & 34.0      \\
Mask2Former-VIS*    & Swin-L   & Full     & 25.8 & 46.5      & 24.4      & 13.7   & 32.2      \\
\ours                 & Swin-L   & Full     & \textbf{39.4} & \textbf{61.5}      & \textbf{41.3}      & \textbf{18.1}   & \textbf{43.3}      \\\midrule
\ours                 & Swin-L   & 1\%      & 31.7 & 54.9      & 31.3      & 16.3   & 36.1      \\
\ours                 & Swin-L   & 5\%      & 35.7 & 60.1      & 35.8      & 17.3   & 39.9      \\
\ours                 & Swin-L   & 10\%     & 37.2 & 60.7      & 38.0      & 17.3   & 41.1      \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figs/qual_sheep.pdf}
  \caption{Qualitative results on OVIS. \ours stably tracks all the sheep in the video. 
  Using mask overlap based heuristics instead leads to multiple identity switches in tracking.
  Mask2Former-VIS* uses per-clip training that is difficult to optimize on the challenging OVIS dataset.}
  \label{fig:qual}
\end{figure}



\noindent\textbf{Occluded VIS (OVIS).}  The results for OVIS are shown in \tabref{ovis}. 
Mask2Former-VIS* denotes our application of Mask2Former-VIS to OVIS. Since videos in OVIS can have up to hundreds of frames, we apply Mask2Former-VIS to non-overlapping sliding windows of length 30. The outputs from these clips are then merged by our post-processing. \ours is an online method and does not need modification to apply to OVIS. 
\ours shows significant improvement compared to existing works on OVIS. 
With ResNet-50 backbone, \ours outperforms previous state-of-the-art TeViT with MsgShifT backbone by 7.6\% AP. With Swin-L backbone, \ours outperforms previous best result MaskTrack R-CNN*+SWA by 10.5\% AP, which is the winner of the 1st OVIS Challenge. Their key observation is that sampling frames that are far apart in OVIS leads to drastically different features and makes it hard to train MaskTrack R-CNN. This is in contrast to YouTube-VIS datasets, in which the videos are shorter and there are less temporal variations within the video. We observe the same phenomenon when training Mask2Former-VIS*. However, the limited sampling reference frame strategy of MaskTrack R-CNN*+SWA still does not work in this case. Mask2Former-VIS* uses a fully end-to-end loss instead of an explicit tracking loss to learn temporal association, which makes the learning even harder in OVIS. On the other hand, \ours is image-based and does not need to worry about the temporal sampling strategy to train the model. This is contrary to common belief that per-frame approaches are worse for scenarios with heavy occlusions. Instead, our image-based approach leads to easier and better learning on OVIS. 
We show that an image instance segmentation model that can segment occluded instances in each frame is also good at associating such instances across frames. 
\figref{qual} shows qualitative results. \ours stably tracks all the sheep in the video. Using mask overlap based heuristics instead of query matching leads to multiple identity switches in tracking.
Mask2Former-VIS* does not have as good segmentation masks because its training is interfered by heavy occlusions and large appearance deformations between frames in OVIS.  

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figs/qual_fail.pdf}
  \caption{Failure cases of \ours on OVIS. When an object instance disappear from a video, \ours can fail by associating its query embedding to a wrong mask without overlap (top). This is because we do not use mask overlap heuristics in our work. On the other hand, we are also limited by the image instance segmentation model, which might not work well on close-up objects (bottom). }
  \label{fig:qual-fail}
\end{figure}


\figref{qual-fail} shows additional qualitative results on failure cases of \ours on the OVIS dataset. As discussed in \secref{match}, \ours does not use heuristics to handle the birth and death of object instances. The death of an object instance is correctly handled if its query is matched to a query in the next frame that produces an empty mask. Despite its simplicity and effectiveness, the drawback of this approach is that there is nothing stopping the model from matching the disappearing query to a query with a non-empty mask. From  to  in the top row of \figref{qual-fail}, as the fish in the lower left leaves the frame, \ours associates it to a mask covering the tail of a nearby fish. From  to , when the fish in the upper left leaves the frame, \ours again associates it to a mask covering the head of a nearby fish. Since the associated masks are non-empty, \ours fails to correctly handle the death of these instances. On the other hand, when the two dogs in the bottom row of \figref{qual-fail} are covered in , \ours correctly associates their queries to empty masks. \ours further correctly handles the object births in . However, \ours is limited by the segmentation of the image segmentation model, which fails to segment the close-up person.


\subsection{Analyzing Query Matching}
\label{sec:analysis}

\begin{table}
  \caption{Comparison of post-processing. Heuristics based on mask over laps lead to significant AP drop on OVIS. Our query matching approach has simpler design without loss of performance.}
  \label{tab:heu}
  \centering
  \tabfontsize
  \begin{tabular}{llccccc}
    \toprule
Method             & Dataset          & AP   & AP & AP & AR & AR \\\midrule
heuristics only    & YouTube-VIS 2019 & 58.2 & 79.2      & 64.1      & 51.3   & 63.6      \\
heuristics + query & YouTube-VIS 2019 & 61.3 & 82.8      & \textbf{68.7}      & 54.3   & 66.3      \\
query only         & YouTube-VIS 2019 & \textbf{61.6} & \textbf{83.3}      & 68.6      & \textbf{54.8}   & \textbf{66.6}      \\\midrule
heuristics only    & YouTube-VIS 2021 & 52.7 & 75.3      & 57.3      & 44.4   & 58.3      \\
heuristics + query & YouTube-VIS 2021 & 55.1 & 76.2      & 61.9      & \textbf{46.0}   & 60.7      \\
query only         & YouTube-VIS 2021 & \textbf{55.3} & \textbf{76.6}      & \textbf{62.0}      & 45.9   & \textbf{60.8}      \\\midrule
heuristics only    & Occluded VIS     & 31.7 & 56.0      & 31.3      & 15.8   & 35.8      \\
heuristics + query & Occluded VIS     & 39.1 & \textbf{62.5}      & 40.8      & 17.7   & \textbf{43.4}      \\
query only         & Occluded VIS     & \textbf{39.4} & 61.5      & \textbf{41.3}      & \textbf{18.1}   & 43.3      \\
    \bottomrule
  \end{tabular}
\end{table}



The success of \ours depends on whether query matching is good for tracking instances. We conduct ablation studies by comparing it to manually designed heuristics. We use the bipartite matching heuristics in \secref{training} for tracking by treating instances in the last frame as groundtruth. The results are in \tabref{heu}. Using heuristics lead to around 3\% AP drop on both YouTube-VIS 2019 and 2021. It leads to more significant drop on OVIS (7.7\%) due to heavier occlusions. We also combine query matching and heuristics with equal weights, which has mixed results. Our query only approach is simpler and more generalizable without loss of performance.


We visualize the learned query embeddings by t-SNE~\cite{van2008visualizing} in \figref{tsne}. Each plot is for a video in the training set. We visualize the training set to see the effect of an image only objective (to segment instances in an image) on query embeddings across different frames. Query embeddings of the same instance have the same color. We obtain the instance IDs for queries by bipartite matching its outputs to groundtruth instances, which have consistent IDs across frames. Without any video-based tracking objective, query embeddings of the same instances are already grouped into distinct clusters, even for the OVIS dataset. This supports our design of only using image-based objectives. In Appendix~\ref{sec:tsne-val}, we further visualize query embeddings on videos not used in training.


\begin{table}
  \caption{Results for adding supervision to query matching. The supervision can provide dataset dependent benefit if the temporal hyper-parameters are selected properly. }
  \label{tab:sup}
  \centering
  \tabfontsize
  \begin{tabular}{llccccc}
    \toprule
Method                & Dataset          & AP   & AP & AP & AR & AR \\\midrule
\ours                  & YouTube-VIS 2019 & \textbf{61.6} & \textbf{83.3}      & \textbf{68.6}      & \textbf{54.8}   & \textbf{66.6}      \\
+ Supervised Matching & YouTube-VIS 2019 & 61.0 & 82.1      & 67.6      & 54.3   & 66.1      \\
+ Limited Range       & YouTube-VIS 2019 & 60.7 & 82.5      & 67.0      & 54.1   & 65.5      \\\midrule
\ours                  & YouTube-VIS 2021 & \textbf{55.3} & 76.6      & \textbf{62.0}      & \textbf{45.9}   & \textbf{60.8}      \\
+ Supervised Matching & YouTube-VIS 2021 & 54.4 & 75.7      & 60.6      & 45.5   & 59.5      \\
+ Limited Range       & YouTube-VIS 2021 & 55.2 & \textbf{77.0}      & 61.5      & 45.4   & 60.1      \\\midrule
\ours                  & Occluded VIS    & 39.4 & 61.5      & \textbf{41.3}      & 18.1   & \textbf{43.3}      \\
+ Supervised Matching & Occluded VIS     & 38.7 & 61.2      & 39.6      & 17.9   & 42.4      \\
+ Limited Range       & Occluded VIS     & \textbf{39.6} & \textbf{63.2}      & 41.0      & \textbf{18.2}   & 43.0      \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Effect of Video-based Training} 
\label{sec:sup}

While we have shown that \ours achieves state-of-the-art VIS performance without video-based training, it is interesting to see how we can leverage video annotation when it is available. We use the video annotation to supervise our matching as in previous per-frame works~\cite{yang2019video,yang2021crossover,QueryTrack}. Given two sampled frames, we use a hinge loss  to  ensure that the correct associations of queries have the highest inner products compared to that of other queries between the two frames~\cite{QueryTrack}. The results are in  \tabref{sup}. The ``Supervised Matching'' rows mean directly applying the matching supervision to the original frame sampling process. In our case, this means that the two sampled frames might be separated up to 20 frames. As pointed out in previous works, frames that are far separated increase the training difficulty and can hurt model performances especially with occlusions~\cite{li2021limited}. We thus also consider the ``Limited Range'' training to only sample consecutive frames for supervised matching, as we only need to match consecutive frames. From the results, directly applying ``Supervised Matching'' hurt performances on all three datasets. Adding ``Limited Range'' recovers most of the performances for YouTube-VIS 2021 and OVIS. On OVIS, it even marginally outperforms the original \ours. However, this improvement does rely on the dataset dependent sampling range. We believe it is possible and important to use video-based training to further improve \ours, although this would take away \ours's practical advantages of only needing sparse annotations and having a simple training pipeline. Appendix~\ref{sec:limit} discusses further limitations of not using video information in training.