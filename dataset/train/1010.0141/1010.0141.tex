\documentclass{article}
\pdfoutput=1
\usepackage{proceed2e}
\usepackage{times}


\usepackage{amsmath, amssymb, times}
\usepackage{epsfig}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsthm}


\newcommand{\mathbs}{\boldsymbol}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}

\def\etal{{et~al.}}
\def\Lo{~}
\def\Lt{~}
\def\Lp{~}

\renewcommand{\algorithmiccomment}[1]{//#1}

\title{ Projections with Box Constraints}

\author{
Mithun Das Gupta  \\
Epson Research and Development, Inc. \\
San Jose, CA.\\
\And
Sanjeev Kumar \\
Dept. Elec and Comp Engg. \\
University of California, San Diego \\
\And
Jing Xiao \\
Epson Research and Development, Inc. \\
San Jose, CA. \\
}





\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
We study the \Lo minimization problem with additional box constraints. We motivate the problem with two different views of optimality considerations. We look into imposing such constraints in projected gradient techniques and propose a worst case linear time algorithm to perform such projections.
We demonstrate the merits and effectiveness
of our algorithms on synthetic as well as real experiments.
\end{abstract}


\section{Introduction}
In the domain of constrained optimization, it is well understood that  norm penalty imposes smoothness constraint while the  norm imposes sparsity~\cite{Ng04}. Lately, sparse representations have been shown to be extremely efficient in encoding specific kinds of data, mainly, obeying power decay law in some transform space e.g. DCT etc. Donoho~\cite{Donoho04} provided sufficient conditions for obtaining an optimal -norm solution which is sparse. Recent
work on compressed sensing~\cite{Candes06,Tsaig06} further explores how  constraints can be used for recovering a sparse signal sampled below the Nyquist rate.

 regularized maximum likelihood can be cast as a constrained optimization problem. Although standard algorithms such as interior-point methods \cite{Tibshirani94,Koh07} offer powerful theoretical guarantees (e.g., polynomial-time complexity, ignoring the cost of evaluating the function), these methods typically require at each iteration the solution of a large, highly ill-conditioned linear system; which are potentially very difficult and expensive.

This paper explores the behavior of  constraint optimization under the presence of bound constraints. Traditional  constraint assumes an infinite upper bound on the magnitude of the predicates.
The idea of upper bounds can be explained very easily by a very simple example. Suppose we want to send a signal of length  which is a combined signal originating from  different sources. Suppose at the receiver side we have  sets of receivers to decode the entire signal of length . Based on the receiver set , the peak signal strength which the receivers can handle can be different. This kind of problem, is not handled by traditional  projection. An illustration for the proposed problem, with 2 sets of receivers is shown in Fig.~\ref{Fig:algo1_expl3434345}. Assuming that transmitting 0's or upper bounds cost just 1 bit, the effective sparsity, assuming 8 bits per element, is (5*8+3*1)/64 for \Lo, and (2*8+6*1)/64 for upper bounded \Lo.
\begin{figure}[htbp!]
\includegraphics[width=8cm,height=4cm]{CoL12}
  \includegraphics[width=8cm,height=4cm]{CoUB_L12}
  \caption{Left: \Lo projection and right: upper bounded \Lo projection for the same norm bound. Two colors represent different set of receivers with different upper bounds represented by black lines. \label{Fig:algo1_expl3434345}
}
\end{figure} 
We motivate the inclusion of box constraints by looking at the problem from two different settings.

\textbf{Optimality Gap: } Let us consider an unconstrained problem

where we have introduced the two bound constraints  into the cost function. This can be slightly modified to a function of 2 variables such that

The Lagrangian for this problem can now be written as 

The dual function is given by

The Lagrange dual can now be written as

A change of variables  leads to


Now the duality gap for the bounded problem can be written as

where  is the duality gap for the \Lo problem without the bound constraints. For  fixed at its upper bound ,  and . Under such a condition

As long as  the duality gap is reduced as compared to simple \Lo minimization. Since  is always positive (shown later in Sec.~\ref{SEC:UBSP}), a sufficient condition for reduction in duality gap is , where  is the dual feasible solution for the \Lo problem. The optimality of a particular solution is based on the duality gap and as such any decrement of the gap increases the optimality of the solution obtained. This shows that the optimality gap for the bound constrained \Lo problem can be made arbitrarily closer to zero compared to the similar unbounded \Lo problem.

\textbf{Degrees of freedom for upper bounded problem: }
We study the degrees of freedom of the upper bounded \Lo projection problem in the framework of Stein's unbiased risk estimation (SURE)~\cite{Stein81}. As shown by Zou \etal~\cite{Zou07} the number of non-zero coefficients is an unbiased estimate for the degrees of freedom (DF) of the optimization scheme.
The idea of upper bounds can be explained very easily by a very simple example. Suppose we want to send a signal of length  which is a combined signal originating from  different sources. Suppose at the receiver side we have  sets of receivers to decode the entire signal of length . Based on the receiver set , the peak signal strength which the receivers can handle can be different. This kind of problem, is not handled by traditional  projection. Assuming that the lower bound is zero (e.g. for electrical signals), transmitting upper bounds can cost just 1 bit. The effective sparsity, can be improved by sending 1 bit for all the elements fixed at their corresponding bounds, and sending 8 bit real numbers for all the remaining non-zero entities as compared to sending 8 bit reals for all non-zero entities. We conjecture that the degree of freedom for the bounded \Lo problem is bounded below that for the unbounded \Lo problem, and hence can provide increased sparsity in terms of the bounds.

\begin{figure}[ht!]
\centering
\includegraphics[width=8cm,height=4cm]{sim_df_thicker}
\caption{SURE criteria for \Lo (blue) and upper bounded \Lo (red). \label{Fig:df}
}
\end{figure}
Let us again assume the simple estimation problem . Now for estimating the SURE criterion, we estimate  and then generate the a new set of observation . The covariance between the terms  and  is a scaled measure for the DF. Fig.~\ref{Fig:df} shows the covariance estimate for \Lo compared to upper bounded . Upper bounded \Lo is uniformly bounded below the DF for only . This also emphasizes the conjecture for bounded constrained , the predicates which are fixed at their consecutive upper bound can be considered to fixed and as such do not contribute to the model complexity.






\section{Separable Quadratic Problems}
The problem of separable quadratic programming with linear bound constraints was first considered by Megiddo \etal~\cite{Megiddo93}. They proposed linear time solution to the generic problem by Lagrangian relaxation based on the multidimensional search procedure of~\cite{Megiddo84}. We introduce a novel linear time algorithm for gradient projection based norm minimization problem. Our starting point is an efficient method for projection onto the probabilistic simplex, with additional upper bound constraints. For infinite upper bound, this is the same method as proposed by numerous authors, namely Gafni et al.\cite{Gafni84}, Bertsekas~\cite{Bertsekas99}, Crammer et al.~\cite{Crammer00}, and more recently by ShalevSchwartz et al.~\cite{ShalevShwartz06} and Duchi et al.~\cite{Duchi08}. The basic intuition is that once the vector to be projected is ordered then the projection can be calculated exactly in linear time. Duchi et al.~\cite{Duchi08} proved the similarity of \Lo projection to the simplex projection, although the problem can be traced back to a special case of separable quadratic problem tackled by Megiddo \etal~\cite{Megiddo93}. Although the projection step is linear, the sorting/ordering step is still . We contest that once the upper bounds are introduced, the ordering does not remain as simple as the previous works. In this paper we propose a linear time algorithm which orders the difference of the bounds from the gradient vector to compute the projections on the norm constraint.


\section{Upper bounded Simplex Projection}\label{SEC:UBSP}
The most basic projection task we consider can be formally described as the following optimization problem, for 

where  is the vector of all zeros,  is the vector of upper bounds and  denotes that .
Note that we enforce , because otherwise  is the optimal solution.
We assume that the feasibility of the constraints with respect to each other, and the existence of a solution is guaranteed by the set  being non-empty.
Also note that if  or , where  is the vector of all ones of size , then the problem reduces to the projection onto the simplex problem of~\cite{Duchi08}.
\begin{claim}\label{Claim:x_is less_than_v}
.
\end{claim}

The norm of  is larger than the constraint  as mentioned earlier. Assume there is an optimal projection , with its element . There exists another solution , such that  and all other elements same as , which is bounded and gives a lower value for the cost function, and hence is a better solution than , which is a contradiction.

\begin{claim}\label{Claim:effective_lb}
.
\end{claim}

The maximum value that  can reach is either the upper bound  or  (from Claim~\ref{Claim:x_is less_than_v}), hence the effective upper bound  is the minimum of  or .


From this point onwards, we will assume , if not mentioned otherwise.
The next lemma is the extension of Lemma 1 of Duchi et al.~\cite{Duchi08} for bounded projections.
\begin{lemma}\label{Lemma:v_based_zero_constraint_ordering}
If  and in optimal solution  then , irrespective of ordering of , .
\end{lemma}
\emph{Proof.} We need to prove the above lemma for 2 cases.

[1] . In this case, if another solution is constructed such that  and  are switched keeping all other indices same, then there is a strict decrease in optimal value, generating a contradiction.

[2] . Again let us assume . Let us construct another optimal solution , such that , , where , and keep all the other indices same. It can be easily observed that the norm as well as the upper bound constraint are satisfied for . Now we can show that

which is a contradiction since we constructed a solution better than the optimal solution.


\section{Euclidean Projection onto the box-constrained \Lo Ball}\label{SEC:EPL1}
We modify the problem studied by Duchi et al.~\cite{Duchi08} to the more generic scenario containing the bounds on the predicted vector. We need to find the projection of a vector  onto a feasible region defined by

Note that the vector  is no longer contained within the positive real space. We assume that  guarantees feasibility and existence of a solution.
The range in which the 's should lie can now be characterized as (a) , (b)  and (c) , assuming that  for all cases.

\textbf{Intervals not containing 0: }
Further analysis of the cost function in Eq.\eqref{Eqn:Min}, leads to the observation that conditions (a) and (b), are equivalent under a sign flip. This can be obtained by observing the following identities: (a) distance preservation under sign flip  , (b) 1 norm preservation under sign flip  and (c) range transformation under sign flip.
For such constraints, we can transform the bounds such that all the boundaries are positive.
Once this is done a simple change of variables

leads to the formulation in Eq.\eqref{Eqn:Min} with the lower bound terms 's equal to 0. The equivalent simpler problem is exactly similar to Eq.\eqref{Eqn:Min_generic}, since the  constraint and the simplex constraint are same for positive variables. Also note that
element wise manipulations can be performed when all the bounds do \textit{not} belong to one particular case, without altering the form of the equations.

{\textbf{Interval containing 0: }}
The objective function in Eq.\eqref{Eqn:Min} has expression of the form  () and norm constraint is equivalent to inclusion in \Lp norm ball (). Given a candidate solution , let us define a category of moves which can be used to generate a family of candidate solutions.
By setting some subset of vector components to zero, we can generate corresponding points in all orthants, resulting in a family of  up to  candidate solutions, one in each orthant. We note two properties of orthant projection move which are essential for our exposition. First, orthant projection move preserves \Lp norm ball inclusion constraint for all . Second, under orthant projection moves of ,  is minimized when  and  are in the same orthant.  Together, these two properties ensure that there is a preferred orthant (determined apriori) where optimal solution is guaranteed to lie, as long as none of the other constraints are violated. This observation can be used to generalize Lemma 3 of Duchi et al.~\cite{Duchi08} to a much wider class of problems. For completion and ease of understanding we state the following lemma:
\begin{lemma}\label{Lemma:Duchi3}
\texttt{[Lemma 3, Duchi et al.\cite{Duchi08}]} Let  be an optimal solution of Eq.\eqref{Eqn:Min}. Then,
.
\end{lemma}
Hence,  has the same sign as . Note that this lemma holds true for the upper bounded problem as well, since replacing any variable with 0 does not violate this constraint, and hence the proof for the lemma can be exactly applied to the more generic case mentioned above.

Orthant projection also reveals a possible failure mode for the above generalization. If there are terms other than those of the form of  and there are constraints other than those of the form of inclusion in \Lp norm ball, then apriori preferred orthant selection may not be possible.
Preferred orthant selection allows us to simplify the functional form of \Lp norm terms.
In particular, for \Lo norm projection problem, once problem has been transformed to guarantee that optimal solution lies in first orthant, \Lo norm constraint becomes equivalent to the simplex constraints,  and .

In order to retain this simplification, any generalization involving terms which are not conducive to orthant projection must be explicitly handled. Box constraints are in general not conducive, unless interval contains origin.
For intervals such as , we have the following claim:
\begin{claim}\label{Claim:a_b_v}

\end{claim}



Based on the above discussions we can write the generic upper bounded \Lo norm projection problem as, given any , take the absolute value of the elements and transform it to , transform  based on claim.~\ref{Claim:a_b_v}, find  by solving

and return the final projection as .

Identifying the simplex projection as well as  projection as the same problem leads us to study the unified problem mentioned above in Eq.\eqref{Eqn:Min0}.


The Lagrangian for the above optimization problem (Eq.\eqref{Eqn:Min0}) can be written as

Differentiating with respect to  and comparing to zero gives the first order
optimality condition,

The first complementary slackness KKT condition~\cite{Boyd04}, implies that
, when . Since, , hence  whenever .
The second complementary slackness KKT condition implies that
, means ,  and

The addition of finite upper bound leads to the next complementary slackness condition, namely, when the value of  reaches it maximum value ,  and


\begin{claim}\label{Claim:x_b_v}
 implies .
\end{claim}



Note that the converse is not generally true, that is  does not imply that , since it can still be lower than the upper bound.

\textbf{Corollary 4.}\label{Cor:x_eq_b}  \textit{and}  implies .

One important aspect of the cost  is that the contribution of each  to the total cost is dependent on the distance . From this point onwards we will assume that the upper bound term , such that . Since each  is bounded to be less than , hence  can be thought to be the relative weight determining the order in which 's should be changed to meet the norm constraint. This ordering can also be argued from the fact that the magnitude of the gradient with respect to  is determined by the quantity .

In Lemma~\ref{Lemma:v_based_zero_constraint_ordering}, we have shown that even for upper bounded simplex projection problem (after restriction to first orthant), such constraint ordering is possible for constraints , and is determined by .
In the next lemma we show that similar constraint ordering is possible for upper bound constraints , and is determined by  which is one of our key contributions and forms the basis of the proposed efficient algorithm. Based on the above observations we write a modified version of the lemma 2. from Shalev-Shwartz \etal~\cite{ShalevShwartz06}.
\begin{lemma}\label{Lemma:ub_constraint_order}
Let  be an optimal solution of Eq.\eqref{Eqn:Min0}. Let  and  be two indices such that . If  then  as well.
\end{lemma}
\emph{Proof.} From Eq.\eqref{Eqn:v_theta_gamma_zero}, whenever , then  where . Hence




\subsection{Worst case strongly linear time algorithm}
We now propose an algorithm with strongly linear time worst case complexity which is asymptotically fastest possible. It is based on dependence between  and  along regularization path. We have already shown that, in optimal solution  whenever , and  whenever  and equal to  otherwise. Hence, for any value of , variables 's can be divided into three disjoint groups.

Let us denote the sets of indices of 's in these groups by ,  and  respectively. These sets are functions of . Let optimal  be  and corresponding sets be   and . Relation between  and  can be expressed as,

where  for a set argument, denotes its cardinality. It is evident that  is monotonically decreasing piece-wise linear function of , with  points of discontinuity at  and  values.
The pseudo code of our proposed algorithm is given in Algorithm~\ref{Fig:Algo3}.
The algorithm operates upon merged  and  arrays, maintaining source information.


In the first stage, we find the linear segment corresponding to given . Uncertainty interval  for  is initialized with   and is subsequently reduced in every iteration by bisection at a pivot selected from the elements of merged  and  arrays lying in current uncertainty interval.  For pivot, we use median, found using worst case linear time median finding algorithm~\cite{CLR01}, in order to ensure that number of iterations remains O() and that after every iteration, size of uncertainty interval reduces by a constant fraction. Using Eq.\eqref{eqn:z_theta_relation_suboptimal} to evaluate  at pivot  is not efficient enough for overall linear time complexity, since summations involve O() terms every time, resulting in O( complexity. To rectify this inefficiency, apart from , we maintain two running partial sums across all iterations.

1)  sum of  for all elements which are guaranteed to be set to zero in optimal solution i.e. .

2)  sum of  for all elements which are guaranteed to be set to corresponding upper bounds in optimal solution i.e. .

We also maintain cardinality   and  of these sets. In terms of these partial sums, (\ref{eqn:z_theta_relation_suboptimal}) can be expressed as

If ,   becomes new uncertainty interval, and  and  are updated as

Otherwise, if ,  becomes new uncertainty interval, and , and  are updated as

Iterations continue until there are no more points of discontinuity in the uncertainty interval and ,  and  have been found. Now, following modified version of (\ref{eqn:z_theta_relation_suboptimal}) can be used to evaluate  as


\begin{algorithm}
\caption{Algorithm for worst case strongly linear time projection onto the simplex with finite upper bound.}
\label{Fig:Algo3}
\begin{algorithmic}\STATE REQUIRE ,  , 
\STATE  \COMMENT{maintain source info}
\STATE 
\STATE 
\STATE 
\STATE 

\WHILE{ }

\STATE 
\STATE 
\STATE 

\STATE Evaluate  using (\ref{eqn:z_theta_relation_optimal})

\IF{ }
\STATE 
\STATE Update  using (\ref{eqn:S_L_update})
\ELSE
\STATE 
\STATE Update  using (\ref{eqn:S_R_update})
\ENDIF
\ENDWHILE
\STATE Evaluate  using (\ref{eqn:theta_star})
\STATE RETURN  corresponding to ~(Eq.\ref{Eqn:theta_to_xa}).
\end{algorithmic}
\end{algorithm}


\section{Experiments}\label{SEC:Expts}
The first set of experiments are performed for synthetic data. We generate labeled data belonging to 2 classes such that the probability of the label being 1/0 is distributed according to logistic likelihood ,
where . Additionally, we disturb 10\% of the data labels by introducing false labels based on random draws.
The ground truth parameter vector  is generated from a generalized Gaussian distribution, with rejection, such that the individual elements of the vector are bounded within . Moreover half the entities of  are made zeros to generate a sparse vector. The inference problem is thus an estimation problem with known upper bounds. We minimize the average logistic log loss, and project the gradient vector to the convex space. The norm constraint is determined as a fixed fraction of the dimension of the vector. The estimation error against the iterations, where the error is denoted as , for L1 and our method called UB\_L1 is shown in Fig.~\ref{Fig:comp}(left). Note that the L1 method estimates are outside the bounds which manifests itself as slower rate of convergence as evident from the plots. At convergence UB\_L1 estimate seems much closer to the ground truth parameter vector than the L1 estimate.
\begin{figure*}[htbp!]
\centering
  \includegraphics[width=8cm,height=5cm]{Cotrain}
  \includegraphics[width=8cm,height=5cm]{Cotest}
  \includegraphics[width=16cm,height=5cm]{Cow_comp}
\caption{Top left: plot for abs vs. iteration (training). Top right: similar plot for testing. Bottom: estimated parameter vector . For all panels, blue: L1, red: our method UB\_L1 and green: ground truth. \label{Fig:comp}
}
\end{figure*}

\begin{figure*}[htbp!]
\centering
\includegraphics[width=16cm,height=5cm]{time_comp_all4}
\caption{Left: comparison of our method against Matlab QP. Blue: run time (in seconds) for QP implementation of MATLAB. Red: run time for our linear time method. The horizontal axis runs over the dimension of the input vector . Right: zoomed in red curve. \label{Fig:time_qp_algo2}
}
\end{figure*}
Next we explore the run time performance of our algorithm, against a standard quadratic programming (QP) implementation in MATLAB. Fig.~\ref{Fig:time_qp_algo2} shows the results for such an experiment. For projecting dense vectors with 1M non-zero
elements our method takes around 0.22 seconds.

\textbf{Food distribution}
The next experiment is drawn from a real world scenario and motivates the upper bound constraints.
We start by noting that the problem of food distribution can be easily applied to our case. Suppose the \textit{production} of one food item (e.g. chicken) in 40 states in the US is provided as the initial vector \footnote{http://www.agcensus.usda.gov/Publications/2007}. Assume that  of the total production is put up for sales.

We would like to find the \textit{sales} vector  for the 40 states. The upper bounds can be obtained from the consumption patterns in the previous years. We take production in 2007 as the new vector , and the distribution in 2004 as the upper bound . To remove scale differences the upper bound is normalized such that . The norm constraint . The results for such an experiment are shown in Fig.~\ref{Fig:L1_comp_us}. As the value of  decreases  forces more and more mass into the dominant elements. Our method still tries to satisfy the upper bound constraints, which spreads the distribution at the cost of sparsity. As the supply decreases,  tries to bias the distributions among the states based on the relative weights of the production itself. Our method, on the other hand, applies the demand based upper bounds, and biases the distribution in favor of the states with \textit{maximum disparity} between production and supply.
\begin{figure*}[htbp!]
\centering
  \includegraphics[width=14cm,height=6cm]{L1_ub_act4}
  \caption{Red: Actual sales of Chicken in 40 states in 2007. Blue: our method with upper bounds. Green:  only. Note the small region in the circle which has been enlarged.  completely misses this region whereas our method still provides some value to it.\label{Fig:L1_comp_us}
}
\end{figure*}


\section{Conclusion}\label{SEC:Concl}
In this paper we extend the idea of  constrained gradient projection under the presence of upper bound constraints. We explore simplex projection with upper bounds and bring out the similarities with \Lo projection. We derive criteria for a-priori determination of sequence in which various constraints become active and use such orderings to propose an efficient algorithm. The key insight obtained from our experiments was that  tries to increase the dominant elements while putting zeroes for all the others. Bound constrained , weighs the elements based on their distance from the corresponding bound. This case leads to better predictions, specifically in cases which should be weighed based on the disparity between the demand and supply. The elements with higher disparity get higher weight in the predicted distribution vector.


{\bibliographystyle{plain}
\bibliography{latex8}
}

\end{document}
