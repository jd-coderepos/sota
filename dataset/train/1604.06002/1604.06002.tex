\documentclass[a4paper,UKenglish]{lipics-v2016}


\usepackage{microtype}



\bibliographystyle{plainurl}

\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\newcommand{\REV}[1]{\ensuremath{\overline{#1}}}
\newcommand{\RLBWT}{\ensuremath{\mathsf{RLBWT}}}
\newcommand{\RLCSA}{\ensuremath{\mathsf{RLCSA}}}
\newcommand{\ST}{\ensuremath{\mathsf{ST}}}
\newcommand{\SLT}{\ensuremath{\mathsf{SLT}}}
\newcommand{\MST}{\ensuremath{\mathsf{MST}}}
\newcommand{\SA}{\ensuremath{\mathsf{SA}}}
\newcommand{\LZ}{\ensuremath{\mathsf{LZ}}}
\newcommand{\BWT}{\ensuremath{\mathsf{BWT}}}
\newcommand{\CDAWG}{\ensuremath{\mathsf{CDAWG}}}
\newcommand{\SSA}{\ensuremath{\mathsf{SSA}}}
\newcommand\SP[1]{\mathtt{sp}(#1)}
\newcommand\EP[1]{\mathtt{ep}(#1)} 
\newcommand\INTERVAL[1]{\mathtt{range}(#1)}
\newcommand{\INTERVALFUNCTION}{\ensuremath{\mathbb{I}}}
\newcommand{\Oh}[1]
  {\ensuremath{\mathcal{O}\!\left( {#1} \right)}}
\newcommand{\occ}
  {\ensuremath{\mathsf{occ}}}
\newcommand{\runs}{r}




\newcommand{\newe}{e}
\newcommand{\newel}{e^{\ell}}
\newcommand{\newr}{r}
\newcommand{\newrbar}{\overline{r}}

\title{Practical combinations of repetition-aware data structures\footnote{This work was partially supported by Academy of Finland under grant 284598 (Center of Excellence in Cancer Genetics Research).}}


\author[1]{Djamal Belazzougui}
\author[2]{Fabio Cunial}
\author[3,4]{Travis Gagie}
\author[5]{Nicola Prezza}
\author[6]{Mathieu Raffinot}

\affil[1]{CERIST, Algeria. }
\affil[2]{Max Planck Institute of Molecular Cell Biology and Genetics, Dresden, Germany.
}
\affil[3]{Department of Computer Science, University of Helsinki, Finland.}
\affil[4]{Helsinki Institute for Information Technology, Finland.}
\affil[5]{Department of Mathematics and Computer Science, University of Udine, Italy.}
\affil[6]{Laboratoire Bordelais de Recherche en Informatique, CNRS, Bordeaux, France.}

\authorrunning{D. Belazzougui et al.} 

\Copyright{D. Belazzougui et al.}

\subjclass{E.1 Data structures, F.2.2 Pattern matching.}\keywords{repetitive strings, locate, count, run-length encoded BWT, Lempel-Ziv factorization, CDAWG.}

\EventEditors{John Q. Open and Joan R. Acces}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}




\begin{document}

\maketitle

\begin{abstract}
Highly-repetitive collections of strings are increasingly being amassed by genome sequencing and genetic variation experiments, as well as by storing all versions of human-generated files, like webpages and source code. Existing indexes for locating all the exact occurrences of a pattern in a highly-repetitive string take advantage of a single measure of repetition. However, multiple, distinct measures of repetition all grow sublinearly in the length of a highly-repetitive string. In this paper we explore the practical advantages of combining data structures whose size depends on distinct measures of repetition. The main ingredient of our structures is the run-length encoded BWT (RLBWT), which takes space proportional to the number of runs in the Burrows-Wheeler transform of a string. We describe a range of practical variants that combine RLBWT with the set of boundaries of the Lempel-Ziv 77 factors of a string, which take space proportional to the number of factors. Such variants use, respectively, the RLBWT of a string and the RLBWT of its reverse, or just one RLBWT inside a bidirectional index, or just one RLBWT with support for unidirectional extraction. We also study the practical advantages of combining RLBWT with the compact directed acyclic word graph of a string, a data structure that takes space proportional to the number of one-character extensions of maximal repeats. Our approaches are easy to implement, and provide competitive tradeoffs on significant datasets.
\end{abstract}










































\section{Introduction}\label{sec:introduction}

Locating all the exact occurrences of a string in a massive, highly-repetitive collection of similar texts is a fundamental primitive in the post-genome era, in which genomes from multiple related species, from multiple strains of the same species, and from multiple individuals, are being sequenced at an increasing pace. Most data structures designed for such repetitive collections take space proportional to a specific measure of repetition, for example the number  of factors in a Lempel-Ziv parsing \cite{arroyuelo2012stronger,kreft2013compressing}, or the number  of runs in a Burrows-Wheeler transform \cite{MakinenNSV10}. In previous work we showed how to achieve competitive theoretical tradeoffs between space and time in locate queries, by combining data structures that depend on distinct measures of repetition, where all such measures grow sublinearly in the length of a highly-repetitive string \cite{belazzougui2015composite}. Specifically, we described a data structure that takes approximately  words of space, and that reports all the occurrences of a pattern of length  in a string of length  in  time, where  and  are the number of primary and of secondary occurrences, respectively (see Section \ref{sec:stringIndexes}). This compares favorably to the  reporting time of Lempel-Ziv 77 (LZ77) indexes \cite{kreft2013compressing}, where  is the height of the parse tree. It also compares favorably in space to solutions based on the run-length encoded BWT (RLBWT) and on suffix array samples
\cite{MakinenNSV10}, which take  words of space to achieve
 reporting
time, where  is the sampling rate. We also introduced a data structure whose size depends on the number of right-extensions of maximal repeats, and that reports all the  occurrences of a pattern in  time. The main component of our constructions is the RLBWT, which we use for counting the number of occurrences of a pattern, and which we combine with the CDAWG and with data structures from Lempel-Ziv indexes, rather than with suffix array samples, for answering locate queries.



In this paper we engineer a range of practical variants of such approaches, and we compare their space-time tradeoffs to a representative set of state-of-the-art indexes for repetitive collections, including RLCSA \cite{MakinenNSV10}, a number of LZ77 implementations \cite{kreft2010self}, and a recent implementation of the hybrid index \cite{valenzuela2016chico}. One of our indexes based on RLBWT and LZ77 factors uses an amount of memory comparable to LZ77 indexes, but it answers count queries between two and four orders of magnitude faster than all LZ77 and hybrid index implementations. For long patterns, our index uses less space than the hybrid index, and it answers locate queries between one and two orders of magnitude faster than a number of LZ77 implementations, and as fast as the fastest LZ77 implementation. With short patterns, our index based on RLBWT and CDAWG answers locate queries between four and ten times faster than a version of RLCSA that uses comparable memory, and with extremely short patterns our index achieves speedups even greater than ten with respect to RLCSA.









































\section{Preliminaries}

\subsection{Strings} \label{sec:strings}

Let  be an integer alphabet, let  be a separator, and let  be a string. We denote by  the reverse of , and by  the set of all starting positions of a string  in the circular version of . We set  and  . A \emph{repeat}  is a string that satisfies . A repeat  is \emph{right-maximal} (respectively, \emph{left-maximal}) iff  (respectively, iff ). A \emph{maximal repeat} is a repeat that is both left- and right-maximal. We say that a maximal repeat  is \emph{rightmost} (respectively, \emph{leftmost}) if no string  with  is left-maximal (respectively, if no string  with  is right-maximal).

For reasons of space we assume the reader to be familiar with the notion of \emph{suffix tree}  and of \emph{suffix-link tree} of , which we do not define here. We denote by , or equivalently by , the label of edge , and we denote by  the string label of node . It is well known that a string  is right-maximal (respectively, left-maximal) in  iff  for some internal node  of  (respectively, iff  for some internal node  of ). Since left-maximality is closed under prefix operation, there is a bijection between the set of all maximal repeats of  and the set of all nodes of the suffix tree of  that lie on paths that start from the root and that end at nodes labelled by rightmost maximal repeats. Symmetrically, since right-maximality is closed under suffix operation, there is a bijection between the set of all maximal repeats and the set of all nodes of the suffix tree of  that lie on paths that start from the root and that end at nodes labelled by leftmost maximal repeats.





The \emph{compact directed acyclic word graph} of  (denoted by  in what follows) is the minimal compact automaton that recognizes the set of suffixes of  \cite{blumer1987complete,CrochemoreV97}. It can be seen as the minimization of  in which all leaves are merged to the same node (the sink) that represents  itself, and in which all nodes except the sink are in one-to-one correspondence with the maximal repeats of  \cite{Raffinot2001} (the source corresponds to the empty string). The set of accepting nodes consists of the sink and of all maximal repeats that also occur as a suffix of . Like in the suffix tree, transitions are labelled by substrings of . Since a maximal repeat corresponds to a subset of its right-maximal suffixes,  can be built by putting in the same equivalence class all nodes of  that belong to the same maximal unary path of explicit Weiner links. Note also that the subgraph of  induced by maximal repeats is isomorphic to a spanning tree of .

For reasons of space we assume the reader to be familiar with the notion and uses of the Burrows-Wheeler transform of , including the  array, LF mapping, and backward search. In this paper we use  to denote the BWT of , and we use  to denote the lexicographic interval of a string  in a BWT that is implicit from the context. We say that  is a \emph{run} iff  for all , and moreover if any substring  such that , , and either  or , contains at least two distinct characters. It is well known that repetitions in  induce runs in : for example, the BWT of  consists of  runs of length at least  and of a run of length one. We denote by  the number of runs in , and we call \emph{run-length encoded BWT} (denoted by ) any representation of  that takes  words of space, and that supports rank and select operations (see e.g. \cite{makinen2005succinct1,MakinenNSV10,SirenVMN08}). Since the difference between  and  is negligible in practice, to simplify notation we denote both of them by  when  is implicit from the context.

The \emph{Lempel-Ziv 77 factorization} of  \cite{ziv1977universal}, abbreviated with LZ77 in the rest of the paper, is the greedy decomposition  of  defined as follows. Assume that  is virtually preceded by the set of distinct characters in its alphabet, and assume that  has already been computed for some prefix of length  of : then,  is the longest prefix of  such that there is a  that satisfies . 

In the rest of the paper we drop subscripts whenever they are clear from the context.









































\subsection{String indexes} \label{sec:stringIndexes}

The \emph{run-length compressed suffix array} of , denoted by  in what follows, consists of a run-length compressed rank data structure for , and of a sampled suffix array, denoted by  \cite{MakinenNSV10}. Given a pattern , we use the rank data structure to find the interval of  that contains all characters that precede the occurrences of  in : the length of this interval, uncompressed, is the number of such occurrences. To locate a specific occurrence, we start at the character that precedes it in  and we use rank queries to move backward, until we reach a character whose position has been sampled. Thus, the average time for locating an occurrence is inversely proportional to the size of , and fast locating needs a large SSA regardless of the compressibility of the dataset. M\"akinen et al.\ suggested ways to reduce the size of the SSA \cite{MakinenNSV10}, but they did not perform well enough in real repetitive datasets for the authors to include them in the software they released.

For reasons of space we assume the reader to be familiar with LZ77 indexes (see e.g. \cite{gagie2014lz77,karkkainen1996lempel}). Here we just recall that a \emph{primary occurrence} of a pattern  in  is one that crosses or ends at a phrase boundary in the LZ77 factorization  of . All other occurrences are called \emph{secondary}. Once we have computed primary occurrences, locating all  secondary occurrences reduces to two-sided range reporting, and it takes  time with a data structure of  words of space \cite{karkkainen1996lempel}. To locate primary occurrences, we use a data structure for four-sided range reporting on a  grid, with a marker at  if the -th LZ factor in lexicographic order is preceded in the text by the lexicographically -th reversed prefix ending at a phrase boundary. This data structure takes  words of space, and it returns all the phrase boundaries that are immediately followed by a factor in the specified range, and immediately preceded by a reversed prefix in the specified range, in  time, where  is the number of phrase boundaries reported \cite{chan2011orthogonal}. K\"arkk\"ainen and Ukkonen used two PATRICIA trees \cite{morrison1968patricia}, one for the factors and the other for the reversed prefixes ending at phrase boundaries \cite{karkkainen1996lempel}. To locate primary occurrences, we query the first tree for the range of distinct factors in left-to-right lexicographic order that start with , and we query the second tree for the range of reversed prefixes  starting with , for all . The ranges returned by the trees are correct iff any factor starts with  and any reversed prefix at a phrase boundary starts with . To check the first range, we choose any factor in the range and compare its first  characters to . To check the second range, we choose any reversed prefix in the range and compare its first  characters to . This takes  time for every , thus  time in total, assuming that  is not compressed. Replacing the uncompressed text by an augmented compressed representation, we can store  in  space such that later, given , we can find all  occurrences of  in  time~\cite{gagie2014lz77}.

If we know in advance that all patterns will be of length at most , then we can store in a FM-index the substrings of  consisting of characters within distance  of the nearest phrase boundary, and use that to find primary occurrences. This approach, called \emph{hybrid indexing}, has been proposed several times recently: see e.g. \cite{valenzuela2016chico} and references therein for more details.














































\subsection{Composite repetition-aware string indexes} \label{sec:compositeStringIndexes}

It is possible to combine  with the set of all starting positions  of LZ factors of , building a data structure that takes  words of space, and that reports all the  primary occurrences of a pattern  in  time \cite{belazzougui2015composite}. Since such data structure is at the core of the paper, we summarize how it works in what follows.

The same primary occurrence of  in  can cover up to  boundaries between two LZ factors. Thus, we consider every possible way of placing, inside , the rightmost boundary between two factors, i.e. every possible split of  in two parts  and  for , such that  is either a factor or a proper prefix of a factor. For every such , we use four-sided range reporting queries to list all the occurrences of  in  that conform to the split, as described in Section \ref{sec:stringIndexes}. We encode the sequence  implicitly, as follows: we use a bitvector  such that  iff  for some , i.e. iff  is the last position of a factor. We represent such bitvector as a predecessor data structure with partial ranks, using  words of space \cite{Wi83}. Let  be the suffix tree of , and let  be the set of loci in  of all LZ factors of . Consider the list of node labels , sorted in lexicographic order. It is easy to build a data structure that takes  words of space, and that implements in  time function , which returns the (possibly empty) interval of  in  (see e.g. \cite{belazzougui2015composite}). Together with ,  and , this data structure is the output of our construction.

Given , we first perform a backward search in  to determine the number of occurrences of  in : if this number is zero, we stop. During backward search, we store in a table the interval  of  in  for every . Then, we compute the interval  of  in  for every , using backward search in : if , then  never ends at the last position of a factor, and we can discard this value of . Otherwise, we convert  to the interval  of all the reversed prefixes of  that end at the last position of a factor. Rank operations on  can be implemented in  time using predecessor queries. We get the lexicographic interval of  in the list of all distinct factors of  using operation , in  time. We use such intervals to query the four-sided range reporting data structure.

It is also possible to combine  with , building a data structure that takes  words of space, and that reports all the  occurrences of  in  time, where  is the number of right-extensions of maximal repeats of  \cite{belazzougui2015composite}. Specifically, for every node  in the CDAWG, we store  in a variable . Recall that an arc  in the CDAWG means that maximal repeat  can be obtained by extending maximal repeat  to the right \emph{and to the left}. Thus, for every arc  of the CDAWG, we store the first character of  in a variable , and we store the length of the right extension implied by  in a variable . The length  of the left extension implied by  can be computed by . For every arc of the CDAWG that connects a maximal repeat  to the sink, we store just  and the starting position  of string  in . The total space used by the CDAWG is  words, and the number of runs in  can be shown to be  as well \cite{belazzougui2015composite}. An alternative construction could use  and .

We use the RLBWT to count the number of occurrences of  in  in  time: if this number is not zero, we use the CDAWG to report all the  occurrences of  in  time, using a technique already sketched in \cite{crochemore1997automata}. Specifically, since we know that  occurs in , we perform a blind search for  in the CDAWG, as is typically done with PATRICIA trees. We keep a variable , initialized to zero, that stores the length of the prefix of  that we have matched so far, and we keep a variable , initialized to one, that stores the starting position of  inside the last maximal repeat encountered during the search. For every node  in the CDAWG, we choose the arc  such that  in constant time using hashing, we increment  by , and we increment  by . If the search leads to the sink by an arc , we report  and we stop. If the search ends in a node  that is associated with the maximal repeat , we determine all the occurrences of  in  by performing a depth-first traversal of all nodes reachable from  in the CDAWG , updating variables  and  as described above, and reporting  for every arc  that leads to the sink. The total number of nodes and arcs reachable from  is .












































\section{Combining RLBWT and LZ factors in practice}

We implement\footnote{The source code of all our implementations is available at \cite{githubNicola1,githubNicola2}, and it is based on the SDSL library~\cite{gbmp2014sea}.} the combination of RLBWT and LZ factorization described in Section \ref{sec:compositeStringIndexes}, exploring a range of practical variants of decreasing size. Specifically, in addition to the version described in Section \ref{sec:compositeStringIndexes} (which we call \emph{full} in what follows), we implement a variant in which we drop , simulating it with a bidirectional index (we call this variant \emph{bidirectional} in what follows), a variant in which we drop , the four-sided range reporting data structure, and the subset of suffix tree nodes (we call this variant \emph{light} in what follows), and another variant in which we use a sparse version of the LZ parsing (we call this \emph{sparse} in what follows). Moreover, we design a number of practical optimization to speed up locate queries: see Appendix \ref{sec:heuristics}.

We implement all variants using a representation of the RLBWT that is more space-efficient than the one described in~\cite{SirenVMN08}. Recall that the latter is encoded as follows: they store one character per run in a string , they mark with a one the beginning of each run in a bitvector , and for every  they store the lengths of all runs of character  consecutively in a bit-vector : specifically, every -run of length  is represented in  as . 
This representation allows one to map rank and access queries on  to rank, select and access queries on , , and . By gap-encoding the bitvectors, this representation takes  bits of space. We reduce the multiplicative factor of term  by storing in  just one out of  ones, where  is an arbitrary constant. It is easy to see that we are still able to answer all queries on the RLBWT by using the vectors  to reconstruct the positions of the missing ones in , using  bits of space, but query times are multiplied by a factor . In all our experiments we set  to 1/8. We represent  as a Huffman-encoded string (\texttt{wt\_huff<>} in SDSL), and gap-encoded bitvectors with Elias-Fano (\texttt{sd\_vector<>} in SDSL).











































\subsection{Full index}

The first variant is an engineered version of the data structure described in Section \ref{sec:compositeStringIndexes}. We store both  and . A gap-encoded bitvector  of  bits marks the rank, among all the suffixes of , of every suffix  such that  is the last position of an LZ factor of . Symmetrically, a gap-encoded bitvector  of  bits marks the rank, among all the suffixes of , of every suffix  such that  is the first position of an LZ factor of . 

Geometric range data structures (4-sided and 2-sided) are implemented as wavelet trees (\texttt{wt\_int<>} in SDSL). The 4-sided range data structure supports locating primary occurrences, by storing the permutation of the  LZ factors of , sorted lexicographically, in the order induced by the corresponding ones in : in other words, every character of the wavelet tree is the lexicographic rank of an LZ factor, among all the LZ factors of . For locating primary occurrences, we need to label every point in the 4-sided range data structure with a text position. We allocate  bits rather than  bits to every such label, by using as label the rank of the corresponding one in array , thus the data structure takes  bits of space. The 2-sided range data structure stores  two-dimensional points whose coordinates are both in . For locating secondary occurrences, every such point is again labeled with the rank of the corresponding one in array . The wavelet tree that implements the 2-sided range data structure can only store points whose set of  coordinates is : we map text coordinates to this domain using a gap-encoded bitvector, which takes  bits of space. Some coordinates could be repeated, since two LZ factors could share the same source start point: we keep track of duplicates using a succinct bitvector that takes  bits of space, in which a coordinate that is repeated  times is encoded as . Overall, the 2-sided range data structure takes  bits of space.

Finally, we need a way to compute the lexicographic range of a string among all the LZ factors of . We implement a simpler and more space-efficient strategy than the one proposed in~\cite{belazzougui2015composite}. Specifically, recall that LZ factors are right-maximal substrings of , or equivalently they are nodes of the suffix tree of . Recall also that the BWT intervals of two nodes of the suffix tree of  are either disjoint or contained in one another. We sort the BWT intervals of all LZ factors by the order induced by the pre-order traversal of the suffix tree of : two distinct nonempty intervals  and  are such that  iff , or iff  is contained in . The data structure is just the sorted array  of such intervals, and it takes  bits of space\footnote{The sequence of first positions of all intervals in the sorted array is non-decreasing, thus we use gap encoding to save  bits of space. This adds a multiplicative factor of  to all query times. For clarity we describe just the simpler version in which intervals are encoded as  integers of  bits each.}. Given the BWT interval  of a string , we find its lexicographic range among all sorted distinct LZ factors, in  time, as follows: (1) we binary-search  using the order described above, finding all intervals that are strictly smaller than ; (2) starting from the first interval in  that is greater than or equal to , we find all intervals in  that equal  or are contained in it, i.e. all intervals of factors that are either  itself or a right-extension of . This requires just one binary search, since all such intervals are contiguous in .


In summary, the full index takes  bits of space, and it supports count queries in  time and locate queries in  time.











































\subsection{Bidirectional index}

We can drop  and simulate it using just , by applying the synchronization step performed in bidirectional BWT indexes (see e.g. \cite{belazzougui2014linear} and references therein). This strategy penalizes the time complexity of locate queries, which becomes quadratic in the length of the pattern. Moreover, since in our implementation we store run-lengths separately for each character, a synchronization step requires  rank queries to find the number of characters smaller than a given character inside a BWT interval. This operation could be performed in  time if the string were represented as a wavelet tree. In summary, the bidirectional variant of the index takes  bits of space, it supports count queries in  time, and it supports locate queries in  time. 












































\subsection{Light index with LZ sparsification}

Once we have computed the interval of the pattern in , we can locate all its primary occurrences by just forward-extracting at most  characters for each occurrence inside the range: this is because every primary occurrence of the pattern overlaps with the last position of an LZ factor. We implement forward extraction with select queries on . This approach requires just , the 2-sided range data structure, a gap-encoded bitvector  that marks the last position of every LZ factor in the text, a gap-encoded bitvector  that marks the last position of every LZ factor in , and  integers of  bits each, connecting corresponding ones in  and in : this array plays the role of the sparse suffix array sampling used in RLCSA. 

We can reduce space even further by \emph{sparsifying the LZ factorization}. 
Intuitively, the factorization of a highly-repetitive collection of strings , where  are similar to , is much denser inside  than it is inside , thus excluding long enough contiguous regions from the factorization (i.e. not outputting factors inside such regions) could reduce the number of factors in dense regions. Formally, let , and consider the following generalization\footnote{The version of LZ77 considered in this paper is obtained by setting , and by requiring the text to be virtually preceded by all its distinct characters.} of LZ77, denoted here by LZ77-: we factor  as , where  is the size of the factorization,  for all , and  is the longest prefix of  that appears at least twice in  To make the index described in this section work with LZ77-, we need to sample the suffix array of  at the lexicographic ranks that correspond to the last position of every , and we need to redefine primary occurrences as those that are not fully contained inside an . During locate we now need to extract  additional characters before each occurrence of the pattern, in order to locate primary occurrences that start inside a . The 2-sided range data structure must also be built on the (sources of the) factors . This implementation of the index takes  bits of space, it answers locate queries in  time and count queries in  time.
























































 























































\section{Combining RLBWT and CDAWG in practice}

We implement\footnote{The source code of all our implementations is available at \cite{githubMathieu}.} the combination of RLBWT and CDAWG described in Section \ref{sec:compositeStringIndexes}, and we study the effect of two representations of the CDAWG in memory. In the first representation, the graph is encoded as a sequence of variable-length integers: every integer is represented as a sequence of bytes, in which the seven least significant bits of every byte are used to encode the integer, and the most significant bit flags the last byte of the integer. Nodes are stored in the sequence according to their topological order in the graph obtained from the CDAWG by inverting the direction of all arcs: to encode a pointer from a node  to its successor  in the CDAWG, we store the difference between the first byte of  and the first byte of  in the sequence. If  is the sink, such difference is replaced by a shorter code. We choose to store the length of the maximal repeat that corresponds to each node, rather than the offset of  inside  for every arc , since such lengths are short and smaller than the number of arcs in practice.

In the second encoding we exploit the fact that the subgraph of the suffix tree of  induced by maximal repeats is a spanning tree of  (see Section \ref{sec:strings}). Specifically, we encode such spanning tree with the balanced parenthesis scheme described in \cite{Munro01}, and we resolve the arcs of the CDAWG that belong to the tree using corresponding tree operations. Such operations work on node identifiers, thus we need to convert node identifiers to the first byte in the byte sequence of the CDAWG, and vice versa. For this, we encode the monotone sequence of the first byte of all  nodes in the byte sequence using the quasi-succinct representation by Elias and Fano, which uses at most  bits per starting position, where  is the number of bytes in the byte sequence \cite{elias1975complexity}.

Finally, we observe that classical CDAWG construction algorithms (e.g. the online algorithm described in \cite{CrochemoreV97}) are not space-efficient, and we design linear-time algorithms that build a representation of the CDAWG from  or , using optimal additional space. Specifically, let  be a function that returns the set of distinct characters that appear in , not necessarily in lexicographic order. We prove the following lemmas in Appendix \ref{appendix:cdawg1} and \ref{appendix:cdawg2}:

\begin{lemma} \label{lemma:cdawg1}
Let , where . Given a representation of  that answers  in time  per element in its output, and LF in time , we can build the topology of , as well as the first character and the length of the label of each arc, in randomized  time and zero space in addition to the input and the output. 
\end{lemma}

\begin{lemma} \label{lemma:cdawg2}
Let , where . Given a representation of  that answers  in time  per element in its output, and LF in time , we can build the topology of , as well as the first character and the length of the label of each arc, in randomized  time and  bits of space in addition to the input and the output.
\end{lemma}












































\section{Experimental results} \label{sec:experiments}

We test our implementations on five DNA datasets from the Pizza\&Chili repetitive corpus \cite{pizzachili}, which include the whole genomes of approximately 36 strains of the same eukaryotic species (``Saccharomyces cerevisiae'' and ``Saccharomyces paradoxus'' in the plots), a collection of 23 and approximately 78 thousand substrings of the genome of the same bacterium (respectively ``Escherichia coli'' and ``Haemophilus influenzae''), and an artificially repetitive string obtained by concatenating a hundred mutated copies of the same substring of the human genome (denoted by ``pseudo-real'' in the plots)\footnote{Compressing such files with  \cite{p7zip}, an implementation of LZ77 with large window, makes the uncompressed files between 21 and 370 times bigger than the corresponding compressed files \cite{pizzachili}.}. We compare our results to the FM index implementation in SDSL \cite{gbmp2014sea} with sampling rate  for  (represented by black circles in all plots), to an implementation of RLCSA\footnote{We compile the sequential version with \texttt{PSI\_FLAGS} and \texttt{SA\_FLAGS} turned off (thus, a gap-encoded bitvector rather than a succinct bitvector is used to mark sampled positions in the suffix array). The block size of psi vectors (\texttt{RLCSA\_BLOCK\_SIZE}) is 32 bytes.} \cite{adamnovak} with the same sampling rates (triangles in the plots), to the five variants in the implementation of the LZ77 index described in \cite{kreft2010self} (squares), and to a recent implementation of the compressed hybrid index \cite{valenzuela2016chico} (diamonds). The FM index uses RRR bitvectors in its wavelet tree. For brevity, we call LZ1 the implementation of the LZ77 index that uses suffix trie and reverse trie. For each process, and for each pattern length  for , we measure the maximum resident set size and the number of CPU seconds that the process spends in user mode\footnote{We perform all experiments on a single core of a 6-core, 2.50 GHz, Intel Xeon E5-2640 processor, with access to 128GiB of RAM and running CentOS 6.3. We measure resources with GNU Time 1.7, and we compile with GCC 5.3.0.}, both for locate and for count queries, discarding the time for loading the indexes and averaging our measurements over one thousand patterns\footnote{We generate random patterns that contain just characters in  using the \texttt{genpatterns} tool from the Pizza\&Chili corpus \cite{pizzachili}.}.

We observe two distinct regimes for locate queries, corresponding to short patterns (shorter than approximately 64) and to long patterns, respectively (Figures \ref{fig:tradeoffs} and \ref{fig:countLocate}). Our full, bidirectional and light index implementations (red circles in all plots) do not achieve any new useful tradeoff, in any dataset, neither with short nor with long patterns. As expected, the running time per pattern of the bidirectional index depends quadratically on pattern length, but we observe a superlinear growth for the light index as well. The optimizations described in Appendix \ref{sec:heuristics} (red dots) are effective only for the bidirectional index, their effectiveness increases with pattern length, and they manage to shave up to 80\% of running time with patterns of length 1024. The size of the bidirectional index on disk is on average 20\% smaller than the size of the full index on disk, and the size of the light index on disk is approximately 20\% smaller than the size of the bidirectional index on disk.

We experiment with skipping  characters before opening a new phrase in the light index with LZ sparsification (green in all plots), where . The size of the sparse index with skip rate  on disk is approximately 35\% smaller than the size of the light index on disk. With short patterns, the memory used by the sparse index becomes smaller than RLCSA and comparable to the LZ index, but its running time per occurrence is between one and two orders of magnitude greater than the LZ index and comparable to RLCSA with sampling rates equal to or greater than 2048 (Figure \ref{fig:tradeoffs}, top). \emph{With long patterns, however, the sparse index becomes between one and two orders of magnitude faster than all variants of the LZ index, except variant LZ1, while using comparable memory}. As a function of pattern length, the running time per occurrence of the sparse index grows more slowly than the running time of LZ1, suggesting that \emph{the sparse index becomes as fast as LZ1 for patterns of length between 1024 and 2048} (Figure \ref{fig:countLocate}, top). The sparse index is approximately 1.5 orders of magnitude slower than the hybrid index, but since the size of the hybrid index depends on maximum pattern length, \emph{the sparse index becomes smaller than the hybrid index for patterns of length between 64 and 128}, and possibly even shorter (Figure \ref{fig:disk}, top). As expected, the sparse index is faster than both the LZ index and the hybrid index in count queries, especially for short patterns: specifically, \emph{the sparse index is between two and four orders of magnitude faster than all variants of the LZ index}, with the largest difference for patterns of length 8 (Figure \ref{fig:countLocate}, bottom). The difference between the sparse index and variant LZ1 shrinks as pattern length increases. Similar trends hold for the hybrid index. The full, bidirectional and light indexes show similar count times as the sparse index.

\begin{figure}
\begin{center}
\includegraphics[width=1\textwidth]{tradeoffs-eps-converted-to.pdf}
\caption{Space-time tradeoffs of our indexes (color) and of the state of the art (black). Top row: patterns of length 16. Bottom row: patterns of length 512. For more clarity, RLCSA and sparse index are tested also on additional configurations not mentioned in Section \ref{sec:experiments}.
\label{fig:tradeoffs}
}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=1\textwidth]{cdawg-eps-converted-to.pdf}
\caption{Space-time traeoffs of the CDAWG (blue) compared to RLCSA (triangles) with sampling rate , . Patterns of length 8, 6, 4, 2 (from left to right).
\label{fig:cdawg}
}
\end{center}
\end{figure}



\begin{figure}
\begin{center}
\includegraphics[width=1\textwidth]{countLocate-eps-converted-to.pdf}
\caption{Locate time per occurrence (top) and count time per pattern (bottom), as a function of pattern length, for the sparse index with skip rate , , the LZ77 index, and the hybrid index. Count plots show also the FM index and RLCSA.
\label{fig:countLocate}
}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=1\textwidth]{disk-eps-converted-to.pdf}
\caption{(Top) Disk size of the sparse index with skip rate , , compared to the hybrid index with maximum pattern length , , the LZ77 index, and RLCSA with sampling rate , . (Bottom) Disk size of the CDAWG compared to RLCSA with sampling rate , .
\label{fig:disk}
}
\end{center}
\end{figure}

The disk size of the CDAWG is comparable to the disk size of RLCSA with sampling rate between 4 and 8 (Figure \ref{fig:disk}, bottom). Using the succinct representation of the CDAWG (blue dots in all plots) shaves between 20\% and 30\% of the disk size and resident set of the non-succinct representation (blue circles). However, using the non-succinct representation shaves between 20\% and 80\% of the time of the succinct representation, depending on dataset and pattern length. Using the CDAWG to answer locate queries does not achieve any new tradeoff with long patterns (Figure \ref{fig:tradeoffs}, bottom). However, \emph{with short patterns the running time per occurrence of the CDAWG is between 4 and 10 times smaller than the running time per occurrence of a version of RLCSA that uses comparable memory, and with patterns of length two the CDAWG achieves speedups even greater than 10}.















































\subparagraph*{Acknowledgements.}

We thank Miguel \'{A}ngel Mart\'{i}nez for providing implementations of the variants described in \cite{kreft2010self}, and Daniel Valenzuela for providing the implementation described in \cite{valenzuela2016chico}.






\section{Future work}

Designing indexes for repetitive texts is an increasingly active field and it is beyond the scope of this paper to review all the recent proposals --- see, e.g.,~\cite{FGHP14,GGKNP14,TTS14,valenzuela2016chico} --- especially since some have not been implemented. We would like to draw attention to an index described simultaneously and independently by Do et al.~\cite{DJSS14} and Gagie et al.~\cite{GGKNP12}, however, because we think it can be improved and made competitive in practice.

Their index is intended for collections of many similar strings, such as databases of genomes from the same species.  The main idea is to choose one of the strings as a reference and build an FM-index for the Relative Lempel-Ziv parse~\cite{KPZ10} of the entire dataset with respect to that reference, treating the phrases as meta-characters.  Using FM-indexes for the reference and its reverse and some auxiliary data structures, we can apply dynamic programming to quickly compute all the ways any given pattern can be decomposed into the suffix of a phrase (possibly empty), a sequence of complete phrases, and the prefix of a phrase (possibly empty).  This is possible because the dictionary of potential phrases --- i.e., all the substrings of the reference --- is fixed, albeit very large, and does not change while we parse (in contrast to the dictionaries for LZ77 and LZ78).

Using the FM-index for the parse and some more auxiliary data structures, we can quickly find whether and where any of the possible decompositions of the pattern occur in the the parse of the dataset.  These occurrences correspond to occurrences of the pattern that cross phrase boundaries.  We can then quickly find all the other occurrences of the pattern in the dataset.  Unfortunately, if there are many distinct phrases in the parse, the FM-index for it may not compress well.  This raises the question of how we can reduce the number of distinct phrases without increasing the number of phrases too much.

Suppose we build a compressed th-order de Bruijn graph for the collection of strings (i.e., a th-order de Bruijn graph in which we have collapsed every maximal path whose internal nodes have in- and out-degree 1); assign each of its edges with a distinct meta-character; consider each string as a walk on the graph and, for each edge the walk crosses, replacing the substring causing us to cross that edge by the edge's meta-character.  This results in a parse in which the number of distinct phrases is the number of edges in the graph (assuming each of the strings in the collection has length at least ).  Notice also that, for any pattern of length at least , the pattern corresponds to at most one walk on the uncompressed graph (which may start and finish in the middle of edges in the compressed graph); if there is no such walk, then the pattern does not occur in the collection.  Using a de Bruijn graph also removes the need for choosing a reference and using range reporting.

We may be able to improve the compression by, first, removing low-frequency edges in the original de Bruijn graph before compressing it and, second, replacing substrings only when they cause us to cross high-frequency edges with sufficiently long edge labels in the the compressed de Bruijn graph.  With this modification, a pattern may appear in the collection even if it does not correspond to a walk on the uncompressed graph.  However, if a substring causes us to cross a high-frequency edge with a long edge label in the compressed graph, then we can certainly replace that substring by the edge's meta-character.  It follows that, for any pattern of length at least , we need perform at most four searches in the FM-index for the parse to determine whether and where that pattern occurs in the collection.

We plan to implement and test this modification of Do et al.'s and Gagie et al.'s index soon and report the results in a future paper.



































\subparagraph*{Acknowledgements.}

We thank Miguel \'{A}ngel Mart\'{i}nez for providing implementations of the variants described in \cite{kreft2010self}, and Daniel Valenzuela for providing the implementation described in \cite{valenzuela2016chico}.













\begin{thebibliography}{10}

\bibitem{arroyuelo2012stronger}
Diego Arroyuelo, Gonzalo Navarro, and Kunihiko Sadakane.
\newblock Stronger {Lempel-Ziv} based compressed text indexing.
\newblock {\em Algorithmica}, 62(1-2):54--101, 2012.

\bibitem{belazzougui2014linear}
Djamal Belazzougui.
\newblock Linear time construction of compressed text indices in compact space.
\newblock In {\em Proceedings of the 46th Annual ACM Symposium on Theory of
  Computing}, pages 148--193. ACM, 2014.

\bibitem{belazzougui2015composite}
Djamal Belazzougui, Fabio Cunial, Travis Gagie, Nicola Prezza, and Mathieu
  Raffinot.
\newblock Composite repetition-aware data structures.
\newblock In {\em Combinatorial Pattern Matching}, pages 26--39. Springer,
  2015.

\bibitem{blumer1987complete}
Anselm Blumer, Janet Blumer, David Haussler, Ross McConnell, and Andrzej
  Ehrenfeucht.
\newblock Complete inverted files for efficient text retrieval and analysis.
\newblock {\em Journal of the ACM}, 34(3):578--595, 1987.

\bibitem{chan2011orthogonal}
Timothy~M Chan, Kasper~Green Larsen, and Mihai P{\u{a}}tra{\c{s}}cu.
\newblock Orthogonal range searching on the {RAM}, revisited.
\newblock In {\em Proceedings of the twenty-seventh annual symposium on
  computational geometry}, pages 1--10. ACM, 2011.

\bibitem{crochemore1997automata}
Maxime Crochemore and Christophe Hancart.
\newblock Automata for matching patterns.
\newblock In {\em Handbook of formal languages}, pages 399--462. Springer,
  1997.

\bibitem{CrochemoreV97}
Maxime Crochemore and Renaud V\'erin.
\newblock Direct construction of compact directed acyclic word graphs.
\newblock In Alberto Apostolico and Jotun Hein, editors, {\em CPM}, volume 1264
  of {\em Lecture Notes in Computer Science}, pages 116--129. Springer, 1997.

\bibitem{DJSS14}
Huy~Hoang Do, Jesper Jansson, Kunihiko Sadakane, and Wing{-}Kin Sung.
\newblock Fast relative lempel-ziv self-index for similar sequences.
\newblock {\em Theoretical Computer Science}, 532:14--30, 2014.

\bibitem{elias1975complexity}
Peter Elias and Richard~A Flower.
\newblock The complexity of some simple retrieval problems.
\newblock {\em Journal of the ACM (JACM)}, 22(3):367--379, 1975.

\bibitem{FGHP14}
Hector Ferrada, Travis Gagie, Tommi Hirvola, and Simon J.~Puglisi.
\newblock Hybrid indexes for repetitive datasets.
\newblock Philosophical Transactions of the Royal Society of London A, 372(2016), 2014.

\bibitem{pizzachili}
Paolo Ferragina and Gonzalo Navarro.
\newblock {Pizza\&Chili repetitive corpus}.
\newblock \url{http://pizzachili.dcc.uchile.cl/repcorpus.html}.
\newblock Accessed: 2016-04-10.

\bibitem{GGKNP12}
Travis Gagie, Pawel Gawrychowski, Juha K{\"{a}}rkk{\"{a}}inen, Yakov Nekrich,
  and Simon~J. Puglisi.
\newblock A faster grammar-based self-index.
\newblock In {\em Proceedings of the 6th Conference on Language and Automata
  Theory and Applications}, pages 240--251, 2012.

\bibitem{gagie2014lz77}
Travis Gagie, Pawe{\l} Gawrychowski, Juha K{\"a}rkk{\"a}inen, Yakov Nekrich,
  and Simon~J Puglisi.
\newblock {LZ77}-based self-indexing with faster pattern matching.
\newblock In {\em {LATIN} 2014: Theoretical Informatics}, pages 731--742.
  Springer, 2014.

\bibitem{GGKNP14}
Travis Gagie, Pawel Gawrychowski, Juha K{\"{a}}rkk{\"{a}}inen, Yakov Nekrich,
  and Simon~J. Puglisi.
\newblock Lz77-based self-indexing with faster pattern matching.
\newblock In {\em Proceedings of the 11th Latin American Symposium on
  Theoretical Informatics}, pages 731--742, 2014.

\bibitem{gbmp2014sea}
Simon Gog, Timo Beller, Alistair Moffat, and Matthias Petri.
\newblock From theory to practice: Plug and play with succinct data structures.
\newblock In {\em 13th International Symposium on Experimental Algorithms, (SEA
  2014)}, pages 326--337, 2014.

\bibitem{karkkainen1996lempel}
Juha K{\"a}rkk{\"a}inen and Esko Ukkonen.
\newblock {Lempel-Ziv} parsing and sublinear-size index structures for string
  matching.
\newblock In {\em Proc. 3rd South American Workshop on String Processing
  (WSP'96}, pages 141--155, 1996.

\bibitem{kreft2010self}
Sebastian Kreft.
\newblock Self-index based on lz77.
\newblock Master's thesis, Department of Computer Science, University of Chile,
  2010.

\bibitem{kreft2013compressing}
Sebastian Kreft and Gonzalo Navarro.
\newblock On compressing and indexing repetitive sequences.
\newblock {\em Theoretical Computer Science}, 483:115--133, 2013.

\bibitem{KPZ10}
Shanika Kuruppu, Simon~J. Puglisi, and Justin Zobel.
\newblock Relative lempel-ziv compression of genomes for large-scale storage
  and retrieval.
\newblock In {\em Proceedings of the 17th Symposium on String Processing and
  Information Retrieval}, pages 201--206, 2010.

\bibitem{makinen2005succinct1}
Veli M{\"a}kinen and Gonzalo Navarro.
\newblock Succinct suffix arrays based on run-length encoding.
\newblock In {\em Combinatorial Pattern Matching}, pages 45--56. Springer,
  2005.

\bibitem{MakinenNSV10}
Veli M{\"{a}}kinen, Gonzalo Navarro, Jouni Sir{\'{e}}n, and Niko
  V{\"{a}}lim{\"{a}}ki.
\newblock Storage and retrieval of highly repetitive sequence collections.
\newblock {\em Journal of Computational Biology}, 17(3):281--308, 2010.

\bibitem{morrison1968patricia}
Donald~R Morrison.
\newblock Patriciaâ€”practical algorithm to retrieve information coded in
  alphanumeric.
\newblock {\em Journal of the ACM (JACM)}, 15(4):514--534, 1968.

\bibitem{Munro01}
J.~Ian Munro and Venkatesh Raman.
\newblock Succinct representation of balanced parentheses and static trees.
\newblock {\em SIAM J. Comput.}, 31(3):762--776, March 2002.
\newblock URL: \url{http://dx.doi.org/10.1137/S0097539799364092}, \href
  {http://dx.doi.org/10.1137/S0097539799364092}
  {\path{doi:10.1137/S0097539799364092}}.

\bibitem{adamnovak}
Adam Novak.
\newblock Convenient repository for/fork of the {RLCSA} library.
\newblock \url{https://github.com/adamnovak/rlcsa}.
\newblock Accessed: 2016-04-10.

\bibitem{p7zip}
Igor Pavlov.
\newblock {P7ZIP} home.
\newblock \url{http://p7zip.sourceforge.net}.
\newblock Accessed: 2016-04-10.

\bibitem{githubNicola1}
Nicola Prezza.
\newblock \texttt{lz-rlbwt}: {R}un-length compressed {B}urrows-{W}heeler
  transform with {LZ77} suffix array sampling.
\newblock \url{https://github.com/nicolaprezza/lz-rlbwt}.
\newblock Accessed: 2016-04-10.

\bibitem{githubNicola2}
Nicola Prezza.
\newblock \texttt{lz-rlbwt-sparse}: {R}un-length compressed {B}urrows-{W}heeler
  transform with sparse {LZ77} suffix array sampling.
\newblock \url{https://github.com/nicolaprezza/lz-rlbwt-sparse}.
\newblock Accessed: 2016-04-10.

\bibitem{githubMathieu}
Mathieu Raffinot.
\newblock \texttt{locate-cdawg}: {R}eplacing sampling by {CDAWG} localisation
  in {BWT} indexing approaches.
\newblock \url{https://github.com/mathieuraffinot/locate-cdawg}.
\newblock Accessed: 2016-04-10.

\bibitem{Raffinot2001}
Mathieu Raffinot.
\newblock On maximal repeats in strings.
\newblock {\em Information Processing Letters}, 80(3):165--169, 2001.

\bibitem{SirenVMN08}
Jouni Sir{\'{e}}n, Niko V{\"{a}}lim{\"{a}}ki, Veli M{\"{a}}kinen, and Gonzalo
  Navarro.
\newblock Run-length compressed indexes are superior for highly repetitive
  sequence collections.
\newblock In {\em String Processing and Information Retrieval, 15th
  International Symposium, {SPIRE} 2008, Melbourne, Australia, November 10-12,
  2008.}, pages 164--175, 2008.

\bibitem{TTS14}
Yoshimasa Takabatake, Yasuo Tabei, and Hiroshi Sakamoto.
\newblock Improved esp-index: {A} practical self-index for highly repetitive
  texts.
\newblock In {\em Proceedings of the 13th Symposium on Experimental
  Algorithms}, pages 338--350, 2014.

\bibitem{valenzuela2016chico}
Daniel Valenzuela.
\newblock {CHICO}: A compressed hybrid index for repetitive collections.
\newblock In {\em Proceedings of the fifteenth International Symposium on
  Experimental Algorithms (SEA 2016)}, Lecture Notes in Computer Science.
  Springer, June 2016.

\bibitem{Wi83}
Dan~E Willard.
\newblock Log-logarithmic worst-case range queries are possible in space
  {Theta}(n).
\newblock {\em Information Processing Letters}, 17(2):81--84, 1983.

\bibitem{ziv1977universal}
Jacob Ziv and Abraham Lempel.
\newblock A universal algorithm for sequential data compression.
\newblock {\em IEEE Transactions on information theory}, 23(3):337--343, 1977.

\end{thebibliography}





































\appendix


\section{Proof of Lemma \ref{lemma:cdawg1}} \label{appendix:cdawg1}

We use the algorithm described in \cite{belazzougui2014linear} to enumerate a representation of every node of  by performing a depth-first traversal of the \emph{suffix link tree} of . Such algorithm works in  time and in  bits of working space\footnote{The enumeration algorithm described in \cite{belazzougui2014linear} uses the stack trick to fit the working space in  bits. Without such trick, the working space would be proportional to the largest number of left-extensions of maximal repeats that lie in the same path of the suffix-link tree of , which can be charged to the output of Lemma \ref{lemma:cdawg1}.}
, and it provides, for each node , its interval in  and the length of its label, as well as the list of all its children, and for every such child , the interval of  in  and the first character of the label of edge  in . 

Since the label of every node of the CDAWG is a maximal repeat of , the set of nodes of the CDAWG (excluding the sink ) is in one-to-one correspondence with a subset of the nodes of the suffix tree. Specifically, a node  of  corresponds to a node  of the CDAWG if and only if  is a left-maximal substring of . We can check the left-maximality of  by counting the number of distinct characters in the BWT interval of . Every time this number is greater than one, we have discovered a new node  of the CDAWG, and we assign to it a unique identifier by incrementing a global counter. Then, we scan every child  of  in , and we store in a hash table a tuple , where  is the interval of  and is used as key, and  is the unique number assigned to . Note that every quadruplet we insert in the hash table has a unique key. If the BWT interval of  is of length one, then  is a leaf, an arc connects  in the CDAWG to the sink  with character , and  is the first character of the label of edge  in . We compute the starting position of  for of all such arcs  in the CDAWG in batch, by inverting  and querying the hash table. The hash table can be implemented to support both insertion and querying in  randomized time. BWT inversion takes  time and an amount of memory that can be charged to the output.

To build all arcs of the CDAWG that are not directed to the sink, we perform another traversal of the suffix-link tree, in the same order as the first traversal. Assume that, during this second traversal, we enumerate a node  whose BWT interval is present in the hash table: then,  is the child of a node  of the suffix tree that corresponds to a node  of the CDAWG. If  corresponds to a node  of the CDAWG as well, i.e. if  if a left-maximal substring of , we add arc  to the CDAWG. Otherwise  is not left-maximal, thus the CDAWG must contain arc  where  is the shortest left-extension of  to be left-maximal, and the corresponding node  in the suffix tree can be reached from  by a unary path in the suffix-link tree. Thus, we keep an auxiliary buffer, initially empty. Every time we encounter a node  of the suffix tree whose interval  is such that a tuple  exists in the hash table, we append to the buffer tuple . Moreover, if  is left-maximal, we empty the buffer and we transform every tuple  in the buffer into an arc  of the CDAWG. The size of such buffer can be charged to the output.





\section{Proof of Lemma \ref{lemma:cdawg2}} \label{appendix:cdawg2}

We proceed as in Appendix \ref{appendix:cdawg1}, traversing the suffix-link tree of , and enumerating the intervals in  of every node of the suffix tree of , i.e. of every right-maximal substring of . Once we detect that a node  is also left-maximal in , we create a new node  of the CDAWG, we assign a new unique identifier to it, we enumerate all the \emph{left-extensions}  in , and we push tuple  in a hash table, where  is the interval of  in  and is used as key, and  is the unique number assigned to . If the BWT interval of  is of length one, an arc connects  to the sink  with character  in the CDAWG. We compute the starting position in  of  for of all such arcs  in the CDAWG in batch, by inverting  and querying the hash table. Note that, since we are possibly pushing the intervals in  of the destinations of \emph{implicit Weiner links} in the suffix tree of , the hash table has to allow the presence of distinct tuples with the same key.

To build all arcs of the CDAWG that are not directed to the sink, we perform another traversal of the suffix-link tree of , in the same order as the first traversal. Assume that, during this second traversal, we enumerate a node  of the suffix tree of  whose interval in  is present in the hash table in a set of tuples \}. If  corresponds to a node  of the CDAWG as well, i.e. if  if a left-maximal substring of , we add arc  to the CDAWG for every tuple in . Otherwise  is not left-maximal in , thus the CDAWG must contain arc  for every tuple in , where  is the shortest left-extension of  to be left-maximal in , and the corresponding node  in the suffix tree of  can be reached from  by a unary path in the suffix-link tree of . Thus, we keep an auxiliary buffer, initially empty. Every time we encounter a node  of the suffix tree of  whose interval  is such that a set of tuples  exists in the hash table, we append to the buffer a corresponding set of tuples . Moreover, if  is left-maximal in , we empty the buffer and we transform every tuple  in the buffer into an arc  of the CDAWG. The size of such buffer can be charged to the output.











\section{Speeding up locate queries on indexes based on RLBWT and LZ factors} \label{sec:heuristics}

On indexes based on RLBWT and LZ factors, locate queries can be further engineered in a number of ways:
\begin{enumerate}
\item Thanks to the RLBWT, we know the total number of occurrences of  in  before starting to locate them: thus, we can stop locating as soon as we have found all occurrences.

\item We could add a compressed bitvector  that flags a position  iff  for some . As we backward-search  in , we could mark the positions  such that the interval of  in  contains only zeros in , and discard them in the following steps. For every discarded suffix, this strategy saves a  backward search in the bidirectional index.


\item The four-sided range reporting data structure could use the reversed prefixes  rather than the reversed prefixes  for all . This would allow checking, at position  in , whether  ends at a position  of  such that  is the last position of a factor, and such that . To implement an ever more stringent filter, one could store an additional four-sided range reporting data structure that uses the reversed prefixes  for , reverting to the four-sided data structure for  when .

\item In the bidirectional index, we could quit the synchronized backward search for  in  as soon as we find a suffix  which is not right-maximal, and which is not followed by  in , or which does not contain a factor as a suffix. Moreover, we could quit the backward search as soon as we detect that  is right-maximal, but it is neither a factor nor the suffix of a factor, and it does not contain a factor as a suffix. Such tests can be implemented e.g. using variations of function , and by replacing the interval of  with the  subintervals of  for all , as described in \cite{belazzougui2014linear}. Every backward step would have a  overhead in this case.

\item In the bidirectional index, we could speed up the backward search that we have to perform for every , by precomputing a table of intervals in  and  for all strings of length . This would take at most  bits of additional space, or at most  bits if we just store the -mers that suffix a factor.
\end{enumerate}

Due to lack of space, in the main paper we study just the effects of the first two optimizations.

\end{document}