

\documentclass[runningheads]{llncs}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{booktabs} \usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xspace}
\usepackage{cite}

\usepackage{makecell}
\usepackage{xcolor}
\usepackage{multirow} \usepackage{bigstrut} 



\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}

\usepackage{pdfpages} 



\usepackage[accsupp]{axessibility}  




\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\clearpage{}\usepackage{bm} 

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\def\support{\mbox{support}}
\def\diag{\mbox{diag}}
\def\rank{\mbox{rank}}
\def\grad{\mbox{\text{grad}}}
\def\dist{\mbox{dist}}
\def\sgn{\mbox{sgn}}
\def\tr{\mbox{tr}}
\def\card{{\mbox{Card}}}

\def\balpha{\mbox{{\boldmath }}}
\def\bbeta{\mbox{{\boldmath }}}
\def\bzeta{\mbox{{\boldmath }}}
\def\bgamma{\mbox{{\boldmath }}}
\def\bdelta{\mbox{{\boldmath }}}
\def\bmu{\mbox{{\boldmath }}}
\def\bftau{\mbox{{\boldmath }}}
\def\beps{\mbox{{\boldmath }}}
\def\blambda{\mbox{{\boldmath }}}
\def\bLambda{\mbox{{\boldmath }}}
\def\bnu{\mbox{{\boldmath }}}
\def\bomega{\mbox{{\boldmath }}}
\def\bfeta{\mbox{{\boldmath }}}
\def\bsigma{\mbox{{\boldmath }}}
\def\bzeta{\mbox{{\boldmath }}}
\def\bphi{\mbox{{\boldmath }}}
\def\bxi{\mbox{{\boldmath }}}
\def\bvphi{\mbox{{\boldmath }}}
\def\bdelta{\mbox{{\boldmath }}}
\def\bvarpi{\mbox{{\boldmath }}}
\def\bvarsigma{\mbox{{\boldmath }}}
\def\bXi{\mbox{{\boldmath }}}
\def\bmW{\mbox{{\boldmath }}}
\def\bmY{\mbox{{\boldmath }}}

\def\bPi{\mbox{{\boldmath }}}

\def\bOmega{\mbox{{\boldmath }}}
\def\bDelta{\mbox{{\boldmath }}}
\def\bPi{\mbox{{\boldmath }}}
\def\bSigma{\mbox{{\boldmath }}}
\def\bUpsilon{\mbox{{\boldmath }}}

\def\mA{{\mathcal A}}
\def\mB{{\mathcal B}}
\def\mC{{\mathcal C}}
\def\mD{{\mathcal D}}
\def\mE{{\mathcal E}}
\def\mF{{\mathcal F}}
\def\mG{{\mathcal G}}
\def\mH{{\mathcal H}}
\def\mI{{\mathcal I}}
\def\mJ{{\mathcal J}}
\def\mK{{\mathcal K}}
\def\mL{{\mathcal L}}
\def\mM{{\mathcal M}}
\def\mN{{\mathcal N}}
\def\mO{{\mathcal O}}
\def\mP{{\mathcal P}}
\def\mQ{{\mathcal Q}}
\def\mR{{\mathcal R}}
\def\mS{{\mathcal S}}
\def\mT{{\mathcal T}}
\def\mU{{\mathcal U}}
\def\mV{{\mathcal V}}
\def\mW{{\mathcal W}}
\def\mX{{\mathcal X}}
\def\mY{{\mathcal Y}}
\def\mZ{{\mathcal{Z}}}



\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\def\bmA{{\mathbfcal A}}
\def\bmB{{\mathbfcal B}}
\def\bmC{{\mathbfcal C}}
\def\bmD{{\mathbfcal D}}
\def\bmE{{\mathbfcal E}}
\def\bmF{{\mathbfcal F}}
\def\bmG{{\mathbfcal G}}
\def\bmH{{\mathbfcal H}}
\def\bmI{{\mathbfcal I}}
\def\bmJ{{\mathbfcal J}}
\def\bmK{{\mathbfcal K}}
\def\bmL{{\mathbfcal L}}
\def\bmM{{\mathbfcal M}}
\def\bmN{{\mathbfcal N}}
\def\bmO{{\mathbfcal O}}
\def\bmP{{\mathbfcal P}}
\def\bmQ{{\mathbfcal Q}}
\def\bmR{{\mathbfcal R}}
\def\bmS{{\mathbfcal S}}
\def\bmT{{\mathbfcal T}}
\def\bmU{{\mathbfcal U}}
\def\bmV{{\mathbfcal V}}
\def\bmW{{\mathbfcal W}}
\def\bmX{{\mathbfcal X}}
\def\bmY{{\mathbfcal Y}}
\def\bmZ{{\mathbfcal Z}}



\def\0{{\bf 0}}
\def\1{{\bf 1}}

\def\bA{{\bf A}}
\def\bB{{\bf B}}
\def\bC{{\bf C}}
\def\bD{{\bf D}}
\def\bE{{\bf E}}
\def\bF{{\bf F}}
\def\bG{{\bf G}}
\def\bH{{\bf H}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bK{{\bf K}}
\def\bL{{\bf L}}
\def\bM{{\bf M}}
\def\bN{{\bf N}}
\def\bO{{\bf O}}
\def\bP{{\bf P}}
\def\bQ{{\bf Q}}
\def\bR{{\bf R}}
\def\bS{{\bf S}}
\def\bT{{\bf T}}
\def\bU{{\bf U}}
\def\bV{{\bf V}}
\def\bW{{\bf W}}
\def\bX{{\bf X}}
\def\bY{{\bf Y}}
\def\bZ{{\bf{Z}}}


\def\ba{{\bf a}}
\def\bb{{\bf b}}
\def\bc{{\bf c}}
\def\bd{{\bf d}}
\def\be{{\bf e}}
\def\bff{{\bf f}}
\def\bg{{\bf g}}
\def\bh{{\bf h}}
\def\bi{{\bf i}}
\def\bj{{\bf j}}
\def\bk{{\bf k}}
\def\bl{{\bf l}}
\def\bn{{\bf n}}
\def\bo{{\bf o}}
\def\bp{{\bf p}}
\def\bq{{\bf q}}
\def\br{{\bf r}}
\def\bs{{\bf s}}
\def\bt{{\bf t}}
\def\bu{{\bf u}}
\def\bv{{\bf v}}
\def\bw{{\bf w}}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bz{{\bf z}}

\def\hy{\hat{y}}
\def\hby{\hat{{\bf y}}}


\def\mmE{{\mathbb E}}
\def\mmP{{\mathrm P}}
\def\mmB{{\mathrm B}}
\def\mmR{{\mathbb R}}
\def\mmV{{\mathbb V}}
\def\mmN{{\mathbb N}}
\def\mmZ{{\mathbb Z}}
\def\mMLr{{\mM_{\leq k}}}
\def\mmI{{\mathbb I}}
\def\mmS{{\mathbb S}}
\def\mmL{{\mathbb L}}

\def\tC{\tilde{C}}
\def\tk{\tilde{r}}
\def\tJ{\tilde{J}}
\def\tbx{\tilde{\bx}}
\def\tbK{\tilde{\bK}}
\def\tL{\tilde{L}}
\def\tbPi{\mbox{{\boldmath }}}
\def\tw{{\bf \tilde{w}}}



\def\barx{\bar{\bx}}

\def\pd{{\succ\0}}
\def\psd{{\succeq\0}}
\def\vphi{\varphi}
\def\trsp{{\sf T}}

\def\mRMD{{\mathrm{D}}}
\def \DKL{{D_{KL}}}
\def\st{{\mathrm{s.t.}}}
\def\nth{{\mathrm{th}}}


\def\bx{{\bf x}}
\def\bX{{\bf X}}
\def\by{{\bf y}}
\def\bY{{\bf Y}}
\def\bw{{\bf w}}
\def\bW{{\bf W}}
\def\balpha{{\bm \alpha}}
\def\bbeta{{\bm \beta}}
\def\boldeta{{\bm \eta}}
\def\boldEta{{\bm \Eta}}
\def\bgamma{{\bm \gamma}}
\def\bGamma{{\bm \Gamma}}
\def\bmu{{\bm \mu}}
\def\bbeta{{\bm \beta}}

\def\bK{{\bf K}}
\def\bb{{\bf b}}
\def\bg{{\bf g}}
\def\bp{{\bf p}}
\def\bP{{\bf P}}
\def\bh{{\bf h}}
\def\bc{{\bf c}}
\def\bz{{\bf z}}


\def\st{{\mathrm{s.t.}}}
\def\tr{\mathrm{tr}}
\def\grad{{\mathrm{grad}}}

\newtheorem{coll}{Corollary}
\newtheorem{deftn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}


\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\def\eg{\emph{e.g.,}} \def\Eg{\emph{E.g.}}
\def\ie{\emph{i.e.,}} \def\Ie{\emph{I.e.}}
\def\cf{\emph{c.f.}} \def\Cf{\emph{C.f.}}
\def\etc{\emph{etc.}} \def\vs{\emph{vs.}}
\def\wrt{{w.r.t.}} \def\dof{d.o.f}
\def\etal{{\em et al.\/}\,}\clearpage{}

\newcommand{\topline}{\toprule[0.1em]}
\newcommand{\midline}{\midrule[0.075em]}
\newcommand{\doublemidline}{\midrule\midrule}
\newcommand{\bottomline}{\bottomrule[0.1em]}

\usepackage{xspace}

\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{3844}  

\setlength{\textfloatsep}{6pt}

\newcommand{\name}{Densely-Anchored Sampling\xspace}
\newcommand{\shortname}{DAS\xspace}
\newcommand{\lowername}{densely-anchored sampling\xspace}

\newcommand{\bfs}{Discriminative Feature Scaling\xspace}
\newcommand{\shortbfs}{DFS\xspace}

\newcommand{\tmb}{Memorized Transformation Shifting\xspace}
\newcommand{\shorttmb}{MTS\xspace}

\newcommand{\fullstop}{~}
\newcommand{\na}{\text{N/A}}



\newcommand{\crrevision}{\textcolor{black}}

\newcommand{\lec}{\textcolor{black}}
\newcommand{\vtars}{\textcolor{black}}
\newcommand{\xxl}{\textcolor{black}}
\newcommand{\xhm}{\textcolor{black}}
\newcommand{\zsh}{\textcolor{black}}
\newcommand{\ice}{\textcolor{black}}
\newcommand{\kui}{\textcolor{black}}
\newcommand{\revision}{\textcolor{black}}
\def\hd{\textcolor{black}}

\newcommand{\improve}{\textbf}
\newcommand{\important}{\textcolor{red}}

\definecolor{corrected}{RGB}{177,222,140}
\definecolor{wrong}{RGB}{255,51,0}

\title{\shortname: \name for Deep Metric Learning} 

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{\shortname: \name for Deep Metric Learning}
\author{Lizhao Liu\inst{1,2} \and
Shangxin Hunag\inst{1} \and
Zhuangwei Zhuang\inst{1} \and \\
Ran Yang\inst{1} \and
Mingkui Tan\inst{1,3} \and
Yaowei Wang\inst{2}\thanks{Corresponding authors.}}
\authorrunning{L. Liu et al.}
\institute{
South China University of Technology PengCheng Laboratory 
\\Information Technology R\&D Innovation Center of Peking University
\email{\{selizhaoliu,sevtars,z.zhuangwei,msyangran\}@mail.scut.edu.cn}\texttt{,}\\
\email{mingkuitan@scut.edu.cn}\texttt{,} \email{wangyw@pcl.ac.cn}
}






\makeatletter
\renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or
		\mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
		\or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\maketitle

\begin{abstract}
\kui{Deep Metric Learning (DML) serves to learn an embedding function to project semantically similar data into nearby embedding space and plays a vital role in many applications, such as image retrieval and face recognition. However, the performance of DML methods often highly depends on sampling methods to choose effective data from the embedding space in the training. In practice, the embeddings in the embedding space are obtained by some deep models, \lec{where the embedding space is often with} \ice{barren area} due to the absence of training points, resulting in so called ``missing embedding'' issue.}
\lec{This issue may impair the sample quality, which leads to degenerated DML performance. In this work, we investigate how to alleviate the ``missing embedding'' issue to improve the sampling quality and achieve effective DML. To this end, we propose a \name (\shortname) scheme that \revision{considers the embedding with corresponding data point as ``anchor'' and exploits the anchor's} nearby embedding space to densely produce embeddings without data points. Specifically, we propose to exploit the embedding space around single anchor with \bfs (\shortbfs) and multiple anchors with \tmb (\shorttmb). In this way, by combing the embeddings with and without data points, we are able to provide more embeddings to facilitate the sampling process thus boosting the performance of DML.
Our method is effortlessly integrated into existing DML frameworks and improves them without bells and whistles. Extensive experiments on three benchmark datasets demonstrate the superiority of our method.}
\keywords{\small{Deep Metric Learning \and Missing Embedding \and Embedding Space Exploitation \and \name}}
\end{abstract}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{imgs/motivationV8.pdf}
    \caption{\lec{Illustration of the ``missing embedding'' issue. The data points of similar semantics are mapped into the \revision{nearby} embedding space \lec{that \revision{is} often with} \ice{barren area} due to the absence of data points,} resulting in the ``missing embedding'' issue\fullstop}
    \label{fig:motivation1}
\end{figure}


\section{Introduction}
Deep Metric learning (DML) is the foundation of various applications, including face recognition, verification~\cite{deng2019arcface,schroff2015facenet}, image retrieval~\cite{hoi2010semi}, image clustering~\cite{guo2017improved}, image classification~\cite{ding2016robust}, few-shot learning~\cite{liu2020dynamic}, \crrevision{video representation learning~\cite{chen2020rspnet} and sound generation~\cite{chen2020generating}} \etc~Since it was introduced, it has sparked considerable interest in the community, where academics have offered a variety of methods~\cite{hadsell2006dimensionality,movshovitz2017no,oh2016deep,schroff2015facenet,sohn2016improved,wang2019multi,wu2017sampling} and have made substantial progress~\cite{musgrave2020metric,roth2020revisiting}. The goal of DML is to learn \lec{a deep model} that is capable of mapping semantically similar \lec{data points} to \lec{similar embeddings in the embedding space.} To accomplish this, most existing approaches~\cite{chen2017beyond,hadsell2006dimensionality,schroff2015facenet,ustinova2016learning,wang2019multi,wu2017sampling} \lec{train the deep model with loss functions that bring the embeddings from semantically similar data points close to each other and vice versa. However, some embeddings may have limited contribution or bring no improvement to train the deep model~\cite{wu2017sampling}, or even lead to bad local minima early on in training (such as a collapsed model)~\cite{schroff2015facenet}. Thus, sampling informative and stable embeddings is very important to facilitate the training of deep model~\cite{wu2017sampling}.} As a result, improving the sample quality is of significance \lec{to achieve effective DML}. There are two commonly used measures for this goal: designing more effective sampling methods or providing more \lec{embeddings}.

Pioneering efforts have made substantial progress toward the design of effective sampling methods upon embedding pairs~\cite{roth2020revisiting,schroff2015facenet,wu2017sampling} or a full batch of embeddings~\cite{movshovitz2017no,oh2016deep}. \lec{These methods typically perform sampling on a batch of embeddings, which often leads to inaccurate sampling results due to the following reasons.}
First, the batch size is typically constrained by the memory of a single GPU as the sampling process typically cannot cross different GPU devices~\cite{roth2020revisiting}. Second, even with GPU that has sufficient memory to support a larger batch size, \lec{the embedding space that contains the embeddings embedded by deep models may still with \ice{barren area} due to the absence of data points, resulting in a ``missing embedding'' issue} (as shown in Fig.~\ref{fig:motivation1}). \lec{Thus, the limited amount of embeddings may impair the sample quality and the performance of DML.} \lec{Based on the above analyses, we ask: ``Can we overcome the inaccurate sampling issue brought by the absence of data points?''}

Very recently, a few attempts~\cite{duan2018deep,gu2021proxy,ko2020embedding,wang2020cross,zheng2019hardness} have been committed to answering this question by pseudo embedding generation. Hard \lec{example} generation \lec{approaches}~\cite{duan2018deep,zheng2019hardness} generate hard embeddings from easy embeddings with an additional generative adversarial network or auto-encoder. Embedding expansion~\cite{ko2020embedding} performs interpolation between embeddings to achieve augmentation in embedding space. Cross batch memory~\cite{wang2020cross} maintains embeddings from previous iterations and considers them are still informative in the current batch in terms of sampling. However, these approaches either leverage additional sub-network~\cite{duan2018deep,zheng2019hardness}, which introduce extra training cost, or need further modification to the sampling and loss computation process~\cite{ko2020embedding,wang2020cross} in standard DML~\cite{hadsell2006dimensionality,hermans2017defense,schroff2015facenet,sohn2016improved,wang2019multi}, which may limit their applicability to other tasks.

\lec{In this paper, we seek to densely produce embeddings without data points to alleviate the ``missing embedding'' issue. In this way, with the combination of embeddings with and without data points, we are able to provide more embeddings for sampling to improve the sample quality and achieve effective DML.} Our motivation stems from a fundamental hypothesis of metric learning: \textit{the embeddings that are close to each other in the embedding space have similar semantics.} \revision{Unfortunately, how to exploit the embedding space to produce effective embeddings without data points remains an unsolved problem.} \lec{\revision{To this end}, we propose a \name (\shortname) scheme to \revision{consider the embedding with data point as ``anchor'' and densely} exploit \revision{the anchor's} nearby embedding space to produce embeddings that have no corresponding data points. \revision{The proposed \shortname is consist of} two modules, namely, \bfs (\shortbfs) and \tmb (\shorttmb), which exploit the embedding space around single and multiple embeddings respectively to produce effective embeddings with no corresponding data points. To be specific, based on observations that effective semantics for one embedding are highly activated features~\cite{bau2017network,bau2020understanding,escorcia2015relationship}, \shortbfs identifies these features and applies random scaling on them. In this sense, we are able to exploit the embedding space around a single embedding by enhancing or weakening its effective semantics to produce embeddings. 
Based on the fact that semantic differences (\ie~transformations) of intra-class embeddings can be added to other embeddings to generate effective embeddings~\cite{lin2018deep}, we assume that they can be added in a way like word embeddings~\cite{mikolov2013linguistic}: \crrevision{\textit{}}. Thus, our \tmb module exploits the embedding space among multiple embeddings by adding (\ie~shift) intra-class embeddings' semantic differences to other embeddings of the same class to produce effective embeddings. }


Our main contributions are summarized as follows. {\textbf{First}}, \lec{we propose a novel and \crrevision{plug-and-play} \name (\shortname) scheme \crrevision{that exploits embeddings' nearby embedding space and densely produces embeddings without data points to improve the sampling quality and performance of DML.}} \textbf{Second}, \lec{we propose \crrevision{two modules, namely} \bfs (\shortbfs) \crrevision{and \tmb (\shorttmb)} to exploit embedding space around a single \crrevision{and multiple} embeddings to produce embeddings.} \crrevision{\textbf{Last}, extensive experiments demonstrate the effectiveness of the proposed method.} 
\section{Related Work}
\noindent\textbf{Sampling Methods in DML.} \lec{Sampling informative and stable embeddings is vital to train the deep model in DML~\cite{roth2020revisiting,schroff2015facenet,wu2017sampling}.} Thus, various sampling approaches~\cite{ge2018deep,roth2020revisiting,schroff2015facenet,wang2019multi,wu2017sampling} have been tailored \lec{to effectively sample the embeddings to train the deep model}.
To further improve sampling efficiency, some researchers propose to leverage a whole batch of embeddings~\cite{hermans2017defense,kim2020proxy,movshovitz2017no,wang2019multi,zhu2020fewer}. Even though sophisticated sampling methods improve DML, \lec{due to the absence of data points, sampling embeddings that are often with ``missing embedding'' leads to inaccurate sampling, thereby degenerating the final performance.} \lec{In this paper, we propose a \shortname scheme to produce \lec{embeddings with no data points by exploiting embeddings' nearby embedding space} to achieve effective DML.}

\noindent\textbf{Loss Functions for DML.} \lec{Studies on DML losses} can be grouped into two categories: pair-based and proxy-based. The pair-based losses~\cite{hadsell2006dimensionality,hu2014discriminative,kim2019deep,schroff2015facenet,sohn2016improved,wang2014learning,wang2019ranked,wang2019multi,wu2017sampling,yu2019deep} are constructed upon the pairwise distance between embeddings. Although pair-based approaches mine the rich information among vast embedding pairs, they typically encounter the sampling effective embedding pairs issues. Instead, proxy-based~\cite{aziere2019ensemble,deng2019arcface,kim2020proxy,liu2017sphereface,movshovitz2017no,qian2019softtriple,zhai2018classification} losses introduce the concept of ``proxy'' as a class representation and avoid the sampling issue by optimizing the embedding close to its proxy. \revision{However, proxy-based methods are very difficult to train when the number of classes is extremely large~\cite{oh2016deep}, limiting their applicability to real-life scenarios. Thus, in this paper, we focus on developing an effective technique to alleviates the general data sampling issue for pair-based methods that have wider applicability and delivers boosted performance for them.}



\noindent\textbf{Pseudo Embedding Generation}. \lec{Methods on synthesizing pseudo embedding have been recently shown as an important technique to improve DML~\cite{duan2018deep,ko2020embedding,lin2018deep,wang2020cross,zhao2018adversarial,zheng2019hardness}.}
\crrevision{DAML~\cite{duan2018deep} uses an additional generative adversarial network to generates only hard negatives to improve the model training.
HDML~\cite{zhao2018adversarial} leverages the {inter-class} information for embedding generation on the sampled embeddings.
DVML~\cite{lin2018deep} apply extra generator and decoder to model the class centers that may have inaccurate distance estimation to the real embeddings and employs them to generate embeddings.}
Embedding expansion~\cite{ko2020embedding} linearly interpolates the embeddings to obtain more embeddings. Cross batch memory~\cite{wang2020cross} stores embeddings from previous batches and considers them beneficial for the upcoming sampling process. However these approaches~\cite{ko2020embedding,wang2020cross} suffer from \crrevision{the following} limitations: \textbf{First}, the generated embeddings can only be considered as negative during sampling, which may \lec{limit the power of them}. \textbf{Second}, the sampling or loss computation process need to be modified to dock with them, \crrevision{limiting their applicability.} \crrevision{\textbf{Third}, an additional sub-network is introduced in order to generate embeddings, which brings heavy computation cost. \textbf{Last}, information source that may have inaccurate distance measurement to real embeddings such as class centers and inter-class differences are considered to produce embeddings.} \crrevision{Different from} them, \shortname \crrevision{is a light-weight module, which produces embeddings by densely sampling around the ``anchor'' and serves as a plug-and-play component to facilitate the sampling process in standard DML.}


\noindent\textbf{Data Augmentation.} \lec{Augmentation in data space such as image~\cite{shorten2019survey} has been widely studied and is considered an important technique to avoid overfitting.} Recently, many efforts have been made to design effective augmentation methods~\cite{devries2017dataset,chu2020feature,volpi2018adversarial,wang2019implicit,yin2019feature} in feature space, aiming to provide more features for training when the source data (\eg~images) is scarce. These methods often introduce complicated training processes~\cite{chu2020feature,volpi2018adversarial}. \revision{\crrevision{Wang~\etal~\cite{wang2019implicit} propose ISDA that} estimates semantic differences with covariance matrices and develops an improved version of cross-entropy loss. The computation and memory consumption of covariance matrices are \crrevision{much} heavier than \shortname. Moreover, \crrevision{ISDA can not be trivially extended to pair-based DML loss.} \crrevision{Yin~\etal~\cite{yin2019feature} introduce FTL that} requires extra networks (\eg~decoder and feature transfer module) and a carefully designed bi-stage training strategy to achieve feature translation, which is less efficient and general than \shortname.} Unlike these methods, we view the feature space augmentation as an approach to fill the ``missing \revision{embedding}'' in the \revision{embedding} space and propose a simpler solution under the context of DML.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{imgs/frameworkV8.pdf}
    \caption{\lec{Illustration of our \name (\shortname) scheme, which leverages two modules to exploit \revision{anchors' (\ie~embeddings with data points)} nearby embedding space to densely produce embeddings without data points:} \shortbfs performs random scaling on the discriminative features to \lec{produce embeddings} around a single embedding; \shorttmb \lec{exploits the embedding space among multiple embeddings by} adding the intra-class semantic differences to embeddings of the same class to \lec{produce embeddings.} With \shortname, we \lec{alleviate the ``missing embedding'' issue by providing more embeddings for sampling, thereby, achieving effective DML\fullstop} 
    }
    \label{fig:overall}
\end{figure*}

\section{\name}

\crrevision{\lec{In this paper, we seek to improve DML by alleviating the ``missing embedding'' issue incurred in sampling.} The overall process of \shortname scheme is shown in Fig.~\ref{fig:overall}.}

\noindent\textbf{Notation}. Let  be an embedding \lec{with data point , where}  is a deep model (\eg~CNNs) with learnable parameters . Let  be the label of the embedding . \lec{Let  denote embedding without data point.} Following previous methods~\cite{schroff2015facenet}, we normalize both  and  to the -dimensional hyper-sphere (\ie~). We omit the normalization process for brevity. Let  denote the number of training classes.

\subsection{Problem Definition and Motivation}
\label{sec:overall}
\lec{DML seeks to learn a deep model that keeps similar data points close, and vice versa. Formally, we define the distance between two embeddings as follows~\cite{wu2017sampling}:

where  denotes the  norm. For any positive pair of embeddings (), the distance should be small; Whilst for negative pair (), it should be large. In practice, limited by computing resources, it is infeasible to optimize every element in . Therefore, it is necessary to sample effective embedding pair for the objective construction (take the contrastive loss as an example):

where  is the indication function,  is a margin,  indicates indexes of the sampled embedding \revision{pairs} and  denotes some sampling function.
}

\ice{Without causing ambiguity, we denote the points on the embedding space as ``\textbf{anchor points}'', based on which we will conduct sampling \lec{to train the deep model}. Note that each \lec{data} point shall have an anchor point on the embedding space. However, due to the absence of training data, the embedding space may have a lot of ``barren area''.}
\lec{
Thus, it is very difficult to provide sufficient anchor points for sampling and learn a deep model with good performance. To this end, we propose to densely produce anchor points with no data points to facilitate the sampling, \revision{thereby improving the training} in DML. 
}


Specifically, we \revision{first} propose to exploit the embedding space around a single embedding by enhancing or weakening its semantics. We term this process as semantic scaling.
\revision{Second}, we propose to add (\ie~shift) intra-class differences (\ie~transformations) to embeddings to exploit the embedding space among multiple embeddings. We term this process as semantic shifting. Based on the above analyses, the formulation of \shortname scheme is formulated as

where  and  are embeddings with and without data points, respectively.  denotes the Hadamard product.  are semantic scaling and semantic shifting factors, respectively. Moreover, given a set of semantic scaling and shifting factor pairs , we are able to produce a set of embeddings by


\crrevision{Therefore,} the semantic scaling and shifting factors is essential to the quality of the \lec{produced} embeddings and we propose \shortbfs and \shorttmb to \crrevision{acquire} them:

where \shortbfs and \shorttmb take embeddings from the same class as input and produce the semantic scaling and shifting factors, respectively.

\subsection{\bfs}
\label{sec:bfs}

\begin{figure*}[!h]
    \centering
    \begin{minipage}[t]{.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/bfsV10.pdf}
        \caption{Illustration of the proposed \bfs (\shortbfs) module, which identifies the discriminative features (\eg~channels) and applies different random scaling to them to produce embeddings around a single embedding\fullstop 
}
        \label{fig:bfs}
    \end{minipage}
    \hskip 0.08in
    \begin{minipage}[t]{.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/tmbV8.pdf}
        \caption{Illustration of the proposed \tmb (\shorttmb) module. \shorttmb adds the intra-class transformations to embeddings of the same class to \lec{produce embeddings among multiple embeddings\fullstop}}
        \label{fig:tmb}
    \end{minipage}
\end{figure*}



We seek to obtain effective semantic scaling factors to \lec{produce anchor points} around a single anchor. Thus, as shown in Fig.~\ref{fig:bfs}, our \bfs (\shortbfs) module carries out the semantic scaling mechanism by finding the discriminative features in an embedding and applying random scaling on them. The feasibility of \shortbfs comes from two aspects. First, visual attributes can be predicted reliably using a sparse number of neurons from CNNs~\cite{escorcia2015relationship}. Second, in CNNs, neurons that match a diverse set of object concepts are highly activated~\cite{bau2017network,bau2020understanding}. In practice, the embedding typically occupies a high dimensional embedding space, which contains semantics for all training classes and semantics for each class are diverse. Thus, the semantics for one class are more likely to be noise for another. In this sense, it is very important to find out the effective \eg~discriminative features for each class in order to perform semantic scaling. To this end, we propose to identify the effective semantics by counting the number of occurrences of the highly-activated neurons for each class.

To be specific, we first initialize a Frequency Recorder Matrix (FRM)  as a zero matrix. Then, given a set of embeddings  from class , we update  as follows:

where  is an operator to select the top- elements from the vector . During training, we constantly update the FRM by recording the position of highly-activated neurons from the embeddings of the same class. In this way, FRM\footnote{Visualization of the frequency recorder matrix is in the supplementary.} serves as a stable, accurate \crrevision{and} effective semantics identifier for \crrevision{training} classes. Given the FRM, we compute the
class-wise binary channel mask  by

Given an embedding , we attain the semantic scaling factor by

where  and  and  is a hyper-parameter to be set. \revision{Note that we only randomly scale the discriminative features while leaving the indiscriminative ones intact.} To produce more than one scaling factor, we \revision{repeatedly} sample  from the uniform distribution. 

\subsection{\tmb}
\label{sec:tmb}



\lec{To produce anchors without data points among multiple anchors, we propose a module to provide effective semantic shifting factors. As shown in Fig.~\ref{fig:tmb}, our \tmb (\shorttmb) module}
exploit intra-class embeddings' nearby embedding space by leveraging the differences between embeddings and adding them to other embeddings of the same class. On the basis of that semantic differences of embeddings \lec{can be added to other embedding to generate effective embeddings}~\cite{lin2018deep}, the motivation of our \shorttmb comes from the semantic relations of word embedding~\cite{mikolov2013linguistic}: \crrevision{\textit{}}, where a ``woman'' pluses ``royal'' semantics (\eg~transformation) becomes a ``Queen''.


The transformations from both inter-class and intra-class embeddings are candidates for our design choices. However, 
the majority of the inter-class transformations typically are not transferable due to large inter-class differences. Thus, we only consider intra-class embeddings to attain the transformations. As suggested by the latest research~\cite{roth2020revisiting}, sampling only two images for each class in a batch consistently achieves good performance, \lec{which} leads to very limited transformations we can obtain in one batch \ie~two transformations. To address this issue, we \lec{construct} a bank to memorize the transformations from previous iterations to ensure the diversity of the transformations. Specifically, we construct a transformation bank , where  is the bank capacity for each class. Then, during training, once we obtain a set of embeddings  from the class , we calculate the transformations between them by

Then, the transformations are \crrevision{en-queued} into  according to the \crrevision{FIFO} principle to ensure that the transformations in the bank are in a relatively fresh state:

Note that  is reset to 1 when it reaches . Finally, with the assistance of the bank , given an embedding v, we retrieve the semantic shifting factor as follow

 is a hyper-parameter. Multiple shifting factors \crrevision{are formed by repeat sampling.}

\begin{algorithm}[t]
    \footnotesize{
    \caption{Training method of \shortname-based DML}
    \label{alg:esc_based_dml}
\begin{algorithmic}[1]
    \REQUIRE
    Training image-label pairs ;
    the embedding function ;
number of embeddings to produce ;
    number of training classes ;
    transformation bank capacity ; 
    learning rate . \\
    \ENSURE
    Optimized embedding function . \\
    \STATE Initialize  from ImageNet pretrained model. 
    
    \STATE Initialize the frequency recorder matrix . \\
    \STATE Initialize the transformation bank . \\
    
    \WHILE{not converge}
        \STATE Obtain a batch image-label pairs  from .
        \STATE Compute embeddings .
        
        \STATE // \textbf{\emph{{perform semantic scaling \crrevision{by \bfs}}}}
        \STATE Update the frequency recorder matrix  by Eqn.~(\ref{eqn:frm}). \\
        \STATE Acquire semantic scaling factors  by Eqn.~(\ref{eqn:scaling_factors}). \\
        \STATE // \textbf{\emph{{perform semantic shifting \crrevision{by \tmb}}}}
        \STATE Obtain intra-class transformations  by Eqn.~(\ref{eqn:transformation}). \\
        \STATE Update the transformation bank  by Eqn.~(\ref{eqn:fifo}). \\
        \STATE Attain semantic shifting factors  by Eqn.~(\ref{eqn:shifting}). \\
        \STATE // \textbf{\emph{{perform \lowername}}}
        \STATE Produce embeddings  by Eqn.~(\ref{eqn:esc_multi}). \\
        
\STATE Sample positive and negative embedding sets. \\

        \STATE Compute the training loss  by Eqn.~(\ref{eqn:loss}). \\
        \STATE Update the parameters  by . \\
\ENDWHILE
    \end{algorithmic}
    }
\end{algorithm}

\subsection{DML with \name}
The overall algorithm of \lec{integrating \shortname into DML} is detailed in Algorithm~\ref{alg:esc_based_dml}. Given \crrevision{anchor}-label pairs , we produce embedding-label pairs  with no data points by \shortname scheme, where \crrevision{ since the class semantic is preserved.}
Then, embedding-label pairs with or without data points are fed into the sampling module to obtain the positive and negative embedding sets (\eg~pairs, triplets, \etc~\crrevision{specified by} the \crrevision{sampling} and loss functions): . \crrevision{Last}, given a DML loss function \footnote{
See supplementary for detailed DML sampling methods and loss functions.
}, \shortname-based DML objective function is formulated as:






\begin{table*}[!ht]\small
    {
    \caption{Comparisons with \crrevision{SoTA} methods \crrevision{on} CUB, CARS and SOP. The best results are in \textbf{bold}. ~indicates the reimplementation by~\cite{ko2021learning}. \dag~denotes our reimplementation\fullstop
	\label{tab:sota}
	}
    \resizebox{\linewidth}{!}{
	\begin{tabular}{lc|cccc|cccc|cccc}
\toprule
		\multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & \multicolumn{4}{c|}{CUB} & \multicolumn{4}{c|}{CARS} & \multicolumn{4}{c}{SOP} \\ \cline{3-14} & & R@1 & R@2 & R@4 & R@8 & R@1 & R@2 & R@4 & R@8 & R@1 & R@10 & R@100 & R@1000  \\
\midrule
		Margin~\cite{wu2017sampling} &  & 63.60 & 74.40 & 83.10 & 90.00 & 79.60 & 86.50 & 91.90 & 95.10 & 72.70 & 86.20 & 93.80 & 98.00 \\
        HDC~\cite{yuan2017hard} &  & 53.60 & 65.70 & 77.00 & 85.60 & 73.70 & 83.20 & 89.50 & 93.80 & 69.50 & 84.40 & 92.80 & 97.70 \\
        A-BIER~\cite{opitz2018deep} &  & 57.50 & 68.70 & 78.30 & 86.20 & 82.00 & 89.00 & 93.20 & 96.10 & 74.20 & 86.90 & 94.00 & 97.80 \\
        ABE~\cite{kim2018attention} &  & 60.60 & 71.50 & 79.80 & 87.40 & 85.20 & 90.50 & 94.00 & 96.10 & 76.30 & 88.40 & 94.80 & 98.20 \\
        HTL~\cite{ge2018deep} &  & 57.10 & 68.80 & 78.70 & 86.50 & 81.40 & 88.00 & 92.70 & 95.70 & 74.80 & 88.30 & 94.80 & 98.40 \\
        RLL-H~\cite{wang2019ranked} &  & 57.40 & 69.70 & 79.20 & 86.90 & 74.00 & 83.60 & 90.10 & 94.10 & 76.10 & 89.10 & 95.40 & \na \\
        SoftTriple~\cite{qian2019softtriple} &  & 65.40 & 76.40 & 84.50 & 90.40 & 84.50 & 90.70 & 94.50 & 96.90 & 78.30 & 90.30 & 95.90 & \na \\
        MS~\cite{wang2019multi} &  & 65.70 & 77.00 & 86.30 & 91.20 & 84.10 & 90.40 & 94.00 & 96.50 & 78.20 & 90.50 & 96.00 & 98.70 \\
        ProxyGML~\cite{zhu2020fewer} &  & 66.60 & 77.60 & 86.40 & \na & 85.50 & 91.80 & 95.30 & \na & 78.00 & 90.60 & 96.20 & \na \\
        ProxyAnchor~\cite{kim2020proxy} &  & 68.40 & 79.20 & 86.80 & 91.60 & 86.10 & 91.70 & 95.00 & 97.30 & 79.10 & 90.80 & 96.20 & 98.70 \\
        \midrule
Contrastive + XBM~\cite{wang2020cross} &  & 65.80 & 75.90 & 84.00 & 89.90 & 82.00 & 88.70 & 93.10 & 96.10 & 79.50 & 90.80 & 96.10 & 98.70 \\
        MS~\cite{wang2019multi} &  & 64.50 & 76.20 & 84.60 & 90.50 & 82.10 & 88.80 & 93.20 & 96.10 & 76.30 & 89.70 & 96.00 & 98.80 \\
        MS + EE~\cite{ko2020embedding} &  & 65.10 & 76.80 & 86.10 & 91.00 & 82.70 & 89.20 & 93.80 & 96.40 & 77.00 & 89.50 & 96.00 & 98.80 \\
        ProxyAnchor + MemVir~\cite{ko2021learning} &  & 69.00 & 79.20 & 86.80 & 91.60 & 86.70 & 92.00 & 95.20 & 97.40 & 79.70 & 91.00 & 96.30 & 98.60 \\
\midrule
        MS~\cite{wang2019multi} &  & 65.72 & 77.19 & 85.74 & 91.56 & 83.86 & 90.41 & 94.64 & 96.99 & 76.89 & 89.58 & 95.59 & 98.60 \\
        MS + \shortname() (Ours) &  & 67.07 & 78.11 & 86.43 & 91.88 & 85.66 & 91.60 & 95.27 & 97.37 & 78.16 & 90.26 & 95.99 & 98.76 \\
        MS~\cite{wang2019multi} &  & 66.46 & 77.28 & 85.85 & 91.69 & 83.99 & 90.39 & 94.51 & 96.80 & 79.53 & 91.06 & 96.30 & 98.83 \\
        MS + \shortname() (Ours) &  & \textbf{69.19} & \textbf{79.25} & \textbf{87.09} & \textbf{92.62} & \textbf{87.84} & \textbf{93.15} & \textbf{95.99} & \textbf{97.85} & \textbf{80.59} & \textbf{91.80} & \textbf{96.68} & \textbf{98.95} \\
\bottomrule
	\end{tabular}
	}
	}
	\centering
	\vskip 0.08in
\end{table*}

\section{Experiments}

\noindent\textbf{Datasets.} We \crrevision{use} three \crrevision{popular} benchmarks: 1) {CUB2011-200 (CUB)}~\cite{wah2011caltech}, a fine-grained bird dataset with the first 100 categories for training and another 100 categories for testing. 2) {CARS196 (CARS)}~\cite{krause20133d}, a fine-grained vehicle dataset \crrevision{with} the first 98 classes \crrevision{for training} and another 98 classes \crrevision{for testing}. 3) {Stanford Online Products (SOP)}~\cite{oh2016deep}, a large-scale online products dataset \crrevision{with} the train and test partitions as 11,318 classes and another 11,316 classes, respectively.

\noindent\textbf{Implementation details\footnote{See supplementary for more details.}} We leverage two popular backbones: ResNet50~\cite{he2016deep} () and Inception BN~\cite{ioffe2015batch} (), where their parameters are initialized from ImageNet~\cite{deng2009imagenet} pre-trained models and  denotes the embedding dimension. Note that some approaches also consider GoogleNet~\cite{szegedy2015going} () as the backbone. Here, we mainly consider the settings of .  is used as the default backbone. The embedding layer is randomly initialized. Regarding evaluation metrics, Recall at k (R@k)~\cite{jegou2010product}, Normalized Mutual Information (NMI)~\cite{schutze2008introduction} and F1 score (F1)~\cite{sohn2016improved} are used, where R@k measures the image retrieval performance while F1 and NMI measure the image clustering performance. For hyper-parameters in \shortname, we set  by default.\footnote{Experiments on hyper-parameters  are in the supplementary.} \crrevision{Our source code is publicly available at \url{https://github.com/lizhaoliu-Lec/DAS}.}


\begin{table*}[ht!]
    \centering
    \caption{Comparisons with pair-based methods on CUB, CARS and SOP. \crrevision{[S] and [D] denote semi-hard and distance weighted sampling, respectively\fullstop}}
	\label{tab:improvement}
    \resizebox{0.75\linewidth}{!}{
	\begin{tabular}{l|ccc|ccc|ccc}
\toprule
		\multirow{2}[0]{*}{Method} & \multicolumn{3}{c|}{CUB} & \multicolumn{3}{c|}{CARS} & \multicolumn{3}{c}{SOP} \\ \cline{2-10} & R@1 & F1 & NMI & R@1 & F1 & NMI & R@1 & F1 & NMI \\
		\midrule
		Triplet [S]~\cite{schroff2015facenet} & 60.25 & 32.82 & 64.64 & 74.64 & 31.98 & 63.22 & 73.51 & 33.47 & 89.33 \\
		Triplet [S] {+ \shortname} & \textbf{60.82} & \textbf{33.86} & \textbf{65.67} & \textbf{77.21} & \textbf{33.88} & \textbf{64.84} & \textbf{73.99} & \textbf{33.91} & \textbf{89.42}  \\
		\midrule
        Triplet [D]~\cite{wu2017sampling} & 62.68 & 36.39 & 67.03 & 78.86 & 35.80 & 65.85 & 77.54 & 37.10 & 90.05 \\
		Triplet [D] {+ \shortname} & \textbf{64.28} & \textbf{38.16} & \textbf{68.06} & \textbf{82.63} & \textbf{39.14} & \textbf{68.12} & \textbf{77.95} & \textbf{37.64} & \textbf{90.18} \\
		\midrule
		Contrastive [D]~\cite{wu2017sampling} & 61.65 & 35.23 & 66.58 & 76.03 & 32.77 & 64.09 & 73.13 & 35.60 & 89.78 \\
		Contrastive [D] {+ \shortname} & \textbf{63.67} & \textbf{36.25} & \textbf{67.15} &
		\textbf{80.74} & \textbf{36.07} & \textbf{65.93} & \textbf{74.80} & \textbf{36.21} & \textbf{89.89} \\
		\midrule
		Margin~\cite{wu2017sampling} & 62.61 & 37.33 & 67.58 &
		80.10 & 37.85 & 67.15 & 78.69 & 39.20 & 90.50 \\
		Margin {+ \shortname} & \textbf{64.50} & \textbf{37.86} & \textbf{68.04} &
		\textbf{82.29} & \textbf{38.22} & \textbf{67.94} & \textbf{79.14} & \textbf{39.52} & \textbf{90.56} \\ \midrule
		GenLifted~\cite{hermans2017defense} & 58.81 & 34.64 & 65.50 & 72.45 & 32.43 & 64.00 & 76.18 & 37.26 & 90.13 \\
		GenLifted {+ \shortname} & \textbf{59.94} & \textbf{35.09} & \textbf{66.07} &
		\textbf{73.55} & \textbf{32.85} & \textbf{64.11} & \textbf{76.92} & \textbf{37.64} & \textbf{90.21} \\
		\midrule
		N-Pair~\cite{sohn2016improved} & 60.55 & 36.94 & 67.19 & 77.35 & 36.26 & 66.74 & 77.71 & 37.13 & 90.15 \\
		N-Pair {+ \shortname} & \textbf{62.81} & \textbf{38.37} & \textbf{68.43} & \textbf{79.93} & \textbf{38.06} & \textbf{68.20} & \textbf{77.98} & \textbf{37.82} & \textbf{90.28} \\
		\midrule
		MS~\cite{wang2019multi} & 62.63 & 38.88 & 68.19 & 82.04 & 40.85 & 69.45 & 78.89 & 37.53 & 90.12 \\
		MS {+ \shortname} & \textbf{64.13} & \textbf{39.18} & \textbf{69.08} &
		\textbf{83.31} & \textbf{42.78} & \textbf{70.77} & \textbf{79.44} & \textbf{38.77} & \textbf{90.40} \\
    \bottomrule
	\end{tabular}
	}
\end{table*}

\subsection{Comparison with State-of-the-arts}
\label{sec:exp_sota}

In this section, we compare our method with state-of-the-art competitors to investigate the effectiveness of \shortname. The results are shown in Table~\ref{tab:sota}. For fair comparisons, the results of the closely related baseline EE are from the reimplementation by~\cite{ko2021learning} using the stronger  backbone ( in the original paper). Our approach is based on MS loss and achieves superior performance on all datasets and evaluation metrics. \textbf{First}, when combining the  backbone and \shortname, we are able to boost the R@1 metrics by  on CUB and  on CARS, which shows that \shortname is able to deliver more accurate image retrieval results even \lec{with higher embedding dimension (\ie~512)}. \textbf{Second}, for our closely relative opponent, EE, its improvements on MS are marginal, showing that simply performing interpolation to generate embeddings is inferior to \shortname. \textbf{Last}, even for a strong baseline, ProxyAnchor that leverages the advanced training techniques, and sophisticated loss, we still outperform it considerably.

\subsection{Effectiveness of \shortname on Pair-based Loss}
\label{sec:exp_pair}

\noindent\textbf{Quantitative results.}~To investigate the efficacy of \shortname, we conduct experiments \crrevision{with  backbone} on the widely used pair-based losses. We reimplement all baselines under the same settings for a fair comparison. The results are presented in Table~\ref{tab:improvement}. For considered pair-based losses, \shortname is able to improve their performance on both image retrieval and clustering metrics. Notably, for approaches such as GenLifted and N-Pair that leverage the whole batch of embeddings for loss computation, \shortname still improves their performance, showing its effectiveness. Last, even for a very strong baseline, MS, that considers different kinds of relationships among embedding pairs and designs sophisticated weighting mechanism, \shortname is able to greatly improve it without bells and whistles.

\noindent\textbf{Convergence analyses.}~We provide the results of training loss and test set R@1 in Fig.~\ref{fig:training} to analyze the training behaviors of \shortname.\footnote{Results on more pair-based losses are in the supplementary.} The loss curve with \shortname decreases smoother than that without \shortname, showing that \shortname provides more embeddings to facilitate sampling, thereby, stabilizing the training. Moreover, with \shortname, the training loss is higher than the baseline, one possible reason is that \shortname is able to act as a regularizer to avoid overfitting, which is consistent with the result that \shortname achieves a higher test set R@1. Similar phenomenon is also observed in other embeddings generation methods~\cite{ko2020embedding}.

\begin{figure}
    \makebox[\linewidth][c]{\begin{subfigure}
    \centering
    \includegraphics[width=.40\linewidth]{imgs/TripletD_loss.pdf}
\end{subfigure}

    \begin{subfigure}
    \centering
    \includegraphics[width=.40\linewidth]{imgs/TripletD_recall1.pdf}
\end{subfigure}}
    \caption{The training loss and test set R@1 on CARS. The sampling method and loss function are triplet loss and distance weighted sampling, respectively\fullstop}
    \label{fig:training}
\end{figure}

\begin{table*}[!h]
    \centering
    \begin{minipage}[t]{.47\linewidth}
        \centering
        \caption{Comparisons with various sampling methods on CARS\fullstop}
        \label{tab:different_sampling}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{llcccc}
\toprule
    		Method & Sampling & \shortname & R@1 & F1 & NMI \\
\midrule
    		\multirow{8}{*}{Triplet} & \multirow{2}{*}{Random} &  & 74.21 &33.41 & 64.28 \\
    		& & \checkmark & \textbf{76.79} & \textbf{35.21} & \textbf{65.49} \\ \cline{2-6} &  \multirow{2}{*}{Semi-hard~\cite{schroff2015facenet}} &  & 74.64 & 31.98 & 63.22 \\
    		& & \checkmark & \textbf{77.10} & \textbf{33.82} & \textbf{65.03} \\ \cline{2-6} & \multirow{2}{*}{Soft-hard~\cite{roth2020revisiting}} & & 79.20 & 35.55 & 66.08 \\
    		& & \checkmark & \textbf{80.54} & \textbf{37.52} & \textbf{66.77} \\ \cline{2-6} & \multirow{2}{*}{Distance~\cite{wu2017sampling}} & & 78.86 & 35.80 & 65.85 \\
    		& & \checkmark & \textbf{81.34} & \textbf{37.27} & \textbf{67.21} \\
    		\midrule
    		\multirow{4}{*}{Contrastive} & \multirow{2}{*}{Random} &  & 42.44 & 15.83 & 48.87 \\
    		& & \checkmark & \textbf{50.79} & \textbf{19.40} & \textbf{52.71} \\ \cline{2-6} & \multirow{2}{*}{Distance~\cite{wu2017sampling}} & & 76.03 & 32.77 & 64.09 \\
    		& & \checkmark & \textbf{80.70} & \textbf{35.47} & \textbf{66.01} \\
    		\bottomrule
\end{tabular}
	    }
	    
    \end{minipage}
    \hskip 0.02in
    \begin{minipage}[t]{.50\linewidth}
        \centering
        \caption{Comparisons with more related works on CARS\fullstop}
    	\label{tab:sota}
    	\vspace{0.03in}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{l|c|cccc}
            \topline
    		Method & Backbone & R@1 & R@2 & R@4 & R@8 \\
            \midline
N-Pair + HDML~\cite{zheng2021hardness} &  & 68.90 & 78.90 & 85.80 & 90.90 \\
            N-Pair + HDML-A~\cite{zheng2021hardness} &  & 81.10 & 88.80 & 93.70 & 96.70 \\
            N-Pair + \shortname (Ours) &  & \textbf{83.70} & \textbf{90.33} & \textbf{94.47} & \textbf{96.77} \\ \midline
MS + SEC~\cite{zhang2020deep} &  & 85.73 & 91.96 & 95.51 & 97.54 \\
            MS + \shortname (Ours) &  & 85.66 & 91.60 & 95.27 & 97.37 \\
            MS + SEC + \shortname (Ours) &  & \textbf{87.80} & \textbf{93.16} & \textbf{96.18} & \textbf{98.01} \\
            \midline
Margin + DiVA~\cite{milbich2020diva} &  & 83.10 & 90.00 & \na & \na \\
            Margin + \shortname (Ours) &  & \textbf{84.85} & \textbf{90.32} & {93.99} & {96.40} \\
            \midline
            ProxyNCA++~\cite{teh2020proxynca++} &  & 86.50 & 92.50 & 95.70 & 97.70 \\
            Margin + DiVA &  & 82.20 & 89.00 & \na & \na \\
            Margin + DRML~\cite{zheng2021deepiccv} &  & 73.30 & 83.00 & 89.80 & 94.40 \\
            Margin + DCML~\cite{zheng2021deepcvpr} &  & 85.20 & 91.80 & \textbf{96.00} & \textbf{98.00} \\
            Margin + \shortname (Ours) &  & \textbf{88.34} & \textbf{93.21} & {95.92} & {97.59} \\
            \bottomline
    	\end{tabular}
    	}
    \end{minipage}
    \vskip 0.08in
\end{table*}

\subsection{Effectiveness of \shortname on Sampling Method}
\label{sec:exp_sample}
In this section, we investigate the effectiveness of \shortname by evaluating it with different sampling approaches. We choose two \crrevision{popular} loss functions: triplet and contrastive losses \crrevision{that} are sensitive to the sampling methods. \crrevision{Therefore, } various sampling approaches \crrevision{are} tailored for them. The experiment results are presented in Table~\ref{tab:different_sampling}. For two loss functions, despite the choice of sampling approaches, \shortname is able to improve them considerably on both image retrieval and clustering metrics. Notably, when applying \shortname, triplet loss with random sampling outperform the one with the semi-hard sampling. This indicates that producing more embeddings for sampling is as important as the sampling approach. 


\subsection{Qualitative Results}
\label{sec:exp_qualitative}
To better understand our method, we compare contrastive loss (with distance weighted sampling) w/ or w/o \shortname and visualize image retrieval results on both CARS (Fig.~\ref{fig:cars&online_compare}) and SOP (Fig.~\ref{fig:cars&online_compare}) datasets.\footnote{More qualitative results are in the supplementary.} On CARS, the model performs better with \shortname despite the background noises or the interference from the car's color, demonstrating that \shortname enforces the model to focus on real semantics. On SOP, the model with \shortname is insensitive to drastic viewpoint changes. These results verify the generalization ability and robustness of \shortname under various scenes.

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{imgs/cars_sop_compare.pdf}
    \caption{Top 3 retrieved results from the  trained w/ or w/o \shortname. \textcolor{corrected}{green} and \textcolor{wrong}{red} rectangles indicate desired and undesired results, respectively\fullstop}
    \label{fig:cars&online_compare}
\end{figure*}

\subsection{Ablation Studies}
\label{sec:exp_ablation}
{\parindent0pt

\revision{\noindent{\textbf{Effect of \shortbfs and \shorttmb.}}} In this section, we perform ablation studies to evaluate the performance gain by each module in \shortname. The loss function and sampling method are triplet loss and distance weighted sampling, respectively. The results are in Table~\ref{tab:ablation}. First, \shortbfs, alone, boosts R@1 by +2.42\%, verifying that producing embeddings around a single embedding is able to force the model to focus on real semantics and achieve better image retrieval results. Second, with \shorttmb only, the clustering metrics are greatly improved, indicating that producing embeddings among multiple embeddings are beneficial to the image clustering task. Last, with \shortbfs and \shortname, all metrics are further improved, suggesting that \shortbfs and \shorttmb reinforce and complement each other.

\begin{table*}
    \centering
    \caption{Ablation studies on CARS\fullstop}
    \label{tab:ablation}
    \resizebox{0.55\textwidth}{!}{
        \begin{tabular}{cclll}
            \toprule
	        \shortbfs & \shorttmb & R@1 & F1 & NMI \\
            \midrule
	        &  &  78.86 & 35.80 & 65.85 \\
	        \checkmark &  &  81.28 \improve{(+2.42)} & 36.22 \improve{(+0.42)} & 66.84 \improve{(+0.99)} \\
	        & \checkmark &  81.83 \improve{(+2.97)} & 38.26 \improve{(+2.46)} & 67.81 \improve{(+1.96)} \\
	        \checkmark & \checkmark &  82.63 \improve{(+3.77)} & 39.14     \improve{(+3.34)} & 68.12 \improve{(+2.27)} \\
            \bottomrule
        \end{tabular}
    }
\end{table*}

\revision{\noindent{\textbf{Effect of  in \shortbfs.}} Multi-dimensional embeddings have a diverse set of semantic features~\cite{bau2017network,bau2020understanding}. The top-K mask is effective to discover the discriminative features.
We perform experiments on SOP, a large-scale and diverse dataset with margin loss to support our claim. With {\small{}}, we obtain results {\small{}}, where substantial improvements are observed  when  and larger  leads to worse results.} 

\revision{\noindent{\textbf{Effect of  in \shorttmb.}} 
The larger bank capacity () allows us to access intra-class transformations from current and previous iterations, which improves the diversity of the produced embeddings. We conduct experiments on CARS with margin loss by setting {\small{}} and obtain {\small{}}, showing that history transformations ({\small{}}) bring slight improvements.}
}

\subsection{Further Discussions}

\noindent{\crrevision{\textbf{More discussions on \shortbfs.} One may question whether the proposed \shortbfs requires the linear assumption on high dimensional feature space. In fact, we do not make this assumption and our method is built on a very basic hypothesis of metric learning: the embeddings that are close to each other in the embedding space have similar semantics. More critically, \shortbfs does not rely on the linearity assumption. Instead, \shortbfs is based on the observations that effective semantics for one embedding are highly activated features~\cite{bau2017network,bau2020understanding}.
}}

\noindent{\crrevision{\textbf{More discussions on \shorttmb.} Li~\etal~\cite{li2019memory} also apply a memory module is leveraged to store abundant features and conduct neighborhood search upon them to enhance the discriminative power of a general CNN feature on the image search and few-shot learning tasks. Unlike them, \shortname constructs a memory bank with the intra-class embedding transformations, which allows us to access intra-class transformations from current and previous iterations, and thus improves the diversity of produced embeddings for DML.}}

\noindent\textbf{Comparisons with more related works.} \vtars{In Table~\ref{tab:sota}, we compare \shortname with HDML~\cite{zheng2021hardness}, SEC~\cite{zhang2020deep}, DiVA~\cite{milbich2020diva}, ProxyNCA++~\cite{teh2020proxynca++}, DRML~\cite{zheng2021deepiccv}, DCML~\cite{zheng2021deepcvpr} on CARS under the same settings. We can see that \shortname outperforms HDML, DiVA, DRML and DCML, and achieves comparable performance to SEC. Note that \shortname can be also incorporated into SEC. As a result, using MS loss, SEC + \shortname outperforms SEC by 2.07\% in R@1. These results further verify the applicability of \shortname on some regularization techniques in DML.
}

\section{Conclusion}
\lec{In this paper, we propose a \name (\shortname) scheme to alleviate the ``missing embedding'' issue incurred during DML sampling. To this end, we propose to produce embeddings with no data points by exploiting the embeddings' nearby embedding space. Specifically, we propose a \shortbfs module that identifies an embedding's discriminative features and performs random scaling on them to exploit the embedding space around it. Moreover, we propose a \shorttmb module to exploit embedding space among multiple embedding by adding the intra-class semantic differences to embeddings of the same class. By combining the embeddings with and without data points, \shortname provides more embeddings for sampling to improve the sampling quality and achieve effective DML. Extensive experiments with various loss functions and sampling methods on three public available benchmarks show that \shortname is effective. In the future, we plan to apply \shortname to other areas such as self-supervised learning that require sampling.
}

\noindent{\textbf{Acknowledgements.} This work was partially supported by Peng Cheng Laboratory Research Project No. PCL2021A07, National Natural Science Foundation of China (NSFC) 62072190, Program for Guangdong Introducing Innovative and Enterpreneurial Teams 2017ZT07X183.}



{\small
\bibliographystyle{splncs04}
\bibliography{egbib.bib}
}

\clearpage
\includepdf[pages={1}]{3844-supp.pdf}
\includepdf[pages={2}]{3844-supp.pdf}
\includepdf[pages={3}]{3844-supp.pdf}
\includepdf[pages={4}]{3844-supp.pdf}
\includepdf[pages={5}]{3844-supp.pdf}
\includepdf[pages={6}]{3844-supp.pdf}
\includepdf[pages={7}]{3844-supp.pdf}
\includepdf[pages={8}]{3844-supp.pdf}
\includepdf[pages={9}]{3844-supp.pdf}
\includepdf[pages={10}]{3844-supp.pdf}
\includepdf[pages={11}]{3844-supp.pdf}
\includepdf[pages={12}]{3844-supp.pdf}
\includepdf[pages={13}]{3844-supp.pdf}
\includepdf[pages={14}]{3844-supp.pdf}

\end{document}
