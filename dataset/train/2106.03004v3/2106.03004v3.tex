\documentclass{article}





\usepackage[preprint]{neurips_2021}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{xcolor} 
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\usepackage[colorlinks,citecolor=mydarkblue,urlcolor=mydarkblue,linkcolor=mydarkblue,filecolor=mydarkblue]{hyperref}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{multirow}
\usepackage{graphicx}  
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage[textsize=scriptsize,textwidth=2.2cm]{todonotes}
\usepackage{etoolbox}
\usepackage{enumitem}
\usepackage[hang,flushmargin]{footmisc} \usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphics}
\usepackage{caption}



\newtoggle{showtodos}
\togglefalse{showtodos} 



\usepackage{amsmath,amsfonts,bm}

\newcommand{\D}{\mathcal{D}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\ytilde}{\tilde{y}}
\newcommand{\vxtilde}{\tilde{\vx}}
\newcommand{\data}{\{\vx_n, y_n\}_{n=1}^N}
\newcommand{\map}{\mathsf{MAP}}
\newcommand{\wmap}{\mathsf{WMAP}}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\iftoggle{showtodos}{
\newcommand{\jie}[1]{\todo[color=green!40]{Jie: #1}}
\newcommand{\stan}[1]{\todo[color=blue!40]{Stan: #1}}
\newcommand{\balaji}[1]{\todo[color=red!20]{Balaji: #1}}
\newcommand{\todoinline}[1]{\todo[inline,color=red!20]{#1}}
}{
\newcommand{\jie}[1]{}
\newcommand{\stan}[1]{}
\newcommand{\balaji}[1]{}
\newcommand{\todoinline}[1]{}
}

 \usepackage{caption}
\usepackage{subcaption}


\title{
Exploring the Limits of Out-of-Distribution Detection }



\newcommand{\email}[1]{{\href{mailto:#1}{\url{#1}} }} 

\author{Stanislav Fort\thanks{Equal contribution.}
\\
Stanford University\\
\email{sfort1@stanford.edu} \\
\And
Jie Ren\footnotemark[1] \\
Google Research, Brain Team \\
\email{jjren@google.com}
\And
Balaji Lakshminarayanan
\\
Google Research, Brain Team \\
\email{balajiln@google.com}
}



\begin{document}

\maketitle
\vspace{-1em}
\begin{abstract}
Near out-of-distribution detection (OOD) is a major challenge for deep neural networks. We demonstrate that 
large-scale pre-trained transformers can significantly improve the state-of-the-art (SOTA) on a range of near OOD tasks across different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD detection, we improve the AUROC from 85\% (current SOTA) to 96\% using Vision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD detection benchmark, we improve the AUROC from 66\% to 77\% using transformers and unsupervised pre-training. 
To further improve performance, we explore the few-shot outlier exposure setting where a few examples from outlier classes may be available; we show that  
pre-trained transformers are particularly well-suited for outlier exposure, and that the AUROC of OOD detection on CIFAR-100 vs CIFAR-10  can be improved to 98.7\% with just 1 image per OOD class, and 99.46\% with 10 images per OOD class. 
For multi-modal image-text pre-trained transformers such as CLIP, we explore a new way of using just the names of outlier classes as a sole source of information without any accompanying images, and show that this outperforms previous SOTA on standard vision OOD benchmark tasks. 
 


\end{abstract}

\vspace{-1em}
\section{Introduction}

Deep neural networks are increasingly used in high-stakes applications such as healthcare \citep{roy2021does,ren2019likelihood}.  Safe deployment of models requires that models not only be accurate but also be robust to distribution shift \citep{amodei2016concrete}. Neural networks can assign high-confidence predictions to mis-classified inputs \citep{guo2017calibration,lakshminarayanan2017simple} as well as test inputs that do not belong  to one of the training classes \citep{nguyen2015deep}. This motivates the need for methods that can reliably detect out-of-distribution (OOD) inputs. 
There has been a lot of progress in detecting OOD inputs including methods based on discriminative models \citep{hendrycks2016baseline,lee2018simple,liang2017enhancing,liu2020simple} as well as methods based on deep generative models  \citep{nalisnick2019hybrid,zhang2020hybrid}. \begin{figure}[ht]
	\centering
	\includegraphics[width=0.32\linewidth]{figures/section_resnet.pdf}
	\includegraphics[width=0.32\linewidth]{figures/section_vit_pretrain_only.pdf}
	\includegraphics[width=0.32\linewidth]{figures/section_vit_finetuned.pdf}
	\caption{A two-dimensional PCA projection of the space of embedding vectors for 3 models, with examples of 2 in-distribution (from CIFAR-100) and 1 out-of-distribution class (from CIFAR-10). The color coding shows the Mahalanobis outlier score, while the points are projections of embeddings of members of the in-distribution CIFAR-100 classes "sunflowers" (black plus signs) and "turtle" (yellow crosses), and the OOD CIFAR-10 class "automobile" (red circles). The left panel shows a ResNet-20 trained on CIFAR-100, which assigns low Mahalanobis distance to OOD inputs and leads to overlapping clusters of class embeddings. The ViT pre-trained on ImageNet-21k (middle panel) is able to distinguish classes from each other well, but does not lead to well-separated outlier scores. ViT fine-tuned on CIFAR-100 (right panel) is great at clustering embeddings based on class, as well as assigning high Mahalanobis distance to OOD inputs (red).
	}
	\vspace{-1.5em}
	\label{fig:overview}
\end{figure}

The difficulty of the OOD detection task depends on how semantically close the outliers are to the inlier classes.
\citet{winkens2020contrastive} distinguish between \emph{near-OOD} tasks which are harder and  \emph{far-OOD} tasks which are easier, as evidenced by the difference in state-of-the-art (SOTA) for area under the receiver operating characteristic curve (AUROC). For instance, for a model trained on CIFAR-100 (which consists of classes such as mammals, fish, flowers, fruits, household devices, trees, vehicles, insects, etc),  a far-OOD  task would be detecting digits from the street-view house numbers (SVHN) dataset as outliers.  For the same model, detecting images from  the CIFAR-10 dataset (which consists of the following 10 classes: airplane,	automobile,  
bird, cat, 	deer, dog, 	frog, horse, ship, truck) would be considered a near-OOD task, which is more difficult as the classes are semantically similar. 
There has been impressive progress on far-OOD detection, for instance there are several approaches which can achieve AUROC close to 99\%  on CIFAR-100 (in) vs SVHN (out) task, cf.~\citep{sastry2020detecting}. However, the state-of-the-art for near-OOD detection is much lower, for instance the SOTA AUROC for CIFAR-100 (in) vs CIFAR-10 (out) task is around 85\% \citep{zhang2020hybrid} which is considerably lower than the SOTA for far-OOD tasks. Similar trends are observed in other modalities such as genomics where the SOTA AUROC of near-OOD detection is only 66\% \citep{ren2019likelihood}. 
Improving the SOTA for these near-OOD detection tasks and closing the performance gap between near-OOD detection and far-OOD detection is one of the key challenges in ensuring the safe deployment of models.

 

Large-scale pre-trained transformers have led to significant accuracy improvements in multiple domains, cf.~Bidirectional Encoder Representations from Transformers (BERT) for text \citep{devlin2018bert}, Vision Transformers (ViT) for images \citep{dosovitskiy2020image},  Contrastive Language?Image Pre-training (CLIP) trained on image-text pairs \citep{clip}.   
We show that classifiers obtained by fine-tuning large-scale pre-trained transformers are significantly better at near-OOD detection. Intuitively, large-scale pre-training makes classifiers less vulnerable to \emph{shortcut learning}  \citep{geirhos2020shortcut}, making these representations better suited for near-OOD detection. 
Figure~\ref{fig:overview} visualizes two-dimensional PCA projections of representations from residual networks (ResNet) \citep{he2016deep} trained on CIFAR-100 and ViT model pre-trained on ImageNet-21k and fine-tuned on CIFAR-100; we can observe that representations obtained by fine-tuning pre-trained transformers are better suited at near-OOD detection than representations from ResNet just trained on CIFAR-100.  


Motivated by real-world applications which demand very high level of OOD detection for safe deployment, we explore variants of outlier exposure to further improve OOD detection.  
We show that pre-trained transformers are particularly well-suited at leveraging known outliers due to their high-quality representations (see Figure~\ref{fig:overview}). We systematically vary the number of outlier examples per class, and show that even a handful of known outliers can significantly improve OOD detection. We refer to this setting as \emph{few-shot outlier exposure}. 
For multi-modal pre-trained transformers, we explore a new form of outlier exposure that leverages names of outlier classes without any accompanying images, and show that this can significantly improve OOD detection for zero-shot classification. 









In summary, our contributions are the following:
\begin{itemize}[leftmargin=1em,itemsep=0em]
    \item We show that pre-trained transformers lead to significant improvements on  near-OOD benchmarks. Concretely, we improve the AUROC of OOD detection on CIFAR-100 vs CIFAR-10 from 85\% (current SOTA) to 96\% using ViT pre-trained on ImageNet-21k, and improve the AUROC on a genomics OOD detection benchmark from 66\% (current SOTA) to 77\% using BERT. 
\item  We show that pre-trained transformers are well-suited for few-shot outlier exposure. With just 10 labeled examples per class, we can  improve the AUROC of OOD detection on CIFAR-100 vs  CIFAR-10 to 99\%, and improve the AUROC of OOD detection on genomics to 86\%. 
\item We explore OOD detection for pre-trained multi-modal image-text transformers in the zero-shot classification setting, and show that just using the names of outlier classes as candidate text labels for CLIP, we can achieve AUROC of 94.8\% on CIFAR-100 vs CIFAR-10 task. On easier far-OOD tasks such as CIFAR- vs SVHN, we achieve AUROC of 99.6\% and 99.9\% respectively.
\end{itemize}






\section{Background and Related work}

\vspace{-0.5em}
\paragraph{Notation} 
We assume that we have an in-distribution dataset  of  pairs where  denotes the input feature vector, and  denotes the class label. 
Let  denote an out-of-distribution dataset of  pairs  where . 
Depending on how different   is from , we categorize the OOD detection tasks into near-OOD and far-OOD.
We first study the scenario where the model is fine-tuned only on the training set   without any access to OOD data. The test set contains  and  for evaluating OOD performance using AUROC.
Next, we explore the scenario where a small number of OOD examples are available for training, i.e. the few-shot outlier exposure setting. 
In this setting, the training set contains , where  is often smaller than 100 per OOD class.
\vspace{-0.5em}
\subsection{Methods for detecting OOD inputs}
\vspace{-0.5em}
We describe a few popular techniques for detecting OOD inputs using neural networks.
\vspace{-0.5em}
\paragraph{Maximum over softmax  probabilities (MSP)}
A baseline method for OOD detection is to use the maximum softmax probability as the confidence score, i.e.  \citep{hendrycks2016baseline}. 
While being slightly worse than other techniques, its simplicity and performance make it an ideal baseline.
\vspace{-0.5em}





\paragraph{Mahalanobis distance}
\citet{lee2018simple} proposed to fit a Gaussian distribution to the class-conditional embeddings and use the Mahalanobis distance for OOD detection. 
Let  denote the embedding (e.g. the penultimate layer before computing the logits) of an input .
We fit a Gaussian distribution to the embeddings, computing per-class mean 

and a shared covariance matrix 
The Mahalanobis score (negative of the distance) is then computed as:  






\vspace{-1em}
\paragraph{Outlier exposure}
\citet{hendrycks2018deep} proposed \emph{outlier exposure} which leverages a large dataset of known outliers.  For classification problems, the model is trained to predict uniform distribution over labels for these inputs. \citet{thulasidasan2021a} proposed to use a single outlier class as the  class for a -way classification problem. 
\citet{roy2021does} showed that leveraging the labels of known outliers (rather than assigning all known outliers to a single class) can further improve OOD detection performance. 






\vspace{-0.5em}
\subsection{Pre-training neural networks}

Architectures using self-attention, typically based on the Transformer \citep{vaswani2017attention}, often combined with large-scale pre-training directly on raw text, have been very popular for natural language processing (NLP) tasks in recent years \citep{devlin2018bert, NIPS2015_7137debd, peters2018deep, Howard_2018, radford2018improving, raffel2019exploring}. Often followed by fine-tuning on a smaller, downstream dataset, large-scale pre-training techniques lead to highly informative embeddings that are broadly useful in natural language tasks. The advantage of the transformer architecture is its ability to scale to very large model sizes, reaching up to  1 trillion parameter mark \citep{fedus2021switch}. Large pre-trained models, such as  GPT-3 \citep{brown2020language}, have shown the potential of large-scale task-agnostic pre-training in language. 

The Vision Transformer (ViT) \citep{dosovitskiy2020image} has shown that self-attention combined with large-scale pre-training is a viable strategy for vision tasks as well. The performance of ViT is comparable to other state-of-the-art models, while being more efficient to train. Its ability to quickly fine-tune to a smaller, downstream dataset, generalizing even in a few-shot regime, makes it an attractive backbone for tasks such as out-of-distribution (OOD) detection. In this paper, we use ViT pre-trained on ImageNet-21k \citep{ridnik2021imagenet21k}.






Multi-modal text-image transformers such as CLIP (Contrastive Language-Image Pre-Training) \citep{clip} pre-train on 400 million (image, text) pairs from the internet to learn to predict a raw text caption from an image, and by doing so develop state-of-the-art visual representations and their natural language counterparts. \citet{clip} showed that CLIP improves robustness to natural distribution shift. In this paper, we use the shared image-text embedding to introduce a new zero-shot OOD detection method (Section~\ref{sec:clip}). 
\citet{hendrycks2019usingpretraining} show that pre-training improves OOD detection for non-transformer architectures.  Self-supervised learning techniques have also been shown to improve OOD detection; \citet{hendrycks2019usingssl} use rotation prediction and \citet{winkens2020contrastive} use contrastive training to improve near-OOD detection.  

\paragraph{Robustness of pre-trained transformers}
\citet{hendrycks2020pretrained} show that pre-trained transformers improve OOD detection in NLP. Pre-trained BERT has been has used as a backbone for OOD detection in language, cf.~\citep{liu2020simple}.  
Unlike them, we focus on vision and genomics modalities, and specifically on near-OOD detection benchmarks.  
Investigating the robustness of pre-trained ViT is an active research area and there are several concurrent papers exploring robustness of ViT to adversarial perturbations and common corruptions (ImageNet-C).   
\citet{bhojanapalli2021understanding} show  the robustness of pre-trained transformers to input perturbations, and of transformers to layer removal. \citet{caron2021emerging} demonstrate many emerging properties in self-supervised ViTs, while \citet{shao2021adversarial, mahmood2021robustness} show that they are more robust to adversarial perturbations, and \citet{naseer2021intriguing} that they have less texture bias. \citet{paul2021vision, minderer2021revisiting} show that ViT is more robust to distribution shift and natural adversarial examples \citep{paul2021vision}, and \citet{mao2021robust} propose a robust ViT.
To the best of our knowledge, we are the first to show that pre-trained ViT can significantly improve near-OOD detection in vision benchmarks, and show that few-shot outlier exposure can further improve performance.  



\section{Near-OOD detection on image classification benchmarks}


\begin{wrapfigure}{r}{0.5\textwidth}
	\centering
	\vspace{-2em}
	\includegraphics[width=0.49\linewidth]{figures/summary_CIFAR100_to_CIFAR10_id17203610.pdf} \includegraphics[width=0.49\linewidth]{figures/summary_CIFAR10_to_CIFAR100_id8118810.pdf}
\vspace{-2em}
	\caption{Left: CIFAR-100 vs CIFAR-10 OOD AUROC for previous state-of-the-art \cite{zhang2020hybrid}, our fine-tuned ViT with two different backbones (ViT-B\_16 and R50+ViT-B\_16). 
Right: CIFAR-10 vs CIFAR-100 OOD task.}
	\label{fig:summary_plots}
	\vspace{-1em}
\end{wrapfigure}
\subsection{Fine-tuning the Vision Transformer}\label{sec:finetuned:vit}
We use the Vision Transformer (ViT) architecture \citep{dosovitskiy2020image} and its pre-trained model checkpoints.\footnote{\url{https://github.com/google-research/vision_transformer}} The checkpoints are pre-trained on ImageNet-21k \citep{5206848}. We fine-tune the full ViT architecture on a downstream task that is either the CIFAR-10 or CIFAR-100 classification problem (using a TPU in Google Colab). We found that we get better generalization and higher quality embeddings when we do not use data augmentation for fine-tuning. Once the model is fine-tuned, we get its pre-logit embeddings (the layer immediately preceding the final layer) for the train and test sets of CIFAR-10 and CIFAR-100 to use for out-of-distribution tasks. 
For OOD detection, we use the maximum over softmax probabilities (labeled as \verb|MSP|) and the  
Mahalanobis distance (labeled as \verb|Maha|).  

\begin{table}[h]
\begin{center}	
\vspace{-1em}
\caption{ImageNet-21k pre-trained ViT/BiT/MLP-Mixer fine-tuned on the in-distribution training set. }
\begin{tabular}{ c|c|c|c|c|c } 
	Model & \makecell{In-\\distribution} & \makecell{fine-tuned\\test\\accuracy} & \makecell{Out-\\distribution} & \makecell{Mahalanobis\\AUROC} & \makecell{MSP\\AUROC} \\
	\hline
	BiT-M R50x1 & CIFAR-100 & 87.01\% & CIFAR-10 & 81.71\% & 81.15\% \\ 
	BiT-M R101x3 & CIFAR-100 & 91.55\% & CIFAR-10 & 90.10\% & 83.69\% \\ 
ViT-B\_16 & CIFAR-100 & 90.95\% & CIFAR-10 & 95.53\% & 91.89\% \\
	R50+ViT-B\_16 & CIFAR-100 & 91.71\% & CIFAR-10 & \textbf{96.23}\% & 92.08\% \\
		MLP-Mixer-B\_16 & CIFAR-100 & 90.40\% & CIFAR-10 & 95.31\% & 90.22\% \\
\hline
	BiT-M R50x1 & CIFAR-10 & 97.47\% & CIFAR-100 & 95.52\% & 85.87\% \\
	BiT-M R101x3 & CIFAR-10 & 97.36\% & CIFAR-100 & 94.55\% & 85.34\% \\
	ViT-B\_16 & CIFAR-10 & 98.10\% & CIFAR-100 & 98.42\% & 97.68\% \\
	R50+ViT-B\_16 & CIFAR-10 & 98.70\% & CIFAR-100 & \textbf{98.52\%} & 97.75\% \\
		MLP-Mixer-B\_16 & CIFAR-10 & 97.58\% & CIFAR-100 & 97.85\% & 96.28\% \\
\bottomrule
\end{tabular}
\vspace{-1em}
\label{tab:ViT_fine-tuned}
\end{center}
\end{table}

The results are summarized in Table~\ref{tab:ViT_fine-tuned} and Figure~\ref{fig:summary_plots}. 
We observe that the MSP baseline yields 
surprisingly good results when used on top of a large pre-trained transformer that has been fine-tuned on the in-distribution training set. 
The Mahalanobis distance technique improves OOD detection even further. Applying Mahalanobis distance to a pre-trained ViT fine-tuned on CIFAR-100, we achieve AUROC of 96\% on CIFAR-100 vs CIFAR-10, significantly improving over the previous SOTA of 85\% using a hybrid model \citep{zhang2020hybrid}.  
To study the effect of model architecture, we also evaluate OOD  performance on another large-scale pre-trained model, Big Transfer (BiT) \citep{kolesnikov2019big}\footnote{\url{https://github.com/google-research/big\_transfer}}, as a comparison to ViT. We use the BiT-M R50x1 and R101x3 models pre-trained on ImageNet-21k, and fine-tune the full model architecture on CIFAR-10 and CIFAR-100 respectively. 
The results are shown in Table~\ref{tab:ViT_fine-tuned}. 
For both directions, the AUROCs for BiT are lower than that for ViT.
More importantly, BiT uses a different model architecture, ResNet, instead of a transformer, which may explains the large difference in the OOD performance. As an additional ablation, we fine-tuned the MLP-Mixer pre-trained on ImageNet-21k \citep{tolstikhin2021mlpmixer}\footnote{\url{https://github.com/google-research/vision_transformer}}, a high-performance all-MLP architecture for vision, and compared its performance to the Vision Transformer (ViT) and BiT. 
The summary of our results can be found in Table~\ref{tab:ViT_fine-tuned}. We observe that MLP-mixer outperforms BiT as well, which adds additional evidence that pre-training helps architectures such as ViT and MLP-mixer more than it helps BiT. 

Due to semantic similarity between classes in CIFAR, this task is hard for humans as well.\footnote{To approximately estimate the human performance, we performed the CIFAR-100 vs CIFAR-10 near-OOD detection ourselves. We set-up a simple GUI to randomly sample images from both CIFAR-100 and CIFAR-10 datasets, where the task was to identify images that belong to the CIFAR-10 classes. Note that this setup is easier for humans as they only have to remember 10 classes in CIFAR-10 (as opposed to the 100 classes in CIFAR-100). The estimated human performance was 96\% AUROC which demonstrates the difficulty of the task. We do not claim that this is the best possible human performance, we will open source the code so that readers can estimate their OOD detection performance themselves. Further details are available in Appendix~\ref{app:human}.}  
We also evaluated the performance of our approach on popular far-OOD benchmarks such as CIFAR-* vs SVHN and  CIFAR-* vs Textures, and achieve  very high AUROC values of around  or higher, see Table~\ref{tab:ViT_fine-tuned_additional_dataset} in Appendix~\ref{sec:app:additional}. We mainly focus on the difficult near-OOD as this is a more challenging and realistic problem; 
many methods can achieve high AUROC on the easier far-OOD benchmarks, but do not perform as well in near-OOD tasks, cf.~\citep[Table 1]{winkens2020contrastive} which compares many methods on near-OOD and far-OOD tasks. 

Since ViT models are typically pre-trained using a large labeled set, we ran additional ablations to understand how much of the improvement is due to supervision vs transformers.  
To assess the role of supervised pre-training, we compared the results to a ViT pre-trained on ImageNet-21k in a self-supervised way that does not use any labels. We took a pre-trained checkpoint from \citet{caron2021emerging}, and fine-tuned it on CIFAR-100. The results are shown as DINO ViT-B\_16 in  Table~\ref{tab:pretraining:ablations}. 
Since the fine-tuned test accuracy is lower for DINO ViT-B\_16, we also include an ViT-B\_16 that was fine-tuned for fewer steps to achieve comparable test accuracy as DINO ViT-B\_16.  
Note that even though DINO ViT-B\_16 is pre-trained without labels, the AUROC is significantly higher than the current SOTA for CIFAR-100 vs CIFAR-10. 
The difference between DINO ViT-B\_16 vs early stopped ViT-B\_16 shows the difference due to supervision during pre-training. 
We found that the OOD detection is lower for ViT models with lower fine-tuned test accuracy, see Appendix~\ref{sec:accuracy-vs-ood} for these results. Hence, we believe that better strategies for unsupervised pre-training and fine-tuning can further improve OOD detection performance. We also experimented with larger ViT models such as ViT-L\_16 and ensembles, and found that they improve AUROC of OOD detection to ~98\% on CIFAR-100 vs CIFAR-10 task, see Appendix~\ref{sec:accuracy-vs-ood}.  
In Section~\ref{sec:genomics}, we explore unsupervised pre-training for transformers to improve near-OOD detection in genomics. 

\begin{table}[h]
\begin{center}	
\vspace{-1em}
\caption{Additional ablations to measure the effect of supervised pre-training.  indicates self-supervised pre-training without labels.
}
\begin{tabular}{ c|c|c|c|c|c } 
	Model & \makecell{In-\\distribution} & \makecell{fine-tuned\\test\\accuracy} & \makecell{Out-\\distribution} & \makecell{Mahalanobis\\AUROC} & \makecell{MSP\\AUROC} \\
	\hline
DINO ViT-B\_16 & CIFAR-100 & 88.95\% & CIFAR-10 & 88.78\% & 81.25\% \\
	ViT-B\_16 (early stop) & CIFAR-100 & 88.71\% & CIFAR-10 & 93.05\% & 88.82\% \\
ViT-B\_16 & CIFAR-100 & 90.95\% & CIFAR-10 & 95.53\% & 91.89\% \\
	R50+ViT-B\_16 & CIFAR-100 & 91.71\% & CIFAR-10 & \textbf{96.23}\% & 92.08\% \\
\bottomrule
\end{tabular}
\vspace{-1em}
\label{tab:pretraining:ablations}
\end{center}
\end{table}














\vspace{-0.5em}
\subsection{Few-shot outlier exposure using ViT}


\vspace{-0.5em}


In Section~\ref{sec:finetuned:vit}, we demonstrated that fine-tuning a pre-trained ViT model can improve near-OOD detection (with relatively simple OOD detection techniques such as MSP and Mahalanobis distance). Figure~\ref{fig:overview} shows that the representations from fine-tuned ViT models are well-clustered. This motivates \textit{few-shot outlier exposure}, where we assume just a handful of  known outliers (and optionally their labels). This setting is motivated by real-world applications which require high-quality OOD detection and teams are willing  to collect a handful of known outlier examples (rather than just rely on modeling approaches) to improve safety. Another setting is the case where models are being continuously re-trained; once an outlier is detected, it is desirable to include that in the training corpus to encourage the model to correctly detect similar examples as outliers in the future.  


\begin{figure}[ht]
    \centering
     \vspace{-0.5em}
    \includegraphics[width=\linewidth]{figures/outlier_exposure_horizontal.pdf}
    \caption{Few-shot outlier exposure with pre-trained transformers. The OOD samples are used to fine-tune a simple classifier (linear classifier for ViT which uses supervised pre-training, and shallow MLP with one hidden layer for genomics which uses unsupervised pre-training).
    We use the in-distribution classes in addition to multiple OOD classes (when labels available), or a single OOD class. The confidence score is the sum of probabilities corresponding to the in-distribution classes. 
    }
    \vspace{-0.5em}
    \label{fig:few-shot_oe}
\end{figure}

The general approach is shown in Figure~\ref{fig:few-shot_oe}. 
By using the in-distribution training set  with  classes and a small number of known OOD examples from  with  classes, we train a simple classifier  that maps an embedding vector  to a probability vector , which concatenates the in- and out-of-distribution classes. 
We considered two types of outlier exposure, one which assumes access to outlier labels (similar to \citep{roy2021does}) and one where all the outlier examples are collapsed to a single th class (similar to \citep{thulasidasan2021a}). 
For models pre-trained with labels (such as ViT), we use a linear classifier. For models that use unsupervised pre-training (e.g.~genomics in Section~\ref{sec:genomics}), we use a shallow multi-layer perceptron (MLP) with a single hidden layer so that fine-tuning can learn discriminative features. 
We use the sum of the probabilities of all  in-distribution classes as the confidence score for the OOD detection task, 
. 
When training the MLP  with few-shot OOD examples, there could be many more examples of the in-distribution data than the small number of OOD data. We therefore oversample the OOD inputs by a factor that we calculate as . This makes the training classes approximately balanced during training. We used a single layer MLP from \verb|scikit-learn| \citep{scikit-learn}, batch size 200,  penalty of , learning rate  with \textit{Adam}, maximum of 1,000 iterations.  
Algorithm~\ref{alg:outlier_exposure_training} and Algorithm~\ref{alg:outlier_exposure_scoring} describe the details of training and scoring. 






\begin{center}
\vspace{-1em}
\scalebox{.9}{\begin{minipage}{0.625\textwidth}
\begin{algorithm}[H]
   \caption{Few-shot outlier exposure training}
   \label{alg:outlier_exposure_training}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} In-distribution train set  with  classes, out-of-distribution train subset  with  classes, oversampling factor , a pre-trained feature extractor , a simple classification head .   
\STATE {\bfseries Initialize:} Initialize  at random, generate random batches from  , oversampling  by 
.
   \FOR{ {\bfseries to} }
   \STATE  \\
\STATE
   SGD update of  w.r.t 
   \ENDFOR
\end{algorithmic}
\end{algorithm}
\end{minipage}

\hspace{1em}

\begin{minipage}{0.425\textwidth}
\begin{algorithm}[H]
   \caption{Few-shot outlier inference}
   \label{alg:outlier_exposure_scoring}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} \\
   In-distribution test set  with  classes, out-of-distribution test subset  with  classes, a pre-trained  from inputs to embedding vectors, a trained classification head .   
\STATE Compute 
    \STATE Compute 
    \STATE Compute AUROC based on the scores.
\end{algorithmic}
\end{algorithm}
\end{minipage}
}
\end{center}



We systemically vary the number of known outliers from 1-10 examples per OOD class and evaluate the OOD detection performance. We also show results for higher numbers of examples per class for completeness as it illustrates how quickly the performance saturates. 
 Figure~\ref{fig:outlier_exposure_R50ViTB16_fine-tunedonCIFAR100} and Table~\ref{tab:CIFAR100toCIFAR10_outlier_exposure} show the few-shot outlier exposure results for CIFAR-100 vs CIFAR-10 and CIFAR-10 vs CIFAR-100. We evaluate performance of the pre-trained transformer (without any fine-tuning on CIFAR-*) as well as a fine-tuned transformer. We observe that even with 1-10 known outliers per class, we can achieve  AUROC for near-OOD detection on CIFAR-100 vs CIFAR-10. 
 Interestingly, we observe that having labels for outliers is less important when the transformer is fine-tuned on in-distribution (dashed vs solid red lines) than in the scenario where the transformer is not fine-tuned (dashed vs solid blue lines). 
 Intuitively, the embeddings obtained by fine-tuning a pre-trained transformer are well-clustered, so just a handful of known outliers can significantly improve OOD detection, as illustrated in  
 Figure~\ref{fig:outlier_exposure_landscape_cuts} in Appendix~\ref{sec:outlier:exposure:landscape}. 


\begin{figure}[ht]
	\centering
	\includegraphics[width=0.49\linewidth]{figures/outlier_exposure_R50ViT16_together_id38517871.pdf}
	\includegraphics[width=0.49\linewidth]{figures/outlier_exposure_toCIFAR100_R50ViT16_together_id42750805.pdf}
	\vspace{-0.5em}
\caption{The effect of few-shot outlier exposure and fine-tuning on CIFAR-100 vs  CIFAR-10 (left) and CIFAR-10 vs  CIFAR-100 (right) using a R50+ViT-B\_16  pre-trained on ImageNet-21k. Fine-tuning on in-distribution (red) prior to outlier exposure outperforms no fine-tuning (blue).
	}
	\vspace{-1em}
	\label{fig:outlier_exposure_R50ViTB16_fine-tunedonCIFAR100}
\end{figure}



\begin{table}[h]
\begin{center}	
\vspace{-0.5em}
\caption{ImageNet-21k pre-trained ViT (optionally fine-tuned on in-distribution), with an additional final layer that was trained using the in-distribution train set and a small number of examples of the OOD train set (including the  OOD class labels, corresponding to the red curves in Figure~\ref{fig:outlier_exposure_R50ViTB16_fine-tunedonCIFAR100}). }
\vspace{-0.5em}
\label{tab:CIFAR100toCIFAR10_outlier_exposure}
\begin{subtable}[h]{0.48\textwidth}
\centering
\caption{ CIFAR-100 vs CIFAR-10 AUROC results.}
\resizebox{\textwidth}{!}{
\begin{tabular}{ c|c|c } 
	\makecell{
	Number of OOD\without \\ fine-tuning)} & \makecell{R+ViT\\fine-tuned on\\CIFAR-100} \\
	\hline
     1 & 88.73  1.08\% & 98.70  0.08\% \\
      2 & 92.94  0.55\% &	99.02  0.15\% \\
       3 & 93.25  0.59\% &	99.16  0.11\% \\
    10 & 95.73  0.31\% &	99.46  0.01\% \\
       100 & 97.70  0.01\% & 99.67  0.01\% \\
    \bottomrule \end{tabular}
}
\end{subtable}
\hfill
\begin{subtable}[h]{0.48\textwidth}
\centering
\caption{ CIFAR-10 vs CIFAR-100 AUROC results.}
\resizebox{\textwidth}{!}{
\begin{tabular}{ c|c|c } 
	\makecell{Number of OOD\without \\ fine-tuning)} & \makecell{R+ViT\\fine-tuned on\\CIFAR-10} \\
	\hline
     1 & 94.35  0.05\% & 98.96  0.05\% \\
      2 & 95.10  0.30\% &	99.11  0.04\% \\
       3 & 95.60  0.01\% &	99.17  0.03\% \\
    10 & 96.42  0.02\% &	99.29  0.02\% \\
       100 & 97.38  0.01\% & 99.50  0.01\% \\
\bottomrule
\end{tabular}
}
\end{subtable}
\end{center}
\end{table}

\vspace{-1em}
\section{Near OOD detection of genomic sequences}
\vspace{-0.5em}
\label{sec:genomics}
We investigate OOD detection in genomics as another input modality for near-OOD detection. 
\citet{ren2019likelihood} proposed a benchmark dataset\footnote{\url{https://www.tensorflow.org/datasets/catalog/genomics\_ood}} for OOD detection in genomics, motivated by the real-world problem of bacteria identification based on genomic sequences.   
Real bacteria sequencing data can contain approximately 60-80\% of sequences from unknown classes that have not been studied before. Hence, a classifier trained on all known classes so far will be inevitably asked to predict on genomes that do not belong to one of the known classes. 
Since different bacteria classes are discovered gradually over the years, \citet{ren2019likelihood} use a set of 10 bacteria classes that were discovered before the year 2011 as in-distribution classes, a set of 60 bacteria classes discovered between 2011-2016 as the validation OOD, and a set of 60 different bacteria classes discovered after 2016 as the test OOD. 
The training set only contains genomic sequences of in-distribution classes. The validation and test sets contain sequences from both in-distribution and OOD classes. The genomic sequence is of fixed length of 250 base pairs, composed by characters of {A, C, G, T}. 
In the previous work, 1-dimensional Convolutional Neural Networks (1D CNN) were used to build the classifier for the 10 in-distributional classes, and the maximum of softmax probabilities (MSP) and Mahalanobis distance were used for OOD detection. The best AUROC for MSP was only 66.14\%, and 62.41\% for Mahalanobis distance \citep{ren2019likelihood}.

Similar to the previous section, we explore the usefulness of pre-trained transformers and fine-tuning for near-OOD detection.  
Unsupervised pre-training and fine-tuning approach has been applied to several bio-informatic problems, such as protein function prediction \citep{elnaggar2020prottrans, littmann2021embeddings}, protein structure prediction \citep{rives2021biological},
and predicting promoters, splice sites and transcription factor binding sites \citep{ji2020dnabert}, but it has not yet been studied how pre-training could help near-OOD detection.  


\begin{figure}[ht]
	\centering
\begin{subfigure}[h]{0.52\textwidth}{ 	\includegraphics[width=\linewidth]{figures/Bert-genomics.pdf}
          \caption{BERT pre-training and fine-tuning for genomics.}
         \label{fig:genomics:bert}
 }\end{subfigure}
 \begin{subfigure}[h]{0.42\textwidth}{ 
 \includegraphics[width=\linewidth]{figures/dist_vs_auc.pdf}
           \caption{AUROC of near-OOD detection}
         \label{fig:genomics:auroc}
 }\end{subfigure}
 \vspace{-0.5em}
	\caption{(a) Model architecture for BERT pre-training and fine-tuning. The unsupervised pre-training model uses a transformer encoder to predict the masked token (shown in red). The fine-tuned model adds a simple classification head (a single linear projection) to predict in-distribution classes. (b) The relationship between the minimum genetic distance and the AUROC scores for the 60 OOD test classes. The pre-train+fine-tune based MSP and Mahalanobis distance methods have significantly higher AUROC overall, and the positive correlation between the minimum distance and the AUROC are more prominent than for the baseline model. }
\vspace{-1em}
	\label{fig:genomics}
\end{figure}

\paragraph{Unsupervised BERT pre-training and supervised fine-tuning}
We first pre-train the transformer model in an unsupervised fashion as in BERT to capture biologically relevant properties.
For unlabeled in-distribution sequences in the training set, we randomly mask the characters in the sequence at the rate of 0.15, feed the masked sequence into transformer-based model of 8 heads and 6 layers and embedding dimension 512, and predict the masked characters. 
To boost the performance, we add the unlabeled validation data to the training set for pre-training.
In the fine-tuning stage, we load the pre-trained transformer model, mean pool the embeddings over the positions, and add a single linear projection classification head for 10 in-distribution classes on top of the embeddings. The setup is shown in Figure~\ref{fig:genomics:bert}. 
All the parameters in the model including those in the pre-trained transformer and those in the classification head are fine-tuned using the labeled training data.
The model is pre-trained for 300,000 steps using learning rate of 0.001 and Adam optimizer \citep{kingma2014adam} on TPU,
and the accuracy for predicting the masked token is 48.35\%. 
The model is fine-tuned for 100,000 steps at the learning rate of 0.0001, and the classification accuracy is 89.84\%. 
We use the validation in-distribution and validation OOD data to select the best model checkpoint for each of the two methods and evaluate on test set. 


\begin{table}[h]
\begin{center}	
\caption{Genomics OOD BERT pre-trained and fine-tuned on the in-distribution training set. Error bars  represent standard deviation over 3 runs. See Table~\ref{tab:genomics_full} in Appendix~\ref{app:genomics:results} for AUPRC and FPR95.}
\begin{tabular}{ c|c|c|c } 
\makecell{Model} & \makecell{Test Accuracy} & \makecell{Mahalanobis\\AUROC} & \makecell{MSP\\AUROC} \\\hline
1D CNN \citep{ren2019likelihood} & 85.930.11\% & 64.750.73\% & 65.840.46\% \\
BERT pre-train and fine-tune & 89.840.00\% & \textbf{77.490.04\%} & 73.530.03\% \\ \hline
\end{tabular}
\vspace{-0.5em}
\label{tab:genomics}
\end{center}
\end{table}

The results are reported in Table \ref{tab:genomics}. It can be seen that using the approach of pre-training transformer and fine-tuning, the OOD detection performance is significantly improved, from 64.75\% to 77.49\% for Mahalanobis distance, and from 65.84\% to 73.53\% for MSP. The in-distribution accuracy also improves a bit, from 85.93\% to 89.84\%. 
We also study the relationship between the genetic distance and the AUROC of OOD detection for the 60 test OOD classes. 
We compute the genetic distance using the popular alignment-free method  which is based on the similarity between the word frequencies of the two genomes \citep{ren2018alignment, reinert2009alignment}. Studies have shown that this genetic distance reflects true evolutionary distances \citep{chan2014inferring, bernard2016alignment}.
For each of the 60 OOD test classes, we use the minimum genetic distance between this OOD class to any of the 10 in-distribution classes as the final distance measure. 
Figure \ref{fig:genomics} shows the AUROC and the minimum distance for each of the 60 OOD classes. We expect the AUROC is higher as the distance is greater. 
Using the baseline 1D CNN model, we did not see obvious correlation between the AUROC and the minimum distance, with , based on Mahalanobis distance. 
The AUROC based on MSP method has positive correlation to the minimum distance, with .
After we use the pre-trained+fine-tuned transformer, both MSP and Mahalanobis distance methods have significantly higher AUROC overall, and the positive correlation between the minimum distance and the AUROC is more prominent than for the baseline model. 

\begin{wrapfigure}{r}{0.45\textwidth}
\centering
      \vspace{-2em}
    \includegraphics[width=1.0\linewidth]{figures/genomics_oe.pdf}
    \caption{Few-shot outlier exposure for genomics OOD. The x-axis shows the number of outliers per class that the model was exposed to. The y-axis is OOD AUROC in \%. The shading shows the standard deviation over 3 runs. See Table \ref{tab:genomics_oe} for exact numbers.}
    \vspace{-1em}
    \label{fig:genomics_oe}
\end{wrapfigure}
\paragraph{Few-shot outlier exposure} 
Given that pre-trained and fine-tuned model improves the OOD performance, we next explore the idea of few shot outlier exposure to further boost the performance. 
We randomly select 1, 2, 5, 10, 100 examples per test OOD class and add them to the training set respectively. 
For each input  in the training set, we extract its corresponding embedding vector  from the above pre-trained and fine-tuned model (or alternatively the model without fine-tuning). 
We construct a single layer perceptron network of 1024 units for classifying each individual to in-distribution classes and OOD classes, as shown in Figure~\ref{fig:few-shot_oe}. 
At inference time, we use the sum of the probability of in-distribution classes as the final confidence score for OOD detection. 
Additionally, we also tried the idea of collapsing all OOD classes into one single class (as in \citep{thulasidasan2021a}) for comparison.
The model is trained for 10,000 steps with the learning rate of 0.001. The best model checkpoint is selected based on the highest AUROC on a small set of validation dataset disjoint from the test set.


Results are shown in Figure~\ref{fig:genomics_oe}. We observe that exposing to just a small number of OOD examples significantly improves the OOD performance, increasing AUROC from 76.73\% to 88.48\%. 
As expected, using the embeddings from the fine-tuned model (blue lines) is better than that from the model without fine-tuning (purple lines).
Also, using the outlier labels (purple solid line) has a slightly better performance than collapsing the OOD classes into a single class (purple dashed line) using the pre-trained embeddings without fine-tuning. 









\vspace{-0.5em}
\section{Using candidate labels with multi-modal text-image models such as CLIP}\label{sec:clip}
\vspace{-0.5em}
Multi-modal transformers such as CLIP \citep{clip}, which are pre-trained on image-text pairs, have been shown to perform well on zero-shot classification tasks. 
\emph{We show that such multi-modal transformers open the door to new forms of outlier exposure which can significantly improve   
out-of-distribution (OOD) detection in the  zero-shot classification setting.} Our goal is to show that multi-modal transformers can leverage a weaker form of outlier exposure than the few-shot outlier exposure assumption in previous sections, and improve their safety for zero-shot classification.   

We use the pre-trained CLIP model\footnote{\url{https://github.com/openai/CLIP}} (specifically ViT-B/32) that was trained on 400 million (text, image) pairs from the internet. Its image encoder can map an image  into an embedding vector , while its text encoder can do the same for a string  as . By choosing a set of  \textit{candidate labels} for an image, the similarity between the embedding of the candidate label  and an image  can be used as the  component of the image's embedding vector  as . \begin{figure}[th]
	\centering
\includegraphics[width=\linewidth]{figures/OOD_CLIP_cartoon_larger.pdf}
	\caption{Using candidate text labels and an image-text multi-modal model (CLIP) to produce an embedding vector for OOD detection. We use two sets of candidate labels, evaluate the semantic alignment of the image with each label, apply softmax, and use the sum of probabilities in the first (in) set as an OOD score. Note that this is zero-shot classification and the model is not fine-tuned and does not leverage any in-distribution or OOD images/labels. 
It only uses the names of the classes (or other informative words) as candidate labels and works well due to the strong pre-training of CLIP.}
	\vspace{-1em}
	\label{fig:CLIP_text_image_cartoon}
\end{figure}


\vspace{-0.5em}
\paragraph{Zero-shot outlier exposure} In the zero-shot classification setting, the candidate labels are chosen to describe the semantic content of the in-distribution classes (e.g. names of the classes). We propose to include the candidate labels related to the out-of-distribution classes, and utilize this knowledge as a very weak form of outlier exposure in multi-modal models. This could be relevant in applications, where we might not actually have any outlier images for fine-tuning but we might know the \emph{names} or \emph{descriptions} of outlier classes. 




The procedure is shown in Figure~\ref{fig:CLIP_text_image_cartoon}. We choose two groups of candidate labels, in-distribution and out-of-distribution labels (e.g. CIFAR-100 and CIFAR-10 class names). We produce an embedding vector  for each image , apply softmax to get probabilities as . Those get split to , and , where . 
Similar to Figure~\ref{fig:few-shot_oe}, we use  as the confidence score. 
By choosing the candidate labels to represent the in-distribution and OOD dataset we would like to distinguish (e.g. CIFAR-100 and 10), we can get a very informative score that leads to AUROC above previous SOTA, despite no exposure to the training set of the in-distribution (zero-shot). Our results are shown in Table~\ref{tab:CLIP-candidate-labels}, with additional results in Table~\ref{tab:CLIP-candidate-labels-opposites}.


\begin{table}[ht]
\begin{center}	
\vspace{-1em}
\caption{Zero-shot OOD detection using image-text multi-modal models. We compare CLIP that uses only the names of in-distribution classes (baseline) and compare it to our proposed variant that uses just the names of out-of-distribution classes as candidate labels.  
Even in the zero-shot setting (without any fine-tuning on either in-distribution or OOD dataset), we outperform previous SOTA. We show additional results in  Table~\ref{tab:CLIP-candidate-labels-opposites}. }
\vspace{0.5em}
\begin{tabular}{ c|c|c|c|c } 
	Distribution 1 & Distribution 2 & Labels 1 & Labels 2 & AUROC \\
	\hline
		CIFAR-100 & CIFAR-10 & CIFAR-100 names & --- & 69.49\% \\
				CIFAR-100 & CIFAR-10 & CIFAR-100 names & CIFAR-10 names & 94.68\% \\
\hline
				CIFAR-10 & CIFAR-100 & CIFAR-10 names & ---  & 89.17\% \\
				CIFAR-10 & CIFAR-100 & CIFAR-10 names & CIFAR-100 names & 94.68\% \\

\hline
	CIFAR-100 & SVHN & CIFAR-100 names & --- & 93.05\% \\
	CIFAR-100 & SVHN & CIFAR-100 names & ["number"] & 99.67\% \\
	\hline
	CIFAR-10 & SVHN & CIFAR-10 names & --- & 96.90\% \\
	CIFAR-10 & SVHN & CIFAR-10 names & ["number"] & 99.95\% \\ \hline
\end{tabular}
\label{tab:CLIP-candidate-labels}
\end{center}
\end{table}


\section{Conclusion}
We focus on the challenging problem of near-OOD detection. We show that fine-tuning large-scale pre-trained transformers and using few-shot outlier exposure can significantly improve the SOTA. On the CIFAR-100 vs CIFAR-10 visual OOD detection  benchmark, we improve the SOTA AUROC from 85\% to 96\% (without outlier exposure) and 99\% (with outlier exposure), essentially closing the gap between SOTA and the ideal performance. 
On a challenging genomics benchmark, we improve the  SOTA from 66\% to 77\% using BERT (without outlier exposure) and 88\% (with outlier exposure). We also show that multi-modal pre-trained transformers open the door to new, weaker forms of outlier exposure which only use names of OOD inputs; we apply this to CLIP and achieve AUROC of 94.7\% in the zero-shot classification setting. 
We believe that our findings will be of interest to the research community as well as practitioners working on safety-critical applications. 




\section*{Acknowledgements}
 We thank Abhijit Guha Roy, Jim Winkens, Jeremiah Liu and the anonymous reviewers for helpful feedback. We thank David Dohan and Andreea Gane for the helpful advice on BERT genomics model pre-training. We thank Basil Mustafa for providing the BiT model checkpoints. We thank Matthias Minderer for his helpful advice on ViT. We thank Winston Pouse for useful discussions on human performance.
\bibliography{main}
\bibliographystyle{plainnat}





















\clearpage
\newpage
\appendix





\section{Measuring human performance on CIFAR-100 vs CIFAR-10 OOD task}\label{app:human}
While the typical accuracy a human reaches is often known for classification tasks, there is a lack of such benchmark for near-OOD detection. We decided to measure human performance on the task of distinguishing CIFAR-100 and CIFAR-10. To do that, we wrote a simple graphical user interface (GUI) where a user is presented with a fixed number of images randomly chosen from the in-distribution and out-of-distribution test sets (CIFAR-10 and 100 in our case). The user then clicks on the images they believe belong to the in-distribution. To make this easier, we allow the user to choose the images belonging to the individual classes of the in-distribution. An example of our GUI is shown in Figure~\ref{fig:human_bechmark}.
\begin{figure}[ht]
	\centering
\includegraphics[width=1.0\linewidth]{figures/human_benchmark_GUI.jpg}
\\	\vspace{1em}
\includegraphics[width=1.0\linewidth]{figures/human_benchmark_GUI2.jpg}
	\\	\vspace{1em}
	\includegraphics[width=1.0\linewidth]{figures/human_benchmark_GUI3.jpg}
	\caption{Graphical User Interface (GUI) for human benchmarking of near-OOD detection. The user clicks on images belonging to any of the 10 in-distribution CIFAR-10 classes. The user is shown 20 images at a time, and once they are done with their selection, they press a key and a new group of randomly chosen images (equal probability of CIFAR-10 and CIFAR-100) is shown. The figure shows 3 randomly chosen groups of 20 images, together with user-selected CIFAR-10 images framed by the color corresponding to their selected class.}
	\label{fig:human_bechmark}
\end{figure}


In the case of CIFAR-10 and CIFAR-100 distributions (selecting the classes of CIFAR-10), the user clicks on all images belonging to each of the 10 classes: \emph{airplane,	automobile,  
bird, cat, 	deer, dog, 	frog, horse, ship, truck}. This is done without any exposure to the training set or examples of any of the classes, based only on the class names and using the fact that people are generally very familiar with the semantic concept of these labels. Coincidentally, this is quite similar to the kind of familiarity large pre-trained transformers gain by the breath of their pre-training.



We calculated the AUROC for the CIFAR-10 / CIFAR-100 task on their test sets, where choosing any of the 10 classes was acceptable as a valid CIFAR-10 selection by the user. The results are shown in Table~\ref{tab:human-benchmark}. The average AUROC weighted by the number of images in each trial is AUROC . 
\begin{table}[h]
\begin{center}	
\caption{Author's results on CIFAR-100 vs CIFAR-10 distinguishing task.}
\begin{tabular}{ c|c|c } 
	Date & Number of images & AUROC \\
	\hline
	April 7 2021 & 1140 & 96.14\% \\
	April 25 2021 & 1700 & 95.67\% \\
	April 27 2021 & 940 & 96.03\% \\ \hline
\end{tabular}
\label{tab:human-benchmark}
\end{center}
\end{table}

\section{Visualizing the effect of outlier exposure on fine-tuned pre-trained models}\label{sec:outlier:exposure:landscape}
Figure~\ref{fig:outlier_exposure_landscape_cuts} shows the outlier score on near-OOD task.  
Intuitively, the embeddings obtained by fine-tuning a pre-trained transformer are well-clustered, so just a handful of known outliers can significantly improve OOD detection. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.24\linewidth]{figures/section_ViT_finetuned_noexposure_id91171169.pdf}
	\includegraphics[width=0.24\linewidth]{figures/section_ViT_finetuned_exposure1_id97897481.pdf}
	\includegraphics[width=0.24\linewidth]{figures/section_ViT_finetuned_exposure2_id468196.pdf}
	\includegraphics[width=0.24\linewidth]{figures/section_ViT_finetuned_exposure100_id61900463.pdf}
\caption{The effect of outlier exposure (CIFAR-10) on ViT a model fine-tuned on CIFAR-100. Each panel shows the same two-dimensional PCA cut of the space of embeddings. Examples of 2 in-distribution (CIFAR-100) classes (black plus signs = "bus", yellow crosses = "pickup truck"), and 1 out-of-distribution (CIFAR-10) class (red dot = "automobile"). The left-most plot shows Mahalanobis OOD score as the color coding. The more OOD outlier exposure (second to left to right), the better aligned the OOD probability contours with the underlying classes. 
	}
	\label{fig:outlier_exposure_landscape_cuts}
\end{figure}

\section{Additional results for visual OOD detection}\label{sec:app:additional}
We tested our OOD detection on several more tasks in addition to CIFAR- CIFAR- in Table~\ref{tab:ViT_fine-tuned}. We used the \textit{Describable Textures dataset} (DTD, or Textures in our tables) \citep{cimpoi14describing}, \textit{Places365-Standard} (Places365 in our tables) \citep{zhou2017places} and SVHN \citet{Netzer2011} datasets as provided by \verb|tensorflow| datasets.\footnote{\url{https://www.tensorflow.org/datasets}} In all cases we use the test set of each dataset. 


Besides the area under the receiver operating characteristic curve  (AUROC), we also evaluate OOD performance using the area under the precision-recall curve (AUPRC) and the false positive rate at N\% true positive rate (FPRN), as in \cite{hendrycks2018deep}.
Since our goal is to detect OOD, we treat OOD test set as the positive set and in-distribution test set as the negative set. 
AUROC and AUPRC are threshold independent, evaluating the overall OOD performance across multiple thresholds. 
AUROC is also sample size independent, while AUPRC is sensitive to detect imbalance between positive and negative sets. 
FPRN computes the false positive rate at which \% of OOD data is recalled. As a common practice, we set . 





\begin{table}[h]
\begin{center}	
\caption{ImageNet-21k pre-trained Vision Transformer fine-tuned on the in-distribution training set. More datasets in addition to Table~\ref{tab:ViT_fine-tuned}. 
}
\vspace{0.5em}
\resizebox{\textwidth}{!}{
\begin{tabular}{ c|c|c|c|c|c|c|c|c } 
	 \makecell{In-\\distribution} & \makecell{fine-tuned\\test\\accuracy} & \makecell{Out-\\distribution} & \makecell{Maha.\\AUROC } &
	\makecell{Maha.\\AUPRC} &
	\makecell{Maha.\\FPR95} &
	\makecell{MSP\\AUROC} &
	\makecell{MSP\\AUPRC} &
	\makecell{MSP\\FPR95} \\
\hline
\multirow{4}{*}{CIFAR-100}	 & \multirow{4}{*}{91.71\%}  & CIFAR-10 & 96.23\% & 96.32\% & 18.73\% & 92.13\% & 92.57\% & 37.73\% \\
&	& Textures & 99.03\% & 96.53\% & 4.27\% & 96.28\% & 99.22\% & 17.30\% \\
 &  & SVHN & 97.80\% & 98.87\% & 8.42\% & 97.15\% & 93.61\% & 13.05\% \\
& & Places365 & 93.95\% & 99.78\% & 29.22\% & 88.37\% & 39.57\% & 51.02\% \\
\hline
 	\multirow{4}{*}{CIFAR-10}	 & \multirow{4}{*}{98.70\% } 
& CIFAR-100 & 98.52\% & 98.70\% & 6.89\% & 97.79\% & 97.72\% &  9.76\% \\
& & Textures & 99.97\% & 99.83\% & 0.05\% & 99.59\% & 99.92\% & 1.73\% \\
& & SVHN & 99.58\% & 99.82\% & 1.90\% & 98.77\% & 97.75\% & 4.03\% \\
& & Places365 & 98.51\% & 99.95\% & 4.54\% & 97.14\% & 50.92\% & 10.13\% \\
\bottomrule
\end{tabular}
}
\label{tab:ViT_fine-tuned_additional_dataset}
\end{center}
\end{table}




The additional results are shown in Table~\ref{tab:ViT_fine-tuned_additional_dataset}. SVHN and Textures are far-OOD tasks and our methods achieve AUROC of around  or higher. Places365 is a harder OOD task, see the discussion in \citep{winkens2020contrastive} where they compute Confusion Log Probability (CLP) score for different OOD datasets and demonstrate that CIFAR-100 vs CIFAR-10 and CIFAR-* vs Places365 are more difficult OOD detection tasks than CIFAR-* vs SVHN and CIFAR-* vs Textures. On CIFAR-100 vs Places365, we achieve 93.9\% (\citet{winkens2020contrastive} report 82\%) and on CIFAR-10 vs Places365, we achieve 98.5\% (\citet{winkens2020contrastive} report 95\%).




\subsection{Scaling of OOD performance with in-distribution test accuracy}\label{sec:accuracy-vs-ood}
In this section, we evaluate the relationship between fine-tuned in-distribution test accuracy and OOD detection performance. 
We looked at intermediate checkpoints during fine-tuning and observed the dependence of the in-distribution test accuracy and OOD detection performance (AUROC). We focused on the CIFAR-100 fine-tuning and the CIFAR-100 vs CIFAR-10 OOD detection task as it was the most challenging widely used benchmark we studied. The results 
are shown in Figure~\ref{fig:ViT_and_Mixer_scaling}. We observe that the higher the in-distribution test accuracy, the higher the OOD detection AUROC. For the Vision Transformer, fine-tuning with and without augmentation leads to the same relationship between accuracy and OOD detection performance. However, we found that training with augmentation leads to a slower convergence. We found that the scaling relationship we identified on ViT-B\_16 holds remarkably well even for large ViT models, such as ViT-L\_16 as shown in Figure~\ref{fig:ViT_and_Mixer_scaling}. As shown in Table~\ref{tab:ViT_fine-tuned:additional}, larger ViT models such as ViT-L\_16 and ensemble of ViT models further improve OOD detection, improving the AUROC from 96.23\% to approximately 98\%, establishing a new state-of-the-art on this task.   


\begin{table}[h]
\begin{center}	
\vspace{-1em}
\caption{ImageNet-21k pre-trained ViT  fine-tuned on the in-distribution training set. }
\begin{tabular}{ c|c|c|c|c|c } 
	Model & \makecell{In-\\distribution} & \makecell{fine-tuned\\test\\accuracy} & \makecell{Out-\\distribution} & \makecell{Mahalanobis\\AUROC} & \makecell{MSP\\AUROC} \\
	\hline
ViT-B\_16 & CIFAR-100 & 90.95\% & CIFAR-10 & 95.53\% & 91.89\% \\
	R50+ViT-B\_16 & CIFAR-100 & 91.71\% & CIFAR-10 & \textbf{96.23}\% & 92.08\% \\
ViT-L\_16 & CIFAR-100 & 94.73\% & CIFAR-10 & \textbf{97.98\%} & 94.28\% \\
	ViT ensemble & CIFAR-100 & --- & CIFAR-10 & \textbf{98.11\%} & 95.15\% \\
\bottomrule
\end{tabular}
\vspace{-1em}
\label{tab:ViT_fine-tuned:additional}
\end{center}
\end{table}

\begin{figure}[ht]
	\centering
\includegraphics[width=0.6\linewidth]{figures/ViT_augment_comparison_id4654017.pdf}
	\vspace{-0.5em}
	\caption{The dependence of OOD performance on in-distribution test accuracy for ViT models fine-tuned on CIFAR-100. Square markers represent different checkpoints during fine-tuning (lower accuracy corresponds to fewer steps of fine-tuning). 
The purple markers represent larger ViTs from \citet{steiner2021augreg}. The higher the accuracy on the in-distribution, the better the OOD performance. The blue linear fit uses only the square datapoints, but intersects the purple datapoints (large ViTs) well, suggesting a robust scaling.}
	\vspace{-1em}
	\label{fig:ViT_and_Mixer_scaling}
\end{figure}



\subsection{Additional results for zero-shot OOD detection using CLIP}

In Table~\ref{tab:CLIP-candidate-labels-opposites} we extend the zero-shot OOD detection results for CLIP in Table~\ref{tab:CLIP-candidate-labels} by using the names of the in-distribution classes and comparing them to the \textit{the opposite of} concatenated with the same names. The performance is above random, but not significant compared to even using just the in-distribution names, as shown in Table~\ref{tab:CLIP-candidate-labels}.

\begin{table}[ht]
\begin{center}	
\vspace{-1em}
\caption{Zero-shot OOD detection using image-text multi-modal models. In Table~\ref{tab:CLIP-candidate-labels} we show the performance of CLIP in the zero-shot OOD regime with candidate labels that are the names of the classes of the in-distribution, and optionally the names of the classes of the out-distribution dataset. In this table we present results using the names of the in-distribution classes as compared to \textit{the opposite} concatenated with the same names.
}
\vspace{0.5em}
\begin{tabular}{ c|c|c|c|c } 
	Distribution 1 & Distribution 2 & Labels 1 & Labels 2 & AUROC \\
	\hline
		CIFAR-100 & CIFAR-10 & CIFAR-100 names & \makecell{"the opposite of"\\+CIFAR-100 names} & 61.37\% \\
				CIFAR-10 & CIFAR-100 & CIFAR-10 names & \makecell{"the opposite of"\\+CIFAR-10 names}  & 71.894\% \\
\hline
\end{tabular}
\vspace{-1em}
\label{tab:CLIP-candidate-labels-opposites}
\end{center}
\end{table}



\subsection{Qualitative analysis of OOD detection using ViT}\label{app:failure:cases}

In this section we present some qualitative failure cases of OOD detection. The results for the CIFAR-100 (in-distribution) vs CIFAR-10 (out-distribution) experiment are shown in Figure~\ref{fig:mistakes_cifar100tocifar10}. For the CIFAR-10 images that are mistakenly classified as in-distribution (CIFAR-100), the top images are actually mislabeled images from the CIFAR-10 test set (a fox labeled as a \textit{cat}, and a kangaroo labeled as a \textit{deer}). They are followed by \textit{automobiles} and \textit{trucks} (both CIFAR-10 classes) that are seen as \textit{buses} and \textit{streetcars} (similar CIFAR-100 classes). In those cases, the distinction could be unclear even to a human.  
\begin{figure}[ht]
	\centering

 \begin{subfigure}[h]{1.0\textwidth}{ 	\includegraphics[width=\linewidth]{figures/cifar10_images_8_examples_with_maha_id81951428.pdf}
          \caption{OOD images (CIFAR-10) closest to the in-distribution (CIFAR-100).
          }
         \label{fig:cifar10_mistaken_as_cifar100}
 }\end{subfigure}
\\ \begin{subfigure}[h]{1.0\textwidth}{ 	\includegraphics[width=\linewidth]{figures/cifar100_images_8_closest_to_cifar10_images_id44246237.pdf}
          \caption{The in-distribution (CIFAR-100) images with the closest embedding vector to images in Figure~\ref{fig:cifar10_mistaken_as_cifar100}.
          }
         \label{fig:similar_cifar100_to_cifar10}
 }\end{subfigure}
\caption{CIFAR-100 (in-distribution) vs CIFAR-10 (out-distribution): Images from the out-distribution that reached the lowest Mahalanobis distances from the in-distribution (Figure~\ref{fig:cifar10_mistaken_as_cifar100}) and the in-distribution images with the most similar embedding vectors (Figure~\ref{fig:similar_cifar100_to_cifar10}). The comparison to the closest in-distribution images demonstrates that OOD detection in these particular cases might be failing due to the limitations of the labelling in the original dataset and genuine semantic ambiguity of some classes. The first two images in Figure~\ref{fig:cifar10_mistaken_as_cifar100} do not belong to CIFAR-10 and have likely been included by accident. 
}
	\vspace{-1em}
	\label{fig:mistakes_cifar100tocifar10}
\end{figure}


\begin{figure}[ht]
	\centering
 \begin{subfigure}[h]{1.0\textwidth}{ 	\includegraphics[width=\linewidth]{figures/svhn_images_8_examples_with_maha_id5834374.pdf}
          \caption{OOD images (SVHN) closest to the in-distribution (CIFAR-100).
          }
         \label{fig:svhn_mistaken_as_cifar100}
 }\end{subfigure}
 \\
  \begin{subfigure}[h]{1.0\textwidth}{ 	\includegraphics[width=\linewidth]{figures/cifar100_images_8_closest_to_SVHN_images_id60701065.pdf}
          \caption{The in-distribution (CIFAR-100) images with the closest embedding vector to images in Figure~\ref{fig:svhn_mistaken_as_cifar100}.}
         \label{fig:similar_cifar100_to_svhn}
 }\end{subfigure}
\caption{CIFAR-100 (in-distribution) vs SVHN (out-distribution): Images from the out-distribution that reached the lowest Mahalanobis distances from the in-distribution (Figure~\ref{fig:svhn_mistaken_as_cifar100}) and the in-distribution images with the most similar embedding vectors (Figure~\ref{fig:similar_cifar100_to_svhn}). The most similar images demonstrate that the some of the digits genuinely look like worms.}
	\vspace{-1em}
	\label{fig:mistakes_svhn}
\end{figure}
We show several qualitative failure cases for the CIFAR-100 (in-distribution) vs SVHN (out-distribution) experiment in Figure~\ref{fig:mistakes_svhn}.
The out-distribution images that are most like CIFAR-100 (Figure~\ref{fig:svhn_mistaken_as_cifar100}) are numbers "2", "5" and "6" written in an especially wiggly font, sometimes with plastic-like filler, giving them a very worm-like quality that the model perceives. The last image we show gets perceived as a cloud, likely due to its deep blue background and low resolution. In Figure~\ref{fig:similar_cifar100_to_svhn} we show the images from the in-distribution that have the closest embedding vector to the SVHN mistakes. They do look very similar.   

Overall the mistakes our models make are semantically meaningful to a human. For CIFAR-10, they contain mislabeled examples of the CIFAR-10 test set that should not have been included in the first place, or genuine imperfect categories such as \textit{automobile}, \textit{truck}, \textit{bus}, and \textit{streetcar}, which are even harder to distinguish in the  resolution. For SVHN, the mistakes we get are also very plausible to a human.

Figure~\ref{fig:confusing_class_pairs_vehicles} and Figure~\ref{fig:confusing_class_pairs_animals} show examples of the most confusing pairs of (OOD class (CIFAR-10), in-distribution class (CIFAR-100)). We looked at the OOD examples with the lowest Mahalanobis distances, e.g. the ones that are the most in-distribution-like to the network at hand (ViT fine-tuned on the CIFAR-100 train set). Going from the smallest distance up, we sorted these images based on the tuple of which CIFAR-10 class the OOD image came from, and which class the CIFAR-100 test set image that has the most similar embedding vector to it belongs. We show examples that come from classes related to \textbf{vehicles} in Figure~\ref{fig:confusing_class_pairs_vehicles}, and to \textbf{animals} in Figure~\ref{fig:confusing_class_pairs_animals}. Some of the least-OOD-like CIFAR-10 examples shown are literal mistakes, and they should not have been included in CIFAR-10 at all (e.g. the first column "truck" being actually a tractor in Figure~\ref{fig:confusing_class_pairs_vehicles}, or the first column "cat" in Figure~\ref{fig:confusing_class_pairs_animals} being actually a fox). Many of the classes have a genuine semantic overlap, especially among the vehicles. For example, the CIFAR-10 class "automobile" has a large number of vans, that are also included in the CIFAR-100 class "bus". The CIFAR-10 class "truck" and the CIFAR-100 class "pickup truck" share equally mutually similar vehicles.









\begin{figure}[ht]
	\centering
	\vspace{-5.0em}
	\includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1002_truck_streetcar_cifar100_vs_cifar10_id10524041.pdf}
\hrule
    \includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1273_truck_bus_cifar100_vs_cifar10_id10524041.pdf}
    \hrule
    \includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1343_truck_tractor_cifar100_vs_cifar10_id10524041.pdf}
    \hrule
\includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1424_automobile_pickup_truck_cifar100_vs_cifar10_id10524041.pdf}
    \hrule
    \includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1424_truck_pickup_truck_cifar100_vs_cifar10_id10524041.pdf}
    \hrule
    \includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1451_automobile_bus_cifar100_vs_cifar10_id10524041.pdf}
    \hrule
    \includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1531_truck_tank_cifar100_vs_cifar10_id10524041.pdf}
	\caption{CIFAR-100 (in-distribution) vs CIFAR-10 (out-distribution), hard to distinguish pairs of classes involving \textbf{vehicles}: The plots show pairs of images, top row from the out-distribution (CIFAR-10) and the bottom row having the closest embedding vector to it from the in-distribution (CIFAR-100), for pairs of classes for which OOD images are the closest to the in-distribution (=hardest to distinguish). Our examples show that there is a genuine semantic overlap between some CIFAR-10 and CIFAR-100 classes that limits the OOD performance of any model.}
	\vspace{-1em}
	\label{fig:confusing_class_pairs_vehicles}
\end{figure}
\begin{figure}[ht]
	\centering
	\vspace{-5.0em}
	\includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1308_cat_tiger_cifar100_vs_cifar10_id10524041.pdf}
	\hrule
\includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1325_deer_rabbit_cifar100_vs_cifar10_id10524041.pdf}
\hrule
\includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1436_cat_otter_cifar100_vs_cifar10_id10524041.pdf}
\hrule
\includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1459_cat_porcupine_cifar100_vs_cifar10_id10524041.pdf}
\hrule
\includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1538_cat_fox_cifar100_vs_cifar10_id10524041.pdf}
\hrule
\includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1685_horse_cattle_cifar100_vs_cifar10_id10524041.pdf}
\hrule
\includegraphics[width=0.9\linewidth]{figures/failure_figures/confusing_classes_meandist1606_deer_cattle_cifar100_vs_cifar10_id10524041.pdf}
	\caption{CIFAR-100 (in-distribution) vs CIFAR-10 (out-distribution), hard to distinguish pairs of classes involving \textbf{animals}: The plots show pairs of images, top row from the out-distribution (CIFAR-10) and the bottom row having the closest embedding vector to it from the in-distribution (CIFAR-100), for pairs of classes for which OOD images are the closest to the in-distribution (=hardest to distinguish). Our examples show that there is a genuine semantic overlap between some CIFAR-10 and CIFAR-100 classes that limits the OOD performance of any model.}
	\vspace{-1em}
	\label{fig:confusing_class_pairs_animals}
\end{figure}




\section{Additional results for genomics OOD detection}\label{app:genomics:results}

We report additional results for genomics in Table~\ref{tab:genomics_full} and Table~\ref{tab:genomics_oe}. 

\begin{table}[h]
\begin{center}	
\caption{Genomics OOD detection using  pre-trained BERT and fine-tuned on the in-distribution training set. The error bars shown are standard deviations over 3 runs.}
\vspace{0.5em}
\resizebox{\textwidth}{!}{
\begin{tabular}{ c|c|c|c|c|c|c|c } 
\multirow{2}{*}{Model} & \multirow{2}{*}{Test Accuracy} & \multicolumn{3}{c|}{Mahalanobis} & \multicolumn{3}{c}{MSP} \\
                      &                      & AUROC    & AUPRC   & FPR95   & AUROC & AUPRC & FPR95 \\ \hline
1D CNN \citep{ren2019likelihood} & 85.930.11\% & 64.750.73\% & 60.250.82\% & 77.760.84\% & 65.840.46\% & 62.240.31\% & 89.790.18\% \\
BERT pre-train and fine-tune & 89.840.00\% & 77.490.04\% & 78.790.06\% & 68.220.13\% & 73.530.03\% & 73.860.03 & 85.390.07\% \\ \hline
\end{tabular}
}
\vspace{-0.5em}
\label{tab:genomics_full}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}	
\caption{Few-shot outlier exposure for genomics OOD. The numbers here correspond to Figure \ref{fig:genomics_oe}. The error bar is the standard deviation of 3 runs.}
\vspace{0.5em}
\begin{tabular}{ c|c|c|c|c } 
\makecell{\# OOD examples\\ per class} & \makecell{w/o fine-tuning\\ignore outlier label} & \makecell{w/o fine-tuning\\use outlier label} & \makecell{w/ fine-tuning\\ignore outlier label} & \makecell{w/ fine-tuning\\use outlier label} \\ \hline
1 & 75.321.14\% & 74.950.87\% & 84.360.60\% & 82.211.10\% \\
2 & 75.670.50\% & 76.010.19\% & 84.810.83\% & 83.740.47\% \\
5 & 77.841.06\% & 77.670.71\% & 86.540.58\% & 85.940.28\% \\
10 & 77.811.37\% & 79.000.69\% & 87.020.45\% & 86.491.43\% \\
100 & 80.210.14\% & 82.170.33\% & 87.940.62\% & 88.490.51\% \\ \hline
\end{tabular}
\label{tab:genomics_oe}
\end{center}
\end{table}


\end{document}