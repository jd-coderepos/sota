\newpage
\onecolumn
{\huge Appendix}


\def\E{{\bf E}}
\def\x{{\bf x}}
\def\r{{\bf r}}
\def\rhat{\hat{r}}

\section{Proofs}

\label{app:proofs}

\subsection{Proof of Theorem~\ref{thm:bijection}}
\label{app:proof-bij}
\thmbij*
\begin{proof}
Let . We will show that  is equal to the -dependent marginal distribution  by verifying (1) the starting distribution is correct when ; (2) the continuity equation holds, \ie . The starting distribution is , which confirms that . The continuity equation can be expressed as:

It means that  satisfies the continuity equation for any . Together, we conclude that . Lastly, note that the terminal distribution is 

\end{proof}

\subsection{Proof of Theorem~\ref{thm:minimizer}}
\label{app:thm-minimizer}
\thmmin*
\begin{proof}
The minimizer at  in \Eqref{eq:new-D-aug} is

The choice of perturbation kernel is

By substituting the perturbation kernel in \Eqref{eq:minimizer-D}, we have:

\end{proof}

\subsection{Proof of Theorem~\ref{thm:inf-D-field}}
\label{app:proof-thmfield}
\thmfield*
\begin{proof}
The  component in the Poisson field can be re-expressed as

 where the perturbation kernel . The direction of the score can also be written down in a similar form: 
 
where . Since , and obviously , then . It suffices to prove that the perturbation kernel  point-wisely converge to the Gaussian kernel , \ie , to ensure . Given , 
    
Hence , and we establish that . We can rewrite the drift term in the PFGM++ ODE as 

which establishes the first part of the theorem. For the second part, by the change-of-variable , the PFGM++ ODE is 

which is equivalent to the diffusion ODE.
\end{proof}


\subsection{Proof of Proposition~\ref{prop:obj}}
\label{app:proof-propobj}
\propobj*
\begin{proof}
    For , the minimizer in PFGM++ objective~(\Eqref{eq:pfgmpp-obj}) at point  is


On the other hand, the minimizer in denoising score matching at point  in noise level  is


Combining \Eqref{eq:minimizer-pfgmpp} and \Eqref{eq:minimizer-diffusion}, we have 


\end{proof}






















\section{Practical Sampling Procedures of Perturbation Kernel and Prior Distribution}
\label{app:sample-prior}
In this section, we discuss how to simple from the perturbation kernel  in practice. We first decompose  in hyperspherical coordinates to , where  is the uniform distribution over the angle component and the distribution of the perturbed radius  is

The sampling procedure of the radius distribution encompasses three steps:

Next, we prove that . Note that the pdf of the inverse beta distribution is

By change-of-variable, the pdf of  is


Note that  has mean  and variance . Hence when ,   would highly concentrate on a specific value, resolving the heavy-tailed problem. We can sample the uniform angel component by . Together, sampling from the perturbation kernel  is equivalent to setting . On the other hand, the prior distribution is 

We observe that  the same as the perturbation kernel . Hence we can sample from the prior following  with  defined above and .


\begin{comment}







\end{comment}

\section{ for Phase Alignment}
\subsection{Analysis}
\label{app:phase-align}



In this section, we examine the phase of intermediate marginal distribution  under different s to derive an alignment method for hyper-parameters. Consider a -dimensional dataset  in which the average distance to the nearest neighbor is about . We consider an arbitrary datapoint  and denote its nearest neighbor as . We assume , and uniform prior on .


To characterize the phases of , we study the perturbation point . According to Appendix~\ref{app:sample-prior}, the distance  is roughly . Since  is isotropic, with high probability, the two vectors  are approximately orthogonal. In particular, the vector product  w.h.p. It reveals that . \Figref{fig:align-analysis} depicts the relative positions of  and the perturbed point .

\begin{figure*}[t]
\centering    \includegraphics[width=0.8\textwidth]{img/align_plot.png}
    \caption{Illustration of the phase alignment analysis}
    \label{fig:align-analysis}
\end{figure*}

The ratio of the posterior of the  and   ---  ---  is an indicator of different phases of field~\cite{Xu2023StableTF}: point in the nearer field tends to have a smaller ratio. Indeed, the ratio would gradually decay from  to  when moving from the far to the near field.  We can calculate the ratio of the coefficients after approximating the distance :


Hence the relation  should hold to keep the ratio invariant of the parameter . On the other hand, by Theorem~\ref{thm:inf-D-field} we know that  is equivalent to  when . To achieve phase alignment on the dataset, one should roughly set .

\subsection{Practical Hyperparameter Transfer from Diffusion Models}
\label{app:transfer-diff}

\subsubsection{Transfer EDM training and sampling}

We list out and compare the EDM training algorithm~(Alg~\ref{algorithm-edm}) and the PFGM++ with transferred hyper-parameters~(Alg~\ref{algorithm-edm-pfgmpp}). The major modification is to replace the Gaussian noise  with the additive noise , where . We highlight the major modifications in \textcolor{blue}{blue}.

We also show the sampling algorithms of EDM~(Alg~\ref{algorithm-edm-sample}) and PFGM++~(Alg~\ref{algorithm-edm-pfgmpp-sample}). Note that we only change the prior sampling process while the for-loop is identical for both algorithms, since EDM~\citep{Karras2022ElucidatingTD} sets , and . Thus we can use the original samplers of EDM without further modification.

\begin{minipage}{0.46\textwidth}
\vspace{-46pt}
\begin{algorithm}[H]
    \centering
    \caption{EDM training}\label{algorithm-edm}
    \begin{algorithmic}[1]
        \STATE Sample a batch of data  from 
        \STATE Sample standard deviations  from  
        \STATE Sample noise vectors 
        \STATE Get perturbed data 
        \STATE Calculate loss 
        \STATE Update the network parameter  via Adam optimizer
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.50\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{PFGM++ training with hyperparameter transferred from EDM}\label{algorithm-edm-pfgmpp}
    \begin{algorithmic}[1]
        \STATE Sample a batch of data  from 
        \STATE Sample standard deviations  from  
        \STATE \textcolor{blue}{Sample  from : }
        \STATE  \textcolor{blue}{Sample radiuses }
        \STATE  \textcolor{blue}{Sample uniform angles , with }
        \STATE \textcolor{blue}{Get perturbed data }
        \STATE {Calculate loss }
        \STATE Update the network parameter  via Adam optimizer
    \end{algorithmic}
\end{algorithm}
\end{minipage}

\begin{minipage}{0.46\textwidth}
\vspace{-47pt}
\begin{algorithm}[H]
    \centering
    \caption{EDM sampling~(Heunâ€™s  order method)}\label{algorithm-edm-sample}
    \begin{algorithmic}[1]
\STATE 
\FOR{}
\STATE 
\STATE 
\IF{}
\STATE 
\STATE 
\ENDIF
\ENDFOR
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.50\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{PFGM++ training with hyperparameter transferred from EDM}\label{algorithm-edm-pfgmpp-sample}
    \begin{algorithmic}[1]
 \STATE \textcolor{blue}{Set  }
        \STATE  \textcolor{blue}{Sample radius 
       and uniform angle , with }
        \STATE \textcolor{blue}{Get initial data }
\FOR{}
\STATE 
\STATE 
\IF{}
\STATE 
\STATE 
\ENDIF
\ENDFOR
    \end{algorithmic}
\end{algorithm}
\end{minipage}

\subsubsection{Transfer DDPM~(continuous) training and sampling}

Here we demonstrate the ``zero-shot" transfer of hyperparameters from DDPM to PFGM++, using the  formula. We highlight the modifications in \textcolor{blue}{blue}. In particular, we list the DDPM training/sampling algorithms~(Alg~\ref{algorithm-ddpm}/Alg~\ref{algorithm-ddpm-sample}), and their counterparts in PFGM++~(Alg~\ref{algorithm-ddpm-pfgmpp}/Alg~\ref{algorithm-ddpm-pfgmpp-sample}) for comparions. Let  and  be the maximum/minimum values of  in DDPM~\cite{Ho2020DenoisingDP}. Similar to \citet{Song2021ScoreBasedGM}, we denote , with  and . For example, on CIFAR-10,  and  with . We would like to note that the s in the sampling algorithms~(Alg~\ref{algorithm-ddpm-sample} and Alg~\ref{algorithm-ddpm-pfgmpp-sample}) monotonically decrease from  to  as  increases.

\begin{minipage}{0.46\textwidth}
\vspace{-69pt}
\begin{algorithm}[H]
    \centering
    \caption{DDPM training}\label{algorithm-ddpm}
    \begin{algorithmic}[1]
        \STATE Sample a batch of data  from 
        \STATE Sample time  with  
        \STATE Get perturbed data ,
        where 
        \STATE Calculate loss 
        \STATE Update the network parameter  via Adam optimizer
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.50\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{PFGM++ training with hyperparameter transferred from DDPM}\label{algorithm-ddpm-pfgmpp}
    \begin{algorithmic}[1]
        \STATE Sample a batch of data  from 
       \STATE Sample time  from 
       \STATE \textcolor{blue}{Get  from : }
        \STATE \textcolor{blue}{Sample  from : }
        \STATE  \textcolor{blue}{Sample radiuses }
        \STATE  \textcolor{blue}{Sample uniform angles , with }
        \STATE \textcolor{blue}{Get perturbed data }
        \STATE \textcolor{blue}{Calculate loss }
        \STATE Update the network parameter  via Adam optimizer
    \end{algorithmic}
\end{algorithm}
\end{minipage}

\begin{minipage}{0.46\textwidth}
\vspace{-44pt}
\begin{algorithm}[H]
    \centering
    \caption{DDIM sampling}\label{algorithm-ddpm-sample}
    \begin{algorithmic}[1]
\STATE 
\FOR{}
\STATE \\

\ENDFOR
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.50\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{PFGM++ sampling transferred from DDIM}\label{algorithm-ddpm-pfgmpp-sample}
    \begin{algorithmic}[1]
     \STATE \textcolor{blue}{Set }
        \STATE  \textcolor{blue}{Sample radius 
       and  uniform angle , with }
        \STATE \textcolor{blue}{Get initial data }
\FOR{}
\STATE \\

\ENDFOR
    \end{algorithmic}
\end{algorithm}
\end{minipage}






\section{Experimental Details}
\label{app:exp}
We show the experimental setups in section~\ref{sec:benefits}, as well as the training, sampling, and evaluation details for PFGM++. All the experiments are run on four NVIDIA A100 GPUs or eight NVIDIA V100 GPUs.


\subsection{Experiments for the Analysis in Sec~\ref{sec:benefits}}
\label{app:be-exp}
In the experiments of section~\ref{sec:diffusion} and section~\ref{sec:behavior}, we need to access the posterior  to calculate the mean TVD. We sample a large batch  with  on CIFAR-10 to empirically approximate the posterior:


We sample a large batch of  to approximate all the expectations in section~\ref{sec:benefits}, such as the average TVDs.
\subsection{Training Details}

We borrow the architectures, preconditioning techniques, optimizers, exponential moving average~(EMA) schedule, and hyper-parameters from previous state-of-the-art diffusion model EDM~\cite{Karras2022ElucidatingTD}. We apply the alignment method in section~\ref{sec:diffusion} to transfer their well-tuned hyper-parameters. 

For architecture, we use the improved NCSN++~\cite{Karras2022ElucidatingTD} for the CIFAR-10 dataset~(batch size ), and the improved DDPM++ for the FFHQ dataset~(batch size ). For optimizers, following EDM, we adopt the Adam optimizer with a learning rate of . We further incorporate the EMA schedule, learning rate warm-up, and data augmentations in EDM. Please refer to Appendix F in EDM paper~\cite{Karras2022ElucidatingTD} for details.

The most prominent improvements in EDM are the preconditioning and the new training distribution for , \ie . Specifically, adding these two techniques to the vanilla diffusion objective~(\Eqref{eq:diffusion-obj}), their effective training objective can be written as:

with the predicted normalized score function in the vanilla diffusion objective~(\Eqref{eq:diffusion-obj}) re-parameterized as 

, with .  are all the hyper-parameters in the preconditioning. The training distribution  is the log-normal distribution with , and the loss weighting . 

Recall that the hyper-parameter alignment rule  can transfer the hyper-parameter from diffusion models~() to finite s. Hence we can directly set  in those hyper-parameters for preconditioning. In addition, the training distribution  can be derived via the change-of-variable formula, \ie . The final PFGM++ objective after incorporating these techniques into \Eqref{eq:pfgmpp-obj} is:

with the predicted normalized electric field in the vanilla PFGM++ objective~(\Eqref{eq:pfgmpp-obj}) re-parameterized as 

\subsection{Sampling Details}

For sampling, following EDM~\cite{Karras2022ElucidatingTD}, we also use Heun's  method~(improved Euler method)~\cite{Ascher1998ComputerMF} as the ODE solver for .

We adopt the same parameterized scheme in EDM to determine the evaluation points during -step ODE sampling:

where  controls the relative density of evaluation points in the near field. We set  as in EDM, and ~( are the hyper-parameters in EDM, controlling the starting/terminal evaluation points) following the  alignment rule.
\subsection{Evaluation Details}
\label{app:eval}
For the evaluation, we compute the FrÃ©chet distance between 50000 generated samples and the pre-computed statistics of CIFAR-10 and FFHQ. On CIFAR-10, we follow the evaluation protocol in EDM~\cite{Karras2022ElucidatingTD}, which repeats the generation three times with different seeds for each checkpoint and reports the minimum FID score. However, we observe that the FID score has a large fluctuation across checkpoints, and the minimum FID score of EDM in our re-run experiment does not align with the original results reported in \cite{Karras2022ElucidatingTD}. \Figref{fig:ffhq} shows that the FID score could have a variation of  during the training of a total of 200 million images~\cite{Karras2022ElucidatingTD}. To better evaluate the model performance, Table~\ref{tab:ffhq} reports the average FID over the Top-3 checkpoints instead. In \Figref{fig:ffhq-avg}, we further demonstrate the moving average of the FID score with a window of K images. It shows that  consistently outperforms other baselines in the same training iterations, in agreement with the results in Table~\ref{tab:ffhq}.

\begin{figure*}
\centering
    \subfigure[w/o moving average]{\includegraphics[width=0.8\textwidth]{img/ffhq.pdf}\label{fig:ffhq}}
        \subfigure[w/ moving average]{\includegraphics[width=0.8\textwidth]{img/ffhq_avg.pdf}\label{fig:ffhq-avg}}
        \caption{FID score in the training course when varying , \textbf{(a)} w/o and \textbf{(b)} w/ moving average.}
\end{figure*}


\subsection{Experiments for Robustness}
\label{app:robust-exp}

\paragraph{Controlled experiments with } In the controlled noise setting, we inject noise into the intermediate point  in each of the  ODE steps by  where . Since  has roughly the same phase as  in diffusion models, we pick  standard deviation of  when the intermediate step is .

\paragraph{Post-training quantization} In the post-training quantization experiments on CIFAR-10, we quantize the weights of convolutional layers excluding the  layers, as we empirically observe that these input/output layers are more critical for sample quality.
\section{Extra Experiments}

\subsection{Stable Target Field}
\label{app:stf}

\citet{Xu2023StableTF} propose a Stable Target Field objective for training the diffusion models:

where they sample a large batch of samples  from the data distribution to approximate the score function at . They show that the new target can enhance the stability of converged models in different runs/seeds. PFGM++ can be trained in a similar fashion by replacing the target  in perturbation-based objective~(\Eqref{eq:pfgmpp-obj}) with

When , the new target reduces to the original target. Similar to \cite{Xu2023StableTF}, one can show that the bias of the new target together with its trace-of-covariance shrinks to zero as we increase the size of the large batch. This new target can alleviate the variations between random seeds. With the new STF-style target, Table~\ref{tab:cifar-stf} shows that when setting , the model obtains the same FID score as the diffusion models~(EDM~\cite{Karras2022ElucidatingTD}). It aligns with the theoretical results in Sec~\ref{sec:diffusion}, which states that PFGM++ recover the diffusion model when . 
\begin{table}[htbp]
    \small
    \centering
    \caption{FID and NFE on CIFAR-10, using the Stable Target Field~\cite{Xu2023StableTF} in training objective.}
    \begin{tabular}{l c c c}
    \toprule
         &FID  & NFE \\
        \midrule
          &1.90 & 35\\
        ~\cite{Karras2022ElucidatingTD}  &1.90 & 35\\
        \bottomrule
    \end{tabular}
    \label{tab:cifar-stf}
\end{table}


\subsection{Extended CIFAR-10 Samples when varying }
\label{app:robust}


To see how the sample quality varies with , we visualize the generative samples of models trained with  and . We pick . \Figref{fig:robust_vis} shows that the smaller s produce better samples compared to larger . Diffusion models~() generate noisy images that appear to be out of the data distribution when , in contrast to the clean images by .
\begin{figure*}
    \centering
\subfigure[~(FID=1.96)]{\includegraphics[width=0.26\textwidth]{img/D_64_alpha_0.png}}\hfill
\subfigure[~(FID=1.97)]{\includegraphics[width=0.26\textwidth]{img/D_64_alpha_0.1.png}}\hfill
\subfigure[~(FID=2.07)]{\includegraphics[width=0.26\textwidth]{img/D_64_alpha_0.2.png}}

\subfigure[~(FID=1.92)]{\includegraphics[width=0.26\textwidth]{img/D_128_alpha_0.png}}\hfill
\subfigure[~(FID=1.95)]{\includegraphics[width=0.26\textwidth]{img/D_128_alpha_0.1.png}}\hfill
\subfigure[~(FID=2.19)]{\includegraphics[width=0.26\textwidth]{img/D_128_alpha_0.2.png}}

\subfigure[~(FID=1.92)]{\includegraphics[width=0.26\textwidth]{img/D_2048_alpha_0.png}}\hfill
\subfigure[~(FID=1.95)]{\includegraphics[width=0.26\textwidth]{img/D_2048_alpha_0.1.png}}\hfill
\subfigure[~(FID=2.19)]{\includegraphics[width=0.26\textwidth]{img/D_2048_alpha_0.2.png}}

\subfigure[~(FID=1.98)]{\includegraphics[width=0.26\textwidth]{img/D_inf_alpha_0.png}}\hfill
\subfigure[~(FID=9.27)]{\includegraphics[width=0.26\textwidth]{img/D_inf_alpha_0.1.png}}\hfill
\subfigure[~(FID=92.41)]{\includegraphics[width=0.26\textwidth]{img/D_inf_alpha_0.2.png}}
    \caption{Generated samples on CIFAR-10 with varied hyper-parameter for noise injection~(). Images from top to bottom rows are produced by models trained with . We use the same random seeds for finite s during image generation.}
    \label{fig:robust_vis}
\end{figure*}

\subsection{Extended FFHQ Samples}

In \Figref{fig:ffhq-vis}, we provide samples generated by the  case and EDM~(the  case).
\begin{figure*}
\centering
    \subfigure[~(FID=2.43)]{\includegraphics[width=0.45\textwidth]{img/ffhq_D_128.png}}\hfill
    \subfigure[EDM~()~(FID=2.53)]{\includegraphics[width=0.45\textwidth]{img/ffhq_edm.png}}
    \caption{Generated images on FFHQ  dataset, by \textbf{(left)}  and \textbf{(right)} EDM~().}
    \label{fig:ffhq-vis}
\end{figure*}


\section{Potential Negative Social Impact}
\label{app:impact}

The deep generative model is a burgeoning field and has significant potential for shaping our society. Our work presents a novel family of generative models, the PFGM++, which subsume previous high-performing models and provide greater flexibility. The PFGM++ have many potential applications, particularly in areas that require both robustness and high-quality output. However, it is important to note that the usage of these models can have both positive and negative implications, depending on the specific application. For instance, the PFGM++ can be used to create realistic image and audio samples, but it can also contribute to the development of deepfake technology and potentially lead to social scams. Additionally, the data-collecting process for generative models may infringe upon intellectual property rights. To address these concerns, further research is needed to provide robustness guarantees for generative models and to foster collaborations with experts in socio-technical fields.
