\section{Experiments}
\label{sec:experiments}
Firstly, we analyze the layer-wise representations of the SOTA SSL model in order to illustrate the suboptimal pre-trained representations of the later layers (Section~\ref{sec:sota_cka}). To address this shortcoming, we propose a novel pre-training pipeline, MeSa, that not only achieves improved quantitative performance in both in-distribution (Section~\ref{sec:magis}) and out-of-distribution (Section~\ref{sec:ood}) settings but also learns better layer-wise representations (Section~\ref{sec:magis_cka}). Lastly, we investigate the impact of different pre-training datasets on the layer-wise features (Section~\ref{sec:lsun_pretraining}) and conclude with a comprehensive comparison against the SOTA depth estimation models (Section~\ref{sec:sota}).
\subsection{Layer-wise Analysis of the SOTA SSL Model}
\label{sec:sota_cka}
\begin{table}[b]
\caption{Geometric and supervised pre-training have complementary benefits to masked pre-training and result in significant improvements. MP: masked pre-training, GP: geometric pre-training, SP: supervised pre-training.}
\label{table:nyu}
\centering
\begin{tabular}{lcccccc}
\toprule Pre-training Strategy & RMSE  &  &  &  &  &  \\
\midrule
MP~\citep{xie2022revealing} & 0.287 & 0.949 & 0.994 & 0.999 & 0.083 & 0.035 \\
MP + GP & 0.269 & 0.951 & 0.993 & 0.998 & 0.076 & 0.033 \\
MP + GP + SP (MeSa) & \textbf{0.265} & \textbf{0.954} & \textbf{0.995} & \textbf{0.999} & \textbf{0.074} & \textbf{0.032} \\\midrule
Relative Improvement (\%) & 7.64 & 0.54 & 0.06 & 0 & 10.2 & 8.05 \\
\bottomrule
\end{tabular}
\end{table}
Although the SOTA SSL model yields excellent performance, it is unclear which parts of the pre-trained model contribute to this improvement. In order to understand this, we analyze the layer-wise representations of this masked pre-trained model using the techniques discussed in Section~\ref{sec:layer_wise_analysis}.

Based on the analysis in Figure~\ref{fig:sota_cka} (left), we discover a U-shaped pattern in the representation space, where the representations initially start getting farther apart but then become more similar to the initial layers towards the end of the network. The top row of Figure~\ref{fig:sota_cka} (left) compares the representation of layer 0 against the representations of layers 0-8. The similarity of layer 0â€™s representation with itself is 1 as expected. Thereafter, for layers 1-4, the representations begin to diverge from layer 0's representation, as indicated by the decreasing similarities (darker colors). However, interestingly, past layer 4, the similarity increases (lighter colors) suggesting that the representations are becoming more similar to the layer 0 representation. This U-shaped pattern is likely a result of the pre-training objective, which is to reconstruct masked portions of the image. Hence, the later layers end up having similar representations to the first few layers. A similar pattern has also been observed in~\citep{pasad2021layer,pasad2023comparative} in the context of speech processing.

Whereas this U-shaped pattern is useful for solving the self-supervised pretext task, it is not obvious if it aids in learning effective representations for downstream tasks. To investigate this, we compare the similarity of the pre-trained and fine-tuned representations of each layer in Figure~\ref{fig:sota_cka} (right). From the diagonal line (top-left to bottom-right), we observe that the similarities are quite small for the later layers (5-8). Lower similarities imply that these layers undergo significant changes during fine-tuning, indicating that they are not effective for the downstream depth estimation task~\citep{neyshabur2020being}.
\subsection{MeSa}
\label{sec:magis}
To address the aforementioned drawbacks, we propose a novel pre-training pipeline, MeSa, that utilizes the complementary benefits of masked, geometric, and supervised pre-training.

Table~\ref{table:nyu} shows the depth estimation results of the three pre-training strategies on the NYUv2 dataset. The results demonstrate significant performance improvements achieved by all three learning strategies, highlighting their complementary strengths. Overall, our approach surpasses the SOTA SSL model, which relies solely on masked pre-training, by 10.2\% on absolute relative error (0.0830.074) and 7.64\% on RMSE (0.2870.265).
\begin{figure}[t]
  \centering
  \renewcommand{\arraystretch}{0.0} \begin{tabular}{c}
    \begin{subfigure}{0.89\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Images/NYUv2/kitchen_rgb_00133_pred_depths.png}
    \end{subfigure} \\
    \begin{subfigure}{0.89\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Images/NYUv2/kitchen_rgb_00133_abs_rel_err_depths.png}
    \end{subfigure} \\
  \end{tabular}
  \caption{Qualitative comparisons of the three pre-training strategies on NYUv2. Leftright: Image, MP, MP+GP, MP+GP+SP (MeSa). \textbf{Top}: predicted depth maps, \textbf{bottom}: error maps (bluelower error; redhigher error).}
  \label{fig:nyu_vis}
\end{figure}
Figure~\ref{fig:nyu_vis} presents qualitative results of the three pre-training strategies on the NYUv2 dataset, demonstrating significant improvements through the integration of geometric and supervised pre-training. When relying solely on masked pre-training, as in the SOTA SSL method, artifacts appear at the top and bottom of the predicted depth maps due to the absence of ground-truth depth (during fine-tuning) in these regions. By utilizing geometric pre-training, our method effectively eliminates these artifacts since the photometric loss utilizes 3D projective geometry and learns accurate depth across the entire image without relying on ground-truth. Moreover, geometric and supervised pre-training also help enhance the sharpness of the predicted depth maps.
\subsection{Out-of-distribution Performance}
\label{sec:ood}
In this section, we evaluate the performance of the three pre-training strategies in the out-of-distribution (OOD) setting. To this end, we evaluate the models trained on the NYUv2 dataset directly on the IBims-1 dataset without any additional fine-tuning.
\begin{table}[b]
\caption{In addition to improving the overall depth estimation accuracy, geometric and supervised pre-training also enhance the accuracy of depth boundaries and planar regions in the OOD setting. MP: masked pre-training, GP: geometric pre-training, SP: supervised pre-training.}
\label{table:ibims}
\centering
\begin{tabular}{lccccc}
\toprule Pre-training Strategy &  &  &  &  & AbsRel  \\
\midrule
MP~\citep{xie2022revealing} & 2.40 & 30.05 & 3.26 & 8.16 & 0.089 \\
MP + GP & 2.25 & 24.69 & 2.46 & \textbf{6.29} & \textbf{0.082} \\
MP + GP + SP (MeSa) & \textbf{2.16} & \textbf{19.31} & \textbf{2.17} & 6.44 & 0.083 \\\midrule
Relative Improvement (\%) & 10.3 & 35.8 & 33.4 & 22.9 & 8.4 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{table:ibims} presents the OOD results on the IBims-1 dataset. In addition to the overall depth estimation accuracy (measured via AbsRel), it is also important to improve the accuracy of depth boundaries (measured via  and ) as well as planar regions (measured via  and ), both of which are critical for many real-world applications. Our results highlight that we surpass the SOTA masked pre-training method on all five metrics in the OOD setting. Notably, we achieve substantial improvements of 35.8\% (30.0519.31) and 33.4\% (3.262.17) on depth boundary completeness () and planarity (), respectively.

Figure~\ref{fig:ibims_vis} visualizes the OOD performance of the three pre-training strategies. From the results, we observe a significant improvement in the sharpness of depth maps when leveraging geometric and supervised pre-training. Moreover, the localization of depth boundaries is also greatly enhanced.
\subsection{Layer-wise Analysis of MeSa}
\label{sec:magis_cka}
\begin{figure}[t]
  \centering
  \renewcommand{\arraystretch}{0.0} \begin{tabular}{c}
    \begin{subfigure}{0.92\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Images/IBims/livingroom_14_pred_depths.png}
    \end{subfigure} \\
    \begin{subfigure}{0.92\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Images/IBims/livingroom_14_abs_rel_err_depths.png}
    \end{subfigure} \\
    \begin{subfigure}{0.92\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Images/IBims/livingroom_14_pred_edges.png}
    \end{subfigure} \\
  \end{tabular}
  \caption{Qualitative comparisons of the OOD performance of the three pre-training strategies on IBims-1. Leftright: Image, MP, MP+GP, MP+GP+SP (MeSa). \textbf{Top}: predicted depth maps, \textbf{middle}: error maps (bluelower error; redhigher error), \textbf{bottom}: edge maps.}
  \label{fig:ibims_vis}
\end{figure}
\begin{figure}[t]
    \begin{minipage}{.33\linewidth}
    \centering
        \includegraphics[width=0.95\textwidth]{Images/CKA/fig5a.png}
        \vspace{5pt}
        \begin{picture}(0,0)
        \put(-125,6){\rotatebox{90}{{\footnotesize \texttt{Pre-trained Layers}}}}
        \put(-110,-7){{\footnotesize \texttt{Fine-tuned Layers}}}
        \end{picture}
    \end{minipage}\hfill
    \begin{minipage}{.33\linewidth}
    \centering
        \includegraphics[width=0.95\textwidth]{Images/CKA/fig5b.png}
        \vspace{5pt}
        \begin{picture}(0,0)
        \put(-125,6){\rotatebox{90}{{\footnotesize \texttt{Pre-trained Layers}}}}
        \put(-110,-7){{\footnotesize \texttt{Fine-tuned Layers}}}
        \end{picture}
    \end{minipage}\hfill
    \begin{minipage}{.33\linewidth}
    \centering
        \includegraphics[width=0.95\textwidth]{Images/CKA/fig5c.png}
        \vspace{5pt}
        \begin{picture}(0,0)
        \put(-125,6){\rotatebox{90}{{\footnotesize \texttt{Pre-trained Layers}}}}
        \put(-110,-7){{\footnotesize \texttt{Fine-tuned Layers}}}
        \end{picture}
    \end{minipage}
    \caption{Layer-wise analysis of the three pre-training strategies, comparing the representations of the pre-trained model to the fine-tuned model. MeSa effectively pre-trains the entire network, including the last few layers. Leftright: MP, MP+GP, MP+GP+SP (MeSa).}
    \label{fig:magis_cka}
\end{figure}
In this section, we analyze layer-wise representations of the MeSa pre-trained network to ascertain that our pre-training strategy indeed produces improved representations for the later layers, thereby successfully overcoming the drawbacks of the SOTA SSL method.

Similar to Section~\ref{sec:sota_cka}, for each layer, we compare the similarity of its pre-trained representation to its fine-tuned representation (Figure~\ref{fig:magis_cka}). As mentioned earlier, when using masked pre-training (MP) alone, the later layers (5-8) are not particularly beneficial for depth estimation since they undergo significant changes during fine-tuning, illustrated by the low similarity values. On the other hand, incorporating geometric pre-training (GP) alongside MP (i.e., MP+GP) allows for more effective representations to be learned deeper into the network, as shown by the higher similarities (lighter colors) until layer 6. However, layers 7-8 are still not optimally utilized. By leveraging all three pre-training strategies (i.e., MP+GP+SP), even these last two layers have comparatively higher similarities. Hence, MeSa effectively pre-trains the entire network, including the last few layers, for downstream depth estimation. The Appendix shows more CKA layer-wise analyses illustrating the efficacy of MeSa pre-training.
\subsection{Impact of Different Pre-training Datasets on Layer-wise Features}
\label{sec:lsun_pretraining}
In this section, we study the impact of different pre-training datasets on the layer-wise pre-trained features. To investigate this, we compare the representations of two networks pre-trained on LSUN and ImageNet respectively. LSUN is less diverse (featuring mostly indoor scenes) in comparison to ImageNet, allowing us to understand the impact of pre-training on these two datasets of varying diversities. To ensure a fair comparison, we use the same size of pre-training dataset (1200K) in both cases. For ImageNet, we use the pre-trained model provided by~\citet{xie2022simmim,xie2022revealing} whereas for LSUN, we pre-train a Swin-v2-L network using SimMIM~\citep{xie2022simmim} on a subset of the LSUN dataset. The subset consists of 300K images each from the bedroom, dining room, living room, and kitchen categories.

Similar to the analysis in Section~\ref{sec:sota_cka}, Figure~\ref{fig:imagenet_vs_lsun_p_p_cka} (in the Appendix) compares the representations of various layers within the pre-trained networks, whereas Figure~\ref{fig:imagenet_vs_lsun_p_ft_cka} compares the representations of the pre-trained networks and the (depth estimation) fine-tuned networks. The analysis in Figure~\ref{fig:imagenet_vs_lsun_p_p_cka} (top row) reveals that the intermediate layers of the LSUN pre-trained network exhibit lower similarities (darker colors) with layer 0 compared to the similarities of the corresponding layers of the ImageNet pre-trained network. Since earlier layers (e.g., layer 0) capture low-level features, this suggests that pre-training with LSUN learns more high-level representations in the middle layers that are beneficial for tasks requiring higher-level reasoning, as illustrated by the superior depth estimation results on NYUv2. This is further corroborated by Figure~\ref{fig:imagenet_vs_lsun_p_ft_cka}, which demonstrates that the LSUN pre-trained features are more effective for the downstream task than the ImageNet pre-trained features, as evidenced by the higher similarities (lighter colors) between the pre-trained and fine-tuned features along the diagonal line for LSUN compared to ImageNet. Since the images in LSUN have a smaller diversity compared to those in ImageNet, we hypothesize that pre-training on LSUN forces the network to reconstruct masked portions of similar images, thereby encouraging it to utilize more layers for higher level reasoning than for RGB reconstruction (Figure~\ref{fig:imagenet_vs_lsun_p_p_cka}).
\subsection{Comparison with SOTA Methods}
\label{sec:sota}
\begin{table}
\caption{Comparison with SOTA depth estimation methods on the NYUv2 dataset. MeSa outperforms previous methods across all metrics.}
\label{table:sota}
\centering
\begin{tabular}{lcccccc}\toprule
Method & RMSE  &  &  &  &  &  \\ \midrule
BTS~\citep{kirillov2019panoptic} & 0.392 & 0.885 & 0.978 & 0.995 & 0.110 & 0.047 \\
AdaBins~\citep{bhat2021adabins} & 0.364 & 0.903 & 0.984 & 0.997 & 0.103 & 0.044 \\
DPT~\citep{ranftl2021vision} & 0.357 & 0.904 & 0.988 & 0.998 & 0.110 & 0.045 \\
P3Depth~\citep{patil2022p3depth} & 0.356 & 0.898 & 0.981 & 0.996 & 0.104 & 0.043 \\
NeWCRFs~\citep{yuan2022new} & 0.334 & 0.922 & 0.992 & 0.998 & 0.095 & 0.041 \\
Xie et al.~\citep{xie2022revealing} & 0.287 & 0.949 & 0.994 & \textbf{0.999} & 0.083 & 0.035 \\
AiT~\citep{ning2023all} & 0.275 & 0.954 & 0.994 & \textbf{0.999} & 0.076 & 0.033 \\
ZoeDepth~\citep{bhat2023zoedepth} & 0.270 & 0.955 & \textbf{0.995} & \textbf{0.999} & 0.075 & 0.032 \\
VPD~\citep{zhao2023unleashing} & 0.254 & \textbf{0.964} & \textbf{0.995} & \textbf{0.999} & 0.069 & 0.030 \\ \midrule
\METHOD & \textbf{0.238} & \textbf{0.964} & \textbf{0.995} & \textbf{0.999} & \textbf{0.066} & \textbf{0.029} \\
\bottomrule
\end{tabular}
\end{table}
Table~\ref{table:sota} compares MeSa against state-of-the-art methods on the NYUv2 dataset. As in recent works, we primarily focus on RMSE since performance on the other metrics is highly saturated. Our method is most directly comparable to~\citep{xie2022revealing}, which we surpass by a large margin of 17.1\%. Moreover, even without utilizing any recently proposed techniques, such as soft token and mask augmentation~\citep{ning2023all}, metric bins module~\citep{bhat2023zoedepth} or pre-trained text-to-image diffusion model~\citep{zhao2023unleashing}, our approach also outperforms the most recent methods. Since our method is agnostic to these techniques, it could also be used in conjunction with them to yield further improvements. Overall, we surpass the current best approach~\citep{zhao2023unleashing} by 6.3\%, establishing a new SOTA on the challenging NYUv2 dataset.