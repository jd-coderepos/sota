\documentclass[nohyperref]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} \usepackage{courier}


\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2022}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{algorithm} 
\usepackage{algorithmic} 
\usepackage{dsfont}
\usepackage{wrapfig}
\usepackage{subcaption}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}
\def\rmSigma{{\boldsymbol{\Sigma}}}
\def\rmLambda{{\boldsymbol{\Lambda}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vsigma{{\bm{\sigma}}}
\def\veps{{\bm{\eps}}}
\def\vtheta{{\bm{\theta}}}
\def\vlambda{{\bm{\lambda}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{listings,newtxtt}

\definecolor{deepblue}{rgb}{0,0,0.6}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\lstdefinestyle{python}{
language=Python,
basicstyle=\ttfamily\small,
commentstyle=\color{deepred},
otherkeywords={},             keywordstyle=\color{deepgreen},
emph={},          emphstyle=\color{deepblue},    stringstyle=\color{deepred},
showstringspaces=false            }



\usepackage{courier}

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{framextopmargin=50pt}





\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\renewcommand{\floatpagefraction}{0.9}


\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{simple diffusion}

\begin{document}

\twocolumn[
\icmltitle{simple diffusion: End-to-end diffusion for high resolution images}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Emiel Hoogeboom}{equal,goog}
\icmlauthor{Jonathan Heek}{equal,goog}
\icmlauthor{Tim Salimans}{goog}
\end{icmlauthorlist}

\icmlaffiliation{goog}{Google Research, Brain Team, Amsterdam, Netherlands}


\icmlcorrespondingauthor{Emiel Hoogeboom}{emielh@google.com}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 



\begin{abstract}
Currently, applying diffusion models in pixel space of high resolution images is difficult. Instead, existing approaches focus on diffusion in lower dimensional spaces (latent diffusion), or have multiple super-resolution levels of generation referred to as cascades. The downside is that these approaches add additional complexity to the diffusion framework.

This paper aims to improve denoising diffusion for high resolution images while keeping the model as simple as possible. The paper is centered around the research question: How can one train a standard denoising diffusion models on high resolution images, and still obtain performance comparable to these alternate approaches? 

The four main findings are: 1) the noise schedule should be adjusted for high resolution images, 2) It is sufficient to scale only a particular part of the architecture, 3) dropout should be added at specific locations in the architecture, and 4) downsampling is an effective strategy to avoid high resolution feature maps. Combining these simple yet effective techniques, we achieve state-of-the-art on image generation among diffusion models without sampling modifiers on ImageNet. \end{abstract}



\begin{figure}
    \centering
    \includegraphics[width=.21\textwidth]{images/frog_sweater.jpg} \hspace{.01\textwidth}
    \includegraphics[width=.21\textwidth]{images/owl_piano.jpg} \\
    \vspace{.24cm}
    \includegraphics[width=.44\textwidth]{images/robots_chess.jpg}
    \caption{\textit{A dslr photo of a frog wearing a sweater}, \textit{An owl playing the piano, vivid, fantasy art}, and \textit{two robots playing chess with New York in the background}. Except for the frozen text encoder, \textit{simple diffusion} is trained end-to-end and images are generated in full pixel space.}
    \label{fig:frog_sweater}
\end{figure}

\section{Introduction}
Score-based diffusion models have become increasingly popular for data generation. In essence the idea is simple: one pre-defines a diffusion process, which gradually destroys information by adding random noise. Then, the opposite direction defines the denoising process, which is approximated with a neural network.

Diffusion models have shown to be extremely effective for image, audio, and video generation. However, for higher resolutions the literature typically operates on lower dimensional latent spaces (latent diffusion) \citep{rombach2022highresolution} or divides the generative process into multiple sub-problems, for instance via super-resolution (cascaded diffusion) \citep{ho2022cascaded} or mixtures-of-denoising-experts \citep{balaji2022ediffi}. The disadvantage is that these approaches introduce additional complexity and usually do not support a single end-to-end training setup.

\begin{figure*}[]
    \centering
    \includegraphics[width=.9\textwidth]{images/overview_samples.jpg}\vspace{-.2cm}
    \caption{Generated images with \textit{simple diffusion}. Importantly, each image is generated in full image space by a single diffusion model without any cascades (super-resolution) or mixtures of experts. Samples are drawn from the U-Net model with guidance scale 4.}
    \label{fig:overview}
\end{figure*}

In this paper, we aim to improve standard denoising diffusion for higher resolutions while keeping the model as simple as possible. Our four main findings are that 1) the noise schedule should be adjusted for larger images,  adding more noise as the resolution increases. 2) It is sufficient to scale the U-Net architecture on the  resolution to improve performance. Taking this one step further is the U-ViT architecture, a U-Net with a transformer backbone. 3) Dropout should be added for improved performance, but not on the highest resolution feature maps. And finally 4) for higher resolutions, one can down-sample without performance degradation. 
Most importantly, these results are obtained using just a single model and an end-to-end training setup. After using existing distillation techniques which now only have to be applied to a single stage, the model can generate an image in 0.4 seconds.
 
\section{Background: Diffusion Models}
\label{sec:background}

A diffusion model generates data by learning the reverse of a destruction process. Commonly, the diffusion process gradually adds Gaussian noise over time. It is convenient to express the process directly in the marginals  which is given by:

where  are hyperparameters that determine how much signal is destroyed at a timestep , which can be continuous for instance . Here,  is decreasing and  is increasing, both larger than zero. We consider a variance preserving process, which fixes the relation between  to be . Assuming the diffusion process is Markov, the transition distributions are given by:

where  and  and .

\textbf{Noise schedule}
An often used noise schedule is the -cosine schedule where  which under the variance preserving assumption implies . An important finding from \citep{kingma2021vdm} is that it is the signal-to-noise ratio  that matters, which is then  or in log space . 


\textbf{Denoising}
Conditioned on a single datapoint , the denoising process can be written as:

where  and . An important and surprising result in literature is that when  is approximated by a neural network , then one can define the learned distribution  without loss of generality as . This works because as , the true denoising distribution for all datapoints  (which is typically unknown) will become equal to  \citep{song2021scorebasedsde}.

\textbf{Parametrization}
The network does not need to approximate  directly, and experimentally it has been found that other predictions produce higher visual quality. Studying the re-parametrization of the marginal  which is  where , one can for instance choose the \textit{epsilon} parametrization where the neural net predicts . To obtain , one computes . The problem with the epsilon parametrization is that it gives unstable sampling near . An alternative parametrization without this issue is called \textit{v prediction} and was proposed in \citep{salimans2022progressive}, it is defined as .

Note that given  one can obtain  and  via the identities  and  . In initial experiments we found \textit{v prediction} to train more reliably, especially for larger resolutions, and therefore we use this parametrization throughout this paper.

\begin{figure*}
    \centering
    \includegraphics[interpolate=false,width=.7\textwidth]{images/diffusion_shifted.pdf}
    \includegraphics[width=.235\textwidth]{images/plots_shifted.pdf}
    \vspace{-.2cm}
    \caption{The standard and shifted diffusion noise on an image of , that is visualized by average pooling to a resolution of . The top row shows a conventional cosine schedule, the bottom row shows our proposed shifted schedule.}
    \label{fig:diffusion_shifted}
\end{figure*}

\textbf{Optimization}
To train the model, we use the standard epsilon loss from \citep{ho2020denoising}. A way to motivate this choice of loss, is that using variational inference one can derive a lowerbound (in continuous time) on the model log-likelihood as done in \citep{kingma2021vdm}:
\small\normalsize
where for a well-defined process  for discrete , , and where  is a weighting function which for the equation to be true needs to be  where . In practice, we generally use the unweighted loss on  (meaning that ) which in \citep{ho2020denoising} was found to give superior sample quality. See Appendix~\ref{sec:addition_info_diffusion} for additional useful background information.

 
\section{Method: simple diffusion}
In this section, we introduce several modifications that enable denoising diffusion to work well on high resolutions.

\subsection{Adjusting Noise Schedules}
One of the modifications is the noise schedule that is typically used for diffusion models. The most common schedules is the -cosine schedule, which under the variance preserving assumption amounts to  (ignoring the boundaries around  and  for this analysis) \citep{nichol2021improvedddpm}. This schedule was originally proposed to improve the performance on CIFAR10 which has a resolution of  and ImageNet . 


However, for high resolutions not enough noise is added. For instance, inspecting the top row of Figure~\ref{fig:diffusion_shifted} shows that for the standard cosine schedule, the global structure of the image is largely defined already for a wide range in time. This is problematic because the generative denoising process only has a small time window to decide on the global structure of the image. We argue that for higher resolutions, this schedule can be changed in a predictable way to retain good visual sample quality.

To illustrate this need in more detail, let us study a  problem. Given an input image  the diffusion distribution for pixel  is given by . Commonly, diffusion models use network architectures that use \textit{downsampling} to operate on lower resolution feature maps, in our case with average pooling. Suppose we average pool , where we let indices  denote the pixels in a  square that is being pooled. This new pixel is . Recall that for variance of independent random variables is additive meaning that  and that  for a constant . Letting  denote the first pixel of the average pooled input image, we find that . The lower resolution pixel  only has half the amount of noise. We hypothesize that as resolutions increase this is problematic, as much fewer diffusion time is spent on the lower resolution, a stage at which the global consistency is generated.

\begin{figure*}
\centering
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti/city_dome.jpg}
\caption{\textit{A render of a bright and colorful city under a dome}}
 \end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti/raccoon_saxophone.jpg}
\caption{\textit{A raccoon playing the saxophone}}
 \end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti/panda_forest.jpg}
\caption{\textit{A panda walking through the Jungle, futuristic art}}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti/strawberry_smoothie.jpg}
\caption{\textit{A cartoon of a strawberry drinking a smoothie}}
\end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti/robot_skateboard.jpg}
\caption{\textit{A surrealistic painting of a robot riding a skateboard}}
\end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti/frog_wooden_statue.jpg}
\caption{\textit{A statue of a frog made of wood}}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti/sunflower_sunglasses.jpg}
\caption{\textit{A sunflower wearing sunglasses}}
\end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti/butterfly_neon.jpg}
\caption{\textit{A neon sign of a butterfly}}
\end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti/futuristic_coffee.jpg}
\caption{\textit{A painting of futuristic coffee machine, vivid colors}}
\end{subfigure}
\caption{Text to image samples at resolution . This model was distilled and as a result generating a single image takes 0.42 seconds on a TPUv4 (excluding the text encoder). Similarly, generating a batch of 8 images takes 2.00 seconds.}
\end{figure*}

\begin{figure*}
\centering
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti_256/dog_cycling.jpg}
\caption{\textit{A dog riding a bicycle through Amsterdam}}
\end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti_256/car_desert.jpg}
\caption{\textit{A futuristic car driving through the desert}}
 \end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti_256/distillation_gold.jpg}
\caption{\textit{A distillation machine on a table creating gold}}
\end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\includegraphics[width=.99\textwidth]{images/tti_256/elephant_abstract.jpg}
\caption{\textit{An abstract painting of an elephant}}
\end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti_256/city_overgrown.jpg}
\caption{\textit{A futuristic city overgrown by nature}}
\end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti_256/lion_vangogh.jpg}
\caption{\textit{A Van Gogh painting of a lion}}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti_256/sphere_city.jpg}
\caption{\textit{A city inside a glass pearl}}
\end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti_256/horse_hat.jpg}
\caption{\textit{A horse wearing a hat}}
\end{subfigure} \hfill
\begin{subfigure}[t]{0.33\textwidth}
\centering
\includegraphics[width=.99\textwidth]{images/tti_256/balloon_brain.jpg}
\caption{\textit{A balloon in the shape of the Google Brain logo}}
\end{subfigure}
\caption{Text to image samples generated with simple diffusion at resolution .}
\end{figure*}

One can further derive that the  to  ratio at this lower resolution is twice as high, meaning that the signal to noise ratio is  as high. And so , or in general:

In summary, after averaging over a window of size , the ratio  to  increases by a factor  (and thus the  by ). Hence, we argue that the noise schedule could be defined with respect to some reference resolution, say  or  for which the schedules were initially designed and successfully tested. 
In our approach one first chooses a reference resolution, for example  (a reasonable choice as we will see empirically). At the reference resolution we define the noise schedule  which in turn defines the desired  at full resolution :

the signal to noise ratio is simply multiplied by , which for our setting  reduces the signal-to-noise ratio at high resolution. In log-space, this implies a simple shift of  (see Figure~\ref{fig:diffusion_shifted_snr}). For example, the equation of a noise schedule for images of 128  128 and a reference resolution of 64 the schedule is:

Recall that under a variance preserving process, the diffusion parameters can be computed as  and .

Finally, it may be worthwhile to study the concurrent and complementary work \citep{chen2023importancenoise} which also analyzes adjusted noise schedules for higher resolution images and describes several other improvements as well.

\begin{figure}
    \centering
    \includegraphics[width=.3\textwidth]{images/plot_logsnr.pdf}\vspace{-.1cm}
    \caption{Log signal to noise ratio for the original and shifted cosine schedule.}
    \label{fig:diffusion_shifted_snr}
\end{figure}

\paragraph{Interpolating schedules}
A potential downside of shifting the schedule is that high frequency details are now generated much later in the diffusion process due to the increased per-pixel noise. However, we postulate that high-frequency details are weakly correlated when conditioning on the global/low-frequency features that are already generated. It should therefore be possible to generate the high-frequency details in few diffusion steps. Alternatively, one can \textit{interpolate} different shift schedules, for example for a resolution of  one could include higher frequency details by starting at shift 32 and interpolating in log-space to shift 256. The schedule for  equals:

which has more equal weighting over low, mid and high frequency details.

\subsection{Multiscale training loss}
In the last section we argued that the noise schedule of our diffusion model should be adjusted when training on high resolution images so that the signal-to-noise ratio at our base resolution is held constant. However, even when adjusting the noise schedule in this way, the training loss on images of increasingly high resolution is dominated by high frequency details. To correct for this we propose replacing the standard training loss by a multiscale version that consists of evaluating our standard training loss at downsampled resolutions with a weighting factor that increases for the lower resolutions. We find that the multiscale loss enables quicker convergence especially at resolutions greater than . Our original training loss at the  resolution can be written as 

where  denotes downsampling to the  resolution. If this resolution is identical to the native resolution of our model  and data , the downsampling does not do anything and can be removed from this equation. Otherwise,  can be considered as an adjusted denoising model for data at non-native resolution . Since downsampling an image is a linear operation, we have that , and this way of constructing the lower-resolution model is thus indeed consistent with our original model.

We then propose training our high resolution model against the multiscale training loss comprising of multiple resolutions. For instance for the resolutions  the loss would be: 
.

That is, we train against a weighted sum of training losses for resolutions starting at a base resolution (in this case ) and always including the final resolution of . We find that losses for higher resolution are noisier on average, and we therefore decrease the relative weight of the loss as we increase the resolution.


\subsection{Scaling the Architecture}
Another question is how to scale the architecture. Typical model architectures half the channels each time the resolution is doubled such that the flops per operation is the same but the number of features doubles. The computational intensity (flops / features) also halves each time the resolution doubles. Low computational intensity leads to poor utilization of the accelerator and large activations result in out-of-memory issues. As such, we prefer to scale on the lower resolutions feature maps. Our hypothesis is that mainly scaling on a particular resolution, namely the  resolution is sufficient to improve performance within a range of network sizes we consider. Typically, low resolution operations have relatively small feature maps. To illustrate this, consider for example

costs  GB for a feature map whereas for a  feature map with  channels, a feature map costs  GB, given they are stored in a 16 bit float format. 

Parameters have a smaller memory footprint: The typical size of a convolutional kernel is MB and MB for  channels, with  replications for the gradient, optimizer state and exponential moving average. The point is, at a resolution of  both the size of feature maps are manageable at  and the required space for the parameters is manageable.


\begin{table}
    \centering
    \caption{Memory and compute for a convolutional layer at the typical sizes encountered in diffusion architectures. Using more channels is usually much cheaper at lower resolutions in terms of memory,  for this example.}
    \label{tab:memory_computation}
    \scalebox{.8}{
    \begin{tabular}{l l l}
    \toprule
    Size & ( & (  \\ \midrule
    Conv Kernel Memory & 2.8MB & 180MB \\
    Feature Map Memory & 16GB & 0.5GB \\ 
    Total Memory & 16GB & 0.7GB \\  \midrule
    Compute (TFLOPS) & 9 & 2.3 \\
        \bottomrule
    \end{tabular}}
\end{table}

Summarizing this back-of-the-envelope calculation in Table~\ref{tab:memory_computation} one can see that for the same memory constraint, one can fit GB  GB  layers at  versus only  at .


Other reasons to choose this resolution is because it is the one at which self-attention starts being used in many existing works in the diffusion literature \citep{ho2020denoising,nichol2021improvedddpm}. Furthermore, it is the  resolution at which vision transformers for classification can operate successfully \citep{dosovitskiy2021imageisworth}. Although this may not be the ideal way to scale the architecture, we will show empirically that scaling the  level works well.

An observant ML practitioner may have realized that when using multiple devices naively, parameters are replicated (typical in JAX and Flax) or stored on the first device (PyTorch). Both cases result in a situation where the memory requirements per device for the feature maps decreases with  as desired, but the parameter requirement is unaffected and requires a lot of memory. We scale mostly at a low resolution where activations are relatively small but parameter matrices are large . We found that sharding the weights allows us to scale to much larger models without requiring more complicated parallelization approaches like model parallelism.

\paragraph{Avoiding high resolution feature maps}
High resolution feature maps are memory expensive. If the number of FLOPs is kept constant, memory still scales linearly with the resolution.

In practise, it is not possible to decrease the channels beyond a certain size without sacrificing accelerator utilization. Modern accelerators have a very high ratio between compute and memory bandwidth. Therefore, a low channel count can make operation memory bound, causing a mostly idling accelerator and worse than expected wall-clock performance.

To avoid doing computations on the highest resolutions, we down-sample images immediately as a the first step of the neural network, and up-sample as the last step. Surprisingly, even though the neural networks are cheaper computationally and in terms of memory, we find empirically that they also achieve better performance. We have two approaches to choose from.

One approach is to use the invertible and linear 5/3 wavelet (as used in JPEG2000) to transform the image to lower resolution frequency responses as demonstrated in Figure~\ref{fig:dwt}. Here, the different feature responses are concatenated spatially for visual purposes. In the network, the responses are concatenated over the channel axis. When more than one level of DWT is applied (here there are two), then the responses differ in resolution. This is resolved by finding the lowest resolution (in the figure ) and reshaping pixels for the higher resolution feature maps, in the case of  they are reshaped , as a typical space to depth operations. A guide on the implementation of the DWT can be found here\footnote{\url{http://trueharmoniccolours.co.uk/Blog/?p=14}}.

If the above seems to complicated, there also exists a simpler solution if one is willing to pay a small performance penalty. As a first layer one can use a  convolutional layer with stride , and an identically shaped \textit{transposed} convolutional layer as a last layer. This is equivalent to what is called patching in transformer literature. Empirically we show this performs similarly, albeit slightly worse.


\begin{figure}
    \centering
    \includegraphics[width=0.235\textwidth]{images/tiger_gray.jpg}\hfill
    \includegraphics[width=0.235\textwidth]{images/dwt.jpg}
    \caption{The `5/3' DWT transform transforms an image to low and high frequency response feature maps. Left: original image. Right: The different frequency responses of a two-level DWT, outputs are four  maps and three  maps. An alternative to up- and down-sample is to use (transposed) convolutional layers.}
    \label{fig:dwt}
\end{figure}

\subsection{Dropout}
In architecture typically used in diffusion, a global dropout hyperparameter is used for the residual blocks, at all resolutions. In CDM \citep{ho2022cascaded}, dropout is used to generate images at lower resolutions. For the conditional higher resolution images, no dropout is used. However, various other forms of augmentation are performed on the data. This indicates that regularization is important, even for models operating on high resolutions. However, as we will demonstrate empirically, the naive method of adding dropout in all residual blocks does not give desired results. 

Since our network design only scales the network size at lower resolutions, we hypothesize that it should be sufficient to only add dropout add the lower resolutions. This avoids regularizing the high resolution layers which are memory-wise expensive, while still using the dropout regularization that has been successful for models trained on lower resolution images.

\subsection{The U-ViT architecture}
Taken the above described changes to the architecture one step further, one can replace convolutional layers with MLP blocks if the architecture already uses self-attention at that resolution. This bridges the transformers for diffusion introduced by \citep{peebles2022scalable} with U-Nets, replacing its backbone with a transformer. Consequently, this relatively small change means that we now are using transformer blocks at these resolutions. The main benefit is that the combination of self-attention and MLP blocks has high accelerator utilization, and thus large models train somewhat faster. See Appendix~\ref{app:experimental_details} for more details regarding this architecture. In essence, this U-Vision Transformer (U-ViT) architecture can be seen as a small convolutional U-Net which through multiple levels down-samples to the  resolution. Here a large transformer is applied. After this, the upsampling is again done via the convolutional U-Net.

\subsection{Text to image generation}
As a proof of concept, we also train a simple diffusion model that is conditioned on text data. Following \citep{saharia2022imagen} we use the T5 XXL \citep{raffel2020exploring} text encoder as conditioning. For further details see Appendix~\ref{app:experimental_details}. We train three models: One on images of resolution  for a direct comparison to models in literature, one on  and one on . For the last, non-square resolution, images are rotated during prepossessing if their width is smaller than their height, along which a `portrait mode' flag is set to true. As a result, this model can generate natively in a 5:3 aspect ratio for both landscape and portrait orientation. 
\section{Related Work}
Score-based diffusion models \citep{sohldickstein2015diffusion,song2019generativemodellingestimatinggradient,ho2020denoising} are a generative model that pre-defines a stochastic destruction process. The generative process is learned by approximating the reverse process with the help of neural networks. 

Diffusion models have been succesfully applied to image generation \citep{ho2020denoising,ho2022cascaded}, speech generation \citep{chen2020wavegrad,kong2021diffwave}, video generation \citep{singer2022makeavideo,saharia2022imagen}.

Diffusion models for high resolutions (for example ) on complicated data (such as ImageNet) are generally not learned directly. Instead, approaches in literature divide the generative process into sub-problems via super-resolution \citep{ho2022cascaded}, or mixtures-of-denoisers \citep{feng2022ernievilg,balaji2022ediffi}. Alternatively, other approaches project high resolution data down to a lower dimensional latent space \citep{rombach2022highresolution}. These techniques can also be combined to even further sub-divide the generative problems. Although this sub-division generally makes optimization easier, a downside is that engineering complexity increases: Instead of dealing with a single model, one needs to train and keep track of multiple models. In this paper, we show that it is possible to train a single denoising diffusion model for resolutions up to  with only a small number modifications with respect to the original (modern) formulation in \citep{ho2020denoising}.

 
\section{Experiments}
\label{sec:results}


\subsection{Effects of the proposed modifications}

\begin{table}
    \centering
    \caption{Noise Schedule on ImageNet 128 and 256.}\vspace{-.2cm}
    \label{tab:i128_i256_noise_schedule}
    \scalebox{.9}{
    \begin{tabular}{l l l}
    \toprule
    Noise Schedule & FID train & FID eval \\ \midrule
    \textbf{128  128 resolution} \\
    cosine (original at 128) & 2.96 & 3.38 \\
    cosine (shifted to 64) & 2.41 & 3.03 \\
    cosine (shifted to 32) & \textbf{2.26} & \textbf{2.88} \\ \midrule
    \textbf{256  256 resolution} \\
    cosine (original at 256) & 7.65 & 6.87 \\
    cosine (shifted to 128) & 5.05 & 4.74 \\
    cosine (shifted to 64) & 3.94 & 3.89 \\
    cosine (shifted to 32) & \textbf{3.76} & \textbf{3.71} \\
        \bottomrule
    \end{tabular}}\vspace{-.2cm}
\end{table}

\textbf{Noise schedule}
In this experiment it is studied how the noise schedule effects the quality of generated images, evaluated on FID50K score on both train and eval data splits. Recall that our hypothesis was that the cosine schedule does not add sufficient noise, but can be adjusted by `shifting' its log SNR curve using the ratio between the image resolution and the noise resolution. In these experiments, the noise resolution is varied from the original image resolution (corresponding to the conventional cosine schedule) all the way down to  by factors of two.

As can be seen in Table~\ref{tab:i128_i256_noise_schedule} for ImageNet at resolution 128  128 and resolution 256  256, shifting the noise schedule considerably improves performance. The difference is especially noticeable at the higher resolution, where the difference is 7.65 for the original cosine schedule against 3.76 for the shifted schedule in FID on the train data. Notice that the difference in performance between the shift towards either 64 and 32 is relatively small, albeit slightly better for the 32 shift. Given that the difference is small and that the shift 64 schedule performed slightly better in early iterations, we generally recommend the shift 64 schedule.

\begin{table}
    \centering
    \caption{Dropout Ablation on ImageNet 128}\vspace{-.2cm}
    \label{tab:dropout}
    \scalebox{.9}{
    \begin{tabular}{l r r r r}
    \toprule
    Starting from Resolution & FID train & FID eval \\ \midrule
    128 & 3.19 & 3.85 \\
    64 & \textbf{2.27} & \textbf{2.85} \\
    32 & 2.31 & 2.87 \\
    16 & 2.41 & 3.03 \\
    no dropout (at 700K iters) & 3.74 & 3.91 \\
        \bottomrule
    \end{tabular}}
\end{table}

\textbf{Dropout}
The ImageNet dataset has roughly 1 million images. As noted by prior work, it is important to regularize the networks to avoid overfitting \citep{ho2022cascaded,dhariwal2021diffusionbeatgans}. Although dropout has been successfully applied to networks at resolutions of , it is often disabled for models operating on high resolutions. In this experiment we enable dropout only on a subset of the network layers: Only for resolutions below the given `starting resolution' hyperparameter. For example, if the starting resolution is , then dropout is applied to modules operating on resolutions ,  and .


Recall our hypothesis that it should be sufficient to regularize the modules of the network that operate on the lower resolution feature maps. As presented in Table~\ref{tab:dropout}, this hypothesis holds. For this experiment on images of , adding dropout from resolutions  all perform comparatively. Although adding dropout from  performed a little worse, we use this setting throughout the remainder of the experiments because it converged faster in early iterations.


The experiment also shows two settings that do not work well at all and should be avoided: either adding no dropout, or adding dropout starting from the same resolution as the data. This may explain why dropout for high resolution diffusion has not been widely used thus far: Typically dropout is set as a global parameter for all feature maps at all resolutions, but this experiment shows that such a regularization is too aggressive.

\textbf{Architecture scaling}
In this section we study the effect of increasing the amount of  network modules. In U-Nets, the number of blocks hyperparameter typically refers to the number of blocks on the `down' path. In many implementations, the `up' blocks use one additional block. When the table reads `2 + 3' blocks, that means 2 down blocks and 3 up blocks, which would in literature be referred to as 2 blocks.

Generally, increasing the number of modules improves the performance as can be seen in Table~\ref{tab:scaling}. An interesting exception to this is the eval FID going from  to  blocks, which decreases slightly. We believe that this may indicate that the network should be more strongly regularized as it grows. This effect will later be observed to be amplified for the larger U-ViT architectures.

\begin{table}
    \centering
    \caption{Scaling the U-Net architecture}\vspace{-.2cm}
    \label{tab:scaling}
    \scalebox{.95}{
    \begin{tabular}{l l l r}
    \toprule
    \# blocks at  & FID train & FID eval & steps / sec \\ \midrule
    2 \textcolor{gray}{+ 3} & 3.42 & 3.59 & \textbf{114}\% \\
    4 \textcolor{gray}{+ 5} & 2.98 & 3.29 & 100\% \\
    8 \textcolor{gray}{+ 9} & 2.46 & \textbf{3.00} & 76\% \\
    12 \textcolor{gray}{+ 13} & \textbf{2.41} & 3.03 & 62\% \\
        \bottomrule
    \end{tabular}}
\end{table}

\begin{table}
    \centering
    \caption{Downsampling strategies on ImageNet 512  512.}\vspace{-.2cm}
    \label{tab:downsampling}
    \scalebox{.9}{
    \begin{tabular}{l r r r r}
    \toprule
    Strategy & FID train & FID eval & steps / sec \\ \midrule
    None & 5.60 & 5.23 & 100\% \\
    DWT-1 & 5.42 & 4.97 & 139\% \\
    DWT-2 & \textbf{4.85} & \textbf{4.58} & \textbf{146}\% \\
    Conv-(2  2) & 5.99 & 5.33 & 137\% \\
    Conv-(4  4) & 5.04 & 4.80 & \textbf{146}\% \\
        \bottomrule
    \end{tabular}}
\end{table}


\begin{table}
    \centering
    \caption{Multiscale loss. Note that the  models use the shift 32 and the  use shift 64. This loss modifications is helpful for the highest resolution, but diminishes performance slightly for lower resolutions.}
    \label{tab:multiscale}\scalebox{.9}{
    \begin{tabular}{l l l l}
    \toprule
    Resolution & FID train & FID eval & IS \\ \midrule
256 & \textbf{3.76} & \textbf{3.71} & \textbf{171.6} \\
    + multiscale loss (32) & 4.00 & 3.89 & 171.0 \\ \midrule
    512 & 4.85 & 4.58 & 156.1 \\
     + multiscale loss (32) & \textbf{4.30} & \textbf{4.28} & \textbf{171.0} \\ 
        \bottomrule
    \end{tabular}}
\end{table}
\begin{table}
    \centering
    \caption{Comparison to generative models in the literature on ImageNet without any guidance or other sampling modifications, except () which use temperature scaling.}\vspace{-.2cm}
    \label{tab:literature_comparison}
    \scalebox{.8}{
    \begin{tabular}{l r r l}
    \toprule 
    & \multicolumn{2}{c}{FID} \\
    Method & train & eval & IS \\ \midrule 
    \textbf{128  128 resolution} \\
    ADM \citep{dhariwal2021diffusionbeatgans} & 5.91 \\
    CDM () \citep{ho2022cascaded} & 3.52 & 3.76 &  128.8 {\small  2.51} \\
    RIN \citep{jabri2022scalable} & 2.75 & & 144.1 \\
    simple diffusion (U-Net) (ours) & 2.26 & \textbf{2.88} & 137.3 {\small  2.03} \\
    simple diffusion (U-ViT, L) (ours) & \textbf{1.94} & 3.23 & \textbf{171.9} {\small  3.24} \\ \midrule
    \textbf{256  256 resolution} \\
    BigGAN-deep (no truncation) & 6.9\hspace{.175cm} & & 171.4 {\small  2} \\
    MaskGIT \citep{chang2022maskgit} & 6.18 & & 182.1 \\
    DPC (full 5) \citep{anonymous2023discrete} & 4.45 & & \textbf{244.8} \\ \midrule
    \textit{Denoising diffusion models} \\ 
    ADM \citep{dhariwal2021diffusionbeatgans} & 10.94 \\
    CDM () \citep{ho2022cascaded} & 4.88 & 4.63 & 158.71 {\small  2.26} \\
    LDM-4 \citep{rombach2022highresolution} & 10.56 & & 103.49 \\
    RIN \citep{jabri2022scalable} & 4.51 & & 161.0 \\
    DiT-XL/2 \citep{peebles2022scalable} & 9.62 & & 121.5 \\
    simple diffusion (U-Net) (ours) & 3.76 & \textbf{3.71} & 171.6 {\small  3.07} \\
    simple diffusion (U-ViT, L) (ours) & \textbf{2.77} & 3.75 & 211.8 {\small  2.93} \\ 
    \midrule
    \textbf{512  512 resolution} \\ 
    MaskGIT \citep{chang2022maskgit} & 7.32 & & 156.0 \\
    DPC (U) \citep{anonymous2023discrete} & 3.62 &  & \textbf{249.4} \\ \midrule
    \textit{Denoising diffusion models} \\ 
    ADM \citep{dhariwal2021diffusionbeatgans}& 23.24 \\
    DiT-XL/2 \citep{peebles2022scalable} & 12.03 & & 105.3 \\
    simple diffusion (U-Net) (ours) & 4.30 & \textbf{4.28} & 171.0 {\small  3.00} \\
    simple diffusion (U-ViT, L) (ours) & \textbf{3.54} & 4.53 & 205.3{\small  2.65} \\
        \bottomrule
    \end{tabular}}
\end{table}


\begin{table}
    \centering
    \caption{Text to image result on zero-shot COCO}\vspace{-.2cm}
    \label{tab:text_to_image_fid}
    \scalebox{.9}{
    \begin{tabular}{l r}
    \toprule
    Method & FID@30K 256 \\ \midrule
    GLIDE \citep{nichol2022glide} & 12.24 \\
    Dalle-2 \citep{ramesh2022hierarchicaltextconditional} & 10.39 \\
    Imagen \citep{saharia2022imagen} & 7.27 \\
    Muse \citep{chang2023muse} & 7.88 \\
    Parti \citep{yu2022scalingautoregressive} & 7.23 \\
    eDiff-I \citep{balaji2022ediffi} & \textbf{6.95} \\
    simple diffusion (U-ViT) (ours) & 8.3 \\
        \bottomrule
    \end{tabular}}
\end{table}

\textbf{Avoiding higher resolution feature maps}
Lastly, we want to study the effect of downsampling techniques to avoid high resolution feature maps. For this experiment we first have a standard U-Net for images of resolution 512. Then, when we downsample (either to 256 or to 128) using conventional layers or the DWT. For this study the total number of blocks is kept the same, by distributing the high resolution blocks that are skipped over the lower resolution blocks. For more details see Appendix~\ref{app:experimental_details}. 

Recall that our hypothesis was that downsampling should not cost much in sample quality, while considerably making the model faster. Surprisingly, in addition to being faster, models that use downsampling strategies actually obtain better sample quality. It seems that downsampling for such a high resolution enables the network to optimize better for sample quality. Most importantly, it allows training without absurdly large feature maps without performance degradation.

\textbf{Multiscale Loss}
For this final experiment, we test the difference between the standard loss and the multiscale loss, which adds more emphasis on lower frequencies in the image. For the resolutions 256 and 512 we report the sample quality in FID score for a model trained with the multiscale loss enabled or disabled. As can be seen in Figure~\ref{tab:multiscale}, for 256 the loss does not seem to have much effect and performs slightly worse. However, for the larger 512 resolution the loss has an impact and reduces FID score. 


\subsection{Comparison with literature}
In this section, simple diffusion is compared to existing approaches in literature. Although very useful for generating beautiful images, we specifically choose to only compare to methods without guidance (or other sampling modifications such as rejection sampling) to see how well the model is fitted. These sampling modifications may produce inflated scores on visual quality metrics \citep{ho2022classifierfreeguidance}.

Interestingly, the larger U-ViT models perform very well on train FID and Inception Score (IS), outperforming all existing methods in literature (Table~\ref{tab:literature_comparison}). However, the U-Net models perform better on eval FID. We believe this to be an extrapolation of the effect we observed before in Table~\ref{tab:scaling}, where increasing the architecture size did not necessarily result in better eval FID. For samples from the models see Figures~\ref{fig:overview} \& \ref{fig:random_samples_imagenet}.
In summary, simple diffusion achieves SOTA FID scores on class-conditional ImageNet generation among all other types of approaches without sampling modifications. We think this is an incredibly promising result: by adjusting the diffusion schedule and modifying the loss, simple diffusion is a single stage model that operates on resolutions as large as 512  512 with high performance. See Appendix~\ref{app:additional_exps} for additional results.


\paragraph{Text to image}
In this experiment we train a text-to-image model following \citep{saharia2022imagen}. In addition to the self-attention and mlp block, this network also has cross-attention in the transformer that operates on T5 XXL text embeddings. As can be seen in Table~\ref{tab:text_to_image_fid}, simple diffusion is a little better than some recent text-to-image models such as DALLE-2, although it still lacks behind Imagen. Importantly, our model is the first model that can generate images of this quality using only a single diffusion model that is trained end-to-end.


 
\section{Conclusion}
In summary, we have introduced several simple modifications of the original denoising diffusion formulation that work well for high resolution images. Without sampling modifiers, simple diffusion achieves state-of-the-art performance on ImageNet in FID score and can be easily trained in an end-to-end setup. Furthermore, to the best of our knowledge this is the first single-stage text to image model that can generate images with such high visual quality. 

\clearpage

\bibliography{iclr2022_conference.bib}
\bibliographystyle{icml2022}


\clearpage
\appendix
\onecolumn
\section{Additional Background Information on Diffusion Models}
\label{sec:addition_info_diffusion}
This section is a more detailed summary of relevant background information on denoising diffusion. For one, it can be helpful to understand how modern denoising diffusion models \citep{ho2020denoising} are trained using the formulations from \citep{kingma2021vdm}
First we define how signal is destroyed (diffused), which is the algorithmic equivalent to sampling :
\begin{lstlisting}[style=python]
def diffuse(x, alpha_t, sigma_t):
  eps_t = noise_normal_like(x)
  z_t = alpha_t * x + sigma_t * eps_t
  return z_t, eps_t
\end{lstlisting}

For the specific optimization setting we generally use (v-prediction, epsilon loss) the loss can be computed as defined below. This is the algorithmic equivalent of  as proposed by \citep{ho2020denoising,kingma2021vdm}:
\begin{lstlisting}[style=python]
def loss(x):
  t = noise_uniform(size=x.shape[0]) # Sample a batch of timesteps.
  logsnr_t = logsnr_schedule(t)
  alpha_t = sqrt(sigmoid(logsnr))
  sigma_t = sqrt(sigmoid(-logsnr))
  z_t, eps_t = diffuse(x, alpha_t, sigma_t)
  v_pred = uvit(z_t, logsnr_t)
  eps_pred = sigma_t * z_t + alpha_t * v_t
  return mse(eps_pred, eps_t)
\end{lstlisting}

In case of conditioning (for example ImageNet class number of a text embedding), these are added as an input to the uvit call, but do not influence the diffusion process in other ways. The conditioning is dropped out  of the time, so that the models can additionally be used with classifier-free guidance.


The standard cosine logsnr schedule (taking care of boundaries) can be defined as:
\begin{lstlisting}[style=python]
def logsnr_schedule_cosine(t, logsnr_min=-15, logsnr_max=+15):
  t_min = atan(exp(-0.5 * logsnr_max))
  t_max = atan(exp(-0.5 * logsnr_min))
  return -2 * log(tan(t_min + t * (t_max - t_min)))
\end{lstlisting}

One can then define the shifted schedule as:
\begin{lstlisting}[style=python]
def logsnr_schedule_cosine_shifted(t, image_d, noise_d):
  return logsnr_schedule_cosine(t) + 2 log(noise_d / image_d)
\end{lstlisting}

And the interpolated schedule as:
\begin{lstlisting}[style=python]
def logsnr_schedule_cosine_shifted(t, image_d, noise_d_low, noise_d_high):
  logsnr_low = logsnr_schedule_cosine_shifted(t, image_d, noise_d_low)
  logsnr_high = logsnr_schedule_cosine_shifted(t, image_d, noise_d_high)
  return t * logsnr_low + (1 - t) * logsnr_high
\end{lstlisting}

Care needs to be taken that the minimum and maximum logsnr hyperparameters are shifted along with the entire schedule, so care needs to be taken when these endpoints are used to define the embedding in the architecture.

\paragraph{Sampling}
In this work we use the standard ddpm sampler unless noted otherwise. Below is the algorithmic equivalent of the generative process of sampling  and then repeatedly sampling :

\begin{lstlisting}[style=python]
def sample(x_shape):
  # lowest_idx can be 0 or 1.
  z_t = noise_normal(x_shape)
  for t in reversed(range(lowest_idx+1, num_steps+1)):
    u_t = t / num_steps
    u_s = (t - 1) / num_steps
    logsnr_t = logsnr_schedule(u_t)
    logsnr_s = logsnr_schedule(u_s)
    v_pred = uvit(z_t, logsnr_t)
    z_t = sampler_step(z_t, v_pred, logsnr_t, logsnr_s)

  # Final prediction, do not sample x ~ p(x | z_lowest) but take the mean prediction:
  logsnr_lowest = logsnr_schedule(lowest_idx / num_steps)
  v_pred = uvit(z_t, logsnr_lowest)
  x_pred = alpha_t * z_t - sigma_t * v_pred
  x_pred = clip_x(x_pred)
  return x_pred
\end{lstlisting}


\begin{lstlisting}[style=python]
def ddpm_sampler_step(z_t, v_pred, logsnr_t, logsnr_s):
  x_pred = alpha_t * z_t - sigma_t * v_pred
  x_pred = clip_x(x_pred)
  
  mu = exp(logsnr_t - logsnr_s) * alpha_st * z_t + (1 - exp(logsnr_t - logsnr_s)) * alpha_s * x
  # Variance can be any interpolation of the following two in log-space:
  min_lvar = (1 - exp(logsnr_t - logsnr_s)) + log_sigmoid(-logsnr_s)
  max_lvar = (1 - exp(logsnr_t - logsnr_s)) + log_sigmoid(-logsnr_t)
  noise_param = 0.2
  sigma = sqrt(exp(noise_param * max_logvar + (1 - noise_param) * min_logvar))
  return mu + sigma * normal_noise_like(z_t)
\end{lstlisting}
where noise\_param is set to 0.2 with the exception of MSCOCO FID evaluation, where it is set to 1.0.

An important but not often discussed detail is that during sampling it is helpful to clip the predictions in x-space, below gives an example for static clipping, for dynamic clipping see \citep{saharia2022imagen}:
\begin{lstlisting}[style=python]
def clip_x(x):
  # x should be between -1 and 1.
  return clip(x, -1, 1)
\end{lstlisting}


\paragraph{Classifier-free guidance}
In classifier-free guidance \citep{ho2022classifierfreeguidance}, one drops out the conditioning signal occasionally during training (Usually about 10\% of the time). This allows one to train models,  in addition to the model one normally trains which is . The epsilon predictions of these models can then be recombined with a guidance scale. For :

One can substitute  by  or  and the result ends up being equivalent due to linearity and terms cancelling out. Note we will report the guidance scale as  as is done often in literature, not to be confused by reporting  itself.

\paragraph{Distillation}
Like many diffusion models, simple diffusion can also be distilled to reduce the number of sampling steps and neural net evaluations \citep{meng2022ondistillation} to reduce the number of sampling steps. For a distilled U-ViT model, generating a single image takes 0.42 seconds on a TPUv4. Similarly, generating a batch of 8 images takes 2.00 seconds.

\section{Experimental details}
\label{app:experimental_details}

In this section, specific details on the experiments are given. Firstly, the standard optimizer settings for the U-Net experiments.
\subsection{U-Net settings}
\begin{lstlisting}
unet default optimization settings:
    batch_size=512,
    optimizer='adam',
    adam_beta1=0.9,
    adam_beta2=0.99,  except for ImageNet 128 which is adam_beta2=0.999
    adam_eps=1.e-12,  
    learning_rate=5e-5,
    learning_rate_warmup_steps=10_000,
    weight_decay=0.0,
    ema_decay=0.9999,
    grad_clip=1.0,
\end{lstlisting}


\begin{lstlisting}
Specific settings for the UNet on ImageNet 128 experiment:
    base_channels=128,
    emb_channels=1024,                              (for diffusion time, image class)
    channel_multiplier=[1, 2, 4, 8, 8],
    num_res_blocks=[3, 4, 4, 12, 4],                (unless noted otherwise)
    attn_resolutions=[8, 16],
    num_heads=4,
    dropout_from_resolution=16,                     (unless noted otherwise)
    dropout=0.1,
    patching_type='none'
    schedule={'name': 'cosine_shifted, 'shift': 64} (unless noted otherwise)
    num_train_steps=1_500_000
\end{lstlisting}


\begin{lstlisting}
Specific settings for the UNet on ImageNet 256 experiment:
    base_channels=128,
    emb_channels=1024,                              (for diffusion time, image class)
    channel_multiplier=[1, 1, 2, 4, 8, 8],
    num_res_blocks=[1, 2, 2, 4, 12, 4],
    attn_resolutions=[8, 16],
    num_heads=4,
    dropout_from_resolution=16,
    dropout=0.1,
    patching_type='none'
    schedule={'name': 'cosine_shifted, 'shift': 64} (unless noted otherwise)
    num_train_steps=2_000_000
\end{lstlisting}


\begin{lstlisting}
Setting for the UNet on ImageNet 512 experiment:
    base_channels=128,
    emb_channels=1024,                              (for diffusion time, image class)
    attn_resolutions=[8, 16],
    num_heads=4,
    dropout_from_resolution=16,
    dropout=0.1,
    patching_type='dwt_2'                           (unless noted otherwise)
    schedule={'name': 'cosine_shifted, 'shift': 64} (unless noted otherwise)
    num_train_steps=2_000_000
\end{lstlisting}
To keep the number of residual blocks the same, high resolution blocks that are skipped by down-sampling are added to the lower resolution levels. With no downsampling, the architecture uses:
\begin{lstlisting}
channel_multiplier=[1, 1, 1, 2, 4, 8, 8], num_res_blocks=[1, 1, 2, 2, 4, 12, 4],
\end{lstlisting}
In case of  downsampling the architecture uses:
\begin{lstlisting}
channel_multiplier=[1, 2, 2, 4, 8, 8], num_res_blocks=[2, 2, 2, 4, 12, 4],
\end{lstlisting}
In case of  downsampling the architecture uses:
\begin{lstlisting}
channel_multiplier=[2, 3, 4, 8, 8], num_res_blocks=[3, 3, 4, 12, 4],
\end{lstlisting}



\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{images/unet.pdf} \vspace{.05cm}
    \includegraphics[width=.8\textwidth]{images/uvit.pdf}
    \caption{The difference between the U-Net and U-ViT architecture. In essence, the convolutional layers are replaced by MLP blocks on levels with self-attention. These now form transformer blocks which are connected via residual connections, only the ResBlocks on higher levels use skip connections. Circular arrows denote that such a block can be repeated multiple times.}
    \label{fig:unet_vs_uvit}
\end{figure}


\subsection{U-ViT settings}
The U-ViT is a very similar architecture to the U-Net (see Figure~\ref{fig:unet_vs_uvit}). The two major differences are that 1) When a module has self-attention, it uses an MLP block instead of a convolutional layer, making their combination a transformer block. And 2) the transformer blocks in the middle do not use skip connections, only residual connections. The default optimization settings for ImageNet for the U-ViT are:
\begin{lstlisting}
uvit default optimization settings:
    optimizer='adam',
    adam_beta1=0.9,
    adam_beta2=0.99,
    adam_eps=1.e-12,
    learning_rate=1e-4,
    learning_rate_warmup_steps=10_000,
    weight_decay=0.0,
    ema_decay=0.9999,
    grad_clip=1.0,
    batch_size=2048,
    num_train_steps=500_000,
\end{lstlisting}

And the architecture settings are almost the same for all resolutions ,  and .
\begin{lstlisting}
uvit default architecture settings for 512:
    optimizer='adam',
    adam_beta1=0.9,
    adam_beta2=0.99,
    adam_eps=1.e-12,
    learning_rate=1e-4,
    learning_rate_warmup_steps=10_000,
    weight_decay=0.0,
    ema_decay=0.9999,
    grad_clip=1.0,
    batch_size=2048,
    base_channels=128,
    emb_channels=1024,
    channel_multiplier=[1, 2, 4, 16],
    num_res_blocks=[2, 2, 2],
    num_transformer_blocks=36,
    num_heads=4,
    transformer_dropout=0.2,
    logsnr_input_type='linear',
    patching_type='dwt_5/3_2',
    mean_type='v',
    mean_loss_type='v_mse',
\end{lstlisting}
where the patching type is either \texttt{'none'} for , \texttt{'dwt\_1'} for 256 and  \texttt{'dwt\_2'} for 512. Note also that the loss is computed on v instead of epsilon. This may not be very important: in small experiments we observed only minor performance differences between the two. Note also that the batch size is larger (2048) which does affect FID and IS performance considerably. The text to image model was trained for 700K steps.



\subsubsection{Pseudo-code for U-ViT modules}
The Transformer blocks consist of a self-attention and mlp block. These are defined as one would expect, for completeness given below in pseudo-code:
\begin{lstlisting}[style=python]
def mlp_block(x, emb, expansion_factor=4):
  B, HW, C = x.shape 
  x = Normalize(x)
  mlp_h = Dense(x, expansion_factor * C)
  emb_scale = DenseGeneral(emb, mlp_h.shape[2:])
  emb_shift = DenseGeneral(emb, mlp_h.shape[2:])

  mlp_h = swish(mlp_h)
  scale, shift = emb_out
  mlp_h = mlp_h * (1. + scale[:, None]) + shift[:, None]
  if config.transformer_dropout > 0.:
    mlp_h = Dropout(mlp_h, config.transformer_dropout)

  out = Dense(mlp_h, C, kernel_init=zeros)
  return out
  
def self_attention(x, text_emb):
  B, HW, C = x.shape
  B, T, TC = text_emb.shape
  head_dim = C // config.num_heads

  x_norm = Normalize(x)

  q = DenseGeneral(x_norm, (num_heads, head_dim))
  k = DenseGeneral(x_norm, (num_heads, head_dim))
  v = DenseGeneral(x_norm, (num_heads, head_dim))

  q = NormalizeWithBias(q)
  k = NormalizeWithBias(k)
  q = q * q.shape[-1] ** -0.5
  weights = einsum("bqhd,bkhd->bhqk", q, k)
  weights = softmax(weights)
  attn_vals = einsum("bhqk,bkhd->bqhd", weights, v)

  out = DenseGeneral(attn_vals, C, axis=(-2, -1), kernel_init=zeros)
  return out
  
def transformer_block(x, text_emb, emb):
  x += mlp_block(x, emb)
  x += self_attention(x, text_emb)
  return x
\end{lstlisting}


Another important block is the standard ResBlock, pseudo-code given below:
\begin{lstlisting}[style=python]
def resnet_block(x, emb, skip_h=None):
  B, H, W, C = x.shape
  h = NormalizeWithBias(x)
  if skip_h is not None:
    skip_h = NormalizeWithBias(skip_h)
    h = (h + skip_h) / sqrt(2)
  h = swish(h)
  h = Conv2D(h, out_ch, (3, 3), (1, 1))
  emb_out = Dense(emb, 2 * out_ch)[:, None, None, :]

  scale, shift = split(emb_out, 2, axis=-1)
  h = NormalizeWithBias(h) * (1 + scale) + shift
  h = swish(h)
  h = Conv2D(h, out_ch, (3, 3), (1, 1), kernel_init=zeros)
  return x + h
\end{lstlisting}


Given these building blocks, one can define the U-ViT architecture:
\begin{lstlisting}[style=python]
def uvit(x, logsnr):
  B, H, W, C = x.shape
  emb = get_logsnr_emb(logsnr)

  h0 = EmbedInput(config.base_channels * config.channel_multiplier[0])(x)
  hs = []
  last_h = h0
  # Down path.
  for i_level in range(len(config.num_res_blocks))):
    for i_block in range(config.num_res_blocks[i_level]):
      last_h = resnet_block(last_h, emb)
      hs.append(last_h)

    last_h = downsample(
        last_h, config.base_channels * config.channel_multiplier[i_level+1])
  
  # The transformer.
  last_h = last_h.reshape(B, H * W, C)
  last_h += param("pos_emb", initializers.normal(0.01), last_h.shape[1:])[None]
  for _ in range(config.num_transformer_blocks):
    last_h = transformer_block(last_h, text_emb, emb)
  last_h = last_h.reshape(B, H, W, C)

  # Up path.
  for i_level in reversed(range(len(config.num_res_blocks)))):
    last_h = upsample(last_h, config.base_channels * config.channel_multiplier[i_level])
    for i_block in range(config.num_res_blocks[i_level]):
      last_h = resnet_block(last_h, emb, skip_h=hs.pop())
  
  out = ProjectOutput(last_h, C)
  return out
\end{lstlisting}


As one can see, it's very similar to the UNet, the middle part is now a transformer which does not have convolutional layers but mlp blocks with only residual connections.



 

\newpage\section{Additional Experiments}
\label{app:additional_exps}

\paragraph{Guidance scale} In Table~\ref{tab:guidance} we show the effect of guidance on the ImageNet models. For relatively small levels of guidance, samples immediately gain a lot in IS at the cost of especially eval FID. Furthermore, Figure~\ref{fig:clipvsfid} shows the Clip versus MSCOCO FID30K score for the text to image model. Following others such as \citep{saharia2022imagen}, images are sampled by conditioning on 30K randomly sampled texts from the MSCOCO validation set, computed against the full validation set as a reference.
\begin{table}[H]
    \centering
    \caption{Guidance scale, the shifted schedule is quite sensitive to guidance.}
    \label{tab:guidance}
    \scalebox{.9}{
    \begin{tabular}{l | r r l | r r l | r r l }
    \toprule
    U-ViT & \multicolumn{3}{c}{ImageNet 128} & \multicolumn{3}{c}{ImageNet 256} & \multicolumn{3}{c}{ImageNet 512} \\ \midrule
    guidance & FID train & FID eval & IS & FID train & FID eval & IS & FID train & FID eval & IS \\ \midrule
    1.00 & \textbf{1.94} & \textbf{3.23} & 171.9 {\small  3.2} & 2.77 & \textbf{3.75} & 211.8 {\small  2.9} & 3.54 & 4.53 & 205.3{\small  2.7} \\
    1.05 & 2.05 & 3.57 & 189.9{\small  3.5} & 2.46 & 3.80 & 235.3 {\small  4.9} & 3.14 & \textbf{4.43} & 228.5{\small  4.2} \\
    1.10 & 2.35 & 4.10 & 207.0{\small  3.5} & \textbf{2.44} & 4.08 & 256.3 {\small  5.0} & \textbf{3.02} & 4.60 & 248.7{\small  3.4} \\
    1.20 & 3.24 & 5.36 & 237.6{\small  3.6} & 2.96 & 5.10 & 289.8 {\small  4.1} & 3.33 & 5.43 & 284.6{\small  2.8} \\
    1.40 & 5.58 & 8.26 & 285.2{\small  2.0} & 4.69 & 7.50 & 342.2 {\small  5.1} & 4.97 & 7.89 & 339.9{\small  3.8} \\
    1.80 & 9.77 & 13.06 & 340.1{\small  3.6} & 8.21 & 11.81 & 398.0 {\small  5.4} & 8.38 & 12.15 & 401.7{\small  5.2} \\
    2.00 & 11.47 & 14.96 & 359.2{\small  5.6} & 9.59 & 13.44 & 416.4 {\small  4.7} & 9.68 & 13.68 & 416.2{\small  4.8} \\
    3.00 & 15.85 & 19.75 & \textbf{399.2}{\small  2.9} & 13.61 & 18.00 & \textbf{455.7} {\small  4.2} & 13.79 & 18.42 & \textbf{461.4}{\small  5.0} \\
        \bottomrule
    \end{tabular}}\vspace{-.2cm}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/clipvsfid.pdf}
    \caption{Clip vs FID30K score on zero-shot MSCOCO at resolution 256  256. For guidance scales 1.00, 1.25, 1.40, 1.50, 2.0, 3.0, 4.0.}
    \label{fig:clipvsfid}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[b]{0.495\textwidth}
\centering
\includegraphics[width=.23\textwidth]{images/i256/eagle_0_g4.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/eagle_1_g4.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/macaw_0_g4.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/macaw_1_g4.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/eagle_2_g4.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/eagle_3_g4.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/macaw_2_g4.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/macaw_3_g4.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/malamute_0_g4.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/malamute_1_g4.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/vulcano_0_g4.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/vulcano_1_g4.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/malamute_2_g4.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/malamute_3_g4.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/vulcano_2_g4.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/vulcano_3_g4.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/frog_0_g4.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/frog_1_g4.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/siamese_cat_0_g4.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/siamese_cat_1_g4.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/frog_2_g4.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/frog_3_g4.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/siamese_cat_2_g4.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/siamese_cat_3_g4.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/balloon_0_g4.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/balloon_1_g4.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/cheeseburger_0_g4.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/cheeseburger_1_g4.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/balloon_2_g4.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/balloon_3_g4.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/cheeseburger_2_g4.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/cheeseburger_3_g4.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/monarch_0_g4.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/monarch_1_g4.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/bea_eater_0_g4.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/bea_eater_1_g4.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/monarch_2_g4.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/monarch_3_g4.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/bea_eater_2_g4.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/bea_eater_3_g4.jpg} \\ \vspace{.17cm}
\caption{Guidance scale 4}
 \end{subfigure} \hfill
\begin{subfigure}[b]{0.495\textwidth}
\centering
\includegraphics[width=.23\textwidth]{images/i256/eagle_0.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/eagle_1.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/macaw_0.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/macaw_1.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/eagle_2.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/eagle_3.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/macaw_2.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/macaw_3.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/malamute_0.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/malamute_1.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/vulcano_0.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/vulcano_1.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/malamute_2.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/malamute_3.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/vulcano_2.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/vulcano_3.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/frog_0.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/frog_1.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/siamese_cat_0.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/siamese_cat_1.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/frog_2.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/frog_3.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/siamese_cat_2.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/siamese_cat_3.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/balloon_0.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/balloon_1.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/cheeseburger_0.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/cheeseburger_1.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/balloon_2.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/balloon_3.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/cheeseburger_2.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/cheeseburger_3.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/monarch_0.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/monarch_1.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/bea_eater_0.jpg} \hspace{.04cm} \includegraphics[width=.23\textwidth]{images/i256/bea_eater_1.jpg} \\ \vspace{.17cm}
\includegraphics[width=.23\textwidth]{images/i256/monarch_2.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/monarch_3.jpg} \hspace{.04cm}
    \includegraphics[width=.23\textwidth]{images/i256/bea_eater_2.jpg} \hspace{.04cm } \includegraphics[width=.23\textwidth]{images/i256/bea_eater_3.jpg} \\ \vspace{.17cm}
\caption{Guidance scale 1 (No guidance)}
 \end{subfigure}
\caption{Random (not cherry picked) samples from the U-ViT on ImageNet 256  256.}
\label{fig:random_samples_imagenet}
\end{figure}





 




\end{document}
