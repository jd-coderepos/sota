\section{Experiments}

\subsection{Experimental Settings} \label{sub:exp_setting}
\textbf{Datasets.} We verified the effectiveness of our method on three commonly used benchmark datasets: CIFAR-10, CIFAR-100 \cite{ref:data_cifar}, Tiny ImageNet~\cite{data:tinyimagenet}, and iNaturalist 2018 \cite{ref:data_iNat}.
The CIFAR-10 and CIFAR-100 datasets consist of 50,000 training images and 10,000 test images with 10 and 100 classes, respectively. 
Meanwhile, Tiny ImageNet contains 200 classes for training, in which each class has 500 images. 
Its test set contains 10,000 images.
Since CIFAR and Tiny ImageNet are evenly distributed, we have made these datasets imbalanced according to \cite{ref:cui_belongie_cvpr19, ref:buda_mazurowski_2018}, respectively.
Primarily, we investigate two common types of imbalance: (i) long-tailed imbalance \cite{ref:cui_belongie_cvpr19} and (ii) step imbalance \cite{ref:buda_mazurowski_2018}.
In long-tailed imbalance, the number of training samples for each class decreases exponentially from the largest majority class to the smallest minority class. To construct long-tailed imbalanced datasets, the number of selected samples in the -th class was set to , where  is the original number of the -th class.
Meanwhile, in step imbalance, the classes are divided into two groups: the majority class group and minority class group. 
Every class within a group contains the same number of samples, and the class in the majority class group has many more samples than that in the minority class group.  
For evaluation, we used the original test set. 
The imbalance ratio  is defined by .
Thus, the imbalance ratio represents the degree of imbalance in the dataset. 
We evaluated the performance of our method under various imbalance ratios from 10 to 200.

The iNaturalist 2018 dataset is a large-scale real-world dataset containing 437,513 training images and 24,426 test images with 8,142 classes.
iNaturalist 2018 exhibits long-tailed imbalance, whose imbalance ratio is 500.
We used the official training and test splits in our experiments. \vspace{2mm}

\textbf{Baselines.}
We compared our algorithm with the following cost-sensitive loss methods:
(1) Our baseline model, which is trained on the standard cross-entropy loss. 
Comparing our model with this baseline enables us to clearly understand how much our training scheme has improved the performance;
(2) focal loss \cite{ref:lin_focal_loss_iccv17}, which increases the relative loss for hard samples and down-weights well-classified samples;
(3) CB loss \cite{ref:cui_belongie_cvpr19}, which re-weights the loss inversely proportional to the effective number of samples;
(4) LDAM loss \cite{ref:cao_ldam_neurips2019}, which regularizes the minority classes to have larger margins.  

Since our IB loss can be easily combined with other methods, we employee two further variants.
First, IB + CB uses the effective number in CB loss, instead of using  in IB.
Second, IB + focal uses focal loss during the fine-tuning phase, instead of using the cross-entropy loss.
We demonstrate that combination with other methods can further improve the performance.

\textbf{Implementation Details.} \label{sub:impl_detail}
We used PyTorch \cite{ref:Pytorch} to implement and train all the models in the paper, and we used ResNet architecture \cite{ref:resnet_cvpr2016} for all datasets. 
For CIFAR datasets, we used randomly initialized ResNet-32.
The networks were trained for 200 epochs with stochastic gradient descent (SGD) (momentum = 0.9).
Following the training strategy in \cite{ref:cui_belongie_cvpr19, ref:cao_ldam_neurips2019}, the initial learning rate was set to 0.1 and then decayed by 0.01 at 160 epochs and again at 180 epochs. 
Furthermore, we used a linear warm-up of the learning rate \cite{ref:goyal_he_corr2017} in the first five epochs.
Since our method uses a two-phase training schedule, we trained for the first 100 epochs with the standard cross-entropy loss, then fine-tuned the networks using the IB loss for the next 100 epochs.
We trained the models for CIFAR on a single NVIDIA GTX 1080Ti with a batch size of 128.
For Tiny ImageNet, we employed ResNet-18 and used the stochastic gradient descent with a momentum of 0.9, and weight decay of  for training. 
The networks were initially trained for 50 epochs, and then fine-tuned for the subsequent 50 epochs with IB loss.
The learning rate at the start was set to 0.1 and was dropped by a factor of 0.1 after 50 and 90 epochs.
For iNaturalist 2018, we trained ResNet-50 with four GTX 1080Ti GPUs.
The networks were initially trained for 50 epochs and then fine-tuned for the subsequent 150 epochs with IB loss.
The learning rate at the start was set to 0.01 and was decreased by a factor of 0.1 after 30 and 180 epochs.

As a simple but important implementation trick, we added  to  to prevent numerical instability in inversion when the influence approaches zero.
We discuss the influence of the hyperparameter () in the following section.

\subsection{Analysis}
To validate the proposed method, we conducted extensive experiments.

\textbf{Is influence meaningful for re-weighting?}
First, to confirm whether influence can act as a meaningful clue of re-weighting for class imbalance learning, we compared the influences between a balanced dataset and an imbalanced dataset.
For an imbalanced CIFAR-10, we used the long-tailed version of CIFAR-10 with the imbalance ratio , in which
the largest class, `plane' (i.e., class index 0), contains 5,000 samples, while the smallest class, `truck' (i.e., class index 9), contains only 50 samples.
We trained ResNet-32 with a standard cross-entropy loss for 200 epochs, as described in Implementation Details, on both the balanced (original) and imbalanced CIFAR-10.
We plotted the influences of both classes in Figure \ref{fig:bal_imbal}.
We scaled the influences to between 0 and 1 for each dataset. 
Since the minority class contains only 50 samples, we selected the highest 50 samples for comparison. 
As illustrated in Figure \ref{fig:bal_imbal}, there was little difference in the distributions of the influences between the classes in the balanced dataset.
However, in the imbalanced dataset, the minority samples had significantly less influence on the model than did the majority samples.
This result corroborates that majority samples greatly contribute to forming a decision boundary, and re-weighting their influences can improve the generalization of the model.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{figure/bal_imbal.png}
\caption{\textbf{Comparison of Influences between balanced and imbalanced dataset.}
We plotted the influences of samples on ResNet-32 trained on the original CIFAR-10 and the imbalanced version of CIFAR-10. The solid and dashed lines represent the influences of the imbalanced data and balanced data, respectively.
While there is little difference in the balanced dataset, it can be seen that the influence of the dominant class is much greater than that of the minor class in the imbalance dataset.
} \label{fig:bal_imbal}
\end{center}
\end{figure}


\textbf{Magnitude of Influence.}
In Section \ref{sec:ibfactor}, we used  norm to compute the magnitude of the influences.
We investigated performance variations depending on three vector norms to compute the magnitude of the gradient vector : , , .
As indicated in Table \ref{ref:table_norms},  norm, which provides a distinctive change of influence around the equilibrium point, exhibits the best classification accuracy on CIFAR-10 with multiple imbalance ratios.



\begin{table}[h]
\caption{Comparison of norms. Using  norm yields the best performance.}
\small
\centering
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c}{\textbf{CIFAR-10}} & \multicolumn{2}{c}{\textbf{CIFAR-100}} \\ \cmidrule(lr){2-3}\cmidrule(lr){4-5}
Imbalance ()            &           &          &           &      \\ \midrule\midrule
             & \textbf{78.41} & \textbf{85.80}& \textbf{40.85} & \textbf{52.85}         \\ 
             & 75.67          & 84.35        & 36.41          & 50.95         \\ 
        & 77.23          & 84.30         & 37.48          & 50.99         \\ 
\bottomrule
\end{tabular}
\label{ref:table_norms}
\vspace{-2mm}
\end{table}



\begin{figure}[h]
\vspace{-0.2cm}
\begin{center}
\includegraphics[width=0.95\linewidth]{figure/acc_loss.png}
\caption{\textbf{Influence-balanced training scheme.}
We varied the \textit{training epochs for the normal training}, , to determine the best transition time from the normal training to the influence-balance fine-tuning. 
We achieved the best performance 
when setting the transition time to the point when the training loss converges.
}
\label{fig:training_epochs}
\end{center}
\end{figure}


\textbf{Timing for starting fine-tuning for balancing.}
Our training scheme is divided into two phases: normal training and fine-tuning for balancing. 
This must determine the transition time between normal training and fine-tuning for balancing. 
Hence, we investigated the results on how much the transition time affects the performance and determined the best transition time.
For this, we experimented on the long-tailed version of  CIFAR-10 with imbalance ratios of  and .
In Figure \ref{fig:training_epochs}, the -axis represents the number of training epochs  for the normal training phase.
We varied the transition time, , from 0 to 120 while the total number of training epochs was fixed at 200. 
The solid line represents the classification accuracy earned by the models for each training schedule.
To analyze the relationship between the convergence of the normal training phase and the transition timing, we plotted the standard cross-entropy loss without adopting the IB loss for the whole training epochs (dashed lines).


From Figure \ref{fig:training_epochs}, it can be observed that the proposed method demonstrates robust performance regardless of the choice of transition time .
Yet, the transition to fine-tuning after the 100th epoch yields the best performance when the training loss has converged.
Since the influence function is derived from the loss minimization context~\cite{ref:Koh_Liang_2017}, it is reasonable to begin the fine-tuning phase after the learning converges. 




\textbf{Effects of .}
As mentioned in Implementation Details, for all datasets, we added the hyperparameter () to  to prevent numerical instability.
To analyze the effects of the hyperparameter, we conducted experiments with the following denominators for the IB loss~(\ref{eq:ibce}):
(a) , (b) , (c) , and (d) . 
We iterated experiments three times with different random seeds on the long-tailed CIFAR-10 ().
As presented in Table~\ref{table:epsilon}, setting  to  yields the best performance.
Thus, we set   as  in all the experiments.
However, when we did not use the IB weighting factor, the accuracy greatly decreased.

\begin{table}[h]
\caption{Effects of .}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lllll}
\toprule
Epsiilon & (a) IB+1e-8 & (b) IB+1e-3 & (c) IB+1e-2    & (d) 1e-3 \\ \midrule
Accuracy &  &  &  &       \\ \bottomrule
\end{tabular}
\label{table:epsilon}
}\vspace{-0.4cm}

\end{table}



\begin{table*}[t]
\caption{
Class-wise classification accuracy (\%) of ResNet-32 on imbalanced CIFAR-10 dataset.
The number of test samples for each class is the same as 1000.
The best results are marked in bold. 
}
\vspace{0.2cm}
\label{result-class-wise}
\centering
\small
{
\begin{tabular}{
l
ccccc
ccccc
} 
    \toprule
    &  \multicolumn{10}{c}{\textbf{Imbalanced CIFAR-10}}
    \\
    \cmidrule(){2-11}
Class 
    & plane & car & bird & cat & deer 
    & dog & frog & horse & ship & truck 
    \\
    \midrule\midrule
    
    \textit{\textbf{Long-Tailed}} ()
    &  &  &  &  & 
    &  &  &  &  & 
    \\
    \#Training samples
    & 5000 & 3237 & 2096 & 1357 & 878 
    & 568 & 368 & 238 & 154 & 100 
    \\
    \midrule
    Baseline (CE)
& \textbf{97.4}
        & \textbf{98.0}
        & \textbf{84.0}
        & \textbf{80.3}
        & 78.8
        & 68.4
        & 76.1
        & 64.5
        & 57.0
        & 52.0
        \\
        
     Focal \cite{ref:lin_focal_loss_iccv17}
& 91.6
        & 95.1
        & 73.1
        & 59.2
        & 67.8
        & 67.2
        & 84.2
        & 77.3
        & \textbf{83.9}
        & 61.8
        \\
    
    CB ~\cite{ref:cui_belongie_cvpr19}
& 92.9
        & 96.3
        & 79.2
        & 75.1
        & 82.4
        & 69.9
        & 75.0
        & 69.1
        & 73.6
        & 66.8
        \\
        
    LDAM~\cite{ref:cao_ldam_neurips2019}
& 96.9
        & 98.5
        & 82.9
        & 74.7
        & 82.8
        & 69.0
        & 78.5
        & 69.9
        & 65.3
        & 66.0
        \\
        
    LDAM-DRW~\cite{ref:cao_ldam_neurips2019}
& 94.8
        & 97.8
        & 82.6
        & 72.3
        & 85.3
        & 73.0
        & 82.0
        & 76.7
        & 75.8
        & 72.4
        \\
    
    \midrule
    
    IB 
& 92.2
        & 96.2
        & 81.3
        & 66.6
        & \textbf{85.7}
        & \textbf{76.4}
        & 81.7
        & 75.9
        & 79.9
        & \textbf{81.1}
        \\
        
    IB + CB
& 93.8
        & 97.2
        & 78.1
        & 64.8
        & 84.8
        & 74.2
        & \textbf{86.4}
        & \textbf{79.7}
        & 79.5
        & 76.9
        \\
    
    IB + Focal 
& 90.9
        & 96.1
        & 81.7
        & 69.0
        & 82.0
        & 75.7
        & 85.2
        & 77.5
        & 80.2
        & 76.8
        \\
    
    \midrule\midrule
    
    \textit{\textbf{Step-Imbalance}} ()
    &  &  &  &  & 
    &  &  &  &  & 
    \\
    
    \#Training samples
    & 5000 & 5000 & 5000 & 5000 & 5000 
    & 100 & 100 & 100 & 100 & 100 
    \\
    \midrule
    
    Baseline (CE)
& 95.9
        & \textbf{99.2}
        & \textbf{91.5}
        & \textbf{91.9}
        & 95.5
        & 24.8
        & 40.2
        & 46.7
        & 52.7
        & 55.1
        \\
        
    Focal \cite{ref:lin_focal_loss_iccv17}
& 96.3
        & 93.9
        & 91.2
        & 90.5
        & \textbf{95.7}
        & 20.0
        & 46.7
        & 48.8
        & 56.1
        & 57.6
        \\
    
    CB ~\cite{ref:cui_belongie_cvpr19}
& 87.4
        & 96.3
        & 76.8
        & 77.0
        & 85.7
        & 34.6
        & 61.5
        & 56.5
        & 68.7
        & 63.8
        \\
        
    LDAM~\cite{ref:cao_ldam_neurips2019}
& \textbf{96.4}
        & 98.5
        & 91.1
        & 90.2
        & 94.6
        & 28.3
        & 50.3
        & 57.0
        & 56.2
        & 64.4
        \\
        
    LDAM-DRW~\cite{ref:cao_ldam_neurips2019}
& 94.5
        & 97.2
        & 88.0
        & 84.5
        & 94.3
        & 50.4
        & 69.9
        & 71.4
        & 74.6
        & 76.0
        \\
    
    \midrule
    
    IB 
& 94.0
        & 97.7
        & 86.7
        & 83.2
        & 93.8
        & 56.9
        & 71.0
        & \textbf{75.1}
        & 76.5
        & 81.7
        \\
        
    IB + CB
& 91.8
        & 95.7
        & 86.6
        & 79.4
        & 93.6
        & 62.8
        & 77.2
        & 72.3
        & 74.2
        & \textbf{87.3}
        \\
    
    IB + Focal 
& 91.2
        & 96.4
        & 83.3
        & 77.1
        & 92.0
        & \textbf{64.8}
        & \textbf{78.0}
        & 74.4
        & \textbf{83.5}
        & 83.1
        \\
    
    
    
    \bottomrule
\end{tabular}
\vspace{-2mm}
}
\end{table*}

 
\subsection{Comparison of Class-Wise Accuracy.}
\vspace{-1mm}
In this section, to validate that the performance improvement has actually resulted from the minority classes, not from the majority classes, we report the class-wise accuracy on both the long-tailed and the step-imbalanced CIFAR-10.
We compare the proposed method with the state-of-the-art cost-sensitive loss methods. 
Since previous studies do not report the class-wise accuracy on the imbalanced CIFAR-10, we implemented the baseline methods~ \cite{ref:lin_focal_loss_iccv17, ref:cui_belongie_cvpr19, ref:cao_ldam_neurips2019}. 
For the implementation of LDAM~\cite{ref:cao_ldam_neurips2019}, we used their official implementation code to reproduce the results. 


The overall results are reported in Table \ref{result-class-wise}.
As presented in Table~\ref{result-class-wise}, existing methods exhibit severe performance degradation in the minority classes.
That is, the reported improvements from the existing methods were attributed to the majority classes, not the minority classes.
In contrast, the proposed IB loss exhibited a significant improvement in all the minority classes. 


It is noteworthy that the performance improvement was not significant, especially on the step-imbalanced CIFAR-10 with the focal loss~\cite{ref:lin_focal_loss_iccv17} method.
We argue that this demonstrates that most hard examples are majority samples in highly imbalanced data and that those samples enforce the decision boundary to be overfitted.
In contrast, our proposed influence-balanced re-weighing can mitigate the influences of the majority samples that cause overfitting.
As a result, it can achieve robust and superior performance for the minority classes with a very small number of samples.

Although using the influence-balanced loss alone can achieve significant enhancement for the classification of the minority classes, it is beneficial to combine it with other methods. 
For example, the results indicate that applying the influence-balanced loss with the focal loss can encourage the network to learn `good' hard samples, while down-weighting the influential ones that induce overfitting. 

\begin{table*}[t]
\caption{
Classification accuracy (\%) of ResNet-32 on imbalanced CIFAR-10 and CIFAR-100 datasets.
``" indicates that the results are copied from the original paper, 
and ``" means that the results are from the experiments in CB~\cite{ref:cui_belongie_cvpr19}.
The best results are marked in bold.
}
\vspace{0.2cm}
\label{result-table}
\centering
\small
{
\begin{tabular}{
l
ccccc
ccccc
} 
    \toprule
    &  \multicolumn{5}{c}{\textbf{Imbalanced CIFAR-10}}  
    &  \multicolumn{5}{c}{\textbf{Imbalanced CIFAR-100}} 
    \\
    \cmidrule(lr){2-6}\cmidrule(lr){7-11}
    Imbalance ()
    & 200 & 100 & 50 & 20 & 10 
    & 200 & 100 & 50 & 20 & 10 
    \\
    \midrule\midrule
    \textit{\textbf{Long-Tailed}}
    &  &  &  &  & 
    &  &  &  &  & 
    \\
    Baseline (CE)
& 66.28	
        & 70.87	
        & 78.22	
        & 82.43
        & 86.49
& 33.54	
        & 38.05	
        & 43.71	
        & 51.21	
        & 56.96
        \\
        
    Focal~\cite{ref:lin_focal_loss_iccv17}
& 65.29	
        & 70.38 	
        & 76.71 
        & 82.76 
        & 86.66 
& 35.62
        & 38.41 
        & 44.32 
        & 51.95
        & 55.78
        \\
    
    CB ~\cite{ref:cui_belongie_cvpr19}
& 68.89	
        & 74.57	
        & 79.27	
        & 84.36	
        & 87.49
& 36.23	
        & 39.60	
        & 45.32	
        & 52.59	
        & 57.99
        \\
        
    LDAM~\cite{ref:cao_ldam_neurips2019}
& -
        & 73.35
        & -
        & -
        & 86.96
& -
        & 39.6
        & -
        & -
        & 56.91
        \\
    
    LDAM-DRW~\cite{ref:cao_ldam_neurips2019}
& -	
        & 77.03	
        & -	
        & -	
        & 88.16
& -	
        & 42.04	
        & -	
        & -	
        & 57.99
        \\
    
    \midrule
    IB 
& 73.96	
        & 78.26	
        & \textbf{81.70}
        & \textbf{85.8}	
        & \textbf{88.25}
& 37.31	
        & \textbf{42.14}	
        & 46.22	
        & 52.63	
        & 57.13
        \\
    
    IB + CB 
& 73.69	
        & 78.04	
        & 81.54	
        & 85.42
        & 88.09
& 37.06	
        & 41.31	
        & 46.16	
        & 52.74	
        & 56.78
        \\
        
    IB + Focal 
& \textbf{75.05}	
        & \textbf{79.76	}
        & 81.51	
        & 85.31	
        & 88.04
& \textbf{38.23}	
        & 42.06	
        & \textbf{47.49}	
        & \textbf{53.28}	
        & \textbf{58.20}
        \\
    
    \midrule\midrule
    
    \textit{\textbf{Step-Imbalance}}
    &  &  &  &  &  
    &  &  &  &  &  
    \\
    
    Baseline (CE)
& 56.97	
        & 64.81	
        & 69.35
        & 79.71	
        & 84.16
& 38.29	
        & 39.27	
        & 41.65	
        & 48.55	
        & 54.13
        \\
    
    LDAM~\cite{ref:cao_ldam_neurips2019}
& -
        & 66.58
        & -
        & -
        & 85.00
& -
        & 39.58
        & -
        & -
        & 56.27
        \\
        
    LDAM-DRW~\cite{ref:cao_ldam_neurips2019}
& -	
        & 76.92	
        & -	
        & -	
        & 87.81
& -	
        & 45.36	
        & -	
        & -	
        & 59.46
        \\
        
    \midrule
    IB 
& 72.15	
        & 76.53	
        & 81.66	
        & 85.41	
        & 87.72
& 39.66	
        & \textbf{45.39}	
        & \textbf{48.93}	
        & 53.57	
        & 57.96
        \\
    
    IB + CB 
& 69.96	
        & 75.97	
        & 82.09	
        & 85.27	
        & \textbf{88.01}
& 39.69	
        & 45.27	
        & 48.80
        & 53.42	
        & 57.86
        \\
        
    IB + Focal 
& \textbf{74.12}	
        & \textbf{77.97}	
        & \textbf{82.38}	
        & \textbf{85.68}	
        & 87.90
& \textbf{40.39}	
        & 44.96	
        & 48.92	
        & \textbf{54.53}	
        & \textbf{59.54}
        \\
    
    \bottomrule
\end{tabular}
}
\vspace{0.2cm}
\end{table*}

 





\subsection{Comparison with State-of-the-Art}
\vspace{-1mm}



\textbf{Experimental results on CIFAR.}
The overall classification accuracy is provided in Table \ref{result-table}.
The model performance is reported on the unbiased test set as the same as the other methods.
The results indicate that adopting the proposed influence-balanced loss significantly improves the generalization performance and outperforms the recent cost-sensitive loss methods. 
On multiple benchmark datasets, using IB loss alone could achieve the best performance.
This suggests that it is effective for the robustness of the model to balance the influence of samples responsible for overfitting the decision boundary.
When combined with other methods~\cite{ref:cui_belongie_cvpr19, ref:lin_focal_loss_iccv17}, we could further improve the accuracy on multiple datasets.
This indicates that our proposed method of down-weighting influential samples that induce overfitting can benefit other methods as well. \vspace{1mm}

\textbf{Experimental results on Tiny ImageNet.}
We evaluated our method on Tiny ImageNet.
While we performed the experiments for the other baselines, the results of LDAM were copied from their original paper.
As presented in Table \ref{TinyImageNet}, IB loss outperforms other baselines on Tiny ImageNet as well.




\textbf{Experimental results on iNaturalist 2018.}
We evaluated our method on the large-scale real-world image data, iNaturalist 2018.
We compared our method with the state-of-the-art loss-based methods. 
Table~\ref{table:result_inat} reveals that simply balancing the influence of loss could achieve considerable improvement. 


