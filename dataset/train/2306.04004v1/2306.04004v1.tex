

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2023}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\def\mA{{\mathbf{A}}}
\def\mB{{\mathbf{B}}}
\def\mC{{\mathbf{C}}}
\def\mD{{\mathbf{D}}}
\def\mE{{\mathbf{E}}}
\def\mF{{\mathbf{F}}}
\def\mG{{\mathbf{G}}}
\def\mH{{\mathbf{H}}}
\def\mI{{\mathbf{I}}}
\def\mJ{{\mathbf{J}}}
\def\mK{{\mathbf{K}}}
\def\mL{{\mathbf{L}}}
\def\mM{{\mathbf{M}}}
\def\mN{{\mathbf{N}}}
\def\mO{{\mathbf{O}}}
\def\mP{{\mathbf{P}}}
\def\mQ{{\mathbf{Q}}}
\def\mR{{\mathbf{R}}}
\def\mS{{\mathbf{S}}}
\def\mT{{\mathbf{T}}}
\def\mU{{\mathbf{U}}}
\def\mV{{\mathbf{V}}}
\def\mW{{\mathbf{W}}}
\def\mX{{\mathbf{X}}}
\def\mY{{\mathbf{Y}}}
\def\mZ{{\mathbf{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\def\vzero{{\mathbf{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\va{{\bm{a}}}
\def\vb{{\mathbf{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\mathbf{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\mathbf{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\mathbf{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\mathbf{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\mathbf{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\mathbf{z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Randomized Schur Complement Views for Graph Contrastive Learning}

\begin{document}

\twocolumn[
\icmltitle{Randomized Schur Complement Views for Graph Contrastive Learning}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Vignesh Kothapalli}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Courant Institute of Mathematical Sciences, New York University, New York, USA}


\icmlcorrespondingauthor{Vignesh Kothapalli}{vk2115@nyu.edu}


\icmlkeywords{Graph Contrastive Learning, Graph Neural Networks, Schur Complements, Randomized Numerical Linear Algebra}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
We introduce a randomized topological augmentor based on Schur complements for Graph Contrastive Learning (GCL). Given a graph laplacian matrix, the technique generates unbiased approximations of its Schur complements and treats the corresponding graphs as augmented views. We discuss the benefits of our approach, provide theoretical justifications and present connections with graph diffusion. Unlike previous efforts, we study the empirical effectiveness of the augmentor in a controlled fashion by varying the design choices for subsequent GCL phases, such as encoding and contrasting. Extensive experiments on node and graph classification benchmarks demonstrate that our technique consistently outperforms pre-defined and adaptive augmentation approaches to achieve state-of-the-art results.
\end{abstract}


\section{Introduction}

Understanding the structural properties and semantics of graph data typically requires domain expertise and efficient computational tools.  Advances in machine learning techniques such as Graph Neural Networks (GNN) \citep{gori2005new, scarselli2008graph, bruna2014spectral, henaff2015deep, welling2016semi, niepert2016learning, bronstein2017geometric, xu2018powerful, wu2020comprehensive, zhou2020graph} have paved a path for representation learning on internet scale graphs and significantly alleviated the human effort. However, real-world graphs such as social networks, citation networks, supply chains and media networks are continuously evolving, which makes labeling an extremely challenging task to accomplish. Self-supervised learning (SSL) addresses this issue by optimizing pretext objectives and learning generalizable representations for downstream tasks \citep{jin2020self, wu2021self, liu2022graph, xie2022self}. This learning paradigm has been quite popular for vision \citep{gidaris2018unsupervised, hjelm2018learning, chen2020simple, jing2020self}, language domains \citep{mikolov2013efficient, devlin2018bert, radford2018improving, lan2019albert} and is relatively new to graphs.

Contrastive learning on graphs \citep{velickovic2019deep, sun2019infograph, zhu2020deep, you2020graph, hassani2020contrastive} is a variant of SSL, whose pretext task is aimed at maximizing representation agreement across augmented views. A typical GNN-based GCL framework comprises three main components: 1. Data augmentors, 2. GNN encoders and 3. Contrastive objectives with corresponding modes. Data augmentation techniques in the literature can be categorized based on: 1. Feature masking/perturbations \citep{you2020graph, thakoor2021bootstrapped}, 2. Structural perturbations \citep{you2020graph, hassani2020contrastive, zeng2021contrastive}, 3. Sub-graph sampling \citep{hu2019strategies, qiu2020gcc, jiao2020sub, zhu2021transfer}, 4. Adaptive and learnable structural perturbations \citep{zhu2021graph, yin2022autogcl}. Efforts leveraging these techniques tend to focus on the collective comparison of GCL frameworks and lack controlled experimentation pertaining to the augmentation phase. 

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{images/gcl.png}}
\caption{A generalized dual branch GCL framework. The input graph  is augmented by  to generate graph views. These graph views are encoded using GNNs  to compute node embeddings. The node embeddings are projected using MLPs  to generate node features. Additionally, the node embeddings are aggregated using a permutation invariant function  and projected using  to compute graph-level features. Depending on the contrastive mode, the graph and node features are contrasted.} 
\label{fig:gcl}
\end{center}
\vskip -0.2in
\end{figure*}

A recent empirical study on GCL by \citet{zhu2021empirical} partially addresses this issue and compares some of the widely used augmentors with a fixed encoder design and contrastive objective. In particular, the study focuses on techniques such as feature masking, node dropping, edge perturbations, sub-graph sampling based on random walks, diffusions based on Personalized Page Rank (PPR), Heat Kernel (HK) and Markov Diffusion Kernel (MDK). From a topological augmentation perspective, pre-defined stochastic techniques such as node dropping and edge perturbations  tend to be faster in practice but fail to preserve structural information of the original graph \citep{hubler2008metropolis}. Surprisingly, views obtained by such simple techniques have been shown to outperform diffusion-based strategies when contrasting solely based on node embeddings. Despite this parity, \citet{zhu2021empirical} show that, when diffusion-based approaches are combined with node dropping, there is a substantial increase in the unsupervised node and graph classification performance. An attempt to avoid such trial and error based design of augmentors was made by \citet{yin2022autogcl}, where learnable augmentors are trained in an end-to-end fashion to generate semantic label preserving views. Such a technique is limited to node-dropping operations and induces non-negligible computational overheads if edge perturbations were to be included. Furthermore, since learnable augmentors modify learning objectives, leveraging them to explore and design new GCL objectives can be challenging. On the other hand, the adaptive augmentation technique proposed by \citet{zhu2021graph} drops edges based on their degree, page rank or eigenvector centrality scores and preserves pre-defined structural information across views. However, metrics such as eigenvector centrality are expensive to compute and negate the performance benefits of augmentors. These observations highlight the persisting augmentation problem of \textbf{identifying} and \textbf{preserving} structural properties across \textbf{stochastically} generated views in a \textbf{computationally efficient} manner.

In our work, we leverage the properties of Schur complements to address these issues. Schur complements are typically obtained during Gaussian Elimination (GE) and are pervasive in numerical analysis, probability and statistics \citep{cottle1974manifestations, ouellette1981schur, zhang2006schur}. From a graph theoretic perspective, GE essentially leads to node dropping and edge perturbations such that the resulting Schur complements preserve random walk transition probabilities of the remaining nodes (with respect to the original graph). Our technique exploits this property and generates randomized yet unbiased approximations of Schur complements as the augmented views. Additionally, we exploit the role of Schur complements in matrix inversions and showcase computational optimizations for diffusion-based augmentors. Overall, the potential benefits of this class of topological augmentors remain unexplored for GCL and we take a step towards understanding them. To summarize, our contributions are as follows:
\begin{itemize}
    \item We design a randomized Schur complement based topological augmentor for GCL which is fast, stochastic and effective on a wide range of framework designs.
    \item We show that by preserving the combinatorial properties of Schur complements in expectation, the augmentor achieves state-of-the-art results on unsupervised node and graph classification tasks.
    \item Using our technique, we present an efficient approach to achieve diffusion followed by sub-graph sampling.
\end{itemize}

\section{Preliminaries}
\subsection{Notations}
Let  denote an undirected graph with nodes  and edges . An edge between nodes  is given by . We drop the subscript in  when indexing can be avoided. The neighborhood of  is given by . For notational convenience,  represents a node  being on either end of an edge . A weight function  maps an edge  to its weight . Also,  is overloaded to denote the total weight of a node as the sum of weights of edges incident on it: . Similarly, the degree of a node  is the total number of edges incident on it. The feature matrix of  is given by  where features of a node  are given by the row . Finally, we consider the graph  to be associated with a weighted positive semi-definite laplacian matrix . 



\subsection{Graph Contrastive Learning Frameworks}

In this section, we briefly describe the functionality of GCL framework components (see Figure \ref{fig:gcl}) and defer additional details on the mathematical formulations to Appendix \ref{appendix:experimental_setup}.

\subsubsection{Augmentors}

Without loss of generality, let  represent a class of augmentation functions from which  are chosen. These functions transform  to the respective augmented views  by perturbing the topology, features or both. \citet{hassani2020contrastive} observed that, when  represents a class of diffusion-based augmentors, contrasting more than 2 views didn't improve the performance on the downstream node and graph classification tasks. Additionally, the approach of \citet{velickovic2019deep} uses only a single view to contrast the node level and graph level embeddings. Analyzing the effect of the number of views based on choices of  is still an open problem. Meanwhile, for the rest of this paper, we default it to 2.

\subsubsection{Encoders and Projectors}

Depending on design choices for the framework, one can choose either a shared encoder \citep{you2020graph} or dedicated encoders \citep{hassani2020contrastive} to compute the node embeddings for . Typically, GNNs based on Graph Convolution Network (GCN) \citep{kipf2016semi} or Graph Isomorphism Network (GIN) \citep{xu2018powerful} are employed. We denote the node level embeddings of  as . Next, an MLP-based projector is employed to project the embeddings onto a latent space. We denote the projected node level features of  as . Additionally, the node embeddings are aggregated using a permutation invariant function  (eg: mean) and projected to compute graph level features\footnote{Designs such as MVGRL \cite{hassani2020contrastive} employ MLP projections to improve the graph level representations.} . 

\subsubsection{Contrastive modes and objectives}

With  and  being available, a relevant contrastive objective is chosen for optimization. Popular choices include: Noise Contrastive Estimation based InfoNCE \cite{oord2018representation}, Jensen-Shannon Divergence (JSD) \cite{lin1991divergence}, Bootstrapping Latent (BL) \cite{grill2020bootstrap}, Variance-Invariance-Covariance Regularization (VICReg) \cite{bardes2021vicreg} and Barlow Twins (BT) \cite{zbontar2021barlow} losses. Finally, a mode is chosen to determine the granularity of contrasting:

\begin{enumerate}
    \item \textbf{local-local (l-l)}: The objective is solely dependent on the node-level features.
    \item \textbf{global-local (g-l)}: The objective is dependent on both node and graph-level features.
    \item \textbf{global-global (g-g)}: The objective is solely dependent on the graph-level features.
\end{enumerate}

The choice of mode and objective not only affects the GCL framework performance but also plays a key role in computational efficiency. For instance, \citet{zhu2021empirical} empirically show that, in local-local mode, losses such as BL, BT, and VICReg consume  less memory than InfoNCE and JSD while achieving comparable performance on node and graph classification tasks. Finally, when it comes to objectives such as InfoNCE and JSD, choosing the optimal set of negative samples is not straightforward. Especially, the difficulty lies in achieving a balance between computational overheads of negative sampling \citep{chuang2020debiased} (with hard negatives if required by design \citep{robinson2020contrastive}) and performance improvements.

\subsection{Schur Complements and Gaussian Elimination }

Every edge  of graph  forms an elementary laplacian given by the outer product:
. Here,  denotes the standard basis vector at index  and  denotes the adjoint. The weighted Laplacian  can now be formulated as follows:

Since every  is positive semi-definite, so is their weighted sum . The off-diagonal elements of  are given by:  and the diagonal elements by . Now, without loss of generality, consider the matrix representation of  as:

Where  and . The Schur complement of  with respect to nodes  is a positive semi-definite matrix that is obtained when  is eliminated via a GE step:

Alternatively, a graph theoretic interpretation of GE states that  is obtained by removing the  graph corresponding to  from  and adding the induced  graph. Formally:

Where  for any node  are given by:


This result indicates that  is a weighted Laplacian matrix with an associated graph. For simplicity, we denote  as  when the notion of a graph is suitable for the context. Schur complements hold an interesting graph theoretic property that, the random walk transition probabilities of the remaining nodes  through the eliminated vertex  (with respect to ) are preserved in . Intuitively, the list of nodes visited by random walks on  is equivalent in distribution to the list of nodes in  visited by random walks on . Refer to section 3 in \citet{durfee2019fully} and section 4.1 in \citet{gao2022fully} for detailed discussions on this property.

\section{Methodology}

To leverage these combinatorial properties of Schur complements in our contrastive views, two issues need to be addressed: The  overhead to compute a clique and a lack of stochasticity in the gaussian elimination procedure. We address both these issues by developing a clique approximation procedure that inherently introduces randomness. Thus, by eliminating nodes via GE and computing unbiased approximations of the induced cliques, our augmentation technique generates randomized Schur complements which preserve the random walk transition probabilities of the remaining nodes in expectation.

The need for stochasticity is based on the empirical results of \citet{zhu2021empirical}, where introducing randomness to the views (eg: combining diffusion with node dropping) improved contrastive learning performance on downstream classification tasks. Although a rigorous analysis of such behavior is not present in the literature, we provide interesting insights on GCL performance due to randomness introduced by our approximation procedure.






\subsection{Randomized Schur Complements}

We present our Schur complement approximation procedure in Algorithm \ref{alg:rlap}.\footnote{The code is available at: \href{https://github.com/kvignesh1420/rlap}{https://github.com/kvignesh1420/rlap}} We name it  to denote the laplacian nature of its output. The input to  is the graph , the fraction of nodes to eliminate , node elimination scheme  and the neighbor ordering scheme . The output is a randomized Schur complement of  after eliminating  nodes\footnote{Elimination in this context indicates that a node is disconnected from all its neighbors. The dimensions of  remain  with corresponding row and column set to .}. The scheme  indicates the order in which  nodes are eliminated. The possibilities for such a scheme are huge but we limit our analysis to random ordering and ordering based on node degree. In the `random' scheme, a node is randomly selected at each step of the outer loop. In the `degree' scheme, a priority queue is maintained to eliminate nodes based on their degree, i.e. nodes with lower degrees are eliminated before the highly connected ones.

\begin{algorithm}[tb]
  \caption{}
  \label{alg:rlap}
\begin{algorithmic}
  \STATE {\bfseries Input:} graph , node drop fraction ,  node elimination scheme , neighbor ordering scheme .
  \STATE \textbf{set} 
  \REPEAT
  \STATE 
  \STATE 
  \FOR{ {\bfseries to} }
  \STATE // \textit{choose  based on the conditional probability}
  \STATE 
  \STATE // \textit{compute weight of the new edge between }
  \STATE 
  \STATE 
  \ENDFOR
  \STATE  
  \STATE 
  \UNTIL{all the  nodes are eliminated based on }
  \STATE \textbf{return} 
\end{algorithmic}
\end{algorithm}

Now, without loss of generality, consider the  iteration of the outer loop where node  is being eliminated. The first step is to order the neighbors  and store the result in  \footnote{ are indexed by  to denote the current state.}. The order decided by  affects the arrangement of edges in the approximated clique . Specifically, while iterating over the neighbors  in the inner loop, we compute a conditional probability  of choosing a neighbor , based on which, a new edge between  is created. The choice of  affects this conditional probability and can lead to sparser or denser variants of approximated cliques. Now, the weighted elementary laplacian  of the newly formed edge  is computed and added to  \footnote{Here  are indexed on  for notational convenience, but have a one-to-one correspondence with our original node set }. Note that, the inner loop iterates over  nodes for approximating the clique and avoids the  overhead for exact computation. Finally,  is subtracted and  is added to  to continue the elimination process.


\begin{theorem}
\label{thm:rlap}
Given an undirected graph , node dropping fraction , node elimination scheme  and neighbor ordering scheme , the output of  is an unbiased estimator of the schur complement , where  denotes the set of nodes that remain after  eliminations.
\end{theorem}
\begin{proof} Without loss of generality, let  be the state after eliminating  nodes. Let  indicate the node which is being eliminated in the  iteration of the outer loop. The proof is based on the loop invariant that  after the end of this iteration. With this guarantee, we continue the elimination process for  iterations and achieve the desired randomized Schur complement which equals  under expectation.  The proof is available in Appendix \ref{app:poc_rlap} along with the tail bounds of deviation for the laplacian matrix martingale.
\end{proof}



\subsection{Connections with Graph Diffusion}

For a graph , the diffusion operator is a polynomial filter on  which mitigates the effect of noisy neighbors. The diffused output  is given by:

Where  is a transition matrix \citep{klicpera2019diffusion} and can be chosen as either  or ,  are the adjacency and degree matrices for  respectively,   is the scaling coefficient with  and  representing the coefficients for personalized page rank \citep{page1999pagerank} and heat kernels with diffusion time  \citep{kondor2002diffusion, chung2007heat} respectively. When , the closed forms of diffusions are given by:



Note that, for , computing  is an expensive operation over the entire graph and leads to undesired bottlenecks when the desired augmented view during contrasting is just a relatively smaller sub-graph. We address this issue in the following theorem using .

\begin{theorem}
\label{thm:theta_schur}
Let . The sub-graph with nodes  sampled from  is given by , where  is the approximation of schur complement  given by ,  is a diagonal matrix formed by selecting entries corresponding to  from  and  represent the adjacency and degree matrices w.r.t .
\end{theorem}
\begin{proof}
The proof is a simple expansion of the limit:

 \\ 
\\ 
\\
\\
 \\


The equality 
is based on the guarantees of Theorem \ref{thm:rlap} when  and . The equality  is based on the standard matrix inversion lemma. Refer to \citet{gallier2010notes} and Appendix C.4 in \citet{boyd2004convex} for further details.
\end{proof}

When  is not close to ,  doesn't represent a graph laplacian but instead represents a symmetric diagonally dominant matrix (SDDM). Similar to the laplacian property of Schur complements, it can be shown that Schur complements of SDDM matrices are also SDDM (refer Appendix A, Lemma A.1 in \citet{fahrbach2020faster} for the proof). To better understand the implications of Theorem \ref{thm:theta_schur}, consider a graph  on which the PPR diffusion operator needs to be applied, followed by a sub-graph sampling operation with respect to nodes . Such a requirement is pervasive in GCL when views are based on diffusion matrices. However, the overheads of computing  for medium-large scale graphs can be significant. Based on the empirical analysis of \citet{klicpera2019diffusion}, the optimal choice of  for  typically lies in a close range of . This implies  is close to , which justifies our assumption in Theorem \ref{thm:theta_schur} for practical settings. Thus, instead of diffusion followed by sub-graph sampling, one can apply  on  to obtain the randomized Schur complement which is relatively sparse, followed by the diffusion operator (up to  scaling). See Figure \ref{fig:rlap_diffusion} for an illustration.


\begin{figure}[ht]
\vskip 0.2in
\centering
\centerline{\includegraphics[width=\columnwidth]{images/rlap_diffusion.png}}
\caption{Illustration of diffusion + sampling vs  + diffusion with a toy example. In the first row, diffusion is applied on a graph  followed by sampling the sub-graph corresponding to nodes: . In the second row, the expected output of  on  with  followed by diffusion on the randomized Schur complement gives the same desired sub-graph.} 
\label{fig:rlap_diffusion}
\vskip -0.2in
\end{figure}


\subsection{Augmentation with rLap}

\begin{algorithm}[tb]
  \caption{A generalized GCL framework with .}
  \label{alg:rlap_gcl}
\begin{algorithmic}
  \STATE {\bfseries Input:} Graph(s)  , node elimination scheme , neighbor ordering scheme , fraction of nodes to eliminate for both views , encoders , node level projectors , aggregation function , graph level projectors ,  mode .
  \REPEAT
  \FOR{  in }
  \STATE  \hfill // first view
  \STATE  \hfill //node embeddings
  \STATE  \hfill // features
  \STATE  \hfill // second view
  \STATE  \hfill //node embeddings
  \STATE  \hfill // features
  \ENDFOR
  \STATE // compute objectives based on contrastive modes
  \STATE 
  \FOR{ and }
  \IF{}
   \STATE   
   \ELSIF{} 
   \STATE   
   \ELSIF{} 
   \STATE   
   \ENDIF
  \ENDFOR
  \STATE // compute gradients with appropriate normalization 
  \STATE 
  \UNTIL{convergence/tolerance}
\end{algorithmic}
\end{algorithm}

We present a generalized GCL framework with  as the augmentor in Algorithm \ref{alg:rlap_gcl}. Given a collection of graphs , the blueprint presented in Algorithm \ref{alg:rlap_gcl} can be used for learning node and graph features via different design choices for encoders (including projectors) and contrastive objectives. For every graph ,  generates the randomized schur complement views , for which, the graph features  and node features  are learnt by the encoders and projectors. The learning objective depends on the contrastive mode , which can be either `l-l', `g-l' or `g-g' as defined in the preliminaries. Most of the GCL frameworks in the literature can be derived from this blueprint. For instance, by replacing  with a uniform node dropping augmentor, using shared GCN-based encoders , MLP-projectors , `g-g' contrastive mode and InfoNCE loss , we obtain the widely used GraphCL framework \citep{you2020graph}. See Appendix \ref{appendix:experimental_setup} for illustrations and details.

However, recent efforts in developing augmentation techniques tend to restrict the design choices of encoders and objectives when comparing the overall GCL performance with prior works. Such an approach fails to isolate the effectiveness and limitations of an augmentor from other design choices. We break this norm and follow established benchmark practices of \citet{zhu2021empirical} to perform controlled experiments in the following section.

\section{Experiments}

Owing to the framework-agnostic nature of augmentors, we conduct controlled experiments and evaluate their effectiveness on unsupervised node and graph classification tasks under the linear evaluation protocol \citep{velickovic2019deep}. The controlled settings pertain to fixing the shared/dedicated nature of encoder(s), contrastive modes, and objectives (see Table \ref{table:exp_settings}). Especially, we leverage the designs of 4 widely used GCL frameworks: GRACE \citep{zhu2020deep}, MVGRL \citep{hassani2020contrastive}, GraphCL \citep{you2020graph} and BGRL \citep{thakoor2021bootstrapped}. Comprehensive details on experimental settings, datasets, augmentors, design choices, evaluation protocols  and baselines are available in Appendix \ref{appendix:experimental_setup}. In the following results for , we employ a `random' node selection scheme as  and an ordering scheme based on increasing order of edge weights as . A detailed ablation study on these schemes is presented in Appendix \ref{app:rlap_ablation}.

\begin{table}[ht]
\caption{Control settings for evaluating augmentors.}
\label{table:exp_settings}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Setting & design choices\\
\midrule
Dataset     & \textbf{eval} \\
Augmentor   & \textbf{eval} \\
Encoders    &  Shared, dedicated \\
Mode    & \textit{l-l, g-l, g-g} \\
Objective   & InfoNCE, JSD, BL\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Node Classification Results}

\begin{table*}[ht!]
\centering
\caption{Evaluation (in accuracy) on benchmark node datasets with \textbf{GRACE} based design.}
\label{table:results_grace}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Augmentor & CORA & Amazon-Photo & PubMed & Coauthor-CS & Coauthor-Phy \\
\midrule
EdgeAddition &  &   &  &  &  \\
EdgeDropping &  &  &   &  &  \\
EdgeDroppingDegree & \underline{}  &  &  &  &  \\
EdgeDroppingEVC &  &  &   &  & \\
EdgeDroppingPR &  &   &  &  &   \\
MarkovDiffusion &   &    &  &  &  \\
NodeDropping &   & \underline{} &  & \underline{} & \underline{}  \\
PPRDiffusion &   &  &  &  &  \\
RandomWalkSubgraph &  &  & \underline{}  &  & \\
rLap &   &  &  &   &  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[ht!]
\centering
\caption{Evaluation (in accuracy) on benchmark node datasets with \textbf{MVGRL} based design.}
\label{table:results_mvgrl}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Augmentor & CORA & Amazon-Photo & PubMed & Coauthor-CS & Coauthor-Phy \\
\midrule
EdgeAddition &   &  &   &  &  \\
EdgeDropping &   &  &   &    & \underline{}\\
EdgeDroppingDegree &   &  &  &  & \\
EdgeDroppingEVC &   &  & \underline{} & \underline{} &  \\
EdgeDroppingPR &  &   &  &  & \\
MarkovDiffusion &   & \underline{} &  &  &  \\
NodeDropping & \underline{} &  &  &  &   \\
PPRDiffusion &   &   &  &  &  \\
RandomWalkSubgraph &   &  &  &  & \\
rLap &  &  &   &  &  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

For node classification tasks, we report the performance using GRACE and MVGRL-based designs in Table \ref{table:results_grace}, \ref{table:results_mvgrl} respectively. The GRACE design corresponds to shared encoders, `l-l' contrastive mode, and InfoNCE objective. In this setting,  achieves the best performance across all datasets, followed by EdgeDropping variants and NodeDropping. It is interesting to observe that PPRDiffusion and MarkovDiffusion approaches tend to perform relatively poorly in this setting. This is due to the fixed structure of the augmented views during training that a shared encoder is attempting to contrast. Intuitively, the lack of stochasticity fails to present noisy views to the model and improve its contrastive ability. On the other hand, for every node that is eliminated by , it incorporates the lost information in the remaining sub-graph and preserves the random walk-based combinatorial properties in expectation. This approach of leveraging global information in augmented views seems to benefit the GNN encoders. Additionally, since randomized Schur complements are based on node and edge perturbations, one can intuitively consider it as an effective combination of the two.

On the other hand, MVGRL design corresponds to dedicated encoders, `g-l' contrastive mode, and JSD objective. In this setting, diffusion-based methods performed relatively better on datasets such as CORA and AMAZON-PHOTO when compared to  and NodeDropping. During our experiments on PUBMED, COAUTHOR-CS, and COAUTHOR-PHY, the PPRDiffusion technique led to out-of-memory (OOM) issues on a 32GB GPU, which we addressed using sub-graph sampling approaches. One can observe that our technique is outperformed by diffusion and adaptive augmentors such as EdgeDroppingEVC on CORA and AMAZON-PHOTO and the margin of improvement with  on PUBMED, COAUTHOR-CS and COAUTHOR-PHY is minimal. However, we emphasize the observation that computational overheads of PPRDiffusion, MarkovDiffusion and EdgeDroppingEVC techniques are significantly large when compared to  (see Table \ref{table:node_aug_stats}). In this setting,  achieves the best trade-off between performance and computational overheads. Furthermore, based on Theorem \ref{thm:theta_schur}, one can combine  with PPRDiffusion to reduce the computational overheads (see Appendix \ref{app:add_exp}). An interesting observation related to MVGRL design and non-diffusion based augmentors is that a perturbation ratio  close to  for one of the views was preferred in most of the experiments. Intuitively, one can think of this setting as contrasting a highly perturbed graph with a relatively less perturbed one, which reflects the essence of multi-view contrasting.

\subsection{Graph Classification Results}



\begin{table*}[ht!]
\centering
\caption{Evaluation (in accuracy) on benchmark graph datasets with \textbf{GraphCL} based design.}
\label{table:results_graphcl}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Augmentor & PROTEINS & IMDB-BINARY & MUTAG & IMDB-MULTI & NCI1 \\
\midrule
EdgeAddition &   &    &    &   &  \\
EdgeDropping &   &   &   &    &  \\
EdgeDroppingDegree &  &  &  & \underline{} &    \\
EdgeDroppingEVC &  & \underline{}  &  &   &   \\
EdgeDroppingPR &   &   &  &  & \underline{} \\
MarkovDiffusion &   &   &  &   & \\
NodeDropping &   &   &    &   &  \\
PPRDiffusion &    &    & \underline{}  &  &  \\
RandomWalkSubgraph & \underline{}  &   &    &  &  \\
rLap &   &   &   &  &   \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[ht!]
\centering
\caption{Evaluation (in accuracy) on benchmark graph datasets with \textbf{BGRL} based design. }
\label{table:results_bgrl}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Augmentor & PROTEINS & IMDB-BINARY & MUTAG &  IMDB-MULTI & NCI1\\
\midrule
EdgeAddition &   &   &   &  &  \\
EdgeDropping &   &  \underline{} &  &  &  \\
EdgeDroppingDegree &  &    &  & \underline{} &  \\
EdgeDroppingEVC &   &   &   &  &  \\
EdgeDroppingPR &   &    &  &   &  \\
MarkovDiffusion &   &    & \underline{} &   &   \\
NodeDropping &    &   &  &  &  \\
PPRDiffusion &   &   &  &   &  \\
RandomWalkSubgraph & \underline{}  & &  &   & \underline{} \\
rLap &  &   &  &  &    \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

For graph classification tasks, we report the performance using GraphCL and BGRL designs in Table \ref{table:results_graphcl}, \ref{table:results_bgrl} respectively. GraphCL design corresponds to shared encoders, `g-g' contrastive mode and InfoNCE objective. Similar to our observations with shared encoders and InfoNCE loss in the node classification setting,  outperformed other augmentors on PROTEINS, MUTAG, IMDB-MULTI and NCI1 datasets and is quite close to the EdgeDroppingPR technique on IMDB-BINARY dataset. EdgedroppingPR was the best adaptive edge augmentor on PROTEINS, IMDB-BINARY and NCI1 whereas EdgeDroppingDegree performed well on MUTAG and IMDB-MULTI. One should note that leveraging PageRank information in EdgeDroppingPR to perform stochastic augmentations consistently led to better performance than the PPRDiffusion approach, which underscores the benefits of stochasticity. Interestingly, we found that this particular design preferred augmentation ratios () to be nearly equal (see Table \ref{table:hp_graphcl}), indicating that in `g-g' mode, the shared encoders prefer to contrast similar sized graphs.

In the next setting, we use a modified BGRL design which corresponds to shared encoders, `g-l' contrastive mode and BL objective. Unlike previously mentioned designs, BGRL employs an online and target encoder with weight-sharing based on a moving average technique. The original BGRL design \citep{thakoor2021bootstrapped} uses an `l-l' contrastive mode without negative samples and focuses on node classification settings. In our work, we leverage the modularity of the PyGCL library and incorporate the `g-l' contrastive mode for graph classification. With this design, we report some of the highest observed performances on the PROTEINS dataset in unsupervised settings, with  achieving the highest classification accuracy of . Additionally, it achieves the best results on MUTAG, IMDB-MULTI and NCI1 but continues to suffer on IMDB-BINARY. However, in our ablation studies (see Appendix \ref{app:rlap_ablation}), we observed that by changing the node elimination scheme from being a `random' selection to a minimum `degree' selection, this  variant achieves  accuracy on IMDB-BINARY and surpasses other techniques.

\section{Future Research}

\textbf{ Towards distributed augmentation:} Current approaches to augment large-scale graphs tend to rely on  simpler techniques such as edge dropping \cite{thakoor2021bootstrapped}. Thus, by extending  augmentation to a distributed setting and leveraging its connection with graph diffusion, future efforts can explore and unlock the potential benefits of a richer class of augmentations for large-scale GCL. However, as a current challenge to achieving this goal, we observed that the resulting views of  can be denser than the  approach. Thus,  can address the bottlenecks of PPRDiffusion by saving memory during augmentation, but the GNN encoders would eventually consume additional memory due to the message-passing operations on relatively more edges. Although this overhead might not be significant for small-medium scale graphs, we believe that the reader should be aware of these practical trade-offs when dealing with large-scale graphs.

\textbf{ Towards randomized second-order optimizers:} Computing the inverse of the Hessian for large neural networks poses significant computational overheads to optimizers that aim to leverage the curvature of the loss landscape. A popular technique to address this issue is to compute layer-wise block diagonal approximations of the hessian \citep{martens2015optimizing, osawa2019large, hoefler2021sparsity}, which is relatively easy to invert. Alternatively, to capture the second-order information for a subset of parameters (for example, pertaining to a single layer), one can leverage the matrix inversion lemma and compute randomized Schur complements with respect to these parameters to negate the need to invert large Hessian matrices. Especially, by extending , this approach can be computationally efficient while also incorporating unbiased curvature information into the parameter updates. This strategy can be of significant interest when only a subset of neural network layers need to be fine-tuned for downstream tasks.

\textbf{ Towards fine-grained augmentor benchmarks:} Our paper presents an extensive augmentor benchmark with respect to various GCL design choices. Thus, future efforts can leverage and extend these fine-grained comparisons for reproducible and fair comparison of augmentors.

\section{Conclusion}

In this work, we designed a fast, stochastic augmentor based on randomized Schur complements and demonstrated its flexibility and effectiveness on a variety of GCL designs and benchmark datasets. Our theoretical analysis and extensive experiments have proved that  achieves a perfect balance with respect to performance and computational overheads while achieving state-of-the-art accuracies on unsupervised node and graph classification tasks. Especially, we achieved an unsupervised graph classification accuracy of  on the PROTEINS dataset under linear evaluation and exceeded current state-of-the-art results. To conclude, we emphasize the potential of this new class of augmentation techniques that enable GCL frameworks to leverage the stochasticity and combinatorial properties of randomized Schur complements.

\section*{Acknowledgements}

VK would like to thank Prof. Jonathan Weare, Zhengdao Chen,  and the anonymous reviewers for their constructive feedback and prompt discussions during the preparation of this manuscript.








\bibliography{main}
\bibliographystyle{icml2023}


\newpage
\appendix
\onecolumn

\section{Related Work}
\label{app:related_work}

Earlier works on representation learning techniques on graphs with limited labels leveraged exploration-based approaches such as DeepWalk \citep{perozzi2014deepwalk} and node2vec \citep{grover2016node2vec}. Inspired by the skip-gram model of word2vec \citep{mikolov2013efficient}, such techniques generate similar embeddings for nodes that co-occur in a random walk. On the other hand, techniques such as Large-scale Information Network Embedding (LINE) \citep{tang2015line} and Predictive Text Embedding (PTE) \citep{tang2015pte} learn representations by contrasting nodes with negative samples drawn from a noisy distribution. Eventually, in the work of \citet{qiu2018network}, DeepWalk, node2vec, LINE and PTE were unified under a common matrix factorization scheme for computing the network/graph embeddings. However, their limitations on scalability and incorporating global information in the learning process persisted.

Recent efforts in GCL leverage GNNs as scalable feature encoders and are inspired by techniques/frameworks in vision \citep{gidaris2018unsupervised, hjelm2018learning, chen2020simple, jing2020self} and language domains \citep{mikolov2013efficient, devlin2018bert, radford2018improving, lan2019albert}. Some of the notable efforts by \citet{velickovic2019deep, sun2019infograph, zhu2020deep, you2020graph, hassani2020contrastive} employ these GCL frameworks and aim to minimize objectives based on mutual information. With this growing line of work, a critical aspect of most GCL frameworks that is devised based on trial and error is the augmentation phase. The node/edge perturbation techniques introduced in \citet{you2020graph} have been widely employed due to their simplicity and are empirically effective on unsupervised, self-supervised node and graph classification tasks \citep{zhu2021empirical, thakoor2021bootstrapped}. To the contrary, \citet{hassani2020contrastive} showed that augmentations such as graph diffusions can outperform the stochastic techniques with dedicated encoders. Since then, efforts have been made to devise augmentations that can leverage structural information during the training process either in a pre-defined \citep{zhu2021graph} or learnable fashion \citep{yin2022autogcl}. The adaptive augmentation technique by \citet{zhu2021graph} takes a step in this direction and focuses on edge-dropping mechanisms based on centrality measures. We observed in our experiments that, it is difficult to choose a particular centrality measure that can work for both node and graph-based tasks. Furthermore, measures such as eigenvector centrality are computationally expensive, which makes such adaptive techniques orders of magnitude slower than simple edge dropping. 

To address these issues, we designed the  algorithm, which is inspired by the approximate gaussian elimination (AGE) technique introduced by \citet{kyng2016approximate}. The AGE technique leverages the idea of effective resistances \citep{spielman2008graph} and falls in the line of work on fast approximate linear system solvers for SDDM matrices \citep{spielman2003solving, spielman2004nearly, koutis2011nearly, cohen2014solving, kyng2016sparsified}. It has been influential in efficiently solving linear systems of equations \citep{cohen2018solving, peng2021solving, cohen2021solving, chen2021rchol}, sampling random spanning trees \citep{durfee2017sampling} and matrix scaling \citep{cohen2017matrix}. The technique of \citet{kyng2016approximate} was later adopted by \citet{fahrbach2020faster} for generating node embeddings using sparse matrix factorization techniques. However, to the best of our knowledge, techniques along this line of work have not been explored for GCL.


\section{Proof for Theorem \ref{thm:rlap}}
\label{app:poc_rlap}

Recall from the preliminaries that: 


The factor  in the denominator addresses duplicate laplacians in the quadratic sum. We assume that  doesn't have self-loops and eliminate these redundant summations to rewrite  as:


To employ this reformulation in Algorithm \ref{alg:rlap}, we assume  as the Schur complement at the beginning of  iteration of the outer loop and  as the selected node for elimination. Then:






In Eq. \ref{eq:new_clique_sc}, we eliminated half the effort needed to compute , but introduced an ordering of the neighbors . This ordering is determined by  in Algorithm \ref{alg:rlap} and leads to different conditional probabilities  while computing the approximate clique . The ordered nodes are given by  and  for notational convenience. Then,  is computed as follows:



By substituting the conditional probabilities and edge weights as per Algorithm \ref{alg:rlap}, we get:



The factors cancel out and simplify the expectation to:




Thus, after node  has been eliminated,  is an unbiased estimator of the actual clique. Now, by removing the star and adding  to , we get , which is the unbiased schur complement approximation after  iterations:

By repeating this process for  iterations, we get the desired randomized schur complement .

\subsection{Deviation Analysis}

Considering , one can observe that  is a laplacian matrix, satisfying the following laplacian approximations:



This implies that at every step we remove a rank 1 matrix corresponding to . Thus, the sequence  forms a random process such that:


The last equality is based on the result that  as shown above. This implies,  for  and the `deviation' sequence  is a \textit{\textbf{matrix martingale}} with  initial value. The analysis for the `deviation' martingale  aids in understanding the randomness that is induced due to a series of Schur complement approximations.


Our line of analysis is inspired by the approach of \citet{kyng2016approximate} which was later presented in \citet{tropp2019matrix}. We relax the assumptions and transformations for spectral approximations and bounding the effective resistances. These assumptions were critical in the analysis of \citet{kyng2016approximate, tropp2019matrix} as the motivation was to solve the laplacian system of linear equations. However, since we are primarily concerned with augmentations of graphs, we avoid such constraints.

\textbf{Analysis Sketch:} We primarily leverage the corrector process approach introduced in \citet{tropp2019matrix}. By building the corrector process for the martingale , we provide a tail bound for the singular values of these deviations, which is a direct application of Theorem 7.4 in \citet{tropp2019matrix}. We start by building the individual corrector matrices and then extend them to build the corrector process.

\subsubsection{Correctors}

\begin{definition}
\textbf{Corrector process} \citep{tropp2019matrix}(Sec 7.2): Consider a function , a martingale  and a predictable random process  of self-adjoint matrices. We define  as a corrector process of martingale  if the real-valued random process  is a positive supermartingale \citep{williams1991probability, chow2003probability}. 
\end{definition}

\begin{definition}
\textbf{Corrector} \citep{tropp2019matrix}(Sec 7.3): Consider a function , a random self-adjoint matrix , a fixed matrix . We  define the corrector  as the matrix satisfying:



This bound must hold true for all fixed matrices , where  denotes a space of real  self-adjoint matrices.
\end{definition}

\begin{corollary}
\label{cor:tropp_lieb}
\citep{tropp2019matrix}(Corollary 7.7): Let  be a self-adjoint matrix and  be a random self-adjoint matrix. Then for , 


\end{corollary}

This corollary is specific to corrector matrices of the form  and will be used to build the Bernstein and Chernoff correctors for random matrices with arbitrary spectral norm.

\subsubsection{The Bernstein Corrector}

From these definitions, let  be the random self-adjoint matrix. Let  indicate the spectral norm of  ( operator norm), which indicates its largest singular value. Since , we can directly apply a generalized version of the Bernstein corrector for . The result in Proposition 7.8 of \citet{tropp2019matrix} assumes  and requires some form of normalization in the analysis. We avoid such assumptions and extend the result to any arbitrary bound . 

We begin with a scalar case and extend it to the matrix case. If , then the exponential  can be bounded by:

Similarly, if  then, we can extend the above bound to matrix form as follows:


Where  indicates inequality in the semi-definite order. For example: if , then . Also, the second inequality is based on the operator monotone property of logarithms (see Chapter 8 in \citet{tropp2015introduction}). From Corollary \ref{cor:tropp_lieb}, observe that by replacing  with  the inequality is still preserved. Thus, we can use  as our corrector matrix for . The variance term  can be given by:



The second equality is due to the zero mean of . The last semi-definite order inequality is straightforward since  indicates the largest singular value of , which bounds . Additionally, if we assume the spectral norm of the approximated cliques to be bounded , we get . Putting all these results together, we get the Bernstein corrector as follows:


The Bernstein corrector handles the randomness associated with the clique approximation after a node  is selected (i.e, due to the ordering scheme  and its implications on the edge formation probabilities). Now, to handle the randomness associated with this node selection operation, we build the Chernoff corrector.
\subsubsection{The Chernoff Corrector}

Based on the formulations of , we obtain a relation between them as follows:


Where  as introduced before. Since  is positive semi-definite and  is formed by a subset of edges of , we obtain the following semi-definite order inequalities:


This result can now be used to bound  over the possible choices of . Here  represents the set of nodes from which  is chosen uniformly at random for elimination. This setting corresponds to the case of the `random' node elimination scheme .


Where  is the elementary laplacian formed by the nodes at either end of edge . The last term in the above equation indicates that, for every node , we add the weighted laplacians for all edges in . Thus, we are iterating over all the edges in  and adding the weighted elementary laplacians twice. Formally:


Now, we prove an extension of the Chernoff bound lemma to random matrices as follows:

\begin{lemma}
\label{lemma:extended_chernoff}
Let  be a random self-adjoint matrix satisfying: , Then , .
\end{lemma}
\begin{proof}
The classical version on this statement uses  and has been proved in \citet{tropp2012user} and Lemma 1.12 in \citet{tropp2019matrix}. The proof sketch for the custom case is straightforward and can be derived from the observation that the function  is convex, singular values of  lie in  and  for valid . This gives us:

\end{proof}
By leveraging Lemma \ref{lemma:extended_chernoff} and Eq. \ref{eq:clique_star_sc_ineq} , we can bound  as . Now, if we assume a bound on the largest singular values of  as , the Chernoff corrector can be given using Eq. \ref{eq:exp_clique_bound} as follows:




\subsubsection{Composite Corrector}

Putting together the Bernstein corrector in Eq. \ref{eq:bernstein_corrector} and the Chernoff corrector in Eq. \ref{eq:chernoff_corrector}, we can create a composite corrector that accounts for the randomness determined by  and . The composition rule stated and proved in section 7.3.7 of \citet{tropp2019matrix} is defined here for context.

\begin{proposition}
\label{prop:corrector_composition}
\citep{tropp2019matrix} : The composite rule states that, given the sigma fields , Let  be a random matrix measurable over ,  are measurable over ,  are measurable over . For , suppose:

holds true, then  is the corrector for .
\end{proposition}

Where  represents function composition. To use this proposition, we can compose the Bernstein corrector  from Eq. \ref{eq:bernstein_corrector} and the Chernoff corrector  from Eq. \ref{eq:chernoff_corrector} to define the composite corrector as follows:



\subsubsection{Deviation bound}

\begin{definition}
\label{def:corrector_to_corrector_process}
\textbf{Corrector to Corrector process:} Proposition 7.10 in \citet{tropp2019matrix}: For a function , let  be a matrix martingale of self-adjoint matrices with difference sequence . Let  be a predictable sequence of self-adjoint matrices. For each , suppose  is a corrector for , conditional on the sigma field , then the predictable process  generates a corrector  for the martingale .
\end{definition}

Since we are interested in the  martingale, each matrix in this sequence can be expanded as:

Where the second equality follows from Eq. \ref{eq:lap_clique_diff_seq}. Therefore, based on Definition \ref{def:corrector_to_corrector_process}, the corrector process  for martingale  is given by:


Finally, we state the master tail bound theorem for matrix martingales (see Theorem 7.4 in \citet{tropp2019matrix} and \citet{freedman1975tail} for details on the proof based on martingale stopping time).
\begin{theorem}
\label{thm:matrix_martingale_master_tail_bound}
\citep{tropp2019matrix} If  is a corrector process for a martingale  of self-adjoint matrices and  returns the largest singular value, then:


\end{theorem}

The deviation tail bound for the martingale  can now be given as:

Where . The first inequality is a split based on the singular values so that Theorem \ref{thm:matrix_martingale_master_tail_bound} can be applied, to obtain the tail bound. The factor  instead of  (as per Theorem \ref{thm:matrix_martingale_master_tail_bound}) is obtained due to the two cases of singular value bounds in the second inequality of Eq. \ref{eq:martingale_deviation_bound} and based on the validity of the corrector for the negation of the martingale . The  term in the bound is obtained from the  dependent scalar terms of the composite corrector in Eq. \ref{eq:composite_corrector_process}. Observe that, based on Eq. \ref{eq:R_star_C_relation} and \ref{eq:clique_star_sc_ineq}, it is sufficient if we can track  during the elimination process as  essentially captures the clique approximations . Thus, by controlling , we can have a provable bound on the randomness induced during the view generation steps using .  

The analysis by \citet{kyng2016approximate, tropp2019matrix} provides one such approach. Their work applies a normalization map to the clique approximations  and leverages the concept of effective resistances \citep{spielman2008graph} to split the edges in the graph into  multi-edges (each with weight ) and bound the spectral norm . A key takeaway from their approach is that the probability of deviation can be reduced by increasing . However, note that this line of analysis aims at better pre-conditioning of the laplacian linear system with . In our analysis, since we are only concerned with introducing randomness into our graph augmentations and preserving the Schur complement properties in expectation, we present a generalized analysis and avoid strict assumptions.


\textbf{Runtime:} To analyse the runtime complexity of Algorithm \ref{alg:rlap}, observe that at iteration , when a node  is being eliminated, the corresponding  can be removed from  in  with efficient sparse tensor data types. Next, applying  can lead to  overheads. The clique sampling loop runs  times with  overheads to sample a neighbor and add the new edge. This can be achieved by using a bi-directional linked list representation of the graph with an additional list for node pointer lookup. Finally, assuming a random node elimination scheme  (i.e, our default variant of ), , the time complexity to eliminate  nodes is:



Note that if one employs a random neighbor ordering scheme for , the time complexity can be reduced to . Additionally, one can carefully design a priority queue that maintains the degrees of nodes and looks up the elimination node in . Thus,  based on min-degree elimination can still be achieved in  time.

\section{Ablation study on randomized Schur complements}
\label{app:rlap_ablation}

In the following ablation study, we consider two variants of  and three variants of  as mentioned in Table \ref{table:rlap_variants} for . The scheme  indicates a random selection of nodes for elimination and  indicates a selection based on the minimum degree nodes which haven't been eliminated. Formally, by employing a priority queue of nodes based on their current degree values, the  scheme returns a node with the lowest degree at each iteration for elimination. Once a node  has been selected, the functionality of  is to order the neighbors  based on weights of edges . The schemes  indicate sorting of  in the ascending, descending and random order of edge weights respectively. The variants of  follow the  naming convention.

\begin{table*}[ht!]
\centering
\caption{Variants of  based on .}
\label{table:rlap_variants}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|c|c|c|c}
\toprule
Variant &  &  description &  &  description\\
\midrule
rLap-rand-asc & rand & random selection & asc & ascending order of   \\
rLap-rand-desc & rand & random selection & desc & descending order of   \\
rLap-rand-rand & rand & random selection & rand & random order of   \\
rLap-deg-asc & deg & min-degree selection & asc & ascending order of   \\
rLap-deg-desc & deg & min-degree selection & desc & descending order of   \\
rLap-deg-rand & deg & min-degree selection & rand & random order of   \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

On a related note, the  approach by \citet{fahrbach2020faster} attains the same goal of approximating Schur complements in expectation and is closely related to our work. However, it differs from  in its clique approximation approach and chooses the  scheme for node elimination. The notion of  doesn't apply to their edge sampling procedure when approximating the clique. To understand the benefits of our approach and the role of , we compare the variants of  (including ) on all the benchmark datasets.

Table \ref{table:rlap_ablation_grace}, \ref{table:rlap_ablation_mvgrl} compare the unsupervised node classification performance of  variants and  using the GRACE and MVGRL design. The setup and evaluation protocols are the same as per Appendix \ref{appendix:experimental_setup}. In a majority of cases, the  variants outperformed  variants, followed by . Thus, showcasing the effectiveness of our clique sampling strategy. A similar pattern can be observed for unsupervised graph classification tasks in Table \ref{table:rlap_ablation_graphcl}, \ref{table:rlap_ablation_bgrl} using the GraphCL and BGRL designs respectively. However, the  approach dominates the  variants when the BGRL design is employed for IMDB-BINARY, i.e the minimum degree-based elimination scheme for  seems to outperform the random variants in this case.

\begin{table*}[ht!]
\centering
\caption{Evaluation (in accuracy) on benchmark node datasets with \textbf{GRACE} design and  variants.}
\label{table:rlap_ablation_grace}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Augmentor & CORA & Amazon-Photo & PubMed & Coauthor-CS & Coauthor-Phy \\
\midrule
rLap-rand-asc &  &  &  &    &  \\
rLap-rand-desc &   &  &  &  &  \underline{} \\
rLap-rand-rand &  & \underline{} & \underline{} &   &  \\
rLap-deg-asc  &    &  &  & \underline{} &   \\
rLap-deg-desc & \underline{} &   &  &  &  \\
rLap-deg-rand &  &  &  &  &  \\
Vertex coarsening &  &  &  &  &  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[ht!]
\centering
\caption{Evaluation (in accuracy) on benchmark node datasets with \textbf{MVGRL} design and  variants.}
\label{table:rlap_ablation_mvgrl}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Augmentor & CORA & Amazon-Photo & PubMed & Coauthor-CS & Coauthor-Phy \\
\midrule
rLap-rand-asc &  &  &   &   &  \\
rLap-rand-desc &  &   &  &  & \underline{}  \\
rLap-rand-rand &  &  &    &   &  \\
rLap-deg-asc &  &   &  &  &   \\
rLap-deg-desc & \underline{} &  &  & \underline{}  &   \\
rLap-deg-rand &  &  & \underline{} &  &   \\
Vertex coarsening &  &  \underline{} &  &  &  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[ht!]
\centering
\caption{Evaluation (in accuracy) on benchmark graph datasets with \textbf{GraphCL} design and  variants.}
\label{table:rlap_ablation_graphcl}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Augmentor & PROTEINS & IMDB-BINARY & MUTAG & IMDB-MULTI & NCI1\\
\midrule
rLap-rand-asc &  &   &   &  &   \\
rLap-rand-desc &  &  & \underline{} & \underline{} &  \\
rLap-rand-rand &  &  &  &  & \\
rLap-deg-asc & \underline{} &   &  &  &   \\
rLap-deg-desc &  & \underline{} &   &  & \underline{}  \\
rLap-deg-rand &  &  &  &  &  \\
Vertex Coarsening &  &   &  &  & \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[ht!]
\centering
\caption{Evaluation (in accuracy) on benchmark graph datasets with \textbf{BGRL} design and  variants.}
\label{table:rlap_ablation_bgrl}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Augmentor & PROTEINS & IMDB-BINARY & MUTAG & IMDB-MULTI & NCI1\\
\midrule
rLap-rand-asc &  &   &  &  &  \\
rLap-rand-desc & \underline{} &  &  &  &   \\
rLap-rand-rand &  &  &  &  &  \\
rLap-deg-asc &  & \underline{} & \underline{} &   & \underline{} \\
rLap-deg-desc &  &   &  &  &  \\
rLap-deg-rand &  &   &  & \underline{} &  \\
Vertex coarsening &  &  &  &  &  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Maximum singular value and edge count analysis}

For a better understanding of the effectiveness of these variants, we set  and vary the choices of  to track  as an indicator of . As analyzing hundreds of Schur complements in graph classification settings defeats the purpose of our analysis, we restricted our analysis to single graph node classification datasets to systematically track the Schur complements. Figure \ref{fig:rlap_ablation_cora}(a) plots the trend of  using  and  variants with a default setting of  on the CORA dataset. Figure \ref{fig:rlap_ablation_cora}(b) plots the edge count trend of  for these three techniques. Since  with  gave the best performance on most of the benchmark datasets, we tracked  for  with  and  in Figure \ref{fig:rlap_ablation_cora}(c), with an edge count trend being presented in Figure \ref{fig:rlap_ablation_cora}(d). A similar analysis is conducted for AMAZON-PHOTO (see Figure \ref{fig:rlap_ablation_amazon_photo}), PUBMED (see Figure \ref{fig:rlap_ablation_pubmed}), COAUTHOR-CS (see Figure \ref{fig:rlap_ablation_coauthor_cs}) and COAUTHOR-PHY (see Figure \ref{fig:rlap_ablation_coauthor_phy}).

The  values for  with  tend to be relatively smaller than  and  approaches on CORA and AMAZON-PHOTO. However, the variance in their values is larger than the latter two. On the contrary, the Schur complements have relatively higher  values for PUBMED, COAUTHOR-CS and COAUTHOR-PHY datasets. This indicates higher stochasticity in the augmented graph views when compared to  with , and . Also, note that degree-based elimination techniques lead to relatively dense  as they generate Schur complements corresponding to a set of highly connected nodes at every iteration. Thus,  generated by such approaches lead to extra computation overheads for message passing in GNN encoders. For instance,  computed by  with  on COAUTHOR-PHY and AMAZON-PHOTO with  is sparser by  edges respectively when compared to  obtained using  with . Since  is based on minimum degree-based elimination, we see a close resemblance in its trends of  and edge counts with  based on .

On the other hand, when  is fixed and  is varied, the  variants tend to generate  with similar trends of  across datasets. However, we observe a noticeable difference in the edge count trend as . For instance,  with  is sparser by  edges when compared to  on the AMAZON-PHOTO dataset. The relatively higher average degree of nodes in the AMAZON-PHOTO graph is one of the reasons for this observation. On the other hand, PUBMED has a relatively lower average degree which explains a minor difference of  edges in Schur complements of these  variants. Intuitively, nodes with higher weights (indicative of connectivity) have a higher conditional probability of having an edge in the approximate clique. When , such nodes are processed in the initial iterations of the inner loop and later iterations lead to new edges between nodes with relatively less degree. The scenario is flipped when  and the sampled edges tend to frequently merge with existing ones (note that we merge edges to avoid multi-graphs). Thus, leading to relatively sparser .


\begin{figure}[H]
\vskip 0.2in
\begin{center}
\begin{tabular}{cc}
\centering
  \includegraphics[width=70mm, height=40mm]{images/CORA_max_sv_trend.png} &   \includegraphics[width=70mm, height=40mm]{images/CORA_edge_count_trend.png} \\
(a) Trend of  on CORA & (b)  Trend of  edge count on CORA \2pt]
\end{tabular}
\caption{Plots of  and edge count vs fraction of perturbation  for  variants and  on CORA. For the  variants, plots (a), (b) illustrate the trends when  and plots (c), (d) illustrate the trends when . }
\label{fig:rlap_ablation_cora}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[H]
\vskip 0.2in
\begin{center}
\begin{tabular}{cc}
\centering
  \includegraphics[width=70mm, height=40mm]{images/AMAZON-PHOTO_max_sv_trend.png} &   \includegraphics[width=70mm, height=40mm]{images/AMAZON-PHOTO_edge_count_trend.png} \\
(a) Trend of  on AMAZON-PHOTO & (b)  Trend of  edge count on AMAZON-PHOTO \2pt]
\end{tabular}
\caption{Plots of  and edge count vs fraction of perturbation  for  variants and  on AMAZON-PHOTO. For the  variants, plots (a), (b) illustrate the trends when  and plots (c), (d) illustrate the trends when .  }
\label{fig:rlap_ablation_amazon_photo}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[H]
\vskip 0.2in
\begin{center}
\begin{tabular}{cc}
\centering
  \includegraphics[width=70mm, height=40mm]{images/PUBMED_max_sv_trend.png} &   \includegraphics[width=70mm, height=40mm]{images/PUBMED_edge_count_trend.png} \\
(a) Trend of  on PUBMED & (b)  Trend of  edge count on PUBMED\2pt]
\end{tabular}
\caption{Plots of  and edge count vs fraction of perturbation  for  variants and  on PUBMED. For the  variants, plots (a), (b) illustrate the trends when  and plots (c), (d) illustrate the trends when .   }
\label{fig:rlap_ablation_pubmed}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[H]
\vskip 0.2in
\begin{center}
\begin{tabular}{cc}
\centering
  \includegraphics[width=70mm, height=40mm]{images/COAUTHOR-CS_max_sv_trend.png} &   \includegraphics[width=70mm, height=40mm]{images/COAUTHOR-CS_edge_count_trend.png} \\
(a) Trend of  on COAUTHOR-CS & (b)  Trend of  edge count on COAUTHOR-CS\2pt]
\end{tabular}
\caption{Plots of  and edge count vs fraction of perturbation  for  variants and  on COAUTHOR-CS. For the  variants, plots (a), (b) illustrate the trends when  and plots (c), (d) illustrate the trends when .   }
\label{fig:rlap_ablation_coauthor_cs}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[H]
\vskip 0.2in
\begin{center}
\begin{tabular}{cc}
\centering
  \includegraphics[width=70mm, height=40mm]{images/COAUTHOR-PHY_max_sv_trend.png} &   \includegraphics[width=70mm, height=40mm]{images/COAUTHOR-PHY_edge_count_trend.png} \\
(a) Trend of  on COAUTHOR-PHY & (b)  Trend of  edge count on COAUTHOR-PHY \2pt]
\end{tabular}
\caption{Plots of  and edge count vs fraction of perturbation  for  variants and  on COAUTHOR-PHY. For the  variants, plots (a), (b) illustrate the trends when  and plots (c), (d) illustrate the trends when .   }
\label{fig:rlap_ablation_coauthor_phy}
\end{center}
\vskip -0.2in
\end{figure}



\section{Experimental setup}
\label{appendix:experimental_setup}


\subsection{Platform specifications}
We conduct experiments on a virtual machine with 8 Intel(R) Xeon(R) Platinum 8268 CPUs, 24GB of RAM and 1 Quadro RTX 8000 GPU with 32GB of allocated memory. For reproducible experiments, we leverage the open-source PyGCL framework by \citet{zhu2021empirical} along with PyTorch 1.12.1 \citep{paszke2019pytorch} and PyTorch-Geometric (PyG) 2.1.0 \citep{Fey/Lenssen/2019}. To ensure low latencies, the  algorithm is written in C++, uses the Eigen library for tensor operations, is built using Bazel and has Python bindings for wider adaptability.

\subsection{Datasets}
We experiment on 10 widely used node and graph classification datasets. For node classification, we experiment on CORA \citep{mccallum2000automating}, PUBMED \citep{sen2008collective}, AMAZON-PHOTO, COAUTHOR-CS and COAUTHOR-PHY \citep{shchur2018pitfalls}.
For graph classification, we use the MUTAG \citep{debnath1991structure}, PROTEINS-full (PROTEINS) \citep{borgwardt2005protein}, IMDB-BINARY, IMDB-MULTI \citep{yanardag2015deep} and NCI1 \citep{wale2008comparison} datasets. All the datasets are available in PyG and can be imported directly. Table \ref{table:datasets} provides a summary of their statistics.

\begin{table*}[ht!]
\centering
\caption{Statistics of benchmark datasets from PyG}
\label{table:datasets}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccr}
\toprule
Dataset & Domain & Task &  \#Graphs & Avg \#nodes & Avg \#edges & \#feat & \#classes \\
\midrule
CORA & CS & Node &  &  &  &  & \\
AMAZON-PHOTO & E-Commerce & Node &  &  &  &  &  \\
PUBMED &  Medical & Node &  &  &  &  &  \\
COAUTHOR-CS & CS & Node &  &  &  &  &  \\
COAUTHOR-PHY & Phy & Node &  &  &  &  &  \\
\midrule
MUTAG & Bio & Graph &  &  &  &  &  \\
PROTEINS & Bio & Graph &  &  &  &  &  \\
IMDB-BINARY & Movie & Graph &  &  &  &  &  \\
IMDB-MULTI & Social & Graph &  &  &  &  &  \\
NCI1 & Bio & Graph &  &  &  &  &  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Augmentors}

\textbf{EdgeAddition:} To randomly add edges to the graph , a mask  is sampled using the Bernoulli distribution as follows: , where  is the probability of adding an edge. Since we use two views for contrasting, masks  are sampled using edge addition probabilities  respectively and applied to the adjacency matrix  of  to obtain: . Here  indicates a bit-wise OR operator.  As per the notation defined in preliminaries, the resulting views  are the graphs corresponding to .

\textbf{EdgeDropping:} This technique uses the same mask sampling process as EdgeAddition for  but employs a bit-wise AND operator to mask and drop the edges. 

\textbf{EdgeDroppingDegree:} Unlike the above-mentioned EdgeDropping scheme, this adaptive technique takes into account the degree of nodes at either end of an edge and performs a weighted sampling. The probability of dropping an edge between nodes  is given by:



Where  and  correspond to the maximum and average of log degree centrality scores for edges in the graph. Furthermore,  is the desired (user-provided) probability of dropping an edge and  is the cut-off probability to truncate high probabilities of removal \citep{zhu2021graph}.

\textbf{EdgeDroppingEVC:} The leading eigen vector centrality of a node  is equal to  where  and  is the eigen vector corresponding to the largest eigen value . By replacing  in Eq. \ref{eq:ada_aug_edd} with , we get the eigenvector centrality based edge dropping scheme.

\textbf{EdgeDroppingPR:} The page-rank centrality vector represents the weights computed by the page rank algorithm \citep{page1999pagerank} and is given by  \citep{zhu2021graph}. By replacing  in Eq. \ref{eq:ada_aug_edd} with , we get the page-rank centrality based edge dropping scheme.

\textbf{NodeDropping:} Similar to the edge dropping scheme, this technique drops a node  from the graph based on the  distribution, where  is the probability of dropping a node. Computationally, this is equivalent to setting the row and column values corresponding to the dropped node to .

\textbf{RandomWalkSubgraph:} This is a sub-graph sampling technique where a random walk with restart is performed on  and the sub-graph induced by this random walk is taken as the augmented view \citep{zhu2021empirical}.

\textbf{PPRDiffusion, MarkovDiffusion:} The diffusion operator on a graph (introduced in Eq. \ref{eq:diffusion}) is an effective way of capturing global structural information. The PPRDiffusion and MarkovDiffusion schemes are based on this formulation as follows:

Where  represents the maximum step count for the markov diffusion \citep{zhu2020simple},  in  indicates the dampening factor and . We use  and  in our experiments. Next, to address the increase in edge connectivity of the diffused graph, we perform sparsification based on thresholding with  as proposed by \citet{klicpera2019diffusion}. Note that for diffusion based augmentors, the second view is the original graph without any augmentation and the notion of perturbation  doesn't apply.

\textbf{rLap}: Based on our setup and Algorithm \ref{alg:rlap}, the augmented view is generated by , where  is the input weighted graph,  indicates the fraction of nodes to eliminate,  is the node elimination scheme and  is the neighbor ordering scheme. The algorithm leverages the weighted laplacian  to perform GE-based elimination for  iterations and returns the randomized Schur complement matrix (which is also a laplacian) and represents the augmented view of .

\textbf{Feature Masking}: Based on our setup, a graph  is associated with a feature matrix . For every row , a mask  is sampled from a Bernoulli distribution as follows: , where  is the probability of masking an entry. These masks are then applied on the rows of  to obtain the masked features . Here  indicates a bit-wise AND operator.

\subsection{Contrastive objectives}

We employ Information Noise Contrastive Estimation (InfoNCE), Jensen-Shannon Divergence (JSD), and Bootstrap Latent (BL) objectives in our experiments.

\textbf{InfoNCE} \citep{oord2018representation}: Without loss of generality, consider a sample  for which  and  indicate the set of similar and dissimilar samples respectively. Samples can indicate nodes or graphs as per the context. The InfoNCE loss is now given by:

Where  is the scaling/temperature parameter and  measures the similarity of features  learned by the GNN encoder (followed by projector as per design) as follows:

\textbf{JSD} \citep{lin1991divergence}: With the same setup as InfoNCE, we use JSD loss based on softplus and sigmoid function  as:



\textbf{BL} \citep{thakoor2021bootstrapped}: The BL loss is independent of the dissimilar or negative samples  and attempts to maximize the cosine similarity between embeddings of similar samples. We follow the design presented in \citet{thakoor2021bootstrapped} and employ an online and target encoder to generate embeddings and minimize the following:


We follow the standard benchmarking procedure by \citet{zhu2021empirical} and incorporate additional constraints presented in  \citet{thakoor2021bootstrapped}, such as the exponential moving average-based update of target encoder parameters.

\subsection{GCL framework designs}

\textbf{GRACE:} Figure \ref{fig:gcl_grace} illustrates the design for a GRACE \citep{zhu2020deep} inspired framework. The input graph  is augmented by  to generate views . These graphs are encoded using a shared GNN  and a shared MLP projector  to generate node features. The node-level features across these two views are contrasted using the InfoNCE objective under `l-l' mode. We employ this design for node classification where congruent counterparts of nodes form the positive/similar samples and the rest of the nodes are selected as negative/dissimilar samples. We deviate from the design of \citet{zhu2020deep} by incorporating only inter-view dissimilar samples to reduce the computational overheads.

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{images/gcl_grace.png}}
\caption{GRACE design with shared encoder , shared node feature projector , l-l contrastive mode and InfoNCE objective.} 
\label{fig:gcl_grace}
\end{center}
\vskip -0.2in
\end{figure*}

\textbf{MVGRL:} Figure \ref{fig:gcl_mvgrl} illustrates the design for a MVGRL \citep{hassani2020contrastive} inspired framework. The input graph  is augmented by  to generate views . These graphs are encoded using dedicated GNNs  and projected using a shared MLP  to compute node features. The node embeddings computed by  are aggregated using a permutation invariant function  (eg: mean) and projected using a shared MLP  to obtain graph features. Finally, these graph and node-level features and contrasted using the JSD objective under `g-l' mode. In this mode, when only a single graph  is available in the dataset, the positive samples for graph features of  include the node features of  with negative samples being their noisy versions. The same procedure is applied to the graph features of  and node features of  \citep{velickovic2019deep, hassani2020contrastive, zhu2021empirical}.

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{images/gcl_mvgrl.png}}
\caption{MVGRL design with dedicated encoders , shared node feature projector , shared graph feature projector , g-l contrastive mode and JSD objective.} 
\label{fig:gcl_mvgrl}
\end{center}
\vskip -0.2in
\end{figure*}

\textbf{GraphCL:} Figure \ref{fig:gcl_graphcl} illustrates the design for a GraphCL \citep{you2020graph} inspired framework. The input graph  is augmented by  to generate views . These graphs are encoded using a shared GNN  to obtain the node embeddings. These node embeddings are aggregated using a permutation invariant function  (eg: mean) and projected using a shared MLP  to obtain graph features. Finally, these graph features are contrasted using the InfoNCE objective under `g-g' mode. For the graph features of , the graph features of  are considered as positive samples and the graph features of other graphs in the training batch are treated as negative samples.
 
\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{images/gcl_graphcl.png}}
\caption{GraphCL design with shared encoder , shared graph feature projector , g-g contrastive mode and InfoNCE objective.} 
\label{fig:gcl_graphcl}
\end{center}
\vskip -0.2in
\end{figure*}

\textbf{BGRL:} Figure \ref{fig:gcl_bgrl_g2l} illustrates the design for a BGRL \citep{thakoor2021bootstrapped} inspired framework. The input graph  is augmented by  to generate views . Unlike the previously mentioned frameworks, these graphs are encoded using an online and target GNNs  to compute node embeddings. The online encoder  computes node-level embeddings for both the views and projects them using an MLP . The target encoder  and MLP projector  repeat the same procedure, followed by computing the graph-level features\footnote{Unlike MVGRL where MLP-based projections  are employed, BGRL skips the graph feature projection step.} using a permutation invariant function  (eg: mean). Finally, these graph and node-level features and contrasted using the BL objective under `g-l' mode. The parameters of  are updated based on an exponential moving average of  parameters. This design is independent of negative samples and avoids the computational overheads of employing graph and node features across the training batch.

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{images/gcl_bgrl_g2l.png}}
\caption{BGRL design with online, target encoders , their node projectors , g-l contrastive mode and BL objective.} 
\label{fig:gcl_bgrl_g2l}
\end{center}
\vskip -0.2in
\end{figure*}


\subsection{Evaluation protocols}

Generally, GCL frameworks differ in the choice of augmentors, encoder designs, contrastive modes and objective functions. Thus, it is unclear how the performance of an augmentor can be fairly tested. To address this issue, we fix as many configurations as possible and focus solely on the impact of the augmentor. Our reasoning is based on the well-established benchmark study by \citet{zhu2021empirical}. Adhering to this approach, we leverage the designs of widely adopted frameworks such as GRACE \citep{zhu2020deep}, MVGRL \citep{hassani2020contrastive}, GraphCL \citep{you2020graph} and BGRL \citep{thakoor2021bootstrapped}, where topological augmentation is necessary and evaluate on unsupervised node and graph classification tasks. These frameworks were chosen so as to extensively experiment on encoder designs, contrastive modes and objectives. We leverage graph convolution network (GCN) \citep{kipf2016semi} and graph isomorphism network (GIN) \cite{xu2018powerful} based encoders for node and graph-based tasks respectively. Also, based on the empirical benchmark observations in \citet{zhu2021empirical}, we perform feature masking with a constant masking ratio of  in all the experiments and perform an extensive grid-search over the hyper-parameters to find their ideal values. The perturbation ratios  are selected from  to prevent excessive graph corruption. The number of layers in the encoder are chosen from , hidden feature dimensions are chosen from , learning rate is chosen from , weight decay is set to  and the maximum epoch count is set to . An early stopping strategy with a patience interval of 50 epochs with respect to contrastive loss is also employed. Finally, we choose the Adam optimizer for learning the embeddings. For testing, we follow a linear evaluation protocol \citep{velickovic2019deep, zhu2021empirical} where the embeddings learned by the encoders in unsupervised settings are classified using a logistic regression classifier with a train, validation and test split of  respectively. The process is repeated with  random splits and the classification accuracies are reported. The selected hyper-parameters based on a grid search are reported in Table \ref{table:hp_grace}, \ref{table:hp_mvgrl}, \ref{table:hp_graphcl}, \ref{table:hp_bgrl} and can be reproduced with the provided code and instructions.


\begin{table}[H]
\centering
\caption{Hyper-parameters for GRACE-based design for node-classification}
\label{table:hp_grace}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccccr}
\toprule
Dataset & mode & loss &  &  &  & L & lr & wd & hidden dim \\
\midrule
CORA & l-l & InfoNCE &  &  &  &  &  &  & \\
AMAZON-PHOTO & l-l & InfoNCE &  &  &  &  &  &  & \\ 
PUBMED & l-l & InfoNCE &  &  &  &  &  &  & \\
COAUTHOR-CS & l-l & InfoNCE &  &  &  &  &  &  & \\
COAUTHOR-PHY & l-l & InfoNCE &  &  &  &  &  &  & \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[H]
\centering
\caption{Hyper-parameters for MVGRL-based design for node-classification}
\label{table:hp_mvgrl}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccccr}
\toprule
Dataset & mode & loss &  &  & L & lr & wd & hidden dim \\
\midrule
CORA & g-l & JSD &  &  &   &  &  & \\
AMAZON-PHOTO & g-l & JSD &  &  &  &  &  & \\
PUBMED & g-l & JSD &  &  &  &  &  & \\
COAUTHOR-CS & g-l & JSD &  &  &  &  &  & \\
COAUTHOR-PHY & g-l & JSD &  &  &  &  &  & \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[H]
\centering
\caption{Hyper-parameters for GraphCL-based design for graph-classification}
\label{table:hp_graphcl}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccccr}
\toprule
Dataset & mode & loss &  & &  & L & lr & wd & hidden dim \\
\midrule
PROTEINS & g-g & InfoNCE &  &  &  &  &  &  & \\
IMDB-BINARY & g-g & InfoNCE &  &  &  &  &  &  & \\
MUTAG & g-g & InfoNCE &  &  &  &  &  &  & \\
IMDB-MULTI & g-g & InfoNCE &  &  &  &  &  &  & \\
NCI1 & g-g & InfoNCE &  &  &  &  &  &  & \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{table}[H]
\centering
\caption{Hyper-parameters for BGRL-based design for graph-classification}
\label{table:hp_bgrl}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccccr}
\toprule
Dataset & mode & loss &  &  & L & lr & wd & hidden dim \\
\midrule
PROTEINS & g-l & BL &  &  &  &  &  & \\
IMDB-BINARY & g-l & BL &  &  &  &  &  & \\
MUTAG & g-l & BL &  &  &  &  &  & \\
IMDB-MULTI & g-l & BL &  &  &  &  &  & \\
NCI1 & g-l & BL &  &  &  &  &  & \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Baselines}

We report the baseline node and graph classification performance from previously published reports \citep{hassani2020contrastive, you2020graph, zhu2021graph, xu2021infogcl}. For node classification, we leverage raw features \citep{velickovic2019deep}, DeepWalk \citep{perozzi2014deepwalk}, Graph Auto Encoder (GAE) \citep{kipf2016variational}, Deep Graph Infomax (DGI) \citep{velickovic2019deep}, GCN networks and Graph Attention Networks (GAT) \citep{velivckovic2017graph} (see Table \ref{table:node_baselines}). For graph classification, we leverage Graphlet Kernel (GK) \citep{shervashidze2009efficient}, Weisfeiler-Lehman Kernel (WL) \citep{shervashidze2011weisfeiler}, Deep Graph Kernel (DGK) \citep{yanardag2015deep}, Multi-scale Laplacian Graph Kernel (MLG) \citep{kondor2016multiscale}, node2vec \citep{grover2016node2vec}, sub2vec \citep{adhikari2018sub2vec}, graph2vec \citep{narayanan2017graph2vec}, InfoGraph \citep{sun2019infograph}, GIN, GCN and GAT networks (see Table \ref{table:graph_baselines}).


\begin{table*}[ht!]
\centering
\caption{Baseline node classification accuracies from published reports. }
\label{table:node_baselines}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Method & CORA & Amazon-Photo & PubMed & Coauthor-CS & Coauthor-Phy \\
\midrule
Raw Feat &  &  &  &  &   \\
DeepWalk &  &  &  &  &  \\
DeepWalk+feat &  &  &  &  &  \\
GAE &  &  &  &  &  \\
DGI &  &  &  &  &   \\
\midrule
GCN &  &  &  &  &   \\
GAT &  &  &  &  &  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[ht!]
\centering
\caption{Baseline graph classification accuracies from published reports. }
\label{table:graph_baselines}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Method & MUTAG & PROTEINS & IMDB-BINARY & IMDB-MULTI & NCI1 \\
\midrule
GK &  &  &  &  &  \\
WL &  &  &  &  &  \\
DGK  &  &  &  &  & \\
MLG &  &  &  &  &  \\
\midrule
node2vec  &  &  &  &  &  \\
sub2vec &  &  &  &  &  \\
graph2vec  &  &  &  &  &  \\
InfoGraph &  &  &  &  &  \\
\midrule
GIN &  &  &  &  &  \\
GCN &  &  &  &  &  \\
GAT &  &  &  &  &  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\section{Augmentor Overheads}
\label{appendix:aug_overhead}

We establish an augmentor overhead benchmark in terms of memory usage and latency for an extensive range of techniques on the benchmark datasets. Specifically, we measure:

\textbf{MEMORY:} The memory (in MB) that is required solely by the augmentation phase.

\textbf{LATENCY(CPU):} The wall-clock time (in seconds) for performing the augmentation.

\textbf{LATENCY(w/ GPU):} The wall-clock time (in seconds) for performing the augmentation when GPU is available.

We run  experiments for each augmentor-dataset combination with a fixed perturbation ratio of  (and use a batch size of  for graph datasets) to report the mean and standard deviation of the metrics in Table \ref{table:node_aug_stats}, \ref{table:graph_aug_stats}. From Table \ref{table:node_aug_stats}, we can observe that simple NodeDropping and EdgeDropping schemes are computationally the most efficient, with EdgeDroppingEVC and diffusion-based approaches being the least. The code for EdgeDroppingEVC has been adopted from the original paper by \citet{zhu2021graph} and leverages the NetworkX library to compute centrality scores. Furthermore, since this approach lacks GPU support, the hardware acceleration is minimal. In fact, the overheads of data transfers from GPU to CPU and the construction of NetworkX graphs significantly increase the memory consumption and latencies for this technique. Similar patterns can be observed for other augmentors as well where the GPU-CPU overheads dominate the parallelization benefits. On the contrary, diffusion-based approaches leverage GPU acceleration for heavy matrix operations and gain significant speedups to achieve reasonable latencies. A similar pattern can be observed in Table \ref{table:graph_aug_stats} for graph datasets with a batch size of . Note that some of the results show  variance due to numerical rounding.

The overheads of  in Table \ref{table:node_aug_stats}, \ref{table:graph_aug_stats} lie within a very narrow margin of the most efficient techniques such as EdgeDropping, NodeDropping and RandomWalkSubgraph. These low latencies are achieved by designing the augmentor in C++ and minimizing the interaction with Python APIs. Additionally, we compile Eigen without MKL optimizations and also do not support a GPU kernel for  yet, which explains the lack of performance improvement when hardware acceleration is available. However, we tend to achieve minimal resource consumption by just leveraging the C++ APIs to the fullest. We leave the development of GPU support for  as future work.


\begin{table}[H]
\centering
\caption{Statistics of resource consumption by augmentors on node classification datasets.}
\label{table:node_aug_stats}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llll|l}
\toprule
         augmentor &      dataset &                memory &           latency(cpu) &          latency(w/ gpu) \\
\midrule

      EdgeAddition &         CORA &        &       &   \\
      EdgeDropping &         CORA &      &       &   \\
EdgeDroppingDegree &         CORA &      &      &   \\
   EdgeDroppingEVC &         CORA &       &    &   \\
    EdgeDroppingPR &         CORA &      &    &      \\
   MarkovDiffusion &         CORA &   &    &   \\
      NodeDropping &         CORA &      &       &   \\
      PPRDiffusion &         CORA &     &    &   \\
RandomWalkSubgraph &         CORA &       &        &   \\
         
              rLap &         CORA &        &    &   \\
              \midrule
              EdgeAddition & AMAZON-PHOTO &     &    &   \\
      EdgeDropping & AMAZON-PHOTO &      &    &   \\
EdgeDroppingDegree & AMAZON-PHOTO &     &    &   \\
   EdgeDroppingEVC & AMAZON-PHOTO &       &        &    \\
    EdgeDroppingPR & AMAZON-PHOTO &     &    &      \\
   MarkovDiffusion & AMAZON-PHOTO &    &    &   \\
      NodeDropping & AMAZON-PHOTO &        &    &   \\
      PPRDiffusion & AMAZON-PHOTO &    &    &   \\
RandomWalkSubgraph & AMAZON-PHOTO &      &    &   \\
              rLap & AMAZON-PHOTO &      &    &   \\
              \midrule

      EdgeAddition &       PUBMED &     &    &   \\
      EdgeDropping &       PUBMED &      &     &   \\
EdgeDroppingDegree &       PUBMED &     &      &   \\
   EdgeDroppingEVC &       PUBMED &   &    &   \\
    EdgeDroppingPR &       PUBMED &      &    &   \\
   MarkovDiffusion &       PUBMED &    &   &    \\
      NodeDropping &       PUBMED &       &    &      \\
      PPRDiffusion &       PUBMED &    &   &   \\
RandomWalkSubgraph &       PUBMED &      &    &    \\
              rLap &       PUBMED &     &    &   \\
\midrule
      EdgeAddition &  COAUTHOR-CS &     &    &   \\
      EdgeDropping &  COAUTHOR-CS &      &     &   \\
EdgeDroppingDegree &  COAUTHOR-CS &      &    &   \\
   EdgeDroppingEVC &  COAUTHOR-CS &    &   &   \\
    EdgeDroppingPR &  COAUTHOR-CS &       &    &    \\
   MarkovDiffusion &  COAUTHOR-CS &  &   &   \\
      NodeDropping &  COAUTHOR-CS &      &    &   \\
      PPRDiffusion &  COAUTHOR-CS &    &   &   \\
RandomWalkSubgraph &  COAUTHOR-CS &      &    &    \\
  
              rLap &  COAUTHOR-CS &      &    &  \\
              \midrule
      EdgeAddition & COAUTHOR-PHY &     &    &   \\
      EdgeDropping & COAUTHOR-PHY &     &    &       \\
EdgeDroppingDegree & COAUTHOR-PHY &      &    &      \\
   EdgeDroppingEVC & COAUTHOR-PHY &   &   &  \\
    EdgeDroppingPR & COAUTHOR-PHY &     &    &    \\
   MarkovDiffusion & COAUTHOR-PHY &        &  &  \\
      NodeDropping & COAUTHOR-PHY &      &    &    \\
      PPRDiffusion & COAUTHOR-PHY &      &   &   \\
RandomWalkSubgraph & COAUTHOR-PHY &      &     &   \\
            
              rLap & COAUTHOR-PHY &     &     &   \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{table}[H]
\centering
\caption{Statistics of resource consumption by augmentors on graph classification datasets.}
\label{table:graph_aug_stats}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llll|l}
\toprule
         augmentor &      dataset &                memory &           latency(cpu) &          latency(w/ gpu) \\
\midrule
      EdgeAddition &  IMDB-BINARY &      &    &   \\
      EdgeDropping &  IMDB-BINARY &        &    &   \\
EdgeDroppingDegree &  IMDB-BINARY &      &    &    \\
   EdgeDroppingEVC &  IMDB-BINARY &   &    &   \\
    EdgeDroppingPR &  IMDB-BINARY &      &    &   \\
   MarkovDiffusion &  IMDB-BINARY &   &    &    \\
      NodeDropping &  IMDB-BINARY &       &    &   \\
      PPRDiffusion &  IMDB-BINARY &    &     &   \\
RandomWalkSubgraph &  IMDB-BINARY &      &    &   \\            
              rLap &  IMDB-BINARY &      &    &   \\
              \midrule
      EdgeAddition &   IMDB-MULTI &      &    &   \\
      EdgeDropping &   IMDB-MULTI &      &       &   \\
EdgeDroppingDegree &   IMDB-MULTI &      &    &   \\
   EdgeDroppingEVC &   IMDB-MULTI &   &     &   \\
    EdgeDroppingPR &   IMDB-MULTI &      &    &   \\
   MarkovDiffusion &   IMDB-MULTI &    &    &    \\
      NodeDropping &   IMDB-MULTI &      &    &   \\
      PPRDiffusion &   IMDB-MULTI &    &    &   \\
RandomWalkSubgraph &   IMDB-MULTI &      &    &   \\
              rLap &   IMDB-MULTI &      &    &   \\
              \midrule
      EdgeAddition &        MUTAG &        &    &   \\
      EdgeDropping &        MUTAG &      &       &   \\
EdgeDroppingDegree &        MUTAG &      &    &   \\
   EdgeDroppingEVC &        MUTAG &   &    &   \\
    EdgeDroppingPR &        MUTAG &      &    &    \\
   MarkovDiffusion &        MUTAG &    &    &   \\
      NodeDropping &        MUTAG &      &       &   \\
      PPRDiffusion &        MUTAG &    &    &    \\
RandomWalkSubgraph &        MUTAG &       &       &   \\          
              rLap &        MUTAG &      &       &   \\
              \midrule
      EdgeAddition &     PROTEINS &      &    &   \\
      EdgeDropping &     PROTEINS &        &    &    \\
EdgeDroppingDegree &     PROTEINS &     &    &   \\
   EdgeDroppingEVC &     PROTEINS &   &    &   \\
    EdgeDroppingPR &     PROTEINS &       &    &   \\
   MarkovDiffusion &     PROTEINS &   &   &   \\
      NodeDropping &     PROTEINS &       &    &   \\
      PPRDiffusion &     PROTEINS &   &    &    \\
RandomWalkSubgraph &     PROTEINS &       &    &   \\
              rLap &     PROTEINS &      &    &   \\
              \midrule
      EdgeAddition &         NCI1 &      &    &    \\
      EdgeDropping &         NCI1 &      &    &   \\
EdgeDroppingDegree &         NCI1 &      &    &   \\
   EdgeDroppingEVC &         NCI1 &   &    &   \\
    EdgeDroppingPR &         NCI1 &      &    &   \\
   MarkovDiffusion &         NCI1 &      &   &     \\
      NodeDropping &         NCI1 &      &     &   \\
      PPRDiffusion &         NCI1 &    &    &    \\
RandomWalkSubgraph &         NCI1 &      &    &   \\
              rLap &         NCI1 &      &     &   \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\pagebreak

\section{Additional Experiments}
\label{app:add_exp}

\subsection{PPR Diffusion and rLap}

In our experiments, we observed that PPRDiffusion on medium scale graphs and MVGRL design leads to OOM and requires sub-graph sampling to proceed with the GCL phases. To this end, we observed a decrease in unsupervised node classification performance when the size of the sub-graph decreased (see Table \ref{table:results_mvgrl_subgraph_size}). As the nodes for a sub-graph are randomly selected, one can't guarantee that the sub-graph will capture useful structural information for the GNN encoders to exploit. In a worst case scenario, if all the nodes of a sub-graph are disconnected, the GNN encoder essentially acts as an MLP. Thus, as the size of the sub-graph reduces, we can expect a heavy corruption of topological information in the augmented views.

\begin{table*}[ht!]
\centering
\caption{Evaluation (in accuracy) on benchmark node datasets with \textbf{MVGRL} based design and PPR Diffusion with varying sub-graph sizes. PPRDiffusion indicates the full graph view and PPRDiffusion- indicates a sampled sub-graph with  nodes as the graph view. }
\label{table:results_mvgrl_subgraph_size}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Augmentor & CORA & Amazon-Photo & PubMed & Coauthor-CS & Coauthor-Phy \\
\midrule
PPRDiffusion &   &   & OOM & OOM & OOM \\
PPRDiffusion- &   &   &  &  &  \\
PPRDiffusion- &  &  &  &  &  \\
PPRDiffusion- &  &  &  &  &  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsubsection{Reducing computational overheads}

For graphs such as COAUTHOR-PHY, the computational overheads of PPRDiffusion are relatively significant when compared to other augmentors (see Table \ref{table:node_aug_stats}). The standard practice to avoid such bottlenecks is to cache the output of the diffusion operator  and reuse it for sampling sub-graphs. To further reduce the overheads of computing  prior to caching, we leverage Theorem \ref{thm:theta_schur}.

\textbf{Sketch:} The COAUTHOR-PHY graph  represents  nodes and  edges. To randomly sample a sub-graph of size  from its diffused variant , one can compute a randomized Schur complement  using  with , apply the PPR diffusion operator on  to obtain  and cache it.  represents  nodes from which a sub-graph of size  can be sampled with relatively less overheads.

We benchmark the computational overheads of:

\begin{itemize}
    \item PPRDiffusion on COAUTHOR-PHY graph followed by random sampling of a sub-graph with  nodes.
    \item  on COAUTHOR-PHY graph with  followed by PPRDiffusion and random sampling of a sub-graph with  nodes.
\end{itemize}

The results in Figure \ref{fig:ppr_rlapppr_overheads} demonstrate the effectiveness of the latter approach in achieving  reduction in memory consumption,  reduction in CPU only latencies and  reduction in latencies when GPU is available. We also compared the unsupervised node classification accuracy and observed a slight performance improvement of . 

\textbf{Limitations and Tradeoffs:} From Algorithm \ref{alg:rlap}, notice that after every outer loop iteration, the number of edges in the graph representing the current Schur complement state will be less than or equal to the number of edges at the beginning of the iteration. To quantify the impact of diffusion on these randomized Schur complements, we measured the edge counts of sub-graphs of size  randomly sampled from the following graphs \footnote{We consider the  variant of , which is the default setting. }:

\begin{itemize}
    \item \textbf{COAUTHOR-CS:PPR}: PPRDiffusion applied on COAUTHOR-CS.
    \item \textbf{COAUTHOR-CS:rLapPPR}:  on COAUTHOR-CS with  followed by PPRDiffusion.
    \item \textbf{COAUTHOR-PHY:PPR}: PPRDiffusion applied on COAUTHOR-PHY.
    \item \textbf{COAUTHOR-PHY:rLapPPR}:  on COAUTHOR-PHY with  followed by PPRDiffusion.
\end{itemize}

The results in Figure \ref{fig:rlap_rlapppr_subgraph_edge_0_5} show that the randomly sampled sub-graph from \textbf{COAUTHOR-CS:rLapPPR} has  the number of edges in \textbf{COAUTHOR-CS:PPR}. Similarly, the sub-graph from \textbf{COAUTHOR-PHY:rLapPPR} has  the number of edges in \textbf{COAUTHOR-PHY:PPR}. Thus, although one can save memory and time to address the bottlenecks of PPRDiffusion, the GNN encoders would eventually consume additional memory due to the message passing operations on relatively more edges. 

In such situations, if training latencies take precedence over memory constraints, then applying  and proceeding with PPRDiffusion will lead to better training throughput. Else, based on the edge count pattern in Figure \ref{fig:rlap_rlapppr_subgraph_edge_gamma_variants}, the practitioner can use a lower  value and consider the trade-offs between augmentation latencies and available memory as per their setup.


\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\begin{tabular}{cc}
\centering
  \includegraphics[width=70mm]{images/ppr_rlapppr_memory.png} &   \includegraphics[width=70mm]{images/ppr_rlapppr_cpu.png} \\
(a) Memory (MB) overhead & (b) Latency (sec) overhead  \2pt]
\end{tabular}
\caption{Computational overheads and unsupervised node classification accuracy of PPRDiffusion and  + PPRDiffusion with  on COAUTHOR-PHY dataset.}
\label{fig:ppr_rlapppr_overheads}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.5\textwidth]{images/rlap_rlapppr_subgraph_edges.png}}
\caption{Plots of edge counts of randomly sampled sub-graphs of size  with  for  based techniques.} 
\label{fig:rlap_rlapppr_subgraph_edge_0_5}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\begin{tabular}{cc}
\centering
  \includegraphics[width=70mm]{images/ppr_rlapppr_edge_counts_frac_0.1.png} &   \includegraphics[width=70mm]{images/ppr_rlapppr_edge_counts_frac_0.2.png} \\
(a)  & (b)   \2pt]
\end{tabular}
\caption{Plots of edge counts of randomly sampled sub-graphs of size  with varying  for  based techniques.}
\label{fig:rlap_rlapppr_subgraph_edge_gamma_variants}
\end{center}
\vskip -0.2in
\end{figure}




\end{document}
