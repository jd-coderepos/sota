\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{bbm}
\newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{\newenvironment{#1}[1]
  {\renewcommand\customgenericname{#2}\renewcommand\theinnercustomgeneric{##1}\innercustomgeneric
  }
  {\endinnercustomgeneric}
}

\newcustomtheorem{customthm}{Theorem}
\newcustomtheorem{customdef}{Definition}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{multirow}
\usepackage{amsfonts}




\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 \cvprfinalcopy 

\def\cvprPaperID{4853} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Learning Smooth Representation for Unsupervised Domain Adaptation}

\author{Guanyu Cai, Yuqin Wang, and Lianghua He\\
Department of Computer Science and Technology, Tongji University\\
Shanghai, China 201804\\
{\tt\small \{caiguanyu,wangyuqin,helianghua\}@tongji.edu.cn}
}

\maketitle


\begin{abstract}
In unsupervised domain adaptation, existing methods have achieved remarkable performance, but few pay attention to the Lipschitz constraint. It has been studied that not just reducing the divergence between distributions, but the satisfaction of Lipschitz continuity guarantees an error bound for the target distribution. In this paper, we adopt this principle and extend it to a deep end-to-end model. We define a formula named \textbf {local smooth discrepancy} to measure the Lipschitzness for target distribution in a pointwise way. Further, several critical factors affecting the error bound are taken into account in our proposed optimization strategy to ensure the effectiveness and stability. Empirical evidence shows that the proposed method is comparable or superior to the state-of-the-art methods and our modifications are important for the validity. 
\end{abstract}



\section{Introduction}
\label{introduction}

The performance of various computer vision problems has been significantly improved with the development of deep convolutional neural networks (CNN)~\cite{krizhevsky2012imagenet}. However, a precondition of this improvement is that numerous labeled samples are needed and test samples are drawn from the same distribution with training ones. Once there exists a \emph{dataset shift} between the training and test samples, the performance of a CNN model decreases dramatically~\cite{ben2010theory,donahue2014decaf:}. In order to tackle this problem, unsupervised domain adaptation (UDA) transfers knowledge from a labeled source domain to an unlabeled target domain.

A bunch of classical UDA methods is to match moments of features in the source and target domains. They regard moments of a distribution as the main characteristics~\cite{zellinger2017central,sun2016deep,pmlr-v37-long15}. By matching moments, they hope to match distributions of different domains. Other kinds of remarkable UDA methods are based on adversarial training strategy.~\cite{ganin2016domain} first introduces a domain classifier to make distributions of distinct domains matching.~\cite{liu2016coupled,bousmalis2017unsupervised} also propose methods that are effective based on such a principle. In summary, these methods follow a standard schema where they first estimate the divergence between different distributions and then seek an appropriate optimization strategy to minimize it. This schema is proposed in~\cite{ben2010theory}, and the most remarkable UDA methods are established on it.

However, this typical schema is limited by two issues. Various divergence estimations indeed capture characteristics of distributions, whereas it is impossible to obtain complete information. Many estimations are proposed~\cite{ben2010theory,zellinger2017central,sun2016deep,pmlr-v37-long15}, but none of them is proved to be the most effective one. Furthermore, it is sophisticated to design an optimization strategy to minimize divergence. Direct minimization strategy~\cite{zellinger2017central,sun2016deep,pmlr-v37-long15} shows poor performance while adversarial training is unstable because of the gradient vanishing problem~\cite{8833506,arjovsky2017towards}.

Another schema proposed in~\cite{Ben-David2014} introduces a basic UDA error bounded by probabilistic Lipschitzness. It inspires us to tackle UDA problem by strengthening Lipschitz continuity in the target distribution with numerous low-quality samples. \cite{shu2018a, mao2019virtual} adopt this principle and propose effective methods. However, these methods still involve typical schema and regard Lipschitz constraint as a regularization term. The reserved adversarial training procedure tends to cause performance drop because of its instability. Moreover, there are several essential factors discussed in~\cite{Ben-David2014}, such as the dimension of samples and batchsize, ignored by previous work which easily lead to a collapsed model.

In this paper, we further extend the probabilistic Lipschitz constraint by  proposing a more concise way to achieve Lipschitz continuity in the target distribution through a newly defined formula, with deep end-to-end networks. An optimization strategy that takes factors analyzed in~\cite{Ben-David2014} into consideration is established to enable our model efficient and stable. In particular, our model contains a feature generator and classifier. The classifier tries to classify source samples correctly and detect sensitive target samples that break down the Lipschitz constraint. The feature generator is trained to strengthen the Lipschitzness of these sensitive samples. To measure the Lipschitzness for target distribution in a pointwise way, we define a formula named {\bf local smooth discrepancy}. Two specific computing methods are introduced. Utilizing local smooth discrepancy, we propose a detailed optimization strategy to tackle the UDA problem which considers the effects of the dimension and batchsize of samples on the basic error bound. 

\section{Realted Work}
\label{relatedworkd}

A theoretical work proposed in~\cite{ben2010theory} confirms that a discrepancy between the source and target distributions causes a model invalid in the target domain. Because the distribution of a domain is difficult to illustrate, intuitive thought is to match moments of distributions instead.~\cite{pmlr-v37-long15} matches expectations of features.~\cite{sun2016deep} takes the covariance of features into consideration. Moreover,~\cite{zellinger2017central} utilizes high-order moments to align distributions of different domains. These methods perform well in numerous settings.

As DANN~\cite{ganin2016domain} proposed, methods based on adversarial training become popular gradually. These methods introduce a domain classifier to predict which domain a sample is drawn from. At the same time, a feature generator is trained to fool the domain classifier so that features from different domains are matched. \cite{tzeng2017adversarial} follows this philosophy and implements adversarial training in feature space. Several methods implement adversarial training in pixel level~\cite{murez2018image,bousmalis2017unsupervised,liu2016coupled}. These methods try to generate target images from labeled source images. In this way, a classification model is able to be trained with labeled target images. Specifically, PixelDA~\cite{bousmalis2017unsupervised} follows the training strategy of generative adversarial networks (GANs) and obtains excellent performance on digits datasets. \cite{murez2018image,liu2016coupled} introduce training strategy similar to cycle GAN to improve their performance.

Besides taking marginal distribution into consideration, several methods utilize category-discriminative information~\cite{8451152,10.1007/978-3-030-01424-7_34,DAS201880}. JAN~\cite{long2017deep} modifies DAN~\cite{pmlr-v37-long15} to match joint distributions. CADA~\cite{long2018conditional} subtly changes the training strategy of DANN~\cite{ganin2016domain} and achieves remarkable results. ATDA~\cite{pmlr-v70-saito17a} adds two auxiliary classifiers to assist in generating valid pseudo labels for target samples. It constructs decision boundaries for target domain in two different views. Moreover, MCD~\cite{saito2018maximum} gives us a concise explanation of why matching marginal distributions still causes misclassification. It also proposes a siamese-like network and adversarial training strategy to solve the UDA problem. MCD~\cite{saito2018maximum} is comparable or superior to the existing methods on several benchmark domain adaptation datasets.

\section{Preliminary}
\label{prelinary}
In this section, we give a brief description of UDA and define several properties relevant to practical learning challenges. Moreover, a basic UDA error bound is derived from these properties~\cite{Ben-David2014}\footnote {Note that the theorems assume binary classification (). However, they can be directly extended to multi-class settings}. 

Let  be some domain set where  is a divergence metric over . In UDA setup, we denote  and  the source and target distributions, respectively. The marginal distributions of  and  over  are denoted by  and  and their labeling rules are denoted by  and . The goal is to learn a function  which predicts correct labels for samples in  with respect to  over . For any hypothesis , we define the \textit{error} with respect to  by . Thus, the Bayes optimal error for  is .


Besides basic notations, in the context of realistic UDA problems, there are some properties expressing either some relationship between the source and target distributions or conditions of these distributions that facilitate learning~\cite{Ben-David2014}.
\begin{customdef}{1}
(Covariate shift) Source and target distributions satisfy the \textit{covariate shift} property if they have the same labeling rule, i.e. ~\cite{Sugiyama05generalizationerror}.
\label{def1}
\end{customdef}

The common labeling function is denoted as . While this property seems to make UDA easy, the fact is that it is a rather weak assumption where a UDA learner has no idea how the common labeling rule behaves outside the scope of labeled source samples.
\begin{customdef}{2}
(Probabilistic Lipschitzness) Let .  is - w.r.t. a distribution  over  if, for all :

\label{def2}
\end{customdef}

This definition generalize the standard - property where a function  holds  for all . It may be viewed as a formalization of \textit{cluster assumption} which is widely adopted in semi-supervised learning~\cite{miyato2018virtual}. This assumption introduces an additional regularization that decision boundaries not go through the high-density regions are preferred.
\begin{customdef}{3}
(Weight ratio) Let  be a collection of subsets of  measurable with respect to both  and . For some  we define the -weight ratio of the source and target distributions with respect to  as

Further, the \textit{weight ratio} of the source and target distributions with respect to  is defined as

\label{def3}
\end{customdef}

We denote the weight ratio with respect to the collection of all sets that are  and  measurable by . A basic observation about UDA is that it may be infeasible when  and  are supported on disjoint domain regions. To guard against such scenarios, it is common to assume .

According to definition~\ref{def1},~\ref{def2} and~\ref{def3},~\cite{Ben-David2014} derives an upper error bound for general UDA learning based on the Nearest Neighbor algorithm. Note that  should be a fixed-dimension space and  should be of finite VC-dimension such that the -weight ratio can be estimated from finite samples~\cite{Ben-David2014}. In detail, we assume the domain is the unit cube .  denotes the set of axis aligned rectangles in  and, given some , let  the class of axis aligned rectangles with sidelength .

Given a labeled sample batch , and  denotes samples without labels of . For any sample ,  denotes the label of  in  (using the majority of their labels) and  denotes the nearest neighbor to  in , .  is defined by  for all .
\begin{customthm}{1}\label{the1}
For some domain , some  and , let  be a class of pairs  of source and target distributions over  satisfying the covariate shift assumption, with , and their common labeling function  satisfying the -probabilistic-Lipschitz property w.r.t the target distribution, for some function . Then, for all m, and all ,
 
where m denotes the size of  containing points sampled i.i.d. from .
\end{customthm}



\section{Learning Smooth Representation}
\label{ourmethod}

\begin{figure*}[htp]
\centering
\includegraphics[width = \textwidth]{motivation.pdf}
\caption{A visual illustration of how the proposed method achieves adaptation. ({\bf Upper}) The left block illustrates the dataset shift between different domains. The other three blocks depict the process of forming a robust margin between target samples and the decision boundary. At the same time, how distributions of the target domains changes are described. At last, distributions of the target and source domains are matched. ({\bf Lower}) The optimization schedule is able to project sensitive samples to new locations away from the decision boundary in three steps. Dashed lines indicate fixed network parameters.}
\label{fig:motivation}
\end{figure*} 

In theorem~\ref{the1}, the error of general UDA learning is bounded by three terms. First,  refers to the Bayes optimal error for . It is an ideal value discussed in the theoretical analysis which is impossible to obtain in practical settings. Therefore, we focus on the other two terms in this paper. Second,  refers to the degree of how target samples satisfying the probabilistic Lipschitzness. Because  is positively related to , our goal is to decrease , in other words, strenghthen probabolistic-Lipschitz property with respect to the target distribution. Finally, the last term is relevant to multiple factors, for example, the lower bound  for the weight ratio of source and target distributions, the dimension of domain  and the size of . Note that the weight ratio  is predetermined for a specific pair . Therefore, it is impracticable to optimize its lower bound . To achieve a tighter error bound for the target distribution, it is reasonable to discover appropriate  and . These analyses indeed inspire us to tackle the UDA problem, while no applicable algorithm is proposed in~\cite{Ben-David2014}. In this paper, we adopt the well-established principles and extend them to a feasible and effective algorithm.

\subsection{Deep End-to-End Model}
\label{deep}

Although the proposed principles are reasonable, the very first problem we should deal with is how to design a deep end-to-end model aligned well with these principles. In recent years, deep neural networks with a huge number of parameters optimized by gradient-based algorithms are powerful on large-scale datasets~\cite{ben2010theory}. However, the basic error bound is derived with respect to the Nearest Neighbor algorithm which is non-parametric and non-differentiable. In order to make the proposed model's capacity high enough, we replace the Nearest Neighbor algorithm with a deep neural network.

Let us recall the meaning of  first. For any sample  in the target distribution,  is responsible for labeling it with the label of  which denotes the nearest sample in . The size of  is determined by  and the label of  is determined by the majority of . The intuition behind this algorithm is to seek  samples in the source distribution and then use them to label a sample in the target distribution. In fact, this intuition is well-aligned with the deep learning algorithm. In the deep learning algorithm, we can sample  points from the source distribution  to form a batch, update a neural network with a forward and back propagation, and inference the label of . Therefore,  is revised to , where  is a neural network parameterized by .  is updated with some gradient-based algorithm, .  refers to the batchsize during training. In this setting, optimizing  is an end-to-end process after choosing an appropriate loss function.

Although a deep end-to-end model bring us a higher capacity and convenient optimization procedure, we need to concern two points. First, the non-parametric Nearest Neighbor algorithm can estimate  directly without any optimization, while  estimated by  changes every iteration because  is updated. In the beginning,  is large while it converges to a small value as optimizing . Second, despite the proposed algorithm aligns well with the intuition in~\cite{Ben-David2014}, it can not ensure the rigor of theorem~\ref{the1}. Therefore, more modifications are necessary to bound 
 such that the proposed algorithm can work well in the practical situation.

\subsection{Local Smooth Discrepancy}

Besides extending theorem~\ref{the1} to a deep end-to-end model, how to decrease  is another crucial problem. According to equation~\ref{eq4}, the basic UDA error bound is positively related to . Meanwhile,  is lower bounded by  as illustrated in definition~\ref{def2}. Thus, minimizing  instead of  which is troublesome to estimate is a straightforward idea.

A naive estimation for  is . In this estimation, for any , we should sample a number of  to make sure the estimation precise. In a deep end-to-end model, this procedure requires several forward propagations for one batch and it is confronted with large computational quantity. In~\cite{grandvalet2004semi-supervised}, a replacement named local Lipschitz property for Lipschitz continuity is proposed. Local Lipschitz continuity assumes that every point  has a neighborhood  such that  is Lipschitz continuous with respect to  and points in . According to this assumption, we propose a concrete formula named {\bf local smooth discrepancy} (LSD) to measure the degree that a sample  breaks down the local Lipschitz property:

where  is a discrepancy function that measures the divergence between two outputs of , and  denotes the maximum norm of . In , a sample  adds  to detect another sample in 's neighborhood.  controls the range of sampling in 's neighborhood. As for the choice of , we employ cross-entropy loss function in all experiments. In addtion, in this setting, we specifies  as L1 norm such that for a point , points in its neighborhood denote as , where .

Although LSD is well-defined, there is still an essential point we should pay more attention to. Specifically,  is limited only by its norm in equation \ref{eq5} and its direction is ignored. In fact, the goal of adding  includes detecting sensitive samples that not satisfy local Lipschitz property. If all  belong to the same category with , it means that the direction of  could not detect sensitive samples. In this condition, sensitive samples are not modified to be local Lipshitz continuous. In order to solve this problem, we propose two plans to produce , an isotropic one and an anisotropic one.

{\bf Isotropic Plan} In the isotropic plan, we draw  from a Gaussian distribution and normalize it to satisfy . The formula of LSD is modified into:


{\bf Anisotropic Plan} Anisotropic plan only looks for  which lead  with different labels from  and ignores  in other directions. To reach this goal, we take the insight from adversarial attack~\cite{goodfellow2014explaining}. Adversarial attack applies a certain hardly perceptible perturbation, which is found by maximizing the model's prediction error, to an image to cause the model misclassify~\cite{goodfellow2014explaining}. This philosophy fits well with our goal which tries to seek some noise to make the consequential samples belong to different classes. However, true labels are needed in this setting. In UDA problem, true labels for the target domain is unreachable. Therefore, we make several modifications in traditional adversarial attack methods. In detail, traditional adversarial attack methods approximate adversarial perturbation by:

Instead, we approximate it by:

where  denotes transforming the softmax output of  to a one-hot vector. Equation \ref{eq8} computes gradients of  and replaces  with . These modifications result in a new LSD for anisotropic noise:
  

\subsection{Optimization Strategy}

After defining LSD, what we should optimize to reduce  is clearly described, while how to optimize it and what we should concern to constrain the third term in theorem~\ref{the1} need a detailed discussion.

Basically, we can adopt equation~\ref{eq6} or \ref{eq9} to the loss function and use a gradient-based algorithm for optimization. However, there is an essential factor deserved to be considered. The dimension of domain  affects the basic error bound sharply. For instance, if we solve a UDA learning for some image domains,  potentially varies over a large range of values because the size of images is totally different. Considering that a large  leads to a loose error bound, LSD is embedded in feature space instead of image level such that  is decreased acutely. Equation~\ref{eq6} and \ref{eq9} are revised into


where  and  denote the feature extractor and classifier in , respectively. Their parameters are denoted as  and .

Therefore, we propose an optimization strategy in the feature space in three steps. First, we train  and  in the source domain.
 
where  is an indicator function, and  denotes that there are  classes in a task. Then, we produce sensitive samples which break down the local Lipschitz property. Note that in theorem~\ref{the1}, Lipschitz property constraint is satisfied w.r.t the target distribution such that we focus on target samples in this step. In our work, sensitive samples  are generated in the feature space of :

where , and  is a general notation for the adding noise. In practice, we set  for an isotropic plan or  for an anisotropic plan. Finally, we train  to minimize LSD for target samples. Only parameters of  are updated in this step.  is trained to project  to the same category with :

where  denotes parameters of , and  denotes cross-entropy loss function. Equation \ref{eq12} \ref{eq13} and \ref{eq14} are repeated in the optimization schedule as shown in Figure~\ref{fig:motivation}.

Here we give an intuitive understanding underlying our optimization strategy. As shown in Figure~\ref{fig:motivation}, a  well-trained in the source domain is able to classify source samples correctly. For a robust model, there is a large margin between the decision boundary and samples. Such distribution makes sure that local Lipschitz property is well-satisfied. However, a dataset shift across different domains results in a probability that a set of target samples would cross the boundary decided by a source domain. These samples are misclassified and local Lipschitz constraint is broken down. Therefore, we regard samples near the decision boundary as sensitive ones which easily lead errors to . It is similar to the overfitting problem where extreme nonlinearity of deep neural networks leads to a phenomenon that performance drop happens when facing out-of-distribution samples. In a UDA problem, the dataset shift further exacerbates this situation. Numerous target samples are regarded as out-of-distribution samples for . In order to obtain a  work well in the target domain,  needs to project  into feature space away from the decision boundary.

In our strategy, we try to form a large margin between target samples and the source decision boundary. Samples close to the boundary are detected and  is forced to project them far away from the boundary. Under such an optimization strategy, representation of the target distribution becomes "smooth" gradually which means that the local Lipschitz constraint is ensured. When the algorithm converges, not only the dataset shift between source and target domain is reduced, but also source and target samples achieve a large margin with the decision boundary. The ideal smooth representation of the target domain is learned in our method.

\begin{table*}[htbp]
\centering
\caption{Classification accuracy percentage of digits classification experiment among all four tasks. The first row corresponds to the performance if no adaption is implemented. We evaluate three SRDA models with different plans for adding noise. SRDA* denotes the models which are optimized in the image level. The results are cited from each study.}
\setlength{\tabcolsep}{7mm}{
\begin{tabular}{l|cccc}
\hline\hline
\multirow{3}{*}{Method}& SVHN &SYNSIG & MNIST & USPS\\
& &  &  & \\
&MNIST & GTSRB &USPS &MNIST\\
\hline
Source Only& 67.1 & 85.1 & 76.7 & 63.4\\
\hline
DAN~\cite{pmlr-v37-long15}& 71.1 & 91.1 & - & - \\
DANN~\cite{ganin2016domain}& 71.1 & 88.7 & 77.1 & 73.0 \\
DSN~\cite{NIPS2016_6254}& 82.7 & 93.1 & 91.3 & - \\
ADDA~\cite{tzeng2017adversarial}& 76.0 & - & 89.4 & 90.1\\
CoGAN~\cite{liu2016coupled}& - & - & 91.2 & 89.1\\
ATDA~\cite{pmlr-v70-saito17a}& 86.2 & {\bf 96.2} &- & - \\
ASSC~\cite{Haeusser_2017_ICCV}& 95.7 & 82.8 & - & -\\
DRCN~\cite{DBLP:conf/eccv/GhifaryKZBL16}& 82.0 & - & 91.8 & 73.7\\
MCD~\cite{saito2018maximum}&96.2 & 94.4 & 94.2 & 94.1\\
\hline
SRDA(FGSM)&95.96 & 90.46 & 81.53 & 95.78\\
SRDA*(FGSM)&22.70&collapse&32.73&85.37\\
SRDA(VAT)& 97.92& 89.51& 85.00&95.49 \\
SRDA*(VAT)&89.47&29.04&88.49&92.17\\
SRDA(RAN)& {\bf 98.91} & 93.61 & {\bf 94.76}& 95.03\\
SRDA*(RAN)&89.51&49.86&93.25&{\bf 95.95}\\
\hline\hline
\end{tabular}}
\label{tab:digits}
\end{table*}

\section{Experiments and Discussion}
\label{experiments}

In order to verify the effectiveness of the proposed method (SRDA), we conduct several classification experiments on standard benchmark datasets. First, we test SRDA on several digits classification datasets which are the most common datasets for UDA. Second, we test it on a more complex and massive dataset, VisDA~\cite{peng2017visda:}, to show the advanced performance of SRDA. Then, we conduct experiments to demonstrate how the dimension and batchsize of samples affect the performance of SRDA. Finally, we analyze the limitation of SRDA on Office-31 dataset. In all experiments, we implement models with Pytorch, and employ the optimization schedule we propose.

\subsection{Digits Classification}

We evaluate four types of adaptation scenarios by utilizing the digits datasets, MNIST~\cite{726791}, USPS~\cite{291440}, Synthetic Traffic Signs (SYNSIG)~\cite{10.1007/978-3-319-02895-8_52}, Street View House Numbers (SVHN)~\cite{37648} and German Traffic Signs Recognition Benchmark (GTSRB)~\cite{6033395}. Specifically, MNIST, USPS and SVHN consist of 10 classes, whereas SYNSIG and GTSRB are traffic sign datasets which consist of 43 classes. In this experiment, we set four transfer tasks: SVHNMNIST, SYNSIGGTSRB, MNISTUSPS and USPSMNIST. In detail, the dataset shift in SVHNMNIST is caused by a different property of an image that SVHN contains RGB images while MNIST contains grayscale images. The shift between USPS and MNIST is relatively small because both of them are handwritten digit datasets and contain grayscale images. Images in SYNSIG and GTSRB have distinct properties because those in SYNSIG are synthesized and the rest is collected from the real world. For a fair comparison, we follow the protocols provided in MCD~\cite{saito2018maximum} and ADDA~\cite{tzeng2017adversarial}.

In this experiment, in order to verify the robustness of SRDA, we implement both isotropic and anisotropic plans. For the isotropic plan, we sample noise from a standard Gaussian distribution. In all four tasks, hyper-parameter  is set to 0.5 and the learning rate is set to . For the anisotropic plan, sensitive samples are generated in two different ways. We choose two classical adversarial attack algorithm, namely FGSM~\cite{goodfellow2014explaining} and VAT~\cite{miyato2018virtual}, to produce noise adding to a feature vector. Note that FGSM~\cite{goodfellow2014explaining} needs true labels to execute a backpropagation to compute gradients, so that we use pseudo labels to replace them. We set batchsize to 128 in all tasks for both plans and all models are trained for 150 epochs.


\begin{table*}[htbp]
\centering
\caption{Classification accuracy percentage of VisDA classification experiment. The first row corresponds to the performance if no adaption is implemented. Columns in the middle correspond to different categories and the column on the right represents average accuracy. We evaluate three SRDA models with different plans for adding noise. SRDA* denotes the models which are optimized in the image level. The number behind MCD denotes different hyper-parameters. The results are cited from each study.}
\label{tab:visda}
\begin{tabular}{l|cccccccccccc|r}
\hline\hline
Method&Pl&Bc&Bs&Ca&Hr&Kf&Mc&Ps&Pt&Sk&Tr&Tk&Avg\\
\hline
Source Only&55.1&53.3 &61.9 &59.1&80.6&17.9&79.7&31.2&81.0&26.5&73.5&8.5&52.4\\
\hline
DAN~\cite{pmlr-v37-long15}&87.1&63.0&76.5&42.0&90.3&42.9&85.9&53.1&49.7&
36.3&{\bf 85.8}&20.7&61.1\\
DANN~\cite{ganin2016domain}&81.9&{\bf 77.7}&82.8&44.3&81.2&29.5&65.1&28.6&
51.9&{\bf 54.6}&82.8&7.8&57.4\\
MCD()~\cite{saito2018maximum}&81.1&55.3&83.6&{\bf 65.7}&87.6&72.7&83.1&73.9&85.3&
47.7&73.2&27.1&69.7\\
MCD()~\cite{saito2018maximum}&90.3&49.3&82.1&62.9&{\bf 91.8}&69.4&83.8&72.8&79.8&
53.3&81.5&{\bf 29.7}&70.6\\
MCD()~\cite{saito2018maximum}&87.0&60.9&{\bf 83.7}&64.0&88.9&79.6&84.7&76.9&{\bf 88.6}&
40.3&83.0&25.8&71.9\\
\hline
SRDA(FGSM)&90.1&67.0&82.3&56.0&84.8&{\bf 88.2}&90.3&77.0&
82.5&26.8&85.0&16.2&71.1\\
SRDA*(FGSM)&0.8&0.9&8.8&3.0&1.0&12.9&69.5&0.1&
2.5&0.7&26.8&1.4&11.9\\
SRDA(VAT)&89.4&43.5&81.2&60.2&81.1&57.6&{\bf 93.7}&76.6&
81.8&41.3&79.6&22.0&69.5\\
SRDA*(VAT)&2.6&1.4&2.1&1.6&4.1&23.3&48.9&0.7&
21.8&1.4&3.7&1.3&9.8\\
SRDA(RAN)&{\bf 90.9}&74.8&81.9&59.1&87.5&77.3&89.9&{\bf 79.4}&
85.3&40.6&85.1&21.6&{\bf 73.5}\\
SRDA*(RAN)&40.2&40.2&55.9&62.9&60.5&75.9&83.0&61.7&
73.0&23.2&80.8&5.7&58.0\\
\hline\hline
\end{tabular}
\end{table*}

Results of the digits classification experiment are shown in Table~\ref{tab:digits}. We compare our three SRDA models, namely SRDA(FGSM), SRDA(VAT) and SRDA(RAN), with other state-of-the-art UDA algorithms such as DAN~\cite{pmlr-v37-long15}, DANN~\cite{ganin2016domain}, DSN~\cite{NIPS2016_6254}, ADDA~\cite{tzeng2017adversarial}, CoGAN~\cite{liu2016coupled}, ATDA~\cite{pmlr-v70-saito17a}, ASSC~\cite{Haeusser_2017_ICCV}, DRCN~\cite{DBLP:conf/eccv/GhifaryKZBL16} and MCD~\cite{saito2018maximum}. Among all four tasks, SRDA ranks first in three of them. Especially in USPSMNIST which is the most difficult task, our three models are the top three. Only MCD is comparable to them and other methods are inferior to ours with a large margin. In SVHNMNIST and MNISTUSPS, SRDA (RAN) ranks first. In SYNSIGGTSRB, our models do not obtain the best results. We conclude that it is caused by the relative satisfying results when no adaptation is implemented. Once a model without adaptation allocates target samples satisfying Lipschitz constraint well, SRDA is hard to detect enough sensitive samples to optimize . Thus, SRDA can not improve the baseline a lot. The fact that improvement in the easiest task SYNSIGGTSRB is the smallest verifies our conclusion. 

\subsection{VisDA Classification}

We further assess SRDA on a more complex object classification dataset. VisDA in this experiment constructs an adaptation from synthetic-object to real-object images. It contains more than 280K images belonging to 12 categories. These images are divided into training, validation and test sets. There are 152,397 training images synthesized by rendering 3D models with different angles and lighting conditions. The validation images are collected from MSCOCO~\cite{10.1007/978-3-319-10602-1_48} and amount to 55,388 in total. In this experiment, we regard the training set as a source domain and the validation set as a target domain. Similarly, in order to ensure fairness, we utilize the same backbone network, ResNet101~\cite{7780459}, with MCD~\cite{saito2018maximum}. The setting of generator and classifier networks is also the same. In addition, we also employ the entropy minimization trick used in MCD~\cite{saito2018maximum}.

In this experiment, we also implement both isotropic and anisotropic plans. For the anisotropic plan, FGSM~\cite{goodfellow2014explaining} and VAT~\cite{miyato2018virtual} algorithms are implemented. All models are trained for 15 epochs and batchsize is  32. Learning rate is  and hyper-parameter  is set to 0.5. Similarly, we compare SRDA(FGSM), SRDA(VAT) and SRDA(RAN) with several typical methods, such as DAN~\cite{pmlr-v37-long15}, DANN~\cite{ganin2016domain}, and MCD~\cite{saito2018maximum} which is the state-of-the-art method.

Results of the VisDA classification experiment are shown in Table~\ref{tab:visda}. SRDA and MCD~\cite{saito2018maximum} achieve much better accuracy than other methods. Moreover, SRDA(RAN) ranks first among all the models and SRDA (FGSM) obtains comparable accuracy with MCD~\cite{saito2018maximum}. In detail, SRDA(RAN) achieves the best results in class plane and person, SRDA(VAT) achieves the best result in class motor cycle and SRDA(FGSM) gets the best result in class knife. An interesting phenomenon is that three models of SRDA perform diversely among these categories. For example, in class knife, SRDA(FGSM) performs much better than the others and SRDA(VAT) ranks first in class motor cycle. Overall, SRDA(RAN) performs best. This reflects the importance of detecting sensitive samples. A well-defined plan which could seek more sensitive samples and a metric that could illustrate the smoothness of samples precisely are hopeful to further promote the proposed method.

\subsection{Image Level versus Feature Space}

In our optimization strategy, LSD is minimized in the feature space instead of image level. We determine it because the dimension of image  makes a great impact on the basic error bound. If the optimization is conducted in the image level,  would be much larger than in the feature space and a loose error bound is formalized. To verify this assumption, we assess SRDA that optimized in the image level on both digits and VisDA datasets. In detail, we adopt equations~\ref{eq6} and \ref{eq9} for the isotropic and anisotropic plans, respectively, as our optimization goals. The three models are denoted as SRDA*(FGSM), SRDA*(VAT) and SRDA*(RAN). For a fair comparison, all the settings are the same with models optimized in the feature space.

Results of the models optimized in the image level are shown in Table~\ref{tab:digits} and \ref{tab:visda}. Except that SRDA*(RAN) and SRDA*(VAT) perform slightly better than SRDA(RAN) and SRDA(VAT) in USPSMNIST and MNISTUSPS, respectively, all models optimized in the feature space obtain much better performance than in the image level. Particularly, SRDA*(FGSM) is quite sensitive to  where its performance drops a lot in almost tasks and even it collapses in SYNSIGGTSRB. In the VisDA classification experiment, SRDA*(FGSM) and SRDA*(VAT) perform like random guessing, and the accuracy of SRDA*(RAN) decrease 15.5\% with respect to SRDA(RAN). Large-scale images in VisDA dataset aggravate the influence of  because the difference of  between the image level and feature space is more obvious than in the digits classification experiment. Therefore, we conclude that optimization in the feature space is necessary to reduce the value of . In addition, we find that the isotropic plan is the most robust plan.

\subsection{Batchsize Analysis}

According to theorem~\ref{the1}, the basic UDA error bound is related to the batchsize  where a large batchsize leads to a small error bound. This assumption is confirmed in other computer vision problem~\cite{he2018bag,brock2018large}. In order to verify how  influences the performance of SRDA, we evaluate SRDA with different . Except batchsize, other settings follow the VisDA classification experiment.

Results are shown in Table~\ref{tab:batchsize}. For all the three SRDA models, the average accuracy decreases as batchsize gets smaller. Specially, when  is set to , the performance drops rapidly even is lower than . When , the performance drops gradually and maintains the accuracy over . The trend shows a evidence that the batchsize  should be set large enough for optimizing SRDA, otherwise a loose error bound can not ensure the effectiveness.

\begin{table}[htbp]
\centering
\caption{Classification accuracy percentage of VisDA classification experiment for different batchsize.}
\label{tab:batchsize}
\begin{tabular}{l|cc}
\hline\hline
Method& Batchsize & Average Accuracy \\
\hline
\multirow{4}{*}{SRDA(FGSM)}& 32& {\bf 71.1}\\
& 16& 69.1\\
& 8& 64.5\\
& 4& 29.5\\
\hline
\multirow{4}{*}{SRDA(VAT)}& 32& {\bf 69.5}\\
& 16& 66.5\\
& 8& 62.2\\
& 4& 34.3\\
\hline
\multirow{4}{*}{SRDA(RAN)}& 32& {\bf 73.5}\\
& 16& 71.1\\
& 8& 64.2\\
& 4& 51.4\\
\hline\hline
\end{tabular}
\end{table}

\subsection{Limitation}

As illustrated in~\cite{Ben-David2014}, to conpensate the lack of high-quality samples, a large quantity of low-quality data is needed. Thus, we choose a small-scale dataset, Office-31 which comprises only 4,110 images and 31 categories collected from three domains: AMAZON (A) with 2,817 images, DSLR (D) with 498 images and WEBCAM (W) with 795 images, to verify this assumption. We focus on the most difficult four task: AD, AW, DA and WA and test SRDA (RAN) and SRDA (FGSM) in this experiment. Both of them are trained for 50 epochs and batchsize is set to 32. Learning rate is set to 1e-3 and hyper-parameter  is set to 0.5 for all four tasks. We compare them with several classical DA models such as GFK~\cite{gong2012geodesic}, TCA~\cite{pan2010domain}, DAN~\cite{pmlr-v37-long15}, RTN~\cite{long2016unsupervised} and DANN~\cite{ganin2016domain}. All models utilze the same backbone, ResNet101. 

Results of the Office-31 classification experiment are shown in Table~\ref{tab:office31}. Overall, SRDA is comparable to RTN and performs worse than DANN. In particular, SRDA performs poor in DA and WA whereas in AD, AW, its performance is acceptable. Considering the extreme small scale of D and W, the results are reasonable. This experiment comfirms our assumption that SRDA indeed requires a great quantity of data. We suggest that the amount of images in Office-31 is not effficient to detect enough sensitive samples to train . 

\begin{table}[htbp]
\centering
\caption{Classification accuracy percentage of Office-31 classification experiment. The first row corresponds to the performance if no adaption is implemented.  We evaluate two SRDA models with different plans for adding noise. The results are cited from published papers.}
\label{tab:office31}
\begin{tabular}{l|ccccc}
\hline\hline
\multirow{3}{*}{Method}& A &A & D & W & \multirow{3}{*}{AVG}\\
& &  &  & &\\
&D & W &A &A&\\
\hline
Source Only&68.9& 68.4 &62.5 &60.7&65.2\\
\hline
GFK~\cite{gong2012geodesic}& 74.5 & 72.8 & 63.4 & 61.0&67.9\\
TCA~\cite{pan2010domain}& 74.1 & 72.7 & 61.7 & 60.9&67.4\\
DAN~\cite{pmlr-v37-long15}&78.6&80.5&63.6&62.8&71.4\\
RTN~\cite{long2016unsupervised}& 77.5 & 84.5 & 66.2 & 64.8& 73.3\\
DANN~\cite{ganin2016domain}&79.7& 82.0&{\bf 68.2}& {\bf 67.4}&{\bf 74.3}\\
\hline
SRDA(FGSM)&{\bf 82.5}&{\bf 84.7}&62.5&61.0&72.7\\
SRDA(RAN)&78.8&83.2&67.3&64.8&73.5\\
\hline\hline
\end{tabular}
\end{table}

\section{Conclusion}
\label{conclusion}

In this paper, we propose a method for UDA inspired by the probabilistic Lipschitz constraint. We extend principles analyzed in~\cite{Ben-David2014} to a deep end-to-end model and practical optimization strategy. The key to strengthening Lipschitz continuity is to minimize the local smooth discrepancy we defined. To avoid a loose error bound, the optimization strategy is subtly designed by considering the dimension of samples and batchsize. Experiments demonstrate that pure Lipschitz constraint is effective for UDA and factors we discuss are critical to an efficient and stable model.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{srda}
}


\newpage
\onecolumn
\section{Supplementary}

\subsection{Discussion of Local Smooth Discrepancy}

In order to verify that LSD we defined indeed reflects the degree Lipschitz constraint is satisfied and the performance of a model, we show the relationship between LSD and accuracy in Figure~\ref{fig:lossacc}. Three models, SRDA (RAN), SRDA (FGSM) and SRDA (VAT), are assessed on VisDA. We follow the settings in VisDA classification experiment. Note that because we get the accuracy every epoch and LSD is recorded every step, we show the accuracy after a quadratic interpolation.

As shown in Figure~\ref{fig:randomlossacc},~\ref{fig:fgsmlossacc}, and~\ref{fig:vatlossacc}, the accuracy of all three models gradually increases as LSD decreases. This indicates that the proposed LSD is a reasonable metric to evaluate the performance of a UDA model. In the VisDA classification experiment, SRDA (RAN) performs best on VisDA, SRDA (FGSM) ranks second and SRDA (VAT) is the worst model among them. In fact, the order of performance on VisDA also corresponds to LSD. SRDA (VAT) shows the highest loss, SRDA (RAN) obtains the lowest loss and SRDA (FGSM) ranks in the middle. The result verifies that Lipschitz continuous property is a key factor that affects the performance of a UDA model. Our explanation of performance drop in the target domain based on smoothness is also confirmed. This is a remarkable property because the adversarial training, which is the most widely used DA method, lacks a metric to supervise the training procedure. Although the adversarial loss is an approximation of    distance, it is not able to reflect the real performance of a model because the training procedure is a min-max game. Instead, LSD is a proper metric to supervise the training period as SRDA utilizes a pure minimization optimization.

\begin{figure*}[h]
  \centering
  \subfigure[SRDA (RAN)]{
    \label{fig:randomlossacc} \includegraphics[width=0.24\columnwidth]{randomlossacc.pdf}}
\subfigure[SRDA (FGSM)]{
    \label{fig:fgsmlossacc} \includegraphics[width=0.24\columnwidth]{fgsmlossacc.pdf}}
\subfigure[SRDA (VAT)]{
    \label{fig:vatlossacc} \includegraphics[width=0.24\columnwidth]{vatlossacc.pdf}}
\subfigure[MCD]{
    \label{fig:lossaccMCD} \includegraphics[width=0.24\columnwidth]{lossaccMCD.pdf}} 
\caption{Three figures on the left display relationship between LSD (red line) and accuracy (blue line) during the training period. Three SRDA models are evaluated on VisDA. As discrepancy decreases, the accuracy increases. The figure on the right display relationship between LSD and accuracy in MCD. The model with higher accuracy gets a lower LSD.}
  \label{fig:lossacc} \end{figure*} 

Moreover, to prove that LSD is a general metric to assess the performance of a UDA model, we further test it on MCD with different accuracy. In this experiment, we follow the settings described in MCD and train models on VisDA. Overall, we train 12 MCD models with different accuracy by tuning hyper-parameters. As LSD is not the objective function of MCD, we introduce the original FGSM algorithm to generate adversarial samples on image level. Traditional white adversarial attack algorithms generate samples with their own networks. This paradigm introduces a new variable that adversarial samples are not the same for different MCD models. Thus, we generate adversarial samples with SRDA (RAN) in this experiment to ensure fairness for each MCD model.  is also set to 0.5. With these adversarial images, LSD is calculated with their corresponding images in the target domain. To ensure the validity of the results, both classifiers in MCD are tested.

As is shown in Figure~\ref{fig:lossaccMCD}, there is an obvious relationship between LSD and accuracy that a low LSD corresponds to high accuracy. We train 12 MCD models with accuracy belongs to   
. LSD of both classifiers  gradually decreases from 0.6 to 0.3 roughly. This means that LSD is a reasonable general metric to evaluate the performance of a UDA model. However, there are also several MCD model with high accuracy showing relative high LSD. We argue that the randomness of the gradient descent algorithm causes this fluctuation.

\subsection{Visualization of Representation}

We further analyze the behavior of SRDA by shown T-SNE embeddings of the feature space where we conduct our optimization strategy. In particular, we visualize the embeddings for MNISTUSPS and VisDA classification.

{\bf MNISTUSPS} In Figure~\ref{fig:mnist_usps_embedding}, we show visualizations of the model without adaptation, SRDA(FGSM), SRDA(VAT) and SRDA(RAN) for MNISTUSPS. SRDA(VAT) and SRDA(RAN) separate target samples into ten clusters whereas SRDA(FGSM) only form nine clusters. It is clear that SRDA (RAN) not only separates target samples into different clusters but also aligns the source and target distributions well. In Figure~\ref{fig:mnist_usps_all_embedding}, all SRDA models are optimized in the image level. SRDA*(FGSM) shows a strong clustering of the USPS samples (red). Note that SRDA*(VAT) aligns the source and target distribution well whereas SRDA(VAT) does not show this property. It associates with the fact that SRDA*(VAT) obtains higher accuracy in MNISTUSPS than SRDA(VAT). 

\begin{figure*}[h]
  \centering
  \subfigure[Source Only]{
    \label{fig:mnist_usps_noadapt} \includegraphics[width=0.24\columnwidth]{mnist_usps_noadapt_embedding.pdf}}
\subfigure[SRDA (FGSM)]{
    \label{fig:mnist_usps_fgsm} \includegraphics[width=0.24\columnwidth]{mnist_usps_fgsm_embedding.pdf}}
\subfigure[SRDA (VAT)]{
    \label{fig:mnist_usps_vat} \includegraphics[width=0.24\columnwidth]{mnist_usps_vat_embedding.pdf}}
\subfigure[SRDA (RAN)]{
    \label{fig:mnist_usps_random} \includegraphics[width=0.24\columnwidth]{mnist_usps_random_embedding.pdf}} 
\caption{T-SNE plots of SRDA performed optiimization in the feature space for MNIST (blue)USPS (red).}
  \label{fig:mnist_usps_embedding} \end{figure*}

\begin{figure*}[h]
  \centering
  \subfigure[SRDA*(FGSM)]{
    \label{fig:mnist_usps_fgsm} \includegraphics[width=0.32\columnwidth]{mnist_usps_fgsm_all_embedding.pdf}}
\subfigure[SRDA*(VAT)]{
    \label{fig:mnist_usps_vat} \includegraphics[width=0.32\columnwidth]{mnist_usps_vat_all_embedding.pdf}}
\subfigure[SRDA*(RAN)]{
    \label{fig:mnist_usps_random} \includegraphics[width=0.32\columnwidth]{mnist_usps_random_all_embedding.pdf}} 
\caption{T-SNE plots of SRDA performed optimization in the image level for MNIST (blue)USPS (red).}
  \label{fig:mnist_usps_all_embedding} \end{figure*}

\begin{figure*}[h]
  \centering
  \subfigure[SRDA(FGSM)]{
    \label{fig:mnist_usps_fgsm} \includegraphics[width=0.32\columnwidth]{fgsm_embedding.pdf}}
\subfigure[SRDA(VAT)]{
    \label{fig:mnist_usps_vat} \includegraphics[width=0.32\columnwidth]{vat_embedding.pdf}}
\subfigure[SRDA(RAN)]{
    \label{fig:mnist_usps_random} \includegraphics[width=0.32\columnwidth]{random_embedding.pdf}} 
\caption{T-SNE plots of SRDA which performed optimization in the feature space for VisDA classification experiment. Blue points denotes source samples whereas red points denotes target samples.}
  \label{fig:visda_embedding} \end{figure*}

\begin{figure*}[h]
  \centering
  \subfigure[SRDA*(FGSM)]{
    \label{fig:mnist_usps_fgsm} \includegraphics[width=0.32\columnwidth]{fgsm_all_embedding.pdf}}
\subfigure[SRDA*(VAT)]{
    \label{fig:mnist_usps_vat} \includegraphics[width=0.32\columnwidth]{vat_all_embedding.pdf}}
\subfigure[SRDA*(RAN)]{
    \label{fig:mnist_usps_random} \includegraphics[width=0.32\columnwidth]{random_all_embedding.pdf}} 
\caption{T-SNE plots of SRDA which performed optimization in the image level for VisDA classification experiment. Blue points denotes source samples whereas red points denotes target samples.}
  \label{fig:visda_all_embedding} \end{figure*} 

{\bf VisDA} In Figure~\ref{fig:visda_embedding}, we show visualizations of SRDA(FGSM), SRDA(VAT) and SRDA(RAN) for the VisDA classification experiment. All three models seperate target samples into several clusters and try to align the source and target distributions. However, none of them could form twelve clusters clearly like the source distribution and there exists adhesion between different clusters. In Figure~\ref{fig:visda_all_embedding}, we show visualizations of SRDA*(FGSM), SRDA*(VAT) and SRDA*(RAN) for the VisDA classification experiment. All of them demonstrate strong clustering of the target samples, even the source samples gather together. The source and target distributions are separated in all models.
\end{document}
