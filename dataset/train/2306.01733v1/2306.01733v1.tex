\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage{mathtools}
\usepackage{multicol}

\usepackage{threeparttablex}\usepackage{longtable}\usepackage[dvipsnames]{xcolor}

\usepackage{scalerel}
\usepackage{stmaryrd}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{cite}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{booktabs} \usepackage[font=small]{caption} 
\usepackage{sidecap}
\sidecaptionvpos{figure}{c}

\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage[normalem]{ulem}

\usepackage{pifont}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[export]{adjustbox}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,
	breaklines=true,
	captionpos=b,
	keepspaces=false,
	numbers=left,
	numbersep=5pt,
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=2,
	basicstyle=\ttfamily\scriptsize
}

\lstset{style=mystyle}




\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\definecolor{mycolor}{HTML}{F7F8E0}
\definecolor{darkcyan}{RGB}{0,113,194}
\definecolor{forestgreen}{RGB}{34,139,34}

\hypersetup{colorlinks,breaklinks=true,
citecolor=darkcyan
}

\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
\newcolumntype{Z}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{\hspace*{-\tabcolsep}}}



\iccvfinalcopy 

\def\iccvPaperID{7308} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{DocFormerv2: Local Features for Document Understanding}

\author{Srikar Appalaraju \thanks{Corresponding author.}, \quad Peng Tang , \quad Qi Dong, \quad Nishant Sankaran, Yichu Zhou \thanks{Work conducted during an internship at Amazon.}, \quad R. Manmatha \\
 AWS AI Labs \quad
 School of Computing at University of Utah\\
{\tt\small \{srikara, tangpen, qdon, nishsank, manmatha\}@amazon.com, flyaway@cs.utah.edu}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi


\newcommand{\xrightarrowdbl}[2][]{\leftarrow\mathrel{\mkern-14mu}\xrightarrow[#1]{#2}
}
\newcommand{\papertitle}{DocFormerv2 }
\newcommand{\papertitlenospace}{DocFormerv2}
\newcommand{\papertitleshort}{DFv2 }
\newcommand{\papertitleshortnospace}{DFv2}
\renewcommand\theadset{\def\arraystretch{.85}}


\begin{abstract}
   We propose \papertitlenospace, a multi-modal transformer for Visual Document Understanding (VDU). The VDU domain entails understanding documents (beyond mere OCR predictions) e.g., extracting information from a form, VQA for documents and other tasks. VDU is challenging as it needs a model to make sense of multiple modalities (visual, language and spatial) to make a prediction. Our approach, termed \papertitle is an encoder-decoder transformer which takes as input - vision, language and spatial features. \papertitle is pre-trained with unsupervised tasks employed asymmetrically i.e., two novel document tasks on encoder and one on the auto-regressive decoder. The unsupervised tasks have been carefully designed to ensure that the pre-training encourages local-feature alignment between multiple modalities. \papertitle when evaluated on nine  datasets shows state-of-the-art performance over strong baselines e.g. TabFact (4.3\%), InfoVQA (1.4\%), FUNSD (1\%). Furthermore, to show generalization capabilities, on three VQA tasks involving scene-text, \papertitle outperforms previous comparably-sized models and even does better than much larger models (such as GIT2, PaLi and Flamingo) on some tasks.
    Extensive ablations show that due to its pre-training, \papertitle understands multiple modalities better than prior-art in VDU.
\end{abstract}
\vspace{-2em}



\section{Introduction}
\label{sec:intro}



Documents have become ubiquitous carriers of information, including forms, tables, invoices, and other structured documents. Many such documents require visual and layout understanding to make sense (just the text string is insufficient). Visual Document Understanding (VDU) is the task of leveraging machine learning techniques to comprehend such scanned documents, such as PDFs or images. Popular VDU tasks include Document and Tables VQA \cite{mathew2020docvqa,Chen2019TabFactAL}, sequence labeling for key-value identification in forms \cite{Jaume2019FUNSDAD}, entity extraction \cite{park2019cord}, and document classification \cite{harley2015icdar}. While modern deep-learning based OCR models \cite{litman2020scatter} have proven to be effective in extracting text from documents, the naive approach of linearizing the OCR-text and feeding it to a language model is sub-optimal. This is because the content of a document is presented according to a visual layout and structure that must be taken into account for accurate understanding. Naively linearizing the text from left-to-right will result in sub-optimal performance as the semantic meaning alters based on layout, as shown in Figure \ref{fig:splash} - Table \ref{table:cord},\ref{table:funsd} has experiments demonstrating this. Instead, VDU requires a multi-modal approach that can comprehend text and visual features in the context of a document's 2D layout.

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{iccv2023AuthorKit/images/dfv2_splash3.png}
  \caption{\textbf{Visual Document Understanding}. Snippet of a document receipt from DocVQA \cite{mathew2021docvqa}. VDU tasks could include a model asked to predict "SOLD TO" address (VQA) or predict all relations ("SOLD TO"  address, "SHIP TO"  address) or asked to infer info from table (at the top).
  }
  \label{fig:splash}
  \vspace{-5mm}
\end{figure}



Multi-modal training in general entails feature alignment. Specific to vision-language learning this means aligning a piece of text with an arbitrary span of pixels in visual space \cite{hoyoro,Kim2021ViLTVT,CLIPRadford2021LearningTV,wang2022git,Alayrac2022FlamingoAV,biten2022latr,appalaraju2021docformer,Hao2022MixGenAN,appalaraju2020towards,Li2022SeeTek,Chen2022PaLI}. How those features are aligned makes a big difference. In VDU, a majority of the tasks require \textit{local and layout-relative} understanding of the document. For example, in document VQA, semantic labeling or entity extraction, a model needs to make sense of text in-relation to where the text is placed in a document. E.g.: "1" when placed at the top-right/bottom-left of a document is to be interpreted as a page-number vs as a number when placed anywhere else. 

\begin{table}
\centering
	\scalebox{0.72}{
		\begin{tabular}{l|c|l|c|l|cH}
			Model & Year & Conf. & Arch. & Input Mod. & Vision Branch & Core Idea\\ 
\midrule
			LayoutLMv1 \cite{xu2020layoutlm} & 2020 & KDD & E & T + S & - & spatial emb.  \\
			DocFormerv1 \cite{appalaraju2021docformer} & 2021 & ICCV & E & T + V + S & Resnet50 & - \\
			LayoutLMv2 \cite{xu2020layoutlmv2} & 2021 & ACL & E & T + V + S & ResNeXt 101 & - \\
			SelfDoc \cite{Li2021SelfDoc} & 2021 & CVPR & E & - & - & - \\
LayoutLMv3 \cite{huang2022layoutlmv3} & 2022 & ACM & E & T + V + S & Linear & - \\
			BROS \cite{bros2020hong} & 2022 & AAAI & E & T + S & - & -  \\ 
XYLayoutLM \cite{Gu2022XYLayoutLM} & 2022 & CVPR & E & T + V + S & ResNeXt 101 & - \\
			FormNet \cite{Lee2022FormNet} & 2022 & ACL & E  & - & - & - \\
			ERNIE-Layout \cite{peng2022ernie} & 2022 & EMNLP & E & T + V + S & F-RCNN & - \\
LiLT \cite{Wang2022LiLT} & 2022 & ACL & E & T + S & - & 3 on encoder \\
			XDoc \cite{Chen2022XDoc} & 2022 & EMNLP & E & T & - & - \\
			\midrule
			TILT \cite{powalski2021going} & 2021 & ICDAR & E + D & T + V + S & U-Net & 1 on decoder \\
			\midrule
			DocFormerv2 (ours) & 2023 & - & E + D & T + V + S & Linear & 2 task on enc, 1 on dec \\
\end{tabular}
	}
	\caption{\textbf{VDU Related Work}: In this table, a summary of VDU prior art is presented  with their architecture (E: Encoder, D: Decoder), the input (T: text, V: vision, S: spatial features), the vision features branch and core idea behind the work.}
	\label{table:prior_art}
\end{table}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{iccv2023AuthorKit/images/dfv2_priorart_horizontal_4.png}
\caption{\textbf{VDU Paradigms:} Broad state of Visual Document Understanding (VDU) approaches. In \textbf{a)} E-only LayoutLM \cite{xu2020layoutlm} and variants. \textbf{b)} E+D but only language-task TILT \cite{powalski2021going}. \textbf{c)} Ours }
\label{fig:prior_art}
\end{figure}









Based on this domain understanding of VDU and its challenges, we present \papertitle (\papertitleshortnospace) which is an encoder-decoder multi-modal transformer. 
In this work, we meticulously devise two novel unsupervised pre-training tasks with the objective of incorporating local semantic information of a document into the model. These pre-training tasks impart the ability to the model to accurately locate relevant information within the document. We also depart from VDU prior-art\cite{powalski2021going,Tang2022UnifyingUDOP} as we introduce a novel asymmetrical method of pre-training. i.e., multi-task pre-training on encoder (two tasks) and decoder (one task).
We propose two novel pre-training tasks on encoder with the intent to enrich the encoder with local semantic information. The tasks aid in by fusing and aligning multi-modal input and generating efficient representations for the decoder. 
We show that these pre-training tasks are necessary for effective VDU (see \S \ref{sec:experiments:ablation_experiments}). 
Furthermore, we demonstrate that a simplified linear visual layer is sufficient to encapsulate visual features, simplifying the architecture from previous VDU research \cite{xu2020layoutlmv2, Li2021SelfDoc, powalski2021going} which required specific visual encoders \cite{dosovitskiy2020image,Liu2021Swin, he_cvpr2016_resnet}.



Experimentally we demonstrate that \papertitle achieves state-of-the-art performance on  five VDU tasks. In addition, we demonstrate the versatility of \papertitle by utilizing its pre-trained model and fine-tuning it on text-VQA tasks from a completely different domain. Our approach yields superior performance on three distinct text-VQA datasets, surpassing comparable models and in some datasets much bigger models like GIT2 \cite{wang2022git}, PaLi \cite{Chen2022PaLI} and Flamingo \cite{Alayrac2022FlamingoAV}. 
Therefore, the primary contributions of this paper are as follows:



\begin{itemize}[leftmargin=*]
	\item Asymmetrical method of pre-training for VDU: Two novel tasks on the encoder which encourage local multi-modal feature collaboration (\textit{Token-to-Line} task and \textit{Token-to-Grid} task) and one on the decoder \S \ref{sec:approach:pretrain}. \item Simplified Visual branch: \papertitle is end-to-end trainable and it does not rely on a pre-trained object detection network for visual features simplifying its architecture. On five varied downstream VDU tasks, \papertitle achieves state of the art results \S \ref{sec:experiments}. 
	\item We also show \papertitle versatility by fine-tuning it on a totally different domain - text-VQA datasets without changing the pre-training. \papertitle beats strong baselines and achieves state-of-the-art numbers on three text-VQA datasets amongst similar model sizes. Selectively, on Text-VQA it out-performs much larger models like PaLi-3B \textcolor{forestgreen}{+6.8\%}, PaLi-15B \textcolor{forestgreen}{+1.5\%} and Flamingo\cite{Alayrac2022FlamingoAV} \textcolor{forestgreen}{(+9.9\%)} (106x \papertitle size in the num. of parameters) by  absolute accuracy \S \ref{sec:experiments:textvqa}. 
\end{itemize}

\noindent Furthermore, we conducted comprehensive ablation experiments to demonstrate the advantages of our pre-training tasks, the model's resilience to input noise, and the efficacy of the simplified visual branch.


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.7\linewidth]{iccv2023AuthorKit/images/dfv2_pretrain_architecture_detailed2_blurred.png}
  \caption{\textbf{\papertitle  Pre-train Architecture}. After pre-train, the two prediction heads (token-to-line and grid) on encoder are removed, rest of the architecture remains the same for down-stream tasks. Read section \ref{sec:approach:arch} for more details on  and . All components are end-to-end trainable. Best viewed in color. }
  \label{fig:docformerv2_pretrain_architecture}
\end{figure*}



\section{Related Work}
\label{sec:related_work}
VDU research has attracted considerable attention over the past few years \cite{Wang2022ABF,xu2020layoutlm,xu2020layoutlmv2,appalaraju2021docformer,Li2021SelfDoc,powalski2021going,Li2021MarkupLM,huang2022layoutlmv3,hong2020bros,Gu2022UnifiedUDOC,Gu2022XYLayoutLM,Lee2022FormNet,Wang2022LiLT,Chen2022XDoc,Tang2022UnifyingUDOP,Borchmann2021DUE,peng2022ernie,li2021structurallm}. Prominent published research papers in this area are catalogued in Table \ref{table:prior_art}. As can be observed, the research focus has been lopsided towards encoder-only models. While TILT \cite{powalski2021going} proposed a encoder-decoder transformer for VDU, they only train it on one pre-training task (masked language modeling) and also use a bulky visual CNN. Our approach \papertitle, not only simplifies the architecture by not using a separate visual module (CNN or Transformer based) and has multiple unsupervised pre-training tasks. 














\section{Approach}
\label{sec:approach}


\subsection{Architecture}
\label{sec:approach:arch}

\papertitle (\papertitleshort) is a multi-modal encoder-decoder transformer architecture (see fig. \ref{fig:docformerv2_pretrain_architecture}). Three variations of \papertitleshort are designed - small, base and large variants (see table \ref{table:dfv2_variants}).
\papertitleshort takes multi-modal inputs, the image of the document , text  extracted by an OCR model along with OCR bounding box co-ordinates as spatial features . \papertitleshort has a unified multi-modal encoder where the multi-modal features fuse and align with the help of novel pre-training tasks (see \S \ref{sec:approach:pretrain}). 


\label{sec:approach:arch:visual}
\noindent\textbf{Visual features:} \papertitleshort has a simplified visual branch contrary to most VDU prior-art (fig. \ref{fig:prior_art}). \papertitleshort consumes a flattened image sequence as visual input. Specifically, let  be the image of a document. A simple  is used to create an image embedding. The weights are randomly initialized for pre-training. As documents tend to have lots of white-space, the linear down-sampling layer gives an opportunity for the model to only keep relevant visual features. Based on our ablation experiments (see \S \ref{table:ablation:image_encoder}), this simple approach gives better results than using expensive image encoders such as Swin, ViT  \cite{Liu2021Swin, dosovitskiy2020image, Ronneberger2015UNet} or bulky object-detection networks like FRCNN variants \cite{Ren2015FasterFRCNN} as was used in VDU prior-art \cite{powalski2021going,appalaraju2021docformer,xu2021layoutlmv2}.
Since transformer layers are permutation-invariant, a learnable 2D-positional encoding  is also computed. Finally, 


\label{sec:approach:arch:language}
\noindent\textbf{Language features:} Let  be the predicted text extracted via an OCR model for a document image. \papertitleshort uses a sentence-piece sub-word tokenizer \cite{Kudo2018SentencePiece} to get tokens . A maximum sequence limit  is applied during training and testing, so if the number of OCR tokens is greater than , the rest is ignored. If the sequence length is less than , the sequence is padded. The OCR tokens  are sent to a learnable embedding layer  to create a text embedding .

\label{sec:approach:arch:spatial}
\noindent\textbf{Spatial features:} For each OCR word , the OCR model predicts its bounding-box location in the normalized form . This information is encoded using four learnable spatial embedding layers -  for encoding a word horizontal spatial information ,  for the vertical coordinate ,  for word height  and  for the width . The spatial features not only encode the location of a word in the document but also provides cues about a word's font-size and thereby its importance in a document (via  and ). Specifically, spatial features  .  Finally, .



\label{sec:approach:arch:other}
\noindent\textbf{Other features:}  and  features are different modalities (fig. \ref{fig:docformerv2_pretrain_architecture}). As the model has no idea it is being fed multi-modal input, another learnable embedding  is used to provide cues to the model about the multi-modal input. A modality-embedding  learns nuances of different modalities, which generates  embedding for visual modality and  for text. Finally,  and .  and  are concatenated in the sequence dimension to form the input sequence to the \papertitleshort encoder.

\begin{table}
\centering
\scalebox{0.9}{
\begin{tabular}{c|c|c|c|c|c}
model & dim & ff & \# attn. H & \# layers (E,D) & \# params  \\
  \midrule
  small & 512 & 2048 & 8 & 6,6 & 66M \\
  base & 768 & 3072 & 12 & 12,12 & 232M \\
  large & 1024 & 4096 & 16 & 24,24 & 750M \\
\end{tabular}
}
\caption{\textbf{\papertitle variants}: dim is embedding dimensionality. ff is output dim of feed-forward layer. E is encoder and D is decoder. attn. H is attention heads.
}
\label{table:dfv2_variants}
\vspace{-0.6cm}
\end{table}

\subsection{Unsupervised Document Pre-training}
\label{sec:approach:pretrain}

In \papertitle we follow the now well established practice of unsupervised pre-training followed by downstream task fine-tuning. Furthermore, with the intent of eliciting the maximum benefit from unsupervised pre-training, we designed the pre-training tasks as a close proxy for downstream tasks. We now describe the two novel pre-training tasks employed on the encoder and the language modeling task on decoder. All three tasks are performed at the same time and the final loss is a linear combination of all three losses for each iteration.

\label{sec:approach:pretrain:tok_to_line}
\noindent\textbf{Encoder Token-to-Line Task:} We share the intuition that for VDU tasks local feature semantic alignment is important. Most of the related information for key-value prediction in a form or VQA is either on the same line or adjacent lines of a document e.g., see fig. \ref{fig:token_to_line}, in order to predict the value for \verb|"TOTAL"| (box a), the model has to look in the same line (to its right - \verb|"a, b, c, dFL_{tol}t_i(x_1, y_1)g_ig_i = \lfloor\frac{x_1}{\Delta_x}\rfloor + \lfloor\frac{y_1}{\Delta_y}\rfloor\cdot m,\Delta_x\Delta_ymL_{tog}\overline{S}L_{dlm}L_{final} = k * L_{tol} + l * L_{tog} + m * L_{dlm}k, l, m_{\text{large}}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{base}}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{large}}^{\dagger}_{\text{large}}^{\dagger}_{\text{large}}_{\text{large}}^{\dagger}^{\dagger}_{\text{base}}_{\text{base}}_{\text{base}}_{\text{base}}_{\text{base}}_{\text{base}}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{base}}_{\text{base}}_{\text{large}}_{\text{base}}_{\text{large}}_{\text{base}}_{\text{large}}_{\text{base}}_{\text{large}}_{\text{base}}_{\text{large}}_{\text{base}}_{\text{base}}_{\text{base}}_{\text{base}}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{base}}_{\text{large}}_{\text{base}}_{\text{large}}_{\text{base}}_{\text{large}}_{\text{base}}_{\text{large}}_{\text{base}}_{\text{large}}_{\text{base}}_{\text{large}}<>_{\text{base}}_{\text{base}}_{\text{large}}_{\text{base}}_{\text{large}}_{\text{base}}_{\text{base}}^{\dagger}_{\text{large}}^{\dagger}_{\text{base}}_{\text{large}}_{\text{base}}^{\dagger}_{\text{large}}^{\dagger}^{\dagger}\ge_{\text{large}}_{\text{large}}_{\text{base}}_{\text{base}}^{\dagger}_{\text{large}}^{\dagger}_{\text{base}}_{\text{large}}_{\text{base}}^{\dagger}_{\text{large}}^{\dagger}^{\dagger}_{\text{large}}_{\text{large}}_{\text{large}}_{\text{base}}_{\text{base}}_{\text{base}}_{\text{base}}^{\dagger}^{\dagger}_{\text{base}}_{\text{base}}p128_{\text{small}}{_{\text{small}}}_{\text{small}}_{\text{small}}$ and 1M documents from IDL, with the Vision and Token-to-line enabled. 


\section{Conclusion}

Our work \papertitle highlights the importance of two novel pre-training tasks and the efficacy of enriching encoder representations with local semantic information via pre-training tasks. We perform experiments on eight varied datasets (five on VDU and three on scene-text VQA) achieving state-of-the-art numbers on all datasets. Based on ablations, we also show the various 
design choices and its impact on downstream performance. 



























{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}