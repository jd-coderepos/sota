\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/EMAT_architecture.pdf}
\end{center}
\caption{
Architecture of the proposed Efficient Key-Value Memory Augmented Transformers (EMAT): factual knowledge is stored in a key-value memory (\cref{sec:kvm}) where keys and values correspond to questions and answers, respectively;
during inference, the model retrieves information from the memory via MIPS (\cref{sec:retriever}) and uses it to condition the generation process.
}
\label{fig:arch}
\end{figure*}

\section{Efficient Memory-Augmented Transformer}  \label{sec:emat}


In this work we propose Efficient Memory-Augmented Transformer (\ModelName), a model architecture that uses a key-value memory to store millions of dense question-answer representations to inform its predictions (see \cref{fig:arch}).
Given an input sequence , \ModelName's encoder first produces a dense query  to retrieve from the memory .
The returned key-value representations corresponding to the retrieved  key-value pairs are .
Finally, the decoder generates the target sequence  conditioned on the input  and retrieved key-value pairs .




\subsection{Key-Value Memory} \label{sec:kvm}


The key-value memory  contains representations of keys  and values , with each key  mapping to one value .
Since we use PAQ~\citep{paq} as our knowledge source, each key represents a question, and its value represents the corresponding answer.
We use \ModelName's encoder to encode the question and the answer separately, and it produces key and value embeddings from -th and -th layer of encoder respectively.


\cref{fig:arch} (left) shows details regarding how key (question) and value (answer) are encoded in \ModelName.
To encode the key embeddings, we first concatenate a prefix  of length  with the question  as input, and then obtain the hidden states at the -th layer , where  is the length of the question  prepended with \prefix.
Then,  is passed through a convolutional neural network layer to produce , 
and we use the prefix part as our final key representation  .
For value embeddings, we prepend a prefix to the answer, feed  into the model,
and use the prefix's representation at the -th layer of encoder  as our value representation, where  is the size of hidden representations.




\subsection{Memory Retrieval} \label{sec:retriever}


The goal of the retriever is to retrieve relevant entries from the key-value memory  to inform the downstream generation tasks.
\ModelName's encoder embeds the question into a query  using the same procedure as the key embeddings, described in \cref{sec:kvm}.
We conduct an extra step of flattening for both  and  by averaging:   
.
The key-value encoder shares the parameters with the question encoder,
and we define the query-key similarity by the inner product between the flattened query representation and key representation .
At inference time, this operation can be efficiently computed using Maximum Inner Product Search (MIPS)
to retrieve the top- key-value pairs  based on the similarity.
MIPS implementations such as \texttt{faiss}~\citep{faiss} enable searching across millions of vectors in milliseconds on a CPU.
The retrieved key-value pairs  are then integrated in later layers of \ModelName's encoder.


\subsection{Key-Value Integration} \label{sec:integration}


Once we have retrieves the top- key-value pairs , they need to be incorporated into the model. 
More specifically, in the -th layer, all the key embeddings in  are ordered by their corresponding similarity scores, and concatenated into a matrix .
Then it is prepended to the -th layer's hidden states.
To distinguish the different keys, we additionally add relative positional encodings to .
In the -th layer, the value embedding in  are concatenated in the same way to produce , and it is \emph{added} to the positions where their corresponding key embeddings are prepended to.
The updated hidden states continue the forward pass of the remaining transformer encoder layers.
Finally, the decoder generates the answer condition on the output of the encoder, which already integrates the retrieved key-value representations.



\section{Training Pipeline of \ModelName} \label{sec:pipeline}

\subsection{Pre-Training} \label{sec:pretrain}


\paragraph{Auto-encoding Tasks}
We use T5-base's pre-trained parameters to initialise \ModelName, but the prefix embeddings and key encoder's convolutional layer are trained from scratch.
To obtain better representation of key and value, we pre-train \ModelName with auto-encoding training objectives. We use PAQ-L1, a simplified version of PAQ that consists of 14M QA pairs, as the pre-training corpus.
The model is trained to recover the input question  given the key embeddings , and the answer  given the value embeddings , as shown in \cref{fig:arch} (left).
The two tasks key auto-encoding (KAE) and value auto-encoding (VAE) can be formalised as:



\paragraph{Generation Task}
Besides the problem of representing questions and answers in key-value memory , we also need the model to make use of  for downstream tasks. Thus, it is also critical to pre-train the model to learn the key-value integration module defined in \cref{sec:integration}.
Since PAQ provides a large number of QA pairs, we consider a generation task built on PAQ to pre-train the model.
More concretely, for each QA pair  in PAQ, we use the RePAQ model~\citep{paq} to retrieve  other relevant QA pairs from PAQ, and retrieve their corresponding keys  and values  from the memory .
Then, the model is trained to generate the answer  given the question  and the key-value embeddings corresponding to the retrieved QA pairs.
The objective can be defined as follows:

We adopt a multi-task pre-training objective to minimise .

\subsection{Fine-Tuning on Downstream Tasks} \label{sec:finetune}


After pre-training, we fine-tune both the memory retrieval module and the generation of \ModelName on the downstream tasks.


\paragraph{Retrieval Objective}
Learning to retrieve relevant key-value pairs that provide useful evidence to solve a given task can be challenging due to the lack of labelled data.
To solve this problem, we propose a weakly-supervised method to optimise the retriever.
Specifically, we first rank all retrieved key-value pairs retrieved from the memory  by their inner product scores. We consider the top retrieved key-value pairs: for each retrieved key-value pair, if its corresponding answer is lexically matched with the target output, then the key-value pair is selected as positive sample to optimise the retriever.
For short output generation tasks such as ODQA, we match the answer corresponding to the retrieved value with the target answer.
For long sequence generation tasks such as open-domain dialogue and long-form QA, we normalise the target sequence (\ie, lower-casing and removing stop words), and check whether if the retrieved value (answer) is contained in the normalised target sequence.
Since these key-value pairs are more likely to lead to the correct answer, they can be used to provide a weakly-supervised training signal to the retrieval component of \ModelName.


We denote the selected positive key-value pairs as , where each pair  is composed by a key component  and a value component .
We sample a key-value pair  from  based on the similarity between the corresponding key  and the query :

We then select  negative pairs  that do not match the target sequence.
Finally, the positive pairs  and the negative pairs  are used to train the retriever, by optimising the following objective:



\paragraph{Memory Caching for More Efficient Training}
As described above, \ModelName uses MIPS for retrieving the key-value pairs that are the most relevant to solve the current task.
However, updating the memory  after each training update may not be feasible when the number of entries in  is very large.
To alleviate this problem, we design a \emph{memory caching} mechanism. At the beginning of each training epoch, we freeze the memory  and, for each training example, we retrieve the top- key-value pairs.
The memory  is updated only at the end of the epoch by re-encoding all entries in the knowledge source.




\paragraph{Overall Fine-Tuning Objective}
The generator is optimised to generate the target  given the input  and the top- retrieved key-value pairs :

so the overall fine-tuning objective is .

\subsection{Inference} \label{sec:inference}

During inference, we use a fast Hierarchical Navigable Small World~\citep[HNSW,][]{DBLP:journals/pami/MalkovY20} graph index, generated by \texttt{faiss}, to search and retrieve from the key-value memory .
If the , the search process can run in parallel with the evaluation of the layers  in \ModelName.
Since the search process can be efficiently executed on CPU, it does not increase the GPU memory requirements of the model.


