\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/EMAT_architecture.pdf}
\end{center}
\caption{
Architecture of the proposed Efficient Key-Value Memory Augmented Transformers (EMAT): factual knowledge is stored in a key-value memory (\cref{sec:kvm}) where keys and values correspond to questions and answers, respectively;
during inference, the model retrieves information from the memory via MIPS (\cref{sec:retriever}) and uses it to condition the generation process.
}
\label{fig:arch}
\end{figure*}

\section{Efficient Memory-Augmented Transformer}  \label{sec:emat}


In this work we propose Efficient Memory-Augmented Transformer (\ModelName), a model architecture that uses a key-value memory to store millions of dense question-answer representations to inform its predictions (see \cref{fig:arch}).
Given an input sequence $X=(x_1, \cdots, x_{|X|})$, \ModelName's encoder first produces a dense query $\mathbf{q}$ to retrieve from the memory $\mathbf{M}$.
The returned key-value representations corresponding to the retrieved $k$ key-value pairs are $Z = (z_1, \cdots, z_k)$.
Finally, the decoder generates the target sequence $Y=(y_1, \cdots, y_{|Y|})$ conditioned on the input $X$ and retrieved key-value pairs $Z$.




\subsection{Key-Value Memory} \label{sec:kvm}


The key-value memory $\mathbf{M}=(\mathbf{K}, \mathbf{V})$ contains representations of keys $\mathbf{K}$ and values $\mathbf{V}$, with each key $\mathbf{k}_i$ mapping to one value $\mathbf{v}_i$.
Since we use PAQ~\citep{paq} as our knowledge source, each key represents a question, and its value represents the corresponding answer.
We use \ModelName's encoder to encode the question and the answer separately, and it produces key and value embeddings from $l_k$-th and $l_v$-th layer of encoder respectively.


\cref{fig:arch} (left) shows details regarding how key (question) and value (answer) are encoded in \ModelName.
To encode the key embeddings, we first concatenate a prefix $\prefix$ of length $\textup{P}$ with the question $q$ as input, and then obtain the hidden states at the $l_k$-th layer $\mathbf{h}^{l_k} = [\mathbf{h}_1^{l_k}, \cdots, \mathbf{h}_n^{l_k} ]$, where $n$ is the length of the question $q$ prepended with \prefix.
Then, $\mathbf{h}^{l_k}$ is passed through a convolutional neural network layer to produce $[\mathbf{c}_1, \cdots, \mathbf{c}_{n}]$, 
and we use the prefix part as our final key representation $\mathbf{k} =[\mathbf{c}_1, \cdots, \mathbf{c}_{\textup{P}}] \in \mathbb{R}^{\textup{P} \times h}$ .
For value embeddings, we prepend a prefix to the answer, feed $[\prefix; a]$ into the model,
and use the prefix's representation at the $l_v$-th layer of encoder $\mathbf{v} = [\mathbf{h}_1^{l_v}, \cdots, \mathbf{h}_{\textup{P}}^{l_v}] \in \mathbb{R}^{\textup{P} \times h}$ as our value representation, where $h$ is the size of hidden representations.




\subsection{Memory Retrieval} \label{sec:retriever}


The goal of the retriever is to retrieve relevant entries from the key-value memory $\mathbf{M}$ to inform the downstream generation tasks.
\ModelName's encoder embeds the question into a query $\mathbf{q}$ using the same procedure as the key embeddings, described in \cref{sec:kvm}.
We conduct an extra step of flattening for both $\mathbf{q}$ and $\mathbf{k}$ by averaging:   
$\bar{\mathbf{k}} = \text{flatten}(\mathbf{k}) = \frac{1}{\textup{P}} \sum_{j=1}^{\textup{P}} \mathbf{k}_{j}$.
The key-value encoder shares the parameters with the question encoder,
and we define the query-key similarity by the inner product between the flattened query representation and key representation $\text{sim}(\mathbf{q}, \mathbf{k}) = \langle \bar{\mathbf{q}}, \bar{\mathbf{k}} \rangle$.
At inference time, this operation can be efficiently computed using Maximum Inner Product Search (MIPS)
to retrieve the top-$k$ key-value pairs $Z = \{(\mathbf{k}_i, \mathbf{v}_i)\}_{i=1}^{k}$ based on the similarity.
MIPS implementations such as \texttt{faiss}~\citep{faiss} enable searching across millions of vectors in milliseconds on a CPU.
The retrieved key-value pairs $Z$ are then integrated in later layers of \ModelName's encoder.


\subsection{Key-Value Integration} \label{sec:integration}


Once we have retrieves the top-$k$ key-value pairs $Z$, they need to be incorporated into the model. 
More specifically, in the $l_c$-th layer, all the key embeddings in $Z$ are ordered by their corresponding similarity scores, and concatenated into a matrix $\mathbf{K}'=[\mathbf{k}_i, \cdots, \mathbf{k}_k] \in \mathbb{R}^{\textup{P} k \times h}$.
Then it is prepended to the $l_c$-th layer's hidden states.
To distinguish the different keys, we additionally add relative positional encodings to $\mathbf{K}'$.
In the $l_v$-th layer, the value embedding in $\mathbf{Z}$ are concatenated in the same way to produce $\mathbf{V}'$, and it is \emph{added} to the positions where their corresponding key embeddings are prepended to.
The updated hidden states continue the forward pass of the remaining transformer encoder layers.
Finally, the decoder generates the answer condition on the output of the encoder, which already integrates the retrieved key-value representations.



\section{Training Pipeline of \ModelName} \label{sec:pipeline}

\subsection{Pre-Training} \label{sec:pretrain}


\paragraph{Auto-encoding Tasks}
We use T5-base's pre-trained parameters to initialise \ModelName, but the prefix embeddings and key encoder's convolutional layer are trained from scratch.
To obtain better representation of key and value, we pre-train \ModelName with auto-encoding training objectives. We use PAQ-L1, a simplified version of PAQ that consists of 14M QA pairs, as the pre-training corpus.
The model is trained to recover the input question $x$ given the key embeddings $\mathbf{k}$, and the answer $y$ given the value embeddings $\mathbf{v}$, as shown in \cref{fig:arch} (left).
The two tasks key auto-encoding (KAE) and value auto-encoding (VAE) can be formalised as:
\begin{equation*}
\begin{aligned}
\mathcal{L}_{\text{KAE}} = & - \sum_{i=1}^{|X|}  \log P(x_i \mid \mathbf{k}, x_{<i}), \\
\mathcal{L}_{\text{VAE}} = & - \sum_{i=1}^{|Y|}  \log P(y_i \mid \mathbf{v}, y_{<i}).
\end{aligned}
\end{equation*}


\paragraph{Generation Task}
Besides the problem of representing questions and answers in key-value memory $\mathbf{M}$, we also need the model to make use of $\mathbf{M}$ for downstream tasks. Thus, it is also critical to pre-train the model to learn the key-value integration module defined in \cref{sec:integration}.
Since PAQ provides a large number of QA pairs, we consider a generation task built on PAQ to pre-train the model.
More concretely, for each QA pair $(x, y)$ in PAQ, we use the RePAQ model~\citep{paq} to retrieve $10$ other relevant QA pairs from PAQ, and retrieve their corresponding keys $\mathbf{K}'_x = [\mathbf{k}_{1}, \cdots, \mathbf{k}_{10}]$ and values $\mathbf{V}'_x = [\mathbf{v}_{1}, \cdots, \mathbf{v}_{10}]$ from the memory $\mathbf{M}$.
Then, the model is trained to generate the answer $y$ given the question $x$ and the key-value embeddings corresponding to the retrieved QA pairs.
The objective can be defined as follows:
\begin{equation*}
\mathcal{L}_{\text{Gen}} = - \sum_{i=1}^{|Y|}  \log P(y_i \mid x, \mathbf{K}'_x, \mathbf{V}'_x, y_{<i}).
\end{equation*}
We adopt a multi-task pre-training objective to minimise $\mathcal{L}_{\text{KAE}} + \mathcal{L}_{\text{VAE}} + \mathcal{L}_{\text{Gen}}$.

\subsection{Fine-Tuning on Downstream Tasks} \label{sec:finetune}


After pre-training, we fine-tune both the memory retrieval module and the generation of \ModelName on the downstream tasks.


\paragraph{Retrieval Objective}
Learning to retrieve relevant key-value pairs that provide useful evidence to solve a given task can be challenging due to the lack of labelled data.
To solve this problem, we propose a weakly-supervised method to optimise the retriever.
Specifically, we first rank all retrieved key-value pairs retrieved from the memory $\mathbf{M}$ by their inner product scores. We consider the top retrieved key-value pairs: for each retrieved key-value pair, if its corresponding answer is lexically matched with the target output, then the key-value pair is selected as positive sample to optimise the retriever.
For short output generation tasks such as ODQA, we match the answer corresponding to the retrieved value with the target answer.
For long sequence generation tasks such as open-domain dialogue and long-form QA, we normalise the target sequence (\ie, lower-casing and removing stop words), and check whether if the retrieved value (answer) is contained in the normalised target sequence.
Since these key-value pairs are more likely to lead to the correct answer, they can be used to provide a weakly-supervised training signal to the retrieval component of \ModelName.


We denote the selected positive key-value pairs as $Z^+ = (z_1^+, \cdots, z_r^+)$, where each pair $z_{i}^{+} = (\mathbf{k}_{i}^{+}, \mathbf{v}_{i}^{+})$ is composed by a key component $k_{i}^{+}$ and a value component $v_{i}^{+}$.
We sample a key-value pair $z_{i}^+$ from $Z^+$ based on the similarity between the corresponding key $\mathbf{k}_{i}^{+}$ and the query $\mathbf{q}$:
\begin{equation*}
\begin{aligned}
P_{\eta}(z_i^+ \mid q) & = \frac{\exp ( \text{sim}(\mathbf{q}, \mathbf{k}_i^+  ))} {\sum_{j=1}^r \exp ( \text{sim}( \mathbf{q}, \mathbf{k}_j^+ ) ) }, \\
z^+ & \sim P_\eta (\cdot \mid q, Z^+).
\end{aligned}
\end{equation*}
We then select $m$ negative pairs $\{z^-_j\}_{j=1}^m$ that do not match the target sequence.
Finally, the positive pairs $z^+$ and the negative pairs $z^-$ are used to train the retriever, by optimising the following objective:
\begin{equation*}
\begin{aligned}
&\mathcal{L}_{\text{Ret}} =
\\
&-\log \frac{\exp (\text{sim}(\mathbf{q}, \mathbf{k}_i^+  )  )}{ \exp ( \text{sim}(\mathbf{q}, \mathbf{k}_i^+  ) ) + \sum_{j=1}^{m} \exp( \text{sim}(\mathbf{q}, \mathbf{k}_j^-  ) ) }.
\end{aligned}
\end{equation*}


\paragraph{Memory Caching for More Efficient Training}
As described above, \ModelName uses MIPS for retrieving the key-value pairs that are the most relevant to solve the current task.
However, updating the memory $\mathbf{M}$ after each training update may not be feasible when the number of entries in $\mathbf{M}$ is very large.
To alleviate this problem, we design a \emph{memory caching} mechanism. At the beginning of each training epoch, we freeze the memory $\mathbf{M}$ and, for each training example, we retrieve the top-$n$ key-value pairs.
The memory $\mathbf{M}$ is updated only at the end of the epoch by re-encoding all entries in the knowledge source.




\paragraph{Overall Fine-Tuning Objective}
The generator is optimised to generate the target $y$ given the input $x$ and the top-$n$ retrieved key-value pairs $Z$:
\begin{equation*}
\mathcal{L}_{\text{Gen}} = -\sum_{i=1}^{|Y|}  \log P(y_i \mid x, Z, y_{<i}),
\end{equation*}
so the overall fine-tuning objective is $\mathcal{L}_{\text{Ret}} + \mathcal{L}_{\text{Gen}}$.

\subsection{Inference} \label{sec:inference}

During inference, we use a fast Hierarchical Navigable Small World~\citep[HNSW,][]{DBLP:journals/pami/MalkovY20} graph index, generated by \texttt{faiss}, to search and retrieve from the key-value memory $\mathbf{M}$.
If the $l_k < l_c$, the search process can run in parallel with the evaluation of the layers $l_k + 1, \cdots, l_c - 1$ in \ModelName.
Since the search process can be efficiently executed on CPU, it does not increase the GPU memory requirements of the model.


