\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage{pifont}\usepackage{enumitem}

\newcommand{\gcnhred}[1]{{\color{BrickRed} #1}}
\newcommand{\gcnhgreen}[1]{{\color{ForestGreen} #1}}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{GCNH: A Simple Method For Representation Learning On Heterophilous Graphs} 

\author{\IEEEauthorblockN{Andrea Cavallo\IEEEauthorrefmark{1}\IEEEauthorrefmark{2},
Claas Grohnfeldt\IEEEauthorrefmark{2}, Michele Russo\IEEEauthorrefmark{2},
Giulio Lovisotto\IEEEauthorrefmark{2} and Luca Vassio\IEEEauthorrefmark{1}}
\IEEEauthorblockA{Politecnico di Torino\IEEEauthorrefmark{1}, Huawei Munich Research Center\IEEEauthorrefmark{2}\\
\IEEEauthorrefmark{1}\texttt{\{name\}.\{surname\}}@polito.it, \IEEEauthorrefmark{2}\texttt{\{name\}.\{surname\}}@huawei.com}}



\newcommand{\htwogcn}{H\textsubscript{2}GCN}

\maketitle

\begin{abstract}

Graph Neural Networks (GNNs) are well-suited for learning on homophilous graphs, i.e., graphs in which edges tend to connect nodes of the same type. Yet, achievement of consistent GNN performance on heterophilous graphs remains an open research problem. Recent works have proposed extensions to standard GNN architectures to improve performance on heterophilous graphs, trading off model simplicity for prediction accuracy.
However, these models fail to capture basic graph properties, such as neighborhood label distribution, which are fundamental for learning.

In this work, we propose GCN for Heterophily (GCNH), a simple yet effective GNN architecture applicable to both heterophilous and homophilous scenarios.
GCNH learns and combines \textit{separate} representations for a node and its neighbors, using one \textit{learned importance coefficient} per layer to balance the contributions of center nodes and neighborhoods. 
We conduct extensive experiments on eight real-world graphs and a set of synthetic graphs with varying degrees of heterophily to demonstrate how the design choices for GCNH lead to a sizable improvement over a vanilla GCN.
Moreover, GCNH outperforms state-of-the-art models of much higher complexity on four out of eight benchmarks, while producing comparable results on the remaining datasets.
Finally, we discuss and analyze the lower complexity of GCNH, which results in fewer trainable parameters and faster training times than other methods, and show how GCNH mitigates the oversmoothing problem.



\end{abstract}

\begin{IEEEkeywords}
Graph Neural Networks, heterophily, disassortativity, graph representation learning
\end{IEEEkeywords}

\section{Introduction}\label{sec:introduction}

GNNs are core components of current state-of-the-art methods for learning and prediction on graph-structured data across domains and applications. Their capability to encode semantic and contextual information into node embeddings is known to be particularly effective on homophilous graphs, i.e., graphs in which nodes of the same type tend to be connected~\cite{kipf_semi_2017,velickovic_graph_2018}.
On the other hand, in the case of heterophilous graphs, in which neighboring nodes tend to be dissimilar in type, the achievement of competitive, or at least consistent, GNN prediction accuracy remains an open research goal~\cite{zhu_beyond_2020,jin_universal_2021}. 

Among other explanations for the inconsistent GNN performance in the heterophilous case, ~\cite{zheng_graph_2022} suggests that the message aggregation strategies of standard GNNs could lead to weakly representative node embeddings, due to a disproportionately high contribution of dissimilar neighbors.
That is why some works attempt to improve the generalization capability of GNNs to heterophilous networks by either modifying the graph structure or tailoring aggregation strategies and network architecture for such scenario~\cite{abu_mixhop_2019, zhu_beyond_2020, jin_universal_2021, wang_tree_2021,pei_geom-gcn_2019,suresh_breaking_2021,wei_node_2021,he_block_2022,yan_two_2021,bodnar_neural_2022,bo_beyond_2021,du_gbk_2022,yang_diverse_2021,chen_simple_2020,chien_adaptive_2021}.
Nevertheless, \cite{ma_is_2022} shows that a vanilla Graph Convolutional Network (GCN) can still \textit{outperform} more complex (heterophilous) models on some heterophilous graphs.
\cite{ma_is_2022} and~\cite{cavallo_2ncs_2022} suggest that this contradiction originates from the fact that the edge homophily ratio should not be considered a representative indicator of GCN performance, and instead recommend taking into account neighborhood structure and distribution of node labels.

Based on these observations, we propose a simple yet effective GNN architecture, namely GCN for Heterophily (GCNH), to improve node representation capabilities for applications on heterophilous graphs.
Differently from other GNNs, GCNH learns two different functions that encode a node and its neighbors \textit{separately}.
In addition, we allow the GCNH layer to flexibly assign different relevance to the information present in the neighborhood versus the information present in the center node.
We discuss and show how this design mitigates noisy neighborhood representations from negatively influencing the learned embeddings while allowing informative neighbors to strongly influence the final node embedding.
These extensions make GCNH more adaptive to heterophily than a vanilla GCN, while also improving prediction accuracy over more complex models designed for heterophily on common benchmarks.

We evaluate GCNH on the task of node classification using eight common real-world datasets and one set of synthetic graphs with varying degrees of heterophily.
We show that GCNH is able to learn meaningful representations of nodes independently of the homophily level of the graph, achieving new state-of-the-art performance on heterophilous graph datasets while producing results comparable to the state-of-the-art on homophilous benchmarks.

Our main contributions are summarized below.
\begin{itemize}
    \item We present GCNH, a simple yet effective GNN architecture that improves graph representation learning capabilities on heterophilous graphs while preserving the advantages of GCN.
    \item We present extensive experiments on real and synthetic datasets with varying degrees of heterophily for the node classification task. GCNH improves over the state-of-the-art on four (out of eight) real-world benchmarks while performing comparably to the state-of-the-art on all other datasets, including homophilous graphs.
    \item We showcase the lower complexity of GCNH compared to other state-of-the-art models, both in terms of the training time and the number of trainable parameters, and how it mitigates the oversmoothing problem.
\end{itemize}



\section{Related work}

The analysis and improvement of GNN performance on heterophilous graphs have received increasing attention in recent years~\cite{abu_mixhop_2019, zhu_beyond_2020, jin_universal_2021, wang_tree_2021,pei_geom-gcn_2019,suresh_breaking_2021,wei_node_2021,he_block_2022,yan_two_2021,bodnar_neural_2022,bo_beyond_2021,du_gbk_2022,yang_diverse_2021,chen_simple_2020,chien_adaptive_2021}.
GNNs generate node embeddings in two steps: message transformation, where node messages, i.e., features or embeddings from the previous layer, are transformed, and message aggregation, where the final node embeddings are generated by aggregating the messages of neighbors in the graph. One of the earliest and today most common GNN models is the GCN~\cite{kipf_semi_2017}, which defines message transformation as a learnable linear layer and message aggregation as the average of the messages from the neighbors and from the center node.

Several works have proposed new GNN architectures or graph transformations to make the message-passing framework suitable for applications on non-homophilous graphs. We compare a number of these approaches to our method in Section~\ref{sec:experiments}. Borrowing the categorization introduced in~\cite{zheng_graph_2022}, these approaches can be subdivided as follows.
\subsection{Non-Local Neighbor Extension}
These methods selectively extend the receptive field of GNNs to include potentially important nodes located outside of the local neighborhood. These approaches are based on the assumption that neighboring nodes may be dissimilar and that information about the target node, presumably carried by nodes with the same label, is available in nodes belonging to higher-order neighborhoods. In particular, methods such as~\cite{abu_mixhop_2019, zhu_beyond_2020, jin_universal_2021, wang_tree_2021} create latent representations for nodes using multiple neighborhoods at different hop distances and combine them into one overall embedding. Other approaches, such as~\cite{pei_geom-gcn_2019,suresh_breaking_2021,wei_node_2021,he_block_2022}, do not consider entire neighborhoods but single nodes as potential neighbors, regardless of their location in the graph, and aggregate messages from the nodes that are estimated to be more relevant. 
\subsection{GNN Architecture Adaptation}
Methods in this category modify the GNN architecture with the goal of improving representation learning capabilities on heterophilous graphs. A common approach is to estimate the relevance of a node to a given target node with respect to the prediction task at hand, and to assign individual weights to messages from neighbors based on their relevance. This approach is used in~\cite{yan_two_2021,bodnar_neural_2022,bo_beyond_2021,du_gbk_2022,yang_diverse_2021} among the others. Another approach is to learn separate embeddings for the neighborhood and the target node and merge them at a later stage. This mitigates the phenomenon of noisy embeddings, where information from potentially highly dissimilar neighbors is mixed with the features of the target node. Methods that follow this approach include~\cite{zhu_beyond_2020,suresh_breaking_2021}. 
In addition, \cite{zhu_beyond_2020,chen_simple_2020,chien_adaptive_2021} adopt the strategy of treating the information captured by different GNN layers separately, instead of first aggregating the information from all layers and then using the final node embeddings for prediction.


\section{Preliminaries}
\subsection{Notation And Problem Statement}
Let  be an unweighted and undirected graph, where  is the set of nodes and  is the set of edges. The connectivity information in the graph is represented by the adjacency matrix , where  is the number of nodes and matrix elements  are equal to 1 if nodes  and  are adjacent and 0 otherwise. Each node is associated with a feature vector  of size , and the complete set of features in the graph is denoted by . The \textit{neighbors} of a node  are the set of nodes adjacent to it, denoted by . Note that  does not include node .

The task addressed in this work is \textit{supervised node classification}. In this scenario, each node is associated with a label  representing the class the node belongs to, where  is the set of labels. The task corresponds to learning a mapping  that uses the information of the graph , the features  and the labels to map nodes into their ground-truth class.
As is standard practice, we add a linear layer that maps the final node representations of the last network layer  (with  total number of layers and  size of the final node embeddings) to class probabilities with the addition of a softmax. For a node :

where  is a learnable matrix. We omit the bias term from the equation for simplicity.
Note that  is the probability distribution over classes for node .
During inference, networks assign to the node the class with maximum probability:
~
To train the model, we minimize the negative log-likelihood loss on the training data
,  where  is the predicted label for node .
The loss function  describes the distance between the true label  and the predicted label. We focus on the transductive case: we work on single-graph and separate nodes into training, validation and test set.

\subsection{Homophily And Heterophily}
\textit{Graph homophily} is a social science-inspired property of graphs~\cite{mcpherson_birds_2001}, defined as the extent to which similar nodes in a graph are connected. Although other definitions of node similarity exist~\cite{yang_diverse_2021, ma_is_2022, cavallo_2ncs_2022}, this work focuses on label similarity, i.e., we define nodes as similar if they share the same label, and we measure graph homophily using the standard \textit{edge homophily ratio}, denoted by :

which quantifies the fraction of edges in a graph that connect nodes with the same label.
Graphs with low values of \textit{h} are called \textit{heterophilous} or \textit{disassortative}. As discussed in Section~\ref{sec:introduction}, GNNs perform inconsistently on this category of graphs -- we analyze this point in depth in Section~\ref{sec:experiments}.



\begin{figure*}[t]
\centering
\includegraphics[width=0.8\linewidth]{images/gcnh_layer.pdf}
\caption{Architecture of the GCNH layer. To create an updated representation for the center node (red), the GCNH layer explicitly separates the node from its 1-hop neighborhood (blue). Two different MLPs encode center node and neighborhood separately ( and ,  respectively). The encoded neighbors are then aggregated with a permutation invariant function . Finally, the aggregated neighborhood and the encoded center node are combined together with a \textit{learnable} weighting factor  which regulates the contributions of the center node versus its neighborhood to produce the final output embedding for the node.}
\label{fig:gcnh_architecture2}
\end{figure*}


\section{Method}\label{sec:method}
In this section, we describe the architecture of the model proposed in this paper, namely GCNH. Figure~\ref{fig:gcnh_architecture2} illustrates the structure of the GCNH layer.

\subsection{GCNH Layer Formulation}
A GCNH network is composed of one or more, , GCNH layers.
The  layer receives in input the adjacency matrix and the node representations computed at the previous layer   and produces updated node representations . We set .

Within a layer, node representations are transformed through two separate 1-layer MLPs,  and , 
resulting in latent representations  and  of the target node and its neighborhood, respectively.
For a node , at layer , we formally describe this step as follows. First, an intermediate representation is computed for the node :

Secondly, all the representations of 's neighbors () are  updated based on their current representations  and aggregated together to obtain a neighborhood representation:

In Equations~\ref{eq:gcnh_feat_trans1b} and~\ref{eq:gcnh_feat_trans2b},  are learnable matrices,  is a generic activation function,  is the size of the embeddings created at the -th layer and  is a permutation invariant aggregation function over the nodes . We omit the bias terms of the MLPs in the equations for clarity.
The final output embedding for node  is obtained as a linear combination of  and , parametrized by a learnable scalar value :

where  is normalized between 0 and 1 with a sigmoid function.
Note that the MLPs,  and  are different for each layer . We omit superscripts in the equations for clarity.


\begin{table*}[t]
    \footnotesize
    \centering
    \addtolength{\leftskip} {-2cm}
    \addtolength{\rightskip}{-2cm}
\begin{tabular}{ c c c c c c c c c }
    \toprule
    \textbf{Benchmark} & \textbf{Cornell} & \textbf{Texas} & \textbf{Wisconsin} & \textbf{Film} & \textbf{Chameleon} & \textbf{Squirrel} & \textbf{Cora} & \textbf{Citeseer} \\
     & 0.30 & 0.11 & 0.21 & 0.22 & 0.23 & 0.22 & 0.81 & 0.74 \\
    \midrule
    \textbf{MLP} & 81.896.40 & 80.814.75 & 85.293.31 & 36.530.70 & 46.212.99 & 28.771.56 & 75.692.00 & 74.021.90 \\
    \textbf{GCN} & 60.545.30 & 55.145.16 & 51.763.06 & 27.321.10 & 64.822.24 & 53.432.01 & 86.981.27 & 76.501.36 \\
    \textbf{GAT} & 61.895.05 & 52.166.63 & 49.414.09 & 27.440.89 & 60.262.50 & 40.721.55 & 87.301.10 & 76.551.23 \\
    \textbf{Geom-GCN} & 60.543.67 & 66.762.72 & 64.513.66 & 31.591.15 & 60.002.81 & 43.801.48 & 85.351.57 & \textbf{\textcolor{red}{78.021.15}} \\
    \textbf{\htwogcn{}} & 82.705.28 & 84.867.23 & \textbf{\textcolor{violet}{87.654.98}} & 35.701.00 & 60.112.15 & 36.481.86 & \textbf{\textcolor{violet}{87.871.20}} & 77.111.57 \\
    \textbf{GPRGNN} & 80.278.11 & 78.384.36 & 82.944.21 & 34.631.22 & 46.581.71 & 31.611.24 & \textbf{\textcolor{blue}{87.951.18}} & \textbf{\textcolor{violet}{77.131.67}} \\
    \textbf{GGCN} & \textbf{\textcolor{blue}{85.686.63}} & \textbf{\textcolor{violet}{84.864.55}} & 86.863.29 & \textbf{\textcolor{blue}{37.541.56}} & \textbf{\textcolor{blue}{71.141.84}} & \textbf{\textcolor{violet}{55.171.58}} & \textbf{\textcolor{red}{87.951.05}} & \textbf{\textcolor{blue}{77.141.45}} \\
    \textbf{O(d)-SD} & \textbf{\textcolor{violet}{84.864.71}} & \textbf{\textcolor{blue}{85.955.51}} & \textbf{\textcolor{red}{89.414.74}} & \textbf{\textcolor{red}{37.811.15}} & \textbf{\textcolor{violet}{68.041.58}} & \textbf{\textcolor{blue}{56.341.32}} & 86.901.13 & 76.701.57 \\
    \midrule
    \textbf{GCNH} & \textbf{\textcolor{red}{86.496.98}} & \textbf{\textcolor{red}{87.843.87}} & \textbf{\textcolor{blue}{87.653.59}} & \textbf{\textcolor{violet}{36.891.50}} & \textbf{\textcolor{red}{71.561.86}} & \textbf{\textcolor{red}{61.851.54}} & 86.881.04 & 75.811.14 \\
\bottomrule

    \end{tabular}
    \caption{Mean classification accuracy and standard deviation for GCNH on real-world datasets, on the 10 splits taken from~\cite{pei_geom-gcn_2019}. Best results are in \textbf{\textcolor{red}{red}}, second best results in \textbf{\textcolor{blue}{blue}} and third best in \textbf{\textcolor{violet}{violet}}. The results for the baselines are taken from~\cite{yan_two_2021} and~\cite{bodnar_neural_2022}. We use sum as the aggregation function in GCNH; other parameters of GCNH are selected from the best-performing configuration (see Table~\ref{tab:gcnh_hyp} in the Appendix for details on the hyperparameters). }
    \label{tab:main_results}
\end{table*}



\subsection{GCNH Design Choices}
\label{sec:gcnh-design}

GCNH introduces two main design choices which differentiate it from a standard GCN: (i) the separate encoding of the target node and its neighbors (Equations~\ref{eq:gcnh_feat_trans1b}, ~\ref{eq:gcnh_feat_trans2b}) and (ii) the explicit parameterization of the contributions of neighborhood and center node with  (Equation~\ref{eq:gcnh_aggr}). 

The positive impact of separately processing the node and its neighborhood has been previously outlined in~\cite{zhu_beyond_2020,suresh_breaking_2021}.
Intuitively, in homophilous settings, where neighbors are similar, aggregation from the neighborhood brings useful information. On the other hand, in heterophilous graphs, dissimilar neighbors might bring detrimental information that a GNN cannot easily ignore.
Compared to~\cite{zhu_beyond_2020}, GCNH takes the separation principle further, while minimizing the amount of complexity that this choice adds to the network architecture.
Specifically, three choices distinguish GCNH from the model proposed in~\cite{zhu_beyond_2020}: (i) we only use 1-hop neighborhoods, (ii) we learn separate MLPs for center node and neighborhood and (iii) we combine separate embeddings using a learned linear combination instead of concatenation.
Note that (i) leads to faster computation while retaining most of the information useful for node representation, as shown in Section~\ref{sec:experiments}, (ii) improves flexibility and (iii) leads to models with fewer parameters. (i) and (iii) lead to lower time complexity compared to~\cite{zhu_beyond_2020} (see Section~\ref{sec:complexity} for a time complexity analysis).

Combined with the embedding separation, the explicit modeling of the importance of the neighborhood informativeness with the coefficient  in GCNH allows to adaptively determine the impact of the neighborhood on the final node embeddings based on how informative the neighbors are.
Note that informative neighbors might also exist in heterophilous graphs~\cite{ma_is_2022, cavallo_2ncs_2022}; therefore, the contribution of neighbors is not necessarily related to the homophily level of the network.
Intuitively, modeling and learning  explicitly provides a helpful inductive bias that allows the network to directly prefer the center node versus neighborhood information. Section~\ref{sec:experiments} shows how models that are equipped with similar, or better, flexibility (such as GAT~\cite{velickovic_graph_2018}) perform poorly in comparison~\cite{lim_large_2021}.

\subsection{Time Complexity}\label{sec:complexity}
We compute the time complexity of a generic GCNH layer  by analyzing the individual processing steps of the model.

First, the node representations  are transformed, separately for the center node and the neighbors, as described in Equations~\eqref{eq:gcnh_feat_trans1b} and \eqref{eq:gcnh_feat_trans2b}. This step has a time complexity of . Subsequently, the neighbors' representations are aggregated and merged with the center-node embedding according to Equation~\eqref{eq:gcnh_aggr}. All the neighbor aggregation functions tested (see Section \ref{sec:agg_func}) have a time complexity of , whereas the weighted sum with the self-node representation has a time complexity of . This last term is dominated by the complexity of the transformation. In total, the overall time complexity of the GCNH layer is . The linear dependency of the complexity of the GCNH layer on the number of nodes  makes it easier to scale on large graphs compared to attention-based models (e.g., GGCN~\cite{yan_two_2021}), whose complexity depends quadratically on .


\section{Experiments}\label{sec:experiments}
 
In this section, we analyze the learning capability of GCNH on the task of supervised node classification, commonly used for models dealing with heterophilous graphs. We perform the evaluation both on synthetic datasets with different levels of homophily ratio and on the real-world graphs widely used in related works. Further information about the datasets is reported in Appendix \ref{app:datasets}.


\subsection{Baselines}
The baselines used for comparison belong to two main categories. The first category includes well-known methods:
\begin{itemize}
    \item \textbf{MLP}: Multi-Layer Perceptron.
    \item \textbf{GCN} \cite{kipf_semi_2017}: Graph Convolutional Network.
    \item \textbf{GAT} \cite{velickovic_graph_2018}: Graph Attention Network.
\end{itemize}
Methods in the second category, instead, are specifically designed for heterophilous graphs. In particular:
\begin{itemize}
    \item \textbf{Geom-GCN} \cite{pei_geom-gcn_2019} maps nodes to a latent space and defines a new graph based on embedding similarity.
    \item \textbf{H\textsubscript{2}GCN} \cite{zhu_beyond_2020} introduces three specific designs to boost performance on heterophilous graphs: ego and neighbor-embedding separation, higher-order neighborhoods and combination of intermediate representations.
    \item \textbf{GPRGNN} \cite{chien_adaptive_2021} uses PageRank to determine relations between nodes.
    \item \textbf{GGCN} \cite{yan_two_2021} applies two designs to extend GNNs: degree correction and signed messages.
    \item \textbf{O(d)-SD} \cite{bodnar_neural_2022} is based on sheaf diffusion.
\end{itemize}

\subsection{Experimental Setting}
We evaluate model performance in terms of classification accuracy, i.e., the percentage of nodes in the test set that are assigned to the correct class.
We test different values for hyperparameters and we select the best ones. We report further information about the grids for the hyperparameters in Appendix~\ref{app:hyp}. The aggregation function  in Equation~\eqref{eq:gcnh_feat_trans2b} is an element-wise sum unless differently specified (see Section~\ref{sec:agg_func} for a comparison of aggregation functions).
We compute classification accuracies on 10 different train/validation/test splits provided by \cite{pei_geom-gcn_2019} for each dataset, and we report mean and standard deviation. The sizes of the splits are 48\%/32\%/20\%. For each dataset, we train the models on the training sets and the model performing best on average across the validation sets is used on the test sets, on which we compute the accuracy values.

We run experiments on an NVIDIA Tesla V100 PCIE with 16 GB, except for the experiments in Table \ref{tab:training times} that we run on an NVIDIA Tesla T4. The code is implemented in Python and PyTorch is used for deep learning models. Our code is available at \url{https://github.com/SmartData-Polito/GCNH}.


\subsection{Results On Real-World Datasets}
Table~\ref{tab:main_results} reports classification accuracies for GCNH and baselines on real-world datasets. 
GCNH achieves state-of-the-art performance on four out of the eight datasets used; it ranks second and third on Wisconsin and Film and performs slightly worse on the two homophilous graphs Cora and Citeseer.
The results on all the heterophilous graphs prove the effectiveness of the design choices of GCNH. 
On Cornell, Texas, Wisconsin and Film, GCNH avoids the detrimental influence of neighbors on the final embeddings observed, for example, in GCN and GAT, with respect to which improvements are large (up to 35\%).
On Chameleon and Squirrel, GCNH manages to encode the useful neighborhood information present in the graph thanks to its separate processing of node and 1-hop neighbors. Indeed, on these two graphs, GCN performs quite well (it ranks fourth on both), meaning that information contained in the 1-hop neighborhood is useful for node classification, whereas more complex models (e.g. Geom-GCN, \htwogcn{} and GPRGNN) fail to capture this property and perform worse (from 5 to 20\% accuracy drop with respect to GCN). 
On the homophilous graphs Cora and Citeseer, GCNH performs comparably to the other models, although slightly worse. 
These results show that GCNH flexibly adapts to various homophily settings, achieving consistent performance.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/gcnh_syn.pdf}
\caption{Comparison of the classification accuracy achieved by a set of models on the syn-cora synthetic datasets. Each point shows the average accuracy on three datasets generated with a specific homophily ratio.}
\label{fig:gcnh_syn}
\end{figure}

\subsection{Results On Synthetic Datasets}

To better understand how GCNH deals with different levels of homophily, we evaluate it on synthetic graphs covering a wide range of values of , while keeping node features and other graph properties unchanged. 
We provide additional details about the dataset used in Appendix~\ref{app:dataset:syn}.

Figure~\ref{fig:gcnh_syn} shows the accuracy of GCNH and three other simple baselines: MLP, whose performance is not affected by the value of  as it is a graph-unaware method, and GCN and GAT, which achieve significantly worse results on heterophilous settings.
The hyperparameter configurations for these baselines are reported in Appendix~\ref{app:baselines_syn_hyp}. 
GCNH's performance is significantly less dependent on the homophily level of the graph than the performances of GAT and GCN. 
On heterophilous graphs, GCNH improves by almost 50\% over GCN and GAT and by 2/8\% over MLP, meaning that the separate encoding of the neighbors has a positive impact. 
On homophilous graphs, GCNH performs comparably to GCN and GAT, achieving perfect accuracy on perfectly homophilous graphs ().

\begin{table*}[t]
    \footnotesize
    \centering
    \addtolength{\leftskip} {-2cm}
    \addtolength{\rightskip}{-2cm}
    \setlength{\tabcolsep}{0.44em}
\begin{tabular}{ c c c | c c c c c c c c }
    \toprule
    & Separate MLPs & Learned   &\textbf{Cornell} & \textbf{Texas} & \textbf{Wisconsin} & \textbf{Film} & \textbf{Chameleon} & \textbf{Squirrel} & \textbf{Cora} & \textbf{Citeseer} \\
\midrule
\textbf{GCN} & \xmark & \xmark & 60.54 & 55.14 & 51.76 & 27.32 & 64.82 & 53.43 & \textbf{86.98} & \textbf{76.50} \\\midrule 
\textbf{GCNH} & \cmark & \xmark & 83.78(\gcnhgreen{23.2}) & 86.49(\gcnhgreen{31.3}) & 85.49(\gcnhgreen{33.7}) & 36.01(\gcnhgreen{8.7}) & 70.22(\gcnhgreen{5.4}) & 59.74(\gcnhgreen{6.3}) & 86.90(\gcnhred{0.1}) & 75.65(\gcnhred{0.8}) \\ 
\textbf{GCNH} & \cmark & \cmark & \textbf{86.49}(\gcnhgreen{25.9}) & \textbf{87.84}(\gcnhgreen{32.7}) & \textbf{87.65}(\gcnhgreen{35.9}) & \textbf{36.89}(\gcnhgreen{9.6}) & \textbf{71.56}(\gcnhgreen{6.7}) & \textbf{61.85}(\gcnhgreen{8.4}) & 86.88(\gcnhred{0.1}) & 75.81(\gcnhred{0.7}) \\  
    \bottomrule
    \end{tabular}
    \caption{Mean classification accuracy of GCN and ablated GCNH. In parentheses, we report the performance \gcnhgreen{improvement} or \gcnhred{degradation} from the GCN baseline. ``Separate MLPs'' refers to whether we learn separate linear layers for node and neighborhoods ( in Equation~\ref{eq:gcnh_feat_trans1b} and~\ref{eq:gcnh_feat_trans2b}, respectively). ``Learned '' refers to whether we learn  as a parameter or not; if not, it is fixed at =0.5. Best results are in \textbf{bold}.}
    \label{tab:ablation_exp}
    
\end{table*}



\begin{table*}[t]
    \footnotesize
    \centering
    \addtolength{\leftskip} {-2cm}
    \addtolength{\rightskip}{-2cm}
\begin{tabular}{ c c c c c c c c c }
    \toprule
    \textbf{Benchmark} & \textbf{Cornell} & \textbf{Texas} & \textbf{Wisconsin} & \textbf{Film} & \textbf{Chameleon} & \textbf{Squirrel} & \textbf{Cora} & \textbf{Citeseer} \\
\midrule
    \textbf{GCNH} (sum) & 86.496.98 & 87.843.87 & 87.653.59 & \textbf{36.891.50} & 71.561.86 & 61.851.54 & \textbf{86.881.04} & 75.811.14 \\
    \textbf{GCNH} (mean) & \textbf{86.494.41} & \textbf{88.923.24} & \textbf{89.805.29} & 36.831.44 & 55.641.95 & 39.241.02 & 85.751.57 & \textbf{76.000.99} \\
\textbf{GCNH} (max) & 85.418.18 & 88.654.56 & 88.043.26 & 	36.071.26 & \textbf{71.641.85} & \textbf{63.351.99} & 86.681.10 & 75.961.33 \\
    \bottomrule

    \end{tabular}
    \caption{Mean classification accuracy and standard deviation for GCNH with different aggregation functions in Equation~\ref{eq:gcnh_feat_trans2b}. Note that results for ``max'' are obtained full-batch training only (see Table~\ref{tab:gcnh_hyp} and discussion in Section~\ref{sec:training_times}), as it allows for more efficient implementation. Best results are in \textbf{bold}.}
    \label{tab:aggfunc_exp}
\end{table*}



\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/gcnh_beta_analysis.pdf}
\caption{Values of learned parameter  for GCNH on real and synthetic datasets. Results are obtained using the best-performing hyperparameter configuration for GCNH models with one layer.}
\label{fig:gcnh_beta_analysis}
\end{figure}

\subsection{Analysis of the GCNH Design}\label{sec:experiments-analysis}
To measure the impact of the design choices we made for GCNH, we perform an ablation study by removing the two main components of GCNH, namely, the separate MLPs and the learned importance coefficient , to see how they affect the performance.
We report the results in Table~\ref{tab:ablation_exp}, where we use a standard GCN as a baseline.
The table shows how both components bring an improvement in classification accuracy on heterophilous graphs, with a small tradeoff on the performance on homophilous graphs.
The MLPs separation leads to the largest gain, and its combination with the  coefficient consistently brings further improvements.


We analyze further the behavior of .
Since  balances the contribution of messages of self-node and its neighbors, we expect heterophilous graphs to lead to larger s given that neighborhoods are generally less informative in those cases.
Figure~\ref{fig:gcnh_beta_analysis} shows the values of the parameter  on different graphs.
On synthetic graphs,  is evidently correlated with the edge homophily ratio measured by .
On most real graphs, the values follow a trend similar to that observed in synthetic graphs.
Chameleon and Squirrel are exceptions: we find that, while these datasets are heterophilous according to the  metric, GCNH tends to learn low values of , corresponding to highly informative neighborhoods.
This corroborates the findings outlined in~\cite{ma_is_2022}, which pointed out a similar contradiction: how a standard GCN performs unexpectedly well on the heterophilous Chameleon and Squirrel, even outperforming heterophily-specific methods.
In fact, this result further suggests that edge homophily ratio is not suited to describe neighborhood informativeness in general.


\subsection{Aggregation Functions}
\label{sec:agg_func}
We test three different aggregation functions for  in Equation~\eqref{eq:gcnh_feat_trans2b}: element-wise sum, element-wise mean across the neighbors and element-wise max. 
Table~\ref{tab:aggfunc_exp} reports a comparison of the node classification results for these aggregation functions.


\begin{table}
    \centering
    \footnotesize
    \addtolength{\leftskip} {-2cm}
    \addtolength{\rightskip}{-2cm}
    \setlength{\tabcolsep}{0.3em}
    \begin{tabular}{ c c | c c c c c}
    \toprule
     & \textbf{N. Params} & \textbf{Cora} & \textbf{Citeseer} & \textbf{Chameleon} & \textbf{Squirrel} & \textbf{Film} \\
    \midrule
\textbf{GGCN} & 118~k & 96.05 & 99.94 & 74.83 & 331.64 & 628.36  \\
    \textbf{O(d)-SD} & 46~k & 19.64 & 20.15 & 48.87 & 275.90 & 44.22 \\
    \midrule
    \textbf{GCNH} (sum) & 30~k & \textbf{8.79} & \textbf{10.28} & \textbf{8.56} & \textbf{12.61} & 15.59 \\
\textbf{GCNH} (max) & 30~k & 11.26 & 13.19 & 10.71 & 17.20 & \textbf{12.82}\\
    \bottomrule
    
    \end{tabular}
    \caption{Number of trainable parameters and training times (sec) for two state-of-the-art methods and GCNH on several graphs. Training is performed for 200 epochs on 10 splits for each dataset. Models have one layer and hidden size 16. Shortest times for each dataset are in \textbf{bold}.}
    \label{tab:training times}
\end{table}




\subsection{GCNH And Oversmoothing}
\label{sec:oversmoothing}
The performance of GNNs is known to gradually decrease when increasing the number of layers. This decay is partly attributed to oversmoothing, i.e., repeated graph convolutions eventually making node embeddings indistinguishable from each other~\cite{li_insights_2018, oono_graph_2020}.
We show experimentally that the design choices of GCNH  alleviate the oversmoothing problem. As shown in Figure~\ref{fig:oversmoothing}, GCNH's accuracy decreases just slightly when increasing the number of layers, whereas increasing the layers of GCN leads to a larger drop in performance; see Section~\ref{app:sec:oversmoothing} in Appendix for the experimental details.

\subsection{Training Times And Trainable Parameters}
\label{sec:training_times}
We report in Table~\ref{tab:training times} the training times required by GCNH and two of its main competitors GGCN~\cite{yan_two_2021} and O(d)-SD~\cite{bodnar_neural_2022}, as well as their number of trainable parameters. 
We take the implementations of these methods from the repositories of the authors~\cite{yujun-yan-code,neural-sheaf-diffusion-code}.
We report the result for GCNH using sum and max aggregation. We omit results for mean since sum and mean require the same amount of computation as they can be implemented efficiently with a matrix multiplication, i.e., their training times are the same. Max, instead, requires a scattered max pooling operation for which we use the implementation in~\cite{pytorch-scatter}.
GCNH is noticeably faster and has fewer trainable parameters than both GGCN and O(d)-SD.


\begin{figure}
\begin{subfigure}{.24\textwidth}

    \includegraphics[width=\textwidth]{images/oversmoothing_cora.pdf}
    \caption{Cora}
    \label{fig:oversmoothing_cora}
    \end{subfigure}
\hfill
\begin{subfigure}{.24\textwidth}
    \includegraphics[width=\textwidth]{images/oversmoothing_citeseer.pdf}
    \caption{Citeseer}
    \label{fig:oversmoothing_citeseer}
\end{subfigure}


\caption{Accuracy for GCNH and GCN with different numbers of layers. With a large number of layers, GCNH prevents oversmoothing.}
\label{fig:oversmoothing}
\end{figure}

\section{Conclusions}
We introduced GCNH, a simple yet effective GNN architecture that improves representation capabilities on heterophilous graphs. 
GCNH leverages two design choices: (i) two distinct learnable mapping functions that separately encode the center node and its neighbors into intermediate representations and (ii) the importance coefficients , one per layer, to balance the contributions of these two representations on the final node embeddings.
We analyze and demonstrate how these two components enhance the representation capabilities of GCNH compared to GCN, as, together, they reduce the detrimental contributions of noisy neighborhoods to the updated node representations, while making effective use of basic structural information beneficial for classification. 
Through extensive experiments on real and synthetic graphs, we show that GCNH performs competitively on the node classification task, outperforming state-of-the-art methods on four out of eight common real-world datasets.  
In addition to its effectiveness, GCNH's simple design results in significantly faster training times and fewer trainable parameters compared to competing methods.



\begin{thebibliography}{00}

\bibitem{kipf_semi_2017} Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Networks. In: \textit{International Conference on Learning Representations (ICLR)}, 2017.
\bibitem{velickovic_graph_2018} Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph Attention Networks. In: \textit{International Conference on Learning Representations (ICLR)}, 2018.
\bibitem{zhu_beyond_2020} Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs. In: \textit{Advances in Neural Information Processing Systems (NeurIPS)}, 2020.
\bibitem{jin_universal_2021}Di Jin, Zhizhi Yu, Cuiying Huo, Rui Wang, Xiao Wang, Dongxiao He, and Jiawei Han. Universal Graph Convolutional Networks. In: \textit{Advances in Neural Information Processing Systems (NeurIPS)}, 2021.
\bibitem{zheng_graph_2022} Xin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S. Yu. Graph Neural Networks for Graphs with Heterophily: A Survey. In: \textit{arXiv:2202.07082}, 2022
\bibitem{pei_geom-gcn_2019} Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-GCN: Geometric Graph Convolutional Networks. In: \textit{International Conference on Learning Representations (ICLR)}, 2020.
\bibitem{wang_tree_2021}Yu Wang and Tyler Derr. Tree Decomposed Graph Neural Network. In: \textit{Proceedings of the 30th ACM International Conference on Information and Knowledge Management}, 2021.
\bibitem{wei_node_2021} Wei Jin, Tyler Derr, Yiqi Wang, Yao Ma, Zitao Liu, and Jiliang Tang. Node Similarity Preserving Graph Convolutional Networks. In: \textit{Proceedings of the 14th ACM International Conference on Web Search and Data Mining}, 2021.
\bibitem{bo_beyond_2021} Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond Low-frequency Information in Graph Convolutional Networks. In: \textit{Conference on Artificial Intelligence (AAAI)}, 2021.
\bibitem{chen_simple_2020}Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, Yaliang Li. Simple and Deep Graph Convolutional Networks. In: \textit{International Conference on Machine Learning (ICML)}, 2020
\bibitem{du_gbk_2022} Lun Du, Xiaozhou Shi, Qiang Fu, Xiaojun Ma, Hengyu Liu, Shi Han, and Dongmei Zhang. 2022. GBK-GNN: Gated Bi-Kernel Graph Neural Networks for Modeling Both Homophily and Heterophily. In: \textit{Proceedings of the ACM Web Conference 2022 (WWW '22)}, 2022
\bibitem{yan_two_2021} Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra. Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks. In: \textit{International Conference on Data Mining (ICDM)}, 2022.
\bibitem{bodnar_neural_2022} Cristian Bodnar, Francesco Di Giovanni, Benjamin Paul Chamberlain, Pietro Lio, and Michael M. Bronstein. Neural Sheaf Diffusion: A Topological Perspective on Heterophily and Oversmoothing in GNNs. In: \textit{ICLR 2022 Workshop on Geometrical and Topological Representation Learning}, 2022. 
\bibitem{chien_adaptive_2021} Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive Universal Generalized PageRank Graph Neural Network. In: \textit{International Conference on Learning Representations (ICLR)}, 2021. 
\bibitem{abu_mixhop_2019}Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing. In: \textit{International Conference on Machine Learning (ICML)}, 2019.
\bibitem{he_block_2022}Dongxiao He, Chundong Liang, Huixin Liu, Mingxiang Wen, Pengfei Jiao, and Zhiyong Feng. Block modeling-guided graph convolutional neural networks. In: \textit{Conference on Artificial Intelligence (AAAI)}, 2022.
\bibitem{yang_diverse_2021}Liang Yang, Mengzhe Li, Liyang Liu, Bingxin Niu, Chuan Wang, Xiaochun Cao, and Yuanfang Guo. Diverse Message Passing for Attribute with Heterophily. In: \textit{Advances in Neural Information Processing Systems (NeurIPS)}, 2021.
\bibitem{suresh_breaking_2021}Susheel Suresh, Vinith Budde, Jennifer Neville, Pan Li, and Jianzhu Ma. Breaking the Limit of Graph Neural Networks by Improving the Assortativity of Graphs with Local Mixing Patterns. In: \textit{ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, 2021.
\bibitem{ma_is_2022}Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is Homophily a Necessity for Graph Neural Networks? In: \textit{International Conference on Learning Representations (ICLR)}, 2022.

\bibitem{cavallo_2ncs_2022} Andrea Cavallo, Claas Grohnfeldt, Michele Russo, Giulio Lovisotto, and Luca Vassio. 2-Hop Neighbor Class Similarity (2NCS): A Graph Structural Metric Indicative of Graph Neural Network Performance. In: \textit{arXiv:2212.13202}, 2022.
\bibitem{mcpherson_birds_2001}Miller Mcpherson, Lynn Smith-Lovin, and James Cook. Birds of a Feather: Homophily in Social Networks. In: \textit{Annual Review of Sociology 27}, 2001.
\bibitem{lim_large_2021}Derek Lim, Felix Matthew Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Prasad Bhalerao, and Ser-Nam Lim. Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods. In: \textit{Advances in Neural Information Processing Systems (NeurIPS)}, 2021.
\bibitem{oono_graph_2020}Kenta Oono  and Taiji Suzuki. Graph Neural Networks Exponentially Lose Expressive Power for Node Classification. In: \textit{International Conference on Learning Representations (ICLR)}, 2020.
\bibitem{li_insights_2018} Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper Insights Into Graph Convolutional Networks for Semi-Supervised Learning. In: \textit{Conference on Artificial Intelligence (AAAI)}, 2018.
\bibitem{yujun-yan-code} Yujun Yan. Public code for ``Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks'', [Online]: \url{https://github.com/yujun-yan/heterophily\_and\_oversmoothing}, Accessed on 03-02-2023.
\bibitem{neural-sheaf-diffusion-code} Twitter Research. Public code for ``Neural Sheaf Diffusion'', [Online]: \url{https://github.com/twitter-research/neural-sheaf-diffusion}, Accessed on 03-02-2023.
\bibitem{pytorch-scatter} Matthias Fey et al. PyTorch Scatter, [Online]: \url{https://github.com/rusty1s/pytorch\_scatter}, Accessed on 03-02-2023.
\bibitem{simplifying-code} Sunil Kumar Maurya. Public code for ``Improving Graph Neural Networks with Simple Architecture Design'', [Online]: \url{https://github.com/sunilkmaurya/FSGNN}, Accessed on 03-02-2023.
\bibitem{geom-gcn-code} Graph-structured Data Mining and Machine Learning at University of Illinois at Urbana-Champaign (UIUC) and Jilin University (JLU). Public code for ``Geom-GCN: Geometric Graph Convolutional Networks'', [Online]: \url{https://github.com/graphdml-uiuc-jlu/geom-gcn}, Accessed on 03-02-2023.
\bibitem{webkbproject}Carnegie Mellow University, CMU World Wide Knowledge Base, [Online]: \url{http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/}, Accessed on 03-02-2023.
\bibitem{tang_social_2009}Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social Influence Analysis in Large-Scale Networks. In: \textit{ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD}, 2009.
\bibitem{platonov2023critical}Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko and Liudmila Prokhorenkova. A critical look at evaluation of GNNs under heterophily: Are we really making progress? In: \textit{International Conference on Learning Representations (ICLR)} (2023).
\bibitem{namata_query_2012}Galileo Mark Namata, Ben London, Lise Getoor, and Bert Huang. Query-driven Active Surveying for Collective Classification. In: \textit{Workshop on Mining and Learning with Graphs}, 2012.
\bibitem{sen_collective_2008}Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective Classification in Network Data. In: \textit{AI Magazine 29.3}, 2008.
\end{thebibliography}

\appendices

\vspace{6cm}

\begin{table*}[t]
    \centering
    \footnotesize
    \addtolength{\leftskip} {-2cm}
    \addtolength{\rightskip}{-2cm}
\begin{tabular}{c c c c c c c c c c }
    \toprule
    \textbf{Benchmark} & \textbf{Cornell} & \textbf{Texas} & \textbf{Wisconsin} & \textbf{Film} & \textbf{Chameleon} & \textbf{Squirrel} & \textbf{Cora} & \textbf{Citeseer} & \textbf{syn-cora} \\
    \midrule
    \textbf{\#Nodes} & 183 & 183 & 251 & 7,600 & 2,277 & 5,201 & 2,708 & 3,327 & 1490 \\
    \textbf{\#Edges} & 280 & 295 & 466 & 26,752 & 31,421 & 198,493 & 1,433 & 3,703 & 2965 to 2968 \\
    \textbf{\#Classes} & 5 & 5 & 5 & 5 & 5 & 5 & 7 & 6 & 5 \\
    \textbf{\#Features} & 1,703 & 1,703 & 1,703 & 931 & 2,325 & 2,089 & 1,433 & 3,703 & 1,433 \\
    \textbf{Homophily } & 0.30 & 0.11 & 0.21 & 0.22 & 0.23 & 0.22 & 0.81 & 0.74 & [0, 0.1, ..., 1] \\
    \bottomrule

    \end{tabular}
    \caption{Statistics of real-world and synthetic datasets.}
    \label{tab:data_stat}

\end{table*}

\begin{table*}[t]
    \centering
    \footnotesize
    \addtolength{\leftskip} {-1cm}
    \addtolength{\rightskip}{-1cm}
    \begin{tabular}{ c c c c c c c }
    \toprule
    \textbf{Dataset} & \textbf{\#Layers} & \textbf{Batch size} & \textbf{\#Epochs} & \textbf{Hidden size} & \textbf{Dropout rate} &  \\
    \midrule
    \textbf{Cornell} & \{\textbf{1}, 2, 3\} & \{\textbf{50}, \} & \{100, 200, \textbf{300}\} & \{\textbf{16}, 32, 64\} & \{0.0, \textbf{0.25}, 0.5\} & 0.75 \\
    \textbf{Texas} & \{\textbf{1}, 2, 3\} & \{50, \} & \{100, 200, \textbf{300}\} & \{16, \textbf{32}, 64\} & \{0.0, \textbf{0.25}, 0.5\} & 0.62 \\
    \textbf{Wisconsin} & \{1, \textbf{2}, 3\} & \{\textbf{50}, \} & \{100, 200, \textbf{300}\} & \{16, \textbf{32}, 64\} & \{0.0, \textbf{0.3}, 0.6\} & 0.70, 0.69\\
    \textbf{Film} & \{1, \textbf{2}, 3\} & \{300, \textbf{500}, \} & \{\textbf{150}\} & \{16, \textbf{32}, 64\} & \{0.0, 0.3, \textbf{0.6}\} & 0.72, 0.75 \\
    \textbf{Chameleon} & \{\textbf{1}, 2, 3\} & \{\textbf{300}, 600, \} & \{500, \textbf{1000}, 1500\} & \{16, \textbf{32}, 64\} & \{\textbf{0.0}, 0.25, 0.5\} & 0.12 \\
    \textbf{Squirrel} & \{\textbf{1}, 2, 3\} & \{300, \textbf{1400}, \} & \{500, 1000, \textbf{1500}\} & \{16, \textbf{32}, 64\} & \{\textbf{0.0}, 0.25, 0.5\} & 0.35 \\
    \textbf{Cora} & \{1, \textbf{2}, 3\} & \{\textbf{150}, 300, \} & \{\textbf{300}\} & \{16, 32, \textbf{64}\} & \{0.0, 0.25, 0.5, \textbf{0.75}\} & 0.69, 0.61 \\
    \textbf{Citeseer} & \{\textbf{1}, 2, 3\} & \{150, \textbf{300}, \} & \{\textbf{300}\} & \{\textbf{16}, 32, 64\} & \{0.0, \textbf{0.25}, 0.5, 0.75\} & 0.59 \\
    \textbf{syn-cora} & \{1, 2, 3\} & \{300, \} & \{300\} & \{16, 32\} & \{0.0, 0.25, 0.5\} \\
    \bottomrule
    
    \end{tabular}
    \caption{Hyperparameters and learned values of  for GCNH. For each dataset, the best hyperparameters are in \textbf{bold}. For 2-layer models, the values of  are in the order . For syn-cora, the best hyperparameters and the values of  are not reported as they depend on the homophily value of the graph.} 
    \label{tab:gcnh_hyp}
\end{table*}

\newpage
\section{Datasets}
\label{app:datasets}

This section and Table~\ref{tab:data_stat} report details about the datasets.

\subsection{Real-World Datasets}
\label{sec:real_world_datasets}
These datasets are commonly used in most of the works dealing with heterophilous graphs.
Chameleon and Squirrel are taken from~\cite{simplifying-code}, the others from~\cite{geom-gcn-code}.
We present further details in the following:
\begin{itemize}[leftmargin=.5cm]
    \item \textbf{Texas, Wisconsin and Cornell} are webpage datasets collected from different universities~\cite{webkbproject}. Nodes represent web pages and edges are hyperlinks between them. \item \textbf{Film} (\textbf{Actor}), is the actor-only induced subgraph of the film-director-actor-writer network \cite{tang_social_2009}. Nodes correspond to Wikipedia pages of actors and edges denote the co-occurrence of two actors on the same page. \item \textbf{Chameleon and Squirrel} are Wikipedia pages on the specific topics of chameleons and squirrels. Nodes are Wikipedia pages and edges are mutual links between them. 
\textit{Note:} contemporary to this work,~\cite{platonov2023critical} highlights drawbacks of these two datasets.
    \item \textbf{Cora and Citeseer} are citation networks where nodes represent papers and edges represent citations of one paper by another \cite{sen_collective_2008, namata_query_2012}. These datasets are treated as undirected.
\end{itemize}

\subsection{Synthetic Datasets}
\label{app:dataset:syn}
The synthetic datasets used in this work are taken from \cite{zhu_beyond_2020}, which defines a graph generation strategy similar to \cite{abu_mixhop_2019}. The homophily ratio of the graph is defined \textit{a priori}. Then, the graph is generated such that the degree distribution follows a power law. Classes are assigned randomly and features are sampled from nodes of the corresponding class in the real graph Cora. For each of the specified homophily levels, three different graphs are generated.

\section{Hyperparameters And Experimental Details}
\label{app:hyp}
We report here additional details about the experiments in this work. All models are trained with Adam optimizer, learning rate  and weight decay . 
\subsection{Baselines On Synthetic Datasets}
\label{app:baselines_syn_hyp}
We obtain the results for MLP, GCN and GAT on the synthetic graphs in Figure \ref{fig:gcnh_syn} through hyperparameter optimization among the following values: 100 epochs, \{1,2,3\} layers, hidden size \{16,32\}. For all experiments on the synthetic datasets, we randomly generate train/evaluation/test splits with sizes 50\%/20\%/30\%. We compute results on a single split for each dataset and we average the results of three different graphs for each level of homophily ratio reported in the figure.
\subsection{GCNH}
\label{app:gcnh_hyp}
The hyperparameters for GCNH on real and synthetic graphs are optimized among the values reported in Table \ref{tab:gcnh_hyp}. The activation function  in Equations \ref{eq:gcnh_feat_trans1b} and \ref{eq:gcnh_feat_trans2b} is LeakyReLU. We perform batching by forwarding, for each batch, the complete graph through the model and computing the loss only on the nodes in the current batch.

\subsection{Experiments On Oversmoothing}\label{app:sec:oversmoothing}
The hyperparameters of the models in Section~\ref{sec:oversmoothing} are optimized among the following values: batch size 300, epochs \{300,500\}, hidden size \{16,32,64\}, dropout rate \{0.0,0.5\}. Note that the results for GCN in Table \ref{tab:main_results} are taken from~\cite{yan_two_2021}, while those in Figure \ref{fig:oversmoothing} are obtained from our own experiments, and they slightly differ because the hyperparameter grid search is different.

\end{document}
