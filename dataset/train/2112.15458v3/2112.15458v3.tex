\documentclass[11pt]{article}

\usepackage[sort]{natbib}
\usepackage{fancyhdr}
\usepackage{url}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{color}
\usepackage{amsmath,amssymb,amsfonts,mathrsfs}
\usepackage{url}
\usepackage{lipsum}

\usepackage[capitalize]{cleveref}



\usepackage{booktabs, multirow} \usepackage{soul}\usepackage[table]{xcolor} 

\oddsidemargin 0.2cm
\topmargin -1.0cm
\textheight 23.0cm
\textwidth 16cm
\parindent=0pt
\parskip 1ex
\renewcommand{\baselinestretch}{1.1}
\pagestyle{fancy}


\newcommand{\hamid}[1]{\textcolor{orange}{Hamid: #1}\PackageWarning{Hamid:}{#1!}}
\newcommand{\tho}[1]{\textcolor{black}{#1}}



\lhead{\normalsize \textrm{Response Letter (IEEE RA-L 22-2468)}}
\chead{}
\rhead{\normalsize November 2022}
\lfoot{\normalsize \textrm{}}
\cfoot{\thepage}
\rfoot{}
\setlength{\fboxrule}{4pt}\setlength{\fboxsep}{2ex}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\newcommand{\fw}{\boldsymbol{\beta}}
\newcommand{\sw}{\boldsymbol{\alpha}}
\newcommand{\bias}{\beta_0}
\newcommand{\wbias}{\widehat{\beta}_0}	
\newcommand{\beq}{}
\newcommand{\beqa}{}
\newcommand{\eqreff}[1]{Eq.~(\ref{#1})}
\newcommand{\Lipc}{\mathcal{L}}
\newcommand{\tfw}{\tilde{\boldsymbol{\beta}}}
\def\bb{b}
\def\kk{s}
\newcommand{\numf}{\tau}
\def\tgamma{\tilde{\boldsymbol{\gamma}}}
\def\bgamma{{\boldsymbol{\gamma}}}
\newcommand{\tbias}{\tilde{\beta}_0}
\def\ie{\emph{i.e}.,~} \def\Ie{\emph{I.e}.,~}
\def\Argmin{{\rm Argmin}} 
\def\argmin{{\rm argmin}} 
\def\RR{\mathbb{R}}
\def\eps{{\epsilon}}
\newcommand{\swi}{\alpha_i}
\def\xxi{{\bf x}_i}
\def\yyi{\yy_i}
\newcommand{\yy}{{\rm{y}}}



\usepackage{quoting}
\newcommand{\tquote}[1]{{\color{black}\begin{quote}``#1''\end{quote}}}
\newcommand{\equote}[1]{\tquote{\qting{#1}}}
\newcommand{\todo}[1]{{\color{blue}\{ToDo: #1\}}}
\begin{document}




\setcitestyle{square}
{\Large
\textbf{Submission ID:} IEEE RA-L 22-2468

\textbf{Accurate and Real-time 3D Pedestrian Detection Using an Efficient Attentive Pillar Network}
}


\vspace{15pt}
We appreciate the valuable comments and feedback from the associate editor and the reviewers. Based on the comments, we have revised the manuscript. Point-to-point responses to the reviewers' comments and the modifications in the manuscript are listed in the following. We also provide a version of the revised manuscript, in which all the modified text are typeset in \textcolor{blue}{blue} in the summary of changes attachment.

\vspace{20pt}
\underline{\textbf{Response to Comments from Associate Editor:}}

\textcolor{red}{Reviewers appreciate the contributions. However, the weaknesses
mentioned, especially the clarifications on the notations and the lack
of references and comparisons to the prior work, need to be addressed
before publication. These changes are essential for the paper to be
self-sufficient and more comprehensive in its presentation.}

\textbf{Response:} We thank the editor for the positive feedback. In this revised version of the manuscript, we addressed all comments raised by the reviewers. Specifically:

\begin{itemize}
    \item We included the requested recent papers and provided a comparison between our framework and these papers.
    \item We revised the formulas and figures for better consistency and comprehension.
    \item We performed more analyses on the performance of our framework and fixed the writing issues pointed out by the reviewers.
    
\end{itemize}


\vspace{20pt}
\underline{\textbf{Response to Comments from Reviewer \#1:}}

\textcolor{red}{\textbf{Summary:} The overall architecture of the proposed network is based on PointPillar (and
partially TANet) but the authors introduced several nice modifications to
mitigate pedestrian detection issues, including pillar-aware attention
modules and a mini-BiFPN module.
The authors bring point-wise, channel-wise attention and task-aware attention
modules in the conversion process from 3D point cloud to refined features.
The effect of average pooling on medium/hard cases is out of my expectation.
Finally, the method was validated in three popular datasets. The novelty of
the paper is fair but the experiment conducted is quite comprehensive.}

We thank the reviewer for the positive feedback on the idea of our paper. We carefully addressed the comments. Please see the responses below.

\textcolor{red}{Fig.1 is a bit hazy (Your arxiv version is slightly better). Please try to
use a sharper version.} 

\textbf{Response:} Thank you for pointing this out. We replaced the image in Fig. 1 with a higher-quality version. Additionally, as requested by R2, we included EQ-PVRCNN\cite{EQPVRCNN} (published in CVPR 2022) in this figure for comparison.

\textcolor{red}{In section 3. C. I suggest the author adds an additional detailed description of each module. Without Fig.4, it is really hard to understand at first what
they are and for what purpose they are designed, just like the first sentence
of task-aware attention tells me that it is similar to SENet.} 

\textbf{Response:} Fig.4 and relevant descriptions has been modified for better comprehension. Also, we included more details in Section III.C (Task-aware attention). 

\textcolor{red}{Furthermore, the use of two PAA modules seems to follow TANet. Have you tried other numbers of PAA modules? I am also not clear why 18 channels increased
to 64 finally. Could you elaborate on that?} 

\textbf{Response:} We include Table 1 below, containing new ablation results on how different numbers of stacked PAA modules affect the model's performance. Interestingly, when we stack three PAA modules or more, the performance improvement rate decelerates, \ie mAP does not considerably increase as in the cases of stacking one and two PAA modules. Although stacking more PAA modules yields the best mAP output, the inference fps deteriorates as a consequence, as shown in this table. As a result, to balance the accuracy and speed, we use two PAA modules in other experiments. This new analysis is added to the revised manuscript in Section V. (Number of stacked PAA modules).
\begin{table}[!h]\centering
\scriptsize
\begin{tabular}{lcccccc}\toprule
\multirow{2}{*}{\textbf{Modules}} &\multicolumn{5}{c}{\textbf{Pedestrian}} \\\cmidrule{2-6}
&\textbf{Easy} &\textbf{Moderate} &\textbf{Hard} &\textbf{mAP} &\textbf{FPS} \\\cmidrule{1-6}
\textbf{Baseline} &66.7 &61.1 &56.5 &61.4 &41.5 \\
\textbf{+ 1 PAA} &68.29 &62.36 &56.64 &62.43 &34.2 \\
\textbf{+ 2 PAA} &69.70 &64.02 &58.74 &64.2 &27.4 \\
\textbf{+ 3 PAA} &\textbf{70.37} &\textbf{64.35} &\textbf{58.92} &\textbf{64.55} &19.3 \\\cmidrule{1-6}
\end{tabular}
\caption{Ablation study on the number of stacked PAA modules}\label{tab:stackPAA_analysis}
\vspace{-2em}
\end{table} \vspace{+1em}


\textcolor{red}{In section 3.D, I am a bit confused about your contribution. It is the
selection and modification of BiFPN which can fuse multi-level features both
well and fast, right?} 

\textbf{Response:} To better clarify our contribution on Mini-BiFPN, \tho{We added more details regarding our contribution on Mini-BiFPN in Section III. D.}. More specifically, Our proposed Mini-BiFPN's contributions are the \textit{module selection, architecture integration, the efficacy and promising performance on the pedestrian class}. We examined the model's performance contribution in the Ablation Studies Tab. 5.


\textcolor{red}{In section 4, Table 2, the methods used to be compared are not cited, unlike
Tables 1 and 3. Also, the number of methods used in Nuscenes and JRDB is much
smaller than that in KITTI 3D. Please explain.} 

\textbf{Response:} Compared to KITTI\cite{geiger2013vision} or Nuscenes\cite{caesar2020nuscenes}, JRDB\cite{martin2021jrdb} is a fairly new dataset; as a result, the number of submissions on the JRDB leaderboard is limited but expanding as its popularity grows.
Regarding Nuscenes, the complete leaderboard\footnote{eval.ai/web/challenges/challenge-page/356/leaderboard/1012} only provides mAP, NDS, and other metrics for \textbf{\textit{all classes}}, and we were only able to get per-class performance data from the previous version of the leaderboard\footnote{www.nuscenes.org/object-detection?externalData=no\&mapData=no\&modalities=Lidar}, which was not frequently updated.
Therefore, there is no official leaderboard for the pedestrian class, and it would be very difficult and time-consuming to compare the performance of just one single class.
Thus we solely compare our method to the baseline and other pillar-based approaches. Furthermore, the primary goal of comprehensively evaluating PiFeNet on numerous datasets is to demonstrate that our approach's strong performance is not due to randomness or overfitting on any particular dataset. 

\textcolor{red}{In section 5 - Mini-BiFPN module analysis, the second sentence is a bit
weird: "Since aggregating ...", maybe can put it in another way.} 

\textbf{Response:} Thank you for bringing this to our attention; we revised the phrase (in Section V. (Mini-BiFPN modules analysis)) for greater cohesion and comprehension. 





\vspace{20pt}
\underline{\textbf{Response to Comments from Reviewer \#2:}}

\textcolor{red}{\textbf{Summary:} This paper tackles the task of 3D object detection from LiDAR point cloud data by improving the pillar feature extraction and implementing
a multiscale architecture. The proposed architecture is carefully
designed to target problems of pedestrian detection (e.g., small
detection size). The model is tested on 3 benchmark datasets and shows
competitive results.}

\textcolor{red}{Strengths:
\begin{itemize} \vspace{-1em} \setlength\itemsep{-0.2em}
    \item The paper is easy to follow, and the provided figures are easy to
understand.
    \item Extensive quantitative ablation results show the effect of each
proposed block.
    \item Performance is strong and competitive with SOTA. (See weaknesses).
    \item The problem tackled is very important to the vision and autonomous
vehicles community.
\end{itemize}
}

We appreciate the positive feedback from R2 on the overall structure, performance, motivation, and experimental design. We have carefully considered your comments; please see the replies below:

\textcolor{red}{More recent results (e.g., cvpr 2022) perform better than the
proposed approach. These results are omitted [1,2]. Please add them to
the manuscript and discuss how the proposed approach compares against
these approaches in terms of performance and inference time.} 

\textbf{Response: } We much appreciate R2's effort in identifying relevant literature. 
We have included and compared our method with the suggested CVPR 2022 method EQ-PVRCNN\cite{EQPVRCNN}. Please refer to the updated Fig. 1 and Tab. 1 in the manuscript. 

Regarding CasA\cite{wu2022casa} approach: At the time of our manuscript submission to RA-L, this submission to the KITTI leaderboard was not yet published by any venue (CasA publication date seems to be 31 August 2022, while PiFeNet was submitted on 22 August 2022). Therefore, CasA was not included in the submitted version as we compared our method against all peer-reviewed and published works at the time of submission; We have now included CasA in our literature review, but we believe that comparison with the papers that appear after our submission may not be fair. If R2 and Associate Editor deem it advisable, we are happy to include a comparison between PiFeNet and CasA in the camera-ready version. 


\textcolor{red}{The authors make strong claims relating the proposed modules to more
expressive pillar features and detecting far pedestrians with more
accuracy. It is hard to support such claims without qualitative
results. For example, the authors claim that PAA enhances the features
such that the model no longer confuses people with trees. A figure
showing trees detected as people before using PAA and the same image
not detecting them with PAA would make a much stronger argument.
Similarly, for far and small-sized detections when using Mini Bi-FPN.} 

\textbf{Response:} Thank you for the great suggestions. As in Fig.5 B and C, there are many poles and trees that are successfully classified as background, representing the reliability of the proposed attention modules in generating refined features compared to the baseline PointPillars~\cite{lang2019pointpillars}. Also, we included a new figure, \ie Fig. 7, comparing the performance of PiFeNet and the baseline PointPillars for exactly the same scene, qualitatively, to show the improvement before and after attention modules. We also provide Figures 5B and 5D, presenting some of our failure cases. 

\textcolor{red}{Some architecture choices seem random. It is unclear why any of the
choices in Mini-BiFPN are made. It is important to discuss why the
connections in Figure 5 are chosen. Why swish function?} 

\textbf{Response:} Our objective initially is to develop a lightweight yet accurate feature network capable of fusing multi-scale features generated by the PAA module. Mini-BiFPN's bidirectional flow design is \textit{influenced by BiFPN} from \cite{tan2020efficientdet}; more specifically, the number of layers, network depth, and the number of repeated fusion blocks are adjusted \textit{to increase the module's efficiency while maintaining its high degree of precision}. Swish activation is \textit{just an adoption} from \cite{tan2020efficientdet}. We added the motivation and additional details in Section III. D.

\textcolor{red}{The Task-aware attention section is not very clear. Are the alpha and
beta parameters vectors? They are used with subscript k in equation 2
implying that there is a value for each channel but also initialized
with singular values [1,0,0,0]. Please clarify.} 

\textbf{Response:} We apologise for any misunderstanding this may have created; Section III. C. (Task-aware attention) has been revised to be more reflective and comprehensible. 

\textcolor{red}{Figure 3 shows the input to the PAA module as three 3D feature grids
(3 x P x N x C), whereas Figure 4 shows only a single 3D feature grid
(P x N x C). Is this on purpose to show a batch size of three? It is
better to show only a single 3D cuboid.} 

\textbf{Response:} We have revised both Fig. 3 and Fig. 4 so that they are consistent with one another, per R2 suggestion. Thanks for the recommendation to use a single 3D cuboid; We also included smaller cuboids inside the main one to help readers differentiate between the pillars. 

\textcolor{red}{Figure 5 is not really needed. It is already a part of Figure 3. The
space can be used to qualitatively show the effect of the proposed
modules as previously discussed.} 

\textbf{Response:} Figure 3 is revised to provide additional information, we agree that Figure 5 (in the previous version) is redundant and removed. 



\vspace{20pt}
\underline{\textbf{Response to Comments from Reviewer \#3:}}

\textcolor{red}{\textbf{Summary:} This work focuses on detecting people in 3D from the pillar representation of
point cloud data. It introduces two components:
1. an FC-based module (PAA; Fig. 4) that computes the attention of features
(point-wise and channel-wise, thus extracting more efficient pillar feature
representations) and
2. adapted of a bi-directional sampling-based convolutional module (to
integrated latent features of image features with varying scales; Fig. 5).
This method is evaluated on three benchmarks: KITTI (Tab. 1), Nuscenes (Tab.
2), and JRDB (Tab. 3). The results indicate that this method outperforms SotA
accuracy-wise while being fast enough for real-time as claimed (Fig. 1).
The contents are organized well}

We appreciate the kind comments. We have thoroughly considered your feedback; the responses are provided below:

\textcolor{red}{Since this is an end-to-end trainable model, it is apt to mention the loss
functions somewhere in the Methods section instead of Experiments} 

\textbf{Response:} Similar to SECOND~\cite{yan2018second}, we used the Focal loss~\cite{lin2017focal} for classification and L1 loss for bounding box regression. We added this information in section III. E. 

\textcolor{red}{Does the set  in Eqn 2 refer to the two downstream tasks,
\_viz.\_, classification and box regression?} 

\textbf{Response:} Eq. 2 does not refer to the downstream tasks, it is the formula of Task-aware attention activation, which is  where the set  means  and ,  and . The Eq. 2 is now updated and more information has been added for better understanding.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.7\linewidth]{img/ablation_pretrained.pdf}
\end{center}
\vspace{-1em}
   \caption{Performance on KITTI \textit{val} set of two training strategies. Interestingly, the model with randomly initialised weights has better performance and can stably learn during training compared to the model initialised with JRDB pretrained weights}
\label{fig:ablation_pretrained}
\end{figure}
\textcolor{red}{How well would the representations generalise to real-time data or
cross-datasets? Any experiments on this?} 

\textbf{Response:} The representation in our model can be generalised if we use it between in-domain datasets (similar to the other supervised techniques). However, these datasets (JRDB, KITTI and Nuscenes) can be considered out-of-domain respect to each other considering the environment domain shift, sensor direction, location setup, and LiDAR resolution differences between datasets. To verify this, we tried to train PiFeNet on KITTI using pretrained weights from JRDB and observed that the pretraining-finetuning strategy is not necessarily useful in practice (see \cref{fig:ablation_pretrained} above).  Similar observation has also been reported in the prior works \eg~\cite{jia2022domain}. Therefore, this is not a limitation of our framework as it is designed for supervised learning not domain adaptation since the datasets are not in-domain.
In the revised manuscript, we highlighted this point. This suggestion can be a potential future research direction, \ie a domain adaptive object detection from point cloud data. 

\textcolor{red}{Perhaps I am missing something, but Fig. 4 and the first term in Eqn. 1
are inconsistent (order of activation function and weights; trivial
remark)} 

\textbf{Response:} Thank you for pointing out this issue; we have fixed the Eq. 1.





































































\bibliographystyle{splncs04}
\bibliography{egbib}



\end{document}
