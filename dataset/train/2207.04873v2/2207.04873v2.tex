\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}


\section{Method}

\subsection{}\label{sec:sup_hrank}


We define the  in the main paper as:




We detail in~\cref{fig:supp_hrank_figure} how the  in~\cref{eq:sup_hierarchical_rank} is computed in the example from Fig.~2b of the main paper. Given a ``Lada \#2'' query, we set the relevances as follows: if  (\ie  is also a ``Lada \#2''), ; if  (\ie  is another model of ``Lada''), ; and if  ( is a ``Car''), . Relevance of negatives (other vehicles) is set to 0.


\definecolor{amethyst}{rgb}{0.6, 0.4, 0.8}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{illustrations/supplementary/supp_hrank.pdf}
    \caption{ for each retrieval results given a ``\textcolor{Green}{Lada \#2}'' \textcolor{amethyst}{query} with relevances of~\cref{sec:sup_hrank} and the hierarchical tree of Fig.~2a of the main paper.}
    \label{fig:supp_hrank_figure}
\end{figure}


In this instance,  because  and . Here, the closest common ancestor in the hierarchical tree shared by the query and instances  and  is ``Cars''. For binary labels, we would have ; this would not take into account the semantic similarity between the query and instance .


\subsection{}

We define  in the main paper as:




We illustrate in~\cref{fig:sup_dif_ap_hap} how the  is computed for both rankings of Fig.~2b of the main paper. We use the same relevances as in~\cref{sec:sup_hrank}. The  of the first example is greater () than of the second one () because the error is less severe. On the contrary, the AP only considers binary labels and is the same for both rankings ().



\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{illustrations/supplementary/sup_ap_hap.pdf}
    \caption{AP and  for two different rankings when Given a ``\textcolor{Green}{Lada \#2}'' \textcolor{amethyst}{query} and relevances of~\cref{sec:sup_hrank}. The  of the top row is greater (0.78) than the bottom one's (0.67) as the error in  is less severe for the top row. Whereas the AP is the same for both rankings (0.45).}
    \label{fig:sup_dif_ap_hap}
\end{figure}

\medbreak

\textcolor{black}{One property of AP is that it can be interpreted as the area under the precision-recall curve.  from~\cref{eq:sup_def_hap} can also be interpreted as the area under a hierarchical-precision-recall curve by defining a Hierarchical Recall () and a Hierarchical Precision () as:}



\textcolor{black}{So that  can be re-written as:}


\textcolor{black}{\cref{eq:hap_generalizes_ap} recovers Eq. (3) from the main paper, meaning that  generalizes this property of AP beyond binary labels. To further motivate  we will justify the normalization constant for , and show that ,  and  are consistent generalization of AP, R@k, P@k.}


\subsubsection{Normalization constant for } When all instances are perfectly ranked, all instances  that are ranked before instance  () have a relevance that is higher or equal than 's, \ie  and \\ . So, for each instance :


The total sum . This means that we need to normalize by  in order to constrain  between 0 and 1. This results in the definition of  from~\cref{eq:sup_def_hap}.


\subsubsection{ is a consistent generalization of AP} In a binary setting, AP is defined as follows:




 is equivalent to AP in a binary setting (). Indeed, the relevance function is  for fine-grained instances and 0 otherwise in the binary case. Therefore  which is the same definition as  in AP. Furthermore the normalization constant of , , is equal to the number of fine-grained instances in the binary setting, \ie . This means that  in this case.

\medbreak
\textcolor{black}{ is also a consistent generalization of R@k, indeed: }

\medbreak
\textcolor{black}{Finally,  is also a consistent generalization of P@k:\\ }


\subsubsection{Link between  and the weighted average of AP}

Let us define the AP for the semantic level  as the binary AP with the set of positives being all instances that belong the same level, \ie :



\fbox{
\parbox{0.9\linewidth}{\begin{property}\label{prop:link_hap_ap} For any relevance function  , with positive weights  such that :



\ie  is equal the weighted average of the AP at all semantic levels.
\end{property}
}
}



\medbreak
\medbreak
\textbf{Proof of Property~\ref{prop:link_hap_ap}}

\medbreak
Denoting , we obtain from~\cref{eq:sup_ap_level}:



We define  to ease notations, so:



We define  so that we can sum over  instead of  and inverse the summations. Note that rank does not depend on , on contrary to .


We replace  in~\cref{eq:it_is_inverted} with its definition from~\cref{eq:sup_ap_level}:


We define the following relevance function:

By construction of : 


Using the definition of the relevance function from~\cref{eq:temporary_relevance} and~\cref{eq:see_the_min}, we can rewrite~\cref{eq:nearl_hrank} with :





\cref{eq:almost_hap} lacks the normalization constant  in order to have the same shape as  in~\cref{eq:sup_def_hap}. So we must prove that :


We have proved that  with the relevance function of~\cref{eq:temporary_relevance}:


Finally we show, for an instance , :


\ie the relevance of~\cref{eq:temporary_relevance} is the same as the relevance of Property~\ref{prop:link_hap_ap}. This concludes the proof of Property~\ref{prop:link_hap_ap}. 


\subsection{Direct optimisation of }

\subsubsection{Decomposing  and }

We have , for an instance  we can define the following subsets:  and , so that . So we can rewrite :




Similarly we can define  and , with . So we can rewrite :




\subsubsection{Gradients for } We further decompose  from Eq.~5 of the main paper, using , :




\begin{table}[ht]
    \caption{Decomposition of  for optimization.}
    \label{tab:sup_choice_optim}
    \centering
    \begin{tabularx}{1\textwidth}{l YYYYYYY }
        \toprule
          &  &  &  &  &  &  &  \\
         \midrule
        Optimization & \textcolor{blue}{\cmark} & \textcolor{blue}{\cmark} & \textcolor{blue}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark}  \\
         \bottomrule
    \end{tabularx}
\end{table}

We choose to only optimize with respect to the terms indicated with \textcolor{blue}{\cmark} in~\cref{tab:sup_choice_optim}.

\medbreak
\textbf{}:  which means that in order to decrease  we must lower , which is an expected behaviour, as it will force  to have a better ranking if it ranked after negative instances (in ).

\medbreak
\textbf{}: if we suppose that  is a constant, then  which means that in order to decrease  we must lower , which is an expected behaviour, as it will force  to have a better ranking if it ranked after negative instances (in ).

\medbreak
\textbf{}: if we suppose that  is a constant,  which means that in order to decrease  we must increase , which is an expected behaviour, as it will force  to be ranked after other instances of higher relevance (in ).

\medbreak
We choose to not optimize with respect to , , , .


\medbreak

\textbf{ \& }: Optimizing through  has no impact so we choose not to optimize it, indeed . This is the case because inversions between instances of same relevance has no impact on . This is also the case for .


\medbreak

\textbf{}:  depends on  and the relevance of the other instances that are before. We note that  indeed when the  increases  increases and the increase rate can not be equal or greater than 



When optimizing through  we can no longer explicitly control the sign of . For example if  and  are null (\ie not instances of higher or equal relevance are above ),  remains and is greater than  and  can be greater than  resulting in an overall negative gradient, which is an unexpected behaviour. This is why we choose to not optimize through .


\medbreak

\textbf{}: We have  indeed all instances  ranked before  have a strictly higher relevance, \ie , so we can write:




Optimizing trough  instead of only  diminishes the magnitude of the resulting gradient, so we decide to not optimize through . 


\subsubsection{Approximating } In order to have a lower bound on  we approximate the Heaviside step function  with a smooth lower bound:


 is illustrated in~\cref{fig:sup_hrank_surrogate}. Using  we can approximate : . Because : . In our experiments we use: , , .

\subsubsection{Approximating } In order to have an upper bound on  we approximate the Heaviside with a smooth upper bound as given in~\cite{ramzi2021robust}:


 is illustrated in~\cref{fig:sup_hrank_surrogate}. Using  we can approximate : . Because : . We use the hyper-parameters: , , .


\medbreak
We illustrate in~\cref{fig:sup_hrank_surrogate}  and in~\cref{fig:sup_hrank_surrogate}  \vs . The margins denote the fact the even when the instance  is correctly ranked (lower cosine similarity than  in~\cref{fig:sup_hrank_surrogate} and higher in~\cref{fig:sup_hrank_surrogate}) we still want to backbropagate gradient which leads to more robust training.
\begin{figure}[ht!]
    \centering
        
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=1\textwidth]{illustrations/supplementary/hrank_surrogate.pdf}
        \caption{Illustration of  in~\cref{eq:sup_h_uparrow}.}
        \label{fig:sup_hrank_surrogate}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=1\textwidth]{illustrations/supplementary/rank_surrogate.pdf}
    \caption{Illustration of  in~\cref{eq:sup_h_uparrow}.}
    \label{fig:sup_rank_surrogate}
    \end{subfigure}

    \caption{Illustrations of the two approximations of the Heaviside step function used to approximate  and .}
    \label{fig:sup_surrogates}
\end{figure}

\subsection{Discussion}

\textcolor{black}{HAPPIER requires the definition of the relevance function. In our work, we leverage the hierarchical tree between concepts to this end. Is this a strong assumption? We argue that the access to a hierarchical tree is not a prohibitive factor. Hierarchical trees are available for a surprising number of datasets (CUB-200-2011~\cite{WahCUB_200_2011}, Cars196~\cite{cars196}, InShop~\cite{liuLQWTcvpr16DeepFashion}, SOP~\cite{oh2016deep}), including \emph{large scale} ones (iNaturalist~\cite{van2018inaturalist}, the three DyML datasets~\cite{sun2021dynamic} and also Imagenet~\cite{imagenet}). Even when hierarchical relations are not directly available, they are not that difficult to obtain since the tree complexity depends only on the number of classes and not of examples. Hierarchical relations can be semi-automatically obtained by grouping fine-grained labels in existing datasets, as was previously done by \eg~\cite{chang2021your}. For instance, while hierarchical labels are not directly available in scene or landmarks datasets~\cite{Radenovic-CVPR18}, this could be extended to them at a reasonable cost, \eg in Paris6k ``Sacre Coeur'' might be considered closer to ``Notre Dame'' than to the ``Moulin Rouge''. The large lexical database Wordnet~\cite{wordnet} can also be used to define hierarchies between labels and define semantic similarity, as in Imagenet~\cite{imagenet} or the SUN database~\cite{SUN}. Furthermore, our approach can be extended to leverage general knowledge beyond hierarchical taxonomies, by defining more general relevance functions built on \eg continuous similarities or attributes~\cite{parikh2011relative}.}


\section{Experiments}

\subsection{Datasets}


\subsubsection{Stanford Online Product (SOP)} \cite{oh2016deep} is a standard dataset for Image Retrieval it has two levels of semantic scales, the object Id (fine) and the object category (coarse). It depicts Ebay online objects, with \num{120053} images of \num{22634} objects (Id) classified into \num{12} (coarse) categories (\eg bikes, coffee makers \etc), see~\cref{fig:sup_sop_illust}. We use the reference train and test splits from~\cite{oh2016deep}. The dataset can be downloaded at: \url{https://cvgl.stanford.edu/projects/lifted_struct/}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{illustrations/supplementary/sup_sop_illust.pdf}
    \caption{Images from Stanford Online Products.}
    \label{fig:sup_sop_illust}
\end{figure}

\subsubsection{iNaturalist-2018 Base/Full} iNaturalist-2018 is a dataset that has been used for Image Retrieval in recent works~\cite{smoothap,ramzi2021robust}. It depicts animals, plants, mushroom \etc in wildlife, see~\cref{fig:sup_inat_illust}, it has in total \num{461 939} images and \num{8142} fine-grained classes (``Species''). We use two different sets of annotations: a set of annotations with 2 semantic levels the species (fine) and intermediate scale (coarse), we term this dataset iNat-base, and the full biological taxonomy which consists of 7 semantic levels (``Species'', ``Genus'' \dots) we term this dataset iNat-full. We use the standard Image Retrieval splits from~\cite{smoothap}. The dataset can be downloaded at: \href{https://github.com/visipedia/inat_comp/tree/master/2018}{\nolinkurl{github.com/visipedia/inat_comp}}, and the retrieval splits at: \href{https://drive.google.com/file/d/1sXfkBTFDrRU3__-NUs1qBP3sf_0uMB98/view?usp=sharing}{\nolinkurl{drive.google.com}}. 


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{illustrations/supplementary/sup_inat_illust.pdf}
    \caption{Images from iNaturalist-2018.}
    \label{fig:sup_inat_illust}
\end{figure}


\subsubsection{DyML-datasets} The DyML benchmark~\cite{sun2021dynamic} is composed of three datasets, DyML-V that depicts vehicles, DyML-A that depicts animals, DyML-P that depicts online products. The training set has three levels of semantic (), and each image is annotated with the label corresponding to each level (like SOP and iNat-base/full), however the test protocol is different. At test time for each dataset there is three sub-datasets, each sub-dataset aims at evaluating the model on a specific hierarchical level (\eg ``Fine''), so we can only compute binary metrics on each sub-dataset. We describe in~\cref{tab:sup_stats_dyml} the statistics of the train and test datasets. The three datasets can be downloaded at: \href{https://onedrive.live.com/?authkey=\%21AMLHa5h\%2D56ZZL94\&id=F4EF5F480284E1C2\%21106\&cid=F4EF5F480284E1C2}{\nolinkurl{onedrive.live.com}}.


\begin{table*}[ht]
    \vspace{-0.6\intextsep}
\caption{Statistics of the three train and test DyML benchmarks~\cite{sun2021dynamic}.
    }
    \label{tab:sup_stats_dyml} 
    \centering
    \begin{tabularx}{\textwidth}{Y Y | YY | YY | YY }
        \toprule
         \multicolumn{2}{c|}{\multirow{2}{*}{Datasets}} & \multicolumn{2}{c|}{DyML-Vehicle} & \multicolumn{2}{c|}{DyML-Animal} & \multicolumn{2}{c}{DyML-Product}\\
\cmidrule{3-8}
         && train & test & train & test & train & test \\
         \midrule
         \multirow{2}{*}{Coarse} & Classes & 5 & 6 & 5 & 5 & 36 & 6 \\\
         & Images & 343.1 K & 5.9 K & 407.8 K & 12.5 K & 747.1 K & 1.5 K \\
         \midrule
         \multirow{2}{*}{Middle} & Classes & 89 & 127 & 28 & 17 & 169 & 37 \\
         & Images & 343.1 K & 34.3 K & 407.8 K & 23.1 K & 747.1 K & 1.5 K \\
         \midrule
         \multirow{2}{*}{Fine} & Classes & 36,301 & 8,183 & 495 & 162 & 1,609 & 315 \\
         & Images & 343.1 K & 63.5 K & 407.8 K & 11.3 K & 747.1 K & 1.5 K \\
         \bottomrule
    \end{tabularx}
\end{table*}

\subsection{Implementation details}\label{sec:sup_implementation_details}

\subsubsection{SOP \& iNat-base/full} Our model is a ResNet-50~\cite{he2015deep} pretrained on Imagenet to which we append a \texttt{LayerNormalization} layer with no affine parameters after the (average) pooling and a Linear layer that reduces the embeddings size from  to . We use the Adam~\cite{kingma2014adam} optimizer with a base learning rate of  and weight decay of  for SOP and a base learning rate of  and weight decay of  for iNat-base/full. The learning rate is decreased using cosine annealing decay, for 75 epochs on SOP and 100 epochs on iNat-base/full. We ``warm up'' our model for 5 epochs, \ie the pretrained weights are not optimized. We use standard data augmentation: \texttt{RandomResizedCrop} and \texttt{RandomHorizontalFlip}, with a final crop size of , at test time we use \texttt{CenterCrop}. We set the random seed to  in all our experiments. We use a fixed batch size of 256 and use the hard sampling strategy from~\cite{cakir2019deep} on SOP and the standard class balanced sampling~\cite{zhai2018classification} (4 instances per class) on iNat-base/full.

\subsubsection{DyML} We use a ResNet-34~\cite{he2015deep} randomly initialized on DyML-V\&A and pretrained on Imagenet for DyML-P, following~\cite{sun2021dynamic}. We use an SGD optimizer with Nesterov momentum (0.9), a base learning rate of  on DyML-V\&A and  on DyML-P with a weight decay of . We use cosine annealing decay to reduce the learning rate for  epochs on DyML-V\&A and  on DyML-P. We use the same data augmentation and random seed as for SOP and iNat-base. We also use the class balanced sampling (4 instances per class) with a fixed batch size of .


\subsection{Metrics}

The ASI~\cite{fagin2003comparing} measures at each rank  the set intersection proportion () between the ranked list  and the ground truth ranking , with  the total number of positives. As it compares intersection the ASI can naturally take into account the different levels of semantic: 




The NDCG~\cite{croft2010search} is the reference metric in information retrieval, we define it using the semantic level  of each instance:




To compute the AP for the semantic level  we consider that all instances with semantic levels  are positives:


\subsection{Source Code}

Our code is based on \texttt{PyTorch}~\cite{pytorch}. We use utilities from \texttt{Pytorch Metric Learning}~\cite{PML} \eg for samplers and losses, \texttt{Hydra}~\cite{hydra} to handle configuration files (Yaml), \texttt{tsnecuda}~\cite{chan2019gpu} to compute t-SNE reductions using GPUs and standard Python libraries such as \texttt{NumPy}~\cite{harris2020array} or \texttt{Matplotlib}~\cite{matplotlib}.

We use the publicly available implementations of the NSM loss~\cite{zhai2018classification}\footnote{\url{https://github.com/azgo14/classification_metric_learning}} which is under an Apache-2.0 license, of NCA++\cite{teh2020proxynca++}\footnote{\url{https://github.com/euwern/proxynca_pp}} which is under an MIT license, of ROADMAP~\cite{ramzi2021robust}\footnote{\url{https://github.com/elias-ramzi/ROADMAP}} which is under an MIT license, we use the implementation of \texttt{Pytorch Metric Learning}~\cite{PML}\footnote{\url{https://github.com/KevinMusgrave/pytorch-metric-learning}} for the ~\cite{wu2017sampling} (MIT license), and finally we have implemented the CSL~\cite{sun2021dynamic} after discussion with the authors and we will make it part of our repository.

We had access to both Nvidia Quadro RTX 5000 and Tesla V-100 (16 GiB GPUs). We use mixed precision training~\cite{micikevicius2017mixed}, which is native to \texttt{PyTorch}, to accelerate training, making our models train for up to 7 hours on Stanford Online Products, 25 hours on iNaturalist-2018, less than 20 hours on both DyML-A and DyML-V and 6 hours on DyML-P.

\subsection{On DyML results} Their is no public code available to reproduce the results of~\cite{sun2021dynamic}. After personal correspondence with the authors, we have been able to re-implement the CSL method from~\cite{sun2021dynamic}. We report the differences in performances between our results and theirs in~\cref{tab:sup_discrepancies}. Our implementation of CSL performs better on the three datasets which is the results of our better training recipes detailed in~\cref{sec:sup_implementation_details}. Our discussions with the authors of~\cite{sun2021dynamic} confirmed that the performances obtained with our re-implementation of CSL are valid and representative of the method’s potential.


\begin{table*}[ht]
\caption{Difference in performances for CSL between results reported in~\cite{sun2021dynamic} and our experiments on the DyML benchmarks.
    }
    \label{tab:sup_discrepancies} 
    \centering
    \begin{tabularx}{\textwidth}{l l YYY | YYY | YYY }
        \toprule
         & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{DyML-Vehicle} & \multicolumn{3}{c|}{DyML-Animal} & \multicolumn{3}{c}{DyML-Product}\\
        \cmidrule{3-11}
         && mAP & ASI & R@1 & mAP & ASI & R@1 & mAP & ASI & R@1 \\
         \midrule
         & CSL~\cite{sun2021dynamic} & 12.1 & 23.0 & 25.2 & 31.0 & 45.2 & 52.3 & 28.7 & 29.0 & 54.3 \\
          & CSL (ours) & 30.0 & 43.6 & 87.1 & 40.8 & 46.3 & 60.9 & 31.1 & 40.7 & 52.7 \\
         \bottomrule
    \end{tabularx}
\end{table*}


\section{Qualitative results}


\subsection{Robustness to }

\textcolor{black}{Fig. 4b of the main paper illustrates that HAPPIER is robust with respect to  with performances increasing for most values between 0.1 and 0.9. In addition, we also show in~\cref{fig:tsne_sup} that for  HAPPIER leads to a better organization of the embedding space than a fine-grained baseline (see Fig. 4a in main paper). This is expected since the lower  is, the more emphasis is put on optimizing , which organizes the embedding space in a hierarchical structure.}


\begin{figure}[ht]
    \centering
        
    \begin{subfigure}[ht]{0.3\textwidth}
        \includegraphics[width=1\textwidth]{illustrations/rebuttal/sop_HAPPIER_lambda0.3.pdf}
        \caption{}
        \label{fig:tsne_happier_03}
    \end{subfigure}
    ~
    \begin{subfigure}[ht]{0.3\textwidth}
    \includegraphics[width=1\textwidth]{illustrations/rebuttal/sop_HAPPIER_lambda0.5.pdf}
    \caption{}
    \label{fig:tsne_happier_05}
    \end{subfigure}~
    \begin{subfigure}[ht]{0.3\textwidth}
    \includegraphics[width=1\textwidth]{illustrations/rebuttal/sop_HAPPIER_lambda0.9.pdf}
    \caption{}
    \label{fig:tsne_happier_09}
    \end{subfigure}

    \caption{t-SNE visualisation of the embedding space of models trained with HAPPIER on SOP with different values of . Each point is the average embedding of each fine-grained label (object instance) and the colors represent coarse labels (object category, \eg bike, coffee maker).}
    \label{fig:tsne_sup}
\end{figure}


\subsection{Comparison of HAPPIER \vs CSL}

\textcolor{black}{In~\cref{fig:happier_vs_csl}, we compare HAPPIER against the hierarchical image retrieval method CSL~\cite{sun2021dynamic}. We observe qualitative improvements where HAPPIER results in a better ranking. This highlights the benefit of optimizing directly a hierarchical metric, \ie , rather than optimizing a proxy based triplet loss as in CSL.}

\begin{figure}[ht]
    \centering
        
    \begin{subfigure}[t]{\textwidth}
        \includegraphics[width=\textwidth]{illustrations/supplementary/sop_compa_CSL_3400.pdf}
\end{subfigure}
    
    \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=\textwidth]{illustrations/supplementary/sop_compa_CSL_353.pdf}
\end{subfigure}

    \caption{Qualitative comparison of \HAPPIER \vs CSL~\cite{sun2021dynamic} on SOP}
    \label{fig:happier_vs_csl}
\end{figure}


\subsection{Controlled errors: iNat-base} 

We showcase in~\cref{fig:sup_qualitative_results} errors of \HAPPIER \vs a fine-grained baseline on iNat-base. On~\cref{fig:sup_qual_inat_good}, we illustrate how a model trained with \HAPPIER makes mistakes that are less severe than a baseline model trained only on the fine-grained level. On~\cref{fig:sup_qual_inat_error}, we show an example where both models fail to retrieve the correct fine-grained instances, however the model trained with \HAPPIER retrieves images of bikes that are semantically more similar to the query.



\begin{figure}[ht]
    \centering
        
    \begin{subfigure}[t]{\textwidth}
        \includegraphics[width=1\textwidth]{illustrations/supplementary/inat_qualitativ_2.pdf}
        \caption{\HAPPIER can help make less severe mistakes. The inversion on the bottom row are with negative instances (in \textcolor{red}{red}), where as with \HAPPIER (top row) inversions are with instances sharing the same coarse label (in \textcolor{orange}{orange}).}
        \label{fig:sup_qual_inat_good}
    \end{subfigure}
    
    \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=1\textwidth]{illustrations/supplementary/inat_qualitativ_1.pdf}
    \caption{In this example, the models fail to retrieve the correct fine grained images. However \HAPPIER still retrieves images with the same coarse label (in \textcolor{orange}{orange}) whereas the baseline retrieves images that are dissimilar semantically to the query (in \textcolor{red}{red}).}
    \label{fig:sup_qual_inat_error}
    \end{subfigure}

    \caption{Qualitative examples of failure cases from a standard fine-grained model corrected by training with \HAPPIER.}
    \label{fig:sup_qualitative_results}
\end{figure}


\subsection{Controlled errors: iNat-full} 

We illustrate in~\cref{fig:sup_qual_inat_full_happier,fig:sup_qual_inat_full_baseline} an example of a query image and the top  retrieved results on iNat-full (). Given the same query both models failed to retrieve the correct fine-grained images (that would be in ). The standard model in~\cref{fig:sup_qual_inat_full_baseline} retrieves images that are semantically more distant than the images retrieved with \HAPPIER in~\cref{fig:sup_qual_inat_full_happier}. For example \HAPPIER retrieves images that are either in  or  (only one instance is in ) whereas the standard model retrieves instances that are in  or .


\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{illustrations/supplementary/inat_full_qual_happier.pdf}
    \caption{Images retrieved for the \textcolor{amethyst}{query image} by a model trained with \textbf{\HAPPIER} on iNat-full ().}
    \label{fig:sup_qual_inat_full_happier}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{illustrations/supplementary/inat_full_qual_baseline.pdf}
    \caption{Images retrieved for the \textcolor{amethyst}{query image} by a model trained with standard model on iNat-full ().}
    \label{fig:sup_qual_inat_full_baseline}
\end{figure}