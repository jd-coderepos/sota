\begin{figure}[b!]
    \centering
    \newcolumntype{C}{>{\centering\arraybackslash}p{3em}}
    \hspace*{-0.5cm} 
    \renewcommand{\arraystretch}{0.1}
    \begin{tabular}{CCCCCCCCCC}
        \includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/input/03001627_d2c465e85d2e8f1fcea003eff0268278_09.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dn/03001627_d2c465e85d2e8f1fcea003eff0268278_09.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/atlasnet/03001627_d2c465e85d2e8f1fcea003eff0268278_09.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/pix2mesh/03001627_d2c465e85d2e8f1fcea003eff0268278_09.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dcnn/03001627_d2c465e85d2e8f1fcea003eff0268278_09.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/im_svr/03001627_d2c465e85d2e8f1fcea003eff0268278_09.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/occu_net/03001627_d2c465e85d2e8f1fcea003eff0268278_09.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours_cam/03001627_d2c465e85d2e8f1fcea003eff0268278_09.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours/03001627_d2c465e85d2e8f1fcea003eff0268278_09.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/gt/03001627_d2c465e85d2e8f1fcea003eff0268278_09.png}
        \\
        \includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/input/02828884_6a237c1e61fe2bba8490246570868c8d_17.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dn/02828884_6a237c1e61fe2bba8490246570868c8d_17.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/atlasnet/02828884_6a237c1e61fe2bba8490246570868c8d_17.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/pix2mesh/02828884_6a237c1e61fe2bba8490246570868c8d_17.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dcnn/02828884_6a237c1e61fe2bba8490246570868c8d_17.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/im_svr/02828884_6a237c1e61fe2bba8490246570868c8d_17.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/occu_net/02828884_6a237c1e61fe2bba8490246570868c8d_17.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours_cam/02828884_6a237c1e61fe2bba8490246570868c8d_17.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours/02828884_6a237c1e61fe2bba8490246570868c8d_17.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/gt/02828884_6a237c1e61fe2bba8490246570868c8d_17.png}
        \\
        \includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/input/03001627_17e916fc863540ee3def89b32cef8e45_08.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dn/03001627_17e916fc863540ee3def89b32cef8e45_08.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/atlasnet/03001627_17e916fc863540ee3def89b32cef8e45_08.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/pix2mesh/03001627_17e916fc863540ee3def89b32cef8e45_08.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dcnn/03001627_17e916fc863540ee3def89b32cef8e45_08.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/im_svr/03001627_17e916fc863540ee3def89b32cef8e45_08.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/occu_net/03001627_17e916fc863540ee3def89b32cef8e45_08.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours_cam/03001627_17e916fc863540ee3def89b32cef8e45_08.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours/03001627_17e916fc863540ee3def89b32cef8e45_08.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/gt/03001627_17e916fc863540ee3def89b32cef8e45_08.png}
        \\
        \includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/input/04379243_2fca68e0ce294506fe3e90bc90e90c63_10.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dn/04379243_2fca68e0ce294506fe3e90bc90e90c63_10.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/atlasnet/04379243_2fca68e0ce294506fe3e90bc90e90c63_10.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/pix2mesh/04379243_2fca68e0ce294506fe3e90bc90e90c63_10.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dcnn/04379243_2fca68e0ce294506fe3e90bc90e90c63_10.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/im_svr/04379243_2fca68e0ce294506fe3e90bc90e90c63_10.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/occu_net/04379243_2fca68e0ce294506fe3e90bc90e90c63_10.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours_cam/04379243_2fca68e0ce294506fe3e90bc90e90c63_10.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours/04379243_2fca68e0ce294506fe3e90bc90e90c63_10.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/gt/04379243_2fca68e0ce294506fe3e90bc90e90c63_10.png}
        \\
        \includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/input/03001627_df23ca11080bb439676c272956dad3c2_05.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dn/03001627_df23ca11080bb439676c272956dad3c2_05.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/atlasnet/03001627_df23ca11080bb439676c272956dad3c2_05.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/pix2mesh/03001627_df23ca11080bb439676c272956dad3c2_05.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dcnn/03001627_df23ca11080bb439676c272956dad3c2_05.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/im_svr/03001627_df23ca11080bb439676c272956dad3c2_05.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/occu_net/03001627_df23ca11080bb439676c272956dad3c2_05.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours_cam/03001627_df23ca11080bb439676c272956dad3c2_05.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours/03001627_df23ca11080bb439676c272956dad3c2_05.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/gt/03001627_df23ca11080bb439676c272956dad3c2_05.png}
        \\
        \includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/input/04090263_144201418b5dccef1a857c47fd0ecdc3_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dn/04090263_144201418b5dccef1a857c47fd0ecdc3_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/atlasnet/04090263_144201418b5dccef1a857c47fd0ecdc3_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/pix2mesh/04090263_144201418b5dccef1a857c47fd0ecdc3_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dcnn/04090263_144201418b5dccef1a857c47fd0ecdc3_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/im_svr/04090263_144201418b5dccef1a857c47fd0ecdc3_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/occu_net/04090263_144201418b5dccef1a857c47fd0ecdc3_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours_cam/04090263_144201418b5dccef1a857c47fd0ecdc3_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours/04090263_144201418b5dccef1a857c47fd0ecdc3_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/gt/04090263_144201418b5dccef1a857c47fd0ecdc3_03.png}
        \\
        \includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/input/04379243_ccf36a20b7ef3bd128071d61462a212d_17.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dn/04379243_ccf36a20b7ef3bd128071d61462a212d_17.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/atlasnet/04379243_ccf36a20b7ef3bd128071d61462a212d_17.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/pix2mesh/04379243_ccf36a20b7ef3bd128071d61462a212d_17.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dcnn/04379243_ccf36a20b7ef3bd128071d61462a212d_17.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/im_svr/04379243_ccf36a20b7ef3bd128071d61462a212d_17.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/occu_net/04379243_ccf36a20b7ef3bd128071d61462a212d_17.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours_cam/04379243_ccf36a20b7ef3bd128071d61462a212d_17.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours/04379243_ccf36a20b7ef3bd128071d61462a212d_17.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/gt/04379243_ccf36a20b7ef3bd128071d61462a212d_17.png}
        \\
        \includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/input/02691156_f26ea1a00455f44fb88e2a19106395c2_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dn/02691156_f26ea1a00455f44fb88e2a19106395c2_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/atlasnet/02691156_f26ea1a00455f44fb88e2a19106395c2_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/pix2mesh/02691156_f26ea1a00455f44fb88e2a19106395c2_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dcnn/02691156_f26ea1a00455f44fb88e2a19106395c2_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/im_svr/02691156_f26ea1a00455f44fb88e2a19106395c2_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/occu_net/02691156_f26ea1a00455f44fb88e2a19106395c2_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours_cam/02691156_f26ea1a00455f44fb88e2a19106395c2_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours/02691156_f26ea1a00455f44fb88e2a19106395c2_03.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/gt/02691156_f26ea1a00455f44fb88e2a19106395c2_03.png}
        \\
        \includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/input/02691156_8b0d9118e9e7dc3f95c9af648ce95367_06.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dn/02691156_8b0d9118e9e7dc3f95c9af648ce95367_06.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/atlasnet/02691156_8b0d9118e9e7dc3f95c9af648ce95367_06.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/pix2mesh/02691156_8b0d9118e9e7dc3f95c9af648ce95367_06.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dcnn/02691156_8b0d9118e9e7dc3f95c9af648ce95367_06.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/im_svr/02691156_8b0d9118e9e7dc3f95c9af648ce95367_06.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/occu_net/02691156_8b0d9118e9e7dc3f95c9af648ce95367_06.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours_cam/02691156_8b0d9118e9e7dc3f95c9af648ce95367_06.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours/02691156_8b0d9118e9e7dc3f95c9af648ce95367_06.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/gt/02691156_8b0d9118e9e7dc3f95c9af648ce95367_06.png}
        \\
        \includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/input/03636649_e4c9bb21fe5bfeb3e21f078602e2eda8_02.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dn/03636649_e4c9bb21fe5bfeb3e21f078602e2eda8_02.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/atlasnet/03636649_e4c9bb21fe5bfeb3e21f078602e2eda8_02.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/pix2mesh/03636649_e4c9bb21fe5bfeb3e21f078602e2eda8_02.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dcnn/03636649_e4c9bb21fe5bfeb3e21f078602e2eda8_02.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/im_svr/03636649_e4c9bb21fe5bfeb3e21f078602e2eda8_02.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/occu_net/03636649_e4c9bb21fe5bfeb3e21f078602e2eda8_02.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours_cam/03636649_e4c9bb21fe5bfeb3e21f078602e2eda8_02.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours/03636649_e4c9bb21fe5bfeb3e21f078602e2eda8_02.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/gt/03636649_e4c9bb21fe5bfeb3e21f078602e2eda8_02.png}
        \\
        \includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/input/04379243_8654b644c766dd23d1dcc55e36186e4e_12.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dn/04379243_8654b644c766dd23d1dcc55e36186e4e_12.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/atlasnet/04379243_8654b644c766dd23d1dcc55e36186e4e_12.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/pix2mesh/04379243_8654b644c766dd23d1dcc55e36186e4e_12.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dcnn/04379243_8654b644c766dd23d1dcc55e36186e4e_12.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/im_svr/04379243_8654b644c766dd23d1dcc55e36186e4e_12.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/occu_net/04379243_8654b644c766dd23d1dcc55e36186e4e_12.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours_cam/04379243_8654b644c766dd23d1dcc55e36186e4e_12.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours/04379243_8654b644c766dd23d1dcc55e36186e4e_12.png}
        &\includegraphics[width=0.12\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/gt/04379243_8654b644c766dd23d1dcc55e36186e4e_12.png}
        \\
        \includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/input/04090263_b12d95ad4abbf097e149ba69dbb3f6c3_15.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dn/04090263_b12d95ad4abbf097e149ba69dbb3f6c3_15.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/atlasnet/04090263_b12d95ad4abbf097e149ba69dbb3f6c3_15.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/pix2mesh/04090263_b12d95ad4abbf097e149ba69dbb3f6c3_15.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/3dcnn/04090263_b12d95ad4abbf097e149ba69dbb3f6c3_15.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/im_svr/04090263_b12d95ad4abbf097e149ba69dbb3f6c3_15.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/occu_net/04090263_b12d95ad4abbf097e149ba69dbb3f6c3_15.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours_cam/04090263_b12d95ad4abbf097e149ba69dbb3f6c3_15.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/ours/04090263_b12d95ad4abbf097e149ba69dbb3f6c3_15.png}
        &\includegraphics[width=0.08\textwidth,height=0.065\textwidth,keepaspectratio]{figures/experiments/main/gt/04090263_b12d95ad4abbf097e149ba69dbb3f6c3_15.png}
        \\ \small{Input}& \small{3DN} & \small{AtlasNet} & \small{Pix2Mesh} & \small{3DCNN} & \small{IMNET}& \small{OccNet} & \small{Ours$_{cam}$} & \small{Ours} & \small{GT}
    \end{tabular}
    \caption {Single-view reconstruction results of various methods. `GT' denotes ground truth shapes. Best viewed on screen with zooming in.}
    \label{fig:qual_main} 
\end{figure}

\section{Experiments}
\vspace{-10pt}


We perform quantitative and qualitative comparisons on single-view 3D reconstruction with state-of-the-art methods~\cite{groueix2018,wang2018pixel2mesh,wang20193dn,chen2018learning,Mescheder2019CVPR} in Section~\ref{sec:exp:main}. We also compare the performance of our method on camera pose estimation with \cite{insafutdinov18pointclouds} in Section~\ref{sec:exp:cam}. We further conduct ablation studies in Section~\ref{sec:exp:ablation} and showcase several applications in Section~\ref{sec:exp:app}. More qualitative results and all detailed network architectures can be found in supplementary.
\vspace{-5pt}

\paragraph{Dataset} For both camera prediction and SDF prediction, we follow the settings of \cite{groueix2018,wang2018pixel2mesh,wang20193dn,Mescheder2019CVPR}, and use the ShapeNet Core dataset~\cite{shapenet}, which includes 13 object categories, and an official training/testing split to train and test our method. We train a single network on all categories and report the test results generated by this network.

\vspace{-5pt}
Choy et al.~\cite{choy20163d} provide a dataset of renderings of ShapeNet Core models where each model is rendered from 24 views with limited variation in terms of camera orientation. In order to make our method more general, we provide a new 2D dataset \footnote{\url{https://github.com/Xharlie/ShapenetRender_more_variation}} composed of renderings of the models in ShapeNet Core. Specifically, for each mesh model, our dataset provides 36 renderings with smaller variation(similar to ~\cite{choy20163d}'s) and 36 views with a larger variation(bigger yaw angle range and larger distance variation). Unlike Choy et al., we allow the object to move away from the origin, therefore, providing more degrees of freedom in terms of camera parameters. We ignore the "Roll" angle of the camera since it is very rare in a real-world scenarios. We also render higher resolution images (224 by 224 instead of the original 137 by 137). Finally, to facilitate future studies, we also pair each rendered RGBA image with a depth image, a normal map and an albedo image as shown in Figure \ref{fig:newdataset}.
\begin{figure}[htb!]
    \centering
    \newcolumntype{C}{>{\centering\arraybackslash}p{8em}}
    \hspace*{-0.5cm} 
    \renewcommand{\arraystretch}{1}
    \begin{tabular}{CCCC}
        \includegraphics[width=0.2\textwidth,height=0.2\textwidth,keepaspectratio]{figures/experiments/newdataset/rgba.png}
        &\includegraphics[width=0.2\textwidth,height=0.2\textwidth,keepaspectratio]{figures/experiments/newdataset/albedo.png}
        &\includegraphics[width=0.2\textwidth,height=0.2\textwidth,keepaspectratio]{figures/experiments/newdataset/depth.png}
        &\includegraphics[width=0.2\textwidth,height=0.2\textwidth,keepaspectratio]{figures/experiments/newdataset/normal.png}
    \\ \small{RGBA}& \small{Albedo} & \small{Depth} & \small{Normal}
    \end{tabular}
    \caption {Each view of each object has four representations correspondingly }
    \label{fig:newdataset} 
\end{figure}
\vspace{-15pt}
\vspace{-5pt}
\paragraph{Data Preparation and Implementation Details}
For each 3D mesh in ShapeNet Core, we first generate an SDF grid with resolution $256^3$ using \cite{xu2014signed, sin2013vega}. Models in ShapeNet Core are aligned and we choose this aligned model space as our world space where each render view in \cite{choy20163d} represents a transformation to a different camera space. 

We train our camera pose estimation network and SDF prediction network separately. For both networks, we use VGG-16~\cite{simonyan2014very} as the image encoder. When training the SDF prediction network, we extract the local features using the ground truth camera parameters. As mentioned in Section~\ref{sec:method:DISN}, DISN is able to generate a signed distance field with an arbitrary resolution by continuously sampling points and regressing their SDF values. However, in practice, we are interested in points near the iso-surface $\mathcal{S}_0$. Therefore, we use Monte Carlo sampling to choose $2048$ grid points under Gaussian distribution $\mathcal{N}(0, 0.1)$ during training. We choose $m_1 = 4$, $m_2 = 1$, and $\delta = 0.01$ as the parameters of Equation~\ref{loss:SDF}. Our network is implemented with TensorFlow. We use the Adam optimizer with a learning rate of $1\times10^{-4}$ and a batch size of $16$.  

For testing, we first use the camera pose prediction network to estimate the camera parameters for the input image and feed the estimated parameters as input to SDF prediction. We follow the aforementioned surface reconstruction procedure (Section~\ref{sec:method:surface}) to generate the output mesh. 

\paragraph{Evaluation Metrics}
For quantitative evaluations, we apply four commonly used metrics to compute the difference between a reconstructed mesh object and its ground truth mesh: (1) Chamfer Distance (CD), (2) Earth Moverâ€™s Distance (EMD) between uniformly sampled point clouds, (3) Intersection over Union (IoU) on voxelized meshes, and (4) F-Score~\cite{tatarchenko2019single}. The definitions of CD and EMD can be found in the supplemental. 


\subsection{Single-view Reconstruction Comparison With State-of-the-art Methods}
\label{sec:exp:main} 
In this section, we compare our approach on single-view reconstruction with state-of-the-art methods: AtlasNet~\cite{groueix2018}, Pixel2Mesh~\cite{wang2018pixel2mesh}, 3DN~\cite{wang20193dn}, OccNet~\cite{Mescheder2019CVPR} and IMNET~\cite{chen2018learning}. AtlasNet~\cite{groueix2018} and Pixel2Mesh~\cite{wang2018pixel2mesh} generate a fixed-topology mesh from a 2D image. 3DN \cite{wang20193dn} deforms a given source mesh to reconstruct the target model. When comparing to this method, we choose a source mesh from a given set of templates by querying a template embedding as proposed in the original work. IMNET~\cite{chen2018learning} and OccNet~\cite{Mescheder2019CVPR} both predict the sign of SDF to reconstruct 3D shapes. Since IMNET trains an individual model for each category, we implement their model following the original paper and train a single model on all 13 categories. Due to mismatch between the scales of shapes reconstructed by our method and OccNet, we only report their IoU, which is scale-invariant. In addition, we train a 3D CNN model, denoted by `3DCNN', where the encoder is the same as DISN and a decoder is a volumetric 3D CNN structure with an output dimension of $64^3$. The ground truth for 3DCNN is the SDF values on all $64^3$ grid locations. For both IMNET and 3DCNN, we use the same surface reconstruction method as ours to output reconstructed meshes. We also report the results of DISN using estimated camera poses and ground truth poses, denoted by `Ours$_{cam}$' and `Ours' respectively. AtlasNet, Pixel2Mesh, and 3DN use explicit surface generation, while 3DCNN, IMNET, OccNet, and our methods reconstruct implicit surfaces. 

As shown in Table~\ref{tab:quant_main}, DISN outperforms all other models in EMD and IoU. Only 3DN performs better than our model on CD, however, 3DN requires more information than ours in the form of a source mesh as input. Figure~\ref{fig:qual_main} shows qualitative results. As illustrated in both quantitative and qualitative results, implicit surface representation provides a flexible method of generating topology-variant 3D meshes. Comparisons to 3D CNN show that predicting SDF values for given points produces smoother surfaces than generating a fixed 3D volume using an image embedding. We speculate that this is due to SDF being a continuous function with respect to point locations. It is harder for a deep network to approximate an overall SDF volume with global image features only. Moreover, our method outperforms IMNET and OccNet in terms of recovering shape details. For example, in Figure~\ref{fig:qual_main}, local feature extraction enables our method to generate different patterns of the chair backs in the first three rows, while other methods fail to capture such details. We further validate the effectiveness of our local feature extraction module in Section~\ref{sec:exp:ablation}. Although using ground truth camera poses (i.e., 'Ours') outperforms using predicted camera poses (i.e., 'Ours$_{cam}$') in quantitative results, respective qualitative results demonstrate no significant difference. 


\begin{table}[htb!]
\hspace*{-0.9cm}
\setlength{\tabcolsep}{1.5pt} \setlength{\arrayrulewidth}{1pt}
\begin{tabular}{c|c|ccccccccccccc|c}
                \Xhline{2\arrayrulewidth}
                     &            & plane & bench & box   & car   & chair & display & lamp  & speaker & rifle & sofa  & table & phone & boat  & Mean  \\ 
                     \hline
\multirow{7}{*}{EMD} & AtlasNet   & 3.39  & 3.22  & 3.36  & 3.72  & 3.86  & 3.12    & 5.29  & 3.75    & 3.35  & 3.14  & 3.98  & 3.19  & 4.39  & 3.67  \\
                     & Pxl2mesh & 2.98  & 2.58  & 3.44  & 3.43  & 3.52  & 2.92    & 5.15  & 3.56    & 3.04  & 2.70  & 3.52  & 2.66  & 3.94  & 3.34  \\
                     & 3DN        & 3.30  & 2.98  & 3.21  & 3.28  & 4.45  & 3.91    & \textbf{3.99}  & 4.47    & 2.78  & 3.31  & 3.94  & 2.70  & 3.92  & 3.56  \\
                     & IMNET     & 2.90  &    2.80  &    3.14  &    2.73  &    3.01  &    2.81    & 5.85  & 3.80    &    2.65  &    2.71  &    3.39  &    2.14    &  2.75  & 3.13       \\
                     & 3D CNN     & 3.36   & 2.90 & 3.06  & \textbf{2.52}   & 3.01  &  2.85  & 4.73   & \textbf{3.35}   &  2.71  &  \textbf{2.60}  & \textbf{3.09}  & 2.10  &  \textbf{2.67}  & 3.00 \\
                     & Ours$_{cam}$& \textbf{2.67}    & \textbf{2.48} &    \textbf{3.04}    & 2.67 &    \textbf{2.67}    & \textbf{2.73} & 4.38 &    3.47 & \textbf{2.30} & 2.62 & 3.11 & \textbf{2.06} & 2.77 & \textbf{2.84}       \\
                     & Ours& 2.45    & 2.41 &    2.99    & 2.52 &    2.62    & 2.63 & 4.11 &    3.37 & 1.93 & 2.55 & 3.07 & 2.00    & 2.55 & 2.71     \\\hline
\multirow{7}{*}{CD}  & AtlasNet   & \textbf{5.98}  & 6.98  & 13.76 & 17.04 & 13.21 & 7.18    & 38.21 & 15.96   & 4.59  & 8.29  & 18.08 & 6.35  & 15.85 & 13.19 \\
                     & Pxl2mesh & 6.10  & \textbf{6.20}  & 12.11 & 13.45 & 11.13 & \textbf{6.39}    & 31.41 & \textbf{14.52}   & 4.51  & \textbf{6.54}  & 15.61 & 6.04  & 12.66 & 11.28 \\
                     & 3DN        & 6.75  & 7.96  & \textbf{8.34}  & 7.09  & 17.53 & 8.35    & \textbf{12.79} & 17.28   & \textbf{3.26}  & 8.27  & 14.05 & \textbf{5.18}  & 10.20 & \textbf{9.77}  \\
                     & IMNET     & 12.65 &    15.10 &    11.39 &    8.86  &    11.27 &    13.77   & 63.84 & 21.83   &    8.73  &    10.30 &    17.82 & 7.06     & 13.25 & 16.61       \\
                     & 3D CNN     & 10.47  & 10.94  & 10.40  & \textbf{5.26}  & 11.15  & 11.78  & 35.97  &  17.97 & 6.80 &  9.76  & \textbf{13.35}  & 6.30   &  \textbf{9.80}  & 12.30 \\
                     & Ours$_{cam}$&  9.96    & 8.98    & 10.19 &    5.39 & \textbf{7.71}   &    10.23 & 25.76 &     17.90 & 5.58 &    9.16 & 13.59 &    6.40 & 11.91 & 10.98  \\
                     & Ours& 9.01    & 8.32  & 9.98  &    4.92  &    7.54  &    9.58  &    22.73 &     16.70 & 4.36 & 8.71 & 13.29 &    6.21    & 10.87 & 10.17  \\\hline
\multirow{7}{*}{IoU} & AtlasNet   & 39.2  & 34.2  & 20.7  & 22.0  & 25.7  & 36.4    & 21.3  & 23.2    & 45.3  & 27.9  & 23.3  & 42.5  & 28.1  & 30.0  \\
                     & Pxl2mesh & 51.5  & 40.7  & 43.4  & 50.1  & 40.2  & 55.9    & 29.1  & 52.3    & 50.9  & 60.0  & 31.2  & 69.4  & 40.1  & 47.3  \\
                     & 3DN        & 54.3  & 39.8  & 49.4  & 59.4  & 34.4  & 47.2    & 35.4  & 45.3    & 57.6  & 60.7  & 31.3  & 71.4  & 46.4  & 48.7  \\
                     & IMNET     & 55.4 & 49.5 & 51.5 & 74.5 & 52.2 & 56.2 & 29.6 & 52.6 &    52.3 &    64.1 &    45.0 &    70.9     & 56.6 & 54.6      \\
                     & 3D CNN     & 50.6 & 44.3 &    52.3 &    \textbf{76.9} &    52.6 &    51.5 &    36.2 &    58.0 &    50.5 &    \textbf{67.2} &    50.3 &    70.9    & \textbf{57.4} & 55.3     \\
                     & OccNet     & 54.7 & 45.2 &    \textbf{73.2} &    73.1 &    50.2 &    47.9 &    \textbf{37.0} &    \textbf{65.3} &    45.8 &    67.1 &    \textbf{50.6} &    70.9    & 52.1 & 56.4     \\
                     & \textbf{DISN$_{cam}$}& \textbf{57.5} &    \textbf{52.9} &    52.3 &    74.3 &    \textbf{54.3} &    \textbf{56.4} &    34.7 &    54.9 &    \textbf{59.2} &    65.9 &    47.9 &    \textbf{72.9}    & 55.9 & \textbf{57.0}       \\
                     & \textbf{DISN} & 61.7 &    54.2 &    53.1 &    77.0 &    54.9 &    57.7 &    39.7 &    55.9 &    68.0 &    67.1 &    48.9 &    73.6    & 60.2 & 59.4      \\\hline
                \Xhline{2\arrayrulewidth}
\end{tabular}
\vspace{5pt}
\caption{Quantitative results on ShapeNet Core for various methods. Metrics are CD ($\times 0.001$, the smaller the better), EMD ($\times 100$, the smaller the better) and IoU ($\%$, the larger the better). CD and EMD are computed on $2048$ points.}
\label{tab:quant_main}
\vspace{-15pt}
\end{table}


We also compute the F-score (see Table \ref{tab:fscore}) which measures the percentage of surface area that is reconstructed correctly and thus provides a reliable metric \cite{tatarchenko2019single}. In our evaluations, we use $F_1 = 2 * (\text{Precision} \cdot \text{Recall})/ (\text{Precision} + \text{Recall})$. We uniformly sample points from both ground truth and generated meshes. We define precision as the percentage of the generated points whose distance to the closest ground truth point is less than a threshold. Similarly, we define recall as the percentage of ground truth points whose distance to the closest generated point is less than a threshold.

\begin{table}[]
\begin{minipage}{.6\textwidth}
    \centering
    \setlength{\tabcolsep}{3pt} \setlength{\arrayrulewidth}{1pt}
    \begin{tabular}{c|cccccc}
        \hline
        Threshold(\%) & 0.5\%                     & 1\%                       & 2\%                       & 5\%                       & 10\%                      & 20\%                      \\ \hline
        3DCNN                & 0.064                     & 0.295                     & 0.691                     & 0.935                     & 0.984                     & 0.997                     \\ \hline
        IMNet                & 0.063                     & 0.286                     & 0.673                     & 0.922                     & 0.977                     & 0.995                     \\ \hline
        DISN          & 0.079                     & 0.327                     & 0.718                     & 0.943                     & 0.984                     & 0.996                     \\ \hline
        DISN$_{cam}$         & \multicolumn{1}{l}{0.070} & \multicolumn{1}{l}{0.307} & \multicolumn{1}{l}{0.700} & \multicolumn{1}{l}{0.940} & \multicolumn{1}{l}{0.986} & \multicolumn{1}{l}{0.998} \\ \hline
        \bottomrule
    \end{tabular}
    \vspace{3pt}
    \caption {F-Score for varying thresholds (\% of reconstruction volume side length, same as \cite{tatarchenko2019single}) on all categories.}
    \label{tab:fscore}
\end{minipage}
\vspace{-10pt}
\hspace*{0.1cm}
\begin{minipage}{.4\textwidth}
    \centering
    \setlength{\arrayrulewidth}{1pt}
    \begin{tabular}{c|ccc}
                    \Xhline{2\arrayrulewidth}
        &  \cite{insafutdinov18pointclouds} & Ours & Ours$_{new}$ \\ \hline
    $d_{3D}$ &  0.073  &  \textbf{0.047}  &  0.059 \\ \hline
    $d_{2D}$  &  4.86  &   \textbf{2.95}  &  4.38/2.67 \\ 
                    \Xhline{2\arrayrulewidth}
    \end{tabular}
    \vspace{3pt}
    \caption {Camera pose estimation comparison. The unit of $d_{2D}$ is pixels.}
\label{tab:camest}
\end{minipage}
\vspace{-10pt}
\end{table}


\vspace{-10pt}
\subsection{Camera Pose Estimation}
\label{sec:exp:cam}
\vspace{-5pt}

We compare our camera pose estimation with \cite{insafutdinov18pointclouds}. Given a point cloud $PC_w$ in world coordinates for an input image, we transform $PC_w$ using the predicted camera pose and compute the mean distance $d_{3D}$ between the transformed point cloud and the ground truth point cloud in camera space. We also compute the 2D reprojection error $d_{2D}$ of the transformed point cloud after we project it onto the input image. Table~\ref{tab:camest} reports $d_{3D}$ and $d_{2D}$ of \cite{insafutdinov18pointclouds} and our method. With the help of the $6D$ rotation representation, our method outperforms \cite{insafutdinov18pointclouds} by $2$ pixels in terms of 2D reprojection error. We also train and test the pose estimation on the new 2D dataset. Even these images possess more view variation, because of the better rendering quality, we can achieve an average 2D distance of 4.38 pixels on 224 by 224 images (2.67 pixels if normalized to the original resolution of 137 by 137).
\subsection{Ablation Studies}
\label{sec:exp:ablation}
\vspace{-5pt}
To show the impact of the camera pose estimation, local feature extraction, and different network architectures, we conduct ablation studies on the ShapeNet ``chair'' category, since it has the greatest variety. Table~\ref{tab:quant_ablation} reports the quantitative results and Figure~\ref{fig:qual_ablation} shows the qualitative results.
\vspace{-5pt}

\begin{figure*}[!htb]
    \newcolumntype{C}{>{\centering\arraybackslash}p{3em}}
    \centering
    \begin{tabular}{CCCCCCCCC}
\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/input/03001627_5a60c649a221293d72ed554eb3baedcc_13.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/locwobin_cam/03001627_5a60c649a221293d72ed554eb3baedcc_13.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/locwobin/03001627_5a60c649a221293d72ed554eb3baedcc_13.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/noloco/03001627_5a60c649a221293d72ed554eb3baedcc_13.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/onestream_cam/03001627_5a60c649a221293d72ed554eb3baedcc_13.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/onestream/03001627_5a60c649a221293d72ed554eb3baedcc_13.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/ours_cam/03001627_5a60c649a221293d72ed554eb3baedcc_13.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/ours/03001627_5a60c649a221293d72ed554eb3baedcc_13.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/gt/03001627_5a60c649a221293d72ed554eb3baedcc_13.png}
        \\
        \includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/input/03001627_3d32d89db2286377e63c6421b71f17c8_08.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/locwobin_cam/03001627_3d32d89db2286377e63c6421b71f17c8_08.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/locwobin/03001627_3d32d89db2286377e63c6421b71f17c8_08.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/noloco/03001627_3d32d89db2286377e63c6421b71f17c8_08.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/onestream_cam/03001627_3d32d89db2286377e63c6421b71f17c8_08.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/onestream/03001627_3d32d89db2286377e63c6421b71f17c8_08.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/ours_cam/03001627_3d32d89db2286377e63c6421b71f17c8_08.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/ours/03001627_3d32d89db2286377e63c6421b71f17c8_08.png}
        &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/ablation/gt/03001627_3d32d89db2286377e63c6421b71f17c8_08.png}
\\\small{Input}& \small{Binary$_{cam}$} & \small{Binary} & \small{Global} & \small{One-stream$_{cam}$} & \small{One-stream} & \small{Two-stream$_{cam}$} & \small{Two-stream} & \small{GT}
    \end{tabular}
    \caption { Qualitative results of our method using different settings. `GT' denotes ground truth shapes, and `$_{cam}$' denotes models with estimated camera parameters. }
    \label{fig:qual_ablation} 
    \vspace{-10pt}
\end{figure*}

\paragraph{Camera Pose Estimation} 
As is shown in Section~\ref{sec:exp:cam}, camera pose estimation potentially introduces uncertainty to the local feature extraction process with an average reprojection error of $2.95$ pixels. Although the quantitative reconstruction results with ground truth camera parameters are constantly superior to the results with estimated parameters in Table~\ref{tab:quant_ablation}, Figure~\ref{fig:qual_ablation} demonstrates that a small difference in the image projection does not affect the reconstruction quality significantly. 
\vspace{-5pt}

\paragraph{Binary Classification}
Previous studies~\cite{Mescheder2019CVPR,chen2018learning} formulate SDF prediction as a binary classification problem by predicting the probability of a point being inside or outside the surface $\mathcal{S}_0$. Even though Section~\ref{sec:exp:main} illustrates our superior performance over \cite{Mescheder2019CVPR,chen2018learning}, we further validate the effectiveness of our regression supervision by comparing with classification supervision using our own network structure. Instead of producing a SDF value, we train our network with classification supervision and output the probability of a point being inside the mesh surface. We use a softmax cross entropy loss to optimize this network. We report the result of this classification network as `Binary'.
\vspace{-10pt}


\paragraph{Local Feature Extraction}
Local image features of each point provide access to the corresponding local information that captures shape details. To validate the effectiveness of this information, we remove the `local features extraction' module from DISN and denote this setting by `Global'. This model predicts the SDF value solely based on the global image features. By comparing `Global' with other methods in Table~\ref{tab:quant_ablation} and Figure~\ref{fig:qual_ablation}, we conclude that local feature extraction helps the model capture shape details and improve the reconstruction quality by a large margin. 
\vspace{-5pt}

\paragraph{Network Structures}
To further assess the impact of different network architectures, in addition to our original architecture with two decoders (which we call 'Two-stream'), we also introduce a `One-stream' architecture where the global features, the local features, and the point features are concatenated and fed into a single decoder which predicts the SDF value. Detailed structure of this architecture can be found in the supplementary. As illustrated in Table~\ref{tab:quant_ablation} and Figure~\ref{fig:qual_ablation}, the original Two-stream setting is slightly superior to One-stream, which shows that DISN is robust to different network architectures.



\begin{table}[!hbt]
\begin{adjustwidth}{-8pt}{0pt}
\setlength\tabcolsep{6pt} \begin{tabular}{c|cccccc}
\Xhline{2\arrayrulewidth}
Camera &  Binary & Global & One-stream  & Two-stream \\
 Pose &   ground truth | estimated &  n/a   &     ground truth | estimated  &             ground truth | estimated \\\hline
EMD &   2.88 | 2.99 &   2.75 | n/a &   2.71 | 2.74 &  \textbf{2.62} | \textbf{2.65}  \\ \hline
CD  &   8.27 | 8.80 &   7.64 | n/a &   7.86 | 8.30 &  \textbf{7.55} | \textbf{7.63}    \\ \hline
IoU &   54.9 | 53.5 &   54.8 | n/a &   53.6 | 53.5 &  \textbf{55.3} | \textbf{53.9}    \\ 
\Xhline{2\arrayrulewidth}
\end{tabular}
\vspace{3pt}
\caption {Quantitative results on the category ``chair''. CD ($\times 0.001$), EMD ($\times 100$) and IoU (\%).
}
\label{tab:quant_ablation}
\vspace{-20pt}
\end{adjustwidth}
\end{table}

\vspace{-5pt}
\subsection{Applications}
\label{sec:exp:app}
\vspace{-5pt}

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-30pt}
    \newcolumntype{C}{>{\centering\arraybackslash}p{2em}}
    \begin{center}
        \begin{tabular}{CCCCCC}
            \includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/interpolation/1/snapshot00.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/interpolation/1/snapshot01.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/interpolation/1/snapshot04.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/interpolation/1/snapshot06.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/interpolation/1/snapshot09.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/interpolation/1/snapshot11.png}
            \\
            \includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/interpolation/2/snapshot00.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/interpolation/2/snapshot01.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/interpolation/2/snapshot04.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/interpolation/2/snapshot06.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/interpolation/2/snapshot09.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/interpolation/2/snapshot11.png}
        \end{tabular}
    \end{center}
    \caption{Shape interpolation result.}
    \label{fig:interpolation}
\end{wrapfigure}
\paragraph{Shape interpolation}
Figure~\ref{fig:interpolation} shows shape interpolation results where we interpolate both global and local image features going from the leftmost sample to the rightmost. We see that the generated shape is gradually transformed.



\vspace{-5pt}
\paragraph{Test with online product images}

\begin{wrapfigure}{l}{0.55\textwidth}
    \hspace{-5pt}
    \vspace{-10pt}
    \newcolumntype{C}{>{\centering\arraybackslash}p{1.8em}}
    \begin{center}
        \begin{tabular}{CCCCCCC}
\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/580b57fcd9996e24bc43c270.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/Download-Chair-Transparent.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/Download-Table-PNG-HD-1.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/garden-bencht.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/outdoor-lounge-chairs-hdwoschotbr-64_1000.jpg}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/table-transparent-clear-background.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/white2drawerbedsidedrawers.png}
            \\
\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/580b57fcd9996e24bc43c27000.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/Download-Chair-Transparent00.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/Download-Table-PNG-HD-100.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/garden-bencht00.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/outdoor-lounge-chairs-hdwoschotbr-64_100000.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/table-transparent-clear-background00.png}
            &\includegraphics[width=0.07\textwidth,height=0.08\textwidth,keepaspectratio]{figures/experiments/online_images/white2drawerbedsidedrawers00.png}
        \end{tabular}
    \end{center}
    \caption{Test our model on online product images.}
    \label{fig:online}
    \vspace{-10pt}
\end{wrapfigure}

Figure \ref{fig:online} illustrates 3D reconstruction results by DISN on online product images. Note that our model is trained on rendered images, this experiment validates the domain transferability of DISN.
\vspace{-5pt}


\paragraph{Multi-view reconstruction}
Our model can also take multiple 2D views of the same object as input. After extracting the global and the local image features for each view, we apply max pooling and use the resulting features as input to each decoder. We have retrained our network for 3 input views and visualize some results in Figure \ref{fig:multiview_model}. Combining multi-view features helps DISN to further address shape details.
\begin{wrapfigure}{r}{0.48\textwidth}
    \newcolumntype{C}{>{\centering\arraybackslash}p{2.4em}}
    \vspace{-35pt}
    \begin{center}
        \begin{tabular}{CCCCC}
            \includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/multiview/8f226d6b3089d3b7bca860dd9b04c52c_06.png}
            &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/multiview/03001627_8f226d6b3089d3b7bca860dd9b04c52c_06.png}
            &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/multiview/8f226d6b3089d3b7bca860dd9b04c52c_04.png}
            &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/multiview/8f226d6b3089d3b7bca860dd9b04c52c_14.png}
            &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/multiview/8f226d6b3089d3b7bca860dd9b04c52c_ref_img_resized0.png}
            \\
            \includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/multiview/e467cfa616850a4a990561fc34164364_23.png}
            &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/multiview/03001627_e467cfa616850a4a990561fc34164364_23.png}
            &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/multiview/e467cfa616850a4a990561fc34164364_13.png}
            &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/multiview/e467cfa616850a4a990561fc34164364_17.png}
            &\includegraphics[width=0.1\textwidth,height=0.1\textwidth,keepaspectratio]{figures/experiments/multiview/e467cfa616850a4a990561fc34164364_ref_img_resized0.png}
            \\
            (a) & (b) & (c) & (d) & (e)
        \end{tabular}
    \end{center}
    \vspace{-5pt}
    \caption{Multi-view reconstruction results. (a) Single-view input. (b) Reconstruction result from (a). (c)\&(d) Two other views. (e) Multi-view reconstruction result from (a), (c) and (d).}
    \label{fig:multiview_model}
    \vspace{-15pt}
\end{wrapfigure}

