\section{Experiments}
\label{sec:expers}

In this section we report experiments assessing the effectiveness of our DSAM-based architecture in the DG scenario, using two different backbone architectures (a ResNet-18 \cite{he2016deep} and an AlexNet \cite{krizhevsky2012imagenet}), on two different databases. We first describe the datasets used (section \ref{sec:dataset}), and then we proceed to report the model setup (section \ref{sec:model}) and the training protocol adopted (section \ref{sec:training}). Section \ref{sec:results} reports and comments upon the experimental results obtained.

    \subsection{Datasets}
    \label{sec:dataset}

We performed experiments on two different databases. The \textbf{PACS} database \cite{li2017deeper} has been recently introduced to support research on DG, and it is quickly becoming the standard reference benchmark for this research thread. It consists of  images, of resolution , taken from four different visual domains (Photo, Art paintings, Cartoon and Sketches), depicting seven categories. We followed the experimental protocol of \cite{li2017deeper} and trained our models considering three domains as source datasets and the remaining one as target.

The \textbf{Office-Home} dataset \cite{venkateswara2017Deep} was introduced to support research on DA for object recognition. It provides images from four different domains: Artistic images, Clip art, Product images and Real-world images. Each domain depicts  object categories that can be found typically in office and home settings. We are not aware of previous work using the Office-Home dataset in DG scenarios, hence we decided to follow also here the experimental setup introduced in \cite{li2017deeper} and described above for PACS.

    \begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.2\textwidth]{art_painting_dog}
\includegraphics[width=.2\textwidth]{cartoon_dog}
\includegraphics[width=.2\textwidth]{sketch_dog}
\includegraphics[width=.2\textwidth]{photo_dog}
\includegraphics[width=.2\textwidth]{art_house}
\includegraphics[width=.2\textwidth]{cartoon_house}
\includegraphics[width=.2\textwidth]{sketch_house}
\includegraphics[width=.2\textwidth]{photo_house}
\includegraphics[width=.2\textwidth]{art_person}
\includegraphics[width=.2\textwidth]{cartoon_person}
\includegraphics[width=.2\textwidth]{sketch_person}
\includegraphics[width=.2\textwidth]{photo_person}
\includegraphics[width=.2\textwidth]{art_guitar}
\includegraphics[width=.2\textwidth]{cartoon_guitar}
\includegraphics[width=.2\textwidth]{sketch_guitar}
\includegraphics[width=.2\textwidth]{photo_guitar}
\end{minipage}\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.2\textwidth]{Art}
\includegraphics[width=.2\textwidth]{Clipart}
\includegraphics[width=.2\textwidth]{Product}
\includegraphics[width=.2\textwidth]{Real-World}
\includegraphics[width=.2\textwidth]{Art_backpack}
\includegraphics[width=.2\textwidth]{Clipart_backpack}
\includegraphics[width=.2\textwidth]{Product_backpack}
\includegraphics[width=.2\textwidth]{Real-World_backpack}
\includegraphics[width=.2\textwidth]{Art_ruler}
\includegraphics[width=.2\textwidth]{Clipart_ruler}
\includegraphics[width=.2\textwidth]{Product_ruler}
\includegraphics[width=.2\textwidth]{Real-World_ruler}
\includegraphics[width=.2\textwidth]{Art_spoon}
\includegraphics[width=.2\textwidth]{Clipart_spoon}
\includegraphics[width=.2\textwidth]{Product_spoon}
\includegraphics[width=.2\textwidth]{Real-World_spoon}
\end{minipage}
\caption{Exemplar images for the PACS (left) and Office-Home (right) databases, on selected categories. We see that for both databases, the variations among domains for the same category can vary a lot.}
\end{figure}
    
    \subsection{Model setup}
	\label{sec:model}

	\textbf{Aggregation Nodes}. We implemented the aggregation nodes as 1x1 convolutional filters followed by nonlinearity. Compared to \cite{yu2017deep}, we did not use batch normalization in the aggregations, since we empirically found it detrimental for our difficult DG targets. Whenever the inputs of a node have different scales, we downsampled with the same strategy used in the backbone model. For ResNet-18 experiments, we further regularized the convolutional inputs of our aggregations with dropout.
    
    \textbf{Aggregation of Fully Connected Layers}. We observe that a fully connected layer's output can be seen as a 4-dimensional (N, C, H, W) tensor with collapsed height and width dimensions, as each unit's output is a function of the entire input image. A 1x1 convolutional layer whose input is such a tensor coincides with a fully connected layer whose input is a 2-dimensional (N, C) tensor, so for simplicity we implemented those aggregations with fully connected layers instead of convolutions.
    
    \textbf{Model Initialization}. We experimented with two different backbone models: AlexNet and ResNet-18, both of which are pre-trained on the ImageNet 1000 object categories \cite{ILSVRC15}. We initialized our aggregation modules   with random uniform initialization. We connected the aggregation nodes with the output of the AlexNet's layers when using AlexNet as backbone, or with the exit of each residual block when using ResNet-18. 
    
    \subsection{Training setup}
    \label{sec:training}
    We finetuned our models on  source domains and tested on the remaining target. We splitted our training sets in 90\% train and 10\% validation, and used the best performing model on the validation set for the final test, following the validation strategy described in Section \ref{sec:methodology}. For preprocessing, we used random zooming with rescaling, horizontal flipping, brightness/contrast/saturation/hue perturbations and normalization using ImageNet's statistics. We used a batch size of 96 (32 images per source domain) and trained using SGD with momentum set at 0.9 and initial learning rate at 0.01 and 0.007 for ResNet's and AlexNet's experiments respectively. We considered an epoch as the minimum number of steps necessary to iterate over the largest source domain and we trained our models for 30 epochs, scaling the learning rate by a factor of 0.2 every 10 epochs. We used the same setup to train our ResNet-18 Deep All baselines. We repeated each experiment 5 times, averaging the results.
    
    \subsection{Results}
    \label{sec:results}
    
    We run a first set of experiments with the D-SAMs using an AlexNet as backbone, to compare our results with those reported in the literature by previous works, as AlexNet has been so far the convnet of choice in DG. Results are reported in table \ref{table:pacs}. We see that our approach outperforms previous work by a sizable margin, showing the value of our architecture. Particularly, we underline that D-SAMs obtain remarkable performances on the challenging setting where the 'Sketch' domain acts as target.
    
    We then run a second set of experiments, using both the PACS and Office-Home dataset, using as backbone architecture a ResNet-18. The goal of this set of experiments is on one side to showcase how our approach can be easily used with different  networks, on the other side to perform an ablation study with respect to the possibility to use D-SAMs not only in an end-to-end classification framework, but also to learn feature representations, suitable for domain generalization. To this end, we report results on both databases using the end-to-end approach tested in the AlexNet experiments, plus results obtained using the feature representations learned by ,  and the combination of the two. Specifically, we extract and  normalize features from 
the last pooling layer of each component. We integrate features of ’s modules 
with concatenation, and train the SVM classifier leaving the hyperparameter  
at the default value. Our results in table \ref{table:resnetpacs} and \ref{table:resnetoh} show that the SVM classifier 
trained on the  normalized features always outperforms the corresponding end-
 to-end models, and that ’s and ’s features have similar performance, with ’s 
 features outperforming the corresponding Deep All features while requiring no 
 computational overhead for inference. 
   
    
    
    \begin{table}
\centering
\caption{PACS end-to-end results using D-SAMs coupled with the AlexNet architecture.}
\label{table:pacs}
\begin{tabular}{p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}}
\hline
\textbf{}    & Deep All \cite{li2017deeper} & TF \cite{li2017deeper} & MLDG \cite{MLDG_AAA18}           & SSN \cite{mancini2018best}         & D-SAMs           \\
\hline
art painting & 64.91    & 62.86        & \textbf{66.23} & 64.10          & 63.87          \\
cartoon      & 64.28    & 66.97        & 66.88          & 66.80          & \textbf{70.70} \\
photo        & 86.67    & 89.50        & 88.00          & \textbf{90.20} & 85.55          \\
sketch       & 53.08    & 57.51        & 58.96          & 60.10          & \textbf{64.66} \\
\hline
avg          & 67.24    & 69.21        & 70.01          & 70.30          & \textbf{71.20}
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{PACS results with ResNet-18 using features (top-rows) and end-to-end accuracy (bottom rows).}
\label{table:resnetpacs}
\begin{tabular}{p{2.5cm}p{1.9cm}p{1.9cm}p{1.9cm}p{1.9cm}|p{1.9cm}}
\hline
\textbf{}   & art painting & cartoon & sketch & photo & Avg   \\
\hline
Deep All (feat.) & 77.06        & \textbf{77.81}   & 74.09  & 93.28 & 80.56 \\
 (feat.)    & \textbf{79.57}        & 76.94   & 75.47  & 94.16 & 81.54 \\
 (feat.)    & 79.48        & 77.13   & 75.30  & 94.30 & \textbf{81.55} \\
 (feat.) & 79.44 & 77.22 & 75.33 & 94.19 & 81.54
\\
\hline
Deep All    & 77.84        & 75.89   & 69.27  & 95.19  & 79.55 \\
D-SAMs       & 77.33        & 72.43   & \textbf{77.83}  &  \textbf{95.30} & 80.72
\end{tabular}
\end{table}

\begin{table}
\caption{OfficeHome results with ResNet-18 using features (top rows) and end-to-end accuracy (bottom rows).}
\label{table:resnetoh}
\begin{tabular}
{p{2.5cm}p{1.9cm}p{1.9cm}p{1.9cm}p{1.9cm}|p{1.9cm}}
\hline
\textbf{}      & Art   & Clipart & Product & Real-World & Avg   \\
\hline
Deep All (feat.) & 52.66 & 48.35   & 71.37   & 71.47      & 60.96 \\
 (feat.) & 54.55 & \textbf{49.37}   & 71.38   & \textbf{72.17}      & \textbf{61.87} \\
 (feat.)     & 54.53    & 49.04      & 71.57      & 71.90         & 61.76    \\
 (feat.) & 54.54 & 49.05 & \textbf{71.58} & 72.03 & 61.80 \\
\hline
Deep All       & 55.59 & 42.42   & 70.34   & 70.86      & 59.81 \\
D-SAMs          & \textbf{58.03} & 44.37   & 69.22   & 71.45      & 60.77
\end{tabular}
\end{table}