\documentclass{sig-alternate}

\newtheorem{lemma}{Lemma}
\usepackage{amsmath,url,cite,mathrsfs}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subfigure}


\begin{document}


\title{A Very Efficient Scheme for Estimating Entropy of Data Streams Using Compressed Counting}


\numberofauthors{1}
\author{
\alignauthor
Ping Li\\
       \affaddr{Cornell University}\\
              \affaddr{Ithaca, NY 14850}\\
       \email{pingli@cornell.edu}
}


\maketitle

\noindent \today
\begin{abstract}
\textbf{\em Compressed Counting (CC)} was recently proposed for approximating the th frequency moments of data streams, for . Under the {\em relaxed strict-Turnstile} model, CC dramatically improves the standard  algorithm based on {\em symmetric stable random projections}, especially as .
A direct application of CC is to estimate the entropy, which is an important summary statistic in Web/network measurement and often serves a crucial ``feature'' for data mining.  The R\'enyi entropy and the Tsallis entropy  are functions of the th frequency moments; and both approach the Shannon entropy as . A recent theoretical work suggested using the th frequency moment to approximate the Shannon entropy with  and very small  (e.g., ).


In this study, we experiment using CC to estimate frequency moments, R\'enyi entropy, Tsallis entropy, and  Shannon entropy, on real Web crawl data. We demonstrate the variance-bias trade-off in estimating  Shannon entropy and provide practical recommendations. In particular, our experiments enable us to draw some important conclusions:
\begin{itemize}
\item As , CC dramatically improves {\em symmetric stable random projections} in estimating frequency moments, R\'enyi entropy, Tsallis entropy, and Shannon entropy. The improvements appear to approach ``infinity.''
\item CC is a highly practical algorithm for estimating Shannon entropy (from either R\'enyi or Tsallis entropy) with . Only a very small sample (e.g., 20) is needed to achieve a high accuracy (e.g.,  relative errors).
\item Using {\em symmetric stable random projections} and  with very small  does not provide a practical algorithm because the required sample size is enormous.
\item If we do need to use {\em symmetric stable random projections} for estimating Shannon entropy, we should exploit the variance-bias trade-off by letting  be away from 1, for much better performance.
\item Even in terms of the best achievable performance in estimating Shannon entropy, CC still considerably improves {\em symmetric stable random projections} by one or two magnitudes, both in terms of the estimation accuracy and the required sample size (storage space). 
\end{itemize}


\end{abstract}
\section{Introduction}

The general theme of ``scaling up for high dimensional data and high speed data streams'' is among the  ``ten challenging problems in data mining research''\cite{Article:ICDM10}. This paper focuses on a very efficient algorithm for estimating the entropy of data streams using a recently developed randomized algorithm called \textbf{\em Compressed Counting (CC)} by Li \cite{Article:Li_CC_v0,Article:Li_CC,Report:Li_CC_oq}. The underlying technique of CC is {\em maximally-skewed stable random projections}. Our experiments on real Web crawl data demonstrate that CC can approximate entropy with very high accuracy. In particular, CC (dramatically) improves {\em symmetric stable random projections} (Indyk \cite{Article:Indyk_JACM06} and Li \cite{Proc:Li_SODA08}) for estimating entropy, under the {\em relaxed strict-Turnstile} model.


\subsection{Data Streams and Relaxed\\ Strict-Turnstile Model}

While traditional machine learning and mining algorithms often assume static data, in reality, data are often constantly updated. Mining data streams\cite{Book:Henzinger_99,Proc:Babcock_PODS02,Proc:Aggarwal_KDD04,Article:Muthukrishnan_05} in (e.g.,) 100 TB scale  databases has become an important area of research, as network data can easily reach that scale\cite{Article:ICDM10}. Search engines are a typical source of data streams (Babcock {\em et.al.} \cite{Proc:Babcock_PODS02}).

We consider the  {\em Turnstile}  stream model (Muthukrishnan \cite{Article:Muthukrishnan_05}). The input  stream ,  arriving sequentially describes the underlying signal , meaning
 where the increment  can be either positive (insertion) or negative (deletion). For example, in an online bookstore,  may record the total number of books that user  has ordered up to time  and  denotes the number of books that this user orders () or cancels () at  .

It is often reasonable to assume , although  may be either negative or positive. Restricting  results in the {\em strict-Turnstile} model, which suffices for describing almost all natural phenomena. For example, in an online store, it is  not possible to cancel orders that do not exist.

\textbf{Compressed Counting (CC)} assumes a {\em relaxed strict-Turnstile} model by only enforcing  at the  one cares about.  At other times ,  CC allows  to be arbitrary. This is more general than the {\em strict-Turnstile} model.




\subsection{Moments and Entropies of Data Streams}

The th frequency moment is a fundamental statistic:

\noindent When ,  is the sum of the stream. It is obvious that one can compute  exactly and trivially using a simple counter, because .


 is basically a histogram and we can view  as probabilities. An extremely useful (especially in Web and networks\cite{Proc:Zhao_IMC07,Proc:Mei_WSDM08}) summary statistic is the Shannon entropy:


Various generalizations of the Shannon entropy exist. The R\'enyi entropy\cite{Proc:Renyi_61}, denoted by , is defined as

The Tsallis entropy\cite{Article:Havrda_67,Article:Tsallis_88}, denoted by ,  is defined as,

\noindent which was first introduced by Havrda and Charv\'at\cite{Article:Havrda_67} and later popularized by Tsallis\cite{Article:Tsallis_88}.

It is easy to verify that, as , both the R\'enyi entropy and Tsallis entropy  converge to the Shannon entropy. Thus  in the limit sense. For this fact, one can also consult \url{http://en.wikipedia.org/wiki/Renyi_entropy}.

Therefore, both the R\'enyi entropy and Tsallis entropy can be computed from the th frequency moment; and one can approximate the Shannon entropy from either  or  by using . In fact, several studies (Zhao {\em et.al.} \cite{Proc:Zhao_IMC07} and Harvey {\em et.al.} \cite{Article:Harvey_entropy_arXiv08,Proc:Harvey_FOCS08})   have used this idea to approximate the Shannon entropy. 

We should mention that \cite{Article:Li_CC} proposed estimating the logarithmic moment, , using  with . Their idea is very similar to that in estimating entropy. 


\subsection{Challenges in Data Stream Computations}

Because the elements, , are time-varying,  a na\'ive counting mechanism requires a system of  counters to compute  exactly (unless ). This is not always realistic when  is large and the data are  frequently updated at very high rate.  For example, if  records activities for each user , identified by his/her IP address, then potentially  (possibly much larger in the near future).

Due to the huge volume, streaming data are often not (fully) stored, even on disks\cite{Proc:Babcock_PODS02}.  One common strategy is to store only a small ``sample'' the data; and sampling has become an important topic in Web search and data streams\cite{Proc:Henzinger_WWW00,Proc:Bar-Yossef_VLDB00,Article:Henzinger_03}. While some modern databases (e.g., Yahoo!'s 2-petabyte database) and government agencies do store the whole data history, the data analysis  often has to be conducted on a (hopefully) representative small sample of the data. As it is well-understood that  general-purpose simple sampling-based methods often can not give reliable approximation guarantees\cite{Proc:Babcock_PODS02}, developing special-purpose (and one-pass) sampling/sketching techniques in streaming data has become an active area of research.

\subsection{Previous Studies on Approximating Frequency Moments and Entropy}

Pioneered by Alon {\it et.al.}\cite{Proc:Alon_STOC96}, the problem of approximating  in data streams has been heavily studied\cite{Proc:Feigenbaum_FOCS99,Proc:Indyk_FOCS00,Proc:Saks_STOC02,Proc:Kumar_FOCS02,Proc:Indyk_STOC05,Proc:Woodruff_SODA04,Proc:Ganguly_RANDOM07}. The method of {\em symmetric stable random projections} (Indyk \cite{Article:Indyk_JACM06}, Li \cite{Proc:Li_SODA08}) is regarded to be  practical and accurate.

We have mentioned that computing the first moment  in {\em strict-Turnstile} model is trivial using a simple counter. One might naturally speculate that when , computing (approximating)  should be also easy. However, none of the previous algorithms including {\em symmetric stable random projections} could capture this intuition. For example, Figure \ref{fig_comp_var_factor} in Section \ref{sec_CC} shows that the performance of {\em symmetric stable random projections} is roughly the same for  and , even though  should be trivial.

\textbf{Compressed Counting (CC)}\cite{Article:Li_CC_v0,Article:Li_CC,Report:Li_CC_oq} was recently proposed to overcome the drawback of previous algorithms at . CC improves {\em symmetric stable random projections} uniformly for all  and the improvement is in a sense ``infinite'' when  as shown in Figure \ref{fig_comp_var_factor} in Section \ref{sec_CC}. However, no empirical studies on CC have been reported.

Zhao {\em et.al.}\cite{Proc:Zhao_IMC07} applied {\em symmetric stable random projections} to approximate the Shannon entropy. \cite{Article:Li_CC} cited \cite{Proc:Zhao_IMC07}, as one application of {\em Compressed Counting} (CC).  A nice  theoretical paper in FOCS'08 by Harvey {\em et.al.}\cite{Article:Harvey_entropy_arXiv08,Proc:Harvey_FOCS08} provided the criterion to choose the  so that the Shannon entropy can be approximated with a guaranteed accuracy, using the th frequency moment. \cite{Proc:Harvey_FOCS08} cited both {\em symmetric stable random projections}\cite{Article:Indyk_JACM06,Proc:Li_SODA08} and {\em Compressed Counting}\cite{Article:Li_CC}.


There are other methods for estimating entropy, e.g., \cite{Proc:Guha_SODA06}, which we do not compare with in this study.

\subsection{Summary of Our Contributions}

Our main contribution is the first empirical study of Compressed Counting for estimating entropy.  Some theoretical analysis is also conducted.
\begin{itemize}
\item  We apply Compressed Counting (CC) to compute the R\'enyi entropy, the Tsallis entropy, and the Shannon entropy, on real Web crawl data.
\item We empirically compare CC with {\em symmetric stable random projections} and demonstrate the huge improvement. Thus, our work helps establish CC as a promising practical tool in data stream computations.
\item We provide some theoretical analysis for approximating entropy, for example, the variance-bias trade-off.
\item Our empirical work leads to practical recommendations for various estimators developed in \cite{Article:Li_CC_v0,Article:Li_CC,Report:Li_CC_oq}.
\end{itemize}

For estimating the Shannon entropy, the theoretical work by Harvey {\em et.al.}\cite{Article:Harvey_entropy_arXiv08,Proc:Harvey_FOCS08} used {\em symmetric stable random projections} or CC as a subroutine (a two-stage ``black-box'' approach). That is, they first determined at what  value,  (or ) is close to  within a required accuracy. Then they used this chosen th frequency moment to approximate the Shannon entropy, independent of whether the frequency moments are estimated using CC or {\em symmetric stable random projections}.

In comparisons, we demonstrate that  estimating Shannon entropy is  a variance-bias trade-off; and hence the performance is highly coupled with the underlying estimators. The two-stage ``black-box'' approach   \cite{Article:Harvey_entropy_arXiv08,Proc:Harvey_FOCS08} may have  some theoretical advantage (e.g., simplifying the analysis), while our variance-bias analysis directly reflects the real-world situation and leads to practical recommendations.
\begin{itemize}
\item  \cite{Article:Harvey_entropy_arXiv08,Proc:Harvey_FOCS08} let  and provided the procedures to compute  (or a series of 's). If one actually carries out the calculation, their  is very small (like  or smaller). Consequently their theoretically calculated sample size may be (impractically) large, especially when using {\em symmetric stable random projections}.
\item In comparison, we  provide a practical recommendation for estimating Shannon entropy: using CC with  and the {\em optimal quantile} estimator. Only a  small sample (e.g., 20) can achieve a high accuracy (e.g.,  relative errors).
\item We  demonstrate that due to the variance-bias trade-off, there will be an ``optimal''  value that could attain the best mean square errors for estimating the Shannon entropy. This optimal   can be quite away from 1  when using {\em symmetric stable random projections}.
\end{itemize}

\subsection{Organization}

Section \ref{sec_entropy} reviews  some applications of entropy. The basic methodologies of CC and various estimators for recovering the th frequency moments are reviewed in Section \ref{sec_CC}. We analyze in Section \ref{sec_entropy_est} the biases and variances in estimating entropies.  Experiments on real Web crawl data are presented in Section \ref{sec_exp}. Finally, Section \ref{sec_conclusion} concludes the paper.

\section{Some Applications of Entropy}\label{sec_entropy}

\subsection{The Shannon Entropy}

The Shannon entropy,  defined in (\ref{eqn_Shannon}), is a fundamental measure of randomness. A recent paper in WSDM'08 (Mei and Church \cite{Proc:Mei_WSDM08}) was devoted to estimating the Shannon entropy of MSN search logs, to help answer some basic problems in Web search, such as,  {\em how big is the web?}

The search logs can be naturally viewed as data streams, although \cite{Proc:Mei_WSDM08} only analyzed several ``snapshots'' of a sample of MSN search logs.  The sample used in \cite{Proc:Mei_WSDM08} contained 10 million <Query, URL,IP> triples; each triple corresponded to a click from a particular IP address on a particular URL for a particular query.  \cite{Proc:Mei_WSDM08} drew their important conclusions on this (hopefully) representative sample. We believe one can (quite easily) apply Compressed Counting (CC) on the same task, on the whole history of MSN (or other search engines) search logs instead of a (static) sample.\\


Using the Shannon entropy as an important ``feature'' for mining anomalies is a widely used technique (e.g., \cite{Proc:Lakhina_SIGCOMM05}). In IMC'07, Zhao {\it et.al.}\cite{Proc:Zhao_IMC07} applied {\em symmetric stable random projections} to estimate the Shannon entropy for all origin-destination (OD) flows in network measurement, for clustering traffic and detecting traffic anomalies.

Detecting anomaly events in real-time (DDoS attacks, network failures, etc.) is highly beneficial in monitoring network performance degradation and service disruptions. Zhao {\it et.al.}\cite{Proc:Zhao_IMC07}  hoped to capture those events in real-time by examining the entropy of every OD flow.  They resorted to approximate algorithms because measuring the Shannon entropy in real-time is not possible on high-speed links due to its memory requirements and high computational cost.\\


\subsection{The R\'enyi Entropy}

The R\'enyi entropy,  defined in (\ref{eqn_Renyi}), is a generalization of the classical Shannon entropy .  is function of the frequency moment  and approaches  as . Thus it is natural to use  with  to approximate .

The R\'enyi entropy has other applications. It is a diversity index in ecology \cite{Article:Tothmeresz_95,Article:Ricotta_02,Article:Liu_06}. It is used for analyzing expander graphs\cite{Article:Hoory_06} and other applications, e.g. \cite{Article:Zyczkowski_03}.


\subsection{The Tsallis Entropy}

The Tsallis entropy,   defined in (\ref{eqn_Tsallis}), is another generalization of the Shannon entropy . Since  as , the Tsallis entropy provides another algorithm for approximating the Shannon entropy.

The Tsallis entropy is widely used in statistical physics and  mechanics. Interested readers may consult the link \\ \url{www.cscs.umich.edu/~crshalizi/notabene/tsallis.html}.


\section{Review Compressed Counting (CC)}\label{sec_CC}


Compressed Counting (CC) assumes the {\em relaxed strict-Turnstile} data stream model. Its underlying technique is based on {\em maximally-skewed stable random projections}.

\subsection{Maximally-Skewed Stable Distributions}

A random variable
 follows a maximally-skewed -stable distribution if the Fourier transform of its density  is\cite{Book:Zolotarev_86}

where , , and . We denote .
The skewness parameter  for general stable distributions ranges in ; but CC uses , i.e., maximally-skewed. Previously, the method of {\em symmetric stable random projections}\cite{Article:Indyk_JACM06,Proc:Li_SODA08} used .



Consider two independent variables, . For any non-negative constants  and , the ``-stability'' follows from  properties of Fourier transforms:


Note that if , then the above stability holds for any constants  and . This is why {\em symmetric stable random projections}\cite{Article:Indyk_JACM06,Proc:Li_SODA08} can work on general data but CC only works on non-negative data  (i.e., {\em relaxed strict-Turnstile model}). Since we are interested in the entropy, the non-negativity constraint is natural, because the probability should be non-negative.

\subsection{Random Projections}

Conceptually, one can generate a  matrix  and multiply it with the data stream , i.e., . The resultant vector  is only of length . The entries of , ,  are i.i.d. samples of a stable distribution .

By property of Fourier transforms, the entries of ,   to ,  are i.i.d. samples of a stable distribution
{\small}
\noindent whose scale parameter  is exactly the th frequency moment of .

Therefore, CC boils down to a statistical estimation problem. If we can estimate the scale parameter from  samples, we can then estimate the frequency moments and  entropies.

For real implementations, one should conduct  incrementally. This is possible because the {\em Turnstile} model (\ref{eqn_Turnstile}) is  a linear updating model. That is, for every incoming , we update  for  to .  Entries of  are generated on-demand as necessary.

\subsection{The Efficiency in Processing Time}

Ganguly and Cormode \cite{Proc:Ganguly_RANDOM07} commented that, when  is large, generating entries of  on-demand and multiplications ,  to , can be prohibitive when  data arrive at very high rate.  This can be a drawback of {\em stable random projections}. An easy ``fix'' is to use  as small as possible.

At the same , all procedures of CC and {\em symmetric stable random projections} are the same except the entries in  follow different distributions. Thus, both methods have the same efficiency in processing time at the same . However, since CC is much more accurate especially when , it requires a much smaller   for reaching a specified level of accuracy. For example, while using {\em symmetric stable random projections} with  is prohibitive, using CC with  only may be practically feasible.  Therefore, CC in a sense naturally provides a solution to the problem of processing efficiency. 

\subsection{Three Statistical Estimators for CC}

In this study, we consider three estimators from \cite{Article:Li_CC_v0,Article:Li_CC,Report:Li_CC_oq}, which are promising for good performance near .


Recall CC boils down to estimating the scale parameter  from  i.i.d. samples .

\subsubsection{The Geometric Mean Estimator}
\vspace{-0.1in}




This estimator is strictly unbiased, i.e., ,  and its asymptotic (i.e., as ) variance is
{\scriptsize }
\noindent As , the asymptotic variance  approaches zero.

The geometric mean estimator is  important for theoretical analysis. For example, \cite{Article:Li_CC} showed that when  (i.e., ), the ``constant''  in its sample complexity bound  approaches  at the rate of . That is, as , the complexity becomes  instead of  . Note that  is the well-known large-deviation bound for {\em symmetric stable random projections}. 
The sample complexity bound determines the sample size  needed for achieving a relative accuracy within a  factor of the truth.

In many theory papers, the ``constants'' in tail bounds are often ignored. The geometric mean estimator for CC demonstrates that in special cases the ``constants'' may be so small that they should not be treated as ``constants'' any more. 

\subsubsection{The Harmonic Mean Estimator}


which is asymptotically unbiased and has variance
{\small}

 is defined only for  and is considerably more accurate than the geometric mean estimator .

\subsubsection{The Optimal Quantile Estimator}


where


To compute , one sorts ,  to  and uses the th smallest, i.e., .   is chosen to minimize the asymptotic variance.

\cite{Report:Li_CC_oq} provides the values for , , as well as the asymptotic variances. For convenience, we tabulate the values for  in Table \ref{tab_oq}. The last column contains the asymptotic variances (with ) without the  factor.

\begin{table}[h]
\caption{\small
 }
\begin{center}{\scriptsize
\begin{tabular}{l l l l}
\hline \hline
 &  &  &Var\\\hline
0.80 &0.108 &2.256365  &0.15465894  \\
0.90 &0.101 &5.400842 \  &0.04116676 \\
0.95 &0.098 &11.74773 &0.01059831    \\
0.98  & 0.0944 &30.82616 & 0.001724739  \\
0.989 & 0.0941 &56.86694 &0.0005243589 \\
1.011 & 0.8904 &58.83961  &0.0005554749 \\
1.02 & 0.8799 &32.76892  &0.001901498  \\
1.05 & 0.855 &13.61799   &0.01298757 \\
1.10 & 0.827 &7.206345   &0.05717725 \\
1.20 & 0.799 &4.011459   &0.2516604 \\
\hline\hline
\end{tabular}
}
\end{center}
\label{tab_oq}
\end{table}


Compared with the geometric mean and harmonic mean estimators,  and , the optimal quantile estimator  has some noticeable advantages:
\begin{itemize}
\item When the sample size  is not too small (e.g., ),  is more accurate then , especially for . It is also more accurate than , when  is close to 1. Our experiments will verify this point.
\item  is computationally more efficient because both  and  require  fractional power operations, which are expensive.
\end{itemize}

The drawbacks of the optimal quantile estimator are:
\begin{itemize}
\item For small samples (e.g., ),  exhibits bad behaviors when .
\item Its theoretical analysis, e.g., variances and tail bounds, is based on the density function of skewed stable distributions, which do not have closed-forms.
\item The  parameters,  and , are obtained from the numerically-computed density functions. \cite{Report:Li_CC_oq} provided  and  values for  and .
\end{itemize}

\subsubsection{The Geometric Mean Estimator for Symmetric Stable Random Projections}


For {\em symmetric stable random projections}, the following geometric mean estimator is close to be statistically optimal when  \cite{Proc:Li_SODA08}:

\noindent where .

Therefore, we only compare CC with this estimator, which was explicitly used  in \cite{Article:Harvey_entropy_arXiv08,Proc:Harvey_FOCS08}  for the task of residual moment estimation for the general Turnstile model. 

\subsubsection{Comparisons of Asymptotic Variances}

Figure \ref{fig_comp_var_factor} compares the variances of the three estimators for CC, as well as the geometric mean estimator for {\em symmetric stable random projections}.

\begin{figure}[h]
\begin{center}
\includegraphics[width = 2.3in]{comp_var_factor.eps}
\end{center}
\vspace{-0.3in}
\caption{Let  be an estimator of  with asymptotic variance . We plot the  values for the {\em geometric mean} estimator,  the {\em harmonic mean} estimator (for ), and the {\em optimal quantile} estimator, along with the  values for the {\em geometric mean} estimator for {\em symmetric stable random projections} in \cite{Proc:Li_SODA08} (``symmetric GM'').
}\label{fig_comp_var_factor}
\end{figure}


\subsection{Sampling from Maximally-Skewed Stable Random Distributions}


The standard procedure for sampling from skewed stable distributions is based on the Chambers-Mallows-Stuck method\cite{Article:Chambers_JASA76}. One first generates an exponential random variable with mean 1, ,  and a uniform random variable , then,

where  when  and  when . \\



Sampling from symmetric () stable distributions uses the same procedure with . Thus, the only difference is the  term, which is  a constant and can be removed out of the sampling procedure and put back to the estimates in the end, which in fact also provides better numerical stability when . Note  that the estimators (\ref{eqn_F_gm}) and (\ref{eqn_F_hm}) already contain  in the numerators. Thus, we can sample  instead of  and evaluate (\ref{eqn_F_gm}) (\ref{eqn_F_hm}) without .


\section{Estimating Entropies Using CC}\label{sec_entropy_est}

The basic procedure is to first estimate the th frequency moment  using CC and then compute various entropies using the estimated . Here we use  to denote a generic estimator of , which could be ,  ,  ,  or .

In the following subsections, we analyze the variances and biases in estimating the R\'enyi entropy , the Tsallis entropy , and the Shannon entropy .

\subsection{R\'enyi Entropy}

We denote a generic estimator of  by :

which becomes ,  ,  ,  and , respectively, when  becomes ,  ,  ,  or . Since  can be computed exactly and trivially using a simple counter, we assume it is a constant.

Since  is unbiased or asymptotically unbiased,  is also asymptotically unbiased. The asymptotic variance of  can be computed by Taylor expansions (the so-called ``delta method'' in statistics):




\subsection{Tsallis Entropy}
The generic estimator for the Tsallis entropy  would be

which is asymptotically unbiased and has variance


\subsection{Shannon Entropy}


We use  and  to denote the estimators for Shannon entropy using the estimated  and , respectively.

The variances remain unchanged, i.e.,



However,  and  are no longer unbiased, even asymptotically (unless ). The biases would be

The   biases arise from the estimation biases in  and  and diminish  quickly as  increases. In fact, there are standard statistics procedures to reduce the  bias to .  However, the ``intrinsic biases,''  and , can not be removed by increasing ; they can only be reduced by letting  close to 1.



The total error is usually measured by the mean square error: MSE = Bias + Var. Clearly, there is a variance-bias trade-off in estimating  using  or .  For a particular data stream, at each sample size , there will be an optimal  to attain the smallest MSE. The optimal  is data-dependent and hence some prior knowledge of the data is needed in order to determine it. The prior knowledge may be accumulated during the data stream process. Alternatively, we could seek an estimator that is very accurate near  to alleviate the variance-bias affect.

\section{Experiments}\label{sec_exp}

The goal of the experimental study is to demonstrate the effectiveness of Compressed Counting (CC) for estimating entropies and to determine a good strategy for estimating the Shannon entropy.  In particular, we focus on the estimation accuracy and would like to verify the formulas for (asymptotic) variances in (\ref{eqn_Renyi_est_var}) and (\ref{eqn_Tsallis_est_var}).  

\subsection{Data}

Since the estimation accuracy is what we are interested in, we can simply use static data instead of real data streams. This is because the projected data vector  is the same, regardless whether it is computed at once  (i.e., static) or incrementally (i.e., dynamic). As we have commented, the processing and storage cost of CC is the same as the cost of {\em symmetric stable random projections} at the same sample size . Therefore, to compare these two methods, it suffices to compare their estimation accuracies.

Ten English words are selected from a chunk of Web crawl data with  pages: THE, A, THIS, HAVE, FUN, FRIDAY, NAME, BUSINESS, RICE, and TWIST. The words are selected fairly randomly, except that we make sure they cover a whole range of sparsity, from function words (e.g., A, THE), to  common words (e.g., FRIDAY) to rare words (e.g., TWIST).

Thus, as summarized in Table \ref{tab_data},  our data set consists of ten vectors of length  and the entries are the numbers of word occurrences in each document.

Table \ref{tab_data} indicates that the R\'enyi entropy  provides a much better approximation to the Shannon entropy , than the Tsallis entropy  does. On the other hand, if the purpose is to find a summary statistic that is different from the Shannon entropy (i.e., sensitive to ), then the Tsallis entropy may be more suitable.


\begin{table}[h]
\caption{\small  The data set consists of 10 English words selected from a chunk of  Web pages, forming 10 vectors of length  whose values are the word occurrences. The table lists their numbers of non-zeros (sparsity), the Shannon entropy , the R\'enyi entropy  and the Tsallis entropy  (for  and 1.05).
 }
\begin{center}{\scriptsize\tiny
\begin{tabular}{l l l l l l l}
\hline \hline\\
Word &Nonzero  &  &  &   &  & \\  \\\hline
TWIST &274 &5.4873 &5.4962 &5.4781 &6.3256 &4.7919\\
RICE &490 &5.4474 &5.4997 &5.3937 &6.3302 &4.7276\\
FRIDAY &2237 &7.0487 &7.1039 &6.9901 &8.5292 &5.8993 \\
FUN &3076 & 7.6519   & 7.6821 &    7.6196  &   9.3660  &   6.3361\\
BUSINESS &8284 &8.3995 &8.4412 &8.3566 &10.502 &6.8305\\
NAME & 9423 &8.5162 &9.5677 &8.4618 &10.696 &6.8996\\
HAVE & 17522  &8.9782 &9.0228 & 8.9335 & 11.402 & 7.2050\\
THIS & 27695  &9.3893 &9.4370 &9.3416 &12.059 &7.4634 \\
A    & 39063  &9.5463  &9.5981  &9.4950  &12.318   &7.5592\\
THE  & 42754  & 9.4231 &9.4828  &9.3641  &12.133  &7.4775\\
\hline\hline
\end{tabular}
}
\end{center}
\label{tab_data}
\end{table}
\begin{figure}[h]
\begin{center}\mbox{
{\includegraphics[width=1.75in]{twist_F_gm.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{twist_F_hm.eps}}}\\\mbox{
{\includegraphics[width=1.75in]{twist_F_oq.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{twist_F_gm_sym.eps}}
}
\end{center}
\vspace{-0.15in}
\caption{Frequency moments, , for TWIST.  Solid curves are   empirical mean square errors (MSEs) and dashed curves are theoretical asymptotic variances in (\ref{eqn_F_gm_var}), (\ref{eqn_F_hm_var}), (\ref{eqn_F_gm_sym_var}), and Table \ref{tab_oq}.  ``F,gm'' stands for the geometric mean estimator   (\ref{eqn_F_gm}), ``F,hm'' for the harmonic mean estimator    (\ref{eqn_F_hm}), ``F,oq'' for the optimal quantile estimator  (\ref{eqn_F_oq}), and ``F,gm,sym'' for the geometric mean estimator  (\ref{eqn_F_gm_sym}) in {\em symmetric stable random projections}. }\label{fig_twist_F}
\end{figure}

\subsection{Results}

The results for estimating frequency moments, R\'enyi entropy, Tsallis entropy, and Shannon entropy are presented in the following subsections, in terms of the normalized (i.e., relative) mean square errors (MSEs), e.g., , , etc. After normalization, we observe that the results are quite similar across different words. To avoid boring the readers, not all words are selected for the presentation. However, we provides the experimental results for all 10 words, in estimating Shannon entropy.

In our experiments, the sample size  ranges from  to . We choose  and . This is because \cite{Report:Li_CC_oq} only provided the optimal quantile estimator for  and . For the geometric mean and harmonic mean estimators, we actually had no problem of using (e.g.,)  or .

\subsubsection{Estimating Frequency Moments}




Figure \ref{fig_twist_F}, Figure \ref{fig_rice_F}, and Figure \ref{fig_friday_F} provide the MSEs for estimating the th  frequency moments, , for TWIST, RICE, and FRIDAY, respectively.
\begin{itemize}
\item The errors of the three estimators for CC decrease (to zero, potentially) as , while the errors of {\em symmetric stable random projections} do not vary much near . The improvement of CC is enormous as . For example, when  and , the MSE of CC using the optimal quantile estimator is about  while the MSE of {\em symmetric stable random projections} is about , a 10000-fold error reduction.
\item The optimal quantile estimator  is in general more accurate than the geometric mean and harmonic mean estimators near . However, for small  (e.g., 20) and ,  exhibits some bad behaviors, which disappear when  (or even ).
\item The theoretical asymptotic variances in (\ref{eqn_F_gm_var}), (\ref{eqn_F_hm_var}), (\ref{eqn_F_gm_sym_var}), and Table \ref{tab_oq} are accurate.
\end{itemize}

\newpage

\begin{figure}[h]
\begin{center}\mbox{
{\includegraphics[width=1.75in]{rice_F_gm.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{rice_F_hm.eps}}}\\\mbox{
{\includegraphics[width=1.75in]{rice_F_oq.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{rice_F_gm_sym.eps}}}
\end{center}
\vspace{-0.15in}
\caption{ Frequency moments, , for RICE. See the caption of Figure \ref{fig_twist_F} for more explanations.}\label{fig_rice_F}
\end{figure}
\vspace{-0.2in}
\begin{figure}[h]
\begin{center}\mbox{
{\includegraphics[width=1.75in]{friday_F_gm.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{friday_F_hm.eps}}}\\\mbox{
{\includegraphics[width=1.75in]{friday_F_oq.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{friday_F_gm_sym.eps}}\vspace{-0.1in}
}
\end{center}
\vspace{-0.15in}
\caption{ Frequency moments, , for FRIDAY. }\label{fig_friday_F}
\end{figure}


\vspace{0.2in}
\subsubsection{Estimating R\'enyi Entropy}

Figure \ref{fig_twist_H} plots the MSEs for estimating the R\'eny entropy for TWIST,  with the curves for  removed.
The figure illustrates that: (1) CC improves {\em symmetric stable rand projections} enormously when ; (2) The generic variance formula (\ref{eqn_Renyi_est_var}) is accurate.

\begin{figure}[h]
\begin{center}\mbox{
{\includegraphics[width=1.75in]{twist_H_gm.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{twist_H_hm.eps}}}\\\mbox{
{\includegraphics[width=1.75in]{twist_H_oq.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{twist_H_gm_sym.eps}}
}
\end{center}
\vspace{-0.15in}
\caption{R\'eny entropy, , for TWIST. The theoretical variances (dashed) are computed from (\ref{eqn_Renyi_est_var}).  }\label{fig_twist_H}
\end{figure}


\vspace{0.2in}

\subsubsection{Estimating Tsallis Entropy}


Figure \ref{fig_rice_T} plots the MSEs for estimating the Tsallis entropy for RICE, illustrating that: (1) CC improves {\em symmetric stable rand projections} enormously when ; (2) The generic variance formula (\ref{eqn_Tsallis_est_var}) is accurate.

\begin{figure}[h]
\begin{center}\mbox{
{\includegraphics[width=1.75in]{rice_T_gm.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{rice_T_hm.eps}}}\\\mbox{
{\includegraphics[width=1.75in]{rice_T_oq.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{rice_T_gm_sym.eps}}
}
\end{center}
\vspace{-0.15in}
\caption{Tsallis entropy, , for RICE. The theoretical variances (dashed) are computed from (\ref{eqn_Tsallis_est_var}). }\label{fig_rice_T}
\end{figure}

\vspace{0.2in}
\subsubsection{Estimating Shannon Entropy\\ from R\'enyi Entropy}


\begin{figure}[h]
\begin{center}\mbox{
{\includegraphics[width=1.75in]{rice_HR_gm.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{rice_HR_hm.eps}}}\vspace{-0.1in}
\mbox{
{\includegraphics[width=1.75in]{rice_HR_oq.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{rice_HR_gm_sym.eps}}\vspace{-0.1in}
}
\end{center}
\vspace{-0.25in}
\caption{Shannon entropy, , estimated from R\'enyi entropy, , for RICE. Curves are the mean square errors (MSEs). }\label{fig_rice_HR}
\end{figure}

Figure \ref{fig_rice_HR} illustrates the MSEs from estimating the Shannon entropy using the R\'enyi entropy, for RICE.
\begin{itemize}
\item Using {\em symmetric stable random projections} with  and very small  is not a good strategy and not practically feasible because the required sample size is enormous. For example, using , we need  in order to achieve a relative MSE of . 
\item There is clearly a variance-bias trade-off, especially for the geometric mean and harmonic mean estimator.  That is, for each , there is an ``optimal''  which achieves the smallest MSE.
\item Using the optimal quantile estimator does not show a strong variance-bias trade-off, because its has very small variance near  and its MSEs are mainly dominated by the (intrinsic) biases, .
\item The improvement of CC over {\em symmetric stable random projections} is very large when  is close 1. When  is away from 1, the improvement becomes less obvious because the MSEs are dominated by the biases.
\item
Using the optimal quantile estimator with  very close to 1 (preferably  ) is our recommended procedure for estimating Shannon entropy from R\'enyi entropy.
\end{itemize}


For a fixed  and , we can see that CC improves {\em symmetric stable random projections} enormously when .  If we follow the theoretical suggestion of \cite{Article:Harvey_entropy_arXiv08,Proc:Harvey_FOCS08} by using (e.g.) , then the improvement of CC over {\em symmetric stable random projections} will be enormous.

As a practical recommendation, we do not suggest letting  too close to 1 when using {\em symmetric stable random projections}. Instead, one should take advantage of the variance-bias trade-off by using  away from 1. There will be an ``optimal''  that attains the smallest mean square error (MSE), at each . 


As illustrated in  Figure \ref{fig_rice_HR}, CC is not affected much by the variance-bias trade-off and it is preferable to choose   close to 1 when using the optimal quantile estimator. Therefore, we will present the comparisons mainly in terms of the minimum MSEs (i.e., best achievable performance), which we believe actually heavyily favors {\em symmetric stable random projections}.

\clearpage

Figures \ref{fig_HR_min} presents the minimum MSEs for all 10 words:
\begin{itemize}
\item The optimal quantile estimator is the most accurate. For example, using , the relative MSE is only less than  (or even ), which may be already accurate enough for some applications.
\item For every , CC reduces the (minimum) MSE roughly by 20- to 50-fold, compared to {\em symmetric stable random projections}. This is comparing the curves in the vertical direction.
\item To achieve the same accuracy as {\em symmetric stable random projections}, CC requires a much smaller , a reduction by about 50-fold (using the optimal quantile estimator). This is comparing the curves in the horizontal direction.
\item The results are quite similar for all 10 words. While it is boring to present all 10 words, the results deliver a strong hint that the performance of CC and its improvement over {\em symmetric stable random projections} should hold
    universally, not just for these 10 words.
\end{itemize}

\begin{figure}[h]
\begin{center}\mbox{
{\includegraphics[width=1.75in]{twist_HR_min.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{rice_HR_min.eps}}}
\mbox{
{\includegraphics[width=1.75in]{friday_HR_min.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{fun_HR_min.eps}}}
\mbox{
{\includegraphics[width=1.75in]{business_HR_min.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{name_HR_min.eps}}}
\mbox{
{\includegraphics[width=1.75in]{have_HR_min.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{this_HR_min.eps}}
}
\mbox{
{\includegraphics[width=1.75in]{a_HR_min.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{the_HR_min.eps}}
}
\end{center}
\vspace{-0.15in}
\caption{Shannon entropy, , estimated from R\'enyi  entropy, , for 10 words in Table \ref{tab_data}. Curves are the minimum MSEs at each .  }\label{fig_HR_min}
\end{figure}



\subsubsection{Estimating Shannon Entropy\\ from Tsallis Entropy}

\begin{figure}[h]
\begin{center}\mbox{
{\includegraphics[width=1.75in]{rice_HT_gm.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{rice_HT_hm.eps}}}
\mbox{
{\includegraphics[width=1.75in]{rice_HT_oq.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{rice_HT_gm_sym.eps}}
}
\end{center}
\vspace{-0.15in}
\caption{ Shannon entropy, , estimated from Tsallis entropy, , for RICE. Curves are MSEs.}\label{fig_rice_HT}
\end{figure}


Figure \ref{fig_rice_HT} illustrates the MSEs from estimating Shannon entropy using Tsallis entropy, for RICE:
\begin{itemize}
\item Using {\em symmetric stable random projections} with  and very small  is not a good strategy and not practically feasible. For example, when , using  can only achieve a relative MSE of .
\item The effect of the variance-bias trade-off for geometric mean and harmonic mean estimators, is even more significant, because the (intrinsic) bias  is large, as reported in Table \ref{tab_data}
\item The MSEs of the optimal quantile estimator is not affected much by , because its variance is negligible compared to the (intrinsic) bias.
\end{itemize}


Figures \ref{fig_HT_min} presents the minimum MSEs for all 10 words:
\begin{itemize}
\item The optimal quantile estimator is the most accurate.  With , the relative MSE is only less than  (or even ).
\item When , using the optimal quantile estimator, CC reduces  minimum MSEs by roughly 20-  to 50-fold, compared to {\em symmetric stable random projections}. When , the reduction is about 5- to 15-fold.
\item Even with , {\em Symmetric table random projections} can not achieve the same accuracy as CC using the optimal quantile estimator with  only.
\end{itemize}

Again, using the optimal quantile estimator with  would be our recommended procedure for estimating Shannon entropy from Tsallis entropy.


\begin{figure}[h]
\begin{center}\mbox{
{\includegraphics[width=1.75in]{twist_HT_min.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{rice_HT_min.eps}}}
\mbox{
{\includegraphics[width=1.75in]{friday_HT_min.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{fun_HT_min.eps}}
}\\
\mbox{
{\includegraphics[width=1.75in]{business_HT_min.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{name_HT_min.eps}}}
\mbox{
{\includegraphics[width=1.75in]{have_HT_min.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{this_HT_min.eps}}
}
\mbox{
{\includegraphics[width=1.75in]{a_HT_min.eps}} \hspace{-0.1in}
{\includegraphics[width=1.75in]{the_HT_min.eps}}
}
\end{center}
\vspace{-0.15in}
\caption{Shannon entropy, , estimated from Tsallis entropy, , for 10 words in Table \ref{tab_data}. Curves are the minimum MSEs at each . }\label{fig_HT_min}
\end{figure}
\clearpage
\section{Conclusion}\label{sec_conclusion}

Network data and Web search data are naturally dynamic and can be viewed as data streams. The entropy is an extremely useful summary statistic and has numerous applications, for example, anomaly detection in Web mining and network diagnosis.

Efficiently and accurately computing the entropy in ultra-large and frequently updating data streams, in one-pass, is an active topic of research. A recent trend is to use the th frequency moments with  to approximate the entropy. For example, \cite{Article:Harvey_entropy_arXiv08,Proc:Harvey_FOCS08} proposed using the  frequency moments with very small  (e.g.,  or smaller).

For estimating the th frequency moments, the recently proposed {\em Compressed Counting (CC)} dramatically improves the standard data stream algorithm based on {\em symmetric stable random projections}, especially when . However, it had never been empirically evaluated before this work.

We experimented with CC to approximate the R\'enyi entropy, the Tsallis entropy, and the Shannon entropy. Some theoretical analysis on the biases and variances was provided. Extensive empirical studies based on some Web crawl data were conducted.

Based on the theoretical and empirical results, important conclusions can be drawn:
\begin{itemize}
\item Compressed Counting (CC) is numerically stable and is capable of providing highly accurate estimates of the th frequency moments. When  is close to 1, the improvements of CC over {\em symmetric stable random projections} in estimating frequency moments is enormous; in fact, the improvements tend to ``infinity'' when .
\item When  is close 1,  the optimal quantile estimator for CC is more accurate than the geometric mean and harmonic mean estimators, except when  and the sample size  is very small (e.g., ).
\item It appears not a practical algorithm to approximate the Shannon entropy using {\em symmetric stable random projections} with   and very small . When we do need to use {\em symmetric stable random projections}, we should take advantage of the variance-bias trade-off by using  away from 1 for achieving smaller mean square errors (MSEs).

\item CC is able to provide highly accurate estimates of the Shannon entropy using either the R\'enyi entropy or the Tsallis entropy. In terms of the best achieable MSEs, the improvements over {\em symmetric stable random projections} can be about 20- to 50-fold.
\item When estimating Shannon entropy from R\'enyi entropy, in order to reach the same accuracy as CC, {\em symmetric stable random projections} would need about 50 times more samples than CC. When estimating Shannon entropy from Tsallis entropy, {\em symmetric stable random projections}  could not reach the same accuracy as CC even with 500 times more samples.
\item The R\'enyi entropy provides a better tool for estimating the Shannon entropy than the Tsallis entropy does.
\item Our recommended procedure for estimating the Shannon entropy is to use CC with the optimal quantile estimator and  close 1 (e.g., ).
\item Since CC only needs a very small sample to achieve a good accuracy, the processing time of CC will be much reduced, compared to {\em symmetric stable random projections}, if the same level of accuracy is desired.
\end{itemize}

The technique of estimating Shannon entropy using {\em symmetric stable random projections} has been applied with some success in practical applications, such as  network anomaly detection and diagnosis\cite{Proc:Zhao_IMC07}. One major issue reported in \cite{Proc:Zhao_IMC07} (also \cite{Proc:Ganguly_RANDOM07}), is that the required sample size using {\em symmetric stable random projections} could be prohibitive for their real-time applications. Since CC can dramatically reduce the required sample size, we are passionate that  using Compressed Counting for estimating Shannon entropy will be highly practical and beneficial to real-world Web/network/data stream problems.


\section*{Acknowledgement}
This work is supported by Grant NSF DMS-0808864 and a gift from Google. The author would like to thank Jelani Nelson for  helpful communications. The author thanks Kenneth Church.




\begin{thebibliography}{10}

\bibitem{Proc:Aggarwal_KDD04}
Charu~C. Aggarwal, Jiawei Han, Jianyong Wang, and Philip~S. Yu.
\newblock On demand classification of data streams.
\newblock In {\em KDD}, pages 503--508, Seattle, WA, 2004.

\bibitem{Proc:Alon_STOC96}
Noga Alon, Yossi Matias, and Mario Szegedy.
\newblock The space complexity of approximating the frequency moments.
\newblock In {\em STOC}, pages 20--29, Philadelphia, PA, 1996.

\bibitem{Proc:Babcock_PODS02}
Brian Babcock, Shivnath Babu, Mayur Datar, Rajeev Motwani, and Jennifer Widom.
\newblock Models and issues in data stream systems.
\newblock In {\em PODS}, pages 1--16, Madison, WI, 2002.

\bibitem{Proc:Bar-Yossef_VLDB00}
Ziv Bar-Yossef, Alexander~C. Berg, Steve Chien, Jittat Fakcharoenphol, and Dror
  Weitz.
\newblock Approximating aggregate queries about web pages via random walks.
\newblock In {\em VLDB}, pages 535--544, Cairo, Egypt, 2000.

\bibitem{Proc:Kumar_FOCS02}
Ziv Bar-Yossef, T.~S. Jayram, Ravi Kumar, and D.~Sivakumar.
\newblock An information statistics approach to data stream and communication
  complexity.
\newblock In {\em FOCS}, pages 209--218, Vancouver, BC, Canada, 2002.

\bibitem{Article:Chambers_JASA76}
John~M. Chambers, C.~L. Mallows, and B.~W. Stuck.
\newblock A method for simulating stable random variables.
\newblock {\em Journal of the American Statistical Association},
  71(354):340--344, 1976.

\bibitem{Proc:Feigenbaum_FOCS99}
Joan Feigenbaum, Sampath Kannan, Martin Strauss, and Mahesh Viswanathan.
\newblock An approximate -difference algorithm for massive data streams.
\newblock In {\em FOCS}, pages 501--511, New York, 1999.

\bibitem{Proc:Ganguly_RANDOM07}
Sumit Ganguly and Graham Cormode.
\newblock On estimating frequency moments of data streams.
\newblock In {\em APPROX-RANDOM}, pages 479--493, Princeton, NJ, 2007.

\bibitem{Proc:Guha_SODA06}
Sudipto Guha, Andrew McGregor, and Suresh Venkatasubramanian.
\newblock Streaming and sublinear approximation of entropy and information
  distances.
\newblock In {\em SODA}, pages 733 -- 742, Miami, FL, 2006.

\bibitem{Article:Harvey_entropy_arXiv08}
Nicholas J.~A. Harvey, Jelani Nelson, and Krzysztof Onak.
\newblock Sketching and streaming entropy via approximation theory.
\newblock {\em CoRR}, abs/0804.4138, 2008.

\bibitem{Proc:Harvey_FOCS08}
Nicholas J.~A. Harvey, Jelani Nelson, and Krzysztof Onak.
\newblock Sketching and streaming entropy via approximation theory.
\newblock In {\em FOCS}, 2008.

\bibitem{Article:Havrda_67}
M~E. Havrda and F.~Charv\'at.
\newblock Quantification methods of classification processes: Concept of
  structural -entropy.
\newblock {\em Kybernetika}, 3:30--35, 1967.

\bibitem{Article:Henzinger_03}
Monika~.R. Henzinge.
\newblock Algorithmic challenges in web search engines.
\newblock {\em Internet Mathematics}, 1(1):115--126, 2003.

\bibitem{Proc:Henzinger_WWW00}
Monika~R. Henzinger, Allan Heydon, Michael Mitzenmacher, and Marc Najork.
\newblock On near-uniform url sampling.
\newblock In {\em WWW}, Amsterdam, The Netherlands, 2000.

\bibitem{Book:Henzinger_99}
Monika~R. Henzinger, Prabhakar Raghavan, and Sridhar Rajagopalan.
\newblock {\em Computing on Data Streams}.
\newblock American Mathematical Society, Boston, MA, USA, 1999.

\bibitem{Article:Hoory_06}
Shlomo Hoory, Nathan Linial, and Avi Wigderson.
\newblock Exander graphs and their applications.
\newblock {\em Bulletin of the AMS}, 43(4):439--561, 2006.

\bibitem{Proc:Indyk_FOCS00}
Piotr Indyk.
\newblock Stable distributions, pseudorandom generators, embeddings and data
  stream computation.
\newblock In {\em FOCS}, pages 189--197, Redondo Beach, CA, 2000.

\bibitem{Article:Indyk_JACM06}
Piotr Indyk.
\newblock Stable distributions, pseudorandom generators, embeddings, and data
  stream computation.
\newblock {\em Journal of ACM}, 53(3):307--323, 2006.

\bibitem{Proc:Indyk_STOC05}
Piotr Indyk and David~P. Woodruff.
\newblock Optimal approximations of the frequency moments of data streams.
\newblock In {\em STOC}, pages 202--208, Baltimore, MD, 2005.

\bibitem{Proc:Lakhina_SIGCOMM05}
Anukool Lakhina, Mark Crovella, and Christophe Diot.
\newblock Mining anomalies using traffic feature distributions.
\newblock In {\em SIGCOMM}, pages 217--228, Philadelphia, PA, 2005.

\bibitem{Article:Li_CC}
Ping Li.
\newblock Compressed counting.
\newblock {\em CoRR}, abs/0802.2305, 2008.

\bibitem{Proc:Li_SODA08}
Ping Li.
\newblock Estimators and tail bounds for dimension reduction in 
  () using stable random projections.
\newblock In {\em SODA}, pages 10 -- 19, 2008.

\bibitem{Article:Li_CC_v0}
Ping Li.
\newblock On approximating frequency moments of data streams with skewed
  projections.
\newblock {\em CoRR}, abs/0802.0802, 2008.

\bibitem{Report:Li_CC_oq}
Ping Li.
\newblock The optimal quantile estimator for compressed counting.
\newblock Technical report, (\url{http://arxiv.org/PS_cache/arxiv/pdf/0808/0808.1766v1.pdf}), 2008.

\bibitem{Article:Liu_06}
Canran Liu, Robert~J. Whittaker, Keeping Ma, and Jay~R. Malcolm.
\newblock Unifying and distinguishing diversity odering methods of rcomparing
  communities.
\newblock {\em Population Ecology}, 49(2):89--100, 2007.

\bibitem{Proc:Mei_WSDM08}
Qiaozhu Mei and Kenneth Church.
\newblock Entropy of search logs: How hard is search? with personalization?
  with backoff?
\newblock In {\em WSDM}, pages 45 -- 54, Palo Alto, CA, 2008.

\bibitem{Article:Muthukrishnan_05}
S.~Muthukrishnan.
\newblock Data streams: Algorithms and applications.
\newblock {\em Foundations and Trends in Theoretical Computer Science},
  1:117--236, 2 2005.

\bibitem{Proc:Renyi_61}
Alfred R\'enyi.
\newblock On measures of information and entropy.
\newblock In {\em The 4th Berkeley Symposium on Mathematics, Statistics and
  Probability 1960}, pages 547--561, 1961.

\bibitem{Article:Ricotta_02}
Carlo Ricotta, Alessandra Pacini, and Giancarlo Avena.
\newblock Parametric scaling from species to growth-form dierversity.
\newblock {\em Biosystems}, 65(2-3):179--186, 2002.

\bibitem{Proc:Saks_STOC02}
Michael~E. Saks and Xiaodong Sun.
\newblock Space lower bounds for distance approximation in the data stream
  model.
\newblock In {\em STOC}, pages 360--369, Montreal, Quebec, Canada, 2002.

\bibitem{Article:Tothmeresz_95}
Bela T\'othm\'er\'esz.
\newblock Comparison of different methods for diversity ordering.
\newblock {\em Journal of Vegetation Science}, 6(2):283--290, 1995.

\bibitem{Article:Tsallis_88}
Constantino Tsallis.
\newblock Possible generalization of boltzmann-gibbs statistics.
\newblock {\em Journal of Statistical Physics}, 52:479--487, 1988.

\bibitem{Proc:Woodruff_SODA04}
David~P. Woodruff.
\newblock Optimal space lower bounds for all frequency moments.
\newblock In {\em SODA}, pages 167--175, New Orleans, LA, 2004.

\bibitem{Article:ICDM10}
Qiang Yang and Xingdong Wu.
\newblock 10 challeng problems in data mining research.
\newblock {\em International Journal of Information Technology and Decision
  Making}, 5(4):597--604, 2006.

\bibitem{Proc:Zhao_IMC07}
Haiquan Zhao, Ashwin Lall, Mitsunori Ogihara, Oliver Spatscheck, Jia Wang, and
  Jun Xu.
\newblock A data streaming algorithm for estimating entropies of od flows.
\newblock In {\em IMC}, San Diego, CA, 2007.

\bibitem{Book:Zolotarev_86}
Vladimir~M. Zolotarev.
\newblock {\em One-dimensional Stable Distributions}.
\newblock American Mathematical Society, Providence, RI, 1986.

\bibitem{Article:Zyczkowski_03}
Karol Zyczkowski.
\newblock R\'enyi extrapolation of shannon entropy.
\newblock {\em Open Systems \& Information Dynamics}, 10(3):297--310, 2003.

\end{thebibliography}

\end{document} 