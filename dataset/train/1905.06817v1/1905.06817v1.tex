\label{sec:losses}


\begin{figure}[t]
	\centerline{
		\includegraphics[width=0.8\columnwidth]{ringnet_ring_only.pdf} 
	}
\caption{RingNet takes multiple images of the same person (Subject A) and an image of a different person (Subject B) during training and enforces shape consistency between the same subjects and shape inconsistency between the different subjects. The computed 3D landmarks from the predicted 3D mesh projected into 2D domain to compute loss with ground-truth 2D landmarks. During inference, RingNet takes a single image as input and predicts the corresponding 3D mesh. Images are taken from~\cite{Cao18}. The figure is a simplified version for illustration purpose.}
	\label{fig:ringnet}
\end{figure}

\begin{figure}[t]
	\centerline{
		\includegraphics[width=0.6\columnwidth]{ringnet2_ring_element_only.pdf} 
	}
	\caption{Ring element that outputs a 3D mesh for an image.}
	\label{fig:ringnet_element}
\end{figure}



The goal of our method is to estimate 3D head and face shape from a single face image . 
Given an image, we assume the face is detected, loosely cropped, and approximately centered. 
During training, our method leverages 2D landmarks and identity labels as input.
During inference it uses only image pixels; 2D landmarks and identity labels are not used.




\textbf{Key idea:} The key idea can be summarized as follows: 1) The face shape of a person remains unchanged, even though an image of the face may vary in viewing angle, lighting condition, resolution, occlusion, expression or other factors. 2) Every person has a unique face shape (not considering identical twins). 


We leverage this idea by introducing a shape consistency loss, embodied in our ring-structured network. RingNet (Fig.~\ref{fig:ringnet}) is a multiple encoder-decoder based architecture, with weight sharing between the encoders, and shape constraints on the shape variables. Each encoder in the ring is a combination of a feature extractor network and a regressor network. Imposing shape constraints on the shape variables forces the network to disentangle facial shape, expression, head pose, and camera parameters. We use FLAME~\cite{Bolkart1} as a decoder to reconstruct 3D faces from the semantically meaningful embedding, and to obtain a decoupling within the embedding space into semantically meaningful parameters (i.e. shape, expression, and pose parameters). 

We introduce the FLAME decoder, the RingNet architecture, and the losses in more details in the following. 


\subsection{FLAME model}

FLAME uses linear transformations to describe identity and expression dependent shape variations, and standard linear blend skinning (LBS) to model neck, jaw, and eyeball rotations around  joints. 
Parametrized by coefficients for shape, , pose , and expression , FLAME returns  vertices. 
FLAME models identity dependent shape variations , corrective pose blendshapes , and expression blendshapes  as linear transformations with learned bases , , and . 
Given a template  in the ``zero pose'', identity, pose, and expression blendshapes, are modeled as vertex offsets from .

Each of the pose vectors  contains (K+1) rotation vectors in axis-angle representation;  i.e.~one vector per joint plus the global rotation. 
The blend skinning function  then rotates the vertices around the joints , linearly smoothed by blendweights . 
More formally, FLAME is given as

with

The joints are defined as a function of  since different face shapes require different joint locations. 
We use Equation \ref{eq:flamemodel} for decoding our embedding space to generate a 3D mesh of a complete head and face. 

\subsection{RingNet}

The recent advances in face recognition (e.g.~\cite{Zhang2017}) and facial landmark detection (e.g.~\cite{bulat2017far,simon2017hand}) have led to large image datasets with identity labels and 2D face landmarks. For training, we assume a corpus of 2D face images , corresponding identity labels , and landmarks . 

The shape consistency assumption can be formalized by  (i.e.~the face shape of one subject should remain the same across multiple images) and  (i.e.~the face shape of different subjects should be distinct). RingNet introduces a ring-shaped architecture that jointly optimizes for shape consistency for an arbitrary number input images in parallel. For details regarding the shape consistency, see Section~\ref{sec:losses}.

RingNet is divided into  ring elements  as shown in Figure~\ref{fig:ringnet}, where each  consists of an encoder and a decoder network (see Figure~\ref{fig:ringnet_element}). The encoders share weights across , the decoder weights remain fixed during training. The encoder is a combination of a feature extractor network  and regression network . Given an image ,  outputs a high-dimensional vector, which is then encoded by  into a semantically meaningful vector 
(i.e.,~). This vector can be expressed as a concatenation of the camera, pose, shape and expression parameters, i.e., , where  are FLAME parameters. 

For simplicity we omit  in the following and use  and . The regression network iteratively regresses  in an iterative error feedback loop~\cite{kanazawaHMR18, Carreira1}, instead of directly regressing  from . In each iteration step, progressive shifts from the previous estimate are made to reach the current estimate. Formally the regression network takes the concatenated  as input and gives  as output. Then we update the current estimate by,

This iterative network performs multiple regression iterations per iteration of the entire RingNet training. The initial estimate is set to . The output of the regression network is then fed to the differentiable FLAME decoder network which outputs the 3D head mesh.

The number of ring elements  is a hyper-parameter of our network, which determines the number of images processed in parallel with optimized consistency on the . RingNet allows to use any combination of images of the same subject and images of different subjects in parallel. However, without loss of generality, we feed face images of the same identity to  and different identity to . Hence for each input training batch, each slice consists of  images of the same person and one image of another person (see Fig.~\ref{fig:ringnet}). 



\subsection{Shape consistency loss}

For simplicity let us call two subjects who have same identity label  ``matched pairs'' and two subjects who have different identity labels are ``unmatched pairs''.
A key goal of our work is to make a robust end-to-end trainable network that can produce the same shapes from images of the same subject and different shapes for different subjects.
In other words we want to make our shape generators discriminative. 
We enforce this by requiring matched pairs to have a distance in shape space that is smaller by a margin, , than the distance for unmatched pairs.
Distance is computed in the space of face shape parameters, which corresponds to a Euclidean space of vertices in the neutral pose.

In the RingNet structure,  and  produce  and , which are matched pairs when  and .
Similarly  and  produce  and , which are unmatched pairs when .
Our shape constancy term is then

Thus we minimize the following loss while training RingNet end-to-end, 

which is normalized to,

where  is the batch size for each element in the ring. 


\subsection{2D feature loss}

Finally we compute the  loss between the ground-truth landmarks provided during the training procedure and the predicted landmarks. Note that we do not directly predict 2D landmarks, but 3D meshes with known topology, from which the landmarks are retrieved.

Given the FLAME template mesh, we define for each OpenPose~\cite{simon2017hand} keypoint the corresponding 3D point in the mesh surface.
Note that this is the only place where we provide supervision that connects 2D and 3D. 
This is done only once.
While the mouth, nose, eye, and eyebrow keypoints have a fixed corresponding 3D point (referred to as static 3D landmarks), the position of the contour features changes with head pose (referred to as dynamic 3D landmarks).
Similar to~\cite{Cao:2014,Tewari2}, we model the contour landmarks as dynamically moving with the global head rotation (see Sup. Mat.). 
To automatically compute this dynamic contour, we rotate the FLAME template between -20 and 40 degrees to the left and right, render the mesh with texture, run OpenPose to predict 2D landmarks, and project these 2D points to the 3D surface. 
The resulting trajectories are symmetrically transferred between the left and right side of the face.


During training, RingNet outputs 3D meshes, computes the static and dynamic 3D landmarks for these meshes, and projects these into the image plane using the camera parameters predicted in the encoder output.
Henceforth we compute the following  loss between the projected landmarks  and the ground-truth 2D landmarks .
 
where  is the confidence score of each ground-truth landmark which is provided by the 2D landmark predictor. We set it to  if the confidence is above  and to  otherwise.
The total loss , which trains RingNet end-to-end is

where the  are the weights of each loss term and the last two terms regularize the shape and expression coefficients. Since  and  are scaled by the squared variance, the L2 norm of  and  represent the Mahalanobis distance in the orthogonal shape and expression space.


\subsection{Implementation details}

The feature extractor network uses a pre-trained ResNet-50~\cite{He1} architecture, also optimized during training. The feature extractor network outputs a 2048 dimensional vector. That serves as input to the regression network. The regression network consists of two fully-connected layers of dimension 512 with ReLu activation and dropout, followed by a final linear fully-connected layer with 159-dimensional output. 
To this 159-dimensional output vector we concatenate the camera, pose, shape, and expression parameters. The first three elements represent scale and 2D image translation. The following 6 elements are the global rotation and jaw rotation, each in axis-angle representation. The neck and eyeball rotations of FLAME are not regressed since the facial landmarks do not impose any constraints on the neck. The next 100 elements are the shape parameters, followed by 50 expression parameters of FLAME. The differentiable FLAME layer is kept fixed during training. 
We train RingNet for 10 epochs with a constant learning rate of 1e-4, and use Adam~\cite{kinga2015method} for optimization. The different model parameters are , , , , , . The RingNet architecture is implemented in Tensorflow~\cite{abadi2016tensorflow} and will be made publicly available.
We use VGG2 Face database \cite{Cao18} as our training dataset which consists of face images and their corresponding labels.
We run OpenPose  \cite{simon2017hand} on the database and compute 68 landmark points on the face.
OpenPose fails for many cases.
After cleaning for the failed cases we have around 800K images with their corresponding labels and facial landmarks for our training corpus.
We also consider around 3000 extreme pose images with corresponding landmarks provided by \cite{bulat2017far}.
Since for these extreme images we do not have any labels we replicate each image with random crops and scale for matched pair consideration.
