\documentclass[11pt,letterpaper]{article}

\usepackage{fullpage}
\usepackage{amsmath, amsfonts, amssymb, epsf, epsfig, amsthm,dsfont}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{xspace}
\usepackage{url}
\usepackage[textsize=tiny,disable]{todonotes}
\usepackage[xcolor=pdftex,ulem=normalem]{changes}
\definechangesauthor[Philipp Woelfel]{PW}{blue}
\setauthormarkup[right]{}

\newcommand{\paren}[1]{{\left({#1}\right)}}
\newcommand{\bparen}[1]{{\bigl({#1}\bigr)}}
\newcommand{\Bparen}[1]{{\Bigl({#1}\Bigr)}}
\newcommand{\bbparen}[1]{{\biggl({#1}\biggr)}}
\newcommand{\BBparen}[1]{{\Biggl({#1}\Biggr)}}
\newcommand{\nparen}[1]{({#1})}

\makeatletter
\providecommand{\@prob}{{\mathrm{Prob}}}
\newcommand{\prob}{\@prob}
\newcommand{\Prob}[2][]{\@prob_{#1}\paren{#2}}
\newcommand{\bProb}[2][]{\@prob_{#1}\bparen{#2}}
\newcommand{\BProb}[2][]{\@prob_{#1}\Bparen{#2}}
\newcommand{\bbProb}[2][]{\@prob_{#1}\bbparen{#2}}
\newcommand{\BBProb}[2][]{\@prob_{#1}\BBparen{#2}}
\newcommand{\nProb}[2][]{\@prob_{#1}\nparen{#2}}

\newcommand{\CondProb}[3][]{\Prob[#1]{#2\,\left\vert\vphantom{#2#3}\right.\,#3}}
\newcommand{\bCondProb}[3][]{\bProb[#1]{#2\,\big\vert\,#3}}
\newcommand{\BCondProb}[3][]{\BProb[#1]{#2\,\Big\vert\,#3}}
\newcommand{\bbCondProb}[3][]{\bbProb[#1]{#2\,\bigg\vert\,#3}}
\newcommand{\BBCondProb}[3][]{\BBProb[#1]{\#2\,\Bigg\vert\,#3}}
\newcommand{\nCondProb}[3][]{\nProb[#1]{#2\,\vert\,#3}}


\providecommand{\@Exp}{{\mathrm{E}}}
\newcommand{\Exp}[2][]{\@Exp_{#1}\left[{#2}\right]}
\newcommand{\bExp}[2][]{\@Exp_{#1}\bigl[{#2}\bigr]}
\newcommand{\BExp}[2][]{\@Exp_{#1}\Bigl[{#2}\Bigr]}
\newcommand{\bbExp}[2][]{\@Exp_{#1}\biggl[{#2}\biggr]}
\newcommand{\BBExp}[2][]{\@Exp_{#1}\Biggl[{#2}\Biggr]}
\newcommand{\nExp}[2][]{\@Exp_{#1}[{#2}]}

\newcommand{\CondExp}[3][]{\Exp[#1]{\,#2\,\left\vert\vphantom{#2#3}\right.\,#3\,}}
\newcommand{\bCondExp}[3][]{\bExp[#1]{\,#2\,\big\vert\,#3\,}}
\newcommand{\BCondExp}[3][]{\BExp[#1]{\,#2\,\Big\vert\,#3\,}}
\newcommand{\bbCondExp}[3][]{\bbExp[#1]{\,#2\,\bigg\vert\,#3\,}}
\newcommand{\BBCondExp}[3][]{\BBExp[#1]{\,\#2\,\Bigg\vert\,#3\,}}
\newcommand{\nCondExp}[3][]{\nExp[#1]{\,#2\,\vert\,#3\,}}

\makeatother


\usepackage{tikz}
\usetikzlibrary{automata,positioning,calc,shapes,backgrounds}


\usepackage[
  boxruled,
  linesnumbered
]{algorithm2e}
\DontPrintSemicolon
\SetAlgoSkip{normalskip}
\SetAlgoInsideSkip{medskip}

\newcommand{\IlIf}[2]{\KwSty{if} #1 \KwSty{then} #2}
\newcommand{\IlRepeat}[2]{\KwSty{repeat} #2 \KwSty{until} #1}

\def\read{\text{\tt read}\xspace}
\newcommand{\xwrite}{\text{\tt write}\xspace}
\newcommand{\fetchInc}{\text{\tt fetch\&inc}\xspace}
\newcommand{\fetchDec}{\text{\tt fetch\&dec}\xspace}
\newcommand{\fetchSet}{\text{\tt fetch\&set}\xspace}
\newcommand{\llsc}{\text{\sf load-linked/store-conditional}\xspace}
\newcommand{\cas}{\text{\tt compare\&swap}\xspace}
\newcommand{\SC}{\text{\tt SC}\xspace}
\newcommand{\LL}{\text{\tt LL}\xspace}
\newcommand{\collect}{\text{\tt collect}\xspace}
\SetKw{True}{True}
\SetKw{False}{False}

\SetKwFunction{CAS}{CAS}
\SetKwFunction{Scan}{scan}
\SetKwFunction{Update}{update}
\SetKwFunction{Read}{Read}
\SetKwFunction{Write}{Write}
\SetKwFunction{MRread}{MRSWread}
\SetKwFunction{MRwrite}{MRSWwrite}
\SetKwFunction{FAI}{fetch\&inc}
\SetKwFunction{FAD}{fetch\&dec}
\SetKwFunction{LoadBalance}{loadBalance}
\SetKwFunction{Enqueue}{enq}
\SetKwFunction{Dequeue}{deq}
\SetKwFunction{MRwrite}{MRSWwrite}
\SetKwFunction{MRread}{MRSWread}

\newcommand{\cat}{\ensuremath{\parallel}}
\newcommand{\sh}{\backslash}
\newcommand{\br}[1]{\left( #1 \right)}
\newcommand{\bs}[1]{\left[ #1 \right]}
\newcommand{\bc}[1]{\left\{ #1 \right\}}
\newcommand{\ba}[1]{\left| #1 \right|}
\newcommand{\bg}[1]{\left< #1 \right>}
\newcommand{\eps}{\varepsilon}
\newcommand{\imps}{\ \ \Longrightarrow\ \ }
\newcommand{\cimps}{\Longrightarrow}
\newcommand{\impby}{\ \ \Longleftarrow\ \ }
\newcommand{\bicond}{\ \ \Longleftrightarrow\ \ }
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\fb}[2]{\left( \frac{#1}{#2} \right)}
\newcommand{\mx}[2]{\ensuremath{\max\!\bc{#1,#2}}}
\newcommand{\mxt}[3]{\ensuremath{\max\!\bc{#1,#2,#3}}}
\newcommand{\twodef}[4]{\ensuremath{\left\{\begin{array}{rl}#1&\mbox{if\ \ }#2\\#3&\mbox{if\ \ }#4\end{array}\right.}}
\newcommand{\twodefo}[3]{\ensuremath{\left\{\begin{array}{rl}#1&\mbox{if\ \ }#2\\#3&\mbox{otherwise}\end{array}\right.}}
\newcommand{\threedef}[6]{\ensuremath{\left\{\begin{array}{rl}#1&\mbox{if\ \ }#2\\#3&\mbox{if\ \ }#4\\#5&\mbox{if\ \ }#6\end{array}\right.}}
\newcommand{\threedefo}[5]{\ensuremath{\left\{\begin{array}{rl}#1&\mbox{if\ \ }#2\\#3&\mbox{if\ \ }#4\\#5&\mbox{otherwise}\end{array}\right.}}
\newcommand{\fourdef}[8]{\ensuremath{\left\{\begin{array}{rl}#1&\mbox{if\ \ }#2\\#3&\mbox{if\ \ }#4\\#5&\mbox{if\ \ }#6\\#7&\mbox{if\ \ }#8\end{array}\right.}}
\newcommand{\fourdefo}[7]{\ensuremath{\left\{\begin{array}{rl}#1&\mbox{if\ \ }#2\\#3&\mbox{if\ \ }#4\\#5&\mbox{if\ \ }#6\\#7&\mbox{otherwise}\end{array}\right.}}
\newcommand{\twoitem}[2]{\ensuremath{\left\{\begin{array}{r}#1\\#2\end{array}\right.}}
\renewcommand{\mod}{\ensuremath{\ \mathrm{mod}\ }}
\newcommand{\chus}[2]{\ensuremath{\br{\stacktwo{\!\!{#1}\!\!}{\!\!{#2}\!\!}}}}
\newcommand{\lcm}[2]{\ensuremath{\mbox{lcm}\br{{#1},{#2}}}}
\newcommand{\zfield}[1]{\ensuremath{\mathbb{Z}/#1\mathbb{Z}}}
\newcommand{\ceil}[1]{\ensuremath{ \left\lceil #1 \right\rceil }}
\newcommand{\floor}[1]{\ensuremath{ \lfloor #1 \rfloor }}
\newcommand{\qq}{\ensuremath{\mathbb{Q}}}
\newcommand{\rr}{\ensuremath{\mathbb{R}}}
\newcommand{\twostack}[2]{\ensuremath{\left\{\begin{array}{l}#1\\#2\end{array}\right.}}
\newcommand{\stacktwo}[2]{{\ensuremath{\begin{array}{c} {#1} \\ {#2} \end{array}}}}
\newcommand{\stacktwolal}[2]{{\ensuremath{\begin{array}{l} {#1} \\ {#2} \end{array}}}}
\newcommand{\stackthree}[3]{{\ensuremath{\begin{array}{c} {#1} \\ {#2} \\ {#3} \end{array}}}}
\newcommand{\stackfour}[4]{\ensuremath{\begin{array}{c} {#1} \\ {#2} \\ {#3} \\ {#4} \end{array}}}
\newcommand{\fourmx}[4]{\ensuremath{\br{\begin{array}{cc} {#1} & {#2} \\ {#3} & {#4} \end{array} }}}
\newcommand{\tab}{\hspace{1cm}}
\newcommand{\twotab}{\tab\tab}
\newcommand{\threetab}{\tab\tab\tab}
\newcommand{\fourtab}{\tab\tab\tab\tab}
\newcommand{\bigtab}[1]{\hspace{#1}}
\newcommand{\goesto}{\rightarrow}
\newcommand{\groupf}[1]{\ensuremath{\mathbb{F}_{#1}}}
\newcommand{\groupfp}{\ensuremath{\mathbb{F}_p}}
\newcommand{\groupfpx}[1]{\ensuremath{\mathbb{F}_{#1}^{\cdot}}}
\newcommand{\frob}[1]{\ensuremath{F_{#1}}}
\newcommand{\glnfp}[2]{\ensuremath{GL(#1, \mathbb{F}_{#2 })}}
\newcommand{\tx}[1]{\mathrm{#1}}
\newcommand{\itx}[1]{\mathrm{\ #1}}
\newcommand{\itxi}[1]{\mathrm{\ #1\ }}
\newcommand{\iitxii}[1]{\mathrm{\ \ #1\ \ }}
\newcommand{\ttx}[1]{\tab\mathrm{#1}\tab}
\newcommand{\groupsltwof}[1]{\ensuremath{SL(2, \mathbb{F}_{#1})}}
\newcommand{\nequiv}{\not\equiv}
\newcommand{\ol}[1]{\ensuremath{\overline{#1}}}
\newcommand{\nsubg}{\vartriangleleft}
\newcommand{\dss}[1]{\begin{displaystyle} #1 \end{displaystyle}}
\newcommand{\fourmxbig}[4]{ \fourmx{ \dss{#1} }{ \dss{#2} }{ \dss{#3} }{ \dss{#4} }   }
\newcommand{\ul}[1]{ \underline{#1} }
\newcommand{\del}{ \nabla }
\newcommand{\diff}{ \partial }

\newcommand{\U}{\ensuremath{\mathsf{U}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\F}{\ensuremath{\mathbb{F}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
\renewcommand{\O}{\ensuremath{{O}}}
\newcommand{\Bigo}{\ensuremath{{O}}}
\newcommand{\Bigtheta}{\ensuremath{{\mathit \Theta}}}
\newcommand{\Bigomega}{\ensuremath{{\mathit \Omega}}}
\newcommand{\littleomega}{\ensuremath{{\mathit \omega}}}

\newcommand{\smalleritemsep}{\itemsep -0.05in}


\newtheoremstyle{coolstyle}
    {9pt}{9pt}{\slshape}{}{\bfseries}{.}{.5em} {}\theoremstyle{coolstyle}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}[theorem]{Claim}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{specification}[theorem]{Specification}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{question}[theorem]{Question}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{remark}[theorem]{Remark}

\renewcommand{\AA}{\mathcal{A}}
\newcommand{\BB}{\mathcal{B}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\DD}{\mathcal{D}}
\newcommand{\EE}{\mathcal{E}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\QQ}{\mathcal{Q}}
\newcommand{\RR}{\mathcal{R}}
\renewcommand{\SS}{\mathcal{S}}
\newcommand{\VV}{\mathcal{V}}
\newcommand{\WW}{\mathcal{W}}



\newcommand{\IIN}{\mathds{N}}

\newcommand{\vc}{{\vec{c}}}
\newcommand{\vd}{{\vec{d}}}
\newcommand{\tc}{{\tilde{c}}}



\newcommand{\optype}{operation type}
\newcommand{\Optype}{Operation type}
\newcommand{\optypetype}{type}
\newcommand{\opex}{operation execution} \newcommand{\Opex}{Operation execution}
\newcommand{\astep}{atomic step} \newcommand{\opop}{operation} \newcommand{\Opop}{Operation}
\newcommand{\Op}{Op}
\newcommand{\States}{\ensuremath{\mathcal{S}}}
\newcommand{\sinit}{\ensuremath{s_{init}}}
\newcommand{\Resps}{\ensuremath{\mathcal{R}}}
\newcommand{\Procs}{\ensuremath{\mathcal{P}}}
\newcommand{\Vars}{\ensuremath{\mathcal{V}}}
\newcommand{\Hists}{\ensuremath{\mathcal{H}}}
\newcommand{\Typ}{\ensuremath{\tau}}
\newcommand{\B}{\ensuremath{\mathcal{B}}}
\newcommand{\init}{\ensuremath{v_0}}
\newcommand{\ot}{ot}
\newcommand{\ox}{ox}
\newcommand{\re}{ret}
\newcommand{\RefObj}{\O_{\Typ}}
\newcommand{\Done}{OK}

\newcommand{\close}[1]{\ensuremath{\text{close}\left(#1\right)}}

\newcommand{\obj}{\ensuremath{\mathit{O}}}
\newcommand{\op}{\ensuremath{\mathit{op}}}
\newcommand{\inv}[1]{\ensuremath{inv(#1)}}
\newcommand{\rsp}[1]{\ensuremath{rsp(#1)}}
\newcommand{\invImp}[1]{\ensuremath{{M-inv}(#1)}}
\newcommand{\rspImp}[1]{\ensuremath{{M-rsp}(#1)}}


\newcommand{\Aalg}{-algorithm}
\newcommand{\Lalg}{-algorithm}

\newcommand{\typetupl}{\ensuremath{(\States, \sinit, \Ops, \Resps, \delta)}}
\newcommand{\algotupl}{\ensuremath{(\Procs, \B, \Hists)}}
\newcommand{\impltupl}{\ensuremath{(\Typ, \Procs, \B, \Hists)}}

\newcommand{\Tstep}{termination operation}

\newcommand{\obs}{observable}
\newcommand{\QH}{\ensuremath{\mathcal{S}}}

\newcommand{\T}{\mathcal{T}}
\newcommand{\LinFn}{\ensuremath{f}}


\begin{document}

\title{Linearizable Implementations Do Not Suffice for Randomized Distributed Computation }
\author{Wojciech Golab\thanks{Research conducted mostly during a postdoctoral fellowship at the University of Calgary.}\,\,\thanks{
Research partially supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada.}\\
HP Labs\\
\url{wojciech.golab@hp.com}
\and
Lisa Higham{}\\
University of Calgary\\
\url{higham@ucalgary.ca}
\and Philipp Woelfel{}\\
University of Calgary\\
\url{woelfel@ucalgary.ca}
}


\begin{titlepage}
\maketitle
\thispagestyle{empty}
\todo{
   A general note: We have too many LaTeX macro definitions. E.g., I believe the reason for having macros such as {\tt \textbackslash obj} is to allow an easy change of the symbol we use for objects.
   This doesn't work, though: It is impossible for all co-authors to keep track of all the macros, so this will always lead to inconsistencies (e.g., right now {\tt\textbackslash obj} is not used consistently), and then the benefit of using the macro in the first place goes away. Moreover, sometimes we have to use different symbols for objects, and I'd much prefer to type {\tt O} over typing {\tt\textbackslash obj}
   IMO using macros so extensively does not work well for multi-author documents.
   Of course it's o.k.\ to use macros, when it helps to reduce the typesetting effort significantly (as e.g., our macro for {\tt\textbackslash BB} does). But I would much prefer if we could avoid (and get rid of) all macros that don't fall in this category.
}

\begin{abstract}
Linearizability is the gold standard among algorithm designers for deducing the correctness of a distributed algorithm
using implemented shared objects from the correctness of the corresponding algorithm using atomic versions of the same objects.
We show that linearizability does not suffice for this purpose when processes can exploit randomization,
and we discuss the existence of alternative correctness conditions.
This paper makes the following contributions:
\begin{itemize}

\item
Various examples demonstrate that using well-known linearizable implementations of objects (e.g., snapshots)
in place of atomic objects can change the probability distribution of the outcomes that the adversary is able to generate.
In some cases,
an oblivious adversary can create a probability distribution of outcomes for an algorithm with implemented,
linearizable objects,
that not even a strong adversary can generate for the same algorithm with atomic objects.

\item
A new correctness condition for shared object implementations, called \emph{strong linearizability}, is defined.
We prove that a strong adversary (i.e., one that sees the outcome of each coin flip immediately) gains no
additional power when atomic objects are replaced by strongly linearizable implementations.
In general, no strictly weaker correctness condition suffices to ensure this.
We also show that strong linearizability is a local and composable property.

\item
In contrast to the situation for the strong adversary,
for a natural weaker adversary (one that cannot see a process' coin flip until its next operation on a shared object)
we prove that there is no correspondingly general correctness condition.
Specifically, any linearizable implementation of counters
from atomic registers and load-linked/store-conditional objects,
that satisfies a natural locality property,
necessarily gives the weak adversary more power than it has with atomic counters.

\end{itemize}
\end{abstract}

\end{titlepage}

\pagestyle{plain}

\SetAlCapNameSty{textsc}
\SetAlCapSty{textsc}
\SetFuncSty{textsc}

\section{Introduction}

Linearizability is the gold standard among algorithm designers for deducing the correctness of
a distributed algorithm using implemented shared objects from the correctness of the corresponding
algorithm using atomic\footnote{
  In this paper, an \emph{atomic} operation is one that happens instantaneously, i.e., it is indivisible.
  But in the literature, the notion of atomicity is not used consistently.
  E.g., in her textbook \cite{Lynch_DistributedAlgorithms1996}, Lynch defines atomic objects to be linearizable,
        but Anderson and Gouda \cite{journals/ipl/AndersonG88} define atomicity in terms of instantaneous operations.}
versions of the same objects.
We explore this in more detail, showing that linearizability does not suffice for this purpose
when processes can exploit randomization.

In an asynchronous distributed system, processes collaborate
by executing an algorithm that applies operations to a collection of shared objects.
If the operations on these objects are atomic, then the result of the execution
is the same as some sequential execution that could arise from an arbitrary interleaving
of the processes' steps.
Alternatively, some objects could be replaced by a set of software methods for the different operations on those objects.
Processes would then invoke the appropriate method in order to simulate the intended atomic operation.
In this case, there is a finer granularity to the interleaving of process steps.
Consequently, we need to be sure that each possible result (e.g., the algorithm's return value for each process) that can arise from using the software methods
could also have arisen if the operations were atomic.

This requirement is ensured if the methods provided for each object constitute
a \emph{linearizable implementation} \cite{her:lin} of the object.
Linearizability is an especially useful and important correctness condition because it is  a \emph{local} property.
That is, if each object in a collection of objects is replaced by its linearizable implementation,
then the result of any execution that can arise from the concurrent use of the whole collection is one
that could have also happened if the objects were atomic.

Linearizable implementations, however, do not preserve the probability distribution of the possible results
as we transform the atomic system to the implemented one.
An \emph{adversary}, which schedules process steps, can ``stretch out'' a method call that was originally an atomic operation,
and concurrently inspect the outcome of other processes' coin flips.
Based on the outcomes, the scheduler can choose between alternative executions of the ongoing method call.
As we will illustrate through examples,
the consequences of this additional flexibility can be powerful and subtle,
allowing the behaviour of the implemented system to differ dramatically from that
of the atomic system.
In particular, the adversary can manipulate executions so that low-probability worst-case results
in the atomic system become much more probable in the implemented system.

We will see that our ability to curtail an adversary's additional power, which it can gain
when atomic objects are replaced by linearizable implementations,
depends in part upon the original power of the adversary.
Various adversaries have been defined in literature,
differing in their ability to base scheduling decisions on the random choices made by the algorithm
(see \cite{Aspnes2003_DistrComp} for an overview of adversary models).
The main results in this paper concern two adversary models.
Informally,
when a process is scheduled by a \emph{strong} adversary, the process executes only its next atomic operation,
whether on a local or a shared object.  (Coins are local objects.)
When a process is scheduled by a \emph{weak} adversary
it executes up to and including its next step on a shared object.
Thus, a strong adversary can intervene between a coin flip and the next step by the same process,
whereas a weak adversary cannot.
Further discussion of these adversaries, including formal definitions, appears in Section \ref{model.sec}.

\subsection*{Summary of contributions}

\noindent
\textbf{1.} Several examples demonstrate that using linearizable implemented objects in place of atomic objects in randomized algorithms allows the adversary to change the probability distribution of results.
Therefore, in order to safely use implemented objects in randomized algorithms,
it does not suffice to simply claim that these implementations are linearizable.

\noindent
\textbf{2.} A new correctness condition for shared object implementations, called \emph{strong linearizability},
which is strictly stronger than linearizability, is defined.
      We prove that a strong adversary against a randomized algorithm using strongly linearizable objects
      has exactly the same power as a strong adversary against the same algorithm using atomic objects.
      Conversely, if the set of histories that arise from a strong adversary scheduling an algorithm with implemented
      linearizable objects
      is  ``equivalent'' to the set of histories that can arise from some strong adversary scheduling the same algorithm
      with atomic objects, then the former set of histories must be strongly linearizable.
      We also show that several known universal constructions of linearizable objects with common progress properties
      (e.g., wait-freedom) provide strong linearizability.
      Finally, we prove that strong linearizability, like linearizability, is both a local and a composable property.

\noindent
\textbf{3.} In contrast to the situation for strong adversaries,
for weak adversaries strong linearizability has no counterpart.
For example, for some randomized algorithms,
weak adversaries always gain additional power when strong counters
(that support \fetchInc and \fetchDec operations)
are replaced with ``natural'' linearizable implementations based on a set of base objects supporting
reads, writes and \llsc operations.
Consequently, to prevent weak adversaries from gaining additional power, the implementation
of the counter would require additional base object types beyond what is necessary for linearizability.
This result is obtained by a technically involved proof;
it holds even for randomized implementations with fairly weak progress conditions (e.g., lock-freedom).

\medskip
Randomization has become an important technique in the design of distributed algorithms;
it allows us to circumvent some substantial impossibilities and complexity lower bounds of deterministic algorithms.
Our results impact the design of randomized algorithms that use shared objects
not directly supported through atomic primitives in hardware.
First, simulating the required shared objects in software using ``only'' linearizable implementations can break the algorithm.
Second, such algorithms are much easier to fix (using strong linearizability)
if they are designed from the outset to work against strong adversaries,
but not so if they are designed only to work against weak adversaries.
Third, since there are strongly linearizable universal constructions using consensus objects, which can be implemented using \cas,
any system that provides \cas in hardware can implement any object in a strongly linearizable way.


\section{Examples}

We begin with two examples to provide intuition and motivation,
and  delay the model details, which are needed for our technical results, until the next section.
The examples illustrate how an adversary in a randomized algorithm gains additional power when atomic objects
are replaced with implemented ones.

\paragraph{Atomic versus linearizable snapshots.}
An  process snapshot object is a vector  of length 
that supports the atomic operations  and  by any process .
Operation  writes  to  while leaving all  unchanged; and
 returns the vector of values () to .

Initialize a snapshot object for three processes to .
Suppose the processes  and  are executing the following code,
and the adversary is trying to minimize the sum of the values returned in 's scan.

\begin{quote}
\medskip
\noindent
:    \\
:  ;\quad  \\
:  ;\quad  uniform-random;\quad 
\end{quote}
\medskip

To keep the sum in 's \Scan\ low,
the adversary can schedule either both or neither of 's \Update\ operations before 's \Scan.
If the adversary is weak, the same holds for 's \Update\ operations.
Thus, under the best strategy for a weak adversary, the expected value of the sum in 's \Scan is 0.
If the adversary is strong,
its best strategy is to schedule 's \Scan before 's second \Update if 's coin flip returns 1
and after if it returns~.
Thus, under the best strategy for a strong adversary, the expected value
of the sum in 's \Scan\ is .

Now suppose instead that \Update\ and \Scan\ are implemented from atomic registers
by the well-known wait-free linearizable algorithm due to Afek, Attiya, Dolev, Gafni, Merritt and Shavit \cite{aadgms:snapshots}.
In this algorithm, the snapshot object is implemented as an array  of registers.
Let a \emph{collect} denote a series of  atomic reads, one for each element of , in some fixed order.
To perform a \Scan, each process  repeatedly collects until either two successive collects are
identical (a \emph{successful double collect}),
or  observes that another process, say , has executed at least two \Update\ operations to  during 's \Scan.
In the second case,  returns the last \Scan written (as we explain shortly) by  during an \Update (a \emph{borrowed scan}).
To perform an \Update, each process  must first perform a \Scan\ and then write the result of the \Scan\
together with its \Update\ argument into .
This ensures that if a \Scan\ has enough failed double collects, then a borrowed \Scan is possible.
With this implementation, the adversary can maneuver ,  and  as shown in Figure~\ref{fig:snapshot}.

\begin{figure*}[htbp]
\begin{center}
\begin{tikzpicture}[
>=latex, very thick,
on grid,
auto,
]
\footnotesize

\newcommand{\outerop}[4][]{\fill[fill=gray!30] () rectangle ();
  \draw[-]  (#2) -- node[swap,#1] {#4} (#3);
  \draw[very thick] () -- ();
  \draw[very thick] () -- ();
}

\newcommand{\innerop}[4][]{\fill[fill=gray!70] () rectangle ();
  \draw[<->]  (#2) -- node[#1] {#4} (#3);
}


\node (r) {\normalsize };
\node[below = 1.5 of r] (q) {\normalsize };
\node[above = 1.5 of r] (p) {\normalsize };

\coordinate[right = .4 of p ] (pScanBegin);
\coordinate[right = 16 of p ] (pScanEnd) ;
\outerop{pScanBegin}{pScanEnd}{\Scan}

\coordinate[right =.6 of p ] (pCollect1Begin) ;
\coordinate[right = 1.4 of pCollect1Begin ] (pCollect1End) ;
\innerop{pCollect1Begin}{pCollect1End}{\collect}

\coordinate[right =11.6 of p ] (pCollect2Begin) ;
\coordinate[right = 1.5 of pCollect2Begin ] (pCollect2End) ;
\innerop{pCollect2Begin}{pCollect2End}{\collect}

\coordinate[right =13.9 of p ] (pCollect3Begin) ;
\coordinate[right = 1.5 of pCollect3Begin ] (pCollect3End) ;
\innerop{pCollect3Begin}{pCollect3End}{\collect}


\coordinate[right = 5.5 of q ] (qUpdate2Begin) ;
\coordinate[right = 1.9 of qUpdate2Begin ] (qUpdate2End) ;
\outerop{qUpdate2Begin}{qUpdate2End}{}

\node[circle,text centered,fill=blue!20,draw=blue!200,thick,
  right = 8.35 of q] (coinflip) {};

\coordinate[right = 9.3 of q] (qUpdate3Begin);
\coordinate[right = 2.3 of qUpdate3Begin ] (qUpdate3End);
\outerop{qUpdate3Begin}{qUpdate3End}{}


\coordinate[right = 2 of r ] (rUpdate1Begin) ;
\coordinate[right = 1.9 of rUpdate1Begin ] (rUpdate1End) ;
\outerop{rUpdate1Begin}{rUpdate1End}{}

\coordinate[right = 4.3 of r] (rUpdate2Begin) ;
\coordinate[right = 16 of r ] (rUpdate2End) ;
\outerop{rUpdate2Begin}{rUpdate2End}{}

\coordinate[right = 4.5 of r ] (rScanBegin) ;
\coordinate[right = 1 of rScanBegin ] (rScanEnd) ;
\innerop{rScanBegin}{rScanEnd}{}

\coordinate[right = 13.5 of r] (rWrite1);
\coordinate[right = 15.7 of r] (rWrite2);

\node (label) at () {};

\path[->,shorten >= 2pt] (label) edge[pos=0.4] node {} (rWrite1);
\path[->,shorten >= 2pt] (label) edge[pos=0.4] node[swap] {} (rWrite2);

\draw[dotted,thick] () -- ();
\draw[dotted,thick] () -- ();


\end{tikzpicture}
\end{center}
\vspace{-1em}
\caption{A ``bad'' scheduling using an implemented linearizable snapshot.}
\label{fig:snapshot}
\end{figure*}

In this execution,  applies a \Scan\ that returns a view  with sum~2 as the first part of its second \Update.
Then, the adversary chooses where to schedule the remainder of 's second \Update,
which is the write to  of .
If 's coin flip is~, it schedules this write after 's third collect.
In this case,  will have a successful double collect, which returns a view with sum .
If 's coin flip is~1, the adversary schedules 's write between 's second and third collects.
In this case,  will have a failed double collect but will have seen  \Update\ twice.
Accordingly,  borrows 's \Scan, and so 's \Scan\ also returns the view  with sum~2.
Thus, the adversary can force an expected sum in 's \Scan\ of only .
Notice, furthermore, that only a weak adversary was used to achieve this execution in the system with an implemented
snapshot object.

\paragraph{Atomic versus linearizable registers.}
Since the implemented method calls give the adversary more power than it has when operations are atomic,
we might conjecture that this additional power could be curtailed by appropriately restricting the adversary.
The next example shows that this is not always possible.

Let  denote a multi-valued atomic single-reader/single-writer (SRSW) register initialized to~1.
Let processes  and  execute the following code:

\begin{quote}
\medskip
\noindent
: ;\quad  uniform-random;\quad     \\
:  
\medskip
\end{quote}

Suppose that a strong adversary is trying to minimize the value that  reads.
Then the adversary's best strategy is to have  execute its \Read\ either before or after both of 's \Write\ operations.
In either case, the expected value of 's \Read\ is~1.

Now suppose, instead, that  is implemented
using Vidyasankar's linearizable implementation of
single-reader/single-writer (SRSW) multivalued registers from
SRSW atomic bits \cite{vid:registers}.
In this construction, an array  of
SRSW binary registers is used to represent a register with domain
.
Value  is represented by  and .
The implementation is shown in Figure~\ref{fig:multivalued_registers}.


\begin{figure} \begin{minipage}[t]{.4\textwidth}
\begin{function}[H]
  \;
  \For{}{
    \;
  }
  \caption{Write()}
\end{function}
\end{minipage}\hfill
\begin{minipage}[t]{.5\textwidth}
\begin{function}[H]
  \;
  \IlRepeat{}{}\;
  \;
  \For{}{
    \IlIf{}{}
  }
  \Return{}
  \caption{Read()()}
\end{function}
\end{minipage}
\caption{Linearizable implementation of multivalued SRSW registers from atomic bits.}
\label{fig:multivalued_registers}
\end{figure}

Under this implementation, if the register is initialized with the value~1,
the adversary's best strategy is to schedule as follows:
First  reads ``up'' seeing   and then .
Next,  takes all of its steps, then finally  takes its remaining steps where it reads ``down''.
With probability 1/2,  executed  and  will return~0;
with probability 1/2,  executed  and  will return~1.
Hence the expected value returned by 's \Read\ is~1/2.

In this example, the adversary makes all its scheduling decisions in advance;
it does not exploit knowledge of the outcome of coin flips while the computation proceeds.
Even reducing the power of the adversary from strong to this weakest \emph{oblivious} one
does not curtail its power sufficiently to retain the expected behaviour of the algorithm
when  is an atomic register.

These examples motivate our central question:
What is required to preserve the behaviour of a randomized algorithm
when atomic operations are replaced by method calls?
The rest of this paper addresses this question.



\section{Model and Definitions}
\label{model.sec}


We consider a distributed shared memory system consisting of a set  of  processes communicating via a set of globally shared base objects.

A shared object is an instance of a \emph{type}, which supports some set of operations.
Each such operation \op\ consists of an \emph{invocation} including operation arguments, denoted \inv{\op},
and a \emph{matching response} including the return value, denoted \rsp{\op}.
A \emph{type} is defined by a \emph{sequential specification}, which determines the set of sequences of operations that can occur on any object of that type \cite{her:lin}.
A sequence is \emph{valid} for object  if it is in the sequential specification of the type of \obj.

In this paper, we restrict ourselves to deterministic types (except for coin objects as described below).
I.e., if  and  are valid sequences and , then .\todo{Check!}

A process is a sequential thread of control that invokes operations on shared base objects and receives the responses of such operations.
Processes also have access to independent random experiments.
Let  be an arbitrary countable set, called the \emph{coin flip domain}.
A process step can invoke a \emph{flip} operation (with no arguments) on a \emph{coin} object,
which returns a \emph{coin flip} in  as the matching response.

An \emph{implementation} of a \emph{target type}  is a distributed method using other implemented or base objects.
It takes as input the description of an operation invocation, and outputs a response, such that if multiple processes call the method multiple times \emph{sequentially}, then the resulting sequence of method invocations and responses matches the sequential specification of . \todo{Check the  part ``such that if multiple\dots''!}
An implementation is deterministic, if it uses no coin objects; in this paper we consider only deterministic implementations of types.
An \emph{implemented object} is a method that implements a type.\todo{Not sure how to phrase this properly}


Each individual process  executes its program by executing a sequence of operations on shared objects, where the first operation is fixed and the -th operation invocation, , is a function of the responses  received from the preceding  operations (including flip operations).


Steps of multiple processes interleave, resulting in a \emph{history} , which is a sequence of \emph{steps}, i.e., invocations and responses corresponding to the operations executed by all processes on all base objects and all implemented objects.

Thus, the projection of  onto the steps of any process, , denoted , is a sequence of steps consistent with 's program.


We say that an operation  is \emph{atomic in history }, if 's invocation is either the last step in , or else is followed immediately in  by a matching response.
(Note that in related literature, an atomic operation is typically represented by a single event.
However, for technical reasons that become more clear in Section~\ref{strongAdversary.sec},  the invocation/response representation is more convenient in this paper.)
Operations on implemented objects are never atomic, while operations on base objects may or may not be atomic.
(We assume that an operation on an implemented object internally applies at least one base object operation.)
A history  is \emph{sequential} if all operations in  are atomic.

A history, , defines a partial \emph{happens before} order  on its operations, where, for operations  and ,  if and only if in  the response of  occurs before the invocation of .
(The relation  is a total order if and only if  is sequential.)


A sequential history, , is \emph{valid} if, for any object \obj, the projection of  onto the steps associated with \obj, denoted , is in the sequential specification of the type of \obj.
The new history formed from concatenating history  to the end of history  is denoted .\todo{Definition of concatenation seems out of place; this paragraph is about sequential histories}

A  history that arises from an algorithm that uses an implemented object \obj\
can be \emph{interpreted} as a history  of the same algorithm using a base object of the same type:
 is obtained from  by omitting, for each operation \op\ on \obj, say by process ,
  all the steps that appear in  after the invocation  and before the matching response .
Thus, for each operation \op\ on \obj\ in ,
\inv{\op} corresponds to the method invocation that simulates operation \op,
\rsp{\op} corresponds to the response of that method call, and
all operations on the base objects within the method call are omitted.
The \emph{set of histories of an implementation} is the set of histories where processes access an object instantiated using the implementation (and no other implemented object).\todo{I find the ``and nothing else'' part rather confusing. It is unclear, whether these histories are interpreted or not; I believe they are not.}
If  is a set of histories, then  denotes the set of interpretations of histories in .

For correctness, an interpreted history should ``correspond'' to one that could arise from an atomic object.
This is captured by the correctness property called \emph{linearizability} \cite{her:lin}.
(Note that in literature sometimes the term \emph{atomic object} is used to denote a linearizable object, see e.g.\ \cite{Lynch_DistributedAlgorithms1996}.)
An operation, \op, is \emph{complete in a history } if  contains both \inv{\op} and a matching \rsp{\op}.
Since a process is a sequential thread of control,
\todo{``From the definition of a process'' seems wrong. And why do we need this sentence, anyway?}
we see that every operation in , except possibly the last one, is complete.
A \emph{linearization} of a history  is a valid sequential history 
that contains all completed operations of  and possibly some non-completed ones (with matching responses added),
and where  extends .
A history  is \emph{linearizable} if it has at least one linearization.
(Note that a history containing operations on implemented objects is not linearizable in general because it encodes operations
  on base objects, but its interpretation might be linearizable.)
\todo{Perhaps we should introduce the convention that when we talk about the linearization of a history  we mean the linearization of ?}

An implementation of a shared object type is \emph{linearizable} if its set of histories
contains only histories whose interpretations are linearizable.

Flip operations on a coin object are always atomic,
and return a value from the set  defined earlier.
A vector  is called a \emph{coin flip vector}.
History  \emph{observes} the coin flip vector ,
if the -th flip operation in  returns value .
For a history  that contains  flip operations,
let  denote the prefix of  that ends with the -th invocation of a flip operation; if fewer than  flips occur during , then  denotes .

The order in which steps of processes interleave is given by a
\emph{schedule}, which is a (possibly infinite) sequence of process IDs.
History  \emph{observes} schedule ,
if in  the -th step is one executed by process .

Schedules are generated by an \emph{adversary}.
Typically, adversaries take the past execution into account to schedule the next process.
We are concerned primarily with two adversaries.
Informally, a \emph{weak adversary} cannot intervene between a flip operation and the next operation invocation by the same process.
This means that in any history, any flip operation by a process  is immediately followed by an invocation step by .
In contrast, a \emph{strong adversary} can use the response of the coin flip to determine which process takes the next step.
The following definitions serve to unify these adversaries, and can easily be seen to capture these informal notions.
An \emph{adversary} is a mapping .
An algorithm  together with an adversary  and a coin flip vector 
generates the unique history, denoted ,
that observes the schedule  and the coin flip vector ,
and where  all processes perform steps as dictated by .

\vspace{-2pt}
\begin{itemize}
\setlength{\itemsep}{-2.8pt}
\item
An adversary without additional restrictions is called an \emph{offline adversary}.
(An offline adversary can ``see'' all the coin flips in advance and can use them to make current scheduling decisions.)
\item
Adversary  is \emph{strong for algorithm }
if, for any two coin flip vectors  and  that have a common prefix of length ,
 .
(A strong adversary cannot use future coin flips to make current scheduling decisions.)
\item
Adversary  is \emph{weak for algorithm }
if it is  strong for algorithm  and is additionally constrained so that,
in , every flip by process  is followed immediately by the invocation of some operation by .
(A weak adversary cannot use future coin flips or the current coin flip to make the next scheduling decision.)
\item
Adversary  is \emph{oblivious} if  is a constant function, that is,  is the same for all .
(An oblivious adversary cannot use coin flips at all to make scheduling decisions.)
\end{itemize}

A strong adversary is commonly considered in the distributed algorithm literature.
Our weak adversary is similar to other adversaries in the literature,
such as that assumed by Chor, Israeli and Li \cite{CIL1987_PODC},
and further discussed by Abrahamson \cite{Abrahamson88_PODC}.
However, while their adversary cannot intervene between flip operations and writes,
it can intervene between flips and reads. (No other atomic operations are considered.)
Our goal is to compare the behaviour of systems with atomic objects to those with implemented objects,
for arbitrary objects that could support stronger operations than just reads and writes.
Consequently, we assume that an adversary treats all operations consistently;
it cannot intervene between a flip and some operations but not others.
Furthermore, always binding a flip operation to the next step of the same process,
instead of binding only if that next step is a write,
serves to strengthen our impossibility result for weak adversaries in Section~\ref{weakAdversary.sec}.

As we compare the powers of different adversaries in the remainder of the paper,
we will refer repeatedly to the following notion of equivalence:
\begin{definition}\label{def_advequiv}
  Let  and  be two algorithms and  and  be two adversaries.
  We say that  and  are \emph{equivalent}
  if for any coin flip vector ,
  there exists a sequential history that is a linearization of  and
  of .
\end{definition}

Some of the results discussed in Sections~\ref{strongAdversary.sec} and \ref{weakAdversary.sec} refer
to well-known progress requirements.
An implementation of a shared object type is \emph{wait-free} if in any history,
each method call incurs a finite number of steps.
An implementation is \emph{lock-free} if in any history, either
each method call takes finitely many steps, or else infinitely many method calls complete.
An implementation is \emph{terminating} if in any history, either
each method call takes finitely many steps, or else some process that takes finitely many
steps invokes a method call that it does not complete.



\label{strongAdversary.sec}\sloppy

In this section, we discuss a novel technique for limiting the additional power a strong adversary may gain
against an algorithm when atomic objects used by the algorithm are replaced with implemented objects.

\section{Strong Linearizability}
We define a correctness property stronger than linearizability, called \emph{strong linearizability},
and prove that under any strong adversary,
strongly linearizable implementations of shared objects preserve
the probability space of computations of an algorithm using such objects.
We also show that strong linearizability maintains
locality and composability---powerful properties that facilitate algorithm design.

For a set of histories , let \close{\HH} denote the prefix-closure of .
That is,  if and only if there is a sequence, , of invocation and response steps such that
. (Recall that the operator  denotes concatenation.)
Consider a function  that maps a set  of histories to a set  of histories.
We say that  is \emph{prefix preserving}, if for any two histories , where  is a prefix of ,  is a prefix of .

\begin{definition}\label{def:prefix-linearizability}
  A set of histories  is \emph{strongly linearizable} if there exists a function  mapping
  histories in  to sequential histories, such that\\label{eq:interpretation-property}
    \text{If , then either  is a prefix of  or vice versa.}\tag{}
  
H'_i &=& \inv{op_p}, \inv{op_q}, \inv{op_r}, \rsp{op_r}, \inv{\fl_r}, \rsp{\fl_r, i}, \rsp{op_p}, \rsp{op_q}

f(G') &=& \inv{op_r}, \rsp{op_r}\\
f(H'_0) &=&  \inv{op_r}, \rsp{op_r}, \inv{op_p}, \rsp{op_p}, \inv{op_q}, \rsp{op_q}, \inv{\fl_r}, \rsp{\fl_r, 0}\\
f(H'_1) &=&  \inv{op_r}, \rsp{op_r}, \inv{\fl_r}, \rsp{\fl_r, 1}, \inv{op_q}, \rsp{op_q}, \inv{op_p}, \rsp{op_p}
\label{eq:normalization_additional_property}
    \text{The response of the last operation in  occurs in .}\tag{}
  
    f^\ast(G)|{O_i}\text{ is a prefix of }f^\ast(H)|O_i.
  \label{eq:Proof_of_N}
  \text{for any operation  in :}\ \
    cf_i\prec_{H^\ast} op
    \ \Leftrightarrow\ cf_i\prec_{G^\ast} op.
  \label{eq:def_lambda}
  f_j\paren{H^{(k)}\|O_j}=f_j\paren{H^{(k-1)}\|O_j}\circ\lambda
\label{eq:loclity-i.s.}
  f^{(k)}(H)=f^{(k-1)}(H)\circ\lambda
\begin{split}
 f^{(k)}(H)|O_i
 &\stackrel{(\ref{eq:loclity-i.s.})}{=}
 \Bparen{f^{(k-1)}(H)\circ\lambda}\Big|O_i
 =
 f^{(k-1)}(H)|O_i
 \stackrel{I.H.}{=}
 f_i\paren{H^{(k-1)}\|O_i}
 \\ &=
 f_i\bbparen{\paren{H^{(k-1)}\circ\beta}\|O_i}
 =
 f_i\paren{H^{(k)}|O_i}.
\end{split}\begin{split}
  f_j\paren{H^{(k)}\|O_j}
  \stackrel{(\ref{eq:def_lambda})}{=}
  f_j\paren{H^{(k-1)}\|O_j}\circ\lambda
  \stackrel{I.H.}{=}
  \paren{f^{(k-1)}\paren{H}|O_j}\circ\lambda
  =
  \paren{f^{(k-1)}\paren{H}\circ\lambda}|O_j
  =
  f^{(k)}(H)|O_j.
\end{split}
  f^{(k)}(G)
  =
  f^{(k)}\paren{G^{(k-1)}}
  =
  f^{(k)}(G)
  =
  f^{(k-1)}\paren{G^{(k-1)}},

f^{(k-1)}\paren{H^{(k-1)}}
  \stackrel{(c)}=
  f^{(k-1)}\paren{H}.

  f(H)=
  \begin{cases}
    f^{(|H|)}(H) &\text{if  is finite, and}\\
    f^{(0)}(H)\circ\zeta^{(0)}(H)\circ \zeta^{(1)}(H)\circ \zeta^{(2)}(H)\circ\cdots & \text{if .}
  \end{cases}

  f(G)
  =
  f^{(\ell)}(G)
  \stackrel{(c)}{=}
  f^{(\ell)}(H)
  =
  f^{(0)}(H)\circ\zeta^{(0)}(H)\circ\cdots \circ\zeta^{(\ell)}(H)

    & pt_{E}(op_1)=t_E\bigl(inv(op_1)\bigr),\quad\text{and}\\
    & pt_{E}(op_{i})=\max\left\{t_E\bigl(inv(op_i)\bigr),\,T^\ast\bigl(pt_{E}(op_{i-1})\bigr)\right\}\quad\text{for .}
  
  pt_E(op_i)
  =
  T^\ast\bigl(pt_E(op_{i-1})\bigr)
  \leq
  \frac{pt_E(op_{i-1})+t_E\bigl(rsp(op_j)\bigr)}{2}
  < t_E\bigl(rsp(op_j)\bigr).
  
    pt_E(op)=pt''_{\alpha(E)}(op).
  
  \Phi(\AA):=\max\bigl\{\Exp{X_{p.\AA}}\,|\,p\in\PP\bigr\}.

     \Phi(\AA)\leq\frac{K_{\max}-1}{\sqrt{n}}.
  
    \sum_{0\leq j< m}\frac{b_j}{m}\leq \frac{K-1}{m}.
  
    \Phi(\AA)=\Omega(K_{max})=\Omega(\sqrt{n}).
  
  \Exp[p,\tc]{X(\AA_p,p,\tc)}=\Omega(\sqrt{n}).
  \label{eq:weak_after_averaging}
  \Exp[\tc]{X(\AA_p,p,\tc}=\Omega(\sqrt{n}).

 \Phi(\AA_p)=\max_{q\in\PP}\bigl(\Exp[\tc]{X(\AA_p,q,\tc)}\bigr)\geq\Exp[\tc]{X(\AA_p,p,\tc)}

  \Exp{L}=\Omega(|\PP_j|).
  
    \CondExp{L}{\EE}\geq (|\VV|-1)/2.
  \label{eq:register-configur-indep}
    \forall q,q'\in\VV:\ C_R(q)= C_R(q').\footnote{With  we denote the register configuration obtained if the random process .
    Similarly we write , , , and .}
  \label{eq:H'-indep}
    \forall q,q'\in\VV:\ H'(q)\sim_{\WW}H'(q').
  \label{eq:configur-indep}
    \forall q,q'\in\VV:\ C(q)\sim_{\WW}C(q').
  
    \forall q,q'\in\VV:\ H''(q)\sim_{\WW}H''(q').
  
    \forall q,q'\in\VV:\ H(q)\sim_{\WW}H(q').
  \label{eq:prob_p_in_Q}
    \Prob{p\in\QQ}=\frac{|\QQ|}{|\PP_0|}.
  \label{eq:prob_p_in_V}
    \Prob{\EE}=\Prob{p\in\VV}=\frac{|\VV|}{|\PP_0|}.
  \label{eq:L_p|p_in_V}
    \CondExp{L}{\EE}\cdot\Prob{\EE}
    \geq
    \frac{|\VV|-1}{2}\cdot\frac{|\VV|}{|\PP_0|}.
  \label{eq:L_p|p_in_R}
    \CondExp{L}{p\in\QQ}\cdot\Prob{p\in\QQ}
    \geq
    |\PP_0|\cdot\frac{|\QQ|}{|\PP_0|}.
  \label{eq:L_p|p_in_W}
    \CondExp{L}{p\not\in\QQ\cup\VV}\cdot\Prob{p\not\in\QQ\cup\VV}
    \geq
    (|\PP_0|-|\QQ|)\cdot\paren{1-\frac{|\QQ|+|\VV|}{|\PP_0|}}
    \\ \geq
    \frac{(|\PP_0|-|\QQ|-|\VV|)^2}{|\PP_0|}.
  
    Z=\max\bigl\{|\VV|(|\VV|-1)/2,\,|\QQ|\cdot |\PP_0|,\,(|\PP_0|-|\QQ|-|\VV|)^2\bigr\}.
  
    Z\geq\min\left\{\frac{(|\PP_0|/3)(|\PP_0|/3-1)}{2},\ \frac{|\PP_0|^2}{3},\ (|\PP_0|/3)^2\right\}=\Omega(|\PP_0|^2).
  
    \Exp{L}
    \geq
    \frac{1}{|\PP_0|}\cdot Z
    =
    \Omega(|\PP_0|).
  
    \forall 0\leq i<m:\ (1-\delta)\mu<|\PP_i|<(1+\delta)\mu.
  
    \bProb{Y_i\leq (1-\delta)\mu\,\vee\,Y_i\geq (1+\delta)\mu}\leq
    2\cdot e^{-\Omega(\mu)}
    =
    e^{-\Omega(\sqrt{n})}.
  
    (1-\delta)m<|P_0|,\dots,|\PP_{m-1}|<(1+\delta)m.
  \label{eq:L_given_Gamma}
    \CondExp{L}{\Gamma}=\Omega(m).
  
    K\leq\max_{0\leq i<m}|\PP_i|+1<(1+\delta)m+1,
  
    \Exp{X(\AA_p,p,\tc)}
    \geq
    \CondExp{X(\AA_p,p,\tc)}{\Gamma}\cdot\Prob{\Gamma}
    =
    \CondExp{L}{\Gamma}\cdot\bparen{1-o(1)}
    =\Omega(m).
  
\end{proof}




\paragraph{Acknowledgments}
We are grateful to the anonymous referees of STOC 2011 for their comments.
Thanks also to Faith Ellen for her detailed feedback.


\newpage
\bibliographystyle{alpha}
\newcommand{\etalchar}[1]{}
\begin{thebibliography}{GHHW07}

\bibitem[AAD{\etalchar{+}}93]{aadgms:snapshots}
Y.~Afek, H.~Attiya, D.~Dolev, E.~Gafni, M.~Merritt, and N.~Shavit.
\newblock Atomic snapsots of shared memory.
\newblock {\em J. ACM}, 40(4):873--890, 1993.

\bibitem[Abr88]{Abrahamson88_PODC}
Karl~R. Abrahamson.
\newblock On achieving consensus using a shared memory.
\newblock In {\em PODC '88: Proc. of the 6th annual ACM symposium on Principles
  of distributed computing}, pages 291--302, 1988.

\bibitem[AG88]{journals/ipl/AndersonG88}
James~H. Anderson and Mohamed~G. Gouda.
\newblock Atomic semantics of nonatomic programs.
\newblock {\em Inf. Process. Lett.}, 28(2):99--103, 1988.

\bibitem[Asp03]{Aspnes2003_DistrComp}
James Aspnes.
\newblock Randomized protocols for asynchronous consensus.
\newblock {\em Distributed Computing}, 16:165--175, 2003.

\bibitem[CIL87]{CIL1987_PODC}
Benny Chor, Amos Israeli, and Ming Li.
\newblock On processor coordination using asynchronous hardware.
\newblock In {\em PODC '87: Proc. of the 6th annual ACM symposium on Principles
  of distributed computing}, pages 86--97, 1987.

\bibitem[GHHW07]{ghhw:cas}
W.~Golab, V.~Hadzilacos, D.~Hendler, and P.~Woelfel.
\newblock Constant-rmr implementations of cas and other synchronization
  primitives using read and write operations.
\newblock In {\em PODC '07: Proc. of the 26th annual ACM symposium on
  Principles of distributed computing}, pages 0--0, 2007.

\bibitem[Gol10]{golab:phd}
W.~Golab.
\newblock {\em Constant-RMR Implementations of CAS and Other Synchronization
  Primitives Using Read and Write Operations}.
\newblock PhD thesis, University of Toronto, 2010.

\bibitem[Her91]{herl:wait}
M.~Herlihy.
\newblock Wait-free synchronization.
\newblock {\em ACM TOPLAS}, 13(1), January 1991.

\bibitem[HLM03]{her:of}
M.~Herlihy, V.~Luchangco, and M.~Moir.
\newblock Obstruction-free synchronization: Double-ended queues as an example.
\newblock In {\em ICDCS '03: Proc. of the 23rd International Conference on
  Distributed Computing Systems}, page 522, Washington, DC, USA, 2003. IEEE
  Computer Society.

\bibitem[HS08]{hs:art}
M.~Herlihy and N.~Shavit.
\newblock {\em The Art of Multiprocessor Programming}.
\newblock Morgan Kaufmann Publishers, first edition, 2008.

\bibitem[HW90]{her:lin}
M.~Herlihy and J.~M. Wing.
\newblock Linearizability: A correctness condition for concurrent objects.
\newblock {\em ACM TOPLAS}, 12(3):463--492, July 1990.

\bibitem[IL93]{il:timestamps}
A.~Israeli and M.~Li.
\newblock Bounded time-stamps.
\newblock {\em Distributed Computing}, 6(4):205--209, 1993.

\bibitem[Lyn96]{Lynch_DistributedAlgorithms1996}
Nancy Lynch.
\newblock {\em Distributed Algorithms}.
\newblock Morgan Kaufman, 1996.

\bibitem[VA86]{vit:atom}
P.~Vitanyi and B.~Awerbuch.
\newblock Atomic shared register access by asynchronous hardware.
\newblock In {\em Proc. of the 27th Annual Symposium on Foundations of Comupter
  Science}, pages 233--243. IEEE, 1986.

\bibitem[Vid88]{vid:registers}
K.~Vidyasankar.
\newblock Converting lamport's regular register to atomic register.
\newblock {\em Information Processing Letters}, 28(6):287--290, August 1988.

\bibitem[YA95]{yang:fast}
J.-H. Yang and J.~Anderson.
\newblock A fast, scalable mutual exclusion algorithm.
\newblock {\em Distributed Computing}, 9(1):51--60, August 1995.

\end{thebibliography}


\newpage
\appendix

\let\appsection\section

\appsection{Additional Examples}
\label{sec-appendix-examples}

When, in a randomized distributed algorithm,
atomic operations are replaced by their linearizable implementations,
the scheduler's power can increase.
Even when the power of the scheduler of the implemented algorithm is constrained significantly from its
power in the atomic model, the scheduler can still create a probability distribution of computations that
is dramatically different from what is attainable in the atomic case.

The following are various additional examples of this phenomenon.

\paragraph{Implementing multi-reader/single-writer registers from single-reader/single-writer registers.}

Let  be a two-reader atomic register initialized to 0,
accessed by writer , and readers  and  that are executing the code:
\begin{quote}
: .\MRwrite(1); \quad   := uniform-random; \quad   .\MRwrite()  \\
:  .\MRread () \\
:  .\MRread ()
\end{quote}

Suppose the strong adversary is trying to minimize the value of 's \MRread.
If  is an atomic register,
then the adversary's best strategy is to have  execute its
\MRread\ either before or after both of 's \MRwrite\ operations.
In either case the expected value of 's \MRread\ is 0.

Now suppose, instead, that  is implemented using
the algorithm shown in Figure~\ref{fig:multireader_registers}.
This construction
uses 6 single-reader/single-writer registers:

where the subscript - denotes a register written by  and read by .
Each is initialized to the initial value of .

\begin{figure}
 \begin{minipage}[t]{.4\textwidth}
 \begin{function}[H]
   \caption{.MRSWwrite()}
    ,  is a sequence number initialized to 1\;
     \;
    

 \end{function}
 \end{minipage}\hfill
 \begin{minipage}[t]{.55\textwidth}
 \begin{function}[H]
   \caption{.MRSWread()}
    code for process  \;
    \;
    \;
    \;
   let  be such that  \;
    \;
    \;
   return 
 \end{function}
 \end{minipage}\caption{Linearizable Implementation of MRSW Registers from SRSW Registers}\label{fig:multireader_registers}
\end{figure}

Under this implementation, the adversary's could schedule as follows:
First one step of : .\read, which will return 0;
then all of 's steps (that is,  all of its first \MRwrite, its flip and its second \MRwrite).
If the value of  is 1,
then the adversary next schedules the rest of 's steps,
followed by all 's steps in its .\MRread.
In this case the implementation of .\MRread by  returns 0.
If the value of  is -1,
then the adversary next schedules all 's steps in its .\MRread\
followed by the remainder of 's .\MRread.
In this case,  will discover the updated value of , when it
executes .\read, and the implementation returns -1.
Hence the expected value returned by  is -1/2.

Notice that this schedule is available even to the weak adversary.
It needs the coin flip value only after  has completed all its operations.
So reducing the power of the adversary from strong to weak
does not curtail its power sufficiently to retain the expected behaviour of the algorithm
when  is an atomic register.

This algorithm in Figure  \ref{fig:multireader_registers} is
the wait-free linearizable implementation
of multi-reader/multi-writer multivalued atomic registers from
single-reader/single-writer multivalued atomic registers
using unbounded sequence numbers
due to Vitanyi and Awerbuch (\cite{vit:atom})
when specialized to two readers and one writer.
The increased power of the weak adversary in the implementation over the power
of the strong adversary in the atomic case also extends to the case when there is more than one writer.



\paragraph{Implementing a queue with atomic increment objects.}
Let   be a queue object, initially empty and accessed by  ,  and  that are executing the code:

\begin{quote}
:   .\Enqueue(0)\\
:   .\Enqueue(1)\\
: .\Enqueue(2);\  := uniform-random; \ .\Dequeue; .\Dequeue; .\Dequeue\\
\end{quote}

The adversary's goal is to achieve the following:
\begin{enumerate}
 \item[(a)] all of 's \Dequeue\ operations succeed,
 \item[(b)] the \Dequeue operation that returns 1 precedes the \Dequeue operation that returns 2, and
 \item[(c)] the return value of 's first \Dequeue\ operation equals the result of the flip.
\end{enumerate}

Even if the adversary is strong, in order to achieve (b), it must schedule 's \Enqueue operation before 's flip operation.
Hence, by the time the flip occurs, the decision whether 0 or 1 is in front of the queue has been made, and cannot be changed by the adversary.
Therefore, the probability that 's first \Dequeue operation returns the value of the flip is at most .

\begin{figure}[bt]
 \begin{center}
 \begin{minipage}[t]{.4\textwidth}
 \begin{function}[H]
   \caption{.enq()}
     .\fetchInc()  \;
    .\xwrite()
 \end{function}
 \end{minipage}\hfill \begin{minipage}[t]{.4\textwidth}
   \begin{function}[H]
   \caption{.deq()}
   \While{\True}{
     .\read{}\;
     \For{}{
       \;
       \IlIf{}{\Return{}}
      }
    }
   \end{function}
 \end{minipage}
 \end{center}
 \caption{Implementation of the Herlihy-Wing Queue}\label{fig:queue}
\end{figure}
Herlihy and Wing give a linearizable implementation of
a queue using read-modify-write base objects
that support the operations \fetchSet, \fetchInc and \read
\cite{her:lin}.
The queue is represented by an unbounded array of items with a tail pointer.
The implementation is shown in Figure \ref{fig:queue}.

If the above algorithm uses this queue implementation, then the weak adversary could schedule as shown in Figure~\ref{fig:queue-schedule}.
Here, the left bar in the drawing of a queue method call denotes that method call's first shared memory access, and the right bar its last.
I.e., the \fetchInc operations of the \Enqueue operations occur in the order , , .
And both,  and  execute their  before the flip happens.
If the result of the flip is 0, then  writes immediately, before any \Dequeue operation starts.
Therefore, the first \Dequeue operation will return 0.
If the result of the flip is 1, then 's \xwrite is delayed until after the first \Dequeue operation completed.
In this case the first \Dequeue operation will return 1.
In either case, the second \Dequeue operation will return 0 or 1, and the third will return 2.
Hence, even the weak adversary can achieve with probability 1 that (a), (b), and (c) are satisfied.
\begin{figure}[bt]
\begin{tikzpicture}[
>=latex, very thick,
framed,
on grid,
auto,
]
\footnotesize

\newcommand{\outerop}[4][]{\fill[fill=gray!30] () rectangle ();
  \draw[-]  (#2) -- node[swap,#1] {#4} (#3);
  \draw[very thick] () -- ();
  \draw[very thick] () -- ();
}

\newcommand{\innerop}[4][]{\fill[fill=gray!70] () rectangle ();
  \draw[<->]  (#2) -- node[#1] {#4} (#3);
}


\node (r) {\normalsize };
\node[below = 1 of r] (q) {\normalsize };
\node[above = 1 of r] (p) {\normalsize };

\coordinate[right = .4 of p ] (pScanBegin);
\coordinate[right = 7.1 of p ] (pScanEnd) ;
\outerop{pScanBegin}{pScanEnd}{}


\coordinate[right = 3.4 of q ] (qUpdate2Begin) ;
\coordinate[right = 1.9 of qUpdate2Begin ] (qUpdate2End) ;
\outerop{qUpdate2Begin}{qUpdate2End}{}

\node[circle,text centered,fill=blue!20,draw=blue!200,thick,
  right = 6.4 of q] (coinflip) {};

\coordinate[right = 7.5 of q] (qDeq1Begin);
\coordinate[right = 2.3 of qDeq1Begin ] (qDeq1End);
\outerop{qDeq1Begin}{qDeq1End}{}

\coordinate[right = 10.5 of q] (qDeq2Begin);
\coordinate[right = 2.3 of qDeq2Begin ] (qDeq2End);
\outerop{qDeq2Begin}{qDeq2End}{}

\coordinate[right = 13.5 of q] (qDeq3Begin);
\coordinate[right = 2.3 of qDeq3Begin ] (qDeq3End);
\outerop{qDeq3Begin}{qDeq3End}{}


\coordinate[right = 1 of r ] (rUpdate1Begin) ;
\coordinate[right = 1.9 of rUpdate1Begin ] (rUpdate1End) ;
\outerop{rUpdate1Begin}{rUpdate1End}{}
\end{tikzpicture}

\vskip2\bigskipamount
\begin{tikzpicture}[
>=latex, very thick,
framed,
on grid,
auto,
]
\footnotesize

\newcommand{\outerop}[4][]{\fill[fill=gray!30] () rectangle ();
  \draw[-]  (#2) -- node[swap,#1] {#4} (#3);
  \draw[very thick] () -- ();
  \draw[very thick] () -- ();
}

\newcommand{\innerop}[4][]{\fill[fill=gray!70] () rectangle ();
  \draw[<->]  (#2) -- node[#1] {#4} (#3);
}


\node (r) {\normalsize };
\node[below = 1 of r] (q) {\normalsize };
\node[above = 1 of r] (p) {\normalsize };

\coordinate[right = .4 of p ] (pScanBegin);
\coordinate[right = 10.1 of p ] (pScanEnd) ;
\outerop{pScanBegin}{pScanEnd}{}


\coordinate[right = 3.4 of q ] (qUpdate2Begin) ;
\coordinate[right = 1.9 of qUpdate2Begin ] (qUpdate2End) ;
\outerop{qUpdate2Begin}{qUpdate2End}{}

\node[circle,text centered,fill=blue!20,draw=blue!200,thick,
  right = 6.4 of q] (coinflip) {};

\coordinate[right = 7.5 of q] (qDeq1Begin);
\coordinate[right = 2.3 of qDeq1Begin ] (qDeq1End);
\outerop{qDeq1Begin}{qDeq1End}{}

\coordinate[right = 10.5 of q] (qDeq2Begin);
\coordinate[right = 2.3 of qDeq2Begin ] (qDeq2End);
\outerop{qDeq2Begin}{qDeq2End}{}

\coordinate[right = 13.5 of q] (qDeq3Begin);
\coordinate[right = 2.3 of qDeq3Begin ] (qDeq3End);
\outerop{qDeq3Begin}{qDeq3End}{}


\coordinate[right = 1 of r ] (rUpdate1Begin) ;
\coordinate[right = 1.9 of rUpdate1Begin ] (rUpdate1End) ;
\outerop{rUpdate1Begin}{rUpdate1End}{}
\end{tikzpicture}
\caption{A ``bad'' scheduling for the queue algorithm.}
\label{fig:queue-schedule}
\end{figure}



\end{document}
