\documentclass{llncs}
\usepackage[dvips]{graphicx}
\newcommand{\mat}[1]{\mbox{ \boldmath }}





\title{A New Approach to Efficient Enumeration by Push-out Amortization}

\author{Takeaki Uno}
\institute{
\email{uno@nii.jp}, National Institute of Informatics, 
2-1-2, Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan
}

\begin{document}

\maketitle


\begin{abstract}
Enumeration algorithms have been one of recent hot topics in
 theoretical computer science.
Different from other problems, enumeration has many interesting aspects,
 such as the computation time can be shorter than the total output size,
 by sophisticated ordering of output solutions.
One more example is that the recursion of the enumeration algorithm is 
 often structured well, thus we can have good amortized analysis, and 
 interesting algorithms for reducing the amortized complexity.
However, there is a lack of deep studies from these points of views; there 
 are only few results on the fundamentals of enumeration, such as 
 a basic design of an algorithm that is applicable to many problems.
In this paper, we address new approaches on the complexity analysis,
 and propose a new way of amortized analysis {\it Push Out
 Amortization} for enumeration algorithms, where the computation time
 of an iteration is amortized by using all its descendant iterations.
We clarify sufficient conditions on the enumeration algorithm
 so that the amortized analysis works.
By the amortization, we show that many elimination orderings,
 matchings in a graph, connected vertex induced subgraphs in a graph,
 and spanning trees can be enumerated in  time for each
 solution by simple algorithms with simple proofs.
\end{abstract}




\vspace{-10mm}
\section{Introduction}
\vspace{-2mm}

Suppose that there is a simple algorithm to solve a problem, and we 
 have two improvements on the time complexity; (a) is by developing a 
 new algorithm with a small complexity, and (b) proves that its complexity 
 is actually small by complexity analysis.
Both types of improvements are important in theoretical computer science,
 but these days almost all results are on the type of (a).
Developing simple algorithms in (a) is non-trivial, thus many recent
 algorithms and their complexity analysis are difficult to understand.
Moreover, these types of algorithms often require some structures in the
 input, hence the problem formulations tend to be distant from the real world.
On contrary, (b) type has a great advantage on these points.
Even though the analysis is complicated, we can hide the difficulty by
 producing general statements applicable to many problems.
At least, we do not have to implement the complicated proofs in a program.
According to this motivation, we study on complexity analysis in this paper,
 that is amortized analysis for enumeration algorithms.

Amortized analysis is a paradigm of complexity analysis.
In the paradigm, we charge the cost of iterations with long
 computation time to those with shorter time, to make the upper
 bound of computation time of an iteration shorter.
Compared to usual complexity analysis considering the worst case, 
 the amortized analysis is often more powerful, for example dynamic
 tree, union find, and some enumeration algorithms\cite{DynamicTree,UnionFind}.
In the case of dynamic tree, the cost of changing the shape of the tree
 is charged to the preceding changes with smaller costs, and
 attains  average time complexity for each
 change where  is the size of the tree.
The time complexity is not attained by usual worst case analysis, 
 and it seems to be hard to obtain algorithms with the same
 complexity by the analysis.
This is similar to the union find algorithm, and the resulted 
 time complexity is  while straightforward algorithms
 take  time.
The concept of ``charging the cost'' made a paradigm shift on the 
 design of algorithms.
Some enumeration algorithms are designed so that the time complexity
 of an iteration is linear in the number of subproblems, to make the
 average computation time per child will be short\cite{KpRm95,SrTmUn97}.

Enumeration is now rapidly increasing its presence in theoretical 
 computer science.
One of the biggest reasons comes from its importance in application areas.
An example is the pattern mining problems in data mining.
The problem is to find all the patterns belonging to a class of structures,
 such as subsets and trees, such that the patterns satisfy some constraints
 in the given database, such as appearing at least  times.
One more motivation is that there have not been many studies including
 simple problems, thus there is a great possibility.
On the other hand, enumeration has several interesting aspects which we can 
 not observe in other problems.
For example, by dealing only with the difference between output solutions,
 we can often attain the computation time shorter than its output size, by 
 outputting the solutions by the differences.
Another example is its well-structured recursion. 
We can frequently have several structural results on enumeration, and
 it gives interesting algorithms and mathematical properties, while 
 it is hard to characterize when a brunch and bound algorithm cuts off
 subproblems.
Structured recursion often gives a good amortization.
Thus, there is a great interests on investigating amortized analysis 
 on enumeration algorithms.

According to this motivation and interests, this paper addresses
 amortized analysis of enumeration algorithms.
One of our goals on this topic is to fill the gap between theory and practice.
In practice, enumeration algorithms are often quite efficient and
 than the theoretical upper bound on the computation time.
Filling the gap gives understandings for both the theoretical and 
 practical properties on the data and algorithms; the properties 
 of the data accelerating the algorithms, and the the mechanism of the
 algorithms that enable us to attain smaller bounds.

We have observed that the recursive structures of enumeration algorithms
 satisfies a property which we call {\em bottom-expanded}.
Iterations of enumeration algorithms generate several recursive calls.
Thus, the number of iterations exponentially increases 
 in deeper levels of the recursion.
On the other hand, iterations on deeper levels often have relatively
 small inputs compared to upper levels.
Thus, we can expect that iterations near by the root of the recursion are
 few and spend a long time, and iterations near by the bottom of the
 recursions are many and spend very short time.
In practice, we can frequently observe this, especially in many kinds of
 pattern mining algorithms.
This also implies that the amortized computation time per
 iteration, or even per solution, is short.
This mechanism is what we call bottom-expanded.
We can see this mechanism not only in practice but also classic
 enumeration algorithms.

This mechanism motivated us to develop a good amortized analysis.
However, amortization is not easy in general, since it is hard to globally
 estimate the number of iterations and computation time.
Thus, in many existing studies, the computation time is amortized between 
 a parent and its children, and sometimes its
 grandchildren\cite{enumPEO,Ep90,Ep94,ksubtree,KpRm95,SrTmUn97}.
These local structures are easier to analyze than the global structures.
Extensions of this idea to more global structures are non-trivial.
For example, if we want to amortize between iterations in different subtrees
 of the recursion, we have to understand the relation and the correspondence
 between all iterations in the different subtrees.
This is often a difficult task.

In this paper, we propose a new way of carrying out amortized analysis of
 the time complexity of enumeration algorithms, and propose new algorithms
 for enumeration of matchings, elimination orderings, and connected 
 vertex induced subgraphs.
We also show that the amortized analysis can prove the existing complexity
 results in very simple ways, for the enumerations of spanning trees,
 perfect elimination orderings, and perfect sequences, while the existing
  algorithms often need sophisticated algorithms or data structures.
We can also see that the condition in the analysis is often satisfied
 in practice, thus this amortized analysis explains why the enumeration
 algorithms are efficient in practice.
These satisfy out basic motivations for this kind of studies.

Our amortization of an iteration is basically done with all its descendants.
For each iteration, we push out its computation time to its children 
 so that the assigned time is proportional to their computation time.
By applying this push-out from the root of the recursion to deeper levels,
 the long computation time near the root is diffused to deeper levels,
 that have shorter time on average.
Since it is very hard to capture the structure of the recursion, we give a
 condition called {\em Push-out condition} such that the amortized
 computation time is bounded when the condition is satisfied.
As the condition is given to the relation between each iteration and its
 children, proving the satisfiability of the condition is often not difficult.

As a result, to give a bound to amortized time complexity, what we have
 to do is to prove that the condition holds for some algorithms.
In this way, we propose algorithms for enumerating matchings,
 elimination orderings, and connected vertex induced subgraphs, and 
 prove that the condition holds for each.
These lead that these graph objects can be enumerated in constant time per
 solution.
We also show that the condition holds for the algorithm for spanning tree
 enumeration, and this gives a very simple proof compared to the existing
 ones.

The paper is organized as follows.
Section \ref{sec:prlm} is for preliminaries, and Section \ref{sec:PO} describes
 our Push out amortization and Push out condition.
Sections \ref{sec:elim}, \ref{sec:match}, \ref{sec:CIS} and \ref{sec:sptree}
 show algorithms and their proofs.
We conclude the paper in Section \ref{sec:cncl}.


\vspace{-2mm}
\section{Preliminaries}\label{sec:prlm}
\vspace{-2mm}

Let  be an enumeration algorithm.
Suppose that  is a recursive type algorithm, i.e., composed of 
 a subroutine that recursively calls itself several times (or none).
Thus, the recursion structure of the algorithm forms a tree.
We call the subroutine, or the execution of the subroutine an
 {\em iteration}.
Note that an iteration does not include the computation done in the 
 subroutines recursively called by the iteration, thus no iteration is
 included in another.
When the algorithm is composed of several kinds of subroutines and
 operations, and thus the recursion is a nest of several kind of subroutines.
In such cases, we consider a series of iterations of different types as 
 an iteration.

When an iteration  recursively calls an iteration ,  is called 
the {\em parent} of , and  is called a {\em child} of .
The {\em root iteration} is that with no parent.
For non-root iteration , its parent is unique, and is denoted by .
The set of the children of  is denoted by .
The parent-child relation between iterations forms a tree structure
 called a {\em recursion tree}.
An iteration is called a {\em leaf iteration} if it has no child, and 
 an {\em inner iteration} otherwise.

For iteration , an upper bound of the
 execution time (the number of operations) of  is denoted by .
Here we exclude the computation for the output process from the
 computation time.
We remind that  is the time for local execution time, and thus does
 not included the computation time in the recursive calls generated by .
For example, when ,  is written as  for some 
 constant .
 is the maximum  among all leaf iterations .
Here,  can be either constant, or a polynomial of the input size.
If  is an inner iteration, let .

In this paper, we assume that a graph is stored in a style of adjacency list.
For a vertex subset  of a graph , the {\em induced
 subgraph} of  is the graph whose vertex set is , and whose edge
 set contains the edges of  connecting two vertices of .
An edge is called a {\em bridge} if its removal increases the number
 of connected components in the graph.
An edge  is said to be {\em parallel} to  if  and  have the same
 endpoints, and be {\em series} to 
  if  is a bridge in  and not so in .

For an edge  of a graph , we denote the graph obtained by removing
  from  by , and that by removing  and edges adjacent
 to  by .
Similarly, for a vertex  of ,  is the graph obtained 
 from  by removing  and edges incident to .
For an edge  of , the graph {\em contracted} by , denoted 
 by , is the graph obtained by unifying
  the vertices  and  into one.
For an edge set ,  denotes the graph 
 .

\vspace{-2mm}
\section{Push Out Amortization}\label{sec:PO}
\vspace{-2mm}

The size of the input of each iteration for a recursive algorithm often
 decreases as the depth of the recursion. 
Thus, iterations near the root iteration take a relatively long
 time, and iterations near leaf iterations take a relatively short time.
Motivated by this observation, we amortize the computation time by 
 moving the computation time of each iteration to its children.
We carry out this move from the top to the bottom, so that the computation
 time of ancestors is recursively diffused to their descendants.
When we can obtain a short amortized computation time in this way, iterations
 with long computation times have many descendants at least proportional
 to their computation time; the average computation time per iteration
 will be long only when they have few descendants.
However, it is not easy to prove that any inner iteration has sufficiently
 many descendants.
Instead of that, we use some local conditions, related to a parent and
 children.
Suppose that  and  are two constants.\\

\vspace{-1mm}
\noindent 
{\bf \em Push Out Condition (PO condition):} for iteration ,
.\\
\vspace{-1mm}

\noindent
Fig. \ref{fig:POcond} shows a simple example of this condition.
After the assignment of the computation time of 
 to children and the remaining to itself, the inequation
  holds.
This implies that the computation time of one level of
 recursion intuitively increases as the depth, unless there are not so many
  leaf iterations.
Considering that enumeration algorithms usually spend less time in deeper 
 levels of the recursion, we can see that this implies that each iteration
  has many children on average.
This is in some sense not a typical condition to bound the time complexity 
 of recursive algorithms; usually we want to decrease the total computation
 time in deeper levels.
However, in the enumeration, the number of leaf iterations is fixed, and 
 thereby the total computation time in the bottom level is also fixed.
Thus, this condition implies that the total computation time is short.

\begin{theorem}\label{poa}
If any inner iteration of an enumeration algorithm satisfies PO condition,
 the amortized computation time of an iteration is .
\end{theorem}
\vspace{-3mm}

\begin{figure}[t]
\begin{center}
\begin{minipage}{160pt}
\vspace{-4mm}
  \begin{center}
  \includegraphics[scale=0.4]{pic1.eps}
  \end{center}
\caption{An iteration, its children, and their computation time represented
    by rectangle lengths; seems to be inefficient if children take long time,
     but this results in many descendants indeed.
 }\label{fig:POcond}
\end{minipage}
\hspace{5mm}
\begin{minipage}{160pt}
\vspace{-4mm}
  \begin{center}
  \includegraphics[scale=0.4]{pic2.eps}
  \end{center}
\vspace{-4mm}
  \caption{Push out rule; an iteration (center) receives computation time 
  from its parent (while rectangle), and delivers it together with its
   computation time (gray rectangle) to its children, proportional to
    their computation time.
  }\label{fig:POrule}
\end{minipage}
\end{center}
\vspace{-4mm}
\end{figure}

\proof
To prove the lemma, we charge the computation time.
We neither move the operations nor modify the algorithm, but just
 charge the computation time; the computation time can be considered
 as tokens, and we move the tokens so that each iteration has a small
 number of tokens.
We charge the computation time from an iteration to its children, i.e.,
 from the top of the recursion tree to the bottom.
Thus, an iteration receives computation time from its parent.
We charge (push out) its computation time and that received from its parent 
 to its children.
The computation time is charged to the children, in proportion of their
 individual computation time, using the following rule.\\

\vspace{-1mm}
\noindent
{\bf \em Push out rule:}
Suppose that iteration  receives a computation time of  from
 its parent, thus  has computation time of  in total.
Then, we fix  of the computation
 time to , and charge (push out) the remaining computation time of quantity
  to its children.
Each child  of  receives computation time proportional to ,
 i.e., 

\vspace{-1mm}

\vspace{-3mm}

\noindent
See Fig. \ref{fig:POrule} as an example.
According to this rule, we charge the computation time from the root
 iteration to leaf iterations, so that each inner iteration has 
  computation time.
Since the sum of the number of children over all nodes in a tree is
 no greater than the number of nodes in a tree,
  this is equivalent to that each iteration has  time.
The remaining issue is to prove the statement of the lemma by showing
 that each leaf iteration receives computation time of , 
 and it is sufficient to prove the statement.
To show that, we state the following claim.\\
 
\vspace{-2mm}
\noindent 
{\bf \em Claim}: if we charge computation time in the manner of the push
 out rule, each iteration  receives computation time of at most
  from its parent, i.e., \\
\vspace{-2mm}

\noindent
The root iteration satisfies this condition.
Suppose that an iteration  satisfies it.
Then, for any child  of ,  receives computation time of 

\vspace{-4mm}

\vspace{-3mm}

\noindent
Since PO condition is satisfied, 
 .
Thus, 

\vspace{-1mm}

\vspace{-2mm}

\noindent
By induction, any iteration satisfies the condition in the claim.
\qed

Note that PO condition does not require for the iterations to have
 at least two children.


\vspace{-2mm}
\section{Enumeration of Elimination Ordering}\label{sec:elim}
\vspace{-2mm}

Let  be a class of structures such as sets, graphs, and sequences.
Suppose that any structure  consists of a set of elements
 called an {\em ground set}, that is denoted by .
Examples of ground sets are the vertex set of a graph, the edge set of 
 a graph, the cells of a matrix, and the letters of a string.
The empty structure  is the unique structure that has
 , and hereafter we consider only  including 
 the empty structure.
For each , we define the set of 
 {\em removable elements} , such that for each removable element
 , the removal of  from  results in a structure
 .
We denote the removal of  from  by , and we assume that
 no two different structures can be generated by the removal of .
By using removable elements, we define {\em elimination orderings}.
An elimination ordering is an ordering  of
 elements in  iteratively removed from  until  is , i.e.,
 any  is removable in the structure  that is obtained by repeatedly 
 removing  to  from .
Example of elimination ordering are removing leaves from a tree,
 and perfect elimination ordering of a chordal graph.
A simple algorithm for enumerating elimination orderings can be described
 as follows.

\begin{tabbing}
{\bf Algorithm} EnumElimOrdering ()\\
1. {\bf if} , {\bf output}  where ;
 {\bf return}\\ 
2. {\bf for} each element  {\bf do}\\
3. \ \ {\bf if} , {\bf call} EnumElimOrdering
 ()\\
4. {\bf end for}
\end{tabbing}

Suppose that we are given a structure  in a class  and
 removable ground set  for ground set .
We suppose that for any , we can list all 
 in  time, where  is a polynomial
 of , and  is a function where  is an invariant of the
 input structure, such as the number of edges in the original graph.
We also assume that a removal of element takes  time.

\begin{theorem}\label{elim}
Elimination orderings of a class  can be enumerated
 in  time for each, if  holds for each 
 such that  is larger than a constant number .
\end{theorem}

\proof
We first bound the computation time except for the output processes, 
 that is, step 1 of EnumElimOrdering.
First, we choose two constants  and  such that 
  holds for any .
Since  is a polynomial function,  converges to
 , thus such  always exists.
Let  be an iteration.
When  inputs  with , the computation time 
 is , except for the output process.
Hence, we have .
For the case , the computation time of  is bounded
 by .
For the case , we have 

\vspace{-2mm}

\vspace{-3mm}

\noindent
since  has at least two children.
Thus,  satisfies PO condition with any constant .
From Theorem \ref{poa}, except for the output process, the computation
 time is bounded by  time for each iteration whose input
  has at least  elements.
Since any inner iteration  has exactly one child only if ,
 the number of inner iterations is bounded by the number of leaf
 iterations, multiplied by .
Therefore, the computation time for each elimination ordering can be 
 bounded by  time.

Next, let us consider the output process.
Instead of explicitly outputting elimination orderings, we output
 each elimination ordering  by the difference from  that is
  output just before .
We can output them compactly in this way.
Although the difference can be large up to , we can see
 that it is bounded by the number of operations done from the previous 
 output process.
Thus, the size of all output differences, except for the first one output 
 in the usual way, is at most proportional to the total computation time.
Therefore, the computation time for the output process
 is also bounded by  time for each.
\qed

The next corollary immediately follows from the theorem.

\begin{corollary}
For a given set class, elimination ordering can be enumerated 
 by EnumElimOrdering in  amortized time for each, if each
 inner iteration generates at least two recursive calls, and 
 takes  time, where  is a polynomial of . \qed
\end{corollary}

There are actually several elimination orderings to which this 
 theorem can be applied, and they are listed below.
For conciseness, we have described each by their structures and
 removable elements.\\

\vspace{-1mm}
{\bf Example (a): perfect elimination orderings of a chordal
 graph\cite{enumPEO}}\\
For a graph, a vertex is called {\em simplicial} if the vertices adjacent
 to it form a clique.
An elimination orderings of simplicial vertex is called {\em perfect
elimination ordering}\cite{elimord}, and a graph is {\em chordal}
 if it has a perfect elimination ordering.
We define  by the set of chordal graphs,  by the vertex
 set of , and  by the set of its simplicial vertices.

It is known that any chordal graph  admits a clique tree whose
 vertices are maximal cliques of .
If  is a clique, all vertices in  are simplicial.
If not, it is known that there are at least two cliques that has a vertex
 that is not included in the other maximal cliques.
Note that these cliques are leaf cliques of a clique tree, where 
 the vertices of a clique tree are maximal cliques of , 
 each edge connects overlapping cliques, and the maximal cliques 
 including any vertex forms a subtree of the clique tree.
The vertex is simplicial, hence  always holds.
Since we can check whether a vertex is simplicial or not in 
 time, we can enumerate all perfect elimination orderings in 
 time for each.
Note that although the algorithm in \cite{enumPEO} already attained the same 
 time complexity, our analysis yields much simpler algorithm and proof,
\\

\vspace{-1mm}
{\bf Example (b): perfect sequence\cite{perfectseq}}\\
 is the class of chordal graphs , and  is the set of
 maximal cliques in .
A maximal clique is removable if it is a leaf of some clique trees of ,
 and the removal of a maximal clique  from  is the removal of all
  vertices of  that do not belong to another maximal clique.
The removal of the vertices results in the graph that includes remaining 
 maximal cliques, and no new maximal clique appears in the graph.
Note that a clique tree has at least two leaves if it has more than one 
 vertex, thus .
An elimination ordering is called a {\em perfect sequence}.
Since all removable maximal cliques can be found in polynomial
 time in the number of maximal cliques\cite{perfectseq},
 all perfect sequences are enumerated in  time for each.\\

\vspace{-1mm}
The elimination orderings induced by following removable elements 
 can be also enumerated in  time for each.
 
\vspace{-2mm}
\begin{itemize}
\item non-cut vertices of connected graph
\item points on surface of convex hull of a point set in plane
\item leaves of a tree
\item vertices of degrees less than seven of a simple planar graph.
\end{itemize}


\vspace{-4mm}
\section{Enumeration of Matchings}\label{sec:match}
\vspace{-2mm}

A {\it matching} of a graph is an edge subset of a graph 
 such that no two edges are adjacent.
The matchings are enumerated by the following algorithm.

\begin{tabbing}
{\bf Algorithm} EnumMatching ()\\
1: {\bf if}  {\bf then output} ; {\bf return}\\
2: choose an edge  from \\
3: {\bf call} EnumMatching ()\\
4: {\bf call} EnumMatching ()
\end{tabbing}

\noindent
The time complexity of an iteration of EnumMatching is .
Since each inner iteration generates two children, the computation
 time for each matching is , and no better algorithm
 has been proposed in the literature.
A leaf iteration takes  time, thus .
However, PO condition may not hold for some iterations.
This cannot be better than  in straightforward ways.

PO condition does not hold when many edges are adjacent to .
In such cases,  has few edges, thus the subproblem of
  takes short time so that PO condition does not hold.
To avoid this situation, we modify the way of recursion as follows
 so that in such cases the iteration has many children.
Let  be the vertices adjacent to , and .
We partition the matchings to be enumerated into 

\vspace{-1mm}
\begin{itemize}
\item matchings including 
\item matchings including 
\item 
\item matchings including 
\item matchings including no edge incident to .
\end{itemize}
\vspace{-1mm}

We see that any matching belongs to exactly one of these groups.
To recur, we derive  and .
 and  can be derived in  time.
To shorten the computation time for  for , 
 we construct  from .
We add all edges of  incident to  to ,
 and remove all edges adjacent to , and obtain .
This can be done in  time.
To construct  for all , we need 

\vspace{-1mm}

\vspace{-3mm}

\noindent
time.
Thus, the computation time of an iteration is bounded by 
 with a constant .
The algorithm is described as follows.

\begin{tabbing}
{\bf Algorithm} EnumMatching2 ()\\
1: {\bf if}  {\bf then output} ; {\bf return}\\
2: choose a vertex  having the maximum degree in \\
3: {\bf call} EnumMatching2 ()\\
4: {\bf for} each edge  adjacent to ,
 {\bf call} EnumMatching2 ()
\end{tabbing}


\begin{theorem}\label{match}
All matchings in a graph can be enumerated in  time for each,
 with  space.
\end{theorem}

\proof
The amortized computation time for outputting process is
 bounded by  for each by using difference as elimination ordering.
Let us consider an inner iteration .
In the iteration , if , we generate at least 
 recursive calls, thus we have  and PO
 condition is satisfied by choosing sufficiently large .
If , the subproblems of  take 
 at least  time, and the subproblems of 
 take at least  time.
Hence, by setting , we have 

\vspace{-1mm}
 
\vspace{-3mm}

\noindent
 thereby PO condition holds.
Remind that each inner iteration generates two or more recursive calls, 
 the number of iterations does not exceed the twice the number of matchings.
Since any inner iteration satisfies PO condition and
 , the statement holds.
We remind that we assumed that there is no isolated vertex in the
 input graph, and thus the number of matchings in the graph is 
 greater than the number of vertices, and the number of edges.
\qed 


\vspace{-2mm}
\section{Enumeration of Connected Vertex Induced Subgraphs}\label{sec:CIS}
\vspace{-2mm}

We consider the problem of enumerating all vertex sets of the given graph  inducing connected subgraphs (connected induced subgraphs in short).
In literature, an algorithm is proposed that runs in  time for
 each\cite{AvFk96}.
For the enumeration, it is sufficient to enumerate all connected 
 induced subgraphs including the given vertex .
For a vertex  adjacent to , the connected induced subgraphs
 including  are partitioned into those including  and
 those not including .
The former subgraphs are connected induced subgraphs in 
  and the latter subgraphs are those in .
We have the following algorithm according to this partition, and we 
 prove that this algorithm satisfies PO condition.

\begin{tabbing}
{\bf Algorithm} EnumConnect ()\\
1: {\bf if}  {\bf then output} ; {\bf return}\\
2: choose a vertex  adjacent to \\
3: {\bf call} EnumConnect ()\\
4: {\bf call} EnumConnect ()
\end{tabbing}

\begin{theorem}\label{connect}
All connected vertex induced subgraphs in a graph can be enumerated in 
 time for each, with  space.
\end{theorem}

\proof
The correctness of the algorithm and the bound for memory usage are clear.
Since each inner iteration generates exactly two recursive calls, 
 the number of iterations is linearly bounded by the number of connected
 induced subgraphs, and .

As same to the matching enumeration, the computation time for outputting 
process is bounded by  for each.
An inner iteration  of the algorithm takes  time.
We assume that  for a constant , and 
 leaf iteration takes  time, since .
The constant factor of three is a key to PO condition.

The degree of  is at least  in , and 
  in .
Note that  and  are degrees of  and  in .
From this, we can see that the child iteration of  takes
 at least  time, and that of  takes
 at least  time.
Their sum is at least

\vspace{-1mm}

\vspace{-3mm}

\noindent
Setting , we can see that  satisfies PO condition.
Thanks to Theorem \ref{poa}, the computation time for each connected
 induced subgraph is .
\qed


\vspace{-2mm}
\section{Spanning Trees}\label{sec:sptree}
\vspace{-2mm}

A subtree  of a graph  is called a {\em spanning tree} if any 
 vertex of  is incident to at least one edge of .
Any spanning tree has  edges.
There have already been several studies on this
 problem\cite{KpRm95,SrTmUn97,Un99}, and \cite{Un99} is the simplest
 and uses an amortized analysis similar to us.
Without loss of generality, we assume that the input graph does not
 have any bridge.

Let  be an edge of .
If there are several edges  parallel to ,
 let  and .
We see that at most one edge from  can be included in a spanning tree, 
 thus we enumerate spanning trees in .
We further enumerate spanning trees in  if it is connected.
Any spanning tree is enumerated in exactly one of these.
When  has no parallel edges,  can have series edges.
If there are several edges  series to , again
 let  and .
We also see that any spanning tree includes at least  edges of ,
 thus we enumerate spanning trees in .
We further enumerate spanning trees in  if  is not the edges of
 a cycle.
Also in this case, any spanning tree is enumerated once among these.
By using these subdivisions, we construct the following algorithm.

\begin{tabbing}
{\bf Algorithm} EnumSpanningTree ()\\
1: {\bf if}  {\bf then output} ; {\bf return}\\ 
2: choose an edge  from \\
3:  ;\\ 
\ \ \ \ \ \\
4: {\bf for} each , \ {\bf call} EnumSpanningTree
 ()\\
5: {\bf for} each , \ {\bf call} EnumSpanningTree
 ()
\end{tabbing}

\noindent
We observe that these  subgraphs are actually isomorphic
 in both cases except for the edge label , thus constructing
 these graphs takes  time.


\begin{theorem}\label{spantree}
All spanning trees in a graph can be enumerated in  time
 for each, with  space.
\end{theorem}

\proof
The space complexity of the algorithm is  and an 
iteration takes  time since all edges parallel/series to
 an edge can be found by two connected
 component decomposition in  time.
If no edge is parallel or series to ,
 we generate two subproblems of  edges, thus PO condition holds.
If  edges are parallel or series to , we have at least 
 subproblems of  edges.
When ,  holds for some 
 , and PO condition holds.
When ,  holds, 
 PO condition holds for  and some .
Since each iteration generates at least two recursive calls or outputs a
 solution, the number of iterations is at most twice the number of solutions,
 therefore the statement holds.
\qed



\vspace{-2mm}
\section{Conclusion}\label{sec:cncl}
\vspace{-2mm}

We introduced a new way of looking at amortizing the computation time of 
 enumeration algorithms, by local conditions of recursion trees.
We clarified the conditions that are sufficient to give non-trivial upper 
 bounds for the average computation time of iterations that only depended
 on the relation between the computation time of a parent iteration and
  that of its child iterations.
We showed that many algorithms for elimination orderings have good
 properties so that the conditions are satisfied, and thus enumerated 
 in constant time for each.
Several other enumeration algorithms for matchings, connected vertex
 induced subgraphs, and spanning trees were also described, whose
 time complexities are  for each solution.

There are many problems for those enumeration algorithms that do not
 satisfy the conditions.
An interesting future work is to develop new algorithms for these
 problems, that satisfy the conditions.
Another direction is to study other conditions for bounding 
 amortized computation time.
Further studies on amortized analysis will possibly fill the gaps between 
 theory and practice, and clarify the mechanisms of enumeration algorithms.
 
\ \\
\noindent
{\bf \large Acknowledgments: }
Part of this research is supported by the Funding Program for World-Leading
 Innovative R\&D on Science and Technology, Japan, and Grant-in-Aid for
 Scientific Research (KAKENHI), Japan.


\vspace{-3mm}
\begin{thebibliography}{99}
\vspace{-4mm}
\bibitem{AvFk96} D. Avis and K. Fukuda, 
  Reverse Search for Enumeration,
  {\it Discrete Applied Mathematics} {\bf 65}, pp.~21-46 (1996).
\bibitem{enumPEO} L. S. Chandran, L. Ibarra, F. Ruskey, and J. Sawada, 
  Generating and Characterizing the Perfect Elimination Orderings of a
  Chordal Graph,
  {\it Theoretical Computer Science} {\bf 307}, pp.~303-317 (2003).
\bibitem{Ep90} D. Eppstein,
  Finding the  Smallest Spanning Trees,
  {\it SWAT 90, Lecture Notes in Computer Science} {\bf 447}, pp.~38-47 (1990).
\bibitem{Ep94} D. Eppstein,
  Finding the  Shortest Paths,
  {\it FOCS 94}, pp.~154-165 (1994).
\bibitem{ksubtree} R. Ferreira, R. Grossi and R. Rizzi,
  Output-sensitive Listing of Bounded-size Trees in Undirected Graphs'',
  {\it ESA 2011, Lecture Notes in Computer Science} {\bf 6942},
   pp.~275-286 (2011).
\bibitem{FIMI} Frequent Itemset Mining Dataset Repository, http://fimi.cs.helsinki.fi/data/
\bibitem{UnionFind} H. N. Gabow, R. E. Tarjan,
  A Linear-time Algorithm for a Special Case of Disjoint Set Union
  {\it STOC '83}, pp.~246-251 (1983).
\bibitem{KpRm95} H. N. Kapoor and H. Ramesh, 
  Algorithms for Enumerating all Spanning Trees of Undirected and Weighted
  Graphs,
  {\it SIAM Journal on Computing}, {\bf 24}, pp.~247-265 (1995).
\bibitem{perfectseq} Y. Matsui, R. Uehara and T. Uno,
  Enumeration of the Perfect Sequences of a Chordal Graph,
  {\it Theoretical Computer Science} {\bf411}, pp.~3635-3641 (2010).
\bibitem{edgecol} Y. Matsui and T. Matsui,
  Enumeration Algorithm of the Edge Colorings in Bipartite Graphs,
  {\it Lecture Notes in Computer Science} {\bf 1120},pp.~18--26 (1995).
\bibitem{elimord} D. J. Rose, R. E. Tarjan, and G. S. Lueker,
  Algorithmic Aspects of Vertex Elimination on Graphs,
  {\it SIAM Journal on Computing} {\bf 5}, pp.~266-283 (1976).
\bibitem{SrTmUn97} A. Shioura, A. Tamura and T. Uno, 
  An Optimal Algorithm for Scanning All Spanning Trees of Undirected Graphs,
  {\it SIAM Journal on Computing} {\bf 26}, pp.~678-692 (1997).
\bibitem{DynamicTree} D. D. Sleator, R. E. Tarjan, 
  A Data Structure for Dynamic Trees, 
  {\it STOC '81}, pp.~114--122 (1981).
\bibitem{Un99} T. Uno, 
  A New Approach for Speeding Up Enumeration Algorithms and Its Application
  for Matroid Bases, {\it COCOON 99,
  Lecture Notes in Computer Science} {\bf 1627}, pp.~349-359 (1999).

\end{thebibliography}




\end{document}
